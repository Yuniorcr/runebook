<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ru" datatype="htmlbody" original="tensorflow">
    <body>
      <group id="tensorflow">
        <trans-unit id="c879ab9978d630c89fff8cae92e9db4343f09490" translate="yes" xml:space="preserve">
          <source>The minibatch size &lt;code&gt;N&lt;/code&gt; is extracted from &lt;code&gt;sparse_shape[0]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="adafbd0cce3b5754618ae2da17c9902f6cae336a" translate="yes" xml:space="preserve">
          <source>The mode of a gamma distribution is &lt;code&gt;(shape - 1) / rate&lt;/code&gt; when &lt;code&gt;shape &amp;gt; 1&lt;/code&gt;, and &lt;code&gt;NaN&lt;/code&gt; otherwise. If &lt;code&gt;self.allow_nan_stats&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, an exception will be raised rather than returning &lt;code&gt;NaN&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ccf3763dcb0dcb0536020350fdb380b642b68362" translate="yes" xml:space="preserve">
          <source>The model architecture, allowing to re-instantiate the model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c5272bf28fc9b4f93ddcbb47262f8771af50748" translate="yes" xml:space="preserve">
          <source>The model weights.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff3e822749aa45b217109835dbb8a4c4930f2cb5" translate="yes" xml:space="preserve">
          <source>The most basic RNN cell.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92ef4fa2dcf9047cf590c368e2e76fcb4cd1ce27" translate="yes" xml:space="preserve">
          <source>The most common initialization pattern is to use the convenience function &lt;code&gt;global_variables_initializer()&lt;/code&gt; to add an Op to the graph that initializes all the variables. You then run that Op after launching the graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be28bd956ec3dd71f96903bf3e90304d770c19da" translate="yes" xml:space="preserve">
          <source>The most common use case for this function occurs when feature ids and their corresponding values are stored in &lt;code&gt;Example&lt;/code&gt; protos on disk. &lt;code&gt;parse_example&lt;/code&gt; will return a batch of ids and a batch of values, and this function joins them into a single logical &lt;code&gt;SparseTensor&lt;/code&gt; for use in functions such as &lt;code&gt;sparse_tensor_dense_matmul&lt;/code&gt;, &lt;code&gt;sparse_to_dense&lt;/code&gt;, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c37b111e7c1a2dfc5ac3657ac3eb40f6697f662" translate="yes" xml:space="preserve">
          <source>The moving averages are computed using exponential decay. You specify the decay value when creating the &lt;code&gt;ExponentialMovingAverage&lt;/code&gt; object. The shadow variables are initialized with the same initial values as the trained variables. When you run the ops to maintain the moving averages, each shadow variable is updated with the formula:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="94df2d413ae96a0a117fa46769022f06a44e6ff4" translate="yes" xml:space="preserve">
          <source>The name of the &lt;code&gt;TensorArray&lt;/code&gt; (even if passed in) is uniquified: each time a new &lt;code&gt;TensorArray&lt;/code&gt; is created at runtime it is assigned its own name for the duration of the run. This avoids name collisions if a &lt;code&gt;TensorArray&lt;/code&gt; is created within a &lt;code&gt;while_loop&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8fcdbd427dc016b6defe717193d13e8a5ff2954b" translate="yes" xml:space="preserve">
          <source>The name of the device on which &lt;code&gt;values&lt;/code&gt; will be produced, or &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="37ecde1c1b33e8dfdb61f844e3753c1a790bb482" translate="yes" xml:space="preserve">
          <source>The name of the device on which this tensor will be produced, or None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="513390ceda7c6a57cd491aa38c353737462bdd7c" translate="yes" xml:space="preserve">
          <source>The name of the device to which this op has been assigned, if any.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf636fc4624183a99bcc4c7c7fbe0a44c0ebb8f7" translate="yes" xml:space="preserve">
          <source>The name of the module which registered the flag with this name. If no such module exists (i.e. no flag with this name exists), we return default.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d3b4cc387786491f2cb51bc2065fde0324f391f" translate="yes" xml:space="preserve">
          <source>The name of the scope itself can be captured by &lt;code&gt;with g.name_scope(...) as scope:&lt;/code&gt;, which stores the name of the scope in the variable &lt;code&gt;scope&lt;/code&gt;. This value can be used to name an operation that represents the overall result of executing the ops in a scope. For example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c5bdc797aad3e657271b859b7dc8d873a1c5776b" translate="yes" xml:space="preserve">
          <source>The name of the table.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f8f2953a3d445a0fc6a56065d426682fba17223" translate="yes" xml:space="preserve">
          <source>The name of the underlying accumulator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="96809ba43c27a83619ec011b30fe50853f001590" translate="yes" xml:space="preserve">
          <source>The name of the underlying queue.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f89f168ead7cdce407789bc51f70ca1fc8c5be20" translate="yes" xml:space="preserve">
          <source>The name of this &lt;code&gt;IndexedSlices&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="da911221082c7da030b3afee2bb6138d5ed15e00" translate="yes" xml:space="preserve">
          <source>The name of this ExponentialMovingAverage object.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0b8c507e7f0e7b6a1373421e05579d1965778f3" translate="yes" xml:space="preserve">
          <source>The name of this head.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d713b72d40825a59e98f4bc908a8370877bdd57f" translate="yes" xml:space="preserve">
          <source>The name of this variable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d01abc5e97d43035681b7621f742a7e0e4f87f3" translate="yes" xml:space="preserve">
          <source>The name or URL of the session master.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="947e2daa3bb98d0bc923e5ca008b708b99d1db05" translate="yes" xml:space="preserve">
          <source>The natural log of the determinant of &lt;code&gt;matrix&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e172fe7e51a351d69f78266150cbe93d41a966dd" translate="yes" xml:space="preserve">
          <source>The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e63348dbbced4923128564bd3253cc502100689d" translate="yes" xml:space="preserve">
          <source>The new generator will be initialized by one of the following ways, with decreasing precedence: (1) If &lt;code&gt;copy_from&lt;/code&gt; is not None, the new generator is initialized by copying information from another generator. (3) If &lt;code&gt;state&lt;/code&gt; and &lt;code&gt;alg&lt;/code&gt; are not None (they must be set together), the new generator is initialized by a state.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8cbbc58e1d6e45f7538d48a88e9145ff0816310e" translate="yes" xml:space="preserve">
          <source>The new generator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c92d3a1d88d5bb846fb6922638676753f654e62e" translate="yes" xml:space="preserve">
          <source>The new generators will be put on the current device (possible different from the old generator's), for example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e426c7afe47b5f55c3dfa85e599a773e3d38d777" translate="yes" xml:space="preserve">
          <source>The new variable is added to the graph collections listed in &lt;code&gt;collections&lt;/code&gt;, which defaults to &lt;code&gt;[GraphKeys.GLOBAL_VARIABLES]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b23ad53a08722aede7339ef0814174529ec55158" translate="yes" xml:space="preserve">
          <source>The next element in the queue, i.e. a tuple &lt;code&gt;(inputs, targets)&lt;/code&gt; or &lt;code&gt;(inputs, targets, sample_weights)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8bc3974c53a4bc03fb9e4842cff39c86a0d8870a" translate="yes" xml:space="preserve">
          <source>The non-zero values in the represented dense tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a794ea3a692cf3249d1fcc991ee60c5b4cf01417" translate="yes" xml:space="preserve">
          <source>The normal &lt;code&gt;ServingInputReceiver&lt;/code&gt; always returns a feature dict, even if it contains only one entry, and so can be used only with models that accept such a dict. For models that accept only a single raw feature, the &lt;code&gt;serving_input_receiver_fn&lt;/code&gt; provided to &lt;a href=&quot;../../compat/v1/estimator/estimator#export_saved_model&quot;&gt;&lt;code&gt;Estimator.export_saved_model()&lt;/code&gt;&lt;/a&gt; should return this &lt;code&gt;TensorServingInputReceiver&lt;/code&gt; instead. See: https://github.com/tensorflow/tensorflow/issues/11674</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="61b02be5d26057c5d50879cdbf7a93b5057253da" translate="yes" xml:space="preserve">
          <source>The number of batches in the Sequence.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e8251a908793c7f7ac158d55fc205bd7b475f75" translate="yes" xml:space="preserve">
          <source>The number of classes, &lt;code&gt;K&lt;/code&gt;, must not exceed:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26bbe52e7e5ab91d858fdf92eb9164c22baffaaa" translate="yes" xml:space="preserve">
          <source>The number of cores per replica.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1fb954517b208177e2433a3bd0d5008bff1f85f" translate="yes" xml:space="preserve">
          <source>The number of devices attached to this input pipeline. This will be automatically set by MultiDeviceIterator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4e68945735abb3b2e9d8c11a95d91c8a876e87a" translate="yes" xml:space="preserve">
          <source>The number of dimensions of the input tensors must match, and all dimensions except &lt;code&gt;axis&lt;/code&gt; must be equal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="516b803f18d035a65572fcbdfc2900dc7b8d516a" translate="yes" xml:space="preserve">
          <source>The number of ragged dimensions in this ragged tensor value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04f639280f7ec10513cb4839cfeda1135a578887" translate="yes" xml:space="preserve">
          <source>The number of ragged dimensions in this ragged tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="87460175f8c6b31c8d362f42e4ca508e7497ab37" translate="yes" xml:space="preserve">
          <source>The number of replicas of the computation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="605dc8d4d33374c29a7f650a9b0f5eab0e0e6145" translate="yes" xml:space="preserve">
          <source>The number of tasks defined in the given job.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a2d5e323b8ac2af372dafbb2ccb4f737135220d" translate="yes" xml:space="preserve">
          <source>The numerics checking mechanism will cause any TensorFlow eager execution or graph execution to error out as soon as an op's output tensor contains infinity or NaN.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4302dccd839f803738d3151980944012f9528e1e" translate="yes" xml:space="preserve">
          <source>The numpy dtype of values in this tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="10073c85f700d07cd9e27b920a2bb28c0e1e6aa0" translate="yes" xml:space="preserve">
          <source>The object will be registered under the key 'package&amp;gt;name' where &lt;code&gt;name&lt;/code&gt;, defaults to the object name if not passed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ceae2a98903e4e9554b3ffa1be72b116c87afeec" translate="yes" xml:space="preserve">
          <source>The one-hot tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2214e2794dfd82c41f2566b3dfe3c682d8ae3fc2" translate="yes" xml:space="preserve">
          <source>The only change you have to do to the single program code is to indicate if the program is running as the &lt;em&gt;chief&lt;/em&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ac713e042499abe0ad5ad2caf8a0b3b2c0f1068" translate="yes" xml:space="preserve">
          <source>The only difference with a regular &lt;code&gt;Session&lt;/code&gt; is that an &lt;code&gt;InteractiveSession&lt;/code&gt; installs itself as the default session on construction. The methods &lt;a href=&quot;../../tensor#eval&quot;&gt;&lt;code&gt;tf.Tensor.eval&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../operation#run&quot;&gt;&lt;code&gt;tf.Operation.run&lt;/code&gt;&lt;/a&gt; will use that session to run ops.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31601f998aa3aad1333ec413e01d618a81800515" translate="yes" xml:space="preserve">
          <source>The only public method of a 'Flag' object is parse(), but it is typically only called by a 'FlagValues' object. The parse() method is a thin wrapper around the 'ArgumentParser' parse() method. The parsed value is saved in .value, and the .present attribute is updated. If this flag was already present, an Error is raised.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0ab6e564ddbe98c027618ff7ffc593086f5c97a" translate="yes" xml:space="preserve">
          <source>The op extracts fields from a serialized protocol buffers message into tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4524786c0641258f1c43ff84a36d1e64fc7c7b2a" translate="yes" xml:space="preserve">
          <source>The op isn't guaranteed to raise an error if the input matrix is not invertible. &lt;a href=&quot;../debugging/check_numerics&quot;&gt;&lt;code&gt;tf.debugging.check_numerics&lt;/code&gt;&lt;/a&gt; can be applied to the output to detect invertibility problems.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb943cd0e3db6ad9fffc66a537388998786a4519" translate="yes" xml:space="preserve">
          <source>The op serializes protobuf messages provided in the input tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4c5664cfd278d67f9bbf9eb0ae9656ce1cd8f8e" translate="yes" xml:space="preserve">
          <source>The op uses LU decomposition with partial pivoting to compute the inverses.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47e700a39a6284fa854225b8729284d0f8f18e3a" translate="yes" xml:space="preserve">
          <source>The operation blocks until sufficient number of gradients have been successfully applied to the accumulator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8790d3057b1a7dc53661bb4f237b850fe1057d19" translate="yes" xml:space="preserve">
          <source>The operation casts &lt;code&gt;x&lt;/code&gt; (in case of &lt;code&gt;Tensor&lt;/code&gt;) or &lt;code&gt;x.values&lt;/code&gt; (in case of &lt;code&gt;SparseTensor&lt;/code&gt; or &lt;code&gt;IndexedSlices&lt;/code&gt;) to &lt;code&gt;dtype&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93211cf6300703e21c3cdf804cff040a7b2a55a6" translate="yes" xml:space="preserve">
          <source>The operation logs a warning if we attempt to set to a time step that is lower than the accumulator's own time step.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec87d8afa16231d11efcb471f5e52a7d2a894559" translate="yes" xml:space="preserve">
          <source>The operation must run in the same address space as the Python program that calls &lt;a href=&quot;numpy_function&quot;&gt;&lt;code&gt;tf.numpy_function()&lt;/code&gt;&lt;/a&gt;. If you are using distributed TensorFlow, you must run a &lt;a href=&quot;distribute/server&quot;&gt;&lt;code&gt;tf.distribute.Server&lt;/code&gt;&lt;/a&gt; in the same process as the program that calls &lt;a href=&quot;numpy_function&quot;&gt;&lt;code&gt;tf.numpy_function&lt;/code&gt;&lt;/a&gt; you must pin the created operation to a device in that server (e.g. using &lt;code&gt;with tf.device():&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2eb160b49a8c0e9200d47f580c46000b2caec03e" translate="yes" xml:space="preserve">
          <source>The operation must run in the same address space as the Python program that calls &lt;a href=&quot;py_func&quot;&gt;&lt;code&gt;tf.compat.v1.py_func()&lt;/code&gt;&lt;/a&gt;. If you are using distributed TensorFlow, you must run a &lt;a href=&quot;../../distribute/server&quot;&gt;&lt;code&gt;tf.distribute.Server&lt;/code&gt;&lt;/a&gt; in the same process as the program that calls &lt;a href=&quot;py_func&quot;&gt;&lt;code&gt;tf.compat.v1.py_func()&lt;/code&gt;&lt;/a&gt; and you must pin the created operation to a device in that server (e.g. using &lt;code&gt;with tf.device():&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3cd48c99e2e66fcb5c484164b2f3d03eb702bcd9" translate="yes" xml:space="preserve">
          <source>The operation must run in the same address space as the Python program that calls &lt;a href=&quot;py_function&quot;&gt;&lt;code&gt;tf.py_function()&lt;/code&gt;&lt;/a&gt;. If you are using distributed TensorFlow, you must run a &lt;a href=&quot;distribute/server&quot;&gt;&lt;code&gt;tf.distribute.Server&lt;/code&gt;&lt;/a&gt; in the same process as the program that calls &lt;a href=&quot;py_function&quot;&gt;&lt;code&gt;tf.py_function()&lt;/code&gt;&lt;/a&gt; and you must pin the created operation to a device in that server (e.g. using &lt;code&gt;with tf.device():&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="12a018d5761a73fb797a8185b603d78340c9da47" translate="yes" xml:space="preserve">
          <source>The operation of &lt;code&gt;raw_rnn&lt;/code&gt;, in pseudo-code, is basically the following:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="666e744ea2e81d27484651c5a90d39fe7463b5f8" translate="yes" xml:space="preserve">
          <source>The operation returns the cardinality of &lt;code&gt;dataset&lt;/code&gt;. The operation may return &lt;a href=&quot;../experimental#INFINITE_CARDINALITY&quot;&gt;&lt;code&gt;tf.data.experimental.INFINITE_CARDINALITY&lt;/code&gt;&lt;/a&gt; if &lt;code&gt;dataset&lt;/code&gt; contains an infinite number of elements or &lt;a href=&quot;../experimental#UNKNOWN_CARDINALITY&quot;&gt;&lt;code&gt;tf.data.experimental.UNKNOWN_CARDINALITY&lt;/code&gt;&lt;/a&gt; if the analysis fails to determine the number of elements in &lt;code&gt;dataset&lt;/code&gt; (e.g. when the dataset source is a file).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b1aa4d33454ed3399a3f83fe0b47437e589dd1db" translate="yes" xml:space="preserve">
          <source>The operation supports data types (for &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;dtype&lt;/code&gt;) of &lt;code&gt;uint8&lt;/code&gt;, &lt;code&gt;uint16&lt;/code&gt;, &lt;code&gt;uint32&lt;/code&gt;, &lt;code&gt;uint64&lt;/code&gt;, &lt;code&gt;int8&lt;/code&gt;, &lt;code&gt;int16&lt;/code&gt;, &lt;code&gt;int32&lt;/code&gt;, &lt;code&gt;int64&lt;/code&gt;, &lt;code&gt;float16&lt;/code&gt;, &lt;code&gt;float32&lt;/code&gt;, &lt;code&gt;float64&lt;/code&gt;, &lt;code&gt;complex64&lt;/code&gt;, &lt;code&gt;complex128&lt;/code&gt;, &lt;code&gt;bfloat16&lt;/code&gt;. In case of casting from complex types (&lt;code&gt;complex64&lt;/code&gt;, &lt;code&gt;complex128&lt;/code&gt;) to real types, only the real part of &lt;code&gt;x&lt;/code&gt; is returned. In case of casting from real types to complex types (&lt;code&gt;complex64&lt;/code&gt;, &lt;code&gt;complex128&lt;/code&gt;), the imaginary part of the returned value is set to &lt;code&gt;0&lt;/code&gt;. The handling of complex types here matches the behavior of numpy.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="633cea8689c5cf5392fe403777889735793f4042" translate="yes" xml:space="preserve">
          <source>The operation that (conditionally) applies a gradient to the accumulator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be412a34e6685ca321b38fae5794bdc3419b0f98" translate="yes" xml:space="preserve">
          <source>The operation that closes the queue.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2e5f0ceeecca2686d84b94bc3fa75e7225fc1f4" translate="yes" xml:space="preserve">
          <source>The operation that enqueues a batch of tuples of tensors to the queue.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="59123e207e4121d978f651d18670cf600162571e" translate="yes" xml:space="preserve">
          <source>The operation that enqueues a new tuple of tensors to the queue.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99898763428cef52c27daa776a42aecf7741168c" translate="yes" xml:space="preserve">
          <source>The operation that failed, if known.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21259fcd1f38c739e8e877f9049101d45ed6faeb" translate="yes" xml:space="preserve">
          <source>The operation that initializes the table.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d4203f69addf4215a9f75fa10050cf92a106167" translate="yes" xml:space="preserve">
          <source>The operation was aborted, typically due to a concurrent action.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d2fbee8f6406df6777f5eab458e89a6b6c00fa3" translate="yes" xml:space="preserve">
          <source>The operator before inversion.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dca167c313a3afe14fe0a9adc69f0f5e8d1c9afb" translate="yes" xml:space="preserve">
          <source>The operator before taking the adjoint.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd77aed70dd9284b7d6c31fc643a9815d5df72eb" translate="yes" xml:space="preserve">
          <source>The optimization options associated with the dataset. See &lt;a href=&quot;experimental/optimizationoptions&quot;&gt;&lt;code&gt;tf.data.experimental.OptimizationOptions&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d41bfab8487c080f4a52aaa4180a3c960ee957f3" translate="yes" xml:space="preserve">
          <source>The optimizer adds nodes to the graph to collect gradients and pause the trainers until variables are updated. For the Parameter Server job:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54fde7cedba37819ea597c67bd03d0eccd830d20" translate="yes" xml:space="preserve">
          <source>The optional &lt;code&gt;feed_dict&lt;/code&gt; argument allows the caller to override the value of tensors in the graph. Each key in &lt;code&gt;feed_dict&lt;/code&gt; can be one of the following types:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="802a640d90b4b6b27f2c21f94a40f0d9fba6af8f" translate="yes" xml:space="preserve">
          <source>The optional &lt;code&gt;feed_dict&lt;/code&gt; argument allows the caller to override the value of tensors in the graph. See run() for more information.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eacc79006fb44e1fd531dba67ace0548202f07d4" translate="yes" xml:space="preserve">
          <source>The optional &lt;code&gt;num_updates&lt;/code&gt; parameter allows one to tweak the decay rate dynamically. It is typical to pass the count of training steps, usually kept in a variable that is incremented at each step, in which case the decay rate is lower at the start of training. This makes moving averages move faster. If passed, the actual decay rate used is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d5f2024e82b1f112cdc773a7a0d099d108bab47" translate="yes" xml:space="preserve">
          <source>The optional &lt;code&gt;options&lt;/code&gt; argument expects a [&lt;code&gt;RunOptions&lt;/code&gt;] proto. The options allow controlling the behavior of this particular step (e.g. turning tracing on).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5dd8b71d547dc1502ab3faa2f08ccf41ba7e8b18" translate="yes" xml:space="preserve">
          <source>The optional &lt;code&gt;reshape&lt;/code&gt; argument, if &lt;code&gt;True&lt;/code&gt;, allows restoring a variable from a save file where the variable had a different shape, but the same number of elements and type. This is useful if you have reshaped a variable and want to reload it from an older checkpoint.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e970942946ff441e315a637342ca6559423698ca" translate="yes" xml:space="preserve">
          <source>The optional &lt;code&gt;run_metadata&lt;/code&gt; argument expects a [&lt;code&gt;RunMetadata&lt;/code&gt;] proto. When appropriate, the non-Tensor output of this step will be collected there. For example, when users turn on tracing in &lt;code&gt;options&lt;/code&gt;, the profiled info will be collected into this argument and passed back.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4463c9db10422ab0ce9fc83cbdf23e0b400b34c3" translate="yes" xml:space="preserve">
          <source>The optional &lt;code&gt;sharded&lt;/code&gt; argument, if &lt;code&gt;True&lt;/code&gt;, instructs the saver to shard checkpoints per device.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9211a71d92ea323063ffa11455da47bbb168e612" translate="yes" xml:space="preserve">
          <source>The optional &lt;code&gt;signatures&lt;/code&gt; argument controls which methods in &lt;code&gt;obj&lt;/code&gt; will be available to programs which consume &lt;code&gt;SavedModel&lt;/code&gt;s, for example serving APIs. Python functions may be decorated with &lt;code&gt;@tf.function(input_signature=...)&lt;/code&gt; and passed as signatures directly, or lazily with a call to &lt;code&gt;get_concrete_function&lt;/code&gt; on the method decorated with &lt;code&gt;@tf.function&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5eb10eb93ba6242a60bc81190f01e4562cc7c7f2" translate="yes" xml:space="preserve">
          <source>The options are &quot;global&quot; in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af401239a74cc22bb0656ceab277ae88265e81dd" translate="yes" xml:space="preserve">
          <source>The original method wrapped such that it enters the module's name scope.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f891d19803fb509490762be082d2f8ade6148c86" translate="yes" xml:space="preserve">
          <source>The original registered Flag objects can be retrieved through the use of the dictionary-like operator, &lt;strong&gt;getitem&lt;/strong&gt;: x = FLAGS['longname'] # access the registered Flag object</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b71549bb8296bfb172b0e5fa059a19b9055f4136" translate="yes" xml:space="preserve">
          <source>The other method &lt;code&gt;uniform&lt;/code&gt; only covers the range [minval, maxval), which cannot be &lt;code&gt;dtype&lt;/code&gt;'s full range because &lt;code&gt;maxval&lt;/code&gt; is of type &lt;code&gt;dtype&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d650eca6a136e51fc7375e0969bc2ea1115b2e2" translate="yes" xml:space="preserve">
          <source>The output &lt;code&gt;SparseTensor&lt;/code&gt; object's shape values for all dimensions but the first are the max across the input &lt;code&gt;SparseTensor&lt;/code&gt; objects' shape values for the corresponding dimensions. Its first shape value is &lt;code&gt;N&lt;/code&gt;, the minibatch size.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27cd935b967006cc01a667a4b3f9d0444d7da4c5" translate="yes" xml:space="preserve">
          <source>The output &lt;code&gt;SparseTensor&lt;/code&gt; will be in row-major order and will have the same shape as the input.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25a24380035e136d9ea98af6a346f119051ef56c" translate="yes" xml:space="preserve">
          <source>The output Tensor as described above, dimensions will vary based on the op provided.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47c4d6134bd40a175fa44afa8f9b377d5850de3c" translate="yes" xml:space="preserve">
          <source>The output consists of two tensors LU and P containing the LU decomposition of all input submatrices &lt;code&gt;[..., :, :]&lt;/code&gt;. LU encodes the lower triangular and upper triangular factors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="048c6e1cfd308f6012ab77e90cbc674670b136c0" translate="yes" xml:space="preserve">
          <source>The output elements are taken from the input at intervals given by the &lt;code&gt;rate&lt;/code&gt; argument, as in dilated convolutions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="01cfec4a29cc69f6abe94502062c73070782ab76" translate="yes" xml:space="preserve">
          <source>The output elements will be resorted to preserve the sort order along increasing dimension number.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="38c8f8944795768b987abf44161bc29810671bc1" translate="yes" xml:space="preserve">
          <source>The output is a tensor of rank &lt;code&gt;k+1&lt;/code&gt; with dimensions &lt;code&gt;[I, J, ..., L, M, N]&lt;/code&gt;. If &lt;code&gt;k&lt;/code&gt; is scalar or &lt;code&gt;k[0] == k[1]&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b9f3b180f0596bc470d2fb79c1dedac7776f93d9" translate="yes" xml:space="preserve">
          <source>The output is a tensor of shape &lt;code&gt;[..., M, K]&lt;/code&gt;. If &lt;code&gt;adjoint&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; then the innermost matrices in &lt;code&gt;output&lt;/code&gt; satisfy matrix equations &lt;code&gt;matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]&lt;/code&gt;. If &lt;code&gt;adjoint&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; then the strictly then the innermost matrices in &lt;code&gt;output&lt;/code&gt; satisfy matrix equations &lt;code&gt;adjoint(matrix[..., i, k]) * output[..., k, j] = rhs[..., i, j]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="24258a9b3cf1537928d726a4c142766d14a612d5" translate="yes" xml:space="preserve">
          <source>The output is a tensor of the same shape as &lt;code&gt;rhs&lt;/code&gt;: either &lt;code&gt;[..., M]&lt;/code&gt; or &lt;code&gt;[..., M, K]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d3db7f6f2ef07d73508fa555433bf8d83a75eaa" translate="yes" xml:space="preserve">
          <source>The output is a tensor of the same shape as the input containing the Cholesky decompositions for all input submatrices &lt;code&gt;[..., :, :]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02cc9038a532f36829ca3735f949f57712e9a776" translate="yes" xml:space="preserve">
          <source>The output locations corresponding to the implicitly zero elements in the sparse tensor will be zero (i.e., will not take up storage space), regardless of the contents of the dense tensor (even if it's +/-INF and that INF*0 == NaN).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b30d98674beeba8558b72dc407638d1340b74552" translate="yes" xml:space="preserve">
          <source>The output of the 1-arg function that takes the &lt;code&gt;step&lt;/code&gt; is &lt;code&gt;values[0]&lt;/code&gt; when &lt;code&gt;step &amp;lt;= boundaries[0]&lt;/code&gt;, &lt;code&gt;values[1]&lt;/code&gt; when &lt;code&gt;step &amp;gt; boundaries[0]&lt;/code&gt; and &lt;code&gt;step &amp;lt;= boundaries[1]&lt;/code&gt;, ..., and values[-1] when &lt;code&gt;step &amp;gt; boundaries[-1]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d04584f160db196293c2030ae17c2dacfbfcd30d" translate="yes" xml:space="preserve">
          <source>The output of this Op is a single bounding box that may be used to crop the original image. The output is returned as 3 tensors: &lt;code&gt;begin&lt;/code&gt;, &lt;code&gt;size&lt;/code&gt; and &lt;code&gt;bboxes&lt;/code&gt;. The first 2 tensors can be fed directly into &lt;a href=&quot;../../../slice&quot;&gt;&lt;code&gt;tf.slice&lt;/code&gt;&lt;/a&gt; to crop the image. The latter may be supplied to &lt;a href=&quot;../../../image/draw_bounding_boxes&quot;&gt;&lt;code&gt;tf.image.draw_bounding_boxes&lt;/code&gt;&lt;/a&gt; to visualize what the bounding box looks like.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e995a951ce7a3cd16e04e8cd70419e327350a30" translate="yes" xml:space="preserve">
          <source>The output of this Op is a single bounding box that may be used to crop the original image. The output is returned as 3 tensors: &lt;code&gt;begin&lt;/code&gt;, &lt;code&gt;size&lt;/code&gt; and &lt;code&gt;bboxes&lt;/code&gt;. The first 2 tensors can be fed directly into &lt;a href=&quot;../slice&quot;&gt;&lt;code&gt;tf.slice&lt;/code&gt;&lt;/a&gt; to crop the image. The latter may be supplied to &lt;a href=&quot;draw_bounding_boxes&quot;&gt;&lt;code&gt;tf.image.draw_bounding_boxes&lt;/code&gt;&lt;/a&gt; to visualize what the bounding box looks like.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="867a41c15dc52766b0de85d04ee44344a9063866" translate="yes" xml:space="preserve">
          <source>The output of this method is a 3D &lt;code&gt;Tensor&lt;/code&gt; of shape &lt;code&gt;[batch_size, T, D]&lt;/code&gt;. &lt;code&gt;T&lt;/code&gt; is the maximum sequence length for this batch, which could differ from batch to batch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eac10a40d7ace5a75143c2fbccbd028ece77a9fb" translate="yes" xml:space="preserve">
          <source>The output slice &lt;code&gt;i&lt;/code&gt; along dimension &lt;code&gt;batch_axis&lt;/code&gt; is then given by input slice &lt;code&gt;i&lt;/code&gt;, with the first &lt;code&gt;seq_lengths[i]&lt;/code&gt; slices along dimension &lt;code&gt;seq_axis&lt;/code&gt; reversed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d0270145e0963e3fa6b4466fddef88fdbf60e481" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[1, 2, 2, 1]&lt;/code&gt; and value:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aff644b1606cf977042c3f329c0b44912b343d5f" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[1, 2, 2, 3]&lt;/code&gt; and value:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26ecc2d485656620c8ddae36c3377b136f2c8ef6" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[1, 4, 4, 1]&lt;/code&gt; and value:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="413cce56d7e73e0557f6955da012eeb198f78207" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[2, 2, 4, 1]&lt;/code&gt; and value:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="580524501d7754a464d140a489afd8a69565b7e8" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[4, 1, 1, 1]&lt;/code&gt; and value:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="43b93cea1c5beede2692779450404b22ddd16414" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[4, 1, 1, 3]&lt;/code&gt; and value:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="863590a97a6747705662d02c1d09d524c465ba85" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[4, 2, 2, 1]&lt;/code&gt; and value:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="951a2f1acb5ea54cd01c5c69fad552b3af0398ec" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[8, 1, 2, 1]&lt;/code&gt; and value:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b991f9c184601aa1cf79a030dce35cd2e1811686" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[8, 1, 3, 1]&lt;/code&gt; and value:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="018a92a28c57571fe5c1edcfb924689c52abc4b8" translate="yes" xml:space="preserve">
          <source>The output tensor, of rank 3.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f156d1cf6b13fd91d5c1751333de4bfa7481e5ec" translate="yes" xml:space="preserve">
          <source>The output tensors for the loop variables after the loop. If &lt;code&gt;return_same_structure&lt;/code&gt; is True, the return value has the same structure as &lt;code&gt;loop_vars&lt;/code&gt;. If &lt;code&gt;return_same_structure&lt;/code&gt; is False, the return value is a Tensor, TensorArray or IndexedSlice if the length of &lt;code&gt;loop_vars&lt;/code&gt; is 1, or a list otherwise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0bda5fbd6b0783b500c1218ac7c4c0e7746a4905" translate="yes" xml:space="preserve">
          <source>The output tensors for the loop variables after the loop. The return value has the same structure as &lt;code&gt;loop_vars&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1bcada90c4c3eacc21413e8c1e200a0e90b07ffc" translate="yes" xml:space="preserve">
          <source>The output will then have shape &lt;code&gt;(32, 10, 32)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d23cb786d0f9d1271884eaa7c1f7cc8c8a656c8" translate="yes" xml:space="preserve">
          <source>The output will then have shape &lt;code&gt;(32, 10, 8)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="262b53d310948dd3d4d6f3d1b24c8027dcc78d4c" translate="yes" xml:space="preserve">
          <source>The outputs from all shards are concatenated back together along their 0-th dimension.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fec5d02e1747f7a22db33ab6777959dc638f2020" translate="yes" xml:space="preserve">
          <source>The outputs of functions used as &lt;code&gt;signatures&lt;/code&gt; must either be flat lists, in which case outputs will be numbered, or a dictionary mapping string keys to &lt;code&gt;Tensor&lt;/code&gt;, in which case the keys will be used to name outputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4fe37226e907ad6b84032ccb7a1a60c428d9656" translate="yes" xml:space="preserve">
          <source>The padded size of each dimension D of the output is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d91e185b3ec2b95e4e28b058fb7de6ed54c4f63d" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;concentration&lt;/code&gt; and &lt;code&gt;rate&lt;/code&gt; must be shaped in a way that supports broadcasting (e.g. &lt;code&gt;concentration + rate&lt;/code&gt; is a valid operation).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4bf0f9162b85380657aa85a48762d0989942eac" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;df&lt;/code&gt;, &lt;code&gt;loc&lt;/code&gt;, and &lt;code&gt;scale&lt;/code&gt; must be shaped in a way that supports broadcasting (e.g. &lt;code&gt;df + loc + scale&lt;/code&gt; is a valid operation).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f624db393a0283cdff3712a3c4bd723d749a30a5" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;loc&lt;/code&gt; and &lt;code&gt;scale&lt;/code&gt; must be shaped in a way that supports broadcasting (e.g. &lt;code&gt;loc + scale&lt;/code&gt; is a valid operation).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2bb7ef60bf0239928d550799fd1318ab8a8e22c" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;loc&lt;/code&gt; and &lt;code&gt;scale&lt;/code&gt; must be shaped in a way that supports broadcasting (e.g., &lt;code&gt;loc / scale&lt;/code&gt; is a valid operation).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="caea2d657314fc74a3b4c82a6b1a8e2927d4e265" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;low&lt;/code&gt; and &lt;code&gt;high&lt;/code&gt; must be shaped in a way that supports broadcasting (e.g., &lt;code&gt;high - low&lt;/code&gt; is a valid operation).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f861e09ace62d9cf6a436ce61f90b0514dff3612" translate="yes" xml:space="preserve">
          <source>The parameters can be intuited via their relationship to mean and stddev,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78832ff2cd51fc67f3284e123b0a8c7a1096d66b" translate="yes" xml:space="preserve">
          <source>The parent could be a module when the child is a function at module scope. Or the parent could be a class when a class' method is being replaced. The named child is set to new_child, while the prior definition is saved away for later, when UnsetAll() is called.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc13d476090ccb6ba884188ddcc28a81f62b0726" translate="yes" xml:space="preserve">
          <source>The parse() method checks to make sure that the string argument is a legal value and convert it to a native type. If the value cannot be converted, it should throw a 'ValueError' exception with a human readable explanation of why the value is illegal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf0794be97523b0be42896e118da1be5a61ff4da" translate="yes" xml:space="preserve">
          <source>The parsed value in native type.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8cff8c184992ecf7d79d6a190fc8e2ad81b8dd10" translate="yes" xml:space="preserve">
          <source>The partitioned embedding in &lt;code&gt;embedding_weights&lt;/code&gt; must all be the same shape except for the first dimension. The first dimension is allowed to vary as the vocabulary size is not necessarily a multiple of &lt;code&gt;P&lt;/code&gt;. &lt;code&gt;embedding_weights&lt;/code&gt; may be a &lt;code&gt;PartitionedVariable&lt;/code&gt; as returned by using &lt;a href=&quot;../compat/v1/get_variable&quot;&gt;&lt;code&gt;tf.compat.v1.get_variable()&lt;/code&gt;&lt;/a&gt; with a partitioner.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e157288a667a186f035deba918682c801fa1abf" translate="yes" xml:space="preserve">
          <source>The partitioned embedding in &lt;code&gt;embedding_weights&lt;/code&gt; must all be the same shape except for the first dimension. The first dimension is allowed to vary as the vocabulary size is not necessarily a multiple of &lt;code&gt;P&lt;/code&gt;. &lt;code&gt;embedding_weights&lt;/code&gt; may be a &lt;code&gt;PartitionedVariable&lt;/code&gt; as returned by using &lt;a href=&quot;../get_variable&quot;&gt;&lt;code&gt;tf.compat.v1.get_variable()&lt;/code&gt;&lt;/a&gt; with a partitioner.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b3b7bcbb761db59d963bb51839f46a4b29f4af4e" translate="yes" xml:space="preserve">
          <source>The path is relative to tensorflow/</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f48baa97e568a9b9e3b5f52a6ab7e866fecd8550" translate="yes" xml:space="preserve">
          <source>The path of the output proto file.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c6d384388d19ed3fa790b1769ea860a0e6be8d0" translate="yes" xml:space="preserve">
          <source>The path to the new checkpoint. It is also recorded in the &lt;code&gt;checkpoints&lt;/code&gt; and &lt;code&gt;latest_checkpoint&lt;/code&gt; properties.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ee0e0b9b831848f3b4065f81ba73b2d7593c0f8" translate="yes" xml:space="preserve">
          <source>The path to the specified file present in the data attribute of py_test or py_binary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c28b39685fa19f5fe1fe963ce50d396e16dbe87f" translate="yes" xml:space="preserve">
          <source>The path to the specified file present in the data attribute of py_test or py_binary. Falls back to returning the same as get_data_files_path if it fails to detect a bazel runfiles directory.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="87756ef48ceb2a46a88324fa1e20a23d5ed02a12" translate="yes" xml:space="preserve">
          <source>The path to which the SavedModel protocol buffer was written.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7281e72ad08903abeb2bd587c7c0dfd0d8772e6" translate="yes" xml:space="preserve">
          <source>The pattern follows the re2 syntax (https://github.com/google/re2/wiki/Syntax)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c52ca33f774b534fe80a4db87092be76893e002" translate="yes" xml:space="preserve">
          <source>The peephole implementation is based on:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="baf8710cc0a85723dd8aa2526d303f0863e63136" translate="yes" xml:space="preserve">
          <source>The performance of &lt;code&gt;LinearOperatorAdjoint&lt;/code&gt; depends on the underlying operators performance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80d488d70fd1fbe1594d7c6c118ae3795bd1145d" translate="yes" xml:space="preserve">
          <source>The performance of &lt;code&gt;LinearOperatorBlockDiag&lt;/code&gt; on any operation is equal to the sum of the individual operators' operations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b2ab24a37ac3d66ce6ff090c15ce4c8f5b1d480" translate="yes" xml:space="preserve">
          <source>The performance of &lt;code&gt;LinearOperatorComposition&lt;/code&gt; on any operation is equal to the sum of the individual operators' operations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f94bad31467a9ea9f4b4d537674d8e77b0cf1a22" translate="yes" xml:space="preserve">
          <source>The performance of &lt;code&gt;LinearOperatorInversion&lt;/code&gt; depends on the underlying operators performance: &lt;code&gt;solve&lt;/code&gt; and &lt;code&gt;matmul&lt;/code&gt; are swapped, and determinant is inverted.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bc0efd32d22cd6d37162492744eb9c09073101ec" translate="yes" xml:space="preserve">
          <source>The performance of &lt;code&gt;LinearOperatorKronecker&lt;/code&gt; on any operation is equal to the sum of the individual operators' operations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3adcc83502b5bf35b4db3b88505bc6feb67f24c9" translate="yes" xml:space="preserve">
          <source>The polygamma function is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="692359d83812b544d928c04fadf8b5c3c2df67a0" translate="yes" xml:space="preserve">
          <source>The possible values are: &lt;code&gt;GATE_NONE&lt;/code&gt;, &lt;code&gt;GATE_OP&lt;/code&gt;, and &lt;code&gt;GATE_GRAPH&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d5d3e18deacf1c7ef93bee6d5b865603caff1a2a" translate="yes" xml:space="preserve">
          <source>The potentially support list contains a list of ops that are partially or fully supported, which is derived by simply scanning op names to check whether they can be handled without real conversion and specific parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aaf184bbd1e736ec9f0efcfa8ff77fd74b286f9f" translate="yes" xml:space="preserve">
          <source>The prefix of the most recent checkpoint in &lt;code&gt;directory&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0cdbe0557abf05d8c31ba3846734c76d96d3bcc3" translate="yes" xml:space="preserve">
          <source>The primary usecase for this API is to put tensors in a set/dictionary. We can't put tensors in a set/dictionary as &lt;code&gt;tensor.__hash__()&lt;/code&gt; is no longer available starting Tensorflow 2.0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14c5ca1e155e62f217839df947a76604aa7befe7" translate="yes" xml:space="preserve">
          <source>The primary usecase for this API is to put variables in a set/dictionary. We can't put variables in a set/dictionary as &lt;code&gt;variable.__hash__()&lt;/code&gt; is no longer available starting Tensorflow 2.0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bfffea38233a7f94cfc568cf11eefdfb38392417" translate="yes" xml:space="preserve">
          <source>The probability density function (pdf) is,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d639692ffbb24f5fe23fd0cf8f29f9ea56836d74" translate="yes" xml:space="preserve">
          <source>The probability density function (pdf) of this distribution is,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2092fccabcd3a2abdef73412d545365899001b8" translate="yes" xml:space="preserve">
          <source>The probability mass function (pmf) is,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b1597763a35d57db9c807276fa76fead0e6bf3d7" translate="yes" xml:space="preserve">
          <source>The processing of each sample contains the following steps: 1) standardize each sample (usually lowercasing + punctuation stripping) 2) split each sample into substrings (usually words) 3) recombine substrings into tokens (usually ngrams) 4) index tokens (associate a unique int value with each token) 5) transform each sample using this index, either into a vector of ints or a dense float vector.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e9c7a33590d968d1c4d2adb1f14e30ca9d6e013" translate="yes" xml:space="preserve">
          <source>The provided generator can be finite in which case the class will throw a &lt;code&gt;StopIteration&lt;/code&gt; exception.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8129f9559f62303b98c598e584d99bb1465d425d" translate="yes" xml:space="preserve">
          <source>The provided value can be a python boolean, a scalar boolean Tensor, or or a callable providing such a value; if a callable is passed it will be invoked on-demand to determine whether summary writing will occur.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d84a7589708b247ba11ef0a6241d0276fd1c0415" translate="yes" xml:space="preserve">
          <source>The pseudo-inverse of a matrix &lt;code&gt;A&lt;/code&gt;, is defined as: 'the matrix that 'solves' [the least-squares problem] &lt;code&gt;A @ x = b&lt;/code&gt;,' i.e., if &lt;code&gt;x_hat&lt;/code&gt; is a solution, then &lt;code&gt;A_pinv&lt;/code&gt; is the matrix such that &lt;code&gt;x_hat = A_pinv @ b&lt;/code&gt;. It can be shown that if &lt;code&gt;U @ Sigma @ V.T = A&lt;/code&gt; is the singular value decomposition of &lt;code&gt;A&lt;/code&gt;, then &lt;code&gt;A_pinv = V @ inv(Sigma) U^T&lt;/code&gt;. [(Strang, 1980)][1]</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b22641be848196f7d9c02ba6fc3610602324007" translate="yes" xml:space="preserve">
          <source>The purpose of this function is to allow users of existing layers to slowly transition to Keras layers API without breaking existing functionality.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d088d5e5d63d8b56ec540e3afc689671a1995e6f" translate="yes" xml:space="preserve">
          <source>The purpose of this scope is to allow users of existing layers to slowly transition to a Keras layers API without breaking existing functionality.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c5cb70c5a3b792465bd5ce774c8ea2af3bb85e3" translate="yes" xml:space="preserve">
          <source>The python function &lt;code&gt;fn&lt;/code&gt; will be called once with symbolic arguments specified in the &lt;code&gt;signature&lt;/code&gt;, traced, and turned into a graph function. Any variables created by &lt;code&gt;fn&lt;/code&gt; will be owned by the object returned by &lt;code&gt;wrap_function&lt;/code&gt;. The resulting graph function can be called with tensors which match the signature.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b20ff491ff5822812275688511b761f22476aa65" translate="yes" xml:space="preserve">
          <source>The range of pixel values for the output image might be slightly different from the range for the input image because of limited numerical precision. To guarantee an output range, for example &lt;code&gt;[0.0, 1.0]&lt;/code&gt;, apply &lt;a href=&quot;../../../clip_by_value&quot;&gt;&lt;code&gt;tf.clip_by_value&lt;/code&gt;&lt;/a&gt; to the output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="afabff9ee6287914682ddea57965fb5330ae1825" translate="yes" xml:space="preserve">
          <source>The reason we get 'A2' instead 'A1' on the second call of &lt;a href=&quot;uniform&quot;&gt;&lt;code&gt;tf.random.uniform&lt;/code&gt;&lt;/a&gt; above is because the same &lt;a href=&quot;uniform&quot;&gt;&lt;code&gt;tf.random.uniform&lt;/code&gt;&lt;/a&gt; kernel (i.e. internel representation) is used by TensorFlow for all calls of it with the same arguments, and the kernel maintains an internal counter which is incremented every time it is executed, generating different results.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="466c31e39da78ea430b94cdcadf9fd7405e0f0cf" translate="yes" xml:space="preserve">
          <source>The reason we get 'A2' instead 'A1' on the second call of &lt;a href=&quot;uniform&quot;&gt;&lt;code&gt;tf.random.uniform&lt;/code&gt;&lt;/a&gt; above is because the secand call uses a different operation seed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1280cfc6968446075563e60ad83288d637db87ba" translate="yes" xml:space="preserve">
          <source>The reconstruct one or more matrices from their LU decomposition(s).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="39397157700bab61929a7d8ce8314e15144efa4b" translate="yes" xml:space="preserve">
          <source>The reduced SparseTensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="162a81b2ee639009df98d0138880fb964fcbc8fc" translate="yes" xml:space="preserve">
          <source>The reduced Tensor or the reduced SparseTensor if &lt;code&gt;output_is_sparse&lt;/code&gt; is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e34999a75a17ec65b85f93cb20fe03874e9d9e8" translate="yes" xml:space="preserve">
          <source>The reduced Tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa52820210f038254236a33cd72deaaa4f2921cc" translate="yes" xml:space="preserve">
          <source>The reduced tensor (number of nonzero values).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5089f15002ac696c728750d278b7a2bb4b8292fd" translate="yes" xml:space="preserve">
          <source>The reduced tensor, of the same dtype as the input_tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="377005a84a788fcaae09139342bfe31ee6c3e407" translate="yes" xml:space="preserve">
          <source>The reduced tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32126f130f539d8b348c3776e0d288e0eb3023ae" translate="yes" xml:space="preserve">
          <source>The reference to the TensorArray.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="626421317b8daa7c4481578d7b1400cbc817027c" translate="yes" xml:space="preserve">
          <source>The regularized incomplete beta integral is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08f7982eae76c3d1b8d5c546c5c6087586c14c26" translate="yes" xml:space="preserve">
          <source>The request does not have valid authentication credentials.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11fac311a887cb1b94b956ea27985cb8f91c9eab" translate="yes" xml:space="preserve">
          <source>The requirements to use the cuDNN implementation are:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d06ff612c69fb3df1f3d794bb785e13f8da40620" translate="yes" xml:space="preserve">
          <source>The result is a 4-D tensor of shape &lt;code&gt;[batch_size, glimpse_height, glimpse_width, channels]&lt;/code&gt;. The channels and batch dimensions are the same as that of the input tensor. The height and width of the output windows are specified in the &lt;code&gt;size&lt;/code&gt; parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af8e15d374bd08a8ea2bebed96d25f27af99cc58" translate="yes" xml:space="preserve">
          <source>The result is a 4D tensor which is indexed by batch, row, and column. &lt;code&gt;output[i, x, y]&lt;/code&gt; contains a flattened patch of size &lt;code&gt;sizes[1], sizes[2]&lt;/code&gt; which is taken from the input starting at &lt;code&gt;images[i, x*strides[1], y*strides[2]]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc0fc847805d05865dd1fc80479599ef10e030da" translate="yes" xml:space="preserve">
          <source>The result of calling parse_example on these examples will produce a dictionary with entries for &quot;ids&quot; and &quot;values&quot;. Passing those two objects to this function along with vocab_size=6, will produce a &lt;code&gt;SparseTensor&lt;/code&gt; that sparsely represents all three instances. Namely, the &lt;code&gt;indices&lt;/code&gt; property will contain the coordinates of the non-zero entries in the feature matrix (the first dimension is the row number in the matrix, i.e., the index within the batch, and the second dimension is the column number, i.e., the feature id); &lt;code&gt;values&lt;/code&gt; will contain the actual values. &lt;code&gt;shape&lt;/code&gt; will be the shape of the original matrix, i.e., (3, 6). For our example above, the output will be equal to:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7181bf7e73106c9b2b97d9fffba14ef65fbf1f8" translate="yes" xml:space="preserve">
          <source>The result of this op should be passed through a &lt;code&gt;sparse_to_dense&lt;/code&gt; operation, then added to the logits of the sampled classes. This removes the contradictory effect of accidentally sampling the true target classes as noise classes for the same example.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="01298564c21aafdba5ad8531a719b0b62528e9fe" translate="yes" xml:space="preserve">
          <source>The result will have those bits set, that are different in &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. The computation is performed on the underlying representations of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="685b62e5962fffb0313c314fe8766a83e91dc7ce" translate="yes" xml:space="preserve">
          <source>The result will have those bits set, that are set in &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; or both. The computation is performed on the underlying representations of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b74f37adc87a210e3154c60f498551da0c0a3084" translate="yes" xml:space="preserve">
          <source>The result will have those bits set, that are set in both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. The computation is performed on the underlying representations of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a454b218545175a81109283a1b62827f2751cc1d" translate="yes" xml:space="preserve">
          <source>The resulting &lt;code&gt;Tensor&lt;/code&gt; of parsing a single &lt;code&gt;SequenceExample&lt;/code&gt; or &lt;code&gt;Example&lt;/code&gt; has a static &lt;code&gt;shape&lt;/code&gt; of &lt;code&gt;[None] + shape&lt;/code&gt; and the specified &lt;code&gt;dtype&lt;/code&gt;. The resulting &lt;code&gt;Tensor&lt;/code&gt; of parsing a &lt;code&gt;batch_size&lt;/code&gt; many &lt;code&gt;Example&lt;/code&gt;s has a static &lt;code&gt;shape&lt;/code&gt; of &lt;code&gt;[batch_size, None] + shape&lt;/code&gt; and the specified &lt;code&gt;dtype&lt;/code&gt;. The entries in the &lt;code&gt;batch&lt;/code&gt; from different &lt;code&gt;Examples&lt;/code&gt; will be padded with &lt;code&gt;default_value&lt;/code&gt; to the maximum length present in the &lt;code&gt;batch&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a538e992c4e715c1f91b7c40110e2510e29c989" translate="yes" xml:space="preserve">
          <source>The resulting SavedModel is then servable with an input named &quot;x&quot;, its value having any shape and dtype float32.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ef25e64d2e423f7de1f3a02ba7a25085c38c8a1" translate="yes" xml:space="preserve">
          <source>The resulting tensor is populated with values of type &lt;code&gt;dtype&lt;/code&gt;, as specified by arguments &lt;code&gt;value&lt;/code&gt; and (optionally) &lt;code&gt;shape&lt;/code&gt; (see examples below).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b674f3072695f2111704701dbc3947bfb383f3f" translate="yes" xml:space="preserve">
          <source>The resulting tensor is populated with values of type &lt;code&gt;dtype&lt;/code&gt;, as specified by arguments &lt;code&gt;value&lt;/code&gt; following the desired &lt;code&gt;shape&lt;/code&gt; of the new tensor (see examples below).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef74e8f16df07a307babda0cfa120db0e980020d" translate="yes" xml:space="preserve">
          <source>The resulting tensor would look like this:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="381b1e6543678db4ed459c1cba156ecc8ca01172" translate="yes" xml:space="preserve">
          <source>The resulting update to ref would look like this:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cbbd169c06c5469910e771887e06b5ed0c01b738" translate="yes" xml:space="preserve">
          <source>The resulting update to v would look like this:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5cc6293ac412a772df6f0e78ca55482c98eb64a2" translate="yes" xml:space="preserve">
          <source>The results of the lookup are concatenated into a dense tensor. The returned tensor has shape &lt;code&gt;shape(ids) + shape(params)[1:]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f1f40e1bca978cc7aa38e548317ee19cb7dfd8c" translate="yes" xml:space="preserve">
          <source>The return value has the same type as &lt;code&gt;images&lt;/code&gt; if &lt;code&gt;method&lt;/code&gt; is &lt;a href=&quot;../../../image/resizemethod#NEAREST_NEIGHBOR&quot;&gt;&lt;code&gt;ResizeMethod.NEAREST_NEIGHBOR&lt;/code&gt;&lt;/a&gt;. It will also have the same type as &lt;code&gt;images&lt;/code&gt; if the size of &lt;code&gt;images&lt;/code&gt; can be statically determined to be the same as &lt;code&gt;size&lt;/code&gt;, because &lt;code&gt;images&lt;/code&gt; is returned in this case. Otherwise, the return value has type &lt;code&gt;float32&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="90335752084dfbdd6479bc3425e07c2c184fac03" translate="yes" xml:space="preserve">
          <source>The return value has the same type as &lt;code&gt;images&lt;/code&gt; if &lt;code&gt;method&lt;/code&gt; is &lt;a href=&quot;resizemethod#NEAREST_NEIGHBOR&quot;&gt;&lt;code&gt;ResizeMethod.NEAREST_NEIGHBOR&lt;/code&gt;&lt;/a&gt;. Otherwise, the return value has type &lt;code&gt;float32&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e3b67eed807726677126c856d0a6214fa8f8e4a" translate="yes" xml:space="preserve">
          <source>The return value of &lt;code&gt;merge_fn&lt;/code&gt;, except for &lt;code&gt;PerReplica&lt;/code&gt; values which are unpacked.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="111bba19cc7fb0062ed6fe66a53428292ab2ad62" translate="yes" xml:space="preserve">
          <source>The returned &lt;code&gt;RaggedTensor&lt;/code&gt; corresponds with the python list defined by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ecbcae5eb984661e7de2445f57c60c5e40e3d894" translate="yes" xml:space="preserve">
          <source>The returned &lt;code&gt;Session&lt;/code&gt; will be the innermost session on which a &lt;code&gt;Session&lt;/code&gt; or &lt;code&gt;Session.as_default()&lt;/code&gt; context has been entered.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="95a65357f0f57659490bed4f233c4a4d8fc08588" translate="yes" xml:space="preserve">
          <source>The returned &lt;code&gt;Tensor&lt;/code&gt; will be close to an exact solution if &lt;code&gt;A&lt;/code&gt; is well conditioned. Otherwise closeness will vary. See class docstring for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="210a2253a38d62adf49c231834e4374a228751a1" translate="yes" xml:space="preserve">
          <source>The returned callable will have the same return type as &lt;code&gt;tf.Session.run(fetches, ...)&lt;/code&gt;. For example, if &lt;code&gt;fetches&lt;/code&gt; is a &lt;a href=&quot;../../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt;, the callable will return a numpy ndarray; if &lt;code&gt;fetches&lt;/code&gt; is a &lt;a href=&quot;../../operation&quot;&gt;&lt;code&gt;tf.Operation&lt;/code&gt;&lt;/a&gt;, it will return &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bfdda7d94aac832546a913098111bedb3a5af104" translate="yes" xml:space="preserve">
          <source>The returned callable will take &lt;code&gt;len(feed_list)&lt;/code&gt; arguments whose types must be compatible feed values for the respective elements of &lt;code&gt;feed_list&lt;/code&gt;. For example, if element &lt;code&gt;i&lt;/code&gt; of &lt;code&gt;feed_list&lt;/code&gt; is a &lt;a href=&quot;../../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;i&lt;/code&gt;th argument to the returned callable must be a numpy ndarray (or something convertible to an ndarray) with matching element type and shape. See &lt;code&gt;tf.Session.run&lt;/code&gt; for details of the allowable feed key and value types.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5ae9baaba07ae2a863bb456cb56b210b15bfeca4" translate="yes" xml:space="preserve">
          <source>The returned dataset is a wrapped strategy dataset which creates a multidevice iterator under the hood. It prefetches the input data to the specified devices on the worker. The returned distributed dataset can be iterated over similar to how regular datasets can.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f7e2363fe125c1a5cdb798835abe8de5ac8aef0" translate="yes" xml:space="preserve">
          <source>The returned dictionary can be used as arg 'features' in &lt;a href=&quot;../../../io/parse_example&quot;&gt;&lt;code&gt;tf.io.parse_example&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27148d9610fe5857a584045bc355cd08d4ce68b2" translate="yes" xml:space="preserve">
          <source>The returned dictionary can be used as arg 'features' in &lt;a href=&quot;../io/parse_example&quot;&gt;&lt;code&gt;tf.io.parse_example&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e41cdf1dd9ec5b48c6c1a627e04bc9cd0f5646ec" translate="yes" xml:space="preserve">
          <source>The returned distributed dataset can be iterated over similar to how regular datasets can. NOTE: Currently, the user cannot add any more transformations to a distributed dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e5c9b1714c669dc39fd802e652891ab6982a58a" translate="yes" xml:space="preserve">
          <source>The returned graph will be the innermost graph on which a &lt;a href=&quot;../../graph#as_default&quot;&gt;&lt;code&gt;Graph.as_default()&lt;/code&gt;&lt;/a&gt; context has been entered, or a global default graph if none has been explicitly created.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b38dad137146b9ef38ba7c509e2c0442b75c1f0" translate="yes" xml:space="preserve">
          <source>The returned iterator implements the Python iterator protocol and therefore can only be used in eager mode.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e0593141fa29e4d333dddf121d2181c125317c5" translate="yes" xml:space="preserve">
          <source>The returned iterator is not bound to a particular dataset, and it has no &lt;code&gt;initializer&lt;/code&gt;. To initialize the iterator, run the operation returned by &lt;code&gt;Iterator.make_initializer(dataset)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35c004b6fa1c65f47bcd7047f0f10a1978a605f3" translate="yes" xml:space="preserve">
          <source>The returned operation is a dequeue operation and will throw &lt;a href=&quot;../../../errors/outofrangeerror&quot;&gt;&lt;code&gt;tf.errors.OutOfRangeError&lt;/code&gt;&lt;/a&gt; if the input queue is exhausted. If this operation is feeding another input queue, its queue runner will catch this exception, however, if this operation is used in your main thread you are responsible for catching this yourself.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="283d98979ab1e70cde916469a5e89b9e786cbb6c" translate="yes" xml:space="preserve">
          <source>The returned status object has the following methods:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="76a9be24217f14af8b4e90ac374a00f3e69a4114" translate="yes" xml:space="preserve">
          <source>The returned tensor will contain a serialized &lt;a href=&quot;../../summary&quot;&gt;&lt;code&gt;tf.compat.v1.summary.Summary&lt;/code&gt;&lt;/a&gt; protocol buffer, which can be used with the standard TensorBoard logging facilities.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="82d1c361e6f8f0d84bf0a1bcea9214b49e8cf3d3" translate="yes" xml:space="preserve">
          <source>The returned tensor's dimension i will correspond to the input dimension &lt;code&gt;perm[i]&lt;/code&gt;. If &lt;code&gt;perm&lt;/code&gt; is not given, it is set to (n-1...0), where n is the rank of the input tensor. Hence by default, this operation performs a regular matrix transpose on 2-D input Tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9c115cc2d00147127224aefb2bd6a8e1bbe0c5c" translate="yes" xml:space="preserve">
          <source>The returned tensor's dimension i will correspond to the input dimension &lt;code&gt;perm[i]&lt;/code&gt;. If &lt;code&gt;perm&lt;/code&gt; is not given, it is set to (n-1...0), where n is the rank of the input tensor. Hence by default, this operation performs a regular matrix transpose on 2-D input Tensors. If conjugate is True and &lt;code&gt;a.dtype&lt;/code&gt; is either &lt;code&gt;complex64&lt;/code&gt; or &lt;code&gt;complex128&lt;/code&gt; then the values of &lt;code&gt;a&lt;/code&gt; are conjugated and transposed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="838e6e70eeb12ce61fde540b38b4044a46a8f134" translate="yes" xml:space="preserve">
          <source>The returned tensors are &lt;a href=&quot;../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt;s if &lt;code&gt;input&lt;/code&gt; is a scalar, or &lt;a href=&quot;../raggedtensor&quot;&gt;&lt;code&gt;tf.RaggedTensor&lt;/code&gt;&lt;/a&gt;s otherwise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85216d0bd898c6d1680f5aac974d5348d1d96c2f" translate="yes" xml:space="preserve">
          <source>The row-split indices for this ragged tensor's &lt;code&gt;values&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5854e78835764512ec754b4ac011bb9b48f7d380" translate="yes" xml:space="preserve">
          <source>The row_splits for all ragged dimensions in this ragged tensor value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5158c81c5ea5bb74de68daed2eacbf5dc32d4ad8" translate="yes" xml:space="preserve">
          <source>The runtime is then free to make optimizations based on this.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca5cdf682a9605ca998129dd4a854da399041aba" translate="yes" xml:space="preserve">
          <source>The same array (Numpy array if &lt;code&gt;x&lt;/code&gt; was a Numpy array, or TensorFlow tensor if &lt;code&gt;x&lt;/code&gt; was a tensor), cast to its new type.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4b3e7c37b806fc9d4b600343aa28d9179c53706" translate="yes" xml:space="preserve">
          <source>The same as &lt;a href=&quot;../../raggedtensor#__div__&quot;&gt;&lt;code&gt;tf.compat.v1.div(x,y)&lt;/code&gt;&lt;/a&gt; for integers, but uses &lt;code&gt;tf.floor(tf.compat.v1.div(x,y))&lt;/code&gt; for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by &lt;code&gt;x // y&lt;/code&gt; floor division in Python 3 and in Python 2.7 with &lt;code&gt;from __future__ import division&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="07525f9495ce7a62b86179e9f6083e34f20d8d8b" translate="yes" xml:space="preserve">
          <source>The same as &lt;a href=&quot;../raggedtensor#__div__&quot;&gt;&lt;code&gt;tf.compat.v1.div(x,y)&lt;/code&gt;&lt;/a&gt; for integers, but uses &lt;code&gt;tf.floor(tf.compat.v1.div(x,y))&lt;/code&gt; for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by &lt;code&gt;x // y&lt;/code&gt; floor division in Python 3 and in Python 2.7 with &lt;code&gt;from __future__ import division&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3240798db52d492b4a5e0a4f1101b0ce22e5dea6" translate="yes" xml:space="preserve">
          <source>The same as &lt;a href=&quot;raggedtensor#__div__&quot;&gt;&lt;code&gt;tf.compat.v1.div(x,y)&lt;/code&gt;&lt;/a&gt; for integers, but uses &lt;code&gt;tf.floor(tf.compat.v1.div(x,y))&lt;/code&gt; for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by &lt;code&gt;x // y&lt;/code&gt; floor division in Python 3 and in Python 2.7 with &lt;code&gt;from __future__ import division&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d6016fab50d93b1e77917677d258e0e003f237e6" translate="yes" xml:space="preserve">
          <source>The same tensor &lt;code&gt;x&lt;/code&gt;, unchanged.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c3935638fc889928b31f22a28b40af71a593afd" translate="yes" xml:space="preserve">
          <source>The samples are differentiable w.r.t. alpha and beta. The derivatives are computed using the approach described in the paper</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92952659dac1bf101ab8c26a5fd0e7570c899948" translate="yes" xml:space="preserve">
          <source>The sampling probabilities are generated according to the sampling distribution used in word2vec:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e70c09e21b733e9d2a808af0f600268962663fb" translate="yes" xml:space="preserve">
          <source>The save counter variable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab43a8a642f72175fe828def8c8d03e9a692fe7b" translate="yes" xml:space="preserve">
          <source>The saved checkpoint includes variables created by this object and any trackable objects it depends on at the time &lt;a href=&quot;../../../train/checkpoint#save&quot;&gt;&lt;code&gt;Checkpoint.save()&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21ba0881308b74c027819f31603cfc6647b52c80" translate="yes" xml:space="preserve">
          <source>The saved checkpoint includes variables created by this object and any trackable objects it depends on at the time &lt;a href=&quot;checkpoint#save&quot;&gt;&lt;code&gt;Checkpoint.save()&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3b7070f1ecc16e817793f6b5d01b2b53b58703e" translate="yes" xml:space="preserve">
          <source>The saved model contains: - the model's configuration (topology) - the model's weights - the model's optimizer's state (if any)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eeb866c0e9b8a790b2b06bb4bf72fe83fcf3b7fd" translate="yes" xml:space="preserve">
          <source>The savefile includes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff3fd5f27f08bbd84c446309ecdaecd1825d0b40" translate="yes" xml:space="preserve">
          <source>The scalar PSNR between a and b. The returned tensor has type &lt;a href=&quot;../../tf#float32&quot;&gt;&lt;code&gt;tf.float32&lt;/code&gt;&lt;/a&gt; and shape [batch_size, 1].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd78e4034ea5bbefe47a5585f3fa965bc5acaa8e" translate="yes" xml:space="preserve">
          <source>The scaled exponential unit activation: &lt;code&gt;scale * elu(x, alpha)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7cdf9b27ca1dd2bbd540e30d82d6f1126674d88" translate="yes" xml:space="preserve">
          <source>The scaling_factor is determined from &lt;code&gt;min_range&lt;/code&gt;, &lt;code&gt;max_range&lt;/code&gt;, and &lt;code&gt;narrow_range&lt;/code&gt; in a way that is compatible with &lt;code&gt;QuantizeAndDequantize{V2|V3}&lt;/code&gt; and &lt;code&gt;QuantizeV2&lt;/code&gt;, using the following algorithm:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d82c0e384909c0dd188de5ead9b2872459677ae8" translate="yes" xml:space="preserve">
          <source>The schedule a 1-arg callable that produces a decayed learning rate when passed the current optimizer step. This can be useful for changing the learning rate value across different invocations of optimizer functions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2ad88fa788374ec95b4583696db082a634b18eb" translate="yes" xml:space="preserve">
          <source>The schedule a 1-arg callable that produces a decayed learning rate when passed the current optimizer step. This can be useful for changing the learning rate value across different invocations of optimizer functions. It is computed as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71d2993ed1ad7b32ffe04dab2486da899c3f2739" translate="yes" xml:space="preserve">
          <source>The schedule is a 1-arg callable that produces a decayed learning rate when passed the current optimizer step. This can be useful for changing the learning rate value across different invocations of optimizer functions. It is computed as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2bf5542bf013cf2cd0c7e2cee3c5c6cd6c529ad0" translate="yes" xml:space="preserve">
          <source>The scope name.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50f230356c528b135a5e5c8be0185503e716f9b3" translate="yes" xml:space="preserve">
          <source>The second call of &lt;code&gt;foo&lt;/code&gt; returns '(A2, A2)' instead of '(A1, A1)' because &lt;a href=&quot;uniform&quot;&gt;&lt;code&gt;tf.random.uniform&lt;/code&gt;&lt;/a&gt; maintains an internal counter. If you want &lt;code&gt;foo&lt;/code&gt; to return '(A1, A1)' every time, use the stateless random ops such as &lt;a href=&quot;stateless_uniform&quot;&gt;&lt;code&gt;tf.random.stateless_uniform&lt;/code&gt;&lt;/a&gt;. Also see &lt;a href=&quot;experimental/generator&quot;&gt;&lt;code&gt;tf.random.experimental.Generator&lt;/code&gt;&lt;/a&gt; for a new set of stateful random ops that use external variables to manage their states.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08568c721bd015f102f6c3ec1b535d6153171292" translate="yes" xml:space="preserve">
          <source>The second dict contains the feature_list key/values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="73916e07f6ea34c81abd799027349571045df1c6" translate="yes" xml:space="preserve">
          <source>The second innermost dimension of &lt;code&gt;diagonal&lt;/code&gt; has double meaning. When &lt;code&gt;k&lt;/code&gt; is scalar or &lt;code&gt;k[0] == k[1]&lt;/code&gt;, &lt;code&gt;M&lt;/code&gt; is part of the batch size [I, J, ..., M], and the output tensor is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b310dae88696133dc7ed85369e494d665ee6beaf" translate="yes" xml:space="preserve">
          <source>The second variant is compatible with CuDNNGRU (GPU-only) and allows inference on CPU. Thus it has separate biases for &lt;code&gt;kernel&lt;/code&gt; and &lt;code&gt;recurrent_kernel&lt;/code&gt;. To use this variant, set &lt;code&gt;'reset_after'=True&lt;/code&gt; and &lt;code&gt;recurrent_activation='sigmoid'&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44fd41a3b2fd550cb5e331da22b4349e8495f086" translate="yes" xml:space="preserve">
          <source>The second variant is compatible with CuDNNGRU (GPU-only) and allows inference on CPU. Thus it has separate biases for &lt;code&gt;kernel&lt;/code&gt; and &lt;code&gt;recurrent_kernel&lt;/code&gt;. Use &lt;code&gt;'reset_after'=True&lt;/code&gt; and &lt;code&gt;recurrent_activation='sigmoid'&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b1ff7b195e25bb1e934a1faddac3cb31e9dda14" translate="yes" xml:space="preserve">
          <source>The selected tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14448741add9013596276b373f9b2b4fe8b71b2b" translate="yes" xml:space="preserve">
          <source>The sequence of &lt;code&gt;Tensor&lt;/code&gt; objects representing the data inputs of this op.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c721fd10cd508d808b1312a2194dc7e4890a7fc" translate="yes" xml:space="preserve">
          <source>The serialized &lt;code&gt;GraphDef&lt;/code&gt; can be imported into another &lt;code&gt;Graph&lt;/code&gt; (using &lt;a href=&quot;graph_util/import_graph_def&quot;&gt;&lt;code&gt;tf.import_graph_def&lt;/code&gt;&lt;/a&gt;) or used with the &lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/cc/index&quot;&gt;C++ Session API&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec73351af802bdf6746d29fb3d40b6d4c532bba4" translate="yes" xml:space="preserve">
          <source>The set of absent/default values may be specified using a vector of lengths or a padding value (but not both). If &lt;code&gt;lengths&lt;/code&gt; is specified, then the output tensor will satisfy &lt;code&gt;output[row] = tensor[row][:lengths[row]]&lt;/code&gt;. If 'lengths' is a list of lists or tuple of lists, those lists will be used as nested row lengths. If &lt;code&gt;padding&lt;/code&gt; is specified, then any row &lt;em&gt;suffix&lt;/em&gt; consisting entirely of &lt;code&gt;padding&lt;/code&gt; will be excluded from the returned &lt;code&gt;RaggedTensor&lt;/code&gt;. If neither &lt;code&gt;lengths&lt;/code&gt; nor &lt;code&gt;padding&lt;/code&gt; is specified, then the returned &lt;code&gt;RaggedTensor&lt;/code&gt; will have no absent/default values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c1258f12879c9bd1b917a210f7db35cdd522088" translate="yes" xml:space="preserve">
          <source>The set of ops to be run as part of the main op upon the load operation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bc1bd93e711b8be01e066d838cfcf34409f0c38" translate="yes" xml:space="preserve">
          <source>The shape is computed using shape inference functions that are registered in the Op for each &lt;code&gt;Operation&lt;/code&gt;. See &lt;a href=&quot;tensorshape&quot;&gt;&lt;code&gt;tf.TensorShape&lt;/code&gt;&lt;/a&gt; for more details of what a shape represents.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5319f783b1231c02b92342d27602eef300f0a629" translate="yes" xml:space="preserve">
          <source>The shape of arguments to &lt;code&gt;__init__&lt;/code&gt;, &lt;code&gt;cdf&lt;/code&gt;, &lt;code&gt;log_cdf&lt;/code&gt;, &lt;code&gt;prob&lt;/code&gt;, and &lt;code&gt;log_prob&lt;/code&gt; reflect this broadcasting, as does the return value of &lt;code&gt;sample&lt;/code&gt; and &lt;code&gt;sample_n&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3720998b3d73539e8c9661d288914e2cdaa82ba7" translate="yes" xml:space="preserve">
          <source>The shape of the output tensor is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b54a3296d32e764f7303629c8f4e133de6a9a7b0" translate="yes" xml:space="preserve">
          <source>The shape of the output will be:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b04d4e03100c6f081b5e785ea772cca017db1933" translate="yes" xml:space="preserve">
          <source>The shapes of the two operands must match: broadcasting is not supported.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="348113de328d02812f9ca2ed7f6d52b929c19ce9" translate="yes" xml:space="preserve">
          <source>The simplest form of RNN network generated is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff2f171ddc511ae06abeb2ff57b6d8a5b4678a63" translate="yes" xml:space="preserve">
          <source>The simplest form of scatter is to insert individual elements in a tensor by index. For example, say we want to insert 4 scattered elements in a rank-1 tensor with 8 elements.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f366cdfba69e2fc90954206f2645c89f45fbcd17" translate="yes" xml:space="preserve">
          <source>The simplest form of tensor_scatter_add is to add individual elements to a tensor by index. For example, say we want to add 4 elements in a rank-1 tensor with 8 elements.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3af9f04f82cbb797b19d70da5fb65e10e6f6ef91" translate="yes" xml:space="preserve">
          <source>The simplest form of tensor_scatter_sub is to subtract individual elements from a tensor by index. For example, say we want to insert 4 scattered elements in a rank-1 tensor with 8 elements.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="802d517fc471afa54d770d15cdbc678a89e0c7a9" translate="yes" xml:space="preserve">
          <source>The simplest version of &lt;code&gt;map_fn&lt;/code&gt; repeatedly applies the callable &lt;code&gt;fn&lt;/code&gt; to a sequence of elements from first to last. The elements are made of the tensors unpacked from &lt;code&gt;elems&lt;/code&gt;. &lt;code&gt;dtype&lt;/code&gt; is the data type of the return value of &lt;code&gt;fn&lt;/code&gt;. Users must provide &lt;code&gt;dtype&lt;/code&gt; if it is different from the data type of &lt;code&gt;elems&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7f6d20e0852a13ae739990f6fedc34688b995c0" translate="yes" xml:space="preserve">
          <source>The simplest version of &lt;code&gt;scan&lt;/code&gt; repeatedly applies the callable &lt;code&gt;fn&lt;/code&gt; to a sequence of elements from first to last. The elements are made of the tensors unpacked from &lt;code&gt;elems&lt;/code&gt; on dimension 0. The callable fn takes two tensors as arguments. The first argument is the accumulated value computed from the preceding invocation of fn, and the second is the value at the current position of &lt;code&gt;elems&lt;/code&gt;. If &lt;code&gt;initializer&lt;/code&gt; is None, &lt;code&gt;elems&lt;/code&gt; must contain at least one element, and its first element is used as the initializer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a38981091aad7a606ea4d4d2a567d64ac905976" translate="yes" xml:space="preserve">
          <source>The simplest way to create a dataset is to create it from a python &lt;code&gt;list&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="017e6c560666c8b8adcda856df4242248fd294ae" translate="yes" xml:space="preserve">
          <source>The sizes of the pooling regions are generated randomly but are fairly uniform. For example, let's look at the height dimension, and the constraints on the list of rows that will be pool boundaries.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50a231b5d401442573df3e17bbb0c7335e8e5d44" translate="yes" xml:space="preserve">
          <source>The softmax of each vector x is calculated by &lt;code&gt;exp(x)/tf.reduce_sum(exp(x))&lt;/code&gt;. The input values in are the log-odds of the resulting probability.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6d8793457c083f0d11acb319bb58de8dcf61657" translate="yes" xml:space="preserve">
          <source>The softplus activation: &lt;code&gt;log(exp(x) + 1)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a35f61726860bd793a42b2269f7f12bfff5f5061" translate="yes" xml:space="preserve">
          <source>The softplus activation: &lt;code&gt;x / (abs(x) + 1)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6542d7e39f0c310801daa6538837ad6138ffae82" translate="yes" xml:space="preserve">
          <source>The solution is to ensure any access to the underlying resource &lt;code&gt;v&lt;/code&gt; is only processed through a critical section:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8a9c87c3bbd16608d71f6244bf102c899fcfe323" translate="yes" xml:space="preserve">
          <source>The solution is to wrap the model construction and execution in a keras-style scope:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e590f462584ca8d9fda3b6ea3ab70cb8db99fef3" translate="yes" xml:space="preserve">
          <source>The source of the non-determinism will be platform- and time-dependent.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b94d68698f376ae68caf545ac2ad9ff65451535" translate="yes" xml:space="preserve">
          <source>The sparse implementation of this algorithm (used when the gradient is an IndexedSlices object, typically because of &lt;a href=&quot;../../../gather&quot;&gt;&lt;code&gt;tf.gather&lt;/code&gt;&lt;/a&gt; or an embedding lookup in the forward pass) does apply momentum to variable slices even if they were not used in the forward pass (meaning they have a gradient equal to zero). Momentum decay (beta1) is also applied to the entire momentum accumulator. This means that the sparse behavior is equivalent to the dense behavior (in contrast to some momentum implementations which ignore momentum unless a variable slice was actually used).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c28505e1992f5a2d21a399d52aaf1d49a6228f7" translate="yes" xml:space="preserve">
          <source>The sparse implementation of this algorithm (used when the gradient is an IndexedSlices object, typically because of &lt;a href=&quot;../../gather&quot;&gt;&lt;code&gt;tf.gather&lt;/code&gt;&lt;/a&gt; or an embedding lookup in the forward pass) does apply momentum to variable slices even if they were not used in the forward pass (meaning they have a gradient equal to zero). Momentum decay (beta1) is also applied to the entire momentum accumulator. This means that the sparse behavior is equivalent to the dense behavior (in contrast to some momentum implementations which ignore momentum unless a variable slice was actually used).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b88004b19728b58b00753256fbdb980b735e8d2" translate="yes" xml:space="preserve">
          <source>The split indices for the ragged tensor value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b29667fc1f908940634484906fb6441d097a55ad" translate="yes" xml:space="preserve">
          <source>The standard &lt;code&gt;segment_*&lt;/code&gt; functions assert that the segment indices are sorted. If you have unsorted indices use the equivalent &lt;code&gt;unsorted_segment_&lt;/code&gt; function. Thses functions take an additional argument &lt;code&gt;num_segments&lt;/code&gt; so that the output tensor can be efficiently allocated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6cf74e13a5dc987612c9d705626f37de29ad545f" translate="yes" xml:space="preserve">
          <source>The standard library uses various well-known names to collect and retrieve values associated with a graph. For example, the &lt;code&gt;tf.Optimizer&lt;/code&gt; subclasses default to optimizing the variables collected under &lt;code&gt;tf.GraphKeys.TRAINABLE_VARIABLES&lt;/code&gt; if none is specified, but it is also possible to pass an explicit list of variables.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ce8c36f190ecab29f1604c69bb1fdd2396d6f9bd" translate="yes" xml:space="preserve">
          <source>The standard pattern for updating variables is to:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1df550e9ac2ff1efc86381fe4007d40151e6c36c" translate="yes" xml:space="preserve">
          <source>The started thread is added to the list of threads managed by the supervisor so it does not need to be passed to the &lt;code&gt;stop()&lt;/code&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22b0c0a61720b2b49f3bdb46b68ab2ffe3bd7116" translate="yes" xml:space="preserve">
          <source>The started thread.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd546c154f1294509a35cef6daf7e05272419f55" translate="yes" xml:space="preserve">
          <source>The state of the optimizer, allowing to resume training exactly where you left off.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="712f8a6f55bf7c5b4fd684dab030efe40f46ccee" translate="yes" xml:space="preserve">
          <source>The statically known shape of this ragged tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="05eef5966a4733b45681c7bb2c1cfa7fcd7e3ded" translate="yes" xml:space="preserve">
          <source>The statistics options associated with the dataset. See &lt;a href=&quot;experimental/statsoptions&quot;&gt;&lt;code&gt;tf.data.experimental.StatsOptions&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a8c6e46a80c3223660eded889659ea2c4138b476" translate="yes" xml:space="preserve">
          <source>The step set by &lt;a href=&quot;set_step&quot;&gt;&lt;code&gt;tf.summary.experimental.set_step()&lt;/code&gt;&lt;/a&gt; if one has been set, otherwise None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5dff7b910b7ad17251b2c5a9c0f91aaacc64e975" translate="yes" xml:space="preserve">
          <source>The str() operator of a 'FlagValues' object provides help for all of the registered 'Flag' objects.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13be6068510e1bba9bcaba7458adf0809e711454" translate="yes" xml:space="preserve">
          <source>The strategy may choose to put the variable on multiple devices, like mirrored variables, but unlike mirrored variables we don't synchronize the updates to them to make sure they have the same value. Instead, the synchronization is performed when reading in cross-replica context. In a replica context, reads and writes are performed on the local copy (we allow reads so you can write code like &lt;code&gt;v = 0.9*v + 0.1*update&lt;/code&gt;). We don't allow operations like &lt;code&gt;v.assign_add&lt;/code&gt; in a cross-replica context for sync on read variables; right now we don't have a use case for such updates and depending on the aggregation mode such updates may not be sensible.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="644f3a81fca55d1677b34a373591148f236e0ab3" translate="yes" xml:space="preserve">
          <source>The string &quot;tensorflow&quot;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56132600b50269345758de3a44957eaf5ba36a10" translate="yes" xml:space="preserve">
          <source>The string name of the device to which this op has been assigned, or an empty string if it has not been assigned to a device.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8992e1dfe2880c9a4de6beb1ac8e0230422a7a79" translate="yes" xml:space="preserve">
          <source>The string name of the underlying Queue.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a96aa4c9d9595a33bb0f9789d55b573d138edebb" translate="yes" xml:space="preserve">
          <source>The string name of this tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c67de356b037fd5e8be6d855afae37205aa00794" translate="yes" xml:space="preserve">
          <source>The string path to the exported directory or &lt;code&gt;None&lt;/code&gt; if export is skipped.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5efe1c4c830550f08c9bea1e979d7c9a7668a12" translate="yes" xml:space="preserve">
          <source>The string path to the exported directory.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb20559dca432fe77c06ffe1211357d7726b77ab" translate="yes" xml:space="preserve">
          <source>The structure of the components of this optional.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="10e607f82b834bedf8082612b2f31785cf1834ed" translate="yes" xml:space="preserve">
          <source>The stubbing is using the builtin getattr and setattr. So, the &lt;strong&gt;get&lt;/strong&gt; and &lt;strong&gt;set&lt;/strong&gt; will be called when stubbing (TODO: A better idea would probably be to manipulate obj.&lt;strong&gt;dict&lt;/strong&gt; instead of getattr() and setattr()).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b87677b2e7091b18f84e0a5a53ce47bf2fbc17a" translate="yes" xml:space="preserve">
          <source>The sum of the squared distance from each point in the first batch of inputs to its nearest cluster center.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa003ab56e1eb5342f9795fdbfba586bfe2f1900" translate="yes" xml:space="preserve">
          <source>The supervisor is notified of any exception raised by one of the services. After an exception is raised, &lt;code&gt;should_stop()&lt;/code&gt; returns &lt;code&gt;True&lt;/code&gt;. In that case the training loop should also stop. This is why the training loop has to check for &lt;code&gt;sv.should_stop()&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b372ddff28f44f63c6e232f1c30d44e36e67c586" translate="yes" xml:space="preserve">
          <source>The table key dtype.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45ea7c0740084e6be8859e3f6c6a5a25999322a4" translate="yes" xml:space="preserve">
          <source>The table value dtype.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="39fc3d1afa951ce1fc0c6acdffe30bf9d05aaa6f" translate="yes" xml:space="preserve">
          <source>The task of an Enqueuer is to use parallelism to speed up preprocessing. This is done with processes or threads.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d203e324545cf312d1311ac6ed39dbe8533902e" translate="yes" xml:space="preserve">
          <source>The temporary directory.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba8d97a1a9f6a392dad767f9f5f935fc3b942c5f" translate="yes" xml:space="preserve">
          <source>The tensor at index &lt;code&gt;index&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1b8bac0316b66126d9a5f3d03ead635dc9c4e503" translate="yes" xml:space="preserve">
          <source>The tensor is shuffled along dimension 0, such that each &lt;code&gt;value[j]&lt;/code&gt; is mapped to one and only one &lt;code&gt;output[i]&lt;/code&gt;. For example, a mapping that might occur for a 3x2 tensor is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="263280cf1684ff5fb9dc90aff492167c49c357e8" translate="yes" xml:space="preserve">
          <source>The tensors in the &lt;code&gt;TensorArray&lt;/code&gt; selected by &lt;code&gt;indices&lt;/code&gt;, packed into one tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14d5f93ad26c02d132da873b0983deda8cc5ede0" translate="yes" xml:space="preserve">
          <source>The tensors returned by the callable identified by &lt;code&gt;branch_index&lt;/code&gt;, or those returned by &lt;code&gt;default&lt;/code&gt; if no key matches and &lt;code&gt;default&lt;/code&gt; was provided, or those returned by the max-keyed &lt;code&gt;branch_fn&lt;/code&gt; if no &lt;code&gt;default&lt;/code&gt; is provided.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ea9d9d23c406f164fd2c56245bf91a63b2aa423" translate="yes" xml:space="preserve">
          <source>The tensors returned by the first pair whose predicate evaluated to True, or those returned by &lt;code&gt;default&lt;/code&gt; if none does.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="62c909c71b66d6f5bfc11fa3e5279c6761971045" translate="yes" xml:space="preserve">
          <source>The tensors returned from &lt;code&gt;fn()&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f38446647c9bdb5f6c17c4098bf60d7f38504af" translate="yes" xml:space="preserve">
          <source>The tensors will be printed to the log, with &lt;code&gt;INFO&lt;/code&gt; severity. If you are not seeing the logs, you might want to add the following line after your imports:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11bfdd9c3d39d4a32c1a375c3ce8bfd375bd0b18" translate="yes" xml:space="preserve">
          <source>The tf.tpu.Topology object for the topology of the TPU cluster.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="befdc8701902b7376e60ef81bed3d724c3bb2879" translate="yes" xml:space="preserve">
          <source>The the elements of the output vector are in range (0, 1) and sum to 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e8a1b74b382d4bec7d008b052874fc8efea06387" translate="yes" xml:space="preserve">
          <source>The threading options associated with the dataset. See &lt;a href=&quot;experimental/threadingoptions&quot;&gt;&lt;code&gt;tf.data.experimental.ThreadingOptions&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3e4fd90c9e18807d72bafd4f4716080dd6bc34f" translate="yes" xml:space="preserve">
          <source>The total number of dimensions in a &lt;code&gt;RaggedTensor&lt;/code&gt; is called its &lt;em&gt;rank&lt;/em&gt;, and the number of ragged dimensions in a &lt;code&gt;RaggedTensor&lt;/code&gt; is called its &lt;em&gt;ragged-rank&lt;/em&gt;. A &lt;code&gt;RaggedTensor&lt;/code&gt;'s ragged-rank is fixed at graph creation time: it can't depend on the runtime values of &lt;code&gt;Tensor&lt;/code&gt;s, and can't vary dynamically for different session runs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7366f216b0f07a06f98c4cfb72b6efd8edad2b69" translate="yes" xml:space="preserve">
          <source>The total variation is the sum of the absolute differences for neighboring pixel-values in the input images. This measures how much noise is in the images.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4cd5555b6721cf29a977af9cf8bb6114222ca75d" translate="yes" xml:space="preserve">
          <source>The total variation of &lt;code&gt;images&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b1be006e04763947d6204304381da8047ab403c6" translate="yes" xml:space="preserve">
          <source>The trace of input tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e883a1ea41a9a9c563ecd15d66e0cea02e0fd475" translate="yes" xml:space="preserve">
          <source>The transformation calls &lt;code&gt;reduce_func&lt;/code&gt; successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The &lt;code&gt;initial_state&lt;/code&gt; argument is used for the initial state and the final state is returned as the result.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a76374d8db32ee668d78ea0cbd0b4fb2bd9d2baf" translate="yes" xml:space="preserve">
          <source>The transpose of &lt;code&gt;atrous_conv2d&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="24fae844f58c6b39229d2519fcdd2125a204111b" translate="yes" xml:space="preserve">
          <source>The transpose of &lt;code&gt;conv1d&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5584a45278a21e44f011bf21c57aa5501a09482" translate="yes" xml:space="preserve">
          <source>The transpose of &lt;code&gt;conv2d&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88458abd7b78099fd74249ee9fe81104261b47e7" translate="yes" xml:space="preserve">
          <source>The transpose of &lt;code&gt;conv3d&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4974672a5f0bdf2f1f4a3ac2ea5cc95568047305" translate="yes" xml:space="preserve">
          <source>The transpose of &lt;code&gt;convolution&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1519a20ef4fe586180643d48497a05b2e0fad92f" translate="yes" xml:space="preserve">
          <source>The tuple of concatenated tensors that was dequeued.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="947a12be95ae913828fb29965c58acb416f681d0" translate="yes" xml:space="preserve">
          <source>The tuple of tensors that was dequeued.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fcce8e7308a1f27fd06f367ce630638c93d290a0" translate="yes" xml:space="preserve">
          <source>The two arguments should be data trees consisting of trees of dicts and lists. They will be deeply compared by walking into the contents of dicts and lists; other items will be compared using the == operator. If the two structures differ in content, the failure message will indicate the location within the structures where the first difference is found. This may be helpful when comparing large structures.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c63ea59f28eeb3c5ef7f12904295f5acd2abba7d" translate="yes" xml:space="preserve">
          <source>The two optional lists, &lt;code&gt;shapes&lt;/code&gt; and &lt;code&gt;names&lt;/code&gt;, must be of the same length as &lt;code&gt;dtypes&lt;/code&gt; if provided. The values at a given index &lt;code&gt;i&lt;/code&gt; indicate the shape and name to use for the corresponding queue component in &lt;code&gt;dtypes&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8cccbb2c85e31880b8682b2d0adfb113b78b64da" translate="yes" xml:space="preserve">
          <source>The type of compression for the record.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e39ef94a4f11a83b4f0f2df2ede7e0ef31ede65" translate="yes" xml:space="preserve">
          <source>The type of loss reduction used in the head.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e1c8635c975f4590fc90901ef34444f7e5713cc" translate="yes" xml:space="preserve">
          <source>The type of sharding that auto-shard should attempt. If this is set to FILE, then we will attempt to shard by files (each worker will get a set of files to process). If we cannot find a set of files to shard for at least one file per worker, we will error out. When this option is selected, make sure that you have enough files so that each worker gets at least one file. There will be a runtime error thrown if there are insufficient files. If this is set to DATA, then we will shard by elements produced by the dataset, and each worker will process the whole dataset and discard the portion that is not for itself. If this is set to OFF, then we will not autoshard, and each worker will receive a copy of the full dataset. This option is set to AUTO by default, AUTO will attempt to first shard by FILE, and fall back to sharding by DATA if we cannot find a set of files to shard.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5ac8c9ef31246fab5b3b784f6fdb5ffec2b2eba" translate="yes" xml:space="preserve">
          <source>The type of the op (e.g. &lt;code&gt;&quot;MatMul&quot;&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f4d546108b4584207f8473dd28294356ddeede5" translate="yes" xml:space="preserve">
          <source>The type specification of an element of this dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31f5cdd5cadead57bcfc00f19cdc637b778a8dfa" translate="yes" xml:space="preserve">
          <source>The type specification of an element of this iterator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bce40406c73aa0072207096be99446cc37cd3334" translate="yes" xml:space="preserve">
          <source>The types of the tensors in &lt;code&gt;values&lt;/code&gt; must match the schema for the fields specified in &lt;code&gt;field_names&lt;/code&gt;. All the tensors in &lt;code&gt;values&lt;/code&gt; must have a common shape prefix, &lt;em&gt;batch_shape&lt;/em&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5381b2e579e07c748270f78ee35dd566e3eb442" translate="yes" xml:space="preserve">
          <source>The typical scenario for &lt;code&gt;ExponentialMovingAverage&lt;/code&gt; is to compute moving averages of variables during training, and restore the variables from the computed moving averages during evaluations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3da77fbceb0e840441ce8c39192555e97fa2928b" translate="yes" xml:space="preserve">
          <source>The underlying accumulator reference.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c25ef637b6290d9c594d8a4f22f2ec949aa1bd01" translate="yes" xml:space="preserve">
          <source>The underlying queue reference.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d25a2d45745d4308947942d9bddb3fe51f90e219" translate="yes" xml:space="preserve">
          <source>The update rule for &lt;code&gt;variable&lt;/code&gt; with gradient &lt;code&gt;g&lt;/code&gt; uses an optimization described at the end of section 2 of the paper:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dcaa87717690c893cddcbb06c2774de0d640ef88" translate="yes" xml:space="preserve">
          <source>The update rule for &lt;code&gt;variable&lt;/code&gt; with gradient &lt;code&gt;g&lt;/code&gt; uses an optimization described at the end of section 7.1 of the paper:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a9901f8d5a239b98ad1f04b6bbdac6a25b5dbe0" translate="yes" xml:space="preserve">
          <source>The updated config has something needed to run a strategy, e.g. configuration to run collective ops, or device filters to improve distributed training performance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="60dd7fe2afadb5341f05d411ef207c2859229807" translate="yes" xml:space="preserve">
          <source>The updated copy of the &lt;code&gt;config_proto&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d9e39fd60700bb347ed0039e54d9af3ef5a5100" translate="yes" xml:space="preserve">
          <source>The updated decorator. If decorator_func is not a tf_decorator, new_target is returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d8daec55baf3c813ce2f2fc4a5eab596ea78fbf" translate="yes" xml:space="preserve">
          <source>The upper regularized incomplete Gamma function is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a16fb2ccec6d9e76f7b18eacf6123674146710e7" translate="yes" xml:space="preserve">
          <source>The user could also use &lt;code&gt;make_input_fn_iterator&lt;/code&gt; if they want to customize which input is fed to which replica/worker etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab789081815c49957f21bd695c2a3924b82c20ff" translate="yes" xml:space="preserve">
          <source>The user is given the option of raising an exception or returning &lt;code&gt;NaN&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d78e502adf2b1729b9f368b8135c69dbb97bac0" translate="yes" xml:space="preserve">
          <source>The usual cross-entropy cost is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="55894b0693429bc52d5ca851ab5e7e50a45cb526" translate="yes" xml:space="preserve">
          <source>The valid keyword arguments in kwds are:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2e9dcb12c3ec9343c2b4d76ebee61442f18c708d" translate="yes" xml:space="preserve">
          <source>The value &lt;code&gt;delta&lt;/code&gt; is added to all components of the tensor &lt;code&gt;image&lt;/code&gt;. &lt;code&gt;image&lt;/code&gt; is converted to &lt;code&gt;float&lt;/code&gt; and scaled appropriately if it is in fixed-point representation, and &lt;code&gt;delta&lt;/code&gt; is converted to the same data type. For regular images, &lt;code&gt;delta&lt;/code&gt; should be in the range &lt;code&gt;[0,1)&lt;/code&gt;, as it is added to the image in floating point representation, where pixel values are in the &lt;code&gt;[0,1)&lt;/code&gt; range.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0cc40289aaf5ca63368c701a28edf746620f686" translate="yes" xml:space="preserve">
          <source>The value of &lt;code&gt;self.value &amp;gt; other.value&lt;/code&gt; if both are known, otherwise None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="28eee150e3c68dabbd6edcffc1df83846a31e392" translate="yes" xml:space="preserve">
          <source>The value of &lt;code&gt;self.value &amp;gt;= other.value&lt;/code&gt; if both are known, otherwise None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ac388cf68a7917e640172de08cfda57213cf191" translate="yes" xml:space="preserve">
          <source>The value of &lt;code&gt;self.value &amp;lt; other.value&lt;/code&gt; if both are known, otherwise None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b01cea77304d81c49aa6bfe991a86b9994cc913b" translate="yes" xml:space="preserve">
          <source>The value of &lt;code&gt;self.value &amp;lt;= other.value&lt;/code&gt; if both are known, otherwise None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="97a93b3e6e5220aba9e50c43c510d2bbbc1a87e0" translate="yes" xml:space="preserve">
          <source>The value of such a flag is a list that contains the individual values from all the appearances of that flag on the command-line.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a54451b41292d69deaf5cd8ce0771065680d134d" translate="yes" xml:space="preserve">
          <source>The value of the attr, as a Python object.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e7716dffe6353c157e4a2c3f2e655d155be6b4b" translate="yes" xml:space="preserve">
          <source>The value of the flag is always a list, even if the option was only supplied once, and even if the default value is a single value</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf8b3cb3b48dec54fd1680c59ba7cefb3dfc3602" translate="yes" xml:space="preserve">
          <source>The value of the variable after the update.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d29c684e1056225784225bef1dc61fddd3c411d6" translate="yes" xml:space="preserve">
          <source>The value of this dimension, or None if it is unknown.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc40d49743dfe15cf0fa34edb80ecfa5afa70d65" translate="yes" xml:space="preserve">
          <source>The value or values returned by &lt;code&gt;map_func&lt;/code&gt; determine the structure of each element in the returned dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27a9bdb78e692ccca6e6eeacfe9cf8259c37e6d7" translate="yes" xml:space="preserve">
          <source>The value returned by &lt;code&gt;run()&lt;/code&gt; has the same shape as the &lt;code&gt;fetches&lt;/code&gt; argument, where the leaves are replaced by the corresponding values returned by TensorFlow.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6455c45d9b49b1bb8f4ec9b5dff00531b9342ec2" translate="yes" xml:space="preserve">
          <source>The value returned by the &lt;code&gt;activity_regularizer&lt;/code&gt; is divided by the input batch size so that the relative weighting between the weight regularizers and the activity regularizers does not change with the batch size.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c61a71eb4077cf30110803e381657708b49ae932" translate="yes" xml:space="preserve">
          <source>The values must include 0. There can be no duplicate values or negative values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1050986413ba8aa09be08e1ada6c99bfa286c827" translate="yes" xml:space="preserve">
          <source>The values not defined in &lt;code&gt;sp_input&lt;/code&gt; don't participate in the reduce max, as opposed to be implicitly assumed 0 -- hence it can return negative values for sparse &lt;code&gt;axis&lt;/code&gt;. But, in case there are no values in &lt;code&gt;axis&lt;/code&gt;, it will reduce to 0. See second example below.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15f0cbbec41c8cec7a1600faa3fd0ba6c8267aeb" translate="yes" xml:space="preserve">
          <source>The values not defined in &lt;code&gt;sp_input&lt;/code&gt; don't participate in the reduce max, as opposed to be implicitly assumed 0 -- hence it can return negative values for sparse &lt;code&gt;reduction_axes&lt;/code&gt;. But, in case there are no values in &lt;code&gt;reduction_axes&lt;/code&gt;, it will reduce to 0. See second example below.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="01b28056c2bd0c58a2e89c42fd350d1a9e497580" translate="yes" xml:space="preserve">
          <source>The values of &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;scale&lt;/code&gt; are chosen so that the mean and variance of the inputs are preserved between two consecutive layers as long as the weights are initialized correctly (see &lt;a href=&quot;../initializers/lecun_normal&quot;&gt;&lt;code&gt;lecun_normal&lt;/code&gt; initialization&lt;/a&gt;) and the number of inputs is &quot;large enough&quot; (see references for more information).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="340b53ddcc61e1600456fa6a942b3a9db9dcd7f4" translate="yes" xml:space="preserve">
          <source>The variable &lt;code&gt;x&lt;/code&gt; updated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b58067393f30493e389388e4808eed6d24490c13" translate="yes" xml:space="preserve">
          <source>The variable dtype of this policy, or None if the variable dtype should be inferred from the inputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb822034c75bacc81fb53dec6e4b25c54f49ac48" translate="yes" xml:space="preserve">
          <source>The variable dtype of this policy.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf801794eaaba6ef2f8f7f433ed37a63d7d2327d" translate="yes" xml:space="preserve">
          <source>The variance for Student's T equals</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15e91da427b6cf2b2fbc5ac9447fe8b24dd8aa82" translate="yes" xml:space="preserve">
          <source>The width the output tensor is &lt;code&gt;input_depth * block_size&lt;/code&gt;, whereas the height is &lt;code&gt;input_height * block_size&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78fda38eef982ddecbfd2142c00746b57a76815b" translate="yes" xml:space="preserve">
          <source>The word index dictionary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="273384d93f6363b2a2a4c34767e0268890f5d5af" translate="yes" xml:space="preserve">
          <source>The wrapped input tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4a82e3bb086ae7e63715facddf72f40e0765006" translate="yes" xml:space="preserve">
          <source>The wrapped output tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52457270e7a01e13ee00378b3108ea8fc12315e0" translate="yes" xml:space="preserve">
          <source>The wrapped value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c29d607aec40364caa8044d1b086cb311bf2d17" translate="yes" xml:space="preserve">
          <source>Theano-like behavior example</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d073abde2a1df4b5c62a21448d955ee1c50a80df" translate="yes" xml:space="preserve">
          <source>Then the output is a dictionary:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f77a2ae4d69da67530e4f97b9e84f16f29f79cdf" translate="yes" xml:space="preserve">
          <source>Then,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18c6dda9e31a83fdb5a90169c27005c658e8056f" translate="yes" xml:space="preserve">
          <source>Then, row_pooling_sequence should satisfy:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3debd4416d24691c45dc01b19b3489468194d8c2" translate="yes" xml:space="preserve">
          <source>There are a number of questions to ask in the decision process, including:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="372d16d9e0a1868251f75c59e5f513839dddd179" translate="yes" xml:space="preserve">
          <source>There are many different ways to implement atrous convolution (see the refs above). The implementation here reduces</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4b4c86e082be3a02799da3aa4833e74149042874" translate="yes" xml:space="preserve">
          <source>There are nodes like Identity and CheckNumerics that are only useful during training, and can be removed in graphs that will be used for nothing but inference. Here we identify and remove them, returning an equivalent graph. To be specific, CheckNumerics nodes are always removed, and Identity nodes that aren't involved in control edges are spliced out so that their input and outputs are directly connected.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4fe5597954ba439b4679d614be0e1e5164b606b" translate="yes" xml:space="preserve">
          <source>There are several delicate issues when running multiple threads that way: closing the queues in sequence as the input is exhausted, correctly catching and reporting exceptions, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="20bc80fe9d0b075a12bc0575d2ba4a5d5fe1269d" translate="yes" xml:space="preserve">
          <source>There are several ways to run the conversion:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a0c34e11b24dd273612b5e842b9ca3db6c2dc5f" translate="yes" xml:space="preserve">
          <source>There are three important concepts associated with TensorFlow Distributions shapes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3536015a851856ea989836d31de530e9b064da8c" translate="yes" xml:space="preserve">
          <source>There are two means to control the logging verbosity:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cfe77a90ab442852a7e0e410775f1f4aa154097a" translate="yes" xml:space="preserve">
          <source>There are two possible return values, &quot;google&quot; (when TensorFlow is running in a Google-internal environment) or an empty string (when TensorFlow is running elsewhere).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c121ae8dd46f0cedd4d9939e53220e70d1795c85" translate="yes" xml:space="preserve">
          <source>There are two questions to ask in the decision process: Do you need gradients computed as sparse too? Is your sparse data represented as two &lt;code&gt;SparseTensor&lt;/code&gt;s: ids and values? There is more explanation about data format below. If you answer any of these questions as yes, consider using &lt;a href=&quot;../nn/embedding_lookup_sparse&quot;&gt;&lt;code&gt;tf.nn.embedding_lookup_sparse&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0878716bd35f31f600ff16927cc66d2e1bdcca8d" translate="yes" xml:space="preserve">
          <source>There are two variants of the GRU implementation. The default one is based on &lt;a href=&quot;https://arxiv.org/abs/1406.1078v3&quot;&gt;v3&lt;/a&gt; and has reset gate applied to hidden state before matrix multiplication. The other one is based on &lt;a href=&quot;https://arxiv.org/abs/1406.1078v1&quot;&gt;original&lt;/a&gt; and has the order reversed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3d0da97f844d09b6bac8339000119f443c41934" translate="yes" xml:space="preserve">
          <source>There are two variants. The default one is based on 1406.1078v3 and has reset gate applied to hidden state before matrix multiplication. The other one is based on original 1406.1078v1 and has the order reversed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="19c11322b50f362088a37650ccaf3b2e479b115a" translate="yes" xml:space="preserve">
          <source>There are two versions of the API: ExportSavedModelApiVersion.V1 and V2.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c409af06e5c37c31d5d6e63ae53409968bb739b2" translate="yes" xml:space="preserve">
          <source>There are two ways to create decorators that TensorFlow can introspect into. This is important for documentation generation purposes, so that function signatures aren't obscured by the (*args, **kwds) signature that decorators often provide.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49196c8dfb0588201d8b9733a14917623170a741" translate="yes" xml:space="preserve">
          <source>There are two ways to instantiate a &lt;code&gt;Model&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3e498654dcff830e5be467e55fbbbbc20e2910f" translate="yes" xml:space="preserve">
          <source>There are two ways to use the moving averages for evaluations:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14b50269ad70f1441184d03f5b9e983ae0e09462" translate="yes" xml:space="preserve">
          <source>There is a special node with &lt;code&gt;task_type&lt;/code&gt; as &lt;code&gt;evaluator&lt;/code&gt;, which is not part of the (training) &lt;code&gt;cluster_spec&lt;/code&gt;. It handles the distributed evaluation job.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0a273890eb3544a6c0002e05e862f10bbfd245d" translate="yes" xml:space="preserve">
          <source>There is an equivalent description in terms of the [batch] spectrum &lt;code&gt;H&lt;/code&gt; and Fourier transforms. Here we consider &lt;code&gt;A.shape = [N, N]&lt;/code&gt; and ignore batch dimensions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34b53209d83711d378deaa8b481a2f90355eb0e3" translate="yes" xml:space="preserve">
          <source>There is an equivalent description in terms of the [batch] spectrum &lt;code&gt;H&lt;/code&gt; and Fourier transforms. Here we consider &lt;code&gt;A.shape = [N, N]&lt;/code&gt; and ignore batch dimensions. Define the discrete Fourier transform (DFT) and its inverse by</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70db84bf8987013d7aeb820bf2eec70af7a073ca" translate="yes" xml:space="preserve">
          <source>There is no need to delete the directory after the test.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b246461630b3c81ed498d37ca7dc4b93263133aa" translate="yes" xml:space="preserve">
          <source>There is often a need to lift variable initialization ops out of control-flow scopes, function-building graphs, and gradient tapes. Entering an &lt;code&gt;init_scope&lt;/code&gt; is a mechanism for satisfying these desiderata. In particular, entering an &lt;code&gt;init_scope&lt;/code&gt; has three effects:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9600bed3adf3351d3b6785f43a000c7d51f5ddee" translate="yes" xml:space="preserve">
          <source>There is one exception: if the final (i.e., innermost) element(s) of &lt;code&gt;partitions&lt;/code&gt; are &lt;code&gt;UniformRowLength&lt;/code&gt;s, then the values are simply reshaped (as a higher-dimensional &lt;a href=&quot;../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt;), rather than being wrapped in a &lt;a href=&quot;../raggedtensor&quot;&gt;&lt;code&gt;tf.RaggedTensor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69f7128e30301479f7dbfc0ac8f483f3d6d00a4d" translate="yes" xml:space="preserve">
          <source>Therefore we introduce some decoupling using a queue. The queue contains the work units and the Reader dequeues from the queue when it is asked to produce a record (via Read()) but it has finished the last work unit.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86cd25893d67c9049a975fb7c203728b1fcc6c8b" translate="yes" xml:space="preserve">
          <source>These are arguments passed to the optimizer subclass constructor (the &lt;code&gt;__init__&lt;/code&gt; method), and then passed to &lt;code&gt;self._set_hyper()&lt;/code&gt;. They can be either regular Python values (like 1.0), tensors, or callables. If they are callable, the callable will be called during &lt;code&gt;apply_gradients()&lt;/code&gt; to get the value for the hyper parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d1eefae8eebd09f04dd361373c7c577f0cf1144d" translate="yes" xml:space="preserve">
          <source>These conversion options are experimental. They are subject to change without notice and offer no guarantees.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c291964ed47da0b87e8a53387f07ca95b28e890c" translate="yes" xml:space="preserve">
          <source>These indices specify where the values for each row begin in &lt;code&gt;self.values&lt;/code&gt;. &lt;code&gt;rt.row_starts()&lt;/code&gt; is equal to &lt;code&gt;rt.row_splits[:-1]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99b0058dc30c77b214aebabddab907af2da4a216" translate="yes" xml:space="preserve">
          <source>These indices specify where the values for each row end in &lt;code&gt;self.values&lt;/code&gt;. &lt;code&gt;rt.row_limits(self)&lt;/code&gt; is equal to &lt;code&gt;rt.row_splits[:-1]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="23fdcaa94183d818b1595f88d3e5484cc7528a37" translate="yes" xml:space="preserve">
          <source>These layers expose 3 keyword arguments:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="76c2c66d43654cd0b29da8b2ca646058c82ed16b" translate="yes" xml:space="preserve">
          <source>These might be stored sparsely in the following Example protos by storing only the feature ids (column number if the vectors are treated as a matrix) of the non-zero elements and the corresponding values:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df0616278e0670c1f8a838c05616c2906a5ad628" translate="yes" xml:space="preserve">
          <source>These sufficient statistics are computed using the one pass algorithm on an input that's optionally shifted. See: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c983cdacd0a0593413a5e4ab438ec1b6414a3f5" translate="yes" xml:space="preserve">
          <source>These typically correspond to model heads.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2e13849b656ad081d144e8fb21e76af2be31973" translate="yes" xml:space="preserve">
          <source>These values are similar to values from a &lt;code&gt;random_normal_initializer&lt;/code&gt; except that values more than two standard deviations from the mean are discarded and re-drawn. This is the recommended initializer for neural network weights and filters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="035fa9e40dd64ae60fe7c5634de4cc5ea73a82db" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;LinearOperator&lt;/code&gt; is initialized to have shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; by providing &lt;code&gt;spectrum&lt;/code&gt;, a &lt;code&gt;[B1,...,Bb, N0, N1, N2]&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt; with &lt;code&gt;N0*N1*N2 = N&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7dc76893f409505eb4af8e3ca9d9b50f0907d32c" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;LinearOperator&lt;/code&gt; is initialized to have shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; by providing &lt;code&gt;spectrum&lt;/code&gt;, a &lt;code&gt;[B1,...,Bb, N0, N1]&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt; with &lt;code&gt;N0*N1 = N&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45faa2260146699c38e2dafd91d1b8425180a30a" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;LinearOperator&lt;/code&gt; is initialized to have shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; by providing &lt;code&gt;spectrum&lt;/code&gt;, a &lt;code&gt;[B1,...,Bb, N]&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14d38c8190663bcf5aee0e569e1c9fa815f97d9a" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;LinearOperator&lt;/code&gt; is initialized with boolean flags of the form &lt;code&gt;is_X&lt;/code&gt;, for &lt;code&gt;X = non_singular, self_adjoint, positive_definite, square&lt;/code&gt;. These have the following meaning</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="59e2aeacdf121218eb0206f7223d277271e20ff4" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;LinearOperator&lt;/code&gt; is initialized with boolean flags of the form &lt;code&gt;is_X&lt;/code&gt;, for &lt;code&gt;X = non_singular, self_adjoint, positive_definite, square&lt;/code&gt;. These have the following meaning:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c893a77a5cadc49b62c18f0fba68970cb1dcbaf6" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;LinearOperator&lt;/code&gt; is initialized with boolean flags of the form &lt;code&gt;is_X&lt;/code&gt;, for &lt;code&gt;X = non_singular&lt;/code&gt;, &lt;code&gt;self_adjoint&lt;/code&gt;, &lt;code&gt;positive_definite&lt;/code&gt;, &lt;code&gt;diag_update_positive&lt;/code&gt; and &lt;code&gt;square&lt;/code&gt;. These have the following meaning:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1af767d14077ff11d4e7fc9b014e22bbf1d448de" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;Model&lt;/code&gt; has a dependency named &quot;input_transform&quot; on its &lt;code&gt;Dense&lt;/code&gt; layer, which in turn depends on its variables. As a result, saving an instance of &lt;code&gt;Regress&lt;/code&gt; using &lt;a href=&quot;../../../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; will also save all the variables created by the &lt;code&gt;Dense&lt;/code&gt; layer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6bb0cfce4d3ef61c634cae7fd40aef46d6d26e65" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;Model&lt;/code&gt; has a dependency named &quot;input_transform&quot; on its &lt;code&gt;Dense&lt;/code&gt; layer, which in turn depends on its variables. As a result, saving an instance of &lt;code&gt;Regress&lt;/code&gt; using &lt;a href=&quot;checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; will also save all the variables created by the &lt;code&gt;Dense&lt;/code&gt; layer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b5871012b5796b5a485fbd2ea90e6e96dc8a6361" translate="yes" xml:space="preserve">
          <source>This API allows querying the physical hardware resources prior to runtime initialization. Thus, giving an opportunity to call any additional configuration APIs. This is in contrast to &lt;a href=&quot;list_logical_devices&quot;&gt;&lt;code&gt;tf.config.list_logical_devices&lt;/code&gt;&lt;/a&gt;, which triggers runtime initialization in order to list the configured devices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb9102764cadc1d75fc4b0e80481d3d9ed63d68c" translate="yes" xml:space="preserve">
          <source>This Estimator implements the following variants of the K-means algorithm:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80c1b7fb2eecacd5d699b9ee230de5704ca37c8d" translate="yes" xml:space="preserve">
          <source>This Multinomial distribution is parameterized by &lt;code&gt;probs&lt;/code&gt;, a (batch of) length-&lt;code&gt;K&lt;/code&gt;&lt;code&gt;prob&lt;/code&gt; (probability) vectors (&lt;code&gt;K &amp;gt; 1&lt;/code&gt;) such that &lt;code&gt;tf.reduce_sum(probs, -1) = 1&lt;/code&gt;, and a &lt;code&gt;total_count&lt;/code&gt; number of trials, i.e., the number of trials per draw from the Multinomial. It is defined over a (batch of) length-&lt;code&gt;K&lt;/code&gt; vector &lt;code&gt;counts&lt;/code&gt; such that &lt;code&gt;tf.reduce_sum(counts, -1) = total_count&lt;/code&gt;. The Multinomial is identically the Binomial distribution when &lt;code&gt;K = 2&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d0c42e63acc7dcc7eea605a766b11674d1c53e82" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] != y[i]&lt;/code&gt; holds for every pair of (possibly broadcast) elements of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="74ec1aa4f8491ea53037a0fc532d4741eb9218bd" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] &amp;gt; 0&lt;/code&gt; holds for every element of &lt;code&gt;x&lt;/code&gt;. If &lt;code&gt;x&lt;/code&gt; is empty, this is trivially satisfied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64284c5803db04a210d1a9048bfe82459a849a9d" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] &amp;gt; y[i]&lt;/code&gt; holds for every pair of (possibly broadcast) elements of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="382c384f5bde2874efffdf9480af145ba234b875" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] &amp;gt;= 0&lt;/code&gt; holds for every element of &lt;code&gt;x&lt;/code&gt;. If &lt;code&gt;x&lt;/code&gt; is empty, this is trivially satisfied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="501dbba0dad9772b46dd6dbe6d32dfc34b3073e4" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] &amp;gt;= y[i]&lt;/code&gt; holds for every pair of (possibly broadcast) elements of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86a98c771b136d5122e127f8b045988c2c9039d5" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] &amp;lt; 0&lt;/code&gt; holds for every element of &lt;code&gt;x&lt;/code&gt;. If &lt;code&gt;x&lt;/code&gt; is empty, this is trivially satisfied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a26f9f0c29e64714922c2bda7e956bffe84f5c0c" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] &amp;lt; y[i]&lt;/code&gt; holds for every pair of (possibly broadcast) elements of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c63795d0aad1b74648e6a4624d3ec74f6c7d4a1e" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] &amp;lt;= 0&lt;/code&gt; holds for every element of &lt;code&gt;x&lt;/code&gt;. If &lt;code&gt;x&lt;/code&gt; is empty, this is trivially satisfied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52d38297a91871d25885ecc0eb75b01217f176df" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] &amp;lt;= y[i]&lt;/code&gt; holds for every pair of (possibly broadcast) elements of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a274dbd07debbab7fc4511fde87c45ce7dd5f11a" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] - y[i] &amp;lt; atol + rtol * tf.abs(y[i])&lt;/code&gt; holds for every pair of (possibly broadcast) elements of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="afe67f7444ede4a73d34ba9ae37a78f686f755dd" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] == y[i]&lt;/code&gt; holds for every pair of (possibly broadcast) elements of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47fcdf77a806254f145cef6045f72f7418b87096" translate="yes" xml:space="preserve">
          <source>This Op checks that a collection of tensors shape relationships satisfies given constraints.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70d8b32973294baa04c2b6c2343f69d0d35b251b" translate="yes" xml:space="preserve">
          <source>This Op checks that the rank of &lt;code&gt;x&lt;/code&gt; is equal to &lt;code&gt;rank&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45bf526016d2ed135926c16509b05dd90dd8b920" translate="yes" xml:space="preserve">
          <source>This Op checks that the rank of &lt;code&gt;x&lt;/code&gt; is greater or equal to &lt;code&gt;rank&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef8abcb7ef7c82e56e48acf10aee7a915b661e8e" translate="yes" xml:space="preserve">
          <source>This Op checks that the rank of &lt;code&gt;x&lt;/code&gt; is in &lt;code&gt;ranks&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1956a13a2a0626040a0d05adca12d21c39b428f4" translate="yes" xml:space="preserve">
          <source>This Op takes a SparseTensor and is the sparse counterpart to &lt;a href=&quot;../../math/reduce_max&quot;&gt;&lt;code&gt;tf.reduce_max()&lt;/code&gt;&lt;/a&gt;. In contrast to SparseReduceSum, this Op returns a SparseTensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21543202b1cccf9199ac85ba0b6367bb37a43438" translate="yes" xml:space="preserve">
          <source>This Op takes a SparseTensor and is the sparse counterpart to &lt;a href=&quot;../../math/reduce_max&quot;&gt;&lt;code&gt;tf.reduce_max()&lt;/code&gt;&lt;/a&gt;. In particular, this Op also returns a dense &lt;code&gt;Tensor&lt;/code&gt; instead of a sparse one.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a92cf7c266fad63ba2ede16d013a7954c0a3ed23" translate="yes" xml:space="preserve">
          <source>This Op takes a SparseTensor and is the sparse counterpart to &lt;a href=&quot;../../math/reduce_sum&quot;&gt;&lt;code&gt;tf.reduce_sum()&lt;/code&gt;&lt;/a&gt;. In contrast to SparseReduceSum, this Op returns a SparseTensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eff3316c87e58a0e09844e295d654acad826f230" translate="yes" xml:space="preserve">
          <source>This Op takes a SparseTensor and is the sparse counterpart to &lt;a href=&quot;../../math/reduce_sum&quot;&gt;&lt;code&gt;tf.reduce_sum()&lt;/code&gt;&lt;/a&gt;. In particular, this Op also returns a dense &lt;code&gt;Tensor&lt;/code&gt; instead of a sparse one.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7321159b3f380865d8a3ddbbd1ea9288e20a93c6" translate="yes" xml:space="preserve">
          <source>This Op takes a SparseTensor and is the sparse counterpart to &lt;a href=&quot;../math/reduce_max&quot;&gt;&lt;code&gt;tf.reduce_max()&lt;/code&gt;&lt;/a&gt;. In particular, this Op also returns a dense &lt;code&gt;Tensor&lt;/code&gt; if &lt;code&gt;output_is_sparse&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, or a &lt;code&gt;SparseTensor&lt;/code&gt; if &lt;code&gt;output_is_sparse&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a574e60c745da5b56aeabab5fc189ca97095b4e6" translate="yes" xml:space="preserve">
          <source>This Op takes a SparseTensor and is the sparse counterpart to &lt;a href=&quot;../math/reduce_sum&quot;&gt;&lt;code&gt;tf.reduce_sum()&lt;/code&gt;&lt;/a&gt;. In particular, this Op also returns a dense &lt;code&gt;Tensor&lt;/code&gt; if &lt;code&gt;output_is_sparse&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, or a &lt;code&gt;SparseTensor&lt;/code&gt; if &lt;code&gt;output_is_sparse&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9513360fe89de6564516b0de822250d992516167" translate="yes" xml:space="preserve">
          <source>This adjusts the dynamic range of the gradient evaluation by scaling up the &lt;code&gt;loss&lt;/code&gt; value. The gradient values are then scaled back down by the recipricol of the loss scale. This is useful in reduced precision training where small gradient values would otherwise underflow the representable range.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4742c30308cfd6082fa96c4df5f33b7fbb5afa83" translate="yes" xml:space="preserve">
          <source>This allows communication and coordination when there are multiple calls to the step_fn triggered by a call to &lt;code&gt;strategy.experimental_run_v2(step_fn, ...)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6406854276bccf2ce7a8330ff5c67c7e93bc6f24" translate="yes" xml:space="preserve">
          <source>This allows creating a sub-tensor from part of the current contents of a variable. See &lt;a href=&quot;../../tensor#__getitem__&quot;&gt;&lt;code&gt;tf.Tensor.&lt;strong&gt;getitem&lt;/strong&gt;&lt;/code&gt;&lt;/a&gt; for detailed examples of slicing.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f9bd14a03a5b9808d578e330775308ae0518e1f" translate="yes" xml:space="preserve">
          <source>This allows creating a sub-tensor from part of the current contents of a variable. See &lt;a href=&quot;tensor#__getitem__&quot;&gt;&lt;code&gt;tf.Tensor.&lt;strong&gt;getitem&lt;/strong&gt;&lt;/code&gt;&lt;/a&gt; for detailed examples of slicing.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a0031976f29ed8ed5b38f0545ffe09ca05a84a3" translate="yes" xml:space="preserve">
          <source>This allows reading and writing to this tensors w/o copies. This more closely mirrors the C++ Interpreter class interface's tensor() member, hence the name. Be careful to not hold these output references through calls to &lt;code&gt;allocate_tensors()&lt;/code&gt; and &lt;code&gt;invoke()&lt;/code&gt;. This function cannot be used to read intermediate results.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aab58add7bb75e6c4119bf13ecc09894979d960d" translate="yes" xml:space="preserve">
          <source>This allows you to save the entirety of the state of a model in a single file.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="083c822ae2751294f6b801ab1aec618144643d76" translate="yes" xml:space="preserve">
          <source>This also supports either output striding via the optional &lt;code&gt;strides&lt;/code&gt; parameter or atrous convolution (also known as convolution with holes or dilated convolution, based on the French word &quot;trous&quot; meaning holes in English) via the optional &lt;code&gt;dilation_rate&lt;/code&gt; parameter. Currently, however, output striding is not supported for atrous convolutions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0282c5f56f860d1be334e321248ec8429b6f156d" translate="yes" xml:space="preserve">
          <source>This also supports either output striding via the optional &lt;code&gt;strides&lt;/code&gt; parameter or atrous convolution (also known as convolution with holes or dilated convolution, based on the French word &quot;trous&quot; meaning holes in English) via the optional &lt;code&gt;dilations&lt;/code&gt; parameter. Currently, however, output striding is not supported for atrous convolutions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40ecf12ea54cbb9c03cee235c92870bdadc05d0e" translate="yes" xml:space="preserve">
          <source>This assumes the input dictionary contains a &lt;code&gt;SparseTensor&lt;/code&gt; for key 'terms', and a &lt;code&gt;SparseTensor&lt;/code&gt; for key 'frequencies'. These 2 tensors must have the same indices and dense shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec5e01502dedce9b5506d5284b5d16bed3dce6f8" translate="yes" xml:space="preserve">
          <source>This avoids adding &lt;code&gt;numpy_input&lt;/code&gt; as a large constant in the graph, and copies the data to the machine or machines that will be processing the input.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="542cdf0657b556d7d4f13254bf996896da1af7f2" translate="yes" xml:space="preserve">
          <source>This behaves similarly to &lt;a href=&quot;../../name_scope&quot;&gt;&lt;code&gt;tf.name_scope&lt;/code&gt;&lt;/a&gt;, except that it returns a generated summary tag in addition to the scope name. The tag is structurally similar to the scope name - derived from the user-provided name, prefixed with enclosing name scopes if any - but we relax the constraint that it be uniquified, as well as the character set limitation (so the user-provided name can contain characters not legal for scope names; in the scope name these are removed).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d690c9d397a6da08fd08939e695a0fb6ea89e4b" translate="yes" xml:space="preserve">
          <source>This behavior gives control to callers on what to do if checkpoints do not come fast enough or stop being generated. For example, if callers have a way to detect that the training has stopped and know that no new checkpoints will be generated, they can provide a &lt;code&gt;timeout_fn&lt;/code&gt; that returns &lt;code&gt;True&lt;/code&gt; when the training has stopped. If they know that the training is still going on they return &lt;code&gt;False&lt;/code&gt; instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d87911d57456c793352550a56288318dbdc2112" translate="yes" xml:space="preserve">
          <source>This behavior has been introduced in TensorFlow 2.0, in order to enable &lt;code&gt;layer.trainable = False&lt;/code&gt; to produce the most commonly expected behavior in the convnet fine-tuning use case.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="566da943133ad9f7ff89e0ae58ccee97f0942cd9" translate="yes" xml:space="preserve">
          <source>This behavior only occurs as of TensorFlow 2.0. In 1.*, setting &lt;code&gt;layer.trainable = False&lt;/code&gt; would freeze the layer but would not switch it to inference mode.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="947e43a536a4f632b306b7d5c3e4805a08d13eec" translate="yes" xml:space="preserve">
          <source>This blocks the calling thread until the thread whose join() method is called terminates -- either normally or through an unhandled exception or until the optional timeout occurs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f576e406288a87f269b5d6306d3d486ac32d4798" translate="yes" xml:space="preserve">
          <source>This boolean flag determines whether variables should be initialized as they are instantiated (default), or if the user should handle the initialization (e.g. via &lt;a href=&quot;../../compat/v1/initialize_all_variables&quot;&gt;&lt;code&gt;tf.compat.v1.initialize_all_variables()&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d320c46f5814a56c1f3f60058177b848b6fe981" translate="yes" xml:space="preserve">
          <source>This call blocks until a set of threads have terminated. The set of thread is the union of the threads passed in the &lt;code&gt;threads&lt;/code&gt; argument and the list of threads that registered with the coordinator by calling &lt;a href=&quot;coordinator#register_thread&quot;&gt;&lt;code&gt;Coordinator.register_thread()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51c54c7d5726811d44822079b16e485771669c24" translate="yes" xml:space="preserve">
          <source>This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="062df39159f4972c7eb0c0bbc3c621a80385af7d" translate="yes" xml:space="preserve">
          <source>This callback is automatically applied to every Keras model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a2209138c3b99588918296a296e9d5cba348ee9" translate="yes" xml:space="preserve">
          <source>This callback is automatically applied to every Keras model. The &lt;code&gt;History&lt;/code&gt; object gets returned by the &lt;code&gt;fit&lt;/code&gt; method of models.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a8fccab0666eafd61c1fb78dee506da35fb92ce" translate="yes" xml:space="preserve">
          <source>This callback is constructed with anonymous functions that will be called at the appropriate time. Note that the callbacks expects positional arguments, as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d2fd53cd69211c630c172b830b110ef6741f78e7" translate="yes" xml:space="preserve">
          <source>This callback logs events for TensorBoard, including:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c4d9ff06bd1c217d75cb9a57ebc28626f964add" translate="yes" xml:space="preserve">
          <source>This can always be checked statically, so this method returns nothing.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f195f030f5d6a3952c927a303c10a3d51f131c5" translate="yes" xml:space="preserve">
          <source>This can be faster than multiple individual &lt;code&gt;reduce&lt;/code&gt;s because we can fuse several tensors into one or multiple packs before reduction.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4c1e8ee1a694edb65fdf5329eae65afdec683d9" translate="yes" xml:space="preserve">
          <source>This can be used as a &quot;join&quot; mechanism for parallel computations: all the argument tensors can be computed in parallel, but the values of any tensor returned by &lt;code&gt;tuple&lt;/code&gt; are only available after all the parallel computations are done.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66b08400383b183cc1cba0b591ec90b5fe85d73d" translate="yes" xml:space="preserve">
          <source>This can be used as a loss-function during optimization so as to suppress noise in images. If you have a batch of images, then you should calculate the scalar loss-value as the sum: &lt;code&gt;loss = tf.reduce_sum(tf.image.total_variation(images))&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff61da80eb1038583fadf6e588b06357b27a55d8" translate="yes" xml:space="preserve">
          <source>This can be useful for debugging or profiling. For example, let's say you implemented a simple iterative sqrt function, and you want to collect the intermediate values and plot the convergence. Appending the values to a list in &lt;code&gt;@tf.function&lt;/code&gt; normally wouldn't work since it will just record the Tensors being traced, not the values. Instead, you can do the following.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5bc85a9ecd105989dec28b806afbbe4faa69f248" translate="yes" xml:space="preserve">
          <source>This can be useful if you want to log debug a training algorithm, report stats about the slots, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de803dd8ed0f3487c59166443a170b97850a2c8d" translate="yes" xml:space="preserve">
          <source>This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c35c7cb7e9df11039724d697524b210395fe269e" translate="yes" xml:space="preserve">
          <source>This class assumes each worker is running the same code independently, but parameter servers are running a standard server. This means that while each worker will synchronously compute a single gradient update across all GPUs, updates between workers proceed asynchronously. Operations that occur only on the first replica (such as incrementing the global step), will occur on the first replica &lt;em&gt;of every worker&lt;/em&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ea4dadc6d5580b6a9567a54f238c77dab3d0a06" translate="yes" xml:space="preserve">
          <source>This class caches file writers, one per directory.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9243b860278b8d7215f95a8eb101fc7a883ab5b" translate="yes" xml:space="preserve">
          <source>This class can create placeholders for tf.Tensors, tf.SparseTensors, and tf.RaggedTensors by choosing 'sparse=True' or 'ragged=True'.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b93eb86d4b8b199ed8de190972226d76d3ddbda5" translate="yes" xml:space="preserve">
          <source>This class defines the API to add Ops to train a model. You never use this class directly, but instead instantiate one of its subclasses such as &lt;a href=&quot;sgd&quot;&gt;&lt;code&gt;tf.keras.optimizers.SGD&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;adam&quot;&gt;&lt;code&gt;tf.keras.optimizers.Adam&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1921ba1f544676f9bbcb9030a711c397087751d1" translate="yes" xml:space="preserve">
          <source>This class defines the API to add Ops to train a model. You never use this class directly, but instead instantiate one of its subclasses such as &lt;code&gt;GradientDescentOptimizer&lt;/code&gt;, &lt;code&gt;AdagradOptimizer&lt;/code&gt;, or &lt;code&gt;MomentumOptimizer&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1b4934ebc6e87e946a8327b096ccaba6f902fbb" translate="yes" xml:space="preserve">
          <source>This class defines the key and value used for tf.lookup.TextFileInitializer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bbef232ee84dd43e03ff0bb31e2fff3d62ddb086" translate="yes" xml:space="preserve">
          <source>This class exports the serving graph and checkpoints at the end.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2398f265a0682e91ce8fc30c31cc00a32b85cf94" translate="yes" xml:space="preserve">
          <source>This class exports the serving graph and checkpoints of the best models.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f378b5fe5691dc39f9cd3d236136a8685104c4fd" translate="yes" xml:space="preserve">
          <source>This class has been deprecated. Please use &lt;a href=&quot;../../../lite/tfliteconverter&quot;&gt;&lt;code&gt;lite.TFLiteConverter&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8616741a1ce86b957920807cdf7bdf7cd9ce000e" translate="yes" xml:space="preserve">
          <source>This class has two primary purposes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e41eb7b59b50f02eb388655630751957c627b643" translate="yes" xml:space="preserve">
          <source>This class implements &lt;code&gt;__enter__&lt;/code&gt; and &lt;code&gt;__exit__&lt;/code&gt;, and can be used in &lt;code&gt;with&lt;/code&gt; blocks like a normal file.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="61d78e6233b72d451051551f631a8d253b9c0d10" translate="yes" xml:space="preserve">
          <source>This class implements a simple mechanism to coordinate the termination of a set of threads.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c453470b9fb7d2dd4c8372805a2c5b45dcafe1a" translate="yes" xml:space="preserve">
          <source>This class in stateful and thread-compatible.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6c71f2399c238428100228254e29a64f6c45e19" translate="yes" xml:space="preserve">
          <source>This class is a simple wrapper for a pair of &lt;code&gt;Tensor&lt;/code&gt; objects:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aaddfb846b1d90d727149969bb3d79f9de0c7466" translate="yes" xml:space="preserve">
          <source>This class is a small wrapper that takes care of session creation and checkpoint recovery. It also provides functions that to facilitate coordination among multiple training threads or processes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17463f3d93317c6b5e6f0cc508b2a6f39e7358ea" translate="yes" xml:space="preserve">
          <source>This class is deprecated. For synchrononous training, please use &lt;a href=&quot;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute&quot;&gt;Distribution Strategies&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4616d8f2e476585f5e22705937707332b5baeb5b" translate="yes" xml:space="preserve">
          <source>This class is deprecated. Please use &lt;a href=&quot;monitoredtrainingsession&quot;&gt;&lt;code&gt;tf.compat.v1.train.MonitoredTrainingSession&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42d3bc86be52a8736e6d7aa317815110401307ef" translate="yes" xml:space="preserve">
          <source>This class is heavily overloaded:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22170de053f3420fc08a96260e613e3fa8486018" translate="yes" xml:space="preserve">
          <source>This class is meant to be used with dynamic iteration primitives such as &lt;code&gt;while_loop&lt;/code&gt; and &lt;code&gt;map_fn&lt;/code&gt;. It supports gradient back-propagation via special &quot;flow&quot; control flow dependencies.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="121c6274230c0f7d1b806b54f9d580ba8e2d228a" translate="yes" xml:space="preserve">
          <source>This class merges the output of multiple &lt;code&gt;Head&lt;/code&gt; objects. Specifically:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7cd980fdc04e7da43f8abb3e3be6ffdf25b7685d" translate="yes" xml:space="preserve">
          <source>This class performs a model export everytime the new model is better than any existing model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="391aed511e9154256ca236f23d6877cb0f9d37db" translate="yes" xml:space="preserve">
          <source>This class performs a single export at the end of training.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b83c252083ca3edbed9d1168a0ed9755da22c25f" translate="yes" xml:space="preserve">
          <source>This class performs a union given two or more existing ClusterResolvers. It merges the underlying ClusterResolvers, and returns one unified ClusterSpec when cluster_spec is called. The details of the merge function is documented in the cluster_spec function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="631d6a58bb184da4fc166c1a1dbaf36a0e45d9c8" translate="yes" xml:space="preserve">
          <source>This class performs the softmax operation for you, so inputs should be e.g. linear projections of outputs by an LSTM.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d2810b13f641fd9987b2ce8b38f939b3369079e3" translate="yes" xml:space="preserve">
          <source>This class processes one step within the whole time sequence input, whereas &lt;code&gt;tf.keras.layer.GRU&lt;/code&gt; processes the whole sequence.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a1d5a35670a2cc4cfac2401e85535a1c8acffb8" translate="yes" xml:space="preserve">
          <source>This class processes one step within the whole time sequence input, whereas &lt;code&gt;tf.keras.layer.LSTM&lt;/code&gt; processes the whole sequence.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85aecc3ec7757c0f4286579ce3ab5119031dd197" translate="yes" xml:space="preserve">
          <source>This class processes one step within the whole time sequence input, whereas &lt;code&gt;tf.keras.layer.SimpleRNN&lt;/code&gt; processes the whole sequence.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2cd1ab18438d633073fa2c3300d94950ce386b5" translate="yes" xml:space="preserve">
          <source>This class regularly exports the serving graph and checkpoints.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44829f1dbd5fa6ec4b7c405e96fb0cb7473fedd5" translate="yes" xml:space="preserve">
          <source>This class specifies the configurations for an &lt;code&gt;Estimator&lt;/code&gt; run.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="349b70f1528f5d483fc48af054599841c5669982" translate="yes" xml:space="preserve">
          <source>This class takes in a sequence of data-points gathered at equal intervals, along with time series parameters such as stride, length of history, etc., to produce batches for training/validation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45bad515b0c24287a7857bc49730849b980995da" translate="yes" xml:space="preserve">
          <source>This classifier ignores feature values and will learn to predict the average value of each label. For single-label problems, this will predict the probability distribution of the classes as seen in the labels. For multi-label problems, this will predict the fraction of examples that are positive for each class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1203741987f9f009e5e86b580142b5f23085017c" translate="yes" xml:space="preserve">
          <source>This computes the internal data stats related to the data-dependent transformations, based on an array of sample data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="38619017d4eec4e645fa2f91a523c788477cd3cb" translate="yes" xml:space="preserve">
          <source>This condition holds if for every pair of (possibly broadcast) elements &lt;code&gt;x[i]&lt;/code&gt;, &lt;code&gt;y[i]&lt;/code&gt;, we have</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="462aa0ba39503de33a78d3a4ed1c72d0eb71fa77" translate="yes" xml:space="preserve">
          <source>This condition holds if for every pair of (possibly broadcast) elements &lt;code&gt;x[i]&lt;/code&gt;, &lt;code&gt;y[i]&lt;/code&gt;, we have &lt;code&gt;x[i] != y[i]&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e25b6ae62c3ffc2ef4953d25e9698856f0a3b77a" translate="yes" xml:space="preserve">
          <source>This condition holds if for every pair of (possibly broadcast) elements &lt;code&gt;x[i]&lt;/code&gt;, &lt;code&gt;y[i]&lt;/code&gt;, we have &lt;code&gt;x[i] &amp;gt; y[i]&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad801aabcdd92fbf94ec233e05dfeadd68d460b3" translate="yes" xml:space="preserve">
          <source>This condition holds if for every pair of (possibly broadcast) elements &lt;code&gt;x[i]&lt;/code&gt;, &lt;code&gt;y[i]&lt;/code&gt;, we have &lt;code&gt;x[i] &amp;gt;= y[i]&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="512ed74cbec98906ad1a4d1866d9484e4d2ad504" translate="yes" xml:space="preserve">
          <source>This condition holds if for every pair of (possibly broadcast) elements &lt;code&gt;x[i]&lt;/code&gt;, &lt;code&gt;y[i]&lt;/code&gt;, we have &lt;code&gt;x[i] &amp;lt; y[i]&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81434868d39629bbc35c170df36a994a92015b46" translate="yes" xml:space="preserve">
          <source>This condition holds if for every pair of (possibly broadcast) elements &lt;code&gt;x[i]&lt;/code&gt;, &lt;code&gt;y[i]&lt;/code&gt;, we have &lt;code&gt;x[i] &amp;lt;= y[i]&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93ee9d4d5438fdc7dae43626edce4fb33cefa73b" translate="yes" xml:space="preserve">
          <source>This condition holds if for every pair of (possibly broadcast) elements &lt;code&gt;x[i]&lt;/code&gt;, &lt;code&gt;y[i]&lt;/code&gt;, we have &lt;code&gt;x[i] == y[i]&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1107421e32062956898c5a66e60e49bd95c7ed45" translate="yes" xml:space="preserve">
          <source>This constraint can be applied to any &lt;code&gt;Conv2D&lt;/code&gt; layer version, including &lt;code&gt;Conv2DTranspose&lt;/code&gt; and &lt;code&gt;SeparableConv2D&lt;/code&gt;, and with either &lt;code&gt;&quot;channels_last&quot;&lt;/code&gt; or &lt;code&gt;&quot;channels_first&quot;&lt;/code&gt; data format. The method assumes the weight tensor is of shape &lt;code&gt;(rows, cols, input_depth, output_depth)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5e140df8d55549c17df75f17763f4c88b908cd9" translate="yes" xml:space="preserve">
          <source>This constructor creates both a &lt;code&gt;variable&lt;/code&gt; Op and an &lt;code&gt;assign&lt;/code&gt; Op to set the variable to its initial value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0853b5f1d96849514913dcea4715e205fe7e9be9" translate="yes" xml:space="preserve">
          <source>This constructor is private -- please use one of the following ops to build &lt;code&gt;RaggedTensor&lt;/code&gt;s:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa9706d28ea0bf76646310aace552e8f79dee4fd" translate="yes" xml:space="preserve">
          <source>This constructor only applies if the algorithm is a counter-based algorithm. See method &lt;code&gt;key&lt;/code&gt; for the meaning of &quot;key&quot; and &quot;counter&quot;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="abd1f92b92080f9534139ad50b5578924ec063f6" translate="yes" xml:space="preserve">
          <source>This contains most of the synchronization implementation and also wraps the apply_gradients() from the real optimizer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dac62a0f0dbdc3c6a9231598871c80876ac21c3b" translate="yes" xml:space="preserve">
          <source>This context handler simplifies the exception handling. Use it as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92e2326270a118c9b1678bb4952443a859f4c559" translate="yes" xml:space="preserve">
          <source>This context manager can be used to override the gradient function that will be used for ops within the scope of the context.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="790e879037e30f5e5546c1be4c06bf6476dca2dd" translate="yes" xml:space="preserve">
          <source>This context manager captures all writes to a given stream inside of a &lt;code&gt;CapturedWrites&lt;/code&gt; object. When this context manager is created, it yields the &lt;code&gt;CapturedWrites&lt;/code&gt; object. The captured contents can be accessed by calling &lt;code&gt;.contents()&lt;/code&gt; on the &lt;code&gt;CapturedWrites&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41c375fab79db6dca2a3052fe26a3bb1f0a48127" translate="yes" xml:space="preserve">
          <source>This context manager creates and automatically recovers a session. It optionally starts the standard services that handle checkpoints and summaries. It monitors exceptions raised from the &lt;code&gt;with&lt;/code&gt; block or from the services and stops the supervisor as needed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1dfc6367da2112861ba5a19ab74136e2bfa59bd7" translate="yes" xml:space="preserve">
          <source>This context manager pushes a name scope, which will make the name of all operations added within it have a prefix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cadf2e29564a69f6d9e78e18436e128fc8603a09" translate="yes" xml:space="preserve">
          <source>This context manager validates that the (optional) &lt;code&gt;values&lt;/code&gt; are from the same graph, ensures that graph is the default graph, and pushes a name scope and a variable scope.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9fe8f8972b4d68b13082f0d96ff8de4b430acf3b" translate="yes" xml:space="preserve">
          <source>This context manager validates that the given &lt;code&gt;values&lt;/code&gt; are from the same graph, makes that graph the default graph, and pushes a name scope in that graph (see &lt;a href=&quot;../../../../graph#name_scope&quot;&gt;&lt;code&gt;tf.Graph.name_scope&lt;/code&gt;&lt;/a&gt; for more details on that).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8bc9405ecd91e0aa7f29a32f0b001ccc17dfce1e" translate="yes" xml:space="preserve">
          <source>This convenience method requires a session where the graph containing this variable has been launched. If no session is passed, the default session is used. See &lt;a href=&quot;compat/v1/session&quot;&gt;&lt;code&gt;tf.compat.v1.Session&lt;/code&gt;&lt;/a&gt; for more information on launching a graph and on sessions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69597b8afaecf26bf780e17dbe96db99a176f9db" translate="yes" xml:space="preserve">
          <source>This convenience method requires a session where the graph containing this variable has been launched. If no session is passed, the default session is used. See &lt;a href=&quot;session&quot;&gt;&lt;code&gt;tf.compat.v1.Session&lt;/code&gt;&lt;/a&gt; for more information on launching a graph and on sessions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="049bd1098346a19ff3fab486ff6a7b47c3fd7819" translate="yes" xml:space="preserve">
          <source>This creates a &lt;code&gt;LinearOperator&lt;/code&gt; of the form &lt;code&gt;A = L + U D V^H&lt;/code&gt;, with &lt;code&gt;L&lt;/code&gt; a &lt;code&gt;LinearOperator&lt;/code&gt;, &lt;code&gt;U, V&lt;/code&gt; both [batch] matrices, and &lt;code&gt;D&lt;/code&gt; a [batch] diagonal matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="20e733fb47526c81e38b7224224a075d45127bb0" translate="yes" xml:space="preserve">
          <source>This creates a named directory on disk that is isolated to this test, and will be properly cleaned up by the test. This avoids several pitfalls of creating temporary directories for test purposes, as well as makes it easier to setup directories and verify their contents.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="75714df8d9550841d3043fbd62dad500169275e6" translate="yes" xml:space="preserve">
          <source>This creates a named file on disk that is isolated to this test, and will be properly cleaned up by the test. This avoids several pitfalls of creating temporary files for test purposes, as well as makes it easier to setup files, their data, read them back, and inspect them when a test fails.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42646afd71d42ede988399add1a74a5f6b4c93c2" translate="yes" xml:space="preserve">
          <source>This creates a tuple of tensors with the same values as the &lt;code&gt;tensors&lt;/code&gt; argument, except that the value of each tensor is only returned after the values of all tensors have been computed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9dc1c4d1a56233dc4a8e886da12f3f0a495623a" translate="yes" xml:space="preserve">
          <source>This dataset fills a buffer with &lt;code&gt;buffer_size&lt;/code&gt; elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="97fff19a84ee410a08999fab8be4d199f9066fdc" translate="yes" xml:space="preserve">
          <source>This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="38966f21ae4fd315566bc2e9e41902f2d11ad1fd" translate="yes" xml:space="preserve">
          <source>This decorator allows fine grained control over the gradients of a sequence for operations. This may be useful for multiple reasons, including providing a more efficient or numerically stable gradient for a sequence of operations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="507c4c48f689d10862643a9c65b8ad1497b61251" translate="yes" xml:space="preserve">
          <source>This decorator injects the decorated class or function into the Keras custom object dictionary, so that it can be serialized and deserialized without needing an entry in the user-provided custom object dict. It also injects a function that Keras will call to get the object's serializable string key.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99ba7a22516d6648f4353818a77d3239c06fe661" translate="yes" xml:space="preserve">
          <source>This decorator is only used when defining a new op type. For an op with &lt;code&gt;m&lt;/code&gt; inputs and &lt;code&gt;n&lt;/code&gt; outputs, the gradient function is a function that takes the original &lt;code&gt;Operation&lt;/code&gt; and &lt;code&gt;n&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt; objects (representing the gradients with respect to each output of the op), and returns &lt;code&gt;m&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt; objects (representing the partial gradients with respect to each input of the op).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31c4ff795b26f67c26110ef2a390028f451d7769" translate="yes" xml:space="preserve">
          <source>This defines the skeleton for all implementations of ClusterResolvers. ClusterResolvers are a way for TensorFlow to communicate with various cluster management systems (e.g. GCE, AWS, etc...).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0df31e40439c17db3321616e25b535cdb0316252" translate="yes" xml:space="preserve">
          <source>This definition of cell differs from the definition used in the literature. In the literature, 'cell' refers to an object with a single scalar output. This definition refers to a horizontal array of such units.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0725a72c209eb53df1440f5a39a25dbf2ab21a14" translate="yes" xml:space="preserve">
          <source>This distribution has parameters: degree of freedom &lt;code&gt;df&lt;/code&gt;, location &lt;code&gt;loc&lt;/code&gt;, and &lt;code&gt;scale&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11a222f9738197dadd6eb7c453e22ae24d0f3567" translate="yes" xml:space="preserve">
          <source>This does not close the session.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e3bddf2a09aebf2b7d2bb27cde671e8328a7dc27" translate="yes" xml:space="preserve">
          <source>This does not undo the effects of loss scaling. Any optimizers wrapped with a LossScaleOptimizer will continue to do loss scaling, although this loss scaling will no longer be useful if the optimizer is used in new Sessions, as the graph rewrite no longer converts the graph to use float16.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02c54112c729e547b6e3e03e55cab5ced9766b9c" translate="yes" xml:space="preserve">
          <source>This does not undo the effects of loss scaling. Any optimizers wrapped with a LossScaleOptimizer will continue to do loss scaling, although this loss scaling will no longer be useful, as the graph rewrite no longer converts tf.functions to use float16.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3bfa5300d7e625917e03fb074107ed12abbdcbbe" translate="yes" xml:space="preserve">
          <source>This eliminates the overhead of &lt;code&gt;k-1&lt;/code&gt; calls to &lt;code&gt;space_to_batch_nd&lt;/code&gt; and &lt;code&gt;batch_to_space_nd&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c1542adb4efbb08745cf8660782eb015be7e9a8" translate="yes" xml:space="preserve">
          <source>This enables the new behavior.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b70309885ebd2feded30214b2bff057bfcef522b" translate="yes" xml:space="preserve">
          <source>This enables variables to be read as bfloat16 type when using get_variable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b4ec5604b14a4a73674990062864327b53e189b" translate="yes" xml:space="preserve">
          <source>This enumeration represents optional conversion options.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26d8315a3de6d7adfdf3bbd23a3ef83e0e870212" translate="yes" xml:space="preserve">
          <source>This estimator ignores feature values and will learn to predict the average value of each label. E.g. for single-label classification problems, this will predict the probability distribution of the classes as seen in the labels. For multi-label classification problems, it will predict the ratio of examples that contain each class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f03708fe58a6d112475001cb64ef56af4e257ac" translate="yes" xml:space="preserve">
          <source>This example instantiates a TextVectorization layer that lowercases text, splits on whitespace, strips punctuation, and outputs integer vocab indices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b8a0001b21c834c94ffc58110b29147780ed1bc" translate="yes" xml:space="preserve">
          <source>This exception is most commonly raised when running an operation that reads a &lt;a href=&quot;../variable&quot;&gt;&lt;code&gt;tf.Variable&lt;/code&gt;&lt;/a&gt; before it has been initialized.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c5a8c57c6d5ff0bef6c24b2e2bd3f673907515f" translate="yes" xml:space="preserve">
          <source>This exception is not currently used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f99d6b607eb42d98eb52dda612321114e3e91d9d" translate="yes" xml:space="preserve">
          <source>This exception is raised in &quot;end-of-file&quot; conditions, such as when a &lt;code&gt;tf.QueueBase.dequeue&lt;/code&gt; operation is blocked on an empty queue, and a &lt;code&gt;tf.QueueBase.close&lt;/code&gt; operation executes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78d71c0636a2aa1fd087c8298d2d2e031276f9b2" translate="yes" xml:space="preserve">
          <source>This exception is raised when some invariant expected by the runtime has been broken. Catching this exception is not recommended.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7057b737e9dfe1ba0218e510ec51533edf1eab95" translate="yes" xml:space="preserve">
          <source>This exists primarily to support the definition of type-specific summary ops like scalar() and image(), and is not intended for direct use unless defining a new type-specific summary op.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7cea24b85385b2c626baad7c000ac087d0ac17d" translate="yes" xml:space="preserve">
          <source>This file includes functions and constants from core (model_utils) and export.py</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ed5997b223ee14f22d4d6301133fe48a3becd70" translate="yes" xml:space="preserve">
          <source>This flag will have a value of None, True or False. None is possible if default=None and the user does not specify the flag on the command line.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="edc3e42fb098d2f82cf52f313f0a2bbc39d59acf" translate="yes" xml:space="preserve">
          <source>This foldl operator repeatedly applies the callable &lt;code&gt;fn&lt;/code&gt; to a sequence of elements from first to last. The elements are made of the tensors unpacked from &lt;code&gt;elems&lt;/code&gt; on dimension 0. The callable fn takes two tensors as arguments. The first argument is the accumulated value computed from the preceding invocation of fn, and the second is the value at the current position of &lt;code&gt;elems&lt;/code&gt;. If &lt;code&gt;initializer&lt;/code&gt; is None, &lt;code&gt;elems&lt;/code&gt; must contain at least one element, and its first element is used as the initializer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c9efc4e60690682d054976cd526187b0ec74f90" translate="yes" xml:space="preserve">
          <source>This foldr operator repeatedly applies the callable &lt;code&gt;fn&lt;/code&gt; to a sequence of elements from last to first. The elements are made of the tensors unpacked from &lt;code&gt;elems&lt;/code&gt;. The callable fn takes two tensors as arguments. The first argument is the accumulated value computed from the preceding invocation of fn, and the second is the value at the current position of &lt;code&gt;elems&lt;/code&gt;. If &lt;code&gt;initializer&lt;/code&gt; is None, &lt;code&gt;elems&lt;/code&gt; must contain at least one element, and its first element is used as the initializer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e47bc5be6551c40b285b99814572b5e2b249a8b5" translate="yes" xml:space="preserve">
          <source>This function adds operations to the current session. To compute the error using a particular device, such as a GPU, use the standard methods for setting a device (e.g. using with sess.graph.device() or setting a device function in the session constructor).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b1c988623a891fbccad5b0fbb3db51dcb83f9a90" translate="yes" xml:space="preserve">
          <source>This function adds the following to the current &lt;code&gt;Graph&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7391056d9f36003196c437fbb04f6186eed40dde" translate="yes" xml:space="preserve">
          <source>This function allows expressing computations in a TensorFlow graph as Python functions. In particular, it wraps a Python function &lt;code&gt;func&lt;/code&gt; in a once-differentiable TensorFlow operation that executes it with eager execution enabled. As a consequence, &lt;a href=&quot;py_function&quot;&gt;&lt;code&gt;tf.py_function&lt;/code&gt;&lt;/a&gt; makes it possible to express control flow using Python constructs (&lt;code&gt;if&lt;/code&gt;, &lt;code&gt;while&lt;/code&gt;, &lt;code&gt;for&lt;/code&gt;, etc.), instead of TensorFlow control flow constructs (&lt;a href=&quot;cond&quot;&gt;&lt;code&gt;tf.cond&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;while_loop&quot;&gt;&lt;code&gt;tf.while_loop&lt;/code&gt;&lt;/a&gt;). For example, you might use &lt;a href=&quot;py_function&quot;&gt;&lt;code&gt;tf.py_function&lt;/code&gt;&lt;/a&gt; to implement the log huber function:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa49b9c5fb3cc27a08b15ff4c8fbc286bffc2bb3" translate="yes" xml:space="preserve">
          <source>This function allows replacing a function wrapped by &lt;code&gt;decorator_func&lt;/code&gt;, assuming the decorator that wraps the function is written as described below.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1af1fe4443bcb9edf1df9ce5ca5cc1b073069a3" translate="yes" xml:space="preserve">
          <source>This function also returns a &lt;code&gt;should_apply_gradients&lt;/code&gt; bool. If False, gradients should not be applied to the variables that step, as nonfinite gradients were found, and the loss scale has been be updated to reduce the chance of finding nonfinite gradients in the next step. Some loss scale classes will always return True, as they cannot adjust themselves in response to nonfinite gradients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a970dd1ee86d1ed9e7a0f4a5fcb3e14f9903fdf2" translate="yes" xml:space="preserve">
          <source>This function assumes that &lt;code&gt;img1&lt;/code&gt; and &lt;code&gt;img2&lt;/code&gt; are image batches, i.e. the last three dimensions are [height, width, channels].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="05e4418b517bd473a485966680131f4c4c444d83" translate="yes" xml:space="preserve">
          <source>This function attempts to partially evaluate the given tensor, and returns its value as a numpy ndarray if this succeeds.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf86c424c057435bb07a468ed262a1bc87e5ff48" translate="yes" xml:space="preserve">
          <source>This function can be called at the beginning of the program (before &lt;code&gt;Tensors&lt;/code&gt;, &lt;code&gt;Graphs&lt;/code&gt; or other structures have been created, and before devices have been initialized. It switches all global behaviors that are different between TensorFlow 1.x and 2.x to behave as intended for 1.x.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="10057d11c28687d408d56022580e8ea6fe2a793b" translate="yes" xml:space="preserve">
          <source>This function can be called at the beginning of the program (before &lt;code&gt;Tensors&lt;/code&gt;, &lt;code&gt;Graphs&lt;/code&gt; or other structures have been created, and before devices have been initialized. It switches all global behaviors that are different between TensorFlow 1.x and 2.x to behave as intended for 2.x.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b42e569daa67e0a1534f20b78d521ba43e71fa0e" translate="yes" xml:space="preserve">
          <source>This function can be used to calculate a suitable paddings argument for use with space_to_batch_nd and batch_to_space_nd.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c744362e6d55098ce01656aff7353b51e78a3f8e" translate="yes" xml:space="preserve">
          <source>This function can be useful when composing a new operation in Python (such as &lt;code&gt;my_func&lt;/code&gt; in the example above). All standard Python op constructors apply this function to each of their Tensor-valued inputs, which allows those ops to accept numpy arrays, Python lists, and scalars in addition to &lt;code&gt;Tensor&lt;/code&gt; objects.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="217d0b2441a5002feaf4d45bd68846f61852099e" translate="yes" xml:space="preserve">
          <source>This function can compute several different vector norms (the 1-norm, the Euclidean or 2-norm, the inf-norm, and in general the p-norm for p &amp;gt; 0) and matrix norms (Frobenius, 1-norm, 2-norm and inf-norm).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ea434ce2125739c9300b55f884e2363f68836bb" translate="yes" xml:space="preserve">
          <source>This function can only be called before any Graphs, Ops, or Tensors have been created. It can be used at the beginning of the program for complex migration projects from TensorFlow 1.x to 2.x.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="588718f27309c0577e10569b211613b30b1126d6" translate="yes" xml:space="preserve">
          <source>This function casts the input to &lt;code&gt;dtype&lt;/code&gt; without applying any scaling. If there is a danger that values would over or underflow in the cast, this op applies the appropriate clamping before the cast.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="09decee67bf539dd150a70f2c58935c601acb068" translate="yes" xml:space="preserve">
          <source>This function computes the exponential of every element in the input tensor. i.e. &lt;code&gt;exp(x)&lt;/code&gt; or &lt;code&gt;e^(x)&lt;/code&gt;, where &lt;code&gt;x&lt;/code&gt; is the input tensor. &lt;code&gt;e&lt;/code&gt; denotes Euler's number and is approximately equal to 2.718281. Output is positive for any real input.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c0a191a50e5e9af59c01d8569f306fc43d930b1" translate="yes" xml:space="preserve">
          <source>This function computes the matrix logarithm using the Schur-Parlett algorithm. Details of the algorithm can be found in Section 11.6.2 of: Nicholas J. Higham, Functions of Matrices: Theory and Computation, SIAM 2008. ISBN 978-0-898716-46-7.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be1f544dd8e61bda50e9ba11ee30c7fa32e6f8ea" translate="yes" xml:space="preserve">
          <source>This function converts Python objects of various types to &lt;code&gt;Tensor&lt;/code&gt; objects. It accepts &lt;code&gt;Tensor&lt;/code&gt; objects, numpy arrays, Python lists, and Python scalars. For example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e487da37eb7d13adb8d956491efcafe1ed6b6abd" translate="yes" xml:space="preserve">
          <source>This function creates a new Generator object (and the Variable object within), which does not work well with tf.function because (1) tf.function puts restrictions on Variable creation thus reset_global_generator can't be freely used inside tf.function; (2) redirecting a global variable to a new object is problematic with tf.function because the old object may be captured by a 'tf.function'ed function and still be used by it. A 'tf.function'ed function only keeps weak references to variables, so deleting a variable and then calling that function again may raise an error, as demonstrated by random_test.py/RandomTest.testResetGlobalGeneratorBadWithDefun .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bd8be377fdf00edb4e98ffd4a0d17d3f66c21a2" translate="yes" xml:space="preserve">
          <source>This function divides &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, forcing Python 2 semantics. That is, if &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are both integers then the result will be an integer. This is in contrast to Python 3, where division with &lt;code&gt;/&lt;/code&gt; is always a float while division with &lt;code&gt;//&lt;/code&gt; is always an integer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d24aa00a66ce40fc5a2092349ae7643f400f331a" translate="yes" xml:space="preserve">
          <source>This function enables you to use a &lt;a href=&quot;../dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; in a stateless &quot;tensor-in tensor-out&quot; expression, without creating a &lt;a href=&quot;../../compat/v1/data/iterator&quot;&gt;&lt;code&gt;tf.compat.v1.data.Iterator&lt;/code&gt;&lt;/a&gt;. This can be useful when your preprocessing transformations are expressed as a &lt;code&gt;Dataset&lt;/code&gt;, and you want to use the transformation at serving time. For example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af17991dd15e2b7fa7774cc8e8187690043ab3dd" translate="yes" xml:space="preserve">
          <source>This function exists only for backwards compatibility purposes; new code should use &lt;code&gt;__floordiv__&lt;/code&gt; via the syntax &lt;code&gt;x // y&lt;/code&gt;. Using &lt;code&gt;x // y&lt;/code&gt; communicates clearly that the result rounds down, and is forward compatible to Python 3.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a2f6439e571c218e4f990c2f6f9f31b011962aa" translate="yes" xml:space="preserve">
          <source>This function exists only to have a better error message. Instead of: &lt;code&gt;TypeError: unsupported operand type(s) for /: 'Dimension' and 'int'&lt;/code&gt;, this function will explicitly call for usage of &lt;code&gt;//&lt;/code&gt; instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6bf09410a9b47558afad7eb9fc6a9929885c6dd9" translate="yes" xml:space="preserve">
          <source>This function exists only to have a better error message. Instead of: &lt;code&gt;TypeError: unsupported operand type(s) for /: 'int' and 'Dimension'&lt;/code&gt;, this function will explicitly call for usage of &lt;code&gt;//&lt;/code&gt; instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7046a2ab2d223422699f2f1db88f8efbcff064ed" translate="yes" xml:space="preserve">
          <source>This function exports the graph, saver, and collection objects into &lt;code&gt;MetaGraphDef&lt;/code&gt; protocol buffer with the intention of it being imported at a later time or location to restart training, run inference, or be a subgraph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6a27fb60b44269b202e3c908e7cb802642c2ac28" translate="yes" xml:space="preserve">
          <source>This function forces Python 3 division operator semantics where all integer arguments are cast to floating types first. This op is generated by normal &lt;code&gt;x / y&lt;/code&gt; division in Python 3 and in Python 2.7 with &lt;code&gt;from __future__ import division&lt;/code&gt;. If you want integer division that rounds down, use &lt;code&gt;x // y&lt;/code&gt; or &lt;code&gt;tf.math.floordiv&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b139bf71180b25aa620a27f39c22f066fe31fcf7" translate="yes" xml:space="preserve">
          <source>This function generalizes the &lt;a href=&quot;non_max_suppression&quot;&gt;&lt;code&gt;tf.image.non_max_suppression&lt;/code&gt;&lt;/a&gt; op by also supporting a Soft-NMS (with Gaussian weighting) mode (c.f. Bodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score of other overlapping boxes instead of directly causing them to be pruned. Consequently, in contrast to &lt;a href=&quot;non_max_suppression&quot;&gt;&lt;code&gt;tf.image.non_max_suppression&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;tf.image.non_max_suppression_v2&lt;/code&gt; returns the new scores of each input box in the second output, &lt;code&gt;selected_scores&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f78cfdbddd70d6ebd322125da902964ca2c0221" translate="yes" xml:space="preserve">
          <source>This function generates a weighted sum based on output dimension &lt;code&gt;units&lt;/code&gt;. Weighted sum refers to logits in classification problems. It refers to the prediction itself for linear regression problems.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7164aa4227947230adf26e333ecf50d207f9d4e" translate="yes" xml:space="preserve">
          <source>This function ignores flags whose value is None. Each flag assignment is separated by a newline.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c81aeff7442b1d7f71d30de1b5da89dfe187de7" translate="yes" xml:space="preserve">
          <source>This function in addition also allows assignment to a sliced range. This is similar to &lt;code&gt;__setitem__&lt;/code&gt; functionality in Python. However, the syntax is different so that the user can capture the assignment operation for grouping or passing to &lt;code&gt;sess.run()&lt;/code&gt;. For example,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f477dc6f411c004d0b07e1741886385a8bad1741" translate="yes" xml:space="preserve">
          <source>This function is a more primitive version of &lt;code&gt;dynamic_rnn&lt;/code&gt; that provides more direct access to the inputs each iteration. It also provides more control over when to start and finish reading the sequence, and what to emit for the output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b35cb926e7e56c5e0bb4984bfdd19c6c71696446" translate="yes" xml:space="preserve">
          <source>This function is a simpler wrapper around the more general &lt;a href=&quot;convolution&quot;&gt;&lt;code&gt;tf.nn.convolution&lt;/code&gt;&lt;/a&gt;, and exists only for backwards compatibility. You can use &lt;a href=&quot;convolution&quot;&gt;&lt;code&gt;tf.nn.convolution&lt;/code&gt;&lt;/a&gt; to perform 1-D, 2-D, or 3-D atrous convolution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1fde94a598e8772eaea3cc47dd741a3aa23b5c85" translate="yes" xml:space="preserve">
          <source>This function is analogous to &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html&quot;&gt;&lt;code&gt;numpy.linalg.pinv&lt;/code&gt;&lt;/a&gt;. It differs only in default value of &lt;code&gt;rcond&lt;/code&gt;. In &lt;code&gt;numpy.linalg.pinv&lt;/code&gt;, the default &lt;code&gt;rcond&lt;/code&gt; is &lt;code&gt;1e-15&lt;/code&gt;. Here the default is &lt;code&gt;10. * max(num_rows, num_cols) * np.finfo(dtype).eps&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c82b17de9fe920f72f7a5369e74c77b8a5f75d81" translate="yes" xml:space="preserve">
          <source>This function is based on the standard SSIM implementation from: Wang, Z., Bovik, A. C., Sheikh, H. R., &amp;amp; Simoncelli, E. P. (2004). Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ecacc04269db8f28f91780c2b346f24c2d101704" translate="yes" xml:space="preserve">
          <source>This function is called between epochs/steps, when a metric is evaluated during training.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a75cbe1aa934dcb50e2537aae264b28dbab09250" translate="yes" xml:space="preserve">
          <source>This function is called by FLAGS(argv). It scans the input list for a flag that looks like: --flagfile=</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e57b5dff2b7e230f6a359d3b07cb810778cc3047" translate="yes" xml:space="preserve">
          <source>This function is called in the main TensorFlow &lt;code&gt;__init__.py&lt;/code&gt; file, user should not need to call it, except during complex migrations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0831cfa041ba84a9c1bb953a8575728ee9dbe8bf" translate="yes" xml:space="preserve">
          <source>This function is faster and numerically stabler than &lt;code&gt;bessel_i0(x)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f2e499db54e57708a372078c16af5df37036099" translate="yes" xml:space="preserve">
          <source>This function is faster and numerically stabler than &lt;code&gt;bessel_i1(x)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7e374cf0f52d98689ddb22ec7b9e2a428cd4956" translate="yes" xml:space="preserve">
          <source>This function is implemented using a queue. A &lt;code&gt;QueueRunner&lt;/code&gt; for the queue is added to the current &lt;code&gt;Graph&lt;/code&gt;'s &lt;code&gt;QUEUE_RUNNER&lt;/code&gt; collection.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c4c3026638168daaa4ab1a3c114680f01d9d246" translate="yes" xml:space="preserve">
          <source>This function is more numerically stable than log(sum(exp(input))). It avoids overflows caused by taking the exp of large inputs and underflows caused by taking the log of small inputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80c0f02cfab4a0ee1574d2a1145f5fca28f12b11" translate="yes" xml:space="preserve">
          <source>This function is only available with the TensorFlow backend for the time being.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11a7ac7f028baf06c483242a158777b17ee8b353" translate="yes" xml:space="preserve">
          <source>This function is only used when defining a new op type. It may be used for ops such as &lt;a href=&quot;size&quot;&gt;&lt;code&gt;tf.size()&lt;/code&gt;&lt;/a&gt; that are not differentiable. For example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="577a32736f43471cd2cd1266738343fb80596d48" translate="yes" xml:space="preserve">
          <source>This function is the canonical way to get/validate an object of one of the allowed types from an external argument reference in the Session API.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="691cc7f8251e205cf948be38afb6e034f26a2e72" translate="yes" xml:space="preserve">
          <source>This function is used to perform parallel lookups on the list of tensors in &lt;code&gt;params&lt;/code&gt;. It is a generalization of &lt;a href=&quot;../../../gather&quot;&gt;&lt;code&gt;tf.gather&lt;/code&gt;&lt;/a&gt;, where &lt;code&gt;params&lt;/code&gt; is interpreted as a partitioning of a large embedding tensor. &lt;code&gt;params&lt;/code&gt; may be a &lt;code&gt;PartitionedVariable&lt;/code&gt; as returned by using &lt;a href=&quot;../get_variable&quot;&gt;&lt;code&gt;tf.compat.v1.get_variable()&lt;/code&gt;&lt;/a&gt; with a partitioner.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="237e26d699d02ce05b0e4079a36fba68db8a8789" translate="yes" xml:space="preserve">
          <source>This function is used to perform parallel lookups on the list of tensors in &lt;code&gt;params&lt;/code&gt;. It is a generalization of &lt;a href=&quot;../gather&quot;&gt;&lt;code&gt;tf.gather&lt;/code&gt;&lt;/a&gt;, where &lt;code&gt;params&lt;/code&gt; is interpreted as a partitioning of a large embedding tensor. &lt;code&gt;params&lt;/code&gt; may be a &lt;code&gt;PartitionedVariable&lt;/code&gt; as returned by using &lt;a href=&quot;../compat/v1/get_variable&quot;&gt;&lt;code&gt;tf.compat.v1.get_variable()&lt;/code&gt;&lt;/a&gt; with a partitioner.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f577da20d8fc0c2379f7e51688987afe782aa9c2" translate="yes" xml:space="preserve">
          <source>This function is useful for unit testing. A unit test can test using the mixed precision graph rewrite, then disable it so future unit tests continue using float32.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d61d06f477fe487d45a847bca66c249257c10fe4" translate="yes" xml:space="preserve">
          <source>This function is useful for unit testing. A unit tests can test using the mixed precision graph rewrite, then disable it so future unit tests continue using float32. If this is done, unit tests should not share a single session, as &lt;code&gt;enable_mixed_precision_graph_rewrite&lt;/code&gt; and &lt;code&gt;disable_mixed_precision_graph_rewrite&lt;/code&gt; have no effect on existing sessions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d4e537bea1c76cb2cbcdaeafa3c9baedcb0e335e" translate="yes" xml:space="preserve">
          <source>This function may be used in the &lt;code&gt;options&lt;/code&gt; argument in functions that save a SavedModel (&lt;a href=&quot;save&quot;&gt;&lt;code&gt;tf.saved_model.save&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../keras/models/save_model&quot;&gt;&lt;code&gt;tf.keras.models.save_model&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="23d4449e48af4e4a9efae86d49cf037eac266df3" translate="yes" xml:space="preserve">
          <source>This function only gets the device policy for the current thread. Any subsequently started thread will again use the default policy.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69c8cb582945069c7d9496b069e6902700fc7e3d" translate="yes" xml:space="preserve">
          <source>This function only sets the device policy for the current thread. Any subsequently started thread will again use the default policy.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="128baaf391e006514e7e2e442ed3dbb64e14a3c9" translate="yes" xml:space="preserve">
          <source>This function performs the equivalent of</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fac08ba1f46139129a2f42247910a712f1085f49" translate="yes" xml:space="preserve">
          <source>This function prefixes the name with the current variable scope and performs reuse checks. See the &lt;a href=&quot;https://tensorflow.org/guide/variables&quot;&gt;Variable Scope How To&lt;/a&gt; for an extensive description of how reusing works. Here is a basic example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85bc4aa7e8b39b071c3a83bcd2bcb628b96948b5" translate="yes" xml:space="preserve">
          <source>This function produces signatures intended for use with the TensorFlow Serving Classify API (tensorflow_serving/apis/prediction_service.proto), and so constrains the input and output types to those allowed by TensorFlow Serving.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="795898cb16fc0e553ecaa87cbe16e8417edef412" translate="yes" xml:space="preserve">
          <source>This function produces signatures intended for use with the TensorFlow Serving Predict API (tensorflow_serving/apis/prediction_service.proto). This API imposes no constraints on the input and output types.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0bd91f24173f9c6c06caccaa86fb2987286936b9" translate="yes" xml:space="preserve">
          <source>This function produces signatures intended for use with the TensorFlow Serving Regress API (tensorflow_serving/apis/prediction_service.proto), and so constrains the input and output types to those allowed by TensorFlow Serving.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="da2207550b053f2e41ed03fce80a539629a65e2e" translate="yes" xml:space="preserve">
          <source>This function provides a way to import a serialized TensorFlow &lt;a href=&quot;https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto&quot;&gt;&lt;code&gt;GraphDef&lt;/code&gt;&lt;/a&gt; protocol buffer, and extract individual objects in the &lt;code&gt;GraphDef&lt;/code&gt; as &lt;a href=&quot;../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../operation&quot;&gt;&lt;code&gt;tf.Operation&lt;/code&gt;&lt;/a&gt; objects. Once extracted, these objects are placed into the current default &lt;code&gt;Graph&lt;/code&gt;. See &lt;a href=&quot;../graph#as_graph_def&quot;&gt;&lt;code&gt;tf.Graph.as_graph_def&lt;/code&gt;&lt;/a&gt; for a way to create a &lt;code&gt;GraphDef&lt;/code&gt; proto.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="59bf9d9f9818c530570cd3baaed9172f0b655e0a" translate="yes" xml:space="preserve">
          <source>This function raises &lt;code&gt;ValueError&lt;/code&gt; unless it can be certain that the given &lt;code&gt;tensor&lt;/code&gt; is a scalar. &lt;code&gt;ValueError&lt;/code&gt; is also raised if the shape of &lt;code&gt;tensor&lt;/code&gt; is unknown.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a733b4ff78c36cb4d261221c5118416ce8061d8e" translate="yes" xml:space="preserve">
          <source>This function reinstantiates model state by: 1) loading model topology from json (this will eventually come from metagraph). 2) loading model weights from checkpoint.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff0b45b19346e757b8eb62e1bc7385c9d06f1d69" translate="yes" xml:space="preserve">
          <source>This function returns a tensor whose elements are defined by &lt;code&gt;equation&lt;/code&gt;, which is written in a shorthand form inspired by the Einstein summation convention. As an example, consider multiplying two matrices A and B to form a matrix C. The elements of C are given by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b7b7a286f6486a214583e48df98d413e08472ce" translate="yes" xml:space="preserve">
          <source>This function should &lt;em&gt;not&lt;/em&gt; be used for operations that have a well-defined gradient that is not yet implemented.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c538dca8aca927a7d8bbd1b0bb2e590f6c5ad3a5" translate="yes" xml:space="preserve">
          <source>This function supports a subset of tf.gather, see tf.gather for details on usage.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e857d76db9c7bac209f1715cec1437cf523540d8" translate="yes" xml:space="preserve">
          <source>This function swaps half-spaces for all axes listed (defaults to all). Note that &lt;code&gt;y[0]&lt;/code&gt; is the Nyquist component only if &lt;code&gt;len(x)&lt;/code&gt; is even.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="700dee7b88966a0547c5ef0b6e43b47147bea1dd" translate="yes" xml:space="preserve">
          <source>This function takes a &lt;code&gt;MetaGraphDef&lt;/code&gt; protocol buffer as input. If the argument is a file containing a &lt;code&gt;MetaGraphDef&lt;/code&gt; protocol buffer , it constructs a protocol buffer from the file content. The function then adds all the nodes from the &lt;code&gt;graph_def&lt;/code&gt; field to the current graph, recreates all the collections, and returns a saver constructed from the &lt;code&gt;saver_def&lt;/code&gt; field.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="94fc01032dc6978adba1e0b4d64fc3f4b8d2c710" translate="yes" xml:space="preserve">
          <source>This function transforms a list of &lt;code&gt;num_samples&lt;/code&gt; sequences (lists of integers) into a 2D Numpy array of shape &lt;code&gt;(num_samples, num_timesteps)&lt;/code&gt;. &lt;code&gt;num_timesteps&lt;/code&gt; is either the &lt;code&gt;maxlen&lt;/code&gt; argument if provided, or the length of the longest sequence otherwise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c60464500521000032d7d0dd44fb1270d9c6691c" translate="yes" xml:space="preserve">
          <source>This function transforms a sequence of word indexes (list of integers) into tuples of words of the form:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0cf6cff1a07ce3f9dd6dd3457b8400fd6ba9faf8" translate="yes" xml:space="preserve">
          <source>This function uses substring matching, i.e. the matching succeeds if &lt;em&gt;any&lt;/em&gt; substring of the error message matches &lt;em&gt;any&lt;/em&gt; regex in the list. This is more convenient for the user than full-string matching.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6ea3c37a0724be70d72aba9456ec479ad3e1efd0" translate="yes" xml:space="preserve">
          <source>This function validates that &lt;code&gt;obj&lt;/code&gt; represents an element of this graph, and gives an informative error message if it is not.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89bde45a9d6e8e699bac43f2a09b2f06fe6c3d12" translate="yes" xml:space="preserve">
          <source>This function will modify the tensors passed in as it adds more operations and hence changing the consumers of the operations of the input tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4e12e3274ef470af1606308b5afb5383f0e4373" translate="yes" xml:space="preserve">
          <source>This function works on either a single image (&lt;code&gt;image&lt;/code&gt; is a 3-D Tensor), or a batch of images (&lt;code&gt;image&lt;/code&gt; is a 4-D Tensor).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3bc4d91f81e39f3ea54e11412256f8f82c2a442a" translate="yes" xml:space="preserve">
          <source>This function wraps tensor placeholders in a supervised_receiver_fn with the expectation that the features and labels appear precisely as the model_fn expects them. Features and labels can therefore be dicts of tensors, or raw tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="061aef760e659c96c1579128f6e2913f90a85e0a" translate="yes" xml:space="preserve">
          <source>This has the effect of transforming sliding window operations into the corresponding &quot;atrous&quot; operation in which the input is sampled at the specified &lt;code&gt;dilation_rate&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="10c41b6251ef0bcbec27e5d4c8ebb2292ccebcaa" translate="yes" xml:space="preserve">
          <source>This helper method provides a higher-level alternative to using &lt;code&gt;tf.contrib.summary.summary_writer_initializer_op&lt;/code&gt; and &lt;code&gt;tf.contrib.summary.graph&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="671c9eb768ee498e26815ceb625e7ee7c309f7e7" translate="yes" xml:space="preserve">
          <source>This hook delays execution until global step reaches to &lt;code&gt;wait_until_step&lt;/code&gt;. It is used to gradually start workers in distributed settings. One example usage would be setting &lt;code&gt;wait_until_step=int(K*log(task_id+1))&lt;/code&gt; assuming that task_id=0 is the chief.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a0fa0f63117915fa2c817fa42b4c67a4f5c8b33" translate="yes" xml:space="preserve">
          <source>This hook requests stop after either a number of steps have been executed or a last step has been reached. Only one of the two options can be specified.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="29326e19d0dc3f4c325692b2b9f948f424da0e4f" translate="yes" xml:space="preserve">
          <source>This hook saves the state of the iterators in the &lt;code&gt;Graph&lt;/code&gt; so that when training is resumed the input pipeline continues from where it left off. This could potentially avoid overfitting in certain pipelines where the number of training steps per eval are small compared to the dataset size or if the training pipeline is pre-empted.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="362202ba29e8d6beed9c2059ae03ede13f3cd76f" translate="yes" xml:space="preserve">
          <source>This hook should be used if the input pipeline state needs to be saved separate from the model checkpoint. Doing so may be useful for a few reasons:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6ae1f6a6bb162ce4153046b8500ec1c1479c758" translate="yes" xml:space="preserve">
          <source>This identifies the replica that is part of a sync group. Currently we assume that all sync groups contain the same number of replicas. The value of the replica id can range from 0 to &lt;code&gt;num_replica_in_sync&lt;/code&gt; - 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ecd5efd797cabcd540fdce983afef271dcb8292a" translate="yes" xml:space="preserve">
          <source>This implementation of RMSprop uses plain momentum, not Nesterov momentum.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69398f914b35c5353b93476810576462db6eec45" translate="yes" xml:space="preserve">
          <source>This implements the anisotropic 2-D version of the formula described here:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eaa40933c0c7c31f3c54363539d17de16903a6ff" translate="yes" xml:space="preserve">
          <source>This includes ops from TF 2.0 tf.summary and TF 1.x tf.contrib.summary (except for &lt;code&gt;tf.contrib.summary.graph&lt;/code&gt; and &lt;code&gt;tf.contrib.summary.import_event&lt;/code&gt;), but does &lt;em&gt;not&lt;/em&gt; include TF 1.x tf.summary ops.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2cbffbf5b87e7d08406cd7f76d2f7936c22b5d07" translate="yes" xml:space="preserve">
          <source>This includes the operations to synchronize replicas: aggregate gradients, apply to variables, increment global step, insert tokens to token queue.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc77baa2dba67576cde30b92064b3afa6abe84f1" translate="yes" xml:space="preserve">
          <source>This induces quasi-linear speedup on up to 8 GPUs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e91bcd2748922b865fbdb3c28f16a3f92c87b0e" translate="yes" xml:space="preserve">
          <source>This initializer assigns one entry in the table for each line in the file.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93a30392533a2023857f78a57a8100de69db56b7" translate="yes" xml:space="preserve">
          <source>This initializes a new Kubernetes ClusterResolver. The ClusterResolver will attempt to talk to the Kubernetes master to retrieve all the instances of pods matching a label selector.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ded15c2fc6dc030fcd9627bdc75c00460c2cb65d" translate="yes" xml:space="preserve">
          <source>This is (mostly) a special case of &lt;a href=&quot;../math/add&quot;&gt;&lt;code&gt;tf.add&lt;/code&gt;&lt;/a&gt; where &lt;code&gt;bias&lt;/code&gt; is restricted to 1-D. Broadcasting is supported, so &lt;code&gt;value&lt;/code&gt; may have any number of dimensions. Unlike &lt;a href=&quot;../math/add&quot;&gt;&lt;code&gt;tf.add&lt;/code&gt;&lt;/a&gt;, the type of &lt;code&gt;bias&lt;/code&gt; is allowed to differ from &lt;code&gt;value&lt;/code&gt; in the case where both types are quantized.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f67d123191520f4a8994999dc8cd88f7fe320b7" translate="yes" xml:space="preserve">
          <source>This is EXPERIMENTAL and subject to change.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02f8d625292f24a8f6fa6d8a843ae5c003159e97" translate="yes" xml:space="preserve">
          <source>This is a class method that describes what key/value arguments are required to instantiate the given &lt;code&gt;Distribution&lt;/code&gt; so that a particular shape is returned for that instance's call to &lt;code&gt;sample()&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="012a7836bd88d1e3c2c595506564d4812be83ba5" translate="yes" xml:space="preserve">
          <source>This is a class method that describes what key/value arguments are required to instantiate the given &lt;code&gt;Distribution&lt;/code&gt; so that a particular shape is returned for that instance's call to &lt;code&gt;sample()&lt;/code&gt;. Assumes that the sample's shape is known statically.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fead40a760af2fb3753f3e33415c41be437e055a" translate="yes" xml:space="preserve">
          <source>This is a companion method to &lt;code&gt;add_queue_runner()&lt;/code&gt;. It just starts threads for all queue runners collected in the graph. It returns the list of all threads.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="309ae6e7b107ea04158b03fa5671cc2764a5cdf7" translate="yes" xml:space="preserve">
          <source>This is a context class that is passed to the user's input function and contains information about the compute replicas and input pipelines. The number of compute replicas (in sync training) helps compute the local batch size from the desired global batch size for each replica. The input pipeline information can be used to return a different subset of the input in each replica (for e.g. shard the input pipeline, use a different input source etc).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1659a3286a3bedb8cf721b60523b4bfde4d51caf" translate="yes" xml:space="preserve">
          <source>This is a convenience method that converts RGB images to float representation, adjusts their brightness, and then converts them back to the original data type. If several adjustments are chained, it is advisable to minimize the number of redundant conversions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bcc711030b8e6b303b05fbb2eadad7a51295960" translate="yes" xml:space="preserve">
          <source>This is a convenience method that converts RGB images to float representation, adjusts their contrast, and then converts them back to the original data type. If several adjustments are chained, it is advisable to minimize the number of redundant conversions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="689582a22623e8b58ba6e0ed65682c48dc03d8f0" translate="yes" xml:space="preserve">
          <source>This is a convenience method that converts RGB images to float representation, converts them to HSV, add an offset to the saturation channel, converts back to RGB and then back to the original data type. If several adjustments are chained it is advisable to minimize the number of redundant conversions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="157983bd7b4bf865fccf95ec9e2bc8ddd381392d" translate="yes" xml:space="preserve">
          <source>This is a convenience method that converts an RGB image to float representation, converts it to HSV, add an offset to the hue channel, converts back to RGB and then back to the original data type. If several adjustments are chained it is advisable to minimize the number of redundant conversions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b85cb6fb2b17c1cfe1f8becbb4b4eeadf576adb" translate="yes" xml:space="preserve">
          <source>This is a convenience method that converts an image to uint8 representation, encodes it to jpeg with &lt;code&gt;jpeg_quality&lt;/code&gt;, decodes it, and then converts back to the original data type.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab28ad9476b59e4ae1d47c731d1e80592c84261d" translate="yes" xml:space="preserve">
          <source>This is a deprecated version of &lt;code&gt;fractional_avg_pool&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89fac60f109fd0365fd768e5b0c1efda980aac29" translate="yes" xml:space="preserve">
          <source>This is a deprecated version of &lt;code&gt;fractional_max_pool&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="066f7ab7935722a8bd3e79702ee438f7b2aad916" translate="yes" xml:space="preserve">
          <source>This is a difference between DatasetV1 and DatasetV2. DatasetV1 does not take anything in its constructor whereas in the DatasetV2, we expect subclasses to create a variant_tensor and pass it in to the super() call.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e7c85e6a460e7ff32a9089528b0d0f38042adaa0" translate="yes" xml:space="preserve">
          <source>This is a faster way to train a softmax classifier over a huge number of classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7e11ee2edab133ca8b8374faafd78e7821ed539" translate="yes" xml:space="preserve">
          <source>This is a legacy behaviour of TensorFlow and is highly discouraged.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3be19c517fb6a6f6742dcaed9be8bb79e5a6a4d" translate="yes" xml:space="preserve">
          <source>This is a legacy version of the more general BatchToSpaceND.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2584be3bd38ee90f3ac17e87b40fd5fb1908bdb" translate="yes" xml:space="preserve">
          <source>This is a legacy version of the more general SpaceToBatchND.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0ae3cb7ef651d4c8fb957d4b4823d4c0713533f" translate="yes" xml:space="preserve">
          <source>This is a low-level interface for creating an &lt;code&gt;Operation&lt;/code&gt;. Most programs will not call this method directly, and instead use the Python op constructors, such as &lt;a href=&quot;constant&quot;&gt;&lt;code&gt;tf.constant()&lt;/code&gt;&lt;/a&gt;, which add ops to the default graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1cd850dc655362d680df73debd3d92a42edd1883" translate="yes" xml:space="preserve">
          <source>This is a method that implementers of subclasses of &lt;code&gt;Layer&lt;/code&gt; or &lt;code&gt;Model&lt;/code&gt; can override if they need a state-creation step in-between layer instantiation and layer call.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="62c4042c7b665a4296b8d1a943304048eb64aec7" translate="yes" xml:space="preserve">
          <source>This is a nonzero integer. See the get_ident() function. Thread identifiers may be recycled when a thread exits and another thread is created. The identifier is available even after the thread has exited.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31eb573dcb4f05e127842ce7a2c523b4f9fd2e61" translate="yes" xml:space="preserve">
          <source>This is a reduction created for Nvidia DGX-1 which assumes GPUs connects like that on DGX-1 machine. If you have different GPU inter-connections, it is likely that it would be slower than &lt;a href=&quot;reductiontoonedevice&quot;&gt;&lt;code&gt;tf.distribute.ReductionToOneDevice&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81cb44c6cca42a31a07fbae208fd1a7a56619cfa" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;a href=&quot;../../../random/categorical&quot;&gt;&lt;code&gt;tf.random.categorical&lt;/code&gt;&lt;/a&gt;: if run twice with the same seeds, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="812a072511f22cc831859b50e28ff4fe62133693" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;a href=&quot;normal&quot;&gt;&lt;code&gt;tf.random.normal&lt;/code&gt;&lt;/a&gt;: if run twice with the same seeds, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd6096e0d7eff9f42170cc05a1fe61ba85be91f4" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;a href=&quot;truncated_normal&quot;&gt;&lt;code&gt;tf.random.truncated_normal&lt;/code&gt;&lt;/a&gt;: if run twice with the same seeds, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b120804ac2994699c2b37663ac0990fd63dfd70" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;a href=&quot;uniform&quot;&gt;&lt;code&gt;tf.random.uniform&lt;/code&gt;&lt;/a&gt;: if run twice with the same seeds, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ada11b1aaca11d67319d958bd7d1da13f15ebb7f" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;code&gt;tf.categorical&lt;/code&gt;: if run twice with the same seeds, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9603abb8f073d177d05702e0bc2ac4de5a04a74c" translate="yes" xml:space="preserve">
          <source>This is a wrapper to the &lt;code&gt;hashing_trick&lt;/code&gt; function using &lt;code&gt;hash&lt;/code&gt; as the hashing function; unicity of word to index mapping non-guaranteed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ac7a813777192ceaa721f12e2b167eefa5ce19d7" translate="yes" xml:space="preserve">
          <source>This is always checked statically, so this method returns nothing.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="acb8330008d86d94ea39351a6992d0b396765e55" translate="yes" xml:space="preserve">
          <source>This is an abstract class which allows extensions to TensorFlow's object-based checkpointing (see &lt;a href=&quot;../checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt;). For example a wrapper for NumPy arrays:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8bbaa33b6f8b11a0355ba861cdf03d3e08622a92" translate="yes" xml:space="preserve">
          <source>This is an identity op (behaves like &lt;a href=&quot;../../identity&quot;&gt;&lt;code&gt;tf.identity&lt;/code&gt;&lt;/a&gt;) with the side effect of printing &lt;code&gt;data&lt;/code&gt; when evaluating.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04be17d14e8556e45ddfd971016c00de8c4f5796" translate="yes" xml:space="preserve">
          <source>This is an implementation of cluster resolvers for Kubernetes. When given the the Kubernetes namespace and label selector for pods, we will retrieve the pod IP addresses of all running pods matching the selector, and return a ClusterSpec based on that information.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fbfdd6a723bd3e94666f26590d4249971ef2fb48" translate="yes" xml:space="preserve">
          <source>This is an implementation of cluster resolvers for Slurm clusters. This allows the specification of jobs and task counts, number of tasks per node, number of GPUs on each node and number of GPUs for each task. It retrieves system attributes by Slurm environment variables, resolves allocated computing node names, constructs a cluster and returns a ClusterResolver object which can be use for distributed TensorFlow.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71688349367e3a46bf633affe83f7f67a539cf79" translate="yes" xml:space="preserve">
          <source>This is an implementation of cluster resolvers for the Google Cloud TPU service. As Cloud TPUs are in alpha, you will need to specify a API definition file for this to consume, in addition to a list of Cloud TPUs in your Google Cloud Platform project.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad683e8f469fc31a1a6f2ec923965536267cfedf" translate="yes" xml:space="preserve">
          <source>This is an implementation of cluster resolvers for the Google Compute Engine instance group platform. By specifying a project, zone, and instance group, this will retrieve the IP address of all the instances within the instance group and return a ClusterResolver object suitable for use for distributed TensorFlow.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a371677e8e3150f82fcdcb41fbdf4b2aece50db2" translate="yes" xml:space="preserve">
          <source>This is an implementation of cluster resolvers when using TF_CONFIG to set information about the cluster. The cluster spec returned will be initialized from the TF_CONFIG environment variable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2e4a40b99bc22ad76a6b8d488160787266d96b7" translate="yes" xml:space="preserve">
          <source>This is because evaluating the gradient graph does not require evaluating the constant(1) op created in the forward pass.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b402b2ea93d17cfe8814c2e2dd9513661013af48" translate="yes" xml:space="preserve">
          <source>This is called to signal the hooks that a new session has been created. This has two essential differences with the situation in which &lt;code&gt;begin&lt;/code&gt; is called:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e53a92d847e73238072fa6528b3217c26234a931" translate="yes" xml:space="preserve">
          <source>This is completely equivalent to the slightly longer code:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb94386b1c4a4c290298db5397c611953054ddc2" translate="yes" xml:space="preserve">
          <source>This is convenient in interactive shells and &lt;a href=&quot;http://ipython.org&quot;&gt;IPython notebooks&lt;/a&gt;, as it avoids having to pass an explicit &lt;code&gt;Session&lt;/code&gt; object to run ops.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c4ed05585b78549393533536c5eaedd31668459" translate="yes" xml:space="preserve">
          <source>This is different from &lt;code&gt;get_collection()&lt;/code&gt; which always returns a copy of the collection list if it exists and never creates an empty collection.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e7866f4c4c548ff00d81ad9995e04b7c1f8a6da" translate="yes" xml:space="preserve">
          <source>This is different from &lt;code&gt;get_collection_ref()&lt;/code&gt; which always returns the actual collection list if it exists in that it returns a new list each time it is called.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="731ffc6237ea1b1cd740ef79a9e5ef7a26a2d7c5" translate="yes" xml:space="preserve">
          <source>This is essentially a shortcut for &lt;code&gt;assign(self, value)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0bbe53c86ac89d70d42d35929c4d3967934b3af3" translate="yes" xml:space="preserve">
          <source>This is essentially a shortcut for &lt;code&gt;assign_add(self, delta)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13da478f41797a8113b6021c13a3264c6404bfc3" translate="yes" xml:space="preserve">
          <source>This is essentially a shortcut for &lt;code&gt;assign_sub(self, delta)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e2b79a6063d963599f65675c034b474bb3cf4380" translate="yes" xml:space="preserve">
          <source>This is essentially a shortcut for &lt;code&gt;count_up_to(self, limit)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f513c169143d5db244b63790e45478adc90df06" translate="yes" xml:space="preserve">
          <source>This is expected to return a constant value that will not be changed throughout its life cycle.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cbd5047389c2714c42e18e715b6900b97495f191" translate="yes" xml:space="preserve">
          <source>This is for use with models that expect a single &lt;code&gt;Tensor&lt;/code&gt; or &lt;code&gt;SparseTensor&lt;/code&gt; as an input feature, as opposed to a dict of features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f78d4311c49702fc3e064ee729929a77dc6e0a0" translate="yes" xml:space="preserve">
          <source>This is implemented as a generalized linear model, see https://en.wikipedia.org/wiki/Generalized_linear_model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b5e2c7b524f309c97d9b307ab3f360df187084ea" translate="yes" xml:space="preserve">
          <source>This is intended to be used on signals (or images). Produces a PSNR value for each image in batch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c8364ff06f44a0d2f95291cd9a883d7572dfead" translate="yes" xml:space="preserve">
          <source>This is just a shortcut for &lt;code&gt;variables_initializer(global_variables())&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df7f814ad895f2447cfabb704fb289f9c5058c9f" translate="yes" xml:space="preserve">
          <source>This is just a shortcut for &lt;code&gt;variables_initializer(local_variables())&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f6a76852476cdaa13e9bdd3f2afe687e9e8f273" translate="yes" xml:space="preserve">
          <source>This is like &lt;code&gt;sigmoid_cross_entropy_with_logits()&lt;/code&gt; except that &lt;code&gt;pos_weight&lt;/code&gt;, allows one to trade off recall and precision by up- or down-weighting the cost of a positive error relative to a negative error.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45f8d315bd404d0acf0d22377d0fcd8dbfde7b6a" translate="yes" xml:space="preserve">
          <source>This is mathematically equivalent to the classic formula below, but the use of an &lt;code&gt;assign_sub&lt;/code&gt; op (the &lt;code&gt;&quot;-=&quot;&lt;/code&gt; in the formula) allows concurrent lockless updates to the variables:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8c5b780da30371ec04532fe08dfb27ab4f7f20a6" translate="yes" xml:space="preserve">
          <source>This is more efficient than using separate &lt;a href=&quot;../reverse&quot;&gt;&lt;code&gt;tf.reverse&lt;/code&gt;&lt;/a&gt; ops.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eabc0e8c9ba11bd382b9ff6e31e4cc504e020ab9" translate="yes" xml:space="preserve">
          <source>This is more efficient than using separate &lt;a href=&quot;../reverse&quot;&gt;&lt;code&gt;tf.reverse&lt;/code&gt;&lt;/a&gt; ops. The &lt;code&gt;reverse&lt;/code&gt; and &lt;code&gt;exclusive&lt;/code&gt; kwargs can also be combined:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80ea1c276768e8dc61eb8fd686066ffd896d6dd9" translate="yes" xml:space="preserve">
          <source>This is not a graph construction method, it does not add ops to the graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be0ccfef7a257148d1bd487cc01dfdee2f9dcbb7" translate="yes" xml:space="preserve">
          <source>This is similar to &lt;code&gt;embedding_column&lt;/code&gt;, except that it produces a list of embedding columns that share the same embedding weights.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16ef5940d98e86cc048c2b1d955cc28ebe55e81d" translate="yes" xml:space="preserve">
          <source>This is supposed to be executed in the beginning of the chief/sync thread so that even if the total_num_replicas is less than replicas_to_aggregate, the model can still proceed as the replicas can compute multiple steps per variable update. Make sure: &lt;code&gt;num_tokens &amp;gt;= replicas_to_aggregate - total_num_replicas&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0eb2e902e8ddf903b8c32989be7bc5b84650328" translate="yes" xml:space="preserve">
          <source>This is the Python 2.x counterpart to &lt;code&gt;__bool__()&lt;/code&gt; above.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d8b6a8f2029ea1fc60a937a0c43797bc06e4fc9" translate="yes" xml:space="preserve">
          <source>This is the V1 version of this layer that uses variable_scope's to create variables which works well with PartitionedVariables. Variable scopes are deprecated in V2, so the V2 version uses name_scopes instead. But currently that lacks support for partitioned variables. Use this if you need partitioned variables.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f355bc341013b9a7925572a2dcb6ac68143ef849" translate="yes" xml:space="preserve">
          <source>This is the V2 version of this layer that uses name_scopes to create variables instead of variable_scopes. But this approach currently lacks support for partitioned variables. In that case, use the V1 version instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45048f38a2e249e88fc3cdaca3ce8480b40e2329" translate="yes" xml:space="preserve">
          <source>This is the angle ( \theta \in [-\pi, \pi] ) such that [ x = r \cos(\theta) ] and [ y = r \sin(\theta) ] where (r = \sqrt(x^2 + y^2) ).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d2e50218603262fb1a544e117d169f38db78b601" translate="yes" xml:space="preserve">
          <source>This is the base class for implementing RNN cells with custom behavior.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e013372239f70ba2378635884ba4bb573e7d626" translate="yes" xml:space="preserve">
          <source>This is the class from which all layers inherit.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8fa4ff7f2578477f255b6f02a47ae45758b90edc" translate="yes" xml:space="preserve">
          <source>This is the correct way to perform gradient clipping (for example, see &lt;a href=&quot;http://arxiv.org/abs/1211.5063&quot;&gt;Pascanu et al., 2012&lt;/a&gt; (&lt;a href=&quot;http://arxiv.org/pdf/1211.5063.pdf&quot;&gt;pdf&lt;/a&gt;)).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="76694ede23dda167a78335c0c1bd3e85284fad75" translate="yes" xml:space="preserve">
          <source>This is the crossentropy metric class to be used when there are multiple label classes (2 or more). Here we assume that labels are given as a &lt;code&gt;one_hot&lt;/code&gt; representation. eg., When labels values are [2, 0, 1], &lt;code&gt;y_true&lt;/code&gt; = [[0, 0, 1], [1, 0, 0], [0, 1, 0]].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d187cef3999e110bfd14733d1bc7141db46b4d0d" translate="yes" xml:space="preserve">
          <source>This is the crossentropy metric class to be used when there are only two label classes (0 and 1).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb90fff0e52ece653003dbcf512572b6437c5795" translate="yes" xml:space="preserve">
          <source>This is the dtype layers will create their variables in, unless a layer explicitly chooses a different dtype. If this is different than &lt;a href=&quot;policy#compute_dtype&quot;&gt;&lt;code&gt;Policy.compute_dtype&lt;/code&gt;&lt;/a&gt;, Layers will cast variables to the compute dtype to avoid type errors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08b035fd1ef991c0ac17661834339a93b3d11556" translate="yes" xml:space="preserve">
          <source>This is the dtype layers will do their computations in.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e888c610bf02b0ac69c58288afbe898eebab389" translate="yes" xml:space="preserve">
          <source>This is the first part of &lt;code&gt;minimize()&lt;/code&gt;. It returns a list of (gradient, variable) pairs where &quot;gradient&quot; is the gradient for &quot;variable&quot;. Note that &quot;gradient&quot; can be a &lt;code&gt;Tensor&lt;/code&gt;, an &lt;code&gt;IndexedSlices&lt;/code&gt;, or &lt;code&gt;None&lt;/code&gt; if there is no gradient for the given variable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b03c235347a9cd23cdcbcc99319938e9531d2ad0" translate="yes" xml:space="preserve">
          <source>This is the opposite of stack.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2e4bca768b327e659a826ca7018b5b658cdefd12" translate="yes" xml:space="preserve">
          <source>This is the opposite of unstack. The numpy equivalent is</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40e4198c7947cb171a27f0f2b96b5e41f088c377" translate="yes" xml:space="preserve">
          <source>This is the opposite of unstack. The numpy equivalent is &lt;code&gt;np.stack&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="97cb38281f3bf589012ea6423fb54fdf27897538" translate="yes" xml:space="preserve">
          <source>This is the recommended way to check if a checkpoint exists, since it takes into account the naming difference between V1 and V2 formats.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ffa5c6451bfed59ad42e7f927c856cf80634c8e" translate="yes" xml:space="preserve">
          <source>This is the recommended way to get the mtimes, since it takes into account the naming difference between V1 and V2 formats.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e10b2d35c35f3fc21ee9a235cd260123a0e63325" translate="yes" xml:space="preserve">
          <source>This is the same as the number of Read executions that have succeeded.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f339ca988bca6265b32a0422f47f6bd298db3ffc" translate="yes" xml:space="preserve">
          <source>This is the second part of &lt;code&gt;minimize()&lt;/code&gt;. It returns an &lt;code&gt;Operation&lt;/code&gt; that applies gradients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="929a719e4fd77bb16e29532f382d40f922f3c1e7" translate="yes" xml:space="preserve">
          <source>This is the second part of &lt;code&gt;minimize()&lt;/code&gt;. It returns an &lt;code&gt;Operation&lt;/code&gt; that conditionally applies gradients if all gradient values are finite. Otherwise no update is performed (nor is &lt;code&gt;global_step&lt;/code&gt; incremented).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e8ee3ea42d612b02e009f45c4fa84d1cc896a5de" translate="yes" xml:space="preserve">
          <source>This is true if the variable dtype is not the same as the compute dtype.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="599e1941b7c0f4f9a0022f3bce79b85ba3adcf41" translate="yes" xml:space="preserve">
          <source>This is typically used to create the weights of &lt;code&gt;Layer&lt;/code&gt; subclasses.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="05a2eba266c663d9424df49b8db51e4cc8c2df9e" translate="yes" xml:space="preserve">
          <source>This is used only for TfLite, it provides hints and it also makes the variables in the desired for the tflite ops (transposed and seaparated).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad15eb7677663d9a7242cb23213706bc03ee0734" translate="yes" xml:space="preserve">
          <source>This is used only for TfLite, it provides hints and it also makes the variables in the desired for the tflite ops.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa63c8ad524a0fffe274fd0f59f47f9f37029b1b" translate="yes" xml:space="preserve">
          <source>This is used to convert from a TensorFlow GraphDef, SavedModel or tf.keras model into either a TFLite FlatBuffer or graph visualization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42aa887e1a89f1c6a2cebd74bb895f76b927be77" translate="yes" xml:space="preserve">
          <source>This is used to decide whether loss should be scaled in optimizer (used only for estimator + v1 optimizer use case).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b17cf82d54ada528ca921a0efa90b52815fa4b91" translate="yes" xml:space="preserve">
          <source>This is used to prepare for toco conversion of complex intrinsic usages. Note: only one of session or graph_def should be used, not both.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7bb1bcc7e14a92bfe2b3cdc7e1ae060a3c52e44" translate="yes" xml:space="preserve">
          <source>This is useful any time you want to compute a value with TensorFlow but need to pretend that the value was a constant. Some examples include:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="615f9e8a59ff4f1c3fc7f3ec9b1dd3650508efec" translate="yes" xml:space="preserve">
          <source>This is useful as a placeholder in code that expects a context manager.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b9563699660ae889442bbf807a27eb9beba27100" translate="yes" xml:space="preserve">
          <source>This is useful for separating training updates and state updates, e.g. when we need to update a layer's internal state during prediction.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="946e89dc12a7c9a5e1ede20576867301b896367c" translate="yes" xml:space="preserve">
          <source>This is useful for sequence tasks in which the elements have variable length. Grouping together elements that have similar lengths reduces the total fraction of padding in a batch which increases training step efficiency.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="750a87642623af70551e01a6394d91315e9bfbb7" translate="yes" xml:space="preserve">
          <source>This is useful if you don't want to exit the context manager for the tape, or can't because the desired reset point is inside a control flow construct:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c4209d707783ab42b6537361cd0e771e2838f9d" translate="yes" xml:space="preserve">
          <source>This is useful in summaries to measure and report sparsity. For example,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af423eb943d44142b4ba2df380bb45f9370ab357" translate="yes" xml:space="preserve">
          <source>This is useful to eliminate per-test boilerplate when context managers are used. For example, instead of decorating every test with &lt;code&gt;@mock.patch&lt;/code&gt;, simply do &lt;code&gt;self.foo = self.enter_context(mock.patch(...))' in&lt;/code&gt;setUp()`.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f67d0f15d4bd847d95e4812f46c68a7b2ce669c" translate="yes" xml:space="preserve">
          <source>This is useful to mitigate overfitting (you could see it as a form of random data augmentation). Gaussian Noise (GS) is a natural choice as corruption process for real valued inputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d38a2f7346a2a0766d59c6174961d13ece4817fb" translate="yes" xml:space="preserve">
          <source>This is useful when validating the result of a broadcasting operation when the tensors do not have statically known shapes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50937613d6e3934b243fbeba16769c301a7b4618" translate="yes" xml:space="preserve">
          <source>This is useful when validating the result of a broadcasting operation when the tensors have statically known shapes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d56f48520fc92b32fe411b251279af631392c0d0" translate="yes" xml:space="preserve">
          <source>This is useful when you need to extract a subset of slices in an &lt;code&gt;IndexedSlices&lt;/code&gt; object.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7324163610c415fe69e9edf14d1514614e0096c2" translate="yes" xml:space="preserve">
          <source>This is where the layer's logic lives.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51b266b637bd972731e9b8ded931808387760c03" translate="yes" xml:space="preserve">
          <source>This iterator-constructing method can be used to create an iterator that is reusable with many different datasets.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6651501cdbb63552c7881c732a891c9b596e4cd4" translate="yes" xml:space="preserve">
          <source>This layer can add rows and columns of zeros at the top, bottom, left and right side of an image tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8f7de7a90e40a1acb832cfeb3e997b1fdd42909" translate="yes" xml:space="preserve">
          <source>This layer can be called multiple times with different features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b23ebdf3426b29c2bb4017b933209c527b0b2eaf" translate="yes" xml:space="preserve">
          <source>This layer can only be used as the first layer in a model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fbd759fae81bda98338ee6eb24eac5b449a08594" translate="yes" xml:space="preserve">
          <source>This layer creates a convolution kernel that is convolved (actually cross-correlated) with the layer input to produce a tensor of outputs. If &lt;code&gt;use_bias&lt;/code&gt; is True (and a &lt;code&gt;bias_initializer&lt;/code&gt; is provided), a bias vector is created and added to the outputs. Finally, if &lt;code&gt;activation&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt;, it is applied to the outputs as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a9977284bd794456f41f5d8f8d36aed1d3801d36" translate="yes" xml:space="preserve">
          <source>This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. If &lt;code&gt;use_bias&lt;/code&gt; is True, a bias vector is created and added to the outputs. Finally, if &lt;code&gt;activation&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt;, it is applied to the outputs as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d01bf01ba17a42cbf8ad8412a4cd7bfe52f00c78" translate="yes" xml:space="preserve">
          <source>This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If &lt;code&gt;use_bias&lt;/code&gt; is True, a bias vector is created and added to the outputs. Finally, if &lt;code&gt;activation&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt;, it is applied to the outputs as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="884d29443e655c08f257cc412193f76628fd846b" translate="yes" xml:space="preserve">
          <source>This layer has basic options for managing text in a Keras model. It transforms a batch of strings (one sample = one string) into either a list of token indices (one sample = 1D tensor of integer token indices) or a dense representation (one sample = 1D tensor of float values representing data about the sample's tokens).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bca79ddc4abf3d361de2fa2d5bdf3c1e02faac7e" translate="yes" xml:space="preserve">
          <source>This layer implements the operation: &lt;code&gt;outputs = activation(inputs * kernel + bias)&lt;/code&gt; Where &lt;code&gt;activation&lt;/code&gt; is the activation function passed as the &lt;code&gt;activation&lt;/code&gt; argument (if not &lt;code&gt;None&lt;/code&gt;), &lt;code&gt;kernel&lt;/code&gt; is a weights matrix created by the layer, and &lt;code&gt;bias&lt;/code&gt; is a bias vector created by the layer (only if &lt;code&gt;use_bias&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35e71e883bb0be18fa94fd4d0a457376805923f0" translate="yes" xml:space="preserve">
          <source>This layer performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels. If &lt;code&gt;use_bias&lt;/code&gt; is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="344f6b570b33a446c2eb6dc9ce40efd08ad87ee8" translate="yes" xml:space="preserve">
          <source>This layer supports masking for input data with a variable number of timesteps. To introduce masks to your data, use an [tf.keras.layers.Embedding] layer with the &lt;code&gt;mask_zero&lt;/code&gt; parameter set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe1b0f3431a8ec7d138b525163f2ad9cf543ffa7" translate="yes" xml:space="preserve">
          <source>This layer will coerce its inputs into a normal distribution centered around 0 with standard deviation 1. It accomplishes this by precomputing the mean and variance of the data, and calling (input-mean)/sqrt(var) at runtime.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fbf50d844249a2e7240d57251d949994666050a2" translate="yes" xml:space="preserve">
          <source>This library contains all implementations of ClusterResolvers. ClusterResolvers are a way of specifying cluster information for distributed execution. Built on top of existing &lt;code&gt;ClusterSpec&lt;/code&gt; framework, ClusterResolvers are a way for TensorFlow to communicate with various cluster management systems (e.g. GCE, AWS, etc...).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6b2d72a41e6a38bd121d03e086a0c141b5bab2a" translate="yes" xml:space="preserve">
          <source>This makes the TensorFlow Lite interpreter accessible in Python. It is possible to use this interpreter in a multithreaded Python environment, but you must be sure to call functions of a particular instance from only one thread at a time. So if you want to have 4 threads running different inferences simultaneously, create an interpreter for each one as thread-local data. Similarly, if you are calling invoke() in one thread on a single interpreter but you want to use tensor() on another thread once it is done, you must use a synchronization primitive between the threads to ensure invoke has returned before calling tensor().</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6846fc96931fb6bb57a7f6152c9caf47e33ab2da" translate="yes" xml:space="preserve">
          <source>This makes the summary tag more predictable and consistent for the user.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99fc0b3a4e959624cfdd7190647db3380c4e327c" translate="yes" xml:space="preserve">
          <source>This may be useful for checking HTML output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="381d4e4134c335f7e7e4d713df90044462182059" translate="yes" xml:space="preserve">
          <source>This may occur, for example, if an operation receives an input tensor that has an invalid value or shape. For example, the &lt;a href=&quot;../linalg/matmul&quot;&gt;&lt;code&gt;tf.matmul&lt;/code&gt;&lt;/a&gt; op will raise this error if it receives an input that is not a matrix, and the &lt;a href=&quot;../reshape&quot;&gt;&lt;code&gt;tf.reshape&lt;/code&gt;&lt;/a&gt; op will raise this error if the new shape does not match the number of elements in the input tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c451c9f3d5af265bc5daacf860461f46903194b" translate="yes" xml:space="preserve">
          <source>This may only be used inside &lt;code&gt;self.scope()&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8407349f1434fe18f1d4f684e5cf94ad12755be8" translate="yes" xml:space="preserve">
          <source>This means that the result of matrix multiplication &lt;code&gt;v = Au&lt;/code&gt; has &lt;code&gt;Lth&lt;/code&gt; column given circular convolution between &lt;code&gt;h&lt;/code&gt; with the &lt;code&gt;Lth&lt;/code&gt; column of &lt;code&gt;u&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bac1b11600b86163afb8ed50f9d2fd7c08c803e" translate="yes" xml:space="preserve">
          <source>This method allows you to define a &quot;feedable&quot; iterator where you can choose between concrete iterators by feeding a value in a &lt;code&gt;tf.Session.run&lt;/code&gt; call. In that case, &lt;code&gt;string_handle&lt;/code&gt; would be a &lt;a href=&quot;../placeholder&quot;&gt;&lt;code&gt;tf.compat.v1.placeholder&lt;/code&gt;&lt;/a&gt;, and you would feed it with the value of &lt;code&gt;tf.data.Iterator.string_handle&lt;/code&gt; in each step.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ed29b40d37b37daebb7172d60a5cb7452fb1937" translate="yes" xml:space="preserve">
          <source>This method also allows multi-arity &lt;code&gt;elems&lt;/code&gt; and accumulator. If &lt;code&gt;elems&lt;/code&gt; is a (possibly nested) list or tuple of tensors, then each of these tensors must have a matching first (unpack) dimension. The second argument of &lt;code&gt;fn&lt;/code&gt; must match the structure of &lt;code&gt;elems&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11d1041ad958b2d24d55eb0ab2e6d83f39085f6b" translate="yes" xml:space="preserve">
          <source>This method also allows multi-arity &lt;code&gt;elems&lt;/code&gt; and output of &lt;code&gt;fn&lt;/code&gt;. If &lt;code&gt;elems&lt;/code&gt; is a (possibly nested) list or tuple of tensors, then each of these tensors must have a matching first (unpack) dimension. The signature of &lt;code&gt;fn&lt;/code&gt; may match the structure of &lt;code&gt;elems&lt;/code&gt;. That is, if &lt;code&gt;elems&lt;/code&gt; is &lt;code&gt;(t1, [t2, t3, [t4, t5]])&lt;/code&gt;, then an appropriate signature for &lt;code&gt;fn&lt;/code&gt; is: &lt;code&gt;fn = lambda (t1, [t2, t3, [t4, t5]]):&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="766a4862e3d658751f25e8ca0813b9af0856b88d" translate="yes" xml:space="preserve">
          <source>This method behaves differently than self.session(): for performance reasons &lt;code&gt;cached_session&lt;/code&gt; will by default reuse the same session within the same test. The session returned by this function will only be closed at the end of the test (in the TearDown function).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7294d73f60860f63726337f7edc669a47fd434f" translate="yes" xml:space="preserve">
          <source>This method builds a new graph by first calling the &lt;code&gt;serving_input_receiver_fn&lt;/code&gt; to obtain feature &lt;code&gt;Tensor&lt;/code&gt;s, and then calling this &lt;code&gt;Estimator&lt;/code&gt;'s &lt;code&gt;model_fn&lt;/code&gt; to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given &lt;code&gt;export_dir_base&lt;/code&gt;, and writes a &lt;code&gt;SavedModel&lt;/code&gt; into it containing a single &lt;code&gt;tf.MetaGraphDef&lt;/code&gt; saved from this session.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c6ee7778196b7d2b41326a073f8b56a7c8a9a62d" translate="yes" xml:space="preserve">
          <source>This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's &lt;code&gt;Input&lt;/code&gt;s. These losses become part of the model's topology and are tracked in &lt;code&gt;get_config&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="72b1f5316c49fd45d1ca1593424ee77777a4e789" translate="yes" xml:space="preserve">
          <source>This method can be called multiple times, and will merge the given &lt;code&gt;shape&lt;/code&gt; with the current shape of this tensor. It can be used to provide additional information about the shape of this tensor that cannot be inferred from the graph alone. For example, this can be used to provide additional information about the shapes of images:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7667fea48f6e7bf7add0c54e02f390885694a1d8" translate="yes" xml:space="preserve">
          <source>This method can be used after a call to &lt;a href=&quot;enable_check_numerics&quot;&gt;&lt;code&gt;tf.debugging.enable_check_numerics()&lt;/code&gt;&lt;/a&gt; to disable the numerics-checking mechanism that catches inifnity and NaN values output by ops executed eagerly or in tf.function-compiled graphs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab134bc99ff08cb5e34c3578ec66ab8ee325437d" translate="yes" xml:space="preserve">
          <source>This method can be used for several purposes. For example, where &lt;code&gt;experimental_distribute_dataset&lt;/code&gt; is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in &lt;code&gt;experimental_distribute_dataset&lt;/code&gt;). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed. &lt;code&gt;experimental_distribute_dataset&lt;/code&gt; may also sometimes fail to split the batch across replicas on a worker. In that case, this method can be used where that limitation does not exist.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d445a1a8ccfc13d2cbb47a0cd8852bd14137cba" translate="yes" xml:space="preserve">
          <source>This method can be used inside a subclassed layer or model's &lt;code&gt;call&lt;/code&gt; function, in which case &lt;code&gt;losses&lt;/code&gt; should be a Tensor or list of Tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a08143c632d6d9a6c3bb58abdfe351796aa7d61" translate="yes" xml:space="preserve">
          <source>This method can be used to assert that there exists a shape that both &lt;code&gt;self&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; represent.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="311f5dd67cb7916aeab132002e12e94284e8fdb8" translate="yes" xml:space="preserve">
          <source>This method can be used to create &lt;code&gt;RaggedTensor&lt;/code&gt;s with multiple uniform outer dimensions. For example, a &lt;code&gt;RaggedTensor&lt;/code&gt; with shape &lt;code&gt;[2, 2, None]&lt;/code&gt; can be constructed with this method from a &lt;code&gt;RaggedTensor&lt;/code&gt; values with shape &lt;code&gt;[4, None]&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="53a52d98446448532e361c5ab0b1a6a4bc35f1f2" translate="yes" xml:space="preserve">
          <source>This method can be used to merge partitions created by &lt;code&gt;dynamic_partition&lt;/code&gt; as illustrated on the following example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d34cdddad9212e7710a82e27967cd30b8fc5e14d" translate="yes" xml:space="preserve">
          <source>This method can be used to run a step function for training a number of times using input from a dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c709bdcba30505f75d83241dd8ba3817ec876a1" translate="yes" xml:space="preserve">
          <source>This method currently blocks forever.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a08e7549042177e7c94f4230a7e19c5cac25a2de" translate="yes" xml:space="preserve">
          <source>This method generalizes to higher-dimensions by simply providing a list for both the sp_ids as well as the vocab_size. In this case the resulting &lt;code&gt;SparseTensor&lt;/code&gt; has the following properties: - &lt;code&gt;indices&lt;/code&gt; is equivalent to &lt;code&gt;sp_ids[0].indices&lt;/code&gt; with the last dimension discarded and concatenated with &lt;code&gt;sp_ids[0].values, sp_ids[1].values, ...&lt;/code&gt;. - &lt;code&gt;values&lt;/code&gt; is simply &lt;code&gt;sp_values.values&lt;/code&gt;. - If &lt;code&gt;sp_ids.dense_shape = [D0, D1, ..., Dn, K]&lt;/code&gt;, then &lt;code&gt;output.shape = [D0, D1, ..., Dn] + vocab_size&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e7e44aa7677b60789a9ebef54c74a9714ec4de2a" translate="yes" xml:space="preserve">
          <source>This method has similar semantics to the built-in &lt;code&gt;zip()&lt;/code&gt; function in Python, with the main difference being that the &lt;code&gt;datasets&lt;/code&gt; argument can be an arbitrary nested structure of &lt;code&gt;Dataset&lt;/code&gt; objects.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1925370323ce05b6dc0cdc8787e9c0d6ebf8403a" translate="yes" xml:space="preserve">
          <source>This method is a convenience wrapper for creating a &lt;a href=&quot;server&quot;&gt;&lt;code&gt;tf.distribute.Server&lt;/code&gt;&lt;/a&gt; with a &lt;a href=&quot;../train/serverdef&quot;&gt;&lt;code&gt;tf.train.ServerDef&lt;/code&gt;&lt;/a&gt; that specifies a single-process cluster containing a single task in a job called &lt;code&gt;&quot;local&quot;&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="05c81c4b69f5f8cdea4eba6f7d8bb00b2d7cd37f" translate="yes" xml:space="preserve">
          <source>This method is automatically called when the StubOutForTesting() object is deleted; there is no need to call it explicitly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="992fe7b39731079182b90eaf498e1ed5e5a20148" translate="yes" xml:space="preserve">
          <source>This method is completely compatible with the &lt;code&gt;tf.Session.run()&lt;/code&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7dd7ea7386a00c65842a1da48fad6c58e2f712f" translate="yes" xml:space="preserve">
          <source>This method is for use by TestCase subclasses that need to register their own type equality functions to provide nicer error messages.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66283121e6883c956c7613fffa84d220528690f4" translate="yes" xml:space="preserve">
          <source>This method is idempotent. Calling it multiple times has the same effect as calling it once.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d1d2fc5aba91c121cb755ded927621c464c711dd" translate="yes" xml:space="preserve">
          <source>This method is only needed if you compute gradients manually, e.g. with &lt;a href=&quot;../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;. In that case, call this method to scale the loss before passing the loss to &lt;a href=&quot;../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;. If you use &lt;a href=&quot;../../optimizers/optimizer#minimize&quot;&gt;&lt;code&gt;LossScaleOptimizer.minimize&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;lossscaleoptimizer#get_gradients&quot;&gt;&lt;code&gt;LossScaleOptimizer.get_gradients&lt;/code&gt;&lt;/a&gt;, loss scaling is automatically applied and this method is unneeded.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ffe062127379160c3973cd4f84270b4f4f8ccf9" translate="yes" xml:space="preserve">
          <source>This method is only needed if you compute gradients manually, e.g. with &lt;a href=&quot;../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;. In that case, call this method to unscale the gradients after computing them with &lt;a href=&quot;../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;. If you use &lt;a href=&quot;../../optimizers/optimizer#minimize&quot;&gt;&lt;code&gt;LossScaleOptimizer.minimize&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;lossscaleoptimizer#get_gradients&quot;&gt;&lt;code&gt;LossScaleOptimizer.get_gradients&lt;/code&gt;&lt;/a&gt;, loss scaling is automatically applied and this method is unneeded.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d15ade5009ad723801c2c48bded790a3845587f" translate="yes" xml:space="preserve">
          <source>This method is optional if you are just training and executing models, exporting to and from SavedModels, or using weight checkpoints.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ed4e772dabffd01cfde2b74159ef69c9f3e04f5" translate="yes" xml:space="preserve">
          <source>This method is required for Keras &lt;code&gt;model_to_estimator&lt;/code&gt;, saving and loading models to HDF5 formats, Keras model cloning, some visualization utilities, and exporting models to and from JSON.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6fdad9c5ed22f16c319e244d8261ea4ee3d279dd" translate="yes" xml:space="preserve">
          <source>This method is smart and works at the module, class, and instance level while preserving proper inheritance. It will not stub out C types however unless that has been explicitly allowed by the type.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="feecb6a599cedcf2305768be36a750d8591e9329" translate="yes" xml:space="preserve">
          <source>This method is the reverse of &lt;code&gt;get_config&lt;/code&gt;, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by &lt;code&gt;set_weights&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="422559cd09a8b37855dfa1becf9159996c55a88c" translate="yes" xml:space="preserve">
          <source>This method is the reverse of &lt;code&gt;get_config&lt;/code&gt;, capable of instantiating the same optimizer from the config dictionary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7d6ed78b790787a16f4abeccc0ecd949df6cc2b" translate="yes" xml:space="preserve">
          <source>This method is the reverse of &lt;code&gt;get_config&lt;/code&gt;, capable of instantiating the same regularizer from the config dictionary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="188de6cc0c368ed4ab40eeabc0c2f0a4587200cc" translate="yes" xml:space="preserve">
          <source>This method is thread-safe.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="430e31afd2b8ce36dbd30c0726de058dc4716f34" translate="yes" xml:space="preserve">
          <source>This method is used by Keras &lt;code&gt;model_to_estimator&lt;/code&gt;, saving and loading models to HDF5 formats, Keras model cloning, some visualization utilities, and exporting models to and from JSON.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="457bbe494ff8a7cc3168e5b2acb3fc1d7df09a01" translate="yes" xml:space="preserve">
          <source>This method is used to convert a dictionary into a sequence of parameters for a binary that parses arguments using this module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f40fabbeaa1bcaea8e9654a487713070e7848ee" translate="yes" xml:space="preserve">
          <source>This method is useful for recovering the &quot;self._last_checkpoints&quot; state.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5eb2940a1b8924b3a2b4c6c370fcb87bd404d70c" translate="yes" xml:space="preserve">
          <source>This method may be called concurrently from multiple threads.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="879b4ac0bfff915fe8733219e840864c530b0386" translate="yes" xml:space="preserve">
          <source>This method must be used as a context manager, and will yield a recording object with two attributes: &lt;code&gt;output&lt;/code&gt; and &lt;code&gt;records&lt;/code&gt;. At the end of the context manager, the &lt;code&gt;output&lt;/code&gt; attribute will be a list of the matching formatted log messages and the &lt;code&gt;records&lt;/code&gt; attribute will be a list of the corresponding LogRecord objects.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45b78faa33c30a4ca2de4d4a2a7e0ea54433a6aa" translate="yes" xml:space="preserve">
          <source>This method overrides unittest.TestCase.shortDescription(), which only returns the first line of the docstring, obscuring the name of the test upon failure.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee5a67ef5f4ab3abd49e8e0aa6ab37b4c495718e" translate="yes" xml:space="preserve">
          <source>This method promotes a completely unknown shape to one with a known rank.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f08536c5c4bd5b0bf81a621ac1af31ff012f949c" translate="yes" xml:space="preserve">
          <source>This method requires a session in which the graph was launched. It creates a list of threads, optionally starting them. There is one thread for each op passed in &lt;code&gt;enqueue_ops&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed37572237615338abb9afe9f97f9e51444c9050" translate="yes" xml:space="preserve">
          <source>This method requires that you are running in eager mode and the dataset's element_spec contains only &lt;code&gt;TensorSpec&lt;/code&gt; components.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df938127180a900a9c0ee2c326f70990ca05bad0" translate="yes" xml:space="preserve">
          <source>This method returns True just before the run() method starts until just after the run() method terminates. The module function enumerate() returns a list of all alive threads.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67c5a6af4d2a477a788bf93645d98e3fc1667925" translate="yes" xml:space="preserve">
          <source>This method runs one &quot;step&quot; of TensorFlow computation, by running the necessary graph fragment to execute every &lt;code&gt;Operation&lt;/code&gt; and evaluate every &lt;code&gt;Tensor&lt;/code&gt; in &lt;code&gt;fetches&lt;/code&gt;, substituting the values in &lt;code&gt;feed_dict&lt;/code&gt; for the corresponding input values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ef4f95365ffd49356edce6309d3da564feb3d3b" translate="yes" xml:space="preserve">
          <source>This method runs the ops added by the constructor for restoring variables. It requires a session in which the graph was launched. The variables to restore do not have to have been initialized, as restoring is itself a way to initialize variables.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6a0628c028c0b695f5ff3c6935b2aad50149fac6" translate="yes" xml:space="preserve">
          <source>This method runs the ops added by the constructor for saving variables. It requires a session in which the graph was launched. The variables to save must also have been initialized.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9df50570004b7fc9f261d6b98cab329e6a66afe0" translate="yes" xml:space="preserve">
          <source>This method sets the vocabulary and DF data for this layer directly, instead of analyzing a dataset through 'adapt'. It should be used whenever the vocab (and optionally document frequency) information is already known. If vocabulary data is already present in the layer, this method will either replace it, if 'append' is set to False, or append to it (if 'append' is set to True).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="381d5d74ef1ba876c849eaf0c748814799df311b" translate="yes" xml:space="preserve">
          <source>This method should be used if you want to create multiple graphs in the same process. For convenience, a global default graph is provided, and all ops will be added to this graph if you do not create a new graph explicitly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="801d04a3f8f77fe9f713a4af6184910c6103cec6" translate="yes" xml:space="preserve">
          <source>This method should be used to create all threads in test cases, as otherwise there is a risk that a thread will silently fail, and/or assertions made in the thread will not be respected.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="84ddde0d41fb950f979365b1dd4b04e51a4464b1" translate="yes" xml:space="preserve">
          <source>This method simply combines calls &lt;code&gt;compute_gradients()&lt;/code&gt; and &lt;code&gt;apply_gradients()&lt;/code&gt;. If you want to process the gradient before applying them call &lt;code&gt;compute_gradients()&lt;/code&gt; and &lt;code&gt;apply_gradients()&lt;/code&gt; explicitly instead of using this function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f6d383158cc853c41c5bc430e4c2e31944ed6aa" translate="yes" xml:space="preserve">
          <source>This method simply computes gradient using &lt;a href=&quot;../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt; and calls &lt;code&gt;apply_gradients()&lt;/code&gt;. If you want to process the gradient before applying then call &lt;a href=&quot;../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;apply_gradients()&lt;/code&gt; explicitly instead of using this function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c45540d5f88e8d63143978a302ef3be7b4897a3" translate="yes" xml:space="preserve">
          <source>This method simply computes gradient using &lt;a href=&quot;../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt; and calls &lt;code&gt;apply_gradients()&lt;/code&gt;. If you want to process the gradient before applying then call &lt;a href=&quot;../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;apply_gradients()&lt;/code&gt; explicitly instead of using this function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9fbf54940adbbc3b34153fb181199a19b56bd888" translate="yes" xml:space="preserve">
          <source>This method supports the case where attr_name is a staticmethod or a classmethod of obj.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b1e95fe0c7a12cd011cb442b54315c64dbbef85c" translate="yes" xml:space="preserve">
          <source>This method supports the case where child_name is a staticmethod or a classmethod of parent.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c5357e5e49a58f1873923787332f7877e186b5d" translate="yes" xml:space="preserve">
          <source>This method takes effect only on the thread in which it is called.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7cd6047c37611c4ef1b1add7ec4d3db22681db24" translate="yes" xml:space="preserve">
          <source>This method will also be called as a result of recovering a wrapped session, not only at the beginning of the overall session.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="72f0ec32b5d6b0d5f31ec5a44977c2b73ae22a24" translate="yes" xml:space="preserve">
          <source>This method will raise a RuntimeError if called more than once on the same thread object.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="218a6eaea58d633bb4c07cebec2cce2a54164fc8" translate="yes" xml:space="preserve">
          <source>This method works similar to tf.map_fn but is optimized to run much faster, possibly with a much larger memory footprint. The speedups are obtained by vectorization (see https://arxiv.org/pdf/1903.04243.pdf). The idea behind vectorization is to semantically launch all the invocations of &lt;code&gt;fn&lt;/code&gt; in parallel and fuse corresponding operations across all these invocations. This fusion is done statically at graph generation time and the generated code is often similar in performance to a manually fused version.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe5b3fca46a696592f5d3933d9a6c2e14a59d105" translate="yes" xml:space="preserve">
          <source>This method wraps the provided session in an &lt;code&gt;Event&lt;/code&gt; protocol buffer and adds it to the event file.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b654fd9b9bcfd951b8584e5c172fdf88b6f87e9" translate="yes" xml:space="preserve">
          <source>This method wraps the provided summary in an &lt;code&gt;Event&lt;/code&gt; protocol buffer and adds it to the event file.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e573192331e3d216aaf00fffe24f089e83e9c9ad" translate="yes" xml:space="preserve">
          <source>This method, unlike assertCountEqual, doesn't care about any duplicates in the expected and actual sequences.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b693febd3a0c9fc69fb14a0f914449cb9729097" translate="yes" xml:space="preserve">
          <source>This metric creates four local variables, &lt;code&gt;true_positives&lt;/code&gt;, &lt;code&gt;true_negatives&lt;/code&gt;, &lt;code&gt;false_positives&lt;/code&gt; and &lt;code&gt;false_negatives&lt;/code&gt; that are used to compute the AUC. To discretize the AUC curve, a linearly spaced set of thresholds is used to compute pairs of recall and precision values. The area under the ROC-curve is therefore computed using the height of the recall values by the false positive rate, while the area under the PR-curve is the computed using the height of the precision values by the recall.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8310e01ff09f3ad784585cc77cdb8fe7f1ac0213" translate="yes" xml:space="preserve">
          <source>This metric creates four local variables, &lt;code&gt;true_positives&lt;/code&gt;, &lt;code&gt;true_negatives&lt;/code&gt;, &lt;code&gt;false_positives&lt;/code&gt; and &lt;code&gt;false_negatives&lt;/code&gt; that are used to compute the precision at the given recall. The threshold for the given recall value is computed and used to evaluate the corresponding precision.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a469d6ce618a21614949a4828cde8383fb0456e0" translate="yes" xml:space="preserve">
          <source>This metric creates four local variables, &lt;code&gt;true_positives&lt;/code&gt;, &lt;code&gt;true_negatives&lt;/code&gt;, &lt;code&gt;false_positives&lt;/code&gt; and &lt;code&gt;false_negatives&lt;/code&gt; that are used to compute the sensitivity at the given specificity. The threshold for the given specificity value is computed and used to evaluate the corresponding sensitivity.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff9252130946c80b57ae4c5ee9b47938ca02b839" translate="yes" xml:space="preserve">
          <source>This metric creates four local variables, &lt;code&gt;true_positives&lt;/code&gt;, &lt;code&gt;true_negatives&lt;/code&gt;, &lt;code&gt;false_positives&lt;/code&gt; and &lt;code&gt;false_negatives&lt;/code&gt; that are used to compute the specificity at the given sensitivity. The threshold for the given sensitivity value is computed and used to evaluate the corresponding specificity.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb652b4471c79604ae7dfb8ff87ca222abc482c6" translate="yes" xml:space="preserve">
          <source>This metric creates one variable, &lt;code&gt;total&lt;/code&gt;, that is used to compute the sum of &lt;code&gt;values&lt;/code&gt;. This is ultimately returned as &lt;code&gt;sum&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="77a69992763ecf548ab84fef5482d353d7a9cba3" translate="yes" xml:space="preserve">
          <source>This metric creates two local variables, &lt;code&gt;total&lt;/code&gt; and &lt;code&gt;count&lt;/code&gt; that are used to compute the frequency with which &lt;code&gt;y_pred&lt;/code&gt; matches &lt;code&gt;y_true&lt;/code&gt;. This frequency is ultimately returned as &lt;code&gt;binary accuracy&lt;/code&gt;: an idempotent operation that simply divides &lt;code&gt;total&lt;/code&gt; by &lt;code&gt;count&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="10a549b6038018b3cebc025343c39cb24e67e040" translate="yes" xml:space="preserve">
          <source>This metric creates two local variables, &lt;code&gt;total&lt;/code&gt; and &lt;code&gt;count&lt;/code&gt; that are used to compute the frequency with which &lt;code&gt;y_pred&lt;/code&gt; matches &lt;code&gt;y_true&lt;/code&gt;. This frequency is ultimately returned as &lt;code&gt;categorical accuracy&lt;/code&gt;: an idempotent operation that simply divides &lt;code&gt;total&lt;/code&gt; by &lt;code&gt;count&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d4e9dd5712eaf45fc10b998b8c6d4caf2e34bae" translate="yes" xml:space="preserve">
          <source>This metric creates two local variables, &lt;code&gt;total&lt;/code&gt; and &lt;code&gt;count&lt;/code&gt; that are used to compute the frequency with which &lt;code&gt;y_pred&lt;/code&gt; matches &lt;code&gt;y_true&lt;/code&gt;. This frequency is ultimately returned as &lt;code&gt;sparse categorical accuracy&lt;/code&gt;: an idempotent operation that simply divides &lt;code&gt;total&lt;/code&gt; by &lt;code&gt;count&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="300536328f19b08910251bd6b961009b9a37a3b7" translate="yes" xml:space="preserve">
          <source>This metric creates two local variables, &lt;code&gt;total&lt;/code&gt; and &lt;code&gt;count&lt;/code&gt; that are used to compute the mean relative absolute error. This average is weighted by &lt;code&gt;sample_weight&lt;/code&gt;, and it is ultimately returned as &lt;code&gt;mean_relative_error&lt;/code&gt;: an idempotent operation that simply divides &lt;code&gt;total&lt;/code&gt; by &lt;code&gt;count&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0bf885c318184cc4d7f8536f5a3a6cfc0be45917" translate="yes" xml:space="preserve">
          <source>This metric creates two local variables, &lt;code&gt;true_positives&lt;/code&gt; and &lt;code&gt;false_negatives&lt;/code&gt;, that are used to compute the recall. This value is ultimately returned as &lt;code&gt;recall&lt;/code&gt;, an idempotent operation that simply divides &lt;code&gt;true_positives&lt;/code&gt; by the sum of &lt;code&gt;true_positives&lt;/code&gt; and &lt;code&gt;false_negatives&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="093560f8538560280d9ed1cd9a3cab29a6cbb355" translate="yes" xml:space="preserve">
          <source>This metric creates two variables, &lt;code&gt;total&lt;/code&gt; and &lt;code&gt;count&lt;/code&gt; that are used to compute the average of &lt;code&gt;values&lt;/code&gt;. This average is ultimately returned as &lt;code&gt;mean&lt;/code&gt; which is an idempotent operation that simply divides &lt;code&gt;total&lt;/code&gt; by &lt;code&gt;count&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c9b94b35f1d2e4eb96d2ecbbedfd0249ca4dde6" translate="yes" xml:space="preserve">
          <source>This metric keeps the average cosine similarity between &lt;code&gt;predictions&lt;/code&gt; and &lt;code&gt;labels&lt;/code&gt; over a stream of data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="418d70f20044dbd3397385f2c468b1eaa3216144" translate="yes" xml:space="preserve">
          <source>This model accepts sparse float inputs as well:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a4737a2c04c7eb94694e1e82664a0b0d934f20d" translate="yes" xml:space="preserve">
          <source>This model approximates the following function:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="82e7fe60bedc5bc6c52d1131a24d660c1d1e5baf" translate="yes" xml:space="preserve">
          <source>This model jointly train a linear and a dnn model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e8ea575479c33e1d3b54efb7336850606e620ed3" translate="yes" xml:space="preserve">
          <source>This module contains experimental &lt;code&gt;Dataset&lt;/code&gt; sources and transformations that can be used in conjunction with the &lt;a href=&quot;../../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; API. Note that the &lt;a href=&quot;../../../data/experimental&quot;&gt;&lt;code&gt;tf.data.experimental&lt;/code&gt;&lt;/a&gt; API is not subject to the same backwards compatibility guarantees as &lt;a href=&quot;../../../data&quot;&gt;&lt;code&gt;tf.data&lt;/code&gt;&lt;/a&gt;, but we will provide deprecation advice in advance of removing existing functionality.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="048da49888cbf092642d30c4a165ddde49bb4a0b" translate="yes" xml:space="preserve">
          <source>This module contains experimental &lt;code&gt;Dataset&lt;/code&gt; sources and transformations that can be used in conjunction with the &lt;a href=&quot;dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; API. Note that the &lt;a href=&quot;experimental&quot;&gt;&lt;code&gt;tf.data.experimental&lt;/code&gt;&lt;/a&gt; API is not subject to the same backwards compatibility guarantees as &lt;a href=&quot;../data&quot;&gt;&lt;code&gt;tf.data&lt;/code&gt;&lt;/a&gt;, but we will provide deprecation advice in advance of removing existing functionality.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="458fef549ca5bb1cdd37d04e9f0b5b8e3b96d46e" translate="yes" xml:space="preserve">
          <source>This must be called by the constructors of subclasses.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b854be37a7975d22746ea61f4632ebd3ee101333" translate="yes" xml:space="preserve">
          <source>This must be called by the constructors of subclasses. Note that Optimizer instances should not bind to a single graph, and so shouldn't keep Tensors as member variables. Generally you should be able to use the _set_hyper()/state.get_hyper() facility instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="10ca194e65aab7cdbd21fa158a845dc67377d2ce" translate="yes" xml:space="preserve">
          <source>This must be set before start() is called, otherwise RuntimeError is raised. Its initial value is inherited from the creating thread; the main thread is not a daemon thread and therefore all threads created in the main thread default to daemon = False.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0329801ef1692f0ddc2ae7e5af20ed6a131c6c7c" translate="yes" xml:space="preserve">
          <source>This only ensures that the data has made its way out of the process without any guarantees on whether it's written to disk. This means that the data would survive an application crash but not necessarily an OS crash.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93e1dc6b2d1103564814280c522ceb3f62da8aaa" translate="yes" xml:space="preserve">
          <source>This op adds entries with the specified &lt;code&gt;default_value&lt;/code&gt; at index &lt;code&gt;[row, 0]&lt;/code&gt; for any row in the input that does not already have a value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c96c116d1d672bbaf2e116ec518227e07309624" translate="yes" xml:space="preserve">
          <source>This op also returns an indicator vector such that</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0a295b7a7559c4062aef820bad409706b47254e" translate="yes" xml:space="preserve">
          <source>This op also supports decoding JPEGs and PNGs, though it is cleaner to use &lt;a href=&quot;decode_image&quot;&gt;&lt;code&gt;tf.image.decode_image&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b09a5c52da344b1eb6cdb8c06bc391b1939c62f" translate="yes" xml:space="preserve">
          <source>This op also supports decoding JPEGs and non-animated GIFs since the interface is the same, though it is cleaner to use &lt;a href=&quot;decode_image&quot;&gt;&lt;code&gt;tf.image.decode_image&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42b9c8de7bcd9a44756c5acb1ef87661725abb7d" translate="yes" xml:space="preserve">
          <source>This op also supports decoding PNGs and non-animated GIFs since the interface is the same, though it is cleaner to use &lt;a href=&quot;decode_image&quot;&gt;&lt;code&gt;tf.image.decode_image&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd4491d1a6601d1e84570bcd363e85b0e7d5c12e" translate="yes" xml:space="preserve">
          <source>This op assumes that there is at least one id for each row in the dense tensor represented by sp_ids (i.e. there are no rows with empty features), and that all the indices of sp_ids are in canonical row-major order.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff99bdd028bae5711c127316ae24a743c30a9349" translate="yes" xml:space="preserve">
          <source>This op can be substantially more efficient than &lt;a href=&quot;case&quot;&gt;&lt;code&gt;tf.case&lt;/code&gt;&lt;/a&gt; when exactly one branch will be selected. &lt;a href=&quot;switch_case&quot;&gt;&lt;code&gt;tf.switch_case&lt;/code&gt;&lt;/a&gt; is more like a C++ switch/case statement than &lt;a href=&quot;case&quot;&gt;&lt;code&gt;tf.case&lt;/code&gt;&lt;/a&gt;, which is more like an if/elif/elif/else chain.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88d4e65ada667e4924bd61aa913d0b719a3489bc" translate="yes" xml:space="preserve">
          <source>This op can be used to override the gradient for complicated functions. For example, suppose y = f(x) and we wish to apply a custom function g for backprop such that dx = g(dy). In Python,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a70d4bf94e1aa31fec2daa089308a302909ea19e" translate="yes" xml:space="preserve">
          <source>This op collects patches from the input image, as if applying a convolution. All extracted patches are stacked in the depth (last) dimension of the output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c18dad5d5d6899466a10670d53475ca56625c85" translate="yes" xml:space="preserve">
          <source>This op converts between data types, scaling the values appropriately before casting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc622534ae3fa62514818ea7bebb60dccd64c26d" translate="yes" xml:space="preserve">
          <source>This op creates a &lt;a href=&quot;https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto&quot;&gt;&lt;code&gt;Summary&lt;/code&gt;&lt;/a&gt; protocol buffer that contains the union of all the values in the input summaries.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26981876c06a781e2a944a631697cc4482c0e90e" translate="yes" xml:space="preserve">
          <source>This op cuts a rectangular part out of &lt;code&gt;image&lt;/code&gt;. The top-left corner of the returned image is at &lt;code&gt;offset_height, offset_width&lt;/code&gt; in &lt;code&gt;image&lt;/code&gt;, and its lower-right corner is at &lt;code&gt;offset_height + target_height, offset_width + target_width&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6b4a71613ffb1856f3dff1fb3fb7c9c68b7515a" translate="yes" xml:space="preserve">
          <source>This op decompresses each element of the &lt;code&gt;bytes&lt;/code&gt; input &lt;code&gt;Tensor&lt;/code&gt;, which is assumed to be compressed using the given &lt;code&gt;compression_type&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e56e06059315b7dda1620664824a84ff8d7bc3c" translate="yes" xml:space="preserve">
          <source>This op does not &lt;a href=&quot;https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html&quot;&gt;broadcast&lt;/a&gt; its inputs. If you need broadcasting, use &lt;a href=&quot;add&quot;&gt;&lt;code&gt;tf.math.add&lt;/code&gt;&lt;/a&gt; (or the &lt;code&gt;+&lt;/code&gt; operator) instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32ac8411f8421961cf71053e4e574031251a142f" translate="yes" xml:space="preserve">
          <source>This op does nothing if &lt;code&gt;offset_*&lt;/code&gt; is zero and the image already has size &lt;code&gt;target_height&lt;/code&gt; by &lt;code&gt;target_width&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25438ddc9615e500386687f63662f19536c3cd65" translate="yes" xml:space="preserve">
          <source>This op first slices &lt;code&gt;input&lt;/code&gt; along the dimension &lt;code&gt;batch_axis&lt;/code&gt;, and for each slice &lt;code&gt;i&lt;/code&gt;, reverses the first &lt;code&gt;seq_lengths[i]&lt;/code&gt; elements along the dimension &lt;code&gt;seq_axis&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7225b3ea22ec4deb1adfd4a695bd1a7623e154b" translate="yes" xml:space="preserve">
          <source>This op implements the CTC loss as presented in the article:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2221b86f1776b0a82d12fa22eaebb9f19fcdaeda" translate="yes" xml:space="preserve">
          <source>This op is a convenience wrapper around &lt;code&gt;sparse_to_dense&lt;/code&gt; for &lt;code&gt;SparseTensor&lt;/code&gt;s.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="498581ecde1f847b44ae5faacdf090686c5a82aa" translate="yes" xml:space="preserve">
          <source>This op is conceptually identical to,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d1fddd748b0417b58ea1729816edada8cf15be10" translate="yes" xml:space="preserve">
          <source>This op is deprecated. See &lt;a href=&quot;../../../nn/batch_normalization&quot;&gt;&lt;code&gt;tf.nn.batch_normalization&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="350e179bf73709c36862e34c13946cb30d6be41a" translate="yes" xml:space="preserve">
          <source>This op is deprecated. See &lt;a href=&quot;batch_normalization&quot;&gt;&lt;code&gt;tf.nn.batch_normalization&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bfe73bfb3165baeb1cd799227e5df39e5edc40e8" translate="yes" xml:space="preserve">
          <source>This op is equivalent to applying the normal &lt;a href=&quot;../nn/softmax&quot;&gt;&lt;code&gt;tf.nn.softmax()&lt;/code&gt;&lt;/a&gt; to each innermost logical submatrix with shape &lt;code&gt;[B, C]&lt;/code&gt;, but with the catch that &lt;em&gt;the implicitly zero elements do not participate&lt;/em&gt;. Specifically, the algorithm is equivalent to:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf3e5eda158a5f12c2f2da1b9b93833d5ff71185" translate="yes" xml:space="preserve">
          <source>This op is only defined for complex matrices. If A is positive-definite and real, then casting to a complex matrix, taking the logarithm and casting back to a real matrix will give the correct result.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b4088dd89f372d9134efcda4f21dccb2a4e43e51" translate="yes" xml:space="preserve">
          <source>This op is similar to &lt;code&gt;tf.strings.decode(...)&lt;/code&gt;, but it also returns the start offset for each character in its respective string. This information can be used to align the characters with the original byte sequence.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="30dc65726d5913cc5cf094798e27be8207b68832" translate="yes" xml:space="preserve">
          <source>This op is used during session initialization when a Scaffold is initialized without specifying the local_init_op arg. It includes &lt;a href=&quot;../local_variables_initializer&quot;&gt;&lt;code&gt;tf.compat.v1.local_variables_initializer&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../tables_initializer&quot;&gt;&lt;code&gt;tf.compat.v1.tables_initializer&lt;/code&gt;&lt;/a&gt;, and also initializes local session resources.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7494eb2e4d00b6f1a2a80a896b2a97c11ea84bbb" translate="yes" xml:space="preserve">
          <source>This op only parses the image header, so it is much faster than DecodeJpeg.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="256678fd9ba7a89e98c94dad29bebb27865a6123" translate="yes" xml:space="preserve">
          <source>This op parses a serialized sequence example into a tuple of dictionaries, each mapping keys to &lt;code&gt;Tensor&lt;/code&gt; and &lt;code&gt;SparseTensor&lt;/code&gt; objects. The first dictionary contains mappings for keys appearing in &lt;code&gt;context_features&lt;/code&gt;, and the second dictionary contains mappings for keys appearing in &lt;code&gt;sequence_features&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b74d19b6cde7e379b7fc76473e78e58968efe064" translate="yes" xml:space="preserve">
          <source>This op parses serialized examples into a dictionary mapping keys to &lt;code&gt;Tensor&lt;/code&gt;, &lt;code&gt;SparseTensor&lt;/code&gt;, and &lt;code&gt;RaggedTensor&lt;/code&gt; objects. &lt;code&gt;features&lt;/code&gt; is a dict from keys to &lt;code&gt;VarLenFeature&lt;/code&gt;, &lt;code&gt;RaggedFeature&lt;/code&gt;, &lt;code&gt;SparseFeature&lt;/code&gt;, and &lt;code&gt;FixedLenFeature&lt;/code&gt; objects. Each &lt;code&gt;VarLenFeature&lt;/code&gt; and &lt;code&gt;SparseFeature&lt;/code&gt; is mapped to a &lt;code&gt;SparseTensor&lt;/code&gt;; each &lt;code&gt;RaggedFeature&lt;/code&gt; is mapped to a &lt;code&gt;RaggedTensor&lt;/code&gt;; and each &lt;code&gt;FixedLenFeature&lt;/code&gt; is mapped to a &lt;code&gt;Tensor&lt;/code&gt;. See &lt;a href=&quot;../../io/parse_example&quot;&gt;&lt;code&gt;tf.io.parse_example&lt;/code&gt;&lt;/a&gt; for more details about feature dictionaries.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="845b399d2e9c5818b169dacff650d84da1b8a72a" translate="yes" xml:space="preserve">
          <source>This op parses serialized examples into a dictionary mapping keys to &lt;code&gt;Tensor&lt;/code&gt;&lt;code&gt;SparseTensor&lt;/code&gt;, and &lt;code&gt;RaggedTensor&lt;/code&gt; objects. &lt;code&gt;features&lt;/code&gt; is a dict from keys to &lt;code&gt;VarLenFeature&lt;/code&gt;, &lt;code&gt;SparseFeature&lt;/code&gt;, &lt;code&gt;RaggedFeature&lt;/code&gt;, and &lt;code&gt;FixedLenFeature&lt;/code&gt; objects. Each &lt;code&gt;VarLenFeature&lt;/code&gt; and &lt;code&gt;SparseFeature&lt;/code&gt; is mapped to a &lt;code&gt;SparseTensor&lt;/code&gt;; each &lt;code&gt;FixedLenFeature&lt;/code&gt; is mapped to a &lt;code&gt;Tensor&lt;/code&gt;; and each &lt;code&gt;RaggedFeature&lt;/code&gt; is mapped to a &lt;code&gt;RaggedTensor&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="787b46fef668ea2442d8cf59e886173e8f1beeb1" translate="yes" xml:space="preserve">
          <source>This op parses serialized sequence examples into a tuple of dictionaries, each mapping keys to &lt;code&gt;Tensor&lt;/code&gt; and &lt;code&gt;SparseTensor&lt;/code&gt; objects. The first dictionary contains mappings for keys appearing in &lt;code&gt;context_features&lt;/code&gt;, and the second dictionary contains mappings for keys appearing in &lt;code&gt;sequence_features&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ca1b1701aa5c064253c44ce1aad741cb653df7b" translate="yes" xml:space="preserve">
          <source>This op reports an &lt;code&gt;InvalidArgument&lt;/code&gt; error if any value is not finite.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a524bdef26722689a646035ff3a30a89b59c896f" translate="yes" xml:space="preserve">
          <source>This op runs in &lt;code&gt;O(M log M)&lt;/code&gt; time, where &lt;code&gt;M&lt;/code&gt; is the total number of non-empty values across all inputs. This is due to the need for an internal sort in order to concatenate efficiently across an arbitrary dimension.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="edecba4e86376242d273ba622ca63b2965a5a438" translate="yes" xml:space="preserve">
          <source>This op translates a tensor containing Example records, encoded using the &lt;a href=&quot;https://developers.google.com/protocol-buffers/docs/proto3#json&quot;&gt;standard JSON mapping&lt;/a&gt;, into a tensor containing the same records encoded as binary protocol buffers. The resulting tensor can then be fed to any of the other Example-parsing ops.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49b534f6ddaaf2bd4e7772b31b4be9cf92fcfbd7" translate="yes" xml:space="preserve">
          <source>This operation blocks until that finishes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14d876872d62a220ab5753fd3b852a5a6bcb76ef" translate="yes" xml:space="preserve">
          <source>This operation can be used with &lt;code&gt;output_encoding = input_encoding&lt;/code&gt; to enforce correct formatting for inputs even if they are already in the desired encoding.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf8af72c7ec5c03a0a302901ae0b02f8840934f4" translate="yes" xml:space="preserve">
          <source>This operation computes</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df2c8f127a35805203dd552ccd88657669212f89" translate="yes" xml:space="preserve">
          <source>This operation computes the inverse of an index permutation. It takes a 1-D integer tensor &lt;code&gt;x&lt;/code&gt;, which represents the indices of a zero-based array, and swaps each value with its index position. In other words, for an output tensor &lt;code&gt;y&lt;/code&gt; and an input tensor &lt;code&gt;x&lt;/code&gt;, this operation computes the following:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d8b4d24b16bc4a84c7216a06caa8665f602e4b2d" translate="yes" xml:space="preserve">
          <source>This operation concatenates queue-element component tensors along the 0th dimension to make a single component tensor. All of the components in the dequeued tuple will have size &lt;code&gt;n&lt;/code&gt; in the 0th dimension.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50e9d3be55bb6834daac8ca12d0b390851428378" translate="yes" xml:space="preserve">
          <source>This operation concatenates queue-element component tensors along the 0th dimension to make a single component tensor. If the queue has not been closed, all of the components in the dequeued tuple will have size &lt;code&gt;n&lt;/code&gt; in the 0th dimension.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45295c6769989505efcf8c49b52125eb620acebf" translate="yes" xml:space="preserve">
          <source>This operation converts Unicode code points to script codes corresponding to each code point. Script codes correspond to International Components for Unicode (ICU) UScriptCode values. See http://icu-project.org/apiref/icu4c/uscript_8h.html. Returns -1 (USCRIPT_INVALID_CODE) for invalid codepoints. Output shape will match input shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="46e590d89bb172f52098449de9dab107e8dbb7d2" translate="yes" xml:space="preserve">
          <source>This operation corresponds to &lt;code&gt;numpy.tensordot(a, b, axes)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11cc84360970a3caa94b4965487c085edd1841b7" translate="yes" xml:space="preserve">
          <source>This operation creates a new tensor by adding sparse &lt;code&gt;updates&lt;/code&gt; to the passed in &lt;code&gt;tensor&lt;/code&gt;. This operation is very similar to &lt;code&gt;tf.scatter_nd_add&lt;/code&gt;, except that the updates are added onto an existing tensor (as opposed to a variable). If the memory for the existing tensor cannot be re-used, a copy is made and updated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c390e65b4cc57f97f7e73aea3a4e81f629906485" translate="yes" xml:space="preserve">
          <source>This operation creates a new tensor by applying sparse &lt;code&gt;updates&lt;/code&gt; to the passed in &lt;code&gt;tensor&lt;/code&gt;. This operation is very similar to &lt;a href=&quot;scatter_nd&quot;&gt;&lt;code&gt;tf.scatter_nd&lt;/code&gt;&lt;/a&gt;, except that the updates are scattered onto an existing tensor (as opposed to a zero-tensor). If the memory for the existing tensor cannot be re-used, a copy is made and updated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35ada26abd351b9c5bf627e9862c2252c700b1b5" translate="yes" xml:space="preserve">
          <source>This operation creates a new tensor by replicating &lt;code&gt;input&lt;/code&gt;&lt;code&gt;multiples&lt;/code&gt; times. The output tensor's i'th dimension has &lt;code&gt;input.dims(i) * multiples[i]&lt;/code&gt; elements, and the values of &lt;code&gt;input&lt;/code&gt; are replicated &lt;code&gt;multiples[i]&lt;/code&gt; times along the 'i'th dimension. For example, tiling &lt;code&gt;[a b c d]&lt;/code&gt; by &lt;code&gt;[2]&lt;/code&gt; produces &lt;code&gt;[a b c d a b c d]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c90c8c96fbb8b355bef6addc97e2ca668679eab" translate="yes" xml:space="preserve">
          <source>This operation creates a new tensor by subtracting sparse &lt;code&gt;updates&lt;/code&gt; from the passed in &lt;code&gt;tensor&lt;/code&gt;. This operation is very similar to &lt;code&gt;tf.scatter_nd_sub&lt;/code&gt;, except that the updates are subtracted from an existing tensor (as opposed to a variable). If the memory for the existing tensor cannot be re-used, a copy is made and updated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0aaf5e994752ce058c39711d4638a06e40443f56" translate="yes" xml:space="preserve">
          <source>This operation creates a tensor of shape &lt;code&gt;dims&lt;/code&gt; and fills it with &lt;code&gt;value&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2acd7b755c2b5e055f5081527ae8768ab15af691" translate="yes" xml:space="preserve">
          <source>This operation divides &quot;spatial&quot; dimensions &lt;code&gt;[1, ..., M]&lt;/code&gt; of the input into a grid of blocks of shape &lt;code&gt;block_shape&lt;/code&gt;, and interleaves these blocks with the &quot;batch&quot; dimension (0) such that in the output, the spatial dimensions &lt;code&gt;[1, ..., M]&lt;/code&gt; correspond to the position within the grid, and the batch dimension combines both the position within a spatial block and the original batch position. Prior to division into blocks, the spatial dimensions of the input are optionally zero padded according to &lt;code&gt;paddings&lt;/code&gt;. See below for a precise description.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d3e7305e25146c306e9ca9dac7912d7b2d4ded2" translate="yes" xml:space="preserve">
          <source>This operation extracts a slice of size &lt;code&gt;size&lt;/code&gt; from a tensor &lt;code&gt;input_&lt;/code&gt; starting at the location specified by &lt;code&gt;begin&lt;/code&gt;. The slice &lt;code&gt;size&lt;/code&gt; is represented as a tensor shape, where &lt;code&gt;size[i]&lt;/code&gt; is the number of elements of the 'i'th dimension of &lt;code&gt;input_&lt;/code&gt; that you want to slice. The starting location (&lt;code&gt;begin&lt;/code&gt;) for the slice is represented as an offset in each dimension of &lt;code&gt;input_&lt;/code&gt;. In other words, &lt;code&gt;begin[i]&lt;/code&gt; is the offset into the i'th dimension of &lt;code&gt;input_&lt;/code&gt; that you want to slice from.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="29ab911f3d597d49d14cf454f1533a4b19e5e15b" translate="yes" xml:space="preserve">
          <source>This operation extracts the specified region from the tensor. The notation is similar to NumPy with the restriction that currently only support basic indexing. That means that using a non-scalar tensor as input is not currently allowed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d3ec2c39ae0a5c24c1de4e22eea70e184fc371a" translate="yes" xml:space="preserve">
          <source>This operation has a gradient and thus allows for training &lt;code&gt;min&lt;/code&gt; and &lt;code&gt;max&lt;/code&gt; values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b78d17d35c34babe04bca7ab5360f7b9d4aad4d2" translate="yes" xml:space="preserve">
          <source>This operation has the same semantics as &lt;code&gt;reshape&lt;/code&gt; on the represented dense tensor. The indices of non-empty values in &lt;code&gt;sp_input&lt;/code&gt; are recomputed based on the new dense shape, and a new &lt;code&gt;SparseTensor&lt;/code&gt; is returned containing the new indices and new shape. The order of non-empty values in &lt;code&gt;sp_input&lt;/code&gt; is unchanged.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1bd3f36c5bf6b7e281276b8f3d298543774510a1" translate="yes" xml:space="preserve">
          <source>This operation is a no-op when executing eagerly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c77c133cb2aa87b66c50cea9b24338cf94df91e2" translate="yes" xml:space="preserve">
          <source>This operation is equivalent to the following steps:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a85336f9b64b0941e75fed8873dd343470ffb664" translate="yes" xml:space="preserve">
          <source>This operation is for training only. It is generally an underestimate of the full softmax loss.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6db0a8622252241b3dd8e4da9fc3d6ba84977a9c" translate="yes" xml:space="preserve">
          <source>This operation is related to &lt;code&gt;squeeze()&lt;/code&gt;, which removes dimensions of size 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49c94ac01402b2bd4ac870cdfc937202b7e69050" translate="yes" xml:space="preserve">
          <source>This operation is related to:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f8a4568939439baca88acb2c04e8494963c44c55" translate="yes" xml:space="preserve">
          <source>This operation is significantly more numerically stable than the equivalent tensorflow operation &lt;code&gt;tf.math.log(tf.math.cumsum(tf.math.exp(x)))&lt;/code&gt;, although computes the same result given infinite numerical precision. However, note that in some cases, it may be less stable than &lt;a href=&quot;reduce_logsumexp&quot;&gt;&lt;code&gt;tf.math.reduce_logsumexp&lt;/code&gt;&lt;/a&gt; for a given element, as it applies the &quot;log-sum-exp trick&quot; in a different way.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0cce4836efcc5d3f1c90cad3d4974cf4b16512f4" translate="yes" xml:space="preserve">
          <source>This operation is similar to tensor_scatter_add, except that the tensor is zero-initialized. Calling &lt;a href=&quot;scatter_nd&quot;&gt;&lt;code&gt;tf.scatter_nd(indices, values, shape)&lt;/code&gt;&lt;/a&gt; is identical to &lt;code&gt;tensor_scatter_add(tf.zeros(shape, values.dtype), indices, values)&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50f836730593fffe361ee867e1bf79d15e8af917" translate="yes" xml:space="preserve">
          <source>This operation is sometimes called &quot;deconvolution&quot; after &lt;a href=&quot;http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf&quot;&gt;Deconvolutional Networks&lt;/a&gt;, but is actually the transpose (gradient) of &lt;code&gt;conv2d&lt;/code&gt; rather than an actual deconvolution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cdd40bbfb18d0876cdd3d1bf655721101051d082" translate="yes" xml:space="preserve">
          <source>This operation is sometimes called &quot;deconvolution&quot; after &lt;a href=&quot;http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf&quot;&gt;Deconvolutional Networks&lt;/a&gt;, but is actually the transpose (gradient) of &lt;code&gt;convolution&lt;/code&gt; rather than an actual deconvolution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a80e63d8e3617ef22a7c11bc4ac5ce2aac92b83c" translate="yes" xml:space="preserve">
          <source>This operation is sometimes called &quot;deconvolution&quot; after &lt;a href=&quot;https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf&quot;&gt;Deconvolutional Networks&lt;/a&gt;, but is really the transpose (gradient) of &lt;code&gt;atrous_conv2d&lt;/code&gt; rather than an actual deconvolution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e16e89f92d27118cbc90336d321edbdccabf257" translate="yes" xml:space="preserve">
          <source>This operation is sometimes called &quot;deconvolution&quot; after &lt;a href=&quot;https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf&quot;&gt;Deconvolutional Networks&lt;/a&gt;, but is really the transpose (gradient) of &lt;code&gt;conv1d&lt;/code&gt; rather than an actual deconvolution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="74c54a1240dd3ebac671477622bdae5730c68b14" translate="yes" xml:space="preserve">
          <source>This operation is sometimes called &quot;deconvolution&quot; after &lt;a href=&quot;https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf&quot;&gt;Deconvolutional Networks&lt;/a&gt;, but is really the transpose (gradient) of &lt;code&gt;conv2d&lt;/code&gt; rather than an actual deconvolution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78632da26e5444bf46256b6e1c192a603e6c3c00" translate="yes" xml:space="preserve">
          <source>This operation is sometimes called &quot;deconvolution&quot; after &lt;a href=&quot;https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf&quot;&gt;Deconvolutional Networks&lt;/a&gt;, but is really the transpose (gradient) of &lt;code&gt;conv3d&lt;/code&gt; rather than an actual deconvolution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="20f728b5bb5e16f6250024eb3cf614e43cfbbe07" translate="yes" xml:space="preserve">
          <source>This operation is typically used to clip gradients before applying them with an optimizer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5397809ea0e6b96d049101c13960159515304d67" translate="yes" xml:space="preserve">
          <source>This operation is useful for resizing the activations between convolutions (but keeping all data), e.g. instead of pooling. It is also useful for training purely convolutional models.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e6cbba91c74639c8cb40de16428e2bef0563e905" translate="yes" xml:space="preserve">
          <source>This operation is useful if you want to add a batch dimension to a single element. For example, if you have a single image of shape &lt;code&gt;[height, width, channels]&lt;/code&gt;, you can make it a batch of 1 image with &lt;code&gt;expand_dims(image, 0)&lt;/code&gt;, which will make the shape &lt;code&gt;[1, height, width, channels]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6316e455129ed9fa859e28aacc9886a66315035c" translate="yes" xml:space="preserve">
          <source>This operation is useful if you want to add a batch dimension to a single element. For example, if you have a single image of shape &lt;code&gt;[height, width, channels]&lt;/code&gt;, you can make it a batch of one image with &lt;code&gt;expand_dims(image, 0)&lt;/code&gt;, which will make the shape &lt;code&gt;[1, height, width, channels]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d1d3aca1842cf510032065cfce3509b5c793096" translate="yes" xml:space="preserve">
          <source>This operation outputs &quot;ref&quot; after the update is done. This makes it easier to chain operations that need to use the reset value. Unlike &lt;a href=&quot;../../math/add&quot;&gt;&lt;code&gt;tf.math.add&lt;/code&gt;&lt;/a&gt;, this op does not broadcast. &lt;code&gt;ref&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt; must have the same shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e16eef80cf37c2d9c02fad5e714a6e559684854" translate="yes" xml:space="preserve">
          <source>This operation outputs &lt;code&gt;ref&lt;/code&gt; after the update is done. This makes it easier to chain operations that need to use the reset value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80080e1227c1f588b580293eb11f01d8754cc2fa" translate="yes" xml:space="preserve">
          <source>This operation outputs &lt;code&gt;ref&lt;/code&gt; after the update is done. This makes it easier to chain operations that need to use the reset value. Unlike &lt;a href=&quot;../../math/subtract&quot;&gt;&lt;code&gt;tf.math.subtract&lt;/code&gt;&lt;/a&gt;, this op does not broadcast. &lt;code&gt;ref&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt; must have the same shape.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="211cf88f878719f0fdbc3eba7700cfd25f62b94c" translate="yes" xml:space="preserve">
          <source>This operation outputs &lt;code&gt;ref&lt;/code&gt; after the update is done. This makes it easier to chain operations that need to use the updated value. Duplicate entries are handled correctly: if multiple &lt;code&gt;indices&lt;/code&gt; reference the same location, their contributions add.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f7141134307e0a174b2d64cd3ba50899cf746d15" translate="yes" xml:space="preserve">
          <source>This operation outputs a Tensor that holds the new value of &lt;code&gt;ref&lt;/code&gt; after the value has been assigned. This makes it easier to chain operations that need to use the reset value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d8dcb952f1fd946dc8a99ab12f18c6c9f3bad230" translate="yes" xml:space="preserve">
          <source>This operation pads a &lt;code&gt;tensor&lt;/code&gt; according to the &lt;code&gt;paddings&lt;/code&gt; you specify. &lt;code&gt;paddings&lt;/code&gt; is an integer tensor with shape &lt;code&gt;[n, 2]&lt;/code&gt;, where n is the rank of &lt;code&gt;tensor&lt;/code&gt;. For each dimension D of &lt;code&gt;input&lt;/code&gt;, &lt;code&gt;paddings[D, 0]&lt;/code&gt; indicates how many values to add before the contents of &lt;code&gt;tensor&lt;/code&gt; in that dimension, and &lt;code&gt;paddings[D, 1]&lt;/code&gt; indicates how many values to add after the contents of &lt;code&gt;tensor&lt;/code&gt; in that dimension. If &lt;code&gt;mode&lt;/code&gt; is &quot;REFLECT&quot; then both &lt;code&gt;paddings[D, 0]&lt;/code&gt; and &lt;code&gt;paddings[D, 1]&lt;/code&gt; must be no greater than &lt;code&gt;tensor.dim_size(D) - 1&lt;/code&gt;. If &lt;code&gt;mode&lt;/code&gt; is &quot;SYMMETRIC&quot; then both &lt;code&gt;paddings[D, 0]&lt;/code&gt; and &lt;code&gt;paddings[D, 1]&lt;/code&gt; must be no greater than &lt;code&gt;tensor.dim_size(D)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ece6bb1bca3b5a4f71fcb2a4c7bb7def98bb6471" translate="yes" xml:space="preserve">
          <source>This operation performs non_max_suppression on the inputs per batch, across all classes. Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Note that this algorithm is agnostic to where the origin is in the coordinate system. Also note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm. The output of this operation is the final boxes, scores and classes tensor returned after performing non_max_suppression.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92bf8506361b0c3190382a71e7f126a1495f07cd" translate="yes" xml:space="preserve">
          <source>This operation randomly samples a tensor of sampled classes (&lt;code&gt;sampled_candidates&lt;/code&gt;) from the range of integers &lt;code&gt;[0, range_max)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2656ea13a552cbb0836e0031c2afa8bfe371a2e1" translate="yes" xml:space="preserve">
          <source>This operation requires that:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85455e262fab5e8f9fc55d0016d69887ed46e6fb" translate="yes" xml:space="preserve">
          <source>This operation reshapes the &quot;batch&quot; dimension 0 into &lt;code&gt;M + 1&lt;/code&gt; dimensions of shape &lt;code&gt;block_shape + [batch]&lt;/code&gt;, interleaves these blocks back into the grid defined by the spatial dimensions &lt;code&gt;[1, ..., M]&lt;/code&gt;, to obtain a result with the same rank as the input. The spatial dimensions of this intermediate result are then optionally cropped according to &lt;code&gt;crops&lt;/code&gt; to produce the output. This is the reverse of SpaceToBatch. See below for a precise description.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe8665b789d92e174b87e3b8d8b3131e0c5a95d4" translate="yes" xml:space="preserve">
          <source>This operation returns a 1-D integer tensor representing the shape of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a59463d2fe92b4018e2beec28525b7ad3bbf202" translate="yes" xml:space="preserve">
          <source>This operation returns a tensor &lt;code&gt;y&lt;/code&gt; containing all of the unique elements of &lt;code&gt;x&lt;/code&gt; sorted in the same order that they occur in &lt;code&gt;x&lt;/code&gt;. This operation also returns a tensor &lt;code&gt;idx&lt;/code&gt; the same size as &lt;code&gt;x&lt;/code&gt; that contains the index of each value of &lt;code&gt;x&lt;/code&gt; in the unique output &lt;code&gt;y&lt;/code&gt;. Finally, it returns a third tensor &lt;code&gt;count&lt;/code&gt; that contains the count of each element of &lt;code&gt;y&lt;/code&gt; in &lt;code&gt;x&lt;/code&gt;. In other words:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="536871e049e74061332eb50e7568b95429f16ba6" translate="yes" xml:space="preserve">
          <source>This operation returns a tensor &lt;code&gt;y&lt;/code&gt; containing all of the unique elements of &lt;code&gt;x&lt;/code&gt; sorted in the same order that they occur in &lt;code&gt;x&lt;/code&gt;; &lt;code&gt;x&lt;/code&gt; does not need to be sorted. This operation also returns a tensor &lt;code&gt;idx&lt;/code&gt; the same size as &lt;code&gt;x&lt;/code&gt; that contains the index of each value of &lt;code&gt;x&lt;/code&gt; in the unique output &lt;code&gt;y&lt;/code&gt;. In other words:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08bfd9d4b0608003900efe52523c1412fe2e10f1" translate="yes" xml:space="preserve">
          <source>This operation returns a tensor of type &lt;code&gt;dtype&lt;/code&gt; with shape &lt;code&gt;shape&lt;/code&gt; and all elements set to one.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="46403ed383d4150a23258a456b0faa0e79d56dbe" translate="yes" xml:space="preserve">
          <source>This operation returns a tensor of type &lt;code&gt;dtype&lt;/code&gt; with shape &lt;code&gt;shape&lt;/code&gt; and all elements set to zero.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="476cbc978c015f266a3353221077ac3bad3036de" translate="yes" xml:space="preserve">
          <source>This operation returns a tensor with the &lt;code&gt;diagonal&lt;/code&gt; part of the &lt;code&gt;input&lt;/code&gt;. The &lt;code&gt;diagonal&lt;/code&gt; part is computed as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e2b8bbf455ec7dda7bf033c368dcb34b158ee06e" translate="yes" xml:space="preserve">
          <source>This operation returns the same result as the C++ std::nextafter function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cba2fef602700f559d54f64ca39679c7fe5a39cf" translate="yes" xml:space="preserve">
          <source>This operation returns true if the queue is closed and false if the queue is open.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc131f95b04b052706f9dc5a2e0863ccce212278" translate="yes" xml:space="preserve">
          <source>This operation signals that no more elements will be enqueued in the given queue. Subsequent &lt;code&gt;enqueue&lt;/code&gt; and &lt;code&gt;enqueue_many&lt;/code&gt; operations will fail. Subsequent &lt;code&gt;dequeue&lt;/code&gt; and &lt;code&gt;dequeue_many&lt;/code&gt; operations will continue to succeed if sufficient elements remain in the queue. Subsequently dequeue and dequeue_many operations that would otherwise block waiting for more elements (if close hadn't been called) will now fail immediately.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed8ef290710f5b794405ab349bc8048a526b3051" translate="yes" xml:space="preserve">
          <source>This operation slices each component tensor along the 0th dimension to make multiple queue elements. All of the tensors in &lt;code&gt;vals&lt;/code&gt; must have the same size in the 0th dimension.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b06acef6473406f9a6972f0bdd65c856a80109e7" translate="yes" xml:space="preserve">
          <source>This operation takes variable-length sequences (&lt;code&gt;hypothesis&lt;/code&gt; and &lt;code&gt;truth&lt;/code&gt;), each provided as a &lt;code&gt;SparseTensor&lt;/code&gt;, and computes the Levenshtein distance. You can normalize the edit distance by length of &lt;code&gt;truth&lt;/code&gt; by setting &lt;code&gt;normalize&lt;/code&gt; to true.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4b844fb17a610af4c2dfbda43ce1ac8c2ea894a7" translate="yes" xml:space="preserve">
          <source>This operation tends to perform well when &lt;code&gt;A&lt;/code&gt; is more sparse, if the column size of the product is small (e.g. matrix-vector multiplication), if &lt;code&gt;sp_a.dense_shape&lt;/code&gt; takes on large values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1051e1cb11d005837380c93974db8cb36d834ab5" translate="yes" xml:space="preserve">
          <source>This operation will generate a string suitable to be saved out to create a .wav audio file. It will be encoded in the 16-bit PCM format. It takes in float values in the range -1.0f to 1.0f, and any outside that value will be clamped to that range.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf6b9f0d6051cd83422b6ad2cb1617b5b6c44a65" translate="yes" xml:space="preserve">
          <source>This operation will output a tensor of shape &lt;code&gt;[1, 1, 1, 4]&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="01c0abdcd1d63eef5d525eb4d621b9072cafca4f" translate="yes" xml:space="preserve">
          <source>This operation will output a tensor of shape &lt;code&gt;[1, 2, 2, 1]&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9c1bb170834121c155816075bc30535b34c3026" translate="yes" xml:space="preserve">
          <source>This operation would return the following:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c09d355015c640623e774187c625203da78fb3a6" translate="yes" xml:space="preserve">
          <source>This operation would return:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6585b279d1753d7936601f5ce821925656bfd5b2" translate="yes" xml:space="preserve">
          <source>This operation, for block size of 2, will return the following tensor of shape &lt;code&gt;[1, 2, 2, 3]&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="123e5c864ddc20e423825601cb576b64af53b67e" translate="yes" xml:space="preserve">
          <source>This operation, for block_size of 2, will return the following tensor of shape &lt;code&gt;[1, 1, 1, 12]&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2a0a130eb7e8314a609a2db135e21936c4a0de8b" translate="yes" xml:space="preserve">
          <source>This operator acts like a (batch) matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, M, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;m x n&lt;/code&gt; matrix. Again, this matrix &lt;code&gt;A&lt;/code&gt; may not be materialized, but for purposes of identifying and working with compatible arguments the shape is relevant.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9624d2140d2af8604c19c56292430ec7141d96ff" translate="yes" xml:space="preserve">
          <source>This operator acts like a [batch] Toeplitz matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x N&lt;/code&gt; matrix. This matrix &lt;code&gt;A&lt;/code&gt; is not materialized, but for purposes of broadcasting this shape will be relevant.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ae991b61103c038a5ab6e26593ecbc7152e117f2" translate="yes" xml:space="preserve">
          <source>This operator acts like a [batch] diagonal matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x N&lt;/code&gt; matrix. This matrix &lt;code&gt;A&lt;/code&gt; is not materialized, but for purposes of broadcasting this shape will be relevant.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5eea0437af7326077ca375b7627f553585750e30" translate="yes" xml:space="preserve">
          <source>This operator acts like a [batch] identity matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x N&lt;/code&gt; matrix. This matrix &lt;code&gt;A&lt;/code&gt; is not materialized, but for purposes of broadcasting this shape will be relevant.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7036bf9c433fed406d0eb267aee69b2fb875c75f" translate="yes" xml:space="preserve">
          <source>This operator acts like a [batch] lower triangular matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x N&lt;/code&gt; matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d5825859d807d72eb10161ef6b5b940c6fe0b9b" translate="yes" xml:space="preserve">
          <source>This operator acts like a [batch] matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, M, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;M x N&lt;/code&gt; matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f99131310964b3eb6cd5147dace10a254e9fff2" translate="yes" xml:space="preserve">
          <source>This operator acts like a [batch] of householder reflections with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x N&lt;/code&gt; matrix. This matrix &lt;code&gt;A&lt;/code&gt; is not materialized, but for purposes of broadcasting this shape will be relevant.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c9497726784db48f14f606b75baa3af89ad92c3" translate="yes" xml:space="preserve">
          <source>This operator acts like a [batch] of permutations with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x N&lt;/code&gt; matrix. This matrix &lt;code&gt;A&lt;/code&gt; is not materialized, but for purposes of broadcasting this shape will be relevant.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="400eb5597c413c726e34a9f1e1bb7b69ea62bb78" translate="yes" xml:space="preserve">
          <source>This operator acts like a [batch] zero matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, N, M]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x M&lt;/code&gt; matrix. This matrix &lt;code&gt;A&lt;/code&gt; is not materialized, but for purposes of broadcasting this shape will be relevant.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7102c53ecb6c13af874ea168315014a6e7dc84b" translate="yes" xml:space="preserve">
          <source>This operator acts like a block circulant matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x N&lt;/code&gt; matrix. This matrix &lt;code&gt;A&lt;/code&gt; is not materialized, but for purposes of broadcasting this shape will be relevant.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c216274d20b8ffa480f3dfffdabad95ff0cda74" translate="yes" xml:space="preserve">
          <source>This operator acts like a circulant matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x N&lt;/code&gt; matrix. This matrix &lt;code&gt;A&lt;/code&gt; is not materialized, but for purposes of broadcasting this shape will be relevant.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f0a750eb3c4adb095723453eef766bb4bcd89f8" translate="yes" xml:space="preserve">
          <source>This operator acts like a scaled [batch] identity matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is a scaled version of the &lt;code&gt;N x N&lt;/code&gt; identity matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b1442fc44774d80a4540791883d0146d00de836" translate="yes" xml:space="preserve">
          <source>This operator acts on [batch] matrix with compatible shape. &lt;code&gt;x&lt;/code&gt; is a batch matrix with compatible shape for &lt;code&gt;matmul&lt;/code&gt; and &lt;code&gt;solve&lt;/code&gt; if</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4840abea8050987eeb44a958dc938a96dc5772f" translate="yes" xml:space="preserve">
          <source>This operator acts on batch matrices with compatible shape. FILL IN WHAT IS MEANT BY COMPATIBLE SHAPE</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="53228a3fac3cf2f50966b1ada24ef3b0466deb1e" translate="yes" xml:space="preserve">
          <source>This operator combines one or more linear operators &lt;code&gt;[op1,...,opJ]&lt;/code&gt;, building a new &lt;code&gt;LinearOperator&lt;/code&gt;, whose underlying matrix representation is square and has each operator &lt;code&gt;opi&lt;/code&gt; on the main diagonal, and zero's elsewhere.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="564367f646774e8a861547adb060150a45779d5f" translate="yes" xml:space="preserve">
          <source>This operator composes one or more linear operators &lt;code&gt;[op1,...,opJ]&lt;/code&gt;, building a new &lt;code&gt;LinearOperator&lt;/code&gt; representing the Kronecker product: &lt;code&gt;op1 x op2 x .. opJ&lt;/code&gt; (we omit parentheses as the Kronecker product is associative).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c0d85ebac206e74618bf7317ca7c96fff15c835" translate="yes" xml:space="preserve">
          <source>This operator composes one or more linear operators &lt;code&gt;[op1,...,opJ]&lt;/code&gt;, building a new &lt;code&gt;LinearOperator&lt;/code&gt; with action defined by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e7241eaa8846c5c4338cb18a140b07ab3b606ae" translate="yes" xml:space="preserve">
          <source>This operator corresponds to a real matrix if and only if &lt;code&gt;H&lt;/code&gt; is Hermitian.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f9242e838252063460c0276fd01dfac0ef01ff4" translate="yes" xml:space="preserve">
          <source>This operator corresponds to a real-valued matrix if and only if its spectrum is Hermitian.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c618e51b272c823eef5139b9cc3a948f981f98e8" translate="yes" xml:space="preserve">
          <source>This operator is able to broadcast the leading (batch) dimensions, which sometimes requires copying data. If &lt;code&gt;batch_shape&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, the operator can take arguments of any batch shape without copying. See examples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66cb218ad1e2e8f68e038b90d3bfb45bd6e74e63" translate="yes" xml:space="preserve">
          <source>This operator is able to broadcast the leading (batch) dimensions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="348a23cb6559c85829c515ac4244685e0efb46f2" translate="yes" xml:space="preserve">
          <source>This operator is considered non-singular if</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d06a5054c0e8f5a488a0e50140ca1317d8ee8b76" translate="yes" xml:space="preserve">
          <source>This operator is positive definite if and only if &lt;code&gt;Real{H} &amp;gt; 0&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa787331edbdbf77b6543a837e62e8acfa555fdd" translate="yes" xml:space="preserve">
          <source>This operator is self-adjoint if and only if &lt;code&gt;H&lt;/code&gt; is real.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0434efb4cb6fe37aceabe23ccb563dea7d82d49a" translate="yes" xml:space="preserve">
          <source>This operator is similar to the unsorted segment sum operator found &lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;(here)&lt;/a&gt;. Instead of computing the sum over segments, it computes the maximum such that:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0203b5b1fedf802a969aad64e740ba2ad439f1c" translate="yes" xml:space="preserve">
          <source>This operator is similar to the unsorted segment sum operator found &lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;(here)&lt;/a&gt;. Instead of computing the sum over segments, it computes the minimum such that:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b525fce235ebc8f800bcb2ded78049fa74f120a" translate="yes" xml:space="preserve">
          <source>This operator is similar to the unsorted segment sum operator found &lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;(here)&lt;/a&gt;. Instead of computing the sum over segments, it computes the product of all entries belonging to a segment such that:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="59fd9e153dcbd12bffedb241b5f88000e9237c11" translate="yes" xml:space="preserve">
          <source>This operator is similar to the unsorted segment sum operator found &lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;here&lt;/a&gt;. Additionally to computing the sum over segments, it divides the results by sqrt(N).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4a7cb1afea6269197bd76bbd4732c1dbecdf7fc" translate="yes" xml:space="preserve">
          <source>This operator is similar to the unsorted segment sum operator found &lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;here&lt;/a&gt;. Instead of computing the sum over segments, it computes the mean of all entries belonging to a segment such that:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="83929e057e5aae77fa1c5e82493e5f81ffd7c7d5" translate="yes" xml:space="preserve">
          <source>This operator represents the adjoint of another operator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="011faa1fcbdac7f7db43d7798ee8f420d25df1c3" translate="yes" xml:space="preserve">
          <source>This operator represents the inverse of another operator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="caac5d3bbf4d0945a7bd985970cd4106cce90915" translate="yes" xml:space="preserve">
          <source>This operator wraps a [batch] matrix &lt;code&gt;A&lt;/code&gt; (which is a &lt;code&gt;Tensor&lt;/code&gt;) with shape &lt;code&gt;[B1,...,Bb, M, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;M x N&lt;/code&gt; matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7a64029550dddacbaa84a5c9b1e8fcd91b9538b" translate="yes" xml:space="preserve">
          <source>This optimizer class is &lt;a href=&quot;../../distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt; aware, which means it automatically sums gradients across all replicas. To average gradients, you divide your loss by the global batch size, which is done automatically if you use &lt;a href=&quot;../../keras&quot;&gt;&lt;code&gt;tf.keras&lt;/code&gt;&lt;/a&gt; built-in training or evaluation loops. See the &lt;code&gt;reduction&lt;/code&gt; argument of your loss which should be set to &lt;a href=&quot;../losses/reduction#SUM_OVER_BATCH_SIZE&quot;&gt;&lt;code&gt;tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE&lt;/code&gt;&lt;/a&gt; for averaging or &lt;a href=&quot;../losses/reduction#SUM&quot;&gt;&lt;code&gt;tf.keras.losses.Reduction.SUM&lt;/code&gt;&lt;/a&gt; for not.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9ca3ab79a1177cb06784d426253ffabbbc2a6a4" translate="yes" xml:space="preserve">
          <source>This optimizer takes care of regularization of unseen features in a mini batch by updating them when they are seen with a closed form update rule that is equivalent to having updated them on every mini-batch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a8b5c268bde39d241894c5b95721103ebdbc425" translate="yes" xml:space="preserve">
          <source>This optimizer wraps another optimizer and applies loss scaling to it via a &lt;code&gt;LossScale&lt;/code&gt;. Loss scaling is applied whenever gradients are computed, either through &lt;code&gt;minimize()&lt;/code&gt; or &lt;code&gt;get_gradients()&lt;/code&gt;. The loss scale is updated via &lt;a href=&quot;../../../mixed_precision/experimental/lossscale#update&quot;&gt;&lt;code&gt;LossScale.update()&lt;/code&gt;&lt;/a&gt; whenever gradients are applied, either through &lt;code&gt;minimize()&lt;/code&gt; or &lt;code&gt;apply_gradients()&lt;/code&gt;. For example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a13f36128710145f1603618c3c0bcd6462911f2d" translate="yes" xml:space="preserve">
          <source>This optimizer wraps another optimizer and applies loss scaling to it via a &lt;code&gt;LossScale&lt;/code&gt;. Loss scaling is applied whenever gradients are computed, such as through &lt;code&gt;minimize()&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="43b5cb445e044712f2e69a5344f02f5be0ddda93" translate="yes" xml:space="preserve">
          <source>This outputs a &lt;code&gt;batch_size&lt;/code&gt; bool array, an entry &lt;code&gt;out[i]&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt; if the prediction for the target class is finite (not inf, -inf, or nan) and among the top &lt;code&gt;k&lt;/code&gt; predictions among all predictions for example &lt;code&gt;i&lt;/code&gt;. Note that the behavior of &lt;code&gt;InTopK&lt;/code&gt; differs from the &lt;code&gt;TopK&lt;/code&gt; op in its handling of ties; if multiple classes have the same prediction value and straddle the top-&lt;code&gt;k&lt;/code&gt; boundary, all of those classes are considered to be in the top &lt;code&gt;k&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="345380820e023cc35c074a4e6039205226041719" translate="yes" xml:space="preserve">
          <source>This overload raises a &lt;code&gt;TypeError&lt;/code&gt; when the user inadvertently treats a &lt;code&gt;Tensor&lt;/code&gt; as a boolean (most commonly in an &lt;code&gt;if&lt;/code&gt; or &lt;code&gt;while&lt;/code&gt; statement), in code that was not converted by AutoGraph. For example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f63c68c6b6d06032eeae666e8d05a28d8b6e750" translate="yes" xml:space="preserve">
          <source>This package defines ops for manipulating ragged tensors (&lt;a href=&quot;../../raggedtensor&quot;&gt;&lt;code&gt;tf.RaggedTensor&lt;/code&gt;&lt;/a&gt;), which are tensors with non-uniform shapes. In particular, each &lt;code&gt;RaggedTensor&lt;/code&gt; has one or more &lt;em&gt;ragged dimensions&lt;/em&gt;, which are dimensions whose slices may have different lengths. For example, the inner (column) dimension of &lt;code&gt;rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []]&lt;/code&gt; is ragged, since the column slices (&lt;code&gt;rt[0, :]&lt;/code&gt;, ..., &lt;code&gt;rt[4, :]&lt;/code&gt;) have different lengths. For a more detailed description of ragged tensors, see the &lt;a href=&quot;../../raggedtensor&quot;&gt;&lt;code&gt;tf.RaggedTensor&lt;/code&gt;&lt;/a&gt; class documentation and the &lt;a href=&quot;https://www.tensorflow.org/guide/ragged_tensors&quot;&gt;Ragged Tensor Guide&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e4f0989d73eb8b679bcdf7fff408eece47a531f" translate="yes" xml:space="preserve">
          <source>This package defines ops for manipulating ragged tensors (&lt;a href=&quot;raggedtensor&quot;&gt;&lt;code&gt;tf.RaggedTensor&lt;/code&gt;&lt;/a&gt;), which are tensors with non-uniform shapes. In particular, each &lt;code&gt;RaggedTensor&lt;/code&gt; has one or more &lt;em&gt;ragged dimensions&lt;/em&gt;, which are dimensions whose slices may have different lengths. For example, the inner (column) dimension of &lt;code&gt;rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []]&lt;/code&gt; is ragged, since the column slices (&lt;code&gt;rt[0, :]&lt;/code&gt;, ..., &lt;code&gt;rt[4, :]&lt;/code&gt;) have different lengths. For a more detailed description of ragged tensors, see the &lt;a href=&quot;raggedtensor&quot;&gt;&lt;code&gt;tf.RaggedTensor&lt;/code&gt;&lt;/a&gt; class documentation and the &lt;a href=&quot;https://www.tensorflow.org/guide/ragged_tensors&quot;&gt;Ragged Tensor Guide&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca381bc7587c1d86ccea71cf77a3f51f18c01785" translate="yes" xml:space="preserve">
          <source>This partitioner will shard a Variable along one axis, attempting to keep the maximum shard size below &lt;code&gt;max_shard_bytes&lt;/code&gt;. In practice, this is not always possible when sharding along only one axis. When this happens, this axis is sharded as much as possible (i.e., every dimension becomes a separate shard).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd457ac6c886ad4d6a9b1c2c9ed70e07ea8ad467" translate="yes" xml:space="preserve">
          <source>This produces files called &quot;timeline-</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="650852868a3688b5e43945d526af2b7a7abb7659" translate="yes" xml:space="preserve">
          <source>This regressor ignores feature values and will learn to predict the average value of each label.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b4e11e2fc93d180cb8cb79e60f96113138df290" translate="yes" xml:space="preserve">
          <source>This returns a ClusterSpec object for use based on information from the specified initialization parameters and Slurm environment variables. The cluster specification is resolved each time this function is called. The resolver extract hostnames of nodes by scontrol and pack tasks in that order until a node a has number of tasks that is equal to specification. GPUs on nodes are allocated to tasks by specification through setting CUDA_VISIBLE_DEVICES environment variable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ce42bdd47f55adee51dc4ae146a85efc32785746" translate="yes" xml:space="preserve">
          <source>This returns a ClusterSpec object for use based on information from the specified instance group. We will retrieve the information from the GCE APIs every time this method is called.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d4e36425f3d932370ce3f31b5b37e6bd2c5dbb8b" translate="yes" xml:space="preserve">
          <source>This returns a function outputting &lt;code&gt;features&lt;/code&gt; and &lt;code&gt;targets&lt;/code&gt; based on the dict of numpy arrays. The dict &lt;code&gt;features&lt;/code&gt; has the same keys as the &lt;code&gt;x&lt;/code&gt;. The dict &lt;code&gt;targets&lt;/code&gt; has the same keys as the &lt;code&gt;y&lt;/code&gt; if &lt;code&gt;y&lt;/code&gt; is a dict.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="091f487524e5d0cdc7f2f7122f0e27fecc0512d2" translate="yes" xml:space="preserve">
          <source>This returns the job name and task index for the process which calls this function according to its rank and cluster specification. The job name and task index are set after a cluster is constructed by cluster_spec otherwise defaults to None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e86e167f9dfaedec2287e6304deddcf9a677504" translate="yes" xml:space="preserve">
          <source>This returns the number of accelerator cores (such as GPUs and TPUs) available per worker.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="43f97f599a6e5c9f5b6d870e441e77232b9c4ee2" translate="yes" xml:space="preserve">
          <source>This sampler is useful when the target classes approximately follow such a distribution - for example, if the classes represent words in a lexicon sorted in decreasing order of frequency. If your classes are not ordered by decreasing frequency, do not use this op.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4116f5a13b723775fce909b3d142c6ace24a3f4" translate="yes" xml:space="preserve">
          <source>This set may grow over time, so it's important the signature of creators is as mentioned above.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6fe6f784df295b3702cbf845894c09377130127e" translate="yes" xml:space="preserve">
          <source>This simply wraps &lt;code&gt;compute_gradients()&lt;/code&gt; from the real optimizer. The gradients will be aggregated in &lt;code&gt;apply_gradients()&lt;/code&gt; so that user can modify the gradients like clipping with per replica global norm if needed. The global norm with aggregated gradients can be bad as one replica's huge gradients can hurt the gradients from other replicas.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8231e710a02b190ee62057d8ce9e6843f584733f" translate="yes" xml:space="preserve">
          <source>This simply wraps the compute_gradients() from the real optimizer. The gradients will be aggregated in the apply_gradients() so that user can modify the gradients like clipping with per replica global norm if needed. The global norm with aggregated gradients can be bad as one replica's huge gradients can hurt the gradients from other replicas.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9535c2c156931877173be6ce0ef4fcc7c39ade4d" translate="yes" xml:space="preserve">
          <source>This simply wraps the get_slot() from the actual optimizer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="627b6a914c45360f47c7270bf7239e5630834ac1" translate="yes" xml:space="preserve">
          <source>This simply wraps the get_slot_names() from the actual optimizer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="481aba790f7180557002a29e34e41b2834d82d24" translate="yes" xml:space="preserve">
          <source>This starts services in the background. The services started depend on the parameters to the constructor and may include:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="61ba97b856d34fcc84e3cd4a4201db78d7018540" translate="yes" xml:space="preserve">
          <source>This strategy implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to &lt;a href=&quot;../../../../distribute/mirroredstrategy&quot;&gt;&lt;code&gt;tf.distribute.MirroredStrategy&lt;/code&gt;&lt;/a&gt;, it creates copies of all variables in the model on each device across all workers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f19046d3b0cb6c64ecb2b4f8764cc9d33c4854f1" translate="yes" xml:space="preserve">
          <source>This strategy implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to &lt;a href=&quot;../mirroredstrategy&quot;&gt;&lt;code&gt;tf.distribute.MirroredStrategy&lt;/code&gt;&lt;/a&gt;, it creates copies of all variables in the model on each device across all workers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="648f5d6b8ceebd17f689c7c0ab8ef16660a7747c" translate="yes" xml:space="preserve">
          <source>This strategy requires two jobs: workers and parameter servers. Variables and updates to those variables will be assigned to parameter servers and other operations are assigned to workers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="609b539c1ba8e5fb73c25fa137b65336a8f05b90" translate="yes" xml:space="preserve">
          <source>This strategy uses one replica per device and sync replication for its multi-GPU version.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f77a1c7f27ca177eb5967fdc02f021a08479ef96" translate="yes" xml:space="preserve">
          <source>This symbol is also exported to v2 in tf.estimator namespace. See https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/hooks/basic_session_run_hooks.py</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee30a785484036470f77ee9c09ff132832518ed4" translate="yes" xml:space="preserve">
          <source>This takes an ordinary &lt;code&gt;dataset&lt;/code&gt; and &lt;code&gt;replica_fn&lt;/code&gt; and runs it distributed using a particular &lt;a href=&quot;strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt; named &lt;code&gt;my_strategy&lt;/code&gt; above. Any variables created in &lt;code&gt;replica_fn&lt;/code&gt; are created using &lt;code&gt;my_strategy&lt;/code&gt;'s policy, and library functions called by &lt;code&gt;replica_fn&lt;/code&gt; can use the &lt;code&gt;get_replica_context()&lt;/code&gt; API to implement distributed-specific behavior.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f43e50d6d041bdda53ec0ccc184fa3940b822601" translate="yes" xml:space="preserve">
          <source>This takes in a few parameters and creates a GCEClusterResolver project. It will then use these parameters to query the GCE API for the IP addresses of each instance in the instance group.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dcbb15c135f93b202dd4afc6d928c452a899b686" translate="yes" xml:space="preserve">
          <source>This takes in parameters and creates a SlurmClusterResolver object. It uses those parameters to check which nodes will processes reside on and resolves their hostnames. With the number of the GPUs on each node and number of GPUs for each task it offsets the port number for each process and allocates GPUs to tasks by setting environment variables. The resolver currently supports homogeneous tasks and default Slurm process allocation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c32e52f6772f69af41778414a120795e360f2147" translate="yes" xml:space="preserve">
          <source>This thread class is intended to be used with a &lt;code&gt;Coordinator&lt;/code&gt;. It repeatedly runs code specified either as &lt;code&gt;target&lt;/code&gt; and &lt;code&gt;args&lt;/code&gt; or by the &lt;code&gt;run_loop()&lt;/code&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cbe44aae232eb7beccd1bb717d150f53dfed09d2" translate="yes" xml:space="preserve">
          <source>This tracking then allows saving variable values to &lt;a href=&quot;https://www.tensorflow.org/guide/checkpoint&quot;&gt;training checkpoints&lt;/a&gt;, or to &lt;a href=&quot;https://www.tensorflow.org/guide/saved_model&quot;&gt;SavedModels&lt;/a&gt; which include serialized TensorFlow graphs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="84d3d178d35882538e5cb6113338132ed1567a47" translate="yes" xml:space="preserve">
          <source>This transformation applies &lt;code&gt;map_func&lt;/code&gt; to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. &lt;code&gt;map_func&lt;/code&gt; can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b91ea223c9ec8adff0faaecc5c9b88a252c4af69" translate="yes" xml:space="preserve">
          <source>This transformation combines multiple consecutive elements of the input dataset into a single element.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35f6c3c6efa792b9215faf94c8c00f35d19886a0" translate="yes" xml:space="preserve">
          <source>This transformation is a stateful relative of &lt;a href=&quot;../dataset#map&quot;&gt;&lt;code&gt;tf.data.Dataset.map&lt;/code&gt;&lt;/a&gt;. In addition to mapping &lt;code&gt;scan_func&lt;/code&gt; across the elements of the input dataset, &lt;code&gt;scan()&lt;/code&gt; accumulates one or more state tensors, whose initial values are &lt;code&gt;initial_state&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e7258027c5f693626ed2b2e68a04576c8a8c285" translate="yes" xml:space="preserve">
          <source>This transformation maps each consecutive element in a dataset to a key using &lt;code&gt;key_func&lt;/code&gt; and groups the elements by key. It then applies &lt;code&gt;reduce_func&lt;/code&gt; to at most &lt;code&gt;window_size_func(key)&lt;/code&gt; elements matching the same key. All except the final window for each key will contain &lt;code&gt;window_size_func(key)&lt;/code&gt; elements; the final window may be smaller.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a21aaa6c0a7b4d11f0bde650d661805100a01ad7" translate="yes" xml:space="preserve">
          <source>This transformation maps element of a dataset to a key using &lt;code&gt;key_func&lt;/code&gt; and groups the elements by key. The &lt;code&gt;reducer&lt;/code&gt; is used to process each group; its &lt;code&gt;init_func&lt;/code&gt; is used to initialize state for each group when it is created, the &lt;code&gt;reduce_func&lt;/code&gt; is used to update the state every time an element is mapped to the matching group, and the &lt;code&gt;finalize_func&lt;/code&gt; is used to map the final state to an output value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="24ab6194aaa04fc157672b906f20ba2e1b1ec8cc" translate="yes" xml:space="preserve">
          <source>This updates the checkpoint file containing a CheckpointState proto.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5bb7cb617dbb67bdd4c7f55f6d436fa2f1b33e3c" translate="yes" xml:space="preserve">
          <source>This uses &lt;a href=&quot;../norm&quot;&gt;&lt;code&gt;tf.linalg.norm&lt;/code&gt;&lt;/a&gt; to compute the norm along &lt;code&gt;axis&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7945ebd1b4ae121cdd796a53759129e7e59a0846" translate="yes" xml:space="preserve">
          <source>This usually returns the master from the first ClusterResolver passed in, but you can override this by specifying the task_type and task_id.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4285af5e3616d9c051e0b5382ab0649693cc39b6" translate="yes" xml:space="preserve">
          <source>This utility function provides consistent behavior for both local (non-distributed) and distributed configurations. The default distribution configuration is parameter server-based between-graph replication. For other types of distribution configurations such as all-reduce training, please use &lt;a href=&quot;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute&quot;&gt;DistributionStrategies&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50e5c41b60a6dacb228de3d2a27ac083ca26570e" translate="yes" xml:space="preserve">
          <source>This utility function trains, evaluates, and (optionally) exports the model by using the given &lt;code&gt;estimator&lt;/code&gt;. All training related specification is held in &lt;code&gt;train_spec&lt;/code&gt;, including training &lt;code&gt;input_fn&lt;/code&gt; and training max steps, etc. All evaluation and export related specification is held in &lt;code&gt;eval_spec&lt;/code&gt;, including evaluation &lt;code&gt;input_fn&lt;/code&gt;, steps, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="370bf5deaf68ca405b51cda0b005ed741251919f" translate="yes" xml:space="preserve">
          <source>This utility method replaces the deprecated-in-V2 &lt;code&gt;tf.compat.v1.Dataset.output_classes&lt;/code&gt; property.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="46213afe2d7d2a75cd43b97252d79901207a3be4" translate="yes" xml:space="preserve">
          <source>This utility method replaces the deprecated-in-V2 &lt;code&gt;tf.compat.v1.Dataset.output_shapes&lt;/code&gt; property.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="917e4af53cd9619ae0480dec3aee8643df47493a" translate="yes" xml:space="preserve">
          <source>This utility method replaces the deprecated-in-V2 &lt;code&gt;tf.compat.v1.Dataset.output_types&lt;/code&gt; property.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e1997fea43a5c765a33c5bb8c3d18f8bd998811c" translate="yes" xml:space="preserve">
          <source>This value is ultimately returned as &lt;code&gt;auc&lt;/code&gt;, an idempotent operation that computes the area under a discretized curve of precision versus recall values (computed using the aforementioned variables). The &lt;code&gt;num_thresholds&lt;/code&gt; variable controls the degree of discretization with larger numbers of thresholds more closely approximating the true AUC. The quality of the approximation may vary dramatically depending on &lt;code&gt;num_thresholds&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65677d697851bf6819d55d4ff4251a12696e0771" translate="yes" xml:space="preserve">
          <source>This value is ultimately returned as &lt;code&gt;auc&lt;/code&gt;, an idempotent operation that computes the area under a discretized curve of precision versus recall values (computed using the aforementioned variables). The &lt;code&gt;num_thresholds&lt;/code&gt; variable controls the degree of discretization with larger numbers of thresholds more closely approximating the true AUC. The quality of the approximation may vary dramatically depending on &lt;code&gt;num_thresholds&lt;/code&gt;. The &lt;code&gt;thresholds&lt;/code&gt; parameter can be used to manually specify thresholds which split the predictions more evenly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca17fcf3f560f3e845deeb7ae0f38bd32e476e10" translate="yes" xml:space="preserve">
          <source>This version enqueues a different list of tensors in different threads. It adds the following to the current &lt;code&gt;Graph&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56d946ffec0b3da9d0ec8b7171e5c9d7eac7532e" translate="yes" xml:space="preserve">
          <source>This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a36f7273ef31d9cbeb00b22a3f221c68cd1e40a0" translate="yes" xml:space="preserve">
          <source>This version performs the same function as Dropout, however it drops entire 2D feature maps instead of individual elements. If adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout2D will help promote independence between feature maps and should be used instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d0ef991814863883d725a0aaf896bf4a46fb3d16" translate="yes" xml:space="preserve">
          <source>This version performs the same function as Dropout, however it drops entire 3D feature maps instead of individual elements. If adjacent voxels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout3D will help promote independence between feature maps and should be used instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d54f43064a89a76630348aa955f910392f5b82d" translate="yes" xml:space="preserve">
          <source>This will clear all caches, even those that are maintained through sequential calls to tf.tpu.experimental.initialize_tpu_system, such as the compilation cache.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d1d5a90de0626fc107829e4415b87374a7e5143d" translate="yes" xml:space="preserve">
          <source>This wrapper allows to apply a layer to every temporal slice of an input.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4911cf935e00a37c587b392c63c4d1259cd5359" translate="yes" xml:space="preserve">
          <source>This wraps &lt;code&gt;func_&lt;/code&gt; in a Template and partially evaluates it. Templates are functions that create variables the first time they are called and reuse them thereafter. In order for &lt;code&gt;func_&lt;/code&gt; to be compatible with a &lt;code&gt;Template&lt;/code&gt; it must have the following properties:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9934a34210519520663a7f89fe0d027dd3ebea0c" translate="yes" xml:space="preserve">
          <source>This wraps &lt;code&gt;variables()&lt;/code&gt; from the actual optimizer. It does not include the &lt;code&gt;SyncReplicasOptimizer&lt;/code&gt;'s local step.</source>
          <target state="new"/>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
