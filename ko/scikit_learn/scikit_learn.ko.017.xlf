<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="a344504140059db6f88458066c961354f795c6a7" translate="yes" xml:space="preserve">
          <source>This method has some performance overhead hence it is better to call partial_fit on chunks of data that are as large as possible (as long as fitting in the memory budget) to hide the overhead.</source>
          <target state="translated">이 방법에는 약간의 성능 오버 헤드가 있으므로 오버 헤드를 숨기려면 가능한 한 (메모리 예산에 맞는 한) 데이터 청크에 대해 partial_fit을 호출하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="ceeeb1e178fb9959d4ffe786c9917a8fc68013bb" translate="yes" xml:space="preserve">
          <source>This method has the same order of complexity than an &lt;a href=&quot;#ordinary-least-squares&quot;&gt;Ordinary Least Squares&lt;/a&gt;.</source>
          <target state="translated">이 방법은 &lt;a href=&quot;#ordinary-least-squares&quot;&gt;일반 최소 제곱&lt;/a&gt; 과 동일한 순서의 복잡성을 갖습니다 .</target>
        </trans-unit>
        <trans-unit id="8149d43c4db3023940e20ddee18dfd4caaeb24b1" translate="yes" xml:space="preserve">
          <source>This method is expected to be called several times consecutively on different chunks of a dataset so as to implement out-of-core or online learning.</source>
          <target state="translated">이 방법은 코어 외부 또는 온라인 학습을 구현하기 위해 데이터 집합의 여러 청크에서 연속적으로 여러 번 호출 될 것으로 예상됩니다.</target>
        </trans-unit>
        <trans-unit id="7914e16bc2009e2d4fc76b2006a65a031a42eb76" translate="yes" xml:space="preserve">
          <source>This method is just there to implement the usual API and hence work in pipelines.</source>
          <target state="translated">이 방법은 일반적인 API를 구현하기위한 것이므로 파이프 라인에서 작동합니다.</target>
        </trans-unit>
        <trans-unit id="53a4e9f6af590018ff1a37b6f68db2405dd79282" translate="yes" xml:space="preserve">
          <source>This method is just there to mark the fact that this transformer can work in a streaming setup.</source>
          <target state="translated">이 방법은이 변환기가 스트리밍 설정에서 작동 할 수 있다는 사실을 표시하기위한 것입니다.</target>
        </trans-unit>
        <trans-unit id="1ad1f3e2c791502dcf55d0ea0c930c86843ade76" translate="yes" xml:space="preserve">
          <source>This method is meant to be called concurrently by the multiprocessing callback. We rely on the thread-safety of dispatch_one_batch to protect against concurrent consumption of the unprotected iterator.</source>
          <target state="translated">이 메소드는 멀티 프로세싱 콜백에 의해 동시에 호출됩니다. 우리는 dispatch_one_batch의 스레드 안전성에 의존하여 보호되지 않은 반복자의 동시 소비를 방지합니다.</target>
        </trans-unit>
        <trans-unit id="4e81340dab29281a8d6b3bd99833383bb408f46c" translate="yes" xml:space="preserve">
          <source>This method is not deterministic: it computes a quantity called the free energy on X, then on a randomly corrupted version of X, and returns the log of the logistic function of the difference.</source>
          <target state="translated">이 방법은 결정적이지 않습니다. X에서 자유 에너지라고하는 수량을 계산 한 다음 임의로 손상된 X 버전에서 수량을 계산하고 차이의 로지스틱 함수 로그를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="483c17ab697933f17e74386d9739e36cf3fc93e7" translate="yes" xml:space="preserve">
          <source>This method is only available for log loss and modified Huber loss.</source>
          <target state="translated">이 방법은 로그 손실 및 수정 된 Huber 손실에만 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d7475ebc10f647671bee9a4be7afee1b81279276" translate="yes" xml:space="preserve">
          <source>This method provides a safe way to take a distance matrix as input, while preserving compatibility with many other algorithms that take a vector array.</source>
          <target state="translated">이 방법은 거리 행렬을 입력으로 취하는 안전한 방법을 제공하면서 벡터 배열을 사용하는 다른 많은 알고리즘과의 호환성을 유지합니다.</target>
        </trans-unit>
        <trans-unit id="b5d8e2fef5ebecb0c66f96f2926a64438412f355" translate="yes" xml:space="preserve">
          <source>This method provides a safe way to take a kernel matrix as input, while preserving compatibility with many other algorithms that take a vector array.</source>
          <target state="translated">이 방법은 커널 행렬을 입력으로 취하는 안전한 방법을 제공하면서 벡터 배열을 취하는 다른 많은 알고리즘과의 호환성을 유지합니다.</target>
        </trans-unit>
        <trans-unit id="0fc2db598aaa9c1a0947d8f73a1238d30285a532" translate="yes" xml:space="preserve">
          <source>This method takes either a vector array or a distance matrix, and returns a distance matrix. If the input is a vector array, the distances are computed. If the input is a distances matrix, it is returned instead.</source>
          <target state="translated">이 메서드는 벡터 배열 또는 거리 행렬을 가져 와서 거리 행렬을 반환합니다. 입력이 벡터 배열 인 경우 거리가 계산됩니다. 입력 값이 거리 행렬이면 대신 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="924736c0bae89c3f4376281e6a105fa549739eb4" translate="yes" xml:space="preserve">
          <source>This method takes either a vector array or a kernel matrix, and returns a kernel matrix. If the input is a vector array, the kernels are computed. If the input is a kernel matrix, it is returned instead.</source>
          <target state="translated">이 메서드는 벡터 배열이나 커널 행렬을 가져 와서 커널 행렬을 반환합니다. 입력이 벡터 배열 인 경우 커널이 계산됩니다. 입력이 커널 행렬이면 대신 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="b80df7fbbffdde8aff9c30af4a5bee17e602075b" translate="yes" xml:space="preserve">
          <source>This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme.</source>
          <target state="translated">이 방법은 피쳐를 균일하거나 정규 분포를 따르도록 변환합니다. 따라서 특정 기능에 대해이 변환은 가장 빈번한 값을 퍼뜨리는 경향이 있습니다. 또한 (마진) 이상치의 영향을 줄입니다. 따라서 강력한 전처리 체계입니다.</target>
        </trans-unit>
        <trans-unit id="4a6448f2646e45809baed97b35289a513191212b" translate="yes" xml:space="preserve">
          <source>This method works similarly to the builtin &lt;code&gt;apply&lt;/code&gt;, except that the function is called only if the cache is not up to date.</source>
          <target state="translated">이 메소드 는 캐시가 최신 상태가 아닌 경우에만 함수가 호출된다는 점을 제외하고 내장 &lt;code&gt;apply&lt;/code&gt; 와 유사하게 작동 합니다.</target>
        </trans-unit>
        <trans-unit id="3b270b097c02b54b15c6706faba2a05992e48391" translate="yes" xml:space="preserve">
          <source>This metric is furthermore symmetric: switching &lt;code&gt;label_true&lt;/code&gt; with &lt;code&gt;label_pred&lt;/code&gt; will return the same score value. This can be useful to measure the agreement of two independent label assignments strategies on the same dataset when the real ground truth is not known.</source>
          <target state="translated">이 메트릭은 더 대칭 적입니다. &lt;code&gt;label_true&lt;/code&gt; 로 &lt;code&gt;label_pred&lt;/code&gt; 를 전환 하면 동일한 점수 값이 반환됩니다. 이는 실제 사실을 알 수없는 경우 동일한 데이터 세트에 대해 두 개의 독립적 인 레이블 지정 전략의 일치를 측정하는 데 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b8da4b4fabd4786b82c03e2c15a17659173e15c8" translate="yes" xml:space="preserve">
          <source>This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won&amp;rsquo;t change the score value in any way.</source>
          <target state="translated">이 메트릭은 레이블의 절대 값과 무관합니다. 클래스 또는 클러스터 레이블 값의 순열은 점수 값을 변경하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="bfbb6fef2be45da43d1153172735208301268ab3" translate="yes" xml:space="preserve">
          <source>This metric is not symmetric: switching &lt;code&gt;label_true&lt;/code&gt; with &lt;code&gt;label_pred&lt;/code&gt; will return the &lt;a href=&quot;sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt;&lt;code&gt;completeness_score&lt;/code&gt;&lt;/a&gt; which will be different in general.</source>
          <target state="translated">이 메트릭은 대칭이 아닙니다. &lt;code&gt;label_true&lt;/code&gt; 로 &lt;code&gt;label_pred&lt;/code&gt; 를 전환 하면 일반적으로 다른 &lt;a href=&quot;sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt; &lt;code&gt;completeness_score&lt;/code&gt; &lt;/a&gt; 를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="d6ecae2ce63387462768b5daf2f548b32eba4de4" translate="yes" xml:space="preserve">
          <source>This metric is not symmetric: switching &lt;code&gt;label_true&lt;/code&gt; with &lt;code&gt;label_pred&lt;/code&gt; will return the &lt;a href=&quot;sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt;&lt;code&gt;homogeneity_score&lt;/code&gt;&lt;/a&gt; which will be different in general.</source>
          <target state="translated">이 메트릭은 대칭이 아닙니다. &lt;code&gt;label_true&lt;/code&gt; 로 &lt;code&gt;label_pred&lt;/code&gt; 를 전환 하면 일반적으로 다른 &lt;a href=&quot;sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt; &lt;code&gt;homogeneity_score&lt;/code&gt; &lt;/a&gt; 가 반환 됩니다.</target>
        </trans-unit>
        <trans-unit id="e3d29b108d9b9b14881da7a1115d336ab614cf19" translate="yes" xml:space="preserve">
          <source>This metric is used in multilabel ranking problem, where the goal is to give better rank to the labels associated to each sample.</source>
          <target state="translated">이 메트릭은 다중 레이블 순위 문제에서 사용되며, 목표는 각 샘플과 관련된 레이블의 순위를 높이는 것입니다.</target>
        </trans-unit>
        <trans-unit id="70091de439c388c847d5db9bb63c11ef6af9aff3" translate="yes" xml:space="preserve">
          <source>This might be made more clear by an example:</source>
          <target state="translated">이것은 예를 통해 더 명확해질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="87c35466b9ea86a2466ad7ff0fff224c499553d9" translate="yes" xml:space="preserve">
          <source>This model has many parameters, however the default values are quite reasonable (please see the &lt;a href=&quot;classes#text-feature-extraction-ref&quot;&gt;reference documentation&lt;/a&gt; for the details):</source>
          <target state="translated">이 모델에는 많은 매개 변수가 있지만 기본값은 상당히 합리적입니다 (자세한 내용은 &lt;a href=&quot;classes#text-feature-extraction-ref&quot;&gt;참조 설명서&lt;/a&gt; 를 참조 하십시오).</target>
        </trans-unit>
        <trans-unit id="a96824cdb923fbde7424d806cedfbff3d185c97a" translate="yes" xml:space="preserve">
          <source>This model is an extension of the Sequential Karhunen-Loeve Transform from: &lt;code&gt;A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and its Application to Images, IEEE Transactions on Image Processing, Volume 9, Number 8, pp. 1371-1374, August 2000.&lt;/code&gt; See &lt;a href=&quot;http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf&quot;&gt;http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf&lt;/a&gt;</source>
          <target state="translated">이 모델은 &lt;code&gt;A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and its Application to Images, IEEE Transactions on Image Processing, Volume 9, Number 8, pp. 1371-1374, August 2000.&lt;/code&gt; 참조 &lt;a href=&quot;http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf&quot;&gt;http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5eaf31e8c5d94894f814f950dadb507546a62c7a" translate="yes" xml:space="preserve">
          <source>This model is similar to the basic Label Propagation algorithm, but uses affinity matrix based on the normalized graph Laplacian and soft clamping across the labels.</source>
          <target state="translated">이 모델은 기본 레이블 전파 알고리즘과 유사하지만 정규화 된 그래프 라플라시안 및 레이블 전체의 소프트 클램핑을 기반으로 선호도 매트릭스를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="31cfc2556e1a899818d23b9328ee0744c892d322" translate="yes" xml:space="preserve">
          <source>This model optimizes the log-loss function using LBFGS or stochastic gradient descent.</source>
          <target state="translated">이 모델은 LBFGS 또는 확률 적 경사 하강을 사용하여 로그 손실 기능을 최적화합니다.</target>
        </trans-unit>
        <trans-unit id="a4064d8d27531f23ac21cbcbd5928e344df2525d" translate="yes" xml:space="preserve">
          <source>This model optimizes the squared-loss using LBFGS or stochastic gradient descent.</source>
          <target state="translated">이 모델은 LBFGS 또는 확률 적 경사 하강을 사용하여 제곱 손실을 최적화합니다.</target>
        </trans-unit>
        <trans-unit id="5745ffae87fdf8e03232a3372f515cd928402ef0" translate="yes" xml:space="preserve">
          <source>This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).</source>
          <target state="translated">이 모델은 손실 함수가 선형 최소 제곱 함수이고 정규화가 l2-norm으로 제공되는 회귀 모델을 해결합니다. 릿지 회귀 또는 Tikhonov 정규화라고도합니다. 이 추정기는 다변량 회귀를 기본적으로 지원합니다 (즉, y가 2 차원 배열 [n_samples, n_targets] 인 경우).</target>
        </trans-unit>
        <trans-unit id="f19c01936c0bc27e43d782c2c60b0838b0f4894d" translate="yes" xml:space="preserve">
          <source>This module contains both distance metrics and kernels. A brief summary is given on the two here.</source>
          <target state="translated">이 모듈에는 거리 측정치와 커널이 모두 포함되어 있습니다. 여기에 두 가지 간단한 요약이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="ea7156035377b3d98062532f66578932992ce326" translate="yes" xml:space="preserve">
          <source>This module contains two loaders. The first one, &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt;&lt;/a&gt;, returns a list of the raw texts that can be fed to text feature extractors such as &lt;a href=&quot;../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt; with custom parameters so as to extract feature vectors. The second one, &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized#sklearn.datasets.fetch_20newsgroups_vectorized&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_20newsgroups_vectorized&lt;/code&gt;&lt;/a&gt;, returns ready-to-use features, i.e., it is not necessary to use a feature extractor.</source>
          <target state="translated">이 모듈에는 두 개의 로더가 있습니다. 첫 번째 &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt; &lt;/a&gt; 는 &lt;a href=&quot;../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt; 와 같은 텍스트 피처 추출기에 공급할 수있는 원시 텍스트의 목록을 반환하여 피처 벡터를 추출합니다. 두 번째 &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized#sklearn.datasets.fetch_20newsgroups_vectorized&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_20newsgroups_vectorized&lt;/code&gt; &lt;/a&gt; 는 즉시 사용 가능한 기능을 반환합니다. 즉, 기능 추출기를 사용할 필요가 없습니다.</target>
        </trans-unit>
        <trans-unit id="e5dbda685e3f3b6ebc21c265d6368ea8386c5f03" translate="yes" xml:space="preserve">
          <source>This module implements multiclass learning algorithms:</source>
          <target state="translated">이 모듈은 멀티 클래스 학습 알고리즘을 구현합니다.</target>
        </trans-unit>
        <trans-unit id="40fee252ae7de928b5fc80e9416986beafe64ef4" translate="yes" xml:space="preserve">
          <source>This module implements multioutput regression and classification.</source>
          <target state="translated">이 모듈은 다중 출력 회귀 및 분류를 구현합니다.</target>
        </trans-unit>
        <trans-unit id="f4b5a1fcc345642615c5317116147407ee22511b" translate="yes" xml:space="preserve">
          <source>This module offers support for multi-output problems by implementing this strategy in both &lt;a href=&quot;generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt;&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.tree.decisiontreeregressor#sklearn.tree.DecisionTreeRegressor&quot;&gt;&lt;code&gt;DecisionTreeRegressor&lt;/code&gt;&lt;/a&gt;. If a decision tree is fit on an output array Y of size &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt; then the resulting estimator will:</source>
          <target state="translated">이 모듈은 &lt;a href=&quot;generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt; &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.tree.decisiontreeregressor#sklearn.tree.DecisionTreeRegressor&quot;&gt; &lt;code&gt;DecisionTreeRegressor&lt;/code&gt; &lt;/a&gt; 모두에서이 전략을 구현하여 다중 출력 문제를 지원합니다 . 결정 트리가 &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt; 크기의 출력 배열 Y에 맞는 경우 결과 추정기는 다음을 수행합니다.</target>
        </trans-unit>
        <trans-unit id="69b4d83c7d3d58178d932db005c229bef475361e" translate="yes" xml:space="preserve">
          <source>This normalization is implemented by the &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; class:</source>
          <target state="translated">이 정규화는 &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt; 클래스에 의해 구현됩니다 .</target>
        </trans-unit>
        <trans-unit id="36bde3d3011eb8df53f7587f509f261b5588364f" translate="yes" xml:space="preserve">
          <source>This object uses workers to compute in parallel the application of a function to many different arguments. The main functionality it brings in addition to using the raw multiprocessing or concurrent.futures API are (see examples for details):</source>
          <target state="translated">이 객체는 워커를 사용하여 여러 다른 인수에 대한 함수 적용을 병렬로 계산합니다. 원시 멀티 프로세싱 또는 동시 .futures API 사용과 함께 제공되는 주요 기능은 다음과 같습니다 (자세한 내용은 예제 참조).</target>
        </trans-unit>
        <trans-unit id="43f85d31a1122b5ce913b6ac1587dc97b9fac8c9" translate="yes" xml:space="preserve">
          <source>This package also features helpers to fetch larger datasets commonly used by the machine learning community to benchmark algorithms on data that comes from the &amp;lsquo;real world&amp;rsquo;.</source>
          <target state="translated">이 패키지에는 머신 러닝 커뮤니티에서 일반적으로 사용하는 '대규모'데이터에서 알고리즘을 벤치마킹하기 위해 일반적으로 사용하는 더 큰 데이터 세트를 가져 오는 도우미도 있습니다.</target>
        </trans-unit>
        <trans-unit id="f678792533c8ea573ae326139ccc463721df5e43" translate="yes" xml:space="preserve">
          <source>This parameter has been renamed to n_components and will be removed in version 0.21. .. deprecated:: 0.19</source>
          <target state="translated">이 매개 변수는 n_components로 이름이 변경되었으며 버전 0.21에서 제거됩니다. .. 사용되지 않음 :: 0.19</target>
        </trans-unit>
        <trans-unit id="2008376a104f1f0d722babab4554d9900556a331" translate="yes" xml:space="preserve">
          <source>This parameter is ignored if vocabulary is not None.</source>
          <target state="translated">어휘가 없음이 아닌 경우이 매개 변수는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="4896edc233b2b945e9bfe76cd7c644f9b147f395" translate="yes" xml:space="preserve">
          <source>This parameter is ignored when &lt;code&gt;fit_intercept&lt;/code&gt; is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use &lt;a href=&quot;sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;sklearn.preprocessing.StandardScaler&lt;/code&gt;&lt;/a&gt; before calling &lt;code&gt;fit&lt;/code&gt; on an estimator with &lt;code&gt;normalize=False&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;fit_intercept&lt;/code&gt; 가 False로 설정 되면이 매개 변수는 무시됩니다 . True 인 경우 회귀 X는 평균을 빼고 l2-norm으로 나누어 회귀 전에 정규화됩니다. 표준화하려는 경우 &lt;code&gt;normalize=False&lt;/code&gt; 인 추정기에 &lt;code&gt;fit&lt;/code&gt; 을 호출하기 전에 &lt;a href=&quot;sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;sklearn.preprocessing.StandardScaler&lt;/code&gt; &lt;/a&gt; 를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="18c6fb9267c5496a0a4415bd237b294d4f877c13" translate="yes" xml:space="preserve">
          <source>This parameter is required for multiclass/multilabel targets. If &lt;code&gt;None&lt;/code&gt;, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:</source>
          <target state="translated">이 매개 변수는 멀티 클래스 / 멀티 라벨 대상에 필요합니다. 경우 &lt;code&gt;None&lt;/code&gt; , 각 클래스에 대한 점수가 반환됩니다. 그렇지 않으면 데이터에서 수행되는 평균화 유형을 결정합니다.</target>
        </trans-unit>
        <trans-unit id="4b54c5323e385131687ecd8fe6362000ef5ec12b" translate="yes" xml:space="preserve">
          <source>This parameters can be accessed through the members &lt;code&gt;dual_coef_&lt;/code&gt; which holds the product \(y_i \alpha_i\), &lt;code&gt;support_vectors_&lt;/code&gt; which holds the support vectors, and &lt;code&gt;intercept_&lt;/code&gt; which holds the independent term \(\rho\) :</source>
          <target state="translated">이 파라미터는 부재를 통해 액세스 할 수 &lt;code&gt;dual_coef_&lt;/code&gt; 제품 \ (y_i \ alpha_i \)를 보유하는 &lt;code&gt;support_vectors_&lt;/code&gt; 지지 벡터를 보유하며 &lt;code&gt;intercept_&lt;/code&gt; 독립적 용어 \ 보유 (\ RHO \)</target>
        </trans-unit>
        <trans-unit id="f0e92a41ff311d2df395e780ee2f06d2a63e9cbc" translate="yes" xml:space="preserve">
          <source>This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.</source>
          <target state="translated">이러한 임의의 나무의 숲에서 평균 된이 경로 길이는 정규성과 측정 기능의 척도입니다.</target>
        </trans-unit>
        <trans-unit id="80dec09fc285dd6f9c561fc2931162bad1982cdb" translate="yes" xml:space="preserve">
          <source>This plot compares the decision surfaces learned by a decision tree classifier (first column), by a random forest classifier (second column), by an extra- trees classifier (third column) and by an AdaBoost classifier (fourth column).</source>
          <target state="translated">이 그림은 의사 결정 트리 분류기 (첫 번째 열), 임의 포리스트 분류기 (두 번째 열), 엑스트라 트리 분류기 (세 번째 열) 및 AdaBoost 분류기 (네 번째 열)에 의해 학습 된 결정 표면을 비교합니다.</target>
        </trans-unit>
        <trans-unit id="8e05dc7c1a0ec44a96abb884d1ce621cca93db4e" translate="yes" xml:space="preserve">
          <source>This problem can safely be ignored when the number of samples is more than a thousand and the number of clusters is less than 10. &lt;strong&gt;For smaller sample sizes or larger number of clusters it is safer to use an adjusted index such as the Adjusted Rand Index (ARI)&lt;/strong&gt;.</source>
          <target state="translated">표본 수가 1,000 개 이상이고 군집 수가 10 개 미만인 경우에는이 문제를 무시해도됩니다. &lt;strong&gt;표본 크기가 작거나 군집 수가 많은 경우 조정 랜드 지수 ( ARI)&lt;/strong&gt; .</target>
        </trans-unit>
        <trans-unit id="fcd7e110cbbcbd004685bcd45cf928dd3da1b5b1" translate="yes" xml:space="preserve">
          <source>This procedure (spectral clustering on an image) is an efficient approximate solution for finding normalized graph cuts.</source>
          <target state="translated">이 절차 (이미지의 스펙트럼 클러스터링)는 정규화 된 그래프 컷을 찾기위한 효율적인 근사 솔루션입니다.</target>
        </trans-unit>
        <trans-unit id="7a8651d966c336f363771d222433438f229cb5c2" translate="yes" xml:space="preserve">
          <source>This regressor is useful as a simple baseline to compare with other (real) regressors. Do not use it for real problems.</source>
          <target state="translated">이 회귀는 다른 (진정한) 회귀와 비교하기위한 간단한 기준으로 유용합니다. 실제 문제에는 사용하지 마십시오.</target>
        </trans-unit>
        <trans-unit id="566769fe300350777b617d7ceed39620171814da" translate="yes" xml:space="preserve">
          <source>This scaler can also be applied to sparse CSR or CSC matrices by passing &lt;code&gt;with_mean=False&lt;/code&gt; to avoid breaking the sparsity structure of the data.</source>
          <target state="translated">이 스케일러는 &lt;code&gt;with_mean=False&lt;/code&gt; 를 전달 하여 데이터의 희소성 구조가 깨지지 않도록 스파 스 CSR 또는 CSC 행렬에도 적용 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="bf350412f5695ebe05a62d114269ff308d8edf9f" translate="yes" xml:space="preserve">
          <source>This scaler can also be applied to sparse CSR or CSC matrices.</source>
          <target state="translated">이 스케일러는 스파 스 CSR 또는 CSC 매트릭스에도 적용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="096cae15373bbc176ca80052b34794345347f334" translate="yes" xml:space="preserve">
          <source>This score can be used to select the n_features features with the highest values for the test chi-squared statistic from X, which must contain only non-negative features such as booleans or frequencies (e.g., term counts in document classification), relative to the classes.</source>
          <target state="translated">이 점수는 X에서 테스트 카이 제곱 통계량에 대해 가장 높은 값을 갖는 n_features 피처를 선택하는 데 사용될 수 있습니다. 여기에는 부울 또는 빈도 (예 : 문서 분류의 용어 수)와 같은 음이 아닌 피처 만 포함해야합니다. 클래스.</target>
        </trans-unit>
        <trans-unit id="bc67f7a4c884a57cb8d65e3051862bc296a2adb5" translate="yes" xml:space="preserve">
          <source>This score is identical to &lt;a href=&quot;sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt;&lt;code&gt;normalized_mutual_info_score&lt;/code&gt;&lt;/a&gt; with the &lt;code&gt;'arithmetic'&lt;/code&gt; option for averaging.</source>
          <target state="translated">이 점수는 평균화를위한 &lt;code&gt;'arithmetic'&lt;/code&gt; 옵션 이있는 &lt;a href=&quot;sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt; &lt;code&gt;normalized_mutual_info_score&lt;/code&gt; &lt;/a&gt; 와 동일합니다 .</target>
        </trans-unit>
        <trans-unit id="81b377dd4306b3470c7efb10edcaa8d55b57210b" translate="yes" xml:space="preserve">
          <source>This section illustrates the use of a &lt;code&gt;Pipeline&lt;/code&gt; with &lt;code&gt;GridSearchCV&lt;/code&gt;</source>
          <target state="translated">이 섹션에서는 &lt;code&gt;GridSearchCV&lt;/code&gt; 와 함께 &lt;code&gt;Pipeline&lt;/code&gt; 을 사용하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="929a45e65dc1a0cd0b685d6194be9ae4708eb857" translate="yes" xml:space="preserve">
          <source>This should make it possible to check that the cross-validation score is in the same range as before.</source>
          <target state="translated">이를 통해 교차 검증 점수가 이전과 동일한 범위에 있는지 확인할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="08af461e03baea2ad13f22a738ff3dde29c2a50f" translate="yes" xml:space="preserve">
          <source>This shows an example of a neighbors-based query (in particular a kernel density estimate) on geospatial data, using a Ball Tree built upon the Haversine distance metric &amp;ndash; i.e. distances over points in latitude/longitude. The dataset is provided by Phillips et. al. (2006). If available, the example uses &lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;basemap&lt;/a&gt; to plot the coast lines and national boundaries of South America.</source>
          <target state="translated">다음은 Haversine 거리 측정법에 기반한 볼 트리 (위도 / 경도 점에서의 거리)를 사용하여 지리 공간 데이터에 대한 이웃 기반 쿼리 (특히 커널 밀도 추정)의 예를 보여줍니다. 데이터 세트는 Phillips et. 알. (2006). 가능한 &lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;경우이&lt;/a&gt; 예에서는 베이스 맵 을 사용 하여 남미의 해안선과 국가 경계를 그립니다.</target>
        </trans-unit>
        <trans-unit id="38716491ad409e2f991bc1db8f7b1d944921bf99" translate="yes" xml:space="preserve">
          <source>This sort of preprocessing can be streamlined with the &lt;a href=&quot;compose#pipeline&quot;&gt;Pipeline&lt;/a&gt; tools. A single object representing a simple polynomial regression can be created and used as follows:</source>
          <target state="translated">이러한 종류의 전처리는 &lt;a href=&quot;compose#pipeline&quot;&gt;파이프 라인&lt;/a&gt; 도구를 사용하여 간소화 할 수 있습니다 . 간단한 다항식 회귀를 나타내는 단일 객체를 다음과 같이 만들고 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4e6050ab2083fb67a848ffe7f83ae292a8f60f62" translate="yes" xml:space="preserve">
          <source>This strategy can also be used for multilabel learning, where a classifier is used to predict multiple labels for instance, by fitting on a 2-d matrix in which cell [i, j] is 1 if sample i has label j and 0 otherwise.</source>
          <target state="translated">이 전략은 또한 다중 레이블 학습에 사용될 수 있습니다. 예를 들어, 샘플 i에 레이블 j가 있고 0이 아닌 경우 셀 [i, j]가 1 인 2 차원 행렬에 피팅함으로써 분류자가 여러 레이블을 예측하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="8cd6a64f79d725ef1cdd342315685305aab51887" translate="yes" xml:space="preserve">
          <source>This strategy consists in fitting one classifier per class pair. At prediction time, the class which received the most votes is selected. Since it requires to fit &lt;code&gt;n_classes * (n_classes - 1) / 2&lt;/code&gt; classifiers, this method is usually slower than one-vs-the-rest, due to its O(n_classes^2) complexity. However, this method may be advantageous for algorithms such as kernel algorithms which don&amp;rsquo;t scale well with &lt;code&gt;n_samples&lt;/code&gt;. This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used &lt;code&gt;n_classes&lt;/code&gt; times.</source>
          <target state="translated">이 전략은 클래스 쌍당 하나의 분류자를 피팅하는 것으로 구성됩니다. 예측 시간에 가장 많은 표를받은 클래스가 선택됩니다. &lt;code&gt;n_classes * (n_classes - 1) / 2&lt;/code&gt; 분류기 에 맞아야하기 때문에이 방법은 일반적으로 O (n_classes ^ 2) 복잡성으로 인해 1 대 1보다 느립니다. 그러나이 방법은 &lt;code&gt;n_samples&lt;/code&gt; 와 함께 잘 확장되지 않는 커널 알고리즘과 같은 알고리즘에 유리할 수 있습니다 . 각 개별 학습 문제는 데이터의 작은 하위 집합 만 포함하고 나머지는 전체 데이터 세트가 &lt;code&gt;n_classes&lt;/code&gt; 시간 동안 사용 되기 때문 입니다.</target>
        </trans-unit>
        <trans-unit id="de640b51f281cc0046c1a9e652c58d4e96ebef0b" translate="yes" xml:space="preserve">
          <source>This strategy consists of fitting one classifier per target. This is a simple strategy for extending classifiers that do not natively support multi-target classification</source>
          <target state="translated">이 전략은 대상 당 하나의 분류기를 맞추는 것으로 구성됩니다. 이것은 다중 대상 분류를 기본적으로 지원하지 않는 분류기를 확장하기위한 간단한 전략입니다.</target>
        </trans-unit>
        <trans-unit id="7d6fb6c6a7f2b79b31f4627b09bbb93dcc2f3bc4" translate="yes" xml:space="preserve">
          <source>This strategy consists of fitting one regressor per target. This is a simple strategy for extending regressors that do not natively support multi-target regression.</source>
          <target state="translated">이 전략은 대상 당 하나의 회귀자를 피팅하는 것으로 구성됩니다. 이것은 다중 대상 회귀를 기본적으로 지원하지 않는 회귀 확장을위한 간단한 전략입니다.</target>
        </trans-unit>
        <trans-unit id="27cbaceed22a6f90d5ae07c700825d27bfca1f78" translate="yes" xml:space="preserve">
          <source>This strategy has several advantages:</source>
          <target state="translated">이 전략에는 몇 가지 장점이 있습니다.</target>
        </trans-unit>
        <trans-unit id="91d49a77db9793bc904e6dbf1c0dda029b6c110b" translate="yes" xml:space="preserve">
          <source>This strategy is illustrated below.</source>
          <target state="translated">이 전략은 아래에 설명되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="02849272fb07ed52ea9b00f7ccc02a748c971372" translate="yes" xml:space="preserve">
          <source>This strategy, also known as &lt;strong&gt;one-vs-all&lt;/strong&gt;, is implemented in &lt;a href=&quot;generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;OneVsRestClassifier&lt;/code&gt;&lt;/a&gt;. The strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only &lt;code&gt;n_classes&lt;/code&gt; classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and only one classifier, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy and is a fair default choice.</source>
          <target state="translated">&lt;strong&gt;one-vs-all&lt;/strong&gt; 이라고도하는이 전략 은 &lt;a href=&quot;generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;OneVsRestClassifier&lt;/code&gt; &lt;/a&gt; 에서 구현됩니다 . 전략은 클래스 당 하나의 분류 기준을 맞추는 것으로 구성됩니다. 각 분류 자에 대해 클래스는 다른 모든 클래스에 적합합니다. 계산 효율성 ( &lt;code&gt;n_classes&lt;/code&gt; 분류 자만 필요) 외에도이 접근 방식의 한 가지 장점은 해석 가능성입니다. 각 클래스는 하나의 분류 자로 만 표시되므로 해당 분류자를 검사하여 클래스에 대한 지식을 얻을 수 있습니다. 이것이 가장 일반적으로 사용되는 전략이며 공정한 기본 선택입니다.</target>
        </trans-unit>
        <trans-unit id="27ac94ab878e8b852843daf6fdc6644656d86a0e" translate="yes" xml:space="preserve">
          <source>This submodule contains functions that approximate the feature mappings that correspond to certain kernels, as they are used for example in support vector machines (see &lt;a href=&quot;svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt;). The following feature functions perform non-linear transformations of the input, which can serve as a basis for linear classification or other algorithms.</source>
          <target state="translated">이 서브 모듈들이 지원 벡터 기계의 예를 들어 사용으로 특정 커널에 그 해당 기능 매핑을 근사 함수를 포함합니다 ( &lt;a href=&quot;svm#svm&quot;&gt;지원 벡터 기계&lt;/a&gt; ). 다음 기능 함수는 입력의 비선형 변환을 수행하며 선형 분류 또는 기타 알고리즘의 기초가 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fcbae9cfcf0bd03c252edbce0ec54b00175fa149" translate="yes" xml:space="preserve">
          <source>This test can be applied to classes or instances. Classes currently have some additional tests that related to construction, while passing instances allows the testing of multiple options.</source>
          <target state="translated">이 테스트는 클래스 또는 인스턴스에 적용 할 수 있습니다. 클래스에는 현재 건설과 관련된 몇 가지 추가 테스트가 있지만 인스턴스를 통과하면 여러 옵션을 테스트 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="fe4512e0330ee3f9d4d908b10acadfb35dcb4b46" translate="yes" xml:space="preserve">
          <source>This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.</source>
          <target state="translated">이 텍스트 벡터 라이저 구현은 해싱 트릭을 사용하여 정수 인덱스 맵핑을 특징으로하는 토큰 문자열 이름을 찾습니다.</target>
        </trans-unit>
        <trans-unit id="90704510327932a48fb3c52155398163f97475e2" translate="yes" xml:space="preserve">
          <source>This transformation is often used as an alternative to zero mean, unit variance scaling.</source>
          <target state="translated">이 변환은 종종 평균 제로 평균 단위 분산 스케일링의 대안으로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="2f720ce11fea82f150f2e1314efc3b6e74d2aa65" translate="yes" xml:space="preserve">
          <source>This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion).</source>
          <target state="translated">이 변환기는 밀도가 높은 numpy 배열과 scipy.sparse 매트릭스 모두에서 작동 할 수 있습니다 (복사 / 변환 부담을 피하려면 CSR 형식을 사용하십시오).</target>
        </trans-unit>
        <trans-unit id="22f51ce5936d304512713b7c2d6420a9339ed07c" translate="yes" xml:space="preserve">
          <source>This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently.</source>
          <target state="translated">이 변압기는 잘린 특이 값 분해 (SVD)를 통해 선형 차원 축소를 수행합니다. PCA와 달리이 추정기는 특이 값 분해를 계산하기 전에 데이터를 중앙에 두지 않습니다. 즉, scipy.sparse 행렬을 효율적으로 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c058bdf39166834e7b3e02bee2a3dc32df1bea5c" translate="yes" xml:space="preserve">
          <source>This transformer turns lists of mappings (dict-like objects) of feature names to feature values into Numpy arrays or scipy.sparse matrices for use with scikit-learn estimators.</source>
          <target state="translated">이 변환기는 기능 이름과 기능 값의 매핑 (dict-like 객체) 목록을 scikit-learn 추정기와 함께 사용하기 위해 Numpy 배열 또는 scipy.sparse 행렬로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="6fa17f9065133747f6dee2c04fd6c387874c04ab" translate="yes" xml:space="preserve">
          <source>This tutorial will explore &lt;em&gt;statistical learning&lt;/em&gt;, the use of machine learning techniques with the goal of &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_inference&quot;&gt;statistical inference&lt;/a&gt;: drawing conclusions on the data at hand.</source>
          <target state="translated">이 튜토리얼 에서는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_inference&quot;&gt;통계적 추론을&lt;/a&gt; 목표로 머신 러닝 기술을 사용하는 &lt;em&gt;통계 학습&lt;/em&gt; , 즉 데이터에 대한 결론을 도출합니다.</target>
        </trans-unit>
        <trans-unit id="d770164eec068c2686e18d9f67f7678a7fe3d2ab" translate="yes" xml:space="preserve">
          <source>This uses the Benjamini-Hochberg procedure. &lt;code&gt;alpha&lt;/code&gt; is an upper bound on the expected false discovery rate.</source>
          <target state="translated">이것은 Benjamini-Hochberg 절차를 사용합니다. &lt;code&gt;alpha&lt;/code&gt; 는 예상되는 잘못된 발견 비율의 상한입니다.</target>
        </trans-unit>
        <trans-unit id="25630de50e6415b67bb72ea47abf6e457ed32d31" translate="yes" xml:space="preserve">
          <source>This uses the score defined by &lt;code&gt;scoring&lt;/code&gt; where provided, and the &lt;code&gt;best_estimator_.score&lt;/code&gt; method otherwise.</source>
          <target state="translated">제공된 경우 &lt;code&gt;scoring&lt;/code&gt; 정의 된 스코어를 사용하고 그렇지 않으면 &lt;code&gt;best_estimator_.score&lt;/code&gt; 메소드를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="57e78bad29e459afb6f7e8e9a46e0b5e6e5f4fa9" translate="yes" xml:space="preserve">
          <source>This value is valid if class_weight parameter in fit() is not set.</source>
          <target state="translated">fit ()의 class_weight 매개 변수가 설정되지 않은 경우이 값이 유효합니다.</target>
        </trans-unit>
        <trans-unit id="0973d55bbbd406d7d050b325e278935eae6a368e" translate="yes" xml:space="preserve">
          <source>This value of the mutual information and also the normalized variant is not adjusted for chance and will tend to increase as the number of different labels (clusters) increases, regardless of the actual amount of &amp;ldquo;mutual information&amp;rdquo; between the label assignments.</source>
          <target state="translated">상호 정보 및 정규화 된 변형의이 값은 우연히 조정되지 않으며 레이블 할당 사이의 &quot;상호 정보&quot;의 실제 양에 관계없이 다른 레이블 (클러스터)의 수가 증가함에 따라 증가하는 경향이 있습니다.</target>
        </trans-unit>
        <trans-unit id="ec27c204380d1bf1bec671104c4e5cc563667983" translate="yes" xml:space="preserve">
          <source>This visualization is an example of a &lt;em&gt;kernel density estimation&lt;/em&gt;, in this case with a top-hat kernel (i.e. a square block at each point). We can recover a smoother distribution by using a smoother kernel. The bottom-right plot shows a Gaussian kernel density estimate, in which each point contributes a Gaussian curve to the total. The result is a smooth density estimate which is derived from the data, and functions as a powerful non-parametric model of the distribution of points.</source>
          <target state="translated">이 시각화는 &lt;em&gt;커널 밀도 추정&lt;/em&gt; 의 예이며 ,이 경우에는 탑햇 커널 (즉, 각 지점에서 정사각형 블록)을 사용합니다. 보다 부드러운 커널을 사용하여보다 원활한 배포를 복구 할 수 있습니다. 오른쪽 아래 그림은 가우스 커널 밀도 추정값을 보여줍니다. 각 포인트는 가우스 곡선을 총계에 기여합니다. 결과는 데이터에서 도출 된 부드러운 밀도 추정치이며 점 분포의 강력한 비모수 적 모델로 작동합니다.</target>
        </trans-unit>
        <trans-unit id="1cad85e71e9b226b43b5778c8058de4fe70516a7" translate="yes" xml:space="preserve">
          <source>This warning is used to notify the user that BLAS was not used for dot operation and hence the efficiency may be affected.</source>
          <target state="translated">이 경고는 BLAS가 도트 작업에 사용되지 않았으므로 효율성에 영향을 줄 수 있음을 사용자에게 알리는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="7b4c8162b5298ba9d922a2200274b38ddddf44e8" translate="yes" xml:space="preserve">
          <source>This warning notifies the user that the efficiency may not be optimal due to some reason which may be included as a part of the warning message. This may be subclassed into a more specific Warning class.</source>
          <target state="translated">이 경고는 경고 메시지의 일부로 포함될 수있는 어떤 이유로 인해 효율성이 최적이 아닐 수 있음을 사용자에게 알립니다. 이것은보다 구체적인 경고 클래스로 서브 클래 싱 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ec79da6e5e29f4afd0662e82ec298c39ed6dbabd" translate="yes" xml:space="preserve">
          <source>This warning occurs when some input data needs to be converted or interpreted in a way that may not match the user&amp;rsquo;s expectations.</source>
          <target state="translated">이 경고는 일부 입력 데이터를 사용자의 기대에 맞지 않는 방식으로 변환하거나 해석해야 할 때 발생합니다.</target>
        </trans-unit>
        <trans-unit id="14994b75958434504d6803fa4be46a86d6219fc9" translate="yes" xml:space="preserve">
          <source>This was originally a term weighting scheme developed for information retrieval (as a ranking function for search engines results) that has also found good use in document classification and clustering.</source>
          <target state="translated">이는 원래 문서 분류 및 클러스터링에서 유용하게 사용되는 정보 검색 (검색 엔진 결과의 순위 함수로)을 위해 개발 된 가중치 체계입니다.</target>
        </trans-unit>
        <trans-unit id="90d00e9f85af52e63288d2fca3d9f513dce9de12" translate="yes" xml:space="preserve">
          <source>This, however, is not the case in the Ledoit-Wolf procedure when the population covariance happens to be a multiple of the identity matrix. In this case, the Ledoit-Wolf shrinkage estimate approaches 1 as the number of samples increases. This indicates that the optimal estimate of the covariance matrix in the Ledoit-Wolf sense is multiple of the identity. Since the population covariance is already a multiple of the identity matrix, the Ledoit-Wolf solution is indeed a reasonable estimate.</source>
          <target state="translated">그러나 모집단 공분산이 항등 행렬의 배수 인 경우 Ledoit-Wolf 절차에서는 그렇지 않습니다. 이 경우, 샘플 수가 증가함에 따라 Ledoit-Wolf 수축 추정치는 1에 가까워집니다. 이것은 Ledoit-Wolf 의미에서 공분산 행렬의 최적 추정치가 동일성의 배수임을 나타냅니다. 모집단 공분산은 이미 항등 행렬의 배수이므로 Ledoit-Wolf 솔루션은 실제로 합리적인 추정치입니다.</target>
        </trans-unit>
        <trans-unit id="e911226999d28ae4c4eb95cef049955b008548cf" translate="yes" xml:space="preserve">
          <source>Those 3 metrics are independent of the absolute values of the labels: a permutation of the class or cluster label values won&amp;rsquo;t change the score values in any way.</source>
          <target state="translated">이 3 가지 메트릭은 레이블의 절대 값과 무관합니다. 클래스 또는 클러스터 레이블 값의 순열은 점수 값을 변경하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="a17151aff3f6e79d6bfb3bc3e7c5d30ff3b77b7d" translate="yes" xml:space="preserve">
          <source>Those metrics are based on normalized conditional entropy measures of the clustering labeling to evaluate given the knowledge of a Ground Truth class labels of the same samples.</source>
          <target state="translated">이러한 메트릭은 클러스터링 레이블링의 정규화 된 조건부 엔트로피 측정 값을 기반으로하여 동일한 샘플의 Ground Truth 클래스 레이블에 대한 지식을 평가합니다.</target>
        </trans-unit>
        <trans-unit id="831e093286e91d34e1415d38600e7c8277f14a07" translate="yes" xml:space="preserve">
          <source>Though not technically a variant of LLE, Local tangent space alignment (LTSA) is algorithmically similar enough to LLE that it can be put in this category. Rather than focusing on preserving neighborhood distances as in LLE, LTSA seeks to characterize the local geometry at each neighborhood via its tangent space, and performs a global optimization to align these local tangent spaces to learn the embedding. LTSA can be performed with function &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt;&lt;code&gt;locally_linear_embedding&lt;/code&gt;&lt;/a&gt; or its object-oriented counterpart &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;, with the keyword &lt;code&gt;method = 'ltsa'&lt;/code&gt;.</source>
          <target state="translated">기술적으로 LLE의 변형은 아니지만 LTSA (Local tangent space alignment)는 LLE과 알고리즘 적으로 유사하여이 범주에 넣을 수 있습니다. LTSA는 LLE에서와 같이 이웃 거리를 유지하는 데 초점을 맞추기보다는 접선 공간을 통해 각 이웃의 로컬 지오메트리를 특성화하고 이러한 접선 공간을 정렬하기 위해 전역 최적화를 수행하여 임베딩을 학습합니다. LTSA는 함수로 수행 될 수 &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt; &lt;code&gt;locally_linear_embedding&lt;/code&gt; &lt;/a&gt; 또는 객체 지향 상대 &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt; 키워드와 &lt;code&gt;method = 'ltsa'&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="3dcfc8b5bdf930c1b66451c5dc30f486901100ee" translate="yes" xml:space="preserve">
          <source>Three different types of SVM-Kernels are displayed below. The polynomial and RBF are especially useful when the data-points are not linearly separable.</source>
          <target state="translated">아래에는 세 가지 유형의 SVM 커널이 표시되어 있습니다. 다항식과 RBF는 데이터 포인트를 선형으로 분리 할 수 ​​없을 때 특히 유용합니다.</target>
        </trans-unit>
        <trans-unit id="2eb0d5d5e8d716a06a5bc42a652c1781cf721343" translate="yes" xml:space="preserve">
          <source>Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors.</source>
          <target state="translated">샘플 기능의 이진화 (부울 맵핑)에 대한 임계 값입니다. None이면 입력은 이미 이진 벡터로 구성된 것으로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="ac359cd376aaf3163dffdc564a92f8f55ebdbfe9" translate="yes" xml:space="preserve">
          <source>Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.</source>
          <target state="translated">수목의 조기 정지에 대한 임계 값. 불순물이 임계 값을 초과하면 노드가 분리되고, 그렇지 않으면 잎입니다.</target>
        </trans-unit>
        <trans-unit id="d2dde1e4fd07fa9e4ff99bf50a843f5c394281b4" translate="yes" xml:space="preserve">
          <source>Threshold for shrinking centroids to remove features.</source>
          <target state="translated">중심점을 축소하여 피처를 제거하는 임계 값입니다.</target>
        </trans-unit>
        <trans-unit id="5b50eca69565a6240c9cb586697767c09ac4525e" translate="yes" xml:space="preserve">
          <source>Threshold on the size of arrays passed to the workers that triggers automated memory mapping in temp_folder. Can be an int in Bytes, or a human-readable string, e.g., &amp;lsquo;1M&amp;rsquo; for 1 megabyte. Use None to disable memmapping of large arrays. Only active when backend=&amp;rdquo;loky&amp;rdquo; or &amp;ldquo;multiprocessing&amp;rdquo;.</source>
          <target state="translated">temp_folder에서 자동화 된 메모리 매핑을 트리거하는 작업자에게 전달되는 배열 크기의 임계 값입니다. 바이트 단위의 int 또는 사람이 읽을 수있는 문자열 (예 : 1MB의 경우 '1M') 일 수 있습니다. 대형 배열의 emma 핑을 비활성화하려면 없음을 사용하십시오. 백엔드 =&amp;rdquo;loky&amp;rdquo;또는&amp;ldquo;multiprocessing&amp;rdquo;인 경우에만 활성화됩니다.</target>
        </trans-unit>
        <trans-unit id="3bf722c4ec04176f091be4d50fbd629d5b754a20" translate="yes" xml:space="preserve">
          <source>Threshold used for rank estimation in SVD solver.</source>
          <target state="translated">SVD 솔버에서 순위 추정에 사용되는 임계 값입니다.</target>
        </trans-unit>
        <trans-unit id="0168a115989469a76c56e8c46c0d56b1a01f88c6" translate="yes" xml:space="preserve">
          <source>Threshold used for rank estimation.</source>
          <target state="translated">순위 추정에 사용되는 임계 값입니다.</target>
        </trans-unit>
        <trans-unit id="558232b0add0e7cf1e4001c15b7a509781ecfb59" translate="yes" xml:space="preserve">
          <source>Threshold used in the binary and multi-label cases.</source>
          <target state="translated">이진 및 다중 레이블 경우에 사용되는 임계 값입니다.</target>
        </trans-unit>
        <trans-unit id="d260a173cc06214ee3d2352996ea165371c17c29" translate="yes" xml:space="preserve">
          <source>Thresholding</source>
          <target state="translated">Thresholding</target>
        </trans-unit>
        <trans-unit id="8265a18b28c2cb3c5a28ceb45384d9a49c2f7715" translate="yes" xml:space="preserve">
          <source>Thresholding is clearly not useful for denoising, but it is here to show that it can produce a suggestive output with very high speed, and thus be useful for other tasks such as object classification, where performance is not necessarily related to visualisation.</source>
          <target state="translated">임계 값은 노이즈 제거에는 분명히 유용하지 않지만 매우 빠른 속도로 암시적인 출력을 생성 할 수 있으므로 성능과 시각화가 반드시 관련이있는 것은 아닌 객체 분류와 같은 다른 작업에 유용합니다.</target>
        </trans-unit>
        <trans-unit id="c4d29a75003891e7d5c5dbb3dea7166bf19f4ab9" translate="yes" xml:space="preserve">
          <source>Thresholding is very fast but it does not yield accurate reconstructions. They have been shown useful in literature for classification tasks. For image reconstruction tasks, orthogonal matching pursuit yields the most accurate, unbiased reconstruction.</source>
          <target state="translated">임계 값은 매우 빠르지 만 정확한 재구성을 제공하지는 않습니다. 그것들은 분류 작업에 대한 문헌에서 유용한 것으로 나타났습니다. 이미지 재구성 작업의 경우 직교 매칭 추구는 가장 정확하고 편견없는 재구성을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="3904c870d9e800cc53a98ecb8acef59d010fad3d" translate="yes" xml:space="preserve">
          <source>Throw a ValueError if X contains NaN or infinity.</source>
          <target state="translated">X에 NaN 또는 무한대가 포함 된 경우 ValueError를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="8b8612c016401dc529cb09be5ddd6996fe872d9c" translate="yes" xml:space="preserve">
          <source>Thus in binary classification, the count of true negatives is \(C_{0,0}\), false negatives is \(C_{1,0}\), true positives is \(C_{1,1}\) and false positives is \(C_{0,1}\).</source>
          <target state="translated">따라서 이진 분류에서 실제 음수의 개수는 \ (C_ {0,0} \)이고, 위 음수는 \ (C_ {1,0} \)이며, 양수는 \ (C_ {1,1} \)입니다. 오 탐지는 \ (C_ {0,1} \)입니다.</target>
        </trans-unit>
        <trans-unit id="877864e25b035038afd6bbe5a72ca90fb8e0741e" translate="yes" xml:space="preserve">
          <source>Thus the median of the input becomes the mean of the output, centered at 0. The normal output is clipped so that the input&amp;rsquo;s minimum and maximum &amp;mdash; corresponding to the 1e-7 and 1 - 1e-7 quantiles respectively &amp;mdash; do not become infinite under the transformation.</source>
          <target state="translated">따라서 입력의 중앙값은 0을 중심으로하는 출력의 평균이됩니다. 일반 출력은 1e-7 및 1-1e-7 Quantile에 각각 해당하는 입력의 최소값과 최대 값이 무한히되지 않도록 클리핑됩니다. 변형.</target>
        </trans-unit>
        <trans-unit id="0808b4cdf67452766c8c5389635c6458f9990f5b" translate="yes" xml:space="preserve">
          <source>Thus, most of the target signal (34.4ppm) is explained by a long-term rising trend (length-scale 41.8 years). The periodic component has an amplitude of 3.27ppm, a decay time of 180 years and a length-scale of 1.44. The long decay time indicates that we have a locally very close to periodic seasonal component. The correlated noise has an amplitude of 0.197ppm with a length scale of 0.138 years and a white-noise contribution of 0.197ppm. Thus, the overall noise level is very small, indicating that the data can be very well explained by the model. The figure shows also that the model makes very confident predictions until around 2015</source>
          <target state="translated">따라서 대부분의 목표 신호 (34.4ppm)는 장기 상승 추세 (길이 규모 41.8 년)로 설명됩니다. 주기 성분은 3.27ppm의 진폭, 180 년의 붕괴 시간 및 1.44의 길이 스케일을가집니다. 붕괴 시간이 길다는 것은 계절적으로 매우 가까운 계절적 구성 요소가 있음을 나타냅니다. 상관 노이즈는 0.138 년의 길이 스케일과 0.197ppm의 화이트 노이즈 기여로 0.197ppm의 진폭을가집니다. 따라서 전체 노이즈 레벨이 매우 작아서 모델이 데이터를 잘 설명 할 수 있음을 나타냅니다. 또한이 모델은 2015 년 경까지 모델이 매우 자신있게 예측할 수 있음을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="4ab3c0245825ab663f7197647adadf073e4b3e64" translate="yes" xml:space="preserve">
          <source>Thus, most of the target signal (34.4ppm) is explained by a long-term rising trend (length-scale 41.8 years). The periodic component has an amplitude of 3.27ppm, a decay time of 180 years and a length-scale of 1.44. The long decay time indicates that we have a locally very close to periodic seasonal component. The correlated noise has an amplitude of 0.197ppm with a length scale of 0.138 years and a white-noise contribution of 0.197ppm. Thus, the overall noise level is very small, indicating that the data can be very well explained by the model. The figure shows also that the model makes very confident predictions until around 2015.</source>
          <target state="translated">따라서 대부분의 목표 신호 (34.4ppm)는 장기 상승 추세 (길이 규모 41.8 년)로 설명됩니다. 주기 성분은 3.27ppm의 진폭, 180 년의 붕괴 시간 및 1.44의 길이 스케일을가집니다. 붕괴 시간이 길다는 것은 계절적으로 매우 가까운 계절적 구성 요소가 있음을 나타냅니다. 상관 잡음은 0.138ppm의 길이 스케일과 0.197ppm의 화이트 노이즈 기여로 0.197ppm의 진폭을가집니다. 따라서 전체 노이즈 레벨이 매우 작아서 모델이 데이터를 잘 설명 할 수 있음을 나타냅니다. 또한이 모델은 2015 년까지 모델이 매우 자신있게 예측할 수 있음을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="af16f18f91308907d1dd8226e54112fa0bd29044" translate="yes" xml:space="preserve">
          <source>Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH: An efficient data clustering method for large databases. &lt;a href=&quot;http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf&quot;&gt;http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf&lt;/a&gt;</source>
          <target state="translated">Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH : 대규모 데이터베이스를위한 효율적인 데이터 클러스터링 방법. &lt;a href=&quot;http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf&quot;&gt;http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d53cad37906f55db18b858cf86bfeef9ad9688eb" translate="yes" xml:space="preserve">
          <source>Tibshirani, R., Hastie, T., Narasimhan, B., &amp;amp; Chu, G. (2002). Diagnosis of multiple cancer types by shrunken centroids of gene expression. Proceedings of the National Academy of Sciences of the United States of America, 99(10), 6567-6572. The National Academy of Sciences.</source>
          <target state="translated">Tibshirani, R., Hastie, T., Narasimhan, B., &amp;amp; Chu, G. (2002). 유전자 발현의 중심을 줄임으로써 다중 암 유형의 진단. 미국 국립 과학원 (National Academy of Sciences), 99 (10), 6567-6572의 절차. 국립 과학 아카데미.</target>
        </trans-unit>
        <trans-unit id="a297f524f28779281bb4e53d7b6af672dcac3672" translate="yes" xml:space="preserve">
          <source>Ties are broken using the secondary method from Leeuw, 1977.</source>
          <target state="translated">1977 년 Leeuw의 2 차 방법을 사용하여 연결을 끊습니다.</target>
        </trans-unit>
        <trans-unit id="59976e05663a4d82c80a3273030c2c2f87094f4d" translate="yes" xml:space="preserve">
          <source>Ties between features with equal scores will be broken in an unspecified way.</source>
          <target state="translated">동일한 점수를 가진 지형지 물 간의 연계는 지정되지 않은 방식으로 중단됩니다.</target>
        </trans-unit>
        <trans-unit id="c41dd9e78b42392c90f4c6ddfb54f7863f5482f1" translate="yes" xml:space="preserve">
          <source>Ties in &lt;code&gt;y_scores&lt;/code&gt; are broken by giving maximal rank that would have been assigned to all tied values.</source>
          <target state="translated">&lt;code&gt;y_scores&lt;/code&gt; 의 동점은 모든 묶인 값에 지정된 최대 순위를 지정하여 끊어집니다.</target>
        </trans-unit>
        <trans-unit id="ba73dffe02601a1abd345b6200b276334877401b" translate="yes" xml:space="preserve">
          <source>Time Series cross-validator</source>
          <target state="translated">시계열 교차 유효성 검사기</target>
        </trans-unit>
        <trans-unit id="bbad16d201e3f82cae87bba42e6286ebcef9d190" translate="yes" xml:space="preserve">
          <source>Time series data is characterised by the correlation between observations that are near in time (&lt;em&gt;autocorrelation&lt;/em&gt;). However, classical cross-validation techniques such as &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt;&lt;code&gt;ShuffleSplit&lt;/code&gt;&lt;/a&gt; assume the samples are independent and identically distributed, and would result in unreasonable correlation between training and testing instances (yielding poor estimates of generalisation error) on time series data. Therefore, it is very important to evaluate our model for time series data on the &amp;ldquo;future&amp;rdquo; observations least like those that are used to train the model. To achieve this, one solution is provided by &lt;a href=&quot;generated/sklearn.model_selection.timeseriessplit#sklearn.model_selection.TimeSeriesSplit&quot;&gt;&lt;code&gt;TimeSeriesSplit&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">시계열 데이터는 거의 시간에 가까운 관측치 간의 상관 관계를 특징으로합니다 (자가 &lt;em&gt;상관&lt;/em&gt; ). 그러나, 클래식 교차 검증 기술 &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt; &lt;code&gt;ShuffleSplit&lt;/code&gt; 이&lt;/a&gt; 샘플은 독립적이고 동일하게 분포하고, 교육 및 테스트 인스턴스 간의 불합리한 상관 관계를 초래 가정 시계열 데이터에 (일반화 오류의 가난한 추정치를 산출). 따라서 모델 학습에 사용되는 것과 거의 같은 &quot;미래&quot;관측치에 대한 시계열 데이터의 모델을 평가하는 것이 매우 중요합니다. 이를 위해 &lt;a href=&quot;generated/sklearn.model_selection.timeseriessplit#sklearn.model_selection.TimeSeriesSplit&quot;&gt; &lt;code&gt;TimeSeriesSplit&lt;/code&gt; &lt;/a&gt; 에서 하나의 솔루션을 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="a6268d75d578276d37dec9fce6dea804677e6b49" translate="yes" xml:space="preserve">
          <source>Timeout limit for each task to complete. If any task takes longer a TimeOutError will be raised. Only applied when n_jobs != 1</source>
          <target state="translated">완료 할 각 작업의 시간 초과 제한. 작업이 오래 걸리면 TimeOutError가 발생합니다. n_jobs! = 1 인 경우에만 적용</target>
        </trans-unit>
        <trans-unit id="8879146d32620a1e603bf26a188c9426e79a5ed0" translate="yes" xml:space="preserve">
          <source>To address the computational inefficiencies of the brute-force approach, a variety of tree-based data structures have been invented. In general, these structures attempt to reduce the required number of distance calculations by efficiently encoding aggregate distance information for the sample. The basic idea is that if point \(A\) is very distant from point \(B\), and point \(B\) is very close to point \(C\), then we know that points \(A\) and \(C\) are very distant, &lt;em&gt;without having to explicitly calculate their distance&lt;/em&gt;. In this way, the computational cost of a nearest neighbors search can be reduced to \(O[D N \log(N)]\) or better. This is a significant improvement over brute-force for large \(N\).</source>
          <target state="translated">무차별 대입 방식의 계산 비 효율성을 해결하기 위해 다양한 트리 기반 데이터 구조가 발명되었습니다. 일반적으로 이러한 구조는 샘플에 대한 집계 거리 정보를 효율적으로 인코딩하여 필요한 거리 계산 횟수를 줄입니다. 기본 아이디어는 \ (A \) 지점이 \ (B \) 지점에서 멀리 떨어져 있고 \ (B \) 지점이 \ (C \) 지점에 매우 가까운 경우 \ (A \ ) 및 \ (C \)는 &lt;em&gt;거리를 명시 적으로 계산할 필요없이&lt;/em&gt; 매우 먼 &lt;em&gt;거리에 있습니다&lt;/em&gt; . 이러한 방식으로, 가장 가까운 이웃 검색의 계산 비용을 \ (O [DN \ log (N)] \) 이상으로 줄일 수 있습니다. 이것은 큰 \ (N \)에 대한 무차별 대입보다 훨씬 개선 된 것입니다.</target>
        </trans-unit>
        <trans-unit id="81ec528524d941df99755c9bb7fceaf80c6a8752" translate="yes" xml:space="preserve">
          <source>To address the inefficiencies of KD Trees in higher dimensions, the &lt;em&gt;ball tree&lt;/em&gt; data structure was developed. Where KD trees partition data along Cartesian axes, ball trees partition data in a series of nesting hyper-spheres. This makes tree construction more costly than that of the KD tree, but results in a data structure which can be very efficient on highly structured data, even in very high dimensions.</source>
          <target state="translated">더 큰 차원에서 KD 트리의 비 효율성을 해결하기 위해 &lt;em&gt;볼 트리&lt;/em&gt; 데이터 구조가 개발되었습니다. KD 트리가 데카르트 축을 따라 데이터를 분할하는 경우, 볼 트리는 일련의 중첩 하이퍼 스피어에서 데이터를 분할합니다. 이로 인해 트리 구성이 KD 트리보다 비용이 많이 들지만 데이터 구조는 결과적으로 매우 높은 차원에서도 매우 구조화 된 데이터에서 매우 효율적일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a11d5f0b5df4ea3ced24dc7521fb6d9f97740ba3" translate="yes" xml:space="preserve">
          <source>To address this concern, a number of supervised and unsupervised linear dimensionality reduction frameworks have been designed, such as Principal Component Analysis (PCA), Independent Component Analysis, Linear Discriminant Analysis, and others. These algorithms define specific rubrics to choose an &amp;ldquo;interesting&amp;rdquo; linear projection of the data. These methods can be powerful, but often miss important non-linear structure in the data.</source>
          <target state="translated">이 문제를 해결하기 위해 주성분 분석 (PCA), 독립 성분 분석, 선형 판별 분석 등의 여러 감독 및 감독되지 않은 선형 차원 축소 프레임 워크가 설계되었습니다. 이 알고리즘은 데이터의 &quot;관심있는&quot;선형 투영을 선택하기 위해 특정 루 브릭을 정의합니다. 이러한 방법은 강력 할 수 있지만 종종 데이터에서 중요한 비선형 구조를 놓치게됩니다.</target>
        </trans-unit>
        <trans-unit id="c134b5f4c4fa3b034f915a1c4077d9f58401c669" translate="yes" xml:space="preserve">
          <source>To address this issue you can use &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;sklearn.decomposition.PCA&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;whiten=True&lt;/code&gt; to further remove the linear correlation across features.</source>
          <target state="translated">이 문제를 해결하기 위해 &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;sklearn.decomposition.PCA&lt;/code&gt; &lt;/a&gt; 를 whiten &lt;code&gt;whiten=True&lt;/code&gt; 와 함께 사용하여 피처 의 선형 상관 관계를 추가로 제거 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5dfb268e42cc748904256c70f6b80b2da01edce9" translate="yes" xml:space="preserve">
          <source>To also transform a test set \(X\), we multiply it with \(V_k\):</source>
          <target state="translated">테스트 세트 \ (X \)도 변환하려면 \ (V_k \)를 곱하십시오.</target>
        </trans-unit>
        <trans-unit id="8ef7600ab8e39fc13b7dc9585804325c8072844d" translate="yes" xml:space="preserve">
          <source>To avoid instability issues in case the system is under-determined, regularization can be applied (Ridge regression) via the &lt;code&gt;ridge_alpha&lt;/code&gt; parameter.</source>
          <target state="translated">시스템이 결정되지 않은 경우 불안정성 문제를 피하기 위해 &lt;code&gt;ridge_alpha&lt;/code&gt; 매개 변수 를 통해 정규화를 적용 할 수 있습니다 (Ridge Regression) .</target>
        </trans-unit>
        <trans-unit id="622754a0a375aafa66f9d8336ffd00fcfc0a1948" translate="yes" xml:space="preserve">
          <source>To avoid memory copy the caller should pass a CSC matrix.</source>
          <target state="translated">메모리 복사를 피하려면 호출자가 CSC 매트릭스를 전달해야합니다.</target>
        </trans-unit>
        <trans-unit id="21a05d95ccb73f51d245c302b02e9a8f32df0276" translate="yes" xml:space="preserve">
          <source>To avoid memory copy the caller should pass a CSR matrix.</source>
          <target state="translated">메모리 복사를 피하려면 호출자가 CSR 매트릭스를 전달해야합니다.</target>
        </trans-unit>
        <trans-unit id="e5ab0a4f687079cc593610e8d8c0f15b79824d4d" translate="yes" xml:space="preserve">
          <source>To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format.</source>
          <target state="translated">메모리 재 할당을 피하려면 해당 형식을 사용하여 메모리에 초기 데이터를 직접 할당하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="ee93c7e2ac06e08c1567b0ca209ad480ea5f1b80" translate="yes" xml:space="preserve">
          <source>To avoid the computation of global clustering, for every call of &lt;code&gt;partial_fit&lt;/code&gt; the user is advised</source>
          <target state="translated">글로벌 클러스터링 계산을 피하기 위해 &lt;code&gt;partial_fit&lt;/code&gt; 을 호출 할 때마다 사용자에게 권장됩니다.</target>
        </trans-unit>
        <trans-unit id="e81cbf7353acbc69eeae43ca8cf143e58e658d10" translate="yes" xml:space="preserve">
          <source>To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called &lt;code&gt;tf&lt;/code&gt; for Term Frequencies.</source>
          <target state="translated">이러한 잠재적 불일치를 피하려면 문서에서 각 단어의 발생 횟수를 문서의 총 단어 수로 나누면 충분합니다. 이러한 새로운 기능 을 용어 빈도의 경우 &lt;code&gt;tf&lt;/code&gt; 라고 합니다.</target>
        </trans-unit>
        <trans-unit id="39f01dfdcdf5847fd1935ba52ba9be2bfc80430b" translate="yes" xml:space="preserve">
          <source>To avoid this problem, nested CV effectively uses a series of train/validation/test set splits. In the inner loop (here executed by &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt;), the score is approximately maximized by fitting a model to each training set, and then directly maximized in selecting (hyper)parameters over the validation set. In the outer loop (here in &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt;), generalization error is estimated by averaging test set scores over several dataset splits.</source>
          <target state="translated">이 문제를 피하기 위해 중첩 된 CV는 일련의 기차 / 검증 / 테스트 세트 스플릿을 효과적으로 사용합니다. 내부 루프 (여기서는 &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; 에&lt;/a&gt; 의해 실행 됨 )에서, 각 트레이닝 세트에 모델을 맞추면 점수가 대략 최대화 된 다음 검증 세트에서 (하이퍼) 파라미터를 선택하여 직접 최대화됩니다. 외부 루프 (여기에서 &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; )에서 일반화 오류는 여러 데이터 세트 분할에 대한 테스트 세트 점수를 평균하여 추정됩니다.</target>
        </trans-unit>
        <trans-unit id="a37b39aad5fcf98e98548e781cdec5193cfe7b97" translate="yes" xml:space="preserve">
          <source>To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array.</source>
          <target state="translated">불필요한 메모리 중복을 피하려면 fit 메소드의 X 인수를 Fortran-contiguous numpy 배열로 직접 전달해야합니다.</target>
        </trans-unit>
        <trans-unit id="d9c2f7485084c926a2f68d8587d615406cc01649" translate="yes" xml:space="preserve">
          <source>To be in favorable recovery conditions, we sample the data from a model with a sparse inverse covariance matrix. In addition, we ensure that the data is not too much correlated (limiting the largest coefficient of the precision matrix) and that there a no small coefficients in the precision matrix that cannot be recovered. In addition, with a small number of observations, it is easier to recover a correlation matrix rather than a covariance, thus we scale the time series.</source>
          <target state="translated">유리한 회복 조건에 있도록 희소 역공 분산 행렬을 사용하여 모델에서 데이터를 샘플링합니다. 또한, 데이터가 너무 상관 관계가없고 (정밀도 행렬의 최대 계수 제한) 정밀도 행렬에 복구 할 수없는 작은 계수가 없는지 확인합니다. 또한, 관측치가 적 으면 공분산보다는 상관 행렬을 복구하는 것이 더 쉬워 시계열의 크기를 조정합니다.</target>
        </trans-unit>
        <trans-unit id="f7fd313aae703eaa110952d34fbc2e74f81a873c" translate="yes" xml:space="preserve">
          <source>To be removed in 0.21</source>
          <target state="translated">0.21에서 제거</target>
        </trans-unit>
        <trans-unit id="b656a9f4366f6cbcc5b1e6914e7bc1a8d099ee57" translate="yes" xml:space="preserve">
          <source>To be removed in 0.22</source>
          <target state="translated">0.22에서 제거</target>
        </trans-unit>
        <trans-unit id="bc387423485c5a73576ae6f9089ec34a8b143ae6" translate="yes" xml:space="preserve">
          <source>To begin with, all values for \(r\) and \(a\) are set to zero, and the calculation of each iterates until convergence. As discussed above, in order to avoid numerical oscillations when updating the messages, the damping factor \(\lambda\) is introduced to iteration process:</source>
          <target state="translated">우선, \ (r \) 및 \ (a \)의 모든 값은 0으로 설정되며 각 계산은 수렴 될 때까지 반복됩니다. 위에서 설명한 것처럼, 메시지를 업데이트 할 때 수치 진동을 피하기 위해 감쇠 프로세스 \ (\ lambda \)가 반복 프로세스에 도입되었습니다.</target>
        </trans-unit>
        <trans-unit id="ebc5cb56aa5d3da850d595b902c1384fa4142906" translate="yes" xml:space="preserve">
          <source>To begin, we&amp;rsquo;ll visualize our data.</source>
          <target state="translated">먼저 데이터를 시각화합니다.</target>
        </trans-unit>
        <trans-unit id="57e47e513e200b11a216f9768279c1f81e7b3157" translate="yes" xml:space="preserve">
          <source>To benchmark different estimators for your case you can simply change the &lt;code&gt;n_features&lt;/code&gt; parameter in this example: &lt;a href=&quot;../auto_examples/applications/plot_prediction_latency#sphx-glr-auto-examples-applications-plot-prediction-latency-py&quot;&gt;Prediction Latency&lt;/a&gt;. This should give you an estimate of the order of magnitude of the prediction latency.</source>
          <target state="translated">사례에 대해 다른 추정량을 벤치마킹하려면 이 예에서 &lt;code&gt;n_features&lt;/code&gt; 매개 변수를 변경하면 됩니다 : &lt;a href=&quot;../auto_examples/applications/plot_prediction_latency#sphx-glr-auto-examples-applications-plot-prediction-latency-py&quot;&gt;예측 지연 시간&lt;/a&gt; . 이를 통해 예측 대기 시간의 크기 순서를 추정 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="892d831a9e807296347eacb2e8474830ca349663" translate="yes" xml:space="preserve">
          <source>To compare a set of found biclusters to the set of true biclusters, two similarity measures are needed: a similarity measure for individual biclusters, and a way to combine these individual similarities into an overall score.</source>
          <target state="translated">발견 된 biclusters 세트와 실제 biclusters 세트를 비교하려면 두 개의 유사성 측정이 필요합니다. 개별 biclusters에 대한 유사성 측정과 이러한 개별 유사성을 전체 점수로 결합하는 방법입니다.</target>
        </trans-unit>
        <trans-unit id="f0ff37a06cd777b22ebe208ab3110388f720b201" translate="yes" xml:space="preserve">
          <source>To compare individual biclusters, several measures have been used. For now, only the Jaccard index is implemented:</source>
          <target state="translated">개별 biclusters를 비교하기 위해 몇 가지 측정이 사용되었습니다. 현재 Jaccard 인덱스 만 구현되었습니다.</target>
        </trans-unit>
        <trans-unit id="30a2aa60dabe3d1d8b8497c6228442c6c55454f4" translate="yes" xml:space="preserve">
          <source>To control display of warnings.</source>
          <target state="translated">경고 표시를 제어합니다.</target>
        </trans-unit>
        <trans-unit id="6c3d05eecff544d238db6888c87daeb42794f44b" translate="yes" xml:space="preserve">
          <source>To control the verbosity of the procedure.</source>
          <target state="translated">절차의 상세 정도를 제어합니다.</target>
        </trans-unit>
        <trans-unit id="d66f891ca7bde7537002ad52d27fc9dd62dd5881" translate="yes" xml:space="preserve">
          <source>To convert categorical features to such integer codes, we can use the &lt;a href=&quot;generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt;&lt;code&gt;OrdinalEncoder&lt;/code&gt;&lt;/a&gt;. This estimator transforms each categorical feature to one new feature of integers (0 to n_categories - 1):</source>
          <target state="translated">범주 형 기능을 이러한 정수 코드로 변환하기 위해 &lt;a href=&quot;generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt; &lt;code&gt;OrdinalEncoder&lt;/code&gt; 를&lt;/a&gt; 사용할 수 있습니다 . 이 추정기는 각 범주 형 피쳐를 정수의 새로운 피쳐 (0-n_categories-1)로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="693e8d8ca1982fe1e279b5869b2b710976d06558" translate="yes" xml:space="preserve">
          <source>To counter this effect we can discount the expected RI \(E[\text{RI}]\) of random labelings by defining the adjusted Rand index as follows:</source>
          <target state="translated">이 효과를 방지하기 위해 다음과 같이 조정 된 랜드 인덱스를 정의하여 임의 레이블의 예상 RI \ (E [\ text {RI}] \)를 할인 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2ccfac714af4138a2df70ede11b2ff4e1963a414" translate="yes" xml:space="preserve">
          <source>To create positive examples click the left mouse button; to create negative examples click the right button.</source>
          <target state="translated">긍정적 인 예를 만들려면 마우스 왼쪽 버튼을 클릭하십시오. 부정적인 예를 만들려면 오른쪽 버튼을 클릭하십시오.</target>
        </trans-unit>
        <trans-unit id="d0bfc36f728f01d8f998e7e774b5cc731a5652d7" translate="yes" xml:space="preserve">
          <source>To disable convergence detection based on inertia, set max_no_improvement to None.</source>
          <target state="translated">관성을 기준으로 수렴 감지를 비활성화하려면 max_no_improvement를 None으로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="644ea86209186f0b63818c18611416bf68aa348b" translate="yes" xml:space="preserve">
          <source>To disable convergence detection based on normalized center change, set tol to 0.0 (default).</source>
          <target state="translated">정규화 된 중심 변경을 기반으로 수렴 감지를 비활성화하려면 l을 0.0 (기본값)으로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="8f49411326bd4f684fb56ae33f90d4fd9150ab8c" translate="yes" xml:space="preserve">
          <source>To do the exercises, copy the content of the &amp;lsquo;skeletons&amp;rsquo; folder as a new folder named &amp;lsquo;workspace&amp;rsquo;:</source>
          <target state="translated">연습을하려면 'skeletons'폴더의 내용을 'workspace'라는 새 폴더로 복사하십시오.</target>
        </trans-unit>
        <trans-unit id="79cf44a84fa8878b10f291a31335b47430451015" translate="yes" xml:space="preserve">
          <source>To each column, a different transformation can be applied, such as preprocessing or a specific feature extraction method:</source>
          <target state="translated">각 열에 전처리 또는 특정 기능 추출 방법과 같은 다른 변환을 적용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a0a5ce85df1e1aafd2ebf61b7efd7098beb62d7b" translate="yes" xml:space="preserve">
          <source>To estimate a probabilistic model (e.g. a Gaussian model), estimating the precision matrix, that is the inverse covariance matrix, is as important as estimating the covariance matrix. Indeed a Gaussian model is parametrized by the precision matrix.</source>
          <target state="translated">확률 적 모델 (예 : 가우시안 모델)을 추정하려면 역 공분산 행렬 인 정밀 행렬을 추정하는 것이 공분산 행렬을 추정하는 것만 큼 중요합니다. 실제로 가우스 모델은 정밀 행렬로 매개 변수화됩니다.</target>
        </trans-unit>
        <trans-unit id="06985e50b51113b200d13cecad3eedd2a07fa798" translate="yes" xml:space="preserve">
          <source>To evaluate the impact of the scale of the dataset (&lt;code&gt;n_samples&lt;/code&gt; and &lt;code&gt;n_features&lt;/code&gt;) while controlling the statistical properties of the data (typically the correlation and informativeness of the features), it is also possible to generate synthetic data.</source>
          <target state="translated">데이터 의 통계적 속성 (일반적으로 피처의 상관 관계 및 정보)을 제어하면서 데이터 세트의 스케일 ( &lt;code&gt;n_samples&lt;/code&gt; 및 &lt;code&gt;n_features&lt;/code&gt; ) 의 영향을 평가하기 위해 합성 데이터를 생성 할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="8b81da86f6a51388bf88e8748866dfbc1da10ddb" translate="yes" xml:space="preserve">
          <source>To fully specify a dataset, you need to provide a name and a version, though the version is optional, see &lt;a href=&quot;#openml-versions&quot;&gt;Dataset Versions&lt;/a&gt; below. The dataset contains a total of 1080 examples belonging to 8 different classes:</source>
          <target state="translated">데이터 세트를 완전히 지정하려면 이름과 버전을 제공해야합니다. 버전은 선택 사항이지만 아래의 &lt;a href=&quot;#openml-versions&quot;&gt;데이터 세트 버전을&lt;/a&gt; 참조하십시오 . 데이터 세트에는 8 개의 다른 클래스에 속하는 총 1080 개의 예제가 있습니다.</target>
        </trans-unit>
        <trans-unit id="eddbe44ecc236b178d14592239180b4231c2f462" translate="yes" xml:space="preserve">
          <source>To get a better measure of prediction accuracy (which we can use as a proxy for goodness of fit of the model), we can successively split the data in &lt;em&gt;folds&lt;/em&gt; that we use for training and testing:</source>
          <target state="translated">(우리는 모델의 적합도에 대한 프록시로 사용할 수 있습니다) 예측 정확도의 더 나은 측정을 얻으려면, 우리는 연속적으로 데이터 분할 할 수 있습니다 &lt;em&gt;주름을&lt;/em&gt; 우리가 교육 및 테스트에 사용하는 :</target>
        </trans-unit>
        <trans-unit id="8f2c7c86e5d1b0f8592203b4a46517414e54ce05" translate="yes" xml:space="preserve">
          <source>To get identical results for each split, set &lt;code&gt;random_state&lt;/code&gt; to an integer.</source>
          <target state="translated">각 분할에 대해 동일한 결과를 얻으려면 &lt;code&gt;random_state&lt;/code&gt; 를 정수로 설정하십시오 .</target>
        </trans-unit>
        <trans-unit id="0228140936e0aced4eaa7c77d90637025c4d0909" translate="yes" xml:space="preserve">
          <source>To get started with this tutorial, you must first install &lt;em&gt;scikit-learn&lt;/em&gt; and all of its required dependencies.</source>
          <target state="translated">이 자습서를 시작하려면 먼저 &lt;em&gt;scikit-learn&lt;/em&gt; 및 모든 필수 종속성을 설치해야합니다 .</target>
        </trans-unit>
        <trans-unit id="8d91bad777aec839541c338ab9f11be081ee54c6" translate="yes" xml:space="preserve">
          <source>To get the signed distance to the hyperplane use &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier.decision_function&quot;&gt;&lt;code&gt;SGDClassifier.decision_function&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">초평면까지의 서명 거리를 얻으려면 &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier.decision_function&quot;&gt; &lt;code&gt;SGDClassifier.decision_function&lt;/code&gt; 을&lt;/a&gt; 사용 하십시오 .</target>
        </trans-unit>
        <trans-unit id="6ae2612052e54b7be6598947088a287fffd01403" translate="yes" xml:space="preserve">
          <source>To illustrate &lt;a href=&quot;generated/sklearn.dummy.dummyclassifier#sklearn.dummy.DummyClassifier&quot;&gt;&lt;code&gt;DummyClassifier&lt;/code&gt;&lt;/a&gt;, first let&amp;rsquo;s create an imbalanced dataset:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.dummy.dummyclassifier#sklearn.dummy.DummyClassifier&quot;&gt; &lt;code&gt;DummyClassifier&lt;/code&gt; &lt;/a&gt; 를 설명하기 위해 먼저 불균형 데이터 셋을 만들어 봅시다 :</target>
        </trans-unit>
        <trans-unit id="e7a02a8f3e922c68cd6ce64dca33ce54683ffb1b" translate="yes" xml:space="preserve">
          <source>To illustrate this with a simple example, let&amp;rsquo;s assume we have 3 classifiers and a 3-class classification problems where we assign equal weights to all classifiers: w1=1, w2=1, w3=1.</source>
          <target state="translated">간단한 예를 통해이를 설명하기 위해 모든 분류 자에 동일한 가중치를 할당하는 3 개의 분류 자 ​​및 3 가지 분류 문제가 있다고 가정 해 봅시다 : w1 = 1, w2 = 1, w3 = 1.</target>
        </trans-unit>
        <trans-unit id="27fe4060cc8aa9166cda2609863b9fdd12999baf" translate="yes" xml:space="preserve">
          <source>To illustrate this, PCA is performed comparing the use of data with &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt; applied, to unscaled data. The results are visualized and a clear difference noted. The 1st principal component in the unscaled set can be seen. It can be seen that feature #13 dominates the direction, being a whole two orders of magnitude above the other features. This is contrasted when observing the principal component for the scaled version of the data. In the scaled version, the orders of magnitude are roughly the same across all the features.</source>
          <target state="translated">이를 설명하기 위해 PCA는 &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; 가&lt;/a&gt; 적용된 데이터 사용과 비 스케일 데이터를 비교하여 수행 됩니다. 결과가 시각화되고 명확한 차이가 나타납니다. 스케일링되지 않은 세트의 첫 번째 주성분을 볼 수 있습니다. 특징 # 13이 방향을 지배하는 것을 알 수 있으며, 다른 특징보다 전체적으로 2 배의 크기이다. 이는 스케일링 된 데이터 버전의 주성분을 관찰 할 때 대비됩니다. 확장 버전에서, 크기의 순서는 모든 기능에서 거의 동일합니다.</target>
        </trans-unit>
        <trans-unit id="952f60109f87198cc4767d483a08ad921abb5966" translate="yes" xml:space="preserve">
          <source>To improve the conditioning of the problem (i.e. mitigating the &lt;a href=&quot;#curse-of-dimensionality&quot;&gt;The curse of dimensionality&lt;/a&gt;), it would be interesting to select only the informative features and set non-informative ones, like feature 2 to 0. Ridge regression will decrease their contribution, but not set them to zero. Another penalization approach, called &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; (least absolute shrinkage and selection operator), can set some coefficients to zero. Such methods are called &lt;strong&gt;sparse method&lt;/strong&gt; and sparsity can be seen as an application of Occam&amp;rsquo;s razor: &lt;em&gt;prefer simpler models&lt;/em&gt;.</source>
          <target state="translated">문제의 컨디셔닝을 개선하기 위해 (즉 , &lt;a href=&quot;#curse-of-dimensionality&quot;&gt;차원의 저주&lt;/a&gt; 완화 ), 특징적인 기능 만 선택하고 기능 2에서 0과 같은 비 정보적인 기능을 설정하는 것이 흥미로울 것입니다. 그것들을 0으로 만듭니다. &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; (최소 절대 수축 및 선택 연산자) 라고하는 또 다른 벌칙 접근법 은 일부 계수를 0으로 설정할 수 있습니다. 이러한 방법을 &lt;strong&gt;스파 스 방법&lt;/strong&gt; 이라고 하며 희소성은 Occam의 면도기를 적용한 것으로 볼 수 있습니다 . &lt;em&gt;더 간단한 모델 선호&lt;/em&gt; .</target>
        </trans-unit>
        <trans-unit id="f028c20036ea78694db90136b8cb3004f099e0bf" translate="yes" xml:space="preserve">
          <source>To limit the memory consumption, we queue examples up to a fixed amount before feeding them to the learner.</source>
          <target state="translated">메모리 소비를 제한하기 위해 예제를 학습자에게 공급하기 전에 고정 된 양까지 대기열에 넣습니다.</target>
        </trans-unit>
        <trans-unit id="61f860c325e06c4f97b9f4c7ced3d5279054856d" translate="yes" xml:space="preserve">
          <source>To load from an external dataset, please refer to &lt;a href=&quot;../../datasets/index#external-datasets&quot;&gt;loading external datasets&lt;/a&gt;.</source>
          <target state="translated">외부 데이터 세트에서로드하려면 &lt;a href=&quot;../../datasets/index#external-datasets&quot;&gt;외부 데이터 세트로드&lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="d787fd22509da728f07846c2b5d3ecac1d6b4105" translate="yes" xml:space="preserve">
          <source>To load the data and visualize the images:</source>
          <target state="translated">데이터를로드하고 이미지를 시각화하려면</target>
        </trans-unit>
        <trans-unit id="b4be4dde535adc435619c6f0e295e0ce05bad72b" translate="yes" xml:space="preserve">
          <source>To make the example run faster, we use very few hidden units, and train only for a very short time. Training longer would result in weights with a much smoother spatial appearance.</source>
          <target state="translated">예제를 더 빠르게 실행하기 위해 숨겨진 유닛을 거의 사용하지 않고 짧은 시간 동안 만 훈련합니다. 더 오래 훈련하면 훨씬 더 부드러운 공간 모양의 가중치가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="96ba992a3c5af68caa74d107191c5a3c32806c93" translate="yes" xml:space="preserve">
          <source>To make the preprocessor, tokenizer and analyzers aware of the model parameters it is possible to derive from the class and override the &lt;code&gt;build_preprocessor&lt;/code&gt;, &lt;code&gt;build_tokenizer&lt;/code&gt; and &lt;code&gt;build_analyzer&lt;/code&gt; factory methods instead of passing custom functions.</source>
          <target state="translated">전 처리기, 토크 나이저 및 분석기가 모델 매개 변수를 인식하도록하기 위해 사용자 정의 함수를 전달하는 대신 클래스에서 파생하여 &lt;code&gt;build_preprocessor&lt;/code&gt; , &lt;code&gt;build_tokenizer&lt;/code&gt; 및 &lt;code&gt;build_analyzer&lt;/code&gt; 팩토리 메소드를 대체 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c0f08b8475e4b67e5147698ce9ccb818f0394d27" translate="yes" xml:space="preserve">
          <source>To make this more explicit, consider the following notation:</source>
          <target state="translated">이를보다 명확하게하려면 다음 표기법을 고려하십시오.</target>
        </trans-unit>
        <trans-unit id="8ada09feb86f8f3751dffbeeaba0e1e4f69156a7" translate="yes" xml:space="preserve">
          <source>To obtain a fully probabilistic model, the output \(y\) is assumed to be Gaussian distributed around \(X w\):</source>
          <target state="translated">완전 확률 모델을 얻기 위해 출력 \ (y \)는 \ (X w \) 주위에 가우시안 분포 된 것으로 가정합니다.</target>
        </trans-unit>
        <trans-unit id="65178eef58b048e690e1c210520e38da80789880" translate="yes" xml:space="preserve">
          <source>To perform classification with generalized linear models, see &lt;a href=&quot;#logistic-regression&quot;&gt;Logistic regression&lt;/a&gt;.</source>
          <target state="translated">일반화 된 선형 모형으로 분류를 수행하려면 &lt;a href=&quot;#logistic-regression&quot;&gt;로지스틱 회귀를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="72dc32b7225454e8d7c0ec26f14d95b55df2c79a" translate="yes" xml:space="preserve">
          <source>To quantify estimation error, we plot the likelihood of unseen data for different values of the shrinkage parameter. We also show the choices by cross-validation, or with the LedoitWolf and OAS estimates.</source>
          <target state="translated">추정 오차를 정량화하기 위해 수축 매개 변수의 다른 값에 대해 보이지 않는 데이터의 가능성을 표시합니다. 또한 교차 검증 또는 LedoitWolf 및 OAS 추정을 통해 선택 사항을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="397393d3de29d9ed57b4f231bd553afc264bafab" translate="yes" xml:space="preserve">
          <source>To return the corresponding classical subsets of kddcup 99. If None, return the entire kddcup 99 dataset.</source>
          <target state="translated">kddcup 99의 해당 클래식 하위 집합을 반환하려면 None 인 경우 전체 kddcup 99 데이터 집합을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="b0e502baa68f0434bb574994337b41db58fa07c4" translate="yes" xml:space="preserve">
          <source>To run cross-validation on multiple metrics and also to return train scores, fit times and score times.</source>
          <target state="translated">여러 메트릭에서 교차 유효성 검사를 실행하고 기차 점수, 시간 및 점수 시간을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="2d79b95a40b4d1ea2f276f13509aebc984e2d932" translate="yes" xml:space="preserve">
          <source>To see how this generalizes the binary log loss given above, note that in the binary case, \(p_{i,0} = 1 - p_{i,1}\) and \(y_{i,0} = 1 - y_{i,1}\), so expanding the inner sum over \(y_{i,k} \in \{0,1\}\) gives the binary log loss.</source>
          <target state="translated">이것이 위에서 주어진 이진 로그 손실을 일반화하는 방법을 보려면 이진 경우 \ (p_ {i, 0} = 1-p_ {i, 1} \) 및 \ (y_ {i, 0} = 1- y_ {i, 1} \)이므로 \ (y_ {i, k} \ in \ {0,1 \} \)에 대한 내부 합계를 확장하면 이진 로그 손실이 발생합니다.</target>
        </trans-unit>
        <trans-unit id="25684d8b1766d360b665e8498a44a626b2b5bd13" translate="yes" xml:space="preserve">
          <source>To set &lt;code&gt;n_clusters=None&lt;/code&gt; initially</source>
          <target state="translated">&lt;code&gt;n_clusters=None&lt;/code&gt; 을 초기에 설정하려면</target>
        </trans-unit>
        <trans-unit id="78f293aec6a6c458449c6dc3bcd71b696525a449" translate="yes" xml:space="preserve">
          <source>To speed up the algorithm, accept only those bins with at least min_bin_freq points as seeds.</source>
          <target state="translated">알고리즘 속도를 높이려면 최소 min_bin_freq 포인트가 시드로 지정된 빈만 허용하십시오.</target>
        </trans-unit>
        <trans-unit id="e1dfcbed698085a7ab462cf760bf24e65a9e8400" translate="yes" xml:space="preserve">
          <source>To speed up the algorithm, accept only those bins with at least min_bin_freq points as seeds. If not defined, set to 1.</source>
          <target state="translated">알고리즘 속도를 높이려면 최소 min_bin_freq 포인트가 시드로 지정된 빈만 허용하십시오. 정의되지 않은 경우 1로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="fd2f04d7c6e080a2cce0d2e7339e598ba17acac5" translate="yes" xml:space="preserve">
          <source>To try to predict the outcome on a new document we need to extract the features using almost the same feature extracting chain as before. The difference is that we call &lt;code&gt;transform&lt;/code&gt; instead of &lt;code&gt;fit_transform&lt;/code&gt; on the transformers, since they have already been fit to the training set:</source>
          <target state="translated">새 문서에서 결과를 예측하려면 이전과 거의 동일한 기능 추출 체인을 사용하여 기능을 추출해야합니다. 차이점은 &lt;code&gt;transform&lt;/code&gt; 에서 &lt;code&gt;fit_transform&lt;/code&gt; 대신 transform 을 호출한다는 것 입니다. 트랜스포머는 이미 훈련 세트에 적합했기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="5da67914dc5314b6125944bb748e1a3d6c08f736" translate="yes" xml:space="preserve">
          <source>To understand the use of LDA in dimensionality reduction, it is useful to start with a geometric reformulation of the LDA classification rule explained above. We write \(K\) for the total number of target classes. Since in LDA we assume that all classes have the same estimated covariance \(\Sigma\), we can rescale the data so that this covariance is the identity:</source>
          <target state="translated">차원 축소에서 LDA의 사용을 이해하려면 위에서 설명한 LDA 분류 규칙의 기하학적 재구성으로 시작하는 것이 유용합니다. 총 대상 클래스 수는 \ (K \)입니다. LDA에서는 모든 클래스의 추정 된 공분산 \ (\ Sigma \)가 동일하다고 가정하므로이 공분산이 항등이되도록 데이터의 크기를 조정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6df5e0eab0e1e02def9ae99e68c6ddf1a841d6d1" translate="yes" xml:space="preserve">
          <source>To use &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you need to instantiate the estimator with the &lt;code&gt;novelty&lt;/code&gt; parameter set to &lt;code&gt;True&lt;/code&gt; before fitting the estimator:</source>
          <target state="translated">참신 탐지에 레이블 (예 : 레이블을 예측하거나 새로운 보이지 않는 데이터의 비정상 점수를 계산)에 &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt; 을 사용하려면 추정기를 맞추기 전에 &lt;code&gt;novelty&lt;/code&gt; 매개 변수를 &lt;code&gt;True&lt;/code&gt; 로 설정 하여 추정기를 인스턴스화해야합니다 .</target>
        </trans-unit>
        <trans-unit id="011ed6ad19bc2f123579a7e50ff8b4dad33bf360" translate="yes" xml:space="preserve">
          <source>To use joblib.Memory to cache the svmlight file:</source>
          <target state="translated">joblib.Memory를 사용하여 svmlight 파일을 캐시하려면 다음을 수행하십시오.</target>
        </trans-unit>
        <trans-unit id="e11936ea84de206f18d8b708b6f4eca9fb6c8b59" translate="yes" xml:space="preserve">
          <source>To use text files in a scikit-learn classification or clustering algorithm, you will need to use the &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; module to build a feature extraction transformer that suits your problem.</source>
          <target state="translated">scikit-learn 분류 또는 클러스터링 알고리즘에서 텍스트 파일을 사용하려면 &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; 모듈을 사용하여 문제에 맞는 기능 추출 변환기를 작성해야합니다.</target>
        </trans-unit>
        <trans-unit id="c7aa2d2ef894f356c354736ecb4235681bee95b2" translate="yes" xml:space="preserve">
          <source>To use this dataset with scikit-learn, we transform each 8x8 image into a feature vector of length 64</source>
          <target state="translated">이 데이터 세트를 scikit-learn과 함께 사용하기 위해 각 8x8 이미지를 길이가 64 인 특징 벡터로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="f5d271c927cff9ea25de07089e51938b7e86a2a7" translate="yes" xml:space="preserve">
          <source>To use this model as a classifier, we just need to estimate from the training data the class priors \(P(y=k)\) (by the proportion of instances of class \(k\)), the class means \(\mu_k\) (by the empirical sample class means) and the covariance matrices (either by the empirical sample class covariance matrices, or by a regularized estimator: see the section on shrinkage below).</source>
          <target state="translated">이 모델을 분류 자로 사용하려면 학습 데이터에서 클래스 우선 순위 \ (P (y = k) \) (클래스 \ (k \)의 인스턴스 비율에 따라)를 추정하면됩니다. \ mu_k \) (경험적 샘플 클래스 수단에 의해) 및 공분산 행렬 (경험적 샘플 클래스 공분산 행렬에 의해 또는 정규화 된 추정기에 의해 : 아래 수축에 대한 섹션 참조).</target>
        </trans-unit>
        <trans-unit id="5d015bfb570917361c4b4abaaa59f5e623d8c463" translate="yes" xml:space="preserve">
          <source>To validate a model we need a scoring function (see &lt;a href=&quot;model_evaluation#model-evaluation&quot;&gt;Model evaluation: quantifying the quality of predictions&lt;/a&gt;), for example accuracy for classifiers. The proper way of choosing multiple hyperparameters of an estimator are of course grid search or similar methods (see &lt;a href=&quot;grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt;) that select the hyperparameter with the maximum score on a validation set or multiple validation sets. Note that if we optimized the hyperparameters based on a validation score the validation score is biased and not a good estimate of the generalization any longer. To get a proper estimate of the generalization we have to compute the score on another test set.</source>
          <target state="translated">모델의 유효성을 검사하려면 분류기의 정확도와 같은 스코어링 기능 ( &lt;a href=&quot;model_evaluation#model-evaluation&quot;&gt;모델 평가 : 예측 품질 정량화&lt;/a&gt; 참조)이 필요합니다 . 추정기의 다중 하이퍼 파라미터를 선택하는 올바른 방법은 검증 세트 또는 다중 검증 세트에서 최대 점수를 갖는 &lt;a href=&quot;grid_search#grid-search&quot;&gt;하이퍼 파라미터&lt;/a&gt; 를 선택하는 그리드 검색 또는 유사한 방법 ( 추정기의 하이퍼 파라미터 조정 참조 )입니다. 유효성 검사 점수를 기준으로 하이퍼 파라미터를 최적화하면 유효성 검사 점수가 편향되어 일반화를 더 이상 잘 평가할 수 없습니다. 일반화의 적절한 추정치를 얻으려면 다른 테스트 세트에서 점수를 계산해야합니다.</target>
        </trans-unit>
        <trans-unit id="baa10199f999bc30e58b0035ac2f6e51132399ed" translate="yes" xml:space="preserve">
          <source>To visualize the probability weighting, we fit each classifier on the training set and plot the predicted class probabilities for the first sample in this example dataset.</source>
          <target state="translated">확률 가중치를 시각화하기 위해 각 분류자를 학습 세트에 맞추고이 예제 데이터 세트의 첫 번째 샘플에 대해 예측 된 클래스 확률을 플로팅합니다.</target>
        </trans-unit>
        <trans-unit id="ba234a16bb1a2ae4619585ca04988c1afd574060" translate="yes" xml:space="preserve">
          <source>Tokenize the documents and count the occurrences of token and return them as a sparse matrix</source>
          <target state="translated">문서를 토큰 화하고 토큰 발생을 계산하여 희소 행렬로 반환</target>
        </trans-unit>
        <trans-unit id="e89caeb25fc24a274e225b242d49cc6fb7ddfa72" translate="yes" xml:space="preserve">
          <source>Tokenizing text with &lt;code&gt;scikit-learn&lt;/code&gt;</source>
          <target state="translated">scikit &lt;code&gt;scikit-learn&lt;/code&gt; 텍스트 토큰 화</target>
        </trans-unit>
        <trans-unit id="45d4a0ebe499a5d042ac0f7bc4284501d3667758" translate="yes" xml:space="preserve">
          <source>Tolerance for &amp;lsquo;arpack&amp;rsquo; method Not used if eigen_solver==&amp;rsquo;dense&amp;rsquo;.</source>
          <target state="translated">'arpack'방법에 대한 허용 오차 eigen_solver == 'dense'인 경우 사용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="318dfc593e0123f93a8fe309f411532f48eea756" translate="yes" xml:space="preserve">
          <source>Tolerance for ARPACK. 0 means machine precision. Ignored by randomized SVD solver.</source>
          <target state="translated">ARPACK에 대한 허용 오차. 0은 기계 정밀도를 의미합니다. 무작위 SVD 솔버에서는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="13511570864a98fa2d61f43da929b11b4894937f" translate="yes" xml:space="preserve">
          <source>Tolerance for Hessian eigenmapping method. Only used if &lt;code&gt;method == 'hessian'&lt;/code&gt;</source>
          <target state="translated">Hessian 고유 매핑 방법에 대한 허용 오차. &lt;code&gt;method == 'hessian'&lt;/code&gt; 경우에만 사용</target>
        </trans-unit>
        <trans-unit id="e4c877ba267607a99e62c8b31f7891feda117cf7" translate="yes" xml:space="preserve">
          <source>Tolerance for Hessian eigenmapping method. Only used if method == &amp;lsquo;hessian&amp;rsquo;</source>
          <target state="translated">Hessian 고유 매핑 방법에 대한 허용 오차. method == 'hessian'인 경우에만 사용</target>
        </trans-unit>
        <trans-unit id="aeb25ea9c0101939a4336136b4e11db71f1bb1be" translate="yes" xml:space="preserve">
          <source>Tolerance for modified LLE method. Only used if &lt;code&gt;method == 'modified'&lt;/code&gt;</source>
          <target state="translated">수정 된 LLE 방법에 대한 허용 오차. &lt;code&gt;method == 'modified'&lt;/code&gt; 경우에만 사용</target>
        </trans-unit>
        <trans-unit id="b6502cfb6f414093cd5faf0376953824eae5e86f" translate="yes" xml:space="preserve">
          <source>Tolerance for modified LLE method. Only used if method == &amp;lsquo;modified&amp;rsquo;</source>
          <target state="translated">수정 된 LLE 방법에 대한 허용 오차. method == 'modified'인 경우에만 사용</target>
        </trans-unit>
        <trans-unit id="40eaf2d9a188116c07595886d4a67c9121557ecf" translate="yes" xml:space="preserve">
          <source>Tolerance for singular values computed by svd_solver == &amp;lsquo;arpack&amp;rsquo;.</source>
          <target state="translated">svd_solver == 'arpack'에 의해 계산 된 특이 값에 대한 공차.</target>
        </trans-unit>
        <trans-unit id="a495f50d68c5f0d21905244c442ac1ec46831c6d" translate="yes" xml:space="preserve">
          <source>Tolerance for stopping criteria.</source>
          <target state="translated">정지 기준에 대한 허용 오차.</target>
        </trans-unit>
        <trans-unit id="1f900b2be351c5e1d6397b25c9a2e6c5e5c36343" translate="yes" xml:space="preserve">
          <source>Tolerance for stopping criterion.</source>
          <target state="translated">기준 중지에 대한 허용 오차.</target>
        </trans-unit>
        <trans-unit id="4d73abe23fd3517118aa70ae58840719c14ae6a0" translate="yes" xml:space="preserve">
          <source>Tolerance for the early stopping. When the loss is not improving by at least tol for &lt;code&gt;n_iter_no_change&lt;/code&gt; iterations (if set to a number), the training stops.</source>
          <target state="translated">조기 정지에 대한 허용 오차. &lt;code&gt;n_iter_no_change&lt;/code&gt; 반복 (숫자로 설정된 경우)에 대해 손실이 적어도 tol만큼 개선되지 않으면 훈련이 중지됩니다.</target>
        </trans-unit>
        <trans-unit id="334a1d6597d473e85cc8725e20828e0c9824ea02" translate="yes" xml:space="preserve">
          <source>Tolerance for the optimization. When the loss or score is not improving by at least &lt;code&gt;tol&lt;/code&gt; for &lt;code&gt;n_iter_no_change&lt;/code&gt; consecutive iterations, unless &lt;code&gt;learning_rate&lt;/code&gt; is set to &amp;lsquo;adaptive&amp;rsquo;, convergence is considered to be reached and training stops.</source>
          <target state="translated">최적화에 대한 허용 오차. &lt;code&gt;learning_rate&lt;/code&gt; 가 'adaptive'로 설정되어 있지 않으면 &lt;code&gt;n_iter_no_change&lt;/code&gt; 연속 반복에 대해 손실 또는 점수가 적어도 &lt;code&gt;tol&lt;/code&gt; 만큼 개선 되지 않으면 수렴에 도달 한 것으로 간주되고 교육이 중지됩니다.</target>
        </trans-unit>
        <trans-unit id="6938a4dcb29969d15aaa6cafefb8f09b830ed305" translate="yes" xml:space="preserve">
          <source>Tolerance for the stopping condition.</source>
          <target state="translated">정지 조건에 대한 허용 오차.</target>
        </trans-unit>
        <trans-unit id="3a49445cc3e76e8c0deab47f4b10c5bd7dc33960" translate="yes" xml:space="preserve">
          <source>Tolerance of the stopping condition.</source>
          <target state="translated">정지 조건의 허용 오차.</target>
        </trans-unit>
        <trans-unit id="48a48ded1ae1ed29a7ddaed19c15db301472918d" translate="yes" xml:space="preserve">
          <source>Tolerance on update at each iteration.</source>
          <target state="translated">각 반복에서 업데이트에 대한 공차.</target>
        </trans-unit>
        <trans-unit id="a2223ba588ac8a94dc6928512bbe1ae559b46f6b" translate="yes" xml:space="preserve">
          <source>Tolerance used in the iterative algorithm default 1e-06.</source>
          <target state="translated">반복 알고리즘 기본값 1e-06에 사용 된 공차입니다.</target>
        </trans-unit>
        <trans-unit id="20a2955c412dcae35aa2ef964ce8c2d4b1c07dcb" translate="yes" xml:space="preserve">
          <source>Tolerance when calculating spatial median.</source>
          <target state="translated">공간 중앙값을 계산할 때의 공차.</target>
        </trans-unit>
        <trans-unit id="f3d0c54c4b7882f5280f0492c26f2bf33d35d2a2" translate="yes" xml:space="preserve">
          <source>Tony Blair</source>
          <target state="translated">토니 블레어</target>
        </trans-unit>
        <trans-unit id="e1781cb6d03ccb2216639c1d54de7540b9fc2c2b" translate="yes" xml:space="preserve">
          <source>Tools for imputing missing values are discussed at &lt;a href=&quot;impute#impute&quot;&gt;Imputation of missing values&lt;/a&gt;.</source>
          <target state="translated">결 측값 대치 도구는 결 측값 &lt;a href=&quot;impute#impute&quot;&gt;대치&lt;/a&gt; 에서 설명 합니다 .</target>
        </trans-unit>
        <trans-unit id="0d184ce2992ee425b9d4cc3d528da94fb4da399d" translate="yes" xml:space="preserve">
          <source>Tophat kernel (&lt;code&gt;kernel = 'tophat'&lt;/code&gt;)</source>
          <target state="translated">Tophat 커널 ( &lt;code&gt;kernel = 'tophat'&lt;/code&gt; )</target>
        </trans-unit>
        <trans-unit id="0954aa60533f43dc3b2b9a9cbdee11a74f79eada" translate="yes" xml:space="preserve">
          <source>Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</source>
          <target state="translated">음이 아닌 행렬 인수 분해 및 잠재 된 Dirichlet 할당을 사용한 주제 추출</target>
        </trans-unit>
        <trans-unit id="97129616afbfcb01d33b44619c8bf267194395ac" translate="yes" xml:space="preserve">
          <source>Total Phenols:</source>
          <target state="translated">총 페놀 :</target>
        </trans-unit>
        <trans-unit id="24a9e81269d05c734577a89440230faee238f7b2" translate="yes" xml:space="preserve">
          <source>Total log-likelihood of the data in X.</source>
          <target state="translated">X에있는 데이터의 총 로그 우도</target>
        </trans-unit>
        <trans-unit id="4055747cee4593e58e4a6dcbfa7d6ccc61845cf0" translate="yes" xml:space="preserve">
          <source>Total number of documents. Only used in the &lt;code&gt;partial_fit&lt;/code&gt; method.</source>
          <target state="translated">총 문서 수 &lt;code&gt;partial_fit&lt;/code&gt; 메소드 에서만 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="aed40ed5719d059f29eba1177189a60b01059871" translate="yes" xml:space="preserve">
          <source>Total phenols</source>
          <target state="translated">총 페놀</target>
        </trans-unit>
        <trans-unit id="babba0bc0e9a3e36ce98f362a62519c8eacb94cb" translate="yes" xml:space="preserve">
          <source>Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method Andrew V. Knyazev &lt;a href=&quot;https://doi.org/10.1137%2FS1064827500366124&quot;&gt;https://doi.org/10.1137%2FS1064827500366124&lt;/a&gt;</source>
          <target state="translated">최적의 전처리 된 고유 솔버를 향하여 : 로컬 최적의 블록 전처리 된 공액 구배 방법 Andrew V. Knyazev &lt;a href=&quot;https://doi.org/10.1137%2FS1064827500366124&quot;&gt;https://doi.org/10.1137%2FS1064827500366124&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e3ae1e8d052cc2d0c25bbda7e5d0370ec624b1e8" translate="yes" xml:space="preserve">
          <source>Toy example of 1D regression using linear, polynomial and RBF kernels.</source>
          <target state="translated">선형, 다항식 및 RBF 커널을 사용한 1D 회귀 분석의 장난감 예.</target>
        </trans-unit>
        <trans-unit id="264fa08a131d6382d6715d8c951f2b5bea1c373c" translate="yes" xml:space="preserve">
          <source>Traceback example, note how the line of the error is indicated as well as the values of the parameter passed to the function that triggered the exception, even though the traceback happens in the child process:</source>
          <target state="translated">역 추적 예제, 하위 프로세스에서 역 추적이 발생하더라도 오류 행이 표시되는 방식과 예외를 트리거 한 함수에 전달 된 매개 변수의 값을 확인하십시오.</target>
        </trans-unit>
        <trans-unit id="8718fa41b5577d15733c0d074d4e6ea2d5f88486" translate="yes" xml:space="preserve">
          <source>Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.</source>
          <target state="translated">추적, International Computer of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, 2008 년 5 월.</target>
        </trans-unit>
        <trans-unit id="6e4826fce9da6f5f03d1b11115df13e0bc514c4a" translate="yes" xml:space="preserve">
          <source>Train all data by multiple calls to partial_fit.</source>
          <target state="translated">partial_fit을 여러 번 호출하여 모든 데이터를 학습합니다.</target>
        </trans-unit>
        <trans-unit id="bd98708380c7e60a9c0a687f254abca8478a36f8" translate="yes" xml:space="preserve">
          <source>Train and test sizes may be different in each fold, with a difference of at most &lt;code&gt;n_classes&lt;/code&gt;.</source>
          <target state="translated">최대 &lt;code&gt;n_classes&lt;/code&gt; 의 차이로 각 폴드마다 학습 및 테스트 크기가 다를 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1c08c1bee3835bcafdb50e8cdda68c68d71fa67e" translate="yes" xml:space="preserve">
          <source>Train error vs Test error</source>
          <target state="translated">열차 오차 vs 시험 오차</target>
        </trans-unit>
        <trans-unit id="357c94d50b669e3c60f0758a6140ed17dc81af61" translate="yes" xml:space="preserve">
          <source>Train l1-penalized logistic regression models on a binary classification problem derived from the Iris dataset.</source>
          <target state="translated">Iris 데이터 세트에서 파생 된 이진 분류 문제에 대해 l1 형벌 로지스틱 회귀 모델을 학습합니다.</target>
        </trans-unit>
        <trans-unit id="0cfcb0c276264df865da734aa7faab6c6b43fed6" translate="yes" xml:space="preserve">
          <source>Train the model using libsvm (low-level method)</source>
          <target state="translated">libsvm을 사용하여 모델 학습 (저수준 방법)</target>
        </trans-unit>
        <trans-unit id="ff5331ad7dc89bf5a9dd23c31ab738af7815c499" translate="yes" xml:space="preserve">
          <source>Training a classifier</source>
          <target state="translated">분류기 훈련</target>
        </trans-unit>
        <trans-unit id="0f0630eb2ecfdd0ed6f7defc6642e6c0143bcbf3" translate="yes" xml:space="preserve">
          <source>Training data</source>
          <target state="translated">훈련 데이터</target>
        </trans-unit>
        <trans-unit id="6c7c988c62ce8a65ab6394bf4f62bdef696bbe60" translate="yes" xml:space="preserve">
          <source>Training data, requires length = n_samples</source>
          <target state="translated">훈련 데이터, 길이 = n_samples 필요</target>
        </trans-unit>
        <trans-unit id="70fa8e3174eef9a1ccfc0bda8b38c7cfbf09ffd6" translate="yes" xml:space="preserve">
          <source>Training data, where n_samples in the number of samples and n_features is the number of features.</source>
          <target state="translated">학습 데이터. 여기서 샘플 수의 n_samples 및 n_features는 피처 수입니다.</target>
        </trans-unit>
        <trans-unit id="1d999bb02f6364cf15c69e5533af993a3fc0fdd8" translate="yes" xml:space="preserve">
          <source>Training data, where n_samples is the number of samples and n_features is the number of features.</source>
          <target state="translated">교육 데이터. 여기서 n_samples는 샘플 수이고 n_features는 기능 수입니다.</target>
        </trans-unit>
        <trans-unit id="f12731d4ed32a02266da09997a2bf0e000555cf6" translate="yes" xml:space="preserve">
          <source>Training data, which is also required for prediction. If kernel == &amp;ldquo;precomputed&amp;rdquo; this is instead the precomputed training matrix, shape = [n_samples, n_samples].</source>
          <target state="translated">예측에 필요한 훈련 데이터. 커널 ==&amp;ldquo;사전 계산 된&amp;rdquo;경우 대신 사전 계산 된 훈련 행렬 모양 = [n_samples, n_samples]입니다.</target>
        </trans-unit>
        <trans-unit id="c5441fed149296831061b9151bd71d563327dc0d" translate="yes" xml:space="preserve">
          <source>Training data.</source>
          <target state="translated">훈련 데이터.</target>
        </trans-unit>
        <trans-unit id="4319dec91a5574f9382b1b679ba9c82bf44c0f15" translate="yes" xml:space="preserve">
          <source>Training data. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] if metric=&amp;rsquo;precomputed&amp;rsquo;.</source>
          <target state="translated">훈련 데이터. 배열 또는 행렬 인 경우 metric = 'precomputed'인 경우 [n_samples, n_features] 또는 [n_samples, n_samples]를 형성하십시오.</target>
        </trans-unit>
        <trans-unit id="3cc715a75ede17772899f7cc9ab69882475a79bc" translate="yes" xml:space="preserve">
          <source>Training data. If kernel == &amp;ldquo;precomputed&amp;rdquo; this is instead a precomputed kernel matrix, shape = [n_samples, n_samples].</source>
          <target state="translated">훈련 데이터. 커널 ==&amp;ldquo;사전 계산 된&amp;rdquo;경우 이는 대신 사전 계산 된 커널 행렬입니다. shape = [n_samples, n_samples].</target>
        </trans-unit>
        <trans-unit id="b12ede4c226e6e2f235813d30bce55744269c03f" translate="yes" xml:space="preserve">
          <source>Training data. Must fulfill input requirements of first step of the pipeline.</source>
          <target state="translated">훈련 데이터. 파이프 라인의 첫 번째 단계에 대한 입력 요구 사항을 충족해야합니다.</target>
        </trans-unit>
        <trans-unit id="d5044fd4a2ac02d5a0b137f2f3b7fd6b8f65a006" translate="yes" xml:space="preserve">
          <source>Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If &lt;code&gt;y&lt;/code&gt; is mono-output then &lt;code&gt;X&lt;/code&gt; can be sparse.</source>
          <target state="translated">훈련 데이터. 불필요한 메모리 중복을 피하기 위해 포트란 연속 데이터로 직접 전달하십시오. 경우 &lt;code&gt;y&lt;/code&gt; 단일 출력 후 &lt;code&gt;X&lt;/code&gt; 는 희소 수있다.</target>
        </trans-unit>
        <trans-unit id="8ed7855d8da328d2505a0bcd1c3302665b72cb3d" translate="yes" xml:space="preserve">
          <source>Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If y is mono-output, X can be sparse.</source>
          <target state="translated">훈련 데이터. 불필요한 메모리 중복을 피하기 위해 포트란 연속 데이터로 직접 전달하십시오. y가 단일 출력이면 X가 희박 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4c004afef287030c2dcb4a937f43adb06e1cdf0e" translate="yes" xml:space="preserve">
          <source>Training data. Shape [n_samples, n_features], or [n_samples, n_samples] if affinity==&amp;rsquo;precomputed&amp;rsquo;.</source>
          <target state="translated">훈련 데이터. affinity == 'precomputed'인 경우 [n_samples, n_features] 또는 [n_samples, n_samples]를 형성하십시오.</target>
        </trans-unit>
        <trans-unit id="be8959fb1d08ac2482d5adecb9cc6d42cd3487ff" translate="yes" xml:space="preserve">
          <source>Training instances to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous.</source>
          <target state="translated">클러스터링 할 교육 인스턴스. 데이터가 C 순서로 변환되므로 주어진 데이터가 C 연속적이지 않은 경우 메모리 복사가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="1fb973e1446d09c43085a14e14217bfa82f35fac" translate="yes" xml:space="preserve">
          <source>Training set and testing set</source>
          <target state="translated">훈련 세트 및 테스트 세트</target>
        </trans-unit>
        <trans-unit id="ea59a824d416e7ea0dc63df33b1afa59cdb64566" translate="yes" xml:space="preserve">
          <source>Training set.</source>
          <target state="translated">훈련 세트.</target>
        </trans-unit>
        <trans-unit id="3c518c488676e60e90ca53bcc0aa7271b669fe4d" translate="yes" xml:space="preserve">
          <source>Training set: only the shape is used to find optimal random matrix dimensions based on the theory referenced in the afore mentioned papers.</source>
          <target state="translated">훈련 세트 : 위에서 언급 한 논문에서 참조 된 이론을 바탕으로 최적의 랜덤 매트릭스 치수를 찾기 위해 모양 만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="68d52cda6c0756d21c2527d22eda07d7e45f55d9" translate="yes" xml:space="preserve">
          <source>Training target.</source>
          <target state="translated">훈련 목표.</target>
        </trans-unit>
        <trans-unit id="32e48bd3169f82f98b7879700514da5daba97549" translate="yes" xml:space="preserve">
          <source>Training targets. Must fulfill label requirements for all steps of the pipeline.</source>
          <target state="translated">훈련 목표. 파이프 라인의 모든 단계에 대한 레이블 요구 사항을 충족해야합니다.</target>
        </trans-unit>
        <trans-unit id="bc89d708a926da60c1e855065f294a150e4844da" translate="yes" xml:space="preserve">
          <source>Training vector, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples and &lt;code&gt;n_features&lt;/code&gt; is the total number of features.</source>
          <target state="translated">벡터, 훈련 &lt;code&gt;n_samples&lt;/code&gt; 샘플의 수입니다 &lt;code&gt;n_features&lt;/code&gt; 하는 기능의 총 수입니다.</target>
        </trans-unit>
        <trans-unit id="325dc392b957558d0accbc4c288eabf85d0d476c" translate="yes" xml:space="preserve">
          <source>Training vector, where n_samples in the number of samples and n_features is the number of features.</source>
          <target state="translated">학습 벡터. 여기서 샘플 수의 n_samples 및 n_features는 피처 수입니다.</target>
        </trans-unit>
        <trans-unit id="01000b19ae19a1d02ea4ceb374852ca509745c92" translate="yes" xml:space="preserve">
          <source>Training vector, where n_samples in the number of samples and n_features is the number of features. Note that centroid shrinking cannot be used with sparse matrices.</source>
          <target state="translated">학습 벡터. 여기서 샘플 수의 n_samples 및 n_features는 피처 수입니다. 중심 수축은 희소 행렬과 함께 사용할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="6a8354ff2f178d04d8fd18ac8502cba6a9d2e53e" translate="yes" xml:space="preserve">
          <source>Training vector, where n_samples is the number of samples and n_features is the number of features.</source>
          <target state="translated">n_samples는 샘플 수이고 n_features는 피처 수입니다.</target>
        </trans-unit>
        <trans-unit id="66e0bce9861c05444da85a9795d5edcc3de5cb5e" translate="yes" xml:space="preserve">
          <source>Training vectors, where n_samples is the number of samples and n_features is the number of features.</source>
          <target state="translated">n_samples는 샘플 수이고 n_features는 피처 수입니다.</target>
        </trans-unit>
        <trans-unit id="f64b8abd734d5648613b346b4bb3c97a56c66bbf" translate="yes" xml:space="preserve">
          <source>Training vectors, where n_samples is the number of samples and n_features is the number of features. For kernel=&amp;rdquo;precomputed&amp;rdquo;, the expected shape of X is (n_samples, n_samples).</source>
          <target state="translated">n_samples는 샘플 수이고 n_features는 피처 수입니다. kernel =&amp;rdquo;precomputed&amp;rdquo;의 경우 X의 예상 모양은 (n_samples, n_samples)입니다.</target>
        </trans-unit>
        <trans-unit id="ee5e82a19ba6d9a5b4fc8f431028f4e0ae5cae2a" translate="yes" xml:space="preserve">
          <source>Training vectors, where n_samples is the number of samples and n_features is the number of predictors.</source>
          <target state="translated">여기에서 n_samples는 샘플 수이고 n_features는 예측 변수 수입니다.</target>
        </trans-unit>
        <trans-unit id="3dbb8cbc3c8d093280069e8e889d1e0c62e1afde" translate="yes" xml:space="preserve">
          <source>Transform X back to its original space.</source>
          <target state="translated">X를 원래 공간으로 되돌립니다.</target>
        </trans-unit>
        <trans-unit id="dbfeebba6e53c937056143e8cf1258378ae1c26d" translate="yes" xml:space="preserve">
          <source>Transform X back to original space.</source>
          <target state="translated">X를 원래 공간으로 되돌립니다.</target>
        </trans-unit>
        <trans-unit id="2fcdcd20eec681f04cc400d1e7a3d3a35f46ced9" translate="yes" xml:space="preserve">
          <source>Transform X into subcluster centroids dimension.</source>
          <target state="translated">X를 하위 클러스터 중심 치수로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="da3379264043ea94358e5b4b01ce80967c41f1a5" translate="yes" xml:space="preserve">
          <source>Transform X separately by each transformer, concatenate results.</source>
          <target state="translated">트랜스포머별로 X를 개별적으로 변환하여 결과를 연결합니다.</target>
        </trans-unit>
        <trans-unit id="054e9dc484301382a53ef7807c44414f413c3b43" translate="yes" xml:space="preserve">
          <source>Transform X to a cluster-distance space.</source>
          <target state="translated">X를 군집 거리 공간으로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="9340d4e978871cfc2faf3772609beb4370b76837" translate="yes" xml:space="preserve">
          <source>Transform X to ordinal codes.</source>
          <target state="translated">X를 서수로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="d750cda6e828d45a370fec4538601ee99b5443be" translate="yes" xml:space="preserve">
          <source>Transform X using one-hot encoding.</source>
          <target state="translated">one-hot 인코딩을 사용하여 X를 변환하십시오.</target>
        </trans-unit>
        <trans-unit id="f9e88a65d85f54852f98655b3f250fdbf7750c92" translate="yes" xml:space="preserve">
          <source>Transform X using the forward function.</source>
          <target state="translated">앞으로 함수를 사용하여 X를 변형하십시오.</target>
        </trans-unit>
        <trans-unit id="fb06535ce9222390887b51d0862f28eec382f495" translate="yes" xml:space="preserve">
          <source>Transform X using the inverse function.</source>
          <target state="translated">역함수를 사용하여 X를 변형합니다.</target>
        </trans-unit>
        <trans-unit id="55b2dc92fd17631d37a113cafae1257246c63b9f" translate="yes" xml:space="preserve">
          <source>Transform X.</source>
          <target state="translated">X 변환</target>
        </trans-unit>
        <trans-unit id="df5b966033d10ab5ffd4498c25f3563581fac3a4" translate="yes" xml:space="preserve">
          <source>Transform a count matrix to a normalized tf or tf-idf representation</source>
          <target state="translated">카운트 행렬을 표준화 된 tf 또는 tf-idf 표현으로 변환</target>
        </trans-unit>
        <trans-unit id="c6579300b554475d257c93a2551d1e7ac8d00f29" translate="yes" xml:space="preserve">
          <source>Transform a count matrix to a tf or tf-idf representation</source>
          <target state="translated">카운트 행렬을 tf 또는 tf-idf 표현으로 변환</target>
        </trans-unit>
        <trans-unit id="cfe77beec60d283a1ae2557849fffc568b20c2b6" translate="yes" xml:space="preserve">
          <source>Transform a new matrix using the built clustering</source>
          <target state="translated">내장 된 클러스터링을 사용하여 새로운 매트릭스 변환</target>
        </trans-unit>
        <trans-unit id="eb758f2f9f4d3b4a21a0f5aa711d86b7f433cb44" translate="yes" xml:space="preserve">
          <source>Transform a sequence of documents to a document-term matrix.</source>
          <target state="translated">일련의 문서를 문서 용어 행렬로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="90d7961623626a54873e65ae75f5e5aedaf80a7d" translate="yes" xml:space="preserve">
          <source>Transform a sequence of instances to a scipy.sparse matrix.</source>
          <target state="translated">일련의 인스턴스를 scipy.sparse 행렬로 변환하십시오.</target>
        </trans-unit>
        <trans-unit id="482237f55f57c5ab1436ea9ad6e0ca3a5497f2c8" translate="yes" xml:space="preserve">
          <source>Transform a signal as a sparse combination of Ricker wavelets. This example visually compares different sparse coding methods using the &lt;a href=&quot;../../modules/generated/sklearn.decomposition.sparsecoder#sklearn.decomposition.SparseCoder&quot;&gt;&lt;code&gt;sklearn.decomposition.SparseCoder&lt;/code&gt;&lt;/a&gt; estimator. The Ricker (also known as Mexican hat or the second derivative of a Gaussian) is not a particularly good kernel to represent piecewise constant signals like this one. It can therefore be seen how much adding different widths of atoms matters and it therefore motivates learning the dictionary to best fit your type of signals.</source>
          <target state="translated">Ricker 웨이블릿의 스파 스 조합으로 신호를 변환합니다. 이 예제는 &lt;a href=&quot;../../modules/generated/sklearn.decomposition.sparsecoder#sklearn.decomposition.SparseCoder&quot;&gt; &lt;code&gt;sklearn.decomposition.SparseCoder&lt;/code&gt; &lt;/a&gt; 추정기를 사용하여 다른 희소 코딩 방법을 시각적으로 비교합니다 . Ricker (멕시코 모자 또는 가우시안의 2 차 파생물이라고도 함)는 이와 같은 조각 단위의 일정한 신호를 나타내는 데 특히 좋은 커널이 아닙니다. 따라서 서로 다른 폭의 원자를 추가하는 것이 얼마나 중요한지 알 수 있으므로 신호 유형에 가장 잘 맞는 사전을 배우도록 동기를 부여합니다.</target>
        </trans-unit>
        <trans-unit id="3c3158f9e95a76dac9ab046600d246dc683b1322" translate="yes" xml:space="preserve">
          <source>Transform array or sparse matrix X back to feature mappings.</source>
          <target state="translated">배열 또는 희소 행렬 X를 형상 매핑으로 다시 변환합니다.</target>
        </trans-unit>
        <trans-unit id="43aed443a30ff04a0a7d38cae0c2e3f2c765ad45" translate="yes" xml:space="preserve">
          <source>Transform between iterable of iterables and a multilabel format</source>
          <target state="translated">이터 러블의 이터 러블과 멀티 라벨 형식 간 변환</target>
        </trans-unit>
        <trans-unit id="8428b18b095eb02611727f6a1283e0146f4aea18" translate="yes" xml:space="preserve">
          <source>Transform binary labels back to multi-class labels</source>
          <target state="translated">이진 레이블을 다시 다중 클래스 레이블로 변환</target>
        </trans-unit>
        <trans-unit id="2d5fb2d774241a80b97c22822072a1cd5822cad7" translate="yes" xml:space="preserve">
          <source>Transform data X according to the fitted model.</source>
          <target state="translated">적합 모델에 따라 데이터 X를 변환합니다.</target>
        </trans-unit>
        <trans-unit id="e993947ab9336eb409d6a8eb55c55e2b5b858d46" translate="yes" xml:space="preserve">
          <source>Transform data back to its original space.</source>
          <target state="translated">데이터를 원래 공간으로 다시 변환하십시오.</target>
        </trans-unit>
        <trans-unit id="b922af176e5b4295d0d766cd496f7523d4754428" translate="yes" xml:space="preserve">
          <source>Transform data to polynomial features</source>
          <target state="translated">다항식 피처로 데이터 변환</target>
        </trans-unit>
        <trans-unit id="f1a4a6b05048c3643e26b0d505b52199b7895296" translate="yes" xml:space="preserve">
          <source>Transform dataset.</source>
          <target state="translated">데이터 세트를 변환합니다.</target>
        </trans-unit>
        <trans-unit id="0146265304f248a8c03f040ea5d584981839ec0b" translate="yes" xml:space="preserve">
          <source>Transform documents to document-term matrix.</source>
          <target state="translated">문서를 문서 용어 행렬로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="778e7579ae52504e167839efee081ba3167de93f" translate="yes" xml:space="preserve">
          <source>Transform feature-&amp;gt;value dicts to array or sparse matrix.</source>
          <target state="translated">피처-&amp;gt; 값을 배열 또는 희소 행렬로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="c8f4f5c3bee4bfd8c782321e0d4eb227c2d3191b" translate="yes" xml:space="preserve">
          <source>Transform features using quantiles information.</source>
          <target state="translated">Quantile 정보를 사용하여 기능을 변환하십시오.</target>
        </trans-unit>
        <trans-unit id="ace4ae2489dd9688eddb3a58e732664d39d28a92" translate="yes" xml:space="preserve">
          <source>Transform labels back to original encoding.</source>
          <target state="translated">레이블을 원래 인코딩으로 다시 변환하십시오.</target>
        </trans-unit>
        <trans-unit id="76c682df30bb4975f2641f2e89f16cc0b5f2d625" translate="yes" xml:space="preserve">
          <source>Transform labels to normalized encoding.</source>
          <target state="translated">레이블을 정규화 된 인코딩으로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="e6d8f7568400d53b2f444fa6cbf018c08b09552e" translate="yes" xml:space="preserve">
          <source>Transform multi-class labels to binary labels</source>
          <target state="translated">멀티 클래스 레이블을 이진 레이블로 변환</target>
        </trans-unit>
        <trans-unit id="ec1f3a72d306387b537de1b3b116fbdf51b17550" translate="yes" xml:space="preserve">
          <source>Transform new data by linear interpolation</source>
          <target state="translated">선형 보간으로 새로운 데이터 변환</target>
        </trans-unit>
        <trans-unit id="7e25dbc81754715628745ec728c6c249ac9d1737" translate="yes" xml:space="preserve">
          <source>Transform new points into embedding space.</source>
          <target state="translated">새로운 점을 임베드 공간으로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="17e15b65d999776fc7cb047cfc38d87f9b340eec" translate="yes" xml:space="preserve">
          <source>Transform the data X according to the fitted NMF model</source>
          <target state="translated">피팅 된 NMF 모델에 따라 데이터 X 변환</target>
        </trans-unit>
        <trans-unit id="28a4737ac1d13b4e451237dc699b89c49f7fb862" translate="yes" xml:space="preserve">
          <source>Transform the given indicator matrix into label sets</source>
          <target state="translated">주어진 지표 매트릭스를 레이블 세트로 변환</target>
        </trans-unit>
        <trans-unit id="38739bda11e07f48ac023acb8323ed328f115bd5" translate="yes" xml:space="preserve">
          <source>Transform the given label sets</source>
          <target state="translated">주어진 레이블 세트를 변환</target>
        </trans-unit>
        <trans-unit id="92a052e88a019f5aca9bb96a9137d202560617b4" translate="yes" xml:space="preserve">
          <source>Transform the sources back to the mixed data (apply mixing matrix).</source>
          <target state="translated">소스를 혼합 데이터로 다시 변환하십시오 (믹싱 매트릭스 적용).</target>
        </trans-unit>
        <trans-unit id="6414c408546f181e607c3ec28647dd72e64872ea" translate="yes" xml:space="preserve">
          <source>Transform your features into a higher dimensional, sparse space. Then train a linear model on these features.</source>
          <target state="translated">특징을 더 높은 차원의 희소 공간으로 변환하십시오. 그런 다음 이러한 기능에 대해 선형 모델을 학습하십시오.</target>
        </trans-unit>
        <trans-unit id="d3709f378c935401f6b259df9cce5a50135da098" translate="yes" xml:space="preserve">
          <source>Transformed array.</source>
          <target state="translated">변형 된 배열.</target>
        </trans-unit>
        <trans-unit id="4a8a97e010ec7ac27b50257ef7ee542c13ba8846" translate="yes" xml:space="preserve">
          <source>Transformed data</source>
          <target state="translated">변환 된 데이터</target>
        </trans-unit>
        <trans-unit id="d460e113769e190612a2b959c1c729d7e8676439" translate="yes" xml:space="preserve">
          <source>Transformed data in the binned space.</source>
          <target state="translated">비닝 된 공간에서 변환 된 데이터</target>
        </trans-unit>
        <trans-unit id="14642329121567cf9f5775d8a6512d3b978fccd2" translate="yes" xml:space="preserve">
          <source>Transformed data matrix</source>
          <target state="translated">변환 된 데이터 매트릭스</target>
        </trans-unit>
        <trans-unit id="0d3a338b719647431757293955a1513d13c572f4" translate="yes" xml:space="preserve">
          <source>Transformed data.</source>
          <target state="translated">변환 된 데이터.</target>
        </trans-unit>
        <trans-unit id="08b12f8aaa8632b66a6a22bc4de550a469c3cc9c" translate="yes" xml:space="preserve">
          <source>Transformed dataset.</source>
          <target state="translated">변환 된 데이터 세트.</target>
        </trans-unit>
        <trans-unit id="eeb85e59603c1cea29acb31c92a29204737376ea" translate="yes" xml:space="preserve">
          <source>Transformed input.</source>
          <target state="translated">변환 된 입력.</target>
        </trans-unit>
        <trans-unit id="0a6145f06a4913811002ff339bc5284d2892e790" translate="yes" xml:space="preserve">
          <source>Transformed samples</source>
          <target state="translated">변형 된 샘플</target>
        </trans-unit>
        <trans-unit id="6d517e36599e67ec967c7be2afac6d7777579d1a" translate="yes" xml:space="preserve">
          <source>Transformer used in &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 하고 &lt;code&gt;predict&lt;/code&gt; 되는 변압기 .</target>
        </trans-unit>
        <trans-unit id="43471a7ace97310a9577002aa9803e00d83e6192" translate="yes" xml:space="preserve">
          <source>Transformers are usually combined with classifiers, regressors or other estimators to build a composite estimator. The most common tool is a &lt;a href=&quot;#pipeline&quot;&gt;Pipeline&lt;/a&gt;. Pipeline is often used in combination with &lt;a href=&quot;#feature-union&quot;&gt;FeatureUnion&lt;/a&gt; which concatenates the output of transformers into a composite feature space. &lt;a href=&quot;#transformed-target-regressor&quot;&gt;TransformedTargetRegressor&lt;/a&gt; deals with transforming the &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-target&quot;&gt;target&lt;/a&gt; (i.e. log-transform &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-171&quot;&gt;y&lt;/a&gt;). In contrast, Pipelines only transform the observed data (&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-x&quot;&gt;X&lt;/a&gt;).</source>
          <target state="translated">변압기는 일반적으로 분류기, 회귀 기 또는 기타 추정기와 결합하여 합성 추정기를 구축합니다. 가장 일반적인 도구는 &lt;a href=&quot;#pipeline&quot;&gt;파이프 라인&lt;/a&gt; 입니다. 파이프 라인은 종종 변압기의 출력을 복합 피처 공간으로 연결하는 &lt;a href=&quot;#feature-union&quot;&gt;FeatureUnion&lt;/a&gt; 과 함께 사용됩니다 . &lt;a href=&quot;#transformed-target-regressor&quot;&gt;TransformedTargetRegressor&lt;/a&gt; 는 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-target&quot;&gt;대상&lt;/a&gt; 변환 (즉, 로그 변환 &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-171&quot;&gt;y&lt;/a&gt; )을 처리합니다. 반대로 파이프 라인은 관측 된 데이터 ( &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-x&quot;&gt;X&lt;/a&gt; ) 만 변환합니다 .</target>
        </trans-unit>
        <trans-unit id="b69fe15775501662a569d2ed632ddbe970e558ef" translate="yes" xml:space="preserve">
          <source>Transformers for missing value imputation</source>
          <target state="translated">결 측값 대치 변압기</target>
        </trans-unit>
        <trans-unit id="4804df5ca1652cab2567ab10e41eae2d30b7e99a" translate="yes" xml:space="preserve">
          <source>Transforming Classifier Scores into Accurate Multiclass Probability Estimates, B. Zadrozny &amp;amp; C. Elkan, (KDD 2002)</source>
          <target state="translated">분류기 점수를 정확한 멀티 클래스 확률 추정치로 변환, B. Zadrozny &amp;amp; C. Elkan, (KDD 2002)</target>
        </trans-unit>
        <trans-unit id="74f517360774a680819178109aa2b52b87d4fd99" translate="yes" xml:space="preserve">
          <source>Transforming distance to well-behaved similarities</source>
          <target state="translated">거리를 올바르게 동작하는 유사성으로 변환</target>
        </trans-unit>
        <trans-unit id="dfe17b4b683a10ef2eafef30897d9c629bf96dd6" translate="yes" xml:space="preserve">
          <source>Transforms discretized data back to original feature space.</source>
          <target state="translated">이산화 된 데이터를 원래 피쳐 공간으로 다시 변환합니다.</target>
        </trans-unit>
        <trans-unit id="5bd7a9a7032f01002afe1d21dc1635e87bd5dbc6" translate="yes" xml:space="preserve">
          <source>Transforms features by scaling each feature to a given range.</source>
          <target state="translated">각 기능을 지정된 범위로 조정하여 기능을 변환합니다.</target>
        </trans-unit>
        <trans-unit id="45675a7235910659531092f94ca2cac1226cb6a9" translate="yes" xml:space="preserve">
          <source>Transforms lists of feature-value mappings to vectors.</source>
          <target state="translated">피처-값 매핑 목록을 벡터로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="ff596a653686d4986dda1851c66682d846f4bf4d" translate="yes" xml:space="preserve">
          <source>Transforms the image samples in X into a matrix of patch data.</source>
          <target state="translated">X의 이미지 샘플을 패치 데이터 매트릭스로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="d6a25d0e3691aa7e7e60fcc1d61ee6046c7d09b3" translate="yes" xml:space="preserve">
          <source>Tree-based estimators (see the &lt;a href=&quot;classes#module-sklearn.tree&quot;&gt;&lt;code&gt;sklearn.tree&lt;/code&gt;&lt;/a&gt; module and forest of trees in the &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module) can be used to compute feature importances, which in turn can be used to discard irrelevant features (when coupled with the &lt;a href=&quot;generated/sklearn.feature_selection.selectfrommodel#sklearn.feature_selection.SelectFromModel&quot;&gt;&lt;code&gt;sklearn.feature_selection.SelectFromModel&lt;/code&gt;&lt;/a&gt; meta-transformer):</source>
          <target state="translated">트리 기반 추정량합니다 (참조 &lt;a href=&quot;classes#module-sklearn.tree&quot;&gt; &lt;code&gt;sklearn.tree&lt;/code&gt; 의&lt;/a&gt; 에서 나무의 모듈과 숲 &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; 와 결합 할 때 다시 (관련이없는 기능을 삭제하는 데 사용할 수있는 컴퓨팅 기능 importances에 사용할 수있는 모듈) &lt;a href=&quot;generated/sklearn.feature_selection.selectfrommodel#sklearn.feature_selection.SelectFromModel&quot;&gt; &lt;code&gt;sklearn.feature_selection.SelectFromModel&lt;/code&gt; &lt;/a&gt; 메타 트랜스포머) :</target>
        </trans-unit>
        <trans-unit id="81d8ac0c0739336d0bbd6f8053b2fde8039cdb1c" translate="yes" xml:space="preserve">
          <source>Triangle Inequality: d(x, y) + d(y, z) &amp;gt;= d(x, z)</source>
          <target state="translated">삼각형 부등식 : d (x, y) + d (y, z)&amp;gt; = d (x, z)</target>
        </trans-unit>
        <trans-unit id="331d2c199452ae22aa8941c5bcbe6a7fe41c68b5" translate="yes" xml:space="preserve">
          <source>Tristan Fletcher: &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.651.8603&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Relevance Vector Machines explained&lt;/a&gt;</source>
          <target state="translated">Tristan Fletcher : &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.651.8603&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;관련성 벡터 머신 설명&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3b19d80cd81c13647ace615b9d73da08b4d8c61b" translate="yes" xml:space="preserve">
          <source>True : always precompute distances</source>
          <target state="translated">True : 항상 사전 거리 계산</target>
        </trans-unit>
        <trans-unit id="ebac1d7d68472a848a069828ba36b6b2dd6227bb" translate="yes" xml:space="preserve">
          <source>True binary labels in binary indicator format.</source>
          <target state="translated">이진 표시기 형식의 실제 이진 레이블.</target>
        </trans-unit>
        <trans-unit id="94ad072572f1b0d8a6896ff3fd5d5269c3006cce" translate="yes" xml:space="preserve">
          <source>True binary labels or binary label indicators.</source>
          <target state="translated">이진 레이블 또는 이진 레이블 표시기</target>
        </trans-unit>
        <trans-unit id="173029937373f6d16ed7438491b1a8131b2bc4cb" translate="yes" xml:space="preserve">
          <source>True binary labels. If labels are not either {-1, 1} or {0, 1}, then pos_label should be explicitly given.</source>
          <target state="translated">진정한 이진 레이블. 레이블이 {-1, 1} 또는 {0, 1}이 아닌 경우 pos_label을 명시 적으로 제공해야합니다.</target>
        </trans-unit>
        <trans-unit id="cba1cd7ae39f91f6d0e2908d3956200bdf94de07" translate="yes" xml:space="preserve">
          <source>True if estimator is a classifier and False otherwise.</source>
          <target state="translated">추정기가 분류 자이면 true이고, 그렇지 않으면 False입니다.</target>
        </trans-unit>
        <trans-unit id="7e34070b9f977933411df9b814860396cade02bb" translate="yes" xml:space="preserve">
          <source>True if estimator is a regressor and False otherwise.</source>
          <target state="translated">추정기가 회귀 변수이면 true이고, 그렇지 않으면 False입니다.</target>
        </trans-unit>
        <trans-unit id="d039a95b006860b5b92d23f84015c0458d6fdb1a" translate="yes" xml:space="preserve">
          <source>True if the array returned from predict is to be in sparse CSC format. Is automatically set to True if the input y is passed in sparse format.</source>
          <target state="translated">predict에서 반환 된 배열이 희소 한 CSC 형식이면 true입니다. 입력 y가 희소 형식으로 전달되면 자동으로 True로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="018f28ffd2c7241c63be51b46bba3e8aa528d907" translate="yes" xml:space="preserve">
          <source>True if the input data to transform is given as a sparse matrix, False otherwise.</source>
          <target state="translated">변환 할 입력 데이터가 희소 행렬로 제공되면 true이고, 그렇지 않으면 False입니다.</target>
        </trans-unit>
        <trans-unit id="06236e43536e8bd62b7d950e36ddd9dca022a999" translate="yes" xml:space="preserve">
          <source>True if the output at fit is 2d, else false.</source>
          <target state="translated">맞는 출력이 2d이면 true이고, 그렇지 않으면 false입니다.</target>
        </trans-unit>
        <trans-unit id="abda54d00232aa3c71419926e38966e372a6e200" translate="yes" xml:space="preserve">
          <source>True if the returned array from transform is desired to be in sparse CSR format.</source>
          <target state="translated">변환에서 반환 된 배열이 희소 한 CSR 형식이되도록하려면 True입니다.</target>
        </trans-unit>
        <trans-unit id="b3f5d1c4b9aeea8d97315ada02d3f0f3b6e0dbc5" translate="yes" xml:space="preserve">
          <source>True labels for X.</source>
          <target state="translated">X에 대한 실제 레이블.</target>
        </trans-unit>
        <trans-unit id="24a7816a0ae25d0715f83e367246b56738200fc8" translate="yes" xml:space="preserve">
          <source>True mutual information can&amp;rsquo;t be negative. If its estimate turns out to be negative, it is replaced by zero.</source>
          <target state="translated">진정한 상호 정보는 부정적 일 수 없습니다. 추정치가 음수로 판명되면 0으로 대체됩니다.</target>
        </trans-unit>
        <trans-unit id="e857c90bead41164c28f265fe201e3c7a69f5d75" translate="yes" xml:space="preserve">
          <source>True target, consisting of integers of two values. The positive label must be greater than the negative label.</source>
          <target state="translated">두 값의 정수로 구성된 진정한 대상. 양수 레이블은 음수 레이블보다 커야합니다.</target>
        </trans-unit>
        <trans-unit id="5f6f5563a268706baa91536cfcd1565c453cd8e7" translate="yes" xml:space="preserve">
          <source>True targets of binary classification in range {-1, 1} or {0, 1}.</source>
          <target state="translated">{-1, 1} 또는 {0, 1} 범위의 이진 분류 대상입니다.</target>
        </trans-unit>
        <trans-unit id="6e2bbfc40bf0e63b31fb5b0351be961b49cc74be" translate="yes" xml:space="preserve">
          <source>True targets.</source>
          <target state="translated">진정한 목표.</target>
        </trans-unit>
        <trans-unit id="18dd5ee40d70767a2f6629e8ff8969a87290115e" translate="yes" xml:space="preserve">
          <source>True values for X</source>
          <target state="translated">X의 참값</target>
        </trans-unit>
        <trans-unit id="81e3774c236b4c61a22388a1a822b3e69fb3e5a6" translate="yes" xml:space="preserve">
          <source>True values for X.</source>
          <target state="translated">X에 대한 참값.</target>
        </trans-unit>
        <trans-unit id="0d7ce48badf2f0a91a36bec7511768633417749f" translate="yes" xml:space="preserve">
          <source>True when convergence was reached in fit(), False otherwise.</source>
          <target state="translated">fit ()에서 수렴에 도달하면 true이고, 그렇지 않으면 False입니다.</target>
        </trans-unit>
        <trans-unit id="d333cd18e174fe06286d776d55b1b9eaf760ce1b" translate="yes" xml:space="preserve">
          <source>True: Force all values of X to be finite.</source>
          <target state="translated">True : X의 모든 값을 유한하게 만듭니다.</target>
        </trans-unit>
        <trans-unit id="260b9ffda14105c1fd2ffa31d36eae7c83268ddd" translate="yes" xml:space="preserve">
          <source>True: the results is casted to an unsigned int</source>
          <target state="translated">True : 결과는 unsigned int로 캐스트됩니다.</target>
        </trans-unit>
        <trans-unit id="00b6f6ebc7b7070cf35772b16b427811573346a1" translate="yes" xml:space="preserve">
          <source>Try classifying classes 1 and 2 from the iris dataset with SVMs, with the 2 first features. Leave out 10% of each class and test prediction performance on these observations.</source>
          <target state="translated">SVM을 사용하여 홍채 데이터 세트에서 클래스 1과 2를 분류 해보십시오. 각 클래스의 10 %를 제외하고 이러한 관측치에 대한 예측 성능을 테스트하십시오.</target>
        </trans-unit>
        <trans-unit id="5449ae93c54cf3f8e79ab0ee95bb4ee118bd4f84" translate="yes" xml:space="preserve">
          <source>Try classifying the digits dataset with nearest neighbors and a linear model. Leave out the last 10% and test prediction performance on these observations.</source>
          <target state="translated">가장 가까운 이웃과 선형 모델로 숫자 데이터 세트를 분류하십시오. 이 관측치에서 마지막 10 %를 제외하고 예측 성능을 테스트하십시오.</target>
        </trans-unit>
        <trans-unit id="7a783eca4388b1c7b8c830f476caa080328ba3c3" translate="yes" xml:space="preserve">
          <source>Try playing around with the &lt;code&gt;analyzer&lt;/code&gt; and &lt;code&gt;token normalisation&lt;/code&gt; under &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; 에서 &lt;code&gt;analyzer&lt;/code&gt; 및 &lt;code&gt;token normalisation&lt;/code&gt; 로 놀아보십시오 .</target>
        </trans-unit>
        <trans-unit id="b7e468fa3f6cfdfb33fa6bf28dcdf3165bf89507" translate="yes" xml:space="preserve">
          <source>Try to differentiate the two first classes of the iris data</source>
          <target state="translated">홍채 데이터의 첫 번째 두 가지 등급을 차별화</target>
        </trans-unit>
        <trans-unit id="1bc09af6523c25787ea3631aac8c3f52f8bc29dc" translate="yes" xml:space="preserve">
          <source>Try using &lt;a href=&quot;../../modules/decomposition#lsa&quot;&gt;Truncated SVD&lt;/a&gt; for &lt;a href=&quot;https://en.wikipedia.org/wiki/Latent_semantic_analysis&quot;&gt;latent semantic analysis&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://en.wikipedia.org/wiki/Latent_semantic_analysis&quot;&gt;잠재 의미 체계 분석에 &lt;/a&gt;&lt;a href=&quot;../../modules/decomposition#lsa&quot;&gt;잘린 SVD&lt;/a&gt; 를 사용해보십시오 .</target>
        </trans-unit>
        <trans-unit id="126c548fa4d4a4c65c7759b8f0eb82ce5677dab5" translate="yes" xml:space="preserve">
          <source>Tsoumakas, G., Katakis, I., &amp;amp; Vlahavas, I. (2010). Mining multi-label data. In Data mining and knowledge discovery handbook (pp. 667-685). Springer US.</source>
          <target state="translated">Tsoumakas, G., Katakis, I. &amp;amp; Vlahavas, I. (2010). 다중 레이블 데이터 마이닝. 데이터 마이닝 및 지식 검색 핸드북 (pp. 667-685). 스프링거 미국.</target>
        </trans-unit>
        <trans-unit id="0d016a3ee3141a6ebd01b9c31169fb6ec8d37fd6" translate="yes" xml:space="preserve">
          <source>Tuning the hyper-parameters of an estimator</source>
          <target state="translated">추정기의 하이퍼 파라미터 튜닝</target>
        </trans-unit>
        <trans-unit id="2e926727653886b165b872a4b6b2a62bf90f0bd1" translate="yes" xml:space="preserve">
          <source>Tuple of row and column indicators for a set of biclusters.</source>
          <target state="translated">biclusters 세트에 대한 행 및 열 표시기의 튜플.</target>
        </trans-unit>
        <trans-unit id="c054700312acf34cbacbc98d115120c76da3a6d5" translate="yes" xml:space="preserve">
          <source>Turn seed into a np.random.RandomState instance</source>
          <target state="translated">시드를 np.random.RandomState 인스턴스로 전환</target>
        </trans-unit>
        <trans-unit id="007a671747688cedb01751baca6545483f05de7a" translate="yes" xml:space="preserve">
          <source>Tutorial setup</source>
          <target state="translated">튜토리얼 설정</target>
        </trans-unit>
        <trans-unit id="025f75efad84ed2b985f2818a53e81aa77abca7c" translate="yes" xml:space="preserve">
          <source>Tutorial: A tutorial on statistical-learning for scientific data processing</source>
          <target state="translated">학습서 : 과학 데이터 처리를위한 통계 학습에 대한 학습서</target>
        </trans-unit>
        <trans-unit id="e7df3b5ebbaf2fd3b4b4579edbd7ce42f46db699" translate="yes" xml:space="preserve">
          <source>Tutorial: An introduction to machine learning with scikit-learn</source>
          <target state="translated">튜토리얼 : scikit-learn을 이용한 머신 러닝 소개</target>
        </trans-unit>
        <trans-unit id="8682fb6c27e32858369b74e47f829fad6c8d3a68" translate="yes" xml:space="preserve">
          <source>Tutorial: Choosing the right estimator</source>
          <target state="translated">학습서 : 올바른 추정기 선택</target>
        </trans-unit>
        <trans-unit id="2865c0d93493065de0c34da92792e35a845226d3" translate="yes" xml:space="preserve">
          <source>Tutorial: Model selection</source>
          <target state="translated">튜토리얼 : 모델 선택</target>
        </trans-unit>
        <trans-unit id="b7709b919b68974b71489ceb95a00ef31c4eda72" translate="yes" xml:space="preserve">
          <source>Tutorial: Putting it all together</source>
          <target state="translated">튜토리얼 : 모두 모아보기</target>
        </trans-unit>
        <trans-unit id="b0b3bc4bbf4e62230750bf24baeb122d7a994298" translate="yes" xml:space="preserve">
          <source>Tutorial: Statistical learning</source>
          <target state="translated">튜토리얼 : 통계 학습</target>
        </trans-unit>
        <trans-unit id="a149365421f01d98250256737c350c42a4cd4b82" translate="yes" xml:space="preserve">
          <source>Tutorial: Supervised learning</source>
          <target state="translated">튜토리얼 :지도 학습</target>
        </trans-unit>
        <trans-unit id="4353f067a68843e09ae0691ab9f9c44ef2e6db23" translate="yes" xml:space="preserve">
          <source>Tutorial: Unsupervised learning</source>
          <target state="translated">튜토리얼 : 비지도 학습</target>
        </trans-unit>
        <trans-unit id="206fac7baeed5ee14f8990630b6607a7c33e8644" translate="yes" xml:space="preserve">
          <source>Tutorial: Working With Text Data</source>
          <target state="translated">학습서 : 텍스트 데이터 작업</target>
        </trans-unit>
        <trans-unit id="b919de3c63710fd07133db7062fb5a1fbffa0bfe" translate="yes" xml:space="preserve">
          <source>Tutorial: scikit-learn Tutorials</source>
          <target state="translated">튜토리얼 : scikit-learn 튜토리얼</target>
        </trans-unit>
        <trans-unit id="654171647baa6be8557a5d627cf35c7075ebb257" translate="yes" xml:space="preserve">
          <source>Tutorials</source>
          <target state="translated">Tutorials</target>
        </trans-unit>
        <trans-unit id="995550b74403db560a3a2ea8d3906cd56b901336" translate="yes" xml:space="preserve">
          <source>Two algorithms are demoed: ordinary k-means and its more scalable cousin minibatch k-means.</source>
          <target state="translated">일반 k- 평균과보다 확장 가능한 사촌 미니 배치 k- 평균이라는 두 가지 알고리즘이 시연됩니다.</target>
        </trans-unit>
        <trans-unit id="3b934d458351995534fed7d234de7b14c38f4cd4" translate="yes" xml:space="preserve">
          <source>Two approaches for performing calibration of probabilistic predictions are provided: a parametric approach based on Platt&amp;rsquo;s sigmoid model and a non-parametric approach based on isotonic regression (&lt;a href=&quot;classes#module-sklearn.isotonic&quot;&gt;&lt;code&gt;sklearn.isotonic&lt;/code&gt;&lt;/a&gt;). Probability calibration should be done on new data not used for model fitting. The class &lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt;&lt;code&gt;CalibratedClassifierCV&lt;/code&gt;&lt;/a&gt; uses a cross-validation generator and estimates for each split the model parameter on the train samples and the calibration of the test samples. The probabilities predicted for the folds are then averaged. Already fitted classifiers can be calibrated by &lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt;&lt;code&gt;CalibratedClassifierCV&lt;/code&gt;&lt;/a&gt; via the parameter cv=&amp;rdquo;prefit&amp;rdquo;. In this case, the user has to take care manually that data for model fitting and calibration are disjoint.</source>
          <target state="translated">확률 예측의 교정을 수행하기위한 두 가지 접근법, 즉 Platt의 시그 모이 드 모델을 기반으로하는 파라 메트릭 접근법과 등장 성 회귀를 기반으로하는 비모수 적 접근법 ( &lt;a href=&quot;classes#module-sklearn.isotonic&quot;&gt; &lt;code&gt;sklearn.isotonic&lt;/code&gt; &lt;/a&gt; )이 제공됩니다. 모델 피팅에 사용되지 않은 새 데이터에 대해서는 확률 교정을 수행해야합니다. &lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt; &lt;code&gt;CalibratedClassifierCV&lt;/code&gt; &lt;/a&gt; 클래스 는 교차 검증 생성기를 사용하고 각 분할에 대해 트레인 샘플의 모델 매개 변수와 테스트 샘플의 교정을 추정합니다. 그런 다음 접기에 대해 예측 된 확률을 평균합니다. 이미 장착 된 분류기는 다음을 통해 교정 할 수 있습니다.&lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt; &lt;code&gt;CalibratedClassifierCV&lt;/code&gt; &lt;/a&gt;cv =&amp;rdquo;prefit&amp;rdquo;매개 변수를 통해 CalibratedClassifierCV. 이 경우 사용자는 모델 피팅 및 캘리브레이션 데이터가 분리되도록 수동으로 관리해야합니다.</target>
        </trans-unit>
        <trans-unit id="de7e8d6ad699213a292d0c528c5ddad33bca14ae" translate="yes" xml:space="preserve">
          <source>Two consequences of imposing a connectivity can be seen. First clustering with a connectivity matrix is much faster.</source>
          <target state="translated">연결성을 부과하면 두 가지 결과를 볼 수 있습니다. 연결 매트릭스를 사용한 첫 번째 클러스터링이 훨씬 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="73ece4ca1e1779fbf5110031e848f2313deac958" translate="yes" xml:space="preserve">
          <source>Two cross-validation loops are performed in parallel: one by the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; estimator to set &lt;code&gt;gamma&lt;/code&gt; and the other one by &lt;code&gt;cross_val_score&lt;/code&gt; to measure the prediction performance of the estimator. The resulting scores are unbiased estimates of the prediction score on new data.</source>
          <target state="translated">두 개의 교차 검증 루프가 병렬로 수행됩니다. 하나는 &lt;code&gt;gamma&lt;/code&gt; 를 설정하기 위해 &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt; 추정기에 의해, 다른 하나 는 추정기의 예측 성능을 측정하기 위해 &lt;code&gt;cross_val_score&lt;/code&gt; 에 의해 다른 것 입니다. 결과 점수는 새 데이터에 대한 예측 점수의 편향되지 않은 추정치입니다.</target>
        </trans-unit>
        <trans-unit id="d467efdd44bf60415fc895b8589d9d81d46d02ec" translate="yes" xml:space="preserve">
          <source>Two families of ensemble methods are usually distinguished:</source>
          <target state="translated">앙상블 방법의 두 제품군은 일반적으로 구별됩니다.</target>
        </trans-unit>
        <trans-unit id="e8ac95de27d48015555c5b981483208d51b8f268" translate="yes" xml:space="preserve">
          <source>Two feature extraction methods can be used in this example:</source>
          <target state="translated">이 예에서는 두 가지 특징 추출 방법을 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ebb2ce8305b879c94bfe5ff4f307e7a35d679e99" translate="yes" xml:space="preserve">
          <source>Two plots will be shown for each scaler/normalizer/transformer. The left figure will show a scatter plot of the full data set while the right figure will exclude the extreme values considering only 99 % of the data set, excluding marginal outliers. In addition, the marginal distributions for each feature will be shown on the side of the scatter plot.</source>
          <target state="translated">각 스케일러 / 노멀 라이저 / 트랜스포머에 대해 두 개의 플롯이 표시됩니다. 왼쪽 그림은 전체 데이터 세트의 산점도를 표시하고 오른쪽 그림은 한계 이상 값을 제외하고 데이터 세트의 99 % 만 고려하여 극단 값을 제외합니다. 또한 각 피처에 대한 한계 분포는 산점도 측면에 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="348f286c4d7d6b276984cd38d102dc527023a237" translate="yes" xml:space="preserve">
          <source>Two separate datasets are used for the two different plots. The reason behind this is the &lt;code&gt;l1&lt;/code&gt; case works better on sparse data, while &lt;code&gt;l2&lt;/code&gt; is better suited to the non-sparse case.</source>
          <target state="translated">두 개의 서로 다른 플롯에 대해 두 개의 개별 데이터 세트가 사용됩니다. 그 이유는 &lt;code&gt;l1&lt;/code&gt; 사례가 희소 데이터에서 더 잘 작동하고 &lt;code&gt;l2&lt;/code&gt; 가 희소가 아닌 경우에 더 적합하기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="32895c2e5eacd1283051e2d5a4f1fd3f826fb6ed" translate="yes" xml:space="preserve">
          <source>Two-class AdaBoost</source>
          <target state="translated">2 급 AdaBoost</target>
        </trans-unit>
        <trans-unit id="b8fde32df7d701e50fc79883cdf21005c9469e51" translate="yes" xml:space="preserve">
          <source>Type casting</source>
          <target state="translated">타입 캐스팅</target>
        </trans-unit>
        <trans-unit id="7b90464c9a3a0593a486a1facdfd06e25cac2162" translate="yes" xml:space="preserve">
          <source>Type of SVM: C SVC, nu SVC, one class, epsilon SVR, nu SVR</source>
          <target state="translated">SVM 유형 : C SVC, nu SVC, 1 클래스, 엡실론 SVR, nu SVR</target>
        </trans-unit>
        <trans-unit id="fb24034e0a15fdb11753c4c561fec377a847eca1" translate="yes" xml:space="preserve">
          <source>Type of SVM: C_SVC, NuSVC, OneClassSVM, EpsilonSVR or NuSVR respectively. 0 by default.</source>
          <target state="translated">SVM 유형 : 각각 C_SVC, NuSVC, OneClassSVM, EpsilonSVR 또는 NuSVR. 기본적으로 0입니다.</target>
        </trans-unit>
        <trans-unit id="05734831eef4f60aabd73eed1535149e1780b49e" translate="yes" xml:space="preserve">
          <source>Type of kernel.</source>
          <target state="translated">커널 유형.</target>
        </trans-unit>
        <trans-unit id="7784bde958a1d323776ea14d0478698cc397c040" translate="yes" xml:space="preserve">
          <source>Type of returned matrix: &amp;lsquo;connectivity&amp;rsquo; will return the connectivity matrix with ones and zeros, and &amp;lsquo;distance&amp;rsquo; will return the distances between neighbors according to the given metric.</source>
          <target state="translated">반환 된 매트릭스의 유형 : 'connectivity'는 1과 0으로 연결성 매트릭스를 반환하고 'distance'는 주어진 메트릭에 따라 이웃 사이의 거리를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="029a83801426f186d4049ef92d5f3d3590b1d125" translate="yes" xml:space="preserve">
          <source>Type of returned matrix: &amp;lsquo;connectivity&amp;rsquo; will return the connectivity matrix with ones and zeros, in &amp;lsquo;distance&amp;rsquo; the edges are Euclidean distance between points.</source>
          <target state="translated">반환 된 매트릭스의 유형 : '연결성'은 1과 0으로 연결성 매트릭스를 반환합니다. '거리'에서 가장자리는 점 사이의 유클리드 거리입니다.</target>
        </trans-unit>
        <trans-unit id="e2af4c36790c9137ba49cdb815accc60f6f75311" translate="yes" xml:space="preserve">
          <source>Type of store backend for reading/writing cache files. Default: &amp;lsquo;local&amp;rsquo;. The &amp;lsquo;local&amp;rsquo; backend is using regular filesystem operations to manipulate data (open, mv, etc) in the backend.</source>
          <target state="translated">캐시 파일 읽기 / 쓰기를위한 저장소 백엔드 유형 기본값은 'local'입니다. '로컬'백엔드는 일반 파일 시스템 작업을 사용하여 백엔드의 데이터 (open, mv 등)를 조작합니다.</target>
        </trans-unit>
        <trans-unit id="858cba7a97e85950fd69a9661ce88f3dff1bf729" translate="yes" xml:space="preserve">
          <source>Type of the matrix returned by fit_transform() or transform().</source>
          <target state="translated">fit_transform () 또는 transform ()에 의해 반환되는 행렬의 유형입니다.</target>
        </trans-unit>
        <trans-unit id="b6e792a3d08a7bd144dac10e42edb461fd3dd2e3" translate="yes" xml:space="preserve">
          <source>Type to use in computing the mean. For integer inputs, the default is &lt;code&gt;float64&lt;/code&gt;; for floating point inputs, it is the same as the input dtype.</source>
          <target state="translated">평균 계산에 사용할 유형입니다. 정수 입력의 경우 기본값은 &lt;code&gt;float64&lt;/code&gt; 입니다 . 부동 소수점 입력의 경우 입력 dtype과 동일합니다.</target>
        </trans-unit>
        <trans-unit id="f77afa338a167babd59a76b7f498d6550ba586ff" translate="yes" xml:space="preserve">
          <source>Under the assumption that the data are Gaussian distributed, Chen et al. &lt;a href=&quot;#id6&quot; id=&quot;id5&quot;&gt;[2]&lt;/a&gt; derived a formula aimed at choosing a shrinkage coefficient that yields a smaller Mean Squared Error than the one given by Ledoit and Wolf&amp;rsquo;s formula. The resulting estimator is known as the Oracle Shrinkage Approximating estimator of the covariance.</source>
          <target state="translated">데이터가 가우스 분포라는 가정하에 Chen et al. &lt;a href=&quot;#id6&quot; id=&quot;id5&quot;&gt;[2]&lt;/a&gt; 는 Ledoit와 Wolf의 공식보다 작은 평균 제곱 오차를 산출하는 수축 계수를 선택하기위한 공식을 도출했다. 결과 추정량을 공분산의 Oracle Shrinkage Approximating Estimator라고합니다.</target>
        </trans-unit>
        <trans-unit id="b57ce0246b95ca0ff3d95c499722ea513c24180d" translate="yes" xml:space="preserve">
          <source>Underfitting vs. Overfitting</source>
          <target state="translated">과적 합과 과적 합</target>
        </trans-unit>
        <trans-unit id="10dab5fb240281c20bb10ad043cbba82ec3b0bd6" translate="yes" xml:space="preserve">
          <source>Understanding the decision tree structure</source>
          <target state="translated">의사 결정 트리 구조 이해</target>
        </trans-unit>
        <trans-unit id="a381b476a1bdd0042346ffd8d02655a7131aa344" translate="yes" xml:space="preserve">
          <source>Undo the scaling of X according to feature_range.</source>
          <target state="translated">feature_range에 따라 X의 스케일링을 취소하십시오.</target>
        </trans-unit>
        <trans-unit id="11b4a2e4a2b6531b9e75ad23f03f25fd4f8ecde0" translate="yes" xml:space="preserve">
          <source>Uniform weights are used by default.</source>
          <target state="translated">기본적으로 균일 한 가중치가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="9421754583ed6d327fe582bef2d0e2d0f17ba0c4" translate="yes" xml:space="preserve">
          <source>Unique class labels.</source>
          <target state="translated">고유 한 클래스 레이블.</target>
        </trans-unit>
        <trans-unit id="6efd4cf40567c19c24a13e3421a6d1109a47da44" translate="yes" xml:space="preserve">
          <source>Uniquely holds the label for each class.</source>
          <target state="translated">각 클래스의 레이블을 고유하게 보유합니다.</target>
        </trans-unit>
        <trans-unit id="15f758514c6db2ef9033ee5636e4d09d40ce9747" translate="yes" xml:space="preserve">
          <source>Univariate Feature Selection</source>
          <target state="translated">일 변량 피처 선택</target>
        </trans-unit>
        <trans-unit id="820dda4dd874419c514343cc2737763cc18b33d1" translate="yes" xml:space="preserve">
          <source>Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the &lt;code&gt;transform&lt;/code&gt; method:</source>
          <target state="translated">일 변량 기능 선택은 일 변량 통계 테스트를 기반으로 최상의 기능을 선택하여 작동합니다. 추정기의 전처리 단계로 볼 수 있습니다. Scikit-learn은 기능 선택 루틴을 &lt;code&gt;transform&lt;/code&gt; 메소드 를 구현하는 객체로 노출합니다 .</target>
        </trans-unit>
        <trans-unit id="b943e7c2ae0f248f889b02c7d797d243c0d56e6a" translate="yes" xml:space="preserve">
          <source>Univariate feature selector with configurable mode.</source>
          <target state="translated">구성 가능한 모드가있는 일 변량 기능 선택기.</target>
        </trans-unit>
        <trans-unit id="05b44ce5dcc153b8702db1e0eab0c9af1fb62f9c" translate="yes" xml:space="preserve">
          <source>Univariate feature selector with configurable strategy.</source>
          <target state="translated">구성 가능한 전략을 가진 일 변량 기능 선택기.</target>
        </trans-unit>
        <trans-unit id="d9b7ebeeb7d99a7c69a9087085473be1721215ee" translate="yes" xml:space="preserve">
          <source>Univariate linear regression tests.</source>
          <target state="translated">일 변량 선형 회귀 테스트.</target>
        </trans-unit>
        <trans-unit id="833fdc74927caa030c0f5f51bade98d55541b93d" translate="yes" xml:space="preserve">
          <source>Unlabeled entries in &lt;code&gt;y&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;y&lt;/code&gt; 에서 레이블이없는 항목</target>
        </trans-unit>
        <trans-unit id="d5f2b47c1710490958929f8f01f5858025c133e1" translate="yes" xml:space="preserve">
          <source>Unless otherwise specified, input will be cast to &lt;code&gt;float64&lt;/code&gt;:</source>
          <target state="translated">달리 지정하지 않으면 입력은 &lt;code&gt;float64&lt;/code&gt; 로 캐스트됩니다 .</target>
        </trans-unit>
        <trans-unit id="6027b38892a9b0df12f36988c98a41a4655ff464" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt;, the representation of a vector is obtained in an additive fashion, by superimposing the components, without subtracting. Such additive models are efficient for representing images and text.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; 와 달리 , 벡터의 표현은 구성 요소를 빼지 않고 중첩하여 부가적인 방식으로 얻습니다. 이러한 추가 모델은 이미지와 텍스트를 표현하는 데 효율적입니다.</target>
        </trans-unit>
        <trans-unit id="f73d55c7488edaa74f7dc85966062a8a5be2b94b" translate="yes" xml:space="preserve">
          <source>Unlike most other scores, R^2 score may be negative (it need not actually be the square of a quantity R).</source>
          <target state="translated">대부분의 다른 점수와 달리 R ^ 2 점수는 음수 일 수 있습니다 (실제로 수량 R의 제곱 일 필요는 없음).</target>
        </trans-unit>
        <trans-unit id="fb0dd07f15380472f302743b85f6d7c3f60efb9d" translate="yes" xml:space="preserve">
          <source>Unlike the previous scalers, the centering and scaling statistics of this scaler are based on percentiles and are therefore not influenced by a few number of very large marginal outliers. Consequently, the resulting range of the transformed feature values is larger than for the previous scalers and, more importantly, are approximately similar: for both features most of the transformed values lie in a [-2, 3] range as seen in the zoomed-in figure. Note that the outliers themselves are still present in the transformed data. If a separate outlier clipping is desirable, a non-linear transformation is required (see below).</source>
          <target state="translated">이전 스케일러와 달리이 스케일러의 중심 및 스케일링 통계는 백분위 수를 기반으로하므로 소수의 매우 큰 한계 값 이상치의 영향을받지 않습니다. 결과적으로 변환 된 피처 값의 결과 범위는 이전 스케일러보다 크고, 더 중요하게는 거의 비슷합니다. 두 피처 모두 변환 된 값의 대부분은 확대 / 축소에서 볼 수 있듯이 [-2, 3] 범위에 있습니다. 그림에서. 특이 치 자체는 여전히 변환 된 데이터에 존재합니다. 별도의 이상치 클리핑이 필요한 경우 비선형 변환이 필요합니다 (아래 참조).</target>
        </trans-unit>
        <trans-unit id="be4091e1f0941887f57bdebf1b1a9b607356f1f1" translate="yes" xml:space="preserve">
          <source>Unlike the previous transformations, normalization refers to a per sample transformation instead of a per feature transformation.</source>
          <target state="translated">이전 변환과 달리 정규화는 기능별 변환 대신 샘플 당 변환을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="d6efdeaf0fd8663d7b74628a841a2a21988919e0" translate="yes" xml:space="preserve">
          <source>Unregularized graph based semi-supervised learning</source>
          <target state="translated">정규화되지 않은 그래프 기반의 반 감독 학습</target>
        </trans-unit>
        <trans-unit id="27bee227769f6c4dd6bbb550e3dab104adba94bb" translate="yes" xml:space="preserve">
          <source>Unsupervised Outlier Detection using Local Outlier Factor (LOF)</source>
          <target state="translated">LOF (Local Outlier Factor)를 사용한 감독되지 않은 이상치 탐지</target>
        </trans-unit>
        <trans-unit id="7b5353048e77b9864be0e146d8fe78c3034a09d3" translate="yes" xml:space="preserve">
          <source>Unsupervised Outlier Detection.</source>
          <target state="translated">감독되지 않은 이상치 탐지.</target>
        </trans-unit>
        <trans-unit id="091dbb252c60dafbec16ee540eb9588c7d736a5b" translate="yes" xml:space="preserve">
          <source>Unsupervised learner for implementing neighbor searches.</source>
          <target state="translated">이웃 검색을 구현하기위한 비지도 학습자.</target>
        </trans-unit>
        <trans-unit id="336bcbb510eed89e13ba021c352635cdc0677016" translate="yes" xml:space="preserve">
          <source>Unsupervised learning: seeking representations of the data</source>
          <target state="translated">비지도 학습 : 데이터의 표현 추구</target>
        </trans-unit>
        <trans-unit id="568c5820f9e5362ca266c9125e695403019435a8" translate="yes" xml:space="preserve">
          <source>Unused parameter.</source>
          <target state="translated">사용하지 않은 매개 변수.</target>
        </trans-unit>
        <trans-unit id="207a5be036cc811b3313bce86d86c7d5b4302176" translate="yes" xml:space="preserve">
          <source>Update k means estimate on a single mini-batch X.</source>
          <target state="translated">업데이트 k는 단일 미니 배치 X에 대한 추정치를 의미합니다.</target>
        </trans-unit>
        <trans-unit id="718430f889e80a3494a47ee7bec5ca3329673e5e" translate="yes" xml:space="preserve">
          <source>Updated feature-wise means.</source>
          <target state="translated">기능별 수단이 업데이트되었습니다.</target>
        </trans-unit>
        <trans-unit id="4f10d24907ba29eca99bd941c7ead98539a4f5b4" translate="yes" xml:space="preserve">
          <source>Updated feature-wise variances.</source>
          <target state="translated">기능별 차이가 업데이트되었습니다.</target>
        </trans-unit>
        <trans-unit id="693a7de21c7466734e7ffa03c5ae997209e7997d" translate="yes" xml:space="preserve">
          <source>Updated number of seen samples.</source>
          <target state="translated">본 샘플 수를 업데이트했습니다.</target>
        </trans-unit>
        <trans-unit id="410e0e09369f3d862bca36022b47e478be0933f7" translate="yes" xml:space="preserve">
          <source>Updates the model using the data in X as a mini-batch.</source>
          <target state="translated">X의 데이터를 미니 배치로 사용하여 모델을 업데이트합니다.</target>
        </trans-unit>
        <trans-unit id="48b5dd5eaa54e931a34dd1d6396ec1c9d66da80b" translate="yes" xml:space="preserve">
          <source>Urbanowicz R.J., Moore, J.H. &lt;a href=&quot;https://doi.org/10.1007/s12065-015-0128-8&quot;&gt;ExSTraCS 2.0: description and evaluation of a scalable learning classifier system&lt;/a&gt;, Evol. Intel. (2015) 8: 89.</source>
          <target state="translated">Urbanowicz RJ, Moore, JH &lt;a href=&quot;https://doi.org/10.1007/s12065-015-0128-8&quot;&gt;ExSTraCS 2.0 : 확장 가능한 학습 분류기 시스템&lt;/a&gt; , Evol. 인텔. (2015) 8:89.</target>
        </trans-unit>
        <trans-unit id="173610cb31251b28e80fadc258036215d99d7128" translate="yes" xml:space="preserve">
          <source>Usage examples:</source>
          <target state="translated">사용 예 :</target>
        </trans-unit>
        <trans-unit id="272998fc40498f57127bf4e7cf71805cd53c9500" translate="yes" xml:space="preserve">
          <source>Use 0 when &lt;code&gt;Y&lt;/code&gt; contains the output of decision_function (classifier). Use 0.5 when &lt;code&gt;Y&lt;/code&gt; contains the output of predict_proba.</source>
          <target state="translated">&lt;code&gt;Y&lt;/code&gt; 에 decision_function (분류기)의 출력이 포함 된 경우 0을 사용하십시오 . &lt;code&gt;Y&lt;/code&gt; 에 predict_proba의 출력이 포함 된 경우 0.5를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="e620712d1a8872ff21d8a3d8ca61cf7867c4f8c8" translate="yes" xml:space="preserve">
          <source>Use &lt;code&gt;min_samples_split&lt;/code&gt; or &lt;code&gt;min_samples_leaf&lt;/code&gt; to ensure that multiple samples inform every decision in the tree, by controlling which splits will be considered. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. Try &lt;code&gt;min_samples_leaf=5&lt;/code&gt; as an initial value. If the sample size varies greatly, a float number can be used as percentage in these two parameters. While &lt;code&gt;min_samples_split&lt;/code&gt; can create arbitrarily small leaves, &lt;code&gt;min_samples_leaf&lt;/code&gt; guarantees that each leaf has a minimum size, avoiding low-variance, over-fit leaf nodes in regression problems. For classification with few classes, &lt;code&gt;min_samples_leaf=1&lt;/code&gt; is often the best choice.</source>
          <target state="translated">사용 &lt;code&gt;min_samples_split&lt;/code&gt; 또는 &lt;code&gt;min_samples_leaf&lt;/code&gt; 를 여러 샘플 분할이 고려 될 제어하여 트리에서 모든 결정을 통보하도록. 매우 작은 숫자는 일반적으로 나무가 과적 합을 의미하는 반면, 큰 숫자는 나무가 데이터를 학습하지 못하게합니다. &lt;code&gt;min_samples_leaf=5&lt;/code&gt; 를 초기 값으로 사용해보십시오 . 샘플 크기가 크게 변하면 부동 소수점 숫자를이 두 매개 변수에서 백분율로 사용할 수 있습니다. &lt;code&gt;min_samples_split&lt;/code&gt; 은 임의로 작은 잎을 만들 수 있지만 min_samples_leaf 는 각 잎이 최소 크기를 가지도록하여 회귀 문제에서 낮은 분산, &lt;code&gt;min_samples_leaf&lt;/code&gt; 잎 노드를 피합니다. 클래스가 적은 분류의 &lt;code&gt;min_samples_leaf=1&lt;/code&gt; 경우 종종 최선의 선택입니다.</target>
        </trans-unit>
        <trans-unit id="3429b333ce93c8bae91a85fe794f080132090825" translate="yes" xml:space="preserve">
          <source>Use &lt;code&gt;negative_outlier_factor_&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;negative_outlier_factor_&lt;/code&gt; 사용</target>
        </trans-unit>
        <trans-unit id="7760fd58346bed0a2b559e055012bd8a21868552" translate="yes" xml:space="preserve">
          <source>Use SelectFromModel meta-transformer along with Lasso to select the best couple of features from the Boston dataset.</source>
          <target state="translated">Lasso와 함께 SelectFromModel 메타 변환기를 사용하여 Boston 데이터 세트에서 가장 적합한 기능을 선택하십시오.</target>
        </trans-unit>
        <trans-unit id="71685d673fcc400f8aa4ec7ef3e4a61bf3bbf5f7" translate="yes" xml:space="preserve">
          <source>Use approximate bound as score.</source>
          <target state="translated">대략적인 한계를 점수로 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="400d526bc775314ded26ebb1519045ccc0979588" translate="yes" xml:space="preserve">
          <source>Use density = 1 / 3.0 if you want to reproduce the results from Achlioptas, 2001.</source>
          <target state="translated">2001 년 Achlioptas의 결과를 재현하려면 밀도 = 1 / 3.0을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="38fb3c866f165060e0d95ec1a873c702ff2c91dc" translate="yes" xml:space="preserve">
          <source>Use only on new data</source>
          <target state="translated">새로운 데이터에만 사용</target>
        </trans-unit>
        <trans-unit id="0eb6d7f6360fc3b257840e6d0ece909142d961e3" translate="yes" xml:space="preserve">
          <source>Use splitting criteria that compute the average reduction across all n outputs.</source>
          <target state="translated">모든 n 출력에 대한 평균 감소를 계산하는 분할 기준을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="d4a5711bd46bd2a4542a66497bb7f3342ea23a7f" translate="yes" xml:space="preserve">
          <source>Use the Akaike information criterion (AIC), the Bayes Information criterion (BIC) and cross-validation to select an optimal value of the regularization parameter alpha of the &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; estimator.</source>
          <target state="translated">Akaike 정보 기준 (AIC), Bayes 정보 기준 (BIC) 및 교차 검증을 사용하여 &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;올가미&lt;/a&gt; 추정기 의 정규화 매개 변수 알파의 최적 값을 선택하십시오 .</target>
        </trans-unit>
        <trans-unit id="35f6c244b8fd2da4794beb17214246bc1c30610f" translate="yes" xml:space="preserve">
          <source>Usecase</source>
          <target state="translated">Usecase</target>
        </trans-unit>
        <trans-unit id="2d6d1bb4bf090f9031a078f80f81b5cfa346c5fb" translate="yes" xml:space="preserve">
          <source>Used for internal caching. By default, no caching is done. If a string is given, it is the path to the caching directory.</source>
          <target state="translated">내부 캐싱에 사용됩니다. 기본적으로 캐싱은 수행되지 않습니다. 문자열이 제공되면, 캐싱 디렉토리의 경로입니다.</target>
        </trans-unit>
        <trans-unit id="a659f5ae4cfedf0686976ca2f311fa3c53dbc2ce" translate="yes" xml:space="preserve">
          <source>Used for randomizing the singular value decomposition and the k-means initialization. Use an int to make the randomness deterministic. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">특이 값 분해 및 k- 평균 초기화를 무작위 화하는 데 사용됩니다. int를 사용하여 임의성을 결정 론적으로 만드십시오. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="04206996c9eca941c8da97d474cf5a147f4b8713" translate="yes" xml:space="preserve">
          <source>Used to cache the fitted transformers of the pipeline. By default, no caching is performed. If a string is given, it is the path to the caching directory. Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly. Use the attribute &lt;code&gt;named_steps&lt;/code&gt; or &lt;code&gt;steps&lt;/code&gt; to inspect estimators within the pipeline. Caching the transformers is advantageous when fitting is time consuming.</source>
          <target state="translated">파이프 라인의 장착 된 변압기를 캐시하는 데 사용됩니다. 기본적으로 캐싱은 수행되지 않습니다. 문자열이 제공되면, 캐싱 디렉토리의 경로입니다. 캐싱을 활성화하면 피팅하기 전에 변압기 클론이 트리거됩니다. 따라서 파이프 라인에 제공된 변압기 인스턴스를 직접 검사 할 수 없습니다. 파이프 라인 내 추정기를 검사 하려면 &lt;code&gt;named_steps&lt;/code&gt; 또는 &lt;code&gt;steps&lt;/code&gt; 속성을 사용하십시오 . 피팅에 시간이 오래 걸리면 변압기 캐싱이 유리합니다.</target>
        </trans-unit>
        <trans-unit id="b831b0ccc618367ad6990c063d4f6b68c21b54a0" translate="yes" xml:space="preserve">
          <source>Used to cache the output of the computation of the tree. By default, no caching is done. If a string is given, it is the path to the caching directory.</source>
          <target state="translated">트리 계산의 출력을 캐시하는 데 사용됩니다. 기본적으로 캐싱은 수행되지 않습니다. 문자열이 제공되면, 캐싱 디렉토리의 경로입니다.</target>
        </trans-unit>
        <trans-unit id="ec2d1021c796c8396406488bdd5b004de6014166" translate="yes" xml:space="preserve">
          <source>Used to specify the norm used in the penalization. The &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers support only l2 penalties.</source>
          <target state="translated">벌칙에 사용되는 표준을 지정하는 데 사용됩니다. 'newton-cg', 'sag'및 'lbfgs'솔버는 l2 페널티 만 지원합니다.</target>
        </trans-unit>
        <trans-unit id="5a789f5832dbb34972e6ab11f85f2125e32dddce" translate="yes" xml:space="preserve">
          <source>Useful for applying a non-linear transformation in regression problems. This transformation can be given as a Transformer such as the QuantileTransformer or as a function and its inverse such as &lt;code&gt;log&lt;/code&gt; and &lt;code&gt;exp&lt;/code&gt;.</source>
          <target state="translated">회귀 문제에서 비선형 변환을 적용하는 데 유용합니다. 이 변환은 QuantileTransformer와 같은 Transformer로 제공되거나 &lt;code&gt;log&lt;/code&gt; 및 &lt;code&gt;exp&lt;/code&gt; 와 같은 함수 및 역으로 제공 될 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="665be16622d5dce4c265443eced33f5309efed0f" translate="yes" xml:space="preserve">
          <source>Useful only for the newton-cg, sag and lbfgs solvers. Maximum number of iterations taken for the solvers to converge.</source>
          <target state="translated">newton-cg, sag 및 lbfgs 솔버에만 유용합니다. 솔버가 수렴하는 데 걸린 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="1f7081fc6e8837c157dbcac4dfe150624d77daa3" translate="yes" xml:space="preserve">
          <source>Useful only when the solver &amp;lsquo;liblinear&amp;rsquo; is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a &amp;ldquo;synthetic&amp;rdquo; feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes &lt;code&gt;intercept_scaling * synthetic_feature_weight&lt;/code&gt;.</source>
          <target state="translated">솔버 'liblinear'가 사용되고 self.fit_intercept가 True로 설정된 경우에만 유용합니다. 이 경우, x는 [x, self.intercept_scaling]이되고, 즉 intercept_scaling과 동일한 상수 값을 가진 &quot;합성&quot;기능이 인스턴스 벡터에 추가됩니다. 절편은 &lt;code&gt;intercept_scaling * synthetic_feature_weight&lt;/code&gt; 됩니다.</target>
        </trans-unit>
        <trans-unit id="cb4f9242d2c5bef801309a7b11564e9f2917779f" translate="yes" xml:space="preserve">
          <source>Useful tutorials for developing a feel for some of scikit-learn's applications in the machine learning field.</source>
          <target state="translated">머신 러닝 분야의 일부 scikit-learn 응용 프로그램에 대한 느낌을 개발하는 데 유용한 자습서.</target>
        </trans-unit>
        <trans-unit id="bec249e659662f7d5947bf09a1ea1d4a552885b0" translate="yes" xml:space="preserve">
          <source>User Guide</source>
          <target state="translated">사용자 설명서</target>
        </trans-unit>
        <trans-unit id="32b4edc9350cb6d93cc0316d635ce26e35fa63d2" translate="yes" xml:space="preserve">
          <source>Uses BLAS GEMM as replacement for numpy.dot where possible to avoid unnecessary copies.</source>
          <target state="translated">불필요한 복사본을 피하기 위해 BLAS GEMM을 numpy.dot 대신 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ac4bd4f4f631e790604905794abbd6ed09d66803" translate="yes" xml:space="preserve">
          <source>Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.</source>
          <target state="translated">의사 결정 기능 (지원 벡터라고 함)에 훈련 지점의 하위 집합을 사용하므로 메모리 효율성도 높습니다.</target>
        </trans-unit>
        <trans-unit id="4a4b56e0bea50fff706430a1a94289a4e5e03040" translate="yes" xml:space="preserve">
          <source>Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.</source>
          <target state="translated">화이트 박스 모델을 사용합니다. 주어진 상황이 모델에서 관찰 가능한 경우 조건에 대한 설명은 부울 논리에 의해 쉽게 설명됩니다. 대조적으로, (예를 들어, 인공 신경 네트워크에서) 블랙 박스 모델에서, 결과를 해석하기가 더 어려울 수있다.</target>
        </trans-unit>
        <trans-unit id="ced5d25baa7aaf39837296d764096d52eb67f5ca" translate="yes" xml:space="preserve">
          <source>Uses sampling the fourier transform of the kernel characteristic at regular intervals.</source>
          <target state="translated">정기적으로 커널 특성의 푸리에 변환 샘플링을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="2c633fc259072170ae02b4cf8b2266258b09ddc5" translate="yes" xml:space="preserve">
          <source>Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform).</source>
          <target state="translated">fit (또는 fit_transform)에서 학습 한 어휘 및 문서 빈도 (df)를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="cdea6e25f0fe24b03bf907dbf26253d4f40a11df" translate="yes" xml:space="preserve">
          <source>Using &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; or &lt;code&gt;loss=&quot;modified_huber&quot;&lt;/code&gt; enables the &lt;code&gt;predict_proba&lt;/code&gt; method, which gives a vector of probability estimates \(P(y|x)\) per sample \(x\):</source>
          <target state="translated">사용 &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; 또는 &lt;code&gt;loss=&quot;modified_huber&quot;&lt;/code&gt; 가능 &lt;code&gt;predict_proba&lt;/code&gt; 의 확률 추정치 \ (P (Y | X) \)의 벡터 제공 방법에있어서, 당 샘플 \ (X는 \)</target>
        </trans-unit>
        <trans-unit id="4072d118d17ebbe341d060ec8f347bb287543668" translate="yes" xml:space="preserve">
          <source>Using FunctionTransformer to select columns</source>
          <target state="translated">FunctionTransformer를 사용하여 열 선택</target>
        </trans-unit>
        <trans-unit id="af570039f4e6335c176e5a0ced20f61ade37ec58" translate="yes" xml:space="preserve">
          <source>Using KBinsDiscretizer to discretize continuous features</source>
          <target state="translated">KBinsDiscretizer를 사용하여 연속 기능을 분리</target>
        </trans-unit>
        <trans-unit id="58dd6560cd1dd1ff16a956c31444ffff4530a294" translate="yes" xml:space="preserve">
          <source>Using L1 penalization as provided by &lt;code&gt;LinearSVC(loss='l2', penalty='l1',
dual=False)&lt;/code&gt; yields a sparse solution, i.e. only a subset of feature weights is different from zero and contribute to the decision function. Increasing &lt;code&gt;C&lt;/code&gt; yields a more complex model (more feature are selected). The &lt;code&gt;C&lt;/code&gt; value that yields a &amp;ldquo;null&amp;rdquo; model (all weights equal to zero) can be calculated using &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt;&lt;code&gt;l1_min_c&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;LinearSVC(loss='l2', penalty='l1', dual=False)&lt;/code&gt; 에서 제공하는 L1 불이익을 사용하면 스파 스 솔루션이 생성됩니다. 즉, 기능 가중치의 하위 집합 만 0과 다르고 결정 함수에 기여합니다. &lt;code&gt;C&lt;/code&gt; 를 늘리면 더 복잡한 모델이 생성됩니다 (더 많은 기능이 선택됨). &lt;code&gt;C&lt;/code&gt; 의 은 &quot;널 (null)&quot;모델을 산출 값은 (모든 가중치가 0)을 사용하여 계산 될 수 &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt; &lt;code&gt;l1_min_c&lt;/code&gt; 를&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="0033ecf2999fee9c8f71baf5ea186698e26a6aa1" translate="yes" xml:space="preserve">
          <source>Using a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; without cache enabled, it is possible to inspect the original instance such as:</source>
          <target state="translated">캐시를 사용하지 않고 &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt; 을 사용하면 다음 과 같은 원래 인스턴스를 검사 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4b38a6f3576ed137cc4e9a9f59798fb707d4dde4" translate="yes" xml:space="preserve">
          <source>Using a single underlying feature the model learns both the x and y coordinate as output.</source>
          <target state="translated">단일 기본 기능을 사용하여 모델은 x 및 y 좌표를 출력으로 학습합니다.</target>
        </trans-unit>
        <trans-unit id="6d5ea28ea8efb863c08e76177dc50acce9324f64" translate="yes" xml:space="preserve">
          <source>Using a small &lt;code&gt;max_features&lt;/code&gt; value can significantly decrease the runtime.</source>
          <target state="translated">작은 &lt;code&gt;max_features&lt;/code&gt; 값을 사용 하면 런타임이 크게 줄어 듭니다.</target>
        </trans-unit>
        <trans-unit id="03c1a9468c05dba06aec630e70cb3912379c2cb0" translate="yes" xml:space="preserve">
          <source>Using its &lt;code&gt;partial_fit&lt;/code&gt; method on chunks of data fetched sequentially from the local hard drive or a network database.</source>
          <target state="translated">로컬 하드 드라이브 또는 네트워크 데이터베이스에서 순차적으로 페치 된 데이터 청크에 &lt;code&gt;partial_fit&lt;/code&gt; 메소드를 사용 합니다.</target>
        </trans-unit>
        <trans-unit id="ee5e8e298a940fdf98e8e52d2f16edd9327907f7" translate="yes" xml:space="preserve">
          <source>Using kernels</source>
          <target state="translated">커널 사용</target>
        </trans-unit>
        <trans-unit id="9cfba89ca182507cccdcf4094af12bc5a4c62801" translate="yes" xml:space="preserve">
          <source>Using orthogonal matching pursuit for recovering a sparse signal from a noisy measurement encoded with a dictionary</source>
          <target state="translated">사전으로 인코딩 된 노이즈 측정에서 희소 신호를 복구하기 위해 직교 매칭 추적 사용</target>
        </trans-unit>
        <trans-unit id="24a0ae926e510d4f01c040899e37f954b1d9b717" translate="yes" xml:space="preserve">
          <source>Using pre_dispatch in a producer/consumer situation, where the data is generated on the fly. Note how the producer is first called 3 times before the parallel loop is initiated, and then called to generate new data on the fly:</source>
          <target state="translated">생산자 / 소비자 상황에서 pre_dispatch를 사용하여 데이터가 즉시 생성됩니다. 병렬 루프가 시작되기 전에 생산자를 처음 3 번 호출 한 다음 새 데이터를 즉시 생성하기 위해 호출되는 방법에 유의하십시오.</target>
        </trans-unit>
        <trans-unit id="efd1182f39233190c4734b5ce34b9cfcf1bbe0bb" translate="yes" xml:space="preserve">
          <source>Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008.</source>
          <target state="translated">t-SNE 사용. 기계 학습 연구 저널 9 : 2579-2605, 2008.</target>
        </trans-unit>
        <trans-unit id="52758be5ab8f6ab028b2bf9ad6db5d2b69896663" translate="yes" xml:space="preserve">
          <source>Using the &lt;code&gt;TfidfTransformer&lt;/code&gt;&amp;rsquo;s default settings, &lt;code&gt;TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)&lt;/code&gt; the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which is computed as</source>
          <target state="translated">은 Using &lt;code&gt;TfidfTransformer&lt;/code&gt; 의 기본 설정을 &lt;code&gt;TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)&lt;/code&gt; , 용어 빈도, 기간이 주어진 문서의 발생 횟수, IDF 구성 요소에 곱하는 이것은 다음과 같이 계산됩니다</target>
        </trans-unit>
        <trans-unit id="95d62a973e97428e1fb3d7b86f3a392143b00b8c" translate="yes" xml:space="preserve">
          <source>Using the GraphicalLasso estimator to learn a covariance and sparse precision from a small number of samples.</source>
          <target state="translated">GraphicalLasso 추정기를 사용하여 적은 수의 샘플에서 공분산 및 희소 정밀도를 학습합니다.</target>
        </trans-unit>
        <trans-unit id="f9e94a3c6c72e38b7c72fe500879db6b6bb31872" translate="yes" xml:space="preserve">
          <source>Using the Iris dataset, we can construct a tree as follows:</source>
          <target state="translated">Iris 데이터 셋을 사용하여 다음과 같이 트리를 구성 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f990799abb31a15673ef02f50b5506399075290a" translate="yes" xml:space="preserve">
          <source>Using the expected value, the adjusted mutual information can then be calculated using a similar form to that of the adjusted Rand index:</source>
          <target state="translated">예상 값을 사용하여 조정 된 상호 정보는 조정 된 랜드 인덱스와 유사한 형식을 사용하여 계산할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="525fdffa79943fda791ea83f2bc571cbb29e45c1" translate="yes" xml:space="preserve">
          <source>Using the naive conditional independence assumption that</source>
          <target state="translated">순진한 조건부 독립성 가정을 사용하여</target>
        </trans-unit>
        <trans-unit id="ded2353583b49d229cc248063161251fcd75da59" translate="yes" xml:space="preserve">
          <source>Using the prediction pipeline in a grid search</source>
          <target state="translated">그리드 검색에서 예측 파이프 라인 사용</target>
        </trans-unit>
        <trans-unit id="4b65de55e2dd198ac6e2aecd67614d2f3e1d68d9" translate="yes" xml:space="preserve">
          <source>Using the results of the previous exercises and the &lt;code&gt;cPickle&lt;/code&gt; module of the standard library, write a command line utility that detects the language of some text provided on &lt;code&gt;stdin&lt;/code&gt; and estimate the polarity (positive or negative) if the text is written in English.</source>
          <target state="translated">이전 연습의 결과와 표준 라이브러리 의 &lt;code&gt;cPickle&lt;/code&gt; 모듈을 사용하여 &lt;code&gt;stdin&lt;/code&gt; 에 제공된 일부 텍스트의 언어를 감지 하고 텍스트가 영어로 작성된 경우 극성 (양수 또는 음수)을 추정 하는 명령 행 유틸리티를 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="271df6087c1c40487d3dd610dcce2afcb5a005f8" translate="yes" xml:space="preserve">
          <source>Using this modification, the tf-idf of the third term in document 1 changes to 1.8473:</source>
          <target state="translated">이 수정을 사용하면 문서 1의 세 번째 용어 인 tf-idf가 1.8473으로 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="35033b7b1c0300bd76803da2e755fdbe07a7c28b" translate="yes" xml:space="preserve">
          <source>Utilities from joblib:</source>
          <target state="translated">joblib의 유틸리티 :</target>
        </trans-unit>
        <trans-unit id="c9ee5681d3c59f7541c27a38b67edf46259e187b" translate="yes" xml:space="preserve">
          <source>V</source>
          <target state="translated">V</target>
        </trans-unit>
        <trans-unit id="8d1950c14bc870de437b37f806aaa51c635a9ec3" translate="yes" xml:space="preserve">
          <source>V measure</source>
          <target state="translated">V 측정</target>
        </trans-unit>
        <trans-unit id="a6ed7787c295565530f8c589d9ab12370f5f5b3d" translate="yes" xml:space="preserve">
          <source>V or VI</source>
          <target state="translated">V 또는 VI</target>
        </trans-unit>
        <trans-unit id="e659ac0cd03fda8c2776727471548e055d6ecaa7" translate="yes" xml:space="preserve">
          <source>V-Measure (NMI with arithmetic mean option.)</source>
          <target state="translated">V- 측정 (산술 평균 옵션이있는 NMI)</target>
        </trans-unit>
        <trans-unit id="893c35d89ea6a5c89935fd8eeed462af4410524f" translate="yes" xml:space="preserve">
          <source>V-Measure is furthermore symmetric: swapping &lt;code&gt;labels_true&lt;/code&gt; and &lt;code&gt;label_pred&lt;/code&gt; will give the same score. This does not hold for homogeneity and completeness. V-Measure is identical to &lt;a href=&quot;sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt;&lt;code&gt;normalized_mutual_info_score&lt;/code&gt;&lt;/a&gt; with the arithmetic averaging method.</source>
          <target state="translated">또한 V-Measure는 대칭입니다. &lt;code&gt;labels_true&lt;/code&gt; 와 &lt;code&gt;label_pred&lt;/code&gt; 를 바꾸면 동일한 점수가 부여됩니다. 이것은 동질성과 완전성을 유지하지 않습니다. V-Measure는 산술 평균법 으로 &lt;a href=&quot;sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt; &lt;code&gt;normalized_mutual_info_score&lt;/code&gt; &lt;/a&gt; 와 동일합니다 .</target>
        </trans-unit>
        <trans-unit id="47faeee4990a814efec079e593cccd036ad6778a" translate="yes" xml:space="preserve">
          <source>V-measure cluster labeling given a ground truth.</source>
          <target state="translated">확실한 사실을 고려한 V- 측정 클러스터 라벨링.</target>
        </trans-unit>
        <trans-unit id="a4fd517acce42be80ab9791260ae88f5cbdf1a52" translate="yes" xml:space="preserve">
          <source>V. Metsis, I. Androutsopoulos and G. Paliouras (2006). &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.61.5542&quot;&gt;Spam filtering with Naive Bayes &amp;ndash; Which Naive Bayes?&lt;/a&gt; 3rd Conf. on Email and Anti-Spam (CEAS).</source>
          <target state="translated">V. Metsis, I. Androutsopoulos 및 G. Paliouras (2006). &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.61.5542&quot;&gt;Naive Bayes를 사용한 스팸 필터링 &amp;ndash; 어떤 Naive Bayes? &lt;/a&gt;3 차 회의 이메일 및 스팸 방지 (CEAS)</target>
        </trans-unit>
        <trans-unit id="942786415758a60976a047b84669d53dbc12ecc2" translate="yes" xml:space="preserve">
          <source>V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with naive Bayes &amp;ndash; Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).</source>
          <target state="translated">V. Metsis, I. Androutsopoulos 및 G. Paliouras (2006). 순진한 베이로 스팸 필터링 &amp;ndash; 어떤 순진한 베이? 3 차 회의 이메일 및 스팸 방지 (CEAS)</target>
        </trans-unit>
        <trans-unit id="ca4c1a38268237e7698432accf1a9a5a48b6fb08" translate="yes" xml:space="preserve">
          <source>Valid metrics for pairwise_distances.</source>
          <target state="translated">pairwise_distances에 유효한 메트릭입니다.</target>
        </trans-unit>
        <trans-unit id="26700c8a25eea6fd902a0bb4963f14783c982650" translate="yes" xml:space="preserve">
          <source>Valid metrics for pairwise_kernels</source>
          <target state="translated">pairwise_kernels에 유효한 메트릭</target>
        </trans-unit>
        <trans-unit id="850c962c3f063b586578621ee12bbb84d6f38bb7" translate="yes" xml:space="preserve">
          <source>Valid options:</source>
          <target state="translated">유효한 옵션 :</target>
        </trans-unit>
        <trans-unit id="493108de26f61e3b76acfe363b14fa409e1fdc9a" translate="yes" xml:space="preserve">
          <source>Valid parameter keys can be listed with &lt;code&gt;get_params()&lt;/code&gt;.</source>
          <target state="translated">유효한 매개 변수 키는 &lt;code&gt;get_params()&lt;/code&gt; 로 나열 될 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1a791ec45bdf21e7b9592679ed5472b3c5ab8093" translate="yes" xml:space="preserve">
          <source>Valid parameter keys can be listed with get_params().</source>
          <target state="translated">유효한 매개 변수 키는 get_params ()로 나열 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="90d9fefcb561047bbbd742b13c3608c6fcf1e657" translate="yes" xml:space="preserve">
          <source>Valid values for metric are:</source>
          <target state="translated">메트릭의 유효한 값은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="47edea5ff3c24dcb16dc15647447ec5c4a9d77c2" translate="yes" xml:space="preserve">
          <source>Valid values for metric are::</source>
          <target state="translated">메트릭의 유효한 값은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="ec59a2a93f21b1aa3fbc16cd26b3b2dbab6fa78b" translate="yes" xml:space="preserve">
          <source>Validation curve.</source>
          <target state="translated">검증 곡선.</target>
        </trans-unit>
        <trans-unit id="675b8482a7f9f38fa965a1efe10c6a6ee6ec5cdb" translate="yes" xml:space="preserve">
          <source>Value added to the diagonal of the kernel matrix during fitting. Larger values correspond to increased noise level in the observations. This can also prevent a potential numerical issue during fitting, by ensuring that the calculated values form a positive definite matrix. If an array is passed, it must have the same number of entries as the data used for fitting and is used as datapoint-dependent noise level. Note that this is equivalent to adding a WhiteKernel with c=alpha. Allowing to specify the noise level directly as a parameter is mainly for convenience and for consistency with Ridge.</source>
          <target state="translated">피팅하는 동안 커널 행렬의 대각선에 추가 된 값입니다. 값이 클수록 관측치의 노이즈 수준이 증가합니다. 또한 계산 된 값이 양의 한정 행렬을 형성하도록하여 피팅하는 동안 잠재적 인 수치 문제를 방지 할 수 있습니다. 어레이가 전달되면 피팅에 사용되는 데이터와 동일한 수의 항목이 있어야하며 데이터 포인트 종속 노이즈 레벨로 사용됩니다. 이것은 c = alpha로 WhiteKernel을 추가하는 것과 같습니다. 노이즈 레벨을 매개 변수로 직접 지정할 수 있도록하는 것은 주로 편의성과 Ridge와의 일관성을위한 것입니다.</target>
        </trans-unit>
        <trans-unit id="cdffc29f88adeffd4bcff100fb7aa53a66956d52" translate="yes" xml:space="preserve">
          <source>Value for numerical stability in adam. Only used when solver=&amp;rsquo;adam&amp;rsquo;</source>
          <target state="translated">아담의 수치 안정성 값. solver = 'adam'인 경우에만 사용</target>
        </trans-unit>
        <trans-unit id="c6350d6ef6528ee88ef12a12465bebefd1891dd8" translate="yes" xml:space="preserve">
          <source>Value of the pseudo-likelihood (proxy for likelihood).</source>
          <target state="translated">유사 가능성의 값 (가능성에 대한 프록시).</target>
        </trans-unit>
        <trans-unit id="7c9f9f5dcfc8aebd4eea110198ededa32c4b1278" translate="yes" xml:space="preserve">
          <source>Value to assign to the score if an error occurs in estimator fitting. If set to &amp;lsquo;raise&amp;rsquo;, the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. Default is &amp;lsquo;raise&amp;rsquo; but from version 0.22 it will change to np.nan.</source>
          <target state="translated">추정기 피팅에서 오류가 발생할 경우 점수에 지정할 값입니다. '올림'으로 설정하면 오류가 발생합니다. 숫자 값이 제공되면 FitFailedWarning이 발생합니다. 이 매개 변수는 수정 단계에 영향을 미치지 않으므로 항상 오류가 발생합니다. 기본값은 '올림'이지만 버전 0.22부터 np.nan으로 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="51fe27463b1bbffac6c175b98fff3a09a8ef007a" translate="yes" xml:space="preserve">
          <source>Value to assign to the score if an error occurs in estimator fitting. If set to &amp;lsquo;raise&amp;rsquo;, the error is raised. If set to &amp;lsquo;raise-deprecating&amp;rsquo;, a FutureWarning is printed before the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. Default is &amp;lsquo;raise-deprecating&amp;rsquo; but from version 0.22 it will change to np.nan.</source>
          <target state="translated">추정기 피팅에서 오류가 발생할 경우 점수에 지정할 값입니다. '올림'으로 설정하면 오류가 발생합니다. '사용 중지'로 설정하면 오류가 발생하기 전에 FutureWarning이 인쇄됩니다. 숫자 값이 제공되면 FitFailedWarning이 발생합니다. 이 매개 변수는 수정 단계에 영향을 미치지 않으므로 항상 오류가 발생합니다. 기본값은 '감추기'이지만 버전 0.22부터는 np.nan으로 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="402a4cfb84a22d3670431b1576518e6587de93b8" translate="yes" xml:space="preserve">
          <source>Value to use for the dummy feature.</source>
          <target state="translated">더미 기능에 사용할 값입니다.</target>
        </trans-unit>
        <trans-unit id="82321dd8f607145fb8d2875c3e367d82d45dfc71" translate="yes" xml:space="preserve">
          <source>Value with which negative labels must be encoded.</source>
          <target state="translated">음수 레이블을 인코딩해야하는 값입니다.</target>
        </trans-unit>
        <trans-unit id="4e0758fceaa4f106e89501aa7c196eea7d1ad1c2" translate="yes" xml:space="preserve">
          <source>Value with which positive labels must be encoded.</source>
          <target state="translated">양수 레이블을 인코딩해야하는 값입니다.</target>
        </trans-unit>
        <trans-unit id="6a7ed2e67e56dace630120ac5c7bd01e4d032523" translate="yes" xml:space="preserve">
          <source>Values greater than the threshold map to 1, while values less than or equal to the threshold map to 0. With the default threshold of 0, only positive values map to 1.</source>
          <target state="translated">임계 값보다 큰 값은 1에 매핑되고 임계 값보다 작거나 같은 값은 0에 매핑됩니다. 기본 임계 값이 0이면 양수 값만 1에 매핑됩니다.</target>
        </trans-unit>
        <trans-unit id="0a659f48fb09b2c2dd949774cc3bd6b7ffd6fd87" translate="yes" xml:space="preserve">
          <source>Values in each bin have the same nearest center of a 1D k-means cluster.</source>
          <target state="translated">각 구간의 값은 1D k- 평균 군집의 가장 가까운 중심을 갖습니다.</target>
        </trans-unit>
        <trans-unit id="c67d763b94a97115ba7ee9d49115a272cb508cf6" translate="yes" xml:space="preserve">
          <source>Values of n_samples samples drawn from Gaussian process and evaluated at query points.</source>
          <target state="translated">n_samples의 값은 가우스 프로세스에서 추출되어 쿼리 지점에서 평가됩니다.</target>
        </trans-unit>
        <trans-unit id="1cd1b62dfd3b6a63572d1bf631789bd088451d1d" translate="yes" xml:space="preserve">
          <source>Values of the visible layer after one Gibbs step.</source>
          <target state="translated">1 Gibbs 단계 후 표시되는 레이어의 값입니다.</target>
        </trans-unit>
        <trans-unit id="c9500aef779ad4ab4355590ca05b9c0de0aa083a" translate="yes" xml:space="preserve">
          <source>Values of the visible layer to start from.</source>
          <target state="translated">보이는 레이어의 값으로 시작합니다.</target>
        </trans-unit>
        <trans-unit id="d9ca5115511a5b00d878e1de5b9daa8f530b632c" translate="yes" xml:space="preserve">
          <source>Values of the visible layer. Must be all-boolean (not checked).</source>
          <target state="translated">보이는 레이어의 값. 모두 부울이어야합니다 (확인되지 ​​않음).</target>
        </trans-unit>
        <trans-unit id="445d09e482944669dc8a6e893591f45bda28254b" translate="yes" xml:space="preserve">
          <source>Vanschoren, van Rijn, Bischl and Torgo &lt;a href=&quot;https://arxiv.org/pdf/1407.7722.pdf&quot;&gt;&amp;ldquo;OpenML: networked science in machine learning&amp;rdquo;&lt;/a&gt;, ACM SIGKDD Explorations Newsletter, 15(2), 49-60, 2014.</source>
          <target state="translated">Vanschoren, van Rijn, Bischl 및 Torgo &lt;a href=&quot;https://arxiv.org/pdf/1407.7722.pdf&quot;&gt;&amp;ldquo;OpenML : 기계 학습의 네트워크 과학&amp;rdquo;&lt;/a&gt; , ACM SIGKDD Explorations Newsletter, 15 (2), 49-60, 2014.</target>
        </trans-unit>
        <trans-unit id="ba47c39bbafea14cc75438e6420272a547d1a3e3" translate="yes" xml:space="preserve">
          <source>Variance explained by each of the selected components.</source>
          <target state="translated">선택한 각 구성 요소에 따라 차이가 설명됩니다.</target>
        </trans-unit>
        <trans-unit id="17b5cb397e6ad9830d59b5fc66bebb45024c7ef4" translate="yes" xml:space="preserve">
          <source>Variances of individual features.</source>
          <target state="translated">개별 기능의 차이.</target>
        </trans-unit>
        <trans-unit id="7933f72bf76c6cfbc1dde87498522f5e833878e6" translate="yes" xml:space="preserve">
          <source>Variational Bayesian estimation of a Gaussian mixture.</source>
          <target state="translated">가우스 혼합의 변형 베이지안 추정.</target>
        </trans-unit>
        <trans-unit id="e459652719d6e0edf38bb3c9f14dba040888c62f" translate="yes" xml:space="preserve">
          <source>Variational inference is an extension of expectation-maximization that maximizes a lower bound on model evidence (including priors) instead of data likelihood. The principle behind variational methods is the same as expectation-maximization (that is both are iterative algorithms that alternate between finding the probabilities for each point to be generated by each mixture and fitting the mixture to these assigned points), but variational methods add regularization by integrating information from prior distributions. This avoids the singularities often found in expectation-maximization solutions but introduces some subtle biases to the model. Inference is often notably slower, but not usually as much so as to render usage unpractical.</source>
          <target state="translated">변형 추론은 데이터 가능성 대신 모형 증거 (사전 포함)의 하한을 최대화하는 기대 최대화의 확장입니다. 변형 방법의 기본 원리는 기대 최대화와 동일합니다 (둘 다 각 혼합물에 의해 생성 될 각 점에 대한 확률 찾기와 이러한 할당 된 점에 혼합물 맞추기간에 번갈아 가며 반복되는 알고리즘입니다). 이전 배포판의 정보 통합 이를 통해 기대 최대화 솔루션에서 흔히 발견되는 특이점을 피할 수 있지만 모델에 미묘한 편향이 발생합니다. 추론은 종종 속도가 느리지 만 일반적으로 사용을 실용적이지 않게하는 정도는 아닙니다.</target>
        </trans-unit>
        <trans-unit id="009e019794c4b5a288d65b8e0eabe9064e298c81" translate="yes" xml:space="preserve">
          <source>Variational inference techniques for the Dirichlet process still work with a finite approximation to this infinite mixture model, but instead of having to specify a priori how many components one wants to use, one just specifies the concentration parameter and an upper bound on the number of mixture components (this upper bound, assuming it is higher than the &amp;ldquo;true&amp;rdquo; number of components, affects only algorithmic complexity, not the actual number of components used).</source>
          <target state="translated">Dirichlet 공정에 대한 변형 추론 기법은 여전히이 무한 혼합 모델에 대한 유한 근사법으로 작동하지만 사용하고자하는 성분 수를 사전에 지정하는 대신 농도 매개 변수와 혼합물 수의 상한을 지정합니다. 구성 요소 ( &quot;실제&quot;구성 요소 수보다 높다고 가정하면이 상한은 실제 사용되는 구성 요소 수가 아니라 알고리즘 복잡성에만 영향을 미칩니다).</target>
        </trans-unit>
        <trans-unit id="bfc42eb1bb86ce5f62b086e9ffd3c67a080b0730" translate="yes" xml:space="preserve">
          <source>Variational parameters for topic word distribution. Since the complete conditional for topic word distribution is a Dirichlet, &lt;code&gt;components_[i, j]&lt;/code&gt; can be viewed as pseudocount that represents the number of times word &lt;code&gt;j&lt;/code&gt; was assigned to topic &lt;code&gt;i&lt;/code&gt;. It can also be viewed as distribution over the words for each topic after normalization: &lt;code&gt;model.components_ / model.components_.sum(axis=1)[:, np.newaxis]&lt;/code&gt;.</source>
          <target state="translated">주제 단어 분포에 대한 변형 매개 변수. 주제 단어 분포에 대한 완전한 조건은 Dirichlet이므로 &lt;code&gt;components_[i, j]&lt;/code&gt; 는 단어 &lt;code&gt;j&lt;/code&gt; 가 주제 &lt;code&gt;i&lt;/code&gt; 에 할당 된 횟수를 나타내는 의사 수로 볼 수 있습니다 . 정규화 후 각 주제에 대한 단어에 대한 분포로 볼 수도 있습니다. &lt;code&gt;model.components_ / model.components_.sum(axis=1)[:, np.newaxis]&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="65cb8d21f19957f8f56d7ec3aeccadf0c1efbc6f" translate="yes" xml:space="preserve">
          <source>Various Agglomerative Clustering on a 2D embedding of digits</source>
          <target state="translated">자릿수의 2D 임베딩에 대한 다양한 집계 클러스터링</target>
        </trans-unit>
        <trans-unit id="b1e276370580ceeca94d9a25d3b75abf89a69938" translate="yes" xml:space="preserve">
          <source>Varying regularization in Multi-layer Perceptron</source>
          <target state="translated">다층 퍼셉트론의 다양한 정규화</target>
        </trans-unit>
        <trans-unit id="1e178759402bc070ae202c1ae1b1666da0dd6a9c" translate="yes" xml:space="preserve">
          <source>Vector Quantization Example</source>
          <target state="translated">벡터 양자화 예</target>
        </trans-unit>
        <trans-unit id="ba8b3829eecac2c5ad85a64a161b0e7f2fae54cf" translate="yes" xml:space="preserve">
          <source>Vector of errors at each iteration.</source>
          <target state="translated">각 반복에서 오류의 벡터입니다.</target>
        </trans-unit>
        <trans-unit id="93c6f0903309fd539ceb7d4710cf07f7db8cb1d8" translate="yes" xml:space="preserve">
          <source>Verbose mode when fitting the model.</source>
          <target state="translated">모델을 피팅 할 때 상세 모드.</target>
        </trans-unit>
        <trans-unit id="eeb343db47ab9124ef417c41d9bdb92839772dc0" translate="yes" xml:space="preserve">
          <source>Verbose output during PD computations. Defaults to 0.</source>
          <target state="translated">PD 계산 중 자세한 출력. 기본값은 0입니다.</target>
        </trans-unit>
        <trans-unit id="65d3f9d357c8217e4b4161cc31c9983e755e9511" translate="yes" xml:space="preserve">
          <source>Verbosity flag, controls the debug messages that are issued as functions are evaluated.</source>
          <target state="translated">Verbosity 플래그는 함수가 평가 될 때 발행되는 디버그 메시지를 제어합니다.</target>
        </trans-unit>
        <trans-unit id="a606460a9db19f2456900341e545d542d386dcd0" translate="yes" xml:space="preserve">
          <source>Verbosity level.</source>
          <target state="translated">상세 수준.</target>
        </trans-unit>
        <trans-unit id="9cadb350a887ea26b759cec53fba259b94be0b70" translate="yes" xml:space="preserve">
          <source>Verbosity level. Setting verbose &amp;gt; 0 will display additional information depending on the solver used.</source>
          <target state="translated">상세 수준. verbose&amp;gt; 0을 설정하면 사용 된 솔버에 따라 추가 정보가 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="c09635f4883cd7798a06597eead68509e08bef52" translate="yes" xml:space="preserve">
          <source>Verbosity mode.</source>
          <target state="translated">상세 모드.</target>
        </trans-unit>
        <trans-unit id="4ee9c427e0cc678ca91bc7294708dc43db03512b" translate="yes" xml:space="preserve">
          <source>Versatile: different &lt;a href=&quot;#gp-kernels&quot;&gt;kernels&lt;/a&gt; can be specified. Common kernels are provided, but it is also possible to specify custom kernels.</source>
          <target state="translated">다용도 : 다른 &lt;a href=&quot;#gp-kernels&quot;&gt;커널을&lt;/a&gt; 지정할 수 있습니다. 일반적인 커널이 제공되지만 사용자 정의 커널을 지정할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="4b16fac84e3102257e26d97dc6bcc2775a76c275" translate="yes" xml:space="preserve">
          <source>Versatile: different &lt;a href=&quot;#svm-kernels&quot;&gt;Kernel functions&lt;/a&gt; can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.</source>
          <target state="translated">다용도 : 의사 결정 기능에 대해 다른 &lt;a href=&quot;#svm-kernels&quot;&gt;커널 기능을&lt;/a&gt; 지정할 수 있습니다. 일반적인 커널이 제공되지만 사용자 정의 커널을 지정할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="102caab3d934ebf62d9240e41edbdfb96ec830ff" translate="yes" xml:space="preserve">
          <source>Version of the dataset. Can only be provided if also &lt;code&gt;name&lt;/code&gt; is given. If &amp;lsquo;active&amp;rsquo; the oldest version that&amp;rsquo;s still active is used. Since there may be more than one active version of a dataset, and those versions may fundamentally be different from one another, setting an exact version is highly recommended.</source>
          <target state="translated">데이터 세트의 버전입니다. &lt;code&gt;name&lt;/code&gt; 이 지정된 경우에만 제공 할 수 있습니다 . '활성'인 경우 여전히 활성 상태 인 가장 오래된 버전이 사용됩니다. 데이터 세트의 활성 버전이 둘 이상있을 수 있으며 해당 버전이 근본적으로 다를 수 있으므로 정확한 버전을 설정하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="2d0ab9e3d9a896817cdfc684c77fbf9f0c4f1aac" translate="yes" xml:space="preserve">
          <source>Version: RCV1-v2, vectors, full sets, topics multilabels.</source>
          <target state="translated">버전 : RCV1-v2, 벡터, 전체 세트, 주제 다중 레이블.</target>
        </trans-unit>
        <trans-unit id="3496b8ff284f2499d5b89bafe815a9579d5a779b" translate="yes" xml:space="preserve">
          <source>Very large &lt;code&gt;n_samples&lt;/code&gt;, medium &lt;code&gt;n_clusters&lt;/code&gt;</source>
          <target state="translated">매우 큰 &lt;code&gt;n_samples&lt;/code&gt; , 중간 &lt;code&gt;n_clusters&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="90edfe52a065a045edbdd25bf2f2316a234658ac" translate="yes" xml:space="preserve">
          <source>Very large &lt;code&gt;n_samples&lt;/code&gt;, medium &lt;code&gt;n_clusters&lt;/code&gt; with &lt;a href=&quot;#mini-batch-kmeans&quot;&gt;MiniBatch code&lt;/a&gt;</source>
          <target state="translated">매우 큰 &lt;code&gt;n_samples&lt;/code&gt; , 매체 &lt;code&gt;n_clusters&lt;/code&gt; 와 &lt;a href=&quot;#mini-batch-kmeans&quot;&gt;MiniBatch 코드&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="56b71e89fb1079caaadefd0889e9a22e8b0560e3" translate="yes" xml:space="preserve">
          <source>Videos</source>
          <target state="translated">Videos</target>
        </trans-unit>
        <trans-unit id="f74a22805e87ed187a5bf75da0c74d88f9fc4e05" translate="yes" xml:space="preserve">
          <source>Vinh et al. (2010) named variants of NMI and AMI by their averaging method [VEB2010]. Their &amp;lsquo;sqrt&amp;rsquo; and &amp;lsquo;sum&amp;rsquo; averages are the geometric and arithmetic means; we use these more broadly common names.</source>
          <target state="translated">Vinh et al. (2010)은 평균화 방법 [VEB2010]으로 NMI 및 AMI의 변형을 명명했습니다. 그들의 'sqrt'와 'sum'평균은 기하 및 산술 수단입니다. 우리는 이러한보다 일반적인 이름을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="40fdfddfc4699e8e891d021e6ce4e4252243f9c7" translate="yes" xml:space="preserve">
          <source>Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance, JMLR</source>
          <target state="translated">Vinh, Epps, Bailey (2010). 군집화 비교를위한 정보 이론적 측정 : 변형, 특성, 정규화 및 수정, JMLR에 대한 수정</target>
        </trans-unit>
        <trans-unit id="64eeb8915eff8290b0627c8aa96dd46ee4bda6f1" translate="yes" xml:space="preserve">
          <source>Visualise your tree as you are training by using the &lt;code&gt;export&lt;/code&gt; function. Use &lt;code&gt;max_depth=3&lt;/code&gt; as an initial tree depth to get a feel for how the tree is fitting to your data, and then increase the depth.</source>
          <target state="translated">&lt;code&gt;export&lt;/code&gt; 기능 을 사용하여 훈련하는 동안 트리를 시각화하십시오 . 사용 &lt;code&gt;max_depth=3&lt;/code&gt; 초기 트리 깊이로는 나무가 데이터에 피팅되는 방법에 대한 느낌을 얻을하고 깊이를 증가시킵니다.</target>
        </trans-unit>
        <trans-unit id="d175985b87dd9f620aa960059c730b4a35e3bcb5" translate="yes" xml:space="preserve">
          <source>Visualization</source>
          <target state="translated">Visualization</target>
        </trans-unit>
        <trans-unit id="38623835e09f3cd243cdf966707dad9de4ecfeda" translate="yes" xml:space="preserve">
          <source>Visualization of MLP weights on MNIST</source>
          <target state="translated">MNIST의 MLP 가중치 시각화</target>
        </trans-unit>
        <trans-unit id="a291ff40a157c9afd67fb01fd3c6b6d368d78978" translate="yes" xml:space="preserve">
          <source>Visualization of predictions obtained from different models.</source>
          <target state="translated">다른 모델에서 얻은 예측 시각화.</target>
        </trans-unit>
        <trans-unit id="a6c65fa336dc53edc04e670583358fb288a99997" translate="yes" xml:space="preserve">
          <source>Visualize cross-validation indices for many CV objects</source>
          <target state="translated">많은 CV 객체에 대한 교차 검증 지수 시각화</target>
        </trans-unit>
        <trans-unit id="4a2bddc914855cb9100c33fc5e346e89e1926136" translate="yes" xml:space="preserve">
          <source>Visualize our data</source>
          <target state="translated">데이터 시각화</target>
        </trans-unit>
        <trans-unit id="28ba2de8813583360eb0588f62cb6dc19a1f4072" translate="yes" xml:space="preserve">
          <source>Visualize the resulting regions</source>
          <target state="translated">결과 영역을 시각화</target>
        </trans-unit>
        <trans-unit id="b77da1f257213eacf5971ec98d2f34c8fe910374" translate="yes" xml:space="preserve">
          <source>Visualizing cross-validation behavior in scikit-learn</source>
          <target state="translated">scikit-learn에서 교차 유효성 검사 동작 시각화</target>
        </trans-unit>
        <trans-unit id="e6614e53d3137ea4329f7b88a52f014060c402bb" translate="yes" xml:space="preserve">
          <source>Visualizing the stock market structure</source>
          <target state="translated">주식 시장 구조 시각화</target>
        </trans-unit>
        <trans-unit id="3e1e0ef10e7a115946f530e934380438421c5b34" translate="yes" xml:space="preserve">
          <source>Vocabulary: classification and regression</source>
          <target state="translated">어휘 : 분류 및 회귀</target>
        </trans-unit>
        <trans-unit id="dd7b37acd93acaf11b82a9ebbc3eea819b85e0ef" translate="yes" xml:space="preserve">
          <source>W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) 163-171.</source>
          <target state="translated">WH Wolberg, WN Street 및 OL Mangasarian. 미세 바늘 흡 인물로부터 유방암을 진단하는 기계 학습 기술. 암 편지 77 (1994) 163-171.</target>
        </trans-unit>
        <trans-unit id="985d7c09beb3a8622b6c063b009de9296fdb89ed" translate="yes" xml:space="preserve">
          <source>W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis. IS&amp;amp;T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San Jose, CA, 1993.</source>
          <target state="translated">WN Street, WH Wolberg 및 OL Mangasarian. 유방 종양 진단을위한 핵 특징 추출. IS &amp;amp; T / SPIE 1993 전자 이미징에 관한 국제 심포지엄 : 과학 및 기술, 1905 권, 861-870 쪽, 캘리포니아 주 산호세, 1993.</target>
        </trans-unit>
        <trans-unit id="0525374f4c7331dc5d256feba32a265d327160ed" translate="yes" xml:space="preserve">
          <source>WDBC-Benign</source>
          <target state="translated">WDBC-Benign</target>
        </trans-unit>
        <trans-unit id="fd630df285b07b6076d3b38d88445f6031d3902e" translate="yes" xml:space="preserve">
          <source>WDBC-Malignant</source>
          <target state="translated">WDBC-Malignant</target>
        </trans-unit>
        <trans-unit id="9b852a8108b3e892136da9e7da9f0a5bb56540ea" translate="yes" xml:space="preserve">
          <source>WMinkowskiDistance</source>
          <target state="translated">WMinkowskiDistance</target>
        </trans-unit>
        <trans-unit id="4e8ee595c7db5dd5f284f8fb603cc66c6c8287ae" translate="yes" xml:space="preserve">
          <source>Ward clustering based on a Feature matrix.</source>
          <target state="translated">기능 매트릭스를 기반으로하는 와드 클러스터링.</target>
        </trans-unit>
        <trans-unit id="1af684513cf70467c9307765e01677f8970cff6e" translate="yes" xml:space="preserve">
          <source>Ward hierarchical clustering</source>
          <target state="translated">와드 계층 적 클러스터링</target>
        </trans-unit>
        <trans-unit id="d2173ac976f5809b703436c0de33dab1598b674c" translate="yes" xml:space="preserve">
          <source>Ward is the most effective method for noisy data.</source>
          <target state="translated">Ward는 시끄러운 데이터에 가장 효과적인 방법입니다.</target>
        </trans-unit>
        <trans-unit id="e9c45563358e813f157ba81b33143542165ba84e" translate="yes" xml:space="preserve">
          <source>Warning</source>
          <target state="translated">Warning</target>
        </trans-unit>
        <trans-unit id="44d79cfceaac3d6c963709a9f443a7578dfdd3fa" translate="yes" xml:space="preserve">
          <source>Warning class used if there is an error while fitting the estimator.</source>
          <target state="translated">추정기를 피팅하는 동안 오류가 발생하면 경고 클래스가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="c56f46b7e83e71f0bcaf54dacd8cfc15c71ef274" translate="yes" xml:space="preserve">
          <source>Warning class used to notify the user of any change in the behavior.</source>
          <target state="translated">동작 변경 사항을 사용자에게 알리는 데 사용되는 경고 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="43a2ad1c4144ef567b3006486da55db4df5bcce7" translate="yes" xml:space="preserve">
          <source>Warning used to notify implicit data conversions happening in the code.</source>
          <target state="translated">코드에서 발생하는 암시 적 데이터 변환을 알리는 데 사용되는 경고입니다.</target>
        </trans-unit>
        <trans-unit id="69bb37448a64e3ca58327c210b042b57b19259a7" translate="yes" xml:space="preserve">
          <source>Warning used to notify the user of inefficient computation.</source>
          <target state="translated">비효율적 인 계산을 사용자에게 알리는 경고.</target>
        </trans-unit>
        <trans-unit id="95fde5bcc048210bdd2da0e9628c10dadee1ce1e" translate="yes" xml:space="preserve">
          <source>Warning used when the dot operation does not use BLAS.</source>
          <target state="translated">도트 작업에서 BLAS를 사용하지 않을 때 사용되는 경고입니다.</target>
        </trans-unit>
        <trans-unit id="77d4a9d6a0a46436c153d66828a62d378a7e1f5d" translate="yes" xml:space="preserve">
          <source>Warning used when the metric is invalid</source>
          <target state="translated">메트릭이 유효하지 않은 경우 사용되는 경고</target>
        </trans-unit>
        <trans-unit id="d0aaba5d13d0d7235d440530a46ccfa6343b56ff" translate="yes" xml:space="preserve">
          <source>Warning: Extra-trees should only be used within ensemble methods.</source>
          <target state="translated">경고 : 여분의 트리는 앙상블 방법 내에서만 사용해야합니다.</target>
        </trans-unit>
        <trans-unit id="c2eb6fdf9d13ab34884681ef0c66f41e16b52aad" translate="yes" xml:space="preserve">
          <source>Warning: this function is experimental and subject to change in a future version of joblib.</source>
          <target state="translated">경고 :이 기능은 실험용이며 이후 버전의 joblib에서 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b33d3bb4e4bfe5e80c2407f4ead429c7fa466ef0" translate="yes" xml:space="preserve">
          <source>We achieved 83.5% accuracy. Let&amp;rsquo;s see if we can do better with a linear &lt;a href=&quot;../../modules/svm#svm&quot;&gt;support vector machine (SVM)&lt;/a&gt;, which is widely regarded as one of the best text classification algorithms (although it&amp;rsquo;s also a bit slower than na&amp;iuml;ve Bayes). We can change the learner by simply plugging a different classifier object into our pipeline:</source>
          <target state="translated">83.5 %의 정확도를 달성했습니다. 최고의 텍스트 분류 알고리즘 중 하나로 널리 알려진 선형 &lt;a href=&quot;../../modules/svm#svm&quot;&gt;지원 벡터 머신 (SVM)을&lt;/a&gt; 사용하여 더 나은 작업을 수행 할 수 있는지 살펴 보겠습니다 ( Naive Bayes보다 약간 느리지 만). 다른 분류 자 ​​객체를 파이프 라인에 간단히 연결하여 학습자를 변경할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5e4234559a6f8eb7829d45ed1528d4d2b014e858" translate="yes" xml:space="preserve">
          <source>We achieved 91.3% accuracy using the SVM. &lt;code&gt;scikit-learn&lt;/code&gt; provides further utilities for more detailed performance analysis of the results:</source>
          <target state="translated">SVM을 사용하여 91.3 %의 정확도를 달성했습니다. &lt;code&gt;scikit-learn&lt;/code&gt; 은 결과의보다 자세한 성능 분석을위한 추가 유틸리티를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="4073bf855ec3741a8d17116f66549a3127ee1b52" translate="yes" xml:space="preserve">
          <source>We add observation noise to these waveforms. We generate very sparse noise: only 6% of the time points contain noise. As a result, the l1 norm of this noise (ie &amp;ldquo;cityblock&amp;rdquo; distance) is much smaller than it&amp;rsquo;s l2 norm (&amp;ldquo;euclidean&amp;rdquo; distance). This can be seen on the inter-class distance matrices: the values on the diagonal, that characterize the spread of the class, are much bigger for the Euclidean distance than for the cityblock distance.</source>
          <target state="translated">이 파형에 관측 노이즈를 추가합니다. 우리는 매우 드문 노이즈를 생성합니다. 6 %의 포인트 만 노이즈를 포함합니다. 결과적으로이 잡음의 l1 규범 (즉,&amp;ldquo;cityblock&amp;rdquo;거리)은 l2 규범 (&amp;ldquo;유클리드&amp;rdquo;거리)보다 훨씬 작습니다. 이것은 클래스 간 거리 행렬에서 볼 수 있습니다. 클래스의 확산을 특징으로하는 대각선의 값은 도시 블록 거리보다 유클리드 거리에서 훨씬 큽니다.</target>
        </trans-unit>
        <trans-unit id="5fc281c3e8b120a8bce90f392d5367731bdfa8e6" translate="yes" xml:space="preserve">
          <source>We also plot predictions and uncertainties for ARD for one dimensional regression using polynomial feature expansion. Note the uncertainty starts going up on the right side of the plot. This is because these test samples are outside of the range of the training samples.</source>
          <target state="translated">또한 다항식 확장을 사용하여 1 차원 회귀에 대한 ARD의 예측 및 불확실성을 플로팅합니다. 플롯의 오른쪽에서 불확실성이 증가하기 시작합니다. 이러한 테스트 샘플이 교육 샘플 범위를 벗어나기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="bda5a1ba58eab4f9acd36ef763104051f247918e" translate="yes" xml:space="preserve">
          <source>We also plot predictions and uncertainties for Bayesian Ridge Regression for one dimensional regression using polynomial feature expansion. Note the uncertainty starts going up on the right side of the plot. This is because these test samples are outside of the range of the training samples.</source>
          <target state="translated">또한 다항식 확장을 사용하여 1 차원 회귀에 대한 베이지안 릿지 회귀에 대한 예측 및 불확실성을 플로팅합니다. 플롯의 오른쪽에서 불확실성이 증가하기 시작합니다. 이러한 테스트 샘플이 교육 샘플 범위를 벗어나기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="b29f4bff482ccd99c1af96e146d78b516ea318aa" translate="yes" xml:space="preserve">
          <source>We also use warm_start=True which means that the coefficients of the models are reused to initialize the next model fit to speed-up the computation of the full-path.</source>
          <target state="translated">또한 warm_start = True를 사용합니다. 이는 전체 경로 계산 속도를 높이기 위해 다음 모델 적합을 초기화하기 위해 모델의 계수가 재사용됨을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="0f24896ee5efd7713cddd30b90821fa31665d4c0" translate="yes" xml:space="preserve">
          <source>We assume that the observations are independent and identically distributed (i.i.d.).</source>
          <target state="translated">관측치가 독립적이고 동일하게 분포되어 있다고 가정합니다 (iid).</target>
        </trans-unit>
        <trans-unit id="b19a456f973e90e2b4e21cdba9f00b15173fd0ef" translate="yes" xml:space="preserve">
          <source>We call &lt;strong&gt;vectorization&lt;/strong&gt; the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the &lt;strong&gt;Bag of Words&lt;/strong&gt; or &amp;ldquo;Bag of n-grams&amp;rdquo; representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.</source>
          <target state="translated">&lt;strong&gt;벡터화&lt;/strong&gt; 를 텍스트 문서 모음을 숫자 특징 벡터로 변환하는 일반적인 프로세스 라고 합니다. 이 특정 전략 (토큰 화, 카운팅 및 정규화) &lt;strong&gt;을 단어&lt;/strong&gt; 의 &lt;strong&gt;백&lt;/strong&gt; (Bas &lt;strong&gt;of Words)&lt;/strong&gt; 또는 &quot;N- 그램의 백&quot;표현이라고합니다. 문서는 단어에서 단어의 상대 위치 정보를 완전히 무시하면서 단어 발생으로 설명됩니다.</target>
        </trans-unit>
        <trans-unit id="096029ae48dbb08bf7baa8066a510ed0e5cf55ab" translate="yes" xml:space="preserve">
          <source>We can also predict based on an unfitted model by using the GP prior. In addition to the mean of the predictive distribution, also its standard deviation (return_std=True) or covariance (return_cov=True). Note that at most one of the two can be requested.</source>
          <target state="translated">GP를 사용하여 적합하지 않은 모델을 기반으로 예측할 수도 있습니다. 예측 분포의 평균 외에도 표준 편차 (return_std = True) 또는 공분산 (return_cov = True)입니다. 둘 중 하나만 요청할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7f60df7f123136fa11f4ddf222fab31a6a72d17d" translate="yes" xml:space="preserve">
          <source>We can choose &lt;code&gt;alpha&lt;/code&gt; to minimize left out error, this time using the diabetes dataset rather than our synthetic data:</source>
          <target state="translated">이번에는 합성 데이터 대신 당뇨병 데이터 세트를 사용하여 오류를 최소화하기 위해 &lt;code&gt;alpha&lt;/code&gt; 를 선택할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="469d4e6eabf44c56eec1620cbb6087744c30d3f6" translate="yes" xml:space="preserve">
          <source>We can clearly see that the median house price shows a linear relationship with the median income (top left) and that the house price drops when the avg. occupants per household increases (top middle). The top right plot shows that the house age in a district does not have a strong influence on the (median) house price; so does the average rooms per household. The tick marks on the x-axis represent the deciles of the feature values in the training data.</source>
          <target state="translated">우리는 평균 주택 가격이 평균 소득 (왼쪽 상단)과 선형 관계를 나타내고 평균 가격이 하락할 때 주택 가격이 하락 함을 분명히 알 수 있습니다. 가구당 거주자 증가 (중간 상단). 오른쪽 맨 위 줄거리는 지구의 주택 연령이 (중간) 주택 가격에 큰 영향을 미치지 않음을 보여줍니다. 가구당 평균 방도 마찬가지입니다. x 축의 눈금은 교육 데이터에있는 특성 값의 십진수를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="c32b4a200d58f731852c3f4a8eefb32ef18f31ed" translate="yes" xml:space="preserve">
          <source>We can keep the remaining rating columns by setting &lt;code&gt;remainder='passthrough'&lt;/code&gt;. The values are appended to the end of the transformation:</source>
          <target state="translated">&lt;code&gt;remainder='passthrough'&lt;/code&gt; 설정하여 나머지 등급 열을 유지할 수 있습니다 . 변환의 끝에 값이 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="a59ff6e4288992e31a9513b51da5a036927e8ec8" translate="yes" xml:space="preserve">
          <source>We can now load the list of files matching those categories as follows:</source>
          <target state="translated">이제 다음과 같이 해당 범주와 일치하는 파일 목록을로드 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f1f864cfbdff004081b5667459fd9c49d8fcf703" translate="yes" xml:space="preserve">
          <source>We can now quickly sample a training set while holding out 40% of the data for testing (evaluating) our classifier:</source>
          <target state="translated">이제 분류기를 테스트 (평가) 할 데이터의 40 %를 유지하면서 학습 세트를 신속하게 샘플링 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="850dcea5aea59348ecbdc161cc86fe56ad9b64b5" translate="yes" xml:space="preserve">
          <source>We can reduce the dimension even more, to a chosen \(L\), by projecting onto the linear subspace \(H_L\) which maximizes the variance of the \(\mu^*_k\) after projection (in effect, we are doing a form of PCA for the transformed class means \(\mu^*_k\)). This \(L\) corresponds to the &lt;code&gt;n_components&lt;/code&gt; parameter used in the &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.transform&lt;/code&gt;&lt;/a&gt; method. See &lt;a href=&quot;#id4&quot; id=&quot;id2&quot;&gt;[3]&lt;/a&gt; for more details.</source>
          <target state="translated">투영 후 \ (\ mu ^ * _ k \)의 분산을 최대화하는 선형 부분 공간 \ (H_L \)에 투영하여 선택한 \ (L \)로 치수를 훨씬 더 줄일 수 있습니다. 변환 된 클래스 수단 \ (\ mu ^ * _ k \))에 대해 PCA 양식을 수행하고 있습니다. 이 \ (L \) 는 &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.transform&lt;/code&gt; &lt;/a&gt; 메소드에 사용 된 &lt;code&gt;n_components&lt;/code&gt; 매개 변수에 해당합니다 . 자세한 내용은 &lt;a href=&quot;#id4&quot; id=&quot;id2&quot;&gt;[3]&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="bee45302ee9842b9e6d2e72404b6369d8b7e97cc" translate="yes" xml:space="preserve">
          <source>We can see that for low values of &lt;code&gt;n_components&lt;/code&gt; the distribution is wide with many distorted pairs and a skewed distribution (due to the hard limit of zero ratio on the left as distances are always positives) while for larger values of n_components the distortion is controlled and the distances are well preserved by the random projection.</source>
          <target state="translated">&lt;code&gt;n_components&lt;/code&gt; 의 낮은 값에 대해서는 분포가 왜곡 된 많은 쌍과 비대칭 분포 (거리가 항상 양수이므로 왼쪽의 0 비율의 하드 한계로 인해)가 넓고 n_components의 값이 클수록 왜곡이 제어되고 분포가 넓다는 것을 알 수 있습니다. 거리는 랜덤 투영에 의해 잘 보존됩니다.</target>
        </trans-unit>
        <trans-unit id="d51b2a4e76fb7eb99807202c5234812c15585e4c" translate="yes" xml:space="preserve">
          <source>We can see that if the maximum depth of the tree (controlled by the &lt;code&gt;max_depth&lt;/code&gt; parameter) is set too high, the decision trees learn too fine details of the training data and learn from the noise, i.e. they overfit.</source>
          <target state="translated">트리의 최대 깊이 ( &lt;code&gt;max_depth&lt;/code&gt; 매개 변수로 제어 )가 너무 높게 설정되어 있으면 의사 결정 트리가 훈련 데이터에 대해 너무 세밀한 정보를 배우고 소음으로부터 배우는 것, 즉 과적 합을 알 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5bc27bfd8c709ab5505734b2124db3dbd09e7af4" translate="yes" xml:space="preserve">
          <source>We can see that, although feature 2 has a strong coefficient on the full model, it conveys little information on &lt;code&gt;y&lt;/code&gt; when considered with feature 1.</source>
          <target state="translated">피처 2는 전체 모델에 대해 강한 계수를 갖지만 피처 1로 간주 될 때 &lt;code&gt;y&lt;/code&gt; 에 대한 정보를 거의 전달하지 않음 을 알 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="208e5f1f40787dd5dcd62a253128bd90d5a3414b" translate="yes" xml:space="preserve">
          <source>We can turn those concept as scores &lt;a href=&quot;generated/sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt;&lt;code&gt;homogeneity_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt;&lt;code&gt;completeness_score&lt;/code&gt;&lt;/a&gt;. Both are bounded below by 0.0 and above by 1.0 (higher is better):</source>
          <target state="translated">이러한 개념을 &lt;a href=&quot;generated/sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt; &lt;code&gt;homogeneity_score&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt; &lt;code&gt;completeness_score&lt;/code&gt; &lt;/a&gt; 점수로 전환 할 수 있습니다 . 둘 다 0.0과 1.0 이상으로 제한됩니다 (높을수록 좋음).</target>
        </trans-unit>
        <trans-unit id="592289b1a6fdac7a6256bbd4a62b88701d16e505" translate="yes" xml:space="preserve">
          <source>We can use the function &lt;a href=&quot;generated/sklearn.model_selection.learning_curve#sklearn.model_selection.learning_curve&quot;&gt;&lt;code&gt;learning_curve&lt;/code&gt;&lt;/a&gt; to generate the values that are required to plot such a learning curve (number of samples that have been used, the average scores on the training sets and the average scores on the validation sets):</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.learning_curve#sklearn.model_selection.learning_curve&quot;&gt; &lt;code&gt;learning_curve&lt;/code&gt; &lt;/a&gt; 함수를 사용하여 이러한 학습 곡선 (사용 된 샘플 수, 훈련 세트의 평균 점수 및 유효성 검사 세트의 평균 점수)을 표시하는 데 필요한 값을 생성 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="3c55a99ecafce9d11edefa5f7981438b525c546b" translate="yes" xml:space="preserve">
          <source>We classify 8x8 images of digits into two classes: 0-4 against 5-9. The visualization shows coefficients of the models for varying C.</source>
          <target state="translated">우리는 숫자의 8x8 이미지를 두 가지 클래스로 분류합니다 : 0-4 대 5-9. 시각화는 다양한 C에 대한 모델의 계수를 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="4c2ceff57e3b51d317fe78664f4ea8312ed35966" translate="yes" xml:space="preserve">
          <source>We consider 3 features x_1, x_2, x_3 distributed uniformly over [0, 1], the target depends on them as follows:</source>
          <target state="translated">[0, 1]에 균일하게 분포 된 3 가지 기능 x_1, x_2, x_3을 고려합니다. 대상은 다음과 같이 기능에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="8b738423194ebf355d81c34c5848e446f6f25c86" translate="yes" xml:space="preserve">
          <source>We create a multi-label dataset, to illustrate the precision-recall in multi-label settings</source>
          <target state="translated">우리는 다중 레이블 설정에서 정밀도 기억을 설명하기 위해 다중 레이블 데이터 세트를 만듭니다.</target>
        </trans-unit>
        <trans-unit id="7df3a30efca0a13e3a362147c9be1fa02a3e02fd" translate="yes" xml:space="preserve">
          <source>We don&amp;rsquo;t allow:</source>
          <target state="translated">우리는 허용하지 않습니다 :</target>
        </trans-unit>
        <trans-unit id="c17cba12ad6e5c2f931d1de0fcadd712e7bb0a0a" translate="yes" xml:space="preserve">
          <source>We first find the separating plane with a plain SVC and then plot (dashed) the separating hyperplane with automatically correction for unbalanced classes.</source>
          <target state="translated">먼저 평범한 SVC로 분리 평면을 찾은 다음 불균형 클래스를 자동으로 수정하여 분리 초평면을 플로팅 (대시)합니다.</target>
        </trans-unit>
        <trans-unit id="916afde7457af1caed530318ff87e1a7a139918e" translate="yes" xml:space="preserve">
          <source>We found that &lt;code&gt;max_leaf_nodes=k&lt;/code&gt; gives comparable results to &lt;code&gt;max_depth=k-1&lt;/code&gt; but is significantly faster to train at the expense of a slightly higher training error. The parameter &lt;code&gt;max_leaf_nodes&lt;/code&gt; corresponds to the variable &lt;code&gt;J&lt;/code&gt; in the chapter on gradient boosting in &lt;a href=&quot;#f2001&quot; id=&quot;id14&quot;&gt;[F2001]&lt;/a&gt; and is related to the parameter &lt;code&gt;interaction.depth&lt;/code&gt; in R&amp;rsquo;s gbm package where &lt;code&gt;max_leaf_nodes == interaction.depth + 1&lt;/code&gt; .</source>
          <target state="translated">우리는 그 발견 &lt;code&gt;max_leaf_nodes=k&lt;/code&gt; 비교 결과를 제공 &lt;code&gt;max_depth=k-1&lt;/code&gt; 하지만, 약간 높은 훈련 오류의 비용으로 훨씬 빠르게 양성하는 것입니다. 파라미터 &lt;code&gt;max_leaf_nodes&lt;/code&gt; 의 변수에 대응하는 &lt;code&gt;J&lt;/code&gt; 의 증대 구배의 장 &lt;a href=&quot;#f2001&quot; id=&quot;id14&quot;&gt;[F2001]&lt;/a&gt; 과는 파라미터 관련된 &lt;code&gt;interaction.depth&lt;/code&gt; R의 GBM 패키지 여기서 &lt;code&gt;max_leaf_nodes == interaction.depth + 1&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="72f7189dc223c470430f67b7332d9ced78363339" translate="yes" xml:space="preserve">
          <source>We found that Averaged SGD works best with a larger number of features and a higher eta0</source>
          <target state="translated">평균 SGD가 더 많은 수의 기능과 더 높은 eta0에서 가장 잘 작동한다는 것을 알았습니다.</target>
        </trans-unit>
        <trans-unit id="ecc641e4f0c3be544f8ba1b6978d12b64c778e22" translate="yes" xml:space="preserve">
          <source>We generate data from three groups of waveforms. Two of the waveforms (waveform 1 and waveform 2) are proportional one to the other. The cosine distance is invariant to a scaling of the data, as a result, it cannot distinguish these two waveforms. Thus even with no noise, clustering using this distance will not separate out waveform 1 and 2.</source>
          <target state="translated">세 그룹의 파형에서 데이터를 생성합니다. 두 파형 (파형 1 및 파형 2)은 서로 비례합니다. 코사인 거리는 데이터의 스케일링에 불변하므로 결과적으로이 두 파형을 구별 할 수 없습니다. 따라서 노이즈가 없어도이 거리를 사용하는 클러스터링은 파형 1과 2를 분리하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="03835cacd7901d07f747c14f209b80543758c4fd" translate="yes" xml:space="preserve">
          <source>We have seen that some estimators can transform data and that some estimators can predict variables. We can also create combined estimators:</source>
          <target state="translated">우리는 일부 추정자가 데이터를 변환 할 수 있고 일부 추정기는 변수를 예측할 수 있음을 확인했습니다. 결합 된 추정량을 생성 할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="996013b3c282443801da622c65fee1deca3cef01" translate="yes" xml:space="preserve">
          <source>We have seen that sparsity could be used to mitigate the curse of dimensionality, &lt;em&gt;i.e&lt;/em&gt; an insufficient amount of observations compared to the number of features. Another approach is to merge together similar features: &lt;strong&gt;feature agglomeration&lt;/strong&gt;. This approach can be implemented by clustering in the feature direction, in other words clustering the transposed data.</source>
          <target state="translated">우리는 희소성이 차원의 저주, &lt;em&gt;즉&lt;/em&gt; 특징의 수에 비해 불충분 한 관측치 의 저주를 완화하는 데 사용될 수 있음을 보았습니다 . 또 다른 접근법은 비슷한 기능을 통합하는 것입니다 : &lt;strong&gt;기능 응집&lt;/strong&gt; . 이 접근법은 특징 방향으로 클러스터링, 즉, 전치 된 데이터를 클러스터링함으로써 구현 될 수있다.</target>
        </trans-unit>
        <trans-unit id="f70b46435230cf70b0b3d749ceef620b9bcdee9d" translate="yes" xml:space="preserve">
          <source>We have specifically abstained from an optimization used by authors of both papers, a QR decomposition used in specific situations to reduce the algorithmic complexity of the SVD. The source for this technique is &lt;code&gt;Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5, section 5.4.4, pp 252-253.&lt;/code&gt;. This technique has been omitted because it is advantageous only when decomposing a matrix with &lt;code&gt;n_samples&lt;/code&gt; (rows) &amp;gt;= 5/3 * &lt;code&gt;n_features&lt;/code&gt; (columns), and hurts the readability of the implemented algorithm. This would be a good opportunity for future optimization, if it is deemed necessary.</source>
          <target state="translated">SVD의 알고리즘 복잡성을 줄이기 위해 특정 상황에서 사용되는 QR 분해는 두 논문의 저자가 사용하는 최적화를 구체적으로 생략했습니다. 이 기술의 출처는 &lt;code&gt;Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5, section 5.4.4, pp 252-253.&lt;/code&gt; . 이 기법은 &lt;code&gt;n_samples&lt;/code&gt; (행)&amp;gt; = 5/3 * &lt;code&gt;n_features&lt;/code&gt; (열) 로 행렬을 분해 할 때만 유리 하고 구현 된 알고리즘의 가독성을 손상시키기 때문에 생략되었습니다 . 필요할 경우 향후 최적화를위한 좋은 기회가 될 것입니다.</target>
        </trans-unit>
        <trans-unit id="db298eced453f06f8efed43be73a03415c98ca01" translate="yes" xml:space="preserve">
          <source>We have to reconstruct model and parameters to make sure we stay in sync with the python object.</source>
          <target state="translated">파이썬 객체와 동기화되도록 모델과 파라미터를 재구성해야합니다.</target>
        </trans-unit>
        <trans-unit id="f2683785e1f3e3ccb40d3941fed7d5fa67a33cfc" translate="yes" xml:space="preserve">
          <source>We introduce a new parameter \(\nu\) which controls the number of support vectors and training errors. The parameter \(\nu \in (0, 1]\) is an upper bound on the fraction of training errors and a lower bound of the fraction of support vectors.</source>
          <target state="translated">지원 벡터 수와 학습 오류를 제어하는 ​​새로운 매개 변수 \ (\ nu \)를 소개합니다. 매개 변수 \ (\ nu \ in (0, 1] \)는 훈련 오류의 부분에 대한 상한과 지원 벡터의 부분에 대한 하한입니다.</target>
        </trans-unit>
        <trans-unit id="284f00654c2880797bacd24e85e3bc2f5ec8be8d" translate="yes" xml:space="preserve">
          <source>We no longer get the collisions, but this comes at the expense of a much larger dimensionality of the output space. Of course, other terms than the 19 used here might still collide with each other.</source>
          <target state="translated">우리는 더 이상 충돌을 일으키지 않지만, 이는 출력 공간의 훨씬 더 큰 차원 성을 희생시킵니다. 물론 여기서 사용 된 19 이외의 용어는 여전히 서로 충돌 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0e7a6d4d2e128e719f76bf1799c4515162612bfb" translate="yes" xml:space="preserve">
          <source>We observe a tendency towards clearer shapes as the preplexity value increases.</source>
          <target state="translated">우리는 preplexity 값이 증가함에 따라 더 명확한 모양으로 향하는 경향을 관찰합니다.</target>
        </trans-unit>
        <trans-unit id="b4e7d2188aa62b346706f6bd2a1ae94095756883" translate="yes" xml:space="preserve">
          <source>We plot predicted labels on both training and held out test data using a variety of GMM covariance types on the iris dataset. We compare GMMs with spherical, diagonal, full, and tied covariance matrices in increasing order of performance. Although one would expect full covariance to perform best in general, it is prone to overfitting on small datasets and does not generalize well to held out test data.</source>
          <target state="translated">우리는 훈련에 예측 된 레이블을 표시하고 홍채 데이터 세트에서 다양한 GMM 공분산 유형을 사용하여 테스트 데이터를 실시했습니다. GMM을 구형, 대각선, 전체 및 묶인 공분산 행렬과 비교하여 성능 순서를 향상시킵니다. 전체 공분산이 일반적으로 최고 성능을 기대할 수 있지만 소규모 데이터 세트에 과적 합하기 쉽고 테스트 데이터를 잘 나타내기에는 일반화되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="8a444360e57dc0870f14b3c959a7b626922e7571" translate="yes" xml:space="preserve">
          <source>We see that &lt;code&gt;SVC&lt;/code&gt; doesn&amp;rsquo;t do much better than a dummy classifier. Now, let&amp;rsquo;s change the kernel:</source>
          <target state="translated">우리는 &lt;code&gt;SVC&lt;/code&gt; 가 더미 분류기보다 낫지 않다는 것을 알았습니다. 이제 커널을 바꾸자 :</target>
        </trans-unit>
        <trans-unit id="9619f66671fbeb88bbee2c1b3d850c22bdb6ab25" translate="yes" xml:space="preserve">
          <source>We see that the accuracy was boosted to almost 100%. A cross validation strategy is recommended for a better estimate of the accuracy, if it is not too CPU costly. For more information see the &lt;a href=&quot;cross_validation#cross-validation&quot;&gt;Cross-validation: evaluating estimator performance&lt;/a&gt; section. Moreover if you want to optimize over the parameter space, it is highly recommended to use an appropriate methodology; see the &lt;a href=&quot;grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt; section for details.</source>
          <target state="translated">정확도가 거의 100 %로 향상되었습니다. CPU가 너무 비싸지 않은 경우 정확도를 더 잘 추정하려면 교차 검증 전략이 권장됩니다. 자세한 정보는 &lt;a href=&quot;cross_validation#cross-validation&quot;&gt;교차 유효성 검증 : 추정기 성능 평가&lt;/a&gt; 섹션을 참조하십시오. 또한 매개 변수 공간을 최적화하려면 적절한 방법을 사용하는 것이 좋습니다. 자세한 내용은 &lt;a href=&quot;grid_search#grid-search&quot;&gt;추정기의 하이퍼 파라미터 튜닝&lt;/a&gt; 섹션을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="d680bb4e5101fe3a9df93911d26cfa982767e679" translate="yes" xml:space="preserve">
          <source>We see that the resulting &lt;em&gt;polynomial regression&lt;/em&gt; is in the same class of linear models we&amp;rsquo;d considered above (i.e. the model is linear in \(w\)) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data.</source>
          <target state="translated">결과 &lt;em&gt;다항식 회귀 분석&lt;/em&gt; 은 위에서 고려한 것과 같은 클래스의 선형 모델에 있으며 (즉, 모델은 \ (w \)에서 선형 임) 동일한 기술로 해결할 수 있습니다. 이러한 기본 기능으로 구축 된 고차원 공간 내에서 선형 적합을 고려함으로써 모델은 훨씬 더 넓은 범위의 데이터에 적합하게 유연성을 갖습니다.</target>
        </trans-unit>
        <trans-unit id="1a0e7af8231b7a460dcfd9acf44c6323ad1ea844" translate="yes" xml:space="preserve">
          <source>We selected two sets of two variables from the Boston housing data set as an illustration of what kind of analysis can be done with several outlier detection tools. For the purpose of visualization, we are working with two-dimensional examples, but one should be aware that things are not so trivial in high-dimension, as it will be pointed out.</source>
          <target state="translated">몇 가지 이상치 탐지 도구를 사용하여 어떤 종류의 분석을 수행 할 수 있는지 설명하기 위해 Boston Housing 데이터 세트에서 두 가지 변수의 두 세트를 선택했습니다. 시각화의 목적으로, 우리는 2 차원 예제로 작업하고 있지만, 지적 된 바와 같이, 사물이 고차원에서는 그리 사소한 것이 아님을 알아야합니다.</target>
        </trans-unit>
        <trans-unit id="940e7a9289cb117779f9b0e089fc6f41ec456057" translate="yes" xml:space="preserve">
          <source>We should also note that small differences in scores results from the random splits of the cross-validation procedure. Those spurious variations can be smoothed out by increasing the number of CV iterations &lt;code&gt;n_splits&lt;/code&gt; at the expense of compute time. Increasing the value number of &lt;code&gt;C_range&lt;/code&gt; and &lt;code&gt;gamma_range&lt;/code&gt; steps will increase the resolution of the hyper-parameter heat map.</source>
          <target state="translated">또한 점수의 작은 차이는 교차 유효성 검사 절차의 무작위 분할로 인해 발생합니다. 계산 시간을 희생하면서 CV 반복 횟수 &lt;code&gt;n_splits&lt;/code&gt; 의 수를 늘려 이러한 스퓨리어스 변형을 완화 할 수 있습니다 . &lt;code&gt;C_range&lt;/code&gt; 및 &lt;code&gt;gamma_range&lt;/code&gt; 단계 의 값 수를 늘리면 하이퍼 파라미터 히트 맵의 해상도가 높아집니다.</target>
        </trans-unit>
        <trans-unit id="1cbc5d306e08afa1d68b1045fc6a567611cf26d2" translate="yes" xml:space="preserve">
          <source>We show that linear_model.Lasso provides the same results for dense and sparse data and that in the case of sparse data the speed is improved.</source>
          <target state="translated">linear_model.Lasso는 밀도가 높고 희소 데이터에 대해 동일한 결과를 제공하며 희소 데이터의 경우 속도가 향상됨을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="efbf964638e7ca0fbdc3e1bd2a5029a9cbed6b8c" translate="yes" xml:space="preserve">
          <source>We start by training a label propagation model with only 10 labeled points, then we select the top five most uncertain points to label. Next, we train with 15 labeled points (original 10 + 5 new ones). We repeat this process four times to have a model trained with 30 labeled examples. Note you can increase this to label more than 30 by changing &lt;code&gt;max_iterations&lt;/code&gt;. Labeling more than 30 can be useful to get a sense for the speed of convergence of this active learning technique.</source>
          <target state="translated">레이블이 지정된 10 개의 포인트만으로 레이블 전파 모델을 학습 한 다음 레이블을 지정할 가장 불확실한 상위 5 개 포인트를 선택합니다. 다음으로 15 개의 레이블이있는 포인트 (원래 10 + 5 개의 새로운 포인트)로 훈련합니다. 이 프로세스를 4 번 반복하여 30 개의 레이블이있는 예제로 훈련 된 모델을 만듭니다. &lt;code&gt;max_iterations&lt;/code&gt; 를 변경 하여이 값을 30 이상으로 늘리도록 늘릴 수 있습니다 . 30 개가 넘는 라벨링은이 능동 학습 기술의 수렴 속도를 이해하는 데 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b926aa4a7c89ca684b4dc714781356e70fac60ed" translate="yes" xml:space="preserve">
          <source>We thus transform the KDD Data set into two different data sets: SA and SF.</source>
          <target state="translated">따라서 KDD 데이터 세트를 SA와 SF의 두 가지 데이터 세트로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="13122bf873819e99a4ec7d32c926bbee95474f9b" translate="yes" xml:space="preserve">
          <source>We use a GridSearchCV to set the dimensionality of the PCA</source>
          <target state="translated">PCA의 차원을 설정하기 위해 GridSearchCV를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="83c28baded6fcaa48849ef740f04075945b33344" translate="yes" xml:space="preserve">
          <source>We use clustering to group together quotes that behave similarly. Here, amongst the &lt;a href=&quot;../../modules/clustering#clustering&quot;&gt;various clustering techniques&lt;/a&gt; available in the scikit-learn, we use &lt;a href=&quot;../../modules/clustering#affinity-propagation&quot;&gt;Affinity Propagation&lt;/a&gt; as it does not enforce equal-size clusters, and it can choose automatically the number of clusters from the data.</source>
          <target state="translated">클러스터링을 사용하여 유사하게 동작하는 따옴표를 그룹화합니다. 여기서 scikit-learn에서 사용할 수 있는 &lt;a href=&quot;../../modules/clustering#clustering&quot;&gt;다양한 클러스터링 기술&lt;/a&gt; 중에서 &lt;a href=&quot;../../modules/clustering#affinity-propagation&quot;&gt;동질&lt;/a&gt; 크기의 클러스터를 적용하지 않으므로 Affinity Propagation 을 사용 하며 데이터에서 클러스터 수를 자동으로 선택할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="78d7bc0e66fad92fa188a5bbaa0a7aaa581101c6" translate="yes" xml:space="preserve">
          <source>We use sparse inverse covariance estimation to find which quotes are correlated conditionally on the others. Specifically, sparse inverse covariance gives us a graph, that is a list of connection. For each symbol, the symbols that it is connected too are those useful to explain its fluctuations.</source>
          <target state="translated">희소 역공 분산 추정을 사용하여 어떤 따옴표가 다른 따옴표와 조건 적으로 상관되어 있는지 찾습니다. 특히 희소 역공 분산은 그래프를 보여줍니다. 이는 연결 목록입니다. 각 심볼에 대해 연결된 심볼도 변동을 설명하는 데 유용합니다.</target>
        </trans-unit>
        <trans-unit id="4e3f49993eefd3c100d45584ffc552355d0d52e7" translate="yes" xml:space="preserve">
          <source>We validate the above bounds on the digits dataset or on the 20 newsgroups text document (TF-IDF word frequencies) dataset:</source>
          <target state="translated">숫자 데이터 세트 또는 20 개의 뉴스 그룹 텍스트 문서 (TF-IDF 단어 빈도) 데이터 세트에서 위의 경계를 검증합니다.</target>
        </trans-unit>
        <trans-unit id="4bd2e73735072e414ab7c853666f3b851cc359c1" translate="yes" xml:space="preserve">
          <source>We want to compare the performance of the MiniBatchKMeans and KMeans: the MiniBatchKMeans is faster, but gives slightly different results (see &lt;a href=&quot;../../modules/clustering#mini-batch-kmeans&quot;&gt;Mini Batch K-Means&lt;/a&gt;).</source>
          <target state="translated">MiniBatchKMeans와 KMeans의 성능을 비교하려고합니다. MiniBatchKMeans는 빠르지 만 약간 다른 결과를 제공합니다 ( &lt;a href=&quot;../../modules/clustering#mini-batch-kmeans&quot;&gt;Mini Batch K-Means 참조&lt;/a&gt; ).</target>
        </trans-unit>
        <trans-unit id="b8ae1ec8b6dd301757e7e59be0a9aeff645f7f18" translate="yes" xml:space="preserve">
          <source>We will cluster a set of data, first with KMeans and then with MiniBatchKMeans, and plot the results. We will also plot the points that are labelled differently between the two algorithms.</source>
          <target state="translated">먼저 KMeans와 MiniBatchKMeans를 사용하여 일련의 데이터를 클러스터링하고 결과를 플로팅합니다. 또한 두 알고리즘간에 다르게 레이블이 지정된 점을 플로팅합니다.</target>
        </trans-unit>
        <trans-unit id="b4cfcf9a2f9d6095c0707be11ca996fe9017bf9d" translate="yes" xml:space="preserve">
          <source>We will probably have to use an estimator or a parametrization of the current estimator that can learn more complex concepts (i.e. has a lower bias). If the training score is much greater than the validation score for the maximum number of training samples, adding more training samples will most likely increase generalization. In the following plot you can see that the SVM could benefit from more training examples.</source>
          <target state="translated">더 복잡한 개념을 배울 수있는 (예를 들어, 편향이 더 낮은) 추정기 또는 현재 추정기의 매개 변수를 사용해야 할 것입니다. 훈련 점수가 최대 훈련 샘플 수에 대한 검증 점수보다 훨씬 큰 경우, 훈련 샘플을 더 추가하면 일반화가 증가 할 가능성이 높습니다. 다음 그림에서 SVM이 더 많은 교육 예제를 통해 이점을 얻을 수 있음을 알 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3f09acb611dde4d0822059b7dd910a6bf0d4be22" translate="yes" xml:space="preserve">
          <source>We will review here the orders of magnitude you can expect from a number of scikit-learn estimators in different contexts and provide some tips and tricks for overcoming performance bottlenecks.</source>
          <target state="translated">여기에서는 다양한 상황에서 여러 가지 scikit-learn 추정기에서 기대할 수있는 정도의 순서를 검토하고 성능 병목 현상을 극복하기위한 몇 가지 팁과 요령을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="5ab95440492dec088ab7d086a3622b86acadc7e7" translate="yes" xml:space="preserve">
          <source>We&amp;rsquo;ll define a function that lets us visualize the behavior of each cross-validation object. We&amp;rsquo;ll perform 4 splits of the data. On each split, we&amp;rsquo;ll visualize the indices chosen for the training set (in blue) and the test set (in red).</source>
          <target state="translated">각 교차 유효성 검사 개체의 동작을 시각화 할 수있는 함수를 정의하겠습니다. 우리는 4 개의 데이터 분할을 수행 할 것입니다. 각 스플릿마다 트레이닝 세트 (파란색)와 테스트 세트 (빨간색)에 선택된 인덱스를 시각화합니다.</target>
        </trans-unit>
        <trans-unit id="70c023ae490fc71ae664cdd80527ab708a5db4b9" translate="yes" xml:space="preserve">
          <source>We&amp;rsquo;ve already encountered some parameters such as &lt;code&gt;use_idf&lt;/code&gt; in the &lt;code&gt;TfidfTransformer&lt;/code&gt;. Classifiers tend to have many parameters as well; e.g., &lt;code&gt;MultinomialNB&lt;/code&gt; includes a smoothing parameter &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;SGDClassifier&lt;/code&gt; has a penalty parameter &lt;code&gt;alpha&lt;/code&gt; and configurable loss and penalty terms in the objective function (see the module documentation, or use the Python &lt;code&gt;help&lt;/code&gt; function to get a description of these).</source>
          <target state="translated">우리는 이미 같은 일부 매개 변수가 발생했습니다 &lt;code&gt;use_idf&lt;/code&gt; 에서 &lt;code&gt;TfidfTransformer&lt;/code&gt; . 분류 기에도 많은 매개 변수가 있습니다. 예를 들면, &lt;code&gt;MultinomialNB&lt;/code&gt; 는 스무딩 파라미터 포함 &lt;code&gt;alpha&lt;/code&gt; 와 &lt;code&gt;SGDClassifier&lt;/code&gt; 는 페널티 매개 변수가 &lt;code&gt;alpha&lt;/code&gt; 목적 함수와 구성 손실과 처벌 조건 (모듈 설명서를 참조하거나 파이썬 사용 &lt;code&gt;help&lt;/code&gt; 이들의 설명을 얻을 기능).</target>
        </trans-unit>
        <trans-unit id="96df76d7fa199e301349be570d5ef4d0bb6a7f3d" translate="yes" xml:space="preserve">
          <source>Weight given to each sample.</source>
          <target state="translated">각 샘플에 주어진 무게.</target>
        </trans-unit>
        <trans-unit id="a7a3ce3e7a16ef99378bfef64690454462da25e9" translate="yes" xml:space="preserve">
          <source>Weight matrix, where n_features in the number of visible units and n_components is the number of hidden units.</source>
          <target state="translated">가중치 행렬 (여기서 표시 단위 수의 n_features 및 n_components는 숨겨진 단위 수)입니다.</target>
        </trans-unit>
        <trans-unit id="d0664e46a183d0a2a2e3afcbc3b6c5ba30a9d4ef" translate="yes" xml:space="preserve">
          <source>Weight of each sample, such that a sample with a weight of at least &lt;code&gt;min_samples&lt;/code&gt; is by itself a core sample; a sample with negative weight may inhibit its eps-neighbor from being core. Note that weights are absolute, and default to 1.</source>
          <target state="translated">적어도 &lt;code&gt;min_samples&lt;/code&gt; 의 중량을 갖는 샘플이 그 자체가 핵심 샘플이되도록 각 샘플의 중량 ; 중량이 음수 인 샘플은 EPS 이웃이 코어가되지 않을 수 있습니다. 가중치는 절대 값이며 기본값은 1입니다.</target>
        </trans-unit>
        <trans-unit id="5cc536fd8cf249ec4c2e295665e0070f1b9cec67" translate="yes" xml:space="preserve">
          <source>Weight of precision in harmonic mean.</source>
          <target state="translated">조화 평균의 정밀도 가중치.</target>
        </trans-unit>
        <trans-unit id="6cd90e03c276712f974984f65620519bcea49500" translate="yes" xml:space="preserve">
          <source>Weight vector(s).</source>
          <target state="translated">무게 벡터.</target>
        </trans-unit>
        <trans-unit id="ac0d2c9a738f9c54a5d208ddac8f22019ff9c627" translate="yes" xml:space="preserve">
          <source>Weight, Waist and Pulse.</source>
          <target state="translated">체중, 허리 및 맥박.</target>
        </trans-unit>
        <trans-unit id="c74e4e7c5caf95682fb65872b5814741f06c7fac" translate="yes" xml:space="preserve">
          <source>Weighted average</source>
          <target state="translated">가중 평균</target>
        </trans-unit>
        <trans-unit id="39392047d0260b6fc1e042825fdae9116d630e28" translate="yes" xml:space="preserve">
          <source>Weighted average probability for each class per sample.</source>
          <target state="translated">샘플 당 각 클래스의 가중 평균 확률.</target>
        </trans-unit>
        <trans-unit id="9a702ae7f12a23bf0cde9786ae821e6b340d4991" translate="yes" xml:space="preserve">
          <source>Weights applied to individual samples (1. for unweighted).</source>
          <target state="translated">개별 샘플에 적용되는 분동 (무가 중의 경우 1.).</target>
        </trans-unit>
        <trans-unit id="0c68217fe30f051f7b997da07ad569653cfdb104" translate="yes" xml:space="preserve">
          <source>Weights applied to individual samples. If not provided, uniform weights are assumed.</source>
          <target state="translated">개별 샘플에 적용된 분동. 제공하지 않으면 균일 한 가중치가 가정됩니다.</target>
        </trans-unit>
        <trans-unit id="3070fe087be2b6fbe15f65d4db644297c1112686" translate="yes" xml:space="preserve">
          <source>Weights applied to individual samples. If not provided, uniform weights are assumed. These weights will be multiplied with class_weight (passed through the constructor) if class_weight is specified</source>
          <target state="translated">개별 샘플에 적용된 분동. 제공하지 않으면 균일 한 가중치가 가정됩니다. class_weight를 지정하면이 가중치에 class_weight (생성자를 통해 전달됨)가 곱해집니다.</target>
        </trans-unit>
        <trans-unit id="86b1d5826d4836f8e129b6346c9e9e60976aafee" translate="yes" xml:space="preserve">
          <source>Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel.</source>
          <target state="translated">피쳐에 지정된 가중치 (원초 문제의 계수). 이것은 선형 커널의 경우에만 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4b149f5e057b5bff95048ebeff46bfac0e7a368d" translate="yes" xml:space="preserve">
          <source>Weights assigned to the features.</source>
          <target state="translated">기능에 할당 된 가중치.</target>
        </trans-unit>
        <trans-unit id="8064bf8d5d6b0c15f7ed813823cc0da43a8bc7e6" translate="yes" xml:space="preserve">
          <source>Weights associated with classes in the form &lt;code&gt;{class_label: weight}&lt;/code&gt;. If not given, all classes are supposed to have weight one.</source>
          <target state="translated">&lt;code&gt;{class_label: weight}&lt;/code&gt; 형식의 클래스와 연관된 가중치 입니다. 주어지지 않으면 모든 수업은 1을가집니다.</target>
        </trans-unit>
        <trans-unit id="96f3238c530c2df09403ab213fee9515feb36c99" translate="yes" xml:space="preserve">
          <source>Weights associated with classes in the form &lt;code&gt;{class_label: weight}&lt;/code&gt;. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.</source>
          <target state="translated">&lt;code&gt;{class_label: weight}&lt;/code&gt; 형식의 클래스와 연관된 가중치 입니다. 주어지지 않으면 모든 수업은 1을가집니다. 다중 출력 문제의 경우, y 열과 동일한 순서로 dict 목록을 제공 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4a87f3dd4dfb432aa1a51752552e165c9cc23301" translate="yes" xml:space="preserve">
          <source>Weights associated with classes. If not given, all classes are supposed to have weight one.</source>
          <target state="translated">클래스와 관련된 가중치. 주어지지 않으면 모든 수업은 1을가집니다.</target>
        </trans-unit>
        <trans-unit id="4837ab63e8195a91fca82bbd82590df1bbe7fcc4" translate="yes" xml:space="preserve">
          <source>Weights for each estimator in the boosted ensemble.</source>
          <target state="translated">부스트 앙상블의 각 추정기에 대한 가중치.</target>
        </trans-unit>
        <trans-unit id="55ddc90a49d39d4b40d722b720afa82441019829" translate="yes" xml:space="preserve">
          <source>Weights on each point of the regression. If None, weight is set to 1 (equal weights).</source>
          <target state="translated">회귀의 각 지점에 대한 가중치. 없음 인 경우 가중치는 1 (동일 가중치)로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="0946f675292deb36a2ff9f4a33806ccd9e9e1833" translate="yes" xml:space="preserve">
          <source>Weights. If set to None, all weights will be set to 1 (equal weights).</source>
          <target state="translated">무게. 없음으로 설정하면 모든 가중치가 1 (동일 가중치)로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="c1f521c553b00dab458900dd1fb3f949a6ba99ab" translate="yes" xml:space="preserve">
          <source>Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level. For instance a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a predict_proba value close to 0.8, approx. 80% actually belong to the positive class.</source>
          <target state="translated">잘 교정 된 분류기는 확률 적 분류기이며 predict_proba 메소드의 출력을 신뢰 수준으로 직접 해석 할 수 있습니다. 예를 들어, 잘 보정 된 (이진) 분류기는 샘플에 대해 predict_proba 값을 제공 한 샘플 중 대략 0.8에 가깝도록 샘플을 분류해야합니다. 실제로 80 %는 긍정적 인 계층에 속합니다.</target>
        </trans-unit>
        <trans-unit id="6db2509535d857954c618d97c17994b5d42574ae" translate="yes" xml:space="preserve">
          <source>Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level. For instance, a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a predict_proba value close to 0.8, approximately 80% actually belong to the positive class. The following plot compares how well the probabilistic predictions of different classifiers are calibrated:</source>
          <target state="translated">잘 교정 된 분류기는 확률 적 분류기이며 predict_proba 메소드의 출력을 신뢰 수준으로 직접 해석 할 수 있습니다. 예를 들어, 잘 보정 된 (이진) 분류기는 샘플에 대해 predict_proba 값이 0.8에 가까운 샘플 중 약 80 %가 실제로 양의 클래스에 속하도록 샘플을 분류해야합니다. 다음 그림은 여러 분류기의 확률 론적 예측이 얼마나 잘 교정되었는지 비교합니다.</target>
        </trans-unit>
        <trans-unit id="830bc34728ca0799f710b4883d58631c6dfded76" translate="yes" xml:space="preserve">
          <source>Wether to include meta-estimators that are somehow special and can not be default-constructed sensibly. These are currently Pipeline, FeatureUnion and GridSearchCV</source>
          <target state="translated">어쨌든 특별하고 현명하게 기본 구성이 불가능한 메타 추정기를 포함시켜야합니다. 이들은 현재 파이프 라인, FeatureUnion 및 GridSearchCV입니다.</target>
        </trans-unit>
        <trans-unit id="d4f157bc9962e4b0dc2a197ed14e50902555d749" translate="yes" xml:space="preserve">
          <source>What are all the various decision tree algorithms and how do they differ from each other? Which one is implemented in scikit-learn?</source>
          <target state="translated">다양한 의사 결정 트리 알고리즘은 무엇이며 서로 어떻게 다른가요? scikit-learn에서 어느 것이 구현됩니까?</target>
        </trans-unit>
        <trans-unit id="a1f5f9cd3d06157b8582b1ca4000a5bc395e765a" translate="yes" xml:space="preserve">
          <source>What this example shows us is the behavior &amp;ldquo;rich getting richer&amp;rdquo; of agglomerative clustering that tends to create uneven cluster sizes. This behavior is pronounced for the average linkage strategy, that ends up with a couple of singleton clusters, while in the case of single linkage we get a single central cluster with all other clusters being drawn from noise points around the fringes.</source>
          <target state="translated">이 예에서 우리가 보여주는 것은 고르지 않은 클러스터 크기를 만드는 경향이있는 응집 클러스터링의 &quot;풍부 해짐&quot;동작입니다. 이 동작은 평균 연결 전략에서 두드러지며, 이는 단일 결합 클러스터로 끝나는 반면 단일 결합의 경우 프린지 주변의 노이즈 지점에서 다른 모든 클러스터가 그려지는 단일 중앙 클러스터를 얻습니다.</target>
        </trans-unit>
        <trans-unit id="32580ac608fcdadc1c2050695a6c24cae1fcef1f" translate="yes" xml:space="preserve">
          <source>What we can see that:</source>
          <target state="translated">우리가 볼 수있는 것 :</target>
        </trans-unit>
        <trans-unit id="bd10ebd98b3e733be938ffeb72efbd3793589a2c" translate="yes" xml:space="preserve">
          <source>When &lt;a href=&quot;generated/sklearn.decomposition.latentdirichletallocation#sklearn.decomposition.LatentDirichletAllocation&quot;&gt;&lt;code&gt;LatentDirichletAllocation&lt;/code&gt;&lt;/a&gt; is applied on a &amp;ldquo;document-term&amp;rdquo; matrix, the matrix will be decomposed into a &amp;ldquo;topic-term&amp;rdquo; matrix and a &amp;ldquo;document-topic&amp;rdquo; matrix. While &amp;ldquo;topic-term&amp;rdquo; matrix is stored as &lt;code&gt;components_&lt;/code&gt; in the model, &amp;ldquo;document-topic&amp;rdquo; matrix can be calculated from &lt;code&gt;transform&lt;/code&gt; method.</source>
          <target state="translated">되면 &lt;a href=&quot;generated/sklearn.decomposition.latentdirichletallocation#sklearn.decomposition.LatentDirichletAllocation&quot;&gt; &lt;code&gt;LatentDirichletAllocation&lt;/code&gt; &lt;/a&gt; 는 &quot;문서 용어&quot;행렬에 도포하고, 매트릭스는 &quot;항목 용어&quot;행렬 및 &quot;문서 주제&quot;행렬로 분해한다. &quot;토픽 용어&quot;매트릭스는 모델에서 &lt;code&gt;components_&lt;/code&gt; 로 저장되지만 &quot;문서 토픽&quot;매트릭스는 &lt;code&gt;transform&lt;/code&gt; 방법 에서 계산할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="d35c02f0322e905eb57e225a27ca83f7c5f6cc47" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;axis=0&lt;/code&gt;, columns which only contained missing values at &lt;code&gt;fit&lt;/code&gt; are discarded upon &lt;code&gt;transform&lt;/code&gt;.</source>
          <target state="translated">경우 &lt;code&gt;axis=0&lt;/code&gt; 단에서 누락 값을 포함 컬럼 &lt;code&gt;fit&lt;/code&gt; 에 폐기 &lt;code&gt;transform&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7cf98e7188203ecb0d30e3564b161d5393433799" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;axis=1&lt;/code&gt;, an exception is raised if there are rows for which it is not possible to fill in the missing values (e.g., because they only contain missing values).</source>
          <target state="translated">경우 &lt;code&gt;axis=1&lt;/code&gt; , 예외가 발생된다 (그들은 단지 누락 값을 포함하기 때문에, 예)는 누락 값을 입력 할 수되지 않은 행이 있다면.</target>
        </trans-unit>
        <trans-unit id="4bce6b6fbd4b9fe79bfc7468f7df9f938d3ce841" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;fit&lt;/code&gt; does not converge, &lt;code&gt;cluster_centers_&lt;/code&gt; becomes an empty array and all training samples will be labelled as &lt;code&gt;-1&lt;/code&gt;. In addition, &lt;code&gt;predict&lt;/code&gt; will then label every sample as &lt;code&gt;-1&lt;/code&gt;.</source>
          <target state="translated">때 &lt;code&gt;fit&lt;/code&gt; 수렴하지 않는, &lt;code&gt;cluster_centers_&lt;/code&gt; 는 하늘의 배열이되고 모든 교육 샘플로 표시됩니다 &lt;code&gt;-1&lt;/code&gt; . 또한 &lt;code&gt;predict&lt;/code&gt; 는 모든 샘플에 &lt;code&gt;-1&lt;/code&gt; 레이블을 붙 입니다.</target>
        </trans-unit>
        <trans-unit id="f7c77c5939a6b7b3a7ed9ce47827d49725522a57" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;gamma&lt;/code&gt; is very small, the model is too constrained and cannot capture the complexity or &amp;ldquo;shape&amp;rdquo; of the data. The region of influence of any selected support vector would include the whole training set. The resulting model will behave similarly to a linear model with a set of hyperplanes that separate the centers of high density of any pair of two classes.</source>
          <target state="translated">때 &lt;code&gt;gamma&lt;/code&gt; 매우 작고, 모델도 제한되고 복잡하거나 데이터의 &quot;형태&quot;를 캡처 할 수 없습니다. 선택된 서포트 벡터의 영향 영역에는 전체 트레이닝 세트가 포함됩니다. 결과 모델은 두 클래스의 모든 쌍의 고밀도 중심을 분리하는 초평면 세트가있는 선형 모델과 유사하게 작동합니다.</target>
        </trans-unit>
        <trans-unit id="6579a89bba02e2aa5596acf0a356de3285b5831f" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;learning_method&lt;/code&gt; is &amp;lsquo;online&amp;rsquo;, use mini-batch update. Otherwise, use batch update.</source>
          <target state="translated">때 &lt;code&gt;learning_method&lt;/code&gt; 가 '온라인'이다, 미니 일괄 업데이트를 사용합니다. 그렇지 않으면 배치 업데이트를 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="1789662661fe322dc45828ec3c3eb62749643034" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;novelty&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt; be aware that you must only use &lt;code&gt;predict&lt;/code&gt;, &lt;code&gt;decision_function&lt;/code&gt; and &lt;code&gt;score_samples&lt;/code&gt; on new unseen data and not on the training samples as this would lead to wrong results. The scores of abnormality of the training samples are always accessible through the &lt;code&gt;negative_outlier_factor_&lt;/code&gt; attribute.</source>
          <target state="translated">때 &lt;code&gt;novelty&lt;/code&gt; 설정되어 &lt;code&gt;True&lt;/code&gt; 만 사용해야한다는 인식 &lt;code&gt;predict&lt;/code&gt; , &lt;code&gt;decision_function&lt;/code&gt; 을 하고 &lt;code&gt;score_samples&lt;/code&gt; 새로운 보이지 않는 데이터가 아니라이 잘못된 결과로 이어질 것으로 훈련 샘플에. 훈련 샘플의 비정상 점수는 항상 &lt;code&gt;negative_outlier_factor_&lt;/code&gt; 속성을 통해 액세스 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="f3329fe0cead2a208482794593628ff4dce53789" translate="yes" xml:space="preserve">
          <source>When False, either &lt;code&gt;a&lt;/code&gt; or &lt;code&gt;b&lt;/code&gt; being sparse will yield sparse output. When True, output will always be an array.</source>
          <target state="translated">False 인 경우 &lt;code&gt;a&lt;/code&gt; 또는 &lt;code&gt;b&lt;/code&gt; 가 드문 드문 드문 드문 출력됩니다. True 인 경우 출력은 항상 배열입니다.</target>
        </trans-unit>
        <trans-unit id="0baad85c0e4d647371baa109869739d44cb0f600" translate="yes" xml:space="preserve">
          <source>When True (False by default) the &lt;code&gt;components_&lt;/code&gt; vectors are divided by &lt;code&gt;n_samples&lt;/code&gt; times &lt;code&gt;components_&lt;/code&gt; to ensure uncorrelated outputs with unit component-wise variances.</source>
          <target state="translated">경우 트루 (기본적으로 false) &lt;code&gt;components_&lt;/code&gt; 의 벡터에 의해 분할된다 &lt;code&gt;n_samples&lt;/code&gt; 시간 &lt;code&gt;components_&lt;/code&gt; 부 성분 와이즈 차이와 상관 출력을 보장하기 위해.</target>
        </trans-unit>
        <trans-unit id="3eaa2881688032030c765cb871d4c787aff03fe7" translate="yes" xml:space="preserve">
          <source>When True (False by default) the &lt;code&gt;components_&lt;/code&gt; vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances.</source>
          <target state="translated">True (기본적으로 False) 인 경우 &lt;code&gt;components_&lt;/code&gt; vector 는 n_samples의 제곱근을 곱한 다음 특이 값으로 나눠 단위 성분 별 분산으로 상관되지 않은 출력을 보장합니다.</target>
        </trans-unit>
        <trans-unit id="afb8087ad0f4a499dd14f72be9807792c351ecd8" translate="yes" xml:space="preserve">
          <source>When True, an absolute value is applied to the features matrix prior to returning it. When used in conjunction with alternate_sign=True, this significantly reduces the inner product preservation property.</source>
          <target state="translated">True 인 경우 반환하기 전에 기능 매트릭스에 절대 값이 적용됩니다. alternate_sign = True와 함께 사용하면 내부 제품 보존 속성이 크게 줄어 듭니다.</target>
        </trans-unit>
        <trans-unit id="204d5c48d22bd9cd74d36182840231c3ecac4f55" translate="yes" xml:space="preserve">
          <source>When True, an alternating sign is added to the features as to approximately conserve the inner product in the hashed space even for small n_features. This approach is similar to sparse random projection.</source>
          <target state="translated">True 인 경우 작은 n_features의 경우에도 해시 된 공간에서 내부 제품을 대략 보존하기 위해 대체 부호가 기능에 추가됩니다. 이 방법은 희소 랜덤 프로젝션과 유사합니다.</target>
        </trans-unit>
        <trans-unit id="e08ec276d8bba6d4be8a6e677715ca1a35d4dd15" translate="yes" xml:space="preserve">
          <source>When a grouped cross-validator is used, the group labels are also passed on to the &lt;code&gt;split&lt;/code&gt; method of the cross-validator. The cross-validator uses them for grouping the samples while splitting the dataset into train/test set.</source>
          <target state="translated">그룹화 된 교차 유효성 검사기를 사용하면 그룹 레이블도 교차 유효성 검사기 의 &lt;code&gt;split&lt;/code&gt; 방법으로 전달됩니다 . 교차 검증기는 데이터 세트를 트레인 / 테스트 세트로 분할하면서 샘플을 그룹화하기 위해이 샘플을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="6c0f90cdc44694d62adb6ac9bccd5513d6bf148f" translate="yes" xml:space="preserve">
          <source>When all training samples have equal similarities and equal preferences, the assignment of cluster centers and labels depends on the preference. If the preference is smaller than the similarities, &lt;code&gt;fit&lt;/code&gt; will result in a single cluster center and label &lt;code&gt;0&lt;/code&gt; for every sample. Otherwise, every training sample becomes its own cluster center and is assigned a unique label.</source>
          <target state="translated">모든 트레이닝 샘플이 동일한 유사성과 동일한 환경 설정을 갖는 경우, 클러스터 센터 및 레이블의 할당은 환경 설정에 따라 다릅니다. 선호도가 유사성보다 작은 경우 &lt;code&gt;fit&lt;/code&gt; 하면 모든 표본 에 대해 단일 군집 중심과 레이블 &lt;code&gt;0&lt;/code&gt; 이 생성됩니다. 그렇지 않으면 모든 교육 샘플이 자체 클러스터 센터가되고 고유 한 레이블이 할당됩니다.</target>
        </trans-unit>
        <trans-unit id="c440a8e563c9cc7b9752f14072284d81697bdfcd" translate="yes" xml:space="preserve">
          <source>When all training samples have equal similarities and equal preferences, the assignment of cluster centers and labels depends on the preference. If the preference is smaller than the similarities, a single cluster center and label &lt;code&gt;0&lt;/code&gt; for every sample will be returned. Otherwise, every training sample becomes its own cluster center and is assigned a unique label.</source>
          <target state="translated">모든 트레이닝 샘플이 동일한 유사성과 동일한 환경 설정을 갖는 경우, 클러스터 센터 및 레이블의 할당은 환경 설정에 따라 다릅니다. 기본 설정이 유사성보다 작은 경우 모든 샘플에 대해 단일 군집 중심과 레이블 &lt;code&gt;0&lt;/code&gt; 이 반환됩니다. 그렇지 않으면 모든 교육 샘플이 자체 클러스터 센터가되고 고유 한 레이블이 할당됩니다.</target>
        </trans-unit>
        <trans-unit id="2494d11b03713922afdad3fa1bc52bb7064577b4" translate="yes" xml:space="preserve">
          <source>When alpha is very large, the regularization effect dominates the squared loss function and the coefficients tend to zero. At the end of the path, as alpha tends toward zero and the solution tends towards the ordinary least squares, coefficients exhibit big oscillations. In practise it is necessary to tune alpha in such a way that a balance is maintained between both.</source>
          <target state="translated">알파가 매우 크면 정규화 효과가 제곱 손실 함수를 지배하며 계수는 0 인 경향이 있습니다. 경로의 끝에서 알파는 0을 향하고 솔루션은 일반적인 최소 제곱을 향함에 따라 계수는 큰 진동을 나타냅니다. 실제로, 둘 사이의 균형이 유지되도록 알파를 조정해야합니다.</target>
        </trans-unit>
        <trans-unit id="e93a4fdf68d407dc869190eca6cd7dfaf57ad986" translate="yes" xml:space="preserve">
          <source>When applying LOF for outlier detection, there are no &lt;code&gt;predict&lt;/code&gt;, &lt;code&gt;decision_function&lt;/code&gt; and &lt;code&gt;score_samples&lt;/code&gt; methods but only a &lt;code&gt;fit_predict&lt;/code&gt; method. The scores of abnormality of the training samples are accessible through the &lt;code&gt;negative_outlier_factor_&lt;/code&gt; attribute. Note that &lt;code&gt;predict&lt;/code&gt;, &lt;code&gt;decision_function&lt;/code&gt; and &lt;code&gt;score_samples&lt;/code&gt; can be used on new unseen data when LOF is applied for novelty detection, i.e. when the &lt;code&gt;novelty&lt;/code&gt; parameter is set to &lt;code&gt;True&lt;/code&gt;. See &lt;a href=&quot;#novelty-with-lof&quot;&gt;Novelty detection with Local Outlier Factor&lt;/a&gt;.</source>
          <target state="translated">이상치 검출 LOF을 적용 할 때, 어떤이없는 &lt;code&gt;predict&lt;/code&gt; , &lt;code&gt;decision_function&lt;/code&gt; 및 &lt;code&gt;score_samples&lt;/code&gt; 방법 만 만 &lt;code&gt;fit_predict&lt;/code&gt; 의 방법을. 훈련 샘플의 비정상 점수는 &lt;code&gt;negative_outlier_factor_&lt;/code&gt; 속성을 통해 액세스 할 수 있습니다 . LOF가 참신 탐지에 적용되는 경우, 즉 &lt;code&gt;novelty&lt;/code&gt; 매개 변수가 &lt;code&gt;True&lt;/code&gt; 로 설정된 경우 &lt;code&gt;predict&lt;/code&gt; 않은 새로운 데이터에 대해 predict , &lt;code&gt;decision_function&lt;/code&gt; 및 &lt;code&gt;score_samples&lt;/code&gt; 를 사용할 수 있습니다 . &lt;a href=&quot;#novelty-with-lof&quot;&gt;로컬 이상치 요인을 사용한 참신 탐지를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="869141a5ff3e1444543cdaec8c9968684d76b66e" translate="yes" xml:space="preserve">
          <source>When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.</source>
          <target state="translated">어휘를 만들 때 주어진 문턱 값보다 엄격하게 높은 문서 빈도를 가진 용어는 무시합니다 (corpus-specific stop words). float 인 경우 매개 변수는 문서의 비율을 나타내며 정수 절대 계수입니다. 어휘가 없음이 아닌 경우이 매개 변수는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="1f13ad80586b74ad6fdc521fde036c051cf6e0e7" translate="yes" xml:space="preserve">
          <source>When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.</source>
          <target state="translated">어휘를 작성할 때 문서 빈도가 주어진 임계 값보다 엄격하게 낮은 용어는 무시하십시오. 이 값은 문헌에서 차단이라고도합니다. float 인 경우 매개 변수는 문서의 비율을 나타내며 정수 절대 계수입니다. 어휘가 없음이 아닌 경우이 매개 변수는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="8ed788b8f95069d89dcb15b1bea5b5e7da9a9648" translate="yes" xml:space="preserve">
          <source>When calling &lt;code&gt;fit&lt;/code&gt;, an affinity matrix is constructed using either kernel function such the Gaussian (aka RBF) kernel of the euclidean distanced &lt;code&gt;d(X, X)&lt;/code&gt;:</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 을 호출 할 때 선호도 행렬은 유클리드 거리 &lt;code&gt;d(X, X)&lt;/code&gt; 의 가우스 (일명 RBF) 커널과 같은 커널 함수를 사용하여 구성됩니다 .</target>
        </trans-unit>
        <trans-unit id="da9c4333516559f145c3dcae1d1fd9f5e0da891d" translate="yes" xml:space="preserve">
          <source>When doing classification in scikit-learn, &lt;code&gt;y&lt;/code&gt; is a vector of integers or strings.</source>
          <target state="translated">scikit-learn에서 분류를 수행 할 때 &lt;code&gt;y&lt;/code&gt; 는 정수 또는 문자열로 구성된 벡터입니다.</target>
        </trans-unit>
        <trans-unit id="fb08dbb1ad477236e66b72b0cef9bccfd1ceec61" translate="yes" xml:space="preserve">
          <source>When doing supervised learning, a simple sanity check consists of comparing one&amp;rsquo;s estimator against simple rules of thumb. &lt;a href=&quot;generated/sklearn.dummy.dummyclassifier#sklearn.dummy.DummyClassifier&quot;&gt;&lt;code&gt;DummyClassifier&lt;/code&gt;&lt;/a&gt; implements several such simple strategies for classification:</source>
          <target state="translated">지도 학습을 수행 할 때 간단한 위생 검사는 추정기를 간단한 경험 규칙과 비교하여 구성됩니다. &lt;a href=&quot;generated/sklearn.dummy.dummyclassifier#sklearn.dummy.DummyClassifier&quot;&gt; &lt;code&gt;DummyClassifier&lt;/code&gt; &lt;/a&gt; 는 분류를위한 몇 가지 간단한 전략을 구현합니다.</target>
        </trans-unit>
        <trans-unit id="3bf9748662d1dbe365a15ba24b002d016f11408b" translate="yes" xml:space="preserve">
          <source>When evaluating different settings (&amp;ldquo;hyperparameters&amp;rdquo;) for estimators, such as the &lt;code&gt;C&lt;/code&gt; setting that must be manually set for an SVM, there is still a risk of overfitting &lt;em&gt;on the test set&lt;/em&gt; because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can &amp;ldquo;leak&amp;rdquo; into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called &amp;ldquo;validation set&amp;rdquo;: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.</source>
          <target state="translated">SVM에 대해 수동으로 설정해야하는 &lt;code&gt;C&lt;/code&gt; 설정 과 같은 추정기의 다른 설정 (&amp;ldquo;하이 파라미터&amp;rdquo;)을 평가할 때 추정기 가 최적으로 수행 될 때까지 매개 변수를 조정할 수 있으므로 &lt;em&gt;테스트 세트&lt;/em&gt; 에 과적 합의 위험이 여전히 있습니다. 이런 식으로 테스트 세트에 대한 지식이 모델에 &quot;누설&quot;될 수 있으며 평가 지표는 더 이상 일반화 성능에 대해보고하지 않습니다. 이 문제를 해결하기 위해 데이터 집합의 또 다른 부분을 소위 &quot;유효성 검사 세트&quot;라고 할 수 있습니다. 훈련은 훈련 세트에서 진행된 후 유효성 검사 세트에서 평가가 수행 된 후 실험이 성공한 것으로 보입니다 테스트 세트에서 최종 평가를 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b6d52e3adfb55f9048c5ef57545d77d640ae7db4" translate="yes" xml:space="preserve">
          <source>When evaluating text classifiers on the 20 Newsgroups data, you should strip newsgroup-related metadata. In scikit-learn, you can do this by setting &lt;code&gt;remove=('headers', 'footers', 'quotes')&lt;/code&gt;. The F-score will be lower because it is more realistic.</source>
          <target state="translated">20 개의 뉴스 그룹 데이터에서 텍스트 분류자를 평가할 때 뉴스 그룹 관련 메타 데이터를 제거해야합니다. scikit-learn에서 &lt;code&gt;remove=('headers', 'footers', 'quotes')&lt;/code&gt; 를 설정하여이를 수행 할 수 있습니다 . F- 점수가 더 현실적이기 때문에 더 낮아집니다.</target>
        </trans-unit>
        <trans-unit id="b44ace677df67ec762b5f7d213266d4181953b1c" translate="yes" xml:space="preserve">
          <source>When evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process: it is recommended to split the data into a &lt;strong&gt;development set&lt;/strong&gt; (to be fed to the &lt;code&gt;GridSearchCV&lt;/code&gt; instance) and an &lt;strong&gt;evaluation set&lt;/strong&gt; to compute performance metrics.</source>
          <target state="translated">결과 모델을 평가할 때 그리드 검색 프로세스 중에 보이지 않은 보류 된 샘플에서 모델을 수행하는 것이 중요합니다. 데이터를 &lt;strong&gt;개발 세트&lt;/strong&gt; ( &lt;code&gt;GridSearchCV&lt;/code&gt; 인스턴스에 제공) 및 &lt;strong&gt;평가 세트&lt;/strong&gt; 로 분할하는 것이 좋습니다. 성능 지표를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="1a86b5b9ee94c593acad1b7968fd00ab3b487df9" translate="yes" xml:space="preserve">
          <source>When feature values are strings, this transformer will do a binary one-hot (aka one-of-K) coding: one boolean-valued feature is constructed for each of the possible string values that the feature can take on. For instance, a feature &amp;ldquo;f&amp;rdquo; that can take on the values &amp;ldquo;ham&amp;rdquo; and &amp;ldquo;spam&amp;rdquo; will become two features in the output, one signifying &amp;ldquo;f=ham&amp;rdquo;, the other &amp;ldquo;f=spam&amp;rdquo;.</source>
          <target state="translated">기능 값이 문자열 인 경우이 변환기는 이진 one-hot (일명 K) 코딩을 수행합니다. 하나의 부울 값 기능은 기능이 취할 수있는 가능한 문자열 값 각각에 대해 구성됩니다. 예를 들어, &quot;ham&quot;및 &quot;spam&quot;값을 사용할 수있는 기능 &quot;f&quot;는 출력에서 ​​&quot;f = ham&quot;을 나타내는 두 가지 기능, 다른 하나는 &quot;f = spam&quot;이됩니다.</target>
        </trans-unit>
        <trans-unit id="e3d8c3576d6baaf6f77dc4e34223e91a48a8c662" translate="yes" xml:space="preserve">
          <source>When fitting a model to a matrix X_train and evaluating it against a matrix X_test, it is essential that X_train and X_test have the same number of features (X_train.shape[1] == X_test.shape[1]). This may not be the case if you load the files individually with load_svmlight_file.</source>
          <target state="translated">모델을 행렬 X_train에 피팅하고 행렬 X_test에 대해 평가할 때 X_train과 X_test는 동일한 수의 기능 (X_train.shape [1] == X_test.shape [1])을 가져야합니다. load_svmlight_file을 사용하여 파일을 개별적으로로드하는 경우에는 해당되지 않을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="618912ddd6add9bc398f8cf8343e255bb5f0ac59" translate="yes" xml:space="preserve">
          <source>When in doubt, use &lt;a href=&quot;#ransac-regression&quot;&gt;RANSAC&lt;/a&gt;</source>
          <target state="translated">의심 &lt;a href=&quot;#ransac-regression&quot;&gt;스러운&lt;/a&gt; 경우 RANSAC을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="7ebc317e66e94c9c3ce7d1b975021eefa97b880b" translate="yes" xml:space="preserve">
          <source>When individual estimators are fast to train or predict using &lt;code&gt;n_jobs&amp;gt;1&lt;/code&gt; can result in slower performance due to the overhead of spawning processes.</source>
          <target state="translated">개별 견적자가 &lt;code&gt;n_jobs&amp;gt;1&lt;/code&gt; 을 사용하여 빠르게 훈련하거나 예측하는 경우 생성 프로세스 오버 헤드로 인해 성능이 저하 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5223d409a8ec7650bd8559a52e22ddb32a950ada" translate="yes" xml:space="preserve">
          <source>When loss=&amp;rdquo;modified_huber&amp;rdquo;, probability estimates may be hard zeros and ones, so taking the logarithm is not possible.</source>
          <target state="translated">loss =&amp;rdquo;modified_huber&amp;rdquo;인 경우 확률 추정치는 0과 1 일 수 있으므로 로그를 취할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="63fb46936ec9f0c71e69b897d1f9d91941d422e3" translate="yes" xml:space="preserve">
          <source>When modeling text corpora, the model assumes the following generative process for a corpus with \(D\) documents and \(K\) topics:</source>
          <target state="translated">텍스트 코포 라를 모델링 할 때이 모델은 \ (D \) 문서와 \ (K \) 토픽을 가진 모음에 대해 다음과 같은 생성 프로세스를 가정합니다.</target>
        </trans-unit>
        <trans-unit id="744347c999c68da3080dd50ae1b985e4b97e385e" translate="yes" xml:space="preserve">
          <source>When one has insufficiently many points per mixture, estimating the covariance matrices becomes difficult, and the algorithm is known to diverge and find solutions with infinite likelihood unless one regularizes the covariances artificially.</source>
          <target state="translated">혼합물 당 점이 충분하지 않으면 공분산 행렬을 추정하기가 어려워지며, 알고리즘은 공분산을 인위적으로 정규화하지 않는 한 무한한 가능성으로 솔루션을 분기하고 찾는 것으로 알려져 있습니다.</target>
        </trans-unit>
        <trans-unit id="e24f09e860711fd3a63a21415134601f90e4efc0" translate="yes" xml:space="preserve">
          <source>When parametrized by error using the parameter &lt;code&gt;tol&lt;/code&gt;: argmin ||gamma||_0 subject to ||y - Xgamma||^2 &amp;lt;= tol</source>
          <target state="translated">매개 변수 &lt;code&gt;tol&lt;/code&gt; 을 사용하여 오류로 매개 변수화되는 경우 : argmin || gamma || _0 || y-Xgamma || ^ 2 &amp;lt;= tol</target>
        </trans-unit>
        <trans-unit id="70b3334d846e26366c45a04fb1a4a39af8e3ec45" translate="yes" xml:space="preserve">
          <source>When parametrized by the number of non-zero coefficients using &lt;code&gt;n_nonzero_coefs&lt;/code&gt;: argmin ||y - Xgamma||^2 subject to ||gamma||_0 &amp;lt;= n_{nonzero coefs}</source>
          <target state="translated">&lt;code&gt;n_nonzero_coefs&lt;/code&gt; 를 사용하여 0이 아닌 계수의 수로 매개 변수화되는 경우 : argmin || y-Xgamma || ^ 2 || gamma || _0 &amp;lt;= n_ {nonzero coefs}</target>
        </trans-unit>
        <trans-unit id="10d326cee514d3b967129b254954126bcda90793" translate="yes" xml:space="preserve">
          <source>When performing classification one often wants to predict not only the class label, but also the associated probability. This probability gives some kind of confidence on the prediction. This example demonstrates how to display how well calibrated the predicted probabilities are and how to calibrate an uncalibrated classifier.</source>
          <target state="translated">분류를 수행 할 때 종종 클래스 레이블뿐만 아니라 관련 확률도 예측하려고합니다. 이 확률은 예측에 대한 일종의 신뢰를 제공합니다. 이 예제는 예측 된 확률이 얼마나 잘 교정되었는지와 교정되지 않은 분류기를 교정하는 방법을 표시하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="533cc79868c5585705042f97bd64dbf9b1c6133f" translate="yes" xml:space="preserve">
          <source>When performing classification you often want not only to predict the class label, but also obtain a probability of the respective label. This probability gives you some kind of confidence on the prediction. Some models can give you poor estimates of the class probabilities and some even do not support probability prediction. The calibration module allows you to better calibrate the probabilities of a given model, or to add support for probability prediction.</source>
          <target state="translated">분류를 수행 할 때 종종 클래스 레이블을 예측할뿐만 아니라 각 레이블의 확률을 얻으려고합니다. 이 확률은 예측에 대한 일종의 신뢰를 제공합니다. 일부 모델은 클래스 확률에 대한 잘못된 추정치를 제공 할 수 있으며 일부는 확률 예측을 지원하지 않습니다. 보정 모듈을 사용하면 주어진 모델의 확률을보다 잘 보정하거나 확률 예측에 대한 지원을 추가 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ede14543224daf4bd7da97ebebdbcb4bdb8fe514" translate="yes" xml:space="preserve">
          <source>When performing classification you often want to predict not only the class label, but also the associated probability. This probability gives you some kind of confidence on the prediction. However, not all classifiers provide well-calibrated probabilities, some being over-confident while others being under-confident. Thus, a separate calibration of predicted probabilities is often desirable as a postprocessing. This example illustrates two different methods for this calibration and evaluates the quality of the returned probabilities using Brier&amp;rsquo;s score (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Brier_score&quot;&gt;https://en.wikipedia.org/wiki/Brier_score&lt;/a&gt;).</source>
          <target state="translated">분류를 수행 할 때 종종 클래스 레이블뿐만 아니라 관련 확률도 예측하려고합니다. 이 확률은 예측에 대한 일종의 신뢰를 제공합니다. 그러나 모든 분류자가 교정이 잘 된 확률을 제공하는 것은 아니며 일부는 지나치게 확신하고 다른 일부는 확신이 부족합니다. 따라서, 예측 된 확률의 개별 교정은 종종 후 처리로서 바람직하다. 이 예제는이 교정에 대한 두 가지 다른 방법을 보여주고 Brier의 점수를 사용하여 반환 된 확률의 품질을 평가합니다 ( &lt;a href=&quot;https://en.wikipedia.org/wiki/Brier_score&quot;&gt;https://en.wikipedia.org/wiki/Brier_score 참조&lt;/a&gt; ).</target>
        </trans-unit>
        <trans-unit id="e99cb78745b2020137c83400ee3d8d22f37293c0" translate="yes" xml:space="preserve">
          <source>When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified, ensuring X is C-contiguous. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean, in this case it will also not ensure that data is C-contiguous which may cause a significant slowdown.</source>
          <target state="translated">사전 계산 거리의 경우 데이터를 먼저 중앙에 배치하는 것이 더 정확합니다. copy_x가 True (기본값)이면 원본 데이터는 수정되지 않고 X가 C 연속적입니다. False 인 경우 원래 데이터가 수정되고 함수가 리턴되기 전에 되돌려 지지만 데이터 평균을 빼고 추가하여 작은 숫자 차이가 발생할 수 있습니다.이 경우 데이터가 C 연속적임을 보장하지 않습니다. 상당한 속도 저하.</target>
        </trans-unit>
        <trans-unit id="55ac178846c6596090a6f8d0133ba62fcd26aebb" translate="yes" xml:space="preserve">
          <source>When predicting, the true labels will not be available. Instead the predictions of each model are passed on to the subsequent models in the chain to be used as features.</source>
          <target state="translated">예측할 때 실제 레이블을 사용할 수 없습니다. 대신 각 모델의 예측이 체인의 후속 모델로 전달되어 피처로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="441463f4c28b96b4ca0739417ce251a6d1637336" translate="yes" xml:space="preserve">
          <source>When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces &lt;a href=&quot;#h1998&quot; id=&quot;id3&quot;&gt;[H1998]&lt;/a&gt;.</source>
          <target state="translated">데이터 세트의 임의의 부분 집합이 특징의 임의의 부분 집합으로 그려 질 때, 방법은 무작위 부분 공간 &lt;a href=&quot;#h1998&quot; id=&quot;id3&quot;&gt;[H1998]&lt;/a&gt; 으로 알려져있다 .</target>
        </trans-unit>
        <trans-unit id="ed3a388f4c5d9809937b1b2fceecd5d5d41437fc" translate="yes" xml:space="preserve">
          <source>When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting &lt;a href=&quot;#b1999&quot; id=&quot;id1&quot;&gt;[B1999]&lt;/a&gt;.</source>
          <target state="translated">데이터 세트의 임의의 부분 집합이 샘플의 임의의 부분 집합으로 그려 질 때이 알고리즘을 붙여 넣기 &lt;a href=&quot;#b1999&quot; id=&quot;id1&quot;&gt;[B1999]라고&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="f494cf947867cf3181ff2c5eaa996adc8d0ce783" translate="yes" xml:space="preserve">
          <source>When requesting a dataset with a name that is in mock_datasets, this object creates a fake dataset in a StringIO object and returns it. Otherwise, it raises an HTTPError.</source>
          <target state="translated">이름이 mock_datasets 인 데이터 집합을 요청하면이 개체는 StringIO 개체에 가짜 데이터 집합을 만들어 반환합니다. 그렇지 않으면 HTTPError가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="d66c9681351cfa02a7cc3145d8e8cc7e7a3877b3" translate="yes" xml:space="preserve">
          <source>When samples are drawn with replacement, then the method is known as Bagging &lt;a href=&quot;#b1996&quot; id=&quot;id2&quot;&gt;[B1996]&lt;/a&gt;.</source>
          <target state="translated">샘플을 교체하여 채취 할 때이 방법을 배깅 (Bagging) &lt;a href=&quot;#b1996&quot; id=&quot;id2&quot;&gt;[B1996]이라고&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="b67d588266cee585b7c0608db5b116728771921f" translate="yes" xml:space="preserve">
          <source>When self.fit_intercept is True, instance vector x becomes &lt;code&gt;[x, self.intercept_scaling]&lt;/code&gt;, i.e. a &amp;ldquo;synthetic&amp;rdquo; feature with constant value equals to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased.</source>
          <target state="translated">self.fit_intercept가 True 인 경우 인스턴스 벡터 x는 &lt;code&gt;[x, self.intercept_scaling]&lt;/code&gt; . 즉, intercept_scaling과 동일한 상수 값을 가진 &quot;합성&quot;기능이 인스턴스 벡터에 추가됩니다. 절편은 intercept_scaling * 합성 피처 중량이됩니다. 참고! 합성 피처 중량은 다른 모든 피처와 마찬가지로 l1 / l2 정규화에 적용됩니다. 합성 피처 가중치 (따라서 인터셉트)에 대한 정규화의 영향을 줄이려면 intercept_scaling을 늘려야합니다.</target>
        </trans-unit>
        <trans-unit id="339fba0d20ce2052ad9daa9e3c6cc55589d832bc" translate="yes" xml:space="preserve">
          <source>When self.fit_intercept is True, instance vector x becomes [x, self.intercept_scaling], i.e. a &amp;ldquo;synthetic&amp;rdquo; feature with constant value equals to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased.</source>
          <target state="translated">self.fit_intercept가 True 인 경우 인스턴스 벡터 x는 [x, self.intercept_scaling]이됩니다. 즉, intercept_scaling과 동일한 상수 값을 가진 &quot;합성&quot;기능이 인스턴스 벡터에 추가됩니다. 절편은 intercept_scaling * 합성 피처 중량이됩니다. 참고! 합성 피처 중량은 다른 모든 피처와 마찬가지로 l1 / l2 정규화에 적용됩니다. 합성 피처 가중치 (따라서 인터셉트)에 대한 정규화의 영향을 줄이려면 intercept_scaling을 늘려야합니다.</target>
        </trans-unit>
        <trans-unit id="ad06c0ce7c1fc91387cf888768a953ddfc43c4b6" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;False&lt;/code&gt;, ignore special characters for PostScript compatibility.</source>
          <target state="translated">&lt;code&gt;False&lt;/code&gt; 로 설정되면 PostScript 호환성을 위해 특수 문자를 무시하십시오.</target>
        </trans-unit>
        <trans-unit id="d1b93df55570608a785000a0ebdde0c1a478b680" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, change the display of &amp;lsquo;values&amp;rsquo; and/or &amp;lsquo;samples&amp;rsquo; to be proportions and percentages respectively.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정되면 '값'및 / 또는 '샘플'표시가 각각 비율 및 백분율로 변경됩니다.</target>
        </trans-unit>
        <trans-unit id="3843d20a53a1ef3b59460be1fa7cb77d3c2ee1f6" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, draw all leaf nodes at the bottom of the tree.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정되면 트리의 맨 아래에 모든 리프 노드를 그립니다.</target>
        </trans-unit>
        <trans-unit id="1dd8441b2d9ea649f69d55eb070005c43dff3ea1" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, draw node boxes with rounded corners and use Helvetica fonts instead of Times-Roman.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정하면 모서리가 둥근 노드 상자를 그리고 Times-Roman 대신 Helvetica 글꼴을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="37e5e7c90dcc2881b20280bd58a636a2601aa0c6" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, forces the coefficients to be positive.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정 하면 계수가 양수가됩니다.</target>
        </trans-unit>
        <trans-unit id="a556178389cf10e9a8f97ab94b5cbc137168d877" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, orient tree left to right rather than top-down.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정되면 트리가 하향식이 아닌 왼쪽에서 오른쪽으로 향합니다.</target>
        </trans-unit>
        <trans-unit id="db2d0a81092e78238cdb916143e482a964d5a789" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, paint nodes to indicate majority class for classification, extremity of values for regression, or purity of node for multi-output.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정하면 분류를위한 대다수 클래스, 회귀 값의 극단 또는 다중 출력에 대한 노드 순도를 나타내도록 노드를 페인트합니다.</target>
        </trans-unit>
        <trans-unit id="387704ecb7f5fc4e8c941c08ae18f72e58104663" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정되면 이전 호출의 솔루션을 재사용하여 앙상블에 더 많은 추정기를 추가하십시오. 그렇지 않으면 이전 솔루션을 지우십시오. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="e08becb1d7f3dabb059c61e3163efbfc5e9d6bd5" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정 하면 이전 호출의 솔루션을 재사용하여 앙상블에 더 많은 추정기를 추가하고 그렇지 않으면 완전히 새로운 포리스트에 맞 춥니 다. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="1ddb18c96e1fca0312edb00f224f2db160c80a30" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정되면 이전 호출의 솔루션을 초기화에 맞게 다시 사용하십시오. 그렇지 않으면 이전 솔루션을 지우십시오. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="303e5cc9af6656c2592224b864d50a2e7f014b54" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, show the ID number on each node.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정되면 각 노드에서 ID 번호를 표시하십시오.</target>
        </trans-unit>
        <trans-unit id="d19057d05dc608969ed0862b9054b2defc3fb482" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, show the impurity at each node.</source>
          <target state="translated">&lt;code&gt;True&lt;/code&gt; 로 설정되면 각 노드에서 불순물을 표시하십시오.</target>
        </trans-unit>
        <trans-unit id="0199409f1eeb1aa7581c717d5c23e182af166761" translate="yes" xml:space="preserve">
          <source>When set to True, computes the averaged SGD weights and stores the result in the &lt;code&gt;coef_&lt;/code&gt; attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So &lt;code&gt;average=10&lt;/code&gt; will begin averaging after seeing 10 samples.</source>
          <target state="translated">True로 설정하면 평균 SGD 가중치를 계산하고 결과를 &lt;code&gt;coef_&lt;/code&gt; 속성 에 저장 합니다. int를 1보다 큰 값으로 설정하면 표시된 총 샘플 수가 평균에 도달하면 평균화가 시작됩니다. 따라서 &lt;code&gt;average=10&lt;/code&gt; 은 10 개의 샘플을 본 후 평균화되기 시작합니다.</target>
        </trans-unit>
        <trans-unit id="19c4ca669b90d48df2f3aa161b89103ea08c0cbd" translate="yes" xml:space="preserve">
          <source>When set to True, computes the averaged SGD weights and stores the result in the &lt;code&gt;coef_&lt;/code&gt; attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples.</source>
          <target state="translated">True로 설정하면 평균 SGD 가중치를 계산하고 결과를 &lt;code&gt;coef_&lt;/code&gt; 속성 에 저장 합니다. int를 1보다 큰 값으로 설정하면 표시된 총 샘플 수가 평균에 도달하면 평균화가 시작됩니다. 따라서 평균 = 10은 10 개의 샘플을 본 후 평균화되기 시작합니다.</target>
        </trans-unit>
        <trans-unit id="e60b70114bd43f63c876204c608aaaaca2e5c6d2" translate="yes" xml:space="preserve">
          <source>When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">True로 설정하면 이전 호출의 솔루션을 재사용하여 앙상블에 더 많은 추정기를 추가하고 그렇지 않으면 완전히 새로운 앙상블에 맞 춥니 다. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="0615ecfccfe9c12eea86adfdd92570d0b3a6f9cf" translate="yes" xml:space="preserve">
          <source>When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">True로 설정하면 이전 호출의 솔루션을 초기화에 맞게 다시 사용하십시오. 그렇지 않으면 이전 솔루션을 지우십시오. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="869683b3c71be56286e995de01f21c989fc735d8" translate="yes" xml:space="preserve">
          <source>When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">True로 설정하면 이전 호출의 솔루션을 초기화에 맞게 다시 사용하십시오. 그렇지 않으면 이전 솔루션을 지우십시오. liblinear 솔버에는 쓸모가 없습니다. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="61362ce0a02977970071e88e9dc71d066251fcde" translate="yes" xml:space="preserve">
          <source>When specifying multiple metrics, the &lt;code&gt;refit&lt;/code&gt; parameter must be set to the metric (string) for which the &lt;code&gt;best_params_&lt;/code&gt; will be found and used to build the &lt;code&gt;best_estimator_&lt;/code&gt; on the whole dataset. If the search should not be refit, set &lt;code&gt;refit=False&lt;/code&gt;. Leaving refit to the default value &lt;code&gt;None&lt;/code&gt; will result in an error when using multiple metrics.</source>
          <target state="translated">여러 통계를 지정하는 경우 &lt;code&gt;refit&lt;/code&gt; 매개 변수가있는 메트릭 (문자열)로 설정해야합니다 &lt;code&gt;best_params_&lt;/code&gt; 가 발견하고 구축하는 데 사용됩니다 &lt;code&gt;best_estimator_&lt;/code&gt; 전체 데이터 세트에 있습니다. 검색을 다시 작성하지 않으려면 &lt;code&gt;refit=False&lt;/code&gt; 로 설정하십시오 . 여러 값을 사용하는 경우 기본값을 '재설정 &lt;code&gt;None&lt;/code&gt; '으로두면 오류가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="765386eefc1dd2e9ed203024ee007c99e07f6618" translate="yes" xml:space="preserve">
          <source>When strategy == &amp;ldquo;constant&amp;rdquo;, fill_value is used to replace all occurrences of missing_values. If left to the default, fill_value will be 0 when imputing numerical data and &amp;ldquo;missing_value&amp;rdquo; for strings or object data types.</source>
          <target state="translated">strategy ==&amp;ldquo;constant&amp;rdquo;인 경우 fill_value를 사용하여 missing_value의 모든 발생을 대체합니다. 기본값을 그대로두면 숫자 데이터를 대치 할 때 fill_value는 0이되고 문자열 또는 객체 데이터 형식은 &quot;missing_value&quot;가됩니다.</target>
        </trans-unit>
        <trans-unit id="b3973bbeeefced2bd7eb8aaa05fcc0fadc5e600b" translate="yes" xml:space="preserve">
          <source>When the &lt;code&gt;cv&lt;/code&gt; argument is an integer, &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; uses the &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; strategies by default, the latter being used if the estimator derives from &lt;a href=&quot;generated/sklearn.base.classifiermixin#sklearn.base.ClassifierMixin&quot;&gt;&lt;code&gt;ClassifierMixin&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">때 &lt;code&gt;cv&lt;/code&gt; 인수가 정수, &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; 용도 &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt; , 기본적으로부터 추정의 도출 경우 사용되는 후자 전략을 &lt;a href=&quot;generated/sklearn.base.classifiermixin#sklearn.base.ClassifierMixin&quot;&gt; &lt;code&gt;ClassifierMixin&lt;/code&gt; 을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="c7d238875ff93bafa2620f7a2d7675b937a2183b" translate="yes" xml:space="preserve">
          <source>When the algorithm does not converge, it returns an empty array as &lt;code&gt;cluster_center_indices&lt;/code&gt; and &lt;code&gt;-1&lt;/code&gt; as label for each training sample.</source>
          <target state="translated">알고리즘이 수렴하지 않으면 빈 배열을 &lt;code&gt;cluster_center_indices&lt;/code&gt; 로 , &lt;code&gt;-1&lt;/code&gt; 을 각 학습 샘플의 레이블 로 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="b4fdfb364ee084bf57bd04a0f7c8f0c41739edf4" translate="yes" xml:space="preserve">
          <source>When the data is not initially in the &lt;code&gt;(n_samples, n_features)&lt;/code&gt; shape, it needs to be preprocessed in order to be used by scikit-learn.</source>
          <target state="translated">데이터가 처음에 &lt;code&gt;(n_samples, n_features)&lt;/code&gt; 모양이 아닌 경우 scikit-learn에서 사용하려면 데이터를 사전 처리해야합니다.</target>
        </trans-unit>
        <trans-unit id="aaeec02a52e8579f06c37808c9e18fa50d637a08" translate="yes" xml:space="preserve">
          <source>When there are more than two labels, the value of the MCC will no longer range between -1 and +1. Instead the minimum value will be somewhere between -1 and 0 depending on the number and distribution of ground true labels. The maximum value is always +1.</source>
          <target state="translated">라벨이 두 개 이상인 경우 MCC 값은 더 이상 -1과 +1 사이가 아닙니다. 대신 최소값은 실제 라벨의 수와 분포에 따라 -1과 0 사이입니다. 최대 값은 항상 +1입니다.</target>
        </trans-unit>
        <trans-unit id="5c69ffd1e0dc71befa67c00726bc6370582b5df5" translate="yes" xml:space="preserve">
          <source>When there is no correlation between the outputs, a very simple way to solve this kind of problem is to build n independent models, i.e. one for each output, and then to use those models to independently predict each one of the n outputs. However, because it is likely that the output values related to the same input are themselves correlated, an often better way is to build a single model capable of predicting simultaneously all n outputs. First, it requires lower training time since only a single estimator is built. Second, the generalization accuracy of the resulting estimator may often be increased.</source>
          <target state="translated">출력간에 상관 관계가없는 경우 이러한 종류의 문제를 해결하는 매우 간단한 방법은 n 개의 독립적 인 모델, 즉 각 출력에 대한 모델을 만든 다음 해당 모델을 사용하여 n 개의 출력 각각을 독립적으로 예측하는 것입니다. 그러나 동일한 입력과 관련된 출력 값이 서로 관련되어있을 가능성이 높기 때문에 n 개의 모든 출력을 동시에 예측할 수있는 단일 모델을 구축하는 것이 더 좋은 방법입니다. 첫째, 하나의 추정기만 구축되므로 교육 시간이 단축됩니다. 둘째, 결과 추정기의 일반화 정확도가 종종 증가 될 수있다.</target>
        </trans-unit>
        <trans-unit id="8c7fa59e2b8c1ce3abb542d1d5f76caddd1816f1" translate="yes" xml:space="preserve">
          <source>When this environment variable is set to a non zero value, scikit-learn uses the site joblib rather than its vendored version. Consequently, joblib must be installed for scikit-learn to run. Note that using the site joblib is at your own risks: the versions of scikt-learn and joblib need to be compatible. In addition, dumps from joblib.Memory might be incompatible, and you might loose some caches and have to redownload some datasets.</source>
          <target state="translated">이 환경 변수가 0이 아닌 값으로 설정되면 scikit-learn은 공급 업체 버전이 아닌 사이트 joblib를 사용합니다. 따라서 scikit-learn을 실행하려면 joblib이 설치되어 있어야합니다. 사이트 joblib를 사용하는 것은 사용자 자신의 책임입니다. scikt-learn 및 joblib의 버전은 호환 가능해야합니다. 또한 joblib.Memory의 덤프가 호환되지 않을 수 있으며 일부 캐시를 느슨하게하고 일부 데이터 세트를 다시 다운로드해야 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="743d4762d28a3d61488ddf12d10bce762b152657" translate="yes" xml:space="preserve">
          <source>When this environment variable is set to a non zero value, the tests that need network access are skipped.</source>
          <target state="translated">이 환경 변수가 0이 아닌 값으로 설정되면 네트워크 액세스가 필요한 테스트를 건너 뜁니다.</target>
        </trans-unit>
        <trans-unit id="beb3490e0a85d3bcf9f4888cb75a6b1ea2e1e6a8" translate="yes" xml:space="preserve">
          <source>When training an SVM with the &lt;em&gt;Radial Basis Function&lt;/em&gt; (RBF) kernel, two parameters must be considered: &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt;. The parameter &lt;code&gt;C&lt;/code&gt;, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low &lt;code&gt;C&lt;/code&gt; makes the decision surface smooth, while a high &lt;code&gt;C&lt;/code&gt; aims at classifying all training examples correctly. &lt;code&gt;gamma&lt;/code&gt; defines how much influence a single training example has. The larger &lt;code&gt;gamma&lt;/code&gt; is, the closer other examples must be to be affected.</source>
          <target state="translated">RBF ( &lt;em&gt;Radial Basis Function&lt;/em&gt; ) 커널을 사용 하여 SVM을 학습 할 때는 두 개의 매개 변수 &lt;code&gt;C&lt;/code&gt; 와 &lt;code&gt;gamma&lt;/code&gt; 를 고려해야합니다 . 모든 SVM 커널에 공통적 인 매개 변수 &lt;code&gt;C&lt;/code&gt; 는 의사 결정 표면의 단순성에 대한 교육 예제의 오 분류를 제거합니다. &lt;code&gt;C&lt;/code&gt; 가 낮 으면 결정 표면이 매끄럽고 &lt;code&gt;C&lt;/code&gt; 가 높으면 모든 교육 예제를 올바르게 분류하는 것입니다. &lt;code&gt;gamma&lt;/code&gt; 는 단일 훈련 예제가 얼마나 많은 영향을 미치는지를 정의합니다. &lt;code&gt;gamma&lt;/code&gt; 가 클수록 다른 예제에 더 가깝게 영향을 주어야합니다.</target>
        </trans-unit>
        <trans-unit id="4f5d7a3d8a7ab119798fbec5ead8471db219e63d" translate="yes" xml:space="preserve">
          <source>When true, the result is adjusted for chance, so that random performance would score 0, and perfect performance scores 1.</source>
          <target state="translated">true 인 경우 결과는 확률에 따라 조정되므로 임의의 성능은 0, 완벽한 성능은 1입니다.</target>
        </trans-unit>
        <trans-unit id="a756308318fb23f07be0498c8c83a0d088f9279a" translate="yes" xml:space="preserve">
          <source>When truncated SVD is applied to term-document matrices (as returned by &lt;code&gt;CountVectorizer&lt;/code&gt; or &lt;code&gt;TfidfVectorizer&lt;/code&gt;), this transformation is known as &lt;a href=&quot;http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf&quot;&gt;latent semantic analysis&lt;/a&gt; (LSA), because it transforms such matrices to a &amp;ldquo;semantic&amp;rdquo; space of low dimensionality. In particular, LSA is known to combat the effects of synonymy and polysemy (both of which roughly mean there are multiple meanings per word), which cause term-document matrices to be overly sparse and exhibit poor similarity under measures such as cosine similarity.</source>
          <target state="translated">잘린 SVD가 용어 문서 행렬에 적용되면 ( &lt;code&gt;CountVectorizer&lt;/code&gt; 또는 &lt;code&gt;TfidfVectorizer&lt;/code&gt; 에 의해 반환 됨 )이 변환은 이러한 행렬을 낮은 차원의 &quot;의미 적&quot;공간으로 변환하기 때문에 LSA ( &lt;a href=&quot;http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf&quot;&gt;잠재적 의미 분석&lt;/a&gt; )라고합니다. 특히, LSA는 동의어와 다 가공의 효과 (단어 단어 당 여러 의미가 있음을 의미 함)와 싸우는 것으로 알려져 있으며, 이로 인해 용어 문서 행렬이 지나치게 희박 해지고 코사인 유사성 등의 측정에서 유사성이 떨어집니다.</target>
        </trans-unit>
        <trans-unit id="726430099b436b4edbed2772b4e46aec59296fe0" translate="yes" xml:space="preserve">
          <source>When used for text classification with tf-idf vectors, this classifier is also known as the Rocchio classifier.</source>
          <target state="translated">tf-idf 벡터로 텍스트 분류에 사용될 때이 분류기는 Rocchio 분류기로도 알려져 있습니다.</target>
        </trans-unit>
        <trans-unit id="b1373e66b4937bbac8defef2fa925bed61a8d596" translate="yes" xml:space="preserve">
          <source>When used to &lt;em&gt;transform&lt;/em&gt; data, PCA can reduce the dimensionality of the data by projecting on a principal subspace.</source>
          <target state="translated">데이터 를 &lt;em&gt;변환&lt;/em&gt; 하는 데 사용될 때 PCA는 기본 서브 스페이스에 투영하여 데이터의 차원을 줄일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="93429c223efcd8d6622a4c8e5f0e71f13358e226" translate="yes" xml:space="preserve">
          <source>When using &lt;a href=&quot;../../modules/classes#module-sklearn.multiclass&quot;&gt;&lt;code&gt;multiclass classifiers&lt;/code&gt;&lt;/a&gt;, the learning and prediction task that is performed is dependent on the format of the target data fit upon:</source>
          <target state="translated">&lt;a href=&quot;../../modules/classes#module-sklearn.multiclass&quot;&gt; &lt;code&gt;multiclass classifiers&lt;/code&gt; &lt;/a&gt; 사용할 때 수행되는 학습 및 예측 작업은 다음에 맞는 대상 데이터의 형식에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="cb56aa339cbb48c9a75d42d2c7bf7e7858b3170b" translate="yes" xml:space="preserve">
          <source>When using ensemble methods base upon bagging, i.e. generating new training sets using sampling with replacement, part of the training set remains unused. For each classifier in the ensemble, a different part of the training set is left out.</source>
          <target state="translated">배깅을 기반으로 앙상블 방법을 사용할 때, 즉 교체와 함께 샘플링을 사용하여 새 훈련 세트를 생성하는 경우 훈련 세트의 일부는 사용되지 않습니다. 앙상블의 각 분류 자에 대해 훈련 세트의 다른 부분이 제외됩니다.</target>
        </trans-unit>
        <trans-unit id="a9696826aefafecc5b32845bc270f3c2cabe6664" translate="yes" xml:space="preserve">
          <source>When using these images, please give credit to AT&amp;amp;T Laboratories Cambridge.</source>
          <target state="translated">이 이미지를 사용할 때는 AT &amp;amp; T Laboratories Cambridge에 감사의 뜻을 전하십시오.</target>
        </trans-unit>
        <trans-unit id="5a6f9e7437ff7d6762455e24a46c4771dc2e0a37" translate="yes" xml:space="preserve">
          <source>When using, for example, &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;cross validation&lt;/a&gt;, to set the amount of regularization with &lt;code&gt;C&lt;/code&gt;, there will be a different amount of samples between the main problem and the smaller problems within the folds of the cross validation.</source>
          <target state="translated">예를 들어, &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;교차 검증&lt;/a&gt; 을 사용하여 &lt;code&gt;C&lt;/code&gt; 로 정규화 양을 설정 하면 교차 검증의 접힘 내에서 주요 문제와 작은 문제 사이에 다른 양의 샘플이 있습니다.</target>
        </trans-unit>
        <trans-unit id="5dae0cb2a4d8c33bb8e68ee9ffd452db73e27eb2" translate="yes" xml:space="preserve">
          <source>When we apply clustering to the data, we find that the clustering reflects what was in the distance matrices. Indeed, for the Euclidean distance, the classes are ill-separated because of the noise, and thus the clustering does not separate the waveforms. For the cityblock distance, the separation is good and the waveform classes are recovered. Finally, the cosine distance does not separate at all waveform 1 and 2, thus the clustering puts them in the same cluster.</source>
          <target state="translated">데이터에 군집화를 적용하면 군집화에 거리 행렬의 내용이 반영됩니다. 실제로 유클리드 거리의 경우 노이즈로 인해 클래스가 잘못 분리되므로 클러스터링이 파형을 분리하지 않습니다. 도시 블록 거리의 경우 분리가 양호하고 파형 클래스가 복구됩니다. 마지막으로 코사인 거리는 모든 파형 1과 2에서 분리되지 않으므로 클러스터링은 동일한 클러스터에 배치합니다.</target>
        </trans-unit>
        <trans-unit id="8975b7dd097c19a6af3c2b4b090f357f96aab52d" translate="yes" xml:space="preserve">
          <source>When working with covariance estimation, the usual approach is to use a maximum likelihood estimator, such as the &lt;a href=&quot;../../modules/generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt;&lt;code&gt;sklearn.covariance.EmpiricalCovariance&lt;/code&gt;&lt;/a&gt;. It is unbiased, i.e. it converges to the true (population) covariance when given many observations. However, it can also be beneficial to regularize it, in order to reduce its variance; this, in turn, introduces some bias. This example illustrates the simple regularization used in &lt;a href=&quot;../../modules/covariance#shrunk-covariance&quot;&gt;Shrunk Covariance&lt;/a&gt; estimators. In particular, it focuses on how to set the amount of regularization, i.e. how to choose the bias-variance trade-off.</source>
          <target state="translated">공분산 추정으로 작업 할 때 일반적인 접근 방식은 &lt;a href=&quot;../../modules/generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt; &lt;code&gt;sklearn.covariance.EmpiricalCovariance&lt;/code&gt; &lt;/a&gt; 와 같은 최대 가능성 추정기를 사용하는 것 입니다. 그것은 편향되지 않으며, 즉 많은 관측치가 주어지면 실제 (인구) 공분산으로 수렴합니다. 그러나 분산을 줄이려면 정규화하는 것이 좋습니다. 이것은 차례로 약간의 편견을 가져옵니다. 이 예는 &lt;a href=&quot;../../modules/covariance#shrunk-covariance&quot;&gt;Shrunk Covariance&lt;/a&gt; Estimators 에서 사용되는 간단한 정규화를 보여줍니다 . 특히, 정규화의 양을 설정하는 방법, 즉 바이어스-분산 트레이드 오프를 선택하는 방법에 중점을 둡니다.</target>
        </trans-unit>
        <trans-unit id="17b704aa73a46ef6f9edddecff620d33c0b705d7" translate="yes" xml:space="preserve">
          <source>When you want to apply different transformations to each field of the data, see the related class &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt; (see &lt;a href=&quot;#column-transformer&quot;&gt;user guide&lt;/a&gt;).</source>
          <target state="translated">데이터의 각 필드에 다른 변환을 적용하려면 관련 클래스 &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt; 를&lt;/a&gt; 참조하십시오 ( &lt;a href=&quot;#column-transformer&quot;&gt;사용자 안내서&lt;/a&gt; 참조 ).</target>
        </trans-unit>
        <trans-unit id="ffd889b6ef09600260c419603787a00c67ae551d" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;TP&lt;/code&gt; is the number of &lt;strong&gt;True Positive&lt;/strong&gt; (i.e. the number of pair of points that belong to the same clusters in both the true labels and the predicted labels), &lt;code&gt;FP&lt;/code&gt; is the number of &lt;strong&gt;False Positive&lt;/strong&gt; (i.e. the number of pair of points that belong to the same clusters in the true labels and not in the predicted labels) and &lt;code&gt;FN&lt;/code&gt; is the number of &lt;strong&gt;False Negative&lt;/strong&gt; (i.e the number of pair of points that belongs in the same clusters in the predicted labels and not in the true labels).</source>
          <target state="translated">어디 &lt;code&gt;TP&lt;/code&gt; 가 의 수입니다 &lt;strong&gt;진정한 포지티브&lt;/strong&gt; (즉, 진정한 레이블과 예측 라벨 모두 동일한 클러스터에 속하는 점 쌍의 수), &lt;code&gt;FP&lt;/code&gt; 는 의 수 &lt;strong&gt;거짓 양성&lt;/strong&gt; (속하는 점의 쌍, 즉 수 진정한 라벨에서가 아니라 예측 라벨에서)와 같은 클러스터 &lt;code&gt;FN&lt;/code&gt; 의 수 &lt;strong&gt;위음성&lt;/strong&gt; (예측 된 라벨에서가 아니라) 실제 라벨에서 동일한 클러스터에 속하는 점의 쌍, 즉 수.</target>
        </trans-unit>
        <trans-unit id="276f699fa82da6208d110c0a23b40d61550922dd" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;TP&lt;/code&gt; is the number of &lt;strong&gt;True Positive&lt;/strong&gt; (i.e. the number of pair of points that belongs in the same clusters in both &lt;code&gt;labels_true&lt;/code&gt; and &lt;code&gt;labels_pred&lt;/code&gt;), &lt;code&gt;FP&lt;/code&gt; is the number of &lt;strong&gt;False Positive&lt;/strong&gt; (i.e. the number of pair of points that belongs in the same clusters in &lt;code&gt;labels_true&lt;/code&gt; and not in &lt;code&gt;labels_pred&lt;/code&gt;) and &lt;code&gt;FN&lt;/code&gt; is the number of &lt;strong&gt;False Negative&lt;/strong&gt; (i.e the number of pair of points that belongs in the same clusters in &lt;code&gt;labels_pred&lt;/code&gt; and not in &lt;code&gt;labels_True&lt;/code&gt;).</source>
          <target state="translated">어디 &lt;code&gt;TP&lt;/code&gt; 가 의 수입니다 &lt;strong&gt;진정한 양성&lt;/strong&gt; (모두 같은 클러스터에 속하는 점의 쌍, 즉 수 &lt;code&gt;labels_true&lt;/code&gt; 및 &lt;code&gt;labels_pred&lt;/code&gt; 가 ), &lt;code&gt;FP&lt;/code&gt; 는 의 수를 &lt;strong&gt;거짓 긍정적&lt;/strong&gt; 동일한 클러스터에 속하는 (점의 쌍, 즉 수 에서 &lt;code&gt;labels_true&lt;/code&gt; 및하지에서 &lt;code&gt;labels_pred&lt;/code&gt; )와 &lt;code&gt;FN&lt;/code&gt; 은 의 수 &lt;strong&gt;위음성&lt;/strong&gt; (동일한 클러스터에 속하는 점의 쌍, 즉 수 &lt;code&gt;labels_pred&lt;/code&gt; 및하지에서 &lt;code&gt;labels_True&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="e78195e2eb2711f3bb8a0d7f4e3c56eea492c48d" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;delta&lt;/code&gt; is a free parameter representing the width of the Gaussian kernel.</source>
          <target state="translated">여기서 &lt;code&gt;delta&lt;/code&gt; 는 가우스 커널의 너비를 나타내는 자유 매개 변수입니다.</target>
        </trans-unit>
        <trans-unit id="6d88fb8179777bb060d39d8e880a1a6ec89efb59" translate="yes" xml:space="preserve">
          <source>Where C is the number of permutations whose score &amp;gt;= the true score.</source>
          <target state="translated">여기서 C는 점수&amp;gt; = 실제 점수 인 순열 수입니다.</target>
        </trans-unit>
        <trans-unit id="0426d1b8d26623c0079356962f68cf2595f6d67a" translate="yes" xml:space="preserve">
          <source>Where D is the matrix of distances for the input data X, D_fit is the matrix of distances for the output embedding X_fit, and K is the isomap kernel:</source>
          <target state="translated">여기서 D는 입력 데이터 X에 대한 거리의 행렬이고, D_fit은 X_fit을 포함하는 출력에 대한 거리의 행렬이며, K는 아이소 맵 커널입니다.</target>
        </trans-unit>
        <trans-unit id="77844a8258430d31f41a5c27fd5c3c817a4c46f5" translate="yes" xml:space="preserve">
          <source>Where \(C_2^{n_{samples}}\) is the total number of possible pairs in the dataset (without ordering).</source>
          <target state="translated">여기서 \ (C_2 ^ {n_ {samples}} \)는 데이터 세트에서 가능한 순서의 총 수입니다 (순서없이).</target>
        </trans-unit>
        <trans-unit id="45f9706f8e40bfee3b8f4082279b7c1694d8aead" translate="yes" xml:space="preserve">
          <source>Where \(K\) is the precision matrix to be estimated, and \(S\) is the sample covariance matrix. \(\|K\|_1\) is the sum of the absolute values of off-diagonal coefficients of \(K\). The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R &lt;code&gt;glasso&lt;/code&gt; package.</source>
          <target state="translated">여기서 \ (K \)는 추정 할 정밀 행렬이고 \ (S \)는 샘플 공분산 행렬입니다. \ (\ | K \ | _1 \)는 \ (K \)의 대각 외 계수의 절대 값의 합입니다. 이 문제를 해결하기 위해 사용 된 알고리즘은 Friedman 2008 Biostatistics 논문의 GLasso 알고리즘입니다. R &lt;code&gt;glasso&lt;/code&gt; 패키지 와 동일한 알고리즘 입니다.</target>
        </trans-unit>
        <trans-unit id="23038cc6fb25ab648004d5485267f6db76cb9eda" translate="yes" xml:space="preserve">
          <source>Where \(N(x_i)\) is the neighborhood of samples within a given distance around \(x_i\) and \(m\) is the &lt;em&gt;mean shift&lt;/em&gt; vector that is computed for each centroid that points towards a region of the maximum increase in the density of points. This is computed using the following equation, effectively updating a centroid to be the mean of the samples within its neighborhood:</source>
          <target state="translated">여기서 \ (N (x_i) \)는 \ (x_i \) 주위의 주어진 거리 내의 샘플의 이웃이고 \ (m \)은 최대 증가 영역을 가리키는 각 중심에 대해 계산 된 &lt;em&gt;평균 이동&lt;/em&gt; 벡터입니다. 점의 밀도. 이것은 다음 방정식을 사용하여 계산되며, 중심 근처를 효과적으로 주변의 샘플 평균으로 업데이트합니다.</target>
        </trans-unit>
        <trans-unit id="c4be16a40b65a723b122e1219966e4db066c1f56" translate="yes" xml:space="preserve">
          <source>Where \(R\) is the diagonal matrix with entry \(i\) equal to \(\sum_{j} A_{ij}\) and \(C\) is the diagonal matrix with entry \(j\) equal to \(\sum_{i} A_{ij}\).</source>
          <target state="translated">여기서 \ (R \)은 \ (i \) 항목이 \ (\ sum_ {j} A_ {ij} \)와 같은 대각 행렬이고 \ (C \)는 \ (j \) 항목이 같은 대각 행렬입니다. \ (\ sum_ {i} A_ {ij} \)까지</target>
        </trans-unit>
        <trans-unit id="06bd15907339a321f45a7707ee037e3bf4bb294c" translate="yes" xml:space="preserve">
          <source>Where \(\langle \cdot, \cdot \rangle\) denotes the inner product in the Hilbert space.</source>
          <target state="translated">여기서 \ (\ langle \ cdot, \ cdot \ rangle \)은 힐버트 공간의 내부 제품을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="b505305e3f68e0055136674f6671623549265da7" translate="yes" xml:space="preserve">
          <source>Where \(\log_e (x)\) means the natural logarithm of \(x\). This metric is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate.</source>
          <target state="translated">여기서 \ (\ log_e (x) \)는 \ (x \)의 자연 로그를 의미합니다. 이 지표는 인구 수, 수년에 걸친 상품의 평균 판매 등과 같이 지수 성장이있는 대상이 사용될 때 사용하는 것이 가장 좋습니다.이 지표는 예측치가 과대 평가 된 것보다 크게 예측하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="dc652afd01d2a671ac597240d27fcb8fb6f2cb88" translate="yes" xml:space="preserve">
          <source>Where \(s(i, k)\) is the similarity between samples \(i\) and \(k\). The availability of sample \(k\) to be the exemplar of sample \(i\) is given by:</source>
          <target state="translated">여기서 \ (s (i, k) \)는 샘플 \ (i \)와 \ (k \)의 유사성입니다. 샘플 \ (i \)의 예가 될 수있는 샘플 \ (k \)의 가용성은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="9fa1e5b532b4ed4720d23061371103281dda3d83" translate="yes" xml:space="preserve">
          <source>Where r is defined per sample, we need to make use of &lt;code&gt;start&lt;/code&gt;:</source>
          <target state="translated">r이 샘플마다 정의되는 경우 &lt;code&gt;start&lt;/code&gt; 를 사용해야 합니다 .</target>
        </trans-unit>
        <trans-unit id="c16b06fa7e959786262fbf5823a1d1a66514be0c" translate="yes" xml:space="preserve">
          <source>Where the step length \(\gamma_m\) is chosen using line search:</source>
          <target state="translated">행 길이를 사용하여 스텝 길이 \ (\ gamma_m \)를 선택한 경우 :</target>
        </trans-unit>
        <trans-unit id="1e4c7785f80a06d28d2e2b965c377764abc1c026" translate="yes" xml:space="preserve">
          <source>Where to from here</source>
          <target state="translated">여기서 어디로</target>
        </trans-unit>
        <trans-unit id="17eb390ca1dec9880beb722610077dafb8edc9ac" translate="yes" xml:space="preserve">
          <source>Where u and v are any rows taken from a dataset of shape [n_samples, n_features] and p is a projection by a random Gaussian N(0, 1) matrix with shape [n_components, n_features] (or a sparse Achlioptas matrix).</source>
          <target state="translated">여기서 u와 v는 모양 [n_samples, n_features]의 데이터 집합에서 가져온 행이고 p는 모양이 [n_components, n_features] (또는 희소 한 Achlioptas 행렬) 인 임의 가우스 N (0, 1) 행렬에 의한 투영입니다.</target>
        </trans-unit>
        <trans-unit id="c5759c4abe89df832c23e0269eba38e4f1ad0ad7" translate="yes" xml:space="preserve">
          <source>Where u and v are any rows taken from a dataset of shape [n_samples, n_features], eps is in ]0, 1[ and p is a projection by a random Gaussian N(0, 1) matrix with shape [n_components, n_features] (or a sparse Achlioptas matrix).</source>
          <target state="translated">u와 v는 모양 [n_samples, n_features]의 데이터 세트에서 가져온 행이며, eps는] 0, 1 [에 있으며 p는 [n_components, n_features] 모양의 임의 가우스 N (0, 1) 행렬에 의한 투영입니다. (또는 희소 Achlioptas 매트릭스).</target>
        </trans-unit>
        <trans-unit id="7e741bc3dcef0123eeda11543758853be2aac149" translate="yes" xml:space="preserve">
          <source>Where:</source>
          <target state="translated">Where:</target>
        </trans-unit>
        <trans-unit id="07f1abf8acdb3dcd49bde3ee8a201c3421831b31" translate="yes" xml:space="preserve">
          <source>Whether &lt;code&gt;feature_names_&lt;/code&gt; and &lt;code&gt;vocabulary_&lt;/code&gt; should be sorted when fitting. True by default.</source>
          <target state="translated">피팅 할 때 &lt;code&gt;feature_names_&lt;/code&gt; 및 &lt;code&gt;vocabulary_&lt;/code&gt; 를 정렬해야하는지 여부 입니다. 기본적으로 true입니다.</target>
        </trans-unit>
        <trans-unit id="e61b5eefa6a0d8caaa65c0cf06600523e6eded8c" translate="yes" xml:space="preserve">
          <source>Whether a forced copy will be triggered. If copy=False, a copy might be triggered by a conversion.</source>
          <target state="translated">강제 복사가 트리거되는지 여부 copy = False이면 변환으로 인해 복사가 트리거 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6817dee267fec06b200988a35092c9fafdb28df4" translate="yes" xml:space="preserve">
          <source>Whether a prefit model is expected to be passed into the constructor directly or not. If True, &lt;code&gt;transform&lt;/code&gt; must be called directly and SelectFromModel cannot be used with &lt;code&gt;cross_val_score&lt;/code&gt;, &lt;code&gt;GridSearchCV&lt;/code&gt; and similar utilities that clone the estimator. Otherwise train the model using &lt;code&gt;fit&lt;/code&gt; and then &lt;code&gt;transform&lt;/code&gt; to do feature selection.</source>
          <target state="translated">프리 피트 모델이 생성자로 직접 전달 될지 여부입니다. True 인 경우 &lt;code&gt;transform&lt;/code&gt; 직접 호출해야하며 SelectFromModel을 &lt;code&gt;cross_val_score&lt;/code&gt; , &lt;code&gt;GridSearchCV&lt;/code&gt; 및 추정기를 복제하는 유사한 유틸리티와 함께 사용할 수 없습니다 . 그렇지 않으면 &lt;code&gt;fit&lt;/code&gt; 사용하여 모델을 학습 한 다음 형상 선택을 수행하도록 &lt;code&gt;transform&lt;/code&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="e67f675f1639113224879739e0229eb2671df0d3" translate="yes" xml:space="preserve">
          <source>Whether an array will be forced to be fortran or c-style.</source>
          <target state="translated">배열을 강제 또는 c 스타일로 강제할지 여부</target>
        </trans-unit>
        <trans-unit id="57d1f88164d54372295bc307285273118c662089" translate="yes" xml:space="preserve">
          <source>Whether an array will be forced to be fortran or c-style. When order is None (default), then if copy=False, nothing is ensured about the memory layout of the output array; otherwise (copy=True) the memory layout of the returned array is kept as close as possible to the original array.</source>
          <target state="translated">배열을 강제 또는 c 스타일로 강제할지 여부 order가 None (기본값) 인 경우 copy = False이면 출력 배열의 메모리 레이아웃에 대해 아무 것도 보장되지 않습니다. 그렇지 않으면 (copy = True) 반환 된 배열의 메모리 레이아웃이 원래 배열에 최대한 가깝게 유지됩니다.</target>
        </trans-unit>
        <trans-unit id="f6541283616583040ce3e73f070cbb436dbc5da9" translate="yes" xml:space="preserve">
          <source>Whether bootstrap samples are used when building trees.</source>
          <target state="translated">나무를 만들 때 부트 스트랩 샘플을 사용할지 여부</target>
        </trans-unit>
        <trans-unit id="344e324f858395af07d0534305d3cd485a522869" translate="yes" xml:space="preserve">
          <source>Whether column indices in f are zero-based (True) or one-based (False). If column indices are one-based, they are transformed to zero-based to match Python/NumPy conventions. If set to &amp;ldquo;auto&amp;rdquo;, a heuristic check is applied to determine this from the file contents. Both kinds of files occur &amp;ldquo;in the wild&amp;rdquo;, but they are unfortunately not self-identifying. Using &amp;ldquo;auto&amp;rdquo; or True should always be safe when no &lt;code&gt;offset&lt;/code&gt; or &lt;code&gt;length&lt;/code&gt; is passed. If &lt;code&gt;offset&lt;/code&gt; or &lt;code&gt;length&lt;/code&gt; are passed, the &amp;ldquo;auto&amp;rdquo; mode falls back to &lt;code&gt;zero_based=True&lt;/code&gt; to avoid having the heuristic check yield inconsistent results on different segments of the file.</source>
          <target state="translated">f의 열 인덱스가 0부터 시작하는지 (True) 또는 1에서 시작인지 (False). 열 인덱스가 1부터 시작하면 Python / NumPy 규칙과 일치하도록 0부터 시작합니다. &quot;auto&quot;로 설정하면 휴리스틱 검사가 적용되어 파일 내용에서이를 확인합니다. 두 종류의 파일 모두&amp;ldquo;와일드하게&amp;rdquo;발생하지만 불행히도 자체 식별은 아닙니다. &lt;code&gt;offset&lt;/code&gt; 이나 &lt;code&gt;length&lt;/code&gt; 가 전달 되지 않으면 &quot;auto&quot;또는 True를 사용하는 것이 항상 안전해야합니다 . 경우 &lt;code&gt;offset&lt;/code&gt; 또는 &lt;code&gt;length&lt;/code&gt; 전달, &quot;자동&quot;모드는 다시 하락한다 &lt;code&gt;zero_based=True&lt;/code&gt; 파일의 다른 세그먼트에 휴리스틱 검사 수율 일치하지 않는 결과를 피하기 위해.</target>
        </trans-unit>
        <trans-unit id="f00bfe1387e4927051573cb3d574287560206c66" translate="yes" xml:space="preserve">
          <source>Whether column indices in f are zero-based (True) or one-based (False). If column indices are one-based, they are transformed to zero-based to match Python/NumPy conventions. If set to &amp;ldquo;auto&amp;rdquo;, a heuristic check is applied to determine this from the file contents. Both kinds of files occur &amp;ldquo;in the wild&amp;rdquo;, but they are unfortunately not self-identifying. Using &amp;ldquo;auto&amp;rdquo; or True should always be safe when no offset or length is passed. If offset or length are passed, the &amp;ldquo;auto&amp;rdquo; mode falls back to zero_based=True to avoid having the heuristic check yield inconsistent results on different segments of the file.</source>
          <target state="translated">f의 열 인덱스가 0부터 시작하는지 (True) 또는 1에서 시작인지 (False). 열 인덱스가 1부터 시작하면 Python / NumPy 규칙과 일치하도록 0부터 시작합니다. &quot;auto&quot;로 설정하면 휴리스틱 검사가 적용되어 파일 내용에서이를 확인합니다. 두 종류의 파일 모두&amp;ldquo;와일드하게&amp;rdquo;발생하지만 불행히도 자체 식별은 아닙니다. 오프셋이나 길이가 전달되지 않으면 &quot;auto&quot;또는 True를 사용하는 것이 항상 안전해야합니다. 오프셋 또는 길이가 전달되면 &quot;자동&quot;모드는 파일의 다른 세그먼트에서 휴리스틱 검사 결과가 일치하지 않는 결과를 피하기 위해 zero_based = True로 돌아갑니다.</target>
        </trans-unit>
        <trans-unit id="be05ee9a303aba9073e602f5af4606db8dba467c" translate="yes" xml:space="preserve">
          <source>Whether column indices should be written zero-based (True) or one-based (False).</source>
          <target state="translated">열 인덱스를 0부터 시작하는지 (True) 또는 1부터 시작해야하는지 (False).</target>
        </trans-unit>
        <trans-unit id="426c392bb98aa08b186e867710abfa024378fa01" translate="yes" xml:space="preserve">
          <source>Whether features are drawn with replacement.</source>
          <target state="translated">기능을 교체하여 그릴 지 여부</target>
        </trans-unit>
        <trans-unit id="b1153e9c8e50c0d286811aaf218a31fa047ae93d" translate="yes" xml:space="preserve">
          <source>Whether or not a second normalization of the weights is performed. The default behavior mirrors the implementations found in Mahout and Weka, which do not follow the full algorithm described in Table 9 of the paper.</source>
          <target state="translated">가중치의 두 번째 정규화 수행 여부입니다. 기본 동작은 Mahout 및 Weka에있는 구현을 반영하며,이 문서의 표 9에 설명 된 전체 알고리즘을 따르지 않습니다.</target>
        </trans-unit>
        <trans-unit id="1500a013d74d8899d6c67c8489e35470d425474d" translate="yes" xml:space="preserve">
          <source>Whether or not the model should use an intercept, i.e. a biased hyperplane, is controlled by the parameter &lt;code&gt;fit_intercept&lt;/code&gt;.</source>
          <target state="translated">모델이 인터셉트 (즉, 바이어스 된 초평면)를 사용해야하는지 여부는 &lt;code&gt;fit_intercept&lt;/code&gt; 매개 변수로 제어됩니다 .</target>
        </trans-unit>
        <trans-unit id="c17699d69c93a1c9674665b22c836f689e7a71a8" translate="yes" xml:space="preserve">
          <source>Whether or not the training data should be shuffled after each epoch.</source>
          <target state="translated">각 에포크 후에 트레이닝 데이터를 섞어 야하는지의 여부.</target>
        </trans-unit>
        <trans-unit id="2f96d4cd24a907c94c91a597b369db5ae9d183fe" translate="yes" xml:space="preserve">
          <source>Whether or not the training data should be shuffled after each epoch. Defaults to True.</source>
          <target state="translated">각 에포크 후에 트레이닝 데이터를 섞어 야하는지의 여부. 기본값은 True입니다.</target>
        </trans-unit>
        <trans-unit id="21e287c040da0ab9f7612eda4a9df397d21447be" translate="yes" xml:space="preserve">
          <source>Whether or not to compute labels for each fit.</source>
          <target state="translated">각 적합치에 대한 레이블을 계산할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="ae7627e3aed47d9187a8dde354d4bd8908f66e18" translate="yes" xml:space="preserve">
          <source>Whether or not to consider raw Mahalanobis distances as the decision function. Must be False (default) for compatibility with the others outlier detection tools.</source>
          <target state="translated">원시 Mahalanobis 거리를 결정 기능으로 고려할지 여부. 다른 이상치 탐지 도구와의 호환성을 위해 False (기본값) 여야합니다.</target>
        </trans-unit>
        <trans-unit id="d7d36162415efc2dece0359749393df764e8f212" translate="yes" xml:space="preserve">
          <source>Whether or not to fit the intercept. This can be set to False if the data is already centered around the origin.</source>
          <target state="translated">절편에 맞는지 여부. 데이터가 이미 원점을 중심으로하는 경우 False로 설정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="99ecd63bc30b233e173e08d6a58d2b609112b08a" translate="yes" xml:space="preserve">
          <source>Whether or not to make a copy of the given data. If set to False, the initial data will be overwritten.</source>
          <target state="translated">주어진 데이터의 사본을 만들지 여부. False로 설정하면 초기 데이터를 덮어 씁니다.</target>
        </trans-unit>
        <trans-unit id="0d8aa347bdbdfa6dd79051c8fee2f95fc5666522" translate="yes" xml:space="preserve">
          <source>Whether or not to mark each sample as the first nearest neighbor to itself. If &lt;code&gt;None&lt;/code&gt;, then True is used for mode=&amp;rsquo;connectivity&amp;rsquo; and False for mode=&amp;rsquo;distance&amp;rsquo; as this will preserve backwards compatibility.</source>
          <target state="translated">각 샘플을 가장 가까운 이웃으로 표시할지 여부입니다. 경우 &lt;code&gt;None&lt;/code&gt; , 다음 진정한 모드 = '연결'을 위해 사용하지 않고 모드 False입니다 =이 같은 '거리'는 이전 버전과의 호환성을 유지합니다.</target>
        </trans-unit>
        <trans-unit id="a1d6eb056f01f6a77d40d6f787612d8008f1be4b" translate="yes" xml:space="preserve">
          <source>Whether or not to return a sparse CSR matrix, as default behavior, or to return a dense array compatible with dense pipeline operators.</source>
          <target state="translated">스파 스 CSR 매트릭스를 기본 동작으로 반환할지 또는 밀도가 높은 파이프 라인 연산자와 호환되는 밀도 배열을 반환할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="8f592bf838896fb605ecc15c063970bf58250eab" translate="yes" xml:space="preserve">
          <source>Whether or not to return the number of iterations.</source>
          <target state="translated">반복 횟수를 리턴할지 여부.</target>
        </trans-unit>
        <trans-unit id="3433b032133d6a741db0a6ccce9ce8005a84877d" translate="yes" xml:space="preserve">
          <source>Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.</source>
          <target state="translated">분할하기 전에 데이터를 셔플 링할지 여부입니다. shuffle = False 인 경우 계층화는 없음이어야합니다.</target>
        </trans-unit>
        <trans-unit id="d797771ee8d7ac8a664344b3d1655c54bf4a7c5a" translate="yes" xml:space="preserve">
          <source>Whether or not to shuffle the data: might be important for models that make the assumption that the samples are independent and identically distributed (i.i.d.), such as stochastic gradient descent.</source>
          <target state="translated">데이터 셔플 링 여부 : 표본이 확률 적 경사 하강과 같이 독립적이고 동일하게 분포되어 있다고 가정하는 모델에 중요 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="169aa17090e9b82766c818a4a09acd1cb51fcb24" translate="yes" xml:space="preserve">
          <source>Whether samples are drawn with replacement.</source>
          <target state="translated">교체로 샘플을 채취할지 여부</target>
        </trans-unit>
        <trans-unit id="b61c81ce74fc75ce6a90c790bfafe984d14c7994" translate="yes" xml:space="preserve">
          <source>Whether score_func is a score function (default), meaning high is good, or a loss function, meaning low is good. In the latter case, the scorer object will sign-flip the outcome of the score_func.</source>
          <target state="translated">score_func가 점수 함수 (기본값)인지, 높음을 의미하거나 손실 함수, 낮음을 의미합니다. 후자의 경우, 득점자 개체는 score_func의 결과를 부호 넘기 게됩니다.</target>
        </trans-unit>
        <trans-unit id="01c7a3b4947bb7aed2272a78dd753a14b9e24fdc" translate="yes" xml:space="preserve">
          <source>Whether score_func requires predict_proba to get probability estimates out of a classifier.</source>
          <target state="translated">score_func가 분류기에서 확률 추정치를 얻기 위해 predict_proba가 필요한지 여부</target>
        </trans-unit>
        <trans-unit id="e86601bb1c2db478564381eb9b1a33fc72f3ad69" translate="yes" xml:space="preserve">
          <source>Whether score_func takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method.</source>
          <target state="translated">score_func가 지속적인 의사 결정을 수행하는지 여부 이는 decision_function 또는 predict_proba 메소드가있는 추정기를 사용하는 2 진 분류에만 작동합니다.</target>
        </trans-unit>
        <trans-unit id="62e384e32324c7d8c05ac06534cda98d3cbc0def" translate="yes" xml:space="preserve">
          <source>Whether support is a list of indices.</source>
          <target state="translated">지원이 지수 목록인지 여부.</target>
        </trans-unit>
        <trans-unit id="970ea0e030e6e4be6469b0282edfb558e0e9066d" translate="yes" xml:space="preserve">
          <source>Whether the algorithm should be applied to M.T instead of M. The result should approximately be the same. The &amp;lsquo;auto&amp;rsquo; mode will trigger the transposition if M.shape[1] &amp;gt; M.shape[0] since this implementation of randomized SVD tend to be a little faster in that case.</source>
          <target state="translated">알고리즘이 M 대신 MT에 적용되어야하는지 여부. 결과는 대략 동일해야합니다. 이 경우 무작위 SVD의 구현이 약간 더 빠르기 때문에 M.shape [1]&amp;gt; M.shape [0] 인 경우 '자동'모드는 조옮김을 트리거합니다.</target>
        </trans-unit>
        <trans-unit id="3226951d2ead1297a694651602f8538f5e93757c" translate="yes" xml:space="preserve">
          <source>Whether the covariance vector Xy must be copied by the algorithm. If False, it may be overwritten.</source>
          <target state="translated">공분산 벡터 Xy를 알고리즘으로 복사해야하는지 여부 False이면 덮어 쓸 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d2b667c3b7746e3e678455d8b2d703997aeb5e60" translate="yes" xml:space="preserve">
          <source>Whether the deflation be done on a copy. Let the default value to True unless you don&amp;rsquo;t care about side effects</source>
          <target state="translated">사본에서 수축이 수행되는지 여부. 부작용에 신경 쓰지 않는 한 기본값을 True로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="1cba92780e42c1ebe55ada467260206516898de8" translate="yes" xml:space="preserve">
          <source>Whether the deflation should be done on a copy. Let the default value to True unless you don&amp;rsquo;t care about side effect</source>
          <target state="translated">디플레이션이 사본에서 수행되어야하는지 여부 부작용에 신경 쓰지 않는 한 기본값을 True로 설정하십시오.</target>
        </trans-unit>
        <trans-unit id="99f0df0508624fcfafef99671905c951f197a398" translate="yes" xml:space="preserve">
          <source>Whether the design matrix X must be copied by the algorithm. A false value is only helpful if X is already Fortran-ordered, otherwise a copy is made anyway.</source>
          <target state="translated">알고리즘으로 설계 행렬 X를 복사해야하는지 여부 잘못된 값은 X가 이미 주문 된 경우에만 유용합니다. 그렇지 않으면 복사가 이루어집니다.</target>
        </trans-unit>
        <trans-unit id="f62ef19aafedabcbf9aa2a6c1cbcccaa900e840f" translate="yes" xml:space="preserve">
          <source>Whether the feature should be made of word or character n-grams.</source>
          <target state="translated">기능이 단어 또는 문자 n- 그램으로 이루어져야하는지 여부</target>
        </trans-unit>
        <trans-unit id="9c1354d44a66effd212e821b1aba74fe08612248" translate="yes" xml:space="preserve">
          <source>Whether the feature should be made of word or character n-grams. Option &amp;lsquo;char_wb&amp;rsquo; creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.</source>
          <target state="translated">기능이 단어 또는 문자 n- 그램으로 이루어져야하는지 여부 옵션 'char_wb'는 단어 경계 안의 텍스트에서만 문자 n- 그램을 만듭니다. 단어 가장자리의 n- 그램은 공백으로 채워집니다.</target>
        </trans-unit>
        <trans-unit id="824ad07968fefbc8aa1fba0ade7c849f0d099562" translate="yes" xml:space="preserve">
          <source>Whether the gram matrix must be copied by the algorithm. A false value is only helpful if it is already Fortran-ordered, otherwise a copy is made anyway.</source>
          <target state="translated">알고리즘으로 그램 행렬을 복사해야하는지 여부 잘못된 값은 이미 주문한 경우에만 유용합니다. 그렇지 않으면 어쨌든 복사됩니다.</target>
        </trans-unit>
        <trans-unit id="23571fa5c9f0e8d5b2ce0915807aa480e23639d3" translate="yes" xml:space="preserve">
          <source>Whether the imputer mask format should be sparse or dense.</source>
          <target state="translated">임 피터 마스크 형식이 희박해야하는지, 아니면 조밀해야하는지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="b70758cdff0513f8822d9fb7393f16dda3f13af9" translate="yes" xml:space="preserve">
          <source>Whether the imputer mask should represent all or a subset of features.</source>
          <target state="translated">임 피터 마스크가 모든 기능 또는 일부 기능을 나타내는 지 여부</target>
        </trans-unit>
        <trans-unit id="11709e37813efd2480c1d891188cf0d8201c8a69" translate="yes" xml:space="preserve">
          <source>Whether the intercept should be estimated or not. If &lt;code&gt;False&lt;/code&gt;, the data is assumed to be already centered.</source>
          <target state="translated">절편을 추정해야하는지 여부 경우 &lt;code&gt;False&lt;/code&gt; 데이터가 이미 중심을 가정한다.</target>
        </trans-unit>
        <trans-unit id="0ccd5f6458ed970eecf03c787120d2831e50f140" translate="yes" xml:space="preserve">
          <source>Whether the intercept should be estimated or not. If False, the data is assumed to be already centered.</source>
          <target state="translated">절편을 추정해야하는지 여부 False이면 데이터가 이미 중앙에있는 것으로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="2fafec857734808cd372cb104d91a568d25acbf6" translate="yes" xml:space="preserve">
          <source>Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True.</source>
          <target state="translated">절편을 추정해야하는지 여부 False이면 데이터가 이미 중앙에있는 것으로 간주됩니다. 기본값은 True입니다.</target>
        </trans-unit>
        <trans-unit id="efe887a2120302f3ddf508ce39115dacb905298c" translate="yes" xml:space="preserve">
          <source>Whether the parameter was found to be a named parameter of the estimator&amp;rsquo;s fit method.</source>
          <target state="translated">매개 변수가 추정기의 적합 방법의 명명 된 매개 변수인지 여부</target>
        </trans-unit>
        <trans-unit id="e3e6070e7b1bf06bd46c63ade906ee83f84569ff" translate="yes" xml:space="preserve">
          <source>Whether the power iterations are normalized with step-by-step QR factorization (the slowest but most accurate), &amp;lsquo;none&amp;rsquo; (the fastest but numerically unstable when &lt;code&gt;n_iter&lt;/code&gt; is large, e.g. typically 5 or larger), or &amp;lsquo;LU&amp;rsquo; factorization (numerically stable but can lose slightly in accuracy). The &amp;lsquo;auto&amp;rsquo; mode applies no normalization if &lt;code&gt;n_iter&lt;/code&gt; &amp;lt;= 2 and switches to LU otherwise.</source>
          <target state="translated">단계별 QR 분해 (가장 느리지 만 가장 정확한)로 전력 반복이 정규화되는지, '없음'( &lt;code&gt;n_iter&lt;/code&gt; 가 클 때 가장 빠르지 만 수치 적으로 불안정 함 ( 예 : 일반적으로 5 이상)) 또는 'LU'분해 (수적으로) 안정적이지만 정확도가 약간 떨어질 수 있습니다). '자동'모드는 &lt;code&gt;n_iter&lt;/code&gt; &amp;lt;= 2 인 경우 정규화를 적용하지 않으면 LU로 전환됩니다.</target>
        </trans-unit>
        <trans-unit id="00c2bc5048e0182a905dad0c4dec40740dd521b8" translate="yes" xml:space="preserve">
          <source>Whether the relationship is increasing or decreasing.</source>
          <target state="translated">관계가 증가 또는 감소하는지 여부</target>
        </trans-unit>
        <trans-unit id="dc0314689b038e45038d5534e1499b2766f8b916" translate="yes" xml:space="preserve">
          <source>Whether the return value is an array of sparse matrix depends on the type of the input X.</source>
          <target state="translated">반환 값이 희소 행렬의 배열인지 여부는 입력 X의 유형에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="ed44b1dc96dab239fb6ac2dc375f2ee5fe3ae793" translate="yes" xml:space="preserve">
          <source>Whether the target values y are normalized, i.e., the mean of the observed target values become zero. This parameter should be set to True if the target values&amp;rsquo; mean is expected to differ considerable from zero. When enabled, the normalization effectively modifies the GP&amp;rsquo;s prior based on the data, which contradicts the likelihood principle; normalization is thus disabled per default.</source>
          <target state="translated">목표 값 y가 정규화되는지 여부, 즉 관찰 된 목표 값의 평균은 0이된다. 목표 값의 평균이 0과 크게 다를 것으로 예상되는 경우이 매개 변수를 True로 설정해야합니다. 활성화되면 정규화는 데이터를 기반으로 GP의 사전을 효과적으로 수정하므로 가능성 원리와 모순됩니다. 따라서 표준화는 기본적으로 비활성화되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="4d56de522c2f2c8fa83698a0d6921644d35a5428" translate="yes" xml:space="preserve">
          <source>Whether the task is a classification task, in which case stratified KFold will be used.</source>
          <target state="translated">작업이 분류 작업인지 여부에 따라 계층화 된 KFold가 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="6cb255093bce0eff775d5414b35fcd6fbd07931b" translate="yes" xml:space="preserve">
          <source>Whether this is a multilabel classifier</source>
          <target state="translated">이것이 다중 레이블 분류기인지 여부</target>
        </trans-unit>
        <trans-unit id="b1be5efdade94da8e67722b4ba2f983efa0225c5" translate="yes" xml:space="preserve">
          <source>Whether to allow 2-d y (array or sparse matrix). If false, y will be validated as a vector. y cannot have np.nan or np.inf values if multi_output=True.</source>
          <target state="translated">2-dy (배열 또는 희소 행렬) 허용 여부입니다. False이면 y는 벡터로 검증됩니다. multi_output = True 인 경우 np.nan 또는 np.inf 값을 가질 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="8ecc2d4e014d3f2172c25597969963bd34e18f05" translate="yes" xml:space="preserve">
          <source>Whether to allow X.ndim &amp;gt; 2.</source>
          <target state="translated">X.ndim 허용 여부&amp;gt; 2</target>
        </trans-unit>
        <trans-unit id="d5dcbf9253f384309cfbc4a594ef7b057c1cb7e6" translate="yes" xml:space="preserve">
          <source>Whether to also return the code U or just the dictionary V.</source>
          <target state="translated">코드 U 또는 사전 V 만 반환할지 여부</target>
        </trans-unit>
        <trans-unit id="c6289192e1f815e2a760fe289e8841e73f51c42c" translate="yes" xml:space="preserve">
          <source>Whether to be verbose.</source>
          <target state="translated">장황한 지 여부</target>
        </trans-unit>
        <trans-unit id="ccada94fe77cfa7bf4094a2aaa2265ce9f9f4e5f" translate="yes" xml:space="preserve">
          <source>Whether to cache downloaded datasets using joblib.</source>
          <target state="translated">joblib을 사용하여 다운로드 한 데이터 세트를 캐시할지 여부.</target>
        </trans-unit>
        <trans-unit id="86614eccba121d18979d29b5e6a1aceed9dc9343" translate="yes" xml:space="preserve">
          <source>Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (e.g. data is expected to be already centered).</source>
          <target state="translated">이 모델의 절편을 계산할지 여부입니다. False로 설정하면 계산에 인터셉트가 사용되지 않습니다 (예 : 데이터가 이미 중앙에있을 것으로 예상 됨).</target>
        </trans-unit>
        <trans-unit id="943768cddf06aea5cdead26cd3dff69cb74196ef" translate="yes" xml:space="preserve">
          <source>Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).</source>
          <target state="translated">이 모델의 절편을 계산할지 여부입니다. false로 설정하면 계산에 인터셉트가 사용되지 않습니다 (예 : 데이터가 이미 중심에있을 것으로 예상 됨).</target>
        </trans-unit>
        <trans-unit id="27f8a383f138130dd1f45baee7b1d68ad4ce59f4" translate="yes" xml:space="preserve">
          <source>Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be already centered).</source>
          <target state="translated">이 모델의 절편을 계산할지 여부입니다. false로 설정하면 계산에 인터셉트가 사용되지 않습니다 (즉, 데이터가 이미 중앙에있을 것으로 예상 됨).</target>
        </trans-unit>
        <trans-unit id="0336ff17d311b84566800e5cf35b415ab2b27f38" translate="yes" xml:space="preserve">
          <source>Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations.</source>
          <target state="translated">이 모델의 절편을 계산할지 여부입니다. false로 설정하면 계산에 인터셉트가 사용되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="471ed2aff4494fad57e380482ee0a58232333b20" translate="yes" xml:space="preserve">
          <source>Whether to check that &lt;code&gt;transform&lt;/code&gt; followed by &lt;code&gt;inverse_transform&lt;/code&gt; or &lt;code&gt;func&lt;/code&gt; followed by &lt;code&gt;inverse_func&lt;/code&gt; leads to the original targets.</source>
          <target state="translated">&lt;code&gt;transform&lt;/code&gt; 후 &lt;code&gt;inverse_transform&lt;/code&gt; 또는 &lt;code&gt;func&lt;/code&gt; 다음에 &lt;code&gt;inverse_func&lt;/code&gt; 가 있는지 여부를 확인 하면 원래 대상이됩니다.</target>
        </trans-unit>
        <trans-unit id="3c61d748141856e990caff79c0b01fd8a4086321" translate="yes" xml:space="preserve">
          <source>Whether to check that or &lt;code&gt;func&lt;/code&gt; followed by &lt;code&gt;inverse_func&lt;/code&gt; leads to the original inputs. It can be used for a sanity check, raising a warning when the condition is not fulfilled.</source>
          <target state="translated">확인 여부 또는 &lt;code&gt;func&lt;/code&gt; 다음에 &lt;code&gt;inverse_func&lt;/code&gt; 가 있는지 여부 는 원래 입력으로 이어집니다. 상태 점검에 사용되어 조건이 충족되지 않으면 경고를 발생시킵니다.</target>
        </trans-unit>
        <trans-unit id="50850a0df7fa2328560b0d7ebe31ee1142901517" translate="yes" xml:space="preserve">
          <source>Whether to compute &lt;code&gt;y_&lt;/code&gt; is increasing (if set to True) or decreasing (if set to False)</source>
          <target state="translated">&lt;code&gt;y_&lt;/code&gt; 계산 증가 (True로 설정된 경우) 또는 감소 (False로 설정된 경우)</target>
        </trans-unit>
        <trans-unit id="5fbe7e9ef9385d70942ace9201de11e6cead8f8d" translate="yes" xml:space="preserve">
          <source>Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned.</source>
          <target state="translated">제곱 오차 표준 또는 오류 표준을 계산할지 여부 True (기본값)이면 제곱 오류 표준이 반환됩니다. False이면 오류 규범이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="01086dba4779bb032a62d90a9f9152dc1b833baf" translate="yes" xml:space="preserve">
          <source>Whether to copy X and Y, or perform in-place computations.</source>
          <target state="translated">X와 Y를 복사 할 것인지 또는 내부 계산을 수행 할 것인지.</target>
        </trans-unit>
        <trans-unit id="725302de0fd34aabdbfc0e0affbd4f4fb754127c" translate="yes" xml:space="preserve">
          <source>Whether to copy X and Y, or perform in-place normalization.</source>
          <target state="translated">X와 Y를 복사 할 것인지 또는 내부 정규화를 수행 할 것인지.</target>
        </trans-unit>
        <trans-unit id="7462b314a4fe2fff7847f10014abb6b1183fa84f" translate="yes" xml:space="preserve">
          <source>Whether to copy X and operate on the copy or perform in-place operations.</source>
          <target state="translated">X를 복사하여 복사 작업을 수행 할 것인지 아니면 내부 작업을 수행 할 것인지를 결정합니다.</target>
        </trans-unit>
        <trans-unit id="3692936737114d51a6bb410cb3531454c9ad1d68" translate="yes" xml:space="preserve">
          <source>Whether to copy the precomputed covariance matrix; if False, it may be overwritten.</source>
          <target state="translated">미리 계산 된 공분산 행렬을 복사할지 여부; False이면 덮어 쓸 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="324d6fe1307968790ddbb142ded331e1979517da" translate="yes" xml:space="preserve">
          <source>Whether to create a copy of X and operate on it or to perform inplace computation (default behaviour).</source>
          <target state="translated">X의 사본을 작성하여 작업 할 것인지 또는 내부 계산을 수행 할 것인지 (기본 동작).</target>
        </trans-unit>
        <trans-unit id="62527ff1b5ed50e080e833b6d4fd94a862b78590" translate="yes" xml:space="preserve">
          <source>Whether to drop some suboptimal thresholds which would not appear on a plotted ROC curve. This is useful in order to create lighter ROC curves.</source>
          <target state="translated">플로팅 된 ROC 곡선에 나타나지 않는 일부 차선의 임계 값을 제거할지 여부입니다. 더 가벼운 ROC 곡선을 만드는 데 유용합니다.</target>
        </trans-unit>
        <trans-unit id="9851e7fdf1748b99ff7c24c940b7ce5f334a6a27" translate="yes" xml:space="preserve">
          <source>Whether to drop the first eigenvector. For spectral embedding, this should be True as the first eigenvector should be constant vector for connected graph, but for spectral clustering, this should be kept as False to retain the first eigenvector.</source>
          <target state="translated">첫 번째 고유 벡터를 제거할지 여부입니다. 스펙트럼 임베딩의 경우 첫 번째 고유 벡터가 연결된 그래프의 상수 벡터 여야하므로 True 여야하지만 스펙트럼 클러스터링의 경우 첫 번째 고유 벡터를 유지하려면 False로 유지해야합니다.</target>
        </trans-unit>
        <trans-unit id="5f99ffcc1bc3b15acf95363130c37cd5b381a91e" translate="yes" xml:space="preserve">
          <source>Whether to enable probability estimates. This must be enabled prior to calling &lt;code&gt;fit&lt;/code&gt;, and will slow down that method.</source>
          <target state="translated">확률 추정을 활성화할지 여부. &lt;code&gt;fit&lt;/code&gt; 호출하기 전에 활성화해야하며 해당 메소드의 속도가 느려집니다.</target>
        </trans-unit>
        <trans-unit id="4a4497ffca40d00ae7a4f4bda08e5dece2337700" translate="yes" xml:space="preserve">
          <source>Whether to enforce positivity when finding the code.</source>
          <target state="translated">코드를 찾을 때 양성을 시행할지 여부.</target>
        </trans-unit>
        <trans-unit id="2920a1115fbc090ce8de93fb299e0d53a98e0404" translate="yes" xml:space="preserve">
          <source>Whether to enforce positivity when finding the dictionary</source>
          <target state="translated">사전을 찾을 때 양성을 시행할지 여부</target>
        </trans-unit>
        <trans-unit id="cc323427a6369f3d4c345df0c7eab9b613a4b184" translate="yes" xml:space="preserve">
          <source>Whether to enforce positivity when finding the dictionary.</source>
          <target state="translated">사전을 찾을 때 양성을 시행할지 여부.</target>
        </trans-unit>
        <trans-unit id="0c86815e9d15a8d79f49100b7de99ab0894ffb4b" translate="yes" xml:space="preserve">
          <source>Whether to enforce positivity when finding the encoding.</source>
          <target state="translated">인코딩을 찾을 때 양성을 시행할지 여부.</target>
        </trans-unit>
        <trans-unit id="67962e408072673f64867d41053d2df5bd8ffd92" translate="yes" xml:space="preserve">
          <source>Whether to ensure that y has a numeric type. If dtype of y is object, it is converted to float64. Should only be used for regression algorithms.</source>
          <target state="translated">y에 숫자 유형이 있는지 여부입니다. y의 dtype이 object이면 float64로 변환됩니다. 회귀 알고리즘에만 사용해야합니다.</target>
        </trans-unit>
        <trans-unit id="a89a89f1fbc0cff73f883e4748e4c43034f0361e" translate="yes" xml:space="preserve">
          <source>Whether to filter invalid parameters or not.</source>
          <target state="translated">잘못된 매개 변수를 필터링할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="8bd7eb051fe8793acf6505d7ff57ac8a40be3e89" translate="yes" xml:space="preserve">
          <source>Whether to fit an intercept for the model. In this case the shape of the returned array is (n_cs, n_features + 1).</source>
          <target state="translated">모형에 절편을 맞출 지 여부입니다. 이 경우 반환 된 배열의 모양은 (n_cs, n_features + 1)입니다.</target>
        </trans-unit>
        <trans-unit id="8e39ad37924100e174516b8fe05585ae0c11a660" translate="yes" xml:space="preserve">
          <source>Whether to include &amp;ldquo;special&amp;rdquo; label estimator or test processors.</source>
          <target state="translated">&amp;ldquo;특별한&amp;rdquo;라벨 추정기 또는 테스트 프로세서 포함 여부.</target>
        </trans-unit>
        <trans-unit id="84818ba086581abd044063f9dd3c5b2beb12eda7" translate="yes" xml:space="preserve">
          <source>Whether to include meta-estimators that can be constructed using an estimator as their first argument. These are currently BaseEnsemble, OneVsOneClassifier, OutputCodeClassifier, OneVsRestClassifier, RFE, RFECV.</source>
          <target state="translated">추정기를 첫 번째 인수로 사용하여 구성 할 수있는 메타 추정기를 포함할지 여부입니다. 이들은 현재 BaseEnsemble, OneVsOneClassifier, OutputCodeClassifier, OneVsRestClassifier, RFE, RFECV입니다.</target>
        </trans-unit>
        <trans-unit id="74d7014a87bc3e04b7cb4d5881013af41aaf9d5f" translate="yes" xml:space="preserve">
          <source>Whether to include train scores.</source>
          <target state="translated">열차 점수 포함 여부</target>
        </trans-unit>
        <trans-unit id="b770fc1f2ccfc02ef3107a2ecac741b2276c37e8" translate="yes" xml:space="preserve">
          <source>Whether to learn class prior probabilities or not. If false, a uniform prior will be used.</source>
          <target state="translated">수업 사전 확률을 배울 지 여부. False이면 균일 한 사전이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="8606bf192804810fba78c6bd2b8805fda4483156" translate="yes" xml:space="preserve">
          <source>Whether to load only 10 percent of the data.</source>
          <target state="translated">데이터의 10 % 만로드할지 여부</target>
        </trans-unit>
        <trans-unit id="3156faf4c491e77c08d3500dd2b8c76947137632" translate="yes" xml:space="preserve">
          <source>Whether to load or not the content of the different files. If true a &amp;lsquo;data&amp;rsquo; attribute containing the text information is present in the data structure returned. If not, a filenames attribute gives the path to the files.</source>
          <target state="translated">다른 파일의 내용을로드할지 여부 true 인 경우 텍스트 정보를 포함하는 'data'속성이 반환 된 데이터 구조에 있습니다. 그렇지 않은 경우 파일 이름 속성은 파일 경로를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="81f8d6a01f7cffb7f53496e9cfd84f1ce27de740" translate="yes" xml:space="preserve">
          <source>Whether to make X at least 2d.</source>
          <target state="translated">X를 2d 이상으로 만들지 여부</target>
        </trans-unit>
        <trans-unit id="2a578215f5e1bceb4eddd518c894532f84a10916" translate="yes" xml:space="preserve">
          <source>Whether to make a copy of X. If &lt;code&gt;False&lt;/code&gt;, the input X gets overwritten during fitting.</source>
          <target state="translated">X의 사본을 만들지 여부. &lt;code&gt;False&lt;/code&gt; 인 경우 피팅 중 입력 X를 덮어 씁니다.</target>
        </trans-unit>
        <trans-unit id="e2abdc017941ef25090c0789fe34541086dc5677" translate="yes" xml:space="preserve">
          <source>Whether to make a copy of the given data. If set to False, the initial data will be overwritten.</source>
          <target state="translated">주어진 데이터의 복사본을 만들지 여부 False로 설정하면 초기 데이터를 덮어 씁니다.</target>
        </trans-unit>
        <trans-unit id="df62a350cab30808585d28e267100de2daf97811" translate="yes" xml:space="preserve">
          <source>Whether to normalize the output matrix to make the leading diagonal elements all 1</source>
          <target state="translated">선행 대각선 요소를 모두 만들기 위해 출력 행렬을 정규화할지 여부 1</target>
        </trans-unit>
        <trans-unit id="ba6415e4db38e5cea33cf7fab1a514fcf5285867" translate="yes" xml:space="preserve">
          <source>Whether to perform precomputations. Improves performance when n_targets or n_samples is very large.</source>
          <target state="translated">사전 계산 수행 여부 n_targets 또는 n_samples가 매우 큰 경우 성능을 향상시킵니다.</target>
        </trans-unit>
        <trans-unit id="4010b2bff9133aaf08486f7fb645e8e39634583e" translate="yes" xml:space="preserve">
          <source>Whether to presort the data to speed up the finding of best splits in fitting. Auto mode by default will use presorting on dense data and default to normal sorting on sparse data. Setting presort to true on sparse data will raise an error.</source>
          <target state="translated">피팅에서 최상의 분할을 찾는 속도를 높이기 위해 데이터를 미리 정렬할지 여부입니다. 자동 모드는 기본적으로 밀도가 높은 데이터에 대해 사전 분류를 사용하고 희소 데이터에 대한 기본 정렬을 기본으로 사용합니다. 스파 스 데이터에서 presort를 true로 설정하면 오류가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="29f75c6794195c888ce8399680c96d5b14e65048" translate="yes" xml:space="preserve">
          <source>Whether to presort the data to speed up the finding of best splits in fitting. For the default settings of a decision tree on large datasets, setting this to true may slow down the training process. When using either a smaller dataset or a restricted depth, this may speed up the training.</source>
          <target state="translated">피팅에서 최상의 분할을 찾는 속도를 높이기 위해 데이터를 미리 정렬할지 여부입니다. 큰 데이터 세트에서 의사 결정 트리의 기본 설정의 경우이를 true로 설정하면 훈련 프로세스가 느려질 수 있습니다. 더 작은 데이터 세트 또는 제한된 깊이를 사용하면 훈련 속도가 빨라질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="cc4d3f96c9bc487e50b4fc4701212f323c65bca6" translate="yes" xml:space="preserve">
          <source>Whether to print progress messages to stdout.</source>
          <target state="translated">진행 메시지를 stdout에 인쇄할지 여부</target>
        </trans-unit>
        <trans-unit id="b44ea19ee67df3ef30c54f3be25f24e67c4f3a60" translate="yes" xml:space="preserve">
          <source>Whether to raise a value error if X is not 2d.</source>
          <target state="translated">X가 2d가 아닌 경우 값 오류를 발생 시킬지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="b5d06b3c83946e2cd2c05192b834ad6e01c6a1d5" translate="yes" xml:space="preserve">
          <source>Whether to raise an error on np.inf and np.nan in X. The possibilities are:</source>
          <target state="translated">X에서 np.inf 및 np.nan에 오류를 발생 시킬지 여부입니다. 가능성은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="eaa329f6263ead71ba810671bba1e6a99379040d" translate="yes" xml:space="preserve">
          <source>Whether to raise an error on np.inf and np.nan in X. This parameter does not influence whether y can have np.inf or np.nan values. The possibilities are:</source>
          <target state="translated">X에서 np.inf 및 np.nan에 오류를 발생 시킬지 여부 가능성은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c551edd4edb4cf1081c3e3d3eb3a36ad8f839a51" translate="yes" xml:space="preserve">
          <source>Whether to raise an error or ignore if an unknown categorical feature is present during transform (default is to raise). When this parameter is set to &amp;lsquo;ignore&amp;rsquo; and an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. In the inverse transform, an unknown category will be denoted as None.</source>
          <target state="translated">변환 중에 알 수없는 범주 형 피처가 존재하는 경우 오류를 발생 시킬지 또는 무시할지 여부 (기본값은 올림). 이 매개 변수가 '무시'로 설정되고 변환 중에 알 수없는 카테고리가 발견되면이 기능에 대한 1- 인코딩 된 열이 모두 0이됩니다. 역변환에서 알 수없는 범주는 없음으로 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="55b5cf4021bca4319afc6cff07e1e1c5a8e21c80" translate="yes" xml:space="preserve">
          <source>Whether to return a one-vs-rest (&amp;lsquo;ovr&amp;rsquo;) decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one (&amp;lsquo;ovo&amp;rsquo;) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2).</source>
          <target state="translated">다른 모든 분류기로 모양 (n_samples, n_classes)에 대한 1 대 1 나머지 ( 'ovr') 결정 함수를 반환할지 또는 모양 (n_samples)이있는 libsvm의 원래 1 대 1 ( 'ovo') 결정 함수를 반환할지 여부 , n_classes * (n_classes-1) / 2).</target>
        </trans-unit>
        <trans-unit id="3e008ca3901c2f4656b69b334cd2bbf88df838cc" translate="yes" xml:space="preserve">
          <source>Whether to return a one-vs-rest (&amp;lsquo;ovr&amp;rsquo;) decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one (&amp;lsquo;ovo&amp;rsquo;) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one (&amp;lsquo;ovo&amp;rsquo;) is always used as multi-class strategy.</source>
          <target state="translated">다른 모든 분류기로 모양 (n_samples, n_classes)에 대한 1 대 1 나머지 ( 'ovr') 결정 함수를 반환할지 또는 모양 (n_samples)이있는 libsvm의 원래 1 대 1 ( 'ovo') 결정 함수를 반환할지 여부 , n_classes * (n_classes-1) / 2). 그러나 일대일 ( 'ovo')은 항상 다중 클래스 전략으로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="ebccc28d7f21db5d9325894eec5fc9e6d02d15c3" translate="yes" xml:space="preserve">
          <source>Whether to return dense output even when the input is sparse. If &lt;code&gt;False&lt;/code&gt;, the output is sparse if both input arrays are sparse.</source>
          <target state="translated">입력이 드문 경우에도 조밀 한 출력을 반환할지 여부입니다. 경우 &lt;code&gt;False&lt;/code&gt; 를 모두 입력 배열이 부족한 경우, 출력은 스파 스입니다.</target>
        </trans-unit>
        <trans-unit id="12da7a3ff0421b885caca94ef2caa65b1b016c5f" translate="yes" xml:space="preserve">
          <source>Whether to return every value of the nonzero coefficients along the forward path. Useful for cross-validation.</source>
          <target state="translated">순방향 경로를 따라 0이 아닌 계수의 모든 값을 반환할지 여부입니다. 교차 유효성 검사에 유용합니다.</target>
        </trans-unit>
        <trans-unit id="6a16a084b2e44866fb8e9c04ea892f46ef9407d0" translate="yes" xml:space="preserve">
          <source>Whether to return the estimators fitted on each split.</source>
          <target state="translated">각 분할에 맞는 견적서를 반환할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="642c6fa9e4316671c7770b59f9d72b6641ef628f" translate="yes" xml:space="preserve">
          <source>Whether to return the number of iterations.</source>
          <target state="translated">반복 횟수를 반환할지 여부.</target>
        </trans-unit>
        <trans-unit id="b549a24da790409f4ec3623e7b38a8b83191c416" translate="yes" xml:space="preserve">
          <source>Whether to return the standard deviation of posterior prediction.</source>
          <target state="translated">사후 예측의 표준 편차를 반환할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="b33ff2a27948731af021590a907c614a500fc29d" translate="yes" xml:space="preserve">
          <source>Whether to return the standard deviation of posterior prediction. All zeros in this case.</source>
          <target state="translated">사후 예측의 표준 편차를 반환할지 여부입니다. 이 경우 모든 0입니다.</target>
        </trans-unit>
        <trans-unit id="ef9a1d5fe208dc604ffba3d0e60394e32f61bdb0" translate="yes" xml:space="preserve">
          <source>Whether to scale X and Y.</source>
          <target state="translated">X와 Y를 스케일할지 여부</target>
        </trans-unit>
        <trans-unit id="06ccde346dfb7273af2d0810793b1fb5e47df0dc" translate="yes" xml:space="preserve">
          <source>Whether to show informative labels for impurity, etc. Options include &amp;lsquo;all&amp;rsquo; to show at every node, &amp;lsquo;root&amp;rsquo; to show only at the top root node, or &amp;lsquo;none&amp;rsquo; to not show at any node.</source>
          <target state="translated">불순물에 대한 정보 레이블을 표시할지 여부 등의 옵션에는 모든 노드에 표시되는 'all', 최상위 루트 노드에만 표시되는 'root'또는 노드에 표시되지 않는 'none'이 포함됩니다.</target>
        </trans-unit>
        <trans-unit id="bcd035c2f558018eebfd4e5534f5df5bef89069a" translate="yes" xml:space="preserve">
          <source>Whether to shuffle dataset.</source>
          <target state="translated">데이터 세트를 섞을 지 여부.</target>
        </trans-unit>
        <trans-unit id="8704d580bb26c2f6617363a0297f26abfb9fda30" translate="yes" xml:space="preserve">
          <source>Whether to shuffle each stratification of the data before splitting into batches.</source>
          <target state="translated">배치로 분할하기 전에 데이터의 각 계층화를 섞을 지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="5de7b9303caa771da78304a93ebeac224ba77f9b" translate="yes" xml:space="preserve">
          <source>Whether to shuffle samples in each iteration. Only used when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;.</source>
          <target state="translated">각 반복에서 샘플을 섞을 지 여부. solver = 'sgd'또는 'adam'인 경우에만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="3558a0c3a77b9ce3c242cc621a2d776c46a133af" translate="yes" xml:space="preserve">
          <source>Whether to shuffle the data before splitting into batches.</source>
          <target state="translated">배치로 분할하기 전에 데이터를 셔플할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="ddc8f26baf73b311e3fd82ce49af7a5449330660" translate="yes" xml:space="preserve">
          <source>Whether to shuffle the data before splitting it in batches.</source>
          <target state="translated">데이터를 일괄 적으로 분할하기 전에 셔플할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="f23a63355a4e388bf6ee14cbad5c7c9c8b8c8007" translate="yes" xml:space="preserve">
          <source>Whether to shuffle the samples.</source>
          <target state="translated">샘플을 섞을 지 여부.</target>
        </trans-unit>
        <trans-unit id="a5c75421672ae29d738aa02687f5d9f3ec0cf20c" translate="yes" xml:space="preserve">
          <source>Whether to shuffle training data before taking prefixes of it based on``train_sizes``.</source>
          <target state="translated">``train_sizes ''를 기반으로 접두사를 사용하기 전에 교육 데이터를 셔플할지 여부입니다.</target>
        </trans-unit>
        <trans-unit id="ce33fd98a79a5db67d420efb6fcabed70acb96c4" translate="yes" xml:space="preserve">
          <source>Whether to sort x before computing. If False, assume that x must be either monotonic increasing or monotonic decreasing. If True, y is used to break ties when sorting x. Make sure that y has a monotonic relation to x when setting reorder to True.</source>
          <target state="translated">계산하기 전에 x를 정렬할지 여부 False이면 x가 단조 증가 또는 단조 감소 여야한다고 가정하십시오. True이면 x를 정렬 할 때 연결을 끊기 위해 y가 사용됩니다. 재정렬을 True로 설정할 때 y가 x와 단조로운 관계인지 확인하십시오.</target>
        </trans-unit>
        <trans-unit id="5091491cac888f3972a1197cfef1256978c18b5f" translate="yes" xml:space="preserve">
          <source>Whether to split the sparse feature vector into the concatenation of its negative part and its positive part. This can improve the performance of downstream classifiers.</source>
          <target state="translated">희소 피쳐 벡터를 음수 부분과 양수 부분의 연결로 분할할지 여부입니다. 이는 다운 스트림 분류기의 성능을 향상시킬 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8b58e41338c55eaf50346244bd6082e4f3c1a628" translate="yes" xml:space="preserve">
          <source>Whether to use Nesterov&amp;rsquo;s momentum. Only used when solver=&amp;rsquo;sgd&amp;rsquo; and momentum &amp;gt; 0.</source>
          <target state="translated">Nesterov의 운동량 사용 여부. solver = 'sgd'이고 운동량이&amp;gt; 0 일 때만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="d97545296a16ed9ee24e38ded3551b9344d66c64" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram and Xy matrix to speed up calculations. Improves performance when &lt;code&gt;n_targets&lt;/code&gt; or &lt;code&gt;n_samples&lt;/code&gt; is very large. Note that if you already have such matrices, you can pass them directly to the fit method.</source>
          <target state="translated">계산 속도를 높이기 위해 사전 계산 된 Gram 및 Xy 매트릭스 사용 여부 &lt;code&gt;n_targets&lt;/code&gt; 또는 &lt;code&gt;n_samples&lt;/code&gt; 가 매우 큰 경우 성능을 향상시킵니다 . 이러한 행렬이 이미있는 경우 직접 적합한 방법으로 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7dc600168d6ae4c500452eb14badcdfddafb11ff" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram matrix to speed up calculations. If set to &amp;lsquo;auto&amp;rsquo; let us decide. The Gram matrix can also be passed as argument, but it will be used only for the selection of parameter alpha, if alpha is &amp;lsquo;aic&amp;rsquo; or &amp;lsquo;bic&amp;rsquo;.</source>
          <target state="translated">사전 계산 된 그람 매트릭스를 사용하여 계산 속도를 높일 지 여부 'auto'로 설정하면 결정하겠습니다. 그람 행렬은 인수로 전달 될 수도 있지만, 알파가 'aic'또는 'bic'인 경우 매개 변수 alpha를 선택하는 경우에만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="0057a82fcc5e3ed086a2d1961e27e45b3f58597f" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram matrix to speed up calculations. If set to &lt;code&gt;'auto'&lt;/code&gt; let us decide. The Gram matrix can also be passed as argument.</source>
          <target state="translated">사전 계산 된 그람 매트릭스를 사용하여 계산 속도를 높일 지 여부 &lt;code&gt;'auto'&lt;/code&gt; 로 설정하면 결정하겠습니다. 그램 행렬은 인수로 전달 될 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="419f7e60c7c4f4f84360a1078ab4b36ce5bf208a" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram matrix to speed up calculations. If set to &lt;code&gt;'auto'&lt;/code&gt; let us decide. The Gram matrix can also be passed as argument. For sparse input this option is always &lt;code&gt;True&lt;/code&gt; to preserve sparsity.</source>
          <target state="translated">사전 계산 된 그람 매트릭스를 사용하여 계산 속도를 높일 지 여부 &lt;code&gt;'auto'&lt;/code&gt; 로 설정하면 결정하겠습니다. 그램 행렬은 인수로 전달 될 수도 있습니다. 희소 입력의 경우이 옵션은 희소성을 유지하기 위해 항상 &lt;code&gt;True&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="18cfe378dd4d6390fca460de4e70af73f097cdf4" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram matrix to speed up calculations. If set to &lt;code&gt;'auto'&lt;/code&gt; let us decide. The Gram matrix cannot be passed as argument since we will use only subsets of X.</source>
          <target state="translated">사전 계산 된 그람 매트릭스를 사용하여 계산 속도를 높일 지 여부 &lt;code&gt;'auto'&lt;/code&gt; 로 설정하면 결정하겠습니다. 우리는 X의 부분 집합 만 사용할 것이기 때문에 그람 행렬은 인수로 전달 될 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="2bc24bd08e69b99e2a7b63ee3e5ac92e16a75bc1" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram matrix to speed up calculations. The Gram matrix can also be passed as argument. For sparse input this option is always &lt;code&gt;True&lt;/code&gt; to preserve sparsity.</source>
          <target state="translated">사전 계산 된 그람 매트릭스를 사용하여 계산 속도를 높일 지 여부 그램 행렬은 인수로 전달 될 수도 있습니다. 희소 입력의 경우이 옵션은 희소성을 유지하기 위해 항상 &lt;code&gt;True&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="1efc80d653c0b8715eb15bdf1b19f3becd74a957" translate="yes" xml:space="preserve">
          <source>Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside a fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.</source>
          <target state="translated">유효성 검사 점수가 향상되지 않을 때 조기 중지를 사용하여 교육을 종료할지 여부 True로 설정하면 n_iter_no_change 연속 epoch에 대해 유효성 검사 점수가 최소한 tol만큼 개선되지 않으면 훈련 데이터의 일부를 유효성 검사로 자동으로 설정하고 훈련을 종료합니다.</target>
        </trans-unit>
        <trans-unit id="9af303ba091050935df1b1843777cae63d69ca4c" translate="yes" xml:space="preserve">
          <source>Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least &lt;code&gt;tol&lt;/code&gt; for &lt;code&gt;n_iter_no_change&lt;/code&gt; consecutive epochs. Only effective when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;</source>
          <target state="translated">유효성 검사 점수가 향상되지 않을 때 조기 중지를 사용하여 교육을 종료할지 여부 true로 설정하면 &lt;code&gt;n_iter_no_change&lt;/code&gt; 연속 epoch에 대해 유효성 검사 점수가 최소한 &lt;code&gt;tol&lt;/code&gt; 만큼 개선되지 않으면 훈련 데이터의 10 %를 유효성 검사로 자동 설정하고 교육을 종료 합니다. solver = 'sgd'또는 'adam'일 때만 유효</target>
        </trans-unit>
        <trans-unit id="998484dc55c21823abb36e2b54f5b8f623a0a41f" translate="yes" xml:space="preserve">
          <source>Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for &lt;code&gt;n_iter_no_change&lt;/code&gt; consecutive epochs. Only effective when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;</source>
          <target state="translated">유효성 검사 점수가 향상되지 않을 때 조기 중지를 사용하여 교육을 종료할지 여부 true로 설정하면 &lt;code&gt;n_iter_no_change&lt;/code&gt; 연속 epoch에 대해 유효성 검사 점수가 최소한 tol만큼 개선되지 않으면 훈련 데이터의 10 %를 유효성 검사로 자동 설정하고 교육을 종료 합니다. solver = 'sgd'또는 'adam'일 때만 유효</target>
        </trans-unit>
        <trans-unit id="cec695b146ca339e70be6934dce1a5005aa741f4" translate="yes" xml:space="preserve">
          <source>Whether to use early stopping to terminate training when validation. score is not improving. If set to True, it will automatically set aside a fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.</source>
          <target state="translated">유효성 검사시 조기 중지를 사용하여 교육을 종료할지 여부 점수가 향상되지 않습니다. True로 설정하면 n_iter_no_change 연속 epoch에 대해 유효성 검사 점수가 최소한 tol만큼 개선되지 않으면 훈련 데이터의 일부를 유효성 검사로 자동으로 설정하고 훈련을 종료합니다.</target>
        </trans-unit>
        <trans-unit id="aad0324e5e5b11eda436b08ef9513e34456be6fd" translate="yes" xml:space="preserve">
          <source>Whether to use mini-batch k-means, which is faster but may get different results.</source>
          <target state="translated">미니 배치 k- 평균을 사용할지 여부는 더 빠르지 만 다른 결과를 얻을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="098e9f05a9707c21daca2709c375c5f67a6fc326" translate="yes" xml:space="preserve">
          <source>Whether to use out-of-bag samples to estimate the R^2 on unseen data.</source>
          <target state="translated">보이지 않는 데이터에 대한 R ^ 2를 추정하기 위해 가방 외부 샘플 사용 여부입니다.</target>
        </trans-unit>
        <trans-unit id="cf1378e2f07c46392b05b68a8d3fb4a1b87de256" translate="yes" xml:space="preserve">
          <source>Whether to use out-of-bag samples to estimate the generalization accuracy.</source>
          <target state="translated">일반화 정확도를 평가하기 위해 비 가방 샘플을 사용할지 여부.</target>
        </trans-unit>
        <trans-unit id="b9d7a8d80bd713aecc3b75a6342d626d4ac28b69" translate="yes" xml:space="preserve">
          <source>Whether to use out-of-bag samples to estimate the generalization error.</source>
          <target state="translated">일반화 오류를 추정하기 위해 가방 외부 샘플 사용 여부입니다.</target>
        </trans-unit>
        <trans-unit id="3aa24f38e2caae33363ee03e7cecc2915d7b4a6e" translate="yes" xml:space="preserve">
          <source>Whether to use the shrinking heuristic.</source>
          <target state="translated">축소 휴리스틱 사용 여부</target>
        </trans-unit>
        <trans-unit id="0fcee6cb3493ee9e39e4cf5b70ffc63bc5b7fc72" translate="yes" xml:space="preserve">
          <source>Whether to zip the stored data on disk. If an integer is given, it should be between 1 and 9, and sets the amount of compression. Note that compressed arrays cannot be read by memmapping.</source>
          <target state="translated">저장된 데이터를 디스크에 압축할지 여부 정수가 제공되면 1과 9 사이 여야하며 압축 량을 설정합니다. 압축 배열은 memmapping으로 읽을 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="a8c18609d5573425cb13e22e1562611896bad931" translate="yes" xml:space="preserve">
          <source>Whether transform should produce scipy.sparse matrices. True by default.</source>
          <target state="translated">변환이 scipy.sparse 행렬을 생성해야하는지 여부 기본적으로 true입니다.</target>
        </trans-unit>
        <trans-unit id="96e3c213b0373954a6f42c2953a288bf58e9c8e1" translate="yes" xml:space="preserve">
          <source>Whether y_prob needs to be normalized into the bin [0, 1], i.e. is not a proper probability. If True, the smallest value in y_prob is mapped onto 0 and the largest one onto 1.</source>
          <target state="translated">y_prob를 bin [0, 1]로 정규화해야하는지 여부, 즉 적절한 확률이 아닙니다. True이면 y_prob에서 가장 작은 값은 0에 매핑되고 가장 큰 값은 1에 매핑됩니다.</target>
        </trans-unit>
        <trans-unit id="673ccf9c3156ee120e948d124a09a8f79d0986df" translate="yes" xml:space="preserve">
          <source>Which SVD method to use. If &amp;lsquo;lapack&amp;rsquo; use standard SVD from scipy.linalg, if &amp;lsquo;randomized&amp;rsquo; use fast &lt;code&gt;randomized_svd&lt;/code&gt; function. Defaults to &amp;lsquo;randomized&amp;rsquo;. For most applications &amp;lsquo;randomized&amp;rsquo; will be sufficiently precise while providing significant speed gains. Accuracy can also be improved by setting higher values for &lt;code&gt;iterated_power&lt;/code&gt;. If this is not sufficient, for maximum precision you should choose &amp;lsquo;lapack&amp;rsquo;.</source>
          <target state="translated">사용할 SVD 방법. 'lapack'이 scipy.linalg의 표준 SVD를 사용하는 경우 'randomized'는 빠른 &lt;code&gt;randomized_svd&lt;/code&gt; 기능을 사용하십시오. 기본값은 '무작위 화'입니다. 대부분의 응용 분야에서 '무작위 화'는 상당한 속도 향상을 제공하면서 충분히 정밀합니다. &lt;code&gt;iterated_power&lt;/code&gt; 에 대해 더 높은 값을 설정하여 정확도를 향상시킬 수도 있습니다 . 이것이 충분하지 않으면, 최대 정밀도를 위해 'lapack'을 선택해야합니다.</target>
        </trans-unit>
        <trans-unit id="f51286fa5438dabdb2fdc9e6a35c79244bd208d2" translate="yes" xml:space="preserve">
          <source>Which affinity to use. At the moment &lt;code&gt;precomputed&lt;/code&gt; and &lt;code&gt;euclidean&lt;/code&gt; are supported. &lt;code&gt;euclidean&lt;/code&gt; uses the negative squared euclidean distance between points.</source>
          <target state="translated">사용할 친화력 현재 &lt;code&gt;precomputed&lt;/code&gt; 및 &lt;code&gt;euclidean&lt;/code&gt; 가 지원됩니다. &lt;code&gt;euclidean&lt;/code&gt; 점 사이의 음의 제곱 유클리드 거리를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="0a291d7f5d694ce4dae77d370f938e385f2f41cf" translate="yes" xml:space="preserve">
          <source>Which kind of estimators should be returned. If None, no filter is applied and all estimators are returned. Possible values are &amp;lsquo;classifier&amp;rsquo;, &amp;lsquo;regressor&amp;rsquo;, &amp;lsquo;cluster&amp;rsquo; and &amp;lsquo;transformer&amp;rsquo; to get estimators only of these specific types, or a list of these to get the estimators that fit at least one of the types.</source>
          <target state="translated">어떤 종류의 추정기가 리턴되어야합니까. None이면 필터가 적용되지 않고 모든 추정기가 반환됩니다. 가능한 값은 이러한 특정 유형의 추정기만 가져 오는 '분류기', '회귀 기', '클러스터'및 '변환기'또는 하나 이상의 유형에 맞는 추정기를 얻기위한 이들 목록입니다.</target>
        </trans-unit>
        <trans-unit id="9ab873abc046d5e79c6628d1e73d15a9a8c8a011" translate="yes" xml:space="preserve">
          <source>Which linkage criterion to use. The linkage criterion determines which distance to use between sets of features. The algorithm will merge the pairs of cluster that minimize this criterion.</source>
          <target state="translated">사용할 연계 기준입니다. 연계 기준에 따라 피쳐 세트간에 사용할 거리가 결정됩니다. 알고리즘은이 기준을 최소화하는 클러스터 쌍을 병합합니다.</target>
        </trans-unit>
        <trans-unit id="17ba6fc895d15866ea1e60a4db791726755dc58f" translate="yes" xml:space="preserve">
          <source>Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion.</source>
          <target state="translated">사용할 연계 기준입니다. 연계 기준에 따라 관측 세트간에 사용할 거리가 결정됩니다. 알고리즘은이 기준을 최소화하는 클러스터 쌍을 병합합니다.</target>
        </trans-unit>
        <trans-unit id="7ca40022d1c5dcd68056f855d9cb29fbb48c9062" translate="yes" xml:space="preserve">
          <source>Which model is the best is a matter of subjective judgement: do we want to favor models that only capture the big picture to summarize and explain most of the structure of the data while ignoring the details or do we prefer models that closely follow the high density regions of the signal?</source>
          <target state="translated">어떤 모델이 가장 좋은지는 주관적 판단의 문제입니다. 우리는 세부 사항을 무시하면서 데이터 구조의 대부분을 요약하고 설명하기 위해 큰 그림 만 캡처하는 모델을 선호하고 싶습니까? 신호의 영역?</target>
        </trans-unit>
        <trans-unit id="e8cddae54ed0b1b16ee030ff58543e83ff547ded" translate="yes" xml:space="preserve">
          <source>While Isomap, LLE and variants are best suited to unfold a single continuous low dimensional manifold, t-SNE will focus on the local structure of the data and will tend to extract clustered local groups of samples as highlighted on the S-curve example. This ability to group samples based on the local structure might be beneficial to visually disentangle a dataset that comprises several manifolds at once as is the case in the digits dataset.</source>
          <target state="translated">Isomap, LLE 및 변형은 단일 연속 저 차원 매니 폴드를 전개하는 데 가장 적합하지만 t-SNE는 데이터의 로컬 구조에 중점을두고 S- 곡선 예제에서 강조된대로 클러스터 된 로컬 샘플 그룹을 추출하는 경향이 있습니다. 로컬 구조를 기반으로 샘플을 그룹화하는이 기능은 숫자 데이터 세트에서와 같이 한 번에 여러 매니 폴드를 포함하는 데이터 세트를 시각적으로 얽히는 데 도움이 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d80a7676bafb31f5f9bcb99f7aed7e1817a9d535" translate="yes" xml:space="preserve">
          <source>While SVM models derived from &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; and &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; use &lt;code&gt;C&lt;/code&gt; as regularization parameter, most other estimators use &lt;code&gt;alpha&lt;/code&gt;. The exact equivalence between the amount of regularization of two models depends on the exact objective function optimized by the model. For example, when the estimator used is &lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt; regression, the relation between them is given as \(C = \frac{1}{alpha}\).</source>
          <target state="translated">&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; 및 &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; 에서 파생 된 SVM 모델 은 &lt;code&gt;C&lt;/code&gt; 를 정규화 매개 변수로 사용하지만 대부분의 다른 추정기는 &lt;code&gt;alpha&lt;/code&gt; 를 사용 합니다. 두 모델의 정규화 양 사이의 정확한 동등성은 모델에 의해 최적화 된 정확한 목적 함수에 달려 있습니다. 예를 들어, 사용 된 추정값이 &lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt; 회귀 분석 인 경우 이들 사이의 관계는 \ (C = \ frac {1} {alpha} \)로 지정됩니다.</target>
        </trans-unit>
        <trans-unit id="c20daeb41107431c48223b43ad4fe138aa4845d5" translate="yes" xml:space="preserve">
          <source>While experimenting with any learning algorithm, it is important not to test the prediction of an estimator on the data used to fit the estimator as this would not be evaluating the performance of the estimator on &lt;strong&gt;new data&lt;/strong&gt;. This is why datasets are often split into &lt;em&gt;train&lt;/em&gt; and &lt;em&gt;test&lt;/em&gt; data.</source>
          <target state="translated">학습 알고리즘을 실험하는 동안 &lt;strong&gt;새 데이터&lt;/strong&gt; 에 대한 추정기의 성능을 평가하지 않으므로 추정기에 맞는 데 사용 된 데이터에 대한 추정기의 예측을 테스트하지 않는 것이 중요 &lt;strong&gt;합니다&lt;/strong&gt; . 그렇기 때문에 데이터 세트가 종종 &lt;em&gt;열차&lt;/em&gt; 데이터 와 &lt;em&gt;테스트&lt;/em&gt; 데이터 로 분리됩니다 .</target>
        </trans-unit>
        <trans-unit id="d04cd5b9d0463d9bc89f841d571873fec4953569" translate="yes" xml:space="preserve">
          <source>While i.i.d. data is a common assumption in machine learning theory, it rarely holds in practice. If one knows that the samples have been generated using a time-dependent process, it&amp;rsquo;s safer to use a &lt;a href=&quot;#timeseries-cv&quot;&gt;time-series aware cross-validation scheme&lt;/a&gt; Similarly if we know that the generative process has a group structure (samples from collected from different subjects, experiments, measurement devices) it safer to use &lt;a href=&quot;#group-cv&quot;&gt;group-wise cross-validation&lt;/a&gt;.</source>
          <target state="translated">iid 데이터는 머신 러닝 이론에서 일반적인 가정이지만 실제로는 거의 유지되지 않습니다. 샘플이 시간 종속 프로세스를 사용 하여 생성 된 것을 알고 있다면 &lt;a href=&quot;#timeseries-cv&quot;&gt;시계열 인식 교차 검증 체계&lt;/a&gt; 를 사용하는 것이 더 안전합니다. 마찬가지로 생성 프로세스에 그룹 구조 (다른 주제에서 수집 한 샘플, 실험)가 있음을 알고있는 경우 &lt;a href=&quot;#group-cv&quot;&gt;그룹 단위 교차 검증&lt;/a&gt; 을 사용하는 것이 더 안전합니다 .</target>
        </trans-unit>
        <trans-unit id="0526a7a936d1a8c691dcdaa23691304fc8ebc7ef" translate="yes" xml:space="preserve">
          <source>While in the spirit of an online algorithm, the class &lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt;&lt;code&gt;MiniBatchSparsePCA&lt;/code&gt;&lt;/a&gt; does not implement &lt;code&gt;partial_fit&lt;/code&gt; because the algorithm is online along the features direction, not the samples direction.</source>
          <target state="translated">온라인 알고리즘의 정신에 있지만 &lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt; &lt;code&gt;MiniBatchSparsePCA&lt;/code&gt; &lt;/a&gt; 클래스 는 알고리즘이 샘플 방향이 아닌 기능 방향을 따라 온라인이기 때문에 &lt;code&gt;partial_fit&lt;/code&gt; 을 구현하지 않습니다 .</target>
        </trans-unit>
        <trans-unit id="c8d4a37562dd2bb8e9910fd82f43df80f682c63a" translate="yes" xml:space="preserve">
          <source>While many algorithms (such as SVM, K-nearest neighbors, and logistic regression) require features to be normalized, intuitively we can think of Principle Component Analysis (PCA) as being a prime example of when normalization is important. In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the &amp;lsquo;weight&amp;rsquo; axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect.</source>
          <target state="translated">많은 알고리즘 (예 : SVM, K- 최근 접 이웃 및 로지스틱 회귀)과 같이 기능을 정규화해야하지만 직관적으로 PCA (Principle Component Analysis)는 정규화가 중요한 경우의 주요 예라고 생각할 수 있습니다. PCA에서는 분산을 최대화하는 구성 요소에 관심이 있습니다. 하나의 구성 요소 (예 : 사람의 키)가 각각의 스케일 (미터 대 킬로)로 인해 다른 구성 요소 (예 : 무게)보다 덜 변하면, PCA는 최대 편차의 방향이 '무게'축과 더 밀접하게 일치한다고 판단 할 수 있습니다. 스케일되지 않습니다. 1 미터의 높이 변화는 1 킬로그램의 무게 변화보다 훨씬 더 중요하다고 간주 될 수 있기 때문에, 이것은 분명히 부정확합니다.</target>
        </trans-unit>
        <trans-unit id="3e272f5577ff59f34cc0835d41b4f8bd0e7fa207" translate="yes" xml:space="preserve">
          <source>While models saved using one version of scikit-learn might load in other versions, this is entirely unsupported and inadvisable. It should also be kept in mind that operations performed on such data could give different and unexpected results.</source>
          <target state="translated">한 버전의 scikit-learn을 사용하여 저장된 모델은 다른 버전에서로드 될 수 있지만 이는 완전히 지원되지 않으며 바람직하지 않습니다. 또한 이러한 데이터에 대해 수행 된 작업은 예상치 못한 결과를 초래할 수 있음을 명심해야합니다.</target>
        </trans-unit>
        <trans-unit id="5cdd1c0f02e03a5a1b156076678017404f8561ab" translate="yes" xml:space="preserve">
          <source>While multiclass data is provided to the metric, like binary targets, as an array of class labels, multilabel data is specified as an indicator matrix, in which cell &lt;code&gt;[i, j]&lt;/code&gt; has value 1 if sample &lt;code&gt;i&lt;/code&gt; has label &lt;code&gt;j&lt;/code&gt; and value 0 otherwise.</source>
          <target state="translated">이진 대상과 같은 멀티 클래스 데이터가 클래스 레이블의 배열로 메트릭에 제공되는 반면 멀티 라벨 데이터는 표시기 행렬로 지정됩니다. &lt;code&gt;[i, j]&lt;/code&gt; 샘플 &lt;code&gt;i&lt;/code&gt; 에 레이블 &lt;code&gt;j&lt;/code&gt; 가 있고 값이 0이면 셀 [i, j] 에 값 1이 있습니다. .</target>
        </trans-unit>
        <trans-unit id="ed422f68a65d31027201e13d55d1a7c59ba06f45" translate="yes" xml:space="preserve">
          <source>While not particularly fast to process, Python&amp;rsquo;s &lt;code&gt;dict&lt;/code&gt; has the advantages of being convenient to use, being sparse (absent features need not be stored) and storing feature names in addition to values.</source>
          <target state="translated">처리 속도가 빠르지는 않지만 Python의 &lt;code&gt;dict&lt;/code&gt; 에는 사용하기 편리하고, 희박하고 (없는 기능은 저장할 필요가 없음) 값 외에도 기능 이름을 저장하는 이점이 있습니다.</target>
        </trans-unit>
        <trans-unit id="6575e62afd8d9d86f5e5759f0c1f4bf12add49e4" translate="yes" xml:space="preserve">
          <source>While some local positioning information can be preserved by extracting n-grams instead of individual words, bag of words and bag of n-grams destroy most of the inner structure of the document and hence most of the meaning carried by that internal structure.</source>
          <target state="translated">개별 단어 대신 n- 그램을 추출하여 일부 로컬 위치 정보를 보존 할 수 있지만, bag of words 및 n-gram은 문서의 내부 구조의 대부분을 파괴하므로 해당 내부 구조가 의미하는 대부분의 의미를 파괴합니다.</target>
        </trans-unit>
        <trans-unit id="f9eb71a899b2efe02a5e86463ae919f1b60ed0f7" translate="yes" xml:space="preserve">
          <source>While the &lt;a href=&quot;generated/sklearn.decomposition.truncatedsvd#sklearn.decomposition.TruncatedSVD&quot;&gt;&lt;code&gt;TruncatedSVD&lt;/code&gt;&lt;/a&gt; transformer works with any (sparse) feature matrix, using it on tf&amp;ndash;idf matrices is recommended over raw frequency counts in an LSA/document processing setting. In particular, sublinear scaling and inverse document frequency should be turned on (&lt;code&gt;sublinear_tf=True, use_idf=True&lt;/code&gt;) to bring the feature values closer to a Gaussian distribution, compensating for LSA&amp;rsquo;s erroneous assumptions about textual data.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.truncatedsvd#sklearn.decomposition.TruncatedSVD&quot;&gt; &lt;code&gt;TruncatedSVD&lt;/code&gt; &lt;/a&gt; 변환기는 모든 (가급적) 피처 매트릭스와 함께 작동 하지만 LSA / 문서 처리 설정에서 원시 주파수 카운트보다 tf-idf 매트릭스에서이를 사용하는 것이 좋습니다. 특히, 피처 값을 가우시안 분포에 가깝게 가져 와서 텍스트 데이터에 대한 LSA의 잘못된 가정을 보상하기 위해 &lt;code&gt;sublinear_tf=True, use_idf=True&lt;/code&gt; 스케일링 및 역 문서 빈도를 켜야합니다 ( sublinear_tf = True, use_idf = True ).</target>
        </trans-unit>
        <trans-unit id="29a2650e25fbd5181744282ce886a2b96e4ac93d" translate="yes" xml:space="preserve">
          <source>While the above example sets the &lt;code&gt;standardize&lt;/code&gt; option to &lt;code&gt;False&lt;/code&gt;, &lt;a href=&quot;generated/sklearn.preprocessing.powertransformer#sklearn.preprocessing.PowerTransformer&quot;&gt;&lt;code&gt;PowerTransformer&lt;/code&gt;&lt;/a&gt; will apply zero-mean, unit-variance normalization to the transformed output by default.</source>
          <target state="translated">상기 예는 설정 동안 &lt;code&gt;standardize&lt;/code&gt; 옵션을 ' &lt;code&gt;False&lt;/code&gt; , &lt;a href=&quot;generated/sklearn.preprocessing.powertransformer#sklearn.preprocessing.PowerTransformer&quot;&gt; &lt;code&gt;PowerTransformer&lt;/code&gt; 는&lt;/a&gt; 기본적으로 변환 출력하는 제로 - 평균 단위 - 분산 정규화를 적용한다.</target>
        </trans-unit>
        <trans-unit id="e50cabfff84e84e9590f53c8299406f023e4e46d" translate="yes" xml:space="preserve">
          <source>While the hyperparameters chosen by optimizing LML have a considerable larger LML, they perform slightly worse according to the log-loss on test data. The figure shows that this is because they exhibit a steep change of the class probabilities at the class boundaries (which is good) but have predicted probabilities close to 0.5 far away from the class boundaries (which is bad) This undesirable effect is caused by the Laplace approximation used internally by GPC.</source>
          <target state="translated">LML을 최적화하여 선택한 하이퍼 파라미터는 상당히 큰 LML을 갖지만 테스트 데이터의 로그 손실에 따라 약간 성능이 떨어집니다. 이 그림은 클래스 경계에서 클래스 확률의 급격한 변화를 나타 내기 때문에 (좋은) 클래스 클래스 경계에서 0.5에 가까운 확률로 예측했기 때문에 (나쁜)이 바람직하지 않은 효과는 GPC에서 내부적으로 사용하는 라플라스 근사치.</target>
        </trans-unit>
        <trans-unit id="17e6df329175ba9fee759ed839a4afe469b184fd" translate="yes" xml:space="preserve">
          <source>While the tf&amp;ndash;idf normalization is often very useful, there might be cases where the binary occurrence markers might offer better features. This can be achieved by using the &lt;code&gt;binary&lt;/code&gt; parameter of &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;. In particular, some estimators such as &lt;a href=&quot;naive_bayes#bernoulli-naive-bayes&quot;&gt;Bernoulli Naive Bayes&lt;/a&gt; explicitly model discrete boolean random variables. Also, very short texts are likely to have noisy tf&amp;ndash;idf values while the binary occurrence info is more stable.</source>
          <target state="translated">tf-idf 정규화는 종종 매우 유용하지만 이진 발생 마커가 더 나은 기능을 제공 할 수 있습니다. &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;binary&lt;/code&gt; 매개 변수를 사용하면 됩니다. 특히, &lt;a href=&quot;naive_bayes#bernoulli-naive-bayes&quot;&gt;Bernoulli Naive Bayes&lt;/a&gt; 와 같은 일부 추정량에서는 불연속 부울 랜덤 변수를 명시 적으로 모델링합니다. 또한 매우 짧은 텍스트에는 노이즈가 많은 tf-idf 값이있을 수 있지만 이진 발생 정보는 더 안정적입니다.</target>
        </trans-unit>
        <trans-unit id="ed181b0221327c005991b804ef6da84808fa6b75" translate="yes" xml:space="preserve">
          <source>While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional data.</source>
          <target state="translated">이 예제는 알고리즘에 대한 직관을 제공하지만이 직감은 매우 높은 차원의 데이터에는 적용되지 않을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="20e141fdf1f5d5d16051f029790d3b9e841c723d" translate="yes" xml:space="preserve">
          <source>While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. &lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt;&lt;code&gt;RandomizedSearchCV&lt;/code&gt;&lt;/a&gt; implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:</source>
          <target state="translated">매개 변수 설정 그리드를 사용하는 것이 현재 가장 널리 사용되는 매개 변수 최적화 방법이지만 다른 검색 방법에는 더 유리한 특성이 있습니다. &lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt; &lt;code&gt;RandomizedSearchCV&lt;/code&gt; &lt;/a&gt; 는 매개 변수에 대한 무작위 검색을 구현하며, 여기서 각 설정은 가능한 매개 변수 값에 대한 분포에서 샘플링됩니다. 이는 철저한 검색에 비해 두 가지 주요 이점이 있습니다.</target>
        </trans-unit>
        <trans-unit id="ae7c1638fd1917cb535ca7c68b4bd5f19a47ea30" translate="yes" xml:space="preserve">
          <source>White kernel.</source>
          <target state="translated">화이트 커널.</target>
        </trans-unit>
        <trans-unit id="6eaf9e8193566018ccba0d72a95d7647c23f2585" translate="yes" xml:space="preserve">
          <source>Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions.</source>
          <target state="translated">화이트닝은 변환 된 신호 (구성 요소의 상대 분산 스케일)에서 일부 정보를 제거하지만 데이터를 고정 배선 가정을 존중하여 다운 스트림 추정기의 예측 정확도를 언젠가 향상시킬 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="07aaa00ad7b994406ce70b8bd7598f7e15e6859a" translate="yes" xml:space="preserve">
          <source>Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometimes improve the predictive accuracy of the downstream estimators by making data respect some hard-wired assumptions.</source>
          <target state="translated">화이트닝은 변환 된 신호 (구성 요소의 상대 분산 스케일)에서 일부 정보를 제거하지만 데이터를 일부 고정 배선 가정을 존중하도록하여 다운 스트림 추정기의 예측 정확도를 향상시킬 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b5ed864ec9d16ad31c6639d1d4c3bf64e3372001" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for Davies-Bouldin index.</source>
          <target state="translated">Davies-Bouldin 지수에 대한 Wikipedia 항목.</target>
        </trans-unit>
        <trans-unit id="a0184957526e21d06d99d8f077fe30eb4aaec4f9" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for contingency matrix</source>
          <target state="translated">우발성 매트릭스에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="55a3b17abc1268c1d436ba897c97b456b993b4ea" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the (normalized) Mutual Information</source>
          <target state="translated">(정규화 된) 상호 정보에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="1f069c9fec7504cb4f8a493de2e1b54ffc547081" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Adjusted Mutual Information</source>
          <target state="translated">조정 된 상호 정보에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="ca2fe3eff096e2c0ff94d3c0f6ce61af74cc646f" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Brier score.</source>
          <target state="translated">Brier 점수를위한 Wikipedia 항목.</target>
        </trans-unit>
        <trans-unit id="ffd655e9eb3a21416da69aac696bc5ce043a000f" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Cohen&amp;rsquo;s kappa.</source>
          <target state="translated">코헨의 카파 위키 백과.</target>
        </trans-unit>
        <trans-unit id="8d8ae14fc3bcf00321ca2d4b9c37c609195c6275" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the F1-score</source>
          <target state="translated">F1- 점수에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="0d85777073541b6f8aecb3488f1962f6903fd77c" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Fowlkes-Mallows Index</source>
          <target state="translated">Fowlkes-Mallows Index에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="738fb31d9583a6207339f58c0335e89437aa096f" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Jaccard index</source>
          <target state="translated">Jaccard 인덱스에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="d69dce297a7e32abae3549494346594b424875bc" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Matthews Correlation Coefficient</source>
          <target state="translated">Matthews 상관 계수에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="d1c0692994293b3fef98ac5de7dd74e23175c8d1" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Precision and recall</source>
          <target state="translated">정밀도 및 리콜을위한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="6c2dd7ccbd3afed766d1ee6ce92b068445c27bbb" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Receiver operating characteristic</source>
          <target state="translated">수신기 작동 특성에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="caae1d529b64ebeb0d4804273e9107122a389ac6" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the adjusted Rand index</source>
          <target state="translated">조정 랜드 인덱스에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="af0472efa729237e92d89bb05e9ca0c8e7f37b5f" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Coefficient of determination</source>
          <target state="translated">결정 계수에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="e345be5719f19335870d8d3a8cdd20b6bd307aa0" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Hamming distance</source>
          <target state="translated">해밍 거리의 위키 백과 항목</target>
        </trans-unit>
        <trans-unit id="1857fa6b095ad66d104ea60f4be3df45f12529a3" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Hinge loss</source>
          <target state="translated">힌지 손실에 대한 위키 백과 항목</target>
        </trans-unit>
        <trans-unit id="d4ccd1b47442c7552ebe73794cdd38515c5ffdef" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Lasso</source>
          <target state="translated">올가미의 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="8751f23b19110bb289e70c6d8c900548f6c9b761" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Least-angle regression</source>
          <target state="translated">최소 각 회귀에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="ed8f4a303fe71f9ad0ec1e1b74ef6fe644dad80d" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Silhouette Coefficient</source>
          <target state="translated">실루엣 계수에 대한 Wikipedia 항목</target>
        </trans-unit>
        <trans-unit id="0b665174747365aef367583fb0c32fb021d06a22" translate="yes" xml:space="preserve">
          <source>Wikipedia principal eigenvector</source>
          <target state="translated">Wikipedia 주요 고유 벡터</target>
        </trans-unit>
        <trans-unit id="713348b23d025b202ea7f033591c046a82a1973b" translate="yes" xml:space="preserve">
          <source>Will be ignored when &lt;code&gt;y_true&lt;/code&gt; is binary.</source>
          <target state="translated">&lt;code&gt;y_true&lt;/code&gt; 가 이진 이면 무시됩니다 .</target>
        </trans-unit>
        <trans-unit id="af498f4dd6f24dbc1f93745e77fe6ed29d0b9d0c" translate="yes" xml:space="preserve">
          <source>Will return sparse matrix if set True else will return an array.</source>
          <target state="translated">True로 설정하면 희소 행렬을 반환하고 그렇지 않으면 배열을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="f02c359862a5df44abc185413e06bdb77cfc5770" translate="yes" xml:space="preserve">
          <source>Williams, C.K.I. and Seeger, M. &amp;ldquo;Using the Nystroem method to speed up kernel machines&amp;rdquo;, Advances in neural information processing systems 2001</source>
          <target state="translated">Williams, CKI 및 Seeger, M.&amp;ldquo;Nystroem 방법을 사용하여 커널 시스템 속도 향상&amp;rdquo;, 신경 정보 처리 시스템의 발전 2001</target>
        </trans-unit>
        <trans-unit id="a20af0cf6ba0496377888d152bfba536fcfdefc1" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;adjusted=True&lt;/code&gt;, balanced accuracy reports the relative increase from \(\texttt{balanced-accuracy}(y, \mathbf{0}, w) = \frac{1}{\text{n\_classes}}\). In the binary case, this is also known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Youden%27s_J_statistic&quot;&gt;*Youden&amp;rsquo;s J statistic*&lt;/a&gt;, or &lt;em&gt;informedness&lt;/em&gt;.</source>
          <target state="translated">로 &lt;code&gt;adjusted=True&lt;/code&gt; 균형 정밀도 \에서 상대적 증가 (\ texttt {균형 정밀도} (Y \ mathbf {0}, w) = \ FRAC {1} {\ 텍스트 {n \ _classes를}} \)보고. 이진 경우에는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Youden%27s_J_statistic&quot;&gt;* Youden 's J statistic *&lt;/a&gt; 또는 &lt;em&gt;informedness&lt;/em&gt; 라고도 합니다.</target>
        </trans-unit>
        <trans-unit id="8a7d860e7dc8979710329f97e747eaff0d3415d3" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;early_stopping=False&lt;/code&gt;, the model is fitted on the entire input data and the stopping criterion is based on the objective function computed on the input data.</source>
          <target state="translated">함께 &lt;code&gt;early_stopping=False&lt;/code&gt; 모델은 전체 입력 데이터에 장착되고, 정지 기준은 상기 입력 데이터에 대해 계산 된 목적 함수에 기초한다.</target>
        </trans-unit>
        <trans-unit id="4ceb9e226f3e04a8a66252e3801eed93f740afd9" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;early_stopping=True&lt;/code&gt;, the input data is split into a training set and a validation set. The model is then fitted on the training set, and the stopping criterion is based on the prediction score computed on the validation set. The size of the validation set can be changed with the parameter &lt;code&gt;validation_fraction&lt;/code&gt;.</source>
          <target state="translated">함께 &lt;code&gt;early_stopping=True&lt;/code&gt; , 입력 데이터는 트레이닝 세트와 검증 집합으로 분할된다. 그런 다음 모델은 학습 세트에 맞춰지고 중지 기준은 유효성 검사 세트에서 계산 된 예측 점수를 기반으로합니다. 검증 세트의 크기는 파라미터로 변경할 수 있습니다 &lt;code&gt;validation_fraction&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="318aced6d4dfc924ad223bd54e79ede301143b06" translate="yes" xml:space="preserve">
          <source>With SGD or Adam, training supports online and mini-batch learning.</source>
          <target state="translated">SGD 또는 Adam을 통해 교육은 온라인 및 미니 배치 학습을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="bebfaf6a5f7ee4311c7425773ef87a0b1b61dcc0" translate="yes" xml:space="preserve">
          <source>With SVMs and logistic-regression, the parameter C controls the sparsity: the smaller C the fewer features selected. With Lasso, the higher the alpha parameter, the fewer features selected.</source>
          <target state="translated">SVM 및 로지스틱 회귀 분석을 통해 매개 변수 C는 희소성을 제어합니다. C가 작을수록 선택된 기능이 줄어 듭니다. 알파 매개 변수가 높을수록 기능이 적게 선택된 올가미를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="2e07775067fbbb8cee792ed1d4b4b0282fd223be" translate="yes" xml:space="preserve">
          <source>With \(P'(j) = |V_j| / N\). The mutual information (MI) between \(U\) and \(V\) is calculated by:</source>
          <target state="translated">\ (P '(j) = | V_j | / N \)로. \ (U \)와 \ (V \) 사이의 상호 정보 (MI)는 다음에 의해 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="5313dd287c9c493fc21b86c81282cea7d0608304" translate="yes" xml:space="preserve">
          <source>With agglomerative clustering, it is possible to specify which samples can be clustered together by giving a connectivity graph. Graphs in scikit-learn are represented by their adjacency matrix. Often, a sparse matrix is used. This can be useful, for instance, to retrieve connected regions (sometimes also referred to as connected components) when clustering an image:</source>
          <target state="translated">응집 클러스터링을 사용하면 연결 그래프를 제공하여 함께 클러스터링 할 수있는 샘플을 지정할 수 있습니다. Scikit-learn의 그래프는 인접 행렬로 표시됩니다. 종종 희소 행렬이 사용됩니다. 예를 들어 이미지를 클러스터링 할 때 연결된 영역 (때로는 연결된 구성 요소라고도 함)을 검색하는 데 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ebae629f7af13ae26b867ab75161458173d10bdc" translate="yes" xml:space="preserve">
          <source>With regard to decision trees, this strategy can readily be used to support multi-output problems. This requires the following changes:</source>
          <target state="translated">의사 결정 트리와 관련하여이 전략을 사용하여 다중 출력 문제를 쉽게 지원할 수 있습니다. 다음과 같은 변경이 필요합니다.</target>
        </trans-unit>
        <trans-unit id="d6eab2b8513179355ba20cab88473d0665849027" translate="yes" xml:space="preserve">
          <source>With such an abundance of clues that distinguish newsgroups, the classifiers barely have to identify topics from text at all, and they all perform at the same high level.</source>
          <target state="translated">뉴스 그룹을 구별 할 수있는 풍부한 단서가 있기 때문에 분류자는 텍스트에서 주제를 거의 식별하지 않아도되며 모두 동일한 수준에서 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="ba25a12704b8225df22eb5cee35ebe73afb76c8b" translate="yes" xml:space="preserve">
          <source>With sum_over_features equal to False it returns the componentwise distances.</source>
          <target state="translated">sum_over_features가 False 인 경우 구성 요소 별 거리를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="03d84c3da120d3c6633bcd8017a1f00e1bb6dad8" translate="yes" xml:space="preserve">
          <source>With this class, the base_estimator is fit on the train set of the cross-validation generator and the test set is used for calibration. The probabilities for each of the folds are then averaged for prediction. In case that cv=&amp;rdquo;prefit&amp;rdquo; is passed to __init__, it is assumed that base_estimator has been fitted already and all data is used for calibration. Note that data for fitting the classifier and for calibrating it must be disjoint.</source>
          <target state="translated">이 클래스를 사용하면 base_estimator가 교차 검증 생성기의 트레인 세트에 적합하고 테스트 세트가 교정에 사용됩니다. 각각의 접힘에 대한 확률은 예측을 위해 평균화된다. cv =&amp;rdquo;prefit&amp;rdquo;이 __init__에 전달 된 경우 base_estimator가 이미 장착 된 것으로 가정하고 모든 데이터가 교정에 사용됩니다. 분류기를 장착하고 교정하기위한 데이터는 분리되어 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="07ec442186310e3d4d8de1ef730f033183a12d2a" translate="yes" xml:space="preserve">
          <source>With this re-labeling of the data, our problem can be written</source>
          <target state="translated">이 데이터의 레이블을 다시 지정하면 문제를 작성할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="bb9dc2936468de0109f9958206c68ba68552df6f" translate="yes" xml:space="preserve">
          <source>With this setup, a single distance calculation between a test point and the centroid is sufficient to determine a lower and upper bound on the distance to all points within the node. Because of the spherical geometry of the ball tree nodes, it can out-perform a &lt;em&gt;KD-tree&lt;/em&gt; in high dimensions, though the actual performance is highly dependent on the structure of the training data. In scikit-learn, ball-tree-based neighbors searches are specified using the keyword &lt;code&gt;algorithm = 'ball_tree'&lt;/code&gt;, and are computed using the class &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;sklearn.neighbors.BallTree&lt;/code&gt;&lt;/a&gt;. Alternatively, the user can work with the &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; class directly.</source>
          <target state="translated">이 설정을 사용하면 테스트 포인트와 중심 사이의 단일 거리 계산으로 노드 내의 모든 포인트까지의 거리에 대한 하한 및 상한을 결정할 수 있습니다. 볼 트리 노드의 구형 형상으로 인해 실제 성능은 훈련 데이터의 구조에 크게 의존하지만 &lt;em&gt;KD 트리&lt;/em&gt; 보다 높은 차원에서 성능을 발휘할 수 있습니다. 공 - 트리 기반의 이웃 검색 키워드 사용하여 지정, scikit - 학습 &lt;code&gt;algorithm = 'ball_tree'&lt;/code&gt; , 및 클래스 사용하여 계산된다 &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;sklearn.neighbors.BallTree&lt;/code&gt; 을&lt;/a&gt; . 또는 사용자는 &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt; 클래스를 직접 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c5b891d6db4b2b53f2c329c62187e665d9759f8a" translate="yes" xml:space="preserve">
          <source>Without any prior information on the sample, the number of projections required to reconstruct the image is of the order of the linear size &lt;code&gt;l&lt;/code&gt; of the image (in pixels). For simplicity we consider here a sparse image, where only pixels on the boundary of objects have a non-zero value. Such data could correspond for example to a cellular material. Note however that most images are sparse in a different basis, such as the Haar wavelets. Only &lt;code&gt;l/7&lt;/code&gt; projections are acquired, therefore it is necessary to use prior information available on the sample (its sparsity): this is an example of &lt;strong&gt;compressive sensing&lt;/strong&gt;.</source>
          <target state="translated">샘플에 대한 사전 정보가 없으면 이미지를 재구성하는 데 필요한 투영 수는 이미지의 선형 크기 &lt;code&gt;l&lt;/code&gt; 정도 입니다 (픽셀). 단순화를 위해 여기서는 객체 경계의 픽셀 만 0이 아닌 희소 이미지로 간주합니다. 이러한 데이터는 예를 들어 셀룰러 물질에 해당 할 수있다. 그러나 대부분의 이미지는 Haar 웨이블릿과 같이 다른 기준으로 희소합니다. 단지 &lt;code&gt;l/7&lt;/code&gt; 돌기 따라서 샘플 (그 희소성)에서 사용할 사전 정보를 사용하는 것이 필요하고, 취득한 :이 예이다 &lt;strong&gt;압축 센싱&lt;/strong&gt; .</target>
        </trans-unit>
        <trans-unit id="087783a9ac4373b41b03a4f66d5eaf61d7d47ff1" translate="yes" xml:space="preserve">
          <source>Without reduce_func:</source>
          <target state="translated">reduce_func없이 :</target>
        </trans-unit>
        <trans-unit id="e6002e635270be50830b0534ba0aafc304922d8b" translate="yes" xml:space="preserve">
          <source>Without shuffling, &lt;code&gt;X&lt;/code&gt; horizontally stacks features in the following order: the primary &lt;code&gt;n_informative&lt;/code&gt; features, followed by &lt;code&gt;n_redundant&lt;/code&gt; linear combinations of the informative features, followed by &lt;code&gt;n_repeated&lt;/code&gt; duplicates, drawn randomly with replacement from the informative and redundant features. The remaining features are filled with random noise. Thus, without shuffling, all useful features are contained in the columns &lt;code&gt;X[:, :n_informative + n_redundant + n_repeated]&lt;/code&gt;.</source>
          <target state="translated">셔플 링없이 &lt;code&gt;X&lt;/code&gt; 는 기본 &lt;code&gt;n_informative&lt;/code&gt; 기능, 정보 기능의 &lt;code&gt;n_redundant&lt;/code&gt; 선형 조합, &lt;code&gt;n_repeated&lt;/code&gt; 복제본, 정보 및 중복 기능의 대체로 무작위로 추출 된 순서로 기능을 가로 로 쌓습니다 . 나머지 기능은 임의 노이즈로 채워집니다. 따라서 셔플 링없이 모든 유용한 기능이 &lt;code&gt;X[:, :n_informative + n_redundant + n_repeated]&lt;/code&gt; 열에 포함됩니다 .</target>
        </trans-unit>
        <trans-unit id="d2a146386973596d64e5c0f348ec45ab36bab658" translate="yes" xml:space="preserve">
          <source>Working With Text Data</source>
          <target state="translated">텍스트 데이터 작업</target>
        </trans-unit>
        <trans-unit id="5b5ef6667bd92ea247084ea267c265251f4aa7de" translate="yes" xml:space="preserve">
          <source>Works with sparse matrices. Only works if &lt;code&gt;rows_&lt;/code&gt; and &lt;code&gt;columns_&lt;/code&gt; attributes exist.</source>
          <target state="translated">희소 행렬과 함께 작동합니다. &lt;code&gt;rows_&lt;/code&gt; 및 &lt;code&gt;columns_&lt;/code&gt; 속성이 존재하는 경우에만 작동 합니다.</target>
        </trans-unit>
        <trans-unit id="926da419b9cc98b9060a6d00fb8d48cd55be86f9" translate="yes" xml:space="preserve">
          <source>Wrapper for kernels in sklearn.metrics.pairwise.</source>
          <target state="translated">sklearn.metrics.pairwise의 커널 래퍼입니다.</target>
        </trans-unit>
        <trans-unit id="f986c2ac1f7dce99239d5b1ba2c2c97de265f3fa" translate="yes" xml:space="preserve">
          <source>Write a text classification pipeline to classify movie reviews as either positive or negative.</source>
          <target state="translated">영화 리뷰를 긍정적 또는 부정적으로 분류하기 위해 텍스트 분류 파이프 라인을 작성하십시오.</target>
        </trans-unit>
        <trans-unit id="c1b32a0493a32a44864910f6b6b9c9398af4b20e" translate="yes" xml:space="preserve">
          <source>Write a text classification pipeline using a custom preprocessor and &lt;code&gt;CharNGramAnalyzer&lt;/code&gt; using data from Wikipedia articles as training set.</source>
          <target state="translated">Wikipedia 기사의 데이터를 학습 세트로 사용하여 사용자 지정 전처리 &lt;code&gt;CharNGramAnalyzer&lt;/code&gt; 사용 하여 텍스트 분류 파이프 라인을 작성하십시오 .</target>
        </trans-unit>
        <trans-unit id="c72e193d2469d6cfb2918ba7a00dbc8ed1d451d6" translate="yes" xml:space="preserve">
          <source>Wu, Lin and Weng, &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf&quot;&gt;&amp;ldquo;Probability estimates for multi-class classification by pairwise coupling&amp;rdquo;&lt;/a&gt;, JMLR 5:975-1005, 2004.</source>
          <target state="translated">Wu, Lin 및 Weng, &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf&quot;&gt;&amp;ldquo;페어 와이즈 커플 링에 의한 멀티 클래스 분류의 확률 추정치&amp;rdquo;&lt;/a&gt; , JMLR 5 : 975-1005, 2004.</target>
        </trans-unit>
        <trans-unit id="c8c1574205d07b839af62817660ba2b78f320cd0" translate="yes" xml:space="preserve">
          <source>X block loadings vectors.</source>
          <target state="translated">X 블록 로딩 벡터.</target>
        </trans-unit>
        <trans-unit id="b8076ad410e1a569012d16107ff003e5d358439f" translate="yes" xml:space="preserve">
          <source>X block to latents rotations.</source>
          <target state="translated">잠복 회전에 대한 X 블록.</target>
        </trans-unit>
        <trans-unit id="a6b8640132f42899bc713ee5acc307439a5b7049" translate="yes" xml:space="preserve">
          <source>X block weights vectors.</source>
          <target state="translated">X 블록 가중치 벡터.</target>
        </trans-unit>
        <trans-unit id="e6bc2e58339df2a473a9897261f25e31780f738c" translate="yes" xml:space="preserve">
          <source>X is projected on the first principal components previously extracted from a training set.</source>
          <target state="translated">X는 이전에 훈련 세트에서 추출한 첫 번째 주요 구성 요소에 투영됩니다.</target>
        </trans-unit>
        <trans-unit id="7228d382c859d348525ccb5bce51e5752c38bc04" translate="yes" xml:space="preserve">
          <source>X is stored for future use, as &lt;code&gt;transform&lt;/code&gt; needs X to interpolate new input data.</source>
          <target state="translated">X는 향후 사용을 위해 저장되는 &lt;code&gt;transform&lt;/code&gt; 보간 새로운 입력 데이터 X의 요구한다.</target>
        </trans-unit>
        <trans-unit id="3074bef8d8da5f206ce501f5438e3d5abb038064" translate="yes" xml:space="preserve">
          <source>X must have been produced by this DictVectorizer&amp;rsquo;s transform or fit_transform method; it may only have passed through transformers that preserve the number of features and their order.</source>
          <target state="translated">X는이 DictVectorizer의 transform 또는 fit_transform 메소드에 의해 생성되어야합니다. 기능의 수와 순서를 유지하는 변압기 만 통과했을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d0ad8e13f68af13dec8ad59c4f3a6a0df7a4de08" translate="yes" xml:space="preserve">
          <source>X scores.</source>
          <target state="translated">X 점수.</target>
        </trans-unit>
        <trans-unit id="28a3e4c54c0fde2f1aaa67a11fe405d430c2fe41" translate="yes" xml:space="preserve">
          <source>X transformed in the new space.</source>
          <target state="translated">새로운 공간에서 X가 변형되었습니다.</target>
        </trans-unit>
        <trans-unit id="feedfda54d7431e28acb98075b2c2bd9cf8331f2" translate="yes" xml:space="preserve">
          <source>Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon University, 2002 &lt;a href=&quot;http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf&quot;&gt;http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf&lt;/a&gt;</source>
          <target state="translated">Xiaojin Zhu와 Zoubin Ghahramani. 레이블 전파를 통해 레이블이있는 데이터와 레이블이없는 데이터로부터 학습 기술 보고서 ​​CMU-CALD-02-107, 2002 년 카네기 멜론 대학교 &lt;a href=&quot;http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf&quot;&gt;http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5c986ee528dd9bdf7a6c8d0101f77976c47ae9d2" translate="yes" xml:space="preserve">
          <source>Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang: &lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;Theil-Sen Estimators in a Multiple Linear Regression Model.&lt;/a&gt;</source>
          <target state="translated">Xin Dang, Hanxiang Peng, Xueqin Wang 및 Heping Zhang : &lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;다중 선형 회귀 모델의 테일 센 추정기&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c93217dd923de34853280b8058e56203ef9ee737" translate="yes" xml:space="preserve">
          <source>Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed.</source>
          <target state="translated">Xy = 사전 계산할 수있는 np.dot (XT, y) 그램 행렬이 사전 계산 된 경우에만 유용합니다.</target>
        </trans-unit>
        <trans-unit id="23eb4d3f4155395a74e9d534f97ff4c1908f5aac" translate="yes" xml:space="preserve">
          <source>Y</source>
          <target state="translated">Y</target>
        </trans-unit>
        <trans-unit id="aac13ced89d2b311880e53ba16f36f4513402a98" translate="yes" xml:space="preserve">
          <source>Y block loadings vectors.</source>
          <target state="translated">Y 블록 로딩 벡터.</target>
        </trans-unit>
        <trans-unit id="148708c0aec99251158277d2fc4d038d62f32551" translate="yes" xml:space="preserve">
          <source>Y block to latents rotations.</source>
          <target state="translated">잠복 회전에 Y 블록.</target>
        </trans-unit>
        <trans-unit id="8f4ded8aca1a84f4452774f8bc622751045ade48" translate="yes" xml:space="preserve">
          <source>Y block weights vectors.</source>
          <target state="translated">Y 블록 가중치 벡터.</target>
        </trans-unit>
        <trans-unit id="780dd8f1641062cfc0af001d2fcfedba3262be26" translate="yes" xml:space="preserve">
          <source>Y scores.</source>
          <target state="translated">Y 점수.</target>
        </trans-unit>
        <trans-unit id="93b5936ef31b077aecad8b961838c412166e9fd0" translate="yes" xml:space="preserve">
          <source>Y. Freund, R. Schapire, &amp;ldquo;A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting&amp;rdquo;, 1995.</source>
          <target state="translated">Y. Freund, R. Schapire,&amp;ldquo;온라인 학습의 의사 결정 이론 일반화 및 부스팅 적용&amp;rdquo;, 1995.</target>
        </trans-unit>
        <trans-unit id="bf931371fe813e68af145bc28f1f0c59ead42876" translate="yes" xml:space="preserve">
          <source>Y. Freund, and R. Schapire, &amp;ldquo;A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting&amp;rdquo;, 1997.</source>
          <target state="translated">Y. Freund, R. Schapire,&amp;ldquo;온라인 학습의 의사 결정 이론적 일반화 및 응용 프로그램 향상&amp;rdquo;, 1997.</target>
        </trans-unit>
        <trans-unit id="982b5c305af507a5853864a0280fe0330e5fb9d9" translate="yes" xml:space="preserve">
          <source>Y[argmin[i], :] is the row in Y that is closest to X[i, :].</source>
          <target state="translated">Y [argmin [i], :]는 X [i, :]에 가장 가까운 Y의 행입니다.</target>
        </trans-unit>
        <trans-unit id="3526f607bcd4f51ad0bc05f814579a42c2c0ba57" translate="yes" xml:space="preserve">
          <source>Yellow</source>
          <target state="translated">Yellow</target>
        </trans-unit>
        <trans-unit id="44e848b37858df8125129ee3d3911783b05fb21f" translate="yes" xml:space="preserve">
          <source>Yields indices to split data into training and test sets.</source>
          <target state="translated">데이터를 교육 및 테스트 세트로 분할하기위한 인덱스를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="c970e3f1e790a2a4cd28b40401902501b9bc2d74" translate="yes" xml:space="preserve">
          <source>Yields:</source>
          <target state="translated">Yields:</target>
        </trans-unit>
        <trans-unit id="e285a8203a02b899c727c909bb971f8a5290d1a9" translate="yes" xml:space="preserve">
          <source>You can &lt;a href=&quot;grid_search#grid-search&quot;&gt;grid search&lt;/a&gt; over parameters of all estimators in the pipeline at once.</source>
          <target state="translated">파이프 라인에서 모든 추정기의 매개 변수를 한 번에 &lt;a href=&quot;grid_search#grid-search&quot;&gt;그리드 검색&lt;/a&gt; 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="e4f0eb08d1e594cb4ba39dda583b2124c29e8d3e" translate="yes" xml:space="preserve">
          <source>You can adjust the number of categories by giving their names to the dataset loader or setting them to None to get the 20 of them.</source>
          <target state="translated">데이터 세트 로더에 이름을 제공하거나 카테고리를 없음으로 설정하여 카테고리 수를 조정하여 카테고리 수를 조정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d6a91645b832623d5d5110588ef04aebdc451880" translate="yes" xml:space="preserve">
          <source>You can already copy the skeletons into a new folder somewhere on your hard-drive named &lt;code&gt;sklearn_tut_workspace&lt;/code&gt; where you will edit your own files for the exercises while keeping the original skeletons intact:</source>
          <target state="translated">&lt;code&gt;sklearn_tut_workspace&lt;/code&gt; 라는 하드 드라이브의 어딘가에 골격을 이미 복사 할 수 있습니다. 여기서 원래 골격을 그대로 유지하면서 연습을 위해 자신의 파일을 편집 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="1cf0d94595a131d36f8236532aeafb049eaa6dbc" translate="yes" xml:space="preserve">
          <source>You can also specify both the name and the version, which also uniquely identifies the dataset:</source>
          <target state="translated">이름과 버전을 모두 지정하여 데이터 세트를 고유하게 식별 할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="ae5f0da778f6ff8c0d5d1c2ccd6f86bebea3d9e0" translate="yes" xml:space="preserve">
          <source>You can also use your own defined kernels by passing a function to the keyword &lt;code&gt;kernel&lt;/code&gt; in the constructor.</source>
          <target state="translated">생성자 의 키워드 &lt;code&gt;kernel&lt;/code&gt; 에 함수를 전달하여 자체 정의 된 커널을 사용할 수도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="c90fd70bc9584ad72350fadce865213cd8c472be" translate="yes" xml:space="preserve">
          <source>You can combine &lt;code&gt;KBinsDiscretizer&lt;/code&gt; with &lt;a href=&quot;sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt; if you only want to preprocess part of the features.</source>
          <target state="translated">당신은 결합 할 수 있습니다 &lt;code&gt;KBinsDiscretizer&lt;/code&gt; 을 함께 &lt;a href=&quot;sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt; &lt;/a&gt; 당신은 단지 기능의 전처리 부분에 원하는 경우.</target>
        </trans-unit>
        <trans-unit id="579522a3d8d5bf3b4958e7c2d681266388521dce" translate="yes" xml:space="preserve">
          <source>You can define your own kernels by either giving the kernel as a python function or by precomputing the Gram matrix.</source>
          <target state="translated">커널을 파이썬 함수로 제공하거나 Gram 매트릭스를 사전 계산하여 자신의 커널을 정의 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5a14b3f83e3bd81fa3adef5b677dc7c591d450db" translate="yes" xml:space="preserve">
          <source>You can display the BLAS / LAPACK implementation used by your NumPy / SciPy / scikit-learn install with the following commands:</source>
          <target state="translated">다음 명령을 사용하여 NumPy / SciPy / scikit-learn 설치에 사용 된 BLAS / LAPACK 구현을 표시 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="929abd63168ac2d721d4708b8ef8be3cd51b08a0" translate="yes" xml:space="preserve">
          <source>You can ensure that &lt;code&gt;func&lt;/code&gt; and &lt;code&gt;inverse_func&lt;/code&gt; are the inverse of each other by setting &lt;code&gt;check_inverse=True&lt;/code&gt; and calling &lt;code&gt;fit&lt;/code&gt; before &lt;code&gt;transform&lt;/code&gt;. Please note that a warning is raised and can be turned into an error with a &lt;code&gt;filterwarnings&lt;/code&gt;:</source>
          <target state="translated">&lt;code&gt;inverse_func&lt;/code&gt; &lt;code&gt;check_inverse=True&lt;/code&gt; 를 설정 하고 &lt;code&gt;transform&lt;/code&gt; before &lt;code&gt;fit&lt;/code&gt; 을 호출 하여 &lt;code&gt;func&lt;/code&gt; 과 inverse_func 가 서로 역함을 보장 할 수 있습니다 . 경고가 발생하고 &lt;code&gt;filterwarnings&lt;/code&gt; 경고로 인해 오류가 발생할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="8fe8cd91261eb27750ae7b2f82fa15c24077f346" translate="yes" xml:space="preserve">
          <source>You can generate even more flexible model scorers by constructing your own scoring object from scratch, without using the &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt;&lt;code&gt;make_scorer&lt;/code&gt;&lt;/a&gt; factory. For a callable to be a scorer, it needs to meet the protocol specified by the following two rules:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt; &lt;code&gt;make_scorer&lt;/code&gt; &lt;/a&gt; 팩토리 를 사용하지 않고 자체 스코어링 오브젝트를 처음부터 구성하여보다 유연한 모델 스코어러를 생성 할 수 있습니다 . 콜 러블이 스코어러가 되려면 다음 두 규칙에 지정된 프로토콜을 충족해야합니다.</target>
        </trans-unit>
        <trans-unit id="cb07d258c61c328d902779de990b642f82ba2beb" translate="yes" xml:space="preserve">
          <source>You can get more information on the dataset by looking at the &lt;code&gt;DESCR&lt;/code&gt; and &lt;code&gt;details&lt;/code&gt; attributes:</source>
          <target state="translated">&lt;code&gt;DESCR&lt;/code&gt; 및 &lt;code&gt;details&lt;/code&gt; 속성 을보고 데이터 세트에 대한 자세한 정보를 얻을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="316dc294ff0e2890db335b30189c691f1a723809" translate="yes" xml:space="preserve">
          <source>You can now see many things that these features have overfit to:</source>
          <target state="translated">이제이 기능들이 과적 합한 많은 것들을 볼 수 있습니다 :</target>
        </trans-unit>
        <trans-unit id="ee86b8b814976ee239ecfc8e86907133be9d3afc" translate="yes" xml:space="preserve">
          <source>You can see that 16 non-zero feature tokens were extracted in the vector output: this is less than the 19 non-zeros extracted previously by the &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; on the same toy corpus. The discrepancy comes from hash function collisions because of the low value of the &lt;code&gt;n_features&lt;/code&gt; parameter.</source>
          <target state="translated">벡터 출력에서 ​​0이 아닌 기능 토큰 16 개가 추출 된 것을 볼 수 있습니다. 이는 동일한 장난감 코퍼스 에서 &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt; 에 의해 이전에 추출 된 0이 아닌 0보다 작습니다 . 불일치는 &lt;code&gt;n_features&lt;/code&gt; 매개 변수 의 낮은 값으로 인해 해시 함수 충돌에서 비롯 됩니다.</target>
        </trans-unit>
        <trans-unit id="c31033fd31d22147ea7534b97a7d63f646fe3e48" translate="yes" xml:space="preserve">
          <source>You can then edit the content of the workspace without fear of losing the original exercise instructions.</source>
          <target state="translated">그런 다음 원래 연습 지침을 잃을 염려없이 작업 공간의 내용을 편집 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e21dcf7dc4d8353d8949b3e6ddc2c35c364a9b4a" translate="yes" xml:space="preserve">
          <source>You cannot nest objects with parallel computing (&lt;code&gt;n_jobs&lt;/code&gt; different than 1).</source>
          <target state="translated">병렬 컴퓨팅 ( 1과 다른 &lt;code&gt;n_jobs&lt;/code&gt; )으로 오브젝트를 중첩 할 수 없습니다 .</target>
        </trans-unit>
        <trans-unit id="d086a1b811e8ad563a3cd7d98758c535aff811c7" translate="yes" xml:space="preserve">
          <source>You could try UTF-8 and disregard the errors. You can decode byte strings with &lt;code&gt;bytes.decode(errors='replace')&lt;/code&gt; to replace all decoding errors with a meaningless character, or set &lt;code&gt;decode_error='replace'&lt;/code&gt; in the vectorizer. This may damage the usefulness of your features.</source>
          <target state="translated">UTF-8을 시도하고 오류를 무시할 수 있습니다. 바이트 문자열을 &lt;code&gt;bytes.decode(errors='replace')&lt;/code&gt; 로 디코딩하여 모든 디코딩 오류를 의미없는 문자로 &lt;code&gt;decode_error='replace'&lt;/code&gt; 거나 벡터 화기에서 decode_error = 'replace' 를 설정할 수 있습니다. 기능의 유용성을 손상시킬 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c0d5a5afa92ed6aa301d13299623473f530c94ba" translate="yes" xml:space="preserve">
          <source>You may also load two (or more) datasets at once:</source>
          <target state="translated">한 번에 두 개 이상의 데이터 세트를로드 할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="a822ec525f0ce269b9b885feec474e1f8b512e04" translate="yes" xml:space="preserve">
          <source>You may also retain the estimator fitted on each training set by setting &lt;code&gt;return_estimator=True&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;return_estimator=True&lt;/code&gt; 를 설정하여 각 트레이닝 세트에 맞는 추정량을 유지할 수도 있습니다 .</target>
        </trans-unit>
        <trans-unit id="c9bcd37e9efb4d07a927274f7ad017afc3f094c8" translate="yes" xml:space="preserve">
          <source>You may be able to find out what kind of encoding it is in general using the UNIX command &lt;code&gt;file&lt;/code&gt;. The Python &lt;code&gt;chardet&lt;/code&gt; module comes with a script called &lt;code&gt;chardetect.py&lt;/code&gt; that will guess the specific encoding, though you cannot rely on its guess being correct.</source>
          <target state="translated">UNIX 명령 &lt;code&gt;file&lt;/code&gt; 사용하여 일반적으로 어떤 종류의 인코딩인지 확인할 수 있습니다 . 파이썬 &lt;code&gt;chardet&lt;/code&gt; 의 모듈라는 스크립트와 함께 제공 &lt;code&gt;chardetect.py&lt;/code&gt; 당신의 추측이 정확 인에 의존하지 수 있지만, 특정 인코딩을 추측됩니다.</target>
        </trans-unit>
        <trans-unit id="04405b99190799597dab92e2ef615219d1447404" translate="yes" xml:space="preserve">
          <source>You may load a dataset like as follows:</source>
          <target state="translated">다음과 같이 데이터 세트를로드 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f9f71500b978e09c529098f893f05269c79caaff" translate="yes" xml:space="preserve">
          <source>You may want to include the parameters of the preprocessors in a &lt;a href=&quot;grid_search#grid-search&quot;&gt;parameter search&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;grid_search#grid-search&quot;&gt;매개 변수 검색에&lt;/a&gt; 전 처리기의 매개 변수를 포함시킬 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="4cbe0908270a3a4effe7f03ed10c6fc1b573bdb1" translate="yes" xml:space="preserve">
          <source>You might get slightly different results with the solver liblinear than with the others since this uses LIBLINEAR which penalizes the intercept.</source>
          <target state="translated">솔버 liblinear를 사용하면 인터리브를 사용하는 경우 인터셉트에 페널티를주는 LIBLINEAR을 사용하므로 결과가 약간 다를 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="eacc5e93bbce61c3d762f60af9c0b85d6ab90006" translate="yes" xml:space="preserve">
          <source>You might have noticed that the samples were shuffled randomly when we called &lt;code&gt;fetch_20newsgroups(..., shuffle=True, random_state=42)&lt;/code&gt;: this is useful if you wish to select only a subset of samples to quickly train a model and get a first idea of the results before re-training on the complete dataset later.</source>
          <target state="translated">&lt;code&gt;fetch_20newsgroups(..., shuffle=True, random_state=42)&lt;/code&gt; 호출 할 때 샘플이 무작위로 섞여 있음을 알았을 것입니다 . 이는 모델을 빠르게 훈련시키고 첫 번째를 얻기 위해 샘플의 하위 집합 만 선택하려는 경우에 유용합니다 나중에 전체 데이터 세트를 다시 학습하기 전에 결과에 대한 아이디어.</target>
        </trans-unit>
        <trans-unit id="1c0c1bb33d891f47deca1c516d19aa08bd8443b9" translate="yes" xml:space="preserve">
          <source>You only have to call &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt; once on your data to fit a whole sequence of estimators.</source>
          <target state="translated">당신은 전화로이 &lt;code&gt;fit&lt;/code&gt; 하고 &lt;code&gt;predict&lt;/code&gt; 추정량의 전체 순서에 맞게 데이터를 한 번.</target>
        </trans-unit>
        <trans-unit id="8f00f0e599f4c3a7b71dcab47e41c43fc685f526" translate="yes" xml:space="preserve">
          <source>You should also make sure that the stop word list has had the same preprocessing and tokenization applied as the one used in the vectorizer. The word &lt;em&gt;we&amp;rsquo;ve&lt;/em&gt; is split into &lt;em&gt;we&lt;/em&gt; and &lt;em&gt;ve&lt;/em&gt; by CountVectorizer&amp;rsquo;s default tokenizer, so if &lt;em&gt;we&amp;rsquo;ve&lt;/em&gt; is in &lt;code&gt;stop_words&lt;/code&gt;, but &lt;em&gt;ve&lt;/em&gt; is not, &lt;em&gt;ve&lt;/em&gt; will be retained from &lt;em&gt;we&amp;rsquo;ve&lt;/em&gt; in transformed text. Our vectorizers will try to identify and warn about some kinds of inconsistencies.</source>
          <target state="translated">또한 중지 단어 목록에 벡터 라이저에서 사용 된 것과 동일한 사전 처리 및 토큰 화가 적용되어 있는지 확인해야합니다. &lt;em&gt;우리가 한&lt;/em&gt; 단어 &lt;em&gt;는 &lt;/em&gt;&lt;em&gt;우리&lt;/em&gt; 와 &lt;em&gt;ve&lt;/em&gt; 로 나뉩니다&lt;em&gt;&lt;/em&gt; 그렇다면, CountVectorizer의 기본 토크 나이로 &lt;em&gt;우리가했습니다&lt;/em&gt; 에 &lt;code&gt;stop_words&lt;/code&gt; 하지만 &lt;em&gt;적이&lt;/em&gt; 아니라, &lt;em&gt;적이&lt;/em&gt; 에서 유지됩니다 &lt;em&gt;우리가했습니다&lt;/em&gt; 변형 텍스트. 벡터 라이저는 일부 불일치에 대해 식별하고 경고합니다.</target>
        </trans-unit>
        <trans-unit id="03e4dbf3891b38cb2bcd04772f91e994b5e9c01b" translate="yes" xml:space="preserve">
          <source>Your dataset consists of heterogeneous data types (e.g. raster images and text captions)</source>
          <target state="translated">데이터 세트는 이기종 데이터 유형 (예 : 래스터 이미지 및 텍스트 캡션)으로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="7b0a68e70dc900bed821b4c342a07edfa667e451" translate="yes" xml:space="preserve">
          <source>Your dataset is stored in a Pandas DataFrame and different columns require different processing pipelines.</source>
          <target state="translated">데이터 세트는 Pandas DataFrame에 저장되며 열마다 다른 처리 파이프 라인이 필요합니다.</target>
        </trans-unit>
        <trans-unit id="cf246e4fd612425ede440737acf49a3220f42916" translate="yes" xml:space="preserve">
          <source>Your kernel must take as arguments two matrices of shape &lt;code&gt;(n_samples_1, n_features)&lt;/code&gt;, &lt;code&gt;(n_samples_2, n_features)&lt;/code&gt; and return a kernel matrix of shape &lt;code&gt;(n_samples_1, n_samples_2)&lt;/code&gt;.</source>
          <target state="translated">커널 인수 모양의 두 개의 행렬로 수행해야합니다 &lt;code&gt;(n_samples_1, n_features)&lt;/code&gt; , &lt;code&gt;(n_samples_2, n_features)&lt;/code&gt; 모양의 커널 매트릭스 반환 &lt;code&gt;(n_samples_1, n_samples_2)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a97cec9f16597107c699027a6e02cf1c0426b74a" translate="yes" xml:space="preserve">
          <source>ZN proportion of residential land zoned for lots over 25,000 sq.ft.</source>
          <target state="translated">주거 용지의 ZN 비율은 25,000 평방 피트가 넘는 부지에 구역화되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="712d097b167e76a6e9d59b3e5e274cb4dc4edfe4" translate="yes" xml:space="preserve">
          <source>Zadrozny and Elkan, &amp;ldquo;Transforming classifier scores into multiclass probability estimates&amp;rdquo;, SIGKDD&amp;lsquo;02, &lt;a href=&quot;http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf&quot;&gt;http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf&lt;/a&gt;</source>
          <target state="translated">Zadrozny와 Elkan,&amp;ldquo;분류 자 점수를 멀티 클래스 확률 추정값으로 변환&amp;rdquo;, SIGKDD'02, &lt;a href=&quot;http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf&quot;&gt;http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f05a65af97509516c00dcac126500e3f1415b5be" translate="yes" xml:space="preserve">
          <source>Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels.</source>
          <target state="translated">다항식과 S 자형 커널에 대한 영 계수. 다른 커널에서는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="e35caa5ca631cf4323249c1e10ca37b600a29376" translate="yes" xml:space="preserve">
          <source>Zero is the lowest possible score. Values closer to zero indicate a better partition.</source>
          <target state="translated">최저 점수는 0입니다. 0에 가까울수록 더 나은 파티션을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="4196df3003bc4705f3359c145eca39ac9042a13b" translate="yes" xml:space="preserve">
          <source>Zero-one classification loss.</source>
          <target state="translated">제로원 분류 손실.</target>
        </trans-unit>
        <trans-unit id="ee137211a128584365e4b492f8f1e31e317a831d" translate="yes" xml:space="preserve">
          <source>Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C. Local features and kernels for classification of texture and object categories: A comprehensive study International Journal of Computer Vision 2007 &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf&quot;&gt;http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf&lt;/a&gt;</source>
          <target state="translated">Zhang, J. 및 Marszalek, M. 및 Lazebnik, S. 및 Schmid, C. 텍스처 및 객체 범주의 분류를위한 로컬 기능 및 커널 : 종합 연구 International Computer of 2007 Vision &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf&quot;&gt;http://research.microsoft.com/ ko-kr / um / people / manik / projects / trade-off / papers / ZhangIJCV06.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2f2ef1b5180fd57b17245a5c505519733d35270d" translate="yes" xml:space="preserve">
          <source>Zhu, H. Zou, S. Rosset, T. Hastie, &amp;ldquo;Multi-class AdaBoost&amp;rdquo;, 2009.</source>
          <target state="translated">Zhu, H. Zou, S. Rosset, T. Hastie,&amp;ldquo;멀티 클래스 AdaBoost&amp;rdquo;, 2009.</target>
        </trans-unit>
        <trans-unit id="8ce45cc584babf565a133f667c041638840fdfd3" translate="yes" xml:space="preserve">
          <source>Zoubir A., Koivunen V., Chakhchoukh Y. and Muma M. (2012). Robust estimation in signal processing: A tutorial-style treatment of fundamental concepts. IEEE Signal Processing Magazine 29(4), 61-80.</source>
          <target state="translated">Zoubir A., ​​Koivunen V., Chakhchoukh Y. 및 Muma M. (2012). 신호 처리에서의 강력한 평가 : 기본 개념에 대한 튜토리얼 스타일 처리. IEEE 신호 처리 매거진 29 (4), 61-80.</target>
        </trans-unit>
        <trans-unit id="cd3417b4282b09dc45879fe7c77bee8859983780" translate="yes" xml:space="preserve">
          <source>[&amp;lsquo;rbf&amp;rsquo;, &amp;lsquo;sigmoid&amp;rsquo;, &amp;lsquo;polynomial&amp;rsquo;, &amp;lsquo;poly&amp;rsquo;, &amp;lsquo;linear&amp;rsquo;, &amp;lsquo;cosine&amp;rsquo;]</source>
          <target state="translated">[ 'rbf', 'sigmoid', 'polynomial', 'poly', 'linear', 'cosine']</target>
        </trans-unit>
        <trans-unit id="7c0453b88eaf6a5b1a0ac2faa1dec6c20e0dda6a" translate="yes" xml:space="preserve">
          <source>[1, x_2, x_2 ** 2, x_2 ** 3, &amp;hellip;], &amp;hellip;]</source>
          <target state="translated">[1, x_2, x_2 ** 2, x_2 ** 3,&amp;hellip;],&amp;hellip;]</target>
        </trans-unit>
        <trans-unit id="af237073ca841ce40d3b1c3f9ec3b84ba9e8c1ce" translate="yes" xml:space="preserve">
          <source>[1] &amp;ldquo;Online Learning for Latent Dirichlet Allocation&amp;rdquo;, Matthew D. Hoffman,</source>
          <target state="translated">[1] &quot;잠재적 인 Dirichlet 할당을위한 온라인 학습&quot;, Matthew D. Hoffman,</target>
        </trans-unit>
        <trans-unit id="a5828c16246e11e0eda2596d27fdd402ee57d009" translate="yes" xml:space="preserve">
          <source>[1] &amp;ldquo;Shrinkage Algorithms for MMSE Covariance Estimation&amp;rdquo; Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.</source>
          <target state="translated">[1] &quot;MMSE 공분산 추정을위한 수축 알고리즘&quot;Chen et al., IEEE Trans. 서명. Proc., Volume 58, Issue 10, 2010 년 10 월.</target>
        </trans-unit>
        <trans-unit id="c5ae55965c66d78c700f954c5d28c9832964e702" translate="yes" xml:space="preserve">
          <source>[1] &amp;ldquo;Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning&amp;rdquo; by A. Rahimi and Benjamin Recht. (&lt;a href=&quot;http://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf&quot;&gt;http://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf&lt;/a&gt;)</source>
          <target state="translated">[1] A. Rahimi와 Benjamin Recht의&amp;ldquo;무작위 주방 싱크의 가중치 합계 : 학습에서 최소화를 무작위로 대체&amp;rdquo;. ( &lt;a href=&quot;http://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf&quot;&gt;http://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="851ede0920efe80a8308115ddfdc22058d99b224" translate="yes" xml:space="preserve">
          <source>[1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for</source>
          <target state="translated">[1] Hinton, GE, Osindero, S. 및 Teh, Y. 빠른 학습 알고리즘</target>
        </trans-unit>
        <trans-unit id="c4ab6918e1971671fbb440a7f8e61bfcc4315791" translate="yes" xml:space="preserve">
          <source>[1] P. J. Rousseeuw. Least median of squares regression. J. Am</source>
          <target state="translated">[1] PJ Rousseeuw. 최소 제곱 회귀 분석 암</target>
        </trans-unit>
        <trans-unit id="4becf43125cdaf0ec29f63ac1f954b679ab8e6bc" translate="yes" xml:space="preserve">
          <source>[1] Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux. In Semi-Supervised Learning (2006), pp. 193-216</source>
          <target state="translated">[1] Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux. 반 감독 학습 (2006), pp. 193-216</target>
        </trans-unit>
        <trans-unit id="9a201577697a06c9ac689a946ae70d44d48c0e7c" translate="yes" xml:space="preserve">
          <source>[1] van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional Data</source>
          <target state="translated">[1] van der Maaten, LJP; Hinton, GE, 고차원 데이터 시각화</target>
        </trans-unit>
        <trans-unit id="8eeff125eef3cfca1ff3f8b3157054b95e0b3509" translate="yes" xml:space="preserve">
          <source>[2] &amp;ldquo;Stochastic Variational Inference&amp;rdquo;, Matthew D. Hoffman, David M. Blei,</source>
          <target state="translated">[2] &quot;확률 론적 변이 추론&quot;, Matthew D. Hoffman, David M. Blei,</target>
        </trans-unit>
        <trans-unit id="ea3c887d7b7624a41f686043b166f388d3617ff3" translate="yes" xml:space="preserve">
          <source>[2] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. Efficient Non-Parametric Function Induction in Semi-Supervised Learning. AISTAT 2005 &lt;a href=&quot;http://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf&quot;&gt;http://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf&lt;/a&gt;</source>
          <target state="translated">[2] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. Semi-Supervised Learning에서 효율적인 비모수 적 기능 유도. AISTAT 2005 &lt;a href=&quot;http://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf&quot;&gt;http://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="390f7912993134abee39b500fe8ff987c558bcc7" translate="yes" xml:space="preserve">
          <source>[2] Tieleman, T. Training Restricted Boltzmann Machines using</source>
          <target state="translated">[2] Tieleman, T. 교육 제한 볼츠만 머신</target>
        </trans-unit>
        <trans-unit id="936e8131576c3002ff436671f77d81f6c95e71d7" translate="yes" xml:space="preserve">
          <source>[2] Wilson, E. B., &amp;amp; Hilferty, M. M. (1931). The distribution of chi-square.</source>
          <target state="translated">[2] Wilson, EB, &amp;amp; Hilferty, MM (1931). 카이-제곱의 분포.</target>
        </trans-unit>
        <trans-unit id="5cccbf6c7fe7c1f50410b68e37c12f00b67f9330" translate="yes" xml:space="preserve">
          <source>[2] van der Maaten, L.J.P. t-Distributed Stochastic Neighbor Embedding</source>
          <target state="translated">[2] van der Maaten, LJP t- 분산 스토캐스틱 이웃 임베딩</target>
        </trans-unit>
        <trans-unit id="740947d1c8302c56dc8a9209234aab173f75acad" translate="yes" xml:space="preserve">
          <source>[3] L.J.P. van der Maaten. Accelerating t-SNE using Tree-Based Algorithms.</source>
          <target state="translated">LJP van der Maaten. 트리 기반 알고리즘을 사용하여 t-SNE 가속화</target>
        </trans-unit>
        <trans-unit id="a16dde9090b3c419b1ba6d8027b90785ddf73263" translate="yes" xml:space="preserve">
          <source>[3] Matthew D. Hoffman&amp;rsquo;s onlineldavb code. Link:</source>
          <target state="translated">[3] Matthew D. Hoffman의 onlineldavb 코드. 링크:</target>
        </trans-unit>
        <trans-unit id="2091fb37b7afd77ae2e8e60855d4cad2538ba378" translate="yes" xml:space="preserve">
          <source>[B1996]</source>
          <target state="translated">[B1996]</target>
        </trans-unit>
        <trans-unit id="66fa89cedf249bba6f8bbd7ca59a6edba1eb2520" translate="yes" xml:space="preserve">
          <source>[B1998]</source>
          <target state="translated">[B1998]</target>
        </trans-unit>
        <trans-unit id="7665023d9511c3ea5a7a3e7baca06057eba900d6" translate="yes" xml:space="preserve">
          <source>[B1999]</source>
          <target state="translated">[B1999]</target>
        </trans-unit>
        <trans-unit id="5c075f95c1e65a7e49dec5ad30ee36d1fb13b2b6" translate="yes" xml:space="preserve">
          <source>[B2001]</source>
          <target state="translated">[B2001]</target>
        </trans-unit>
        <trans-unit id="04bec92cc809290da2bf608da574e1877293633f" translate="yes" xml:space="preserve">
          <source>[B2011]</source>
          <target state="translated">[B2011]</target>
        </trans-unit>
        <trans-unit id="2b6c9f7f2623b948c281da789073abc37bb7f8fa" translate="yes" xml:space="preserve">
          <source>[ButlerDavies]</source>
          <target state="translated">[ButlerDavies]</target>
        </trans-unit>
        <trans-unit id="1d442f2d1661e89f1bc869a582a8d649d24881e4" translate="yes" xml:space="preserve">
          <source>[D1997]</source>
          <target state="translated">[D1997]</target>
        </trans-unit>
        <trans-unit id="20e70caf2f764d35f0c5f51eb6c2cc52a6f947f2" translate="yes" xml:space="preserve">
          <source>[Davis2006]</source>
          <target state="translated">[Davis2006]</target>
        </trans-unit>
        <trans-unit id="3b0f17e8250c1e7b54512123b77c31bb0899ae7a" translate="yes" xml:space="preserve">
          <source>[Everingham2010]</source>
          <target state="translated">[Everingham2010]</target>
        </trans-unit>
        <trans-unit id="e5ff04dac92d8d5710e2d4ade54a4899d9a01662" translate="yes" xml:space="preserve">
          <source>[F1999]</source>
          <target state="translated">[F1999]</target>
        </trans-unit>
        <trans-unit id="ddfaba8b68f822d387eefcc49dbcf89bee83fcbe" translate="yes" xml:space="preserve">
          <source>[F2001]</source>
          <target state="translated">[F2001]</target>
        </trans-unit>
        <trans-unit id="5712ff07224dea2f09976ad1b5f9060cea098ee8" translate="yes" xml:space="preserve">
          <source>[FS1995]</source>
          <target state="translated">[FS1995]</target>
        </trans-unit>
        <trans-unit id="111b120f6e2f3a7d9723c16133fcfb1ce7b7557b" translate="yes" xml:space="preserve">
          <source>[Flach2015]</source>
          <target state="translated">[Flach2015]</target>
        </trans-unit>
        <trans-unit id="0be1b91bf292e6f295e73f8ba1626aa315ca1709" translate="yes" xml:space="preserve">
          <source>[Guyon2015]</source>
          <target state="translated">[Guyon2015]</target>
        </trans-unit>
        <trans-unit id="16deff704a4f867ca18d98b22e63afcb70ec96d2" translate="yes" xml:space="preserve">
          <source>[H1998]</source>
          <target state="translated">[H1998]</target>
        </trans-unit>
        <trans-unit id="346ddc10d1a23f1ff0ba05ea1da881f4666595c5" translate="yes" xml:space="preserve">
          <source>[HTF2009]</source>
          <target state="translated">[HTF2009]</target>
        </trans-unit>
        <trans-unit id="2b6bce181ae06e6796d39628b4dc12ca65767e20" translate="yes" xml:space="preserve">
          <source>[HTF]</source>
          <target state="translated">[HTF]</target>
        </trans-unit>
        <trans-unit id="8f4756ba18c793a637ad7568fd4450479f47b718" translate="yes" xml:space="preserve">
          <source>[Hubert1985]</source>
          <target state="translated">[Hubert1985]</target>
        </trans-unit>
        <trans-unit id="1c2acae56920363d695dc395b30aa0dac0656aa7" translate="yes" xml:space="preserve">
          <source>[Jen09]</source>
          <target state="translated">[Jen09]</target>
        </trans-unit>
        <trans-unit id="52833f723be6af6645b6622da4d471b65e89d942" translate="yes" xml:space="preserve">
          <source>[Kelleher2015]</source>
          <target state="translated">[Kelleher2015]</target>
        </trans-unit>
        <trans-unit id="8d63f432cd9715591fb04662784fbed01426124f" translate="yes" xml:space="preserve">
          <source>[L2014]</source>
          <target state="translated">[L2014]</target>
        </trans-unit>
        <trans-unit id="e6f810474b9d9bf1966e66d024bfd2d0d9af7983" translate="yes" xml:space="preserve">
          <source>[LG2012]</source>
          <target state="translated">[LG2012]</target>
        </trans-unit>
        <trans-unit id="4108a351bec333bc41371d8be81bbf1f0bd216a0" translate="yes" xml:space="preserve">
          <source>[LS2010]</source>
          <target state="translated">[LS2010]</target>
        </trans-unit>
        <trans-unit id="36c1f9470816b8da81a8ed80471ace8df2456c31" translate="yes" xml:space="preserve">
          <source>[M2012]</source>
          <target state="translated">[M2012]</target>
        </trans-unit>
        <trans-unit id="536dc6f43e65eedbc7dcd92d64ef850b0a7951d3" translate="yes" xml:space="preserve">
          <source>[MRS2008]</source>
          <target state="translated">[MRS2008]</target>
        </trans-unit>
        <trans-unit id="350f14f810397ce0cd7520f35f0fae7d54d72023" translate="yes" xml:space="preserve">
          <source>[Manning2008]</source>
          <target state="translated">[Manning2008]</target>
        </trans-unit>
        <trans-unit id="0cc214a1564fbbf90b4daa0b0972b6f8408e3afc" translate="yes" xml:space="preserve">
          <source>[Mosley2013]</source>
          <target state="translated">[Mosley2013]</target>
        </trans-unit>
        <trans-unit id="73be0b37b87d3c88f49438b3a7ca251dc3d3c1d2" translate="yes" xml:space="preserve">
          <source>[Mrl09]</source>
          <target state="translated">[Mrl09]</target>
        </trans-unit>
        <trans-unit id="690e639d5d468ec30ab9e54cb03287a1d07d286f" translate="yes" xml:space="preserve">
          <source>[NQY18]</source>
          <target state="translated">[NQY18]</target>
        </trans-unit>
        <trans-unit id="95849b59dfe0de62fa4f930bc19ca3b64ad51c5b" translate="yes" xml:space="preserve">
          <source>[R2007]</source>
          <target state="translated">[R2007]</target>
        </trans-unit>
        <trans-unit id="d27f89b25dd2806ef3b69d37ac341ea761b1f775" translate="yes" xml:space="preserve">
          <source>[RR2007]</source>
          <target state="translated">[RR2007]</target>
        </trans-unit>
        <trans-unit id="99aae3a9e5135b3c3c9e6f81c5583f53ea54b161" translate="yes" xml:space="preserve">
          <source>[RVD]</source>
          <target state="translated">[RVD]</target>
        </trans-unit>
        <trans-unit id="7fc4fd0834c6c78f63966f47416f1e672a05b032" translate="yes" xml:space="preserve">
          <source>[RVDriessen]</source>
          <target state="translated">[RVDriessen]</target>
        </trans-unit>
        <trans-unit id="9a3d290ec7e0cf466e2e530b732c430fd93742e5" translate="yes" xml:space="preserve">
          <source>[RW2006]</source>
          <target state="translated">[RW2006]</target>
        </trans-unit>
        <trans-unit id="ab40883d6ce3b576febad86ad20a220ae60fc722" translate="yes" xml:space="preserve">
          <source>[Rouseeuw1984]</source>
          <target state="translated">[Rouseeuw1984]</target>
        </trans-unit>
        <trans-unit id="f4ff5aad65e1461e94ef70a659337011d0c58126" translate="yes" xml:space="preserve">
          <source>[Rousseeuw]</source>
          <target state="translated">[Rousseeuw]</target>
        </trans-unit>
        <trans-unit id="74f2d2c5b044f5dc96afc1e0482d2495c57d5370" translate="yes" xml:space="preserve">
          <source>[Urbanowicz2015]</source>
          <target state="translated">[Urbanowicz2015]</target>
        </trans-unit>
        <trans-unit id="34cb3f1593778c84c25ed6665ee9a323140a68d9" translate="yes" xml:space="preserve">
          <source>[VEB2009] Vinh, Epps, and Bailey, (2009). &amp;ldquo;Information theoretic measures for clusterings comparison&amp;rdquo;. Proceedings of the 26th Annual International Conference on Machine Learning - ICML &amp;lsquo;09. &lt;a href=&quot;https://dl.acm.org/citation.cfm?doid=1553374.1553511&quot;&gt;doi:10.1145/1553374.1553511&lt;/a&gt;. ISBN 9781605585161.</source>
          <target state="translated">[VEB2009] Vinh, Epps 및 Bailey (2009). &amp;ldquo;클러스터링 비교를위한 정보 이론적 조치&amp;rdquo;. 기계 학습에 관한 제 26 회 연례 국제 회의 절차-ICML '09. &lt;a href=&quot;https://dl.acm.org/citation.cfm?doid=1553374.1553511&quot;&gt;doi : 10.1145 / 1553374.1553511&lt;/a&gt; . ISBN 9781605585161.</target>
        </trans-unit>
        <trans-unit id="48a5d15f42b24744d500812bf01a83ad55ecae83" translate="yes" xml:space="preserve">
          <source>[VEB2010] Vinh, Epps, and Bailey, (2010). &amp;ldquo;Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance&amp;rdquo;. JMLR &amp;lt;&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&lt;/a&gt;&amp;gt;</source>
          <target state="translated">[VEB2010] Vinh, Epps 및 Bailey (2010). &amp;ldquo;클러스터링 비교를위한 정보 이론적 측정 : 변형, 속성, 정규화 및 수정에 대한 수정&amp;rdquo;. JMLR &amp;lt; &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&lt;/a&gt; &amp;gt;</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
