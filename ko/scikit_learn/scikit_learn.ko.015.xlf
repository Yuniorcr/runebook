<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="65eaa1a409cbf0736a7b1da17a35a153fc9af91f" translate="yes" xml:space="preserve">
          <source>Test samples</source>
          <target state="translated">테스트 샘플</target>
        </trans-unit>
        <trans-unit id="29446ed524d3e237184352cbcf6e1c5aaeb464e4" translate="yes" xml:space="preserve">
          <source>Test samples with shape = (n_samples, n_features) or None. For some estimators this may be a precomputed kernel matrix instead, shape = (n_samples, n_samples_fitted], where n_samples_fitted is the number of samples used in the fitting for the estimator. Passing None as test samples gives the same result as passing real test samples, since DummyRegressor operates independently of the sampled observations.</source>
          <target state="translated">shape = (n_samples, n_features) 또는 None으로 샘플을 테스트합니다. 일부 추정기의 경우, 이는 사전 계산 된 커널 매트릭스 일 수 있습니다. 대신 shape = (n_samples, n_samples_fitted], 여기서 n_samples_fitted는 추정기 피팅에 사용 된 샘플 수입니다. DummyRegressor는 샘플링 된 관찰과 독립적으로 작동하기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="12cad09d9d4837878fb37fd506e5f7fb71a801c9" translate="yes" xml:space="preserve">
          <source>Test samples with shape = (n_samples, n_features) or None. Passing None as test samples gives the same result as passing real test samples, since DummyClassifier operates independently of the sampled observations.</source>
          <target state="translated">shape = (n_samples, n_features) 또는 None으로 샘플을 테스트합니다. DummyClassifier가 샘플링 된 관측치와 독립적으로 작동하므로 테스트 샘플로 없음을 전달하면 실제 테스트 샘플을 전달하는 것과 동일한 결과가 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="0c1d5bbb82f5cfc66b7e35e84179b02f4b13f8b1" translate="yes" xml:space="preserve">
          <source>Test samples.</source>
          <target state="translated">테스트 샘플.</target>
        </trans-unit>
        <trans-unit id="bdec5057d32ebe97bd4c525fff2435009f4be0a0" translate="yes" xml:space="preserve">
          <source>Test samples. For some estimators this may be a precomputed kernel matrix instead, shape = (n_samples, n_samples_fitted], where n_samples_fitted is the number of samples used in the fitting for the estimator.</source>
          <target state="translated">테스트 샘플. 일부 추정기의 경우, 이는 사전 계산 된 커널 매트릭스 일 수 있습니다. 대신 shape = (n_samples, n_samples_fitted]입니다. 여기서 n_samples_fitted는 추정기 피팅에 사용 된 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="7f9baccc70399290d29568f9812594e6335c4ae0" translate="yes" xml:space="preserve">
          <source>Test with permutations the significance of a classification score</source>
          <target state="translated">분류 점수의 중요성을 순열로 테스트</target>
        </trans-unit>
        <trans-unit id="c67f73aee0c3dc1034cba236cfe8c8526d7a9123" translate="yes" xml:space="preserve">
          <source>Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.</source>
          <target state="translated">텍스트 분석은 기계 학습 알고리즘의 주요 응용 분야입니다. 그러나 원시 데이터, 일련의 기호는 가변 길이의 원시 텍스트 문서가 아닌 고정 크기의 숫자 피처 벡터를 기대하기 때문에 알고리즘 자체에 직접 공급 될 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="990226708ed5e3f7319c13e5c3e22d22fd438346" translate="yes" xml:space="preserve">
          <source>Text is made of characters, but files are made of bytes. These bytes represent characters according to some &lt;em&gt;encoding&lt;/em&gt;. To work with text files in Python, their bytes must be &lt;em&gt;decoded&lt;/em&gt; to a character set called Unicode. Common encodings are ASCII, Latin-1 (Western Europe), KOI8-R (Russian) and the universal encodings UTF-8 and UTF-16. Many others exist.</source>
          <target state="translated">텍스트는 문자로 구성되지만 파일은 바이트로 구성됩니다. 이 바이트는 일부 &lt;em&gt;인코딩&lt;/em&gt; 에 따라 문자를 나타냅니다 . Python에서 텍스트 파일로 작업하려면 바이트를 유니 코드라고하는 문자 세트 로 &lt;em&gt;디코딩&lt;/em&gt; 해야합니다 . 일반적인 인코딩은 ASCII, Latin-1 (서유럽), KOI8-R (러시아어) 및 범용 인코딩 UTF-8 및 UTF-16입니다. 다른 많은 존재합니다.</target>
        </trans-unit>
        <trans-unit id="2a2f9f7e298485c4a85bf6b791046a1b63087cf9" translate="yes" xml:space="preserve">
          <source>Text preprocessing, tokenizing and filtering of stopwords are all included in &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;, which builds a dictionary of features and transforms documents to feature vectors:</source>
          <target state="translated">텍스트 사전 처리, 토큰 화 및 중지 어 필터링은 모두 &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; 에&lt;/a&gt; 포함되어 있습니다. 이 기능은 피처 사전을 작성하고 문서를 피처 벡터로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="79142cb36f8945e4d341c82cf5dc060dc02c8f0f" translate="yes" xml:space="preserve">
          <source>Text summary of the precision, recall, F1 score for each class. Dictionary returned if output_dict is True. Dictionary has the following structure:</source>
          <target state="translated">각 클래스의 정밀도, 리콜, F1 점수에 대한 텍스트 요약. output_dict가 True 인 경우 사전이 반환되었습니다. 사전의 구조는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="f042ff208f7aff2fdebc20ebdcfe3611681222ed" translate="yes" xml:space="preserve">
          <source>Tf is &amp;ldquo;n&amp;rdquo; (natural) by default, &amp;ldquo;l&amp;rdquo; (logarithmic) when &lt;code&gt;sublinear_tf=True&lt;/code&gt;. Idf is &amp;ldquo;t&amp;rdquo; when use_idf is given, &amp;ldquo;n&amp;rdquo; (none) otherwise. Normalization is &amp;ldquo;c&amp;rdquo; (cosine) when &lt;code&gt;norm='l2'&lt;/code&gt;, &amp;ldquo;n&amp;rdquo; (none) when &lt;code&gt;norm=None&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;sublinear_tf=True&lt;/code&gt; 인 경우 Tf는 기본적으로 &quot;n&quot;(자연), &quot;l&quot;(대수) 입니다. use_idf가 제공되면 Idf는 &quot;t&quot;이고, 그렇지 않으면 &quot;n&quot;(없음)입니다. &lt;code&gt;norm='l2'&lt;/code&gt; 인 경우 정규화는 &quot;c&quot;(코사인) 이고 &lt;code&gt;norm=None&lt;/code&gt; 인 경우 &quot;n&quot;(없음) 입니다.</target>
        </trans-unit>
        <trans-unit id="97730bbab5383bbe19dd65de91be719c29295304" translate="yes" xml:space="preserve">
          <source>Tf means &lt;strong&gt;term-frequency&lt;/strong&gt; while tf&amp;ndash;idf means term-frequency times &lt;strong&gt;inverse document-frequency&lt;/strong&gt;: \(\text{tf-idf(t,d)}=\text{tf(t,d)} \times \text{idf(t)}\).</source>
          <target state="translated">TF 수단 &lt;strong&gt;용어 주파수&lt;/strong&gt; 동안 TF-IDF 수단 용어 주파수 회 &lt;strong&gt;문서 주파수 역&lt;/strong&gt; : \ (\ 텍스트 {TF-IDF (t, d)} = \ 텍스트 {TF (t, d)} \ 시간 \ 텍스트 {IDF (티)}\).</target>
        </trans-unit>
        <trans-unit id="f1cc0d39fa695e88ce231644990ae2b503602ddb" translate="yes" xml:space="preserve">
          <source>Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.</source>
          <target state="translated">Tf는 항-주파수를 의미하고 tf-idf는 항-주파수 시간과 문서 주파수의 역수를 의미합니다. 이것은 정보 검색에서 일반적으로 사용되는 가중치 체계이며 문서 분류에도 유용합니다.</target>
        </trans-unit>
        <trans-unit id="771178a448f62a1d367a9045d97d08b26eb6869c" translate="yes" xml:space="preserve">
          <source>Tf-idf-weighted document-term matrix.</source>
          <target state="translated">tf-idf 가중 문서 용어 행렬.</target>
        </trans-unit>
        <trans-unit id="daaa1c74bc4f107e3ab2cba2520cc29b4809af00" translate="yes" xml:space="preserve">
          <source>TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most frequent words to features indices and hence compute a word occurrence frequency (sparse) matrix. The word frequencies are then reweighted using the Inverse Document Frequency (IDF) vector collected feature-wise over the corpus.</source>
          <target state="translated">TfidfVectorizer는 메모리 내 어휘 (python dict)를 사용하여 가장 빈번한 단어를 피처 인덱스에 매핑하고 단어 발생 빈도 (스파 스) 행렬을 계산합니다. 그런 다음 단어 빈도는 말뭉치에 걸쳐 기능별로 수집 된 IDF (Inverse Document Frequency) 벡터를 사용하여 다시 가중치가 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="2c1e749a66bcce49e10daf1b4a981af3bebb9284" translate="yes" xml:space="preserve">
          <source>That this function takes time at least quadratic in n_samples. For large datasets, it&amp;rsquo;s wise to set that parameter to a small value.</source>
          <target state="translated">이 함수는 n_samples에서 적어도 2 차 시간이 걸립니다. 큰 데이터 집합의 경우 해당 매개 변수를 작은 값으로 설정하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="fb20b5f965f5eb1a2ab7f1c1219e1e8adbfdfc00" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced&amp;rdquo; heuristic is inspired by Logistic Regression in Rare Events Data, King, Zen, 2001.</source>
          <target state="translated">&quot;균형&quot;휴리스틱은 2001 년 King, Zen, Rare Events Data의 Logistic Regression에서 영감을 얻었습니다.</target>
        </trans-unit>
        <trans-unit id="f4d692dace6b9f963c8a80ff6fb54e77b0936bf5" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced&amp;rdquo; mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;</source>
          <target state="translated">&quot;balanced&quot;모드는 y 값을 사용하여 입력 데이터의 클래스 주파수에 반비례하는 가중치를 &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; 로 자동 조정합니다.</target>
        </trans-unit>
        <trans-unit id="ef34b0ee7fbdfc2770447dcdf0759da38193f229" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced&amp;rdquo; mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;.</source>
          <target state="translated">&quot;balanced&quot;모드는 y 값을 사용하여 입력 데이터의 클래스 주파수에 반비례하는 가중치를 &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; 로 자동 조정 합니다.</target>
        </trans-unit>
        <trans-unit id="66610aa2288acbb0ecf69897c27cb9799d361b2f" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced&amp;rdquo; mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data: &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;.</source>
          <target state="translated">&quot;balanced&quot;모드는 y 값을 사용하여 입력 데이터의 클래스 주파수에 반비례하는 가중치를 자동으로 조정합니다 : &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ef1bbf84c17d648e39cb34085672c6faaeb47082" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced_subsample&amp;rdquo; mode is the same as &amp;ldquo;balanced&amp;rdquo; except that weights are computed based on the bootstrap sample for every tree grown.</source>
          <target state="translated">&amp;ldquo;balanced_subsample&amp;rdquo;모드는 가중 된 모든 트리에 대한 부트 스트랩 샘플을 기반으로 가중치가 계산된다는 점을 제외하면&amp;ldquo;balanced_subsample&amp;rdquo;모드는&amp;ldquo;balanced&amp;rdquo;와 동일합니다.</target>
        </trans-unit>
        <trans-unit id="9570d1ba54b75e486c6a8fe70ab0af402d1567cc" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;lbfgs&amp;rdquo;, &amp;ldquo;sag&amp;rdquo; and &amp;ldquo;newton-cg&amp;rdquo; solvers only support L2 penalization and are found to converge faster for some high dimensional data. Setting &lt;code&gt;multi_class&lt;/code&gt; to &amp;ldquo;multinomial&amp;rdquo; with these solvers learns a true multinomial logistic regression model &lt;a href=&quot;#id26&quot; id=&quot;id23&quot;&gt;[5]&lt;/a&gt;, which means that its probability estimates should be better calibrated than the default &amp;ldquo;one-vs-rest&amp;rdquo; setting.</source>
          <target state="translated">&quot;lbfgs&quot;, &quot;sag&quot;및 &quot;newton-cg&quot;솔버는 L2 불이익 만 지원하며 일부 고차원 데이터의 경우 더 빨리 수렴됩니다. 이러한 솔버를 사용하여 &lt;code&gt;multi_class&lt;/code&gt; 를 &quot;다항식&quot;으로 설정 하면 진정한 다항 로지스틱 회귀 모델 &lt;a href=&quot;#id26&quot; id=&quot;id23&quot;&gt;[5]을&lt;/a&gt; 알게 되므로 확률 추정값이 기본 &quot;1 대 1 대&quot;설정보다 더 잘 교정되어야합니다.</target>
        </trans-unit>
        <trans-unit id="c76cf618fdea0e603c8990092e5d960628df7c0c" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;new&amp;rdquo; data consists of linear combinations of the input data, with weights probabilistically drawn given the KDE model.</source>
          <target state="translated">&quot;새로운&quot;데이터는 입력 데이터의 선형 조합으로 구성되며 KDE 모델에 따라 가중 적으로 가중치가 그려집니다.</target>
        </trans-unit>
        <trans-unit id="420229f24d72cfc948f72b9aaf53e46dfcb25b62" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;sag&amp;rdquo; solver uses a Stochastic Average Gradient descent &lt;a href=&quot;#id27&quot; id=&quot;id24&quot;&gt;[6]&lt;/a&gt;. It is faster than other solvers for large datasets, when both the number of samples and the number of features are large.</source>
          <target state="translated">&quot;새그&quot;솔버는 확률 평균 그라디언트 디센트 (Stochastic Average Gradient descent)를 사용합니다 &lt;a href=&quot;#id27&quot; id=&quot;id24&quot;&gt;[6]&lt;/a&gt; . 샘플 수와 피처 수가 모두 많은 경우 대규모 데이터 세트의 경우 다른 솔버보다 빠릅니다.</target>
        </trans-unit>
        <trans-unit id="cf55bf4220bbf6e5ad8c38b76c827b53a1e3f193" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;saga&amp;rdquo; solver &lt;a href=&quot;#id28&quot; id=&quot;id25&quot;&gt;[7]&lt;/a&gt; is a variant of &amp;ldquo;sag&amp;rdquo; that also supports the non-smooth &lt;code&gt;penalty=&amp;rdquo;l1&amp;rdquo;&lt;/code&gt; option. This is therefore the solver of choice for sparse multinomial logistic regression.</source>
          <target state="translated">&quot;사가&quot;솔버 &lt;a href=&quot;#id28&quot; id=&quot;id25&quot;&gt;[7]&lt;/a&gt; 는 &quot;sag&quot;의 변형으로 부드러운 &lt;code&gt;penalty=&amp;rdquo;l1&amp;rdquo;&lt;/code&gt; 옵션 도 지원합니다 . 따라서 이것은 희소 다항 로지스틱 회귀 분석을 위해 선택하는 솔버입니다.</target>
        </trans-unit>
        <trans-unit id="77bf2f7306c562160b3a78c9199a57470ad6395e" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;saga&amp;rdquo; solver is often the best choice. The &amp;ldquo;liblinear&amp;rdquo; solver is used by default for historical reasons.</source>
          <target state="translated">&quot;사가&quot;솔버가 종종 최선의 선택입니다. 역사적인 이유로 &quot;liblinear&quot;솔버가 기본적으로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="068bc43bd479e1422a1e2139866c2ca587dbb3ad" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;steepness&amp;rdquo; of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.</source>
          <target state="translated">ROC 곡선의 &quot;스티프니스&quot;도 중요합니다. 위양성 비율을 최소화하면서 실제 양의 비율을 최대화하는 것이 이상적입니다.</target>
        </trans-unit>
        <trans-unit id="0eb5d5532023d8acbeeebff557bc347056c3c6a7" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;target&amp;rdquo; for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised perspective.</source>
          <target state="translated">이 데이터베이스의 &quot;대상&quot;은 사진을 찍는 사람의 신원을 나타내는 0에서 39 사이의 정수입니다. 그러나 클래스 당 10 개의 예제 만있는이 상대적으로 작은 데이터 세트는 감독되지 않은 또는 반 감독 된 관점에서 더 흥미 롭습니다.</target>
        </trans-unit>
        <trans-unit id="6893a2ecba3f5b3ceba43b94c7037a23940a0678" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;auto&amp;rsquo; mode is the default and is intended to pick the cheaper option of the two depending upon the shape and format of the training data.</source>
          <target state="translated">'자동'모드는 기본값이며 훈련 데이터의 모양과 형식에 따라 두 가지 중 더 저렴한 옵션을 선택하기위한 것입니다.</target>
        </trans-unit>
        <trans-unit id="0f80449b3a36a9645d51b541d6ac4415080a7df2" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;cd&amp;rsquo; solver can only optimize the Frobenius norm. Due to the underlying non-convexity of NMF, the different solvers may converge to different minima, even when optimizing the same distance function.</source>
          <target state="translated">'cd'솔버는 Frobenius 표준 만 최적화 할 수 있습니다. NMF의 근본적인 비 볼록성으로 인해 동일한 거리 함수를 최적화 할 때에도 서로 다른 솔버가 서로 다른 최소 점으로 수렴 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="370b11b6ae177f24cc2d42a049dda5c0d7e30775" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;eigen&amp;rsquo; solver is based on the optimization of the between class scatter to within class scatter ratio. It can be used for both classification and transform, and it supports shrinkage. However, the &amp;lsquo;eigen&amp;rsquo; solver needs to compute the covariance matrix, so it might not be suitable for situations with a high number of features.</source>
          <target state="translated">'고유'솔버는 클래스 스 캐터 비율 사이의 클래스 스 캐터 간 최적화를 기반으로합니다. 분류 및 변환에 모두 사용할 수 있으며 수축을 지원합니다. 그러나 '고 유량'솔버는 공분산 행렬을 계산해야하므로 많은 피쳐가있는 상황에는 적합하지 않을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ccda076fda793672987d7568e3ca12c3047fb684" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;log&amp;rsquo; loss gives logistic regression, a probabilistic classifier. &amp;lsquo;modified_huber&amp;rsquo; is another smooth loss that brings tolerance to outliers as well as probability estimates. &amp;lsquo;squared_hinge&amp;rsquo; is like hinge but is quadratically penalized. &amp;lsquo;perceptron&amp;rsquo; is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see SGDRegressor for a description.</source>
          <target state="translated">'로그'손실은 확률 적 분류 기인 로지스틱 회귀를 제공합니다. 'modified_huber'는 확률 추정치뿐만 아니라 특이 치에 대한 내성을 제공하는 또 다른 부드러운 손실입니다. 'squared_hinge'는 힌지와 비슷하지만 2 차적으로 불이익을받습니다. 'perceptron'은 perceptron 알고리즘에서 사용되는 선형 손실입니다. 다른 손실은 회귀를 위해 설계되었지만 분류에도 유용 할 수 있습니다. 설명은 SGDRegressor를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="5302138e8a256151a982f3c737747f8db1fec2f7" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;lsqr&amp;rsquo; solver is an efficient algorithm that only works for classification. It supports shrinkage.</source>
          <target state="translated">'lsqr'솔버는 분류에만 작동하는 효율적인 알고리즘입니다. 수축을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="e2a91334301a1b93c477cd479a707ce044fefdf3" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo;, and &amp;lsquo;lbfgs&amp;rsquo; solvers support only L2 regularization with primal formulation. The &amp;lsquo;liblinear&amp;rsquo; solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.</source>
          <target state="translated">'newton-cg', 'sag'및 'lbfgs'솔버는 기본 공식을 사용한 L2 정규화 만 지원합니다. 'liblinear'솔버는 L2 페널티에 대해서만 이중 공식을 사용하여 L1 및 L2 정규화를 모두 지원합니다.</target>
        </trans-unit>
        <trans-unit id="ade2e6c6872bcfb8e63408411f739881d7395764" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;squared_loss&amp;rsquo; refers to the ordinary least squares fit. &amp;lsquo;huber&amp;rsquo; modifies &amp;lsquo;squared_loss&amp;rsquo; to focus less on getting outliers correct by switching from squared to linear loss past a distance of epsilon. &amp;lsquo;epsilon_insensitive&amp;rsquo; ignores errors less than epsilon and is linear past that; this is the loss function used in SVR. &amp;lsquo;squared_epsilon_insensitive&amp;rsquo; is the same but becomes squared loss past a tolerance of epsilon.</source>
          <target state="translated">'squared_loss'는 일반적인 최소 제곱 적합을 나타냅니다. 'huber'는 엡실론 거리를지나 제곱에서 선형 손실로 전환하여 특이 치를 올바르게 얻는 데 덜 집중하도록 'squared_loss'를 수정합니다. 'epsilon_insensitive'는 epsilon보다 작은 오류를 무시하고 그 직전의 선형입니다. SVR에서 사용되는 손실 기능입니다. 'squared_epsilon_insensitive'는 동일하지만 엡실론 공차를지나 제곱 손실이됩니다.</target>
        </trans-unit>
        <trans-unit id="388443bd992c152f7c80a788085a15982e280e0a" translate="yes" xml:space="preserve">
          <source>The (scaled) interquartile range for each feature in the training set.</source>
          <target state="translated">훈련 세트의 각 기능에 대한 (확장 된) 사 분위수 범위.</target>
        </trans-unit>
        <trans-unit id="b3533a4edec1fdb05f12a2a421dc320606ca77c6" translate="yes" xml:space="preserve">
          <source>The (sometimes surprising) observation is that this is &lt;em&gt;still a linear model&lt;/em&gt;: to see this, imagine creating a new variable</source>
          <target state="translated">(때로는 놀라운) 관찰은 이것이 &lt;em&gt;여전히 선형 모델이라는 것입니다&lt;/em&gt; . 이것을 보려면 새 변수를 만드는 것을 상상해보십시오.</target>
        </trans-unit>
        <trans-unit id="ae37fbc1863417aba870f086fd7dcb7d12932667" translate="yes" xml:space="preserve">
          <source>The (x,y) position of the lower-left corner, in degrees</source>
          <target state="translated">왼쪽 아래 모서리의 (x, y) 위치 (도)</target>
        </trans-unit>
        <trans-unit id="bcd6ca42c3472afbe27069a62710b5c531496d9b" translate="yes" xml:space="preserve">
          <source>The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper &amp;ldquo;Newsweeder: Learning to filter netnews,&amp;rdquo; though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.</source>
          <target state="translated">20 개의 뉴스 그룹 데이터 세트는 약 20,000 개의 뉴스 그룹 문서 모음으로, 20 개의 다른 뉴스 그룹에서 (거의) 균등하게 분할됩니다. 우리가 아는 한, Ken Lang은 원래 그의 논문 &quot;Newsweeder : Learn to filter netnews&quot;를 위해 수집 한 것이지만,이 컬렉션을 명시 적으로 언급하지는 않습니다. 20 개의 뉴스 그룹 모음은 텍스트 분류 및 텍스트 클러스터링과 같은 기계 학습 기술의 텍스트 응용 프로그램 실험에 널리 사용되는 데이터 세트가되었습니다.</target>
        </trans-unit>
        <trans-unit id="4b2a042059fffe007deb9ebabf02d8062c1e6bda" translate="yes" xml:space="preserve">
          <source>The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.</source>
          <target state="translated">20 개의 뉴스 그룹 데이터 세트는 교육 (또는 개발)과 테스트 (또는 성능 평가)를위한 두 개의 하위 세트로 분할 된 20 개의 주제에 대한 약 18000 개의 뉴스 그룹 게시물로 구성됩니다. 열차와 테스트 세트의 분할은 특정 날짜 전후에 게시 된 메시지를 기반으로합니다.</target>
        </trans-unit>
        <trans-unit id="c380ecdb017c04631da3ca1753b6ddf07ce8267f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.cluster&quot;&gt;&lt;code&gt;sklearn.cluster&lt;/code&gt;&lt;/a&gt; module gathers popular unsupervised clustering algorithms.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.cluster&quot;&gt; &lt;code&gt;sklearn.cluster&lt;/code&gt; 의&lt;/a&gt; 모듈은 인기 자율 클러스터링 알고리즘을 수집합니다.</target>
        </trans-unit>
        <trans-unit id="6a798b177e574d6ff4ac12be7b93e3b8f8d74b71" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; module includes methods and algorithms to robustly estimate the covariance of features given a set of points. The precision matrix defined as the inverse of the covariance is also estimated. Covariance estimation is closely related to the theory of Gaussian Graphical Models.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.covariance&quot;&gt; &lt;code&gt;sklearn.covariance&lt;/code&gt; 의&lt;/a&gt; 모듈 견고 점들의 세트를 주어진 기능의 공분산을 추정하는 방법 및 알고리즘을 포함한다. 공분산의 역으로 ​​정의 된 정밀 행렬도 추정됩니다. 공분산 추정은 가우스 그래픽 모델 이론과 밀접한 관련이 있습니다.</target>
        </trans-unit>
        <trans-unit id="d1230decfda989b60168bc88df7b70ef79122b2d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.datasets&quot;&gt;&lt;code&gt;sklearn.datasets&lt;/code&gt;&lt;/a&gt; module includes utilities to load datasets, including methods to load and fetch popular reference datasets. It also features some artificial data generators.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.datasets&quot;&gt; &lt;code&gt;sklearn.datasets&lt;/code&gt; 의&lt;/a&gt; 모듈로드 인기 기준 데이터 세트를 페치하는 방법을 포함하는 데이터 세트에로드 유틸리티를 포함한다. 또한 일부 인공 데이터 생성기가 있습니다.</target>
        </trans-unit>
        <trans-unit id="94bdb0abc615359801b0dde8f5ed432fa774aae6" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.decomposition&quot;&gt;&lt;code&gt;sklearn.decomposition&lt;/code&gt;&lt;/a&gt; module includes matrix decomposition algorithms, including among others PCA, NMF or ICA. Most of the algorithms of this module can be regarded as dimensionality reduction techniques.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.decomposition&quot;&gt; &lt;code&gt;sklearn.decomposition&lt;/code&gt; 의&lt;/a&gt; 모듈은, 행렬 분해 알고리즘을 포함하는 다른 PCA, ICA 중 NMF 또는 포함된다. 이 모듈의 대부분의 알고리즘은 차원 축소 기술로 간주 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="cce83af2900332bbe995457713d2e3977e6cea91" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module includes ensemble-based methods for classification, regression and anomaly detection.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; 모듈 분류, 회귀 이상 검출을위한 앙상블 기반 방법을 포함한다.</target>
        </trans-unit>
        <trans-unit id="a3b34965d608c8571221686c4eaa7dd2128cda34" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.exceptions&quot;&gt;&lt;code&gt;sklearn.exceptions&lt;/code&gt;&lt;/a&gt; module includes all custom warnings and error classes used across scikit-learn.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.exceptions&quot;&gt; &lt;code&gt;sklearn.exceptions&lt;/code&gt; 의&lt;/a&gt; 모듈에 걸쳐 사용되는 모든 사용자 정의 경고 및 오류 클래스 포함 scikit을 배우기.</target>
        </trans-unit>
        <trans-unit id="953e85b8304fe86126d3f8d4d49e2c347def818a" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.feature_extraction&quot;&gt;&lt;code&gt;sklearn.feature_extraction&lt;/code&gt;&lt;/a&gt; module deals with feature extraction from raw data. It currently includes methods to extract features from text and images.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.feature_extraction&quot;&gt; &lt;code&gt;sklearn.feature_extraction&lt;/code&gt; 의&lt;/a&gt; 모듈은 원시 데이터에서 특징 추출 다루고있다. 현재 텍스트와 이미지에서 기능을 추출하는 방법이 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="2ff97b1fa019f5f400e3468860cd96aea04f63dc" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.feature_extraction.image&quot;&gt;&lt;code&gt;sklearn.feature_extraction.image&lt;/code&gt;&lt;/a&gt; submodule gathers utilities to extract features from images.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.feature_extraction.image&quot;&gt; &lt;code&gt;sklearn.feature_extraction.image&lt;/code&gt; 의&lt;/a&gt; 서브 모듈 집결 유틸리티는 이미지에서 특징을 추출합니다.</target>
        </trans-unit>
        <trans-unit id="5fb7f21374928a29d973d39c8eed205507941559" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.feature_extraction.text&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt;&lt;/a&gt; submodule gathers utilities to build feature vectors from text documents.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.feature_extraction.text&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; &lt;/a&gt; 서브 모듈 집결 유틸리티는 텍스트 문서로부터 특징 벡터를 구축합니다.</target>
        </trans-unit>
        <trans-unit id="f8894b12541a4b0b591ccbf9c1c5e42bf4d0c13b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.feature_selection&quot;&gt;&lt;code&gt;sklearn.feature_selection&lt;/code&gt;&lt;/a&gt; module implements feature selection algorithms. It currently includes univariate filter selection methods and the recursive feature elimination algorithm.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.feature_selection&quot;&gt; &lt;code&gt;sklearn.feature_selection&lt;/code&gt; 의&lt;/a&gt; 모듈 구현을 선택 알고리즘 기능. 현재는 일 변량 필터 선택 방법과 재귀 기능 제거 알고리즘을 포함합니다.</target>
        </trans-unit>
        <trans-unit id="af392a06a08e896e0c1f9a845ceba81c0151ed14" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.gaussian_process&quot;&gt;&lt;code&gt;sklearn.gaussian_process&lt;/code&gt;&lt;/a&gt; module implements Gaussian Process based regression and classification.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.gaussian_process&quot;&gt; &lt;code&gt;sklearn.gaussian_process&lt;/code&gt; 의&lt;/a&gt; 모듈 구현 가우시안 프로세스를 기반으로 회귀 및 분류.</target>
        </trans-unit>
        <trans-unit id="e25959a779b184ae02a906c2808f68686c73aab5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.kernel_approximation&quot;&gt;&lt;code&gt;sklearn.kernel_approximation&lt;/code&gt;&lt;/a&gt; module implements several approximate kernel feature maps base on Fourier transforms.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.kernel_approximation&quot;&gt; &lt;code&gt;sklearn.kernel_approximation&lt;/code&gt; 의&lt;/a&gt; 모듈 구현 몇 가지 대략적인 커널 기능은 푸리에 변환에 기반을 매핑합니다.</target>
        </trans-unit>
        <trans-unit id="cf3dc31fd9ef458aef6de4af32bf51a7e61106a1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.linear_model&quot;&gt;&lt;code&gt;sklearn.linear_model&lt;/code&gt;&lt;/a&gt; module implements generalized linear models. It includes Ridge regression, Bayesian Regression, Lasso and Elastic Net estimators computed with Least Angle Regression and coordinate descent. It also implements Stochastic Gradient Descent related algorithms.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.linear_model&quot;&gt; &lt;code&gt;sklearn.linear_model&lt;/code&gt; 의&lt;/a&gt; 모듈을 구현하는 선형 모델 일반화. 여기에는 최소 회귀 및 좌표 하강으로 계산 된 릿지 회귀, 베이지안 회귀, 올가미 및 탄성 망 추정기가 포함됩니다. 또한 확률 적 그라디언트 디센트 관련 알고리즘을 구현합니다.</target>
        </trans-unit>
        <trans-unit id="75e43c84de91a4a1d643ca88db0beef9e86494b8" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.manifold&quot;&gt;&lt;code&gt;sklearn.manifold&lt;/code&gt;&lt;/a&gt; module implements data embedding techniques.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.manifold&quot;&gt; &lt;code&gt;sklearn.manifold&lt;/code&gt; &lt;/a&gt; 모듈을 구현하는 데이터 임베딩 기술.</target>
        </trans-unit>
        <trans-unit id="f55aa3d6c230d41fe62ec5929fa59e00645b5d62" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; module includes score functions, performance metrics and pairwise metrics and distance computations.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; 의&lt;/a&gt; 모듈 점수 기능, 성능 메트릭 및 페어 메트릭 및 거리 계산을 포함한다.</target>
        </trans-unit>
        <trans-unit id="90553131dabe004a613ea2c87be35b6b6db9a1ae" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.metrics.cluster&quot;&gt;&lt;code&gt;sklearn.metrics.cluster&lt;/code&gt;&lt;/a&gt; submodule contains evaluation metrics for cluster analysis results. There are two forms of evaluation:</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.metrics.cluster&quot;&gt; &lt;code&gt;sklearn.metrics.cluster&lt;/code&gt; 의&lt;/a&gt; 서브 모듈은 클러스터 분석 결과에 대한 평가 지표가 포함되어 있습니다. 평가에는 두 가지 형태가 있습니다.</target>
        </trans-unit>
        <trans-unit id="537333336506a029d4e76c0c5320f3e14636c908" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.mixture&quot;&gt;&lt;code&gt;sklearn.mixture&lt;/code&gt;&lt;/a&gt; module implements mixture modeling algorithms.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.mixture&quot;&gt; &lt;code&gt;sklearn.mixture&lt;/code&gt; 의&lt;/a&gt; 모듈 구현 혼합물 모델링 알고리즘.</target>
        </trans-unit>
        <trans-unit id="1b9bcfe9136ee1328197e574e66457c49aa39ded" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.naive_bayes&quot;&gt;&lt;code&gt;sklearn.naive_bayes&lt;/code&gt;&lt;/a&gt; module implements Naive Bayes algorithms. These are supervised learning methods based on applying Bayes&amp;rsquo; theorem with strong (naive) feature independence assumptions.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.naive_bayes&quot;&gt; &lt;code&gt;sklearn.naive_bayes&lt;/code&gt; 는&lt;/a&gt; 구현을 나이브 베이 즈 알고리즘 모듈을 포함한다. 이것들은 강력한 (순진한) 특징 독립성 가정으로 베이 즈 정리를 적용하는 것에 기초한지도 학습 방법입니다.</target>
        </trans-unit>
        <trans-unit id="31da4b6c2407f749b7e6e441bc1101265b243a97" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; module implements the k-nearest neighbors algorithm.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; 는&lt;/a&gt; 구현에게 K-가장 가까운 이웃 알고리즘을 모듈.</target>
        </trans-unit>
        <trans-unit id="637db5b82af4c4775ad8c11b2cc086c407ac4adc" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.neural_network&quot;&gt;&lt;code&gt;sklearn.neural_network&lt;/code&gt;&lt;/a&gt; module includes models based on neural networks.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.neural_network&quot;&gt; &lt;code&gt;sklearn.neural_network&lt;/code&gt; 의&lt;/a&gt; 모듈은 신경 네트워크를 기반으로 모델을 포함하고 있습니다.</target>
        </trans-unit>
        <trans-unit id="97c84f48ddcbce9eff5bb423de61ca9bed7742a5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline&lt;/code&gt;&lt;/a&gt; module implements utilities to build a composite estimator, as a chain of transforms and estimators.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline&lt;/code&gt; &lt;/a&gt; 모듈 구현 유틸리티 변환하여 추정기의 체인과 같은 복합 추정기를 구축.</target>
        </trans-unit>
        <trans-unit id="3e4a3abf94a63259dfe9d5546d6b613a02821c2d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.preprocessing&quot;&gt;&lt;code&gt;sklearn.preprocessing&lt;/code&gt;&lt;/a&gt; module includes scaling, centering, normalization, binarization and imputation methods.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.preprocessing&quot;&gt; &lt;code&gt;sklearn.preprocessing&lt;/code&gt; &lt;/a&gt; 모듈은 스케일링 중심 정규화 이진화 전가 및 방법을 포함한다.</target>
        </trans-unit>
        <trans-unit id="5fd91efb13a21a364a66a195be3f60dbc3429cc3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.semi_supervised&quot;&gt;&lt;code&gt;sklearn.semi_supervised&lt;/code&gt;&lt;/a&gt; module implements semi-supervised learning algorithms. These algorithms utilized small amounts of labeled data and large amounts of unlabeled data for classification tasks. This module includes Label Propagation.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.semi_supervised&quot;&gt; &lt;code&gt;sklearn.semi_supervised&lt;/code&gt; &lt;/a&gt; 모듈의 구현은 반지도 학습 알고리즘. 이러한 알고리즘은 분류 작업에 소량의 레이블이 지정된 데이터와 대량의 레이블이없는 데이터를 사용했습니다. 이 모듈에는 레이블 전파가 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="6b1e5de562db4c7ae4499c2a4fcb3f75a3027318" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.svm&quot;&gt;&lt;code&gt;sklearn.svm&lt;/code&gt;&lt;/a&gt; module includes Support Vector Machine algorithms.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.svm&quot;&gt; &lt;code&gt;sklearn.svm&lt;/code&gt; 의&lt;/a&gt; 모듈은 지원 벡터 기계 알고리즘을 포함하고 있습니다.</target>
        </trans-unit>
        <trans-unit id="ef3c16856f883650f7c10c3b8b62a045804fc7da" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.tree&quot;&gt;&lt;code&gt;sklearn.tree&lt;/code&gt;&lt;/a&gt; module includes decision tree-based models for classification and regression.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.tree&quot;&gt; &lt;code&gt;sklearn.tree&lt;/code&gt; &lt;/a&gt; 모듈은 분류와 회귀를위한 의사 결정 트리 기반 모델이 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="fd392963cb16a5b60813f22af8246fa4065eb565" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.utils&quot;&gt;&lt;code&gt;sklearn.utils&lt;/code&gt;&lt;/a&gt; module includes various utilities.</source>
          <target state="translated">&lt;a href=&quot;#module-sklearn.utils&quot;&gt; &lt;code&gt;sklearn.utils&lt;/code&gt; 의&lt;/a&gt; 모듈은 다양한 유틸리티가 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="9c614be243d558a71ca4ede548ecf4767e4adc7c" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;simple example on this dataset&lt;/a&gt; illustrates how starting from the original problem one can shape the data for consumption in scikit-learn.</source>
          <target state="translated">&lt;a href=&quot;../../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;이 데이터 세트에&lt;/a&gt; 대한 간단한 예 는 원래 문제에서 시작하여 scikit-learn에서 소비 할 데이터를 구성하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="424aa7402b9869b036306a671e3630b4177e36b0" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../modules/tree#tree&quot;&gt;decision trees&lt;/a&gt; is used to fit a sine curve with addition noisy observation. As a result, it learns local linear regressions approximating the sine curve.</source>
          <target state="translated">&lt;a href=&quot;../../modules/tree#tree&quot;&gt;의사 결정 트리가&lt;/a&gt; 추가 시끄러운 관찰과 사인 곡선에 맞게 사용된다. 결과적으로 사인 곡선에 가까운 국소 선형 회귀를 학습합니다.</target>
        </trans-unit>
        <trans-unit id="eb2cbae46431d84a4889d55d659950b594e78664" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../modules/tree#tree&quot;&gt;decision trees&lt;/a&gt; is used to predict simultaneously the noisy x and y observations of a circle given a single underlying feature. As a result, it learns local linear regressions approximating the circle.</source>
          <target state="translated">&lt;a href=&quot;../../modules/tree#tree&quot;&gt;의사 결정 나무는&lt;/a&gt; 동시에 시끄러운 x와 하나의 기본 기능 주어진 원의 Y 관찰을 예측하는 데 사용됩니다. 결과적으로 원에 근사한 국소 선형 회귀를 학습합니다.</target>
        </trans-unit>
        <trans-unit id="4ea0ad8f51ec5bec92f088b272fa90a6ac2d5b55" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt;&lt;/a&gt; function is a data fetching / caching functions that downloads the data archive from the original &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;20 newsgroups website&lt;/a&gt;, extracts the archive contents in the &lt;code&gt;~/scikit_learn_data/20news_home&lt;/code&gt; folder and calls the &lt;a href=&quot;../modules/generated/sklearn.datasets.load_files#sklearn.datasets.load_files&quot;&gt;&lt;code&gt;sklearn.datasets.load_files&lt;/code&gt;&lt;/a&gt; on either the training or testing set folder, or both of them:</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt; 의&lt;/a&gt; 기능은 데이터 원본에서 보관 다운로드하는 것이 기능을 캐시 / 데이터 가져 오는 것입니다 &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;(20) 뉴스 그룹 웹 사이트&lt;/a&gt; 에서 아카이브 내용을 추출 &lt;code&gt;~/scikit_learn_data/20news_home&lt;/code&gt; 폴더와 통화 &lt;a href=&quot;../modules/generated/sklearn.datasets.load_files#sklearn.datasets.load_files&quot;&gt; &lt;code&gt;sklearn.datasets.load_files&lt;/code&gt; 을&lt;/a&gt; 하거나 훈련에 또는 테스트 세트 폴더 또는 둘 다 :</target>
        </trans-unit>
        <trans-unit id="26c038b3ea935758dab579b3237ee5588d78f251" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_pairs#sklearn.datasets.fetch_lfw_pairs&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_lfw_pairs&lt;/code&gt;&lt;/a&gt; datasets is subdivided into 3 subsets: the development &lt;code&gt;train&lt;/code&gt; set, the development &lt;code&gt;test&lt;/code&gt; set and an evaluation &lt;code&gt;10_folds&lt;/code&gt; set meant to compute performance metrics using a 10-folds cross validation scheme.</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_pairs#sklearn.datasets.fetch_lfw_pairs&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_lfw_pairs&lt;/code&gt; 의&lt;/a&gt; 데이터 세트가 3 개 하위 집합으로 세분화되어 개발 &lt;code&gt;train&lt;/code&gt; 세트, 개발 &lt;code&gt;test&lt;/code&gt; 세트 및 평가 &lt;code&gt;10_folds&lt;/code&gt; 는 10 폴드 교차 검증 방식을 사용 컴퓨팅 성능 측정에 의미 설정합니다.</target>
        </trans-unit>
        <trans-unit id="d14958ad2582740fd909337c2882b7ba18717e2a" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module includes two averaging algorithms based on randomized &lt;a href=&quot;tree#tree&quot;&gt;decision trees&lt;/a&gt;: the RandomForest algorithm and the Extra-Trees method. Both algorithms are perturb-and-combine techniques &lt;a href=&quot;#b1998&quot; id=&quot;id5&quot;&gt;[B1998]&lt;/a&gt; specifically designed for trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; 모듈은 무작위에 따라 두 평균 알고리즘이 포함되어 &lt;a href=&quot;tree#tree&quot;&gt;의사 결정 트리&lt;/a&gt; 랜덤 포레스트 알고리즘과 엑스트라 나무 방법 :. 두 알고리즘 모두 나무를 위해 특별히 설계된 섭동 및 결합 기법이다 &lt;a href=&quot;#b1998&quot; id=&quot;id5&quot;&gt;[B1998]&lt;/a&gt; . 이는 분류기 구성에 임의성을 도입하여 다양한 분류기 집합이 생성됨을 의미합니다. 앙상블의 예측은 개별 분류기의 평균 예측으로 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="fbeef59e0313a7e281a500dd36152abed677fa2e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.feature_extraction&quot;&gt;&lt;code&gt;sklearn.feature_extraction&lt;/code&gt;&lt;/a&gt; module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.feature_extraction&quot;&gt; &lt;code&gt;sklearn.feature_extraction&lt;/code&gt; 의&lt;/a&gt; 모듈은 텍스트 및 이미지 등의 형식으로 구성된 데이터 세트에서 기계 학습 알고리즘에 의해 지원되는 형식으로 특징을 추출 할 수있다.</target>
        </trans-unit>
        <trans-unit id="565412031e53246181e593ab56b9ab7f3accb362" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; module implements several loss, score, and utility functions to measure classification performance. Some metrics might require probability estimates of the positive class, confidence values, or binary decisions values. Most implementations allow each sample to provide a weighted contribution to the overall score, through the &lt;code&gt;sample_weight&lt;/code&gt; parameter.</source>
          <target state="translated">그만큼 &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; 은&lt;/a&gt; , 구현을 여러 손실을 모듈 점수 및 유틸리티 기능 분류 성능을 측정 할 수 있습니다. 일부 메트릭에는 양의 클래스, 신뢰도 값 또는 이진 결정 값의 확률 추정값이 필요할 수 있습니다. 대부분의 구현에서는 각 샘플이 &lt;code&gt;sample_weight&lt;/code&gt; 매개 변수를통해 전체 점수에 가중치를 부여 할 수있습니다.</target>
        </trans-unit>
        <trans-unit id="6986be647f522d4ad92a86deccdacfb4588f163b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; module implements several loss, score, and utility functions to measure regression performance. Some of those have been enhanced to handle the multioutput case: &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;mean_squared_error&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt;&lt;code&gt;mean_absolute_error&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; 은&lt;/a&gt; , 구현을 여러 손실을 모듈 점수 및 유틸리티 함수는 회귀 성능을 측정 할 수 있습니다. 그중 일부는 다중 출력 사례를 처리하도록 향상되었습니다 : &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;mean_squared_error&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt; &lt;code&gt;mean_absolute_error&lt;/code&gt; &lt;/a&gt; ,&lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; &lt;/a&gt; 및&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; 을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="e3af9dc32993fb04e5c47da4dea690da48a6baa4" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; module implements several loss, score, and utility functions. For more information see the &lt;a href=&quot;clustering#clustering-evaluation&quot;&gt;Clustering performance evaluation&lt;/a&gt; section for instance clustering, and &lt;a href=&quot;biclustering#biclustering-evaluation&quot;&gt;Biclustering evaluation&lt;/a&gt; for biclustering.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; &lt;/a&gt; 구현을 여러 손실, 점수 및 유틸리티 기능을 모듈. 자세한 정보 는 인스턴스 클러스터링에 대한 &lt;a href=&quot;clustering#clustering-evaluation&quot;&gt;클러스터링 성능 평가&lt;/a&gt; 섹션 및 &lt;a href=&quot;biclustering#biclustering-evaluation&quot;&gt;바이&lt;/a&gt; 클러스터링에 대한 Biclustering 평가 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="095cb4e1ad7cf586616a563cdbf95404fbb2310e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt; submodule implements utilities to evaluate pairwise distances or affinity of sets of samples.</source>
          <target state="translated">그만큼 &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; 의&lt;/a&gt; 서브 모듈 구현 유틸리티는 페어 거리 또는 샘플 세트의 친 화성을 평가.</target>
        </trans-unit>
        <trans-unit id="60f0063776d96ccddba5880841f7defdb7f0d5d9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt;&lt;code&gt;sklearn.multiclass&lt;/code&gt;&lt;/a&gt; module implements &lt;em&gt;meta-estimators&lt;/em&gt; to solve &lt;code&gt;multiclass&lt;/code&gt; and &lt;code&gt;multilabel&lt;/code&gt; classification problems by decomposing such problems into binary classification problems. Multitarget regression is also supported.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt; &lt;code&gt;sklearn.multiclass&lt;/code&gt; 의&lt;/a&gt; 모듈 구현의 &lt;em&gt;메타 - 추정기&lt;/em&gt; 해결 &lt;code&gt;multiclass&lt;/code&gt; 및 &lt;code&gt;multilabel&lt;/code&gt; 이진 분류 문제에 대한 이러한 문제를 분해하여 분류 문제. 다중 대상 회귀도 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="8db5d205727541fd60809b9d143967244bf8e79b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.random_projection&quot;&gt;&lt;code&gt;sklearn.random_projection&lt;/code&gt;&lt;/a&gt; module implements a simple and computationally efficient way to reduce the dimensionality of the data by trading a controlled amount of accuracy (as additional variance) for faster processing times and smaller model sizes. This module implements two types of unstructured random matrix: &lt;a href=&quot;#gaussian-random-matrix&quot;&gt;Gaussian random matrix&lt;/a&gt; and &lt;a href=&quot;#sparse-random-matrix&quot;&gt;sparse random matrix&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.random_projection&quot;&gt; &lt;code&gt;sklearn.random_projection&lt;/code&gt; 의&lt;/a&gt; 모듈 구현하는 간단하고 계산적으로 효율적인 방법은 빠른 처리 시간과 작은 모델 크기에 (추가의 편차 등) 정도의 양을 조절하여 거래 데이터의 차원을 감소시킨다. 이 모듈은 두 가지 유형의 비정형 랜덤 매트릭스를 구현합니다.&lt;a href=&quot;#gaussian-random-matrix&quot;&gt; 가우시안 랜덤 매트릭스&lt;/a&gt; 와 &lt;a href=&quot;#sparse-random-matrix&quot;&gt;희소 랜덤 매트릭스의&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="aaa03339275413a44471cce8ed9110742f7e29fb" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt;&lt;code&gt;AgglomerativeClustering&lt;/code&gt;&lt;/a&gt; object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy:</source>
          <target state="translated">그만큼 &lt;a href=&quot;generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt; &lt;code&gt;AgglomerativeClustering&lt;/code&gt; 의&lt;/a&gt; 객체가 수행하는 상향식 접근 방식을 사용하여 계층 적 클러스터링 : 자신의 클러스터의 각 관측을 시작하고, 클러스터는 연속적으로 함께 병합됩니다. 연결 기준에 따라 병합 전략에 사용되는 메트릭이 결정됩니다.</target>
        </trans-unit>
        <trans-unit id="913b5a9805377fabb258d2653b5b70e8adeffb2c" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.bicluster.spectralbiclustering#sklearn.cluster.bicluster.SpectralBiclustering&quot;&gt;&lt;code&gt;SpectralBiclustering&lt;/code&gt;&lt;/a&gt; algorithm assumes that the input data matrix has a hidden checkerboard structure. The rows and columns of a matrix with this structure may be partitioned so that the entries of any bicluster in the Cartesian product of row clusters and column clusters are approximately constant. For instance, if there are two row partitions and three column partitions, each row will belong to three biclusters, and each column will belong to two biclusters.</source>
          <target state="translated">그만큼 &lt;a href=&quot;generated/sklearn.cluster.bicluster.spectralbiclustering#sklearn.cluster.bicluster.SpectralBiclustering&quot;&gt; &lt;code&gt;SpectralBiclustering&lt;/code&gt; 의&lt;/a&gt; 알고리즘이 입력 데이터 행렬은 숨겨진 바둑판 구조를 갖는 것으로 가정한다. 이러한 구조를 갖는 매트릭스의 행 및 열은 분할되어 행 클러스터 및 열 클러스터의 데카르트 곱에서 임의의 bicluster의 엔트리가 대략 일정하도록 할 수있다. 예를 들어, 2 개의 행 파티션과 3 개의 열 파티션이있는 경우 각 행은 3 개의 biclusters에 속하고 각 열은 2 개의 biclusters에 속합니다.</target>
        </trans-unit>
        <trans-unit id="96812a842015efa920168397038c120e6e957561" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.bicluster.spectralcoclustering#sklearn.cluster.bicluster.SpectralCoclustering&quot;&gt;&lt;code&gt;SpectralCoclustering&lt;/code&gt;&lt;/a&gt; algorithm finds biclusters with values higher than those in the corresponding other rows and columns. Each row and each column belongs to exactly one bicluster, so rearranging the rows and columns to make partitions contiguous reveals these high values along the diagonal:</source>
          <target state="translated">그만큼 &lt;a href=&quot;generated/sklearn.cluster.bicluster.spectralcoclustering#sklearn.cluster.bicluster.SpectralCoclustering&quot;&gt; &lt;code&gt;SpectralCoclustering&lt;/code&gt; 의&lt;/a&gt; 알고리즘은 높은 대응 다른 행과 열의 값보다 biclusters를 찾는다. 각 행과 각 열은 정확히 하나의 bicluster에 속하므로 행과 열을 다시 정렬하여 파티션을 연속적으로 만들면 대각선을 따라 이러한 높은 값이 나타납니다.</target>
        </trans-unit>
        <trans-unit id="9debcd56df8be7e32ea091b79dc8e313d63ea1d3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.birch#sklearn.cluster.Birch&quot;&gt;&lt;code&gt;Birch&lt;/code&gt;&lt;/a&gt; builds a tree called the Characteristic Feature Tree (CFT) for the given data. The data is essentially lossy compressed to a set of Characteristic Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Characteristic Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children.</source>
          <target state="translated">그만큼 &lt;a href=&quot;generated/sklearn.cluster.birch#sklearn.cluster.Birch&quot;&gt; &lt;code&gt;Birch&lt;/code&gt; &lt;/a&gt; 주어진 데이터에 대한 특징 트리 (CFT)라는 나무를 구축합니다. 데이터는 본질적으로 일련의 특성 피쳐 노드 (CF 노드)로 손실 압축됩니다. CF 노드에는 특성 기능 서브 클러스터 (CF 서브 클러스터)라고하는 많은 서브 클러스터가 있으며 비 터미널 CF 노드에있는 이러한 CF 서브 클러스터는 CF 노드를 자식으로 가질 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e8b115edebda7f7bf86445503d0ce08900b4f8de" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;DBSCAN&lt;/code&gt;&lt;/a&gt; algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of &lt;em&gt;core samples&lt;/em&gt;, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, &lt;code&gt;min_samples&lt;/code&gt; and &lt;code&gt;eps&lt;/code&gt;, which define formally what we mean when we say &lt;em&gt;dense&lt;/em&gt;. Higher &lt;code&gt;min_samples&lt;/code&gt; or lower &lt;code&gt;eps&lt;/code&gt; indicate higher density necessary to form a cluster.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt; &lt;code&gt;DBSCAN&lt;/code&gt; &lt;/a&gt; 고밀도 영역이 저밀도 영역으로 구분로서 알고리즘은 클러스터를 플레이. 이 다소 일반적인 관점으로 인해 DBSCAN에서 찾은 클러스터는 k-means와 달리 클러스터가 볼록한 모양이라고 가정하는 것과는 달리 어떤 모양이든 될 수 있습니다. DBSCAN의 핵심 구성 요소는 &lt;em&gt;핵심 샘플&lt;/em&gt; 개념이며 , 이는 밀도가 높은 영역에있는 샘플입니다. 따라서 클러스터는 서로 가까이있는 (일부 거리 측정으로 측정 된) 코어 샘플 세트와 코어 샘플에 가깝지만 코어 샘플이 아닌 비 코어 샘플 세트입니다. 알고리즘, 두 개의 매개 변수가 &lt;code&gt;min_samples&lt;/code&gt; 및 &lt;code&gt;eps&lt;/code&gt; 공식적으로 정의, 우리가 말할 때 우리가 무엇을 의미하는지에 관해 이하 &lt;code&gt;eps&lt;/code&gt; &lt;em&gt; 밀도&lt;/em&gt; 합니다. 더 높은 &lt;code&gt;min_samples&lt;/code&gt; 는 클러스터를 형성하는 데 필요한 높은 밀도를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="844222980d29de5ed47698200091f13bdd09a284" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.featureagglomeration#sklearn.cluster.FeatureAgglomeration&quot;&gt;&lt;code&gt;FeatureAgglomeration&lt;/code&gt;&lt;/a&gt; uses agglomerative clustering to group together features that look very similar, thus decreasing the number of features. It is a dimensionality reduction tool, see &lt;a href=&quot;unsupervised_reduction#data-reduction&quot;&gt;Unsupervised dimensionality reduction&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.featureagglomeration#sklearn.cluster.FeatureAgglomeration&quot;&gt; &lt;code&gt;FeatureAgglomeration&lt;/code&gt; 은&lt;/a&gt; 함께 그룹으로 응집성 클러스터링을 사용하여 기능의 수를 감소, 매우 비슷한 표정을 갖추고 있습니다. 차원 축소 도구 입니다. &lt;a href=&quot;unsupervised_reduction#data-reduction&quot;&gt;감독되지 않은 차원 축소를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="3ccd68ae912b5d8e7b609345a756782aaea1f1b3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt;&lt;code&gt;KMeans&lt;/code&gt;&lt;/a&gt; algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the &lt;a href=&quot;inertia&quot;&gt;inertia&lt;/a&gt; or within-cluster sum-of-squares. This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt; &lt;code&gt;KMeans&lt;/code&gt; 은&lt;/a&gt; 으로 알려진 기준 최소화 등분 n 개의 그룹으로 분리 된 샘플을 시도하여 클러스터 데이터 산법 &lt;a href=&quot;inertia&quot;&gt;관성&lt;/a&gt; 또는 클러스터 내 제곱합한다. 이 알고리즘을 사용하려면 클러스터 수를 지정해야합니다. 그것은 많은 수의 샘플로 잘 확장되며 다양한 분야의 광범위한 응용 분야에서 사용되었습니다.</target>
        </trans-unit>
        <trans-unit id="f8f9ba49e304c2e7e84cbf4122c9838a65e0d463" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt;&lt;code&gt;MiniBatchKMeans&lt;/code&gt;&lt;/a&gt; is a variant of the &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt;&lt;code&gt;KMeans&lt;/code&gt;&lt;/a&gt; algorithm which uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function. Mini-batches are subsets of the input data, randomly sampled in each training iteration. These mini-batches drastically reduce the amount of computation required to converge to a local solution. In contrast to other algorithms that reduce the convergence time of k-means, mini-batch k-means produces results that are generally only slightly worse than the standard algorithm.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt; &lt;code&gt;MiniBatchKMeans&lt;/code&gt; 은&lt;/a&gt; 의 변종이다 &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt; &lt;code&gt;KMeans&lt;/code&gt; &lt;/a&gt; 여전히 동일한 목적 함수를 최적화하는 동안, 계산 시간을 줄이기 위해 미니 일괄 알고리즘을 사용. 미니-배치는 입력 데이터의 하위 집합으로, 각 트레이닝 반복에서 무작위로 샘플링됩니다. 이러한 미니 배치는 로컬 솔루션으로 수렴하는 데 필요한 계산량을 크게 줄입니다. k- 평균의 수렴 시간을 줄이는 다른 알고리즘과 달리 미니 배치 k- 평균은 일반적으로 표준 알고리즘보다 약간 더 나쁜 결과를 생성합니다.</target>
        </trans-unit>
        <trans-unit id="381def8c4d001638003d40e7acf9264b0a49ea0f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;ColumnTransformer&lt;/code&gt;&lt;/a&gt; helps performing different transformations for different columns of the data, within a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; that is safe from data leakage and that can be parametrized. &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;ColumnTransformer&lt;/code&gt;&lt;/a&gt; works on arrays, sparse matrices, and &lt;a href=&quot;http://pandas.pydata.org/pandas-docs/stable/&quot;&gt;pandas DataFrames&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;ColumnTransformer&lt;/code&gt; 는&lt;/a&gt; 내에서 데이터의 다른 열의 다른 변환을 수행하는 데 도움이 &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt; 데이터 유출로부터 안전하게되어 있고 그 매개 변수화 될 수있다. &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;ColumnTransformer&lt;/code&gt; &lt;/a&gt; 는 배열, 희소 행렬 및 &lt;a href=&quot;http://pandas.pydata.org/pandas-docs/stable/&quot;&gt;팬더 DataFrames에서 작동&lt;/a&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="37d727244bb97826f98eb0365b95bf6cd4afb239" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt; class is experimental and the API is subject to change.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;compose.ColumnTransformer&lt;/code&gt; 의&lt;/a&gt; 클래스는 실험이며, API는 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="83e5137b54932bec66ccc36542bace6834598b69" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.covariance.graphicallasso#sklearn.covariance.GraphicalLasso&quot;&gt;&lt;code&gt;GraphicalLasso&lt;/code&gt;&lt;/a&gt; estimator uses an l1 penalty to enforce sparsity on the precision matrix: the higher its &lt;code&gt;alpha&lt;/code&gt; parameter, the more sparse the precision matrix. The corresponding &lt;a href=&quot;generated/sklearn.covariance.graphicallassocv#sklearn.covariance.GraphicalLassoCV&quot;&gt;&lt;code&gt;GraphicalLassoCV&lt;/code&gt;&lt;/a&gt; object uses cross-validation to automatically set the &lt;code&gt;alpha&lt;/code&gt; parameter.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.covariance.graphicallasso#sklearn.covariance.GraphicalLasso&quot;&gt; &lt;code&gt;GraphicalLasso&lt;/code&gt; 의&lt;/a&gt; 추정기 정밀도 행렬 희소성을 적용하는 L1 페널티를 사용하여 상위의 &lt;code&gt;alpha&lt;/code&gt; 매개 더 희박한 정확도 행렬. 해당 &lt;a href=&quot;generated/sklearn.covariance.graphicallassocv#sklearn.covariance.GraphicalLassoCV&quot;&gt; &lt;code&gt;GraphicalLassoCV&lt;/code&gt; &lt;/a&gt; 객체는 교차 검증을 사용하여 &lt;code&gt;alpha&lt;/code&gt; 매개 변수 를 자동으로 설정합니다 .</target>
        </trans-unit>
        <trans-unit id="9447390bf3cfd368da76e6282f132428b32dfff8" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; object also provides a probabilistic interpretation of the PCA that can give a likelihood of data based on the amount of variance it explains. As such it implements a &lt;code&gt;score&lt;/code&gt; method that can be used in cross-validation:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; 의&lt;/a&gt; 객체는 또한 설명 분산의 양에 따라 데이터의 가능성을 줄 수있는 PCA의 확률 적 해석을 제공합니다. 따라서 교차 유효성 검사에 사용할 수 있는 &lt;code&gt;score&lt;/code&gt; 방법을 구현합니다 .</target>
        </trans-unit>
        <trans-unit id="de1125bcd2177e15b5b35e281621b5bbf18681e1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; object is very useful, but has certain limitations for large datasets. The biggest limitation is that &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; only supports batch processing, which means all of the data to be processed must fit in main memory. The &lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt;&lt;code&gt;IncrementalPCA&lt;/code&gt;&lt;/a&gt; object uses a different form of processing and allows for partial computations which almost exactly match the results of &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; while processing the data in a minibatch fashion. &lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt;&lt;code&gt;IncrementalPCA&lt;/code&gt;&lt;/a&gt; makes it possible to implement out-of-core Principal Component Analysis either by:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; 의&lt;/a&gt; 객체는 매우 유용하지만, 대규모 데이터 세트에 대한 특정 제한이 있습니다. 가장 큰 제한은 &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; 가 일괄 처리 만 지원한다는 것입니다. 즉, 처리 할 모든 데이터가 기본 메모리에 맞아야합니다. &lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt; &lt;code&gt;IncrementalPCA&lt;/code&gt; 의&lt;/a&gt; 목적은 처리의 다른 형태를 사용하는 거의 정확한 결과와 일치하는 부분의 계산을 허용 &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; 를&lt;/a&gt; minibatch 방식으로 데이터를 처리하는 동안. &lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt; &lt;code&gt;IncrementalPCA&lt;/code&gt; 를&lt;/a&gt; 사용하면 다음 중 하나를 통해 코어 외부 주요 구성 요소 분석을 구현할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ecd6b33d3bd2199aadcf263cff0e5246cde4bd39" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.decomposition.sparsecoder#sklearn.decomposition.SparseCoder&quot;&gt;&lt;code&gt;SparseCoder&lt;/code&gt;&lt;/a&gt; object is an estimator that can be used to transform signals into sparse linear combination of atoms from a fixed, precomputed dictionary such as a discrete wavelet basis. This object therefore does not implement a &lt;code&gt;fit&lt;/code&gt; method. The transformation amounts to a sparse coding problem: finding a representation of the data as a linear combination of as few dictionary atoms as possible. All variations of dictionary learning implement the following transform methods, controllable via the &lt;code&gt;transform_method&lt;/code&gt; initialization parameter:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.sparsecoder#sklearn.decomposition.SparseCoder&quot;&gt; &lt;code&gt;SparseCoder&lt;/code&gt; 의&lt;/a&gt; 목적은 고정 된 발 원자 희소 선형 조합으로 신호를 변환하기 위해 사용될 수있는 추정 장치이며, 이산 웨이블릿 기반으로 사전 등을 미리 계산. 따라서이 객체는 &lt;code&gt;fit&lt;/code&gt; 메소드를 구현하지 않습니다 . 변환은 희소 한 코딩 문제에 해당한다 : 가능한 적은 수의 사전 원자의 선형 조합으로서 데이터의 표현을 찾는 것. 사전 학습의 모든 변형은 다음의 변환 메소드를 구현하며, &lt;code&gt;transform_method&lt;/code&gt; 초기화 매개 변수 를 통해 제어 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="51b760d25143490b212d3dce7b63a475beffe57d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.feature_extraction.image.extract_patches_2d#sklearn.feature_extraction.image.extract_patches_2d&quot;&gt;&lt;code&gt;extract_patches_2d&lt;/code&gt;&lt;/a&gt; function extracts patches from an image stored as a two-dimensional array, or three-dimensional with color information along the third axis. For rebuilding an image from all its patches, use &lt;a href=&quot;generated/sklearn.feature_extraction.image.reconstruct_from_patches_2d#sklearn.feature_extraction.image.reconstruct_from_patches_2d&quot;&gt;&lt;code&gt;reconstruct_from_patches_2d&lt;/code&gt;&lt;/a&gt;. For example let use generate a 4x4 pixel picture with 3 color channels (e.g. in RGB format):</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.image.extract_patches_2d#sklearn.feature_extraction.image.extract_patches_2d&quot;&gt; &lt;code&gt;extract_patches_2d&lt;/code&gt; &lt;/a&gt; 이미지로부터 추출 기능 패치는 2 차원 어레이로서 저장하거나, 제 축을 따라 색 정보와 입체. 모든 패치에서 이미지를 다시 작성하려면 &lt;a href=&quot;generated/sklearn.feature_extraction.image.reconstruct_from_patches_2d#sklearn.feature_extraction.image.reconstruct_from_patches_2d&quot;&gt; &lt;code&gt;reconstruct_from_patches_2d&lt;/code&gt; 를&lt;/a&gt; 사용하십시오 . 예를 들어 3 가지 색상 채널 (예 : RGB 형식)로 4x4 픽셀 사진을 생성 해 보겠습니다.</target>
        </trans-unit>
        <trans-unit id="13de12da96c62cdbe967814b1ded04e8c85eef13" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.feature_extraction.image.patchextractor#sklearn.feature_extraction.image.PatchExtractor&quot;&gt;&lt;code&gt;PatchExtractor&lt;/code&gt;&lt;/a&gt; class works in the same way as &lt;a href=&quot;generated/sklearn.feature_extraction.image.extract_patches_2d#sklearn.feature_extraction.image.extract_patches_2d&quot;&gt;&lt;code&gt;extract_patches_2d&lt;/code&gt;&lt;/a&gt;, only it supports multiple images as input. It is implemented as an estimator, so it can be used in pipelines. See:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.image.patchextractor#sklearn.feature_extraction.image.PatchExtractor&quot;&gt; &lt;code&gt;PatchExtractor&lt;/code&gt; 의&lt;/a&gt; 와 동일한 방식으로 클래스 작품 &lt;a href=&quot;generated/sklearn.feature_extraction.image.extract_patches_2d#sklearn.feature_extraction.image.extract_patches_2d&quot;&gt; &lt;code&gt;extract_patches_2d&lt;/code&gt; 은&lt;/a&gt; , 만 입력으로 여러 이미지를 지원합니다. 추정기로 구현되므로 파이프 라인에서 사용할 수 있습니다. 보다:</target>
        </trans-unit>
        <trans-unit id="5f1d9b11617dc21530d4a8e947334af50d14c144" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt; also comes with the following limitations:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; 은&lt;/a&gt; 또한 다음과 같은 제한 사항이 포함되어 있습니다 :</target>
        </trans-unit>
        <trans-unit id="dec1e79879a530cf5d8d2ea5bf189d8118bb1961" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessclassifier#sklearn.gaussian_process.GaussianProcessClassifier&quot;&gt;&lt;code&gt;GaussianProcessClassifier&lt;/code&gt;&lt;/a&gt; implements Gaussian processes (GP) for classification purposes, more specifically for probabilistic classification, where test predictions take the form of class probabilities. GaussianProcessClassifier places a GP prior on a latent function \(f\), which is then squashed through a link function to obtain the probabilistic classification. The latent function \(f\) is a so-called nuisance function, whose values are not observed and are not relevant by themselves. Its purpose is to allow a convenient formulation of the model, and \(f\) is removed (integrated out) during prediction. GaussianProcessClassifier implements the logistic link function, for which the integral cannot be computed analytically but is easily approximated in the binary case.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessclassifier#sklearn.gaussian_process.GaussianProcessClassifier&quot;&gt; &lt;code&gt;GaussianProcessClassifier&lt;/code&gt; 의&lt;/a&gt; 구체적 테스트 클래스 예측 확률의 형태를 취할 확률적인 분류를위한 분류 목적을 위해 구현 가우시안 프로세스 (GP). GaussianProcessClassifier는 잠재 함수 \ (f \)보다 먼저 GP를 배치 한 다음 링크 함수를 통해 스쿼드되어 확률 적 분류를 얻습니다. 잠재 함수 \ (f \)는 소위 방해 함수로, 그 값은 관찰되지 않고 자체적으로 관련이 없습니다. 그것의 목적은 모델의 편리한 공식화를 허용하고, 예측 동안 \ (f \)가 제거 (통합)된다. GaussianProcessClassifier는 적분을 분석적으로 계산할 수 없지만 이진 경우에 쉽게 근사되는 로지스틱 링크 함수를 구현합니다.</target>
        </trans-unit>
        <trans-unit id="2a70b80f4163a2c6bfd08f3c8b84b458a9627cc7" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessregressor#sklearn.gaussian_process.GaussianProcessRegressor&quot;&gt;&lt;code&gt;GaussianProcessRegressor&lt;/code&gt;&lt;/a&gt; implements Gaussian processes (GP) for regression purposes. For this, the prior of the GP needs to be specified. The prior mean is assumed to be constant and zero (for &lt;code&gt;normalize_y=False&lt;/code&gt;) or the training data&amp;rsquo;s mean (for &lt;code&gt;normalize_y=True&lt;/code&gt;). The prior&amp;rsquo;s covariance is specified by a passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessregressor#sklearn.gaussian_process.GaussianProcessRegressor&quot;&gt; &lt;code&gt;GaussianProcessRegressor&lt;/code&gt; 의&lt;/a&gt; 회귀 목적으로 구현 가우시안 프로세스 (GP). 이를 위해서는 GP의 이전을 지정해야합니다. 이전 평균은 상수 및 0 ( &lt;code&gt;normalize_y=False&lt;/code&gt; 의 경우 ) 또는 훈련 데이터의 평균 ( &lt;code&gt;normalize_y=True&lt;/code&gt; 의 경우 )으로 가정합니다. 사전의 공분산은 &lt;a href=&quot;#gp-kernels&quot;&gt;커널&lt;/a&gt; 객체를 전달하여 지정됩니다 . 커널의 하이퍼 파라미터는 전달 &lt;code&gt;optimizer&lt;/code&gt; 기반으로 LLM (log-marginal-likelihood)을 최대화하여 GaussianProcessRegressor를 피팅하는 동안 최적화 됩니다. LML에 여러 개의 로컬 옵티마가있을 수 있으므로 다음을 지정하여 옵티 마이저를 반복적으로 시작할 수 있습니다. &lt;code&gt;n_restarts_optimizer&lt;/code&gt; . 첫 번째 실행은 항상 커널의 초기 하이퍼 파라미터 값에서 시작하여 수행됩니다. 후속 실행은 허용 된 값의 범위에서 무작위로 선택된 하이퍼 파라미터 값에서 수행됩니다. 초기 하이퍼 파라미터를 고정 상태로 유지해야하는 경우 최적화 프로그램으로 &lt;code&gt;None&lt;/code&gt; 을 전달할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="a013933edad2184a36ef1d15fcbf21a794c0c7f5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.constantkernel#sklearn.gaussian_process.kernels.ConstantKernel&quot;&gt;&lt;code&gt;ConstantKernel&lt;/code&gt;&lt;/a&gt; kernel can be used as part of a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt;&lt;code&gt;Product&lt;/code&gt;&lt;/a&gt; kernel where it scales the magnitude of the other factor (kernel) or as part of a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt;&lt;code&gt;Sum&lt;/code&gt;&lt;/a&gt; kernel, where it modifies the mean of the Gaussian process. It depends on a parameter \(constant\_value\). It is defined as:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.constantkernel#sklearn.gaussian_process.kernels.ConstantKernel&quot;&gt; &lt;code&gt;ConstantKernel&lt;/code&gt; 의&lt;/a&gt; 커널은의 일부로 사용할 수있는 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt; &lt;code&gt;Product&lt;/code&gt; &lt;/a&gt; 는 다른 요소 (커널)의 크기 또는의 한 부분으로 확장 커널 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt; &lt;code&gt;Sum&lt;/code&gt; &lt;/a&gt; 는 가우시안 프로세스의 평균 수정 커널. 매개 변수 \ (constant \ _value \)에 따라 다릅니다. 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="a1a78e3b1d5985ce1973c8989ff0075d7d078b79" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt; kernel is commonly combined with exponentiation. An example with exponent 2 is shown in the following figure:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt; &lt;code&gt;DotProduct&lt;/code&gt; &lt;/a&gt; 커널은 일반적으로 지수와 결합된다. 지수가 2 인 예는 다음 그림에 표시되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="299194d50f816028e01666a91e5aaf031d88d5c0" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt; kernel is non-stationary and can be obtained from linear regression by putting \(N(0, 1)\) priors on the coefficients of \(x_d (d = 1, . . . , D)\) and a prior of \(N(0, \sigma_0^2)\) on the bias. The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt; kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter \(\sigma_0^2\). For \(\sigma_0^2 = 0\), the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt; &lt;code&gt;DotProduct&lt;/code&gt; 의&lt;/a&gt; 커널 \ (N (0, 1) \)의 계수에 전과 \ (x_d (d = 1..., D) \) 비 정지하고 바꾸어 선형 회귀로부터 획득 될 수 있고, 종래 바이어스에서 \ (N (0, \ sigma_0 ^ 2) \) &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt; &lt;code&gt;DotProduct&lt;/code&gt; 의&lt;/a&gt; 커널은 기원에 대한 좌표가 아니라 번역의 회전에 불변이다. 매개 변수는 \ (\ sigma_0 ^ 2 \)에 의해 매개 변수화됩니다. \ (\ sigma_0 ^ 2 = 0 \)의 경우 커널을 동종 선형 커널이라고하며, 그렇지 않으면 동종이 아닙니다. 커널은</target>
        </trans-unit>
        <trans-unit id="9b254fd92e2584b8ca8c7f30ca756b0bac24ec2b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.expsinesquared#sklearn.gaussian_process.kernels.ExpSineSquared&quot;&gt;&lt;code&gt;ExpSineSquared&lt;/code&gt;&lt;/a&gt; kernel allows modeling periodic functions. It is parameterized by a length-scale parameter \(l&amp;gt;0\) and a periodicity parameter \(p&amp;gt;0\). Only the isotropic variant where \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.expsinesquared#sklearn.gaussian_process.kernels.ExpSineSquared&quot;&gt; &lt;code&gt;ExpSineSquared&lt;/code&gt; &lt;/a&gt; 커널은주기 함수 모델링을 허용한다. 길이 스케일 매개 변수 \ (l&amp;gt; 0 \) 및 주기성 매개 변수 \ (p&amp;gt; 0 \)에 의해 매개 변수화됩니다. 현재 \ (l \)가 스칼라 인 등방성 변형 만 지원됩니다. 커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="4fa9c3925ee31fe17ddb7d4f95d9aa563f446e7b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.matern#sklearn.gaussian_process.kernels.Matern&quot;&gt;&lt;code&gt;Matern&lt;/code&gt;&lt;/a&gt; kernel is a stationary kernel and a generalization of the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; kernel. It has an additional parameter \(\nu\) which controls the smoothness of the resulting function. It is parameterized by a length-scale parameter \(l&amp;gt;0\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs \(x\) (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.matern#sklearn.gaussian_process.kernels.Matern&quot;&gt; &lt;code&gt;Matern&lt;/code&gt; &lt;/a&gt; 커널은 고정 커널과의 일반화 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; 의&lt;/a&gt; 커널. 결과 함수의 부드러움을 제어하는 ​​추가 매개 변수 \ (\ nu \)가 있습니다. 스칼라 (커널의 등방성 변형) 또는 입력 값과 동일한 수의 벡터 (이방성 변형) 일 수있는 길이 스케일 매개 변수 ({l&amp;gt; 0 \)로 매개 변수화됩니다. 커널). 커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="9cc391d0a3f24c35e3e834b704611ffc08dfc23c" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt;&lt;code&gt;RationalQuadratic&lt;/code&gt;&lt;/a&gt; kernel can be seen as a scale mixture (an infinite sum) of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; kernels with different characteristic length-scales. It is parameterized by a length-scale parameter \(l&amp;gt;0\) and a scale mixture parameter \(\alpha&amp;gt;0\) Only the isotropic variant where \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt; &lt;code&gt;RationalQuadratic&lt;/code&gt; &lt;/a&gt; 커널의 규모 혼합물 (무한 합)으로 볼 수 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; 의&lt;/a&gt; 다양한 특성 길이 비늘 커널. 길이 스케일 모수 \ (l&amp;gt; 0 \) 및 스케일 혼합 모수 \ (\ alpha&amp;gt; 0 \)에 의해 모수화됩니다. \ (l \)가 스칼라 인 등방성 변형 만 지원됩니다. 커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="a0d4e8e5df8534d7f797dec945fa5951797b46d9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; kernel is a stationary kernel. It is also known as the &amp;ldquo;squared exponential&amp;rdquo; kernel. It is parameterized by a length-scale parameter \(l&amp;gt;0\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs \(x\) (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; 의&lt;/a&gt; 커널은 고정 된 커널입니다. &quot;제곱 지수&quot;커널이라고도합니다. 스칼라 (커널의 등방성 변형) 또는 입력 값과 동일한 수의 벡터 (이방성 변형) 일 수있는 길이 스케일 매개 변수 ({l&amp;gt; 0 \)로 매개 변수화됩니다. 커널). 커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="8f4b1aa7c1e397df865fdc8d1fd65546b5eaaf2f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt;&lt;code&gt;MissingIndicator&lt;/code&gt;&lt;/a&gt; transformer is useful to transform a dataset into corresponding binary matrix indicating the presence of missing values in the dataset. This transformation is useful in conjunction with imputation. When using imputation, preserving the information about which values had been missing can be informative.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt; &lt;code&gt;MissingIndicator&lt;/code&gt; &lt;/a&gt; 유용 변환기는 데이터 세트에서 누락 값의 존재를 나타내는 이진 행렬에 대응하는 데이터 세트를 변환한다. 이 변환은 대치와 함께 유용합니다. 대치 사용시 누락 된 값에 대한 정보를 보존하는 것이 유익 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c0c9736c8ad276e3deabee46eb181026e0204e8e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;SimpleImputer&lt;/code&gt;&lt;/a&gt; class also supports categorical data represented as string values or pandas categoricals when using the &lt;code&gt;'most_frequent'&lt;/code&gt; or &lt;code&gt;'constant'&lt;/code&gt; strategy:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;SimpleImputer&lt;/code&gt; 의&lt;/a&gt; 클래스는 사용하는 경우 문자열 값 또는 팬더 categoricals로 표현 범주 형 데이터 지원 &lt;code&gt;'most_frequent'&lt;/code&gt; 또는 &lt;code&gt;'constant'&lt;/code&gt; 전략을 :</target>
        </trans-unit>
        <trans-unit id="4d17103c250c5ab6ac126fd9a857f81de53fed6f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;SimpleImputer&lt;/code&gt;&lt;/a&gt; class also supports sparse matrices:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;SimpleImputer&lt;/code&gt; 의&lt;/a&gt; 클래스는 스파 스 매트릭스를 지원합니다 :</target>
        </trans-unit>
        <trans-unit id="618df5d6360d655fcf582933900e1cb3bcf02379" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;SimpleImputer&lt;/code&gt;&lt;/a&gt; class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;SimpleImputer&lt;/code&gt; 의&lt;/a&gt; 클래스는 누락 된 값을 전가하기위한 기본 전략을 제공합니다. 결 측값은 제공된 상수 값으로 결측되거나 결 측값이있는 각 열의 통계 (평균, 중간 또는 가장 빈번한)를 사용하여 대치 될 수 있습니다. 이 클래스는 다른 결 측값 인코딩도 허용합니다.</target>
        </trans-unit>
        <trans-unit id="6f59968771d1dbbd03a744853045d3c0b7aa414b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; constructs an approximate mapping for the radial basis function kernel, also known as &lt;em&gt;Random Kitchen Sinks&lt;/em&gt;&lt;a href=&quot;#rr2007&quot; id=&quot;id2&quot;&gt;[RR2007]&lt;/a&gt;. This transformation can be used to explicitly model a kernel map, prior to applying a linear algorithm, for example a linear SVM:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; 은&lt;/a&gt; 라고도 방사형 기저 함수 커널 대략 매핑 구축해 &lt;em&gt;랜덤 부엌 싱크 &lt;/em&gt;&lt;a href=&quot;#rr2007&quot; id=&quot;id2&quot;&gt;[RR2007를]&lt;/a&gt; . 이 변환은 선형 알고리즘 (예 : 선형 SVM)을 적용하기 전에 커널 맵을 명시 적으로 모델링하는 데 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="44948166b7a6695399209dd8e1e1f1af0b0058e5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt;&lt;code&gt;HuberRegressor&lt;/code&gt;&lt;/a&gt; differs from using &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; with loss set to &lt;code&gt;huber&lt;/code&gt; in the following ways.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt; &lt;code&gt;HuberRegressor&lt;/code&gt; 의&lt;/a&gt; 사용과 다른 &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; 을&lt;/a&gt; 분실 세트 &lt;code&gt;huber&lt;/code&gt; 다음과 같은 방법으로한다.</target>
        </trans-unit>
        <trans-unit id="7ab9e90f2b3f808e98761c90cab46994be6820bb" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt;&lt;code&gt;HuberRegressor&lt;/code&gt;&lt;/a&gt; is different to &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt; because it applies a linear loss to samples that are classified as outliers. A sample is classified as an inlier if the absolute error of that sample is lesser than a certain threshold. It differs from &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.ransacregressor#sklearn.linear_model.RANSACRegressor&quot;&gt;&lt;code&gt;RANSACRegressor&lt;/code&gt;&lt;/a&gt; because it does not ignore the effect of the outliers but gives a lesser weight to them.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt; &lt;code&gt;HuberRegressor&lt;/code&gt; 가&lt;/a&gt; 다릅니다 &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt; 가 이상치로 분류된다 샘플 선형 손실을 적용하기 때문이다. 샘플의 절대 오차가 특정 임계 값보다 작은 경우 샘플은 inlier로 분류됩니다. &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.linear_model.ransacregressor#sklearn.linear_model.RANSACRegressor&quot;&gt; &lt;code&gt;RANSACRegressor&lt;/code&gt; &lt;/a&gt; 는 특이 치의 영향을 무시하지는 않지만 가중치는 적기 때문에 다릅니다 .</target>
        </trans-unit>
        <trans-unit id="70ef4a25a40856b26fd987533d68c36540345c20" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt; is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer parameter values, effectively reducing the number of variables upon which the given solution is dependent. For this reason, the Lasso and its variants are fundamental to the field of compressed sensing. Under certain conditions, it can recover the exact set of non-zero weights (see &lt;a href=&quot;../auto_examples/applications/plot_tomography_l1_reconstruction#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py&quot;&gt;Compressive sensing: tomography reconstruction with L1 prior (Lasso)&lt;/a&gt;).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt; 스파 스 계수를 추정하는 선형 모델이다. 매개 변수 값이 적은 솔루션을 선호하는 경향이 있기 때문에 특정 상황에서 유용하며, 주어진 솔루션이 의존하는 변수의 수를 효과적으로 줄입니다. 이러한 이유로 Lasso와 그 변형은 압축 감지 분야의 기본입니다. 특정 조건 하에서 정확한 0이 아닌 가중치 세트를 복구 할 수 있습니다 ( &lt;a href=&quot;../auto_examples/applications/plot_tomography_l1_reconstruction#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py&quot;&gt;압축 감지 : L1 이전의 단층 촬영 재구성 (올가미) 참조&lt;/a&gt; ).</target>
        </trans-unit>
        <trans-unit id="8c1095adf7bd87312f73373efdee9c54e778b1be" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt;&lt;code&gt;MultiTaskElasticNet&lt;/code&gt;&lt;/a&gt; is an elastic-net model that estimates sparse coefficients for multiple regression problems jointly: &lt;code&gt;Y&lt;/code&gt; is a 2D array, of shape &lt;code&gt;(n_samples, n_tasks)&lt;/code&gt;. The constraint is that the selected features are the same for all the regression problems, also called tasks.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt; &lt;code&gt;MultiTaskElasticNet&lt;/code&gt; 는&lt;/a&gt; 공동 회귀 문제 성긴 계수를 추정하는 탄성 네트 모델이다 : &lt;code&gt;Y&lt;/code&gt; 는 2 차원 어레이의 형태이다 &lt;code&gt;(n_samples, n_tasks)&lt;/code&gt; . 제약 조건은 선택한 기능이 작업이라고도하는 모든 회귀 문제에 대해 동일하다는 것입니다.</target>
        </trans-unit>
        <trans-unit id="0855f24dbbabd45aa8775e800911f6fb3f3411c3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt;&lt;code&gt;MultiTaskLasso&lt;/code&gt;&lt;/a&gt; is a linear model that estimates sparse coefficients for multiple regression problems jointly: &lt;code&gt;y&lt;/code&gt; is a 2D array, of shape &lt;code&gt;(n_samples, n_tasks)&lt;/code&gt;. The constraint is that the selected features are the same for all the regression problems, also called tasks.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt; &lt;code&gt;MultiTaskLasso&lt;/code&gt; 는&lt;/a&gt; 공동 회귀 문제 성긴 계수를 추정하는 선형 모델이다 : &lt;code&gt;y&lt;/code&gt; 2 차원 어레이의 형태이다 &lt;code&gt;(n_samples, n_tasks)&lt;/code&gt; . 제약 조건은 선택한 기능이 작업이라고도하는 모든 회귀 문제에 대해 동일하다는 것입니다.</target>
        </trans-unit>
        <trans-unit id="d927ce91bac1b6df649580c487bdf34ce5f21e13" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.perceptron#sklearn.linear_model.Perceptron&quot;&gt;&lt;code&gt;Perceptron&lt;/code&gt;&lt;/a&gt; is another simple classification algorithm suitable for large scale learning. By default:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.perceptron#sklearn.linear_model.Perceptron&quot;&gt; &lt;code&gt;Perceptron&lt;/code&gt; &lt;/a&gt; 대규모 학습에 적합한 또 다른 간단한 분류 알고리즘이다. 기본적으로:</target>
        </trans-unit>
        <trans-unit id="dc6941408ce5829c04ee30dacb5eec313120d42e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; estimator uses a generalization of the median in multiple dimensions. It is thus robust to multivariate outliers. Note however that the robustness of the estimator decreases quickly with the dimensionality of the problem. It looses its robustness properties and becomes no better than an ordinary least squares in high dimension.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; 의&lt;/a&gt; 추정기는 다중 차원의 중간의 일반화를 이용한다. 따라서 다변량 이상치에 강합니다. 그러나 추정의 견고성은 문제의 차원에 따라 빠르게 감소합니다. 견고성이 떨어지고 높은 치수의 보통 최소 제곱보다 낫지 않습니다.</target>
        </trans-unit>
        <trans-unit id="497dd0db8e84137ac4b140cff3317a3423c09e22" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt;&lt;code&gt;accuracy_score&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Accuracy_and_precision&quot;&gt;accuracy&lt;/a&gt;, either the fraction (default) or the count (normalize=False) of correct predictions.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt; &lt;code&gt;accuracy_score&lt;/code&gt; 의&lt;/a&gt; 함수 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Accuracy_and_precision&quot;&gt;정밀도&lt;/a&gt; , 분획 (기본) 또는 정확한 예측 계수 (표준화 = 거짓)를 하나.</target>
        </trans-unit>
        <trans-unit id="734b9a83298cb0e5bb404a0eb951bb89a38c30c1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;http://en.wikipedia.org/w/index.php?title=Information_retrieval&amp;amp;oldid=793358396#Average_precision&quot;&gt;average precision&lt;/a&gt; (AP) from prediction scores. The value is between 0 and 1 and higher is better. AP is defined as</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt; &lt;code&gt;average_precision_score&lt;/code&gt; &lt;/a&gt; 기능은 계산 &lt;a href=&quot;http://en.wikipedia.org/w/index.php?title=Information_retrieval&amp;amp;oldid=793358396#Average_precision&quot;&gt;평균 정밀도&lt;/a&gt; 예측 점수에서 (AP)를. 값은 0과 1 사이이며 높을수록 좋습니다. AP는 다음과 같이 정의됩니다</target>
        </trans-unit>
        <trans-unit id="4c1e547613cb8b25282a0a1d2e2d0fa8b86fab4a" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.balanced_accuracy_score#sklearn.metrics.balanced_accuracy_score&quot;&gt;&lt;code&gt;balanced_accuracy_score&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Accuracy_and_precision&quot;&gt;balanced accuracy&lt;/a&gt;, which avoids inflated performance estimates on imbalanced datasets. It is the macro-average of recall scores per class or, equivalently, raw accuracy where each sample is weighted according to the inverse prevalence of its true class. Thus for balanced datasets, the score is equal to accuracy.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.balanced_accuracy_score#sklearn.metrics.balanced_accuracy_score&quot;&gt; &lt;code&gt;balanced_accuracy_score&lt;/code&gt; 의&lt;/a&gt; 기능은 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Accuracy_and_precision&quot;&gt;균형 정확성&lt;/a&gt; 불균형 데이터 세트에 비정상적으로 실적 추정을 피한다. 이는 클래스 당 리콜 점수의 거시 평균 또는 각 클래스가 실제 클래스의 역 유병률에 따라 가중치를 적용하는 원시 정확도입니다. 따라서 균형 잡힌 데이터 세트의 경우 점수는 정확도와 같습니다.</target>
        </trans-unit>
        <trans-unit id="6c29a08df4ccee7316d3d3b84f8a1be99122d005" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt;&lt;code&gt;brier_score_loss&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Brier_score&quot;&gt;Brier score&lt;/a&gt; for binary classes. Quoting Wikipedia:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt; &lt;code&gt;brier_score_loss&lt;/code&gt; &lt;/a&gt; 기능은 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Brier_score&quot;&gt;찔레 점수&lt;/a&gt; 바이너리 클래스를. 인용 위키 백과 :</target>
        </trans-unit>
        <trans-unit id="80ef899573ebb3be6112621f7df5aadf4a0d99d9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.classification_report#sklearn.metrics.classification_report&quot;&gt;&lt;code&gt;classification_report&lt;/code&gt;&lt;/a&gt; function builds a text report showing the main classification metrics. Here is a small example with custom &lt;code&gt;target_names&lt;/code&gt; and inferred labels:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.classification_report#sklearn.metrics.classification_report&quot;&gt; &lt;code&gt;classification_report&lt;/code&gt; 의&lt;/a&gt; 기능은 주요 분류 통계를 보여주는 텍스트 보고서를 작성합니다. 다음은 맞춤 &lt;code&gt;target_names&lt;/code&gt; 및 유추 된 레이블 이 포함 된 작은 예입니다 .</target>
        </trans-unit>
        <trans-unit id="646495d784f725b3203da7b1895753c47a46c957" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.confusion_matrix#sklearn.metrics.confusion_matrix&quot;&gt;&lt;code&gt;confusion_matrix&lt;/code&gt;&lt;/a&gt; function evaluates classification accuracy by computing the confusion matrix with each row corresponding to the true class &amp;lt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Confusion_matrix&quot;&gt;https://en.wikipedia.org/wiki/Confusion_matrix&lt;/a&gt;&amp;gt;`_. (Wikipedia and other references may use different convention for axes.)</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.confusion_matrix#sklearn.metrics.confusion_matrix&quot;&gt; &lt;code&gt;confusion_matrix&lt;/code&gt; 의&lt;/a&gt; 기능은 각 행 &amp;lt;실제 클래스에 대응 혼란 매트릭스 연산에 의한 분류의 정확도를 평가 &lt;a href=&quot;https://en.wikipedia.org/wiki/Confusion_matrix&quot;&gt;https://en.wikipedia.org/wiki/Confusion_matrix&lt;/a&gt; &amp;gt;`_한다. (Wikipedia 및 기타 참조는 축에 대해 다른 규칙을 사용할 수 있습니다.)</target>
        </trans-unit>
        <trans-unit id="627c762ce2611d603b6cf9dd93706bacfe9a64ab" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.coverage_error#sklearn.metrics.coverage_error&quot;&gt;&lt;code&gt;coverage_error&lt;/code&gt;&lt;/a&gt; function computes the average number of labels that have to be included in the final prediction such that all true labels are predicted. This is useful if you want to know how many top-scored-labels you have to predict in average without missing any true one. The best value of this metrics is thus the average number of true labels.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.coverage_error#sklearn.metrics.coverage_error&quot;&gt; &lt;code&gt;coverage_error&lt;/code&gt; 의&lt;/a&gt; 기능은 모든 사실 레이블이 예측되도록 최종 예측에 포함되어야하는 라벨의 평균 수를 계산합니다. 이것은 실제 레이블을 잃지 않고 평균적으로 얼마나 많은 최상위 레이블을 예측해야하는지 알고 싶을 때 유용합니다. 따라서이 측정 항목의 최상의 가치는 평균 실제 라벨 수입니다.</target>
        </trans-unit>
        <trans-unit id="365c5eb64f0dc2e33be205a551210f568e81546d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Explained_variation&quot;&gt;explained variance regression score&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; 는&lt;/a&gt; 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Explained_variation&quot;&gt;설명 분산 회귀 점수를&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="16accfb21d784810c328541c85b1894b818cde8e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.hamming_loss#sklearn.metrics.hamming_loss&quot;&gt;&lt;code&gt;hamming_loss&lt;/code&gt;&lt;/a&gt; computes the average Hamming loss or &lt;a href=&quot;https://en.wikipedia.org/wiki/Hamming_distance&quot;&gt;Hamming distance&lt;/a&gt; between two sets of samples.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.hamming_loss#sklearn.metrics.hamming_loss&quot;&gt; &lt;code&gt;hamming_loss&lt;/code&gt; 는&lt;/a&gt; 평균 해밍 손실 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Hamming_distance&quot;&gt;해밍 거리가&lt;/a&gt; 샘플들의 두 세트 사이에있다.</target>
        </trans-unit>
        <trans-unit id="6d1238c9791f472ba1850e50c0898d87bebfa2d3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; function computes the average distance between the model and the data using &lt;a href=&quot;https://en.wikipedia.org/wiki/Hinge_loss&quot;&gt;hinge loss&lt;/a&gt;, a one-sided metric that considers only prediction errors. (Hinge loss is used in maximal margin classifiers such as support vector machines.)</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt; 함수는 모델 및 사용 데이터 사이의 평균 거리 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Hinge_loss&quot;&gt;힌지 손실&lt;/a&gt; 하는 경우에만 예측 에러를 고려한 해당 측정 한 양면. 힌지 손실은 서포트 벡터 머신과 같은 최대 마진 분류기에서 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="29930da8eb2c0b1ff7129cc1cbfbb0416883031e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.jaccard_similarity_score#sklearn.metrics.jaccard_similarity_score&quot;&gt;&lt;code&gt;jaccard_similarity_score&lt;/code&gt;&lt;/a&gt; function computes the average (default) or sum of &lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;Jaccard similarity coefficients&lt;/a&gt;, also called the Jaccard index, between pairs of label sets.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.jaccard_similarity_score#sklearn.metrics.jaccard_similarity_score&quot;&gt; &lt;code&gt;jaccard_similarity_score&lt;/code&gt; 의&lt;/a&gt; 함수의 평균 (기본) 또는 합 연산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;인 Jaccard 유사도 계수&lt;/a&gt; 라벨 세트들의 쌍들 사이에는 인 Jaccard 인덱스라고 불리는,.</target>
        </trans-unit>
        <trans-unit id="cbf6f35f94b93c090ec2e48a35d245f91dcfca65" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.label_ranking_average_precision_score#sklearn.metrics.label_ranking_average_precision_score&quot;&gt;&lt;code&gt;label_ranking_average_precision_score&lt;/code&gt;&lt;/a&gt; function implements label ranking average precision (LRAP). This metric is linked to the &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt; function, but is based on the notion of label ranking instead of precision and recall.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.label_ranking_average_precision_score#sklearn.metrics.label_ranking_average_precision_score&quot;&gt; &lt;code&gt;label_ranking_average_precision_score&lt;/code&gt; 의&lt;/a&gt; 기능 구현은 순위 평균 정밀도 (LRAP를) 레이블을 붙입니다. 이 메트릭은 &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt; &lt;code&gt;average_precision_score&lt;/code&gt; &lt;/a&gt; 함수에 연결되어 있지만 정밀도 및 호출 대신 레이블 순위의 개념을 기반으로합니다.</target>
        </trans-unit>
        <trans-unit id="79620a85c10a9be19922ad33cc7395bed79c9e4e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.label_ranking_loss#sklearn.metrics.label_ranking_loss&quot;&gt;&lt;code&gt;label_ranking_loss&lt;/code&gt;&lt;/a&gt; function computes the ranking loss which averages over the samples the number of label pairs that are incorrectly ordered, i.e. true labels have a lower score than false labels, weighted by the inverse of the number of ordered pairs of false and true labels. The lowest achievable ranking loss is zero.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.label_ranking_loss#sklearn.metrics.label_ranking_loss&quot;&gt; &lt;code&gt;label_ranking_loss&lt;/code&gt; &lt;/a&gt; 기능은 거짓과 진실 라벨의 순서쌍의 숫자의 역으로 가중 샘플에 걸쳐 평균, 즉 잘못 정렬 레이블 쌍의 수, 진정한 라벨이 거짓 라벨보다 낮은 점수가 순위 손실을 계산합니다. 달성 가능한 최저 순위 손실은 0입니다.</target>
        </trans-unit>
        <trans-unit id="4b0810d3cef1ca02b990e21053d774c24a0e430b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.log_loss#sklearn.metrics.log_loss&quot;&gt;&lt;code&gt;log_loss&lt;/code&gt;&lt;/a&gt; function computes log loss given a list of ground-truth labels and a probability matrix, as returned by an estimator&amp;rsquo;s &lt;code&gt;predict_proba&lt;/code&gt; method.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.log_loss#sklearn.metrics.log_loss&quot;&gt; &lt;code&gt;log_loss&lt;/code&gt; 의&lt;/a&gt; 함수로 계산은의 추정기에 의해 반환 손실은 지상 진실 라벨 확률 매트릭스의리스트를 소정 로그온 &lt;code&gt;predict_proba&lt;/code&gt; 의 방법.</target>
        </trans-unit>
        <trans-unit id="3661b0b19cd7cbd747b2bf1ce7b4a383102a7abe" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt;&lt;code&gt;matthews_corrcoef&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Matthews_correlation_coefficient&quot;&gt;Matthew&amp;rsquo;s correlation coefficient (MCC)&lt;/a&gt; for binary classes. Quoting Wikipedia:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt; &lt;code&gt;matthews_corrcoef&lt;/code&gt; 의&lt;/a&gt; 함수 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Matthews_correlation_coefficient&quot;&gt;매튜의 상관 계수 (MCC)를&lt;/a&gt; 이진 클래스. 인용 위키 백과 :</target>
        </trans-unit>
        <trans-unit id="8af6e2db7d7843522a3d6755e0b8d1e9cbd1e8f1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt;&lt;code&gt;mean_absolute_error&lt;/code&gt;&lt;/a&gt; function computes &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;&gt;mean absolute error&lt;/a&gt;, a risk metric corresponding to the expected value of the absolute error loss or \(l1\)-norm loss.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt; &lt;code&gt;mean_absolute_error&lt;/code&gt; 의&lt;/a&gt; 기능은 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;&gt;평균 절대 오차&lt;/a&gt; 규범 손실 - 절대 에러 또는 손실 \ (L1 \)의 기대 값에 대응하는 메트릭 위험.</target>
        </trans-unit>
        <trans-unit id="798074cb4600d0906a9ad4975942309f6067f034" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;mean_squared_error&lt;/code&gt;&lt;/a&gt; function computes &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;&gt;mean square error&lt;/a&gt;, a risk metric corresponding to the expected value of the squared (quadratic) error or loss.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;mean_squared_error&lt;/code&gt; 의&lt;/a&gt; 기능은 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;&gt;평균 제곱 에러&lt;/a&gt; 의 제곱 (이차) 에러 또는 손실의 기대 값에 대응하는 메트릭 위험.</target>
        </trans-unit>
        <trans-unit id="e4bbccf6d12a691e935e91e2611be809f1bb4329" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt;&lt;code&gt;mean_squared_log_error&lt;/code&gt;&lt;/a&gt; function computes a risk metric corresponding to the expected value of the squared logarithmic (quadratic) error or loss.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt; &lt;code&gt;mean_squared_log_error&lt;/code&gt; 의&lt;/a&gt; 함수 제곱 대수 (이차) 에러 또는 손실의 기대 값에 대응하는 위험 메트릭 계산한다.</target>
        </trans-unit>
        <trans-unit id="39f48c0bbd67ae6ad01f06368c176486b0da9e3b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt;&lt;code&gt;median_absolute_error&lt;/code&gt;&lt;/a&gt; does not support multioutput.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt; &lt;code&gt;median_absolute_error&lt;/code&gt; 는&lt;/a&gt; 다중 출력을 지원하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="4c03eab2aa446025510f65f3a7e796a70c97a820" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt;&lt;code&gt;median_absolute_error&lt;/code&gt;&lt;/a&gt; is particularly interesting because it is robust to outliers. The loss is calculated by taking the median of all absolute differences between the target and the prediction.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt; &lt;code&gt;median_absolute_error&lt;/code&gt; 는&lt;/a&gt; 이 이상치에 대한 강력한 있기 때문에 특히 흥미 롭다. 손실은 목표와 예측 사이의 모든 절대 차이의 중앙값을 취하여 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="9f5631acff2238ea5bcb79376fef6d2e143756a6" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.precision_recall_curve#sklearn.metrics.precision_recall_curve&quot;&gt;&lt;code&gt;precision_recall_curve&lt;/code&gt;&lt;/a&gt; computes a precision-recall curve from the ground truth label and a score given by the classifier by varying a decision threshold.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.precision_recall_curve#sklearn.metrics.precision_recall_curve&quot;&gt; &lt;code&gt;precision_recall_curve&lt;/code&gt; 는&lt;/a&gt; 지상 진실 라벨에서 정밀 리콜 곡선과 결정 임계 값을 변화시킴으로써 분류에 의해 주어진 점수를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="d16f1c84f45a895677960253a6c6c45cf8b671e5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; accept an additional value &lt;code&gt;'variance_weighted'&lt;/code&gt; for the &lt;code&gt;multioutput&lt;/code&gt; parameter. This option leads to a weighting of each individual score by the variance of the corresponding target variable. This setting quantifies the globally captured unscaled variance. If the target variables are of different scale, then this score puts more importance on well explaining the higher variance variables. &lt;code&gt;multioutput='variance_weighted'&lt;/code&gt; is the default value for &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; for backward compatibility. This will be changed to &lt;code&gt;uniform_average&lt;/code&gt; in the future.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; 은&lt;/a&gt; 추가로 값을 받아 &lt;code&gt;'variance_weighted'&lt;/code&gt; 에 대한 &lt;code&gt;multioutput&lt;/code&gt; 매개 변수를. 이 옵션을 사용하면 해당 대상 변수의 분산에 따라 각 개별 점수의 가중치가 적용됩니다. 이 설정은 전체적으로 캡처 된 비 스케일 분산을 정량화합니다. 목표 변수가 다른 척도 인 경우이 점수는 높은 분산 변수를 잘 설명하는 데 더 중요합니다. 이전 버전과의 호환성을 위해 &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt; 의 기본값은 multioutput &lt;code&gt;multioutput='variance_weighted'&lt;/code&gt; 입니다 . 앞으로는 &lt;code&gt;uniform_average&lt;/code&gt; 로 변경 될 것입니다.</target>
        </trans-unit>
        <trans-unit id="093b69e0a2a3ccbb33c0abc5ad459d307bcf6553" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; function computes R&amp;sup2;, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Coefficient_of_determination&quot;&gt;coefficient of determination&lt;/a&gt;. It provides a measure of how well future samples are likely to be predicted by the model. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; 의&lt;/a&gt; 함수 R&amp;sup2; 상기 계산 &lt;a href=&quot;https://en.wikipedia.org/wiki/Coefficient_of_determination&quot;&gt;판정 계수&lt;/a&gt; . 모델이 미래의 샘플을 얼마나 잘 예측할 수 있는지 측정합니다. 최고 점수는 1.0이며 음수가 될 수 있습니다 (모델이 임의로 악화 될 수 있기 때문에). 입력 특성을 무시하고 항상 y의 예상 값을 예측하는 상수 모델은 0.0의 R ^ 2 점수를 얻습니다.</target>
        </trans-unit>
        <trans-unit id="6cdf15299db25ef1f3b3628cdbe3b0de1b0d0ab3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt; function computes the area under the receiver operating characteristic (ROC) curve, which is also denoted by AUC or AUROC. By computing the area under the roc curve, the curve information is summarized in one number. For more information see the &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve&quot;&gt;Wikipedia article on AUC&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; 의&lt;/a&gt; 기능은 AUC 또는 AUROC로 표시되는 수신기 동작 특성 (ROC) 곡선 아래의 면적을 계산한다. roc 곡선 아래의 면적을 계산함으로써, 곡선 정보는 하나의 숫자로 요약됩니다. 자세한 내용은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve&quot;&gt;AUC Wikipedia 기사를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="fbc2259a8dc85fa7ed24780d0f0e2ac678fbcad9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt;&lt;code&gt;zero_one_loss&lt;/code&gt;&lt;/a&gt; function computes the sum or the average of the 0-1 classification loss (\(L_{0-1}\)) over \(n_{\text{samples}}\). By default, the function normalizes over the sample. To get the sum of the \(L_{0-1}\), set &lt;code&gt;normalize&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt; &lt;code&gt;zero_one_loss&lt;/code&gt; &lt;/a&gt; 함수 합 또는 0-1 분류 손실 (\ (L_ {0-1} \)) 위에 \ (N _ {\ 텍스트 샘플 {}} \)의 평균을 계산한다. 기본적으로 함수는 샘플에서 정규화됩니다. \ (L_ {0-1} \)의 합계를 얻으려면 &lt;code&gt;normalize&lt;/code&gt; 를 &lt;code&gt;False&lt;/code&gt; 로 설정하십시오 .</target>
        </trans-unit>
        <trans-unit id="0ce78ef531a28f4df4b9620cf7454901e00bf965" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt;&lt;code&gt;BayesianGaussianMixture&lt;/code&gt;&lt;/a&gt; object implements a variant of the Gaussian mixture model with variational inference algorithms. The API is similar as the one defined by &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt;&lt;code&gt;GaussianMixture&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt; &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; 의&lt;/a&gt; 객체 구현 변분 추론 알고리즘 가우시안 혼합 모델의 변형. API는 &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt; &lt;code&gt;GaussianMixture&lt;/code&gt; 에&lt;/a&gt; 의해 정의 된 것과 유사합니다 .</target>
        </trans-unit>
        <trans-unit id="7755b185d3bd9567bc77a31c3d84e8be2c332f90" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt;&lt;code&gt;GaussianMixture&lt;/code&gt;&lt;/a&gt; comes with different options to constrain the covariance of the difference classes estimated: spherical, diagonal, tied or full covariance.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt; &lt;code&gt;GaussianMixture&lt;/code&gt; 는&lt;/a&gt; 차이 클래스의 공분산을 제한하는 다양한 옵션을 제공하는 것은 추정 : 구형 대각선, 연결 또는 전체 공분산을.</target>
        </trans-unit>
        <trans-unit id="9cb1ef419e28ff804b54eef9fc18747641996ac9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt;&lt;code&gt;GaussianMixture&lt;/code&gt;&lt;/a&gt; object implements the &lt;a href=&quot;#expectation-maximization&quot;&gt;expectation-maximization&lt;/a&gt; (EM) algorithm for fitting mixture-of-Gaussian models. It can also draw confidence ellipsoids for multivariate models, and compute the Bayesian Information Criterion to assess the number of clusters in the data. A &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture.fit&quot;&gt;&lt;code&gt;GaussianMixture.fit&lt;/code&gt;&lt;/a&gt; method is provided that learns a Gaussian Mixture Model from train data. Given test data, it can assign to each sample the Gaussian it mostly probably belong to using the &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture.predict&quot;&gt;&lt;code&gt;GaussianMixture.predict&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt; &lt;code&gt;GaussianMixture&lt;/code&gt; &lt;/a&gt; 객체 는 가우스 혼합 모델에 적합하도록 EM ( &lt;a href=&quot;#expectation-maximization&quot;&gt;expectation-maximization&lt;/a&gt; ) 알고리즘을 구현합니다 . 또한 다변량 모델에 대한 신뢰 타원체를 그리고 베이지안 정보 기준을 계산하여 데이터의 군집 수를 평가할 수 있습니다. &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture.fit&quot;&gt; &lt;code&gt;GaussianMixture.fit&lt;/code&gt; 에&lt;/a&gt; 있어서 열차 데이터 배운다 가우시안 혼합 모델에 제공된다. 테스트 데이터가 주어지면 &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture.predict&quot;&gt; &lt;code&gt;GaussianMixture.predict&lt;/code&gt; &lt;/a&gt; 메소드 를 사용하여 주로 속하는 가우스를 각 샘플에 할당 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="0de55d1a8cabbad74e59064d298db364a4949211" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; instance implements the usual estimator API: when &amp;ldquo;fitting&amp;rdquo; it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; 의&lt;/a&gt; 인스턴스를 구현 일반적인 추정의 API : 데이터 세트에 그것을 &quot;피팅&quot;모든 매개 변수 값의 가능한 조합을 평가하고 최적의 조합은 유지됩니다.</target>
        </trans-unit>
        <trans-unit id="45a8a9b77f54ed1a1a49e1eebdf058e73c1ad33a" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit&quot;&gt;&lt;code&gt;GroupShuffleSplit&lt;/code&gt;&lt;/a&gt; iterator behaves as a combination of &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt;&lt;code&gt;ShuffleSplit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt;&lt;code&gt;LeavePGroupsOut&lt;/code&gt;&lt;/a&gt;, and generates a sequence of randomized partitions in which a subset of groups are held out for each split.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit&quot;&gt; &lt;code&gt;GroupShuffleSplit&lt;/code&gt; &lt;/a&gt; 반복자의 조합으로서 동작 &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt; &lt;code&gt;ShuffleSplit&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt; &lt;code&gt;LeavePGroupsOut&lt;/code&gt; &lt;/a&gt; 및 그룹들의 서브 세트가 각각의 분할에 대해 내밀하는 랜덤 분할의 시퀀스를 생성한다.</target>
        </trans-unit>
        <trans-unit id="ec15ef26f9a075815cd5139e68468c23802a4936" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt;&lt;code&gt;ShuffleSplit&lt;/code&gt;&lt;/a&gt; iterator will generate a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt; &lt;code&gt;ShuffleSplit&lt;/code&gt; 는&lt;/a&gt; 반복자는 독립적 인 기차 / 테스트 데이터 세트 분할의 사용자 정의 번호를 생성합니다. 샘플을 먼저 섞은 다음 기차와 테스트 세트로 나눕니다.</target>
        </trans-unit>
        <trans-unit id="9d19931407bdaf26c2091f0ff552c9705b61d84b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; (LOF) algorithm computes a score (called local outlier factor) reflecting the degree of abnormality of the observations. It measures the local density deviation of a given data point with respect to its neighbors. The idea is to detect the samples that have a substantially lower density than their neighbors.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt; (LOF) 알고리즘은 관찰의 이상 정도를 반영 (현지 특이 요인이라고 함) 점수를 계산합니다. 주변에 대한 주어진 데이터 포인트의 로컬 밀도 편차를 측정합니다. 아이디어는 이웃보다 밀도가 실질적으로 낮은 샘플을 탐지하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="7ee66eef276fb6deecd16d4b6508f3c8417f58ed" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt;&lt;code&gt;NearestCentroid&lt;/code&gt;&lt;/a&gt; classifier has a &lt;code&gt;shrink_threshold&lt;/code&gt; parameter, which implements the nearest shrunken centroid classifier. In effect, the value of each feature for each centroid is divided by the within-class variance of that feature. The feature values are then reduced by &lt;code&gt;shrink_threshold&lt;/code&gt;. Most notably, if a particular feature value crosses zero, it is set to zero. In effect, this removes the feature from affecting the classification. This is useful, for example, for removing noisy features.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt; &lt;code&gt;NearestCentroid&lt;/code&gt; &lt;/a&gt; 분류는이 &lt;code&gt;shrink_threshold&lt;/code&gt; 하는 구현하는 가장 가까운 수축 중심 분류 매개 변수를. 실제로 각 중심에 대한 각 지형지 물의 값은 해당 지형지 물의 클래스 내 분산으로 나뉩니다. 그런 다음 기능 값은 &lt;code&gt;shrink_threshold&lt;/code&gt; 만큼 줄어 듭니다 . 특히, 특정 기능 값이 0을 초과하면 0으로 설정됩니다. 실제로, 이는 분류에 영향을주는 기능을 제거합니다. 예를 들어 노이즈 기능을 제거하는 데 유용합니다.</target>
        </trans-unit>
        <trans-unit id="43771e48f74cd166aa989881024956f81686fd39" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt;&lt;code&gt;NearestCentroid&lt;/code&gt;&lt;/a&gt; classifier is a simple algorithm that represents each class by the centroid of its members. In effect, this makes it similar to the label updating phase of the &lt;code&gt;sklearn.KMeans&lt;/code&gt; algorithm. It also has no parameters to choose, making it a good baseline classifier. It does, however, suffer on non-convex classes, as well as when classes have drastically different variances, as equal variance in all dimensions is assumed. See Linear Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;sklearn.discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) and Quadratic Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt;&lt;code&gt;sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) for more complex methods that do not make this assumption. Usage of the default &lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt;&lt;code&gt;NearestCentroid&lt;/code&gt;&lt;/a&gt; is simple:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt; &lt;code&gt;NearestCentroid&lt;/code&gt; &lt;/a&gt; 분류기는 부재의 중심으로 각각의 클래스를 나타내는 간단한 알고리즘이다. 실제로 이것은 &lt;code&gt;sklearn.KMeans&lt;/code&gt; 알고리즘 의 레이블 업데이트 단계와 유사 합니다. 또한 선택할 매개 변수가 없으므로 훌륭한 기준 분류 기준이됩니다. 그러나 볼록하지 않은 클래스뿐만 아니라 모든 차원에서 동일한 분산이 가정되므로 클래스가 크게 다른 분산을 갖는 경우에는 문제가 발생합니다. 이 가정을하지 않는보다 복잡한 방법 은 선형 판별 분석 ( &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt; &lt;code&gt;sklearn.discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt; ) 및 2 차 판별 분석 ( &lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt; &lt;code&gt;sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt; )을 참조하십시오. 기본 &lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt; &lt;code&gt;NearestCentroid&lt;/code&gt; 사용&lt;/a&gt; 간단하다 :</target>
        </trans-unit>
        <trans-unit id="39bfe25102ea45948a285842eddb73a441e962a3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; is built using a list of &lt;code&gt;(key, value)&lt;/code&gt; pairs, where the &lt;code&gt;key&lt;/code&gt; is a string containing the name you want to give this step and &lt;code&gt;value&lt;/code&gt; is an estimator object:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt; 목록을 사용하여 구축 &lt;code&gt;(key, value)&lt;/code&gt; 쌍, &lt;code&gt;key&lt;/code&gt; 이 단계를주고 싶어하고 이름이 포함 된 문자열 &lt;code&gt;value&lt;/code&gt; 추정량 개체입니다 :</target>
        </trans-unit>
        <trans-unit id="c923ad3a865326ec6c72af48e5305bcc89674896" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.random_projection.gaussianrandomprojection#sklearn.random_projection.GaussianRandomProjection&quot;&gt;&lt;code&gt;sklearn.random_projection.GaussianRandomProjection&lt;/code&gt;&lt;/a&gt; reduces the dimensionality by projecting the original input space on a randomly generated matrix where components are drawn from the following distribution \(N(0, \frac{1}{n_{components}})\).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.random_projection.gaussianrandomprojection#sklearn.random_projection.GaussianRandomProjection&quot;&gt; &lt;code&gt;sklearn.random_projection.GaussianRandomProjection&lt;/code&gt; 이&lt;/a&gt; 성분은 다음 분배 \에서 도출되는 랜덤하게 생성 행렬에 원래의 입력 공간 (N (0, \ FRAC {1} {N_ {성분}}) \) 돌출하여 차원을 줄인다.</target>
        </trans-unit>
        <trans-unit id="bf402b0deb2f6872588357c0d1a4ee6663793993" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.random_projection.sparserandomprojection#sklearn.random_projection.SparseRandomProjection&quot;&gt;&lt;code&gt;sklearn.random_projection.SparseRandomProjection&lt;/code&gt;&lt;/a&gt; reduces the dimensionality by projecting the original input space using a sparse random matrix.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.random_projection.sparserandomprojection#sklearn.random_projection.SparseRandomProjection&quot;&gt; &lt;code&gt;sklearn.random_projection.SparseRandomProjection&lt;/code&gt; 는&lt;/a&gt; 희소 랜덤 행렬을 이용하여 원래의 입력 공간으로 돌출하여 차원을 줄인다.</target>
        </trans-unit>
        <trans-unit id="bfc6d74a43acae56b2f26724a2d5ae8338eaf262" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.tree.export_graphviz#sklearn.tree.export_graphviz&quot;&gt;&lt;code&gt;export_graphviz&lt;/code&gt;&lt;/a&gt; exporter also supports a variety of aesthetic options, including coloring nodes by their class (or value for regression) and using explicit variable and class names if desired. Jupyter notebooks also render these plots inline automatically:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.tree.export_graphviz#sklearn.tree.export_graphviz&quot;&gt; &lt;code&gt;export_graphviz&lt;/code&gt; 의&lt;/a&gt; 수출 또한 클래스 노드를 착색 (또는 값 회귀를위한) 원하는 경우 명시 적 변수와 클래스 이름을 사용하여 포함 미적 옵션의 다양한 지원합니다. Jupyter 노트북은 이러한 플롯을 자동으로 인라인으로 렌더링합니다.</target>
        </trans-unit>
        <trans-unit id="4a15d0b16a3ea1ca7d8280fbccb678c643c12770" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;https://en.wikipedia.org/wiki/F1_score&quot;&gt;F-measure&lt;/a&gt; (\(F_\beta\) and \(F_1\) measures) can be interpreted as a weighted harmonic mean of the precision and recall. A \(F_\beta\) measure reaches its best value at 1 and its worst score at 0. With \(\beta = 1\), \(F_\beta\) and \(F_1\) are equivalent, and the recall and the precision are equally important.</source>
          <target state="translated">&lt;a href=&quot;https://en.wikipedia.org/wiki/F1_score&quot;&gt;F-계수&lt;/a&gt; (\ (F_ \ 베타 \)와 \ (F_1 \) 방법)은 정밀 리콜의 가중 조화 평균으로서 해석 될 수있다. \ (F_ \ beta \) 측정 값은 1에서 가장 높은 값에 도달하고 최악의 점수는 0에 도달합니다. 리콜과 정밀도도 똑같이 중요합니다.</target>
        </trans-unit>
        <trans-unit id="88cf2fa597f50207550c56a5d725499fc42ad462" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;Johnson-Lindenstrauss lemma&lt;/a&gt; states that any high dimensional dataset can be randomly projected into a lower dimensional Euclidean space while controlling the distortion in the pairwise distances.</source>
          <target state="translated">&lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;존슨 Lindenstrauss 표제어는&lt;/a&gt; 페어 와이즈 거리의 왜곡을 조절하면서 모든 고차원 세트 임의로 낮은 차원 유클리드 공간에 투영 될 수 있다는 것을 말한다.</target>
        </trans-unit>
        <trans-unit id="96678c00449216bcbe65a0961a8b25b8baf7a396" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; class can adapt its number of mixture components automatically. The parameter &lt;code&gt;weight_concentration_prior&lt;/code&gt; has a direct link with the resulting number of components with non-zero weights. Specifying a low value for the concentration prior will make the model put most of the weight on few components set the remaining components weights very close to zero. High values of the concentration prior will allow a larger number of components to be active in the mixture.</source>
          <target state="translated">&lt;code&gt;BayesianGaussianMixture&lt;/code&gt; 의 클래스는 자동으로 혼합 구성 요소의 수를 적용 할 수 있습니다. &lt;code&gt;weight_concentration_prior&lt;/code&gt; 매개 변수 는 가중치가 0이 아닌 결과 구성 요소 수와 직접 링크됩니다. 이전 농도에 대해 낮은 값을 지정하면 모델이 소수의 구성 요소에 대부분의 가중치를 적용하여 나머지 구성 요소 가중치를 0에 매우 가깝게 설정합니다. 이전 농도의 높은 값은 혼합물에서 더 많은 성분이 활성화 될 수있게한다.</target>
        </trans-unit>
        <trans-unit id="c17c439e95320993d0276d174b035cd14b7ce3b3" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;C&lt;/code&gt; parameter controls the amount of regularization in the &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; object: a large value for &lt;code&gt;C&lt;/code&gt; results in less regularization. &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; gives &lt;a href=&quot;#shrinkage&quot;&gt;Shrinkage&lt;/a&gt; (i.e. non-sparse coefficients), while &lt;code&gt;penalty=&quot;l1&quot;&lt;/code&gt; gives &lt;a href=&quot;#sparsity&quot;&gt;Sparsity&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;C&lt;/code&gt; 의 파라미터를 제어에서 정규화 양 &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; 목적 :위한 큰 값 &lt;code&gt;C&lt;/code&gt; 를 적게 정규화 결과. &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; 는 &lt;a href=&quot;#shrinkage&quot;&gt;수축&lt;/a&gt; (즉, 희소 계수)을 제공하고 &lt;code&gt;penalty=&quot;l1&quot;&lt;/code&gt; 은 &lt;a href=&quot;#sparsity&quot;&gt;희소성을&lt;/a&gt; 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="9164d9a9144eaecf5fe284f2e40277e82f0b8068" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;C&lt;/code&gt; parameter trades off correct classification of training examples against maximization of the decision function&amp;rsquo;s margin. For larger values of &lt;code&gt;C&lt;/code&gt;, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. A lower &lt;code&gt;C&lt;/code&gt; will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy. In other words``C`` behaves as a regularization parameter in the SVM.</source>
          <target state="translated">&lt;code&gt;C&lt;/code&gt; 의 결정 함수의 수익성의 극대화에 대한 사례를 교육의 올바른 분류 오프 파라미터 거래. &lt;code&gt;C&lt;/code&gt; 값이 클수록 결정 기능이 모든 교육 지점을 올바르게 분류하는 것이 더 좋으면 더 작은 마진이 허용됩니다. &lt;code&gt;C&lt;/code&gt; 가 낮을수록 훈련 정확도를 높이 면서 더 큰 마진을 가지므로 더 간단한 결정 기능을 얻을 수 있습니다. 즉,``C ''는 SVM에서 정규화 매개 변수로 작동합니다.</target>
        </trans-unit>
        <trans-unit id="9aa2de3f6ced8022ed53d959fe2e18d24e70ecf1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;DESCR&lt;/code&gt; contains a free-text description of the data, while &lt;code&gt;details&lt;/code&gt; contains a dictionary of meta-data stored by openml, like the dataset id. For more details, see the &lt;a href=&quot;https://docs.openml.org/#data&quot;&gt;OpenML documentation&lt;/a&gt; The &lt;code&gt;data_id&lt;/code&gt; of the mice protein dataset is 40966, and you can use this (or the name) to get more information on the dataset on the openml website:</source>
          <target state="translated">&lt;code&gt;DESCR&lt;/code&gt; 는 동안, 데이터의 자유 텍스트 설명을 포함 &lt;code&gt;details&lt;/code&gt; 데이터 세트 ID처럼 openml에 저장된 메타 데이터의 사전을 포함하고 있습니다. 자세한 내용은 참조 &lt;a href=&quot;https://docs.openml.org/#data&quot;&gt;OpenML 문서 &lt;/a&gt; &lt;code&gt;data_id&lt;/code&gt; 쥐의 단백질 데이터 세트는 40966입니다, 당신은 openml 웹 사이트의 데이터 세트에 대한 자세한 정보를 얻을이 (또는 이름)를 사용할 수 있습니다 :</target>
        </trans-unit>
        <trans-unit id="81a0037eca65e4ed69e2a49ca4871b0ce138bc10" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;Normalizer&lt;/code&gt; rescales the vector for each sample to have unit norm, independently of the distribution of the samples. It can be seen on both figures below where all samples are mapped onto the unit circle. In our example the two selected features have only positive values; therefore the transformed data only lie in the positive quadrant. This would not be the case if some original features had a mix of positive and negative values.</source>
          <target state="translated">&lt;code&gt;Normalizer&lt;/code&gt; 독립적 샘플의 분포의 각각의 샘플을 단위 놈을위한 벡터를 다시 조정. 아래의 두 그림에서 모든 샘플이 단위 원에 매핑되어 있습니다. 이 예에서 선택한 두 피처는 양수 값만 갖습니다. 따라서 변환 된 데이터는 양의 사분면에만 있습니다. 일부 원래 기능에 양수 값과 음수 값이 혼합되어있는 경우에는 그렇지 않습니다.</target>
        </trans-unit>
        <trans-unit id="c108938c180fba7834742cb26f12054acc7a2184" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;PCA&lt;/code&gt; fitting is only computed at the evaluation of the first configuration of the &lt;code&gt;C&lt;/code&gt; parameter of the &lt;code&gt;LinearSVC&lt;/code&gt; classifier. The other configurations of &lt;code&gt;C&lt;/code&gt; will trigger the loading of the cached &lt;code&gt;PCA&lt;/code&gt; estimator data, leading to save processing time. Therefore, the use of caching the pipeline using &lt;code&gt;memory&lt;/code&gt; is highly beneficial when fitting a transformer is costly.</source>
          <target state="translated">&lt;code&gt;PCA&lt;/code&gt; 의 결합 만의 제 1 구성의 평가에서 계산되는 &lt;code&gt;C&lt;/code&gt; 에서 의 파라미터 &lt;code&gt;LinearSVC&lt;/code&gt; 의 분류. &lt;code&gt;C&lt;/code&gt; 의 다른 구성은 캐시 된 &lt;code&gt;PCA&lt;/code&gt; 추정기 데이터 의로드를 트리거하여 처리 시간을 절약합니다. 따라서 &lt;code&gt;memory&lt;/code&gt; 사용하여 파이프 라인 캐싱을 사용 하면 변압기를 장착하는 데 많은 비용이 듭니다.</target>
        </trans-unit>
        <trans-unit id="17baab07595bc9a1b9010bf52fc5ebb7c1555943" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;RandomForestClassifier&lt;/code&gt; is trained using &lt;em&gt;bootstrap aggregation&lt;/em&gt;, where each new tree is fit from a bootstrap sample of the training observations \(z_i = (x_i, y_i)\). The &lt;em&gt;out-of-bag&lt;/em&gt; (OOB) error is the average error for each \(z_i\) calculated using predictions from the trees that do not contain \(z_i\) in their respective bootstrap sample. This allows the &lt;code&gt;RandomForestClassifier&lt;/code&gt; to be fit and validated whilst being trained &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;RandomForestClassifier&lt;/code&gt; 을 사용하여 훈련 &lt;em&gt;부트 스트랩 집계&lt;/em&gt; 각각의 새로운 나무 훈련 관측 \ (z_i = (x_i로부터, y_i) \)의 부트 스트랩 샘플에서 적합합니다. &lt;em&gt;밖으로의 가방&lt;/em&gt; (OOB) 오류가 각의 평균 오류입니다 \ (z_i \)가 포함되지 않은 나무에서 예측을 사용하여 계산 \ (z_i \) 각각의 부트 스트랩 샘플입니다. 이를 통해 &lt;code&gt;RandomForestClassifier&lt;/code&gt; 를 학습 하는 동안 적합하고 검증 할 수 있습니다 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="093b9af7ff877738a1c191bf8fe58f666ce1b93c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;VotingClassifier&lt;/code&gt; can also be used together with &lt;code&gt;GridSearch&lt;/code&gt; in order to tune the hyperparameters of the individual estimators:</source>
          <target state="translated">&lt;code&gt;VotingClassifier&lt;/code&gt; 은 또한 함께 사용될 수있다 &lt;code&gt;GridSearch&lt;/code&gt; 개별 추정기의 하이퍼 파라미터 조정 순서 :</target>
        </trans-unit>
        <trans-unit id="a92f24d7703ca3728952e82f17755a0b7604fe2c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;alpha&lt;/code&gt; parameter controls the degree of sparsity of the coefficients estimated.</source>
          <target state="translated">&lt;code&gt;alpha&lt;/code&gt; 매개 변수 추정 계수의 희박의 정도를 제어한다.</target>
        </trans-unit>
        <trans-unit id="d22b7c9a5ce685adf8551f77a733c13cab8fe2d4" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;best_estimator_&lt;/code&gt;, &lt;code&gt;best_index_&lt;/code&gt;, &lt;code&gt;best_score_&lt;/code&gt; and &lt;code&gt;best_params_&lt;/code&gt; correspond to the scorer (key) that is set to the &lt;code&gt;refit&lt;/code&gt; attribute.</source>
          <target state="translated">&lt;code&gt;best_estimator_&lt;/code&gt; , &lt;code&gt;best_index_&lt;/code&gt; , &lt;code&gt;best_score_&lt;/code&gt; 및 &lt;code&gt;best_params_&lt;/code&gt; 받는 설정 기록원 (키)에 대응하는 &lt;code&gt;refit&lt;/code&gt; 속성.</target>
        </trans-unit>
        <trans-unit id="ea04fcbf02a8de4070a990d8e4f932cdf0278b0d" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;beta&lt;/code&gt; parameter determines the weight of precision in the combined score. &lt;code&gt;beta &amp;lt; 1&lt;/code&gt; lends more weight to precision, while &lt;code&gt;beta &amp;gt; 1&lt;/code&gt; favors recall (&lt;code&gt;beta -&amp;gt; 0&lt;/code&gt; considers only precision, &lt;code&gt;beta -&amp;gt; inf&lt;/code&gt; only recall).</source>
          <target state="translated">&lt;code&gt;beta&lt;/code&gt; 파라미터는 정밀 결합 점수의 중량을 결정한다. &lt;code&gt;beta &amp;lt; 1&lt;/code&gt; 은 정밀도에 더 많은 가중치를 부여하는 반면, &lt;code&gt;beta &amp;gt; 1&lt;/code&gt; 은 리콜을 선호합니다 ( &lt;code&gt;beta -&amp;gt; 0&lt;/code&gt; 은 정밀도, &lt;code&gt;beta -&amp;gt; inf&lt;/code&gt; 리콜 만 고려 ).</target>
        </trans-unit>
        <trans-unit id="1cbc8f71751f51b8523a564fa7c56816a950df46" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;clf&lt;/code&gt; (for classifier) estimator instance is first fitted to the model; that is, it must &lt;em&gt;learn&lt;/em&gt; from the model. This is done by passing our training set to the &lt;code&gt;fit&lt;/code&gt; method. For the training set, we&amp;rsquo;ll use all the images from our dataset, except for the last image, which we&amp;rsquo;ll reserve for our predicting. We select the training set with the &lt;code&gt;[:-1]&lt;/code&gt; Python syntax, which produces a new array that contains all but the last item from &lt;code&gt;digits.data&lt;/code&gt;:</source>
          <target state="translated">&lt;code&gt;clf&lt;/code&gt; 추정기 인스턴스 (분류기)를 제 모델에 장착되고; 즉 , 모델에서 &lt;em&gt;배워야&lt;/em&gt; 합니다 . 훈련 세트를 &lt;code&gt;fit&lt;/code&gt; 방법 으로 전달하면됩니다 . 훈련 세트의 경우 예측을 위해 예약 할 마지막 이미지를 제외하고 데이터 세트의 모든 이미지를 사용합니다. &lt;code&gt;[:-1]&lt;/code&gt; Python 구문으로 훈련 세트를 선택합니다 .이 구문은 &lt;code&gt;digits.data&lt;/code&gt; 에서 마지막 항목을 제외한 모든 항목을 포함하는 새 배열을 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="0655de3e2b717fc73c590188fdaa9881c37414a7" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;cross_validate&lt;/code&gt; function differs from &lt;code&gt;cross_val_score&lt;/code&gt; in two ways -</source>
          <target state="translated">&lt;code&gt;cross_validate&lt;/code&gt; 에서 기능 다릅니다 &lt;code&gt;cross_val_score&lt;/code&gt; 두 가지 방법 -</target>
        </trans-unit>
        <trans-unit id="f3aad90428722c87b3422d97c1856aa8204966ed" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;cv_results_&lt;/code&gt; parameter can be easily imported into pandas as a &lt;code&gt;DataFrame&lt;/code&gt; for further inspection.</source>
          <target state="translated">&lt;code&gt;cv_results_&lt;/code&gt; 의 매개 변수는 쉽게로 팬더로 가져올 수 있습니다 &lt;code&gt;DataFrame&lt;/code&gt; 추가 검사합니다.</target>
        </trans-unit>
        <trans-unit id="6d0144f231c5dfa868fda545c176e4a6eca95147" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;data_id&lt;/code&gt; also uniquely identifies a dataset from OpenML:</source>
          <target state="translated">&lt;code&gt;data_id&lt;/code&gt; 또한 고유 OpenML에서 데이터 세트를 식별합니다 :</target>
        </trans-unit>
        <trans-unit id="7bfd5d978dedbc7159d3a401f357dd25ad25428a" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;decision_function&lt;/code&gt; method is also defined from the scoring function, in such a way that negative values are outliers and non-negative ones are inliers:</source>
          <target state="translated">&lt;code&gt;decision_function&lt;/code&gt; 의 방법은 음의 값이 특이점이 아닌 부정적인 라이어가있는 것과 같은 방법에있어서, 스코어링 함수로 정의된다 :</target>
        </trans-unit>
        <trans-unit id="62eacbcbc4132ef1f5317a0a939d640165e31df3" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;decision_function&lt;/code&gt; method of &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt;&lt;code&gt;NuSVC&lt;/code&gt;&lt;/a&gt; gives per-class scores for each sample (or a single score per sample in the binary case). When the constructor option &lt;code&gt;probability&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, class membership probability estimates (from the methods &lt;code&gt;predict_proba&lt;/code&gt; and &lt;code&gt;predict_log_proba&lt;/code&gt;) are enabled. In the binary case, the probabilities are calibrated using Platt scaling: logistic regression on the SVM&amp;rsquo;s scores, fit by an additional cross-validation on the training data. In the multiclass case, this is extended as per Wu et al. (2004).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt; &lt;code&gt;NuSVC&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;decision_function&lt;/code&gt; 메소드는 각 샘플에 대한 클래스 별 점수 (또는 이진 경우 샘플 당 단일 점수)를 제공합니다. 생성자 옵션 &lt;code&gt;probability&lt;/code&gt; 이 &lt;code&gt;True&lt;/code&gt; 로 설정되면 ( &lt;code&gt;predict_proba&lt;/code&gt; 및 &lt;code&gt;predict_log_proba&lt;/code&gt; 메소드에서) 클래스 멤버쉽 확률 추정 이 사용됩니다. 이진 경우 확률은 Platt scaling : SVM의 점수에 대한 로지스틱 회귀를 사용하여 훈련 데이터에 대한 추가 교차 검증에 따라 교정됩니다. 멀티 클래스의 경우, 이것은 Wu et al. (2004).</target>
        </trans-unit>
        <trans-unit id="ac9899847d23144b2277382b5ec5bf6360733941" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;features&lt;/code&gt; parameter can be set to &lt;code&gt;'all'&lt;/code&gt; to returned all features whether or not they contain missing values:</source>
          <target state="translated">는 &lt;code&gt;features&lt;/code&gt; 매개 변수가 설정 될 수있다 &lt;code&gt;'all'&lt;/code&gt; 가 누락 된 값을 포함할지 여부를 반환 모든 기능에 :</target>
        </trans-unit>
        <trans-unit id="0e29ac108f496200ac17f2eb1912fca379623586" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;features&lt;/code&gt; parameter is used to choose the features for which the mask is constructed. By default, it is &lt;code&gt;'missing-only'&lt;/code&gt; which returns the imputer mask of the features containing missing values at &lt;code&gt;fit&lt;/code&gt; time:</source>
          <target state="translated">&lt;code&gt;features&lt;/code&gt; 마스크가 구성되어있는 기능을 선택하는 데 사용되는 매개 변수입니다. 기본적으로 &lt;code&gt;'missing-only'&lt;/code&gt; 으로 &lt;code&gt;fit&lt;/code&gt; 시간에 결 측값이 포함 된 피처의 마스크를 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="9b06298ca8347e48ebdf3df08a4c5d05e9d2dcbb" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;fit&lt;/code&gt; function takes two arguments: &lt;code&gt;n_components&lt;/code&gt;, which is the target dimensionality of the feature transform, and &lt;code&gt;gamma&lt;/code&gt;, the parameter of the RBF-kernel. A higher &lt;code&gt;n_components&lt;/code&gt; will result in a better approximation of the kernel and will yield results more similar to those produced by a kernel SVM. Note that &amp;ldquo;fitting&amp;rdquo; the feature function does not actually depend on the data given to the &lt;code&gt;fit&lt;/code&gt; function. Only the dimensionality of the data is used. Details on the method can be found in &lt;a href=&quot;#rr2007&quot; id=&quot;id3&quot;&gt;[RR2007]&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; : 함수는 두 인자 얻어 &lt;code&gt;n_components&lt;/code&gt; 기능의 목표 차원 변환을하고, 그리고 &lt;code&gt;gamma&lt;/code&gt; 상기 RBF 커널의 파라미터. &lt;code&gt;n_components&lt;/code&gt; 가 높을수록 커널 근사값이 향상되고 커널 SVM에서 생성 한 결과와 더 유사한 결과가 나타납니다. 기능 기능은 실제로 주어진 데이터에 의존하지 않는다 &quot;피팅&quot;고주의 &lt;code&gt;fit&lt;/code&gt; 기능. 데이터의 차원 만 사용됩니다. 방법에 대한 자세한 내용은 &lt;a href=&quot;#rr2007&quot; id=&quot;id3&quot;&gt;[RR2007]&lt;/a&gt; 에서 찾을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="759d68cbc1b0e0c960b6f5234a5fe3b985174684" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;init&lt;/code&gt; attribute determines the initialization method applied, which has a great impact on the performance of the method. &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; implements the method Nonnegative Double Singular Value Decomposition. NNDSVD &lt;a href=&quot;#id13&quot; id=&quot;id7&quot;&gt;[4]&lt;/a&gt; is based on two SVD processes, one approximating the data matrix, the other approximating positive sections of the resulting partial SVD factors utilizing an algebraic property of unit rank matrices. The basic NNDSVD algorithm is better fit for sparse factorization. Its variants NNDSVDa (in which all zeros are set equal to the mean of all elements of the data), and NNDSVDar (in which the zeros are set to random perturbations less than the mean of the data divided by 100) are recommended in the dense case.</source>
          <target state="translated">&lt;code&gt;init&lt;/code&gt; 특성은 상기 방법의 성능에 큰 영향을인가하는 초기화 방법을 결정한다. &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt; 는 음이 아닌 이중 특이 값 분해 방법을 구현합니다. NNDSVD &lt;a href=&quot;#id13&quot; id=&quot;id7&quot;&gt;[4]&lt;/a&gt; 는 두 개의 SVD 프로세스를 기반으로하며, 하나는 데이터 매트릭스에 근사하고, 다른 하나는 단위 랭크 매트릭스의 대수적 특성을 이용하는 결과적인 부분 SVD 요인의 양의 섹션에 근사합니다. 기본 NNDSVD 알고리즘은 희소 인수 분해에 더 적합합니다. 변형 NNDSVDa (모든 0이 데이터의 모든 요소의 평균과 동일하게 설정 됨) 및 NNDSVDar (0이 데이터의 평균을 100으로 나눈 값보다 작은 임의의 섭동으로 설정 됨)이 밀도가 높을 때 권장 됨 케이스.</target>
        </trans-unit>
        <trans-unit id="5aaf2ff68858d9c24eece58235794e4a322e1ce9" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;intercept_&lt;/code&gt; member is not converted.</source>
          <target state="translated">&lt;code&gt;intercept_&lt;/code&gt; 회원은 변환되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="0e6b2b935d052640da205c359a0d82666ebb9942" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;is_data_valid&lt;/code&gt; and &lt;code&gt;is_model_valid&lt;/code&gt; functions allow to identify and reject degenerate combinations of random sub-samples. If the estimated model is not needed for identifying degenerate cases, &lt;code&gt;is_data_valid&lt;/code&gt; should be used as it is called prior to fitting the model and thus leading to better computational performance.</source>
          <target state="translated">&lt;code&gt;is_data_valid&lt;/code&gt; 및 &lt;code&gt;is_model_valid&lt;/code&gt; 기능을 식별 할 수 있도록 임의의 서브 샘플 축퇴 조합을 거부한다. 퇴화 사례를 식별하기 위해 추정 된 모델이 필요하지 않은 경우 &lt;code&gt;is_data_valid&lt;/code&gt; 는 모델을 피팅하기 전에 호출되므로 계산 성능이 향상됩니다.</target>
        </trans-unit>
        <trans-unit id="a1aa8bc8d7f393abce9beb6161257c50a1665624" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;len(features)&lt;/code&gt; plots are arranged in a grid with &lt;code&gt;n_cols&lt;/code&gt; columns. Two-way partial dependence plots are plotted as contour plots.</source>
          <target state="translated">&lt;code&gt;len(features)&lt;/code&gt; 플롯과 격자 배열되어 &lt;code&gt;n_cols&lt;/code&gt; 의 열입니다. 양방향 부분 의존도 플롯은 등고선 플롯으로 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="4d39915194cee376ca662b61de9924274942d60a" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;make_columntransformer&lt;/code&gt; function is available to more easily create a &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;ColumnTransformer&lt;/code&gt;&lt;/a&gt; object. Specifically, the names will be given automatically. The equivalent for the above example would be:</source>
          <target state="translated">&lt;code&gt;make_columntransformer&lt;/code&gt; 의 기능을 더 쉽게 만들 수 있습니다 &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;ColumnTransformer&lt;/code&gt; 의&lt;/a&gt; 개체를. 특히 이름이 자동으로 주어집니다. 위의 예와 동등한 것은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="e3ab7886f45f0e5bcfcb494c5dda13f6aee54058" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;mean_fit_time&lt;/code&gt;, &lt;code&gt;std_fit_time&lt;/code&gt;, &lt;code&gt;mean_score_time&lt;/code&gt; and &lt;code&gt;std_score_time&lt;/code&gt; are all in seconds.</source>
          <target state="translated">&lt;code&gt;mean_fit_time&lt;/code&gt; , &lt;code&gt;std_fit_time&lt;/code&gt; , &lt;code&gt;mean_score_time&lt;/code&gt; 및 &lt;code&gt;std_score_time&lt;/code&gt; 는 초 모두.</target>
        </trans-unit>
        <trans-unit id="4dc710fdb008b124893854b1db0d772123e2f23c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;out_of_bounds&lt;/code&gt; parameter handles how x-values outside of the training domain are handled. When set to &amp;ldquo;nan&amp;rdquo;, predicted y-values will be NaN. When set to &amp;ldquo;clip&amp;rdquo;, predicted y-values will be set to the value corresponding to the nearest train interval endpoint. When set to &amp;ldquo;raise&amp;rdquo;, allow &lt;code&gt;interp1d&lt;/code&gt; to throw ValueError.</source>
          <target state="translated">&lt;code&gt;out_of_bounds&lt;/code&gt; 훈련 도메인의 x 값의 외부를 처리하는 방법을 핸들 매개 변수입니다. &quot;nan&quot;으로 설정하면 예측 된 y 값은 NaN이됩니다. &quot;clip&quot;으로 설정하면 예측 된 y 값이 가장 가까운 열차 간격 끝점에 해당하는 값으로 설정됩니다. &amp;ldquo;raise&amp;rdquo;로 설정되면 &lt;code&gt;interp1d&lt;/code&gt; 가 ValueError를 발생 시키 도록하십시오.</target>
        </trans-unit>
        <trans-unit id="3c0e73047db48d9d2c7fabbdd5b52d96e2b806a9" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;partial_fit&lt;/code&gt; method call of naive Bayes models introduces some computational overhead. It is recommended to use data chunk sizes that are as large as possible, that is as the available RAM allows.</source>
          <target state="translated">순진 Bayes 모델 의 &lt;code&gt;partial_fit&lt;/code&gt; 메소드 호출은 일부 계산 오버 헤드를 발생시킵니다. 사용 가능한 RAM이 허용하는 한 큰 데이터 청크 크기를 사용하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="c075895944959b3ce70972d2605db496c74ee36b" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;preprocessing&lt;/code&gt; module further provides a utility class &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt;&lt;code&gt;Normalizer&lt;/code&gt;&lt;/a&gt; that implements the same operation using the &lt;code&gt;Transformer&lt;/code&gt; API (even though the &lt;code&gt;fit&lt;/code&gt; method is useless in this case: the class is stateless as this operation treats samples independently).</source>
          <target state="translated">&lt;code&gt;preprocessing&lt;/code&gt; 모듈은 상기 유틸리티 클래스 제공 &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt; &lt;code&gt;Normalizer&lt;/code&gt; &lt;/a&gt; 구현하여 동일한 동작한다는 &lt;code&gt;Transformer&lt;/code&gt; (짝수 비록 API가 &lt;code&gt;fit&lt;/code&gt; : 클래스이 조작 취급 샘플 무국적 독립적 인 방법은이 경우 쓸모 참조).</target>
        </trans-unit>
        <trans-unit id="deb9c47cdc32b652eb6b8e87508a50e73ddd6520" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;preprocessing&lt;/code&gt; module further provides a utility class &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt; that implements the &lt;code&gt;Transformer&lt;/code&gt; API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. This class is hence suitable for use in the early steps of a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">&lt;code&gt;preprocessing&lt;/code&gt; 모듈은 더 유틸리티 클래스 제공 &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; &lt;/a&gt; 구현하는 것을 &lt;code&gt;Transformer&lt;/code&gt; API는 않도록 훈련 세트에 대한 평균과 표준 편차를 계산하기 위해 테스트 세트에 나중에 다시 적용 같은 변환 할 수 있습니다. 따라서이 클래스는 &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt; 의 초기 단계에서 사용하기에 적합합니다 .</target>
        </trans-unit>
        <trans-unit id="1c988d2d2cf78f045059c60f8f22ddfe99f6b9b8" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;random_state&lt;/code&gt; parameter defaults to &lt;code&gt;None&lt;/code&gt;, meaning that the shuffling will be different every time &lt;code&gt;KFold(..., shuffle=True)&lt;/code&gt; is iterated. However, &lt;code&gt;GridSearchCV&lt;/code&gt; will use the same shuffling for each set of parameters validated by a single call to its &lt;code&gt;fit&lt;/code&gt; method.</source>
          <target state="translated">&lt;code&gt;random_state&lt;/code&gt; 의 에 매개 변수의 기본값은 &lt;code&gt;None&lt;/code&gt; 셔플마다 다를 수 있음을 의미 &lt;code&gt;KFold(..., shuffle=True)&lt;/code&gt; 반복된다. 그러나 &lt;code&gt;GridSearchCV&lt;/code&gt; 는 한 번의 &lt;code&gt;fit&lt;/code&gt; 메소드 호출로 유효성이 검증 된 각 매개 변수 세트에 대해 동일한 셔플 링을 사용합니다 .</target>
        </trans-unit>
        <trans-unit id="269b993dc29ab1401256004d5e02aa3f5322b3b6" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;remainder&lt;/code&gt; parameter can be set to an estimator to transform the remaining rating columns. The transformed values are appended to the end of the transformation:</source>
          <target state="translated">&lt;code&gt;remainder&lt;/code&gt; 매개 변수는 나머지 평가 열을 변환하는 추정으로 설정 될 수있다. 변환 된 값이 변환 끝에 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="5cf025ac399ddc1c71c004c7cb8bd73171357561" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;shrinkage&lt;/code&gt; parameter can also be manually set between 0 and 1. In particular, a value of 0 corresponds to no shrinkage (which means the empirical covariance matrix will be used) and a value of 1 corresponds to complete shrinkage (which means that the diagonal matrix of variances will be used as an estimate for the covariance matrix). Setting this parameter to a value between these two extrema will estimate a shrunk version of the covariance matrix.</source>
          <target state="translated">&lt;code&gt;shrinkage&lt;/code&gt; 파라미터 수동 0없이 수축 0에 대응하는 값 (경험적 공분산 행렬을 의미 사용되는) 특정 1. 완전 수축 한 대응 값 (사이에 설정 될 수있는 수단 대각 그 분산 행렬은 공분산 행렬의 추정치로 사용됩니다). 이 매개 변수를이 두 극값 사이의 값으로 설정하면 공분산 행렬의 축소 된 버전이 추정됩니다.</target>
        </trans-unit>
        <trans-unit id="b439b9c10b67475737ae3e151e90a6ed815edbd1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.covariance&lt;/code&gt; package implements a robust estimator of covariance, the Minimum Covariance Determinant &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;[3]&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;sklearn.covariance&lt;/code&gt; 의 패키지 공분산 견고한 추정기 최소 공분산 행렬식 구현한다 &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;[3]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="6908af1748b7fe666814b53ca6e0a8cab66eb012" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.datasets&lt;/code&gt; package embeds some small toy datasets as introduced in the &lt;a href=&quot;../tutorial/basic/tutorial#loading-example-dataset&quot;&gt;Getting Started&lt;/a&gt; section.</source>
          <target state="translated">&lt;code&gt;sklearn.datasets&lt;/code&gt; 의 패키지에 도입 된 일부 작은 장난감의 데이터 세트를 내장 &lt;a href=&quot;../tutorial/basic/tutorial#loading-example-dataset&quot;&gt;시작&lt;/a&gt; 절을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="82c9f1d0e48a88b08f4d85a858e0d70b67f55eec" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.datasets&lt;/code&gt; package is able to download datasets from the repository using the function &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_openml#sklearn.datasets.fetch_openml&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_openml&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;sklearn.datasets&lt;/code&gt; 의 패키지 함수를 사용하여 저장소에서 다운로드 데이터 세트에 수 &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_openml#sklearn.datasets.fetch_openml&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_openml&lt;/code&gt; 을&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="8a80b4da04f92037fdd1ffd5771651becd3a649e" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.preprocessing&lt;/code&gt; package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.</source>
          <target state="translated">&lt;code&gt;sklearn.preprocessing&lt;/code&gt; 패키지는 몇 가지 일반적인 유틸리티 함수 변압기 클래스 하류 추정기에 더 적합한 표현으로 원시 특징 벡터를 변경합니다.</target>
        </trans-unit>
        <trans-unit id="9a666cbe94ae4d4ef285ecec57ff29087d1b1c92" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;stop_words_&lt;/code&gt; attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling.</source>
          <target state="translated">&lt;code&gt;stop_words_&lt;/code&gt; 속성 이 커지고 모델 크기가 커질 수 있습니다. 이 속성은 검사 용으로 만 제공되며 delattr을 사용하여 안전하게 제거하거나 산 세척 전에 없음으로 설정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a2df492be59e0b5e94ab9ece47f7ab58734f8d4a" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;svm.OneClassSVM&lt;/code&gt; is known to be sensitive to outliers and thus does not perform very well for outlier detection. This estimator is best suited for novelty detection when the training set is not contaminated by outliers. That said, outlier detection in high-dimension, or without any assumptions on the distribution of the inlying data is very challenging, and a One-class SVM might give useful results in these situations depending on the value of its hyperparameters.</source>
          <target state="translated">&lt;code&gt;svm.OneClassSVM&lt;/code&gt; 는 아웃 라이어 검출 잘 수행하지 않는 따라서 특이점에 민감한 것으로 공지되어있다. 이 추정기는 훈련 세트가 특이 치에 의해 오염되지 않은 경우 신규성 검출에 가장 적합합니다. 즉, 고차원 또는 특이 데이터 분포에 대한 가정이없는 이상 치를 탐지하는 것은 매우 어려운 일이며, 1 클래스 SVM은 하이퍼 파라미터의 값에 따라 이러한 상황에서 유용한 결과를 제공 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a0172c625369e43fa830669101114953189c751e" translate="yes" xml:space="preserve">
          <source>The &lt;em&gt;kernel function&lt;/em&gt; can be any of the following:</source>
          <target state="translated">&lt;em&gt;커널 함수는&lt;/em&gt; 다음 중 하나가 될 수 있습니다 :</target>
        </trans-unit>
        <trans-unit id="6eff32d476fb25ebcdfdcb46623a8711aa972809" translate="yes" xml:space="preserve">
          <source>The &lt;strong&gt;conditional entropy of clusters given class&lt;/strong&gt;\(H(K|C)\) and the &lt;strong&gt;entropy of clusters&lt;/strong&gt;\(H(K)\) are defined in a symmetric manner.</source>
          <target state="translated">&lt;strong&gt;클래스&lt;/strong&gt; \ (H (K | C) \)가 &lt;strong&gt;주어진 클러스터&lt;/strong&gt; 의 &lt;strong&gt;조건부 엔트로피와 클러스터&lt;/strong&gt; 의 &lt;strong&gt;엔트로피&lt;/strong&gt; \ (H (K) \)는 대칭 방식으로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="2143ba51f2aad99e99e8cd60a62ccfdecbf0f265" translate="yes" xml:space="preserve">
          <source>The AMI returns a value of 1 when the two partitions are identical (ie perfectly matched). Random partitions (independent labellings) have an expected AMI around 0 on average hence can be negative.</source>
          <target state="translated">AMI는 두 파티션이 동일하면 (즉, 완벽하게 일치하는) 1의 값을 반환합니다. 무작위 파티션 (독립적 인 레이블링)은 평균적으로 약 0의 예상 AMI를 가지므로 음수 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="00e3722885bdadae5430c6fecaacfc1cced893f6" translate="yes" xml:space="preserve">
          <source>The API is experimental in version 0.20 (particularly the return value structure), and might have small backward-incompatible changes in future releases.</source>
          <target state="translated">API는 버전 0.20 (특히 반환 값 구조)에서 실험 중이며 이후 릴리스에서 이전 버전과 호환되지 않는 작은 변경 사항이있을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="22556f00c2cdfc8c60673e1c663299619a64901c" translate="yes" xml:space="preserve">
          <source>The BIC criterion can be used to select the number of components in a Gaussian Mixture in an efficient way. In theory, it recovers the true number of components only in the asymptotic regime (i.e. if much data is available and assuming that the data was actually generated i.i.d. from a mixture of Gaussian distribution). Note that using a &lt;a href=&quot;#bgmm&quot;&gt;Variational Bayesian Gaussian mixture&lt;/a&gt; avoids the specification of the number of components for a Gaussian mixture model.</source>
          <target state="translated">BIC 기준을 사용하여 가우스 혼합의 성분 수를 효율적으로 선택할 수 있습니다. 이론적으로, 그것은 점근 정권에서만 (즉, 가용 한 많은 데이터가 있고 데이터가 실제로 가우시안 분포의 혼합물에서 iid로 생성되었다고 가정하는 경우)에만 실제 수의 구성 요소를 복구합니다. 사용합니다 &lt;a href=&quot;#bgmm&quot;&gt;변분 베이지안 가우시안 혼합하여&lt;/a&gt; 가우시안 혼합 모델에 대한 부품 수의 사양을 방지 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c912c462511f5921d016fed26135f626922ccc57" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut implementation only works when the target dimensionality is 3 or less. The 2D case is typical when building visualizations.</source>
          <target state="translated">Barnes-Hut 구현은 목표 차원이 3 이하인 경우에만 작동합니다. 시각화를 구축 할 때는 2D 사례가 일반적입니다.</target>
        </trans-unit>
        <trans-unit id="49027a83447d6307ec530f0dd81bd5ad76360faa" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.</source>
          <target state="translated">Barnes-Hut t-SNE 방법은 2 차원 또는 3 차원 임베딩으로 제한됩니다.</target>
        </trans-unit>
        <trans-unit id="f05f901deabcb1ee376f8697fc79932cbf746ce6" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut t-SNE that has been implemented here is usually much slower than other manifold learning algorithms. The optimization is quite difficult and the computation of the gradient is \(O[d N log(N)]\), where \(d\) is the number of output dimensions and \(N\) is the number of samples. The Barnes-Hut method improves on the exact method where t-SNE complexity is \(O[d N^2]\), but has several other notable differences:</source>
          <target state="translated">여기서 구현 된 Barnes-Hut t-SNE는 일반적으로 다른 매니 폴드 학습 알고리즘보다 훨씬 느립니다. 최적화는 매우 어렵고 그래디언트 계산은 \ (O [d N log (N)] \)입니다. 여기서 \ (d \)는 출력 차원의 수이고 \ (N \)은 샘플 수입니다. Barnes-Hut 방법은 t-SNE 복잡성이 \ (O [d N ^ 2] \) 인 정확한 방법에서 개선되지만 몇 가지 다른 차이점이 있습니다.</target>
        </trans-unit>
        <trans-unit id="e724094f6d5c410991717a9e2fc045d6798def45" translate="yes" xml:space="preserve">
          <source>The Birch algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters.</source>
          <target state="translated">버치 알고리즘에는 임계 값과 분기 인자라는 두 가지 매개 변수가 있습니다. 분기 계수는 노드의 하위 클러스터 수를 제한하고 임계 값은 입력 샘플과 기존 하위 클러스터 사이의 거리를 제한합니다.</target>
        </trans-unit>
        <trans-unit id="8fa70be719c3475e8e012f73acc572a21c03cdd8" translate="yes" xml:space="preserve">
          <source>The Boston house-price data has been used in many machine learning papers that address regression problems.</source>
          <target state="translated">보스턴 주택 가격 데이터는 회귀 문제를 해결하는 많은 기계 학습 논문에서 사용되었습니다.</target>
        </trans-unit>
        <trans-unit id="455243540177422da87f9c3aa93de30456a2bae3" translate="yes" xml:space="preserve">
          <source>The Boston house-price data of Harrison, D. and Rubinfeld, D.L. &amp;lsquo;Hedonic prices and the demand for clean air&amp;rsquo;, J. Environ. Economics &amp;amp; Management, vol.5, 81-102, 1978. Used in Belsley, Kuh &amp;amp; Welsch, &amp;lsquo;Regression diagnostics &amp;hellip;&amp;rsquo;, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter.</source>
          <target state="translated">DL'Hedonic 가격과 청정 공기 수요 '인 Harrison, D. 및 Rubinfeld의 보스턴 주택 가격 데이터, J. Environ. Economics &amp;amp; Management, vol.5, 81-102, 1978. Belsley, Kuh &amp;amp; Welsch, 'Regression diagnostics&amp;hellip;', Wiley, 1980 년에 사용됨. NB 후자 244-261 페이지의 표에는 다양한 변형이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="0e6c056d908bee991f648998f41ff72e8a7301c2" translate="yes" xml:space="preserve">
          <source>The CF Subclusters hold the necessary information for clustering which prevents the need to hold the entire input data in memory. This information includes:</source>
          <target state="translated">CF 서브 클러스터는 클러스터링에 필요한 정보를 보유하므로 전체 입력 데이터를 메모리에 보유 할 필요가 없습니다. 이 정보에는 다음이 포함됩니다.</target>
        </trans-unit>
        <trans-unit id="b96c855c122e21633007587f3600e092339a5383" translate="yes" xml:space="preserve">
          <source>The Calinski-Harabaz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">Calinski-Harabaz 지수는 일반적으로 DBSCAN을 통해 얻은 밀도 기반 클러스터와 같은 다른 개념의 클러스터보다 볼록 클러스터에 대해 더 높습니다.</target>
        </trans-unit>
        <trans-unit id="6cdbc3a888667e8344981c2a8742122994627087" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier described in Rennie et al.</source>
          <target state="translated">Rennie et al.에 기술 된 보완 베이브 분류기.</target>
        </trans-unit>
        <trans-unit id="2b0b8b0a4dd5523203a6ee4b5d195c2e6a35c0fe" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier described in Rennie et al. (2003).</source>
          <target state="translated">Rennie et al.에 기술 된 보완 베이브 분류기. (2003).</target>
        </trans-unit>
        <trans-unit id="3398d87a3aceb1ea8375c52fcc6a67cec5c63df9" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier was designed to correct the &amp;ldquo;severe assumptions&amp;rdquo; made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.</source>
          <target state="translated">Complement Naive Bayes 분류기는 표준 Multinomial Naive Bayes 분류 기가 만든 &quot;심각한 가정&quot;을 수정하도록 설계되었습니다. 불균형 데이터 세트에 특히 적합합니다.</target>
        </trans-unit>
        <trans-unit id="7c176cfd9628fae51161daa03c193aeae01d49ea" translate="yes" xml:space="preserve">
          <source>The Contrastive Divergence method suggests to stop the chain after a small number of iterations, \(k\), usually even 1. This method is fast and has low variance, but the samples are far from the model distribution.</source>
          <target state="translated">Contrastive Divergence 방법은 적은 반복 횟수 (\ (k \), 일반적으로 1) 후에 체인을 중지하도록 제안합니다.</target>
        </trans-unit>
        <trans-unit id="1d09474bd461f8756796df6e9a77749a8489b6d3" translate="yes" xml:space="preserve">
          <source>The DBSCAN algorithm is deterministic, always generating the same clusters when given the same data in the same order. However, the results can differ when data is provided in a different order. First, even though the core samples will always be assigned to the same clusters, the labels of those clusters will depend on the order in which those samples are encountered in the data. Second and more importantly, the clusters to which non-core samples are assigned can differ depending on the data order. This would happen when a non-core sample has a distance lower than &lt;code&gt;eps&lt;/code&gt; to two core samples in different clusters. By the triangular inequality, those two core samples must be more distant than &lt;code&gt;eps&lt;/code&gt; from each other, or they would be in the same cluster. The non-core sample is assigned to whichever cluster is generated first in a pass through the data, and so the results will depend on the data ordering.</source>
          <target state="translated">DBSCAN 알고리즘은 결정 론적이며 동일한 순서로 동일한 데이터가 제공 될 때 항상 동일한 클러스터를 생성합니다. 그러나 데이터가 다른 순서로 제공되면 결과가 다를 수 있습니다. 첫째, 핵심 샘플이 항상 동일한 클러스터에 할당 되더라도 해당 클러스터의 레이블은 해당 샘플이 데이터에서 발견되는 순서에 따라 달라집니다. 두 번째로 중요한 것은 비 핵심 샘플이 할당 된 클러스터는 데이터 순서에 따라 다를 수 있습니다. 이것은 코어가 아닌 샘플이 다른 클러스터의 &lt;code&gt;eps&lt;/code&gt; 에서 2 개의 코어 샘플까지 의 거리를 가질 때 발생 합니다. 삼각 불평등에 의해,이 두 핵심 표본은 &lt;code&gt;eps&lt;/code&gt; 보다 먼 거리에 있어야합니다서로 또는 같은 클러스터에있을 것입니다. 코어가 아닌 샘플은 데이터를 통과 할 때 먼저 생성되는 클러스터에 할당되므로 결과는 데이터 순서에 따라 달라집니다.</target>
        </trans-unit>
        <trans-unit id="b5783662e991f62ab3cc63e2843b11c10be0af6e" translate="yes" xml:space="preserve">
          <source>The Davies-Boulding index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained from DBSCAN.</source>
          <target state="translated">Davies-Boulding 지수는 일반적으로 DBSCAN에서 얻은 것과 같은 밀도 기반 클러스터와 같은 다른 개념의 클러스터보다 볼록 클러스터에 대해 더 높습니다.</target>
        </trans-unit>
        <trans-unit id="9dae6187a2e4264ce3764c47d6ced56e22112bc3" translate="yes" xml:space="preserve">
          <source>The Digit Dataset</source>
          <target state="translated">숫자 데이터 세트</target>
        </trans-unit>
        <trans-unit id="15949e73904f01f7a29e0206a3232b388b3af43d" translate="yes" xml:space="preserve">
          <source>The Dirichlet process prior allows to define an infinite number of components and automatically selects the correct number of components: it activates a component only if it is necessary.</source>
          <target state="translated">Dirichlet 프로세스는 사전에 무한한 수의 구성 요소를 정의하고 올바른 수의 구성 요소를 자동으로 선택합니다. 필요한 경우에만 구성 요소를 활성화합니다.</target>
        </trans-unit>
        <trans-unit id="d2e49448333a2cd92baf05825e927765f1835316" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is commonly combined with exponentiation.</source>
          <target state="translated">DotProduct 커널은 일반적으로 지수와 결합됩니다.</target>
        </trans-unit>
        <trans-unit id="842c0c72bca462c8ce3e9ff0cead821c33430a8a" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is non-stationary and can be obtained from linear regression by putting N(0, 1) priors on the coefficients of x_d (d = 1, . . . , D) and a prior of N(0, sigma_0^2) on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter sigma_0^2. For sigma_0^2 =0, the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</source>
          <target state="translated">DotProduct 커널은 정지하지 않으며 x_d (d = 1,..., D)의 계수와 N (0, sigma_0 ^ 2 이전의 계수에 N (0, 1)을 두어 선형 회귀에서 얻을 수 있습니다. 편견에. DotProduct 커널은 원점에 대한 좌표 회전에는 변하지 않지만 번역은 아닙니다. sigma_0 ^ 2 매개 변수로 매개 변수화됩니다. sigma_0 ^ 2 = 0의 경우 커널을 동종 선형 커널이라고하며, 그렇지 않으면 동종이 아닙니다. 커널은</target>
        </trans-unit>
        <trans-unit id="c219b28ff4dd0afb8b865b96896a279315780153" translate="yes" xml:space="preserve">
          <source>The Elastic Net mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15.</source>
          <target state="translated">0 &amp;lt;= l1_ratio &amp;lt;= 1 인 Elastic Net 믹싱 매개 변수 l1_ratio = 0은 L2 페널티, l1_ratio = 1 ~ L1에 해당합니다. 기본값은 0.15입니다.</target>
        </trans-unit>
        <trans-unit id="71317f65b5997839fb7f2d86b8c73dc4399ad254" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with 0 &amp;lt; l1_ratio &amp;lt;= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1/L2 and L2.</source>
          <target state="translated">0 &amp;lt;l1_ratio &amp;lt;= 1 인 ElasticNet 믹싱 매개 변수 l1_ratio = 1 인 경우 페널티는 L1 / L2 페널티입니다. l1_ratio = 0의 경우 L2 페널티입니다. 들면 &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; 패널티는 L1 / L2와 L2의 조합이다.</target>
        </trans-unit>
        <trans-unit id="f913d5ab29d41b1d0b8510f8960f88320058e481" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with 0 &amp;lt; l1_ratio &amp;lt;= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1/L2 and L2. This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in &lt;code&gt;[.1, .5, .7,
.9, .95, .99, 1]&lt;/code&gt;</source>
          <target state="translated">0 &amp;lt;l1_ratio &amp;lt;= 1 인 ElasticNet 믹싱 매개 변수 l1_ratio = 1 인 경우 페널티는 L1 / L2 페널티입니다. l1_ratio = 0의 경우 L2 페널티입니다. 들면 &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; 패널티는 L1 / L2와 L2의 조합이다. 이 매개 변수는 목록 일 수 있으며,이 경우 다른 값은 교차 유효성 검증에 의해 테스트되고 최상의 예측 점수를 제공하는 값이 사용됩니다. l1_ratio에 대한 값 목록의 올바른 선택은 &lt;code&gt;[.1, .5, .7, .9, .95, .99, 1]&lt;/code&gt; 에서와 같이 더 많은 값을 1 (예 : 올가미)에 가깝고 0 (예 : 릿지)에 두는 경우가 많습니다 . 95, .99, 1]</target>
        </trans-unit>
        <trans-unit id="62ded71b403044434993092527c854b8f53e5c17" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. For &lt;code&gt;l1_ratio = 0&lt;/code&gt; the penalty is an L2 penalty. &lt;code&gt;For l1_ratio = 1&lt;/code&gt; it is an L1 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="translated">&lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt; ElasticNet 믹싱 파라미터 . 들면 &lt;code&gt;l1_ratio = 0&lt;/code&gt; 패널티 L2 패널티이다. &lt;code&gt;For l1_ratio = 1&lt;/code&gt; 경우 L1 페널티입니다. 들면 &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; 패널티는 L1과 L2의 조합이다.</target>
        </trans-unit>
        <trans-unit id="706ede4c7ae02c53bd29139976a5c9ab95013fca" translate="yes" xml:space="preserve">
          <source>The ExpSineSquared kernel allows modeling periodic functions. It is parameterized by a length-scale parameter length_scale&amp;gt;0 and a periodicity parameter periodicity&amp;gt;0. Only the isotropic variant where l is a scalar is supported at the moment. The kernel given by:</source>
          <target state="translated">ExpSineSquared 커널은 주기적 기능 모델링을 허용합니다. 길이 스케일 매개 변수 length_scale&amp;gt; 0 및 주기성 매개 변수 주기성&amp;gt; 0으로 매개 변수화됩니다. l이 스칼라 인 등방성 변형 만 지원됩니다. 커널은 다음을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="bd1aecbb19e2286cfafdebb8292f1071a3eb508c" translate="yes" xml:space="preserve">
          <source>The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.</source>
          <target state="translated">F- 베타 점수는 정밀도 및 리콜의 가중 고조파 평균으로 해석 될 수 있으며, F- 베타 점수는 1에서 최고 값에, 0에서 최악의 점수에 도달합니다.</target>
        </trans-unit>
        <trans-unit id="fcc8a075bfea5f42d0e38256200386182d33cc42" translate="yes" xml:space="preserve">
          <source>The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0.</source>
          <target state="translated">F- 베타 점수는 가중 고조파 정밀도 및 리콜 평균으로 1에서 최적의 값에 도달하고 0에서 최악의 값에 도달합니다.</target>
        </trans-unit>
        <trans-unit id="553773b1d0f7903e424f857dbba13f4b2343a5a1" translate="yes" xml:space="preserve">
          <source>The F-beta score weights recall more than precision by a factor of &lt;code&gt;beta&lt;/code&gt;. &lt;code&gt;beta == 1.0&lt;/code&gt; means recall and precision are equally important.</source>
          <target state="translated">F- 베타 점수 가중치는 &lt;code&gt;beta&lt;/code&gt; 계수로 정밀도 이상의 것을 회상 합니다. &lt;code&gt;beta == 1.0&lt;/code&gt; 은 리콜 및 정밀도가 똑같이 중요하다는 것을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="3f53f44feaa854c0fdf5365ef5e43bc7fa440b01" translate="yes" xml:space="preserve">
          <source>The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:</source>
          <target state="translated">F1 점수는 정밀도 및 회수의 가중 평균으로 해석 될 수 있으며, 여기서 F1 점수는 1에서 최고 값에 도달하고 0에서 최악의 점수에 도달합니다. F1 점수에 대한 정밀도 및 회수의 상대적 기여는 동일합니다. F1 점수의 공식은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="518509d08bd98ece386f11a6583f35b4f559fdac" translate="yes" xml:space="preserve">
          <source>The Figure below shows four one-way and one two-way partial dependence plots for the California housing dataset:</source>
          <target state="translated">아래 그림은 캘리포니아 주택 데이터 세트에 대한 단방향 및 양방향 부분 의존도 플롯 4 개를 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="2280ec0b9d5ba5eef024fd7f6041defdab8c9d48" translate="yes" xml:space="preserve">
          <source>The Figure below shows the contours of the different regularization terms in the parameter space when \(R(w) = 1\).</source>
          <target state="translated">아래 그림은 \ (R (w) = 1 \) 인 경우 매개 변수 공간에서 다른 정규화 항의 윤곽을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="af21e4c771514d1aca6cd1c14dcff1ce67a0e477" translate="yes" xml:space="preserve">
          <source>The Fowlkes-Mallows index (&lt;a href=&quot;generated/sklearn.metrics.fowlkes_mallows_score#sklearn.metrics.fowlkes_mallows_score&quot;&gt;&lt;code&gt;sklearn.metrics.fowlkes_mallows_score&lt;/code&gt;&lt;/a&gt;) can be used when the ground truth class assignments of the samples is known. The Fowlkes-Mallows score FMI is defined as the geometric mean of the pairwise precision and recall:</source>
          <target state="translated">Fowlkes-Mallows 지수 ( &lt;a href=&quot;generated/sklearn.metrics.fowlkes_mallows_score#sklearn.metrics.fowlkes_mallows_score&quot;&gt; &lt;code&gt;sklearn.metrics.fowlkes_mallows_score&lt;/code&gt; &lt;/a&gt; )는 샘플의 기본 진리 클래스 할당이 알려진 경우 사용할 수 있습니다. Fowlkes-Mallows 점수 FMI는 쌍별 정밀도의 기하학적 평균으로 정의되며 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="bfdc8ed29be1f90cbfb4dc9576ce17a736f93294" translate="yes" xml:space="preserve">
          <source>The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of the precision and recall:</source>
          <target state="translated">Fowlkes-Mallows 지수 (FMI)는 정밀도와 회수 사이의 기하학적 평균으로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="90c804ef552ac504772d87a0c90e2454ea50931f" translate="yes" xml:space="preserve">
          <source>The GP prior mean is assumed to be zero. The prior&amp;rsquo;s covariance is specified by a passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="translated">GP 사전 평균은 0으로 가정합니다. 사전의 공분산은 &lt;a href=&quot;#gp-kernels&quot;&gt;커널&lt;/a&gt; 객체를 전달하여 지정됩니다 . 커널의 하이퍼 파라미터는 전달 &lt;code&gt;optimizer&lt;/code&gt; 기반으로 LLM (log-marginal-likelihood)을 최대화하여 GaussianProcessRegressor를 피팅하는 동안 최적화 됩니다. LML에 여러 개의 로컬 옵티마가있을 수 있으므로 &lt;code&gt;n_restarts_optimizer&lt;/code&gt; 를 지정하여 옵티 마이저를 반복적으로 시작할 수 있습니다 . 첫 번째 실행은 항상 커널의 초기 하이퍼 파라미터 값에서 시작하여 수행됩니다. 후속 실행은 허용 된 값의 범위에서 무작위로 선택된 하이퍼 파라미터 값에서 수행됩니다. 초기 하이퍼 파라미터를 고정 상태로 유지해야하는 경우 최적화 프로그램으로 &lt;code&gt;None&lt;/code&gt; 을 전달할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="01a974069deaad9f4a696951139fa6f180c4610d" translate="yes" xml:space="preserve">
          <source>The HLLE algorithm comprises three stages:</source>
          <target state="translated">HLLE 알고리즘은 3 단계로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="eaec5120030b979edc1dcb8e844874e0db8c76f1" translate="yes" xml:space="preserve">
          <source>The Hamming loss is the fraction of labels that are incorrectly predicted.</source>
          <target state="translated">해밍 손실은 잘못 예측 된 레이블의 일부입니다.</target>
        </trans-unit>
        <trans-unit id="ee89b1587a7c7b8d189cccf628a41e55c5d7ebe4" translate="yes" xml:space="preserve">
          <source>The Hamming loss is upperbounded by the subset zero-one loss. When normalized over samples, the Hamming loss is always between 0 and 1.</source>
          <target state="translated">해밍 손실은 부분적인 제로원 손실에 의해 상한이됩니다. 샘플에 대해 정규화 할 때 해밍 손실은 항상 0과 1 사이입니다.</target>
        </trans-unit>
        <trans-unit id="2f575d5541e123fdf35ce0dd5c6fb4373f14dde3" translate="yes" xml:space="preserve">
          <source>The Huber Regressor optimizes the squared loss for the samples where &lt;code&gt;|(y - X'w) / sigma| &amp;lt; epsilon&lt;/code&gt; and the absolute loss for the samples where &lt;code&gt;|(y - X'w) / sigma| &amp;gt; epsilon&lt;/code&gt;, where w and sigma are parameters to be optimized. The parameter sigma makes sure that if y is scaled up or down by a certain factor, one does not need to rescale epsilon to achieve the same robustness. Note that this does not take into account the fact that the different features of X may be of different scales.</source>
          <target state="translated">Huber Regressor는 &lt;code&gt;|(y - X'w) / sigma| &amp;lt; epsilon&lt;/code&gt; 샘플에서 제곱 손실을 최적화합니다. &amp;lt;엡실론 및 샘플에 대한 절대 손실 &lt;code&gt;|(y - X'w) / sigma| &amp;gt; epsilon&lt;/code&gt; , 여기서 w 및 sigma는 최적화 할 매개 변수입니다. 매개 변수 sigma는 y가 특정 요소에 의해 확대 또는 축소되는 경우 동일한 견고성을 달성하기 위해 엡실론의 크기를 조정할 필요가 없도록합니다. 이것은 X의 다른 기능이 다른 스케일 일 수 있다는 사실을 고려하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="a428acf86562c566ebfc0a4a1d3e42c700a1a75f" translate="yes" xml:space="preserve">
          <source>The Huber and epsilon-insensitive loss functions can be used for robust regression. The width of the insensitive region has to be specified via the parameter &lt;code&gt;epsilon&lt;/code&gt;. This parameter depends on the scale of the target variables.</source>
          <target state="translated">후버 및 엡실론에 둔감 한 손실 함수를 사용하여 강력한 회귀 분석을 수행 할 수 있습니다. 민감하지 않은 영역의 너비는 &lt;code&gt;epsilon&lt;/code&gt; 매개 변수를 통해 지정해야합니다 . 이 매개 변수는 대상 변수의 척도에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="0a2150cee6da11610a14f68e745e5d0639a39459" translate="yes" xml:space="preserve">
          <source>The Iris Dataset</source>
          <target state="translated">아이리스 데이터 셋</target>
        </trans-unit>
        <trans-unit id="7c0ef25371cb6aecb434ff0290ad88905a1c2367" translate="yes" xml:space="preserve">
          <source>The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour and Virginica) with 4 attributes: sepal length, sepal width, petal length and petal width.</source>
          <target state="translated">홍채 데이터 세트는 꽃받침 길이, 꽃받침 너비, 꽃잎 길이 및 꽃잎 너비의 4 가지 속성을 가진 3 가지 종류의 홍채 꽃 (Setosa, Versicolour 및 Virginica)을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="3877561d1fbee6aff8708ef4ec5fcabe6115833e" translate="yes" xml:space="preserve">
          <source>The IsolationForest &amp;lsquo;isolates&amp;rsquo; observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.</source>
          <target state="translated">IsolationForest는 피처를 임의로 선택한 다음 선택한 피처의 최대 값과 최소값 사이의 분할 값을 임의로 선택하여 관찰을 '격리'합니다.</target>
        </trans-unit>
        <trans-unit id="9e47d533732129c7b308a9673031913c04e17afd" translate="yes" xml:space="preserve">
          <source>The Isomap algorithm comprises three stages:</source>
          <target state="translated">Isomap 알고리즘은 세 단계로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="43f0627e56441244b0d44b83d4abbf8510d9eac0" translate="yes" xml:space="preserve">
          <source>The Jaccard index [1], or Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets, is used to compare set of predicted labels for a sample to the corresponding set of labels in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">교차점의 크기를 두 레이블 집합의 합집합 크기로 나눈 것으로 정의 된 Jaccard 색인 [1] 또는 Jaccard 유사성 계수는 &lt;code&gt;y_true&lt;/code&gt; 의 샘플에 대해 예측 레이블 집합을 해당 레이블 집합과 비교하는 데 사용됩니다. .</target>
        </trans-unit>
        <trans-unit id="54b0fb2f21ad6f516f16dd3562af3d3b867660fe" translate="yes" xml:space="preserve">
          <source>The Jaccard similarity coefficient of the \(i\)-th samples, with a ground truth label set \(y_i\) and predicted label set \(\hat{y}_i\), is defined as</source>
          <target state="translated">지면 진실 레이블 세트 \ (y_i \) 및 예측 레이블 세트 \ (\ hat {y} _i \)를 가진 \ (i \) 번째 샘플의 Jaccard 유사성 계수는 ​​다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="1b6ab133b67b3354e1eaee98fb0609deafb4a4e0" translate="yes" xml:space="preserve">
          <source>The Johnson-Lindenstrauss bound for embedding with random projections</source>
          <target state="translated">임의 투영법으로 임베딩 할 수있는 Johnson-Lindenstrauss</target>
        </trans-unit>
        <trans-unit id="2e420c9d276fc6531e2d5a1fd3367ac4465f247c" translate="yes" xml:space="preserve">
          <source>The KDD Cup &amp;lsquo;99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the &lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting &amp;lsquo;abnormal&amp;rsquo; data, ie</source>
          <target state="translated">KDD Cup '99 데이터 세트는 MIT Lincoln Lab [1]이 만든 1998 DARPA 침입 탐지 시스템 (IDS) 평가 데이터 세트의 tcpdump 부분을 처리하여 생성되었습니다. 인공 데이터 ( &lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;데이터 세트 홈페이지&lt;/a&gt; 에 설명되어 있음 )는 폐쇄 된 네트워크와 손으로 주입 된 공격을 사용하여 생성되어 백그라운드에서 정상적인 활동으로 여러 유형의 공격을 생성했습니다. 초기 목표는 감독 학습 알고리즘에 대한 대규모 훈련 세트를 생성하는 것이기 때문에 실제 세계에서는 비현실적이며 비정상적인 데이터 탐지를 목표로하는 감독되지 않은 이상 탐지에는 부적합한 비정상 데이터의 비율 (80.1 %)이 많으며, 즉</target>
        </trans-unit>
        <trans-unit id="42488af2a88df53e7b3301c7d5a125afdc9cd2ce" translate="yes" xml:space="preserve">
          <source>The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will be minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different initializations will end up in local minima of the KL divergence. Hence, it is sometimes useful to try different seeds and select the embedding with the lowest KL divergence.</source>
          <target state="translated">원래 공간과 포함 된 공간에서 관절 확률의 KL (Kullback-Leibler) 발산은 경사 하강에 의해 최소화됩니다. KL 분기는 볼록하지 않습니다. 즉, 초기화가 다른 여러 번 다시 시작하면 KL 분기의 로컬 최소값이됩니다. 따라서 다른 시드를 시도하고 KL 분기가 가장 낮은 임베딩을 선택하는 것이 유용한 경우가 있습니다.</target>
        </trans-unit>
        <trans-unit id="5c5e824f9a2e5664a81c2abf8d8de696419cc018" translate="yes" xml:space="preserve">
          <source>The LARS model can be used using estimator &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt;&lt;code&gt;Lars&lt;/code&gt;&lt;/a&gt;, or its low-level implementation &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">LARS 모델은 추정기 &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt; &lt;code&gt;Lars&lt;/code&gt; &lt;/a&gt; 또는 저수준 구현 &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; 를&lt;/a&gt; 사용하여 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="5604e6549ba6e538c77a361942c228433b92a65e" translate="yes" xml:space="preserve">
          <source>The LTSA algorithm comprises three stages:</source>
          <target state="translated">LTSA 알고리즘은 3 단계로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="5921a49032d4468242b223b9e118c35472eff0fa" translate="yes" xml:space="preserve">
          <source>The Lars algorithm provides the full path of the coefficients along the regularization parameter almost for free, thus a common operation consist of retrieving the path with function &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Lars 알고리즘은 정규화 매개 변수를 따라 계수의 전체 경로를 거의 무료로 제공하므로 일반적인 작업은 &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; &lt;/a&gt; 함수를 사용하여 경로를 검색하는 것으로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="0f5aa687d48724082c940128def6bdb3f6066405" translate="yes" xml:space="preserve">
          <source>The Lasso optimization function varies for mono and multi-outputs.</source>
          <target state="translated">올가미 최적화 기능은 모노 및 멀티 출력에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="3c1b2c3b1551762e6f1da2d8a29e9acf6404c123" translate="yes" xml:space="preserve">
          <source>The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where number of features is greater than number of samples. Elsewhere prefer cd which is more numerically stable.</source>
          <target state="translated">사용할 올가미 솔버 : 좌표 하강 또는 LARS. 피처 수가 샘플 수보다 큰 희소 기본 그래프에는 LARS를 사용하십시오. 다른 곳에서는 더 수치 적으로 안정적인 cd를 선호합니다.</target>
        </trans-unit>
        <trans-unit id="2de8b3228aef7c3b031e79cb56d858268d66f64e" translate="yes" xml:space="preserve">
          <source>The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where p &amp;gt; n. Elsewhere prefer cd which is more numerically stable.</source>
          <target state="translated">사용할 올가미 솔버 : 좌표 하강 또는 LARS. p&amp;gt; n 인 매우 희박한 기본 그래프에는 LARS를 사용하십시오. 다른 곳에서는 더 수치 적으로 안정적인 cd를 선호합니다.</target>
        </trans-unit>
        <trans-unit id="6c4c48b93f6a66f7991382ba54fcfe5fb18a6ebb" translate="yes" xml:space="preserve">
          <source>The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt;&lt;code&gt;ledoit_wolf&lt;/code&gt;&lt;/a&gt; function of the &lt;code&gt;sklearn.covariance&lt;/code&gt; package, or it can be otherwise obtained by fitting a &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt;&lt;code&gt;LedoitWolf&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">공분산 행렬의 Ledoit-Wolf 추정기 는 &lt;code&gt;sklearn.covariance&lt;/code&gt; 패키지 의 &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt; &lt;code&gt;ledoit_wolf&lt;/code&gt; &lt;/a&gt; 함수를 사용하여 샘플에서 계산 하거나 &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt; &lt;code&gt;LedoitWolf&lt;/code&gt; &lt;/a&gt; 객체를 동일한 샘플 에 피팅하여 얻을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="aec2f6a4d0fb4e3ecc2ba56ea33e122b851a535b" translate="yes" xml:space="preserve">
          <source>The Linnerud dataset constains two small dataset:</source>
          <target state="translated">Linnerud 데이터 세트는 두 개의 작은 데이터 세트를 구성합니다.</target>
        </trans-unit>
        <trans-unit id="0460202c91e1eee28482597354a8366af4e5eb02" translate="yes" xml:space="preserve">
          <source>The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for novelty detection. Note that when LOF is used for novelty detection you MUST not use predict, decision_function and score_samples on the training set as this would lead to wrong results. You must only use these methods on new unseen data (which are not in the training set). See &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;User Guide&lt;/a&gt;: for details on the difference between outlier detection and novelty detection and how to use LOF for outlier detection.</source>
          <target state="translated">LOF (Local Outlier Factor) 알고리즘은 감독되지 않은 이상 탐지 방법으로, 주변에 대한 주어진 데이터 포인트의 로컬 밀도 편차를 계산합니다. 이웃보다 밀도가 실질적으로 낮은 샘플을 특이 치로 간주합니다. 이 예는 참신 탐지에 LOF를 사용하는 방법을 보여줍니다. LOF가 참신 탐지에 사용되는 경우 훈련 결과에 대해 predict, decision_function 및 score_samples를 사용해서는 안됩니다. 결과가 잘못 될 수 있습니다. 보이지 않는 새로운 데이터 (훈련 세트에없는 데이터)에만 이러한 방법을 사용해야합니다. 이상치 탐지와 신규성 탐지의 차이점 및 이상치 탐지에 LOF를 사용하는 방법에 대한 자세한 내용 은 &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;사용자 안내서&lt;/a&gt; :를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="d0e3831e71a419f2c6e3df80ad462189c4208703" translate="yes" xml:space="preserve">
          <source>The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for outlier detection which is the default use case of this estimator in scikit-learn. Note that when LOF is used for outlier detection it has no predict, decision_function and score_samples methods. See &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;User Guide&lt;/a&gt;: for details on the difference between outlier detection and novelty detection and how to use LOF for novelty detection.</source>
          <target state="translated">LOF (Local Outlier Factor) 알고리즘은 감독되지 않은 이상 탐지 방법으로, 주변에 대한 주어진 데이터 포인트의 로컬 밀도 편차를 계산합니다. 이웃보다 밀도가 실질적으로 낮은 샘플을 특이 치로 간주합니다. 이 예는 scikit-learn에서이 추정기의 기본 사용 사례 인 이상치 탐지에 LOF를 사용하는 방법을 보여줍니다. LOF가 이상치 탐지에 사용될 때는 predict, decision_function 및 score_samples 메소드가 없습니다. 특이 치 탐지와 참신 탐지의 차이점과 참신 탐지에 LOF를 사용하는 방법에 대한 자세한 내용은 사용 &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;설명서를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="e334ea68d0fb9bc258e189ffac7524671f8600d5" translate="yes" xml:space="preserve">
          <source>The MLLE algorithm comprises three stages:</source>
          <target state="translated">MLLE 알고리즘은 3 단계로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="514baa8efb372e193dd4209109d33c4d6c755f88" translate="yes" xml:space="preserve">
          <source>The Matplotlib Figure object.</source>
          <target state="translated">Matplotlib Figure 객체입니다.</target>
        </trans-unit>
        <trans-unit id="5adf2471ff49dbf27acc1a8b4862b530e5b52366" translate="yes" xml:space="preserve">
          <source>The Matthews correlation coefficient (+1 represents a perfect prediction, 0 an average random prediction and -1 and inverse prediction).</source>
          <target state="translated">Matthews 상관 계수 (+1은 완벽한 예측, 0은 평균 임의 예측, -1 및 역 예측)를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="21efcbf7188af02e8a17818fd892c631c3cc436c" translate="yes" xml:space="preserve">
          <source>The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary and multiclass classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient. [source: Wikipedia]</source>
          <target state="translated">Matthews 상관 계수는 이진 및 멀티 클래스 분류의 품질 측정으로 기계 학습에 사용됩니다. 그것은 참과 거짓 긍정과 부정을 고려하고 일반적으로 클래스의 크기가 매우 다른 경우에도 사용할 수있는 균형 잡힌 측정으로 간주됩니다. MCC는 본질적으로 -1과 +1 사이의 상관 계수 값입니다. +1의 계수는 완전 예측, 0은 평균 랜덤 예측, -1은 역 예측을 나타냅니다. 통계량은 파이 계수라고도합니다. [출처 : Wikipedia]</target>
        </trans-unit>
        <trans-unit id="d3bca24cc7fb644916020d52b90a1ddb7136c5be" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant covariance estimator is to be applied on Gaussian-distributed data, but could still be relevant on data drawn from a unimodal, symmetric distribution. It is not meant to be used with multi-modal data (the algorithm used to fit a MinCovDet object is likely to fail in such a case). One should consider projection pursuit methods to deal with multi-modal datasets.</source>
          <target state="translated">최소 공분산 결정 공분산 추정값은 가우스 분포 데이터에 적용되지만 단봉 대칭 대칭 분포에서 가져온 데이터와 관련이있을 수 있습니다. 다중 모달 데이터와 함께 사용하기위한 것이 아닙니다 (이 경우 MinCovDet 객체에 맞는 알고리즘이 실패 할 수 있음). 멀티 모달 데이터 셋을 다루기위한 프로젝션 추구 방법을 고려해야합니다.</target>
        </trans-unit>
        <trans-unit id="451133ac20cdcd95892f17968e1c348a4b0f4b8f" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">PJRousseuw는 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; 에서 최소 공분산 결정 인자 (MCD)를 도입했습니다 .</target>
        </trans-unit>
        <trans-unit id="d811e159b509e69af06fa8b1a6b421d8a9ea40ca" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in [1].</source>
          <target state="translated">PJRousseuw는 [1]에서 최소 공분산 결정 인자 (MCD)를 도입했습니다.</target>
        </trans-unit>
        <trans-unit id="92f2af418b12a7a58727045874e736340d33efbd" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust estimator of a data set&amp;rsquo;s covariance introduced by P.J. Rousseeuw in &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;[3]&lt;/a&gt;. The idea is to find a given proportion (h) of &amp;ldquo;good&amp;rdquo; observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations (&amp;ldquo;consistency step&amp;rdquo;). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set (&amp;ldquo;reweighting step&amp;rdquo;).</source>
          <target state="translated">최소 공분산 결정 인자 추정기는 &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;[3]의&lt;/a&gt; PJ Rousseeuw에 의해 도입 된 데이터 세트의 공분산에 대한 강력한 추정기입니다 . 아이디어는 특이 치가 아닌 &quot;좋은&quot;관측치의 주어진 비율 (h)을 찾고 경험적 공분산 행렬을 계산하는 것입니다. 이 경험적 공분산 행렬은 수행 된 관측의 선택을 보상하기 위해 재조정된다 ( &quot;일관성 단계&quot;). 최소 공분산 결정 요인 추정값을 계산 한 후, Mahalanobis 거리에 따라 관측치에 가중치를 부여하여 데이터 세트의 공분산 행렬의 가중치를 재 추정 할 수 있습니다 ( &quot;가중치 단계&quot;).</target>
        </trans-unit>
        <trans-unit id="26a5ec87ffc45e2d236a64ca891cc8c307910b23" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to \(\frac{n_\text{samples} - n_\text{features}-1}{2}\) outliers) estimator of covariance. The idea is to find \(\frac{n_\text{samples} + n_\text{features}+1}{2}\) observations whose empirical covariance has the smallest determinant, yielding a &amp;ldquo;pure&amp;rdquo; subset of observations from which to compute standards estimates of location and covariance. After a correction step aiming at compensating the fact that the estimates were learned from only a portion of the initial data, we end up with robust estimates of the data set location and covariance.</source>
          <target state="translated">최소 공분산 결정 인자 추정값은 강력하고 높은 고 장점입니다 (즉, 최대 \ (\ frac {n_ \ text {samples}-n_ \ text {features}-). 1} {2} \) 특이 치) 공분산 추정값. 아이디어의 경험적 공분산이 가장 작은 결정 인 \ (\ frac {n_ \ text {samples} + n_ \ text {features} +1} {2} \) 관측 값을 찾아서 &quot;순수한&quot;관측 값의 하위 집합을 생성하는 것입니다. 위치 및 공분산의 표준 추정치를 계산합니다. 초기 데이터의 일부에서만 추정값을 학습했다는 사실을 보상하기 위해 수정 단계를 수행 한 후 데이터 세트 위치 및 공분산에 대한 강력한 추정치가됩니다.</target>
        </trans-unit>
        <trans-unit id="868c6b821132444289ad5893f70525225594997c" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to \(\frac{n_\text{samples}-n_\text{features}-1}{2}\) outliers) estimator of covariance. The idea is to find \(\frac{n_\text{samples}+n_\text{features}+1}{2}\) observations whose empirical covariance has the smallest determinant, yielding a &amp;ldquo;pure&amp;rdquo; subset of observations from which to compute standards estimates of location and covariance.</source>
          <target state="translated">최소 공분산 결정 인자 추정값은 강력하고 높은 고 장점입니다 (즉, \ (\ frac {n_ \ text {samples} -n_ \ text {features}-)까지 오염도가 높은 데이터 세트의 공분산 행렬을 추정하는 데 사용할 수 있습니다. 1} {2} \) 특이 치) 공분산 추정값. 아이디어의 경험적 공분산이 가장 작은 결정 인 \ (\ frac {n_ \ text {samples} + n_ \ text {features} +1} {2} \) 관측 값을 찾아서 &quot;순수한&quot;관측 값의 하위 집합을 생성하는 것입니다. 위치 및 공분산의 표준 추정치를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="b2ce758bd900de6631654a32b9b8962161ffb45f" translate="yes" xml:space="preserve">
          <source>The Mutual Information is a measure of the similarity between two labels of the same data. Where \(|U_i|\) is the number of the samples in cluster \(U_i\) and \(|V_j|\) is the number of the samples in cluster \(V_j\), the Mutual Information between clusterings \(U\) and \(V\) is given as:</source>
          <target state="translated">상호 정보는 동일한 데이터의 두 레이블 간의 유사성을 측정 한 것입니다. 여기서 \ (| U_i | \)는 클러스터 \ (U_i \)의 샘플 수이고 \ (| V_j | \)는 클러스터 \ (V_j \)의 샘플 수이며, 클러스터링 간의 상호 정보 \ ( U \) 및 \ (V \)는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="4607a569600ef039f2071e80730b55efccd8dd8d" translate="yes" xml:space="preserve">
          <source>The Nystroem method, as implemented in &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is a general method for low-rank approximations of kernels. It achieves this by essentially subsampling the data on which the kernel is evaluated. By default &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; uses the &lt;code&gt;rbf&lt;/code&gt; kernel, but it can use any kernel function or a precomputed kernel matrix. The number of samples used - which is also the dimensionality of the features computed - is given by the parameter &lt;code&gt;n_components&lt;/code&gt;.</source>
          <target state="translated">Nystroem에서 구현 된 &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; 방법은 커널의 낮은 순위 근사를위한 일반적인 방법입니다. 커널이 평가되는 데이터를 본질적으로 서브 샘플링하여이를 달성합니다. 기본적으로 &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; 은 &lt;code&gt;rbf&lt;/code&gt; 커널을 사용하지만 모든 커널 기능이나 사전 계산 된 커널 매트릭스를 사용할 수 있습니다. 사용 된 샘플의 수는 계산 된 형상의 차원이기도하며 매개 변수 &lt;code&gt;n_components&lt;/code&gt; 로 제공 됩니다.</target>
        </trans-unit>
        <trans-unit id="ad47de32445041d6ef99f77621d867a03cd9751f" translate="yes" xml:space="preserve">
          <source>The OAS estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt;&lt;code&gt;oas&lt;/code&gt;&lt;/a&gt; function of the &lt;code&gt;sklearn.covariance&lt;/code&gt; package, or it can be otherwise obtained by fitting an &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;OAS&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">공분산 행렬의 OAS 추정기는 &lt;code&gt;sklearn.covariance&lt;/code&gt; 패키지 의 &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt; &lt;code&gt;oas&lt;/code&gt; &lt;/a&gt; 함수를 사용하여 샘플에서 계산 하거나 &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt; &lt;code&gt;OAS&lt;/code&gt; &lt;/a&gt; 객체를 동일한 샘플 에 피팅하여 얻을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="6f45a4e82bc4c1b489f40318ecca4028acd0f46e" translate="yes" xml:space="preserve">
          <source>The One-Class SVM has been introduced by Sch&amp;ouml;lkopf et al. for that purpose and implemented in the &lt;a href=&quot;svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; module in the &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; object. It requires the choice of a kernel and a scalar parameter to define a frontier. The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter. This is the default in the scikit-learn implementation. The \(\nu\) parameter, also known as the margin of the One-Class SVM, corresponds to the probability of finding a new, but regular, observation outside the frontier.</source>
          <target state="translated">One-Class SVM은 Sch&amp;ouml;lkopf et al. 이를 위해 &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;svm.OneClassSVM&lt;/code&gt; &lt;/a&gt; 객체 의 &lt;a href=&quot;svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; 모듈에서 구현 합니다. 프론티어를 정의하려면 커널과 스칼라 매개 변수를 선택해야합니다. RBF 커널은 대역폭 매개 변수를 설정하기위한 정확한 공식이나 알고리즘이 없지만 일반적으로 선택됩니다. 이것이 scikit-learn 구현의 기본값입니다. One-Class SVM의 마진이라고도하는 \ (\ nu \) 매개 변수는 프론티어 외부에서 새롭지 만 규칙적인 관찰을 찾을 가능성에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="ce00ae4f245475e7026810e89464783863843edd" translate="yes" xml:space="preserve">
          <source>The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction.</source>
          <target state="translated">로지스틱 회귀는 예측을 수행하는 반면 PCA는 감독되지 않은 차원 축소를 수행합니다.</target>
        </trans-unit>
        <trans-unit id="33c279bdcb922f55f225b113a682e1a5cf28cc6d" translate="yes" xml:space="preserve">
          <source>The RBF kernel is a stationary kernel. It is also known as the &amp;ldquo;squared exponential&amp;rdquo; kernel. It is parameterized by a length-scale parameter length_scale&amp;gt;0, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">RBF 커널은 고정 커널입니다. &quot;제곱 지수&quot;커널이라고도합니다. 스칼라 (커널의 등방성 변형) 또는 입력 X (커널의 이방성 변형)와 동일한 수의 차원을 가진 벡터 일 수있는 길이 스케일 매개 변수 length_scale&amp;gt; 0으로 매개 변수화됩니다. 커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="f47997c4010f2e20accd04690970d6547a71bfc2" translate="yes" xml:space="preserve">
          <source>The RBF kernel will produce a fully connected graph which is represented in memory by a dense matrix. This matrix may be very large and combined with the cost of performing a full matrix multiplication calculation for each iteration of the algorithm can lead to prohibitively long running times. On the other hand, the KNN kernel will produce a much more memory-friendly sparse matrix which can drastically reduce running times.</source>
          <target state="translated">RBF 커널은 밀도가 높은 매트릭스로 메모리에 표시되는 완전히 연결된 그래프를 생성합니다. 이 행렬은 매우 클 수 있으며 알고리즘의 각 반복에 대해 전체 행렬 곱셈 계산을 수행하는 비용과 결합하면 실행 시간이 엄청나게 길어질 수 있습니다. 반면, KNN 커널은 훨씬 더 메모리 친화적 인 희소 행렬을 생성하여 실행 시간을 크게 줄일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="512de356729b7d551b43d5d76bd8681dea941e19" translate="yes" xml:space="preserve">
          <source>The RBM tries to maximize the likelihood of the data using a particular graphical model. The parameter learning algorithm used (&lt;a href=&quot;#sml&quot;&gt;Stochastic Maximum Likelihood&lt;/a&gt;) prevents the representations from straying far from the input data, which makes them capture interesting regularities, but makes the model less useful for small datasets, and usually not useful for density estimation.</source>
          <target state="translated">RBM은 특정 그래픽 모델을 사용하여 데이터의 가능성을 최대화하려고합니다. 사용 된 모수 학습 알고리즘 ( &lt;a href=&quot;#sml&quot;&gt;Stochastic Maximum Likelihood&lt;/a&gt; )은 표현이 입력 데이터에서 멀리 벗어나는 것을 방지하여 흥미로운 규칙을 캡처하지만 모형이 작은 데이터 세트에는 유용하지 않으며 밀도 추정에는 유용하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="95d48012cfd5cdfda48a80ad45de8292db32665c" translate="yes" xml:space="preserve">
          <source>The R^2 score or ndarray of scores if &amp;lsquo;multioutput&amp;rsquo; is &amp;lsquo;raw_values&amp;rsquo;.</source>
          <target state="translated">'multioutput'이 'raw_values'인 경우 R ^ 2 점수 또는 점수의 ndarray입니다.</target>
        </trans-unit>
        <trans-unit id="57cf85e67b74d4b36ce4e00a60cc2afb37409d42" translate="yes" xml:space="preserve">
          <source>The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.</source>
          <target state="translated">Rand Index는 예측 및 실제 군집화에서 동일하거나 다른 군집에 할당 된 모든 표본 쌍과 계수 쌍을 고려하여 두 군집 간의 유사성 측정을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="21d3f8892ca41bf337636e90759152be22d9788c" translate="yes" xml:space="preserve">
          <source>The RandomTreesEmbedding, from the &lt;a href=&quot;../../modules/classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module, is not technically a manifold embedding method, as it learn a high-dimensional representation on which we apply a dimensionality reduction method. However, it is often useful to cast a dataset into a representation in which the classes are linearly-separable.</source>
          <target state="translated">&lt;a href=&quot;../../modules/classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; 모듈 의 RandomTreesEmbedding 은 차원 축소 방법을 적용하는 고차원 표현을 배우기 때문에 기술적으로 매니 폴드 임베딩 방법이 아닙니다. 그러나 데이터 세트를 클래스를 선형으로 분리 할 수있는 표현으로 캐스트하는 것이 종종 유용합니다.</target>
        </trans-unit>
        <trans-unit id="ddaea903844387569c2d558e504bb67163af0e53" translate="yes" xml:space="preserve">
          <source>The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length-scales. It is parameterized by a length-scale parameter length_scale&amp;gt;0 and a scale mixture parameter alpha&amp;gt;0. Only the isotropic variant where length_scale is a scalar is supported at the moment. The kernel given by:</source>
          <target state="translated">RationalQuadratic 커널은 특성 길이가 다른 RBF 커널의 스케일 혼합 (무한 합)으로 볼 수 있습니다. 길이 스케일 파라미터 length_scale&amp;gt; 0 및 스케일 혼합 파라미터 alpha&amp;gt; 0으로 파라미터 화됩니다. length_scale이 스칼라 인 등방성 변형 만 지원됩니다. 커널은 다음을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="66cebb865fc92b18216814d7cca47981b6cb6b41" translate="yes" xml:space="preserve">
          <source>The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a multidimensional scaling algorithm which minimizes an objective function (the &lt;em&gt;stress&lt;/em&gt;) using a majorization technique. Stress majorization, also known as the Guttman Transform, guarantees a monotone convergence of stress, and is more powerful than traditional techniques such as gradient descent.</source>
          <target state="translated">SMACOF (COmplicated Function을 MAjorizing하여 스케일링) 알고리즘은 주요 기술을 사용하여 목적 함수 ( &lt;em&gt;스트레스&lt;/em&gt; )를 최소화하는 다차원 스케일링 알고리즘입니다 . Guttman Transform이라고도하는 응력 주요 화는 응력의 모노톤 수렴을 보장하며 경사 하강과 같은 전통적인 기술보다 강력합니다.</target>
        </trans-unit>
        <trans-unit id="e2c4971923aff3f7fe2f8678fb678a8a1fe12716" translate="yes" xml:space="preserve">
          <source>The SMACOF algorithm for metric MDS can summarized by the following steps:</source>
          <target state="translated">메트릭 MDS에 대한 SMACOF 알고리즘은 다음 단계로 요약 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9991accadfafce42fd0097ac5bf1452a92166503" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient &lt;em&gt;s&lt;/em&gt; for a single sample is then given as:</source>
          <target state="translated">실루엣 계수는 &lt;em&gt;s의&lt;/em&gt; 단일 샘플에 대해 다음과 같이 주어진다 :</target>
        </trans-unit>
        <trans-unit id="977018d27e576be441891586d29895a67820e9bb" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample.</source>
          <target state="translated">샘플 세트에 대한 실루엣 계수는 각 샘플에 대한 실루엣 계수의 평균으로 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="a8aa62253fcb64807b1a0f2a014811a76f4c09cc" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is a measure of how well samples are clustered with samples that are similar to themselves. Clustering models with a high Silhouette Coefficient are said to be dense, where samples in the same cluster are similar to each other, and well separated, where samples in different clusters are not very similar to each other.</source>
          <target state="translated">실루엣 계수는 샘플이 자신과 유사한 샘플로 얼마나 잘 클러스터링되는지를 측정합니다. 높은 실루엣 계수를 갖는 군집 모델은 동일한 군집의 샘플이 서로 유사하고 서로 다른 군집의 샘플이 서로 매우 유사하지 않은 경우 분리되어 밀도가 높다고합니다.</target>
        </trans-unit>
        <trans-unit id="5da8604230381cdf66d3aec27ea08ace466e606c" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is calculated using the mean intra-cluster distance (&lt;code&gt;a&lt;/code&gt;) and the mean nearest-cluster distance (&lt;code&gt;b&lt;/code&gt;) for each sample. The Silhouette Coefficient for a sample is &lt;code&gt;(b - a) / max(a,
b)&lt;/code&gt;. Note that Silhouette Coefficient is only defined if number of labels is 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</source>
          <target state="translated">실루엣 계수는 각 샘플에 대한 평균 클러스터 내 거리 ( &lt;code&gt;a&lt;/code&gt; ) 및 평균 가장 가까운 클러스터 거리 ( &lt;code&gt;b&lt;/code&gt; )를 사용하여 계산됩니다 . 표본의 실루엣 계수는 &lt;code&gt;(b - a) / max(a, b)&lt;/code&gt; 입니다. Silhouette Coefficient는 레이블 수가 2 &amp;lt;= n_labels &amp;lt;= n_samples-1 인 경우에만 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="4273a368cac49099b8caa7c819b214a9d85c0ff4" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is calculated using the mean intra-cluster distance (&lt;code&gt;a&lt;/code&gt;) and the mean nearest-cluster distance (&lt;code&gt;b&lt;/code&gt;) for each sample. The Silhouette Coefficient for a sample is &lt;code&gt;(b - a) / max(a,
b)&lt;/code&gt;. To clarify, &lt;code&gt;b&lt;/code&gt; is the distance between a sample and the nearest cluster that the sample is not a part of. Note that Silhouette Coefficient is only defined if number of labels is 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</source>
          <target state="translated">실루엣 계수는 각 샘플에 대한 평균 클러스터 내 거리 ( &lt;code&gt;a&lt;/code&gt; ) 및 평균 가장 가까운 클러스터 거리 ( &lt;code&gt;b&lt;/code&gt; )를 사용하여 계산됩니다 . 표본의 실루엣 계수는 &lt;code&gt;(b - a) / max(a, b)&lt;/code&gt; 입니다. 명확히하기 위해, &lt;code&gt;b&lt;/code&gt; 는 샘플과 샘플이 포함되지 않은 가장 가까운 군집 사이의 거리입니다. Silhouette Coefficient는 레이블 수가 2 &amp;lt;= n_labels &amp;lt;= n_samples-1 인 경우에만 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="74a577cf3f9facf0caa36a3cd46ce91a25197ef1" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">Silhouette Coefficient는 DBSCAN을 통해 얻은 것과 같은 밀도 기반 클러스터와 같은 다른 개념의 클러스터보다 볼록 클러스터에 일반적으로 더 높습니다.</target>
        </trans-unit>
        <trans-unit id="b08373a116f8a28b8f59b4a939f8e86bc18a285c" translate="yes" xml:space="preserve">
          <source>The Spearman correlation coefficient is estimated from the data, and the sign of the resulting estimate is used as the result.</source>
          <target state="translated">Spearman 상관 계수는 데이터에서 추정되며 결과 추정의 부호가 결과로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="1aca461bec942a3ea49c28ca350e80c03610ed92" translate="yes" xml:space="preserve">
          <source>The Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages:</source>
          <target state="translated">스펙트럼 임베딩 (Laplacian Eigenmaps) 알고리즘은 3 단계로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="530048073ab2f0fd4beae1a41150629fce3bbc04" translate="yes" xml:space="preserve">
          <source>The TF-IDF vectorized posts form a word frequency matrix, which is then biclustered using Dhillon&amp;rsquo;s Spectral Co-Clustering algorithm. The resulting document-word biclusters indicate subsets words used more often in those subsets documents.</source>
          <target state="translated">TF-IDF 벡터화 된 포스트는 워드 주파수 매트릭스를 형성 한 다음 Dhillon의 Spectral Co-Clustering 알고리즘을 사용하여 bicluster됩니다. 결과적인 문서 단어 biclusters는 해당 하위 집합 문서에서 더 자주 사용되는 하위 집합 단어를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="a51b4bd7337a8a43bf76bb00c70355d636e98cf2" translate="yes" xml:space="preserve">
          <source>The V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean &lt;a href=&quot;#b2011&quot; id=&quot;id15&quot;&gt;[B2011]&lt;/a&gt;.</source>
          <target state="translated">V- 측정은 실제로 상기 논의 된 상호 정보 (NMI)와 동일하며, 집계 함수는 산술 평균이다 &lt;a href=&quot;#b2011&quot; id=&quot;id15&quot;&gt;[B2011]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="28e22a992327910c0281c7997fd0381a055b2da5" translate="yes" xml:space="preserve">
          <source>The V-measure is the harmonic mean between homogeneity and completeness:</source>
          <target state="translated">V- 측정은 균질성과 완전성 사이의 조화 평균입니다.</target>
        </trans-unit>
        <trans-unit id="9d831d6fbe54cab7c78889ea027eda02af846327" translate="yes" xml:space="preserve">
          <source>The Yeo-Johnson transform is given by:</source>
          <target state="translated">Yeo-Johnson 변환은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="dd310b3ef8cade9b7b44463cf24d9960d1a07902" translate="yes" xml:space="preserve">
          <source>The \(\ell = \lceil \log_2 k \rceil\) singular vectors, starting from the second, provide the desired partitioning information. They are used to form the matrix \(Z\):</source>
          <target state="translated">\ (\ ell = \ lceil \ log_2 k \ rceil \) 특이 벡터는 두 번째부터 시작하여 원하는 분할 정보를 제공합니다. 행렬 \ (Z \)를 구성하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="6bae7eee8e57958cb7a015c09e85982ab959fe35" translate="yes" xml:space="preserve">
          <source>The \(k\)-neighbors classification in &lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt; is the most commonly used technique. The optimal choice of the value \(k\) is highly data-dependent: in general a larger \(k\) suppresses the effects of noise, but makes the classification boundaries less distinct.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt; &lt;code&gt;KNeighborsClassifier&lt;/code&gt; &lt;/a&gt; 의 \ (k \)-이웃 분류 는 가장 일반적으로 사용되는 기술입니다. \ (k \) 값의 최적 선택은 데이터에 따라 크게 달라집니다. 일반적으로 큰 \ (k \)는 노이즈의 영향을 억제하지만 분류 경계를 덜 뚜렷하게 만듭니다.</target>
        </trans-unit>
        <trans-unit id="7ea48a47287b56e3f66c9ec81d51c291e8fedf59" translate="yes" xml:space="preserve">
          <source>The above vectorization scheme is simple but the fact that it holds an &lt;strong&gt;in- memory mapping from the string tokens to the integer feature indices&lt;/strong&gt; (the &lt;code&gt;vocabulary_&lt;/code&gt; attribute) causes several &lt;strong&gt;problems when dealing with large datasets&lt;/strong&gt;:</source>
          <target state="translated">위의 벡터화 체계는 간단하지만 &lt;strong&gt;문자열 토큰에서 정수 피처 인덱스&lt;/strong&gt; ( &lt;code&gt;vocabulary_&lt;/code&gt; 속성) &lt;strong&gt;로의 메모리 내 맵핑을&lt;/strong&gt; 보유한다는 사실은 &lt;strong&gt;큰 데이터 세트를 처리 할 때&lt;/strong&gt; 몇 가지 &lt;strong&gt;문제점을&lt;/strong&gt; 발생시킵니다 .</target>
        </trans-unit>
        <trans-unit id="58279b870becff96bab0fc34e17d7f045af03b34" translate="yes" xml:space="preserve">
          <source>The abstract base class for all kernels is &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt;. Kernel implements a similar interface as &lt;code&gt;Estimator&lt;/code&gt;, providing the methods &lt;code&gt;get_params()&lt;/code&gt;, &lt;code&gt;set_params()&lt;/code&gt;, and &lt;code&gt;clone()&lt;/code&gt;. This allows setting kernel values also via meta-estimators such as &lt;code&gt;Pipeline&lt;/code&gt; or &lt;code&gt;GridSearch&lt;/code&gt;. Note that due to the nested structure of kernels (by applying kernel operators, see below), the names of kernel parameters might become relatively complicated. In general, for a binary kernel operator, parameters of the left operand are prefixed with &lt;code&gt;k1__&lt;/code&gt; and parameters of the right operand with &lt;code&gt;k2__&lt;/code&gt;. An additional convenience method is &lt;code&gt;clone_with_theta(theta)&lt;/code&gt;, which returns a cloned version of the kernel but with the hyperparameters set to &lt;code&gt;theta&lt;/code&gt;. An illustrative example:</source>
          <target state="translated">모든 커널의 추상 기본 클래스는 &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt; 입니다. 커널은 &lt;code&gt;Estimator&lt;/code&gt; 와 유사한 인터페이스를 구현하여 &lt;code&gt;get_params()&lt;/code&gt; , &lt;code&gt;set_params()&lt;/code&gt; 및 &lt;code&gt;clone()&lt;/code&gt; 메소드를 제공합니다 . 이를 통해 &lt;code&gt;Pipeline&lt;/code&gt; 또는 &lt;code&gt;GridSearch&lt;/code&gt; 와 같은 메타 추정기를 통해 커널 값을 설정할 수 있습니다 . 커널의 중첩 구조 (커널 연산자를 적용하여 아래 참조)로 인해 커널 매개 변수의 이름이 상대적으로 복잡해질 수 있습니다. 일반적으로 이진 커널 연산자의 경우 왼쪽 피연산자의 매개 변수 앞에 &lt;code&gt;k1__&lt;/code&gt; 이 있고 오른쪽 피연산자의 매개 변수 앞에 &lt;code&gt;k2__&lt;/code&gt; 가 있습니다. 있습니다. 추가적인 편의 방법은 &lt;code&gt;clone_with_theta(theta)&lt;/code&gt; 커널의 복제 된 버전을 반환하지만 하이퍼 파라미터는 &lt;code&gt;theta&lt;/code&gt; 로 설정됩니다 . 예시적인 예 :</target>
        </trans-unit>
        <trans-unit id="d6dfb2a865fc7561c9990f60831354c8c2b2acf1" translate="yes" xml:space="preserve">
          <source>The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: &amp;ldquo;Robust Linear Programming Discrimination of Two Linearly Inseparable Sets&amp;rdquo;, Optimization Methods and Software 1, 1992, 23-34].</source>
          <target state="translated">3 차원 공간에서 분리 평면을 얻는 데 사용되는 실제 선형 프로그램은 다음에 설명되어 있습니다. 34].</target>
        </trans-unit>
        <trans-unit id="5c16c787f9e7f87c21ce4f86533b13082a6022ce" translate="yes" xml:space="preserve">
          <source>The actual number of iteration performed by the solver. Only returned if &lt;code&gt;return_n_iter&lt;/code&gt; is True.</source>
          <target state="translated">솔버가 수행 한 실제 반복 횟수입니다. &lt;code&gt;return_n_iter&lt;/code&gt; 가 True 인 경우에만 반환됩니다 .</target>
        </trans-unit>
        <trans-unit id="c6a597b70b1c3dba7fc91a33b2b458a1718ae376" translate="yes" xml:space="preserve">
          <source>The actual number of iterations to reach the stopping criterion.</source>
          <target state="translated">중지 기준에 도달하기위한 실제 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="be3e6ed927427ff958a3dc691ec6f2ca38b153e8" translate="yes" xml:space="preserve">
          <source>The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over every binary fit.</source>
          <target state="translated">중지 기준에 도달하기위한 실제 반복 횟수입니다. 멀티 클래스 적합의 경우 모든 이진 적합보다 최대 값입니다.</target>
        </trans-unit>
        <trans-unit id="905122fa4ae495ec172d69201d288d9cbd8eb107" translate="yes" xml:space="preserve">
          <source>The actual number of neighbors used for &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt;&lt;code&gt;kneighbors&lt;/code&gt;&lt;/a&gt; queries.</source>
          <target state="translated">&lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt; &lt;code&gt;kneighbors&lt;/code&gt; &lt;/a&gt; 쿼리에 사용 된 실제 이웃 수입니다 .</target>
        </trans-unit>
        <trans-unit id="99f948dd8ff8ebdf8ccc1268f27f992c4d54a1e0" translate="yes" xml:space="preserve">
          <source>The actual number of samples</source>
          <target state="translated">실제 샘플 수</target>
        </trans-unit>
        <trans-unit id="257683b273bf01e4ad40af7f18f19c426c837478" translate="yes" xml:space="preserve">
          <source>The additive chi squared kernel as used here is given by</source>
          <target state="translated">여기에 사용 된 추가 카이 제곱 커널은</target>
        </trans-unit>
        <trans-unit id="721e722946fa16cdc1d4e80a122a59df383d3b35" translate="yes" xml:space="preserve">
          <source>The additive chi squared kernel is a kernel on histograms, often used in computer vision.</source>
          <target state="translated">추가 카이 제곱 커널은 히스토그램의 커널로 컴퓨터 비전에 자주 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="509930cd9616089e31b59f87837fde9a2850682a" translate="yes" xml:space="preserve">
          <source>The additive version of this kernel</source>
          <target state="translated">이 커널의 추가 버전</target>
        </trans-unit>
        <trans-unit id="479259eedb54024948ef963b0d114b00fbb05f1b" translate="yes" xml:space="preserve">
          <source>The adjacency matrix is used to compute a normalized graph Laplacian whose spectrum (especially the eigenvectors associated to the smallest eigenvalues) has an interpretation in terms of minimal number of cuts necessary to split the graph into comparably sized components.</source>
          <target state="translated">인접 행렬은 스펙트럼 (특히 가장 작은 고유 값과 관련된 고유 벡터)이 그래프를 비슷한 크기의 구성 요소로 나누는 데 필요한 최소한의 컷 수로 해석되는 정규화 된 그래프 라플라시안을 계산하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="b4f6e52ff52334d95b6f16934aecd26800758ca5" translate="yes" xml:space="preserve">
          <source>The adjacency matrix of the graph to embed.</source>
          <target state="translated">포함 할 그래프의 인접 행렬입니다.</target>
        </trans-unit>
        <trans-unit id="f8c06d6c9bebb104301c962acd25687eac4074b4" translate="yes" xml:space="preserve">
          <source>The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation).</source>
          <target state="translated">따라서 조정 된 Rand 지수는 군집 및 표본 수에 관계없이 임의의 표식에 대해 0.0에 가깝고 군집이 동일 할 때 정확히 1.0 (최대 순열)에 가까운 값을 갖도록 보장됩니다.</target>
        </trans-unit>
        <trans-unit id="83e2303d70450b875b04241e83f4a4153932e0a9" translate="yes" xml:space="preserve">
          <source>The advantage of using approximate explicit feature maps compared to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_trick&quot;&gt;kernel trick&lt;/a&gt;, which makes use of feature maps implicitly, is that explicit mappings can be better suited for online learning and can significantly reduce the cost of learning with very large datasets. Standard kernelized SVMs do not scale well to large datasets, but using an approximate kernel map it is possible to use much more efficient linear SVMs. In particular, the combination of kernel map approximations with &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; can make non-linear learning on large datasets possible.</source>
          <target state="translated">암시 적으로 기능 맵을 사용 하는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_trick&quot;&gt;커널 트릭에&lt;/a&gt; 비해 대략적인 명시 적 기능 맵을 사용할 경우의 장점은 명시 적 맵핑이 온라인 학습에 더 적합 할 수 있고 매우 큰 데이터 세트에서 학습 비용을 크게 줄일 수 있다는 것입니다. 표준 커널 화 된 SVM은 대규모 데이터 세트로 확장 할 수 없지만 대략적인 커널 맵을 사용하면 훨씬 더 효율적인 선형 SVM을 사용할 수 있습니다. 특히 커널 맵 근사치와 &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 를 결합하면 대규모 데이터 세트에 대한 비선형 학습이 가능합니다.</target>
        </trans-unit>
        <trans-unit id="e8cf8d3d0c8dfd7b6a3c8351c56803f2fdf0d2d8" translate="yes" xml:space="preserve">
          <source>The advantages of Bayesian Regression are:</source>
          <target state="translated">베이지안 회귀의 장점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="d7269fefe21b18a1d9b1e8a11da2f35edf3e36ec" translate="yes" xml:space="preserve">
          <source>The advantages of GBRT are:</source>
          <target state="translated">GBRT의 장점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="6bdcff89c23af609c3f964058bbddc38e45558f8" translate="yes" xml:space="preserve">
          <source>The advantages of Gaussian processes are:</source>
          <target state="translated">가우스 프로세스의 장점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c632bf8abb99b63a04554183dcc32e66bb0cf004" translate="yes" xml:space="preserve">
          <source>The advantages of LARS are:</source>
          <target state="translated">LARS의 장점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="54923c815d40d3138e9fbe2ad92c97dd60ef1b2e" translate="yes" xml:space="preserve">
          <source>The advantages of Multi-layer Perceptron are:</source>
          <target state="translated">다층 퍼셉트론의 장점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="1dabd14761114fadc83f1114e989744a341117db" translate="yes" xml:space="preserve">
          <source>The advantages of Stochastic Gradient Descent are:</source>
          <target state="translated">확률 적 경사 하강의 장점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="a12cea148a9927c20c87185a7ad428a4bd953760" translate="yes" xml:space="preserve">
          <source>The advantages of support vector machines are:</source>
          <target state="translated">서포트 벡터 머신의 장점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="b3ce019dea8f1fa7178760b3ed7e1bed715f9894" translate="yes" xml:space="preserve">
          <source>The affinity matrix describing the relationship of the samples to embed. &lt;strong&gt;Must be symmetric&lt;/strong&gt;.</source>
          <target state="translated">삽입 할 샘플의 관계를 설명하는 선호도 매트릭스입니다. &lt;strong&gt;대칭이어야합니다&lt;/strong&gt; .</target>
        </trans-unit>
        <trans-unit id="f8f51936a34abb4d8304c65310991fbeb781a2b1" translate="yes" xml:space="preserve">
          <source>The algorithm automatically sets the number of clusters, instead of relying on a parameter &lt;code&gt;bandwidth&lt;/code&gt;, which dictates the size of the region to search through. This parameter can be set manually, but can be estimated using the provided &lt;code&gt;estimate_bandwidth&lt;/code&gt; function, which is called if the bandwidth is not set.</source>
          <target state="translated">알고리즘은 매개 변수 &lt;code&gt;bandwidth&lt;/code&gt; 에 의존하지 않고 클러스터 수를 자동으로 설정하여 검색 할 영역의 크기를 결정합니다. 이 매개 변수는 수동으로 설정 될 수 있지만 제공된 &lt;code&gt;estimate_bandwidth&lt;/code&gt; _ 대역폭 기능을 사용하여 추정 할 수 있습니다.이 기능은 대역폭이 설정되지 않은 경우에 호출됩니다.</target>
        </trans-unit>
        <trans-unit id="999ae100a463cc0ff60234ca1a1687522770fa40" translate="yes" xml:space="preserve">
          <source>The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and efficiency. Since the number of least square solutions is &amp;ldquo;n_samples choose n_subsamples&amp;rdquo;, it can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets are chosen randomly. In a final step, the spatial median (or L1 median) is calculated of all least square solutions.</source>
          <target state="translated">이 알고리즘은 X에서 샘플의 n_subsamples 크기를 가진 부분 집합에 대해 최소 제곱 해를 계산합니다. 피처 수와 샘플 사이의 n_subsamples 값은 견고성과 효율성 사이의 절충으로 추정기로 이어집니다. 최소 제곱 해의 수는&amp;ldquo;n_samples choose n_subsamples&amp;rdquo;이므로 매우 클 수 있으므로 max_subpopulation으로 제한 될 수 있습니다. 이 한계에 도달하면 서브 세트가 무작위로 선택됩니다. 최종 단계에서, 공간 중앙값 (또는 L1 중앙값)은 모든 최소 제곱 솔루션으로 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="19207c781cd295df6565aab588711bf1a9323bef" translate="yes" xml:space="preserve">
          <source>The algorithm can also be understood through the concept of &lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;Voronoi diagrams&lt;/a&gt;. First the Voronoi diagram of the points is calculated using the current centroids. Each segment in the Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated to the mean of each segment. The algorithm then repeats this until a stopping criterion is fulfilled. Usually, the algorithm stops when the relative decrease in the objective function between iterations is less than the given tolerance value. This is not the case in this implementation: iteration stops when centroids move less than the tolerance.</source>
          <target state="translated">이 알고리즘은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;Voronoi 다이어그램&lt;/a&gt; 의 개념을 통해서도 이해 될 수 있습니다 . 먼저 점의 보로 노이 다이어그램은 현재 중심을 사용하여 계산됩니다. Voronoi 다이어그램의 각 세그먼트는 별도의 클러스터가됩니다. 둘째, 중심이 각 세그먼트의 평균으로 업데이트됩니다. 그런 다음 알고리즘은 중지 기준이 충족 될 때까지이를 반복합니다. 일반적으로 반복 사이의 목적 함수의 상대적 감소가 주어진 공차 값보다 작 으면 알고리즘이 중지됩니다. 이 구현에서는 그렇지 않습니다. 중심이 공차보다 작게 이동하면 반복이 중지됩니다.</target>
        </trans-unit>
        <trans-unit id="ac387733191797f4111e67a021bb9641f7899ac2" translate="yes" xml:space="preserve">
          <source>The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R &lt;code&gt;glasso&lt;/code&gt; package.</source>
          <target state="translated">이 문제를 해결하기 위해 사용 된 알고리즘은 Friedman 2008 Biostatistics 논문의 GLasso 알고리즘입니다. R &lt;code&gt;glasso&lt;/code&gt; 패키지 와 동일한 알고리즘 입니다.</target>
        </trans-unit>
        <trans-unit id="2d7b95845283c4d98cc4167d4e0748ee6c69e84f" translate="yes" xml:space="preserve">
          <source>The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. &amp;ldquo;Algorithms for computing the sample variance: Analysis and recommendations.&amp;rdquo; The American Statistician 37.3 (1983): 242-247:</source>
          <target state="translated">증분 평균 및 표준에 대한 알고리즘은 Chan, Tony F., Gene H. Golub 및 Randall J. LeVeque의 방정식 1.5a, b에 나와 있습니다. &quot;샘플 분산 계산 알고리즘 : 분석 및 권장 사항.&quot; 미국 통계 학자 37.3 (1983) : 242-247 :</target>
        </trans-unit>
        <trans-unit id="c4e655f6aa7cf22a58a123dc60b12c815d9f7df3" translate="yes" xml:space="preserve">
          <source>The algorithm is adapted from Guyon [1] and was designed to generate the &amp;ldquo;Madelon&amp;rdquo; dataset.</source>
          <target state="translated">이 알고리즘은 Guyon [1]에서 채택되었으며 &quot;Madelon&quot;데이터 세트를 생성하도록 설계되었습니다.</target>
        </trans-unit>
        <trans-unit id="da28c971693f926f3030184039a925bcc2b1ca76" translate="yes" xml:space="preserve">
          <source>The algorithm is from Marsland [1].</source>
          <target state="translated">알고리즘은 Marsland에서 온 것입니다 [1].</target>
        </trans-unit>
        <trans-unit id="94c105f8aebba6a360eae1c93878348f10f58fca" translate="yes" xml:space="preserve">
          <source>The algorithm is not highly scalable, as it requires multiple nearest neighbor searches during the execution of the algorithm. The algorithm is guaranteed to converge, however the algorithm will stop iterating when the change in centroids is small.</source>
          <target state="translated">이 알고리즘은 알고리즘을 실행하는 동안 가장 가까운 인접 검색을 여러 개 필요로하기 때문에 확장 성이 뛰어나지 않습니다. 알고리즘 수렴이 보장되지만 중심 변화가 적을 때 알고리즘 반복이 중지됩니다.</target>
        </trans-unit>
        <trans-unit id="3dc6012ab672dce7fe920dc60add16368612e345" translate="yes" xml:space="preserve">
          <source>The algorithm is similar to forward stepwise regression, but instead of including variables at each step, the estimated parameters are increased in a direction equiangular to each one&amp;rsquo;s correlations with the residual.</source>
          <target state="translated">이 알고리즘은 정방향 회귀 회귀와 유사하지만 각 단계에서 변수를 포함하는 대신 추정 된 매개 변수는 잔차와의 상관 관계에 대해 등각 방향으로 증가합니다.</target>
        </trans-unit>
        <trans-unit id="0335c4d6784d824f2c45b2464a3517f723de00c5" translate="yes" xml:space="preserve">
          <source>The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error.</source>
          <target state="translated">알고리즘은 확률 론적이며 다른 시드로 여러 번 다시 시작하면 다른 임베딩이 생성 될 수 있습니다. 그러나 오류가 가장 적은 임베딩을 선택하는 것은 완벽합니다.</target>
        </trans-unit>
        <trans-unit id="7c7de93467b285ca3f0b63a2764ebf7e5ab511ed" translate="yes" xml:space="preserve">
          <source>The algorithm iterates between two major steps, similar to vanilla k-means. In the first step, \(b\) samples are drawn randomly from the dataset, to form a mini-batch. These are then assigned to the nearest centroid. In the second step, the centroids are updated. In contrast to k-means, this is done on a per-sample basis. For each sample in the mini-batch, the assigned centroid is updated by taking the streaming average of the sample and all previous samples assigned to that centroid. This has the effect of decreasing the rate of change for a centroid over time. These steps are performed until convergence or a predetermined number of iterations is reached.</source>
          <target state="translated">이 알고리즘은 바닐라 k- 평균과 유사한 두 가지 주요 단계를 반복합니다. 첫 번째 단계에서 \ (b \) 샘플은 데이터 집합에서 무작위로 추출되어 미니 배치를 형성합니다. 그런 다음 가장 가까운 중심에 할당됩니다. 두 번째 단계에서는 중심이 업데이트됩니다. k- 평균과 대조적으로, 이것은 샘플마다 수행된다. 미니 배치의 각 샘플에 대해 할당 된 중심은 샘플 및 해당 중심에 할당 된 모든 이전 샘플의 스트리밍 평균을 취하여 업데이트됩니다. 이것은 시간이 지남에 따라 중심에 대한 변화율을 감소시키는 효과가 있습니다. 이러한 단계는 수렴 또는 미리 결정된 반복 횟수에 도달 할 때까지 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="eac8adf7ae77ba9768138371c6c71edd5a3a76a7" translate="yes" xml:space="preserve">
          <source>The algorithm partitions the rows and columns of a matrix so that a corresponding blockwise-constant checkerboard matrix provides a good approximation to the original matrix.</source>
          <target state="translated">이 알고리즘은 행렬의 행과 열을 분할하여 대응하는 블록 단위 상수 바둑판 행렬이 원래 행렬에 대한 근사치를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="36533938d0e94afa89a69e7b9609c61646ab71f5" translate="yes" xml:space="preserve">
          <source>The algorithm splits the complete input sample data into a set of inliers, which may be subject to noise, and outliers, which are e.g. caused by erroneous measurements or invalid hypotheses about the data. The resulting model is then estimated only from the determined inliers.</source>
          <target state="translated">이 알고리즘은 전체 입력 샘플 데이터를 노이즈의 영향을받을 수있는 일련의 인라이너와 데이터에 대한 잘못된 가설로 인해 발생하는 이상 값으로 나눕니다. 결과 모델은 결정된 이너만으로 추정됩니다.</target>
        </trans-unit>
        <trans-unit id="76e87cc3ec2713663d8449fef17b4e0c4101c305" translate="yes" xml:space="preserve">
          <source>The algorithm stops when it reaches a preset maximum number of iterations; or when the improvement in loss is below a certain, small number.</source>
          <target state="translated">미리 설정된 최대 반복 횟수에 도달하면 알고리즘이 중지됩니다. 또는 손실의 개선이 특정의 작은 수보다 낮은 경우.</target>
        </trans-unit>
        <trans-unit id="841b170fb7ba2429816cb7c51af4dbae57b471af" translate="yes" xml:space="preserve">
          <source>The algorithm supports sample weights, which can be given by a parameter &lt;code&gt;sample_weight&lt;/code&gt;. This allows to assign more weight to some samples when computing cluster centers and values of inertia. For example, assigning a weight of 2 to a sample is equivalent to adding a duplicate of that sample to the dataset \(X\).</source>
          <target state="translated">이 알고리즘은 &lt;code&gt;sample_weight&lt;/code&gt; 매개 변수로 제공 될 수있는 샘플 가중치를 지원합니다 . 따라서 클러스터 중심과 관성 값을 계산할 때 일부 샘플에 더 많은 가중치를 할당 할 수 있습니다. 예를 들어 샘플에 가중치 2를 할당하는 것은 해당 샘플의 복제본을 데이터 세트 \ (X \)에 추가하는 것과 같습니다.</target>
        </trans-unit>
        <trans-unit id="a8238e48cf484817aaf23aa2adc5e4a70681d78f" translate="yes" xml:space="preserve">
          <source>The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details.</source>
          <target state="translated">NearestNeighbors 모듈에서 포인트 단위 거리를 계산하고 가장 가까운 이웃을 찾는 데 사용하는 알고리즘입니다. 자세한 내용은 NearestNeighbors 모듈 설명서를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="3602b334476c395c578aab7a7d09d75f92100f28" translate="yes" xml:space="preserve">
          <source>The algorithm treats the input data matrix as a bipartite graph: the rows and columns of the matrix correspond to the two sets of vertices, and each entry corresponds to an edge between a row and a column. The algorithm approximates the normalized cut of this graph to find heavy subgraphs.</source>
          <target state="translated">이 알고리즘은 입력 데이터 행렬을 이분 그래프로 취급합니다. 행렬의 행과 열은 두 정점 세트에 해당하고 각 항목은 행과 열 사이의 가장자리에 해당합니다. 알고리즘은이 그래프의 정규화 된 컷을 근사하여 무거운 하위 그래프를 찾습니다.</target>
        </trans-unit>
        <trans-unit id="e4086364ea17b4442e0265a08361358aa304a247" translate="yes" xml:space="preserve">
          <source>The algorithm used to estimate the weights. It will be called n_components times, i.e. once for each iteration of the outer loop.</source>
          <target state="translated">가중치를 추정하는 데 사용되는 알고리즘입니다. n_components 번, 즉 외부 루프가 반복 될 때마다 한 번씩 호출됩니다.</target>
        </trans-unit>
        <trans-unit id="e4e3e486ab0114bca02d02635f9410b77ca99a25" translate="yes" xml:space="preserve">
          <source>The algorithm used to fit the model is coordinate descent.</source>
          <target state="translated">모델에 맞는 알고리즘은 좌표 하강입니다.</target>
        </trans-unit>
        <trans-unit id="17e35eb9c6004e2d60b73104c93585aacf4a2e98" translate="yes" xml:space="preserve">
          <source>The algorithmic complexity of affinity propagation is quadratic in the number of points.</source>
          <target state="translated">친화도 전파의 알고리즘 복잡도는 포인트 수에서 2 차적입니다.</target>
        </trans-unit>
        <trans-unit id="4c657d81ec38ba678d89f7b7de187e376e50c0f3" translate="yes" xml:space="preserve">
          <source>The algorithms for regression and classification only differ in the concrete loss function used.</source>
          <target state="translated">회귀 및 분류 알고리즘은 사용 된 구체적 손실 함수 만 다릅니다.</target>
        </trans-unit>
        <trans-unit id="50542b2bc530bb15fc1bcd67d7b073d61bea03ab" translate="yes" xml:space="preserve">
          <source>The alpha parameter in the stability selection article used to randomly scale the features. Should be between 0 and 1.</source>
          <target state="translated">안정성 선택 기사의 알파 매개 변수는 기능을 임의로 스케일하는 데 사용됩니다. 0과 1 사이 여야합니다.</target>
        </trans-unit>
        <trans-unit id="35d70c04237d10e5da496ff67ac57fce44665490" translate="yes" xml:space="preserve">
          <source>The alpha parameter of the GraphicalLasso setting the sparsity of the model is set by internal cross-validation in the GraphicalLassoCV. As can be seen on figure 2, the grid to compute the cross-validation score is iteratively refined in the neighborhood of the maximum.</source>
          <target state="translated">모델 희소성을 설정하는 GraphicalLasso의 알파 매개 변수는 GraphicalLassoCV에서 내부 교차 검증에 의해 설정됩니다. 그림 2에서 볼 수 있듯이 교차 검증 점수를 계산하는 그리드는 최대 값 근처에서 반복적으로 구체화됩니다.</target>
        </trans-unit>
        <trans-unit id="2075fb3135d145905cc5a41db16871a4bf5168da" translate="yes" xml:space="preserve">
          <source>The alpha-quantile of the huber loss function and the quantile loss function. Only if &lt;code&gt;loss='huber'&lt;/code&gt; or &lt;code&gt;loss='quantile'&lt;/code&gt;.</source>
          <target state="translated">후버 손실 함수 및 양자 손실 함수의 알파 양자. &lt;code&gt;loss='huber'&lt;/code&gt; 또는 &lt;code&gt;loss='quantile'&lt;/code&gt; 인 경우에만 해당됩니다 .</target>
        </trans-unit>
        <trans-unit id="28a2d4f18d224d5f96bb30d3faa50cdca5ce2935" translate="yes" xml:space="preserve">
          <source>The alphas along the path where models are computed.</source>
          <target state="translated">모델이 계산되는 경로의 알파입니다.</target>
        </trans-unit>
        <trans-unit id="4390acc7061cc013783802ff89b6303810b0044d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set.</source>
          <target state="translated">데이터 세트의 오염량, 즉 데이터 세트의 특이 치 비율.</target>
        </trans-unit>
        <trans-unit id="db48eba23a1b19c738dbbe1eb3626efb11bd507f" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. If &amp;lsquo;auto&amp;rsquo;, the decision function threshold is determined as in the original paper.</source>
          <target state="translated">데이터 세트의 오염량, 즉 데이터 세트의 특이 치 비율. 의사 결정 기능에서 임계 값을 정의하기 위해 적합 할 때 사용됩니다. '자동'인 경우 결정 기능 임계 값은 원본 용지에서와 같이 결정됩니다.</target>
        </trans-unit>
        <trans-unit id="0b69accc98489279e23dd931886f63d4aafd913d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the decision function. If &amp;ldquo;auto&amp;rdquo;, the decision function threshold is determined as in the original paper.</source>
          <target state="translated">데이터 세트의 오염량, 즉 데이터 세트의 특이 치 비율. 피팅 할 때 결정 기능에 대한 임계 값을 정의하는 데 사용됩니다. &quot;자동&quot;인 경우 결정 기능 임계 값은 원본 용지에서와 같이 결정됩니다.</target>
        </trans-unit>
        <trans-unit id="057670e523b801c82d76306d2019fbc0b1e55fa7" translate="yes" xml:space="preserve">
          <source>The amount of penalization chosen by cross validation</source>
          <target state="translated">교차 검증에 의해 선택된 처벌 금액</target>
        </trans-unit>
        <trans-unit id="0a8889ae5955aac97641248ca40c9031408985ae" translate="yes" xml:space="preserve">
          <source>The amount of variance explained by each of the selected components.</source>
          <target state="translated">선택한 각 성분에 의해 설명 된 분산 량.</target>
        </trans-unit>
        <trans-unit id="0c35119665a5a383bf2a2ea2491d6548297b77b9" translate="yes" xml:space="preserve">
          <source>The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.</source>
          <target state="translated">입력 샘플의 이상 점수는 숲에서 나무의 평균 이상 점수로 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="f5bff141bb39bd3c0bc3c7fd23ee875e735e27a8" translate="yes" xml:space="preserve">
          <source>The anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. These are considered outliers.</source>
          <target state="translated">각 표본의 이상 점수를 국소 이상치라고합니다. 주변에 대한 주어진 샘플의 밀도의 국소 편차를 측정합니다. 이상 점수는 물체가 주변 이웃과 얼마나 고립되어 있는지에 달려 있습니다. 보다 정확하게는, 국소성은 k- 최근 접 이웃에 의해 주어지며, 그 거리는 국부 밀도를 추정하는데 사용된다. 샘플의 국부 밀도를 이웃의 국부 밀도와 비교함으로써, 이웃보다 밀도가 실질적으로 낮은 샘플을 식별 할 수있다. 이들은 이상치로 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="9d2b05e02bf58b122225781451ec470ef3bbad43" translate="yes" xml:space="preserve">
          <source>The anomaly score of the input samples. The lower, the more abnormal.</source>
          <target state="translated">입력 샘플의 이상 점수. 낮을수록 비정상적입니다.</target>
        </trans-unit>
        <trans-unit id="7ea9f6456fff678e2acc40131f68202326a3c878" translate="yes" xml:space="preserve">
          <source>The anomaly score of the input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.</source>
          <target state="translated">입력 샘플의 이상 점수. 낮을수록 비정상적입니다. 음수 점수는 특이 치를 나타내고 양수 점수는 특이 치를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="f8f5cba472ebac3a5205adfc98746f94cfb55c44" translate="yes" xml:space="preserve">
          <source>The approximate feature map provided by &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt;&lt;code&gt;AdditiveChi2Sampler&lt;/code&gt;&lt;/a&gt; can be combined with the approximate feature map provided by &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; to yield an approximate feature map for the exponentiated chi squared kernel. See the &lt;a href=&quot;#vz2010&quot; id=&quot;id5&quot;&gt;[VZ2010]&lt;/a&gt; for details and &lt;a href=&quot;#vvz2010&quot; id=&quot;id6&quot;&gt;[VVZ2010]&lt;/a&gt; for combination with the &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt; &lt;code&gt;AdditiveChi2Sampler&lt;/code&gt; 에서&lt;/a&gt; 제공 하는 대략적인 기능 맵을 &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; 에서 제공하는 대략적인 기능 맵과 결합 하여 지수 카이 제곱 커널에 대한 대략적인 기능 맵을 생성 할 수 있습니다 . 참조 &lt;a href=&quot;#vz2010&quot; id=&quot;id5&quot;&gt;[VZ2010]&lt;/a&gt; 자세한 내용과에 대한 &lt;a href=&quot;#vvz2010&quot; id=&quot;id6&quot;&gt;[VVZ2010]&lt;/a&gt; 와 조합 &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="98f8783bf76339c13180f5a0c21989e5d9843d3d" translate="yes" xml:space="preserve">
          <source>The approximate number of singular vectors required to explain most of the data by linear combinations.</source>
          <target state="translated">선형 조합으로 대부분의 데이터를 설명하는 데 필요한 대략적인 특이 벡터 수입니다.</target>
        </trans-unit>
        <trans-unit id="01540c9a0c0d75da953bc96aafe62b761a8754d9" translate="yes" xml:space="preserve">
          <source>The approximate number of singular vectors required to explain most of the input data by linear combinations. Using this kind of singular spectrum in the input allows the generator to reproduce the correlations often observed in practice.</source>
          <target state="translated">대부분의 입력 데이터를 선형 조합으로 설명하는 데 필요한 대략적인 특이 벡터 수입니다. 입력에 이런 종류의 단일 스펙트럼을 사용하면 발생기가 실제로 관찰되는 상관 관계를 재현 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9b1212b40ba5f9b8205d7b64e2e9c20d35b415b3" translate="yes" xml:space="preserve">
          <source>The arithmetic mean is the sum of the elements along the axis divided by the number of elements.</source>
          <target state="translated">산술 평균은 축을 따라 요소의 합을 요소 수로 나눈 값입니다.</target>
        </trans-unit>
        <trans-unit id="35a07a5d9cec4bce6db2b0fe073857a7979c5b16" translate="yes" xml:space="preserve">
          <source>The array has 0.16% of non zero values.</source>
          <target state="translated">이 배열에는 0이 아닌 값의 0.16 %가 있습니다.</target>
        </trans-unit>
        <trans-unit id="449fd4c0217076d90d22633d6cb32b7300c30336" translate="yes" xml:space="preserve">
          <source>The array of (log)-density evaluations, shape = X.shape[:-1]</source>
          <target state="translated">(log) 밀도 평가의 배열, 모양 = X.shape [:-1]</target>
        </trans-unit>
        <trans-unit id="d00a652e1f004fd3ddfd653b6794ad2aba8108c7" translate="yes" xml:space="preserve">
          <source>The array of log(density) evaluations.</source>
          <target state="translated">로그 (밀도) 평가 배열입니다.</target>
        </trans-unit>
        <trans-unit id="531360783ebbdd4b65c83458a2d186f114e4086b" translate="yes" xml:space="preserve">
          <source>The automatic estimation from Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604 by Thomas P. Minka is also compared.</source>
          <target state="translated">PCA에 대한 차원 자동 선택의 자동 추정. Thomas P. Minka의 NIPS 2000 : 598-604도 비교됩니다.</target>
        </trans-unit>
        <trans-unit id="37bf2d4736a8a0a6781e876ebdb656796aca8913" translate="yes" xml:space="preserve">
          <source>The available cross validation iterators are introduced in the following section.</source>
          <target state="translated">사용 가능한 교차 유효성 검증 반복기는 다음 섹션에서 소개됩니다.</target>
        </trans-unit>
        <trans-unit id="52d681893d146fe7dad6c4424a94a534bab69431" translate="yes" xml:space="preserve">
          <source>The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration.</source>
          <target state="translated">평균 복잡도는 O (kn T)로 주어지며, n은 샘플 수이고 T는 반복 수입니다.</target>
        </trans-unit>
        <trans-unit id="d9092bb0da47977d9b43f41f12fd7dcc75d26801" translate="yes" xml:space="preserve">
          <source>The average number of labels per instance. More precisely, the number of labels per sample is drawn from a Poisson distribution with &lt;code&gt;n_labels&lt;/code&gt; as its expected value, but samples are bounded (using rejection sampling) by &lt;code&gt;n_classes&lt;/code&gt;, and must be nonzero if &lt;code&gt;allow_unlabeled&lt;/code&gt; is False.</source>
          <target state="translated">인스턴스 당 평균 레이블 수입니다. 보다 정확하게는 샘플 당 레이블 수 는 예상 값 으로 &lt;code&gt;n_labels&lt;/code&gt; 를 사용하여 Poisson 분포에서 가져 오지만 샘플은 &lt;code&gt;n_classes&lt;/code&gt; 에 의해 제한되고 (거부 샘플링 사용) , &lt;code&gt;allow_unlabeled&lt;/code&gt; 가 False 이면 0이 아니어야합니다 .</target>
        </trans-unit>
        <trans-unit id="50521765649b7ccece3cb452f18896c14c55c5d8" translate="yes" xml:space="preserve">
          <source>The average precision score in multi-label settings</source>
          <target state="translated">다중 레이블 설정의 평균 정밀도 점수</target>
        </trans-unit>
        <trans-unit id="7f192c1d1db49dfae3574eb70a5374ae8608eb71" translate="yes" xml:space="preserve">
          <source>The averaged intercept term.</source>
          <target state="translated">평균 절편 항입니다.</target>
        </trans-unit>
        <trans-unit id="5f1e3570b35bda8ded1b9651eb310bcefe0589f6" translate="yes" xml:space="preserve">
          <source>The axes with which the grid has been created or None if the grid has been given.</source>
          <target state="translated">그리드가 생성 된 축 또는 그리드가 제공된 경우 없음</target>
        </trans-unit>
        <trans-unit id="2af1a0461163ef9a917dd17f9a6c7032c07c79bb" translate="yes" xml:space="preserve">
          <source>The axis along which to impute.</source>
          <target state="translated">대치 할 축입니다.</target>
        </trans-unit>
        <trans-unit id="efae1008b127cbb418b23469c7f222e2a6660b67" translate="yes" xml:space="preserve">
          <source>The bag of words representation is quite simplistic but surprisingly useful in practice.</source>
          <target state="translated">단어 모음은 매우 단순하지만 실제로는 놀랍게도 유용합니다.</target>
        </trans-unit>
        <trans-unit id="a41734ff5b2ce49bfc6f83ebb1bb324e76d37d70" translate="yes" xml:space="preserve">
          <source>The bags of words representation implies that &lt;code&gt;n_features&lt;/code&gt; is the number of distinct words in the corpus: this number is typically larger than 100,000.</source>
          <target state="translated">단어 백은 &lt;code&gt;n_features&lt;/code&gt; 가 코퍼스의 고유 단어 수임을 나타냅니다 .이 숫자는 일반적으로 100,000보다 큽니다.</target>
        </trans-unit>
        <trans-unit id="eac13f6898f72e0eaad5c85bd1b456a5882d1a72" translate="yes" xml:space="preserve">
          <source>The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.</source>
          <target state="translated">이진 및 멀티 클래스 분류 문제의 균형 잡힌 정확성은 불균형 데이터 세트를 처리합니다. 각 클래스에서 얻은 리콜의 평균으로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="fd188197709d9753bf6d2d7d8ebbb1b211377cd3" translate="yes" xml:space="preserve">
          <source>The bandwidth here acts as a smoothing parameter, controlling the tradeoff between bias and variance in the result. A large bandwidth leads to a very smooth (i.e. high-bias) density distribution. A small bandwidth leads to an unsmooth (i.e. high-variance) density distribution.</source>
          <target state="translated">여기서 대역폭은 평활화 매개 변수로 작용하여 결과에서 바이어스와 분산 간의 균형을 제어합니다. 큰 대역폭은 매우 매끄러운 (즉, 높은 바이어스) 밀도 분포로 이어집니다. 대역폭이 작 으면 불균형 (즉, 고 분산) 밀도 분포가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="220a3cae014ca1f6f44493020405f4e460f4038d" translate="yes" xml:space="preserve">
          <source>The bandwidth of the kernel.</source>
          <target state="translated">커널의 대역폭.</target>
        </trans-unit>
        <trans-unit id="eaa1089b1fdc51cb43f528072a19cd3f3ec50c61" translate="yes" xml:space="preserve">
          <source>The bandwidth parameter.</source>
          <target state="translated">대역폭 매개 변수</target>
        </trans-unit>
        <trans-unit id="ca9efc037882c6303cbf300aed185a1d9a7dd7ae" translate="yes" xml:space="preserve">
          <source>The bar plot indicates the accuracy, training time (normalized) and test time (normalized) of each classifier.</source>
          <target state="translated">막대 그림은 각 분류기의 정확도, 훈련 시간 (정규화) 및 테스트 시간 (정규화)을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="6ef0fbe42e5306306c086ebee574632386d4e293" translate="yes" xml:space="preserve">
          <source>The base classifier is a random forest classifier with 25 base estimators (trees). If this classifier is trained on all 800 training datapoints, it is overly confident in its predictions and thus incurs a large log-loss. Calibrating an identical classifier, which was trained on 600 datapoints, with method=&amp;rsquo;sigmoid&amp;rsquo; on the remaining 200 datapoints reduces the confidence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center. This calibration results in a lower log-loss. Note that an alternative would have been to increase the number of base estimators which would have resulted in a similar decrease in log-loss.</source>
          <target state="translated">기본 분류기는 25 개의 기본 추정기 (트리)가있는 임의의 포리스트 분류기입니다. 이 분류 기가 800 개의 훈련 데이터 포인트 모두에 대해 훈련 된 경우 예측에 대한 확신이 많으므로 큰 로그 손실이 발생합니다. 나머지 200 개 데이터 포인트에 대해 method = 'sigmoid'를 사용하여 600 개 데이터 포인트에 대해 학습 된 동일한 분류기를 교정하면 예측의 신뢰도가 감소합니다. 즉, 확률 벡터가 단면의 가장자리에서 중심으로 이동합니다. 이 교정으로 로그 손실이 줄어 듭니다. 대안은 기본 추정기의 수를 늘려서 로그 손실이 비슷하게 감소했음을 유의하십시오.</target>
        </trans-unit>
        <trans-unit id="3f89ef1e6cb2a54f74eb0a982beea8ffd3333592" translate="yes" xml:space="preserve">
          <source>The base classifier is a random forest classifier with 25 base estimators (trees). If this classifier is trained on all 800 training datapoints, it is overly confident in its predictions and thus incurs a large log-loss. Calibrating an identical classifier, which was trained on 600 datapoints, with method=&amp;rsquo;sigmoid&amp;rsquo; on the remaining 200 datapoints reduces the confidence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center:</source>
          <target state="translated">기본 분류기는 25 개의 기본 추정기 (트리)가있는 임의의 포리스트 분류기입니다. 이 분류 기가 800 개의 훈련 데이터 포인트 모두에 대해 훈련 된 경우 예측에 대한 확신이 많으므로 큰 로그 손실이 발생합니다. 나머지 200 개의 데이터 포인트에 대해 method = 'sigmoid'를 사용하여 600 개의 데이터 포인트에 대해 학습 된 동일한 분류기를 교정하면 예측의 신뢰도가 감소합니다. 즉, 확률 벡터를 단면의 가장자리에서 중심으로 이동합니다.</target>
        </trans-unit>
        <trans-unit id="cdab13e5017fd96224833fdcfcd75615e152e1af" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper &lt;code&gt;classes_&lt;/code&gt; and &lt;code&gt;n_classes_&lt;/code&gt; attributes. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;</source>
          <target state="translated">부스트 앙상블이 만들어지는 기본 추정기. 적절한 &lt;code&gt;classes_&lt;/code&gt; 및 &lt;code&gt;n_classes_&lt;/code&gt; 속성 뿐만 아니라 샘플 가중치를 지원해야 합니다. &lt;code&gt;None&lt;/code&gt; 인 경우 기본 추정기는 &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="39ba9d176a83a208ed96d60f33bec321ac051ac5" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;</source>
          <target state="translated">부스트 앙상블이 만들어지는 기본 추정기. 샘플 가중치 지원이 필요합니다. 경우 &lt;code&gt;None&lt;/code&gt; , 그베이스 추정기는 없다 &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="09e53f3d87743d060ad05fb04ae38587d00307b0" translate="yes" xml:space="preserve">
          <source>The base estimator from which the classifier chain is built.</source>
          <target state="translated">분류기 체인이 작성되는 기본 추정기입니다.</target>
        </trans-unit>
        <trans-unit id="c229b061f41bb16a20d56916c466508eaa778c17" translate="yes" xml:space="preserve">
          <source>The base estimator from which the ensemble is grown.</source>
          <target state="translated">앙상블이 성장하는 기본 추정기.</target>
        </trans-unit>
        <trans-unit id="5066de8ea8cb5c40493cb3ffbb0e912b0ff83ff9" translate="yes" xml:space="preserve">
          <source>The base estimator from which the transformer is built. This can be both a fitted (if &lt;code&gt;prefit&lt;/code&gt; is set to True) or a non-fitted estimator. The estimator must have either a &lt;code&gt;feature_importances_&lt;/code&gt; or &lt;code&gt;coef_&lt;/code&gt; attribute after fitting.</source>
          <target state="translated">변압기가 구축되는 기본 추정기입니다. 이는 &lt;code&gt;prefit&lt;/code&gt; ( 사전 적합도 가 True로 설정된 경우 )이거나 적합하지 않은 추정기입니다. 추정기에는 피팅 후 &lt;code&gt;feature_importances_&lt;/code&gt; 또는 &lt;code&gt;coef_&lt;/code&gt; 속성 이 있어야합니다 .</target>
        </trans-unit>
        <trans-unit id="8935e197daa0e6c85ff9f9b0e235723b5ff13893" translate="yes" xml:space="preserve">
          <source>The base estimator from which the transformer is built. This is stored only when a non-fitted estimator is passed to the &lt;code&gt;SelectFromModel&lt;/code&gt;, i.e when prefit is False.</source>
          <target state="translated">변압기가 구축되는 기본 추정기입니다. 이것은 적합하지 않은 추정기가 &lt;code&gt;SelectFromModel&lt;/code&gt; 로 전달 될 때, 즉 prefit이 False 인 경우 에만 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="8062156aa44a8a655e18edffbba0a99e0e098c8a" translate="yes" xml:space="preserve">
          <source>The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.</source>
          <target state="translated">데이터 세트의 임의의 부분 집합에 맞는 기본 추정량입니다. None이면, 기본 추정기는 의사 결정 트리입니다.</target>
        </trans-unit>
        <trans-unit id="d80c39c018abc387662fda22711c8b7bb4707717" translate="yes" xml:space="preserve">
          <source>The base kernel</source>
          <target state="translated">기본 커널</target>
        </trans-unit>
        <trans-unit id="1b7bc12e1d1eb0bb8e39e15748d7f9b27d93ef1a" translate="yes" xml:space="preserve">
          <source>The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the &lt;code&gt;weights&lt;/code&gt; keyword. The default value, &lt;code&gt;weights = 'uniform'&lt;/code&gt;, assigns uniform weights to each neighbor. &lt;code&gt;weights = 'distance'&lt;/code&gt; assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights.</source>
          <target state="translated">기본 가장 가까운 이웃 분류는 균일 한 가중치를 사용합니다. 즉, 쿼리 지점에 지정된 값은 가장 가까운 이웃의 단순 과반수 투표에서 계산됩니다. 어떤 상황에서는 이웃이 더 가까워 지도록 이웃을 가중시키는 것이 좋습니다. 이것은 &lt;code&gt;weights&lt;/code&gt; 키워드를 통해 달성 할 수 있습니다 . 기본값 &lt;code&gt;weights = 'uniform'&lt;/code&gt; 은 각 이웃에 균일 한 가중치를 지정합니다. &lt;code&gt;weights = 'distance'&lt;/code&gt; 는 쿼리 지점으로부터의 거리의 역 비례에 가중치를 할당합니다. 대안 적으로, 가중치를 계산하기 위해 거리의 사용자 정의 함수가 제공 될 수있다.</target>
        </trans-unit>
        <trans-unit id="650ceb2a1485e5f890a3dcff86c2748037640c3b" translate="yes" xml:space="preserve">
          <source>The basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. Under some circumstances, it can be advantageous to weight points such that nearby points contribute more to the regression than faraway points. This can be accomplished through the &lt;code&gt;weights&lt;/code&gt; keyword. The default value, &lt;code&gt;weights = 'uniform'&lt;/code&gt;, assigns equal weights to all points. &lt;code&gt;weights = 'distance'&lt;/code&gt; assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied, which will be used to compute the weights.</source>
          <target state="translated">가장 가까운 기본 이웃 회귀 분석은 균일 한 가중치를 사용합니다. 즉, 로컬 이웃의 각 포인트는 쿼리 포인트의 분류에 균일하게 기여합니다. 일부 상황에서, 가까운 지점보다 가까운 지점이 회귀에 더 많이 기여하도록 가중치를 적용하는 것이 유리할 수 있습니다. 이것은 &lt;code&gt;weights&lt;/code&gt; 키워드를 통해 달성 할 수 있습니다 . 기본값 &lt;code&gt;weights = 'uniform'&lt;/code&gt; 은 모든 포인트에 동일한 가중치를 할당합니다. &lt;code&gt;weights = 'distance'&lt;/code&gt; 는 쿼리 지점으로부터의 거리의 역 비례하는 가중치를 할당합니다. 대안 적으로, 거리의 사용자 정의 함수가 제공 될 수 있으며, 이는 가중치를 계산하는데 사용될 것이다.</target>
        </trans-unit>
        <trans-unit id="1cb5ab68855135ab83ff44b066a7bba092358a07" translate="yes" xml:space="preserve">
          <source>The behavior of &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; is summarized in the following table.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt; 의 동작은 다음 표에 요약되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="9c1d95adae9acea07102a8bfe1db513459ebeaee" translate="yes" xml:space="preserve">
          <source>The behavior of the model is very sensitive to the &lt;code&gt;gamma&lt;/code&gt; parameter. If &lt;code&gt;gamma&lt;/code&gt; is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with &lt;code&gt;C&lt;/code&gt; will be able to prevent overfitting.</source>
          <target state="translated">모델의 동작은 &lt;code&gt;gamma&lt;/code&gt; 매개 변수에 매우 민감합니다 . &lt;code&gt;gamma&lt;/code&gt; 가 너무 큰 경우 , 서포트 벡터의 영향 영역의 반경은 서포트 벡터 자체 만 포함하며 &lt;code&gt;C&lt;/code&gt; 를 사용한 정규화 양은 과적 합을 방지 할 수 없습니다 .</target>
        </trans-unit>
        <trans-unit id="2ab99fa4c73a04aa03f4ac31b15851c1ddbe51a1" translate="yes" xml:space="preserve">
          <source>The below plot uses the first two features. See &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;here&lt;/a&gt; for more information on this dataset.</source>
          <target state="translated">아래 그림은 처음 두 기능을 사용합니다. 이 데이터 세트에 대한 자세한 내용 은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;여기&lt;/a&gt; 를 참조 하십시오 .</target>
        </trans-unit>
        <trans-unit id="4992d67c6e1aff2322ba4c5bb3c9e0c82a7a36cf" translate="yes" xml:space="preserve">
          <source>The best model is selected by cross-validation.</source>
          <target state="translated">교차 검증에 의해 최상의 모델이 선택됩니다.</target>
        </trans-unit>
        <trans-unit id="1a3f42d6835c4f6686d54e99d9f9cd0e3a597fec" translate="yes" xml:space="preserve">
          <source>The best performance is 1 with &lt;code&gt;normalize == True&lt;/code&gt; and the number of samples with &lt;code&gt;normalize == False&lt;/code&gt;.</source>
          <target state="translated">최고의 성능은 &lt;code&gt;normalize == True&lt;/code&gt; 1 이고 &lt;code&gt;normalize == False&lt;/code&gt; 인 샘플 수입니다 .</target>
        </trans-unit>
        <trans-unit id="4b2ab3d16cd4ed6ed82ae137ea7b0ef6d79ff832" translate="yes" xml:space="preserve">
          <source>The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.</source>
          <target state="translated">가장 좋은 p- 값은 1 / (n_permutations + 1)이고 최악은 1.0입니다.</target>
        </trans-unit>
        <trans-unit id="ed9ac623a873633695e926c14de64904b54167f4" translate="yes" xml:space="preserve">
          <source>The best possible score is 1.0, lower values are worse.</source>
          <target state="translated">가장 좋은 점수는 1.0이며 값이 낮을수록 나쁩니다.</target>
        </trans-unit>
        <trans-unit id="9230aaef9f0a1fa5e57a82e7f44b36cb5d04d36e" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.</source>
          <target state="translated">가장 좋은 값은 1이고 가장 나쁜 값은 -1입니다. 0에 가까운 값은 겹치는 클러스터를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="6e2eac60da3011cdc30f86be85be8ffa7e5f1da5" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.</source>
          <target state="translated">가장 좋은 값은 1이고 가장 나쁜 값은 -1입니다. 0에 가까운 값은 겹치는 클러스터를 나타냅니다. 음수 값은 일반적으로 다른 군집이 더 유사하므로 표본이 잘못된 군집에 할당되었음을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="b4a689b86b5cda51ef6628ef622a0daf756424bd" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is 0 when &lt;code&gt;adjusted=False&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;adjusted=False&lt;/code&gt; 때 가장 좋은 값은 1이고 가장 나쁜 값은 0 입니다.</target>
        </trans-unit>
        <trans-unit id="488558d8da77f986fd351608404cb6a07bd4fe58" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is 0.</source>
          <target state="translated">가장 좋은 값은 1이고 가장 나쁜 값은 0입니다.</target>
        </trans-unit>
        <trans-unit id="b82813aa45a878f8df07ac95972e2e5116e63bed" translate="yes" xml:space="preserve">
          <source>The bias term in the underlying linear model.</source>
          <target state="translated">기본 선형 모형의 바이어스 항입니다.</target>
        </trans-unit>
        <trans-unit id="d5675ace96b0ccc75bff7a0b67255343283bf5b9" translate="yes" xml:space="preserve">
          <source>The bicluster label of each column.</source>
          <target state="translated">각 열의 bicluster 레이블.</target>
        </trans-unit>
        <trans-unit id="d135838b65ddeda2dac8521941211167551d33d9" translate="yes" xml:space="preserve">
          <source>The bicluster label of each row.</source>
          <target state="translated">각 행의 bicluster 레이블.</target>
        </trans-unit>
        <trans-unit id="4f153639172a63c8dba2ed27e16715aaa100936d" translate="yes" xml:space="preserve">
          <source>The bipartite structure allows for the use of efficient block Gibbs sampling for inference.</source>
          <target state="translated">이분자 구조는 추론을 위해 효율적인 블록 Gibbs 샘플링을 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b104bf424ac624d7987a6846ffafb73fcb6a3323" translate="yes" xml:space="preserve">
          <source>The bounding box for each cluster center when centers are generated at random.</source>
          <target state="translated">중심이 무작위로 생성 될 때 각 클러스터 중심의 경계 상자입니다.</target>
        </trans-unit>
        <trans-unit id="de87510aaa8a4634fe7b670444f1141d0b934e04" translate="yes" xml:space="preserve">
          <source>The breast cancer dataset is a classic and very easy binary classification dataset.</source>
          <target state="translated">유방암 데이터 세트는 고전적이고 매우 쉬운 이진 분류 데이터 세트입니다.</target>
        </trans-unit>
        <trans-unit id="2045b20286b76d60e5fa1f74a1df8fc02a45c66b" translate="yes" xml:space="preserve">
          <source>The brier score loss is also between 0 to 1 and the lower the score (the mean square difference is smaller), the more accurate the prediction is. It can be thought of as a measure of the &amp;ldquo;calibration&amp;rdquo; of a set of probabilistic predictions.</source>
          <target state="translated">치어 점수 손실도 0에서 1 사이이며 점수가 낮을수록 (평균 제곱 차이가 작을수록) 예측이 더 정확합니다. 그것은 확률 론적 예측의 &quot;보정&quot;의 척도로서 생각 될 수있다.</target>
        </trans-unit>
        <trans-unit id="74481417cca6e6adb9f25dfbb8e31b8fc201eb01" translate="yes" xml:space="preserve">
          <source>The callable to use for the inverse transformation. This will be passed the same arguments as inverse transform, with args and kwargs forwarded. If inverse_func is None, then inverse_func will be the identity function.</source>
          <target state="translated">역변환에 사용할 수 있습니다. 이것은 args와 kwargs가 전달 된 역변환과 같은 인수로 전달됩니다. inverse_func가 None이면 inverse_func는 항등 함수입니다.</target>
        </trans-unit>
        <trans-unit id="81ea3433c521d90153f147dfae64d0df290c9757" translate="yes" xml:space="preserve">
          <source>The callable to use for the transformation. This will be passed the same arguments as transform, with args and kwargs forwarded. If func is None, then func will be the identity function.</source>
          <target state="translated">변환에 사용할 호출 가능 이것은 args와 kwargs를 전달하여 transform과 동일한 인수로 전달됩니다. func가 None이면 func은 항등 함수입니다.</target>
        </trans-unit>
        <trans-unit id="1f38539ea641dd8b50f2f7d85a7f466ba670ba50" translate="yes" xml:space="preserve">
          <source>The categories of each feature determined during fitting (in order of the features in X and corresponding with the output of &lt;code&gt;transform&lt;/code&gt;).</source>
          <target state="translated">피팅하는 동안 결정된 각 지형지 물의 범주 (X의 지형지 물 순서 및 &lt;code&gt;transform&lt;/code&gt; 의 출력에 해당 ).</target>
        </trans-unit>
        <trans-unit id="9d4177cfd25fe2d42f6a09b10ba8f700ec03cc45" translate="yes" xml:space="preserve">
          <source>The chi squared kernel is given by</source>
          <target state="translated">카이 제곱 커널은</target>
        </trans-unit>
        <trans-unit id="e654b311ef425d553f0f6705f2dd43fc95968d41" translate="yes" xml:space="preserve">
          <source>The chi squared kernel is most commonly used on histograms (bags) of visual words.</source>
          <target state="translated">카이 제곱 커널은 시각적 단어의 히스토그램 (가방)에서 가장 일반적으로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="085e153138bf3768d8fc53856c1f1238c6d3caff" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is a very popular choice for training non-linear SVMs in computer vision applications. It can be computed using &lt;a href=&quot;generated/sklearn.metrics.pairwise.chi2_kernel#sklearn.metrics.pairwise.chi2_kernel&quot;&gt;&lt;code&gt;chi2_kernel&lt;/code&gt;&lt;/a&gt; and then passed to an &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;sklearn.svm.SVC&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;kernel=&quot;precomputed&quot;&lt;/code&gt;:</source>
          <target state="translated">카이 제곱 커널은 컴퓨터 비전 응용 프로그램에서 비선형 SVM을 교육하는 데 매우 널리 사용됩니다. 그것은 사용하여 계산 될 수있다 &lt;a href=&quot;generated/sklearn.metrics.pairwise.chi2_kernel#sklearn.metrics.pairwise.chi2_kernel&quot;&gt; &lt;code&gt;chi2_kernel&lt;/code&gt; 을&lt;/a&gt; 한 다음에 전달 &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;sklearn.svm.SVC&lt;/code&gt; &lt;/a&gt; 와 함께 &lt;code&gt;kernel=&quot;precomputed&quot;&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="9233e345f2318b5fc92b12dd745f5adf2df3c33c" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is computed between each pair of rows in X and Y. X and Y have to be non-negative. This kernel is most commonly applied to histograms.</source>
          <target state="translated">카이 제곱 커널은 X와 Y의 각 행 쌍 사이에서 계산됩니다. X와 Y는 음이 아니어야합니다. 이 커널은 히스토그램에 가장 일반적으로 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="a8ba729b106653acd0bc97ad998e8dac7b2b635e" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is given by:</source>
          <target state="translated">카이 제곱 커널은 다음과 같이 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="5ac1548c51d0c4529f9b6250f5d16db6b392dc40" translate="yes" xml:space="preserve">
          <source>The children of each non-leaf node. Values less than &lt;code&gt;n_features&lt;/code&gt; correspond to leaves of the tree which are the original samples. A node &lt;code&gt;i&lt;/code&gt; greater than or equal to &lt;code&gt;n_features&lt;/code&gt; is a non-leaf node and has children &lt;code&gt;children_[i - n_features]&lt;/code&gt;. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node &lt;code&gt;n_features + i&lt;/code&gt;</source>
          <target state="translated">각 비 리프 노드의 자식 &lt;code&gt;n_features&lt;/code&gt; 보다 작은 값 은 원래 샘플 인 나무의 잎에 해당합니다. &lt;code&gt;n_features&lt;/code&gt; 보다 크거나 같은 노드 &lt;code&gt;i&lt;/code&gt; 는 비 리프 노드이며 자식 &lt;code&gt;children_[i - n_features]&lt;/code&gt; 있습니다. 대안으로 i 번째 반복에서, children [i] [0] 및 children [i] [1]이 병합되어 노드 &lt;code&gt;n_features + i&lt;/code&gt; 를 형성합니다.</target>
        </trans-unit>
        <trans-unit id="c8989abb78a441b49758b99ec48d8b090a3ce248" translate="yes" xml:space="preserve">
          <source>The children of each non-leaf node. Values less than &lt;code&gt;n_samples&lt;/code&gt; correspond to leaves of the tree which are the original samples. A node &lt;code&gt;i&lt;/code&gt; greater than or equal to &lt;code&gt;n_samples&lt;/code&gt; is a non-leaf node and has children &lt;code&gt;children_[i - n_samples]&lt;/code&gt;. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node &lt;code&gt;n_samples + i&lt;/code&gt;</source>
          <target state="translated">각 비 리프 노드의 자식 &lt;code&gt;n_samples&lt;/code&gt; 보다 작은 값 은 원래 샘플 인 트리의 잎에 해당합니다. &lt;code&gt;n_samples&lt;/code&gt; 보다 크거나 같은 노드 &lt;code&gt;i&lt;/code&gt; 는 리프가 아닌 노드이며 자식 &lt;code&gt;children_[i - n_samples]&lt;/code&gt; 있습니다. 대안으로 i 번째 반복에서, children [i] [0] 및 children [i] [1]이 병합되어 노드 &lt;code&gt;n_samples + i&lt;/code&gt; 를 형성합니다.</target>
        </trans-unit>
        <trans-unit id="f3473fe6f2b500963d93290672a7a3ef3f212da2" translate="yes" xml:space="preserve">
          <source>The choice of features is not particularly helpful, but serves to illustrate the technique.</source>
          <target state="translated">기능 선택은 특별히 도움이되지 않지만 기술을 설명하는 데 도움이됩니다.</target>
        </trans-unit>
        <trans-unit id="1f460fe2e0e34556a3c6a8a6d017a5f63eeb9ea9" translate="yes" xml:space="preserve">
          <source>The cholesky decomposition of the precision matrices of each mixture component. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">각 혼합물 성분의 정밀 매트릭스의 콜레 스키 분해. 정밀 행렬은 공분산 행렬의 역수입니다. 공분산 행렬은 대칭 양수로 한정되므로 가우스 혼합은 정밀 행렬에 의해 동등하게 매개 변수화 될 수 있습니다. 공분산 행렬 대신 정밀 행렬을 저장하면 테스트 시간에 새로운 샘플의 로그 가능성을보다 효율적으로 계산할 수 있습니다. 모양은 &lt;code&gt;covariance_type&lt;/code&gt; 에 따라 다릅니다 .</target>
        </trans-unit>
        <trans-unit id="54b709e28c0bc302f278f75cf2f9281781fc40ea" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; used with the optional parameter &lt;code&gt;svd_solver='randomized'&lt;/code&gt; is very useful in that case: since we are going to drop most of the singular vectors it is much more efficient to limit the computation to an approximated estimate of the singular vectors we will keep to actually perform the transform.</source>
          <target state="translated">선택적 매개 변수 &lt;code&gt;svd_solver='randomized'&lt;/code&gt; 와 함께 사용되는 &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt; 클래스 는이 경우에 매우 유용합니다. 대부분의 특이 벡터를 제거 할 것이기 때문에 계산을 단일 벡터의 대략적인 추정치로 제한하는 것이 훨씬 효율적입니다. 실제로 변환을 수행하십시오.</target>
        </trans-unit>
        <trans-unit id="9afc238719eb2575519db8e476eb36c39ec071a2" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.feature_extraction.dictvectorizer#sklearn.feature_extraction.DictVectorizer&quot;&gt;&lt;code&gt;DictVectorizer&lt;/code&gt;&lt;/a&gt; can be used to convert feature arrays represented as lists of standard Python &lt;code&gt;dict&lt;/code&gt; objects to the NumPy/SciPy representation used by scikit-learn estimators.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.dictvectorizer#sklearn.feature_extraction.DictVectorizer&quot;&gt; &lt;code&gt;DictVectorizer&lt;/code&gt; &lt;/a&gt; 클래스를 사용하면 표준 Python &lt;code&gt;dict&lt;/code&gt; 객체 목록으로 표시되는 피처 배열 을 scikit-learn 추정기에서 사용하는 NumPy / SciPy 표현 으로 변환 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="0b3d7d58279a2952a288a94db8a135a534d78ec7" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; is a high-speed, low-memory vectorizer that uses a technique known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_hashing&quot;&gt;feature hashing&lt;/a&gt;, or the &amp;ldquo;hashing trick&amp;rdquo;. Instead of building a hash table of the features encountered in training, as the vectorizers do, instances of &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; apply a hash function to the features to determine their column index in sample matrices directly. The result is increased speed and reduced memory usage, at the expense of inspectability; the hasher does not remember what the input features looked like and has no &lt;code&gt;inverse_transform&lt;/code&gt; method.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt; 클래스 는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_hashing&quot;&gt;기능 해싱&lt;/a&gt; 또는 &quot;해싱 트릭&quot; 이라고하는 기술을 사용하는 고속 저 메모리 벡터 라이저 입니다 . 벡터화 프로그램과 마찬가지로 교육에서 발생하는 기능의 해시 테이블을 작성하는 대신 &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt; 인스턴스는 기능에 해시 함수를 적용하여 샘플 행렬에서 열 인덱스를 직접 결정합니다. 결과적으로 검사 성을 희생하면서 속도를 높이고 메모리 사용량을 줄입니다. hasher는 입력 기능이 어떻게 생겼는지 기억하지 &lt;code&gt;inverse_transform&lt;/code&gt; 메소드 가 없습니다 .</target>
        </trans-unit>
        <trans-unit id="46148e4d160f9510def06597f1921af425f74fcb" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt;&lt;code&gt;IsotonicRegression&lt;/code&gt;&lt;/a&gt; fits a non-decreasing function to data. It solves the following problem:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt; &lt;code&gt;IsotonicRegression&lt;/code&gt; &lt;/a&gt; 클래스 는 비 감소 함수를 데이터에 맞 춥니 다. 다음과 같은 문제를 해결합니다.</target>
        </trans-unit>
        <trans-unit id="04846022f8485d75461ddcb301a4717897728672" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt;&lt;code&gt;AdditiveChi2Sampler&lt;/code&gt;&lt;/a&gt; implements this component wise deterministic sampling. Each component is sampled \(n\) times, yielding \(2n+1\) dimensions per input dimension (the multiple of two stems from the real and complex part of the Fourier transform). In the literature, \(n\) is usually chosen to be 1 or 2, transforming the dataset to size &lt;code&gt;n_samples * 5 * n_features&lt;/code&gt; (in the case of \(n=2\)).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt; &lt;code&gt;AdditiveChi2Sampler&lt;/code&gt; &lt;/a&gt; 클래스 는이 컴포넌트의 현명한 결정적 샘플링을 구현합니다. 각 구성 요소는 \ (n \) 번 샘플링되어 입력 차원 당 \ (2n + 1 \) 차원을 생성합니다 (두 개의 배수는 푸리에 변환의 실수 부분과 복잡한 부분에서 유래 함). 문헌에서 \ (n \)은 일반적으로 1 또는 2로 선택되어 데이터 세트를 크기 &lt;code&gt;n_samples * 5 * n_features&lt;/code&gt; (\ (n = 2 \)의 경우)로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="9fdc0ed638cb0889fa5320ee5ada72b80a16402f" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.elasticnetcv#sklearn.linear_model.ElasticNetCV&quot;&gt;&lt;code&gt;ElasticNetCV&lt;/code&gt;&lt;/a&gt; can be used to set the parameters &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) and &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) by cross-validation.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.elasticnetcv#sklearn.linear_model.ElasticNetCV&quot;&gt; &lt;code&gt;ElasticNetCV&lt;/code&gt; &lt;/a&gt; 클래스를 사용하여 교차 유효성 검사를 통해 &lt;code&gt;alpha&lt;/code&gt; (\ (\ alpha \)) 및 &lt;code&gt;l1_ratio&lt;/code&gt; (\ (\ rho \)) 매개 변수를 설정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="c89182d0633b09742e745213024a53ccec4fee0a" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnetcv#sklearn.linear_model.MultiTaskElasticNetCV&quot;&gt;&lt;code&gt;MultiTaskElasticNetCV&lt;/code&gt;&lt;/a&gt; can be used to set the parameters &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) and &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) by cross-validation.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnetcv#sklearn.linear_model.MultiTaskElasticNetCV&quot;&gt; &lt;code&gt;MultiTaskElasticNetCV&lt;/code&gt; &lt;/a&gt; 클래스를 사용하여 교차 유효성 검증을 통해 매개 변수 &lt;code&gt;alpha&lt;/code&gt; (\ (\ alpha \)) 및 &lt;code&gt;l1_ratio&lt;/code&gt; (\ (\ rho \)) 를 설정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="6c66ee2c7a53fbda2b2b2da602d1e782a1fcc1b3" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a first-order SGD learning routine. The algorithm iterates over the training examples and for each example updates the model parameters according to the update rule given by</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 클래스 는 1 차 SGD 학습 루틴을 구현합니다. 알고리즘은 학습 예제를 반복하고 각 예제마다 다음에 의해 제공된 업데이트 규칙에 따라 모델 매개 변수를 업데이트합니다.</target>
        </trans-unit>
        <trans-unit id="f128e57d09cf8b511f002f70c40429b250044323" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 클래스 는 분류에 대한 다양한 손실 함수 및 처벌을 지원하는 일반 확률 적 경사 하강 학습 루틴을 구현합니다.</target>
        </trans-unit>
        <trans-unit id="9be5b4c1c13de0290ee1bddf8fed519138b844e9" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; is well suited for regression problems with a large number of training samples (&amp;gt; 10.000), for other problems we recommend &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt;, or &lt;a href=&quot;generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt;&lt;code&gt;ElasticNet&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; 클래스 는 선형 손실 회귀 모델에 맞게 다양한 손실 함수 및 위약금을 지원하는 일반 확률 적 경사 하강 학습 루틴을 구현합니다. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; 는 많은 훈련 샘플 (&amp;gt; 10.000)의 회귀 문제에 적합하며 &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt; &lt;code&gt;ElasticNet&lt;/code&gt; 을&lt;/a&gt; 권장하는 다른 문제에 적합 합니다.</target>
        </trans-unit>
        <trans-unit id="5ca16e4c08c0574f508b6c526beeb2111eade54a" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;OneClassSVM&lt;/code&gt;&lt;/a&gt; implements a One-Class SVM which is used in outlier detection.</source>
          <target state="translated">클래스 &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;OneClassSVM&lt;/code&gt; 의&lt;/a&gt; 구현 이상치 탐지에 사용되는 하나의 클래스 SVM.</target>
        </trans-unit>
        <trans-unit id="5eaffd69df806c5b8353c88358cfd0a49b275168" translate="yes" xml:space="preserve">
          <source>The class label for which the PDPs should be computed. Only if gbrt is a multi-class model. Must be in &lt;code&gt;gbrt.classes_&lt;/code&gt;.</source>
          <target state="translated">PDP를 계산할 클래스 레이블입니다. gbrt가 다중 클래스 모델 인 경우에만 해당됩니다. &lt;code&gt;gbrt.classes_&lt;/code&gt; 에 있어야합니다 .</target>
        </trans-unit>
        <trans-unit id="2a31ab08188f24bd31e99f218cab5467eddf2ef0" translate="yes" xml:space="preserve">
          <source>The class labels.</source>
          <target state="translated">클래스 라벨.</target>
        </trans-unit>
        <trans-unit id="075fd1f474c95d9f3895b75ead4b1f9c99be54a8" translate="yes" xml:space="preserve">
          <source>The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;.</source>
          <target state="translated">입력 샘플의 클래스 로그 확률. 클래스의 순서는 &lt;code&gt;classes_&lt;/code&gt; 속성의 순서와 일치합니다 .</target>
        </trans-unit>
        <trans-unit id="f9e49144e04860d9a1f4a8512901a503e5d41c6f" translate="yes" xml:space="preserve">
          <source>The class of Matern kernels is a generalization of the RBF and the absolute exponential kernel parameterized by an additional parameter nu. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions).</source>
          <target state="translated">Matern 커널 클래스는 RBF의 일반화이며 추가 매개 변수 nu로 매개 변수화 된 절대 지수 커널입니다. nu가 작을수록 근사 함수의 부드러움이 줄어 듭니다. nu = inf의 경우 커널은 RBF 커널과 같고 nu = 0.5의 경우 절대 지수 커널과 같습니다. 중요한 중간 값은 nu = 1.5 (한 번 차별화 함수)와 nu = 2.5 (두 번 차별화 함수)입니다.</target>
        </trans-unit>
        <trans-unit id="e48205f573bda857c27ec3937eb80960daed3556" translate="yes" xml:space="preserve">
          <source>The class ordering is preserved:</source>
          <target state="translated">수업 순서는 유지됩니다.</target>
        </trans-unit>
        <trans-unit id="81478ef84d2d645ae9c4e632247f7f3bc9052a92" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of outputs is the same of that of the &lt;code&gt;classes_&lt;/code&gt; attribute.</source>
          <target state="translated">입력 샘플의 클래스 확률. 출력 순서는 &lt;code&gt;classes_&lt;/code&gt; 속성 의 순서와 동일 합니다.</target>
        </trans-unit>
        <trans-unit id="8ea2403e3ddde25cdaa7213df112f25928f5232d" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;.</source>
          <target state="translated">입력 샘플의 클래스 확률. 클래스의 순서는 &lt;code&gt;classes_&lt;/code&gt; 속성의 순서와 일치합니다 .</target>
        </trans-unit>
        <trans-unit id="64b22dfb54911895a611d3b9cfbaf3a3f14d97f5" translate="yes" xml:space="preserve">
          <source>The class to report if &lt;code&gt;average='binary'&lt;/code&gt; and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting &lt;code&gt;labels=[pos_label]&lt;/code&gt; and &lt;code&gt;average != 'binary'&lt;/code&gt; will report scores for that label only.</source>
          <target state="translated">&lt;code&gt;average='binary'&lt;/code&gt; 이고 데이터가 이진 인지보고 할 클래스 입니다. 데이터가 멀티 클래스 또는 멀티 라벨 인 경우 무시됩니다. 설정 &lt;code&gt;labels=[pos_label]&lt;/code&gt; 및 &lt;code&gt;average != 'binary'&lt;/code&gt; 라벨에만 점수를보고합니다.</target>
        </trans-unit>
        <trans-unit id="b02b6e7e5cca0ff24c2691f6e7d2bc9c247d17d1" translate="yes" xml:space="preserve">
          <source>The class to use to build the returned adjacency matrix.</source>
          <target state="translated">리턴 된 인접 행렬을 빌드하는 데 사용할 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="08bf02b9f7a917eed20c9c94c384791c0ed4578f" translate="yes" xml:space="preserve">
          <source>The class with respect to which we perform a one-vs-all fit. If None, then it is assumed that the given problem is binary.</source>
          <target state="translated">우리가 일대일 적합을 수행하는 클래스. None이면 주어진 문제가 이진 인 것으로 가정합니다.</target>
        </trans-unit>
        <trans-unit id="26dc8660d1112d77620f09027ce7b9fbee6027a3" translate="yes" xml:space="preserve">
          <source>The classes &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; provide functionality to fit linear models for classification and regression using different (convex) loss functions and different penalties. E.g., with &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt;, &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; fits a logistic regression model, while with &lt;code&gt;loss=&quot;hinge&quot;&lt;/code&gt; it fits a linear support vector machine (SVM).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; 클래스는 서로 다른 (볼록한) 손실 함수와 서로 다른 위약금을 사용하여 분류 및 회귀에 대한 선형 모델에 맞는 기능을 제공합니다. 예를 들어 &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; 로 &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 는 로지스틱 회귀 모델에 적합하고 &lt;code&gt;loss=&quot;hinge&quot;&lt;/code&gt; 로 선형 지원 벡터 머신 (SVM)에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="d29e2c71b01d97c9c93dc5aad937ec38e91b7ca9" translate="yes" xml:space="preserve">
          <source>The classes &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; provide two criteria to stop the algorithm when a given level of convergence is reached:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; 클래스 는 주어진 수렴 수준에 도달하면 알고리즘을 중지하는 두 가지 기준을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="d7ef8ada32aed500913b0cf053368e18ea306f33" translate="yes" xml:space="preserve">
          <source>The classes in &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; can handle either NumPy arrays or &lt;code&gt;scipy.sparse&lt;/code&gt; matrices as input. For dense matrices, a large number of possible distance metrics are supported. For sparse matrices, arbitrary Minkowski metrics are supported for searches.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; &lt;/a&gt; 의 클래스는 NumPy 배열 또는 &lt;code&gt;scipy.sparse&lt;/code&gt; 행렬을 입력으로 처리 할 수 ​​있습니다 . 밀도가 높은 매트릭스의 경우 가능한 많은 거리 메트릭이 지원됩니다. 희소 행렬의 경우 검색을 위해 임의의 Minkowski 메트릭이 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="f39d71815c55889dd46cb64855cc1d40f837d2d9" translate="yes" xml:space="preserve">
          <source>The classes in the &lt;a href=&quot;classes#module-sklearn.feature_selection&quot;&gt;&lt;code&gt;sklearn.feature_selection&lt;/code&gt;&lt;/a&gt; module can be used for feature selection/dimensionality reduction on sample sets, either to improve estimators&amp;rsquo; accuracy scores or to boost their performance on very high-dimensional datasets.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.feature_selection&quot;&gt; &lt;code&gt;sklearn.feature_selection&lt;/code&gt; &lt;/a&gt; 모듈 의 클래스는 샘플 세트의 기능 선택 / 차원 감소에 사용되어 추정기의 정확도 점수를 높이거나 매우 높은 차원의 데이터 집합에서 성능을 향상시킬 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="df672af473b055efb339763c1ea56e27edeec8c9" translate="yes" xml:space="preserve">
          <source>The classes in this submodule allow to approximate the embedding \(\phi\), thereby working explicitly with the representations \(\phi(x_i)\), which obviates the need to apply the kernel or store training examples.</source>
          <target state="translated">이 서브 모듈의 클래스는 임베드 \ (\ phi \)를 근사화하여 \ (\ phi (x_i) \) 표현과 함께 명시 적으로 작동하므로 커널을 적용하거나 교육 예제를 저장할 필요가 없습니다.</target>
        </trans-unit>
        <trans-unit id="41647bba96e968e0bd58965ae65f2bc0365d42c6" translate="yes" xml:space="preserve">
          <source>The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).</source>
          <target state="translated">클래스 레이블 (단일 출력 문제) 또는 클래스 레이블 배열 목록 (다중 출력 문제)</target>
        </trans-unit>
        <trans-unit id="772daeb9b0eaa408b264b0fa548b1cbd24779805" translate="yes" xml:space="preserve">
          <source>The classes labels.</source>
          <target state="translated">클래스 레이블.</target>
        </trans-unit>
        <trans-unit id="13f18ce429d97546a8b16cf132e8e6c03147cb50" translate="yes" xml:space="preserve">
          <source>The classic implementation of the clustering method based on the Lloyd&amp;rsquo;s algorithm. It consumes the whole set of input data at each iteration.</source>
          <target state="translated">로이드 알고리즘을 기반으로 한 클러스터링 방법의 고전적인 구현. 각 반복에서 전체 입력 데이터 세트를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="4b0c76ea0ae326d19f535ab9520b9dd024cf9124" translate="yes" xml:space="preserve">
          <source>The classification is performed by projecting to the first two principal components found by PCA and CCA for visualisation purposes, followed by using the &lt;a href=&quot;../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; metaclassifier using two SVCs with linear kernels to learn a discriminative model for each class. Note that PCA is used to perform an unsupervised dimensionality reduction, while CCA is used to perform a supervised one.</source>
          <target state="translated">분류는 시각화 목적으로 PCA와 CCA에서 찾은 첫 번째 두 가지 주요 구성 요소에 투영 한 다음 선형 커널이있는 두 개의 SVC를 사용하여 각 클래스의 차별적 모델을 배우는 &lt;a href=&quot;../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; &lt;/a&gt; 메타 분류기 를 사용하여 수행됩니다 . PCA는 감독되지 않은 차원 축소를 수행하는 데 사용되고 CCA는 감독 된 차원 축소를 수행하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="d9bcc8debd316bfd1c4b9e73630f6ba7215b9e3d" translate="yes" xml:space="preserve">
          <source>The classifier whose output decision function needs to be calibrated to offer more accurate predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.</source>
          <target state="translated">보다 정확한 predict_proba 출력을 제공하기 위해 출력 결정 기능을 교정해야하는 분류기. cv = prefit 인 경우 분류기는 이미 데이터에 적합해야합니다.</target>
        </trans-unit>
        <trans-unit id="88e39a3dfd7087282361f3b39109354f7ff32ede" translate="yes" xml:space="preserve">
          <source>The code below also illustrates how the construction and the computation of the predictions can be parallelized within multiple jobs.</source>
          <target state="translated">아래 코드는 예측의 구성과 계산이 여러 작업 내에서 병렬화되는 방법도 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="6a1cc4b5491e9b42fd445850086762fcb248e836" translate="yes" xml:space="preserve">
          <source>The code below plots the dependency of y against individual x_i and normalized values of univariate F-tests statistics and mutual information.</source>
          <target state="translated">아래 코드는 개별 x_i에 대한 y의 종속성과 일 변량 F- 검정 통계량 및 상호 정보의 정규화 된 값을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="4b0a65eb61a5277b5e28aef5c3b0e7ea577bdc07" translate="yes" xml:space="preserve">
          <source>The code-examples in the above tutorials are written in a &lt;em&gt;python-console&lt;/em&gt; format. If you wish to easily execute these examples in &lt;strong&gt;IPython&lt;/strong&gt;, use:</source>
          <target state="translated">위 튜토리얼의 코드 예제는 &lt;em&gt;python-console&lt;/em&gt; 형식으로 작성되었습니다 . &lt;strong&gt;IPython&lt;/strong&gt; 에서 이러한 예제를 쉽게 실행하려면 다음을 사용하십시오.</target>
        </trans-unit>
        <trans-unit id="ce298b790cf6df954f5ce5058b04debe04deb707" translate="yes" xml:space="preserve">
          <source>The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the regression sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">계수 R ^ 2는 (1-u / v)로 정의되며, 여기서 u는 잔차 제곱합 ((y_true-y_pred) ** 2) .sum ()이고 v는 회귀 제곱합 ((y_true- y_true.mean ()) ** 2) .sum (). 최고 점수는 1.0이며 음수가 될 수 있습니다 (모델이 임의로 악화 될 수 있기 때문에). 입력 특성을 무시하고 항상 y의 예상 값을 예측하는 상수 모델은 0.0의 R ^ 2 점수를 얻습니다.</target>
        </trans-unit>
        <trans-unit id="48121e6567bc65fc6ea38acd7d461ac73e2ea472" translate="yes" xml:space="preserve">
          <source>The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">계수 R ^ 2는 (1-u / v)로 정의되며, 여기서 u는 잔차 제곱합 ((y_true-y_pred) ** 2) .sum ()이고 v는 총 제곱합 ((y_true- y_true.mean ()) ** 2) .sum (). 최고 점수는 1.0이며 음수가 될 수 있습니다 (모델이 임의로 악화 될 수 있기 때문에). 입력 특성을 무시하고 항상 y의 예상 값을 예측하는 상수 모델은 0.0의 R ^ 2 점수를 얻습니다.</target>
        </trans-unit>
        <trans-unit id="f0dabbd241b37afcd99edc97579580c22f2703f9" translate="yes" xml:space="preserve">
          <source>The coefficient of the underlying linear model. It is returned only if coef is True.</source>
          <target state="translated">기본 선형 모형의 계수입니다. coef가 True 인 경우에만 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="b2933da174c8fa332cc9ad841c0c25a1ab1996d2" translate="yes" xml:space="preserve">
          <source>The coefficients can be forced to be positive.</source>
          <target state="translated">계수는 양수로 강제 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3c89d0ba57b5481659a50cc73b873694c355fd9b" translate="yes" xml:space="preserve">
          <source>The coefficients of the linear model: &lt;code&gt;Y = X coef_ + Err&lt;/code&gt;</source>
          <target state="translated">선형 모형의 계수 : &lt;code&gt;Y = X coef_ + Err&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a7757730afe97cb8593dea4757d82727872c7529" translate="yes" xml:space="preserve">
          <source>The coefficients, the residual sum of squares and the variance score are also calculated.</source>
          <target state="translated">계수, 잔차 제곱합 및 분산 점수도 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="f15e1f2c7fb96cd43c32056a6c407c9c1c570883" translate="yes" xml:space="preserve">
          <source>The collection of fitted base estimators.</source>
          <target state="translated">적합한 기본 추정기 모음입니다.</target>
        </trans-unit>
        <trans-unit id="e97d4f2642ef3372648a38fef3350e771647c3d6" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators as defined in &lt;code&gt;estimators&lt;/code&gt; that are not &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;None&lt;/code&gt; 이 아닌 &lt;code&gt;estimators&lt;/code&gt; 정의 된 적합 추정치 모음입니다 .</target>
        </trans-unit>
        <trans-unit id="0b6e390487ac628e8895818624f2b32fd0c95d36" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators.</source>
          <target state="translated">적합 추정치 모음입니다.</target>
        </trans-unit>
        <trans-unit id="929cef17b33a8aadbb4cff8f87d4813d9e794dca" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators. &lt;code&gt;loss_.K&lt;/code&gt; is 1 for binary classification, otherwise n_classes.</source>
          <target state="translated">적합 추정치 모음입니다. &lt;code&gt;loss_.K&lt;/code&gt; 는 이진 분류의 경우 1이고, 그렇지 않으면 n_classes입니다.</target>
        </trans-unit>
        <trans-unit id="34213cac4f6d6096a4a72655d4c46d8da28f907c" translate="yes" xml:space="preserve">
          <source>The collection of fitted transformers as tuples of (name, fitted_transformer, column). &lt;code&gt;fitted_transformer&lt;/code&gt; can be an estimator, &amp;lsquo;drop&amp;rsquo;, or &amp;lsquo;passthrough&amp;rsquo;. In case there were no columns selected, this will be the unfitted transformer. If there are remaining columns, the final element is a tuple of the form: (&amp;lsquo;remainder&amp;rsquo;, transformer, remaining_columns) corresponding to the &lt;code&gt;remainder&lt;/code&gt; parameter. If there are remaining columns, then &lt;code&gt;len(transformers_)==len(transformers)+1&lt;/code&gt;, otherwise &lt;code&gt;len(transformers_)==len(transformers)&lt;/code&gt;.</source>
          <target state="translated">튜플 (이름, 장착 _ 변압기, 열)로 장착 된 변압기 모음입니다. &lt;code&gt;fitted_transformer&lt;/code&gt; 는 추정기, 'drop'또는 'passthrough'일 수 있습니다. 열이 선택되지 않은 경우 이것은 적합하지 않은 변압기입니다. 나머지 열이있는 경우 마지막 요소는 &lt;code&gt;remainder&lt;/code&gt; 매개 변수에 해당하는 양식 ( 'remainder', 변환기, 잔여 _ 열)의 튜플입니다 . 남은 열이 있으면 &lt;code&gt;len(transformers_)==len(transformers)+1&lt;/code&gt; 이고, 그렇지 않으면 &lt;code&gt;len(transformers_)==len(transformers)&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="d6d471d12278a874c4225d83bbd25d7f04a0a976" translate="yes" xml:space="preserve">
          <source>The color map illustrates the decision function learned by the SVC.</source>
          <target state="translated">컬러 맵은 SVC가 학습 한 결정 기능을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="093f5986f8d055d5b47f11ad021ce6ecfcef91e9" translate="yes" xml:space="preserve">
          <source>The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.</source>
          <target state="translated">표시기 [n_nodes_ptr [i] : n_nodes_ptr [i + 1]]의 열은 i 번째 추정기에 대한 표시기 값을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="f0cca3e664aa52c0e892615c12365c0a6af40452" translate="yes" xml:space="preserve">
          <source>The combination used in this example is not particularly helpful on this dataset and is only used to illustrate the usage of FeatureUnion.</source>
          <target state="translated">이 예에서 사용 된 조합은이 데이터 세트에서 특별히 도움이되지 않으며 FeatureUnion의 사용법을 보여주기 위해서만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="4524a6f67fb21e2d5fb712c219044b7460dbad0e" translate="yes" xml:space="preserve">
          <source>The components of the random matrix are drawn from N(0, 1 / n_components).</source>
          <target state="translated">랜덤 행렬의 성분은 N (0, 1 / n_components)에서 추출됩니다.</target>
        </trans-unit>
        <trans-unit id="4d179dd17ec2e9e8ac79ee8fb26a8b8727033655" translate="yes" xml:space="preserve">
          <source>The compromise between l1 and l2 penalization chosen by cross validation</source>
          <target state="translated">교차 검증에 의해 선택된 l1과 l2 불이익의 절충</target>
        </trans-unit>
        <trans-unit id="37c8e8aa42af430070b2b87be4c5db5db294cc7b" translate="yes" xml:space="preserve">
          <source>The computation during &lt;code&gt;fit&lt;/code&gt; is:</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 중 계산 은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="cc728704a5b1deaa221b61ada3dd42ad8e661fbd" translate="yes" xml:space="preserve">
          <source>The computation during &lt;code&gt;predict&lt;/code&gt; is:</source>
          <target state="translated">&lt;code&gt;predict&lt;/code&gt; 중 계산 은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="cc5659751ed96e6a9a511b247760eba7bd9fab63" translate="yes" xml:space="preserve">
          <source>The computation of Davies-Bouldin is simpler than that of Silhouette scores.</source>
          <target state="translated">Davies-Bouldin의 계산은 Silhouette 점수의 계산보다 간단합니다.</target>
        </trans-unit>
        <trans-unit id="d526bd934b93818cdf8a3fccada8e6b2b34d84f2" translate="yes" xml:space="preserve">
          <source>The computational overhead of each SVD is &lt;code&gt;O(batch_size * n_features ** 2)&lt;/code&gt;, but only 2 * batch_size samples remain in memory at a time. There will be &lt;code&gt;n_samples / batch_size&lt;/code&gt; SVD computations to get the principal components, versus 1 large SVD of complexity &lt;code&gt;O(n_samples * n_features ** 2)&lt;/code&gt; for PCA.</source>
          <target state="translated">각 SVD의 계산 오버 헤드는 &lt;code&gt;O(batch_size * n_features ** 2)&lt;/code&gt; 이지만 한 번에 2 * batch_size 샘플 만 메모리에 남아 있습니다. 주요 구성 요소를 얻기위한 &lt;code&gt;n_samples / batch_size&lt;/code&gt; SVD 계산 이 있으며 PCA 의 경우 복잡한 &lt;code&gt;O(n_samples * n_features ** 2)&lt;/code&gt; 의 1 개의 큰 SVD가 있습니다 .</target>
        </trans-unit>
        <trans-unit id="2fa163515de812b4d78a2bbcef87e239a36ae4d1" translate="yes" xml:space="preserve">
          <source>The concept of early stopping is simple. We specify a &lt;code&gt;validation_fraction&lt;/code&gt; which denotes the fraction of the whole dataset that will be kept aside from training to assess the validation loss of the model. The gradient boosting model is trained using the training set and evaluated using the validation set. When each additional stage of regression tree is added, the validation set is used to score the model. This is continued until the scores of the model in the last &lt;code&gt;n_iter_no_change&lt;/code&gt; stages do not improve by atleast &lt;code&gt;tol&lt;/code&gt;. After that the model is considered to have converged and further addition of stages is &amp;ldquo;stopped early&amp;rdquo;.</source>
          <target state="translated">조기 중지의 개념은 간단합니다. 모델의 유효성 검증 손실을 평가하기 위해 학습과 별도로 유지 될 전체 데이터 세트의 비율을 나타내는 &lt;code&gt;validation_fraction&lt;/code&gt; 을 지정합니다 . 그라디언트 부스팅 모델은 훈련 세트를 사용하여 훈련되고 검증 세트를 사용하여 평가됩니다. 회귀 트리의 각 추가 단계가 추가되면 유효성 검사 세트가 모델 스코어링에 사용됩니다. 이것은 마지막 &lt;code&gt;n_iter_no_change&lt;/code&gt; 단계 에서 모델의 점수 가 적어도 &lt;code&gt;tol&lt;/code&gt; 에 의해 향상되지 않을 때까지 계속 됩니다 . 그 후 모델이 수렴 된 것으로 간주되고 단계의 추가가 &quot;조기 중지&quot;됩니다.</target>
        </trans-unit>
        <trans-unit id="feb60f640c62fd0856cb50e8401267d4f55d4774" translate="yes" xml:space="preserve">
          <source>The concrete &lt;code&gt;LossFunction&lt;/code&gt; object.</source>
          <target state="translated">구체적인 &lt;code&gt;LossFunction&lt;/code&gt; 객체입니다.</target>
        </trans-unit>
        <trans-unit id="4f21e1340272b60cf06a69153cc19d0140d625cf" translate="yes" xml:space="preserve">
          <source>The concrete loss function can be set via the &lt;code&gt;loss&lt;/code&gt; parameter. &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; supports the following loss functions:</source>
          <target state="translated">콘크리트 손실 기능은 &lt;code&gt;loss&lt;/code&gt; 매개 변수 를 통해 설정할 수 있습니다 . &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; 는 다음과 같은 손실 함수를 지원합니다.</target>
        </trans-unit>
        <trans-unit id="c24a50928ec0729629e45e8b53b6d8b9f0112f79" translate="yes" xml:space="preserve">
          <source>The concrete loss function can be set via the &lt;code&gt;loss&lt;/code&gt; parameter. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; supports the following loss functions:</source>
          <target state="translated">콘크리트 손실 기능은 &lt;code&gt;loss&lt;/code&gt; 매개 변수 를 통해 설정할 수 있습니다 . &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; 는 다음과 같은 손실 기능을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="a5b98bb304bc3ec6a422167859a2f5da5580fa47" translate="yes" xml:space="preserve">
          <source>The concrete penalty can be set via the &lt;code&gt;penalty&lt;/code&gt; parameter. SGD supports the following penalties:</source>
          <target state="translated">구체적 패널티는 &lt;code&gt;penalty&lt;/code&gt; 파라미터 를 통해 설정할 수 있습니다 . SGD는 다음과 같은 처벌을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="c0c66ccf788e88932593e02308e1dcae0bb5c1f0" translate="yes" xml:space="preserve">
          <source>The conditional probability distribution of each unit is given by the logistic sigmoid activation function of the input it receives:</source>
          <target state="translated">각 유닛의 조건부 확률 분포는 다음과 같은 입력을받는 로지스틱 시그 모이 드 활성화 함수에 의해 제공됩니다.</target>
        </trans-unit>
        <trans-unit id="6e94f7f28c2cc9bab8c93a85a4438b802d82d10d" translate="yes" xml:space="preserve">
          <source>The confidence score for a sample is the signed distance of that sample to the hyperplane.</source>
          <target state="translated">표본의 신뢰 점수는 해당 표본과 초평면 사이의 부호있는 거리입니다.</target>
        </trans-unit>
        <trans-unit id="b129f98741f30e6ae05e0872d86b02b5a921e905" translate="yes" xml:space="preserve">
          <source>The connectivity constraints are imposed via an connectivity matrix: a scipy sparse matrix that has elements only at the intersection of a row and a column with indices of the dataset that should be connected. This matrix can be constructed from a-priori information: for instance, you may wish to cluster web pages by only merging pages with a link pointing from one to another. It can also be learned from the data, for instance using &lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt;&lt;/a&gt; to restrict merging to nearest neighbors as in &lt;a href=&quot;../auto_examples/cluster/plot_agglomerative_clustering#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py&quot;&gt;this example&lt;/a&gt;, or using &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt;&lt;code&gt;sklearn.feature_extraction.image.grid_to_graph&lt;/code&gt;&lt;/a&gt; to enable only merging of neighboring pixels on an image, as in the &lt;a href=&quot;../auto_examples/cluster/plot_coin_ward_segmentation#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py&quot;&gt;coin&lt;/a&gt; example.</source>
          <target state="translated">연결 제약 조건은 연결 매트릭스를 통해 적용됩니다. 연결해야하는 데이터 집합의 인덱스가있는 행과 열의 교차 지점에만 요소가있는 scipy 스파 스 행렬. 이 매트릭스는 사전 정보로 구성 할 수 있습니다. 예를 들어 링크가있는 페이지 만 서로 링크하여 웹 페이지를 클러스터링 할 수 있습니다. &lt;a href=&quot;../auto_examples/cluster/plot_agglomerative_clustering#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py&quot;&gt;예를 들어 &lt;/a&gt;&lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt; &lt;code&gt;sklearn.feature_extraction.image.grid_to_graph&lt;/code&gt; &lt;/a&gt; 를 사용 하여이 예제 와 같이 가장 가까운 이웃으로의 병합을 제한 &lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt; &lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt; &lt;/a&gt; 를 사용하여 데이터에서 배울 수 있습니다 . &lt;a href=&quot;../auto_examples/cluster/plot_coin_ward_segmentation#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py&quot;&gt;동전&lt;/a&gt; 예.</target>
        </trans-unit>
        <trans-unit id="cba08514635a6da99fcbf046720afd85a65680ea" translate="yes" xml:space="preserve">
          <source>The constant value which defines the covariance: k(x_1, x_2) = constant_value</source>
          <target state="translated">공분산을 정의하는 상수 값 : k (x_1, x_2) = constant_value</target>
        </trans-unit>
        <trans-unit id="83836055dd86bf18d0180179c00f49511034950c" translate="yes" xml:space="preserve">
          <source>The contingency table calculated is typically utilized in the calculation of a similarity statistic (like the others listed in this document) between the two clusterings.</source>
          <target state="translated">계산 된 분할 표는 일반적으로 두 군집 간의 유사성 통계 (이 문서에 나열된 다른 통계)를 계산하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="ca0505a0687635a00a190438e7d26956e48c468c" translate="yes" xml:space="preserve">
          <source>The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.</source>
          <target state="translated">수렴 임계 값입니다. 하한 평균 게인이이 임계 값보다 낮 으면 EM 반복이 중지됩니다.</target>
        </trans-unit>
        <trans-unit id="0e26ef32f94bda64e8f6e396c3c303a3311ac4e1" translate="yes" xml:space="preserve">
          <source>The convergence threshold. EM iterations will stop when the lower bound average gain on the likelihood (of the training data with respect to the model) is below this threshold.</source>
          <target state="translated">수렴 임계 값입니다. 모델에 대한 훈련 데이터의 가능성에 대한 하한 평균 이득이이 임계 값보다 낮 으면 EM 반복이 중지됩니다.</target>
        </trans-unit>
        <trans-unit id="80f05388942910150044c6d47584d8bddc73ef4c" translate="yes" xml:space="preserve">
          <source>The converse mapping from feature name to column index is stored in the &lt;code&gt;vocabulary_&lt;/code&gt; attribute of the vectorizer:</source>
          <target state="translated">기능 이름에서 열 인덱스로의 대화 매핑 은 벡터 화기 의 &lt;code&gt;vocabulary_&lt;/code&gt; 속성에 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="42e30f7491cd0bbd1be9eaa1008d97761b319a5f" translate="yes" xml:space="preserve">
          <source>The converted and validated X.</source>
          <target state="translated">변환 및 검증 된 X</target>
        </trans-unit>
        <trans-unit id="043f2ec629cc510b9c1127fe6b10b6fe4448d241" translate="yes" xml:space="preserve">
          <source>The converted and validated y.</source>
          <target state="translated">변환되고 검증 된 y.</target>
        </trans-unit>
        <trans-unit id="81a7bc326e697c66d8802c043c85fabeabbf241b" translate="yes" xml:space="preserve">
          <source>The converted dataname.</source>
          <target state="translated">변환 된 데이터 이름</target>
        </trans-unit>
        <trans-unit id="e11d4d7608fe1fa28c8a673b1389ac6181cb7877" translate="yes" xml:space="preserve">
          <source>The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights \(w_1\), \(w_2\), &amp;hellip;, \(w_N\) to each of the training samples. Initially, those weights are all set to \(w_i = 1/N\), so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence &lt;a href=&quot;#htf&quot; id=&quot;id10&quot;&gt;[HTF]&lt;/a&gt;.</source>
          <target state="translated">AdaBoost의 핵심 원칙은 반복적으로 수정 된 데이터 버전에 약한 학습자 순서 (즉, 작은 의사 결정 트리와 같이 임의 추측보다 약간 더 나은 모델)를 맞추는 것입니다. 그런 다음 이들 모두의 예측은 가중 과반수 투표 (또는 합계)를 통해 결합되어 최종 예측을 생성합니다. 소위 부스팅 반복에서 데이터 수정은 각 트레이닝 샘플에 가중치 \ (w_1 \), \ (w_2 \),&amp;hellip;, \ (w_N \)를 적용하는 것으로 구성됩니다. 처음에는 이러한 가중치가 모두 \ (w_i = 1 / N \)으로 설정되어 있으므로 첫 번째 단계는 단순히 약한 학습자에게 원본 데이터를 학습시킵니다. 연속적인 반복마다 샘플 가중치가 개별적으로 수정되고 학습 알고리즘이 가중치가 적용된 데이터에 다시 적용됩니다. 주어진 단계에서이전 단계에서 유도 된 부스트 모델에 의해 잘못 예측 된 트레이닝 예제는 가중치가 증가하는 반면, 올바르게 예측 된 가중치는 감소합니다. 반복이 진행됨에 따라 예측하기 어려운 예는 계속 증가하는 영향을받습니다. 이에 따라 각각의 약한 학습자는 순서에서 이전 예제에서 놓친 예제에 집중해야합니다.&lt;a href=&quot;#htf&quot; id=&quot;id10&quot;&gt;[HTF]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="81757ae28a30627b83ded3cb76d108f655541caf" translate="yes" xml:space="preserve">
          <source>The correlation between each regressor and the target is computed, that is, ((X[:, i] - mean(X[:, i])) * (y - mean_y)) / (std(X[:, i]) * std(y)).</source>
          <target state="translated">각 회귀 변수와 목표 간의 상관 관계가 계산됩니다. 즉, ((XX :: i)-mean (X [:, i]) * (y-mean_y)) / (std (X [:, i]) ) * std (y)).</target>
        </trans-unit>
        <trans-unit id="302d596f93db30183d7b9abc1999d9cfb95f2d1f" translate="yes" xml:space="preserve">
          <source>The corresponding image is:</source>
          <target state="translated">해당 이미지는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="8f6395cc147644bb3051e08a46acbc9cd47145af" translate="yes" xml:space="preserve">
          <source>The cosine distance is defined as &lt;code&gt;1 - cosine_similarity&lt;/code&gt;: the lowest value is 0 (identical point) but it is bounded above by 2 for the farthest points. Its value does not depend on the norm of the vector points but only on their relative angles.</source>
          <target state="translated">코사인 거리는 &lt;code&gt;1 - cosine_similarity&lt;/code&gt; 로 정의됩니다 . 가장 낮은 값은 0 (동일 포인트)이지만 가장 먼 포인트의 경우 2로 제한됩니다. 그 값은 벡터 점의 규범에 의존하지 않고 상대 각도에만 의존합니다.</target>
        </trans-unit>
        <trans-unit id="c4b95b481d689de7dde58522ad60a16deab9f841" translate="yes" xml:space="preserve">
          <source>The cosine distance is equivalent to the half the squared euclidean distance if each sample is normalized to unit norm</source>
          <target state="translated">코사인 거리는 각 표본이 단위 규범으로 정규화되는 경우 제곱 유클리드 거리의 절반에 해당합니다.</target>
        </trans-unit>
        <trans-unit id="de99d6d126cf03f253c85bcda8fea4fdd47045eb" translate="yes" xml:space="preserve">
          <source>The cost function of an isomap embedding is</source>
          <target state="translated">아이소 맵 임베딩의 비용 함수는</target>
        </trans-unit>
        <trans-unit id="40200d49debb9a752670d9f4e91dc77cd9d66f0a" translate="yes" xml:space="preserve">
          <source>The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.</source>
          <target state="translated">트리를 사용하는 비용 (즉, 데이터 예측)은 트리를 훈련시키는 데 사용되는 데이터 포인트 수에 로그입니다.</target>
        </trans-unit>
        <trans-unit id="c9b6813659bffb0c08aeb974fcecdaaf97e786b0" translate="yes" xml:space="preserve">
          <source>The covariance matrix of a data set is known to be well approximated by the classical &lt;em&gt;maximum likelihood estimator&lt;/em&gt; (or &amp;ldquo;empirical covariance&amp;rdquo;), provided the number of observations is large enough compared to the number of features (the variables describing the observations). More precisely, the Maximum Likelihood Estimator of a sample is an unbiased estimator of the corresponding population&amp;rsquo;s covariance matrix.</source>
          <target state="translated">관측치 수가 피처 개수 (관측치를 설명하는 변수)에 비해 충분히 클 경우, 데이터 세트의 공분산 행렬은 고전적인 &lt;em&gt;최대 우도 추정기&lt;/em&gt; (또는 &quot;임시 공분산&quot;)에 의해 근사화되는 것으로 알려져 있습니다 . 보다 정확하게는 표본의 최대 우도 추정값은 해당 모집단의 공분산 행렬의 편향 추정량입니다.</target>
        </trans-unit>
        <trans-unit id="ff9747571dfcab155502f0d51c20ced0a4eda87c" translate="yes" xml:space="preserve">
          <source>The covariance matrix will be this value times the unit matrix. This dataset only produces symmetric normal distributions.</source>
          <target state="translated">공분산 행렬은이 값에 단위 행렬을 곱한 값입니다. 이 데이터 세트는 대칭 정규 분포 만 생성합니다.</target>
        </trans-unit>
        <trans-unit id="d4f25f1f739fb6455b136c8d6d48a91032e89970" translate="yes" xml:space="preserve">
          <source>The covariance of each mixture component. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">각 혼합 성분의 공분산. 모양은 &lt;code&gt;covariance_type&lt;/code&gt; 에 따라 다릅니다 .</target>
        </trans-unit>
        <trans-unit id="37e0aa90ea88b46d75d1f0f3b55b29aae27815f8" translate="yes" xml:space="preserve">
          <source>The covariance to compare with.</source>
          <target state="translated">비교할 공분산입니다.</target>
        </trans-unit>
        <trans-unit id="a8664fdb20de58ef24fb2b254f60a073e2dad9f4" translate="yes" xml:space="preserve">
          <source>The cross decomposition module contains two main families of algorithms: the partial least squares (PLS) and the canonical correlation analysis (CCA).</source>
          <target state="translated">교차 분해 모듈에는 부분 최소 제곱 (PLS)과 표준 상관 분석 (CCA)이라는 두 가지 주요 알고리즘 제품군이 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="5725093c72b0120c682574584b056d0b295efdd9" translate="yes" xml:space="preserve">
          <source>The cross validation score obtained on the training data</source>
          <target state="translated">교육 데이터에서 얻은 교차 검증 점수</target>
        </trans-unit>
        <trans-unit id="7d9ee6d7f0b44964f013a45e604e6c29cc8a3f8f" translate="yes" xml:space="preserve">
          <source>The cross-validation can then be performed easily:</source>
          <target state="translated">그런 다음 교차 검증을 쉽게 수행 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b91dc81955b1dbc1a750c7c0ae1e516f1cb5789e" translate="yes" xml:space="preserve">
          <source>The cross-validation score can be directly calculated using the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; helper. Given an estimator, the cross-validation object and the input dataset, the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; splits the data repeatedly into a training and a testing set, trains the estimator using the training set and computes the scores based on the testing set for each iteration of cross-validation.</source>
          <target state="translated">교차 유효성 검사 점수는 &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; 도우미를 사용하여 직접 계산할 수 있습니다 . 추정기, 교차 유효성 검사 객체 및 입력 데이터 집합이 주어지면 &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; 는 데이터를 학습 및 테스트 집합으로 반복 분할하고 학습 집합을 사용하여 추정 자를 학습하고 교차의 각 반복에 대한 테스트 집합을 기반으로 점수를 계산합니다. 확인.</target>
        </trans-unit>
        <trans-unit id="0c9e67494c034c3aceceb77302443d5c2bb54301" translate="yes" xml:space="preserve">
          <source>The cross-validation scores such that &lt;code&gt;grid_scores_[i]&lt;/code&gt; corresponds to the CV score of the i-th subset of features.</source>
          <target state="translated">&lt;code&gt;grid_scores_[i]&lt;/code&gt; 가 특징의 i 번째 부분 집합의 CV 점수에 해당하도록 교차 검증 점수 .</target>
        </trans-unit>
        <trans-unit id="7186db76e260a93ceff6e7a012a18028bce16f5b" translate="yes" xml:space="preserve">
          <source>The current implementation uses ball trees and kd-trees to determine the neighborhood of points, which avoids calculating the full distance matrix (as was done in scikit-learn versions before 0.14). The possibility to use custom metrics is retained; for details, see &lt;code&gt;NearestNeighbors&lt;/code&gt;.</source>
          <target state="translated">현재 구현에서는 볼 트리와 kd- 트리를 사용하여 포인트의 주변을 결정하여 전체 거리 매트릭스 계산을 피합니다 (0.14 이전의 scikit-learn 버전에서 수행됨). 맞춤 측정 항목을 사용할 가능성이 유지됩니다. 자세한 내용은 &lt;code&gt;NearestNeighbors&lt;/code&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="e3ec89890503565d6b8dae28b11789379133ea78" translate="yes" xml:space="preserve">
          <source>The current loss computed with the loss function.</source>
          <target state="translated">손실 함수로 계산 된 현재 손실.</target>
        </trans-unit>
        <trans-unit id="a415ef6f77731ce51df5a94c5015135ebd3f462c" translate="yes" xml:space="preserve">
          <source>The curse of dimensionality</source>
          <target state="translated">차원의 저주</target>
        </trans-unit>
        <trans-unit id="3eaad00bc0bba0577db9b826b646317a24a90409" translate="yes" xml:space="preserve">
          <source>The data</source>
          <target state="translated">자료</target>
        </trans-unit>
        <trans-unit id="973622d96c176a0a77b0dde8f6175466c5afec2c" translate="yes" xml:space="preserve">
          <source>The data is always a 2D array, shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt;, although the original data may have had a different shape. In the case of the digits, each original sample is an image of shape &lt;code&gt;(8, 8)&lt;/code&gt; and can be accessed using:</source>
          <target state="translated">원래 데이터의 모양이 다를 수 있지만 데이터는 항상 2D 배열 인 shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt; 입니다. 숫자의 경우 각 원본 샘플은 모양 &lt;code&gt;(8, 8)&lt;/code&gt; 의 이미지이며 다음을 사용하여 액세스 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="0e6af14908ee7619e613da95e5ef5e8434ef93d9" translate="yes" xml:space="preserve">
          <source>The data is assumed to be non-negative, and is often normalized to have an L1-norm of one. The normalization is rationalized with the connection to the chi squared distance, which is a distance between discrete probability distributions.</source>
          <target state="translated">데이터는 음이 아닌 것으로 가정되며, 종종 L1- 노름이 1로 정규화됩니다. 정규화는 카이 제곱 거리 (이산 확률 분포 사이의 거리)에 연결하여 합리화됩니다.</target>
        </trans-unit>
        <trans-unit id="aa2161d468d436ce31e1b1a4a535d19d4acf7995" translate="yes" xml:space="preserve">
          <source>The data is generated with the &lt;code&gt;make_checkerboard&lt;/code&gt; function, then shuffled and passed to the Spectral Biclustering algorithm. The rows and columns of the shuffled matrix are rearranged to show the biclusters found by the algorithm.</source>
          <target state="translated">&lt;code&gt;make_checkerboard&lt;/code&gt; 함수를 사용하여 데이터를 생성 한 다음 섞어서 스펙트럼 바이 클러스터링 알고리즘으로 전달합니다. 셔플 링 된 행렬의 행과 열은 알고리즘에서 찾은 biclusters를 보여주기 위해 재 배열됩니다.</target>
        </trans-unit>
        <trans-unit id="549dd87c3787b6b83f2456e4cff9d8aa392d12c1" translate="yes" xml:space="preserve">
          <source>The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine.</source>
          <target state="translated">이 데이터는 세 개의 다른 경운기에 의해 이탈리아의 같은 지역에서 재배 된 와인의 화학적 분석 결과입니다. 세 가지 유형의 와인에서 발견되는 다른 성분에 대해 13 가지의 다른 측정이 이루어집니다.</target>
        </trans-unit>
        <trans-unit id="279fbf7584e00e713412e8ff4546300536ffeb5a" translate="yes" xml:space="preserve">
          <source>The data matrix</source>
          <target state="translated">데이터 매트릭스</target>
        </trans-unit>
        <trans-unit id="a36ea8c8057a9ae699a69f099ee316c3295afaf3" translate="yes" xml:space="preserve">
          <source>The data matrix, with p features and n samples. The data set must be the one which was used to compute the raw estimates.</source>
          <target state="translated">p 개의 특징과 n 개의 샘플이있는 데이터 매트릭스. 데이터 세트는 원시 추정치를 계산하는 데 사용 된 데이터 세트 여야합니다.</target>
        </trans-unit>
        <trans-unit id="f0ed1480aeed96966c83d245c0e82839723677ea" translate="yes" xml:space="preserve">
          <source>The data matrix.</source>
          <target state="translated">데이터 매트릭스.</target>
        </trans-unit>
        <trans-unit id="70cee63c42777050ee29bb5c41f5ed813382b6c9" translate="yes" xml:space="preserve">
          <source>The data of the returned sparse matrix. By default it is int</source>
          <target state="translated">반환 된 희소 행렬의 데이터입니다. 기본적으로 int입니다</target>
        </trans-unit>
        <trans-unit id="13b8870b16632bb144f99a987da0a17b912fd261" translate="yes" xml:space="preserve">
          <source>The data of the returned sparse matrix. By default it is the dtype of img</source>
          <target state="translated">반환 된 희소 행렬의 데이터입니다. 기본적으로 img의 dtype입니다.</target>
        </trans-unit>
        <trans-unit id="a9c1f6d9c961581c21421066f6ce8f7c709084a9" translate="yes" xml:space="preserve">
          <source>The data on which &lt;code&gt;gbrt&lt;/code&gt; was trained.</source>
          <target state="translated">&lt;code&gt;gbrt&lt;/code&gt; 가 훈련 된 데이터 .</target>
        </trans-unit>
        <trans-unit id="c66d20ce5405cff4e02a00c321afd4016f52379a" translate="yes" xml:space="preserve">
          <source>The data on which &lt;code&gt;gbrt&lt;/code&gt; was trained. It is used to generate a &lt;code&gt;grid&lt;/code&gt; for the &lt;code&gt;target_variables&lt;/code&gt;. The &lt;code&gt;grid&lt;/code&gt; comprises &lt;code&gt;grid_resolution&lt;/code&gt; equally spaced points between the two &lt;code&gt;percentiles&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;gbrt&lt;/code&gt; 가 훈련 된 데이터 . 생성하는 데 사용되는 &lt;code&gt;grid&lt;/code&gt; 에 대한 &lt;code&gt;target_variables&lt;/code&gt; 을 . &lt;code&gt;grid&lt;/code&gt; 는 두 &lt;code&gt;percentiles&lt;/code&gt; 사이에 동일한 간격 의 그리드 &lt;code&gt;grid_resolution&lt;/code&gt; 구성 됩니다.</target>
        </trans-unit>
        <trans-unit id="65c0e8f3b46fedbf2f41b7e1589997d8de30db77" translate="yes" xml:space="preserve">
          <source>The data set contains images of hand-written digits: 10 classes where each class refers to a digit.</source>
          <target state="translated">데이터 세트에는 손으로 쓴 숫자의 이미지가 포함됩니다. 각 클래스는 숫자를 나타내는 10 개의 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="a49ccf8db4843f7af3fe75d2e385d4c5ae4a247b" translate="yes" xml:space="preserve">
          <source>The data that should be scaled.</source>
          <target state="translated">확장해야 할 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="4362ce6dec3d09ef8de03aa7af16c30845dd1b85" translate="yes" xml:space="preserve">
          <source>The data that should be transformed back.</source>
          <target state="translated">다시 변환해야하는 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="47ba75ee664e21a51401b4c1203a08e7a876f44e" translate="yes" xml:space="preserve">
          <source>The data to be transformed by subset.</source>
          <target state="translated">서브 세트로 변환 할 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="03bb048cfbbc3212fd60edf266a3822d3176ef87" translate="yes" xml:space="preserve">
          <source>The data to be transformed using a power transformation.</source>
          <target state="translated">전력 변환을 사용하여 변환 할 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="73ce115221fb870e05c90b8f043ca278fdee74a1" translate="yes" xml:space="preserve">
          <source>The data to be transformed.</source>
          <target state="translated">변환 할 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="df7f442215a9627450351a5239ccb7b837681c69" translate="yes" xml:space="preserve">
          <source>The data to binarize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">요소별로 이진화 할 데이터입니다. 불필요한 사본을 피하기 위해 scipy.sparse 행렬은 CSR 형식이어야합니다.</target>
        </trans-unit>
        <trans-unit id="5401bf0fdb954957c9a3f345344e508dbd6bac54" translate="yes" xml:space="preserve">
          <source>The data to binarize, element by element. scipy.sparse matrices should be in CSR or CSC format to avoid an un-necessary copy.</source>
          <target state="translated">요소별로 이진화 할 데이터입니다. scipy.sparse 매트릭스는 불필요한 사본을 피하기 위해 CSR 또는 CSC 형식이어야합니다.</target>
        </trans-unit>
        <trans-unit id="515108ae5aad51ada5b234ccff65d93cdd42711e" translate="yes" xml:space="preserve">
          <source>The data to center and scale.</source>
          <target state="translated">중심 및 스케일링 할 데이터.</target>
        </trans-unit>
        <trans-unit id="975edd1be086184330a8a3ebbf935588408d4a98" translate="yes" xml:space="preserve">
          <source>The data to determine the categories of each feature.</source>
          <target state="translated">각 기능의 범주를 결정하는 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="a77a7e1c81043cc97f700ad0d213253f486b563a" translate="yes" xml:space="preserve">
          <source>The data to encode.</source>
          <target state="translated">인코딩 할 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="d843c1d7e5e8986d5ad23251cff2a94ef8ee7955" translate="yes" xml:space="preserve">
          <source>The data to fit.</source>
          <target state="translated">맞는 데이터.</target>
        </trans-unit>
        <trans-unit id="e7fbe72ba2fcf8fff679b5e28b0fef9589e7589d" translate="yes" xml:space="preserve">
          <source>The data to fit. Can be for example a list, or an array.</source>
          <target state="translated">맞는 데이터. 예를 들어 목록 또는 배열 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="00725d5de44209593a99040fefb16433276154c2" translate="yes" xml:space="preserve">
          <source>The data to fit. Can be, for example a list, or an array at least 2d.</source>
          <target state="translated">맞는 데이터. 예를 들어 목록 또는 2d 이상의 배열 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="53846ac28d489b02b9949ef562a942f522e52d48" translate="yes" xml:space="preserve">
          <source>The data to normalize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">요소별로 정규화 할 데이터입니다. 불필요한 사본을 피하기 위해 scipy.sparse 행렬은 CSR 형식이어야합니다.</target>
        </trans-unit>
        <trans-unit id="c13e1ef3ee3dcba79bb567a64bd572c32814ce01" translate="yes" xml:space="preserve">
          <source>The data to normalize, row by row. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">정규화 할 데이터입니다. 불필요한 사본을 피하기 위해 scipy.sparse 행렬은 CSR 형식이어야합니다.</target>
        </trans-unit>
        <trans-unit id="a092ffb0fb8aec81b90c0802c73b2ae6cfa80dcc" translate="yes" xml:space="preserve">
          <source>The data to transform, row by row. Sparse input should preferably be in CSC format.</source>
          <target state="translated">행 단위로 변환 할 데이터입니다. 스파 스 입력은 CSC 형식이어야합니다.</target>
        </trans-unit>
        <trans-unit id="3f0555e5cf2c3f2eb80a653fe1f67911e223ec90" translate="yes" xml:space="preserve">
          <source>The data to transform.</source>
          <target state="translated">변환 할 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="f0a016c9443bbddef85796fd3691360a3c817ea0" translate="yes" xml:space="preserve">
          <source>The data used to compute the mean and standard deviation used for later scaling along the features axis.</source>
          <target state="translated">피쳐 축을 따라 나중에 스케일링하는 데 사용되는 평균 및 표준 편차를 계산하는 데 사용되는 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="55438370e51e74e9e8aa5e6ed6a37b2c111bd898" translate="yes" xml:space="preserve">
          <source>The data used to compute the median and quantiles used for later scaling along the features axis.</source>
          <target state="translated">피쳐 축을 따라 나중에 스케일링하는 데 사용되는 중앙값 및 Quantile을 계산하는 데 사용되는 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="0ae3bec0fd19bdece5355d54cf80d1ae7bcc7c26" translate="yes" xml:space="preserve">
          <source>The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.</source>
          <target state="translated">피쳐 축을 따라 나중에 스케일링하는 데 사용되는 기능별 최소값 및 최대 값을 계산하는 데 사용되는 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="e26762b4f27727d3210d3fe2a64ad24b366c1062" translate="yes" xml:space="preserve">
          <source>The data used to estimate the optimal transformation parameters.</source>
          <target state="translated">최적 변환 매개 변수를 추정하는 데 사용되는 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="56c320de81ba4f246c360453cbd2da4e63d63c7f" translate="yes" xml:space="preserve">
          <source>The data used to fit the model. If &lt;code&gt;copy_X=False&lt;/code&gt;, then &lt;code&gt;X_fit_&lt;/code&gt; is a reference. This attribute is used for the calls to transform.</source>
          <target state="translated">모형을 맞추는 데 사용 된 데이터입니다. 경우 &lt;code&gt;copy_X=False&lt;/code&gt; 다음 &lt;code&gt;X_fit_&lt;/code&gt; 은 참조입니다. 이 속성은 변환 호출에 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="03376550c822ad450e2d14840686210c4804ebc6" translate="yes" xml:space="preserve">
          <source>The data used to scale along the features axis.</source>
          <target state="translated">형상 축을 따라 스케일링하는 데 사용되는 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="c4374e3fd9ee0863ad791a18672a80272e651c9f" translate="yes" xml:space="preserve">
          <source>The data used to scale along the features axis. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csc_matrix&lt;/code&gt;. Additionally, the sparse matrix needs to be nonnegative if &lt;code&gt;ignore_implicit_zeros&lt;/code&gt; is False.</source>
          <target state="translated">형상 축을 따라 스케일링하는 데 사용되는 데이터입니다. 희소 행렬이 제공되면 희소 &lt;code&gt;csc_matrix&lt;/code&gt; 로 변환됩니다 . &lt;code&gt;ignore_implicit_zeros&lt;/code&gt; 가 False 인 경우 희소 행렬은 음수가 아니어야 합니다.</target>
        </trans-unit>
        <trans-unit id="848647eb355f23af31ef1bf06cbb1e0d945eea47" translate="yes" xml:space="preserve">
          <source>The data used to scale along the specified axis.</source>
          <target state="translated">지정된 축을 따라 스케일링하는 데 사용되는 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="db728424571c6d0aa73c940cbe613870747b4067" translate="yes" xml:space="preserve">
          <source>The data was used with many others for comparing various classifiers. The classes are separable, though only RDA has achieved 100% correct classification. (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) (All results using the leave-one-out technique)</source>
          <target state="translated">이 데이터는 다양한 분류기를 비교하기 위해 다른 많은 사람들과 함께 사용되었습니다. RDA만이 100 % 정확한 분류를 달성했지만 클래스는 분리 가능합니다. (RDA : 100 %, QDA 99.4 %, LDA 98.9 %, 1NN 96.1 % (z 변환 된 데이터)) (Leave-One-Out 기술을 사용한 모든 결과)</target>
        </trans-unit>
        <trans-unit id="660fe59aa4f1107f7d968ac4c0471ddbff4317b5" translate="yes" xml:space="preserve">
          <source>The data.</source>
          <target state="translated">자료.</target>
        </trans-unit>
        <trans-unit id="02bda070eb404181c9796aadeceacf78aac05356" translate="yes" xml:space="preserve">
          <source>The dataset can be compressed, either by removing exact duplicates if these occur in your data, or by using BIRCH. Then you only have a relatively small number of representatives for a large number of points. You can then provide a &lt;code&gt;sample_weight&lt;/code&gt; when fitting DBSCAN.</source>
          <target state="translated">데이터에서 중복이 발생하는 경우 정확한 중복을 제거하거나 BIRCH를 사용하여 데이터 세트를 압축 할 수 있습니다. 그런 다음 많은 포인트에 대해 상대적으로 적은 수의 담당자 만 있습니다. 그런 다음 &lt;code&gt;sample_weight&lt;/code&gt; 을 피팅 할 때 sample_weight 를 제공 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="6bbac4d66b74c67e38ab2b8e6e075ffda05e6522" translate="yes" xml:space="preserve">
          <source>The dataset is called &amp;ldquo;Twenty Newsgroups&amp;rdquo;. Here is the official description, quoted from the &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;website&lt;/a&gt;:</source>
          <target state="translated">데이터 세트를 &quot;20 개의 뉴스 그룹&quot;이라고합니다. 다음은 &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;웹 사이트&lt;/a&gt; 에서 인용 한 공식 설명입니다 .</target>
        </trans-unit>
        <trans-unit id="c4573e45f21f00151e7512eaf1b6ae1cd41e22bd" translate="yes" xml:space="preserve">
          <source>The dataset is from Zhu et al [1].</source>
          <target state="translated">데이터 세트는 Zhu et al [1]에서 가져온 것입니다.</target>
        </trans-unit>
        <trans-unit id="1a602d6b3832c4acaa760453be6cb4d26914f03f" translate="yes" xml:space="preserve">
          <source>The dataset is generated using the &lt;code&gt;make_biclusters&lt;/code&gt; function, which creates a matrix of small values and implants bicluster with large values. The rows and columns are then shuffled and passed to the Spectral Co-Clustering algorithm. Rearranging the shuffled matrix to make biclusters contiguous shows how accurately the algorithm found the biclusters.</source>
          <target state="translated">&lt;code&gt;make_biclusters&lt;/code&gt; 함수를 사용하여 데이터 세트를 생성합니다 .이 함수는 작은 값의 행렬을 작성하고 큰 값으로 bicluster를 이식합니다. 그런 다음 행과 열이 섞여 스펙트럼 공동 클러스터링 알고리즘으로 전달됩니다. 셔플 된 행렬을 재 배열하여 biclusters를 연속적으로 만들면 알고리즘이 biclusters를 얼마나 정확하게 찾았는지 알 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4131a4e690502c19383d02f0465052ba8df886e1" translate="yes" xml:space="preserve">
          <source>The dataset is structured such that points nearby in index order are nearby in parameter space, leading to an approximately block-diagonal matrix of K-nearest neighbors. Such a sparse graph is useful in a variety of circumstances which make use of spatial relationships between points for unsupervised learning: in particular, see &lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt;&lt;code&gt;sklearn.manifold.Isomap&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;sklearn.manifold.LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt;&lt;code&gt;sklearn.cluster.SpectralClustering&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">데이터 세트는 인덱스 순서로 근처의 포인트가 파라미터 공간에서 근처에 있도록 구성되어 K- 인접 이웃의 대략적인 블록 대각선 매트릭스로 이어진다. 이러한 희소 그래프는 &lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt; &lt;code&gt;sklearn.manifold.Isomap&lt;/code&gt; &lt;/a&gt; 학습을위한 포인트 간의 공간 관계를 사용하는 다양한 상황에서 유용합니다. 특히 sklearn.manifold.Isomap , &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;sklearn.manifold.LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt; &lt;code&gt;sklearn.cluster.SpectralClustering&lt;/code&gt; 을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="774649dea26c780c5e43d2464445b85ef137fcca" translate="yes" xml:space="preserve">
          <source>The dataset is the Boston Housing dataset (resp. 20 Newsgroups) for regression (resp. classification).</source>
          <target state="translated">데이터 집합은 회귀 (각 분류)에 대한 Boston Housing 데이터 집합 (각 20 개 뉴스 그룹)입니다.</target>
        </trans-unit>
        <trans-unit id="c0f7ef482ab49522d3cd87d64587ff804f6727c1" translate="yes" xml:space="preserve">
          <source>The dataset used for evaluation is a 2D grid of isotropic Gaussian clusters widely spaced.</source>
          <target state="translated">평가에 사용 된 데이터 세트는 넓은 간격으로 등방성 가우스 클러스터의 2D 그리드입니다.</target>
        </trans-unit>
        <trans-unit id="c07599d3a55d8254ba468cc109830533370f5dcf" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is Reuters-21578 as provided by the UCI ML repository. It will be automatically downloaded and uncompressed on first run.</source>
          <target state="translated">이 예에서 사용 된 데이터 집합은 UCI ML 리포지토리에서 제공 한 Reuters-21578입니다. 처음 실행하면 자동으로 다운로드되고 압축 해제됩니다.</target>
        </trans-unit>
        <trans-unit id="f610543312a82d7ead69951d61b82950d647ad3c" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is a preprocessed excerpt of the &amp;ldquo;Labeled Faces in the Wild&amp;rdquo;, aka &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;:</source>
          <target state="translated">이 예제에서 사용 된 데이터 세트는 사전 처리 된 &quot;야생의 레이블이있는 얼굴&quot;(일명 &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW)&lt;/a&gt; 발췌입니다 .</target>
        </trans-unit>
        <trans-unit id="257296081e1c4b1a90a4ff7a918d81d5746f6705" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is a preprocessed excerpt of the &amp;ldquo;Labeled Faces in the Wild&amp;rdquo;, also known as &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;:</source>
          <target state="translated">이 예제에서 사용 된 데이터 세트는 &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt; 라고도 알려진 &quot;야생의 레이블이있는 얼굴&quot;의 사전 처리 된 발췌입니다 .</target>
        </trans-unit>
        <trans-unit id="0e491b7a83df0aa734f40ae016e056d9d5ef3d92" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is the 20 newsgroups dataset which will be automatically downloaded and then cached and reused for the document classification example.</source>
          <target state="translated">이 예에서 사용 된 데이터 세트는 20 개의 뉴스 그룹 데이터 세트이며 자동으로 다운로드 된 후 문서 분류 예에 대해 캐시 및 재사용됩니다.</target>
        </trans-unit>
        <trans-unit id="b391209f39032c4c37a7d267548501e64198b514" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is the 20 newsgroups dataset. It will be automatically downloaded, then cached.</source>
          <target state="translated">이 예에서 사용 된 데이터 집합은 20 개의 뉴스 그룹 데이터 집합입니다. 자동으로 다운로드 된 다음 캐시됩니다.</target>
        </trans-unit>
        <trans-unit id="b6261e60f08853c79c52801fadd1554784335b15" translate="yes" xml:space="preserve">
          <source>The dataset used is the Wine Dataset available at UCI. This dataset has continuous features that are heterogeneous in scale due to differing properties that they measure (i.e alcohol content, and malic acid).</source>
          <target state="translated">사용 된 데이터 세트는 UCI에서 사용 가능한 와인 데이터 세트입니다. 이 데이터 세트에는 측정하는 특성 (알코올 함량 및 말산)이 다르기 때문에 이질적인 규모의 이종 특성이 있습니다.</target>
        </trans-unit>
        <trans-unit id="2320c3b81471635dac46a3209d55417f5e2427c2" translate="yes" xml:space="preserve">
          <source>The dataset will be downloaded from the &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume5/lewis04a/&quot;&gt;rcv1 homepage&lt;/a&gt; if necessary. The compressed size is about 656 MB.</source>
          <target state="translated">필요한 경우 &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume5/lewis04a/&quot;&gt;rcv1 홈페이지&lt;/a&gt; 에서 데이터 세트가 다운로드됩니다 . 압축 된 크기는 약 656MB입니다.</target>
        </trans-unit>
        <trans-unit id="a104c196a5a6b4961911da3fe75edb0c176425f0" translate="yes" xml:space="preserve">
          <source>The datasets also contain a full description in their &lt;code&gt;DESCR&lt;/code&gt; attribute and some contain &lt;code&gt;feature_names&lt;/code&gt; and &lt;code&gt;target_names&lt;/code&gt;. See the dataset descriptions below for details.</source>
          <target state="translated">데이터 세트에는 &lt;code&gt;DESCR&lt;/code&gt; 속성 에 대한 전체 설명이 포함되어 있으며 일부에는 &lt;code&gt;feature_names&lt;/code&gt; 및 &lt;code&gt;target_names&lt;/code&gt; 가 포함되어 있습니다 . 자세한 내용은 아래 데이터 세트 설명을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="f5da29d39d3005818dbe52be4adbd180f7105540" translate="yes" xml:space="preserve">
          <source>The decision function is:</source>
          <target state="translated">결정 기능은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c0e6f8aea5cc6c4291ed60eb8cccabc092233917" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The columns correspond to the classes in sorted order, as they appear in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">입력 샘플의 결정 기능. 열은 &lt;code&gt;classes_&lt;/code&gt; 속성에 표시된 대로 정렬 된 순서로 클래스에 해당합니다 . 회귀 및 이진 분류는 &lt;code&gt;k == 1&lt;/code&gt; 인 특수한 경우이며 , 그렇지 않으면 &lt;code&gt;k==n_classes&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="06ac27333a17bb0e5a70b7bdd7a4a5eafb52284d" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of outputs is the same of that of the &lt;code&gt;classes_&lt;/code&gt; attribute. Binary classification is a special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;. For binary classification, values closer to -1 or 1 mean more like the first or second class in &lt;code&gt;classes_&lt;/code&gt;, respectively.</source>
          <target state="translated">입력 샘플의 결정 기능. 출력 순서는 &lt;code&gt;classes_&lt;/code&gt; 속성 의 순서와 동일 합니다. 이진 분류는 &lt;code&gt;k == 1&lt;/code&gt; 인 특수한 경우 이고, 그렇지 않으면 &lt;code&gt;k==n_classes&lt;/code&gt; 입니다. 이진 분류의 경우 -1 또는 1에 가까운 값은 각각 &lt;code&gt;classes_&lt;/code&gt; 의 첫 번째 또는 두 번째 클래스와 비슷 합니다.</target>
        </trans-unit>
        <trans-unit id="271144c81e33420b0281bbd1b4c29fbb1374c6ac" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">입력 샘플의 결정 기능. 클래스의 순서는 &lt;code&gt;classes_&lt;/code&gt; 속성의 순서와 일치합니다 . 회귀 및 이진 분류는 &lt;code&gt;k == 1&lt;/code&gt; 인 특수한 경우이며 , 그렇지 않으면 &lt;code&gt;k==n_classes&lt;/code&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="c04fb9b1c64a374fbc2424dc1dc2e69edec92fae" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="translated">입력 샘플의 결정 기능. 클래스의 순서는 &lt;code&gt;classes_&lt;/code&gt; 속성의 순서와 일치합니다 . 회귀 및 이진 분류는 모양 [n_samples]의 배열을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="7a4d1c2d06404b468f740213ed27426daa73066a" translate="yes" xml:space="preserve">
          <source>The decision rule for Bernoulli naive Bayes is based on</source>
          <target state="translated">Bernoulli 순진 Bayes의 결정 규칙은</target>
        </trans-unit>
        <trans-unit id="0e0b3ca71cdfb4dae3b0e05518ff98bb98da9e44" translate="yes" xml:space="preserve">
          <source>The decision tree structure can be analysed to gain further insight on the relation between the features and the target to predict. In this example, we show how to retrieve:</source>
          <target state="translated">의사 결정 트리 구조를 분석하여 피처와 예측 대상 간의 관계에 대한 추가 정보를 얻을 수 있습니다. 이 예에서는 다음을 검색하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="7930dea6ad7960124a06f25452c69dc408a6af03" translate="yes" xml:space="preserve">
          <source>The decision tree to be exported to GraphViz.</source>
          <target state="translated">GraphViz로 내보낼 의사 결정 트리.</target>
        </trans-unit>
        <trans-unit id="41d54cbac8ff4bb674faa62fe3a273da70a2c8ec" translate="yes" xml:space="preserve">
          <source>The decision values for the samples are computed by adding the normalized sum of pair-wise classification confidence levels to the votes in order to disambiguate between the decision values when the votes for all the classes are equal leading to a tie.</source>
          <target state="translated">표본에 대한 결정 값은 모든 등급에 대한 투표가 동일 할 때 결정 값을 명확하게하기 위해 표준화 된 쌍별 분류 신뢰 수준의 합을 투표에 추가하여 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="9db4a0cde49703606f721a9d732403d53161167d" translate="yes" xml:space="preserve">
          <source>The decoding strategy depends on the vectorizer parameters.</source>
          <target state="translated">디코딩 전략은 벡터 라이저 매개 변수에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="8ab5c7033c87e7cea9882c536950d8e60cf30550" translate="yes" xml:space="preserve">
          <source>The default coding of images is based on the &lt;code&gt;uint8&lt;/code&gt; dtype to spare memory. Often machine learning algorithms work best if the input is converted to a floating point representation first. Also, if you plan to use &lt;code&gt;matplotlib.pyplpt.imshow&lt;/code&gt; don&amp;rsquo;t forget to scale to the range 0 - 1 as done in the following example.</source>
          <target state="translated">이미지의 기본 코딩은 메모리를 절약하기 위한 &lt;code&gt;uint8&lt;/code&gt; dtype을 기반으로합니다 . 입력이 부동 소수점 표현으로 먼저 변환되는 경우 종종 기계 학습 알고리즘이 가장 잘 작동합니다. 또한 &lt;code&gt;matplotlib.pyplpt.imshow&lt;/code&gt; 를 사용하려는 경우 다음 예에서와 같이 0-1 범위로 조정하는 것을 잊지 마십시오.</target>
        </trans-unit>
        <trans-unit id="ec537a685ef4f8a1d5936e658cd9e5b93a584d77" translate="yes" xml:space="preserve">
          <source>The default configuration tokenizes the string by extracting words of at least 2 letters. The specific function that does this step can be requested explicitly:</source>
          <target state="translated">기본 구성은 2 자 이상의 단어를 추출하여 문자열을 토큰 화합니다. 이 단계를 수행하는 특정 기능을 명시 적으로 요청할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3d025c574ea3cc62ecf87d5b0a9f83d7b3c75d6a" translate="yes" xml:space="preserve">
          <source>The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module &lt;a href=&quot;../classes#module-sklearn.model_selection&quot;&gt;&lt;code&gt;sklearn.model_selection&lt;/code&gt;&lt;/a&gt; module for the list of possible cross-validation objects.</source>
          <target state="translated">사용되는 기본 교차 유효성 검사 생성기는 Stratified K-Folds입니다. 정수가 제공되면 사용 된 접기 수입니다. 가능한 교차 유효성 검사 객체 목록은 &lt;a href=&quot;../classes#module-sklearn.model_selection&quot;&gt; &lt;code&gt;sklearn.model_selection&lt;/code&gt; &lt;/a&gt; 모듈을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="53050da8c5d49a141b5e2d80a70c0fa67fd7eb00" translate="yes" xml:space="preserve">
          <source>The default dataset is the digits dataset. To run the example on the twenty newsgroups dataset, pass the &amp;ndash;twenty-newsgroups command line argument to this script.</source>
          <target state="translated">기본 데이터 세트는 숫자 데이터 세트입니다. 20 개의 뉴스 그룹 데이터 세트에서 예제를 실행하려면 &amp;ndash;twenty-newsgroups 명령 행 인수를이 스크립트에 전달하십시오.</target>
        </trans-unit>
        <trans-unit id="713044bd0fefe6c8ff38e7a00814412176e5caf0" translate="yes" xml:space="preserve">
          <source>The default error message is, &amp;ldquo;This %(name)s instance is not fitted yet. Call &amp;lsquo;fit&amp;rsquo; with appropriate arguments before using this method.&amp;rdquo;</source>
          <target state="translated">기본 오류 메시지는&amp;ldquo;이 % (name) s 인스턴스가 아직 설치되지 않았습니다. 이 방법을 사용하기 전에 적절한 인수로 'fit'을 호출하십시오.&amp;rdquo;</target>
        </trans-unit>
        <trans-unit id="bd1a08c69ea4f3f536ea9f567759a2eca8958b5b" translate="yes" xml:space="preserve">
          <source>The default parameters (n_samples / n_features / n_components) should make the example runnable in a couple of tens of seconds. You can try to increase the dimensions of the problem, but be aware that the time complexity is polynomial in NMF. In LDA, the time complexity is proportional to (n_samples * iterations).</source>
          <target state="translated">기본 매개 변수 (n_samples / n_features / n_components)는 수십 초 안에 예제를 실행할 수 있어야합니다. 문제의 차원을 늘리려 고 시도 할 수 있지만 NMF에서 시간 복잡성은 다항식입니다. LDA에서 시간 복잡도는 (n_samples * 반복)에 비례합니다.</target>
        </trans-unit>
        <trans-unit id="e964bf4dcf7d784d0dee842886fe27c5060b1af1" translate="yes" xml:space="preserve">
          <source>The default setting is &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt;. The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter &lt;code&gt;l1_ratio&lt;/code&gt; controls the convex combination of L1 and L2 penalty.</source>
          <target state="translated">기본 설정은 &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; 입니다. L1 페널티는 스파 스 해를 유발하여 대부분의 계수를 0으로 만듭니다. Elastic Net은 상관 관계가 높은 속성이있는 경우 L1 페널티의 일부 결함을 해결합니다. &lt;code&gt;l1_ratio&lt;/code&gt; 매개 변수 는 L1 및 L2 페널티의 볼록한 조합을 제어합니다.</target>
        </trans-unit>
        <trans-unit id="d600d29df52c52da3fc6f38fdcf73036ecf8cbdf" translate="yes" xml:space="preserve">
          <source>The default slice is a rectangular shape around the face, removing most of the background:</source>
          <target state="translated">기본 슬라이스는 얼굴 주위의 직사각형 모양으로 대부분의 배경을 제거합니다.</target>
        </trans-unit>
        <trans-unit id="f2f8fa0d6181b12438bd7660924ca4c2f89302ad" translate="yes" xml:space="preserve">
          <source>The default solver is &amp;lsquo;svd&amp;rsquo;. It can perform both classification and transform, and it does not rely on the calculation of the covariance matrix. This can be an advantage in situations where the number of features is large. However, the &amp;lsquo;svd&amp;rsquo; solver cannot be used with shrinkage.</source>
          <target state="translated">기본 솔버는 'svd'입니다. 분류와 변환을 모두 수행 할 수 있으며 공분산 행렬의 계산에 의존하지 않습니다. 이는 기능 수가 많은 상황에서 이점이 될 수 있습니다. 그러나 'svd'솔버는 축소와 함께 사용할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="2307e8c1575f885543e9c12c3e37adbd9d66c3dd" translate="yes" xml:space="preserve">
          <source>The default strategy implements one step of the bootstrapping procedure.</source>
          <target state="translated">기본 전략은 부트 스트랩 절차의 한 단계를 구현합니다.</target>
        </trans-unit>
        <trans-unit id="f17725906dcf13fd5a8abccea776a77c1270d7a9" translate="yes" xml:space="preserve">
          <source>The default value &lt;code&gt;max_features=&quot;auto&quot;&lt;/code&gt; uses &lt;code&gt;n_features&lt;/code&gt; rather than &lt;code&gt;n_features / 3&lt;/code&gt;. The latter was originally suggested in [1], whereas the former was more recently justified empirically in [2].</source>
          <target state="translated">기본 값 &lt;code&gt;max_features=&quot;auto&quot;&lt;/code&gt; 사용의 &lt;code&gt;n_features&lt;/code&gt; 보다는 &lt;code&gt;n_features / 3&lt;/code&gt; . 후자는 원래 [1]에서 제안되었지만 전자는 [2]에서 경험적으로 더 정당화되었다.</target>
        </trans-unit>
        <trans-unit id="8ad5c264779356671425d0f732ca8de7347758f3" translate="yes" xml:space="preserve">
          <source>The default values for the parameters controlling the size of the trees (e.g. &lt;code&gt;max_depth&lt;/code&gt;, &lt;code&gt;min_samples_leaf&lt;/code&gt;, etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values.</source>
          <target state="translated">트리 크기를 제어하는 ​​매개 변수의 기본값 (예 : &lt;code&gt;max_depth&lt;/code&gt; , &lt;code&gt;min_samples_leaf&lt;/code&gt; 등)은 완전히 자르고 정리되지 않은 트리로 이어지며 일부 데이터 세트에서 잠재적으로 매우 클 수 있습니다. 메모리 소비를 줄이려면 트리의 복잡성과 크기를 해당 매개 변수 값을 설정하여 제어해야합니다.</target>
        </trans-unit>
        <trans-unit id="4209f696edb2b5615bc39c07cc72cc1b347ebd54" translate="yes" xml:space="preserve">
          <source>The definitive description of key concepts and API elements for using scikit-learn and developing compatible tools.</source>
          <target state="translated">scikit-learn을 사용하고 호환 가능한 도구를 개발하기위한 주요 개념과 API 요소에 대한 명확한 설명.</target>
        </trans-unit>
        <trans-unit id="767ee5df767908c4f8729c932c4a3d808fe558cd" translate="yes" xml:space="preserve">
          <source>The degree of the polynomial features. Default = 2.</source>
          <target state="translated">다항식 특징의 정도입니다. 기본값은 2입니다.</target>
        </trans-unit>
        <trans-unit id="c0ae4038f8b612638e22a63002b9a0592ebf4ed6" translate="yes" xml:space="preserve">
          <source>The density of w, between 0 and 1</source>
          <target state="translated">0과 1 사이의 w의 밀도</target>
        </trans-unit>
        <trans-unit id="64211b28a272c8cc95fc17412f0b29158a9e57aa" translate="yes" xml:space="preserve">
          <source>The desired absolute tolerance of the result. A larger tolerance will generally lead to faster execution. Default is 0.</source>
          <target state="translated">결과의 원하는 절대 공차. 공차가 클수록 일반적으로 실행 속도가 빨라집니다. 기본값은 0입니다.</target>
        </trans-unit>
        <trans-unit id="d976f3ec62dae27876361a8dc74b8b1f3089859a" translate="yes" xml:space="preserve">
          <source>The desired relative tolerance of the result. A larger tolerance will generally lead to faster execution. Default is 1E-8.</source>
          <target state="translated">결과의 원하는 상대 공차. 공차가 클수록 일반적으로 실행 속도가 빨라집니다. 기본값은 1E-8입니다.</target>
        </trans-unit>
        <trans-unit id="e06750706d15ac4ccfc7d440948abe624b2a5a8e" translate="yes" xml:space="preserve">
          <source>The diabetes dataset consists of 10 physiological variables (age, sex, weight, blood pressure) measure on 442 patients, and an indication of disease progression after one year:</source>
          <target state="translated">당뇨병 데이터 세트는 442 명의 환자에 대한 10 가지 생리 학적 변수 (연령, 성별, 체중, 혈압) 측정치와 1 년 후 질병 진행의 지표로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="9fc4977037d4a4b0eddabbe726bd2d0de7d06481" translate="yes" xml:space="preserve">
          <source>The dict at &lt;code&gt;search.cv_results_['params'][search.best_index_]&lt;/code&gt; gives the parameter setting for the best model, that gives the highest mean score (&lt;code&gt;search.best_score_&lt;/code&gt;).</source>
          <target state="translated">&lt;code&gt;search.cv_results_['params'][search.best_index_]&lt;/code&gt; 의 dict 는 가장 높은 평균 점수 ( &lt;code&gt;search.best_score_&lt;/code&gt; ) 를 제공하는 최상의 모델에 대한 매개 변수 설정을 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="bfaa3d320392a9f6c8addb9130a129b654790105" translate="yes" xml:space="preserve">
          <source>The dictionary atoms used for sparse coding. Lines are assumed to be normalized to unit norm.</source>
          <target state="translated">희소 코딩에 사용되는 사전 원자. 선은 단위 규범으로 정규화되는 것으로 가정합니다.</target>
        </trans-unit>
        <trans-unit id="7373631cc038017694fe3a2fdfb8d4bfc8e42605" translate="yes" xml:space="preserve">
          <source>The dictionary factor in the matrix factorization.</source>
          <target state="translated">행렬 분해의 사전 인수입니다.</target>
        </trans-unit>
        <trans-unit id="09a0fc68f904b631d6aa1871e7b81e42ac764443" translate="yes" xml:space="preserve">
          <source>The dictionary is fitted on the distorted left half of the image, and subsequently used to reconstruct the right half. Note that even better performance could be achieved by fitting to an undistorted (i.e. noiseless) image, but here we start from the assumption that it is not available.</source>
          <target state="translated">사전은 이미지의 왜곡 된 왼쪽 절반에 맞춰지고 오른쪽 절반을 재구성하는 데 사용됩니다. 왜곡되지 않은 (즉, 노이즈가없는) 이미지에 맞춰 더 나은 성능을 얻을 수 있지만 여기서는 사용할 수 없다는 가정에서 시작합니다.</target>
        </trans-unit>
        <trans-unit id="03531d72d72ed916646e794ac15c0622fe9c9b59" translate="yes" xml:space="preserve">
          <source>The dictionary learning objects offer, via the &lt;code&gt;split_code&lt;/code&gt; parameter, the possibility to separate the positive and negative values in the results of sparse coding. This is useful when dictionary learning is used for extracting features that will be used for supervised learning, because it allows the learning algorithm to assign different weights to negative loadings of a particular atom, from to the corresponding positive loading.</source>
          <target state="translated">사전 학습 객체는 &lt;code&gt;split_code&lt;/code&gt; 매개 변수 를 통해 희소 코딩 결과에서 양수 값과 음수 값을 분리 할 수있는 가능성을 제공합니다. 이것은 사전 학습이 감독 학습에 사용될 기능을 추출하는 데 사용될 때 유용합니다. 학습 알고리즘은 학습 알고리즘이 특정 원자의 음의 하중에 해당하는 양의 하중에 다른 가중치를 할당 할 수 있기 때문입니다.</target>
        </trans-unit>
        <trans-unit id="c5012873b1ea68e5deed2b04cdb4e90687b2c340" translate="yes" xml:space="preserve">
          <source>The dictionary matrix against which to solve the sparse coding of the data. Some of the algorithms assume normalized rows for meaningful output.</source>
          <target state="translated">데이터의 희소 코딩을 해결하기위한 사전 매트릭스. 일부 알고리즘은 의미있는 출력을 위해 정규화 된 행을 가정합니다.</target>
        </trans-unit>
        <trans-unit id="f137f3a53fdf8d92f7463e89f6383b5a88f3c320" translate="yes" xml:space="preserve">
          <source>The dictionary with normalized components (D).</source>
          <target state="translated">정규화 된 구성 요소가있는 사전 (D)</target>
        </trans-unit>
        <trans-unit id="d49d4bb0bf7407d291ade3beb50a3cbdf55548f8" translate="yes" xml:space="preserve">
          <source>The difference between LeavePGroupsOut and GroupShuffleSplit is that the former generates splits using all subsets of size &lt;code&gt;p&lt;/code&gt; unique groups, whereas GroupShuffleSplit generates a user-determined number of random test splits, each with a user-determined fraction of unique groups.</source>
          <target state="translated">LeavePGroupsOut과 GroupShuffleSplit의 차이점은 전자가 크기 &lt;code&gt;p&lt;/code&gt; 고유 그룹 의 모든 서브 세트를 사용하여 분할을 생성하는 반면 GroupShuffleSplit은 사용자가 결정한 고유 그룹의 일부를 사용하여 사용자가 결정한 임의의 테스트 분할을 생성한다는 것입니다.</target>
        </trans-unit>
        <trans-unit id="f8e596d2cfcc0c0ae56904e18bdffdb0d463a655" translate="yes" xml:space="preserve">
          <source>The difference between LeavePGroupsOut and LeaveOneGroupOut is that the former builds the test sets with all the samples assigned to &lt;code&gt;p&lt;/code&gt; different values of the groups while the latter uses samples all assigned the same groups.</source>
          <target state="translated">LeavePGroupsOut과 LeaveOneGroupOut의 차이점은 전자 는 그룹에 서로 다른 &lt;code&gt;p&lt;/code&gt; 개의 값에 할당 된 모든 샘플을 사용하여 테스트 세트를 작성하는 반면, 후자는 동일한 그룹에 할당 된 샘플을 사용한다는 점입니다.</target>
        </trans-unit>
        <trans-unit id="123fc66dcb39304255b8e25c6c10c654dbb2b0f3" translate="yes" xml:space="preserve">
          <source>The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of \(P(x_i \mid y)\).</source>
          <target state="translated">다른 순진 Bayes 분류기는 주로 \ (P (x_i \ mid y) \)의 분포에 대한 가정에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="fc85907a08e94b0f17af8c0c01aad9ef06729185" translate="yes" xml:space="preserve">
          <source>The digits dataset is made of 1797 8x8 images of hand-written digits</source>
          <target state="translated">자릿수 데이터 세트는 손으로 쓴 자릿수의 1797 8x8 이미지로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="46ad3d3589f97f144056aa3785c730497272c03e" translate="yes" xml:space="preserve">
          <source>The dimension of the projected subspace.</source>
          <target state="translated">투영 된 부분 공간의 치수.</target>
        </trans-unit>
        <trans-unit id="5572a493038acd1179abd12e1cd0c48e6709dea1" translate="yes" xml:space="preserve">
          <source>The dimension of the projection subspace.</source>
          <target state="translated">투영 부분 공간의 치수.</target>
        </trans-unit>
        <trans-unit id="984d56bddca949d9f1aa278aca87eb46a4072ffd" translate="yes" xml:space="preserve">
          <source>The dimensionality of the resulting representation is &lt;code&gt;n_out &amp;lt;= n_estimators * max_leaf_nodes&lt;/code&gt;. If &lt;code&gt;max_leaf_nodes == None&lt;/code&gt;, the number of leaf nodes is at most &lt;code&gt;n_estimators * 2 ** max_depth&lt;/code&gt;.</source>
          <target state="translated">결과 표현의 차원은 &lt;code&gt;n_out &amp;lt;= n_estimators * max_leaf_nodes&lt;/code&gt; 입니다. 경우 &lt;code&gt;max_leaf_nodes == None&lt;/code&gt; 대부분에서, 리프 노드의 수입니다 &lt;code&gt;n_estimators * 2 ** max_depth&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="068779d9c792d7cec6633fa1a442091cb7f6f6f3" translate="yes" xml:space="preserve">
          <source>The dimensions and distribution of Random Projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset.</source>
          <target state="translated">랜덤 프로젝션 행렬의 크기와 분포는 데이터 집합의 두 샘플 사이의 쌍별 거리를 유지하도록 제어됩니다.</target>
        </trans-unit>
        <trans-unit id="1fc3554ec8e7cb3510bb9bcb462301a368807c4a" translate="yes" xml:space="preserve">
          <source>The dimensions and distribution of random projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset. Thus random projection is a suitable approximation technique for distance based method.</source>
          <target state="translated">랜덤 프로젝션 행렬의 크기와 분포는 데이터 집합의 두 샘플 사이의 쌍별 거리를 유지하도록 제어됩니다. 따라서 랜덤 프로젝션은 거리 기반 방법에 적합한 근사 기법입니다.</target>
        </trans-unit>
        <trans-unit id="16b41d4b4452bd551af457fee2f8b60407215654" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet).</source>
          <target state="translated">중량 분포 (Dirichlet)에 대한 각 성분의 디리클레 농도.</target>
        </trans-unit>
        <trans-unit id="6b13240ddd6531c8fdb797758cabfbb4633f5e80" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet). The type depends on &lt;code&gt;weight_concentration_prior_type&lt;/code&gt;:</source>
          <target state="translated">중량 분포 (Dirichlet)에 대한 각 성분의 디리클레 농도. 유형은 &lt;code&gt;weight_concentration_prior_type&lt;/code&gt; 에 따라 다릅니다 .</target>
        </trans-unit>
        <trans-unit id="b470809fd29296951902d887ca553f032b8a40c4" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet). This is commonly called gamma in the literature. The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the mixture weights simplex. The value of the parameter must be greater than 0. If it is None, it&amp;rsquo;s set to &lt;code&gt;1. / n_components&lt;/code&gt;.</source>
          <target state="translated">중량 분포 (Dirichlet)에 대한 각 성분의 디리클레 농도. 이것은 일반적으로 문헌에서 감마라고합니다. 농도가 높을수록 중앙에 더 많은 질량이 들어가고 더 많은 성분이 활성화되고 농도가 낮을수록 혼합물 무게의 가장 자리에 더 많은 질량이 생성됩니다. 매개 변수의 값은 0보다 커야합니다. None이면 &lt;code&gt;1. / n_components&lt;/code&gt; 설정됩니다 . / n_components .</target>
        </trans-unit>
        <trans-unit id="cbf40d766d38685ebd59a9222ccafd32de07a34f" translate="yes" xml:space="preserve">
          <source>The disadvantages of Bayesian regression include:</source>
          <target state="translated">베이지안 회귀의 단점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="ec84046fbad625e7f2f2f6744eab5866f4d72369" translate="yes" xml:space="preserve">
          <source>The disadvantages of GBRT are:</source>
          <target state="translated">GBRT의 단점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="cfaf1a3af8c2483843ec10c36faa80968b886bf2" translate="yes" xml:space="preserve">
          <source>The disadvantages of Gaussian processes include:</source>
          <target state="translated">가우스 프로세스의 단점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c8e0c1974308fc5ae09e195effcd7396a8db8601" translate="yes" xml:space="preserve">
          <source>The disadvantages of Multi-layer Perceptron (MLP) include:</source>
          <target state="translated">MLP (Multi-layer Perceptron)의 단점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="c71179ab39a085455ac6f6888a2f6fd3afe84d6f" translate="yes" xml:space="preserve">
          <source>The disadvantages of Stochastic Gradient Descent include:</source>
          <target state="translated">확률 적 그라데이션 하강의 단점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="84c28c658fb7eb00eb9347365efe97c2f04c30f3" translate="yes" xml:space="preserve">
          <source>The disadvantages of decision trees include:</source>
          <target state="translated">의사 결정 트리의 단점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="47feb0b4f4476b90d13a7091a56a1b0f68ad3a62" translate="yes" xml:space="preserve">
          <source>The disadvantages of support vector machines include:</source>
          <target state="translated">서포트 벡터 머신의 단점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="266f0e7f0f21de8e86a9f2ce2320de66f17358df" translate="yes" xml:space="preserve">
          <source>The disadvantages of the LARS method include:</source>
          <target state="translated">LARS 방법의 단점은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="fe9e13a1ad2c431f3f290607f881625ead9408c0" translate="yes" xml:space="preserve">
          <source>The disadvantages to using t-SNE are roughly:</source>
          <target state="translated">t-SNE 사용의 단점은 대략 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="a7ea673185d2a40501e91d63f573c417ac055cd1" translate="yes" xml:space="preserve">
          <source>The distance metric to use</source>
          <target state="translated">사용할 거리 측정 항목</target>
        </trans-unit>
        <trans-unit id="0063e1a06973d2332dc280d77426f57f299b9005" translate="yes" xml:space="preserve">
          <source>The distance metric to use. Note that not all metrics are valid with all algorithms. Refer to the documentation of &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; for a description of available algorithms. Note that the normalization of the density output is correct only for the Euclidean distance metric. Default is &amp;lsquo;euclidean&amp;rsquo;.</source>
          <target state="translated">사용할 거리 측정 항목입니다. 모든 메트릭에 모든 메트릭이 유효한 것은 아닙니다. 사용 가능한 알고리즘에 대한 설명은 &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt; 의 설명서를 참조하십시오 . 밀도 출력의 정규화는 유클리드 거리 측정법에 대해서만 정확합니다. 기본값은 '유클리드'입니다.</target>
        </trans-unit>
        <trans-unit id="3c4790f8b52fd2ea34a9c9dfcea72e27e7f13da0" translate="yes" xml:space="preserve">
          <source>The distance metric used to calculate the k-Neighbors for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is &amp;lsquo;euclidean&amp;rsquo; (&amp;lsquo;minkowski&amp;rsquo; metric with the p param equal to 2.)</source>
          <target state="translated">각 샘플 포인트에 대한 k- 이웃을 계산하는 데 사용되는 거리 메트릭입니다. DistanceMetric 클래스는 사용 가능한 메트릭 목록을 제공합니다. 기본 거리는 '유클리드'(p 매개 변수가 2 인 'minkowski'측정 항목)입니다.</target>
        </trans-unit>
        <trans-unit id="f687007319ee4f163c7b6668932fdc048712a932" translate="yes" xml:space="preserve">
          <source>The distance metric used to calculate the neighbors within a given radius for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is &amp;lsquo;euclidean&amp;rsquo; (&amp;lsquo;minkowski&amp;rsquo; metric with the param equal to 2.)</source>
          <target state="translated">각 샘플 포인트에 대해 주어진 반경 내에서 이웃을 계산하는 데 사용되는 거리 메트릭입니다. DistanceMetric 클래스는 사용 가능한 메트릭 목록을 제공합니다. 기본 거리는 '유클리드'(매개 변수가 2 인 'minkowski'측정 항목)입니다.</target>
        </trans-unit>
        <trans-unit id="7e12a027c556a152948ba7c771e2cb65d4ac3e99" translate="yes" xml:space="preserve">
          <source>The distinct labels used in classifying instances.</source>
          <target state="translated">인스턴스 분류에 사용되는 고유 레이블입니다.</target>
        </trans-unit>
        <trans-unit id="2333c2cc37e50157e20c383f74b00f781cb37a1b" translate="yes" xml:space="preserve">
          <source>The distortion introduced by a random projection &lt;code&gt;p&lt;/code&gt; is asserted by the fact that &lt;code&gt;p&lt;/code&gt; is defining an eps-embedding with good probability as defined by:</source>
          <target state="translated">랜덤 프로젝션 &lt;code&gt;p&lt;/code&gt; 에 의해 야기 된 왜곡 은 &lt;code&gt;p&lt;/code&gt; 가 다음에 의해 정의 된 바와 같이 좋은 확률로 eps 임베딩을 정의 한다는 사실에 의해 주장 된다 :</target>
        </trans-unit>
        <trans-unit id="befef2d542b10663820383e2f1e59843332d14a0" translate="yes" xml:space="preserve">
          <source>The distortion introduced by a random projection &lt;code&gt;p&lt;/code&gt; only changes the distance between two points by a factor (1 +- eps) in an euclidean space with good probability. The projection &lt;code&gt;p&lt;/code&gt; is an eps-embedding as defined by:</source>
          <target state="translated">랜덤 프로젝션 &lt;code&gt;p&lt;/code&gt; 에 의해 도입 된 왜곡 은 유클리드 공간에서 두 점 사이의 거리를 유클리드 공간에서 인자 (1 + -eps)만큼만 변경합니다. 투영 &lt;code&gt;p&lt;/code&gt; 는 다음에 의해 정의 된 eps 임베딩입니다.</target>
        </trans-unit>
        <trans-unit id="b6d88e183a439e17415e4b06c82155c9e34fa344" translate="yes" xml:space="preserve">
          <source>The distributions in &lt;code&gt;scipy.stats&lt;/code&gt; prior to version scipy 0.16 do not allow specifying a random state. Instead, they use the global numpy random state, that can be seeded via &lt;code&gt;np.random.seed&lt;/code&gt; or set using &lt;code&gt;np.random.set_state&lt;/code&gt;. However, beginning scikit-learn 0.18, the &lt;a href=&quot;classes#module-sklearn.model_selection&quot;&gt;&lt;code&gt;sklearn.model_selection&lt;/code&gt;&lt;/a&gt; module sets the random state provided by the user if scipy &amp;gt;= 0.16 is also available.</source>
          <target state="translated">버전 scipy 0.16 이전의 &lt;code&gt;scipy.stats&lt;/code&gt; 에 있는 분포에서는 임의 상태를 지정할 수 없습니다. 대신, &lt;code&gt;np.random.seed&lt;/code&gt; 를 통해 시드 하거나 &lt;code&gt;np.random.set_state&lt;/code&gt; 를 사용하여 설정할 수있는 전역 numpy 임의 상태를 사용 합니다 . 그러나 scikit-learn 0.18부터는&lt;a href=&quot;classes#module-sklearn.model_selection&quot;&gt; &lt;code&gt;sklearn.model_selection&lt;/code&gt; 의&lt;/a&gt; 모듈 scipy&amp;gt; = 0.16도 가능한 경우, 사용자에 의해 제공되는 임의의 상태를 설정한다.</target>
        </trans-unit>
        <trans-unit id="e4d3a4f449c4df99f288fa9346370fef4e50edc3" translate="yes" xml:space="preserve">
          <source>The dual gap at the end of the optimization for the optimal alpha (&lt;code&gt;alpha_&lt;/code&gt;).</source>
          <target state="translated">최적 알파에 대한 최적화 종료시 이중 간격 ( &lt;code&gt;alpha_&lt;/code&gt; ) 입니다.</target>
        </trans-unit>
        <trans-unit id="1c5b7990e4f078095970dbd44954a4ff0f6bdb22" translate="yes" xml:space="preserve">
          <source>The dual gaps at the end of the optimization for each alpha.</source>
          <target state="translated">각 알파에 대한 최적화 종료시 이중 간격.</target>
        </trans-unit>
        <trans-unit id="b5eac57a33eeb0d09a6d5ebfcbe611bef2da07d8" translate="yes" xml:space="preserve">
          <source>The edges of each bin. Contain arrays of varying shapes &lt;code&gt;(n_bins_, )&lt;/code&gt; Ignored features will have empty arrays.</source>
          <target state="translated">각 용지함의 가장자리. 다양한 모양의 배열을 포함 &lt;code&gt;(n_bins_, )&lt;/code&gt; 무시 된 피처에는 빈 배열이 있습니다.</target>
        </trans-unit>
        <trans-unit id="50c07dc6eab301b9d9953e2bd5849c6aa3abe8d7" translate="yes" xml:space="preserve">
          <source>The effect of the transformer is weaker than on the synthetic data. However, the transform induces a decrease of the MAE.</source>
          <target state="translated">변압기의 효과는 합성 데이터보다 약합니다. 그러나, 변환은 MAE의 감소를 유도한다.</target>
        </trans-unit>
        <trans-unit id="8b7256d0b08ed885751aee08f85ac36bb70ae336" translate="yes" xml:space="preserve">
          <source>The effective size of the batch is computed here. If there are no more jobs to dispatch, return False, else return True.</source>
          <target state="translated">배치의 유효 크기가 여기에서 계산됩니다. 디스패치 할 작업이 더 이상 없으면 False를, 그렇지 않으면 True를 리턴하십시오.</target>
        </trans-unit>
        <trans-unit id="a044cf1265a1684d79e7b48898279d20ad6f0dac" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities</source>
          <target state="translated">사용할 고유 값 분해 전략입니다. AMG를 사용하려면 pyamg를 설치해야합니다. 매우 큰 희소 문제에서 더 빠를 수 있지만 불안정성을 초래할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="1dc685b8ff4dd3ccaa26f5f653c44ef0324bd82f" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities.</source>
          <target state="translated">사용할 고유 값 분해 전략입니다. AMG를 사용하려면 pyamg를 설치해야합니다. 매우 큰 희소 문제에서 더 빠를 수 있지만 불안정성을 초래할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="f14ed9796539634c3492800089020cfe2ebbd7af" translate="yes" xml:space="preserve">
          <source>The elastic net optimization function varies for mono and multi-outputs.</source>
          <target state="translated">탄성 순 최적화 기능은 모노 및 멀티 출력에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="e0191353bc3bca50b627784afbd555eb11c1f3dd" translate="yes" xml:space="preserve">
          <source>The empirical covariance matrix of a sample can be computed using the &lt;a href=&quot;generated/sklearn.covariance.empirical_covariance#sklearn.covariance.empirical_covariance&quot;&gt;&lt;code&gt;empirical_covariance&lt;/code&gt;&lt;/a&gt; function of the package, or by fitting an &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt;&lt;code&gt;EmpiricalCovariance&lt;/code&gt;&lt;/a&gt; object to the data sample with the &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.fit&quot;&gt;&lt;code&gt;EmpiricalCovariance.fit&lt;/code&gt;&lt;/a&gt; method. Be careful that results depend on whether the data are centered, so one may want to use the &lt;code&gt;assume_centered&lt;/code&gt; parameter accurately. More precisely, if &lt;code&gt;assume_centered=False&lt;/code&gt;, then the test set is supposed to have the same mean vector as the training set. If not, both should be centered by the user, and &lt;code&gt;assume_centered=True&lt;/code&gt; should be used.</source>
          <target state="translated">샘플의 경험적 공분산 행렬은 패키지 의 &lt;a href=&quot;generated/sklearn.covariance.empirical_covariance#sklearn.covariance.empirical_covariance&quot;&gt; &lt;code&gt;empirical_covariance&lt;/code&gt; &lt;/a&gt; 함수를 사용 하거나 &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.fit&quot;&gt; &lt;code&gt;EmpiricalCovariance.fit&lt;/code&gt; &lt;/a&gt; 방법으로 데이터 샘플에 &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt; &lt;code&gt;EmpiricalCovariance&lt;/code&gt; &lt;/a&gt; 객체를 피팅 하여 계산할 수 있습니다 . 결과는 데이터가 중심에 있는지 여부에 따라 달라 &lt;code&gt;assume_centered&lt;/code&gt; 매개 변수를 정확하게 사용할 수 있습니다 . 보다 정확하게, &lt;code&gt;assume_centered=False&lt;/code&gt; 인 경우 테스트 세트는 학습 세트와 동일한 평균 벡터를 갖습니다. 그렇지 않은 경우, 둘 다 사용자가 중심에 두어야하며 &lt;code&gt;assume_centered=True&lt;/code&gt; 중심 = 참을 사용해야합니다.</target>
        </trans-unit>
        <trans-unit id="d4b0da98d5cb047aa932ef62858edca40e188517" translate="yes" xml:space="preserve">
          <source>The encoded signal (Y).</source>
          <target state="translated">인코딩 된 신호 (Y).</target>
        </trans-unit>
        <trans-unit id="cc2fdf3023f61dc3df7bba02538cce691e7cf2cd" translate="yes" xml:space="preserve">
          <source>The energy function measures the quality of a joint assignment:</source>
          <target state="translated">에너지 함수는 공동 할당의 품질을 측정합니다.</target>
        </trans-unit>
        <trans-unit id="1bfb5cae50ef0b1032c729ea4ab68ff502e7f906" translate="yes" xml:space="preserve">
          <source>The entry &lt;code&gt;test_fold[i]&lt;/code&gt; represents the index of the test set that sample &lt;code&gt;i&lt;/code&gt; belongs to. It is possible to exclude sample &lt;code&gt;i&lt;/code&gt; from any test set (i.e. include sample &lt;code&gt;i&lt;/code&gt; in every training set) by setting &lt;code&gt;test_fold[i]&lt;/code&gt; equal to -1.</source>
          <target state="translated">&lt;code&gt;test_fold[i]&lt;/code&gt; 항목 은 샘플 &lt;code&gt;i&lt;/code&gt; 가 속하는 테스트 세트의 색인을 나타냅니다 . &lt;code&gt;test_fold[i]&lt;/code&gt; 설정 하여 모든 테스트 세트에서 샘플 &lt;code&gt;i&lt;/code&gt; 를 제외 할 수 있습니다 (예 : 모든 트레이닝 세트에 샘플 &lt;code&gt;i&lt;/code&gt; 포함 ) . 를 -1 .</target>
        </trans-unit>
        <trans-unit id="503425d1f7e07b98f32316b1d85343640a4e1294" translate="yes" xml:space="preserve">
          <source>The equivalence between &lt;code&gt;alpha&lt;/code&gt; and the regularization parameter of SVM, &lt;code&gt;C&lt;/code&gt; is given by &lt;code&gt;alpha = 1 / C&lt;/code&gt; or &lt;code&gt;alpha = 1 / (n_samples * C)&lt;/code&gt;, depending on the estimator and the exact objective function optimized by the model.</source>
          <target state="translated">간의 등가 &lt;code&gt;alpha&lt;/code&gt; 및 SVM의 정규화 파라미터는, &lt;code&gt;C&lt;/code&gt; 가 주어진다 &lt;code&gt;alpha = 1 / C&lt;/code&gt; 또는 &lt;code&gt;alpha = 1 / (n_samples * C)&lt;/code&gt; 추정기 모델과 정확한 최적화 목적 함수에 따라.</target>
        </trans-unit>
        <trans-unit id="2fec157ea85a1f513b11852f437f8dc13c9361bb" translate="yes" xml:space="preserve">
          <source>The error message or a substring of the error message.</source>
          <target state="translated">오류 메시지 또는 오류 메시지의 하위 문자열</target>
        </trans-unit>
        <trans-unit id="67a9569df158acc253e75d9fa812424d1f2b58c6" translate="yes" xml:space="preserve">
          <source>The estimated (sparse) precision matrix.</source>
          <target state="translated">추정 된 (희소 한) 정밀 행렬.</target>
        </trans-unit>
        <trans-unit id="cdf0d65945f589fd5da3781e66a0a81a30e934b1" translate="yes" xml:space="preserve">
          <source>The estimated covariance matrix.</source>
          <target state="translated">추정 된 공분산 행렬입니다.</target>
        </trans-unit>
        <trans-unit id="77a18d9bfc3d1fc8448c4fb559b59c860611bbc7" translate="yes" xml:space="preserve">
          <source>The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See &amp;ldquo;Pattern Recognition and Machine Learning&amp;rdquo; by C. Bishop, 12.2.1 p. 574 or &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;.</source>
          <target state="translated">Tipping and Bishop 1999의 확률 적 PCA 모델에 따른 잡음 공분산 추정치. C. Bishop의&amp;ldquo;패턴 인식 및 기계 학습&amp;rdquo;, 12.2.1 p. 574 또는 &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="735cdf6c1c61f38b1092aa1bd4262b34c0d5f0f3" translate="yes" xml:space="preserve">
          <source>The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See &amp;ldquo;Pattern Recognition and Machine Learning&amp;rdquo; by C. Bishop, 12.2.1 p. 574 or &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;. It is required to compute the estimated data covariance and score samples.</source>
          <target state="translated">Tipping and Bishop 1999의 확률 적 PCA 모델에 따른 잡음 공분산 추정치. C. Bishop의&amp;ldquo;패턴 인식 및 기계 학습&amp;rdquo;, 12.2.1 p. 574 또는 &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt; . 추정 된 데이터 공분산 및 스코어 샘플을 계산해야합니다.</target>
        </trans-unit>
        <trans-unit id="cc49065d7efcb582d87899f7997c53b1f8d60c78" translate="yes" xml:space="preserve">
          <source>The estimated noise variance for each feature.</source>
          <target state="translated">각 기능에 대한 추정 노이즈 분산입니다.</target>
        </trans-unit>
        <trans-unit id="960e70b3d078f74487289ecbb537290d1f7473d4" translate="yes" xml:space="preserve">
          <source>The estimated number of components. Relevant when &lt;code&gt;n_components=None&lt;/code&gt;.</source>
          <target state="translated">예상 구성 요소 수입니다. &lt;code&gt;n_components=None&lt;/code&gt; 일 때 관련 .</target>
        </trans-unit>
        <trans-unit id="df011fa4ccaad62cb4a83a0955409a4c4b3035c8" translate="yes" xml:space="preserve">
          <source>The estimated number of components. When n_components is set to &amp;lsquo;mle&amp;rsquo; or a number between 0 and 1 (with svd_solver == &amp;lsquo;full&amp;rsquo;) this number is estimated from input data. Otherwise it equals the parameter n_components, or the lesser value of n_features and n_samples if n_components is None.</source>
          <target state="translated">예상 구성 요소 수입니다. n_components가 'mle'로 설정되거나 0과 1 사이의 숫자 (svd_solver == 'full'사용) 인 경우이 숫자는 입력 데이터에서 추정됩니다. 그렇지 않으면 매개 변수 n_components 또는 n_components가 없음 인 경우 더 작은 n_features 및 n_samples 값과 같습니다.</target>
        </trans-unit>
        <trans-unit id="cd106f5afe35a52902113758dbc47d95941530e6" translate="yes" xml:space="preserve">
          <source>The estimated number of connected components in the graph.</source>
          <target state="translated">그래프에서 연결된 구성 요소의 예상 수입니다.</target>
        </trans-unit>
        <trans-unit id="a98c4d1bc7caeb1163bf14c014655348909c79f3" translate="yes" xml:space="preserve">
          <source>The estimation of the model is done by calculating the slopes and intercepts of a subpopulation of all possible combinations of p subsample points. If an intercept is fitted, p must be greater than or equal to n_features + 1. The final slope and intercept is then defined as the spatial median of these slopes and intercepts.</source>
          <target state="translated">모델의 추정은 p 개의 서브 샘플 포인트의 모든 가능한 조합의 서브 인구의 기울기와 절편을 계산하여 수행됩니다. 절편이 적합하면 p는 n_features + 1 이상이어야합니다. 그런 다음 최종 기울기와 절편은 이러한 기울기와 절편의 공간 중앙값으로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="aed02faa2e2402cc32a5968b7e86a54cff535c4e" translate="yes" xml:space="preserve">
          <source>The estimation of the model is done by iteratively maximizing the marginal log-likelihood of the observations.</source>
          <target state="translated">모형의 추정은 관측치의 한계 로그 우도를 반복적으로 최대화하여 수행됩니다.</target>
        </trans-unit>
        <trans-unit id="44fe06813d3627aaa56531c52961387365e10d66" translate="yes" xml:space="preserve">
          <source>The estimation of the number of degrees of freedom is given by:</source>
          <target state="translated">자유도의 추정은 다음과 같이 주어진다 :</target>
        </trans-unit>
        <trans-unit id="2bed56c931836016f42a33afdedfac5cb8f555f4" translate="yes" xml:space="preserve">
          <source>The estimator also implements &lt;code&gt;partial_fit&lt;/code&gt;, which updates the dictionary by iterating only once over a mini-batch. This can be used for online learning when the data is not readily available from the start, or for when the data does not fit into the memory.</source>
          <target state="translated">추정기는 또한 &lt;code&gt;partial_fit&lt;/code&gt; 을 구현합니다. 하여 미니 배치에서 한 번만 반복하여 사전을 업데이트합니다. 데이터를 처음부터 쉽게 사용할 수 없거나 데이터가 메모리에 맞지 않을 때 온라인 학습에 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="efc1e3841cf31ff119d8e77fb55d7093c1df4dd4" translate="yes" xml:space="preserve">
          <source>The estimator objects for each cv split. This is available only if &lt;code&gt;return_estimator&lt;/code&gt; parameter is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">각 cv 스플릿에 대한 추정기 객체. &lt;code&gt;return_estimator&lt;/code&gt; 매개 변수가 &lt;code&gt;True&lt;/code&gt; 로 설정된 경우에만 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="f33cc8b973e26f8a535386dbd610021f87dfdf8f" translate="yes" xml:space="preserve">
          <source>The estimator or group of estimators to be cloned</source>
          <target state="translated">복제 할 추정기 또는 추정기 그룹</target>
        </trans-unit>
        <trans-unit id="65944411996b76786bcb63db12ac31b08b55648a" translate="yes" xml:space="preserve">
          <source>The estimator that provides the initial predictions. Set via the &lt;code&gt;init&lt;/code&gt; argument or &lt;code&gt;loss.init_estimator&lt;/code&gt;.</source>
          <target state="translated">초기 예측을 제공하는 추정기입니다. &lt;code&gt;init&lt;/code&gt; 인수 또는 &lt;code&gt;loss.init_estimator&lt;/code&gt; 를 통해 설정하십시오 .</target>
        </trans-unit>
        <trans-unit id="ade400270e658832b6e128c7ef187979eae33356" translate="yes" xml:space="preserve">
          <source>The estimators of a pipeline are stored as a list in the &lt;code&gt;steps&lt;/code&gt; attribute:</source>
          <target state="translated">파이프 라인의 추정기는 &lt;code&gt;steps&lt;/code&gt; 속성에 목록으로 저장됩니다 .</target>
        </trans-unit>
        <trans-unit id="a46349c122935a13db8091d877d4179b37995289" translate="yes" xml:space="preserve">
          <source>The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. For example, it is possible to use these estimators to turn a binary classifier or a regressor into a multiclass classifier. It is also possible to use these estimators with multiclass estimators in the hope that their accuracy or runtime performance improves.</source>
          <target state="translated">이 모듈에 제공된 추정기는 메타 추정기입니다. 생성자에 기본 추정기가 제공되어야합니다. 예를 들어,이 추정기를 사용하여 이진 분류기 또는 회귀자를 멀티 클래스 분류기로 바꿀 수 있습니다. 정확성 또는 런타임 성능이 향상되기를 희망하여 이러한 추정기를 멀티 클래스 추정기와 함께 사용할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="1868049da2c813b87c2e3d48a5c71f72cc892852" translate="yes" xml:space="preserve">
          <source>The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. The meta-estimator extends single output estimators to multioutput estimators.</source>
          <target state="translated">이 모듈에 제공된 추정기는 메타 추정기입니다. 생성자에 기본 추정기가 제공되어야합니다. 메타 추정기는 단일 출력 추정기를 다중 출력 추정기로 확장합니다.</target>
        </trans-unit>
        <trans-unit id="fe77b9f934c3d4135d1310eb25c7f2c922f35971" translate="yes" xml:space="preserve">
          <source>The exact API of all functions and classes, as given by the docstrings. The API documents expected types and allowed features for all functions, and all parameters available for the algorithms.</source>
          <target state="translated">docstring이 제공하는 모든 함수 및 클래스의 정확한 API. API는 모든 기능에 대해 예상되는 유형과 허용 된 기능 및 알고리즘에 사용 가능한 모든 매개 변수를 문서화합니다.</target>
        </trans-unit>
        <trans-unit id="213a31afd656f2a0c8b37dd55b40fc5308db3ca1" translate="yes" xml:space="preserve">
          <source>The exact additive chi squared kernel.</source>
          <target state="translated">정확한 추가 카이 제곱 커널.</target>
        </trans-unit>
        <trans-unit id="14b1a49c77fdcb8a0d3244090e7c6df0b2a6cd07" translate="yes" xml:space="preserve">
          <source>The exact chi squared kernel.</source>
          <target state="translated">정확한 카이 제곱 커널.</target>
        </trans-unit>
        <trans-unit id="21c56a998f59a2c6be6550db485efcc395359e33" translate="yes" xml:space="preserve">
          <source>The example below demonstrates how the OOB error can be measured at the addition of each new tree during training. The resulting plot allows a practitioner to approximate a suitable value of &lt;code&gt;n_estimators&lt;/code&gt; at which the error stabilizes.</source>
          <target state="translated">아래 예제는 훈련 중 새 트리를 추가 할 때 OOB 오류를 측정하는 방법을 보여줍니다. 결과 플롯을 통해 실무자는 적절한 &lt;code&gt;n_estimators&lt;/code&gt; 값을 추정 할 수 있습니다. 오류가 안정화되는 .</target>
        </trans-unit>
        <trans-unit id="556b3073adbd8a84d1d4b2fe21db4807c2daf4c8" translate="yes" xml:space="preserve">
          <source>The example below uses a support vector classifier with a non-linear kernel to build a model with optimized hyperparameters by grid search. We compare the performance of non-nested and nested CV strategies by taking the difference between their scores.</source>
          <target state="translated">아래 예제는 비선형 커널과 함께 지원 벡터 분류기를 사용하여 그리드 검색에 의해 최적화 된 하이퍼 파라미터가있는 모델을 만듭니다. 중첩되지 않은 CV 전략과 중첩 된 CV 전략의 성과를 점수 간의 차이로 비교합니다.</target>
        </trans-unit>
        <trans-unit id="4cd198cf307950e0cb10d5d2e6b86c91624772aa" translate="yes" xml:space="preserve">
          <source>The example compares prediction result of linear regression (linear model) and decision tree (tree based model) with and without discretization of real-valued features.</source>
          <target state="translated">이 예에서는 실수 피처의 이산화 유무에 관계없이 선형 회귀 (선형 모델) 및 의사 결정 트리 (트리 기반 모델)의 예측 결과를 비교합니다.</target>
        </trans-unit>
        <trans-unit id="3101cd4fceb55d1be63d1bff5cb3f45996fd9aa0" translate="yes" xml:space="preserve">
          <source>The example demonstrates syntax and speed only; it doesn&amp;rsquo;t actually do anything useful with the extracted vectors. See the example scripts {document_classification_20newsgroups,clustering}.py for actual learning on text documents.</source>
          <target state="translated">이 예제는 구문과 속도 만 보여줍니다. 실제로 추출 된 벡터에는 유용한 기능이 없습니다. 텍스트 문서에 대한 실제 학습은 예제 스크립트 {document_classification_20newsgroups, clustering} .py를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="32ed3af5edbdaa27f9eba4e5a30255c6afab4414" translate="yes" xml:space="preserve">
          <source>The example is engineered to show the effect of the choice of different metrics. It is applied to waveforms, which can be seen as high-dimensional vector. Indeed, the difference between metrics is usually more pronounced in high dimension (in particular for euclidean and cityblock).</source>
          <target state="translated">이 예제는 다양한 메트릭 선택의 효과를 보여 주도록 설계되었습니다. 고차원 벡터로 볼 수있는 파형에 적용됩니다. 실제로 지표 간 차이는 일반적으로 높은 차원에서 특히 두드러집니다 (특히 유클리드 및 도시 블록).</target>
        </trans-unit>
        <trans-unit id="36ed82faabe70da57df289b8a54bb16e8830773a" translate="yes" xml:space="preserve">
          <source>The example shows that the predictions in ridge are strongly influenced by the outliers present in the dataset. The Huber regressor is less influenced by the outliers since the model uses the linear loss for these. As the parameter epsilon is increased for the Huber regressor, the decision function approaches that of the ridge.</source>
          <target state="translated">이 예는 능선의 예측이 데이터 세트에 존재하는 특이 치에 의해 크게 영향을 받는다는 것을 보여줍니다. 모델이 선형 손실을 사용하기 때문에 Huber 회귀 분석은 특이 치의 영향을 덜받습니다. Huber 회귀 변수의 매개 변수 epsilon이 증가함에 따라 의사 결정 기능은 능선의 결정 함수에 접근합니다.</target>
        </trans-unit>
        <trans-unit id="889bc091af3b0106e91a81884d921d1b2df9bdd6" translate="yes" xml:space="preserve">
          <source>The examples below compare Gaussian mixture models with a fixed number of components, to the variational Gaussian mixture models with a Dirichlet process prior. Here, a classical Gaussian mixture is fitted with 5 components on a dataset composed of 2 clusters. We can see that the variational Gaussian mixture with a Dirichlet process prior is able to limit itself to only 2 components whereas the Gaussian mixture fits the data with a fixed number of components that has to be set a priori by the user. In this case the user has selected &lt;code&gt;n_components=5&lt;/code&gt; which does not match the true generative distribution of this toy dataset. Note that with very little observations, the variational Gaussian mixture models with a Dirichlet process prior can take a conservative stand, and fit only one component.</source>
          <target state="translated">아래의 예는 고정 된 개수의 성분을 가진 가우스 혼합 모델을 이전의 디 리클 렛 공정을 사용하는 변형 가우스 혼합 모델과 비교합니다. 여기서 클래식 가우시안 혼합물에는 2 개의 군집으로 구성된 데이터 집합에 5 개의 구성 요소가 장착되어 있습니다. 우리는 이전에 Dirichlet 공정을 사용하는 변형 가우시안 혼합물이 2 개의 성분으로 만 제한 할 수있는 반면 가우시안 혼합물은 사용자가 우선적으로 설정해야하는 고정 된 수의 성분으로 데이터에 적합하다는 것을 알 수 있습니다. 이 경우 사용자는 이 완구 데이터 세트의 실제 생성 분포와 일치하지 않는 &lt;code&gt;n_components=5&lt;/code&gt; 를 선택 했습니다 . 관측이 거의없는 경우, 사전에 Dirichlet 공정을 사용하는 변형 가우스 혼합 모델은 보수적 인 입장을 취하고 하나의 구성 요소에만 적합 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="e5c4a6233958dced6338c85ecba7ef5860ffcbbc" translate="yes" xml:space="preserve">
          <source>The expected value for the mutual information can be calculated using the following equation [VEB2009]. In this equation, \(a_i = |U_i|\) (the number of elements in \(U_i\)) and \(b_j = |V_j|\) (the number of elements in \(V_j\)).</source>
          <target state="translated">상호 정보에 대한 예상 값은 다음 방정식 [VEB2009]을 사용하여 계산할 수 있습니다. 이 방정식에서 \ (a_i = | U_i | \) (\ (U_i \)의 요소 수) 및 \ (b_j = | V_j | \) (\ (V_j \)의 요소 수)</target>
        </trans-unit>
        <trans-unit id="5075a63ec7be1cbe42a641e5ae277f9e4b63f160" translate="yes" xml:space="preserve">
          <source>The experiment is performed on an artificial dataset for binary classification with 100,000 samples (1,000 of them are used for model fitting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The first figure shows the estimated probabilities obtained with logistic regression, Gaussian naive Bayes, and Gaussian naive Bayes with both isotonic calibration and sigmoid calibration. The calibration performance is evaluated with Brier score, reported in the legend (the smaller the better). One can observe here that logistic regression is well calibrated while raw Gaussian naive Bayes performs very badly. This is because of the redundant features which violate the assumption of feature-independence and result in an overly confident classifier, which is indicated by the typical transposed-sigmoid curve.</source>
          <target state="translated">이 실험은 20 개의 특징을 가진 100,000 개의 샘플 (1,000 개가 모델 피팅에 사용됨)로 이진 분류를위한 인공 데이터 세트에서 수행됩니다. 20 개의 기능 중 2 개만 유익하고 10 개는 중복입니다. 첫 번째 그림은 등장 성 교정과 시그 모이 드 교정이 모두 포함 된 로지스틱 회귀 분석, 가우시안 순진 베이 및 가우시안 순 베이로 얻은 추정 확률을 보여줍니다. 범례에서보고 된 Brier 점수로 교정 성능을 평가합니다 (더 작을수록 좋습니다). 여기서 원시 가우시안 순진 베이 즈가 매우 나쁘게 수행하는 동안 로지스틱 회귀 분석이 잘 조정되어 있음을 알 수 있습니다. 이는 기능 독립성의 가정을 위반하는 중복 기능으로 인해 과도하게 신뢰할 수있는 분류기를 생성하며, 이는 전형적인 조옮김 시그 모이 드 곡선으로 표시됩니다.</target>
        </trans-unit>
        <trans-unit id="e1db213d528f30cf745c1554ad2a43991932646f" translate="yes" xml:space="preserve">
          <source>The explained variance or ndarray if &amp;lsquo;multioutput&amp;rsquo; is &amp;lsquo;raw_values&amp;rsquo;.</source>
          <target state="translated">'multioutput'이 'raw_values'인 경우 설명 된 분산 또는 ndarray입니다.</target>
        </trans-unit>
        <trans-unit id="dff7adf6114ae66681ccc96cdb5c8abce36c0555" translate="yes" xml:space="preserve">
          <source>The explicit constant as predicted by the &amp;ldquo;constant&amp;rdquo; strategy. This parameter is useful only for the &amp;ldquo;constant&amp;rdquo; strategy.</source>
          <target state="translated">&quot;일정한&quot;전략에 의해 예측 된 명시 적 상수. 이 매개 변수는 &quot;일정한&quot;전략에만 유용합니다.</target>
        </trans-unit>
        <trans-unit id="96483cbe0c7b434b8387960f89d9c2700f0af59b" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate [default 0.5].</source>
          <target state="translated">역 스케일링 학습률의 지수 [기본값 0.5].</target>
        </trans-unit>
        <trans-unit id="1fa85799d065de026e0253585d451384c9ee6013" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to &amp;lsquo;invscaling&amp;rsquo;. Only used when solver=&amp;rsquo;sgd&amp;rsquo;.</source>
          <target state="translated">역 스케일링 학습률의 지수입니다. learning_rate가 'invscaling'으로 설정된 경우 효과적인 학습 속도를 업데이트하는 데 사용됩니다. solver = 'sgd'인 경우에만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="c7acf68e47ad4fe8cc8cf54d82e56ffc97c60a11" translate="yes" xml:space="preserve">
          <source>The exponent for the base kernel</source>
          <target state="translated">기본 커널의 지수</target>
        </trans-unit>
        <trans-unit id="fe68de8b995171a61c550ec2f7815b01a2d28ec7" translate="yes" xml:space="preserve">
          <source>The exponentiated version of the kernel, which is usually preferable.</source>
          <target state="translated">커널의 지수 버전으로 일반적으로 바람직합니다.</target>
        </trans-unit>
        <trans-unit id="d1490a4bcb15c22959ee30800b231e4413480004" translate="yes" xml:space="preserve">
          <source>The external estimator fit on the reduced dataset.</source>
          <target state="translated">외부 추정기는 축소 된 데이터 세트에 적합합니다.</target>
        </trans-unit>
        <trans-unit id="18d0cd5609d8f78695dc43242ffb9cf995a65ee9" translate="yes" xml:space="preserve">
          <source>The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features):</source>
          <target state="translated">추출 된 TF-IDF 벡터는 30000 차원 이상의 공간에서 샘플에 의해 평균 159 개의 0이 아닌 구성 요소 (0이 아닌 0이 아닌 특징)로 매우 희박합니다.</target>
        </trans-unit>
        <trans-unit id="e5bc1ae1e0e90623cdbf461e900aded771c1b79d" translate="yes" xml:space="preserve">
          <source>The extracted dataset will only retain pictures of people that have at least &lt;code&gt;min_faces_per_person&lt;/code&gt; different pictures.</source>
          <target state="translated">추출 된 데이터 세트는 최소한 &lt;code&gt;min_faces_per_person&lt;/code&gt; 다른 사진 을 가진 사람의 사진 만 유지 합니다.</target>
        </trans-unit>
        <trans-unit id="7cb7e9ef70987101280d538c40ac3f0463557635" translate="yes" xml:space="preserve">
          <source>The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task easier.</source>
          <target state="translated">하이퍼 큐브 크기를 곱하는 요소입니다. 값이 클수록 클러스터 / 클래스가 분산되어 분류 작업이 쉬워집니다.</target>
        </trans-unit>
        <trans-unit id="0cd5de0c793252ae54dc423f82b86e165f05af5f" translate="yes" xml:space="preserve">
          <source>The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher&amp;rsquo;s paper. Note that it&amp;rsquo;s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points.</source>
          <target state="translated">RA Fisher 선생님이 처음 사용한 유명한 Iris 데이터베이스. 데이터 세트는 Fisher의 논문에서 가져 왔습니다. R과 동일하지만 UCI Machine Learning Repository와는 다르며 두 개의 잘못된 데이터 포인트가 있습니다.</target>
        </trans-unit>
        <trans-unit id="e9c1f9439f6a49e182ea739a4c5151e4783dfb88" translate="yes" xml:space="preserve">
          <source>The feature importance scores of a fit gradient boosting model can be accessed via the &lt;code&gt;feature_importances_&lt;/code&gt; property:</source>
          <target state="translated">fit gradient boosting model의 기능 중요도 점수는 &lt;code&gt;feature_importances_&lt;/code&gt; 속성을 통해 액세스 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="40d3ffb0824ad292679a9b4129b9e33a1cef7858" translate="yes" xml:space="preserve">
          <source>The feature matrix. Categorical features are encoded as ordinals.</source>
          <target state="translated">기능 매트릭스. 범주 기능은 서수로 인코딩됩니다.</target>
        </trans-unit>
        <trans-unit id="0a487efba080872bb151e33f60795da2b55ed658" translate="yes" xml:space="preserve">
          <source>The feature ranking, such that &lt;code&gt;ranking_[i]&lt;/code&gt; corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1.</source>
          <target state="translated">&lt;code&gt;ranking_[i]&lt;/code&gt; 와 같은 특징 순위 는 i 번째 특징의 순위 위치에 대응한다. 선택된 (즉, 최상의 추정) 기능에는 순위 1이 할당됩니다.</target>
        </trans-unit>
        <trans-unit id="acbf27a6428b599cd8900c8ca4f4ecae3f0cc8b0" translate="yes" xml:space="preserve">
          <source>The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and &lt;code&gt;max_features=n_features&lt;/code&gt;, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, &lt;code&gt;random_state&lt;/code&gt; has to be fixed.</source>
          <target state="translated">각 분할에서 기능이 항상 임의로 변경됩니다. 따라서 최상의 스플릿을 검색하는 동안 열거 된 여러 스플릿에 대해 기준의 개선이 동일한 경우, 동일한 트레이닝 데이터 및 &lt;code&gt;max_features=n_features&lt;/code&gt; 로도 최적의 스플릿이 달라질 수 있습니다 . 피팅하는 동안 결정적인 동작을 얻으려면 &lt;code&gt;random_state&lt;/code&gt; 를 수정해야합니다.</target>
        </trans-unit>
        <trans-unit id="c25846da01d7420f77d01daefe6ea3229aead8d7" translate="yes" xml:space="preserve">
          <source>The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, &lt;code&gt;max_features=n_features&lt;/code&gt; and &lt;code&gt;bootstrap=False&lt;/code&gt;, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, &lt;code&gt;random_state&lt;/code&gt; has to be fixed.</source>
          <target state="translated">각 분할에서 기능이 항상 임의로 변경됩니다. 따라서 최상의 스플릿을 검색하는 동안 열거 된 여러 스플릿에 대해 기준의 개선이 동일 하면 동일한 학습 데이터 인 &lt;code&gt;max_features=n_features&lt;/code&gt; 및 &lt;code&gt;bootstrap=False&lt;/code&gt; 로도 가장 잘 찾은 스플릿이 달라질 수 있습니다 . 피팅하는 동안 결정적인 동작을 얻으려면 &lt;code&gt;random_state&lt;/code&gt; 를 수정해야합니다.</target>
        </trans-unit>
        <trans-unit id="79b5615baa935539329fdffd05466c4966130183" translate="yes" xml:space="preserve">
          <source>The features indices which will be returned when calling &lt;code&gt;transform&lt;/code&gt;. They are computed during &lt;code&gt;fit&lt;/code&gt;. For &lt;code&gt;features='all'&lt;/code&gt;, it is to &lt;code&gt;range(n_features)&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;transform&lt;/code&gt; 을 호출 할 때 반환되는 기능 인덱스입니다 . 그들은 &lt;code&gt;fit&lt;/code&gt; 하는 동안 계산됩니다 . 들어 &lt;code&gt;features='all'&lt;/code&gt; , 그것은이다 &lt;code&gt;range(n_features)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7bb18d685bec7903d9b61cd6026a227300d22db3" translate="yes" xml:space="preserve">
          <source>The features of &lt;code&gt;X&lt;/code&gt; have been transformed from \([x_1, x_2]\) to \([1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\), and can now be used within any linear model.</source>
          <target state="translated">&lt;code&gt;X&lt;/code&gt; 의 기능이 \ ([x_1, x_2] \)에서 \ ([1, x_1, x_2, x_1 ^ 2, x_1 x_2, x_2 ^ 2] \)로 변환되었으며 이제 모든 선형 모델 내에서 사용할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="b791adba0e25e8ba284f5da492871659f500ccc6" translate="yes" xml:space="preserve">
          <source>The features of X have been transformed from \((X_1, X_2)\) to \((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\).</source>
          <target state="translated">X의 기능이 \ ((X_1, X_2) \)에서 \ ((1, X_1, X_2, X_1 ^ 2, X_1X_2, X_2 ^ 2) \)로 변환되었습니다.</target>
        </trans-unit>
        <trans-unit id="0ec9e3dee3b599b742aa9c67dee3f4cab82e4d00" translate="yes" xml:space="preserve">
          <source>The features of X have been transformed from \((X_1, X_2, X_3)\) to \((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\).</source>
          <target state="translated">X의 기능이 \ ((X_1, X_2, X_3) \)에서 \ ((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3) \)로 변환되었습니다.</target>
        </trans-unit>
        <trans-unit id="eaccd9379a9dbc6cce41fc318001c8bf42339abe" translate="yes" xml:space="preserve">
          <source>The figure below illustrates the effect of shrinkage and subsampling on the goodness-of-fit of the model. We can clearly see that shrinkage outperforms no-shrinkage. Subsampling with shrinkage can further increase the accuracy of the model. Subsampling without shrinkage, on the other hand, does poorly.</source>
          <target state="translated">아래 그림은 수축 및 서브 샘플링이 모델의 적합도에 미치는 영향을 보여줍니다. 수축이 비 수축보다 우수한 것을 알 수 있습니다. 수축이있는 서브 샘플링은 모델의 정확도를 더욱 높일 수 있습니다. 반면에 수축이없는 서브 샘플링은 제대로 수행되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="50a711fcdd2b061e3d348535a1d7a4d9bddb7ed5" translate="yes" xml:space="preserve">
          <source>The figure below shows the results of applying &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; with least squares loss and 500 base learners to the Boston house price dataset (&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt;&lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt;&lt;/a&gt;). The plot on the left shows the train and test error at each iteration. The train error at each iteration is stored in the &lt;code&gt;train_score_&lt;/code&gt; attribute of the gradient boosting model. The test error at each iterations can be obtained via the &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt;&lt;code&gt;staged_predict&lt;/code&gt;&lt;/a&gt; method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. &lt;code&gt;n_estimators&lt;/code&gt;) by early stopping. The plot on the right shows the feature importances which can be obtained via the &lt;code&gt;feature_importances_&lt;/code&gt; property.</source>
          <target state="translated">아래 그림은 최소 제곱 손실과 500 명의 기본 학습자가있는 &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; 를 보스턴 주택 가격 데이터 세트 ( &lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt; &lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt; &lt;/a&gt; ) 에 적용한 결과를 보여줍니다 . 왼쪽의 그림은 각 반복에서 열차 및 테스트 오류를 ​​보여줍니다. 각 반복에서 열차 오차 는 그라디언트 부스팅 모델 의 &lt;code&gt;train_score_&lt;/code&gt; 속성에 저장됩니다 . 각 반복에서 테스트 오류는 &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt; &lt;code&gt;staged_predict&lt;/code&gt; &lt;/a&gt; 메소드를 통해 얻을 수 있으며 각 단계에서 예측을 생성하는 생성기를 반환합니다. 이와 같은 플롯을 사용하여 최적의 트리 수 (예 : &lt;code&gt;n_estimators&lt;/code&gt; 를 결정할 수 있습니다 조기 중지로 ) . 오른쪽 그림은 기능의 중요성을 보여줍니다. &lt;code&gt;feature_importances_&lt;/code&gt; 속성</target>
        </trans-unit>
        <trans-unit id="cc8de6d2f1f6034186ab0af10d91754eadd34954" translate="yes" xml:space="preserve">
          <source>The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly 2*pi (6.28), while KRR chooses the doubled periodicity 4*pi. Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (&amp;ldquo;curse of dimensionality&amp;rdquo;). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.</source>
          <target state="translated">그림은 두 가지 방법이 목표 함수의 합리적인 모델을 배우는 것을 보여줍니다. GPR은 함수의 주기성을 대략 2 * pi (6.28)로 정확하게 식별하는 반면 KRR은 두 배 주기성 4 * pi를 선택합니다. 또한, GPR은 KRR에 사용할 수없는 예측에 대한 합리적인 신뢰 한계를 제공합니다. 두 방법의 주요 차이점은 피팅과 ​​예측에 필요한 시간입니다. KRR을 피팅하는 것은 원칙적으로 빠르지 만 하이퍼 파라미터 최적화에 대한 그리드 검색은 하이퍼 파라미터의 수에 따라 지수 적으로 확장됩니다 ( &quot;차원의 저주&quot;). GPR에서 파라미터의 기울기 기반 최적화는 이러한 지수 스케일링을 겪지 않으므로 3 차원 하이퍼 파라미터 공간이있는이 예에서 훨씬 빠릅니다. 예측 시간은 비슷합니다. 하나,GPR의 예측 분포의 분산을 생성하는 것은 단지 평균을 예측하는 것보다 훨씬 오래 걸립니다.</target>
        </trans-unit>
        <trans-unit id="7a70cd6dc569a4ad4c8d12310e45e3bf56f13e7d" translate="yes" xml:space="preserve">
          <source>The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly \(2*\pi\) (6.28), while KRR chooses the doubled periodicity \(4*\pi\) . Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (&amp;ldquo;curse of dimensionality&amp;rdquo;). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.</source>
          <target state="translated">그림은 두 가지 방법 모두 목표 함수의 합리적인 모델을 학습 함을 보여줍니다. GPR은 함수의 주기성을 대략 \ (2 * \ pi \) (6.28)로 올바르게 식별하는 반면 KRR은 두 배 주기성 \ (4 * \ pi \)를 선택합니다. 또한, GPR은 KRR에 사용할 수없는 예측에 대한 합리적인 신뢰 한계를 제공합니다. 두 방법의 주요 차이점은 피팅과 ​​예측에 필요한 시간입니다. KRR을 피팅하는 것은 원칙적으로 빠르지 만 하이퍼 파라미터 최적화에 대한 그리드 검색은 하이퍼 파라미터의 수에 따라 지수 적으로 확장됩니다 ( &quot;차원의 저주&quot;). GPR에서 파라미터의 기울기 기반 최적화는 이러한 지수 스케일링을 겪지 않으므로 3 차원 하이퍼 파라미터 공간이있는이 예에서 훨씬 빠릅니다. 예측 시간은 비슷합니다. 하나,GPR의 예측 분포의 분산을 생성하는 것은 단지 평균을 예측하는 것보다 훨씬 오래 걸립니다.</target>
        </trans-unit>
        <trans-unit id="4c819c571e9cf27ccc7a880b7ae493959bac8667" translate="yes" xml:space="preserve">
          <source>The figure shows the cumulative sum of the negative OOB improvements as a function of the boosting iteration. As you can see, it tracks the test loss for the first hundred iterations but then diverges in a pessimistic way. The figure also shows the performance of 3-fold cross validation which usually gives a better estimate of the test loss but is computationally more demanding.</source>
          <target state="translated">그림은 부스팅 반복의 함수로서 음의 OOB 개선의 누적 합계를 보여줍니다. 보시다시피, 처음 100 회 반복에 대한 테스트 손실을 추적 한 다음 비관적 인 방식으로 분기합니다. 이 그림은 또한 일반적으로 테스트 손실을 더 잘 추정하지만 계산 상 더 까다로운 3 배 교차 검증의 성능을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="fc5bec6cdf25a030ca7f5736bc605af5657e6e59" translate="yes" xml:space="preserve">
          <source>The figures below are used to illustrate the effect of scaling our &lt;code&gt;C&lt;/code&gt; to compensate for the change in the number of samples, in the case of using an &lt;code&gt;l1&lt;/code&gt; penalty, as well as the &lt;code&gt;l2&lt;/code&gt; penalty.</source>
          <target state="translated">아래 그림은 우리의 스케일링 효과를 설명하기 위해 사용되는 &lt;code&gt;C&lt;/code&gt; 를 사용하는 경우, 샘플의 수의 변화를 보상하기 위해 &lt;code&gt;l1&lt;/code&gt; 패널티뿐만 아니라 &lt;code&gt;l2&lt;/code&gt; 페널티.</target>
        </trans-unit>
        <trans-unit id="0b0d592cc5daeb578d9b82714e3800c3092f375f" translate="yes" xml:space="preserve">
          <source>The figures illustrate the interpolating property of the Gaussian Process model as well as its probabilistic nature in the form of a pointwise 95% confidence interval.</source>
          <target state="translated">그림은 가우시안 프로세스 모델의 보간 속성과 점별 95 % 신뢰 구간 형식의 확률 적 속성을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="e96dfdb18e509ef4ea2f5731757d5c9fff16f17b" translate="yes" xml:space="preserve">
          <source>The figures show the confusion matrix with and without normalization by class support size (number of elements in each class). This kind of normalization can be interesting in case of class imbalance to have a more visual interpretation of which class is being misclassified.</source>
          <target state="translated">그림은 클래스 지원 크기 (각 클래스의 요소 수)에 따른 정규화 유무에 관계없이 혼동 행렬을 보여줍니다. 이러한 종류의 정규화는 클래스 불균형의 경우 어떤 클래스가 잘못 분류되고 있는지에 대한 시각적으로 해석하기에 흥미로울 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="83d2d2e5c2f316a3531e949e8bac6210f7e21821" translate="yes" xml:space="preserve">
          <source>The files themselves are loaded in memory in the &lt;code&gt;data&lt;/code&gt; attribute. For reference the filenames are also available:</source>
          <target state="translated">파일 자체는 &lt;code&gt;data&lt;/code&gt; 속성의 메모리에로드됩니다 . 참고로 파일 이름도 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4834eeb362cb8f80edd0fa52b738fe7db255a20e" translate="yes" xml:space="preserve">
          <source>The filesystem path to the root folder where MLComp datasets are stored, if mlcomp_root is None, the MLCOMP_DATASETS_HOME environment variable is looked up instead.</source>
          <target state="translated">MLComp 데이터 세트가 저장된 루트 폴더의 파일 시스템 경로입니다. mlcomp_root가 None 인 경우 MLCOMP_DATASETS_HOME 환경 변수가 대신 검색됩니다.</target>
        </trans-unit>
        <trans-unit id="5e83705f6f5cf8453734b26cfa75b02cefbf9a06" translate="yes" xml:space="preserve">
          <source>The final sum of similarities is divided by the size of the larger set.</source>
          <target state="translated">유사성의 최종 합은 더 큰 세트의 크기로 나뉩니다.</target>
        </trans-unit>
        <trans-unit id="6023cd4bbdadd0261fe4fb93bbac4f79dd623983" translate="yes" xml:space="preserve">
          <source>The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set).</source>
          <target state="translated">관성 기준의 최종 값 (훈련 세트의 모든 관측치에 대해 가장 가까운 중심까지의 제곱 거리의 합).</target>
        </trans-unit>
        <trans-unit id="65db46a7a2384cdd4de97d3c70536cd9da4c8616" translate="yes" xml:space="preserve">
          <source>The final value of the stress (sum of squared distance of the disparities and the distances for all constrained points).</source>
          <target state="translated">응력의 최종 값 (변위의 제곱 거리와 모든 구속 점의 거리의 합).</target>
        </trans-unit>
        <trans-unit id="7e7a148ceab046969e5524f650bd9948ad6ae0ac" translate="yes" xml:space="preserve">
          <source>The first &lt;code&gt;[.9, .1]&lt;/code&gt; in &lt;code&gt;y_pred&lt;/code&gt; denotes 90% probability that the first sample has label 0. The log loss is non-negative.</source>
          <target state="translated">제 &lt;code&gt;[.9, .1]&lt;/code&gt; 에 &lt;code&gt;y_pred&lt;/code&gt; 첫번째 샘플 라벨 0 로그 손실을 갖는 것을 나타내고, 90 %의 확률은 음수이다.</target>
        </trans-unit>
        <trans-unit id="c41f4a219241dbe01041bd4816c28f057f1f8fc9" translate="yes" xml:space="preserve">
          <source>The first &lt;code&gt;n_samples % n_splits&lt;/code&gt; folds have size &lt;code&gt;n_samples // n_splits + 1&lt;/code&gt;, other folds have size &lt;code&gt;n_samples // n_splits&lt;/code&gt;, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples.</source>
          <target state="translated">첫 번째 &lt;code&gt;n_samples % n_splits&lt;/code&gt; 폴드의 크기는 &lt;code&gt;n_samples // n_splits + 1&lt;/code&gt; 이고 다른 폴드의 크기는 &lt;code&gt;n_samples // n_splits&lt;/code&gt; . 여기서 &lt;code&gt;n_samples&lt;/code&gt; 는 샘플 수입니다.</target>
        </trans-unit>
        <trans-unit id="318062ceb48883ba19b024b3ac85479b5d766c8b" translate="yes" xml:space="preserve">
          <source>The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices. In general, multiple points can be queried at the same time.</source>
          <target state="translated">반환 된 첫 번째 배열에는 1.6보다 가까운 모든 점까지의 거리가 포함되고 반환 된 두 번째 배열에는 인덱스가 포함됩니다. 일반적으로 여러 지점을 동시에 쿼리 할 수 ​​있습니다.</target>
        </trans-unit>
        <trans-unit id="b072f11f22e8fd40f8f011ced740f859610c5839" translate="yes" xml:space="preserve">
          <source>The first base-kernel of the product-kernel</source>
          <target state="translated">제품 커널의 첫 번째 기본 커널</target>
        </trans-unit>
        <trans-unit id="2843deb348cef2ebb64b5cb6c478da3efa931340" translate="yes" xml:space="preserve">
          <source>The first base-kernel of the sum-kernel</source>
          <target state="translated">합 커널의 첫 번째 기본 커널</target>
        </trans-unit>
        <trans-unit id="23baf5dbd5fca89380aba285e5a6dca77d9ff4b8" translate="yes" xml:space="preserve">
          <source>The first column of images shows true faces. The next columns illustrate how extremely randomized trees, k nearest neighbors, linear regression and ridge regression complete the lower half of those faces.</source>
          <target state="translated">이미지의 첫 번째 열은 실제 얼굴을 보여줍니다. 다음 열은 극도로 무작위 화 된 나무, k 개의 가장 가까운 이웃, 선형 회귀 및 능선 회귀가 해당면의 아래쪽 절반을 어떻게 완성하는지 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="758127cc4d0b762bc4a77876dd4551f141ccd7cd" translate="yes" xml:space="preserve">
          <source>The first corresponds to a model with a high noise level and a large length scale, which explains all variations in the data by noise.</source>
          <target state="translated">첫 번째는 노이즈 수준이 높고 길이가 큰 모델에 해당하며 노이즈별로 데이터의 모든 변형을 설명합니다.</target>
        </trans-unit>
        <trans-unit id="2d71e99749591e16661c08d2c19738355ed2fad8" translate="yes" xml:space="preserve">
          <source>The first element of each line can be used to store a target variable to predict.</source>
          <target state="translated">각 라인의 첫 번째 요소는 예측할 대상 변수를 저장하는 데 사용될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d151d2a5867c5ba125b375d964535691cae0b0d6" translate="yes" xml:space="preserve">
          <source>The first example illustrates how robust covariance estimation can help concentrating on a relevant cluster when another one exists. Here, many observations are confounded into one and break down the empirical covariance estimation. Of course, some screening tools would have pointed out the presence of two clusters (Support Vector Machines, Gaussian Mixture Models, univariate outlier detection, &amp;hellip;). But had it been a high-dimensional example, none of these could be applied that easily.</source>
          <target state="translated">첫 번째 예는 강력한 공분산 추정이 다른 클러스터가 존재할 때 관련 클러스터에 집중하는 데 어떻게 도움이 될 수 있는지 보여줍니다. 여기에서 많은 관측치가 하나에 혼동되어 경험적 공분산 추정치를 세분화합니다. 물론 일부 선별 도구는 두 개의 군집 (지원 벡터 머신, 가우스 혼합 모델, 일 변량 이상치 탐지 등)의 존재를 지적했을 것입니다. 그러나 그것이 고차원적인 예라면 이것들 중 어느 것도 쉽게 적용 할 수 없었습니다.</target>
        </trans-unit>
        <trans-unit id="ecfbbb8300d863a918a602e1c4f90841d4b71a3f" translate="yes" xml:space="preserve">
          <source>The first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning):</source>
          <target state="translated">첫 번째 로더는 얼굴 식별 작업에 사용됩니다 : 멀티 클래스 분류 작업 (따라서지도 학습) :</target>
        </trans-unit>
        <trans-unit id="1272790baab931c65e23f13d4b17128820578899" translate="yes" xml:space="preserve">
          <source>The first model is a classical Gaussian Mixture Model with 10 components fit with the Expectation-Maximization algorithm.</source>
          <target state="translated">첫 번째 모델은 Expectation-Maximization 알고리즘에 맞는 10 개의 구성 요소가 포함 된 클래식 가우스 혼합 모델입니다.</target>
        </trans-unit>
        <trans-unit id="a28da4c4f339f2d2823befd3907c74a3bd7b1459" translate="yes" xml:space="preserve">
          <source>The first plot is a visualization of the decision function for a variety of parameter values on a simplified classification problem involving only 2 input features and 2 possible target classes (binary classification). Note that this kind of plot is not possible to do for problems with more features or target classes.</source>
          <target state="translated">첫 번째 플롯은 2 개의 입력 기능과 2 개의 가능한 대상 클래스 (2 진 분류) 만 포함 된 단순화 된 분류 문제에 대한 다양한 매개 변수 값에 대한 결정 함수의 시각화입니다. 이러한 종류의 플롯은 더 많은 기능이나 대상 클래스의 문제에 대해서는 수행 할 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="fdb1a20b80ba5f009466ef7acd3785afda979734" translate="yes" xml:space="preserve">
          <source>The first plot shows one of the problems with using histograms to visualize the density of points in 1D. Intuitively, a histogram can be thought of as a scheme in which a unit &amp;ldquo;block&amp;rdquo; is stacked above each point on a regular grid. As the top two panels show, however, the choice of gridding for these blocks can lead to wildly divergent ideas about the underlying shape of the density distribution. If we instead center each block on the point it represents, we get the estimate shown in the bottom left panel. This is a kernel density estimation with a &amp;ldquo;top hat&amp;rdquo; kernel. This idea can be generalized to other kernel shapes: the bottom-right panel of the first figure shows a Gaussian kernel density estimate over the same distribution.</source>
          <target state="translated">첫 번째 플롯은 히스토그램을 사용하여 1D의 포인트 밀도를 시각화하는 데 따른 문제 중 하나를 보여줍니다. 직관적으로, 히스토그램은 단위 &quot;블록&quot;이 규칙적인 그리드에서 각 점 위에 쌓이는 방식으로 생각할 수 있습니다. 그러나 상위 2 개의 패널에서 볼 수 있듯이 이러한 블록에 대해 그리드를 선택하면 밀도 분포의 기본 모양에 대한 격렬한 아이디어가 생길 수 있습니다. 대신 각 블록을 나타내는 점의 중심에 놓으면 왼쪽 하단 패널에 추정치가 표시됩니다. 이것은 &quot;top hat&quot;커널을 사용한 커널 밀도 추정입니다. 이 아이디어는 다른 커널 형태로 일반화 될 수 있습니다. 첫 번째 그림의 오른쪽 아래 패널은 동일한 분포에 대한 가우스 커널 밀도 추정치를 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="80a6f35d6de94c1655a0ab570a2d81b8e9f0c281" translate="yes" xml:space="preserve">
          <source>The first plot shows that with an increasing number of samples &lt;code&gt;n_samples&lt;/code&gt;, the minimal number of dimensions &lt;code&gt;n_components&lt;/code&gt; increased logarithmically in order to guarantee an &lt;code&gt;eps&lt;/code&gt;-embedding.</source>
          <target state="translated">샘플의 증가와 함께 제 플롯 도시 &lt;code&gt;n_samples&lt;/code&gt; , 치수의 최소 수 &lt;code&gt;n_components&lt;/code&gt; 을 위해 대수적으로 증가는 보장 &lt;code&gt;eps&lt;/code&gt; -embedding한다.</target>
        </trans-unit>
        <trans-unit id="0fa5069414b33f645c4a3a3261f97c50a380bf97" translate="yes" xml:space="preserve">
          <source>The first plot shows the best inertia reached for each combination of the model (&lt;code&gt;KMeans&lt;/code&gt; or &lt;code&gt;MiniBatchKMeans&lt;/code&gt;) and the init method (&lt;code&gt;init=&quot;random&quot;&lt;/code&gt; or &lt;code&gt;init=&quot;kmeans++&quot;&lt;/code&gt;) for increasing values of the &lt;code&gt;n_init&lt;/code&gt; parameter that controls the number of initializations.</source>
          <target state="translated">첫 번째 플롯은 초기화 횟수를 제어하는 &lt;code&gt;n_init&lt;/code&gt; 매개 변수의 값을 늘리기 위해 모델 ( &lt;code&gt;KMeans&lt;/code&gt; 또는 &lt;code&gt;MiniBatchKMeans&lt;/code&gt; )과 init 메소드 ( &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; 또는 &lt;code&gt;init=&quot;kmeans++&quot;&lt;/code&gt; ) 의 각 조합에 대해 도달 한 최상의 관성을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="35a5988878bfd98d1cc6f738d4af80878102b501" translate="yes" xml:space="preserve">
          <source>The first row of output array indicates that there are three samples whose true cluster is &amp;ldquo;a&amp;rdquo;. Of them, two are in predicted cluster 0, one is in 1, and none is in 2. And the second row indicates that there are three samples whose true cluster is &amp;ldquo;b&amp;rdquo;. Of them, none is in predicted cluster 0, one is in 1 and two are in 2.</source>
          <target state="translated">출력 배열의 첫 번째 행은 실제 클러스터가 &quot;a&quot;인 3 개의 샘플이 있음을 나타냅니다. 그중 두 개는 예측 된 클러스터 0에 있고, 하나는 1에 있고, 아무도는 2에 없습니다. 그리고 두 번째 행은 실제 클러스터가 &quot;b&quot;인 세 개의 샘플이 있음을 나타냅니다. 이 중 예측 된 클러스터 0에는 아무것도없고 1은 1에, 2는 2에 있습니다.</target>
        </trans-unit>
        <trans-unit id="94d22c244cece5d36684b54eb3a3c36ef460564d" translate="yes" xml:space="preserve">
          <source>The first two loss functions are lazy, they only update the model parameters if an example violates the margin constraint, which makes training very efficient and may result in sparser models, even when L2 penalty is used.</source>
          <target state="translated">처음 두 손실 함수는 게으 르며, 예제가 마진 제약 조건을 위반하는 경우에만 모델 매개 변수를 업데이트하므로 L2 페널티가 사용되는 경우에도 훈련이 매우 효율적으로 이루어지고 스파 스 모델이 생성 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="865ac3c349894331005e5c9a9918fbe1c4576705" translate="yes" xml:space="preserve">
          <source>The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.</source>
          <target state="translated">또한 장착 된 모델을 사용하여 입력을 가장 차별적 인 방향으로 투사하여 입력의 치수를 줄일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="47ade7eb6fd49b2a025d9a87896bfcf3ba24402d" translate="yes" xml:space="preserve">
          <source>The fitted model.</source>
          <target state="translated">적합 모델.</target>
        </trans-unit>
        <trans-unit id="14120dbe50cfe7a93d61eb3782205eb1e9322e9d" translate="yes" xml:space="preserve">
          <source>The flexibility of controlling the smoothness of the learned function via \(\nu\) allows adapting to the properties of the true underlying functional relation. The prior and posterior of a GP resulting from a Mat&amp;eacute;rn kernel are shown in the following figure:</source>
          <target state="translated">\ (\ nu \)를 통해 학습 된 기능의 부드러움을 제어 할 수있는 유연성 덕분에 진정한 기본 기능 관계의 속성에 적응할 수 있습니다. Mat&amp;eacute;rn 커널로 인한 GP의 앞뒤는 다음 그림과 같습니다.</target>
        </trans-unit>
        <trans-unit id="130cb7c407aba5d31bddc566759f6c2692fa934a" translate="yes" xml:space="preserve">
          <source>The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data.</source>
          <target state="translated">아래의 순서도는 사용자가 데이터를 평가할 추정 방법과 관련하여 문제에 접근하는 방법에 대한 대략적인 가이드를 제공하기 위해 마련된 것입니다.</target>
        </trans-unit>
        <trans-unit id="f162534c96a36fe6b7c86b6775d49d882723bf35" translate="yes" xml:space="preserve">
          <source>The folder names are used as supervised signal label names. The individual file names are not important.</source>
          <target state="translated">폴더 이름은 감독 된 신호 레이블 이름으로 사용됩니다. 개별 파일 이름은 중요하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="f2db8b6a5929e1458b6943c5d307e1ce8a02db89" translate="yes" xml:space="preserve">
          <source>The folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold.</source>
          <target state="translated">폴드는 별개의 그룹의 수가 각각의 폴드에서 대략 동일하다는 의미에서 대략 균형이 잡힌다.</target>
        </trans-unit>
        <trans-unit id="b43c7b928f1a786f0281c067a04292ccfddc7963" translate="yes" xml:space="preserve">
          <source>The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the input variables. In mathematical notion, if \(\hat{y}\) is the predicted value.</source>
          <target state="translated">다음은 목표 값이 입력 변수의 선형 조합 일 것으로 예상되는 회귀를위한 일련의 방법입니다. 수학 개념에서 \ (\ hat {y} \)가 예측 된 값인 경우.</target>
        </trans-unit>
        <trans-unit id="81301a81f070c1a1a05c02b69a367906c9b88d21" translate="yes" xml:space="preserve">
          <source>The following clustering assignment is slightly better, since it is homogeneous but not complete:</source>
          <target state="translated">다음 클러스터링 할당은 균질하지만 완전하지 않기 때문에 약간 더 좋습니다.</target>
        </trans-unit>
        <trans-unit id="563a9a6f3a15e2fd51ba9e9b647870430bf12e58" translate="yes" xml:space="preserve">
          <source>The following code defines a linear kernel and creates a classifier instance that will use that kernel:</source>
          <target state="translated">다음 코드는 선형 커널을 정의하고 해당 커널을 사용할 분류 자 ​​인스턴스를 만듭니다.</target>
        </trans-unit>
        <trans-unit id="b8e362395586349d1fcb8b526001a536e06f5d5d" translate="yes" xml:space="preserve">
          <source>The following code is a bit verbose, feel free to jump directly to the analysis of the &lt;a href=&quot;#results&quot;&gt;results&lt;/a&gt;.</source>
          <target state="translated">다음 코드는 약간 장황하며 &lt;a href=&quot;#results&quot;&gt;결과&lt;/a&gt; 분석으로 바로 이동할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="b414b22c1d4fa210fe910463f00d0b843e42262f" translate="yes" xml:space="preserve">
          <source>The following cross-validation splitters can be used to do that. The grouping identifier for the samples is specified via the &lt;code&gt;groups&lt;/code&gt; parameter.</source>
          <target state="translated">이를 위해 다음과 같은 교차 유효성 검사 스플리터를 사용할 수 있습니다. 샘플의 그룹화 식별자는 &lt;code&gt;groups&lt;/code&gt; 매개 변수 를 통해 지정됩니다 .</target>
        </trans-unit>
        <trans-unit id="6757b5cfa24458748d25f81af60d324180a74dc4" translate="yes" xml:space="preserve">
          <source>The following cross-validators can be used in such cases.</source>
          <target state="translated">이러한 경우 다음 교차 유효성 검사기를 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="6da3fe3659ba9973062031cbded9ac1ee9e80559" translate="yes" xml:space="preserve">
          <source>The following dataset has integer features, two of which are the same in every sample. These are removed with the default setting for threshold:</source>
          <target state="translated">다음 데이터 세트에는 정수 피처가 있으며이 중 두 개는 모든 샘플에서 동일합니다. 이들은 임계 값에 대한 기본 설정으로 제거됩니다.</target>
        </trans-unit>
        <trans-unit id="f50629b49593a5773731e8a4d6d7fbab3d06afa6" translate="yes" xml:space="preserve">
          <source>The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):</source>
          <target state="translated">다음 예제는 데이터를 분할하고 모델을 피팅하며 5 회 연속 점수를 매번 (각각 다른 분할로) 계산하여 홍채 데이터 세트에서 선형 커널 지원 벡터 시스템의 정확도를 추정하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="50761302c7bb4483750fc53752f14a1bf26cb023" translate="yes" xml:space="preserve">
          <source>The following example illustrates 16 components extracted using sparse PCA from the Olivetti faces dataset. It can be seen how the regularization term induces many zeros. Furthermore, the natural structure of the data causes the non-zero coefficients to be vertically adjacent. The model does not enforce this mathematically: each component is a vector \(h \in \mathbf{R}^{4096}\), and there is no notion of vertical adjacency except during the human-friendly visualization as 64x64 pixel images. The fact that the components shown below appear local is the effect of the inherent structure of the data, which makes such local patterns minimize reconstruction error. There exist sparsity-inducing norms that take into account adjacency and different kinds of structure; see &lt;a href=&quot;#jen09&quot; id=&quot;id2&quot;&gt;[Jen09]&lt;/a&gt; for a review of such methods. For more details on how to use Sparse PCA, see the Examples section, below.</source>
          <target state="translated">다음 예는 Olivetti면 데이터 세트에서 스파 스 PCA를 사용하여 추출한 16 개의 구성 요소를 보여줍니다. 정규화 용어가 많은 영 (0)을 유도하는 방법을 볼 수 있습니다. 또한, 데이터의 자연적 구조는 0이 아닌 계수가 수직으로 인접하게한다. 이 모델은 수학적으로이를 강제하지 않습니다. 각 구성 요소는 벡터 \ (h \ in \ mathbf {R} ^ {4096} \)이며 64x64 픽셀 이미지로 인간 친화적 인 시각화를 제외하고는 수직 인접성 개념이 없습니다. 아래에 표시된 구성 요소가 로컬로 표시된다는 사실은 데이터의 고유 한 구조의 영향으로 이러한 로컬 패턴으로 인해 재구성 오류가 최소화됩니다. 인접성과 다른 종류의 구조를 고려한 희소성을 유발하는 규범이 있습니다. &lt;a href=&quot;#jen09&quot; id=&quot;id2&quot;&gt;[Jen09]&lt;/a&gt; 참조그러한 방법의 검토를 위해. Sparse PCA 사용 방법에 대한 자세한 내용은 아래 예 섹션을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="0b4e5d16e2ae8075091f7ec9c797f8dd043b5cfe" translate="yes" xml:space="preserve">
          <source>The following example illustrates how the decision regions may change when a soft &lt;code&gt;VotingClassifier&lt;/code&gt; is used based on an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier:</source>
          <target state="translated">다음 예 는 선형 지원 벡터 머신, 의사 결정 트리 및 K- 최근 접 이웃 분류기를 기반으로 소프트 &lt;code&gt;VotingClassifier&lt;/code&gt; 를 사용할 때 결정 영역이 변경되는 방법을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="fcb0630f60eb0a1b1e5514ec57d9a7f6cbd21ce7" translate="yes" xml:space="preserve">
          <source>The following example illustrates the effect of scaling the regularization parameter when using &lt;a href=&quot;../../modules/svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; for &lt;a href=&quot;../../modules/svm#svm-classification&quot;&gt;classification&lt;/a&gt;. For SVC classification, we are interested in a risk minimization for the equation:</source>
          <target state="translated">다음 예는 &lt;a href=&quot;../../modules/svm#svm&quot;&gt;Support Vector Machine&lt;/a&gt; 을 사용하여 &lt;a href=&quot;../../modules/svm#svm-classification&quot;&gt;분류&lt;/a&gt; 할 때 정규화 매개 변수를 조정하는 효과를 보여줍니다 . SVC 분류의 경우 방정식에 대한 위험 최소화에 관심이 있습니다.</target>
        </trans-unit>
        <trans-unit id="35227ebb51bdeaa9a401c5e44c0ca9ca35e1f0e6" translate="yes" xml:space="preserve">
          <source>The following example shows a color-coded representation of the relative importances of each individual pixel for a face recognition task using a &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt;&lt;code&gt;ExtraTreesClassifier&lt;/code&gt;&lt;/a&gt; model.</source>
          <target state="translated">다음 예제는 &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt; &lt;code&gt;ExtraTreesClassifier&lt;/code&gt; &lt;/a&gt; 모델을 사용하여 얼굴 인식 작업에 대한 각 개별 픽셀의 상대적 중요도를 색상으로 표시 한 것입니다 .</target>
        </trans-unit>
        <trans-unit id="e4e3a609b2ed09432a121fc3917b934875c3544c" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit an AdaBoost classifier with 100 weak learners:</source>
          <target state="translated">다음 예제는 약한 학습자 100 명과 AdaBoost 분류기를 맞추는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="486be98f63ccda08601d245e4a85066d3abba297" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit the majority rule classifier:</source>
          <target state="translated">다음 예는 다수 규칙 분류자를 맞추는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="376fe34989034d77dfd504e8201c3d4b2dfa1573" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the 5 right informative features in the Friedman #1 dataset.</source>
          <target state="translated">다음 예는 Friedman # 1 데이터 세트에서 5 가지 올바른 정보 기능을 검색하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="25b5efc453e06d92e8572618fefd1930c5c8aab3" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the a-priori not known 5 informative features in the Friedman #1 dataset.</source>
          <target state="translated">다음 예는 Friedman # 1 데이터 세트에서 알려지지 않은 5 가지 유익한 기능을 검색하는 방법을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="e99af0f029f79715308d4d1c9ecbe2acb8983e4d" translate="yes" xml:space="preserve">
          <source>The following example will, for instance, transform some British spelling to American spelling:</source>
          <target state="translated">예를 들어 다음 예제는 영국식 철자를 미국식 철자로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="ad9262c5496a82045b0e3b64071f384beed06659" translate="yes" xml:space="preserve">
          <source>The following experiment is performed on an artificial dataset for binary classification with 100,000 samples (1,000 of them are used for model fitting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The figure shows the estimated probabilities obtained with logistic regression, a linear support-vector classifier (SVC), and linear SVC with both isotonic calibration and sigmoid calibration. The Brier score is a metric which is a combination of calibration loss and refinement loss, &lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt;&lt;code&gt;brier_score_loss&lt;/code&gt;&lt;/a&gt;, reported in the legend (the smaller the better). Calibration loss is defined as the mean squared deviation from empirical probabilities derived from the slope of ROC segments. Refinement loss can be defined as the expected optimal loss as measured by the area under the optimal cost curve.</source>
          <target state="translated">다음 실험은 20 개의 특징을 가진 100,000 개의 샘플 (1,000 개가 모델 피팅에 사용됨)로 이진 분류를위한 인공 데이터 세트에서 수행됩니다. 20 개의 기능 중 2 개만 유익하고 10 개는 중복입니다. 그림은 로지스틱 회귀 분석, 선형 지원-벡터 분류기 (SVC) 및 등장 교정 및 시그 모이 드 교정이 모두 포함 된 선형 SVC를 통해 얻은 추정 확률을 보여줍니다. Brier 점수는 교정 손실과 정제 손실, &lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt; &lt;code&gt;brier_score_loss&lt;/code&gt; &lt;/a&gt; 의 조합 인 메트릭입니다., 범례에보고되었습니다 (더 작을수록 좋습니다). 교정 손실은 ROC 세그먼트의 기울기에서 파생 된 경험적 확률의 평균 제곱 편차로 정의됩니다. 정제 손실은 최적 비용 곡선 아래 면적에 의해 측정 된 예상 최적 손실로 정의 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2b3bbe94b0947452c8fc360afc3d0e6f8832ae22" translate="yes" xml:space="preserve">
          <source>The following figure compares &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The learned model of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; is plotted, where both complexity/regularization and bandwidth of the RBF kernel have been optimized using grid-search. The learned functions are very similar; however, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is approx. seven times faster than fitting &lt;code&gt;SVR&lt;/code&gt; (both with grid-search). However, prediction of 100000 target values is more than three times faster with SVR since it has learned a sparse model using only approx. 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="translated">다음 그림은 사인파 목표 함수와 매 5 번째 데이터 포인트에 추가 된 강한 노이즈로 구성된 인공 데이터 세트에서 &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; 와 &lt;code&gt;SVR&lt;/code&gt; 을 비교합니다 . 학습 된 &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; 및 &lt;code&gt;SVR&lt;/code&gt; 모델 이 그려져 있으며 RBF 커널의 복잡성 / 규정 화 및 대역폭이 그리드 검색을 사용하여 최적화되었습니다. 학습 된 기능은 매우 유사합니다. 그러나 &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; 피팅 은 약입니다. &lt;code&gt;SVR&lt;/code&gt; 피팅보다 7 배 더 빠릅니다 (둘 다 그리드 검색 사용). 그러나 SVR을 사용하면 100000 개의 목표 값을 예측하는 데 약 3 배만 빠릅니다. 100 개의 교육 데이터 포인트 중 1/3이 지원 벡터로 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="b11b5ffd2f0d8dbd878722867b9b243b2e73a5ea" translate="yes" xml:space="preserve">
          <source>The following figure compares the location of the non-zeros in W obtained with a simple Lasso or a MultiTaskLasso. The Lasso estimates yields scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns.</source>
          <target state="translated">다음 그림은 W에서 0이 아닌 위치를 간단한 올가미 또는 MultiTaskLasso로 비교 한 것입니다. 올가미는 0이 아닌 산란을 산출하지만 0이 아닌 MultiTaskLasso는 전체 열입니다.</target>
        </trans-unit>
        <trans-unit id="c66df17d5ba5efe635efde7f93de1fc62e75c222" translate="yes" xml:space="preserve">
          <source>The following figure illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise. The figure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited for learning periodic functions. The kernel&amp;rsquo;s hyperparameters control the smoothness (length_scale) and periodicity of the kernel (periodicity). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR.</source>
          <target state="translated">다음 그림은 사인파 목표 함수와 강한 노이즈로 구성된 인공 데이터 세트의 두 가지 방법을 보여줍니다. 이 그림은 주기적 기능 학습에 적합한 ExpSineSquared 커널을 기반으로 KRR 및 GPR의 학습 모델을 비교합니다. 커널의 하이퍼 파라미터는 커널의 부드러움 (길이 _ 스케일) 및주기 (주기)를 제어합니다. 또한 데이터의 노이즈 레벨은 커널의 추가 WhiteKernel 구성 요소와 KRR의 정규화 매개 변수 alpha에 의해 GPR에 의해 명시 적으로 학습됩니다.</target>
        </trans-unit>
        <trans-unit id="c2a01fd3fe0b5b9cd28910783764aacd71509b19" translate="yes" xml:space="preserve">
          <source>The following illustrate the probability density functions of the target before and after applying the logarithmic functions.</source>
          <target state="translated">다음은 로그 함수를 적용하기 전후에 목표의 확률 밀도 함수를 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="b0293fdb9cc21af9db21d1f9f61ccde2a4565147" translate="yes" xml:space="preserve">
          <source>The following image shows how a dictionary learned from 4x4 pixel image patches extracted from part of the image of a raccoon face looks like.</source>
          <target state="translated">다음 이미지는 너구리 얼굴 이미지의 일부에서 추출 된 4x4 픽셀 이미지 패치에서 배운 사전이 어떻게 보이는지 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="a988a0a50289dff96522065b976c597b6e4e63e8" translate="yes" xml:space="preserve">
          <source>The following image shows on the data above the estimated probability using a Gaussian naive Bayes classifier without calibration, with a sigmoid calibration and with a non-parametric isotonic calibration. One can observe that the non-parametric model provides the most accurate probability estimates for samples in the middle, i.e., 0.5.</source>
          <target state="translated">다음 이미지는 교정없이, 시그 모이 드 교정 및 비모수 적 등장 교정으로 가우시안 순진 베이 즈 분류기를 사용하여 추정 된 확률 위의 데이터를 보여줍니다. 비모수 적 모델은 중간에있는 샘플, 즉 0.5에 대해 가장 정확한 확률 추정치를 제공한다는 것을 알 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b45c55863d114ed9459a8475837831820ff65766" translate="yes" xml:space="preserve">
          <source>The following images demonstrate the benefit of probability calibration. The first image present a dataset with 2 classes and 3 blobs of data. The blob in the middle contains random samples of each class. The probability for the samples in this blob should be 0.5.</source>
          <target state="translated">다음 이미지는 확률 보정의 이점을 보여줍니다. 첫 번째 이미지는 2 개의 클래스와 3 개의 데이터 Blob이있는 데이터 집합을 나타냅니다. 가운데의 얼룩에는 각 클래스의 무작위 샘플이 포함되어 있습니다. 이 얼룩의 샘플 확률은 0.5 여야합니다.</target>
        </trans-unit>
        <trans-unit id="807c0e6975f025a025e2951d9c9164c1f3454c71" translate="yes" xml:space="preserve">
          <source>The following loss functions are supported and can be specified using the parameter &lt;code&gt;loss&lt;/code&gt;:</source>
          <target state="translated">다음 손실 기능이 지원되며 매개 변수 &lt;code&gt;loss&lt;/code&gt; 를 사용하여 지정할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="70624e811991b8fac875c2f0dd0db6a8fb2e1e2b" translate="yes" xml:space="preserve">
          <source>The following plots demonstrate the impact of the number of clusters and number of samples on various clustering performance evaluation metrics.</source>
          <target state="translated">다음 그림은 다양한 군집 성능 평가 지표에 대한 군집 수 및 샘플 수의 영향을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="661cd7d1cca4025c0fd4f4a1e938e7140f6ef24b" translate="yes" xml:space="preserve">
          <source>The following sections contain further explanations and examples that illustrate how the tf-idfs are computed exactly and how the tf-idfs computed in scikit-learn&amp;rsquo;s &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;TfidfVectorizer&lt;/code&gt;&lt;/a&gt; differ slightly from the standard textbook notation that defines the idf as</source>
          <target state="translated">다음 섹션에는 tf-idfs가 정확하게 계산되는 방법과 scikit-learn의 &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;TfidfVectorizer&lt;/code&gt; &lt;/a&gt; 에서 계산 된 tf-idfs 가 idf를</target>
        </trans-unit>
        <trans-unit id="78ad5f101e233b65633dffc5c68bec29c1eae0b0" translate="yes" xml:space="preserve">
          <source>The following sections list utilities to generate indices that can be used to generate dataset splits according to different cross validation strategies.</source>
          <target state="translated">다음 섹션에는 다양한 교차 검증 전략에 따라 데이터 세트 분할을 생성하는 데 사용할 수있는 인덱스를 생성하는 유틸리티가 나열되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="5c75d78bf5e5b93a438a72e22f9eee2db09a82ed" translate="yes" xml:space="preserve">
          <source>The following snippet demonstrates how to replace missing values, encoded as &lt;code&gt;np.nan&lt;/code&gt;, using the mean value of the columns (axis 0) that contain the missing values:</source>
          <target state="translated">다음 스 니펫은 &lt;code&gt;np.nan&lt;/code&gt; 이 포함 된 열 (축 0)의 평균값을 사용하여 결 측값을 np.nan으로 인코딩하여 바꾸는 방법을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="46781e9d9b616f25e94b9f1718ce2c06cffa693e" translate="yes" xml:space="preserve">
          <source>The following two references explain the iterations used in the coordinate descent solver of scikit-learn, as well as the duality gap computation used for convergence control.</source>
          <target state="translated">다음의 두 참조는 컨시어지 제어에 사용되는 이중성 갭 계산뿐만 아니라 scikit-learn의 좌표 하강 솔버에 사용 된 반복을 설명합니다.</target>
        </trans-unit>
        <trans-unit id="d492404a63c6d65da32b70e6dcf87e75a94b6a4d" translate="yes" xml:space="preserve">
          <source>The form of the model learned by &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is identical to support vector regression (&lt;code&gt;SVR&lt;/code&gt;). However, different loss functions are used: KRR uses squared error loss while support vector regression uses \(\epsilon\)-insensitive loss, both combined with l2 regularization. In contrast to &lt;code&gt;SVR&lt;/code&gt;, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for \(\epsilon &amp;gt; 0\), at prediction-time.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; 가 학습 한 모델의 형식은 &lt;code&gt;SVR&lt;/code&gt; (벡터 회귀 ) 지원과 동일합니다 . 그러나 서로 다른 손실 함수가 사용됩니다. KRR은 제곱 오차 손실을 사용하지만 지원 벡터 회귀는 \ (\ epsilon \)에 둔감 한 손실을 l2 정규화와 함께 사용합니다. &lt;code&gt;SVR&lt;/code&gt; 과 달리 피팅 &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; 는 닫힌 형태로 수행 할 수 있으며 일반적으로 중간 규모의 데이터 세트에 더 빠릅니다. 반면에 학습 된 모델은 희소성이 아니므로 SVR보다 느리므로 예측 시간에 \ (\ epsilon&amp;gt; 0 \)에 대한 희소 모델을 학습합니다.</target>
        </trans-unit>
        <trans-unit id="cb6eb14fd1b6ffac38e7d6c15d36b7076beafa85" translate="yes" xml:space="preserve">
          <source>The form of the model learned by KRR is identical to support vector regression (SVR). However, different loss functions are used: KRR uses squared error loss while support vector regression uses epsilon-insensitive loss, both combined with l2 regularization. In contrast to SVR, fitting a KRR model can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for epsilon &amp;gt; 0, at prediction-time.</source>
          <target state="translated">KRR이 학습 한 모델의 형식은 SVR (벡터 회귀) 지원과 동일합니다. 그러나 서로 다른 손실 함수가 사용됩니다. KRR은 제곱 오차 손실을 사용하고 지원 벡터 회귀는 엡실론에 둔감 한 손실을 l2 정규화와 함께 사용합니다. SVR과 달리 KRR 모델의 맞춤은 닫힌 형식으로 수행 할 수 있으며 일반적으로 중간 규모의 데이터 집합에 더 빠릅니다. 반면에 학습 된 모델은 희소성이 아니므로 SVR보다 느리므로 예측 시간에 엡실론&amp;gt; 0에 대한 희소 모델을 학습합니다.</target>
        </trans-unit>
        <trans-unit id="785bfd19f7d4d298d07ec2673131bad97bee7c7c" translate="yes" xml:space="preserve">
          <source>The form of these kernels is as follows:</source>
          <target state="translated">이 커널의 형태는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="99fa4cbad7a403537a73165a2b1d717c58bb9b6b" translate="yes" xml:space="preserve">
          <source>The formula that is used to compute the tf-idf of term t is tf-idf(d, t) = tf(t) * idf(d, t), and the idf is computed as idf(d, t) = log [ n / df(d, t) ] + 1 (if &lt;code&gt;smooth_idf=False&lt;/code&gt;), where n is the total number of documents and df(d, t) is the document frequency; the document frequency is the number of documents d that contain term t. The effect of adding &amp;ldquo;1&amp;rdquo; to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(d, t) = log [ n / (df(d, t) + 1) ]).</source>
          <target state="translated">용어 t의 tf-idf를 계산하는 데 사용되는 공식은 tf-idf (d, t) = tf (t) * idf (d, t)이고 idf는 idf (d, t) = log로 계산됩니다. [n / df (d, t)] + 1 ( &lt;code&gt;smooth_idf=False&lt;/code&gt; 인 경우 ), 여기서 n은 총 문서 수이고 df (d, t)는 문서 빈도입니다. 문서 빈도는 용어 t를 포함하는 문서 d의 수입니다. 위 방정식에서 idf에 &quot;1&quot;을 추가하면 idf가 0 인 항, 즉 훈련 세트의 모든 문서에서 발생하는 항이 완전히 무시되지 않습니다. 위의 idf 공식은 idf를 idf (d, t) = log [n / (df (d, t) + 1)]로 정의하는 표준 교과서 표기법과 다릅니다.</target>
        </trans-unit>
        <trans-unit id="2757038bccf5826fff274cfe8684f779a3893302" translate="yes" xml:space="preserve">
          <source>The formula used here does not correspond to the one given in the article. In the original article, formula (23) states that 2/p is multiplied by Trace(cov*cov) in both the numerator and denominator, but this operation is omitted because for a large p, the value of 2/p is so small that it doesn&amp;rsquo;t affect the value of the estimator.</source>
          <target state="translated">여기에 사용 된 공식은 기사에 제공된 공식과 일치하지 않습니다. 원래 기사에서 수식 (23)은 분자와 분모 모두에서 2 / p에 Trace (cov * cov)를 곱한 값을 나타내지 만 큰 p의 경우 2 / p의 값이 너무 작으므로이 연산이 생략됩니다. 추정기의 가치에 영향을 미치지 않습니다.</target>
        </trans-unit>
        <trans-unit id="25426ebad0f81610af376f208a1d7f7cdb7c96c5" translate="yes" xml:space="preserve">
          <source>The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. &lt;code&gt;subsample&lt;/code&gt; interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;. Choosing &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; leads to a reduction of variance and an increase in bias.</source>
          <target state="translated">개별 기본 학습자를 맞추는 데 사용되는 샘플의 비율입니다. 1.0보다 작 으면 확률 적 그라디언트 부스팅이 발생합니다. &lt;code&gt;subsample&lt;/code&gt; 은 &lt;code&gt;n_estimators&lt;/code&gt; 매개 변수와 상호 작용합니다 . &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; 선택하면 분산이 감소하고 바이어스가 증가합니다.</target>
        </trans-unit>
        <trans-unit id="568fbbbfe83b2ef390104a99310641880cb62ce5" translate="yes" xml:space="preserve">
          <source>The fraction of samples to be used in each randomized design. Should be between 0 and 1. If 1, all samples are used.</source>
          <target state="translated">각 무작위 설계에 사용될 샘플의 비율. 0과 1 사이 여야합니다. 1이면 모든 샘플이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="52b16a6d5efc38a8b1d594773d860718286a7706" translate="yes" xml:space="preserve">
          <source>The fraction of samples whose class are randomly exchanged. Larger values introduce noise in the labels and make the classification task harder.</source>
          <target state="translated">클래스가 무작위로 교환되는 샘플의 비율. 값이 클수록 레이블에 노이즈가 발생하고 분류 작업이 더 어려워집니다.</target>
        </trans-unit>
        <trans-unit id="2b8586795acd1de1d253165ce5ff74dbee8020e0" translate="yes" xml:space="preserve">
          <source>The free parameters in the model are C and epsilon.</source>
          <target state="translated">모델의 자유 매개 변수는 C 및 엡실론입니다.</target>
        </trans-unit>
        <trans-unit id="852901dbf93c375963721fc84a4888e6b45c78dd" translate="yes" xml:space="preserve">
          <source>The full description of the dataset</source>
          <target state="translated">데이터 세트에 대한 전체 설명</target>
        </trans-unit>
        <trans-unit id="19877feb57bb09f6d25fbaaa466f42d45ca4d944" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.feature_extraction.image.img_to_graph#sklearn.feature_extraction.image.img_to_graph&quot;&gt;&lt;code&gt;img_to_graph&lt;/code&gt;&lt;/a&gt; returns such a matrix from a 2D or 3D image. Similarly, &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt;&lt;code&gt;grid_to_graph&lt;/code&gt;&lt;/a&gt; build a connectivity matrix for images given the shape of these image.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.image.img_to_graph#sklearn.feature_extraction.image.img_to_graph&quot;&gt; &lt;code&gt;img_to_graph&lt;/code&gt; &lt;/a&gt; 함수 는 2D 또는 3D 이미지에서 이러한 행렬을 반환합니다. 마찬가지로 &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt; &lt;code&gt;grid_to_graph&lt;/code&gt; &lt;/a&gt; 는 이러한 이미지의 모양이 지정된 이미지에 대한 연결성 매트릭스를 작성합니다.</target>
        </trans-unit>
        <trans-unit id="6bbdd98a7d5d87ddd3bbdfb8569481a5e5e04495" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.cohen_kappa_score#sklearn.metrics.cohen_kappa_score&quot;&gt;&lt;code&gt;cohen_kappa_score&lt;/code&gt;&lt;/a&gt; computes &lt;a href=&quot;https://en.wikipedia.org/wiki/Cohen%27s_kappa&quot;&gt;Cohen&amp;rsquo;s kappa&lt;/a&gt; statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.cohen_kappa_score#sklearn.metrics.cohen_kappa_score&quot;&gt; &lt;code&gt;cohen_kappa_score&lt;/code&gt; &lt;/a&gt; 함수는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Cohen%27s_kappa&quot;&gt;Cohen의 카파&lt;/a&gt; 통계량을 계산 합니다. 이 측정법은 분류 자와 대립 진실이 아닌 다른 인간 주석에 의한 레이블을 비교하기위한 것입니다.</target>
        </trans-unit>
        <trans-unit id="93f21bf6b976a0d8004246b9140ac3f9b0310fba" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.laplacian_kernel#sklearn.metrics.pairwise.laplacian_kernel&quot;&gt;&lt;code&gt;laplacian_kernel&lt;/code&gt;&lt;/a&gt; is a variant on the radial basis function kernel defined as:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.pairwise.laplacian_kernel#sklearn.metrics.pairwise.laplacian_kernel&quot;&gt; &lt;code&gt;laplacian_kernel&lt;/code&gt; &lt;/a&gt; 함수 는 다음과 같이 정의 된 방사형 기본 함수 커널의 변형입니다.</target>
        </trans-unit>
        <trans-unit id="3fe1ad7d2ae4a1d31d30a7b5c63e81d53b7568bb" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt;&lt;code&gt;linear_kernel&lt;/code&gt;&lt;/a&gt; computes the linear kernel, that is, a special case of &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt;&lt;code&gt;polynomial_kernel&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;degree=1&lt;/code&gt; and &lt;code&gt;coef0=0&lt;/code&gt; (homogeneous). If &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are column vectors, their linear kernel is:</source>
          <target state="translated">함수 &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt; &lt;code&gt;linear_kernel&lt;/code&gt; 가&lt;/a&gt; 인 선형 커널의 특별한 경우 계산 &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt; &lt;code&gt;polynomial_kernel&lt;/code&gt; &lt;/a&gt; 와 &lt;code&gt;degree=1&lt;/code&gt; 및 &lt;code&gt;coef0=0&lt;/code&gt; (균일)를. 경우 &lt;code&gt;x&lt;/code&gt; 및 &lt;code&gt;y&lt;/code&gt; 열 벡터이고, 그 선형 커널은 :</target>
        </trans-unit>
        <trans-unit id="72295044331d14363e1e1fe37cea504256d7c67f" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt;&lt;code&gt;polynomial_kernel&lt;/code&gt;&lt;/a&gt; computes the degree-d polynomial kernel between two vectors. The polynomial kernel represents the similarity between two vectors. Conceptually, the polynomial kernels considers not only the similarity between vectors under the same dimension, but also across dimensions. When used in machine learning algorithms, this allows to account for feature interaction.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt; &lt;code&gt;polynomial_kernel&lt;/code&gt; &lt;/a&gt; 함수 는 두 벡터 사이의 차수 d 다항식 커널을 계산합니다. 다항식 커널은 두 벡터 간의 유사성을 나타냅니다. 개념적으로 다항식 커널은 동일한 차원에서 벡터 간의 유사성뿐만 아니라 차원에서도 유사성을 고려합니다. 머신 러닝 알고리즘에 사용될 때 기능 상호 작용을 설명 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2f5b051b13aa3478b0b4a17587fa0bec1e6b87b4" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.rbf_kernel#sklearn.metrics.pairwise.rbf_kernel&quot;&gt;&lt;code&gt;rbf_kernel&lt;/code&gt;&lt;/a&gt; computes the radial basis function (RBF) kernel between two vectors. This kernel is defined as:</source>
          <target state="translated">함수 &lt;a href=&quot;generated/sklearn.metrics.pairwise.rbf_kernel#sklearn.metrics.pairwise.rbf_kernel&quot;&gt; &lt;code&gt;rbf_kernel&lt;/code&gt; 은&lt;/a&gt; 두 벡터 사이의 방사형 기저 함수 (RBF) 커널을 계산한다. 이 커널은 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="a4ac87fe4bd82908551f017a1e984b845dfe00ca" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.sigmoid_kernel#sklearn.metrics.pairwise.sigmoid_kernel&quot;&gt;&lt;code&gt;sigmoid_kernel&lt;/code&gt;&lt;/a&gt; computes the sigmoid kernel between two vectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer Perceptron (because, in the neural network field, it is often used as neuron activation function). It is defined as:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.pairwise.sigmoid_kernel#sklearn.metrics.pairwise.sigmoid_kernel&quot;&gt; &lt;code&gt;sigmoid_kernel&lt;/code&gt; &lt;/a&gt; 함수 는 두 벡터 사이의 시그 모이 드 커널을 계산합니다. 시그 모이 드 커널은 쌍곡 탄젠트 (hyperbolic tangent) 또는 다층 퍼셉트론 (Multilayer Perceptron)으로도 알려져 있습니다 (신경망 분야에서는 종종 뉴런 활성화 기능으로 사용되기 때문에). 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="2e327cd188c9064345dfa4a6a6764985ae429bc7" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt;&lt;code&gt;roc_curve&lt;/code&gt;&lt;/a&gt; computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;receiver operating characteristic curve, or ROC curve&lt;/a&gt;. Quoting Wikipedia :</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt; &lt;code&gt;roc_curve&lt;/code&gt; &lt;/a&gt; 함수 는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;수신기 작동 특성 곡선 또는 ROC 곡선을&lt;/a&gt; 계산합니다 . 인용 위키 백과 :</target>
        </trans-unit>
        <trans-unit id="a52cb65c85990e02ab77ee98f8132a1c3b7a7b6c" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; has a similar interface to &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt;, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt; 함수는 cross_val_score 와 유사한 인터페이스를 &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; 입력의 각 요소에 대해 테스트 세트에있을 때 해당 요소에 대해 얻은 예측을 리턴합니다. 모든 요소를 ​​테스트 세트에 정확히 한 번만 할당하는 교차 검증 전략 만 사용할 수 있습니다 (그렇지 않으면 예외가 발생 함).</target>
        </trans-unit>
        <trans-unit id="9da333e152484b3ac872458a2a577c489d5042de" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.model_selection.validation_curve#sklearn.model_selection.validation_curve&quot;&gt;&lt;code&gt;validation_curve&lt;/code&gt;&lt;/a&gt; can help in this case:</source>
          <target state="translated">이 경우 &lt;a href=&quot;generated/sklearn.model_selection.validation_curve#sklearn.model_selection.validation_curve&quot;&gt; &lt;code&gt;validation_curve&lt;/code&gt; &lt;/a&gt; 함수 가 도움이 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ef1bfedb8d96897b52fb746234adf67b6ec8f0b2" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt;&lt;code&gt;normalize&lt;/code&gt;&lt;/a&gt; provides a quick and easy way to perform this operation on a single array-like dataset, either using the &lt;code&gt;l1&lt;/code&gt; or &lt;code&gt;l2&lt;/code&gt; norms:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt; &lt;code&gt;normalize&lt;/code&gt; &lt;/a&gt; 함수 는 &lt;code&gt;l1&lt;/code&gt; 또는 &lt;code&gt;l2&lt;/code&gt; 규범을 사용하여 단일 배열과 유사한 데이터 세트에서이 작업을 빠르고 쉽게 수행 할 수있는 방법을 제공합니다 .</target>
        </trans-unit>
        <trans-unit id="621182b188f45e3dd8884ee1d03015ee530041c8" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt;&lt;code&gt;scale&lt;/code&gt;&lt;/a&gt; provides a quick and easy way to perform this operation on a single array-like dataset:</source>
          <target state="translated">함수 &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt; &lt;code&gt;scale&lt;/code&gt; &lt;/a&gt; 은 단일 배열 형 데이터 세트에서이 작업을 빠르고 쉽게 수행 할 수있는 방법을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="14656519a034ad24c06a19460128dde85e00e72d" translate="yes" xml:space="preserve">
          <source>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in &lt;a href=&quot;#r37d39d7589e2-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; and &lt;a href=&quot;#r37d39d7589e2-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Both methods are based on the idea originally proposed in &lt;a href=&quot;#r37d39d7589e2-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">이 함수는 &lt;a href=&quot;#r37d39d7589e2-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; 및 &lt;a href=&quot;#r37d39d7589e2-3&quot; id=&quot;id3&quot;&gt;[3]에&lt;/a&gt; 설명 된 k- 최근 접 이웃 거리로부터의 엔트로피 추정에 기초한 비모수 적 방법에 의존합니다 . 두 방법 모두 &lt;a href=&quot;#r37d39d7589e2-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt; 에서 원래 제안 된 아이디어를 기반으로합니다 .</target>
        </trans-unit>
        <trans-unit id="cf1d60e35345e80734f9e0bef9342c1dede910a1" translate="yes" xml:space="preserve">
          <source>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in &lt;a href=&quot;#r50b872b699c4-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; and &lt;a href=&quot;#r50b872b699c4-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Both methods are based on the idea originally proposed in &lt;a href=&quot;#r50b872b699c4-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">이 함수는 &lt;a href=&quot;#r50b872b699c4-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; 및 &lt;a href=&quot;#r50b872b699c4-3&quot; id=&quot;id3&quot;&gt;[3]에&lt;/a&gt; 설명 된 k- 최근 접 이웃 거리로부터의 엔트로피 추정에 기초한 비모수 적 방법에 의존합니다 . 두 방법 모두 &lt;a href=&quot;#r50b872b699c4-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt; 에서 원래 제안 된 아이디어를 기반으로합니다 .</target>
        </trans-unit>
        <trans-unit id="fb8314efd5d7c36c4d228413206cd5331b82e456" translate="yes" xml:space="preserve">
          <source>The function requires either the argument &lt;code&gt;grid&lt;/code&gt; which specifies the values of the target features on which the partial dependence function should be evaluated or the argument &lt;code&gt;X&lt;/code&gt; which is a convenience mode for automatically creating &lt;code&gt;grid&lt;/code&gt; from the training data. If &lt;code&gt;X&lt;/code&gt; is given, the &lt;code&gt;axes&lt;/code&gt; value returned by the function gives the axis for each target feature.</source>
          <target state="translated">이 함수에는 부분 의존 함수를 평가할 대상 피처의 값을 지정 하는 인수 &lt;code&gt;grid&lt;/code&gt; 또는 훈련 데이터에서 &lt;code&gt;grid&lt;/code&gt; 를 자동으로 생성하기위한 편리한 모드 인 인수 &lt;code&gt;X&lt;/code&gt; 가 필요합니다 . 경우 &lt;code&gt;X&lt;/code&gt; 가 부여되어 상기 &lt;code&gt;axes&lt;/code&gt; 이 함수에 의해 리턴 된 값은 각각의 타겟 피쳐에 대한 축을 제공한다.</target>
        </trans-unit>
        <trans-unit id="d2fedea237bdc334f6322c4692b523df3b5f6fbf" translate="yes" xml:space="preserve">
          <source>The function to be decorated</source>
          <target state="translated">꾸미는 기능</target>
        </trans-unit>
        <trans-unit id="f7fc9606332ff37416d86060e52aa56595a5e3ce" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;friedman_mse&amp;rdquo; for the mean squared error with improvement score by Friedman, &amp;ldquo;mse&amp;rdquo; for mean squared error, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error. The default value of &amp;ldquo;friedman_mse&amp;rdquo; is generally the best as it can provide a better approximation in some cases.</source>
          <target state="translated">분할 품질을 측정하는 기능입니다. 지원 기준은 Friedman이 개선 한 평균 제곱 오차의 경우 &quot;friedman_mse&quot;, 평균 제곱 오차의 경우 &quot;mse&quot;, 평균 절대 오차의 경우 &quot;mae&quot;입니다. &quot;friedman_mse&quot;의 기본값은 경우에 따라 더 나은 근사치를 제공 할 수 있으므로 일반적으로 최고입니다.</target>
        </trans-unit>
        <trans-unit id="8c0bede693f27f1257cedfa964bd89180e6bfada" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;gini&amp;rdquo; for the Gini impurity and &amp;ldquo;entropy&amp;rdquo; for the information gain.</source>
          <target state="translated">분할 품질을 측정하는 기능입니다. 지원되는 기준은 Gini 불순물에 대한&amp;ldquo;지니&amp;rdquo;및 정보 획득에 대한&amp;ldquo;엔트로피&amp;rdquo;입니다.</target>
        </trans-unit>
        <trans-unit id="03e746b06910ee8c07bc239c0b780e5294140dfb" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;gini&amp;rdquo; for the Gini impurity and &amp;ldquo;entropy&amp;rdquo; for the information gain. Note: this parameter is tree-specific.</source>
          <target state="translated">분할 품질을 측정하는 기능입니다. 지원되는 기준은 Gini 불순물에 대한&amp;ldquo;지니&amp;rdquo;및 정보 획득에 대한&amp;ldquo;엔트로피&amp;rdquo;입니다. 참고 :이 매개 변수는 트리마다 다릅니다.</target>
        </trans-unit>
        <trans-unit id="c211c5fb9289c19b7ab7fb609c5f42d0fd7fb417" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;mse&amp;rdquo; for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, &amp;ldquo;friedman_mse&amp;rdquo;, which uses mean squared error with Friedman&amp;rsquo;s improvement score for potential splits, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error, which minimizes the L1 loss using the median of each terminal node.</source>
          <target state="translated">분할 품질을 측정하는 기능입니다. 지원되는 기준은 평균 제곱 오차에 대해 &quot;mse&quot;이며, 이는 기능 선택 기준으로 분산 감소와 동일하며 각 터미널 노드의 평균 인 &quot;friedman_mse&quot;를 사용하여 L2 손실을 최소화합니다. 평균 절대 오차의 경우 &quot;mae&quot;를 나누고 각 터미널 노드의 중앙값을 사용하여 L1 손실을 최소화합니다.</target>
        </trans-unit>
        <trans-unit id="bf422f8c78875448c6e34d2c008baf4a5621c47d" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;mse&amp;rdquo; for the mean squared error, which is equal to variance reduction as feature selection criterion, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error.</source>
          <target state="translated">분할 품질을 측정하는 기능입니다. 지원되는 기준은 평균 제곱 오차에 대해 &quot;mse&quot;이며, 이는 특징 선택 기준으로 분산 감소와 동일하며 평균 절대 오차에 대해서는 &quot;mae&quot;입니다.</target>
        </trans-unit>
        <trans-unit id="8b3c9548b1b42f5af71c7bfe5f885c712284a1e6" translate="yes" xml:space="preserve">
          <source>The function which is applied on each chunk of the distance matrix, reducing it to needed values. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; is called repeatedly, where &lt;code&gt;D_chunk&lt;/code&gt; is a contiguous vertical slice of the pairwise distance matrix, starting at row &lt;code&gt;start&lt;/code&gt;. It should return an array, a list, or a sparse matrix of length &lt;code&gt;D_chunk.shape[0]&lt;/code&gt;, or a tuple of such objects.</source>
          <target state="translated">거리 행렬의 각 청크에 적용되는 함수로 필요한 값으로 줄입니다. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; 가 반복적으로 호출됩니다. 여기서 &lt;code&gt;D_chunk&lt;/code&gt; 는 행 &lt;code&gt;start&lt;/code&gt; 에서 시작하여 쌍방향 거리 행렬의 연속적인 수직 슬라이스입니다 . 길이가 &lt;code&gt;D_chunk.shape[0]&lt;/code&gt; 인 배열, 목록 또는 희소 행렬 또는 이러한 객체의 튜플을 반환해야 합니다.</target>
        </trans-unit>
        <trans-unit id="34f983a96e6e1ba7a36ae52fb70df739e3a5aca9" translate="yes" xml:space="preserve">
          <source>The functional form of the G function used in the approximation to neg-entropy. Could be either &amp;lsquo;logcosh&amp;rsquo;, &amp;lsquo;exp&amp;rsquo;, or &amp;lsquo;cube&amp;rsquo;. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. Example:</source>
          <target state="translated">음의 엔트로피 근사에 사용되는 G 함수의 기능적 형식입니다. 'logcosh', 'exp'또는 'cube'일 수 있습니다. 자신의 기능을 제공 할 수도 있습니다. 함수의 값과 그 파생물을 포함하는 튜플을 해당 지점에 반환해야합니다. 예:</target>
        </trans-unit>
        <trans-unit id="f3580ae3ff67a44925f6075f1fc46034cd1f3d14" translate="yes" xml:space="preserve">
          <source>The generated array.</source>
          <target state="translated">생성 된 배열.</target>
        </trans-unit>
        <trans-unit id="22dd341f8c92381f47326cd6fbe4fff46dd59a6b" translate="yes" xml:space="preserve">
          <source>The generated matrix.</source>
          <target state="translated">생성 된 행렬.</target>
        </trans-unit>
        <trans-unit id="b670a7959baa25f4a9c290d5df951114cede8bad" translate="yes" xml:space="preserve">
          <source>The generated samples.</source>
          <target state="translated">생성 된 샘플.</target>
        </trans-unit>
        <trans-unit id="61a0705074aa05bf429a2ff6dd9f09842d1c86f2" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the centers. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">센터를 초기화하는 데 사용되는 생성기. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="473c7ed23812d4bf8fcf6c97ab0551b7301ccf95" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the codebook. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">코드북을 초기화하는 데 사용되는 생성기입니다. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="4e831547e83750929880db075c40fc4aa61eb4b8" translate="yes" xml:space="preserve">
          <source>The generator used to randomize the design. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">설계를 무작위 화하는 데 사용되는 생성기. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cd84f22d9e60e3f86bb2455d78aaf109647d76b1" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select a subset of samples. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;sample_size is not None&lt;/code&gt;.</source>
          <target state="translated">생성기는 샘플의 하위 집합을 무작위로 선택하는 데 사용됩니다. int이면 random_state는 난수 생성기에서 사용하는 시드입니다. RandomState 인스턴스 인 경우 random_state는 난수 생성기입니다. 없음 경우, 난수 발생기에 사용되는 RandomState 인스턴스입니다 &lt;code&gt;np.random&lt;/code&gt; . &lt;code&gt;sample_size is not None&lt;/code&gt; 경우에 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="36aa9d7d033388d006150bbc613bf9f61afb2f31" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select the samples from input points for bandwidth estimation. Use an int to make the randomness deterministic. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">대역폭 추정을 위해 입력 포인트에서 샘플을 무작위로 선택하는 데 사용되는 생성기입니다. int를 사용하여 임의성을 결정 론적으로 만드십시오. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;용어집을&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="6c9c479a6511fc655c70a7c51ab0a85c04034b21" translate="yes" xml:space="preserve">
          <source>The goal is to measure the latency one can expect when doing predictions either in bulk or atomic (i.e. one by one) mode.</source>
          <target state="translated">목표는 대량 또는 원자 (예 : 하나씩) 모드에서 예측을 수행 할 때 예상 할 수있는 대기 시간을 측정하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="95b3715b7309073ce23a5e1ae6bd1c83fb5ab128" translate="yes" xml:space="preserve">
          <source>The goal of &lt;strong&gt;ensemble methods&lt;/strong&gt; is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.</source>
          <target state="translated">&lt;strong&gt;앙상블 방법&lt;/strong&gt; 의 목표 는 단일 추정기에 대한 일반 화성 / 강건성을 향상시키기 위해 주어진 학습 알고리즘으로 구축 된 여러 기본 추정기의 예측을 결합하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="4be143708c968fbd1f8e4e8288622de6f1251f81" translate="yes" xml:space="preserve">
          <source>The goal of this example is to analyze the graph of links inside wikipedia articles to rank articles by relative importance according to this eigenvector centrality.</source>
          <target state="translated">이 예제의 목표는이 고유 벡터 중심성에 따라 위키피디아 기사 내의 링크 그래프를 분석하여 상대적 중요성에 따라 기사의 순위를 정하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="5095c3c5239296131fc48d25860f3771ab9b91e3" translate="yes" xml:space="preserve">
          <source>The goal of this example is to show intuitively how the metrics behave, and not to find good clusters for the digits. This is why the example works on a 2D embedding.</source>
          <target state="translated">이 예의 목표는 지표의 작동 방식을 직관적으로 표시하고 숫자에 적합한 클러스터를 찾지 않는 것입니다. 이것이 예제가 2D 임베딩에서 작동하는 이유입니다.</target>
        </trans-unit>
        <trans-unit id="20bd11aa678c9e04777a40ec0f194ab6b7581aa6" translate="yes" xml:space="preserve">
          <source>The goal of this guide is to explore some of the main &lt;code&gt;scikit-learn&lt;/code&gt; tools on a single practical task: analyzing a collection of text documents (newsgroups posts) on twenty different topics.</source>
          <target state="translated">이 안내서의 목표는 20 가지 주제에 대한 텍스트 문서 (뉴스 그룹 게시물) 모음 분석이라는 단일 실제 작업 에서 주요 &lt;code&gt;scikit-learn&lt;/code&gt; 도구 중 일부를 탐색하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="c224302bdf144aacfdfc260a6638fe1107c2e8d4" translate="yes" xml:space="preserve">
          <source>The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.</source>
          <target state="translated">주어진 문서에서 토큰의 원시 발생 빈도 대신 tf-idf를 사용하는 목표는 주어진 코퍼스에서 매우 자주 발생하고 따라서 경험적으로 덜 유용한 토큰의 영향을 축소하는 것입니다. 훈련 말뭉치의 작은 부분.</target>
        </trans-unit>
        <trans-unit id="3325084d0d02139eb921a2545ba48cff39657720" translate="yes" xml:space="preserve">
          <source>The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.</source>
          <target state="translated">커널의 하이퍼 파라미터에 대한 커널 k (X, X)의 기울기. eval_gradient가 True 인 경우에만 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="1d28aeaa2593827c70ee0cbf35f8d5891fd13e08" translate="yes" xml:space="preserve">
          <source>The graph data is fetched from the DBpedia dumps. DBpedia is an extraction of the latent structured data of the Wikipedia content.</source>
          <target state="translated">그래프 데이터는 DBpedia 덤프에서 가져옵니다. DBpedia는 Wikipedia 컨텐츠의 잠재 구조화 된 데이터를 추출한 것입니다.</target>
        </trans-unit>
        <trans-unit id="94a8641249b9d38c741294dd700ba9c013e60d15" translate="yes" xml:space="preserve">
          <source>The graph should contain only one connect component, elsewhere the results make little sense.</source>
          <target state="translated">그래프에는 하나의 연결 구성 요소 만 포함되어야하며, 그렇지 않으면 결과가 의미가 없습니다.</target>
        </trans-unit>
        <trans-unit id="0aca214d3abec7143ce0e3db3703c7759829a517" translate="yes" xml:space="preserve">
          <source>The graphical model of LDA is a three-level Bayesian model:</source>
          <target state="translated">LDA의 그래픽 모델은 3 단계 베이지안 모델입니다.</target>
        </trans-unit>
        <trans-unit id="b70b496cfa90301063390d7d501e40e03165e505" translate="yes" xml:space="preserve">
          <source>The graphical model of an RBM is a fully-connected bipartite graph.</source>
          <target state="translated">RBM의 그래픽 모델은 완전히 연결된 이분 그래프입니다.</target>
        </trans-unit>
        <trans-unit id="43e8165a75805bcff11ce07ae2103365830550d8" translate="yes" xml:space="preserve">
          <source>The grid of &lt;code&gt;target_variables&lt;/code&gt; values for which the partial dependecy should be evaluated (either &lt;code&gt;grid&lt;/code&gt; or &lt;code&gt;X&lt;/code&gt; must be specified).</source>
          <target state="translated">부분 의존성을 평가해야하는 &lt;code&gt;target_variables&lt;/code&gt; 값 의 그리드입니다 ( &lt;code&gt;grid&lt;/code&gt; 또는 &lt;code&gt;X&lt;/code&gt; 를 지정해야 함).</target>
        </trans-unit>
        <trans-unit id="1fb55e8edd66947697c84e91d6ac3fa247bd29e9" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting</source>
          <target state="translated">피팅에 사용되는 알파 그리드</target>
        </trans-unit>
        <trans-unit id="c8ce42c94fb7c4644fc397d481823f721280ec22" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting, for each l1_ratio</source>
          <target state="translated">각 l1_ratio에 대해 피팅에 사용되는 알파 그리드</target>
        </trans-unit>
        <trans-unit id="92edf0193fba09badba27bf7739525bce6da7601" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting, for each l1_ratio.</source>
          <target state="translated">각 l1_ratio에 대해 피팅에 사용되는 알파 격자입니다.</target>
        </trans-unit>
        <trans-unit id="22daba72217df04ec1af1a8a340277c45da4b54e" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting.</source>
          <target state="translated">피팅에 사용되는 알파 격자입니다.</target>
        </trans-unit>
        <trans-unit id="37c7f40c413d2aeb11b8645a838ea3c671b821b6" translate="yes" xml:space="preserve">
          <source>The grid points between 0 and 1: alpha/alpha_max</source>
          <target state="translated">그리드 포인트는 0과 1 사이입니다. alpha / alpha_max</target>
        </trans-unit>
        <trans-unit id="1e90c7186240eacb6693334f5a2cb603522a3c95" translate="yes" xml:space="preserve">
          <source>The grid search instance behaves like a normal &lt;code&gt;scikit-learn&lt;/code&gt; model. Let&amp;rsquo;s perform the search on a smaller subset of the training data to speed up the computation:</source>
          <target state="translated">그리드 검색 인스턴스는 일반적인 &lt;code&gt;scikit-learn&lt;/code&gt; 모델 처럼 동작 합니다. 계산 속도를 높이기 위해 훈련 데이터의 작은 하위 집합에서 검색을 수행해 보겠습니다.</target>
        </trans-unit>
        <trans-unit id="0fb72e7d32db09c24209afb30ebd0c3ef9469a12" translate="yes" xml:space="preserve">
          <source>The grid search provided by &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; exhaustively generates candidates from a grid of parameter values specified with the &lt;code&gt;param_grid&lt;/code&gt; parameter. For instance, the following &lt;code&gt;param_grid&lt;/code&gt;:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; 가&lt;/a&gt; 제공하는 그리드 검색 은 &lt;code&gt;param_grid&lt;/code&gt; 파라미터로 지정된 파라미터 값의 그리드에서 후보를 철저하게 생성합니다 . 예를 들어, 다음 &lt;code&gt;param_grid&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="5d877967ec11cd12214237a4561547e48123553d" translate="yes" xml:space="preserve">
          <source>The guidelines for choosing a metric is to use one that maximizes the distance between samples in different classes, and minimizes that within each class.</source>
          <target state="translated">메트릭 선택에 대한 지침은 서로 다른 클래스의 샘플 간 거리를 최대화하고 각 클래스 내에서이를 최소화하는 메트릭을 사용하는 것입니다.</target>
        </trans-unit>
        <trans-unit id="42b20fdbf19bf6fb28b9336fb1c931e7ebd9ff54" translate="yes" xml:space="preserve">
          <source>The handwritten digit dataset has 1797 total points. The model will be trained using all points, but only 30 will be labeled. Results in the form of a confusion matrix and a series of metrics over each class will be very good.</source>
          <target state="translated">손으로 쓴 숫자 데이터 집합에는 총 1797 개의 점이 있습니다. 모델은 모든 점을 사용하여 학습되지만 30 만 레이블이 지정됩니다. 혼동 행렬 형태와 각 클래스에 대한 일련의 메트릭 결과가 매우 좋습니다.</target>
        </trans-unit>
        <trans-unit id="1dbebc05eff701a160c5a2149017afcbb50998d6" translate="yes" xml:space="preserve">
          <source>The hash function employed is the signed 32-bit version of Murmurhash3.</source>
          <target state="translated">사용 된 해시 함수는 서명 된 32 비트 버전의 Murmurhash3입니다.</target>
        </trans-unit>
        <trans-unit id="6ddf3e17ce9800f43a67078b41067ddc0d8c6a4c" translate="yes" xml:space="preserve">
          <source>The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the simplex.</source>
          <target state="translated">농도가 높을수록 중앙에 더 많은 질량이 들어가고 더 많은 성분이 활성화되고 농도가 낮을수록 단면 가장자리에 더 많은 질량이 생깁니다.</target>
        </trans-unit>
        <trans-unit id="1006ebebebbef4245b7568132e1eabf566a79140" translate="yes" xml:space="preserve">
          <source>The highest p-value for features to be kept.</source>
          <target state="translated">유지할 피처의 가장 높은 p- 값입니다.</target>
        </trans-unit>
        <trans-unit id="0c1d204d0071e8e2a8101579dd644bade4aef0d9" translate="yes" xml:space="preserve">
          <source>The highest uncorrected p-value for features to keep.</source>
          <target state="translated">유지할 피쳐에 대해 수정되지 않은 가장 높은 p- 값입니다.</target>
        </trans-unit>
        <trans-unit id="3708900f6a079bc16cd76c37da5647812d1a0427" translate="yes" xml:space="preserve">
          <source>The histogram of the estimated weights is very peaked, as a sparsity-inducing prior is implied on the weights.</source>
          <target state="translated">희소성을 유발하는 이전의 가중치가 암시되기 때문에 추정 된 가중치의 히스토그램은 매우 정점에 이릅니다.</target>
        </trans-unit>
        <trans-unit id="d24bdc7c6b1e4dbeb474c6e9f76042366464386e" translate="yes" xml:space="preserve">
          <source>The hyperparameters</source>
          <target state="translated">하이퍼 파라미터</target>
        </trans-unit>
        <trans-unit id="25f29cead3cfba36338bcf026da5a846edf25636" translate="yes" xml:space="preserve">
          <source>The i-th score &lt;code&gt;train_score_[i]&lt;/code&gt; is the deviance (= loss) of the model at iteration &lt;code&gt;i&lt;/code&gt; on the in-bag sample. If &lt;code&gt;subsample == 1&lt;/code&gt; this is the deviance on the training data.</source>
          <target state="translated">i 번째 점수 &lt;code&gt;train_score_[i]&lt;/code&gt; 는 백 내 샘플 에서 반복 &lt;code&gt;i&lt;/code&gt; 에서 모델의 이탈 (= 손실)입니다 . &lt;code&gt;subsample == 1&lt;/code&gt; 경우 훈련 데이터에 대한 편차입니다.</target>
        </trans-unit>
        <trans-unit id="7768180d6d5c29f9be6eb9d80f99cbf93007e25a" translate="yes" xml:space="preserve">
          <source>The i.i.d. assumption is broken if the underlying generative process yield groups of dependent samples.</source>
          <target state="translated">기본 생성 프로세스가 종속 샘플 그룹을 생성하면 iid 가정이 깨집니다.</target>
        </trans-unit>
        <trans-unit id="190c7529dfd22ae7ed70d4f84d1deb499abbe749" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;code&gt;VotingClassifier&lt;/code&gt; is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.</source>
          <target state="translated">&lt;code&gt;VotingClassifier&lt;/code&gt; 의 기본 개념은 개념적으로 다른 머신 러닝 분류기를 결합하고 다 수표 또는 평균 예측 확률 (소프트 투표)을 사용하여 클래스 레이블을 예측하는 것입니다. 이러한 분류기는 개별 약점의 균형을 맞추기 위해 동일한 성능의 모델 세트에 유용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f4f9efa7d25a183372e2e9ce75bfe20581578033" translate="yes" xml:space="preserve">
          <source>The image as a numpy array: height x width x color</source>
          <target state="translated">numpy 배열로서의 이미지 : 높이 x 너비 x 색상</target>
        </trans-unit>
        <trans-unit id="77bae2f833c03e6f85b6a43b3aa3ead8f81ce2e5" translate="yes" xml:space="preserve">
          <source>The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms.</source>
          <target state="translated">이미지는 256 그레이 레벨로 양자화되고 부호없는 8 비트 정수로 저장됩니다. 로더는 간격 [0, 1]에서이 값을 부동 소수점 값으로 변환하므로 많은 알고리즘에서 작업하기가 더 쉽습니다.</target>
        </trans-unit>
        <trans-unit id="6db946ce103c288b47eadb4f1488a8fdc91a7488" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients. See &lt;a href=&quot;#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt; for another implementation:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt; 클래스의 구현 은 계수에 맞는 알고리즘으로 좌표 하강을 사용합니다. 다른 구현에 대해서는 &lt;a href=&quot;#least-angle-regression&quot;&gt;최소 각도 회귀&lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="64dabeaa3f8e38c30333615b1b174cbd1348b11e" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt;&lt;code&gt;MultiTaskElasticNet&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt; &lt;code&gt;MultiTaskElasticNet&lt;/code&gt; &lt;/a&gt; 클래스의 구현 에서는 계수에 맞는 알고리즘으로 좌표 하강을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="555e24352725a2133c35e7a52f56a702cd7f81c4" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt;&lt;code&gt;MultiTaskLasso&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt; &lt;code&gt;MultiTaskLasso&lt;/code&gt; &lt;/a&gt; 클래스의 구현 에서는 계수에 맞는 알고리즘으로 좌표 하강을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="f9cb17c43bed4736104925d68075ebd6b07ace68" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 2.1 of &lt;a href=&quot;#rw2006&quot; id=&quot;id1&quot;&gt;[RW2006]&lt;/a&gt;. In addition to the API of standard scikit-learn estimators, GaussianProcessRegressor:</source>
          <target state="translated">구현은 &lt;a href=&quot;#rw2006&quot; id=&quot;id1&quot;&gt;[RW2006]&lt;/a&gt; 의 알고리즘 2.1을 기반으로 합니다. 표준 scikit-learn 추정기의 API 외에도 GaussianProcessRegressor는 :</target>
        </trans-unit>
        <trans-unit id="70780cc3dac260d0fb4e85faed94f4134c9dc0ae" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 2.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.</source>
          <target state="translated">이 구현은 Rasmussen과 Williams의 GPML (Gaussian Processes for Machine Learning) 알고리즘 2.1을 기반으로합니다.</target>
        </trans-unit>
        <trans-unit id="1904d3648b9818da40270920e1855bd9dcd752d1" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.</source>
          <target state="translated">이 구현은 Rasmussen과 Williams의 GPML (Gaussian Processes for Machine Learning)의 알고리즘 3.1, 3.2 및 5.1을 기반으로합니다.</target>
        </trans-unit>
        <trans-unit id="166f55526ba60cbc129406f3899569f7b0eb4efd" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm.</source>
          <target state="translated">구현은 libsvm을 기반으로합니다.</target>
        </trans-unit>
        <trans-unit id="cb75b7ddb016087965f5fad7be27fc6c1cb16290" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.</source>
          <target state="translated">구현은 libsvm을 기반으로합니다. 적합 시간 복잡도는 샘플 수에 따라 2 차 이상이며 2 만 개가 넘는 샘플로 데이터 세트에 맞게 확장하기가 어렵습니다.</target>
        </trans-unit>
        <trans-unit id="08d84e0cf0c38a38100a9dad48e0efc744fd2db8" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; in scikit-learn follows a generalization to a multivariate linear regression model &lt;a href=&quot;#f1&quot; id=&quot;id30&quot;&gt;[8]&lt;/a&gt; using the spatial median which is a generalization of the median to multiple dimensions &lt;a href=&quot;#f2&quot; id=&quot;id31&quot;&gt;[9]&lt;/a&gt;.</source>
          <target state="translated">구현 &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; &lt;/a&gt; - 학습에 scikit은 다변량 선형 회귀 모델로 일반화 다음 &lt;a href=&quot;#f1&quot; id=&quot;id30&quot;&gt;[8]&lt;/a&gt; 다차원의 중앙값을 일반화하여 중간 공간 &lt;a href=&quot;#f2&quot; id=&quot;id31&quot;&gt;[9]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="eb185b2e148e3613cdde933dd05a6b32e3b9acbe" translate="yes" xml:space="preserve">
          <source>The implementation of SGD is influenced by the &lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot;&gt;Stochastic Gradient SVM&lt;/a&gt; of L&amp;eacute;on Bottou. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse feature vectors, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from Shalev-Shwartz et al. 2007. For multi-class classification, a &amp;ldquo;one versus all&amp;rdquo; approach is used. We use the truncated gradient algorithm proposed by Tsuruoka et al. 2009 for L1 regularization (and the Elastic Net). The code is written in Cython.</source>
          <target state="translated">SGD의 구현은 &lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot;&gt;Stochastic Gradient SVM의&lt;/a&gt; 영향을받습니다.L&amp;eacute;on Bottou의 SvmSGD와 유사하게 가중치 벡터는 스칼라와 L2 정규화의 경우 효율적인 가중치 업데이트를 허용하는 벡터의 곱으로 표시됩니다. 희소 특징 벡터의 경우, 인터셉트는 더 자주 업데이트된다는 사실을 설명하기 위해 더 작은 학습 속도 (0.0을 곱한)로 업데이트됩니다. 학습 예제는 순차적으로 선택되며 학습 된 각 예제 후에 학습 속도가 낮아집니다. 우리는 Shalev-Shwartz 등의 학습률 일정을 채택했습니다. 2007. 다중 등급 분류의 경우 &quot;1 대 전체&quot;접근 방식이 사용됩니다. 우리는 Tsuruoka et al. L1 정규화 (및 Elastic Net)의 경우 2009 년. 코드는 Cython으로 작성되었습니다.</target>
        </trans-unit>
        <trans-unit id="234c6abfe6fe107b5770fe372b7d9e14789fc069" translate="yes" xml:space="preserve">
          <source>The implementation of logistic regression in scikit-learn can be accessed from class &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;. This implementation can fit binary, One-vs- Rest, or multinomial logistic regression with optional L2 or L1 regularization.</source>
          <target state="translated">scikit-learn에서 로지스틱 회귀 구현은 &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; 클래스에서 액세스 할 수 있습니다 . 이 구현은 선택적인 L2 또는 L1 정규화를 사용하여 이진, 일대일 또는 다항 로지스틱 회귀에 적합 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="11a45f793137837fb140d6499ba2bfe32aacbae3" translate="yes" xml:space="preserve">
          <source>The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.</source>
          <target state="translated">지형지 물의 중요성은 해당 지형지 물이 가져온 기준의 (정규화 된) 총 축소로 계산됩니다. 지니 중요도라고도합니다.</target>
        </trans-unit>
        <trans-unit id="285ba1aeef83b4f72aaed81188dabeb94680bb69" translate="yes" xml:space="preserve">
          <source>The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. &lt;code&gt;oob_improvement_[0]&lt;/code&gt; is the improvement in loss of the first stage over the &lt;code&gt;init&lt;/code&gt; estimator.</source>
          <target state="translated">이전 반복에 비해 백 아웃 샘플의 손실 개선 (= 이탈). &lt;code&gt;oob_improvement_[0]&lt;/code&gt; 오버 첫번째 단계의 손실의 개선 &lt;code&gt;init&lt;/code&gt; 추정기.</target>
        </trans-unit>
        <trans-unit id="d60773790f2e7bcb156876ca46ea3f1191f69076" translate="yes" xml:space="preserve">
          <source>The impurity at \(m\) is computed using an impurity function \(H()\), the choice of which depends on the task being solved (classification or regression)</source>
          <target state="translated">\ (m \)의 불순물은 불순물 함수 \ (H () \)를 사용하여 계산되며, 선택하는 문제는 해결중인 작업 (분류 또는 회귀)에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="f89a5a9d5e4b0a0db4c9a50975418d448408f475" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature if axis == 0.</source>
          <target state="translated">축 == 0 인 경우 각 피처의 대치 채우기 값입니다.</target>
        </trans-unit>
        <trans-unit id="52ad716a60ba342ef84206f96283ef6240a59825" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature.</source>
          <target state="translated">각 기능에 대한 대치 채우기 값입니다.</target>
        </trans-unit>
        <trans-unit id="7cd5e57b66f65afed5b5b066bdba074e0cee3d73" translate="yes" xml:space="preserve">
          <source>The imputation strategy.</source>
          <target state="translated">대치 전략.</target>
        </trans-unit>
        <trans-unit id="aa1b27a8286b1b98805decd6657b8fa02347921e" translate="yes" xml:space="preserve">
          <source>The index (of the &lt;code&gt;cv_results_&lt;/code&gt; arrays) which corresponds to the best candidate parameter setting.</source>
          <target state="translated">최상의 후보 매개 변수 설정에 해당하는 인덱스 ( &lt;code&gt;cv_results_&lt;/code&gt; 배열의)입니다.</target>
        </trans-unit>
        <trans-unit id="d8f1885dbc976f2ceb3f7711b10b324e6546b97b" translate="yes" xml:space="preserve">
          <source>The index is computed only quantities and features inherent to the dataset.</source>
          <target state="translated">인덱스는 데이터 세트 고유의 수량 및 기능 만 계산됩니다.</target>
        </trans-unit>
        <trans-unit id="eaf4dd06369196819331db8b96a42731aed3d8e0" translate="yes" xml:space="preserve">
          <source>The index is defined as the average similarity between each cluster \(C_i\) for \(i=1, ..., k\) and its most similar one \(C_j\). In the context of this index, similarity is defined as a measure \(R_{ij}\) that trades off:</source>
          <target state="translated">인덱스는 \ (i = 1, ..., k \)에 대한 각 클러스터 \ (C_i \)와 가장 유사한 하나의 \ (C_j \) 사이의 평균 유사성으로 정의됩니다. 이 지수의 맥락에서 유사성은 다음과 같은 트레이드 \ (R_ {ij} \)로 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="f4af6c67193225ac6c1c304797e9ea3402785083" translate="yes" xml:space="preserve">
          <source>The index of the cluster.</source>
          <target state="translated">클러스터의 인덱스입니다.</target>
        </trans-unit>
        <trans-unit id="bf1ee02857437dc2bc3e741c8e3e56ef69d1ae10" translate="yes" xml:space="preserve">
          <source>The index value of a word in the vocabulary is linked to its frequency in the whole training corpus.</source>
          <target state="translated">어휘에서 단어의 색인 값은 전체 훈련 모음에서 빈도와 연결됩니다.</target>
        </trans-unit>
        <trans-unit id="03c2241f87945c783a2d50da20a997d6e73ac147" translate="yes" xml:space="preserve">
          <source>The indexes of the sorted training input samples. If many tree are grown on the same dataset, this allows the ordering to be cached between trees. If None, the data will be sorted here. Don&amp;rsquo;t use this parameter unless you know what to do.</source>
          <target state="translated">정렬 된 훈련 입력 샘플의 인덱스. 동일한 데이터 세트에서 많은 트리가 성장하면 트리간에 순서를 캐시 할 수 있습니다. None이면 여기에서 데이터가 정렬됩니다. 수행 할 작업을 모르면이 매개 변수를 사용하지 마십시오.</target>
        </trans-unit>
        <trans-unit id="0f615a1a241d635bbd7f8073a9962f88455d5840" translate="yes" xml:space="preserve">
          <source>The indicators for cluster membership of each column.</source>
          <target state="translated">각 열의 클러스터 멤버쉽 표시기.</target>
        </trans-unit>
        <trans-unit id="7c7c8e5dd3219610397a571246f91a523e9b0296" translate="yes" xml:space="preserve">
          <source>The indicators for cluster membership of each row.</source>
          <target state="translated">각 행의 클러스터 멤버쉽 표시기.</target>
        </trans-unit>
        <trans-unit id="1d09056572af0ab7d260b8a9a458cace43041c54" translate="yes" xml:space="preserve">
          <source>The inertia matrix uses a Heapq-based representation.</source>
          <target state="translated">관성 행렬은 힙 기반 표현을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="4f68a65921a2da6924b081b35cf28ef22fa8adaa" translate="yes" xml:space="preserve">
          <source>The inferred value of max_features.</source>
          <target state="translated">max_features의 유추 된 값입니다.</target>
        </trans-unit>
        <trans-unit id="9af964a5bdc6c7f6e3e23dbd4ccea9f871ad0d0c" translate="yes" xml:space="preserve">
          <source>The initial coefficients to warm-start the optimization.</source>
          <target state="translated">최적화를 웜 스타트하기위한 초기 계수입니다.</target>
        </trans-unit>
        <trans-unit id="e326d26a40d08d9ebf56e15d175f98f61ace6983" translate="yes" xml:space="preserve">
          <source>The initial guess for the covariance.</source>
          <target state="translated">공분산에 대한 초기 추측.</target>
        </trans-unit>
        <trans-unit id="a31ad0b6b83b24ae472799017e15e5984848ac41" translate="yes" xml:space="preserve">
          <source>The initial guess of the noise variance for each feature. If None, it defaults to np.ones(n_features)</source>
          <target state="translated">각 특징에 대한 잡음 분산의 초기 추측. None이면 기본값은 np.ones (n_features)입니다.</target>
        </trans-unit>
        <trans-unit id="5af1f38d3d578b440a3ef0c3e0d6f491e678143f" translate="yes" xml:space="preserve">
          <source>The initial intercept to warm-start the optimization.</source>
          <target state="translated">최적화를 웜 스타트하기위한 초기 인터셉트.</target>
        </trans-unit>
        <trans-unit id="ffb7aede167c9f36a232ef307d2df81471bb1b4f" translate="yes" xml:space="preserve">
          <source>The initial learning rate for the &amp;lsquo;constant&amp;rsquo;, &amp;lsquo;invscaling&amp;rsquo; or &amp;lsquo;adaptive&amp;rsquo; schedules. The default value is 0.0 as eta0 is not used by the default schedule &amp;lsquo;optimal&amp;rsquo;.</source>
          <target state="translated">'일정한', '진정한'또는 '적응적인'일정의 초기 학습 속도. 기본 일정 'optimal'에서 eta0을 사용하지 않으므로 기본값은 0.0입니다.</target>
        </trans-unit>
        <trans-unit id="f396af141edab02acfc3d8c05ea1f6e174dd0d5b" translate="yes" xml:space="preserve">
          <source>The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;.</source>
          <target state="translated">사용 된 초기 학습 속도. 가중치를 업데이트 할 때 단계 크기를 제어합니다. solver = 'sgd'또는 'adam'인 경우에만 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="8aa315cba5e51992abb63a7c33e92703aa91a364" translate="yes" xml:space="preserve">
          <source>The initial model \(F_{0}\) is problem specific, for least-squares regression one usually chooses the mean of the target values.</source>
          <target state="translated">초기 모델 \ (F_ {0} \)은 문제에 따라 다르므로 최소 제곱 회귀 분석에서는 일반적으로 목표 값의 평균을 선택합니다.</target>
        </trans-unit>
        <trans-unit id="fcbcdc2001232ebf2d1267dbd5c1db8c05bef33b" translate="yes" xml:space="preserve">
          <source>The initial model can also be specified via the &lt;code&gt;init&lt;/code&gt; argument. The passed object has to implement &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">초기 모델은 &lt;code&gt;init&lt;/code&gt; 인수 를 통해 지정할 수도 있습니다 . 전달 된 객체는 &lt;code&gt;fit&lt;/code&gt; 과 &lt;code&gt;predict&lt;/code&gt; 를 구현해야 합니다.</target>
        </trans-unit>
        <trans-unit id="17bd189eec62d21587058ccd88b444461d73119b" translate="yes" xml:space="preserve">
          <source>The initial values of the coefficients.</source>
          <target state="translated">계수의 초기 값</target>
        </trans-unit>
        <trans-unit id="32a92ee0ef2ffe7a34cba56e64f123bd9b2ca181" translate="yes" xml:space="preserve">
          <source>The input data consists of 28x28 pixel handwritten digits, leading to 784 features in the dataset. Therefore the first layer weight matrix have the shape (784, hidden_layer_sizes[0]). We can therefore visualize a single column of the weight matrix as a 28x28 pixel image.</source>
          <target state="translated">입력 데이터는 28x28 픽셀의 필기 숫자로 구성되어 데이터 세트에 784 개의 기능이 있습니다. 그러므로 제 1 레이어 가중치 행렬은 (784, hidden_layer_sizes [0]) 모양을 갖는다. 따라서 가중치 행렬의 단일 열을 28x28 픽셀 이미지로 시각화 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c8ff70c87eb4edb0d6df0b02362dd0f826eb86c0" translate="yes" xml:space="preserve">
          <source>The input data matrix</source>
          <target state="translated">입력 데이터 매트릭스</target>
        </trans-unit>
        <trans-unit id="28402823f8037a445e34f883189d18c9c8586801" translate="yes" xml:space="preserve">
          <source>The input data to complete.</source>
          <target state="translated">완료 할 입력 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="10ea3f8f2392cebb3ea568adacc2ca87c3d24f21" translate="yes" xml:space="preserve">
          <source>The input data to project into a smaller dimensional space.</source>
          <target state="translated">더 작은 차원 공간으로 투사 할 입력 데이터입니다.</target>
        </trans-unit>
        <trans-unit id="8b941f334dfce6c774093743936f754bf8137c6b" translate="yes" xml:space="preserve">
          <source>The input data.</source>
          <target state="translated">입력 데이터</target>
        </trans-unit>
        <trans-unit id="a383a11075a72cc4c498a6da492b12429e0d8bd0" translate="yes" xml:space="preserve">
          <source>The input matrix \(A\) is first normalized to make the checkerboard pattern more obvious. There are three possible methods:</source>
          <target state="translated">바둑판 패턴을보다 명확하게하기 위해 입력 행렬 \ (A \)가 먼저 정규화됩니다. 세 가지 가능한 방법이 있습니다.</target>
        </trans-unit>
        <trans-unit id="5296b71b9c59f1ea88d1300f56d80b11acdd4263" translate="yes" xml:space="preserve">
          <source>The input matrix \(A\) is preprocessed as follows:</source>
          <target state="translated">입력 행렬 \ (A \)는 다음과 같이 전처리됩니다.</target>
        </trans-unit>
        <trans-unit id="1dfee05c320291671e1507b730cbc56e48fcc05b" translate="yes" xml:space="preserve">
          <source>The input samples with only the selected features.</source>
          <target state="translated">선택된 기능 만있는 입력 샘플.</target>
        </trans-unit>
        <trans-unit id="aa9c51d1052bb07178d99dc6b9998838f3fd04f4" translate="yes" xml:space="preserve">
          <source>The input samples.</source>
          <target state="translated">입력 샘플.</target>
        </trans-unit>
        <trans-unit id="691f5d8dd518149d763a76be65224ad6a47a3de2" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, it will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt; and if a sparse matrix is provided to a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">입력 샘플. 내부적으로 는 희소 행렬이 희소 &lt;code&gt;csr_matrix&lt;/code&gt; 에 제공되는 경우 &lt;code&gt;dtype=np.float32&lt;/code&gt; 로 변환됩니다 .</target>
        </trans-unit>
        <trans-unit id="80a0f34908e0a755318c3eb528444068386165b2" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">입력 샘플. 내부적으로 dtype은 &lt;code&gt;dtype=np.float32&lt;/code&gt; 로 변환됩니다 . 희소 행렬이 제공되면 희소 &lt;code&gt;csr_matrix&lt;/code&gt; 로 변환됩니다 .</target>
        </trans-unit>
        <trans-unit id="ad241eeecc4b3d71885329d69a3ecbb931dd0903" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted to a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">입력 샘플. 내부적으로 dtype은 &lt;code&gt;dtype=np.float32&lt;/code&gt; 로 변환됩니다 . 희소 행렬이 제공되면 희소 &lt;code&gt;csr_matrix&lt;/code&gt; 로 변환됩니다 .</target>
        </trans-unit>
        <trans-unit id="b99e327cb34f5c84fcbd4e91cfb496e38c8c302f" translate="yes" xml:space="preserve">
          <source>The input samples. Use &lt;code&gt;dtype=np.float32&lt;/code&gt; for maximum efficiency. Sparse matrices are also supported, use sparse &lt;code&gt;csc_matrix&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">입력 샘플. 효율성을 극대화 하려면 &lt;code&gt;dtype=np.float32&lt;/code&gt; 를 사용하십시오 . 희소 행렬도 지원됩니다 . 최대 효율성을 위해 희소 &lt;code&gt;csc_matrix&lt;/code&gt; 를 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="6e6a9dea87222450502841b10c214ed0343cd360" translate="yes" xml:space="preserve">
          <source>The input set can either be well conditioned (by default) or have a low rank-fat tail singular profile. See &lt;a href=&quot;sklearn.datasets.make_low_rank_matrix#sklearn.datasets.make_low_rank_matrix&quot;&gt;&lt;code&gt;make_low_rank_matrix&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">입력 세트는 잘 조절되거나 (기본적으로) 저지방 꼬리 단일 프로파일을 가질 수 있습니다. 자세한 내용은 &lt;a href=&quot;sklearn.datasets.make_low_rank_matrix#sklearn.datasets.make_low_rank_matrix&quot;&gt; &lt;code&gt;make_low_rank_matrix&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="289f5749e232fcbed1540450cf860647da777cb8" translate="yes" xml:space="preserve">
          <source>The input set is well conditioned, centered and gaussian with unit variance.</source>
          <target state="translated">입력 세트는 잘 조정되고 중앙에 있으며 단위 편차가있는 가우스입니다.</target>
        </trans-unit>
        <trans-unit id="9196dd161fc035e27746b1e485ce67d2bc4a2f97" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are converted to ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature.</source>
          <target state="translated">이 변환기의 입력은 범주 형 (이산) 기능에 의해 취해진 값을 나타내는 정수 또는 문자열의 배열과 유사해야합니다. 피처는 서수로 변환됩니다. 이로 인해 기능 당 단일 열의 정수 (0-n_categories-1)가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="6fca358f78b22e3193c17b331b39a22e0cf2c917" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka &amp;lsquo;one-of-K&amp;rsquo; or &amp;lsquo;dummy&amp;rsquo;) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array.</source>
          <target state="translated">이 변환기의 입력은 범주 형 (이산) 기능에 의해 취해진 값을 나타내는 정수 또는 문자열의 배열과 유사해야합니다. 기능은 one-hot (일명 'K'또는 '더미') 인코딩 체계를 사용하여 인코딩됩니다. 그러면 각 범주에 대한 이진 열이 생성되고 희소 행렬 또는 밀도 배열이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="d6846270bbc73ec820141552eb32b90be4589180" translate="yes" xml:space="preserve">
          <source>The integer id or the string name metadata of the MLComp dataset to load</source>
          <target state="translated">로드 할 MLComp 데이터 세트의 정수 ID 또는 문자열 이름 메타 데이터</target>
        </trans-unit>
        <trans-unit id="0ff5072970d94a45dfd0b3dc59cc200193becdfd" translate="yes" xml:space="preserve">
          <source>The integer labels (0 or 1) for class membership of each sample.</source>
          <target state="translated">각 샘플의 클래스 멤버쉽에 대한 정수 레이블 (0 또는 1)입니다.</target>
        </trans-unit>
        <trans-unit id="a6afcce2561a80c34c914f95ed66620399eb0d7c" translate="yes" xml:space="preserve">
          <source>The integer labels for class membership of each sample.</source>
          <target state="translated">각 샘플의 클래스 멤버쉽에 대한 정수 레이블입니다.</target>
        </trans-unit>
        <trans-unit id="868af1be557e4830c6f25b4ff3211b7a9aad7eb0" translate="yes" xml:space="preserve">
          <source>The integer labels for cluster membership of each sample.</source>
          <target state="translated">각 샘플의 클러스터 멤버쉽에 대한 정수 레이블.</target>
        </trans-unit>
        <trans-unit id="05d4e37f01ed47119c132248581df1196214fb7d" translate="yes" xml:space="preserve">
          <source>The integer labels for quantile membership of each sample.</source>
          <target state="translated">각 샘플의 Quantile 멤버쉽에 대한 정수 레이블입니다.</target>
        </trans-unit>
        <trans-unit id="7da460eaa0239b5647e6b97a087bcede08e5a0c5" translate="yes" xml:space="preserve">
          <source>The intercept of the model. Only returned if &lt;code&gt;return_intercept&lt;/code&gt; is True and if X is a scipy sparse array.</source>
          <target state="translated">모델의 절편입니다. &lt;code&gt;return_intercept&lt;/code&gt; 가 True이고 X가 scipy 스파 스 배열 인 경우에만 반환됩니다 .</target>
        </trans-unit>
        <trans-unit id="f0408bacce1626a183a8eadd09dc2d6f8b9e549c" translate="yes" xml:space="preserve">
          <source>The intercept term.</source>
          <target state="translated">절편 용어.</target>
        </trans-unit>
        <trans-unit id="39d99a92784fd6547680c69e2a029d63ab730943" translate="yes" xml:space="preserve">
          <source>The inverse document frequency (IDF) vector; only defined if &lt;code&gt;use_idf&lt;/code&gt; is True.</source>
          <target state="translated">역 문서 빈도 (IDF) 벡터; &lt;code&gt;use_idf&lt;/code&gt; 가 True 인 경우에만 정의됩니다 .</target>
        </trans-unit>
        <trans-unit id="70e42545b4f301fb133ffa67ef01dbd1c81a14a6" translate="yes" xml:space="preserve">
          <source>The inverse of the Box-Cox transformation is given by:</source>
          <target state="translated">Box-Cox 변환의 역수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="1296b0fc0246edf109c97645c02261b1d3d028bb" translate="yes" xml:space="preserve">
          <source>The inverse of the Yeo-Johnson transformation is given by:</source>
          <target state="translated">Yeo-Johnson 변환의 역수는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="8621ba40f721d68ec6f12f67b5e3cc856e6fc6d9" translate="yes" xml:space="preserve">
          <source>The iris dataset is a classic and very easy multi-class classification dataset.</source>
          <target state="translated">아이리스 데이터 셋은 클래식하고 매우 쉬운 멀티 클래스 분류 데이터 셋입니다.</target>
        </trans-unit>
        <trans-unit id="82f3a3d98edcec3969a3e26fc0789c7c6c1f74ef" translate="yes" xml:space="preserve">
          <source>The iris dataset is a classification task consisting in identifying 3 different types of irises (Setosa, Versicolour, and Virginica) from their petal and sepal length and width:</source>
          <target state="translated">홍채 데이터 세트는 꽃잎과 꽃받침 길이 및 너비에서 3 가지 종류의 홍채 (Setosa, Versicolour 및 Virginica)를 식별하는 분류 작업입니다.</target>
        </trans-unit>
        <trans-unit id="968fa226a66bed28f98c84df2ac306ac992e59c2" translate="yes" xml:space="preserve">
          <source>The isotonic regression optimization problem is defined by:</source>
          <target state="translated">등장 성 회귀 최적화 문제는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="710a9703f51d61fb5f41b8c77a926ca070febd6c" translate="yes" xml:space="preserve">
          <source>The iteration will stop when &lt;code&gt;max{|proj g_i | i = 1, ..., n}&lt;/code&gt; &amp;lt;= &lt;code&gt;tol&lt;/code&gt; where pg_i is the i-th component of the projected gradient.</source>
          <target state="translated">반복이 중지됩니다 때 &lt;code&gt;max{|proj g_i | i = 1, ..., n}&lt;/code&gt; &amp;lt;= &lt;code&gt;tol&lt;/code&gt; 여기서 pg_i는 투영 된 기울기의 i 번째 성분입니다.</target>
        </trans-unit>
        <trans-unit id="d1f978f02193a9d7a16e2ac4f28051f1be20fa33" translate="yes" xml:space="preserve">
          <source>The iterator consumption and dispatching is protected by the same lock so calling this function should be thread safe.</source>
          <target state="translated">반복자 소비 및 디스패치는 동일한 잠금으로 보호되므로이 함수를 호출하면 스레드로부터 안전해야합니다.</target>
        </trans-unit>
        <trans-unit id="e6bcf49b626a9dedb0663face3bd765b3dedb378" translate="yes" xml:space="preserve">
          <source>The ith element in the list represents the bias vector corresponding to layer i + 1.</source>
          <target state="translated">목록의 i 번째 요소는 레이어 i + 1에 해당하는 바이어스 벡터를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="4b396b269a89e1a7151872f147950b58bd31d2c2" translate="yes" xml:space="preserve">
          <source>The ith element in the list represents the weight matrix corresponding to layer i.</source>
          <target state="translated">목록의 i 번째 요소는 레이어 i에 해당하는 가중치 행렬을 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="8ce9197f7d2eba2389cfe44c6d9806383e84232a" translate="yes" xml:space="preserve">
          <source>The ith element represents the number of neurons in the ith hidden layer.</source>
          <target state="translated">i 번째 요소는 i 번째 숨겨진 레이어의 뉴런 수를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="110dde91ce5735296cfcdbdf4849eb2634f008ea" translate="yes" xml:space="preserve">
          <source>The justification for the formula in the loss=&amp;rdquo;modified_huber&amp;rdquo; case is in the appendix B in: &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&lt;/a&gt;</source>
          <target state="translated">loss =&amp;rdquo;modified_huber&amp;rdquo;사례의 공식에 대한 정당성은 부록 B ( &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf)에 있습니다.&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2ad3a7b0acdb773b3a5a78e7f86e7401f887f5a4" translate="yes" xml:space="preserve">
          <source>The k-means algorithm divides a set of \(N\) samples \(X\) into \(K\) disjoint clusters \(C\), each described by the mean \(\mu_j\) of the samples in the cluster. The means are commonly called the cluster &amp;ldquo;centroids&amp;rdquo;; note that they are not, in general, points from \(X\), although they live in the same space. The K-means algorithm aims to choose centroids that minimise the &lt;em&gt;inertia&lt;/em&gt;, or within-cluster sum of squared criterion:</source>
          <target state="translated">k- 평균 알고리즘은 일련의 \ (N \) 샘플 \ (X \)를 \ (K \) 분리 된 클러스터 \ (C \)로 나눕니다. 클러스터. 이 수단을 일반적으로 클러스터 &quot;중심점&quot;이라고합니다. 비록 같은 공간에 살고 있지만 일반적으로 \ (X \)를 가리키는 것은 아닙니다. K- 평균 알고리즘은 &lt;em&gt;관성&lt;/em&gt; 또는 클러스터 내 제곱 기준의 합 을 최소화하는 중심을 선택하는 것을 목표로합니다 .</target>
        </trans-unit>
        <trans-unit id="3042a74be3a11ce6ced2d21d393c8c55a0b044ff" translate="yes" xml:space="preserve">
          <source>The k-means problem is solved using either Lloyd&amp;rsquo;s or Elkan&amp;rsquo;s algorithm.</source>
          <target state="translated">k- 평균 문제는 Lloyd 또는 Elkan 알고리즘을 사용하여 해결됩니다.</target>
        </trans-unit>
        <trans-unit id="698fe2144ac8be0b801483c0a1d13dc4d07c73ce" translate="yes" xml:space="preserve">
          <source>The kappa score (see docstring) is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).</source>
          <target state="translated">카파 점수 (docstring 참조)는 -1과 1 사이의 숫자입니다. 0.8보다 높은 점수는 일반적으로 좋은 일치로 간주됩니다. 0 이하는 동의가 없음을 의미합니다 (실제로 임의의 레이블).</target>
        </trans-unit>
        <trans-unit id="951d69c60ea24190bd0c5a4dcae9aeb94c1c913d" translate="yes" xml:space="preserve">
          <source>The kappa statistic, which is a number between -1 and 1. The maximum value means complete agreement; zero or lower means chance agreement.</source>
          <target state="translated">카파 통계량 -1과 1 사이의 숫자입니다. 최대 값은 완전한 동의를 의미합니다. 0 이하는 기회 동의를 의미합니다.</target>
        </trans-unit>
        <trans-unit id="e7d87035112e23c601a476b538c50df786049966" translate="yes" xml:space="preserve">
          <source>The kernel density estimator can be used with any of the valid distance metrics (see &lt;a href=&quot;generated/sklearn.neighbors.distancemetric#sklearn.neighbors.DistanceMetric&quot;&gt;&lt;code&gt;sklearn.neighbors.DistanceMetric&lt;/code&gt;&lt;/a&gt; for a list of available metrics), though the results are properly normalized only for the Euclidean metric. One particularly useful metric is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;Haversine distance&lt;/a&gt; which measures the angular distance between points on a sphere. Here is an example of using a kernel density estimate for a visualization of geospatial data, in this case the distribution of observations of two different species on the South American continent:</source>
          <target state="translated">커널 밀도 추정기는 유효한 거리 메트릭 ( 사용 가능한 메트릭 목록 은 &lt;a href=&quot;generated/sklearn.neighbors.distancemetric#sklearn.neighbors.DistanceMetric&quot;&gt; &lt;code&gt;sklearn.neighbors.DistanceMetric&lt;/code&gt; &lt;/a&gt; 참조 )과 함께 사용할 수 있지만 결과는 유클리드 메트릭에 대해서만 올바르게 정규화됩니다. 특히 유용한 지표 중 하나 는 구의 점 사이의 각도 거리를 측정 하는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;Haversine 거리&lt;/a&gt; 입니다. 다음은 지리 공간 데이터의 시각화를 위해 커널 밀도 추정값을 사용하는 예입니다.이 경우 남미 대륙에서 서로 다른 두 종의 관측치 분포가 있습니다.</target>
        </trans-unit>
        <trans-unit id="f5acb5ee70e93822d6214e7faa0671c1edbb5e04" translate="yes" xml:space="preserve">
          <source>The kernel is composed of several terms that are responsible for explaining different properties of the signal:</source>
          <target state="translated">커널은 신호의 다른 속성을 설명하는 여러 용어로 구성됩니다.</target>
        </trans-unit>
        <trans-unit id="b2a264496619ccec6a15428322dff95c6847afef" translate="yes" xml:space="preserve">
          <source>The kernel specifying the covariance function of the GP. If None is passed, the kernel &amp;ldquo;1.0 * RBF(1.0)&amp;rdquo; is used as default. Note that the kernel&amp;rsquo;s hyperparameters are optimized during fitting.</source>
          <target state="translated">GP의 공분산 함수를 지정하는 커널. None이 전달되면 커널&amp;ldquo;1.0 * RBF (1.0)&amp;rdquo;이 기본값으로 사용됩니다. 커널의 하이퍼 파라미터는 피팅하는 동안 최적화됩니다.</target>
        </trans-unit>
        <trans-unit id="3f4b36434e95db38ce416f2948e370303e8d7ce4" translate="yes" xml:space="preserve">
          <source>The kernel to use. Valid kernels are [&amp;lsquo;gaussian&amp;rsquo;|&amp;rsquo;tophat&amp;rsquo;|&amp;rsquo;epanechnikov&amp;rsquo;|&amp;rsquo;exponential&amp;rsquo;|&amp;rsquo;linear&amp;rsquo;|&amp;rsquo;cosine&amp;rsquo;] Default is &amp;lsquo;gaussian&amp;rsquo;.</source>
          <target state="translated">사용할 커널. 유효한 커널은 [ 'gaussian'| 'tophat'| 'epanechnikov'| 'exponential'| 'linear'| 'cosine']입니다. 기본값은 'gaussian'입니다.</target>
        </trans-unit>
        <trans-unit id="7d7a3328bfc6be5d4c52eb80a02db6810f6af97a" translate="yes" xml:space="preserve">
          <source>The kernel used for prediction. In case of binary classification, the structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters. In case of multi-class classification, a CompoundKernel is returned which consists of the different kernels used in the one-versus-rest classifiers.</source>
          <target state="translated">예측에 사용 된 커널. 이진 분류의 경우 커널 구조는 매개 변수로 전달 된 것과 동일하지만 최적화 된 하이퍼 파라미터가 있습니다. 멀티 클래스 분류의 경우 one-restus-rest 분류기에서 사용되는 다른 커널로 구성된 CompoundKernel이 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="5a1810e4f4370ea9d4b3eb4b69c5661264f9680f" translate="yes" xml:space="preserve">
          <source>The kernel used for prediction. The structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters</source>
          <target state="translated">예측에 사용 된 커널. 커널의 구조는 매개 변수로 전달 된 것과 동일하지만 최적화 된 하이퍼 파라미터가 있습니다</target>
        </trans-unit>
        <trans-unit id="317ceb705f764af792a5c1b02b0c75da4f7767d2" translate="yes" xml:space="preserve">
          <source>The key &lt;code&gt;'params'&lt;/code&gt; is used to store a list of parameter settings dicts for all the parameter candidates.</source>
          <target state="translated">&lt;code&gt;'params'&lt;/code&gt; 키 는 모든 매개 변수 후보에 대한 매개 변수 설정 목록을 저장하는 데 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="467dfd2cbee563f1dc16d74e0763e99176db4d68" translate="yes" xml:space="preserve">
          <source>The l1-penalized estimator can recover part of this off-diagonal structure. It learns a sparse precision. It is not able to recover the exact sparsity pattern: it detects too many non-zero coefficients. However, the highest non-zero coefficients of the l1 estimated correspond to the non-zero coefficients in the ground truth. Finally, the coefficients of the l1 precision estimate are biased toward zero: because of the penalty, they are all smaller than the corresponding ground truth value, as can be seen on the figure.</source>
          <target state="translated">l1 처벌 추정기는이 비 대각선 구조의 일부를 복구 할 수 있습니다. 희소 정밀도를 배웁니다. 정확한 희소성 패턴을 복구 할 수 없습니다. 너무 많은 0이 아닌 계수를 감지합니다. 그러나, 추정 된 l1의 최고 비제로 계수는 지상 진실의 비제로 계수에 대응한다. 마지막으로, l1 정밀 추정의 계수는 0으로 편향됩니다. 페널티로 인해 그림에서 볼 수 있듯이 모두 해당지면 정확도 값보다 작습니다.</target>
        </trans-unit>
        <trans-unit id="3f115dd7f6ab226f3d1efb2261d982c44eb021f0" translate="yes" xml:space="preserve">
          <source>The label of the positive class</source>
          <target state="translated">포지티브 클래스의 레이블</target>
        </trans-unit>
        <trans-unit id="1daeeb6b0700e2caf583964dcec09c381eeb8ea9" translate="yes" xml:space="preserve">
          <source>The label of the positive class. Only applied to binary &lt;code&gt;y_true&lt;/code&gt;. For multilabel-indicator &lt;code&gt;y_true&lt;/code&gt;, &lt;code&gt;pos_label&lt;/code&gt; is fixed to 1.</source>
          <target state="translated">포지티브 클래스의 레이블입니다. 이진 &lt;code&gt;y_true&lt;/code&gt; 에만 적용됩니다 . 다중 레이블 표시기 &lt;code&gt;y_true&lt;/code&gt; 의 경우 &lt;code&gt;pos_label&lt;/code&gt; 은 1로 고정됩니다.</target>
        </trans-unit>
        <trans-unit id="844ad03ed5a5991044f5c2a9015a7d46ffc83176" translate="yes" xml:space="preserve">
          <source>The label sets.</source>
          <target state="translated">라벨이 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="34e6595f06fcaf1ac9d996e42fb91cf1dc35ee40" translate="yes" xml:space="preserve">
          <source>The labels of the clusters.</source>
          <target state="translated">클러스터의 레이블.</target>
        </trans-unit>
        <trans-unit id="f920ebdbb736d4fac6e624f1f0a24fae7f686c27" translate="yes" xml:space="preserve">
          <source>The laplacian kernel is defined as:</source>
          <target state="translated">라플라시안 커널은 다음과 같이 정의됩니다 :</target>
        </trans-unit>
        <trans-unit id="321fd6d2804d3c8ea8df389dda895240e812e24c" translate="yes" xml:space="preserve">
          <source>The lasso estimate thus solves the minimization of the least-squares penalty with \(\alpha ||w||_1\) added, where \(\alpha\) is a constant and \(||w||_1\) is the \(\ell_1\)-norm of the parameter vector.</source>
          <target state="translated">따라서 올가미 추정은 \ (\ alpha || w || _1 \)가 추가 된 최소 제곱 페널티의 최소화를 해결합니다. 여기서 \ (\ alpha \)는 상수이고 \ (|| w || _1 \)는 모수 벡터의 \ (\ ell_1 \)-norm</target>
        </trans-unit>
        <trans-unit id="374010cdf4f6f26c62c956d36c36442b1f12b646" translate="yes" xml:space="preserve">
          <source>The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser.</source>
          <target state="translated">마지막 특징은 Perceptron이 힌지 손실로 SGD보다 약간 훈련 속도가 빠르며 결과 모델이 희소하다는 것을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="6312a138a2adc5ee223c24bc72b6c9e5099fd7ea" translate="yes" xml:space="preserve">
          <source>The last dataset is an example of a &amp;lsquo;null&amp;rsquo; situation for clustering: the data is homogeneous, and there is no good clustering. For this example, the null dataset uses the same parameters as the dataset in the row above it, which represents a mismatch in the parameter values and the data structure.</source>
          <target state="translated">마지막 데이터 세트는 클러스터링에 대한 '널 (null)'상황의 예입니다. 데이터가 균등하며 클러스터링이 양호하지 않습니다. 이 예에서 null 데이터 집합은 위의 행에있는 데이터 집합과 동일한 매개 변수를 사용합니다. 이는 매개 변수 값과 데이터 구조의 불일치를 나타냅니다.</target>
        </trans-unit>
        <trans-unit id="2cc9bc385685f028c8287b17d0d958b5050b56a0" translate="yes" xml:space="preserve">
          <source>The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold. This ensures that the graph starts on the y axis.</source>
          <target state="translated">마지막 정밀도 및 리콜 값은 각각 1과 0이며 해당 임계 값이 없습니다. 그래야 y 축에서 그래프가 시작됩니다.</target>
        </trans-unit>
        <trans-unit id="9ddff80583e9f2c8a48f3b64e4a9a06a8a8cefcd" translate="yes" xml:space="preserve">
          <source>The last two panels show how we can sample from the last two models. The resulting samples distributions do not look exactly like the original data distribution. The difference primarily stems from the approximation error we made by using a model that assumes that the data was generated by a finite number of Gaussian components instead of a continuous noisy sine curve.</source>
          <target state="translated">마지막 두 패널은 마지막 두 모델에서 샘플링하는 방법을 보여줍니다. 결과 표본 분포는 원래 데이터 분포와 정확히 같지 않습니다. 그 차이는 주로 연속적인 잡음이있는 사인 곡선 대신 유한 한 수의 가우스 성분에 의해 생성되었다고 가정하는 모델을 사용하여 만든 근사 오차에서 비롯됩니다.</target>
        </trans-unit>
        <trans-unit id="a4514c8e853c44e1e4a78bf33662f9b32277ba44" translate="yes" xml:space="preserve">
          <source>The latent variables of X.</source>
          <target state="translated">X의 잠재 변수</target>
        </trans-unit>
        <trans-unit id="03a214378b3ef2e13a3cc6617d05b3fe221146c8" translate="yes" xml:space="preserve">
          <source>The learning rate \(\eta\) can be either constant or gradually decaying. For classification, the default learning rate schedule (&lt;code&gt;learning_rate='optimal'&lt;/code&gt;) is given by</source>
          <target state="translated">학습 속도 \ (\ eta \)는 일정하거나 점진적으로 감소 할 수 있습니다. 분류의 경우 기본 학습 속도 일정 ( &lt;code&gt;learning_rate='optimal'&lt;/code&gt; )은</target>
        </trans-unit>
        <trans-unit id="36c5b5659a911c2db600cd9fbf9c0485b5053e95" translate="yes" xml:space="preserve">
          <source>The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a &amp;lsquo;ball&amp;rsquo; with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help.</source>
          <target state="translated">t-SNE의 학습 속도는 일반적으로 [10.0, 1000.0] 범위입니다. 학습률이 너무 높으면 데이터가 가장 가까운 이웃과 거의 같은 거리에있는 '볼'처럼 보일 수 있습니다. 학습 속도가 너무 낮 으면 대부분의 점수가 특이 치가 적은 짙은 구름에서 압축 된 것처럼 보일 수 있습니다. 비용 함수가 로컬 최소값에 못 미치는 경우 학습률을 높이는 것이 도움이 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="55a4138aa9f2827acbf6f24a7d7d78c5c9231271" translate="yes" xml:space="preserve">
          <source>The learning rate for weight updates. It is &lt;em&gt;highly&lt;/em&gt; recommended to tune this hyper-parameter. Reasonable values are in the 10**[0., -3.] range.</source>
          <target state="translated">체중 업데이트에 대한 학습률. 그것은되어 &lt;em&gt;높은&lt;/em&gt; 이 하이퍼 매개 변수를 조정하는 것이 좋습니다. 합리적인 값은 10 ** [0., -3.] 범위에 있습니다.</target>
        </trans-unit>
        <trans-unit id="aa4707838d034cc31029160760df726f17933ef5" translate="yes" xml:space="preserve">
          <source>The learning rate schedule:</source>
          <target state="translated">학습 속도 일정 :</target>
        </trans-unit>
        <trans-unit id="866660dd294846977c41d9eb05a5f823762d12dd" translate="yes" xml:space="preserve">
          <source>The left and right examples highlight the &lt;code&gt;n_labels&lt;/code&gt; parameter: more of the samples in the right plot have 2 or 3 labels.</source>
          <target state="translated">왼쪽과 오른쪽의 예는 &lt;code&gt;n_labels&lt;/code&gt; 매개 변수를 강조 표시합니다 . 오른쪽 그림의 많은 샘플에는 2 개 또는 3 개의 레이블이 있습니다.</target>
        </trans-unit>
        <trans-unit id="3656a1b3513126216409c003f2c19b4a1bde366e" translate="yes" xml:space="preserve">
          <source>The leftmost layer, known as the input layer, consists of a set of neurons \(\{x_i | x_1, x_2, ..., x_m\}\) representing the input features. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation \(w_1x_1 + w_2x_2 + ... + w_mx_m\), followed by a non-linear activation function \(g(\cdot):R \rightarrow R\) - like the hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values.</source>
          <target state="translated">입력 레이어라고하는 가장 왼쪽 레이어는 입력 기능을 나타내는 뉴런 \ (\ {x_i | x_1, x_2, ..., x_m \} \)으로 구성됩니다. 숨겨진 레이어의 각 뉴런은 가중 선형 합계 \ (w_1x_1 + w_2x_2 + ... + w_mx_m \)와 비선형 활성화 함수 \ (g (\ cdot) : R \로 이전 레이어의 값을 변환합니다. 오른쪽 화살표 R \)-쌍곡선 tan 함수와 유사합니다. 출력 레이어는 마지막 숨겨진 레이어에서 값을 받아 출력 값으로 변환합니다.</target>
        </trans-unit>
        <trans-unit id="7b2e6a226728025cdbf60737ae02746b4f5067e4" translate="yes" xml:space="preserve">
          <source>The length scale of the kernel.</source>
          <target state="translated">커널의 길이 스케일.</target>
        </trans-unit>
        <trans-unit id="658641ffb44e40c4155905b8a662123f9fbe36a4" translate="yes" xml:space="preserve">
          <source>The length scale of the kernel. If a float, an isotropic kernel is used. If an array, an anisotropic kernel is used where each dimension of l defines the length-scale of the respective feature dimension.</source>
          <target state="translated">커널의 길이 스케일. 플로트 인 경우 등방성 커널이 사용됩니다. 어레이 인 경우, 이방성 커널이 사용되며, 여기서 각각의 치수 l은 각각의 피쳐 치수의 길이 스케일을 정의합니다.</target>
        </trans-unit>
        <trans-unit id="c8dac18109c7577d20154429eb0e5e79a703a997" translate="yes" xml:space="preserve">
          <source>The likelihood of the data set with &lt;code&gt;self.covariance_&lt;/code&gt; as an estimator of its covariance matrix.</source>
          <target state="translated">공분산 행렬의 추정값 으로 &lt;code&gt;self.covariance_&lt;/code&gt; 를 사용한 데이터 세트의 가능성 .</target>
        </trans-unit>
        <trans-unit id="9d0f580fc795d85a96fff8300e9ac1a9bc3bbb0b" translate="yes" xml:space="preserve">
          <source>The linear model trained on polynomial features is able to exactly recover the input polynomial coefficients.</source>
          <target state="translated">다항식 피처에 대해 훈련 된 선형 모델은 입력 다항식 계수를 정확하게 복구 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9cf1fb22576b6c52a81ed3fb3cd57a1b7c534cc0" translate="yes" xml:space="preserve">
          <source>The linear models &lt;code&gt;LinearSVC()&lt;/code&gt; and &lt;code&gt;SVC(kernel='linear')&lt;/code&gt; yield slightly different decision boundaries. This can be a consequence of the following differences:</source>
          <target state="translated">선형 모델 &lt;code&gt;LinearSVC()&lt;/code&gt; 와 &lt;code&gt;SVC(kernel='linear')&lt;/code&gt; 약간 다른 결정 경계를 생성합니다. 이는 다음과 같은 차이점의 결과 일 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="02b9c4dcf92e3aac8d7274c2bb8e453c5501153c" translate="yes" xml:space="preserve">
          <source>The list of calibrated classifiers, one for each crossvalidation fold, which has been fitted on all but the validation fold and calibrated on the validation fold.</source>
          <target state="translated">교정 교차점마다 하나씩 교정 분류기의 목록으로, 검증 영역을 제외한 모든 영역에 적합하고 검증 영역에서 교정됩니다.</target>
        </trans-unit>
        <trans-unit id="662dd2cdb21b969ce9a44b42150a945acbfed523" translate="yes" xml:space="preserve">
          <source>The list of values of the objective function and the dual gap at each iteration. Returned only if return_costs is True.</source>
          <target state="translated">목적 함수의 값 목록과 각 반복에서 이중 간격입니다. return_costs가 True 인 경우에만 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="177835998b74cd6b15a39a13b3bf0448af9a1ccf" translate="yes" xml:space="preserve">
          <source>The local outlier factor (LOF) of a sample captures its supposed &amp;lsquo;degree of abnormality&amp;rsquo;. It is the average of the ratio of the local reachability density of a sample and those of its k-nearest neighbors.</source>
          <target state="translated">표본의 국소 이상치 (LOF)는 추정 된 '이상도'를 포착합니다. 이는 샘플과 k- 최근 접 이웃의 로컬 도달 가능성 밀도의 비율의 평균입니다.</target>
        </trans-unit>
        <trans-unit id="f5a19f67242aeecdaccb15b2c01bb7b8a32da9cd" translate="yes" xml:space="preserve">
          <source>The log likelihood at each iteration.</source>
          <target state="translated">각 반복에서 로그 가능성.</target>
        </trans-unit>
        <trans-unit id="d6423c4071c8619e84a84f9bcba64b070be431d3" translate="yes" xml:space="preserve">
          <source>The log-marginal-likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;self.kernel_.theta&lt;/code&gt; 의 로그 한계 가능성</target>
        </trans-unit>
        <trans-unit id="b9f3852ad5acaf7d90c57ecdaa290bd2f37a1610" translate="yes" xml:space="preserve">
          <source>The log-transformed bounds on the kernel&amp;rsquo;s hyperparameters theta</source>
          <target state="translated">커널 하이퍼 파라미터 세타의 로그 변환 된 경계</target>
        </trans-unit>
        <trans-unit id="997c53b0e19e31f9cbe762633463d7411a04ec12" translate="yes" xml:space="preserve">
          <source>The logarithm used is the natural logarithm (base-e).</source>
          <target state="translated">사용 된 로그는 자연 로그입니다 (base-e).</target>
        </trans-unit>
        <trans-unit id="b4b4750e8021650b4da5de07fd1cb495068051c3" translate="yes" xml:space="preserve">
          <source>The logistic regression with One-Vs-Rest is not a multiclass classifier out of the box. As a result it has more trouble in separating class 2 and 3 than the other estimators.</source>
          <target state="translated">One-Vs-Rest를 사용한 로지스틱 회귀는 기본적으로 멀티 클래스 분류 기가 아닙니다. 결과적으로 다른 추정기보다 클래스 2와 3을 분리하는 데 더 많은 어려움이 있습니다.</target>
        </trans-unit>
        <trans-unit id="658bb63624739c29e98c4d11ac78e5bf2f1fae2c" translate="yes" xml:space="preserve">
          <source>The loss function that &lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt;&lt;code&gt;HuberRegressor&lt;/code&gt;&lt;/a&gt; minimizes is given by</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt; &lt;code&gt;HuberRegressor&lt;/code&gt; 가&lt;/a&gt; 최소화 하는 손실 함수 는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="47dc1dfa051244f9df37eb81c616d6773441e3ed" translate="yes" xml:space="preserve">
          <source>The loss function to be used. Defaults to &amp;lsquo;hinge&amp;rsquo;, which gives a linear SVM.</source>
          <target state="translated">사용할 손실 기능. 기본값은 'hinge'이며 선형 SVM을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="ced89f4970549dd56e97e03e89e06fddefc0d81d" translate="yes" xml:space="preserve">
          <source>The loss function to be used. The possible values are &amp;lsquo;squared_loss&amp;rsquo;, &amp;lsquo;huber&amp;rsquo;, &amp;lsquo;epsilon_insensitive&amp;rsquo;, or &amp;lsquo;squared_epsilon_insensitive&amp;rsquo;</source>
          <target state="translated">사용할 손실 기능. 가능한 값은 'squared_loss', 'huber', 'epsilon_insensitive'또는 'squared_epsilon_insensitive'입니다.</target>
        </trans-unit>
        <trans-unit id="dcd8a12c459702396e7eec22715d06aed2aee153" translate="yes" xml:space="preserve">
          <source>The loss function to be used: epsilon_insensitive: equivalent to PA-I in the reference paper. squared_epsilon_insensitive: equivalent to PA-II in the reference paper.</source>
          <target state="translated">사용되는 손실 함수 : epsilon_insensitive : 참조 용지의 PA-I와 동일합니다. squared_epsilon_insensitive : 참조 용지의 PA-II와 동일합니다.</target>
        </trans-unit>
        <trans-unit id="4c1d6cfb55920bc5e055d28ac3a6b6a90335ca56" translate="yes" xml:space="preserve">
          <source>The loss function to be used: hinge: equivalent to PA-I in the reference paper. squared_hinge: equivalent to PA-II in the reference paper.</source>
          <target state="translated">사용되는 손실 기능 : 힌지 : 참조 용지의 PA-I와 동일합니다. squared_hinge : 참조 용지의 PA-II와 동일합니다.</target>
        </trans-unit>
        <trans-unit id="efb9498c0013e0aa10110f406847617e3f7359e7" translate="yes" xml:space="preserve">
          <source>The loss function to use when updating the weights after each boosting iteration.</source>
          <target state="translated">각 부스팅 반복 후 가중치를 업데이트 할 때 사용할 손실 함수입니다.</target>
        </trans-unit>
        <trans-unit id="76e41cb6fdfbaf660e2380953e681777b0e6378c" translate="yes" xml:space="preserve">
          <source>The loss function used is binomial deviance. Regularization via shrinkage (&lt;code&gt;learning_rate &amp;lt; 1.0&lt;/code&gt;) improves performance considerably. In combination with shrinkage, stochastic gradient boosting (&lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt;) can produce more accurate models by reducing the variance via bagging. Subsampling without shrinkage usually does poorly. Another strategy to reduce the variance is by subsampling the features analogous to the random splits in Random Forests (via the &lt;code&gt;max_features&lt;/code&gt; parameter).</source>
          <target state="translated">사용 된 손실 함수는 이항 이탈입니다. 축소 ( &lt;code&gt;learning_rate &amp;lt; 1.0&lt;/code&gt; ) 를 통한 정규화 는 성능을 크게 향상시킵니다. 수축과 결합하여 확률 적 그라디언트 부스팅 ( &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; )은 배깅을 통해 분산을 줄임으로써보다 정확한 모델을 생성 할 수 있습니다. 수축이없는 서브 샘플링은 일반적으로 좋지 않습니다. 분산을 줄이는 또 다른 전략은 랜덤 포레스트의 랜덤 스플릿과 유사한 기능을 서브 샘플링하는 것입니다 ( &lt;code&gt;max_features&lt;/code&gt; 매개 변수 를 통해 ).</target>
        </trans-unit>
        <trans-unit id="b776e738f1fc829ad5cd97ce27a8c60ba5e6ea09" translate="yes" xml:space="preserve">
          <source>The low rank part of the profile can be considered the structured signal part of the data while the tail can be considered the noisy part of the data that cannot be summarized by a low number of linear components (singular vectors).</source>
          <target state="translated">프로파일의 하위 부분은 데이터의 구조화 된 신호 부분으로 간주 될 수있는 반면 테일은 적은 수의 선형 구성 요소 (단일 벡터)로 요약 할 수없는 데이터의 노이즈 부분으로 간주 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="eab337b7facddd1f9b82eece11282a55e0039c48" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on alpha</source>
          <target state="translated">알파의 하한과 상한</target>
        </trans-unit>
        <trans-unit id="59acfd562a002dfced1122413eab7f832a55dd1c" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on constant_value</source>
          <target state="translated">constant_value의 하한 및 상한</target>
        </trans-unit>
        <trans-unit id="3a719a94ceaa10893529e6c7cb2929f1cad3fed1" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on gamma</source>
          <target state="translated">감마의 하한과 상한</target>
        </trans-unit>
        <trans-unit id="1ed15fdef2e92d07e56ad7fd40f1816e9ce5774a" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on l</source>
          <target state="translated">l의 상한과 하한</target>
        </trans-unit>
        <trans-unit id="5a4d3b39beb7cd8132b2abe44c4adfccffd03b85" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on length_scale</source>
          <target state="translated">length_scale의 하한 및 상한</target>
        </trans-unit>
        <trans-unit id="23389af6ff1a664526029d031ba30d8bce5de030" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on noise_level</source>
          <target state="translated">noise_level의 하한 및 상한</target>
        </trans-unit>
        <trans-unit id="5d396b2f8516f35fa726d028e889486242bc7cef" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on periodicity</source>
          <target state="translated">주기성의 하한과 상한</target>
        </trans-unit>
        <trans-unit id="67f0743ba9ff76a5a36032511baa5e62618c04ec" translate="yes" xml:space="preserve">
          <source>The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n &amp;lt;= n &amp;lt;= max_n will be used.</source>
          <target state="translated">추출 될 상이한 n- 그램에 대한 n- 값 범위의 하한 및 상한. min_n &amp;lt;= n &amp;lt;= max_n과 같은 n의 모든 값이 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="1cf3abbfa7f1a3041db626cf750d4ae3ee4a5f71" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used create the extreme values for the &lt;code&gt;grid&lt;/code&gt;. Only if &lt;code&gt;X&lt;/code&gt; is not None.</source>
          <target state="translated">사용 된 하한 및 상한 백분위 수는 &lt;code&gt;grid&lt;/code&gt; 대한 극값을 만듭니다 . &lt;code&gt;X&lt;/code&gt; 가 없음이 아닌 경우에만 .</target>
        </trans-unit>
        <trans-unit id="ff88012ee3d2491153687a1e07b6eefe04629c89" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used to create the extreme values for the PDP axes.</source>
          <target state="translated">PDP 축의 극한 값을 생성하는 데 사용되는 하한 및 상한 백분위 수입니다.</target>
        </trans-unit>
        <trans-unit id="ca509231087e3737e2d90c6b71a37c419327789b" translate="yes" xml:space="preserve">
          <source>The lower left figure plots the pointwise decomposition of the expected mean squared error of a single decision tree. It confirms that the bias term (in blue) is low while the variance is large (in green). It also illustrates the noise part of the error which, as expected, appears to be constant and around &lt;code&gt;0.01&lt;/code&gt;.</source>
          <target state="translated">왼쪽 아래 그림은 단일 의사 결정 트리의 예상 평균 제곱 오차의 점별 분해를 나타냅니다. 편차 항 (파란색)이 낮고 분산이 클 때 (녹색) 확인합니다. 또한 예상대로 일정하고 약 &lt;code&gt;0.01&lt;/code&gt; 인 것으로 보이는 오류의 노이즈 부분을 보여줍니다 .</target>
        </trans-unit>
        <trans-unit id="caf1b50ddc9e3b679e122c659dff7510d4ed2966" translate="yes" xml:space="preserve">
          <source>The lower the better.</source>
          <target state="translated">낮을수록 좋습니다.</target>
        </trans-unit>
        <trans-unit id="bfd900c27bac872165454194e9ba0284baa1f3aa" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems.</source>
          <target state="translated">hole 레 스키 대각선 요인의 계산에서 기계 정밀도 정규화. 조건이 잘못된 시스템의 경우이 값을 늘리십시오.</target>
        </trans-unit>
        <trans-unit id="69abd1713b39f884bea4ffeed0d502e131117e74" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &amp;lsquo;tol&amp;rsquo; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.</source>
          <target state="translated">hole 레 스키 대각선 요인의 계산에서 기계 정밀도 정규화. 조건이 잘못된 시스템의 경우이 값을 늘리십시오. 일부 반복 최적화 기반 알고리즘의 'tol'매개 변수와 달리이 매개 변수는 최적화의 허용 오차를 제어하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="03f1e7333f5d863384674ba69d2200c51c214771" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &lt;code&gt;tol&lt;/code&gt; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.</source>
          <target state="translated">hole 레 스키 대각선 요인의 계산에서 기계 정밀도 정규화. 조건이 좋지 않은 시스템의 경우이를 늘리십시오. 일부 반복 최적화 기반 알고리즘 의 &lt;code&gt;tol&lt;/code&gt; 매개 변수 와 달리이 매개 변수는 최적화의 허용 오차를 제어하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="686cc4307ca584641912f9ca8288a53bb10ce6de" translate="yes" xml:space="preserve">
          <source>The main advantage for Factor Analysis over &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is that it can model the variance in every direction of the input space independently (heteroscedastic noise):</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; 에&lt;/a&gt; 대한 요인 분석의 주요 장점 은 입력 공간의 모든 방향에 대한 분산을 독립적으로 모델링 할 수 있다는 것입니다 (이 분산 잡음).</target>
        </trans-unit>
        <trans-unit id="c6108525766abe52c974030991b5e8a7ddcb6e28" translate="yes" xml:space="preserve">
          <source>The main difficulty in learning Gaussian mixture models from unlabeled data is that it is one usually doesn&amp;rsquo;t know which points came from which latent component (if one has access to this information it gets very easy to fit a separate Gaussian distribution to each set of points). &lt;a href=&quot;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;Expectation-maximization&lt;/a&gt; is a well-founded statistical algorithm to get around this problem by an iterative process. First one assumes random components (randomly centered on data points, learned from k-means, or even just normally distributed around the origin) and computes for each point a probability of being generated by each component of the model. Then, one tweaks the parameters to maximize the likelihood of the data given those assignments. Repeating this process is guaranteed to always converge to a local optimum.</source>
          <target state="translated">레이블이 지정되지 않은 데이터에서 가우시안 혼합 모델을 학습하는 데있어 가장 어려운 점은 일반적으로 어떤 잠재적 구성 요소에서 어떤 지점이 왔는지 알 수 없다는 것입니다. 포인트들). &lt;a href=&quot;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;기대 최대화&lt;/a&gt; 는 반복 프로세스를 통해이 문제를 해결하기 위해 잘 알려진 통계 알고리즘입니다. 첫 번째는 랜덤 성분 (데이터 포인트에 무작위로 중심을 두거나 k- 평균에서 배운 것, 또는 원점을 따라 정상적으로 분포 됨)을 가정하고 각 포인트에 대해 모델의 각 컴포넌트에 의해 생성 될 확률을 계산합니다. 그런 다음 할당이 주어진 데이터의 가능성을 최대화하기 위해 매개 변수를 조정합니다. 이 과정을 반복하면 항상 지역 최적으로 수렴됩니다.</target>
        </trans-unit>
        <trans-unit id="d2139f3f89c20ed8a9cf480b63f0750e12fef92b" translate="yes" xml:space="preserve">
          <source>The main documentation. This contains an in-depth description of all algorithms and how to apply them.</source>
          <target state="translated">주요 문서. 여기에는 모든 알고리즘에 대한 자세한 설명과 적용 방법이 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="5f94da6f46e5f5e5d800fbb67f1c2ae40caf9c89" translate="yes" xml:space="preserve">
          <source>The main drawback of Affinity Propagation is its complexity. The algorithm has a time complexity of the order \(O(N^2 T)\), where \(N\) is the number of samples and \(T\) is the number of iterations until convergence. Further, the memory complexity is of the order \(O(N^2)\) if a dense similarity matrix is used, but reducible if a sparse similarity matrix is used. This makes Affinity Propagation most appropriate for small to medium sized datasets.</source>
          <target state="translated">선호도 전파의 주요 단점은 복잡성입니다. 알고리즘은 \ (O (N ^ 2 T) \) 차수의 시간 복잡성을 가지며, 여기서 \ (N \)는 샘플 수이고 \ (T \)는 수렴까지의 반복 횟수입니다. 또한 밀도가 높은 유사성 매트릭스를 사용하는 경우 메모리 복잡도는 \ (O (N ^ 2) \) 순서이지만 스파 스 유사성 매트릭스를 사용하는 경우에는 감소 할 수 있습니다. 따라서 선호도 전파는 중소 규모의 데이터 집합에 가장 적합합니다.</target>
        </trans-unit>
        <trans-unit id="f23633268affa3b7ac5da4b687edb4096985bac5" translate="yes" xml:space="preserve">
          <source>The main factors that influence the prediction latency are</source>
          <target state="translated">예측 대기 시간에 영향을 미치는 주요 요인은</target>
        </trans-unit>
        <trans-unit id="26167f635fda3c2035f2a73e215b9329a59f7a43" translate="yes" xml:space="preserve">
          <source>The main observations to make are:</source>
          <target state="translated">주요 관찰 사항은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="b5b8962f0fe90f98d375c2c689eb4ed0da37d3b3" translate="yes" xml:space="preserve">
          <source>The main parameters to adjust when using these methods is &lt;code&gt;n_estimators&lt;/code&gt; and &lt;code&gt;max_features&lt;/code&gt;. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are &lt;code&gt;max_features=n_features&lt;/code&gt; for regression problems, and &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; for classification tasks (where &lt;code&gt;n_features&lt;/code&gt; is the number of features in the data). Good results are often achieved when setting &lt;code&gt;max_depth=None&lt;/code&gt; in combination with &lt;code&gt;min_samples_split=2&lt;/code&gt; (i.e., when fully developing the trees). Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM. The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are used by default (&lt;code&gt;bootstrap=True&lt;/code&gt;) while the default strategy for extra-trees is to use the whole dataset (&lt;code&gt;bootstrap=False&lt;/code&gt;). When using bootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. This can be enabled by setting &lt;code&gt;oob_score=True&lt;/code&gt;.</source>
          <target state="translated">이러한 방법을 사용할 때 조정해야 할 주요 매개 변수는 &lt;code&gt;n_estimators&lt;/code&gt; 및 &lt;code&gt;max_features&lt;/code&gt; 입니다. 전자는 숲의 나무 수입니다. 클수록 클수록 좋지만 계산하는 데 시간이 오래 걸립니다. 또한 중요한 수의 나무를 넘어서는 결과는 훨씬 더 나아지지 않습니다. 후자는 노드를 분할 할 때 고려해야 할 기능의 임의 하위 집합 크기입니다. 낮을수록 분산 감소는 커지지 만 바이어스 증가는 커집니다. 실증 좋은 기본 값은 &lt;code&gt;max_features=n_features&lt;/code&gt; 회귀 문제에 대한, 그리고 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; 분류 작업 ( &lt;code&gt;n_features&lt;/code&gt; 데이터의 기능 수입니다). &lt;code&gt;max_depth=None&lt;/code&gt; 을 &lt;code&gt;min_samples_split=2&lt;/code&gt; 와 함께 설정하면 (즉, 트리를 완전히 개발할 때) 좋은 결과를 얻을 수 있습니다 . 이러한 값은 일반적으로 최적이 아니며 많은 RAM을 소비하는 모델이 될 수 있습니다. 최상의 매개 변수 값은 항상 교차 검증되어야합니다. 또한 임의 포리스트에서는 기본적으로 부트 스트랩 샘플이 사용되며 ( &lt;code&gt;bootstrap=True&lt;/code&gt; ) 추가 트리의 기본 전략은 전체 데이터 세트 ( &lt;code&gt;bootstrap=False&lt;/code&gt; ) 를 사용하는 것 입니다. 부트 스트랩 샘플링을 사용할 때 일반화 정확도는 왼쪽 또는 가방 외부 샘플에서 추정 할 수 있습니다. &lt;code&gt;oob_score=True&lt;/code&gt; 를 설정하여 활성화 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="842b51b29e297ee475304c463fec6849d760da97" translate="yes" xml:space="preserve">
          <source>The main purpose of t-SNE is visualization of high-dimensional data. Hence, it works best when the data will be embedded on two or three dimensions.</source>
          <target state="translated">t-SNE의 주요 목적은 고차원 데이터의 시각화입니다. 따라서 데이터가 2 차원 또는 3 차원으로 임베드 될 때 가장 효과적입니다.</target>
        </trans-unit>
        <trans-unit id="85533a7e662fe3db2975f7f26edf978cd22f568d" translate="yes" xml:space="preserve">
          <source>The main theoretical result behind the efficiency of random projection is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;Johnson-Lindenstrauss lemma (quoting Wikipedia)&lt;/a&gt;:</source>
          <target state="translated">랜덤 프로젝션의 효율성에 대한 주요 이론적 결과는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;Johnson-Lindenstrauss lemma (Wikipedia 인용)입니다&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="c6fec5e2fce31198cf0394ae7c6624fc74feba7a" translate="yes" xml:space="preserve">
          <source>The main usage of a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt; is to compute the GP&amp;rsquo;s covariance between datapoints. For this, the method &lt;code&gt;__call__&lt;/code&gt; of the kernel can be called. This method can either be used to compute the &amp;ldquo;auto-covariance&amp;rdquo; of all pairs of datapoints in a 2d array X, or the &amp;ldquo;cross-covariance&amp;rdquo; of all combinations of datapoints of a 2d array X with datapoints in a 2d array Y. The following identity holds true for all kernels k (except for the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt;&lt;code&gt;WhiteKernel&lt;/code&gt;&lt;/a&gt;): &lt;code&gt;k(X) == K(X, Y=X)&lt;/code&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt; 의 주요 사용법은 데이터 포인트 간의 GP 공분산을 계산하는 것입니다. 이를 위해 커널의 &lt;code&gt;__call__&lt;/code&gt; 메소드를 호출 할 수 있습니다. 이 방법은 2d 배열 X에있는 모든 데이터 쌍 쌍의 &quot;자동 공분산&quot;을 계산하거나 2d 배열 X에있는 데이터 지점과 2d 배열 Y에있는 데이터 지점의 모든 조합의 &quot;교차 공분산&quot;을 계산하는 데 사용할 수 있습니다. 다음 커널은 모든 커널 k에 적용됩니다 ( &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt; &lt;code&gt;WhiteKernel&lt;/code&gt; &lt;/a&gt; 제외 ). &lt;code&gt;k(X) == K(X, Y=X)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="54123ac391766adafa62d20ad77d558f0edc6a11" translate="yes" xml:space="preserve">
          <source>The main use-case of the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt;&lt;code&gt;WhiteKernel&lt;/code&gt;&lt;/a&gt; kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter \(noise\_level\) corresponds to estimating the noise-level. It is defined as:e</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt; &lt;code&gt;WhiteKernel&lt;/code&gt; &lt;/a&gt; 커널 의 주요 사용 사례 는 신호의 노이즈 구성 요소를 설명하는 합계 커널의 일부입니다. 매개 변수 \ (noise \ _level \)를 조정하면 노이즈 수준을 추정 할 수 있습니다. 다음과 같이 정의됩니다 .e</target>
        </trans-unit>
        <trans-unit id="3b785a852cf47604fe35c8c001dbd6b426026b75" translate="yes" xml:space="preserve">
          <source>The main use-case of this kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter corresponds to estimating the noise-level.</source>
          <target state="translated">이 커널의 주요 사용 사례는 신호의 노이즈 구성 요소를 설명하는 합 커널의 일부입니다. 해당 파라미터를 튜닝하면 노이즈 레벨을 추정 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b8d24bce3b0ca4f2f608d76c9973e497dd5a4966" translate="yes" xml:space="preserve">
          <source>The major advantage of SGD is its efficiency, which is basically linear in the number of training examples. If X is a matrix of size (n, p) training has a cost of \(O(k n \bar p)\), where k is the number of iterations (epochs) and \(\bar p\) is the average number of non-zero attributes per sample.</source>
          <target state="translated">SGD의 주요 장점은 효율성이며, 이는 훈련 예제 수에서 기본적으로 선형입니다. X가 크기 (n, p)의 행렬이면 \ (O (kn \ bar p) \)의 비용이 듭니다. 여기서 k는 반복 횟수 (에포크)이고 \ (\ bar p \)는 평균입니다. 샘플 당 0이 아닌 속성의 수</target>
        </trans-unit>
        <trans-unit id="c142aa304b117907c4ec82e7c8f2e0e1debc825c" translate="yes" xml:space="preserve">
          <source>The manifold learning implementations available in scikit-learn are summarized below</source>
          <target state="translated">scikit-learn에서 사용 가능한 매니 폴드 학습 구현은 아래에 요약되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="d450abcdfe5ff938dd90f9482f51850eeea5e6fe" translate="yes" xml:space="preserve">
          <source>The mapping relies on a Monte Carlo approximation to the kernel values. The &lt;code&gt;fit&lt;/code&gt; function performs the Monte Carlo sampling, whereas the &lt;code&gt;transform&lt;/code&gt; method performs the mapping of the data. Because of the inherent randomness of the process, results may vary between different calls to the &lt;code&gt;fit&lt;/code&gt; function.</source>
          <target state="translated">매핑은 커널 값에 대한 Monte Carlo 근사에 의존합니다. &lt;code&gt;fit&lt;/code&gt; (가) 반면 기능 수행 몬테 카를로 샘플링 &lt;code&gt;transform&lt;/code&gt; 방법을 수행하는 데이터의 매핑. 프로세스의 고유 한 무작위성으로 인해 결과는 &lt;code&gt;fit&lt;/code&gt; 함수 에 대한 다른 호출마다 다를 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="1292a72996c3834d41499b1c7abdc4ace6de6204" translate="yes" xml:space="preserve">
          <source>The mask of selected features.</source>
          <target state="translated">선택된 지형지 물의 마스크입니다.</target>
        </trans-unit>
        <trans-unit id="c45c8ea0546b1c79f5dc6ae49b8b9eba3b94ab9e" translate="yes" xml:space="preserve">
          <source>The mathematical formulation is the following:</source>
          <target state="translated">수학적 공식은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="d7ffadc6e201c4cc579f6014a4ba6a553d3e6d97" translate="yes" xml:space="preserve">
          <source>The matrix</source>
          <target state="translated">매트릭스</target>
        </trans-unit>
        <trans-unit id="2cec1c9e3ed6a74b7dbd8e5442d20aacdeb523d2" translate="yes" xml:space="preserve">
          <source>The matrix dimension.</source>
          <target state="translated">매트릭스 차원.</target>
        </trans-unit>
        <trans-unit id="e194b944fcebb1be4c3463284dee201bd8108680" translate="yes" xml:space="preserve">
          <source>The matrix inverse of the covariance matrix, often called the precision matrix, is proportional to the partial correlation matrix. It gives the partial independence relationship. In other words, if two features are independent conditionally on the others, the corresponding coefficient in the precision matrix will be zero. This is why it makes sense to estimate a sparse precision matrix: the estimation of the covariance matrix is better conditioned by learning independence relations from the data. This is known as &lt;em&gt;covariance selection&lt;/em&gt;.</source>
          <target state="translated">공분산 행렬의 역행렬은 종종 정밀 행렬이라고도하며 부분 상관 행렬에 비례합니다. 부분적인 독립 관계를 제공합니다. 즉, 두 피처가 다른 피처에 대해 조건부로 독립적 인 경우 정밀 행렬의 해당 계수는 0이됩니다. 이것이 희소 정밀 행렬을 추정하는 것이 합리적입니다. 공분산 행렬의 추정은 데이터로부터 독립 관계를 학습함으로써 더 잘 조절됩니다. 이를 &lt;em&gt;공분산 선택이라고&lt;/em&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="e578a80d83c508825e0e59f2ea21cfd3dab91a77" translate="yes" xml:space="preserve">
          <source>The matrix of features, where NP is the number of polynomial features generated from the combination of inputs.</source>
          <target state="translated">특징의 행렬. 여기서 NP는 입력 조합에서 생성 된 다항식 특징의 수입니다.</target>
        </trans-unit>
        <trans-unit id="64f7c7c3f44e0d1f1fa6c715782355e1f1d2054c" translate="yes" xml:space="preserve">
          <source>The matrix.</source>
          <target state="translated">매트릭스.</target>
        </trans-unit>
        <trans-unit id="f3c27fa93df86412f11493765849dd6aeb05082d" translate="yes" xml:space="preserve">
          <source>The maximum depth of each tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</source>
          <target state="translated">각 나무의 최대 깊이. None이면 모든 나뭇잎이 순수하거나 모든 나뭇잎에 min_samples_split보다 적은 샘플이 포함될 때까지 노드가 확장됩니다.</target>
        </trans-unit>
        <trans-unit id="52e0ef4a9206a897434b4d55c59a4824cb11d988" translate="yes" xml:space="preserve">
          <source>The maximum depth of the representation. If None, the tree is fully generated.</source>
          <target state="translated">표현의 최대 깊이. None이면 트리가 완전히 생성됩니다.</target>
        </trans-unit>
        <trans-unit id="e0ae7481b9fb370fd4191031ea9f70e91832a43a" translate="yes" xml:space="preserve">
          <source>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</source>
          <target state="translated">나무의 최대 깊이. None이면 모든 나뭇잎이 순수하거나 모든 나뭇잎에 min_samples_split보다 적은 샘플이 포함될 때까지 노드가 확장됩니다.</target>
        </trans-unit>
        <trans-unit id="0cec94a1183be1885b8ca965cd0a310a0d6ef03e" translate="yes" xml:space="preserve">
          <source>The maximum distance between two samples for them to be considered as in the same neighborhood.</source>
          <target state="translated">두 샘플 사이의 최대 거리는 동일한 이웃에서와 같이 간주됩니다.</target>
        </trans-unit>
        <trans-unit id="72469eac4bccf834885ed51dab58c805d8cdc971" translate="yes" xml:space="preserve">
          <source>The maximum number of concurrently running jobs, such as the number of Python worker processes when backend=&amp;rdquo;multiprocessing&amp;rdquo; or the size of the thread-pool when backend=&amp;rdquo;threading&amp;rdquo;. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. None is a marker for &amp;lsquo;unset&amp;rsquo; that will be interpreted as n_jobs=1 (sequential execution) unless the call is performed under a parallel_backend context manager that sets another value for n_jobs.</source>
          <target state="translated">백엔드 = &quot;멀티 프로세싱&quot;인 경우 Python 작업자 프로세스 수 또는 백엔드 = &quot;스레딩&quot;인 경우 스레드 풀의 크기와 같이 동시에 실행되는 최대 작업 수 -1이면 모든 CPU가 사용됩니다. 1이 주어지면 병렬 컴퓨팅 코드가 전혀 사용되지 않으므로 디버깅에 유용합니다. -1 미만의 n_jobs에는 (n_cpus + 1 + n_jobs)가 사용됩니다. 따라서 n_jobs = -2의 경우 하나를 제외한 모든 CPU가 사용됩니다. n_jobs에 대한 다른 값을 설정하는 parallel_backend 컨텍스트 관리자에서 호출이 수행되지 않으면 n_jobs = 1 (순차 실행)로 해석되는 'unset'에 대한 마커가 없습니다.</target>
        </trans-unit>
        <trans-unit id="d68dbcc29192b28b1c0e16950a4955da0229e9e9" translate="yes" xml:space="preserve">
          <source>The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.</source>
          <target state="translated">부스팅이 종료되는 최대 추정량입니다. 완벽하게 맞는 경우 학습 절차가 일찍 중단됩니다.</target>
        </trans-unit>
        <trans-unit id="0ace226674121a7119418c464f4452694b61318d" translate="yes" xml:space="preserve">
          <source>The maximum number of features selected scoring above &lt;code&gt;threshold&lt;/code&gt;. To disable &lt;code&gt;threshold&lt;/code&gt; and only select based on &lt;code&gt;max_features&lt;/code&gt;, set &lt;code&gt;threshold=-np.inf&lt;/code&gt;.</source>
          <target state="translated">선택한 점수가 &lt;code&gt;threshold&lt;/code&gt; 하는 최대 기능 수입니다 . &lt;code&gt;threshold&lt;/code&gt; 을 비활성화 하고 &lt;code&gt;max_features&lt;/code&gt; 기반으로 만 선택 하려면 &lt;code&gt;threshold=-np.inf&lt;/code&gt; 를 설정 하십시오 .</target>
        </trans-unit>
        <trans-unit id="1360fbfbf92ec231e131ca0c0a387fb9e5b6a69f" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations</source>
          <target state="translated">최대 반복 횟수</target>
        </trans-unit>
        <trans-unit id="07cf3627f06a0c97fd8b5fd02bbb00e02fc4d154" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations in Newton&amp;rsquo;s method for approximating the posterior during predict. Smaller values will reduce computation time at the cost of worse results.</source>
          <target state="translated">예측 동안 후부를 근사화하기위한 뉴턴 방법의 최대 반복 횟수입니다. 값이 작을수록 결과가 나빠질수록 계산 시간이 줄어 듭니다.</target>
        </trans-unit>
        <trans-unit id="5ecb214f49e2d4dbf3814aaa5686d1fc9fb18e01" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations is usually high enough and does not need any tuning. The optimization consists of two phases: the early exaggeration phase and the final optimization. During early exaggeration the joint probabilities in the original space will be artificially increased by multiplication with a given factor. Larger factors result in larger gaps between natural clusters in the data. If the factor is too high, the KL divergence could increase during this phase. Usually it does not have to be tuned. A critical parameter is the learning rate. If it is too low gradient descent will get stuck in a bad local minimum. If it is too high the KL divergence will increase during optimization. More tips can be found in Laurens van der Maaten&amp;rsquo;s FAQ (see references). The last parameter, angle, is a tradeoff between performance and accuracy. Larger angles imply that we can approximate larger regions by a single point, leading to better speed but less accurate results.</source>
          <target state="translated">최대 반복 횟수는 일반적으로 충분히 높으며 튜닝이 필요하지 않습니다. 최적화는 초기 과장 단계와 최종 최적화의 두 단계로 구성됩니다. 과장 초기에 원래의 공간에서의 합동 확률은 주어진 요인과의 곱셈에 의해 인위적으로 증가합니다. 요인이 클수록 데이터의 자연 군집간에 간격이 더 커집니다. 계수가 너무 높으면이 단계에서 KL 분기가 증가 할 수 있습니다. 일반적으로 튜닝 할 필요는 없습니다. 중요한 매개 변수는 학습 속도입니다. 그래디언트 디센트가 너무 낮 으면 로컬 최소값이 나빠질 수 있습니다. 너무 높으면 최적화 중에 KL 발산이 증가합니다. Laurens van der Maaten의 FAQ에서 더 많은 팁을 찾을 수 있습니다 (참고 자료 참조). 마지막 매개 변수 인 각도는 성능과 정확도 간의 균형입니다.각도가 클수록 단일 지점으로 더 큰 영역을 근사 할 수있어 속도는 향상되지만 정확도는 떨어집니다.</target>
        </trans-unit>
        <trans-unit id="3a263d39eeb80bbc61890473f9e4f7de61b380fb" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations to be run.</source>
          <target state="translated">실행할 최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="42a16ed4baf475d1973848504fece7b7ddcdacc6" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations.</source>
          <target state="translated">최대 반복 횟수입니다.</target>
        </trans-unit>
        <trans-unit id="06bd569f5fc7509f2f4591004e3b0ffe2a685cf9" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;code&gt;partial_fit&lt;/code&gt;. Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.</source>
          <target state="translated">훈련 데이터를 통과하는 최대 패스 수 (일명 에포크). &lt;code&gt;partial_fit&lt;/code&gt; 이 아니라 &lt;code&gt;fit&lt;/code&gt; 메소드 의 동작에만 영향을 미칩니다 . 기본값은 5입니다. 0.21에서 또는 tol이 None이 아닌 경우 기본값은 1000입니다.</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
