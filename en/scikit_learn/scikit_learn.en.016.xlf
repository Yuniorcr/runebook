<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ja" datatype="htmlbody" original="">
    <body>
      <group id="">
        <trans-unit id="73890f2d8730349939758bb99469c8dd6b7e6151" translate="yes" xml:space="preserve">
          <source>The maximum number of patches per image to extract. If max_patches is a float in (0, 1), it is taken to mean a proportion of the total number of patches.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9914613b7b3d16fc7caab060022c123f66024339" translate="yes" xml:space="preserve">
          <source>The maximum number of patches to extract. If max_patches is a float between 0 and 1, it is taken to be a proportion of the total number of patches.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="353b6da890fd9a7a3db82e5fc166d25d61607aa3" translate="yes" xml:space="preserve">
          <source>The maximum number of points on the path used to compute the residuals in the cross-validation</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c53181fb6c1656f1967e529a2fe72fd9bc29ff60" translate="yes" xml:space="preserve">
          <source>The mean and the empirical covariance of the full dataset, which break down as soon as there are outliers in the data set</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f962fad68fff332002b42ee3dcc1e06f41fcfe6c" translate="yes" xml:space="preserve">
          <source>The mean and the empirical covariance of the observations that are known to be good ones. This can be considered as a &amp;ldquo;perfect&amp;rdquo; MCD estimation, so one can trust our implementation by comparing to this case.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f98043dc9f229ba1c70dcb8021abddb5d793d8af" translate="yes" xml:space="preserve">
          <source>The mean of each mixture component.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="05329e8678ffdabb505c75140f9a49322a5129f1" translate="yes" xml:space="preserve">
          <source>The mean of the multi-dimensional normal distribution. If None then use the origin (0, 0, &amp;hellip;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3598a929d1a64528ce62560f1aa02f9fb9981b30" translate="yes" xml:space="preserve">
          <source>The mean predicted probability in each bin.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8aaf5602eb2242bddc9912d47746326b31b0a1d5" translate="yes" xml:space="preserve">
          <source>The mean score and the 95% confidence interval of the score estimate are hence given by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="94d27e1df24bf9a05c82ae625a1197b415bc3fdc" translate="yes" xml:space="preserve">
          <source>The mean value for each feature in the training set. Equal to &lt;code&gt;None&lt;/code&gt; when &lt;code&gt;with_mean=False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b5e7dcb7d882c8689297cb339a998a6e74140e5f" translate="yes" xml:space="preserve">
          <source>The mean, standard error, and &amp;ldquo;worst&amp;rdquo; or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9952f9f2a02ad0002415a0bd8b6c9bae196d7ee8" translate="yes" xml:space="preserve">
          <source>The measure of normality of an observation given a tree is the depth of the leaf containing this observation, which is equivalent to the number of splittings required to isolate this point. In case of several observations n_left in the leaf, the average path length of a n_left samples isolation tree is added.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ff66cd9c701474c3d9f795cee9d82f5e1659bc8" translate="yes" xml:space="preserve">
          <source>The median absolute deviation to non corrupt new data is used to judge the quality of the prediction.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="970571bcac487854f4caea3e516e12da8ac34c22" translate="yes" xml:space="preserve">
          <source>The median value for each feature in the training set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8bf90a15bbbe280812d1ade2a9f9077adb9d41d" translate="yes" xml:space="preserve">
          <source>The memmapping mode used when loading from cache numpy arrays. See numpy.load for the meaning of the arguments.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4971ea4c8c64c5bc8a4bdd1e4563c9705f8166bf" translate="yes" xml:space="preserve">
          <source>The memmapping mode used when loading from cache numpy arrays. See numpy.load for the meaning of the arguments. By default that of the memory object is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85874f620128fa58a2e359ad1c9c828fcfd537bb" translate="yes" xml:space="preserve">
          <source>The memory footprint of randomized &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is also proportional to \(2 \cdot n_{\max} \cdot n_{\mathrm{components}}\) instead of \(n_{\max} \cdot n_{\min}\) for the exact method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4fa2b59a5a9d6863d86f91b2a847ac2c39bc204e" translate="yes" xml:space="preserve">
          <source>The method assumes the inputs come from a binary classifier.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff8ec8205e374ddf520735804ecbfa39550f37bd" translate="yes" xml:space="preserve">
          <source>The method fits the model &lt;code&gt;n_init&lt;/code&gt; times and sets the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for &lt;code&gt;max_iter&lt;/code&gt; times until the change of likelihood or lower bound is less than &lt;code&gt;tol&lt;/code&gt;, otherwise, a &lt;code&gt;ConvergenceWarning&lt;/code&gt; is raised. If &lt;code&gt;warm_start&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then &lt;code&gt;n_init&lt;/code&gt; is ignored and a single initialization is performed upon the first call. Upon consecutive calls, training starts where it left off.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5aeb4fb178bdd7f679ad7ab31606d57c02d18da8" translate="yes" xml:space="preserve">
          <source>The method fits the model n_init times and sets the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for &lt;code&gt;max_iter&lt;/code&gt; times until the change of likelihood or lower bound is less than &lt;code&gt;tol&lt;/code&gt;, otherwise, a &lt;code&gt;ConvergenceWarning&lt;/code&gt; is raised. After fitting, it predicts the most probable label for the input data points.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02e13e01ea0f52a6e082ed53d9636e409ba7f22e" translate="yes" xml:space="preserve">
          <source>The method gained popularity for initializing deep neural networks with the weights of independent RBMs. This method is known as unsupervised pre-training.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="867b88e44ce337abe62bbd2070ac4235fc6ec53b" translate="yes" xml:space="preserve">
          <source>The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b9b2478ee76aa8f8a62d64db00bfa346a8108cd" translate="yes" xml:space="preserve">
          <source>The method to use for calibration. Can be &amp;lsquo;sigmoid&amp;rsquo; which corresponds to Platt&amp;rsquo;s method or &amp;lsquo;isotonic&amp;rsquo; which is a non-parametric approach. It is not advised to use isotonic calibration with too few calibration samples &lt;code&gt;(&amp;lt;&amp;lt;1000)&lt;/code&gt; since it tends to overfit. Use sigmoids (Platt&amp;rsquo;s calibration) in this case.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21e55011234a7eafaab581faf9cf56cabba5a5c2" translate="yes" xml:space="preserve">
          <source>The method used to initialize the weights, the means and the covariances. Must be one of:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="963cb60032e3efabe5bef9b8ad76de7b8282f830" translate="yes" xml:space="preserve">
          <source>The method used to initialize the weights, the means and the precisions. Must be one of:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8015d2d76c1dfbbe3de68f3d3f8aa78fbf89dd67" translate="yes" xml:space="preserve">
          <source>The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; so that it&amp;rsquo;s possible to update each component of a nested object.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d1673cba254e117532409bf5f2a151ed75e6f39" translate="yes" xml:space="preserve">
          <source>The method works on simple kernels as well as on nested kernels. The latter have parameters of the form &lt;code&gt;&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;&lt;/code&gt; so that it&amp;rsquo;s possible to update each component of a nested object.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="653c073b67dfbefaf963d2a9e7b682aaf13d10c5" translate="yes" xml:space="preserve">
          <source>The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42e3e79f7ef752ca5a9bd963af37b6abd9f4c61f" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by &lt;a href=&quot;sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise_distances&lt;/code&gt;&lt;/a&gt; for its metric parameter. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in which case only &amp;ldquo;nonzero&amp;rdquo; elements may be considered neighbors for DBSCAN.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="379ba7f47f97569c7bd4b20c4c77667f05344897" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by metrics.pairwise.pairwise_distances for its metric parameter. The centroids for the samples corresponding to each class is the point from which the sum of the distances (according to the metric) of all samples that belong to that particular class are minimized. If the &amp;ldquo;manhattan&amp;rdquo; metric is provided, this centroid is the median and for all other metrics, the centroid is now set to be the mean.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c83cb22e1c81dc697b4c0637e95b8aeb91bdccce" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by &lt;code&gt;metrics.pairwise.pairwise_distances&lt;/code&gt;. If X is the distance array itself, use &lt;code&gt;metric=&quot;precomputed&quot;&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="76b4b37055a409fe2e4e93cf3ad5227beac3187f" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by &lt;code&gt;sklearn.metrics.pairwise.pairwise_distances&lt;/code&gt;. If X is the distance array itself, use &amp;ldquo;precomputed&amp;rdquo; as the metric.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0f209bf70493187675786c136d5f63b20f1b34d" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d6bfb4b6dd59ce295c63dba458c016e3505d6e2" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is &amp;ldquo;euclidean&amp;rdquo; which is interpreted as squared euclidean distance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d21e688cc862f7d70c6767af76fb5452c5fdcd0" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options specified in PAIRED_DISTANCES, including &amp;ldquo;euclidean&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo;, or &amp;ldquo;cosine&amp;rdquo;. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f36b3ebd6c1e0314a19882dfd7c792465ced8cb2" translate="yes" xml:space="preserve">
          <source>The metric to use when calculating kernel between instances in a feature array. If metric is a string, it must be one of the metrics in pairwise.PAIRWISE_KERNEL_FUNCTIONS. If metric is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be a kernel matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4d18e9d9a4e3ba27fc7d43cfc9aa960df294a66" translate="yes" xml:space="preserve">
          <source>The minimal number of components to guarantee with good probability an eps-embedding with n_samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dcfd1fcfc3ab5a126126fe7777d3f592dbce3714" translate="yes" xml:space="preserve">
          <source>The minimum consensus score, 0, occurs when all pairs of biclusters are totally dissimilar. The maximum score, 1, occurs when both sets are identical.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="546685a327b5ecf09c20bafa81b23b9f31e36223" translate="yes" xml:space="preserve">
          <source>The minimum number of components to guarantee the eps-embedding is given by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d171bb6d353676c30b749e2ca85c8811f4027e78" translate="yes" xml:space="preserve">
          <source>The minimum number of components to guarantees the eps-embedding is given by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78f0aa92767e958a159a7201fe662ff62e3924a6" translate="yes" xml:space="preserve">
          <source>The minimum number of features to be selected. This number of features will always be scored, even if the difference between the original feature count and &lt;code&gt;min_features_to_select&lt;/code&gt; isn&amp;rsquo;t divisible by &lt;code&gt;step&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1db45f422759f4529cb388eaa249fdf3a381a4d3" translate="yes" xml:space="preserve">
          <source>The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least &lt;code&gt;min_samples_leaf&lt;/code&gt; training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="754bd23e1994f02a9f11fc7f2013baebc017ca4d" translate="yes" xml:space="preserve">
          <source>The minimum number of samples required to split an internal node:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34dec0ced4163b0b0c5f6b2dfda2c2ae915251bf" translate="yes" xml:space="preserve">
          <source>The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="adbfb8d83e84eef2c959ef548acb01276646831a" translate="yes" xml:space="preserve">
          <source>The missing indicator for input data. The data type of &lt;code&gt;Xt&lt;/code&gt; will be boolean.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b5249d3ed9eb260ab7aedd15c61af2c7df4b9f1" translate="yes" xml:space="preserve">
          <source>The mixing matrix to be used to initialize the algorithm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bc7bce3b2af118e0efad437caf7d72239aa18a90" translate="yes" xml:space="preserve">
          <source>The mixing matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9852ae9396dc8e73ffef0a95fb9e0d330c7fbbc" translate="yes" xml:space="preserve">
          <source>The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c1fbc40d4a13f1c2e80dae47c2199d2a0ef37a24" translate="yes" xml:space="preserve">
          <source>The model fits a Gaussian density to each class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="744fe4c01d7aac1fa2b93515c5b761f9f450f5fc" translate="yes" xml:space="preserve">
          <source>The model makes assumptions regarding the distribution of inputs. At the moment, scikit-learn only provides &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt;, which assumes the inputs are either binary values or values between 0 and 1, each encoding the probability that the specific feature would be turned on.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e7d91224c848a3199f89b8ed290703400860de0" translate="yes" xml:space="preserve">
          <source>The model need to have probability information computed at training time: fit with attribute &lt;code&gt;probability&lt;/code&gt; set to True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="532fcfc5fc4303622a7e9b0379fb5dd6d278b658" translate="yes" xml:space="preserve">
          <source>The model parameters can be accessed through the members &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="87fcd2dcd4a9683677aa4a82e264db34c1778c7c" translate="yes" xml:space="preserve">
          <source>The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ffaf4f38449ad493c6f07021545ef1cc0f916269" translate="yes" xml:space="preserve">
          <source>The models are ordered from strongest regularized to least regularized. The 4 coefficients of the models are collected and plotted as a &amp;ldquo;regularization path&amp;rdquo;: on the left-hand side of the figure (strong regularizers), all the coefficients are exactly 0. When regularization gets progressively looser, coefficients can get non-zero values one after the other.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41864e959b3c3945768dfd483a7ad2160aff5a56" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; includes the popular boosting algorithm AdaBoost, introduced in 1995 by Freund and Schapire &lt;a href=&quot;#fs1995&quot; id=&quot;id9&quot;&gt;[FS1995]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06014352d795d21d24d63a43012b2cdda688123a" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; provides methods for both classification and regression via gradient boosted regression trees.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3b04b84598eb19b700a6f5a28a6ebcc8d4034a0" translate="yes" xml:space="preserve">
          <source>The module &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; also exposes a set of simple functions measuring a prediction error given ground truth and prediction:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4ba63f9b8e0d9fa0a182e0bb4dc5760121e3e0e" translate="yes" xml:space="preserve">
          <source>The module &lt;code&gt;partial_dependence&lt;/code&gt; provides a convenience function &lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.plot_partial_dependence#sklearn.ensemble.partial_dependence.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt; to create one-way and two-way partial dependence plots. In the below example we show how to create a grid of partial dependence plots: two one-way PDPs for the features &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; and a two-way PDP between the two features:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b2518b0fdafd75a6658a4d2a018bccb79345ce3" translate="yes" xml:space="preserve">
          <source>The module contains the public attributes &lt;code&gt;coefs_&lt;/code&gt; and &lt;code&gt;intercepts_&lt;/code&gt;. &lt;code&gt;coefs_&lt;/code&gt; is a list of weight matrices, where weight matrix at index \(i\) represents the weights between layer \(i\) and layer \(i+1\). &lt;code&gt;intercepts_&lt;/code&gt; is a list of bias vectors, where the vector at index \(i\) represents the bias values added to layer \(i+1\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b31fa7fd3ac8f2a64f0dd29549e8dd9abb96ee19" translate="yes" xml:space="preserve">
          <source>The module: &lt;code&gt;random_projection&lt;/code&gt; provides several tools for data reduction by random projections. See the relevant section of the documentation: &lt;a href=&quot;random_projection#random-projection&quot;&gt;Random Projection&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81f532ac90d157aeffd690ee0c3b7aa6b7bbd149" translate="yes" xml:space="preserve">
          <source>The monitor is called after each iteration with the current iteration, a reference to the estimator and the local variables of &lt;code&gt;_fit_stages&lt;/code&gt; as keyword arguments &lt;code&gt;callable(i, self,
locals())&lt;/code&gt;. If the callable returns &lt;code&gt;True&lt;/code&gt; the fitting procedure is stopped. The monitor can be used for various things such as computing held-out estimates, early stopping, model introspect, and snapshoting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52aa15c9df5da45110f63c8c26b8cfa477557d62" translate="yes" xml:space="preserve">
          <source>The most common parameter amenable to this strategy is the parameter encoding the strength of the regularizer. In this case we say that we compute the &lt;strong&gt;regularization path&lt;/strong&gt; of the estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3156312e704bdb91fdff12aa2e110d4f21e21302" translate="yes" xml:space="preserve">
          <source>The most intuitive way to do so is to use a bags of words representation:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="399fde41b63b2ea676c5145583a3950879f6c9fa" translate="yes" xml:space="preserve">
          <source>The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ec6281766fab2b8f9e9f9f6e92da7d4704e6316" translate="yes" xml:space="preserve">
          <source>The multi-task lasso allows to fit multiple regression problems jointly enforcing the selected features to be the same across tasks. This example simulates sequential measurements, each task is a time instant, and the relevant features vary in amplitude over time while being the same. The multi-task lasso imposes that features that are selected at one time point are select for all time point. This makes feature selection by the Lasso more stable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b55b776834f44186e095c0df9ee2b704ce3630c" translate="yes" xml:space="preserve">
          <source>The multiclass definition here seems the most reasonable extension of the metric used in binary classification, though there is no certain consensus in the literature:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5fdb408d7bc477f0762da630aa0f3aea39db3f13" translate="yes" xml:space="preserve">
          <source>The multiclass support is handled according to a one-vs-one scheme.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66359e593d021aa5ae2d568f4acc056ec0bf4a32" translate="yes" xml:space="preserve">
          <source>The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9fcb660998dc7edb2d34f5dc5854df445bad9a92" translate="yes" xml:space="preserve">
          <source>The multiple metrics can be specified either as a list, tuple or set of predefined scorer names:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a09e250651300d47ed9cd74b128d0a12a5aa8e3e" translate="yes" xml:space="preserve">
          <source>The name of the sample image loaded</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b853190860f83bcb94f9f4edb130e6f04adf4ae5" translate="yes" xml:space="preserve">
          <source>The names &lt;code&gt;vect&lt;/code&gt;, &lt;code&gt;tfidf&lt;/code&gt; and &lt;code&gt;clf&lt;/code&gt; (classifier) are arbitrary. We will use them to perform grid search for suitable hyperparameters below. We can now train the model with a single command:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9146e93a1405c3d2cac65e0084d1b1c7a951b3b3" translate="yes" xml:space="preserve">
          <source>The names of the dataset columns</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0fb0e8bf0410fa55523d36fa2a7a49ac4415117" translate="yes" xml:space="preserve">
          <source>The new dtype will be np.float32 or np.float64, depending on the original type. The function can create a copy or modify the argument depending on the argument copy.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d7b951e7afa9271dd7834467cb67e175d7e626f" translate="yes" xml:space="preserve">
          <source>The new entry \(d(u,v)\) is computed as follows,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c771668f5d4b4d7aa145f31455a67e2ba21af3ce" translate="yes" xml:space="preserve">
          <source>The next figure compares the results obtained for the different type of the weight concentration prior (parameter &lt;code&gt;weight_concentration_prior_type&lt;/code&gt;) for different values of &lt;code&gt;weight_concentration_prior&lt;/code&gt;. Here, we can see the value of the &lt;code&gt;weight_concentration_prior&lt;/code&gt; parameter has a strong impact on the effective number of active components obtained. We can also notice that large values for the concentration weight prior lead to more uniform weights when the type of prior is &amp;lsquo;dirichlet_distribution&amp;rsquo; while this is not necessarily the case for the &amp;lsquo;dirichlet_process&amp;rsquo; type (used by default).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6d38fd34e131451ba8b974153991556ff8b9398" translate="yes" xml:space="preserve">
          <source>The next figure compares the time for fitting and prediction of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; for different sizes of the training set. Fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is faster than &lt;code&gt;SVR&lt;/code&gt; for medium-sized training sets (less than 1000 samples); however, for larger training sets &lt;code&gt;SVR&lt;/code&gt; scales better. With regard to prediction time, &lt;code&gt;SVR&lt;/code&gt; is faster than &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters \(\epsilon\) and \(C\) of the &lt;code&gt;SVR&lt;/code&gt;; \(\epsilon = 0\) would correspond to a dense model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7fb8a1fd588ff5df76899e15821355218c8c70df" translate="yes" xml:space="preserve">
          <source>The next figure compares the time for fitting and prediction of KRR and SVR for different sizes of the training set. Fitting KRR is faster than SVR for medium- sized training sets (less than 1000 samples); however, for larger training sets SVR scales better. With regard to prediction time, SVR is faster than KRR for all sizes of the training set because of the learned sparse solution. Note that the degree of sparsity and thus the prediction time depends on the parameters epsilon and C of the SVR.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8b8c72913234ec998c925f50d9fa1a29ef7082f" translate="yes" xml:space="preserve">
          <source>The next image illustrates how sigmoid calibration changes predicted probabilities for a 3-class classification problem. Illustrated is the standard 2-simplex, where the three corners correspond to the three classes. Arrows point from the probability vectors predicted by an uncalibrated classifier to the probability vectors predicted by the same classifier after sigmoid calibration on a hold-out validation set. Colors indicate the true class of an instance (red: class 1, green: class 2, blue: class 3).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d16a780143f0785e9248e298dec17c9fe8de9d1e" translate="yes" xml:space="preserve">
          <source>The nodes are random variables whose states depend on the state of the other nodes they are connected to. The model is therefore parameterized by the weights of the connections, as well as one intercept (bias) term for each visible and hidden unit, omitted from the image for simplicity.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="55f688a90e745dd5020f6b5c567bbe8f6396fa6e" translate="yes" xml:space="preserve">
          <source>The noise level in the targets can be specified by passing it via the parameter &lt;code&gt;alpha&lt;/code&gt;, either globally as a scalar or per datapoint. Note that a moderate noise level can also be helpful for dealing with numeric issues during fitting as it is effectively implemented as Tikhonov regularization, i.e., by adding it to the diagonal of the kernel matrix. An alternative to specifying the noise level explicitly is to include a WhiteKernel component into the kernel, which can estimate the global noise level from the data (see example below).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99b58f648d173c7793dc0e913318a8835572f0c2" translate="yes" xml:space="preserve">
          <source>The non-fixed, log-transformed hyperparameters of the kernel</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ea3cf8563db967f688cb46c0883a9d020905a15" translate="yes" xml:space="preserve">
          <source>The nonmetric algorithm adds a monotonic regression step before computing the stress.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8cecdfcc92ccd83125ecaa6c4beb7447e43f92cb" translate="yes" xml:space="preserve">
          <source>The norm to use to normalize each non zero sample (or each non-zero feature if axis is 0).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="39e8a7c3ff72c23082d4ee16a84d74279c8584ba" translate="yes" xml:space="preserve">
          <source>The norm to use to normalize each non zero sample.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b4510db24d586496f5cd27e9cd8024ffe16a368e" translate="yes" xml:space="preserve">
          <source>The normalized mutual information is defined as</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad22e0beb8a3a9f5574d757b3678d39d78057ba3" translate="yes" xml:space="preserve">
          <source>The normalizer instance can then be used on sample vectors as any transformer:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="82d79be22b3b3fa23f79d393f8a5b628a27ddb08" translate="yes" xml:space="preserve">
          <source>The number k of neighbors considered, (alias parameter n_neighbors) is typically chosen 1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by objects that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general. When the proportion of outliers is high (i.e. greater than 10 %, as in the example below), n_neighbors should be greater (n_neighbors=35 in the example below).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a35a9d90f2abd9af864433913570f3dbe7f2765" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36ae328d3607f79ec631b3fe327da7ed8cf8098b" translate="yes" xml:space="preserve">
          <source>The number of CPUs to use to do the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="720879c36f3df22b5ec2b112039efcd8b5ba8b1b" translate="yes" xml:space="preserve">
          <source>The number of EM iterations to perform.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40648db4f1524a67ed66976c14cc75bac6eed386" translate="yes" xml:space="preserve">
          <source>The number of atomic tasks to dispatch at once to each worker. When individual evaluations are very fast, dispatching calls to workers can be slower than sequential computation because of the overhead. Batching fast computations together can mitigate this. The &lt;code&gt;'auto'&lt;/code&gt; strategy keeps track of the time it takes for a batch to complete, and dynamically adjusts the batch size to keep the time on the order of half a second, using a heuristic. The initial batch size is 1. &lt;code&gt;batch_size=&quot;auto&quot;&lt;/code&gt; with &lt;code&gt;backend=&quot;threading&quot;&lt;/code&gt; will dispatch batches of a single task at a time as the threading backend has very little overhead and using larger batch size has not proved to bring any gain in that case.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ae39cb6a942b138204dbb141781bfb4c75e3760" translate="yes" xml:space="preserve">
          <source>The number of base estimators in the ensemble.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9fe1c6517ea4c36af295e91a2e2410361ff8432c" translate="yes" xml:space="preserve">
          <source>The number of batches (of tasks) to be pre-dispatched. Default is &amp;lsquo;2*n_jobs&amp;rsquo;. When batch_size=&amp;rdquo;auto&amp;rdquo; this is reasonable default and the workers should never starve.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9d0a981566d58378881b2cabe57053da9d34fb6" translate="yes" xml:space="preserve">
          <source>The number of biclusters to find.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b9c52ec2125ed0e2339a6eae69fb246b4dc5348e" translate="yes" xml:space="preserve">
          <source>The number of biclusters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3424019fa75a2f96f62b30f8336ea4cebfa5c4fe" translate="yes" xml:space="preserve">
          <source>The number of bins to produce. The intervals for the bins are determined by the minimum and maximum of the input data. Raises ValueError if &lt;code&gt;n_bins &amp;lt; 2&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="016567b86fa6f6a4b4c043763b4e841bd445fc32" translate="yes" xml:space="preserve">
          <source>The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d31e8a7065eae23aeb71500302a31ebb485560ec" translate="yes" xml:space="preserve">
          <source>The number of classes</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="77a0204799f583fccb414a4215d0dc841510a2f7" translate="yes" xml:space="preserve">
          <source>The number of classes (for single output problems), or a list containing the number of classes for each output (for multi-output problems).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="221daa93ea83049b71eff667b2da7d0ce3fa89ae" translate="yes" xml:space="preserve">
          <source>The number of classes (or labels) of the classification problem.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b75bf8cc583f50195e96e51d244ce57f23360a0" translate="yes" xml:space="preserve">
          <source>The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b5fc0d622c5abb76da13c6a8f50c84a211eca46" translate="yes" xml:space="preserve">
          <source>The number of classes in the training data</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a096e01744ef01b9d139303dfd4a94a83569986b" translate="yes" xml:space="preserve">
          <source>The number of classes of the classification problem.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21583bce2023d80fa6dedc801ccace76e8a2445f" translate="yes" xml:space="preserve">
          <source>The number of classes to return.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7defdc95bb1ddc0580ac0be76fc251278b72fb84" translate="yes" xml:space="preserve">
          <source>The number of classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="062fe5ee9cf896a5c74f9ba057f89b8b6de5fa8a" translate="yes" xml:space="preserve">
          <source>The number of clusters per class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21f23a23e06d5c295fade98ab37ab55d16e621ca" translate="yes" xml:space="preserve">
          <source>The number of clusters to find.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b4184f0399110f5f8e690a26513dd6f78511c0a5" translate="yes" xml:space="preserve">
          <source>The number of clusters to form as well as the number of centroids to generate.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35d84f4a157603ef94bd9baafa0d524311104205" translate="yes" xml:space="preserve">
          <source>The number of columns in the grid plot (default: 3).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25441c707266953707d0c752071de32d0051d235" translate="yes" xml:space="preserve">
          <source>The number of connected components in the graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d17146f816382e55c9752b437d1e0a90ca8e256" translate="yes" xml:space="preserve">
          <source>The number of cross-validation splits (folds/iterations).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa32f92ae0454e6e9ee52207fe93d7eb6600c995" translate="yes" xml:space="preserve">
          <source>The number of degrees of freedom of each components in the model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2ef77453518fcb650d3ae2f90fcfda15611a36f" translate="yes" xml:space="preserve">
          <source>The number of duplicated features, drawn randomly from the informative and the redundant features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3756e321fd8c5762ae8ac31516f2ecb21110717" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the &lt;code&gt;grid&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b29bfd2ea8e3e1682dd4a79d1706f446dfb38a05" translate="yes" xml:space="preserve">
          <source>The number of equally spaced points on the axes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2df871c8f4e92cb59ed15324ccda38fad42ea75" translate="yes" xml:space="preserve">
          <source>The number of estimators as selected by early stopping (if &lt;code&gt;n_iter_no_change&lt;/code&gt; is specified). Otherwise it is set to &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c41aa32f1f0f6cf009b98065236eda5fafde67a" translate="yes" xml:space="preserve">
          <source>The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="586a175a91db906a71d554d2394ac768c54a011d" translate="yes" xml:space="preserve">
          <source>The number of features for each sample.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7aa2f8ce84af9869f7a766d49383cc26f37d08c4" translate="yes" xml:space="preserve">
          <source>The number of features has to be &amp;gt;= 5.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="129d21ac97bd672e4633231039dccb80b794a16c" translate="yes" xml:space="preserve">
          <source>The number of features to consider when looking for the best split:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bfa1c66757d06ac8d59fc4bf744ebce5057ba13" translate="yes" xml:space="preserve">
          <source>The number of features to draw from X to train each base estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d51bb9401f1843316f34a353f5592cb098582ce3" translate="yes" xml:space="preserve">
          <source>The number of features to select. If &lt;code&gt;None&lt;/code&gt;, half of the features are selected.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a2e8b8c9f83e59315eadeb30286733009f73579" translate="yes" xml:space="preserve">
          <source>The number of features to use. If None, it will be inferred from the maximum column index occurring in any of the files.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="98598b839c3ae52a8bef761a8c292e4971f87fa1" translate="yes" xml:space="preserve">
          <source>The number of features to use. If None, it will be inferred. This argument is useful to load several files that are subsets of a bigger sliced dataset: each subset might not have examples of every feature, hence the inferred shape might vary from one slice to another. n_features is only required if &lt;code&gt;offset&lt;/code&gt; or &lt;code&gt;length&lt;/code&gt; are passed a non-default value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="deca4fdfd1ca37b7e04bbcf3985c1b98fb8a3a95" translate="yes" xml:space="preserve">
          <source>The number of features when &lt;code&gt;fit&lt;/code&gt; is performed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="000ec70bd77b0bf4abd3e711fc085eb14c1166a9" translate="yes" xml:space="preserve">
          <source>The number of features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9910eba7c229f9255e2c9649c38205ffc855802c" translate="yes" xml:space="preserve">
          <source>The number of features. Should be at least 5.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b858bf37b4f474c4599d32f8587786621c82cda" translate="yes" xml:space="preserve">
          <source>The number of informative features, i.e., the number of features used to build the linear model used to generate the output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a77292033054c608772a804dab5a2e1fdf27c5c" translate="yes" xml:space="preserve">
          <source>The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices of a hypercube in a subspace of dimension &lt;code&gt;n_informative&lt;/code&gt;. For each cluster, informative features are drawn independently from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then placed on the vertices of the hypercube.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8860e037fbe039fe78d5e3c48f8e1848feb12312" translate="yes" xml:space="preserve">
          <source>The number of initializations to perform. The best results are kept.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51a26d62cae82295bdfa00f11021a221252f4d93" translate="yes" xml:space="preserve">
          <source>The number of initializations to perform. The result with the highest lower bound value on the likelihood is kept.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="38ff309a985e30ab2e29b9afc192f79c8b9a0c6a" translate="yes" xml:space="preserve">
          <source>The number of integer to sample.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a52a9529854ceea9a883fc2df829925e52c70f47" translate="yes" xml:space="preserve">
          <source>The number of iteration on data batches that has been performed before this call to partial_fit. This is optional: if no number is passed, the memory of the object is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3bc7b28196b3922c89415e845096f6cf78b8faf" translate="yes" xml:space="preserve">
          <source>The number of iterations corresponding to the best stress. Returned only if &lt;code&gt;return_n_iter&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="09e334ba47a4a0e3959de08638739eb9eaaab635" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by lars_path to find the grid of alphas for each target.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b1098a1922ae37c291de7345b0094c64c3a02834" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cb53eef7959113120386cfa7066166d0b269478f" translate="yes" xml:space="preserve">
          <source>The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha. (Is returned when &lt;code&gt;return_n_iter&lt;/code&gt; is set to True).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="33de6e55bb60c2bc5ac457a31b105deefc3f996b" translate="yes" xml:space="preserve">
          <source>The number of iterations the solver has ran.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db80deec4964c14c90bbb2ba0d38dab8d92c99f4" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for &lt;code&gt;fit&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="828e81cc3e32985aa18f25c0aa8cb224a18c0ff7" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c43ba6eb14ab669b0479493b6caf6c135c124b5c" translate="yes" xml:space="preserve">
          <source>The number of jobs to run in parallel for both &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;. &lt;code&gt;None`&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c706846846b90bc0cd14952c05d5f24ee8d014f" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6833984bce80d83c32632eedfcb38fdab54d1b8b" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. If multiple initializations are used (&lt;code&gt;n_init&lt;/code&gt;), each run of the algorithm is computed in parallel.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6691b8acedaea4810fafdb230140a5bee73c0f13" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. It does each target variable in y in parallel. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e3d27815195706836eb73a8c598c8129b01e46ca" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This will only provide speedup for n_targets &amp;gt; 1 and sufficient large problems. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aea0fe9213de8cdb23ff0f2fe182f38ade8bf1a8" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="355b7ccfb662137f73492d9f4ce45fbb16afbbbc" translate="yes" xml:space="preserve">
          <source>The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5a46de70d97dcbce1cd57e62c5d0b48ff30934b" translate="yes" xml:space="preserve">
          <source>The number of jobs to use in the E-step. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27ef8dc8b90e4adb5cf60c4b5861107cfc3fcd66" translate="yes" xml:space="preserve">
          <source>The number of leaves in the tree</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa280d8fa86c4668dcae50e829dc6fe4b82c5c09" translate="yes" xml:space="preserve">
          <source>The number of longitudes (x) and latitudes (y) in the grid</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c1079f8f5554d74d12065ae30d0d0d5ad6da869f" translate="yes" xml:space="preserve">
          <source>The number of mixture components.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1b5111fc1060a675cec3a1f3743c5b7235e4d74d" translate="yes" xml:space="preserve">
          <source>The number of mixture components. Depending on the data and the value of the &lt;code&gt;weight_concentration_prior&lt;/code&gt; the model can decide to not use all the components by setting some component &lt;code&gt;weights_&lt;/code&gt; to values very close to zero. The number of effective components is therefore smaller than n_components.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="add0998b97eff8ce13b181824a749694c3eae657" translate="yes" xml:space="preserve">
          <source>The number of nearest neighbors to return</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15fc684195ef8537dde92583b88b14e2ce9227a5" translate="yes" xml:space="preserve">
          <source>The number of neighbors considered (parameter n_neighbors) is typically set 1) greater than the minimum number of samples a cluster has to contain, so that other samples can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by samples that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="acccc5b01db683b034e704a8a9a779e161564526" translate="yes" xml:space="preserve">
          <source>The number of neighbors considered, (parameter n_neighbors) is typically set 1) greater than the minimum number of samples a cluster has to contain, so that other samples can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by samples that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="877763fdbf37d108bc07acba2c3c7a43a6b1f5d0" translate="yes" xml:space="preserve">
          <source>The number of occurrences of each label in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d77e127c483a951faddd4898060a91624b32c7e2" translate="yes" xml:space="preserve">
          <source>The number of outlying points matters, but also how much they are outliers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="28c74611a0fbfe9d7c9bc15166aba9285108f840" translate="yes" xml:space="preserve">
          <source>The number of outputs when &lt;code&gt;fit&lt;/code&gt; is performed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb79176b795c17c39b28af054f09df09e008175b" translate="yes" xml:space="preserve">
          <source>The number of outputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b09418506b739dbda708239c423400be69d20b7d" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f8c9b6b47de0b026f28768b5a0afa5c012cbf5fc" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7704c4671581e69d735cac67f4c8350e13fe7ef2" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Affects only &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt;&lt;code&gt;kneighbors&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;kneighbors_graph&lt;/code&gt;&lt;/a&gt; methods.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="361059c7cf2c1088b100ce86f03251d9b9b3606a" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d88911bb44ab8ec6ef4a44ece33bbbd3a1ce8ce" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run for neighbors search. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details. Doesn&amp;rsquo;t affect &lt;a href=&quot;#sklearn.neighbors.KNeighborsRegressor.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92152098131975c56771bce4e909262b46d5eb7d" translate="yes" xml:space="preserve">
          <source>The number of parallel jobs to run. &lt;code&gt;None&lt;/code&gt; means 1 unless in a &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend&quot;&gt;&lt;code&gt;joblib.parallel_backend&lt;/code&gt;&lt;/a&gt; context. &lt;code&gt;-1&lt;/code&gt; means using all processors. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-n-jobs&quot;&gt;Glossary&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99143dd67a79337f4ad9e3dbea2cd1e515a938df" translate="yes" xml:space="preserve">
          <source>The number of passes over the training data (aka epochs). Defaults to None. Deprecated, will be removed in 0.21.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec5cdfb884714451da411c4ff8cb1db87b4e7636" translate="yes" xml:space="preserve">
          <source>The number of redundant features. These features are generated as random linear combinations of the informative features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47b1c5caf88dbd07fabbf6968de5281c23c7d24f" translate="yes" xml:space="preserve">
          <source>The number of regression targets, i.e., the dimension of the y output vector associated with a sample. By default, the output is a scalar.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="338ec305a58ac58159822d262713a3e274e16037" translate="yes" xml:space="preserve">
          <source>The number of restarts of the optimizer for finding the kernel&amp;rsquo;s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel&amp;rsquo;s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer == 0 implies that one run is performed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4b3a5a4d2704e96fc904a7b55b3f1c5b2c8d507" translate="yes" xml:space="preserve">
          <source>The number of restarts of the optimizer for finding the kernel&amp;rsquo;s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel&amp;rsquo;s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer=0 implies that one run is performed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f64e7c256c29c0afec3f69dafbe77018e516c64" translate="yes" xml:space="preserve">
          <source>The number of row and column clusters in the checkerboard structure.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d47b9ac186fe69411b80e5cb5c0bc35de2b1552" translate="yes" xml:space="preserve">
          <source>The number of row and column clusters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a93f4e954bb39f85a0cd6723eb5913eb20116e8e" translate="yes" xml:space="preserve">
          <source>The number of sample points on the S curve.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e771006aa290a36177ef8a2fd15f85ed45a9f7ce" translate="yes" xml:space="preserve">
          <source>The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d8023265c3023688c589a292c9dfac5ffedb5a44" translate="yes" xml:space="preserve">
          <source>The number of samples drawn from the Gaussian process</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1589955effed900fd5766b276491de7c2d2b9bf4" translate="yes" xml:space="preserve">
          <source>The number of samples processed by the estimator for each feature. If there are not missing samples, the &lt;code&gt;n_samples_seen&lt;/code&gt; will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across &lt;code&gt;partial_fit&lt;/code&gt; calls.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5282c1e1c469ae21abb6fc766981198bf00b68c4" translate="yes" xml:space="preserve">
          <source>The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across &lt;code&gt;partial_fit&lt;/code&gt; calls.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1793fd9997ff1f0552981d710a775a55fe6d48a0" translate="yes" xml:space="preserve">
          <source>The number of samples to draw from X to train each base estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb3a932b7568c921848c0d1cfe6540980845aa8f" translate="yes" xml:space="preserve">
          <source>The number of samples to take in each batch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45c4c8ec08a2e1d782bf77a47114ec6ebd4dca5e" translate="yes" xml:space="preserve">
          <source>The number of samples to use for each batch. Only used when calling &lt;code&gt;fit&lt;/code&gt;. If &lt;code&gt;batch_size&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, then &lt;code&gt;batch_size&lt;/code&gt; is inferred from the data and set to &lt;code&gt;5 * n_features&lt;/code&gt;, to provide a balance between approximation accuracy and memory consumption.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc9506bec678834c1e3b7d03354e1ff3a3f08ddc" translate="yes" xml:space="preserve">
          <source>The number of samples to use. If not given, all samples are used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06d71b7513bf6cfbd6babc125146f20e54cecd08" translate="yes" xml:space="preserve">
          <source>The number of samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ed3bb3b4a6145be5f5af9755587163fa8bebbaa" translate="yes" xml:space="preserve">
          <source>The number of seconds contained in delta</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd2b02981da97aea3b937c6f113a2aa5413af4a0" translate="yes" xml:space="preserve">
          <source>The number of selected features with cross-validation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f83194da71427b41aa36e9d801ef17814b93c795" translate="yes" xml:space="preserve">
          <source>The number of selected features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ecf3b2d99c2ce29bc7f1b5f51d42517d75e68e85" translate="yes" xml:space="preserve">
          <source>The number of stages of the final model is available at the attribute &lt;code&gt;n_estimators_&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40db2965d8f58e28ab3da3567d512db3201d5fa4" translate="yes" xml:space="preserve">
          <source>The number of times the grid is refined. Not used if explicit values of alphas are passed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b5e4a652e0296cb387fe06bf82a965799e670b1" translate="yes" xml:space="preserve">
          <source>The number of trees in the forest.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b609a37dc2d7220a5b1e0ac4ad20a19f7617a90e" translate="yes" xml:space="preserve">
          <source>The number of weak learners (i.e. regression trees) is controlled by the parameter &lt;code&gt;n_estimators&lt;/code&gt;; &lt;a href=&quot;#gradient-boosting-tree-size&quot;&gt;The size of each tree&lt;/a&gt; can be controlled either by setting the tree depth via &lt;code&gt;max_depth&lt;/code&gt; or by setting the number of leaf nodes via &lt;code&gt;max_leaf_nodes&lt;/code&gt;. The &lt;code&gt;learning_rate&lt;/code&gt; is a hyper-parameter in the range (0.0, 1.0] that controls overfitting via &lt;a href=&quot;#gradient-boosting-shrinkage&quot;&gt;shrinkage&lt;/a&gt; .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68e68bbcb31faf3cd13be5ec90f20eecf860f6d7" translate="yes" xml:space="preserve">
          <source>The number of weak learners is controlled by the parameter &lt;code&gt;n_estimators&lt;/code&gt;. The &lt;code&gt;learning_rate&lt;/code&gt; parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the &lt;code&gt;base_estimator&lt;/code&gt; parameter. The main parameters to tune to obtain good results are &lt;code&gt;n_estimators&lt;/code&gt; and the complexity of the base estimators (e.g., its depth &lt;code&gt;max_depth&lt;/code&gt; or minimum required number of samples to consider a split &lt;code&gt;min_samples_split&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a6d1422bf72f5a61faa255a9a1207169e4a394c6" translate="yes" xml:space="preserve">
          <source>The object solves the same problem as the LassoCV object. However, unlike the LassoCV, it find the relevant alphas values by itself. In general, because of this property, it will be more stable. However, it is more fragile to heavily multicollinear datasets.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="909e016dde1f323cbfe3daf2401dd2bee2829e0c" translate="yes" xml:space="preserve">
          <source>The object to use to fit the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d5c8a5f63b0e142ed66ba0f0aa5318c460719fdf" translate="yes" xml:space="preserve">
          <source>The object&amp;rsquo;s &lt;code&gt;best_score_&lt;/code&gt; and &lt;code&gt;best_params_&lt;/code&gt; attributes store the best mean score and the parameters setting corresponding to that score:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="512dd720938772db73af76fb4221b6596459608c" translate="yes" xml:space="preserve">
          <source>The objective function is minimized with an alternating minimization of W and H.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a139874cf1d9e2c13462d6567d8563d658f0907b" translate="yes" xml:space="preserve">
          <source>The objective function is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf56422fecab64934517c1fdfbcaed66ab27df08" translate="yes" xml:space="preserve">
          <source>The objective function to minimize is in this case</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e546a5e9110cb4077ff5ab03d80b93cb6429070c" translate="yes" xml:space="preserve">
          <source>The observations are assumed to be caused by a linear transformation of lower dimensional latent factors and added Gaussian noise. Without loss of generality the factors are distributed according to a Gaussian with zero mean and unit covariance. The noise is also zero mean and has an arbitrary diagonal covariance matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f8b5be5464139b25f15f97e4d9d483501d55af35" translate="yes" xml:space="preserve">
          <source>The observations to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5af240b8e218ff237309779b56f83d5e52d1af7b" translate="yes" xml:space="preserve">
          <source>The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb71736c8c9acd5b1d75a4e7144f466f3b859a53" translate="yes" xml:space="preserve">
          <source>The obtained score is always strictly greater than 0 and the best value is 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7dea734e0307eec19c3b5341e6f81839af6082a" translate="yes" xml:space="preserve">
          <source>The one-vs-the-rest meta-classifier also implements a &lt;code&gt;predict_proba&lt;/code&gt; method, so long as such a method is implemented by the base classifier. This method returns probabilities of class membership in both the single label and multilabel case. Note that in the multilabel case, probabilities are the marginal probability that a given sample falls in the given class. As such, in the multilabel case the sum of these probabilities over all possible labels for a given sample &lt;em&gt;will not&lt;/em&gt; sum to unity, as they do in the single label case.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f36ee9570dbac199195d25256879fa51a751bbaa" translate="yes" xml:space="preserve">
          <source>The opposite LOF of the training samples. The higher, the more normal. Inliers tend to have a LOF score close to 1 (&lt;code&gt;negative_outlier_factor_&lt;/code&gt; close to -1), while outliers tend to have a larger LOF score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df3e454f1090a47509e70dbea510891595829b28" translate="yes" xml:space="preserve">
          <source>The opposite of the Local Outlier Factor of each input samples. The lower, the more abnormal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4312ae849a138f7db034f6fd7f5a1ea0b817b171" translate="yes" xml:space="preserve">
          <source>The optimal algorithm for a given dataset is a complicated choice, and depends on a number of factors:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e994897b226c2495f8cea3f1e46f2c7029c61954" translate="yes" xml:space="preserve">
          <source>The optimal lambda parameter for minimizing skewness is estimated on each feature independently using maximum likelihood.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="033713ef73311880bab79fd88513749d67a56824" translate="yes" xml:space="preserve">
          <source>The optimization objective for Lasso is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64214421bf615c105d2891f743437d06253a6ade" translate="yes" xml:space="preserve">
          <source>The optimization objective for MultiTaskElasticNet is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f43132c3f7097ed8020d37840c051e421e41e300" translate="yes" xml:space="preserve">
          <source>The optimization objective for MultiTaskLasso is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3bee4ea6ed3cad366ecf4d67dfc05469de43ad46" translate="yes" xml:space="preserve">
          <source>The optimization objective for the case method=&amp;rsquo;lasso&amp;rsquo; is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d89fe15a9f2092d4f8a7ef569d775bf0279d26d" translate="yes" xml:space="preserve">
          <source>The optional extra argument will be appended to the deprecation message and the docstring. Note: to use this with the default value for extra, put in an empty of parentheses:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c86ba504c90ec1412c0ccc3d7f0b2f074294dc1" translate="yes" xml:space="preserve">
          <source>The optional parameter &lt;code&gt;whiten=True&lt;/code&gt; makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="846ba284e005e0c78a1fd443a812172705bbc8f3" translate="yes" xml:space="preserve">
          <source>The order of labels in the classifier chain.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b082acdda498538bd71b7e32b0b230c2144c284" translate="yes" xml:space="preserve">
          <source>The order of the chain can be explicitly set by providing a list of integers. For example, for a chain of length 5.:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9d11a930ecaf4c38c05b12592342da86a8c77ff" translate="yes" xml:space="preserve">
          <source>The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the &lt;code&gt;transformers&lt;/code&gt; list. Columns of the original feature matrix that are not specified are dropped from the resulting transformed feature matrix, unless specified in the &lt;code&gt;passthrough&lt;/code&gt; keyword. Those columns specified with &lt;code&gt;passthrough&lt;/code&gt; are added at the right to the output of the transformers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b5c0a29228cf2f99af9ecdff2f5b2a0dc08ed2d" translate="yes" xml:space="preserve">
          <source>The original data</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="77422b337aacebf704cab947c2765e7ef78426db" translate="yes" xml:space="preserve">
          <source>The original dataset consisted of 92 x 112, while the version available here consists of 64x64 images.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11407c8a67a7b9eefbd2b65794ca1ecbfa48a97d" translate="yes" xml:space="preserve">
          <source>The original formulation of the hashing trick by Weinberger et al. used two separate hash functions \(h\) and \(\xi\) to determine the column index and sign of a feature, respectively. The present implementation works under the assumption that the sign bit of MurmurHash3 is independent of its other bits.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8eec938f1b6ddec4315f3fac08b8dc8e8e67d5d5" translate="yes" xml:space="preserve">
          <source>The original images are 250 x 250 pixels, but the default slice and resize arguments reduce them to 62 x 47.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a555f40ab758aba55bcba7bb9ac36af67b065d6a" translate="yes" xml:space="preserve">
          <source>The other kernels</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5978e5bbc3d0aa56b4c3321d1d9ccbd8b149a1a9" translate="yes" xml:space="preserve">
          <source>The outer product of the row and column label vectors shows a representation of the checkerboard structure.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4e7d9f946f101134a65fd7d38a82dce953516bd7" translate="yes" xml:space="preserve">
          <source>The output &lt;code&gt;y&lt;/code&gt; is created according to the formula:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7348f35a4e7d1a3450461b231ee28ef95517ba57" translate="yes" xml:space="preserve">
          <source>The output is generated by applying a (potentially biased) random linear regression model with &lt;code&gt;n_informative&lt;/code&gt; nonzero regressors to the previously generated input and some gaussian centered noise with some adjustable scale.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c62eccd54e33fb989ef31175162303c11637d7c" translate="yes" xml:space="preserve">
          <source>The output of a singular value decomposition is only unique up to a permutation of the signs of the singular vectors. If &lt;code&gt;flip_sign&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, the sign ambiguity is resolved by making the largest loadings for each component in the left singular vectors positive.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab5b2ab18fdef999b51de35c5e9c33e7d3ac7cc7" translate="yes" xml:space="preserve">
          <source>The output of the 3 models are combined in a 2D graph where nodes represents the stocks and edges the:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c9b52f3c910ded5afcb915db265ed0583f446660" translate="yes" xml:space="preserve">
          <source>The output of transform is sometimes referred to as the 1-of-K coding scheme.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f172299244e465e38a0859a0349f26df08f75701" translate="yes" xml:space="preserve">
          <source>The output of transform is sometimes referred to by some authors as the 1-of-K coding scheme.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58983c4b722f7fa5c156e992e47b9ca634d33156" translate="yes" xml:space="preserve">
          <source>The output values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f348d2a86dcf3bc20a82250a4d5068aa50c67aad" translate="yes" xml:space="preserve">
          <source>The overall complexity of Isomap is \(O[D \log(k) N \log(N)] + O[N^2(k + \log(N))] + O[d N^2]\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9cec4ae56de9cf1cd9cf2f8a0ff2e2e24864f054" translate="yes" xml:space="preserve">
          <source>The overall complexity of MLLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[N (k-D) k^2] + O[d N^2]\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7fa546000c6a79308906d0c040bf81047f6b149e" translate="yes" xml:space="preserve">
          <source>The overall complexity of spectral embedding is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="808f28c633edaf7537ca8395b6bc52f0086ed496" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard HLLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[N d^6] + O[d N^2]\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="63617858af3d41882021a1ab37e78e76cce39983" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard LLE is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b88c53c3806a592f7e00e19aef010a599cfaf4dc" translate="yes" xml:space="preserve">
          <source>The overall complexity of standard LTSA is \(O[D \log(k) N \log(N)] + O[D N k^3] + O[k^2 d] + O[d N^2]\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc12a97c8a559f9672542a072947ef2eae0adcac" translate="yes" xml:space="preserve">
          <source>The p-value, which approximates the probability that the score would be obtained by chance. This is calculated as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f9aaf22278cea2b541fbd8b88b09de54aad3e99" translate="yes" xml:space="preserve">
          <source>The parallel version of K-Means is broken on OS X when &lt;code&gt;numpy&lt;/code&gt; uses the &lt;code&gt;Accelerate&lt;/code&gt; Framework. This is expected behavior: &lt;code&gt;Accelerate&lt;/code&gt; can be called after a fork but you need to execv the subprocess with the Python binary (which multiprocessing does not do under posix).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26cab4ba2ce5f7f1aa5cff1bedcda88264af63ea" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;learning_rate&lt;/code&gt; strongly interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;, the number of weak learners to fit. Smaller values of &lt;code&gt;learning_rate&lt;/code&gt; require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of &lt;code&gt;learning_rate&lt;/code&gt; favor better test error. &lt;a href=&quot;#htf2009&quot; id=&quot;id17&quot;&gt;[HTF2009]&lt;/a&gt; recommend to set the learning rate to a small constant (e.g. &lt;code&gt;learning_rate &amp;lt;= 0.1&lt;/code&gt;) and choose &lt;code&gt;n_estimators&lt;/code&gt; by early stopping. For a more detailed discussion of the interaction between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt; see &lt;a href=&quot;#r2007&quot; id=&quot;id18&quot;&gt;[R2007]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e6ab9c66156a9d2feae55ed2060a2c6f7d6986c" translate="yes" xml:space="preserve">
          <source>The parameter &lt;code&gt;memory&lt;/code&gt; is needed in order to cache the transformers. &lt;code&gt;memory&lt;/code&gt; can be either a string containing the directory where to cache the transformers or a &lt;a href=&quot;https://pythonhosted.org/joblib/memory.html&quot;&gt;joblib.Memory&lt;/a&gt; object:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a19846a3c7376460662acabedd23579aba492f87" translate="yes" xml:space="preserve">
          <source>The parameter \(\nu\) is also called the &lt;strong&gt;learning rate&lt;/strong&gt; because it scales the step length the gradient descent procedure; it can be set via the &lt;code&gt;learning_rate&lt;/code&gt; parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="76b14eb14f0e0be16176b9f0182e58f523e33b2d" translate="yes" xml:space="preserve">
          <source>The parameter epsilon controls the number of samples that should be classified as outliers. The smaller the epsilon, the more robust it is to outliers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="33a23a95b6f6e50b11cce3982dee22dc3afbeaef" translate="yes" xml:space="preserve">
          <source>The parameter grid to explore, as a dictionary mapping estimator parameters to sequences of allowed values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="95ad4f92539995843fbdd8a8c8263fdd9652cae5" translate="yes" xml:space="preserve">
          <source>The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. More specifically, the optimization objective is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="444f64a3bffea2bee3d3973fdc03c3dbbbbf54d9" translate="yes" xml:space="preserve">
          <source>The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio &amp;lt;= 0.01 is not reliable, unless you supply your own sequence of alpha.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3d75707f4ee017e7f35982b539c7d0e6825a3d30" translate="yes" xml:space="preserve">
          <source>The parameter nu controlling the smoothness of the learned function. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions). Note that values of nu not in [0.5, 1.5, 2.5, inf] incur a considerably higher computational cost (appr. 10 times higher) since they require to evaluate the modified Bessel function. Furthermore, in contrast to l, nu is kept fixed to its initial value and not optimized.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa3c09390eafe53f0b58881f0d7db646d5796fe2" translate="yes" xml:space="preserve">
          <source>The parameters \(\sigma_y\) and \(\mu_y\) are estimated using maximum likelihood.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d8334dfb20de589f58500a915aef89a4fab7377d" translate="yes" xml:space="preserve">
          <source>The parameters \(\theta_y\) is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="162d2ed9f70c881f87134a87c0c741cb23832c22" translate="yes" xml:space="preserve">
          <source>The parameters implementation of the &lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt;&lt;code&gt;BayesianGaussianMixture&lt;/code&gt;&lt;/a&gt; class proposes two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="957eda9a79d6f1f87ea7b634983b57c94627c96b" translate="yes" xml:space="preserve">
          <source>The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f20d8d61d83d6376fa9caf869f91f4640e0cc5b" translate="yes" xml:space="preserve">
          <source>The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d4a34fa151b21edbbd9fa634476deabc1cb44cf" translate="yes" xml:space="preserve">
          <source>The parameters of the power transformation for the selected features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="62f294aa209942b08fddf4e74d29c13f78a9e67d" translate="yes" xml:space="preserve">
          <source>The parameters selected are those that maximize the score of the held-out data, according to the scoring parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6d7555a36c15ae2ffe9751024a0eebcf2421d57" translate="yes" xml:space="preserve">
          <source>The parameters selected are those that maximize the score of the left out data, unless an explicit score is passed in which case it is used instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34d594dbc00b197d06cf788b61a5dfe36db335bf" translate="yes" xml:space="preserve">
          <source>The parameters that have been evaluated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f139c5090bf57fe74c58422c7266093265927a67" translate="yes" xml:space="preserve">
          <source>The parent of each node. Only returned when a connectivity matrix is specified, elsewhere &amp;lsquo;None&amp;rsquo; is returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89bfd729c83f5c14808628511ada19e6e2b220e5" translate="yes" xml:space="preserve">
          <source>The partial dependence function evaluated on the &lt;code&gt;grid&lt;/code&gt;. For regression and binary classification &lt;code&gt;n_classes==1&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="053dbcb3edc2e230ef9a4eff0313b65bae4689ca" translate="yes" xml:space="preserve">
          <source>The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter &lt;code&gt;C&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf6ba8c5f2e782ae102fc993ff0f91af87853f26" translate="yes" xml:space="preserve">
          <source>The path of the base directory to use as a data store or None. If None is given, no caching is done and the Memory object is completely transparent. This option replaces cachedir since version 0.12.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e60c1638cc188f171e30720c51f9233ac8e0591d" translate="yes" xml:space="preserve">
          <source>The path to scikit-learn data dir.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="780a6be1d44fa6c4bc32e0e6a573d3b4ad24b942" translate="yes" xml:space="preserve">
          <source>The penalty (aka regularization term) to be used. Defaults to &amp;lsquo;l2&amp;rsquo; which is the standard regularizer for linear SVM models. &amp;lsquo;l1&amp;rsquo; and &amp;lsquo;elasticnet&amp;rsquo; might bring sparsity to the model (feature selection) not achievable with &amp;lsquo;l2&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5ec62ab0e251a48390ce125b16975a3fa4c7ca91" translate="yes" xml:space="preserve">
          <source>The penalty (aka regularization term) to be used. Defaults to None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="195880b4735384ca5f4a3196f2d3109be0fc5155" translate="yes" xml:space="preserve">
          <source>The performance is slightly worse for the randomized search, though this is most likely a noise effect and would not carry over to a held-out test set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66758ccad6c0faa66ee36e2739cca75d7cb80119" translate="yes" xml:space="preserve">
          <source>The performance measure reported by &lt;em&gt;k&lt;/em&gt;-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b50299f6e3a476dff9e3b98d46cee16ed70fcdb" translate="yes" xml:space="preserve">
          <source>The performance of the SAMME and SAMME.R &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt; algorithms are compared. SAMME.R uses the probability estimates to update the additive model, while SAMME uses the classifications only. As the example illustrates, the SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations. The error of each algorithm on the test set after each boosting iteration is shown on the left, the classification error on the test set of each tree is shown in the middle, and the boost weight of each tree is shown on the right. All trees have a weight of one in the SAMME.R algorithm and therefore are not shown.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f1cd52ca3b14d7125644c3724ffbf78c03b2ab3" translate="yes" xml:space="preserve">
          <source>The performance of the selected hyper-parameters and trained model is then measured on a dedicated evaluation set that was not used during the model selection step.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd106efb0f7012bff421a0ad8f3697f8283515ed" translate="yes" xml:space="preserve">
          <source>The periodicity of the kernel.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5dce1e0dbd7277c2f73689bcac70f69df5d2fc02" translate="yes" xml:space="preserve">
          <source>The perplexity is defined as \(k=2^{(S)}\) where \(S\) is the Shannon entropy of the conditional probability distribution. The perplexity of a \(k\)-sided die is \(k\), so that \(k\) is effectively the number of nearest neighbors t-SNE considers when generating the conditional probabilities. Larger perplexities lead to more nearest neighbors and less sensitive to small structure. Conversely a lower perplexity considers a smaller number of neighbors, and thus ignores more global information in favour of the local neighborhood. As dataset sizes get larger more points will be required to get a reasonable sample of the local neighborhood, and hence larger perplexities may be required. Similarly noisier datasets will require larger perplexity values to encompass enough local neighbors to see beyond the background noise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08299376158415917838ffdf1e208cdfe76446d4" translate="yes" xml:space="preserve">
          <source>The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. The choice is not extremely critical since t-SNE is quite insensitive to this parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3d1257b149fa9d88b1966c8dbdc55bf21ea6a0db" translate="yes" xml:space="preserve">
          <source>The placeholder for the missing values. All occurrences of &lt;code&gt;missing_values&lt;/code&gt; will be imputed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c95ae315d785709e445ae217c5567fb1ce65f07" translate="yes" xml:space="preserve">
          <source>The placeholder for the missing values. All occurrences of &lt;code&gt;missing_values&lt;/code&gt; will be imputed. For missing values encoded as np.nan, use the string value &amp;ldquo;NaN&amp;rdquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50d8f581357d70f0923b3a8bf25734f11fbbdc88" translate="yes" xml:space="preserve">
          <source>The plot represents the learning curve of the classifier: the evolution of classification accuracy over the course of the mini-batches. Accuracy is measured on the first 1000 samples, held out as a validation set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2108780ce5e305bd6eefeccab3f1f12e712db7ba" translate="yes" xml:space="preserve">
          <source>The plot shows decision boundaries for Linear Discriminant Analysis and Quadratic Discriminant Analysis. The bottom row demonstrates that Linear Discriminant Analysis can only learn linear boundaries, while Quadratic Discriminant Analysis can learn quadratic boundaries and is therefore more flexible.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17f022b1b904616a8feb650bfceadf60a7908c45" translate="yes" xml:space="preserve">
          <source>The plot shows four one-way and one two-way partial dependence plots. The target variables for the one-way PDP are: median income (&lt;code&gt;MedInc&lt;/code&gt;), avg. occupants per household (&lt;code&gt;AvgOccup&lt;/code&gt;), median house age (&lt;code&gt;HouseAge&lt;/code&gt;), and avg. rooms per household (&lt;code&gt;AveRooms&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d1f9d8f4302df476eee3089e3160330e3191c729" translate="yes" xml:space="preserve">
          <source>The plot shows the regions where the discretized encoding is constant.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a1523290796b6a8ba4024eaf4b48817249c66e13" translate="yes" xml:space="preserve">
          <source>The plots below illustrate the effect the parameter &lt;code&gt;C&lt;/code&gt; has on the separation line. A large value of &lt;code&gt;C&lt;/code&gt; basically tells our model that we do not have that much faith in our data&amp;rsquo;s distribution, and will only consider points close to line of separation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="faaabc25a5f9dd9badc9ef9f18d9770e507882de" translate="yes" xml:space="preserve">
          <source>The plots display firstly what a K-means algorithm would yield using three clusters. It is then shown what the effect of a bad initialization is on the classification process: By setting n_init to only 1 (default is 10), the amount of times that the algorithm will be run with different centroid seeds is reduced. The next plot displays what using eight clusters would deliver and finally the ground truth.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ad47df403d87eb821e2edd090b9e74720bc0be8" translate="yes" xml:space="preserve">
          <source>The plots represent the distribution of the prediction latency as a boxplot.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee583f2ef106ae04159c6d135c18b8bf01049699" translate="yes" xml:space="preserve">
          <source>The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a10b5c6300ecb650c0782e5a6bff1ffc576100bb" translate="yes" xml:space="preserve">
          <source>The point cloud spanned by the observations above is very flat in one direction: one of the three univariate features can almost be exactly computed using the other two. PCA finds the directions in which the data is not &lt;em&gt;flat&lt;/em&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9874b880b8ee39ca50e864dab01006b5213a2351" translate="yes" xml:space="preserve">
          <source>The points.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0cdbbfd6a3924811880d5b83f6e26dafaaff0147" translate="yes" xml:space="preserve">
          <source>The polynomial kernel is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b34b53d18875cd269c1341b177c3bcbb9230141" translate="yes" xml:space="preserve">
          <source>The pooled values for each feature cluster.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6285f4c6cbbb9a8676e04ca09291d710e5d4992f" translate="yes" xml:space="preserve">
          <source>The possible options are &amp;lsquo;hinge&amp;rsquo;, &amp;lsquo;log&amp;rsquo;, &amp;lsquo;modified_huber&amp;rsquo;, &amp;lsquo;squared_hinge&amp;rsquo;, &amp;lsquo;perceptron&amp;rsquo;, or a regression loss: &amp;lsquo;squared_loss&amp;rsquo;, &amp;lsquo;huber&amp;rsquo;, &amp;lsquo;epsilon_insensitive&amp;rsquo;, or &amp;lsquo;squared_epsilon_insensitive&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a756ae9d1d42224e6a27c29b135c093bd570bbe" translate="yes" xml:space="preserve">
          <source>The power of the Minkowski metric to be used to calculate distance between points.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="581738f7dfe64a35233afe8207a1e82379bfb8cd" translate="yes" xml:space="preserve">
          <source>The power transform is useful as a transformation in modeling problems where homoscedasticity and normality are desired. Below are examples of Box-Cox and Yeo-Johnwon applied to six different probability distributions: Lognormal, Chi-squared, Weibull, Gaussian, Uniform, and Bimodal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="837e2e1ea652ea6696a91670a638bda69523a446" translate="yes" xml:space="preserve">
          <source>The power transform method. Available methods are:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1605256f2d1d73782f41770777e3757d50960cf1" translate="yes" xml:space="preserve">
          <source>The power transform method. Currently, &amp;lsquo;box-cox&amp;rsquo; (Box-Cox transform) is the only option available.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e3ef4e072a07501cd2ec598f0a1011b6178f209" translate="yes" xml:space="preserve">
          <source>The precision is the ratio &lt;code&gt;tp / (tp + fp)&lt;/code&gt; where &lt;code&gt;tp&lt;/code&gt; is the number of true positives and &lt;code&gt;fp&lt;/code&gt; the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e318fd9419ebb0c7685cd2fcf271eb0864ec5bb" translate="yes" xml:space="preserve">
          <source>The precision matrices for each component in the mixture. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="027e42693446de348f6d01928068eb7453ef8a97" translate="yes" xml:space="preserve">
          <source>The precision matrix associated to the current covariance object.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d78de957c617714f761992022534f1c5cbc37dc0" translate="yes" xml:space="preserve">
          <source>The precision of each components on the mean distribution (Gaussian).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="208aaa08d6ec7d945a950063ea09933d0bd5ac66" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extend to where means can be placed. Smaller values concentrate the means of each clusters around &lt;code&gt;mean_prior&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b72296dd4015be0343e22e7dc45bdf0f7e18f582" translate="yes" xml:space="preserve">
          <source>The precision prior on the mean distribution (Gaussian). Controls the extend to where means can be placed. Smaller values concentrate the means of each clusters around &lt;code&gt;mean_prior&lt;/code&gt;. The value of the parameter must be greater than 0. If it is None, it&amp;rsquo;s set to 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="03d920e00078b8bd9b4faa0b1aa14a8c65b257bb" translate="yes" xml:space="preserve">
          <source>The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b03d17dd4f0a50b42b8760dd1442678e73495423" translate="yes" xml:space="preserve">
          <source>The predicted class C for each sample in X is returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bfeb54e7bff0bb15dd098a7327cddb86a1801899" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the base estimators in the ensemble.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd67555dc49cf41a5e5df91097299e3752772d2d" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the trees in the forest.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eab6e37cbb9069ff9afad75097cea6fc5d34926b" translate="yes" xml:space="preserve">
          <source>The predicted class log-probabilities of an input sample is computed as the weighted mean predicted class log-probabilities of the classifiers in the ensemble.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47d729548b0a8e226d9e4c530c0297fbe0f5665f" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability estimates. That is, the predicted class is the one with highest mean probability estimate across the trees.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="228bf14c189aaaaad2dc0e65c7c1dff58773904b" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is computed as the class with the highest mean predicted probability. If base estimators do not implement a &lt;code&gt;predict_proba&lt;/code&gt; method, then it resorts to voting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8652ca51db9ef5a2b15410ff134f217292f6ef55" translate="yes" xml:space="preserve">
          <source>The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="46c6bfcd5571fcbc8ac1e94c5ec5ef7b9ca3bcfc" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample are computed as the mean predicted class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same class in a leaf.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="90e3cd15b2277da446a86ed04d9b97d9385dbcbd" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample is computed as the mean predicted class probabilities of the base estimators in the ensemble. If base estimators do not implement a &lt;code&gt;predict_proba&lt;/code&gt; method, then it resorts to voting and the predicted class probabilities of an input sample represents the proportion of estimators predicting each class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f43014d849d28fe556560f6e74f971c02171e223" translate="yes" xml:space="preserve">
          <source>The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="232fb255d370f3424586a7b0c3f74d049bd6d607" translate="yes" xml:space="preserve">
          <source>The predicted class probability is the fraction of samples of the same class in a leaf.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45d2cef0bd7682bfccc693d1957f2c12cdb9bae8" translate="yes" xml:space="preserve">
          <source>The predicted class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d01b8d940e0e34c3c7942798bc90fe03e260970d" translate="yes" xml:space="preserve">
          <source>The predicted classes, or the predict values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b68f3b27cbad35418e1a35aab9bbe1837a195e37" translate="yes" xml:space="preserve">
          <source>The predicted classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9c849804a5f2e4c77a6bd9f1a72aeb60a174b11" translate="yes" xml:space="preserve">
          <source>The predicted log-probability of the sample for each class in the model, where classes are ordered as they are in &lt;code&gt;self.classes_&lt;/code&gt;. Equivalent to log(predict_proba(X))</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b73f981e520b253c83fbe81831bba280a3db569a" translate="yes" xml:space="preserve">
          <source>The predicted probability of the sample for each class in the model, where classes are ordered as they are in &lt;code&gt;self.classes_&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a286cd68d524dde2fbaba23c990f981ec35b78f3" translate="yes" xml:space="preserve">
          <source>The predicted probas.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="53b10e885ebd4a72b834dc0bd6649d47d09469b6" translate="yes" xml:space="preserve">
          <source>The predicted regression target of an input sample is computed as the mean predicted regression targets of the estimators in the ensemble.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d575d8b045eb46db33f9cbaec364a954b218830a" translate="yes" xml:space="preserve">
          <source>The predicted regression target of an input sample is computed as the mean predicted regression targets of the trees in the forest.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21875214f30fbe829fb7e391b984beb01fcc0748" translate="yes" xml:space="preserve">
          <source>The predicted regression value of an input sample is computed as the weighted median prediction of the classifiers in the ensemble.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d728cdaebb39fe6a42651804a843b92d06b095fc" translate="yes" xml:space="preserve">
          <source>The predicted regression values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb2e0fb384bae49f48977b71692091d27df9ee48" translate="yes" xml:space="preserve">
          <source>The predicted target values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16c2ec05bdd825f5e946bffd1661391a4efc1866" translate="yes" xml:space="preserve">
          <source>The predicted value of the input samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71f7a8826bad533ae16d312cbce730009c206751" translate="yes" xml:space="preserve">
          <source>The predicted values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3f70f498146a996702d8957eebe13ff93b2e60c" translate="yes" xml:space="preserve">
          <source>The prediction interpolates the observations (at least for regular kernels).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a5a460e70a5f18f6e563f520727c4e907b64c8a" translate="yes" xml:space="preserve">
          <source>The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and decide based on those if one should refit (online fitting, adaptive fitting) the prediction in some region of interest.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22369282bb204be005e939800f6b06d8c430453c" translate="yes" xml:space="preserve">
          <source>The previously introduced metrics are &lt;strong&gt;not normalized with regards to random labeling&lt;/strong&gt;: this means that depending on the number of samples, clusters and ground truth classes, a completely random labeling will not always yield the same values for homogeneity, completeness and hence v-measure. In particular &lt;strong&gt;random labeling won&amp;rsquo;t yield zero scores especially when the number of clusters is large&lt;/strong&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd0e1bbfb34ee27f92c4a3d8c327812d7edee1f8" translate="yes" xml:space="preserve">
          <source>The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as &lt;em&gt;non-generalizing&lt;/em&gt; machine learning methods, since they simply &amp;ldquo;remember&amp;rdquo; all of its training data (possibly transformed into a fast indexing structure such as a &lt;a href=&quot;#ball-tree&quot;&gt;Ball Tree&lt;/a&gt; or &lt;a href=&quot;#kd-tree&quot;&gt;KD Tree&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="947ce8b8522668844673470c4e6efaad3bb4bafb" translate="yes" xml:space="preserve">
          <source>The prior and posterior of a GP resulting from a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt;&lt;code&gt;RationalQuadratic&lt;/code&gt;&lt;/a&gt; kernel are shown in the following figure:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="43f31a8e2502a5518bdd46434b1800e86463052e" translate="yes" xml:space="preserve">
          <source>The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in the following figure:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f92d48b9507eee6a0b35b438f1fb3d6ff89e93fd" translate="yes" xml:space="preserve">
          <source>The prior of the number of degrees of freedom on the covariance distributions (Wishart).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f920ca641ad93ad7a821ae4139b67430b9eddb8f" translate="yes" xml:space="preserve">
          <source>The prior of the number of degrees of freedom on the covariance distributions (Wishart). If it is None, it&amp;rsquo;s set to &lt;code&gt;n_features&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf6f1da4c11f5b6aa97c72a194e67d10417600f3" translate="yes" xml:space="preserve">
          <source>The prior on the covariance distribution (Wishart). If it is None, the emiprical covariance prior is initialized using the covariance of X. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="449bf6ea1f50ae651db1aabfda8187878d697851" translate="yes" xml:space="preserve">
          <source>The prior on the covariance distribution (Wishart). The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7d0d50e2fe2007735b69660533329d841055128" translate="yes" xml:space="preserve">
          <source>The prior on the mean distribution (Gaussian).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70a81456bc5946b8879a5b610e7810c1053668bd" translate="yes" xml:space="preserve">
          <source>The prior on the mean distribution (Gaussian). If it is None, it&amp;rsquo;s set to the mean of X.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c84abbb5dade5fc9d4b47758b34408cb97bc08f" translate="yes" xml:space="preserve">
          <source>The priors over \(\alpha\) and \(\lambda\) are chosen to be &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_distribution&quot;&gt;gamma distributions&lt;/a&gt;, the conjugate prior for the precision of the Gaussian.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69c3bd38e0a5cd7a1c6a9d7a1b4382f7891ca245" translate="yes" xml:space="preserve">
          <source>The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a547056b7368b638d698d05d3f24b68fc3e86df" translate="yes" xml:space="preserve">
          <source>The probability of each class being drawn. Only returned if &lt;code&gt;return_distributions=True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91c6b31a68e2490c2f85bd0b221deab07bc16086" translate="yes" xml:space="preserve">
          <source>The probability of each feature being drawn given each class. Only returned if &lt;code&gt;return_distributions=True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0b4e863c306f0bcfef6f345210b44e3fb52ffa9" translate="yes" xml:space="preserve">
          <source>The probability that a coefficient is zero (see notes). Larger values enforce more sparsity.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4678dc803cddccad6a08944794f2a1c194908b2a" translate="yes" xml:space="preserve">
          <source>The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c819ed9fb97cd33099d0eff9842389e345641fd" translate="yes" xml:space="preserve">
          <source>The problem solved in clustering</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c5b556d82e8f03aa8505b7b204354187717fb43" translate="yes" xml:space="preserve">
          <source>The problem solved in supervised learning</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab484ff296e26d980b5f93762f848e40818065e8" translate="yes" xml:space="preserve">
          <source>The progress meter: the higher the value of &lt;code&gt;verbose&lt;/code&gt;, the more messages:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4163ad4952f27df98667c245d9d8133cfc5f4bae" translate="yes" xml:space="preserve">
          <source>The project mailing list</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7689a2c8d3ddf3814b537c25de9417bcceff1dc" translate="yes" xml:space="preserve">
          <source>The projected data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1179732ddee68f2a030de922cca4a7f3acc9e816" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. Default is None, which implies that the minimum value of support_fraction will be used within the algorithm: [n_sample + n_features + 1] / 2</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f706b185c5fa578abf1be5586afd62f084c2356" translate="yes" xml:space="preserve">
          <source>The proportion of points to be included in the support of the raw MCD estimate. If None, the minimum value of support_fraction will be used within the algorithm: &lt;code&gt;[n_sample + n_features + 1] / 2&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c5ef332af0dc668a636cc813bd9c0d699622c033" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if &lt;code&gt;n_iter_no_change&lt;/code&gt; is set to an integer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e7040634736e41a179ebec81405480b75c6b1afc" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22eea34be82cd504d8c4cf88134dfff15444db48" translate="yes" xml:space="preserve">
          <source>The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b21c544ac1d6b2edc3be7d45619235a28b43e4c" translate="yes" xml:space="preserve">
          <source>The proportions of samples assigned to each class. If None, then classes are balanced. Note that if &lt;code&gt;len(weights) == n_classes - 1&lt;/code&gt;, then the last class weight is automatically inferred. More than &lt;code&gt;n_samples&lt;/code&gt; samples may be returned if the sum of &lt;code&gt;weights&lt;/code&gt; exceeds 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="824b6a86b9524b4fdb9e2919749fc7db8e534162" translate="yes" xml:space="preserve">
          <source>The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a &amp;lsquo;__&amp;rsquo;, as in the example below. A step&amp;rsquo;s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting to None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd8f7005fb1918c1319c0b334e68b25127992a8d" translate="yes" xml:space="preserve">
          <source>The python source code used to generate the model</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f7347085b6a9f16c7df450dca9dbc878bc14f5cc" translate="yes" xml:space="preserve">
          <source>The quantile to predict using the &amp;ldquo;quantile&amp;rdquo; strategy. A quantile of 0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the maximum.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="38be089aaf53753add8a6e12d9d6e104537de3bf" translate="yes" xml:space="preserve">
          <source>The quantity that we use is the daily variation in quote price: quotes that are linked tend to cofluctuate during a day.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="96cf03fa14346bfc08bd6f97110fef23b56f72d1" translate="yes" xml:space="preserve">
          <source>The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="30a7cdee7bb9697635b16e9d03b7cdd4b400cabd" translate="yes" xml:space="preserve">
          <source>The query sample or samples to compute the Local Outlier Factor w.r.t. the training samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9edc4411fbea590103c5860156a749b07db92a2" translate="yes" xml:space="preserve">
          <source>The query sample or samples to compute the Local Outlier Factor w.r.t. to the training samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f63e59d1f59e9a5989ccb3ba903c403d9c311f4" translate="yes" xml:space="preserve">
          <source>The radius of the subcluster obtained by merging a new sample and the closest subcluster should be lesser than the threshold. Otherwise a new subcluster is started. Setting this value to be very low promotes splitting and vice-versa.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c1fb03bbf68de1d40f0cd10af0722ed019408483" translate="yes" xml:space="preserve">
          <source>The random forest regressor will only ever predict values within the range of observations or closer to zero for each of the targets. As a result the predictions are biased towards the centre of the circle.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff0bc6a79a727353502babbe6e55a993a0f80508" translate="yes" xml:space="preserve">
          <source>The random number generator is used to generate random chain orders.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31617e37a4673ca35baf50a5963330e598289ccc" translate="yes" xml:space="preserve">
          <source>The random symmetric, positive-definite matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ae670050b82bcbccc27a159ae39c91afa76e218" translate="yes" xml:space="preserve">
          <source>The randomized search and the grid search explore exactly the same space of parameters. The result in parameter settings is quite similar, while the run time for randomized search is drastically lower.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f118fa702c125f64290c65f38abc4f6dddff279" translate="yes" xml:space="preserve">
          <source>The raw (unadjusted) Rand index is then given by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b2fe9bd420d4a6948923a1a40e37c6224028547" translate="yes" xml:space="preserve">
          <source>The raw RI score is then &amp;ldquo;adjusted for chance&amp;rdquo; into the ARI score using the following scheme:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="884b3f9d013732b636db9c9b452d7d3559d50f2d" translate="yes" xml:space="preserve">
          <source>The raw robust estimated covariance before correction and re-weighting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70b59f0ad9598471a02599d6a34b12e103ef557b" translate="yes" xml:space="preserve">
          <source>The raw robust estimated location before correction and re-weighting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="246acb046d6b9bd62c3702633fac1169a8d836d0" translate="yes" xml:space="preserve">
          <source>The real data lies in the &lt;code&gt;filenames&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; attributes. The target attribute is the integer index of the category:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="480e842bb5c6e42d98f06aa4f6c096c0c7aba356" translate="yes" xml:space="preserve">
          <source>The recall is the ratio &lt;code&gt;tp / (tp + fn)&lt;/code&gt; where &lt;code&gt;tp&lt;/code&gt; is the number of true positives and &lt;code&gt;fn&lt;/code&gt; the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65f6a86aee18eed07b01f195c73d746d5f2ca909" translate="yes" xml:space="preserve">
          <source>The reconstructed points using the metric MDS and non metric MDS are slightly shifted to avoid overlapping.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cefc093c3489c62b159a3ce090f6d8b10ecaa3b1" translate="yes" xml:space="preserve">
          <source>The reconstruction error computed by each routine can be used to choose the optimal output dimension. For a \(d\)-dimensional manifold embedded in a \(D\)-dimensional parameter space, the reconstruction error will decrease as &lt;code&gt;n_components&lt;/code&gt; is increased until &lt;code&gt;n_components == d&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd5de02ec688b7a05331073a7d6e7032a5c96c24" translate="yes" xml:space="preserve">
          <source>The reconstruction with L1 penalization gives a result with zero error (all pixels are successfully labeled with 0 or 1), even if noise was added to the projections. In comparison, an L2 penalization (&lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt;&lt;/a&gt;) produces a large number of labeling errors for the pixels. Important artifacts are observed on the reconstructed image, contrary to the L1 penalization. Note in particular the circular artifact separating the pixels in the corners, that have contributed to fewer projections than the central disk.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc89055f829a16401789a0501b3aaaa652548713" translate="yes" xml:space="preserve">
          <source>The reduced distance, defined for some metrics, is a computationally more efficient measure which preserves the rank of the true distance. For example, in the Euclidean distance metric, the reduced distance is the squared-euclidean distance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3411437239f8f2dc8ee2706670c4e4a91db1791" translate="yes" xml:space="preserve">
          <source>The reduced samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67c2217cec61f41257c896a393db0ce694f3f635" translate="yes" xml:space="preserve">
          <source>The refitted estimator is made available at the &lt;code&gt;best_estimator_&lt;/code&gt; attribute and permits using &lt;code&gt;predict&lt;/code&gt; directly on this &lt;code&gt;GridSearchCV&lt;/code&gt; instance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1178e040e21448dfc3d87635a64add70ee2fc71f" translate="yes" xml:space="preserve">
          <source>The refitted estimator is made available at the &lt;code&gt;best_estimator_&lt;/code&gt; attribute and permits using &lt;code&gt;predict&lt;/code&gt; directly on this &lt;code&gt;RandomizedSearchCV&lt;/code&gt; instance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb2b5c40285ce2a0d935c8174b2b1df487e2e554" translate="yes" xml:space="preserve">
          <source>The regression target or classification labels, if applicable. Dtype is float if numeric, and object if categorical.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="444eaf2365e5ec07e25a0ed19fde31ede0366773" translate="yes" xml:space="preserve">
          <source>The regressor is used to predict and the &lt;code&gt;inverse_func&lt;/code&gt; or &lt;code&gt;inverse_transform&lt;/code&gt; is applied before returning the prediction.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f20e2edcb6e4e7d6bdbe8de69b9b24c7725d7e1" translate="yes" xml:space="preserve">
          <source>The regularised covariance is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4dfecc4cc3d2dd0e68757701d8e0aef7bde77c4d" translate="yes" xml:space="preserve">
          <source>The regularization mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 &amp;lt; l1_ratio &amp;lt; 1, the penalty is a combination of L1 and L2.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1711bca786b8ddb98e81cd17b706bc6925eee44" translate="yes" xml:space="preserve">
          <source>The regularization parameter C in the LogisticRegression. When C is an array, fit will take each regularization parameter in C one by one for LogisticRegression and store results for each one in &lt;code&gt;all_scores_&lt;/code&gt;, where columns and rows represent corresponding reg_parameters and features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7865779166b38ce9dcfe2e41f3eba5295cbd3874" translate="yes" xml:space="preserve">
          <source>The regularization parameter alpha parameter in the Lasso. Warning: this is not the alpha parameter in the stability selection article which is scaling.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ed37f5cbd4d43ac0f2b6fe6eac86f942c91256a" translate="yes" xml:space="preserve">
          <source>The regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a1b8c0c02e4613adc42b58f49402bda71930b51" translate="yes" xml:space="preserve">
          <source>The regularized (shrunk) covariance is given by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a5a9291b6a07681a32efc9a88d6f2d7eab96435" translate="yes" xml:space="preserve">
          <source>The regularized (shrunk) covariance is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5061099398d935daccb5dfd0421b959c5f17fce1" translate="yes" xml:space="preserve">
          <source>The regularized covariance is given by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bba73ee876f52c89494b029f9c7d24e1239abc01" translate="yes" xml:space="preserve">
          <source>The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dccfd8ff5de1af25843574f310f33b70fd7f2fcc" translate="yes" xml:space="preserve">
          <source>The relationship between recall and precision can be observed in the stairstep area of the plot - at the edges of these steps a small change in the threshold considerably reduces precision, with only a minor gain in recall.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4d930448408dd13aba80cfbcc12949bce572769" translate="yes" xml:space="preserve">
          <source>The relative importance of the fat noisy tail of the singular values profile if &lt;code&gt;effective_rank&lt;/code&gt; is not None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b4ee2ad411363b437ce1738486767191903cc20" translate="yes" xml:space="preserve">
          <source>The relative importance of the fat noisy tail of the singular values profile.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1bf8ab4629789502195bd390130a1b2aa7e78e4e" translate="yes" xml:space="preserve">
          <source>The relative increment in the results before declaring convergence.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ae073568b2b2b27a72266212b55fc2cb2d4cd169" translate="yes" xml:space="preserve">
          <source>The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The &lt;strong&gt;expected fraction of the samples&lt;/strong&gt; they contribute to can thus be used as an estimate of the &lt;strong&gt;relative importance of the features&lt;/strong&gt;. In scikit-learn, the fraction of samples a feature contributes to is combined with the decrease in impurity from splitting them to create a normalized estimate of the predictive power of that feature.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fc6d12d2c584bef7c5a793374c798219700e42f" translate="yes" xml:space="preserve">
          <source>The remaining singular values&amp;rsquo; tail is fat, decreasing as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="006806051caab55f94063393a09eaea2afaeb022" translate="yes" xml:space="preserve">
          <source>The reported averages include micro average (averaging the total true positives, false negatives and false positives), macro average (averaging the unweighted mean per label), weighted average (averaging the support-weighted mean per label) and sample average (only for multilabel classification). See also &lt;a href=&quot;sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt;&lt;code&gt;precision_recall_fscore_support&lt;/code&gt;&lt;/a&gt; for more details on averages.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f7b2580f38f68e2972536327271d77d45324cd3" translate="yes" xml:space="preserve">
          <source>The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18a2377c2d81e0ebb00644fcb1d33204e8d98249" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current X score. This performs the PLS regression known as PLS2. This mode is prediction oriented.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="953a5ab9533846fbfb5f76815143b2efa25824fa" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4786c768ceb1dac6ca78b9b9b4bd8e26183b7ef9" translate="yes" xml:space="preserve">
          <source>The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score. This performs a canonical symmetric version of the PLS regression. But slightly different than the CCA. This is mostly used for modeling.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3050c2e87895e094a17bdbd4ce41119b71fa1e6b" translate="yes" xml:space="preserve">
          <source>The result of &lt;a href=&quot;../../modules/linear_model#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt; is much more strongly biased: the difference is reminiscent of the local intensity value of the original image.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2844a2c76b7226f0533d494b8e60503f59b9c4e8" translate="yes" xml:space="preserve">
          <source>The result of &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; may be different from those obtained using &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; as the elements are grouped in different ways. The function &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; takes an average over cross-validation folds, whereas &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; simply returns the labels (or probabilities) from several distinct models undistinguished. Thus, &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; is not an appropriate measure of generalisation error.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a51791cdd2f13d5121648ef37a39961811acb402" translate="yes" xml:space="preserve">
          <source>The result of calling &lt;code&gt;fit&lt;/code&gt; on a &lt;code&gt;GridSearchCV&lt;/code&gt; object is a classifier that we can use to &lt;code&gt;predict&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0c66fbb583b621a3ed191db16f52e8d9fa96072" translate="yes" xml:space="preserve">
          <source>The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="629d23e2107cca0c4a5d2061641bb34723082f2c" translate="yes" xml:space="preserve">
          <source>The result points are &lt;em&gt;not&lt;/em&gt; necessarily sorted by distance to their query point.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc861dceeba49e88611e2b04a0b3ec8cf37af89c" translate="yes" xml:space="preserve">
          <source>The resulting Calinski-Harabaz score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91d97654f948931244cdd794d37fd9ac58fe871c" translate="yes" xml:space="preserve">
          <source>The resulting Davies-Bouldin score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cfd8b506a89d0c11900fefa11e6003ff64d7a508" translate="yes" xml:space="preserve">
          <source>The resulting Fowlkes-Mallows score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="82343548ff27f39a450415f81d355404a35e8f50" translate="yes" xml:space="preserve">
          <source>The resulting bicluster structure is block-diagonal, since each row and each column belongs to exactly one bicluster.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af7844c7ade28bb6e966130c36de65d0b613a4b9" translate="yes" xml:space="preserve">
          <source>The resulting dataset contains ordinal attributes which can be further used in a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b9ee0d5d80ffdad0cbeed5368095411cec46727" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_exp(X, Y) = k(X, Y) ** exponent</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f2e205585c93945d55d6e2cae73c2d9f60e4b99" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_prod(X, Y) = k1(X, Y) * k2(X, Y)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65c4e9abf2f65b5e239efef34833d44fb903de78" translate="yes" xml:space="preserve">
          <source>The resulting kernel is defined as k_sum(X, Y) = k1(X, Y) + k2(X, Y)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb9ca9651a528caa2f6deb96ee39233de0fa48d4" translate="yes" xml:space="preserve">
          <source>The resulting model is called &lt;em&gt;Bayesian Ridge Regression&lt;/em&gt;, and is similar to the classical &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;. The parameters \(w\), \(\alpha\) and \(\lambda\) are estimated jointly during the fit of the model. The remaining hyperparameters are the parameters of the gamma priors over \(\alpha\) and \(\lambda\). These are usually chosen to be &lt;em&gt;non-informative&lt;/em&gt;. The parameters are estimated by maximizing the &lt;em&gt;marginal log likelihood&lt;/em&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ff5ad65586245c919e0a8e1ef11c4948b5606a5" translate="yes" xml:space="preserve">
          <source>The resulting patches are allocated in a dedicated array.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64dfac2f5dc0c167f2eb731c8522fdbbf80022e7" translate="yes" xml:space="preserve">
          <source>The resulting transformer has then learned a supervised, sparse, high-dimensional categorical embedding of the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f9dd224b05fada5edc12dcb7a4c8d5421d61c2d" translate="yes" xml:space="preserve">
          <source>The return value is a cross-validator which generates the train/test splits via the &lt;code&gt;split&lt;/code&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="63a7df0fd8542a7b486adfe1466de16955c3587a" translate="yes" xml:space="preserve">
          <source>The returned dataset is a &lt;code&gt;scikit-learn&lt;/code&gt; &amp;ldquo;bunch&amp;rdquo;: a simple holder object with fields that can be both accessed as python &lt;code&gt;dict&lt;/code&gt; keys or &lt;code&gt;object&lt;/code&gt; attributes for convenience, for instance the &lt;code&gt;target_names&lt;/code&gt; holds the list of the requested category names:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c5d262a3b65ccb350b8a1d10b0eaf6eba41fc62d" translate="yes" xml:space="preserve">
          <source>The returned estimates for all classes are ordered by label of classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8520c10c8edb7f58383ef36900c902f5398bf785" translate="yes" xml:space="preserve">
          <source>The returned estimates for all classes are ordered by the label of classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9fa1a308376d0d44aa58fe9b54fd102c1f0b956a" translate="yes" xml:space="preserve">
          <source>The returned object is a MemorizedFunc object, that is callable (behaves like a function), but offers extra methods for cache lookup and management. See the documentation for &lt;a href=&quot;https://joblib.readthedocs.io/en/latest/memory.html#joblib.memory.MemorizedFunc&quot;&gt;&lt;code&gt;joblib.memory.MemorizedFunc&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a1e44716730f4f771067bf0b441674e2034e883" translate="yes" xml:space="preserve">
          <source>The richer dictionary on the right is not larger in size, heavier subsampling is performed in order to stay on the same order of magnitude.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f396634dd18eef11c3d60404319f2831b13040b" translate="yes" xml:space="preserve">
          <source>The right figures correspond to the same plots but using instead a bagging ensemble of decision trees. In both figures, we can observe that the bias term is larger than in the previous case. In the upper right figure, the difference between the average prediction (in cyan) and the best possible model is larger (e.g., notice the offset around &lt;code&gt;x=2&lt;/code&gt;). In the lower right figure, the bias curve is also slightly higher than in the lower left figure. In terms of variance however, the beam of predictions is narrower, which suggests that the variance is lower. Indeed, as the lower right figure confirms, the variance term (in green) is lower than for single decision trees. Overall, the bias- variance decomposition is therefore no longer the same. The tradeoff is better for bagging: averaging several decision trees fit on bootstrap copies of the dataset slightly increases the bias term but allows for a larger reduction of the variance, which results in a lower overall mean squared error (compare the red curves int the lower figures). The script output also confirms this intuition. The total error of the bagging ensemble is lower than the total error of a single decision tree, and this difference indeed mainly stems from a reduced variance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4281560832d700a912bb9768ad9253748f3986db" translate="yes" xml:space="preserve">
          <source>The right plot shows the mean squared error between the coefficients found by the model and the chosen vector w. Less regularised models retrieve the exact coefficients (error is equal to 0), stronger regularised models increase the error.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e91440f2fdf83c048c6ec145aa4ba8b2408d7a77" translate="yes" xml:space="preserve">
          <source>The robust MCD, that has a low error provided \(n_\text{samples} &amp;gt; 5n_\text{features}\)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17f09e161fdb50b4bf88ea2669cf0485f026fd48" translate="yes" xml:space="preserve">
          <source>The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="075c19426795ecca36fadcd5142db0c818eae0c2" translate="yes" xml:space="preserve">
          <source>The s parameter used to randomly scale the penalty of different features. Should be between 0 and 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e103a284fd35ca6afde93378ca71e413fedf538" translate="yes" xml:space="preserve">
          <source>The same group will not appear in two different folds (the number of distinct groups has to be at least equal to the number of folds).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0bd48a9cd63a739602e7de633cedbe34c0721a4f" translate="yes" xml:space="preserve">
          <source>The same instance of the transformer can then be applied to some new test data unseen during the fit call: the same scaling and shifting operations will be applied to be consistent with the transformation performed on the train data:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c16a50cac2b1dc1101733e88917cf565ad0ad55a" translate="yes" xml:space="preserve">
          <source>The sample counts that are shown are weighted with any sample_weights that might be present.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="da56cc1e3278be97e394d14d7afaac125d975593" translate="yes" xml:space="preserve">
          <source>The sample weighting rescales the C parameter, which means that the classifier puts more emphasis on getting these points right. The effect might often be subtle. To emphasize the effect here, we particularly weight outliers, making the deformation of the decision boundary very visible.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67f0f1888d07b20765c25e213bac379b2147854b" translate="yes" xml:space="preserve">
          <source>The sampled subsets of integer. The subset of selected integer might not be randomized, see the method argument.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="582c77a6e6e223c6a451ff779c949c2d01a1e4b1" translate="yes" xml:space="preserve">
          <source>The samples in this dataset correspond to 30&amp;times;30m patches of forest in the US, collected for the task of predicting each patch&amp;rsquo;s cover type, i.e. the dominant species of tree. There are seven covertypes, making this a multiclass classification problem. Each sample has 54 features, described on the &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Covertype&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;. Some of the features are boolean indicators, while others are discrete or continuous measurements.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3faf112740a094590f64233617c65a5fa097ee8f" translate="yes" xml:space="preserve">
          <source>The samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1aa98782517735bf609d139ad8030ba79e53e38f" translate="yes" xml:space="preserve">
          <source>The scaler instance can then be used on new data to transform it the same way it did on the training set:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fae25cf01c011421f9b169e383c16cc5b0e7dc5d" translate="yes" xml:space="preserve">
          <source>The scikit-learn project provides a set of machine learning tools that can be used both for novelty or outlier detection. This strategy is implemented with objects learning in an unsupervised way from the data:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2bcbd53d39d84e38eaccf61f40a5b7e3d9f1072" translate="yes" xml:space="preserve">
          <source>The scikit-learn provides an object &lt;a href=&quot;generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope&quot;&gt;&lt;code&gt;covariance.EllipticEnvelope&lt;/code&gt;&lt;/a&gt; that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c6f20ecec2f178819b742529ff67a9012c4e74b9" translate="yes" xml:space="preserve">
          <source>The score above which features should be selected.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fbc2c7bdd77bdb62a30109845f32d883a40f3289" translate="yes" xml:space="preserve">
          <source>The score array for test scores on each cv split.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21374122c0da50ea660a65ea3274fa15ca9abbe5" translate="yes" xml:space="preserve">
          <source>The score array for train scores on each cv split. This is available only if &lt;code&gt;return_train_score&lt;/code&gt; parameter is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9f0e8fbf15571f500f19345d44e98db663f1949" translate="yes" xml:space="preserve">
          <source>The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="10419dac5247bd799d768931a57ece7edc36ff14" translate="yes" xml:space="preserve">
          <source>The score is defined as ratio between the within-cluster dispersion and the between-cluster dispersion.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5fc2e46656049d91b95dd6363e2ef13687eb710" translate="yes" xml:space="preserve">
          <source>The score is defined as the ratio of within-cluster distances to between-cluster distances.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a9ccc42adfd31440d43d06c5c4aab96f5e8222f0" translate="yes" xml:space="preserve">
          <source>The score is fast to compute</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="298450881e1d83a617f17a47cba5b823081f8332" translate="yes" xml:space="preserve">
          <source>The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ebec04d9506729ea096f793265427e501ac6359" translate="yes" xml:space="preserve">
          <source>The score ranges from 0 to 1, or when &lt;code&gt;adjusted=True&lt;/code&gt; is used, it rescaled to the range \(\frac{1}{1 - \text{n\_classes}}\) to 1, inclusive, with performance at random scoring 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99f201771620c880cb8fcf0a4290517d17eb61e7" translate="yes" xml:space="preserve">
          <source>The score ranges from 0 to 1. A high value indicates a good similarity between two clusters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67703aed8c056d27307c0c36c3685d8f65de746b" translate="yes" xml:space="preserve">
          <source>The scorer callable object / function must have its signature as &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e77d6fe812bc8f046068c5fa078dda3a1146e44" translate="yes" xml:space="preserve">
          <source>The scorer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd1c18a79b397962bdc5e0312d35f12a484f084a" translate="yes" xml:space="preserve">
          <source>The scores for each feature along the path.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81a1b1406082100f034b3df6c2ec24562a123854" translate="yes" xml:space="preserve">
          <source>The scores obtained for each permutations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="12753b21f50efe6accfab886a283f46c3614c6fe" translate="yes" xml:space="preserve">
          <source>The scores of HuberRegressor may not be compared directly to both TheilSen and RANSAC because it does not attempt to completely filter the outliers but lessen their effect.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="48d352a6767e745c328aec9195da7218a62bd613" translate="yes" xml:space="preserve">
          <source>The scores of all the scorers are available in the &lt;code&gt;cv_results_&lt;/code&gt; dict at keys ending in &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; (&lt;code&gt;'mean_test_precision'&lt;/code&gt;, &lt;code&gt;'rank_test_precision'&lt;/code&gt;, etc&amp;hellip;)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1676248dcc2b8fd0688c9d8da4bc193152185788" translate="yes" xml:space="preserve">
          <source>The search for the optimal penalization parameter (alpha) is done on an iteratively refined grid: first the cross-validated scores on a grid are computed, then a new refined grid is centered around the maximum, and so on.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="929b69ae47a09aabafb756d8ff0633d79e90c985" translate="yes" xml:space="preserve">
          <source>The searched parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d9a4b24aef1596fe1105c7b9e28ffdd6e589d45" translate="yes" xml:space="preserve">
          <source>The second base-kernel of the product-kernel</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d0afedea31e6e04d617f4edd60a150835c5916d" translate="yes" xml:space="preserve">
          <source>The second base-kernel of the sum-kernel</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="544eb81771c23c2f620b00755dbcd12f00cacc66" translate="yes" xml:space="preserve">
          <source>The second example shows the ability of the Minimum Covariance Determinant robust estimator of covariance to concentrate on the main mode of the data distribution: the location seems to be well estimated, although the covariance is hard to estimate due to the banana-shaped distribution. Anyway, we can get rid of some outlying observations. The One-Class SVM is able to capture the real data structure, but the difficulty is to adjust its kernel bandwidth parameter so as to obtain a good compromise between the shape of the data scatter matrix and the risk of over-fitting the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a46c67db9268b64cb76670e9e1b845e906b608f" translate="yes" xml:space="preserve">
          <source>The second figure shows the calibration curve of a linear support-vector classifier (LinearSVC). LinearSVC shows the opposite behavior as Gaussian naive Bayes: the calibration curve has a sigmoid curve, which is typical for an under-confident classifier. In the case of LinearSVC, this is caused by the margin property of the hinge loss, which lets the model focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e56142dda4889d67d8f5448567e56cb678c48e3c" translate="yes" xml:space="preserve">
          <source>The second figure shows the log-marginal-likelihood for different choices of the kernel&amp;rsquo;s hyperparameters, highlighting the two choices of the hyperparameters used in the first figure by black dots.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69a6b8988bb4a9da22ae7a8129c60d4bfa19ab9d" translate="yes" xml:space="preserve">
          <source>The second loader is typically used for the face verification task: each sample is a pair of two picture belonging or not to the same person:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80a536b57ba05b05dbab01bd549517eccd6caab7" translate="yes" xml:space="preserve">
          <source>The second model is a Bayesian Gaussian Mixture Model with a Dirichlet process prior fit with variational inference. The low value of the concentration prior makes the model favor a lower number of active components. This models &amp;ldquo;decides&amp;rdquo; to focus its modeling power on the big picture of the structure of the dataset: groups of points with alternating directions modeled by non-diagonal covariance matrices. Those alternating directions roughly capture the alternating nature of the original sine signal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ce63f012cd3f9d5ba6717aef4dc1ee5e80e67c9d" translate="yes" xml:space="preserve">
          <source>The second one has a smaller noise level and shorter length scale, which explains most of the variation by the noise-free functional relationship. The second model has a higher likelihood; however, depending on the initial value for the hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is thus important to repeat the optimization several times for different initializations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d5dcf1b11e556ea9365934aae9c750b0508ecb89" translate="yes" xml:space="preserve">
          <source>The second plot demonstrate one single run of the &lt;code&gt;MiniBatchKMeans&lt;/code&gt; estimator using a &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; and &lt;code&gt;n_init=1&lt;/code&gt;. This run leads to a bad convergence (local optimum) with estimated centers stuck between ground truth clusters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41338f596d871499307d96f69f697b91afdfa1c4" translate="yes" xml:space="preserve">
          <source>The second plot is a heatmap of the classifier&amp;rsquo;s cross-validation accuracy as a function of &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt;. For this example we explore a relatively large grid for illustration purposes. In practice, a logarithmic grid from \(10^{-3}\) to \(10^3\) is usually sufficient. If the best parameters lie on the boundaries of the grid, it can be extended in that direction in a subsequent search.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57fb8e025f7ec94b6d1e30748cbff5561f1e9901" translate="yes" xml:space="preserve">
          <source>The second plot shows that an increase of the admissible distortion &lt;code&gt;eps&lt;/code&gt; allows to reduce drastically the minimal number of dimensions &lt;code&gt;n_components&lt;/code&gt; for a given number of samples &lt;code&gt;n_samples&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c6c0ec72e2e2da2def0a20a979b162b66b4f25f" translate="yes" xml:space="preserve">
          <source>The second plot visualized the decision surfaces of the RBF kernel SVM and the linear SVM with approximate kernel maps. The plot shows decision surfaces of the classifiers projected onto the first two principal components of the data. This visualization should be taken with a grain of salt since it is just an interesting slice through the decision surface in 64 dimensions. In particular note that a datapoint (represented as a dot) does not necessarily be classified into the region it is lying in, since it will not lie on the plane that the first two principal components span.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7aa029b23a556153d8e76aebf5393705fc5a931" translate="yes" xml:space="preserve">
          <source>The second use case is to build a completely custom scorer object from a simple python function using &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt;&lt;code&gt;make_scorer&lt;/code&gt;&lt;/a&gt;, which can take several parameters:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c00cf63770db0bb2225569e421e3076426129a55" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator for adding small noise to continuous variables in order to remove repeated values. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06cdf003d2aac9179d5700a91f249ff0a94d6f8e" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator that selects a random feature to update. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;selection&lt;/code&gt; == &amp;lsquo;random&amp;rsquo;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0bc2f2e46810cbdc913e04d68694f60b2b83e0d8" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator that selects a random feature to update. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;selection&lt;/code&gt; == &amp;lsquo;random&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c9b5b4205f687d2590f4c80ac04919e87b2e36db" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data for the dual coordinate descent (if &lt;code&gt;dual=True&lt;/code&gt;). When &lt;code&gt;dual=False&lt;/code&gt; the underlying implementation of &lt;a href=&quot;#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; is not random and &lt;code&gt;random_state&lt;/code&gt; has no effect on the results. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f24e839d9f0c07b405eb078c6f50d58ca84b7fe6" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58318e33f8140e9303ba15931c5e93b11c5df63e" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;sag&amp;rsquo; or &amp;lsquo;liblinear&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e64edd7c479eb06717df1495b4d044bfaa961d0" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator to use when shuffling the data. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;sag&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db7139d208134a3c43811feaafd4124e4ee369a8" translate="yes" xml:space="preserve">
          <source>The seed of the pseudo random number generator used when shuffling the data for probability estimates. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1665069fc9c4c9d9dabb36f1931e17db4945528" translate="yes" xml:space="preserve">
          <source>The set of F values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d03a062cac2973dfcc9173b5fc93f7520db21987" translate="yes" xml:space="preserve">
          <source>The set of labels can be different for each output variable. For instance, a sample could be assigned &amp;ldquo;pear&amp;rdquo; for an output variable that takes possible values in a finite set of species such as &amp;ldquo;pear&amp;rdquo;, &amp;ldquo;apple&amp;rdquo;; and &amp;ldquo;blue&amp;rdquo; or &amp;ldquo;green&amp;rdquo; for a second output variable that takes possible values in a finite set of colors such as &amp;ldquo;green&amp;rdquo;, &amp;ldquo;red&amp;rdquo;, &amp;ldquo;blue&amp;rdquo;, &amp;ldquo;yellow&amp;rdquo;&amp;hellip;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="143c46009e14705cc3449ca19bc2f349662d5283" translate="yes" xml:space="preserve">
          <source>The set of labels for each sample such that &lt;code&gt;y[i]&lt;/code&gt; consists of &lt;code&gt;classes_[j]&lt;/code&gt; for each &lt;code&gt;yt[i, j] == 1&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d10d69a1c09af3eb9fb75910b7cd4ebd942ac2e" translate="yes" xml:space="preserve">
          <source>The set of labels to include when &lt;code&gt;average != 'binary'&lt;/code&gt;, and their order if &lt;code&gt;average is None&lt;/code&gt;. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; are used in sorted order.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc018ce1a1ad3dcbb813a9b943d602142a80bfe5" translate="yes" xml:space="preserve">
          <source>The set of p-values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ae935a83c454932c18aabaf5527bf9d40a05884a" translate="yes" xml:space="preserve">
          <source>The set of regressors that will be tested sequentially.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="418cd7fd5c32b01bfeb33989472034a267c4e833" translate="yes" xml:space="preserve">
          <source>The shape (Nx, Ny) array of pairwise distances between points in X and Y.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be8e19650b3e56af8959e9d1bdb9305ca252ca97" translate="yes" xml:space="preserve">
          <source>The shape of &lt;code&gt;dual_coef_&lt;/code&gt; is &lt;code&gt;[n_class-1, n_SV]&lt;/code&gt; with a somewhat hard to grasp layout. The columns correspond to the support vectors involved in any of the &lt;code&gt;n_class * (n_class - 1) / 2&lt;/code&gt; &amp;ldquo;one-vs-one&amp;rdquo; classifiers. Each of the support vectors is used in &lt;code&gt;n_class - 1&lt;/code&gt; classifiers. The &lt;code&gt;n_class - 1&lt;/code&gt; entries in each row correspond to the dual coefficients for these classifiers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79705c107a83df8d45ef47d82375b7c707f4d5ae" translate="yes" xml:space="preserve">
          <source>The shape of the result.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="770a88634d7b4928209cc52a1f8aea4bce68a8e8" translate="yes" xml:space="preserve">
          <source>The shift offset allows a zero threshold for being an outlier. Only available for novelty detection (when novelty is set to True). The argument X is supposed to contain &lt;em&gt;new data&lt;/em&gt;: if X contains a point from training, it considers the later in its own neighborhood. Also, the samples in X are not considered in the neighborhood of any point.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd58d8b5ba5725b529e01c853ef09676528984ae" translate="yes" xml:space="preserve">
          <source>The shifted opposite of the Local Outlier Factor of each input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13c37fa5739748416ca3f9521f51dec2de533ef6" translate="yes" xml:space="preserve">
          <source>The similarity of two sets of biclusters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7becf18fe4f5e5f9bf982cb2425cc3a834e0c404" translate="yes" xml:space="preserve">
          <source>The simplest metric &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; model, called &lt;em&gt;absolute MDS&lt;/em&gt;, disparities are defined by \(\hat{d}_{ij} = S_{ij}\). With absolute MDS, the value \(S_{ij}\) should then correspond exactly to the distance between point \(i\) and \(j\) in the embedding point.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d805d3b9d3166d024fed65dbe7dc1ddecd0fb40" translate="yes" xml:space="preserve">
          <source>The simplest possible classifier is the &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm&quot;&gt;nearest neighbor&lt;/a&gt;: given a new observation &lt;code&gt;X_test&lt;/code&gt;, find in the training set (i.e. the data used to train the estimator) the observation with the closest feature vector. (Please see the &lt;a href=&quot;../../modules/neighbors#neighbors&quot;&gt;Nearest Neighbors section&lt;/a&gt; of the online Scikit-learn documentation for more information about this type of classifier.)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1fac5c5ae1360f0509b6fb6c9bee7a89fd16eea5" translate="yes" xml:space="preserve">
          <source>The simplest way to accomplish this dimensionality reduction is by taking a random projection of the data. Though this allows some degree of visualization of the data structure, the randomness of the choice leaves much to be desired. In a random projection, it is likely that the more interesting structure within the data will be lost.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a8c7e68caf35057fba3785bb16a14da344816784" translate="yes" xml:space="preserve">
          <source>The simplest way to use cross-validation is to call the &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; helper function on the estimator and the dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b61610397fd4bcbb9699a5ba6221c6a9dfd8212" translate="yes" xml:space="preserve">
          <source>The singular value decomposition, \(A_n = U \Sigma V^\top\), provides the partitions of the rows and columns of \(A\). A subset of the left singular vectors gives the row partitions, and a subset of the right singular vectors gives the column partitions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f785df3fd21ed481c16151f3b91e613616eec382" translate="yes" xml:space="preserve">
          <source>The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the &lt;code&gt;n_components&lt;/code&gt; variables in the lower-dimensional space.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fda37f3c2ceb4ddf1d93860ac5de12a5ffc950e8" translate="yes" xml:space="preserve">
          <source>The size of &lt;code&gt;grid_scores_&lt;/code&gt; is equal to &lt;code&gt;ceil((n_features - min_features_to_select) / step) + 1&lt;/code&gt;, where step is the number of features removed at each iteration.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9867bd15789365ba5d58a7dbdcd5815e87ddbf26" translate="yes" xml:space="preserve">
          <source>The size of the model with the default parameters is \(O( M * N * log (N) )\), where \(M\) is the number of trees and \(N\) is the number of samples. In order to reduce the size of the model, you can change these parameters: &lt;code&gt;min_samples_split&lt;/code&gt;, &lt;code&gt;max_leaf_nodes&lt;/code&gt;, &lt;code&gt;max_depth&lt;/code&gt; and &lt;code&gt;min_samples_leaf&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f17f4e69474d346e8934f03ff8be8d8add88c9f" translate="yes" xml:space="preserve">
          <source>The size of the random matrix to generate.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b31452681b2b7098990045ccc11256312f76473c" translate="yes" xml:space="preserve">
          <source>The size of the regression tree base learners defines the level of variable interactions that can be captured by the gradient boosting model. In general, a tree of depth &lt;code&gt;h&lt;/code&gt; can capture interactions of order &lt;code&gt;h&lt;/code&gt; . There are two ways in which the size of the individual regression trees can be controlled.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb0fc3f910e5dcbfcfe6af4540bbfd0a452530ef" translate="yes" xml:space="preserve">
          <source>The size of the sample to use when computing the Silhouette Coefficient on a random subset of the data. If &lt;code&gt;sample_size is None&lt;/code&gt;, no sampling is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b3605bfba06cab15c1207c4102bf5bbb9eee0a53" translate="yes" xml:space="preserve">
          <source>The size of the set to sample from.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d5051ed3b169b87ed97be7e20bda0cdf9d82ecae" translate="yes" xml:space="preserve">
          <source>The size, the distance and the shape of clusters may vary upon initialization, perplexity values and does not always convey a meaning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6830208b16eb851287384293d35fab1e64fb8f9a" translate="yes" xml:space="preserve">
          <source>The skewed chi squared kernel is given by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bdf0bd9101d435249fe017ac2196fd5049ca1688" translate="yes" xml:space="preserve">
          <source>The smoothing priors \(\alpha \ge 0\) accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting \(\alpha = 1\) is called Laplace smoothing, while \(\alpha &amp;lt; 1\) is called Lidstone smoothing.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4842d9fce84143997f4f4321a8717acf3b670822" translate="yes" xml:space="preserve">
          <source>The solver &amp;ldquo;liblinear&amp;rdquo; uses a coordinate descent (CD) algorithm, and relies on the excellent C++ &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;LIBLINEAR library&lt;/a&gt;, which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a &amp;ldquo;one-vs-rest&amp;rdquo; fashion so separate binary classifiers are trained for all classes. This happens under the hood, so &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; instances using this solver behave as multiclass classifiers. For L1 penalization &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt;&lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt;&lt;/a&gt; allows to calculate the lower bound for C in order to get a non &amp;ldquo;null&amp;rdquo; (all feature weights to zero) model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed7150245f4b6e455142137cba3f8af716071c7a" translate="yes" xml:space="preserve">
          <source>The solver for weight optimization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7be0c6e2677e69b218f8a95f391cfbcac99c36a1" translate="yes" xml:space="preserve">
          <source>The solvers implemented in the class &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; are &amp;ldquo;liblinear&amp;rdquo;, &amp;ldquo;newton-cg&amp;rdquo;, &amp;ldquo;lbfgs&amp;rdquo;, &amp;ldquo;sag&amp;rdquo; and &amp;ldquo;saga&amp;rdquo;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2562620e10231c5093b1e84475f4d077e3e41917" translate="yes" xml:space="preserve">
          <source>The sought maximum memory for temporary distance matrix chunks. When None (default), the value of &lt;code&gt;sklearn.get_config()['working_memory']&lt;/code&gt; is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d35f1b775d7d16bbabc524099db10f7048897e6d" translate="yes" xml:space="preserve">
          <source>The source can also be found &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/tree/master/doc/tutorial/text_analytics&quot;&gt;on Github&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="351456aed7b7d0fc1d0034839135c70dde192f05" translate="yes" xml:space="preserve">
          <source>The source of this tutorial can be found within your scikit-learn folder:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa3459bc05f5ce02810c5dff6ad6cfbeb1ca9a04" translate="yes" xml:space="preserve">
          <source>The spacing between points of the grid, in degrees</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="464028094b69433f3db44d7a263916deccf86cb6" translate="yes" xml:space="preserve">
          <source>The sparse code factor in the matrix factorization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9d9c0a7d6ff3b6246478d922234360e15ed6ab9" translate="yes" xml:space="preserve">
          <source>The sparse code such that each column of this matrix has exactly n_nonzero_coefs non-zero items (X).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee7d500960f94d10a937d21e436dcc2adbbc9a2a" translate="yes" xml:space="preserve">
          <source>The sparse codes</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e1ce62165b4e0dbd0b6b1199e83c88e1c88959d" translate="yes" xml:space="preserve">
          <source>The sparse implementation produces slightly different results than the dense implementation due to a shrunk learning rate for the intercept.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4890de0c56a0e7645deea9088355f84adac629f" translate="yes" xml:space="preserve">
          <source>The sparse vector</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df0fd56f5b70016c4466d03cfc8859f7c3ea7663" translate="yes" xml:space="preserve">
          <source>The sparsity is actually imposed on the cholesky factor of the matrix. Thus alpha does not translate directly into the filling fraction of the matrix itself.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56d99fb198a4da207bde97c2ceeeb3653451f496" translate="yes" xml:space="preserve">
          <source>The sparsity-inducing \(\ell_1\) norm also prevents learning components from noise when few training samples are available. The degree of penalization (and thus sparsity) can be adjusted through the hyperparameter &lt;code&gt;alpha&lt;/code&gt;. Small values lead to a gently regularized factorization, while larger values shrink many coefficients to zero.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e3f3420100e85d456b50524681a1e1c7bbc338a" translate="yes" xml:space="preserve">
          <source>The split code for a single sample has length &lt;code&gt;2 * n_components&lt;/code&gt; and is constructed using the following rule: First, the regular code of length &lt;code&gt;n_components&lt;/code&gt; is computed. Then, the first &lt;code&gt;n_components&lt;/code&gt; entries of the &lt;code&gt;split_code&lt;/code&gt; are filled with the positive part of the regular code vector. The second half of the split code is filled with the negative part of the code vector, only with a positive sign. Therefore, the split_code is non-negative.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="619ea10ad996c929a97e389d99c020b2211157d4" translate="yes" xml:space="preserve">
          <source>The standard LLE algorithm comprises three stages:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5bdc246aecc7e3bec9121428d3f9c450814299f" translate="yes" xml:space="preserve">
          <source>The standard deviation of the clusters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b61516300476f785958503bfbc63e8e96de11d18" translate="yes" xml:space="preserve">
          <source>The standard deviation of the gaussian noise applied to the output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5c05e9131e22b6718043e99a2bae1b92b538981" translate="yes" xml:space="preserve">
          <source>The standard deviation of the gaussian noise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3689d32cff3e62dded279fb6b657e94942cdc7dc" translate="yes" xml:space="preserve">
          <source>The stepwise interpolating function that covers the input domain &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d8cf8d82bcb0bc25727f81a7a8d626fa0a70639" translate="yes" xml:space="preserve">
          <source>The stopping criterion. If it is not None, the iterations will stop when (loss &amp;gt; previous_loss - tol). Defaults to None. Defaults to 1e-3 from 0.21.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba1aa4a75fb51f2479b447aa001f5e6386a1f799" translate="yes" xml:space="preserve">
          <source>The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a02a9e0399d776c0fdc23972d432af0d079552e" translate="yes" xml:space="preserve">
          <source>The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization. See the &amp;lsquo;Multiclass spectral clustering&amp;rsquo; paper referenced below for more details on the discretization approach.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="067b32fbfa4aafe8139a50a2cd0dc62d48f6ce7c" translate="yes" xml:space="preserve">
          <source>The strategy used to choose the split at each node. Supported strategies are &amp;ldquo;best&amp;rdquo; to choose the best split and &amp;ldquo;random&amp;rdquo; to choose the best random split.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4b87188ae01dfb2ea23e8aa9287a121677cbc35" translate="yes" xml:space="preserve">
          <source>The strength of recall versus precision in the F-score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9380c762fbcadf9826438d5ff503ef39938c2642" translate="yes" xml:space="preserve">
          <source>The strength of the LOF algorithm is that it takes both local and global properties of datasets into consideration: it can perform well even in datasets where abnormal samples have different underlying densities. The question is not, how isolated the sample is, but how isolated it is with respect to the surrounding neighborhood.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef559a266ab9339add9416970528df4288b9fe07" translate="yes" xml:space="preserve">
          <source>The string to decode</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71af025de90c113777cd083adeb4a7dc41c643ae" translate="yes" xml:space="preserve">
          <source>The string value &amp;ldquo;auto&amp;rdquo; determines whether y should increase or decrease based on the Spearman correlation estimate&amp;rsquo;s sign.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68913ceeaf791c0f89f5da0e6ea267a6c64604e5" translate="yes" xml:space="preserve">
          <source>The submatrix corresponding to bicluster i.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a10436d8ac7a1230da5ccfb4c02351e0bfbb4701" translate="yes" xml:space="preserve">
          <source>The subset of drawn features for each base estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57675bd8615ef838a99f713d239feb9810e82664" translate="yes" xml:space="preserve">
          <source>The subset of drawn samples for each base estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58b06737b1ee81d6af2156c3790929db0bc0c464" translate="yes" xml:space="preserve">
          <source>The sum of the features (number of words if documents) is drawn from a Poisson distribution with this expected value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="690aa5752a21a529c84a7d2bed72d6956dfbf2f1" translate="yes" xml:space="preserve">
          <source>The support is the number of occurrences of each class in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8f29f24bf343b8a8c6a9702bbb3373c30b3886a" translate="yes" xml:space="preserve">
          <source>The support vector machines in scikit-learn support both dense (&lt;code&gt;numpy.ndarray&lt;/code&gt; and convertible to that by &lt;code&gt;numpy.asarray&lt;/code&gt;) and sparse (any &lt;code&gt;scipy.sparse&lt;/code&gt;) sample vectors as input. However, to use an SVM to make predictions for sparse data, it must have been fit on such data. For optimal performance, use C-ordered &lt;code&gt;numpy.ndarray&lt;/code&gt; (dense) or &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; (sparse) with &lt;code&gt;dtype=float64&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8e93494175fc764f9336519319d2c72af2d3469" translate="yes" xml:space="preserve">
          <source>The target features for which the partial dependecy should be computed (size should be smaller than 3 for visual renderings).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c6694d34ee3bb1ae73f71f475a9064ea6bb5aa61" translate="yes" xml:space="preserve">
          <source>The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="836251639a1d2dd67d806e7910ad6656af016095" translate="yes" xml:space="preserve">
          <source>The target values (class labels in classification, real numbers in regression).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="581ab16d01401b52de3a8a770522ae3215b0d2a3" translate="yes" xml:space="preserve">
          <source>The target values (class labels) as integers or strings.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c42151a6b9abff52d7ec8ba07a5200409a48a3c6" translate="yes" xml:space="preserve">
          <source>The target values (class labels).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af111c076ceef9b210aa6e236368271feda238ba" translate="yes" xml:space="preserve">
          <source>The target values (integers that correspond to classes in classification, real numbers in regression).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="94e263fd180ba7303394b0318de33e42ec7bd528" translate="yes" xml:space="preserve">
          <source>The target values (real numbers).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f52bd7c58bfce68aafb35b3e2f5dd531ddc572f" translate="yes" xml:space="preserve">
          <source>The target values (real numbers). Use &lt;code&gt;dtype=np.float64&lt;/code&gt; and &lt;code&gt;order='C'&lt;/code&gt; for maximum efficiency.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="53a447e32fbe96637e9d7be27a641b24353eba6e" translate="yes" xml:space="preserve">
          <source>The target values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="863f0df40c91ba7090e8eb769163050f217d7bc5" translate="yes" xml:space="preserve">
          <source>The target variable for supervised learning problems.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f60efda845afa3f0dc7adc040cd9b2450fbcc2aa" translate="yes" xml:space="preserve">
          <source>The target variable for supervised learning problems. Stratification is done based on the y labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa20ad6bc67663b59dda9b76a4a065ca1f86af8f" translate="yes" xml:space="preserve">
          <source>The target variable is the median house value for California districts.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40a696d29fc2e13fd302babe7b309a6f769a208a" translate="yes" xml:space="preserve">
          <source>The target variable to try to predict in the case of supervised learning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a34872d2673c11b79098f51c73d69310c6a49da3" translate="yes" xml:space="preserve">
          <source>The task at hand is to predict disease progression from physiological variables.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="609ac3008273ee503e4809fb5a54a64b3f8be85d" translate="yes" xml:space="preserve">
          <source>The ten features are standard independent Gaussian and the target &lt;code&gt;y&lt;/code&gt; is defined by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7581ec7a4d469cfac096612cf94e3958274d6aa" translate="yes" xml:space="preserve">
          <source>The term &amp;ldquo;discrete features&amp;rdquo; is used instead of naming them &amp;ldquo;categorical&amp;rdquo;, because it describes the essence more accurately. For example, pixel intensities of an image are discrete features (but hardly categorical) and you will get better results if mark them as such. Also note, that treating a continuous variable as discrete and vice versa will usually give incorrect results, so be attentive about that.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1748689c159b7c280435a293a84c60a8fa11ddf6" translate="yes" xml:space="preserve">
          <source>The test points for the data. Same format as the training data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4412981e13c1e09172f9e595c07a27a82a32abb" translate="yes" xml:space="preserve">
          <source>The testing set indices for that split.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c079148b8f468ab34a1c911c5b93e7fb1e4fbf43" translate="yes" xml:space="preserve">
          <source>The text feature extractors in scikit-learn know how to decode text files, but only if you tell them what encoding the files are in. The &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; takes an &lt;code&gt;encoding&lt;/code&gt; parameter for this purpose. For modern text files, the correct encoding is probably UTF-8, which is therefore the default (&lt;code&gt;encoding=&quot;utf-8&quot;&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5da204cc914d9d1b370d2a7e3d3f9fed2399ad38" translate="yes" xml:space="preserve">
          <source>The theory says that in order to achieve prediction consistency, the penalty parameter should be kept constant as the number of samples grow.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed8c4048d538066672a56cef623687584e4d51a1" translate="yes" xml:space="preserve">
          <source>The third figure compares kernel density estimates for a distribution of 100 samples in 1 dimension. Though this example uses 1D distributions, kernel density estimation is easily and efficiently extensible to higher dimensions as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0742a25a1bc73abd7670f1b33a5e8f933c99aa55" translate="yes" xml:space="preserve">
          <source>The third model is also a Bayesian Gaussian mixture model with a Dirichlet process prior but this time the value of the concentration prior is higher giving the model more liberty to model the fine-grained structure of the data. The result is a mixture with a larger number of active components that is similar to the first model where we arbitrarily decided to fix the number of components to 10.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1f7cd9ed7ac82273a71a8430edb1e09f61b8cab" translate="yes" xml:space="preserve">
          <source>The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If &amp;ldquo;median&amp;rdquo; (resp. &amp;ldquo;mean&amp;rdquo;), then the &lt;code&gt;threshold&lt;/code&gt; value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., &amp;ldquo;1.25*mean&amp;rdquo;) may also be used. If None and if the estimator has a parameter penalty set to l1, either explicitly or implicitly (e.g, Lasso), the threshold used is 1e-5. Otherwise, &amp;ldquo;mean&amp;rdquo; is used by default.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91a1a785c1bc7a11d4e20e6e119c885bf4142111" translate="yes" xml:space="preserve">
          <source>The threshold value used for feature selection.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a48126d50b83afd18e220a4fcf0cc7ffc7180d9" translate="yes" xml:space="preserve">
          <source>The time complexity of this implementation is &lt;code&gt;O(d ** 2)&lt;/code&gt; assuming d ~ n_features ~ n_components.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff8097e0ec52614ae168c4cd444f04e3f78b14e2" translate="yes" xml:space="preserve">
          <source>The time for fitting the estimator on the train set for each cv split.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc0535c66e14595ad26a449dc475b0e83c5091ba" translate="yes" xml:space="preserve">
          <source>The time for scoring the estimator on the test set for each cv split. (Note time for scoring on the train set is not included even if &lt;code&gt;return_train_score&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1cd8d7a025bee860396ab665c8f7316a009f2f5a" translate="yes" xml:space="preserve">
          <source>The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode=&amp;rsquo;cd&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54800ee92ded7e21b226653650e94d2e245f5dc0" translate="yes" xml:space="preserve">
          <source>The tolerance for the optimization: if the updates are smaller than &lt;code&gt;tol&lt;/code&gt;, the optimization code checks the dual gap for optimality and continues until it is smaller than &lt;code&gt;tol&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7abdf76511114d1a589dafaf8c3b9fea94410d4e" translate="yes" xml:space="preserve">
          <source>The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1174cbfc6c5cd8bb9e51e269de526360c060c73a" translate="yes" xml:space="preserve">
          <source>The tomography projection operation is a linear transformation. In addition to the data-fidelity term corresponding to a linear regression, we penalize the L1 norm of the image to account for its sparsity. The resulting optimization problem is called the &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt;. We use the class &lt;a href=&quot;../../modules/generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;sklearn.linear_model.Lasso&lt;/code&gt;&lt;/a&gt;, that uses the coordinate descent algorithm. Importantly, this implementation is more computationally efficient on a sparse matrix, than the projection operator used here.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6067d7bbaca20238e188da5c4c1f4f9b915cbe46" translate="yes" xml:space="preserve">
          <source>The total number of features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52b059ac9ef2b76cd7b57a438578db960faf18a1" translate="yes" xml:space="preserve">
          <source>The total number of features. These comprise &lt;code&gt;n_informative&lt;/code&gt; informative features, &lt;code&gt;n_redundant&lt;/code&gt; redundant features, &lt;code&gt;n_repeated&lt;/code&gt; duplicated features and &lt;code&gt;n_features-n_informative-n_redundant-n_repeated&lt;/code&gt; useless features drawn at random.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1b95194d002b1fa90370f5ac460e11ae527d46c4" translate="yes" xml:space="preserve">
          <source>The total number of input features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="75f4c14304ea0320f6751402bd1abbcf764fb2d4" translate="yes" xml:space="preserve">
          <source>The total number of points equally divided among classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6fd1a66b2c352f2c105828a73ec9201a20677da3" translate="yes" xml:space="preserve">
          <source>The total number of points generated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b463685bc7194f4e1bd9c9e9566855d8af2442c3" translate="yes" xml:space="preserve">
          <source>The total number of points generated. If odd, the inner circle will have one point more than the outer circle.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="30df1d74b9688e22831b35a50354a3af5ea3a9b9" translate="yes" xml:space="preserve">
          <source>The total number of polynomial output features. The number of output features is computed by iterating over all suitably sized combinations of input features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2341cd15b4f720c4e4ca39627124f453602ba043" translate="yes" xml:space="preserve">
          <source>The traditional way to compute the principal eigenvector is to use the power iteration method:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="486bc8b3be25b906a521777296790c0e7db3392b" translate="yes" xml:space="preserve">
          <source>The training algorithm implemented in &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt; is known as Stochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence (PCD). Optimizing maximum likelihood directly is infeasible because of the form of the data likelihood:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="03936b9df8c7a04402a186159fb53bde128b3907" translate="yes" xml:space="preserve">
          <source>The training data</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7dac9ee1d012d3a04e96a076bd4bb0255f4fa3a" translate="yes" xml:space="preserve">
          <source>The training data contains outliers which are defined as observations that are far from the others. Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the deviant observations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58b640148d7ae1e7f191ee9d800aaef902a6aef0" translate="yes" xml:space="preserve">
          <source>The training data is not polluted by outliers and we are interested in detecting whether a &lt;strong&gt;new&lt;/strong&gt; observation is an outlier. In this context an outlier is also called a novelty.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="431a9d8e158a3b53d56cef19b322f4a781297a84" translate="yes" xml:space="preserve">
          <source>The training data, e.g. a reference to an immutable snapshot</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44e706b0239b6ac2fad3df8ff059d735dbfbf296" translate="yes" xml:space="preserve">
          <source>The training input samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92c6c3d94fd9608db44f0ede45f9bbe4de664d1d" translate="yes" xml:space="preserve">
          <source>The training input samples. Internally, it will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt; and if a sparse matrix is provided to a sparse &lt;code&gt;csc_matrix&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fdce812a7f2c7f82334342af14e6e75d01b61ef1" translate="yes" xml:space="preserve">
          <source>The training input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csc_matrix&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="549582a79c03c6cf3a88a3f5ae8ab3477fe6f4f0" translate="yes" xml:space="preserve">
          <source>The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed9fac6749e01b46bdd0e4eebbecd7ed42fc3bac" translate="yes" xml:space="preserve">
          <source>The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dba95905d10a3a6d61cb924560b16f35840a95de" translate="yes" xml:space="preserve">
          <source>The training points for the data. Each point has three fields:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49afe52f19d67077f586b0d86a3a80bb7b9051b3" translate="yes" xml:space="preserve">
          <source>The training set has size &lt;code&gt;i * n_samples // (n_splits + 1)
+ n_samples % (n_splits + 1)&lt;/code&gt; in the &lt;code&gt;i``th split,
with a test set of size ``n_samples//(n_splits + 1)&lt;/code&gt;, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d0d65f5b4df90d39c92efe2c541dfa70477a3f3" translate="yes" xml:space="preserve">
          <source>The training set indices for that split.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1fe1c9ebc66c3c60358a4b7d7ce6958b71f5be6a" translate="yes" xml:space="preserve">
          <source>The transformation can be triggered by setting either &lt;code&gt;transformer&lt;/code&gt; or the pair of functions &lt;code&gt;func&lt;/code&gt; and &lt;code&gt;inverse_func&lt;/code&gt;. However, setting both options will raise an error.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c1ce2cd56f04ea729576a873f5ef18af794ea62c" translate="yes" xml:space="preserve">
          <source>The transformation is applied on each feature independently. The cumulative density function of a feature is used to project the original values. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ddd72b82186574150d900e6dfd8721345fa62073" translate="yes" xml:space="preserve">
          <source>The transformation is given by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1db1e441acce63afd6d696933447c0b2d453bb8c" translate="yes" xml:space="preserve">
          <source>The transformed data</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a52b9359f518099e81b711eeaccc5e0c7c17eb0" translate="yes" xml:space="preserve">
          <source>The transformed data is then used to train a naive Bayes classifier, and a clear difference in prediction accuracies is observed wherein the dataset which is scaled before PCA vastly outperforms the unscaled version.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80e852f03673351dd5ee00932a57dc4a209cdf2d" translate="yes" xml:space="preserve">
          <source>The transformed data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="831c10597508e086776cd00760f56c708f8d2bba" translate="yes" xml:space="preserve">
          <source>The tree algorithm to use. Valid options are [&amp;lsquo;kd_tree&amp;rsquo;|&amp;rsquo;ball_tree&amp;rsquo;|&amp;rsquo;auto&amp;rsquo;]. Default is &amp;lsquo;auto&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d745501ed4c4af3a67688bd307560f44fb29c904" translate="yes" xml:space="preserve">
          <source>The tree data structure consists of nodes with each node consisting of a number of subclusters. The maximum number of subclusters in a node is determined by the branching factor. Each subcluster maintains a linear sum, squared sum and the number of samples in that subcluster. In addition, each subcluster can also have a node as its child, if the subcluster is not a member of a leaf node.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fbc6ef1abc08faa2debda525a78475d34850618e" translate="yes" xml:space="preserve">
          <source>The true probability in each bin (fraction of positives).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89aec1004fa9767eced66561c3ed3161000c45a1" translate="yes" xml:space="preserve">
          <source>The true score without permuting targets.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="311da69a1a719737450cc586668ba277c4bde011" translate="yes" xml:space="preserve">
          <source>The tutorial folder should contain the following sub-folders:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2e8b6623a8185bbb598cfdf6b3f71f1ee2c2a837" translate="yes" xml:space="preserve">
          <source>The two figures below plot the values of &lt;code&gt;C&lt;/code&gt; on the &lt;code&gt;x-axis&lt;/code&gt; and the corresponding cross-validation scores on the &lt;code&gt;y-axis&lt;/code&gt;, for several different fractions of a generated data-set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ed5804ea6261422d9ec471fffa8f8a41a307d64" translate="yes" xml:space="preserve">
          <source>The two species are:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4aa36c5d8992165f7895075f7b16a37f9864b092" translate="yes" xml:space="preserve">
          <source>The type of criterion to use.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6bc8093d847369045cfca5abe49cd94f035f902d" translate="yes" xml:space="preserve">
          <source>The type of feature values. Passed to Numpy array/scipy.sparse matrix constructors as the dtype argument.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51cef5c60b8823e16a67a0821a4b5311abdcd5ed" translate="yes" xml:space="preserve">
          <source>The type of feature values. Passed to scipy.sparse matrix constructors as the dtype argument. Do not set this to bool, np.boolean or any unsigned integer type.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25d83e9ee3b7a100418b9b6d2bb7936470b09825" translate="yes" xml:space="preserve">
          <source>The type of norm used to compute the error. Available error types: - &amp;lsquo;frobenius&amp;rsquo; (default): sqrt(tr(A^t.A)) - &amp;lsquo;spectral&amp;rsquo;: sqrt(max(eigenvalues(A^t.A)) where A is the error &lt;code&gt;(comp_cov - self.covariance_)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0b5e941b7e074477ab9a734aa8fbc101aeb347c" translate="yes" xml:space="preserve">
          <source>The unchanged dictionary atoms</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="776a52daee9321c0497d40c79109e87f34af6b7e" translate="yes" xml:space="preserve">
          <source>The underlying &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; implementation uses a random number generator to select features when fitting the model with a dual coordinate descent (i.e when &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;). It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter. This randomness can also be controlled with the &lt;code&gt;random_state&lt;/code&gt; parameter. When &lt;code&gt;dual&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt; the underlying implementation of &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; is not random and &lt;code&gt;random_state&lt;/code&gt; has no effect on the results.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68c40938fee4ae1976f2ebe09bc5c5a404f12d37" translate="yes" xml:space="preserve">
          <source>The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller &lt;code&gt;tol&lt;/code&gt; parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f30e1e8a65635cf877334fb55d57ec3cedb0460" translate="yes" xml:space="preserve">
          <source>The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ed849766bc74a6b6038e401c289ce378db99dcc" translate="yes" xml:space="preserve">
          <source>The underlying Tree object. Please refer to &lt;code&gt;help(sklearn.tree._tree.Tree)&lt;/code&gt; for attributes of Tree object and &lt;a href=&quot;../../auto_examples/tree/plot_unveil_tree_structure#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py&quot;&gt;Understanding the decision tree structure&lt;/a&gt; for basic usage of these attributes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eaa9f70240ca573ce446579a8e3077ce3fd3b2de" translate="yes" xml:space="preserve">
          <source>The underlying implementation is MurmurHash3_x86_32 generating low latency 32bits hash suitable for implementing lookup tables, Bloom filters, count min sketch or feature hashing.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b27403493b3e1cf67d088cabd49f20f60beabb5" translate="yes" xml:space="preserve">
          <source>The underlying implementation, liblinear, uses a sparse internal representation for the data that will incur a memory copy.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22bad6cd0a0fca07f198954a6d755d20a0363412" translate="yes" xml:space="preserve">
          <source>The univariate position of the sample according to the main dimension of the points in the manifold.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e1bff9cf5ae34029868daf8dfe6afee04fc2666d" translate="yes" xml:space="preserve">
          <source>The unmixing matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c5425dc309e3cab0153df4bbf4dcfe21bae1aa0" translate="yes" xml:space="preserve">
          <source>The unsupervised data reduction and the supervised estimator can be chained in one step. See &lt;a href=&quot;compose#pipeline&quot;&gt;Pipeline: chaining estimators&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fca2372915cd9dbf5bde8febd27812d65a387223" translate="yes" xml:space="preserve">
          <source>The upper left figure illustrates the predictions (in dark red) of a single decision tree trained over a random dataset LS (the blue dots) of a toy 1d regression problem. It also illustrates the predictions (in light red) of other single decision trees trained over other (and different) randomly drawn instances LS of the problem. Intuitively, the variance term here corresponds to the width of the beam of predictions (in light red) of the individual estimators. The larger the variance, the more sensitive are the predictions for &lt;code&gt;x&lt;/code&gt; to small changes in the training set. The bias term corresponds to the difference between the average prediction of the estimator (in cyan) and the best possible model (in dark blue). On this problem, we can thus observe that the bias is quite low (both the cyan and the blue curves are close to each other) while the variance is large (the red beam is rather wide).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1125be6cb70e3f587339b27b5931d4f81c0d0d9b" translate="yes" xml:space="preserve">
          <source>The usage of &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is described in detail in &lt;a href=&quot;../modules/kernel_approximation#kernel-approximation&quot;&gt;Kernel Approximation&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7012369902c9ebdfcf4a40c832356073db31ae9a" translate="yes" xml:space="preserve">
          <source>The usage of centroid distance limits the distance metric to Euclidean space.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f2ed911398b7e07d993b089b964fc574c420c21" translate="yes" xml:space="preserve">
          <source>The usage of the &lt;a href=&quot;generated/sklearn.kernel_approximation.skewedchi2sampler#sklearn.kernel_approximation.SkewedChi2Sampler&quot;&gt;&lt;code&gt;SkewedChi2Sampler&lt;/code&gt;&lt;/a&gt; is the same as the usage described above for the &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;. The only difference is in the free parameter, that is called \(c\). For a motivation for this mapping and the mathematical details see &lt;a href=&quot;#ls2010&quot; id=&quot;id7&quot;&gt;[LS2010]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fabe9be99adc410c1907ea1daa7576972e4b19e8" translate="yes" xml:space="preserve">
          <source>The use of multi-output nearest neighbors for regression is demonstrated in &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="adaab44ae672254ed2b0f1474d8d83b9d0ff364a" translate="yes" xml:space="preserve">
          <source>The use of multi-output trees for classification is demonstrated in &lt;a href=&quot;../auto_examples/plot_multioutput_face_completion#sphx-glr-auto-examples-plot-multioutput-face-completion-py&quot;&gt;Face completion with a multi-output estimators&lt;/a&gt;. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e18a96fd5f8412fd8540943bfa9bb84448ef6eef" translate="yes" xml:space="preserve">
          <source>The use of multi-output trees for regression is demonstrated in &lt;a href=&quot;../auto_examples/tree/plot_tree_regression_multioutput#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py&quot;&gt;Multi-output Decision Tree Regression&lt;/a&gt;. In this example, the input X is a single real value and the outputs Y are the sine and cosine of X.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d9188616eccce81f2896ac591395f1642a23655" translate="yes" xml:space="preserve">
          <source>The used categories can be found in the &lt;code&gt;categories_&lt;/code&gt; attribute.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ada4ba90a5864aca46a2fd4279e91da24ee2ef32" translate="yes" xml:space="preserve">
          <source>The user-provided initial means, defaults to None, If it None, means are initialized using the &lt;code&gt;init_params&lt;/code&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a1cbbec0505b4f4802950c392df8579b38a3220d" translate="yes" xml:space="preserve">
          <source>The user-provided initial precisions (inverse of the covariance matrices), defaults to None. If it None, precisions are initialized using the &amp;lsquo;init_params&amp;rsquo; method. The shape depends on &amp;lsquo;covariance_type&amp;rsquo;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="930a8f090bbd3f5ab357e6a55436cb80968a37a5" translate="yes" xml:space="preserve">
          <source>The user-provided initial weights, defaults to None. If it None, weights are initialized using the &lt;code&gt;init_params&lt;/code&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="845cdaf2c42f719deb3141928cabec5996b4eedb" translate="yes" xml:space="preserve">
          <source>The usual covariance maximum likelihood estimate can be regularized using shrinkage. Ledoit and Wolf proposed a close formula to compute the asymptotically optimal shrinkage parameter (minimizing a MSE criterion), yielding the Ledoit-Wolf covariance estimate.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d9f8abe9cd1fbb176b951540baae1f3defa7962" translate="yes" xml:space="preserve">
          <source>The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set. In such a case, it would be better to use a robust estimator of covariance to guarantee that the estimation is resistant to &amp;ldquo;erroneous&amp;rdquo; observations in the data set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf61b06542d3e7e313e8463ad6cb36679683eb16" translate="yes" xml:space="preserve">
          <source>The utility function &lt;a href=&quot;generated/sklearn.pipeline.make_pipeline#sklearn.pipeline.make_pipeline&quot;&gt;&lt;code&gt;make_pipeline&lt;/code&gt;&lt;/a&gt; is a shorthand for constructing pipelines; it takes a variable number of estimators and returns a pipeline, filling in the names automatically:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e6fa170ec90d321cecf4d3db13ca13c99c71342b" translate="yes" xml:space="preserve">
          <source>The valid distance metrics, and the function they map to, are:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7aae4519886038a4fe27266a449c263068305fb0" translate="yes" xml:space="preserve">
          <source>The value 2 has the highest score: it appears twice with weights of 1.5 and 2: the sum of these is 3.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf2aaa6a69c13c7a1da598bbc487a099c24264e9" translate="yes" xml:space="preserve">
          <source>The value 4 appears three times: with uniform weights, the result is simply the mode of the distribution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee4d7bc4aea75382ac7c13c2d3ccdb7c85b05695" translate="yes" xml:space="preserve">
          <source>The value by which &lt;code&gt;|y - X'w - c|&lt;/code&gt; is scaled down.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="208f4b20d150446e32489b5141e41ca0c231e069" translate="yes" xml:space="preserve">
          <source>The value of the inertia criterion associated with the chosen partition (if compute_labels is set to True). The inertia is defined as the sum of square distances of samples to their nearest neighbor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="605e5e84334ac11a6b1bcf93811952a1f4c93232" translate="yes" xml:space="preserve">
          <source>The value of the information criteria (&amp;lsquo;aic&amp;rsquo;, &amp;lsquo;bic&amp;rsquo;) across all alphas. The alpha which has the smallest information criterion is chosen. This value is larger by a factor of &lt;code&gt;n_samples&lt;/code&gt; compared to Eqns. 2.15 and 2.16 in (Zou et al, 2007).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31a3294fc25a32d76657f5de9f45e5deb67968f8" translate="yes" xml:space="preserve">
          <source>The value of the largest coefficient.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35a39b75c4cf1051868175c0eb7c4d08a5d8c4c6" translate="yes" xml:space="preserve">
          <source>The value of the smallest coefficient.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d764819e9c8551a1ae19a24b5d4cd3ac77d8b2aa" translate="yes" xml:space="preserve">
          <source>The values corresponding the quantiles of reference.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e64c591fdc6bfea9ce8a9d182cdb9d3354d8a90b" translate="yes" xml:space="preserve">
          <source>The values listed by the ValueError exception correspond to the functions measuring prediction accuracy described in the following sections. The scorer objects for those functions are stored in the dictionary &lt;code&gt;sklearn.metrics.SCORERS&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad8b80eff2782593fcb8941c5fc504eacc683633" translate="yes" xml:space="preserve">
          <source>The values of the parameter that will be evaluated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="896e61b11bd1cccb5014a25cacfbd49baded2da1" translate="yes" xml:space="preserve">
          <source>The values to be assigned to each cluster of samples</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de1379c1aa59c6a0f9d415d70cec6205d70e58b1" translate="yes" xml:space="preserve">
          <source>The variance for each feature in the training set. Used to compute &lt;code&gt;scale_&lt;/code&gt;. Equal to &lt;code&gt;None&lt;/code&gt; when &lt;code&gt;with_std=False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6909f327165fbbe5fe9d7a24773b9839e3b76030" translate="yes" xml:space="preserve">
          <source>The variance of the training samples transformed by a projection to each component.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="73778e0f44de95a7d306fdc5c4bc68ceb4bf8f71" translate="yes" xml:space="preserve">
          <source>The varying values of the coefficients along the path. It is not present if the &lt;code&gt;fit_path&lt;/code&gt; parameter is &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f193fe45abdd49c251c120544e6bc124439f7f88" translate="yes" xml:space="preserve">
          <source>The vector \(h_i\) is called &amp;ldquo;latent&amp;rdquo; because it is unobserved. \(\epsilon\) is considered a noise term distributed according to a Gaussian with mean 0 and covariance \(\Psi\) (i.e. \(\epsilon \sim \mathcal{N}(0, \Psi)\)), \(\mu\) is some arbitrary offset vector. Such a model is called &amp;ldquo;generative&amp;rdquo; as it describes how \(x_i\) is generated from \(h_i\). If we use all the \(x_i\)&amp;lsquo;s as columns to form a matrix \(\mathbf{X}\) and all the \(h_i\)&amp;lsquo;s as columns of a matrix \(\mathbf{H}\) then we can write (with suitably defined \(\mathbf{M}\) and \(\mathbf{E}\)):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4545cfee48c8d7d2034c72caee40a8d82cfcf4ce" translate="yes" xml:space="preserve">
          <source>The verbose setting on the MiniBatchKMeans enables us to see that some clusters are reassigned during the successive calls to partial-fit. This is because the number of patches that they represent has become too low, and it is better to choose a random new cluster.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4c09c360935c392206a8639f0a0b8f4c48b519d" translate="yes" xml:space="preserve">
          <source>The verbosity level</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4b4be8a44f0a986b95a00a99e1db7cc81cee43d8" translate="yes" xml:space="preserve">
          <source>The verbosity level.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bca220a25d0b85cbc18bcc37bc0c66babd623c39" translate="yes" xml:space="preserve">
          <source>The verbosity level. The default, zero, means silent mode.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b790263c39903969cb477edb620406690c93cb40" translate="yes" xml:space="preserve">
          <source>The verbosity level: if non zero, progress messages are printed. Above 50, the output is sent to stdout. The frequency of the messages increases with the verbosity level. If it more than 10, all iterations are reported.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ac67f0eccad920e701e068f47d28e090c55e23b1" translate="yes" xml:space="preserve">
          <source>The verbosity mode of the function. By default that of the memory object is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31fbaba32a3bec24c49d9ccad5d6e7edcbc904dc" translate="yes" xml:space="preserve">
          <source>The versions of scikit-learn and its dependencies</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6a1686cc9631522f215a91033f5d185a9b750f85" translate="yes" xml:space="preserve">
          <source>The vocabulary extracted by this vectorizer is hence much bigger and can now resolve ambiguities encoded in local positioning patterns:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16d6455593e440b1ece6b5d9b609b990b16728ff" translate="yes" xml:space="preserve">
          <source>The weighted average probabilities for a sample would then be calculated as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf3727918257ef88c9160136d7b98b7188c886ca" translate="yes" xml:space="preserve">
          <source>The weighted impurity decrease equation is the following:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b71363fa16752021c27a44ccc8c03e740b0054e3" translate="yes" xml:space="preserve">
          <source>The weights \(w\) of the model can be access:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd37cf293f530a725eccba4fc386300500a89671" translate="yes" xml:space="preserve">
          <source>The weights for each observation in X. If None, all observations are assigned equal weight (default: None)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fd9c75eb2401f4a1ca174b965b1602b9d82941e" translate="yes" xml:space="preserve">
          <source>The weights of each feature computed by the &lt;code&gt;fit&lt;/code&gt; method call are stored in a model attribute:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44235eb4c2d368f2a7e225131bb791a1dc59a457" translate="yes" xml:space="preserve">
          <source>The weights of each mixture components.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd426f25816730ede96a98b838186f9fc1553a50" translate="yes" xml:space="preserve">
          <source>The wine dataset is a classic and very easy multi-class classification dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f15574c628488edf549f3131c3e1a8bfce37b9c" translate="yes" xml:space="preserve">
          <source>The word &amp;ldquo;article&amp;rdquo; is a significant feature, based on how often people quote previous posts like this: &amp;ldquo;In article [article ID], [name] &amp;lt;[e-mail address]&amp;gt; wrote:&amp;rdquo;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="364df8b6c4c1dfcb405abf5ac32289b3f8338a10" translate="yes" xml:space="preserve">
          <source>The word &lt;em&gt;restricted&lt;/em&gt; refers to the bipartite structure of the model, which prohibits direct interaction between hidden units, or between visible units. This means that the following conditional independencies are assumed:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92b782ef9bfc8a9813d7ea0d8efb9106d8f1896e" translate="yes" xml:space="preserve">
          <source>The word boundaries-aware variant &lt;code&gt;char_wb&lt;/code&gt; is especially interesting for languages that use white-spaces for word separation as it generates significantly less noisy features than the raw &lt;code&gt;char&lt;/code&gt; variant in that case. For such languages it can increase both the predictive accuracy and convergence speed of classifiers trained using such features while retaining the robustness with regards to misspellings and word derivations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e8bd120180a9cd30000d6c7bc4b945e5f585fc6" translate="yes" xml:space="preserve">
          <source>The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, &amp;lsquo;How slow is the k-means method?&amp;rsquo; SoCG2006)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08b4bc51642b52ec53b43a5b2dca7586d296a859" translate="yes" xml:space="preserve">
          <source>Theil-Sen Estimator: robust multivariate regression model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26357ed7a886288385aec0522bda7ee91031a5a9" translate="yes" xml:space="preserve">
          <source>Theil-Sen Estimators in a Multiple Linear Regression Model, 2009 Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang &lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bfbe9912c49db3d4af2258ea17cbacd11611139b" translate="yes" xml:space="preserve">
          <source>Theil-Sen Regression</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="43407a9ed591c227c94e72cd0fbe8ae10a8a282d" translate="yes" xml:space="preserve">
          <source>TheilSen is good for small outliers, both in direction X and y, but has a break point above which it performs worse than OLS.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bae71614b2af83560543a8e9bca47722a78d80c8" translate="yes" xml:space="preserve">
          <source>Their harmonic mean called &lt;strong&gt;V-measure&lt;/strong&gt; is computed by &lt;a href=&quot;generated/sklearn.metrics.v_measure_score#sklearn.metrics.v_measure_score&quot;&gt;&lt;code&gt;v_measure_score&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de0b6ab722b9ec4be95cb7086fea273ea373e1de" translate="yes" xml:space="preserve">
          <source>Then fire an ipython shell and run the work-in-progress script with:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f78bb6cc31961e9507c2d8e418f53fe822a9ecd" translate="yes" xml:space="preserve">
          <source>Then one can show that to classify a data point after scaling is equivalent to finding the estimated class mean \(\mu^*_k\) which is closest to the data point in the Euclidean distance. But this can be done just as well after projecting on the \(K-1\) affine subspace \(H_K\) generated by all the \(\mu^*_k\) for all classes. This shows that, implicit in the LDA classifier, there is a dimensionality reduction by linear projection onto a \(K-1\) dimensional space.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="37e493a483dcc9fefbfec81c47f8c835f7340e3f" translate="yes" xml:space="preserve">
          <source>Then the Davies-Bouldin index is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54059dffef3b64a1ba00cd6fbe5ba85d85229195" translate="yes" xml:space="preserve">
          <source>Then the metrics are defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8da3ae858e8aee4768b456c38c7b9a98b999a912" translate="yes" xml:space="preserve">
          <source>Then the multiclass MCC is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="826d143749061228ef1caa51bd344b5a543a3ad8" translate="yes" xml:space="preserve">
          <source>Then the rows of \(Z\) are clustered using &lt;a href=&quot;clustering#k-means&quot;&gt;k-means&lt;/a&gt;. The first &lt;code&gt;n_rows&lt;/code&gt; labels provide the row partitioning, and the remaining &lt;code&gt;n_columns&lt;/code&gt; labels provide the column partitioning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7548beecedd6ae525f17db272cd47fef5592b2e" translate="yes" xml:space="preserve">
          <source>Then, applying the Euclidean (L2) norm, we obtain the following tf-idfs for document 1:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="743e0b74d42270bf2752b1121cf66d48c177b1df" translate="yes" xml:space="preserve">
          <source>Then, the &lt;code&gt;raw_X&lt;/code&gt; to be fed to &lt;code&gt;FeatureHasher.transform&lt;/code&gt; can be constructed using:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4665e6f004c55fa84e60f1409a5b03c15037dd8c" translate="yes" xml:space="preserve">
          <source>Theoretical bounds</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ebe4a41a0b35192395aac8a80293b4ff462a23b" translate="yes" xml:space="preserve">
          <source>There are 3 different APIs for evaluating the quality of a model&amp;rsquo;s predictions:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa910de5e10f2b04644e63996efaedb310779ee6" translate="yes" xml:space="preserve">
          <source>There are a number of ways to convert between a distance metric and a similarity measure, such as a kernel. Let &lt;code&gt;D&lt;/code&gt; be the distance, and &lt;code&gt;S&lt;/code&gt; be the kernel:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="daf4e822cec5d40489080bfb83cea014bb21271c" translate="yes" xml:space="preserve">
          <source>There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17151dd0dcece68b8abecb55a500335bd2f90b73" translate="yes" xml:space="preserve">
          <source>There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fbd19b439fe6ba80531cd23629fc7a9a883a8dcf" translate="yes" xml:space="preserve">
          <source>There are different things to keep in mind when dealing with data corrupted by outliers:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fcc62fa36cf231416a60ac162100095676c743b" translate="yes" xml:space="preserve">
          <source>There are many learning routines which rely on nearest neighbors at their core. One example is &lt;a href=&quot;density#kernel-density&quot;&gt;kernel density estimation&lt;/a&gt;, discussed in the &lt;a href=&quot;density#density-estimation&quot;&gt;density estimation&lt;/a&gt; section.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="84513801465c2346e62e2a9ecaecce1cb0c3994b" translate="yes" xml:space="preserve">
          <source>There are several known issues in our provided &amp;lsquo;english&amp;rsquo; stop word list. See &lt;a href=&quot;#nqy18&quot; id=&quot;id5&quot;&gt;[NQY18]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="87d60889b216a9f6a8543438325d8902e749b92a" translate="yes" xml:space="preserve">
          <source>There are ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="840b342f8bab010ed9a33830388b79c022d751bb" translate="yes" xml:space="preserve">
          <source>There are three different implementations of Support Vector Regression: &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt; provides a faster implementation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; but only considers linear kernels, while &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;NuSVR&lt;/code&gt;&lt;/a&gt; implements a slightly different formulation than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;LinearSVR&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;#svm-implementation-details&quot;&gt;Implementation details&lt;/a&gt; for further details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6325cb8ad2d750db2de1f50457999e3ed60c95ad" translate="yes" xml:space="preserve">
          <source>There are three main kinds of dataset interfaces that can be used to get datasets depending on the desired type of dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d867308d6cfc04930e6d66867b250110233d07e" translate="yes" xml:space="preserve">
          <source>There are two options to assign labels:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="60a8f9c96bc7f93958ce08005db84d1ac5e8a2b4" translate="yes" xml:space="preserve">
          <source>There are two ways of evaluating a biclustering result: internal and external. Internal measures, such as cluster stability, rely only on the data and the result themselves. Currently there are no internal bicluster measures in scikit-learn. External measures refer to an external source of information, such as the true solution. When working with real data the true solution is usually unknown, but biclustering artificial data may be useful for evaluating algorithms precisely because the true solution is known.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21ade7db23177cbafdee1241d820ab218814e247" translate="yes" xml:space="preserve">
          <source>There are two ways to specify multiple scoring metrics for the &lt;code&gt;scoring&lt;/code&gt; parameter:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cdf4947857438b0c51097ba679415b8309e576f2" translate="yes" xml:space="preserve">
          <source>There exists two types of MDS algorithm: metric and non metric. In the scikit-learn, the class &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; implements both. In Metric MDS, the input similarity matrix arises from a metric (and thus respects the triangular inequality), the distances between output two points are then set to be as close as possible to the similarity or dissimilarity data. In the non-metric version, the algorithms will try to preserve the order of the distances, and hence seek for a monotonic relationship between the distances in the embedded space and the similarities/dissimilarities.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e945ec56932f8495741411aac1ef0391f41ea70" translate="yes" xml:space="preserve">
          <source>There is absolutely no guarantee of recovering a ground truth. First, choosing the right number of clusters is hard. Second, the algorithm is sensitive to initialization, and can fall into local minima, although scikit-learn employs several tricks to mitigate this issue.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5fa0b690cccf03ea1d32deadc1c15c3039ee96d" translate="yes" xml:space="preserve">
          <source>There is built-in support for sparse data given in any matrix in a format supported by &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;scipy.sparse&lt;/a&gt;. For maximum efficiency, however, use the CSR matrix format as defined in &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html&quot;&gt;scipy.sparse.csr_matrix&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bf90eed7a42877d9e674dd4cae64f5ef3af07a9" translate="yes" xml:space="preserve">
          <source>There is no general rule to select an alpha parameter for recovery of non-zero coefficients. It can by set by cross-validation (&lt;code&gt;LassoCV&lt;/code&gt; or &lt;code&gt;LassoLarsCV&lt;/code&gt;), though this may lead to under-penalized models: including a small number of non-relevant variables is not detrimental to prediction score. BIC (&lt;code&gt;LassoLarsIC&lt;/code&gt;) tends, on the opposite, to set high values of alpha.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff0ab54a4c7a8a1cf35484dbae4d9dce95d5c026" translate="yes" xml:space="preserve">
          <source>There might be a difference in the scores obtained between &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;solver=liblinear&lt;/code&gt; or &lt;code&gt;LinearSVC&lt;/code&gt; and the external liblinear library directly, when &lt;code&gt;fit_intercept=False&lt;/code&gt; and the fit &lt;code&gt;coef_&lt;/code&gt; (or) the data to be predicted are zeroes. This is because for the sample(s) with &lt;code&gt;decision_function&lt;/code&gt; zero, &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;LinearSVC&lt;/code&gt; predict the negative class, while liblinear predicts the positive class. Note that a model with &lt;code&gt;fit_intercept=False&lt;/code&gt; and having many samples with &lt;code&gt;decision_function&lt;/code&gt; zero, is likely to be a underfit, bad model and you are advised to set &lt;code&gt;fit_intercept=True&lt;/code&gt; and increase the intercept_scaling.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a4216d2986ca1c413a45927f7f8dc07ca811204" translate="yes" xml:space="preserve">
          <source>Therefore, a logarithmic (&lt;code&gt;np.log1p&lt;/code&gt;) and an exponential function (&lt;code&gt;np.expm1&lt;/code&gt;) will be used to transform the targets before training a linear regression model and using it for prediction.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6912f2dbecf0d873a7ad1021b00fc015a0232427" translate="yes" xml:space="preserve">
          <source>These are transformers that are not intended to be used on features, only on supervised learning targets. See also &lt;a href=&quot;compose#transformed-target-regressor&quot;&gt;Transforming target in regression&lt;/a&gt; if you want to transform the prediction target for learning, but evaluate the model in the original (untransformed) space.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c615abfdf912d61f49e70314e0bacb6d3789d48" translate="yes" xml:space="preserve">
          <source>These classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no hyperparameters to tune.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fde55c65b8197fd0cbaa58300d107768af9ed389" translate="yes" xml:space="preserve">
          <source>These constraint are useful to impose a certain local structure, but they also make the algorithm faster, especially when the number of the samples is high.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ebd210a6b7ae21f6cafc3c41203edaaa46e25728" translate="yes" xml:space="preserve">
          <source>These datasets are useful to quickly illustrate the behavior of the various algorithms implemented in scikit-learn. They are however often too small to be representative of real world machine learning tasks.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d2ab26191aa60fb366e6a03a6fce9c7d01208ba" translate="yes" xml:space="preserve">
          <source>These environment variables should be set before importing scikit-learn.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b821932825baa77bb11f8b1f95c8cc38b1d75bf5" translate="yes" xml:space="preserve">
          <source>These estimators are called similarly to their counterparts, with &amp;lsquo;CV&amp;rsquo; appended to their name.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8bef6d4f1ba3e716c219b98d63998eb428a7438d" translate="yes" xml:space="preserve">
          <source>These families of algorithms are useful to find linear relations between two multivariate datasets: the &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; arguments of the &lt;code&gt;fit&lt;/code&gt; method are 2D arrays.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e90c7c20b495d718d6ee1761b52661a07d31f2ac" translate="yes" xml:space="preserve">
          <source>These figures aid in illustrating how a point cloud can be very flat in one direction&amp;ndash;which is where PCA comes in to choose a direction that is not flat.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b3c06102d71e9aa21ce3635f214fb7544bfe93e" translate="yes" xml:space="preserve">
          <source>These functions have an &lt;code&gt;multioutput&lt;/code&gt; keyword argument which specifies the way the scores or losses for each individual target should be averaged. The default is &lt;code&gt;'uniform_average'&lt;/code&gt;, which specifies a uniformly weighted mean over outputs. If an &lt;code&gt;ndarray&lt;/code&gt; of shape &lt;code&gt;(n_outputs,)&lt;/code&gt; is passed, then its entries are interpreted as weights and an according weighted average is returned. If &lt;code&gt;multioutput&lt;/code&gt; is &lt;code&gt;'raw_values'&lt;/code&gt; is specified, then all unaltered individual scores or losses will be returned in an array of shape &lt;code&gt;(n_outputs,)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f744234c4fef479ca52103694dddbbb39569d67" translate="yes" xml:space="preserve">
          <source>These functions return a tuple &lt;code&gt;(X, y)&lt;/code&gt; consisting of a &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; numpy array &lt;code&gt;X&lt;/code&gt; and an array of length &lt;code&gt;n_samples&lt;/code&gt; containing the targets &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ea4b087a47ece8eb67eda1c13d069b3e5f40d70" translate="yes" xml:space="preserve">
          <source>These generators produce a matrix of features and corresponding discrete targets.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7870342f67a34d74383fe781d3e91593569275ef" translate="yes" xml:space="preserve">
          <source>These images how similar features are merged together using feature agglomeration.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f6550a6d877a222d311c7f7d2e4efbab68b15f3" translate="yes" xml:space="preserve">
          <source>These matrices can be used to impose connectivity in estimators that use connectivity information, such as Ward clustering (&lt;a href=&quot;clustering#hierarchical-clustering&quot;&gt;Hierarchical clustering&lt;/a&gt;), but also to build precomputed kernels, or similarity matrices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1476efa19922a5a7b34be362fc1e06a3ae81118" translate="yes" xml:space="preserve">
          <source>These metrics &lt;strong&gt;require the knowledge of the ground truth classes&lt;/strong&gt; while almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1b78634dcd7b938ef7f64bb3d2756751845f06a1" translate="yes" xml:space="preserve">
          <source>These objects take as input a scoring function that returns univariate scores and p-values (or only scores for &lt;a href=&quot;generated/sklearn.feature_selection.selectkbest#sklearn.feature_selection.SelectKBest&quot;&gt;&lt;code&gt;SelectKBest&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_selection.selectpercentile#sklearn.feature_selection.SelectPercentile&quot;&gt;&lt;code&gt;SelectPercentile&lt;/code&gt;&lt;/a&gt;):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3403571e1d1dd77b054e08b8e749a4d9b5c4d316" translate="yes" xml:space="preserve">
          <source>These parameters can be accessed through the members &lt;code&gt;dual_coef_&lt;/code&gt; which holds the difference \(\alpha_i - \alpha_i^*\), &lt;code&gt;support_vectors_&lt;/code&gt; which holds the support vectors, and &lt;code&gt;intercept_&lt;/code&gt; which holds the independent term \(\rho\)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9254aef96f1c8727db185406da2727e74822dda9" translate="yes" xml:space="preserve">
          <source>These quantities are also related to the (\(F_1\)) score, which is defined as the harmonic mean of precision and recall.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ec6209edcb97e57d934d74900289c4f7467ca16" translate="yes" xml:space="preserve">
          <source>These represent the 14 features measured at each point of the map grid. The latitude/longitude values for the grid are discussed below. Missing data is represented by the value -9999.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e11ef00d62661e429b4b798a345a9c8d62a348e6" translate="yes" xml:space="preserve">
          <source>These steps are performed either a maximum number of times (&lt;code&gt;max_trials&lt;/code&gt;) or until one of the special stop criteria are met (see &lt;code&gt;stop_n_inliers&lt;/code&gt; and &lt;code&gt;stop_score&lt;/code&gt;). The final model is estimated using all inlier samples (consensus set) of the previously determined best model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eff89650d9905b662c6df80228e926f2f144c91b" translate="yes" xml:space="preserve">
          <source>These three distances are special cases of the beta-divergence family, with \(\beta = 2, 1, 0\) respectively &lt;a href=&quot;#id15&quot; id=&quot;id8&quot;&gt;[6]&lt;/a&gt;. The beta-divergence are defined by :</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="100dafc268c3e9d0b628da1715aed2440b14ba32" translate="yes" xml:space="preserve">
          <source>These throughputs are achieved on a single process. An obvious way to increase the throughput of your application is to spawn additional instances (usually processes in Python because of the &lt;a href=&quot;https://wiki.python.org/moin/GlobalInterpreterLock&quot;&gt;GIL&lt;/a&gt;) that share the same model. One might also add machines to spread the load. A detailed explanation on how to achieve this is beyond the scope of this documentation though.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e63971597c4df5f2dc150f90b566e1219b6a632a" translate="yes" xml:space="preserve">
          <source>They are not sparse, i.e., they use the whole samples/features information to perform the prediction.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26f84bd6fe103542912ebb1fb588d29515a2fd6d" translate="yes" xml:space="preserve">
          <source>They can be loaded using the following functions:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f7ddbd9f45ee3f137b5eb656135dce7584351da0" translate="yes" xml:space="preserve">
          <source>They expose a &lt;code&gt;split&lt;/code&gt; method which accepts the input dataset to be split and yields the train/test set indices for each iteration of the chosen cross-validation strategy.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="03bbcc98e1d567dd0afc112fe378a99851c98226" translate="yes" xml:space="preserve">
          <source>They lose efficiency in high dimensional spaces &amp;ndash; namely when the number of features exceeds a few dozens.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a673690dbc33eee17ee6f3995f817097918876ff" translate="yes" xml:space="preserve">
          <source>This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9994b645c162ae9e80a2c6bc1c81390f2e92325" translate="yes" xml:space="preserve">
          <source>This Warning is used in meta estimators GridSearchCV and RandomizedSearchCV and the cross-validation helper function cross_val_score to warn when there is an error while fitting the estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f5e3d23312509826c13f1663d34b7e7912ac4af" translate="yes" xml:space="preserve">
          <source>This algorithm can be viewed as an instance or data reduction method, since it reduces the input data to a set of subclusters which are obtained directly from the leaves of the CFT. This reduced data can be further processed by feeding it into a global clusterer. This global clusterer can be set by &lt;code&gt;n_clusters&lt;/code&gt;. If &lt;code&gt;n_clusters&lt;/code&gt; is set to None, the subclusters from the leaves are directly read off, otherwise a global clustering step labels these subclusters into global clusters (labels) and the samples are mapped to the global label of the nearest subcluster.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="895ec6bb1e64f58ea59b9ae781fa620e703125a9" translate="yes" xml:space="preserve">
          <source>This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting &lt;a href=&quot;#r4d113ba76fc0-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;. If samples are drawn with replacement, then the method is known as Bagging &lt;a href=&quot;#r4d113ba76fc0-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces &lt;a href=&quot;#r4d113ba76fc0-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches &lt;a href=&quot;#r4d113ba76fc0-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="870122c945d57891e89c2ba931542331f1b4afdb" translate="yes" xml:space="preserve">
          <source>This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting &lt;a href=&quot;#rb1846455d0e5-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;. If samples are drawn with replacement, then the method is known as Bagging &lt;a href=&quot;#rb1846455d0e5-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt;. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces &lt;a href=&quot;#rb1846455d0e5-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches &lt;a href=&quot;#rb1846455d0e5-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="866e774dfaaba96a720c3090067c49c13cac935f" translate="yes" xml:space="preserve">
          <source>This algorithm finds a (usually very good) approximate truncated singular value decomposition using randomization to speed up the computations. It is particularly fast on large matrices on which you wish to extract only a small number of components. In order to obtain further speed up, &lt;code&gt;n_iter&lt;/code&gt; can be set &amp;lt;=2 (at the cost of loss of precision).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="77977d31f5357c834112cbb27bbc98b5c64c10d7" translate="yes" xml:space="preserve">
          <source>This algorithm has constant memory complexity, on the order of &lt;code&gt;batch_size&lt;/code&gt;, enabling use of np.memmap files without loading the entire file into memory.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9586aed3b1a5b6a2c44b32af5cc0558b6ad496a6" translate="yes" xml:space="preserve">
          <source>This algorithm solves the normalized cut for k=2: it is a normalized spectral clustering.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="01c65a021c885e4e00baaaa5c6652a314a97daa3" translate="yes" xml:space="preserve">
          <source>This algorithm will always use all the components it has access to, needing held-out data or information theoretical criteria to decide how many components to use in the absence of external cues.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ddf04fe856314e7dd4ddddf49f4086029e04d842" translate="yes" xml:space="preserve">
          <source>This allows better model selection than probabilistic PCA in the presence of heteroscedastic noise:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3307a2458ebbefda8ea7fe8b898077ec4cadcf2c" translate="yes" xml:space="preserve">
          <source>This also works where final estimator is &lt;code&gt;None&lt;/code&gt;: all prior transformations are applied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c63b80512d8853cd76b24210ee07543da5c60fc4" translate="yes" xml:space="preserve">
          <source>This assumption is the base of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vector_Space_Model&quot;&gt;Vector Space Model&lt;/a&gt; often used in text classification and clustering contexts.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8142b653ecd3322ad782e8a8807635d6c4c0325e" translate="yes" xml:space="preserve">
          <source>This calibration results in a lower log-loss. Note that an alternative would have been to increase the number of base estimators which would have resulted in a similar decrease in log-loss.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ba422e075fa10216e381bf46530abda4e719fab" translate="yes" xml:space="preserve">
          <source>This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="345a3bd30c8553d3251f728b344d1bd100958670" translate="yes" xml:space="preserve">
          <source>This can be confirmed on a independent testing set with similar remarks:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e60927eda9d0f07135f56fa39eaa1a8f1f9c2813" translate="yes" xml:space="preserve">
          <source>This can be done by introducing &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors&quot;&gt;uninformative priors&lt;/a&gt; over the hyper parameters of the model. The \(\ell_{2}\) regularization used in &lt;a href=&quot;#id2&quot;&gt;Ridge Regression&lt;/a&gt; is equivalent to finding a maximum a posteriori estimation under a Gaussian prior over the parameters \(w\) with precision \(\lambda^{-1}\). Instead of setting &lt;code&gt;lambda&lt;/code&gt; manually, it is possible to treat it as a random variable to be estimated from the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa181018cfeb3219e1e67073f9c58ca90a0c4faa" translate="yes" xml:space="preserve">
          <source>This can be done by using the &lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt;&lt;code&gt;train_test_split&lt;/code&gt;&lt;/a&gt; utility function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="821e5435b62e56a883478da64d6731f6479103d5" translate="yes" xml:space="preserve">
          <source>This can be set to a higher value than the actual number of features in any of the input files, but setting it to a lower value will cause an exception to be raised.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a0bd36c1ccd6d51b38a900236d58695657d5aff" translate="yes" xml:space="preserve">
          <source>This class allows to infer an approximate posterior distribution over the parameters of a Gaussian mixture distribution. The effective number of components can be inferred from the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6130cf2c7564234b715447525f16ff596ebe842e" translate="yes" xml:space="preserve">
          <source>This class can be used to cross-validate time series data samples that are observed at fixed time intervals.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88afb4091eb2a25b2e2017ee8508b1ff5c5ae125" translate="yes" xml:space="preserve">
          <source>This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd60f6580be8e1908408c4fbfd8d3915f46e55b1" translate="yes" xml:space="preserve">
          <source>This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="350693d0493245dcbd676a8f10e001d5020f745e" translate="yes" xml:space="preserve">
          <source>This class implements regularized logistic regression using the &amp;lsquo;liblinear&amp;rsquo; library, &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c107aea4a3526193efe1f31d97ccab92891d87f" translate="yes" xml:space="preserve">
          <source>This class implements the Graphical Lasso algorithm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="456e7e6f7be68de425401d6f66fe28d8a1090538" translate="yes" xml:space="preserve">
          <source>This class implements the algorithm known as AdaBoost-SAMME [2].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="003b6bf538c58eeda3c6fd28b21967bfd8b6c40e" translate="yes" xml:space="preserve">
          <source>This class implements the algorithm known as AdaBoost.R2 [2].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="63c4b52b10df12140782204ea7cf58fe0edab9de" translate="yes" xml:space="preserve">
          <source>This class implements two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e15e7e7d8d04d41fdd2a4f34183baa0366e86dea" translate="yes" xml:space="preserve">
          <source>This class inherits from PLS with mode=&amp;rdquo;A&amp;rdquo; and deflation_mode=&amp;rdquo;canonical&amp;rdquo;, norm_y_weights=True and algorithm=&amp;rdquo;nipals&amp;rdquo;, but svd should provide similar results up to numerical errors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0a1d1daa83e6ad727cb13fd589f28f37b44df20" translate="yes" xml:space="preserve">
          <source>This class inherits from both ValueError and AttributeError to help with exception handling and backward compatibility.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d34b4e0a14d3a494edeba399329a47fb49b3ee6c" translate="yes" xml:space="preserve">
          <source>This class is a low-memory alternative to DictVectorizer and CountVectorizer, intended for large-scale (online) learning and situations where memory is tight, e.g. when running prediction code on embedded devices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e34d865883b535f68990f01a240eaab9f87f080" translate="yes" xml:space="preserve">
          <source>This class is hence suitable for use in the early steps of a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eef0760dd6d25fd731b9abce61656453bb690cee" translate="yes" xml:space="preserve">
          <source>This class is useful when the behavior of &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt;&lt;code&gt;LeavePGroupsOut&lt;/code&gt;&lt;/a&gt; is desired, but the number of groups is large enough that generating all possible partitions with \(P\) groups withheld would be prohibitively expensive. In such a scenario, &lt;a href=&quot;generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit&quot;&gt;&lt;code&gt;GroupShuffleSplit&lt;/code&gt;&lt;/a&gt; provides a random sample (with replacement) of the train / test splits generated by &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt;&lt;code&gt;LeavePGroupsOut&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="edf4c29bfa2afe43016dc0b6660ad50132bd51ec" translate="yes" xml:space="preserve">
          <source>This class provides a uniform interface to fast distance metric functions. The various metrics can be accessed via the &lt;code&gt;get_metric&lt;/code&gt; class method and the metric string identifier (see below). For example, to use the Euclidean distance:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="336f533fd7cf96bc18f597ff7211d31f0cd62386" translate="yes" xml:space="preserve">
          <source>This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aebcf6792861578bf4b4a69a0be71898d4023b6a" translate="yes" xml:space="preserve">
          <source>This class supports both dense and sparse input.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4042c6697e9df3310aa51f4f2bbe289c3d222c6e" translate="yes" xml:space="preserve">
          <source>This class turns sequences of symbolic feature names (strings) into scipy.sparse matrices, using a hash function to compute the matrix column corresponding to a name. The hash function employed is the signed 32-bit version of Murmurhash3.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="afb2ca6e635ea8a6abf9d5cec38428c8ecdc147c" translate="yes" xml:space="preserve">
          <source>This classification dataset is constructed by taking a multi-dimensional standard normal distribution and defining classes separated by nested concentric multi-dimensional spheres such that roughly equal numbers of samples are in each class (quantiles of the \(\chi^2\) distribution).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a35e593acabc67ebdf8fae8d0484d5f85056d4a7" translate="yes" xml:space="preserve">
          <source>This classifier is useful as a simple baseline to compare with other (real) classifiers. Do not use it for real problems.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ceac36d1efc9bb429dd84350330a84101bb5c8c" translate="yes" xml:space="preserve">
          <source>This classifier lost over a lot of its F-score, just because we removed metadata that has little to do with topic classification. It loses even more if we also strip this metadata from the training data:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="064e5da463cfecd3ca166c4023a79d0a6500160d" translate="yes" xml:space="preserve">
          <source>This combination is implementing in &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt;, a transformer class that is mostly API compatible with &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt; is stateless, meaning that you don&amp;rsquo;t have to call &lt;code&gt;fit&lt;/code&gt; on it:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d56591f4aa7f0ebb864c2d8835af7dfb7f8f26b" translate="yes" xml:space="preserve">
          <source>This combines the values of agglomerated features into a single value, and should accept an array of shape [M, N] and the keyword argument &lt;code&gt;axis=1&lt;/code&gt;, and reduce it to an array of size [M].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26c788fe31e79bd1bbf24fbba8a1b8342ff15ce1" translate="yes" xml:space="preserve">
          <source>This consumes less memory than shuffling the data directly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c8cf431c4f1299ae41612f84a50a597aa4a1059" translate="yes" xml:space="preserve">
          <source>This creates binary hashes of input data points by getting the dot product of input points and hash_function then transforming the projection into a binary string array based on the sign (positive/negative) of the projection. A sorted array of binary hashes is created.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="75f340063df2a6996986c297d7d4423dc4417e05" translate="yes" xml:space="preserve">
          <source>This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab90d891a9b7f61040a0bd2fc78eae2488d53a3a" translate="yes" xml:space="preserve">
          <source>This cross-validation object is a variation of &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt;. In the kth split, it returns first k folds as train set and the (k+1)th fold as test set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3d6a6171342c06e134b7087a7e83e3f80ba8a79" translate="yes" xml:space="preserve">
          <source>This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c34ca372157ddad0d3d18ffb66a8717775276f6" translate="yes" xml:space="preserve">
          <source>This data sets consists of 3 different types of irises&amp;rsquo; (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="158d23b76a72fb850a277110200a4b319b51d7f5" translate="yes" xml:space="preserve">
          <source>This database is also available through the UW CS ftp server:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f0ed5f875aa21e65ca9d6116dc0e918ff34c0ff" translate="yes" xml:space="preserve">
          <source>This dataset consists of 20,640 samples and 9 features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2e15d3adbea56bdf31e76cb4ee55fcf6dfb7c05f" translate="yes" xml:space="preserve">
          <source>This dataset is a collection of JPEG pictures of famous people collected over the internet, all details are available on the official website:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c0b9129c637e05601004735cb7bfdbdba431796" translate="yes" xml:space="preserve">
          <source>This dataset is described in Celeux et al [1]. as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e92704f97c6e77c413a0405e7cf7dd2e32191660" translate="yes" xml:space="preserve">
          <source>This dataset is described in Friedman [1] and Breiman [2].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="473d3b557a1c23b381f633c2c2acf0c38c2a328f" translate="yes" xml:space="preserve">
          <source>This dataset is made up of 1797 8x8 images. Each image, like the one shown below, is of a hand-written digit. In order to utilize an 8x8 figure like this, we&amp;rsquo;d have to first transform it into a feature vector with length 64.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="73cdbbbbdb25af126933f658f4b065e86b9c1a55" translate="yes" xml:space="preserve">
          <source>This dataset represents the geographic distribution of species. The dataset is provided by Phillips et. al. (2006).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e72397d5f7691c5c60df49442bb007cee4717786" translate="yes" xml:space="preserve">
          <source>This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47bf559cf1abc9fb33a96a120f583ad0bcd0f9f8" translate="yes" xml:space="preserve">
          <source>This dataset was obtained from the StatLib repository. &lt;a href=&quot;http://lib.stat.cmu.edu/datasets/&quot;&gt;http://lib.stat.cmu.edu/datasets/&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4668c093fae272aec3196fc6d39e4e5a4eede980" translate="yes" xml:space="preserve">
          <source>This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b1b90be23e0adb200134bcc2570822a79ae6e88" translate="yes" xml:space="preserve">
          <source>This demonstrates Label Propagation learning a good boundary even with a small amount of labeled data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="33d36bf521beb70b7b2f4b7e5d24d58f96f46c86" translate="yes" xml:space="preserve">
          <source>This description can be vectorized into a sparse two-dimensional matrix suitable for feeding into a classifier (maybe after being piped into a &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;text.TfidfTransformer&lt;/code&gt;&lt;/a&gt; for normalization):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="12f7e332bc936dbb460a17349dda07780cab7782" translate="yes" xml:space="preserve">
          <source>This determines which warnings will be made in the case that this function is being used to return only one of its metrics.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a1a66d0ad9255f63c11b93170b95da4e6eeaea4f" translate="yes" xml:space="preserve">
          <source>This downscaling is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;tf&amp;ndash;idf&lt;/a&gt; for &amp;ldquo;Term Frequency times Inverse Document Frequency&amp;rdquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="edeebc499fdf886cf2b1fe82f9cc25a148384f70" translate="yes" xml:space="preserve">
          <source>This early stopping strategy is activated if &lt;code&gt;early_stopping=True&lt;/code&gt;; otherwise the stopping criterion only uses the training loss on the entire input data. To better control the early stopping strategy, we can specify a parameter &lt;code&gt;validation_fraction&lt;/code&gt; which set the fraction of the input dataset that we keep aside to compute the validation score. The optimization will continue until the validation score did not improve by at least &lt;code&gt;tol&lt;/code&gt; during the last &lt;code&gt;n_iter_no_change&lt;/code&gt; iterations. The actual number of iterations is available at the attribute &lt;code&gt;n_iter_&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6528dcf2523991bd357ca56474b42d537ada09b8" translate="yes" xml:space="preserve">
          <source>This embedding can also &amp;lsquo;work&amp;rsquo; even if the &lt;code&gt;adjacency&lt;/code&gt; variable is not strictly the adjacency matrix of a graph but more generally an affinity or similarity matrix between samples (for instance the heat kernel of a euclidean distance matrix or a k-NN matrix).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="662188aeeffeee289aab2f0d97150266d90c022d" translate="yes" xml:space="preserve">
          <source>This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="752036d9bd5ae374e975c046f504e0b38de39538" translate="yes" xml:space="preserve">
          <source>This estimator</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36ceeb9f387752a577aeb048b3f21cd319ecfb48" translate="yes" xml:space="preserve">
          <source>This estimator allows different columns or column subsets of the input to be transformed separately and the results combined into a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02199e2b9b2bd941c7464261eedf68a6fd2d82e2" translate="yes" xml:space="preserve">
          <source>This estimator applies a list of transformer objects in parallel to the input data, then concatenates the results. This is useful to combine several feature extraction mechanisms into a single transformer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a93ab3d6ae360c030a56bd4fabca46c42abdaf54" translate="yes" xml:space="preserve">
          <source>This estimator approximates a slightly different version of the additive chi squared kernel then &lt;code&gt;metric.additive_chi2&lt;/code&gt; computes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57f1dab8dd3e838e06f9128461ff865667eb2891" translate="yes" xml:space="preserve">
          <source>This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab45bcef3fd31ebffe9c4724334c2d64921dae44" translate="yes" xml:space="preserve">
          <source>This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning, see the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5fdb17bfdb14f498d9377f8ca9b7cc2199a41d7f" translate="yes" xml:space="preserve">
          <source>This estimator is stateless (besides constructor parameters), the fit method does nothing but is useful when used in a pipeline.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42d08f109b32ce107e9f058e7449521b0b6eab28" translate="yes" xml:space="preserve">
          <source>This estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ce7850baf5a7a3e7ef75716db2d2e4af71c92137" translate="yes" xml:space="preserve">
          <source>This estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ff92b3f8e56701f386ccbc5279ec5fdb2974bc0" translate="yes" xml:space="preserve">
          <source>This estimator scales each feature individually such that the maximal absolute value of each feature in the training set will be 1.0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="354214ed410106228bbb593cd82a49f32a2508f8" translate="yes" xml:space="preserve">
          <source>This estimator supports two algorithms: a fast randomized SVD solver, and a &amp;ldquo;naive&amp;rdquo; algorithm that uses ARPACK as an eigensolver on (X * X.T) or (X.T * X), whichever is more efficient.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3ed5861f671a7b7a1c102cd9c37c8bf2e273de13" translate="yes" xml:space="preserve">
          <source>This estimator will run an extensive test-suite for input validation, shapes, etc. Additional tests for classifiers, regressors, clustering or transformers will be run if the Estimator class inherits from the corresponding mixin from sklearn.base.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7011f5e2b484f266188acd0aba4f3d3175f11ed0" translate="yes" xml:space="preserve">
          <source>This example also shows the usefulness of applying Ridge regression to highly ill-conditioned matrices. For such matrices, a slight change in the target variable can cause huge variances in the calculated weights. In such cases, it is useful to set a certain regularization (alpha) to reduce this variation (noise).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f82721c4674480adff4241dbcac8e543f16c868" translate="yes" xml:space="preserve">
          <source>This example applies to olivetti_faces different unsupervised matrix decomposition (dimension reduction) methods from the module &lt;a href=&quot;../../modules/classes#module-sklearn.decomposition&quot;&gt;&lt;code&gt;sklearn.decomposition&lt;/code&gt;&lt;/a&gt; (see the documentation chapter &lt;a href=&quot;../../modules/decomposition#decompositions&quot;&gt;Decomposing signals in components (matrix factorization problems)&lt;/a&gt;) .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="33924f5409489cd3edd1b22f28ee011b17a585da" translate="yes" xml:space="preserve">
          <source>This example compares 2 dimensionality reduction strategies:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ba8c26b14d0dc5555ed6b18d75cbed15c385668" translate="yes" xml:space="preserve">
          <source>This example compares non-nested and nested cross-validation strategies on a classifier of the iris data set. Nested cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search. Choosing the parameters that maximize non-nested CV biases the model to the dataset, yielding an overly-optimistic score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ef1cb506f6769f9ea8f57cc850c6090d62898eb" translate="yes" xml:space="preserve">
          <source>This example compares the timing of Birch (with and without the global clustering step) and MiniBatchKMeans on a synthetic dataset having 100,000 samples and 2 features generated using make_blobs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8901e1f5225dc1b7e06d2fabb8f06f19a3753c45" translate="yes" xml:space="preserve">
          <source>This example constructs a pipeline that does dimensionality reduction followed by prediction with a support vector classifier. It demonstrates the use of &lt;code&gt;GridSearchCV&lt;/code&gt; and &lt;code&gt;Pipeline&lt;/code&gt; to optimize over different classes of estimators in a single CV run &amp;ndash; unsupervised &lt;code&gt;PCA&lt;/code&gt; and &lt;code&gt;NMF&lt;/code&gt; dimensionality reductions are compared to univariate feature selection during the grid search.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b084d11db0bc218128b35a7afe55ac9fa7c4daf1" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to approximate a function with a polynomial of degree n_degree by using ridge regression. Concretely, from n_samples 1d points, it suffices to build the Vandermonde matrix, which is n_samples x n_degree+1 and has the following form:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8509d7895b98e9b3d2d07ed03eae90baa68f1c72" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to generate a checkerboard dataset and bicluster it using the Spectral Biclustering algorithm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb9a20a0b6ffdb624c38c357324f211d180af27f" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to generate a dataset and bicluster it using the Spectral Co-Clustering algorithm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4e024e96a6e8109f327c2d890a3a85ee374e03bd" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to use &lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt; on a dataset containing different types of features. We use the 20-newsgroups dataset and compute standard bag-of-words features for the subject line and body in separate pipelines as well as ad hoc features on the body. We combine them (with weights) using a ColumnTransformer and finally train a classifier on the combined set of features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="76d491fec0fed042e6d7927f2dc124b698f9bded" translate="yes" xml:space="preserve">
          <source>This example demonstrates the Spectral Co-clustering algorithm on the twenty newsgroups dataset. The &amp;lsquo;comp.os.ms-windows.misc&amp;rsquo; category is excluded because it contains many posts containing nothing but data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6186f51b55d8cba756254a15fe651e5e8504791a" translate="yes" xml:space="preserve">
          <source>This example demonstrates the behavior of Gaussian mixture models fit on data that was not sampled from a mixture of Gaussian random variables. The dataset is formed by 100 points loosely spaced following a noisy sine curve. There is therefore no ground truth value for the number of Gaussian components.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a1949f51dde2d60d7d4d1e707d145c1301537434" translate="yes" xml:space="preserve">
          <source>This example demonstrates the power of semisupervised learning by training a Label Spreading model to classify handwritten digits with sets of very few labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c6020c6a7334e89ced3b2c5f4b02f4ed9989e37f" translate="yes" xml:space="preserve">
          <source>This example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions. The plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called &lt;strong&gt;underfitting&lt;/strong&gt;. A polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model will &lt;strong&gt;overfit&lt;/strong&gt; the training data, i.e. it learns the noise of the training data. We evaluate quantitatively &lt;strong&gt;overfitting&lt;/strong&gt; / &lt;strong&gt;underfitting&lt;/strong&gt; by using cross-validation. We calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab8d51c9ac9762c2930c84a5a03c1c12345a6627" translate="yes" xml:space="preserve">
          <source>This example demonstrates the use of the Box-Cox and Yeo-Johnson transforms through &lt;code&gt;preprocessing.PowerTransformer&lt;/code&gt; to map data from various distributions to a normal distribution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b62ef0be0bf7ce49acab00a23e04164c0becf9d" translate="yes" xml:space="preserve">
          <source>This example does not perform any learning over the data (see &lt;a href=&quot;../applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;Species distribution modeling&lt;/a&gt; for an example of classification based on the attributes in this dataset). It simply shows the kernel density estimate of observed data points in geospatial coordinates.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="03eea75ae69c05ca0f9dc1a13d89bbd210d48145" translate="yes" xml:space="preserve">
          <source>This example doesn&amp;rsquo;t show it, as we&amp;rsquo;re in a low-dimensional space, but another advantage of the Dirichlet process model is that it can fit full covariance matrices effectively even when there are less examples per cluster than there are dimensions in the data, due to regularization properties of the inference algorithm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b398c0b1dbb0f3edb9cc17097a9c9f8696a24cc" translate="yes" xml:space="preserve">
          <source>This example employs several unsupervised learning techniques to extract the stock market structure from variations in historical quotes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41937f256baaea1198c4d043d4bb81b61df0d50b" translate="yes" xml:space="preserve">
          <source>This example fits a Gradient Boosting model with least squares loss and 500 regression trees of depth 4.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e8121408498cf6fb8c886d50eef2580465ff3307" translate="yes" xml:space="preserve">
          <source>This example fits an AdaBoosted decision stump on a non-linearly separable classification dataset composed of two &amp;ldquo;Gaussian quantiles&amp;rdquo; clusters (see &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_gaussian_quantiles#sklearn.datasets.make_gaussian_quantiles&quot;&gt;&lt;code&gt;sklearn.datasets.make_gaussian_quantiles&lt;/code&gt;&lt;/a&gt;) and plots the decision boundary and decision scores. The distributions of decision scores are shown separately for samples of class A and B. The predicted class label for each sample is determined by the sign of the decision score. Samples with decision scores greater than zero are classified as B, and are otherwise classified as A. The magnitude of a decision score determines the degree of likeness with the predicted class label. Additionally, a new dataset could be constructed containing a desired purity of class B, for example, by only selecting samples with a decision score above some value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02c230a18f98a72777c5f2652062014b16511fe8" translate="yes" xml:space="preserve">
          <source>This example has a fair amount of visualization-related code, as visualization is crucial here to display the graph. One of the challenge is to position the labels minimizing overlap. For this we use an heuristic based on the direction of the nearest neighbor along each axis.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="259139974bd9ca7304e763dff02af979bb7908e6" translate="yes" xml:space="preserve">
          <source>This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt;) and a non-stationary kernel (&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt;). On this particular dataset, the &lt;code&gt;DotProduct&lt;/code&gt; kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In practice, however, stationary kernels such as &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; often obtain better results.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a467b781e30c272f4f5e4645f1261bd726b14e8d" translate="yes" xml:space="preserve">
          <source>This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (RBF) and a non-stationary kernel (DotProduct). On this particular dataset, the DotProduct kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In general, stationary kernels often obtain better results.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b43f5231cb55a1a95a64bd8afe5472e7955fc98b" translate="yes" xml:space="preserve">
          <source>This example illustrates and compares the bias-variance decomposition of the expected mean squared error of a single estimator against a bagging ensemble.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0a3450a632d656139c95a4f138b569cec22ba6ef" translate="yes" xml:space="preserve">
          <source>This example illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The first figure compares the learned model of KRR and SVR when both complexity/regularization and bandwidth of the RBF kernel are optimized using grid-search. The learned functions are very similar; however, fitting KRR is approx. seven times faster than fitting SVR (both with grid-search). However, prediction of 100000 target values is more than tree times faster with SVR since it has learned a sparse model using only approx. 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bdbaaa805869c3c13d157f267aeffaf332ff1284" translate="yes" xml:space="preserve">
          <source>This example illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise. The figure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited for learning periodic functions. The kernel&amp;rsquo;s hyperparameters control the smoothness (l) and periodicity of the kernel (p). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="745a420beb7d4cfe3dcb516bc28a36f2576e5395" translate="yes" xml:space="preserve">
          <source>This example illustrates how sigmoid calibration changes predicted probabilities for a 3-class classification problem. Illustrated is the standard 2-simplex, where the three corners correspond to the three classes. Arrows point from the probability vectors predicted by an uncalibrated classifier to the probability vectors predicted by the same classifier after sigmoid calibration on a hold-out validation set. Colors indicate the true class of an instance (red: class 1, green: class 2, blue: class 3).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="12c733d8527ccd2c1862324a52d9d453fa3717b9" translate="yes" xml:space="preserve">
          <source>This example illustrates how the Mahalanobis distances are affected by outlying data: observations drawn from a contaminating distribution are not distinguishable from the observations coming from the real, Gaussian distribution that one may want to work with. Using MCD-based Mahalanobis distances, the two populations become distinguishable. Associated applications are outliers detection, observations ranking, clustering, &amp;hellip; For visualization purpose, the cubic root of the Mahalanobis distances are represented in the boxplot, as Wilson and Hilferty suggest [2]</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e6c2656adbcd3c5e59b375bc18300061f72c931d" translate="yes" xml:space="preserve">
          <source>This example illustrates how the early stopping can used in the &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;sklearn.ensemble.GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; model to achieve almost the same accuracy as compared to a model built without early stopping using many fewer estimators. This can significantly reduce training time, memory usage and prediction latency.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c9534999032b13d85edbba90f8420279af14435d" translate="yes" xml:space="preserve">
          <source>This example illustrates how the early stopping can used in the &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt; model to achieve almost the same accuracy as compared to a model built without early stopping. This can significantly reduce training time. Note that scores differ between the stopping criteria even from early iterations because some of the training data is held out with the validation stopping criterion.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c861b8fa50d5165e1e9cdc8044114c0ba76be7f1" translate="yes" xml:space="preserve">
          <source>This example illustrates how to apply different preprocessing and feature extraction pipelines to different subsets of features, using &lt;a href=&quot;../../modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt;. This is particularly handy for the case of datasets that contain heterogeneous data types, since we may want to scale the numeric features and one-hot encode the categorical ones.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7688b615fac5133028d275260a50b4a9e7d6a213" translate="yes" xml:space="preserve">
          <source>This example illustrates that GPR with a sum-kernel including a WhiteKernel can estimate the noise level of data. An illustration of the log-marginal-likelihood (LML) landscape shows that there exist two local maxima of LML.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aab3b4258aabcd6bfe55c7c5adf04622c59229dc" translate="yes" xml:space="preserve">
          <source>This example illustrates that GPR with a sum-kernel including a WhiteKernel can estimate the noise level of data. An illustration of the log-marginal-likelihood (LML) landscape shows that there exist two local maxima of LML. The first corresponds to a model with a high noise level and a large length scale, which explains all variations in the data by noise. The second one has a smaller noise level and shorter length scale, which explains most of the variation by the noise-free functional relationship. The second model has a higher likelihood; however, depending on the initial value for the hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is thus important to repeat the optimization several times for different initializations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d19a1528c92e37a37fb4fd9cad2cad1ac3d91123" translate="yes" xml:space="preserve">
          <source>This example illustrates the differences between univariate F-test statistics and mutual information.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b411e019157b9b0953b37eb36e27bc6a8ffb3f5f" translate="yes" xml:space="preserve">
          <source>This example illustrates the effect of the parameters &lt;code&gt;gamma&lt;/code&gt; and &lt;code&gt;C&lt;/code&gt; of the Radial Basis Function (RBF) kernel SVM.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f49634dcf1598fde3c403bd7ddd702e3816c634" translate="yes" xml:space="preserve">
          <source>This example illustrates the need for robust covariance estimation on a real data set. It is useful both for outlier detection and for a better understanding of the data structure.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc09ff23a3cae32be328d47d0a0a7e64185cd75a" translate="yes" xml:space="preserve">
          <source>This example illustrates the predicted probability of GPC for an RBF kernel with different choices of the hyperparameters. The first figure shows the predicted probability of GPC with arbitrarily chosen hyperparameters and with the hyperparameters corresponding to the maximum log-marginal-likelihood (LML).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d49b62c58d1a1ad4b52cfdb4df97043fcb25dfc7" translate="yes" xml:space="preserve">
          <source>This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f8e8b4dfa6bc8bbc237c34d26328609c3561c0d3" translate="yes" xml:space="preserve">
          <source>This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. This illustrates the applicability of GPC to non-binary classification. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d60d503f722a9b87495f756d071794c2e2c52164" translate="yes" xml:space="preserve">
          <source>This example illustrates the prior and posterior of a GPR with different kernels. Mean, standard deviation, and 10 samples are shown for both prior and posterior.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e4f09a45ae58596e6f6f82a5f372f88442c0679" translate="yes" xml:space="preserve">
          <source>This example illustrates the use of the &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;multioutput.MultiOutputRegressor&lt;/a&gt; meta-estimator to perform multi-output regression. A random forest regressor is used, which supports multi-output regression natively, so the results can be compared.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8a4413b8994df2fb3a9be31d12e5c3eff453a657" translate="yes" xml:space="preserve">
          <source>This example illustrates visually in the feature space a comparison by results using two different component analysis techniques.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2cd2616385e5968100e838cea35843639b5d4649" translate="yes" xml:space="preserve">
          <source>This example is based on Figure 10.2 from Hastie et al 2009 &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; and illustrates the difference in performance between the discrete SAMME &lt;a href=&quot;#id4&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; boosting algorithm and real SAMME.R boosting algorithm. Both algorithms are evaluated on a binary classification task where the target Y is a non-linear function of 10 input features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f74f72b73b7f5fc12f50525ee3a073668f796ed6" translate="yes" xml:space="preserve">
          <source>This example is based on Section 5.4.3 of &amp;ldquo;Gaussian Processes for Machine Learning&amp;rdquo; [RW2006]. It illustrates an example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-likelihood. The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 2001. The objective is to model the CO2 concentration as a function of the time t.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b3b5741f90375b259727a574f2a0488e7637e5bc" translate="yes" xml:space="preserve">
          <source>This example is based on Section 5.4.3 of &lt;a href=&quot;#rw2006&quot; id=&quot;id2&quot;&gt;[RW2006]&lt;/a&gt;. It illustrates an example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-likelihood. The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 1997. The objective is to model the CO2 concentration as a function of the time t.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ecc1c02a68f38d70271669af08df8c1497b1d98" translate="yes" xml:space="preserve">
          <source>This example is commented in the &lt;a href=&quot;../../tutorial/basic/tutorial#introduction&quot;&gt;tutorial section of the user manual&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9143eb314f05280f157b4081755ef4be5d0d1857" translate="yes" xml:space="preserve">
          <source>This example is meant to illustrate situations where k-means will produce unintuitive and possibly unexpected clusters. In the first three plots, the input data does not conform to some implicit assumption that k-means makes and undesirable clusters are produced as a result. In the last plot, k-means returns intuitive clusters despite unevenly sized blobs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb84e7488212d58818869cb1a848aafb46dc6573" translate="yes" xml:space="preserve">
          <source>This example plots the covariance ellipsoids of each class and decision boundary learned by LDA and QDA. The ellipsoids display the double standard deviation for each class. With LDA, the standard deviation is the same for all the classes, while each class has its own standard deviation with QDA.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="61cf8846c08926de131cab630666b0d7a1ff4033" translate="yes" xml:space="preserve">
          <source>This example plots the ellipsoids obtained from a toy dataset (mixture of three Gaussians) fitted by the &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; class models with a Dirichlet distribution prior (&lt;code&gt;weight_concentration_prior_type='dirichlet_distribution'&lt;/code&gt;) and a Dirichlet process prior (&lt;code&gt;weight_concentration_prior_type='dirichlet_process'&lt;/code&gt;). On each figure, we plot the results for three different values of the weight concentration prior.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0d46d4faf4d4d45c7ba844c05b8ff931d890d6c" translate="yes" xml:space="preserve">
          <source>This example presents the different strategies implemented in KBinsDiscretizer:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aff95617011fc0f39a1d4573107679df90fa3c83" translate="yes" xml:space="preserve">
          <source>This example reproduces Figure 1 of Zhu et al &lt;a href=&quot;#id3&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; and shows how boosting can improve prediction accuracy on a multi-class problem. The classification dataset is constructed by taking a ten-dimensional standard normal distribution and defining three classes separated by nested concentric ten-dimensional spheres such that roughly equal numbers of samples are in each class (quantiles of the \(\chi^2\) distribution).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0193d5eae04ea4c45d1272eb8f24ceb76514e1e" translate="yes" xml:space="preserve">
          <source>This example serves as a visual check that IPCA is able to find a similar projection of the data to PCA (to a sign flip), while only processing a few samples at a time. This can be considered a &amp;ldquo;toy example&amp;rdquo;, as IPCA is intended for large datasets which do not fit in main memory, requiring incremental approaches.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3dfa9a56451c107730b94ff094ad060fe6660b05" translate="yes" xml:space="preserve">
          <source>This example should be taken with a grain of salt, as the intuition conveyed does not necessarily carry over to real datasets. Particularly in high-dimensional spaces, data can more easily be separated linearly. Moreover, using feature discretization and one-hot encoding increases the number of features, which easily lead to overfitting when the number of samples is small.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad21e470ff2bffe875ca12e50c6f8a883eaa0515" translate="yes" xml:space="preserve">
          <source>This example shows an example usage of the &lt;code&gt;split&lt;/code&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="05321a1b8964aa5eaada463be8f8b6ab44686817" translate="yes" xml:space="preserve">
          <source>This example shows characteristics of different anomaly detection algorithms on 2D datasets. Datasets contain one or two modes (regions of high density) to illustrate the ability of algorithms to cope with multimodal data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9671740bdc9e0f010272719df08d61d30b070724" translate="yes" xml:space="preserve">
          <source>This example shows characteristics of different clustering algorithms on datasets that are &amp;ldquo;interesting&amp;rdquo; but still in 2D. With the exception of the last dataset, the parameters of each of these dataset-algorithm pairs has been tuned to produce good clustering results. Some algorithms are more sensitive to parameter values than others.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="408c25df8162bc85c75adf89aefb6c4283aab313" translate="yes" xml:space="preserve">
          <source>This example shows characteristics of different linkage methods for hierarchical clustering on datasets that are &amp;ldquo;interesting&amp;rdquo; but still in 2D.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee904b77cbf769dbe7d1093eb852f864329f4bb5" translate="yes" xml:space="preserve">
          <source>This example shows how kernel density estimation (KDE), a powerful non-parametric density estimation technique, can be used to learn a generative model for a dataset. With this generative model in place, new samples can be drawn. These new samples reflect the underlying model of the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54102d8f78c42d496181e5bcdf5a40bdaee3e42d" translate="yes" xml:space="preserve">
          <source>This example shows how quantile regression can be used to create prediction intervals.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="682ea376dc5fd413204f119c7903367cc1b71149" translate="yes" xml:space="preserve">
          <source>This example shows how to build a classification pipeline with a BernoulliRBM feature extractor and a &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; classifier. The hyperparameters of the entire model (learning rate, hidden layer size, regularization) were optimized by grid search, but the search is not reproduced here because of runtime constraints.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e6287a37f5ab2f7e58b3aa65bfc9b371f8d2e434" translate="yes" xml:space="preserve">
          <source>This example shows how to obtain partial dependence plots from a &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; trained on the California housing dataset. The example is taken from &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd4b844c3488b502b915a6b11d22b13911beaa74" translate="yes" xml:space="preserve">
          <source>This example shows how to perform univariate feature selection before running a SVC (support vector classifier) to improve the classification scores.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47a26e628df959c3e5ed3ffe4f4f3490e8927a8d" translate="yes" xml:space="preserve">
          <source>This example shows how to plot some of the first layer weights in a MLPClassifier trained on the MNIST dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd07bd7e7afed03f3c2a3c74458c42dc14028dfe" translate="yes" xml:space="preserve">
          <source>This example shows how to plot the decision surface for four SVM classifiers with different kernels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52dcf2b8bf47a153b3ba3beca30c9af85b850fad" translate="yes" xml:space="preserve">
          <source>This example shows how to use &lt;code&gt;cross_val_predict&lt;/code&gt; to visualize prediction errors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="351c02b1031f149a08df70cbeed39cd2a6bb9ec7" translate="yes" xml:space="preserve">
          <source>This example shows that Kernel PCA is able to find a projection of the data that makes data linearly separable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="607fdb6fda0285694fd1dd982f83082f2f9a6687" translate="yes" xml:space="preserve">
          <source>This example shows that imputing the missing values can give better results than discarding the samples containing any missing value. Imputing does not always improve the predictions, so please check via cross-validation. Sometimes dropping rows or using marker values is more effective.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6045a3110197ccfd64402c3c879acac73bdaadb" translate="yes" xml:space="preserve">
          <source>This example shows that model selection can be performed with Gaussian Mixture Models using information-theoretic criteria (BIC). Model selection concerns both the covariance type and the number of components in the model. In that case, AIC also provides the right result (not shown to save time), but BIC is better suited if the problem is to identify the right model. Unlike Bayesian procedures, such inferences are prior-free.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ed4db944a196b1cb5ab23d8834c95cd5421c757" translate="yes" xml:space="preserve">
          <source>This example shows that you can do non-linear regression with a linear model, using a pipeline to add non-linear features. Kernel methods extend this idea and can induce very high (even infinite) dimensional feature spaces.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="48dcc848c2d6e1561f6c336d60b0fb518f9ab59a" translate="yes" xml:space="preserve">
          <source>This example shows the ROC response of different datasets, created from K-fold cross-validation. Taking all of these curves, it is possible to calculate the mean area under curve, and see the variance of the curve when the training set is split into different subsets. This roughly shows how the classifier output is affected by changes in the training data, and how different the splits generated by K-fold cross-validation are from one another.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b92e840bf44fca60c7edc394c5ccf3da0546857" translate="yes" xml:space="preserve">
          <source>This example shows the effect of imposing a connectivity graph to capture local structure in the data. The graph is simply the graph of 20 nearest neighbors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a122bd5b47879a72d10715b2e2741901d74ebd5f" translate="yes" xml:space="preserve">
          <source>This example shows the reconstruction of an image from a set of parallel projections, acquired along different angles. Such a dataset is acquired in &lt;strong&gt;computed tomography&lt;/strong&gt; (CT).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="277c7e399c7f521a9367c3279ba6605ffa32bb5b" translate="yes" xml:space="preserve">
          <source>This example shows the use of forests of trees to evaluate the importance of the pixels in an image classification task (faces). The hotter the pixel, the more important.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="181df9c721e88b620fbcd3038af372d2bc17958d" translate="yes" xml:space="preserve">
          <source>This example shows the use of multi-output estimator to complete images. The goal is to predict the lower half of a face given its upper half.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c194d9ad3fd820ff97a5b60a54a77ad5e096ac46" translate="yes" xml:space="preserve">
          <source>This example simulates a multi-label document classification problem. The dataset is generated randomly based on the following process:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="beca62f6aeebe1ac71c16bdf04b277689fc51da6" translate="yes" xml:space="preserve">
          <source>This example uses &lt;a href=&quot;../../modules/clustering#spectral-clustering&quot;&gt;Spectral clustering&lt;/a&gt; on a graph created from voxel-to-voxel difference on an image to break this image into multiple partly-homogeneous regions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b1b3f16a0bb367262a72b24da0008ca16f86a096" translate="yes" xml:space="preserve">
          <source>This example uses a large dataset of faces to learn a set of 20 x 20 images patches that constitute faces.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ad2a4281006cf2ce3833b3619a37abf58d678e0" translate="yes" xml:space="preserve">
          <source>This example uses different scalers, transformers, and normalizers to bring the data within a pre-defined range.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e1bfbae38c6acd2b8b362eacb6ca0227cf12cdc6" translate="yes" xml:space="preserve">
          <source>This example uses the &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt;&lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt;&lt;/a&gt; class to demonstrate the principles of Kernel Density Estimation in one dimension.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c91a9c343e9d52fecff06520d2432c55d78e2a0" translate="yes" xml:space="preserve">
          <source>This example uses the &lt;code&gt;scipy.stats&lt;/code&gt; module, which contains many useful distributions for sampling parameters, such as &lt;code&gt;expon&lt;/code&gt;, &lt;code&gt;gamma&lt;/code&gt;, &lt;code&gt;uniform&lt;/code&gt; or &lt;code&gt;randint&lt;/code&gt;. In principle, any function can be passed that provides a &lt;code&gt;rvs&lt;/code&gt; (random variate sample) method to sample a value. A call to the &lt;code&gt;rvs&lt;/code&gt; function should provide independent random samples from possible parameter values on consecutive calls.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9381762b80079a0275cf5384c50652951b2c3b8" translate="yes" xml:space="preserve">
          <source>This example uses the only the first feature of the &lt;code&gt;diabetes&lt;/code&gt; dataset, in order to illustrate a two-dimensional plot of this regression technique. The straight line can be seen in the plot, showing how linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c6bb5d76743a81844f0fc5afc16345d399cae103" translate="yes" xml:space="preserve">
          <source>This example visualizes some training loss curves for different stochastic learning strategies, including SGD and Adam. Because of time-constraints, we use several small datasets, for which L-BFGS might be more suitable. The general trend shown in these examples seems to carry over to larger datasets, however.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65646a35859e04e16667e59a0e282463725f9c9e" translate="yes" xml:space="preserve">
          <source>This example visualizes the behavior of several common scikit-learn objects for comparison.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49dcb9492cd2c3de6ca468ca869fddbd4adf1109" translate="yes" xml:space="preserve">
          <source>This example visualizes the partitions given by several trees and shows how the transformation can also be used for non-linear dimensionality reduction or non-linear classification.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6888e16176d5ba984d30e5b2aecac9e36e3202eb" translate="yes" xml:space="preserve">
          <source>This example will also work by replacing &lt;code&gt;SVC(kernel=&quot;linear&quot;)&lt;/code&gt; with &lt;code&gt;SGDClassifier(loss=&quot;hinge&quot;)&lt;/code&gt;. Setting the &lt;code&gt;loss&lt;/code&gt; parameter of the &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; equal to &lt;code&gt;hinge&lt;/code&gt; will yield behaviour such as that of a SVC with a linear kernel.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08633b59361c5b4332dffd09b9ac681bbe920080" translate="yes" xml:space="preserve">
          <source>This example, inspired from Chen&amp;rsquo;s publication [1], shows a comparison of the estimated MSE of the LW and OAS methods, using Gaussian distributed data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9260a90e6c35e520398765702d07497fe04f1a8" translate="yes" xml:space="preserve">
          <source>This examples shows how a classifier is optimized by cross-validation, which is done using the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt;&lt;/a&gt; object on a development set that comprises only half of the available labeled data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e16048c7f7cf75798d53fe7e675cc81cd1d6af8b" translate="yes" xml:space="preserve">
          <source>This examples shows the use of forests of trees to evaluate the importance of features on an artificial classification task. The red bars are the feature importances of the forest, along with their inter-trees variability.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4fb4f14900902539137295018be0a0c7a07b1094" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-estimators-tut&quot;&gt;Cross-validated estimators&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;Model selection: choosing estimators and their parameters&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="621b0c8349abb129c7bc150bd9745f46f49af3ab" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#cv-generators-tut&quot;&gt;Cross-validation generators&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/model_selection#model-selection-tut&quot;&gt;Model selection: choosing estimators and their parameters&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6915ba6643fea2f9e387885428a6e6189c7df3bf" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#clf-tut&quot;&gt;Classification&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;Supervised learning: predicting an output variable from high-dimensional observations&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="421684adc1a996556d28fe46ff4c101c2f3063ef" translate="yes" xml:space="preserve">
          <source>This exercise is used in the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#using-kernels-tut&quot;&gt;Using kernels&lt;/a&gt; part of the &lt;a href=&quot;../../tutorial/statistical_inference/supervised_learning#supervised-learning-tut&quot;&gt;Supervised learning: predicting an output variable from high-dimensional observations&lt;/a&gt; section of the &lt;a href=&quot;../../tutorial/statistical_inference/index#stat-learn-tut-index&quot;&gt;A tutorial on statistical-learning for scientific data processing&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dadb46eeaf7a2bf2f8f61dc107ba2d3f5d55d33a" translate="yes" xml:space="preserve">
          <source>This extends to the multiclass case as follows. Let the true labels for a set of samples be encoded as a 1-of-K binary indicator matrix \(Y\), i.e., \(y_{i,k} = 1\) if sample \(i\) has label \(k\) taken from a set of \(K\) labels. Let \(P\) be a matrix of probability estimates, with \(p_{i,k} = \operatorname{Pr}(t_{i,k} = 1)\). Then the log loss of the whole set is</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="826a67cf49f96f56e23921af61e52712fab61d33" translate="yes" xml:space="preserve">
          <source>This factory function wraps scoring functions for use in GridSearchCV and cross_val_score. It takes a score function, such as &lt;code&gt;accuracy_score&lt;/code&gt;, &lt;code&gt;mean_squared_error&lt;/code&gt;, &lt;code&gt;adjusted_rand_index&lt;/code&gt; or &lt;code&gt;average_precision&lt;/code&gt; and returns a callable that scores an estimator&amp;rsquo;s output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5eb7a4eb6e2161d3d9f2a7b4c7010506ccf35ad1" translate="yes" xml:space="preserve">
          <source>This feature corresponds to the sepal length in cm. Once the quantile transformation applied, those landmarks approach closely the percentiles previously defined:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18a1d2c5a41fd4d57af7a6bb802060cade230322" translate="yes" xml:space="preserve">
          <source>This feature selection algorithm looks only at the features (X), not the desired outputs (y), and can thus be used for unsupervised learning.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="677c582ff4a458e9dc8e636909bbbb985fe5cce6" translate="yes" xml:space="preserve">
          <source>This figure is created using the &lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt;&lt;code&gt;PolynomialFeatures&lt;/code&gt;&lt;/a&gt; preprocessor. This preprocessor transforms an input data matrix into a new data matrix of a given degree. It can be used as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="adbd1df9acbf84e51fe5dc83e34aca6a9423eabf" translate="yes" xml:space="preserve">
          <source>This figure shows an example of such an ROC curve:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e1207da0df5038f5f29891db83b7d5022ead8471" translate="yes" xml:space="preserve">
          <source>This folder is used by some large dataset loaders to avoid downloading the data several times.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="697a12fdadac01e298b3e16a8634659c2b054014" translate="yes" xml:space="preserve">
          <source>This format is a text-based format, with one sample per line. It does not store zero valued features hence is suitable for sparse dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2fe6f0ee6734c2a60bcbb1d0d95dc9dfd886002" translate="yes" xml:space="preserve">
          <source>This format is used as the default format for both svmlight and the libsvm command line programs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2e505f490185afab8e1242d5832b6872eb9a667" translate="yes" xml:space="preserve">
          <source>This formulation has two advantages over other ways of computing distances. First, it is computationally efficient when dealing with sparse data. Second, if one argument varies but the other remains unchanged, then &lt;code&gt;dot(x, x)&lt;/code&gt; and/or &lt;code&gt;dot(y, y)&lt;/code&gt; can be pre-computed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd76e45c139161a6c2340aa524dcf0429afb583e" translate="yes" xml:space="preserve">
          <source>This function computes Cohen&amp;rsquo;s kappa &lt;a href=&quot;#r219a3b9132e1-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;, a score that expresses the level of agreement between two annotators on a classification problem. It is defined as</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a26f30c64bc915159ba37349551edb033f8db68" translate="yes" xml:space="preserve">
          <source>This function computes for each row in X, the index of the row of Y which is closest (according to the specified distance).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40ca8ec3788866f2480b1619115207e644d05cac" translate="yes" xml:space="preserve">
          <source>This function computes for each row in X, the index of the row of Y which is closest (according to the specified distance). The minimal distances are also returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e14f241e1c03d6b67a6c0f22515d375ae432487" translate="yes" xml:space="preserve">
          <source>This function crawls the module and gets all classes that inherit from BaseEstimator. Classes that are defined in test-modules are not included. By default meta_estimators such as GridSearchCV are also not included.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="311d27372cf5315019acfac7480657777c623446" translate="yes" xml:space="preserve">
          <source>This function does not try to extract features into a numpy array or scipy sparse matrix. In addition, if load_content is false it does not try to load the files in memory.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="28042729acf4bf75d343c82f013b112dad91f4c8" translate="yes" xml:space="preserve">
          <source>This function generates a GraphViz representation of the decision tree, which is then written into &lt;code&gt;out_file&lt;/code&gt;. Once exported, graphical renderings can be generated using, for example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="90af5dc07a0d6b1b987fbe6284966c35b8f7dbed" translate="yes" xml:space="preserve">
          <source>This function implements Test 1 in:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51510777ab8c25d02b45243629e172250d646e40" translate="yes" xml:space="preserve">
          <source>This function is called with the estimated model and the randomly selected data: &lt;code&gt;is_model_valid(model, X, y)&lt;/code&gt;. If its return value is False the current randomly chosen sub-sample is skipped. Rejecting samples with this function is computationally costlier than with &lt;code&gt;is_data_valid&lt;/code&gt;. &lt;code&gt;is_model_valid&lt;/code&gt; should therefore only be used if the estimated model is needed for making the rejection decision.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4283f593950d3e9c84617d07a78fd011e78bfa4" translate="yes" xml:space="preserve">
          <source>This function is called with the randomly selected data before the model is fitted to it: &lt;code&gt;is_data_valid(X, y)&lt;/code&gt;. If its return value is False the current randomly chosen sub-sample is skipped.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7289fd594a0de96a89a572bf0a7bd6e9501fda52" translate="yes" xml:space="preserve">
          <source>This function is equivalent to mapping load_svmlight_file over a list of files, except that the results are concatenated into a single, flat list and the samples vectors are constrained to all have the same number of features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="828fa7414b6e6676bd49f6624bf5ec1232d13777" translate="yes" xml:space="preserve">
          <source>This function makes it possible to compute this transformation for a fixed set of class labels known ahead of time.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="910452d7cd5e91358a13a185cadee882cea17632" translate="yes" xml:space="preserve">
          <source>This function modifies the estimator in-place.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e0bd9f3948e27380d0112f277598d80d17269d2" translate="yes" xml:space="preserve">
          <source>This function requires the true binary value and the target scores, which can either be probability estimates of the positive class, confidence values, or binary decisions. Here is a small example of how to use the &lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt;&lt;code&gt;roc_curve&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ddc8c678c75b432a0f0fafa6490a9a69a784bf8" translate="yes" xml:space="preserve">
          <source>This function returns a score of the mean square difference between the actual outcome and the predicted probability of the possible outcome. The actual outcome has to be 1 or 0 (true or false), while the predicted probability of the actual outcome can be a value between 0 and 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fbdeef434a34fee928d2d9974f81cfc54768558d" translate="yes" xml:space="preserve">
          <source>This function returns posterior probabilities of classification according to each class on an array of test vectors X.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f7adc46ef6325367984cfe5d6cabca879706d70d" translate="yes" xml:space="preserve">
          <source>This function returns the Silhouette Coefficient for each sample.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="30221178098fdd3682a8c91454092d6226b25e4f" translate="yes" xml:space="preserve">
          <source>This function returns the mean Silhouette Coefficient over all samples. To obtain the values for each sample, use &lt;a href=&quot;sklearn.metrics.silhouette_samples#sklearn.metrics.silhouette_samples&quot;&gt;&lt;code&gt;silhouette_samples&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd812dc35d867d7152375e5009e2698c8db08fc0" translate="yes" xml:space="preserve">
          <source>This function simply returns the valid pairwise distance metrics. It exists to allow for a description of the mapping for each of the valid strings.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa4705a70e55596dcf2ace89a6d2a8d09a9fcccf" translate="yes" xml:space="preserve">
          <source>This function simply returns the valid pairwise distance metrics. It exists, however, to allow for a verbose description of the mapping for each of the valid strings.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d1a7a45215b31f0644d6e686c87b329e59419299" translate="yes" xml:space="preserve">
          <source>This function won&amp;rsquo;t compute the intercept.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a808622a520f852134a2d8734b9e29ce0a669efe" translate="yes" xml:space="preserve">
          <source>This function works with dense 2D arrays only.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7541ac358f5bb3bc5dcdfc1193ff72d6ac233a66" translate="yes" xml:space="preserve">
          <source>This generator method yields the ensemble predicted class probabilities after each iteration of boosting and therefore allows monitoring, such as to determine the predicted class probabilities on a test set after each boost.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c1ad29f5d19940cf714626cd821b0934b5bc400" translate="yes" xml:space="preserve">
          <source>This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="acc74c06c673308a3e484c230b5ff2c8348cfe79" translate="yes" xml:space="preserve">
          <source>This generator method yields the ensemble score after each iteration of boosting and therefore allows monitoring, such as to determine the score on a test set after each boost.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cb7462acd1f1e763247c87d170997bea5c436272" translate="yes" xml:space="preserve">
          <source>This illustrates the &lt;code&gt;datasets.make_multilabel_classification&lt;/code&gt; dataset generator. Each sample consists of counts of two features (up to 50 in total), which are differently distributed in each of two classes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b80113eed9b4b9668cc4e8b638bede5d1f2cf638" translate="yes" xml:space="preserve">
          <source>This implementation bulk-computes all neighborhood queries, which increases the memory complexity to O(n.d) where d is the average number of neighbors, while original DBSCAN had memory complexity O(n). It may attract a higher memory complexity when querying these nearest neighborhoods, depending on the &lt;code&gt;algorithm&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92eb61902d4dfd6571a464d87feff72fe7b32901" translate="yes" xml:space="preserve">
          <source>This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technical Report - CS Technion, April 2008. &lt;a href=&quot;http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&quot;&gt;http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6fb112845601277f8931b295b857e73c1428c8fb" translate="yes" xml:space="preserve">
          <source>This implementation is by default not memory efficient because it constructs a full pairwise similarity matrix in the case where kd-trees or ball-trees cannot be used (e.g. with sparse matrices). This matrix will consume n^2 floats. A couple of mechanisms for getting around this are:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc51c30dcd01f51cada4be15777f17eb95eb7cbd" translate="yes" xml:space="preserve">
          <source>This implementation is not intended for large-scale applications. In particular, scikit-learn offers no GPU support. For much faster, GPU-based implementations, as well as frameworks offering much more flexibility to build deep learning architectures, see &lt;a href=&quot;http://scikit-learn.org/stable/related_projects.html#related-projects&quot;&gt;Related Projects&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e30a130e6449e6025aca5fcf59ecb737e97cb91" translate="yes" xml:space="preserve">
          <source>This implementation is written in Cython and is reasonably fast. However, a faster API-compatible loader is also available at:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b488dd9d3cb1238d47d93805595214963db6dd0c" translate="yes" xml:space="preserve">
          <source>This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42bc7bbc3f5bd0df8efbfbad62976f6ec6db583b" translate="yes" xml:space="preserve">
          <source>This implementation provides the same results that 3 PLS packages provided in the R language (R-project):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="09c013dbb84b7e3d406ea0732bc3a8236ed37cbd" translate="yes" xml:space="preserve">
          <source>This implementation provides the same results that the &amp;ldquo;plspm&amp;rdquo; package provided in the R language (R-project), using the function plsca(X, Y). Results are equal or collinear with the function &lt;code&gt;pls(..., mode = &quot;canonical&quot;)&lt;/code&gt; of the &amp;ldquo;mixOmics&amp;rdquo; package. The difference relies in the fact that mixOmics implementation does not exactly implement the Wold algorithm since it does not normalize y_weights to one.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36e4a374c505873717456a086d5c9ed44e5157f6" translate="yes" xml:space="preserve">
          <source>This implementation will refuse to center scipy.sparse matrices since it would make them non-sparse and would potentially crash the program with memory exhaustion problems.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6684c1532df5f751b6b61c242ea952621dc3f4e8" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense and sparse numpy arrays of floating point values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0df3cb22108e4cd0ed0fcd534ed03e427412c64" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense numpy arrays of floating point values for the features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4abb3ee00da8e0ef45c7b10f884f8d272712ca81" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense numpy arrays or sparse scipy arrays of floating point values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3c22c958df17cff584f8c572beeb762a9a0290e" translate="yes" xml:space="preserve">
          <source>This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c43a7d8bb7931a79100804db2f074a29d45e4b6b" translate="yes" xml:space="preserve">
          <source>This improvement is not visible in the Silhouette Coefficient which is small for both as this measure seem to suffer from the phenomenon called &amp;ldquo;Concentration of Measure&amp;rdquo; or &amp;ldquo;Curse of Dimensionality&amp;rdquo; for high dimensional datasets such as text data. Other measures such as V-measure and Adjusted Rand Index are information theoretic based evaluation scores: as they are only based on cluster assignments rather than distances, hence not affected by the curse of dimensionality.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0d4ffe805942e66e32866a4eb458e728074d78e" translate="yes" xml:space="preserve">
          <source>This initially creates clusters of points normally distributed (std=1) about vertices of an &lt;code&gt;n_informative&lt;/code&gt;-dimensional hypercube with sides of length &lt;code&gt;2*class_sep&lt;/code&gt; and assigns an equal number of clusters to each class. It introduces interdependence between these features and adds various types of further noise to the data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b14c6be212126cf2e3bdc1fe1c6fe8f3c7dc46d" translate="yes" xml:space="preserve">
          <source>This interface is &lt;strong&gt;experimental&lt;/strong&gt; as at version 0.20 and subsequent releases may change attributes without notice (although there should only be minor changes to &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f6be37b4617684744b3ccc169d2c583b6e3ddc1" translate="yes" xml:space="preserve">
          <source>This is a convenience alias to &lt;code&gt;resample(*arrays, replace=False)&lt;/code&gt; to do random permutations of the collections.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4632bc2ee98a17257db1d248b06f38b79a53d4ef" translate="yes" xml:space="preserve">
          <source>This is a convenience function; the transformation is done using the default settings for &lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt;. For more advanced usage (stopword filtering, n-gram extraction, etc.), combine fetch_20newsgroups with a custom &lt;a href=&quot;sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.HashingVectorizer&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.TfidfTransformer&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.TfidfVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b3dbf5e1c5e08d66c77786f2bcbc632afc0312a" translate="yes" xml:space="preserve">
          <source>This is a convenience routine for the sake of testing. For many metrics, the utilities in scipy.spatial.distance.cdist and scipy.spatial.distance.pdist will be faster.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a67e0a846a2ab3c88cb06fc2b7950cd30914abe" translate="yes" xml:space="preserve">
          <source>This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. &lt;a href=&quot;https://goo.gl/U2Uwz2&quot;&gt;https://goo.gl/U2Uwz2&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22bae61d9be3213577df5087cb01b12cfdf8dff4" translate="yes" xml:space="preserve">
          <source>This is a copy of UCI ML Wine recognition datasets. &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c52f45448ee0e84b694b7c38bdbed7fd0e586461" translate="yes" xml:space="preserve">
          <source>This is a copy of UCI ML housing dataset. &lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&quot;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/housing/&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3dee2146876159a5a0b048cd24a612fae4a810ca" translate="yes" xml:space="preserve">
          <source>This is a copy of the test set of the UCI ML hand-written digits datasets &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&quot;&gt;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd4a60c08b29c6d6af237f8cfa14740252c7d04a" translate="yes" xml:space="preserve">
          <source>This is a general function, given points on a curve. For computing the area under the ROC-curve, see &lt;a href=&quot;sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt;. For an alternative way to summarize a precision-recall curve, see &lt;a href=&quot;sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="37510c6c60985c0ea76b5bcc4364db965e5a12fd" translate="yes" xml:space="preserve">
          <source>This is a shorthand for the ColumnTransformer constructor; it does not require, and does not permit, naming the transformers. Instead, they will be given names automatically based on their types. It also does not allow weighting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="022d95ed0540e35ea0bdce867e9a502603bc5f51" translate="yes" xml:space="preserve">
          <source>This is a shorthand for the FeatureUnion constructor; it does not require, and does not permit, naming the transformers. Instead, they will be given names automatically based on their types. It also does not allow weighting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52a890ca0cc5d284d366294db21e8c380349733e" translate="yes" xml:space="preserve">
          <source>This is a shorthand for the Pipeline constructor; it does not require, and does not permit, naming the estimators. Instead, their names will be set to the lowercase of their types automatically.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bcbf4cb6d3eb7ea12d02241a9a60f7a4e6044f4d" translate="yes" xml:space="preserve">
          <source>This is a wrapper for &lt;code&gt;estimator_.predict(X)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17ebe8027dbbfba976bb150f0e9172e06d0c02ec" translate="yes" xml:space="preserve">
          <source>This is a wrapper for &lt;code&gt;estimator_.score(X, y)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="602675ab661ad893c615b29b13c1d56146fcfa0b" translate="yes" xml:space="preserve">
          <source>This is an alternative to passing a &lt;code&gt;backend='backend_name'&lt;/code&gt; argument to the &lt;code&gt;Parallel&lt;/code&gt; class constructor. It is particularly useful when calling into library code that uses joblib internally but does not expose the backend argument in its own API.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47f9c3947e84bb8a914aa6f2f19cb2c5e42e970f" translate="yes" xml:space="preserve">
          <source>This is an example of &lt;strong&gt;bias/variance tradeoff&lt;/strong&gt;: the larger the ridge &lt;code&gt;alpha&lt;/code&gt; parameter, the higher the bias and the lower the variance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d75c7c933c17fefbabe7c2e292b885d0ecac3a21" translate="yes" xml:space="preserve">
          <source>This is an example of applying &lt;a href=&quot;../../modules/generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;sklearn.decomposition.NMF&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.decomposition.latentdirichletallocation#sklearn.decomposition.LatentDirichletAllocation&quot;&gt;&lt;code&gt;sklearn.decomposition.LatentDirichletAllocation&lt;/code&gt;&lt;/a&gt; on a corpus of documents and extract additive models of the topic structure of the corpus. The output is a list of topics, each represented as a list of terms (weights are not shown).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="53176f2993974522405fa17c9a80a84e38a4969c" translate="yes" xml:space="preserve">
          <source>This is an example showing how scikit-learn can be used for classification using an out-of-core approach: learning from data that doesn&amp;rsquo;t fit into main memory. We make use of an online classifier, i.e., one that supports the partial_fit method, that will be fed with batches of examples. To guarantee that the features space remains the same over time we leverage a HashingVectorizer that will project each example into the same feature space. This is especially useful in the case of text classification where new features (words) may appear in each batch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1620bf9fc1f7795235eabc8e2a67657eca16390d" translate="yes" xml:space="preserve">
          <source>This is an example showing how scikit-learn can be used to classify documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features and demonstrates various classifiers that can efficiently handle sparse matrices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="687fdb042e4ef171de769c4722977550577ec678" translate="yes" xml:space="preserve">
          <source>This is an example showing how the scikit-learn can be used to cluster documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features instead of standard numpy arrays.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c505c2f7b70a5aa0d5582bdc56a7d9627b32a4d8" translate="yes" xml:space="preserve">
          <source>This is an example showing the prediction latency of various scikit-learn estimators.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e4f7a05490ee1267f03d9980bace7147baa0b76" translate="yes" xml:space="preserve">
          <source>This is an extension of the algorithm in scipy.stats.mode.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="375819c22c211b4c7fc97205acd724c3a575f620" translate="yes" xml:space="preserve">
          <source>This is an implementation that uses the result of the previous model to speed up computations along the set of solutions, making it faster than sequentially calling LogisticRegression for the different parameters. Note that there will be no speedup with liblinear solver, since it does not handle warm-starting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89098058da4c55a1db96b87aadaa162a0a15baba" translate="yes" xml:space="preserve">
          <source>This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a &lt;code&gt;score&lt;/code&gt; function, or &lt;code&gt;scoring&lt;/code&gt; must be passed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a9f1a5b0fa7d00ad69170d1ab81bf1031dee11a2" translate="yes" xml:space="preserve">
          <source>This is called a &lt;a href=&quot;../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; cross-validation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2e974743bc0fdffbf7238debaf0ee76bb5a5d9b2" translate="yes" xml:space="preserve">
          <source>This is called cosine similarity, because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b01365512b47448f649e450ddd111360a73cfc3" translate="yes" xml:space="preserve">
          <source>This is called the &lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;&gt;curse of dimensionality&lt;/a&gt; and is a core problem that machine learning addresses.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25e5e11d6a0e13a60841f1cb72db59989d03472f" translate="yes" xml:space="preserve">
          <source>This is currently implemented in the following classes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a774a1be5070f83615f896d6d2ec16ebfbe92e4e" translate="yes" xml:space="preserve">
          <source>This is done in 2 steps:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c564a0d4cfad247ff47792f5a12558130a84f0c" translate="yes" xml:space="preserve">
          <source>This is equivalent to fit followed by transform, but more efficiently implemented.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="831022bba18e9ed70a7a762cd8243e7523afeddb" translate="yes" xml:space="preserve">
          <source>This is especially useful when the whole dataset is too big to fit in memory at once.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="75c0bef753e28aecf63e47221c52fe362a027981" translate="yes" xml:space="preserve">
          <source>This is implemented as &lt;code&gt;argmax(decision_function(X), axis=1)&lt;/code&gt; which will return the label of the class with most votes by estimators predicting the outcome of a decision for each possible class pair.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b2a6723fed7b2d139e18e341020f963dc7f8450" translate="yes" xml:space="preserve">
          <source>This is implemented by linking the points X into the graph of geodesic distances of the training data. First the &lt;code&gt;n_neighbors&lt;/code&gt; nearest neighbors of X are found in the training data, and from these the shortest geodesic distances from each point in X to each point in the training data are computed in order to construct the kernel. The embedding of X is the projection of this kernel onto the embedding vectors of the training set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51ae8a82b3eff6fb295f70aa28171d41c73ac3db" translate="yes" xml:space="preserve">
          <source>This is implemented in &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.transform&lt;/code&gt;&lt;/a&gt;. The desired dimensionality can be set using the &lt;code&gt;n_components&lt;/code&gt; constructor parameter. This parameter has no influence on &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.fit&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.predict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de8220f3c95931fc4241bdd5334e66fca04da2f0" translate="yes" xml:space="preserve">
          <source>This is known as &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d1ef16c8ffe6df8c7a1a81b132f67cdda1b92ea" translate="yes" xml:space="preserve">
          <source>This is more efficient than calling fit followed by transform.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1dfb3afc660617ced3002fefa24847b5bb1a14dd" translate="yes" xml:space="preserve">
          <source>This is mostly equivalent to calling:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bbd51f304b678a157464ff7ab03c52123d04218d" translate="yes" xml:space="preserve">
          <source>This is not a symmetric function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b5c42967b0e34c52656dd80a4e659c6f0fa2181" translate="yes" xml:space="preserve">
          <source>This is not exactly the same as &lt;code&gt;sklearn.metrics.additive_chi2_kernel&lt;/code&gt;. The authors of &lt;a href=&quot;#vz2010&quot; id=&quot;id4&quot;&gt;[VZ2010]&lt;/a&gt; prefer the version above as it is always positive definite. Since the kernel is additive, it is possible to treat all components \(x_i\) separately for embedding. This makes it possible to sample the Fourier transform in regular intervals, instead of approximating using Monte Carlo sampling.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f7f802fcac19c1c8ed1c55f673312d761a2c94a7" translate="yes" xml:space="preserve">
          <source>This is not the case for &lt;a href=&quot;generated/sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt;&lt;code&gt;completeness_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt;&lt;code&gt;homogeneity_score&lt;/code&gt;&lt;/a&gt;: both are bound by the relationship:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d73f5b087f2ecb2dadb8778d3d18cfc19507034" translate="yes" xml:space="preserve">
          <source>This is not true for &lt;code&gt;mutual_info_score&lt;/code&gt;, which is therefore harder to judge:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="982101a3d677907e48e034c807cde26531a0468b" translate="yes" xml:space="preserve">
          <source>This is only available if no vocabulary was given.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ca1a333516e3b52907835d092958662636ea528" translate="yes" xml:space="preserve">
          <source>This is particularly important for doing grid searches:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2413be66af5e312ff97e484aff33c56868971fbe" translate="yes" xml:space="preserve">
          <source>This is perhaps the best known database to be found in the pattern recognition literature. Fisher&amp;rsquo;s paper is a classic in the field and is referenced frequently to this day. (See Duda &amp;amp; Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32ef6d8e689e0cbccf726fb7a94339c2864c487f" translate="yes" xml:space="preserve">
          <source>This is present only if &lt;code&gt;refit&lt;/code&gt; is not False.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="717414c2af196799a3d1dc1aec1269a995318377" translate="yes" xml:space="preserve">
          <source>This is similar to the error set size, but weighted by the number of relevant and irrelevant labels. The best performance is achieved with a ranking loss of zero.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="00034266cafd87d919959c8134650d981f2c4466" translate="yes" xml:space="preserve">
          <source>This is the class and function reference of scikit-learn. Please refer to the &lt;a href=&quot;http://scikit-learn.org/stable/user_guide.html#user-guide&quot;&gt;full user guide&lt;/a&gt; for further details, as the class and function raw specifications may not be enough to give full guidelines on their uses. For reference on concepts repeated across the API, see &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;Glossary of Common Terms and API Elements&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c0b38fb17b983574b86706f2172c4c4bac1c6a5" translate="yes" xml:space="preserve">
          <source>This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of the true labels given a probabilistic classifier&amp;rsquo;s predictions. The log loss is only defined for two or more labels. For a single sample with true label yt in {0,1} and estimated probability yp that yt = 1, the log loss is</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a9111de5c7f4d9db0e2dc4faf926c5c47ae0eb12" translate="yes" xml:space="preserve">
          <source>This is the result of calling &lt;code&gt;method&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="611492c50f944d397ce592f12eabee4801e28de1" translate="yes" xml:space="preserve">
          <source>This is the structured version, that takes into account some topological structure between samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb0a6c68cdbb70ef7265210b8b8760243faa6912" translate="yes" xml:space="preserve">
          <source>This is useful for fitting an intercept term with implementations which cannot otherwise fit it directly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c420e62ce6697ac415dbca5798c0153080b025c" translate="yes" xml:space="preserve">
          <source>This is useful if the stored attributes of a previously used model has to be reused. If set to False, then the coefficients will be rewritten for every call to fit. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42f2c0052d88e51f5df6c26752c192f81040ec01" translate="yes" xml:space="preserve">
          <source>This kernel is a popular choice for computing the similarity of documents represented as tf-idf vectors. &lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt;&lt;code&gt;cosine_similarity&lt;/code&gt;&lt;/a&gt; accepts &lt;code&gt;scipy.sparse&lt;/code&gt; matrices. (Note that the tf-idf functionality in &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; can produce normalized vectors, in which case &lt;a href=&quot;generated/sklearn.metrics.pairwise.cosine_similarity#sklearn.metrics.pairwise.cosine_similarity&quot;&gt;&lt;code&gt;cosine_similarity&lt;/code&gt;&lt;/a&gt; is equivalent to &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt;&lt;code&gt;linear_kernel&lt;/code&gt;&lt;/a&gt;, only slower.)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1fbcea40cf4aba903be6eddf36c859a1718e3b8" translate="yes" xml:space="preserve">
          <source>This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6ad60f9f3ef06c9a3898414b0afc593eaff20c1d" translate="yes" xml:space="preserve">
          <source>This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth. The prior and posterior of a GP resulting from an RBF kernel are shown in the following figure:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6cbded70a18b870dfff7fda8d59e204ebd77900f" translate="yes" xml:space="preserve">
          <source>This kind of singular profiles is often seen in practice, for instance:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1df4414d1e47e9ea41e98b8c6e13b5eeb49e06ac" translate="yes" xml:space="preserve">
          <source>This left out portion can be used to estimate the generalization error without having to rely on a separate validation set. This estimate comes &amp;ldquo;for free&amp;rdquo; as no additional data is needed and can be used for model selection.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f52c7809ab985057ba93af9b88459b0d10b33a2" translate="yes" xml:space="preserve">
          <source>This makes sure that the loss function is not heavily influenced by the outliers while not completely ignoring their effect.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4826ff4b754b03a246d73c38aa2c971ffdf335e1" translate="yes" xml:space="preserve">
          <source>This means each weight \(w_{i}\) is drawn from a Gaussian distribution, centered on zero and with a precision \(\lambda_{i}\):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de2308f740624c092eea038ac13deb5af2b1fa80" translate="yes" xml:space="preserve">
          <source>This means that any classifiers handling multi-output multiclass or multi-task classification tasks, support the multi-label classification task as a special case. Multi-task classification is similar to the multi-output classification task with different model formulations. For more information, see the relevant estimator documentation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6157bc0b8c2f67c8a593bf2d12852c000be43e72" translate="yes" xml:space="preserve">
          <source>This measure is not adjusted for chance. Therefore &lt;a href=&quot;sklearn.metrics.adjusted_mutual_info_score#sklearn.metrics.adjusted_mutual_info_score&quot;&gt;&lt;code&gt;adjusted_mutual_info_score&lt;/code&gt;&lt;/a&gt; might be preferred.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cca48ea1404a91d2df7b18cac656df30067cb12b" translate="yes" xml:space="preserve">
          <source>This method allows monitoring (i.e. determine error on testing set) after each boosting iteration.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="778da8cdceb825f45e49260c13130972c415ba18" translate="yes" xml:space="preserve">
          <source>This method allows monitoring (i.e. determine error on testing set) after each stage.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="893e20b8eaafff147aad0ee519c22b2be0783798" translate="yes" xml:space="preserve">
          <source>This method allows to generalize prediction to &lt;em&gt;new observations&lt;/em&gt; (not in the training set). Only available for novelty detection (when novelty is set to True).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e13bbab31202d773dbd5b155d7e0b7799ca54f84" translate="yes" xml:space="preserve">
          <source>This method computes the least squares solution using a singular value decomposition of X. If X is a matrix of size (n, p) this method has a cost of \(O(n p^2)\), assuming that \(n \geq p\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc0919b0c811a79ed295c00492df459fa5ab93e8" translate="yes" xml:space="preserve">
          <source>This method doesn&amp;rsquo;t do anything. It exists purely for compatibility with the scikit-learn transformer API.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89035c0aee87e0125ffcf7bf5f0dbe56fcfb74a9" translate="yes" xml:space="preserve">
          <source>This method has some performance and numerical stability overhead, hence it is better to call partial_fit on chunks of data that are as large as possible (as long as fitting in the memory budget) to hide the overhead.</source>
          <target state="new"/>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
