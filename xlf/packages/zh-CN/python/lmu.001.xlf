<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="https://pypi.org/project/lmu/">
    <body>
      <group id="lmu">
        <trans-unit id="f7986d315f2a9d4b9005d324dcb9fea7029e3373" translate="yes" xml:space="preserve">
          <source>(A, B)</source>
          <target state="translated">（A，B）</target>
        </trans-unit>
        <trans-unit id="e8b310147d49ec4db715247f737630a341a942ea" translate="yes" xml:space="preserve">
          <source>) and the memory vector (</source>
          <target state="translated">）和记忆向量（</target>
        </trans-unit>
        <trans-unit id="9d78d7759ad02cf7cb425cfa08e4a771f7ad35f4" translate="yes" xml:space="preserve">
          <source>) is trained via backpropagation, while the dynamics of the memory remain fixed (&lt;a href=&quot;https://papers.nips.cc/paper/9689-legendre-memory-units-continuous-time-representation-in-recurrent-neural-networks.pdf&quot;&gt;see paper for details&lt;/a&gt;).</source>
          <target state="translated">）是通过反向传播训练的，而存储器的动态保持不变（&lt;a href=&quot;https://papers.nips.cc/paper/9689-legendre-memory-units-continuous-time-representation-in-recurrent-neural-networks.pdf&quot;&gt;有关详细信息，请参见纸张&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="91f01eed1463fd0e19c70b920cb603e7fd73b2fa" translate="yes" xml:space="preserve">
          <source>) with a nonlinear hidden state (</source>
          <target state="translated">）具有非线性隐藏状态（</target>
        </trans-unit>
        <trans-unit id="f34ef1a8f3f40a7a1236002bb13939b9aaab7ef2" translate="yes" xml:space="preserve">
          <source>, if necessary. By default the coupling between the hidden state (</source>
          <target state="translated">，如有必要。默认情况下，隐藏状态之间的耦合（</target>
        </trans-unit>
        <trans-unit id="1a8a70853cdb5616fca8eb1afe92a80fdc273967" translate="yes" xml:space="preserve">
          <source>, which obtains the current best-known psMNIST result (using an RNN) of &lt;strong&gt;97.15%&lt;/strong&gt;. Note, the network is using fewer internal state-variables and neurons than there are pixels in the input sequence. To reproduce the results from the paper, run the notebooks in the</source>
          <target state="translated">，它获得了&lt;strong&gt;97.15％&lt;/strong&gt;的当前最知名的psMNIST结果（使用RNN）。请注意，网络使用的内部状态变量和神经元少于输入序列中的像素。要从纸上复制结果，请在</target>
        </trans-unit>
        <trans-unit id="eb0d2eadad1d59670435428d52f11012b3c56467" translate="yes" xml:space="preserve">
          <source>0.1.0 (June 22, 2020)</source>
          <target state="translated">0.1.0 (2020年6月22日)</target>
        </trans-unit>
        <trans-unit id="f407f992fca896699f798cda98a678821453fbc9" translate="yes" xml:space="preserve">
          <source>A single</source>
          <target state="translated">单一</target>
        </trans-unit>
        <trans-unit id="6eb4915552135823111f476864da3f654c233c54" translate="yes" xml:space="preserve">
          <source>Citation</source>
          <target state="translated">引用</target>
        </trans-unit>
        <trans-unit id="51622bd517ce184905e899085f3bf2c222cf7501" translate="yes" xml:space="preserve">
          <source>GitHub repository includes a pre-trained Keras/TensorFlow model, located at</source>
          <target state="translated">GitHub仓库中包含了一个预训练的Keras/TensorFlow模型,位于</target>
        </trans-unit>
        <trans-unit id="ad6d35f4b9d3d4fd1498c1969d140852aa27ea84" translate="yes" xml:space="preserve">
          <source>Initial release of LMU 0.1.0! Supports Python 3.5+.</source>
          <target state="translated">LMU 0.1.0的初始版本! 支持Python 3.5以上。</target>
        </trans-unit>
        <trans-unit id="5db3dea372a23fb11a890296dd05b06a6f2c43b5" translate="yes" xml:space="preserve">
          <source>LMUCell</source>
          <target state="translated">LMUCell</target>
        </trans-unit>
        <trans-unit id="68142c1fecd8cd5a852b6821a313d6f18e67bd63" translate="yes" xml:space="preserve">
          <source>LMUs in NengoDL (reproducing SotA on psMNIST)</source>
          <target state="translated">NengoDL中的LMU(在psMNIST上复制SotA)</target>
        </trans-unit>
        <trans-unit id="94147d1825c4f1bb9b2a97d3e0d79fe72940ffcb" translate="yes" xml:space="preserve">
          <source>Legendre Memory Units</source>
          <target state="translated">Legendre内存单元</target>
        </trans-unit>
        <trans-unit id="12e48a2c80ec09fba890acbab555ee09d4efed0c" translate="yes" xml:space="preserve">
          <source>Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks</source>
          <target state="translated">Legendre记忆单元。循环神经网络中的连续时间表示法</target>
        </trans-unit>
        <trans-unit id="934f951fd58afc4e51af5bd95b0b06d48ea9da9c" translate="yes" xml:space="preserve">
          <source>Nengo Examples</source>
          <target state="translated">Nengo范例</target>
        </trans-unit>
        <trans-unit id="22d507f2ba74e43593de3ae3f550bf202c076adc" translate="yes" xml:space="preserve">
          <source>Paper</source>
          <target state="translated">文件</target>
        </trans-unit>
        <trans-unit id="5044ba0a5cf607ebea4e58623c6417cb87f682bb" translate="yes" xml:space="preserve">
          <source>Release history</source>
          <target state="translated">发行历史</target>
        </trans-unit>
        <trans-unit id="019292c2324bce7b4b7238901874de053dae61e5" translate="yes" xml:space="preserve">
          <source>Spiking LMUs in Nengo (with online learning)</source>
          <target state="translated">在Nengo中增加LMU(在线学习)。</target>
        </trans-unit>
        <trans-unit id="17fe76b2cbb233c1e454e6043ea8b785cf235f64" translate="yes" xml:space="preserve">
          <source>Spiking LMUs in Nengo Loihi (with online learning)</source>
          <target state="translated">在Nengo Loihi增加LMU(在线学习)</target>
        </trans-unit>
        <trans-unit id="86d3603c97865cc4b8bed02e3a4ce17db2ddf9b9" translate="yes" xml:space="preserve">
          <source>Thanks to all of the contributors for making this possible!</source>
          <target state="translated">感谢所有的贡献者,让这一切成为可能!</target>
        </trans-unit>
        <trans-unit id="93ef0dd827103681fcee453b78be2ff14e1a261d" translate="yes" xml:space="preserve">
          <source>The</source>
          <target state="translated">De</target>
        </trans-unit>
        <trans-unit id="7812ef13cd5d03416a8df141fa5c9d4816525504" translate="yes" xml:space="preserve">
          <source>The API is considered unstable; parts are likely to change in the future.</source>
          <target state="translated">该API被认为是不稳定的;部分内容可能会在未来发生变化。</target>
        </trans-unit>
        <trans-unit id="e844712e089b0a5bc92ef35f09387b6d30e8162a" translate="yes" xml:space="preserve">
          <source>The discretized</source>
          <target state="translated">谨慎的</target>
        </trans-unit>
        <trans-unit id="d7507de5fb53855c565db091e29dff83decf8e06" translate="yes" xml:space="preserve">
          <source>We propose a novel memory cell for recurrent neural networks that dynamically maintains information across long windows of time using relatively few resources. The Legendre Memory Unit (LMU) is mathematically derived to orthogonalize its continuous-time history &amp;ndash; doing so by solving d coupled ordinary differential equations (ODEs), whose phase space linearly maps onto sliding windows of time via the Legendre polynomials up to degree d &amp;minus; 1 (example d=12, shown below).</source>
          <target state="translated">我们提出了一种用于递归神经网络的新型存储单元，该存储单元可以使用相对较少的资源在较长的时间范围内动态维护信息。勒让德存储单元（LMU）是通过数学方法得出的，以使其连续时间历史正交化&amp;ndash;通过求解d耦合常微分方程（ODE），其相空间通过勒让德多项式线性映射到时间滑动窗口上，直至度d- 1（示例d = 12，如下所示）。</target>
        </trans-unit>
        <trans-unit id="af5514d5e2de36d647091221bbc2576aad7eeb67" translate="yes" xml:space="preserve">
          <source>branch in the</source>
          <target state="translated">境内分支</target>
        </trans-unit>
        <trans-unit id="66f1fb81d4d11d662fbb9d02462591f64b87de95" translate="yes" xml:space="preserve">
          <source>branch.</source>
          <target state="translated">分部:</target>
        </trans-unit>
        <trans-unit id="d3134f6bf31c14bcd86b2f42eacf8ca26224ba95" translate="yes" xml:space="preserve">
          <source>directory within the</source>
          <target state="translated">目录内的</target>
        </trans-unit>
        <trans-unit id="71ab8b6afb1bae3df247e0286da35e0da16564ff" translate="yes" xml:space="preserve">
          <source>docs</source>
          <target state="translated">文件</target>
        </trans-unit>
        <trans-unit id="c828076471935e6b0e12c70c56368cf19fbf3762" translate="yes" xml:space="preserve">
          <source>experiments</source>
          <target state="translated">实验</target>
        </trans-unit>
        <trans-unit id="c4b89cb9b895edeb1054b2f4f97660e540b70508" translate="yes" xml:space="preserve">
          <source>expresses the following computational graph in Keras as an RNN layer, which couples the optimal linear memory (</source>
          <target state="translated">将下面的计算图在Keras中表达为RNN层,它将最优的线性内存(</target>
        </trans-unit>
        <trans-unit id="27d5482eebd075de44389774fce28c69f45c8a75" translate="yes" xml:space="preserve">
          <source>h</source>
          <target state="translated">h</target>
        </trans-unit>
        <trans-unit id="fbaee08b1fd3c3083cbbfc97fe14ba0acb73a3ab" translate="yes" xml:space="preserve">
          <source>includes an example for how to use the</source>
          <target state="translated">包括一个如何使用</target>
        </trans-unit>
        <trans-unit id="dd84f8d3e9fee1217eaa3b5560a2dcf1324e9c29" translate="yes" xml:space="preserve">
          <source>lmu</source>
          <target state="translated">lmu</target>
        </trans-unit>
        <trans-unit id="6b0d31c0d563223024da45691584643ac78c96e8" translate="yes" xml:space="preserve">
          <source>m</source>
          <target state="translated">m</target>
        </trans-unit>
        <trans-unit id="8641d131425d9c0df501277f23d70a55c1e2c120" translate="yes" xml:space="preserve">
          <source>matrices are initialized according to the LMU&amp;rsquo;s mathematical derivation with respect to some chosen window length, &amp;theta;. Backpropagation can be used to learn this time-scale, or fine-tune</source>
          <target state="translated">矩阵是根据LMU对某些选定的窗口长度&amp;theta;的数学推导进行初始化的。反向传播可用于学习此时间尺度，或进行微调</target>
        </trans-unit>
        <trans-unit id="18bcc7cebf6c0b3db118c68544699b960be664ae" translate="yes" xml:space="preserve">
          <source>models/psMNIST-standard.hdf5</source>
          <target state="translated">models/psMNIST-standard.hdf5</target>
        </trans-unit>
        <trans-unit id="950593b1f42de841169aa7d59486b7e980bc15cf" translate="yes" xml:space="preserve">
          <source>paper</source>
          <target state="translated">纸张</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
