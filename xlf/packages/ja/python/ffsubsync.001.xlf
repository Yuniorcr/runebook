<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ja" datatype="htmlbody" original="https://pypi.org/project/ffsubsync/">
    <body>
      <group id="ffsubsync">
        <trans-unit id="448fa61d60809d27051e5604a1053eafdfc3dbbb" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;http://www.numpy.org/&quot;&gt;numpy&lt;/a&gt; and, indirectly, &lt;a href=&quot;https://www.netlib.org/fftpack/&quot;&gt;FFTPACK&lt;/a&gt;, which powers the FFT-based algorithm for fast scoring of alignments between subtitles (or subtitles and video)</source>
          <target state="translated">&lt;a href=&quot;http://www.numpy.org/&quot;&gt;numpy&lt;/a&gt;と、間接的に&lt;a href=&quot;https://www.netlib.org/fftpack/&quot;&gt;FFTPACK&lt;/a&gt;。これは、字幕（または字幕とビデオ）間の配置の高速スコアリングのためのFFTベースのアルゴリズムを強化します。</target>
        </trans-unit>
        <trans-unit id="809173aad90d1b10ce189edf1551d1b0699e2f6a" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;https://pypi.org/project/srt/&quot;&gt;srt&lt;/a&gt; for operating on &lt;a href=&quot;https://en.wikipedia.org/wiki/SubRip#SubRip_text_file_format&quot;&gt;SRT files&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://pypi.org/project/srt/&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/SubRip#SubRip_text_file_format&quot;&gt;SRTファイル&lt;/a&gt;を操作するためのsrt</target>
        </trans-unit>
        <trans-unit id="b1d4e9959623befe0fa4c1f81c6a08d9ec02186a" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;https://www.ffmpeg.org/&quot;&gt;ffmpeg&lt;/a&gt; and the &lt;a href=&quot;https://github.com/kkroening/ffmpeg-python&quot;&gt;ffmpeg-python&lt;/a&gt; wrapper, for extracting raw audio from video</source>
          <target state="translated">&lt;a href=&quot;https://www.ffmpeg.org/&quot;&gt;&lt;/a&gt;ビデオから生のオーディオを抽出するためのffmpegと&lt;a href=&quot;https://github.com/kkroening/ffmpeg-python&quot;&gt;ffmpeg-python&lt;/a&gt;ラッパー</target>
        </trans-unit>
        <trans-unit id="f402db43144acbb0439ef8edcbe14c9699525da0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;ffs&lt;/code&gt;, &lt;code&gt;subsync&lt;/code&gt; and &lt;code&gt;ffsubsync&lt;/code&gt; all work as entrypoints:</source>
          <target state="translated">&lt;code&gt;ffs&lt;/code&gt; 、 &lt;code&gt;subsync&lt;/code&gt; と &lt;code&gt;ffsubsync&lt;/code&gt; エントリポイントなど、すべての作業：</target>
        </trans-unit>
        <trans-unit id="14b64238bfe66703a43272be5c9729b5ca77e61f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;ffsubsync&lt;/code&gt; uses the file extension to decide whether to perform voice activity
detection on the audio or to directly extract speech from an srt file.</source>
          <target state="translated">&lt;code&gt;ffsubsync&lt;/code&gt; はファイル拡張子を使用して、音声に対して音声アクティビティ検出を実行するか、srtファイルから音声を直接抽出するかを決定します。</target>
        </trans-unit>
        <trans-unit id="00799ba9e026c577347c81635a03c034bba3e14e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;ffsubsync&lt;/code&gt; usually finishes in 20 to 30 seconds, depending on the length of the
video. The most expensive step is actually extraction of raw audio. If you
already have a correctly synchronized &quot;reference&quot; srt file (in which case audio
extraction can be skipped), &lt;code&gt;ffsubsync&lt;/code&gt; typically runs in less than a second.</source>
          <target state="translated">&lt;code&gt;ffsubsync&lt;/code&gt; は通常、ビデオの長さにもよりますが、20〜30秒で終了します。最も費用のかかるステップは、実際には生のオーディオの抽出です。正しく同期された「参照」srtファイルがすでにある場合（この場合、オーディオ抽出はスキップできます）、 &lt;code&gt;ffsubsync&lt;/code&gt; は通常1秒未満で実行されます。</target>
        </trans-unit>
        <trans-unit id="936db9a8629b6aee1da2ac35146b55e3ac7b77db" translate="yes" xml:space="preserve">
          <source>At the request of some, you can now help cover my coffee expenses using the
Github Sponsors button at the top (recurring monthly payments), or using the below
Paypal Donate button (one-time payment):</source>
          <target state="translated">リクエストに応じて、上部にあるGithubのスポンサーボタン(毎月の定期的な支払い)、または下記のPaypalの寄付ボタン(1回払い)を使用して、私のコーヒーの費用をカバーするのを助けることができます。</target>
        </trans-unit>
        <trans-unit id="56f9b02e470ae04f5605651b0ca5cd5920c7d715" translate="yes" xml:space="preserve">
          <source>Besides general stability and usability improvements, one line
of work aims to extend the synchronization algorithm to handle splits
/ breaks in the middle of video not present in subtitles (or vice versa).
Developing a robust solution will take some time (assuming one is possible).
See &lt;a href=&quot;https://github.com/smacke/ffsubsync/issues/10&quot;&gt;#10&lt;/a&gt; for more details.</source>
          <target state="translated">一般的な安定性と使いやすさの向上に加えて、1つの作業ラインは、同期アルゴリズムを拡張して、字幕に存在しないビデオの途中での分割/中断を処理することを目的としています（またはその逆）。堅牢なソリューションの開発には時間がかかります（可能な場合）。詳細については、&lt;a href=&quot;https://github.com/smacke/ffsubsync/issues/10&quot;&gt;＃10&lt;/a&gt;を参照してください。</target>
        </trans-unit>
        <trans-unit id="bd217ff4214f14c1a032a45a800183c165249895" translate="yes" xml:space="preserve">
          <source>Code in this project is &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;MIT licensed&lt;/a&gt;.</source>
          <target state="translated">このプロジェクトのコードは&lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;MITライセンス&lt;/a&gt;です。</target>
        </trans-unit>
        <trans-unit id="bfac50d6424b5166c3ee2808c85ae7c139b5182f" translate="yes" xml:space="preserve">
          <source>Credits</source>
          <target state="translated">クレジット</target>
        </trans-unit>
        <trans-unit id="1946ed4e6d7a5cdee8942980594304622d5d448e" translate="yes" xml:space="preserve">
          <source>Discretize video and subtitles by time into 10ms windows.</source>
          <target state="translated">10msのウィンドウに時間をかけてビデオと字幕を離散化します。</target>
        </trans-unit>
        <trans-unit id="61f87fb699391c08f8809648c3d512e2529450b6" translate="yes" xml:space="preserve">
          <source>FFsubsync</source>
          <target state="translated">エフエフサブシンク</target>
        </trans-unit>
        <trans-unit id="d14e9d216a8814e80e6ada0f122fa233375bf1ab" translate="yes" xml:space="preserve">
          <source>First, make sure ffmpeg is installed. On MacOS, this looks like:</source>
          <target state="translated">まず、ffmpegがインストールされていることを確認します。MacOSでは、このようになります。</target>
        </trans-unit>
        <trans-unit id="42edbb52a87cd00061f18e023951cf7cd5c5ef15" translate="yes" xml:space="preserve">
          <source>For each 10ms window, determine whether that window contains speech.  This
is trivial to do for subtitles (we just determine whether any subtitle is
&quot;on&quot; during each time window); for video, use an off-the-shelf voice
activity detector (VAD) like
the one built into &lt;a href=&quot;https://webrtc.org/&quot;&gt;webrtc&lt;/a&gt;.</source>
          <target state="translated">10ミリ秒のウィンドウごとに、そのウィンドウに音声が含まれているかどうかを確認します。これは字幕に対して行うのは簡単です（各時間枠の間に字幕が「オン」になっているかどうかを判断するだけです）。ビデオの場合は、&lt;a href=&quot;https://webrtc.org/&quot;&gt;webrtcに&lt;/a&gt;組み込まれているような既製の音声アクティビティ検出器（VAD）を使用します。</target>
        </trans-unit>
        <trans-unit id="c3cba160fe064a0d9a4fc24f20540221a2aff0ed" translate="yes" xml:space="preserve">
          <source>Future Work</source>
          <target state="translated">今後の仕事</target>
        </trans-unit>
        <trans-unit id="ad0a916130b1ffb1194d9d811a12d26d58f8df6c" translate="yes" xml:space="preserve">
          <source>Helping Development</source>
          <target state="translated">開発支援</target>
        </trans-unit>
        <trans-unit id="90ccd6497400b5576aeca1bd94af74aae1e0a250" translate="yes" xml:space="preserve">
          <source>History</source>
          <target state="translated">歴史</target>
        </trans-unit>
        <trans-unit id="8cecc82d72d68f9151cb0901cfe40eeac15bfca6" translate="yes" xml:space="preserve">
          <source>How It Works</source>
          <target state="translated">どのように機能するか</target>
        </trans-unit>
        <trans-unit id="44abaabed373ce699f3a6b3019932545e4158606" translate="yes" xml:space="preserve">
          <source>If you want to live dangerously, you can grab the latest version as follows:</source>
          <target state="translated">危険な生活をしたい方は、以下のように最新版を手に入れましょう。</target>
        </trans-unit>
        <trans-unit id="ce1d79c980df3c47404d3d29df7c08886649970a" translate="yes" xml:space="preserve">
          <source>In most cases, inconsistencies between video and subtitles occur when starting
or ending segments present in video are not present in subtitles, or vice versa.
This can occur, for example, when a TV episode recap in the subtitles was pruned
from video. FFsubsync typically works well in these cases, and in my experience
this covers &amp;gt;95% of use cases. Handling breaks and splits outside of the beginning
and ending segments is left to future work (see below).</source>
          <target state="translated">ほとんどの場合、ビデオに存在する開始セグメントまたは終了セグメントが字幕に存在しない場合、またはその逆の場合に、ビデオと字幕の間の不整合が発生します。これは、たとえば、字幕のTVエピソードの要約がビデオから削除された場合に発生する可能性があります。FFsubsyncは通常、これらのケースでうまく機能します。私の経験では、これはユースケースの95％以上をカバーしています。開始セグメントと終了セグメントの外側でのブレークとスプリットの処理は、将来の作業に任されています（以下を参照）。</target>
        </trans-unit>
        <trans-unit id="fd6c3ebf7befca9f8208f86c76e4d4180303745c" translate="yes" xml:space="preserve">
          <source>Install</source>
          <target state="translated">インストール</target>
        </trans-unit>
        <trans-unit id="659525747c675f26d0d0761f93182ab5228cbc26" translate="yes" xml:space="preserve">
          <source>Into this:</source>
          <target state="translated">これに</target>
        </trans-unit>
        <trans-unit id="0d05fd8158dba0121b4eac110b20f240f7818b92" translate="yes" xml:space="preserve">
          <source>Language-agnostic automatic synchronization of subtitles with video, so that
subtitles are aligned to the correct starting point within the video.</source>
          <target state="translated">言語にとらわれない、ビデオと字幕の自動同期化により、字幕がビデオ内の正しい開始点に合わせて調整されます。</target>
        </trans-unit>
        <trans-unit id="420a9347981effc83ba7d1df62790a49716f38bb" translate="yes" xml:space="preserve">
          <source>Language-agnostic synchronization of subtitles with video.</source>
          <target state="translated">言語にとらわれない、映像と字幕の同期。</target>
        </trans-unit>
        <trans-unit id="3229609e15436ec51bcf00818a69a84dbc58a0c2" translate="yes" xml:space="preserve">
          <source>License</source>
          <target state="translated">ライセンス</target>
        </trans-unit>
        <trans-unit id="a7c04c64ed3f2a9374590c76c50d3b7f1b18e3da" translate="yes" xml:space="preserve">
          <source>Limitations</source>
          <target state="translated">制限事項</target>
        </trans-unit>
        <trans-unit id="beac1d4d9d908d34bbabd5497b31900615c1e739" translate="yes" xml:space="preserve">
          <source>Next, grab the script. It should work with both Python 2 and Python 3:</source>
          <target state="translated">次に、スクリプトを取得します。これはPython 2とPython 3の両方で動作するはずです。</target>
        </trans-unit>
        <trans-unit id="179a4231cbcf797a1ea61865a797b3047681d7b0" translate="yes" xml:space="preserve">
          <source>Now we have two binary strings: one for the subtitles, and one for the
video.  Try to align these strings by matching 0's with 0's and 1's with
1's. We score these alignments as (# video 1's matched w/ subtitle 1's) - (#
video 1's matched with subtitle 0's).</source>
          <target state="translated">これで 2 つのバイナリ文字列ができました:1 つは字幕用、もう 1 つはビデオ用です。これらの文字列を、0と0のマッチングで0'sを、1と1'sのマッチングで揃えてみましょう。私たちは、これらの整列を (#video 1's matched w/subtitle 1's)-(#video 1's matched with subtitle 0's)のようにスコア化します。</target>
        </trans-unit>
        <trans-unit id="bc49bcc1bbf9322f0d673c127d5dd1764357aa92" translate="yes" xml:space="preserve">
          <source>Other excellent Python libraries like &lt;a href=&quot;https://docs.python.org/3/library/argparse.html&quot;&gt;argparse&lt;/a&gt; and &lt;a href=&quot;https://tqdm.github.io/&quot;&gt;tqdm&lt;/a&gt;, not related to the core functionality, but which enable much better experiences for developers and users.</source>
          <target state="translated">&lt;a href=&quot;https://docs.python.org/3/library/argparse.html&quot;&gt;argparse&lt;/a&gt;や&lt;a href=&quot;https://tqdm.github.io/&quot;&gt;tqdm&lt;/a&gt;などの他の優れたPythonライブラリは、コア機能とは関係ありませんが、開発者とユーザーのエクスペリエンスを大幅に向上させます。</target>
        </trans-unit>
        <trans-unit id="2d2cb022bc3d26bd1407c4aa787d5e46e1ad4c3b" translate="yes" xml:space="preserve">
          <source>Speed</source>
          <target state="translated">スピード</target>
        </trans-unit>
        <trans-unit id="6d75a7aec098fab5da5f5616cd98e0d6589fb421" translate="yes" xml:space="preserve">
          <source>The best-scoring alignment from step 3 determines how to offset the subtitles
in time so that they are properly synced with the video. Because the binary
strings are fairly long (millions of digits for video longer than an hour), the
naive O(n^2) strategy for scoring all alignments is unacceptable. Instead, we
use the fact that &quot;scoring all alignments&quot; is a convolution operation and can
be implemented with the Fast Fourier Transform (FFT), bringing the complexity
down to O(n log n).</source>
          <target state="translated">ステップ 3 からのベストスコアのアラインメントは、字幕をビデオと適切に同期させるために、時間内にどのようにオフセットするかを決定します。バイナリ文字列はかなり長いので(1時間を超える動画では数百万桁)、すべてのアライメントをスコアリングするためのナイーブなO(n^2)戦略は受け入れられません。その代わりに、「すべての整列をスコアリングする」というのは畳み込み演算であり、高速フーリエ変換(FFT)で実装できるという事実を利用して、複雑さをO(n log n)まで下げています。</target>
        </trans-unit>
        <trans-unit id="b6cb2e1a9df05d019e4ce2e098c42168484163b6" translate="yes" xml:space="preserve">
          <source>The implementation for this project was started during HackIllinois 2019, for
which it received an &lt;strong&gt;&lt;em&gt;Honorable Mention&lt;/em&gt;&lt;/strong&gt; (ranked in the top 5 projects,
excluding projects that won company-specific prizes).</source>
          <target state="translated">このプロジェクトの実装は、HackIllinois 2019の期間中に開始され、&lt;strong&gt;&lt;em&gt;佳作を受賞しました&lt;/em&gt;&lt;/strong&gt;（企業固有の賞を受賞したプロジェクトを除き、上位5プロジェクトにランクインしました）。</target>
        </trans-unit>
        <trans-unit id="c24de39b3bb0b16a5df396d44c3f3aebba913c44" translate="yes" xml:space="preserve">
          <source>The synchronization algorithm operates in 3 steps:</source>
          <target state="translated">同期アルゴリズムは3つのステップで動作します。</target>
        </trans-unit>
        <trans-unit id="94226cd553bfe5c121e7a05322ec2c6041760ba8" translate="yes" xml:space="preserve">
          <source>There may be occasions where you have a correctly synchronized srt file in a
language you are unfamiliar with, as well as an unsynchronized srt file in your
native language. In this case, you can use the correctly synchronized srt file
directly as a reference for synchronization, instead of using the video as the
reference:</source>
          <target state="translated">慣れない言語で正しく同期されたsrtファイルと、母国語で同期されていないsrtファイルがある場合があります。この場合、ビデオを参照として使用するのではなく、正しく同期されたsrtファイルを同期の参照として直接使用することができます。</target>
        </trans-unit>
        <trans-unit id="da9f448081113e5f2aa6a2b4381ed0a353647124" translate="yes" xml:space="preserve">
          <source>This project would not be possible without the following libraries:</source>
          <target state="translated">このプロジェクトは以下のライブラリがなければ成立しません。</target>
        </trans-unit>
        <trans-unit id="7240d4aab825d0799c8f63afc0c0ff0047392b6e" translate="yes" xml:space="preserve">
          <source>Turn this:</source>
          <target state="translated">これを回して</target>
        </trans-unit>
        <trans-unit id="0bb18642b70b9f8a9c12ccf39487328f306b8e19" translate="yes" xml:space="preserve">
          <source>Usage</source>
          <target state="translated">使用方法</target>
        </trans-unit>
        <trans-unit id="75a062ef757a8dddb6401a1ad2fd757ca432ebd9" translate="yes" xml:space="preserve">
          <source>VAD from &lt;a href=&quot;https://webrtc.org/&quot;&gt;webrtc&lt;/a&gt; and the &lt;a href=&quot;https://github.com/wiseman/py-webrtcvad&quot;&gt;py-webrtcvad&lt;/a&gt; wrapper, for speech detection</source>
          <target state="translated">音声検出用の&lt;a href=&quot;https://webrtc.org/&quot;&gt;webrtc&lt;/a&gt;および&lt;a href=&quot;https://github.com/wiseman/py-webrtcvad&quot;&gt;py-webrtcvad&lt;/a&gt;ラッパーからのVAD</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
