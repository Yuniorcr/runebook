<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ja" datatype="htmlbody" original="https://pypi.org/project/microtokenizer/">
    <body>
      <group id="microtokenizer">
        <trans-unit id="3df16f6c933eb0728594c75fb51b95adb61e18a7" translate="yes" xml:space="preserve">
          <source>A micro tokenizer for Chinese</source>
          <target state="translated">中国語用のマイクロトークナイザー</target>
        </trans-unit>
        <trans-unit id="132c4df0b073db9c52f9e77fb8c87dfe8e21b8ad" translate="yes" xml:space="preserve">
          <source>Trie Tree</source>
          <target state="translated">トリーツリー</target>
        </trans-unit>
        <trans-unit id="f2c966a9c350ecfa1722338efba648d4c549af15" translate="yes" xml:space="preserve">
          <source>graphml</source>
          <target state="translated">グラフミリリットル</target>
        </trans-unit>
        <trans-unit id="fca928f59379c844f872038a327fe65e93e7f455" translate="yes" xml:space="preserve">
          <source>一个微型的中文分词器，目前提供了七种分词算法:</source>
          <target state="translated">一分微型的中文分词器、目前挙アルゴリズム七種分词算法：</target>
        </trans-unit>
        <trans-unit id="9583b952d58ee0612dd65a1543063c3fb9042282" translate="yes" xml:space="preserve">
          <source>一样的字典文件，可以轻松添加自定义字典</source>
          <target state="translated">一表的字書文件、可轻松追加自定义字典</target>
        </trans-unit>
        <trans-unit id="a0ae68d36b1bbf07bebcb6d84e70abc203a4d13d" translate="yes" xml:space="preserve">
          <source>使用隐马尔可夫模型（Hidden Markov Model，HMM）来分词</source>
          <target state="translated">使用隐马尔可夫詩（隠れマルコフモデル、HMM）来分词</target>
        </trans-unit>
        <trans-unit id="128eb104071d6db9975396b466b3f54825500cd3" translate="yes" xml:space="preserve">
          <source>具有良好的扩展性：使用和</source>
          <target state="translated">具有多的扩展性：使用和</target>
        </trans-unit>
        <trans-unit id="36edce019e3e3169d6c4e433690dadc6370de784" translate="yes" xml:space="preserve">
          <source>双向最大匹配法</source>
          <target state="translated">双向大為配法</target>
        </trans-unit>
        <trans-unit id="e3ebe9577cc8a0e86076ff6bc11dde26a179acb4" translate="yes" xml:space="preserve">
          <source>反向最大匹配法</source>
          <target state="translated">反向大為配法</target>
        </trans-unit>
        <trans-unit id="269c8bd09ad58ebefc7c0c4edb0db283eb561451" translate="yes" xml:space="preserve">
          <source>基于 CRF (Conditional Random Field, 条件随机场) 的分词方法</source>
          <target state="translated">基在CRF（条件付き確率場、分詞机场）的分词方法</target>
        </trans-unit>
        <trans-unit id="19444cc564553dcd71e92482b5b9c9fc54b56146" translate="yes" xml:space="preserve">
          <source>微型中文分词器</source>
          <target state="translated">微型中文分词器</target>
        </trans-unit>
        <trans-unit id="d444bbd5840b3b7cb6503fdd355f644c84f1bbef" translate="yes" xml:space="preserve">
          <source>按照词语的频率（概率）来利用构建 DAG（有向无环图）来分词，使用</source>
          <target state="translated">按照词语的频率（概率）来利用多建DAG（有向非巡回）来分词、使用</target>
        </trans-unit>
        <trans-unit id="ec943180e5e717709a43b08c599d4372509d41da" translate="yes" xml:space="preserve">
          <source>提供工具和脚本帮助用户训练自己的分词模型而不是使用内建的模型</source>
          <target state="translated">提供自動和脚本帮助用户训练自己的分词脚本不是使用内建的思</target>
        </trans-unit>
        <trans-unit id="6e730593d5680895f3f0773f501e9b8fe0fbb96c" translate="yes" xml:space="preserve">
          <source>更多内容见仓库 &lt;a href=&quot;https://github.com/howl-anderson/MicroTokenizer&quot;&gt;https://github.com/howl-anderson/MicroTokenizer&lt;/a&gt;</source>
          <target state="translated">更多コンテンツ见仓库&lt;a href=&quot;https://github.com/howl-anderson/MicroTokenizer&quot;&gt;https://github.com/howl-anderson/MicroTokenizer&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3bb943dee413b3cc7628f43358ab0880bbd053ef" translate="yes" xml:space="preserve">
          <source>构建前缀字典树</source>
          <target state="translated">到建前缀字典树</target>
        </trans-unit>
        <trans-unit id="cb73944eaf6c44127066d5713947805bb2f112a1" translate="yes" xml:space="preserve">
          <source>格式的图结构文件，辅助学习者理解算法过程</source>
          <target state="translated">格式的・アルゴリズム文件、辅助学习者理解算法過程</target>
        </trans-unit>
        <trans-unit id="0057269277f2b71e161b9bcd0954a7400cc1c9e6" translate="yes" xml:space="preserve">
          <source>正向最大匹配法</source>
          <target state="translated">正向大為配法</target>
        </trans-unit>
        <trans-unit id="6aa9c00c46bd09f7b120f22c1d466e6bd634b6b1" translate="yes" xml:space="preserve">
          <source>特点 / 特色</source>
          <target state="translated">特点/特色</target>
        </trans-unit>
        <trans-unit id="482f834a78ece822aebc26c4f4d2bf2e41d4348b" translate="yes" xml:space="preserve">
          <source>的算法，具有良好的分词性能</source>
          <target state="translated">的アルゴリズム法、具有的分词分詞</target>
        </trans-unit>
        <trans-unit id="67239e80984a2e341342360a9945a4d1c0cf3f8e" translate="yes" xml:space="preserve">
          <source>结巴分词</source>
          <target state="translated">巴分词</target>
        </trans-unit>
        <trans-unit id="cf7298dff8ea4b2703103cf251e87a8a78cb8ddc" translate="yes" xml:space="preserve">
          <source>自定义能力强</source>
          <target state="translated">自定义能力强</target>
        </trans-unit>
        <trans-unit id="5fe5f062a0310515b1699761c1a41f87afb8671e" translate="yes" xml:space="preserve">
          <source>良好的分词性能：由于使用类似</source>
          <target state="translated">分詞分词分詞：由到使用分詞</target>
        </trans-unit>
        <trans-unit id="69d44b843828f49fbbeaf24d692903c5faa3996b" translate="yes" xml:space="preserve">
          <source>融合 DAG 和 HMM 两种分词模型的结果，按照分词粒度最大化的原则进行融合得到的模型</source>
          <target state="translated">重複DAG和HMM两分分词分詞</target>
        </trans-unit>
        <trans-unit id="488b7e4d21c0fcaa79f790fa3f7eb7130891229c" translate="yes" xml:space="preserve">
          <source>面向教育：可以导出</source>
          <target state="translated">面向教育：再導出出</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
