<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ja" datatype="htmlbody" original="https://pypi.org/project/deepspectrum/">
    <body>
      <group id="deepspectrum">
        <trans-unit id="da39a3ee5e6b4b0d3255bfef95601890afd80709" translate="yes" xml:space="preserve">
          <source/>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1cb4bd88ddb5a8f0c8fe4b85b749da82d823ae72" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;(c) 2017-2020 Shahin Amiriparian, Maurice Gerczuk, Sandra Ottl, Bj&amp;ouml;rn Schuller: Universit&amp;auml;t Augsburg&lt;/strong&gt;
Published under GPLv3, see the LICENSE.md file for details.</source>
          <target state="translated">&lt;strong&gt;（c）2017-2020 Shahin Amiriparian、Maurice Gerczuk、Sandra Ottl、Bj&amp;ouml;rnSchuller：Universit&amp;auml;tAugsburgGPLv3で&lt;/strong&gt; 公開されています。詳細については、LICENSE.mdファイルを参照してください。</target>
        </trans-unit>
        <trans-unit id="ebac6302b62dd2f541f0cf3a23c13026bf32bedf" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;DeepSpectrum&lt;/strong&gt; is a Python toolkit for feature extraction from audio data with pre-trained Image Convolutional Neural Networks (CNNs). It features an extraction pipeline which first creates visual representations for audio data - plots of spectrograms or chromagrams - and then feeds them to a pre-trained Image CNN. Activations of a specific layer then form the final feature vectors.</source>
          <target state="translated">&lt;strong&gt;DeepSpectrum&lt;/strong&gt;は、事前にトレーニングされたImage Convolutional Neural Networks（CNN）を使用してオーディオデータから特徴を抽出するためのPythonツールキットです。これは、最初にオーディオデータの視覚的表現（スペクトログラムまたはクロマトグラムのプロット）を作成し、次にそれらを事前にトレーニングされたImageCNNに供給する抽出パイプラインを備えています。次に、特定のレイヤーのアクティブ化により、最終的な特徴ベクトルが形成されます。</target>
        </trans-unit>
        <trans-unit id="a73c505c2793bd025d626af6731d68b4a45269bf" translate="yes" xml:space="preserve">
          <source>Citing</source>
          <target state="translated">引用</target>
        </trans-unit>
        <trans-unit id="0c895889011b796b94426d854b65030179ae6add" translate="yes" xml:space="preserve">
          <source>If you use DeepSpectrum or any code from DeepSpectrum in your research work, you are kindly asked to acknowledge the use of DeepSpectrum in your publications.</source>
          <target state="translated">研究にDeepSpectrumやDeepSpectrumのコードを使用している場合は、出版物にDeepSpectrumを使用していることを認めていただきたいと思います。</target>
        </trans-unit>
        <trans-unit id="6c8bfe5d094dd6fcbf34f0e0a0c079551ad219fd" translate="yes" xml:space="preserve">
          <source>Please direct any questions or requests to Shahin Amiriparian (shahin.amiriparian at tum.de) or Maurice Gercuk (maurice.gerczuk at informatik.uni-augsburg.de).</source>
          <target state="translated">ご質問やご要望は、Shahin Amiriparian (shahin.amiriparian at tum.de)またはMaurice Gercuk (maurice.gerczuk at informatik.uni-augsburg.de)までお願いします。</target>
        </trans-unit>
        <trans-unit id="171a1e4c9e1f5ec9f1fe0c8e4188ee2bb2b1bb0a" translate="yes" xml:space="preserve">
          <source>S. Amiriparian, M. Gerczuk, S. Ottl, N. Cummins, M. Freitag, S. Pugachevskiy, A. Baird and B. Schuller. Snore Sound Classification using Image-Based Deep Spectrum Features. In Proceedings of INTERSPEECH (Vol. 17, pp. 2017-434)</source>
          <target state="translated">S.このように、いびきは、その原因の一つであると考えられていますが、その原因を解明するためには、いびきの発生源を特定することが重要であると考えられています。このように、いびきは、その発生時には、その発生時に発生した音の中から、その発生時に発生した音がどのように変化するのかを予測することができる。INTERSPEECH (Vol.17,pp.2017-434)の講演論文集に掲載されています。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
