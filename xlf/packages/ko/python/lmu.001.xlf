<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="https://pypi.org/project/lmu/">
    <body>
      <group id="lmu">
        <trans-unit id="f7986d315f2a9d4b9005d324dcb9fea7029e3373" translate="yes" xml:space="preserve">
          <source>(A, B)</source>
          <target state="translated">(A, B)</target>
        </trans-unit>
        <trans-unit id="e8b310147d49ec4db715247f737630a341a942ea" translate="yes" xml:space="preserve">
          <source>) and the memory vector (</source>
          <target state="translated">) 및 메모리 벡터 (</target>
        </trans-unit>
        <trans-unit id="9d78d7759ad02cf7cb425cfa08e4a771f7ad35f4" translate="yes" xml:space="preserve">
          <source>) is trained via backpropagation, while the dynamics of the memory remain fixed (&lt;a href=&quot;https://papers.nips.cc/paper/9689-legendre-memory-units-continuous-time-representation-in-recurrent-neural-networks.pdf&quot;&gt;see paper for details&lt;/a&gt;).</source>
          <target state="translated">)은 역 전파를 통해 훈련되는 반면 메모리의 역학은 고정 된 상태로 유지됩니다 ( &lt;a href=&quot;https://papers.nips.cc/paper/9689-legendre-memory-units-continuous-time-representation-in-recurrent-neural-networks.pdf&quot;&gt;자세한 내용은 문서 참조&lt;/a&gt; ).</target>
        </trans-unit>
        <trans-unit id="91f01eed1463fd0e19c70b920cb603e7fd73b2fa" translate="yes" xml:space="preserve">
          <source>) with a nonlinear hidden state (</source>
          <target state="translated">) 비선형 은닉 상태 (</target>
        </trans-unit>
        <trans-unit id="f34ef1a8f3f40a7a1236002bb13939b9aaab7ef2" translate="yes" xml:space="preserve">
          <source>, if necessary. By default the coupling between the hidden state (</source>
          <target state="translated">, 필요하다면. 기본적으로 숨겨진 상태 (</target>
        </trans-unit>
        <trans-unit id="1a8a70853cdb5616fca8eb1afe92a80fdc273967" translate="yes" xml:space="preserve">
          <source>, which obtains the current best-known psMNIST result (using an RNN) of &lt;strong&gt;97.15%&lt;/strong&gt;. Note, the network is using fewer internal state-variables and neurons than there are pixels in the input sequence. To reproduce the results from the paper, run the notebooks in the</source>
          <target state="translated">, 현재 가장 잘 알려진 psMNIST 결과 (RNN 사용) &lt;strong&gt;97.15 %&lt;/strong&gt; 를 얻습니다 . 네트워크는 입력 시퀀스의 픽셀보다 더 적은 내부 상태 변수와 뉴런을 사용합니다. 종이의 결과를 재현하려면 다음에서 노트북을 실행하십시오.</target>
        </trans-unit>
        <trans-unit id="eb0d2eadad1d59670435428d52f11012b3c56467" translate="yes" xml:space="preserve">
          <source>0.1.0 (June 22, 2020)</source>
          <target state="translated">0.1.0 (2020 년 6 월 22 일)</target>
        </trans-unit>
        <trans-unit id="f407f992fca896699f798cda98a678821453fbc9" translate="yes" xml:space="preserve">
          <source>A single</source>
          <target state="translated">싱글</target>
        </trans-unit>
        <trans-unit id="6eb4915552135823111f476864da3f654c233c54" translate="yes" xml:space="preserve">
          <source>Citation</source>
          <target state="translated">소환</target>
        </trans-unit>
        <trans-unit id="51622bd517ce184905e899085f3bf2c222cf7501" translate="yes" xml:space="preserve">
          <source>GitHub repository includes a pre-trained Keras/TensorFlow model, located at</source>
          <target state="translated">GitHub 저장소에는 사전 학습 된 Keras / TensorFlow 모델이 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="ad6d35f4b9d3d4fd1498c1969d140852aa27ea84" translate="yes" xml:space="preserve">
          <source>Initial release of LMU 0.1.0! Supports Python 3.5+.</source>
          <target state="translated">LMU 0.1.0의 최초 출시! Python 3.5 이상을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="5db3dea372a23fb11a890296dd05b06a6f2c43b5" translate="yes" xml:space="preserve">
          <source>LMUCell</source>
          <target state="translated">LMUCell</target>
        </trans-unit>
        <trans-unit id="68142c1fecd8cd5a852b6821a313d6f18e67bd63" translate="yes" xml:space="preserve">
          <source>LMUs in NengoDL (reproducing SotA on psMNIST)</source>
          <target state="translated">NengoDL의 LMU (psMNIST에서 SotA 재생)</target>
        </trans-unit>
        <trans-unit id="94147d1825c4f1bb9b2a97d3e0d79fe72940ffcb" translate="yes" xml:space="preserve">
          <source>Legendre Memory Units</source>
          <target state="translated">르장 드르 메모리 유닛</target>
        </trans-unit>
        <trans-unit id="12e48a2c80ec09fba890acbab555ee09d4efed0c" translate="yes" xml:space="preserve">
          <source>Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks</source>
          <target state="translated">르장 드르 메모리 유닛 : 반복 신경망에서 연속 시간 표현</target>
        </trans-unit>
        <trans-unit id="934f951fd58afc4e51af5bd95b0b06d48ea9da9c" translate="yes" xml:space="preserve">
          <source>Nengo Examples</source>
          <target state="translated">Nengo 예제</target>
        </trans-unit>
        <trans-unit id="22d507f2ba74e43593de3ae3f550bf202c076adc" translate="yes" xml:space="preserve">
          <source>Paper</source>
          <target state="translated">종이</target>
        </trans-unit>
        <trans-unit id="5044ba0a5cf607ebea4e58623c6417cb87f682bb" translate="yes" xml:space="preserve">
          <source>Release history</source>
          <target state="translated">출시 역사</target>
        </trans-unit>
        <trans-unit id="019292c2324bce7b4b7238901874de053dae61e5" translate="yes" xml:space="preserve">
          <source>Spiking LMUs in Nengo (with online learning)</source>
          <target state="translated">Nengo의 LMU 급증 (온라인 학습 포함)</target>
        </trans-unit>
        <trans-unit id="17fe76b2cbb233c1e454e6043ea8b785cf235f64" translate="yes" xml:space="preserve">
          <source>Spiking LMUs in Nengo Loihi (with online learning)</source>
          <target state="translated">Nengo Loihi의 LMU 급증 (온라인 학습 포함)</target>
        </trans-unit>
        <trans-unit id="86d3603c97865cc4b8bed02e3a4ce17db2ddf9b9" translate="yes" xml:space="preserve">
          <source>Thanks to all of the contributors for making this possible!</source>
          <target state="translated">이를 가능하게 해준 모든 기여자들에게 감사드립니다!</target>
        </trans-unit>
        <trans-unit id="93ef0dd827103681fcee453b78be2ff14e1a261d" translate="yes" xml:space="preserve">
          <source>The</source>
          <target state="translated">그만큼</target>
        </trans-unit>
        <trans-unit id="7812ef13cd5d03416a8df141fa5c9d4816525504" translate="yes" xml:space="preserve">
          <source>The API is considered unstable; parts are likely to change in the future.</source>
          <target state="translated">API는 불안정한 것으로 간주됩니다. 부품은 향후 변경 될 가능성이 있습니다.</target>
        </trans-unit>
        <trans-unit id="e844712e089b0a5bc92ef35f09387b6d30e8162a" translate="yes" xml:space="preserve">
          <source>The discretized</source>
          <target state="translated">이산화 된</target>
        </trans-unit>
        <trans-unit id="d7507de5fb53855c565db091e29dff83decf8e06" translate="yes" xml:space="preserve">
          <source>We propose a novel memory cell for recurrent neural networks that dynamically maintains information across long windows of time using relatively few resources. The Legendre Memory Unit (LMU) is mathematically derived to orthogonalize its continuous-time history &amp;ndash; doing so by solving d coupled ordinary differential equations (ODEs), whose phase space linearly maps onto sliding windows of time via the Legendre polynomials up to degree d &amp;minus; 1 (example d=12, shown below).</source>
          <target state="translated">상대적으로 적은 리소스를 사용하여 긴 시간 동안 정보를 동적으로 유지하는 순환 신경망을위한 새로운 메모리 셀을 제안합니다. 르장 드르 메모리 유닛 (LMU)은 연속 시간 히스토리를 직교 화하기 위해 수학적으로 유도됩니다. d 결합 상미 분 방정식 (ODE)을 풀면 위상 공간이 최대 d 차까지 르장 드르 다항식을 통해 시간의 슬라이딩 윈도우에 선형으로 매핑됩니다. 1 (아래에 표시된 예 d = 12).</target>
        </trans-unit>
        <trans-unit id="af5514d5e2de36d647091221bbc2576aad7eeb67" translate="yes" xml:space="preserve">
          <source>branch in the</source>
          <target state="translated">지점에서</target>
        </trans-unit>
        <trans-unit id="66f1fb81d4d11d662fbb9d02462591f64b87de95" translate="yes" xml:space="preserve">
          <source>branch.</source>
          <target state="translated">분기.</target>
        </trans-unit>
        <trans-unit id="d3134f6bf31c14bcd86b2f42eacf8ca26224ba95" translate="yes" xml:space="preserve">
          <source>directory within the</source>
          <target state="translated">내 디렉토리</target>
        </trans-unit>
        <trans-unit id="71ab8b6afb1bae3df247e0286da35e0da16564ff" translate="yes" xml:space="preserve">
          <source>docs</source>
          <target state="translated">문서</target>
        </trans-unit>
        <trans-unit id="c828076471935e6b0e12c70c56368cf19fbf3762" translate="yes" xml:space="preserve">
          <source>experiments</source>
          <target state="translated">실험</target>
        </trans-unit>
        <trans-unit id="c4b89cb9b895edeb1054b2f4f97660e540b70508" translate="yes" xml:space="preserve">
          <source>expresses the following computational graph in Keras as an RNN layer, which couples the optimal linear memory (</source>
          <target state="translated">Keras의 다음 계산 그래프를 RNN 계층으로 표현하여 최적의 선형 메모리 (</target>
        </trans-unit>
        <trans-unit id="27d5482eebd075de44389774fce28c69f45c8a75" translate="yes" xml:space="preserve">
          <source>h</source>
          <target state="translated">h</target>
        </trans-unit>
        <trans-unit id="fbaee08b1fd3c3083cbbfc97fe14ba0acb73a3ab" translate="yes" xml:space="preserve">
          <source>includes an example for how to use the</source>
          <target state="translated">사용 방법에 대한 예가 포함되어 있습니다.</target>
        </trans-unit>
        <trans-unit id="dd84f8d3e9fee1217eaa3b5560a2dcf1324e9c29" translate="yes" xml:space="preserve">
          <source>lmu</source>
          <target state="translated">lmu</target>
        </trans-unit>
        <trans-unit id="6b0d31c0d563223024da45691584643ac78c96e8" translate="yes" xml:space="preserve">
          <source>m</source>
          <target state="translated">미디엄</target>
        </trans-unit>
        <trans-unit id="8641d131425d9c0df501277f23d70a55c1e2c120" translate="yes" xml:space="preserve">
          <source>matrices are initialized according to the LMU&amp;rsquo;s mathematical derivation with respect to some chosen window length, &amp;theta;. Backpropagation can be used to learn this time-scale, or fine-tune</source>
          <target state="translated">행렬은 선택한 창 길이 &amp;theta;에 대한 LMU의 수학적 파생에 따라 초기화됩니다. 역 전파를 사용하여이 시간 척도를 학습하거나 미세 조정할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="18bcc7cebf6c0b3db118c68544699b960be664ae" translate="yes" xml:space="preserve">
          <source>models/psMNIST-standard.hdf5</source>
          <target state="translated">models / psMNIST-standard.hdf5</target>
        </trans-unit>
        <trans-unit id="950593b1f42de841169aa7d59486b7e980bc15cf" translate="yes" xml:space="preserve">
          <source>paper</source>
          <target state="translated">종이</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
