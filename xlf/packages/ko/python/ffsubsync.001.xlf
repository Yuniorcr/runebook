<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="https://pypi.org/project/ffsubsync/">
    <body>
      <group id="ffsubsync">
        <trans-unit id="448fa61d60809d27051e5604a1053eafdfc3dbbb" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;http://www.numpy.org/&quot;&gt;numpy&lt;/a&gt; and, indirectly, &lt;a href=&quot;https://www.netlib.org/fftpack/&quot;&gt;FFTPACK&lt;/a&gt;, which powers the FFT-based algorithm for fast scoring of alignments between subtitles (or subtitles and video)</source>
          <target state="translated">&lt;a href=&quot;http://www.numpy.org/&quot;&gt;numpy&lt;/a&gt; 및 간접적으로 &lt;a href=&quot;https://www.netlib.org/fftpack/&quot;&gt;FFTPACK&lt;/a&gt; 은 FFT 기반 알고리즘을 사용하여 자막 (또는 자막 및 비디오) 간의 빠른 정렬 점수를 매 깁니다 .</target>
        </trans-unit>
        <trans-unit id="809173aad90d1b10ce189edf1551d1b0699e2f6a" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;https://pypi.org/project/srt/&quot;&gt;srt&lt;/a&gt; for operating on &lt;a href=&quot;https://en.wikipedia.org/wiki/SubRip#SubRip_text_file_format&quot;&gt;SRT files&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://pypi.org/project/srt/&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/SubRip#SubRip_text_file_format&quot;&gt;SRT 파일&lt;/a&gt; 에서 작동하기위한 srt</target>
        </trans-unit>
        <trans-unit id="b1d4e9959623befe0fa4c1f81c6a08d9ec02186a" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;https://www.ffmpeg.org/&quot;&gt;ffmpeg&lt;/a&gt; and the &lt;a href=&quot;https://github.com/kkroening/ffmpeg-python&quot;&gt;ffmpeg-python&lt;/a&gt; wrapper, for extracting raw audio from video</source>
          <target state="translated">&lt;a href=&quot;https://www.ffmpeg.org/&quot;&gt;&lt;/a&gt;비디오에서 원시 오디오를 추출하기위한 ffmpeg 및 &lt;a href=&quot;https://github.com/kkroening/ffmpeg-python&quot;&gt;ffmpeg-python&lt;/a&gt; 래퍼</target>
        </trans-unit>
        <trans-unit id="f402db43144acbb0439ef8edcbe14c9699525da0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;ffs&lt;/code&gt;, &lt;code&gt;subsync&lt;/code&gt; and &lt;code&gt;ffsubsync&lt;/code&gt; all work as entrypoints:</source>
          <target state="translated">&lt;code&gt;ffs&lt;/code&gt; , &lt;code&gt;subsync&lt;/code&gt; 및 &lt;code&gt;ffsubsync&lt;/code&gt; 는 모두 진입 점으로 작동합니다.</target>
        </trans-unit>
        <trans-unit id="14b64238bfe66703a43272be5c9729b5ca77e61f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;ffsubsync&lt;/code&gt; uses the file extension to decide whether to perform voice activity
detection on the audio or to directly extract speech from an srt file.</source>
          <target state="translated">&lt;code&gt;ffsubsync&lt;/code&gt; 는 파일 확장자를 사용하여 오디오에서 음성 활동 감지를 수행할지 또는 srt 파일에서 직접 음성을 추출할지 여부를 결정합니다.</target>
        </trans-unit>
        <trans-unit id="00799ba9e026c577347c81635a03c034bba3e14e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;ffsubsync&lt;/code&gt; usually finishes in 20 to 30 seconds, depending on the length of the
video. The most expensive step is actually extraction of raw audio. If you
already have a correctly synchronized &quot;reference&quot; srt file (in which case audio
extraction can be skipped), &lt;code&gt;ffsubsync&lt;/code&gt; typically runs in less than a second.</source>
          <target state="translated">&lt;code&gt;ffsubsync&lt;/code&gt; 는 일반적으로 동영상 길이에 따라 20 ~ 30 초 내에 완료됩니다. 가장 비용이 많이 드는 단계는 실제로 원시 오디오를 추출하는 것입니다. 이미 올바르게 동기화 된 &quot;참조&quot;srt 파일 (이 경우 오디오 추출을 건너 뛸 수 있음)이있는 경우 &lt;code&gt;ffsubsync&lt;/code&gt; 는 일반적으로 1 초 이내에 실행됩니다.</target>
        </trans-unit>
        <trans-unit id="936db9a8629b6aee1da2ac35146b55e3ac7b77db" translate="yes" xml:space="preserve">
          <source>At the request of some, you can now help cover my coffee expenses using the
Github Sponsors button at the top (recurring monthly payments), or using the below
Paypal Donate button (one-time payment):</source>
          <target state="translated">일부 요청에 따라 상단의 Github 스폰서 버튼 (매월 반복 결제)을 사용하거나 아래 페이팔 기부 버튼 (일회성 결제)을 사용하여 내 커피 비용을 충당 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="56f9b02e470ae04f5605651b0ca5cd5920c7d715" translate="yes" xml:space="preserve">
          <source>Besides general stability and usability improvements, one line
of work aims to extend the synchronization algorithm to handle splits
/ breaks in the middle of video not present in subtitles (or vice versa).
Developing a robust solution will take some time (assuming one is possible).
See &lt;a href=&quot;https://github.com/smacke/ffsubsync/issues/10&quot;&gt;#10&lt;/a&gt; for more details.</source>
          <target state="translated">일반적인 안정성 및 사용성 향상 외에도 한 줄의 작업은 자막에없는 비디오 중간에 분할 / 중단을 처리하도록 동기화 알고리즘을 확장하는 것입니다 (또는 그 반대의 경우). 강력한 솔루션을 개발하려면 시간이 걸립니다 (가능하다고 가정). 자세한 내용은 &lt;a href=&quot;https://github.com/smacke/ffsubsync/issues/10&quot;&gt;# 10&lt;/a&gt; 을 참조하세요.</target>
        </trans-unit>
        <trans-unit id="bd217ff4214f14c1a032a45a800183c165249895" translate="yes" xml:space="preserve">
          <source>Code in this project is &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;MIT licensed&lt;/a&gt;.</source>
          <target state="translated">이 프로젝트의 코드는 &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;MIT 라이센스&lt;/a&gt; 입니다.</target>
        </trans-unit>
        <trans-unit id="bfac50d6424b5166c3ee2808c85ae7c139b5182f" translate="yes" xml:space="preserve">
          <source>Credits</source>
          <target state="translated">크레딧</target>
        </trans-unit>
        <trans-unit id="1946ed4e6d7a5cdee8942980594304622d5d448e" translate="yes" xml:space="preserve">
          <source>Discretize video and subtitles by time into 10ms windows.</source>
          <target state="translated">비디오와 자막을 시간별로 10ms 창으로 구분합니다.</target>
        </trans-unit>
        <trans-unit id="61f87fb699391c08f8809648c3d512e2529450b6" translate="yes" xml:space="preserve">
          <source>FFsubsync</source>
          <target state="translated">FF 서브 싱크</target>
        </trans-unit>
        <trans-unit id="d14e9d216a8814e80e6ada0f122fa233375bf1ab" translate="yes" xml:space="preserve">
          <source>First, make sure ffmpeg is installed. On MacOS, this looks like:</source>
          <target state="translated">먼저 ffmpeg가 설치되어 있는지 확인하십시오. MacOS에서는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="42edbb52a87cd00061f18e023951cf7cd5c5ef15" translate="yes" xml:space="preserve">
          <source>For each 10ms window, determine whether that window contains speech.  This
is trivial to do for subtitles (we just determine whether any subtitle is
&quot;on&quot; during each time window); for video, use an off-the-shelf voice
activity detector (VAD) like
the one built into &lt;a href=&quot;https://webrtc.org/&quot;&gt;webrtc&lt;/a&gt;.</source>
          <target state="translated">각 10ms 창에 대해 해당 창에 음성이 포함되어 있는지 확인합니다. 이것은 자막에 대해 간단하게 수행 할 수 있습니다 (각 시간 창에서 자막이 &quot;켜져&quot;있는지 여부 만 확인합니다). 비디오의 경우 &lt;a href=&quot;https://webrtc.org/&quot;&gt;webrtc에&lt;/a&gt; 내장 된 것과 같은 기성품 음성 활동 감지기 (VAD)를 사용합니다 .</target>
        </trans-unit>
        <trans-unit id="c3cba160fe064a0d9a4fc24f20540221a2aff0ed" translate="yes" xml:space="preserve">
          <source>Future Work</source>
          <target state="translated">미래의 일</target>
        </trans-unit>
        <trans-unit id="ad0a916130b1ffb1194d9d811a12d26d58f8df6c" translate="yes" xml:space="preserve">
          <source>Helping Development</source>
          <target state="translated">개발 지원</target>
        </trans-unit>
        <trans-unit id="90ccd6497400b5576aeca1bd94af74aae1e0a250" translate="yes" xml:space="preserve">
          <source>History</source>
          <target state="translated">역사</target>
        </trans-unit>
        <trans-unit id="8cecc82d72d68f9151cb0901cfe40eeac15bfca6" translate="yes" xml:space="preserve">
          <source>How It Works</source>
          <target state="translated">작동 원리</target>
        </trans-unit>
        <trans-unit id="44abaabed373ce699f3a6b3019932545e4158606" translate="yes" xml:space="preserve">
          <source>If you want to live dangerously, you can grab the latest version as follows:</source>
          <target state="translated">위험하게 살고 싶다면 다음과 같이 최신 버전을 구할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ce1d79c980df3c47404d3d29df7c08886649970a" translate="yes" xml:space="preserve">
          <source>In most cases, inconsistencies between video and subtitles occur when starting
or ending segments present in video are not present in subtitles, or vice versa.
This can occur, for example, when a TV episode recap in the subtitles was pruned
from video. FFsubsync typically works well in these cases, and in my experience
this covers &amp;gt;95% of use cases. Handling breaks and splits outside of the beginning
and ending segments is left to future work (see below).</source>
          <target state="translated">대부분의 경우 비디오에있는 시작 또는 끝 세그먼트가 자막에 없거나 그 반대 일 때 비디오와 자막 사이의 불일치가 발생합니다. 예를 들어 자막의 TV 에피소드 요약이 비디오에서 제거 된 경우 이러한 상황이 발생할 수 있습니다. FFsubsync는 일반적으로 이러한 경우에 잘 작동하며 내 경험상 95 % 이상의 사용 사례를 다룹니다. 시작 및 종료 세그먼트 외부의 중단 및 분할 처리는 향후 작업에 맡겨집니다 (아래 참조).</target>
        </trans-unit>
        <trans-unit id="fd6c3ebf7befca9f8208f86c76e4d4180303745c" translate="yes" xml:space="preserve">
          <source>Install</source>
          <target state="translated">설치</target>
        </trans-unit>
        <trans-unit id="659525747c675f26d0d0761f93182ab5228cbc26" translate="yes" xml:space="preserve">
          <source>Into this:</source>
          <target state="translated">이것으로 :</target>
        </trans-unit>
        <trans-unit id="0d05fd8158dba0121b4eac110b20f240f7818b92" translate="yes" xml:space="preserve">
          <source>Language-agnostic automatic synchronization of subtitles with video, so that
subtitles are aligned to the correct starting point within the video.</source>
          <target state="translated">자막을 비디오와 언어에 구애받지 않고 자동으로 동기화하여 자막이 비디오 내의 올바른 시작 지점에 정렬되도록합니다.</target>
        </trans-unit>
        <trans-unit id="420a9347981effc83ba7d1df62790a49716f38bb" translate="yes" xml:space="preserve">
          <source>Language-agnostic synchronization of subtitles with video.</source>
          <target state="translated">비디오와 자막의 언어에 구애받지 않는 동기화.</target>
        </trans-unit>
        <trans-unit id="3229609e15436ec51bcf00818a69a84dbc58a0c2" translate="yes" xml:space="preserve">
          <source>License</source>
          <target state="translated">특허</target>
        </trans-unit>
        <trans-unit id="a7c04c64ed3f2a9374590c76c50d3b7f1b18e3da" translate="yes" xml:space="preserve">
          <source>Limitations</source>
          <target state="translated">한계</target>
        </trans-unit>
        <trans-unit id="beac1d4d9d908d34bbabd5497b31900615c1e739" translate="yes" xml:space="preserve">
          <source>Next, grab the script. It should work with both Python 2 and Python 3:</source>
          <target state="translated">다음으로 스크립트를 가져옵니다. Python 2와 Python 3에서 모두 작동해야합니다.</target>
        </trans-unit>
        <trans-unit id="179a4231cbcf797a1ea61865a797b3047681d7b0" translate="yes" xml:space="preserve">
          <source>Now we have two binary strings: one for the subtitles, and one for the
video.  Try to align these strings by matching 0's with 0's and 1's with
1's. We score these alignments as (# video 1's matched w/ subtitle 1's) - (#
video 1's matched with subtitle 0's).</source>
          <target state="translated">이제 두 개의 이진 문자열이 있습니다. 하나는 자막 용이고 다른 하나는 비디오 용입니다. 0을 0과 1을 1과 일치시켜이 문자열을 정렬하십시오. 이 정렬은 (# 비디오 1이 자막 1과 일치 함)-(# 비디오 1이 자막 0과 일치 함)으로 점수를 매 깁니다.</target>
        </trans-unit>
        <trans-unit id="bc49bcc1bbf9322f0d673c127d5dd1764357aa92" translate="yes" xml:space="preserve">
          <source>Other excellent Python libraries like &lt;a href=&quot;https://docs.python.org/3/library/argparse.html&quot;&gt;argparse&lt;/a&gt; and &lt;a href=&quot;https://tqdm.github.io/&quot;&gt;tqdm&lt;/a&gt;, not related to the core functionality, but which enable much better experiences for developers and users.</source>
          <target state="translated">&lt;a href=&quot;https://docs.python.org/3/library/argparse.html&quot;&gt;argparse&lt;/a&gt; 및 &lt;a href=&quot;https://tqdm.github.io/&quot;&gt;tqdm&lt;/a&gt; 과 같은 다른 훌륭한 Python 라이브러리 는 핵심 기능과 관련이 없지만 개발자와 사용자에게 훨씬 더 나은 경험을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="2d2cb022bc3d26bd1407c4aa787d5e46e1ad4c3b" translate="yes" xml:space="preserve">
          <source>Speed</source>
          <target state="translated">속도</target>
        </trans-unit>
        <trans-unit id="6d75a7aec098fab5da5f5616cd98e0d6589fb421" translate="yes" xml:space="preserve">
          <source>The best-scoring alignment from step 3 determines how to offset the subtitles
in time so that they are properly synced with the video. Because the binary
strings are fairly long (millions of digits for video longer than an hour), the
naive O(n^2) strategy for scoring all alignments is unacceptable. Instead, we
use the fact that &quot;scoring all alignments&quot; is a convolution operation and can
be implemented with the Fast Fourier Transform (FFT), bringing the complexity
down to O(n log n).</source>
          <target state="translated">3 단계의 최고 점수 정렬은 자막이 비디오와 적절하게 동기화되도록 제 시간에 자막을 오프셋하는 방법을 결정합니다. 이진 문자열이 상당히 길기 때문에 (한 시간보다 긴 비디오의 경우 수백만 자릿수) 모든 정렬을 채점하는 순진한 O (n ^ 2) 전략은 허용되지 않습니다. 대신, &quot;모든 정렬 점수 매기기&quot;는 컨볼 루션 작업이며 FFT (Fast Fourier Transform)로 구현할 수 있다는 사실을 사용하여 복잡성을 O (n log n)로 낮 춥니 다.</target>
        </trans-unit>
        <trans-unit id="b6cb2e1a9df05d019e4ce2e098c42168484163b6" translate="yes" xml:space="preserve">
          <source>The implementation for this project was started during HackIllinois 2019, for
which it received an &lt;strong&gt;&lt;em&gt;Honorable Mention&lt;/em&gt;&lt;/strong&gt; (ranked in the top 5 projects,
excluding projects that won company-specific prizes).</source>
          <target state="translated">이 프로젝트의 구현은 HackIllinois 2019에서 시작되어 &lt;strong&gt;&lt;em&gt;Honorable Mention&lt;/em&gt;&lt;/strong&gt; (회사 별 상을 수상한 프로젝트를 제외한 상위 5 개 프로젝트에 포함)을 받았습니다.</target>
        </trans-unit>
        <trans-unit id="c24de39b3bb0b16a5df396d44c3f3aebba913c44" translate="yes" xml:space="preserve">
          <source>The synchronization algorithm operates in 3 steps:</source>
          <target state="translated">동기화 알고리즘은 3 단계로 작동합니다.</target>
        </trans-unit>
        <trans-unit id="94226cd553bfe5c121e7a05322ec2c6041760ba8" translate="yes" xml:space="preserve">
          <source>There may be occasions where you have a correctly synchronized srt file in a
language you are unfamiliar with, as well as an unsynchronized srt file in your
native language. In this case, you can use the correctly synchronized srt file
directly as a reference for synchronization, instead of using the video as the
reference:</source>
          <target state="translated">익숙하지 않은 언어로 올바르게 동기화 된 srt 파일과 모국어로 동기화되지 않은 srt 파일이있는 경우가있을 수 있습니다. 이 경우 비디오를 참조로 사용하는 대신 올바르게 동기화 된 srt 파일을 동기화를위한 참조로 직접 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="da9f448081113e5f2aa6a2b4381ed0a353647124" translate="yes" xml:space="preserve">
          <source>This project would not be possible without the following libraries:</source>
          <target state="translated">이 프로젝트는 다음 라이브러리 없이는 불가능합니다.</target>
        </trans-unit>
        <trans-unit id="7240d4aab825d0799c8f63afc0c0ff0047392b6e" translate="yes" xml:space="preserve">
          <source>Turn this:</source>
          <target state="translated">이것을 돌려 :</target>
        </trans-unit>
        <trans-unit id="0bb18642b70b9f8a9c12ccf39487328f306b8e19" translate="yes" xml:space="preserve">
          <source>Usage</source>
          <target state="translated">용법</target>
        </trans-unit>
        <trans-unit id="75a062ef757a8dddb6401a1ad2fd757ca432ebd9" translate="yes" xml:space="preserve">
          <source>VAD from &lt;a href=&quot;https://webrtc.org/&quot;&gt;webrtc&lt;/a&gt; and the &lt;a href=&quot;https://github.com/wiseman/py-webrtcvad&quot;&gt;py-webrtcvad&lt;/a&gt; wrapper, for speech detection</source>
          <target state="translated">음성 감지를위한 &lt;a href=&quot;https://webrtc.org/&quot;&gt;webrtc&lt;/a&gt; 및 &lt;a href=&quot;https://github.com/wiseman/py-webrtcvad&quot;&gt;py-webrtcvad&lt;/a&gt; 래퍼 의 VAD</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
