<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="https://pypi.org/project/pyfilefixity/">
    <body>
      <group id="pyfilefixity">
        <trans-unit id="c1a19cd312383081d9e2c55eee4a08e0de4945c8" translate="yes" xml:space="preserve">
          <source>(index file is generated automatically with ecc.txt):</source>
          <target state="translated">(index file is generated automatically with ecc.txt):</target>
        </trans-unit>
        <trans-unit id="eedf89535ec270dad05fe27192344e05678c7faa" translate="yes" xml:space="preserve">
          <source>--ecc_algo</source>
          <target state="translated">--ecc_algo</target>
        </trans-unit>
        <trans-unit id="74d33e8bb68e91efe1b47efc3c53ce591117d796" translate="yes" xml:space="preserve">
          <source>--ecc_algo 1</source>
          <target state="translated">--ecc_algo 1</target>
        </trans-unit>
        <trans-unit id="4bc5f1a03df857308efdd2127a3a109d89cd6c39" translate="yes" xml:space="preserve">
          <source>--ecc_algo 2</source>
          <target state="translated">--ecc_algo 2</target>
        </trans-unit>
        <trans-unit id="ef0ce30599560591d8db3ab093f6a6101aa8495a" translate="yes" xml:space="preserve">
          <source>--ecc_algo 3</source>
          <target state="translated">--ecc_algo 3</target>
        </trans-unit>
        <trans-unit id="dca17bcb829e718ceb389616a615120d9499a7ec" translate="yes" xml:space="preserve">
          <source>--ecc_algo 4</source>
          <target state="translated">--ecc_algo 4</target>
        </trans-unit>
        <trans-unit id="141cc03f48906f90e81f80c6b8d7203147ab71f2" translate="yes" xml:space="preserve">
          <source>--gui</source>
          <target state="translated">--gui</target>
        </trans-unit>
        <trans-unit id="9a8265a5ba2c33881e2717e7581df323a5188174" translate="yes" xml:space="preserve">
          <source>--help</source>
          <target state="translated">--도움</target>
        </trans-unit>
        <trans-unit id="1ffa835f18b0d84b7be1c7c0ecd4eb3e149bf81a" translate="yes" xml:space="preserve">
          <source>-t</source>
          <target state="translated">-티</target>
        </trans-unit>
        <trans-unit id="20cde52d2d15a6cd35b68cb520bfb6ed4c0634f3" translate="yes" xml:space="preserve">
          <source>. This will
install all the necessary stuff along the cython library. Then you can
simply execute again the command</source>
          <target state="translated">. This will install all the necessary stuff along the cython library. Then you can simply execute again the command</target>
        </trans-unit>
        <trans-unit id="34c31a3f0623cfb337a95da5d1ea91d2e06e7a77" translate="yes" xml:space="preserve">
          <source>. You can also detect any corruption using</source>
          <target state="translated">. You can also detect any corruption using</target>
        </trans-unit>
        <trans-unit id="e5ae32e4a07c51fff017d32fb928c603d7714006" translate="yes" xml:space="preserve">
          <source>1- Install a C compiler for your platform. On Linux, gcc should already
be installed. On Windows, you need to use the Visual Studio C compiler
(not MinGW nor Cygwin gcc, they won&amp;rsquo;t work). You can use the &amp;ldquo;Microsoft
Visual C++ Compiler for Python 2.7&amp;rdquo;, and follow these instructions to
make it work if you have Python &amp;lt; 2.7.10:</source>
          <target state="translated">1- Install a C compiler for your platform. On Linux, gcc should already be installed. On Windows, you need to use the Visual Studio C compiler (not MinGW nor Cygwin gcc, they won&amp;rsquo;t work). You can use the &amp;ldquo;Microsoft Visual C++ Compiler for Python 2.7&amp;rdquo;, and follow these instructions to make it work if you have Python &amp;lt; 2.7.10:</target>
        </trans-unit>
        <trans-unit id="126055771fea0dfb803c7c90654cf362c2254e3b" translate="yes" xml:space="preserve">
          <source>1- they need a regular plug to keep the internal magnetic disks
electrified (else the data will just fade away when there&amp;rsquo;s no
residual electricity).</source>
          <target state="translated">1- they need a regular plug to keep the internal magnetic disks electrified (else the data will just fade away when there&amp;rsquo;s no residual electricity).</target>
        </trans-unit>
        <trans-unit id="d3fb64e65e5c67b491fa515c871b5949a6c7f691" translate="yes" xml:space="preserve">
          <source>1- use dd_rescue to make a full bit-per-bit verbatim copy of your drive
before it dies. The nice thing with dd_rescue is that the copy is
exact, and also that it can retries or skip in case of bad sectors (it
won&amp;rsquo;t crash on your suddenly at half the process).</source>
          <target state="translated">1- use dd_rescue to make a full bit-per-bit verbatim copy of your drive before it dies. The nice thing with dd_rescue is that the copy is exact, and also that it can retries or skip in case of bad sectors (it won&amp;rsquo;t crash on your suddenly at half the process).</target>
        </trans-unit>
        <trans-unit id="70908e0ac88fee3627fa4e2c4d4aa6e3bf58989a" translate="yes" xml:space="preserve">
          <source>1. it allows to partially repair a file, even if not all
the blocks can be corrected (in PAR2, a file is repaired only if all
blocks can be repaired, which is a shame because there are still other
blocks that could be repaired and thus produce a less corrupted file) ;</source>
          <target state="translated">1. it allows to partially repair a file, even if not all the blocks can be corrected (in PAR2, a file is repaired only if all blocks can be repaired, which is a shame because there are still other blocks that could be repaired and thus produce a less corrupted file) ;</target>
        </trans-unit>
        <trans-unit id="b84a19459711ac5acd896053c20ce0e2e306b69e" translate="yes" xml:space="preserve">
          <source>2- Use testdisk to restore partition or to copy files based on partition
filesystem informations.</source>
          <target state="translated">2- Use testdisk to restore partition or to copy files based on partition filesystem informations.</target>
        </trans-unit>
        <trans-unit id="f5537b221be45490c41a31d0f32b6f5eaa3630f9" translate="yes" xml:space="preserve">
          <source>2- cd to this folder (where pyFileFixity resides), and execute the
following command:</source>
          <target state="translated">2- cd to this folder (where pyFileFixity resides), and execute the following command:</target>
        </trans-unit>
        <trans-unit id="599640e7c97d4c455121ebedd5fc5260f9a39612" translate="yes" xml:space="preserve">
          <source>2- the reading instrument is directly included and merged with the
data (this is the green electronic board you see from the outside, and
the internal head). This is good for quick consumer use (don&amp;rsquo;t need to
buy another instrument: the HDD can just be plugged and it works), but
it&amp;rsquo;s very bad for long term storage, because the reading instrument is
bound to fail, and a lot faster than the data can fade away: this
means that even if your magnetic disks inside your HDD still holds
your data, if the controller board or the head doesn&amp;rsquo;t work anymore,
your data is just lost. And a head (and a controller board) are almost
impossible to replace, even by professionals, because the pieces are
VERY hard to find (different for each HDD production line) and each
HDD has some small physical defects, thus it&amp;rsquo;s impossible to reproduce
that too (because the head is so close to the magnetic disk that if
you try to do that manually you&amp;rsquo;ll probably fail).</source>
          <target state="translated">2- the reading instrument is directly included and merged with the data (this is the green electronic board you see from the outside, and the internal head). This is good for quick consumer use (don&amp;rsquo;t need to buy another instrument: the HDD can just be plugged and it works), but it&amp;rsquo;s very bad for long term storage, because the reading instrument is bound to fail, and a lot faster than the data can fade away: this means that even if your magnetic disks inside your HDD still holds your data, if the controller board or the head doesn&amp;rsquo;t work anymore, your data is just lost. And a head (and a controller board) are almost impossible to replace, even by professionals, because the pieces are VERY hard to find (different for each HDD production line) and each HDD has some small physical defects, thus it&amp;rsquo;s impossible to reproduce that too (because the head is so close to the magnetic disk that if you try to do that manually you&amp;rsquo;ll probably fail).</target>
        </trans-unit>
        <trans-unit id="e59275e0c54e4da1b2f398a765471d0460c23d1e" translate="yes" xml:space="preserve">
          <source>2. the ecc file format is quite simple and readable, easy to process by
any script, which would allow other softwares to also work on it (and it
was also done in this way to be more resilient against error
corruptions, so that even if an entry is corrupted, other entries are
independent and can maybe be used, thus the ecc is very error tolerant.
This idea was implemented in repair_ecc.py but it could be extended,
especially if you know the pattern of the corruption).</source>
          <target state="translated">2. the ecc file format is quite simple and readable, easy to process by any script, which would allow other softwares to also work on it (and it was also done in this way to be more resilient against error corruptions, so that even if an entry is corrupted, other entries are independent and can maybe be used, thus the ecc is very error tolerant. This idea was implemented in repair_ecc.py but it could be extended, especially if you know the pattern of the corruption).</target>
        </trans-unit>
        <trans-unit id="ed381e1dc60198639f8ffde751e8ca49b65e5952" translate="yes" xml:space="preserve">
          <source>3- If you could not recover your files, you can try file scraping using
&lt;a href=&quot;http://www.cgsecurity.org/wiki/PhotoRec&quot;&gt;photorec&lt;/a&gt; or
&lt;a href=&quot;http://plaso.kiddaland.net/&quot;&gt;plaso&lt;/a&gt; other similar tools as
a last resort to extract data based only from files content (no filename,
often uncorrect filetype, file boundaries may be wrong so some data
may be cut off, etc.).</source>
          <target state="translated">3- If you could not recover your files, you can try file scraping using &lt;a href=&quot;http://www.cgsecurity.org/wiki/PhotoRec&quot;&gt;photorec&lt;/a&gt; or &lt;a href=&quot;http://plaso.kiddaland.net/&quot;&gt;plaso&lt;/a&gt; other similar tools as a last resort to extract data based only from files content (no filename, often uncorrect filetype, file boundaries may be wrong so some data may be cut off, etc.).</target>
        </trans-unit>
        <trans-unit id="b123a82604826f2f2aa574c0522954fefde9f0ee" translate="yes" xml:space="preserve">
          <source>3- You can now launch pyFileFixity like usual, it should automatically
detect the C/Cython compiled files and use that to speedup processing.</source>
          <target state="translated">3- You can now launch pyFileFixity like usual, it should automatically detect the C/Cython compiled files and use that to speedup processing.</target>
        </trans-unit>
        <trans-unit id="090d1eff544575030fa23c70c06c048f160e2096" translate="yes" xml:space="preserve">
          <source>4- If you used pyFileFixity before the failure of your storage media,
you can then use your pre-computed databases to check that files are
intact (rfigc.py) and if they aren&amp;rsquo;t, you can recover them (using
header_ecc.py and structural_adaptive_ecc.py). It can also help if
you recovered your files via data scraping, because your files will be
totally unorganized, but you can use a previously generated database
file to recover the full names and directory tree structure using
rfigc.py &amp;ndash;filescraping_recover.</source>
          <target state="translated">4- If you used pyFileFixity before the failure of your storage media, you can then use your pre-computed databases to check that files are intact (rfigc.py) and if they aren&amp;rsquo;t, you can recover them (using header_ecc.py and structural_adaptive_ecc.py). It can also help if you recovered your files via data scraping, because your files will be totally unorganized, but you can use a previously generated database file to recover the full names and directory tree structure using rfigc.py &amp;ndash;filescraping_recover.</target>
        </trans-unit>
        <trans-unit id="bf1594fd6828a85b88250b933c3a4fcac1bad842" translate="yes" xml:space="preserve">
          <source>: also use the second, fastest RS codec, but
with different parameters (US FAA ADSB UAT RS FEC norm),
thus the generated ECC won&amp;rsquo;t be compatible with algo 1 to 3.
But do not be scared, the ECC will work just the same.</source>
          <target state="translated">: also use the second, fastest RS codec, but with different parameters (US FAA ADSB UAT RS FEC norm), thus the generated ECC won&amp;rsquo;t be compatible with algo 1 to 3. But do not be scared, the ECC will work just the same.</target>
        </trans-unit>
        <trans-unit id="f352e276b02d7accc2ca2be49f5fd45f6f178ced" translate="yes" xml:space="preserve">
          <source>: same as algo 1 but with a faster functions.</source>
          <target state="translated">: same as algo 1 but with a faster functions.</target>
        </trans-unit>
        <trans-unit id="87b11990ae7aebfab737930c0cb75b491b5c5252" translate="yes" xml:space="preserve">
          <source>: use the first Reed-Solomon codec in galois field 2^8 of root 3 with fcr=1.
This is the slowest implementation (but also the most easy code to understand).</source>
          <target state="translated">: use the first Reed-Solomon codec in galois field 2^8 of root 3 with fcr=1. This is the slowest implementation (but also the most easy code to understand).</target>
        </trans-unit>
        <trans-unit id="29b38e501da943a7c5e6da0c117819ef3b2ce112" translate="yes" xml:space="preserve">
          <source>: use the second codec, which is the fastest.
The generated ECC will be compatible with algo 1 and 2.</source>
          <target state="translated">: use the second codec, which is the fastest. The generated ECC will be compatible with algo 1 and 2.</target>
        </trans-unit>
        <trans-unit id="7f731f6019735455d0474f29d3e26f86a732492f" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;http://dar.linux.free.fr/&quot;&gt;DAR (Disk ARchive)&lt;/a&gt;: similar to tar
but non-solid thus allows for partial recovery and per-file access,
plus it saves the directory tree meta-data &amp;ndash; see catalog isolation
&amp;ndash; plus it can handle error correction natively using PAR2 and
encryption. Also supports incremental backup, thus it&amp;rsquo;s a very nice
versatile tool. Crossplatform and opensource.</source>
          <target state="translated">&lt;a href=&quot;http://dar.linux.free.fr/&quot;&gt;DAR (Disk ARchive)&lt;/a&gt;: similar to tar but non-solid thus allows for partial recovery and per-file access, plus it saves the directory tree meta-data &amp;ndash; see catalog isolation &amp;ndash; plus it can handle error correction natively using PAR2 and encryption. Also supports incremental backup, thus it&amp;rsquo;s a very nice versatile tool. Crossplatform and opensource.</target>
        </trans-unit>
        <trans-unit id="f7d0a7def7f822cc277c371ee72e17f073b4ddca" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;http://dvdisaster.net/&quot;&gt;DVDisaster&lt;/a&gt;: error correction at the bit
level for optical mediums (CD, DVD and BD / BluRay Discs). Very good,
it also protects directory tree meta-data and is resilient to
corruption (v2 still has some critical spots but v3 won&amp;rsquo;t have any).</source>
          <target state="translated">&lt;a href=&quot;http://dvdisaster.net/&quot;&gt;DVDisaster&lt;/a&gt;: error correction at the bit level for optical mediums (CD, DVD and BD / BluRay Discs). Very good, it also protects directory tree meta-data and is resilient to corruption (v2 still has some critical spots but v3 won&amp;rsquo;t have any).</target>
        </trans-unit>
        <trans-unit id="d95af33d216cce8507d3d027d132fbf9fe9b09c0" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;http://lekythos.library.ucy.ac.cy/bitstream/handle/10797/13919/ECDL038.pdf?sequence=1&quot;&gt;&amp;ldquo;Just one bit in a million: On the effects of data corruption in files&amp;rdquo; by Volker Heydegger&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;http://lekythos.library.ucy.ac.cy/bitstream/handle/10797/13919/ECDL038.pdf?sequence=1&quot;&gt;&amp;ldquo;Just one bit in a million: On the effects of data corruption in files&amp;rdquo; by Volker Heydegger&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="f43f690690ba73b4d6883ab521ecd8738268b16f" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;http://old.hki.uni-koeln.de/people/herrmann/forschung/heydegger_archiving2008_40.pdf&quot;&gt;&amp;ldquo;Analysing the impact of file formats on data integrity&amp;rdquo; by Volker Heydegger&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;http://old.hki.uni-koeln.de/people/herrmann/forschung/heydegger_archiving2008_40.pdf&quot;&gt;&amp;ldquo;Analysing the impact of file formats on data integrity&amp;rdquo; by Volker Heydegger&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="93a3a2bc042c0957adcacf9fa45e71e004d0bf26" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;http://users.softlab.ntua.gr/~ttsiod/rsbep.html&quot;&gt;rsbep modification by Thanassis
Tsiodras&lt;/a&gt;:
enhanced rsbep to avoid critical spots and faster speed. Also
includes a &amp;ldquo;freeze&amp;rdquo; script to encode your files into a virtual
filesystem (using Python/FUSE) so that even meta-data such as
directory tree are fully protected by the ecc. Great script, but not
maintained, it needs some intensive testing by someone knowledgeable
to guarantee this script is reliable enough for production.</source>
          <target state="translated">&lt;a href=&quot;http://users.softlab.ntua.gr/~ttsiod/rsbep.html&quot;&gt;rsbep modification by Thanassis Tsiodras&lt;/a&gt;: enhanced rsbep to avoid critical spots and faster speed. Also includes a &amp;ldquo;freeze&amp;rdquo; script to encode your files into a virtual filesystem (using Python/FUSE) so that even meta-data such as directory tree are fully protected by the ecc. Great script, but not maintained, it needs some intensive testing by someone knowledgeable to guarantee this script is reliable enough for production.</target>
        </trans-unit>
        <trans-unit id="b8e067c82dc827aed9bb019275bad7c636e154bd" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;http://www.nationalarchives.gov.uk/documents/information-management/guide-to-formats.pdf&quot;&gt;&amp;ldquo;A guide to formats&amp;rdquo;, by The UK national archives&lt;/a&gt; (you want to look at the Recoverability entry in each table).</source>
          <target state="translated">&lt;a href=&quot;http://www.nationalarchives.gov.uk/documents/information-management/guide-to-formats.pdf&quot;&gt;&amp;ldquo;A guide to formats&amp;rdquo;, by The UK national archives&lt;/a&gt; (you want to look at the Recoverability entry in each table).</target>
        </trans-unit>
        <trans-unit id="44b7143298154e49b5891d71ef7544fa3ef9396b" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;https://en.wikipedia.org/wiki/BagIt&quot;&gt;BagIt&lt;/a&gt; with two python
implementations &lt;a href=&quot;https://pypi.python.org/pypi/pybagit/&quot;&gt;here&lt;/a&gt; and
&lt;a href=&quot;https://pypi.python.org/pypi/bagit/&quot;&gt;here&lt;/a&gt;: this is a file
packaging format for sharing and storing archives for long term
preservation, it just formalizes a few common procedures and meta
data that are usually added to files for long term archival (such as
MD5 digest).</source>
          <target state="translated">&lt;a href=&quot;https://en.wikipedia.org/wiki/BagIt&quot;&gt;BagIt&lt;/a&gt; with two python implementations &lt;a href=&quot;https://pypi.python.org/pypi/pybagit/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://pypi.python.org/pypi/bagit/&quot;&gt;here&lt;/a&gt;: this is a file packaging format for sharing and storing archives for long term preservation, it just formalizes a few common procedures and meta data that are usually added to files for long term archival (such as MD5 digest).</target>
        </trans-unit>
        <trans-unit id="4f2aae551721ce236d8d8f9f6cf4c120a113cc07" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;https://github.com/jap/rsarm&quot;&gt;RSArmor&lt;/a&gt; a tool based on
Reed-Solomon to encode binary data files into hexadecimal, so that
you can print the characters on paper. May be interesting for small
datasets (below 100 MB).</source>
          <target state="translated">&lt;a href=&quot;https://github.com/jap/rsarm&quot;&gt;RSArmor&lt;/a&gt; a tool based on Reed-Solomon to encode binary data files into hexadecimal, so that you can print the characters on paper. May be interesting for small datasets (below 100 MB).</target>
        </trans-unit>
        <trans-unit id="1b302d69ec20e4dca7478680982411dbfbf47cf4" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;https://github.com/jmoiron/par2ools&quot;&gt;par2ools&lt;/a&gt;: a set of
additional tools to manage par2 archives</source>
          <target state="translated">&lt;a href=&quot;https://github.com/jmoiron/par2ools&quot;&gt;par2ools&lt;/a&gt;: a set of additional tools to manage par2 archives</target>
        </trans-unit>
        <trans-unit id="c678312c6afa0ded5b07928ae18d600e24380496" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;https://github.com/lsauer/entropy&quot;&gt;Ent&lt;/a&gt; a tool to analyze the
entropy of your files. Can be very interesting to optimize the error
correction algorithm, or your compression tools.</source>
          <target state="translated">&lt;a href=&quot;https://github.com/lsauer/entropy&quot;&gt;Ent&lt;/a&gt; a tool to analyze the entropy of your files. Can be very interesting to optimize the error correction algorithm, or your compression tools.</target>
        </trans-unit>
        <trans-unit id="77b3347ae7a1345c884b66204c0c16546b87b6a3" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;https://pypi.python.org/pypi/Checkm/0.4&quot;&gt;Checkm&lt;/a&gt;: a tool similar
to rfigc.py</source>
          <target state="translated">&lt;a href=&quot;https://pypi.python.org/pypi/Checkm/0.4&quot;&gt;Checkm&lt;/a&gt;: a tool similar to rfigc.py</target>
        </trans-unit>
        <trans-unit id="33496691b15d6a019872d1bde20914b14c1ff32e" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;https://pypi.python.org/pypi/hashfs/&quot;&gt;HashFS&lt;/a&gt; is a non-redundant,
duplication free filesystem, in Python. &lt;strong&gt;Data deduplication&lt;/strong&gt; is very
important for large scale long term storage: since you want your data
to be redundant, this means you will use an additional storage space
for your redundant copies that will be proportional to your original data.
Having duplicated data will consume more storage and more processing
time, for no benefit. That&amp;rsquo;s why it&amp;rsquo;s a good idea to deduplicate your data
prior to create redundant copies: this will be faster and save you money.
Deduplication can either be done manually (by using duplicates removers)
or systematically and automatically using specific filesystems such as
zfs (with deduplication enabled) or hashfs.</source>
          <target state="translated">&lt;a href=&quot;https://pypi.python.org/pypi/hashfs/&quot;&gt;HashFS&lt;/a&gt; is a non-redundant, duplication free filesystem, in Python. &lt;strong&gt;Data deduplication&lt;/strong&gt; is very important for large scale long term storage: since you want your data to be redundant, this means you will use an additional storage space for your redundant copies that will be proportional to your original data. Having duplicated data will consume more storage and more processing time, for no benefit. That&amp;rsquo;s why it&amp;rsquo;s a good idea to deduplicate your data prior to create redundant copies: this will be faster and save you money. Deduplication can either be done manually (by using duplicates removers) or systematically and automatically using specific filesystems such as zfs (with deduplication enabled) or hashfs.</target>
        </trans-unit>
        <trans-unit id="3de5f75ef828687bd2bf55b3b5e0fbb11c4ee16e" translate="yes" xml:space="preserve">
          <source>A speedy Cython implementation of the Reed-Solomon library is included.
It should provide C-speed for all scripts (as long as you use
&amp;ndash;ecc_algo 1 or 2, not 3 nor 4). It is not needed, since a pure-python
implementation is used by default, but it can be useful if you want to
encode big datasets of several hundred of GB.</source>
          <target state="translated">A speedy Cython implementation of the Reed-Solomon library is included. It should provide C-speed for all scripts (as long as you use &amp;ndash;ecc_algo 1 or 2, not 3 nor 4). It is not needed, since a pure-python implementation is used by default, but it can be useful if you want to encode big datasets of several hundred of GB.</target>
        </trans-unit>
        <trans-unit id="d8184696134be0b1ae5269bf4614db3d25cd9f38" translate="yes" xml:space="preserve">
          <source>AVPreserve tools, most notably &lt;a href=&quot;https://github.com/avpreserve/fixity&quot;&gt;fixity&lt;/a&gt;
to monitor for file changes (similarly to rfigc, but actively as a daemon)
and &lt;a href=&quot;https://github.com/avpreserve/interstitial&quot;&gt;interstitial&lt;/a&gt; to detect
interstitial errors in audio digitization workflows (great to ensure you
correctly digitized a whole audio file into WAV without any error).</source>
          <target state="translated">AVPreserve tools, most notably &lt;a href=&quot;https://github.com/avpreserve/fixity&quot;&gt;fixity&lt;/a&gt; to monitor for file changes (similarly to rfigc, but actively as a daemon) and &lt;a href=&quot;https://github.com/avpreserve/interstitial&quot;&gt;interstitial&lt;/a&gt; to detect interstitial errors in audio digitization workflows (great to ensure you correctly digitized a whole audio file into WAV without any error).</target>
        </trans-unit>
        <trans-unit id="3573d95ce3acbb77bcc7cf574b5e3c081f0de1b3" translate="yes" xml:space="preserve">
          <source>Also, the ecc file specification is made to be simple and resilient to
corruption, so that you can process it by your own means if you want to,
without having to study for hours how the code works (contrary to PAR2
format).</source>
          <target state="translated">Also, the ecc file specification is made to be simple and resilient to corruption, so that you can process it by your own means if you want to, without having to study for hours how the code works (contrary to PAR2 format).</target>
        </trans-unit>
        <trans-unit id="4f071846248951c985c5dc186046b526162ac67f" translate="yes" xml:space="preserve">
          <source>Also, you can try to fix some of your files using specialized repairing
tools (but remember that such tool cannot guarantee you the same
recovering capacity as an error correction code - and in addition, error
correction code can tell you when it has recovered successfully). For
example:</source>
          <target state="translated">Also, you can try to fix some of your files using specialized repairing tools (but remember that such tool cannot guarantee you the same recovering capacity as an error correction code - and in addition, error correction code can tell you when it has recovered successfully). For example:</target>
        </trans-unit>
        <trans-unit id="e7a3d1d1d56d76d800867854902f3c712145474d" translate="yes" xml:space="preserve">
          <source>An interesting benefit of this approach is that it has a low storage
(and computational) overhead that scales linearly to the number of
files, whatever their size is: for example, if we have a set of 40k
files for a total size of 60 GB, with a resiliency_rate of 30% and
header_size of 1KB (we limit to the first 1K bytes/characters = our
file header), then, without counting the hash per block and other
meta-data, the final ECC file will be about 2 * resiliency_rate *
number_of_files * header_size = 24.5 MB. This size can be lower if
there are many files smaller than 1KB. This is a pretty low storage
overhead to backup the headers of such a big number of files.</source>
          <target state="translated">An interesting benefit of this approach is that it has a low storage (and computational) overhead that scales linearly to the number of files, whatever their size is: for example, if we have a set of 40k files for a total size of 60 GB, with a resiliency_rate of 30% and header_size of 1KB (we limit to the first 1K bytes/characters = our file header), then, without counting the hash per block and other meta-data, the final ECC file will be about 2 * resiliency_rate * number_of_files * header_size = 24.5 MB. This size can be lower if there are many files smaller than 1KB. This is a pretty low storage overhead to backup the headers of such a big number of files.</target>
        </trans-unit>
        <trans-unit id="99ce38575cddf06a9980f9abecd645b0ef832da4" translate="yes" xml:space="preserve">
          <source>Applications included</source>
          <target state="translated">Applications included</target>
        </trans-unit>
        <trans-unit id="cbb9fa252e60809efa55a7ad83aea5438ef56753" translate="yes" xml:space="preserve">
          <source>Arguments</source>
          <target state="translated">인수</target>
        </trans-unit>
        <trans-unit id="3545db4e0fe9252629af9bf094d6372ea9ce1b48" translate="yes" xml:space="preserve">
          <source>As a rule of thumb, you should ALWAYS keep your ecc file in clear
text, so under no compression nor encryption. This is because in case
the ecc file gets corrupted, if compressed/encrypted, the
decompression/decrypting of the corrupted parts may completely flaw
the whole structure of the ecc file.</source>
          <target state="translated">As a rule of thumb, you should ALWAYS keep your ecc file in clear text, so under no compression nor encryption. This is because in case the ecc file gets corrupted, if compressed/encrypted, the decompression/decrypting of the corrupted parts may completely flaw the whole structure of the ecc file.</target>
        </trans-unit>
        <trans-unit id="abc7da7ecbc04b9172ccf3d74056ab0f93fefdb2" translate="yes" xml:space="preserve">
          <source>At the center, the same image but
with a few symbols corrupted (only 3 in header and 2 in the rest of the file,
which equals to 5 bytes corrupted in total, over 19KB which is the total file size).
Only a few corrupted bytes are enough to make the image looks like totally
unrecoverable, and yet we are lucky, because the image could be unreadable at all
if any of the &amp;ldquo;magic bytes&amp;rdquo; were to be corrupted!</source>
          <target state="translated">At the center, the same image but with a few symbols corrupted (only 3 in header and 2 in the rest of the file, which equals to 5 bytes corrupted in total, over 19KB which is the total file size). Only a few corrupted bytes are enough to make the image looks like totally unrecoverable, and yet we are lucky, because the image could be unreadable at all if any of the &amp;ldquo;magic bytes&amp;rdquo; were to be corrupted!</target>
        </trans-unit>
        <trans-unit id="82820a5063138e5ed1df6036c3cd747b078aa878" translate="yes" xml:space="preserve">
          <source>At the right, the corrupted image was repaired using</source>
          <target state="translated">At the right, the corrupted image was repaired using</target>
        </trans-unit>
        <trans-unit id="00f0bcb4745e38ed51a7ecd3da3754abb931784b" translate="yes" xml:space="preserve">
          <source>Can I compress my data files and my ecc file?</source>
          <target state="translated">Can I compress my data files and my ecc file?</target>
        </trans-unit>
        <trans-unit id="31722d2c07b1a8b04a679021386b5b0c17cdf3fd" translate="yes" xml:space="preserve">
          <source>Can I encrypt my data files and my ecc file ?</source>
          <target state="translated">Can I encrypt my data files and my ecc file ?</target>
        </trans-unit>
        <trans-unit id="eaf88ad2fce96d92cb92aabbfc80d4ee724288f3" translate="yes" xml:space="preserve">
          <source>Can only repair errors and erasures (characters that are replaced by
another character), not deletion nor insertion of characters. However
this should not happen with any storage medium (truncation can occur
if the file bounds is misdetected, in this case pyFileFixity can
partially repair the known parts of the file, but cannot recover the
rest past the truncation, except if you used a resiliency rate of at
least 0.5, in which case any message block can be recreated with only
using the ecc file).</source>
          <target state="translated">Can only repair errors and erasures (characters that are replaced by another character), not deletion nor insertion of characters. However this should not happen with any storage medium (truncation can occur if the file bounds is misdetected, in this case pyFileFixity can partially repair the known parts of the file, but cannot recover the rest past the truncation, except if you used a resiliency rate of at least 0.5, in which case any message block can be recreated with only using the ecc file).</target>
        </trans-unit>
        <trans-unit id="502405462535d0135ba51679b50a9aae30b336d6" translate="yes" xml:space="preserve">
          <source>Cannot protect meta-data, such as folders paths. The paths are
stored, but cannot be recovered (yet? feel free to contribute if you
know how). Only files are protected. Thus if your OS or your storage
medium crashes and truncate a whole directory tree, the directory
tree can&amp;rsquo;t be repaired using the ecc file, and thus you can&amp;rsquo;t access
the files neither. However, you can use file scraping to extract the
files even if the directory tree is lost, and then use RFIGC.py to
reorganize your files correctly. There are alternatives, see the
chapters below: you can either package all your files in a single
archive using DAR or ZIP (thus the ecc will also protect meta-data), or see
DVDisaster as an alternative solution, which is an ecc generator with
support for directory trees meta-data (but only on optical disks).</source>
          <target state="translated">Cannot protect meta-data, such as folders paths. The paths are stored, but cannot be recovered (yet? feel free to contribute if you know how). Only files are protected. Thus if your OS or your storage medium crashes and truncate a whole directory tree, the directory tree can&amp;rsquo;t be repaired using the ecc file, and thus you can&amp;rsquo;t access the files neither. However, you can use file scraping to extract the files even if the directory tree is lost, and then use RFIGC.py to reorganize your files correctly. There are alternatives, see the chapters below: you can either package all your files in a single archive using DAR or ZIP (thus the ecc will also protect meta-data), or see DVDisaster as an alternative solution, which is an ecc generator with support for directory trees meta-data (but only on optical disks).</target>
        </trans-unit>
        <trans-unit id="6d6b53a15a5e0c8dc4e370e626c97383f586446f" translate="yes" xml:space="preserve">
          <source>Cannot recreate a missing file from other available files (except you
have set a resilience_rate at least 0.5), contrary to Parchives
(PAR1/PAR2). Thus, you can only repair a file if you still have it on
your filesystem. If it&amp;rsquo;s missing, pyFileFixity cannot do anything
(yet, this will be implemented in the future).</source>
          <target state="translated">Cannot recreate a missing file from other available files (except you have set a resilience_rate at least 0.5), contrary to Parchives (PAR1/PAR2). Thus, you can only repair a file if you still have it on your filesystem. If it&amp;rsquo;s missing, pyFileFixity cannot do anything (yet, this will be implemented in the future).</target>
        </trans-unit>
        <trans-unit id="afc2b6eb93654fed24d8cce3141c211e0dfdd6a4" translate="yes" xml:space="preserve">
          <source>Cons:</source>
          <target state="translated">Cons:</target>
        </trans-unit>
        <trans-unit id="e8f08b8b4eb006d73b76f99ef32752834afc2eba" translate="yes" xml:space="preserve">
          <source>Cython implementation</source>
          <target state="translated">Cython implementation</target>
        </trans-unit>
        <trans-unit id="3a51f5f66a1ec40c2779c2f0352384b1d228eb4c" translate="yes" xml:space="preserve">
          <source>Display the predicted total ecc file size given your parameters,
and the total time it will take to encode/decode.</source>
          <target state="translated">Display the predicted total ecc file size given your parameters, and the total time it will take to encode/decode.</target>
        </trans-unit>
        <trans-unit id="ad454cbd9b77f21405cde50382fe9ba2a4f52280" translate="yes" xml:space="preserve">
          <source>ECC Algorithms</source>
          <target state="translated">ECC Algorithms</target>
        </trans-unit>
        <trans-unit id="0dbb4771d15baf161aad45c2b93955833eaef88e" translate="yes" xml:space="preserve">
          <source>Encryption: technically, you can encrypt your files without losing
too much redundancy, as long as you use an encryption scheme that is
block-based such as DES: if one block gets corrupted, it won&amp;rsquo;t be
decryptable, but the rest of the files&amp;rsquo; encrypted blocks should be
decryptable without any problem. So encrypting with such algorithms
leads to similar files as non-solid archives such as deflate zip. Of
course, for very long term storage, it&amp;rsquo;s better to avoid encryption
and compression (because you raise the information contained in a
single block of data, thus if you lose one block, you lose more
data), but if it&amp;rsquo;s really necessary to you, you can still maintain
high chances of recovering your files by using block-based
encryption/compression (note: block-based encryption can
be seen as the equivalent of non-solid archives for compression,
because the data is compressed/encrypted in independent blocks,
thus allowing partial uncompression/decryption).</source>
          <target state="translated">Encryption: technically, you can encrypt your files without losing too much redundancy, as long as you use an encryption scheme that is block-based such as DES: if one block gets corrupted, it won&amp;rsquo;t be decryptable, but the rest of the files&amp;rsquo; encrypted blocks should be decryptable without any problem. So encrypting with such algorithms leads to similar files as non-solid archives such as deflate zip. Of course, for very long term storage, it&amp;rsquo;s better to avoid encryption and compression (because you raise the information contained in a single block of data, thus if you lose one block, you lose more data), but if it&amp;rsquo;s really necessary to you, you can still maintain high chances of recovering your files by using block-based encryption/compression (note: block-based encryption can be seen as the equivalent of non-solid archives for compression, because the data is compressed/encrypted in independent blocks, thus allowing partial uncompression/decryption).</target>
        </trans-unit>
        <trans-unit id="5382504d5fdc4388a257e3d7408482b824efc7c7" translate="yes" xml:space="preserve">
          <source>Error correction can seem a bit magical, but for a reasonable intuition,
it can be seen as a way to average the corruption error rate: on
average, a bit will still have the same chance to be corrupted, but
since you have more bits to represent the same data, you lower the
overall chance to lose this bit.</source>
          <target state="translated">Error correction can seem a bit magical, but for a reasonable intuition, it can be seen as a way to average the corruption error rate: on average, a bit will still have the same chance to be corrupted, but since you have more bits to represent the same data, you lower the overall chance to lose this bit.</target>
        </trans-unit>
        <trans-unit id="861245b8779c3c0a240081e66f72046983ca7d16" translate="yes" xml:space="preserve">
          <source>Example usage</source>
          <target state="translated">사용법 예</target>
        </trans-unit>
        <trans-unit id="03688ba6aa340b87549088aa5739944cb6b1dc73" translate="yes" xml:space="preserve">
          <source>FAQ</source>
          <target state="translated">자주하는 질문</target>
        </trans-unit>
        <trans-unit id="ee6812e77b96d0db961bcc413b40a35bd8e57b5c" translate="yes" xml:space="preserve">
          <source>For the moment, only Reed-Solomon is implemented, but it&amp;rsquo;s universal
so you can modify its parameters in lib/eccman.py.</source>
          <target state="translated">For the moment, only Reed-Solomon is implemented, but it&amp;rsquo;s universal so you can modify its parameters in lib/eccman.py.</target>
        </trans-unit>
        <trans-unit id="258d44dadd857946a76d6c1a4a380bbbaf615778" translate="yes" xml:space="preserve">
          <source>For users: what&amp;rsquo;s the advantage of pyFileFixity?</source>
          <target state="translated">For users: what&amp;rsquo;s the advantage of pyFileFixity?</target>
        </trans-unit>
        <trans-unit id="a8328cd432f674ca9d7edfd621c2fe314dc40926" translate="yes" xml:space="preserve">
          <source>From scientific studies, it seems that, at the time of writing this (2015),
BluRay HTL disks are the most resilient against environmental degradation.
To raise the duration, you can also put optical disks in completely opaque boxes
(to avoid light degradation) and in addition you can put any storage medium
(not only optical disks, but also hard drives and anything really) in
&lt;em&gt;completely&lt;/em&gt; air-tight and water-tight bags or box and put in a fridge or a freezer.
This is a law of nature: lower the temperature, lower will be the entropy, in other
words lower will be the degradation over time. It works the same with digital data.</source>
          <target state="translated">From scientific studies, it seems that, at the time of writing this (2015), BluRay HTL disks are the most resilient against environmental degradation. To raise the duration, you can also put optical disks in completely opaque boxes (to avoid light degradation) and in addition you can put any storage medium (not only optical disks, but also hard drives and anything really) in &lt;em&gt;completely&lt;/em&gt; air-tight and water-tight bags or box and put in a fridge or a freezer. This is a law of nature: lower the temperature, lower will be the entropy, in other words lower will be the degradation over time. It works the same with digital data.</target>
        </trans-unit>
        <trans-unit id="8c218bf7ccde41c9cef7fdbb1b87026f7aa54940" translate="yes" xml:space="preserve">
          <source>Furthermore, the currently designed format of the ecc file would allow
two things that are not available in all current file ecc generators
such as PAR2:</source>
          <target state="translated">Furthermore, the currently designed format of the ecc file would allow two things that are not available in all current file ecc generators such as PAR2:</target>
        </trans-unit>
        <trans-unit id="9a33cbdc02c26e1f688012b1833f13e363eee1ff" translate="yes" xml:space="preserve">
          <source>Header Error Correction Code script</source>
          <target state="translated">Header Error Correction Code script</target>
        </trans-unit>
        <trans-unit id="ed3deb7aadd16d1ae676ca8408a2a5fe3ff01e96" translate="yes" xml:space="preserve">
          <source>Helping file fixity (long term storage of data) via redundant error correcting codes and hash auditing.</source>
          <target state="translated">Helping file fixity (long term storage of data) via redundant error correcting codes and hash auditing.</target>
        </trans-unit>
        <trans-unit id="65957edc8ea850de1db8da51756b103c4e5a353e" translate="yes" xml:space="preserve">
          <source>Here are some tools with a similar philosophy to pyFileFixity, which you
can use if they better fit your needs, either as a replacement of
pyFileFixity or as a complement (pyFileFixity can always be used to
generate an ecc file):</source>
          <target state="translated">Here are some tools with a similar philosophy to pyFileFixity, which you can use if they better fit your needs, either as a replacement of pyFileFixity or as a complement (pyFileFixity can always be used to generate an ecc file):</target>
        </trans-unit>
        <trans-unit id="ea02f5a9493946777069cc920f99bccb9c0a1f61" translate="yes" xml:space="preserve">
          <source>Here is an example of what pyFileFixity can do:</source>
          <target state="translated">Here is an example of what pyFileFixity can do:</target>
        </trans-unit>
        <trans-unit id="e79248087c2c9247c08e22a7c81de45fab7841e0" translate="yes" xml:space="preserve">
          <source>Here the portability principle of pyFileFixity prevents this approach.
But you can mimic this workaround on your hard drive for pyFileFixity to
work: you just need to package all your files into one file. This way,
you sort of create a virtual file system: inside the archive, files and
directories have meta-data just like in a filesystem, but from the
outside it&amp;rsquo;s just one file, composed of bytes that we can just encode to
generate an ecc file - in other words, we removed the inodes portability
problem, since this meta-data is stored relatively inside the archive,
the archive manage it, and we can just encode this info like any other
stream of data! The usual way to make an archive from several files is
to use TAR, but this will generate a solid archive which will prevent
partial recovery. An alternative is to use DAR, which is a non-solid
archive version of TAR, with lots of other features too. If you also
want to compress, you can just use ZIP (with DEFLATE algorithm) your
files (this also generates a non-solid archive). You can then use
pyFileFixity to generate an ecc file on your DAR or ZIP archive, which
will then protect both your files just like before and the directories
meta-data too now.</source>
          <target state="translated">Here the portability principle of pyFileFixity prevents this approach. But you can mimic this workaround on your hard drive for pyFileFixity to work: you just need to package all your files into one file. This way, you sort of create a virtual file system: inside the archive, files and directories have meta-data just like in a filesystem, but from the outside it&amp;rsquo;s just one file, composed of bytes that we can just encode to generate an ecc file - in other words, we removed the inodes portability problem, since this meta-data is stored relatively inside the archive, the archive manage it, and we can just encode this info like any other stream of data! The usual way to make an archive from several files is to use TAR, but this will generate a solid archive which will prevent partial recovery. An alternative is to use DAR, which is a non-solid archive version of TAR, with lots of other features too. If you also want to compress, you can just use ZIP (with DEFLATE algorithm) your files (this also generates a non-solid archive). You can then use pyFileFixity to generate an ecc file on your DAR or ZIP archive, which will then protect both your files just like before and the directories meta-data too now.</target>
        </trans-unit>
        <trans-unit id="05b76283f7c2f2680b2ffd37266317638fcdaaaa" translate="yes" xml:space="preserve">
          <source>Highly reliable file fixity watcher: rfigc.py will tell you without
any ambiguity using several attributes if your files have been
corrupted or not, and can even check for images if the header is
valid (ie: if the file can still be opened).</source>
          <target state="translated">Highly reliable file fixity watcher: rfigc.py will tell you without any ambiguity using several attributes if your files have been corrupted or not, and can even check for images if the header is valid (ie: if the file can still be opened).</target>
        </trans-unit>
        <trans-unit id="e6c5500894326673b9a71b7a14cc0d3adce3556c" translate="yes" xml:space="preserve">
          <source>Highly resilient ecc file format against corruption (not only are
your data protected by ecc, the ecc file is protected too against
critical spots, both because there is no header so that each track is
independent and if one track is corrupted beyond repair then other
ecc tracks can still be read, and a .idx file will be generated to
repair the structure of the ecc file to recover all tracks).</source>
          <target state="translated">Highly resilient ecc file format against corruption (not only are your data protected by ecc, the ecc file is protected too against critical spots, both because there is no header so that each track is independent and if one track is corrupted beyond repair then other ecc tracks can still be read, and a .idx file will be generated to repair the structure of the ecc file to recover all tracks).</target>
        </trans-unit>
        <trans-unit id="b948329aac21e442a38bc63760b962b733145008" translate="yes" xml:space="preserve">
          <source>However, in the case that you compress your files, you should generate
the ecc file only &lt;em&gt;after&lt;/em&gt; compression, so that the ecc file applies to
the compressed archive instead of the uncompressed files, else you
risk being unable to correct your files because the uncompression of
corrupted parts may output gibberish, and length extended corrupted
parts (and if the size is different, Reed-Solomon will just freak
out).</source>
          <target state="translated">However, in the case that you compress your files, you should generate the ecc file only &lt;em&gt;after&lt;/em&gt; compression, so that the ecc file applies to the compressed archive instead of the uncompressed files, else you risk being unable to correct your files because the uncompression of corrupted parts may output gibberish, and length extended corrupted parts (and if the size is different, Reed-Solomon will just freak out).</target>
        </trans-unit>
        <trans-unit id="345093f4d93c93504a63074f1539fb601878c7d8" translate="yes" xml:space="preserve">
          <source>IMPORTANT: it is CRITICAL that you use the same parameters for
correcting mode as when you generated the database/ecc files (this is
true for all scripts in this bundle). Of course, some options must be
changed: -g must become -c to correct, and &amp;ndash;update is a particular
case. This works this way on purpose for mainly two reasons: first
because it is very hard to autodetect the parameters from a database
file alone and it would produce lots of false positives, and secondly
(the primary reason) is that storing parameters inside the database file
is highly unresilient against corruption (if this part of the database
is tampered, the whole becomes unreadable, while if they are stored
outside or in your own memory, the database file is always accessible).
Thus, it is advised to write down the parameters you used to generate
your database directly on the storage media you will store your database
file on (eg: if it&amp;rsquo;s an optical disk, write the parameters on the cover
or directly on the disk using a marker), or better memorize them by
heart. If you forget them, don&amp;rsquo;t panic, the parameters are always stored
as comments in the header of the generated ecc files, but you should try
to store them outside of the ecc files anyway.</source>
          <target state="translated">IMPORTANT: it is CRITICAL that you use the same parameters for correcting mode as when you generated the database/ecc files (this is true for all scripts in this bundle). Of course, some options must be changed: -g must become -c to correct, and &amp;ndash;update is a particular case. This works this way on purpose for mainly two reasons: first because it is very hard to autodetect the parameters from a database file alone and it would produce lots of false positives, and secondly (the primary reason) is that storing parameters inside the database file is highly unresilient against corruption (if this part of the database is tampered, the whole becomes unreadable, while if they are stored outside or in your own memory, the database file is always accessible). Thus, it is advised to write down the parameters you used to generate your database directly on the storage media you will store your database file on (eg: if it&amp;rsquo;s an optical disk, write the parameters on the cover or directly on the disk using a marker), or better memorize them by heart. If you forget them, don&amp;rsquo;t panic, the parameters are always stored as comments in the header of the generated ecc files, but you should try to store them outside of the ecc files anyway.</target>
        </trans-unit>
        <trans-unit id="5732c3cd9252a13b6f4d2285ad958fb74542b69c" translate="yes" xml:space="preserve">
          <source>If everything goes alright, the C compiler will compile the .c files
(that were pre-generated by Cython) and you can then use PyFileFixity
scripts just as usual and you should see a huge speedup. Else, if it
doesn&amp;rsquo;t work, you might need to generate .c files using Cython for your
platform (because the pre-generated .c files may be incompatible with
your platform). To do that, you just need to install Cython, which is an
easy task with nowadays Python distributions such as Anaconda: download
32-bit Anaconda installer (on Windows you should avoid the 64-bit, it
may produce weird issues with Cython), then after install, open the
Anaconda Command Prompt and execute:</source>
          <target state="translated">If everything goes alright, the C compiler will compile the .c files (that were pre-generated by Cython) and you can then use PyFileFixity scripts just as usual and you should see a huge speedup. Else, if it doesn&amp;rsquo;t work, you might need to generate .c files using Cython for your platform (because the pre-generated .c files may be incompatible with your platform). To do that, you just need to install Cython, which is an easy task with nowadays Python distributions such as Anaconda: download 32-bit Anaconda installer (on Windows you should avoid the 64-bit, it may produce weird issues with Cython), then after install, open the Anaconda Command Prompt and execute:</target>
        </trans-unit>
        <trans-unit id="c85f5b94d961a0319339e3d28192dca27c6300ec" translate="yes" xml:space="preserve">
          <source>If you get issues, you can see the following post on how to install
Cython:</source>
          <target state="translated">If you get issues, you can see the following post on how to install Cython:</target>
        </trans-unit>
        <trans-unit id="118d36fc5362d1167fa69ebda9b24da9afa602e0" translate="yes" xml:space="preserve">
          <source>If you have any question about Reed-Solomon codes, the best place to
ask is probably here (with the incredible Dilip Sarwate):
&lt;a href=&quot;http://www.dsprelated.com/groups/comp.dsp/1.php?searchfor=reed%20solomon&quot;&gt;http://www.dsprelated.com/groups/comp.dsp/1.php?searchfor=reed%20solomon&lt;/a&gt;</source>
          <target state="translated">If you have any question about Reed-Solomon codes, the best place to ask is probably here (with the incredible Dilip Sarwate): &lt;a href=&quot;http://www.dsprelated.com/groups/comp.dsp/1.php?searchfor=reed%20solomon&quot;&gt;http://www.dsprelated.com/groups/comp.dsp/1.php?searchfor=reed%20solomon&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="36f790b1f7b781a839a1d3c69d986ad6639475b8" translate="yes" xml:space="preserve">
          <source>If you have previously generated a rfigc database, you can use it to enhance the replication repair:</source>
          <target state="translated">If you have previously generated a rfigc database, you can use it to enhance the replication repair:</target>
        </trans-unit>
        <trans-unit id="c3c5dce87f5e9e056d4231c2f49142bddd72b012" translate="yes" xml:space="preserve">
          <source>If you want to build the C/Cython implementation, do the following:</source>
          <target state="translated">If you want to build the C/Cython implementation, do the following:</target>
        </trans-unit>
        <trans-unit id="2ffc49e2faad61eb1a515898dc7639e4ee07416c" translate="yes" xml:space="preserve">
          <source>In any case, RAID cannot detect silent errors automatically, thus you
either have to regularly scan, or you risk to lose some of your data
permanently, and it&amp;rsquo;s far more common than you can expect (eg, with
RAID5, it is enough to have 2 silent errors on two disks on the same
bit for the bit to be unrecoverable). That&amp;rsquo;s why a limit of only 1 or
2 disks failures is just not enough.</source>
          <target state="translated">In any case, RAID cannot detect silent errors automatically, thus you either have to regularly scan, or you risk to lose some of your data permanently, and it&amp;rsquo;s far more common than you can expect (eg, with RAID5, it is enough to have 2 silent errors on two disks on the same bit for the bit to be unrecoverable). That&amp;rsquo;s why a limit of only 1 or 2 disks failures is just not enough.</target>
        </trans-unit>
        <trans-unit id="bf922fbce959e0493b68f07c11c4d2d9f215f3ec" translate="yes" xml:space="preserve">
          <source>In case of a catastrophic event</source>
          <target state="translated">In case of a catastrophic event</target>
        </trans-unit>
        <trans-unit id="a8bc448d8166bfa21a8d43a03f058d3f5e1d0880" translate="yes" xml:space="preserve">
          <source>In case of a catastrophic event of your data due to the failure of your
storage media (eg: your hard drive crashed), then follow the following
steps:</source>
          <target state="translated">In case of a catastrophic event of your data due to the failure of your storage media (eg: your hard drive crashed), then follow the following steps:</target>
        </trans-unit>
        <trans-unit id="aa568f453990618ebeb7aba06f297940994efa31" translate="yes" xml:space="preserve">
          <source>In the end, it&amp;rsquo;s a lot better to just separate the storage medium of
data, with the reading instrument. The medium I advise is optical disks
(whether it&amp;rsquo;s BluRay, DVD, CD or whatever), because the reading
instrument is separate, and the technology (laser reflecting on bumps
and/or pits) is kind of universal, so that even if the technology is
lost one day (deprecated by newer technologies, so that you can&amp;rsquo;t find
the reading instrument anymore because it&amp;rsquo;s not sold anymore), you can
probably emulate a laser using some software to read your optical disk,
just like what the CAMiLEON project did to recover data from the
LaserDiscs of the BBC Domesday Project (see Wikipedia).</source>
          <target state="translated">In the end, it&amp;rsquo;s a lot better to just separate the storage medium of data, with the reading instrument. The medium I advise is optical disks (whether it&amp;rsquo;s BluRay, DVD, CD or whatever), because the reading instrument is separate, and the technology (laser reflecting on bumps and/or pits) is kind of universal, so that even if the technology is lost one day (deprecated by newer technologies, so that you can&amp;rsquo;t find the reading instrument anymore because it&amp;rsquo;s not sold anymore), you can probably emulate a laser using some software to read your optical disk, just like what the CAMiLEON project did to recover data from the LaserDiscs of the BBC Domesday Project (see Wikipedia).</target>
        </trans-unit>
        <trans-unit id="7325f8dcf62491b791026751280302fbdf6947ff" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s difficult to advise a specific format. What we can do is advise the characteristics
of a good file format:</source>
          <target state="translated">It&amp;rsquo;s difficult to advise a specific format. What we can do is advise the characteristics of a good file format:</target>
        </trans-unit>
        <trans-unit id="1fc7832a8e139d33799cca03bcc0843c8c5fa83b" translate="yes" xml:space="preserve">
          <source>Long term storage is thus a very difficult topic: it&amp;rsquo;s like fighting with
death (in this case, the death of data). Indeed, because of entropy,
data will eventually fade away because of various silent errors such as
bit rot. pyFileFixity aims to provide tools to detect any data
corruption, but also fight data corruption by providing repairing tools.</source>
          <target state="translated">Long term storage is thus a very difficult topic: it&amp;rsquo;s like fighting with death (in this case, the death of data). Indeed, because of entropy, data will eventually fade away because of various silent errors such as bit rot. pyFileFixity aims to provide tools to detect any data corruption, but also fight data corruption by providing repairing tools.</target>
        </trans-unit>
        <trans-unit id="396c12246a0985124cc26b048f245e3dd8f1a512" translate="yes" xml:space="preserve">
          <source>NEVER encrypt your ecc file, this is totally useless and
counterproductive.</source>
          <target state="translated">NEVER encrypt your ecc file, this is totally useless and counterproductive.</target>
        </trans-unit>
        <trans-unit id="b4c0cfb7f98bb8e5feff894bb3737a5cb6beccb4" translate="yes" xml:space="preserve">
          <source>No external library needed, only native Python 2.7.x (but with PyPy
it will be way faster!).</source>
          <target state="translated">No external library needed, only native Python 2.7.x (but with PyPy it will be way faster!).</target>
        </trans-unit>
        <trans-unit id="7fe73c5e6199a332e66a4749e829d9ef82477340" translate="yes" xml:space="preserve">
          <source>No limit on the number of files, and it can recursively protect files
in a directory tree.</source>
          <target state="translated">No limit on the number of files, and it can recursively protect files in a directory tree.</target>
        </trans-unit>
        <trans-unit id="bcff9708573609f158488a6d07424a948bfb5a75" translate="yes" xml:space="preserve">
          <source>No magic bytes or header importance (ie, corrupting the header won&amp;rsquo;t prevent opening the file).</source>
          <target state="translated">No magic bytes or header importance (ie, corrupting the header won&amp;rsquo;t prevent opening the file).</target>
        </trans-unit>
        <trans-unit id="91d557fd94fd70a444541c6700af2bb42b7b1f12" translate="yes" xml:space="preserve">
          <source>Note about speed: Also, use a smaller &amp;ndash;max_block_size to greatly
speedup the operations! That&amp;rsquo;s the trick used to compute very quickly RS
ECC on optical discs. You give up a bit of resiliency of course (because
blocks are smaller, thus you protect a smaller number of characters per
ECC. In the end, this should not change much about real resiliency, but
in case you get a big bit error burst on a contiguous block, you may
lose a whole block at once. That&amp;rsquo;s why using RS255 is better, but it&amp;rsquo;s
very time consuming. However, the resiliency ratios still hold, so for
any other case of bit-flipping with average-sized bursts, this should
not be a problem as long as the size of the bursts is smaller than an
ecc block.)</source>
          <target state="translated">Note about speed: Also, use a smaller &amp;ndash;max_block_size to greatly speedup the operations! That&amp;rsquo;s the trick used to compute very quickly RS ECC on optical discs. You give up a bit of resiliency of course (because blocks are smaller, thus you protect a smaller number of characters per ECC. In the end, this should not change much about real resiliency, but in case you get a big bit error burst on a contiguous block, you may lose a whole block at once. That&amp;rsquo;s why using RS255 is better, but it&amp;rsquo;s very time consuming. However, the resiliency ratios still hold, so for any other case of bit-flipping with average-sized bursts, this should not be a problem as long as the size of the bursts is smaller than an ecc block.)</target>
        </trans-unit>
        <trans-unit id="525b5b14e8f28ce3523421093d75b410e49d1b4a" translate="yes" xml:space="preserve">
          <source>Note that all tools are primarily made for command-line usage (type
script.py &amp;ndash;help to get extended info about the accepted arguments), but
you can also use rfigc.py and header_ecc.py with a GUI by using the
&amp;ndash;gui argument (must be the first and only one argument supplied). The
GUI is provided as-is and minimal work will be done to maintain it (the
focus will stay on functionality rather than ergonomy).</source>
          <target state="translated">Note that all tools are primarily made for command-line usage (type script.py &amp;ndash;help to get extended info about the accepted arguments), but you can also use rfigc.py and header_ecc.py with a GUI by using the &amp;ndash;gui argument (must be the first and only one argument supplied). The GUI is provided as-is and minimal work will be done to maintain it (the focus will stay on functionality rather than ergonomy).</target>
        </trans-unit>
        <trans-unit id="76dd32271065d86254a8017542338726d029d636" translate="yes" xml:space="preserve">
          <source>Note that by default, the script is by default in check mode, to avoid
wrong manipulations. It will also alert you if you generate over an
already existing database file.</source>
          <target state="translated">Note that by default, the script is by default in check mode, to avoid wrong manipulations. It will also alert you if you generate over an already existing database file.</target>
        </trans-unit>
        <trans-unit id="c9e1939cac4e2d80ba4c47aa59a302a04f09399f" translate="yes" xml:space="preserve">
          <source>Note that the tools were meant for data archival (protect files that you
won&amp;rsquo;t modify anymore), not for system&amp;rsquo;s files watching nor to protect
all the files on your computer. To do this, you can use a filesystem
that directly integrate error correction code capacity, such as ZFS.</source>
          <target state="translated">Note that the tools were meant for data archival (protect files that you won&amp;rsquo;t modify anymore), not for system&amp;rsquo;s files watching nor to protect all the files on your computer. To do this, you can use a filesystem that directly integrate error correction code capacity, such as ZFS.</target>
        </trans-unit>
        <trans-unit id="b2a7c0faef43c2bee6a497592aaa4bfe91bb7cc3" translate="yes" xml:space="preserve">
          <source>Note: this also works for a single file, just replace &amp;ldquo;your_folder&amp;rdquo; by &amp;ldquo;your_file.ext&amp;rdquo;.</source>
          <target state="translated">Note: this also works for a single file, just replace &amp;ldquo;your_folder&amp;rdquo; by &amp;ldquo;your_file.ext&amp;rdquo;.</target>
        </trans-unit>
        <trans-unit id="710e92f05351fbfd96e871bb4a3560814dd2ca50" translate="yes" xml:space="preserve">
          <source>Of course, you can also protect the whole file, not only the header, using pyFileFixity&amp;rsquo;s</source>
          <target state="translated">Of course, you can also protect the whole file, not only the header, using pyFileFixity&amp;rsquo;s</target>
        </trans-unit>
        <trans-unit id="65362b551dad8d116e4d18b8924209590f90d87e" translate="yes" xml:space="preserve">
          <source>Of course, you can set the resiliency rate for each stage to the values
you want, so that you can even do the opposite: setting a higher
resiliency rate for stage 3 than stage 2 will produce an ecc that is
greater towards the end of the contents of your files.</source>
          <target state="translated">Of course, you can set the resiliency rate for each stage to the values you want, so that you can even do the opposite: setting a higher resiliency rate for stage 3 than stage 2 will produce an ecc that is greater towards the end of the contents of your files.</target>
        </trans-unit>
        <trans-unit id="c332fe763d4b23e163fb5c3abcdc3cf295ba7c00" translate="yes" xml:space="preserve">
          <source>On the left, this is the original image.</source>
          <target state="translated">On the left, this is the original image.</target>
        </trans-unit>
        <trans-unit id="67f496b39219ae5ba263902bc0a27d21f8cd5eb2" translate="yes" xml:space="preserve">
          <source>On the opposite, ECC can correct n-k disks (or files). You can configure
n and k however you want, so that for example you can set k = n/2, which
means that you can recover all your files from only half of them! (once
they are encoded with an ecc file of course).</source>
          <target state="translated">On the opposite, ECC can correct n-k disks (or files). You can configure n and k however you want, so that for example you can set k = n/2, which means that you can recover all your files from only half of them! (once they are encoded with an ecc file of course).</target>
        </trans-unit>
        <trans-unit id="0f845fb91583d4a25173551fa5589b4d07fd1605" translate="yes" xml:space="preserve">
          <source>One main current limitation of pyFileFixity is that it cannot protect
the directory tree meta-data. This means that in the worst case, if a
silent error happens on the inode pointing to the root directory that
you protected with an ecc, the whole directory will vanish, and all the
files inside too. In less worst cases, sub-directories can vanish, but
it&amp;rsquo;s still pretty bad, and since the ecc file doesn&amp;rsquo;t store any
information about inodes, you can&amp;rsquo;t recover the full path.</source>
          <target state="translated">One main current limitation of pyFileFixity is that it cannot protect the directory tree meta-data. This means that in the worst case, if a silent error happens on the inode pointing to the root directory that you protected with an ecc, the whole directory will vanish, and all the files inside too. In less worst cases, sub-directories can vanish, but it&amp;rsquo;s still pretty bad, and since the ecc file doesn&amp;rsquo;t store any information about inodes, you can&amp;rsquo;t recover the full path.</target>
        </trans-unit>
        <trans-unit id="12ecd50362445ebb2431c73d3be7a0fdd51ae520" translate="yes" xml:space="preserve">
          <source>Open application and open specifications under the MIT license (you
can do whatever you want with it and tailor it to your needs if you
want to, or add better decoding procedures in the future as science
progress so that you can better recover your data from your already
generated ecc file).</source>
          <target state="translated">Open application and open specifications under the MIT license (you can do whatever you want with it and tailor it to your needs if you want to, or add better decoding procedures in the future as science progress so that you can better recover your data from your already generated ecc file).</target>
        </trans-unit>
        <trans-unit id="c293ee1b372d74a7da01d1004a48c5da58ee0305" translate="yes" xml:space="preserve">
          <source>Opensourced under the very permissive MIT licence, do whatever you
want!</source>
          <target state="translated">Opensourced under the very permissive MIT licence, do whatever you want!</target>
        </trans-unit>
        <trans-unit id="7d17cf7e348033dde266393ff42a2f4548c32bbb" translate="yes" xml:space="preserve">
          <source>Paper as a storage medium: paper is not a great storage medium,
because it has low storage density (ie, you can only store at most
about 100 KB) and it can also degrade just like other storage mediums,
but you cannot check that automatically since it&amp;rsquo;s not digital. However,
if you are interested, here are a few softwares that do that:
&lt;a href=&quot;http://en.wikipedia.org/wiki/Paper_key&quot;&gt;Paper key&lt;/a&gt;,
&lt;a href=&quot;http://www.ollydbg.de/Paperbak/index.html&quot;&gt;Paperbak&lt;/a&gt;,
&lt;a href=&quot;http://ronja.twibright.com/optar/&quot;&gt;Optar&lt;/a&gt;,
&lt;a href=&quot;https://github.com/penma/dpaper&quot;&gt;dpaper&lt;/a&gt;,
&lt;a href=&quot;http://blog.liw.fi/posts/qr-backup/&quot;&gt;QR Backup&lt;/a&gt;,
&lt;a href=&quot;http://blog.shuningbian.net/2009/10/qrbackup.php&quot;&gt;QR Backup (another)&lt;/a&gt;,
&lt;a href=&quot;http://git.pictorii.com/index.php?p=qrbackup.git&amp;amp;a=summary&quot;&gt;QR Backup (again another)&lt;/a&gt;,
&lt;a href=&quot;http://hansmi.ch/software/qrbackup&quot;&gt;QR Backup (again)&lt;/a&gt;,
&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.303.3101&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;and finally a related paper&lt;/a&gt;.</source>
          <target state="translated">Paper as a storage medium: paper is not a great storage medium, because it has low storage density (ie, you can only store at most about 100 KB) and it can also degrade just like other storage mediums, but you cannot check that automatically since it&amp;rsquo;s not digital. However, if you are interested, here are a few softwares that do that: &lt;a href=&quot;http://en.wikipedia.org/wiki/Paper_key&quot;&gt;Paper key&lt;/a&gt;, &lt;a href=&quot;http://www.ollydbg.de/Paperbak/index.html&quot;&gt;Paperbak&lt;/a&gt;, &lt;a href=&quot;http://ronja.twibright.com/optar/&quot;&gt;Optar&lt;/a&gt;, &lt;a href=&quot;https://github.com/penma/dpaper&quot;&gt;dpaper&lt;/a&gt;, &lt;a href=&quot;http://blog.liw.fi/posts/qr-backup/&quot;&gt;QR Backup&lt;/a&gt;, &lt;a href=&quot;http://blog.shuningbian.net/2009/10/qrbackup.php&quot;&gt;QR Backup (another)&lt;/a&gt;, &lt;a href=&quot;http://git.pictorii.com/index.php?p=qrbackup.git&amp;amp;a=summary&quot;&gt;QR Backup (again another)&lt;/a&gt;, &lt;a href=&quot;http://hansmi.ch/software/qrbackup&quot;&gt;QR Backup (again)&lt;/a&gt;, &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.303.3101&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;and finally a related paper&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="d2497e569f07d67bf296cc8a7b36de165a289082" translate="yes" xml:space="preserve">
          <source>Parchive (PAR1, PAR2, MultiPar): well known error correction file
generator. The big advantage of Parchives is that an ecc block
depends on multiple files: this allows to completely reconstruct a
missing file from scratch using files that are still available. Works
good for most people, but most available Parchive generators are not
satisfiable for me because 1- they do not allow to generate an ecc
for a directory tree recursively (except MultiPar, and even if it is
allowed in the PAR2 specs), 2- they can be very slow to generate
(even with multiprocessor extensions, because the galois field is
over 2^16 instead of 2^8, which is very costly), 3- the spec is not
very resilient to errors and tampering over the ecc file, as it
assumes the ecc file won&amp;rsquo;t be corrupted (I also tested, it&amp;rsquo;s still a
bit resilient, but it could be a lot more with some tweaking of the
spec), 4- it doesn&amp;rsquo;t allow for partial recovery (recovering blocks
that we can and pass the others that are unrecoverable): with PAR2, a
file can be restored fully or it cannot be at all.</source>
          <target state="translated">Parchive (PAR1, PAR2, MultiPar): well known error correction file generator. The big advantage of Parchives is that an ecc block depends on multiple files: this allows to completely reconstruct a missing file from scratch using files that are still available. Works good for most people, but most available Parchive generators are not satisfiable for me because 1- they do not allow to generate an ecc for a directory tree recursively (except MultiPar, and even if it is allowed in the PAR2 specs), 2- they can be very slow to generate (even with multiprocessor extensions, because the galois field is over 2^16 instead of 2^8, which is very costly), 3- the spec is not very resilient to errors and tampering over the ecc file, as it assumes the ecc file won&amp;rsquo;t be corrupted (I also tested, it&amp;rsquo;s still a bit resilient, but it could be a lot more with some tweaking of the spec), 4- it doesn&amp;rsquo;t allow for partial recovery (recovering blocks that we can and pass the others that are unrecoverable): with PAR2, a file can be restored fully or it cannot be at all.</target>
        </trans-unit>
        <trans-unit id="328017a3ec6114ad6b09b3daf2bbb3c451972424" translate="yes" xml:space="preserve">
          <source>Partial recovery allowed (even if a file cannot be completely
recovered, the parts that can will be repaired and then the rest that
can&amp;rsquo;t be repaired will be recopied from the corrupted version).</source>
          <target state="translated">Partial recovery allowed (even if a file cannot be completely recovered, the parts that can will be repaired and then the rest that can&amp;rsquo;t be repaired will be recopied from the corrupted version).</target>
        </trans-unit>
        <trans-unit id="042aa6d6a63ef4409a0e20afcb0ca7083c68c8af" translate="yes" xml:space="preserve">
          <source>Pros:</source>
          <target state="translated">Pros:</target>
        </trans-unit>
        <trans-unit id="032da2d7f590745e3e94f399591647c678ed76bc" translate="yes" xml:space="preserve">
          <source>Protecting directory tree meta-data</source>
          <target state="translated">Protecting directory tree meta-data</target>
        </trans-unit>
        <trans-unit id="d07133c8504b6a0e98371a1b9afe2ca2855a7270" translate="yes" xml:space="preserve">
          <source>Quickstart</source>
          <target state="translated">빠른 시작</target>
        </trans-unit>
        <trans-unit id="e86f99bd169ae9aca0b86d1c15fb084b44eeac61" translate="yes" xml:space="preserve">
          <source>RAID 0 is just using multiple disks just like a single one, to extend
the available storage. Let&amp;rsquo;s skip this one.</source>
          <target state="translated">RAID 0 is just using multiple disks just like a single one, to extend the available storage. Let&amp;rsquo;s skip this one.</target>
        </trans-unit>
        <trans-unit id="5f1ca6e3b755d262163fd9f92ae023c605741f4d" translate="yes" xml:space="preserve">
          <source>RAID 1 is mirroring one disk with a bit-by-bit copy of another disk.
That&amp;rsquo;s completely useless for long term storage: if either disk
fails, or if both disks are partially corrupted, you can&amp;rsquo;t know what
are the correct data and which aren&amp;rsquo;t. As an old saying goes: &amp;ldquo;Never
take 2 compasses: either take 3 or 1, because if both compasses show
different directions, you will never know which one is correct, nor
if both are wrong.&amp;rdquo; That&amp;rsquo;s the principle of Triplication.</source>
          <target state="translated">RAID 1 is mirroring one disk with a bit-by-bit copy of another disk. That&amp;rsquo;s completely useless for long term storage: if either disk fails, or if both disks are partially corrupted, you can&amp;rsquo;t know what are the correct data and which aren&amp;rsquo;t. As an old saying goes: &amp;ldquo;Never take 2 compasses: either take 3 or 1, because if both compasses show different directions, you will never know which one is correct, nor if both are wrong.&amp;rdquo; That&amp;rsquo;s the principle of Triplication.</target>
        </trans-unit>
        <trans-unit id="f745f29bcc39f675df5a9cae0685a3004a379703" translate="yes" xml:space="preserve">
          <source>RAID 5 is based on the triplication idea: you have n disks (but least
3), and if one fails you can recover n-1 disks (resilient to only 1
disk failure, not more).</source>
          <target state="translated">RAID 5 is based on the triplication idea: you have n disks (but least 3), and if one fails you can recover n-1 disks (resilient to only 1 disk failure, not more).</target>
        </trans-unit>
        <trans-unit id="a956a8f7361cacb050e7414c44e931378157fe95" translate="yes" xml:space="preserve">
          <source>RAID 6 is an extension of RAID 5 which is closer to error-correction
since you can correct n-k disks. However, most (all?) currently
commercially available RAID6 devices only implements recovery for at
most n-2 (2 disks failures).</source>
          <target state="translated">RAID 6 is an extension of RAID 5 which is closer to error-correction since you can correct n-k disks. However, most (all?) currently commercially available RAID6 devices only implements recovery for at most n-2 (2 disks failures).</target>
        </trans-unit>
        <trans-unit id="72e555525eaa6e1c5a85d76274e928a1e1562c5f" translate="yes" xml:space="preserve">
          <source>RAID is clearly insufficient for long-term data storage, and in fact it
was primarily meant as a cheap way to get more storage (RAID0) or more
availability (RAID1) of data, not for archiving data, even on a medium
timescale:</source>
          <target state="translated">RAID is clearly insufficient for long-term data storage, and in fact it was primarily meant as a cheap way to get more storage (RAID0) or more availability (RAID1) of data, not for archiving data, even on a medium timescale:</target>
        </trans-unit>
        <trans-unit id="08859b11e210d7463b9eb2b092bad4da27897187" translate="yes" xml:space="preserve">
          <source>Readable ecc file format (compared to PAR2 and most other similar
specifications).</source>
          <target state="translated">Readable ecc file format (compared to PAR2 and most other similar specifications).</target>
        </trans-unit>
        <trans-unit id="4251d937b88de865a0e0f95746c95f43e5aef380" translate="yes" xml:space="preserve">
          <source>Recursive/Relative Files Integrity Generator and Checker in Python (aka RFIGC)</source>
          <target state="translated">Recursive/Relative Files Integrity Generator and Checker in Python (aka RFIGC)</target>
        </trans-unit>
        <trans-unit id="271537aa3799aef3e57eef26b4824eafb00e48ed" translate="yes" xml:space="preserve">
          <source>Recursively generate or check the integrity of files by MD5 and SHA1
hashes, size, modification date or by data structure integrity (only for
images).</source>
          <target state="translated">Recursively generate or check the integrity of files by MD5 and SHA1 hashes, size, modification date or by data structure integrity (only for images).</target>
        </trans-unit>
        <trans-unit id="fb80a39b8286d14c801267bfda15d14e8cf64384" translate="yes" xml:space="preserve">
          <source>Runs on Python 2.7.10 and on PyPy (not yet ported to Python 3 but the libraries are already compatible).</source>
          <target state="translated">Runs on Python 2.7.10 and on PyPy (not yet ported to Python 3 but the libraries are already compatible).</target>
        </trans-unit>
        <trans-unit id="411522052a5b4063e202a3bc2bebc7a1c9a2344c" translate="yes" xml:space="preserve">
          <source>SnapRAID</source>
          <target state="translated">SnapRAID</target>
        </trans-unit>
        <trans-unit id="f0dacede28bad3eb7210228c451bfddf04efac8e" translate="yes" xml:space="preserve">
          <source>Some attempts were made to translate channel theory and error correcting
codes theory to data storage, the first being Reed-Solomon which spawned
the RAID schema. Then CIRC (Cross-interleaved Reed-Solomon coding) was
devised for use on optical discs to recover from scratches, which was
necessary for the technology to be usable for consumers. Since then, new
less-optimal but a lot faster algorithms such as LDPC, turbo-codes and
fountain codes such as RaptorQ were invented (or rediscovered), but they
are still marginally researched for data storage.</source>
          <target state="translated">Some attempts were made to translate channel theory and error correcting codes theory to data storage, the first being Reed-Solomon which spawned the RAID schema. Then CIRC (Cross-interleaved Reed-Solomon coding) was devised for use on optical discs to recover from scratches, which was necessary for the technology to be usable for consumers. Since then, new less-optimal but a lot faster algorithms such as LDPC, turbo-codes and fountain codes such as RaptorQ were invented (or rediscovered), but they are still marginally researched for data storage.</target>
        </trans-unit>
        <trans-unit id="6c6f9e953c99cbcd667b71ea0d6b30b24d33018c" translate="yes" xml:space="preserve">
          <source>Structural Adaptive Error Correction Encoder</source>
          <target state="translated">Structural Adaptive Error Correction Encoder</target>
        </trans-unit>
        <trans-unit id="07c999b2b3e0979ba3c9706c65ecd4c1a3ab5eaf" translate="yes" xml:space="preserve">
          <source>Support directory processing: you can encode an ecc file for a whole
directory of files (with any number of sub-directories and depth).</source>
          <target state="translated">Support directory processing: you can encode an ecc file for a whole directory of files (with any number of sub-directories and depth).</target>
        </trans-unit>
        <trans-unit id="25082e75020d81531b06098959a1c0dd5e8b01c5" translate="yes" xml:space="preserve">
          <source>Support for erasures (null bytes) and even errors-and-erasures, which
literally doubles the repair capabilities. To my knowledge, this is
the only freely available parity software that supports erasures.</source>
          <target state="translated">Support for erasures (null bytes) and even errors-and-erasures, which literally doubles the repair capabilities. To my knowledge, this is the only freely available parity software that supports erasures.</target>
        </trans-unit>
        <trans-unit id="62b9e3a297158ac62f3a9e67a04cda553961f004" translate="yes" xml:space="preserve">
          <source>TODO: write more here</source>
          <target state="translated">TODO: write more here</target>
        </trans-unit>
        <trans-unit id="1a428989bd13dceabd89b698d9fa38eb8a0d175c" translate="yes" xml:space="preserve">
          <source>Table of contents</source>
          <target state="translated">목차</target>
        </trans-unit>
        <trans-unit id="46c04475aab1ffcf4344d85e50fd6ba1ff634cc7" translate="yes" xml:space="preserve">
          <source>TestDisk: for file scraping, when nothing else worked.</source>
          <target state="translated">TestDisk: for file scraping, when nothing else worked.</target>
        </trans-unit>
        <trans-unit id="cd837e66b0a78157d510c6466029192676cec43f" translate="yes" xml:space="preserve">
          <source>The details are long and a bit complicated (I may write a complete article
about it in the future), but the tl;dr answer is that you should use &lt;em&gt;optical disks&lt;/em&gt;,
because it decouples the storage medium and the reading hardware
(eg, at the opposite we have hard drives, which contains both the reading
hardware and the storage medium, so if one fails, you lose both)
and because it&amp;rsquo;s most likely future-proof (you only need a laser, which
is universal, the laser&amp;rsquo;s parameters can always be tweaked).</source>
          <target state="translated">The details are long and a bit complicated (I may write a complete article about it in the future), but the tl;dr answer is that you should use &lt;em&gt;optical disks&lt;/em&gt;, because it decouples the storage medium and the reading hardware (eg, at the opposite we have hard drives, which contains both the reading hardware and the storage medium, so if one fails, you lose both) and because it&amp;rsquo;s most likely future-proof (you only need a laser, which is universal, the laser&amp;rsquo;s parameters can always be tweaked).</target>
        </trans-unit>
        <trans-unit id="d93719c7322684ab016ad95f505115d010b7c2ba" translate="yes" xml:space="preserve">
          <source>The idea is that the critical parts of files usually are placed at the
top, and data becomes less and less critical along the file. What is
meant by critical is both the critical spots (eg: if you tamper only one
character of a file&amp;rsquo;s header you have good chances of losing your entire
file, ie, you cannot even open it) and critically encoded information
(eg: archive formats usually encode compressed symbols as they go along
the file, which means that the first occurrence is encoded, and then the
archive simply writes a reference to the symbol. Thus, the first
occurrence is encoded at the top, and subsequent encoding of this same
data pattern will just be one symbol, and thus it matters less as long
as the original symbol is correctly encoded and its information
preserved, we can always try to restore the reference symbols later).
Moreover, really redundant data will be placed at the top because they
can be reused a lot, while data that cannot be too much compressed will
be placed later, and thus, corruption of this less compressed data is a
lot less critical because only a few characters will be changed in the
uncompressed file (since the data is less compressed, a character change
on the not-so-much compressed data won&amp;rsquo;t have very significant impact on
the uncompressed data).</source>
          <target state="translated">The idea is that the critical parts of files usually are placed at the top, and data becomes less and less critical along the file. What is meant by critical is both the critical spots (eg: if you tamper only one character of a file&amp;rsquo;s header you have good chances of losing your entire file, ie, you cannot even open it) and critically encoded information (eg: archive formats usually encode compressed symbols as they go along the file, which means that the first occurrence is encoded, and then the archive simply writes a reference to the symbol. Thus, the first occurrence is encoded at the top, and subsequent encoding of this same data pattern will just be one symbol, and thus it matters less as long as the original symbol is correctly encoded and its information preserved, we can always try to restore the reference symbols later). Moreover, really redundant data will be placed at the top because they can be reused a lot, while data that cannot be too much compressed will be placed later, and thus, corruption of this less compressed data is a lot less critical because only a few characters will be changed in the uncompressed file (since the data is less compressed, a character change on the not-so-much compressed data won&amp;rsquo;t have very significant impact on the uncompressed data).</target>
        </trans-unit>
        <trans-unit id="6d89b608d330297fb33e82bdc6a90af45a8aa8f3" translate="yes" xml:space="preserve">
          <source>The inability to store these meta-data is because of two choices in the
design: 1- portability: we want the ecc file to work even if we move the
root directory to another place or another storage medium (and of
course, the inode would change), 2- cross-platform compatibility:
there&amp;rsquo;s no way to get and store directory meta-data for all platforms,
but of course we could implement specific instructions for each main
platform, so this point is not really a problem.</source>
          <target state="translated">The inability to store these meta-data is because of two choices in the design: 1- portability: we want the ecc file to work even if we move the root directory to another place or another storage medium (and of course, the inode would change), 2- cross-platform compatibility: there&amp;rsquo;s no way to get and store directory meta-data for all platforms, but of course we could implement specific instructions for each main platform, so this point is not really a problem.</target>
        </trans-unit>
        <trans-unit id="7201cfd57aba74e53b9138b3c148089dcbbee893" translate="yes" xml:space="preserve">
          <source>The only solution is to use a principle of engineering that is long
known and which makes bridges safe: add some &lt;strong&gt;redundancy&lt;/strong&gt;.</source>
          <target state="translated">The only solution is to use a principle of engineering that is long known and which makes bridges safe: add some &lt;strong&gt;redundancy&lt;/strong&gt;.</target>
        </trans-unit>
        <trans-unit id="b378b6b76b5e152b79969653820881580ee9a1c8" translate="yes" xml:space="preserve">
          <source>The other reason RAID is not adapted to long-term storage, is that it
supposes you store your data on hard-drives exclusively. Hard drives
aren&amp;rsquo;t a good storage medium for the long term, for two reasons:</source>
          <target state="translated">The other reason RAID is not adapted to long-term storage, is that it supposes you store your data on hard-drives exclusively. Hard drives aren&amp;rsquo;t a good storage medium for the long term, for two reasons:</target>
        </trans-unit>
        <trans-unit id="f01210593985ce6f6e31800caf16558d7fa8a78f" translate="yes" xml:space="preserve">
          <source>The problem is that most theoretical and pratical works on error
correcting codes has been done almost exclusively on channel
transmission (such as 4G, internet, etc.), but not on data storage,
which is very different for one reason: whereas in a channel we are in a
spatial scheme (both the sender and the receiver are different entities
in space but working at the same timescale), in data storage this is a
temporal scheme: the sender was you storing the data on your medium at
time t, and the receiver is again you but now retrieving the data at
time t+x. Thus, the sender does not exist anymore, thus you cannot ask
the sender to send again some data if it&amp;rsquo;s too much corrupted: in data
storage, if a data is corrupted, it&amp;rsquo;s lost for good, whereas in channel theory,
parts of the data can be submitted again if necessary.</source>
          <target state="translated">The problem is that most theoretical and pratical works on error correcting codes has been done almost exclusively on channel transmission (such as 4G, internet, etc.), but not on data storage, which is very different for one reason: whereas in a channel we are in a spatial scheme (both the sender and the receiver are different entities in space but working at the same timescale), in data storage this is a temporal scheme: the sender was you storing the data on your medium at time t, and the receiver is again you but now retrieving the data at time t+x. Thus, the sender does not exist anymore, thus you cannot ask the sender to send again some data if it&amp;rsquo;s too much corrupted: in data storage, if a data is corrupted, it&amp;rsquo;s lost for good, whereas in channel theory, parts of the data can be submitted again if necessary.</target>
        </trans-unit>
        <trans-unit id="6c812f59f465770d653fd29361d8ba762b60f57a" translate="yes" xml:space="preserve">
          <source>The problem of long term storage</source>
          <target state="translated">The problem of long term storage</target>
        </trans-unit>
        <trans-unit id="4bcdd04680e3eb4699b39ca709b218e0c1527807" translate="yes" xml:space="preserve">
          <source>The project currently include the following pure-python applications:</source>
          <target state="translated">The project currently include the following pure-python applications:</target>
        </trans-unit>
        <trans-unit id="75f49de0cb73e99f34c6079c0a34fc8b36fabded" translate="yes" xml:space="preserve">
          <source>The script is pure-python as are its dependencies: it is thus completely
cross-platform and open source. However, this imply that it is quite
slow, but PyPy v2.5.0 was successfully tested against the script without
any modification, and a speed increase of more 100x could be observed,
so that you can expect a rate of more than 1MB/s, which is quite fast.</source>
          <target state="translated">The script is pure-python as are its dependencies: it is thus completely cross-platform and open source. However, this imply that it is quite slow, but PyPy v2.5.0 was successfully tested against the script without any modification, and a speed increase of more 100x could be observed, so that you can expect a rate of more than 1MB/s, which is quite fast.</target>
        </trans-unit>
        <trans-unit id="6da4dd461956bc1afb3087228f50d91b3be5cea2" translate="yes" xml:space="preserve">
          <source>The script structural-adaptive-ecc.py implements this idea, which can be
seen as an extension of header-ecc.py (and in fact the idea was the
other way around: structural-adaptive-ecc.py was conceived first but was
too complicated, then header-ecc.py was implemented as a working
lessened implementation only for headers, and then
structural-adaptive-ecc.py was finished using header-ecc.py code
progress). It works, it was a quite well tested for my own needs on
datasets of hundred of GB, but it&amp;rsquo;s not foolproof so make sure you test
the script by yourself to see if it&amp;rsquo;s robust enough for your needs (any
feedback about this would be greatly appreciated!).</source>
          <target state="translated">The script structural-adaptive-ecc.py implements this idea, which can be seen as an extension of header-ecc.py (and in fact the idea was the other way around: structural-adaptive-ecc.py was conceived first but was too complicated, then header-ecc.py was implemented as a working lessened implementation only for headers, and then structural-adaptive-ecc.py was finished using header-ecc.py code progress). It works, it was a quite well tested for my own needs on datasets of hundred of GB, but it&amp;rsquo;s not foolproof so make sure you test the script by yourself to see if it&amp;rsquo;s robust enough for your needs (any feedback about this would be greatly appreciated!).</target>
        </trans-unit>
        <trans-unit id="6e804b561b20f42afcb9ef3644daf5615c6ce9ed" translate="yes" xml:space="preserve">
          <source>There also are new generation RAID solutions, mainly software based,
such as SnapRAID or ZFS, which allow you to configure a virtual RAID
with the value n-k that you want. This is just like an ecc file (but a
bit less flexible, since it&amp;rsquo;s not a file but a disk mapping, so that you
can&amp;rsquo;t just copy it around or upload it to a cloud backup hosting). In
addition to recover (n-k) disks, they can also be configured to recover
from partial, sectors failures inside the disk and not just the whole
disk (for a more detailed explanation, see Plank, James S., Mario Blaum,
and James L. Hafner. &amp;ldquo;SD codes: erasure codes designed for how storage
systems really fail.&amp;rdquo; FAST. 2013.).</source>
          <target state="translated">There also are new generation RAID solutions, mainly software based, such as SnapRAID or ZFS, which allow you to configure a virtual RAID with the value n-k that you want. This is just like an ecc file (but a bit less flexible, since it&amp;rsquo;s not a file but a disk mapping, so that you can&amp;rsquo;t just copy it around or upload it to a cloud backup hosting). In addition to recover (n-k) disks, they can also be configured to recover from partial, sectors failures inside the disk and not just the whole disk (for a more detailed explanation, see Plank, James S., Mario Blaum, and James L. Hafner. &amp;ldquo;SD codes: erasure codes designed for how storage systems really fail.&amp;rdquo; FAST. 2013.).</target>
        </trans-unit>
        <trans-unit id="acb42fe7377f3d92c26abfd23793c72c39fdd3fd" translate="yes" xml:space="preserve">
          <source>There are a few studies about the most resilient file formats, such as:</source>
          <target state="translated">There are a few studies about the most resilient file formats, such as:</target>
        </trans-unit>
        <trans-unit id="fe261194a37bb2ee20b8791c018323b14337c10c" translate="yes" xml:space="preserve">
          <source>There are only 2 ways to add redundancy:</source>
          <target state="translated">There are only 2 ways to add redundancy:</target>
        </trans-unit>
        <trans-unit id="347b257156440adc36f5b5511f328dc9ac887aec" translate="yes" xml:space="preserve">
          <source>This project aims to provide a set of open source, cross-platform, easy
to use and easy to maintain (readable code) to protect and manage data
for long term storage. The project is done in pure-Python to meet those criteria.</source>
          <target state="translated">This project aims to provide a set of open source, cross-platform, easy to use and easy to maintain (readable code) to protect and manage data for long term storage. The project is done in pure-Python to meet those criteria.</target>
        </trans-unit>
        <trans-unit id="9f92c03f11bd7ac82f1eaa98123230a8a26b6854" translate="yes" xml:space="preserve">
          <source>This project aims to, first, implement easy tools to evaluate strategies
(filetamper.py) and file fixity (ie, detect if there are corruptions),
and then the goal is to provide an open and easy framework to use
different kinds of error correction codes to protect and repair files.</source>
          <target state="translated">This project aims to, first, implement easy tools to evaluate strategies (filetamper.py) and file fixity (ie, detect if there are corruptions), and then the goal is to provide an open and easy framework to use different kinds of error correction codes to protect and repair files.</target>
        </trans-unit>
        <trans-unit id="c4f898f16783e9390001b6c3ff4ddf2994286470" translate="yes" xml:space="preserve">
          <source>This script implements a variable error correction rate encoder: each
file is ecc encoded using a variable resiliency rate &amp;ndash; using a high
constant resiliency rate for the header part (resiliency rate stage 1,
high), then a variable resiliency rate is applied to the rest of the
file&amp;rsquo;s content, with a higher rate near the beginning of the file
(resiliency rate stage 2, medium) which progressively decreases until
the end of file (resiliency rate stage 3, the lowest).</source>
          <target state="translated">This script implements a variable error correction rate encoder: each file is ecc encoded using a variable resiliency rate &amp;ndash; using a high constant resiliency rate for the header part (resiliency rate stage 1, high), then a variable resiliency rate is applied to the rest of the file&amp;rsquo;s content, with a higher rate near the beginning of the file (resiliency rate stage 2, medium) which progressively decreases until the end of file (resiliency rate stage 3, the lowest).</target>
        </trans-unit>
        <trans-unit id="ed055c0e36daa20689bb2b25356e3a6af641751b" translate="yes" xml:space="preserve">
          <source>This script is originally meant to be used for data archival, by
allowing an easy way to check for silent file corruption. Thus, this
script uses relative paths so that you can easily compute and check the
same redundant data copied on different mediums (hard drives, optical
discs, etc.). This script is not meant for system files corruption
notification, but is more meant to be used from times-to-times to check
up on your data archives integrity (if you need this kind of application,
see &lt;a href=&quot;https://github.com/avpreserve/fixity&quot;&gt;avpreserve&amp;rsquo;s fixity&lt;/a&gt;).</source>
          <target state="translated">This script is originally meant to be used for data archival, by allowing an easy way to check for silent file corruption. Thus, this script uses relative paths so that you can easily compute and check the same redundant data copied on different mediums (hard drives, optical discs, etc.). This script is not meant for system files corruption notification, but is more meant to be used from times-to-times to check up on your data archives integrity (if you need this kind of application, see &lt;a href=&quot;https://github.com/avpreserve/fixity&quot;&gt;avpreserve&amp;rsquo;s fixity&lt;/a&gt;).</target>
        </trans-unit>
        <trans-unit id="bceddc6412ba83d36f284864d1da571c0f1cdab4" translate="yes" xml:space="preserve">
          <source>This script was made for Python 2.7.6, but it should be easily adaptable
to run on Python 3.x.</source>
          <target state="translated">This script was made for Python 2.7.6, but it should be easily adaptable to run on Python 3.x.</target>
        </trans-unit>
        <trans-unit id="efd02f15d72cbccbff6d41bba0d337b1c60d7b6e" translate="yes" xml:space="preserve">
          <source>This script was made to be used in combination with other more common
file redundancy generators (such as PAR2, I advise MultiPar). This is an
additional layer of protection for your files: by using a higher
resiliency rate on the headers of your files, you ensure that you will
be probably able to open them in the future, avoiding the &amp;ldquo;critical
spots&amp;rdquo;, also called &amp;ldquo;fracture-critical&amp;rdquo; in redundancy engineering (where
if you modify just one bit, your whole file may become unreadable,
usually bits residing in the headers - in other words, a single blow
makes the whole thing collapse, just like non-redundant bridges).</source>
          <target state="translated">This script was made to be used in combination with other more common file redundancy generators (such as PAR2, I advise MultiPar). This is an additional layer of protection for your files: by using a higher resiliency rate on the headers of your files, you ensure that you will be probably able to open them in the future, avoiding the &amp;ldquo;critical spots&amp;rdquo;, also called &amp;ldquo;fracture-critical&amp;rdquo; in redundancy engineering (where if you modify just one bit, your whole file may become unreadable, usually bits residing in the headers - in other words, a single blow makes the whole thing collapse, just like non-redundant bridges).</target>
        </trans-unit>
        <trans-unit id="4e222211a95663da3c75bf8f60f620ff3d35da27" translate="yes" xml:space="preserve">
          <source>This section describes how to use the Cython implementation. However,
you should first try PyPy, as it did give 10x to 100x speedup over
Cython in our case.</source>
          <target state="translated">This section describes how to use the Cython implementation. However, you should first try PyPy, as it did give 10x to 100x speedup over Cython in our case.</target>
        </trans-unit>
        <trans-unit id="86e0245633b4022c9e37415aa7054a9e4a57995a" translate="yes" xml:space="preserve">
          <source>This variable error correction rate should allow to protect more the
critical parts of a file (the header and the beginning of a file, for
example in compressed file formats such as zip or jpg this is where the
most importantly strings are encoded) for the same amount of storage as
a standard constant error correction rate.</source>
          <target state="translated">This variable error correction rate should allow to protect more the critical parts of a file (the header and the beginning of a file, for example in compressed file formats such as zip or jpg this is where the most importantly strings are encoded) for the same amount of storage as a standard constant error correction rate.</target>
        </trans-unit>
        <trans-unit id="c013f0739dd9a2ef2425781962e0dee524432b63" translate="yes" xml:space="preserve">
          <source>This works because most files will store the most important information to read them at
their beginning, also called &amp;ldquo;file&amp;rsquo;s header&amp;rdquo;, so repairing this part will almost always ensure
the possibility to read the file (even if the rest of the file is still corrupted, if the header is safe,
you can read it).</source>
          <target state="translated">This works because most files will store the most important information to read them at their beginning, also called &amp;ldquo;file&amp;rsquo;s header&amp;rdquo;, so repairing this part will almost always ensure the possibility to read the file (even if the rest of the file is still corrupted, if the header is safe, you can read it).</target>
        </trans-unit>
        <trans-unit id="516539cb2d470c4b05683a2573162aa1966c6906" translate="yes" xml:space="preserve">
          <source>To check if files were corrupted:</source>
          <target state="translated">To check if files were corrupted:</target>
        </trans-unit>
        <trans-unit id="bb6e514c0110fc0eb49ee26209e735f34515d713" translate="yes" xml:space="preserve">
          <source>To check:</source>
          <target state="translated">To check:</target>
        </trans-unit>
        <trans-unit id="f8c347e2ca94666ece18979b9632049766591833" translate="yes" xml:space="preserve">
          <source>To generate a monitoring database (to later check if files were changed, but no possibility of repairing):</source>
          <target state="translated">To generate a monitoring database (to later check if files were changed, but no possibility of repairing):</target>
        </trans-unit>
        <trans-unit id="4ec63a10bebce0574c8172f04210c1f6bdd7e1df" translate="yes" xml:space="preserve">
          <source>To generate the database (only needed once):</source>
          <target state="translated">To generate the database (only needed once):</target>
        </trans-unit>
        <trans-unit id="55f98da92ec04f5c89ea2c9c7baea6a57b67f22f" translate="yes" xml:space="preserve">
          <source>To get more options for any tool, use</source>
          <target state="translated">To get more options for any tool, use</target>
        </trans-unit>
        <trans-unit id="13575d8db879ccefd4b7aebeb85eacb75cd2e7bf" translate="yes" xml:space="preserve">
          <source>To protect files headers with a file called</source>
          <target state="translated">To protect files headers with a file called</target>
        </trans-unit>
        <trans-unit id="6745aff7f231d6284a68fc6529a083c71396cf87" translate="yes" xml:space="preserve">
          <source>To protect whole files with a file called</source>
          <target state="translated">To protect whole files with a file called</target>
        </trans-unit>
        <trans-unit id="ea1fe16c6559ce768616c40f1051af858049a926" translate="yes" xml:space="preserve">
          <source>To repair an ecc file</source>
          <target state="translated">To repair an ecc file</target>
        </trans-unit>
        <trans-unit id="148b0ddf79f88efc4d963b49c8ac1150a3c4a7f1" translate="yes" xml:space="preserve">
          <source>To repair files headers and store the repaired files in</source>
          <target state="translated">To repair files headers and store the repaired files in</target>
        </trans-unit>
        <trans-unit id="e691cd3c6ac9bd8008387c036ac3e721968b64d0" translate="yes" xml:space="preserve">
          <source>To repair whole files:</source>
          <target state="translated">To repair whole files:</target>
        </trans-unit>
        <trans-unit id="d319d7adebd971ff7f412e09b08f37a5242b87ca" translate="yes" xml:space="preserve">
          <source>To repair your files using multiple copies that you have stored on different mediums:</source>
          <target state="translated">To repair your files using multiple copies that you have stored on different mediums:</target>
        </trans-unit>
        <trans-unit id="4ee123ec3e75935015911f84c7bb17672da80159" translate="yes" xml:space="preserve">
          <source>To run tests on your recovery tools, you can make a Makefile-like configuration file and use:</source>
          <target state="translated">To run tests on your recovery tools, you can make a Makefile-like configuration file and use:</target>
        </trans-unit>
        <trans-unit id="05d3c6574cf07820f9b0fbeb21a6ed97dedf59fb" translate="yes" xml:space="preserve">
          <source>To update your database by appending new files AND removing
inexistent files:</source>
          <target state="translated">To update your database by appending new files AND removing inexistent files:</target>
        </trans-unit>
        <trans-unit id="21b7a37dee042796574041514625609f60399db0" translate="yes" xml:space="preserve">
          <source>To update your database by appending new files:</source>
          <target state="translated">To update your database by appending new files:</target>
        </trans-unit>
        <trans-unit id="5eb513e66a091088e0e959f205ef0bd72a5fbf98" translate="yes" xml:space="preserve">
          <source>To use the GUI with any tool, use</source>
          <target state="translated">To use the GUI with any tool, use</target>
        </trans-unit>
        <trans-unit id="61df8e334ae1cc4ff9e5b97e983655ace759626b" translate="yes" xml:space="preserve">
          <source>To use this monitoring database to recover files names and directory layout after filescraping:</source>
          <target state="translated">To use this monitoring database to recover files names and directory layout after filescraping:</target>
        </trans-unit>
        <trans-unit id="c118a2e1c6ae5269714e6048cb25463c56652bfa" translate="yes" xml:space="preserve">
          <source>To workaround this issue (directory meta-data are critical spots), other
softwares use a one-time storage medium (ie, writing your data along
with generating and writing the ecc). This way, they can access at
the bit level the inode info, and they are guaranted that the inodes
won&amp;rsquo;t ever change. This is the approach taken by DVDisaster: by using
optical mediums, it can compute inodes that will be permanent, and thus
also encode that info in the ecc file. Another approach is to create a
virtual filesystem specifically to store just your files, so that you
manage the inode yourself, and you can then copy the whole filesystem
around (which is really just a file, just like a zip file - which can
also be considered as a mini virtual file system in fact) like
&lt;a href=&quot;http://users.softlab.ntua.gr/~ttsiod/rsbep.html&quot;&gt;rsbep&lt;/a&gt;.</source>
          <target state="translated">To workaround this issue (directory meta-data are critical spots), other softwares use a one-time storage medium (ie, writing your data along with generating and writing the ecc). This way, they can access at the bit level the inode info, and they are guaranted that the inodes won&amp;rsquo;t ever change. This is the approach taken by DVDisaster: by using optical mediums, it can compute inodes that will be permanent, and thus also encode that info in the ecc file. Another approach is to create a virtual filesystem specifically to store just your files, so that you manage the inode yourself, and you can then copy the whole filesystem around (which is really just a file, just like a zip file - which can also be considered as a mini virtual file system in fact) like &lt;a href=&quot;http://users.softlab.ntua.gr/~ttsiod/rsbep.html&quot;&gt;rsbep&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="0e06f4cfadc787dda9939f7c26511dbe45f53468" translate="yes" xml:space="preserve">
          <source>Tools like pyFileFixity (or which can be used as complements)</source>
          <target state="translated">Tools like pyFileFixity (or which can be used as complements)</target>
        </trans-unit>
        <trans-unit id="9f1f4b46cf1ef37d20bbe1c548c479f4c8561f15" translate="yes" xml:space="preserve">
          <source>Two Reed-Solomon codecs are available, they are functionally equivalent
and thoroughly unit tested.</source>
          <target state="translated">Two Reed-Solomon codecs are available, they are functionally equivalent and thoroughly unit tested.</target>
        </trans-unit>
        <trans-unit id="ea31afd22447ac5ddc386d2ddeb902eda4d089c4" translate="yes" xml:space="preserve">
          <source>Variable resiliency rate and header-only resilience, ensuring that
you can always open your files even if partially corrupted (the
structure of your files will be saved, so that you can use other
softwares to repair beyond if this set of script is not sufficient to
totally repair).</source>
          <target state="translated">Variable resiliency rate and header-only resilience, ensuring that you can always open your files even if partially corrupted (the structure of your files will be saved, so that you can use other softwares to repair beyond if this set of script is not sufficient to totally repair).</target>
        </trans-unit>
        <trans-unit id="3fcf28c772b6dc2b4cd7c0d0fd83d60ed3dc1f69" translate="yes" xml:space="preserve">
          <source>Very safe and conservative approach: the recovery process checks that
the recovery was successful before committing a repaired block.</source>
          <target state="translated">Very safe and conservative approach: the recovery process checks that the recovery was successful before committing a repaired block.</target>
        </trans-unit>
        <trans-unit id="8d8d3137f10a168d4bda9ef1df7dc0685816ca62" translate="yes" xml:space="preserve">
          <source>What file formats are the most recoverable?</source>
          <target state="translated">What file formats are the most recoverable?</target>
        </trans-unit>
        <trans-unit id="2def5cddbcf1fb82ea172f5020ae863ff643cb9d" translate="yes" xml:space="preserve">
          <source>What medium should I use to store my data?</source>
          <target state="translated">What medium should I use to store my data?</target>
        </trans-unit>
        <trans-unit id="81c7918f4ff1aa8891ae6e455c3e49d5237709a0" translate="yes" xml:space="preserve">
          <source>Why are data corrupted with time? Entropy, my friend, entropy.
Entropy refers to the universal tendency for systems to become
less ordered over time. Corruption is exactly that: a disorder
in bits order. In other words: &lt;em&gt;the Universe hates your data&lt;/em&gt;.</source>
          <target state="translated">Why are data corrupted with time? Entropy, my friend, entropy. Entropy refers to the universal tendency for systems to become less ordered over time. Corruption is exactly that: a disorder in bits order. In other words: &lt;em&gt;the Universe hates your data&lt;/em&gt;.</target>
        </trans-unit>
        <trans-unit id="6d74bfd0fbe10c3dda3f781a2eda050c869723d7" translate="yes" xml:space="preserve">
          <source>Why not just use RAID ?</source>
          <target state="translated">Why not just use RAID ?</target>
        </trans-unit>
        <trans-unit id="5a2ca364a96d8714bf91679c220bc6ca7e24a6f4" translate="yes" xml:space="preserve">
          <source>You can also use &lt;a href=&quot;http://pypy.org/&quot;&gt;PyPy&lt;/a&gt; to hugely speedup the processing time of any tool here.</source>
          <target state="translated">You can also use &lt;a href=&quot;http://pypy.org/&quot;&gt;PyPy&lt;/a&gt; to hugely speedup the processing time of any tool here.</target>
        </trans-unit>
        <trans-unit id="84c60f2ca6fb65d58559ba44dae6a7ce86a93b9b" translate="yes" xml:space="preserve">
          <source>You can encrypt your data files, but choose a non-solid algorithm
(like AES if I&amp;rsquo;m not mistaken) so that corrupted parts do not prevent
the decoding of subsequent correct parts. Of course, you&amp;rsquo;re lowering a
bit your chances of recovering your data files by encrypting them (the
best chance to keep data for the long term is to keep them in clear
text), but if it&amp;rsquo;s really necessary, using a non-solid encrypting
scheme is a good compromise.</source>
          <target state="translated">You can encrypt your data files, but choose a non-solid algorithm (like AES if I&amp;rsquo;m not mistaken) so that corrupted parts do not prevent the decoding of subsequent correct parts. Of course, you&amp;rsquo;re lowering a bit your chances of recovering your data files by encrypting them (the best chance to keep data for the long term is to keep them in clear text), but if it&amp;rsquo;s really necessary, using a non-solid encrypting scheme is a good compromise.</target>
        </trans-unit>
        <trans-unit id="41f8017c7c727b6672d696bf4440c3026854959d" translate="yes" xml:space="preserve">
          <source>You can generate an ecc file on your encrypted data files, thus
&lt;em&gt;after&lt;/em&gt; encryption, and keep the ecc file in clear text (never encrypt
nor compress it). This is not a security risk at all since the ecc
file does not give any information on the content inside your
encrypted files, but rather just redundant info to correct corrupted
bytes (however if you generate the ecc file on the data files before
encryption, then it&amp;rsquo;s clearly a security risk, and someone could
recover your data without your permission).</source>
          <target state="translated">You can generate an ecc file on your encrypted data files, thus &lt;em&gt;after&lt;/em&gt; encryption, and keep the ecc file in clear text (never encrypt nor compress it). This is not a security risk at all since the ecc file does not give any information on the content inside your encrypted files, but rather just redundant info to correct corrupted bytes (however if you generate the ecc file on the data files before encryption, then it&amp;rsquo;s clearly a security risk, and someone could recover your data without your permission).</target>
        </trans-unit>
        <trans-unit id="52d7fdd29adb97b07731f699517255f5f24f80ee" translate="yes" xml:space="preserve">
          <source>You can specify different ecc algorithms using the</source>
          <target state="translated">You can specify different ecc algorithms using the</target>
        </trans-unit>
        <trans-unit id="e20471be59517ff50364d1b640ce03590a268c03" translate="yes" xml:space="preserve">
          <source>Your data files, that you want to protect, &lt;em&gt;should&lt;/em&gt; remain in clear
text, but you may choose to compress them if it drastically reduces
the size of your files, and if you raise the resilience rate of your
ecc file (so compression may be a good option if you have an
opportunity to trade the file size reduction for more ecc file
resilience). Also, make sure to choose a non-solid compression
algorithm like DEFLATE (zip) so that you can still decode correct
parts even if some are corrupted (else with a solid archive, if one
byte is corrupted, the whole archive may become unreadable).</source>
          <target state="translated">Your data files, that you want to protect, &lt;em&gt;should&lt;/em&gt; remain in clear text, but you may choose to compress them if it drastically reduces the size of your files, and if you raise the resilience rate of your ecc file (so compression may be a good option if you have an opportunity to trade the file size reduction for more ecc file resilience). Also, make sure to choose a non-solid compression algorithm like DEFLATE (zip) so that you can still decode correct parts even if some are corrupted (else with a solid archive, if one byte is corrupted, the whole archive may become unreadable).</target>
        </trans-unit>
        <trans-unit id="6a3d1552172d0a02c2dc808a4a0270f20ed44abb" translate="yes" xml:space="preserve">
          <source>ZFS: a file system which includes ecc correction directly. The whole
filesystem, including directory tree meta-data, are protected. If you
want ecc protection on your computer for all your files, this is the
way to go.</source>
          <target state="translated">ZFS: a file system which includes ecc correction directly. The whole filesystem, including directory tree meta-data, are protected. If you want ecc protection on your computer for all your files, this is the way to go.</target>
        </trans-unit>
        <trans-unit id="ec142ef59a1235125a37a18d356f20009fcbdb9d" translate="yes" xml:space="preserve">
          <source>Zip (with DEFLATE algorithm, using 7-Zip or other tools): allows to
create non-solid archives which are readable by most computers
(ubiquitous algorithm). Non-solid archive means that a zip file can
still unzip correct files even if it is corrupted, because files are
encoded in blocks, and thus even if some blocks are corrupted, the
decoding can happen. A &lt;a href=&quot;https://github.com/klauspost/compress&quot;&gt;fast implementation with enhanced compression
is available in pure Go&lt;/a&gt;
(good for long storage).</source>
          <target state="translated">Zip (with DEFLATE algorithm, using 7-Zip or other tools): allows to create non-solid archives which are readable by most computers (ubiquitous algorithm). Non-solid archive means that a zip file can still unzip correct files even if it is corrupted, because files are encoded in blocks, and thus even if some blocks are corrupted, the decoding can happen. A &lt;a href=&quot;https://github.com/klauspost/compress&quot;&gt;fast implementation with enhanced compression is available in pure Go&lt;/a&gt; (good for long storage).</target>
        </trans-unit>
        <trans-unit id="2d5145ba3bb813f1021b9547f2ec66b7026545d1" translate="yes" xml:space="preserve">
          <source>and do not supply any other argument, eg:</source>
          <target state="translated">and do not supply any other argument, eg:</target>
        </trans-unit>
        <trans-unit id="a9263357049c888ff916d6b7dabc806762fca344" translate="yes" xml:space="preserve">
          <source>and it will this
time rebuild from scratch, by autodetecting that you have Cython
installed, the setup.py script will automatically generate .c files from
.pyx files and then .pyd files (binaries) from .c files.</source>
          <target state="translated">and it will this time rebuild from scratch, by autodetecting that you have Cython installed, the setup.py script will automatically generate .c files from .pyx files and then .pyd files (binaries) from .c files.</target>
        </trans-unit>
        <trans-unit id="6d941eca9f507856fa5e3d86e7a398e08a4df200" translate="yes" xml:space="preserve">
          <source>conda install cython</source>
          <target state="translated">conda install cython</target>
        </trans-unit>
        <trans-unit id="6259964ff5d74ac8572203599c5e1a2fa8352f9a" translate="yes" xml:space="preserve">
          <source>dd_rescue: for disk scraping (allows to forcefully read a whole disk
at the bit level and copy everything it can, passing bad sector with
options to retry them later on after a first full pass over the
correct sectors).</source>
          <target state="translated">dd_rescue: for disk scraping (allows to forcefully read a whole disk at the bit level and copy everything it can, passing bad sector with options to retry them later on after a first full pass over the correct sectors).</target>
        </trans-unit>
        <trans-unit id="33b8b605ed144fcdfaf340fc749793422fbe4686" translate="yes" xml:space="preserve">
          <source>easy_profiler.py is just a quick and simple profiling tool to get
you started quickly on what should be optimized to get more speed, if
you want to contribute to the project feel free to propose a pull
request! (Cython and other optimizations are welcome as long as they
are cross-platform and that an alternative pure-python implementation
is also available).</source>
          <target state="translated">easy_profiler.py is just a quick and simple profiling tool to get you started quickly on what should be optimized to get more speed, if you want to contribute to the project feel free to propose a pull request! (Cython and other optimizations are welcome as long as they are cross-platform and that an alternative pure-python implementation is also available).</target>
        </trans-unit>
        <trans-unit id="5e5ecc8e51a29b4f3060f0b9e0659e6608b4f056" translate="yes" xml:space="preserve">
          <source>ecc.txt</source>
          <target state="translated">ecc.txt</target>
        </trans-unit>
        <trans-unit id="01beb827109e6e93b6f6fda0991505c0b3e946e6" translate="yes" xml:space="preserve">
          <source>ecc.txt.idx</source>
          <target state="translated">ecc.txt.idx</target>
        </trans-unit>
        <trans-unit id="a0979434b54d2e7d2f106f00946e5b49ce8faa5b" translate="yes" xml:space="preserve">
          <source>filetamper.py is a quickly made file corrupter, it will erase or
change characters in the specified file. This is useful for testing
your various protecting strategies and file formats (eg: is PAR2
really resilient against corruption? Are zip archives still partially
extractable after corruption or are rar archives better? etc.). Do
not underestimate the usefulness of this tool, as you should always
check the resiliency of your file formats and of your file protection
strategies before relying on them.</source>
          <target state="translated">filetamper.py is a quickly made file corrupter, it will erase or change characters in the specified file. This is useful for testing your various protecting strategies and file formats (eg: is PAR2 really resilient against corruption? Are zip archives still partially extractable after corruption or are rar archives better? etc.). Do not underestimate the usefulness of this tool, as you should always check the resiliency of your file formats and of your file protection strategies before relying on them.</target>
        </trans-unit>
        <trans-unit id="596eb263bba254c1fe8cf7495c9e5d1418053108" translate="yes" xml:space="preserve">
          <source>for RAID mounting and recovery, you can use &amp;ldquo;Raid faster - recover
better&amp;rdquo; (rfrb) tool by Sabine Seufert and Christian Zoubek:
&lt;a href=&quot;https://github.com/lrq3000/rfrb&quot;&gt;https://github.com/lrq3000/rfrb&lt;/a&gt;</source>
          <target state="translated">for RAID mounting and recovery, you can use &amp;ldquo;Raid faster - recover better&amp;rdquo; (rfrb) tool by Sabine Seufert and Christian Zoubek: &lt;a href=&quot;https://github.com/lrq3000/rfrb&quot;&gt;https://github.com/lrq3000/rfrb&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="28c85ceff7020974857f29484a7b9846c6dc68e5" translate="yes" xml:space="preserve">
          <source>for tar files, you can use &lt;a href=&quot;https://github.com/BestSolution-at/fixtar&quot;&gt;fixtar&lt;/a&gt;.
Similar tools (but older): &lt;a href=&quot;http://www.dmst.aueb.gr/dds/sw/unix/tarfix/&quot;&gt;tarfix&lt;/a&gt;
and &lt;a href=&quot;https://www.datanumen.com/tar-repair/&quot;&gt;tar-repair&lt;/a&gt;.</source>
          <target state="translated">for tar files, you can use &lt;a href=&quot;https://github.com/BestSolution-at/fixtar&quot;&gt;fixtar&lt;/a&gt;. Similar tools (but older): &lt;a href=&quot;http://www.dmst.aueb.gr/dds/sw/unix/tarfix/&quot;&gt;tarfix&lt;/a&gt; and &lt;a href=&quot;https://www.datanumen.com/tar-repair/&quot;&gt;tar-repair&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="6db36ad0628e827f1f4b773e89d4d6c524d812ff" translate="yes" xml:space="preserve">
          <source>future-proof (should be readable in the future).</source>
          <target state="translated">future-proof (should be readable in the future).</target>
        </trans-unit>
        <trans-unit id="74cd4d27e86588575092b3c1aa7978a6b4d84bfa" translate="yes" xml:space="preserve">
          <source>header_ecc.py</source>
          <target state="translated">header_ecc.py</target>
        </trans-unit>
        <trans-unit id="a38d03d9fb3c6fe27ef8824162536726350b50df" translate="yes" xml:space="preserve">
          <source>header_ecc.py, an error correction code using Reed-Solomon
generator/corrector for files headers. The idea is to supplement
other more common redundancy tools such as PAR2 (which is quite
reliable), by adding more resiliency only on the critical parts of
the files: their headers. Using this script, you can significantly
higher the chance of recovering headers, which will allow you to at
least open the files.</source>
          <target state="translated">header_ecc.py, an error correction code using Reed-Solomon generator/corrector for files headers. The idea is to supplement other more common redundancy tools such as PAR2 (which is quite reliable), by adding more resiliency only on the critical parts of the files: their headers. Using this script, you can significantly higher the chance of recovering headers, which will allow you to at least open the files.</target>
        </trans-unit>
        <trans-unit id="cc30060cf8e3d9624515816fc343f355770b21ad" translate="yes" xml:space="preserve">
          <source>hecc.txt</source>
          <target state="translated">hecc.txt</target>
        </trans-unit>
        <trans-unit id="2257c5714882349e7bf6dfecf7f991f90dcd2173" translate="yes" xml:space="preserve">
          <source>if your unicode strings were mangled (ie, you see weird symbols),
try this script that will automatically demangle them:
&lt;a href=&quot;https://github.com/LuminosoInsight/python-ftfy&quot;&gt;https://github.com/LuminosoInsight/python-ftfy&lt;/a&gt;</source>
          <target state="translated">if your unicode strings were mangled (ie, you see weird symbols), try this script that will automatically demangle them: &lt;a href=&quot;https://github.com/LuminosoInsight/python-ftfy&quot;&gt;https://github.com/LuminosoInsight/python-ftfy&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="64d1507160d799e54ec8b594c5339c8d826a794f" translate="yes" xml:space="preserve">
          <source>minimize corruption impact (ie, how much of the file becomes unreadable with a partial corruption? Only the partially corrupted area, or other valid parts too?).</source>
          <target state="translated">minimize corruption impact (ie, how much of the file becomes unreadable with a partial corruption? Only the partially corrupted area, or other valid parts too?).</target>
        </trans-unit>
        <trans-unit id="64871dda9c1ffc31c93a441974c2c0de3193a8a7" translate="yes" xml:space="preserve">
          <source>non-solid (ie, divised into indepedent blocks, so that a corruption to one block doesn&amp;rsquo;t cause a problem to the decoding of other blocks).</source>
          <target state="translated">non-solid (ie, divised into indepedent blocks, so that a corruption to one block doesn&amp;rsquo;t cause a problem to the decoding of other blocks).</target>
        </trans-unit>
        <trans-unit id="accaf3593fb517423b474f91010737449daae346" translate="yes" xml:space="preserve">
          <source>of pyFileFixity.
This repaired only the image header (ie, the first part of the file), so only the first
3 corrupted bytes were repaired, not the 2 bytes in the rest of the file, but we can see
the image looks like it&amp;rsquo;s totally repaired! And the best thing is that it only costed the generation
of a &amp;ldquo;ecc repair file&amp;rdquo;, which size is only 3.3KB (17% of the original file)!</source>
          <target state="translated">of pyFileFixity. This repaired only the image header (ie, the first part of the file), so only the first 3 corrupted bytes were repaired, not the 2 bytes in the rest of the file, but we can see the image looks like it&amp;rsquo;s totally repaired! And the best thing is that it only costed the generation of a &amp;ldquo;ecc repair file&amp;rdquo;, which size is only 3.3KB (17% of the original file)!</target>
        </trans-unit>
        <trans-unit id="cef8bd7c0fbe81a12737091e3e197e4248b60427" translate="yes" xml:space="preserve">
          <source>open source implementation available.</source>
          <target state="translated">open source implementation available.</target>
        </trans-unit>
        <trans-unit id="530394611ed9373d29a16765652818c55a9c6fe4" translate="yes" xml:space="preserve">
          <source>output_folder</source>
          <target state="translated">output_folder</target>
        </trans-unit>
        <trans-unit id="85f98d16f3491e5792978a6b56d844276ec92491" translate="yes" xml:space="preserve">
          <source>parameter from 0.0 to 1.0, 1.0 producing many false positives):</source>
          <target state="translated">parameter from 0.0 to 1.0, 1.0 producing many false positives):</target>
        </trans-unit>
        <trans-unit id="0461e35b8a2164a4a987c641a462ed32af4aea6b" translate="yes" xml:space="preserve">
          <source>python header_ecc.py -i &quot;your_folder&quot; -d &quot;hecc.txt&quot; -l &quot;log.txt&quot; -g-f--ecc_algo 3</source>
          <target state="translated">python header_ecc.py -i &quot;your_folder&quot; -d &quot;hecc.txt&quot; -l &quot;log.txt&quot; -g-f--ecc_algo 3</target>
        </trans-unit>
        <trans-unit id="5311b948ef61e6051a8b2ffb3c2895722941ad4f" translate="yes" xml:space="preserve">
          <source>python header_ecc.py -i &quot;your_folder&quot; -d &quot;hecc.txt&quot; -o &quot;output_folder&quot; -l &quot;log.txt&quot; -c-v--ecc_algo 3</source>
          <target state="translated">python header_ecc.py -i &quot;your_folder&quot; -d &quot;hecc.txt&quot; -o &quot;output_folder&quot; -l &quot;log.txt&quot; -c-v--ecc_algo 3</target>
        </trans-unit>
        <trans-unit id="8d81b293ec275c6ececde23091f58f458303878d" translate="yes" xml:space="preserve">
          <source>python repair_ecc.py -i &quot;ecc.txt&quot; --index &quot;ecc.txt.idx&quot; -o &quot;ecc_repaired.txt&quot; -l &quot;log.txt&quot; -v-f</source>
          <target state="translated">python repair_ecc.py -i &quot;ecc.txt&quot; --index &quot;ecc.txt.idx&quot; -o &quot;ecc_repaired.txt&quot; -l &quot;log.txt&quot; -v-f</target>
        </trans-unit>
        <trans-unit id="af031691cadc8de7561019d74b97c8940bd436e7" translate="yes" xml:space="preserve">
          <source>python repair_ecc.py -i &quot;ecc.txt&quot; -o &quot;ecc_repaired.txt&quot; -l &quot;log.txt&quot; -v-f-t 0.4</source>
          <target state="translated">python repair_ecc.py -i &quot;ecc.txt&quot; -o &quot;ecc_repaired.txt&quot; -l &quot;log.txt&quot; -v-f-t 0.4</target>
        </trans-unit>
        <trans-unit id="22fc6cfc75390f2a283e2bef971114152ccaec9a" translate="yes" xml:space="preserve">
          <source>python rfigc.py --gui</source>
          <target state="translated">python rfigc.py --gui</target>
        </trans-unit>
        <trans-unit id="f0b37c1c850dcc4290ff2d2a451f12dfe8ee96f5" translate="yes" xml:space="preserve">
          <source>python rfigc.py -i &quot;your_folder&quot; -d &quot;dbhash.csv&quot; -g</source>
          <target state="translated">python rfigc.py -i &quot;your_folder&quot; -d &quot;dbhash.csv&quot; -g</target>
        </trans-unit>
        <trans-unit id="79c347c6a6bd2573eb72053f5c66978d0286bd89" translate="yes" xml:space="preserve">
          <source>python rfigc.py -i &quot;your_folder&quot; -d &quot;dbhash.csv&quot; -g-f-l &quot;log.txt&quot;</source>
          <target state="translated">python rfigc.py -i &quot;your_folder&quot; -d &quot;dbhash.csv&quot; -g-f-l &quot;log.txt&quot;</target>
        </trans-unit>
        <trans-unit id="036de87e8ad75647104e86cd2564e63c37addbdb" translate="yes" xml:space="preserve">
          <source>python rfigc.py -i &quot;your_folder&quot; -d &quot;dbhash.csv&quot; -l &quot;log.txt&quot; -o &quot;output_folder&quot; --filescraping_recovery</source>
          <target state="translated">python rfigc.py -i &quot;your_folder&quot; -d &quot;dbhash.csv&quot; -l &quot;log.txt&quot; -o &quot;output_folder&quot; --filescraping_recovery</target>
        </trans-unit>
        <trans-unit id="7479e2d064266376bc7636f568aff61f214fd82f" translate="yes" xml:space="preserve">
          <source>python rfigc.py -i &quot;your_folder&quot; -d &quot;dbhash.csv&quot; -l log.txt -s</source>
          <target state="translated">python rfigc.py -i &quot;your_folder&quot; -d &quot;dbhash.csv&quot; -l log.txt -s</target>
        </trans-unit>
        <trans-unit id="060ba425ca5c314484f289f4107c42ae1df3618b" translate="yes" xml:space="preserve">
          <source>python rfigc.py -i &quot;your_folder&quot; -d &quot;dbhash.csv&quot; -l log.txt -s-e errors.csv</source>
          <target state="translated">python rfigc.py -i &quot;your_folder&quot; -d &quot;dbhash.csv&quot; -l log.txt -s-e errors.csv</target>
        </trans-unit>
        <trans-unit id="445c33a08c3aa8ebc012b2fe1eb165175af518a6" translate="yes" xml:space="preserve">
          <source>python rfigc.py -i &quot;your_folder&quot; -d &quot;dbhash.csv&quot; -u-a</source>
          <target state="translated">python rfigc.py -i &quot;your_folder&quot; -d &quot;dbhash.csv&quot; -u-a</target>
        </trans-unit>
        <trans-unit id="8b0a5c84c2ff658f5b528e40b383d08b4ad7b44e" translate="yes" xml:space="preserve">
          <source>python rfigc.py -i &quot;your_folder&quot; -d &quot;dbhash.csv&quot; -u-a-r</source>
          <target state="translated">python rfigc.py -i &quot;your_folder&quot; -d &quot;dbhash.csv&quot; -u-a-r</target>
        </trans-unit>
        <trans-unit id="b82ef1c1a743a5b1060a8876a740552ac87b3a62" translate="yes" xml:space="preserve">
          <source>python setup.py build_ext --inplace--compiler=msvc</source>
          <target state="translated">python setup.py build_ext --inplace--compiler=msvc</target>
        </trans-unit>
        <trans-unit id="32bc9abe900b776853a68dce08a6155db8de6c11" translate="yes" xml:space="preserve">
          <source>python structural_adaptive_ecc.py -i &quot;your_folder&quot; -d &quot;ecc.txt&quot; -l &quot;log.txt&quot; -g-f-v--ecc_algo 3</source>
          <target state="translated">python structural_adaptive_ecc.py -i &quot;your_folder&quot; -d &quot;ecc.txt&quot; -l &quot;log.txt&quot; -g-f-v--ecc_algo 3</target>
        </trans-unit>
        <trans-unit id="0a201400cbe18512bfe84e3a517d801b596d849c" translate="yes" xml:space="preserve">
          <source>python structural_adaptive_ecc.py -i &quot;your_folder&quot; -d &quot;ecc.txt&quot; -o &quot;output_folder&quot; -l &quot;log.txt&quot; -c-v--ecc_algo 3</source>
          <target state="translated">python structural_adaptive_ecc.py -i &quot;your_folder&quot; -d &quot;ecc.txt&quot; -o &quot;output_folder&quot; -l &quot;log.txt&quot; -c-v--ecc_algo 3</target>
        </trans-unit>
        <trans-unit id="406eba2344448c77414a56e8f45bcba1ba6b16d9" translate="yes" xml:space="preserve">
          <source>repair_ecc.py, a script to repair the structure (ie, the entry and
fields markers/separators) of an ecc file generated by header_ecc.py
or structural_adaptive_ecc.py. The goal is to enhance the
resilience of ecc files against corruption by ensuring that their
structures can be repaired (up to a certain point which is very high
if you use an index backup file, which is a companion file that is
generated along an ecc file).</source>
          <target state="translated">repair_ecc.py, a script to repair the structure (ie, the entry and fields markers/separators) of an ecc file generated by header_ecc.py or structural_adaptive_ecc.py. The goal is to enhance the resilience of ecc files against corruption by ensuring that their structures can be repaired (up to a certain point which is very high if you use an index backup file, which is a companion file that is generated along an ecc file).</target>
        </trans-unit>
        <trans-unit id="3f17cf616d54472869ddc7763fed960c1cb07624" translate="yes" xml:space="preserve">
          <source>replication_repair.py -i &quot;path/to/dir1&quot; &quot;path/to/dir2&quot; &quot;path/to/dir3&quot; -o &quot;path/to/output&quot; --report &quot;rlog.csv&quot; -f-v</source>
          <target state="translated">replication_repair.py -i &quot;path/to/dir1&quot; &quot;path/to/dir2&quot; &quot;path/to/dir3&quot; -o &quot;path/to/output&quot; --report &quot;rlog.csv&quot; -f-v</target>
        </trans-unit>
        <trans-unit id="5c5ee669302167b381a9425c68f4a121c41d3741" translate="yes" xml:space="preserve">
          <source>replication_repair.py -i &quot;path/to/dir1&quot; &quot;path/to/dir2&quot; &quot;path/to/dir3&quot; -o &quot;path/to/output&quot; -d &quot;dbhash.csv&quot; --report &quot;rlog.csv&quot; -f-v</source>
          <target state="translated">replication_repair.py -i &quot;path/to/dir1&quot; &quot;path/to/dir2&quot; &quot;path/to/dir3&quot; -o &quot;path/to/output&quot; -d &quot;dbhash.csv&quot; --report &quot;rlog.csv&quot; -f-v</target>
        </trans-unit>
        <trans-unit id="17b1ae3b36fa4ecb0cfa3805cded5f7fee4701eb" translate="yes" xml:space="preserve">
          <source>replication_repair.py takes advantage of your multiple copies
(replications) of your data over several storage mediums to recover
your data in case it gets corrupted. The goal is to take advantage of
the storage of your archived files into multiple locations: you will
necessarily make replications, so why not use them for repair?
Indeed, it&amp;rsquo;s good practice to keep several identical copies of your data
on several storage mediums, but in case a corruption happens,
usually you will just drop the corrupted copies and keep the intacts ones.
However, if all copies are partially corrupted, you&amp;rsquo;re stuck. This script
aims to take advantage of these multiple copies to recover your data,
without generating a prior ecc file. It works simply by reading through all
your different copies of your data, and it casts a majority vote over each
byte: the one that is the most often occuring will be kept. In engineering,
this is a very common strategy used for very reliable systems such as
space rockets, and is called &amp;ldquo;triple-modular redundancy&amp;rdquo;, because you need
at least 3 copies of your data for the majority vote to work (but the more the
better).</source>
          <target state="translated">replication_repair.py takes advantage of your multiple copies (replications) of your data over several storage mediums to recover your data in case it gets corrupted. The goal is to take advantage of the storage of your archived files into multiple locations: you will necessarily make replications, so why not use them for repair? Indeed, it&amp;rsquo;s good practice to keep several identical copies of your data on several storage mediums, but in case a corruption happens, usually you will just drop the corrupted copies and keep the intacts ones. However, if all copies are partially corrupted, you&amp;rsquo;re stuck. This script aims to take advantage of these multiple copies to recover your data, without generating a prior ecc file. It works simply by reading through all your different copies of your data, and it casts a majority vote over each byte: the one that is the most often occuring will be kept. In engineering, this is a very common strategy used for very reliable systems such as space rockets, and is called &amp;ldquo;triple-modular redundancy&amp;rdquo;, because you need at least 3 copies of your data for the majority vote to work (but the more the better).</target>
        </trans-unit>
        <trans-unit id="d3b05a74aec3330ab8ad12894abee9b565513d96" translate="yes" xml:space="preserve">
          <source>resiliency_tester.py -i &quot;your_folder&quot; -o &quot;test_folder&quot; -c &quot;resiliency_tester_config.txt&quot; -m 3 -l &quot;testlog.txt&quot; -f</source>
          <target state="translated">resiliency_tester.py -i &quot;your_folder&quot; -o &quot;test_folder&quot; -c &quot;resiliency_tester_config.txt&quot; -m 3 -l &quot;testlog.txt&quot; -f</target>
        </trans-unit>
        <trans-unit id="3d67a92bec14e09e8e8125c612716853cfc58499" translate="yes" xml:space="preserve">
          <source>resiliency_tester.py allows you to test the robustness of the
corruption correction of the scripts provided here (or any other
command-line app). You just have to copy the files you want to test inside a
folder, and then the script will copy the files into a test tree, then it
will automatically corrupt the files randomly (you can change the parameters
like block burst and others), then it will run the file repair command-lines
you supply and finally some stats about the repairing power will be
generated. This allows you to easily and objectively compare different set
of parameters, or even different file repair solutions, on the very data
that matters to you, so that you can pick the best option for you.</source>
          <target state="translated">resiliency_tester.py allows you to test the robustness of the corruption correction of the scripts provided here (or any other command-line app). You just have to copy the files you want to test inside a folder, and then the script will copy the files into a test tree, then it will automatically corrupt the files randomly (you can change the parameters like block burst and others), then it will run the file repair command-lines you supply and finally some stats about the repairing power will be generated. This allows you to easily and objectively compare different set of parameters, or even different file repair solutions, on the very data that matters to you, so that you can pick the best option for you.</target>
        </trans-unit>
        <trans-unit id="9cc99d1f646bec7b572c6d3902a8cd96a0c86f7e" translate="yes" xml:space="preserve">
          <source>rfigc.py</source>
          <target state="translated">rfigc.py</target>
        </trans-unit>
        <trans-unit id="a0c4e338b1f8861788ef00dad077299ba0a509ef" translate="yes" xml:space="preserve">
          <source>rfigc.py, a hash auditing tool, similar to md5deep/hashdeep, to
compute a database of your files along with their metadata, so that
later you can check if they were changed/corrupted.</source>
          <target state="translated">rfigc.py, a hash auditing tool, similar to md5deep/hashdeep, to compute a database of your files along with their metadata, so that later you can check if they were changed/corrupted.</target>
        </trans-unit>
        <trans-unit id="9cd7f2601adf86acd4a02c4dfdbac2fd1a09360a" translate="yes" xml:space="preserve">
          <source>rsbep tool that is part of dvbackup package in Debian: allows to
generate an ecc of a stream of bytes. Great to pipe to dar and/or gz
for your backups, if you&amp;rsquo;re on unix or using cygwin.</source>
          <target state="translated">rsbep tool that is part of dvbackup package in Debian: allows to generate an ecc of a stream of bytes. Great to pipe to dar and/or gz for your backups, if you&amp;rsquo;re on unix or using cygwin.</target>
        </trans-unit>
        <trans-unit id="c4124a163ce0ec751528aa6eadd553455300bea0" translate="yes" xml:space="preserve">
          <source>structural_adaptive_ecc.py</source>
          <target state="translated">structural_adaptive_ecc.py</target>
        </trans-unit>
        <trans-unit id="cdfd3d37e919c61baed83f2ce70b32a4dfebc80d" translate="yes" xml:space="preserve">
          <source>structural_adaptive_ecc.py, a variable error correction rate
encoder (kind of a generalization of header_ecc.py). This script
allows to generate an ecc file for the whole content of your files,
not just the header part, using a variable resilience rate: the
header part will be the most protected, then the rest of each file
will be progressively encoded with a smaller and smaller resilience
rate. The assumption is that important information is stored first,
and then data becomes less and less informative (and thus important,
because the end of the file describes less important details). This
assumption is very true for all compressed kinds of formats, such as
JPG, ZIP, Word, ODT, etc&amp;hellip;</source>
          <target state="translated">structural_adaptive_ecc.py, a variable error correction rate encoder (kind of a generalization of header_ecc.py). This script allows to generate an ecc file for the whole content of your files, not just the header part, using a variable resilience rate: the header part will be the most protected, then the rest of each file will be progressively encoded with a smaller and smaller resilience rate. The assumption is that important information is stored first, and then data becomes less and less informative (and thus important, because the end of the file describes less important details). This assumption is very true for all compressed kinds of formats, such as JPG, ZIP, Word, ODT, etc&amp;hellip;</target>
        </trans-unit>
        <trans-unit id="7c26a3c3b587adb57b5fc4e408c46dab4dc23134" translate="yes" xml:space="preserve">
          <source>switch.</source>
          <target state="translated">스위치.</target>
        </trans-unit>
        <trans-unit id="b47872ebb0177c597260f4df07a98eff8e554ea2" translate="yes" xml:space="preserve">
          <source>the second way, and the best, optimal tools ever invented to recover
from data corruption, are the &lt;strong&gt;error correction codes&lt;/strong&gt; (forward
error correction), which are a way to smartly produce redundant codes
from your data so that you can later repair your data using these
additional pieces of information (ie, an ECC generates n blocks for a
file cut in k blocks (with k &amp;lt; n), and then the ecc code can rebuild
the whole file with (at least) any k blocks among the total n blocks
available). In other words, you can correct up to (n-k) erasures. But
error correcting codes can also detect and repair automatically where
the errors are (fully automatic data repair for you !), but at the
cost that you can then only correct (n-k)/2 errors.</source>
          <target state="translated">the second way, and the best, optimal tools ever invented to recover from data corruption, are the &lt;strong&gt;error correction codes&lt;/strong&gt; (forward error correction), which are a way to smartly produce redundant codes from your data so that you can later repair your data using these additional pieces of information (ie, an ECC generates n blocks for a file cut in k blocks (with k &amp;lt; n), and then the ecc code can rebuild the whole file with (at least) any k blocks among the total n blocks available). In other words, you can correct up to (n-k) erasures. But error correcting codes can also detect and repair automatically where the errors are (fully automatic data repair for you !), but at the cost that you can then only correct (n-k)/2 errors.</target>
        </trans-unit>
        <trans-unit id="a13d0cc7bca199c6b60eeb5451af035336dd884b" translate="yes" xml:space="preserve">
          <source>the simple way to add redundancy is to &lt;strong&gt;duplicate&lt;/strong&gt; the object (also
called replication), but for data storage, this eats up a lot of storage
and is not optimal.</source>
          <target state="translated">the simple way to add redundancy is to &lt;strong&gt;duplicate&lt;/strong&gt; the object (also called replication), but for data storage, this eats up a lot of storage and is not optimal.</target>
        </trans-unit>
        <trans-unit id="a738a98a906b3e9a3f1fa90d6431b1bc3f13a73d" translate="yes" xml:space="preserve">
          <source>to repair tabular (2D) data such as .csv, try
&lt;a href=&quot;https://pypi.python.org/pypi/Carpenter/&quot;&gt;Carpenter&lt;/a&gt;.</source>
          <target state="translated">to repair tabular (2D) data such as .csv, try &lt;a href=&quot;https://pypi.python.org/pypi/Carpenter/&quot;&gt;Carpenter&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="941bfee74afa8502d4e9b6267e43746240802fa5" translate="yes" xml:space="preserve">
          <source>using an index file</source>
          <target state="translated">using an index file</target>
        </trans-unit>
        <trans-unit id="cc386d94fa083e1ebc35f812090fdab26e020536" translate="yes" xml:space="preserve">
          <source>without an index file (you can tweak the</source>
          <target state="translated">without an index file (you can tweak the</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
