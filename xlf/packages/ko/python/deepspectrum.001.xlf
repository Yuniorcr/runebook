<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="https://pypi.org/project/deepspectrum/">
    <body>
      <group id="deepspectrum">
        <trans-unit id="da39a3ee5e6b4b0d3255bfef95601890afd80709" translate="yes" xml:space="preserve">
          <source/>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1cb4bd88ddb5a8f0c8fe4b85b749da82d823ae72" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;(c) 2017-2020 Shahin Amiriparian, Maurice Gerczuk, Sandra Ottl, Bj&amp;ouml;rn Schuller: Universit&amp;auml;t Augsburg&lt;/strong&gt;
Published under GPLv3, see the LICENSE.md file for details.</source>
          <target state="translated">&lt;strong&gt;(c) 2017-2020 Shahin Amiriparian, Maurice Gerczuk, Sandra Ottl, Bj&amp;ouml;rn Schuller : Universit&amp;auml;t Augsburg&lt;/strong&gt; Published under GPLv3, 자세한 내용은 LICENSE.md 파일을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="ebac6302b62dd2f541f0cf3a23c13026bf32bedf" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;DeepSpectrum&lt;/strong&gt; is a Python toolkit for feature extraction from audio data with pre-trained Image Convolutional Neural Networks (CNNs). It features an extraction pipeline which first creates visual representations for audio data - plots of spectrograms or chromagrams - and then feeds them to a pre-trained Image CNN. Activations of a specific layer then form the final feature vectors.</source>
          <target state="translated">&lt;strong&gt;DeepSpectrum&lt;/strong&gt; 은 사전 훈련 된 &lt;strong&gt;CNN&lt;/strong&gt; (Image Convolutional Neural Networks)을 사용하여 오디오 데이터에서 특징을 추출하기위한 Python 툴킷입니다. 먼저 오디오 데이터 (스펙트로 그램 또는 크로마 그램의 플롯)에 대한 시각적 표현을 생성 한 다음 미리 훈련 된 이미지 CNN에 피드하는 추출 파이프 라인이 있습니다. 그런 다음 특정 레이어의 활성화가 최종 특징 벡터를 형성합니다.</target>
        </trans-unit>
        <trans-unit id="a73c505c2793bd025d626af6731d68b4a45269bf" translate="yes" xml:space="preserve">
          <source>Citing</source>
          <target state="translated">인용</target>
        </trans-unit>
        <trans-unit id="0c895889011b796b94426d854b65030179ae6add" translate="yes" xml:space="preserve">
          <source>If you use DeepSpectrum or any code from DeepSpectrum in your research work, you are kindly asked to acknowledge the use of DeepSpectrum in your publications.</source>
          <target state="translated">연구 작업에서 DeepSpectrum 또는 DeepSpectrum의 코드를 사용하는 경우 출판물에서 DeepSpectrum을 사용하는 것을 인정해 주시기 바랍니다.</target>
        </trans-unit>
        <trans-unit id="6c8bfe5d094dd6fcbf34f0e0a0c079551ad219fd" translate="yes" xml:space="preserve">
          <source>Please direct any questions or requests to Shahin Amiriparian (shahin.amiriparian at tum.de) or Maurice Gercuk (maurice.gerczuk at informatik.uni-augsburg.de).</source>
          <target state="translated">질문이나 요청은 Shahin Amiriparian (tum.de의 shahin.amiriparian) 또는 Maurice Gercuk (informatik.uni-augsburg.de의 maurice.gerczuk)로 직접 보내주십시오.</target>
        </trans-unit>
        <trans-unit id="171a1e4c9e1f5ec9f1fe0c8e4188ee2bb2b1bb0a" translate="yes" xml:space="preserve">
          <source>S. Amiriparian, M. Gerczuk, S. Ottl, N. Cummins, M. Freitag, S. Pugachevskiy, A. Baird and B. Schuller. Snore Sound Classification using Image-Based Deep Spectrum Features. In Proceedings of INTERSPEECH (Vol. 17, pp. 2017-434)</source>
          <target state="translated">S. Amiriparian, M. Gerczuk, S. Ottl, N. Cummins, M. Freitag, S. Pugachevskiy, A. Baird 및 B. Schuller. 이미지 기반 심층 스펙트럼 기능을 사용한 코골이 소리 분류. INTERSPEECH의 회보에서 (Vol. 17, pp. 2017-434)</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
