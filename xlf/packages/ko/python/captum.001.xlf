<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="https://pypi.org/project/captum/">
    <body>
      <group id="captum">
        <trans-unit id="7e224d6675fc514ea7defb9516638683fceb2bcc" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;Deconvolution&lt;/code&gt;, &lt;code&gt;Neuron Deconvolution&lt;/code&gt;: &lt;a href=&quot;https://arxiv.org/pdf/1311.2901.pdf&quot;&gt;Visualizing and Understanding Convolutional Networks, Matthew D Zeiler et al. 2014&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;Deconvolution&lt;/code&gt; , &lt;code&gt;Neuron Deconvolution&lt;/code&gt; : &lt;a href=&quot;https://arxiv.org/pdf/1311.2901.pdf&quot;&gt;Convolutional Networks 시각화 및 이해, Matthew D Zeiler et al. 2014 년&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f77c45466355b115b010aa83e3ec5115e53cc011" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;DeepLift&lt;/code&gt; assigns similar attribution scores as &lt;code&gt;IntegratedGradients&lt;/code&gt; to inputs,
however it has lower execution time. Another important thing to remember about
DeepLift is that it currently doesn't support all non-linear activation types.
For more details on limitations of the current implementation, please see the
&lt;a href=&quot;https://arxiv.org/abs/1704.02685&quot;&gt;DeepLift paper&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;DeepLift&lt;/code&gt; 는 입력에 &lt;code&gt;IntegratedGradients&lt;/code&gt; 와 유사한 속성 점수를 할당 하지만 실행 시간이 더 짧 습니다. DeepLift에 대해 기억해야 할 또 다른 중요한 점은 현재 모든 비선형 활성화 유형을 지원하지 않는다는 것입니다. 현재 구현의 제한 사항에 대한 자세한 내용은 &lt;a href=&quot;https://arxiv.org/abs/1704.02685&quot;&gt;DeepLift 문서&lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="35d8eea45bfc985cc4e9ed2b9f13dcc77a172786" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;DeepLift&lt;/code&gt;, &lt;code&gt;NeuronDeepLift&lt;/code&gt;, &lt;code&gt;LayerDeepLift&lt;/code&gt;: &lt;a href=&quot;https://arxiv.org/pdf/1704.02685.pdf&quot;&gt;Learning Important Features Through Propagating Activation Differences, Avanti Shrikumar et al. 2017&lt;/a&gt; and &lt;a href=&quot;https://openreview.net/pdf?id=Sy21R9JAW&quot;&gt;Towards better understanding of gradient-based attribution methods for deep neural networks, Marco Ancona et al. 2018&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;DeepLift&lt;/code&gt; , &lt;code&gt;NeuronDeepLift&lt;/code&gt; , &lt;code&gt;LayerDeepLift&lt;/code&gt; : &lt;a href=&quot;https://arxiv.org/pdf/1704.02685.pdf&quot;&gt;활성화 차이 전파를 통해 중요한 기능 학습, Avanti Shrikumar et al. 2017 년&lt;/a&gt; 및 &lt;a href=&quot;https://openreview.net/pdf?id=Sy21R9JAW&quot;&gt;심층 신경망을위한 그래디언트 기반 어트 리뷰 션 방법에 대한 더 나은 이해를 향하여 Marco Ancona et al. 2018 년&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0935b98602a2196eb3ce841148929580db331f26" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;DeepLiftShap&lt;/code&gt; uses &lt;code&gt;DeepLift&lt;/code&gt; to compute attribution score for each
input-baseline pair and averages it for each input across all baselines.</source>
          <target state="translated">&lt;code&gt;DeepLiftShap&lt;/code&gt; 은 &lt;code&gt;DeepLift&lt;/code&gt; 를 사용하여 각 입력 기준선 쌍에 대한 어트 리뷰 션 점수를 계산하고 모든 기준선에서 각 입력에 대해 평균을냅니다.</target>
        </trans-unit>
        <trans-unit id="70f4204e74fbab884413f100ee93ebc43bf69602" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;Feature Permutation&lt;/code&gt;: &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/feature-importance.html&quot;&gt;Permutation Feature Importance&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;Feature Permutation&lt;/code&gt; : &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/feature-importance.html&quot;&gt;순열 기능 중요성&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="dfd45f4166b5d1094537f54fcf044b34d5f360af" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;GradCAM&lt;/code&gt;, &lt;code&gt;Guided GradCAM&lt;/code&gt;: &lt;a href=&quot;https://arxiv.org/abs/1610.02391.pdf&quot;&gt;Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization, Ramprasaath R. Selvaraju et al. 2017&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;GradCAM&lt;/code&gt; , &lt;code&gt;Guided GradCAM&lt;/code&gt; : &lt;a href=&quot;https://arxiv.org/abs/1610.02391.pdf&quot;&gt;Grad-CAM : Gradient-based Localization을 통한 Deep Networks의 시각적 설명, Ramprasaath R. Selvaraju et al. 2017 년&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9c73c2db449b0b144f4acb50ee0a1570caed5366" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;GradientShap&lt;/code&gt; first chooses a random baseline from baselines' distribution, then
adds gaussian noise with std=0.09 to each input example &lt;code&gt;n_samples&lt;/code&gt; times.
Afterwards, it chooses a random point between each example-baseline pair and
computes the gradients with respect to target class (in this case target=0). Resulting
attribution is the mean of gradients * (inputs - baselines)</source>
          <target state="translated">&lt;code&gt;GradientShap&lt;/code&gt; 은 먼저 기준선 분포에서 임의 기준선을 선택한 다음 std = 0.09 인 가우스 노이즈를 각 입력 예제에 &lt;code&gt;n_samples&lt;/code&gt; 번 추가합니다. 그 후, 각 예제 기준선 쌍 사이의 임의의 점을 선택하고 대상 클래스 (이 경우 target = 0)에 대한 기울기를 계산합니다. 결과 속성은 기울기의 평균입니다 * (입력-기준선)</target>
        </trans-unit>
        <trans-unit id="745f47441809340b71cc76401a089bf5ad757fd9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;GradientShap&lt;/code&gt;, &lt;code&gt;NeuronGradientShap&lt;/code&gt;, &lt;code&gt;LayerGradientShap&lt;/code&gt;, &lt;code&gt;DeepLiftShap&lt;/code&gt;, &lt;code&gt;NeuronDeepLiftShap&lt;/code&gt;, &lt;code&gt;LayerDeepLiftShap&lt;/code&gt;: &lt;a href=&quot;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions&quot;&gt;A Unified Approach to Interpreting Model Predictions, Scott M. Lundberg et al. 2017&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;GradientShap&lt;/code&gt; , &lt;code&gt;NeuronGradientShap&lt;/code&gt; , &lt;code&gt;LayerGradientShap&lt;/code&gt; , &lt;code&gt;DeepLiftShap&lt;/code&gt; , &lt;code&gt;NeuronDeepLiftShap&lt;/code&gt; , &lt;code&gt;LayerDeepLiftShap&lt;/code&gt; : &lt;a href=&quot;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions&quot;&gt;모델 예측 해석에 대한 통합 접근 방식, Scott M. Lundberg et al. 2017 년&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="602dfc33d8aba3857725b949b893d500e50a678d" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;Guided Backpropagation&lt;/code&gt;, &lt;code&gt;Neuron Guided Backpropagation&lt;/code&gt;: &lt;a href=&quot;https://arxiv.org/pdf/1412.6806.pdf&quot;&gt;Striving for Simplicity: The All Convolutional Net, Jost Tobias Springenberg et al. 2015&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;Guided Backpropagation&lt;/code&gt; , &lt;code&gt;Neuron Guided Backpropagation&lt;/code&gt; : &lt;a href=&quot;https://arxiv.org/pdf/1412.6806.pdf&quot;&gt;단순성을위한 노력 : 모든 컨볼 루션 네트워크, Jost Tobias Springenberg et al. 2015 년&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="32d5dd2bb7640e2b606a6084f2cafa2a031b478b" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;InputXGradient&lt;/code&gt;: &lt;a href=&quot;https://arxiv.org/abs/1611.07270&quot;&gt;Investigating the influence of noise and distractors on the interpretation of neural networks, Pieter-Jan Kindermans et al. 2016&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;InputXGradient&lt;/code&gt; : &lt;a href=&quot;https://arxiv.org/abs/1611.07270&quot;&gt;신경망 해석에 대한 잡음 및 산만 요소의 영향 조사, Pieter-Jan Kindermans et al. 2016 년&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b69ede686e5d4e904dbf0247c2f663ff896f0a0c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;IntegratedGradients&lt;/code&gt;, &lt;code&gt;LayerIntegratedGradients&lt;/code&gt;: &lt;a href=&quot;https://arxiv.org/abs/1703.01365&quot;&gt;Axiomatic Attribution for Deep Networks, Mukund Sundararajan et al. 2017&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1805.05492&quot;&gt;Did the Model Understand the Question?, Pramod K. Mudrakarta, et al. 2018&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;IntegratedGradients&lt;/code&gt; , &lt;code&gt;LayerIntegratedGradients&lt;/code&gt; : &lt;a href=&quot;https://arxiv.org/abs/1703.01365&quot;&gt;심층 네트워크에 대한 공리적 속성, Mukund Sundararajan et al. 2017&lt;/a&gt; 및 &lt;a href=&quot;https://arxiv.org/abs/1805.05492&quot;&gt;모델이 질문을 이해 했습니까?, Pramod K. Mudrakarta, et al. 2018 년&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1ce041e0904e2eb6fe9e166ae16f4a94c041216f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;InternalInfluence&lt;/code&gt;: &lt;a href=&quot;https://arxiv.org/pdf/1802.03788.pdf&quot;&gt;Influence-Directed Explanations for Deep Convolutional Networks, Klas Leino et al. 2018&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;InternalInfluence&lt;/code&gt; : &lt;a href=&quot;https://arxiv.org/pdf/1802.03788.pdf&quot;&gt;Deep Convolutional Networks에 대한 영향 지향 설명, Klas Leino et al. 2018 년&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6464d80d482910b859e98e94be43a85d78c92fff" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;LayerConductance&lt;/code&gt;: &lt;a href=&quot;https://arxiv.org/pdf/1807.09946.pdf&quot;&gt;Computationally Efficient Measures of Internal Neuron Importance, Avanti Shrikumar et al. 2018&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;LayerConductance&lt;/code&gt; : &lt;a href=&quot;https://arxiv.org/pdf/1807.09946.pdf&quot;&gt;내부 뉴런 중요성의 &lt;/a&gt;계산적으로 효율적인 측정, Avanti Shrikumar et al. 2018 년</target>
        </trans-unit>
        <trans-unit id="a390e01ea3142478285f0168a9d0220de3b869e1" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;NeuronConductance&lt;/code&gt;: &lt;a href=&quot;https://arxiv.org/abs/1805.12233&quot;&gt;How Important is a neuron?, Kedar Dhamdhere et al. 2018&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;NeuronConductance&lt;/code&gt; : &lt;a href=&quot;https://arxiv.org/abs/1805.12233&quot;&gt;뉴런은 얼마나 중요합니까?, Kedar Dhamdhere et al. 2018 년&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7fa53a48f5e924963274d272e5ae392af89f2c41" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;NeuronIntegratedGradients&lt;/code&gt;: &lt;a href=&quot;https://arxiv.org/pdf/1807.09946.pdf&quot;&gt;Computationally Efficient Measures of Internal Neuron Importance, Avanti Shrikumar et al. 2018&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;NeuronIntegratedGradients&lt;/code&gt; : &lt;a href=&quot;https://arxiv.org/pdf/1807.09946.pdf&quot;&gt;내부 뉴런 중요도의 &lt;/a&gt;계산적으로 효율적인 측정, Avanti Shrikumar et al. 2018 년</target>
        </trans-unit>
        <trans-unit id="3406337d51997da5374b792bb810c3069f71f625" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;NoiseTunnel&lt;/code&gt;: &lt;a href=&quot;https://arxiv.org/abs/1810.03292&quot;&gt;Sanity Checks for Saliency Maps, Julius Adebayo et al. 2018&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;NoiseTunnel&lt;/code&gt; : Saliency &lt;a href=&quot;https://arxiv.org/abs/1810.03292&quot;&gt;Maps에 대한 Sanity Checks, Julius Adebayo et al. 2018 년&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="180de40fda30abd90e30c91da063de517cd65e10" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;Occlusion&lt;/code&gt;: &lt;a href=&quot;https://arxiv.org/abs/1311.2901&quot;&gt;Visualizing and Understanding Convolutional Networks&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;Occlusion&lt;/code&gt; : &lt;a href=&quot;https://arxiv.org/abs/1311.2901&quot;&gt;컨볼 루션 네트워크 시각화 및 이해&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ef3e53fd11ec0ec42f34212a68a5a8f640a980a4" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;Saliency&lt;/code&gt;, &lt;code&gt;NeuronGradient&lt;/code&gt;: &lt;a href=&quot;https://arxiv.org/pdf/1312.6034.pdf&quot;&gt;Deep Inside Convolutional Networks: Visualising
Image Classification Models and Saliency Maps, K. Simonyan, et. al. 2014&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;Saliency&lt;/code&gt; , &lt;code&gt;NeuronGradient&lt;/code&gt; : &lt;a href=&quot;https://arxiv.org/pdf/1312.6034.pdf&quot;&gt;깊은 내부 길쌈 네트워크 : 시각화 이미지 분류 모델 및 돌출지도, K. Simonyan, 등. al. 2014 년&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d72716650eca9202bde62762d848353fd9f89e55" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;Shapely Value Sampling&lt;/code&gt;: &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0305054808000804&quot;&gt;Polynomial calculation of the Shapley value based on sampling&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;Shapely Value Sampling&lt;/code&gt; : &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0305054808000804&quot;&gt;샘플링을 기반으로 한 Shapley 값의 다항식 계산&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2f313a5b98f914e2d22e3666af676c317da1a76c" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;Shapely Value&lt;/code&gt;: &lt;a href=&quot;https://apps.dtic.mil/dtic/tr/fulltext/u2/604084.pdf&quot;&gt;A value for n-person games. Contributions to the Theory of Games 2.28 (1953): 307-317&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;Shapely Value&lt;/code&gt; : &lt;a href=&quot;https://apps.dtic.mil/dtic/tr/fulltext/u2/604084.pdf&quot;&gt;n 인 게임의 가치. 게임 이론에 대한 기여 2.28 (1953) : 307-317&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cc62bf28a6aaadab7c58506486d1282344c4f152" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;SmoothGrad&lt;/code&gt;: &lt;a href=&quot;https://arxiv.org/abs/1706.03825&quot;&gt;SmoothGrad: removing noise by adding noise, Daniel Smilkov et al. 2017&lt;/a&gt;</source>
          <target state="translated">&lt;code&gt;SmoothGrad&lt;/code&gt; : &lt;a href=&quot;https://arxiv.org/abs/1706.03825&quot;&gt;SmoothGrad : 노이즈를 추가하여 노이즈 제거, Daniel Smilkov et al. 2017 년&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="97d9a83b33d1a1e1468782cbf91b17ba820547f9" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;pip install -e .[dev]&lt;/code&gt;: Also installs all tools necessary for development
(testing, linting, docs building; see &lt;a href=&quot;#contributing&quot;&gt;Contributing&lt;/a&gt; below).</source>
          <target state="translated">&lt;code&gt;pip install -e .[dev]&lt;/code&gt; : 또한 개발에 필요한 모든 도구를 설치합니다 (테스트, linting, 문서 작성, 아래 &lt;a href=&quot;#contributing&quot;&gt;기여&lt;/a&gt; 참조).</target>
        </trans-unit>
        <trans-unit id="5d7c7835d367a7e4679627572238c42726e0601e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;pip install -e .[insights]&lt;/code&gt;: Also installs all packages necessary for running Captum Insights.</source>
          <target state="translated">&lt;code&gt;pip install -e .[insights]&lt;/code&gt; : Captum Insights 실행에 필요한 모든 패키지도 설치합니다.</target>
        </trans-unit>
        <trans-unit id="d5bb8d5b6fa5ab568327e9ea794621e7ee38d45f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;pip install -e .[tutorials]&lt;/code&gt;: Also installs all packages necessary for running the tutorial notebooks.</source>
          <target state="translated">&lt;code&gt;pip install -e .[tutorials]&lt;/code&gt; : 튜토리얼 노트북을 실행하는 데 필요한 모든 패키지도 설치합니다.</target>
        </trans-unit>
        <trans-unit id="c641fcc1b80e58fb730b1652c8b6c7fc9b227fd8" translate="yes" xml:space="preserve">
          <source>&lt;em&gt;Captum is currently in beta and under active development!&lt;/em&gt;</source>
          <target state="translated">&lt;em&gt;Captum은 현재 베타 버전이며 현재 개발 중입니다!&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="6897acde3a3c220a635980f413f5941e5fc0a6b4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Installation Requirements&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;설치 요구 사항&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="2208733c3a7ee98009723d2713bcd61e00a43c9c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;Manual / Dev install&lt;/strong&gt;</source>
          <target state="translated">&lt;strong&gt;수동 / 개발 설치&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="c3f76124c7fa391369fe5c7fb44c51cf9d1ed726" translate="yes" xml:space="preserve">
          <source>About Captum</source>
          <target state="translated">Captum 정보</target>
        </trans-unit>
        <trans-unit id="d8c0847ea163dcd16d6f2a889df7dce1e95c7176" translate="yes" xml:space="preserve">
          <source>Below is an example of how we can apply &lt;code&gt;DeepLift&lt;/code&gt; and &lt;code&gt;DeepLiftShap&lt;/code&gt; on the
&lt;code&gt;ToyModel&lt;/code&gt; described above. Current implementation of DeepLift supports only
&lt;code&gt;Rescale&lt;/code&gt; rule.
For more details on alternative implementations, please see the &lt;a href=&quot;https://arxiv.org/abs/1704.02685&quot;&gt;DeepLift paper&lt;/a&gt;.</source>
          <target state="translated">다음은 우리가 적용 할 수있는 방법의 예입니다 &lt;code&gt;DeepLift&lt;/code&gt; 및 &lt;code&gt;DeepLiftShap&lt;/code&gt; 을 온 &lt;code&gt;ToyModel&lt;/code&gt; 는 위의 설명은. 현재 DeepLift 구현은 &lt;code&gt;Rescale&lt;/code&gt; 규칙 만 지원합니다 . 대체 구현에 대한 자세한 내용은 &lt;a href=&quot;https://arxiv.org/abs/1704.02685&quot;&gt;DeepLift 문서&lt;/a&gt; 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="021481ec74cf7f851fd8bb8de29f48519b026a55" translate="yes" xml:space="preserve">
          <source>Captum Insights</source>
          <target state="translated">Captum Insights</target>
        </trans-unit>
        <trans-unit id="b5e6347c0220e027a895cf1796661a4eca041832" translate="yes" xml:space="preserve">
          <source>Captum Insights Jupyter Widget</source>
          <target state="translated">Captum Insights Jupyter 위젯</target>
        </trans-unit>
        <trans-unit id="50ea9b39e0ca2b1e147f71e19c8284e932f29b95" translate="yes" xml:space="preserve">
          <source>Captum Insights also has a Jupyter widget providing the same user interface as the web app.
To install and enable the widget, run</source>
          <target state="translated">Captum Insights에는 웹 앱과 동일한 사용자 인터페이스를 제공하는 Jupyter 위젯도 있습니다. 위젯을 설치하고 사용하려면 다음을 실행하십시오.</target>
        </trans-unit>
        <trans-unit id="2284f9438ef0b1f48a107c6a47037d5562ef7a07" translate="yes" xml:space="preserve">
          <source>Captum can also be used by application engineers who are using trained models in production. Captum provides easier troubleshooting through improved model interpretability, and the potential for delivering better explanations to end users on why they&amp;rsquo;re seeing a specific piece of content, such as a movie recommendation.</source>
          <target state="translated">Captum은 프로덕션에서 훈련 된 모델을 사용하는 애플리케이션 엔지니어도 사용할 수 있습니다. Captum은 개선 된 모델 해석 성을 통해보다 쉬운 문제 해결을 제공하고 최종 사용자에게 영화 추천과 같은 특정 콘텐츠가 표시되는 이유에 대해 더 나은 설명을 제공 할 수있는 잠재력을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="e072252aa5db22cac92d10e2402e5669eae223d4" translate="yes" xml:space="preserve">
          <source>Captum helps ML researchers more easily implement interpretability algorithms that can interact with PyTorch models. Captum also allows researchers to quickly benchmark their work against other existing algorithms available in the library.</source>
          <target state="translated">Captum은 ML 연구원이 PyTorch 모델과 상호 작용할 수있는 해석 알고리즘을보다 쉽게 ​​구현할 수 있도록 지원합니다. Captum을 사용하면 연구원이 라이브러리에서 사용 가능한 다른 기존 알고리즘과 비교하여 작업을 빠르게 벤치마킹 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d26bdb66327c68c373a8e184aae92f50105b84ad" translate="yes" xml:space="preserve">
          <source>Captum helps you interpret and understand predictions of PyTorch models by
exploring features that contribute to a prediction the model makes.
It also helps understand which neurons and layers are important for
model predictions.</source>
          <target state="translated">Captum은 모델의 예측에 기여하는 기능을 탐색하여 PyTorch 모델의 예측을 해석하고 이해하는 데 도움이됩니다. 또한 모델 예측에 중요한 뉴런과 계층을 이해하는 데 도움이됩니다.</target>
        </trans-unit>
        <trans-unit id="48db440dde81207b626929193aa5ff14516a272f" translate="yes" xml:space="preserve">
          <source>Captum is BSD licensed, as found in the &lt;a href=&quot;LICENSE&quot;&gt;LICENSE&lt;/a&gt; file.</source>
          <target state="translated">Captum은 &lt;a href=&quot;LICENSE&quot;&gt;LICENSE&lt;/a&gt; 파일 에있는 BSD 라이선스 입니다.</target>
        </trans-unit>
        <trans-unit id="2b556e02df9343beb45e9c79545ddba2410d2664" translate="yes" xml:space="preserve">
          <source>Captum is a model interpretability and understanding library for PyTorch.
Captum means comprehension in latin and contains general purpose implementations
of integrated gradients, saliency maps, smoothgrad, vargrad and others for
PyTorch models. It has quick integration for models built with domain-specific
libraries such as torchvision, torchtext, and others.</source>
          <target state="translated">Captum은 PyTorch를위한 모델 해석 및 이해 라이브러리입니다. Captum은 라틴어로 이해를 의미하며 PyTorch 모델에 대한 통합 그라디언트, 돌출 맵, smoothgrad, vargrad 및 기타의 범용 구현을 포함합니다. torchvision, torchtext 등과 같은 도메인 별 라이브러리로 빌드 된 모델에 대한 빠른 통합이 있습니다.</target>
        </trans-unit>
        <trans-unit id="712c9c0dc955ee835f95d4a0c2e34f430fb91038" translate="yes" xml:space="preserve">
          <source>Captum provides a web interface called Insights for easy visualization and
access to a number of our interpretability algorithms.</source>
          <target state="translated">Captum은 쉽게 시각화하고 다양한 해석 알고리즘에 액세스 할 수 있도록 Insights라는 웹 인터페이스를 제공합니다.</target>
        </trans-unit>
        <trans-unit id="2d82a4b27a4b305690d5ac612046a955778a9fa5" translate="yes" xml:space="preserve">
          <source>Contributing</source>
          <target state="translated">기여</target>
        </trans-unit>
        <trans-unit id="39703b21af9911d0f81c306612570543015fb4da" translate="yes" xml:space="preserve">
          <source>Currently, the library uses gradient-based interpretability algorithms
and attributes contributions to each input of the model with respect to
different neurons and layers, both intermediate and final.</source>
          <target state="translated">현재 라이브러리는 기울기 기반 해석 알고리즘을 사용하고 중간 및 최종의 서로 다른 뉴런 및 레이어와 관련하여 모델의 각 입력에 기여도를 부여합니다.</target>
        </trans-unit>
        <trans-unit id="90ab72c08753cb3eced44073f94e601929bbf4a1" translate="yes" xml:space="preserve">
          <source>Deltas are computed for each &lt;code&gt;n_samples * input.shape[0]&lt;/code&gt; example. The user can,
for instance, average them:</source>
          <target state="translated">각 &lt;code&gt;n_samples * input.shape[0]&lt;/code&gt; 예제 에 대해 델타가 계산됩니다 . 예를 들어 사용자는 다음과 같이 평균 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c6b820410c0912f7db59b8a701baa4c5631073ed" translate="yes" xml:space="preserve">
          <source>For model developers, Captum can be used to improve and troubleshoot models by facilitating the identification of different features that contribute to a model&amp;rsquo;s output in order to design better models and troubleshoot unexpected model outputs.</source>
          <target state="translated">모델 개발자의 경우 Captum을 사용하면 더 나은 모델을 설계하고 예상치 못한 모델 출력 문제를 해결하기 위해 모델 출력에 기여하는 다양한 기능을 쉽게 식별하여 모델을 개선하고 문제를 해결할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="010b85ad56b34c34c7c2a3b2436c740e30428ed5" translate="yes" xml:space="preserve">
          <source>Getting Started</source>
          <target state="translated">시작하기</target>
        </trans-unit>
        <trans-unit id="523b5dd24d78e1688f499261e03d11aabe65c0e9" translate="yes" xml:space="preserve">
          <source>Here is an example how we can use &lt;code&gt;NoiseTunnel&lt;/code&gt; with &lt;code&gt;IntegratedGradients&lt;/code&gt;.</source>
          <target state="translated">다음은 &lt;code&gt;IntegratedGradients&lt;/code&gt; 와 함께 &lt;code&gt;NoiseTunnel&lt;/code&gt; 을 사용하는 방법의 예 입니다.</target>
        </trans-unit>
        <trans-unit id="2951f06ff53b46d9157f54d1ddb461c159a670d0" translate="yes" xml:space="preserve">
          <source>If you'd like to try our bleeding edge features (and don't mind potentially
running into the occasional bug here or there), you can install the latest
master directly from GitHub. For a basic install, run:</source>
          <target state="translated">블리딩 엣지 기능을 사용해보고 싶다면 (그리고 여기나 거기에서 가끔씩 발생하는 버그가 발생할 가능성이있는 경우) GitHub에서 직접 최신 마스터를 설치할 수 있습니다. 기본 설치의 경우 다음을 실행하십시오.</target>
        </trans-unit>
        <trans-unit id="748139302b5f33a6af11cc4d8e2c307feed6a61f" translate="yes" xml:space="preserve">
          <source>In order to smooth and improve the quality of the attributions we can run
&lt;code&gt;IntegratedGradients&lt;/code&gt; and other attribution methods through a &lt;code&gt;NoiseTunnel&lt;/code&gt;.
&lt;code&gt;NoiseTunnel&lt;/code&gt; allows us to use &lt;code&gt;SmoothGrad&lt;/code&gt;, &lt;code&gt;SmoothGrad_Sq&lt;/code&gt; and &lt;code&gt;VarGrad&lt;/code&gt; techniques
to smoothen the attributions by aggregating them for multiple noisy
samples that were generated by adding gaussian noise.</source>
          <target state="translated">&lt;code&gt;NoiseTunnel&lt;/code&gt; 의 품질을 매끄럽고 개선하기 위해 NoiseTunnel을 통해 &lt;code&gt;IntegratedGradients&lt;/code&gt; 및 기타 어트 리뷰 션 방법을 실행할 수 있습니다 . &lt;code&gt;NoiseTunnel&lt;/code&gt; 을 사용하면 &lt;code&gt;SmoothGrad&lt;/code&gt; , &lt;code&gt;SmoothGrad_Sq&lt;/code&gt; 및 &lt;code&gt;VarGrad&lt;/code&gt; 기술을 사용하여 가우스 노이즈를 추가하여 생성 된 여러 노이즈 샘플에 대해 집계하여 속성을 부드럽게 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="37e62d8605abf42b9cf777a51788c55a03639d78" translate="yes" xml:space="preserve">
          <source>In this case, we choose to analyze the first neuron in the linear layer.</source>
          <target state="translated">이 경우 선형 계층의 첫 번째 뉴런을 분석하도록 선택합니다.</target>
        </trans-unit>
        <trans-unit id="c81b79df3c6448eae7c4f80428b54cd5692a17d7" translate="yes" xml:space="preserve">
          <source>Installation</source>
          <target state="translated">설치</target>
        </trans-unit>
        <trans-unit id="726495cac31fed46f3681097ea8adbc806081e82" translate="yes" xml:space="preserve">
          <source>Installing the latest release</source>
          <target state="translated">최신 릴리스 설치</target>
        </trans-unit>
        <trans-unit id="8e78f6ced3cb3e7a0efd619bef2cccb4fa16b14d" translate="yes" xml:space="preserve">
          <source>It computes deltas for each input example-baseline pair, thus resulting to
&lt;code&gt;input.shape[0] * baseline.shape[0]&lt;/code&gt; delta values.</source>
          <target state="translated">각 입력 예제-기준선 쌍에 대한 델타를 계산하므로 &lt;code&gt;input.shape[0] * baseline.shape[0]&lt;/code&gt; 델타 값이됩니다.</target>
        </trans-unit>
        <trans-unit id="a8a7e2465fdc77cafbda6ce7b7d0e3ab7010ee86" translate="yes" xml:space="preserve">
          <source>It doesn't attribute the contribution scores to the input features
but shows the importance of each neuron in selected layer.</source>
          <target state="translated">기여 점수를 입력 기능에 부여하는 것이 아니라 선택한 레이어에서 각 뉴런의 중요성을 보여줍니다.</target>
        </trans-unit>
        <trans-unit id="45f74c2b886455a20211bc6087c7a981cba8b94d" translate="yes" xml:space="preserve">
          <source>Layer conductance shows the importance of neurons for a layer and given input.
It is an extension of path integrated gradients for hidden layers and holds the
completeness property as well.</source>
          <target state="translated">레이어 전도도는 레이어와 주어진 입력에 대한 뉴런의 중요성을 보여줍니다. 숨겨진 레이어에 대한 경로 통합 그라디언트의 확장이며 완전성 속성도 보유합니다.</target>
        </trans-unit>
        <trans-unit id="0022de38b637868d1853d6d090cf915fe72c96d8" translate="yes" xml:space="preserve">
          <source>Let's apply some of those algorithms to a toy model we have created for
demonstration purposes.
For simplicity, we will use the following architecture, but users are welcome
to use any PyTorch model of their choice.</source>
          <target state="translated">데모 용으로 만든 장난감 모델에 이러한 알고리즘 중 일부를 적용 해 보겠습니다. 간단하게하기 위해 다음 아키텍처를 사용하지만 사용자는 원하는 PyTorch 모델을 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="4369c251b91c938021df6f86e075d0f68a6879d5" translate="yes" xml:space="preserve">
          <source>Let's create an instance of our model and set it to eval mode.</source>
          <target state="translated">모델의 인스턴스를 만들고 eval 모드로 설정하겠습니다.</target>
        </trans-unit>
        <trans-unit id="b2e5c40eefa02dca2a0399316902a8d9c9094103" translate="yes" xml:space="preserve">
          <source>Let's define our input and baseline tensors. Baselines are used in some
interpretability algorithms such as &lt;code&gt;IntegratedGradients, DeepLift, GradientShap, NeuronConductance, LayerConductance, InternalInfluence&lt;/code&gt; and
&lt;code&gt;NeuronIntegratedGradients&lt;/code&gt;.</source>
          <target state="translated">입력 및 기준 텐서를 정의 해 보겠습니다. 기준선은 &lt;code&gt;IntegratedGradients, DeepLift, GradientShap, NeuronConductance, LayerConductance, InternalInfluence&lt;/code&gt; 및 &lt;code&gt;NeuronIntegratedGradients&lt;/code&gt; 와 같은 일부 해석 알고리즘에 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="a2d21524039206f00133601c7ce2c40fbb2a88ae" translate="yes" xml:space="preserve">
          <source>Let's look into the internals of our network and understand which layers
and neurons are important for the predictions.</source>
          <target state="translated">네트워크의 내부를 살펴보고 예측에 중요한 계층과 뉴런을 이해해 봅시다.</target>
        </trans-unit>
        <trans-unit id="3229609e15436ec51bcf00818a69a84dbc58a0c2" translate="yes" xml:space="preserve">
          <source>License</source>
          <target state="translated">특허</target>
        </trans-unit>
        <trans-unit id="7151ecc0dc7a5120377ab85e7d7eac19fe0502d7" translate="yes" xml:space="preserve">
          <source>Model interpretability for PyTorch</source>
          <target state="translated">PyTorch에 대한 모델 해석 가능성</target>
        </trans-unit>
        <trans-unit id="64a7acecc018b0b76e1070ab8d77465d6f66f86e" translate="yes" xml:space="preserve">
          <source>More details about the above mentioned &lt;a href=&quot;https://captum.ai/docs/algorithms&quot;&gt;algorithms&lt;/a&gt; and their pros and cons can be found on our &lt;a href=&quot;https://captum.ai/docs/algorithms_comparison_matrix&quot;&gt;web-site&lt;/a&gt;.</source>
          <target state="translated">위에서 언급 한 &lt;a href=&quot;https://captum.ai/docs/algorithms&quot;&gt;알고리즘&lt;/a&gt; 과 장단점 에 대한 자세한 내용은 &lt;a href=&quot;https://captum.ai/docs/algorithms_comparison_matrix&quot;&gt;웹 사이트&lt;/a&gt; 에서 확인할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="c798498ab11132e260e2f04e6a25bc1b01a96272" translate="yes" xml:space="preserve">
          <source>More details on the list of supported algorithms and how to apply
Captum on different types of models can be found in our tutorials.</source>
          <target state="translated">지원되는 알고리즘 목록과 다양한 유형의 모델에 Captum을 적용하는 방법에 대한 자세한 내용은 자습서에서 찾을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a809e022bfbd247fdef4e8f92c06639a09f2e709" translate="yes" xml:space="preserve">
          <source>Next we will use &lt;code&gt;IntegratedGradients&lt;/code&gt; algorithms to assign attribution
scores to each input feature with respect to the first target output.</source>
          <target state="translated">다음으로 &lt;code&gt;IntegratedGradients&lt;/code&gt; 알고리즘을 사용 하여 첫 번째 대상 출력과 관련하여 각 입력 특성에 속성 점수를 할당합니다.</target>
        </trans-unit>
        <trans-unit id="e345eb5cbc3af8ba96268dc94160be1d607ed174" translate="yes" xml:space="preserve">
          <source>Next, we need to define simple input and baseline tensors.
Baselines belong to the input space and often carry no predictive signal.
Zero tensor can serve as a baseline for many tasks.
Some interpretability algorithms such as &lt;code&gt;Integrated Gradients&lt;/code&gt;, &lt;code&gt;Deeplift&lt;/code&gt; and &lt;code&gt;GradientShap&lt;/code&gt; are designed to attribute the change
between the input and baseline to a predictive class or a value that the neural
network outputs.</source>
          <target state="translated">다음으로 간단한 입력 및 기준선 텐서를 정의해야합니다. 기준선은 입력 공간에 속하며 종종 예측 신호를 전달하지 않습니다. 제로 텐서는 많은 작업의 기준선 역할을 할 수 있습니다. &lt;code&gt;Integrated Gradients&lt;/code&gt; , &lt;code&gt;Deeplift&lt;/code&gt; 및 &lt;code&gt;GradientShap&lt;/code&gt; 과 같은 일부 해석 가능성 알고리즘 은 입력과 기준 사이의 변화를 예측 클래스 또는 신경망이 출력하는 값에 귀속하도록 설계되었습니다.</target>
        </trans-unit>
        <trans-unit id="e51de3cf2de9a13ef9bdd27134ba09503e04cca6" translate="yes" xml:space="preserve">
          <source>Now let's look into &lt;code&gt;DeepLiftShap&lt;/code&gt;. Similar to &lt;code&gt;GradientShap&lt;/code&gt;, &lt;code&gt;DeepLiftShap&lt;/code&gt; uses
baseline distribution. In the example below, we use the same baseline distribution
as for &lt;code&gt;GradientShap&lt;/code&gt;.</source>
          <target state="translated">이제 &lt;code&gt;DeepLiftShap&lt;/code&gt; 을 살펴 보겠습니다 . &lt;code&gt;GradientShap&lt;/code&gt; 과 유사하게 &lt;code&gt;DeepLiftShap&lt;/code&gt; 은 기준 분포를 사용합니다. 아래 예에서는 &lt;code&gt;GradientShap&lt;/code&gt; 과 동일한 기준 분포를 사용합니다 .</target>
        </trans-unit>
        <trans-unit id="4bed336194a9a5c86b6a734f03b3570d2aae1a68" translate="yes" xml:space="preserve">
          <source>Output</source>
          <target state="translated">산출</target>
        </trans-unit>
        <trans-unit id="f3c8c95c5e534bcd2ea0034a0d83177efa6923f4" translate="yes" xml:space="preserve">
          <source>Output:</source>
          <target state="translated">산출:</target>
        </trans-unit>
        <trans-unit id="7835db447bc76230b5b0d736b2d78c671d7100a1" translate="yes" xml:space="preserve">
          <source>Outputs</source>
          <target state="translated">출력</target>
        </trans-unit>
        <trans-unit id="b969baf269c9dc13dd2c54d156013b3e06ea3070" translate="yes" xml:space="preserve">
          <source>Positive attribution score means that the input in that particular position
positively contributed to the final prediction and negative means the opposite.
The magnitude of the attribution score signifies the strength of the contribution.
Zero attribution score means no contribution from that particular feature.</source>
          <target state="translated">긍정적 인 귀속 점수는 특정 위치의 입력이 최종 예측에 긍정적으로 기여했음을 의미하고 부정적은 그 반대를 의미합니다. 기여도 점수의 크기는 기여도의 강도를 나타냅니다. 제로 어트 리뷰 션 점수는 특정 기능의 기여가 없음을 의미합니다.</target>
        </trans-unit>
        <trans-unit id="4c0d584e447dd2c5fa55e98d5d7921249c3911f6" translate="yes" xml:space="preserve">
          <source>PyTorch &amp;gt;= 1.2</source>
          <target state="translated">PyTorch&amp;gt; = 1.2</target>
        </trans-unit>
        <trans-unit id="c32b6c1ab053aa1b803595ba447bebbb8760c137" translate="yes" xml:space="preserve">
          <source>Python &amp;gt;= 3.6</source>
          <target state="translated">Python&amp;gt; = 3.6</target>
        </trans-unit>
        <trans-unit id="f00e768dce422689fe65ae881c3b96779cce5814" translate="yes" xml:space="preserve">
          <source>References of Algorithms</source>
          <target state="translated">알고리즘 참조</target>
        </trans-unit>
        <trans-unit id="54d440a5db0ab24f40fddf2b795f3716e25774cf" translate="yes" xml:space="preserve">
          <source>See the &lt;a href=&quot;CONTRIBUTING.md&quot;&gt;CONTRIBUTING&lt;/a&gt; file for how to help out.</source>
          <target state="translated">도움을주는 방법 은 &lt;a href=&quot;CONTRIBUTING.md&quot;&gt;CONTRIBUTING&lt;/a&gt; 파일을 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="9076ccad99e9fe123f5a242ef0d44a11422a0a37" translate="yes" xml:space="preserve">
          <source>Similar to GradientShap in order to compute example-based deltas we can average them per example:</source>
          <target state="translated">GradientShap과 유사하게 예제 기반 델타를 계산하기 위해 예제별로 평균을 낼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="7c2892f73bdaf8f8b7b55fd520cd5b43f07bb285" translate="yes" xml:space="preserve">
          <source>Similar to integrated gradients, DeepLift returns a convergence delta score
per input example. The approximation error is then the absolute
value of the convergence deltas and can serve as a proxy of how accurate the
algorithm's approximation is.</source>
          <target state="translated">통합 그라디언트와 유사하게 DeepLift는 입력 예제별로 수렴 델타 점수를 반환합니다. 근사 오차는 수렴 델타의 절대 값이며 알고리즘의 근사가 얼마나 정확한지에 대한 프록시 역할을 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="c98898a289abd12ad3663b6492c23d0af933dcbb" translate="yes" xml:space="preserve">
          <source>Similar to other attribution algorithms that return convergence delta, &lt;code&gt;LayerConductance&lt;/code&gt;
returns the deltas for each example. The approximation error is then the absolute
value of the convergence deltas and can serve as a proxy of how accurate integral
approximation for given inputs and baselines is.</source>
          <target state="translated">수렴 델타를 반환하는 다른 &lt;code&gt;LayerConductance&lt;/code&gt; 알고리즘과 마찬가지로 LayerConductance 는 각 예에 대한 델타를 반환합니다. 근사 오차는 수렴 델타의 절대 값이며 주어진 입력 및 기준선에 대한 적분 근사값이 얼마나 정확한지에 대한 프록시 역할을 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="08641a32adb6073b217892c1d05043cccc5795f7" translate="yes" xml:space="preserve">
          <source>Similarly, we can apply &lt;code&gt;GradientShap&lt;/code&gt;, &lt;code&gt;DeepLift&lt;/code&gt; and other attribution algorithms to the model.</source>
          <target state="translated">마찬가지로 &lt;code&gt;GradientShap&lt;/code&gt; , &lt;code&gt;DeepLift&lt;/code&gt; 및 기타 속성 알고리즘을 모델에 적용 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="5dfd03564d68a63c3940a933d7149be86e1b24fc" translate="yes" xml:space="preserve">
          <source>Talks and Papers</source>
          <target state="translated">회담 및 논문</target>
        </trans-unit>
        <trans-unit id="5cc6318e21af4a9864dfb66b71e19b22e67f5171" translate="yes" xml:space="preserve">
          <source>Target Audience</source>
          <target state="translated">대상 청중</target>
        </trans-unit>
        <trans-unit id="7c09a880f031a61c434b6eea261034d0953ae737" translate="yes" xml:space="preserve">
          <source>The algorithm outputs an attribution score for each input element and a
convergence delta. The lower the absolute value of the convergence delta the better
is the approximation. If we choose not to return delta,
we can simply not provide &lt;code&gt;return_convergence_delta&lt;/code&gt; input
argument. The absolute value of the returned deltas can be interpreted as an
approximation error for each input sample.
It can also serve as a proxy of how accurate the integral approximation for given
inputs and baselines is.
If the approximation error is large, we can try larger number of integral
approximation steps by setting &lt;code&gt;n_steps&lt;/code&gt; to a larger value. Not all algorithms
return approximation error. Those which do, though, compute it based on the
completeness property of the algorithms.</source>
          <target state="translated">알고리즘은 각 입력 요소와 수렴 델타에 대한 속성 점수를 출력합니다. 수렴 델타의 절대 값이 낮을수록 근사치가 더 좋습니다. 델타를 반환하지 않기로 선택하면 단순히 &lt;code&gt;return_convergence_delta&lt;/code&gt; 입력 인수를 제공 할 수 없습니다 . 반환 된 델타의 절대 값은 각 입력 샘플에 대한 근사 오차로 해석 될 수 있습니다. 또한 주어진 입력 및 기준선에 대한 적분 근사가 얼마나 정확한지에 대한 대리 역할을 할 수 있습니다. 근사 오차가 크면 &lt;code&gt;n_steps&lt;/code&gt; 를 더 큰 값 으로 설정하여 더 많은 수의 적분 근사 단계를 시도 할 수 있습니다 . 모든 알고리즘이 근사 오류를 반환하는 것은 아닙니다. 그러나 알고리즘의 완전성 속성을 기반으로 계산합니다.</target>
        </trans-unit>
        <trans-unit id="9bc5b2cbf539dd2b556b4ee16093e2dd98e5e53f" translate="yes" xml:space="preserve">
          <source>The latest release of Captum is easily installed either via
&lt;a href=&quot;https://www.anaconda.com/distribution/#download-section&quot;&gt;Anaconda&lt;/a&gt; (recommended):</source>
          <target state="translated">Captum의 최신 릴리스는 &lt;a href=&quot;https://www.anaconda.com/distribution/#download-section&quot;&gt;Anaconda&lt;/a&gt; 를 통해 쉽게 설치할 수 있습니다 (권장).</target>
        </trans-unit>
        <trans-unit id="14456e5d622ffef206d84cb2547dd1292a3e6a4b" translate="yes" xml:space="preserve">
          <source>The number of elements in the &lt;code&gt;delta&lt;/code&gt; tensor is equal to: &lt;code&gt;n_samples * input.shape[0]&lt;/code&gt;
In order to get a example-based delta, we can, for example, average them:</source>
          <target state="translated">&lt;code&gt;delta&lt;/code&gt; 텐서 의 요소 수 는 다음과 같습니다. &lt;code&gt;n_samples * input.shape[0]&lt;/code&gt; 예제 기반 델타를 얻기 위해 예를 들어 평균을 낼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="d97f36b304cf7bf6d19d6ba216bf83da80bc9ca1" translate="yes" xml:space="preserve">
          <source>The primary audiences for Captum are model developers who are looking to improve their models and understand which features are important and interpretability researchers focused on identifying algorithms that can better interpret many types of models.</source>
          <target state="translated">Captum의 주요 대상은 모델을 개선하고 어떤 기능이 중요한지 이해하려는 모델 개발자이며 해석 가능성 연구자들은 다양한 유형의 모델을 더 잘 해석 할 수있는 알고리즘을 식별하는 데 중점을 둡니다.</target>
        </trans-unit>
        <trans-unit id="09bb5c690208bfa966db7460586a698914a360bd" translate="yes" xml:space="preserve">
          <source>The slides of our presentation from NeurIPS 2019 can be found &lt;a href=&quot;docs/presentations/Captum_NeurIPS_2019_final.key&quot;&gt;here&lt;/a&gt;</source>
          <target state="translated">NeurIPS 2019의 프레젠테이션 슬라이드는 &lt;a href=&quot;docs/presentations/Captum_NeurIPS_2019_final.key&quot;&gt;여기&lt;/a&gt; 에서 찾을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5d0367e1f5fde1ede3db753f108801cb14982eac" translate="yes" xml:space="preserve">
          <source>To analyze a sample model on CIFAR10 via Captum Insights run</source>
          <target state="translated">Captum Insights 실행을 통해 CIFAR10에서 샘플 모델을 분석하려면</target>
        </trans-unit>
        <trans-unit id="1b78a188c82782ae0e64234fb5851ce273466f93" translate="yes" xml:space="preserve">
          <source>To build Insights you will need &lt;a href=&quot;https://nodejs.org/en/&quot;&gt;Node&lt;/a&gt; &amp;gt;= 8.x
and &lt;a href=&quot;https://yarnpkg.com/en/&quot;&gt;Yarn&lt;/a&gt; &amp;gt;= 1.5.</source>
          <target state="translated">Insights를 빌드하려면 &lt;a href=&quot;https://nodejs.org/en/&quot;&gt;Node&lt;/a&gt; &amp;gt; = 8.x 및 &lt;a href=&quot;https://yarnpkg.com/en/&quot;&gt;Yarn&lt;/a&gt; &amp;gt; = 1.5 가 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="5468c50938c4ec01b3019827d9753bc27eccd242" translate="yes" xml:space="preserve">
          <source>To build and launch from a checkout in a conda environment run</source>
          <target state="translated">conda 환경의 체크 아웃에서 빌드하고 실행하려면 다음을 실행하십시오.</target>
        </trans-unit>
        <trans-unit id="47d309089702b5779d1249906c184aca6a17334f" translate="yes" xml:space="preserve">
          <source>To build the widget from a checkout in a conda environment run</source>
          <target state="translated">conda 환경의 체크 아웃에서 위젯을 빌드하려면 다음을 실행하십시오.</target>
        </trans-unit>
        <trans-unit id="039a5244d60906d43c5c15a0a20b10397b0bc5f9" translate="yes" xml:space="preserve">
          <source>To customize the installation, you can also run the following variants of the
above:</source>
          <target state="translated">설치를 사용자 정의하려면 위의 다음 변형을 실행할 수도 있습니다.</target>
        </trans-unit>
        <trans-unit id="a39ccefe51118817f1b35b636167cdb3bd9357d8" translate="yes" xml:space="preserve">
          <source>To execute unit tests from a manual install, run:</source>
          <target state="translated">수동 설치에서 단위 테스트를 실행하려면 다음을 실행하세요.</target>
        </trans-unit>
        <trans-unit id="4ac1cec1a4406720bdada7e35a448ad0dfd20d82" translate="yes" xml:space="preserve">
          <source>To make computations deterministic, let's fix random seeds.</source>
          <target state="translated">계산을 결정적으로 만들기 위해 임의의 시드를 수정하겠습니다.</target>
        </trans-unit>
        <trans-unit id="eba414e2b73da649f8d414e82f52b11624820726" translate="yes" xml:space="preserve">
          <source>We will apply model interpretability algorithms on the network
mentioned above in order to understand the importance of individual
neurons/layers and the parts of the input that play an important role in the
final prediction.</source>
          <target state="translated">개별 뉴런 / 계층의 중요성과 최종 예측에서 중요한 역할을하는 입력 부분을 이해하기 위해 위에서 언급 한 네트워크에 모델 해석 가능성 알고리즘을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="168626e3b7f39ed35712d4be0f4311e69049dd72" translate="yes" xml:space="preserve">
          <source>We will start with the &lt;code&gt;NeuronConductance&lt;/code&gt;. &lt;code&gt;NeuronConductance&lt;/code&gt; helps us to identify
input features that are important for a particular neuron in a given
layer. It decomposes the computation of integrated gradients via the chain rule by
defining the importance of a neuron as path integral of the derivative of the output
with respect to the neuron times the derivatives of the neuron with respect to the
inputs of the model.</source>
          <target state="translated">&lt;code&gt;NeuronConductance&lt;/code&gt; 부터 시작 하겠습니다 . &lt;code&gt;NeuronConductance&lt;/code&gt; 는 주어진 레이어의 특정 뉴런에 중요한 입력 특징을 식별하는 데 도움이됩니다. 모델의 입력에 대한 뉴런의 미분을 뉴런에 대한 출력의 미분의 경로 적분으로 정의하여 체인 규칙을 통해 통합 된 기울기의 계산을 분해합니다.</target>
        </trans-unit>
        <trans-unit id="3aee6cb4b987c6eb98bae057c8033da66b7ba223" translate="yes" xml:space="preserve">
          <source>With the increase in model complexity and the resulting lack of transparency, model interpretability methods have become increasingly important. Model understanding is both an active area of research as well as an area of focus for practical applications across industries using machine learning. Captum provides state-of-the-art algorithms, including Integrated Gradients, to provide researchers and developers with an easy way to understand which features are contributing to a model&amp;rsquo;s output.</source>
          <target state="translated">모델 복잡성이 증가하고 그로 인해 투명성이 결여됨에 따라 모델 해석 방법이 점점 더 중요 해지고 있습니다. 모델 이해는 기계 학습을 사용하는 산업 전반에 걸친 실용적인 응용 분야에 중점을 둔 연구 분야이자 연구 분야입니다. Captum은 통합 그라디언트를 포함한 최첨단 알고리즘을 제공하여 연구자와 개발자가 모델 출력에 기여하는 기능을 쉽게 이해할 수있는 방법을 제공합니다.</target>
        </trans-unit>
        <trans-unit id="ffc76686099e22242eb4269a3745fa2f6e1ef1d4" translate="yes" xml:space="preserve">
          <source>and navigate to the URL specified in the output.</source>
          <target state="translated">출력에 지정된 URL로 이동합니다.</target>
        </trans-unit>
        <trans-unit id="2eeef8b3dfe3c6e07456229818d93864ce2c6fe7" translate="yes" xml:space="preserve">
          <source>in order to get per example average delta.</source>
          <target state="translated">예제 당 평균 델타를 얻으려면.</target>
        </trans-unit>
        <trans-unit id="d6773e7adcc2adec52348c71e0551f68a9f85abd" translate="yes" xml:space="preserve">
          <source>or via &lt;code&gt;pip&lt;/code&gt;:</source>
          <target state="translated">또는 &lt;code&gt;pip&lt;/code&gt; 를 통해 :</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
