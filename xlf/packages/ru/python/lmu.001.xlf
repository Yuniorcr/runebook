<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ru" datatype="htmlbody" original="https://pypi.org/project/lmu/">
    <body>
      <group id="lmu">
        <trans-unit id="f7986d315f2a9d4b9005d324dcb9fea7029e3373" translate="yes" xml:space="preserve">
          <source>(A, B)</source>
          <target state="translated">(А, Б)</target>
        </trans-unit>
        <trans-unit id="e8b310147d49ec4db715247f737630a341a942ea" translate="yes" xml:space="preserve">
          <source>) and the memory vector (</source>
          <target state="translated">) и вектор памяти (</target>
        </trans-unit>
        <trans-unit id="9d78d7759ad02cf7cb425cfa08e4a771f7ad35f4" translate="yes" xml:space="preserve">
          <source>) is trained via backpropagation, while the dynamics of the memory remain fixed (&lt;a href=&quot;https://papers.nips.cc/paper/9689-legendre-memory-units-continuous-time-representation-in-recurrent-neural-networks.pdf&quot;&gt;see paper for details&lt;/a&gt;).</source>
          <target state="translated">) обучается через обратное распространение, в то время как динамика памяти остается фиксированной ( &lt;a href=&quot;https://papers.nips.cc/paper/9689-legendre-memory-units-continuous-time-representation-in-recurrent-neural-networks.pdf&quot;&gt;подробности см. в статье&lt;/a&gt; ).</target>
        </trans-unit>
        <trans-unit id="91f01eed1463fd0e19c70b920cb603e7fd73b2fa" translate="yes" xml:space="preserve">
          <source>) with a nonlinear hidden state (</source>
          <target state="translated">) с нелинейным скрытым состоянием (</target>
        </trans-unit>
        <trans-unit id="f34ef1a8f3f40a7a1236002bb13939b9aaab7ef2" translate="yes" xml:space="preserve">
          <source>, if necessary. By default the coupling between the hidden state (</source>
          <target state="translated">, если необходимо. По умолчанию связь между скрытым состоянием (</target>
        </trans-unit>
        <trans-unit id="1a8a70853cdb5616fca8eb1afe92a80fdc273967" translate="yes" xml:space="preserve">
          <source>, which obtains the current best-known psMNIST result (using an RNN) of &lt;strong&gt;97.15%&lt;/strong&gt;. Note, the network is using fewer internal state-variables and neurons than there are pixels in the input sequence. To reproduce the results from the paper, run the notebooks in the</source>
          <target state="translated">, который дает наиболее известный на данный момент результат psMNIST (с использованием RNN) &lt;strong&gt;97,15%&lt;/strong&gt; . Обратите внимание, что сеть использует меньше внутренних переменных состояния и нейронов, чем пикселей во входной последовательности. Чтобы воспроизвести результаты из статьи, запустите блокноты в</target>
        </trans-unit>
        <trans-unit id="eb0d2eadad1d59670435428d52f11012b3c56467" translate="yes" xml:space="preserve">
          <source>0.1.0 (June 22, 2020)</source>
          <target state="translated">0,1,0 (22 июня 2020 г.)</target>
        </trans-unit>
        <trans-unit id="f407f992fca896699f798cda98a678821453fbc9" translate="yes" xml:space="preserve">
          <source>A single</source>
          <target state="translated">Один</target>
        </trans-unit>
        <trans-unit id="6eb4915552135823111f476864da3f654c233c54" translate="yes" xml:space="preserve">
          <source>Citation</source>
          <target state="translated">Цитата</target>
        </trans-unit>
        <trans-unit id="51622bd517ce184905e899085f3bf2c222cf7501" translate="yes" xml:space="preserve">
          <source>GitHub repository includes a pre-trained Keras/TensorFlow model, located at</source>
          <target state="translated">Репозиторий GitHub включает в себя предварительно подготовленную модель Keras/TensorFlow,расположенную по адресу</target>
        </trans-unit>
        <trans-unit id="ad6d35f4b9d3d4fd1498c1969d140852aa27ea84" translate="yes" xml:space="preserve">
          <source>Initial release of LMU 0.1.0! Supports Python 3.5+.</source>
          <target state="translated">Начальный релиз LMU 0.1.0! Поддерживает Python 3.5+.</target>
        </trans-unit>
        <trans-unit id="5db3dea372a23fb11a890296dd05b06a6f2c43b5" translate="yes" xml:space="preserve">
          <source>LMUCell</source>
          <target state="translated">LMUCell</target>
        </trans-unit>
        <trans-unit id="68142c1fecd8cd5a852b6821a313d6f18e67bd63" translate="yes" xml:space="preserve">
          <source>LMUs in NengoDL (reproducing SotA on psMNIST)</source>
          <target state="translated">LMU в NengoDL (воспроизведение SotA на psMNIST)</target>
        </trans-unit>
        <trans-unit id="94147d1825c4f1bb9b2a97d3e0d79fe72940ffcb" translate="yes" xml:space="preserve">
          <source>Legendre Memory Units</source>
          <target state="translated">Легендарные единицы памяти</target>
        </trans-unit>
        <trans-unit id="12e48a2c80ec09fba890acbab555ee09d4efed0c" translate="yes" xml:space="preserve">
          <source>Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks</source>
          <target state="translated">Легендарные единицы памяти:Непрерывное представление в повторяющихся нейронных сетях</target>
        </trans-unit>
        <trans-unit id="934f951fd58afc4e51af5bd95b0b06d48ea9da9c" translate="yes" xml:space="preserve">
          <source>Nengo Examples</source>
          <target state="translated">Примеры Ненго</target>
        </trans-unit>
        <trans-unit id="22d507f2ba74e43593de3ae3f550bf202c076adc" translate="yes" xml:space="preserve">
          <source>Paper</source>
          <target state="translated">Документ</target>
        </trans-unit>
        <trans-unit id="5044ba0a5cf607ebea4e58623c6417cb87f682bb" translate="yes" xml:space="preserve">
          <source>Release history</source>
          <target state="translated">История релизов</target>
        </trans-unit>
        <trans-unit id="019292c2324bce7b4b7238901874de053dae61e5" translate="yes" xml:space="preserve">
          <source>Spiking LMUs in Nengo (with online learning)</source>
          <target state="translated">Скачок LMU в Ненго (с онлайн-обучением)</target>
        </trans-unit>
        <trans-unit id="17fe76b2cbb233c1e454e6043ea8b785cf235f64" translate="yes" xml:space="preserve">
          <source>Spiking LMUs in Nengo Loihi (with online learning)</source>
          <target state="translated">Скачок LMU в Nengo Loihi (с онлайн-обучением)</target>
        </trans-unit>
        <trans-unit id="86d3603c97865cc4b8bed02e3a4ce17db2ddf9b9" translate="yes" xml:space="preserve">
          <source>Thanks to all of the contributors for making this possible!</source>
          <target state="translated">Спасибо всем,кто сделал это возможным!</target>
        </trans-unit>
        <trans-unit id="93ef0dd827103681fcee453b78be2ff14e1a261d" translate="yes" xml:space="preserve">
          <source>The</source>
          <target state="translated">The</target>
        </trans-unit>
        <trans-unit id="7812ef13cd5d03416a8df141fa5c9d4816525504" translate="yes" xml:space="preserve">
          <source>The API is considered unstable; parts are likely to change in the future.</source>
          <target state="translated">API считается нестабильным;части,вероятно,изменятся в будущем.</target>
        </trans-unit>
        <trans-unit id="e844712e089b0a5bc92ef35f09387b6d30e8162a" translate="yes" xml:space="preserve">
          <source>The discretized</source>
          <target state="translated">Сдержанный</target>
        </trans-unit>
        <trans-unit id="d7507de5fb53855c565db091e29dff83decf8e06" translate="yes" xml:space="preserve">
          <source>We propose a novel memory cell for recurrent neural networks that dynamically maintains information across long windows of time using relatively few resources. The Legendre Memory Unit (LMU) is mathematically derived to orthogonalize its continuous-time history &amp;ndash; doing so by solving d coupled ordinary differential equations (ODEs), whose phase space linearly maps onto sliding windows of time via the Legendre polynomials up to degree d &amp;minus; 1 (example d=12, shown below).</source>
          <target state="translated">Мы предлагаем новую ячейку памяти для рекуррентных нейронных сетей, которая динамически поддерживает информацию в течение длинных окон времени, используя относительно немного ресурсов. Модуль памяти Лежандра (LMU) математически выведен для ортогонализации его истории в непрерывном времени - делая это путем решения d связанных обыкновенных дифференциальных уравнений (ОДУ), фазовое пространство которых линейно отображается на скользящие окна времени с помощью полиномов Лежандра до степени d - 1 (пример d = 12, показан ниже).</target>
        </trans-unit>
        <trans-unit id="af5514d5e2de36d647091221bbc2576aad7eeb67" translate="yes" xml:space="preserve">
          <source>branch in the</source>
          <target state="translated">расти</target>
        </trans-unit>
        <trans-unit id="66f1fb81d4d11d662fbb9d02462591f64b87de95" translate="yes" xml:space="preserve">
          <source>branch.</source>
          <target state="translated">отделение.</target>
        </trans-unit>
        <trans-unit id="d3134f6bf31c14bcd86b2f42eacf8ca26224ba95" translate="yes" xml:space="preserve">
          <source>directory within the</source>
          <target state="translated">каталог внутри</target>
        </trans-unit>
        <trans-unit id="71ab8b6afb1bae3df247e0286da35e0da16564ff" translate="yes" xml:space="preserve">
          <source>docs</source>
          <target state="translated">документы</target>
        </trans-unit>
        <trans-unit id="c828076471935e6b0e12c70c56368cf19fbf3762" translate="yes" xml:space="preserve">
          <source>experiments</source>
          <target state="translated">эксперименты</target>
        </trans-unit>
        <trans-unit id="c4b89cb9b895edeb1054b2f4f97660e540b70508" translate="yes" xml:space="preserve">
          <source>expresses the following computational graph in Keras as an RNN layer, which couples the optimal linear memory (</source>
          <target state="translated">выражает следующий вычислительный график в Керасе в виде RNN слоя,который соединяет оптимальную линейную память (</target>
        </trans-unit>
        <trans-unit id="27d5482eebd075de44389774fce28c69f45c8a75" translate="yes" xml:space="preserve">
          <source>h</source>
          <target state="translated">h</target>
        </trans-unit>
        <trans-unit id="fbaee08b1fd3c3083cbbfc97fe14ba0acb73a3ab" translate="yes" xml:space="preserve">
          <source>includes an example for how to use the</source>
          <target state="translated">включает в себя пример использования</target>
        </trans-unit>
        <trans-unit id="dd84f8d3e9fee1217eaa3b5560a2dcf1324e9c29" translate="yes" xml:space="preserve">
          <source>lmu</source>
          <target state="translated">lmu</target>
        </trans-unit>
        <trans-unit id="6b0d31c0d563223024da45691584643ac78c96e8" translate="yes" xml:space="preserve">
          <source>m</source>
          <target state="translated">m</target>
        </trans-unit>
        <trans-unit id="8641d131425d9c0df501277f23d70a55c1e2c120" translate="yes" xml:space="preserve">
          <source>matrices are initialized according to the LMU&amp;rsquo;s mathematical derivation with respect to some chosen window length, &amp;theta;. Backpropagation can be used to learn this time-scale, or fine-tune</source>
          <target state="translated">матрицы инициализируются в соответствии с математическим выводом LMU относительно некоторой выбранной длины окна &amp;theta;. Обратное распространение можно использовать для изучения этой шкалы времени или точной настройки</target>
        </trans-unit>
        <trans-unit id="18bcc7cebf6c0b3db118c68544699b960be664ae" translate="yes" xml:space="preserve">
          <source>models/psMNIST-standard.hdf5</source>
          <target state="translated">модели/psMNIST-standard.hdf5</target>
        </trans-unit>
        <trans-unit id="950593b1f42de841169aa7d59486b7e980bc15cf" translate="yes" xml:space="preserve">
          <source>paper</source>
          <target state="translated">бумажный документ</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
