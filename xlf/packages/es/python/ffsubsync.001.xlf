<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="es" datatype="htmlbody" original="https://pypi.org/project/ffsubsync/">
    <body>
      <group id="ffsubsync">
        <trans-unit id="448fa61d60809d27051e5604a1053eafdfc3dbbb" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;http://www.numpy.org/&quot;&gt;numpy&lt;/a&gt; and, indirectly, &lt;a href=&quot;https://www.netlib.org/fftpack/&quot;&gt;FFTPACK&lt;/a&gt;, which powers the FFT-based algorithm for fast scoring of alignments between subtitles (or subtitles and video)</source>
          <target state="translated">&lt;a href=&quot;http://www.numpy.org/&quot;&gt;numpy&lt;/a&gt; e, indirectamente, &lt;a href=&quot;https://www.netlib.org/fftpack/&quot;&gt;FFTPACK&lt;/a&gt; , que impulsa el algoritmo basado en FFT para una puntuaci&amp;oacute;n r&amp;aacute;pida de alineaciones entre subt&amp;iacute;tulos (o subt&amp;iacute;tulos y video)</target>
        </trans-unit>
        <trans-unit id="809173aad90d1b10ce189edf1551d1b0699e2f6a" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;https://pypi.org/project/srt/&quot;&gt;srt&lt;/a&gt; for operating on &lt;a href=&quot;https://en.wikipedia.org/wiki/SubRip#SubRip_text_file_format&quot;&gt;SRT files&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://pypi.org/project/srt/&quot;&gt;srt&lt;/a&gt; para operar en&lt;a href=&quot;https://en.wikipedia.org/wiki/SubRip#SubRip_text_file_format&quot;&gt; archivos SRT&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b1d4e9959623befe0fa4c1f81c6a08d9ec02186a" translate="yes" xml:space="preserve">
          <source>&lt;a href=&quot;https://www.ffmpeg.org/&quot;&gt;ffmpeg&lt;/a&gt; and the &lt;a href=&quot;https://github.com/kkroening/ffmpeg-python&quot;&gt;ffmpeg-python&lt;/a&gt; wrapper, for extracting raw audio from video</source>
          <target state="translated">&lt;a href=&quot;https://www.ffmpeg.org/&quot;&gt;ffmpeg&lt;/a&gt; y el contenedor &lt;a href=&quot;https://github.com/kkroening/ffmpeg-python&quot;&gt;ffmpeg-python&lt;/a&gt; , para extraer audio sin procesar de video</target>
        </trans-unit>
        <trans-unit id="f402db43144acbb0439ef8edcbe14c9699525da0" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;ffs&lt;/code&gt;, &lt;code&gt;subsync&lt;/code&gt; and &lt;code&gt;ffsubsync&lt;/code&gt; all work as entrypoints:</source>
          <target state="translated">&lt;code&gt;ffs&lt;/code&gt; , &lt;code&gt;subsync&lt;/code&gt; y &lt;code&gt;ffsubsync&lt;/code&gt; todo el trabajo como puntos de entrada:</target>
        </trans-unit>
        <trans-unit id="14b64238bfe66703a43272be5c9729b5ca77e61f" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;ffsubsync&lt;/code&gt; uses the file extension to decide whether to perform voice activity
detection on the audio or to directly extract speech from an srt file.</source>
          <target state="translated">&lt;code&gt;ffsubsync&lt;/code&gt; usa la extensi&amp;oacute;n de archivo para decidir si realizar la detecci&amp;oacute;n de actividad de voz en el audio o extraer directamente la voz de un archivo srt.</target>
        </trans-unit>
        <trans-unit id="00799ba9e026c577347c81635a03c034bba3e14e" translate="yes" xml:space="preserve">
          <source>&lt;code&gt;ffsubsync&lt;/code&gt; usually finishes in 20 to 30 seconds, depending on the length of the
video. The most expensive step is actually extraction of raw audio. If you
already have a correctly synchronized &quot;reference&quot; srt file (in which case audio
extraction can be skipped), &lt;code&gt;ffsubsync&lt;/code&gt; typically runs in less than a second.</source>
          <target state="translated">&lt;code&gt;ffsubsync&lt;/code&gt; generalmente termina en 20 a 30 segundos, dependiendo de la duraci&amp;oacute;n del video. El paso m&amp;aacute;s caro es la extracci&amp;oacute;n de audio sin procesar. Si ya tiene un archivo srt de &quot;referencia&quot; sincronizado correctamente (en cuyo caso se puede omitir la extracci&amp;oacute;n de audio), &lt;code&gt;ffsubsync&lt;/code&gt; normalmente se ejecuta en menos de un segundo.</target>
        </trans-unit>
        <trans-unit id="936db9a8629b6aee1da2ac35146b55e3ac7b77db" translate="yes" xml:space="preserve">
          <source>At the request of some, you can now help cover my coffee expenses using the
Github Sponsors button at the top (recurring monthly payments), or using the below
Paypal Donate button (one-time payment):</source>
          <target state="translated">A petición de algunos,ahora puedes ayudarme a cubrir mis gastos de café usando el botón de Patrocinadores de Github en la parte superior (pagos mensuales recurrentes),o usando el botón de Donación de Paypal en la parte inferior (pago único):</target>
        </trans-unit>
        <trans-unit id="56f9b02e470ae04f5605651b0ca5cd5920c7d715" translate="yes" xml:space="preserve">
          <source>Besides general stability and usability improvements, one line
of work aims to extend the synchronization algorithm to handle splits
/ breaks in the middle of video not present in subtitles (or vice versa).
Developing a robust solution will take some time (assuming one is possible).
See &lt;a href=&quot;https://github.com/smacke/ffsubsync/issues/10&quot;&gt;#10&lt;/a&gt; for more details.</source>
          <target state="translated">Adem&amp;aacute;s de las mejoras generales de estabilidad y usabilidad, una l&amp;iacute;nea de trabajo tiene como objetivo extender el algoritmo de sincronizaci&amp;oacute;n para manejar divisiones / interrupciones en el medio del video que no est&amp;aacute; presente en los subt&amp;iacute;tulos (o viceversa). Desarrollar una soluci&amp;oacute;n s&amp;oacute;lida llevar&amp;aacute; alg&amp;uacute;n tiempo (suponiendo que sea posible). Consulte el n. &lt;a href=&quot;https://github.com/smacke/ffsubsync/issues/10&quot;&gt;&amp;deg; 10&lt;/a&gt; para obtener m&amp;aacute;s detalles.</target>
        </trans-unit>
        <trans-unit id="bd217ff4214f14c1a032a45a800183c165249895" translate="yes" xml:space="preserve">
          <source>Code in this project is &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;MIT licensed&lt;/a&gt;.</source>
          <target state="translated">El c&amp;oacute;digo de este proyecto tiene &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;licencia del MIT&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="bfac50d6424b5166c3ee2808c85ae7c139b5182f" translate="yes" xml:space="preserve">
          <source>Credits</source>
          <target state="translated">Créditos</target>
        </trans-unit>
        <trans-unit id="1946ed4e6d7a5cdee8942980594304622d5d448e" translate="yes" xml:space="preserve">
          <source>Discretize video and subtitles by time into 10ms windows.</source>
          <target state="translated">Discretizar el video y los subtítulos por tiempo en ventanas de 10ms.</target>
        </trans-unit>
        <trans-unit id="61f87fb699391c08f8809648c3d512e2529450b6" translate="yes" xml:space="preserve">
          <source>FFsubsync</source>
          <target state="translated">FFsubsync</target>
        </trans-unit>
        <trans-unit id="d14e9d216a8814e80e6ada0f122fa233375bf1ab" translate="yes" xml:space="preserve">
          <source>First, make sure ffmpeg is installed. On MacOS, this looks like:</source>
          <target state="translated">Primero,asegúrese de que la ffmpeg esté instalada.En MacOS,esto parece:</target>
        </trans-unit>
        <trans-unit id="42edbb52a87cd00061f18e023951cf7cd5c5ef15" translate="yes" xml:space="preserve">
          <source>For each 10ms window, determine whether that window contains speech.  This
is trivial to do for subtitles (we just determine whether any subtitle is
&quot;on&quot; during each time window); for video, use an off-the-shelf voice
activity detector (VAD) like
the one built into &lt;a href=&quot;https://webrtc.org/&quot;&gt;webrtc&lt;/a&gt;.</source>
          <target state="translated">Para cada ventana de 10 ms, determine si esa ventana contiene voz. Esto es trivial para los subt&amp;iacute;tulos (solo determinamos si alg&amp;uacute;n subt&amp;iacute;tulo est&amp;aacute; &quot;activado&quot; durante cada ventana de tiempo); para video, use un detector de actividad de voz (VAD) est&amp;aacute;ndar como el integrado en &lt;a href=&quot;https://webrtc.org/&quot;&gt;webrtc&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="c3cba160fe064a0d9a4fc24f20540221a2aff0ed" translate="yes" xml:space="preserve">
          <source>Future Work</source>
          <target state="translated">Trabajo futuro</target>
        </trans-unit>
        <trans-unit id="ad0a916130b1ffb1194d9d811a12d26d58f8df6c" translate="yes" xml:space="preserve">
          <source>Helping Development</source>
          <target state="translated">Ayudar al desarrollo</target>
        </trans-unit>
        <trans-unit id="90ccd6497400b5576aeca1bd94af74aae1e0a250" translate="yes" xml:space="preserve">
          <source>History</source>
          <target state="translated">Historia</target>
        </trans-unit>
        <trans-unit id="8cecc82d72d68f9151cb0901cfe40eeac15bfca6" translate="yes" xml:space="preserve">
          <source>How It Works</source>
          <target state="translated">Cómo funciona</target>
        </trans-unit>
        <trans-unit id="44abaabed373ce699f3a6b3019932545e4158606" translate="yes" xml:space="preserve">
          <source>If you want to live dangerously, you can grab the latest version as follows:</source>
          <target state="translated">Si quieres vivir peligrosamente,puedes coger la última versión de la siguiente manera:</target>
        </trans-unit>
        <trans-unit id="ce1d79c980df3c47404d3d29df7c08886649970a" translate="yes" xml:space="preserve">
          <source>In most cases, inconsistencies between video and subtitles occur when starting
or ending segments present in video are not present in subtitles, or vice versa.
This can occur, for example, when a TV episode recap in the subtitles was pruned
from video. FFsubsync typically works well in these cases, and in my experience
this covers &amp;gt;95% of use cases. Handling breaks and splits outside of the beginning
and ending segments is left to future work (see below).</source>
          <target state="translated">En la mayor&amp;iacute;a de los casos, las inconsistencias entre el video y los subt&amp;iacute;tulos ocurren cuando los segmentos iniciales o finales presentes en el video no est&amp;aacute;n presentes en los subt&amp;iacute;tulos, o viceversa. Esto puede ocurrir, por ejemplo, cuando el resumen de un episodio de televisi&amp;oacute;n en los subt&amp;iacute;tulos se elimin&amp;oacute; del video. FFsubsync generalmente funciona bien en estos casos y, en mi experiencia, esto cubre&amp;gt; 95% de los casos de uso. El manejo de rupturas y divisiones fuera de los segmentos inicial y final se deja para el trabajo futuro (ver m&amp;aacute;s abajo).</target>
        </trans-unit>
        <trans-unit id="fd6c3ebf7befca9f8208f86c76e4d4180303745c" translate="yes" xml:space="preserve">
          <source>Install</source>
          <target state="translated">Instalar</target>
        </trans-unit>
        <trans-unit id="659525747c675f26d0d0761f93182ab5228cbc26" translate="yes" xml:space="preserve">
          <source>Into this:</source>
          <target state="translated">En esto:</target>
        </trans-unit>
        <trans-unit id="0d05fd8158dba0121b4eac110b20f240f7818b92" translate="yes" xml:space="preserve">
          <source>Language-agnostic automatic synchronization of subtitles with video, so that
subtitles are aligned to the correct starting point within the video.</source>
          <target state="translated">Sincronización automática de los subtítulos con el vídeo,para que los subtítulos estén alineados con el punto de partida correcto dentro del vídeo.</target>
        </trans-unit>
        <trans-unit id="420a9347981effc83ba7d1df62790a49716f38bb" translate="yes" xml:space="preserve">
          <source>Language-agnostic synchronization of subtitles with video.</source>
          <target state="translated">Sincronización lingüístico-agnóstica de los subtítulos con el vídeo.</target>
        </trans-unit>
        <trans-unit id="3229609e15436ec51bcf00818a69a84dbc58a0c2" translate="yes" xml:space="preserve">
          <source>License</source>
          <target state="translated">Licencia</target>
        </trans-unit>
        <trans-unit id="a7c04c64ed3f2a9374590c76c50d3b7f1b18e3da" translate="yes" xml:space="preserve">
          <source>Limitations</source>
          <target state="translated">Limitaciones</target>
        </trans-unit>
        <trans-unit id="beac1d4d9d908d34bbabd5497b31900615c1e739" translate="yes" xml:space="preserve">
          <source>Next, grab the script. It should work with both Python 2 and Python 3:</source>
          <target state="translated">A continuación,coge el guión.Debería funcionar tanto con Python 2 como con Python 3:</target>
        </trans-unit>
        <trans-unit id="179a4231cbcf797a1ea61865a797b3047681d7b0" translate="yes" xml:space="preserve">
          <source>Now we have two binary strings: one for the subtitles, and one for the
video.  Try to align these strings by matching 0's with 0's and 1's with
1's. We score these alignments as (# video 1's matched w/ subtitle 1's) - (#
video 1's matched with subtitle 0's).</source>
          <target state="translated">Ahora tenemos dos cadenas binarias:una para los subtítulos y otra para el video.Intenta alinear estas cadenas haciendo coincidir los 0 con los 0 y los 1 con los 1.Anotamos estas alineaciones como (#video 1's emparejado con subtítulos 1's)-(#video 1's emparejado con subtítulos 0's).</target>
        </trans-unit>
        <trans-unit id="bc49bcc1bbf9322f0d673c127d5dd1764357aa92" translate="yes" xml:space="preserve">
          <source>Other excellent Python libraries like &lt;a href=&quot;https://docs.python.org/3/library/argparse.html&quot;&gt;argparse&lt;/a&gt; and &lt;a href=&quot;https://tqdm.github.io/&quot;&gt;tqdm&lt;/a&gt;, not related to the core functionality, but which enable much better experiences for developers and users.</source>
          <target state="translated">Otras excelentes bibliotecas de Python como &lt;a href=&quot;https://docs.python.org/3/library/argparse.html&quot;&gt;argparse&lt;/a&gt; y &lt;a href=&quot;https://tqdm.github.io/&quot;&gt;tqdm&lt;/a&gt; , no relacionadas con la funcionalidad principal, pero que permiten experiencias mucho mejores para desarrolladores y usuarios.</target>
        </trans-unit>
        <trans-unit id="2d2cb022bc3d26bd1407c4aa787d5e46e1ad4c3b" translate="yes" xml:space="preserve">
          <source>Speed</source>
          <target state="translated">Velocidad</target>
        </trans-unit>
        <trans-unit id="6d75a7aec098fab5da5f5616cd98e0d6589fb421" translate="yes" xml:space="preserve">
          <source>The best-scoring alignment from step 3 determines how to offset the subtitles
in time so that they are properly synced with the video. Because the binary
strings are fairly long (millions of digits for video longer than an hour), the
naive O(n^2) strategy for scoring all alignments is unacceptable. Instead, we
use the fact that &quot;scoring all alignments&quot; is a convolution operation and can
be implemented with the Fast Fourier Transform (FFT), bringing the complexity
down to O(n log n).</source>
          <target state="translated">La alineación de mejor puntuación del paso 3 determina cómo compensar los subtítulos en el tiempo para que estén correctamente sincronizados con el vídeo.Debido a que las cadenas binarias son bastante largas (millones de dígitos para el vídeo de más de una hora),la ingenua estrategia de O(n^2)para puntuar todas las alineaciones es inaceptable.En su lugar,usamos el hecho de que &quot;puntuar todas las alineaciones&quot; es una operación de convolución y puede ser implementada con la Transformada rápida de Fourier (FFT),reduciendo la complejidad a O(n log n).</target>
        </trans-unit>
        <trans-unit id="b6cb2e1a9df05d019e4ce2e098c42168484163b6" translate="yes" xml:space="preserve">
          <source>The implementation for this project was started during HackIllinois 2019, for
which it received an &lt;strong&gt;&lt;em&gt;Honorable Mention&lt;/em&gt;&lt;/strong&gt; (ranked in the top 5 projects,
excluding projects that won company-specific prizes).</source>
          <target state="translated">La implementaci&amp;oacute;n de este proyecto se inici&amp;oacute; durante HackIllinois 2019, por lo que recibi&amp;oacute; una &lt;strong&gt;&lt;em&gt;Menci&amp;oacute;n de Honor&lt;/em&gt;&lt;/strong&gt; (clasificado entre los 5 mejores proyectos, excluyendo los proyectos que ganaron premios espec&amp;iacute;ficos de la empresa).</target>
        </trans-unit>
        <trans-unit id="c24de39b3bb0b16a5df396d44c3f3aebba913c44" translate="yes" xml:space="preserve">
          <source>The synchronization algorithm operates in 3 steps:</source>
          <target state="translated">El algoritmo de sincronización funciona en 3 pasos:</target>
        </trans-unit>
        <trans-unit id="94226cd553bfe5c121e7a05322ec2c6041760ba8" translate="yes" xml:space="preserve">
          <source>There may be occasions where you have a correctly synchronized srt file in a
language you are unfamiliar with, as well as an unsynchronized srt file in your
native language. In this case, you can use the correctly synchronized srt file
directly as a reference for synchronization, instead of using the video as the
reference:</source>
          <target state="translated">Puede haber ocasiones en las que tenga un archivo srt correctamente sincronizado en un idioma con el que no esté familiarizado,así como un archivo srt no sincronizado en su idioma nativo.En este caso,puede utilizar el archivo srt correctamente sincronizado directamente como referencia para la sincronización,en lugar de utilizar el vídeo como referencia:</target>
        </trans-unit>
        <trans-unit id="da9f448081113e5f2aa6a2b4381ed0a353647124" translate="yes" xml:space="preserve">
          <source>This project would not be possible without the following libraries:</source>
          <target state="translated">Este proyecto no sería posible sin las siguientes bibliotecas:</target>
        </trans-unit>
        <trans-unit id="7240d4aab825d0799c8f63afc0c0ff0047392b6e" translate="yes" xml:space="preserve">
          <source>Turn this:</source>
          <target state="translated">Gira esto:</target>
        </trans-unit>
        <trans-unit id="0bb18642b70b9f8a9c12ccf39487328f306b8e19" translate="yes" xml:space="preserve">
          <source>Usage</source>
          <target state="translated">Uso</target>
        </trans-unit>
        <trans-unit id="75a062ef757a8dddb6401a1ad2fd757ca432ebd9" translate="yes" xml:space="preserve">
          <source>VAD from &lt;a href=&quot;https://webrtc.org/&quot;&gt;webrtc&lt;/a&gt; and the &lt;a href=&quot;https://github.com/wiseman/py-webrtcvad&quot;&gt;py-webrtcvad&lt;/a&gt; wrapper, for speech detection</source>
          <target state="translated">VAD de &lt;a href=&quot;https://webrtc.org/&quot;&gt;webrtc&lt;/a&gt; y el contenedor &lt;a href=&quot;https://github.com/wiseman/py-webrtcvad&quot;&gt;py-webrtcvad&lt;/a&gt; , para detecci&amp;oacute;n de voz</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
