<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ru" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="76b27001feb75250d904bb68f5aaa46711a6a0f1" translate="yes" xml:space="preserve">
          <source>Expands the dimension &lt;a href=&quot;tensors#torch.Tensor.dim&quot;&gt;&lt;code&gt;dim&lt;/code&gt;&lt;/a&gt; of the &lt;code&gt;self&lt;/code&gt; tensor over multiple dimensions of sizes given by &lt;code&gt;sizes&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e40e001bf1e0b420d657a639d6d67387b6d4923" translate="yes" xml:space="preserve">
          <source>Expected inputs are spatial (4 dimensional). Use &lt;code&gt;upsample_trilinear&lt;/code&gt; fo volumetric (5 dimensional) inputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="551ccd41342efa881333f0f1e558164218f9345a" translate="yes" xml:space="preserve">
          <source>Expected result:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8439d6715059ee70b74368357e1335f786d20b96" translate="yes" xml:space="preserve">
          <source>Expects &lt;code&gt;input&lt;/code&gt; to be &amp;lt;= 2-D tensor and transposes dimensions 0 and 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8133389ca86b79f9ac63f2057897dfbe1cda5cc8" translate="yes" xml:space="preserve">
          <source>Explicit alignment by names</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34533450ac1f4cb8a131c86d48accad539bed843" translate="yes" xml:space="preserve">
          <source>Exponential</source>
          <target state="translated">Exponential</target>
        </trans-unit>
        <trans-unit id="7d2243304a874bff81f4d326427b9649a3f42d91" translate="yes" xml:space="preserve">
          <source>ExponentialFamily</source>
          <target state="translated">ExponentialFamily</target>
        </trans-unit>
        <trans-unit id="e8c6f63a2d8909014d6aec0ec531f11a54dcd1bb" translate="yes" xml:space="preserve">
          <source>ExponentialFamily is the abstract base class for probability distributions belonging to an exponential family, whose probability mass/density function has the form is defined below</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58b807aacff8abe3f97a111c7a1e5f71d192fe91" translate="yes" xml:space="preserve">
          <source>Export a model into ONNX format. This exporter runs your model once in order to get a trace of its execution to be exported; at the moment, it supports a limited set of dynamic models (e.g., RNNs.)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69c85a023a54b1f8defe44bdc84c4b1308b399c8" translate="yes" xml:space="preserve">
          <source>Exporting models with unsupported ONNX operators can be achieved using the &lt;code&gt;operator_export_type&lt;/code&gt; flag in export API. This flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1d6a87a24293323dbd3502f982e0c3f518eed8d" translate="yes" xml:space="preserve">
          <source>Exports an EventList as a Chrome tracing tools file.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ae5fccd8dcd8fc317f8edfc8259af86cd2967a29" translate="yes" xml:space="preserve">
          <source>Expressions</source>
          <target state="translated">Expressions</target>
        </trans-unit>
        <trans-unit id="44bdb40abdeed26ffb35b097c6be620648eb56bc" translate="yes" xml:space="preserve">
          <source>Extending PyTorch</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ff04a6a0866fa949548184e90e852102f7c5167" translate="yes" xml:space="preserve">
          <source>Extension of the Distribution class, which applies a sequence of Transforms to a base distribution. Let f be the composition of transforms applied:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a505bba828df658cd19ca21d48f6b057b88e1432" translate="yes" xml:space="preserve">
          <source>Extra care needs to be taken when backward through &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; outputs. Such operation is really only stable when &lt;code&gt;input&lt;/code&gt; is full rank with all distinct singular values. Otherwise, &lt;code&gt;NaN&lt;/code&gt; can appear as the gradients are not properly defined. Also, notice that double backward will usually do an additional backward through &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; even if the original backward is only on &lt;code&gt;S&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7869f96c8dcb40e05a18db44c02551a0604c8dd8" translate="yes" xml:space="preserve">
          <source>Extra care needs to be taken when backward through outputs. Such operation is really only stable when all eigenvalues are distinct. Otherwise, &lt;code&gt;NaN&lt;/code&gt; can appear as the gradients are not properly defined.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35e4d02caad3850761be7f0a939f52dc8e6d05c6" translate="yes" xml:space="preserve">
          <source>Extracts sliding local blocks from a batched input tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="545d81b806ede14bef050d6bbe02b72b7b5279d1" translate="yes" xml:space="preserve">
          <source>Extracts sliding local blocks from an batched input tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e69f20e9f683920d3fb4329abd951e878b1f9372" translate="yes" xml:space="preserve">
          <source>F</source>
          <target state="translated">F</target>
        </trans-unit>
        <trans-unit id="bf9ce34c9cacb2121a3faa4bcb13d39153683bf1" translate="yes" xml:space="preserve">
          <source>F(\theta)</source>
          <target state="translated">F(\theta)</target>
        </trans-unit>
        <trans-unit id="d1636ed5d55d52fc51dab6a56c1fdb216178f4c5" translate="yes" xml:space="preserve">
          <source>FAST mode algorithm</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d8a2052196e0929b1ddeff4f9fa793509ac8f6c" translate="yes" xml:space="preserve">
          <source>FCN ResNet101</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d876bcc45100c189667e02c674ab45db60de2d8b" translate="yes" xml:space="preserve">
          <source>FCN ResNet50</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70deee53be1d417368b869145a93da9f61814dda" translate="yes" xml:space="preserve">
          <source>FCN ResNet50, ResNet101</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58296524e883e134fa6cc4840027902c76d7e637" translate="yes" xml:space="preserve">
          <source>Factory functions now take a new &lt;code&gt;names&lt;/code&gt; argument that associates a name with each dimension.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="97cdbdc7feff827efb082a6b6dd2727237cd49fd" translate="yes" xml:space="preserve">
          <source>False</source>
          <target state="translated">False</target>
        </trans-unit>
        <trans-unit id="6bca42e6cb3531d60196488b0aafe02849dfad8a" translate="yes" xml:space="preserve">
          <source>False if the compiler is (likely) ABI-incompatible with PyTorch, else True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f66134050db6d6f1871f0a392769d7d300bffc78" translate="yes" xml:space="preserve">
          <source>Faster R-CNN</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="19fa372bf4511d5897a6678506a36d53384e83c1" translate="yes" xml:space="preserve">
          <source>Faster R-CNN ResNet-50 FPN</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ceaa939a1707b8201f9f233e5c8d2c8a11872247" translate="yes" xml:space="preserve">
          <source>Faster R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b9a92e1a2a80529a37624caec86105f3856f6680" translate="yes" xml:space="preserve">
          <source>FeatureDropout (training mode not supported)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="023ddfe2580672ca0eeb5f867eac27e114575b07" translate="yes" xml:space="preserve">
          <source>Features described in this documentation are classified by release status:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ed223cb662e6eadab1b5774d593969ec64456ed" translate="yes" xml:space="preserve">
          <source>Features for large-scale deployments</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e0100beb8ca4fc8e23a8eb611272bb7c779f1f4" translate="yes" xml:space="preserve">
          <source>File descriptor - &lt;code&gt;file_descriptor&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f8affbccfbf014c7f4e4336ada91b93349f5adf6" translate="yes" xml:space="preserve">
          <source>File system - &lt;code&gt;file_system&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="29fd83b7db12e4d9231f579c15fdaeb5d71086e4" translate="yes" xml:space="preserve">
          <source>Fill the main diagonal of a tensor that has at least 2-dimensions. When dims&amp;gt;2, all dimensions of input must be of equal length. This function modifies the input tensor in-place, and returns the input tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="46412e89386beda08d3ce994cbdd41f7b9cb6e05" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with elements drawn from the exponential distribution:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1aea402a861ce53378696f47d930c7de8244acde" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with elements drawn from the geometric distribution:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1abd5eabced3b954f0b9c8f459ed264742cdc1be" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with elements samples from the normal distribution parameterized by &lt;a href=&quot;generated/torch.mean#torch.mean&quot;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.std#torch.std&quot;&gt;&lt;code&gt;std&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be7637b77a168dc9781dec5a6963103f27f1e666" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with numbers sampled from the continuous uniform distribution:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="963887f4ec9debd27ff138179ec127b4ceb1a324" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with numbers sampled from the discrete uniform distribution over &lt;code&gt;[from, to - 1]&lt;/code&gt;. If not specified, the values are usually only bounded by &lt;code&gt;self&lt;/code&gt; tensor&amp;rsquo;s data type. However, for floating point types, if unspecified, range will be &lt;code&gt;[0, 2^mantissa]&lt;/code&gt; to ensure that every value is representable. For example, &lt;code&gt;torch.tensor(1, dtype=torch.double).random_()&lt;/code&gt; will be uniform in &lt;code&gt;[0, 2^53]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7f10ef5f693feaeaa010e2aa027c2e16d3faf27" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with numbers samples from the log-normal distribution parameterized by the given mean</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a537a70caec95c45887848e05dbf76327e57f2bf" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with the specified value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="84e59f1fa6c91fd2336c166329083335c07742c5" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with zeros.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c357bdf1d8cc36baaa85a26ceaf45fb123516806" translate="yes" xml:space="preserve">
          <source>Fills each location of &lt;code&gt;self&lt;/code&gt; with an independent sample from</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="75c253cce2f7953023782f50f2d324ceb99f06ec" translate="yes" xml:space="preserve">
          <source>Fills elements of &lt;code&gt;self&lt;/code&gt; tensor with &lt;code&gt;value&lt;/code&gt; where &lt;code&gt;mask&lt;/code&gt; is True. The shape of &lt;code&gt;mask&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the shape of the underlying tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f2c547e676c650b0294e152d6742725981b5e7b" translate="yes" xml:space="preserve">
          <source>Fills the 2-dimensional input &lt;code&gt;Tensor&lt;/code&gt; with the identity matrix. Preserves the identity of the inputs in &lt;code&gt;Linear&lt;/code&gt; layers, where as many inputs are preserved as possible.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e10a9506b95f83d62c2104cb07cd55facde3766" translate="yes" xml:space="preserve">
          <source>Fills the 2D input &lt;code&gt;Tensor&lt;/code&gt; as a sparse matrix, where the non-zero elements will be drawn from the normal distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e66c6e2e841f8a8523fead84aa32e52990c1b4b" translate="yes" xml:space="preserve">
          <source>Fills the elements of the &lt;code&gt;self&lt;/code&gt; tensor with value &lt;code&gt;val&lt;/code&gt; by selecting the indices in the order given in &lt;code&gt;index&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a21a74e5af8bd75efccfeb2dc749fe2483f779d" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with a (semi) orthogonal matrix, as described in &lt;code&gt;Exact solutions to the nonlinear dynamics of learning in deep linear neural networks&lt;/code&gt; - Saxe, A. et al. (2013). The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06242c8039ea176e183491ddfbaaf7c2d66c68e8" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with values according to the method described in &lt;code&gt;Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification&lt;/code&gt; - He, K. et al. (2015), using a normal distribution. The resulting tensor will have values sampled from</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="342727420e0d4e4b77efe66611c6eb6db5be0acf" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with values according to the method described in &lt;code&gt;Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification&lt;/code&gt; - He, K. et al. (2015), using a uniform distribution. The resulting tensor will have values sampled from</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71e3100123b6866d1997e6df9daeefd940ad80de" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with values according to the method described in &lt;code&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/code&gt; - Glorot, X. &amp;amp; Bengio, Y. (2010), using a normal distribution. The resulting tensor will have values sampled from</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2313477ebbbc49bb7f4bf7d5c39c5ea0206267ac" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with values according to the method described in &lt;code&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/code&gt; - Glorot, X. &amp;amp; Bengio, Y. (2010), using a uniform distribution. The resulting tensor will have values sampled from</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="afacae29047abc8bbea1f5a18cb9f7afddce8004" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with the scalar value &lt;code&gt;0&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0749fb4668d1e4b76f3a1ba9aa423e792bc38d58" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with the scalar value &lt;code&gt;1&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="498455641766cf82c112342c938821f2f05dedf1" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with the value</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92513a6a0cb417782fe4aae045c4a1b66c923b2f" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with values drawn from the normal distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b3d75035cf3e14fc34e56b36e9d5379c6dd95d1" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with values drawn from the uniform distribution</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="53430dbd9b3ec1b5ed7bd9a57a201a7f2d1a60f2" translate="yes" xml:space="preserve">
          <source>Fills the tensor with numbers drawn from the Cauchy distribution:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c6632aebf9bc7adafe6c2b8a271e6b35dc1107ed" translate="yes" xml:space="preserve">
          <source>Fills the {3, 4, 5}-dimensional input &lt;code&gt;Tensor&lt;/code&gt; with the Dirac delta function. Preserves the identity of the inputs in &lt;code&gt;Convolutional&lt;/code&gt; layers, where as many input channels are preserved as possible. In case of groups&amp;gt;1, each group of channels preserves identity</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="695c574fe765006dd0a54d1a5cdae48d767984bd" translate="yes" xml:space="preserve">
          <source>Find the indices from the &lt;em&gt;innermost&lt;/em&gt; dimension of &lt;code&gt;sorted_sequence&lt;/code&gt; such that, if the corresponding values in &lt;code&gt;values&lt;/code&gt; were inserted before the indices, the order of the corresponding &lt;em&gt;innermost&lt;/em&gt; dimension within &lt;code&gt;sorted_sequence&lt;/code&gt; would be preserved.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bc20e9bbc443d3b0fd0b14b6ff920327eb09f1d2" translate="yes" xml:space="preserve">
          <source>Find the indices from the &lt;em&gt;innermost&lt;/em&gt; dimension of &lt;code&gt;sorted_sequence&lt;/code&gt; such that, if the corresponding values in &lt;code&gt;values&lt;/code&gt; were inserted before the indices, the order of the corresponding &lt;em&gt;innermost&lt;/em&gt; dimension within &lt;code&gt;sorted_sequence&lt;/code&gt; would be preserved. Return a new tensor with the same size as &lt;code&gt;values&lt;/code&gt;. If &lt;code&gt;right&lt;/code&gt; is False (default), then the left boundary of &lt;code&gt;sorted_sequence&lt;/code&gt; is closed. More formally, the returned index satisfies the following rules:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="03215b19f0bffce926c83716ff9399ab447bdf55" translate="yes" xml:space="preserve">
          <source>Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8cb7b9aabdd31afa308112ad36cdb297f62b4914" translate="yes" xml:space="preserve">
          <source>Fine grained control is possible with &lt;code&gt;qconfig&lt;/code&gt; and &lt;code&gt;mapping&lt;/code&gt; that act similarly to &lt;code&gt;quantize()&lt;/code&gt;. If &lt;code&gt;qconfig&lt;/code&gt; is provided, the &lt;code&gt;dtype&lt;/code&gt; argument is ignored.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d6ca52cc281a8bbbaa3d9b53b49079e29eb878fa" translate="yes" xml:space="preserve">
          <source>First convert your model from GPU to CPU and then save it, like so:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ce44c84deade8daed8f8cfb3f091e34fe3a19e21" translate="yes" xml:space="preserve">
          <source>First it will prepare the model for calibration, then it calls &lt;code&gt;run_fn&lt;/code&gt; which will run the calibration step, after that we will convert the model to a quantized model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56b7d451e08bf1ccd9aa8267b497203df559b032" translate="yes" xml:space="preserve">
          <source>First, if you repeatedly perform an operation that can produce duplicate entries (e.g., &lt;a href=&quot;#torch.sparse.FloatTensor.add&quot;&gt;&lt;code&gt;torch.sparse.FloatTensor.add()&lt;/code&gt;&lt;/a&gt;), you should occasionally coalesce your sparse tensors to prevent them from growing too large.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e3329b7e66ffa5f9cc2ce06f055a9b6ced15b8ee" translate="yes" xml:space="preserve">
          <source>FisherSnedecor</source>
          <target state="translated">FisherSnedecor</target>
        </trans-unit>
        <trans-unit id="0af14ddb20aabbe1bd98f28f2c2f744284ccedb3" translate="yes" xml:space="preserve">
          <source>Flatten</source>
          <target state="translated">Flatten</target>
        </trans-unit>
        <trans-unit id="557473442912b0bbbc1f4c6573c0fe9837b23ef2" translate="yes" xml:space="preserve">
          <source>Flattens &lt;code&gt;dims&lt;/code&gt; into a single dimension with name &lt;code&gt;out_dim&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="80a931c216b5d2a192745812d7ca953d11d59b70" translate="yes" xml:space="preserve">
          <source>Flattens a contiguous range of dims in a tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e2ed5b97e25777e5a578840b676a3b9e44b41cab" translate="yes" xml:space="preserve">
          <source>Flattens a contiguous range of dims into a tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bcfce6cdf44c8905f6faca75da235eedd5635aac" translate="yes" xml:space="preserve">
          <source>Flattens a contiguous range of dims into a tensor. For use with &lt;code&gt;Sequential&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db0a60d5a36fb0f5467808a539ac8c0b01a8dadd" translate="yes" xml:space="preserve">
          <source>Flip array in the left/right direction, returning a new tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9abeaf079a0d69d0b2ee4704e055dbacab017f3" translate="yes" xml:space="preserve">
          <source>Flip array in the up/down direction, returning a new tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d6b601389a5cfb7f115ef7332083ae3431e4e4e3" translate="yes" xml:space="preserve">
          <source>Flip the entries in each column in the up/down direction. Rows are preserved, but appear in a different order than before.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f14bb6de935d5a8cea7f1dc6e4323a38f4e11462" translate="yes" xml:space="preserve">
          <source>Flip the entries in each row in the left/right direction. Columns are preserved, but appear in a different order than before.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="23c9f78a2bf71d761dc5a430092962a70ec045f2" translate="yes" xml:space="preserve">
          <source>FloatFunctional</source>
          <target state="translated">FloatFunctional</target>
        </trans-unit>
        <trans-unit id="a2d156b66635ae5fe54c5766d1380c6080564834" translate="yes" xml:space="preserve">
          <source>Floating-point Tensors produced in an autocast-enabled region may be &lt;code&gt;float16&lt;/code&gt;. After returning to an autocast-disabled region, using them with floating-point Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s) produced in the autocast region back to &lt;code&gt;float32&lt;/code&gt; (or other dtype if desired). If a Tensor from the autocast region is already &lt;code&gt;float32&lt;/code&gt;, the cast is a no-op, and incurs no additional overhead. Example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0352ecfbeaceff6ef8214a3af42e602c1d2f0c6a" translate="yes" xml:space="preserve">
          <source>Flushes the event file to disk. Call this method to make sure that all pending events have been written to disk.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6ba0db1f814179114ec82fbf188b2eb5be2596e" translate="yes" xml:space="preserve">
          <source>Fold</source>
          <target state="translated">Fold</target>
        </trans-unit>
        <trans-unit id="ab2123970899470af7c3bcdbf9832cd5ea8344b1" translate="yes" xml:space="preserve">
          <source>Following this tutorial &lt;a href=&quot;https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html&quot;&gt;Extending TorchScript with Custom C++ Operators&lt;/a&gt;, you can create and register your own custom ops implementation in PyTorch. Here&amp;rsquo;s how to export such model to ONNX.:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="662ac04d8757de24ad35f426f956e3ec58188dcf" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;#iterable-style-datasets&quot;&gt;iterable-style datasets&lt;/a&gt;, data loading order is entirely controlled by the user-defined iterable. This allows easier implementations of chunk-reading and dynamic batch size (e.g., by yielding a batched sample at each time).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c762389bca1bb2b05603a57f1353b6b58529cac2" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; objects returned by &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;future.wait()&lt;/code&gt; should not be called after &lt;code&gt;shutdown()&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="068880daf2a8a0b7a7463ef0178819889346c85b" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;N&lt;/code&gt;-dimensional padding, use &lt;a href=&quot;../nn.functional#torch.nn.functional.pad&quot;&gt;&lt;code&gt;torch.nn.functional.pad()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0224a530bfe0946c7afd8a4140361f1cd0dcd86b" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;torch.nn.functional&lt;/code&gt; operators, we support the following:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e1e6b2fc17a48e201a5b7dbea3ac88cd7be57af5" translate="yes" xml:space="preserve">
          <source>For CPU tensors, this method is currently only available with MKL. Use &lt;a href=&quot;../backends#torch.backends.mkl.is_available&quot;&gt;&lt;code&gt;torch.backends.mkl.is_available()&lt;/code&gt;&lt;/a&gt; to check if MKL is installed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a9dc733d94431c6a4d80dbab50dc02413b91172" translate="yes" xml:space="preserve">
          <source>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same configuration. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cufft-plan-cache&quot;&gt;cuFFT plan cache&lt;/a&gt; for more details on how to monitor and control the cache.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3f7242e16cf3e0469ec63da054b257bd101a205" translate="yes" xml:space="preserve">
          <source>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. For CPU tensors, an error is thrown.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0338c8903416c8af92041cbd33c12e566bca8ea7" translate="yes" xml:space="preserve">
          <source>For Tensors that have &lt;a href=&quot;#torch.Tensor.requires_grad&quot;&gt;&lt;code&gt;requires_grad&lt;/code&gt;&lt;/a&gt; which is &lt;code&gt;True&lt;/code&gt;, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so &lt;code&gt;grad_fn&lt;/code&gt; is None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ce3ce048e3708a40dbc3057fc7dbd0788b47316f" translate="yes" xml:space="preserve">
          <source>For Tensors that have &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt;&lt;code&gt;requires_grad&lt;/code&gt;&lt;/a&gt; which is &lt;code&gt;True&lt;/code&gt;, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so &lt;code&gt;grad_fn&lt;/code&gt; is None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d2d3af0bd6ef80615c67c0f72bb823e693063c0" translate="yes" xml:space="preserve">
          <source>For a 3-D tensor the output is specified by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f570fa1da407a7894b9ad9cdc01e54242454c59" translate="yes" xml:space="preserve">
          <source>For a 3-D tensor, &lt;code&gt;self&lt;/code&gt; is updated as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd4d8e8f7d7355e68f887ffe9f112d1d44ab2731" translate="yes" xml:space="preserve">
          <source>For a comprehensive list of name inference rules, see &lt;a href=&quot;name_inference#name-inference-reference-doc&quot;&gt;Named Tensors operator coverage&lt;/a&gt;. Here are two common operations that may be useful to go over:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f91f8919582617014f59317b7352c3f1154712ab" translate="yes" xml:space="preserve">
          <source>For a full listing of supported Python features, see &lt;a href=&quot;jit_python_reference#python-language-reference&quot;&gt;Python Language Reference Coverage&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4cd19a97e1a392f82662e959d703403f2351a98b" translate="yes" xml:space="preserve">
          <source>For a gentle introduction to TorchScript, see the &lt;a href=&quot;https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html&quot;&gt;Introduction to TorchScript&lt;/a&gt; tutorial.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e0e8c59f7042dfe9b36d63277e5e7440cb658b4" translate="yes" xml:space="preserve">
          <source>For a tensor &lt;code&gt;input&lt;/code&gt; of sizes</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7afccb9a65464232e6685319e5c664295be01466" translate="yes" xml:space="preserve">
          <source>For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see the &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_export.html&quot;&gt;Loading a PyTorch Model in C++&lt;/a&gt; tutorial.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c08a3397ebd78b324e2b03c935d901274f6914be" translate="yes" xml:space="preserve">
          <source>For bags of constant length and no &lt;code&gt;per_sample_weights&lt;/code&gt;, this class</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="243fa0279fa6c67f52078764a1246089d77121bd" translate="yes" xml:space="preserve">
          <source>For complex functions, no notion of Jacobian exists. Gradcheck verifies if the numerical and analytical values of Wirtinger and Conjugate Wirtinger derivative are consistent. The gradient computation is done under the assumption that the overall function has a real valued output. For functions with complex output, gradcheck compares the numerical and analytical gradients for two values of &lt;code&gt;grad_output&lt;/code&gt;: 1 and 1j. For more details, check out &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/autograd.html#complex-autograd-doc&quot;&gt;Autograd for Complex Numbers&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f725f3358d3c963d673d4ca7969a99e89c8c101" translate="yes" xml:space="preserve">
          <source>For data loading, passing &lt;code&gt;pin_memory=True&lt;/code&gt; to a &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; will automatically put the fetched data Tensors in pinned memory, and thus enables faster data transfer to CUDA-enabled GPUs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ce5d81031fb4d7e97b33ce443ec1d33497cec1cd" translate="yes" xml:space="preserve">
          <source>For details on input arguments, parameters, and implementation see &lt;a href=&quot;generated/torch.nn.conv1d#torch.nn.Conv1d&quot;&gt;&lt;code&gt;Conv1d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36b1210fa3359dec217e0f8625012720c3f10bc6" translate="yes" xml:space="preserve">
          <source>For details on input arguments, parameters, and implementation see &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17fc1bf0dec370ea37946985facbba42ffd22e0f" translate="yes" xml:space="preserve">
          <source>For details on input arguments, parameters, and implementation see &lt;a href=&quot;generated/torch.nn.conv3d#torch.nn.Conv3d&quot;&gt;&lt;code&gt;Conv3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88fa6dcc55dbef1b20fcb850f35a6375f7b3938c" translate="yes" xml:space="preserve">
          <source>For each element in the input sequence, each layer computes the following function:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d139363266e05a6b552e5defd18e04652bf83d44" translate="yes" xml:space="preserve">
          <source>For each mini-batch sample, the loss in terms of the 1D input</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="431645996fbd3fae9490db55c077050efdd9469b" translate="yes" xml:space="preserve">
          <source>For each output location &lt;code&gt;output[n, :, h, w]&lt;/code&gt;, the size-2 vector &lt;code&gt;grid[n, h, w]&lt;/code&gt; specifies &lt;code&gt;input&lt;/code&gt; pixel locations &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, which are used to interpolate the output value &lt;code&gt;output[n, :, h, w]&lt;/code&gt;. In the case of 5D inputs, &lt;code&gt;grid[n, d, h, w]&lt;/code&gt; specifies the &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, &lt;code&gt;z&lt;/code&gt; pixel locations for interpolating &lt;code&gt;output[n, :, d, h, w]&lt;/code&gt;. &lt;code&gt;mode&lt;/code&gt; argument specifies &lt;code&gt;nearest&lt;/code&gt; or &lt;code&gt;bilinear&lt;/code&gt; interpolation method to sample the input pixels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a707b704ee783d908df45582d74fb5eed5208cc2" translate="yes" xml:space="preserve">
          <source>For example, assigning to &lt;code&gt;self&lt;/code&gt; outside of the &lt;code&gt;__init__()&lt;/code&gt; method:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="598cd3a9f173b17b51c024dfc8ec80175fdbbd3c" translate="yes" xml:space="preserve">
          <source>For example, if &lt;code&gt;input&lt;/code&gt; is a vector of size N, the result will also be a vector of size N, with elements.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c5eb0f2bf9e3391a2f0f43ac5e99b77a4de61cf1" translate="yes" xml:space="preserve">
          <source>For example, if &lt;code&gt;input&lt;/code&gt; is of shape:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cb7354d4a324678f0e4b20e916940e161856f119" translate="yes" xml:space="preserve">
          <source>For example, if a dataset contains 100 positive and 300 negative examples of a single class, then &lt;code&gt;pos_weight&lt;/code&gt; for the class should be equal to</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec89e688ce49781ad9518cc67374c388cc79a9c9" translate="yes" xml:space="preserve">
          <source>For example, if the system we use for distributed training has 2 nodes, each of which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would like to all-reduce. The following code can serve as a reference:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2aafe679a24a7e81a27fcc1b208b172dea562d1" translate="yes" xml:space="preserve">
          <source>For example, such a dataset, when accessed with &lt;code&gt;dataset[idx]&lt;/code&gt;, could read the &lt;code&gt;idx&lt;/code&gt;-th image and its corresponding label from a folder on the disk.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="734a9ddb49bc44a14f3817f8ab3e399194fe7014" translate="yes" xml:space="preserve">
          <source>For example, such a dataset, when called &lt;code&gt;iter(dataset)&lt;/code&gt;, could return a stream of data reading from a database, a remote server, or even logs generated in real time.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b076a6f19f5c3f95d3197664aa2092f90ac3e4fd" translate="yes" xml:space="preserve">
          <source>For example, suppose that we wanted to implement an operator by operating directly on &lt;a href=&quot;#torch.sparse.FloatTensor._values&quot;&gt;&lt;code&gt;torch.sparse.FloatTensor._values()&lt;/code&gt;&lt;/a&gt;. Multiplication by a scalar can be implemented in the obvious way, as multiplication distributes over addition; however, square root cannot be implemented directly, since &lt;code&gt;sqrt(a + b) != sqrt(a) +
sqrt(b)&lt;/code&gt; (which is what would be computed if you were given an uncoalesced tensor.)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="287e119f8bc584b5b1a94e98cf1131a5d81ded2a" translate="yes" xml:space="preserve">
          <source>For example, this is very useful when one wants to specify per-layer learning rates:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b3dee60b13728b6faa87ab6d08e758ac130cfdfa" translate="yes" xml:space="preserve">
          <source>For inputs of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, &lt;code&gt;value&lt;/code&gt; must be a real number, otherwise an integer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a79d59fd72d10daf74904a50c1c53f2a33467eeb" translate="yes" xml:space="preserve">
          <source>For inputs of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, arguments &lt;code&gt;beta&lt;/code&gt; and &lt;code&gt;alpha&lt;/code&gt; must be real numbers, otherwise they should be integers</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c25cdbf09c7b02888e46dd28ab2af0cd145a33a9" translate="yes" xml:space="preserve">
          <source>For inputs of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, arguments &lt;code&gt;beta&lt;/code&gt; and &lt;code&gt;alpha&lt;/code&gt; must be real numbers, otherwise they should be integers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2524577d6ea6dbe6d56d704e00d4f3e91229b19" translate="yes" xml:space="preserve">
          <source>For instance, if each data sample consists of a 3-channel image and an integral class label, i.e., each element of the dataset returns a tuple &lt;code&gt;(image, class_index)&lt;/code&gt;, the default &lt;code&gt;collate_fn&lt;/code&gt; collates a list of such tuples into a single tuple of a batched image tensor and a batched class label Tensor. In particular, the default &lt;code&gt;collate_fn&lt;/code&gt; has the following properties:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="abc897209b2f98b7966665fa36a5eddbbc44f66d" translate="yes" xml:space="preserve">
          <source>For instance:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc23cb2e2a00ac46f8d5395098475d4070a042d1" translate="yes" xml:space="preserve">
          <source>For iterable-style datasets, since each worker process gets a replica of the &lt;code&gt;dataset&lt;/code&gt; object, naive multi-process loading will often result in duplicated data. Using &lt;a href=&quot;#torch.utils.data.get_worker_info&quot;&gt;&lt;code&gt;torch.utils.data.get_worker_info()&lt;/code&gt;&lt;/a&gt; and/or &lt;code&gt;worker_init_fn&lt;/code&gt;, users may configure each replica independently. (See &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt;&lt;code&gt;IterableDataset&lt;/code&gt;&lt;/a&gt; documentations for how to achieve this. ) For similar reasons, in multi-process loading, the &lt;code&gt;drop_last&lt;/code&gt; argument drops the last non-full batch of each worker&amp;rsquo;s iterable-style dataset replica.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22c3fe0b8bd316af1ba2f91851ec2f8f7995ee83" translate="yes" xml:space="preserve">
          <source>For legacy reasons, a device can be constructed via a single device ordinal, which is treated as a cuda device. This matches &lt;a href=&quot;tensors#torch.Tensor.get_device&quot;&gt;&lt;code&gt;Tensor.get_device()&lt;/code&gt;&lt;/a&gt;, which returns an ordinal for cuda tensors and is not supported for cpu tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0aa3a8773fa895e1e6152468962406770d9b0977" translate="yes" xml:space="preserve">
          <source>For loops over constant nn.ModuleList</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d0ca847043fc995de69359f3596ce96734b521c7" translate="yes" xml:space="preserve">
          <source>For loops over tuples</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5aa0148bf5ef53de46ea15471ea81ef66c0693f4" translate="yes" xml:space="preserve">
          <source>For loops with range</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c492beb9d1c851a0661d50033a687a1b1aab0e7a" translate="yes" xml:space="preserve">
          <source>For map-style datasets, the main process generates the indices using &lt;code&gt;sampler&lt;/code&gt; and sends them to the workers. So any shuffle randomization is done in the main process which guides loading by assigning indices to load.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d65cf1ed6eabecd00c26213c93e0d17f7fcdf45" translate="yes" xml:space="preserve">
          <source>For more complicated uses of the profilers (like in a multi-GPU case), please see &lt;a href=&quot;https://docs.python.org/3/library/profile.html&quot;&gt;https://docs.python.org/3/library/profile.html&lt;/a&gt; or &lt;a href=&quot;autograd#torch.autograd.profiler.profile&quot;&gt;&lt;code&gt;torch.autograd.profiler.profile()&lt;/code&gt;&lt;/a&gt; for more information.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="676185286638fbcbfe90f50f9df7e7cdf2648359" translate="yes" xml:space="preserve">
          <source>For more examples, please look at the implementations of &lt;a href=&quot;#torch.distributions.gumbel.Gumbel&quot;&gt;&lt;code&gt;Gumbel&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributions.half_cauchy.HalfCauchy&quot;&gt;&lt;code&gt;HalfCauchy&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributions.half_normal.HalfNormal&quot;&gt;&lt;code&gt;HalfNormal&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributions.log_normal.LogNormal&quot;&gt;&lt;code&gt;LogNormal&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributions.pareto.Pareto&quot;&gt;&lt;code&gt;Pareto&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributions.weibull.Weibull&quot;&gt;&lt;code&gt;Weibull&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributions.relaxed_bernoulli.RelaxedBernoulli&quot;&gt;&lt;code&gt;RelaxedBernoulli&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical&quot;&gt;&lt;code&gt;RelaxedOneHotCategorical&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b8781b8ea8a1a7fd16e315cb8caa2f0d8fd81ce" translate="yes" xml:space="preserve">
          <source>For more information on &lt;code&gt;torch.sparse_coo&lt;/code&gt; tensors, see &lt;a href=&quot;sparse#sparse-docs&quot;&gt;torch.sparse&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a0ad772d1091f121cc7efb6b451798990b8911a" translate="yes" xml:space="preserve">
          <source>For more information on tensor views, see &lt;a href=&quot;tensor_view#tensor-view-doc&quot;&gt;Tensor Views&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f3909dab7a0a74dfec923801d50d6afccbbd76f" translate="yes" xml:space="preserve">
          <source>For more information on the &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;tensor_attributes#torch.torch.layout&quot;&gt;&lt;code&gt;torch.layout&lt;/code&gt;&lt;/a&gt; attributes of a &lt;a href=&quot;#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt;, see &lt;a href=&quot;tensor_attributes#tensor-attributes-doc&quot;&gt;Tensor Attributes&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e6568b8e5f7166a4f6114617787be1538e34997e" translate="yes" xml:space="preserve">
          <source>For now, normalization code can be found in &lt;code&gt;references/video_classification/transforms.py&lt;/code&gt;, see the &lt;code&gt;Normalize&lt;/code&gt; function there. Note that it differs from standard normalization for images because it assumes the video is 4d.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="43d57f5d34139266a152ef339407d7c7bfa3e16a" translate="yes" xml:space="preserve">
          <source>For numerical stability the implementation reverts to the linear function when</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e2c95cbc7cf538a8a02cb4f1bc38de8b5a3ed2b9" translate="yes" xml:space="preserve">
          <source>For object detection and instance segmentation, the pre-trained models return the predictions of the following classes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d60fa4ee1b04194485d841887bd2a9720912ce7" translate="yes" xml:space="preserve">
          <source>For one, if either</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="388c4450cdbff00300f1e66a7fe9bdbd7ad63df6" translate="yes" xml:space="preserve">
          <source>For person keypoint detection, the accuracies for the pre-trained models are as follows</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50930fbd40ff369c0fa47ef8013fdffeda924fcf" translate="yes" xml:space="preserve">
          <source>For person keypoint detection, the pre-trained model return the keypoints in the following order:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1547488bbec68c16b5473f238ae370eab49bdca4" translate="yes" xml:space="preserve">
          <source>For references on how to use it, please refer to &lt;a href=&quot;https://github.com/pytorch/examples/tree/master/imagenet&quot;&gt;PyTorch example - ImageNet implementation&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35c8decc2166af07962152ef6f02d95b39a40bf1" translate="yes" xml:space="preserve">
          <source>For simplest usage provide &lt;code&gt;dtype&lt;/code&gt; argument that can be float16 or qint8. Weight-only quantization by default is performed for layers with large weights size - i.e. Linear and RNN variants.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b0de76315890608ccfec148ffc9eb96ab3d19d0" translate="yes" xml:space="preserve">
          <source>For summation index</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ae21afa5351a91a1f97daa42d064cca4e245e30f" translate="yes" xml:space="preserve">
          <source>For test time, we report the time for the model evaluation and postprocessing (including mask pasting in image), but not the time for computing the precision-recall.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="294fafc65e707c55d30f92f6e6fa7597c4416793" translate="yes" xml:space="preserve">
          <source>For the case of two input spatial dimensions this operation is sometimes called &lt;code&gt;im2col&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4294b91105e53a9501af3f666602dc49b4fae690" translate="yes" xml:space="preserve">
          <source>For the case of two output spatial dimensions this operation is sometimes called &lt;code&gt;col2im&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="63464d394b40abaadcd5cd07925b101f22c85d5d" translate="yes" xml:space="preserve">
          <source>For the following examples:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="250e5319dc4351da49602e421d48ab9ddd21e40e" translate="yes" xml:space="preserve">
          <source>For the full list of NCCL environment variables, please refer to &lt;a href=&quot;https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html&quot;&gt;NVIDIA NCCL&amp;rsquo;s official documentation&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fbb3381cc7af3b7907b168edfc7c7438b66cd477" translate="yes" xml:space="preserve">
          <source>For the most part, you shouldn&amp;rsquo;t have to care whether or not a sparse tensor is coalesced or not, as most operations will work identically given a coalesced or uncoalesced sparse tensor. However, there are two cases in which you may need to care.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b5ee688f7705f701edf82664a012c925ae4f0c34" translate="yes" xml:space="preserve">
          <source>For the unpacked case, the directions can be separated using &lt;code&gt;output.view(seq_len, batch, num_directions, hidden_size)&lt;/code&gt;, with forward and backward being direction &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; respectively. Similarly, the directions can be separated in the packed case.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c66bf21ad2ff938e858bedebd02eca91c5de881a" translate="yes" xml:space="preserve">
          <source>For these core statistics, values are broken down as follows.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b3352e82009813f0901947623d7383dc915b831" translate="yes" xml:space="preserve">
          <source>For unsorted sequences, use &lt;code&gt;enforce_sorted = False&lt;/code&gt;. If &lt;code&gt;enforce_sorted&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the sequences should be sorted by length in a decreasing order, i.e. &lt;code&gt;input[:,0]&lt;/code&gt; should be the longest sequence, and &lt;code&gt;input[:,B-1]&lt;/code&gt; the shortest one. &lt;code&gt;enforce_sorted = True&lt;/code&gt; is only necessary for ONNX export.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="74aa708d44a325bbc3a2350d27d2ddf6e3e6b36a" translate="yes" xml:space="preserve">
          <source>For unsorted sequences, use &lt;code&gt;enforce_sorted = False&lt;/code&gt;. If &lt;code&gt;enforce_sorted&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the sequences should be sorted in the order of decreasing length. &lt;code&gt;enforce_sorted = True&lt;/code&gt; is only necessary for ONNX export.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f7d55db4ef6ba383bbd9077e4ad197526d0bc5b" translate="yes" xml:space="preserve">
          <source>Force collects GPU memory after it has been released by CUDA IPC.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65edc3b96f1e2f54f14a87eb92d4dea02b1b6e9b" translate="yes" xml:space="preserve">
          <source>Forces completion of a &lt;code&gt;torch.jit.Future[T]&lt;/code&gt; asynchronous task, returning the result of the task.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc619e95fe61cfbb5a61b8e23e153aa71a53f467" translate="yes" xml:space="preserve">
          <source>Forces completion of a &lt;code&gt;torch.jit.Future[T]&lt;/code&gt; asynchronous task, returning the result of the task. See &lt;a href=&quot;torch.jit.fork#torch.jit.fork&quot;&gt;&lt;code&gt;fork()&lt;/code&gt;&lt;/a&gt; for docs and examples. :param func: an asynchronous task reference, created through &lt;code&gt;torch.jit.fork&lt;/code&gt; :type func: torch.jit.Future[T]</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e827f4fab2c732110bd891aca164ea682ff2e51" translate="yes" xml:space="preserve">
          <source>Forks the RNG, so that when you return, the RNG is reset to the state that it was previously in.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0b8013b454609aa16cd91b06faebccde13182b1" translate="yes" xml:space="preserve">
          <source>Forward and backward hooks defined on &lt;code&gt;module&lt;/code&gt; and its submodules will be invoked &lt;code&gt;len(device_ids)&lt;/code&gt; times, each with inputs located on a particular device. Particularly, the hooks are only guaranteed to be executed in correct order with respect to operations on corresponding devices. For example, it is not guaranteed that hooks set via &lt;a href=&quot;torch.nn.module#torch.nn.Module.register_forward_pre_hook&quot;&gt;&lt;code&gt;register_forward_pre_hook()&lt;/code&gt;&lt;/a&gt; be executed before &lt;code&gt;all&lt;/code&gt;&lt;code&gt;len(device_ids)&lt;/code&gt;&lt;a href=&quot;torch.nn.module#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; calls, but that each such hook be executed before the corresponding &lt;a href=&quot;torch.nn.module#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; call of that device.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6acf90721bbeda5f77e7073663e225cf7715008a" translate="yes" xml:space="preserve">
          <source>Forward and backward hooks defined on &lt;code&gt;module&lt;/code&gt; and its submodules won&amp;rsquo;t be invoked anymore, unless the hooks are initialized in the &lt;code&gt;forward()&lt;/code&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="988135b5646708fe12c52e4f90092901a6ed112d" translate="yes" xml:space="preserve">
          <source>Fractional MaxPooling is described in detail in the paper &lt;a href=&quot;https://arxiv.org/abs/1412.6071&quot;&gt;Fractional MaxPooling&lt;/a&gt; by Ben Graham</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2297da08c1909f5dca5e7ab8bf486b563fbe806e" translate="yes" xml:space="preserve">
          <source>FractionalMaxPool2d</source>
          <target state="translated">FractionalMaxPool2d</target>
        </trans-unit>
        <trans-unit id="d790b402d79ac1a723c790313bcd679999474630" translate="yes" xml:space="preserve">
          <source>Frequently Asked Questions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8c7beab7a6d84d3a1004bcc75ccd72648a4866a0" translate="yes" xml:space="preserve">
          <source>Frobenius norm</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e553dabf708d383e58b9442ad6ace6c5038afc7a" translate="yes" xml:space="preserve">
          <source>From the &lt;code&gt;torch.nn.utils&lt;/code&gt; module</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f92451195f62b1d7cd9731015aa1fa56d4159f9d" translate="yes" xml:space="preserve">
          <source>Fully Convolutional Networks</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1e410ad1472b42cb42cc98962428637290b6706" translate="yes" xml:space="preserve">
          <source>Function</source>
          <target state="translated">Function</target>
        </trans-unit>
        <trans-unit id="d8cdf10face49f05a0d7bce562c1cbcff9eeec04" translate="yes" xml:space="preserve">
          <source>Function Calls</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c7552936b2d7609f5d81c59359e027294ef0188" translate="yes" xml:space="preserve">
          <source>Function is called as the entrypoint of the spawned process. This function must be defined at the top level of a module so it can be pickled and spawned. This is a requirement imposed by multiprocessing.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57608b0f76e1461b0dd084a18f9c93c533b79732" translate="yes" xml:space="preserve">
          <source>Function that computes the Hessian of a given scalar function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de45ff60cbfde5e2d453e6ab4052dd57e579a406" translate="yes" xml:space="preserve">
          <source>Function that computes the Jacobian of a given function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="53e89b5d2efea7a989526e66020381aa4e865a00" translate="yes" xml:space="preserve">
          <source>Function that computes the dot product between a vector &lt;code&gt;v&lt;/code&gt; and the Hessian of a given scalar function at the point given by the inputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="287af51397c51f28570adbfd1523afcb4677415b" translate="yes" xml:space="preserve">
          <source>Function that computes the dot product between a vector &lt;code&gt;v&lt;/code&gt; and the Jacobian of the given function at the point given by the inputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db56d8ae51d7ef133b6d7037c14d1d0f86dcbafc" translate="yes" xml:space="preserve">
          <source>Function that computes the dot product between the Hessian of a given scalar function and a vector &lt;code&gt;v&lt;/code&gt; at the point given by the inputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="62c588612b56212caded208831fd6675991af83c" translate="yes" xml:space="preserve">
          <source>Function that computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector &lt;code&gt;v&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6eb78f68900c241e14e0ca55cb63779b55624f2b" translate="yes" xml:space="preserve">
          <source>Function that measures Binary Cross Entropy between target and output logits.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7648ef7c5f8c3df8793ee3a78cea755f5210492d" translate="yes" xml:space="preserve">
          <source>Function that measures the Binary Cross Entropy between the target and the output.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ffd252ff27f5e82a15818c0d16b964ae35b7608" translate="yes" xml:space="preserve">
          <source>Function that returns True when in compilation and False otherwise. This is useful especially with the @unused decorator to leave code in your model that is not yet TorchScript compatible. .. testcode:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f7128796586951f3b8d4c12bad19a0f376bcf68" translate="yes" xml:space="preserve">
          <source>Function that takes the mean element-wise absolute value difference.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="092441d339641c913a709b12bd8927bee99f6a19" translate="yes" xml:space="preserve">
          <source>Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e285c73ee5b2c775ccf90093dc413f8d49580952" translate="yes" xml:space="preserve">
          <source>Function to draw a sequence of &lt;code&gt;n&lt;/code&gt; points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54711e443d02794b60e7868b9b77738a8238e18e" translate="yes" xml:space="preserve">
          <source>Function to fast-forward the state of the &lt;code&gt;SobolEngine&lt;/code&gt; by &lt;code&gt;n&lt;/code&gt; steps. This is equivalent to drawing &lt;code&gt;n&lt;/code&gt; samples without using the samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef0c07b5cc0a1093381723476ea2c34106682639" translate="yes" xml:space="preserve">
          <source>Function to reset the &lt;code&gt;SobolEngine&lt;/code&gt; to base state.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27655b57b5b53e117874eb4514d19d44a2fa2f28" translate="yes" xml:space="preserve">
          <source>Functional higher level API</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4400d33370b62a4424c92ac993690b6bf723355" translate="yes" xml:space="preserve">
          <source>Functional interface</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c75f6c5a3a8ea37a3aa2ef8f8e4a53197661b6a7" translate="yes" xml:space="preserve">
          <source>Functional interface (quantized).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c6f96173e459065a4f35775c868cf69352b58eaa" translate="yes" xml:space="preserve">
          <source>Functionally equivalent to a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;, but represents a single function and does not have any attributes or Parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f72e9d093d3406e36decddb7c64c8207dc32272" translate="yes" xml:space="preserve">
          <source>Functionally equivalent to a &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;, but represents a single function and does not have any attributes or Parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b961dea1dc0c60ddf9a2c8e9d090f6f7d082483" translate="yes" xml:space="preserve">
          <source>Functions</source>
          <target state="translated">Functions</target>
        </trans-unit>
        <trans-unit id="5bab26ebb87fa09b2599a41edb42c2be419cbfdb" translate="yes" xml:space="preserve">
          <source>Functions don&amp;rsquo;t change much, they can be decorated with &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;torch.jit.unused&lt;/code&gt;&lt;/a&gt; if needed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f08615c088136953c00202fd041b9b4df5e1dfea" translate="yes" xml:space="preserve">
          <source>Furthermore, if the &lt;code&gt;functions&lt;/code&gt; argument is supplied, bindings will be automatically generated for each function specified. &lt;code&gt;functions&lt;/code&gt; can either be a list of function names, or a dictionary mapping from function names to docstrings. If a list is given, the name of each function is used as its docstring.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5bace8a23b9deebac72cffe4881dc6eed9968755" translate="yes" xml:space="preserve">
          <source>Furthermore, the outputs are scaled by a factor of</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bd79f69d6d1d8bfef4415bc3f9876396d21f772" translate="yes" xml:space="preserve">
          <source>Fuses a list of modules into a single module</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="936c5a8bd4c7984ab29544b2ca2086d83494e60f" translate="yes" xml:space="preserve">
          <source>Fuses only the following sequence of modules: conv, bn conv, bn, relu conv, relu linear, relu bn, relu All other sequences are left unchanged. For these sequences, replaces the first item in the list with the fused module, replacing the rest of the modules with identity.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f71e96ba4fcb977ab1e675f428bf9d148fa7baf" translate="yes" xml:space="preserve">
          <source>GELU</source>
          <target state="translated">GELU</target>
        </trans-unit>
        <trans-unit id="1f9ad9bc561d09f2f445363cf93033cdd6037e9c" translate="yes" xml:space="preserve">
          <source>GLU</source>
          <target state="translated">GLU</target>
        </trans-unit>
        <trans-unit id="a6a6318544c9b361fc08c6ed94696c4d207d2748" translate="yes" xml:space="preserve">
          <source>GPU</source>
          <target state="translated">GPU</target>
        </trans-unit>
        <trans-unit id="954c88d52c93dd19e22542fcb20c7d583f7b8447" translate="yes" xml:space="preserve">
          <source>GPU hosts with Ethernet interconnect</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9238bee4212524d967f1550988f8d590152434a1" translate="yes" xml:space="preserve">
          <source>GPU hosts with InfiniBand interconnect</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b56fffb35167d283f21ccca2d0bc8fbd0a6fe43" translate="yes" xml:space="preserve">
          <source>GPU tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51c6274e38d61ebd2c2aa1fabf4fb11564b93c03" translate="yes" xml:space="preserve">
          <source>GRU</source>
          <target state="translated">GRU</target>
        </trans-unit>
        <trans-unit id="1e29d48c3333cba171b9e878119cbab38e34e4a1" translate="yes" xml:space="preserve">
          <source>GRUCell</source>
          <target state="translated">GRUCell</target>
        </trans-unit>
        <trans-unit id="cba508b12182b68f501d6af46c4f03f8fc5d2473" translate="yes" xml:space="preserve">
          <source>Gamma</source>
          <target state="translated">Gamma</target>
        </trans-unit>
        <trans-unit id="473631bf9e98f3b45650e597635bf741c36747b6" translate="yes" xml:space="preserve">
          <source>Gathers a list of tensors in a single process.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd67c5035db82b529f60e273fb31bc01d939b7bc" translate="yes" xml:space="preserve">
          <source>Gathers tensors from multiple GPU devices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dfd7c23593ebd41b22c549217d736ad6cbc14502" translate="yes" xml:space="preserve">
          <source>Gathers tensors from the whole group in a list.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a033b46254c69591de655bd57283ab1b1f241ecf" translate="yes" xml:space="preserve">
          <source>Gathers tensors from the whole group in a list. Each tensor in &lt;code&gt;tensor_list&lt;/code&gt; should reside on a separate GPU</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d1ed345e7cd127b4f17c3621e8381fb806d9f267" translate="yes" xml:space="preserve">
          <source>Gathers values along an axis specified by &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="83e2e53da9f325087f4bce1c008f1d1192e058ed" translate="yes" xml:space="preserve">
          <source>Generally speaking, input to this function should contain values following conjugate symmetry. Note that even if &lt;code&gt;onesided&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, often symmetry on some part is still needed. When this requirement is not satisfied, the behavior of &lt;a href=&quot;#torch.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt; is undefined. Since &lt;a href=&quot;../autograd#torch.autograd.gradcheck&quot;&gt;&lt;code&gt;torch.autograd.gradcheck()&lt;/code&gt;&lt;/a&gt; estimates numerical Jacobian with point perturbations, &lt;a href=&quot;#torch.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt; will almost certainly fail the check.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="925a95fee15d092ef688c243751df4f4117845d7" translate="yes" xml:space="preserve">
          <source>Generate a square mask for the sequence. The masked positions are filled with float(&amp;lsquo;-inf&amp;rsquo;). Unmasked positions are filled with float(0.0).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="05e4b9ca24a152cc1be23c09ff6368077d47b2b3" translate="yes" xml:space="preserve">
          <source>Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices &lt;code&gt;theta&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="535a8bdc4a8b5e249e51bb72f3ff867e269f10d8" translate="yes" xml:space="preserve">
          <source>Generates a Vandermonde matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a86ef66b9f9c0d2af98ab49a15285f8bd20e1d42" translate="yes" xml:space="preserve">
          <source>Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="526c10ebcc2b4397308215e12b1bd4ed9189b980" translate="yes" xml:space="preserve">
          <source>Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies &lt;code&gt;transform()&lt;/code&gt; for every transform in the list.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15364dfe6049d908ff11f7465e040686a232a300" translate="yes" xml:space="preserve">
          <source>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="581df05069d7c712c7a78fa21e0f7b76f4e89392" translate="yes" xml:space="preserve">
          <source>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies &lt;code&gt;transform()&lt;/code&gt; for every transform in the list.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b4909d3090c22e034c883a55c3857c6b650fef97" translate="yes" xml:space="preserve">
          <source>Generates n samples or n batches of samples if the distribution parameters are batched.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="63b4637c6c6f7165bea01744025dd36ab607c841" translate="yes" xml:space="preserve">
          <source>Generates uniformly distributed random samples from the half-open interval &lt;code&gt;[low, high)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d20de03126b297e05c13a7d280f33e24c72c537" translate="yes" xml:space="preserve">
          <source>Generator</source>
          <target state="translated">Generator</target>
        </trans-unit>
        <trans-unit id="009c08ccefb98d44d31c65421a4320ecc2bb6f65" translate="yes" xml:space="preserve">
          <source>Generator.device -&amp;gt; device</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3e705cc61a19f33d7c9c030f107a70569966485" translate="yes" xml:space="preserve">
          <source>Generators</source>
          <target state="translated">Generators</target>
        </trans-unit>
        <trans-unit id="80dadd86173d0ff3979257793d4e45beb238b6a2" translate="yes" xml:space="preserve">
          <source>Generics</source>
          <target state="translated">Generics</target>
        </trans-unit>
        <trans-unit id="50911697822f06c09ce3f706d64281d2991753b3" translate="yes" xml:space="preserve">
          <source>Geometric</source>
          <target state="translated">Geometric</target>
        </trans-unit>
        <trans-unit id="43ac4a1c9cd4df862c76bc8567278e105813907d" translate="yes" xml:space="preserve">
          <source>Get &lt;a href=&quot;#torch.distributed.rpc.WorkerInfo&quot;&gt;&lt;code&gt;WorkerInfo&lt;/code&gt;&lt;/a&gt; of a given worker name. Use this &lt;a href=&quot;#torch.distributed.rpc.WorkerInfo&quot;&gt;&lt;code&gt;WorkerInfo&lt;/code&gt;&lt;/a&gt; to avoid passing an expensive string on every invocation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41265238a92192e4ad5f6debaeb489d9d1f1c823" translate="yes" xml:space="preserve">
          <source>Get the Torch Hub cache directory used for storing downloaded models &amp;amp; weights.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31e03005a5c777fa579a5d2e380424ed9ca0b4c8" translate="yes" xml:space="preserve">
          <source>Get the current default floating point &lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8610c06d92827723a75fd0f1cce98077b3a70ce" translate="yes" xml:space="preserve">
          <source>Get the current default floating point &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5aca0eb2fa5dfc1cb36d36748698ea752a002636" translate="yes" xml:space="preserve">
          <source>Get the include paths required to build a C++ or CUDA extension.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8c5303d4517efbd9f11eb88c60066d56bc4690f4" translate="yes" xml:space="preserve">
          <source>Get the k-th diagonal of a given matrix:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d8e48ff0538ba6cbffe42300fa9158799c05f336" translate="yes" xml:space="preserve">
          <source>Get the square matrix where the input vector is the diagonal:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3d9b9988129bd858a97b5e5dca9086f55e31cf1" translate="yes" xml:space="preserve">
          <source>Gets a non-deterministic random number from std::random_device or the current time and uses it to seed a Generator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16cb932f5f31cf2818eda6bf06d30c94768bb98a" translate="yes" xml:space="preserve">
          <source>Gets the cuda capability of a device.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8c370c58908a07f2882a626fa97d9bcfd7bd61c5" translate="yes" xml:space="preserve">
          <source>Gets the current device of the generator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b9be3fb15dfe408dead8b567c5693ec335fb2ec0" translate="yes" xml:space="preserve">
          <source>Gets the name of a device.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6a2e241a18985fac466002399c13c581b746dc24" translate="yes" xml:space="preserve">
          <source>Getting started with Distributed RPC Framework</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7e10d3aed3a471005914064e3e561dd9fce7d89" translate="yes" xml:space="preserve">
          <source>Given a 3-D tensor and reduction using the multiplication operation, &lt;code&gt;self&lt;/code&gt; is updated as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd7589a87267ad50da5e4440a08034e70102a4b2" translate="yes" xml:space="preserve">
          <source>Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a7e24af56b75cfda37479a07ba5602027c1c762" translate="yes" xml:space="preserve">
          <source>Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ca023eeaa09e79566843e7d6c6a44e41c5d8038" translate="yes" xml:space="preserve">
          <source>Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="488568401d49cd08f3be08869dd3fbd14d9e5113" translate="yes" xml:space="preserve">
          <source>Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70ceca1da9acd2d4b46a6674c2ea2f823b1ff770" translate="yes" xml:space="preserve">
          <source>Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="90c7e80a6051c57ef51f03a4a57294f2d936927c" translate="yes" xml:space="preserve">
          <source>Given a list of quantized Tensors, dequantize them and return a list of fp32 Tensors</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="275d4783ccc8a12c168de20cf782230c04ee1b4f" translate="yes" xml:space="preserve">
          <source>Given a quantized Tensor, &lt;code&gt;self.int_repr()&lt;/code&gt; returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a07592b05f2ffe420d78857907dc0a868ad8e9ba" translate="yes" xml:space="preserve">
          <source>Given a quantized Tensor, dequantize it and return the dequantized float Tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df67c816ade50221be71f968c55c25fc9c22abad" translate="yes" xml:space="preserve">
          <source>Given an &lt;code&gt;input&lt;/code&gt; and a flow-field &lt;code&gt;grid&lt;/code&gt;, computes the &lt;code&gt;output&lt;/code&gt; using &lt;code&gt;input&lt;/code&gt; values and pixel locations from &lt;code&gt;grid&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="993de1cd9d3c83efc214190a94f213ab60dcb670" translate="yes" xml:space="preserve">
          <source>Given running min/max as</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1edda6e755754d8aedb4972c3cf98f079bb5938f" translate="yes" xml:space="preserve">
          <source>Given the legs of a right triangle, return its hypotenuse.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2507bf0b60a0a2f4a957fa33007787504bb4d58f" translate="yes" xml:space="preserve">
          <source>Gives us the following diagnostic information:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d4b28b75d3d71965ffab4c160557a8512917b298" translate="yes" xml:space="preserve">
          <source>Globally prunes tensors corresponding to all parameters in &lt;code&gt;parameters&lt;/code&gt; by applying the specified &lt;code&gt;pruning_method&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b2d596c770a42eb120b6d78a8d729e895cf8f96" translate="yes" xml:space="preserve">
          <source>Globally prunes tensors corresponding to all parameters in &lt;code&gt;parameters&lt;/code&gt; by applying the specified &lt;code&gt;pruning_method&lt;/code&gt;. Modifies modules in place by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c964494fd83a4fa2353af497be43d7d3c6b7430" translate="yes" xml:space="preserve">
          <source>Globally unique id to identify the worker.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2dd8834e4e85debb567290745d3b3e1efdcd0e5d" translate="yes" xml:space="preserve">
          <source>Gloo has been hardened by years of extensive use in PyTorch and is thus very reliable. However, as it was designed to perform collective communication, it may not always be the best fit for RPC. For example, each networking operation is synchronous and blocking, which means that it cannot be run in parallel with others. Moreover, it opens a connection between all pairs of nodes, and brings down all of them when one fails, thus reducing the resiliency and the elasticity of the system.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0493f8c5c9a4c1e6227e147f0649196be2a0314a" translate="yes" xml:space="preserve">
          <source>GoogLeNet</source>
          <target state="translated">GoogLeNet</target>
        </trans-unit>
        <trans-unit id="07ad0c4f37a421022704ed36cdcbf7e981dfe5e6" translate="yes" xml:space="preserve">
          <source>GoogLeNet (Inception v1) model architecture from &lt;a href=&quot;http://arxiv.org/abs/1409.4842&quot;&gt;&amp;ldquo;Going Deeper with Convolutions&amp;rdquo;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a46ca2c8e8067233adba7afe56ba3ef36ef0ac6c" translate="yes" xml:space="preserve">
          <source>GoogleNet</source>
          <target state="translated">GoogleNet</target>
        </trans-unit>
        <trans-unit id="ede11b2557f474a2c7b9c843b53bc7eb91dcac40" translate="yes" xml:space="preserve">
          <source>Gradient Scaling</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6ff674c26d399b6074c452be450688d5b72da6db" translate="yes" xml:space="preserve">
          <source>Gradients are modified in-place.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51f814ea6f476134875113c78aa4479584c4db98" translate="yes" xml:space="preserve">
          <source>Graphs can be inspected as shown to confirm that the computation described by a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; is correct, in both automated and manual fashion, as described below.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db56247cfffad115bd03a34895c4144a2f602e2a" translate="yes" xml:space="preserve">
          <source>GroupNorm</source>
          <target state="translated">GroupNorm</target>
        </trans-unit>
        <trans-unit id="ae9629f4ebb82c6331c0809fa9a0e54b00e578e6" translate="yes" xml:space="preserve">
          <source>Groups</source>
          <target state="translated">Groups</target>
        </trans-unit>
        <trans-unit id="28391105a9b17b63b7212f323d98a19b7d728841" translate="yes" xml:space="preserve">
          <source>Gumbel</source>
          <target state="translated">Gumbel</target>
        </trans-unit>
        <trans-unit id="7cf184f4c67ad58283ecb19349720b0cae756829" translate="yes" xml:space="preserve">
          <source>H</source>
          <target state="translated">H</target>
        </trans-unit>
        <trans-unit id="e19beadee3375715c817358699b6b6b7c3b1c276" translate="yes" xml:space="preserve">
          <source>H=\text{embedding\_dim}</source>
          <target state="translated">H=\text{embedding\_dim}</target>
        </trans-unit>
        <trans-unit id="0daac27dde551de614ce1f8b22990869e521a767" translate="yes" xml:space="preserve">
          <source>H_{all}=\text{num\_directions} * \text{hidden\_size}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5af2c1c42a9e7cbeea8d0ef376277c241943a820" translate="yes" xml:space="preserve">
          <source>H_{in1}=\text{in1\_features}</source>
          <target state="translated">H_{in1}=\text{in1\_features}</target>
        </trans-unit>
        <trans-unit id="553b3def323fb898bbf5a634729bc342796f0364" translate="yes" xml:space="preserve">
          <source>H_{in2}=\text{in2\_features}</source>
          <target state="translated">H_{in2}=\text{in2\_features}</target>
        </trans-unit>
        <trans-unit id="3bcb97c0c827228e3e76ce7ff65ced7d17f29099" translate="yes" xml:space="preserve">
          <source>H_{in}</source>
          <target state="translated">H_{in}</target>
        </trans-unit>
        <trans-unit id="208ac84ce5aa93dcc05f3ab66b20dc1d3775c246" translate="yes" xml:space="preserve">
          <source>H_{in} = \text{in\_features}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="699846905d49c5bcf7b21460adb78513ef80b2fb" translate="yes" xml:space="preserve">
          <source>H_{in}=\text{input\_size}</source>
          <target state="translated">H_{in}=\text{input\_size}</target>
        </trans-unit>
        <trans-unit id="7530e9ad4c2fb5dd91aaec26c4ecbcf1a3a5ae05" translate="yes" xml:space="preserve">
          <source>H_{out}</source>
          <target state="translated">H_{out}</target>
        </trans-unit>
        <trans-unit id="29b7895de890ff23af55e3b18083accf3b3c9504" translate="yes" xml:space="preserve">
          <source>H_{out} = (H_{in} - 1) \times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ab9363dd5f0a667cae76fffcb24249e4641e2c1" translate="yes" xml:space="preserve">
          <source>H_{out} = (H_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb1e8745c8404aa0257819793650f55ce47e4a32" translate="yes" xml:space="preserve">
          <source>H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18b0ad397dc5e56655f52d98246617aead364a80" translate="yes" xml:space="preserve">
          <source>H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{kernel\_size}[0]</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79c01a18bf062cd324b81ee63feda955b8a67b5e" translate="yes" xml:space="preserve">
          <source>H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="092f027e0b0b6c6ccdb35d8eefd3fbd19f119f52" translate="yes" xml:space="preserve">
          <source>H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42cf8f4ab52d42aa9e4d4c9b75634fb9ef976615" translate="yes" xml:space="preserve">
          <source>H_{out} = H_{in} \times \text{upscale\_factor}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="be48d8fe2e8d0b5e7eb05cfe3a720b22ad1fddb5" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor H_{in} \times \text{scale\_factor} \right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2df9cd2b82eb2979d700977c589d82b892fd325a" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding[0]} - \text{dilation[0]} \times (\text{kernel\_size[0]} - 1) - 1}{\text{stride[0]}} + 1\right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="475e8a255144af15d34931776b25e492a4505dc7" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="017fc26768fffbdc1fef6a9416fcdd78e1cd6d94" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[0] - \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d6788aaadc5cc718ef1733396e3d48f7aa8bc1e1" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7eddc94bad71df818c92fb147993424e6e6351b5" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fea56e0b8cb35cbb479b336782329a86326003b9" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} - \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="563db076821ec71bea9cecf69cd0383ccd26729e" translate="yes" xml:space="preserve">
          <source>H_{out} = \text{out\_features}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a917c5666fd0c869393621972c028e97e193eb0" translate="yes" xml:space="preserve">
          <source>H_{out}=\text{hidden\_size}</source>
          <target state="translated">H_{out}=\text{hidden\_size}</target>
        </trans-unit>
        <trans-unit id="baeaafcb89405e03dc701698426b69d67220a62d" translate="yes" xml:space="preserve">
          <source>H_{out}=\text{out\_features}</source>
          <target state="translated">H_{out}=\text{out\_features}</target>
        </trans-unit>
        <trans-unit id="e3b87872ef89415e5807523e9374221302af4a0e" translate="yes" xml:space="preserve">
          <source>HalfCauchy</source>
          <target state="translated">HalfCauchy</target>
        </trans-unit>
        <trans-unit id="4fe2e4918ad02d73bac15c6c080f73da0de6891d" translate="yes" xml:space="preserve">
          <source>HalfNormal</source>
          <target state="translated">HalfNormal</target>
        </trans-unit>
        <trans-unit id="593e90ae43cc7b6082dfac4b6d583f5426501c8b" translate="yes" xml:space="preserve">
          <source>Hamming window function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69d7b15573065426b9ad5d17655afb90ef85c68f" translate="yes" xml:space="preserve">
          <source>Hann window function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9deb0b051207a937d2c2524ee2d17196f4c7e2ae" translate="yes" xml:space="preserve">
          <source>HardShrink</source>
          <target state="translated">HardShrink</target>
        </trans-unit>
        <trans-unit id="a1d971d31da1fbc25ba44171ebf839ffcca3d2d9" translate="yes" xml:space="preserve">
          <source>HardTanh</source>
          <target state="translated">HardTanh</target>
        </trans-unit>
        <trans-unit id="c9c68b0024efc57340d60b6a9fea397c5c6ea7cf" translate="yes" xml:space="preserve">
          <source>HardTanh is defined as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2d44614503eb1489dff12f9f7cd6893be6f2de6" translate="yes" xml:space="preserve">
          <source>Hardshrink</source>
          <target state="translated">Hardshrink</target>
        </trans-unit>
        <trans-unit id="6da2cabc8d832449dfd76781dd5cab3e0527d812" translate="yes" xml:space="preserve">
          <source>Hardsigmoid</source>
          <target state="translated">Hardsigmoid</target>
        </trans-unit>
        <trans-unit id="34e8d8510ac49404bdd474758f835c671f823a90" translate="yes" xml:space="preserve">
          <source>Hardswish</source>
          <target state="translated">Hardswish</target>
        </trans-unit>
        <trans-unit id="c5343dec88b0ad5e872efa7466247b124ae44911" translate="yes" xml:space="preserve">
          <source>Hardtanh</source>
          <target state="translated">Hardtanh</target>
        </trans-unit>
        <trans-unit id="0698ac1b047809947767285ae1c3db0b44a90c83" translate="yes" xml:space="preserve">
          <source>Helper decorator for &lt;code&gt;forward&lt;/code&gt; methods of custom autograd functions (subclasses of &lt;a href=&quot;autograd#torch.autograd.Function&quot;&gt;&lt;code&gt;torch.autograd.Function&lt;/code&gt;&lt;/a&gt;). See the &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/amp_examples.html#amp-custom-examples&quot;&gt;example page&lt;/a&gt; for more detail.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="862f59a03aa723c79d215e788d02145626feeb93" translate="yes" xml:space="preserve">
          <source>Helper decorator for backward methods of custom autograd functions (subclasses of &lt;a href=&quot;autograd#torch.autograd.Function&quot;&gt;&lt;code&gt;torch.autograd.Function&lt;/code&gt;&lt;/a&gt;). Ensures that &lt;code&gt;backward&lt;/code&gt; executes with the same autocast state as &lt;code&gt;forward&lt;/code&gt;. See the &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/amp_examples.html#amp-custom-examples&quot;&gt;example page&lt;/a&gt; for more detail.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e2828b0734fec8e5d21a67190b498444f3d1d6ac" translate="yes" xml:space="preserve">
          <source>Helper function to convert all &lt;code&gt;BatchNorm*D&lt;/code&gt; layers in the model to &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt;&lt;/a&gt; layers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ecbcb0e424c40d9931c2c7c8b44c33c5f4e12465" translate="yes" xml:space="preserve">
          <source>Here</source>
          <target state="translated">Here</target>
        </trans-unit>
        <trans-unit id="2bcc88cec50d653f641638296cdbe26d969f54b5" translate="yes" xml:space="preserve">
          <source>Here are the summary of the accuracies for the models trained on the instances set of COCO train2017 and evaluated on COCO val2017.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dab4abc36186b02c3f92677e0eb34f60ea19ee65" translate="yes" xml:space="preserve">
          <source>Here are the ways to call &lt;code&gt;to&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="445a6f9d83de6be3e9c47cd5d94f26ed51643163" translate="yes" xml:space="preserve">
          <source>Here is a code snippet specifies an entrypoint for &lt;code&gt;resnet18&lt;/code&gt; model if we expand the implementation in &lt;code&gt;pytorch/vision/hubconf.py&lt;/code&gt;. In most case importing the right function in &lt;code&gt;hubconf.py&lt;/code&gt; is sufficient. Here we just want to use the expanded version as an example to show how it works. You can see the full script in &lt;a href=&quot;https://github.com/pytorch/vision/blob/master/hubconf.py&quot;&gt;pytorch/vision repo&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ae7bbf97b180351dca19b9329b791cc507f2b072" translate="yes" xml:space="preserve">
          <source>Here is a simple script which exports a pretrained AlexNet as defined in torchvision into ONNX. It runs a single round of inference and then saves the resulting traced model to &lt;code&gt;alexnet.onnx&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92287b24e3387d7af034be3a29ae6b7bb430d9be" translate="yes" xml:space="preserve">
          <source>Here is an example of handling missing symbolic function for &lt;code&gt;elu&lt;/code&gt; operator. We try to export the model and see the error message as below:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e2d985f7b3554227167f11444cb5f1733937d03c" translate="yes" xml:space="preserve">
          <source>Here is another &lt;a href=&quot;https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html&quot;&gt;tutorial of exporting the SuperResolution model to ONNX.&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3314513abe3b45daef540f3d273944cf3438114e" translate="yes" xml:space="preserve">
          <source>Here the model &lt;code&gt;model&lt;/code&gt; can be an arbitrary &lt;a href=&quot;generated/torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;torch.nn.Module&lt;/code&gt;&lt;/a&gt; object. &lt;code&gt;swa_model&lt;/code&gt; will keep track of the running averages of the parameters of the &lt;code&gt;model&lt;/code&gt;. To update these averages, you can use the &lt;code&gt;update_parameters()&lt;/code&gt; function:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd181c7eb29dc3b0fda6e46bc8cb566ca4c818d9" translate="yes" xml:space="preserve">
          <source>Hessian (Tensor or a tuple of tuple of Tensors) if there are a single input,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3d3237992ce2b8eaf6bc76469f46fd9b7ad4ba34" translate="yes" xml:space="preserve">
          <source>HingeEmbeddingLoss</source>
          <target state="translated">HingeEmbeddingLoss</target>
        </trans-unit>
        <trans-unit id="144dc3699fae1cc6bf638916541c8cf0248b5ad0" translate="yes" xml:space="preserve">
          <source>Histogram represented as a tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6abad81420de9ddfe06289f2d7475110d726e0f7" translate="yes" xml:space="preserve">
          <source>Holds parameters in a dictionary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b433389b7ec9270f0a561a21e07b6fafe96e6b4" translate="yes" xml:space="preserve">
          <source>Holds parameters in a list.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57c99ed1c82d3f1f9d82887040191b4b4abeeaa4" translate="yes" xml:space="preserve">
          <source>Holds submodules in a dictionary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26eb2ae7fc8977e64bf6bd3cc41f59552193cc67" translate="yes" xml:space="preserve">
          <source>Holds submodules in a list.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e7b0c97307f7d5f337291cd7806994ff8fc4ef93" translate="yes" xml:space="preserve">
          <source>Holds the data and list of &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence.batch_sizes&quot;&gt;&lt;code&gt;batch_sizes&lt;/code&gt;&lt;/a&gt; of a packed sequence.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8419c96872dc0338c3c6bf4230e67b362cd3efd6" translate="yes" xml:space="preserve">
          <source>Holds the data and list of &lt;code&gt;batch_sizes&lt;/code&gt; of a packed sequence.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0153f88746391d4a3323f9debd32f41ab51f78cf" translate="yes" xml:space="preserve">
          <source>Host to GPU copies are much faster when they originate from pinned (page-locked) memory. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-memory-pinning&quot;&gt;Use pinned memory buffers&lt;/a&gt; for more details on when and how to use pinned memory generally.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="63507ce15285f9aea8a61982f77258d3225e1e12" translate="yes" xml:space="preserve">
          <source>How to adjust learning rate</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="94d972a8a1e2dd3bfc19dfa88cfbe55c7c0c3689" translate="yes" xml:space="preserve">
          <source>How to implement an entrypoint?</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b1304bfd1ec08ee66df1618b8642dc42d805bfa0" translate="yes" xml:space="preserve">
          <source>How to use an optimizer</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6310012cefe177ccf7e96e77a31017e3653da608" translate="yes" xml:space="preserve">
          <source>However the following will error when caching due to dependency reversal:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25a9b274bc53945b3703ea4d8441695a628a5f50" translate="yes" xml:space="preserve">
          <source>However, &lt;a href=&quot;#torch.nn.EmbeddingBag&quot;&gt;&lt;code&gt;EmbeddingBag&lt;/code&gt;&lt;/a&gt; is much more time and memory efficient than using a chain of these operations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0911b09b4e58b715dde3450d0531ebe9129437bf" translate="yes" xml:space="preserve">
          <source>However, &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence.batch_sizes&quot;&gt;&lt;code&gt;batch_sizes&lt;/code&gt;&lt;/a&gt; should always be a CPU &lt;code&gt;torch.int64&lt;/code&gt; tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8db3600da518c9e30c26a5157e4e57a0a03f2dc3" translate="yes" xml:space="preserve">
          <source>However, if sharding results in multiple workers having incomplete last batches, this estimate can still be inaccurate, because (1) an otherwise complete batch can be broken into multiple ones and (2) more than one batch worth of samples can be dropped when &lt;code&gt;drop_last&lt;/code&gt; is set. Unfortunately, PyTorch can not detect such cases in general.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca73ab65568cd125c2d27a22bbd9e863c10b675d" translate="yes" xml:space="preserve">
          <source>I</source>
          <target state="translated">I</target>
        </trans-unit>
        <trans-unit id="17be3fac04b9eb29173ba1517552ac97aa85e862" translate="yes" xml:space="preserve">
          <source>I. M. Sobol. The distribution of points in a cube and the accurate evaluation of integrals. Zh. Vychisl. Mat. i Mat. Phys., 7:784-802, 1967.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3eeaff2d8f8be416c4b7ba0cc9972581ba73c12" translate="yes" xml:space="preserve">
          <source>INDICES WITH CORRESPONDING NAMES:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e5a975b6add84fd53e3710a9ceac15eb06663b7" translate="yes" xml:space="preserve">
          <source>Identity</source>
          <target state="translated">Identity</target>
        </trans-unit>
        <trans-unit id="751c68a3471b1c791efaee0a8e7c24ea0c266efd" translate="yes" xml:space="preserve">
          <source>If</source>
          <target state="translated">If</target>
        </trans-unit>
        <trans-unit id="632018a30643b60c95c40b0734c7a40ff76a740f" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;#torch.distributions.categorical.Categorical.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; is 1D with length-&lt;code&gt;K&lt;/code&gt;, each element is the relative probability of sampling the class at that index.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0fbd2b7cbc8dbe46ae9eba9716b0dcf6af626a55" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;#torch.distributions.categorical.Categorical.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; is 2D, it is treated as a batch of relative probability vectors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e876238e2103f6c99bd92bbd23268dffc89d5e98" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;#torch.hub.set_dir&quot;&gt;&lt;code&gt;set_dir()&lt;/code&gt;&lt;/a&gt; is not called, default path is &lt;code&gt;$TORCH_HOME/hub&lt;/code&gt; where environment variable &lt;code&gt;$TORCH_HOME&lt;/code&gt; defaults to &lt;code&gt;$XDG_CACHE_HOME/torch&lt;/code&gt;. &lt;code&gt;$XDG_CACHE_HOME&lt;/code&gt; follows the X Design Group specification of the Linux filesystem layout, with a default value &lt;code&gt;~/.cache&lt;/code&gt; if the environment variable is not set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2365a78f9c70fd188f67372dcb4ead5e46155be1" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; &amp;gt; 0, it is above the main diagonal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b581ac7d1874578962f7ebaf7dd7355265bfcb4" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; &amp;lt; 0, it is below the main diagonal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d93aaba0d391e746ad390a6373a6f9bdbe62d27c" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; = 0, it is the main diagonal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a1496c02b09e4951bc82bcd52564a0a26f1a0976" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;(h_0, c_0)&lt;/code&gt; is not provided, both &lt;strong&gt;h_0&lt;/strong&gt; and &lt;strong&gt;c_0&lt;/strong&gt; default to zero.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db361da536d20a314407fbca9d9b4b9534c18d89" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;Model&lt;/code&gt; is instantiated it will result in a compilation error since the compiler doesn&amp;rsquo;t know about &lt;code&gt;x&lt;/code&gt;. There are 4 ways to inform the compiler of attributes on &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5fde3f46b9d751a46e85953fec7714aa67e2342e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;accumulate&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the elements in &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; are added to &lt;code&gt;self&lt;/code&gt;. If accumulate is &lt;code&gt;False&lt;/code&gt;, the behavior is undefined if indices contain duplicate elements.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2201c7c6a621b0e5c67fe66d2f987b652400f785" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;accumulate&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the elements in &lt;code&gt;value&lt;/code&gt; are added to &lt;code&gt;self&lt;/code&gt;. If accumulate is &lt;code&gt;False&lt;/code&gt;, the behavior is undefined if indices contain duplicate elements.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="989ae399047ba12cf5c5c1ad5cbe5e9842000a8a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;as_tuple&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, the output tensor containing indices. If &lt;code&gt;as_tuple&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, one 1-D tensor for each dimension, containing the indices of each nonzero element along that dimension.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42d4ccb0a610720c59f54cf25fbe1940e041af3d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;batch1&lt;/code&gt; is a</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f6f5f33da7723c89d0774922372d006d60d5610a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;beta&lt;/code&gt; is 0, then &lt;code&gt;input&lt;/code&gt; will be ignored, and &lt;code&gt;nan&lt;/code&gt; and &lt;code&gt;inf&lt;/code&gt; in it will not be propagated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dc51e352774290d1926f402524b258eb4db4275e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;center&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default), &lt;code&gt;input&lt;/code&gt; will be padded on both sides so that the</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="965719376b6d5fe07ada3da22176f7f46b7ed555" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;center&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then there will be padding e.g. &lt;code&gt;'constant'&lt;/code&gt;, &lt;code&gt;'reflect'&lt;/code&gt;, etc. Left padding can be trimmed off exactly because they can be calculated but right padding cannot be calculated without additional information.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc14ae1693b0f07d578cbfe76b09c86688e0df82" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;compute_uv&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, the returned &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; matrices will be zero matrices of shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b121802d7ccc1f4651e3f4251de10bedc8ca4f7" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;create_graph=False&lt;/code&gt;, &lt;code&gt;backward()&lt;/code&gt; accumulates into &lt;code&gt;.grad&lt;/code&gt; in-place, which preserves its strides.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="653e8379ac4a0083fba63e656d38b6380a7d066b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;create_graph=True&lt;/code&gt;, &lt;code&gt;backward()&lt;/code&gt; replaces &lt;code&gt;.grad&lt;/code&gt; with a new tensor &lt;code&gt;.grad + new grad&lt;/code&gt;, which attempts (but does not guarantee) matching the preexisting &lt;code&gt;.grad&lt;/code&gt;&amp;rsquo;s strides.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65b33918f4984ada2bac2f4ee54a2728e16bb872" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;descending&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; then the elements are sorted in descending order by value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f30e67b5addf429de66048f84b97eb2d56624113" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;dim&lt;/code&gt; is not given, it defaults to the first dimension found with the size 3. Note that this might be unexpected.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e52a79087756b4493bd3fe7986958f5739aaef82" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;dim&lt;/code&gt; is not given, the last dimension of the &lt;code&gt;input&lt;/code&gt; is chosen.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ffe3df3fa3b821e3480cebbf887d0692dbdcfbb2" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;func&lt;/code&gt; is &lt;code&gt;nn.Module&lt;/code&gt; or &lt;code&gt;forward&lt;/code&gt; of &lt;code&gt;nn.Module&lt;/code&gt;, &lt;code&gt;trace&lt;/code&gt; returns a &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; object with a single &lt;code&gt;forward&lt;/code&gt; method containing the traced code. The returned &lt;code&gt;ScriptModule&lt;/code&gt; will have the same set of sub-modules and parameters as the original &lt;code&gt;nn.Module&lt;/code&gt;. If &lt;code&gt;func&lt;/code&gt; is a standalone function, &lt;code&gt;trace&lt;/code&gt; returns &lt;code&gt;ScriptFunction&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="12f27a06f15ef646d5a97006314eddc5168ac16a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;function&lt;/code&gt; invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won&amp;rsquo;t be equivalent, and unfortunately it can&amp;rsquo;t be detected.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7fc30718c6826ba4f06aa25c6aae50a9ac61ea27" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;grid&lt;/code&gt; has values outside the range of &lt;code&gt;[-1, 1]&lt;/code&gt;, the corresponding outputs are handled as defined by &lt;code&gt;padding_mode&lt;/code&gt;. Options are</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56cdf52769f875d95bfc8ef6e8310ccf2d0ab6ef" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;hop_length&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; (default), it is treated as equal to &lt;code&gt;floor(n_fft / 4)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a42f5f55bf046653a704f6db77ba0e3241e7e2f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; has</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b713735765f53d252a6982eaad4ca993659b98bf" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; has zero determinant, this returns &lt;code&gt;(0, -inf)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e500b7aaaa41be4614fc8206c1a0e4e07b1a2886" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is 1D of shape &lt;code&gt;(N)&lt;/code&gt;,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06376d4d271f3b9dcf5c2b5b33c61831ac5086a5" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is 2D of shape &lt;code&gt;(B, N)&lt;/code&gt;,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d721dee0fdf912ec6fe02d3c521f5fb3cf097977" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="333f1219fe75814d72dd2dbd93db4f7d9396ef8a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a matrix (2-D tensor), then returns a 1-D tensor with the diagonal elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="63f74a4f9eae20a71d7f307933ef2802201be33d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a matrix with &lt;code&gt;m&lt;/code&gt; rows, &lt;code&gt;out&lt;/code&gt; is an matrix of shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27b6ec31f1f65b3c25640d5370bad846221e8b75" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a tensor with more than one dimension, then returns a 2-D tensor with diagonal elements equal to a flattened &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40b5681668244b650aeff8654a98b80d07f3df6a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a vector (1-D tensor), then returns a 2-D square tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35915a3eb4d935ad3d429e632e78a9274b0c797a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a vector (1-D tensor), then returns a 2-D square tensor with the elements of &lt;code&gt;input&lt;/code&gt; as the diagonal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad879c1e17f47008543d08a003ad2cbc09fcfa1b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a vector, &lt;code&gt;out&lt;/code&gt; is a vector of size &lt;code&gt;num_samples&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b37263956a979da36ed328c86d1f8d5bafac1ea" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is an n-dimensional tensor with size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f77fab43f2855d468661a78ecf375280629b91c9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, &lt;code&gt;other&lt;/code&gt; should be a real number, otherwise it should be an integer</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="59214511303529b73d11d23823eff1c49a8f295c" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, &lt;code&gt;value&lt;/code&gt; should be a real number, otherwise it should be an integer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7935f4c92d76a69f5c3018eeffd91bc6bda0cd48" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, args &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt; must be real numbers, otherwise they should be integers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="74011e63fd6b8669cda7c1001070f1e13f3cf86f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is of type FloatTensor or DoubleTensor, &lt;code&gt;other&lt;/code&gt; must be a real number, otherwise it should be an integer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd07eaa51527d175519a582a0f6f85c473a1308f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;is_python_module&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, returns the loaded PyTorch extension as a Python module. If &lt;code&gt;is_python_module&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; returns nothing (the shared library is loaded into the process as a side effect).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="150898b85fa40ffdbf63d6a8c5f8e7d9b416ba17" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim is ``True`&lt;/code&gt;, the output tensors are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension(s) &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim`s are squeezed (see :func:`torch.squeeze&lt;/code&gt;), resulting in the output tensors having fewer dimension than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8dfb26e902cefe829de7730f64927f442131d727" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, both the &lt;code&gt;values&lt;/code&gt; and &lt;code&gt;indices&lt;/code&gt; tensors are the same size as &lt;code&gt;input&lt;/code&gt;, except in the dimension &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in both the &lt;code&gt;values&lt;/code&gt; and &lt;code&gt;indices&lt;/code&gt; tensors having 1 fewer dimension than the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4eef57842bb23530ec99ef24c142564e33a57efb" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output dimensions are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimensions being reduced (&lt;code&gt;dim&lt;/code&gt; or all if &lt;code&gt;dim&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;) where they have size 1. Otherwise, the dimensions being reduced are squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;). If &lt;code&gt;q&lt;/code&gt; is a 1D tensor, an extra dimension is prepended to the output tensor with the same size as &lt;code&gt;q&lt;/code&gt; which represents the quantiles.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8af1f9e4f1fca763580351e3fbd0001f54ae795e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensor is of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension &lt;code&gt;dim&lt;/code&gt; where it is of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;generated/torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in the output tensor having 1 fewer dimension than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71fae5aef8701d9d46f893cd636d6b67ec5e6aab" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensor is of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension &lt;code&gt;dim&lt;/code&gt; where it is of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in the output tensor having 1 fewer dimension than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a9d71d5cbb1ac9891b1a744240b0f98bb2945c1" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensor is of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension(s) &lt;code&gt;dim&lt;/code&gt; where it is of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in the output tensor having 1 (or &lt;code&gt;len(dim)&lt;/code&gt;) fewer dimension(s).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c2f11e03ffcd1caa4f43b25af60e0e5bd33899d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensors are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in the output tensors having 1 fewer dimension than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21c9748f09f2547a22882ea8c179fcab7d38a94c" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensors are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in the outputs tensor having 1 fewer dimension than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd1cd26edf38a8fdee9967389f0e6dac29ec3c27" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensors are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension(s) &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim`s are squeezed (see :func:`torch.squeeze&lt;/code&gt;), resulting in the output tensors having fewer dimensions than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5f40df338bf2f9f730257d36666627ccd6548fc" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;dilation&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt; or &lt;code&gt;stride&lt;/code&gt; is an int or a tuple of length 1, their values will be replicated across all spatial dimensions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="55b84dbfa8c62b3f0cac777d20c854c0409394ae" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;largest&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; then the &lt;code&gt;k&lt;/code&gt; smallest elements are returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa798f0a6b2f55ce7a633ab807b16e5f24185685" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;map_location&lt;/code&gt; is a &lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; object or a string containing a device tag, it indicates the location where all tensors should be loaded.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="048f95b6581ac17eb4c969ed9d078d249d78f4f2" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;map_location&lt;/code&gt; is a callable, it will be called once for each serialized storage with two arguments: storage and location. The storage argument will be the initial deserialization of the storage, residing on the CPU. Each serialized storage has a location tag associated with it which identifies the device it was saved from, and this tag is the second argument passed to &lt;code&gt;map_location&lt;/code&gt;. The builtin location tags are &lt;code&gt;'cpu'&lt;/code&gt; for CPU tensors and &lt;code&gt;'cuda:device_id'&lt;/code&gt; (e.g. &lt;code&gt;'cuda:2'&lt;/code&gt;) for CUDA tensors. &lt;code&gt;map_location&lt;/code&gt; should return either &lt;code&gt;None&lt;/code&gt; or a storage. If &lt;code&gt;map_location&lt;/code&gt; returns a storage, it will be used as the final deserialized object, already moved to the right device. Otherwise, &lt;a href=&quot;#torch.load&quot;&gt;&lt;code&gt;torch.load()&lt;/code&gt;&lt;/a&gt; will fall back to the default behavior, as if &lt;code&gt;map_location&lt;/code&gt; wasn&amp;rsquo;t specified.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e6bc544f9847053786078b7293a8d56bcd4734d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;mat1&lt;/code&gt; is a</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42dc8bdc5d9263d6983656748538a841cde8189e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;mat&lt;/code&gt; is a</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ebf39946f09dcba3bcd6988a72fb252101844e11" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;modules&lt;/code&gt; is an &lt;code&gt;OrderedDict&lt;/code&gt;, a &lt;a href=&quot;#torch.nn.ModuleDict&quot;&gt;&lt;code&gt;ModuleDict&lt;/code&gt;&lt;/a&gt;, or an iterable of key-value pairs, the order of new elements in it is preserved.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9da2da81b60822905c169e5c07b984ccee82a21a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n&lt;/code&gt; is negative, then the inverse of the matrix (if invertible) is raised to the power &lt;code&gt;n&lt;/code&gt;. For a batch of matrices, the batched inverse (if invertible) is raised to the power &lt;code&gt;n&lt;/code&gt;. If &lt;code&gt;n&lt;/code&gt; is 0, then an identity matrix is returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a5b0d2161214cfbfdadaec7bb8185cf215fb328" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n&lt;/code&gt; is the number of dimensions in &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;x.T&lt;/code&gt; is equivalent to &lt;code&gt;x.permute(n-1, n-2, ..., 0)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64021abf601b2ef80dc11cb82f4cc12a6574725a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;nonlinearity&lt;/code&gt; is &lt;code&gt;&amp;lsquo;relu&amp;rsquo;&lt;/code&gt;, then ReLU is used in place of tanh.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14d766061a3de2fff996b1f5b1787217fd8c9c07" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalized&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default is &lt;code&gt;False&lt;/code&gt;), the function returns the normalized STFT results, i.e., multiplied by</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="00b342cfeb1bc29dc26ae492fe4fb871890e749d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;obj&lt;/code&gt; is &lt;code&gt;nn.Module&lt;/code&gt;, &lt;code&gt;script&lt;/code&gt; returns a &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; object. The returned &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; will have the same set of sub-modules and parameters as the original &lt;code&gt;nn.Module&lt;/code&gt;. If &lt;code&gt;obj&lt;/code&gt; is a standalone function, a &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; will be returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a63f6cedcd0f8aaa3b09629c749bc8a65b8e3242" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;offset&lt;/code&gt; &amp;gt; 0, it is above the main diagonal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71789ffff3401f164bb9ff6ccd173e3d93708807" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;offset&lt;/code&gt; &amp;lt; 0, it is below the main diagonal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68479729055a8e84ce055c5fd8fcdf5fc489269d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;offset&lt;/code&gt; = 0, it is the main diagonal.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a6e80bcb7f10675f2ca57bde51217a2a4acefff" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;onesided&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default for real input), only values for</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d02abfd63ad40eea8ad8a0098542cd40d9fd90fd" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;only_inputs&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the function will only return a list of gradients w.r.t the specified inputs. If it&amp;rsquo;s &lt;code&gt;False&lt;/code&gt;, then gradient w.r.t. all remaining leaves will still be computed, and will be accumulated into their &lt;code&gt;.grad&lt;/code&gt; attribute.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d55a2e244595c07c3e007024e25f845d2373549a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;other&lt;/code&gt; is of type FloatTensor or DoubleTensor, &lt;code&gt;alpha&lt;/code&gt; must be a real number, otherwise it should be an integer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d24ea9f71cf8ff7e189e7b233b073ed7e74bfb09" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;output_size&lt;/code&gt;, &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;dilation&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt; or &lt;code&gt;stride&lt;/code&gt; is an int or a tuple of length 1 then their values will be replicated across all spatial dimensions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bcbbd502f1fedcc0223634c589add98b568f5531" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;padding&lt;/code&gt; is non-zero, then the input is implicitly padded with negative infinity on both sides for &lt;code&gt;padding&lt;/code&gt; number of points. &lt;code&gt;dilation&lt;/code&gt; is the stride between the elements within the sliding window. This &lt;a href=&quot;https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md&quot;&gt;link&lt;/a&gt; has a nice visualization of the pooling parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2d29b6dc8926940187a1fc2a87fbebaeec5e10f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;padding&lt;/code&gt; is non-zero, then the input is implicitly zero-padded on all three sides for &lt;code&gt;padding&lt;/code&gt; number of points.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a34bc02f3af80858de013a28d3f5075d29b733ee" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;padding&lt;/code&gt; is non-zero, then the input is implicitly zero-padded on both sides for &lt;code&gt;padding&lt;/code&gt; number of points.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0789404550f41a9218f1f8f6d2d8802206a399d3" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;padding&lt;/code&gt; is non-zero, then the input is implicitly zero-padded on both sides for &lt;code&gt;padding&lt;/code&gt; number of points. &lt;code&gt;dilation&lt;/code&gt; controls the spacing between the kernel points. It is harder to describe, but this &lt;a href=&quot;https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md&quot;&gt;link&lt;/a&gt; has a nice visualization of what &lt;code&gt;dilation&lt;/code&gt; does.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3fe50649c0b4459ddadacb46d032eecac188eedd" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;param.grad&lt;/code&gt; is initially &lt;code&gt;None&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c202579e5b79a5ca47cce55bf1764d6590abf4d9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;param&lt;/code&gt; already has a non-sparse &lt;code&gt;.grad&lt;/code&gt; attribute:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b85948d2056bf712cd0ade9dc5602720a12d6ab2" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;param&lt;/code&gt;&amp;rsquo;s memory is non-overlapping and dense, &lt;code&gt;.grad&lt;/code&gt; is created with strides matching &lt;code&gt;param&lt;/code&gt; (thus matching &lt;code&gt;param&lt;/code&gt;&amp;rsquo;s layout).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c75f70ae17fab5880e35d5beec80b653312b036" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;parameters&lt;/code&gt; is an &lt;code&gt;OrderedDict&lt;/code&gt;, a &lt;a href=&quot;#torch.nn.ParameterDict&quot;&gt;&lt;code&gt;ParameterDict&lt;/code&gt;&lt;/a&gt;, or an iterable of key-value pairs, the order of new elements in it is preserved.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa4de2a8b20f68604caa5fc5c42d01d653f7619e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;reduction&lt;/code&gt; is not &lt;code&gt;'none'&lt;/code&gt; (default &lt;code&gt;'mean'&lt;/code&gt;), then:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e3f6cc94ec59a0b57a35c18da0e38bada6245d56" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;return_complex&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default if input is complex), the return is a &lt;code&gt;input.dim() + 1&lt;/code&gt; dimensional complex tensor. If &lt;code&gt;False&lt;/code&gt;, the output is a &lt;code&gt;input.dim() + 2&lt;/code&gt; dimensional real tensor where the last dimension represents the real and imaginary components.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f865281b07a3f9b21462c504abaff9fa28e7b147" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;self.cycle_momentum&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, this function has a side effect of updating the optimizer&amp;rsquo;s momentum.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86cbdc2f08a338f85c6f8af902eddf736f286a0f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;self&lt;/code&gt; is a sparse COO tensor (i.e., with &lt;code&gt;torch.sparse_coo&lt;/code&gt; layout), this returns a view of the contained indices tensor. Otherwise, this throws an error.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a38a9c9f965f62380fce875a97d72dc47659bfb" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;self&lt;/code&gt; is a sparse COO tensor (i.e., with &lt;code&gt;torch.sparse_coo&lt;/code&gt; layout), this returns a view of the contained values tensor. Otherwise, this throws an error.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="95842db2195edf44c14e3fcd797605bf3a42bb61" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;self&lt;/code&gt; is a sparse COO tensor (i.e., with &lt;code&gt;torch.sparse_coo&lt;/code&gt; layout), this returns the number of dense dimensions. Otherwise, this throws an error.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0bcf04d576bbe19078f6b28ab84fc731b5408840" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;self&lt;/code&gt; is a sparse COO tensor (i.e., with &lt;code&gt;torch.sparse_coo&lt;/code&gt; layout), this returns the number of sparse dimensions. Otherwise, this throws an error.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca7f952167ad5a99c7190c4496d9c80f7e0a4624" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;shared&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then memory is shared between all processes. All changes are written to the file. If &lt;code&gt;shared&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then the changes on the storage do not affect the file.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="015e3e2de9fc9120162aae17d21cac0467229fd9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;sizedim&lt;/code&gt; is the size of dimension &lt;code&gt;dimension&lt;/code&gt; for &lt;code&gt;self&lt;/code&gt;, the size of dimension &lt;code&gt;dimension&lt;/code&gt; in the returned tensor will be &lt;code&gt;(sizedim - size) / step + 1&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e63fb825587df089e2f9c8a5d5a13bc6f9e0bcfa" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;some&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default), the method returns the reduced singular value decomposition i.e., if the last two dimensions of &lt;code&gt;input&lt;/code&gt; are &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;n&lt;/code&gt;, then the returned &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; matrices will contain only</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e496e3c20e80698b4576d97cd438ec7eb85163b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;some&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then this function returns the thin (reduced) QR factorization. Otherwise, if &lt;code&gt;some&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, this function returns the complete QR factorization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ff1fe08ba98d042300c393b324450ec3743e612" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;source&lt;/code&gt; is &lt;code&gt;'github'&lt;/code&gt;, &lt;code&gt;repo_or_dir&lt;/code&gt; is expected to be of the form &lt;code&gt;repo_owner/repo_name[:tag_name]&lt;/code&gt; with an optional tag/branch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ddb98294e27497103be69303e2d273125226255" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;source&lt;/code&gt; is &lt;code&gt;'local'&lt;/code&gt;, &lt;code&gt;repo_or_dir&lt;/code&gt; is expected to be a path to a local directory.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e46fd64e2d63e21f45b5d33db7806f0957cdf7b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;source&lt;/code&gt; is a &lt;code&gt;Storage&lt;/code&gt;, the method sets the underlying storage, offset, size, and stride.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cde3a8bb0ace5c85908b26369901162f2c89a71e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;split_size_or_sections&lt;/code&gt; is a list, then &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; will be split into &lt;code&gt;len(split_size_or_sections)&lt;/code&gt; chunks with sizes in &lt;code&gt;dim&lt;/code&gt; according to &lt;code&gt;split_size_or_sections&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a130a738825270ea8d38803841375a69a70f4847" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;split_size_or_sections&lt;/code&gt; is an integer type, then &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension &lt;code&gt;dim&lt;/code&gt; is not divisible by &lt;code&gt;split_size&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cfc288bc3d95c2b0c35b45e151245ce8a50c4585" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;track_running_stats&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt;, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="12927d84c9ce948d1b220f95a6bfac95c27d8d68" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;track_running_stats&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default &lt;code&gt;momentum&lt;/code&gt; of 0.1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="07322ca8d48ec2924a49a2e5be17cc42cbbed920" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;tracker&lt;/code&gt; sets &lt;code&gt;bvars[&amp;ldquo;force_stop&amp;rdquo;] = True&lt;/code&gt;, the iteration process will be hard-stopped.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69139fd88bd2f170758129dcebe723655bc85791" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;unbiased&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel&amp;rsquo;s correction will be used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0054a72bc16c599350023586c7a4c1a0ef3cdabb" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;unbiased&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then the variance will be calculated via the biased estimator. Otherwise, Bessel&amp;rsquo;s correction will be used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64c7abce64d159f698a9f17e0c36ec9abe8f3696" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ea8581ad2e15c4757c07b341a196b7e3beb08404" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, the returned matrix &lt;code&gt;L&lt;/code&gt; is lower-triangular, and the decomposition has the form:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64a08919bf7ea0ada24f0d42a92ac1bba95e2ff7" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then lower triangular portion is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16f312504df9d82f1149f9aeb9d6c3a7744b7863" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; or not provided,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0de37aa5dde3c55904d77c2e915431e5555caf52" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, and</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d54780fe04a147f268bb2ed2fa8ab8ca14bbc641" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the returned matrix &lt;code&gt;U&lt;/code&gt; is upper-triangular, and the decomposition has the form:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0b760bb14c0f3a83b45512f0cb97f37a555bad1" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;vec1&lt;/code&gt; is a vector of size &lt;code&gt;n&lt;/code&gt; and &lt;code&gt;vec2&lt;/code&gt; is a vector of size &lt;code&gt;m&lt;/code&gt;, then &lt;code&gt;input&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with a matrix of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a620298db8d1a210d3df0072bbfa22f1c6b3dc1" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;win_length&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; (default), it is treated as equal to &lt;code&gt;n_fft&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e41ff352c71d6d3681ae49796d3eaff91163d87a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;window_length&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5ba4029b8d23bd264ef58b8c6c6f2f2dbcc50773" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;window_length&lt;/code&gt; is one, then the returned window is a single element tensor containing a one.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36700c2c10655c29379465dfe1fd6e89caba46f9" translate="yes" xml:space="preserve">
          <source>If Statements</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="37f71555dd25e508495629f5b40797b269bbe017" translate="yes" xml:space="preserve">
          <source>If True, all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph. If False, then initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a286c308254a9cdf1bc57c2c79179a955b02516d" translate="yes" xml:space="preserve">
          <source>If a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension which is expected to be of that specific size.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6cc6559de482ee416e97f4d6264cba5f02719534" translate="yes" xml:space="preserve">
          <source>If a zero-dimension tensor operand has a higher category than dimensioned operands, we promote to a type with sufficient size and category to hold all zero-dim tensor operands of that category.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4cbc07c033e4f3f50b5cdb0467c64644f527b6f8" translate="yes" xml:space="preserve">
          <source>If an op is unlisted, we assume it&amp;rsquo;s numerically stable in &lt;code&gt;float16&lt;/code&gt;. If you believe an unlisted op is numerically unstable in &lt;code&gt;float16&lt;/code&gt;, please file an issue.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad31490b158e1b2eea07f572ad07acc51d64ff3a" translate="yes" xml:space="preserve">
          <source>If any checked tensor in &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;grad_outputs&lt;/code&gt; has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from &lt;code&gt;torch.expand()&lt;/code&gt;), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc7adef2433e41254f246b09a9940775c5a1f72e" translate="yes" xml:space="preserve">
          <source>If any checked tensor in &lt;code&gt;input&lt;/code&gt; has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from &lt;code&gt;torch.expand()&lt;/code&gt;), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e818bb7352c628c52d3f8d4ba3508811d8bc58f" translate="yes" xml:space="preserve">
          <source>If any of these would help your use case, please &lt;a href=&quot;https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22module%3A+named+tensor%22&quot;&gt;search if an issue has already been filed&lt;/a&gt; and if not, &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/new/choose&quot;&gt;file one&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="365d8ae01687e5fc5ba3a3c559e4b1609ec1de03" translate="yes" xml:space="preserve">
          <source>If any optimizer steps were skipped the scale is multiplied by &lt;code&gt;backoff_factor&lt;/code&gt; to reduce it. If &lt;code&gt;growth_interval&lt;/code&gt; unskipped iterations occurred consecutively, the scale is multiplied by &lt;code&gt;growth_factor&lt;/code&gt; to increase it.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1080160de99f6c4b9ac7827f3fbead61bb81527f" translate="yes" xml:space="preserve">
          <source>If both arguments are 2-dimensional, the matrix-matrix product is returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d6fa8132ad71df4e876fdeaeb999a43daf3f4278" translate="yes" xml:space="preserve">
          <source>If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N &amp;gt; 2), then a batched matrix multiply is returned. If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. The non-matrix (i.e. batch) dimensions are &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcasted&lt;/a&gt; (and thus must be broadcastable). For example, if &lt;code&gt;input&lt;/code&gt; is a</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d89fe56ed688f10e95f166f512f223aa5ad0f1fe" translate="yes" xml:space="preserve">
          <source>If both tensors are 1-dimensional, the dot product (scalar) is returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b74b95d5cdc716438937d19d93c000b85e6ccb8b" translate="yes" xml:space="preserve">
          <source>If checkpointed segment contains tensors detached from the computational graph by &lt;code&gt;detach()&lt;/code&gt; or &lt;code&gt;torch.no_grad()&lt;/code&gt;, the backward pass will raise an error. This is because &lt;code&gt;checkpoint&lt;/code&gt; makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the &lt;code&gt;checkpoint&lt;/code&gt; function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6148ee076450f28f4a8ddad929803f8c02667e14" translate="yes" xml:space="preserve">
          <source>If downloaded file is a zip file, it will be automatically decompressed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3632499e9221774229ac5b4849f51f9b7ea5baa6" translate="yes" xml:space="preserve">
          <source>If input has shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6054f9e157471bc8a7a27391680f78509c758f80" translate="yes" xml:space="preserve">
          <source>If it is &lt;code&gt;False&lt;/code&gt;, only eigenvalues are computed. If it is &lt;code&gt;True&lt;/code&gt;, both eigenvalues and eigenvectors are computed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2b14334c440f3e0a03846297c6b858c19d59ebd" translate="yes" xml:space="preserve">
          <source>If neither is specified, &lt;code&gt;init_method&lt;/code&gt; is assumed to be &amp;ldquo;env://&amp;rdquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8af7fa7360ca7c6df177260376e8748ca36e7b2e" translate="yes" xml:space="preserve">
          <source>If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module&amp;rsquo;s &lt;code&gt;_load_from_state_dict&lt;/code&gt; method can compare the version number and do appropriate changes if the state dict is from before the change.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab07e4375ba32a4af6a712b8e6818b3c0e5409bd" translate="yes" xml:space="preserve">
          <source>If no inf/NaN gradients are found, invokes &lt;code&gt;optimizer.step()&lt;/code&gt; using the unscaled gradients. Otherwise, &lt;code&gt;optimizer.step()&lt;/code&gt; is skipped to avoid corrupting the params.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35eb8efeed66172f49e69d3035c935e69cd09a9d" translate="yes" xml:space="preserve">
          <source>If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8bad8088b028a7abefb044cb50129322a5504452" translate="yes" xml:space="preserve">
          <source>If one of the elements being compared is a NaN, then that element is returned. &lt;a href=&quot;#torch.maximum&quot;&gt;&lt;code&gt;maximum()&lt;/code&gt;&lt;/a&gt; is not supported for tensors with complex dtypes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d2edfb80af8b4c10b0daabd533575291d8f3436" translate="yes" xml:space="preserve">
          <source>If one of the elements being compared is a NaN, then that element is returned. &lt;a href=&quot;#torch.minimum&quot;&gt;&lt;code&gt;minimum()&lt;/code&gt;&lt;/a&gt; is not supported for tensors with complex dtypes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c998486ad794fdb83e80c2ef35e8c9ca0a076827" translate="yes" xml:space="preserve">
          <source>If one of the processes exits with a non-zero exit status, the remaining processes are killed and an exception is raised with the cause of termination. In the case an exception was caught in the child process, it is forwarded and its traceback is included in the exception raised in the parent process.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7af111fbf2a8bf97e37886e5888c11a588605b8f" translate="yes" xml:space="preserve">
          <source>If provided, the optional argument &lt;code&gt;weight&lt;/code&gt; should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="969ca4bbce09cb3097e3db4dcee3ccac4fa12d57" translate="yes" xml:space="preserve">
          <source>If replacement is &lt;code&gt;True&lt;/code&gt;, samples are drawn with replacement.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="484e62a9e6c54b0aaa3a5007d8a946f20da7704f" translate="yes" xml:space="preserve">
          <source>If the &lt;code&gt;repeats&lt;/code&gt; is &lt;code&gt;tensor([n1, n2, n3, &amp;hellip;])&lt;/code&gt;, then the output will be &lt;code&gt;tensor([0, 0, &amp;hellip;, 1, 1, &amp;hellip;, 2, 2, &amp;hellip;, &amp;hellip;])&lt;/code&gt; where &lt;code&gt;0&lt;/code&gt; appears &lt;code&gt;n1&lt;/code&gt; times, &lt;code&gt;1&lt;/code&gt; appears &lt;code&gt;n2&lt;/code&gt; times, &lt;code&gt;2&lt;/code&gt; appears &lt;code&gt;n3&lt;/code&gt; times, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a469887feb69df132460464b530071b29b07521" translate="yes" xml:space="preserve">
          <source>If the &lt;code&gt;self.data&lt;/code&gt; Tensor already has the correct &lt;code&gt;torch.dtype&lt;/code&gt; and &lt;code&gt;torch.device&lt;/code&gt;, then &lt;code&gt;self&lt;/code&gt; is returned. Otherwise, returns a copy with the desired configuration.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d823313d98ebbfc4cb390dd8145252b2ceb1eaf" translate="yes" xml:space="preserve">
          <source>If the &lt;code&gt;self&lt;/code&gt; Tensor already has the correct &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, then &lt;code&gt;self&lt;/code&gt; is returned. Otherwise, the returned tensor is a copy of &lt;code&gt;self&lt;/code&gt; with the desired &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a55f7772c88fb21a34f619e7cbc1ddc996bd65e" translate="yes" xml:space="preserve">
          <source>If the &lt;code&gt;spawn&lt;/code&gt; start method is used, &lt;code&gt;worker_init_fn&lt;/code&gt; cannot be an unpicklable object, e.g., a lambda function. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/multiprocessing.html#multiprocessing-best-practices&quot;&gt;Multiprocessing best practices&lt;/a&gt; on more details related to multiprocessing in PyTorch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b8c8166f7ac4def7f8a41f990435d7b737da1e1" translate="yes" xml:space="preserve">
          <source>If the RNN is bidirectional, num_directions should be 2, else it should be 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b65be36d1abe2727420269313c2df666e233f015" translate="yes" xml:space="preserve">
          <source>If the consumer process dies abnormally to a fatal signal, the shared tensor could be forever kept in memory as long as the sending process is running.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="191802e65e024d3ca3eb1bca5bd908ca2613d6eb" translate="yes" xml:space="preserve">
          <source>If the current node is the owner, returns a reference to the local value. Otherwise, throws an exception.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c74db6eba0315e50d747303ae70dc492c51af6b7" translate="yes" xml:space="preserve">
          <source>If the decorated &lt;code&gt;forward&lt;/code&gt; is called outside an autocast-enabled region, &lt;a href=&quot;#torch.cuda.amp.custom_fwd&quot;&gt;&lt;code&gt;custom_fwd&lt;/code&gt;&lt;/a&gt; is a no-op and &lt;code&gt;cast_inputs&lt;/code&gt; has no effect.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9bc28fe59df6be023786ded1b0d7a664101bb1a6" translate="yes" xml:space="preserve">
          <source>If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="805768ae7e7883adbd4eb9cfd347ff7553b939ce" translate="yes" xml:space="preserve">
          <source>If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="603aa447ecbcf1ce44b939cf59f78b72d62d8d4f" translate="yes" xml:space="preserve">
          <source>If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype &lt;code&gt;torch.float16&lt;/code&gt; 4) V100 GPU is used, 5) input data is not in &lt;code&gt;PackedSequence&lt;/code&gt; format persistent algorithm can be selected to improve performance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e2ec6b76ced3aa12445764a71f2f934d55e863cb" translate="yes" xml:space="preserve">
          <source>If the forward pass for a particular op has &lt;code&gt;float16&lt;/code&gt; inputs, the backward pass for that op will produce &lt;code&gt;float16&lt;/code&gt; gradients. Gradient values with small magnitudes may not be representable in &lt;code&gt;float16&lt;/code&gt;. These values will flush to zero (&amp;ldquo;underflow&amp;rdquo;), so the update for the corresponding parameters will be lost.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="787caec1c53ac744f6704113ef9317970b83ba1d" translate="yes" xml:space="preserve">
          <source>If the input argument is a tensor, but ONNX asks for a scalar, we have to explicitly do the conversion. The helper function &lt;code&gt;_scalar&lt;/code&gt; can convert a scalar tensor into a python scalar, and &lt;code&gt;_if_scalar_type_as&lt;/code&gt; can turn a Python scalar into a PyTorch tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9db31a4fe372bf1c4d439edd124d674c08f32005" translate="yes" xml:space="preserve">
          <source>If the main process exits abruptly (e.g. because of an incoming signal), Python&amp;rsquo;s &lt;code&gt;multiprocessing&lt;/code&gt; sometimes fails to clean up its children. It&amp;rsquo;s a known caveat, so if you&amp;rsquo;re seeing any resource leaks after interrupting the interpreter, it probably means that this has just happened to you.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f988049101ff18d4e7918e66cf5ad5885aec7070" translate="yes" xml:space="preserve">
          <source>If the norm of a row is lower than &lt;code&gt;maxnorm&lt;/code&gt;, the row is unchanged</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d44fada1fe05f824e96651b70e1c4a292ba8a891" translate="yes" xml:space="preserve">
          <source>If the object is already present in &lt;code&gt;model_dir&lt;/code&gt;, it&amp;rsquo;s deserialized and returned. The default value of &lt;code&gt;model_dir&lt;/code&gt; is &lt;code&gt;&amp;lt;hub_dir&amp;gt;/checkpoints&lt;/code&gt; where &lt;code&gt;hub_dir&lt;/code&gt; is the directory returned by &lt;a href=&quot;#torch.hub.get_dir&quot;&gt;&lt;code&gt;get_dir()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="572431e231db065e224abcc587fb2e502c2edb94" translate="yes" xml:space="preserve">
          <source>If the object is already present in &lt;code&gt;model_dir&lt;/code&gt;, it&amp;rsquo;s deserialized and returned. The default value of &lt;code&gt;model_dir&lt;/code&gt; is &lt;code&gt;&amp;lt;hub_dir&amp;gt;/checkpoints&lt;/code&gt; where &lt;code&gt;hub_dir&lt;/code&gt; is the directory returned by &lt;a href=&quot;hub#torch.hub.get_dir&quot;&gt;&lt;code&gt;get_dir()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e07c64dd92353460c82bc7c43c0298ff2498e3ef" translate="yes" xml:space="preserve">
          <source>If the operator is a non-ATen operator, the symbolic function has to be added in the corresponding PyTorch Function class. Please read the following instructions:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8de0216c16416c65797272e400b10794b950ac17" translate="yes" xml:space="preserve">
          <source>If the operator is an ATen operator, which means you can find the declaration of the function in &lt;code&gt;torch/csrc/autograd/generated/VariableType.h&lt;/code&gt; (available in generated code in PyTorch install dir), you should add the symbolic function in &lt;code&gt;torch/onnx/symbolic_opset&amp;lt;version&amp;gt;.py&lt;/code&gt; and follow the instructions listed as below:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f469a63a2d68271648256ec0018c99f6ecea6e3" translate="yes" xml:space="preserve">
          <source>If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f56fe0773d874b4f8f3b816d146b62978698c172" translate="yes" xml:space="preserve">
          <source>If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08987c5581da900d9050fec1b266346dd018ed86" translate="yes" xml:space="preserve">
          <source>If the sum to the power of &lt;code&gt;p&lt;/code&gt; is zero, the gradient of this function is not defined. This implementation will set the gradient to zero in this case.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a6455b98cbf68564852f3d65be9e3d85724ba33b" translate="yes" xml:space="preserve">
          <source>If the targets are given as a 1d tensor that is the concatenation of individual targets, the target_lengths must add up to the total length of the tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb673583185e53bcc86746d0d9978c1392f01670" translate="yes" xml:space="preserve">
          <source>If the tensor has a batch dimension of size 1, then &lt;code&gt;squeeze(input)&lt;/code&gt; will also remove the batch dimension, which can lead to unexpected errors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67a5ac190ccd238f6283765db614cb7a1d643567" translate="yes" xml:space="preserve">
          <source>If the torch.fft module is imported then &amp;ldquo;torch.fft&amp;rdquo; will refer to the module and not this function. Use &lt;a href=&quot;../tensors#torch.Tensor.fft&quot;&gt;&lt;code&gt;torch.Tensor.fft()&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d413b6561145ebb882d59d64e1d5bb494eb94bfd" translate="yes" xml:space="preserve">
          <source>If the type of a scalar operand is of a higher category than tensor operands (where complex &amp;gt; floating &amp;gt; integral &amp;gt; boolean), we promote to a type with sufficient size to hold all scalar operands of that category.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1fdbc94ec5c6bbfaeab5c1df5a93f67f15cfdc84" translate="yes" xml:space="preserve">
          <source>If there are multiple maximal values in a reduced row then the indices of the first maximal value are returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dffbe07ea95c30719f442abdb52cf12ce069da58" translate="yes" xml:space="preserve">
          <source>If there are multiple minimal values in a reduced row then the indices of the first minimal value are returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c351680cf4894b25f49425f0d4d516d6d33a16ff" translate="yes" xml:space="preserve">
          <source>If there are multiple minimal values then the indices of the first minimal value are returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f03b96d2971ec564ed4f31e34caa43003898f1d" translate="yes" xml:space="preserve">
          <source>If there are no higher-category zero-dim operands, we promote to a type with sufficient size and category to hold all dimensioned operands.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5962a1d4338b99c456d76f53f49888023b22b5fe" translate="yes" xml:space="preserve">
          <source>If this instance is not enabled, returns an empty dict.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bc9e3883be4333b3f7486d77ab3655dd30a42ad7" translate="yes" xml:space="preserve">
          <source>If this is already of the correct type, no copy is performed and the original object is returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ce68525cc568e97b080be8925035deeb766a0ee2" translate="yes" xml:space="preserve">
          <source>If this object is already in CPU memory and on the correct device, then no copy is performed and the original object is returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6788146c6f952e177f58c27fec835e0c0d256c8a" translate="yes" xml:space="preserve">
          <source>If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="242928789aba900d14b07ac7fbbbd1efba673985" translate="yes" xml:space="preserve">
          <source>If true, undefined output grad tensors will be expanded to tensors full of zeros prior to calling the &lt;code&gt;backward()&lt;/code&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c50d408ff8252bcf7756ca41661606361f0e20e" translate="yes" xml:space="preserve">
          <source>If x1 has shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c21dcebc3506f3faeef2282027b5db5536bb6293" translate="yes" xml:space="preserve">
          <source>If you are profiling CUDA code, the first profiler that &lt;code&gt;bottleneck&lt;/code&gt; runs (cProfile) will include the CUDA startup time (CUDA buffer allocation cost) in its time reporting. This should not matter if your bottlenecks result in code much slower than the CUDA startup time.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bb09e45c509c395b596fc5d452b50e8440b313fe" translate="yes" xml:space="preserve">
          <source>If you are using DistributedDataParallel in conjunction with the &lt;a href=&quot;../rpc#distributed-rpc-framework&quot;&gt;Distributed RPC Framework&lt;/a&gt;, you should always use &lt;a href=&quot;../rpc#torch.distributed.autograd.backward&quot;&gt;&lt;code&gt;torch.distributed.autograd.backward()&lt;/code&gt;&lt;/a&gt; to compute gradients and &lt;a href=&quot;../rpc#torch.distributed.optim.DistributedOptimizer&quot;&gt;&lt;code&gt;torch.distributed.optim.DistributedOptimizer&lt;/code&gt;&lt;/a&gt; for optimizing parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2eaa5f7e49b18ce16b1e20ba98c586a98d8b30c" translate="yes" xml:space="preserve">
          <source>If you are working with a multi-GPU model, this function is insufficient to get determinism. To seed all GPUs, use &lt;a href=&quot;#torch.cuda.manual_seed_all&quot;&gt;&lt;code&gt;manual_seed_all()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70fa782f705679e7b47a8754281b13145f0bdb95" translate="yes" xml:space="preserve">
          <source>If you are working with a multi-GPU model, this function will only initialize the seed on one GPU. To initialize all GPUs, use &lt;a href=&quot;#torch.cuda.seed_all&quot;&gt;&lt;code&gt;seed_all()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79878b7e0f390005d12ab18a0ff2e5e3bf2df40c" translate="yes" xml:space="preserve">
          <source>If you have more than one GPU on each node, when using the NCCL and Gloo backend, &lt;a href=&quot;#torch.distributed.broadcast_multigpu&quot;&gt;&lt;code&gt;broadcast_multigpu()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;#torch.distributed.all_reduce_multigpu&quot;&gt;&lt;code&gt;all_reduce_multigpu()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;#torch.distributed.reduce_multigpu&quot;&gt;&lt;code&gt;reduce_multigpu()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;#torch.distributed.all_gather_multigpu&quot;&gt;&lt;code&gt;all_gather_multigpu()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.distributed.reduce_scatter_multigpu&quot;&gt;&lt;code&gt;reduce_scatter_multigpu()&lt;/code&gt;&lt;/a&gt; support distributed collective operations among multiple GPUs within each node. These functions can potentially improve the overall distributed training performance and be easily used by passing a list of tensors. Each Tensor in the passed tensor list needs to be on a separate GPU device of the host where the function is called. Note that the length of the tensor list needs to be identical among all the distributed processes. Also note that currently the multi-GPU collective functions are only supported by the NCCL backend.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="318b82b9810de4a4d14eae5c37c587578c4b554f" translate="yes" xml:space="preserve">
          <source>If you need manual control over &lt;code&gt;.grad&lt;/code&gt;&amp;rsquo;s strides, assign &lt;code&gt;param.grad =&lt;/code&gt; a zeroed tensor with desired strides before the first &lt;code&gt;backward()&lt;/code&gt;, and never reset it to &lt;code&gt;None&lt;/code&gt;. 3 guarantees your layout is preserved as long as &lt;code&gt;create_graph=False&lt;/code&gt;. 4 indicates your layout is &lt;em&gt;likely&lt;/em&gt; preserved even if &lt;code&gt;create_graph=True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67bf24ac1005b3683fde52841b115c0871efc9f7" translate="yes" xml:space="preserve">
          <source>If you need to move a model to GPU via &lt;code&gt;.cuda()&lt;/code&gt;, please do so before constructing optimizers for it. Parameters of a model after &lt;code&gt;.cuda()&lt;/code&gt; will be different objects with those before the call.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="136147687478e135ab5df6494e85fb08cdedac29" translate="yes" xml:space="preserve">
          <source>If you plan on using this module with a &lt;code&gt;nccl&lt;/code&gt; backend or a &lt;code&gt;gloo&lt;/code&gt; backend (that uses Infiniband), together with a DataLoader that uses multiple workers, please change the multiprocessing start method to &lt;code&gt;forkserver&lt;/code&gt; (Python 3 only) or &lt;code&gt;spawn&lt;/code&gt;. Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will likely experience deadlocks if you don&amp;rsquo;t change this setting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="07ab34f8fafef28d79bd246f9789bf5105b86f8b" translate="yes" xml:space="preserve">
          <source>If you plan to backpropagate through QR, note that the current backward implementation is only well-defined when the first</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1cd7b6977f27c5a4e9182f95ff2f859a518a8917" translate="yes" xml:space="preserve">
          <source>If you use &lt;code&gt;torch.save&lt;/code&gt; on one process to checkpoint the module, and &lt;code&gt;torch.load&lt;/code&gt; on some other processes to recover it, make sure that &lt;code&gt;map_location&lt;/code&gt; is configured properly for every process. Without &lt;code&gt;map_location&lt;/code&gt;, &lt;code&gt;torch.load&lt;/code&gt; would recover the module to devices where the module was saved from.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="74c65669b6d4e5ce564cda9931810dc0c170c246" translate="yes" xml:space="preserve">
          <source>If you want downsampling/general resizing, you should use &lt;code&gt;interpolate()&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ace4de17f2e6daea2f622c0b5c24ff868ca712b5" translate="yes" xml:space="preserve">
          <source>If you wish to checkpoint the scaler&amp;rsquo;s state after a particular iteration, &lt;a href=&quot;#torch.cuda.amp.GradScaler.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; should be called after &lt;a href=&quot;#torch.cuda.amp.GradScaler.update&quot;&gt;&lt;code&gt;update()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="064ec7b53383c8d17c5751c9a34c0ac9daf7e8cd" translate="yes" xml:space="preserve">
          <source>If you&amp;rsquo;re using the Gloo backend, you can specify multiple interfaces by separating them by a comma, like this: &lt;code&gt;export GLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3&lt;/code&gt;. The backend will dispatch operations in a round-robin fashion across these interfaces. It is imperative that all processes specify the same number of interfaces in this variable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40a9a99bd41d3dd35eb2ee9f7f247c013023dc94" translate="yes" xml:space="preserve">
          <source>If your InfiniBand has enabled IP over IB, use Gloo, otherwise, use MPI instead. We are planning on adding InfiniBand support for Gloo in the upcoming releases.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ea2434c9b9d3a93397a8c29208e88de7aacbec3" translate="yes" xml:space="preserve">
          <source>If your use case is always 1-D sorted sequence, &lt;a href=&quot;torch.bucketize#torch.bucketize&quot;&gt;&lt;code&gt;torch.bucketize()&lt;/code&gt;&lt;/a&gt; is preferred, because it has fewer dimension checks resulting in slightly better performance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52c2c0cd2584a6259a5f73e3f48627e08a00b06c" translate="yes" xml:space="preserve">
          <source>If, on the other hand, a backward pass with &lt;code&gt;create_graph=True&lt;/code&gt; is underway (in other words, if you are setting up for a double-backward), each function&amp;rsquo;s execution during backward is given a nonzero, useful &lt;code&gt;seq=&amp;lt;N&amp;gt;&lt;/code&gt;. Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. The relationship between backward and double-backward is conceptually the same as the relationship between forward and backward: The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects&amp;rsquo; &lt;code&gt;apply()&lt;/code&gt; ranges are still tagged with &lt;code&gt;stashed seq&lt;/code&gt; numbers, which can be compared to &lt;code&gt;seq&lt;/code&gt; numbers from the backward pass.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eec64bb7b6af56628b6864bb7faf7664a4d5fe90" translate="yes" xml:space="preserve">
          <source>Ignoring the optional batch dimension, this method computes the following expression:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d5e7e82cbad5c6c7282eb3f91a5d122dc9858f8e" translate="yes" xml:space="preserve">
          <source>ImageNet 1-crop error rates (224x224)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8781d615fd77be9578225c40ac67b9471394cced" translate="yes" xml:space="preserve">
          <source>Implementation</source>
          <target state="translated">Implementation</target>
        </trans-unit>
        <trans-unit id="7aa6ad27b14b39704b337bb5273a54caf43dd27d" translate="yes" xml:space="preserve">
          <source>Implementation details: &lt;a href=&quot;https://arxiv.org/pdf/1806.08342.pdf&quot;&gt;https://arxiv.org/pdf/1806.08342.pdf&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e2301206e5db52d9fb3a1373c4f2d128cbca7b09" translate="yes" xml:space="preserve">
          <source>Implementation details: &lt;a href=&quot;https://arxiv.org/pdf/1806.08342.pdf&quot;&gt;https://arxiv.org/pdf/1806.08342.pdf&lt;/a&gt; section 3.2.2</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="123ad8e975b9151926be8910a7ca29c06e0a08b2" translate="yes" xml:space="preserve">
          <source>Implementing a Parameter Server using Distributed RPC Framework</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44171a5d0f5320fd21c59c6c6516557d63659e70" translate="yes" xml:space="preserve">
          <source>Implementing batch RPC processing</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="138a4e92d697c7080168dd78d1fdd8142d73db58" translate="yes" xml:space="preserve">
          <source>Implements Adadelta algorithm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f2f1bb826222edfff8f08a8965769571e6272e0" translate="yes" xml:space="preserve">
          <source>Implements Adagrad algorithm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f57315b0630de232938139c51a2a3c26106ba982" translate="yes" xml:space="preserve">
          <source>Implements Adam algorithm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c77540f43659b4faff14d608e5c738dcda6383c" translate="yes" xml:space="preserve">
          <source>Implements AdamW algorithm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ee370fe200b91f2533d1c27faf2c64f231383a7" translate="yes" xml:space="preserve">
          <source>Implements Adamax algorithm (a variant of Adam based on infinity norm).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c03be935b69dc866871fdae88659292b8000bbfc" translate="yes" xml:space="preserve">
          <source>Implements Averaged Stochastic Gradient Descent.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="05307e82588713f6b300d4819c8e92a2878569db" translate="yes" xml:space="preserve">
          <source>Implements L-BFGS algorithm, heavily inspired by &lt;code&gt;minFunc &amp;lt;https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html&amp;gt;&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf2d15180174f2200375382c7122c5aecdd4ad8b" translate="yes" xml:space="preserve">
          <source>Implements RMSprop algorithm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f29957e1ffbfbf9022787fc2c7cfe246141046c" translate="yes" xml:space="preserve">
          <source>Implements data parallelism at the module level.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="247ad11e0e8ab7f7233963e6adbe7062bce23a44" translate="yes" xml:space="preserve">
          <source>Implements distributed data parallelism that is based on &lt;code&gt;torch.distributed&lt;/code&gt; package at the module level.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd0d130d28d26e9f00e5bf88ccace4711a7a11be" translate="yes" xml:space="preserve">
          <source>Implements lazy version of Adam algorithm suitable for sparse tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a2f16c33652615d2d8893f5da3944e675b6f0334" translate="yes" xml:space="preserve">
          <source>Implements stochastic gradient descent (optionally with momentum).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="83cda66c04c86732b26204a544bebe240466f333" translate="yes" xml:space="preserve">
          <source>Implements the resilient backpropagation algorithm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1ac98376b8dde644a48731cdda5a6c2f0a79cf77" translate="yes" xml:space="preserve">
          <source>Important Notice</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dbea8ebe841db621fdf156d683ca479b155b0a0f" translate="yes" xml:space="preserve">
          <source>Important consideration in the parameters &lt;code&gt;window&lt;/code&gt; and &lt;code&gt;center&lt;/code&gt; so that the envelop created by the summation of all the windows is never zero at certain point in time. Specifically,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0e8d11c90c88612b519564a3a4ddc8195dcee1a" translate="yes" xml:space="preserve">
          <source>In &lt;code&gt;worker_init_fn&lt;/code&gt;, you may access the PyTorch seed set for each worker with either &lt;a href=&quot;#torch.utils.data.get_worker_info&quot;&gt;&lt;code&gt;torch.utils.data.get_worker_info().seed&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/torch.initial_seed#torch.initial_seed&quot;&gt;&lt;code&gt;torch.initial_seed()&lt;/code&gt;&lt;/a&gt;, and use it to seed other libraries before data loading.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1546cf194023a313eda4a32e23fe8da2e85133d5" translate="yes" xml:space="preserve">
          <source>In a multilayer GRU, the input</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ca4b7a5c3d05548dd2f62668a8daeab0e9ec06f" translate="yes" xml:space="preserve">
          <source>In a multilayer LSTM, the input</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e20dc20835fec339c1d3cc16c346bd578e3d90d9" translate="yes" xml:space="preserve">
          <source>In addition to bools, floats, ints, and Tensors can be used in a conditional and will be implicitly casted to a boolean.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a493aa393d90ed5e727839c89230e8cffd94159" translate="yes" xml:space="preserve">
          <source>In addition to the core statistics, we also provide some simple event counters:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="121bfc6001439fa824c41c891327b9bc48e17ca8" translate="yes" xml:space="preserve">
          <source>In addition, one can now create tensors with &lt;code&gt;requires_grad=True&lt;/code&gt; using factory methods such as &lt;a href=&quot;generated/torch.randn#torch.randn&quot;&gt;&lt;code&gt;torch.randn()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.zeros#torch.zeros&quot;&gt;&lt;code&gt;torch.zeros()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.ones#torch.ones&quot;&gt;&lt;code&gt;torch.ones()&lt;/code&gt;&lt;/a&gt;, and others like the following:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14b2f80199ae20e9c340149a30176ab138c9610a" translate="yes" xml:space="preserve">
          <source>In both cases of single-node distributed training or multi-node distributed training, this utility will launch the given number of processes per node (&lt;code&gt;--nproc_per_node&lt;/code&gt;). If used for GPU training, this number needs to be less or equal to the number of GPUs on the current system (&lt;code&gt;nproc_per_node&lt;/code&gt;), and each process will be operating on a single GPU from &lt;em&gt;GPU 0 to GPU (nproc_per_node - 1)&lt;/em&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e876cc3bbe05952d2705985bee305b58baf29ac" translate="yes" xml:space="preserve">
          <source>In cases like these, tracing would not be appropriate and &lt;a href=&quot;torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;scripting&lt;/code&gt;&lt;/a&gt; is a better choice. If you trace such models, you may silently get incorrect results on subsequent invocations of the model. The tracer will try to emit warnings when doing something that may cause an incorrect trace to be produced.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2210461412c5abe9090cdb0e1f87af39668c1506" translate="yes" xml:space="preserve">
          <source>In certain cases, users may want to handle batching manually in dataset code, or simply load individual samples. For example, it could be cheaper to directly load batched data (e.g., bulk reads from a database or reading continuous chunks of memory), or the batch size is data dependent, or the program is designed to work on individual samples. Under these scenarios, it&amp;rsquo;s likely better to not use automatic batching (where &lt;code&gt;collate_fn&lt;/code&gt; is used to collate the samples), but let the data loader directly return each member of the &lt;code&gt;dataset&lt;/code&gt; object.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86e76c5e72e3158e53103708c732b0ca2fb77314" translate="yes" xml:space="preserve">
          <source>In default &lt;code&gt;reduction&lt;/code&gt; mode &lt;code&gt;'mean'&lt;/code&gt;, the losses are averaged for each minibatch over observations &lt;strong&gt;as well as&lt;/strong&gt; over dimensions. &lt;code&gt;'batchmean'&lt;/code&gt; mode gives the correct KL divergence where losses are averaged over batch dimension only. &lt;code&gt;'mean'&lt;/code&gt; mode&amp;rsquo;s behavior will be changed to the same as &lt;code&gt;'batchmean'&lt;/code&gt; in the next major release.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3aa986e649ca9866807ff9f998ea701984440c5" translate="yes" xml:space="preserve">
          <source>In distributed mode, calling the &lt;code&gt;set_epoch()&lt;/code&gt; method at the beginning of each epoch &lt;strong&gt;before&lt;/strong&gt; creating the &lt;code&gt;DataLoader&lt;/code&gt; iterator is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be always used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d7469a9738cf72345e41f4a9561f6407dd3c1952" translate="yes" xml:space="preserve">
          <source>In each forward, &lt;code&gt;module&lt;/code&gt; is &lt;strong&gt;replicated&lt;/strong&gt; on each device, so any updates to the running module in &lt;code&gt;forward&lt;/code&gt; will be lost. For example, if &lt;code&gt;module&lt;/code&gt; has a counter attribute that is incremented in each &lt;code&gt;forward&lt;/code&gt;, it will always stay at the initial value because the update is done on the replicas which are destroyed after &lt;code&gt;forward&lt;/code&gt;. However, &lt;a href=&quot;#torch.nn.DataParallel&quot;&gt;&lt;code&gt;DataParallel&lt;/code&gt;&lt;/a&gt; guarantees that the replica on &lt;code&gt;device[0]&lt;/code&gt; will have its parameters and buffers sharing storage with the base parallelized &lt;code&gt;module&lt;/code&gt;. So &lt;strong&gt;in-place&lt;/strong&gt; updates to the parameters or buffers on &lt;code&gt;device[0]&lt;/code&gt; will be recorded. E.g., &lt;a href=&quot;torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;BatchNorm2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.nn.utils.spectral_norm#torch.nn.utils.spectral_norm&quot;&gt;&lt;code&gt;spectral_norm()&lt;/code&gt;&lt;/a&gt; rely on this behavior to update the buffers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="985dc1f8616a9367ef67f7e8fb270d75af2e49e6" translate="yes" xml:space="preserve">
          <source>In fact, resetting all &lt;code&gt;.grad&lt;/code&gt;s to &lt;code&gt;None&lt;/code&gt; before each accumulation phase, e.g.:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4c7ae6e45efeeb88e3de47cc32025f3afeae3d6" translate="yes" xml:space="preserve">
          <source>In general, folding and unfolding operations are related as follows. Consider &lt;a href=&quot;#torch.nn.Fold&quot;&gt;&lt;code&gt;Fold&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.nn.unfold#torch.nn.Unfold&quot;&gt;&lt;code&gt;Unfold&lt;/code&gt;&lt;/a&gt; instances created with the same parameters:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14a82233b9f46d949d131d9066b4b1e345f4a14b" translate="yes" xml:space="preserve">
          <source>In general, folding and unfolding operations are related as follows. Consider &lt;a href=&quot;torch.nn.fold#torch.nn.Fold&quot;&gt;&lt;code&gt;Fold&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.nn.Unfold&quot;&gt;&lt;code&gt;Unfold&lt;/code&gt;&lt;/a&gt; instances created with the same parameters:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1855810e3cc280e259c5b815913beac56d322de4" translate="yes" xml:space="preserve">
          <source>In general, the basic method spends least time per iteration. However, the robust methods converge much faster and are more stable. So, the usage of the basic method is generally not recommended but there exist cases where the usage of the basic method may be preferred.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a8901639688080e9ddf9086d6c38d66aa6d29ff" translate="yes" xml:space="preserve">
          <source>In general, use the full-rank SVD implementation &lt;code&gt;torch.svd&lt;/code&gt; for dense matrices due to its 10-fold higher performance characteristics. The low-rank SVD will be useful for huge sparse matrices that &lt;code&gt;torch.svd&lt;/code&gt; cannot handle.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f1278ae47dd736566667f86259def089c9bd6886" translate="yes" xml:space="preserve">
          <source>In general, you should make sure that optimized parameters live in consistent locations when optimizers are constructed and used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="809a8a0c78ca14c8503127d00dd103ee7f6c80b0" translate="yes" xml:space="preserve">
          <source>In many cases either tracing or scripting is an easier approach for converting a model to TorchScript. Tracing and scripting can be composed to suit the particular requirements of a part of a model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6485ce8afe7564cea5c8362029901ba50e16aea8" translate="yes" xml:space="preserve">
          <source>In order to spawn up multiple processes per node, you can use either &lt;code&gt;torch.distributed.launch&lt;/code&gt; or &lt;code&gt;torch.multiprocessing.spawn&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40c808fe1c4d6bdfde7e41f404147333e778923e" translate="yes" xml:space="preserve">
          <source>In order to use CuDNN, the following must be satisfied: &lt;code&gt;targets&lt;/code&gt; must be in concatenated format, all &lt;code&gt;input_lengths&lt;/code&gt; must be &lt;code&gt;T&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5224913092c4a84680bc86a2c5b52f3a9390a2e0" translate="yes" xml:space="preserve">
          <source>In other words, for an input of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f63e1b44e4587b6cbe87da3c99588530141677e" translate="yes" xml:space="preserve">
          <source>In particular, solves</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="43678d1fc6309b5815b7dedc1dd771218daaa610" translate="yes" xml:space="preserve">
          <source>In practice we would sample an action from the output of a network, apply this action in an environment, and then use &lt;code&gt;log_prob&lt;/code&gt; to construct an equivalent loss function. Note that we use a negative because optimizers use gradient descent, whilst the rule above assumes gradient ascent. With a categorical policy, the code for implementing REINFORCE would be as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1cb4118455a2bb7d53d0e1417d8853260cb96078" translate="yes" xml:space="preserve">
          <source>In practice, when working with named tensors, one should avoid having unnamed dimensions because their handling can be complicated. It is recommended to lift all unnamed dimensions to be named dimensions by using &lt;a href=&quot;#torch.Tensor.refine_names&quot;&gt;&lt;code&gt;refine_names()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df5ee95eb608e3ef9c532ed1dab9ce8f7003c283" translate="yes" xml:space="preserve">
          <source>In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting &lt;code&gt;torch.backends.cudnn.deterministic =
True&lt;/code&gt;. Please see the notes on &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/randomness.html&quot;&gt;Reproducibility&lt;/a&gt; for background.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e14a819b75469318610313772eb13bce47023e07" translate="yes" xml:space="preserve">
          <source>In the above example, aten::triu is not supported in ONNX, hence exporter falls back on this op. OperatorExportTypes.RAW: Export raw ir. OperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported in ONNX, fall through and export the operator as is, as a custom ONNX op. Using this mode, the op can be exported and implemented by the user for their runtime backend. Example graph:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b3a5fd174642790bebe72eca626e159e3e5dfcd8" translate="yes" xml:space="preserve">
          <source>In the above example, prim::ListConstruct is not supported, hence exporter falls through.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c25e2887a622adc243b3c60db24b8cbea2277b3" translate="yes" xml:space="preserve">
          <source>In the case of batches of square matrices with size less or equal to 32 on a CUDA device, the LU factorization is repeated for singular matrices due to the bug in the MAGMA library (see magma issue 13).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0075edff8aa44b42fa791f6d655add581639408" translate="yes" xml:space="preserve">
          <source>In the example below, &lt;code&gt;swa_model&lt;/code&gt; is the SWA model that accumulates the averages of the weights. We train the model for a total of 300 epochs and we switch to the SWA learning rate schedule and start to collect SWA averages of the parameters at epoch 160:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7889605442c466f81697836a1b9ec1edd9e39e26" translate="yes" xml:space="preserve">
          <source>In the following table, we use 8 V100 GPUs, with CUDA 10.0 and CUDNN 7.4 to report the results. During training, we use a batch size of 2 per GPU, and during testing a batch size of 1 is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="33fa5776db9b9bbfad4e856c392d72e978768794" translate="yes" xml:space="preserve">
          <source>In the future, &lt;a href=&quot;#torch.conj&quot;&gt;&lt;code&gt;torch.conj()&lt;/code&gt;&lt;/a&gt; may return a non-writeable view for an &lt;code&gt;input&lt;/code&gt; of non-complex dtype. It&amp;rsquo;s recommended that programs not modify the tensor returned by &lt;a href=&quot;#torch.conj&quot;&gt;&lt;code&gt;torch.conj()&lt;/code&gt;&lt;/a&gt; when &lt;code&gt;input&lt;/code&gt; is of non-complex dtype to be compatible with this change.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99f1ee81dc5eba42240face3c5c428f4a78edd8a" translate="yes" xml:space="preserve">
          <source>In the future, there will be backends for other frameworks as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d25cb11f14e2999086551374695bf04a7bfaf842" translate="yes" xml:space="preserve">
          <source>In the past, we were often asked: &amp;ldquo;which backend should I use?&amp;rdquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b0a51e5781bdbc893d9a5aff9fe6d5b37f0885c" translate="yes" xml:space="preserve">
          <source>In the returned &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;, operations that have different behaviors in &lt;code&gt;training&lt;/code&gt; and &lt;code&gt;eval&lt;/code&gt; modes will always behave as if it is in the mode it was in during tracing, no matter which mode the &lt;code&gt;ScriptModule&lt;/code&gt; is in.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b3daaa94feed4c186e9e9a97512618fa018aae7f" translate="yes" xml:space="preserve">
          <source>In the simplest case, the output value of the layer with input size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="239b7a59f23d668ac60e6721fee439268e244221" translate="yes" xml:space="preserve">
          <source>In the single-machine synchronous case, &lt;code&gt;torch.distributed&lt;/code&gt; or the &lt;a href=&quot;generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;torch.nn.parallel.DistributedDataParallel()&lt;/code&gt;&lt;/a&gt; wrapper may still have advantages over other approaches to data-parallelism, including &lt;a href=&quot;generated/torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt;&lt;code&gt;torch.nn.DataParallel()&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e133f6f146acb51fdcd21b510ff4101dead133b" translate="yes" xml:space="preserve">
          <source>In the spatial (4-D) case, for &lt;code&gt;input&lt;/code&gt; with shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51ee81d82611ed15672cbe78ad3be970f77568fa" translate="yes" xml:space="preserve">
          <source>In the symbolic function, if the operator is already standardized in ONNX, we just need to create a node to represent the ONNX operator in the graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="05f2edf0966166823005056e0975e4e593b57b37" translate="yes" xml:space="preserve">
          <source>In the symbolic function, if the operator is already standardized in ONNX, we only need to create a node to represent the ONNX operator in the graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="84a6816bbe45c61bb1b6bef7593d5a6c612e3f5a" translate="yes" xml:space="preserve">
          <source>In these regions, CUDA ops run in an op-specific dtype chosen by autocast to improve performance while maintaining accuracy. See the &lt;a href=&quot;#autocast-op-reference&quot;&gt;Autocast Op Reference&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c41d2ba36e63a36e43970f33786546314d161009" translate="yes" xml:space="preserve">
          <source>In this case, &lt;code&gt;nn.Dropout2d()&lt;/code&gt; will help promote independence between feature maps and should be used instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df2da228c2bd90611f132d29496ecf7b2c5d7aaa" translate="yes" xml:space="preserve">
          <source>In this case, &lt;code&gt;nn.Dropout3d()&lt;/code&gt; will help promote independence between feature maps and should be used instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f8b3c9fa943c89a945d6617acdfae17b22a6204e" translate="yes" xml:space="preserve">
          <source>In this case, data-dependent control flow like this can be captured using &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;torch.jit.script()&lt;/code&gt;&lt;/a&gt; instead:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5eaf16a960fd80b4157d86a90c080399cfb6b65" translate="yes" xml:space="preserve">
          <source>In this case, loading from a map-style dataset is roughly equivalent with:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0dd020ad3088741e2272795fe9b24fb663481ec2" translate="yes" xml:space="preserve">
          <source>In this mode, data fetching is done in the same process a &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; is initialized. Therefore, data loading may block computing. However, this mode may be preferred when resource(s) used for sharing data among processes (e.g., shared memory, file descriptors) is limited, or when the entire dataset is small and can be loaded entirely in memory. Additionally, single-process loading often shows more readable error traces and thus is useful for debugging.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="37db45d569ba92c31b1894c46ec1eedf759f889f" translate="yes" xml:space="preserve">
          <source>In this mode, each time an iterator of a &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; is created (e.g., when you call &lt;code&gt;enumerate(dataloader)&lt;/code&gt;), &lt;code&gt;num_workers&lt;/code&gt; worker processes are created. At this point, the &lt;code&gt;dataset&lt;/code&gt;, &lt;code&gt;collate_fn&lt;/code&gt;, and &lt;code&gt;worker_init_fn&lt;/code&gt; are passed to each worker, where they are used to initialize, and fetch data. This means that dataset access together with its internal IO, transforms (including &lt;code&gt;collate_fn&lt;/code&gt;) runs in the worker process.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85295238e8aebda5327b95ca72369c8df3c9eab9" translate="yes" xml:space="preserve">
          <source>In this mode, the result of every computation will have &lt;code&gt;requires_grad=False&lt;/code&gt;, even when the inputs have &lt;code&gt;requires_grad=True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91c8c0bce1c0356637c4656cf0a0cca8ed0c2a41" translate="yes" xml:space="preserve">
          <source>In this section please find the documentation for named tensor specific APIs. For a comprehensive reference for how names are propagated through other PyTorch operators, see &lt;a href=&quot;name_inference#name-inference-reference-doc&quot;&gt;Named Tensors operator coverage&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6458e074d9a813cc3dc5f6d0f9ad52155e927d58" translate="yes" xml:space="preserve">
          <source>In this variant, only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13102db919fdc00f95907c16ae125b4a1f45c313" translate="yes" xml:space="preserve">
          <source>In version 1.6 changed to this from set_training</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de12c6841ad61e706ed4075c85bb11d26176e1ff" translate="yes" xml:space="preserve">
          <source>In-place correctness checks</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16a694343d462dfe3161b9f7e507c3e45c250203" translate="yes" xml:space="preserve">
          <source>In-place operations on Tensors</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="90ef0f5bd340db15b402881b337b9e6fd1b54dd1" translate="yes" xml:space="preserve">
          <source>In-place random sampling</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1723c07101158e28ed701a45820ffb2cbec954ae" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.abs&quot;&gt;&lt;code&gt;abs()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="24948f16983220b792808971db48b1bcdd5b3fa1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.absolute&quot;&gt;&lt;code&gt;absolute()&lt;/code&gt;&lt;/a&gt; Alias for &lt;a href=&quot;#torch.Tensor.abs_&quot;&gt;&lt;code&gt;abs_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dde70002d7610f16bccc15d518923eec4d107bba" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.acos&quot;&gt;&lt;code&gt;acos()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f828fea297a593b936b38df78e2d795295f80f1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.acosh&quot;&gt;&lt;code&gt;acosh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e2ef98fa9aa6aca8a1bdb5dad128434e35bceaa" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.add&quot;&gt;&lt;code&gt;add()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9afdb8ac221f8f75547e87484016d557ba50f247" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addbmm&quot;&gt;&lt;code&gt;addbmm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dcf43a3561084dec90d7c3a2ecc1b6cc0c26f938" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addcdiv&quot;&gt;&lt;code&gt;addcdiv()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b52599a8d04e379d3f3c9ecc7af347c1222d473b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addcmul&quot;&gt;&lt;code&gt;addcmul()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="212bb18c46269c9b244d88318432c473793d6225" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addmm&quot;&gt;&lt;code&gt;addmm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e04b090badbdcab058cb74254d5f97b547843c5" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addmv&quot;&gt;&lt;code&gt;addmv()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="714cae5e9e959037c53ed668822d40bf8774aa1b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addr&quot;&gt;&lt;code&gt;addr()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85808cd790e595c311410030bdf947098dfb3832" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arccos&quot;&gt;&lt;code&gt;arccos()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f222472fe1a711c429f88dfa21d735fc20269d70" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arccosh&quot;&gt;&lt;code&gt;arccosh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5850fc97599dacb252ef7b71a0bc059e90ac832" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arcsin&quot;&gt;&lt;code&gt;arcsin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a92421400d2c919bc99b517f289d8339a3ae03e4" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arcsinh&quot;&gt;&lt;code&gt;arcsinh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab6c8d62429cad7d186033198296a84d5c952598" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arctan&quot;&gt;&lt;code&gt;arctan()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e025d0bea89a2469024775370356f0fa69f44cc9" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arctanh&quot;&gt;&lt;code&gt;arctanh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1eda7abb65e5c787ce66fb95749143a3f04888ed" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.asin&quot;&gt;&lt;code&gt;asin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d258dbfd544c95327dd54668d3cb8823507d044" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.asinh&quot;&gt;&lt;code&gt;asinh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e752ea6e17fecf2b883b074dbb3aa938a6f7d1c" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.atan&quot;&gt;&lt;code&gt;atan()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f00b83a4661950dbba55ca10e0c95538d6e98aaf" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.atan2&quot;&gt;&lt;code&gt;atan2()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a9b90fa5b79edeea0ea7f804bf39ca483463ba1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.atanh&quot;&gt;&lt;code&gt;atanh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0cf21533d1ef19709a89c1226676212cc25b35e7" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.baddbmm&quot;&gt;&lt;code&gt;baddbmm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="723c45d84b8a47a2aac768faa389e89dbffdc312" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.bitwise_and&quot;&gt;&lt;code&gt;bitwise_and()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f23ac5a50549f7c4c93c7cc4af1a4a7fb14079c7" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.bitwise_not&quot;&gt;&lt;code&gt;bitwise_not()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1643902bef600d4d0d475acf131454814b25dc26" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.bitwise_or&quot;&gt;&lt;code&gt;bitwise_or()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc90a184109b29c52ddf3915b630a1a54ae9634b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.bitwise_xor&quot;&gt;&lt;code&gt;bitwise_xor()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b071498cdd10ed979300e38b8823b05d84b4e9a" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.ceil&quot;&gt;&lt;code&gt;ceil()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd0a36194221856cef72e71a8bee0b50682fd547" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.clamp&quot;&gt;&lt;code&gt;clamp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70bc593dd1c02b1636bd32a3b21498325eaaa85c" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.cos&quot;&gt;&lt;code&gt;cos()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="30b3045f3354737011026cef07419edca16e51b7" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.cosh&quot;&gt;&lt;code&gt;cosh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85698a440de4a886c432db2c8ba93bb04b9783a5" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.digamma&quot;&gt;&lt;code&gt;digamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b9d988d2d91968da3bc8e9addd60ca857a07078" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.div&quot;&gt;&lt;code&gt;div()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40f813960ae0cb30555d66235642030e14081a78" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.divide&quot;&gt;&lt;code&gt;divide()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3593d29a80b1997092e2a7caab1ae4c89ff665ad" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.eq&quot;&gt;&lt;code&gt;eq()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a597279c8f26129b927bac941304ed95baa85e3a" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.erf&quot;&gt;&lt;code&gt;erf()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cbb6ddfcf060f64c41fb12dadf24c5f4104c8248" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.erfc&quot;&gt;&lt;code&gt;erfc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9d3f26f32a7d82b45bafe3bb5a09586684935c0d" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.erfinv&quot;&gt;&lt;code&gt;erfinv()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="843b7383da05ed2bdd2c3d50d63831578e83d793" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.exp&quot;&gt;&lt;code&gt;exp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="592c5733a1bb63ac93435062ef6cf042a3e0764b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.expm1&quot;&gt;&lt;code&gt;expm1()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d5d395d973d45d767241ba61182b45ea933b25b3" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.fix&quot;&gt;&lt;code&gt;fix()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0ad235d8de2027be5bcf6998afc520cb85c9fc4" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.floor&quot;&gt;&lt;code&gt;floor()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1298ca9624e848339b14dbd7ac9e1cc72e8426d5" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.floor_divide&quot;&gt;&lt;code&gt;floor_divide()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ba34126e0ac2c59a401a9332bbc39d964a9f1676" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.fmod&quot;&gt;&lt;code&gt;fmod()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="05577f4319cd8a9afef3327e267c8259cd201bc1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.frac&quot;&gt;&lt;code&gt;frac()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c9a5d890ffefe6a4582f4b8a23dc1a024530071" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.gcd&quot;&gt;&lt;code&gt;gcd()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64f7412f381bbec829d7e064308cdeedbdb4239d" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.ge&quot;&gt;&lt;code&gt;ge()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4e0348ba557fd7f53f8d998e67417ad216e0ca1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.greater&quot;&gt;&lt;code&gt;greater()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8558b60a835a6d4d07c498aab1d1c2aee7927ef" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.greater_equal&quot;&gt;&lt;code&gt;greater_equal()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="23ff3a9c46bb69e3bf80c84bd30a30c21ea9397e" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.gt&quot;&gt;&lt;code&gt;gt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0101731065c7560702b5f46c4e91cdc9923c8847" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.hypot&quot;&gt;&lt;code&gt;hypot()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ae74e50f4d266279d834bc1480fb185d3f5523f" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.i0&quot;&gt;&lt;code&gt;i0()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a9c7375b3919548ed0ffd6165fe0af4e23e758d4" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.lcm&quot;&gt;&lt;code&gt;lcm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c39b8eebf07573ad5eac2472d11ac00d3ec49609" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.le&quot;&gt;&lt;code&gt;le()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de77dd1c89036ad69daa80304475b1011fb3301e" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.lerp&quot;&gt;&lt;code&gt;lerp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c58f50cdbebdcfb188e2bdfa9fcdc829890f845d" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.less&quot;&gt;&lt;code&gt;less()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b9423b0d6372fd7e2833e62f8095c74ec7f8c46f" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.less_equal&quot;&gt;&lt;code&gt;less_equal()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e2671a3a2693a72154fb41d1a6e377bc20e43a8" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.lgamma&quot;&gt;&lt;code&gt;lgamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9af05e667e56282be6af4fe5b0246f9f857e3a1d" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.log&quot;&gt;&lt;code&gt;log()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c10939ba9c9c50755fefd56b392ef3407792f705" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.log10&quot;&gt;&lt;code&gt;log10()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1458e566ebdc0a91bfc06212d913d1e13187212c" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.log1p&quot;&gt;&lt;code&gt;log1p()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b4555443ad8a2acf3c819dcdfbbef49a10a2e972" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.log2&quot;&gt;&lt;code&gt;log2()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="433b0aeae51de4808da5b68a9beefef42daf02ce" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.logical_and&quot;&gt;&lt;code&gt;logical_and()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c70b5c1f8176e07af5e81b152a7bdfca820fb3f9" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.logical_not&quot;&gt;&lt;code&gt;logical_not()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d91ce8af603413f0aefc2d1b8b6eec1f114d044b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.logical_or&quot;&gt;&lt;code&gt;logical_or()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3109d25b88385322d682d606fa91db53903884b9" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.logical_xor&quot;&gt;&lt;code&gt;logical_xor()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="343ccb07f42d386eca14f79b12f59de018d60e86" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.logit&quot;&gt;&lt;code&gt;logit()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9d856395e389956dab5ecceb9942b3af92fe9ea1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.lt&quot;&gt;&lt;code&gt;lt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="23d64d86cc2e5f1327a6e010cdeb7c115938b177" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.mul&quot;&gt;&lt;code&gt;mul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="320ff1f84d1c7cfe2a1624b610ddcce8af66a764" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.multiply&quot;&gt;&lt;code&gt;multiply()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d10598fcd6504f5f6bb4b9ddb4d89823cb2a72a4" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.mvlgamma&quot;&gt;&lt;code&gt;mvlgamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b3a59563ae4042d5dfe02b4b4d28142c18b881db" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.ne&quot;&gt;&lt;code&gt;ne()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0933e5f6d727692e6688d039a1e1af9fcbe844b2" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.neg&quot;&gt;&lt;code&gt;neg()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b0826ad65145f6f9f94c5d871b782df9fd07711" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.negative&quot;&gt;&lt;code&gt;negative()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a9845a362319d27c686423e43aadf669c3c68aa" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.nextafter&quot;&gt;&lt;code&gt;nextafter()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="28a1d8909a6e02b4e7751f03f7b62a7e62637734" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.not_equal&quot;&gt;&lt;code&gt;not_equal()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f16d420f4d416e8798bbbadc72fbaac73ed31ae3" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.polygamma&quot;&gt;&lt;code&gt;polygamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="59d753fe8af8e0e2fdbff27f828f4dc81a011e2f" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.pow&quot;&gt;&lt;code&gt;pow()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52dd6f8c275f23765de5ee7462ecb62204684869" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.reciprocal&quot;&gt;&lt;code&gt;reciprocal()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca45cdac88c7921f089b79a8f83e20d848c48d4a" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.remainder&quot;&gt;&lt;code&gt;remainder()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="053eb7c5cd301fdd12904be2836f5fb0ec275646" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.rename&quot;&gt;&lt;code&gt;rename()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="818df20d8449f1acb432a28eac3f0b43768c2197" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.renorm&quot;&gt;&lt;code&gt;renorm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="53ce6f2d6ab6ae80005a428178e9b77aef382107" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.round&quot;&gt;&lt;code&gt;round()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b623bd7302eab7378ff509a5b3dd4f0470b21494" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.rsqrt&quot;&gt;&lt;code&gt;rsqrt()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1fe0cf7810c0bc56d540daa9ddb7cec1599e41ee" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sgn&quot;&gt;&lt;code&gt;sgn()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cdcdd0daf61ad10208f07b04eb0183a9d4d2ab44" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sigmoid&quot;&gt;&lt;code&gt;sigmoid()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e2cd3c0abcebc8ed45bd6570406d8c9d4c33ea1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sign&quot;&gt;&lt;code&gt;sign()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69d6918f4d893ee927f13c625b9f268b4eea1c8b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sin&quot;&gt;&lt;code&gt;sin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f906cf7cca8e4f9df35be1db35c6f3f24ecf45b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sinh&quot;&gt;&lt;code&gt;sinh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cbe8e3f4ea9deb4f23b9e270df8f53c6c9f24e99" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sqrt&quot;&gt;&lt;code&gt;sqrt()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8821b663c3d7115389c6fb460a014cd6f46b31a6" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.square&quot;&gt;&lt;code&gt;square()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="331d4255e6beb57fd7b54b749ce35473d9c934d2" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.squeeze&quot;&gt;&lt;code&gt;squeeze()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c0246ff36c2a3383754e9573043424c772cead3" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sub&quot;&gt;&lt;code&gt;sub()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="afadc08d16c14a6f3ab63ac2e1777d4a0c639d3d" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.subtract&quot;&gt;&lt;code&gt;subtract()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e9a157c014d95de0d85c9410108016dcf899ffc" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.t&quot;&gt;&lt;code&gt;t()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2bca115f72f0be4030271dbbf4001e5646c23cd" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.tan&quot;&gt;&lt;code&gt;tan()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="abae0cb9439d45a19abc17669152c5675c6b2f91" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.tanh&quot;&gt;&lt;code&gt;tanh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e2861f743f86145802065db61cf1289846a4dadc" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.transpose&quot;&gt;&lt;code&gt;transpose()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45499df8d625edfd4a83589eb3c8afa20bd32bb9" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.tril&quot;&gt;&lt;code&gt;tril()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="065bb546251aa96738ef601a77cd6f37ec7d9659" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.triu&quot;&gt;&lt;code&gt;triu()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="212e370e8fdc9518bb1c8dd81fce7b074d0a6998" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.true_divide_&quot;&gt;&lt;code&gt;true_divide_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e29d58f0e79f4e7e5e919543670aacaa5b39af55" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.trunc&quot;&gt;&lt;code&gt;trunc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e2464f4a6dc92f29b5fe948a1cb58f6a6f8ec52" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.unsqueeze&quot;&gt;&lt;code&gt;unsqueeze()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40042c187d9c08cff699d8c486b17c8fc8e0f71b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.elu&quot;&gt;&lt;code&gt;elu()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fda2e5a23576045d8c887926b8f5f495cbb00502" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.hardtanh&quot;&gt;&lt;code&gt;hardtanh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4e00c79886a4c5fdfdd4fc0c5274b0108988ba92" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.leaky_relu&quot;&gt;&lt;code&gt;leaky_relu()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eda3fb8c00a121092ce0d25f989c8256cb992365" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.relu&quot;&gt;&lt;code&gt;relu()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e3a76eb13d927dc0ac3b8f72061ef5fd006d9d68" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.rrelu&quot;&gt;&lt;code&gt;rrelu()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf12b7cfa392abc73035a860cd7f9148c1c352d9" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.threshold&quot;&gt;&lt;code&gt;threshold()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b7ad9a8774865b3951b77d446d4e908b37c2715" translate="yes" xml:space="preserve">
          <source>Inception (warning: this model is highly sensitive to changes in operator implementation)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86187099fc5837d4a16585e59250186b1bdbfd39" translate="yes" xml:space="preserve">
          <source>Inception v3</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="512c1383f2d8169cfead07824d0994b018f42ff3" translate="yes" xml:space="preserve">
          <source>Inception v3 model architecture from &lt;a href=&quot;http://arxiv.org/abs/1512.00567&quot;&gt;&amp;ldquo;Rethinking the Inception Architecture for Computer Vision&amp;rdquo;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="228505f85662d947dfd09fcfb9c5416638b8b2ae" translate="yes" xml:space="preserve">
          <source>Independent</source>
          <target state="translated">Independent</target>
        </trans-unit>
        <trans-unit id="c2df9b932637fe9d32a0f16da1c11873398f873d" translate="yes" xml:space="preserve">
          <source>Index</source>
          <target state="translated">Index</target>
        </trans-unit>
        <trans-unit id="6f4e6ca52449e10a302a5dfc3ebba098cd7e6757" translate="yes" xml:space="preserve">
          <source>Indexing, Slicing, Joining, Mutating Ops</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e2d7833039dc978e6eb0d1055910e03a86a4609" translate="yes" xml:space="preserve">
          <source>Indices and tables</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4eb1cf386795f140ff47153f7333aea4dda421e5" translate="yes" xml:space="preserve">
          <source>Indices are ordered from left to right according to when each was sampled (first samples are placed in first column).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68fa16ffd48f366e4fa8d57fea78ff03fcab0191" translate="yes" xml:space="preserve">
          <source>Initialization</source>
          <target state="translated">Initialization</target>
        </trans-unit>
        <trans-unit id="8a509cef8fe7210eb4a6fc80831a2072673fceae" translate="yes" xml:space="preserve">
          <source>Initialize PyTorch&amp;rsquo;s CUDA state. You may need to call this explicitly if you are interacting with PyTorch via its C API, as Python bindings for CUDA functionality will not be available until this initialization takes place. Ordinary users should not need this, as all of PyTorch&amp;rsquo;s CUDA methods automatically initialize CUDA state on-demand.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="73e5b96b48f418d64840badaa42ca942b71c67be" translate="yes" xml:space="preserve">
          <source>Initializes RPC primitives such as the local RPC agent and distributed autograd, which immediately makes the current process ready to send and receive RPCs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd0f315e911df950ab4124032aa329f129570289" translate="yes" xml:space="preserve">
          <source>Initializes the default distributed process group, and this will also initialize the distributed package.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="252c7bda7950bbedbb2a4bb4550ac0f5ef1aa6a8" translate="yes" xml:space="preserve">
          <source>Input lists. It should contain correctly-sized tensors on each GPU to be used for input of the collective, e.g. &lt;code&gt;input_tensor_lists[i]&lt;/code&gt; contains the reduce_scatter input that resides on the GPU of &lt;code&gt;output_tensor_list[i]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5bba22431efd0a63a04193e3ddac4464b3801e67" translate="yes" xml:space="preserve">
          <source>Input1:</source>
          <target state="translated">Input1:</target>
        </trans-unit>
        <trans-unit id="feac2b2649c90f11d6d855888f274f7a0752b7d9" translate="yes" xml:space="preserve">
          <source>Input2:</source>
          <target state="translated">Input2:</target>
        </trans-unit>
        <trans-unit id="79d70dcb4f9ee8b7d94ed9539586cc73c0d399da" translate="yes" xml:space="preserve">
          <source>Input:</source>
          <target state="translated">Input:</target>
        </trans-unit>
        <trans-unit id="7b180c0fda0377ef1dc31c7cce34da732fc57c9e" translate="yes" xml:space="preserve">
          <source>Input: LongTensor of arbitrary shape containing the indices to extract</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15c3ba090d23cb0ca95577aa6799755edeefe016" translate="yes" xml:space="preserve">
          <source>Input_lengths: Tuple or tensor of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fcadc5a2f2ce33e2ebb11bf51a1dd2322a5a729a" translate="yes" xml:space="preserve">
          <source>Inputs:</source>
          <target state="translated">Inputs:</target>
        </trans-unit>
        <trans-unit id="f969dd5ae0f7325be616e39a51b06a49fc60e816" translate="yes" xml:space="preserve">
          <source>Inputs: input, (h_0, c_0)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1be4f879f896bd3dcbeaf30b825be75ca856fcfb" translate="yes" xml:space="preserve">
          <source>Inputs: input, h_0</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71bf746d2a9f5ff1025d4c04b2cf1fcfbd657691" translate="yes" xml:space="preserve">
          <source>Inputs: input, hidden</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f3d3cd091d557867390efc2557a3f5b19292265" translate="yes" xml:space="preserve">
          <source>Insert a given module before a given index in the list.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50d8055b5be81f5be9d6435d07e2a4ac2451cc98" translate="yes" xml:space="preserve">
          <source>Inserts the key-value pair into the store based on the supplied &lt;code&gt;key&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt;. If &lt;code&gt;key&lt;/code&gt; already exists in the store, it will overwrite the old value with the new supplied &lt;code&gt;value&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52616e80a59172caf19bcc15fac4910844e40524" translate="yes" xml:space="preserve">
          <source>Inspecting Code</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9efecc53883d2ffb75b3f04810dbaac718af41ba" translate="yes" xml:space="preserve">
          <source>InstanceNorm1d</source>
          <target state="translated">InstanceNorm1d</target>
        </trans-unit>
        <trans-unit id="a1ce20a07b568c5cbed5860b3c884130e1c4f948" translate="yes" xml:space="preserve">
          <source>InstanceNorm2d</source>
          <target state="translated">InstanceNorm2d</target>
        </trans-unit>
        <trans-unit id="f8a9e223352c23318d808b3147734411baf32655" translate="yes" xml:space="preserve">
          <source>InstanceNorm3d</source>
          <target state="translated">InstanceNorm3d</target>
        </trans-unit>
        <trans-unit id="c54533fbd7a6f02423b7a436a597e8275c12dc4d" translate="yes" xml:space="preserve">
          <source>Instances of &lt;a href=&quot;#torch.cuda.amp.autocast&quot;&gt;&lt;code&gt;autocast&lt;/code&gt;&lt;/a&gt; serve as context managers or decorators that allow regions of your script to run in mixed precision.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ab3500f668dc459ea6ccd8a8b7456bb53bb532b" translate="yes" xml:space="preserve">
          <source>Instances of this class should never be created manually. They are meant to be instantiated by functions like &lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt;&lt;code&gt;pack_padded_sequence()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5a54ceeb7f75257e5faecf7dac25c3c19332eb52" translate="yes" xml:space="preserve">
          <source>Instancing a pre-trained model will download its weights to a cache directory. This directory can be set using the &lt;code&gt;TORCH_MODEL_ZOO&lt;/code&gt; environment variable. See &lt;a href=&quot;../model_zoo#torch.utils.model_zoo.load_url&quot;&gt;&lt;code&gt;torch.utils.model_zoo.load_url()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4156151a1b281cbc4bcd7e0576424486039cddfe" translate="yes" xml:space="preserve">
          <source>Integer division with addcdiv is no longer supported, and in a future release addcdiv will perform a true division of tensor1 and tensor2. The historic addcdiv behavior can be implemented as (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype) for integer inputs and as (input + value * tensor1 / tensor2) for float inputs. The future addcdiv behavior is just the latter implementation: (input + value * tensor1 / tensor2), for all dtypes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c54682be64b87caa364aa75498c5eaafa212bed" translate="yes" xml:space="preserve">
          <source>Internally invokes &lt;code&gt;unscale_(optimizer)&lt;/code&gt; (unless &lt;a href=&quot;#torch.cuda.amp.GradScaler.unscale_&quot;&gt;&lt;code&gt;unscale_()&lt;/code&gt;&lt;/a&gt; was explicitly called for &lt;code&gt;optimizer&lt;/code&gt; earlier in the iteration). As part of the &lt;a href=&quot;#torch.cuda.amp.GradScaler.unscale_&quot;&gt;&lt;code&gt;unscale_()&lt;/code&gt;&lt;/a&gt;, gradients are checked for infs/NaNs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f650ccc7d0d6be8b0fb7120cf085b2ff78498f0" translate="yes" xml:space="preserve">
          <source>Interpreting Graphs</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab6b952b1b568a27a27bc4ef9eccfa0e4506f5e1" translate="yes" xml:space="preserve">
          <source>Interpreting the output of this function requires familiarity with the memory allocator internals.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e42baee8c04a7fb48aeb23c27863b3e7f99c9a61" translate="yes" xml:space="preserve">
          <source>Inverse short time Fourier Transform.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2d2a372360fc80cd1c1832163878137a219bb55" translate="yes" xml:space="preserve">
          <source>Inverse short time Fourier Transform. This is expected to be the inverse of &lt;a href=&quot;torch.stft#torch.stft&quot;&gt;&lt;code&gt;stft()&lt;/code&gt;&lt;/a&gt;. It has the same parameters (+ additional optional parameter of &lt;code&gt;length&lt;/code&gt;) and it should return the least squares estimation of the original signal. The algorithm will check using the NOLA condition ( nonzero overlap).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4212bbbb75b521f6e55e2e79c4b2dab9598d7c21" translate="yes" xml:space="preserve">
          <source>Invoking &lt;code&gt;trace&lt;/code&gt; with a module&amp;rsquo;s method captures module parameters (which may require gradients) as &lt;strong&gt;constants&lt;/strong&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b1154e9d45e9b9a76119f9cdf7dd156a3cc44ad" translate="yes" xml:space="preserve">
          <source>Irrespective of the original strides, the returned matrices &lt;code&gt;solution&lt;/code&gt; and &lt;code&gt;LU&lt;/code&gt; will be transposed, i.e. with strides like &lt;code&gt;B.contiguous().transpose(-1, -2).stride()&lt;/code&gt; and &lt;code&gt;A.contiguous().transpose(-1, -2).stride()&lt;/code&gt; respectively.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49986ad18b9e3b5d2fddd594cfccd882001d60e0" translate="yes" xml:space="preserve">
          <source>Irrespective of the original strides, the returned matrix &lt;code&gt;U&lt;/code&gt; will be transposed, i.e. with strides &lt;code&gt;U.contiguous().transpose(-2, -1).stride()&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="82269a6fafdcdbb86e718604476ce9b6c8e7d2a7" translate="yes" xml:space="preserve">
          <source>Irrespective of the original strides, the returned matrix &lt;code&gt;V&lt;/code&gt; will be transposed, i.e. with strides &lt;code&gt;V.contiguous().transpose(-1, -2).stride()&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="01899cc116f34c67486ee354a88f582f6796cc36" translate="yes" xml:space="preserve">
          <source>Irrespective of the original strides, the returned tensors will be transposed, i.e. with strides like &lt;code&gt;input.contiguous().transpose(-2, -1).stride()&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2310615072fec2b4a5a875a4e0dd6ffebdaf12ad" translate="yes" xml:space="preserve">
          <source>Is &lt;code&gt;True&lt;/code&gt; if gradients need to be computed for this Tensor, &lt;code&gt;False&lt;/code&gt; otherwise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa669dbfc27ea4c75455d3b2508d4155513b175a" translate="yes" xml:space="preserve">
          <source>Is &lt;code&gt;True&lt;/code&gt; if the Tensor is a meta tensor, &lt;code&gt;False&lt;/code&gt; otherwise. Meta tensors are like normal tensors, but they carry no data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="adff4bd8499f965b85193b88135bc7439c31730a" translate="yes" xml:space="preserve">
          <source>Is &lt;code&gt;True&lt;/code&gt; if the Tensor is quantized, &lt;code&gt;False&lt;/code&gt; otherwise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a0d8439442e2aedd6e797ff733a51980c711c86" translate="yes" xml:space="preserve">
          <source>Is &lt;code&gt;True&lt;/code&gt; if the Tensor is stored on the GPU, &lt;code&gt;False&lt;/code&gt; otherwise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e91f70a847754923183418ed052bcea694565f54" translate="yes" xml:space="preserve">
          <source>Is the &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; where this Tensor is.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4212d08cece9c425b48ec2c1e60e8be1d1e78b6c" translate="yes" xml:space="preserve">
          <source>Is this Tensor with its dimensions reversed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b5b4f414a6eb16cb4afeb48e5665cb815db584b" translate="yes" xml:space="preserve">
          <source>It achieves two things: - makes the output value exactly one-hot (since we add then subtract y_soft value) - makes the gradient equal to y_soft gradient (since we strip all other gradients)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b6bfab9486b081f92c85bb7d32db79b3eceeba3" translate="yes" xml:space="preserve">
          <source>It always prepends a new dimension as the batch dimension.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f8d2c743873b5a3fbbb90a0d2c5cf8d6a8807aa" translate="yes" xml:space="preserve">
          <source>It automatically converts NumPy arrays and Python numerical values into PyTorch Tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49e4e2f8e005da183a111f2acc6be4571ffd2077" translate="yes" xml:space="preserve">
          <source>It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a7efc8c6cf2fe612e155694f2110de8eb7b9450" translate="yes" xml:space="preserve">
          <source>It contains two entries:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e6db53c7cd43b88eb003448e0a5df6411ba7a87" translate="yes" xml:space="preserve">
          <source>It currently accepts &lt;code&gt;ndarray&lt;/code&gt; with dtypes of &lt;code&gt;numpy.float64&lt;/code&gt;, &lt;code&gt;numpy.float32&lt;/code&gt;, &lt;code&gt;numpy.float16&lt;/code&gt;, &lt;code&gt;numpy.complex64&lt;/code&gt;, &lt;code&gt;numpy.complex128&lt;/code&gt;, &lt;code&gt;numpy.int64&lt;/code&gt;, &lt;code&gt;numpy.int32&lt;/code&gt;, &lt;code&gt;numpy.int16&lt;/code&gt;, &lt;code&gt;numpy.int8&lt;/code&gt;, &lt;code&gt;numpy.uint8&lt;/code&gt;, and &lt;code&gt;numpy.bool&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="401f208e4a6ef5ef6ad2bc031ef0d0f97b3ebf14" translate="yes" xml:space="preserve">
          <source>It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability &amp;gt;= 3.0</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="730337db11f0b320af1c601c156987d99a95f2d4" translate="yes" xml:space="preserve">
          <source>It has been proposed in &lt;a href=&quot;http://jmlr.org/papers/v12/duchi11a.html&quot;&gt;Adaptive Subgradient Methods for Online Learning and Stochastic Optimization&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="051e2d9e2a228306c26b3c61549e79ae202aec25" translate="yes" xml:space="preserve">
          <source>It has been proposed in &lt;a href=&quot;https://arxiv.org/abs/1212.5701&quot;&gt;ADADELTA: An Adaptive Learning Rate Method&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="09050a199eb7d53185c92e9f5b5a66dd6952d42f" translate="yes" xml:space="preserve">
          <source>It has been proposed in &lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam: A Method for Stochastic Optimization&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="333ffdb98cee48515840fbcbba1813b5937b4a78" translate="yes" xml:space="preserve">
          <source>It has been proposed in &lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam: A Method for Stochastic Optimization&lt;/a&gt;. The implementation of the L2 penalty follows changes proposed in &lt;a href=&quot;https://arxiv.org/abs/1711.05101&quot;&gt;Decoupled Weight Decay Regularization&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="01b731554c5955677b611c13e6a10743e819c4a9" translate="yes" xml:space="preserve">
          <source>It has been proposed in &lt;a href=&quot;https://arxiv.org/abs/1608.03983&quot;&gt;SGDR: Stochastic Gradient Descent with Warm Restarts&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ff6bf54974d4bef1fd6f31a4e3985ebfbf18b7e" translate="yes" xml:space="preserve">
          <source>It has been proposed in &lt;a href=&quot;https://arxiv.org/abs/1608.03983&quot;&gt;SGDR: Stochastic Gradient Descent with Warm Restarts&lt;/a&gt;. Note that this only implements the cosine annealing part of SGDR, and not the restarts.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="459d6918fdc7170b0fc1dc7a88c04a7b59fb13a0" translate="yes" xml:space="preserve">
          <source>It has been proposed in &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=131098&quot;&gt;Acceleration of stochastic approximation by averaging&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6563cef1ae60ef9f0cfc196eb441bdefa9d3c7aa" translate="yes" xml:space="preserve">
          <source>It has similar signature as &lt;a href=&quot;../tensors#torch.Tensor.to&quot;&gt;&lt;code&gt;torch.Tensor.to()&lt;/code&gt;&lt;/a&gt;, except optional arguments like &lt;code&gt;non_blocking&lt;/code&gt; and &lt;code&gt;copy&lt;/code&gt; should be passed as kwargs, not args, or they will not apply to the index tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b37442b8edf941c9b64329b2dd98dad1125f396" translate="yes" xml:space="preserve">
          <source>It is also possible to annotate types with Python 3 type hints from the &lt;code&gt;typing&lt;/code&gt; module.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86993dc3f5b11fb441bb568843f0d884da0db7f4" translate="yes" xml:space="preserve">
          <source>It is an inverse operation to &lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt;&lt;code&gt;pack_padded_sequence()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50cbae8d4910e6f042bec5701d96446fe97f36c9" translate="yes" xml:space="preserve">
          <source>It is applied to all slices along dim, and will re-scale them so that the elements lie in the range &lt;code&gt;[0, 1]&lt;/code&gt; and sum to 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79324c046d8fb8ad0505c73e6f4ad6bf30dbe4de" translate="yes" xml:space="preserve">
          <source>It is equivalent to &lt;code&gt;`
ComposeTransform([AffineTransform(0., 2.), SigmoidTransform(), AffineTransform(-1., 2.)])
`&lt;/code&gt; However this might not be numerically stable, thus it is recommended to use &lt;code&gt;TanhTransform&lt;/code&gt; instead.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88fdb36159da698a069b2cd0358110c7f72e3f52" translate="yes" xml:space="preserve">
          <source>It is equivalent to the distribution that &lt;a href=&quot;generated/torch.multinomial#torch.multinomial&quot;&gt;&lt;code&gt;torch.multinomial()&lt;/code&gt;&lt;/a&gt; samples from.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b26d4802e99ec71d25e10e9e447cc2865d7a8327" translate="yes" xml:space="preserve">
          <source>It is especially useful in conjunction with &lt;a href=&quot;generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt;&lt;/a&gt;. In such a case, each process can pass a &lt;code&gt;DistributedSampler&lt;/code&gt; instance as a &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; sampler, and load a subset of the original dataset that is exclusive to it.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a9c65d86833900305f35c9e1c934da02bcf964b7" translate="yes" xml:space="preserve">
          <source>It is generally not recommended to return CUDA tensors in multi-process loading because of many subtleties in using CUDA and sharing CUDA tensors in multiprocessing (see &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/multiprocessing.html#multiprocessing-cuda-note&quot;&gt;CUDA in multiprocessing&lt;/a&gt;). Instead, we recommend using &lt;a href=&quot;#memory-pinning&quot;&gt;automatic memory pinning&lt;/a&gt; (i.e., setting &lt;code&gt;pin_memory=True&lt;/code&gt;), which enables fast data transfer to CUDA-enabled GPUs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec0c9b813cf0aa82dc727b897163ee4aef81fc00" translate="yes" xml:space="preserve">
          <source>It is lazily initialized, so you can always import it, and use &lt;a href=&quot;#torch.cuda.is_available&quot;&gt;&lt;code&gt;is_available()&lt;/code&gt;&lt;/a&gt; to determine if your system supports CUDA.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5de635d4ec610b825189efbe466e3cacb9dcacd9" translate="yes" xml:space="preserve">
          <source>It is not possible to directly backpropagate through random samples. However, there are two main methods for creating surrogate functions that can be backpropagated through. These are the score function estimator/likelihood ratio estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly seen as the basis for policy gradient methods in reinforcement learning, and the pathwise derivative estimator is commonly seen in the reparameterization trick in variational autoencoders. Whilst the score function only requires the value of samples</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c6829a0a0e941307482f3fc9539e6effef1fbd8" translate="yes" xml:space="preserve">
          <source>It is recommended to use &lt;a href=&quot;torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;DistributedDataParallel&lt;/code&gt;&lt;/a&gt;, instead of this class, to do multi-GPU training, even if there is only a single node. See: &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-nn-ddp-instead&quot;&gt;Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/ddp.html#ddp&quot;&gt;Distributed Data Parallel&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e66b3186074888a018543f3b0fff094946ea09d0" translate="yes" xml:space="preserve">
          <source>It is useful when running the program under nvprof:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc50a460e1a231d6b7914ab6f80ce6645cd28d1a" translate="yes" xml:space="preserve">
          <source>It is useful when training a classification problem with &lt;code&gt;C&lt;/code&gt; classes. If provided, the optional argument &lt;code&gt;weight&lt;/code&gt; should be a 1D &lt;code&gt;Tensor&lt;/code&gt; assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3dd49ac73db5350741c77e9746cd12b789fc846" translate="yes" xml:space="preserve">
          <source>It must accept a context &lt;code&gt;ctx&lt;/code&gt; as the first argument, followed by as many outputs did &lt;a href=&quot;#torch.autograd.Function.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; return, and it should return as many tensors, as there were inputs to &lt;a href=&quot;#torch.autograd.Function.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt;. Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="252cdfb99fc0852a5d679618d9953abd9399f85f" translate="yes" xml:space="preserve">
          <source>It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5858d55aee32bb1f338ad58158a7863ffaa0e2a" translate="yes" xml:space="preserve">
          <source>It preserves the data structure, e.g., if each sample is a dictionary, it outputs a dictionary with the same set of keys but batched Tensors as values (or lists if the values can not be converted into Tensors). Same for &lt;code&gt;list&lt;/code&gt; s, &lt;code&gt;tuple&lt;/code&gt; s, &lt;code&gt;namedtuple&lt;/code&gt; s, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99a91e5a4c5f62323cf29af98e5d69957ac9245f" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s like QConfig, but for dynamic quantization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88103e98293e4d193ade88c8ec2a2e41a9017ce8" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s possible to trade off recall and precision by adding weights to positive examples. In the case of multi-label classification the loss can be described as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ecdda59aea5ee67d7d854c969ccf7f4f4b4a4c54" translate="yes" xml:space="preserve">
          <source>Item</source>
          <target state="translated">Item</target>
        </trans-unit>
        <trans-unit id="6a10d92f43c8da36e2a39d8ec8446f793702e774" translate="yes" xml:space="preserve">
          <source>Iterable-style datasets</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4dd80eea3f6c51bf5b9c13a8a47609bd55b30e0e" translate="yes" xml:space="preserve">
          <source>Iterables</source>
          <target state="translated">Iterables</target>
        </trans-unit>
        <trans-unit id="069e2ae56a9eacaa6614576bc398f841898b1686" translate="yes" xml:space="preserve">
          <source>Its signature is similar to &lt;a href=&quot;../tensors#torch.Tensor.to&quot;&gt;&lt;code&gt;torch.Tensor.to()&lt;/code&gt;&lt;/a&gt;, but only accepts floating point desired &lt;code&gt;dtype&lt;/code&gt; s. In addition, this method will only cast the floating point parameters and buffers to &lt;code&gt;dtype&lt;/code&gt; (if given). The integral parameters and buffers will be moved &lt;code&gt;device&lt;/code&gt;, if that is given, but with dtypes unchanged. When &lt;code&gt;non_blocking&lt;/code&gt; is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb5d2f4a58038c71155ddcf3cea35dc3c6d34501" translate="yes" xml:space="preserve">
          <source>JIT</source>
          <target state="translated">JIT</target>
        </trans-unit>
        <trans-unit id="ba8c07b6e8f75ea0f3fccd63588af034ee568ee6" translate="yes" xml:space="preserve">
          <source>Jacobian (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt; or nested tuple of Tensors)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a26e63e0dd630bf0aa29409ebb2007f8ed0989a9" translate="yes" xml:space="preserve">
          <source>Javadoc</source>
          <target state="translated">Javadoc</target>
        </trans-unit>
        <trans-unit id="a7ee38bb7be4fc44198cb2685d9601dcf2b9f569" translate="yes" xml:space="preserve">
          <source>K</source>
          <target state="translated">K</target>
        </trans-unit>
        <trans-unit id="d1dd1d02265a63fb16a7c02869b20a2ff09b23ec" translate="yes" xml:space="preserve">
          <source>K \geq 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d815c4b766be65370cc630500fbc7534d96810e3" translate="yes" xml:space="preserve">
          <source>KL(p \| q)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3789bcc3ea87de653c85546427835735d21460e6" translate="yes" xml:space="preserve">
          <source>KL(p \| q) = \int p(x) \log\frac {p(x)} {q(x)} \,dx</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0d38167f0e193a1ac968e4ff065116f3dd7d3e2" translate="yes" xml:space="preserve">
          <source>KLDivLoss</source>
          <target state="translated">KLDivLoss</target>
        </trans-unit>
        <trans-unit id="2dff5751295255d47b7ebb571ae0d72fa938cb72" translate="yes" xml:space="preserve">
          <source>Keep in mind that only a limited number of optimizers support sparse gradients: currently it&amp;rsquo;s &lt;code&gt;optim.SGD&lt;/code&gt; (&lt;code&gt;CUDA&lt;/code&gt; and &lt;code&gt;CPU&lt;/code&gt;), &lt;code&gt;optim.SparseAdam&lt;/code&gt; (&lt;code&gt;CUDA&lt;/code&gt; and &lt;code&gt;CPU&lt;/code&gt;) and &lt;code&gt;optim.Adagrad&lt;/code&gt; (&lt;code&gt;CPU&lt;/code&gt;)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="61df1ec904a19beceda75adc97e889c2fd5cb69f" translate="yes" xml:space="preserve">
          <source>Keypoint R-CNN</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4dbced9709f5236c9b69e641ffff848fd0986a34" translate="yes" xml:space="preserve">
          <source>Keypoint R-CNN ResNet-50 FPN</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2815e1786777eee9275fa2f0b31c8b705f9edc6" translate="yes" xml:space="preserve">
          <source>Keypoint R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db32172db3cd21ae36313d815fba0dd39bd7127a" translate="yes" xml:space="preserve">
          <source>Keyword Arguments</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c524ac8953f745b9e5b79c69e1b4d39891c5ba1" translate="yes" xml:space="preserve">
          <source>Keyword arguments &lt;code&gt;min_value&lt;/code&gt; and &lt;code&gt;max_value&lt;/code&gt; have been deprecated in favor of &lt;code&gt;min_val&lt;/code&gt; and &lt;code&gt;max_val&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e48bd0570e2bf01526ed152277d5e60977339c96" translate="yes" xml:space="preserve">
          <source>Kicks off the distributed backward pass using the provided roots. This currently implements the &lt;a href=&quot;rpc/distributed_autograd#fast-mode-algorithm&quot;&gt;FAST mode algorithm&lt;/a&gt; which assumes all RPC messages sent in the same distributed autograd context across workers would be part of the autograd graph during the backward pass.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41bb1570ae7ccc5980e78c5f1c45cc0aed0bdbed" translate="yes" xml:space="preserve">
          <source>Kinetics 1-crop accuracies for clip length 16 (16x112x112)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5a5fafee83492d8b176cbbdd5bb0860a6fdbf28" translate="yes" xml:space="preserve">
          <source>Known limitations:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d160e0986aca4714714a16f29ec605af90be704d" translate="yes" xml:space="preserve">
          <source>L</source>
          <target state="translated">L</target>
        </trans-unit>
        <trans-unit id="80f4812ca21133a053b1e37ce0b7c69d130f72e6" translate="yes" xml:space="preserve">
          <source>L = \prod_d \left\lfloor\frac{\text{output\_size}[d] + 2 \times \text{padding}[d] % - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c67a32aa874c2b17f9de746130e43c1407789f98" translate="yes" xml:space="preserve">
          <source>L = \prod_d \left\lfloor\frac{\text{spatial\_size}[d] + 2 \times \text{padding}[d] % - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c775d48eb4f58382ff520cfe8c90ab7190041ada" translate="yes" xml:space="preserve">
          <source>L = \{l_1,\dots,l_N\}^\top</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="77bc26a3089aa1a1138008701f4240f1d24f7ec4" translate="yes" xml:space="preserve">
          <source>L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="38b3e9960c6ed6d7fea333d02fe4a806a8e19d30" translate="yes" xml:space="preserve">
          <source>L1Loss</source>
          <target state="translated">L1Loss</target>
        </trans-unit>
        <trans-unit id="eea818c7023faaf096d89f5dc66b905458038bf3" translate="yes" xml:space="preserve">
          <source>L1Unstructured</source>
          <target state="translated">L1Unstructured</target>
        </trans-unit>
        <trans-unit id="ba6b42d0904cce73f214db608d06bfd30f824246" translate="yes" xml:space="preserve">
          <source>L=C \times \text{upscale\_factor}^2</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e08d4eaec3e70d3b11d0930088b84b2ff38e358f" translate="yes" xml:space="preserve">
          <source>LPPool1d</source>
          <target state="translated">LPPool1d</target>
        </trans-unit>
        <trans-unit id="2ea3c697cd5d5c5710f8ec9f1853900569f0680c" translate="yes" xml:space="preserve">
          <source>LPPool2d</source>
          <target state="translated">LPPool2d</target>
        </trans-unit>
        <trans-unit id="23757b375d1fdee5bda46fff218547176aed63b2" translate="yes" xml:space="preserve">
          <source>LSTM</source>
          <target state="translated">LSTM</target>
        </trans-unit>
        <trans-unit id="2f25bc39b85fe095b000d209878e94802865e367" translate="yes" xml:space="preserve">
          <source>LSTMCell</source>
          <target state="translated">LSTMCell</target>
        </trans-unit>
        <trans-unit id="ecfecac3ee4f2cb5286bbb038509a2e5de7c4c02" translate="yes" xml:space="preserve">
          <source>LU factorization with &lt;code&gt;pivot&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt; is not available for CPU, and attempting to do so will throw an error. However, LU factorization with &lt;code&gt;pivot&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt; is available for CUDA.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0deac72fe13e2e0b87977edeca8e509c0bcfa09e" translate="yes" xml:space="preserve">
          <source>L_p</source>
          <target state="translated">L_p</target>
        </trans-unit>
        <trans-unit id="06c5049b8951b65d33ef4a2ef830eb1114e69b8d" translate="yes" xml:space="preserve">
          <source>L_{out} = (L_{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{dilation} \times (\text{kernel\_size} - 1) + \text{output\_padding} + 1</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b805a9450351de83336b48fb7172bcc72a263dc2" translate="yes" xml:space="preserve">
          <source>L_{out} = \left\lfloor \frac{L_{in} + 2 \times \text{padding} - \text{dilation} \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ceac6a4482e16b31e7e38fe1379ffb2a0580a4c8" translate="yes" xml:space="preserve">
          <source>L_{out} = \left\lfloor \frac{L_{in} + 2 \times \text{padding} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="87439fc8e93caa8a60f8765a05bec347953ce5dc" translate="yes" xml:space="preserve">
          <source>L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation} \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c7d288e1c47c63e7fa88234abfbcbb629553bc4" translate="yes" xml:space="preserve">
          <source>L_{out} = \left\lfloor\frac{L_{in} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="82415f0bf390885258b6da20230789fced78fab6" translate="yes" xml:space="preserve">
          <source>Labels passed as inputs to this module should be sorted according to their frequency. This means that the most frequent label should be represented by the index &lt;code&gt;0&lt;/code&gt;, and the least frequent label should be represented by the index &lt;code&gt;n_classes - 1&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e52f467c6824dfb11f12944d49910287d4e8acf9" translate="yes" xml:space="preserve">
          <source>Language Bindings</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f7c1b0b04225275caa328bda30504fd6b8c37f10" translate="yes" xml:space="preserve">
          <source>Laplace</source>
          <target state="translated">Laplace</target>
        </trans-unit>
        <trans-unit id="1a13e309cb6089d8795a4086c303a44f1aa93eb0" translate="yes" xml:space="preserve">
          <source>Last chunk will be smaller if the tensor size along the given dimension &lt;code&gt;dim&lt;/code&gt; is not divisible by &lt;code&gt;chunks&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ea4ab66267252a04db528f49cff0f01eeac15249" translate="yes" xml:space="preserve">
          <source>Later, saved tensors can be accessed through the &lt;code&gt;saved_tensors&lt;/code&gt; attribute. Before returning them to the user, a check is made to ensure they weren&amp;rsquo;t used in any in-place operation that modified their content.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c11e8b0852264fedca1944abe570a02d58008f36" translate="yes" xml:space="preserve">
          <source>Launch utility</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e7347f29c1c343ae08ddd8deb4a8c8c703257a6" translate="yes" xml:space="preserve">
          <source>LayerNorm</source>
          <target state="translated">LayerNorm</target>
        </trans-unit>
        <trans-unit id="ae8648006d573eefab4fc3a540fa844d3c24c709" translate="yes" xml:space="preserve">
          <source>Leaky Relu</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3272736528de0f5cd99ceafd394209cd4f0d9d4" translate="yes" xml:space="preserve">
          <source>LeakyRELU</source>
          <target state="translated">LeakyRELU</target>
        </trans-unit>
        <trans-unit id="fedab85703859b90b0ad1f8cd46b9b7ea93c8647" translate="yes" xml:space="preserve">
          <source>LeakyReLU</source>
          <target state="translated">LeakyReLU</target>
        </trans-unit>
        <trans-unit id="08736899d926026ffd2bed5a1ba2420f2fe34a6d" translate="yes" xml:space="preserve">
          <source>Learning rate scheduling should be applied after optimizer&amp;rsquo;s update; e.g., you should write your code this way:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7422cede6b29fe51f9334794bb938ed3c22c4e55" translate="yes" xml:space="preserve">
          <source>Least squares estimation of the original signal of size (&amp;hellip;, signal_length)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81a9dc728fc2ca28b37b2c8b862e702ce61a20d2" translate="yes" xml:space="preserve">
          <source>Legacy Constructors</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e6252983d84535ed576c200c0613bfb5c53e74f6" translate="yes" xml:space="preserve">
          <source>Let I_0 be the zeroth order modified Bessel function of the first kind (see &lt;a href=&quot;torch.i0#torch.i0&quot;&gt;&lt;code&gt;torch.i0()&lt;/code&gt;&lt;/a&gt;) and &lt;code&gt;N = L - 1&lt;/code&gt; if &lt;code&gt;periodic&lt;/code&gt; is False and &lt;code&gt;L&lt;/code&gt; if &lt;code&gt;periodic&lt;/code&gt; is True, where &lt;code&gt;L&lt;/code&gt; is the &lt;code&gt;window_length&lt;/code&gt;. This function computes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0f4fa4a2a001fbe73eaa9b243479e76bce8ec6b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s see how &lt;code&gt;match&lt;/code&gt; and &lt;code&gt;unify&lt;/code&gt; are used in name inference in the case of adding two one-dim tensors with no broadcasting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="27c968e6692b41bf6e1a241b870dc41ff1a51b17" translate="yes" xml:space="preserve">
          <source>Libraries</source>
          <target state="translated">Libraries</target>
        </trans-unit>
        <trans-unit id="538c09161b8497f998404cafc34964ed3a445575" translate="yes" xml:space="preserve">
          <source>Licensed under the 3-clause BSD License.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f5f45ae757c1886e79dc62c9c379591270427af" translate="yes" xml:space="preserve">
          <source>Like &lt;em&gt;output&lt;/em&gt;, the layers can be separated using &lt;code&gt;h_n.view(num_layers, num_directions, batch, hidden_size)&lt;/code&gt; and similarly for &lt;em&gt;c_n&lt;/em&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db10501c95280d11842c24308a3aa239dbe03702" translate="yes" xml:space="preserve">
          <source>Like &lt;em&gt;output&lt;/em&gt;, the layers can be separated using &lt;code&gt;h_n.view(num_layers, num_directions, batch, hidden_size)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e802831cc6f03739c21444458e82e5daba72cf20" translate="yes" xml:space="preserve">
          <source>Like with &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, the output length must be given in order to recover an even length output:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7c04c64ed3f2a9374590c76c50d3b7f1b18e3da" translate="yes" xml:space="preserve">
          <source>Limitations</source>
          <target state="translated">Limitations</target>
        </trans-unit>
        <trans-unit id="af502f2b37eea07ed9083c7daf40f34553f6fccd" translate="yes" xml:space="preserve">
          <source>Linear</source>
          <target state="translated">Linear</target>
        </trans-unit>
        <trans-unit id="52f5fe1a2af4bafc542342604407d9368527a7e4" translate="yes" xml:space="preserve">
          <source>Linear / Identity</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="462229a1c5dbd1fa15040281e0145c1fe6a072c6" translate="yes" xml:space="preserve">
          <source>Linear Layers</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0948f7097c1b1ef0dfbf5426722f24d5359dce5" translate="yes" xml:space="preserve">
          <source>Linear functions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="034874347ef77609e931030e8022aaf9f02a5da3" translate="yes" xml:space="preserve">
          <source>LinearReLU</source>
          <target state="translated">LinearReLU</target>
        </trans-unit>
        <trans-unit id="2654ae5325afbc554a0ee441059fbb41cbaf1dbb" translate="yes" xml:space="preserve">
          <source>List Construction</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92ca3314a6145558197b21fe52556246426b2528" translate="yes" xml:space="preserve">
          <source>List all entrypoints available in &lt;code&gt;github&lt;/code&gt; hubconf.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7ba7dcf662374aafe652b03d75cb8f6024519d9" translate="yes" xml:space="preserve">
          <source>Literals</source>
          <target state="translated">Literals</target>
        </trans-unit>
        <trans-unit id="bd26911422165279d6cf54be6a2c3601b7d15a43" translate="yes" xml:space="preserve">
          <source>LnStructured</source>
          <target state="translated">LnStructured</target>
        </trans-unit>
        <trans-unit id="b0a48cb5954a9d7f0650f13e3620734ccd2bae05" translate="yes" xml:space="preserve">
          <source>Load a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; previously saved with &lt;a href=&quot;generated/torch.jit.save#torch.jit.save&quot;&gt;&lt;code&gt;torch.jit.save&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="895bd8b16804ae34cfb0e2dc72b19fd2ce1623cf" translate="yes" xml:space="preserve">
          <source>Load a &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; previously saved with &lt;a href=&quot;torch.jit.save#torch.jit.save&quot;&gt;&lt;code&gt;torch.jit.save&lt;/code&gt;&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6cb35151312e0bc2bc2c91990968d299f6b351f9" translate="yes" xml:space="preserve">
          <source>Load a model from a github repo or a local directory.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bbfedf2587224caa5003ffee100938c3cce6187d" translate="yes" xml:space="preserve">
          <source>Loading Batched and Non-Batched Data</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08eab7f7a8aa4c317badff7188cc2c0bb5835fc2" translate="yes" xml:space="preserve">
          <source>Loading models from Hub</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1df3601943ceea2bf72fecc57231a5842fe092bb" translate="yes" xml:space="preserve">
          <source>Loads a PyTorch C++ extension just-in-time (JIT) from string sources.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0c28fcdf2966da4afbb699048a05d25a1e72589" translate="yes" xml:space="preserve">
          <source>Loads a PyTorch C++ extension just-in-time (JIT).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8172851e5d518987e24368975b30a3f3621f89a" translate="yes" xml:space="preserve">
          <source>Loads an object saved with &lt;a href=&quot;generated/torch.save#torch.save&quot;&gt;&lt;code&gt;torch.save()&lt;/code&gt;&lt;/a&gt; from a file.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="abae7d5b43752c21e187437593ecfd6e103a310d" translate="yes" xml:space="preserve">
          <source>Loads an object saved with &lt;a href=&quot;torch.save#torch.save&quot;&gt;&lt;code&gt;torch.save()&lt;/code&gt;&lt;/a&gt; from a file.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2285e79b168630a8a2e0f5ae0c64959ab5ec6e8" translate="yes" xml:space="preserve">
          <source>Loads the Torch serialized object at the given URL.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e303a8f00a29e8f80582a95270441302240025d" translate="yes" xml:space="preserve">
          <source>Loads the optimizer state.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7aaab562b127aeeb78317020a69983cd185a2928" translate="yes" xml:space="preserve">
          <source>Loads the scaler state. If this instance is disabled, &lt;a href=&quot;#torch.cuda.amp.GradScaler.load_state_dict&quot;&gt;&lt;code&gt;load_state_dict()&lt;/code&gt;&lt;/a&gt; is a no-op.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1cfdfc410207837272e43ead5fb1f22980e56511" translate="yes" xml:space="preserve">
          <source>Loads the schedulers state.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bca03b832694245e7d76094e54b41aee61f32b8c" translate="yes" xml:space="preserve">
          <source>Local file system, &lt;code&gt;init_method=&quot;file:///d:/tmp/some_file&quot;&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e2a7f51460fa624f29eb4d9345e24061c3314f3" translate="yes" xml:space="preserve">
          <source>LocalResponseNorm</source>
          <target state="translated">LocalResponseNorm</target>
        </trans-unit>
        <trans-unit id="0ca5c78f2cf1102c66c5583023caa00fc87c10db" translate="yes" xml:space="preserve">
          <source>Locally disabling gradient computation</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4029ee70e71d54ca0962e31cd594321368f86629" translate="yes" xml:space="preserve">
          <source>LogNormal</source>
          <target state="translated">LogNormal</target>
        </trans-unit>
        <trans-unit id="5f6a43bda80ee771a5be7b5a9a5b959803eff8b6" translate="yes" xml:space="preserve">
          <source>LogSigmoid</source>
          <target state="translated">LogSigmoid</target>
        </trans-unit>
        <trans-unit id="9fcfd80b59ec37c0d7c822778e2ede78ad5ef865" translate="yes" xml:space="preserve">
          <source>LogSoftmax</source>
          <target state="translated">LogSoftmax</target>
        </trans-unit>
        <trans-unit id="2be6f5f3dfe34ae207c15144d936b394d3342629" translate="yes" xml:space="preserve">
          <source>Log_probs: Tensor of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a29b9b1054fef950824734dd96565e57e322c96" translate="yes" xml:space="preserve">
          <source>Logarithm of the sum of exponentiations of the inputs in base-2.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="273c857a62012f5708232560c688d9c8f7c28ba5" translate="yes" xml:space="preserve">
          <source>Logarithm of the sum of exponentiations of the inputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65eac6118ebdd55aa38b41f70efdf532e567ffd8" translate="yes" xml:space="preserve">
          <source>Logical Operators</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca7f4bd6623919e36ffd033c9c80f0e4b3c64f49" translate="yes" xml:space="preserve">
          <source>LogitRelaxedBernoulli</source>
          <target state="translated">LogitRelaxedBernoulli</target>
        </trans-unit>
        <trans-unit id="4e1da61dedde155219f888017b4095799b14e811" translate="yes" xml:space="preserve">
          <source>LongTensor or tuple of LongTensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79894b78077b352e7913286a38c3c0b109363b18" translate="yes" xml:space="preserve">
          <source>LongTensor that has one more dimension with 1 values at the index of last dimension indicated by the input, and 0 everywhere else.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5889a83849e7452eb90474433b00a93e8ed08a2f" translate="yes" xml:space="preserve">
          <source>Look at the paper: &lt;a href=&quot;https://arxiv.org/abs/1609.05158&quot;&gt;Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network&lt;/a&gt; by Shi et. al (2016) for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99663eada4ef450e2ffc9048b04317b33e9db27b" translate="yes" xml:space="preserve">
          <source>Lookup returns the most specific (type,type) match ordered by subclass. If the match is ambiguous, a &lt;code&gt;RuntimeWarning&lt;/code&gt; is raised. For example to resolve the ambiguous situation:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0224525e6e81a08ff829f4d53c8aff4b15f7024" translate="yes" xml:space="preserve">
          <source>Loss Functions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf3496193ce471c7c1c432d246ad87b6a1823e9c" translate="yes" xml:space="preserve">
          <source>Loss functions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85b9d89a2ed9217bacdf503e6a3c96f95e5750aa" translate="yes" xml:space="preserve">
          <source>Lots of information can be logged for one experiment. To avoid cluttering the UI and have better result clustering, we can group plots by naming them hierarchically. For example, &amp;ldquo;Loss/train&amp;rdquo; and &amp;ldquo;Loss/test&amp;rdquo; will be grouped together, while &amp;ldquo;Accuracy/train&amp;rdquo; and &amp;ldquo;Accuracy/test&amp;rdquo; will be grouped separately in the TensorBoard interface.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a851f6b05f669f852f4c7d2a4117b59cc9a5c4f0" translate="yes" xml:space="preserve">
          <source>LowRankMultivariateNormal</source>
          <target state="translated">LowRankMultivariateNormal</target>
        </trans-unit>
        <trans-unit id="c63ae6dd4fc9f9dda66970e827d13f7c73fe841c" translate="yes" xml:space="preserve">
          <source>M</source>
          <target state="translated">M</target>
        </trans-unit>
        <trans-unit id="a452071172e433a963ff286fe987870a980f2d28" translate="yes" xml:space="preserve">
          <source>M (Tensor, optional): the input tensor&amp;rsquo;s mean of size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b186e235f284107df6b4dbe6060d2b6a5d9f1e5" translate="yes" xml:space="preserve">
          <source>MAX</source>
          <target state="translated">MAX</target>
        </trans-unit>
        <trans-unit id="0d96af233d36586b84359d8b5bd0b417787982e1" translate="yes" xml:space="preserve">
          <source>MC3 Network definition</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04e66352aa8f9c4c5f26b71bf380973ada994760" translate="yes" xml:space="preserve">
          <source>MIN</source>
          <target state="translated">MIN</target>
        </trans-unit>
        <trans-unit id="351682e4dc04204d75a824bcbf8f8aa6acc15e71" translate="yes" xml:space="preserve">
          <source>MIXED MODE OF (1) and (2):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="59ce6264cd26f13684de464b9aeb65d1d414e559" translate="yes" xml:space="preserve">
          <source>MNASNet</source>
          <target state="translated">MNASNet</target>
        </trans-unit>
        <trans-unit id="533bea2ee6c045fe09ac0124525af723616f331a" translate="yes" xml:space="preserve">
          <source>MNASNet 1.0</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a7425f07104beddfc0bce0274310b0a2e2ea359" translate="yes" xml:space="preserve">
          <source>MNASNet with depth multiplier of 0.5 from &lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo;MnasNet: Platform-Aware Neural Architecture Search for Mobile&amp;rdquo;&lt;/a&gt;. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="98e07c23513158f84ea34588dbe9369dbc7ac20e" translate="yes" xml:space="preserve">
          <source>MNASNet with depth multiplier of 0.75 from &lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo;MnasNet: Platform-Aware Neural Architecture Search for Mobile&amp;rdquo;&lt;/a&gt;. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71754b43c6a0483cdc892a42e6511e7a6757f6a2" translate="yes" xml:space="preserve">
          <source>MNASNet with depth multiplier of 1.0 from &lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo;MnasNet: Platform-Aware Neural Architecture Search for Mobile&amp;rdquo;&lt;/a&gt;. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f35e9dd2bd116147e38d35cef6e82d4b56bfc05" translate="yes" xml:space="preserve">
          <source>MNASNet with depth multiplier of 1.3 from &lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo;MnasNet: Platform-Aware Neural Architecture Search for Mobile&amp;rdquo;&lt;/a&gt;. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="53880fd71eeaa5e41a5d925ca0243b4befcdb092" translate="yes" xml:space="preserve">
          <source>MSELoss</source>
          <target state="translated">MSELoss</target>
        </trans-unit>
        <trans-unit id="76c7044fa78115fa06a42c399d17d51a9203e830" translate="yes" xml:space="preserve">
          <source>Make a blocking RPC call to run function &lt;code&gt;func&lt;/code&gt; on worker &lt;code&gt;to&lt;/code&gt;. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d7820212a00c819e77e602a3ef50edfe2f50d52" translate="yes" xml:space="preserve">
          <source>Make a non-blocking RPC call to run function &lt;code&gt;func&lt;/code&gt; on worker &lt;code&gt;to&lt;/code&gt;. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe. This method will immediately return a &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; that can be awaited on.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f221c8d616e55bc62ec21b79af5d8282f107f2d3" translate="yes" xml:space="preserve">
          <source>Make a remote call to run &lt;code&gt;func&lt;/code&gt; on worker &lt;code&gt;to&lt;/code&gt; and return an &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;&lt;code&gt;RRef&lt;/code&gt;&lt;/a&gt; to the result value immediately. Worker &lt;code&gt;to&lt;/code&gt; will be the owner of the returned &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;&lt;code&gt;RRef&lt;/code&gt;&lt;/a&gt;, and the worker calling &lt;code&gt;remote&lt;/code&gt; is a user. The owner manages the global reference count of its &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;&lt;code&gt;RRef&lt;/code&gt;&lt;/a&gt;, and the owner &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;&lt;code&gt;RRef&lt;/code&gt;&lt;/a&gt; is only destructed when globally there are no living references to it.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f909b867fd9b369b49e7c33ae5b4db26cd626b58" translate="yes" xml:space="preserve">
          <source>Make sure that &lt;code&gt;MASTER_ADDR&lt;/code&gt; and &lt;code&gt;MASTER_PORT&lt;/code&gt; are set properly on both workers. Refer to &lt;a href=&quot;distributed#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; API for more details. For example,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e23b19686007a5b5511fdca9b9d258395bc61451" translate="yes" xml:space="preserve">
          <source>Make sure that any custom &lt;code&gt;collate_fn&lt;/code&gt;, &lt;code&gt;worker_init_fn&lt;/code&gt; or &lt;code&gt;dataset&lt;/code&gt; code is declared as top level definitions, outside of the &lt;code&gt;__main__&lt;/code&gt; check. This ensures that they are available in worker processes. (this is needed since functions are pickled as references only, not &lt;code&gt;bytecode&lt;/code&gt;.)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3140663730440e28711adcbaaf1e5be8f31a832b" translate="yes" xml:space="preserve">
          <source>Makes a &lt;code&gt;cls&lt;/code&gt; instance with the same data pointer as &lt;code&gt;self&lt;/code&gt;. Changes in the output mirror changes in &lt;code&gt;self&lt;/code&gt;, and the output stays attached to the autograd graph. &lt;code&gt;cls&lt;/code&gt; must be a subclass of &lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e810a7ac67a7f7afc66a0a46bf613aab777bf84d" translate="yes" xml:space="preserve">
          <source>Makes all future work submitted to the given stream wait for this event.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8bb52cb02033ef90d627dbaa4c9db7b7d77cc8ad" translate="yes" xml:space="preserve">
          <source>Makes all future work submitted to the stream wait for an event.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49e2977180d59fe62d760df21fb871ec41183f71" translate="yes" xml:space="preserve">
          <source>Manipulating dimensions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1b2e08cb686bf5745b4dbb6986d0dc5c2cab21bf" translate="yes" xml:space="preserve">
          <source>Manual gradient layouts</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6d825496aa8e7368b658938db8b142e79e1f39ee" translate="yes" xml:space="preserve">
          <source>Many PyTorch functions, which return a view of a tensor, are internally implemented with this function. Those functions, like &lt;a href=&quot;../tensors#torch.Tensor.expand&quot;&gt;&lt;code&gt;torch.Tensor.expand()&lt;/code&gt;&lt;/a&gt;, are easier to read and are therefore more advisable to use.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="181be8aee3d1857e33b8c6d149cff923185af75a" translate="yes" xml:space="preserve">
          <source>Many models use a sigmoid layer right before the binary cross entropy layer. In this case, combine the two layers using &lt;a href=&quot;nn.functional#torch.nn.functional.binary_cross_entropy_with_logits&quot;&gt;&lt;code&gt;torch.nn.functional.binary_cross_entropy_with_logits()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/torch.nn.bcewithlogitsloss#torch.nn.BCEWithLogitsLoss&quot;&gt;&lt;code&gt;torch.nn.BCEWithLogitsLoss&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;binary_cross_entropy_with_logits&lt;/code&gt; and &lt;code&gt;BCEWithLogits&lt;/code&gt; are safe to autocast.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="877edbec3bdc8c92a89c77b353c1bffa8a6d3f53" translate="yes" xml:space="preserve">
          <source>Many of Python&amp;rsquo;s &lt;a href=&quot;https://docs.python.org/3/library/functions.html&quot;&gt;built-in functions&lt;/a&gt; are supported in TorchScript. The &lt;a href=&quot;https://docs.python.org/3/library/math.html#module-math&quot;&gt;&lt;code&gt;math&lt;/code&gt;&lt;/a&gt; module is also supported (see &lt;a href=&quot;jit_builtin_functions#math-module&quot;&gt;math Module&lt;/a&gt; for details), but no other Python modules (built-in or third party) are supported.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a0216a95943fa820f7459f0a77368a7416b7939" translate="yes" xml:space="preserve">
          <source>Map-style datasets</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b9cac864fb0630d3e0375c972324ae14f0a7cbf" translate="yes" xml:space="preserve">
          <source>MarginRankingLoss</source>
          <target state="translated">MarginRankingLoss</target>
        </trans-unit>
        <trans-unit id="1373a08f797bf15732fbadb687eff12b2e03c08f" translate="yes" xml:space="preserve">
          <source>Marks given tensors as modified in an in-place operation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64d7f425f84dfa95b2a3d1450fcc8fbbce25eb01" translate="yes" xml:space="preserve">
          <source>Marks outputs as non-differentiable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b3b6161e2be4020ea8045bf777a1b7a9a9ef9d7" translate="yes" xml:space="preserve">
          <source>Mask R-CNN</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de79ab812f56430c47ab7094aa9e5f95c476c9d5" translate="yes" xml:space="preserve">
          <source>Mask R-CNN ResNet-50 FPN</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e283616925f71932f219908f30d53b3df610dd54" translate="yes" xml:space="preserve">
          <source>Mask R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="370101c55e76bb32e62a76c0f757b67a5a083820" translate="yes" xml:space="preserve">
          <source>Math operations</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f266f996d2555bfc5ac129ce470aa82f278b2388" translate="yes" xml:space="preserve">
          <source>Matrix multiplication ops: &lt;a href=&quot;name_inference#contracts-away-dims-doc&quot;&gt;Contracts away dims&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a75fc5d78987f3dab3a65065c4b1d4f0692e098" translate="yes" xml:space="preserve">
          <source>Matrix product of two tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eccdf32fe79dd9c576c8118d98c630238fca6bb7" translate="yes" xml:space="preserve">
          <source>MaxPool1d</source>
          <target state="translated">MaxPool1d</target>
        </trans-unit>
        <trans-unit id="3b3d541959fe8a8dd18b510e5a3f48b2498a53b4" translate="yes" xml:space="preserve">
          <source>MaxPool2d</source>
          <target state="translated">MaxPool2d</target>
        </trans-unit>
        <trans-unit id="98908ed042b1a9862ea5e9d4da6ed687e958f368" translate="yes" xml:space="preserve">
          <source>MaxPool3d</source>
          <target state="translated">MaxPool3d</target>
        </trans-unit>
        <trans-unit id="4a5419bdd0aea2636e49e9d7ca08451ce7530629" translate="yes" xml:space="preserve">
          <source>MaxUnpool1d</source>
          <target state="translated">MaxUnpool1d</target>
        </trans-unit>
        <trans-unit id="dde49d17029a94b9195f0444462e31e369394942" translate="yes" xml:space="preserve">
          <source>MaxUnpool2d</source>
          <target state="translated">MaxUnpool2d</target>
        </trans-unit>
        <trans-unit id="cbb9a5d6b569b7fb0a0f22c9d9d9d31e12466046" translate="yes" xml:space="preserve">
          <source>MaxUnpool3d</source>
          <target state="translated">MaxUnpool3d</target>
        </trans-unit>
        <trans-unit id="fe4de202eb4956062e7cfb72ed4b684ffbb5bd30" translate="yes" xml:space="preserve">
          <source>Measures the element-wise mean squared error.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="09b86d975dd6b1f5a73e1c3517013bb23d540761" translate="yes" xml:space="preserve">
          <source>Measures the loss given an input tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d01e0fee07072aec458beb4c034b607f10f63a9" translate="yes" xml:space="preserve">
          <source>Members:</source>
          <target state="translated">Members:</target>
        </trans-unit>
        <trans-unit id="af23da357d652a02423313397c1adbd940726581" translate="yes" xml:space="preserve">
          <source>Memory Pinning</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c5536b98ab016fe61807564b4a34c80211a7613" translate="yes" xml:space="preserve">
          <source>Memory management</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c5f0da2191ca27795b8631522c1ca879f1716c9" translate="yes" xml:space="preserve">
          <source>Method Calls</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fcf32dbe7b3f386bd441fb137d4760dd5708ad54" translate="yes" xml:space="preserve">
          <source>Method to compute the entropy using Bregman divergence of the log normalizer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b3dfdaa160857d292036f521c9ea3c58c5d52fbd" translate="yes" xml:space="preserve">
          <source>Methods such as &lt;code&gt;var.backward(), var.detach(), var.register_hook()&lt;/code&gt; now work on tensors with the same method names.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b69c14783d5324daa62920c742f93c096f643f4f" translate="yes" xml:space="preserve">
          <source>Methods which mutate a tensor are marked with an underscore suffix. For example, &lt;code&gt;torch.FloatTensor.abs_()&lt;/code&gt; computes the absolute value in-place and returns the modified tensor, while &lt;code&gt;torch.FloatTensor.abs()&lt;/code&gt; computes the result in a new tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b7c339ea87ccc784ad1deadd85bf646248e9d73" translate="yes" xml:space="preserve">
          <source>Methods which take a device will generally accept a (properly formatted) string or (legacy) integer device ordinal, i.e. the following are all equivalent:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="adfdb53a472af0c4b913a760ea8b9484fe72338d" translate="yes" xml:space="preserve">
          <source>Metric type:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e067f6663a0605e92a7219ee53ccfb43fa76eaf3" translate="yes" xml:space="preserve">
          <source>Migrating to PyTorch 1.2 Recursive Scripting API</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff26bc094b922f3c06f27a4d1e69dcd28d4c0e3c" translate="yes" xml:space="preserve">
          <source>Mixing Tracing and Scripting</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6a693905dba81b2e3c99a490b1ec25a2a6253235" translate="yes" xml:space="preserve">
          <source>MixtureSameFamily</source>
          <target state="translated">MixtureSameFamily</target>
        </trans-unit>
        <trans-unit id="9ff690f5176e9e122a08292f5ff8b357a6c354e0" translate="yes" xml:space="preserve">
          <source>MobileNet V2</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c901047dcc843db7018568e9b72b5dbf5a54540d" translate="yes" xml:space="preserve">
          <source>MobileNet v2</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8ff02892916ff59f7fbd4e617fccd01f6bca576" translate="yes" xml:space="preserve">
          <source>Module</source>
          <target state="translated">Module</target>
        </trans-unit>
        <trans-unit id="9c3f2aba6913de8da00faed8e2df2428bb1257b7" translate="yes" xml:space="preserve">
          <source>Module Attributes</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3cb22c5e4eabc0e7d3ccf2b60e66f53421b63149" translate="yes" xml:space="preserve">
          <source>Module Index</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b2ce983642fdce8da88af1aafe61bc6a6ee4bfb" translate="yes" xml:space="preserve">
          <source>ModuleDict</source>
          <target state="translated">ModuleDict</target>
        </trans-unit>
        <trans-unit id="e12ef0c5e95f31ebc597e0b6fd821d6b604f3f95" translate="yes" xml:space="preserve">
          <source>ModuleList</source>
          <target state="translated">ModuleList</target>
        </trans-unit>
        <trans-unit id="04e9462c0ff02bb9032b92abd45881a3c7e15fb7" translate="yes" xml:space="preserve">
          <source>Modules</source>
          <target state="translated">Modules</target>
        </trans-unit>
        <trans-unit id="9dbae4788d0b2dea68c6e66c9e0ff2aed52f652e" translate="yes" xml:space="preserve">
          <source>Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a649c7ebd14c451b9eadae1b57b3c6722590226" translate="yes" xml:space="preserve">
          <source>More Information about RPC Autograd</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9cb54dedb558ff4177e32b924117d76eb6014c23" translate="yes" xml:space="preserve">
          <source>More Information about RRef</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cea97ccd7b3c8bf2304a429a1f4a38f55fbcf7bc" translate="yes" xml:space="preserve">
          <source>More details can be found in the paper &lt;a href=&quot;https://arxiv.org/abs/1704.07483&quot;&gt;Continuously Differentiable Exponential Linear Units&lt;/a&gt; .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4458c625976778e29149ea1b97ed51860f9fe8ca" translate="yes" xml:space="preserve">
          <source>More details can be found in the paper &lt;a href=&quot;https://arxiv.org/abs/1706.02515&quot;&gt;Self-Normalizing Neural Networks&lt;/a&gt; .</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8edf225c0a72c5fe13ece57cbf2f54c6b24e17b0" translate="yes" xml:space="preserve">
          <source>More than one element of a broadcasted tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d81819b85f2cb3da872c9da3d0ec33cdc7aecdb" translate="yes" xml:space="preserve">
          <source>More than one element of a created tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="304f7ec57645990453077c598f4fe8c37d2314ab" translate="yes" xml:space="preserve">
          <source>More than one element of an expanded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f38e0558c4bf9056c21ea63c5960cbf11fd1b539" translate="yes" xml:space="preserve">
          <source>More than one element of the created tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="599668aa340279a3862bca37e7e4a6db462ba962" translate="yes" xml:space="preserve">
          <source>More than one element of the unfolded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensor, please clone it first.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="805667fbfb7f6f1745944861662036a8e58f3c82" translate="yes" xml:space="preserve">
          <source>Moreover, as for &lt;a href=&quot;#torch.Tensor.gather&quot;&gt;&lt;code&gt;gather()&lt;/code&gt;&lt;/a&gt;, the values of &lt;code&gt;index&lt;/code&gt; must be between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;self.size(dim) - 1&lt;/code&gt; inclusive, and all values in a row along the specified dimension &lt;a href=&quot;#torch.Tensor.dim&quot;&gt;&lt;code&gt;dim&lt;/code&gt;&lt;/a&gt; must be unique.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dbc5f1c387e0131e10dadc48c7f1ed13f16fda56" translate="yes" xml:space="preserve">
          <source>Most attribute types can be inferred, so &lt;code&gt;torch.jit.Attribute&lt;/code&gt; is not necessary. For empty container types, annotate their types using &lt;a href=&quot;https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations&quot;&gt;PEP 526-style&lt;/a&gt; class annotations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed994a7cd5d0a3520430a627f9865ebc68ca6011" translate="yes" xml:space="preserve">
          <source>Moved to &lt;code&gt;torch.hub&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b0f27b71a53a0f7447384ac613f55c4204b1204" translate="yes" xml:space="preserve">
          <source>Moves all model parameters and buffers to the CPU.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e8a79081679af4528b212d87c458a607691d5b3e" translate="yes" xml:space="preserve">
          <source>Moves all model parameters and buffers to the GPU.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e874c62b11df47c2d692fb8935adeff7a382eea3" translate="yes" xml:space="preserve">
          <source>Moves and/or casts the parameters and buffers.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7729e2003ebf66fb44071aeaadd11a5376f40f12" translate="yes" xml:space="preserve">
          <source>Moves the dimension(s) of &lt;code&gt;input&lt;/code&gt; at the position(s) in &lt;code&gt;source&lt;/code&gt; to the position(s) in &lt;code&gt;destination&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35ac3446a4eebf31c31aeb5dad10efd9c64f3049" translate="yes" xml:space="preserve">
          <source>Moves the storage to shared memory.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b5555e73ea9cfb4e4bc00be9e2f28d1b533e2f3" translate="yes" xml:space="preserve">
          <source>Moves the underlying storage to shared memory.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8a37b99360165eb79c6362b8c59ca260f24fc9e9" translate="yes" xml:space="preserve">
          <source>Multi-GPU collective functions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f8c9042e53df42e885d4b0f01c778b6c31356f24" translate="yes" xml:space="preserve">
          <source>Multi-Node multi-process distributed training: (e.g. two nodes)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cffbb32b31aa84528c7a48bc5418f9c2b5817403" translate="yes" xml:space="preserve">
          <source>Multi-process data loading</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e8e85a354a72efff2947c00f8ff7c48fed5f6652" translate="yes" xml:space="preserve">
          <source>MultiHead</source>
          <target state="translated">MultiHead</target>
        </trans-unit>
        <trans-unit id="aaf9b486dc5c2543a40061716ec697ddb8e0fbae" translate="yes" xml:space="preserve">
          <source>MultiLabelMarginLoss</source>
          <target state="translated">MultiLabelMarginLoss</target>
        </trans-unit>
        <trans-unit id="0bd3257d9c850deeb628f346481873a6ea866f39" translate="yes" xml:space="preserve">
          <source>MultiLabelSoftMarginLoss</source>
          <target state="translated">MultiLabelSoftMarginLoss</target>
        </trans-unit>
        <trans-unit id="65baaa8172153705ce14a7161536175046e824b3" translate="yes" xml:space="preserve">
          <source>MultiMarginLoss</source>
          <target state="translated">MultiMarginLoss</target>
        </trans-unit>
        <trans-unit id="d53b7a9617160a0821d5bb5cb1a941ffdfcd6b08" translate="yes" xml:space="preserve">
          <source>MultiheadAttention</source>
          <target state="translated">MultiheadAttention</target>
        </trans-unit>
        <trans-unit id="4319c2711cce37df073896273f25e2e05b39cbdd" translate="yes" xml:space="preserve">
          <source>Multinomial</source>
          <target state="translated">Multinomial</target>
        </trans-unit>
        <trans-unit id="7814aed0b837819b215ee4613ccc9b3a4ba6040e" translate="yes" xml:space="preserve">
          <source>Multiple Assignments</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f5facc713a0a585db79dc694e29dcb69110ceb7" translate="yes" xml:space="preserve">
          <source>Multiplies (&amp;lsquo;scales&amp;rsquo;) a tensor or list of tensors by the scale factor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67620498370f67e43d30e9f331640d50be1f106c" translate="yes" xml:space="preserve">
          <source>Multiplies &lt;code&gt;mat&lt;/code&gt; (given by &lt;code&gt;input3&lt;/code&gt;) by the orthogonal &lt;code&gt;Q&lt;/code&gt; matrix of the QR factorization formed by &lt;a href=&quot;generated/torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt; that is represented by &lt;code&gt;(a, tau)&lt;/code&gt; (given by (&lt;code&gt;input&lt;/code&gt;, &lt;code&gt;input2&lt;/code&gt;)).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="76de5400e24dedf164da206bd33a31ebb079cf8f" translate="yes" xml:space="preserve">
          <source>Multiplies &lt;code&gt;mat&lt;/code&gt; (given by &lt;code&gt;input3&lt;/code&gt;) by the orthogonal &lt;code&gt;Q&lt;/code&gt; matrix of the QR factorization formed by &lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt; that is represented by &lt;code&gt;(a, tau)&lt;/code&gt; (given by (&lt;code&gt;input&lt;/code&gt;, &lt;code&gt;input2&lt;/code&gt;)).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="329fd1dd91c45edaf7c72f188795f5545aaec268" translate="yes" xml:space="preserve">
          <source>Multiplies each element of the input &lt;code&gt;input&lt;/code&gt; with the scalar &lt;code&gt;other&lt;/code&gt; and returns a new resulting tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a50721c699d328b377c86aaee9cd7e1759fd60d6" translate="yes" xml:space="preserve">
          <source>Multiply the learning rate of each parameter group by the factor given in the specified function. When last_epoch=-1, sets initial lr as lr.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d2004ec3a414cc551a0c27958276999f72ee752" translate="yes" xml:space="preserve">
          <source>Multiprocessing best practices</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70f967b7b383e533c089648a15d728ac8a059ce8" translate="yes" xml:space="preserve">
          <source>Multiprocessing package - torch.multiprocessing</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c1a37b0bcc3effc97da7c8ab9689abbfff92b501" translate="yes" xml:space="preserve">
          <source>MultivariateNormal</source>
          <target state="translated">MultivariateNormal</target>
        </trans-unit>
        <trans-unit id="b51a60734da64be0e618bacbea2865a8a7dcd669" translate="yes" xml:space="preserve">
          <source>N</source>
          <target state="translated">N</target>
        </trans-unit>
        <trans-unit id="129ca909e4d3ed97aba21ae73582b3bd13e114f8" translate="yes" xml:space="preserve">
          <source>N = \text{batch size}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9008c254267c7b8e353b34e82cc9a3ccdffad81e" translate="yes" xml:space="preserve">
          <source>N \times 2 \times 3</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1b8b351ea25ef1c009076f028a05723a16aa0eb6" translate="yes" xml:space="preserve">
          <source>N \times 3 \times 4</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="004d23aba5b55baf9486f9aab3e50a94de7147b2" translate="yes" xml:space="preserve">
          <source>N \times C \times D \times H \times W</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14734700ec8d366b9e0bb39033413068e35b488f" translate="yes" xml:space="preserve">
          <source>N \times C \times H \times W</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db16fdaa05353f31c7f5c303d74f40ba9971f144" translate="yes" xml:space="preserve">
          <source>N \times H \times W \times 2</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7fd20744aa54e5cfb63458f679b6501918e1991e" translate="yes" xml:space="preserve">
          <source>N \times M</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c79a64aa7f7a8e49dfba455783b75f9a68074b19" translate="yes" xml:space="preserve">
          <source>N is the batch size, &lt;code&gt;*&lt;/code&gt; means any number of additional dimensions</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="72e211ac7f7abd7ef74bebbff3bdb5e0da1f0db9" translate="yes" xml:space="preserve">
          <source>N-D</source>
          <target state="translated">N-D</target>
        </trans-unit>
        <trans-unit id="7610334d74930225de51e4e4fce7e6de6c627870" translate="yes" xml:space="preserve">
          <source>NCCL has also provided a number of environment variables for fine-tuning purposes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="165a578116c3590cf83add849da91470c0378dc0" translate="yes" xml:space="preserve">
          <source>NLLLoss</source>
          <target state="translated">NLLLoss</target>
        </trans-unit>
        <trans-unit id="c63922691a03fc61bc5d30e1155494b0495ebce4" translate="yes" xml:space="preserve">
          <source>NN module forward passes have code that don&amp;rsquo;t support named tensors and will error out appropriately.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb2adaff54f80ebafd8d75fb8e20d7e6464bee58" translate="yes" xml:space="preserve">
          <source>NN module parameters are unnamed, so outputs may be partially named.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb77d2b5cb211a1bafb61c44a7f17e0c98a57348" translate="yes" xml:space="preserve">
          <source>NN modules are currently unsupported. This can lead to the following when calling modules with named tensor inputs:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0579bfd58f6bcb591b037d0d86676f26b53c86e8" translate="yes" xml:space="preserve">
          <source>NVIDIA Tools Extension (NVTX)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="855336587fa59262965cdb9a2a6114933586800b" translate="yes" xml:space="preserve">
          <source>N_i</source>
          <target state="translated">N_i</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
