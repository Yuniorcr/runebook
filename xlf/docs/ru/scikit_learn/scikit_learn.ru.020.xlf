<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ru" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="e3ab7886f45f0e5bcfcb494c5dda13f6aee54058" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;mean_fit_time&lt;/code&gt;, &lt;code&gt;std_fit_time&lt;/code&gt;, &lt;code&gt;mean_score_time&lt;/code&gt; and &lt;code&gt;std_score_time&lt;/code&gt; are all in seconds.</source>
          <target state="translated">Значения &lt;code&gt;mean_fit_time&lt;/code&gt; , &lt;code&gt;std_fit_time&lt;/code&gt; , &lt;code&gt;mean_score_time&lt;/code&gt; и &lt;code&gt;std_score_time&lt;/code&gt; указаны в секундах.</target>
        </trans-unit>
        <trans-unit id="5067a7dbb7441ffa442bc35298e784d3bbb5d81c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;out_of_bounds&lt;/code&gt; parameter handles how &lt;code&gt;X&lt;/code&gt; values outside of the training domain are handled. When set to &amp;ldquo;nan&amp;rdquo;, predictions will be NaN. When set to &amp;ldquo;clip&amp;rdquo;, predictions will be set to the value corresponding to the nearest train interval endpoint. When set to &amp;ldquo;raise&amp;rdquo; a &lt;code&gt;ValueError&lt;/code&gt; is raised.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4dc710fdb008b124893854b1db0d772123e2f23c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;out_of_bounds&lt;/code&gt; parameter handles how x-values outside of the training domain are handled. When set to &amp;ldquo;nan&amp;rdquo;, predicted y-values will be NaN. When set to &amp;ldquo;clip&amp;rdquo;, predicted y-values will be set to the value corresponding to the nearest train interval endpoint. When set to &amp;ldquo;raise&amp;rdquo;, allow &lt;code&gt;interp1d&lt;/code&gt; to throw ValueError.</source>
          <target state="translated">Параметр &lt;code&gt;out_of_bounds&lt;/code&gt; управляет тем, как обрабатываются значения x вне области обучения. Если установлено значение &amp;laquo;nan&amp;raquo;, прогнозируемые значения y будут NaN. Если установлено значение &amp;laquo;clip&amp;raquo;, прогнозируемые значения y будут установлены на значение, соответствующее ближайшей конечной точке интервала поезда. Если установлено значение &amp;laquo;raise&amp;raquo;, разрешить &lt;code&gt;interp1d&lt;/code&gt; выбросить ValueError.</target>
        </trans-unit>
        <trans-unit id="3c0e73047db48d9d2c7fabbdd5b52d96e2b806a9" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;partial_fit&lt;/code&gt; method call of naive Bayes models introduces some computational overhead. It is recommended to use data chunk sizes that are as large as possible, that is as the available RAM allows.</source>
          <target state="translated">&lt;code&gt;partial_fit&lt;/code&gt; вызов метода наивных моделей Байеса представляет некоторую вычислительную нагрузку. Рекомендуется использовать как можно больший размер блоков данных, т.е. насколько позволяет доступная оперативная память.</target>
        </trans-unit>
        <trans-unit id="12fb3214c445789e7d1fb007eb13c0f9c87b3271" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;penalty&lt;/code&gt; parameter determines the regularization to be used (see description above in the classification section).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c075895944959b3ce70972d2605db496c74ee36b" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;preprocessing&lt;/code&gt; module further provides a utility class &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt;&lt;code&gt;Normalizer&lt;/code&gt;&lt;/a&gt; that implements the same operation using the &lt;code&gt;Transformer&lt;/code&gt; API (even though the &lt;code&gt;fit&lt;/code&gt; method is useless in this case: the class is stateless as this operation treats samples independently).</source>
          <target state="translated">Модуль &lt;code&gt;preprocessing&lt;/code&gt; также предоставляет служебный класс &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt; &lt;code&gt;Normalizer&lt;/code&gt; ,&lt;/a&gt; который реализует ту же операцию с использованием &lt;code&gt;Transformer&lt;/code&gt; API (даже несмотря на то, что в этом случае метод &lt;code&gt;fit&lt;/code&gt; бесполезен: класс не имеет состояния, поскольку эта операция обрабатывает образцы независимо).</target>
        </trans-unit>
        <trans-unit id="deb9c47cdc32b652eb6b8e87508a50e73ddd6520" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;preprocessing&lt;/code&gt; module further provides a utility class &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt; that implements the &lt;code&gt;Transformer&lt;/code&gt; API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. This class is hence suitable for use in the early steps of a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">Модуль &lt;code&gt;preprocessing&lt;/code&gt; дополнительно предоставляет служебный класс &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; ,&lt;/a&gt; который реализует &lt;code&gt;Transformer&lt;/code&gt; API для вычисления среднего и стандартного отклонения на обучающем наборе, чтобы впоследствии иметь возможность повторно применить то же преобразование к набору тестирования. Таким образом, этот класс подходит для использования на ранних этапах &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="1c988d2d2cf78f045059c60f8f22ddfe99f6b9b8" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;random_state&lt;/code&gt; parameter defaults to &lt;code&gt;None&lt;/code&gt;, meaning that the shuffling will be different every time &lt;code&gt;KFold(..., shuffle=True)&lt;/code&gt; is iterated. However, &lt;code&gt;GridSearchCV&lt;/code&gt; will use the same shuffling for each set of parameters validated by a single call to its &lt;code&gt;fit&lt;/code&gt; method.</source>
          <target state="translated">По умолчанию для параметра &lt;code&gt;random_state&lt;/code&gt; установлено значение &lt;code&gt;None&lt;/code&gt; , что означает, что перемешивание будет различным каждый раз, когда &lt;code&gt;KFold(..., shuffle=True)&lt;/code&gt; . Тем не менее, &lt;code&gt;GridSearchCV&lt;/code&gt; будет использовать ту же перестановку для каждого набора параметров подтверждены одним вызова его &lt;code&gt;fit&lt;/code&gt; метода.</target>
        </trans-unit>
        <trans-unit id="269b993dc29ab1401256004d5e02aa3f5322b3b6" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;remainder&lt;/code&gt; parameter can be set to an estimator to transform the remaining rating columns. The transformed values are appended to the end of the transformation:</source>
          <target state="translated">Параметр &lt;code&gt;remainder&lt;/code&gt; может быть установлен в оценщик, чтобы преобразовать оставшиеся столбцы рейтинга. Преобразованные значения добавляются в конец преобразования:</target>
        </trans-unit>
        <trans-unit id="ac9a3f7a78a3eea81f462ceabf4e7d7b7695c3e1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;roc_auc_score&lt;/code&gt; function can also be used in multi-class classification. Two averaging strategies are currently supported: the one-vs-one algorithm computes the average of the pairwise ROC AUC scores, and the one-vs-rest algorithm computes the average of the ROC AUC scores for each class against all other classes. In both cases, the multiclass ROC AUC scores are computed from the probability estimates that a sample belongs to a particular class according to the model. The OvO and OvR algorithms support weighting uniformly (&lt;code&gt;average='macro'&lt;/code&gt;) and weighting by the prevalence (&lt;code&gt;average='weighted'&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5cf025ac399ddc1c71c004c7cb8bd73171357561" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;shrinkage&lt;/code&gt; parameter can also be manually set between 0 and 1. In particular, a value of 0 corresponds to no shrinkage (which means the empirical covariance matrix will be used) and a value of 1 corresponds to complete shrinkage (which means that the diagonal matrix of variances will be used as an estimate for the covariance matrix). Setting this parameter to a value between these two extrema will estimate a shrunk version of the covariance matrix.</source>
          <target state="translated">Параметр &lt;code&gt;shrinkage&lt;/code&gt; также можно установить вручную между 0 и 1. В частности, значение 0 соответствует отсутствию усадки (что означает, что будет использоваться эмпирическая ковариационная матрица), а значение 1 соответствует полной усадке (что означает, что диагональ матрица дисперсий будет использоваться в качестве оценки для ковариационной матрицы). Установка для этого параметра значения между этими двумя экстремумами будет оценивать сокращенную версию ковариационной матрицы.</target>
        </trans-unit>
        <trans-unit id="ac317fd98a77c706ac5d8a7e74a96d7bdbebfe03" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.covariance&lt;/code&gt; package implements a robust estimator of covariance, the Minimum Covariance Determinant &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;3&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b439b9c10b67475737ae3e151e90a6ed815edbd1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.covariance&lt;/code&gt; package implements a robust estimator of covariance, the Minimum Covariance Determinant &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;[3]&lt;/a&gt;.</source>
          <target state="translated">Пакет &lt;code&gt;sklearn.covariance&lt;/code&gt; реализует надежную оценку ковариации, определитель минимальной ковариации &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;[3]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="6908af1748b7fe666814b53ca6e0a8cab66eb012" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.datasets&lt;/code&gt; package embeds some small toy datasets as introduced in the &lt;a href=&quot;../tutorial/basic/tutorial#loading-example-dataset&quot;&gt;Getting Started&lt;/a&gt; section.</source>
          <target state="translated">Пакет &lt;code&gt;sklearn.datasets&lt;/code&gt; включает в себя несколько небольших наборов данных игрушек, как описано в разделе &amp;laquo; &lt;a href=&quot;../tutorial/basic/tutorial#loading-example-dataset&quot;&gt;Начало работы&lt;/a&gt; &amp;raquo;.</target>
        </trans-unit>
        <trans-unit id="82c9f1d0e48a88b08f4d85a858e0d70b67f55eec" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.datasets&lt;/code&gt; package is able to download datasets from the repository using the function &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_openml#sklearn.datasets.fetch_openml&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_openml&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Пакет &lt;code&gt;sklearn.datasets&lt;/code&gt; может загружать наборы данных из репозитория с помощью функции &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_openml#sklearn.datasets.fetch_openml&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_openml&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="8a80b4da04f92037fdd1ffd5771651becd3a649e" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.preprocessing&lt;/code&gt; package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.</source>
          <target state="translated">Пакет &lt;code&gt;sklearn.preprocessing&lt;/code&gt; предоставляет несколько общих служебных функций и классов преобразователей для преобразования необработанных векторов признаков в представление, более подходящее для последующих оценщиков.</target>
        </trans-unit>
        <trans-unit id="9a666cbe94ae4d4ef285ecec57ff29087d1b1c92" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;stop_words_&lt;/code&gt; attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling.</source>
          <target state="translated">&lt;code&gt;stop_words_&lt;/code&gt; атрибут может получить большой и увеличить размер модели при мариновании. Этот атрибут предоставляется только для самоанализа и может быть безопасно удален с помощью delattr или установлен на None перед травлением.</target>
        </trans-unit>
        <trans-unit id="a2df492be59e0b5e94ab9ece47f7ab58734f8d4a" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;svm.OneClassSVM&lt;/code&gt; is known to be sensitive to outliers and thus does not perform very well for outlier detection. This estimator is best suited for novelty detection when the training set is not contaminated by outliers. That said, outlier detection in high-dimension, or without any assumptions on the distribution of the inlying data is very challenging, and a One-class SVM might give useful results in these situations depending on the value of its hyperparameters.</source>
          <target state="translated">&lt;code&gt;svm.OneClassSVM&lt;/code&gt; , как известно, чувствительна к выбросам и , таким образом , не выполняет очень хорошо для обнаружения аномальное значение. Эта оценка лучше всего подходит для обнаружения новизны, когда обучающая выборка не загрязнена выбросами. Тем не менее, обнаружение выбросов в большой размерности или без каких-либо предположений о распределении входящих данных является очень сложной задачей, и SVM одного класса может дать полезные результаты в этих ситуациях в зависимости от значения его гиперпараметров.</target>
        </trans-unit>
        <trans-unit id="ae04908924f55e8ffa997283ff07a48e73585d0d" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;tree_disp&lt;/code&gt; and &lt;code&gt;mlp_disp&lt;/code&gt;&lt;a href=&quot;../../modules/generated/sklearn.inspection.partialdependencedisplay#sklearn.inspection.PartialDependenceDisplay&quot;&gt;&lt;code&gt;PartialDependenceDisplay&lt;/code&gt;&lt;/a&gt; objects contain all the computed information needed to recreate the partial dependence curves. This means we can easily create additional plots without needing to recompute the curves.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0172c625369e43fa830669101114953189c751e" translate="yes" xml:space="preserve">
          <source>The &lt;em&gt;kernel function&lt;/em&gt; can be any of the following:</source>
          <target state="translated">Функция &lt;em&gt;ядра&lt;/em&gt; может быть любой из следующих:</target>
        </trans-unit>
        <trans-unit id="9153a9bf6c588ab4d00c19f8aab755b4d4790f32" translate="yes" xml:space="preserve">
          <source>The &lt;em&gt;reachability&lt;/em&gt; distances generated by OPTICS allow for variable density extraction of clusters within a single data set. As shown in the above plot, combining &lt;em&gt;reachability&lt;/em&gt; distances and data set &lt;code&gt;ordering_&lt;/code&gt; produces a &lt;em&gt;reachability plot&lt;/em&gt;, where point density is represented on the Y-axis, and points are ordered such that nearby points are adjacent. &amp;lsquo;Cutting&amp;rsquo; the reachability plot at a single value produces DBSCAN like results; all points above the &amp;lsquo;cut&amp;rsquo; are classified as noise, and each time that there is a break when reading from left to right signifies a new cluster. The default cluster extraction with OPTICS looks at the steep slopes within the graph to find clusters, and the user can define what counts as a steep slope using the parameter &lt;code&gt;xi&lt;/code&gt;. There are also other possibilities for analysis on the graph itself, such as generating hierarchical representations of the data through reachability-plot dendrograms, and the hierarchy of clusters detected by the algorithm can be accessed through the &lt;code&gt;cluster_hierarchy_&lt;/code&gt; parameter. The plot above has been color-coded so that cluster colors in planar space match the linear segment clusters of the reachability plot. Note that the blue and red clusters are adjacent in the reachability plot, and can be hierarchically represented as children of a larger parent cluster.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6eff32d476fb25ebcdfdcb46623a8711aa972809" translate="yes" xml:space="preserve">
          <source>The &lt;strong&gt;conditional entropy of clusters given class&lt;/strong&gt;\(H(K|C)\) and the &lt;strong&gt;entropy of clusters&lt;/strong&gt;\(H(K)\) are defined in a symmetric manner.</source>
          <target state="translated">&lt;strong&gt;Условная энтропия кластеров данного класса&lt;/strong&gt; \ (Н (К | С) \) и &lt;strong&gt;энтропией кластеров&lt;/strong&gt; \ (Н (К) \) определены в симметричном образе.</target>
        </trans-unit>
        <trans-unit id="5adbfb7d3b47e961b1e75927796552dd26ad41e4" translate="yes" xml:space="preserve">
          <source>The &lt;strong&gt;exposure&lt;/strong&gt; is the duration of the insurance coverage of a given policy, in years.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a256791561f56278bffe5a6063bb1e7227590f9a" translate="yes" xml:space="preserve">
          <source>The AGE and EXPERIENCE coefficients are affected by strong variability which might be due to the collinearity between the 2 features: as AGE and EXPERIENCE vary together in the data, their effect is difficult to tease apart.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e076b7d1d24895a169d091c6b0d7ae51431e1b5b" translate="yes" xml:space="preserve">
          <source>The AGE coefficient is expressed in &amp;ldquo;dollars/hour per living years&amp;rdquo; while the EDUCATION one is expressed in &amp;ldquo;dollars/hour per years of education&amp;rdquo;. This representation of the coefficients has the benefit of making clear the practical predictions of the model: an increase of \(1\) year in AGE means a decrease of \(0.030867\) dollars/hour, while an increase of \(1\) year in EDUCATION means an increase of \(0.054699\) dollars/hour. On the other hand, categorical variables (as UNION or SEX) are adimensional numbers taking either the value 0 or 1. Their coefficients are expressed in dollars/hour. Then, we cannot compare the magnitude of different coefficients since the features have different natural scales, and hence value ranges, because of their different unit of measure. This is more visible if we plot the coefficients.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2143ba51f2aad99e99e8cd60a62ccfdecbf0f265" translate="yes" xml:space="preserve">
          <source>The AMI returns a value of 1 when the two partitions are identical (ie perfectly matched). Random partitions (independent labellings) have an expected AMI around 0 on average hence can be negative.</source>
          <target state="translated">AMI возвращает значение 1,когда два простенка идентичны (т.е.идеально подходят друг другу).Случайные разделы (независимые обозначения)имеют ожидаемый AMI в среднем около 0,следовательно,могут быть отрицательными.</target>
        </trans-unit>
        <trans-unit id="ad838931339007a1bda099c18480308bbb327339" translate="yes" xml:space="preserve">
          <source>The API is experimental (particularly the return value structure), and might have small backward-incompatible changes in future releases.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="00e3722885bdadae5430c6fecaacfc1cced893f6" translate="yes" xml:space="preserve">
          <source>The API is experimental in version 0.20 (particularly the return value structure), and might have small backward-incompatible changes in future releases.</source>
          <target state="translated">API является экспериментальным в версии 0.20 (в частности,структура возвращаемого значения),и может иметь небольшие обратные несовместимые изменения в будущих выпусках.</target>
        </trans-unit>
        <trans-unit id="e07138bd94d8d9dd92474342c51f134fa3423d8d" translate="yes" xml:space="preserve">
          <source>The Ames housing dataset is not shipped with scikit-learn and therefore we will fetch it from &lt;a href=&quot;https://www.openml.org/d/42165&quot;&gt;OpenML&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22556f00c2cdfc8c60673e1c663299619a64901c" translate="yes" xml:space="preserve">
          <source>The BIC criterion can be used to select the number of components in a Gaussian Mixture in an efficient way. In theory, it recovers the true number of components only in the asymptotic regime (i.e. if much data is available and assuming that the data was actually generated i.i.d. from a mixture of Gaussian distribution). Note that using a &lt;a href=&quot;#bgmm&quot;&gt;Variational Bayesian Gaussian mixture&lt;/a&gt; avoids the specification of the number of components for a Gaussian mixture model.</source>
          <target state="translated">Критерий BIC может использоваться для эффективного выбора количества компонентов в гауссовой смеси. Теоретически он восстанавливает истинное количество компонентов только в асимптотическом режиме (т. Е. Если доступно много данных и предполагается, что данные были фактически сгенерированы iid из смеси гауссовых распределений). Обратите внимание, что использование &lt;a href=&quot;#bgmm&quot;&gt;вариационной байесовской гауссовой смеси&lt;/a&gt; позволяет избежать указания количества компонентов для модели гауссовой смеси.</target>
        </trans-unit>
        <trans-unit id="c912c462511f5921d016fed26135f626922ccc57" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut implementation only works when the target dimensionality is 3 or less. The 2D case is typical when building visualizations.</source>
          <target state="translated">Реализация Barnes-Hut работает только тогда,когда целевая размерность не превышает 3.Случай 2D типичен при построении визуализаций.</target>
        </trans-unit>
        <trans-unit id="49027a83447d6307ec530f0dd81bd5ad76360faa" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.</source>
          <target state="translated">Метод Barnes-Hut t-SNE ограничен двух-или трехмерными вставками.</target>
        </trans-unit>
        <trans-unit id="f05f901deabcb1ee376f8697fc79932cbf746ce6" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut t-SNE that has been implemented here is usually much slower than other manifold learning algorithms. The optimization is quite difficult and the computation of the gradient is \(O[d N log(N)]\), where \(d\) is the number of output dimensions and \(N\) is the number of samples. The Barnes-Hut method improves on the exact method where t-SNE complexity is \(O[d N^2]\), but has several other notable differences:</source>
          <target state="translated">Реализованная здесь система Barnes-Hut t-SNE,как правило,работает гораздо медленнее,чем другие разнообразные алгоритмы обучения.Оптимизация достаточно сложна и вычисление градиента-\(O[d N log(N)]\),где \(d\)-количество выходных размеров,а \(N\)-количество отсчетов.Метод Барнс-Хат совершенствуется в точном методе,где сложность t-SNE-\(O[d N^2]\),но имеет ряд других заметных отличий:</target>
        </trans-unit>
        <trans-unit id="e724094f6d5c410991717a9e2fc045d6798def45" translate="yes" xml:space="preserve">
          <source>The Birch algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters.</source>
          <target state="translated">Алгоритм Березы имеет два параметра,порог и коэффициент ветвления.Коэффициент ветвления ограничивает количество подкластеров в узле,а порог-расстояние между входящей выборкой и существующими подкластерами.</target>
        </trans-unit>
        <trans-unit id="8fa70be719c3475e8e012f73acc572a21c03cdd8" translate="yes" xml:space="preserve">
          <source>The Boston house-price data has been used in many machine learning papers that address regression problems.</source>
          <target state="translated">Данные о ценах на дома в Бостоне были использованы во многих документах по машинному обучению,в которых рассматривались проблемы регрессии.</target>
        </trans-unit>
        <trans-unit id="455243540177422da87f9c3aa93de30456a2bae3" translate="yes" xml:space="preserve">
          <source>The Boston house-price data of Harrison, D. and Rubinfeld, D.L. &amp;lsquo;Hedonic prices and the demand for clean air&amp;rsquo;, J. Environ. Economics &amp;amp; Management, vol.5, 81-102, 1978. Used in Belsley, Kuh &amp;amp; Welsch, &amp;lsquo;Regression diagnostics &amp;hellip;&amp;rsquo;, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter.</source>
          <target state="translated">Данные о ценах на жилье в Бостоне, представленные Д. Харрисоном и Рубинфельдом Д. Л. &amp;laquo;Гедонистические цены и спрос на чистый воздух&amp;raquo;, J. Environ. Economics &amp;amp; Management, vol.5, 81-102, 1978. Используется в Belsley, Kuh &amp;amp; Welsch, &amp;laquo;Regression Diagnostics&amp;hellip;&amp;raquo;, Wiley, 1980. NB. В таблице на страницах 244-261 последнего используются различные преобразования.</target>
        </trans-unit>
        <trans-unit id="7bc398b9797cca816dee96d9ed40f902ff743772" translate="yes" xml:space="preserve">
          <source>The Bunch object is a dictionary that exposes its keys are attributes. For more information about Bunch object, see &lt;a href=&quot;../modules/generated/sklearn.utils.bunch#sklearn.utils.Bunch&quot;&gt;&lt;code&gt;sklearn.utils.Bunch&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e6c056d908bee991f648998f41ff72e8a7301c2" translate="yes" xml:space="preserve">
          <source>The CF Subclusters hold the necessary information for clustering which prevents the need to hold the entire input data in memory. This information includes:</source>
          <target state="translated">В подкластерах CF хранится необходимая для кластеризации информация,которая предотвращает необходимость хранить в памяти все входные данные.Эта информация включает в себя:</target>
        </trans-unit>
        <trans-unit id="b64782d12e8b4e97de28df15435a193ab8a372e9" translate="yes" xml:space="preserve">
          <source>The Calinski-Harabasz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b96c855c122e21633007587f3600e092339a5383" translate="yes" xml:space="preserve">
          <source>The Calinski-Harabaz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">Индекс Калински-Харабаза обычно выше для выпуклых кластеров,чем у других концепций кластеров,например,кластеров на основе плотности,подобных тем,которые получены через DBSCAN.</target>
        </trans-unit>
        <trans-unit id="6cdbc3a888667e8344981c2a8742122994627087" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier described in Rennie et al.</source>
          <target state="translated">Классификатор дополнения Naive Bayes,описанный в работе Ренни и др.</target>
        </trans-unit>
        <trans-unit id="2b0b8b0a4dd5523203a6ee4b5d195c2e6a35c0fe" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier described in Rennie et al. (2003).</source>
          <target state="translated">Классификатор дополнения Naive Bayes,описанный в работе Rennie et al.(2003).</target>
        </trans-unit>
        <trans-unit id="3398d87a3aceb1ea8375c52fcc6a67cec5c63df9" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier was designed to correct the &amp;ldquo;severe assumptions&amp;rdquo; made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.</source>
          <target state="translated">Дополнительный наивный байесовский классификатор был разработан для исправления &amp;laquo;серьезных допущений&amp;raquo;, сделанных стандартным полиномиальным наивным байесовским классификатором. Он особенно подходит для несбалансированных наборов данных.</target>
        </trans-unit>
        <trans-unit id="7c176cfd9628fae51161daa03c193aeae01d49ea" translate="yes" xml:space="preserve">
          <source>The Contrastive Divergence method suggests to stop the chain after a small number of iterations, \(k\), usually even 1. This method is fast and has low variance, but the samples are far from the model distribution.</source>
          <target state="translated">Метод контрастной дивергенции предполагает остановку цепочки после небольшого количества итераций,\(k\),обычно даже 1.Этот метод быстр и имеет низкую дисперсию,но выборки далеки от модельного распределения.</target>
        </trans-unit>
        <trans-unit id="1d09474bd461f8756796df6e9a77749a8489b6d3" translate="yes" xml:space="preserve">
          <source>The DBSCAN algorithm is deterministic, always generating the same clusters when given the same data in the same order. However, the results can differ when data is provided in a different order. First, even though the core samples will always be assigned to the same clusters, the labels of those clusters will depend on the order in which those samples are encountered in the data. Second and more importantly, the clusters to which non-core samples are assigned can differ depending on the data order. This would happen when a non-core sample has a distance lower than &lt;code&gt;eps&lt;/code&gt; to two core samples in different clusters. By the triangular inequality, those two core samples must be more distant than &lt;code&gt;eps&lt;/code&gt; from each other, or they would be in the same cluster. The non-core sample is assigned to whichever cluster is generated first in a pass through the data, and so the results will depend on the data ordering.</source>
          <target state="translated">Алгоритм DBSCAN является детерминированным, всегда генерируя одни и те же кластеры при получении одних и тех же данных в том же порядке. Однако результаты могут отличаться, если данные предоставляются в другом порядке. Во-первых, даже если основные образцы всегда будут назначаться одним и тем же кластерам, метки этих кластеров будут зависеть от порядка, в котором эти образцы встречаются в данных. Во-вторых, что более важно, кластеры, которым назначаются неосновные выборки, могут различаться в зависимости от порядка данных. Это может произойти, если образец неосновного керна находится на расстоянии меньше &lt;code&gt;eps&lt;/code&gt; до двух образцов керна в разных кластерах. Согласно треугольному неравенству эти два образца керна должны быть дальше, чем &lt;code&gt;eps&lt;/code&gt; друг от друга, иначе они были бы в одном кластере. Неосновная выборка назначается тому кластеру, который создается первым при прохождении данных, поэтому результаты будут зависеть от порядка данных.</target>
        </trans-unit>
        <trans-unit id="b5783662e991f62ab3cc63e2843b11c10be0af6e" translate="yes" xml:space="preserve">
          <source>The Davies-Boulding index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained from DBSCAN.</source>
          <target state="translated">Индекс Дэвиса-Болдинга,как правило,выше для выпуклых кластеров,чем у других концепций кластеров,например,кластеров на основе плотности,подобных тем,которые получены из DBSCAN.</target>
        </trans-unit>
        <trans-unit id="9dae6187a2e4264ce3764c47d6ced56e22112bc3" translate="yes" xml:space="preserve">
          <source>The Digit Dataset</source>
          <target state="translated">Цифровой набор данных</target>
        </trans-unit>
        <trans-unit id="15949e73904f01f7a29e0206a3232b388b3af43d" translate="yes" xml:space="preserve">
          <source>The Dirichlet process prior allows to define an infinite number of components and automatically selects the correct number of components: it activates a component only if it is necessary.</source>
          <target state="translated">Предварительный процесс Дирихлета позволяет определить бесконечное количество компонентов и автоматически выбирает нужное количество компонентов:он активирует компонент только в случае необходимости.</target>
        </trans-unit>
        <trans-unit id="a257bf9309fabcccadc69c1bb14244b979e6f754" translate="yes" xml:space="preserve">
          <source>The Discounted Cumulative Gain divided by the Ideal Discounted Cumulative Gain (the DCG obtained for a perfect ranking), in order to have a score between 0 and 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d2e49448333a2cd92baf05825e927765f1835316" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is commonly combined with exponentiation.</source>
          <target state="translated">Ядро DotProduct обычно сочетается с экспоненцией.</target>
        </trans-unit>
        <trans-unit id="842c0c72bca462c8ce3e9ff0cead821c33430a8a" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is non-stationary and can be obtained from linear regression by putting N(0, 1) priors on the coefficients of x_d (d = 1, . . . , D) and a prior of N(0, sigma_0^2) on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter sigma_0^2. For sigma_0^2 =0, the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</source>
          <target state="translated">Ядро DotProduct является нестационарным и может быть получено из линейной регрессии путем наложения N(0,1)приоров на коэффициенты x_d (d=1,...,D)и априор N(0,sigma_0^2)на смещение.Ядро DotProduct инвариантно к вращению координат о происхождении,но не к переводам.Оно параметризуется параметром sigma_0^2.Для sigma_0^2 =0 ядро называется однородным линейным ядром,в противном случае оно неоднородно.Ядро задается параметром</target>
        </trans-unit>
        <trans-unit id="078b4d2b566f931479e95c97a38f05103a2ecb7d" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is non-stationary and can be obtained from linear regression by putting \(N(0, 1)\) priors on the coefficients of \(x_d (d = 1, . . . , D)\) and a prior of \(N(0, \sigma_0^2)\) on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter sigma_0 \(\sigma\) which controls the inhomogenity of the kernel. For \(\sigma_0^2 =0\), the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c219b28ff4dd0afb8b865b96896a279315780153" translate="yes" xml:space="preserve">
          <source>The Elastic Net mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15.</source>
          <target state="translated">Параметр смешивания эластичной сети, где 0 &amp;lt;= l1_ratio &amp;lt;= 1. l1_ratio = 0 соответствует штрафу L2, l1_ratio = 1 - L1. По умолчанию 0,15.</target>
        </trans-unit>
        <trans-unit id="bc71ca7c36fa98be5da488c2c0295a9dca2a49c6" translate="yes" xml:space="preserve">
          <source>The Elastic Net mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Only used if &lt;code&gt;penalty&lt;/code&gt; is &amp;lsquo;elasticnet&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="557cde32667038df92bd17ed0a660f8397aa9f15" translate="yes" xml:space="preserve">
          <source>The Elastic-Net mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. Only used if &lt;code&gt;penalty='elasticnet'&lt;/code&gt;. Setting &lt;code&gt;l1_ratio=0&lt;/code&gt; is equivalent to using &lt;code&gt;penalty='l2'&lt;/code&gt;, while setting &lt;code&gt;l1_ratio=1&lt;/code&gt; is equivalent to using &lt;code&gt;penalty='l1'&lt;/code&gt;. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt;1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71317f65b5997839fb7f2d86b8c73dc4399ad254" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with 0 &amp;lt; l1_ratio &amp;lt;= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1/L2 and L2.</source>
          <target state="translated">Параметр смешивания ElasticNet с 0 &amp;lt;l1_ratio &amp;lt;= 1. Для l1_ratio = 1 штрафом является штраф L1 / L2. Для l1_ratio = 0 это штраф L2. Для &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; штраф представляет собой комбинацию L1 / L2 и L2.</target>
        </trans-unit>
        <trans-unit id="f913d5ab29d41b1d0b8510f8960f88320058e481" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with 0 &amp;lt; l1_ratio &amp;lt;= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1/L2 and L2. This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in &lt;code&gt;[.1, .5, .7,
.9, .95, .99, 1]&lt;/code&gt;</source>
          <target state="translated">Параметр смешивания ElasticNet с 0 &amp;lt;l1_ratio &amp;lt;= 1. Для l1_ratio = 1 штрафом является штраф L1 / L2. Для l1_ratio = 0 это штраф L2. Для &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; штраф представляет собой комбинацию L1 / L2 и L2. Этот параметр может быть списком, и в этом случае различные значения проверяются перекрестной проверкой, и используется тот, который дает лучший результат прогнозирования. Обратите внимание, что хороший выбор списка значений для l1_ratio часто состоит в том, чтобы помещать больше значений, близких к 1 (т.е. Лассо) и менее близких к 0 (т.е. Ridge), как в &lt;code&gt;[.1, .5, .7, .9, .95, .99, 1]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="62ded71b403044434993092527c854b8f53e5c17" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. For &lt;code&gt;l1_ratio = 0&lt;/code&gt; the penalty is an L2 penalty. &lt;code&gt;For l1_ratio = 1&lt;/code&gt; it is an L1 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="translated">Параметр смешивания ElasticNet с &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt; . Для &lt;code&gt;l1_ratio = 0&lt;/code&gt; штраф - это штраф L2. &lt;code&gt;For l1_ratio = 1&lt;/code&gt; это штраф L1. Для &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; штраф представляет собой комбинацию L1 и L2.</target>
        </trans-unit>
        <trans-unit id="706ede4c7ae02c53bd29139976a5c9ab95013fca" translate="yes" xml:space="preserve">
          <source>The ExpSineSquared kernel allows modeling periodic functions. It is parameterized by a length-scale parameter length_scale&amp;gt;0 and a periodicity parameter periodicity&amp;gt;0. Only the isotropic variant where l is a scalar is supported at the moment. The kernel given by:</source>
          <target state="translated">Ядро ExpSineSquared позволяет моделировать периодические функции. Он параметризуется параметром масштаба длины length_scale&amp;gt; 0 и параметром периодичности периодичности&amp;gt; 0. На данный момент поддерживается только изотропный вариант, где l - скаляр. Ядро, данное:</target>
        </trans-unit>
        <trans-unit id="648ff6d48b0697b9c2efa5d7b7f9a5acf1d3efbe" translate="yes" xml:space="preserve">
          <source>The ExpSineSquared kernel allows one to model functions which repeat themselves exactly. It is parameterized by a length scale parameter \(l&amp;gt;0\) and a periodicity parameter \(p&amp;gt;0\). Only the isotropic variant where \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31f2158684fd941b75a9b2f295ee3be83d4fb497" translate="yes" xml:space="preserve">
          <source>The Exponentiation kernel takes one base kernel and a scalar parameter \(p\) and combines them via</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd1aecbb19e2286cfafdebb8292f1071a3eb508c" translate="yes" xml:space="preserve">
          <source>The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.</source>
          <target state="translated">Счет F-бета может быть интерпретирован как взвешенное гармоническое значение точности и вспомнить,где счет F-бета достигает своего лучшего значения при 1 и худшего результата при 0.</target>
        </trans-unit>
        <trans-unit id="fcc8a075bfea5f42d0e38256200386182d33cc42" translate="yes" xml:space="preserve">
          <source>The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0.</source>
          <target state="translated">F-бета балл является взвешенным средним гармоническим значением точности и памяти,достигая своего оптимального значения при 1 и его наихудшего значения при 0.</target>
        </trans-unit>
        <trans-unit id="553773b1d0f7903e424f857dbba13f4b2343a5a1" translate="yes" xml:space="preserve">
          <source>The F-beta score weights recall more than precision by a factor of &lt;code&gt;beta&lt;/code&gt;. &lt;code&gt;beta == 1.0&lt;/code&gt; means recall and precision are equally important.</source>
          <target state="translated">Веса по шкале F-beta напоминают больше, чем точность в &lt;code&gt;beta&lt;/code&gt; коэффициенте . &lt;code&gt;beta == 1.0&lt;/code&gt; означает, что полнота и точность одинаково важны.</target>
        </trans-unit>
        <trans-unit id="3f53f44feaa854c0fdf5365ef5e43bc7fa440b01" translate="yes" xml:space="preserve">
          <source>The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:</source>
          <target state="translated">Балл F1 может быть интерпретирован как средневзвешенное значение точности и отзыва,где балл F1 достигает своего лучшего значения в 1 и худшего в 0.Относительный вклад точности и отзыва в балл F1 равны.Формула для оценки F1 равна:</target>
        </trans-unit>
        <trans-unit id="518509d08bd98ece386f11a6583f35b4f559fdac" translate="yes" xml:space="preserve">
          <source>The Figure below shows four one-way and one two-way partial dependence plots for the California housing dataset:</source>
          <target state="translated">На рисунке ниже показаны четыре односторонних и один двусторонний графики частичной зависимости для калифорнийского набора данных по жилью:</target>
        </trans-unit>
        <trans-unit id="8d40f6c8f7d760d5a9c3769f72b85293905bcf21" translate="yes" xml:space="preserve">
          <source>The Figure below shows the contours of the different regularization terms in a 2-dimensional parameter space (\(m=2\)) when \(R(w) = 1\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2280ec0b9d5ba5eef024fd7f6041defdab8c9d48" translate="yes" xml:space="preserve">
          <source>The Figure below shows the contours of the different regularization terms in the parameter space when \(R(w) = 1\).</source>
          <target state="translated">На рисунке ниже показаны контуры различных терминов регуляризации в пространстве параметров,когда \(R(w)=1\).</target>
        </trans-unit>
        <trans-unit id="af21e4c771514d1aca6cd1c14dcff1ce67a0e477" translate="yes" xml:space="preserve">
          <source>The Fowlkes-Mallows index (&lt;a href=&quot;generated/sklearn.metrics.fowlkes_mallows_score#sklearn.metrics.fowlkes_mallows_score&quot;&gt;&lt;code&gt;sklearn.metrics.fowlkes_mallows_score&lt;/code&gt;&lt;/a&gt;) can be used when the ground truth class assignments of the samples is known. The Fowlkes-Mallows score FMI is defined as the geometric mean of the pairwise precision and recall:</source>
          <target state="translated">Индекс Фаулкса-Маллоуз ( &lt;a href=&quot;generated/sklearn.metrics.fowlkes_mallows_score#sklearn.metrics.fowlkes_mallows_score&quot;&gt; &lt;code&gt;sklearn.metrics.fowlkes_mallows_score&lt;/code&gt; &lt;/a&gt; ) может использоваться, когда известны наземные присвоения классов истинности выборкам. FMI по шкале Фаулкса-Маллоуса определяется как среднее геометрическое для попарной точности и отзыва:</target>
        </trans-unit>
        <trans-unit id="bfdc8ed29be1f90cbfb4dc9576ce17a736f93294" translate="yes" xml:space="preserve">
          <source>The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of the precision and recall:</source>
          <target state="translated">Индекс Fowlkes-Mallows (FMI)определяется как среднее геометрическое значение между точностью и отзывом:</target>
        </trans-unit>
        <trans-unit id="fb7ab21b6034e9b9d6397353d40d09f0a02e126e" translate="yes" xml:space="preserve">
          <source>The French Motor Third-Party Liability Claims dataset</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="90c804ef552ac504772d87a0c90e2454ea50931f" translate="yes" xml:space="preserve">
          <source>The GP prior mean is assumed to be zero. The prior&amp;rsquo;s covariance is specified by a passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="translated">Предполагается, что априорное среднее значение GP равно нулю. Ковариация предшествующего уровня определяется передачей объекта &lt;a href=&quot;#gp-kernels&quot;&gt;ядра&lt;/a&gt; . Гиперпараметры ядра оптимизируются во время подгонки GaussianProcessRegressor путем максимизации предельного логарифмического правдоподобия (LML) на основе переданного &lt;code&gt;optimizer&lt;/code&gt; . Поскольку LML может иметь несколько локальных оптимизаторов, оптимизатор можно запускать повторно, указав &lt;code&gt;n_restarts_optimizer&lt;/code&gt; . Первый запуск всегда выполняется, начиная с начальных значений гиперпараметров ядра; последующие прогоны проводятся на основе значений гиперпараметров, выбранных случайным образом из диапазона допустимых значений. Если исходные гиперпараметры следует оставить фиксированными, &lt;code&gt;None&lt;/code&gt; можно передать в качестве оптимизатора.</target>
        </trans-unit>
        <trans-unit id="060746131ee7579fe50d723eccf0216e72cc0775" translate="yes" xml:space="preserve">
          <source>The GP prior mean is assumed to be zero. The prior&amp;rsquo;s covariance is specified by passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9ff401cc389e116b429c312d580f50e1ed79479" translate="yes" xml:space="preserve">
          <source>The Gini coefficient (based on the area under the curve) can be used as a model selection metric to quantify the ability of the model to rank policyholders. Note that this metric does not reflect the ability of the models to make accurate predictions in terms of absolute value of total claim amounts but only in terms of relative amounts as a ranking metric.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="18dcaaaea80b191d9100a9a4ae22f95aa636616a" translate="yes" xml:space="preserve">
          <source>The Gini index reflects the ability of a model to rank predictions irrespective of their absolute values, and therefore only assess their ranking power.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="01a974069deaad9f4a696951139fa6f180c4610d" translate="yes" xml:space="preserve">
          <source>The HLLE algorithm comprises three stages:</source>
          <target state="translated">Алгоритм HLLE состоит из трех этапов:</target>
        </trans-unit>
        <trans-unit id="eaec5120030b979edc1dcb8e844874e0db8c76f1" translate="yes" xml:space="preserve">
          <source>The Hamming loss is the fraction of labels that are incorrectly predicted.</source>
          <target state="translated">Потеря Хамминга-это часть этикеток,которые неправильно предсказаны.</target>
        </trans-unit>
        <trans-unit id="304ac7ce83417558cbe44e7723fbf3a93a036677" translate="yes" xml:space="preserve">
          <source>The Hamming loss is upperbounded by the subset zero-one loss, when &lt;code&gt;normalize&lt;/code&gt; parameter is set to True. It is always between 0 and 1, lower being better.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee89b1587a7c7b8d189cccf628a41e55c5d7ebe4" translate="yes" xml:space="preserve">
          <source>The Hamming loss is upperbounded by the subset zero-one loss. When normalized over samples, the Hamming loss is always between 0 and 1.</source>
          <target state="translated">Потери Хамминга превышают подмножество потерь &quot;ноль-один&quot;.При нормализации по образцам,потеря Хамминга всегда находится в пределах от 0 до 1.</target>
        </trans-unit>
        <trans-unit id="e3001f46869621e71f2b9c72a2ca6b3e38966a32" translate="yes" xml:space="preserve">
          <source>The Haversine (or great circle) distance is the angular distance between two points on the surface of a sphere. The first distance of each point is assumed to be the latitude, the second is the longitude, given in radians. The dimension of the data must be 2.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f575d5541e123fdf35ce0dd5c6fb4373f14dde3" translate="yes" xml:space="preserve">
          <source>The Huber Regressor optimizes the squared loss for the samples where &lt;code&gt;|(y - X'w) / sigma| &amp;lt; epsilon&lt;/code&gt; and the absolute loss for the samples where &lt;code&gt;|(y - X'w) / sigma| &amp;gt; epsilon&lt;/code&gt;, where w and sigma are parameters to be optimized. The parameter sigma makes sure that if y is scaled up or down by a certain factor, one does not need to rescale epsilon to achieve the same robustness. Note that this does not take into account the fact that the different features of X may be of different scales.</source>
          <target state="translated">Регрессор Хубера оптимизирует квадрат потерь для выборок, где &lt;code&gt;|(y - X'w) / sigma| &amp;lt; epsilon&lt;/code&gt; и абсолютные потери для образцов, где &lt;code&gt;|(y - X'w) / sigma| &amp;gt; epsilon&lt;/code&gt; , где w и sigma - параметры для оптимизации. Параметр sigma гарантирует, что если y масштабируется вверх или вниз на определенный коэффициент, нет необходимости изменять масштаб epsilon для достижения такой же надежности. Обратите внимание, что это не принимает во внимание тот факт, что разные функции X могут иметь разный масштаб.</target>
        </trans-unit>
        <trans-unit id="a428acf86562c566ebfc0a4a1d3e42c700a1a75f" translate="yes" xml:space="preserve">
          <source>The Huber and epsilon-insensitive loss functions can be used for robust regression. The width of the insensitive region has to be specified via the parameter &lt;code&gt;epsilon&lt;/code&gt;. This parameter depends on the scale of the target variables.</source>
          <target state="translated">Функции потерь, нечувствительные к Хуберу и эпсилону, можно использовать для надежной регрессии. Ширина нечувствительной области должна быть указана с помощью параметра &lt;code&gt;epsilon&lt;/code&gt; . Этот параметр зависит от масштаба целевых переменных.</target>
        </trans-unit>
        <trans-unit id="0a2150cee6da11610a14f68e745e5d0639a39459" translate="yes" xml:space="preserve">
          <source>The Iris Dataset</source>
          <target state="translated">Iris Dataset</target>
        </trans-unit>
        <trans-unit id="7c0ef25371cb6aecb434ff0290ad88905a1c2367" translate="yes" xml:space="preserve">
          <source>The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour and Virginica) with 4 attributes: sepal length, sepal width, petal length and petal width.</source>
          <target state="translated">Набор данных Iris представляет 3 вида цветов ирисы (сетуса,версикола и вирджиника)с 4 атрибутами:длина чашелистика,ширина чашелистика,длина лепестка и ширина лепестка.</target>
        </trans-unit>
        <trans-unit id="3877561d1fbee6aff8708ef4ec5fcabe6115833e" translate="yes" xml:space="preserve">
          <source>The IsolationForest &amp;lsquo;isolates&amp;rsquo; observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.</source>
          <target state="translated">IsolationForest &amp;laquo;изолирует&amp;raquo; наблюдения, случайным образом выбирая объект, а затем случайным образом выбирая значение разделения между максимальным и минимальным значениями выбранного объекта.</target>
        </trans-unit>
        <trans-unit id="9e47d533732129c7b308a9673031913c04e17afd" translate="yes" xml:space="preserve">
          <source>The Isomap algorithm comprises three stages:</source>
          <target state="translated">Алгоритм Isomap состоит из трех этапов:</target>
        </trans-unit>
        <trans-unit id="43f0627e56441244b0d44b83d4abbf8510d9eac0" translate="yes" xml:space="preserve">
          <source>The Jaccard index [1], or Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets, is used to compare set of predicted labels for a sample to the corresponding set of labels in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">Индекс Жаккара [1] или коэффициент сходства Жаккара, определяемый как размер пересечения, деленный на размер объединения двух наборов меток, используется для сравнения набора предсказанных меток для выборки с соответствующим набором меток в &lt;code&gt;y_true&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="54b0fb2f21ad6f516f16dd3562af3d3b867660fe" translate="yes" xml:space="preserve">
          <source>The Jaccard similarity coefficient of the \(i\)-th samples, with a ground truth label set \(y_i\) and predicted label set \(\hat{y}_i\), is defined as</source>
          <target state="translated">Коэффициент сходства Jaccard для образцов \(i\)-с набором меток &quot;грубой истины&quot; \(y_i\)и прогнозируемым набором меток \(\hat{y}_i\),определен как</target>
        </trans-unit>
        <trans-unit id="1b6ab133b67b3354e1eaee98fb0609deafb4a4e0" translate="yes" xml:space="preserve">
          <source>The Johnson-Lindenstrauss bound for embedding with random projections</source>
          <target state="translated">Джонсон-Линденштраус привязан для встраивания к случайным проекциям.</target>
        </trans-unit>
        <trans-unit id="e9607194f44934f99bc18e8a1d649dcabdd2e40a" translate="yes" xml:space="preserve">
          <source>The K-means algorithm aims to choose centroids that minimise the &lt;strong&gt;inertia&lt;/strong&gt;, or &lt;strong&gt;within-cluster sum-of-squares criterion&lt;/strong&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2e420c9d276fc6531e2d5a1fd3367ac4465f247c" translate="yes" xml:space="preserve">
          <source>The KDD Cup &amp;lsquo;99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the &lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting &amp;lsquo;abnormal&amp;rsquo; data, ie</source>
          <target state="translated">Набор данных KDD Cup '99 был создан путем обработки частей tcpdump из набора данных DARPA Intrusion Detection System (IDS) 1998 года, созданного лабораторией Линкольна Массачусетского технологического института [1]. Искусственные данные (описанные на &lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;домашней странице набора данных&lt;/a&gt; ) были сгенерированы с использованием закрытой сети и вручную введенных атак, чтобы произвести большое количество различных типов атак с нормальной активностью в фоновом режиме. Поскольку первоначальная цель заключалась в создании большого обучающего набора для алгоритмов контролируемого обучения, существует большая часть (80,1%) аномальных данных, которые нереальны в реальном мире и не подходят для неконтролируемого обнаружения аномалий, направленного на обнаружение &amp;laquo;аномальных&amp;raquo; данных. т.е.</target>
        </trans-unit>
        <trans-unit id="c580c78b1750c851ebf65b6b32d643554e25c44f" translate="yes" xml:space="preserve">
          <source>The KDD Cup &amp;lsquo;99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the &lt;a href=&quot;https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting &amp;lsquo;abnormal&amp;rsquo; data, ie</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="42488af2a88df53e7b3301c7d5a125afdc9cd2ce" translate="yes" xml:space="preserve">
          <source>The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will be minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different initializations will end up in local minima of the KL divergence. Hence, it is sometimes useful to try different seeds and select the embedding with the lowest KL divergence.</source>
          <target state="translated">Расхождение вероятностей соединения Куллбек-Лейблера (KL)в исходном пространстве и во встроенном пространстве будет сведено к минимуму за счет градиентного спуска.Обратите внимание,что расхождение KL не является выпуклым,т.е.многократные перезапуски с различными инициализациями закончатся в локальных минимумах расхождения KL.Следовательно,иногда полезно попробовать разные семена и выбрать встраивание с наименьшей расхождением KL.</target>
        </trans-unit>
        <trans-unit id="c1f022c5d8701b9b00fce305ae262139841dede3" translate="yes" xml:space="preserve">
          <source>The L2 regularization parameter. Use 0 for no regularization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b439efc6a95c460b62f48e9b2ed58ff5ae6d780" translate="yes" xml:space="preserve">
          <source>The L2 regularization parameter. Use &lt;code&gt;0&lt;/code&gt; for no regularization (default).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a35ab88c062292779ce626a30b50f45159613b2" translate="yes" xml:space="preserve">
          <source>The LARS model can be used using estimator &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt;&lt;code&gt;Lars&lt;/code&gt;&lt;/a&gt;, or its low-level implementation &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.linear_model.lars_path_gram#sklearn.linear_model.lars_path_gram&quot;&gt;&lt;code&gt;lars_path_gram&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c5e824f9a2e5664a81c2abf8d8de696419cc018" translate="yes" xml:space="preserve">
          <source>The LARS model can be used using estimator &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt;&lt;code&gt;Lars&lt;/code&gt;&lt;/a&gt;, or its low-level implementation &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Модель LARS может использоваться с использованием оценщика &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt; &lt;code&gt;Lars&lt;/code&gt; &lt;/a&gt; или его низкоуровневой реализации &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="5604e6549ba6e538c77a361942c228433b92a65e" translate="yes" xml:space="preserve">
          <source>The LTSA algorithm comprises three stages:</source>
          <target state="translated">Алгоритм LTSA состоит из трех этапов:</target>
        </trans-unit>
        <trans-unit id="5921a49032d4468242b223b9e118c35472eff0fa" translate="yes" xml:space="preserve">
          <source>The Lars algorithm provides the full path of the coefficients along the regularization parameter almost for free, thus a common operation consist of retrieving the path with function &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Алгоритм Ларса предоставляет полный путь коэффициентов по параметру регуляризации почти бесплатно, поэтому обычная операция состоит из получения пути с помощью функции &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="066ac8e9a006935363e41b727fa078b6cbe4a689" translate="yes" xml:space="preserve">
          <source>The Lars algorithm provides the full path of the coefficients along the regularization parameter almost for free, thus a common operation is to retrieve the path with one of the functions &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.linear_model.lars_path_gram#sklearn.linear_model.lars_path_gram&quot;&gt;&lt;code&gt;lars_path_gram&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9503ea7db44738c355b5bdf4d0264864cc9e2161" translate="yes" xml:space="preserve">
          <source>The Lasso is a linear model that estimates sparse coefficients with l1 regularization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f5aa687d48724082c940128def6bdb3f6066405" translate="yes" xml:space="preserve">
          <source>The Lasso optimization function varies for mono and multi-outputs.</source>
          <target state="translated">Функция оптимизации Лассо различается для моно-и мульти-выходов.</target>
        </trans-unit>
        <trans-unit id="3c1b2c3b1551762e6f1da2d8a29e9acf6404c123" translate="yes" xml:space="preserve">
          <source>The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where number of features is greater than number of samples. Elsewhere prefer cd which is more numerically stable.</source>
          <target state="translated">Использовать решатель Лассо:координатный спуск или ЛАРС.Используйте LARS для очень разреженных базовых графов,где количество элементов больше,чем количество сэмплов.В других местах предпочитают cd,более стабильный в числовом выражении.</target>
        </trans-unit>
        <trans-unit id="2de8b3228aef7c3b031e79cb56d858268d66f64e" translate="yes" xml:space="preserve">
          <source>The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where p &amp;gt; n. Elsewhere prefer cd which is more numerically stable.</source>
          <target state="translated">Используемый решатель лассо: координатный спуск или LARS. Используйте LARS для очень разреженных нижележащих графов, где p&amp;gt; n. В других местах предпочитаю компакт-диск, который численно более стабилен.</target>
        </trans-unit>
        <trans-unit id="59ce670bca31c79340517b84d85b37c8ca4886a2" translate="yes" xml:space="preserve">
          <source>The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt;&lt;code&gt;ledoit_wolf&lt;/code&gt;&lt;/a&gt; function of the &lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; package, or it can be otherwise obtained by fitting a &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt;&lt;code&gt;LedoitWolf&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c4c48b93f6a66f7991382ba54fcfe5fb18a6ebb" translate="yes" xml:space="preserve">
          <source>The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt;&lt;code&gt;ledoit_wolf&lt;/code&gt;&lt;/a&gt; function of the &lt;code&gt;sklearn.covariance&lt;/code&gt; package, or it can be otherwise obtained by fitting a &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt;&lt;code&gt;LedoitWolf&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">Оценщик Ледойта-Вольфа ковариационной матрицы может быть вычислен на выборке с &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt; &lt;code&gt;ledoit_wolf&lt;/code&gt; &lt;/a&gt; функции &lt;code&gt;sklearn.covariance&lt;/code&gt; пакета sklearn.covariance , или ее можно получить иным способом, &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt; &lt;code&gt;LedoitWolf&lt;/code&gt; &lt;/a&gt; объект LedoitWolf к той же выборке.</target>
        </trans-unit>
        <trans-unit id="aec2f6a4d0fb4e3ecc2ba56ea33e122b851a535b" translate="yes" xml:space="preserve">
          <source>The Linnerud dataset constains two small dataset:</source>
          <target state="translated">Набор данных Линнеруда содержит два маленьких набора данных:</target>
        </trans-unit>
        <trans-unit id="6be9aee52b8392e35ed4aeda9bdbad80150f0295" translate="yes" xml:space="preserve">
          <source>The Linnerud dataset is a multi-output regression dataset. It consists of three excercise (data) and three physiological (target) variables collected from twenty middle-aged men in a fitness club:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0460202c91e1eee28482597354a8366af4e5eb02" translate="yes" xml:space="preserve">
          <source>The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for novelty detection. Note that when LOF is used for novelty detection you MUST not use predict, decision_function and score_samples on the training set as this would lead to wrong results. You must only use these methods on new unseen data (which are not in the training set). See &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;User Guide&lt;/a&gt;: for details on the difference between outlier detection and novelty detection and how to use LOF for outlier detection.</source>
          <target state="translated">Алгоритм Local Outlier Factor (LOF) - это метод неконтролируемого обнаружения аномалий, который вычисляет локальное отклонение плотности данной точки данных по отношению к ее соседям. Он считает выбросами образцы, которые имеют значительно меньшую плотность, чем их соседи. В этом примере показано, как использовать LOF для обнаружения новизны. Обратите внимание: когда LOF используется для обнаружения новизны, вы НЕ ДОЛЖНЫ использовать в обучающем наборе предиктивную функцию, функцию решения и оценку_выборки, поскольку это приведет к неверным результатам. Вы должны использовать эти методы только для новых невидимых данных (которых нет в обучающем наборе). См. &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;Руководство пользователя&lt;/a&gt; : для получения подробной информации о разнице между обнаружением выбросов и обнаружением новизны, а также о том, как использовать LOF для обнаружения выбросов.</target>
        </trans-unit>
        <trans-unit id="d0e3831e71a419f2c6e3df80ad462189c4208703" translate="yes" xml:space="preserve">
          <source>The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for outlier detection which is the default use case of this estimator in scikit-learn. Note that when LOF is used for outlier detection it has no predict, decision_function and score_samples methods. See &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;User Guide&lt;/a&gt;: for details on the difference between outlier detection and novelty detection and how to use LOF for novelty detection.</source>
          <target state="translated">Алгоритм Local Outlier Factor (LOF) - это метод неконтролируемого обнаружения аномалий, который вычисляет локальное отклонение плотности данной точки данных по отношению к ее соседям. Он считает выбросами образцы, которые имеют значительно меньшую плотность, чем их соседи. В этом примере показано, как использовать LOF для обнаружения выбросов, что является вариантом использования этого средства оценки по умолчанию в scikit-learn. Обратите внимание на то, что когда LOF используется для обнаружения выбросов, у него нет методов прогнозирования, решения_функции и оценки_выборки. См. &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;Руководство пользователя&lt;/a&gt; : для получения подробной информации о разнице между обнаружением выбросов и обнаружением новизны, а также о том, как использовать LOF для обнаружения новизны.</target>
        </trans-unit>
        <trans-unit id="e334ea68d0fb9bc258e189ffac7524671f8600d5" translate="yes" xml:space="preserve">
          <source>The MLLE algorithm comprises three stages:</source>
          <target state="translated">Алгоритм MLLE состоит из трех этапов:</target>
        </trans-unit>
        <trans-unit id="514baa8efb372e193dd4209109d33c4d6c755f88" translate="yes" xml:space="preserve">
          <source>The Matplotlib Figure object.</source>
          <target state="translated">Объект Matplotlib Figure.</target>
        </trans-unit>
        <trans-unit id="5adf2471ff49dbf27acc1a8b4862b530e5b52366" translate="yes" xml:space="preserve">
          <source>The Matthews correlation coefficient (+1 represents a perfect prediction, 0 an average random prediction and -1 and inverse prediction).</source>
          <target state="translated">Коэффициент корреляции Мэттьюса (+1 представляет собой идеальное предсказание,0-среднее случайное предсказание и -1 и обратное предсказание).</target>
        </trans-unit>
        <trans-unit id="21efcbf7188af02e8a17818fd892c631c3cc436c" translate="yes" xml:space="preserve">
          <source>The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary and multiclass classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient. [source: Wikipedia]</source>
          <target state="translated">Коэффициент корреляции Мэттьюса используется в машинном обучении как мера качества бинарных и мультикласных классификаций.Он учитывает истинные и ложные положительные и отрицательные результаты и обычно рассматривается как сбалансированная мера,которая может быть использована даже в том случае,если классы имеют очень разные размеры.MCC,по сути,является значением коэффициента корреляции между -1 и +1.Коэффициент +1 представляет собой идеальное предсказание,0-среднее случайное предсказание и -1-обратное предсказание.Статистика также известна как коэффициент фи.[источник:Википедия]</target>
        </trans-unit>
        <trans-unit id="6faebd1cede47746805364be40ce28b059dea40d" translate="yes" xml:space="preserve">
          <source>The Mean Squared Error (in the sense of the Frobenius norm) between &lt;code&gt;self&lt;/code&gt; and &lt;code&gt;comp_cov&lt;/code&gt; covariance estimators.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3bca24cc7fb644916020d52b90a1ddb7136c5be" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant covariance estimator is to be applied on Gaussian-distributed data, but could still be relevant on data drawn from a unimodal, symmetric distribution. It is not meant to be used with multi-modal data (the algorithm used to fit a MinCovDet object is likely to fail in such a case). One should consider projection pursuit methods to deal with multi-modal datasets.</source>
          <target state="translated">Ковариационный оценщик минимальных ковариаций должен применяться к гауссово-распределенным данным,но все же может быть релевантен для данных,полученных из унимодального,симметричного распределения.Он не предназначен для использования с мультимодальными данными (алгоритм,используемый для подгонки объекта MinCovDet,скорее всего,в этом случае будет неудачным).Для работы с мультимодальными наборами данных следует рассмотреть методы поиска проекций.</target>
        </trans-unit>
        <trans-unit id="451133ac20cdcd95892f17968e1c348a4b0f4b8f" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">Оценка, определяющая минимальную ковариацию (MCD), была введена PJRousseuw в &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="e452597962db33c03eccea44c483adb03cd8dfa7" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in &lt;a href=&quot;#id6&quot; id=&quot;id3&quot;&gt;3&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d811e159b509e69af06fa8b1a6b421d8a9ea40ca" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in [1].</source>
          <target state="translated">Оценка минимального ковариационного детерминанта (MCD)была введена П.Ж.Руссо в [1].</target>
        </trans-unit>
        <trans-unit id="ecfb18631be5aab9ee26a60642b369e5e4e48a3b" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust estimator of a data set&amp;rsquo;s covariance introduced by P.J. Rousseeuw in &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;3&lt;/a&gt;. The idea is to find a given proportion (h) of &amp;ldquo;good&amp;rdquo; observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations (&amp;ldquo;consistency step&amp;rdquo;). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set (&amp;ldquo;reweighting step&amp;rdquo;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="92f2af418b12a7a58727045874e736340d33efbd" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust estimator of a data set&amp;rsquo;s covariance introduced by P.J. Rousseeuw in &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;[3]&lt;/a&gt;. The idea is to find a given proportion (h) of &amp;ldquo;good&amp;rdquo; observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations (&amp;ldquo;consistency step&amp;rdquo;). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set (&amp;ldquo;reweighting step&amp;rdquo;).</source>
          <target state="translated">Оценщик, определяющий минимальную ковариацию, - это надежный оценщик ковариации набора данных, представленный П. Дж. Руссеу в &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;[3]&lt;/a&gt; . Идея состоит в том, чтобы найти заданную пропорцию (h) &amp;laquo;хороших&amp;raquo; наблюдений, которые не являются выбросами, и вычислить их эмпирическую ковариационную матрицу. Затем эту эмпирическую ковариационную матрицу масштабируют, чтобы компенсировать выполненный выбор наблюдений (&amp;laquo;этап согласованности&amp;raquo;). Вычислив оценку определителя минимальной ковариации, можно присвоить веса наблюдениям в соответствии с их расстоянием Махаланобиса, что приведет к повторной оценке ковариационной матрицы набора данных (&amp;laquo;этап повторного взвешивания&amp;raquo;).</target>
        </trans-unit>
        <trans-unit id="26a5ec87ffc45e2d236a64ca891cc8c307910b23" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to \(\frac{n_\text{samples} - n_\text{features}-1}{2}\) outliers) estimator of covariance. The idea is to find \(\frac{n_\text{samples} + n_\text{features}+1}{2}\) observations whose empirical covariance has the smallest determinant, yielding a &amp;ldquo;pure&amp;rdquo; subset of observations from which to compute standards estimates of location and covariance. After a correction step aiming at compensating the fact that the estimates were learned from only a portion of the initial data, we end up with robust estimates of the data set location and covariance.</source>
          <target state="translated">Оценщик, определяющий минимальную ковариацию, является надежной точкой с высокой степенью разбивки (т. Е. Его можно использовать для оценки ковариационной матрицы сильно загрязненных наборов данных до \ (\ frac {n_ \ text {samples} - n_ \ text {features} - 1} {2} \) выбросы) оценка ковариации. Идея состоит в том, чтобы найти \ (\ frac {n_ \ text {samples} + n_ \ text {features} +1} {2} \) наблюдения, эмпирическая ковариация которых имеет наименьший детерминант, и получить &amp;laquo;чистое&amp;raquo; подмножество наблюдений, из которых для вычисления стандартных оценок местоположения и ковариации. После этапа коррекции, направленного на компенсацию того факта, что оценки были получены только из части исходных данных, мы получаем надежные оценки местоположения набора данных и ковариации.</target>
        </trans-unit>
        <trans-unit id="868c6b821132444289ad5893f70525225594997c" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to \(\frac{n_\text{samples}-n_\text{features}-1}{2}\) outliers) estimator of covariance. The idea is to find \(\frac{n_\text{samples}+n_\text{features}+1}{2}\) observations whose empirical covariance has the smallest determinant, yielding a &amp;ldquo;pure&amp;rdquo; subset of observations from which to compute standards estimates of location and covariance.</source>
          <target state="translated">Оценщик, определяющий минимальную ковариацию, является надежной точкой с высокой степенью разбивки (т. Е. Его можно использовать для оценки ковариационной матрицы сильно загрязненных наборов данных, вплоть до \ (\ frac {n_ \ text {samples} -n_ \ text {features} - 1} {2} \) выбросы) оценка ковариации. Идея состоит в том, чтобы найти \ (\ frac {n_ \ text {samples} + n_ \ text {features} +1} {2} \) наблюдения, эмпирическая ковариация которых имеет наименьший детерминант, и получить &amp;laquo;чистое&amp;raquo; подмножество наблюдений, из которых для вычисления стандартных оценок местоположения и ковариации.</target>
        </trans-unit>
        <trans-unit id="b2ce758bd900de6631654a32b9b8962161ffb45f" translate="yes" xml:space="preserve">
          <source>The Mutual Information is a measure of the similarity between two labels of the same data. Where \(|U_i|\) is the number of the samples in cluster \(U_i\) and \(|V_j|\) is the number of the samples in cluster \(V_j\), the Mutual Information between clusterings \(U\) and \(V\) is given as:</source>
          <target state="translated">Взаимная информация является мерой сходства между двумя этикетками одних и тех же данных.Там,где \(|U__i|\)-количество образцов в кластере \(U_i\)и \(|Vj|\)-количество образцов в кластере \(V_j\),&quot;Взаимная информация&quot; между кластерами \(U\)и \(V\)дается как &quot;Взаимная информация&quot;:</target>
        </trans-unit>
        <trans-unit id="4607a569600ef039f2071e80730b55efccd8dd8d" translate="yes" xml:space="preserve">
          <source>The Nystroem method, as implemented in &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is a general method for low-rank approximations of kernels. It achieves this by essentially subsampling the data on which the kernel is evaluated. By default &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; uses the &lt;code&gt;rbf&lt;/code&gt; kernel, but it can use any kernel function or a precomputed kernel matrix. The number of samples used - which is also the dimensionality of the features computed - is given by the parameter &lt;code&gt;n_components&lt;/code&gt;.</source>
          <target state="translated">Метод Nystroem, реализованный в &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; ,&lt;/a&gt; является общим методом для низкоранговых приближений ядер. Это достигается за счет субдискретизации данных, на которых оценивается ядро. По умолчанию &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt; использует ядро &lt;code&gt;rbf&lt;/code&gt; , но может использовать любую функцию ядра или предварительно вычисленную матрицу ядра. Количество используемых выборок, которое также является размерностью вычисленных характеристик, задается параметром &lt;code&gt;n_components&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="acd97cd02b3cf6e9137b2a9dda0c8e62c31c52e4" translate="yes" xml:space="preserve">
          <source>The OAS estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt;&lt;code&gt;oas&lt;/code&gt;&lt;/a&gt; function of the &lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; package, or it can be otherwise obtained by fitting an &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;OAS&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad47de32445041d6ef99f77621d867a03cd9751f" translate="yes" xml:space="preserve">
          <source>The OAS estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt;&lt;code&gt;oas&lt;/code&gt;&lt;/a&gt; function of the &lt;code&gt;sklearn.covariance&lt;/code&gt; package, or it can be otherwise obtained by fitting an &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;OAS&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">ОАГ оценка ковариационной матрицы может быть вычислена на образце с &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt; &lt;code&gt;oas&lt;/code&gt; &lt;/a&gt; функции &lt;code&gt;sklearn.covariance&lt;/code&gt; пакета, или он может быть получен иным путем установки &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt; &lt;code&gt;OAS&lt;/code&gt; &lt;/a&gt; объекта в том же образец.</target>
        </trans-unit>
        <trans-unit id="6f45a4e82bc4c1b489f40318ecca4028acd0f46e" translate="yes" xml:space="preserve">
          <source>The One-Class SVM has been introduced by Sch&amp;ouml;lkopf et al. for that purpose and implemented in the &lt;a href=&quot;svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; module in the &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; object. It requires the choice of a kernel and a scalar parameter to define a frontier. The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter. This is the default in the scikit-learn implementation. The \(\nu\) parameter, also known as the margin of the One-Class SVM, corresponds to the probability of finding a new, but regular, observation outside the frontier.</source>
          <target state="translated">Одноклассная SVM была представлена ​​Sch&amp;ouml;lkopf et al. для этой цели и реализован в модуле &lt;a href=&quot;svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; в объекте &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;svm.OneClassSVM&lt;/code&gt; &lt;/a&gt; . Для определения границы требуется выбор ядра и скалярного параметра. Обычно выбирается ядро ​​RBF, хотя не существует точной формулы или алгоритма для установки его параметра пропускной способности. Это значение по умолчанию в реализации scikit-learn. Параметр \ (\ nu \), также известный как маржа одноклассовой SVM, соответствует вероятности обнаружения нового, но регулярного наблюдения за пределами границы.</target>
        </trans-unit>
        <trans-unit id="ce00ae4f245475e7026810e89464783863843edd" translate="yes" xml:space="preserve">
          <source>The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction.</source>
          <target state="translated">PCA делает неконтролируемое уменьшение размерности,в то время как логистическая регрессия делает предсказание.</target>
        </trans-unit>
        <trans-unit id="04f0f7d7b53f41cd954be6291735d195a21f52b7" translate="yes" xml:space="preserve">
          <source>The Poisson deviance cannot be computed on non-positive values predicted by the model. For models that do return a few non-positive predictions (e.g. &lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;) we ignore the corresponding samples, meaning that the obtained Poisson deviance is approximate. An alternative approach could be to use &lt;a href=&quot;../../modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor&quot;&gt;&lt;code&gt;TransformedTargetRegressor&lt;/code&gt;&lt;/a&gt; meta-estimator to map &lt;code&gt;y_pred&lt;/code&gt; to a strictly positive domain.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bc5a34aa26f7428f3e35a346a70f3689822fe771" translate="yes" xml:space="preserve">
          <source>The Poisson deviance computed as an evaluation metric reflects both the calibration and the ranking power of the model. It also makes a linear assumption on the ideal relationship between the expected value and the variance of the response variable. For the sake of conciseness we did not check whether this assumption holds.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="450889f232106f285e0f6ca8296ef879dd035563" translate="yes" xml:space="preserve">
          <source>The Probability Density Functions (PDF) of these distributions are illustrated in the following figure,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="935532caca213849eac9af7588fc52963397d517" translate="yes" xml:space="preserve">
          <source>The R2 score used when calling &lt;code&gt;score&lt;/code&gt; on a regressor uses &lt;code&gt;multioutput='uniform_average'&lt;/code&gt; from version 0.23 to keep consistent with default value of &lt;a href=&quot;sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt;. This influences the &lt;code&gt;score&lt;/code&gt; method of all the multioutput regressors (except for &lt;a href=&quot;#sklearn.multioutput.MultiOutputRegressor&quot;&gt;&lt;code&gt;MultiOutputRegressor&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11e4fa9675506dfc6e48ea958dcc499e9784ad90" translate="yes" xml:space="preserve">
          <source>The R2 score used when calling &lt;code&gt;score&lt;/code&gt; on a regressor uses &lt;code&gt;multioutput='uniform_average'&lt;/code&gt; from version 0.23 to keep consistent with default value of &lt;a href=&quot;sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt;. This influences the &lt;code&gt;score&lt;/code&gt; method of all the multioutput regressors (except for &lt;a href=&quot;sklearn.multioutput.multioutputregressor#sklearn.multioutput.MultiOutputRegressor&quot;&gt;&lt;code&gt;MultiOutputRegressor&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a83459e6ef7baec271df4e0325c8da4e8711242a" translate="yes" xml:space="preserve">
          <source>The RBF kernel is a stationary kernel. It is also known as the &amp;ldquo;squared exponential&amp;rdquo; kernel. It is parameterized by a length scale parameter \(l&amp;gt;0\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="33c279bdcb922f55f225b113a682e1a5cf28cc6d" translate="yes" xml:space="preserve">
          <source>The RBF kernel is a stationary kernel. It is also known as the &amp;ldquo;squared exponential&amp;rdquo; kernel. It is parameterized by a length-scale parameter length_scale&amp;gt;0, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">Ядро RBF - это стационарное ядро. Это также известно как &amp;laquo;квадрат экспоненциального ядра&amp;raquo;. Он параметризуется параметром масштаба длины length_scale&amp;gt; 0, который может быть либо скаляром (изотропный вариант ядра), либо вектором с тем же числом измерений, что и входные данные X (анизотропный вариант ядра). Ядро выдается:</target>
        </trans-unit>
        <trans-unit id="f47997c4010f2e20accd04690970d6547a71bfc2" translate="yes" xml:space="preserve">
          <source>The RBF kernel will produce a fully connected graph which is represented in memory by a dense matrix. This matrix may be very large and combined with the cost of performing a full matrix multiplication calculation for each iteration of the algorithm can lead to prohibitively long running times. On the other hand, the KNN kernel will produce a much more memory-friendly sparse matrix which can drastically reduce running times.</source>
          <target state="translated">Ядро RBF создаст полностью подключенный график,который будет представлен в памяти плотной матрицей.Эта матрица может быть очень большой и в сочетании со стоимостью выполнения полного расчета умножения матриц для каждой итерации алгоритма может привести к недопустимо длительному времени работы.С другой стороны,кернел KNN создаст гораздо более дружественную к памяти разреженную матрицу,что может резко сократить время работы.</target>
        </trans-unit>
        <trans-unit id="512de356729b7d551b43d5d76bd8681dea941e19" translate="yes" xml:space="preserve">
          <source>The RBM tries to maximize the likelihood of the data using a particular graphical model. The parameter learning algorithm used (&lt;a href=&quot;#sml&quot;&gt;Stochastic Maximum Likelihood&lt;/a&gt;) prevents the representations from straying far from the input data, which makes them capture interesting regularities, but makes the model less useful for small datasets, and usually not useful for density estimation.</source>
          <target state="translated">RBM пытается максимизировать вероятность данных с помощью конкретной графической модели. Используемый алгоритм обучения параметрам ( &lt;a href=&quot;#sml&quot;&gt;стохастический максимум правдоподобия&lt;/a&gt; ) предотвращает отклонение представлений от входных данных, что заставляет их фиксировать интересные закономерности, но делает модель менее полезной для небольших наборов данных и, как правило, бесполезной для оценки плотности.</target>
        </trans-unit>
        <trans-unit id="95d48012cfd5cdfda48a80ad45de8292db32665c" translate="yes" xml:space="preserve">
          <source>The R^2 score or ndarray of scores if &amp;lsquo;multioutput&amp;rsquo; is &amp;lsquo;raw_values&amp;rsquo;.</source>
          <target state="translated">Оценка R ^ 2 или ndarray оценок, если 'multioutput' равен 'raw_values'.</target>
        </trans-unit>
        <trans-unit id="57cf85e67b74d4b36ce4e00a60cc2afb37409d42" translate="yes" xml:space="preserve">
          <source>The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.</source>
          <target state="translated">Индекс Рэнда рассчитывает меру сходства между двумя кластерингами,рассматривая все пары выборок и подсчитывая пары,которые приписываются одним и тем же или разным кластерам в предсказанных и истинных кластерингах.</target>
        </trans-unit>
        <trans-unit id="21d3f8892ca41bf337636e90759152be22d9788c" translate="yes" xml:space="preserve">
          <source>The RandomTreesEmbedding, from the &lt;a href=&quot;../../modules/classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module, is not technically a manifold embedding method, as it learn a high-dimensional representation on which we apply a dimensionality reduction method. However, it is often useful to cast a dataset into a representation in which the classes are linearly-separable.</source>
          <target state="translated">RandomTreesEmbedding из модуля &lt;a href=&quot;../../modules/classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt; технически не является методом встраивания многообразия, так как он изучает многомерное представление, к которому мы применяем метод уменьшения размерности. Однако часто бывает полезно преобразовать набор данных в представление, в котором классы линейно разделимы.</target>
        </trans-unit>
        <trans-unit id="9863f2af9163cbec042d3a8db1d9d0da7a79bddf" translate="yes" xml:space="preserve">
          <source>The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length scales. It is parameterized by a length scale parameter \(l&amp;gt;0\) and a scale mixture parameter \(\alpha&amp;gt;0\). Only the isotropic variant where length_scale \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ddaea903844387569c2d558e504bb67163af0e53" translate="yes" xml:space="preserve">
          <source>The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length-scales. It is parameterized by a length-scale parameter length_scale&amp;gt;0 and a scale mixture parameter alpha&amp;gt;0. Only the isotropic variant where length_scale is a scalar is supported at the moment. The kernel given by:</source>
          <target state="translated">Ядро RationalQuadratic можно рассматривать как смесь масштабов (бесконечную сумму) ядер RBF с различными характерными масштабами длины. Он параметризуется параметром масштаба длины length_scale&amp;gt; 0 и параметром смеси масштабов alpha&amp;gt; 0. На данный момент поддерживается только изотропный вариант, в котором length_scale является скаляром. Ядро, данное:</target>
        </trans-unit>
        <trans-unit id="4290d451a4bc3d8974b88b0877f706f5c5e0d4c1" translate="yes" xml:space="preserve">
          <source>The SAGA solver supports both float64 and float32 bit arrays.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66cebb865fc92b18216814d7cca47981b6cb6b41" translate="yes" xml:space="preserve">
          <source>The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a multidimensional scaling algorithm which minimizes an objective function (the &lt;em&gt;stress&lt;/em&gt;) using a majorization technique. Stress majorization, also known as the Guttman Transform, guarantees a monotone convergence of stress, and is more powerful than traditional techniques such as gradient descent.</source>
          <target state="translated">Алгоритм SMACOF (масштабирование путем мажорирования сопряженной функции) - это алгоритм многомерного масштабирования, который минимизирует целевую функцию ( &lt;em&gt;напряжение&lt;/em&gt; ) с использованием техники мажорирования. Мажоризация стресса, также известная как преобразование Гуттмана, гарантирует монотонную конвергенцию стресса и более эффективна, чем традиционные методы, такие как градиентный спуск.</target>
        </trans-unit>
        <trans-unit id="e2c4971923aff3f7fe2f8678fb678a8a1fe12716" translate="yes" xml:space="preserve">
          <source>The SMACOF algorithm for metric MDS can summarized by the following steps:</source>
          <target state="translated">Алгоритм SMACOF для метрических MDS можно обобщить следующим образом:</target>
        </trans-unit>
        <trans-unit id="9991accadfafce42fd0097ac5bf1452a92166503" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient &lt;em&gt;s&lt;/em&gt; for a single sample is then given as:</source>
          <target state="translated">Тогда силуэтный коэффициент &lt;em&gt;s&lt;/em&gt; для одного образца определяется как:</target>
        </trans-unit>
        <trans-unit id="977018d27e576be441891586d29895a67820e9bb" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample.</source>
          <target state="translated">Коэффициент силуэта для набора образцов приводится как среднее значение коэффициента силуэта для каждого образца.</target>
        </trans-unit>
        <trans-unit id="a8aa62253fcb64807b1a0f2a014811a76f4c09cc" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is a measure of how well samples are clustered with samples that are similar to themselves. Clustering models with a high Silhouette Coefficient are said to be dense, where samples in the same cluster are similar to each other, and well separated, where samples in different clusters are not very similar to each other.</source>
          <target state="translated">Коэффициент силуэта-это мера того,как образцы скважин сгруппированы с образцами,похожими на них самих.Считается,что кластерные модели с высоким коэффициентом силуэта плотны,где образцы в одном и том же кластере похожи друг на друга,и хорошо разделены,где образцы в разных кластерах не очень похожи друг на друга.</target>
        </trans-unit>
        <trans-unit id="5da8604230381cdf66d3aec27ea08ace466e606c" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is calculated using the mean intra-cluster distance (&lt;code&gt;a&lt;/code&gt;) and the mean nearest-cluster distance (&lt;code&gt;b&lt;/code&gt;) for each sample. The Silhouette Coefficient for a sample is &lt;code&gt;(b - a) / max(a,
b)&lt;/code&gt;. Note that Silhouette Coefficient is only defined if number of labels is 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</source>
          <target state="translated">Коэффициент силуэта рассчитывается с использованием среднего расстояния внутри кластера ( &lt;code&gt;a&lt;/code&gt; ) и среднего расстояния до ближайшего кластера ( &lt;code&gt;b&lt;/code&gt; ) для каждого образца. Коэффициент силуэта для образца равен &lt;code&gt;(b - a) / max(a, b)&lt;/code&gt; . Обратите внимание, что коэффициент силуэта определяется только в том случае, если количество меток равно 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</target>
        </trans-unit>
        <trans-unit id="4273a368cac49099b8caa7c819b214a9d85c0ff4" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is calculated using the mean intra-cluster distance (&lt;code&gt;a&lt;/code&gt;) and the mean nearest-cluster distance (&lt;code&gt;b&lt;/code&gt;) for each sample. The Silhouette Coefficient for a sample is &lt;code&gt;(b - a) / max(a,
b)&lt;/code&gt;. To clarify, &lt;code&gt;b&lt;/code&gt; is the distance between a sample and the nearest cluster that the sample is not a part of. Note that Silhouette Coefficient is only defined if number of labels is 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</source>
          <target state="translated">Коэффициент силуэта рассчитывается с использованием среднего расстояния внутри кластера ( &lt;code&gt;a&lt;/code&gt; ) и среднего расстояния до ближайшего кластера ( &lt;code&gt;b&lt;/code&gt; ) для каждого образца. Коэффициент силуэта для образца равен &lt;code&gt;(b - a) / max(a, b)&lt;/code&gt; . Чтобы уточнить, &lt;code&gt;b&lt;/code&gt; - это расстояние между образцом и ближайшим кластером, частью которого образец не является. Обратите внимание, что коэффициент силуэта определяется только в том случае, если количество меток равно 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</target>
        </trans-unit>
        <trans-unit id="74a577cf3f9facf0caa36a3cd46ce91a25197ef1" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">Коэффициент силуэта,как правило,выше для выпуклых кластеров,чем у других концепций кластеров,например,кластеров на основе плотности,подобных тем,которые получены с помощью DBSCAN.</target>
        </trans-unit>
        <trans-unit id="b08373a116f8a28b8f59b4a939f8e86bc18a285c" translate="yes" xml:space="preserve">
          <source>The Spearman correlation coefficient is estimated from the data, and the sign of the resulting estimate is used as the result.</source>
          <target state="translated">Коэффициент корреляции Спирмена оценивается по данным,в качестве результата используется признак результирующей оценки.</target>
        </trans-unit>
        <trans-unit id="1aca461bec942a3ea49c28ca350e80c03610ed92" translate="yes" xml:space="preserve">
          <source>The Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages:</source>
          <target state="translated">Алгоритм спектрального встраивания (Laplacian Eigenmaps)состоит из трех этапов:</target>
        </trans-unit>
        <trans-unit id="5c2cf1989e094ea2eafd60dd4bfcc701bf5b4819" translate="yes" xml:space="preserve">
          <source>The Stack Exchange family of sites hosts &lt;a href=&quot;https://meta.stackexchange.com/q/130524&quot;&gt;multiple subdomains for Machine Learning questions&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="530048073ab2f0fd4beae1a41150629fce3bbc04" translate="yes" xml:space="preserve">
          <source>The TF-IDF vectorized posts form a word frequency matrix, which is then biclustered using Dhillon&amp;rsquo;s Spectral Co-Clustering algorithm. The resulting document-word biclusters indicate subsets words used more often in those subsets documents.</source>
          <target state="translated">Векторизованные сообщения TF-IDF образуют матрицу частот слов, которая затем бикластеризуется с использованием алгоритма спектральной совместной кластеризации Диллона. Полученные в результате бикластеры слов документа указывают на подмножества слов, которые чаще используются в этих подмножествах документов.</target>
        </trans-unit>
        <trans-unit id="a51b4bd7337a8a43bf76bb00c70355d636e98cf2" translate="yes" xml:space="preserve">
          <source>The V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean &lt;a href=&quot;#b2011&quot; id=&quot;id15&quot;&gt;[B2011]&lt;/a&gt;.</source>
          <target state="translated">V-мера фактически эквивалентна описанной выше взаимной информации (NMI), при этом функция агрегирования является средним арифметическим &lt;a href=&quot;#b2011&quot; id=&quot;id15&quot;&gt;[B2011]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="4af22841a0f64abdacec5694995ece03a0f97488" translate="yes" xml:space="preserve">
          <source>The V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean &lt;a href=&quot;#b2011&quot; id=&quot;id16&quot;&gt;[B2011]&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="28e22a992327910c0281c7997fd0381a055b2da5" translate="yes" xml:space="preserve">
          <source>The V-measure is the harmonic mean between homogeneity and completeness:</source>
          <target state="translated">V-измерение-это гармоническое среднее между однородностью и полнотой:</target>
        </trans-unit>
        <trans-unit id="8e9e469c4bec48fbecca2280bcc57b2302181e26" translate="yes" xml:space="preserve">
          <source>The WAGE is increasing when EDUCATION is increasing. Note that the dependence between WAGE and EDUCATION represented here is a marginal dependence, i.e., it describes the behavior of a specific variable without keeping the others fixed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9d831d6fbe54cab7c78889ea027eda02af846327" translate="yes" xml:space="preserve">
          <source>The Yeo-Johnson transform is given by:</source>
          <target state="translated">Преобразование Ё-Джонсона дано:</target>
        </trans-unit>
        <trans-unit id="dd310b3ef8cade9b7b44463cf24d9960d1a07902" translate="yes" xml:space="preserve">
          <source>The \(\ell = \lceil \log_2 k \rceil\) singular vectors, starting from the second, provide the desired partitioning information. They are used to form the matrix \(Z\):</source>
          <target state="translated">\(\ell=\lceil \log_2 k \rceil\)единичные векторы,начиная со второго,предоставляют желаемую информацию о разметке.Они используются для формирования матрицы \(Z\):</target>
        </trans-unit>
        <trans-unit id="0151312e57232e026343f2a42d71b34eecec6ec6" translate="yes" xml:space="preserve">
          <source>The \(\nu\)-SVC formulation &lt;a href=&quot;#id17&quot; id=&quot;id8&quot;&gt;15&lt;/a&gt; is a reparameterization of the \(C\)-SVC and therefore mathematically equivalent.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6bae7eee8e57958cb7a015c09e85982ab959fe35" translate="yes" xml:space="preserve">
          <source>The \(k\)-neighbors classification in &lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt; is the most commonly used technique. The optimal choice of the value \(k\) is highly data-dependent: in general a larger \(k\) suppresses the effects of noise, but makes the classification boundaries less distinct.</source>
          <target state="translated">Классификация \ (k \) - соседей в &lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt; &lt;code&gt;KNeighborsClassifier&lt;/code&gt; &lt;/a&gt; является наиболее часто используемым методом. Оптимальный выбор значения \ (k \) сильно зависит от данных: как правило, большее значение \ (k \) подавляет влияние шума, но делает границы классификации менее четкими.</target>
        </trans-unit>
        <trans-unit id="e8862ed89c87547157e194d96a8f490026b2e0fb" translate="yes" xml:space="preserve">
          <source>The ability to reproduce the data of the regularized model is similar to the one of the non-regularized model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ea48a47287b56e3f66c9ec81d51c291e8fedf59" translate="yes" xml:space="preserve">
          <source>The above vectorization scheme is simple but the fact that it holds an &lt;strong&gt;in- memory mapping from the string tokens to the integer feature indices&lt;/strong&gt; (the &lt;code&gt;vocabulary_&lt;/code&gt; attribute) causes several &lt;strong&gt;problems when dealing with large datasets&lt;/strong&gt;:</source>
          <target state="translated">Вышеупомянутая схема векторизации проста, но тот факт, что она содержит &lt;strong&gt;отображение в памяти строковых токенов на целочисленные индексы объектов&lt;/strong&gt; ( атрибут &lt;code&gt;vocabulary_&lt;/code&gt; ), вызывает несколько &lt;strong&gt;проблем при работе с большими наборами данных&lt;/strong&gt; :</target>
        </trans-unit>
        <trans-unit id="f78fe42a387595f69f94294d9712aa8fff9cf7ee" translate="yes" xml:space="preserve">
          <source>The absolute tolerance to use when comparing scores during early stopping. The higher the tolerance, the more likely we are to early stop: higher tolerance means that it will be harder for subsequent iterations to be considered an improvement upon the reference score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a971e24181d9be44e4bb51f3963013ec9c9d01d5" translate="yes" xml:space="preserve">
          <source>The absolute tolerance to use when comparing scores. The higher the tolerance, the more likely we are to early stop: higher tolerance means that it will be harder for subsequent iterations to be considered an improvement upon the reference score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58279b870becff96bab0fc34e17d7f045af03b34" translate="yes" xml:space="preserve">
          <source>The abstract base class for all kernels is &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt;. Kernel implements a similar interface as &lt;code&gt;Estimator&lt;/code&gt;, providing the methods &lt;code&gt;get_params()&lt;/code&gt;, &lt;code&gt;set_params()&lt;/code&gt;, and &lt;code&gt;clone()&lt;/code&gt;. This allows setting kernel values also via meta-estimators such as &lt;code&gt;Pipeline&lt;/code&gt; or &lt;code&gt;GridSearch&lt;/code&gt;. Note that due to the nested structure of kernels (by applying kernel operators, see below), the names of kernel parameters might become relatively complicated. In general, for a binary kernel operator, parameters of the left operand are prefixed with &lt;code&gt;k1__&lt;/code&gt; and parameters of the right operand with &lt;code&gt;k2__&lt;/code&gt;. An additional convenience method is &lt;code&gt;clone_with_theta(theta)&lt;/code&gt;, which returns a cloned version of the kernel but with the hyperparameters set to &lt;code&gt;theta&lt;/code&gt;. An illustrative example:</source>
          <target state="translated">Абстрактный базовый класс для всех ядер - &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt; . Ядро реализует аналогичный интерфейс, что и &lt;code&gt;Estimator&lt;/code&gt; , предоставляя методы &lt;code&gt;get_params()&lt;/code&gt; , &lt;code&gt;set_params()&lt;/code&gt; и &lt;code&gt;clone()&lt;/code&gt; . Это позволяет устанавливать значения ядра также с помощью мета-оценок, таких как &lt;code&gt;Pipeline&lt;/code&gt; или &lt;code&gt;GridSearch&lt;/code&gt; . Обратите внимание, что из-за вложенной структуры ядер (с применением операторов ядра, см. Ниже) имена параметров ядра могут стать относительно сложными. В общем, для бинарного оператора ядра параметры левого операнда имеют префикс &lt;code&gt;k1__&lt;/code&gt; , а параметры правого операнда - &lt;code&gt;k2__&lt;/code&gt; . Дополнительный метод удобства - &lt;code&gt;clone_with_theta(theta)&lt;/code&gt; , который возвращает клонированную версию ядра, но с гиперпараметрами, установленными на &lt;code&gt;theta&lt;/code&gt; . Наглядный пример:</target>
        </trans-unit>
        <trans-unit id="d6dfb2a865fc7561c9990f60831354c8c2b2acf1" translate="yes" xml:space="preserve">
          <source>The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: &amp;ldquo;Robust Linear Programming Discrimination of Two Linearly Inseparable Sets&amp;rdquo;, Optimization Methods and Software 1, 1992, 23-34].</source>
          <target state="translated">Фактическая линейная программа, используемая для получения разделяющей плоскости в трехмерном пространстве, описана в: [KP Bennett and OL Mangasarian: &amp;laquo;Robust Linear Programming Discrimination of Two Linearly Inseparable Sets&amp;raquo;, Optimization Methods and Software 1, 1992, 23- 34].</target>
        </trans-unit>
        <trans-unit id="5c16c787f9e7f87c21ce4f86533b13082a6022ce" translate="yes" xml:space="preserve">
          <source>The actual number of iteration performed by the solver. Only returned if &lt;code&gt;return_n_iter&lt;/code&gt; is True.</source>
          <target state="translated">Фактическое количество итераций, выполненных решателем. Возвращается, только если &lt;code&gt;return_n_iter&lt;/code&gt; имеет значение True.</target>
        </trans-unit>
        <trans-unit id="9cc109786e99b4c544e4621619329291c5062f5c" translate="yes" xml:space="preserve">
          <source>The actual number of iterations before reaching the stopping criterion.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68fff8e45d31e6c0edd22bde7c766ffe083896ee" translate="yes" xml:space="preserve">
          <source>The actual number of iterations before reaching the stopping criterion. For multiclass fits, it is the maximum over every binary fit.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c6a597b70b1c3dba7fc91a33b2b458a1718ae376" translate="yes" xml:space="preserve">
          <source>The actual number of iterations to reach the stopping criterion.</source>
          <target state="translated">Фактическое количество итераций для достижения критерия остановки.</target>
        </trans-unit>
        <trans-unit id="be3e6ed927427ff958a3dc691ec6f2ca38b153e8" translate="yes" xml:space="preserve">
          <source>The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over every binary fit.</source>
          <target state="translated">Фактическое количество итераций для достижения критерия остановки.Для подгонки по нескольким классам это максимальное значение над каждым бинарным подгоном.</target>
        </trans-unit>
        <trans-unit id="905122fa4ae495ec172d69201d288d9cbd8eb107" translate="yes" xml:space="preserve">
          <source>The actual number of neighbors used for &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt;&lt;code&gt;kneighbors&lt;/code&gt;&lt;/a&gt; queries.</source>
          <target state="translated">Фактическое количество соседей, используемых для запросов о &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt; &lt;code&gt;kneighbors&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="d1e2217f69f6ffa7102e10d2dcd6eed0b5a03524" translate="yes" xml:space="preserve">
          <source>The actual number of quantiles used to discretize the cumulative distribution function.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99f948dd8ff8ebdf8ccc1268f27f992c4d54a1e0" translate="yes" xml:space="preserve">
          <source>The actual number of samples</source>
          <target state="translated">Фактическое количество образцов</target>
        </trans-unit>
        <trans-unit id="83f4574f37ccf239bd98ad8929c323f413ece25a" translate="yes" xml:space="preserve">
          <source>The actual number of samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="257683b273bf01e4ad40af7f18f19c426c837478" translate="yes" xml:space="preserve">
          <source>The additive chi squared kernel as used here is given by</source>
          <target state="translated">Добавочное ядро ци в квадрате,используемое здесь,дано посредством</target>
        </trans-unit>
        <trans-unit id="721e722946fa16cdc1d4e80a122a59df383d3b35" translate="yes" xml:space="preserve">
          <source>The additive chi squared kernel is a kernel on histograms, often used in computer vision.</source>
          <target state="translated">Добавочное квадратичное ядро ци-это ядро на гистограммах,часто используемое в компьютерном зрении.</target>
        </trans-unit>
        <trans-unit id="509930cd9616089e31b59f87837fde9a2850682a" translate="yes" xml:space="preserve">
          <source>The additive version of this kernel</source>
          <target state="translated">Аддитивная версия этого ядра</target>
        </trans-unit>
        <trans-unit id="479259eedb54024948ef963b0d114b00fbb05f1b" translate="yes" xml:space="preserve">
          <source>The adjacency matrix is used to compute a normalized graph Laplacian whose spectrum (especially the eigenvectors associated to the smallest eigenvalues) has an interpretation in terms of minimal number of cuts necessary to split the graph into comparably sized components.</source>
          <target state="translated">Матрица адъюнктуры используется для вычисления нормализованного лаплацкого графика,спектр которого (особенно собственные векторы,связанные с наименьшими собственными значениями)имеет интерпретацию с точки зрения минимального количества разрезов,необходимых для разделения графика на компоненты сравнительно больших размеров.</target>
        </trans-unit>
        <trans-unit id="b4f6e52ff52334d95b6f16934aecd26800758ca5" translate="yes" xml:space="preserve">
          <source>The adjacency matrix of the graph to embed.</source>
          <target state="translated">Матрица примыкания графика для встраивания.</target>
        </trans-unit>
        <trans-unit id="f8c06d6c9bebb104301c962acd25687eac4074b4" translate="yes" xml:space="preserve">
          <source>The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation).</source>
          <target state="translated">Таким образом,скорректированный индекс Rand имеет значение,близкое к 0,0 для случайной маркировки независимо от количества кластеров и образцов и ровно 1,0,когда кластеры идентичны (до перестановки).</target>
        </trans-unit>
        <trans-unit id="83e2303d70450b875b04241e83f4a4153932e0a9" translate="yes" xml:space="preserve">
          <source>The advantage of using approximate explicit feature maps compared to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_trick&quot;&gt;kernel trick&lt;/a&gt;, which makes use of feature maps implicitly, is that explicit mappings can be better suited for online learning and can significantly reduce the cost of learning with very large datasets. Standard kernelized SVMs do not scale well to large datasets, but using an approximate kernel map it is possible to use much more efficient linear SVMs. In particular, the combination of kernel map approximations with &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; can make non-linear learning on large datasets possible.</source>
          <target state="translated">Преимущество использования приблизительных явных карт функций по сравнению с &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_trick&quot;&gt;трюком&lt;/a&gt; с ядром , который неявно использует карты функций, состоит в том, что явные сопоставления могут лучше подходить для онлайн-обучения и могут значительно снизить стоимость обучения с очень большими наборами данных. Стандартные SVM с ядром плохо масштабируются для больших наборов данных, но с помощью приблизительной карты ядра можно использовать гораздо более эффективные линейные SVM. В частности, комбинация аппроксимации карты ядра с &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; может сделать возможным нелинейное обучение на больших наборах данных.</target>
        </trans-unit>
        <trans-unit id="e8cf8d3d0c8dfd7b6a3c8351c56803f2fdf0d2d8" translate="yes" xml:space="preserve">
          <source>The advantages of Bayesian Regression are:</source>
          <target state="translated">Преимущества Байесовской регрессии:</target>
        </trans-unit>
        <trans-unit id="d7269fefe21b18a1d9b1e8a11da2f35edf3e36ec" translate="yes" xml:space="preserve">
          <source>The advantages of GBRT are:</source>
          <target state="translated">Преимущества GBRT:</target>
        </trans-unit>
        <trans-unit id="6bdcff89c23af609c3f964058bbddc38e45558f8" translate="yes" xml:space="preserve">
          <source>The advantages of Gaussian processes are:</source>
          <target state="translated">Преимущества гауссовских процессов:</target>
        </trans-unit>
        <trans-unit id="c632bf8abb99b63a04554183dcc32e66bb0cf004" translate="yes" xml:space="preserve">
          <source>The advantages of LARS are:</source>
          <target state="translated">Преимущества ЛАРС:</target>
        </trans-unit>
        <trans-unit id="54923c815d40d3138e9fbe2ad92c97dd60ef1b2e" translate="yes" xml:space="preserve">
          <source>The advantages of Multi-layer Perceptron are:</source>
          <target state="translated">Преимущества многослойного Perceptron:</target>
        </trans-unit>
        <trans-unit id="1dabd14761114fadc83f1114e989744a341117db" translate="yes" xml:space="preserve">
          <source>The advantages of Stochastic Gradient Descent are:</source>
          <target state="translated">Преимущества стохастического градиентного спуска:</target>
        </trans-unit>
        <trans-unit id="a12cea148a9927c20c87185a7ad428a4bd953760" translate="yes" xml:space="preserve">
          <source>The advantages of support vector machines are:</source>
          <target state="translated">Преимущества опорно-векторных машин:</target>
        </trans-unit>
        <trans-unit id="b3ce019dea8f1fa7178760b3ed7e1bed715f9894" translate="yes" xml:space="preserve">
          <source>The affinity matrix describing the relationship of the samples to embed. &lt;strong&gt;Must be symmetric&lt;/strong&gt;.</source>
          <target state="translated">Матрица аффинности, описывающая отношение образцов к встраиванию. &lt;strong&gt;Должен быть симметричным&lt;/strong&gt; .</target>
        </trans-unit>
        <trans-unit id="f8f51936a34abb4d8304c65310991fbeb781a2b1" translate="yes" xml:space="preserve">
          <source>The algorithm automatically sets the number of clusters, instead of relying on a parameter &lt;code&gt;bandwidth&lt;/code&gt;, which dictates the size of the region to search through. This parameter can be set manually, but can be estimated using the provided &lt;code&gt;estimate_bandwidth&lt;/code&gt; function, which is called if the bandwidth is not set.</source>
          <target state="translated">Алгоритм автоматически устанавливает количество кластеров, вместо того, чтобы полагаться на параметр &lt;code&gt;bandwidth&lt;/code&gt; , который определяет размер области для поиска. Этот параметр может быть установлен вручную, но может быть оценен с помощью прилагаемого &lt;code&gt;estimate_bandwidth&lt;/code&gt; функции, которая вызывается , если ширина полоса не установлена.</target>
        </trans-unit>
        <trans-unit id="999ae100a463cc0ff60234ca1a1687522770fa40" translate="yes" xml:space="preserve">
          <source>The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and efficiency. Since the number of least square solutions is &amp;ldquo;n_samples choose n_subsamples&amp;rdquo;, it can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets are chosen randomly. In a final step, the spatial median (or L1 median) is calculated of all least square solutions.</source>
          <target state="translated">Алгоритм вычисляет решения по методу наименьших квадратов для подмножеств с размером n_subsamples выборок в X. Любое значение n_subsamples между количеством функций и выборок приводит к оценке с компромиссом между надежностью и эффективностью. Поскольку количество решений наименьших квадратов равно &amp;laquo;n_samples выбрать n_subsamples&amp;raquo;, оно может быть чрезвычайно большим и, следовательно, может быть ограничено max_subpopulation. Если этот предел достигнут, подмножества выбираются случайным образом. На заключительном этапе пространственная медиана (или медиана L1) вычисляется для всех решений методом наименьших квадратов.</target>
        </trans-unit>
        <trans-unit id="19207c781cd295df6565aab588711bf1a9323bef" translate="yes" xml:space="preserve">
          <source>The algorithm can also be understood through the concept of &lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;Voronoi diagrams&lt;/a&gt;. First the Voronoi diagram of the points is calculated using the current centroids. Each segment in the Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated to the mean of each segment. The algorithm then repeats this until a stopping criterion is fulfilled. Usually, the algorithm stops when the relative decrease in the objective function between iterations is less than the given tolerance value. This is not the case in this implementation: iteration stops when centroids move less than the tolerance.</source>
          <target state="translated">Алгоритм также можно понять через концепцию &lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;диаграмм Вороного&lt;/a&gt; . Сначала рассчитывается диаграмма Вороного точек с использованием текущих центроидов. Каждый сегмент на диаграмме Вороного становится отдельным кластером. Во-вторых, центроиды обновляются до среднего значения каждого сегмента. Затем алгоритм повторяет это до тех пор, пока не будет выполнен критерий остановки. Обычно алгоритм останавливается, когда относительное уменьшение целевой функции между итерациями меньше заданного значения допуска. В этой реализации дело обстоит иначе: итерация останавливается, когда центроиды перемещаются меньше допуска.</target>
        </trans-unit>
        <trans-unit id="ac387733191797f4111e67a021bb9641f7899ac2" translate="yes" xml:space="preserve">
          <source>The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R &lt;code&gt;glasso&lt;/code&gt; package.</source>
          <target state="translated">Для решения этой проблемы используется алгоритм GLasso из статьи Friedman 2008 Biostatistics. Это тот же алгоритм, что и в пакете R &lt;code&gt;glasso&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="2d7b95845283c4d98cc4167d4e0748ee6c69e84f" translate="yes" xml:space="preserve">
          <source>The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. &amp;ldquo;Algorithms for computing the sample variance: Analysis and recommendations.&amp;rdquo; The American Statistician 37.3 (1983): 242-247:</source>
          <target state="translated">Алгоритм для инкрементного среднего и std приведен в уравнении 1.5a, b у Чана, Тони Ф., Джина Х. Голуба и Рэндалла Дж. Левека. &amp;laquo;Алгоритмы вычисления дисперсии выборки: анализ и рекомендации&amp;raquo;. Американский статистик 37.3 (1983): 242-247:</target>
        </trans-unit>
        <trans-unit id="c4e655f6aa7cf22a58a123dc60b12c815d9f7df3" translate="yes" xml:space="preserve">
          <source>The algorithm is adapted from Guyon [1] and was designed to generate the &amp;ldquo;Madelon&amp;rdquo; dataset.</source>
          <target state="translated">Алгоритм адаптирован из Guyon [1] и был разработан для создания набора данных &amp;laquo;Маделон&amp;raquo;.</target>
        </trans-unit>
        <trans-unit id="da28c971693f926f3030184039a925bcc2b1ca76" translate="yes" xml:space="preserve">
          <source>The algorithm is from Marsland [1].</source>
          <target state="translated">Алгоритм из Марсланда [1].</target>
        </trans-unit>
        <trans-unit id="94c105f8aebba6a360eae1c93878348f10f58fca" translate="yes" xml:space="preserve">
          <source>The algorithm is not highly scalable, as it requires multiple nearest neighbor searches during the execution of the algorithm. The algorithm is guaranteed to converge, however the algorithm will stop iterating when the change in centroids is small.</source>
          <target state="translated">Алгоритм не очень масштабируемый,так как при выполнении алгоритма требуется многократный поиск ближайшего соседа.Алгоритм гарантированно сходится,однако алгоритм прекратит итерацию при малом изменении центроидов.</target>
        </trans-unit>
        <trans-unit id="da3ddd72ab3f78c12c541d3f34c2389bc8e810e8" translate="yes" xml:space="preserve">
          <source>The algorithm is similar to forward stepwise regression, but instead of including features at each step, the estimated coefficients are increased in a direction equiangular to each one&amp;rsquo;s correlations with the residual.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3dc6012ab672dce7fe920dc60add16368612e345" translate="yes" xml:space="preserve">
          <source>The algorithm is similar to forward stepwise regression, but instead of including variables at each step, the estimated parameters are increased in a direction equiangular to each one&amp;rsquo;s correlations with the residual.</source>
          <target state="translated">Алгоритм аналогичен пошаговой регрессии вперед, но вместо включения переменных на каждом шаге оцениваемые параметры увеличиваются в направлении, равносильном корреляциям каждого из них с остатком.</target>
        </trans-unit>
        <trans-unit id="0335c4d6784d824f2c45b2464a3517f723de00c5" translate="yes" xml:space="preserve">
          <source>The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error.</source>
          <target state="translated">Алгоритм является стохастическим,и многократные перезапуски с разными семенами могут дать разные вкрапления.Тем не менее,выбор встраивания с наименьшим количеством ошибок вполне правомерен.</target>
        </trans-unit>
        <trans-unit id="7c7de93467b285ca3f0b63a2764ebf7e5ab511ed" translate="yes" xml:space="preserve">
          <source>The algorithm iterates between two major steps, similar to vanilla k-means. In the first step, \(b\) samples are drawn randomly from the dataset, to form a mini-batch. These are then assigned to the nearest centroid. In the second step, the centroids are updated. In contrast to k-means, this is done on a per-sample basis. For each sample in the mini-batch, the assigned centroid is updated by taking the streaming average of the sample and all previous samples assigned to that centroid. This has the effect of decreasing the rate of change for a centroid over time. These steps are performed until convergence or a predetermined number of iterations is reached.</source>
          <target state="translated">Алгоритм итерирует между двумя основными шагами,похожими на ванильное к-средство.На первом этапе из набора данных случайным образом извлекаются образцы \(b\),чтобы сформировать мини-пакет.Затем они присваиваются ближайшему центроиду.На втором этапе центроиды обновляются.В отличие от k-средних,это делается на основе каждого образца.Для каждой пробы в мини-партии,назначенный центроид обновляется путем взятия потокового среднего значения пробы и всех предыдущих проб,назначенных этому центроиду.Это приводит к уменьшению скорости изменения для центроида с течением времени.Эти шаги выполняются до тех пор,пока не будет достигнута конвергенция или заранее определенное количество итераций.</target>
        </trans-unit>
        <trans-unit id="eac8adf7ae77ba9768138371c6c71edd5a3a76a7" translate="yes" xml:space="preserve">
          <source>The algorithm partitions the rows and columns of a matrix so that a corresponding blockwise-constant checkerboard matrix provides a good approximation to the original matrix.</source>
          <target state="translated">Алгоритм разделяет строки и столбцы матрицы таким образом,чтобы соответствующая блочно-постоянная матрица шахматной доски обеспечивала хорошую аппроксимацию к исходной матрице.</target>
        </trans-unit>
        <trans-unit id="36533938d0e94afa89a69e7b9609c61646ab71f5" translate="yes" xml:space="preserve">
          <source>The algorithm splits the complete input sample data into a set of inliers, which may be subject to noise, and outliers, which are e.g. caused by erroneous measurements or invalid hypotheses about the data. The resulting model is then estimated only from the determined inliers.</source>
          <target state="translated">Алгоритм разделяет полные данные входной выборки на набор проскальзываний,которые могут быть подвержены шумам,и проскальзываний,которые,например,вызваны ошибочными измерениями или недействительными гипотезами о данных.Полученная модель затем оценивается только по определенным проскальзывающим параметрам.</target>
        </trans-unit>
        <trans-unit id="76e87cc3ec2713663d8449fef17b4e0c4101c305" translate="yes" xml:space="preserve">
          <source>The algorithm stops when it reaches a preset maximum number of iterations; or when the improvement in loss is below a certain, small number.</source>
          <target state="translated">Алгоритм останавливается,когда достигает заданного максимального количества итераций;или когда улучшение убытка ниже определенного,небольшого числа.</target>
        </trans-unit>
        <trans-unit id="841b170fb7ba2429816cb7c51af4dbae57b471af" translate="yes" xml:space="preserve">
          <source>The algorithm supports sample weights, which can be given by a parameter &lt;code&gt;sample_weight&lt;/code&gt;. This allows to assign more weight to some samples when computing cluster centers and values of inertia. For example, assigning a weight of 2 to a sample is equivalent to adding a duplicate of that sample to the dataset \(X\).</source>
          <target state="translated">Алгоритм поддерживает выборочные веса, которые могут быть заданы параметром &lt;code&gt;sample_weight&lt;/code&gt; . Это позволяет присвоить некоторым выборкам больший вес при вычислении центров кластеров и значений инерции. Например, присвоение веса 2 выборке эквивалентно добавлению дубликата этой выборки в набор данных \ (X \).</target>
        </trans-unit>
        <trans-unit id="a8238e48cf484817aaf23aa2adc5e4a70681d78f" translate="yes" xml:space="preserve">
          <source>The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details.</source>
          <target state="translated">Алгоритм,который будет использоваться модулем NearestNeighbors для вычисления точечных расстояний и поиска ближайших соседей.Подробности см.в документации к модулю NearestNeighbors.</target>
        </trans-unit>
        <trans-unit id="3602b334476c395c578aab7a7d09d75f92100f28" translate="yes" xml:space="preserve">
          <source>The algorithm treats the input data matrix as a bipartite graph: the rows and columns of the matrix correspond to the two sets of vertices, and each entry corresponds to an edge between a row and a column. The algorithm approximates the normalized cut of this graph to find heavy subgraphs.</source>
          <target state="translated">Алгоритм рассматривает матрицу входных данных как бипартитовый граф:строки и столбцы матрицы соответствуют двум наборам вершин,и каждая запись соответствует фронту между строкой и столбцом.Алгоритм аппроксимирует нормализованный срез этого графика для поиска тяжелых субграфов.</target>
        </trans-unit>
        <trans-unit id="e4086364ea17b4442e0265a08361358aa304a247" translate="yes" xml:space="preserve">
          <source>The algorithm used to estimate the weights. It will be called n_components times, i.e. once for each iteration of the outer loop.</source>
          <target state="translated">Алгоритм,используемый для оценки весов.Он будет называться n_components times,т.е.один раз для каждой итерации внешнего цикла.</target>
        </trans-unit>
        <trans-unit id="e4e3e486ab0114bca02d02635f9410b77ca99a25" translate="yes" xml:space="preserve">
          <source>The algorithm used to fit the model is coordinate descent.</source>
          <target state="translated">Алгоритм,используемый для подгонки модели-спуск по координатам.</target>
        </trans-unit>
        <trans-unit id="17e35eb9c6004e2d60b73104c93585aacf4a2e98" translate="yes" xml:space="preserve">
          <source>The algorithmic complexity of affinity propagation is quadratic in the number of points.</source>
          <target state="translated">Алгоритмическая сложность распространения сродства квадратична в количестве точек.</target>
        </trans-unit>
        <trans-unit id="4c657d81ec38ba678d89f7b7de187e376e50c0f3" translate="yes" xml:space="preserve">
          <source>The algorithms for regression and classification only differ in the concrete loss function used.</source>
          <target state="translated">Алгоритмы регрессии и классификации отличаются только используемой функцией конкретных потерь.</target>
        </trans-unit>
        <trans-unit id="50542b2bc530bb15fc1bcd67d7b073d61bea03ab" translate="yes" xml:space="preserve">
          <source>The alpha parameter in the stability selection article used to randomly scale the features. Should be between 0 and 1.</source>
          <target state="translated">Альфа-параметр в статье выбора устойчивости используется для случайного масштабирования характеристик.Должен быть в диапазоне от 0 до 1.</target>
        </trans-unit>
        <trans-unit id="35d70c04237d10e5da496ff67ac57fce44665490" translate="yes" xml:space="preserve">
          <source>The alpha parameter of the GraphicalLasso setting the sparsity of the model is set by internal cross-validation in the GraphicalLassoCV. As can be seen on figure 2, the grid to compute the cross-validation score is iteratively refined in the neighborhood of the maximum.</source>
          <target state="translated">Альфа-параметр GraphicalLasso,задающий спарсочность модели,задается внутренней перекрестной проверкой в GraphicalLassoCV.Как видно на рисунке 2,сетка для вычисления коэффициента перекрестной проверки итеративно уточняется в окрестности максимума.</target>
        </trans-unit>
        <trans-unit id="2075fb3135d145905cc5a41db16871a4bf5168da" translate="yes" xml:space="preserve">
          <source>The alpha-quantile of the huber loss function and the quantile loss function. Only if &lt;code&gt;loss='huber'&lt;/code&gt; or &lt;code&gt;loss='quantile'&lt;/code&gt;.</source>
          <target state="translated">Альфа-квантиль функции потерь Хьюбера и функция потерь квантиля. Только если &lt;code&gt;loss='huber'&lt;/code&gt; или &lt;code&gt;loss='quantile'&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="28a2d4f18d224d5f96bb30d3faa50cdca5ce2935" translate="yes" xml:space="preserve">
          <source>The alphas along the path where models are computed.</source>
          <target state="translated">Альфы на пути,по которому вычисляются модели.</target>
        </trans-unit>
        <trans-unit id="4390acc7061cc013783802ff89b6303810b0044d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set.</source>
          <target state="translated">Количество загрязнений набора данных,т.е.доля отклонений в наборе данных.</target>
        </trans-unit>
        <trans-unit id="a3cf6bdbcc68281bb427886cdd573766d75b2a0b" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Range is (0, 0.5).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db48eba23a1b19c738dbbe1eb3626efb11bd507f" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. If &amp;lsquo;auto&amp;rsquo;, the decision function threshold is determined as in the original paper.</source>
          <target state="translated">Степень загрязнения набора данных, т. Е. Доля выбросов в наборе данных. Используется при подгонке для определения порога функции принятия решения. Если &amp;laquo;авто&amp;raquo;, порог решающей функции определяется, как в исходной статье.</target>
        </trans-unit>
        <trans-unit id="f37071a773077642311acdcd66bbd39bda02514d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the scores of the samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b69accc98489279e23dd931886f63d4aafd913d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the decision function. If &amp;ldquo;auto&amp;rdquo;, the decision function threshold is determined as in the original paper.</source>
          <target state="translated">Степень загрязнения набора данных, т. Е. Доля выбросов в наборе данных. При подборе это используется для определения порога функции принятия решения. Если &amp;laquo;авто&amp;raquo;, порог решающей функции определяется, как в исходной статье.</target>
        </trans-unit>
        <trans-unit id="a618727acf5955128fa616ec196fb65bef331d06" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the scores of the samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="057670e523b801c82d76306d2019fbc0b1e55fa7" translate="yes" xml:space="preserve">
          <source>The amount of penalization chosen by cross validation</source>
          <target state="translated">Сумма пени,выбранная путем перекрестной проверки</target>
        </trans-unit>
        <trans-unit id="0a8889ae5955aac97641248ca40c9031408985ae" translate="yes" xml:space="preserve">
          <source>The amount of variance explained by each of the selected components.</source>
          <target state="translated">Объяснение разницы по каждому из выбранных компонентов.</target>
        </trans-unit>
        <trans-unit id="0c35119665a5a383bf2a2ea2491d6548297b77b9" translate="yes" xml:space="preserve">
          <source>The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.</source>
          <target state="translated">Балл аномалии входной выборки вычисляется как средний балл аномалии деревьев в лесу.</target>
        </trans-unit>
        <trans-unit id="f5bff141bb39bd3c0bc3c7fd23ee875e735e27a8" translate="yes" xml:space="preserve">
          <source>The anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. These are considered outliers.</source>
          <target state="translated">Показатель аномалии каждой выборки называется Коэффициент локального выброса.Он измеряет локальное отклонение плотности данной выборки относительно ее соседей.Он локален в том смысле,что оценка аномалии зависит от того,насколько изолирован объект по отношению к соседям.Точнее,локальность задается k-самыми близкими соседями,расстояние которых используется для оценки локальной плотности.Сравнивая локальную плотность образца с локальными плотностями его соседей,можно выявить образцы,которые имеют значительно меньшую плотность,чем их соседи.Они рассматриваются как исключения.</target>
        </trans-unit>
        <trans-unit id="9d2b05e02bf58b122225781451ec470ef3bbad43" translate="yes" xml:space="preserve">
          <source>The anomaly score of the input samples. The lower, the more abnormal.</source>
          <target state="translated">Показатель аномалии входных образцов.Чем ниже,тем больше аномалий.</target>
        </trans-unit>
        <trans-unit id="7ea9f6456fff678e2acc40131f68202326a3c878" translate="yes" xml:space="preserve">
          <source>The anomaly score of the input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.</source>
          <target state="translated">Показатель аномалии входных образцов.Чем ниже,тем больше аномалий.Отрицательные оценки представляют отклонения,положительные-отклонения.</target>
        </trans-unit>
        <trans-unit id="f8f5cba472ebac3a5205adfc98746f94cfb55c44" translate="yes" xml:space="preserve">
          <source>The approximate feature map provided by &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt;&lt;code&gt;AdditiveChi2Sampler&lt;/code&gt;&lt;/a&gt; can be combined with the approximate feature map provided by &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; to yield an approximate feature map for the exponentiated chi squared kernel. See the &lt;a href=&quot;#vz2010&quot; id=&quot;id5&quot;&gt;[VZ2010]&lt;/a&gt; for details and &lt;a href=&quot;#vvz2010&quot; id=&quot;id6&quot;&gt;[VVZ2010]&lt;/a&gt; for combination with the &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Приблизительную карту характеристик, предоставленную &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt; &lt;code&gt;AdditiveChi2Sampler&lt;/code&gt; ,&lt;/a&gt; можно объединить с приблизительной картой характеристик, предоставленной &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; ,&lt;/a&gt; чтобы получить приблизительную карту характеристик для ядра с экспоненциальным хи-квадрат. См. &lt;a href=&quot;#vz2010&quot; id=&quot;id5&quot;&gt;[VZ2010]&lt;/a&gt; для получения подробной информации и &lt;a href=&quot;#vvz2010&quot; id=&quot;id6&quot;&gt;[VVZ2010]&lt;/a&gt; для комбинации с &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="98f8783bf76339c13180f5a0c21989e5d9843d3d" translate="yes" xml:space="preserve">
          <source>The approximate number of singular vectors required to explain most of the data by linear combinations.</source>
          <target state="translated">Ориентировочное число сингулярных векторов,необходимое для объяснения большинства данных линейными комбинациями.</target>
        </trans-unit>
        <trans-unit id="01540c9a0c0d75da953bc96aafe62b761a8754d9" translate="yes" xml:space="preserve">
          <source>The approximate number of singular vectors required to explain most of the input data by linear combinations. Using this kind of singular spectrum in the input allows the generator to reproduce the correlations often observed in practice.</source>
          <target state="translated">Ориентировочное количество сингулярных векторов,необходимое для объяснения большей части входных данных линейными комбинациями.Использование такого сингулярного спектра во входных данных позволяет генератору воспроизводить часто встречающиеся на практике корреляции.</target>
        </trans-unit>
        <trans-unit id="9b1212b40ba5f9b8205d7b64e2e9c20d35b415b3" translate="yes" xml:space="preserve">
          <source>The arithmetic mean is the sum of the elements along the axis divided by the number of elements.</source>
          <target state="translated">Среднее арифметическое-это сумма элементов вдоль оси,деленная на количество элементов.</target>
        </trans-unit>
        <trans-unit id="35a07a5d9cec4bce6db2b0fe073857a7979c5b16" translate="yes" xml:space="preserve">
          <source>The array has 0.16% of non zero values.</source>
          <target state="translated">Массив имеет 0,16% ненулевых значений.</target>
        </trans-unit>
        <trans-unit id="a77140549775c60e078b0c26c1b965d02c08ebb3" translate="yes" xml:space="preserve">
          <source>The array of (log)-density evaluations</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="449fd4c0217076d90d22633d6cb32b7300c30336" translate="yes" xml:space="preserve">
          <source>The array of (log)-density evaluations, shape = X.shape[:-1]</source>
          <target state="translated">Массив (log)-оценок плотности,shape=X.shape[:-1]</target>
        </trans-unit>
        <trans-unit id="d00a652e1f004fd3ddfd653b6794ad2aba8108c7" translate="yes" xml:space="preserve">
          <source>The array of log(density) evaluations.</source>
          <target state="translated">Массив оценок журнала (плотности).</target>
        </trans-unit>
        <trans-unit id="0e2b34732ef7f3aed76135ded438ba1b0e2806d3" translate="yes" xml:space="preserve">
          <source>The array of log(density) evaluations. These are normalized to be probability densities, so values will be low for high-dimensional data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="531360783ebbdd4b65c83458a2d186f114e4086b" translate="yes" xml:space="preserve">
          <source>The automatic estimation from Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604 by Thomas P. Minka is also compared.</source>
          <target state="translated">Автоматическая оценка из Автоматического выбора размеров для PCA.NIPS 2000:598-604 Томаса П.Минки также сравнивается.</target>
        </trans-unit>
        <trans-unit id="37bf2d4736a8a0a6781e876ebdb656796aca8913" translate="yes" xml:space="preserve">
          <source>The available cross validation iterators are introduced in the following section.</source>
          <target state="translated">Доступные итераторы перекрестной проверки представлены в следующем разделе.</target>
        </trans-unit>
        <trans-unit id="52d681893d146fe7dad6c4424a94a534bab69431" translate="yes" xml:space="preserve">
          <source>The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration.</source>
          <target state="translated">Средняя сложность дана O(k n T),были n-количество отсчетов,а T-количество итераций.</target>
        </trans-unit>
        <trans-unit id="d9092bb0da47977d9b43f41f12fd7dcc75d26801" translate="yes" xml:space="preserve">
          <source>The average number of labels per instance. More precisely, the number of labels per sample is drawn from a Poisson distribution with &lt;code&gt;n_labels&lt;/code&gt; as its expected value, but samples are bounded (using rejection sampling) by &lt;code&gt;n_classes&lt;/code&gt;, and must be nonzero if &lt;code&gt;allow_unlabeled&lt;/code&gt; is False.</source>
          <target state="translated">Среднее количество этикеток на экземпляр. Точнее, количество меток на выборку берется из распределения Пуассона с &lt;code&gt;n_labels&lt;/code&gt; в качестве ожидаемого значения, но выборки ограничены (с использованием выборки отклонения) &lt;code&gt;n_classes&lt;/code&gt; и должны быть ненулевыми, если &lt;code&gt;allow_unlabeled&lt;/code&gt; имеет значение False.</target>
        </trans-unit>
        <trans-unit id="50521765649b7ccece3cb452f18896c14c55c5d8" translate="yes" xml:space="preserve">
          <source>The average precision score in multi-label settings</source>
          <target state="translated">Средний балл точности в настройках с несколькими этикетками</target>
        </trans-unit>
        <trans-unit id="66fdff3c7719e9049c28a125538dc0d9761fb599" translate="yes" xml:space="preserve">
          <source>The averaged NDCG scores for all samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f192c1d1db49dfae3574eb70a5374ae8608eb71" translate="yes" xml:space="preserve">
          <source>The averaged intercept term.</source>
          <target state="translated">Средний срок перехвата.</target>
        </trans-unit>
        <trans-unit id="9595497d17dae79a02bc4117e4b89bb9f68b4736" translate="yes" xml:space="preserve">
          <source>The averaged intercept term. Only available if &lt;code&gt;average=True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c831f95b759b1e5121cd7c90ec4906a408d65670" translate="yes" xml:space="preserve">
          <source>The averaged sample DCG scores.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f1e3570b35bda8ded1b9651eb310bcefe0589f6" translate="yes" xml:space="preserve">
          <source>The axes with which the grid has been created or None if the grid has been given.</source>
          <target state="translated">Оси,с помощью которых была создана сетка,или Нет,если сетка была задана.</target>
        </trans-unit>
        <trans-unit id="da553a7d6d695e12dca5e56a77d2e07dce1c7a46" translate="yes" xml:space="preserve">
          <source>The axis along which &lt;code&gt;X&lt;/code&gt; will be subsampled. &lt;code&gt;axis=0&lt;/code&gt; will select rows while &lt;code&gt;axis=1&lt;/code&gt; will select columns.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2af1a0461163ef9a917dd17f9a6c7032c07c79bb" translate="yes" xml:space="preserve">
          <source>The axis along which to impute.</source>
          <target state="translated">Ось,вдоль которой вменяется.</target>
        </trans-unit>
        <trans-unit id="efae1008b127cbb418b23469c7f222e2a6660b67" translate="yes" xml:space="preserve">
          <source>The bag of words representation is quite simplistic but surprisingly useful in practice.</source>
          <target state="translated">Мешок с изображением слов достаточно упрощен,но удивительно полезен на практике.</target>
        </trans-unit>
        <trans-unit id="a41734ff5b2ce49bfc6f83ebb1bb324e76d37d70" translate="yes" xml:space="preserve">
          <source>The bags of words representation implies that &lt;code&gt;n_features&lt;/code&gt; is the number of distinct words in the corpus: this number is typically larger than 100,000.</source>
          <target state="translated">Представление пакетов слов подразумевает, что &lt;code&gt;n_features&lt;/code&gt; - это количество отдельных слов в корпусе: это число обычно превышает 100000.</target>
        </trans-unit>
        <trans-unit id="eac13f6898f72e0eaad5c85bd1b456a5882d1a72" translate="yes" xml:space="preserve">
          <source>The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.</source>
          <target state="translated">Сбалансированная точность в двоичных и мульти-классовых проблем классификации для решения несбалансированных наборов данных.Она определяется как среднее значение отзыва,полученного по каждому классу.</target>
        </trans-unit>
        <trans-unit id="fd188197709d9753bf6d2d7d8ebbb1b211377cd3" translate="yes" xml:space="preserve">
          <source>The bandwidth here acts as a smoothing parameter, controlling the tradeoff between bias and variance in the result. A large bandwidth leads to a very smooth (i.e. high-bias) density distribution. A small bandwidth leads to an unsmooth (i.e. high-variance) density distribution.</source>
          <target state="translated">Полоса пропускания здесь действует как сглаживающий параметр,контролирующий компромисс между смещением и дисперсией результата.Большая пропускная способность приводит к очень сглаженному (т.е.высокоскоростному)распределению плотности.Маленькая пропускная способность приводит к негладкому (т.е.высокодисперсионному)распределению плотности.</target>
        </trans-unit>
        <trans-unit id="220a3cae014ca1f6f44493020405f4e460f4038d" translate="yes" xml:space="preserve">
          <source>The bandwidth of the kernel.</source>
          <target state="translated">Полоса пропускания ядра.</target>
        </trans-unit>
        <trans-unit id="eaa1089b1fdc51cb43f528072a19cd3f3ec50c61" translate="yes" xml:space="preserve">
          <source>The bandwidth parameter.</source>
          <target state="translated">Параметр полосы пропускания.</target>
        </trans-unit>
        <trans-unit id="ca9efc037882c6303cbf300aed185a1d9a7dd7ae" translate="yes" xml:space="preserve">
          <source>The bar plot indicates the accuracy, training time (normalized) and test time (normalized) of each classifier.</source>
          <target state="translated">Гистограмма показывает точность,время тренировки (нормализованное)и время тестирования (нормализованное)каждого классификатора.</target>
        </trans-unit>
        <trans-unit id="6ef0fbe42e5306306c086ebee574632386d4e293" translate="yes" xml:space="preserve">
          <source>The base classifier is a random forest classifier with 25 base estimators (trees). If this classifier is trained on all 800 training datapoints, it is overly confident in its predictions and thus incurs a large log-loss. Calibrating an identical classifier, which was trained on 600 datapoints, with method=&amp;rsquo;sigmoid&amp;rsquo; on the remaining 200 datapoints reduces the confidence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center. This calibration results in a lower log-loss. Note that an alternative would have been to increase the number of base estimators which would have resulted in a similar decrease in log-loss.</source>
          <target state="translated">Базовый классификатор - это случайный классификатор лесов с 25 базовыми оценками (деревьями). Если этот классификатор обучен на всех 800 обучающих точках данных, он будет чрезмерно уверен в своих прогнозах и, таким образом, приведет к большим потерям журнала. Калибровка идентичного классификатора, который был обучен на 600 точках данных, с method = 'sigmoid' на оставшихся 200 точках данных снижает достоверность прогнозов, т. Е. Перемещает векторы вероятности от краев симплекса к центру. Эта калибровка приводит к меньшим потерям журнала. Обратите внимание, что альтернативой было бы увеличение количества базовых оценщиков, что привело бы к аналогичному снижению логарифмических потерь.</target>
        </trans-unit>
        <trans-unit id="3f89ef1e6cb2a54f74eb0a982beea8ffd3333592" translate="yes" xml:space="preserve">
          <source>The base classifier is a random forest classifier with 25 base estimators (trees). If this classifier is trained on all 800 training datapoints, it is overly confident in its predictions and thus incurs a large log-loss. Calibrating an identical classifier, which was trained on 600 datapoints, with method=&amp;rsquo;sigmoid&amp;rsquo; on the remaining 200 datapoints reduces the confidence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center:</source>
          <target state="translated">Базовый классификатор - это случайный классификатор лесов с 25 базовыми оценками (деревьями). Если этот классификатор обучен на всех 800 обучающих точках данных, он будет чрезмерно уверен в своих прогнозах и, таким образом, приведет к большим потерям журнала. Калибровка идентичного классификатора, который был обучен на 600 точках данных, с method = 'sigmoid' на оставшихся 200 точках данных снижает достоверность прогнозов, т. Е. Перемещает векторы вероятности от краев симплекса к центру:</target>
        </trans-unit>
        <trans-unit id="2000944fc04d6f6e7393d659c7173e725c27d458" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cdab13e5017fd96224833fdcfcd75615e152e1af" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper &lt;code&gt;classes_&lt;/code&gt; and &lt;code&gt;n_classes_&lt;/code&gt; attributes. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;</source>
          <target state="translated">Базовая оценка, на основе которой строится усиленный ансамбль. Требуется поддержка выборочного взвешивания, а также правильные &lt;code&gt;classes_&lt;/code&gt; и &lt;code&gt;n_classes_&lt;/code&gt; . Если &lt;code&gt;None&lt;/code&gt; , то базовая оценка - &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="7ad7363185e9af79d948e6b752e7cc25ad567f4b" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper &lt;code&gt;classes_&lt;/code&gt; and &lt;code&gt;n_classes_&lt;/code&gt; attributes. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="39ba9d176a83a208ed96d60f33bec321ac051ac5" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;</source>
          <target state="translated">Базовая оценка, на основе которой строится усиленный ансамбль. Требуется поддержка взвешивания образца. Если &lt;code&gt;None&lt;/code&gt; , то базовая оценка - &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="09e53f3d87743d060ad05fb04ae38587d00307b0" translate="yes" xml:space="preserve">
          <source>The base estimator from which the classifier chain is built.</source>
          <target state="translated">Базовый оценщик,на основе которого строится цепочка классификатора.</target>
        </trans-unit>
        <trans-unit id="c229b061f41bb16a20d56916c466508eaa778c17" translate="yes" xml:space="preserve">
          <source>The base estimator from which the ensemble is grown.</source>
          <target state="translated">Базовая оценка,из которой вырастает ансамбль.</target>
        </trans-unit>
        <trans-unit id="5066de8ea8cb5c40493cb3ffbb0e912b0ff83ff9" translate="yes" xml:space="preserve">
          <source>The base estimator from which the transformer is built. This can be both a fitted (if &lt;code&gt;prefit&lt;/code&gt; is set to True) or a non-fitted estimator. The estimator must have either a &lt;code&gt;feature_importances_&lt;/code&gt; or &lt;code&gt;coef_&lt;/code&gt; attribute after fitting.</source>
          <target state="translated">Базовый оценщик, из которого построен трансформатор. Это может быть как подогнанная (если для параметра &lt;code&gt;prefit&lt;/code&gt; установлено значение True), так и не подогнанная оценка. Оценщик должен иметь атрибут &lt;code&gt;feature_importances_&lt;/code&gt; или &lt;code&gt;coef_&lt;/code&gt; после подбора.</target>
        </trans-unit>
        <trans-unit id="8935e197daa0e6c85ff9f9b0e235723b5ff13893" translate="yes" xml:space="preserve">
          <source>The base estimator from which the transformer is built. This is stored only when a non-fitted estimator is passed to the &lt;code&gt;SelectFromModel&lt;/code&gt;, i.e when prefit is False.</source>
          <target state="translated">Базовый оценщик, из которого построен трансформатор. Это сохраняется только в том &lt;code&gt;SelectFromModel&lt;/code&gt; , если в SelectFromModel передается неподготовленный оценщик , то есть когда prefit имеет значение False.</target>
        </trans-unit>
        <trans-unit id="8062156aa44a8a655e18edffbba0a99e0e098c8a" translate="yes" xml:space="preserve">
          <source>The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.</source>
          <target state="translated">Базовый оценщик для подгонки под случайные подмножества набора данных.Если Нет,то базовая оценка является деревом решений.</target>
        </trans-unit>
        <trans-unit id="d80c39c018abc387662fda22711c8b7bb4707717" translate="yes" xml:space="preserve">
          <source>The base kernel</source>
          <target state="translated">Базовое ядро</target>
        </trans-unit>
        <trans-unit id="1b7bc12e1d1eb0bb8e39e15748d7f9b27d93ef1a" translate="yes" xml:space="preserve">
          <source>The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the &lt;code&gt;weights&lt;/code&gt; keyword. The default value, &lt;code&gt;weights = 'uniform'&lt;/code&gt;, assigns uniform weights to each neighbor. &lt;code&gt;weights = 'distance'&lt;/code&gt; assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights.</source>
          <target state="translated">Базовая классификация ближайших соседей использует одинаковые веса: то есть значение, присвоенное точке запроса, вычисляется простым большинством голосов ближайших соседей. При некоторых обстоятельствах лучше взвесить соседей так, чтобы более близкие соседи способствовали большей подгонке. Это можно сделать с помощью ключевого слова &lt;code&gt;weights&lt;/code&gt; . Значение по умолчанию, &lt;code&gt;weights = 'uniform'&lt;/code&gt; , назначает одинаковые веса каждому соседу. &lt;code&gt;weights = 'distance'&lt;/code&gt; назначает веса, пропорциональные обратному расстоянию от точки запроса. В качестве альтернативы может быть предоставлена ​​определяемая пользователем функция расстояния для вычисления весов.</target>
        </trans-unit>
        <trans-unit id="650ceb2a1485e5f890a3dcff86c2748037640c3b" translate="yes" xml:space="preserve">
          <source>The basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. Under some circumstances, it can be advantageous to weight points such that nearby points contribute more to the regression than faraway points. This can be accomplished through the &lt;code&gt;weights&lt;/code&gt; keyword. The default value, &lt;code&gt;weights = 'uniform'&lt;/code&gt;, assigns equal weights to all points. &lt;code&gt;weights = 'distance'&lt;/code&gt; assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied, which will be used to compute the weights.</source>
          <target state="translated">Базовая регрессия ближайших соседей использует одинаковые веса: то есть каждая точка в локальной окрестности вносит единообразный вклад в классификацию точки запроса. При некоторых обстоятельствах может быть выгодно взвесить точки так, чтобы ближайшие точки вносили больший вклад в регрессию, чем удаленные точки. Это можно сделать с помощью ключевого слова &lt;code&gt;weights&lt;/code&gt; . Значение по умолчанию &lt;code&gt;weights = 'uniform'&lt;/code&gt; присваивает всем точкам одинаковые веса. &lt;code&gt;weights = 'distance'&lt;/code&gt; назначает веса, пропорциональные обратному расстоянию от точки запроса. В качестве альтернативы может быть предоставлена ​​определяемая пользователем функция расстояния, которая будет использоваться для вычисления весов.</target>
        </trans-unit>
        <trans-unit id="1cb5ab68855135ab83ff44b066a7bba092358a07" translate="yes" xml:space="preserve">
          <source>The behavior of &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; is summarized in the following table.</source>
          <target state="translated">Поведение &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt; .LocalOutlierFactor суммировано в следующей таблице.</target>
        </trans-unit>
        <trans-unit id="9c1d95adae9acea07102a8bfe1db513459ebeaee" translate="yes" xml:space="preserve">
          <source>The behavior of the model is very sensitive to the &lt;code&gt;gamma&lt;/code&gt; parameter. If &lt;code&gt;gamma&lt;/code&gt; is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with &lt;code&gt;C&lt;/code&gt; will be able to prevent overfitting.</source>
          <target state="translated">Поведение модели очень чувствительно к параметру &lt;code&gt;gamma&lt;/code&gt; . Если &lt;code&gt;gamma&lt;/code&gt; слишком велика, радиус области влияния опорных векторов включает только сам опорный вектор, и никакая регуляризация с помощью &lt;code&gt;C&lt;/code&gt; не сможет предотвратить переобучение.</target>
        </trans-unit>
        <trans-unit id="2ab99fa4c73a04aa03f4ac31b15851c1ddbe51a1" translate="yes" xml:space="preserve">
          <source>The below plot uses the first two features. See &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;here&lt;/a&gt; for more information on this dataset.</source>
          <target state="translated">На приведенном ниже графике используются первые две функции. См. &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;Здесь&lt;/a&gt; для получения дополнительной информации об этом наборе данных.</target>
        </trans-unit>
        <trans-unit id="4992d67c6e1aff2322ba4c5bb3c9e0c82a7a36cf" translate="yes" xml:space="preserve">
          <source>The best model is selected by cross-validation.</source>
          <target state="translated">Лучшая модель выбирается путем перекрестной проверки.</target>
        </trans-unit>
        <trans-unit id="1a3f42d6835c4f6686d54e99d9f9cd0e3a597fec" translate="yes" xml:space="preserve">
          <source>The best performance is 1 with &lt;code&gt;normalize == True&lt;/code&gt; and the number of samples with &lt;code&gt;normalize == False&lt;/code&gt;.</source>
          <target state="translated">Наилучшая производительность - 1 с &lt;code&gt;normalize == True&lt;/code&gt; и количество выборок с &lt;code&gt;normalize == False&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="4b2ab3d16cd4ed6ed82ae137ea7b0ef6d79ff832" translate="yes" xml:space="preserve">
          <source>The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.</source>
          <target state="translated">Лучшее возможное р-значение-1/(n_permutations+1),худшее-1.0.</target>
        </trans-unit>
        <trans-unit id="ed9ac623a873633695e926c14de64904b54167f4" translate="yes" xml:space="preserve">
          <source>The best possible score is 1.0, lower values are worse.</source>
          <target state="translated">Лучший результат-1,0,ниже-хуже.</target>
        </trans-unit>
        <trans-unit id="9230aaef9f0a1fa5e57a82e7f44b36cb5d04d36e" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.</source>
          <target state="translated">Лучшее значение-1,а худшее--1.Значения около 0 указывают на перекрытие кластеров.</target>
        </trans-unit>
        <trans-unit id="6e2eac60da3011cdc30f86be85be8ffa7e5f1da5" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.</source>
          <target state="translated">Лучшее значение-1,а худшее--1.Значения около 0 указывают на перекрытие кластеров.Отрицательные значения обычно указывают на то,что выборка была назначена не тому кластеру,так как другой кластер более похож.</target>
        </trans-unit>
        <trans-unit id="b4a689b86b5cda51ef6628ef622a0daf756424bd" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is 0 when &lt;code&gt;adjusted=False&lt;/code&gt;.</source>
          <target state="translated">Лучшее значение - 1, а худшее значение - 0, когда параметр &lt;code&gt;adjusted=False&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="488558d8da77f986fd351608404cb6a07bd4fe58" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is 0.</source>
          <target state="translated">Лучшее значение-1,а худшее-0.</target>
        </trans-unit>
        <trans-unit id="b82813aa45a878f8df07ac95972e2e5116e63bed" translate="yes" xml:space="preserve">
          <source>The bias term in the underlying linear model.</source>
          <target state="translated">Термин смещения в лежащей в основе линейной модели.</target>
        </trans-unit>
        <trans-unit id="d5675ace96b0ccc75bff7a0b67255343283bf5b9" translate="yes" xml:space="preserve">
          <source>The bicluster label of each column.</source>
          <target state="translated">Билюстерная этикетка каждой колонки.</target>
        </trans-unit>
        <trans-unit id="d135838b65ddeda2dac8521941211167551d33d9" translate="yes" xml:space="preserve">
          <source>The bicluster label of each row.</source>
          <target state="translated">Библетка каждого ряда.</target>
        </trans-unit>
        <trans-unit id="b216c2ddafb03b97c09e36e1d0b96ac53ff1f7ba" translate="yes" xml:space="preserve">
          <source>The bins have identical widths.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0eb1458ca2a62b6254994f184c01ee65afc52b4" translate="yes" xml:space="preserve">
          <source>The bins have the same number of samples and depend on &lt;code&gt;y_prob&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f153639172a63c8dba2ed27e16715aaa100936d" translate="yes" xml:space="preserve">
          <source>The bipartite structure allows for the use of efficient block Gibbs sampling for inference.</source>
          <target state="translated">Двухкомпонентная структура позволяет использовать эффективный отбор проб Гиббса для умозаключений.</target>
        </trans-unit>
        <trans-unit id="a902e8cbc26950a56283f916fe814a1e5f8ed790" translate="yes" xml:space="preserve">
          <source>The bottleneck of a gradient boosting procedure is building the decision trees. Building a traditional decision tree (as in the other GBDTs &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;) requires sorting the samples at each node (for each feature). Sorting is needed so that the potential gain of a split point can be computed efficiently. Splitting a single node has thus a complexity of \(\mathcal{O}(n_\text{features} \times n \log(n))\) where \(n\) is the number of samples at the node.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b104bf424ac624d7987a6846ffafb73fcb6a3323" translate="yes" xml:space="preserve">
          <source>The bounding box for each cluster center when centers are generated at random.</source>
          <target state="translated">Ограничительное поле для каждого кластерного центра,когда центры генерируются случайным образом.</target>
        </trans-unit>
        <trans-unit id="e73634a1f979122b85e5e2e94c3d89b631e61d63" translate="yes" xml:space="preserve">
          <source>The boxes represent repeated sampling.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de87510aaa8a4634fe7b670444f1141d0b934e04" translate="yes" xml:space="preserve">
          <source>The breast cancer dataset is a classic and very easy binary classification dataset.</source>
          <target state="translated">Набор данных по раку молочной железы является классическим и очень простым набором данных бинарной классификации.</target>
        </trans-unit>
        <trans-unit id="2045b20286b76d60e5fa1f74a1df8fc02a45c66b" translate="yes" xml:space="preserve">
          <source>The brier score loss is also between 0 to 1 and the lower the score (the mean square difference is smaller), the more accurate the prediction is. It can be thought of as a measure of the &amp;ldquo;calibration&amp;rdquo; of a set of probabilistic predictions.</source>
          <target state="translated">Потеря по шкале Бриера также находится в диапазоне от 0 до 1, и чем ниже оценка (средняя квадратичная разница меньше), тем точнее прогноз. Его можно рассматривать как меру &amp;laquo;калибровки&amp;raquo; набора вероятностных прогнозов.</target>
        </trans-unit>
        <trans-unit id="b76bd58217534fc8775d360bccb78e720219d44b" translate="yes" xml:space="preserve">
          <source>The calibration is based on the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt; method of the &lt;code&gt;base_estimator&lt;/code&gt; if it exists, else on &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6452eadf0058adcc76ac85d9ade38659e522fb0c" translate="yes" xml:space="preserve">
          <source>The calibration of the model can be assessed by plotting the mean observed value vs the mean predicted value on groups of test samples binned by predicted risk.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="74481417cca6e6adb9f25dfbb8e31b8fc201eb01" translate="yes" xml:space="preserve">
          <source>The callable to use for the inverse transformation. This will be passed the same arguments as inverse transform, with args and kwargs forwarded. If inverse_func is None, then inverse_func will be the identity function.</source>
          <target state="translated">Обратное преобразование.При этом будут передаваться те же аргументы,что и при инверсном преобразовании,с переадресацией аргументов и каргов.Если параметр inverse_func равен None,то параметр inverse_func будет идентификационной функцией.</target>
        </trans-unit>
        <trans-unit id="81ea3433c521d90153f147dfae64d0df290c9757" translate="yes" xml:space="preserve">
          <source>The callable to use for the transformation. This will be passed the same arguments as transform, with args and kwargs forwarded. If func is None, then func will be the identity function.</source>
          <target state="translated">Призыв,который можно использовать для трансформации.При этом будут передаваться те же аргументы,что и при преобразовании,с переадресацией аргументов и каргов.Если функция None,то она будет функцией идентификации.</target>
        </trans-unit>
        <trans-unit id="a1419fd6aa69dd9ba89b3d48a7257eafd52e186a" translate="yes" xml:space="preserve">
          <source>The categorical Naive Bayes classifier is suitable for classification with discrete features that are categorically distributed. The categories of each feature are drawn from a categorical distribution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f38539ea641dd8b50f2f7d85a7f466ba670ba50" translate="yes" xml:space="preserve">
          <source>The categories of each feature determined during fitting (in order of the features in X and corresponding with the output of &lt;code&gt;transform&lt;/code&gt;).</source>
          <target state="translated">Категории каждой функции, определенные во время подгонки (в порядке элементов в X и соответствующих выходным данным &lt;code&gt;transform&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="12b8e6304a220ce13ab40041e41390ae67d86b4e" translate="yes" xml:space="preserve">
          <source>The categories of each feature determined during fitting (in order of the features in X and corresponding with the output of &lt;code&gt;transform&lt;/code&gt;). This includes the category specified in &lt;code&gt;drop&lt;/code&gt; (if any).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11dc40b09acf079281dfd9a81c3951e435409b7d" translate="yes" xml:space="preserve">
          <source>The centers of each cluster. Only returned if &lt;code&gt;return_centers=True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9d4177cfd25fe2d42f6a09b10ba8f700ec03cc45" translate="yes" xml:space="preserve">
          <source>The chi squared kernel is given by</source>
          <target state="translated">Чи-квадратное ядро даётся</target>
        </trans-unit>
        <trans-unit id="e654b311ef425d553f0f6705f2dd43fc95968d41" translate="yes" xml:space="preserve">
          <source>The chi squared kernel is most commonly used on histograms (bags) of visual words.</source>
          <target state="translated">Квадратное ядро ци наиболее часто используется на гистограммах (сумках)визуальных слов.</target>
        </trans-unit>
        <trans-unit id="085e153138bf3768d8fc53856c1f1238c6d3caff" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is a very popular choice for training non-linear SVMs in computer vision applications. It can be computed using &lt;a href=&quot;generated/sklearn.metrics.pairwise.chi2_kernel#sklearn.metrics.pairwise.chi2_kernel&quot;&gt;&lt;code&gt;chi2_kernel&lt;/code&gt;&lt;/a&gt; and then passed to an &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;sklearn.svm.SVC&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;kernel=&quot;precomputed&quot;&lt;/code&gt;:</source>
          <target state="translated">Ядро хи-квадрат - очень популярный выбор для обучения нелинейных SVM в приложениях компьютерного зрения. Его можно вычислить с помощью &lt;a href=&quot;generated/sklearn.metrics.pairwise.chi2_kernel#sklearn.metrics.pairwise.chi2_kernel&quot;&gt; &lt;code&gt;chi2_kernel&lt;/code&gt; ,&lt;/a&gt; а затем передать в &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;sklearn.svm.SVC&lt;/code&gt; &lt;/a&gt; с &lt;code&gt;kernel=&quot;precomputed&quot;&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="9233e345f2318b5fc92b12dd745f5adf2df3c33c" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is computed between each pair of rows in X and Y. X and Y have to be non-negative. This kernel is most commonly applied to histograms.</source>
          <target state="translated">Ядро в хи-квадрат вычисляется между каждой парой строк в X и Y.X и Y должны быть неотрицательными.Это ядро наиболее часто применяется к гистограммам.</target>
        </trans-unit>
        <trans-unit id="a8ba729b106653acd0bc97ad998e8dac7b2b635e" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is given by:</source>
          <target state="translated">Ядро в хи-квадрат дано:</target>
        </trans-unit>
        <trans-unit id="959a186ffdea8e09c8857bbd475b2cf453d363af" translate="yes" xml:space="preserve">
          <source>The child estimator template used to create the collection of fitted sub-estimators.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5ac1548c51d0c4529f9b6250f5d16db6b392dc40" translate="yes" xml:space="preserve">
          <source>The children of each non-leaf node. Values less than &lt;code&gt;n_features&lt;/code&gt; correspond to leaves of the tree which are the original samples. A node &lt;code&gt;i&lt;/code&gt; greater than or equal to &lt;code&gt;n_features&lt;/code&gt; is a non-leaf node and has children &lt;code&gt;children_[i - n_features]&lt;/code&gt;. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node &lt;code&gt;n_features + i&lt;/code&gt;</source>
          <target state="translated">Потомки каждого нелистового узла. Значения меньше &lt;code&gt;n_features&lt;/code&gt; соответствуют листьям дерева, которые являются исходными образцами. Узел &lt;code&gt;i&lt;/code&gt; , больший или равный &lt;code&gt;n_features&lt;/code&gt; , является нелистовым узлом и имеет дочерние &lt;code&gt;children_[i - n_features]&lt;/code&gt; . В качестве альтернативы на i-й итерации дочерние &lt;code&gt;n_features + i&lt;/code&gt; [i] [0] и children [i] [1] объединяются в узел n_features + i</target>
        </trans-unit>
        <trans-unit id="c8989abb78a441b49758b99ec48d8b090a3ce248" translate="yes" xml:space="preserve">
          <source>The children of each non-leaf node. Values less than &lt;code&gt;n_samples&lt;/code&gt; correspond to leaves of the tree which are the original samples. A node &lt;code&gt;i&lt;/code&gt; greater than or equal to &lt;code&gt;n_samples&lt;/code&gt; is a non-leaf node and has children &lt;code&gt;children_[i - n_samples]&lt;/code&gt;. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node &lt;code&gt;n_samples + i&lt;/code&gt;</source>
          <target state="translated">Потомки каждого нелистового узла. Значения меньше &lt;code&gt;n_samples&lt;/code&gt; соответствуют листьям дерева, которые являются исходными выборками. Узел &lt;code&gt;i&lt;/code&gt; , больший или равный &lt;code&gt;n_samples&lt;/code&gt; , является нелистовым узлом и имеет дочерние &lt;code&gt;children_[i - n_samples]&lt;/code&gt; . В качестве альтернативы на i-й итерации дочерние &lt;code&gt;n_samples + i&lt;/code&gt; [i] [0] и children [i] [1] объединяются в узел n_samples + i</target>
        </trans-unit>
        <trans-unit id="f3473fe6f2b500963d93290672a7a3ef3f212da2" translate="yes" xml:space="preserve">
          <source>The choice of features is not particularly helpful, but serves to illustrate the technique.</source>
          <target state="translated">Выбор функций не особенно полезен,но служит для иллюстрации техники.</target>
        </trans-unit>
        <trans-unit id="15475f4de31a0dbbaa5b0396fc18010629c57dd3" translate="yes" xml:space="preserve">
          <source>The choice of the distribution depends on the problem at hand:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f460fe2e0e34556a3c6a8a6d017a5f63eeb9ea9" translate="yes" xml:space="preserve">
          <source>The cholesky decomposition of the precision matrices of each mixture component. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">Холецкое разложение матриц точности каждого компонента смеси. Матрица точности - это обратная матрица ковариации. Ковариационная матрица является симметричной положительно определенной, поэтому смесь гауссовских значений может быть эквивалентно параметризована матрицами точности. Сохранение матриц точности вместо ковариационных матриц делает более эффективным вычисление логарифмической вероятности появления новых выборок во время тестирования. Форма зависит от &lt;code&gt;covariance_type&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="8e7b3855f3de3292a39f43c9800aebcf16bc77cb" translate="yes" xml:space="preserve">
          <source>The claim &lt;strong&gt;frequency&lt;/strong&gt; is the number of claims divided by the exposure, typically measured in number of claims per year.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="54b709e28c0bc302f278f75cf2f9281781fc40ea" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; used with the optional parameter &lt;code&gt;svd_solver='randomized'&lt;/code&gt; is very useful in that case: since we are going to drop most of the singular vectors it is much more efficient to limit the computation to an approximated estimate of the singular vectors we will keep to actually perform the transform.</source>
          <target state="translated">Класс &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; ,&lt;/a&gt; используемый с необязательным параметром &lt;code&gt;svd_solver='randomized'&lt;/code&gt; , очень полезен в этом случае: поскольку мы собираемся отбросить большую часть сингулярных векторов, гораздо эффективнее ограничить вычисления приближенной оценкой сингулярных векторов, мы будем продолжайте фактически выполнять преобразование.</target>
        </trans-unit>
        <trans-unit id="9afc238719eb2575519db8e476eb36c39ec071a2" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.feature_extraction.dictvectorizer#sklearn.feature_extraction.DictVectorizer&quot;&gt;&lt;code&gt;DictVectorizer&lt;/code&gt;&lt;/a&gt; can be used to convert feature arrays represented as lists of standard Python &lt;code&gt;dict&lt;/code&gt; objects to the NumPy/SciPy representation used by scikit-learn estimators.</source>
          <target state="translated">Класс &lt;a href=&quot;generated/sklearn.feature_extraction.dictvectorizer#sklearn.feature_extraction.DictVectorizer&quot;&gt; &lt;code&gt;DictVectorizer&lt;/code&gt; &lt;/a&gt; можно использовать для преобразования массивов функций, представленных в виде списков стандартных объектов Python &lt;code&gt;dict&lt;/code&gt; , в представление NumPy / SciPy, используемое оценщиками scikit-learn.</target>
        </trans-unit>
        <trans-unit id="0b3d7d58279a2952a288a94db8a135a534d78ec7" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; is a high-speed, low-memory vectorizer that uses a technique known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_hashing&quot;&gt;feature hashing&lt;/a&gt;, or the &amp;ldquo;hashing trick&amp;rdquo;. Instead of building a hash table of the features encountered in training, as the vectorizers do, instances of &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; apply a hash function to the features to determine their column index in sample matrices directly. The result is increased speed and reduced memory usage, at the expense of inspectability; the hasher does not remember what the input features looked like and has no &lt;code&gt;inverse_transform&lt;/code&gt; method.</source>
          <target state="translated">Класс &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt; - это высокоскоростной векторизатор с низким объемом памяти, в котором используется метод, известный как &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_hashing&quot;&gt;хеширование функций&lt;/a&gt; или &amp;laquo;трюк с хешированием&amp;raquo;. Вместо построения хэш-таблицы функций, встречающихся при обучении, как это делают векторизаторы, экземпляры &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt; применяют хеш-функцию к функциям, чтобы напрямую определить их индекс столбца в образцах матриц. В результате повышается скорость и уменьшается использование памяти за счет возможности проверки; хешер не запоминает, как выглядели входные функции, и не имеет метода &lt;code&gt;inverse_transform&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="46148e4d160f9510def06597f1921af425f74fcb" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt;&lt;code&gt;IsotonicRegression&lt;/code&gt;&lt;/a&gt; fits a non-decreasing function to data. It solves the following problem:</source>
          <target state="translated">Класс &lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt; &lt;code&gt;IsotonicRegression&lt;/code&gt; &lt;/a&gt; соответствует неубывающей функции для данных. Это решает следующую проблему:</target>
        </trans-unit>
        <trans-unit id="0773ec8e22bf216d4df607f3329b18b1168f7c09" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt;&lt;code&gt;IsotonicRegression&lt;/code&gt;&lt;/a&gt; fits a non-decreasing real function to 1-dimensional data. It solves the following problem:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04846022f8485d75461ddcb301a4717897728672" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt;&lt;code&gt;AdditiveChi2Sampler&lt;/code&gt;&lt;/a&gt; implements this component wise deterministic sampling. Each component is sampled \(n\) times, yielding \(2n+1\) dimensions per input dimension (the multiple of two stems from the real and complex part of the Fourier transform). In the literature, \(n\) is usually chosen to be 1 or 2, transforming the dataset to size &lt;code&gt;n_samples * 5 * n_features&lt;/code&gt; (in the case of \(n=2\)).</source>
          <target state="translated">Класс &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt; &lt;code&gt;AdditiveChi2Sampler&lt;/code&gt; &lt;/a&gt; реализует эту компонентную детерминированную выборку. Каждый компонент выбирается \ (n \) раз, что дает \ (2n + 1 \) измерений для каждого входного измерения (кратное двум происходит из действительной и комплексной части преобразования Фурье). В литературе \ (n \) обычно выбирается равным 1 или 2, преобразуя набор данных в размер &lt;code&gt;n_samples * 5 * n_features&lt;/code&gt; (в случае \ (n = 2 \)).</target>
        </trans-unit>
        <trans-unit id="9fdc0ed638cb0889fa5320ee5ada72b80a16402f" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.elasticnetcv#sklearn.linear_model.ElasticNetCV&quot;&gt;&lt;code&gt;ElasticNetCV&lt;/code&gt;&lt;/a&gt; can be used to set the parameters &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) and &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) by cross-validation.</source>
          <target state="translated">Класс &lt;a href=&quot;generated/sklearn.linear_model.elasticnetcv#sklearn.linear_model.ElasticNetCV&quot;&gt; &lt;code&gt;ElasticNetCV&lt;/code&gt; &lt;/a&gt; можно использовать для установки параметров &lt;code&gt;alpha&lt;/code&gt; (\ (\ alpha \)) и &lt;code&gt;l1_ratio&lt;/code&gt; (\ (\ rho \)) путем перекрестной проверки.</target>
        </trans-unit>
        <trans-unit id="c89182d0633b09742e745213024a53ccec4fee0a" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnetcv#sklearn.linear_model.MultiTaskElasticNetCV&quot;&gt;&lt;code&gt;MultiTaskElasticNetCV&lt;/code&gt;&lt;/a&gt; can be used to set the parameters &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) and &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) by cross-validation.</source>
          <target state="translated">Класс &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnetcv#sklearn.linear_model.MultiTaskElasticNetCV&quot;&gt; &lt;code&gt;MultiTaskElasticNetCV&lt;/code&gt; &lt;/a&gt; можно использовать для установки параметров &lt;code&gt;alpha&lt;/code&gt; (\ (\ alpha \)) и &lt;code&gt;l1_ratio&lt;/code&gt; (\ (\ rho \)) путем перекрестной проверки.</target>
        </trans-unit>
        <trans-unit id="6c66ee2c7a53fbda2b2b2da602d1e782a1fcc1b3" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a first-order SGD learning routine. The algorithm iterates over the training examples and for each example updates the model parameters according to the update rule given by</source>
          <target state="translated">Класс &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; реализует процедуру обучения SGD первого порядка. Алгоритм повторяет обучающие примеры и для каждого примера обновляет параметры модели в соответствии с правилом обновления, заданным</target>
        </trans-unit>
        <trans-unit id="f128e57d09cf8b511f002f70c40429b250044323" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification.</source>
          <target state="translated">Класс &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; реализует простую процедуру обучения стохастическим градиентным спуском, которая поддерживает различные функции потерь и штрафы за классификацию.</target>
        </trans-unit>
        <trans-unit id="3fc72b8fffc5a512701e2a944bd472c169bc1771" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification. Below is the decision boundary of a &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; trained with the hinge loss, equivalent to a linear SVM.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9be5b4c1c13de0290ee1bddf8fed519138b844e9" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; is well suited for regression problems with a large number of training samples (&amp;gt; 10.000), for other problems we recommend &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt;, or &lt;a href=&quot;generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt;&lt;code&gt;ElasticNet&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Класс &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; реализует простую программу обучения стохастическим градиентным спуском, которая поддерживает различные функции потерь и штрафы для соответствия моделям линейной регрессии. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; хорошо подходит для задач регрессии с большим количеством обучающих выборок (&amp;gt; 10.000), для других задач мы рекомендуем &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt; или &lt;a href=&quot;generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt; &lt;code&gt;ElasticNet&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="5ca16e4c08c0574f508b6c526beeb2111eade54a" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;OneClassSVM&lt;/code&gt;&lt;/a&gt; implements a One-Class SVM which is used in outlier detection.</source>
          <target state="translated">Класс &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;OneClassSVM&lt;/code&gt; &lt;/a&gt; реализует одноклассную SVM, которая используется для обнаружения выбросов.</target>
        </trans-unit>
        <trans-unit id="5eaffd69df806c5b8353c88358cfd0a49b275168" translate="yes" xml:space="preserve">
          <source>The class label for which the PDPs should be computed. Only if gbrt is a multi-class model. Must be in &lt;code&gt;gbrt.classes_&lt;/code&gt;.</source>
          <target state="translated">Метка класса, для которого должны быть вычислены PDP. Только если gbrt - мультиклассовая модель. Должен быть в &lt;code&gt;gbrt.classes_&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="2a31ab08188f24bd31e99f218cab5467eddf2ef0" translate="yes" xml:space="preserve">
          <source>The class labels.</source>
          <target state="translated">Классные этикетки.</target>
        </trans-unit>
        <trans-unit id="7c57202048886b9c3874e17b392c104a7a21da98" translate="yes" xml:space="preserve">
          <source>The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="075fd1f474c95d9f3895b75ead4b1f9c99be54a8" translate="yes" xml:space="preserve">
          <source>The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;.</source>
          <target state="translated">Класс логарифмических вероятностей входных выборок. Порядок классов соответствует &lt;code&gt;classes_&lt;/code&gt; в атрибуте classes_ .</target>
        </trans-unit>
        <trans-unit id="b5b69c523a5547b26d366d35f11f7dab77d71024" translate="yes" xml:space="preserve">
          <source>The class of Matern kernels is a generalization of the &lt;a href=&quot;sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt;. It has an additional parameter \(\nu\) which controls the smoothness of the resulting function. The smaller \(\nu\), the less smooth the approximated function is. As \(\nu\rightarrow\infty\), the kernel becomes equivalent to the &lt;a href=&quot;sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; kernel. When \(\nu = 1/2\), the Mat&amp;eacute;rn kernel becomes identical to the absolute exponential kernel. Important intermediate values are \(\nu=1.5\) (once differentiable functions) and \(\nu=2.5\) (twice differentiable functions).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9e49144e04860d9a1f4a8512901a503e5d41c6f" translate="yes" xml:space="preserve">
          <source>The class of Matern kernels is a generalization of the RBF and the absolute exponential kernel parameterized by an additional parameter nu. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions).</source>
          <target state="translated">Класс материнских ядер является обобщением RBF и абсолютного экспоненциального ядра,параметризованного дополнительным параметром nu.Чем меньше nu,тем менее гладкой является аппроксимируемая функция.Для nu=inf,ядро становится эквивалентным ядру ФБМ,а для nu=0.5-абсолютному экспоненциальному ядру.Важными промежуточными значениями являются nu=1.5 (одноразовые дифференцируемые функции)и nu=2.5 (двуразовые дифференцируемые функции).</target>
        </trans-unit>
        <trans-unit id="e48205f573bda857c27ec3937eb80960daed3556" translate="yes" xml:space="preserve">
          <source>The class ordering is preserved:</source>
          <target state="translated">Сохраняется классовый заказ:</target>
        </trans-unit>
        <trans-unit id="0401a90c4f50bc0cacd4ce11e3ddd76a2d543405" translate="yes" xml:space="preserve">
          <source>The class prior probabilities. By default, the class proportions are inferred from the training data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9e493bffc66549584cbf656024fe268e745fcd4" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6a7a001ab832443f9ea75be4af1e68e4b9eb028c" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of outputs is the same of that of the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt; attribute.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81478ef84d2d645ae9c4e632247f7f3bc9052a92" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of outputs is the same of that of the &lt;code&gt;classes_&lt;/code&gt; attribute.</source>
          <target state="translated">Вероятности классов входных выборок. Порядок вывода такой же, как &lt;code&gt;classes_&lt;/code&gt; атрибута classes_ .</target>
        </trans-unit>
        <trans-unit id="eb02560f00e2596a11150c32ba96675012f41151" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ea2403e3ddde25cdaa7213df112f25928f5232d" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;.</source>
          <target state="translated">Вероятности классов входных выборок. Порядок классов соответствует &lt;code&gt;classes_&lt;/code&gt; в атрибуте classes_ .</target>
        </trans-unit>
        <trans-unit id="64b22dfb54911895a611d3b9cfbaf3a3f14d97f5" translate="yes" xml:space="preserve">
          <source>The class to report if &lt;code&gt;average='binary'&lt;/code&gt; and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting &lt;code&gt;labels=[pos_label]&lt;/code&gt; and &lt;code&gt;average != 'binary'&lt;/code&gt; will report scores for that label only.</source>
          <target state="translated">Класс, который будет сообщать, если &lt;code&gt;average='binary'&lt;/code&gt; и данные являются двоичными. Если данные являются многоклассовыми или многоклассовыми, это будет проигнорировано; установка &lt;code&gt;labels=[pos_label]&lt;/code&gt; и &lt;code&gt;average != 'binary'&lt;/code&gt; будет сообщать оценки только для этого ярлыка.</target>
        </trans-unit>
        <trans-unit id="b02b6e7e5cca0ff24c2691f6e7d2bc9c247d17d1" translate="yes" xml:space="preserve">
          <source>The class to use to build the returned adjacency matrix.</source>
          <target state="translated">Класс,который будет использоваться для построения матрицы возвращаемых примыканий.</target>
        </trans-unit>
        <trans-unit id="08bf02b9f7a917eed20c9c94c384791c0ed4578f" translate="yes" xml:space="preserve">
          <source>The class with respect to which we perform a one-vs-all fit. If None, then it is assumed that the given problem is binary.</source>
          <target state="translated">Класс,по отношению к которому мы выполняем единое целое.Если Нет,то предполагается,что данная задача является двоичной.</target>
        </trans-unit>
        <trans-unit id="26dc8660d1112d77620f09027ce7b9fbee6027a3" translate="yes" xml:space="preserve">
          <source>The classes &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; provide functionality to fit linear models for classification and regression using different (convex) loss functions and different penalties. E.g., with &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt;, &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; fits a logistic regression model, while with &lt;code&gt;loss=&quot;hinge&quot;&lt;/code&gt; it fits a linear support vector machine (SVM).</source>
          <target state="translated">Классы &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; и &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; предоставляют функциональные возможности для соответствия линейным моделям классификации и регрессии с использованием различных (выпуклых) функций потерь и различных штрафов. Например, с &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; , &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; соответствует модели логистической регрессии, в то время как с &lt;code&gt;loss=&quot;hinge&quot;&lt;/code&gt; это соответствует линейной опорных векторов (SVM).</target>
        </trans-unit>
        <trans-unit id="d29e2c71b01d97c9c93dc5aad937ec38e91b7ca9" translate="yes" xml:space="preserve">
          <source>The classes &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; provide two criteria to stop the algorithm when a given level of convergence is reached:</source>
          <target state="translated">Классы &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; и &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; предоставляют два критерия для остановки алгоритма при достижении заданного уровня сходимости:</target>
        </trans-unit>
        <trans-unit id="d7ef8ada32aed500913b0cf053368e18ea306f33" translate="yes" xml:space="preserve">
          <source>The classes in &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; can handle either NumPy arrays or &lt;code&gt;scipy.sparse&lt;/code&gt; matrices as input. For dense matrices, a large number of possible distance metrics are supported. For sparse matrices, arbitrary Minkowski metrics are supported for searches.</source>
          <target state="translated">Классы в &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; &lt;/a&gt; могут обрабатывать либо массивы NumPy, либо матрицы &lt;code&gt;scipy.sparse&lt;/code&gt; в качестве входных данных. Для плотных матриц поддерживается большое количество возможных метрик расстояния. Для разреженных матриц для поиска поддерживаются произвольные метрики Минковского.</target>
        </trans-unit>
        <trans-unit id="f39d71815c55889dd46cb64855cc1d40f837d2d9" translate="yes" xml:space="preserve">
          <source>The classes in the &lt;a href=&quot;classes#module-sklearn.feature_selection&quot;&gt;&lt;code&gt;sklearn.feature_selection&lt;/code&gt;&lt;/a&gt; module can be used for feature selection/dimensionality reduction on sample sets, either to improve estimators&amp;rsquo; accuracy scores or to boost their performance on very high-dimensional datasets.</source>
          <target state="translated">Классы в модуле &lt;a href=&quot;classes#module-sklearn.feature_selection&quot;&gt; &lt;code&gt;sklearn.feature_selection&lt;/code&gt; &lt;/a&gt; могут использоваться для выбора признаков / уменьшения размерности на выборочных наборах, либо для улучшения показателей точности оценщиков, либо для повышения их производительности на очень многомерных наборах данных.</target>
        </trans-unit>
        <trans-unit id="df672af473b055efb339763c1ea56e27edeec8c9" translate="yes" xml:space="preserve">
          <source>The classes in this submodule allow to approximate the embedding \(\phi\), thereby working explicitly with the representations \(\phi(x_i)\), which obviates the need to apply the kernel or store training examples.</source>
          <target state="translated">Классы в этом подмодуле позволяют приблизить встраивание \(\phi\),тем самым явно работая с представлениями \(\phi(x_i)\),что избавляет от необходимости применять ядро или хранить обучающие примеры.</target>
        </trans-unit>
        <trans-unit id="41647bba96e968e0bd58965ae65f2bc0365d42c6" translate="yes" xml:space="preserve">
          <source>The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).</source>
          <target state="translated">Метки классов (единственная выходная задача),или список массивов меток классов (многовыходная задача).</target>
        </trans-unit>
        <trans-unit id="772daeb9b0eaa408b264b0fa548b1cbd24779805" translate="yes" xml:space="preserve">
          <source>The classes labels.</source>
          <target state="translated">Этикетки классов.</target>
        </trans-unit>
        <trans-unit id="13f18ce429d97546a8b16cf132e8e6c03147cb50" translate="yes" xml:space="preserve">
          <source>The classic implementation of the clustering method based on the Lloyd&amp;rsquo;s algorithm. It consumes the whole set of input data at each iteration.</source>
          <target state="translated">Классическая реализация метода кластеризации на основе алгоритма Ллойда. Он использует весь набор входных данных на каждой итерации.</target>
        </trans-unit>
        <trans-unit id="b3e4f700832d6cdb3a215961499bbf68e1aa40b8" translate="yes" xml:space="preserve">
          <source>The classification is performed by projecting to the first two principal components found by PCA and CCA for visualisation purposes, followed by using the &lt;a href=&quot;../../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; metaclassifier using two SVCs with linear kernels to learn a discriminative model for each class. Note that PCA is used to perform an unsupervised dimensionality reduction, while CCA is used to perform a supervised one.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4b0c76ea0ae326d19f535ab9520b9dd024cf9124" translate="yes" xml:space="preserve">
          <source>The classification is performed by projecting to the first two principal components found by PCA and CCA for visualisation purposes, followed by using the &lt;a href=&quot;../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; metaclassifier using two SVCs with linear kernels to learn a discriminative model for each class. Note that PCA is used to perform an unsupervised dimensionality reduction, while CCA is used to perform a supervised one.</source>
          <target state="translated">Классификация выполняется путем проецирования на первые два основных компонента, найденных PCA и CCA для целей визуализации, с последующим использованием метаклассификатора &lt;a href=&quot;../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; &lt;/a&gt; с использованием двух SVC с линейными ядрами для изучения дискриминационной модели для каждого класса. Обратите внимание, что PCA используется для выполнения неконтролируемого уменьшения размерности, в то время как CCA используется для выполнения контролируемого уменьшения.</target>
        </trans-unit>
        <trans-unit id="3f7b4b923ee0ab9dbf257d91692081f3d6b6945f" translate="yes" xml:space="preserve">
          <source>The classification target. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;target&lt;/code&gt; will be a pandas Series.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c83c4f2209684d200eb3e2732a4357005bab63e" translate="yes" xml:space="preserve">
          <source>The classifier which predicts given the output of &lt;code&gt;estimators_&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9bcc8debd316bfd1c4b9e73630f6ba7215b9e3d" translate="yes" xml:space="preserve">
          <source>The classifier whose output decision function needs to be calibrated to offer more accurate predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.</source>
          <target state="translated">Классификатор,чья функция принятия решения о выходе должна быть откалибрована,чтобы обеспечить более точные выходы predict_proba.Если cv=prefit,классификатор должен был уже соответствовать данным.</target>
        </trans-unit>
        <trans-unit id="4188695350bfd2d35a500d1eee9968d78806da89" translate="yes" xml:space="preserve">
          <source>The classifier whose output need to be calibrated to provide more accurate &lt;code&gt;predict_proba&lt;/code&gt; outputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="917e6ef2892859cfeed06565c1a2e19769195da1" translate="yes" xml:space="preserve">
          <source>The cluster ordered list of sample indices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88e39a3dfd7087282361f3b39109354f7ff32ede" translate="yes" xml:space="preserve">
          <source>The code below also illustrates how the construction and the computation of the predictions can be parallelized within multiple jobs.</source>
          <target state="translated">Нижеприведенный код также иллюстрирует,как построение и вычисление предсказаний может быть распараллелено в рамках нескольких заданий.</target>
        </trans-unit>
        <trans-unit id="6a1cc4b5491e9b42fd445850086762fcb248e836" translate="yes" xml:space="preserve">
          <source>The code below plots the dependency of y against individual x_i and normalized values of univariate F-tests statistics and mutual information.</source>
          <target state="translated">Приведенный ниже код отображает зависимость y от индивидуальных x_i и нормализованных значений одномерной статистики F-тестов и взаимной информации.</target>
        </trans-unit>
        <trans-unit id="4b0a65eb61a5277b5e28aef5c3b0e7ea577bdc07" translate="yes" xml:space="preserve">
          <source>The code-examples in the above tutorials are written in a &lt;em&gt;python-console&lt;/em&gt; format. If you wish to easily execute these examples in &lt;strong&gt;IPython&lt;/strong&gt;, use:</source>
          <target state="translated">Примеры кода в приведенных выше руководствах написаны в формате &lt;em&gt;консоли Python&lt;/em&gt; . Если вы хотите легко выполнить эти примеры в &lt;strong&gt;IPython&lt;/strong&gt; , используйте:</target>
        </trans-unit>
        <trans-unit id="ce298b790cf6df954f5ce5058b04debe04deb707" translate="yes" xml:space="preserve">
          <source>The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the regression sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">Коэффициент R^2 определяется как (1-u/v),где u-остаточная сумма квадратов ((y_true-y_pred)**2).sum()и v-сумма регрессии квадратов ((y_true-y_true.mean()))**2).sum().Лучшая возможная оценка-1.0 и она может быть отрицательной (потому что модель может быть произвольно хуже).Константная модель,которая всегда предсказывает ожидаемое значение y,без учета входных характеристик,получит оценку R^2 в 0.0.</target>
        </trans-unit>
        <trans-unit id="48121e6567bc65fc6ea38acd7d461ac73e2ea472" translate="yes" xml:space="preserve">
          <source>The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">Коэффициент R^2 определяется как (1-u/v),где u-остаточная сумма квадратов ((y_true-y_pred)**2).sum()и v-сумма квадратов ((y_true-y_true.mean()))**2).sum().Лучшая возможная оценка-1.0 и она может быть отрицательной (потому что модель может быть произвольно хуже).Константная модель,которая всегда предсказывает ожидаемое значение y,без учета входных характеристик,получит оценку R^2 в 0.0.</target>
        </trans-unit>
        <trans-unit id="a7b8f1c4eb5f60ca5d7c57d0dcd2ad470def6e62" translate="yes" xml:space="preserve">
          <source>The coefficient estimates for Ordinary Least Squares rely on the independence of the features. When features are correlated and the columns of the design matrix \(X\) have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed target, producing a large variance. This situation of &lt;em&gt;multicollinearity&lt;/em&gt; can arise, for example, when data are collected without an experimental design.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0dabbd241b37afcd99edc97579580c22f2703f9" translate="yes" xml:space="preserve">
          <source>The coefficient of the underlying linear model. It is returned only if coef is True.</source>
          <target state="translated">Коэффициент лежащей в основе линейной модели.Возвращается только в том случае,если коеф равен True.</target>
        </trans-unit>
        <trans-unit id="8b0531322a0447d32da022aa29f51bdb5853bf2b" translate="yes" xml:space="preserve">
          <source>The coefficients \(w\) of the model can be accessed:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8bf2184b15d694a8d6ef65832d1a84477f9388bd" translate="yes" xml:space="preserve">
          <source>The coefficients are significantly different. AGE and EXPERIENCE coefficients are both positive but they now have less influence on the prediction.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2933da174c8fa332cc9ad841c0c25a1ab1996d2" translate="yes" xml:space="preserve">
          <source>The coefficients can be forced to be positive.</source>
          <target state="translated">Коэффициенты могут быть принудительно положительными.</target>
        </trans-unit>
        <trans-unit id="3c89d0ba57b5481659a50cc73b873694c355fd9b" translate="yes" xml:space="preserve">
          <source>The coefficients of the linear model: &lt;code&gt;Y = X coef_ + Err&lt;/code&gt;</source>
          <target state="translated">Коэффициенты линейной модели: &lt;code&gt;Y = X coef_ + Err&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="5395d49e7d69c07e1b2416a99bbd87627621ae2f" translate="yes" xml:space="preserve">
          <source>The coefficients, the residual sum of squares and the coefficient of determination are also calculated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7757730afe97cb8593dea4757d82727872c7529" translate="yes" xml:space="preserve">
          <source>The coefficients, the residual sum of squares and the variance score are also calculated.</source>
          <target state="translated">Вычисляются также коэффициенты,остаточная сумма квадратов и оценка дисперсии.</target>
        </trans-unit>
        <trans-unit id="f15e1f2c7fb96cd43c32056a6c407c9c1c570883" translate="yes" xml:space="preserve">
          <source>The collection of fitted base estimators.</source>
          <target state="translated">Коллекция подогнанных базовых смет.</target>
        </trans-unit>
        <trans-unit id="4d0721afe1a467c5d2eb2443395ac7ab7a4e3f8b" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators as defined in &lt;code&gt;estimators&lt;/code&gt; that are not &amp;lsquo;drop&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e97d4f2642ef3372648a38fef3350e771647c3d6" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators as defined in &lt;code&gt;estimators&lt;/code&gt; that are not &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">Набор подобранных суб-оценок, как определено в &lt;code&gt;estimators&lt;/code&gt; , которые не равны &lt;code&gt;None&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="0b6e390487ac628e8895818624f2b32fd0c95d36" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators.</source>
          <target state="translated">Коллекция встроенных субоценщиков.</target>
        </trans-unit>
        <trans-unit id="929cef17b33a8aadbb4cff8f87d4813d9e794dca" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators. &lt;code&gt;loss_.K&lt;/code&gt; is 1 for binary classification, otherwise n_classes.</source>
          <target state="translated">Коллекция подобранных суб-оценок. &lt;code&gt;loss_.K&lt;/code&gt; равен 1 для двоичной классификации, иначе n_classes.</target>
        </trans-unit>
        <trans-unit id="34213cac4f6d6096a4a72655d4c46d8da28f907c" translate="yes" xml:space="preserve">
          <source>The collection of fitted transformers as tuples of (name, fitted_transformer, column). &lt;code&gt;fitted_transformer&lt;/code&gt; can be an estimator, &amp;lsquo;drop&amp;rsquo;, or &amp;lsquo;passthrough&amp;rsquo;. In case there were no columns selected, this will be the unfitted transformer. If there are remaining columns, the final element is a tuple of the form: (&amp;lsquo;remainder&amp;rsquo;, transformer, remaining_columns) corresponding to the &lt;code&gt;remainder&lt;/code&gt; parameter. If there are remaining columns, then &lt;code&gt;len(transformers_)==len(transformers)+1&lt;/code&gt;, otherwise &lt;code&gt;len(transformers_)==len(transformers)&lt;/code&gt;.</source>
          <target state="translated">Коллекция подогнанных трансформаторов в виде кортежей (имя, подогнанный_трансформатор, столбец). &lt;code&gt;fitted_transformer&lt;/code&gt; может быть оценочным, &amp;laquo;drop&amp;raquo; или &amp;laquo;passthrough&amp;raquo;. Если столбцы не выбраны, это будет непригодный трансформатор. Если есть оставшиеся столбцы, последний элемент представляет собой кортеж в форме: ('остаток', преобразователь, оставшиеся_столбцы), соответствующий параметру &lt;code&gt;remainder&lt;/code&gt; . Если есть оставшиеся столбцы, то &lt;code&gt;len(transformers_)==len(transformers)+1&lt;/code&gt; , иначе &lt;code&gt;len(transformers_)==len(transformers)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="3626dc1999e74681fe5d5d9d41be14f043b3b76d" translate="yes" xml:space="preserve">
          <source>The collection of patches extracted from the image, where &lt;code&gt;n_patches&lt;/code&gt; is either &lt;code&gt;max_patches&lt;/code&gt; or the total number of patches that can be extracted.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd0348cc380532b01c9102e25a743ff6f1c3fc49" translate="yes" xml:space="preserve">
          <source>The collection of patches extracted from the images, where &lt;code&gt;n_patches&lt;/code&gt; is either &lt;code&gt;n_samples * max_patches&lt;/code&gt; or the total number of patches that can be extracted.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d6d471d12278a874c4225d83bbd25d7f04a0a976" translate="yes" xml:space="preserve">
          <source>The color map illustrates the decision function learned by the SVC.</source>
          <target state="translated">Цветная карта иллюстрирует функцию принятия решений,усвоенную SVC.</target>
        </trans-unit>
        <trans-unit id="093f5986f8d055d5b47f11ad021ce6ecfcef91e9" translate="yes" xml:space="preserve">
          <source>The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.</source>
          <target state="translated">Колонки из indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]дают значение индикатора для i-й оценки.</target>
        </trans-unit>
        <trans-unit id="f0cca3e664aa52c0e892615c12365c0a6af40452" translate="yes" xml:space="preserve">
          <source>The combination used in this example is not particularly helpful on this dataset and is only used to illustrate the usage of FeatureUnion.</source>
          <target state="translated">Комбинация,используемая в этом примере,не особенно полезна для данного набора данных и используется только для иллюстрации использования FeatureUnion.</target>
        </trans-unit>
        <trans-unit id="238b85659a6f96b691291ac49250c2a48af41530" translate="yes" xml:space="preserve">
          <source>The complete set of patches. If the patches contain colour information, channels are indexed along the last dimension: RGB patches would have &lt;code&gt;n_channels=3&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b4849a6d39c196dd952c4e98da8a227fce685751" translate="yes" xml:space="preserve">
          <source>The complexity parameter \(\alpha \geq 0\) controls the amount of shrinkage: the larger the value of \(\alpha\), the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4524a6f67fb21e2d5fb712c219044b7460dbad0e" translate="yes" xml:space="preserve">
          <source>The components of the random matrix are drawn from N(0, 1 / n_components).</source>
          <target state="translated">Компоненты случайной матрицы взяты из N(0,1/n_компонент).</target>
        </trans-unit>
        <trans-unit id="4d179dd17ec2e9e8ac79ee8fb26a8b8727033655" translate="yes" xml:space="preserve">
          <source>The compromise between l1 and l2 penalization chosen by cross validation</source>
          <target state="translated">Компромисс между l1 и l2 пенализацией,выбранной путем перекрестной проверки.</target>
        </trans-unit>
        <trans-unit id="37c8e8aa42af430070b2b87be4c5db5db294cc7b" translate="yes" xml:space="preserve">
          <source>The computation during &lt;code&gt;fit&lt;/code&gt; is:</source>
          <target state="translated">Расчет во время &lt;code&gt;fit&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="cc728704a5b1deaa221b61ada3dd42ad8e661fbd" translate="yes" xml:space="preserve">
          <source>The computation during &lt;code&gt;predict&lt;/code&gt; is:</source>
          <target state="translated">Расчет во время &lt;code&gt;predict&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="cc5659751ed96e6a9a511b247760eba7bd9fab63" translate="yes" xml:space="preserve">
          <source>The computation of Davies-Bouldin is simpler than that of Silhouette scores.</source>
          <target state="translated">Вычисление Дэвиса-Болдина проще,чем вычисление баллов по силуэту.</target>
        </trans-unit>
        <trans-unit id="d526bd934b93818cdf8a3fccada8e6b2b34d84f2" translate="yes" xml:space="preserve">
          <source>The computational overhead of each SVD is &lt;code&gt;O(batch_size * n_features ** 2)&lt;/code&gt;, but only 2 * batch_size samples remain in memory at a time. There will be &lt;code&gt;n_samples / batch_size&lt;/code&gt; SVD computations to get the principal components, versus 1 large SVD of complexity &lt;code&gt;O(n_samples * n_features ** 2)&lt;/code&gt; for PCA.</source>
          <target state="translated">Вычислительные издержки каждого SVD &lt;code&gt;O(batch_size * n_features ** 2)&lt;/code&gt; , но только 2 * batch_size выборки остаются в памяти одновременно. Для получения основных компонентов будет &lt;code&gt;n_samples / batch_size&lt;/code&gt; вычислений SVD, по сравнению с 1 большим SVD сложности &lt;code&gt;O(n_samples * n_features ** 2)&lt;/code&gt; для PCA.</target>
        </trans-unit>
        <trans-unit id="2fa163515de812b4d78a2bbcef87e239a36ae4d1" translate="yes" xml:space="preserve">
          <source>The concept of early stopping is simple. We specify a &lt;code&gt;validation_fraction&lt;/code&gt; which denotes the fraction of the whole dataset that will be kept aside from training to assess the validation loss of the model. The gradient boosting model is trained using the training set and evaluated using the validation set. When each additional stage of regression tree is added, the validation set is used to score the model. This is continued until the scores of the model in the last &lt;code&gt;n_iter_no_change&lt;/code&gt; stages do not improve by atleast &lt;code&gt;tol&lt;/code&gt;. After that the model is considered to have converged and further addition of stages is &amp;ldquo;stopped early&amp;rdquo;.</source>
          <target state="translated">Концепция раннего прекращения приема пищи проста. Мы указываем &lt;code&gt;validation_fraction&lt;/code&gt; , который обозначает долю всего набора данных, которая будет храниться в стороне от обучения, чтобы оценить потерю валидации модели. Модель повышения градиента обучается с использованием обучающего набора и оценивается с помощью проверочного набора. Когда добавляется каждый дополнительный этап дерева регрессии, набор проверки используется для оценки модели. Это продолжается до тех пор, пока оценки модели на последних этапах &lt;code&gt;n_iter_no_change&lt;/code&gt; не улучшатся по крайней мере на &lt;code&gt;tol&lt;/code&gt; . После этого модель считается сходимой, и дальнейшее добавление стадий &amp;laquo;прерывается&amp;raquo;.</target>
        </trans-unit>
        <trans-unit id="feb60f640c62fd0856cb50e8401267d4f55d4774" translate="yes" xml:space="preserve">
          <source>The concrete &lt;code&gt;LossFunction&lt;/code&gt; object.</source>
          <target state="translated">Конкретный объект &lt;code&gt;LossFunction&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="4f21e1340272b60cf06a69153cc19d0140d625cf" translate="yes" xml:space="preserve">
          <source>The concrete loss function can be set via the &lt;code&gt;loss&lt;/code&gt; parameter. &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; supports the following loss functions:</source>
          <target state="translated">Конкретную функцию потерь можно установить с помощью параметра &lt;code&gt;loss&lt;/code&gt; . &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt; поддерживает следующие функции потерь:</target>
        </trans-unit>
        <trans-unit id="c24a50928ec0729629e45e8b53b6d8b9f0112f79" translate="yes" xml:space="preserve">
          <source>The concrete loss function can be set via the &lt;code&gt;loss&lt;/code&gt; parameter. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; supports the following loss functions:</source>
          <target state="translated">Конкретную функцию потерь можно установить с помощью параметра &lt;code&gt;loss&lt;/code&gt; . &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt; поддерживает следующие функции потерь:</target>
        </trans-unit>
        <trans-unit id="a5b98bb304bc3ec6a422167859a2f5da5580fa47" translate="yes" xml:space="preserve">
          <source>The concrete penalty can be set via the &lt;code&gt;penalty&lt;/code&gt; parameter. SGD supports the following penalties:</source>
          <target state="translated">Конкретный штраф можно установить с помощью параметра &lt;code&gt;penalty&lt;/code&gt; . SGD поддерживает следующие штрафы:</target>
        </trans-unit>
        <trans-unit id="c0c66ccf788e88932593e02308e1dcae0bb5c1f0" translate="yes" xml:space="preserve">
          <source>The conditional probability distribution of each unit is given by the logistic sigmoid activation function of the input it receives:</source>
          <target state="translated">Условное распределение вероятностей каждой единицы задается логистической сигмоидальной функцией активации входа,который она получает:</target>
        </trans-unit>
        <trans-unit id="6e94f7f28c2cc9bab8c93a85a4438b802d82d10d" translate="yes" xml:space="preserve">
          <source>The confidence score for a sample is the signed distance of that sample to the hyperplane.</source>
          <target state="translated">Доверительная оценка для выборки-это подписанное расстояние от этой выборки до гиперплоскости.</target>
        </trans-unit>
        <trans-unit id="b129f98741f30e6ae05e0872d86b02b5a921e905" translate="yes" xml:space="preserve">
          <source>The connectivity constraints are imposed via an connectivity matrix: a scipy sparse matrix that has elements only at the intersection of a row and a column with indices of the dataset that should be connected. This matrix can be constructed from a-priori information: for instance, you may wish to cluster web pages by only merging pages with a link pointing from one to another. It can also be learned from the data, for instance using &lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt;&lt;/a&gt; to restrict merging to nearest neighbors as in &lt;a href=&quot;../auto_examples/cluster/plot_agglomerative_clustering#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py&quot;&gt;this example&lt;/a&gt;, or using &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt;&lt;code&gt;sklearn.feature_extraction.image.grid_to_graph&lt;/code&gt;&lt;/a&gt; to enable only merging of neighboring pixels on an image, as in the &lt;a href=&quot;../auto_examples/cluster/plot_coin_ward_segmentation#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py&quot;&gt;coin&lt;/a&gt; example.</source>
          <target state="translated">Ограничения связности накладываются через матрицу связности: нечеткую разреженную матрицу, которая имеет элементы только на пересечении строки и столбца с индексами набора данных, которые должны быть связаны. Эта матрица может быть построена из априорной информации: например, вы можете сгруппировать веб-страницы, объединяя только страницы со ссылкой, указывающей одну на другую. Это также можно узнать из данных, например, используя &lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt; &lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt; ,&lt;/a&gt; чтобы ограничить слияние до ближайших соседей, как в &lt;a href=&quot;../auto_examples/cluster/plot_agglomerative_clustering#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py&quot;&gt;этом примере&lt;/a&gt; , или используя &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt; &lt;code&gt;sklearn.feature_extraction.image.grid_to_graph&lt;/code&gt; ,&lt;/a&gt; чтобы разрешить только слияние соседних пикселей на изображении, как в &lt;a href=&quot;../auto_examples/cluster/plot_coin_ward_segmentation#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py&quot;&gt;монета&lt;/a&gt; пример.</target>
        </trans-unit>
        <trans-unit id="cba08514635a6da99fcbf046720afd85a65680ea" translate="yes" xml:space="preserve">
          <source>The constant value which defines the covariance: k(x_1, x_2) = constant_value</source>
          <target state="translated">Значение константы,определяющей ковариацию:k(x_1,x_2)=значение константы_значение</target>
        </trans-unit>
        <trans-unit id="83836055dd86bf18d0180179c00f49511034950c" translate="yes" xml:space="preserve">
          <source>The contingency table calculated is typically utilized in the calculation of a similarity statistic (like the others listed in this document) between the two clusterings.</source>
          <target state="translated">Рассчитанная таблица непредвиденных обстоятельств обычно используется при расчете статистики сходства (как и другие,перечисленные в настоящем документе)между двумя кластерами.</target>
        </trans-unit>
        <trans-unit id="ca0505a0687635a00a190438e7d26956e48c468c" translate="yes" xml:space="preserve">
          <source>The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.</source>
          <target state="translated">Порог конвергенции.ЭМ итерации прекратятся,когда нижняя граница среднего прироста окажется ниже этого порога.</target>
        </trans-unit>
        <trans-unit id="0e26ef32f94bda64e8f6e396c3c303a3311ac4e1" translate="yes" xml:space="preserve">
          <source>The convergence threshold. EM iterations will stop when the lower bound average gain on the likelihood (of the training data with respect to the model) is below this threshold.</source>
          <target state="translated">Порог конвергенции.ЭМ итерации прекратятся,когда нижняя граница среднего прироста по вероятности (данных тренировки относительно модели)окажется ниже этого порога.</target>
        </trans-unit>
        <trans-unit id="80f05388942910150044c6d47584d8bddc73ef4c" translate="yes" xml:space="preserve">
          <source>The converse mapping from feature name to column index is stored in the &lt;code&gt;vocabulary_&lt;/code&gt; attribute of the vectorizer:</source>
          <target state="translated">Обратное отображение имени элемента в индекс столбца хранится в &lt;code&gt;vocabulary_&lt;/code&gt; атрибут векторизатора:</target>
        </trans-unit>
        <trans-unit id="42e30f7491cd0bbd1be9eaa1008d97761b319a5f" translate="yes" xml:space="preserve">
          <source>The converted and validated X.</source>
          <target state="translated">Преобразованный и подтвержденный X.</target>
        </trans-unit>
        <trans-unit id="f4a00aeed47e5a0cd669edcdec893bcad4bc87b6" translate="yes" xml:space="preserve">
          <source>The converted and validated array.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="043f2ec629cc510b9c1127fe6b10b6fe4448d241" translate="yes" xml:space="preserve">
          <source>The converted and validated y.</source>
          <target state="translated">преобразованные и утвержденные y.</target>
        </trans-unit>
        <trans-unit id="81a7bc326e697c66d8802c043c85fabeabbf241b" translate="yes" xml:space="preserve">
          <source>The converted dataname.</source>
          <target state="translated">Преобразованное имя Датана.</target>
        </trans-unit>
        <trans-unit id="223ada656bcdd3fee604b90acc8ae4baf52723e0" translate="yes" xml:space="preserve">
          <source>The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44e5d228f37bd8405cfefcfa348ce3d27d939cb4" translate="yes" xml:space="preserve">
          <source>The copy of UCI ML Wine Data Set dataset is downloaded and modified to fit</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e11d4d7608fe1fa28c8a673b1389ac6181cb7877" translate="yes" xml:space="preserve">
          <source>The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights \(w_1\), \(w_2\), &amp;hellip;, \(w_N\) to each of the training samples. Initially, those weights are all set to \(w_i = 1/N\), so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence &lt;a href=&quot;#htf&quot; id=&quot;id10&quot;&gt;[HTF]&lt;/a&gt;.</source>
          <target state="translated">Основной принцип AdaBoost состоит в том, чтобы подогнать последовательность слабых учеников (т. Е. Моделей, которые лишь немного лучше, чем случайное предположение, например, небольшие деревья решений) на повторно изменяемых версиях данных. Прогнозы от всех из них затем объединяются посредством взвешенного большинства голосов (или суммы) для получения окончательного прогноза. Модификации данных на каждой так называемой итерации повышения состоят в применении весов \ (w_1 \), \ (w_2 \),&amp;hellip;, \ (w_N \) к каждой из обучающих выборок. Первоначально все эти веса установлены на \ (w_i = 1 / N \), так что первый шаг просто обучает слабого ученика на исходных данных. Для каждой последующей итерации веса выборки индивидуально изменяются, и алгоритм обучения повторно применяется к повторно взвешенным данным. На данном шагете обучающие примеры, которые были неправильно предсказаны усиленной моделью, созданной на предыдущем шаге, имеют увеличенные веса, тогда как веса уменьшаются для тех, которые были предсказаны правильно. По мере продолжения итераций, примеры, которые трудно предсказать, получают все большее влияние. Таким образом, каждый последующий слабый ученик вынужден концентрироваться на примерах, которые упускают предыдущие в последовательности.&lt;a href=&quot;#htf&quot; id=&quot;id10&quot;&gt;[HTF]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="2a94b871b5efce504e5077120d25fb83e224b7ee" translate="yes" xml:space="preserve">
          <source>The corpus is a collection of \(D\) documents.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="81757ae28a30627b83ded3cb76d108f655541caf" translate="yes" xml:space="preserve">
          <source>The correlation between each regressor and the target is computed, that is, ((X[:, i] - mean(X[:, i])) * (y - mean_y)) / (std(X[:, i]) * std(y)).</source>
          <target state="translated">Вычисляется корреляция между каждым регрессором и целью,т.е.((X[:,i]-среднее(X[:,i]))*(y-mean_y)*)/(std(X[:,i])*std(y)).</target>
        </trans-unit>
        <trans-unit id="302d596f93db30183d7b9abc1999d9cfb95f2d1f" translate="yes" xml:space="preserve">
          <source>The corresponding image is:</source>
          <target state="translated">Соответствующее изображение:</target>
        </trans-unit>
        <trans-unit id="9a9313289c48f29b6d60eb0302d43d099de5f93d" translate="yes" xml:space="preserve">
          <source>The corresponding training labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8f6395cc147644bb3051e08a46acbc9cd47145af" translate="yes" xml:space="preserve">
          <source>The cosine distance is defined as &lt;code&gt;1 - cosine_similarity&lt;/code&gt;: the lowest value is 0 (identical point) but it is bounded above by 2 for the farthest points. Its value does not depend on the norm of the vector points but only on their relative angles.</source>
          <target state="translated">&lt;code&gt;1 - cosine_similarity&lt;/code&gt; расстояние определяется как 1 - cosine_similarity : наименьшее значение равно 0 (идентичная точка), но оно ограничено сверху 2 для самых дальних точек. Его величина не зависит от нормы векторных точек, а только от их относительных углов.</target>
        </trans-unit>
        <trans-unit id="c4b95b481d689de7dde58522ad60a16deab9f841" translate="yes" xml:space="preserve">
          <source>The cosine distance is equivalent to the half the squared euclidean distance if each sample is normalized to unit norm</source>
          <target state="translated">Расстояние косинуса эквивалентно половине квадратного расстояния эвклида,если каждый образец нормализуется к единице нормы.</target>
        </trans-unit>
        <trans-unit id="59fadedc84e4bf70f9182651c0ff609a02cebdec" translate="yes" xml:space="preserve">
          <source>The cost complexity measure of a single node is \(R_\alpha(t)=R(t)+\alpha\). The branch, \(T_t\), is defined to be a tree where node \(t\) is its root. In general, the impurity of a node is greater than the sum of impurities of its terminal nodes, \(R(T_t)&amp;lt;R(t)\). However, the cost complexity measure of a node, \(t\), and its branch, \(T_t\), can be equal depending on \(\alpha\). We define the effective \(\alpha\) of a node to be the value where they are equal, \(R_\alpha(T_t)=R_\alpha(t)\) or \(\alpha_{eff}(t)=\frac{R(t)-R(T_t)}{|T|-1}\). A non-terminal node with the smallest value of \(\alpha_{eff}\) is the weakest link and will be pruned. This process stops when the pruned tree&amp;rsquo;s minimal \(\alpha_{eff}\) is greater than the &lt;code&gt;ccp_alpha&lt;/code&gt; parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de99d6d126cf03f253c85bcda8fea4fdd47045eb" translate="yes" xml:space="preserve">
          <source>The cost function of an isomap embedding is</source>
          <target state="translated">Функция стоимости встраивания изомапы состоит в том,чтобы</target>
        </trans-unit>
        <trans-unit id="40200d49debb9a752670d9f4e91dc77cd9d66f0a" translate="yes" xml:space="preserve">
          <source>The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.</source>
          <target state="translated">Стоимость использования дерева (т.е.прогнозирования данных)логарифмически зависит от количества точек данных,используемых для обучения дерева.</target>
        </trans-unit>
        <trans-unit id="c9b6813659bffb0c08aeb974fcecdaaf97e786b0" translate="yes" xml:space="preserve">
          <source>The covariance matrix of a data set is known to be well approximated by the classical &lt;em&gt;maximum likelihood estimator&lt;/em&gt; (or &amp;ldquo;empirical covariance&amp;rdquo;), provided the number of observations is large enough compared to the number of features (the variables describing the observations). More precisely, the Maximum Likelihood Estimator of a sample is an unbiased estimator of the corresponding population&amp;rsquo;s covariance matrix.</source>
          <target state="translated">Ковариационная матрица набора данных, как известно, хорошо аппроксимируется классической &lt;em&gt;оценкой максимального правдоподобия&lt;/em&gt; (или &amp;laquo;эмпирической ковариацией&amp;raquo;) при условии, что количество наблюдений достаточно велико по сравнению с количеством характеристик (переменных, описывающих наблюдения). Точнее, оценщик максимального правдоподобия выборки - это несмещенный оценщик ковариационной матрицы соответствующей совокупности.</target>
        </trans-unit>
        <trans-unit id="ff9747571dfcab155502f0d51c20ced0a4eda87c" translate="yes" xml:space="preserve">
          <source>The covariance matrix will be this value times the unit matrix. This dataset only produces symmetric normal distributions.</source>
          <target state="translated">Ковариационная матрица будет представлять собой значение,умноженное на единицу матрицы.Этот набор данных производит только симметричные нормальные распределения.</target>
        </trans-unit>
        <trans-unit id="d4f25f1f739fb6455b136c8d6d48a91032e89970" translate="yes" xml:space="preserve">
          <source>The covariance of each mixture component. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">Ковариация каждого компонента смеси. Форма зависит от &lt;code&gt;covariance_type&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="37e0aa90ea88b46d75d1f0f3b55b29aae27815f8" translate="yes" xml:space="preserve">
          <source>The covariance to compare with.</source>
          <target state="translated">Ковариантность для сравнения.</target>
        </trans-unit>
        <trans-unit id="a8664fdb20de58ef24fb2b254f60a073e2dad9f4" translate="yes" xml:space="preserve">
          <source>The cross decomposition module contains two main families of algorithms: the partial least squares (PLS) and the canonical correlation analysis (CCA).</source>
          <target state="translated">Модуль кросс-декомпозиции содержит два основных семейства алгоритмов:частичные наименьшие квадраты (PLS)и канонический корреляционный анализ (CCA).</target>
        </trans-unit>
        <trans-unit id="5725093c72b0120c682574584b056d0b295efdd9" translate="yes" xml:space="preserve">
          <source>The cross validation score obtained on the training data</source>
          <target state="translated">Перекрестное подтверждение баллов,полученных по данным обучения</target>
        </trans-unit>
        <trans-unit id="7d9ee6d7f0b44964f013a45e604e6c29cc8a3f8f" translate="yes" xml:space="preserve">
          <source>The cross-validation can then be performed easily:</source>
          <target state="translated">В этом случае перекрестная проверка может быть выполнена легко:</target>
        </trans-unit>
        <trans-unit id="58fa7e854fcd94e774f9d9c000c917c76d680086" translate="yes" xml:space="preserve">
          <source>The cross-validation involved in Platt scaling is an expensive operation for large datasets. In addition, the probability estimates may be inconsistent with the scores:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b91dc81955b1dbc1a750c7c0ae1e516f1cb5789e" translate="yes" xml:space="preserve">
          <source>The cross-validation score can be directly calculated using the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; helper. Given an estimator, the cross-validation object and the input dataset, the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; splits the data repeatedly into a training and a testing set, trains the estimator using the training set and computes the scores based on the testing set for each iteration of cross-validation.</source>
          <target state="translated">Оценка перекрестной проверки может быть вычислена напрямую с &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; помощника cross_val_score . Учитывая оценщик, объект перекрестной проверки и входной набор данных, &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; многократно&lt;/a&gt; разделяет данные на обучающий и тестовый набор, обучает оценщик, используя обучающий набор, и вычисляет оценки на основе набора тестирования для каждой итерации перекрестной проверки. Проверка.</target>
        </trans-unit>
        <trans-unit id="0c9e67494c034c3aceceb77302443d5c2bb54301" translate="yes" xml:space="preserve">
          <source>The cross-validation scores such that &lt;code&gt;grid_scores_[i]&lt;/code&gt; corresponds to the CV score of the i-th subset of features.</source>
          <target state="translated">Баллы перекрестной проверки такие, что &lt;code&gt;grid_scores_[i]&lt;/code&gt; соответствует баллу CV i-го подмножества функций.</target>
        </trans-unit>
        <trans-unit id="7186db76e260a93ceff6e7a012a18028bce16f5b" translate="yes" xml:space="preserve">
          <source>The current implementation uses ball trees and kd-trees to determine the neighborhood of points, which avoids calculating the full distance matrix (as was done in scikit-learn versions before 0.14). The possibility to use custom metrics is retained; for details, see &lt;code&gt;NearestNeighbors&lt;/code&gt;.</source>
          <target state="translated">Текущая реализация использует шаровые деревья и kd-деревья для определения окрестности точек, что позволяет избежать вычисления полной матрицы расстояний (как это было сделано в версиях scikit-learn до 0.14). Сохранена возможность использования пользовательских метрик; подробности см . в разделе &amp;laquo; &lt;code&gt;NearestNeighbors&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e3ec89890503565d6b8dae28b11789379133ea78" translate="yes" xml:space="preserve">
          <source>The current loss computed with the loss function.</source>
          <target state="translated">Текущий убыток,вычисленный с помощью функции потерь.</target>
        </trans-unit>
        <trans-unit id="a415ef6f77731ce51df5a94c5015135ebd3f462c" translate="yes" xml:space="preserve">
          <source>The curse of dimensionality</source>
          <target state="translated">Проклятие размерности</target>
        </trans-unit>
        <trans-unit id="3eaad00bc0bba0577db9b826b646317a24a90409" translate="yes" xml:space="preserve">
          <source>The data</source>
          <target state="translated">Данные</target>
        </trans-unit>
        <trans-unit id="973622d96c176a0a77b0dde8f6175466c5afec2c" translate="yes" xml:space="preserve">
          <source>The data is always a 2D array, shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt;, although the original data may have had a different shape. In the case of the digits, each original sample is an image of shape &lt;code&gt;(8, 8)&lt;/code&gt; and can be accessed using:</source>
          <target state="translated">Данные всегда представляют собой двумерный массив формы &lt;code&gt;(n_samples, n_features)&lt;/code&gt; , хотя исходные данные могли иметь другую форму. В случае цифр каждый исходный образец представляет собой изображение формы &lt;code&gt;(8, 8)&lt;/code&gt; и к нему можно получить доступ, используя:</target>
        </trans-unit>
        <trans-unit id="0e6af14908ee7619e613da95e5ef5e8434ef93d9" translate="yes" xml:space="preserve">
          <source>The data is assumed to be non-negative, and is often normalized to have an L1-norm of one. The normalization is rationalized with the connection to the chi squared distance, which is a distance between discrete probability distributions.</source>
          <target state="translated">Предполагается,что данные являются неотрицательными,и часто нормируются как L1-нормы.Нормализация рационализируется соединением с расстоянием в квадрате чи,которое представляет собой расстояние между дискретными распределениями вероятностей.</target>
        </trans-unit>
        <trans-unit id="aa2161d468d436ce31e1b1a4a535d19d4acf7995" translate="yes" xml:space="preserve">
          <source>The data is generated with the &lt;code&gt;make_checkerboard&lt;/code&gt; function, then shuffled and passed to the Spectral Biclustering algorithm. The rows and columns of the shuffled matrix are rearranged to show the biclusters found by the algorithm.</source>
          <target state="translated">Данные генерируются с &lt;code&gt;make_checkerboard&lt;/code&gt; функции make_checkerboard , затем перемешиваются и передаются алгоритму Spectral Biclustering. Строки и столбцы перемешанной матрицы переупорядочиваются, чтобы показать бикластеры, найденные алгоритмом.</target>
        </trans-unit>
        <trans-unit id="cbdb41e13849fa39e43a0c0831b005495a0d0342" translate="yes" xml:space="preserve">
          <source>The data is split according to the cv parameter. Each sample belongs to exactly one test set, and its prediction is computed with an estimator fitted on the corresponding training set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="549dd87c3787b6b83f2456e4cff9d8aa392d12c1" translate="yes" xml:space="preserve">
          <source>The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine.</source>
          <target state="translated">Данные являются результатами химического анализа вин,выращенных в одном и том же регионе Италии тремя разными культиваторами.В трех типах вин было проведено тринадцать различных измерений для различных составляющих.</target>
        </trans-unit>
        <trans-unit id="92b1cf272aa9964f4bcd0a98099303474f1eb6db" translate="yes" xml:space="preserve">
          <source>The data list to learn.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="279fbf7584e00e713412e8ff4546300536ffeb5a" translate="yes" xml:space="preserve">
          <source>The data matrix</source>
          <target state="translated">Матрица данных</target>
        </trans-unit>
        <trans-unit id="8a93d3171d08360f1c2b565ec2f68ceaa0cb40fb" translate="yes" xml:space="preserve">
          <source>The data matrix to learn.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a36ea8c8057a9ae699a69f099ee316c3295afaf3" translate="yes" xml:space="preserve">
          <source>The data matrix, with p features and n samples. The data set must be the one which was used to compute the raw estimates.</source>
          <target state="translated">Матрица данных,с p-функциями и n-ю пробами.Набор данных должен быть тем,который использовался для вычисления необработанных оценок.</target>
        </trans-unit>
        <trans-unit id="f0ed1480aeed96966c83d245c0e82839723677ea" translate="yes" xml:space="preserve">
          <source>The data matrix.</source>
          <target state="translated">Матрица данных.</target>
        </trans-unit>
        <trans-unit id="3e01104c886db328397aa7550432fcf04c711803" translate="yes" xml:space="preserve">
          <source>The data matrix. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;data&lt;/code&gt; will be a pandas DataFrame.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70cee63c42777050ee29bb5c41f5ed813382b6c9" translate="yes" xml:space="preserve">
          <source>The data of the returned sparse matrix. By default it is int</source>
          <target state="translated">Данные возвращаемой разреженной матрицы.По умолчанию это int</target>
        </trans-unit>
        <trans-unit id="13b8870b16632bb144f99a987da0a17b912fd261" translate="yes" xml:space="preserve">
          <source>The data of the returned sparse matrix. By default it is the dtype of img</source>
          <target state="translated">Данные возвращаемой разреженной матрицы.По умолчанию это dtype img</target>
        </trans-unit>
        <trans-unit id="a9c1f6d9c961581c21421066f6ce8f7c709084a9" translate="yes" xml:space="preserve">
          <source>The data on which &lt;code&gt;gbrt&lt;/code&gt; was trained.</source>
          <target state="translated">Данные, на которых &lt;code&gt;gbrt&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c66d20ce5405cff4e02a00c321afd4016f52379a" translate="yes" xml:space="preserve">
          <source>The data on which &lt;code&gt;gbrt&lt;/code&gt; was trained. It is used to generate a &lt;code&gt;grid&lt;/code&gt; for the &lt;code&gt;target_variables&lt;/code&gt;. The &lt;code&gt;grid&lt;/code&gt; comprises &lt;code&gt;grid_resolution&lt;/code&gt; equally spaced points between the two &lt;code&gt;percentiles&lt;/code&gt;.</source>
          <target state="translated">Данные, на которых &lt;code&gt;gbrt&lt;/code&gt; . Он используется для создания &lt;code&gt;grid&lt;/code&gt; для &lt;code&gt;target_variables&lt;/code&gt; . В &lt;code&gt;grid&lt;/code&gt; содержит &lt;code&gt;grid_resolution&lt;/code&gt; равномерно распределенные точки между двумя &lt;code&gt;percentiles&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="2ffaf51af185adce5e53447e39433481435abf03" translate="yes" xml:space="preserve">
          <source>The data samples transformed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65c0e8f3b46fedbf2f41b7e1589997d8de30db77" translate="yes" xml:space="preserve">
          <source>The data set contains images of hand-written digits: 10 classes where each class refers to a digit.</source>
          <target state="translated">Набор данных содержит изображения рукописных цифр:10 классов,где каждый класс обозначает цифру.</target>
        </trans-unit>
        <trans-unit id="a49ccf8db4843f7af3fe75d2e385d4c5ae4a247b" translate="yes" xml:space="preserve">
          <source>The data that should be scaled.</source>
          <target state="translated">Данные,которые должны быть масштабированы.</target>
        </trans-unit>
        <trans-unit id="4362ce6dec3d09ef8de03aa7af16c30845dd1b85" translate="yes" xml:space="preserve">
          <source>The data that should be transformed back.</source>
          <target state="translated">Данные,которые должны быть преобразованы обратно.</target>
        </trans-unit>
        <trans-unit id="47ba75ee664e21a51401b4c1203a08e7a876f44e" translate="yes" xml:space="preserve">
          <source>The data to be transformed by subset.</source>
          <target state="translated">Данные,которые должны быть преобразованы по подмножествам.</target>
        </trans-unit>
        <trans-unit id="03bb048cfbbc3212fd60edf266a3822d3176ef87" translate="yes" xml:space="preserve">
          <source>The data to be transformed using a power transformation.</source>
          <target state="translated">Данные,которые должны быть преобразованы с помощью преобразования силы.</target>
        </trans-unit>
        <trans-unit id="73ce115221fb870e05c90b8f043ca278fdee74a1" translate="yes" xml:space="preserve">
          <source>The data to be transformed.</source>
          <target state="translated">Данные,которые должны быть преобразованы.</target>
        </trans-unit>
        <trans-unit id="df7f442215a9627450351a5239ccb7b837681c69" translate="yes" xml:space="preserve">
          <source>The data to binarize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">Данные для бинаризации,по элементам.матрицы scipy.sparse должны быть в формате CSR,чтобы избежать ненужной копии.</target>
        </trans-unit>
        <trans-unit id="5401bf0fdb954957c9a3f345344e508dbd6bac54" translate="yes" xml:space="preserve">
          <source>The data to binarize, element by element. scipy.sparse matrices should be in CSR or CSC format to avoid an un-necessary copy.</source>
          <target state="translated">Данные для бинаризации,по элементам.матрицы scipy.sparse должны быть в формате CSR или CSC,чтобы избежать ненужной копии.</target>
        </trans-unit>
        <trans-unit id="515108ae5aad51ada5b234ccff65d93cdd42711e" translate="yes" xml:space="preserve">
          <source>The data to center and scale.</source>
          <target state="translated">Данные по центру и масштабу.</target>
        </trans-unit>
        <trans-unit id="975edd1be086184330a8a3ebbf935588408d4a98" translate="yes" xml:space="preserve">
          <source>The data to determine the categories of each feature.</source>
          <target state="translated">Данные для определения категорий каждого элемента.</target>
        </trans-unit>
        <trans-unit id="a77a7e1c81043cc97f700ad0d213253f486b563a" translate="yes" xml:space="preserve">
          <source>The data to encode.</source>
          <target state="translated">Данные для кодирования.</target>
        </trans-unit>
        <trans-unit id="d843c1d7e5e8986d5ad23251cff2a94ef8ee7955" translate="yes" xml:space="preserve">
          <source>The data to fit.</source>
          <target state="translated">Данные подойдут.</target>
        </trans-unit>
        <trans-unit id="e7fbe72ba2fcf8fff679b5e28b0fef9589e7589d" translate="yes" xml:space="preserve">
          <source>The data to fit. Can be for example a list, or an array.</source>
          <target state="translated">Данные подойдут.Может быть,например,список или массив.</target>
        </trans-unit>
        <trans-unit id="00725d5de44209593a99040fefb16433276154c2" translate="yes" xml:space="preserve">
          <source>The data to fit. Can be, for example a list, or an array at least 2d.</source>
          <target state="translated">Данные подойдут.Может быть,например,список или массив не менее 2d.</target>
        </trans-unit>
        <trans-unit id="53846ac28d489b02b9949ef562a942f522e52d48" translate="yes" xml:space="preserve">
          <source>The data to normalize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">Данные для нормализации,элемент за элементом.scipy.sparse матрицы должны быть в формате CSR,чтобы избежать ненужной копии.</target>
        </trans-unit>
        <trans-unit id="c13e1ef3ee3dcba79bb567a64bd572c32814ce01" translate="yes" xml:space="preserve">
          <source>The data to normalize, row by row. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">Данные для нормализации,ряд за рядом.scipy.sparse матрицы должны быть в формате CSR,чтобы избежать ненужной копии.</target>
        </trans-unit>
        <trans-unit id="89435a85b63a4550536a3843494521cdfd812011" translate="yes" xml:space="preserve">
          <source>The data to transform, row by row.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a092ffb0fb8aec81b90c0802c73b2ae6cfa80dcc" translate="yes" xml:space="preserve">
          <source>The data to transform, row by row. Sparse input should preferably be in CSC format.</source>
          <target state="translated">Данные для преобразования,ряд за рядом.Раздельный ввод предпочтительно должен быть в формате CSC.</target>
        </trans-unit>
        <trans-unit id="3f0555e5cf2c3f2eb80a653fe1f67911e223ec90" translate="yes" xml:space="preserve">
          <source>The data to transform.</source>
          <target state="translated">Данные для преобразования.</target>
        </trans-unit>
        <trans-unit id="f0a016c9443bbddef85796fd3691360a3c817ea0" translate="yes" xml:space="preserve">
          <source>The data used to compute the mean and standard deviation used for later scaling along the features axis.</source>
          <target state="translated">Данные,используемые для вычисления среднего и среднеквадратического отклонения,используемого для последующего масштабирования вдоль оси элементов.</target>
        </trans-unit>
        <trans-unit id="55438370e51e74e9e8aa5e6ed6a37b2c111bd898" translate="yes" xml:space="preserve">
          <source>The data used to compute the median and quantiles used for later scaling along the features axis.</source>
          <target state="translated">Данные,используемые для вычисления медианы и квантилей,используемых для последующего масштабирования по оси характеристик.</target>
        </trans-unit>
        <trans-unit id="0ae3bec0fd19bdece5355d54cf80d1ae7bcc7c26" translate="yes" xml:space="preserve">
          <source>The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.</source>
          <target state="translated">Данные,используемые для расчета минимальной и максимальной температуры на единицу площади,используемой для последующего масштабирования вдоль оси характеристик.</target>
        </trans-unit>
        <trans-unit id="e26762b4f27727d3210d3fe2a64ad24b366c1062" translate="yes" xml:space="preserve">
          <source>The data used to estimate the optimal transformation parameters.</source>
          <target state="translated">Данные,используемые для оценки оптимальных параметров преобразования.</target>
        </trans-unit>
        <trans-unit id="56c320de81ba4f246c360453cbd2da4e63d63c7f" translate="yes" xml:space="preserve">
          <source>The data used to fit the model. If &lt;code&gt;copy_X=False&lt;/code&gt;, then &lt;code&gt;X_fit_&lt;/code&gt; is a reference. This attribute is used for the calls to transform.</source>
          <target state="translated">Данные используются для соответствия модели. Если &lt;code&gt;copy_X=False&lt;/code&gt; , то &lt;code&gt;X_fit_&lt;/code&gt; является ссылкой. Этот атрибут используется для вызовов преобразования.</target>
        </trans-unit>
        <trans-unit id="03376550c822ad450e2d14840686210c4804ebc6" translate="yes" xml:space="preserve">
          <source>The data used to scale along the features axis.</source>
          <target state="translated">Данные,используемые для масштабирования вдоль оси характеристик.</target>
        </trans-unit>
        <trans-unit id="c4374e3fd9ee0863ad791a18672a80272e651c9f" translate="yes" xml:space="preserve">
          <source>The data used to scale along the features axis. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csc_matrix&lt;/code&gt;. Additionally, the sparse matrix needs to be nonnegative if &lt;code&gt;ignore_implicit_zeros&lt;/code&gt; is False.</source>
          <target state="translated">Данные, используемые для масштабирования по оси функций. Если предоставляется разреженная матрица, она будет преобразована в разреженную &lt;code&gt;csc_matrix&lt;/code&gt; . Кроме того, разреженная матрица должна быть неотрицательной, если &lt;code&gt;ignore_implicit_zeros&lt;/code&gt; имеет значение False.</target>
        </trans-unit>
        <trans-unit id="848647eb355f23af31ef1bf06cbb1e0d945eea47" translate="yes" xml:space="preserve">
          <source>The data used to scale along the specified axis.</source>
          <target state="translated">Данные,используемые для масштабирования вдоль указанной оси.</target>
        </trans-unit>
        <trans-unit id="db728424571c6d0aa73c940cbe613870747b4067" translate="yes" xml:space="preserve">
          <source>The data was used with many others for comparing various classifiers. The classes are separable, though only RDA has achieved 100% correct classification. (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) (All results using the leave-one-out technique)</source>
          <target state="translated">Эти данные были использованы вместе со многими другими для сравнения различных классификаторов.Классы разделяемы,хотя только RDA достигла 100% правильной классификации.(RDA:100%,QDA 99.4%,LDA 98.9%,1NN 96.1% (z-трансформированные данные)).(Все результаты с использованием техники &quot;Оставь на потом&quot;)</target>
        </trans-unit>
        <trans-unit id="660fe59aa4f1107f7d968ac4c0471ddbff4317b5" translate="yes" xml:space="preserve">
          <source>The data.</source>
          <target state="translated">Данные.</target>
        </trans-unit>
        <trans-unit id="02bda070eb404181c9796aadeceacf78aac05356" translate="yes" xml:space="preserve">
          <source>The dataset can be compressed, either by removing exact duplicates if these occur in your data, or by using BIRCH. Then you only have a relatively small number of representatives for a large number of points. You can then provide a &lt;code&gt;sample_weight&lt;/code&gt; when fitting DBSCAN.</source>
          <target state="translated">Набор данных можно сжать, удалив точные дубликаты, если они встречаются в ваших данных, или используя BIRCH. Тогда у вас будет относительно небольшое количество представителей для большого количества очков. Затем вы можете &lt;code&gt;sample_weight&lt;/code&gt; при установке DBSCAN.</target>
        </trans-unit>
        <trans-unit id="6bbac4d66b74c67e38ab2b8e6e075ffda05e6522" translate="yes" xml:space="preserve">
          <source>The dataset is called &amp;ldquo;Twenty Newsgroups&amp;rdquo;. Here is the official description, quoted from the &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;website&lt;/a&gt;:</source>
          <target state="translated">Набор данных называется &amp;laquo;Двадцать групп новостей&amp;raquo;. Вот официальное описание, цитируемое с &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;сайта&lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="c4573e45f21f00151e7512eaf1b6ae1cd41e22bd" translate="yes" xml:space="preserve">
          <source>The dataset is from Zhu et al [1].</source>
          <target state="translated">Набор данных получен из Чжу и др.[1].</target>
        </trans-unit>
        <trans-unit id="1a602d6b3832c4acaa760453be6cb4d26914f03f" translate="yes" xml:space="preserve">
          <source>The dataset is generated using the &lt;code&gt;make_biclusters&lt;/code&gt; function, which creates a matrix of small values and implants bicluster with large values. The rows and columns are then shuffled and passed to the Spectral Co-Clustering algorithm. Rearranging the shuffled matrix to make biclusters contiguous shows how accurately the algorithm found the biclusters.</source>
          <target state="translated">Набор данных создается с &lt;code&gt;make_biclusters&lt;/code&gt; функции make_biclusters , которая создает матрицу малых значений и имплантирует бикластер с большими значениями. Затем строки и столбцы перемешиваются и передаются алгоритму Spectral Co-Clustering. Перестановка перемешанной матрицы для получения смежных бикластеров показывает, насколько точно алгоритм нашел бикластеры.</target>
        </trans-unit>
        <trans-unit id="4131a4e690502c19383d02f0465052ba8df886e1" translate="yes" xml:space="preserve">
          <source>The dataset is structured such that points nearby in index order are nearby in parameter space, leading to an approximately block-diagonal matrix of K-nearest neighbors. Such a sparse graph is useful in a variety of circumstances which make use of spatial relationships between points for unsupervised learning: in particular, see &lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt;&lt;code&gt;sklearn.manifold.Isomap&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;sklearn.manifold.LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt;&lt;code&gt;sklearn.cluster.SpectralClustering&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Набор данных структурирован таким образом, что соседние точки в порядке индекса находятся рядом в пространстве параметров, что приводит к приблизительно блочно-диагональной матрице K-ближайших соседей. Такой разреженный граф полезен во множестве обстоятельств, которые используют пространственные отношения между точками для обучения без &lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt; &lt;code&gt;sklearn.manifold.Isomap&lt;/code&gt; &lt;/a&gt; : в частности, см. Sklearn.manifold.Isomap , &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;sklearn.manifold.LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt; и &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt; &lt;code&gt;sklearn.cluster.SpectralClustering&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="774649dea26c780c5e43d2464445b85ef137fcca" translate="yes" xml:space="preserve">
          <source>The dataset is the Boston Housing dataset (resp. 20 Newsgroups) for regression (resp. classification).</source>
          <target state="translated">Набор данных-это Бостонский жилищный набор данных (или 20 групп новостей)для регрессии (или классификации).</target>
        </trans-unit>
        <trans-unit id="c0f7ef482ab49522d3cd87d64587ff804f6727c1" translate="yes" xml:space="preserve">
          <source>The dataset used for evaluation is a 2D grid of isotropic Gaussian clusters widely spaced.</source>
          <target state="translated">Набор данных,используемый для оценки,представляет собой двухмерную сетку изотропных гауссовских кластеров,расположенных на большом расстоянии друг от друга.</target>
        </trans-unit>
        <trans-unit id="c07599d3a55d8254ba468cc109830533370f5dcf" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is Reuters-21578 as provided by the UCI ML repository. It will be automatically downloaded and uncompressed on first run.</source>
          <target state="translated">В данном примере используется набор данных Reuters-21578,предоставленный репозиторием UCI ML.Он будет автоматически загружен и распакован при первом запуске.</target>
        </trans-unit>
        <trans-unit id="f610543312a82d7ead69951d61b82950d647ad3c" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is a preprocessed excerpt of the &amp;ldquo;Labeled Faces in the Wild&amp;rdquo;, aka &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;:</source>
          <target state="translated">Набор данных, используемый в этом примере, представляет собой предварительно обработанный отрывок из &amp;laquo;Лица с метками в дикой природе&amp;raquo;, также известного как &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="257296081e1c4b1a90a4ff7a918d81d5746f6705" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is a preprocessed excerpt of the &amp;ldquo;Labeled Faces in the Wild&amp;rdquo;, also known as &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;:</source>
          <target state="translated">Набор данных, используемый в этом примере, представляет собой предварительно обработанный отрывок из &amp;laquo;Маркированных лиц в дикой природе&amp;raquo;, также известного как &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="0e491b7a83df0aa734f40ae016e056d9d5ef3d92" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is the 20 newsgroups dataset which will be automatically downloaded and then cached and reused for the document classification example.</source>
          <target state="translated">Набор данных,используемый в этом примере-это набор данных 20 новостных групп,который будет автоматически загружен,а затем кэширован и повторно использован для примера классификации документов.</target>
        </trans-unit>
        <trans-unit id="b391209f39032c4c37a7d267548501e64198b514" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is the 20 newsgroups dataset. It will be automatically downloaded, then cached.</source>
          <target state="translated">В этом примере используется набор данных 20 групп новостей.Он будет автоматически загружен,затем кэширован.</target>
        </trans-unit>
        <trans-unit id="b6261e60f08853c79c52801fadd1554784335b15" translate="yes" xml:space="preserve">
          <source>The dataset used is the Wine Dataset available at UCI. This dataset has continuous features that are heterogeneous in scale due to differing properties that they measure (i.e alcohol content, and malic acid).</source>
          <target state="translated">Используется набор данных Wine Dataset,доступный в UCI.Этот набор данных имеет непрерывные характеристики,которые являются неоднородными по масштабу в связи с различными свойствами,которые они измеряют (например,содержание алкоголя и яблочной кислоты).</target>
        </trans-unit>
        <trans-unit id="2320c3b81471635dac46a3209d55417f5e2427c2" translate="yes" xml:space="preserve">
          <source>The dataset will be downloaded from the &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume5/lewis04a/&quot;&gt;rcv1 homepage&lt;/a&gt; if necessary. The compressed size is about 656 MB.</source>
          <target state="translated">При необходимости набор данных будет загружен с &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume5/lewis04a/&quot;&gt;домашней страницы rcv1&lt;/a&gt; . Сжатый размер составляет около 656 МБ.</target>
        </trans-unit>
        <trans-unit id="aadaaf4f9f002ac5fc13c03e41ad65b9ea025276" translate="yes" xml:space="preserve">
          <source>The dataset: wages</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a104c196a5a6b4961911da3fe75edb0c176425f0" translate="yes" xml:space="preserve">
          <source>The datasets also contain a full description in their &lt;code&gt;DESCR&lt;/code&gt; attribute and some contain &lt;code&gt;feature_names&lt;/code&gt; and &lt;code&gt;target_names&lt;/code&gt;. See the dataset descriptions below for details.</source>
          <target state="translated">Наборы данных также содержит полное описание в их &lt;code&gt;DESCR&lt;/code&gt; атрибут и некоторые содержат &lt;code&gt;feature_names&lt;/code&gt; и &lt;code&gt;target_names&lt;/code&gt; . Подробности см. В описании наборов данных ниже.</target>
        </trans-unit>
        <trans-unit id="266ce3ccfa4a57091d07a7fe8bfc007064ff062e" translate="yes" xml:space="preserve">
          <source>The decision function computed the final estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26c7542245412a79b0296d695d58227ed1491d25" translate="yes" xml:space="preserve">
          <source>The decision function is equal (up to a constant factor) to the log-posterior of the model, i.e. &lt;code&gt;log p(y = k | x)&lt;/code&gt;. In a binary classification setting this instead corresponds to the difference &lt;code&gt;log p(y = 1 | x) - log p(y = 0 | x)&lt;/code&gt;. See &lt;a href=&quot;../lda_qda#lda-qda-math&quot;&gt;Mathematical formulation of the LDA and QDA classifiers&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f5da29d39d3005818dbe52be4adbd180f7105540" translate="yes" xml:space="preserve">
          <source>The decision function is:</source>
          <target state="translated">Функция принятия решений:</target>
        </trans-unit>
        <trans-unit id="fdcfe75a710515596cfa4af6113e103288190672" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13c22c73db1fc45ace2498b233349bae422df959" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0e6f8aea5cc6c4291ed60eb8cccabc092233917" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The columns correspond to the classes in sorted order, as they appear in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">Решающая функция входных выборок. Столбцы соответствуют классам в отсортированном порядке, как они указаны в атрибуте &lt;code&gt;classes_&lt;/code&gt; . Регрессия и двоичная классификация являются частными случаями с &lt;code&gt;k == 1&lt;/code&gt; , иначе &lt;code&gt;k==n_classes&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ae769935874da2534c2ccb6ca3e5794a84e4d439" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of outputs is the same of that of the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt; attribute. Binary classification is a special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;. For binary classification, values closer to -1 or 1 mean more like the first or second class in &lt;code&gt;classes_&lt;/code&gt;, respectively.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06ac27333a17bb0e5a70b7bdd7a4a5eafb52284d" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of outputs is the same of that of the &lt;code&gt;classes_&lt;/code&gt; attribute. Binary classification is a special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;. For binary classification, values closer to -1 or 1 mean more like the first or second class in &lt;code&gt;classes_&lt;/code&gt;, respectively.</source>
          <target state="translated">Решающая функция входных выборок. Порядок вывода такой же, как &lt;code&gt;classes_&lt;/code&gt; атрибута classes_ . Бинарная классификация - это особый случай с &lt;code&gt;k == 1&lt;/code&gt; , иначе &lt;code&gt;k==n_classes&lt;/code&gt; . Для двоичной классификации значения, близкие к -1 или 1, больше похожи на первый или второй класс в &lt;code&gt;classes_&lt;/code&gt; соответственно.</target>
        </trans-unit>
        <trans-unit id="7a79704b70e2487a7278def4173b216c7abdf67d" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="271144c81e33420b0281bbd1b4c29fbb1374c6ac" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">Решающая функция входных выборок. Порядок классов соответствует &lt;code&gt;classes_&lt;/code&gt; в атрибуте classes_ . Регрессия и двоичная классификация являются частными случаями с &lt;code&gt;k == 1&lt;/code&gt; , иначе &lt;code&gt;k==n_classes&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c04fb9b1c64a374fbc2424dc1dc2e69edec92fae" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="translated">Решающая функция входных выборок. Порядок классов соответствует &lt;code&gt;classes_&lt;/code&gt; в атрибуте classes_ . Регрессия и двоичная классификация создают массив формы [n_samples].</target>
        </trans-unit>
        <trans-unit id="7a4d1c2d06404b468f740213ed27426daa73066a" translate="yes" xml:space="preserve">
          <source>The decision rule for Bernoulli naive Bayes is based on</source>
          <target state="translated">Правило принятия решения для Бернулли Найв Бэйз основано на следующем</target>
        </trans-unit>
        <trans-unit id="5c243c13540ddb2ab9ba5e07515f5ccc5bddcdc4" translate="yes" xml:space="preserve">
          <source>The decision tree estimator to be exported. It can be an instance of DecisionTreeClassifier or DecisionTreeRegressor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0e0b3ca71cdfb4dae3b0e05518ff98bb98da9e44" translate="yes" xml:space="preserve">
          <source>The decision tree structure can be analysed to gain further insight on the relation between the features and the target to predict. In this example, we show how to retrieve:</source>
          <target state="translated">Структура дерева решений может быть проанализирована,чтобы получить дальнейшее представление о связи между характеристиками и целью для прогнозирования.В этом примере мы покажем,как извлечь:</target>
        </trans-unit>
        <trans-unit id="7930dea6ad7960124a06f25452c69dc408a6af03" translate="yes" xml:space="preserve">
          <source>The decision tree to be exported to GraphViz.</source>
          <target state="translated">Дерево решений,которое будет экспортировано в GraphViz.</target>
        </trans-unit>
        <trans-unit id="096b758652a55c20db43d0b3794cde42f9ae935e" translate="yes" xml:space="preserve">
          <source>The decision tree to be plotted.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="41d54cbac8ff4bb674faa62fe3a273da70a2c8ec" translate="yes" xml:space="preserve">
          <source>The decision values for the samples are computed by adding the normalized sum of pair-wise classification confidence levels to the votes in order to disambiguate between the decision values when the votes for all the classes are equal leading to a tie.</source>
          <target state="translated">Значения решений для выборки вычисляются путем сложения нормализованной суммы парных доверительных уровней классификации к голосам для того,чтобы разделить значения решений,когда голоса для всех классов равны,что приводит к ничьей.</target>
        </trans-unit>
        <trans-unit id="9db4a0cde49703606f721a9d732403d53161167d" translate="yes" xml:space="preserve">
          <source>The decoding strategy depends on the vectorizer parameters.</source>
          <target state="translated">Стратегия декодирования зависит от параметров векторизатора.</target>
        </trans-unit>
        <trans-unit id="8ab5c7033c87e7cea9882c536950d8e60cf30550" translate="yes" xml:space="preserve">
          <source>The default coding of images is based on the &lt;code&gt;uint8&lt;/code&gt; dtype to spare memory. Often machine learning algorithms work best if the input is converted to a floating point representation first. Also, if you plan to use &lt;code&gt;matplotlib.pyplpt.imshow&lt;/code&gt; don&amp;rsquo;t forget to scale to the range 0 - 1 as done in the following example.</source>
          <target state="translated">Кодирование изображений по умолчанию основано на dtype &lt;code&gt;uint8&lt;/code&gt; для экономии памяти. Часто алгоритмы машинного обучения работают лучше всего, если входные данные сначала преобразуются в представление с плавающей запятой. Кроме того, если вы планируете использовать &lt;code&gt;matplotlib.pyplpt.imshow&lt;/code&gt; , не забудьте масштабировать до диапазона 0&amp;ndash;1, как показано в следующем примере.</target>
        </trans-unit>
        <trans-unit id="ec537a685ef4f8a1d5936e658cd9e5b93a584d77" translate="yes" xml:space="preserve">
          <source>The default configuration tokenizes the string by extracting words of at least 2 letters. The specific function that does this step can be requested explicitly:</source>
          <target state="translated">В конфигурации по умолчанию токенов строки извлекаются слова как минимум из 2-х букв.Конкретная функция,выполняющая этот шаг,может быть запрошена явно:</target>
        </trans-unit>
        <trans-unit id="3d025c574ea3cc62ecf87d5b0a9f83d7b3c75d6a" translate="yes" xml:space="preserve">
          <source>The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module &lt;a href=&quot;../classes#module-sklearn.model_selection&quot;&gt;&lt;code&gt;sklearn.model_selection&lt;/code&gt;&lt;/a&gt; module for the list of possible cross-validation objects.</source>
          <target state="translated">Генератор перекрестной проверки по умолчанию - это стратифицированные K-складки. Если указано целое число, то это количество использованных складок. См. В модуле &lt;a href=&quot;../classes#module-sklearn.model_selection&quot;&gt; &lt;code&gt;sklearn.model_selection&lt;/code&gt; &lt;/a&gt; список возможных объектов перекрестной проверки.</target>
        </trans-unit>
        <trans-unit id="24e256639376dfcfda93b74d1117680d7d248e59" translate="yes" xml:space="preserve">
          <source>The default dataset is the 20 newsgroups dataset. To run the example on the digits dataset, pass the &lt;code&gt;--use-digits-dataset&lt;/code&gt; command line argument to this script.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="53050da8c5d49a141b5e2d80a70c0fa67fd7eb00" translate="yes" xml:space="preserve">
          <source>The default dataset is the digits dataset. To run the example on the twenty newsgroups dataset, pass the &amp;ndash;twenty-newsgroups command line argument to this script.</source>
          <target state="translated">Набор данных по умолчанию - это набор данных цифр. Чтобы запустить этот пример для набора данных из двадцати групп новостей, передайте этому сценарию аргумент командной строки &amp;ndash;twenty-newsgroups.</target>
        </trans-unit>
        <trans-unit id="8018aa0e057c3b81a78860eae3f5892dde5e1c45" translate="yes" xml:space="preserve">
          <source>The default error message is, &amp;ldquo;This %(name)s instance is not fitted yet. Call &amp;lsquo;fit&amp;rsquo; with appropriate arguments before using this estimator.&amp;rdquo;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="713044bd0fefe6c8ff38e7a00814412176e5caf0" translate="yes" xml:space="preserve">
          <source>The default error message is, &amp;ldquo;This %(name)s instance is not fitted yet. Call &amp;lsquo;fit&amp;rsquo; with appropriate arguments before using this method.&amp;rdquo;</source>
          <target state="translated">Сообщение об ошибке по умолчанию: &amp;laquo;Этот экземпляр% (name) s еще не установлен. Перед использованием этого метода вызовите &amp;laquo;fit&amp;raquo; с соответствующими аргументами &amp;raquo;.</target>
        </trans-unit>
        <trans-unit id="bd1a08c69ea4f3f536ea9f567759a2eca8958b5b" translate="yes" xml:space="preserve">
          <source>The default parameters (n_samples / n_features / n_components) should make the example runnable in a couple of tens of seconds. You can try to increase the dimensions of the problem, but be aware that the time complexity is polynomial in NMF. In LDA, the time complexity is proportional to (n_samples * iterations).</source>
          <target state="translated">Параметры по умолчанию (n_samples/n_features/n_components)должны сделать пример выполнимым за пару десятков секунд.Можно попробовать увеличить размеры проблемы,но имейте в виду,что в NMF временная сложность является полиномиальной.В LDA временная сложность пропорциональна (n_образцов*итераций).</target>
        </trans-unit>
        <trans-unit id="6bb3234d6819185e21b8b8e0778371ddb1ae1e23" translate="yes" xml:space="preserve">
          <source>The default setting is &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt;. The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net &lt;a href=&quot;#id15&quot; id=&quot;id1&quot;&gt;11&lt;/a&gt; solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter &lt;code&gt;l1_ratio&lt;/code&gt; controls the convex combination of L1 and L2 penalty.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e964bf4dcf7d784d0dee842886fe27c5060b1af1" translate="yes" xml:space="preserve">
          <source>The default setting is &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt;. The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter &lt;code&gt;l1_ratio&lt;/code&gt; controls the convex combination of L1 and L2 penalty.</source>
          <target state="translated">Значение по умолчанию - &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; . Штраф L1 приводит к разреженным решениям, сводя большинство коэффициентов к нулю. Elastic Net устраняет некоторые недостатки штрафа L1 при наличии сильно коррелированных атрибутов. Параметр &lt;code&gt;l1_ratio&lt;/code&gt; управляет выпуклой комбинацией штрафов L1 и L2.</target>
        </trans-unit>
        <trans-unit id="d600d29df52c52da3fc6f38fdcf73036ecf8cbdf" translate="yes" xml:space="preserve">
          <source>The default slice is a rectangular shape around the face, removing most of the background:</source>
          <target state="translated">По умолчанию фрагмент имеет прямоугольную форму вокруг лица,удаляя большую часть фона:</target>
        </trans-unit>
        <trans-unit id="f2f8fa0d6181b12438bd7660924ca4c2f89302ad" translate="yes" xml:space="preserve">
          <source>The default solver is &amp;lsquo;svd&amp;rsquo;. It can perform both classification and transform, and it does not rely on the calculation of the covariance matrix. This can be an advantage in situations where the number of features is large. However, the &amp;lsquo;svd&amp;rsquo; solver cannot be used with shrinkage.</source>
          <target state="translated">Решающая программа по умолчанию - svd. Он может выполнять как классификацию, так и преобразование, и он не полагается на вычисление ковариационной матрицы. Это может быть преимуществом в ситуациях, когда количество функций велико. Однако решатель 'svd' нельзя использовать с усадкой.</target>
        </trans-unit>
        <trans-unit id="2307e8c1575f885543e9c12c3e37adbd9d66c3dd" translate="yes" xml:space="preserve">
          <source>The default strategy implements one step of the bootstrapping procedure.</source>
          <target state="translated">Стратегия по умолчанию реализует один шаг процедуры загрузки.</target>
        </trans-unit>
        <trans-unit id="f17725906dcf13fd5a8abccea776a77c1270d7a9" translate="yes" xml:space="preserve">
          <source>The default value &lt;code&gt;max_features=&quot;auto&quot;&lt;/code&gt; uses &lt;code&gt;n_features&lt;/code&gt; rather than &lt;code&gt;n_features / 3&lt;/code&gt;. The latter was originally suggested in [1], whereas the former was more recently justified empirically in [2].</source>
          <target state="translated">Значение по умолчанию &lt;code&gt;max_features=&quot;auto&quot;&lt;/code&gt; использует &lt;code&gt;n_features&lt;/code&gt; , а не &lt;code&gt;n_features / 3&lt;/code&gt; . Последнее было первоначально предложено в [1], тогда как первое было недавно подтверждено эмпирически в [2].</target>
        </trans-unit>
        <trans-unit id="71b5a78b2592e847cac7e9a1c29294e2be25d4eb" translate="yes" xml:space="preserve">
          <source>The default value of &lt;code&gt;copy&lt;/code&gt; changed from False to True in 0.23.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ad5c264779356671425d0f732ca8de7347758f3" translate="yes" xml:space="preserve">
          <source>The default values for the parameters controlling the size of the trees (e.g. &lt;code&gt;max_depth&lt;/code&gt;, &lt;code&gt;min_samples_leaf&lt;/code&gt;, etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values.</source>
          <target state="translated">Значения по умолчанию для параметров, управляющих размером деревьев (например, &lt;code&gt;max_depth&lt;/code&gt; , &lt;code&gt;min_samples_leaf&lt;/code&gt; и т. Д.) , Приводят к полностью выращенным и необрезанным деревьям, которые потенциально могут быть очень большими для некоторых наборов данных. Чтобы уменьшить потребление памяти, сложность и размер деревьев следует контролировать, задавая значения этих параметров.</target>
        </trans-unit>
        <trans-unit id="4209f696edb2b5615bc39c07cc72cc1b347ebd54" translate="yes" xml:space="preserve">
          <source>The definitive description of key concepts and API elements for using scikit-learn and developing compatible tools.</source>
          <target state="translated">Окончательное описание ключевых понятий и элементов API для использования scikit-learn и разработки совместимых инструментов.</target>
        </trans-unit>
        <trans-unit id="767ee5df767908c4f8729c932c4a3d808fe558cd" translate="yes" xml:space="preserve">
          <source>The degree of the polynomial features. Default = 2.</source>
          <target state="translated">Степень полиномичности.По умолчанию=2.</target>
        </trans-unit>
        <trans-unit id="c0ae4038f8b612638e22a63002b9a0592ebf4ed6" translate="yes" xml:space="preserve">
          <source>The density of w, between 0 and 1</source>
          <target state="translated">Плотность w,между 0 и 1</target>
        </trans-unit>
        <trans-unit id="1385416aaed19b5930c017ff407294aed6e786b1" translate="yes" xml:space="preserve">
          <source>The depth of a tree is the maximum distance between the root and any leaf.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64211b28a272c8cc95fc17412f0b29158a9e57aa" translate="yes" xml:space="preserve">
          <source>The desired absolute tolerance of the result. A larger tolerance will generally lead to faster execution. Default is 0.</source>
          <target state="translated">Желаемый абсолютный допуск результата.Более высокий допуск,как правило,приводит к более быстрому выполнению.По умолчанию 0.</target>
        </trans-unit>
        <trans-unit id="d976f3ec62dae27876361a8dc74b8b1f3089859a" translate="yes" xml:space="preserve">
          <source>The desired relative tolerance of the result. A larger tolerance will generally lead to faster execution. Default is 1E-8.</source>
          <target state="translated">Желаемый относительный допуск на результат.Более высокий допуск,как правило,приводит к более быстрому выполнению.По умолчанию 1E-8.</target>
        </trans-unit>
        <trans-unit id="e06750706d15ac4ccfc7d440948abe624b2a5a8e" translate="yes" xml:space="preserve">
          <source>The diabetes dataset consists of 10 physiological variables (age, sex, weight, blood pressure) measure on 442 patients, and an indication of disease progression after one year:</source>
          <target state="translated">Набор данных по диабету состоит из 10 физиологических переменных (возраст,пол,вес,артериальное давление),измеряемых на 442 пациентах,и показаний к прогрессу заболевания через год:</target>
        </trans-unit>
        <trans-unit id="9fc4977037d4a4b0eddabbe726bd2d0de7d06481" translate="yes" xml:space="preserve">
          <source>The dict at &lt;code&gt;search.cv_results_['params'][search.best_index_]&lt;/code&gt; gives the parameter setting for the best model, that gives the highest mean score (&lt;code&gt;search.best_score_&lt;/code&gt;).</source>
          <target state="translated">В слове &lt;code&gt;search.cv_results_['params'][search.best_index_]&lt;/code&gt; указывается параметр для лучшей модели, которая дает наивысший средний балл ( &lt;code&gt;search.best_score_&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="bfaa3d320392a9f6c8addb9130a129b654790105" translate="yes" xml:space="preserve">
          <source>The dictionary atoms used for sparse coding. Lines are assumed to be normalized to unit norm.</source>
          <target state="translated">Атомы из словаря,используемые для разреженного кодирования.Предполагается,что строки нормализуются к единичной норме.</target>
        </trans-unit>
        <trans-unit id="7373631cc038017694fe3a2fdfb8d4bfc8e42605" translate="yes" xml:space="preserve">
          <source>The dictionary factor in the matrix factorization.</source>
          <target state="translated">Фактор словаря в матричной факторизации.</target>
        </trans-unit>
        <trans-unit id="09a0fc68f904b631d6aa1871e7b81e42ac764443" translate="yes" xml:space="preserve">
          <source>The dictionary is fitted on the distorted left half of the image, and subsequently used to reconstruct the right half. Note that even better performance could be achieved by fitting to an undistorted (i.e. noiseless) image, but here we start from the assumption that it is not available.</source>
          <target state="translated">Словарь помещается на искаженную левую половину изображения,а затем используется для реконструкции правой половины.Обратите внимание,что еще лучшей производительности можно было бы добиться,подогнав его под неискаженное (т.е.бесшумное)изображение,но здесь мы начинаем с предположения,что оно недоступно.</target>
        </trans-unit>
        <trans-unit id="03531d72d72ed916646e794ac15c0622fe9c9b59" translate="yes" xml:space="preserve">
          <source>The dictionary learning objects offer, via the &lt;code&gt;split_code&lt;/code&gt; parameter, the possibility to separate the positive and negative values in the results of sparse coding. This is useful when dictionary learning is used for extracting features that will be used for supervised learning, because it allows the learning algorithm to assign different weights to negative loadings of a particular atom, from to the corresponding positive loading.</source>
          <target state="translated">Объекты обучения словаря предлагают с помощью параметра &lt;code&gt;split_code&lt;/code&gt; возможность разделить положительные и отрицательные значения в результатах разреженного кодирования. Это полезно, когда обучение по словарю используется для извлечения функций, которые будут использоваться для обучения с учителем, поскольку это позволяет алгоритму обучения присваивать разные веса отрицательным нагрузкам конкретного атома, от до соответствующей положительной нагрузки.</target>
        </trans-unit>
        <trans-unit id="c5012873b1ea68e5deed2b04cdb4e90687b2c340" translate="yes" xml:space="preserve">
          <source>The dictionary matrix against which to solve the sparse coding of the data. Some of the algorithms assume normalized rows for meaningful output.</source>
          <target state="translated">Матрица словаря,с помощью которой можно решить проблему разреженного кодирования данных.Некоторые алгоритмы предполагают нормализованные строки для осмысленного вывода.</target>
        </trans-unit>
        <trans-unit id="f137f3a53fdf8d92f7463e89f6383b5a88f3c320" translate="yes" xml:space="preserve">
          <source>The dictionary with normalized components (D).</source>
          <target state="translated">Словарь с нормализованными компонентами (D).</target>
        </trans-unit>
        <trans-unit id="d49d4bb0bf7407d291ade3beb50a3cbdf55548f8" translate="yes" xml:space="preserve">
          <source>The difference between LeavePGroupsOut and GroupShuffleSplit is that the former generates splits using all subsets of size &lt;code&gt;p&lt;/code&gt; unique groups, whereas GroupShuffleSplit generates a user-determined number of random test splits, each with a user-determined fraction of unique groups.</source>
          <target state="translated">Разница между LeavePGroupsOut и GroupShuffleSplit заключается в том, что первый генерирует разбиения с использованием всех подмножеств уникальных групп размера &lt;code&gt;p&lt;/code&gt; , тогда как GroupShuffleSplit генерирует определяемое пользователем количество случайных тестовых разбиений, каждое с определяемой пользователем долей уникальных групп.</target>
        </trans-unit>
        <trans-unit id="f8e596d2cfcc0c0ae56904e18bdffdb0d463a655" translate="yes" xml:space="preserve">
          <source>The difference between LeavePGroupsOut and LeaveOneGroupOut is that the former builds the test sets with all the samples assigned to &lt;code&gt;p&lt;/code&gt; different values of the groups while the latter uses samples all assigned the same groups.</source>
          <target state="translated">Разница между LeavePGroupsOut и LeaveOneGroupOut заключается в том, что первый строит тестовые наборы со всеми образцами, назначенными &lt;code&gt;p&lt;/code&gt; различным значениям групп, а второй использует образцы, все которым назначены одни и те же группы.</target>
        </trans-unit>
        <trans-unit id="123fc66dcb39304255b8e25c6c10c654dbb2b0f3" translate="yes" xml:space="preserve">
          <source>The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of \(P(x_i \mid y)\).</source>
          <target state="translated">Различные наивные классификаторы Бэйеса отличаются,главным образом,допущениями,которые они делают в отношении распределения \(P(x_i \mid y)\).</target>
        </trans-unit>
        <trans-unit id="fc85907a08e94b0f17af8c0c01aad9ef06729185" translate="yes" xml:space="preserve">
          <source>The digits dataset is made of 1797 8x8 images of hand-written digits</source>
          <target state="translated">Набор данных из цифр 1797 года 8х8 изображений рукописных цифр</target>
        </trans-unit>
        <trans-unit id="46ad3d3589f97f144056aa3785c730497272c03e" translate="yes" xml:space="preserve">
          <source>The dimension of the projected subspace.</source>
          <target state="translated">Размер проектируемого подпространства.</target>
        </trans-unit>
        <trans-unit id="5572a493038acd1179abd12e1cd0c48e6709dea1" translate="yes" xml:space="preserve">
          <source>The dimension of the projection subspace.</source>
          <target state="translated">Размер подпространства проекции.</target>
        </trans-unit>
        <trans-unit id="984d56bddca949d9f1aa278aca87eb46a4072ffd" translate="yes" xml:space="preserve">
          <source>The dimensionality of the resulting representation is &lt;code&gt;n_out &amp;lt;= n_estimators * max_leaf_nodes&lt;/code&gt;. If &lt;code&gt;max_leaf_nodes == None&lt;/code&gt;, the number of leaf nodes is at most &lt;code&gt;n_estimators * 2 ** max_depth&lt;/code&gt;.</source>
          <target state="translated">Размерность полученного представления равна &lt;code&gt;n_out &amp;lt;= n_estimators * max_leaf_nodes&lt;/code&gt; . Если &lt;code&gt;max_leaf_nodes == None&lt;/code&gt; , количество &lt;code&gt;n_estimators * 2 ** max_depth&lt;/code&gt; узлов не превышает n_estimators * 2 ** max_depth .</target>
        </trans-unit>
        <trans-unit id="068779d9c792d7cec6633fa1a442091cb7f6f6f3" translate="yes" xml:space="preserve">
          <source>The dimensions and distribution of Random Projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset.</source>
          <target state="translated">Размеры и распределение матриц случайных проекций контролируются таким образом,чтобы сохранить парные расстояния между любыми двумя образцами набора данных.</target>
        </trans-unit>
        <trans-unit id="1fc3554ec8e7cb3510bb9bcb462301a368807c4a" translate="yes" xml:space="preserve">
          <source>The dimensions and distribution of random projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset. Thus random projection is a suitable approximation technique for distance based method.</source>
          <target state="translated">Размеры и распределение матриц случайных проекций контролируются таким образом,чтобы сохранить парные расстояния между любыми двумя образцами набора данных.Таким образом,случайная проекция является подходящим методом аппроксимации для метода,основанного на расстояниях.</target>
        </trans-unit>
        <trans-unit id="42010c6a5240a459a14ffe4007a4a9d645a82c6f" translate="yes" xml:space="preserve">
          <source>The dimensions of one patch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16b41d4b4452bd551af457fee2f8b60407215654" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet).</source>
          <target state="translated">Концентрация каждого компонента в дирихлете на распределении веса (Dirichlet).</target>
        </trans-unit>
        <trans-unit id="6b13240ddd6531c8fdb797758cabfbb4633f5e80" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet). The type depends on &lt;code&gt;weight_concentration_prior_type&lt;/code&gt;:</source>
          <target state="translated">Концентрация по дирихле каждого компонента на распределении веса (Дирихле). Тип зависит от &lt;code&gt;weight_concentration_prior_type&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="b470809fd29296951902d887ca553f032b8a40c4" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet). This is commonly called gamma in the literature. The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the mixture weights simplex. The value of the parameter must be greater than 0. If it is None, it&amp;rsquo;s set to &lt;code&gt;1. / n_components&lt;/code&gt;.</source>
          <target state="translated">Концентрация по дирихле каждого компонента на распределении веса (Дирихле). В литературе это обычно называется гамма. Более высокая концентрация помещает больше массы в центр и приведет к большему количеству активных компонентов, в то время как более низкий параметр концентрации приведет к большей массе на краю симплекса весов смеси. Значение параметра должно быть больше 0. Если нет, устанавливается значение &lt;code&gt;1. / n_components&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cbf40d766d38685ebd59a9222ccafd32de07a34f" translate="yes" xml:space="preserve">
          <source>The disadvantages of Bayesian regression include:</source>
          <target state="translated">К недостаткам Байесовской регрессии относятся:</target>
        </trans-unit>
        <trans-unit id="ec84046fbad625e7f2f2f6744eab5866f4d72369" translate="yes" xml:space="preserve">
          <source>The disadvantages of GBRT are:</source>
          <target state="translated">Недостатки GBRT:</target>
        </trans-unit>
        <trans-unit id="cfaf1a3af8c2483843ec10c36faa80968b886bf2" translate="yes" xml:space="preserve">
          <source>The disadvantages of Gaussian processes include:</source>
          <target state="translated">К недостаткам гауссовских процессов относятся:</target>
        </trans-unit>
        <trans-unit id="c8e0c1974308fc5ae09e195effcd7396a8db8601" translate="yes" xml:space="preserve">
          <source>The disadvantages of Multi-layer Perceptron (MLP) include:</source>
          <target state="translated">К недостаткам многослойного перцептрона (MLP)относятся:</target>
        </trans-unit>
        <trans-unit id="c71179ab39a085455ac6f6888a2f6fd3afe84d6f" translate="yes" xml:space="preserve">
          <source>The disadvantages of Stochastic Gradient Descent include:</source>
          <target state="translated">К недостаткам стохастического градиентного спуска относятся:</target>
        </trans-unit>
        <trans-unit id="84c28c658fb7eb00eb9347365efe97c2f04c30f3" translate="yes" xml:space="preserve">
          <source>The disadvantages of decision trees include:</source>
          <target state="translated">К недостаткам деревьев принятия решений относятся:</target>
        </trans-unit>
        <trans-unit id="47feb0b4f4476b90d13a7091a56a1b0f68ad3a62" translate="yes" xml:space="preserve">
          <source>The disadvantages of support vector machines include:</source>
          <target state="translated">К недостаткам опорных векторных машин относятся:</target>
        </trans-unit>
        <trans-unit id="266f0e7f0f21de8e86a9f2ce2320de66f17358df" translate="yes" xml:space="preserve">
          <source>The disadvantages of the LARS method include:</source>
          <target state="translated">К недостаткам метода ЛАРС относятся:</target>
        </trans-unit>
        <trans-unit id="fe9e13a1ad2c431f3f290607f881625ead9408c0" translate="yes" xml:space="preserve">
          <source>The disadvantages to using t-SNE are roughly:</source>
          <target state="translated">Недостатки использования t-SNE примерно одинаковы:</target>
        </trans-unit>
        <trans-unit id="cd2f4d25623f857cb298f31868b60b26f2d776d4" translate="yes" xml:space="preserve">
          <source>The display objects store the computed values that were passed as arguments. This allows for the visualizations to be easliy combined using matplotlib&amp;rsquo;s API. In the following example, we place the displays next to each other in a row.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a7ea673185d2a40501e91d63f573c417ac055cd1" translate="yes" xml:space="preserve">
          <source>The distance metric to use</source>
          <target state="translated">Расстояние метрическое для использования</target>
        </trans-unit>
        <trans-unit id="fade8506c426ca456f7b5f3ebd671dcd06e51421" translate="yes" xml:space="preserve">
          <source>The distance metric to use. It will be same as the &lt;code&gt;metric&lt;/code&gt; parameter or a synonym of it, e.g. &amp;lsquo;euclidean&amp;rsquo; if the &lt;code&gt;metric&lt;/code&gt; parameter set to &amp;lsquo;minkowski&amp;rsquo; and &lt;code&gt;p&lt;/code&gt; parameter set to 2.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0063e1a06973d2332dc280d77426f57f299b9005" translate="yes" xml:space="preserve">
          <source>The distance metric to use. Note that not all metrics are valid with all algorithms. Refer to the documentation of &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; for a description of available algorithms. Note that the normalization of the density output is correct only for the Euclidean distance metric. Default is &amp;lsquo;euclidean&amp;rsquo;.</source>
          <target state="translated">Используемая метрика расстояния. Обратите внимание, что не все показатели действительны для всех алгоритмов. Обратитесь к документации &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt; и &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt; за описанием доступных алгоритмов. Обратите внимание, что нормализация вывода плотности верна только для метрики евклидова расстояния. По умолчанию - &amp;laquo;евклидово&amp;raquo;.</target>
        </trans-unit>
        <trans-unit id="3c4790f8b52fd2ea34a9c9dfcea72e27e7f13da0" translate="yes" xml:space="preserve">
          <source>The distance metric used to calculate the k-Neighbors for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is &amp;lsquo;euclidean&amp;rsquo; (&amp;lsquo;minkowski&amp;rsquo; metric with the p param equal to 2.)</source>
          <target state="translated">Метрика расстояния, используемая для вычисления k-соседей для каждой точки выборки. Класс DistanceMetric предоставляет список доступных показателей. Расстояние по умолчанию - &amp;laquo;евклидово&amp;raquo; (метрика Минковского с параметром p, равным 2.)</target>
        </trans-unit>
        <trans-unit id="f687007319ee4f163c7b6668932fdc048712a932" translate="yes" xml:space="preserve">
          <source>The distance metric used to calculate the neighbors within a given radius for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is &amp;lsquo;euclidean&amp;rsquo; (&amp;lsquo;minkowski&amp;rsquo; metric with the param equal to 2.)</source>
          <target state="translated">Метрика расстояния, используемая для вычисления соседей в пределах заданного радиуса для каждой точки выборки. Класс DistanceMetric предоставляет список доступных показателей. Расстояние по умолчанию - евклидово (метрика Минковского с параметром, равным 2.)</target>
        </trans-unit>
        <trans-unit id="8c96fb568d2fd154ab3cf2ef1e009cb92f123321" translate="yes" xml:space="preserve">
          <source>The distance metric used. It will be same as the &lt;code&gt;metric&lt;/code&gt; parameter or a synonym of it, e.g. &amp;lsquo;euclidean&amp;rsquo; if the &lt;code&gt;metric&lt;/code&gt; parameter set to &amp;lsquo;minkowski&amp;rsquo; and &lt;code&gt;p&lt;/code&gt; parameter set to 2.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a36fd43a3aae80628a6e6b30a7a5575b2bb66390" translate="yes" xml:space="preserve">
          <source>The distances between the row vectors of &lt;code&gt;X&lt;/code&gt; and the row vectors of &lt;code&gt;Y&lt;/code&gt; can be evaluated using &lt;a href=&quot;generated/sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;pairwise_distances&lt;/code&gt;&lt;/a&gt;. If &lt;code&gt;Y&lt;/code&gt; is omitted the pairwise distances of the row vectors of &lt;code&gt;X&lt;/code&gt; are calculated. Similarly, &lt;a href=&quot;generated/sklearn.metrics.pairwise.pairwise_kernels#sklearn.metrics.pairwise.pairwise_kernels&quot;&gt;&lt;code&gt;pairwise.pairwise_kernels&lt;/code&gt;&lt;/a&gt; can be used to calculate the kernel between &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; using different kernel functions. See the API reference for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e12a027c556a152948ba7c771e2cb65d4ac3e99" translate="yes" xml:space="preserve">
          <source>The distinct labels used in classifying instances.</source>
          <target state="translated">Отдельные этикетки,используемые при классификации экземпляров.</target>
        </trans-unit>
        <trans-unit id="2333c2cc37e50157e20c383f74b00f781cb37a1b" translate="yes" xml:space="preserve">
          <source>The distortion introduced by a random projection &lt;code&gt;p&lt;/code&gt; is asserted by the fact that &lt;code&gt;p&lt;/code&gt; is defining an eps-embedding with good probability as defined by:</source>
          <target state="translated">Искажение, вносимое случайной проекцией &lt;code&gt;p&lt;/code&gt; , подтверждается тем фактом, что &lt;code&gt;p&lt;/code&gt; определяет eps-вложение с хорошей вероятностью, как определено следующим образом:</target>
        </trans-unit>
        <trans-unit id="befef2d542b10663820383e2f1e59843332d14a0" translate="yes" xml:space="preserve">
          <source>The distortion introduced by a random projection &lt;code&gt;p&lt;/code&gt; only changes the distance between two points by a factor (1 +- eps) in an euclidean space with good probability. The projection &lt;code&gt;p&lt;/code&gt; is an eps-embedding as defined by:</source>
          <target state="translated">Искажение, вносимое случайной проекцией &lt;code&gt;p&lt;/code&gt; , изменяет расстояние между двумя точками только на коэффициент (1 + - eps) в евклидовом пространстве с хорошей вероятностью. Проекция &lt;code&gt;p&lt;/code&gt; - это eps-вложение, как определено:</target>
        </trans-unit>
        <trans-unit id="b6d88e183a439e17415e4b06c82155c9e34fa344" translate="yes" xml:space="preserve">
          <source>The distributions in &lt;code&gt;scipy.stats&lt;/code&gt; prior to version scipy 0.16 do not allow specifying a random state. Instead, they use the global numpy random state, that can be seeded via &lt;code&gt;np.random.seed&lt;/code&gt; or set using &lt;code&gt;np.random.set_state&lt;/code&gt;. However, beginning scikit-learn 0.18, the &lt;a href=&quot;classes#module-sklearn.model_selection&quot;&gt;&lt;code&gt;sklearn.model_selection&lt;/code&gt;&lt;/a&gt; module sets the random state provided by the user if scipy &amp;gt;= 0.16 is also available.</source>
          <target state="translated">Распределения в &lt;code&gt;scipy.stats&lt;/code&gt; до версии scipy 0.16 не позволяют указывать случайное состояние. Вместо этого они используют глобальное случайное состояние numpy, которое может быть &lt;code&gt;np.random.seed&lt;/code&gt; через np.random.seed или установлено с помощью &lt;code&gt;np.random.set_state&lt;/code&gt; . Однако, начиная с scikit-learn 0.18, модуль &lt;a href=&quot;classes#module-sklearn.model_selection&quot;&gt; &lt;code&gt;sklearn.model_selection&lt;/code&gt; &lt;/a&gt; устанавливает случайное состояние, предоставляемое пользователем, если также доступен scipy&amp;gt; = 0.16.</target>
        </trans-unit>
        <trans-unit id="e4d3a4f449c4df99f288fa9346370fef4e50edc3" translate="yes" xml:space="preserve">
          <source>The dual gap at the end of the optimization for the optimal alpha (&lt;code&gt;alpha_&lt;/code&gt;).</source>
          <target state="translated">Двойной разрыв в конце оптимизации для оптимального альфа ( &lt;code&gt;alpha_&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="1c5b7990e4f078095970dbd44954a4ff0f6bdb22" translate="yes" xml:space="preserve">
          <source>The dual gaps at the end of the optimization for each alpha.</source>
          <target state="translated">Двойные пробелы в конце оптимизации для каждого альфа.</target>
        </trans-unit>
        <trans-unit id="eceb770d3ab0f05ce2328559f3653f372ef1e2c9" translate="yes" xml:space="preserve">
          <source>The dual problem is</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa2b6f5781584ece88d8eb541efa4219ed937f23" translate="yes" xml:space="preserve">
          <source>The dual problem to the primal is</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c16a68f2d6169f4f9aa27b4b94f9b3db38d73c1f" translate="yes" xml:space="preserve">
          <source>The dummy regression model predicts a constant frequency. This model does not attribute the same tied rank to all samples but is none-the-less globally well calibrated (to estimate the mean frequency of the entire population).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b5eac57a33eeb0d09a6d5ebfcbe611bef2da07d8" translate="yes" xml:space="preserve">
          <source>The edges of each bin. Contain arrays of varying shapes &lt;code&gt;(n_bins_, )&lt;/code&gt; Ignored features will have empty arrays.</source>
          <target state="translated">Края каждого бункера. Содержат массивы различной формы &lt;code&gt;(n_bins_, )&lt;/code&gt; Игнорируемые функции будут иметь пустые массивы.</target>
        </trans-unit>
        <trans-unit id="50c07dc6eab301b9d9953e2bd5849c6aa3abe8d7" translate="yes" xml:space="preserve">
          <source>The effect of the transformer is weaker than on the synthetic data. However, the transform induces a decrease of the MAE.</source>
          <target state="translated">Влияние трансформатора слабее,чем на синтетические данные.Однако преобразование вызывает снижение МАЭ.</target>
        </trans-unit>
        <trans-unit id="8b7256d0b08ed885751aee08f85ac36bb70ae336" translate="yes" xml:space="preserve">
          <source>The effective size of the batch is computed here. If there are no more jobs to dispatch, return False, else return True.</source>
          <target state="translated">Эффективный размер партии рассчитывается здесь.Если больше нет заданий для отправки,верните False,иначе верните True.</target>
        </trans-unit>
        <trans-unit id="a044cf1265a1684d79e7b48898279d20ad6f0dac" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities</source>
          <target state="translated">Стратегия разложения по собственному значению.AMG требует установки пиамга.Он может быть быстрее при очень больших и редких проблемах,но может также привести к нестабильности.</target>
        </trans-unit>
        <trans-unit id="1dc685b8ff4dd3ccaa26f5f653c44ef0324bd82f" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities.</source>
          <target state="translated">Стратегия разложения по собственному значению.AMG требует установки пиамга.Он может быть быстрее при очень больших и редких проблемах,но может также привести к нестабильности.</target>
        </trans-unit>
        <trans-unit id="e9d800f8ff5b9787b828397b459d27c3b21cb754" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f14ed9796539634c3492800089020cfe2ebbd7af" translate="yes" xml:space="preserve">
          <source>The elastic net optimization function varies for mono and multi-outputs.</source>
          <target state="translated">Функция оптимизации эластичной сети варьируется для моно-и мульти-выходов.</target>
        </trans-unit>
        <trans-unit id="4d9a50594cec642f130c977c2c5c095fc619b302" translate="yes" xml:space="preserve">
          <source>The elements of the estimators parameter, having been fitted on the training data. If an estimator has been set to &lt;code&gt;'drop'&lt;/code&gt;, it will not appear in &lt;code&gt;estimators_&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0191353bc3bca50b627784afbd555eb11c1f3dd" translate="yes" xml:space="preserve">
          <source>The empirical covariance matrix of a sample can be computed using the &lt;a href=&quot;generated/sklearn.covariance.empirical_covariance#sklearn.covariance.empirical_covariance&quot;&gt;&lt;code&gt;empirical_covariance&lt;/code&gt;&lt;/a&gt; function of the package, or by fitting an &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt;&lt;code&gt;EmpiricalCovariance&lt;/code&gt;&lt;/a&gt; object to the data sample with the &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.fit&quot;&gt;&lt;code&gt;EmpiricalCovariance.fit&lt;/code&gt;&lt;/a&gt; method. Be careful that results depend on whether the data are centered, so one may want to use the &lt;code&gt;assume_centered&lt;/code&gt; parameter accurately. More precisely, if &lt;code&gt;assume_centered=False&lt;/code&gt;, then the test set is supposed to have the same mean vector as the training set. If not, both should be centered by the user, and &lt;code&gt;assume_centered=True&lt;/code&gt; should be used.</source>
          <target state="translated">Матрица эмпирической ковариации выборки может быть вычислена с использованием функции &lt;a href=&quot;generated/sklearn.covariance.empirical_covariance#sklearn.covariance.empirical_covariance&quot;&gt; &lt;code&gt;empirical_covariance&lt;/code&gt; &lt;/a&gt; пакета или путем подгонки объекта &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt; &lt;code&gt;EmpiricalCovariance&lt;/code&gt; &lt;/a&gt; к выборке данных с помощью метода &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.fit&quot;&gt; &lt;code&gt;EmpiricalCovariance.fit&lt;/code&gt; &lt;/a&gt; . Будьте осторожны, чтобы результаты зависели от того, центрированы ли данные, поэтому можно точно использовать параметр &lt;code&gt;assume_centered&lt;/code&gt; . Точнее, если &lt;code&gt;assume_centered=False&lt;/code&gt; , то предполагается, что тестовый набор имеет тот же средний вектор, что и обучающий набор. В противном случае оба должны быть центрированы пользователем, и следует использовать &lt;code&gt;assume_centered=True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d4b0da98d5cb047aa932ef62858edca40e188517" translate="yes" xml:space="preserve">
          <source>The encoded signal (Y).</source>
          <target state="translated">Кодированный сигнал (Y).</target>
        </trans-unit>
        <trans-unit id="cc2fdf3023f61dc3df7bba02538cce691e7cf2cd" translate="yes" xml:space="preserve">
          <source>The energy function measures the quality of a joint assignment:</source>
          <target state="translated">Энергетическая функция измеряет качество совместного задания:</target>
        </trans-unit>
        <trans-unit id="1bfb5cae50ef0b1032c729ea4ab68ff502e7f906" translate="yes" xml:space="preserve">
          <source>The entry &lt;code&gt;test_fold[i]&lt;/code&gt; represents the index of the test set that sample &lt;code&gt;i&lt;/code&gt; belongs to. It is possible to exclude sample &lt;code&gt;i&lt;/code&gt; from any test set (i.e. include sample &lt;code&gt;i&lt;/code&gt; in every training set) by setting &lt;code&gt;test_fold[i]&lt;/code&gt; equal to -1.</source>
          <target state="translated">Запись &lt;code&gt;test_fold[i]&lt;/code&gt; представляет индекс тестового набора, которому принадлежит образец &lt;code&gt;i&lt;/code&gt; . Можно исключить образец &lt;code&gt;i&lt;/code&gt; из любого набора тестов (т.е. включить образец &lt;code&gt;i&lt;/code&gt; в каждый обучающий набор), установив &lt;code&gt;test_fold[i]&lt;/code&gt; равным -1.</target>
        </trans-unit>
        <trans-unit id="503425d1f7e07b98f32316b1d85343640a4e1294" translate="yes" xml:space="preserve">
          <source>The equivalence between &lt;code&gt;alpha&lt;/code&gt; and the regularization parameter of SVM, &lt;code&gt;C&lt;/code&gt; is given by &lt;code&gt;alpha = 1 / C&lt;/code&gt; or &lt;code&gt;alpha = 1 / (n_samples * C)&lt;/code&gt;, depending on the estimator and the exact objective function optimized by the model.</source>
          <target state="translated">Эквивалентность между &lt;code&gt;alpha&lt;/code&gt; и параметром регуляризации SVM, &lt;code&gt;C&lt;/code&gt; задается как &lt;code&gt;alpha = 1 / C&lt;/code&gt; или &lt;code&gt;alpha = 1 / (n_samples * C)&lt;/code&gt; , в зависимости от оценки и точной целевой функции, оптимизированной моделью.</target>
        </trans-unit>
        <trans-unit id="2fec157ea85a1f513b11852f437f8dc13c9361bb" translate="yes" xml:space="preserve">
          <source>The error message or a substring of the error message.</source>
          <target state="translated">Сообщение об ошибке или подстрока сообщения об ошибке.</target>
        </trans-unit>
        <trans-unit id="67a9569df158acc253e75d9fa812424d1f2b58c6" translate="yes" xml:space="preserve">
          <source>The estimated (sparse) precision matrix.</source>
          <target state="translated">Оценочная (разреженная)матрица точности.</target>
        </trans-unit>
        <trans-unit id="cdf0d65945f589fd5da3781e66a0a81a30e934b1" translate="yes" xml:space="preserve">
          <source>The estimated covariance matrix.</source>
          <target state="translated">Оценочная ковариационная матрица.</target>
        </trans-unit>
        <trans-unit id="4b02a2f0d176ac6fb9b484de9fcc2a6b0be67a5a" translate="yes" xml:space="preserve">
          <source>The estimated labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="77a18d9bfc3d1fc8448c4fb559b59c860611bbc7" translate="yes" xml:space="preserve">
          <source>The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See &amp;ldquo;Pattern Recognition and Machine Learning&amp;rdquo; by C. Bishop, 12.2.1 p. 574 or &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;.</source>
          <target state="translated">Расчетная ковариация шума по модели Probabilistic PCA от Tipping and Bishop, 1999. См. &amp;laquo;Распознавание образов и машинное обучение&amp;raquo; К. Бишопа, 12.2.1 с. 574 или &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="735cdf6c1c61f38b1092aa1bd4262b34c0d5f0f3" translate="yes" xml:space="preserve">
          <source>The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See &amp;ldquo;Pattern Recognition and Machine Learning&amp;rdquo; by C. Bishop, 12.2.1 p. 574 or &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;. It is required to compute the estimated data covariance and score samples.</source>
          <target state="translated">Расчетная ковариация шума по модели Probabilistic PCA от Tipping and Bishop, 1999. См. &amp;laquo;Распознавание образов и машинное обучение&amp;raquo; К. Бишопа, 12.2.1 с. 574 или &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt; . Требуется вычислить оценочную ковариацию данных и выборки оценок.</target>
        </trans-unit>
        <trans-unit id="cc49065d7efcb582d87899f7997c53b1f8d60c78" translate="yes" xml:space="preserve">
          <source>The estimated noise variance for each feature.</source>
          <target state="translated">Расчетная дисперсия шума для каждого элемента.</target>
        </trans-unit>
        <trans-unit id="960e70b3d078f74487289ecbb537290d1f7473d4" translate="yes" xml:space="preserve">
          <source>The estimated number of components. Relevant when &lt;code&gt;n_components=None&lt;/code&gt;.</source>
          <target state="translated">Расчетное количество компонентов. Актуально, когда &lt;code&gt;n_components=None&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="df011fa4ccaad62cb4a83a0955409a4c4b3035c8" translate="yes" xml:space="preserve">
          <source>The estimated number of components. When n_components is set to &amp;lsquo;mle&amp;rsquo; or a number between 0 and 1 (with svd_solver == &amp;lsquo;full&amp;rsquo;) this number is estimated from input data. Otherwise it equals the parameter n_components, or the lesser value of n_features and n_samples if n_components is None.</source>
          <target state="translated">Расчетное количество компонентов. Когда для n_components установлено значение &amp;laquo;mle&amp;raquo; или число от 0 до 1 (с svd_solver == &amp;laquo;full&amp;raquo;), это число оценивается по входным данным. В противном случае он равен параметру n_components или меньшему значению n_features и n_samples, если n_components равно None.</target>
        </trans-unit>
        <trans-unit id="cd106f5afe35a52902113758dbc47d95941530e6" translate="yes" xml:space="preserve">
          <source>The estimated number of connected components in the graph.</source>
          <target state="translated">Оценочное количество подключенных компонентов на графике.</target>
        </trans-unit>
        <trans-unit id="54994eb61f1dce6c8894f0827b84346580c11cb2" translate="yes" xml:space="preserve">
          <source>The estimation of the EXPERIENCE coefficient is now less variable and remain important for all models trained during cross-validation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a98c4d1bc7caeb1163bf14c014655348909c79f3" translate="yes" xml:space="preserve">
          <source>The estimation of the model is done by calculating the slopes and intercepts of a subpopulation of all possible combinations of p subsample points. If an intercept is fitted, p must be greater than or equal to n_features + 1. The final slope and intercept is then defined as the spatial median of these slopes and intercepts.</source>
          <target state="translated">Оценка модели производится путем вычисления наклонов и перехватов подпопуляций всех возможных комбинаций точек p-подобразы.В случае перехвата p должно быть больше или равно n_features+1.Окончательный наклон и перехват определяются затем как пространственная медиана этих наклонов и перехватов.</target>
        </trans-unit>
        <trans-unit id="aed02faa2e2402cc32a5968b7e86a54cff535c4e" translate="yes" xml:space="preserve">
          <source>The estimation of the model is done by iteratively maximizing the marginal log-likelihood of the observations.</source>
          <target state="translated">Оценка модели осуществляется путем итеративного максимизации предельной вероятности лога наблюдений.</target>
        </trans-unit>
        <trans-unit id="44fe06813d3627aaa56531c52961387365e10d66" translate="yes" xml:space="preserve">
          <source>The estimation of the number of degrees of freedom is given by:</source>
          <target state="translated">Дается оценка числа степеней свободы:</target>
        </trans-unit>
        <trans-unit id="2bed56c931836016f42a33afdedfac5cb8f555f4" translate="yes" xml:space="preserve">
          <source>The estimator also implements &lt;code&gt;partial_fit&lt;/code&gt;, which updates the dictionary by iterating only once over a mini-batch. This can be used for online learning when the data is not readily available from the start, or for when the data does not fit into the memory.</source>
          <target state="translated">Оценщик также реализует &lt;code&gt;partial_fit&lt;/code&gt; , который обновляет словарь, выполняя итерацию только один раз в мини-пакете. Это можно использовать для онлайн-обучения, когда данные недоступны с самого начала или когда данные не помещаются в память.</target>
        </trans-unit>
        <trans-unit id="efc1e3841cf31ff119d8e77fb55d7093c1df4dd4" translate="yes" xml:space="preserve">
          <source>The estimator objects for each cv split. This is available only if &lt;code&gt;return_estimator&lt;/code&gt; parameter is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">Объекты оценщика для каждого разделения резюме. Это доступно, только если &lt;code&gt;return_estimator&lt;/code&gt; параметра return_estimator установлено значение &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f33cc8b973e26f8a535386dbd610021f87dfdf8f" translate="yes" xml:space="preserve">
          <source>The estimator or group of estimators to be cloned</source>
          <target state="translated">Оценщик или группа оценщиков для клонирования</target>
        </trans-unit>
        <trans-unit id="c1ff61d32d7a7a2deb658878854ba14950f2c891" translate="yes" xml:space="preserve">
          <source>The estimator or group of estimators to be cloned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65944411996b76786bcb63db12ac31b08b55648a" translate="yes" xml:space="preserve">
          <source>The estimator that provides the initial predictions. Set via the &lt;code&gt;init&lt;/code&gt; argument or &lt;code&gt;loss.init_estimator&lt;/code&gt;.</source>
          <target state="translated">Оценщик, который предоставляет начальные прогнозы. Устанавливается с помощью аргумента &lt;code&gt;init&lt;/code&gt; или &lt;code&gt;loss.init_estimator&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="f7ae9218cda2abb5c086ad75a4ab1e94b87ebec1" translate="yes" xml:space="preserve">
          <source>The estimator to use at each step of the round-robin imputation. If &lt;code&gt;sample_posterior&lt;/code&gt; is True, the estimator must support &lt;code&gt;return_std&lt;/code&gt; in its &lt;code&gt;predict&lt;/code&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e68c5f93929a570a29c52208fc20a3ad933e6e26" translate="yes" xml:space="preserve">
          <source>The estimator to visualize.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5314881b9357d8103e571e2b27bb56496dd3b052" translate="yes" xml:space="preserve">
          <source>The estimators of a pipeline are stored as a list in the &lt;code&gt;steps&lt;/code&gt; attribute, but can be accessed by index or name by indexing (with &lt;code&gt;[idx]&lt;/code&gt;) the Pipeline:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ade400270e658832b6e128c7ef187979eae33356" translate="yes" xml:space="preserve">
          <source>The estimators of a pipeline are stored as a list in the &lt;code&gt;steps&lt;/code&gt; attribute:</source>
          <target state="translated">Оценщики конвейера хранятся в виде списка в атрибуте &lt;code&gt;steps&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="bc948c82039e32a77f975544660f3043a6111d3d" translate="yes" xml:space="preserve">
          <source>The estimators of the pipeline can be retrieved by index:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a46349c122935a13db8091d877d4179b37995289" translate="yes" xml:space="preserve">
          <source>The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. For example, it is possible to use these estimators to turn a binary classifier or a regressor into a multiclass classifier. It is also possible to use these estimators with multiclass estimators in the hope that their accuracy or runtime performance improves.</source>
          <target state="translated">Предоставляемые в этом модуле оценщики являются мета-оценщиками:они требуют,чтобы в их конструкторе был предоставлен базовый оценщик.Например,с помощью этих вычислений можно превратить бинарный классификатор или регрессор в многоклассовый классификатор.Также можно использовать эти вычисления с многоклассовыми вычислениями в надежде на то,что их точность или производительность во время выполнения повысится.</target>
        </trans-unit>
        <trans-unit id="1868049da2c813b87c2e3d48a5c71f72cc892852" translate="yes" xml:space="preserve">
          <source>The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. The meta-estimator extends single output estimators to multioutput estimators.</source>
          <target state="translated">Предоставляемые в этом модуле оценщики являются мета-оценщиками:они требуют,чтобы в их конструкторе был предоставлен базовый оценщик.Мета-оценщик расширяет одиночные выходные оценки до многовыходных оценок.</target>
        </trans-unit>
        <trans-unit id="fe77b9f934c3d4135d1310eb25c7f2c922f35971" translate="yes" xml:space="preserve">
          <source>The exact API of all functions and classes, as given by the docstrings. The API documents expected types and allowed features for all functions, and all parameters available for the algorithms.</source>
          <target state="translated">Точное API всех функций и классов,заданное строками docstrings.API документирует ожидаемые типы и разрешенные функции для всех функций,а также все параметры,доступные для алгоритмов.</target>
        </trans-unit>
        <trans-unit id="213a31afd656f2a0c8b37dd55b40fc5308db3ca1" translate="yes" xml:space="preserve">
          <source>The exact additive chi squared kernel.</source>
          <target state="translated">Точное ядро с добавкой Чи в квадрате.</target>
        </trans-unit>
        <trans-unit id="14b1a49c77fdcb8a0d3244090e7c6df0b2a6cd07" translate="yes" xml:space="preserve">
          <source>The exact chi squared kernel.</source>
          <target state="translated">Точное ядро ци в квадрате.</target>
        </trans-unit>
        <trans-unit id="21c56a998f59a2c6be6550db485efcc395359e33" translate="yes" xml:space="preserve">
          <source>The example below demonstrates how the OOB error can be measured at the addition of each new tree during training. The resulting plot allows a practitioner to approximate a suitable value of &lt;code&gt;n_estimators&lt;/code&gt; at which the error stabilizes.</source>
          <target state="translated">Пример ниже демонстрирует, как можно измерить ошибку OOB при добавлении каждого нового дерева во время обучения. Полученный график позволяет практикующему специалисту приблизиться к подходящему значению &lt;code&gt;n_estimators&lt;/code&gt; , при котором ошибка стабилизируется.</target>
        </trans-unit>
        <trans-unit id="556b3073adbd8a84d1d4b2fe21db4807c2daf4c8" translate="yes" xml:space="preserve">
          <source>The example below uses a support vector classifier with a non-linear kernel to build a model with optimized hyperparameters by grid search. We compare the performance of non-nested and nested CV strategies by taking the difference between their scores.</source>
          <target state="translated">В примере ниже используется векторный классификатор поддержки с нелинейным ядром для построения модели с оптимизированными гиперпараметрами с помощью поиска по сетке.Мы сравниваем производительность не-вложенных и вложенных CV стратегий,беря разницу между их баллами.</target>
        </trans-unit>
        <trans-unit id="4cd198cf307950e0cb10d5d2e6b86c91624772aa" translate="yes" xml:space="preserve">
          <source>The example compares prediction result of linear regression (linear model) and decision tree (tree based model) with and without discretization of real-valued features.</source>
          <target state="translated">В примере сравнивается результат прогнозирования линейной регрессии (линейная модель)и дерева решений (древовидная модель)с дискретизацией реально-значимых признаков и без нее.</target>
        </trans-unit>
        <trans-unit id="3101cd4fceb55d1be63d1bff5cb3f45996fd9aa0" translate="yes" xml:space="preserve">
          <source>The example demonstrates syntax and speed only; it doesn&amp;rsquo;t actually do anything useful with the extracted vectors. See the example scripts {document_classification_20newsgroups,clustering}.py for actual learning on text documents.</source>
          <target state="translated">Пример демонстрирует только синтаксис и скорость; на самом деле он не делает ничего полезного с извлеченными векторами. См. Примеры сценариев {document_classification_20newsgroups, clustering} .py для фактического изучения текстовых документов.</target>
        </trans-unit>
        <trans-unit id="32ed3af5edbdaa27f9eba4e5a30255c6afab4414" translate="yes" xml:space="preserve">
          <source>The example is engineered to show the effect of the choice of different metrics. It is applied to waveforms, which can be seen as high-dimensional vector. Indeed, the difference between metrics is usually more pronounced in high dimension (in particular for euclidean and cityblock).</source>
          <target state="translated">Пример сконструирован таким образом,чтобы показать эффект от выбора различных метрик.Он применен к форме волны,которую можно рассматривать как высокоразмерный вектор.Действительно,разница между метриками,как правило,более выражена в высокой размерности (в частности,для эвклидов и цитиблоков).</target>
        </trans-unit>
        <trans-unit id="36ed82faabe70da57df289b8a54bb16e8830773a" translate="yes" xml:space="preserve">
          <source>The example shows that the predictions in ridge are strongly influenced by the outliers present in the dataset. The Huber regressor is less influenced by the outliers since the model uses the linear loss for these. As the parameter epsilon is increased for the Huber regressor, the decision function approaches that of the ridge.</source>
          <target state="translated">Пример показывает,что на предсказания в гребне сильно влияют отклонения,присутствующие в наборе данных.На регрессор Huber в меньшей степени влияют отклонения,так как в модели для них используются линейные потери.По мере увеличения параметра эпсилон для регрессора Гюбера функция принятия решения приближается к функции гребня.</target>
        </trans-unit>
        <trans-unit id="889bc091af3b0106e91a81884d921d1b2df9bdd6" translate="yes" xml:space="preserve">
          <source>The examples below compare Gaussian mixture models with a fixed number of components, to the variational Gaussian mixture models with a Dirichlet process prior. Here, a classical Gaussian mixture is fitted with 5 components on a dataset composed of 2 clusters. We can see that the variational Gaussian mixture with a Dirichlet process prior is able to limit itself to only 2 components whereas the Gaussian mixture fits the data with a fixed number of components that has to be set a priori by the user. In this case the user has selected &lt;code&gt;n_components=5&lt;/code&gt; which does not match the true generative distribution of this toy dataset. Note that with very little observations, the variational Gaussian mixture models with a Dirichlet process prior can take a conservative stand, and fit only one component.</source>
          <target state="translated">В приведенных ниже примерах сравниваются модели гауссовой смеси с фиксированным числом компонентов с вариационными моделями гауссовой смеси с предшествующим процессом Дирихле. Здесь классическая гауссова смесь оснащена 5 компонентами в наборе данных, состоящем из 2 кластеров. Мы можем видеть, что вариационная гауссова смесь с предшествующим процессом Дирихле может ограничиваться только двумя компонентами, тогда как гауссовская смесь соответствует данным с фиксированным числом компонентов, которое должно быть установлено пользователем априори. В этом случае пользователь выбрал &lt;code&gt;n_components=5&lt;/code&gt; , что не соответствует истинному генеративному распределению этого набора данных игрушек. Обратите внимание, что при очень небольшом количестве наблюдений вариационные модели гауссовой смеси с априорным процессом Дирихле могут занять консервативную позицию и соответствовать только одному компоненту.</target>
        </trans-unit>
        <trans-unit id="2cf55b61801dba638e3df4951d7127011c13c263" translate="yes" xml:space="preserve">
          <source>The expected value for the mutual information can be calculated using the following equation &lt;a href=&quot;#veb2009&quot; id=&quot;id13&quot;&gt;[VEB2009]&lt;/a&gt;. In this equation, \(a_i = |U_i|\) (the number of elements in \(U_i\)) and \(b_j = |V_j|\) (the number of elements in \(V_j\)).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e5c4a6233958dced6338c85ecba7ef5860ffcbbc" translate="yes" xml:space="preserve">
          <source>The expected value for the mutual information can be calculated using the following equation [VEB2009]. In this equation, \(a_i = |U_i|\) (the number of elements in \(U_i\)) and \(b_j = |V_j|\) (the number of elements in \(V_j\)).</source>
          <target state="translated">Ожидаемое значение для взаимной информации может быть рассчитано с помощью следующего уравнения [VEB2009].В этом уравнении \(a_i=|U_i|\)(количество элементов в \(U_i\))и \(b_j=|V__13\)(количество элементов в \(V_j\)).</target>
        </trans-unit>
        <trans-unit id="5075a63ec7be1cbe42a641e5ae277f9e4b63f160" translate="yes" xml:space="preserve">
          <source>The experiment is performed on an artificial dataset for binary classification with 100,000 samples (1,000 of them are used for model fitting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The first figure shows the estimated probabilities obtained with logistic regression, Gaussian naive Bayes, and Gaussian naive Bayes with both isotonic calibration and sigmoid calibration. The calibration performance is evaluated with Brier score, reported in the legend (the smaller the better). One can observe here that logistic regression is well calibrated while raw Gaussian naive Bayes performs very badly. This is because of the redundant features which violate the assumption of feature-independence and result in an overly confident classifier, which is indicated by the typical transposed-sigmoid curve.</source>
          <target state="translated">Эксперимент проводится на искусственном наборе данных для бинарной классификации со 100 000 образцов (1000 из них используются для подгонки модели)с 20 особенностями.Из 20 характеристик только 2 являются информативными,а 10-избыточными.На первом рисунке показаны оценочные вероятности,полученные с помощью логистической регрессии,Гауссова наивного Байеса и Гауссова наивного Байеса как с помощью изотонической калибровки,так и сигмовидной калибровки.Калибровочные характеристики оцениваются с помощью баллов Брайера,приведенных в легенде (чем меньше,тем лучше).Здесь можно заметить,что логистическая регрессия хорошо калибрована,в то время как сырая гауссовская наивная Бэйес работает очень плохо.Это происходит из-за избыточных характеристик,которые нарушают предположение о независимости характеристик и приводят к слишком уверенному классификатору,на что указывает типичная транспонированно-сигмоидная кривая.</target>
        </trans-unit>
        <trans-unit id="b61a812b707dfd6196631d345ad23c1f697f044e" translate="yes" xml:space="preserve">
          <source>The experimental data presents a long tail distribution for &lt;code&gt;y&lt;/code&gt;. In all models, we predict the expected frequency of a random variable, so we will have necessarily fewer extreme values than for the observed realizations of that random variable. This explains that the mode of the histograms of model predictions doesn&amp;rsquo;t necessarily correspond to the smallest value. Additionally, the normal distribution used in &lt;code&gt;Ridge&lt;/code&gt; has a constant variance, while for the Poisson distribution used in &lt;code&gt;PoissonRegressor&lt;/code&gt; and &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;, the variance is proportional to the predicted expected value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e1db213d528f30cf745c1554ad2a43991932646f" translate="yes" xml:space="preserve">
          <source>The explained variance or ndarray if &amp;lsquo;multioutput&amp;rsquo; is &amp;lsquo;raw_values&amp;rsquo;.</source>
          <target state="translated">Объясненная дисперсия или ndarray, если 'multioutput' - 'raw_values'.</target>
        </trans-unit>
        <trans-unit id="dff7adf6114ae66681ccc96cdb5c8abce36c0555" translate="yes" xml:space="preserve">
          <source>The explicit constant as predicted by the &amp;ldquo;constant&amp;rdquo; strategy. This parameter is useful only for the &amp;ldquo;constant&amp;rdquo; strategy.</source>
          <target state="translated">Явная константа, предсказываемая &amp;laquo;постоянной&amp;raquo; стратегией. Этот параметр полезен только для &amp;laquo;постоянной&amp;raquo; стратегии.</target>
        </trans-unit>
        <trans-unit id="96483cbe0c7b434b8387960f89d9c2700f0af59b" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate [default 0.5].</source>
          <target state="translated">Экспонент для коэффициента обучения обратному масштабированию [по умолчанию 0,5].</target>
        </trans-unit>
        <trans-unit id="fe088102cec241da3f7e07b043fd901378b61425" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1fa85799d065de026e0253585d451384c9ee6013" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to &amp;lsquo;invscaling&amp;rsquo;. Only used when solver=&amp;rsquo;sgd&amp;rsquo;.</source>
          <target state="translated">Показатель скорости обучения обратному масштабированию. Он используется для обновления эффективной скорости обучения, когда для параметра learning_rate установлено значение invscaling. Используется только когда solver = 'sgd'.</target>
        </trans-unit>
        <trans-unit id="c7acf68e47ad4fe8cc8cf54d82e56ffc97c60a11" translate="yes" xml:space="preserve">
          <source>The exponent for the base kernel</source>
          <target state="translated">Экспонент для базового ядра</target>
        </trans-unit>
        <trans-unit id="fe68de8b995171a61c550ec2f7815b01a2d28ec7" translate="yes" xml:space="preserve">
          <source>The exponentiated version of the kernel, which is usually preferable.</source>
          <target state="translated">Выраженная версия ядра,которая обычно предпочтительнее.</target>
        </trans-unit>
        <trans-unit id="d1490a4bcb15c22959ee30800b231e4413480004" translate="yes" xml:space="preserve">
          <source>The external estimator fit on the reduced dataset.</source>
          <target state="translated">Внешний оценщик подходит к сокращенному набору данных.</target>
        </trans-unit>
        <trans-unit id="18d0cd5609d8f78695dc43242ffb9cf995a65ee9" translate="yes" xml:space="preserve">
          <source>The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features):</source>
          <target state="translated">Извлеченные TF-IDF векторы очень разрежены,в среднем 159 ненулевых компонентов по образцу в более чем 30000-мерном пространстве (менее .5% ненулевых признаков):</target>
        </trans-unit>
        <trans-unit id="e5bc1ae1e0e90623cdbf461e900aded771c1b79d" translate="yes" xml:space="preserve">
          <source>The extracted dataset will only retain pictures of people that have at least &lt;code&gt;min_faces_per_person&lt;/code&gt; different pictures.</source>
          <target state="translated">Извлеченный набор данных будет содержать только фотографии людей, у которых есть хотя бы &lt;code&gt;min_faces_per_person&lt;/code&gt; разных изображений.</target>
        </trans-unit>
        <trans-unit id="413a1ee9e7f0b0741202aa45f471425edb5c2085" translate="yes" xml:space="preserve">
          <source>The extraction method used to extract clusters using the calculated reachability and ordering. Possible values are &amp;ldquo;xi&amp;rdquo; and &amp;ldquo;dbscan&amp;rdquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7cb7e9ef70987101280d538c40ac3f0463557635" translate="yes" xml:space="preserve">
          <source>The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task easier.</source>
          <target state="translated">Коэффициент,умножающий размер гиперкуба.Большие значения распределяют кластеры/классы и облегчают задачу классификации.</target>
        </trans-unit>
        <trans-unit id="65d18b0a37e6414a591311e75d9ded04a4293e5e" translate="yes" xml:space="preserve">
          <source>The factory can be any callable that takes no argument and return an instance of &lt;code&gt;ParallelBackendBase&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0cd5de0c793252ae54dc423f82b86e165f05af5f" translate="yes" xml:space="preserve">
          <source>The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher&amp;rsquo;s paper. Note that it&amp;rsquo;s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points.</source>
          <target state="translated">Знаменитая база данных Iris, впервые использованная сэром Р.А. Фишером. Набор данных взят из статьи Фишера. Обратите внимание, что это то же самое, что и в R, но не как в репозитории машинного обучения UCI, в котором есть две неправильные точки данных.</target>
        </trans-unit>
        <trans-unit id="64135ddd2f4a94ed0611a8692e8099bf4451f815" translate="yes" xml:space="preserve">
          <source>The feature (e.g. &lt;code&gt;[0]&lt;/code&gt;) or pair of interacting features (e.g. &lt;code&gt;[(0, 1)]&lt;/code&gt;) for which the partial dependency should be computed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9c1f9439f6a49e182ea739a4c5151e4783dfb88" translate="yes" xml:space="preserve">
          <source>The feature importance scores of a fit gradient boosting model can be accessed via the &lt;code&gt;feature_importances_&lt;/code&gt; property:</source>
          <target state="translated">Оценки важности функций подходящей модели повышения градиента можно получить через свойство &lt;code&gt;feature_importances_&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="29293679b6f2571a09f6b43f7ca55454fa94d5ff" translate="yes" xml:space="preserve">
          <source>The feature importances.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="596d7f9b612d1877ad65dac2d6189030efea06e6" translate="yes" xml:space="preserve">
          <source>The feature matrix &lt;code&gt;X&lt;/code&gt; should be standardized before fitting. This ensures that the penalty treats features equally.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40d3ffb0824ad292679a9b4129b9e33a1cef7858" translate="yes" xml:space="preserve">
          <source>The feature matrix. Categorical features are encoded as ordinals.</source>
          <target state="translated">Матрица функций.Категорические признаки кодируются как ординалы.</target>
        </trans-unit>
        <trans-unit id="0a487efba080872bb151e33f60795da2b55ed658" translate="yes" xml:space="preserve">
          <source>The feature ranking, such that &lt;code&gt;ranking_[i]&lt;/code&gt; corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1.</source>
          <target state="translated">Ранжирование функции, такое, что &lt;code&gt;ranking_[i]&lt;/code&gt; соответствует позиции ранжирования i-й функции. Выбранным (т.е. оцененным лучшим) характеристикам присваивается ранг 1.</target>
        </trans-unit>
        <trans-unit id="c61b5bb2da747f9d4f147f5fd2e2f8308d20750d" translate="yes" xml:space="preserve">
          <source>The features and estimators that are experimental aren&amp;rsquo;t subject to deprecation cycles. Use them at your own risks!</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="acbf27a6428b599cd8900c8ca4f4ecae3f0cc8b0" translate="yes" xml:space="preserve">
          <source>The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and &lt;code&gt;max_features=n_features&lt;/code&gt;, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, &lt;code&gt;random_state&lt;/code&gt; has to be fixed.</source>
          <target state="translated">При каждом разбиении функции всегда меняются случайным образом. Следовательно, наилучшее найденное разбиение может отличаться даже при тех же данных обучения и &lt;code&gt;max_features=n_features&lt;/code&gt; , если улучшение критерия идентично для нескольких разбиений, перечисленных во время поиска наилучшего разбиения. Чтобы получить детерминированное поведение во время подгонки, необходимо &lt;code&gt;random_state&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c25846da01d7420f77d01daefe6ea3229aead8d7" translate="yes" xml:space="preserve">
          <source>The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, &lt;code&gt;max_features=n_features&lt;/code&gt; and &lt;code&gt;bootstrap=False&lt;/code&gt;, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, &lt;code&gt;random_state&lt;/code&gt; has to be fixed.</source>
          <target state="translated">При каждом разбиении функции всегда меняются случайным образом. Следовательно, наилучшее найденное разбиение может отличаться даже с теми же данными обучения, &lt;code&gt;max_features=n_features&lt;/code&gt; и &lt;code&gt;bootstrap=False&lt;/code&gt; , если улучшение критерия идентично для нескольких разбиений, перечисленных во время поиска наилучшего разбиения. Чтобы получить детерминированное поведение во время подгонки, необходимо &lt;code&gt;random_state&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="79b5615baa935539329fdffd05466c4966130183" translate="yes" xml:space="preserve">
          <source>The features indices which will be returned when calling &lt;code&gt;transform&lt;/code&gt;. They are computed during &lt;code&gt;fit&lt;/code&gt;. For &lt;code&gt;features='all'&lt;/code&gt;, it is to &lt;code&gt;range(n_features)&lt;/code&gt;.</source>
          <target state="translated">Индексы функций, которые будут возвращены при вызове &lt;code&gt;transform&lt;/code&gt; . Они вычисляются во время &lt;code&gt;fit&lt;/code&gt; . Для &lt;code&gt;features='all'&lt;/code&gt; это &lt;code&gt;range(n_features)&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="7bb18d685bec7903d9b61cd6026a227300d22db3" translate="yes" xml:space="preserve">
          <source>The features of &lt;code&gt;X&lt;/code&gt; have been transformed from \([x_1, x_2]\) to \([1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\), and can now be used within any linear model.</source>
          <target state="translated">Характеристики &lt;code&gt;X&lt;/code&gt; были преобразованы из \ ([x_1, x_2] \) в \ ([1, x_1, x_2, x_1 ^ 2, x_1 x_2, x_2 ^ 2] \), и теперь их можно использовать в любой линейной модели. .</target>
        </trans-unit>
        <trans-unit id="b791adba0e25e8ba284f5da492871659f500ccc6" translate="yes" xml:space="preserve">
          <source>The features of X have been transformed from \((X_1, X_2)\) to \((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\).</source>
          <target state="translated">Характеристики X преобразованы с \((X_1,X_2)\)на \((1,X_1,X_2,X_1^2,X_1X_2,X_2^2)\).</target>
        </trans-unit>
        <trans-unit id="0ec9e3dee3b599b742aa9c67dee3f4cab82e4d00" translate="yes" xml:space="preserve">
          <source>The features of X have been transformed from \((X_1, X_2, X_3)\) to \((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\).</source>
          <target state="translated">Характеристики X преобразованы с \((X_1,X_2,X_3)\)на \((1,X_1,X_2,X_3,X_1X_2,X_1X_3,X_2X_3,X_1X_3,X_1X_2X_3)\).</target>
        </trans-unit>
        <trans-unit id="eaccd9379a9dbc6cce41fc318001c8bf42339abe" translate="yes" xml:space="preserve">
          <source>The figure below illustrates the effect of shrinkage and subsampling on the goodness-of-fit of the model. We can clearly see that shrinkage outperforms no-shrinkage. Subsampling with shrinkage can further increase the accuracy of the model. Subsampling without shrinkage, on the other hand, does poorly.</source>
          <target state="translated">На рисунке ниже проиллюстрировано влияние усадки и субдискретизации на хорошую подгонку модели.Мы ясно видим,что усадка превосходит усадку без усадки.Субсэмплирование с усадкой может еще больше повысить точность модели.Субдискретизация без усадки,с другой стороны,работает плохо.</target>
        </trans-unit>
        <trans-unit id="409a640cc1c72ef47cfd3f6ea68f986d77585eba" translate="yes" xml:space="preserve">
          <source>The figure below shows four one-way and one two-way partial dependence plots for the California housing dataset, with a &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50a711fcdd2b061e3d348535a1d7a4d9bddb7ed5" translate="yes" xml:space="preserve">
          <source>The figure below shows the results of applying &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; with least squares loss and 500 base learners to the Boston house price dataset (&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt;&lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt;&lt;/a&gt;). The plot on the left shows the train and test error at each iteration. The train error at each iteration is stored in the &lt;code&gt;train_score_&lt;/code&gt; attribute of the gradient boosting model. The test error at each iterations can be obtained via the &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt;&lt;code&gt;staged_predict&lt;/code&gt;&lt;/a&gt; method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. &lt;code&gt;n_estimators&lt;/code&gt;) by early stopping. The plot on the right shows the feature importances which can be obtained via the &lt;code&gt;feature_importances_&lt;/code&gt; property.</source>
          <target state="translated">На рисунке ниже показаны результаты применения &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt; с потерями по методу наименьших квадратов и 500 базовыми учащимися к набору данных о &lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt; &lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt; &lt;/a&gt; в Бостоне ( sklearn.datasets.load_boston ). На графике слева показан поезд и ошибка теста на каждой итерации. Ошибка поезда на каждой итерации сохраняется в &lt;code&gt;train_score_&lt;/code&gt; модели повышения градиента. Ошибка теста на каждой итерации может быть получена с помощью метода &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt; &lt;code&gt;staged_predict&lt;/code&gt; ,&lt;/a&gt; который возвращает генератор, который дает прогнозы на каждом этапе. &lt;code&gt;n_estimators&lt;/code&gt; можно использовать для определения оптимального количества деревьев (т.е. n_estimators ) путем ранней остановки. На графике справа показаны значения функций, которые можно получить с помощью &lt;code&gt;feature_importances_&lt;/code&gt; свойство.</target>
        </trans-unit>
        <trans-unit id="76597dd5c3aa4f5be5177f9ebf1af69e02fc15a8" translate="yes" xml:space="preserve">
          <source>The figure below shows the results of applying &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; with least squares loss and 500 base learners to the Boston house price dataset (&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt;&lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt;&lt;/a&gt;). The plot on the left shows the train and test error at each iteration. The train error at each iteration is stored in the &lt;code&gt;train_score_&lt;/code&gt; attribute of the gradient boosting model. The test error at each iterations can be obtained via the &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt;&lt;code&gt;staged_predict&lt;/code&gt;&lt;/a&gt; method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. &lt;code&gt;n_estimators&lt;/code&gt;) by early stopping. The plot on the right shows the impurity-based feature importances which can be obtained via the &lt;code&gt;feature_importances_&lt;/code&gt; property.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cc8de6d2f1f6034186ab0af10d91754eadd34954" translate="yes" xml:space="preserve">
          <source>The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly 2*pi (6.28), while KRR chooses the doubled periodicity 4*pi. Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (&amp;ldquo;curse of dimensionality&amp;rdquo;). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.</source>
          <target state="translated">Рисунок показывает, что оба метода изучают разумные модели целевой функции. GPR правильно определяет периодичность функции примерно как 2 * pi (6,28), в то время как KRR выбирает удвоенную периодичность 4 * pi. Кроме того, GPR обеспечивает разумные пределы достоверности прогноза, которые недоступны для KRR. Основное различие между этими двумя методами - время, необходимое для подбора и прогнозирования: хотя подгонка KRR в принципе выполняется быстро, поиск по сетке для оптимизации гиперпараметров экспоненциально масштабируется с увеличением числа гиперпараметров (&amp;laquo;проклятие размерности&amp;raquo;). Оптимизация параметров на основе градиента в GPR не страдает от этого экспоненциального масштабирования и, таким образом, выполняется значительно быстрее в этом примере с трехмерным пространством гиперпараметров. Время для прогнозирования аналогично; тем не мение,создание дисперсии прогнозируемого распределения георадара занимает значительно больше времени, чем просто прогнозирование среднего значения.</target>
        </trans-unit>
        <trans-unit id="7a70cd6dc569a4ad4c8d12310e45e3bf56f13e7d" translate="yes" xml:space="preserve">
          <source>The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly \(2*\pi\) (6.28), while KRR chooses the doubled periodicity \(4*\pi\) . Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (&amp;ldquo;curse of dimensionality&amp;rdquo;). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.</source>
          <target state="translated">Рисунок показывает, что оба метода изучают разумные модели целевой функции. GPR правильно определяет периодичность функции как примерно \ (2 * \ pi \) (6.28), в то время как KRR выбирает удвоенную периодичность \ (4 * \ pi \). Кроме того, GPR обеспечивает разумные пределы достоверности прогноза, которые недоступны для KRR. Основное различие между этими двумя методами - время, необходимое для подбора и прогнозирования: хотя подгонка KRR в принципе выполняется быстро, поиск по сетке для оптимизации гиперпараметров экспоненциально масштабируется с увеличением числа гиперпараметров (&amp;laquo;проклятие размерности&amp;raquo;). Оптимизация параметров на основе градиента в GPR не страдает от этого экспоненциального масштабирования и, таким образом, выполняется значительно быстрее в этом примере с трехмерным пространством гиперпараметров. Время для прогнозирования аналогично; тем не мение,создание дисперсии прогнозируемого распределения георадара занимает значительно больше времени, чем просто прогнозирование среднего значения.</target>
        </trans-unit>
        <trans-unit id="4c819c571e9cf27ccc7a880b7ae493959bac8667" translate="yes" xml:space="preserve">
          <source>The figure shows the cumulative sum of the negative OOB improvements as a function of the boosting iteration. As you can see, it tracks the test loss for the first hundred iterations but then diverges in a pessimistic way. The figure also shows the performance of 3-fold cross validation which usually gives a better estimate of the test loss but is computationally more demanding.</source>
          <target state="translated">На рисунке показана кумулятивная сумма отрицательных улучшений OOB как функция ускоряющей итерации.Как видно,она отслеживает тестовый убыток за первые сто итераций,но затем расходится пессимистическим образом.На рисунке также показана производительность 3-кратной перекрестной проверки,которая обычно дает лучшую оценку потерь теста,но является более требовательной с вычислительной точки зрения.</target>
        </trans-unit>
        <trans-unit id="37bfa29a2e04a43879779df5389399bec3580356" translate="yes" xml:space="preserve">
          <source>The figure shows the trade-off between cross-validated score and the number of PCA components. The balanced case is when n_components=10 and accuracy=0.88, which falls into the range within 1 standard deviation of the best accuracy score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc5bec6cdf25a030ca7f5736bc605af5657e6e59" translate="yes" xml:space="preserve">
          <source>The figures below are used to illustrate the effect of scaling our &lt;code&gt;C&lt;/code&gt; to compensate for the change in the number of samples, in the case of using an &lt;code&gt;l1&lt;/code&gt; penalty, as well as the &lt;code&gt;l2&lt;/code&gt; penalty.</source>
          <target state="translated">Рисунки ниже используются для иллюстрации эффекта масштабирования нашего &lt;code&gt;C&lt;/code&gt; для компенсации изменения количества выборок в случае использования штрафа &lt;code&gt;l1&lt;/code&gt; , а также штрафа &lt;code&gt;l2&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="0b0d592cc5daeb578d9b82714e3800c3092f375f" translate="yes" xml:space="preserve">
          <source>The figures illustrate the interpolating property of the Gaussian Process model as well as its probabilistic nature in the form of a pointwise 95% confidence interval.</source>
          <target state="translated">Цифры иллюстрируют интерполяционное свойство модели Гауссова процесса,а также ее вероятностную природу в виде точечного 95% доверительного интервала.</target>
        </trans-unit>
        <trans-unit id="e96dfdb18e509ef4ea2f5731757d5c9fff16f17b" translate="yes" xml:space="preserve">
          <source>The figures show the confusion matrix with and without normalization by class support size (number of elements in each class). This kind of normalization can be interesting in case of class imbalance to have a more visual interpretation of which class is being misclassified.</source>
          <target state="translated">На рисунках показана путаница с нормализацией и без нормализации по размеру поддержки класса (количество элементов в каждом классе).Подобная нормализация может быть интересна в случае классового дисбаланса,чтобы иметь более визуальную интерпретацию того,какой класс неправильно классифицируется.</target>
        </trans-unit>
        <trans-unit id="e0f5d06a7d2ed2f3c6fb0ae21cf62a8ad73fbd7f" translate="yes" xml:space="preserve">
          <source>The filenames for the images.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c01e855d5c880b7b53c9ce8ac56ca5941d4bf79" translate="yes" xml:space="preserve">
          <source>The filenames holding the dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="83d2d2e5c2f316a3531e949e8bac6210f7e21821" translate="yes" xml:space="preserve">
          <source>The files themselves are loaded in memory in the &lt;code&gt;data&lt;/code&gt; attribute. For reference the filenames are also available:</source>
          <target state="translated">Сами файлы загружаются в память в атрибуте &lt;code&gt;data&lt;/code&gt; . Для справки также доступны имена файлов:</target>
        </trans-unit>
        <trans-unit id="4834eeb362cb8f80edd0fa52b738fe7db255a20e" translate="yes" xml:space="preserve">
          <source>The filesystem path to the root folder where MLComp datasets are stored, if mlcomp_root is None, the MLCOMP_DATASETS_HOME environment variable is looked up instead.</source>
          <target state="translated">Путь файловой системы к корневой папке,в которой хранятся наборы данных MLComp,если mlcomp_root-None,то вместо этого ищется переменная окружения MLCOMP_DATASETS_HOME.</target>
        </trans-unit>
        <trans-unit id="5e83705f6f5cf8453734b26cfa75b02cefbf9a06" translate="yes" xml:space="preserve">
          <source>The final sum of similarities is divided by the size of the larger set.</source>
          <target state="translated">Окончательная сумма сходств делится на размер большего набора.</target>
        </trans-unit>
        <trans-unit id="6023cd4bbdadd0261fe4fb93bbac4f79dd623983" translate="yes" xml:space="preserve">
          <source>The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set).</source>
          <target state="translated">Окончательное значение критерия инерции (сумма квадратных расстояний до ближайшего центроида для всех наблюдений в учебном наборе).</target>
        </trans-unit>
        <trans-unit id="65db46a7a2384cdd4de97d3c70536cd9da4c8616" translate="yes" xml:space="preserve">
          <source>The final value of the stress (sum of squared distance of the disparities and the distances for all constrained points).</source>
          <target state="translated">Окончательное значение напряжения (сумма квадратного расстояния диспропорций и расстояний для всех ограниченных точек).</target>
        </trans-unit>
        <trans-unit id="379910ba877ee4fe0038b0877574596b2d4e7156" translate="yes" xml:space="preserve">
          <source>The first 4 plots use the &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt;&lt;code&gt;make_classification&lt;/code&gt;&lt;/a&gt; with different numbers of informative features, clusters per class and classes. The final 2 plots use &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt;&lt;code&gt;make_blobs&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_gaussian_quantiles#sklearn.datasets.make_gaussian_quantiles&quot;&gt;&lt;code&gt;make_gaussian_quantiles&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e7a148ceab046969e5524f650bd9948ad6ae0ac" translate="yes" xml:space="preserve">
          <source>The first &lt;code&gt;[.9, .1]&lt;/code&gt; in &lt;code&gt;y_pred&lt;/code&gt; denotes 90% probability that the first sample has label 0. The log loss is non-negative.</source>
          <target state="translated">Первые &lt;code&gt;[.9, .1]&lt;/code&gt; в &lt;code&gt;y_pred&lt;/code&gt; обозначают 90% -ную вероятность того, что первая выборка имеет метку 0. Потери в журнале неотрицательны.</target>
        </trans-unit>
        <trans-unit id="c41f4a219241dbe01041bd4816c28f057f1f8fc9" translate="yes" xml:space="preserve">
          <source>The first &lt;code&gt;n_samples % n_splits&lt;/code&gt; folds have size &lt;code&gt;n_samples // n_splits + 1&lt;/code&gt;, other folds have size &lt;code&gt;n_samples // n_splits&lt;/code&gt;, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples.</source>
          <target state="translated">Первые &lt;code&gt;n_samples % n_splits&lt;/code&gt; имеют размер &lt;code&gt;n_samples // n_splits + 1&lt;/code&gt; , другие свертки имеют размер &lt;code&gt;n_samples // n_splits&lt;/code&gt; , где &lt;code&gt;n_samples&lt;/code&gt; - количество выборок.</target>
        </trans-unit>
        <trans-unit id="318062ceb48883ba19b024b3ac85479b5d766c8b" translate="yes" xml:space="preserve">
          <source>The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices. In general, multiple points can be queried at the same time.</source>
          <target state="translated">Первый возвращаемый массив содержит расстояния до всех точек,которые ближе 1.6,а второй массив содержит их индексы.В общем случае,можно опрашивать несколько точек одновременно.</target>
        </trans-unit>
        <trans-unit id="b072f11f22e8fd40f8f011ced740f859610c5839" translate="yes" xml:space="preserve">
          <source>The first base-kernel of the product-kernel</source>
          <target state="translated">Первое базовое ядро продукта-ядро</target>
        </trans-unit>
        <trans-unit id="2843deb348cef2ebb64b5cb6c478da3efa931340" translate="yes" xml:space="preserve">
          <source>The first base-kernel of the sum-kernel</source>
          <target state="translated">Первое базовое ядро сумма-ядро</target>
        </trans-unit>
        <trans-unit id="23baf5dbd5fca89380aba285e5a6dca77d9ff4b8" translate="yes" xml:space="preserve">
          <source>The first column of images shows true faces. The next columns illustrate how extremely randomized trees, k nearest neighbors, linear regression and ridge regression complete the lower half of those faces.</source>
          <target state="translated">В первой колонке изображений изображены истинные лица.Следующие колонки иллюстрируют,как крайне рандомизированные деревья,k ближайших соседей,линейная регрессия и гребневая регрессия завершают нижнюю половину этих граней.</target>
        </trans-unit>
        <trans-unit id="758127cc4d0b762bc4a77876dd4551f141ccd7cd" translate="yes" xml:space="preserve">
          <source>The first corresponds to a model with a high noise level and a large length scale, which explains all variations in the data by noise.</source>
          <target state="translated">Первая соответствует модели с высоким уровнем шума и большой шкалой длины,что объясняет все вариации данных по шумам.</target>
        </trans-unit>
        <trans-unit id="2d71e99749591e16661c08d2c19738355ed2fad8" translate="yes" xml:space="preserve">
          <source>The first element of each line can be used to store a target variable to predict.</source>
          <target state="translated">Первый элемент каждой строки может быть использован для хранения целевой переменной для прогнозирования.</target>
        </trans-unit>
        <trans-unit id="d151d2a5867c5ba125b375d964535691cae0b0d6" translate="yes" xml:space="preserve">
          <source>The first example illustrates how robust covariance estimation can help concentrating on a relevant cluster when another one exists. Here, many observations are confounded into one and break down the empirical covariance estimation. Of course, some screening tools would have pointed out the presence of two clusters (Support Vector Machines, Gaussian Mixture Models, univariate outlier detection, &amp;hellip;). But had it been a high-dimensional example, none of these could be applied that easily.</source>
          <target state="translated">Первый пример показывает, как надежная ковариационная оценка может помочь сосредоточиться на релевантном кластере, когда существует другой. Здесь многие наблюдения объединены в одно и подрывают эмпирическую ковариационную оценку. Конечно, некоторые инструменты скрининга указали бы на наличие двух кластеров (машины опорных векторов, модели гауссовской смеси, одномерное обнаружение выбросов и т. Д.). Но если бы это был многомерный пример, ни один из них не мог бы быть применен так легко.</target>
        </trans-unit>
        <trans-unit id="3ed755a713ce6230a16c443825e8cd0dee1503e8" translate="yes" xml:space="preserve">
          <source>The first example illustrates how the Minimum Covariance Determinant robust estimator can help concentrate on a relevant cluster when outlying points exist. Here the empirical covariance estimation is skewed by points outside of the main cluster. Of course, some screening tools would have pointed out the presence of two clusters (Support Vector Machines, Gaussian Mixture Models, univariate outlier detection, &amp;hellip;). But had it been a high-dimensional example, none of these could be applied that easily.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ecfbbb8300d863a918a602e1c4f90841d4b71a3f" translate="yes" xml:space="preserve">
          <source>The first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning):</source>
          <target state="translated">Первый загрузчик используется для задачи идентификации лиц:многоклассная классификационная задача (отсюда и контролируемое обучение):</target>
        </trans-unit>
        <trans-unit id="1272790baab931c65e23f13d4b17128820578899" translate="yes" xml:space="preserve">
          <source>The first model is a classical Gaussian Mixture Model with 10 components fit with the Expectation-Maximization algorithm.</source>
          <target state="translated">Первая модель-классическая гауссовская модель смеси с 10 компонентами,подходящими под алгоритм &quot;Ожидание-максимизация&quot;.</target>
        </trans-unit>
        <trans-unit id="a28da4c4f339f2d2823befd3907c74a3bd7b1459" translate="yes" xml:space="preserve">
          <source>The first plot is a visualization of the decision function for a variety of parameter values on a simplified classification problem involving only 2 input features and 2 possible target classes (binary classification). Note that this kind of plot is not possible to do for problems with more features or target classes.</source>
          <target state="translated">Первый график представляет собой визуализацию функции принятия решения для различных значений параметров по упрощенной задаче классификации,включающей только 2 входные функции и 2 возможных целевых класса (двоичная классификация).Обратите внимание,что этот вид графика невозможен для задач с большим количеством функций или целевых классов.</target>
        </trans-unit>
        <trans-unit id="fdb1a20b80ba5f009466ef7acd3785afda979734" translate="yes" xml:space="preserve">
          <source>The first plot shows one of the problems with using histograms to visualize the density of points in 1D. Intuitively, a histogram can be thought of as a scheme in which a unit &amp;ldquo;block&amp;rdquo; is stacked above each point on a regular grid. As the top two panels show, however, the choice of gridding for these blocks can lead to wildly divergent ideas about the underlying shape of the density distribution. If we instead center each block on the point it represents, we get the estimate shown in the bottom left panel. This is a kernel density estimation with a &amp;ldquo;top hat&amp;rdquo; kernel. This idea can be generalized to other kernel shapes: the bottom-right panel of the first figure shows a Gaussian kernel density estimate over the same distribution.</source>
          <target state="translated">Первый график показывает одну из проблем с использованием гистограмм для визуализации плотности точек в 1D. Интуитивно гистограмму можно рассматривать как схему, в которой единичный &amp;laquo;блок&amp;raquo; укладывается над каждой точкой на регулярной сетке. Однако, как показано на двух верхних панелях, выбор сетки для этих блоков может привести к сильно различающимся представлениям о базовой форме распределения плотности. Если вместо этого мы центрируем каждый блок в точке, которую он представляет, мы получаем оценку, показанную в нижней левой панели. Это оценка плотности ядра с использованием ядра &amp;laquo;в цилиндре&amp;raquo;. Эта идея может быть обобщена на другие формы ядра: нижняя правая панель первого рисунка показывает оценку плотности ядра по Гауссу для того же распределения.</target>
        </trans-unit>
        <trans-unit id="80a6f35d6de94c1655a0ab570a2d81b8e9f0c281" translate="yes" xml:space="preserve">
          <source>The first plot shows that with an increasing number of samples &lt;code&gt;n_samples&lt;/code&gt;, the minimal number of dimensions &lt;code&gt;n_components&lt;/code&gt; increased logarithmically in order to guarantee an &lt;code&gt;eps&lt;/code&gt;-embedding.</source>
          <target state="translated">Первый график показывает, что с увеличением числа выборок &lt;code&gt;n_samples&lt;/code&gt; минимальное количество измерений &lt;code&gt;n_components&lt;/code&gt; увеличивается логарифмически, чтобы гарантировать &lt;code&gt;eps&lt;/code&gt; -встраивание.</target>
        </trans-unit>
        <trans-unit id="0fa5069414b33f645c4a3a3261f97c50a380bf97" translate="yes" xml:space="preserve">
          <source>The first plot shows the best inertia reached for each combination of the model (&lt;code&gt;KMeans&lt;/code&gt; or &lt;code&gt;MiniBatchKMeans&lt;/code&gt;) and the init method (&lt;code&gt;init=&quot;random&quot;&lt;/code&gt; or &lt;code&gt;init=&quot;kmeans++&quot;&lt;/code&gt;) for increasing values of the &lt;code&gt;n_init&lt;/code&gt; parameter that controls the number of initializations.</source>
          <target state="translated">Первый график показывает наилучшую инерцию, достигаемую для каждой комбинации модели ( &lt;code&gt;KMeans&lt;/code&gt; или &lt;code&gt;MiniBatchKMeans&lt;/code&gt; ) и метода инициализации ( &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; или &lt;code&gt;init=&quot;kmeans++&quot;&lt;/code&gt; ) для увеличения значений параметра &lt;code&gt;n_init&lt;/code&gt; , который управляет количеством инициализаций.</target>
        </trans-unit>
        <trans-unit id="35a5988878bfd98d1cc6f738d4af80878102b501" translate="yes" xml:space="preserve">
          <source>The first row of output array indicates that there are three samples whose true cluster is &amp;ldquo;a&amp;rdquo;. Of them, two are in predicted cluster 0, one is in 1, and none is in 2. And the second row indicates that there are three samples whose true cluster is &amp;ldquo;b&amp;rdquo;. Of them, none is in predicted cluster 0, one is in 1 and two are in 2.</source>
          <target state="translated">Первая строка выходного массива указывает, что есть три образца, истинный кластер которых равен &amp;laquo;a&amp;raquo;. Из них два находятся в прогнозируемом кластере 0, один - в 1 и ни один - в 2. И вторая строка указывает, что есть три образца, истинный кластер которых равен &amp;laquo;b&amp;raquo;. Из них ни один не находится в прогнозируемом кластере 0, один - в 1, а два - в 2.</target>
        </trans-unit>
        <trans-unit id="94d22c244cece5d36684b54eb3a3c36ef460564d" translate="yes" xml:space="preserve">
          <source>The first two loss functions are lazy, they only update the model parameters if an example violates the margin constraint, which makes training very efficient and may result in sparser models, even when L2 penalty is used.</source>
          <target state="translated">Первые две функции потерь ленивы,они обновляют параметры модели только в том случае,если пример нарушает ограничение по марже,что делает обучение очень эффективным и может привести к появлению более щадящих моделей,даже если используется L2 пенальти.</target>
        </trans-unit>
        <trans-unit id="90159f341e51eef650aa99c9ddd91af9915c59a3" translate="yes" xml:space="preserve">
          <source>The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions, using the &lt;code&gt;transform&lt;/code&gt; method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="865ac3c349894331005e5c9a9918fbe1c4576705" translate="yes" xml:space="preserve">
          <source>The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.</source>
          <target state="translated">Подогнанная модель также может быть использована для уменьшения размерности входного сигнала,проецируя его на наиболее дискриминирующие направления.</target>
        </trans-unit>
        <trans-unit id="47ade7eb6fd49b2a025d9a87896bfcf3ba24402d" translate="yes" xml:space="preserve">
          <source>The fitted model.</source>
          <target state="translated">Установленная модель.</target>
        </trans-unit>
        <trans-unit id="474bb96d2aba10d56f1c494aef7da54c3ce49fa5" translate="yes" xml:space="preserve">
          <source>The flattened data matrix. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;data&lt;/code&gt; will be a pandas DataFrame.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14120dbe50cfe7a93d61eb3782205eb1e9322e9d" translate="yes" xml:space="preserve">
          <source>The flexibility of controlling the smoothness of the learned function via \(\nu\) allows adapting to the properties of the true underlying functional relation. The prior and posterior of a GP resulting from a Mat&amp;eacute;rn kernel are shown in the following figure:</source>
          <target state="translated">Гибкость управления гладкостью изученной функции через \ (\ nu \) позволяет адаптироваться к свойствам истинного базового функционального отношения. Априорное и апостериорное GP, полученное из ядра Матерна, показано на следующем рисунке:</target>
        </trans-unit>
        <trans-unit id="130cb7c407aba5d31bddc566759f6c2692fa934a" translate="yes" xml:space="preserve">
          <source>The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data.</source>
          <target state="translated">Приведенная ниже схема предназначена для того,чтобы дать пользователям немного приблизительного руководства о том,как подходить к проблемам в отношении того,какие оценщики должны примерить ваши данные.</target>
        </trans-unit>
        <trans-unit id="f162534c96a36fe6b7c86b6775d49d882723bf35" translate="yes" xml:space="preserve">
          <source>The folder names are used as supervised signal label names. The individual file names are not important.</source>
          <target state="translated">Имена папок используются в качестве имен управляемых сигнальных этикеток.Имена отдельных файлов не важны.</target>
        </trans-unit>
        <trans-unit id="f2db8b6a5929e1458b6943c5d307e1ce8a02db89" translate="yes" xml:space="preserve">
          <source>The folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold.</source>
          <target state="translated">Складки приблизительно сбалансированы в том смысле,что количество различных групп примерно одинаково в каждом сгибе.</target>
        </trans-unit>
        <trans-unit id="7910d480ac1425d50ff9fab631fdf912810ec26e" translate="yes" xml:space="preserve">
          <source>The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the features. In mathematical notation, if \(\hat{y}\) is the predicted value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b43c7b928f1a786f0281c067a04292ccfddc7963" translate="yes" xml:space="preserve">
          <source>The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the input variables. In mathematical notion, if \(\hat{y}\) is the predicted value.</source>
          <target state="translated">Ниже приведен набор методов,предназначенных для регрессии,в которых целевым значением предполагается линейная комбинация входных переменных.В математическом представлении,если \(\hat{y}\)является прогнозируемым значением.</target>
        </trans-unit>
        <trans-unit id="81301a81f070c1a1a05c02b69a367906c9b88d21" translate="yes" xml:space="preserve">
          <source>The following clustering assignment is slightly better, since it is homogeneous but not complete:</source>
          <target state="translated">Следующее кластерное задание немного лучше,так как оно однородно,но не является полным:</target>
        </trans-unit>
        <trans-unit id="563a9a6f3a15e2fd51ba9e9b647870430bf12e58" translate="yes" xml:space="preserve">
          <source>The following code defines a linear kernel and creates a classifier instance that will use that kernel:</source>
          <target state="translated">Следующий код определяет линейное ядро и создает экземпляр классификатора,который будет использовать это ядро:</target>
        </trans-unit>
        <trans-unit id="b8e362395586349d1fcb8b526001a536e06f5d5d" translate="yes" xml:space="preserve">
          <source>The following code is a bit verbose, feel free to jump directly to the analysis of the &lt;a href=&quot;#results&quot;&gt;results&lt;/a&gt;.</source>
          <target state="translated">Следующий код является немного подробным, не стесняйтесь переходить непосредственно к анализу &lt;a href=&quot;#results&quot;&gt;результатов&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="b414b22c1d4fa210fe910463f00d0b843e42262f" translate="yes" xml:space="preserve">
          <source>The following cross-validation splitters can be used to do that. The grouping identifier for the samples is specified via the &lt;code&gt;groups&lt;/code&gt; parameter.</source>
          <target state="translated">Для этого можно использовать следующие разделители перекрестной проверки. Идентификатор группировки для образцов указывается через параметр &lt;code&gt;groups&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6757b5cfa24458748d25f81af60d324180a74dc4" translate="yes" xml:space="preserve">
          <source>The following cross-validators can be used in such cases.</source>
          <target state="translated">В таких случаях могут использоваться следующие перекрестные проверки.</target>
        </trans-unit>
        <trans-unit id="6da3fe3659ba9973062031cbded9ac1ee9e80559" translate="yes" xml:space="preserve">
          <source>The following dataset has integer features, two of which are the same in every sample. These are removed with the default setting for threshold:</source>
          <target state="translated">Следующий набор данных имеет целочисленные характеристики,две из которых одинаковы в каждом примере.Они удаляются с установкой порога по умолчанию:</target>
        </trans-unit>
        <trans-unit id="54024b35aa398c3f2441f7e2cd8d0e6044cd9d46" translate="yes" xml:space="preserve">
          <source>The following estimators have built-in variable selection fitting procedures, but any estimator using a L1 or elastic-net penalty also performs variable selection: typically &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; with an appropriate penalty.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f50629b49593a5773731e8a4d6d7fbab3d06afa6" translate="yes" xml:space="preserve">
          <source>The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):</source>
          <target state="translated">Следующий пример демонстрирует,как оценить точность векторной машины поддержки линейного ядра на наборе данных диафрагмы путем разбиения данных,подгонки модели и вычисления оценки 5 раз подряд (с разными разбиениями каждый раз):</target>
        </trans-unit>
        <trans-unit id="cd40f6b0be33059696fbdf36c2c2af62f27e4578" translate="yes" xml:space="preserve">
          <source>The following example highlights the limitations of impurity-based feature importance in contrast to permutation-based feature importance: &lt;a href=&quot;../auto_examples/inspection/plot_permutation_importance#sphx-glr-auto-examples-inspection-plot-permutation-importance-py&quot;&gt;Permutation Importance vs Random Forest Feature Importance (MDI)&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50761302c7bb4483750fc53752f14a1bf26cb023" translate="yes" xml:space="preserve">
          <source>The following example illustrates 16 components extracted using sparse PCA from the Olivetti faces dataset. It can be seen how the regularization term induces many zeros. Furthermore, the natural structure of the data causes the non-zero coefficients to be vertically adjacent. The model does not enforce this mathematically: each component is a vector \(h \in \mathbf{R}^{4096}\), and there is no notion of vertical adjacency except during the human-friendly visualization as 64x64 pixel images. The fact that the components shown below appear local is the effect of the inherent structure of the data, which makes such local patterns minimize reconstruction error. There exist sparsity-inducing norms that take into account adjacency and different kinds of structure; see &lt;a href=&quot;#jen09&quot; id=&quot;id2&quot;&gt;[Jen09]&lt;/a&gt; for a review of such methods. For more details on how to use Sparse PCA, see the Examples section, below.</source>
          <target state="translated">В следующем примере показаны 16 компонентов, извлеченных с помощью разреженного PCA из набора данных лиц Olivetti. Видно, как член регуляризации порождает много нулей. Кроме того, естественная структура данных заставляет ненулевые коэффициенты быть смежными по вертикали. Модель не обеспечивает этого математически: каждый компонент представляет собой вектор \ (h \ in \ mathbf {R} ^ {4096} \), и нет понятия вертикальной смежности, кроме как во время удобной для человека визуализации в виде изображений размером 64x64 пикселей. Тот факт, что показанные ниже компоненты выглядят локальными, является эффектом внутренней структуры данных, которая позволяет таким локальным шаблонам минимизировать ошибку восстановления. Существуют нормы, вызывающие разреженность, которые учитывают смежность и различные виды структуры; см. &lt;a href=&quot;#jen09&quot; id=&quot;id2&quot;&gt;[Jen09]&lt;/a&gt;для обзора таких методов. Дополнительные сведения об использовании Sparse PCA см. В разделе &amp;laquo;Примеры&amp;raquo; ниже.</target>
        </trans-unit>
        <trans-unit id="218baecbc1e50d08c530ba0bc6ca1335ad228789" translate="yes" xml:space="preserve">
          <source>The following example illustrates how the decision regions may change when a soft &lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; is used based on an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b4e5d16e2ae8075091f7ec9c797f8dd043b5cfe" translate="yes" xml:space="preserve">
          <source>The following example illustrates how the decision regions may change when a soft &lt;code&gt;VotingClassifier&lt;/code&gt; is used based on an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier:</source>
          <target state="translated">В следующем примере показано, как области решений могут измениться при использовании мягкого &lt;code&gt;VotingClassifier&lt;/code&gt; на основе линейной машины опорных векторов , дерева решений и классификатора K-ближайших соседей:</target>
        </trans-unit>
        <trans-unit id="fcb0630f60eb0a1b1e5514ec57d9a7f6cbd21ce7" translate="yes" xml:space="preserve">
          <source>The following example illustrates the effect of scaling the regularization parameter when using &lt;a href=&quot;../../modules/svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; for &lt;a href=&quot;../../modules/svm#svm-classification&quot;&gt;classification&lt;/a&gt;. For SVC classification, we are interested in a risk minimization for the equation:</source>
          <target state="translated">В следующем примере показан эффект масштабирования параметра регуляризации при использовании &lt;a href=&quot;../../modules/svm#svm&quot;&gt;машин опорных векторов&lt;/a&gt; для &lt;a href=&quot;../../modules/svm#svm-classification&quot;&gt;классификации&lt;/a&gt; . Для классификации SVC нас интересует минимизация риска для уравнения:</target>
        </trans-unit>
        <trans-unit id="35227ebb51bdeaa9a401c5e44c0ca9ca35e1f0e6" translate="yes" xml:space="preserve">
          <source>The following example shows a color-coded representation of the relative importances of each individual pixel for a face recognition task using a &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt;&lt;code&gt;ExtraTreesClassifier&lt;/code&gt;&lt;/a&gt; model.</source>
          <target state="translated">В следующем примере показано цветовое представление относительной важности каждого отдельного пикселя для задачи распознавания лиц с использованием модели &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt; &lt;code&gt;ExtraTreesClassifier&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="e4e3a609b2ed09432a121fc3917b934875c3544c" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit an AdaBoost classifier with 100 weak learners:</source>
          <target state="translated">Следующий пример показывает,как подогнать классификатор AdaBoost под 100 слабых учащихся:</target>
        </trans-unit>
        <trans-unit id="08beea172467f4b9200a143a3d6a2c2ed164664c" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit the VotingRegressor:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="486be98f63ccda08601d245e4a85066d3abba297" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit the majority rule classifier:</source>
          <target state="translated">Следующий пример показывает,как соответствовать классификатору правил большинства:</target>
        </trans-unit>
        <trans-unit id="6a236adbadcca90d4c1ec977d69fa3c364957c12" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the 5 most informative features in the Friedman #1 dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="376fe34989034d77dfd504e8201c3d4b2dfa1573" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the 5 right informative features in the Friedman #1 dataset.</source>
          <target state="translated">Следующий пример показывает,как получить 5 правильных информативных функций в наборе данных Friedman #1.</target>
        </trans-unit>
        <trans-unit id="25b5efc453e06d92e8572618fefd1930c5c8aab3" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the a-priori not known 5 informative features in the Friedman #1 dataset.</source>
          <target state="translated">Следующий пример показывает,как получить a-приори неизвестные 5 информативных возможностей в наборе данных Friedman #1.</target>
        </trans-unit>
        <trans-unit id="e99af0f029f79715308d4d1c9ecbe2acb8983e4d" translate="yes" xml:space="preserve">
          <source>The following example will, for instance, transform some British spelling to American spelling:</source>
          <target state="translated">Следующий пример,например,трансформирует некоторую британскую орфографию в американскую:</target>
        </trans-unit>
        <trans-unit id="ad9262c5496a82045b0e3b64071f384beed06659" translate="yes" xml:space="preserve">
          <source>The following experiment is performed on an artificial dataset for binary classification with 100,000 samples (1,000 of them are used for model fitting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The figure shows the estimated probabilities obtained with logistic regression, a linear support-vector classifier (SVC), and linear SVC with both isotonic calibration and sigmoid calibration. The Brier score is a metric which is a combination of calibration loss and refinement loss, &lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt;&lt;code&gt;brier_score_loss&lt;/code&gt;&lt;/a&gt;, reported in the legend (the smaller the better). Calibration loss is defined as the mean squared deviation from empirical probabilities derived from the slope of ROC segments. Refinement loss can be defined as the expected optimal loss as measured by the area under the optimal cost curve.</source>
          <target state="translated">Следующий эксперимент проводится на искусственном наборе данных для двоичной классификации со 100 000 выборок (1000 из них используются для подгонки модели) с 20 признаками. Из 20 функций только 2 являются информативными, а 10 - избыточными. На рисунке показаны оценочные вероятности, полученные с помощью логистической регрессии, линейного классификатора опорных векторов (SVC) и линейного SVC как с изотонической калибровкой, так и с сигмоидной калибровкой. Оценка Бриера - это показатель, который представляет собой комбинацию потери калибровки и потери уточнения, &lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt; &lt;code&gt;brier_score_loss&lt;/code&gt; &lt;/a&gt;, сообщается в легенде (чем меньше, тем лучше). Потеря калибровки определяется как среднеквадратическое отклонение от эмпирических вероятностей, полученных из наклона сегментов ROC. Потери при переработке можно определить как ожидаемые оптимальные потери, измеренные площадью под кривой оптимальных затрат.</target>
        </trans-unit>
        <trans-unit id="7e7b7eaddffc735bc9fdc4de0de59e2218717940" translate="yes" xml:space="preserve">
          <source>The following figure compares &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The learned model of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; is plotted, where both complexity/regularization and bandwidth of the RBF kernel have been optimized using grid-search. The learned functions are very similar; however, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is approximately seven times faster than fitting &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; (both with grid-search). However, prediction of 100000 target values is more than three times faster with &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; since it has learned a sparse model using only approximately 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b3bbe94b0947452c8fc360afc3d0e6f8832ae22" translate="yes" xml:space="preserve">
          <source>The following figure compares &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The learned model of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; is plotted, where both complexity/regularization and bandwidth of the RBF kernel have been optimized using grid-search. The learned functions are very similar; however, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is approx. seven times faster than fitting &lt;code&gt;SVR&lt;/code&gt; (both with grid-search). However, prediction of 100000 target values is more than three times faster with SVR since it has learned a sparse model using only approx. 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="translated">На следующем рисунке &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; сравнение KernelRidge и &lt;code&gt;SVR&lt;/code&gt; на искусственном наборе данных, который состоит из синусоидальной целевой функции и сильного шума, добавленного к каждой пятой точке данных. Построена изученная модель &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; и &lt;code&gt;SVR&lt;/code&gt; , где как сложность / регуляризация, так и полоса пропускания ядра RBF были оптимизированы с помощью поиска по сетке. Изученные функции очень похожи; однако установка &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; составляет ок. в семь раз быстрее, чем установка &lt;code&gt;SVR&lt;/code&gt; (оба с поиском по сетке). Однако прогнозирование 100000 целевых значений более чем в три раза быстрее с SVR, поскольку он изучил разреженную модель, используя только прибл. 1/3 из 100 обучающих точек данных в качестве опорных векторов.</target>
        </trans-unit>
        <trans-unit id="7f576baedfe43e5a873ff30c0a921c0894cd8a99" translate="yes" xml:space="preserve">
          <source>The following figure compares the location of the non-zero entries in the coefficient matrix W obtained with a simple Lasso or a MultiTaskLasso. The Lasso estimates yield scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b11b5ffd2f0d8dbd878722867b9b243b2e73a5ea" translate="yes" xml:space="preserve">
          <source>The following figure compares the location of the non-zeros in W obtained with a simple Lasso or a MultiTaskLasso. The Lasso estimates yields scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns.</source>
          <target state="translated">На следующем рисунке сравнивается расположение ненулевых в W,полученных с простым Лассо или Мультизадачным Лассо.Оценки Лассо дают разрозненные ненулевые значения,в то время как ненулевые значения MultiTaskLasso являются полными столбцами.</target>
        </trans-unit>
        <trans-unit id="c66df17d5ba5efe635efde7f93de1fc62e75c222" translate="yes" xml:space="preserve">
          <source>The following figure illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise. The figure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited for learning periodic functions. The kernel&amp;rsquo;s hyperparameters control the smoothness (length_scale) and periodicity of the kernel (periodicity). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR.</source>
          <target state="translated">На следующем рисунке оба метода показаны на искусственном наборе данных, который состоит из синусоидальной целевой функции и сильного шума. На рисунке сравнивается изученная модель KRR и GPR на основе ядра ExpSineSquared, которое подходит для обучения периодических функций. Гиперпараметры ядра управляют гладкостью (length_scale) и периодичностью ядра (периодичностью). Более того, уровень шума данных явно определяется GPR с помощью дополнительного компонента WhiteKernel в ядре и параметра регуляризации альфа KRR.</target>
        </trans-unit>
        <trans-unit id="7af807be2b2e68501b3d7cc153c6244aca835299" translate="yes" xml:space="preserve">
          <source>The following guide focuses on &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, which might be preferred for small sample sizes since binning may lead to split points that are too approximate in this setting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2a01fd3fe0b5b9cd28910783764aacd71509b19" translate="yes" xml:space="preserve">
          <source>The following illustrate the probability density functions of the target before and after applying the logarithmic functions.</source>
          <target state="translated">Ниже показаны функции плотности вероятности цели до и после применения логарифмических функций.</target>
        </trans-unit>
        <trans-unit id="b0293fdb9cc21af9db21d1f9f61ccde2a4565147" translate="yes" xml:space="preserve">
          <source>The following image shows how a dictionary learned from 4x4 pixel image patches extracted from part of the image of a raccoon face looks like.</source>
          <target state="translated">На следующем изображении показано,как выглядит словарь,выученный из патчей изображения размером 4x4 пикселя,извлечённых из части изображения енота.</target>
        </trans-unit>
        <trans-unit id="a988a0a50289dff96522065b976c597b6e4e63e8" translate="yes" xml:space="preserve">
          <source>The following image shows on the data above the estimated probability using a Gaussian naive Bayes classifier without calibration, with a sigmoid calibration and with a non-parametric isotonic calibration. One can observe that the non-parametric model provides the most accurate probability estimates for samples in the middle, i.e., 0.5.</source>
          <target state="translated">На следующем снимке показаны данные выше оцененной вероятности с использованием гауссовского наивного классификатора Бейеса без калибровки,с сигмовидной калибровкой и с непараметрической изотонической калибровкой.Можно заметить,что непараметрическая модель обеспечивает наиболее точную оценку вероятности для образцов в середине,т.е.0.5.</target>
        </trans-unit>
        <trans-unit id="b45c55863d114ed9459a8475837831820ff65766" translate="yes" xml:space="preserve">
          <source>The following images demonstrate the benefit of probability calibration. The first image present a dataset with 2 classes and 3 blobs of data. The blob in the middle contains random samples of each class. The probability for the samples in this blob should be 0.5.</source>
          <target state="translated">Следующие изображения демонстрируют преимущество калибровки вероятности.На первом изображении представлен набор данных с 2 классами и 3 блоками данных.Капля в середине содержит случайные выборки каждого класса.Вероятность для выборок в этом блоке должна быть 0.5.</target>
        </trans-unit>
        <trans-unit id="ba49479bf1cc39dce48fe5a925ee39da6796af0a" translate="yes" xml:space="preserve">
          <source>The following lists the string metric identifiers and the associated distance metric classes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="807c0e6975f025a025e2951d9c9164c1f3454c71" translate="yes" xml:space="preserve">
          <source>The following loss functions are supported and can be specified using the parameter &lt;code&gt;loss&lt;/code&gt;:</source>
          <target state="translated">Поддерживаются следующие функции потерь, которые можно указать с помощью параметра &lt;code&gt;loss&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="f56ebb3690526678e7e0211eddd43dd69c89de3e" translate="yes" xml:space="preserve">
          <source>The following parts are parallelized:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69eaa3387fa8263cde077e4bb7511faccc7173e0" translate="yes" xml:space="preserve">
          <source>The following plot compares how well the probabilistic predictions of different classifiers are calibrated, using &lt;a href=&quot;generated/sklearn.calibration.calibration_curve#sklearn.calibration.calibration_curve&quot;&gt;&lt;code&gt;calibration_curve&lt;/code&gt;&lt;/a&gt;. The x axis represents the average predicted probability in each bin. The y axis is the &lt;em&gt;fraction of positives&lt;/em&gt;, i.e. the proportion of samples whose class is the positive class (in each bin).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70624e811991b8fac875c2f0dd0db6a8fb2e1e2b" translate="yes" xml:space="preserve">
          <source>The following plots demonstrate the impact of the number of clusters and number of samples on various clustering performance evaluation metrics.</source>
          <target state="translated">На следующих графиках показано влияние количества кластеров и выборок на различные метрики оценки эффективности кластеризации.</target>
        </trans-unit>
        <trans-unit id="661cd7d1cca4025c0fd4f4a1e938e7140f6ef24b" translate="yes" xml:space="preserve">
          <source>The following sections contain further explanations and examples that illustrate how the tf-idfs are computed exactly and how the tf-idfs computed in scikit-learn&amp;rsquo;s &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;TfidfVectorizer&lt;/code&gt;&lt;/a&gt; differ slightly from the standard textbook notation that defines the idf as</source>
          <target state="translated">Следующие разделы содержат дополнительные объяснения и примеры, которые иллюстрируют, как точно вычисляются tf-idfs и как tf-idfs, вычисленные в &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt; и &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;TfidfVectorizer&lt;/code&gt; &lt;/a&gt; scikit-learn, немного отличаются от стандартной нотации учебника, которая определяет idf как</target>
        </trans-unit>
        <trans-unit id="78ad5f101e233b65633dffc5c68bec29c1eae0b0" translate="yes" xml:space="preserve">
          <source>The following sections list utilities to generate indices that can be used to generate dataset splits according to different cross validation strategies.</source>
          <target state="translated">В следующих разделах перечислены утилиты для генерации индексов,которые могут быть использованы для генерации разбиений наборов данных в соответствии с различными стратегиями перекрестной проверки.</target>
        </trans-unit>
        <trans-unit id="999c74375c41b701ce62b520b704cf4b739a66b1" translate="yes" xml:space="preserve">
          <source>The following snippet demonstrates how to replace missing values, encoded as &lt;code&gt;np.nan&lt;/code&gt;, using the mean feature value of the two nearest neighbors of samples with missing values:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c75d78bf5e5b93a438a72e22f9eee2db09a82ed" translate="yes" xml:space="preserve">
          <source>The following snippet demonstrates how to replace missing values, encoded as &lt;code&gt;np.nan&lt;/code&gt;, using the mean value of the columns (axis 0) that contain the missing values:</source>
          <target state="translated">В следующем фрагменте показано, как заменить отсутствующие значения, закодированные как &lt;code&gt;np.nan&lt;/code&gt; , с использованием среднего значения столбцов (ось 0), содержащих отсутствующие значения:</target>
        </trans-unit>
        <trans-unit id="35e4b9d817818dd84b86f006c91682ff56db6021" translate="yes" xml:space="preserve">
          <source>The following subsections are only rough guidelines: the same estimator can fall into multiple categories, depending on its parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6c9fc0377f0a1e8adb0ffabc9be6ea63e8b1dd6" translate="yes" xml:space="preserve">
          <source>The following table lists some specific EDMs and their unit deviance (all of these are instances of the Tweedie family):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8aa5b699e7b8122b521ff7ffa5779f4500a14f28" translate="yes" xml:space="preserve">
          <source>The following table summarizes the penalties supported by each solver:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9d55ab5466e0380dae4e49e3b2bfc5d99624d130" translate="yes" xml:space="preserve">
          <source>The following toy example demonstrates how the model ignores the samples with zero sample weights:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="46781e9d9b616f25e94b9f1718ce2c06cffa693e" translate="yes" xml:space="preserve">
          <source>The following two references explain the iterations used in the coordinate descent solver of scikit-learn, as well as the duality gap computation used for convergence control.</source>
          <target state="translated">Следующие две ссылки объясняют итерации,используемые в решателе координатного спуска Scikit-learn,а также вычисление зазора двойственности,используемого для контроля сходимости.</target>
        </trans-unit>
        <trans-unit id="5c7ef4cd14485e27e01a8eae72d8f974a84e41ab" translate="yes" xml:space="preserve">
          <source>The form of the model learned by &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is identical to support vector regression (&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;). However, different loss functions are used: KRR uses squared error loss while support vector regression uses \(\epsilon\)-insensitive loss, both combined with l2 regularization. In contrast to &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, which learns a sparse model for \(\epsilon &amp;gt; 0\), at prediction-time.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d492404a63c6d65da32b70e6dcf87e75a94b6a4d" translate="yes" xml:space="preserve">
          <source>The form of the model learned by &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is identical to support vector regression (&lt;code&gt;SVR&lt;/code&gt;). However, different loss functions are used: KRR uses squared error loss while support vector regression uses \(\epsilon\)-insensitive loss, both combined with l2 regularization. In contrast to &lt;code&gt;SVR&lt;/code&gt;, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for \(\epsilon &amp;gt; 0\), at prediction-time.</source>
          <target state="translated">Форма модели, изученной &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; , идентична поддерживающей векторной регрессии ( &lt;code&gt;SVR&lt;/code&gt; ). Однако используются разные функции потерь: KRR использует квадратичную потерю ошибок, тогда как регрессия векторов поддержки использует \ (\ epsilon \) - нечувствительные потери, обе в сочетании с регуляризацией l2. В отличие от &lt;code&gt;SVR&lt;/code&gt; , &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt; может выполняться в закрытой форме и обычно быстрее для наборов данных среднего размера. С другой стороны, изученная модель не является разреженной и, следовательно, медленнее, чем SVR, которая изучает разреженную модель для \ (\ epsilon&amp;gt; 0 \) во время прогнозирования.</target>
        </trans-unit>
        <trans-unit id="cb6eb14fd1b6ffac38e7d6c15d36b7076beafa85" translate="yes" xml:space="preserve">
          <source>The form of the model learned by KRR is identical to support vector regression (SVR). However, different loss functions are used: KRR uses squared error loss while support vector regression uses epsilon-insensitive loss, both combined with l2 regularization. In contrast to SVR, fitting a KRR model can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for epsilon &amp;gt; 0, at prediction-time.</source>
          <target state="translated">Форма модели, изученной KRR, идентична поддерживающей векторной регрессии (SVR). Однако используются другие функции потерь: KRR использует квадратичную потерю ошибок, тогда как регрессия векторов поддержки использует нечувствительные к эпсилону потери, обе в сочетании с регуляризацией l2. В отличие от SVR, подгонка модели KRR может быть выполнена в закрытой форме и обычно быстрее для наборов данных среднего размера. С другой стороны, изученная модель не является разреженной и, следовательно, медленнее, чем SVR, который изучает разреженную модель для epsilon&amp;gt; 0 во время прогнозирования.</target>
        </trans-unit>
        <trans-unit id="785bfd19f7d4d298d07ec2673131bad97bee7c7c" translate="yes" xml:space="preserve">
          <source>The form of these kernels is as follows:</source>
          <target state="translated">Форма этих ядер следующая:</target>
        </trans-unit>
        <trans-unit id="cecc632cfa798838e6560a2f37e713a3ae87c1a6" translate="yes" xml:space="preserve">
          <source>The formula that is used to compute the tf-idf for a term t of a document d in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is computed as idf(t) = log [ n / df(t) ] + 1 (if &lt;code&gt;smooth_idf=False&lt;/code&gt;), where n is the total number of documents in the document set and df(t) is the document frequency of t; the document frequency is the number of documents in the document set that contain the term t. The effect of adding &amp;ldquo;1&amp;rdquo; to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(t) = log [ n / (df(t) + 1) ]).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99fa4cbad7a403537a73165a2b1d717c58bb9b6b" translate="yes" xml:space="preserve">
          <source>The formula that is used to compute the tf-idf of term t is tf-idf(d, t) = tf(t) * idf(d, t), and the idf is computed as idf(d, t) = log [ n / df(d, t) ] + 1 (if &lt;code&gt;smooth_idf=False&lt;/code&gt;), where n is the total number of documents and df(d, t) is the document frequency; the document frequency is the number of documents d that contain term t. The effect of adding &amp;ldquo;1&amp;rdquo; to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(d, t) = log [ n / (df(d, t) + 1) ]).</source>
          <target state="translated">Формула, которая используется для вычисления tf-idf члена t: tf-idf (d, t) = tf (t) * idf (d, t), а idf вычисляется как idf (d, t) = log [n / df (d, t)] + 1 (если &lt;code&gt;smooth_idf=False&lt;/code&gt; ), где n - общее количество документов, а df (d, t) - частота документов; частота документа - это количество документов d, содержащих термин t. Эффект добавления &amp;laquo;1&amp;raquo; к idf в приведенном выше уравнении заключается в том, что члены с нулевым idf, т. Е. Термины, которые встречаются во всех документах в обучающем наборе, не будут полностью игнорироваться. (Обратите внимание, что приведенная выше формула idf отличается от стандартной записи в учебниках, которая определяет idf как idf (d, t) = log [n / (df (d, t) + 1)]).</target>
        </trans-unit>
        <trans-unit id="2757038bccf5826fff274cfe8684f779a3893302" translate="yes" xml:space="preserve">
          <source>The formula used here does not correspond to the one given in the article. In the original article, formula (23) states that 2/p is multiplied by Trace(cov*cov) in both the numerator and denominator, but this operation is omitted because for a large p, the value of 2/p is so small that it doesn&amp;rsquo;t affect the value of the estimator.</source>
          <target state="translated">Используемая здесь формула не соответствует приведенной в статье. В исходной статье формула (23) утверждает, что 2 / p умножается на Trace (cov * cov) как в числителе, так и в знаменателе, но эта операция опускается, потому что для большого p значение 2 / p настолько мало что это не влияет на значение оценщика.</target>
        </trans-unit>
        <trans-unit id="25426ebad0f81610af376f208a1d7f7cdb7c96c5" translate="yes" xml:space="preserve">
          <source>The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. &lt;code&gt;subsample&lt;/code&gt; interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;. Choosing &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; leads to a reduction of variance and an increase in bias.</source>
          <target state="translated">Доля образцов, которая будет использоваться для подбора отдельных базовых учащихся. Если меньше 1,0, это приводит к усилению стохастического градиента. &lt;code&gt;subsample&lt;/code&gt; взаимодействует с параметром &lt;code&gt;n_estimators&lt;/code&gt; . Выбор &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; приводит к уменьшению дисперсии и увеличению систематической ошибки.</target>
        </trans-unit>
        <trans-unit id="568fbbbfe83b2ef390104a99310641880cb62ce5" translate="yes" xml:space="preserve">
          <source>The fraction of samples to be used in each randomized design. Should be between 0 and 1. If 1, all samples are used.</source>
          <target state="translated">Доля образцов,которая будет использоваться в каждом рандомизированном дизайне.Должно быть от 0 до 1.Если 1,то используются все образцы.</target>
        </trans-unit>
        <trans-unit id="52b16a6d5efc38a8b1d594773d860718286a7706" translate="yes" xml:space="preserve">
          <source>The fraction of samples whose class are randomly exchanged. Larger values introduce noise in the labels and make the classification task harder.</source>
          <target state="translated">Доля образцов,класс которых обменивается случайным образом.Большие значения вносят шум в метки и усложняют задачу классификации.</target>
        </trans-unit>
        <trans-unit id="3e4c940191ac2ce629537cccbeb4e1290fc15133" translate="yes" xml:space="preserve">
          <source>The fraction of samples whose class is assigned randomly. Larger values introduce noise in the labels and make the classification task harder. Note that the default setting flip_y &amp;gt; 0 might lead to less than n_classes in y in some cases.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b8586795acd1de1d253165ce5ff74dbee8020e0" translate="yes" xml:space="preserve">
          <source>The free parameters in the model are C and epsilon.</source>
          <target state="translated">Свободными параметрами в модели являются С и эпсилон.</target>
        </trans-unit>
        <trans-unit id="852901dbf93c375963721fc84a4888e6b45c78dd" translate="yes" xml:space="preserve">
          <source>The full description of the dataset</source>
          <target state="translated">Полное описание набора данных</target>
        </trans-unit>
        <trans-unit id="d9127a02d65d598dd00fad372a9aa50b138962b6" translate="yes" xml:space="preserve">
          <source>The full description of the dataset.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="19877feb57bb09f6d25fbaaa466f42d45ca4d944" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.feature_extraction.image.img_to_graph#sklearn.feature_extraction.image.img_to_graph&quot;&gt;&lt;code&gt;img_to_graph&lt;/code&gt;&lt;/a&gt; returns such a matrix from a 2D or 3D image. Similarly, &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt;&lt;code&gt;grid_to_graph&lt;/code&gt;&lt;/a&gt; build a connectivity matrix for images given the shape of these image.</source>
          <target state="translated">Функция &lt;a href=&quot;generated/sklearn.feature_extraction.image.img_to_graph#sklearn.feature_extraction.image.img_to_graph&quot;&gt; &lt;code&gt;img_to_graph&lt;/code&gt; &lt;/a&gt; возвращает такую ​​матрицу из 2D или 3D изображения. Точно так же &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt; &lt;code&gt;grid_to_graph&lt;/code&gt; &lt;/a&gt; создает матрицу связности для изображений с учетом формы этих изображений.</target>
        </trans-unit>
        <trans-unit id="0a9dad7d233dabf5b0bcafb3e330d5014a951e15" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.linear_model.lasso_path#sklearn.linear_model.lasso_path&quot;&gt;&lt;code&gt;lasso_path&lt;/code&gt;&lt;/a&gt; is useful for lower-level tasks, as it computes the coefficients along the full path of possible values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6bbdd98a7d5d87ddd3bbdfb8569481a5e5e04495" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.cohen_kappa_score#sklearn.metrics.cohen_kappa_score&quot;&gt;&lt;code&gt;cohen_kappa_score&lt;/code&gt;&lt;/a&gt; computes &lt;a href=&quot;https://en.wikipedia.org/wiki/Cohen%27s_kappa&quot;&gt;Cohen&amp;rsquo;s kappa&lt;/a&gt; statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.</source>
          <target state="translated">Функция &lt;a href=&quot;generated/sklearn.metrics.cohen_kappa_score#sklearn.metrics.cohen_kappa_score&quot;&gt; &lt;code&gt;cohen_kappa_score&lt;/code&gt; &lt;/a&gt; вычисляет статистику &lt;a href=&quot;https://en.wikipedia.org/wiki/Cohen%27s_kappa&quot;&gt;Каппа Коэна&lt;/a&gt; . Эта мера предназначена для сравнения меток, сделанных разными людьми-аннотаторами, а не классификатором и достоверной информацией.</target>
        </trans-unit>
        <trans-unit id="93f21bf6b976a0d8004246b9140ac3f9b0310fba" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.laplacian_kernel#sklearn.metrics.pairwise.laplacian_kernel&quot;&gt;&lt;code&gt;laplacian_kernel&lt;/code&gt;&lt;/a&gt; is a variant on the radial basis function kernel defined as:</source>
          <target state="translated">Функция &lt;a href=&quot;generated/sklearn.metrics.pairwise.laplacian_kernel#sklearn.metrics.pairwise.laplacian_kernel&quot;&gt; &lt;code&gt;laplacian_kernel&lt;/code&gt; &lt;/a&gt; - это вариант ядра радиальной базисной функции, определяемый как:</target>
        </trans-unit>
        <trans-unit id="3fe1ad7d2ae4a1d31d30a7b5c63e81d53b7568bb" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt;&lt;code&gt;linear_kernel&lt;/code&gt;&lt;/a&gt; computes the linear kernel, that is, a special case of &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt;&lt;code&gt;polynomial_kernel&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;degree=1&lt;/code&gt; and &lt;code&gt;coef0=0&lt;/code&gt; (homogeneous). If &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are column vectors, their linear kernel is:</source>
          <target state="translated">Функция &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt; &lt;code&gt;linear_kernel&lt;/code&gt; &lt;/a&gt; вычисляет линейное ядро, то есть частный случай &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt; &lt;code&gt;polynomial_kernel&lt;/code&gt; &lt;/a&gt; со &lt;code&gt;degree=1&lt;/code&gt; и &lt;code&gt;coef0=0&lt;/code&gt; (однородным). Если &lt;code&gt;x&lt;/code&gt; и &lt;code&gt;y&lt;/code&gt; являются векторами-столбцами, их линейное ядро:</target>
        </trans-unit>
        <trans-unit id="72295044331d14363e1e1fe37cea504256d7c67f" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt;&lt;code&gt;polynomial_kernel&lt;/code&gt;&lt;/a&gt; computes the degree-d polynomial kernel between two vectors. The polynomial kernel represents the similarity between two vectors. Conceptually, the polynomial kernels considers not only the similarity between vectors under the same dimension, but also across dimensions. When used in machine learning algorithms, this allows to account for feature interaction.</source>
          <target state="translated">Функция &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt; &lt;code&gt;polynomial_kernel&lt;/code&gt; &lt;/a&gt; вычисляет ядро ​​полинома степени d между двумя векторами. Ядро полинома представляет собой подобие двух векторов. Концептуально полиномиальные ядра учитывают не только сходство между векторами одного и того же измерения, но и между измерениями. При использовании в алгоритмах машинного обучения это позволяет учитывать взаимодействие функций.</target>
        </trans-unit>
        <trans-unit id="2f5b051b13aa3478b0b4a17587fa0bec1e6b87b4" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.rbf_kernel#sklearn.metrics.pairwise.rbf_kernel&quot;&gt;&lt;code&gt;rbf_kernel&lt;/code&gt;&lt;/a&gt; computes the radial basis function (RBF) kernel between two vectors. This kernel is defined as:</source>
          <target state="translated">Функция &lt;a href=&quot;generated/sklearn.metrics.pairwise.rbf_kernel#sklearn.metrics.pairwise.rbf_kernel&quot;&gt; &lt;code&gt;rbf_kernel&lt;/code&gt; &lt;/a&gt; вычисляет ядро радиальной базисной функции (RBF) между двумя векторами. Это ядро ​​определяется как:</target>
        </trans-unit>
        <trans-unit id="a4ac87fe4bd82908551f017a1e984b845dfe00ca" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.sigmoid_kernel#sklearn.metrics.pairwise.sigmoid_kernel&quot;&gt;&lt;code&gt;sigmoid_kernel&lt;/code&gt;&lt;/a&gt; computes the sigmoid kernel between two vectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer Perceptron (because, in the neural network field, it is often used as neuron activation function). It is defined as:</source>
          <target state="translated">Функция &lt;a href=&quot;generated/sklearn.metrics.pairwise.sigmoid_kernel#sklearn.metrics.pairwise.sigmoid_kernel&quot;&gt; &lt;code&gt;sigmoid_kernel&lt;/code&gt; &lt;/a&gt; вычисляет сигмоидальное ядро ​​между двумя векторами. Сигмовидное ядро ​​также известно как гиперболический тангенс или многослойный персептрон (потому что в области нейронной сети он часто используется как функция активации нейрона). Это определяется как:</target>
        </trans-unit>
        <trans-unit id="2e327cd188c9064345dfa4a6a6764985ae429bc7" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt;&lt;code&gt;roc_curve&lt;/code&gt;&lt;/a&gt; computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;receiver operating characteristic curve, or ROC curve&lt;/a&gt;. Quoting Wikipedia :</source>
          <target state="translated">Функция &lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt; &lt;code&gt;roc_curve&lt;/code&gt; &lt;/a&gt; вычисляет &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;кривую рабочей характеристики приемника или кривую ROC&lt;/a&gt; . Цитата из Википедии:</target>
        </trans-unit>
        <trans-unit id="a52cb65c85990e02ab77ee98f8132a1c3b7a7b6c" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; has a similar interface to &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt;, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised).</source>
          <target state="translated">Функция &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt; имеет интерфейс, аналогичный &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt; , но возвращает для каждого элемента во входных данных прогноз, который был получен для этого элемента, когда он был в тестовом наборе. Могут использоваться только стратегии перекрестной проверки, которые назначают все элементы набору тестов ровно один раз (в противном случае возникает исключение).</target>
        </trans-unit>
        <trans-unit id="9da333e152484b3ac872458a2a577c489d5042de" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.model_selection.validation_curve#sklearn.model_selection.validation_curve&quot;&gt;&lt;code&gt;validation_curve&lt;/code&gt;&lt;/a&gt; can help in this case:</source>
          <target state="translated">В этом случае может помочь функция &lt;a href=&quot;generated/sklearn.model_selection.validation_curve#sklearn.model_selection.validation_curve&quot;&gt; &lt;code&gt;validation_curve&lt;/code&gt; &lt;/a&gt; :</target>
        </trans-unit>
        <trans-unit id="ef1bfedb8d96897b52fb746234adf67b6ec8f0b2" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt;&lt;code&gt;normalize&lt;/code&gt;&lt;/a&gt; provides a quick and easy way to perform this operation on a single array-like dataset, either using the &lt;code&gt;l1&lt;/code&gt; or &lt;code&gt;l2&lt;/code&gt; norms:</source>
          <target state="translated">Функция &lt;a href=&quot;generated/sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt; &lt;code&gt;normalize&lt;/code&gt; &lt;/a&gt; обеспечивает быстрый и простой способ выполнить эту операцию с одним массивом данных, используя нормы &lt;code&gt;l1&lt;/code&gt; или &lt;code&gt;l2&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="621182b188f45e3dd8884ee1d03015ee530041c8" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt;&lt;code&gt;scale&lt;/code&gt;&lt;/a&gt; provides a quick and easy way to perform this operation on a single array-like dataset:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt; &lt;code&gt;scale&lt;/code&gt; &lt;/a&gt; функции обеспечивает быстрый и простой способ выполнить эту операцию с одним массивом данных:</target>
        </trans-unit>
        <trans-unit id="6ffd4cf30ad5a8e8b6ec54710ec8f9345471233e" translate="yes" xml:space="preserve">
          <source>The function &lt;code&gt;plot_regression_results&lt;/code&gt; is used to plot the predicted and true targets.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="14656519a034ad24c06a19460128dde85e00e72d" translate="yes" xml:space="preserve">
          <source>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in &lt;a href=&quot;#r37d39d7589e2-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; and &lt;a href=&quot;#r37d39d7589e2-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Both methods are based on the idea originally proposed in &lt;a href=&quot;#r37d39d7589e2-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">Функция основана на непараметрических методах, основанных на оценке энтропии на основе расстояний k ближайших соседей, как описано в &lt;a href=&quot;#r37d39d7589e2-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; и &lt;a href=&quot;#r37d39d7589e2-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt; . Оба метода основаны на идее, первоначально предложенной в &lt;a href=&quot;#r37d39d7589e2-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="cf1d60e35345e80734f9e0bef9342c1dede910a1" translate="yes" xml:space="preserve">
          <source>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in &lt;a href=&quot;#r50b872b699c4-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; and &lt;a href=&quot;#r50b872b699c4-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Both methods are based on the idea originally proposed in &lt;a href=&quot;#r50b872b699c4-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">Функция основана на непараметрических методах, основанных на оценке энтропии на основе расстояний k ближайших соседей, как описано в &lt;a href=&quot;#r50b872b699c4-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; и &lt;a href=&quot;#r50b872b699c4-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt; . Оба метода основаны на идее, первоначально предложенной в &lt;a href=&quot;#r50b872b699c4-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="fb8314efd5d7c36c4d228413206cd5331b82e456" translate="yes" xml:space="preserve">
          <source>The function requires either the argument &lt;code&gt;grid&lt;/code&gt; which specifies the values of the target features on which the partial dependence function should be evaluated or the argument &lt;code&gt;X&lt;/code&gt; which is a convenience mode for automatically creating &lt;code&gt;grid&lt;/code&gt; from the training data. If &lt;code&gt;X&lt;/code&gt; is given, the &lt;code&gt;axes&lt;/code&gt; value returned by the function gives the axis for each target feature.</source>
          <target state="translated">Для функции требуется либо &lt;code&gt;grid&lt;/code&gt; аргументов, которая указывает значения целевых функций, на которых должна оцениваться функция частичной зависимости, либо аргумент &lt;code&gt;X&lt;/code&gt; , который является удобным режимом для автоматического создания &lt;code&gt;grid&lt;/code&gt; из обучающих данных. Если задан &lt;code&gt;X&lt;/code&gt; , значение &lt;code&gt;axes&lt;/code&gt; возвращаемое функцией, дает ось для каждой целевой функции.</target>
        </trans-unit>
        <trans-unit id="d2fedea237bdc334f6322c4692b523df3b5f6fbf" translate="yes" xml:space="preserve">
          <source>The function to be decorated</source>
          <target state="translated">Функция,которая будет украшена</target>
        </trans-unit>
        <trans-unit id="f7fc9606332ff37416d86060e52aa56595a5e3ce" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;friedman_mse&amp;rdquo; for the mean squared error with improvement score by Friedman, &amp;ldquo;mse&amp;rdquo; for mean squared error, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error. The default value of &amp;ldquo;friedman_mse&amp;rdquo; is generally the best as it can provide a better approximation in some cases.</source>
          <target state="translated">Функция измерения качества раскола. Поддерживаемые критерии: &amp;laquo;friedman_mse&amp;raquo; для среднеквадратичной ошибки с оценкой улучшения по Фридману, &amp;laquo;mse&amp;raquo; для среднеквадратичной ошибки и &amp;laquo;mae&amp;raquo; для средней абсолютной ошибки. Значение по умолчанию &amp;laquo;friedman_mse&amp;raquo; обычно является лучшим, поскольку в некоторых случаях оно может обеспечить лучшее приближение.</target>
        </trans-unit>
        <trans-unit id="8c0bede693f27f1257cedfa964bd89180e6bfada" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;gini&amp;rdquo; for the Gini impurity and &amp;ldquo;entropy&amp;rdquo; for the information gain.</source>
          <target state="translated">Функция измерения качества раскола. Поддерживаемые критерии: &amp;laquo;Джини&amp;raquo; для примеси Джини и &amp;laquo;энтропия&amp;raquo; для получения информации.</target>
        </trans-unit>
        <trans-unit id="03e746b06910ee8c07bc239c0b780e5294140dfb" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;gini&amp;rdquo; for the Gini impurity and &amp;ldquo;entropy&amp;rdquo; for the information gain. Note: this parameter is tree-specific.</source>
          <target state="translated">Функция измерения качества раскола. Поддерживаемые критерии: &amp;laquo;Джини&amp;raquo; для примеси Джини и &amp;laquo;энтропия&amp;raquo; для получения информации. Примечание: этот параметр зависит от дерева.</target>
        </trans-unit>
        <trans-unit id="c211c5fb9289c19b7ab7fb609c5f42d0fd7fb417" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;mse&amp;rdquo; for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, &amp;ldquo;friedman_mse&amp;rdquo;, which uses mean squared error with Friedman&amp;rsquo;s improvement score for potential splits, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error, which minimizes the L1 loss using the median of each terminal node.</source>
          <target state="translated">Функция измерения качества раскола. Поддерживаемыми критериями являются &amp;laquo;mse&amp;raquo; для среднеквадратичной ошибки, которая равна уменьшению дисперсии в качестве критерия выбора признаков и минимизирует потери L2 с использованием среднего значения каждого конечного узла, &amp;laquo;friedman_mse&amp;raquo;, который использует среднеквадратичную ошибку с оценкой улучшения Фридмана для потенциального разбивается, и &amp;laquo;mae&amp;raquo; для средней абсолютной ошибки, которая минимизирует потери L1 с использованием медианы каждого конечного узла.</target>
        </trans-unit>
        <trans-unit id="bf422f8c78875448c6e34d2c008baf4a5621c47d" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;mse&amp;rdquo; for the mean squared error, which is equal to variance reduction as feature selection criterion, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error.</source>
          <target state="translated">Функция измерения качества раскола. Поддерживаемые критерии: &amp;laquo;mse&amp;raquo; для среднеквадратичной ошибки, которая равна уменьшению дисперсии в качестве критерия выбора признаков, и &amp;laquo;mae&amp;raquo; для средней абсолютной ошибки.</target>
        </trans-unit>
        <trans-unit id="8599be908b4b8dfee70cafa139c14ed8be2f9fde" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;lsquo;friedman_mse&amp;rsquo; for the mean squared error with improvement score by Friedman, &amp;lsquo;mse&amp;rsquo; for mean squared error, and &amp;lsquo;mae&amp;rsquo; for the mean absolute error. The default value of &amp;lsquo;friedman_mse&amp;rsquo; is generally the best as it can provide a better approximation in some cases.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b3c9548b1b42f5af71c7bfe5f885c712284a1e6" translate="yes" xml:space="preserve">
          <source>The function which is applied on each chunk of the distance matrix, reducing it to needed values. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; is called repeatedly, where &lt;code&gt;D_chunk&lt;/code&gt; is a contiguous vertical slice of the pairwise distance matrix, starting at row &lt;code&gt;start&lt;/code&gt;. It should return an array, a list, or a sparse matrix of length &lt;code&gt;D_chunk.shape[0]&lt;/code&gt;, or a tuple of such objects.</source>
          <target state="translated">Функция, которая применяется к каждому фрагменту матрицы расстояний, уменьшая ее до необходимых значений. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; вызывается повторно, где &lt;code&gt;D_chunk&lt;/code&gt; - это непрерывный вертикальный слой матрицы попарных расстояний, начиная с &lt;code&gt;start&lt;/code&gt; строки . Он должен возвращать массив, список или разреженную матрицу длины &lt;code&gt;D_chunk.shape[0]&lt;/code&gt; или кортеж таких объектов.</target>
        </trans-unit>
        <trans-unit id="0114f911a9d8b12f31f5da2c60626b44d9e0ba26" translate="yes" xml:space="preserve">
          <source>The function which is applied on each chunk of the distance matrix, reducing it to needed values. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; is called repeatedly, where &lt;code&gt;D_chunk&lt;/code&gt; is a contiguous vertical slice of the pairwise distance matrix, starting at row &lt;code&gt;start&lt;/code&gt;. It should return one of: None; an array, a list, or a sparse matrix of length &lt;code&gt;D_chunk.shape[0]&lt;/code&gt;; or a tuple of such objects. Returning None is useful for in-place operations, rather than reductions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34f983a96e6e1ba7a36ae52fb70df739e3a5aca9" translate="yes" xml:space="preserve">
          <source>The functional form of the G function used in the approximation to neg-entropy. Could be either &amp;lsquo;logcosh&amp;rsquo;, &amp;lsquo;exp&amp;rsquo;, or &amp;lsquo;cube&amp;rsquo;. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. Example:</source>
          <target state="translated">Функциональная форма G-функции, используемая в приближении отрицательной энтропии. Может быть либо logcosh, либо exp, либо cube. Вы также можете указать свою функцию. Он должен вернуть кортеж, содержащий значение функции и ее производной в точке. Пример:</target>
        </trans-unit>
        <trans-unit id="f3580ae3ff67a44925f6075f1fc46034cd1f3d14" translate="yes" xml:space="preserve">
          <source>The generated array.</source>
          <target state="translated">Сгенерированный массив.</target>
        </trans-unit>
        <trans-unit id="22dd341f8c92381f47326cd6fbe4fff46dd59a6b" translate="yes" xml:space="preserve">
          <source>The generated matrix.</source>
          <target state="translated">Сгенерированная матрица.</target>
        </trans-unit>
        <trans-unit id="b670a7959baa25f4a9c290d5df951114cede8bad" translate="yes" xml:space="preserve">
          <source>The generated samples.</source>
          <target state="translated">Сгенерированные образцы.</target>
        </trans-unit>
        <trans-unit id="61a0705074aa05bf429a2ff6dd9f09842d1c86f2" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the centers. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">Генератор, используемый для инициализации центров. Если int, random_state - это начальное число, используемое генератором случайных чисел; Если экземпляр RandomState, random_state является генератором случайных чисел; Если None, генератор случайных чисел - это экземпляр RandomState, используемый &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="17ea6c727e52953fb5e03ae24f1b3fcbc132ab70" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the centers. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="473c7ed23812d4bf8fcf6c97ab0551b7301ccf95" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the codebook. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">Генератор, используемый для инициализации кодовой книги. Если int, random_state - это начальное число, используемое генератором случайных чисел; Если экземпляр RandomState, random_state является генератором случайных чисел; Если None, генератор случайных чисел - это экземпляр RandomState, используемый &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a0fb8deb3b7c972152c6c4cb8a6e10d0c386fcd5" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the codebook. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4e831547e83750929880db075c40fc4aa61eb4b8" translate="yes" xml:space="preserve">
          <source>The generator used to randomize the design. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">Генератор, используемый для рандомизации дизайна. Если int, random_state - это начальное число, используемое генератором случайных чисел; Если экземпляр RandomState, random_state является генератором случайных чисел; Если None, генератор случайных чисел - это экземпляр RandomState, используемый &lt;code&gt;np.random&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cd84f22d9e60e3f86bb2455d78aaf109647d76b1" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select a subset of samples. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;sample_size is not None&lt;/code&gt;.</source>
          <target state="translated">Генератор, используемый для случайного выбора подмножества образцов. Если int, random_state - это начальное число, используемое генератором случайных чисел; Если экземпляр RandomState, random_state является генератором случайных чисел; Если None, генератор случайных чисел - это экземпляр RandomState, используемый &lt;code&gt;np.random&lt;/code&gt; . Используется, когда &lt;code&gt;sample_size is not None&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="36aa9d7d033388d006150bbc613bf9f61afb2f31" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select the samples from input points for bandwidth estimation. Use an int to make the randomness deterministic. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">Генератор, используемый для случайного выбора выборок из входных точек для оценки пропускной способности. Используйте int, чтобы сделать случайность детерминированной. См. &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;Глоссарий&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="eebbb245bcabbcd7cdd05b74aa699ac85737839b" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select the samples from input points for bandwidth estimation. Use an int to make the randomness deterministic. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff752b880533b887a66601fd8d804091da8b44b7" translate="yes" xml:space="preserve">
          <source>The goal is to compare different estimators to see which one is best for the &lt;a href=&quot;../../modules/generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;sklearn.impute.IterativeImputer&lt;/code&gt;&lt;/a&gt; when using a &lt;a href=&quot;../../modules/generated/sklearn.linear_model.bayesianridge#sklearn.linear_model.BayesianRidge&quot;&gt;&lt;code&gt;sklearn.linear_model.BayesianRidge&lt;/code&gt;&lt;/a&gt; estimator on the California housing dataset with a single value randomly removed from each row.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6c9c479a6511fc655c70a7c51ab0a85c04034b21" translate="yes" xml:space="preserve">
          <source>The goal is to measure the latency one can expect when doing predictions either in bulk or atomic (i.e. one by one) mode.</source>
          <target state="translated">Целью является измерение латентности,которую можно ожидать при выполнении предсказаний как в массовом,так и в атомном (т.е.один за другим)режиме.</target>
        </trans-unit>
        <trans-unit id="95b3715b7309073ce23a5e1ae6bd1c83fb5ab128" translate="yes" xml:space="preserve">
          <source>The goal of &lt;strong&gt;ensemble methods&lt;/strong&gt; is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.</source>
          <target state="translated">Цель &lt;strong&gt;ансамблевых методов&lt;/strong&gt; состоит в том, чтобы объединить предсказания нескольких базовых оценок, построенных с заданным алгоритмом обучения, чтобы улучшить обобщаемость / надежность по сравнению с одной оценкой.</target>
        </trans-unit>
        <trans-unit id="a736cc9a27a226bf9910cdd3892bdfd0fe07d378" translate="yes" xml:space="preserve">
          <source>The goal of NCA is to learn an optimal linear transformation matrix of size &lt;code&gt;(n_components, n_features)&lt;/code&gt;, which maximises the sum over all samples \(i\) of the probability \(p_i\) that \(i\) is correctly classified, i.e.:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4be143708c968fbd1f8e4e8288622de6f1251f81" translate="yes" xml:space="preserve">
          <source>The goal of this example is to analyze the graph of links inside wikipedia articles to rank articles by relative importance according to this eigenvector centrality.</source>
          <target state="translated">Целью данного примера является анализ графика ссылок внутри статей Википедии для ранжирования статей по относительной важности в соответствии с этим собственным вектором централизации.</target>
        </trans-unit>
        <trans-unit id="5095c3c5239296131fc48d25860f3771ab9b91e3" translate="yes" xml:space="preserve">
          <source>The goal of this example is to show intuitively how the metrics behave, and not to find good clusters for the digits. This is why the example works on a 2D embedding.</source>
          <target state="translated">Цель этого примера-показать интуитивно,как ведут себя метрики,а не находить хорошие кластеры для цифр.Вот почему пример работает на 2D встраивании.</target>
        </trans-unit>
        <trans-unit id="20bd11aa678c9e04777a40ec0f194ab6b7581aa6" translate="yes" xml:space="preserve">
          <source>The goal of this guide is to explore some of the main &lt;code&gt;scikit-learn&lt;/code&gt; tools on a single practical task: analyzing a collection of text documents (newsgroups posts) on twenty different topics.</source>
          <target state="translated">Цель этого руководства - изучить некоторые из основных инструментов &lt;code&gt;scikit-learn&lt;/code&gt; для одной практической задачи: анализа коллекции текстовых документов (сообщений групп новостей) по двадцати различным темам.</target>
        </trans-unit>
        <trans-unit id="c224302bdf144aacfdfc260a6638fe1107c2e8d4" translate="yes" xml:space="preserve">
          <source>The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.</source>
          <target state="translated">Целью использования tf-idf вместо необработанных частот появления жетонов в данном документе является уменьшение влияния жетонов,которые очень часто встречаются в данном корпусе,и которые,следовательно,эмпирически менее информативны,чем черты,которые встречаются в малой части учебного корпуса.</target>
        </trans-unit>
        <trans-unit id="7c4e551c63ec44d66aa3a175c12b51d46fb0ca3e" translate="yes" xml:space="preserve">
          <source>The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when &lt;code&gt;eval_gradient&lt;/code&gt; is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3325084d0d02139eb921a2545ba48cff39657720" translate="yes" xml:space="preserve">
          <source>The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.</source>
          <target state="translated">Градиент ядра k(X,X)относительно гиперпараметра ядра.Возвращается только тогда,когда eval_градиент равен True.</target>
        </trans-unit>
        <trans-unit id="1d28aeaa2593827c70ee0cbf35f8d5891fd13e08" translate="yes" xml:space="preserve">
          <source>The graph data is fetched from the DBpedia dumps. DBpedia is an extraction of the latent structured data of the Wikipedia content.</source>
          <target state="translated">Данные графика извлекаются из дампов DBpedia.DBpedia-это извлечение скрытых структурированных данных из содержимого Википедии.</target>
        </trans-unit>
        <trans-unit id="94a8641249b9d38c741294dd700ba9c013e60d15" translate="yes" xml:space="preserve">
          <source>The graph should contain only one connect component, elsewhere the results make little sense.</source>
          <target state="translated">График должен содержать только один компонент соединения,в остальном результаты не имеют смысла.</target>
        </trans-unit>
        <trans-unit id="0aca214d3abec7143ce0e3db3703c7759829a517" translate="yes" xml:space="preserve">
          <source>The graphical model of LDA is a three-level Bayesian model:</source>
          <target state="translated">Графическая модель LDA представляет собой трехуровневую байесовскую модель:</target>
        </trans-unit>
        <trans-unit id="53c200c93d4342eee587d3c6f82bd52b691b78c8" translate="yes" xml:space="preserve">
          <source>The graphical model of LDA is a three-level generative model:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b70b496cfa90301063390d7d501e40e03165e505" translate="yes" xml:space="preserve">
          <source>The graphical model of an RBM is a fully-connected bipartite graph.</source>
          <target state="translated">Графическая модель УКР представляет собой полностью связанный график бипартита.</target>
        </trans-unit>
        <trans-unit id="43e8165a75805bcff11ce07ae2103365830550d8" translate="yes" xml:space="preserve">
          <source>The grid of &lt;code&gt;target_variables&lt;/code&gt; values for which the partial dependecy should be evaluated (either &lt;code&gt;grid&lt;/code&gt; or &lt;code&gt;X&lt;/code&gt; must be specified).</source>
          <target state="translated">Сетка значений &lt;code&gt;target_variables&lt;/code&gt; , для которых должна быть оценена частичная зависимость ( необходимо указать &lt;code&gt;grid&lt;/code&gt; или &lt;code&gt;X&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="1fb55e8edd66947697c84e91d6ac3fa247bd29e9" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting</source>
          <target state="translated">Решетка альфа-фаз,используемая для арматуры</target>
        </trans-unit>
        <trans-unit id="c8ce42c94fb7c4644fc397d481823f721280ec22" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting, for each l1_ratio</source>
          <target state="translated">Сетка альфов,используемая для подгонки,для каждого l1_рационала</target>
        </trans-unit>
        <trans-unit id="92edf0193fba09badba27bf7739525bce6da7601" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting, for each l1_ratio.</source>
          <target state="translated">Сетка альфов,используемая для подгонки,для каждого l1_рационала.</target>
        </trans-unit>
        <trans-unit id="22daba72217df04ec1af1a8a340277c45da4b54e" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting.</source>
          <target state="translated">Решетка альфа-фаз,используемая для арматуры.</target>
        </trans-unit>
        <trans-unit id="37c7f40c413d2aeb11b8645a838ea3c671b821b6" translate="yes" xml:space="preserve">
          <source>The grid points between 0 and 1: alpha/alpha_max</source>
          <target state="translated">Точки сетки между 0 и 1:alpha/alpha_max</target>
        </trans-unit>
        <trans-unit id="1e90c7186240eacb6693334f5a2cb603522a3c95" translate="yes" xml:space="preserve">
          <source>The grid search instance behaves like a normal &lt;code&gt;scikit-learn&lt;/code&gt; model. Let&amp;rsquo;s perform the search on a smaller subset of the training data to speed up the computation:</source>
          <target state="translated">Экземпляр поиска по сетке ведет себя как обычная &lt;code&gt;scikit-learn&lt;/code&gt; . Давайте выполним поиск на меньшем подмножестве обучающих данных, чтобы ускорить вычисления:</target>
        </trans-unit>
        <trans-unit id="0fb72e7d32db09c24209afb30ebd0c3ef9469a12" translate="yes" xml:space="preserve">
          <source>The grid search provided by &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; exhaustively generates candidates from a grid of parameter values specified with the &lt;code&gt;param_grid&lt;/code&gt; parameter. For instance, the following &lt;code&gt;param_grid&lt;/code&gt;:</source>
          <target state="translated">Поиск по сетке, предоставляемый &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; ,&lt;/a&gt; исчерпывающе генерирует кандидатов из сетки значений параметров, указанных с &lt;code&gt;param_grid&lt;/code&gt; параметра param_grid . Например, следующий &lt;code&gt;param_grid&lt;/code&gt; :</target>
        </trans-unit>
        <trans-unit id="5d877967ec11cd12214237a4561547e48123553d" translate="yes" xml:space="preserve">
          <source>The guidelines for choosing a metric is to use one that maximizes the distance between samples in different classes, and minimizes that within each class.</source>
          <target state="translated">Руководство по выбору метрики заключается в использовании такой метрики,которая максимизирует расстояние между образцами в различных классах и минимизирует его внутри каждого класса.</target>
        </trans-unit>
        <trans-unit id="42b20fdbf19bf6fb28b9336fb1c931e7ebd9ff54" translate="yes" xml:space="preserve">
          <source>The handwritten digit dataset has 1797 total points. The model will be trained using all points, but only 30 will be labeled. Results in the form of a confusion matrix and a series of metrics over each class will be very good.</source>
          <target state="translated">Набор данных из рукописных цифр насчитывает 1797 пунктов.Модель будет обучена с использованием всех точек,но только 30 будет помечено.Результаты в виде матрицы путаницы и ряда метрик над каждым классом будут очень хорошими.</target>
        </trans-unit>
        <trans-unit id="1dbebc05eff701a160c5a2149017afcbb50998d6" translate="yes" xml:space="preserve">
          <source>The hash function employed is the signed 32-bit version of Murmurhash3.</source>
          <target state="translated">Используется хэш-функция-подписанная 32-битная версия Murmurhash3.</target>
        </trans-unit>
        <trans-unit id="86a9e346ee809bf0b27001ea98677db7fe898a9e" translate="yes" xml:space="preserve">
          <source>The higher &lt;code&gt;p&lt;/code&gt; the less weight is given to extreme deviations between true and predicted targets.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6ddf3e17ce9800f43a67078b41067ddc0d8c6a4c" translate="yes" xml:space="preserve">
          <source>The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the simplex.</source>
          <target state="translated">Более высокая концентрация увеличивает массу в центре и приводит к тому,что больше компонентов активны,в то время как более низкий параметр концентрации приводит к увеличению массы на краю симплекса.</target>
        </trans-unit>
        <trans-unit id="d55d0516aec7ebe6efc3bc1e9f0b83c115b36898" translate="yes" xml:space="preserve">
          <source>The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1006ebebebbef4245b7568132e1eabf566a79140" translate="yes" xml:space="preserve">
          <source>The highest p-value for features to be kept.</source>
          <target state="translated">Самое высокое р-значение для сохранения функций.</target>
        </trans-unit>
        <trans-unit id="0c1d204d0071e8e2a8101579dd644bade4aef0d9" translate="yes" xml:space="preserve">
          <source>The highest uncorrected p-value for features to keep.</source>
          <target state="translated">Самое высокое неисправленное р-значение для сохранения функций.</target>
        </trans-unit>
        <trans-unit id="3708900f6a079bc16cd76c37da5647812d1a0427" translate="yes" xml:space="preserve">
          <source>The histogram of the estimated weights is very peaked, as a sparsity-inducing prior is implied on the weights.</source>
          <target state="translated">Гистограмма предполагаемых весов является очень пиковой,так как на весах предполагается,что они вызывают спазмы.</target>
        </trans-unit>
        <trans-unit id="d24bdc7c6b1e4dbeb474c6e9f76042366464386e" translate="yes" xml:space="preserve">
          <source>The hyperparameters</source>
          <target state="translated">Гиперпараметры</target>
        </trans-unit>
        <trans-unit id="25f29cead3cfba36338bcf026da5a846edf25636" translate="yes" xml:space="preserve">
          <source>The i-th score &lt;code&gt;train_score_[i]&lt;/code&gt; is the deviance (= loss) of the model at iteration &lt;code&gt;i&lt;/code&gt; on the in-bag sample. If &lt;code&gt;subsample == 1&lt;/code&gt; this is the deviance on the training data.</source>
          <target state="translated">I-я оценка &lt;code&gt;train_score_[i]&lt;/code&gt; - это отклонение (= потеря) модели на &lt;code&gt;i&lt;/code&gt; -й итерации выборки из пакета. Если &lt;code&gt;subsample == 1&lt;/code&gt; это отклонение данных обучения.</target>
        </trans-unit>
        <trans-unit id="7768180d6d5c29f9be6eb9d80f99cbf93007e25a" translate="yes" xml:space="preserve">
          <source>The i.i.d. assumption is broken if the underlying generative process yield groups of dependent samples.</source>
          <target state="translated">Предположение i.i.d.нарушается,если лежащий в основе генеративного процесса процесс дает группы зависимых образцов.</target>
        </trans-unit>
        <trans-unit id="d5e94b9dd17268ac8457a9d26f68d07b8365584a" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f4d07aca22136c4b5a250fb6a846554681fb509" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;a href=&quot;generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt;&lt;code&gt;VotingRegressor&lt;/code&gt;&lt;/a&gt; is to combine conceptually different machine learning regressors and return the average predicted values. Such a regressor can be useful for a set of equally well performing models in order to balance out their individual weaknesses.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="190c7529dfd22ae7ed70d4f84d1deb499abbe749" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;code&gt;VotingClassifier&lt;/code&gt; is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.</source>
          <target state="translated">Идея &lt;code&gt;VotingClassifier&lt;/code&gt; состоит в том, чтобы объединить концептуально разные классификаторы машинного обучения и использовать большинство голосов или средние предсказанные вероятности (мягкое голосование) для прогнозирования меток классов. Такой классификатор может быть полезен для набора одинаково хорошо работающих моделей, чтобы уравновесить их индивидуальные недостатки.</target>
        </trans-unit>
        <trans-unit id="f4f9efa7d25a183372e2e9ce75bfe20581578033" translate="yes" xml:space="preserve">
          <source>The image as a numpy array: height x width x color</source>
          <target state="translated">Изображение в виде массива numpy:высота х ширина х цвет</target>
        </trans-unit>
        <trans-unit id="77bae2f833c03e6f85b6a43b3aa3ead8f81ce2e5" translate="yes" xml:space="preserve">
          <source>The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms.</source>
          <target state="translated">Изображение квантовано до 256 уровней серого и хранится в виде беззнаковых 8-битных целых чисел;загрузчик преобразует их в значения с плавающей точкой на интервале [0,1],с которыми легче работать для многих алгоритмов.</target>
        </trans-unit>
        <trans-unit id="6db946ce103c288b47eadb4f1488a8fdc91a7488" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients. See &lt;a href=&quot;#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt; for another implementation:</source>
          <target state="translated">Реализация в классе &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt; использует координатный спуск в качестве алгоритма подбора коэффициентов. Смотрите &lt;a href=&quot;#least-angle-regression&quot;&gt;регрессию наименьшего угла&lt;/a&gt; для другой реализации:</target>
        </trans-unit>
        <trans-unit id="64dabeaa3f8e38c30333615b1b174cbd1348b11e" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt;&lt;code&gt;MultiTaskElasticNet&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients.</source>
          <target state="translated">Реализация в классе &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt; &lt;code&gt;MultiTaskElasticNet&lt;/code&gt; &lt;/a&gt; использует спуск координат в качестве алгоритма подбора коэффициентов.</target>
        </trans-unit>
        <trans-unit id="555e24352725a2133c35e7a52f56a702cd7f81c4" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt;&lt;code&gt;MultiTaskLasso&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients.</source>
          <target state="translated">Реализация в классе &lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt; &lt;code&gt;MultiTaskLasso&lt;/code&gt; &lt;/a&gt; использует спуск координат в качестве алгоритма подбора коэффициентов.</target>
        </trans-unit>
        <trans-unit id="f9cb17c43bed4736104925d68075ebd6b07ace68" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 2.1 of &lt;a href=&quot;#rw2006&quot; id=&quot;id1&quot;&gt;[RW2006]&lt;/a&gt;. In addition to the API of standard scikit-learn estimators, GaussianProcessRegressor:</source>
          <target state="translated">Реализация основана на алгоритме 2.1 &lt;a href=&quot;#rw2006&quot; id=&quot;id1&quot;&gt;[RW2006]&lt;/a&gt; . В дополнение к API стандартных оценщиков scikit-learn, GaussianProcessRegressor:</target>
        </trans-unit>
        <trans-unit id="70780cc3dac260d0fb4e85faed94f4134c9dc0ae" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 2.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.</source>
          <target state="translated">Реализация основана на алгоритме 2.1 Гауссовых процессов машинного обучения (GPML)Расмуссена и Вильямса.</target>
        </trans-unit>
        <trans-unit id="1904d3648b9818da40270920e1855bd9dcd752d1" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.</source>
          <target state="translated">Реализация основана на алгоритме 3.1,3.2 и 5.1 Гауссовых процессов машинного обучения (GPML)Расмуссена и Вильямса.</target>
        </trans-unit>
        <trans-unit id="0eeedaeb1d413ea7240926a8ca5811e478d2d62b" translate="yes" xml:space="preserve">
          <source>The implementation is based on an ensemble of ExtraTreeRegressor. The maximum depth of each tree is set to &lt;code&gt;ceil(log_2(n))&lt;/code&gt; where \(n\) is the number of samples used to build the tree (see (Liu et al., 2008) for more details).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="166f55526ba60cbc129406f3899569f7b0eb4efd" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm.</source>
          <target state="translated">Реализация основана на libsvm.</target>
        </trans-unit>
        <trans-unit id="cb75b7ddb016087965f5fad7be27fc6c1cb16290" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.</source>
          <target state="translated">Реализация основана на libsvm.Сложность подгонки по времени более чем квадратична с количеством отсчетов,что затрудняет масштабирование под набор данных более чем с парой 10000 отсчетов.</target>
        </trans-unit>
        <trans-unit id="fa8767a0fc5ecb59976beb65f6a5637eaddaef83" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to datasets with more than a couple of 10000 samples. For large datasets consider using &lt;a href=&quot;sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVR&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDRegressor&lt;/code&gt;&lt;/a&gt; instead, possibly after a &lt;a href=&quot;sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;sklearn.kernel_approximation.Nystroem&lt;/code&gt;&lt;/a&gt; transformer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c37eaa47201071ab85ed7ee61d61deb2b696067c" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using &lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt; instead, possibly after a &lt;a href=&quot;sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;sklearn.kernel_approximation.Nystroem&lt;/code&gt;&lt;/a&gt; transformer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4dec30aaf65f05fda35ca6a3928a075e5f1587d" translate="yes" xml:space="preserve">
          <source>The implementation is designed to:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e44f638bc5a5bdc8405d919df1e1a354ca31f099" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt;&lt;code&gt;ensemble.IsolationForest&lt;/code&gt;&lt;/a&gt; is based on an ensemble of &lt;a href=&quot;generated/sklearn.tree.extratreeregressor#sklearn.tree.ExtraTreeRegressor&quot;&gt;&lt;code&gt;tree.ExtraTreeRegressor&lt;/code&gt;&lt;/a&gt;. Following Isolation Forest original paper, the maximum depth of each tree is set to \(\lceil \log_2(n) \rceil\) where \(n\) is the number of samples used to build the tree (see (Liu et al., 2008) for more details).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="08d84e0cf0c38a38100a9dad48e0efc744fd2db8" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; in scikit-learn follows a generalization to a multivariate linear regression model &lt;a href=&quot;#f1&quot; id=&quot;id30&quot;&gt;[8]&lt;/a&gt; using the spatial median which is a generalization of the median to multiple dimensions &lt;a href=&quot;#f2&quot; id=&quot;id31&quot;&gt;[9]&lt;/a&gt;.</source>
          <target state="translated">Реализация &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; &lt;/a&gt; в scikit-learn следует за обобщением модели многомерной линейной регрессии &lt;a href=&quot;#f1&quot; id=&quot;id30&quot;&gt;[8]&lt;/a&gt; с использованием пространственной медианы, которая является обобщением медианы на несколько измерений &lt;a href=&quot;#f2&quot; id=&quot;id31&quot;&gt;[9]&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="fda50d88e9c81601cfc445f1c078677bdd2f3d40" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; in scikit-learn follows a generalization to a multivariate linear regression model &lt;a href=&quot;#f1&quot; id=&quot;id36&quot;&gt;12&lt;/a&gt; using the spatial median which is a generalization of the median to multiple dimensions &lt;a href=&quot;#f2&quot; id=&quot;id37&quot;&gt;13&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb185b2e148e3613cdde933dd05a6b32e3b9acbe" translate="yes" xml:space="preserve">
          <source>The implementation of SGD is influenced by the &lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot;&gt;Stochastic Gradient SVM&lt;/a&gt; of L&amp;eacute;on Bottou. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse feature vectors, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from Shalev-Shwartz et al. 2007. For multi-class classification, a &amp;ldquo;one versus all&amp;rdquo; approach is used. We use the truncated gradient algorithm proposed by Tsuruoka et al. 2009 for L1 regularization (and the Elastic Net). The code is written in Cython.</source>
          <target state="translated">На реализацию SGD влияет &lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot;&gt;стохастический градиент SVM.&lt;/a&gt;Леона Ботту. Подобно SvmSGD, вектор весов представлен как произведение скаляра и вектора, что позволяет эффективно обновлять вес в случае регуляризации L2. В случае разреженных векторов признаков пересечение обновляется с меньшей скоростью обучения (умноженной на 0,01), чтобы учесть тот факт, что он обновляется чаще. Примеры обучения выбираются последовательно, и скорость обучения снижается после каждого наблюдаемого примера. Мы приняли график скорости обучения от Shalev-Shwartz et al. 2007. Для мультиклассовой классификации используется подход &amp;laquo;один против всех&amp;raquo;. Мы используем алгоритм усеченного градиента, предложенный Tsuruoka et al. 2009 для регуляризации L1 (и эластичной сети). Код написан на Cython.</target>
        </trans-unit>
        <trans-unit id="3db47613e45248213b8540e2c5b681f6afec29a2" translate="yes" xml:space="preserve">
          <source>The implementation of SGD is influenced by the &lt;code&gt;Stochastic Gradient SVM&lt;/code&gt; of &lt;a href=&quot;#id10&quot; id=&quot;id7&quot;&gt;7&lt;/a&gt;. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse input &lt;code&gt;X&lt;/code&gt;, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from &lt;a href=&quot;#id12&quot; id=&quot;id8&quot;&gt;8&lt;/a&gt;. For multi-class classification, a &amp;ldquo;one versus all&amp;rdquo; approach is used. We use the truncated gradient algorithm proposed in &lt;a href=&quot;#id13&quot; id=&quot;id9&quot;&gt;9&lt;/a&gt; for L1 regularization (and the Elastic Net). The code is written in Cython.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="234c6abfe6fe107b5770fe372b7d9e14789fc069" translate="yes" xml:space="preserve">
          <source>The implementation of logistic regression in scikit-learn can be accessed from class &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;. This implementation can fit binary, One-vs- Rest, or multinomial logistic regression with optional L2 or L1 regularization.</source>
          <target state="translated">Доступ к реализации логистической регрессии в scikit-learn можно получить из класса &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt; . Эта реализация может соответствовать двоичной, однозначной или полиномиальной логистической регрессии с дополнительной регуляризацией L2 или L1.</target>
        </trans-unit>
        <trans-unit id="11a45f793137837fb140d6499ba2bfe32aacbae3" translate="yes" xml:space="preserve">
          <source>The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.</source>
          <target state="translated">Значение элемента вычисляется как (нормализованное)общее уменьшение критерия,привносимого этим элементом.Он также известен как значимость Джини.</target>
        </trans-unit>
        <trans-unit id="285ba1aeef83b4f72aaed81188dabeb94680bb69" translate="yes" xml:space="preserve">
          <source>The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. &lt;code&gt;oob_improvement_[0]&lt;/code&gt; is the improvement in loss of the first stage over the &lt;code&gt;init&lt;/code&gt; estimator.</source>
          <target state="translated">Улучшение потерь (= отклонения) на выборках вне упаковки по сравнению с предыдущей итерацией. &lt;code&gt;oob_improvement_[0]&lt;/code&gt; - это улучшение потерь на первом этапе по сравнению с &lt;code&gt;init&lt;/code&gt; оценки.</target>
        </trans-unit>
        <trans-unit id="820150544fe550e2e074294e51088eda9cdc8014" translate="yes" xml:space="preserve">
          <source>The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. &lt;code&gt;oob_improvement_[0]&lt;/code&gt; is the improvement in loss of the first stage over the &lt;code&gt;init&lt;/code&gt; estimator. Only available if &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d60773790f2e7bcb156876ca46ea3f1191f69076" translate="yes" xml:space="preserve">
          <source>The impurity at \(m\) is computed using an impurity function \(H()\), the choice of which depends on the task being solved (classification or regression)</source>
          <target state="translated">Примесь по адресу \(m\)вычисляется с помощью функции примеси \(H()\),выбор которой зависит от решаемой задачи (классификация или регрессия).</target>
        </trans-unit>
        <trans-unit id="d750c1db4cd012b1969f8fbe25c31782ef1879ba" translate="yes" xml:space="preserve">
          <source>The impurity-based feature importance ranks the numerical features to be the most important features. As a result, the non-predictive &lt;code&gt;random_num&lt;/code&gt; variable is ranked the most important!</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb74ef34fa0b599fafdb1111102755a10d538f2b" translate="yes" xml:space="preserve">
          <source>The impurity-based feature importances computed on tree-based models suffer from two flaws that can lead to misleading conclusions. First they are computed on statistics derived from the training dataset and therefore &lt;strong&gt;do not necessarily inform us on which features are most important to make good predictions on held-out dataset&lt;/strong&gt;. Secondly, &lt;strong&gt;they favor high cardinality features&lt;/strong&gt;, that is features with many unique values. &lt;a href=&quot;permutation_importance#permutation-importance&quot;&gt;Permutation feature importance&lt;/a&gt; is an alternative to impurity-based feature importance that does not suffer from these flaws. These two methods of obtaining feature importance are explored in: &lt;a href=&quot;../auto_examples/inspection/plot_permutation_importance#sphx-glr-auto-examples-inspection-plot-permutation-importance-py&quot;&gt;Permutation Importance vs Random Forest Feature Importance (MDI)&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a85ca2be296db64c4eea5e4c85e7d3c8770ab473" translate="yes" xml:space="preserve">
          <source>The impurity-based feature importances.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f89a5a9d5e4b0a0db4c9a50975418d448408f475" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature if axis == 0.</source>
          <target state="translated">Значение вменения заполняет каждый признак,если ось ==0.</target>
        </trans-unit>
        <trans-unit id="52ad716a60ba342ef84206f96283ef6240a59825" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature.</source>
          <target state="translated">Значение вменения заполняется для каждой функции.</target>
        </trans-unit>
        <trans-unit id="66edd575acaaa453b1fb3cfd5f9ed2a63e5987ae" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature. Computing statistics can result in &lt;code&gt;np.nan&lt;/code&gt; values. During &lt;a href=&quot;#sklearn.impute.SimpleImputer.transform&quot;&gt;&lt;code&gt;transform&lt;/code&gt;&lt;/a&gt;, features corresponding to &lt;code&gt;np.nan&lt;/code&gt; statistics will be discarded.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7cd5e57b66f65afed5b5b066bdba074e0cee3d73" translate="yes" xml:space="preserve">
          <source>The imputation strategy.</source>
          <target state="translated">Стратегия вменения.</target>
        </trans-unit>
        <trans-unit id="d67cf9b9c87d5f16b287aedd5a057873a2e85c3c" translate="yes" xml:space="preserve">
          <source>The imputed dataset. &lt;code&gt;n_output_features&lt;/code&gt; is the number of features that is not always missing during &lt;code&gt;fit&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc59b667379a9f19464aefa0413c2d5b611edad6" translate="yes" xml:space="preserve">
          <source>The imputed input data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa1b27a8286b1b98805decd6657b8fa02347921e" translate="yes" xml:space="preserve">
          <source>The index (of the &lt;code&gt;cv_results_&lt;/code&gt; arrays) which corresponds to the best candidate parameter setting.</source>
          <target state="translated">Индекс (из массивов &lt;code&gt;cv_results_&lt;/code&gt; ), который соответствует настройке наилучшего кандидата.</target>
        </trans-unit>
        <trans-unit id="d8f1885dbc976f2ceb3f7711b10b324e6546b97b" translate="yes" xml:space="preserve">
          <source>The index is computed only quantities and features inherent to the dataset.</source>
          <target state="translated">Индекс вычисляется только в количествах и характеристиках,присущих набору данных.</target>
        </trans-unit>
        <trans-unit id="eaf4dd06369196819331db8b96a42731aed3d8e0" translate="yes" xml:space="preserve">
          <source>The index is defined as the average similarity between each cluster \(C_i\) for \(i=1, ..., k\) and its most similar one \(C_j\). In the context of this index, similarity is defined as a measure \(R_{ij}\) that trades off:</source>
          <target state="translated">Индекс определяется как среднее сходство между каждым кластером \(C_i\)для \(i=1,...,k\)и его самым похожим \(C_j\).В контексте этого индекса сходство определяется как мера \(R_{ij}\),которая торгуется:</target>
        </trans-unit>
        <trans-unit id="7ad5522250591909d8f74ff707e1fa5837de3ae6" translate="yes" xml:space="preserve">
          <source>The index is the ratio of the sum of between-clusters dispersion and of inter-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4af6c67193225ac6c1c304797e9ea3402785083" translate="yes" xml:space="preserve">
          <source>The index of the cluster.</source>
          <target state="translated">Индекс кластера.</target>
        </trans-unit>
        <trans-unit id="bf1ee02857437dc2bc3e741c8e3e56ef69d1ae10" translate="yes" xml:space="preserve">
          <source>The index value of a word in the vocabulary is linked to its frequency in the whole training corpus.</source>
          <target state="translated">Значение индекса слова в словаре связано с его частотой во всем учебном корпусе.</target>
        </trans-unit>
        <trans-unit id="03c2241f87945c783a2d50da20a997d6e73ac147" translate="yes" xml:space="preserve">
          <source>The indexes of the sorted training input samples. If many tree are grown on the same dataset, this allows the ordering to be cached between trees. If None, the data will be sorted here. Don&amp;rsquo;t use this parameter unless you know what to do.</source>
          <target state="translated">Индексы отсортированных обучающих входных выборок. Если в одном наборе данных выращивается много деревьев, это позволяет кэшировать упорядочение между деревьями. Если нет, данные будут отсортированы здесь. Не используйте этот параметр, если не знаете, что делать.</target>
        </trans-unit>
        <trans-unit id="0f615a1a241d635bbd7f8073a9962f88455d5840" translate="yes" xml:space="preserve">
          <source>The indicators for cluster membership of each column.</source>
          <target state="translated">Показатели принадлежности к кластерам в каждой колонке.</target>
        </trans-unit>
        <trans-unit id="7c7c8e5dd3219610397a571246f91a523e9b0296" translate="yes" xml:space="preserve">
          <source>The indicators for cluster membership of each row.</source>
          <target state="translated">Показатели членства в кластерах каждой строки.</target>
        </trans-unit>
        <trans-unit id="1d09056572af0ab7d260b8a9a458cace43041c54" translate="yes" xml:space="preserve">
          <source>The inertia matrix uses a Heapq-based representation.</source>
          <target state="translated">Матрица инерции использует представление на основе Heapq.</target>
        </trans-unit>
        <trans-unit id="4f68a65921a2da6924b081b35cf28ef22fa8adaa" translate="yes" xml:space="preserve">
          <source>The inferred value of max_features.</source>
          <target state="translated">Предположительное значение max_features.</target>
        </trans-unit>
        <trans-unit id="9af964a5bdc6c7f6e3e23dbd4ccea9f871ad0d0c" translate="yes" xml:space="preserve">
          <source>The initial coefficients to warm-start the optimization.</source>
          <target state="translated">Начальные коэффициенты для оптимизации &quot;теплый старт&quot;.</target>
        </trans-unit>
        <trans-unit id="e326d26a40d08d9ebf56e15d175f98f61ace6983" translate="yes" xml:space="preserve">
          <source>The initial guess for the covariance.</source>
          <target state="translated">Первоначальная догадка для ковариаций.</target>
        </trans-unit>
        <trans-unit id="a31ad0b6b83b24ae472799017e15e5984848ac41" translate="yes" xml:space="preserve">
          <source>The initial guess of the noise variance for each feature. If None, it defaults to np.ones(n_features)</source>
          <target state="translated">Первоначальная догадка дисперсии шума для каждой функции.Если None,то по умолчанию она равна np.ones(n_features).</target>
        </trans-unit>
        <trans-unit id="5af1f38d3d578b440a3ef0c3e0d6f491e678143f" translate="yes" xml:space="preserve">
          <source>The initial intercept to warm-start the optimization.</source>
          <target state="translated">Начальный перехват для горячего запуска оптимизации.</target>
        </trans-unit>
        <trans-unit id="ffb7aede167c9f36a232ef307d2df81471bb1b4f" translate="yes" xml:space="preserve">
          <source>The initial learning rate for the &amp;lsquo;constant&amp;rsquo;, &amp;lsquo;invscaling&amp;rsquo; or &amp;lsquo;adaptive&amp;rsquo; schedules. The default value is 0.0 as eta0 is not used by the default schedule &amp;lsquo;optimal&amp;rsquo;.</source>
          <target state="translated">Начальная скорость обучения для &amp;laquo;постоянного&amp;raquo;, &amp;laquo;инмасштабируемого&amp;raquo; или &amp;laquo;адаптивного&amp;raquo; расписания. Значение по умолчанию - 0,0, поскольку eta0 не используется в расписании по умолчанию &amp;laquo;оптимальный&amp;raquo;.</target>
        </trans-unit>
        <trans-unit id="3434635df97d9946f02b9082e79d95e6999ccc03" translate="yes" xml:space="preserve">
          <source>The initial learning rate for the &amp;lsquo;constant&amp;rsquo;, &amp;lsquo;invscaling&amp;rsquo; or &amp;lsquo;adaptive&amp;rsquo; schedules. The default value is 0.01.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f396af141edab02acfc3d8c05ea1f6e174dd0d5b" translate="yes" xml:space="preserve">
          <source>The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;.</source>
          <target state="translated">Используемая начальная скорость обучения. Он контролирует размер шага при обновлении весов. Используется только когда solver = 'sgd' или 'adam'.</target>
        </trans-unit>
        <trans-unit id="8aa315cba5e51992abb63a7c33e92703aa91a364" translate="yes" xml:space="preserve">
          <source>The initial model \(F_{0}\) is problem specific, for least-squares regression one usually chooses the mean of the target values.</source>
          <target state="translated">Начальная модель \(F_{0}\)является проблемной,для регрессии наименьших квадратов обычно выбирают среднее из целевых значений.</target>
        </trans-unit>
        <trans-unit id="fcbcdc2001232ebf2d1267dbd5c1db8c05bef33b" translate="yes" xml:space="preserve">
          <source>The initial model can also be specified via the &lt;code&gt;init&lt;/code&gt; argument. The passed object has to implement &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">Исходную модель также можно указать с помощью аргумента &lt;code&gt;init&lt;/code&gt; . Переданный объект должен реализовывать &lt;code&gt;fit&lt;/code&gt; и &lt;code&gt;predict&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="2b0323f433eec5618cbc80c984794d0b5575bec5" translate="yes" xml:space="preserve">
          <source>The initial transformation will be a random array of shape &lt;code&gt;(n_components, n_features)&lt;/code&gt;. Each value is sampled from the standard normal distribution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17bd189eec62d21587058ccd88b444461d73119b" translate="yes" xml:space="preserve">
          <source>The initial values of the coefficients.</source>
          <target state="translated">Начальные значения коэффициентов.</target>
        </trans-unit>
        <trans-unit id="32a92ee0ef2ffe7a34cba56e64f123bd9b2ca181" translate="yes" xml:space="preserve">
          <source>The input data consists of 28x28 pixel handwritten digits, leading to 784 features in the dataset. Therefore the first layer weight matrix have the shape (784, hidden_layer_sizes[0]). We can therefore visualize a single column of the weight matrix as a 28x28 pixel image.</source>
          <target state="translated">Входные данные состоят из 28x28 пиксельных рукописных цифр,что приводит к наличию в наборе данных 784 признаков.Поэтому весовая матрица первого слоя имеет форму (784,hidden_layer_sizes[0]).Поэтому мы можем визуализировать один столбец весовой матрицы в виде изображения размером 28x28 пикселей.</target>
        </trans-unit>
        <trans-unit id="c8ff70c87eb4edb0d6df0b02362dd0f826eb86c0" translate="yes" xml:space="preserve">
          <source>The input data matrix</source>
          <target state="translated">Матрица входных данных</target>
        </trans-unit>
        <trans-unit id="28402823f8037a445e34f883189d18c9c8586801" translate="yes" xml:space="preserve">
          <source>The input data to complete.</source>
          <target state="translated">Входные данные для завершения.</target>
        </trans-unit>
        <trans-unit id="10ea3f8f2392cebb3ea568adacc2ca87c3d24f21" translate="yes" xml:space="preserve">
          <source>The input data to project into a smaller dimensional space.</source>
          <target state="translated">Входные данные для проецирования в меньшее размерное пространство.</target>
        </trans-unit>
        <trans-unit id="8b941f334dfce6c774093743936f754bf8137c6b" translate="yes" xml:space="preserve">
          <source>The input data.</source>
          <target state="translated">Входные данные.</target>
        </trans-unit>
        <trans-unit id="a383a11075a72cc4c498a6da492b12429e0d8bd0" translate="yes" xml:space="preserve">
          <source>The input matrix \(A\) is first normalized to make the checkerboard pattern more obvious. There are three possible methods:</source>
          <target state="translated">Входная матрица \(A\)сначала нормализуется,чтобы сделать шахматный образец более очевидным.Существует три возможных метода:</target>
        </trans-unit>
        <trans-unit id="5296b71b9c59f1ea88d1300f56d80b11acdd4263" translate="yes" xml:space="preserve">
          <source>The input matrix \(A\) is preprocessed as follows:</source>
          <target state="translated">Входная матрица \(A\)препроцессирована следующим образом:</target>
        </trans-unit>
        <trans-unit id="1dfee05c320291671e1507b730cbc56e48fcc05b" translate="yes" xml:space="preserve">
          <source>The input samples with only the selected features.</source>
          <target state="translated">Входные примеры только с выбранными функциями.</target>
        </trans-unit>
        <trans-unit id="aa9c51d1052bb07178d99dc6b9998838f3fd04f4" translate="yes" xml:space="preserve">
          <source>The input samples.</source>
          <target state="translated">Входные образцы.</target>
        </trans-unit>
        <trans-unit id="691f5d8dd518149d763a76be65224ad6a47a3de2" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, it will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt; and if a sparse matrix is provided to a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">Входные образцы. Внутренне он будет преобразован в &lt;code&gt;dtype=np.float32&lt;/code&gt; , а если разреженная матрица будет предоставлена ​​в разреженную &lt;code&gt;csr_matrix&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="80a0f34908e0a755318c3eb528444068386165b2" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">Входные образцы. Внутренне его dtype будет преобразован в &lt;code&gt;dtype=np.float32&lt;/code&gt; . Если предоставляется разреженная матрица, она будет преобразована в разреженную &lt;code&gt;csr_matrix&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ad241eeecc4b3d71885329d69a3ecbb931dd0903" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted to a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">Входные образцы. Внутренне его dtype будет преобразован в &lt;code&gt;dtype=np.float32&lt;/code&gt; . Если предоставляется разреженная матрица, она будет преобразована в разреженную &lt;code&gt;csr_matrix&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="574b784828dbe4184d33a76077708bbde6ac5b78" translate="yes" xml:space="preserve">
          <source>The input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b99e327cb34f5c84fcbd4e91cfb496e38c8c302f" translate="yes" xml:space="preserve">
          <source>The input samples. Use &lt;code&gt;dtype=np.float32&lt;/code&gt; for maximum efficiency. Sparse matrices are also supported, use sparse &lt;code&gt;csc_matrix&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">Входные образцы. Используйте &lt;code&gt;dtype=np.float32&lt;/code&gt; для максимальной эффективности. Также поддерживаются разреженные матрицы, используйте разреженные &lt;code&gt;csc_matrix&lt;/code&gt; для максимальной эффективности.</target>
        </trans-unit>
        <trans-unit id="6e6a9dea87222450502841b10c214ed0343cd360" translate="yes" xml:space="preserve">
          <source>The input set can either be well conditioned (by default) or have a low rank-fat tail singular profile. See &lt;a href=&quot;sklearn.datasets.make_low_rank_matrix#sklearn.datasets.make_low_rank_matrix&quot;&gt;&lt;code&gt;make_low_rank_matrix&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">Входной набор может быть либо хорошо подготовленным (по умолчанию), либо иметь особый профиль с низким уровнем жирности хвоста. См. &lt;a href=&quot;sklearn.datasets.make_low_rank_matrix#sklearn.datasets.make_low_rank_matrix&quot;&gt; &lt;code&gt;make_low_rank_matrix&lt;/code&gt; &lt;/a&gt; для более подробной информации.</target>
        </trans-unit>
        <trans-unit id="289f5749e232fcbed1540450cf860647da777cb8" translate="yes" xml:space="preserve">
          <source>The input set is well conditioned, centered and gaussian with unit variance.</source>
          <target state="translated">Входной набор хорошо кондиционирован,центрирован и гауссов с единичной дисперсией.</target>
        </trans-unit>
        <trans-unit id="9196dd161fc035e27746b1e485ce67d2bc4a2f97" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are converted to ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature.</source>
          <target state="translated">Вход в этот трансформатор должен быть массивом целых чисел или строк,обозначающих значения,принимаемые категориальными (дискретными)признаками.Характеристики преобразуются в порядковые целые числа.В результате получается один столбец целых чисел (от 0 до n_категорий-1)для каждого признака.</target>
        </trans-unit>
        <trans-unit id="a912a205d28c73bb81095b58a51d5b9233f6732c" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka &amp;lsquo;one-of-K&amp;rsquo; or &amp;lsquo;dummy&amp;rsquo;) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array (depending on the &lt;code&gt;sparse&lt;/code&gt; parameter)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6fca358f78b22e3193c17b331b39a22e0cf2c917" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka &amp;lsquo;one-of-K&amp;rsquo; or &amp;lsquo;dummy&amp;rsquo;) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array.</source>
          <target state="translated">Вход в этот преобразователь должен быть подобным массиву целых чисел или строк, обозначающих значения, принимаемые категориальными (дискретными) функциями. Функции кодируются с использованием горячей схемы кодирования (также известной как &amp;laquo;один из K&amp;raquo; или &amp;laquo;фиктивная&amp;raquo;). Это создает двоичный столбец для каждой категории и возвращает разреженную матрицу или плотный массив.</target>
        </trans-unit>
        <trans-unit id="c9721cff229ea27eaddea830cb215d9ecb24c54d" translate="yes" xml:space="preserve">
          <source>The instance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d6846270bbc73ec820141552eb32b90be4589180" translate="yes" xml:space="preserve">
          <source>The integer id or the string name metadata of the MLComp dataset to load</source>
          <target state="translated">Целый идентификатор или метаданные имени строки набора данных MLComp для загрузки</target>
        </trans-unit>
        <trans-unit id="0ff5072970d94a45dfd0b3dc59cc200193becdfd" translate="yes" xml:space="preserve">
          <source>The integer labels (0 or 1) for class membership of each sample.</source>
          <target state="translated">Целые метки (0 или 1)для принадлежности к классу в каждой выборке.</target>
        </trans-unit>
        <trans-unit id="a6afcce2561a80c34c914f95ed66620399eb0d7c" translate="yes" xml:space="preserve">
          <source>The integer labels for class membership of each sample.</source>
          <target state="translated">Целые метки для членства в классе каждой выборки.</target>
        </trans-unit>
        <trans-unit id="868af1be557e4830c6f25b4ff3211b7a9aad7eb0" translate="yes" xml:space="preserve">
          <source>The integer labels for cluster membership of each sample.</source>
          <target state="translated">Целые метки для кластерного членства каждой выборки.</target>
        </trans-unit>
        <trans-unit id="05d4e37f01ed47119c132248581df1196214fb7d" translate="yes" xml:space="preserve">
          <source>The integer labels for quantile membership of each sample.</source>
          <target state="translated">Целые метки для квантильной принадлежности каждой выборки.</target>
        </trans-unit>
        <trans-unit id="7da460eaa0239b5647e6b97a087bcede08e5a0c5" translate="yes" xml:space="preserve">
          <source>The intercept of the model. Only returned if &lt;code&gt;return_intercept&lt;/code&gt; is True and if X is a scipy sparse array.</source>
          <target state="translated">Перехват модели. Возвращается только в том случае, если &lt;code&gt;return_intercept&lt;/code&gt; имеет значение True и если X является scipy разреженным массивом.</target>
        </trans-unit>
        <trans-unit id="f0408bacce1626a183a8eadd09dc2d6f8b9e549c" translate="yes" xml:space="preserve">
          <source>The intercept term.</source>
          <target state="translated">Срок перехвата.</target>
        </trans-unit>
        <trans-unit id="39d99a92784fd6547680c69e2a029d63ab730943" translate="yes" xml:space="preserve">
          <source>The inverse document frequency (IDF) vector; only defined if &lt;code&gt;use_idf&lt;/code&gt; is True.</source>
          <target state="translated">Вектор обратной частоты документа (IDF); определяется только в том случае, если &lt;code&gt;use_idf&lt;/code&gt; имеет значение True.</target>
        </trans-unit>
        <trans-unit id="70e42545b4f301fb133ffa67ef01dbd1c81a14a6" translate="yes" xml:space="preserve">
          <source>The inverse of the Box-Cox transformation is given by:</source>
          <target state="translated">Приводится обратная величина трансформации Бокс-Кокс:</target>
        </trans-unit>
        <trans-unit id="1296b0fc0246edf109c97645c02261b1d3d028bb" translate="yes" xml:space="preserve">
          <source>The inverse of the Yeo-Johnson transformation is given by:</source>
          <target state="translated">Дано обратное преобразование Ё-Джонсона:</target>
        </trans-unit>
        <trans-unit id="8621ba40f721d68ec6f12f67b5e3cc856e6fc6d9" translate="yes" xml:space="preserve">
          <source>The iris dataset is a classic and very easy multi-class classification dataset.</source>
          <target state="translated">Набор данных по радужной оболочке глаза-это классический и очень простой набор классификационных данных для нескольких классов.</target>
        </trans-unit>
        <trans-unit id="82f3a3d98edcec3969a3e26fc0789c7c6c1f74ef" translate="yes" xml:space="preserve">
          <source>The iris dataset is a classification task consisting in identifying 3 different types of irises (Setosa, Versicolour, and Virginica) from their petal and sepal length and width:</source>
          <target state="translated">Набор данных по радужной оболочке глаза-это классификационная задача,состоящая в идентификации 3 различных типов радужной оболочки глаза (Setosa,Versicolour и Virginica)по длине и ширине их лепестков и чашелистиков:</target>
        </trans-unit>
        <trans-unit id="968fa226a66bed28f98c84df2ac306ac992e59c2" translate="yes" xml:space="preserve">
          <source>The isotonic regression optimization problem is defined by:</source>
          <target state="translated">Определена задача оптимизации изотонической регрессии:</target>
        </trans-unit>
        <trans-unit id="710a9703f51d61fb5f41b8c77a926ca070febd6c" translate="yes" xml:space="preserve">
          <source>The iteration will stop when &lt;code&gt;max{|proj g_i | i = 1, ..., n}&lt;/code&gt; &amp;lt;= &lt;code&gt;tol&lt;/code&gt; where pg_i is the i-th component of the projected gradient.</source>
          <target state="translated">Итерация остановится, когда &lt;code&gt;max{|proj g_i | i = 1, ..., n}&lt;/code&gt; &amp;lt;= &lt;code&gt;tol&lt;/code&gt; , где pg_i - i-я компонента спроецированного градиента.</target>
        </trans-unit>
        <trans-unit id="d1f978f02193a9d7a16e2ac4f28051f1be20fa33" translate="yes" xml:space="preserve">
          <source>The iterator consumption and dispatching is protected by the same lock so calling this function should be thread safe.</source>
          <target state="translated">Потребление и распределение итератора защищено одним и тем же замком,поэтому вызов этой функции должен быть нитевидным.</target>
        </trans-unit>
        <trans-unit id="e6bcf49b626a9dedb0663face3bd765b3dedb378" translate="yes" xml:space="preserve">
          <source>The ith element in the list represents the bias vector corresponding to layer i + 1.</source>
          <target state="translated">Элемент ith в списке представляет собой вектор смещения,соответствующий слою i+1.</target>
        </trans-unit>
        <trans-unit id="4b396b269a89e1a7151872f147950b58bd31d2c2" translate="yes" xml:space="preserve">
          <source>The ith element in the list represents the weight matrix corresponding to layer i.</source>
          <target state="translated">Элемент ith в списке представляет собой матрицу весов,соответствующую слою i.</target>
        </trans-unit>
        <trans-unit id="8ce9197f7d2eba2389cfe44c6d9806383e84232a" translate="yes" xml:space="preserve">
          <source>The ith element represents the number of neurons in the ith hidden layer.</source>
          <target state="translated">Элемент ith представляет количество нейронов в скрытом слое.</target>
        </trans-unit>
        <trans-unit id="110dde91ce5735296cfcdbdf4849eb2634f008ea" translate="yes" xml:space="preserve">
          <source>The justification for the formula in the loss=&amp;rdquo;modified_huber&amp;rdquo; case is in the appendix B in: &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&lt;/a&gt;</source>
          <target state="translated">Обоснование формулы в случае loss = &amp;rdquo;modified_huber&amp;rdquo; приведено в приложении B по &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&quot;&gt;адресу&lt;/a&gt; : http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf.</target>
        </trans-unit>
        <trans-unit id="08f434f0a998f2afa3af3de6e921bb2935ec3174" translate="yes" xml:space="preserve">
          <source>The k-means algorithm divides a set of \(N\) samples \(X\) into \(K\) disjoint clusters \(C\), each described by the mean \(\mu_j\) of the samples in the cluster. The means are commonly called the cluster &amp;ldquo;centroids&amp;rdquo;; note that they are not, in general, points from \(X\), although they live in the same space.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ad3a7b0acdb773b3a5a78e7f86e7401f887f5a4" translate="yes" xml:space="preserve">
          <source>The k-means algorithm divides a set of \(N\) samples \(X\) into \(K\) disjoint clusters \(C\), each described by the mean \(\mu_j\) of the samples in the cluster. The means are commonly called the cluster &amp;ldquo;centroids&amp;rdquo;; note that they are not, in general, points from \(X\), although they live in the same space. The K-means algorithm aims to choose centroids that minimise the &lt;em&gt;inertia&lt;/em&gt;, or within-cluster sum of squared criterion:</source>
          <target state="translated">Алгоритм k-средних делит набор \ (N \) выборок \ (X \) на \ (K \) непересекающиеся кластеры \ (C \), каждый из которых описывается средним \ (\ mu_j \) выборок в кластер. Средние значения обычно называют &quot;центроидами&quot; кластера; обратите внимание, что они, как правило, не являются точками из \ (X \), хотя они живут в одном пространстве. Алгоритм K-средних нацелен на выбор центроидов, которые минимизируют &lt;em&gt;инерцию&lt;/em&gt; , или внутрикластерную сумму квадрата критерия:</target>
        </trans-unit>
        <trans-unit id="3042a74be3a11ce6ced2d21d393c8c55a0b044ff" translate="yes" xml:space="preserve">
          <source>The k-means problem is solved using either Lloyd&amp;rsquo;s or Elkan&amp;rsquo;s algorithm.</source>
          <target state="translated">Проблема k-средних решается с использованием алгоритма Ллойда или Элкана.</target>
        </trans-unit>
        <trans-unit id="698fe2144ac8be0b801483c0a1d13dc4d07c73ce" translate="yes" xml:space="preserve">
          <source>The kappa score (see docstring) is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).</source>
          <target state="translated">Оценка kappa (см.доктрину)-это число между -1 и 1.Оценка выше .8 обычно считается хорошим соглашением;ноль или ниже означает отсутствие соглашения (практически случайные пометки).</target>
        </trans-unit>
        <trans-unit id="951d69c60ea24190bd0c5a4dcae9aeb94c1c913d" translate="yes" xml:space="preserve">
          <source>The kappa statistic, which is a number between -1 and 1. The maximum value means complete agreement; zero or lower means chance agreement.</source>
          <target state="translated">Каппа-статистика,которая представляет собой число от -1 до 1.Максимальное значение означает полное соглашение;нулевое или меньшее-соглашение шансов.</target>
        </trans-unit>
        <trans-unit id="e7d87035112e23c601a476b538c50df786049966" translate="yes" xml:space="preserve">
          <source>The kernel density estimator can be used with any of the valid distance metrics (see &lt;a href=&quot;generated/sklearn.neighbors.distancemetric#sklearn.neighbors.DistanceMetric&quot;&gt;&lt;code&gt;sklearn.neighbors.DistanceMetric&lt;/code&gt;&lt;/a&gt; for a list of available metrics), though the results are properly normalized only for the Euclidean metric. One particularly useful metric is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;Haversine distance&lt;/a&gt; which measures the angular distance between points on a sphere. Here is an example of using a kernel density estimate for a visualization of geospatial data, in this case the distribution of observations of two different species on the South American continent:</source>
          <target state="translated">Оценщик плотности ядра можно использовать с любой из допустимых метрик расстояния (см. &lt;a href=&quot;generated/sklearn.neighbors.distancemetric#sklearn.neighbors.DistanceMetric&quot;&gt; &lt;code&gt;sklearn.neighbors.DistanceMetric&lt;/code&gt; &lt;/a&gt; для списка доступных метрик), хотя результаты правильно нормализованы только для евклидовой метрики. Особенно полезной метрикой является &lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;расстояние Хаверсина,&lt;/a&gt; которое измеряет угловое расстояние между точками на сфере. Вот пример использования оценки плотности ядра для визуализации геопространственных данных, в данном случае распределения наблюдений за двумя разными видами на южноамериканском континенте:</target>
        </trans-unit>
        <trans-unit id="f5acb5ee70e93822d6214e7faa0671c1edbb5e04" translate="yes" xml:space="preserve">
          <source>The kernel is composed of several terms that are responsible for explaining different properties of the signal:</source>
          <target state="translated">Ядро состоит из нескольких терминов,которые отвечают за объяснение различных свойств сигнала:</target>
        </trans-unit>
        <trans-unit id="075d7f089a3306b950fac8e515a19a5954a7251d" translate="yes" xml:space="preserve">
          <source>The kernel is given by:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2a264496619ccec6a15428322dff95c6847afef" translate="yes" xml:space="preserve">
          <source>The kernel specifying the covariance function of the GP. If None is passed, the kernel &amp;ldquo;1.0 * RBF(1.0)&amp;rdquo; is used as default. Note that the kernel&amp;rsquo;s hyperparameters are optimized during fitting.</source>
          <target state="translated">Ядро, определяющее ковариационную функцию GP. Если передано None, по умолчанию используется ядро ​​&amp;laquo;1.0 * RBF (1.0)&amp;raquo;. Обратите внимание, что гиперпараметры ядра оптимизируются во время подгонки.</target>
        </trans-unit>
        <trans-unit id="3f4b36434e95db38ce416f2948e370303e8d7ce4" translate="yes" xml:space="preserve">
          <source>The kernel to use. Valid kernels are [&amp;lsquo;gaussian&amp;rsquo;|&amp;rsquo;tophat&amp;rsquo;|&amp;rsquo;epanechnikov&amp;rsquo;|&amp;rsquo;exponential&amp;rsquo;|&amp;rsquo;linear&amp;rsquo;|&amp;rsquo;cosine&amp;rsquo;] Default is &amp;lsquo;gaussian&amp;rsquo;.</source>
          <target state="translated">Используемое ядро. Допустимые ядра: ['gaussian' | 'tophat' | 'epanechnikov' | 'exponential' | 'linear' | 'cosine'] По умолчанию - 'gaussian'.</target>
        </trans-unit>
        <trans-unit id="7d7a3328bfc6be5d4c52eb80a02db6810f6af97a" translate="yes" xml:space="preserve">
          <source>The kernel used for prediction. In case of binary classification, the structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters. In case of multi-class classification, a CompoundKernel is returned which consists of the different kernels used in the one-versus-rest classifiers.</source>
          <target state="translated">Ядро,используемое для прогнозирования.В случае бинарной классификации структура ядра такая же,как и у параметра,но с оптимизированными гиперпараметрами.В случае многоклассовой классификации возвращается CompoundKernel,которое состоит из различных ядер,используемых в классификаторах,использующих одну точку над другой.</target>
        </trans-unit>
        <trans-unit id="5a1810e4f4370ea9d4b3eb4b69c5661264f9680f" translate="yes" xml:space="preserve">
          <source>The kernel used for prediction. The structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters</source>
          <target state="translated">Ядро,используемое для прогнозирования.Структура ядра такая же,как и у переданного в качестве параметра,но с оптимизированными гиперпараметрами</target>
        </trans-unit>
        <trans-unit id="317ceb705f764af792a5c1b02b0c75da4f7767d2" translate="yes" xml:space="preserve">
          <source>The key &lt;code&gt;'params'&lt;/code&gt; is used to store a list of parameter settings dicts for all the parameter candidates.</source>
          <target state="translated">Ключ &lt;code&gt;'params'&lt;/code&gt; используется для хранения списка настроек параметров для всех кандидатов в параметры.</target>
        </trans-unit>
        <trans-unit id="467dfd2cbee563f1dc16d74e0763e99176db4d68" translate="yes" xml:space="preserve">
          <source>The l1-penalized estimator can recover part of this off-diagonal structure. It learns a sparse precision. It is not able to recover the exact sparsity pattern: it detects too many non-zero coefficients. However, the highest non-zero coefficients of the l1 estimated correspond to the non-zero coefficients in the ground truth. Finally, the coefficients of the l1 precision estimate are biased toward zero: because of the penalty, they are all smaller than the corresponding ground truth value, as can be seen on the figure.</source>
          <target state="translated">l1-пеналлизованный оценщик может восстановить часть этой внедиагональной структуры.Он учится с редкой точностью.Он не в состоянии восстановить точную картину редкостности:он обнаруживает слишком много ненулевых коэффициентов.Однако самые высокие ненулевые коэффициенты оцениваемого l1 соответствуют ненулевым коэффициентам в основной истине.Наконец,коэффициенты оценки точности l1 смещены в сторону нуля:из-за штрафа все они меньше соответствующего значения базовой истины,как видно на рисунке.</target>
        </trans-unit>
        <trans-unit id="3f115dd7f6ab226f3d1efb2261d982c44eb021f0" translate="yes" xml:space="preserve">
          <source>The label of the positive class</source>
          <target state="translated">Знак положительного класса</target>
        </trans-unit>
        <trans-unit id="1daeeb6b0700e2caf583964dcec09c381eeb8ea9" translate="yes" xml:space="preserve">
          <source>The label of the positive class. Only applied to binary &lt;code&gt;y_true&lt;/code&gt;. For multilabel-indicator &lt;code&gt;y_true&lt;/code&gt;, &lt;code&gt;pos_label&lt;/code&gt; is fixed to 1.</source>
          <target state="translated">Ярлык положительного класса. Применяется только к двоичному &lt;code&gt;y_true&lt;/code&gt; . Для MultiLabel-индикатора &lt;code&gt;y_true&lt;/code&gt; , &lt;code&gt;pos_label&lt;/code&gt; крепится к 1.</target>
        </trans-unit>
        <trans-unit id="e8fa4295eafd0b055339770364309144cc85ebe4" translate="yes" xml:space="preserve">
          <source>The label of the positive class. When &lt;code&gt;pos_label=None&lt;/code&gt;, if y_true is in {-1, 1} or {0, 1}, &lt;code&gt;pos_label&lt;/code&gt; is set to 1, otherwise an error will be raised.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="844ad03ed5a5991044f5c2a9015a7d46ffc83176" translate="yes" xml:space="preserve">
          <source>The label sets.</source>
          <target state="translated">Наборы этикеток.</target>
        </trans-unit>
        <trans-unit id="4e35273308f7d89cd0147256c1b5ea034960bf40" translate="yes" xml:space="preserve">
          <source>The labels assigned to samples. Points which are not included in any cluster are labeled as -1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34e6595f06fcaf1ac9d996e42fb91cf1dc35ee40" translate="yes" xml:space="preserve">
          <source>The labels of the clusters.</source>
          <target state="translated">Ярлыки кластеров.</target>
        </trans-unit>
        <trans-unit id="f920ebdbb736d4fac6e624f1f0a24fae7f686c27" translate="yes" xml:space="preserve">
          <source>The laplacian kernel is defined as:</source>
          <target state="translated">Лаплацианское ядро определяется как:</target>
        </trans-unit>
        <trans-unit id="06271ccbc4593ebfa05907a5273f644dc124ef42" translate="yes" xml:space="preserve">
          <source>The lasso estimate thus solves the minimization of the least-squares penalty with \(\alpha ||w||_1\) added, where \(\alpha\) is a constant and \(||w||_1\) is the \(\ell_1\)-norm of the coefficient vector.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="321fd6d2804d3c8ea8df389dda895240e812e24c" translate="yes" xml:space="preserve">
          <source>The lasso estimate thus solves the minimization of the least-squares penalty with \(\alpha ||w||_1\) added, where \(\alpha\) is a constant and \(||w||_1\) is the \(\ell_1\)-norm of the parameter vector.</source>
          <target state="translated">Таким образом,оценка лассо решает минимизацию штрафа по методу наименьших квадратов с добавлением \(\alpha ||w||1\),где \(\alpha\)-константа и \(||w||1\)-форма вектора параметра \(\ell1\)-норма.</target>
        </trans-unit>
        <trans-unit id="374010cdf4f6f26c62c956d36c36442b1f12b646" translate="yes" xml:space="preserve">
          <source>The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser.</source>
          <target state="translated">Последняя характеристика подразумевает,что Perceptron немного быстрее тренируется,чем SGD с потерей шарнира,и что получившиеся модели более щадящие.</target>
        </trans-unit>
        <trans-unit id="6312a138a2adc5ee223c24bc72b6c9e5099fd7ea" translate="yes" xml:space="preserve">
          <source>The last dataset is an example of a &amp;lsquo;null&amp;rsquo; situation for clustering: the data is homogeneous, and there is no good clustering. For this example, the null dataset uses the same parameters as the dataset in the row above it, which represents a mismatch in the parameter values and the data structure.</source>
          <target state="translated">Последний набор данных является примером &amp;laquo;нулевой&amp;raquo; ситуации для кластеризации: данные однородны, и нет хорошей кластеризации. В этом примере нулевой набор данных использует те же параметры, что и набор данных в строке над ним, что представляет несоответствие в значениях параметров и структуре данных.</target>
        </trans-unit>
        <trans-unit id="2cc9bc385685f028c8287b17d0d958b5050b56a0" translate="yes" xml:space="preserve">
          <source>The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold. This ensures that the graph starts on the y axis.</source>
          <target state="translated">Последние значения точности и значения вызова равны 1.и 0.соответственно и не имеют соответствующего порога.Это гарантирует,что график начинается по оси y.</target>
        </trans-unit>
        <trans-unit id="9ddff80583e9f2c8a48f3b64e4a9a06a8a8cefcd" translate="yes" xml:space="preserve">
          <source>The last two panels show how we can sample from the last two models. The resulting samples distributions do not look exactly like the original data distribution. The difference primarily stems from the approximation error we made by using a model that assumes that the data was generated by a finite number of Gaussian components instead of a continuous noisy sine curve.</source>
          <target state="translated">Последние две панели показывают,как мы можем взять образец из последних двух моделей.Полученные распределения сэмплов не выглядят точно так же,как оригинальное распределение данных.Разница главным образом обусловлена ошибкой аппроксимации,которую мы сделали при использовании модели,предполагающей,что данные были сгенерированы конечным числом гауссовских компонент вместо непрерывной шумной синусоидальной кривой.</target>
        </trans-unit>
        <trans-unit id="a4514c8e853c44e1e4a78bf33662f9b32277ba44" translate="yes" xml:space="preserve">
          <source>The latent variables of X.</source>
          <target state="translated">Скрытые переменные X.</target>
        </trans-unit>
        <trans-unit id="03a214378b3ef2e13a3cc6617d05b3fe221146c8" translate="yes" xml:space="preserve">
          <source>The learning rate \(\eta\) can be either constant or gradually decaying. For classification, the default learning rate schedule (&lt;code&gt;learning_rate='optimal'&lt;/code&gt;) is given by</source>
          <target state="translated">Скорость обучения \ (\ eta \) может быть постоянной или постепенно снижаться. Для классификации график скорости обучения по умолчанию ( &lt;code&gt;learning_rate='optimal'&lt;/code&gt; ) задается следующим образом:</target>
        </trans-unit>
        <trans-unit id="36c5b5659a911c2db600cd9fbf9c0485b5053e95" translate="yes" xml:space="preserve">
          <source>The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a &amp;lsquo;ball&amp;rsquo; with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help.</source>
          <target state="translated">Скорость обучения t-SNE обычно находится в диапазоне [10.0, 1000.0]. Если скорость обучения слишком высока, данные могут выглядеть как &amp;laquo;шар&amp;raquo; с любой точкой, приблизительно равноудаленной от ближайших соседей. Если скорость обучения слишком низкая, большинство точек может выглядеть сжатым в плотном облаке с небольшим количеством выбросов. Если функция стоимости застревает в плохом локальном минимуме, может помочь увеличение скорости обучения.</target>
        </trans-unit>
        <trans-unit id="55a4138aa9f2827acbf6f24a7d7d78c5c9231271" translate="yes" xml:space="preserve">
          <source>The learning rate for weight updates. It is &lt;em&gt;highly&lt;/em&gt; recommended to tune this hyper-parameter. Reasonable values are in the 10**[0., -3.] range.</source>
          <target state="translated">Скорость обучения для обновления веса. Он &lt;em&gt;настоятельно&lt;/em&gt; рекомендуется настроить этот гипер-параметр. Разумные значения находятся в диапазоне 10 ** [0., -3.].</target>
        </trans-unit>
        <trans-unit id="aa4707838d034cc31029160760df726f17933ef5" translate="yes" xml:space="preserve">
          <source>The learning rate schedule:</source>
          <target state="translated">График успеваемости:</target>
        </trans-unit>
        <trans-unit id="3497cec934b609e53594ddceecdfac3a47086873" translate="yes" xml:space="preserve">
          <source>The learning rate, also known as &lt;em&gt;shrinkage&lt;/em&gt;. This is used as a multiplicative factor for the leaves values. Use &lt;code&gt;1&lt;/code&gt; for no shrinkage.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f65b2c5f287b4c02eec9ec5be4e57eb68fdc4c9" translate="yes" xml:space="preserve">
          <source>The least squares loss (along with the implicit use of the identity link function) of the Ridge regression model seems to cause this model to be badly calibrated. In particular, it tends to underestimate the risk and can even predict invalid negative frequencies.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed4ad540aa3d79251b7dbdba80cd56b31d18c164" translate="yes" xml:space="preserve">
          <source>The least squares solution is computed using the singular value decomposition of X. If X is a matrix of shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt; this method has a cost of \(O(n_{\text{samples}} n_{\text{features}}^2)\), assuming that \(n_{\text{samples}} \geq n_{\text{features}}\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="866660dd294846977c41d9eb05a5f823762d12dd" translate="yes" xml:space="preserve">
          <source>The left and right examples highlight the &lt;code&gt;n_labels&lt;/code&gt; parameter: more of the samples in the right plot have 2 or 3 labels.</source>
          <target state="translated">В левом и правом примерах выделяется параметр &lt;code&gt;n_labels&lt;/code&gt; : больше образцов на правом графике имеют 2 или 3 метки.</target>
        </trans-unit>
        <trans-unit id="3656a1b3513126216409c003f2c19b4a1bde366e" translate="yes" xml:space="preserve">
          <source>The leftmost layer, known as the input layer, consists of a set of neurons \(\{x_i | x_1, x_2, ..., x_m\}\) representing the input features. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation \(w_1x_1 + w_2x_2 + ... + w_mx_m\), followed by a non-linear activation function \(g(\cdot):R \rightarrow R\) - like the hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values.</source>
          <target state="translated">Самый левый слой,известный как входной,состоит из набора нейронов \(\{x_i | x_1,x_2,...,x_m\}\),представляющих входные характеристики.Каждый нейрон в скрытом слое трансформирует значения из предыдущего слоя со взвешенным линейным суммированием \(w_1x_1+w_2x_2+...+w_mx_m\),за которым следует нелинейная функция активации \(g(\cdot):R \rightarrow R\).-как функция гиперболического загара.Выходной слой получает значения от последнего скрытого слоя и преобразует их в выходные значения.</target>
        </trans-unit>
        <trans-unit id="7b2e6a226728025cdbf60737ae02746b4f5067e4" translate="yes" xml:space="preserve">
          <source>The length scale of the kernel.</source>
          <target state="translated">Шкала длины ядра.</target>
        </trans-unit>
        <trans-unit id="658641ffb44e40c4155905b8a662123f9fbe36a4" translate="yes" xml:space="preserve">
          <source>The length scale of the kernel. If a float, an isotropic kernel is used. If an array, an anisotropic kernel is used where each dimension of l defines the length-scale of the respective feature dimension.</source>
          <target state="translated">Шкала длины ядра.При использовании поплавка используется изотропное ядро.Если используется массив,то используется анизотропное ядро,где каждое измерение l определяет шкалу длины соответствующего измерения элемента.</target>
        </trans-unit>
        <trans-unit id="c8dac18109c7577d20154429eb0e5e79a703a997" translate="yes" xml:space="preserve">
          <source>The likelihood of the data set with &lt;code&gt;self.covariance_&lt;/code&gt; as an estimator of its covariance matrix.</source>
          <target state="translated">Вероятность набора данных с &lt;code&gt;self.covariance_&lt;/code&gt; в качестве оценки его ковариационной матрицы.</target>
        </trans-unit>
        <trans-unit id="9d0f580fc795d85a96fff8300e9ac1a9bc3bbb0b" translate="yes" xml:space="preserve">
          <source>The linear model trained on polynomial features is able to exactly recover the input polynomial coefficients.</source>
          <target state="translated">Линейная модель,тренированная на полиномиальных особенностях,способна точно восстановить входные полиномиальные коэффициенты.</target>
        </trans-unit>
        <trans-unit id="9cf1fb22576b6c52a81ed3fb3cd57a1b7c534cc0" translate="yes" xml:space="preserve">
          <source>The linear models &lt;code&gt;LinearSVC()&lt;/code&gt; and &lt;code&gt;SVC(kernel='linear')&lt;/code&gt; yield slightly different decision boundaries. This can be a consequence of the following differences:</source>
          <target state="translated">Линейные модели &lt;code&gt;LinearSVC()&lt;/code&gt; и &lt;code&gt;SVC(kernel='linear')&lt;/code&gt; дают несколько разные границы принятия решения. Это может быть следствием следующих отличий:</target>
        </trans-unit>
        <trans-unit id="e79cf5350a0d3a4c8abb9ba5736a33d5a0c53fd5" translate="yes" xml:space="preserve">
          <source>The linear models assume no interactions between the input variables which likely causes under-fitting. Inserting a polynomial feature extractor (&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt;&lt;code&gt;PolynomialFeatures&lt;/code&gt;&lt;/a&gt;) indeed increases their discrimative power by 2 points of Gini index. In particular it improves the ability of the models to identify the top 5% riskiest profiles.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="086fe9ce38700dda3eefe00cc625a2fbfd905812" translate="yes" xml:space="preserve">
          <source>The linear operator to apply to the data to get the independent sources. This is equal to the unmixing matrix when &lt;code&gt;whiten&lt;/code&gt; is False, and equal to &lt;code&gt;np.dot(unmixing_matrix, self.whitening_)&lt;/code&gt; when &lt;code&gt;whiten&lt;/code&gt; is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3db33e367d642c4ed744d98677d2b41b0f8967e" translate="yes" xml:space="preserve">
          <source>The linear transformation learned during fitting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3af4059661dbcc46e497f8eb762a99fb7562b4b" translate="yes" xml:space="preserve">
          <source>The link function is determined by the &lt;code&gt;link&lt;/code&gt; parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7828a03a3850299c9821c4410a12edc3f4715fc7" translate="yes" xml:space="preserve">
          <source>The link function of the GLM, i.e. mapping from linear predictor &lt;code&gt;X @ coeff + intercept&lt;/code&gt; to prediction &lt;code&gt;y_pred&lt;/code&gt;. Option &amp;lsquo;auto&amp;rsquo; sets the link depending on the chosen family as follows:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df16104005b0ad36b12ecec77e07a777907b9657" translate="yes" xml:space="preserve">
          <source>The linkage distance threshold above which, clusters will not be merged. If not &lt;code&gt;None&lt;/code&gt;, &lt;code&gt;n_clusters&lt;/code&gt; must be &lt;code&gt;None&lt;/code&gt; and &lt;code&gt;compute_full_tree&lt;/code&gt; must be &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e68f70b09864b10f1aea37e78800b7b9e97f9614" translate="yes" xml:space="preserve">
          <source>The list of Elastic-Net mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. Only used if &lt;code&gt;penalty='elasticnet'&lt;/code&gt;. A value of 0 is equivalent to using &lt;code&gt;penalty='l2'&lt;/code&gt;, while 1 is equivalent to using &lt;code&gt;penalty='l1'&lt;/code&gt;. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt;1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b28aee2c83cc27005872232d54b931e2f0eba84" translate="yes" xml:space="preserve">
          <source>The list of calibrated classifiers, one for each cross-validation fold, which has been fitted on all but the validation fold and calibrated on the validation fold.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02b9c4dcf92e3aac8d7274c2bb8e453c5501153c" translate="yes" xml:space="preserve">
          <source>The list of calibrated classifiers, one for each crossvalidation fold, which has been fitted on all but the validation fold and calibrated on the validation fold.</source>
          <target state="translated">Список откалиброванных классификаторов,по одному для каждой перекрестной валидации,который был установлен на всех,кроме валидации,и откалиброван на валидации.</target>
        </trans-unit>
        <trans-unit id="89a2f4c0656f755e155226e23cfb6b592b80d152" translate="yes" xml:space="preserve">
          <source>The list of clusters in the form of &lt;code&gt;[start, end]&lt;/code&gt; in each row, with all indices inclusive. The clusters are ordered according to &lt;code&gt;(end,
-start)&lt;/code&gt; (ascending) so that larger clusters encompassing smaller clusters come after such nested smaller clusters. Since &lt;code&gt;labels&lt;/code&gt; does not reflect the hierarchy, usually &lt;code&gt;len(clusters) &amp;gt;
np.unique(labels)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4b68d75689eb0ac219c6fca1ed0b8741a6d57de9" translate="yes" xml:space="preserve">
          <source>The list of clusters in the form of &lt;code&gt;[start, end]&lt;/code&gt; in each row, with all indices inclusive. The clusters are ordered according to &lt;code&gt;(end, -start)&lt;/code&gt; (ascending) so that larger clusters encompassing smaller clusters come after those smaller ones. Since &lt;code&gt;labels_&lt;/code&gt; does not reflect the hierarchy, usually &lt;code&gt;len(cluster_hierarchy_) &amp;gt; np.unique(optics.labels_)&lt;/code&gt;. Please also note that these indices are of the &lt;code&gt;ordering_&lt;/code&gt;, i.e. &lt;code&gt;X[ordering_][start:end + 1]&lt;/code&gt; form a cluster. Only available when &lt;code&gt;cluster_method='xi'&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="662dd2cdb21b969ce9a44b42150a945acbfed523" translate="yes" xml:space="preserve">
          <source>The list of values of the objective function and the dual gap at each iteration. Returned only if return_costs is True.</source>
          <target state="translated">Перечень значений объективной функции и двойного пробела на каждой итерации.Возвращается только в том случае,если значение return_costs равно True.</target>
        </trans-unit>
        <trans-unit id="177835998b74cd6b15a39a13b3bf0448af9a1ccf" translate="yes" xml:space="preserve">
          <source>The local outlier factor (LOF) of a sample captures its supposed &amp;lsquo;degree of abnormality&amp;rsquo;. It is the average of the ratio of the local reachability density of a sample and those of its k-nearest neighbors.</source>
          <target state="translated">Фактор локального выброса (LOF) образца отражает предполагаемую &amp;laquo;степень отклонения от нормы&amp;raquo;. Это среднее значение отношения локальной плотности достижимости образца и его k-ближайших соседей.</target>
        </trans-unit>
        <trans-unit id="f5a19f67242aeecdaccb15b2c01bb7b8a32da9cd" translate="yes" xml:space="preserve">
          <source>The log likelihood at each iteration.</source>
          <target state="translated">Вероятность журнала на каждой итерации.</target>
        </trans-unit>
        <trans-unit id="d6423c4071c8619e84a84f9bcba64b070be431d3" translate="yes" xml:space="preserve">
          <source>The log-marginal-likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt;</source>
          <target state="translated">Маргинальное логарифмическое правдоподобие &lt;code&gt;self.kernel_.theta&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="28c02970f0172adf1161a050ed9ce282eb08888a" translate="yes" xml:space="preserve">
          <source>The log-posterior of LDA can also be written &lt;a href=&quot;#id7&quot; id=&quot;id1&quot;&gt;3&lt;/a&gt; as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b9f3852ad5acaf7d90c57ecdaa290bd2f37a1610" translate="yes" xml:space="preserve">
          <source>The log-transformed bounds on the kernel&amp;rsquo;s hyperparameters theta</source>
          <target state="translated">Преобразованные в журнал границы гиперпараметров ядра theta</target>
        </trans-unit>
        <trans-unit id="997c53b0e19e31f9cbe762633463d7411a04ec12" translate="yes" xml:space="preserve">
          <source>The logarithm used is the natural logarithm (base-e).</source>
          <target state="translated">Используемый логарифм является натуральным логарифмом (base-e).</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
