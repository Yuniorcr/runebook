<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="tensorflow">
    <body>
      <group id="tensorflow">
        <trans-unit id="7b76c8215965b6d6548dad5cc3be42ab9b948c2e" translate="yes" xml:space="preserve">
          <source>The optional &lt;code&gt;signatures&lt;/code&gt; argument controls which methods in &lt;code&gt;obj&lt;/code&gt; will be available to programs which consume &lt;code&gt;SavedModel&lt;/code&gt;s, for example, serving APIs. Python functions may be decorated with &lt;code&gt;@tf.function(input_signature=...)&lt;/code&gt; and passed as signatures directly, or lazily with a call to &lt;code&gt;get_concrete_function&lt;/code&gt; on the method decorated with &lt;code&gt;@tf.function&lt;/code&gt;.</source>
          <target state="translated">可选的 &lt;code&gt;signatures&lt;/code&gt; 参数控制 &lt;code&gt;obj&lt;/code&gt; 中的哪些方法可用于使用 &lt;code&gt;SavedModel&lt;/code&gt; 的程序，例如服务API。Python函数可以使用 &lt;code&gt;@tf.function(input_signature=...)&lt;/code&gt; 并直接作为签名传递，或者通过 &lt;code&gt;@tf.function&lt;/code&gt; 修饰的方法对 &lt;code&gt;get_concrete_function&lt;/code&gt; 的调用来懒惰地传递。</target>
        </trans-unit>
        <trans-unit id="5eb10eb93ba6242a60bc81190f01e4562cc7c7f2" translate="yes" xml:space="preserve">
          <source>The options are &quot;global&quot; in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values.</source>
          <target state="translated">这些选项是 &quot;全局 &quot;的,因为它们适用于整个数据集。如果选项被多次设置,只要不同的选项不使用不同的非默认值,它们就会被合并。</target>
        </trans-unit>
        <trans-unit id="839958c9eddae1511a49cbdd886b3f520fbec271" translate="yes" xml:space="preserve">
          <source>The order of output arguments here is &lt;code&gt;s&lt;/code&gt;, &lt;code&gt;u&lt;/code&gt;, &lt;code&gt;v&lt;/code&gt; when &lt;code&gt;compute_uv&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, as opposed to &lt;code&gt;u&lt;/code&gt;, &lt;code&gt;s&lt;/code&gt;, &lt;code&gt;v&lt;/code&gt; for numpy.linalg.svd.</source>
          <target state="translated">当 &lt;code&gt;compute_uv&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; 时，此处输出参数的顺序为 &lt;code&gt;s&lt;/code&gt; ， &lt;code&gt;u&lt;/code&gt; ， &lt;code&gt;v&lt;/code&gt; ，而不是numpy.linalg.svd的 &lt;code&gt;u&lt;/code&gt; ， &lt;code&gt;s&lt;/code&gt; ， &lt;code&gt;v&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f3bc730bbfda16a7dd0623a512f8a88cc6217898" translate="yes" xml:space="preserve">
          <source>The original device on which &lt;code&gt;input_dataset&lt;/code&gt; will be placed.</source>
          <target state="translated">将在其上放置 &lt;code&gt;input_dataset&lt;/code&gt; 的原始设备。</target>
        </trans-unit>
        <trans-unit id="18d387209dd33fca87db5f4de4ba44e61656b7a9" translate="yes" xml:space="preserve">
          <source>The original input to &lt;a href=&quot;lu&quot;&gt;&lt;code&gt;tf.linalg.lu&lt;/code&gt;&lt;/a&gt;, i.e., &lt;code&gt;x&lt;/code&gt; as in, &lt;code&gt;lu_reconstruct(*tf.linalg.lu(x))&lt;/code&gt;.</source>
          <target state="translated">&lt;a href=&quot;lu&quot;&gt; &lt;code&gt;tf.linalg.lu&lt;/code&gt; &lt;/a&gt;的原始输入，即 &lt;code&gt;x&lt;/code&gt; ,如 &lt;code&gt;lu_reconstruct(*tf.linalg.lu(x))&lt;/code&gt; 所示。</target>
        </trans-unit>
        <trans-unit id="af401239a74cc22bb0656ceab277ae88265e81dd" translate="yes" xml:space="preserve">
          <source>The original method wrapped such that it enters the module's name scope.</source>
          <target state="translated">原方法包装后,使其进入模块的名称范围。</target>
        </trans-unit>
        <trans-unit id="f891d19803fb509490762be082d2f8ade6148c86" translate="yes" xml:space="preserve">
          <source>The original registered Flag objects can be retrieved through the use of the dictionary-like operator, &lt;strong&gt;getitem&lt;/strong&gt;: x = FLAGS['longname'] # access the registered Flag object</source>
          <target state="translated">可以使用类似于字典的运算符&lt;strong&gt;getitem&lt;/strong&gt;来检索原始注册的Flag对象：x = FLAGS ['longname']＃访问注册的Flag对象</target>
        </trans-unit>
        <trans-unit id="b71549bb8296bfb172b0e5fa059a19b9055f4136" translate="yes" xml:space="preserve">
          <source>The other method &lt;code&gt;uniform&lt;/code&gt; only covers the range [minval, maxval), which cannot be &lt;code&gt;dtype&lt;/code&gt;'s full range because &lt;code&gt;maxval&lt;/code&gt; is of type &lt;code&gt;dtype&lt;/code&gt;.</source>
          <target state="translated">另一种方法 &lt;code&gt;uniform&lt;/code&gt; 仅覆盖范围[MINVAL，MAXVAL），其不能是 &lt;code&gt;dtype&lt;/code&gt; 的全范围内，因为 &lt;code&gt;maxval&lt;/code&gt; 的类型为 &lt;code&gt;dtype&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="8d650eca6a136e51fc7375e0969bc2ea1115b2e2" translate="yes" xml:space="preserve">
          <source>The output &lt;code&gt;SparseTensor&lt;/code&gt; object's shape values for all dimensions but the first are the max across the input &lt;code&gt;SparseTensor&lt;/code&gt; objects' shape values for the corresponding dimensions. Its first shape value is &lt;code&gt;N&lt;/code&gt;, the minibatch size.</source>
          <target state="translated">所有维度的输出 &lt;code&gt;SparseTensor&lt;/code&gt; 对象的形状值，但第一个是相应维度的输入 &lt;code&gt;SparseTensor&lt;/code&gt; 对象的形状值的最大值。它的第一个形状值是 &lt;code&gt;N&lt;/code&gt; （最小批量大小）。</target>
        </trans-unit>
        <trans-unit id="f7899fd3a60fa39b01002b08a0bd2e514e0a6d60" translate="yes" xml:space="preserve">
          <source>The output &lt;code&gt;SparseTensor&lt;/code&gt; object's shape values for the original dimensions are the max across the input &lt;code&gt;SparseTensor&lt;/code&gt; objects' shape values for the corresponding dimensions. The new dimensions match the size of the batch.</source>
          <target state="translated">输出 &lt;code&gt;SparseTensor&lt;/code&gt; 为原始尺寸对象的形状的值是在输入最大 &lt;code&gt;SparseTensor&lt;/code&gt; 对象的形状值相应的尺寸。新尺寸与批次的大小匹配。</target>
        </trans-unit>
        <trans-unit id="27cd935b967006cc01a667a4b3f9d0444d7da4c5" translate="yes" xml:space="preserve">
          <source>The output &lt;code&gt;SparseTensor&lt;/code&gt; will be in row-major order and will have the same shape as the input.</source>
          <target state="translated">输出 &lt;code&gt;SparseTensor&lt;/code&gt; 将按行顺序排列，并具有与输入相同的形状。</target>
        </trans-unit>
        <trans-unit id="3ed7c320b010e6441e9928df89e4f8feea635e72" translate="yes" xml:space="preserve">
          <source>The output &lt;code&gt;y&lt;/code&gt; has the same rank as &lt;code&gt;x&lt;/code&gt;. The shapes of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; satisfy: &lt;code&gt;y.shape[i] == x.shape[perm[i]] for i in [0, 1, ..., rank(x) - 1]&lt;/code&gt;</source>
          <target state="translated">输出 &lt;code&gt;y&lt;/code&gt; 与 &lt;code&gt;x&lt;/code&gt; 具有相同的等级。 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 的形状满足： &lt;code&gt;y.shape[i] == x.shape[perm[i]] for i in [0, 1, ..., rank(x) - 1]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="ac7bc3e3640548894477903c76a63029f0684b7b" translate="yes" xml:space="preserve">
          <source>The output &lt;code&gt;y&lt;/code&gt; has the same rank as &lt;code&gt;x&lt;/code&gt;. The shapes of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; satisfy: &lt;code&gt;y.shape[i] == x.shape[perm[i]] for i in [0, 1, ..., rank(x) - 1]&lt;/code&gt;&lt;code&gt;y[i,j,k,...,s,t,u] == conj(x[perm[i], perm[j], perm[k],...,perm[s], perm[t], perm[u]])&lt;/code&gt;</source>
          <target state="translated">输出 &lt;code&gt;y&lt;/code&gt; 与 &lt;code&gt;x&lt;/code&gt; 具有相同的等级。 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 的形状满足： &lt;code&gt;y.shape[i] == x.shape[perm[i]] for i in [0, 1, ..., rank(x) - 1]&lt;/code&gt; &lt;code&gt;y[i,j,k,...,s,t,u] == conj(x[perm[i], perm[j], perm[k],...,perm[s], perm[t], perm[u]])&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="25a24380035e136d9ea98af6a346f119051ef56c" translate="yes" xml:space="preserve">
          <source>The output Tensor as described above, dimensions will vary based on the op provided.</source>
          <target state="translated">如上所述的输出Tensor,尺寸将根据所提供的操作而变化。</target>
        </trans-unit>
        <trans-unit id="47c4d6134bd40a175fa44afa8f9b377d5850de3c" translate="yes" xml:space="preserve">
          <source>The output consists of two tensors LU and P containing the LU decomposition of all input submatrices &lt;code&gt;[..., :, :]&lt;/code&gt;. LU encodes the lower triangular and upper triangular factors.</source>
          <target state="translated">输出由两个张量LU和P组成，其中包含所有输入子矩阵 &lt;code&gt;[..., :, :]&lt;/code&gt; 的LU分解。LU编码下三角和上三角因子。</target>
        </trans-unit>
        <trans-unit id="797ecd44b1fb4a552924484c7a55ad507593ed55" translate="yes" xml:space="preserve">
          <source>The output dtype; defaults to &lt;a href=&quot;../../../tf#int64&quot;&gt;&lt;code&gt;tf.int64&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">输出dtype; 默认为&lt;a href=&quot;../../../tf#int64&quot;&gt; &lt;code&gt;tf.int64&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="31482f2ff54a3aa8fae10ccd117f3f60685fed5f" translate="yes" xml:space="preserve">
          <source>The output dtype; defaults to &lt;a href=&quot;../../tf#int64&quot;&gt;&lt;code&gt;tf.int64&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">输出dtype; 默认为&lt;a href=&quot;../../tf#int64&quot;&gt; &lt;code&gt;tf.int64&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="048c6e1cfd308f6012ab77e90cbc674670b136c0" translate="yes" xml:space="preserve">
          <source>The output elements are taken from the input at intervals given by the &lt;code&gt;rate&lt;/code&gt; argument, as in dilated convolutions.</source>
          <target state="translated">输出元素按 &lt;code&gt;rate&lt;/code&gt; 参数给定的间隔从输入中获取，如膨胀卷积一样。</target>
        </trans-unit>
        <trans-unit id="01cfec4a29cc69f6abe94502062c73070782ab76" translate="yes" xml:space="preserve">
          <source>The output elements will be resorted to preserve the sort order along increasing dimension number.</source>
          <target state="translated">输出元素的排序顺序会随着维数的增加而保留。</target>
        </trans-unit>
        <trans-unit id="38c8f8944795768b987abf44161bc29810671bc1" translate="yes" xml:space="preserve">
          <source>The output is a tensor of rank &lt;code&gt;k+1&lt;/code&gt; with dimensions &lt;code&gt;[I, J, ..., L, M, N]&lt;/code&gt;. If &lt;code&gt;k&lt;/code&gt; is scalar or &lt;code&gt;k[0] == k[1]&lt;/code&gt;:</source>
          <target state="translated">输出是维数为 &lt;code&gt;[I, J, ..., L, M, N]&lt;/code&gt; 的等级 &lt;code&gt;k+1&lt;/code&gt; 的张量。如果 &lt;code&gt;k&lt;/code&gt; 是标量或 &lt;code&gt;k[0] == k[1]&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="b9f3b180f0596bc470d2fb79c1dedac7776f93d9" translate="yes" xml:space="preserve">
          <source>The output is a tensor of shape &lt;code&gt;[..., M, K]&lt;/code&gt;. If &lt;code&gt;adjoint&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; then the innermost matrices in &lt;code&gt;output&lt;/code&gt; satisfy matrix equations &lt;code&gt;matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]&lt;/code&gt;. If &lt;code&gt;adjoint&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; then the strictly then the innermost matrices in &lt;code&gt;output&lt;/code&gt; satisfy matrix equations &lt;code&gt;adjoint(matrix[..., i, k]) * output[..., k, j] = rhs[..., i, j]&lt;/code&gt;.</source>
          <target state="translated">输出是形状为 &lt;code&gt;[..., M, K]&lt;/code&gt; 的张量。如果 &lt;code&gt;adjoint&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ,则 &lt;code&gt;output&lt;/code&gt; 最里面的矩阵满足矩阵方程 &lt;code&gt;matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]&lt;/code&gt; 。如果 &lt;code&gt;adjoint&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; ,则 &lt;code&gt;output&lt;/code&gt; 的最里面矩阵严格满足矩阵方程 &lt;code&gt;adjoint(matrix[..., i, k]) * output[..., k, j] = rhs[..., i, j]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1488da1cb166b21e2e086e4e08f96c22ee229379" translate="yes" xml:space="preserve">
          <source>The output is a tensor of shape &lt;code&gt;[..., M, N]&lt;/code&gt;. If &lt;code&gt;adjoint&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; then the innermost matrices in &lt;code&gt;output&lt;/code&gt; satisfy matrix equations &lt;code&gt;matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]&lt;/code&gt;. If &lt;code&gt;adjoint&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; then the strictly then the innermost matrices in &lt;code&gt;output&lt;/code&gt; satisfy matrix equations &lt;code&gt;adjoint(matrix[..., i, k]) * output[..., k, j] = rhs[..., i, j]&lt;/code&gt;.</source>
          <target state="translated">输出是形状为 &lt;code&gt;[..., M, N]&lt;/code&gt; 的张量。如果 &lt;code&gt;adjoint&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ,则 &lt;code&gt;output&lt;/code&gt; 最里面的矩阵满足矩阵方程 &lt;code&gt;matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]&lt;/code&gt; 。如果 &lt;code&gt;adjoint&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; ,则 &lt;code&gt;output&lt;/code&gt; 的最里面矩阵严格满足矩阵方程 &lt;code&gt;adjoint(matrix[..., i, k]) * output[..., k, j] = rhs[..., i, j]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="6768fd2c3c792ee19a1c494dc0c8f3d25ef95d97" translate="yes" xml:space="preserve">
          <source>The output is a tensor of shape &lt;code&gt;[..., M, N]&lt;/code&gt;. If &lt;code&gt;adjoint&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; then the innermost matrices in output satisfy matrix equations &lt;code&gt;sum_k matrix[..., i, k] * output[..., k, j] = rhs[..., i, j]&lt;/code&gt;. If &lt;code&gt;adjoint&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; then the innermost matrices in output satisfy matrix equations &lt;code&gt;sum_k adjoint(matrix[..., i, k]) * output[..., k, j] = rhs[..., i, j]&lt;/code&gt;.</source>
          <target state="translated">输出是形状为 &lt;code&gt;[..., M, N]&lt;/code&gt; 的张量。如果 &lt;code&gt;adjoint&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ,则输出中最里面的矩阵满足矩阵方程 &lt;code&gt;sum_k matrix[..., i, k] * output[..., k, j] = rhs[..., i, j]&lt;/code&gt; 。如果 &lt;code&gt;adjoint&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; ,则输出中最里面的矩阵满足矩阵方程 &lt;code&gt;sum_k adjoint(matrix[..., i, k]) * output[..., k, j] = rhs[..., i, j]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="24258a9b3cf1537928d726a4c142766d14a612d5" translate="yes" xml:space="preserve">
          <source>The output is a tensor of the same shape as &lt;code&gt;rhs&lt;/code&gt;: either &lt;code&gt;[..., M]&lt;/code&gt; or &lt;code&gt;[..., M, K]&lt;/code&gt;.</source>
          <target state="translated">输出是与 &lt;code&gt;rhs&lt;/code&gt; 形状相同的张量： &lt;code&gt;[..., M]&lt;/code&gt; 或 &lt;code&gt;[..., M, K]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7d3db7f6f2ef07d73508fa555433bf8d83a75eaa" translate="yes" xml:space="preserve">
          <source>The output is a tensor of the same shape as the input containing the Cholesky decompositions for all input submatrices &lt;code&gt;[..., :, :]&lt;/code&gt;.</source>
          <target state="translated">输出是与所有输入子矩阵 &lt;code&gt;[..., :, :]&lt;/code&gt; 包含Cholesky分解的输入具有相同形状的张量。</target>
        </trans-unit>
        <trans-unit id="6194740e93c206c46e0bfecd7aa22357ddec1b58" translate="yes" xml:space="preserve">
          <source>The output is computed as follows:</source>
          <target state="translated">输出计算如下:</target>
        </trans-unit>
        <trans-unit id="b74ff8ee45b0b8f3cea227a5cb8b0ca2512e0fa1" translate="yes" xml:space="preserve">
          <source>The output is:</source>
          <target state="translated">产出是:</target>
        </trans-unit>
        <trans-unit id="02cc9038a532f36829ca3735f949f57712e9a776" translate="yes" xml:space="preserve">
          <source>The output locations corresponding to the implicitly zero elements in the sparse tensor will be zero (i.e., will not take up storage space), regardless of the contents of the dense tensor (even if it's +/-INF and that INF*0 == NaN).</source>
          <target state="translated">稀疏张量中隐含的零元素所对应的输出位置将为零(即不会占用存储空间),无论密张量的内容如何(即使它是+/-INF,并且INF*0 ==NaN)。</target>
        </trans-unit>
        <trans-unit id="4921b3c960168ec33230d0e2275991a2a480fee7" translate="yes" xml:space="preserve">
          <source>The output of &lt;a href=&quot;../compat/v1/estimator/estimator#evaluate&quot;&gt;&lt;code&gt;Estimator.evaluate&lt;/code&gt;&lt;/a&gt; on this checkpoint.</source>
          <target state="translated">&lt;a href=&quot;../compat/v1/estimator/estimator#evaluate&quot;&gt; &lt;code&gt;Estimator.evaluate&lt;/code&gt; &lt;/a&gt;在此检查点上的输出。</target>
        </trans-unit>
        <trans-unit id="b30d98674beeba8558b72dc407638d1340b74552" translate="yes" xml:space="preserve">
          <source>The output of the 1-arg function that takes the &lt;code&gt;step&lt;/code&gt; is &lt;code&gt;values[0]&lt;/code&gt; when &lt;code&gt;step &amp;lt;= boundaries[0]&lt;/code&gt;, &lt;code&gt;values[1]&lt;/code&gt; when &lt;code&gt;step &amp;gt; boundaries[0]&lt;/code&gt; and &lt;code&gt;step &amp;lt;= boundaries[1]&lt;/code&gt;, ..., and values[-1] when &lt;code&gt;step &amp;gt; boundaries[-1]&lt;/code&gt;.</source>
          <target state="translated">1-ARG函数，它接受所述的输出 &lt;code&gt;step&lt;/code&gt; 是 &lt;code&gt;values[0]&lt;/code&gt; 时 &lt;code&gt;step &amp;lt;= boundaries[0]&lt;/code&gt; ， &lt;code&gt;values[1]&lt;/code&gt; 时 &lt;code&gt;step &amp;gt; boundaries[0]&lt;/code&gt; 和 &lt;code&gt;step &amp;lt;= boundaries[1]&lt;/code&gt; ，...，当 &lt;code&gt;step &amp;gt; boundaries[-1]&lt;/code&gt; 时，则使用value [-1]和值。</target>
        </trans-unit>
        <trans-unit id="d04584f160db196293c2030ae17c2dacfbfcd30d" translate="yes" xml:space="preserve">
          <source>The output of this Op is a single bounding box that may be used to crop the original image. The output is returned as 3 tensors: &lt;code&gt;begin&lt;/code&gt;, &lt;code&gt;size&lt;/code&gt; and &lt;code&gt;bboxes&lt;/code&gt;. The first 2 tensors can be fed directly into &lt;a href=&quot;../../../slice&quot;&gt;&lt;code&gt;tf.slice&lt;/code&gt;&lt;/a&gt; to crop the image. The latter may be supplied to &lt;a href=&quot;../../../image/draw_bounding_boxes&quot;&gt;&lt;code&gt;tf.image.draw_bounding_boxes&lt;/code&gt;&lt;/a&gt; to visualize what the bounding box looks like.</source>
          <target state="translated">该Op的输出是单个边界框，可用于裁剪原始图像。输出以3个张量返回： &lt;code&gt;begin&lt;/code&gt; ， &lt;code&gt;size&lt;/code&gt; 和 &lt;code&gt;bboxes&lt;/code&gt; 。前两个张量可以直接馈入&lt;a href=&quot;../../../slice&quot;&gt; &lt;code&gt;tf.slice&lt;/code&gt; &lt;/a&gt;以裁剪图像。后者可以提供给&lt;a href=&quot;../../../image/draw_bounding_boxes&quot;&gt; &lt;code&gt;tf.image.draw_bounding_boxes&lt;/code&gt; &lt;/a&gt;以可视化边界框的外观。</target>
        </trans-unit>
        <trans-unit id="cc8ae412cea3a206e259aeb86af6654e659ed171" translate="yes" xml:space="preserve">
          <source>The output of this Op is a single bounding box that may be used to crop the original image. The output is returned as 3 tensors: &lt;code&gt;begin&lt;/code&gt;, &lt;code&gt;size&lt;/code&gt; and &lt;code&gt;bboxes&lt;/code&gt;. The first 2 tensors can be fed directly into &lt;a href=&quot;../slice&quot;&gt;&lt;code&gt;tf.slice&lt;/code&gt;&lt;/a&gt; to crop the image. The latter may be supplied to &lt;a href=&quot;../image/draw_bounding_boxes&quot;&gt;&lt;code&gt;tf.image.draw_bounding_boxes&lt;/code&gt;&lt;/a&gt; to visualize what the bounding box looks like.</source>
          <target state="translated">该Op的输出是单个边界框，可用于裁剪原始图像。输出以3个张量返回： &lt;code&gt;begin&lt;/code&gt; ， &lt;code&gt;size&lt;/code&gt; 和 &lt;code&gt;bboxes&lt;/code&gt; 。可以将前2个张量直接馈入&lt;a href=&quot;../slice&quot;&gt; &lt;code&gt;tf.slice&lt;/code&gt; &lt;/a&gt;来裁剪图像。后者可以提供给&lt;a href=&quot;../image/draw_bounding_boxes&quot;&gt; &lt;code&gt;tf.image.draw_bounding_boxes&lt;/code&gt; &lt;/a&gt;以可视化边界框的外观。</target>
        </trans-unit>
        <trans-unit id="5e995a951ce7a3cd16e04e8cd70419e327350a30" translate="yes" xml:space="preserve">
          <source>The output of this Op is a single bounding box that may be used to crop the original image. The output is returned as 3 tensors: &lt;code&gt;begin&lt;/code&gt;, &lt;code&gt;size&lt;/code&gt; and &lt;code&gt;bboxes&lt;/code&gt;. The first 2 tensors can be fed directly into &lt;a href=&quot;../slice&quot;&gt;&lt;code&gt;tf.slice&lt;/code&gt;&lt;/a&gt; to crop the image. The latter may be supplied to &lt;a href=&quot;draw_bounding_boxes&quot;&gt;&lt;code&gt;tf.image.draw_bounding_boxes&lt;/code&gt;&lt;/a&gt; to visualize what the bounding box looks like.</source>
          <target state="translated">该Op的输出是单个边界框，可用于裁剪原始图像。输出以3个张量返回： &lt;code&gt;begin&lt;/code&gt; ， &lt;code&gt;size&lt;/code&gt; 和 &lt;code&gt;bboxes&lt;/code&gt; 。前两个张量可以直接馈入&lt;a href=&quot;../slice&quot;&gt; &lt;code&gt;tf.slice&lt;/code&gt; &lt;/a&gt;以裁剪图像。后者可以提供给&lt;a href=&quot;draw_bounding_boxes&quot;&gt; &lt;code&gt;tf.image.draw_bounding_boxes&lt;/code&gt; &lt;/a&gt;以可视化边界框的外观。</target>
        </trans-unit>
        <trans-unit id="867a41c15dc52766b0de85d04ee44344a9063866" translate="yes" xml:space="preserve">
          <source>The output of this method is a 3D &lt;code&gt;Tensor&lt;/code&gt; of shape &lt;code&gt;[batch_size, T, D]&lt;/code&gt;. &lt;code&gt;T&lt;/code&gt; is the maximum sequence length for this batch, which could differ from batch to batch.</source>
          <target state="translated">该方法的输出是 &lt;code&gt;[batch_size, T, D]&lt;/code&gt; 形状的3D &lt;code&gt;Tensor&lt;/code&gt; 。 &lt;code&gt;T&lt;/code&gt; 是该批次的最大序列长度，可能因批次而异。</target>
        </trans-unit>
        <trans-unit id="753b9e20d6aac1628ca75bf0511c387d43eb2530" translate="yes" xml:space="preserve">
          <source>The output of this operation is a set of integers indexing into the input collection of bounding boxes representing the selected boxes. The bounding box coordinates corresponding to the selected indices can then be obtained using the &lt;code&gt;tf.gather operation&lt;/code&gt;. For example:</source>
          <target state="translated">此操作的输出是一组整数，这些整数索引到表示所选框的边界框的输入集合中。然后，可以使用 &lt;code&gt;tf.gather operation&lt;/code&gt; 获取与所选索引对应的边界框坐标。例如：</target>
        </trans-unit>
        <trans-unit id="da6a14177a19f48147884875b20440b0d347e68f" translate="yes" xml:space="preserve">
          <source>The output shape is identical to the inputs', except along the concat dimension, where it is the sum of the inputs' sizes along that dimension.</source>
          <target state="translated">输出的形状与输入的形状相同,除了在concat维度上,它是输入的尺寸之和。</target>
        </trans-unit>
        <trans-unit id="3b30057b1110d4946855f0f7f77e28efcaa8cc94" translate="yes" xml:space="preserve">
          <source>The output shapes are compatible in a way that the first dimension of all tensors are the same and equal to the number of possible split nodes for each feature.</source>
          <target state="translated">输出形状是兼容的,即所有 tensors 的第一个维度是相同的,等于每个特征的可能分割节点数。</target>
        </trans-unit>
        <trans-unit id="3907a316796c56324c363351b8f0ffb1dbcb1c06" translate="yes" xml:space="preserve">
          <source>The output signature of &lt;code&gt;fn&lt;/code&gt;. Must be specified if &lt;code&gt;fn&lt;/code&gt;'s input and output signatures are different (i.e., if their structures, dtypes, or tensor types do not match). &lt;code&gt;fn_output_signature&lt;/code&gt; can be specified using any of the following:</source>
          <target state="translated">&lt;code&gt;fn&lt;/code&gt; 的输出签名。如果 &lt;code&gt;fn&lt;/code&gt; 的输入和输出签名不同（即，如果它们的结构，dtype或张量类型不匹配），则必须指定。可以使用以下任意一种来指定 &lt;code&gt;fn_output_signature&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="eac10a40d7ace5a75143c2fbccbd028ece77a9fb" translate="yes" xml:space="preserve">
          <source>The output slice &lt;code&gt;i&lt;/code&gt; along dimension &lt;code&gt;batch_axis&lt;/code&gt; is then given by input slice &lt;code&gt;i&lt;/code&gt;, with the first &lt;code&gt;seq_lengths[i]&lt;/code&gt; slices along dimension &lt;code&gt;seq_axis&lt;/code&gt; reversed.</source>
          <target state="translated">沿维度 &lt;code&gt;batch_axis&lt;/code&gt; 的输出切片 &lt;code&gt;i&lt;/code&gt; 然后由输入切片 &lt;code&gt;i&lt;/code&gt; 给出，沿维度seq_axis的第一 &lt;code&gt;seq_lengths[i]&lt;/code&gt; 切片 &lt;code&gt;seq_axis&lt;/code&gt; 颠倒。</target>
        </trans-unit>
        <trans-unit id="b24ce3e8aaca6570e3ec8cec6e775f6c5f2c52dd" translate="yes" xml:space="preserve">
          <source>The output slice &lt;code&gt;i&lt;/code&gt; along dimension &lt;code&gt;batch_dim&lt;/code&gt; is then given by input slice &lt;code&gt;i&lt;/code&gt;, with the first &lt;code&gt;seq_lengths[i]&lt;/code&gt; slices along dimension &lt;code&gt;seq_dim&lt;/code&gt; reversed.</source>
          <target state="translated">沿维度 &lt;code&gt;batch_dim&lt;/code&gt; 的输出切片 &lt;code&gt;i&lt;/code&gt; 然后由输入切片 &lt;code&gt;i&lt;/code&gt; 给出，沿维度seq_dim的第一个 &lt;code&gt;seq_lengths[i]&lt;/code&gt; 切片 &lt;code&gt;seq_dim&lt;/code&gt; 颠倒。</target>
        </trans-unit>
        <trans-unit id="76e5d3d89821526e9a3ed69b0fc0e5605bddfe28" translate="yes" xml:space="preserve">
          <source>The output stream, logging level, or file to print to. Defaults to sys.stderr, but sys.stdout, tf.compat.v1.logging.info, tf.compat.v1.logging.warning, tf.compat.v1.logging.error, absl.logging.info, absl.logging.warning and absl.logging.error are also supported. To print to a file, pass a string started with &quot;file://&quot; followed by the file path, e.g., &quot;file:///tmp/foo.out&quot;.</source>
          <target state="translated">要打印的输出流、日志级别或文件。默认值为 sys.stderr,但也支持 sys.stdout、tf.compat.v1.logging.info、tf.compat.v1.logging.warning、tf.compat.v1.logging.error、absl.logging.info、absl.logging.warning 和 absl.logging.error。要打印到文件,需要传递一个以 &quot;file://&quot;开头的字符串,后面是文件路径,例如 &quot;file:///tmp/foo.out&quot;。</target>
        </trans-unit>
        <trans-unit id="098e1ce5365a1d1a1384c17c4ced06e659b0880f" translate="yes" xml:space="preserve">
          <source>The output subscripts must contain only labels appearing in at least one of the input subscripts. Furthermore, all dimensions mapping to the same axis label must be equal.</source>
          <target state="translated">输出子标必须只包含至少一个输入子标中出现的标签。此外,映射到同一轴标签的所有维度必须相等。</target>
        </trans-unit>
        <trans-unit id="d0270145e0963e3fa6b4466fddef88fdbf60e481" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[1, 2, 2, 1]&lt;/code&gt; and value:</source>
          <target state="translated">输出张量具有形状 &lt;code&gt;[1, 2, 2, 1]&lt;/code&gt; 1、2、2、1 ]和值：</target>
        </trans-unit>
        <trans-unit id="aff644b1606cf977042c3f329c0b44912b343d5f" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[1, 2, 2, 3]&lt;/code&gt; and value:</source>
          <target state="translated">输出张量具有形状 &lt;code&gt;[1, 2, 2, 3]&lt;/code&gt; 1、2、2、3 ]和值：</target>
        </trans-unit>
        <trans-unit id="26ecc2d485656620c8ddae36c3377b136f2c8ef6" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[1, 4, 4, 1]&lt;/code&gt; and value:</source>
          <target state="translated">输出张量具有形状 &lt;code&gt;[1, 4, 4, 1]&lt;/code&gt; 1、4、4、1 ]和值：</target>
        </trans-unit>
        <trans-unit id="413cce56d7e73e0557f6955da012eeb198f78207" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[2, 2, 4, 1]&lt;/code&gt; and value:</source>
          <target state="translated">输出张量的形状为 &lt;code&gt;[2, 2, 4, 1]&lt;/code&gt; 2，2，4，1 ]，其值是：</target>
        </trans-unit>
        <trans-unit id="580524501d7754a464d140a489afd8a69565b7e8" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[4, 1, 1, 1]&lt;/code&gt; and value:</source>
          <target state="translated">输出张量的形状为 &lt;code&gt;[4, 1, 1, 1]&lt;/code&gt; 4，1，1，1 ]，其值是：</target>
        </trans-unit>
        <trans-unit id="43b93cea1c5beede2692779450404b22ddd16414" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[4, 1, 1, 3]&lt;/code&gt; and value:</source>
          <target state="translated">输出张量的形状为 &lt;code&gt;[4, 1, 1, 3]&lt;/code&gt; 4，1，1，3 ]，其值是：</target>
        </trans-unit>
        <trans-unit id="863590a97a6747705662d02c1d09d524c465ba85" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[4, 2, 2, 1]&lt;/code&gt; and value:</source>
          <target state="translated">输出张量具有形状 &lt;code&gt;[4, 2, 2, 1]&lt;/code&gt; 4，2，2，1 ]和值：</target>
        </trans-unit>
        <trans-unit id="951a2f1acb5ea54cd01c5c69fad552b3af0398ec" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[8, 1, 2, 1]&lt;/code&gt; and value:</source>
          <target state="translated">输出张量的形状为 &lt;code&gt;[8, 1, 2, 1]&lt;/code&gt; 8，1，2，1 ]，其值是：</target>
        </trans-unit>
        <trans-unit id="b991f9c184601aa1cf79a030dce35cd2e1811686" translate="yes" xml:space="preserve">
          <source>The output tensor has shape &lt;code&gt;[8, 1, 3, 1]&lt;/code&gt; and value:</source>
          <target state="translated">输出张量的形状为 &lt;code&gt;[8, 1, 3, 1]&lt;/code&gt; 8，1，3，1 ]，其值是：</target>
        </trans-unit>
        <trans-unit id="8a7212b9117e06db921246aafe5499cc8ff0cedb" translate="yes" xml:space="preserve">
          <source>The output tensor is 2-D or higher with shape &lt;code&gt;[..., r_o, c_o]&lt;/code&gt;, where:</source>
          <target state="translated">输出张量为2-D或更高，形状为 &lt;code&gt;[..., r_o, c_o]&lt;/code&gt; ，其中：</target>
        </trans-unit>
        <trans-unit id="c6dc975d4ffc27e7fe4b516c5e078460e61b472d" translate="yes" xml:space="preserve">
          <source>The output tensor is a tensor with dimensions described by 'size' whose values are extracted from 'input' starting at the offsets in 'begin'.</source>
          <target state="translated">输出张量是一个尺寸由'size'描述的张量,其值从'begin'中的偏移量开始从'input'中提取。</target>
        </trans-unit>
        <trans-unit id="018a92a28c57571fe5c1edcfb924689c52abc4b8" translate="yes" xml:space="preserve">
          <source>The output tensor, of rank 3.</source>
          <target state="translated">输出张量,等级为3。</target>
        </trans-unit>
        <trans-unit id="1f5eef2e4414626524178eb10c4ab831f83220c9" translate="yes" xml:space="preserve">
          <source>The output tensor.</source>
          <target state="translated">输出张量。</target>
        </trans-unit>
        <trans-unit id="f156d1cf6b13fd91d5c1751333de4bfa7481e5ec" translate="yes" xml:space="preserve">
          <source>The output tensors for the loop variables after the loop. If &lt;code&gt;return_same_structure&lt;/code&gt; is True, the return value has the same structure as &lt;code&gt;loop_vars&lt;/code&gt;. If &lt;code&gt;return_same_structure&lt;/code&gt; is False, the return value is a Tensor, TensorArray or IndexedSlice if the length of &lt;code&gt;loop_vars&lt;/code&gt; is 1, or a list otherwise.</source>
          <target state="translated">循环后循环变量的输出张量。如果 &lt;code&gt;return_same_structure&lt;/code&gt; 为True，则返回值具有与 &lt;code&gt;loop_vars&lt;/code&gt; 相同的结构。如果 &lt;code&gt;return_same_structure&lt;/code&gt; 是假，返回值是一个张量，TensorArray或IndexedSlice如果长度 &lt;code&gt;loop_vars&lt;/code&gt; 为1，或列表其他。</target>
        </trans-unit>
        <trans-unit id="0bda5fbd6b0783b500c1218ac7c4c0e7746a4905" translate="yes" xml:space="preserve">
          <source>The output tensors for the loop variables after the loop. The return value has the same structure as &lt;code&gt;loop_vars&lt;/code&gt;.</source>
          <target state="translated">循环后循环变量的输出张量。返回值与 &lt;code&gt;loop_vars&lt;/code&gt; 具有相同的结构。</target>
        </trans-unit>
        <trans-unit id="5a54d6c5881e40fdc200259b55b1119a1261f8a4" translate="yes" xml:space="preserve">
          <source>The output type (&lt;code&gt;int32&lt;/code&gt; or &lt;code&gt;int64&lt;/code&gt;). Default is &lt;a href=&quot;../tf#int32&quot;&gt;&lt;code&gt;tf.int32&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">输出类型（ &lt;code&gt;int32&lt;/code&gt; 或 &lt;code&gt;int64&lt;/code&gt; ）。默认值为&lt;a href=&quot;../tf#int32&quot;&gt; &lt;code&gt;tf.int32&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="abf2d0fef772f974b83b58a081ba47917d2e0a4c" translate="yes" xml:space="preserve">
          <source>The output will be a 3x2 matrix:</source>
          <target state="translated">输出将是一个3x2矩阵。</target>
        </trans-unit>
        <trans-unit id="1bcada90c4c3eacc21413e8c1e200a0e90b07ffc" translate="yes" xml:space="preserve">
          <source>The output will then have shape &lt;code&gt;(32, 10, 32)&lt;/code&gt;.</source>
          <target state="translated">然后，输出将具有形状 &lt;code&gt;(32, 10, 32)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="6d23cb786d0f9d1271884eaa7c1f7cc8c8a656c8" translate="yes" xml:space="preserve">
          <source>The output will then have shape &lt;code&gt;(32, 10, 8)&lt;/code&gt;.</source>
          <target state="translated">然后，输出将具有形状 &lt;code&gt;(32, 10, 8)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="95238fe986246430e838c38242139b3ddafc69a1" translate="yes" xml:space="preserve">
          <source>The output(s) of the model. See Functional API example below.</source>
          <target state="translated">模型的输出。见下面的功能API示例。</target>
        </trans-unit>
        <trans-unit id="25ae41f4c527c0741341a2f8b321fd1e43f0f98b" translate="yes" xml:space="preserve">
          <source>The outputs are a deterministic function of &lt;code&gt;shape&lt;/code&gt; and &lt;code&gt;seed&lt;/code&gt;.</source>
          <target state="translated">输出是 &lt;code&gt;shape&lt;/code&gt; 和 &lt;code&gt;seed&lt;/code&gt; 的确定性函数。</target>
        </trans-unit>
        <trans-unit id="179a09c09a45e5c62e2dec3d73d951ec617567f3" translate="yes" xml:space="preserve">
          <source>The outputs are a deterministic function of &lt;code&gt;shape&lt;/code&gt;, &lt;code&gt;seed&lt;/code&gt;, &lt;code&gt;counts&lt;/code&gt;, and &lt;code&gt;probs&lt;/code&gt;.</source>
          <target state="translated">输出是 &lt;code&gt;shape&lt;/code&gt; ， &lt;code&gt;seed&lt;/code&gt; ， &lt;code&gt;counts&lt;/code&gt; 和 &lt;code&gt;probs&lt;/code&gt; 的确定性函数。</target>
        </trans-unit>
        <trans-unit id="d94bca360c8d84e96136112086d8b12a0b843587" translate="yes" xml:space="preserve">
          <source>The outputs are a deterministic function of &lt;code&gt;shape&lt;/code&gt;, &lt;code&gt;seed&lt;/code&gt;, &lt;code&gt;minval&lt;/code&gt;, and &lt;code&gt;maxval&lt;/code&gt;.</source>
          <target state="translated">输出是 &lt;code&gt;shape&lt;/code&gt; ， &lt;code&gt;seed&lt;/code&gt; ， &lt;code&gt;minval&lt;/code&gt; 和 &lt;code&gt;maxval&lt;/code&gt; 的确定性函数。</target>
        </trans-unit>
        <trans-unit id="94289abe2083c6f7cb75f0c826a60167f1e5c8a7" translate="yes" xml:space="preserve">
          <source>The outputs are a deterministic function of &lt;code&gt;shape&lt;/code&gt;, &lt;code&gt;seed&lt;/code&gt;, and &lt;code&gt;alpha&lt;/code&gt;.</source>
          <target state="translated">输出是 &lt;code&gt;shape&lt;/code&gt; ， &lt;code&gt;seed&lt;/code&gt; 和 &lt;code&gt;alpha&lt;/code&gt; 的确定性函数。</target>
        </trans-unit>
        <trans-unit id="228f72119b53e369b51ce6a29a98f2313a7f281f" translate="yes" xml:space="preserve">
          <source>The outputs are a deterministic function of &lt;code&gt;shape&lt;/code&gt;, &lt;code&gt;seed&lt;/code&gt;, and &lt;code&gt;lam&lt;/code&gt;.</source>
          <target state="translated">输出是 &lt;code&gt;shape&lt;/code&gt; ， &lt;code&gt;seed&lt;/code&gt; 和 &lt;code&gt;lam&lt;/code&gt; 的确定性函数。</target>
        </trans-unit>
        <trans-unit id="262b53d310948dd3d4d6f3d1b24c8027dcc78d4c" translate="yes" xml:space="preserve">
          <source>The outputs from all shards are concatenated back together along their 0-th dimension.</source>
          <target state="translated">所有碎片的输出都沿其0-th维度连接在一起。</target>
        </trans-unit>
        <trans-unit id="fec5d02e1747f7a22db33ab6777959dc638f2020" translate="yes" xml:space="preserve">
          <source>The outputs of functions used as &lt;code&gt;signatures&lt;/code&gt; must either be flat lists, in which case outputs will be numbered, or a dictionary mapping string keys to &lt;code&gt;Tensor&lt;/code&gt;, in which case the keys will be used to name outputs.</source>
          <target state="translated">用作 &lt;code&gt;signatures&lt;/code&gt; 的函数的输出必须是平面列表（在这种情况下，输出将被编号），或者是将字符串键映射到 &lt;code&gt;Tensor&lt;/code&gt; 的字典，在这种情况下，键将用于命名输出。</target>
        </trans-unit>
        <trans-unit id="3777ae82c580b33611793d1c79fb38091c169efb" translate="yes" xml:space="preserve">
          <source>The package that this class belongs to.</source>
          <target state="translated">该类所属的包。</target>
        </trans-unit>
        <trans-unit id="f4fe37226e907ad6b84032ccb7a1a60c428d9656" translate="yes" xml:space="preserve">
          <source>The padded size of each dimension D of the output is:</source>
          <target state="translated">输出的每个维度D的填充大小为。</target>
        </trans-unit>
        <trans-unit id="a3493bda610bfabe1d614c17be7bea5bab6bc509" translate="yes" xml:space="preserve">
          <source>The padding algorithm, must be &quot;SAME&quot; or &quot;VALID&quot;. Defaults to &quot;SAME&quot;. See the &quot;returns&quot; section of &lt;a href=&quot;convolution&quot;&gt;&lt;code&gt;tf.nn.convolution&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">填充算法必须为&amp;ldquo; SAME&amp;rdquo;或&amp;ldquo; VALID&amp;rdquo;。默认为&amp;ldquo; SAME&amp;rdquo;。有关详细信息，请参见&lt;a href=&quot;convolution&quot;&gt; &lt;code&gt;tf.nn.convolution&lt;/code&gt; &lt;/a&gt;的&amp;ldquo;返回&amp;rdquo;部分。</target>
        </trans-unit>
        <trans-unit id="94a793cb35afefaa9825726cef177d5eba1c0835" translate="yes" xml:space="preserve">
          <source>The padding algorithm, must be &quot;SAME&quot; or &quot;VALID&quot;. See the &quot;returns&quot; section of &lt;a href=&quot;../../../nn/convolution&quot;&gt;&lt;code&gt;tf.nn.convolution&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">填充算法必须为&amp;ldquo; SAME&amp;rdquo;或&amp;ldquo; VALID&amp;rdquo;。有关详细信息，请参见&lt;a href=&quot;../../../nn/convolution&quot;&gt; &lt;code&gt;tf.nn.convolution&lt;/code&gt; &lt;/a&gt;的&amp;ldquo;返回&amp;rdquo;部分。</target>
        </trans-unit>
        <trans-unit id="82cf1bc0ef022c1c7dcb21d0cd3cf77bf756e7b4" translate="yes" xml:space="preserve">
          <source>The paper demonstrates the performance of MobileNets using &lt;code&gt;alpha&lt;/code&gt; values of 1.0 (also called 100 % MobileNet), 0.35, 0.5, 0.75, 1.0, 1.3, and 1.4 For each of these &lt;code&gt;alpha&lt;/code&gt; values, weights for 5 different input image sizes are provided (224, 192, 160, 128, and 96).</source>
          <target state="translated">本文演示了使用1.0（也称为100％MobileNet），0.35、0.5、0.75、1.0、1.3和1.4的 &lt;code&gt;alpha&lt;/code&gt; 值的MobileNets的性能。对于每个这些 &lt;code&gt;alpha&lt;/code&gt; 值，提供了5种不同输入图像尺寸的权重（224 ，192、160、128和96）。</target>
        </trans-unit>
        <trans-unit id="07d5b7b8864997727d8fb01740ad8b9e08b14f5f" translate="yes" xml:space="preserve">
          <source>The paper demonstrates the performance of MobileNets using &lt;code&gt;alpha&lt;/code&gt; values of 1.0 (also called 100 % MobileNet), 0.75, 0.5 and 0.25. For each of these &lt;code&gt;alpha&lt;/code&gt; values, weights for 4 different input image sizes are provided (224, 192, 160, 128).</source>
          <target state="translated">本文演示了 &lt;code&gt;alpha&lt;/code&gt; 值为1.0（也称为100％MobileNet），0.75、0.5和0.25时MobileNets的性能。对于每个这些 &lt;code&gt;alpha&lt;/code&gt; 值，都提供了4种不同输入图像大小的权重（224、192、160、128）。</target>
        </trans-unit>
        <trans-unit id="d91e185b3ec2b95e4e28b058fb7de6ed54c4f63d" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;concentration&lt;/code&gt; and &lt;code&gt;rate&lt;/code&gt; must be shaped in a way that supports broadcasting (e.g. &lt;code&gt;concentration + rate&lt;/code&gt; is a valid operation).</source>
          <target state="translated">必须以支持广播的方式调整参数 &lt;code&gt;concentration&lt;/code&gt; 和 &lt;code&gt;rate&lt;/code&gt; （例如， &lt;code&gt;concentration + rate&lt;/code&gt; 是有效操作）。</target>
        </trans-unit>
        <trans-unit id="c4bf0f9162b85380657aa85a48762d0989942eac" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;df&lt;/code&gt;, &lt;code&gt;loc&lt;/code&gt;, and &lt;code&gt;scale&lt;/code&gt; must be shaped in a way that supports broadcasting (e.g. &lt;code&gt;df + loc + scale&lt;/code&gt; is a valid operation).</source>
          <target state="translated">参数 &lt;code&gt;df&lt;/code&gt; ， &lt;code&gt;loc&lt;/code&gt; 和 &lt;code&gt;scale&lt;/code&gt; 必须以支持广播的方式成形（例如 &lt;code&gt;df + loc + scale&lt;/code&gt; 是有效操作）。</target>
        </trans-unit>
        <trans-unit id="f624db393a0283cdff3712a3c4bd723d749a30a5" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;loc&lt;/code&gt; and &lt;code&gt;scale&lt;/code&gt; must be shaped in a way that supports broadcasting (e.g. &lt;code&gt;loc + scale&lt;/code&gt; is a valid operation).</source>
          <target state="translated">参数 &lt;code&gt;loc&lt;/code&gt; 和 &lt;code&gt;scale&lt;/code&gt; 必须以支持广播的方式成形（例如 &lt;code&gt;loc + scale&lt;/code&gt; 是有效操作）。</target>
        </trans-unit>
        <trans-unit id="a2bb7ef60bf0239928d550799fd1318ab8a8e22c" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;loc&lt;/code&gt; and &lt;code&gt;scale&lt;/code&gt; must be shaped in a way that supports broadcasting (e.g., &lt;code&gt;loc / scale&lt;/code&gt; is a valid operation).</source>
          <target state="translated">参数 &lt;code&gt;loc&lt;/code&gt; 和 &lt;code&gt;scale&lt;/code&gt; 必须以支持广播的方式成形（例如 &lt;code&gt;loc / scale&lt;/code&gt; 是有效操作）。</target>
        </trans-unit>
        <trans-unit id="caea2d657314fc74a3b4c82a6b1a8e2927d4e265" translate="yes" xml:space="preserve">
          <source>The parameters &lt;code&gt;low&lt;/code&gt; and &lt;code&gt;high&lt;/code&gt; must be shaped in a way that supports broadcasting (e.g., &lt;code&gt;high - low&lt;/code&gt; is a valid operation).</source>
          <target state="translated">参数 &lt;code&gt;low&lt;/code&gt; 和 &lt;code&gt;high&lt;/code&gt; 必须以支持广播的方式成形（例如 &lt;code&gt;high - low&lt;/code&gt; 是有效操作）。</target>
        </trans-unit>
        <trans-unit id="2da0524f5e34c476f58d52fd7e4bbaca30e3840a" translate="yes" xml:space="preserve">
          <source>The parameters apply to and only to the immediately enclosing loop. It only has effect if the loop is staged as a TF while_loop; otherwise the parameters have no effect.</source>
          <target state="translated">这些参数适用于并且只适用于紧邻的循环。只有当循环被暂存为TF while_loop时,它才有效果;否则参数没有效果。</target>
        </trans-unit>
        <trans-unit id="f861e09ace62d9cf6a436ce61f90b0514dff3612" translate="yes" xml:space="preserve">
          <source>The parameters can be intuited via their relationship to mean and stddev,</source>
          <target state="translated">这些参数可以通过它们与均值和stddev的关系直观地表现出来。</target>
        </trans-unit>
        <trans-unit id="78832ff2cd51fc67f3284e123b0a8c7a1096d66b" translate="yes" xml:space="preserve">
          <source>The parent could be a module when the child is a function at module scope. Or the parent could be a class when a class' method is being replaced. The named child is set to new_child, while the prior definition is saved away for later, when UnsetAll() is called.</source>
          <target state="translated">当子类是模块范围内的函数时,父类可能是一个模块,或者当一个类的方法被替换时,父类可能是一个类。或者当一个类的方法被替换时,父类可以是一个类。被命名的子类会被设置为new_child,而之前的定义会被保存起来,以便以后调用UnsetAll()时使用。</target>
        </trans-unit>
        <trans-unit id="cc13d476090ccb6ba884188ddcc28a81f62b0726" translate="yes" xml:space="preserve">
          <source>The parse() method checks to make sure that the string argument is a legal value and convert it to a native type. If the value cannot be converted, it should throw a 'ValueError' exception with a human readable explanation of why the value is illegal.</source>
          <target state="translated">parse()方法检查以确保字符串参数是一个合法的值,并将其转换为本地类型。如果该值不能转换,它应该抛出一个 &quot;ValueError &quot;异常,并以人类可读的方式解释为什么该值是非法的。</target>
        </trans-unit>
        <trans-unit id="cf0794be97523b0be42896e118da1be5a61ff4da" translate="yes" xml:space="preserve">
          <source>The parsed value in native type.</source>
          <target state="translated">本机类型的解析值。</target>
        </trans-unit>
        <trans-unit id="8cff8c184992ecf7d79d6a190fc8e2ad81b8dd10" translate="yes" xml:space="preserve">
          <source>The partitioned embedding in &lt;code&gt;embedding_weights&lt;/code&gt; must all be the same shape except for the first dimension. The first dimension is allowed to vary as the vocabulary size is not necessarily a multiple of &lt;code&gt;P&lt;/code&gt;. &lt;code&gt;embedding_weights&lt;/code&gt; may be a &lt;code&gt;PartitionedVariable&lt;/code&gt; as returned by using &lt;a href=&quot;../compat/v1/get_variable&quot;&gt;&lt;code&gt;tf.compat.v1.get_variable()&lt;/code&gt;&lt;/a&gt; with a partitioner.</source>
          <target state="translated">在分区嵌入 &lt;code&gt;embedding_weights&lt;/code&gt; 必须全部除了第一尺寸相同的形状。由于词汇量不一定是 &lt;code&gt;P&lt;/code&gt; 的倍数，因此可以改变第一维。 &lt;code&gt;embedding_weights&lt;/code&gt; 可以是通过将&lt;a href=&quot;../compat/v1/get_variable&quot;&gt; &lt;code&gt;tf.compat.v1.get_variable()&lt;/code&gt; &lt;/a&gt;与分区程序一起使用返回的 &lt;code&gt;PartitionedVariable&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="8e157288a667a186f035deba918682c801fa1abf" translate="yes" xml:space="preserve">
          <source>The partitioned embedding in &lt;code&gt;embedding_weights&lt;/code&gt; must all be the same shape except for the first dimension. The first dimension is allowed to vary as the vocabulary size is not necessarily a multiple of &lt;code&gt;P&lt;/code&gt;. &lt;code&gt;embedding_weights&lt;/code&gt; may be a &lt;code&gt;PartitionedVariable&lt;/code&gt; as returned by using &lt;a href=&quot;../get_variable&quot;&gt;&lt;code&gt;tf.compat.v1.get_variable()&lt;/code&gt;&lt;/a&gt; with a partitioner.</source>
          <target state="translated">在分区嵌入 &lt;code&gt;embedding_weights&lt;/code&gt; 必须全部除了第一尺寸相同的形状。由于词汇量不一定是 &lt;code&gt;P&lt;/code&gt; 的倍数，因此可以改变第一维。 &lt;code&gt;embedding_weights&lt;/code&gt; 可以是通过将&lt;a href=&quot;../get_variable&quot;&gt; &lt;code&gt;tf.compat.v1.get_variable()&lt;/code&gt; &lt;/a&gt;与分区程序一起使用返回的 &lt;code&gt;PartitionedVariable&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1549f14314cb2911099be0c08464c7d5489a22a6" translate="yes" xml:space="preserve">
          <source>The partitioned embedding in &lt;code&gt;embedding_weights&lt;/code&gt; must all be the same shape except for the first dimension. The first dimension is allowed to vary as the vocabulary size is not necessarily a multiple of num of shards.</source>
          <target state="translated">在分区嵌入 &lt;code&gt;embedding_weights&lt;/code&gt; 必须全部除了第一尺寸相同的形状。由于词汇量不一定是碎片数的倍数，因此可以改变第一维度。</target>
        </trans-unit>
        <trans-unit id="b3b7bcbb761db59d963bb51839f46a4b29f4af4e" translate="yes" xml:space="preserve">
          <source>The path is relative to tensorflow/</source>
          <target state="translated">该路径是相对于tensorflow/</target>
        </trans-unit>
        <trans-unit id="f48baa97e568a9b9e3b5f52a6ab7e866fecd8550" translate="yes" xml:space="preserve">
          <source>The path of the output proto file.</source>
          <target state="translated">输出原文件的路径。</target>
        </trans-unit>
        <trans-unit id="ee14c49610bbef1394c243e137f1c0c32f1960be" translate="yes" xml:space="preserve">
          <source>The path to a directory in which to write checkpoints. A special file named &quot;checkpoint&quot; is also written to this directory (in a human-readable text format) which contains the state of the &lt;code&gt;CheckpointManager&lt;/code&gt;.</source>
          <target state="translated">在其中写入检查点的目录的路径。还将一个名为&amp;ldquo; checkpoint&amp;rdquo;的特殊文件（以人类可读的文本格式）写入此目录，其中包含 &lt;code&gt;CheckpointManager&lt;/code&gt; 的状态。</target>
        </trans-unit>
        <trans-unit id="b3d2248f6c5ba1b79f5657059376b70c88f9d6c1" translate="yes" xml:space="preserve">
          <source>The path to an event file created by a &lt;code&gt;SummaryWriter&lt;/code&gt;.</source>
          <target state="translated">由 &lt;code&gt;SummaryWriter&lt;/code&gt; 创建的事件文件的路径。</target>
        </trans-unit>
        <trans-unit id="9a7713229c9d7bf490b65513fab0d07d2cad4f51" translate="yes" xml:space="preserve">
          <source>The path to the TFRecords file.</source>
          <target state="translated">TFRecords文件的路径。</target>
        </trans-unit>
        <trans-unit id="84c5edf8f8528c6dcc19dfeb36d929c09dc91f28" translate="yes" xml:space="preserve">
          <source>The path to the checkpoint as returned by &lt;code&gt;write&lt;/code&gt;.</source>
          <target state="translated">由 &lt;code&gt;write&lt;/code&gt; 返回的检查点路径。</target>
        </trans-unit>
        <trans-unit id="b8030756c5c759c44f9f3cb2091477d30b127210" translate="yes" xml:space="preserve">
          <source>The path to the checkpoint, as returned by &lt;code&gt;save&lt;/code&gt; or &lt;a href=&quot;../../../train/latest_checkpoint&quot;&gt;&lt;code&gt;tf.train.latest_checkpoint&lt;/code&gt;&lt;/a&gt;. If None (as when there is no latest checkpoint for &lt;a href=&quot;../../../train/latest_checkpoint&quot;&gt;&lt;code&gt;tf.train.latest_checkpoint&lt;/code&gt;&lt;/a&gt; to return), returns an object which may run initializers for objects in the dependency graph. If the checkpoint was written by the name-based &lt;a href=&quot;saver&quot;&gt;&lt;code&gt;tf.compat.v1.train.Saver&lt;/code&gt;&lt;/a&gt;, names are used to match variables.</source>
          <target state="translated">&lt;code&gt;save&lt;/code&gt; 或&lt;a href=&quot;../../../train/latest_checkpoint&quot;&gt; &lt;code&gt;tf.train.latest_checkpoint&lt;/code&gt; &lt;/a&gt;返回的检查点路径。如果为None（如没有要返回的&lt;a href=&quot;../../../train/latest_checkpoint&quot;&gt; &lt;code&gt;tf.train.latest_checkpoint&lt;/code&gt; 的&lt;/a&gt;最新检查点），则返回一个对象，该对象可以为依赖关系图中的对象运行初始化程序。如果检查点由基于名称的&lt;a href=&quot;saver&quot;&gt; &lt;code&gt;tf.compat.v1.train.Saver&lt;/code&gt; 编写&lt;/a&gt;，则名称用于匹配变量。</target>
        </trans-unit>
        <trans-unit id="7d9bb46d11e0a3efc649638be0d00aa480e84e55" translate="yes" xml:space="preserve">
          <source>The path to the checkpoint, as returned by &lt;code&gt;save&lt;/code&gt; or &lt;a href=&quot;latest_checkpoint&quot;&gt;&lt;code&gt;tf.train.latest_checkpoint&lt;/code&gt;&lt;/a&gt;. If the checkpoint was written by the name-based &lt;a href=&quot;../compat/v1/train/saver&quot;&gt;&lt;code&gt;tf.compat.v1.train.Saver&lt;/code&gt;&lt;/a&gt;, names are used to match variables.</source>
          <target state="translated">&lt;code&gt;save&lt;/code&gt; 或&lt;a href=&quot;latest_checkpoint&quot;&gt; &lt;code&gt;tf.train.latest_checkpoint&lt;/code&gt; &lt;/a&gt;返回的检查点路径。如果检查点由基于名称的&lt;a href=&quot;../compat/v1/train/saver&quot;&gt; &lt;code&gt;tf.compat.v1.train.Saver&lt;/code&gt; 编写&lt;/a&gt;，则名称用于匹配变量。</target>
        </trans-unit>
        <trans-unit id="d3c8f8b6fd3cf107eb955208c8e6315d0e1a6163" translate="yes" xml:space="preserve">
          <source>The path to the exported directory as a bytes object.</source>
          <target state="translated">导出目录的路径为字节对象。</target>
        </trans-unit>
        <trans-unit id="2c6d384388d19ed3fa790b1769ea860a0e6be8d0" translate="yes" xml:space="preserve">
          <source>The path to the new checkpoint. It is also recorded in the &lt;code&gt;checkpoints&lt;/code&gt; and &lt;code&gt;latest_checkpoint&lt;/code&gt; properties.</source>
          <target state="translated">新检查点的路径。它还记录在 &lt;code&gt;checkpoints&lt;/code&gt; 和 &lt;code&gt;latest_checkpoint&lt;/code&gt; 属性中。</target>
        </trans-unit>
        <trans-unit id="995443700e473d68dc872f5c69aa35e966bca809" translate="yes" xml:space="preserve">
          <source>The path to the new checkpoint. It is also recorded in the &lt;code&gt;checkpoints&lt;/code&gt; and &lt;code&gt;latest_checkpoint&lt;/code&gt; properties. &lt;code&gt;None&lt;/code&gt; if no checkpoint is saved.</source>
          <target state="translated">新检查点的路径。它还记录在 &lt;code&gt;checkpoints&lt;/code&gt; 和 &lt;code&gt;latest_checkpoint&lt;/code&gt; 属性中。如果未保存任何检查点，则为 &lt;code&gt;None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0ee0e0b9b831848f3b4065f81ba73b2d7593c0f8" translate="yes" xml:space="preserve">
          <source>The path to the specified file present in the data attribute of py_test or py_binary.</source>
          <target state="translated">py_test或py_binary数据属性中指定文件的路径。</target>
        </trans-unit>
        <trans-unit id="c28b39685fa19f5fe1fe963ce50d396e16dbe87f" translate="yes" xml:space="preserve">
          <source>The path to the specified file present in the data attribute of py_test or py_binary. Falls back to returning the same as get_data_files_path if it fails to detect a bazel runfiles directory.</source>
          <target state="translated">py_test或py_binary数据属性中指定文件的路径。如果未能检测到一个 bazel runfiles 目录,则返回与 get_data_files_path 相同的路径。</target>
        </trans-unit>
        <trans-unit id="87756ef48ceb2a46a88324fa1e20a23d5ed02a12" translate="yes" xml:space="preserve">
          <source>The path to which the SavedModel protocol buffer was written.</source>
          <target state="translated">SavedModel协议缓冲区的写入路径。</target>
        </trans-unit>
        <trans-unit id="8be5bab78d1909543ad9986a84ecdbc0832b8254" translate="yes" xml:space="preserve">
          <source>The path to which the SavedModel will be stored.</source>
          <target state="translated">保存SavedModel的路径。</target>
        </trans-unit>
        <trans-unit id="0dab2a06ad633869bd90e943c4184ac0e2fe54a0" translate="yes" xml:space="preserve">
          <source>The pattern follows the re2 syntax (&lt;a href=&quot;https://github.com/google/re2/wiki/Syntax&quot;&gt;https://github.com/google/re2/wiki/Syntax&lt;/a&gt;)</source>
          <target state="translated">该模式遵循re2语法（&lt;a href=&quot;https://github.com/google/re2/wiki/Syntax&quot;&gt;https://github.com/google/re2/wiki/Syntax&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="c7281e72ad08903abeb2bd587c7c0dfd0d8772e6" translate="yes" xml:space="preserve">
          <source>The pattern follows the re2 syntax (https://github.com/google/re2/wiki/Syntax)</source>
          <target state="translated">该模式遵循re2语法(https://github.com/google/re2/wiki/Syntax)</target>
        </trans-unit>
        <trans-unit id="6073374fbdf4fdacc0b93351a02133e64c1915d8" translate="yes" xml:space="preserve">
          <source>The patterns are defined as strings. Supported patterns are defined here. Note that the pattern can be a Python iteratable of string patterns.</source>
          <target state="translated">模式被定义为字符串。这里定义了支持的模式。注意,模式可以是字符串模式的Python迭代。</target>
        </trans-unit>
        <trans-unit id="5c52ca33f774b534fe80a4db87092be76893e002" translate="yes" xml:space="preserve">
          <source>The peephole implementation is based on:</source>
          <target state="translated">窥视孔的实现是基于。</target>
        </trans-unit>
        <trans-unit id="baf8710cc0a85723dd8aa2526d303f0863e63136" translate="yes" xml:space="preserve">
          <source>The performance of &lt;code&gt;LinearOperatorAdjoint&lt;/code&gt; depends on the underlying operators performance.</source>
          <target state="translated">&lt;code&gt;LinearOperatorAdjoint&lt;/code&gt; 的性能取决于基础操作员的性能。</target>
        </trans-unit>
        <trans-unit id="80d488d70fd1fbe1594d7c6c118ae3795bd1145d" translate="yes" xml:space="preserve">
          <source>The performance of &lt;code&gt;LinearOperatorBlockDiag&lt;/code&gt; on any operation is equal to the sum of the individual operators' operations.</source>
          <target state="translated">&lt;code&gt;LinearOperatorBlockDiag&lt;/code&gt; 在任何操作上的性能等于各个操作员的操作之和。</target>
        </trans-unit>
        <trans-unit id="9b2ab24a37ac3d66ce6ff090c15ce4c8f5b1d480" translate="yes" xml:space="preserve">
          <source>The performance of &lt;code&gt;LinearOperatorComposition&lt;/code&gt; on any operation is equal to the sum of the individual operators' operations.</source>
          <target state="translated">&lt;code&gt;LinearOperatorComposition&lt;/code&gt; 在任何操作上的性能等于各个操作员的操作之和。</target>
        </trans-unit>
        <trans-unit id="f94bad31467a9ea9f4b4d537674d8e77b0cf1a22" translate="yes" xml:space="preserve">
          <source>The performance of &lt;code&gt;LinearOperatorInversion&lt;/code&gt; depends on the underlying operators performance: &lt;code&gt;solve&lt;/code&gt; and &lt;code&gt;matmul&lt;/code&gt; are swapped, and determinant is inverted.</source>
          <target state="translated">&lt;code&gt;LinearOperatorInversion&lt;/code&gt; 的性能取决于基础运算符的性能：交换 &lt;code&gt;solve&lt;/code&gt; 和 &lt;code&gt;matmul&lt;/code&gt; ，并颠倒行列式。</target>
        </trans-unit>
        <trans-unit id="bc0efd32d22cd6d37162492744eb9c09073101ec" translate="yes" xml:space="preserve">
          <source>The performance of &lt;code&gt;LinearOperatorKronecker&lt;/code&gt; on any operation is equal to the sum of the individual operators' operations.</source>
          <target state="translated">&lt;code&gt;LinearOperatorKronecker&lt;/code&gt; 在任何操作上的性能等于各个操作员的操作之和。</target>
        </trans-unit>
        <trans-unit id="3adcc83502b5bf35b4db3b88505bc6feb67f24c9" translate="yes" xml:space="preserve">
          <source>The polygamma function is defined as:</source>
          <target state="translated">多角函数定义为:</target>
        </trans-unit>
        <trans-unit id="58805a4c695a0c84e281d698b93c5605de911e7d" translate="yes" xml:space="preserve">
          <source>The port the TensorFlow server is listening on.</source>
          <target state="translated">TensorFlow服务器监听的端口。</target>
        </trans-unit>
        <trans-unit id="278a092ca102f6f63192c91f543df0b76f86bf53" translate="yes" xml:space="preserve">
          <source>The position where padding or truncation happens is determined by the arguments &lt;code&gt;padding&lt;/code&gt; and &lt;code&gt;truncating&lt;/code&gt;, respectively. Pre-padding or removing values from the beginning of the sequence is the default.</source>
          <target state="translated">填充或截断发生的位置分别由参数 &lt;code&gt;padding&lt;/code&gt; 和 &lt;code&gt;truncating&lt;/code&gt; 确定。默认填充是从序列开始处填充或删除值。</target>
        </trans-unit>
        <trans-unit id="9d524ae593bfca1cec825db026e283842ef95ed3" translate="yes" xml:space="preserve">
          <source>The possible number of labels the classification task can have. If this value is not provided, it will be calculated using both predictions and labels array.</source>
          <target state="translated">分类任务可能拥有的标签数量。如果没有提供这个值,将使用预测和标签数组计算。</target>
        </trans-unit>
        <trans-unit id="8efb3a74fd166658f111b04a0c0279818ee74864" translate="yes" xml:space="preserve">
          <source>The possible number of labels the prediction task can have. This value must be provided, since a confusion matrix of dimension = [num_classes, num_classes] will be allocated.</source>
          <target state="translated">预测任务可能拥有的标签数量。必须提供这个值,因为将分配一个维数为[num_classes,num_classes]的混淆矩阵。</target>
        </trans-unit>
        <trans-unit id="3e0a01897541e4cc7358dd44c0abef35501aae05" translate="yes" xml:space="preserve">
          <source>The possible number of labels the prediction task can have. This value must be provided, since two variables with shape = [num_classes] will be allocated.</source>
          <target state="translated">预测任务可能拥有的标签数量。必须提供这个值,因为将分配两个shape=[num_classes]的变量。</target>
        </trans-unit>
        <trans-unit id="692359d83812b544d928c04fadf8b5c3c2df67a0" translate="yes" xml:space="preserve">
          <source>The possible values are: &lt;code&gt;GATE_NONE&lt;/code&gt;, &lt;code&gt;GATE_OP&lt;/code&gt;, and &lt;code&gt;GATE_GRAPH&lt;/code&gt;.</source>
          <target state="translated">可能的值为： &lt;code&gt;GATE_NONE&lt;/code&gt; ， &lt;code&gt;GATE_OP&lt;/code&gt; 和 &lt;code&gt;GATE_GRAPH&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d5d3e18deacf1c7ef93bee6d5b865603caff1a2a" translate="yes" xml:space="preserve">
          <source>The potentially support list contains a list of ops that are partially or fully supported, which is derived by simply scanning op names to check whether they can be handled without real conversion and specific parameters.</source>
          <target state="translated">潜在支持列表包含一个部分或完全支持的操作列表,它是通过简单地扫描操作名称来检查是否可以在没有真正转换和特定参数的情况下处理它们。</target>
        </trans-unit>
        <trans-unit id="7c853ad9a21d66e7bd6fdc646ae396c38787f2e0" translate="yes" xml:space="preserve">
          <source>The predicted outputs, a tensor of size &lt;code&gt;[batch_size, d0, .. dN]&lt;/code&gt; where N+1 is the total number of dimensions in &lt;code&gt;predictions&lt;/code&gt;.</source>
          <target state="translated">预测输出，大小为 &lt;code&gt;[batch_size, d0, .. dN]&lt;/code&gt; 的张量，其中N + 1是 &lt;code&gt;predictions&lt;/code&gt; 中维的总数。</target>
        </trans-unit>
        <trans-unit id="f104bcdd7d5f421504142d2c94d5f8c62a0e743f" translate="yes" xml:space="preserve">
          <source>The predicted outputs.</source>
          <target state="translated">预测产出:</target>
        </trans-unit>
        <trans-unit id="a70f131e2a7ce29596591e28a1cfededaebaff6a" translate="yes" xml:space="preserve">
          <source>The predicted values, a &lt;code&gt;Tensor&lt;/code&gt; of any shape.</source>
          <target state="translated">预测值，任何形状的 &lt;code&gt;Tensor&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="6728a30c6037094fe5c0a9ed3f50b7de76aa2e0d" translate="yes" xml:space="preserve">
          <source>The predicted values, a &lt;code&gt;Tensor&lt;/code&gt; of arbitrary dimensions. Will be cast to &lt;code&gt;bool&lt;/code&gt;.</source>
          <target state="translated">预测值，任意维度的 &lt;code&gt;Tensor&lt;/code&gt; 。将被铸成 &lt;code&gt;bool&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="71f7a8826bad533ae16d312cbce730009c206751" translate="yes" xml:space="preserve">
          <source>The predicted values.</source>
          <target state="translated">预测值;</target>
        </trans-unit>
        <trans-unit id="c4778ceab3533971f6aa86d7a9cab9cffa7b4ebb" translate="yes" xml:space="preserve">
          <source>The predicted values. Each element must be in the range &lt;code&gt;[0, 1]&lt;/code&gt;.</source>
          <target state="translated">预测值。每个元素必须在 &lt;code&gt;[0, 1]&lt;/code&gt; 范围内。</target>
        </trans-unit>
        <trans-unit id="86f018a513047301d5f31c33cb992832bb0aa35d" translate="yes" xml:space="preserve">
          <source>The predicted values. shape = &lt;code&gt;[batch_size, d0, .. dN]&lt;/code&gt;</source>
          <target state="translated">预测值。形状= &lt;code&gt;[batch_size, d0, .. dN]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="e9916875b88168e8e1d45339b98305ac670cab32" translate="yes" xml:space="preserve">
          <source>The predicted values. shape = &lt;code&gt;[batch_size, d0, .. dN]&lt;/code&gt;.</source>
          <target state="translated">预测值。shape = &lt;code&gt;[batch_size, d0, .. dN]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="350857c387e91589b30efc773f0aa8bb73f1dd8b" translate="yes" xml:space="preserve">
          <source>The prediction values.</source>
          <target state="translated">预测值。</target>
        </trans-unit>
        <trans-unit id="22221eb5e9024f9d30d6d77902385bdae34280a3" translate="yes" xml:space="preserve">
          <source>The prefix of a V1 or V2 checkpoint. Typically the result of &lt;code&gt;Saver.save()&lt;/code&gt; or that of &lt;a href=&quot;../../../train/latest_checkpoint&quot;&gt;&lt;code&gt;tf.train.latest_checkpoint()&lt;/code&gt;&lt;/a&gt;, regardless of sharded/non-sharded or V1/V2.</source>
          <target state="translated">V1或V2检查点的前缀。通常，无论分片/&lt;a href=&quot;../../../train/latest_checkpoint&quot;&gt; &lt;code&gt;tf.train.latest_checkpoint()&lt;/code&gt; &lt;/a&gt;或V1 / V2是 &lt;code&gt;Saver.save()&lt;/code&gt; 还是tf.train.latest_checkpoint（）的结果。</target>
        </trans-unit>
        <trans-unit id="aaf184bbd1e736ec9f0efcfa8ff77fd74b286f9f" translate="yes" xml:space="preserve">
          <source>The prefix of the most recent checkpoint in &lt;code&gt;directory&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;directory&lt;/code&gt; 最新检查点的前缀。</target>
        </trans-unit>
        <trans-unit id="0e79e9868436b7bc9300626a15307ccf5673bf8d" translate="yes" xml:space="preserve">
          <source>The prefix to use on all names created within the name scope.</source>
          <target state="translated">在名称范围内创建的所有名称上使用的前缀。</target>
        </trans-unit>
        <trans-unit id="43a7df5de41d0edab56a0d1dc85f3a8f7b95e198" translate="yes" xml:space="preserve">
          <source>The primary case where you need extra work to support mixed precision or float64 is when you create a new tensor, such as with &lt;a href=&quot;../../../ones&quot;&gt;&lt;code&gt;tf.ones&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../../constant&quot;&gt;&lt;code&gt;tf.constant&lt;/code&gt;&lt;/a&gt;. In such cases, you must create the tensor of the correct dtype. For example, suppose you modify the &lt;code&gt;MyDense&lt;/code&gt; layer to add a random number to the output using &lt;a href=&quot;../../../random/normal&quot;&gt;&lt;code&gt;tf.random.normal&lt;/code&gt;&lt;/a&gt;. You must pass the input dtype to &lt;a href=&quot;../../../random/normal&quot;&gt;&lt;code&gt;tf.random.normal&lt;/code&gt;&lt;/a&gt; to ensure the dtypes match.</source>
          <target state="translated">需要额外的工作来支持混合精度或float64的主要情况是在创建新的张量时，例如使用&lt;a href=&quot;../../../ones&quot;&gt; &lt;code&gt;tf.ones&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;../../../constant&quot;&gt; &lt;code&gt;tf.constant&lt;/code&gt; &lt;/a&gt;。在这种情况下，必须创建正确dtype的张量。例如，假设您修改 &lt;code&gt;MyDense&lt;/code&gt; 层以使用&lt;a href=&quot;../../../random/normal&quot;&gt; &lt;code&gt;tf.random.normal&lt;/code&gt; &lt;/a&gt;将随机数添加到输出中。您必须将输入&lt;a href=&quot;../../../random/normal&quot;&gt; &lt;code&gt;tf.random.normal&lt;/code&gt; &lt;/a&gt;传递给tf.random.normal，以确保dtypes匹配。</target>
        </trans-unit>
        <trans-unit id="70445e39cb7672ad4a6b06501270c1f13979a70f" translate="yes" xml:space="preserve">
          <source>The primary use case for this API is to put tensors in a set/dictionary. We can't put tensors in a set/dictionary as &lt;code&gt;tensor.__hash__()&lt;/code&gt; is no longer available starting Tensorflow 2.0.</source>
          <target state="translated">此API的主要用例是将张量放在集合/字典中。我们无法将张量放在集合/字典中，因为 &lt;code&gt;tensor.__hash__()&lt;/code&gt; 从Tensorflow 2.0开始不再可用。</target>
        </trans-unit>
        <trans-unit id="b0c5bb1e4f98a69939280a641cea06563cef1412" translate="yes" xml:space="preserve">
          <source>The primary use case for this API is to put variables in a set/dictionary. We can't put variables in a set/dictionary as &lt;code&gt;variable.__hash__()&lt;/code&gt; is no longer available starting Tensorflow 2.0.</source>
          <target state="translated">该API的主要用例是将变量放入集合/字典中。我们不能将变量作为 &lt;code&gt;variable.__hash__()&lt;/code&gt; 放在集合/字典中.__ hash __（）从Tensorflow 2.0开始不再可用。</target>
        </trans-unit>
        <trans-unit id="0cdbe0557abf05d8c31ba3846734c76d96d3bcc3" translate="yes" xml:space="preserve">
          <source>The primary usecase for this API is to put tensors in a set/dictionary. We can't put tensors in a set/dictionary as &lt;code&gt;tensor.__hash__()&lt;/code&gt; is no longer available starting Tensorflow 2.0.</source>
          <target state="translated">此API的主要用例是将张量放在集合/字典中。我们无法将张量放置在集合/字典中，因为 &lt;code&gt;tensor.__hash__()&lt;/code&gt; 从Tensorflow 2.0开始不再可用。</target>
        </trans-unit>
        <trans-unit id="14c5ca1e155e62f217839df947a76604aa7befe7" translate="yes" xml:space="preserve">
          <source>The primary usecase for this API is to put variables in a set/dictionary. We can't put variables in a set/dictionary as &lt;code&gt;variable.__hash__()&lt;/code&gt; is no longer available starting Tensorflow 2.0.</source>
          <target state="translated">该API的主要用例是将变量放入集合/字典中。我们不能将变量作为 &lt;code&gt;variable.__hash__()&lt;/code&gt; 放在集合/字典中.__ hash __（）从Tensorflow 2.0开始不再可用。</target>
        </trans-unit>
        <trans-unit id="bfffea38233a7f94cfc568cf11eefdfb38392417" translate="yes" xml:space="preserve">
          <source>The probability density function (pdf) is,</source>
          <target state="translated">概率密度函数(pdf)是:</target>
        </trans-unit>
        <trans-unit id="d639692ffbb24f5fe23fd0cf8f29f9ea56836d74" translate="yes" xml:space="preserve">
          <source>The probability density function (pdf) of this distribution is,</source>
          <target state="translated">这种分布的概率密度函数(pdf)是:</target>
        </trans-unit>
        <trans-unit id="c2092fccabcd3a2abdef73412d545365899001b8" translate="yes" xml:space="preserve">
          <source>The probability mass function (pmf) is,</source>
          <target state="translated">概率质量函数(pmf)为:</target>
        </trans-unit>
        <trans-unit id="331d712cc96a7cb254a89657cf4ba910cc781864" translate="yes" xml:space="preserve">
          <source>The processing of each sample contains the following steps:</source>
          <target state="translated">每个样品的处理包括以下步骤:</target>
        </trans-unit>
        <trans-unit id="b1597763a35d57db9c807276fa76fead0e6bf3d7" translate="yes" xml:space="preserve">
          <source>The processing of each sample contains the following steps: 1) standardize each sample (usually lowercasing + punctuation stripping) 2) split each sample into substrings (usually words) 3) recombine substrings into tokens (usually ngrams) 4) index tokens (associate a unique int value with each token) 5) transform each sample using this index, either into a vector of ints or a dense float vector.</source>
          <target state="translated">每个样本的处理包含以下步骤。1)将每个样本标准化(通常是小写+标点符号剥离)2)将每个样本分割成子串(通常是单词)3)将子串重新组合成令牌(通常是ngrams)4)为令牌编制索引(将一个独特的int值与每个令牌联系起来)5)使用该索引将每个样本转化为一个int向量或一个密集的float向量。</target>
        </trans-unit>
        <trans-unit id="bc3e043e3089ab9c2cc2954b166a1bf6364579df" translate="yes" xml:space="preserve">
          <source>The profiler server will exit when the process finishes. The service is defined in tensorflow/core/profiler/profiler_service.proto.</source>
          <target state="translated">进程结束后,profiler服务器将退出。该服务在 tensorflow/core/profiler/profiler_service.proto 中定义。</target>
        </trans-unit>
        <trans-unit id="e67a4435185658688f7c2b52360cda7ea81a0d75" translate="yes" xml:space="preserve">
          <source>The profiler session will be stopped and profile results can be saved.</source>
          <target state="translated">剖析师会话将被停止,剖析结果可以被保存。</target>
        </trans-unit>
        <trans-unit id="ddcf3e7dfeb12410a9d6122f19a6f9f6186a333b" translate="yes" xml:space="preserve">
          <source>The protocol TensorFlow used to communicate between nodes. Defaults to 'grpc'.</source>
          <target state="translated">节点之间用于通信的TensorFlow协议。默认为'grpc'。</target>
        </trans-unit>
        <trans-unit id="5e9c7a33590d968d1c4d2adb1f14e30ca9d6e013" translate="yes" xml:space="preserve">
          <source>The provided generator can be finite in which case the class will throw a &lt;code&gt;StopIteration&lt;/code&gt; exception.</source>
          <target state="translated">提供的生成器可以是有限的，在这种情况下，该类将抛出 &lt;code&gt;StopIteration&lt;/code&gt; 异常。</target>
        </trans-unit>
        <trans-unit id="8129f9559f62303b98c598e584d99bb1465d425d" translate="yes" xml:space="preserve">
          <source>The provided value can be a python boolean, a scalar boolean Tensor, or or a callable providing such a value; if a callable is passed it will be invoked on-demand to determine whether summary writing will occur.</source>
          <target state="translated">所提供的值可以是一个python布尔值、一个标量布尔值Tensor,或者或者是一个提供这种值的callable;如果传递了一个callable,它将被按需调用,以确定是否会发生摘要写入。</target>
        </trans-unit>
        <trans-unit id="d84a7589708b247ba11ef0a6241d0276fd1c0415" translate="yes" xml:space="preserve">
          <source>The pseudo-inverse of a matrix &lt;code&gt;A&lt;/code&gt;, is defined as: 'the matrix that 'solves' [the least-squares problem] &lt;code&gt;A @ x = b&lt;/code&gt;,' i.e., if &lt;code&gt;x_hat&lt;/code&gt; is a solution, then &lt;code&gt;A_pinv&lt;/code&gt; is the matrix such that &lt;code&gt;x_hat = A_pinv @ b&lt;/code&gt;. It can be shown that if &lt;code&gt;U @ Sigma @ V.T = A&lt;/code&gt; is the singular value decomposition of &lt;code&gt;A&lt;/code&gt;, then &lt;code&gt;A_pinv = V @ inv(Sigma) U^T&lt;/code&gt;. [(Strang, 1980)][1]</source>
          <target state="translated">矩阵 &lt;code&gt;A&lt;/code&gt; 的伪逆定义为：&amp;ldquo;可解决[最小二乘问题] &lt;code&gt;A @ x = b&lt;/code&gt; 的矩阵&amp;rdquo;，即，如果 &lt;code&gt;x_hat&lt;/code&gt; 是解，则 &lt;code&gt;A_pinv&lt;/code&gt; 是矩阵，使得 &lt;code&gt;x_hat = A_pinv @ b&lt;/code&gt; 。可以证明，如果 &lt;code&gt;U @ Sigma @ V.T = A&lt;/code&gt; 是的奇异值分解 &lt;code&gt;A&lt;/code&gt; ，然后 &lt;code&gt;A_pinv = V @ inv(Sigma) U^T&lt;/code&gt; 。 [（Strang，1980年）] [1]</target>
        </trans-unit>
        <trans-unit id="9b22641be848196f7d9c02ba6fc3610602324007" translate="yes" xml:space="preserve">
          <source>The purpose of this function is to allow users of existing layers to slowly transition to Keras layers API without breaking existing functionality.</source>
          <target state="translated">这个功能的目的是让现有图层的用户慢慢过渡到Keras图层API,而不破坏现有的功能。</target>
        </trans-unit>
        <trans-unit id="d088d5e5d63d8b56ec540e3afc689671a1995e6f" translate="yes" xml:space="preserve">
          <source>The purpose of this scope is to allow users of existing layers to slowly transition to a Keras layers API without breaking existing functionality.</source>
          <target state="translated">这个范围的目的是允许现有层的用户慢慢过渡到Keras层API,而不破坏现有功能。</target>
        </trans-unit>
        <trans-unit id="7c5cb70c5a3b792465bd5ce774c8ea2af3bb85e3" translate="yes" xml:space="preserve">
          <source>The python function &lt;code&gt;fn&lt;/code&gt; will be called once with symbolic arguments specified in the &lt;code&gt;signature&lt;/code&gt;, traced, and turned into a graph function. Any variables created by &lt;code&gt;fn&lt;/code&gt; will be owned by the object returned by &lt;code&gt;wrap_function&lt;/code&gt;. The resulting graph function can be called with tensors which match the signature.</source>
          <target state="translated">python函数 &lt;code&gt;fn&lt;/code&gt; 将使用 &lt;code&gt;signature&lt;/code&gt; 指定的符号参数调用一次，然后将其跟踪并转换为图函数。 &lt;code&gt;fn&lt;/code&gt; 创建的任何变量将归 &lt;code&gt;wrap_function&lt;/code&gt; 返回的对象所有。可以使用与签名匹配的张量调用生成的图形函数。</target>
        </trans-unit>
        <trans-unit id="d03bcbea9ec248eeaa20713d75d526a9879de1ff" translate="yes" xml:space="preserve">
          <source>The quantity to be monitored needs to be available in &lt;code&gt;logs&lt;/code&gt; dict. To make it so, pass the loss or metrics at &lt;code&gt;model.compile()&lt;/code&gt;.</source>
          <target state="translated">需要在 &lt;code&gt;logs&lt;/code&gt; 字典中提供要监视的数量。为此，请将损失或指标传递给 &lt;code&gt;model.compile()&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4d8a7f5ae13c0deb4cd4a8012ab18a5fdec064e5" translate="yes" xml:space="preserve">
          <source>The queue reference, i.e. the output of the queue op.</source>
          <target state="translated">队列参考,即队列运算的输出。</target>
        </trans-unit>
        <trans-unit id="aaadc5a70ca763680cae28ccccd312176ae7929f" translate="yes" xml:space="preserve">
          <source>The ragged rank for the &lt;code&gt;RaggedTensor&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;RaggedTensor&lt;/code&gt; 的衣衫rank等级</target>
        </trans-unit>
        <trans-unit id="cf2591fdff5d530b2526efe01b41c14d72949b69" translate="yes" xml:space="preserve">
          <source>The random integers are slightly biased unless &lt;code&gt;maxval - minval&lt;/code&gt; is an exact power of two. The bias is small for values of &lt;code&gt;maxval - minval&lt;/code&gt; significantly smaller than the range of the output (either &lt;code&gt;2^32&lt;/code&gt; or &lt;code&gt;2^64&lt;/code&gt;).</source>
          <target state="translated">除非 &lt;code&gt;maxval - minval&lt;/code&gt; 是2的精确乘方，否则随机整数会略有偏差。对于 &lt;code&gt;maxval - minval&lt;/code&gt; 值明显小于输出范围（ &lt;code&gt;2^32&lt;/code&gt; 或 &lt;code&gt;2^64&lt;/code&gt; ）的偏差很小。</target>
        </trans-unit>
        <trans-unit id="b20ff491ff5822812275688511b761f22476aa65" translate="yes" xml:space="preserve">
          <source>The range of pixel values for the output image might be slightly different from the range for the input image because of limited numerical precision. To guarantee an output range, for example &lt;code&gt;[0.0, 1.0]&lt;/code&gt;, apply &lt;a href=&quot;../../../clip_by_value&quot;&gt;&lt;code&gt;tf.clip_by_value&lt;/code&gt;&lt;/a&gt; to the output.</source>
          <target state="translated">由于数值精度有限，输出图像的像素值范围可能与输入图像的范围略有不同。为了保证输出范围，例如 &lt;code&gt;[0.0, 1.0]&lt;/code&gt; &lt;a href=&quot;../../../clip_by_value&quot;&gt; &lt;code&gt;tf.clip_by_value&lt;/code&gt; &lt;/a&gt; ]，请将tf.clip_by_value应用于输出。</target>
        </trans-unit>
        <trans-unit id="e8fedfebee027b536f10130b6d675ab2045757c3" translate="yes" xml:space="preserve">
          <source>The range of pixel values for the output image might be slightly different from the range for the input image because of limited numerical precision. To guarantee an output range, for example &lt;code&gt;[0.0, 1.0]&lt;/code&gt;, apply &lt;a href=&quot;../clip_by_value&quot;&gt;&lt;code&gt;tf.clip_by_value&lt;/code&gt;&lt;/a&gt; to the output.</source>
          <target state="translated">由于数值精度有限，输出图像的像素值范围可能与输入图像的范围略有不同。为了保证输出范围，例如 &lt;code&gt;[0.0, 1.0]&lt;/code&gt; &lt;a href=&quot;../clip_by_value&quot;&gt; &lt;code&gt;tf.clip_by_value&lt;/code&gt; &lt;/a&gt; ]，请将tf.clip_by_value应用于输出。</target>
        </trans-unit>
        <trans-unit id="6294e1714e1b2b241bb1943b70eb74fc15a98222" translate="yes" xml:space="preserve">
          <source>The reason we get 'A2' instead 'A1' on the second call of &lt;a href=&quot;uniform&quot;&gt;&lt;code&gt;tf.random.uniform&lt;/code&gt;&lt;/a&gt; above is because the same &lt;a href=&quot;uniform&quot;&gt;&lt;code&gt;tf.random.uniform&lt;/code&gt;&lt;/a&gt; kernel (i.e. internal representation) is used by TensorFlow for all calls of it with the same arguments, and the kernel maintains an internal counter which is incremented every time it is executed, generating different results.</source>
          <target state="translated">我们在上面的第二次&lt;a href=&quot;uniform&quot;&gt; &lt;code&gt;tf.random.uniform&lt;/code&gt; &lt;/a&gt;调用中获得'A2'而不是'A1'的原因是因为TensorFlow使用相同的&lt;a href=&quot;uniform&quot;&gt; &lt;code&gt;tf.random.uniform&lt;/code&gt; &lt;/a&gt;内核（即内部表示形式）对其具有相同参数的所有调用，内核维护一个内部计数器，该计数器在每次执行时都会递增，从而产生不同的结果。</target>
        </trans-unit>
        <trans-unit id="afabff9ee6287914682ddea57965fb5330ae1825" translate="yes" xml:space="preserve">
          <source>The reason we get 'A2' instead 'A1' on the second call of &lt;a href=&quot;uniform&quot;&gt;&lt;code&gt;tf.random.uniform&lt;/code&gt;&lt;/a&gt; above is because the same &lt;a href=&quot;uniform&quot;&gt;&lt;code&gt;tf.random.uniform&lt;/code&gt;&lt;/a&gt; kernel (i.e. internel representation) is used by TensorFlow for all calls of it with the same arguments, and the kernel maintains an internal counter which is incremented every time it is executed, generating different results.</source>
          <target state="translated">我们在上面的第二次&lt;a href=&quot;uniform&quot;&gt; &lt;code&gt;tf.random.uniform&lt;/code&gt; &lt;/a&gt;调用中获得'A2'而不是'A1'的原因是因为TensorFlow使用相同的&lt;a href=&quot;uniform&quot;&gt; &lt;code&gt;tf.random.uniform&lt;/code&gt; &lt;/a&gt;内核（即内部表示）对所有具有相同参数的调用，内核维护一个内部计数器，该计数器在每次执行时都会递增，从而产生不同的结果。</target>
        </trans-unit>
        <trans-unit id="466c31e39da78ea430b94cdcadf9fd7405e0f0cf" translate="yes" xml:space="preserve">
          <source>The reason we get 'A2' instead 'A1' on the second call of &lt;a href=&quot;uniform&quot;&gt;&lt;code&gt;tf.random.uniform&lt;/code&gt;&lt;/a&gt; above is because the secand call uses a different operation seed.</source>
          <target state="translated">我们在上面第二次&lt;a href=&quot;uniform&quot;&gt; &lt;code&gt;tf.random.uniform&lt;/code&gt; &lt;/a&gt;调用中获得&amp;ldquo; A2&amp;rdquo;而不是&amp;ldquo; A1&amp;rdquo;的原因是因为secand调用使用了不同的操作种子。</target>
        </trans-unit>
        <trans-unit id="784df8a2c4aec00572bf8c70feb9ae2a52a2afdd" translate="yes" xml:space="preserve">
          <source>The reason we get 'A2' instead 'A1' on the second call of &lt;a href=&quot;uniform&quot;&gt;&lt;code&gt;tf.random.uniform&lt;/code&gt;&lt;/a&gt; above is because the second call uses a different operation seed.</source>
          <target state="translated">之所以在上述&lt;a href=&quot;uniform&quot;&gt; &lt;code&gt;tf.random.uniform&lt;/code&gt; &lt;/a&gt;的第二次调用中获得&amp;ldquo; A2&amp;rdquo;而不是&amp;ldquo; A1&amp;rdquo; ，是因为第二次调用使用了不同的操作种子。</target>
        </trans-unit>
        <trans-unit id="1280cfc6968446075563e60ad83288d637db87ba" translate="yes" xml:space="preserve">
          <source>The reconstruct one or more matrices from their LU decomposition(s).</source>
          <target state="translated">从它们的LU分解中重建一个或多个矩阵。</target>
        </trans-unit>
        <trans-unit id="39397157700bab61929a7d8ce8314e15144efa4b" translate="yes" xml:space="preserve">
          <source>The reduced SparseTensor.</source>
          <target state="translated">减少的SparseTensor。</target>
        </trans-unit>
        <trans-unit id="162a81b2ee639009df98d0138880fb964fcbc8fc" translate="yes" xml:space="preserve">
          <source>The reduced Tensor or the reduced SparseTensor if &lt;code&gt;output_is_sparse&lt;/code&gt; is True.</source>
          <target state="translated">如果 &lt;code&gt;output_is_sparse&lt;/code&gt; 为True，则缩减的Tensor或缩减的SparseTensor 。</target>
        </trans-unit>
        <trans-unit id="1e34999a75a17ec65b85f93cb20fe03874e9d9e8" translate="yes" xml:space="preserve">
          <source>The reduced Tensor.</source>
          <target state="translated">减少的Tensor。</target>
        </trans-unit>
        <trans-unit id="fa52820210f038254236a33cd72deaaa4f2921cc" translate="yes" xml:space="preserve">
          <source>The reduced tensor (number of nonzero values).</source>
          <target state="translated">减少的张量(非零值的数量)。</target>
        </trans-unit>
        <trans-unit id="5089f15002ac696c728750d278b7a2bb4b8292fd" translate="yes" xml:space="preserve">
          <source>The reduced tensor, of the same dtype as the input_tensor.</source>
          <target state="translated">缩小的张量,与输入张量的d类型相同。</target>
        </trans-unit>
        <trans-unit id="db036397ce15010ca38b7140e4ed7771da72f4bd" translate="yes" xml:space="preserve">
          <source>The reduced tensor, of the same dtype as the input_tensor. Note, for &lt;code&gt;complex64&lt;/code&gt; or &lt;code&gt;complex128&lt;/code&gt; input, the returned &lt;code&gt;Tensor&lt;/code&gt; will be of type &lt;code&gt;float32&lt;/code&gt; or &lt;code&gt;float64&lt;/code&gt;, respectively.</source>
          <target state="translated">缩减后的张量，其dtype与input_tensor相同。注意，对于 &lt;code&gt;complex64&lt;/code&gt; 或 &lt;code&gt;complex128&lt;/code&gt; 输入，返回的 &lt;code&gt;Tensor&lt;/code&gt; 将分别为 &lt;code&gt;float32&lt;/code&gt; 或 &lt;code&gt;float64&lt;/code&gt; 类型。</target>
        </trans-unit>
        <trans-unit id="377005a84a788fcaae09139342bfe31ee6c3e407" translate="yes" xml:space="preserve">
          <source>The reduced tensor.</source>
          <target state="translated">缩小的张量。</target>
        </trans-unit>
        <trans-unit id="4ca20066ecafb2f76458db7f5aae15b94f4599bf" translate="yes" xml:space="preserve">
          <source>The reduction to apply to the shard losses.</source>
          <target state="translated">适用于碎片损失的减少。</target>
        </trans-unit>
        <trans-unit id="32126f130f539d8b348c3776e0d288e0eb3023ae" translate="yes" xml:space="preserve">
          <source>The reference to the TensorArray.</source>
          <target state="translated">对TensorArray的引用。</target>
        </trans-unit>
        <trans-unit id="bec5f1d035188c5f615977438d1fd8d80cd736d2" translate="yes" xml:space="preserve">
          <source>The regular expressions we want to match against str. See &quot;Notes&quot; above for detailed notes on how this is interpreted.</source>
          <target state="translated">我们要与str.A匹配的正则表达式。关于如何解释的详细说明请参见上面的 &quot;注释&quot;。</target>
        </trans-unit>
        <trans-unit id="626421317b8daa7c4481578d7b1400cbc817027c" translate="yes" xml:space="preserve">
          <source>The regularized incomplete beta integral is defined as:</source>
          <target state="translated">正则化不完全β积分定义为:</target>
        </trans-unit>
        <trans-unit id="6b4ae5690943cd94c52d8b3b53e424b70c0a6f84" translate="yes" xml:space="preserve">
          <source>The remapping tensors can be generated using the GenerateVocabRemapping op.</source>
          <target state="translated">重映射时序可以使用GenerateVocabRemapping操作生成。</target>
        </trans-unit>
        <trans-unit id="6f76fd4e0a09c22f0b1432236fcf10360b759293" translate="yes" xml:space="preserve">
          <source>The remappings are 1-D tensors with the following properties:</source>
          <target state="translated">余下的是具有以下性质的1-D时子。</target>
        </trans-unit>
        <trans-unit id="4eb549ec0edb51386556b04bcb293e63f29c79c8" translate="yes" xml:space="preserve">
          <source>The replacement character codepoint to be used in place of any invalid input when &lt;code&gt;errors='replace'&lt;/code&gt;. Any valid unicode codepoint may be used. The default value is the default unicode replacement character which is 0xFFFD (U+65533).</source>
          <target state="translated">当 &lt;code&gt;errors='replace'&lt;/code&gt; 时，替换字符代码点将用于代替任何无效输入。可以使用任何有效的unicode代码点。默认值为默认的Unicode替换字符，为0xFFFD（U + 65533）。</target>
        </trans-unit>
        <trans-unit id="dc58567cf18ed1da38d660adbff7112eaf4a0c43" translate="yes" xml:space="preserve">
          <source>The replacement codepoint to be used in place of invalid substrings in &lt;code&gt;input&lt;/code&gt; when &lt;code&gt;errors='replace'&lt;/code&gt;.</source>
          <target state="translated">当 &lt;code&gt;errors='replace'&lt;/code&gt; 时，将使用替换代码点代替 &lt;code&gt;input&lt;/code&gt; 的无效子字符串。</target>
        </trans-unit>
        <trans-unit id="5861bd77b8284cbd383ce9fa80004e5915d9c8c5" translate="yes" xml:space="preserve">
          <source>The replacement codepoint to be used in place of invalid substrings in &lt;code&gt;input&lt;/code&gt; when &lt;code&gt;errors='replace'&lt;/code&gt;; and in place of C0 control characters in &lt;code&gt;input&lt;/code&gt; when &lt;code&gt;replace_control_characters=True&lt;/code&gt;.</source>
          <target state="translated">当 &lt;code&gt;errors='replace'&lt;/code&gt; 时，替换代码点将用于代替 &lt;code&gt;input&lt;/code&gt; 的无效子字符串；并在 &lt;code&gt;replace_control_characters=True&lt;/code&gt; 时代替 &lt;code&gt;input&lt;/code&gt; 的C0控制字符。</target>
        </trans-unit>
        <trans-unit id="08f7982eae76c3d1b8d5c546c5c6087586c14c26" translate="yes" xml:space="preserve">
          <source>The request does not have valid authentication credentials.</source>
          <target state="translated">该请求没有有效的认证凭证。</target>
        </trans-unit>
        <trans-unit id="11fac311a887cb1b94b956ea27985cb8f91c9eab" translate="yes" xml:space="preserve">
          <source>The requirements to use the cuDNN implementation are:</source>
          <target state="translated">使用cuDNN实施的要求是:</target>
        </trans-unit>
        <trans-unit id="b7f944aa478f301b9d7d37d3ac6e02aa0ab614e1" translate="yes" xml:space="preserve">
          <source>The rescaling is applied both during training and inference.</source>
          <target state="translated">在训练和推理过程中都会应用重新缩放。</target>
        </trans-unit>
        <trans-unit id="e10dd1c4bdea08780ec12768ba2f0c0cc6e6418e" translate="yes" xml:space="preserve">
          <source>The resizing Ops accept input images as tensors of several types. They always output resized images as float32 tensors.</source>
          <target state="translated">调整大小的操作接受输入图像为几种类型的 tensors。它们总是以float32 tensors输出调整后的图像。</target>
        </trans-unit>
        <trans-unit id="5f32138dd7f669a33bea7319cf66264923452d20" translate="yes" xml:space="preserve">
          <source>The restored checkpoint path if the lastest checkpoint is found and restored. Otherwise None.</source>
          <target state="translated">如果找到并恢复了最新的检查点,则恢复的检查点路径。否则,则为 &quot;无&quot;。</target>
        </trans-unit>
        <trans-unit id="d06ff612c69fb3df1f3d794bb785e13f8da40620" translate="yes" xml:space="preserve">
          <source>The result is a 4-D tensor of shape &lt;code&gt;[batch_size, glimpse_height, glimpse_width, channels]&lt;/code&gt;. The channels and batch dimensions are the same as that of the input tensor. The height and width of the output windows are specified in the &lt;code&gt;size&lt;/code&gt; parameter.</source>
          <target state="translated">结果是形状为 &lt;code&gt;[batch_size, glimpse_height, glimpse_width, channels]&lt;/code&gt; 的4D张量。通道和批次尺寸与输入张量相同。输出窗口的高度和宽度在 &lt;code&gt;size&lt;/code&gt; 参数中指定。</target>
        </trans-unit>
        <trans-unit id="af8e15d374bd08a8ea2bebed96d25f27af99cc58" translate="yes" xml:space="preserve">
          <source>The result is a 4D tensor which is indexed by batch, row, and column. &lt;code&gt;output[i, x, y]&lt;/code&gt; contains a flattened patch of size &lt;code&gt;sizes[1], sizes[2]&lt;/code&gt; which is taken from the input starting at &lt;code&gt;images[i, x*strides[1], y*strides[2]]&lt;/code&gt;.</source>
          <target state="translated">结果是一个4D张量，该张量按批，行和列索引。 &lt;code&gt;output[i, x, y]&lt;/code&gt; 包含一个大小为size &lt;code&gt;sizes[1], sizes[2]&lt;/code&gt; size [2]的扁平化补丁，该补丁从输入开始，从 &lt;code&gt;images[i, x*strides[1], y*strides[2]]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="3408db8eec06fef64e88ca8c2438893f2827e593" translate="yes" xml:space="preserve">
          <source>The result is a [..., M+1, M] matrix with [..., 0,:] containing the eigenvalues, and subsequent [...,1:, :] containing the eigenvectors. The eigenvalues are sorted in non-decreasing order.</source>
          <target state="translated">结果是一个[...,M+1,M]矩阵,其中[...,0,:]包含特征值,随后的[...,1:,:]包含特征向量。特征值按非递减顺序排列。</target>
        </trans-unit>
        <trans-unit id="91e981525729cfb7323f601fab3d959548453e14" translate="yes" xml:space="preserve">
          <source>The result is not a global index to the entire &lt;code&gt;Tensor&lt;/code&gt;, but rather just the index in the last dimension.</source>
          <target state="translated">结果不是整个 &lt;code&gt;Tensor&lt;/code&gt; 的全局索引，而只是最后一个维度的索引。</target>
        </trans-unit>
        <trans-unit id="cc0fc847805d05865dd1fc80479599ef10e030da" translate="yes" xml:space="preserve">
          <source>The result of calling parse_example on these examples will produce a dictionary with entries for &quot;ids&quot; and &quot;values&quot;. Passing those two objects to this function along with vocab_size=6, will produce a &lt;code&gt;SparseTensor&lt;/code&gt; that sparsely represents all three instances. Namely, the &lt;code&gt;indices&lt;/code&gt; property will contain the coordinates of the non-zero entries in the feature matrix (the first dimension is the row number in the matrix, i.e., the index within the batch, and the second dimension is the column number, i.e., the feature id); &lt;code&gt;values&lt;/code&gt; will contain the actual values. &lt;code&gt;shape&lt;/code&gt; will be the shape of the original matrix, i.e., (3, 6). For our example above, the output will be equal to:</source>
          <target state="translated">在这些示例上调用parse_example的结果将产生一个字典，其中包含&amp;ldquo; ids&amp;rdquo;和&amp;ldquo; values&amp;rdquo;的条目。将这两个对象与vocab_size = 6一起传递给此函数，将产生一个稀疏表示所有三个实例的 &lt;code&gt;SparseTensor&lt;/code&gt; 。也就是说， &lt;code&gt;indices&lt;/code&gt; 属性将包含特征矩阵中非零条目的坐标（第一维是矩阵中的行号，即批内的索引，第二维是列号，即，功能ID）； &lt;code&gt;values&lt;/code&gt; 将包含实际值。 &lt;code&gt;shape&lt;/code&gt; 将是原始矩阵的形状，即（3，6）。对于上面的示例，输出将等于：</target>
        </trans-unit>
        <trans-unit id="c532f85716ab7d9d5ac1221540a16a00cf14d0f8" translate="yes" xml:space="preserve">
          <source>The result of one inference step, typically the output of calling the &lt;code&gt;Model&lt;/code&gt; on data.</source>
          <target state="translated">一个推断步骤的结果，通常是对数据调用 &lt;code&gt;Model&lt;/code&gt; 的输出。</target>
        </trans-unit>
        <trans-unit id="b712de5339fc3e663594598e87ccb0f331d98d0d" translate="yes" xml:space="preserve">
          <source>The result of the elementwise &lt;code&gt;!=&lt;/code&gt; operation, or &lt;code&gt;True&lt;/code&gt; if the arguments are not broadcast-compatible.</source>
          <target state="translated">按元素 &lt;code&gt;!=&lt;/code&gt; 操作的结果；如果参数与广播不兼容，则返回 &lt;code&gt;True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="65bf62ac9a12a3ed583f3696b9ec4dfdd36908c0" translate="yes" xml:space="preserve">
          <source>The result of the elementwise &lt;code&gt;+&lt;/code&gt; operation.</source>
          <target state="translated">elementwise &lt;code&gt;+&lt;/code&gt; 运算的结果。</target>
        </trans-unit>
        <trans-unit id="4a0379f511e04f35dbfdf4e95a15f796515177e5" translate="yes" xml:space="preserve">
          <source>The result of the elementwise &lt;code&gt;==&lt;/code&gt; operation, or &lt;code&gt;False&lt;/code&gt; if the arguments are not broadcast-compatible.</source>
          <target state="translated">elementwise &lt;code&gt;==&lt;/code&gt; 操作的结果，如果参数与广播不兼容，则为 &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="a7181bf7e73106c9b2b97d9fffba14ef65fbf1f8" translate="yes" xml:space="preserve">
          <source>The result of this op should be passed through a &lt;code&gt;sparse_to_dense&lt;/code&gt; operation, then added to the logits of the sampled classes. This removes the contradictory effect of accidentally sampling the true target classes as noise classes for the same example.</source>
          <target state="translated">此操作的结果应通过 &lt;code&gt;sparse_to_dense&lt;/code&gt; 操作传递，然后添加到采样类的logit中。对于同一示例，这消除了意外采样真实目标类别作为噪声类别的矛盾影响。</target>
        </trans-unit>
        <trans-unit id="95bce95270c24fcef8ff10f45294119d2cbaf8aa" translate="yes" xml:space="preserve">
          <source>The result will be biased towards the initial value of the variable.</source>
          <target state="translated">结果将偏向于变量的初始值。</target>
        </trans-unit>
        <trans-unit id="01298564c21aafdba5ad8531a719b0b62528e9fe" translate="yes" xml:space="preserve">
          <source>The result will have those bits set, that are different in &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. The computation is performed on the underlying representations of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">结果将设置那些在 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 中不同的位。对 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 的基本表示进行计算。</target>
        </trans-unit>
        <trans-unit id="685b62e5962fffb0313c314fe8766a83e91dc7ce" translate="yes" xml:space="preserve">
          <source>The result will have those bits set, that are set in &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; or both. The computation is performed on the underlying representations of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">结果将设置为 &lt;code&gt;x&lt;/code&gt; ， &lt;code&gt;y&lt;/code&gt; 或两者都设置的那些位。对 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 的基本表示进行计算。</target>
        </trans-unit>
        <trans-unit id="b74f37adc87a210e3154c60f498551da0c0a3084" translate="yes" xml:space="preserve">
          <source>The result will have those bits set, that are set in both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. The computation is performed on the underlying representations of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">结果将设置那些在 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 中都设置的位。对 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 的基本表示进行计算。</target>
        </trans-unit>
        <trans-unit id="a454b218545175a81109283a1b62827f2751cc1d" translate="yes" xml:space="preserve">
          <source>The resulting &lt;code&gt;Tensor&lt;/code&gt; of parsing a single &lt;code&gt;SequenceExample&lt;/code&gt; or &lt;code&gt;Example&lt;/code&gt; has a static &lt;code&gt;shape&lt;/code&gt; of &lt;code&gt;[None] + shape&lt;/code&gt; and the specified &lt;code&gt;dtype&lt;/code&gt;. The resulting &lt;code&gt;Tensor&lt;/code&gt; of parsing a &lt;code&gt;batch_size&lt;/code&gt; many &lt;code&gt;Example&lt;/code&gt;s has a static &lt;code&gt;shape&lt;/code&gt; of &lt;code&gt;[batch_size, None] + shape&lt;/code&gt; and the specified &lt;code&gt;dtype&lt;/code&gt;. The entries in the &lt;code&gt;batch&lt;/code&gt; from different &lt;code&gt;Examples&lt;/code&gt; will be padded with &lt;code&gt;default_value&lt;/code&gt; to the maximum length present in the &lt;code&gt;batch&lt;/code&gt;.</source>
          <target state="translated">将得到的 &lt;code&gt;Tensor&lt;/code&gt; 解析单个的 &lt;code&gt;SequenceExample&lt;/code&gt; 或 &lt;code&gt;Example&lt;/code&gt; 有一个静态的 &lt;code&gt;shape&lt;/code&gt; 的 &lt;code&gt;[None] + shape&lt;/code&gt; 和指定的 &lt;code&gt;dtype&lt;/code&gt; 。将得到的 &lt;code&gt;Tensor&lt;/code&gt; 解析的 &lt;code&gt;batch_size&lt;/code&gt; 许多 &lt;code&gt;Example&lt;/code&gt; S具有一个静态 &lt;code&gt;shape&lt;/code&gt; 的 &lt;code&gt;[batch_size, None] + shape&lt;/code&gt; 和指定的 &lt;code&gt;dtype&lt;/code&gt; 。来自不同 &lt;code&gt;Examples&lt;/code&gt; 的 &lt;code&gt;batch&lt;/code&gt; 的条目将使用 &lt;code&gt;default_value&lt;/code&gt; 填充为 &lt;code&gt;batch&lt;/code&gt; 存在的最大长度。</target>
        </trans-unit>
        <trans-unit id="3a538e992c4e715c1f91b7c40110e2510e29c989" translate="yes" xml:space="preserve">
          <source>The resulting SavedModel is then servable with an input named &quot;x&quot;, its value having any shape and dtype float32.</source>
          <target state="translated">然后,生成的SavedModel可以使用名为 &quot;x &quot;的输入进行服务,它的值具有任何形状和dtype float32。</target>
        </trans-unit>
        <trans-unit id="b71ce2b54de94b7e8ecf2e651366ba952c5ce3a3" translate="yes" xml:space="preserve">
          <source>The resulting dataset is similar to the &lt;code&gt;InterleaveDataset&lt;/code&gt;, except that the dataset will fetch records from the interleaved datasets in parallel.</source>
          <target state="translated">生成的数据集与 &lt;code&gt;InterleaveDataset&lt;/code&gt; 相似，不同的是，该数据集将并行地从交错的数据集中获取记录。</target>
        </trans-unit>
        <trans-unit id="5d8659c50ab2dced91f800b38d49965e49b25d1c" translate="yes" xml:space="preserve">
          <source>The resulting dataset is similar to the &lt;code&gt;InterleaveDataset&lt;/code&gt;, with the exception that if retrieving the next value from a dataset would cause the requester to block, it will skip that input dataset. This dataset is especially useful when loading data from a variable-latency datastores (e.g. HDFS, GCS), as it allows the training step to proceed so long as some data is available.</source>
          <target state="translated">生成的数据集类似于 &lt;code&gt;InterleaveDataset&lt;/code&gt; ，不同之处在于，如果从数据集中检索下一个值将导致请求者阻塞，则它将跳过该输入数据集。从可变延迟数据存储区（例如HDFS，GCS）加载数据时，此数据集特别有用，因为只要有一些数据可用，它就可以继续进行训练。</target>
        </trans-unit>
        <trans-unit id="fd835a476d97294a9a531a06c729ae1427276dc8" translate="yes" xml:space="preserve">
          <source>The resulting function is assumed stateful and will never be optimized.</source>
          <target state="translated">由此产生的函数被认为是有状态的,永远不会被优化。</target>
        </trans-unit>
        <trans-unit id="1f53b61193a80c029c5a448efed06c1a8f6588c5" translate="yes" xml:space="preserve">
          <source>The resulting output shape when using the &quot;same&quot; padding option is: &lt;code&gt;output_shape = input_shape / strides&lt;/code&gt;</source>
          <target state="translated">使用&amp;ldquo;相同&amp;rdquo;填充选项时的最终输出形状为： &lt;code&gt;output_shape = input_shape / strides&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="9ef25e64d2e423f7de1f3a02ba7a25085c38c8a1" translate="yes" xml:space="preserve">
          <source>The resulting tensor is populated with values of type &lt;code&gt;dtype&lt;/code&gt;, as specified by arguments &lt;code&gt;value&lt;/code&gt; and (optionally) &lt;code&gt;shape&lt;/code&gt; (see examples below).</source>
          <target state="translated">生成的张量将填充 &lt;code&gt;dtype&lt;/code&gt; 类型的值，该值由参数 &lt;code&gt;value&lt;/code&gt; 和（可选） &lt;code&gt;shape&lt;/code&gt; 指定（请参见下面的示例）。</target>
        </trans-unit>
        <trans-unit id="0b674f3072695f2111704701dbc3947bfb383f3f" translate="yes" xml:space="preserve">
          <source>The resulting tensor is populated with values of type &lt;code&gt;dtype&lt;/code&gt;, as specified by arguments &lt;code&gt;value&lt;/code&gt; following the desired &lt;code&gt;shape&lt;/code&gt; of the new tensor (see examples below).</source>
          <target state="translated">生成的张量将填充 &lt;code&gt;dtype&lt;/code&gt; 类型的值，该值由参数 &lt;code&gt;value&lt;/code&gt; 遵循新张量的所需 &lt;code&gt;shape&lt;/code&gt; 指定（请参见下面的示例）。</target>
        </trans-unit>
        <trans-unit id="ef74e8f16df07a307babda0cfa120db0e980020d" translate="yes" xml:space="preserve">
          <source>The resulting tensor would look like this:</source>
          <target state="translated">由此产生的张量会是这样的。</target>
        </trans-unit>
        <trans-unit id="381b1e6543678db4ed459c1cba156ecc8ca01172" translate="yes" xml:space="preserve">
          <source>The resulting update to ref would look like this:</source>
          <target state="translated">由此产生的ref更新是这样的。</target>
        </trans-unit>
        <trans-unit id="cbbd169c06c5469910e771887e06b5ed0c01b738" translate="yes" xml:space="preserve">
          <source>The resulting update to v would look like this:</source>
          <target state="translated">结果v的更新是这样的。</target>
        </trans-unit>
        <trans-unit id="993bb51caef566927c6f2034eead7ae47fdb56d0" translate="yes" xml:space="preserve">
          <source>The resulting value &lt;code&gt;output&lt;/code&gt; would look like this:</source>
          <target state="translated">结果值 &lt;code&gt;output&lt;/code&gt; 将如下所示：</target>
        </trans-unit>
        <trans-unit id="5cc6293ac412a772df6f0e78ca55482c98eb64a2" translate="yes" xml:space="preserve">
          <source>The results of the lookup are concatenated into a dense tensor. The returned tensor has shape &lt;code&gt;shape(ids) + shape(params)[1:]&lt;/code&gt;.</source>
          <target state="translated">查找的结果被连接到一个密集的张量中。返回的张量具有shape &lt;code&gt;shape(ids) + shape(params)[1:]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2f1f40e1bca978cc7aa38e548317ee19cb7dfd8c" translate="yes" xml:space="preserve">
          <source>The return value has the same type as &lt;code&gt;images&lt;/code&gt; if &lt;code&gt;method&lt;/code&gt; is &lt;a href=&quot;../../../image/resizemethod#NEAREST_NEIGHBOR&quot;&gt;&lt;code&gt;ResizeMethod.NEAREST_NEIGHBOR&lt;/code&gt;&lt;/a&gt;. It will also have the same type as &lt;code&gt;images&lt;/code&gt; if the size of &lt;code&gt;images&lt;/code&gt; can be statically determined to be the same as &lt;code&gt;size&lt;/code&gt;, because &lt;code&gt;images&lt;/code&gt; is returned in this case. Otherwise, the return value has type &lt;code&gt;float32&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;method&lt;/code&gt; 为&lt;a href=&quot;../../../image/resizemethod#NEAREST_NEIGHBOR&quot;&gt; &lt;code&gt;ResizeMethod.NEAREST_NEIGHBOR&lt;/code&gt; ,&lt;/a&gt;则返回值与 &lt;code&gt;images&lt;/code&gt; 类型相同。它也将有相同类型 &lt;code&gt;images&lt;/code&gt; ，如果大小 &lt;code&gt;images&lt;/code&gt; 可以被静态地确定为相同的 &lt;code&gt;size&lt;/code&gt; ，因为 &lt;code&gt;images&lt;/code&gt; 是在这种情况下返回。否则，返回值的类型为 &lt;code&gt;float32&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d56a51802c9fd20b938ff0224e17cc92fbe0a0fd" translate="yes" xml:space="preserve">
          <source>The return value has the same type as &lt;code&gt;images&lt;/code&gt; if &lt;code&gt;method&lt;/code&gt; is &lt;a href=&quot;../../../image/resizemethod#NEAREST_NEIGHBOR&quot;&gt;&lt;code&gt;tf.image.ResizeMethod.NEAREST_NEIGHBOR&lt;/code&gt;&lt;/a&gt;. It will also have the same type as &lt;code&gt;images&lt;/code&gt; if the size of &lt;code&gt;images&lt;/code&gt; can be statically determined to be the same as &lt;code&gt;size&lt;/code&gt;, because &lt;code&gt;images&lt;/code&gt; is returned in this case. Otherwise, the return value has type &lt;code&gt;float32&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;method&lt;/code&gt; 为&lt;a href=&quot;../../../image/resizemethod#NEAREST_NEIGHBOR&quot;&gt; &lt;code&gt;tf.image.ResizeMethod.NEAREST_NEIGHBOR&lt;/code&gt; ,&lt;/a&gt;则返回值与 &lt;code&gt;images&lt;/code&gt; 具有相同的类型。它也将有相同类型 &lt;code&gt;images&lt;/code&gt; ，如果大小 &lt;code&gt;images&lt;/code&gt; 可以被静态地确定为相同的 &lt;code&gt;size&lt;/code&gt; ，因为 &lt;code&gt;images&lt;/code&gt; 是在这种情况下返回。否则，返回值的类型为 &lt;code&gt;float32&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="90335752084dfbdd6479bc3425e07c2c184fac03" translate="yes" xml:space="preserve">
          <source>The return value has the same type as &lt;code&gt;images&lt;/code&gt; if &lt;code&gt;method&lt;/code&gt; is &lt;a href=&quot;resizemethod#NEAREST_NEIGHBOR&quot;&gt;&lt;code&gt;ResizeMethod.NEAREST_NEIGHBOR&lt;/code&gt;&lt;/a&gt;. Otherwise, the return value has type &lt;code&gt;float32&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;method&lt;/code&gt; 为&lt;a href=&quot;resizemethod#NEAREST_NEIGHBOR&quot;&gt; &lt;code&gt;ResizeMethod.NEAREST_NEIGHBOR&lt;/code&gt; ,&lt;/a&gt;则返回值与 &lt;code&gt;images&lt;/code&gt; 类型相同。否则，返回值的类型为 &lt;code&gt;float32&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="8d380f2c5d4525994512f90f349c7eb51132c6e2" translate="yes" xml:space="preserve">
          <source>The return value has type &lt;code&gt;float32&lt;/code&gt;, unless the &lt;code&gt;method&lt;/code&gt; is &lt;a href=&quot;resizemethod#NEAREST_NEIGHBOR&quot;&gt;&lt;code&gt;ResizeMethod.NEAREST_NEIGHBOR&lt;/code&gt;&lt;/a&gt;, then the return dtype is the dtype of &lt;code&gt;images&lt;/code&gt;:</source>
          <target state="translated">的返回值的类型是 &lt;code&gt;float32&lt;/code&gt; ，除非 &lt;code&gt;method&lt;/code&gt; 是&lt;a href=&quot;resizemethod#NEAREST_NEIGHBOR&quot;&gt; &lt;code&gt;ResizeMethod.NEAREST_NEIGHBOR&lt;/code&gt; &lt;/a&gt;，则返回是D型的D型 &lt;code&gt;images&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="6db6bf11d3c25e837bbcedc93fcb0930b2a99ab3" translate="yes" xml:space="preserve">
          <source>The return value is not the same Tensor as the original, but contains the same values. This operation is fast when used on the same device.</source>
          <target state="translated">返回值与原始的Tensor不一样,但包含相同的值。这种操作在同一设备上使用时速度很快。</target>
        </trans-unit>
        <trans-unit id="3e3b67eed807726677126c856d0a6214fa8f8e4a" translate="yes" xml:space="preserve">
          <source>The return value of &lt;code&gt;merge_fn&lt;/code&gt;, except for &lt;code&gt;PerReplica&lt;/code&gt; values which are unpacked.</source>
          <target state="translated">&lt;code&gt;merge_fn&lt;/code&gt; 的返回值，但未打包的 &lt;code&gt;PerReplica&lt;/code&gt; 值除外。</target>
        </trans-unit>
        <trans-unit id="b729f575891182a1cf2b646557e584015983e0cf" translate="yes" xml:space="preserve">
          <source>The return values from &lt;code&gt;Session.run()&lt;/code&gt; corresponding to the fetches attribute returned in the RunArgs. Note that this has the same shape as the RunArgs fetches. For example: fetches = global_step_tensor =&amp;gt; results = nparray(int) fetches = [train_op, summary_op, global_step_tensor] =&amp;gt; results = [None, nparray(string), nparray(int)] fetches = {'step': global_step_tensor, 'summ': summary_op} =&amp;gt; results = {'step': nparray(int), 'summ': nparray(string)}</source>
          <target state="translated">&lt;code&gt;Session.run()&lt;/code&gt; 的返回值与RunArgs中返回的fetches属性相对应。请注意，它具有与RunArgs提取相同的形状。例如：fetches = global_step_tensor =&amp;gt;结果= nparray（int）fetches = [train_op，summary_op，global_step_tensor] =&amp;gt; results = [None，nparray（string），nparray（int）] fetches = {'step'：global_step_tensor，' summ'：summary_op} =&amp;gt;结果= {'step'：nparray（int），'summ'：nparray（string）}</target>
        </trans-unit>
        <trans-unit id="1e968254f01cab6234b67bba030e47e2fb4dbf82" translate="yes" xml:space="preserve">
          <source>The returned &lt;a href=&quot;../../../../distribute/distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; can be iterated over similar to how regular datasets can. NOTE: The user cannot add any more transformations to a &lt;a href=&quot;../../../../distribute/distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">返回的&lt;a href=&quot;../../../../distribute/distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt;可以类似于常规数据集那样进行迭代。注意：用户不能将任何其他转换添加到&lt;a href=&quot;../../../../distribute/distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e54bb88894be8aec22542ba047bf035708660a8b" translate="yes" xml:space="preserve">
          <source>The returned &lt;a href=&quot;../../../distribute/distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; can be iterated over similar to how regular datasets can. NOTE: The user cannot add any more transformations to a &lt;a href=&quot;../../../distribute/distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">返回的&lt;a href=&quot;../../../distribute/distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt;可以类似于常规数据集那样进行迭代。注意：用户不能将任何其他转换添加到&lt;a href=&quot;../../../distribute/distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="1bc73b952597eebba1bc9614e25b86603811dd74" translate="yes" xml:space="preserve">
          <source>The returned &lt;a href=&quot;../distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; can be iterated over similar to how regular datasets can. NOTE: The user cannot add any more transformations to a &lt;a href=&quot;../distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">返回的&lt;a href=&quot;../distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt;可以类似于常规数据集那样进行迭代。注意：用户不能将任何其他转换添加到&lt;a href=&quot;../distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="77573df9f9325728fade9f377ed047dae4204229" translate="yes" xml:space="preserve">
          <source>The returned &lt;a href=&quot;distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; can be iterated over similar to how regular datasets can. NOTE: The user cannot add any more transformations to a &lt;a href=&quot;distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">返回的&lt;a href=&quot;distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt;可以类似于常规数据集那样进行迭代。注意：用户不能将任何其他转换添加到&lt;a href=&quot;distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="1dea02fa1b7f8947a51fb716ddcefda916820715" translate="yes" xml:space="preserve">
          <source>The returned &lt;a href=&quot;tensorshape&quot;&gt;&lt;code&gt;tf.TensorShape&lt;/code&gt;&lt;/a&gt; is determined at &lt;em&gt;build&lt;/em&gt; time, without executing the underlying kernel. It is not a &lt;a href=&quot;tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt;. If you need a shape &lt;em&gt;tensor&lt;/em&gt;, either convert the &lt;a href=&quot;tensorshape&quot;&gt;&lt;code&gt;tf.TensorShape&lt;/code&gt;&lt;/a&gt; to a &lt;a href=&quot;constant&quot;&gt;&lt;code&gt;tf.constant&lt;/code&gt;&lt;/a&gt;, or use the &lt;a href=&quot;shape&quot;&gt;&lt;code&gt;tf.shape(tensor)&lt;/code&gt;&lt;/a&gt; function, which returns the tensor's shape at &lt;em&gt;execution&lt;/em&gt; time.</source>
          <target state="translated">返回的&lt;a href=&quot;tensorshape&quot;&gt; &lt;code&gt;tf.TensorShape&lt;/code&gt; &lt;/a&gt;是在&lt;em&gt;构建&lt;/em&gt;时确定的，而无需执行基础内核。它不是&lt;a href=&quot;tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt;。如果需要形状&lt;em&gt;张量&lt;/em&gt;，请将&lt;a href=&quot;tensorshape&quot;&gt; &lt;code&gt;tf.TensorShape&lt;/code&gt; &lt;/a&gt;转换为&lt;a href=&quot;constant&quot;&gt; &lt;code&gt;tf.constant&lt;/code&gt; &lt;/a&gt;，或使用&lt;a href=&quot;shape&quot;&gt; &lt;code&gt;tf.shape(tensor)&lt;/code&gt; &lt;/a&gt;函数，该函数在&lt;em&gt;执行&lt;/em&gt;时返回张量的形状。</target>
        </trans-unit>
        <trans-unit id="111bba19cc7fb0062ed6fe66a53428292ab2ad62" translate="yes" xml:space="preserve">
          <source>The returned &lt;code&gt;RaggedTensor&lt;/code&gt; corresponds with the python list defined by:</source>
          <target state="translated">返回的 &lt;code&gt;RaggedTensor&lt;/code&gt; 与以下定义的python列表相对应：</target>
        </trans-unit>
        <trans-unit id="ecbcae5eb984661e7de2445f57c60c5e40e3d894" translate="yes" xml:space="preserve">
          <source>The returned &lt;code&gt;Session&lt;/code&gt; will be the innermost session on which a &lt;code&gt;Session&lt;/code&gt; or &lt;code&gt;Session.as_default()&lt;/code&gt; context has been entered.</source>
          <target state="translated">返回的 &lt;code&gt;Session&lt;/code&gt; 将是已输入 &lt;code&gt;Session&lt;/code&gt; 或 &lt;code&gt;Session.as_default()&lt;/code&gt; 上下文的最里面的会话。</target>
        </trans-unit>
        <trans-unit id="95a65357f0f57659490bed4f233c4a4d8fc08588" translate="yes" xml:space="preserve">
          <source>The returned &lt;code&gt;Tensor&lt;/code&gt; will be close to an exact solution if &lt;code&gt;A&lt;/code&gt; is well conditioned. Otherwise closeness will vary. See class docstring for details.</source>
          <target state="translated">返回的 &lt;code&gt;Tensor&lt;/code&gt; 将接近一个确切的解决方案，如果 &lt;code&gt;A&lt;/code&gt; 良好条件。否则，紧密度会有所不同。有关详细信息，请参见类docstring。</target>
        </trans-unit>
        <trans-unit id="210a2253a38d62adf49c231834e4374a228751a1" translate="yes" xml:space="preserve">
          <source>The returned callable will have the same return type as &lt;code&gt;tf.Session.run(fetches, ...)&lt;/code&gt;. For example, if &lt;code&gt;fetches&lt;/code&gt; is a &lt;a href=&quot;../../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt;, the callable will return a numpy ndarray; if &lt;code&gt;fetches&lt;/code&gt; is a &lt;a href=&quot;../../operation&quot;&gt;&lt;code&gt;tf.Operation&lt;/code&gt;&lt;/a&gt;, it will return &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">返回的可调用对象将具有与 &lt;code&gt;tf.Session.run(fetches, ...)&lt;/code&gt; 相同的返回类型。例如，如果 &lt;code&gt;fetches&lt;/code&gt; 为&lt;a href=&quot;../../tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt;，可调用将返回一个numpy的ndarray;如果 &lt;code&gt;fetches&lt;/code&gt; 是&lt;a href=&quot;../../operation&quot;&gt; &lt;code&gt;tf.Operation&lt;/code&gt; &lt;/a&gt;，它将返回 &lt;code&gt;None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="bfdda7d94aac832546a913098111bedb3a5af104" translate="yes" xml:space="preserve">
          <source>The returned callable will take &lt;code&gt;len(feed_list)&lt;/code&gt; arguments whose types must be compatible feed values for the respective elements of &lt;code&gt;feed_list&lt;/code&gt;. For example, if element &lt;code&gt;i&lt;/code&gt; of &lt;code&gt;feed_list&lt;/code&gt; is a &lt;a href=&quot;../../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;i&lt;/code&gt;th argument to the returned callable must be a numpy ndarray (or something convertible to an ndarray) with matching element type and shape. See &lt;code&gt;tf.Session.run&lt;/code&gt; for details of the allowable feed key and value types.</source>
          <target state="translated">返回的callable将 &lt;code&gt;len(feed_list)&lt;/code&gt; 参数，其类型必须与 &lt;code&gt;feed_list&lt;/code&gt; 的各个元素兼容的feed值。例如，如果元件 &lt;code&gt;i&lt;/code&gt; 的 &lt;code&gt;feed_list&lt;/code&gt; 是&lt;a href=&quot;../../tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt;中， &lt;code&gt;i&lt;/code&gt; 个参数返回的调用必须是numpy的ndarray（或东西可转化成ndarray）与匹配元件的类型和形状。有关允许的Feed键和值类型的详细信息，请参见 &lt;code&gt;tf.Session.run&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5ae9baaba07ae2a863bb456cb56b210b15bfeca4" translate="yes" xml:space="preserve">
          <source>The returned dataset is a wrapped strategy dataset which creates a multidevice iterator under the hood. It prefetches the input data to the specified devices on the worker. The returned distributed dataset can be iterated over similar to how regular datasets can.</source>
          <target state="translated">返回的数据集是一个封装的策略数据集,它在外壳下创建了一个多设备迭代器。它将输入数据预取到worker上的指定设备。返回的分布式数据集可以像常规数据集那样进行迭代。</target>
        </trans-unit>
        <trans-unit id="b16f128652226cd5df44d3210c5abe8996d2a7b8" translate="yes" xml:space="preserve">
          <source>The returned dict may have the following keys:</source>
          <target state="translated">返回的dict可以有以下键:</target>
        </trans-unit>
        <trans-unit id="6f7e2363fe125c1a5cdb798835abe8de5ac8aef0" translate="yes" xml:space="preserve">
          <source>The returned dictionary can be used as arg 'features' in &lt;a href=&quot;../../../io/parse_example&quot;&gt;&lt;code&gt;tf.io.parse_example&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">返回的字典可以在&lt;a href=&quot;../../../io/parse_example&quot;&gt; &lt;code&gt;tf.io.parse_example&lt;/code&gt; 中&lt;/a&gt;用作arg'功能' 。</target>
        </trans-unit>
        <trans-unit id="27148d9610fe5857a584045bc355cd08d4ce68b2" translate="yes" xml:space="preserve">
          <source>The returned dictionary can be used as arg 'features' in &lt;a href=&quot;../io/parse_example&quot;&gt;&lt;code&gt;tf.io.parse_example&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">返回的字典可以在&lt;a href=&quot;../io/parse_example&quot;&gt; &lt;code&gt;tf.io.parse_example&lt;/code&gt; 中&lt;/a&gt;用作arg'功能' 。</target>
        </trans-unit>
        <trans-unit id="e41cdf1dd9ec5b48c6c1a627e04bc9cd0f5646ec" translate="yes" xml:space="preserve">
          <source>The returned distributed dataset can be iterated over similar to how regular datasets can. NOTE: Currently, the user cannot add any more transformations to a distributed dataset.</source>
          <target state="translated">返回的分布式数据集可以像常规数据集那样进行迭代。注意:目前,用户不能再向分布式数据集添加任何转换。</target>
        </trans-unit>
        <trans-unit id="8e5c9b1714c669dc39fd802e652891ab6982a58a" translate="yes" xml:space="preserve">
          <source>The returned graph will be the innermost graph on which a &lt;a href=&quot;../../graph#as_default&quot;&gt;&lt;code&gt;Graph.as_default()&lt;/code&gt;&lt;/a&gt; context has been entered, or a global default graph if none has been explicitly created.</source>
          <target state="translated">返回的图将是已在其中输入&lt;a href=&quot;../../graph#as_default&quot;&gt; &lt;code&gt;Graph.as_default()&lt;/code&gt; &lt;/a&gt;上下文的最里面的图，如果尚未显式创建任何默认图，则将是全局默认图。</target>
        </trans-unit>
        <trans-unit id="a3ca6a76bf18bc7eb9e290fd0f8d92384b3fc63a" translate="yes" xml:space="preserve">
          <source>The returned graph will be the innermost graph on which a &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#as_default&quot;&gt;&lt;code&gt;Graph.as_default()&lt;/code&gt;&lt;/a&gt; context has been entered, or a global default graph if none has been explicitly created.</source>
          <target state="translated">返回的图将是已输入&lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#as_default&quot;&gt; &lt;code&gt;Graph.as_default()&lt;/code&gt; &lt;/a&gt;上下文的最里面的图，如果未显式创建全局默认图，则返回该图。</target>
        </trans-unit>
        <trans-unit id="4983b6188d9b3a5503f72bc1978affb47cac7b6d" translate="yes" xml:space="preserve">
          <source>The returned iterator implements the Python Iterator protocol.</source>
          <target state="translated">返回的迭代器实现了Python Iterator协议。</target>
        </trans-unit>
        <trans-unit id="6b38dad137146b9ef38ba7c509e2c0442b75c1f0" translate="yes" xml:space="preserve">
          <source>The returned iterator implements the Python iterator protocol and therefore can only be used in eager mode.</source>
          <target state="translated">返回的迭代器实现了Python迭代器协议,因此只能在eager模式下使用。</target>
        </trans-unit>
        <trans-unit id="7e0593141fa29e4d333dddf121d2181c125317c5" translate="yes" xml:space="preserve">
          <source>The returned iterator is not bound to a particular dataset, and it has no &lt;code&gt;initializer&lt;/code&gt;. To initialize the iterator, run the operation returned by &lt;code&gt;Iterator.make_initializer(dataset)&lt;/code&gt;.</source>
          <target state="translated">返回的迭代器未绑定到特定数据集，并且没有 &lt;code&gt;initializer&lt;/code&gt; 。要初始化迭代器，请运行 &lt;code&gt;Iterator.make_initializer(dataset)&lt;/code&gt; 返回的操作。</target>
        </trans-unit>
        <trans-unit id="35c004b6fa1c65f47bcd7047f0f10a1978a605f3" translate="yes" xml:space="preserve">
          <source>The returned operation is a dequeue operation and will throw &lt;a href=&quot;../../../errors/outofrangeerror&quot;&gt;&lt;code&gt;tf.errors.OutOfRangeError&lt;/code&gt;&lt;/a&gt; if the input queue is exhausted. If this operation is feeding another input queue, its queue runner will catch this exception, however, if this operation is used in your main thread you are responsible for catching this yourself.</source>
          <target state="translated">返回的操作是出队操作，如果输入队列已用尽，将抛出&lt;a href=&quot;../../../errors/outofrangeerror&quot;&gt; &lt;code&gt;tf.errors.OutOfRangeError&lt;/code&gt; &lt;/a&gt;。如果此操作正在馈入另一个输入队列，则其队列运行器将捕获此异常，但是，如果在您的主线程中使用了此操作，则您有责任自己捕获此异常。</target>
        </trans-unit>
        <trans-unit id="388161f4aeaed3801662f433bd19fe4a8d446b0a" translate="yes" xml:space="preserve">
          <source>The returned permutation may be used to permute the rows and columns of the given sparse matrix. This typically results in permuted sparse matrix's sparse Cholesky (or other decompositions) in having fewer zero fill-in compared to decomposition of the original matrix.</source>
          <target state="translated">返回的换元可以用来对给定稀疏矩阵的行和列进行换元。这通常会导致换置后的稀疏矩阵的稀疏Cholesky(或其他分解)与原始矩阵的分解相比有更少的零填充。</target>
        </trans-unit>
        <trans-unit id="34611a4904c47d805a50142b42fe43a0ecae787f" translate="yes" xml:space="preserve">
          <source>The returned sparse matrix has the same dense shape as the input sparse matrix. For each component &lt;code&gt;A&lt;/code&gt; of the input sparse matrix, the corresponding output sparse matrix represents &lt;code&gt;L&lt;/code&gt;, the lower triangular Cholesky factor satisfying the following identity:</source>
          <target state="translated">返回的稀疏矩阵具有与输入稀疏矩阵相同的密集形状。对于输入稀疏矩阵的每个分量 &lt;code&gt;A&lt;/code&gt; ，对应的输出稀疏矩阵表示 &lt;code&gt;L&lt;/code&gt; ，满足以下标识的下三角Cholesky因子：</target>
        </trans-unit>
        <trans-unit id="283d98979ab1e70cde916469a5e89b9e786cbb6c" translate="yes" xml:space="preserve">
          <source>The returned status object has the following methods:</source>
          <target state="translated">返回的状态对象有以下方法。</target>
        </trans-unit>
        <trans-unit id="b692f2b05a18e92aff1d5bc6f670c1b3f429e602" translate="yes" xml:space="preserve">
          <source>The returned string will be in the form protocol://address, e.g. &quot;grpc://localhost:5050&quot;.</source>
          <target state="translated">返回的字符串将以protocol://address的形式出现,例如 &quot;grpc://localhost:5050&quot;。</target>
        </trans-unit>
        <trans-unit id="c118b1fddbb7639875cad9b7ab197644effe14a5" translate="yes" xml:space="preserve">
          <source>The returned tensor uses the memory shared by dlpack capsules from other framework.</source>
          <target state="translated">返回的张量使用其他框架的dlpack胶囊共享的内存。</target>
        </trans-unit>
        <trans-unit id="76a9be24217f14af8b4e90ac374a00f3e69a4114" translate="yes" xml:space="preserve">
          <source>The returned tensor will contain a serialized &lt;a href=&quot;../../summary&quot;&gt;&lt;code&gt;tf.compat.v1.summary.Summary&lt;/code&gt;&lt;/a&gt; protocol buffer, which can be used with the standard TensorBoard logging facilities.</source>
          <target state="translated">返回的张量将包含序列化的&lt;a href=&quot;../../summary&quot;&gt; &lt;code&gt;tf.compat.v1.summary.Summary&lt;/code&gt; &lt;/a&gt;协议缓冲区，可与标准TensorBoard日志记录工具一起使用。</target>
        </trans-unit>
        <trans-unit id="c2eac31d16e8529bfab56c61e3821bc8b01c5d91" translate="yes" xml:space="preserve">
          <source>The returned tensor's dimension &lt;code&gt;i&lt;/code&gt; will correspond to the input dimension &lt;code&gt;perm[i]&lt;/code&gt;. If &lt;code&gt;perm&lt;/code&gt; is not given, it is set to (n-1...0), where n is the rank of the input tensor. Hence by default, this operation performs a regular matrix transpose on 2-D input Tensors.</source>
          <target state="translated">返回的张量维 &lt;code&gt;i&lt;/code&gt; 将对应于输入维 &lt;code&gt;perm[i]&lt;/code&gt; 。如果未给出 &lt;code&gt;perm&lt;/code&gt; ，则将其设置为（n-1 ... 0），其中n是输入张量的秩。因此，默认情况下，此操作会在2D输入张量上执行常规矩阵转置。</target>
        </trans-unit>
        <trans-unit id="82d1c361e6f8f0d84bf0a1bcea9214b49e8cf3d3" translate="yes" xml:space="preserve">
          <source>The returned tensor's dimension i will correspond to the input dimension &lt;code&gt;perm[i]&lt;/code&gt;. If &lt;code&gt;perm&lt;/code&gt; is not given, it is set to (n-1...0), where n is the rank of the input tensor. Hence by default, this operation performs a regular matrix transpose on 2-D input Tensors.</source>
          <target state="translated">返回的张量的尺寸i将对应于输入尺寸 &lt;code&gt;perm[i]&lt;/code&gt; 。如果未给出 &lt;code&gt;perm&lt;/code&gt; ，则将其设置为（n-1 ... 0），其中n是输入张量的等级。因此，默认情况下，此操作会在2D输入张量上执行常规矩阵转置。</target>
        </trans-unit>
        <trans-unit id="d9c115cc2d00147127224aefb2bd6a8e1bbe0c5c" translate="yes" xml:space="preserve">
          <source>The returned tensor's dimension i will correspond to the input dimension &lt;code&gt;perm[i]&lt;/code&gt;. If &lt;code&gt;perm&lt;/code&gt; is not given, it is set to (n-1...0), where n is the rank of the input tensor. Hence by default, this operation performs a regular matrix transpose on 2-D input Tensors. If conjugate is True and &lt;code&gt;a.dtype&lt;/code&gt; is either &lt;code&gt;complex64&lt;/code&gt; or &lt;code&gt;complex128&lt;/code&gt; then the values of &lt;code&gt;a&lt;/code&gt; are conjugated and transposed.</source>
          <target state="translated">返回的张量的尺寸i将对应于输入尺寸 &lt;code&gt;perm[i]&lt;/code&gt; 。如果未给出 &lt;code&gt;perm&lt;/code&gt; ，则将其设置为（n-1 ... 0），其中n是输入张量的等级。因此，默认情况下，此操作会在2D输入张量上执行常规矩阵转置。如果共轭为True，并且 &lt;code&gt;a.dtype&lt;/code&gt; 要么 &lt;code&gt;complex64&lt;/code&gt; 或 &lt;code&gt;complex128&lt;/code&gt; 然后的值 &lt;code&gt;a&lt;/code&gt; 是共轭转置和。</target>
        </trans-unit>
        <trans-unit id="838e6e70eeb12ce61fde540b38b4044a46a8f134" translate="yes" xml:space="preserve">
          <source>The returned tensors are &lt;a href=&quot;../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt;s if &lt;code&gt;input&lt;/code&gt; is a scalar, or &lt;a href=&quot;../raggedtensor&quot;&gt;&lt;code&gt;tf.RaggedTensor&lt;/code&gt;&lt;/a&gt;s otherwise.</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 为标量，则返回的张量为&lt;a href=&quot;../tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt;，否则为&lt;a href=&quot;../raggedtensor&quot;&gt; &lt;code&gt;tf.RaggedTensor&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="710644f5cd50999519cd748e88ddf36f02a2b455" translate="yes" xml:space="preserve">
          <source>The right-hand side of the &lt;code&gt;!=&lt;/code&gt; operator.</source>
          <target state="translated">&lt;code&gt;!=&lt;/code&gt; 运算符的右侧。</target>
        </trans-unit>
        <trans-unit id="e668edbb07540997101fa0f49a39faa5f38caec8" translate="yes" xml:space="preserve">
          <source>The right-hand side of the &lt;code&gt;+&lt;/code&gt; operator.</source>
          <target state="translated">&lt;code&gt;+&lt;/code&gt; 运算符的右侧。</target>
        </trans-unit>
        <trans-unit id="ff72258325f43cdf735f357c785333acbdbfc89a" translate="yes" xml:space="preserve">
          <source>The right-hand side of the &lt;code&gt;==&lt;/code&gt; operator.</source>
          <target state="translated">&lt;code&gt;==&lt;/code&gt; 运算符的右侧。</target>
        </trans-unit>
        <trans-unit id="ca75c2c84ca7c94aaa9f8f708d895cba56b5cf74" translate="yes" xml:space="preserve">
          <source>The row partition tensors are in the order of the dimensions. At present, the types can be:</source>
          <target state="translated">行分区天数按维数顺序排列。目前,类型可以是。</target>
        </trans-unit>
        <trans-unit id="85216d0bd898c6d1680f5aac974d5348d1d96c2f" translate="yes" xml:space="preserve">
          <source>The row-split indices for this ragged tensor's &lt;code&gt;values&lt;/code&gt;.</source>
          <target state="translated">这个参差不齐的张 &lt;code&gt;values&lt;/code&gt; 的行分割索引。</target>
        </trans-unit>
        <trans-unit id="5854e78835764512ec754b4ac011bb9b48f7d380" translate="yes" xml:space="preserve">
          <source>The row_splits for all ragged dimensions in this ragged tensor value.</source>
          <target state="translated">这个残缺张量值中所有残缺维度的row_splits。</target>
        </trans-unit>
        <trans-unit id="5158c81c5ea5bb74de68daed2eacbf5dc32d4ad8" translate="yes" xml:space="preserve">
          <source>The runtime is then free to make optimizations based on this.</source>
          <target state="translated">运行时就可以在此基础上自由地进行优化。</target>
        </trans-unit>
        <trans-unit id="ca5cdf682a9605ca998129dd4a854da399041aba" translate="yes" xml:space="preserve">
          <source>The same array (Numpy array if &lt;code&gt;x&lt;/code&gt; was a Numpy array, or TensorFlow tensor if &lt;code&gt;x&lt;/code&gt; was a tensor), cast to its new type.</source>
          <target state="translated">相同的阵列（阵列numpy的如果 &lt;code&gt;x&lt;/code&gt; 是一个numpy的阵列，或TensorFlow张量如果 &lt;code&gt;x&lt;/code&gt; 是一个张量），流延到其新的类型。</target>
        </trans-unit>
        <trans-unit id="e4b3e7c37b806fc9d4b600343aa28d9179c53706" translate="yes" xml:space="preserve">
          <source>The same as &lt;a href=&quot;../../raggedtensor#__div__&quot;&gt;&lt;code&gt;tf.compat.v1.div(x,y)&lt;/code&gt;&lt;/a&gt; for integers, but uses &lt;code&gt;tf.floor(tf.compat.v1.div(x,y))&lt;/code&gt; for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by &lt;code&gt;x // y&lt;/code&gt; floor division in Python 3 and in Python 2.7 with &lt;code&gt;from __future__ import division&lt;/code&gt;.</source>
          <target state="translated">与整数的&lt;a href=&quot;../../raggedtensor#__div__&quot;&gt; &lt;code&gt;tf.compat.v1.div(x,y)&lt;/code&gt; &lt;/a&gt;相同，但对浮点参数使用 &lt;code&gt;tf.floor(tf.compat.v1.div(x,y))&lt;/code&gt; ，因此结果始终为整数（ （可能是表示为浮点数的整数）。该op由Python 3和python 2.7中的 &lt;code&gt;x // y&lt;/code&gt; 地板分割生成， &lt;code&gt;from __future__ import division&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="07525f9495ce7a62b86179e9f6083e34f20d8d8b" translate="yes" xml:space="preserve">
          <source>The same as &lt;a href=&quot;../raggedtensor#__div__&quot;&gt;&lt;code&gt;tf.compat.v1.div(x,y)&lt;/code&gt;&lt;/a&gt; for integers, but uses &lt;code&gt;tf.floor(tf.compat.v1.div(x,y))&lt;/code&gt; for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by &lt;code&gt;x // y&lt;/code&gt; floor division in Python 3 and in Python 2.7 with &lt;code&gt;from __future__ import division&lt;/code&gt;.</source>
          <target state="translated">与整数的&lt;a href=&quot;../raggedtensor#__div__&quot;&gt; &lt;code&gt;tf.compat.v1.div(x,y)&lt;/code&gt; &lt;/a&gt;相同，但对浮点参数使用 &lt;code&gt;tf.floor(tf.compat.v1.div(x,y))&lt;/code&gt; ，因此结果始终为整数（ （可能是表示为浮点数的整数）。该op由Python 3和python 2.7中的 &lt;code&gt;x // y&lt;/code&gt; 地板分割生成， &lt;code&gt;from __future__ import division&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="3240798db52d492b4a5e0a4f1101b0ce22e5dea6" translate="yes" xml:space="preserve">
          <source>The same as &lt;a href=&quot;raggedtensor#__div__&quot;&gt;&lt;code&gt;tf.compat.v1.div(x,y)&lt;/code&gt;&lt;/a&gt; for integers, but uses &lt;code&gt;tf.floor(tf.compat.v1.div(x,y))&lt;/code&gt; for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by &lt;code&gt;x // y&lt;/code&gt; floor division in Python 3 and in Python 2.7 with &lt;code&gt;from __future__ import division&lt;/code&gt;.</source>
          <target state="translated">与整数的&lt;a href=&quot;raggedtensor#__div__&quot;&gt; &lt;code&gt;tf.compat.v1.div(x,y)&lt;/code&gt; &lt;/a&gt;相同，但对浮点参数使用 &lt;code&gt;tf.floor(tf.compat.v1.div(x,y))&lt;/code&gt; ，因此结果始终为整数（ （可能是表示为浮点数的整数）。该op由Python 3和python 2.7中的 &lt;code&gt;x // y&lt;/code&gt; 地板分割生成， &lt;code&gt;from __future__ import division&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d6016fab50d93b1e77917677d258e0e003f237e6" translate="yes" xml:space="preserve">
          <source>The same tensor &lt;code&gt;x&lt;/code&gt;, unchanged.</source>
          <target state="translated">相同的张量 &lt;code&gt;x&lt;/code&gt; ，不变。</target>
        </trans-unit>
        <trans-unit id="e28b3d74027804ad355c817f8c256af8b2d25a2a" translate="yes" xml:space="preserve">
          <source>The samples are differentiable w.r.t. alpha and beta. The derivatives are computed using the approach described in (Figurnov et al., 2018).</source>
          <target state="translated">样本是可微分的α和β。导数的计算采用(Figurnov等人,2018)中描述的方法。</target>
        </trans-unit>
        <trans-unit id="2c3935638fc889928b31f22a28b40af71a593afd" translate="yes" xml:space="preserve">
          <source>The samples are differentiable w.r.t. alpha and beta. The derivatives are computed using the approach described in the paper</source>
          <target state="translated">样本是可微分的α和β。导数的计算采用本文所述的方法。</target>
        </trans-unit>
        <trans-unit id="92952659dac1bf101ab8c26a5fd0e7570c899948" translate="yes" xml:space="preserve">
          <source>The sampling probabilities are generated according to the sampling distribution used in word2vec:</source>
          <target state="translated">抽样概率是根据word2vec中使用的抽样分布生成的。</target>
        </trans-unit>
        <trans-unit id="7e70c09e21b733e9d2a808af0f600268962663fb" translate="yes" xml:space="preserve">
          <source>The save counter variable.</source>
          <target state="translated">保存计数器变量。</target>
        </trans-unit>
        <trans-unit id="ab43a8a642f72175fe828def8c8d03e9a692fe7b" translate="yes" xml:space="preserve">
          <source>The saved checkpoint includes variables created by this object and any trackable objects it depends on at the time &lt;a href=&quot;../../../train/checkpoint#save&quot;&gt;&lt;code&gt;Checkpoint.save()&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="translated">保存的检查点包括此对象及其在调用&lt;a href=&quot;../../../train/checkpoint#save&quot;&gt; &lt;code&gt;Checkpoint.save()&lt;/code&gt; &lt;/a&gt;时依赖的任何可跟踪对象创建的变量。</target>
        </trans-unit>
        <trans-unit id="21ba0881308b74c027819f31603cfc6647b52c80" translate="yes" xml:space="preserve">
          <source>The saved checkpoint includes variables created by this object and any trackable objects it depends on at the time &lt;a href=&quot;checkpoint#save&quot;&gt;&lt;code&gt;Checkpoint.save()&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="translated">保存的检查点包括此对象及其在调用&lt;a href=&quot;checkpoint#save&quot;&gt; &lt;code&gt;Checkpoint.save()&lt;/code&gt; &lt;/a&gt;时依赖的任何可跟踪对象创建的变量。</target>
        </trans-unit>
        <trans-unit id="38adbe2f58aa8b45a85ccd9d545a08f193885a40" translate="yes" xml:space="preserve">
          <source>The saved checkpoint includes variables created by this object and any trackable objects it depends on at the time &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#save&quot;&gt;&lt;code&gt;Checkpoint.save()&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="translated">保存的检查点包括此对象及其在调用&lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#save&quot;&gt; &lt;code&gt;Checkpoint.save()&lt;/code&gt; 时&lt;/a&gt;所依赖的任何可跟踪对象创建的变量。</target>
        </trans-unit>
        <trans-unit id="d6290af5f5d901c235fa2702c12d35d14bd8fbb8" translate="yes" xml:space="preserve">
          <source>The saved dataset is saved in multiple file &quot;shards&quot;. By default, the dataset output is divided to shards in a round-robin fashion but custom sharding can be specified via the &lt;code&gt;shard_func&lt;/code&gt; function. For example, you can save the dataset to using a single shard as follows:</source>
          <target state="translated">保存的数据集保存在多个文件&amp;ldquo;碎片&amp;rdquo;中。默认情况下，数据集输出以循环方式划分为碎片，但是可以通过 &lt;code&gt;shard_func&lt;/code&gt; 函数指定自定义碎片。例如，您可以将数据集保存为使用单个碎片，如下所示：</target>
        </trans-unit>
        <trans-unit id="32eabd05b895cc5cbd843e5d58a26478d871b570" translate="yes" xml:space="preserve">
          <source>The saved model contains:</source>
          <target state="translated">保存的模型包括:</target>
        </trans-unit>
        <trans-unit id="d3b7070f1ecc16e817793f6b5d01b2b53b58703e" translate="yes" xml:space="preserve">
          <source>The saved model contains: - the model's configuration (topology) - the model's weights - the model's optimizer's state (if any)</source>
          <target state="translated">保存的模型包含-模型的配置(拓扑)-模型的权重 -模型的优化器的状态(如果有的话)。</target>
        </trans-unit>
        <trans-unit id="eeb866c0e9b8a790b2b06bb4bf72fe83fcf3b7fd" translate="yes" xml:space="preserve">
          <source>The savefile includes:</source>
          <target state="translated">保存文件包括:</target>
        </trans-unit>
        <trans-unit id="2be8f8b013a0bea1b78a799bc083aef46d344f06" translate="yes" xml:space="preserve">
          <source>The scalar &lt;code&gt;default_value&lt;/code&gt; is the value output for keys not present in the table. It must also be of the same type as the table values.</source>
          <target state="translated">标量 &lt;code&gt;default_value&lt;/code&gt; 是表中不存在的键的输出值。它也必须与表值具有相同的类型。</target>
        </trans-unit>
        <trans-unit id="ff3fd5f27f08bbd84c446309ecdaecd1825d0b40" translate="yes" xml:space="preserve">
          <source>The scalar PSNR between a and b. The returned tensor has type &lt;a href=&quot;../../tf#float32&quot;&gt;&lt;code&gt;tf.float32&lt;/code&gt;&lt;/a&gt; and shape [batch_size, 1].</source>
          <target state="translated">a和b之间的标量PSNR。返回的张量的类型为&lt;a href=&quot;../../tf#float32&quot;&gt; &lt;code&gt;tf.float32&lt;/code&gt; &lt;/a&gt;，形状为[batch_size，1]。</target>
        </trans-unit>
        <trans-unit id="dd78e4034ea5bbefe47a5585f3fa965bc5acaa8e" translate="yes" xml:space="preserve">
          <source>The scaled exponential unit activation: &lt;code&gt;scale * elu(x, alpha)&lt;/code&gt;.</source>
          <target state="translated">缩放的指数单位激活： &lt;code&gt;scale * elu(x, alpha)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d7cdf9b27ca1dd2bbd540e30d82d6f1126674d88" translate="yes" xml:space="preserve">
          <source>The scaling_factor is determined from &lt;code&gt;min_range&lt;/code&gt;, &lt;code&gt;max_range&lt;/code&gt;, and &lt;code&gt;narrow_range&lt;/code&gt; in a way that is compatible with &lt;code&gt;QuantizeAndDequantize{V2|V3}&lt;/code&gt; and &lt;code&gt;QuantizeV2&lt;/code&gt;, using the following algorithm:</source>
          <target state="translated">使用与 &lt;code&gt;QuantizeAndDequantize{V2|V3}&lt;/code&gt; 和 &lt;code&gt;QuantizeV2&lt;/code&gt; 兼容的方式，使用以下算法从 &lt;code&gt;min_range&lt;/code&gt; ， &lt;code&gt;max_range&lt;/code&gt; 和 &lt;code&gt;narrow_range&lt;/code&gt; 范围内确定scale_factor：</target>
        </trans-unit>
        <trans-unit id="d82c0e384909c0dd188de5ead9b2872459677ae8" translate="yes" xml:space="preserve">
          <source>The schedule a 1-arg callable that produces a decayed learning rate when passed the current optimizer step. This can be useful for changing the learning rate value across different invocations of optimizer functions.</source>
          <target state="translated">安排一个1-arg的可调用,当通过当前优化器步骤时,产生一个衰减的学习率。这对于改变不同优化器函数调用的学习率值非常有用。</target>
        </trans-unit>
        <trans-unit id="c2ad88fa788374ec95b4583696db082a634b18eb" translate="yes" xml:space="preserve">
          <source>The schedule a 1-arg callable that produces a decayed learning rate when passed the current optimizer step. This can be useful for changing the learning rate value across different invocations of optimizer functions. It is computed as:</source>
          <target state="translated">安排一个1-arg的可调用,当通过当前优化器步骤时,产生一个衰减的学习率。这对于改变不同优化器函数调用的学习率值很有用。它的计算方式为</target>
        </trans-unit>
        <trans-unit id="71d2993ed1ad7b32ffe04dab2486da899c3f2739" translate="yes" xml:space="preserve">
          <source>The schedule is a 1-arg callable that produces a decayed learning rate when passed the current optimizer step. This can be useful for changing the learning rate value across different invocations of optimizer functions. It is computed as:</source>
          <target state="translated">schedule是一个1-arg的可调用函数,当通过当前优化器步骤时,会产生一个衰减的学习率。这对于改变优化器函数不同调用的学习率值很有用。它的计算方式是:</target>
        </trans-unit>
        <trans-unit id="70da9d42a4b299e69bb575bc1c369e715f3022f0" translate="yes" xml:space="preserve">
          <source>The scope for the operations performed in computing the loss.</source>
          <target state="translated">计算损失时进行的操作范围;</target>
        </trans-unit>
        <trans-unit id="2bf5542bf013cf2cd0c7e2cee3c5c6cd6c529ad0" translate="yes" xml:space="preserve">
          <source>The scope name.</source>
          <target state="translated">范围名称。</target>
        </trans-unit>
        <trans-unit id="50f230356c528b135a5e5c8be0185503e716f9b3" translate="yes" xml:space="preserve">
          <source>The second call of &lt;code&gt;foo&lt;/code&gt; returns '(A2, A2)' instead of '(A1, A1)' because &lt;a href=&quot;uniform&quot;&gt;&lt;code&gt;tf.random.uniform&lt;/code&gt;&lt;/a&gt; maintains an internal counter. If you want &lt;code&gt;foo&lt;/code&gt; to return '(A1, A1)' every time, use the stateless random ops such as &lt;a href=&quot;stateless_uniform&quot;&gt;&lt;code&gt;tf.random.stateless_uniform&lt;/code&gt;&lt;/a&gt;. Also see &lt;a href=&quot;experimental/generator&quot;&gt;&lt;code&gt;tf.random.experimental.Generator&lt;/code&gt;&lt;/a&gt; for a new set of stateful random ops that use external variables to manage their states.</source>
          <target state="translated">&lt;code&gt;foo&lt;/code&gt; 的第二次调用返回'（A2，A2）'而不是'（A1，A1）'，因为&lt;a href=&quot;uniform&quot;&gt; &lt;code&gt;tf.random.uniform&lt;/code&gt; &lt;/a&gt;维护一个内部计数器。如果希望 &lt;code&gt;foo&lt;/code&gt; 每次都返回'（A1，A1）'，请使用无状态随机操作，例如&lt;a href=&quot;stateless_uniform&quot;&gt; &lt;code&gt;tf.random.stateless_uniform&lt;/code&gt; &lt;/a&gt;。另请参阅&lt;a href=&quot;experimental/generator&quot;&gt; &lt;code&gt;tf.random.experimental.Generator&lt;/code&gt; ,&lt;/a&gt;以获得使用外部变量管理其状态的一组新的有状态随机操作。</target>
        </trans-unit>
        <trans-unit id="99c1ec05a48b05e1a2d2c74a95c41738070d1d52" translate="yes" xml:space="preserve">
          <source>The second call of &lt;code&gt;foo&lt;/code&gt; returns '(A2, A2)' instead of '(A1, A1)' because &lt;a href=&quot;uniform&quot;&gt;&lt;code&gt;tf.random.uniform&lt;/code&gt;&lt;/a&gt; maintains an internal counter. If you want &lt;code&gt;foo&lt;/code&gt; to return '(A1, A1)' every time, use the stateless random ops such as &lt;a href=&quot;stateless_uniform&quot;&gt;&lt;code&gt;tf.random.stateless_uniform&lt;/code&gt;&lt;/a&gt;. Also see &lt;a href=&quot;generator&quot;&gt;&lt;code&gt;tf.random.experimental.Generator&lt;/code&gt;&lt;/a&gt; for a new set of stateful random ops that use external variables to manage their states.</source>
          <target state="translated">&lt;code&gt;foo&lt;/code&gt; 的第二次调用返回'（A2，A2）'而不是'（A1，A1）'，因为&lt;a href=&quot;uniform&quot;&gt; &lt;code&gt;tf.random.uniform&lt;/code&gt; &lt;/a&gt;维护一个内部计数器。如果您想让 &lt;code&gt;foo&lt;/code&gt; 每次都返回'（A1，A1）'，请使用无状态随机操作，例如&lt;a href=&quot;stateless_uniform&quot;&gt; &lt;code&gt;tf.random.stateless_uniform&lt;/code&gt; &lt;/a&gt;。另请参阅&lt;a href=&quot;generator&quot;&gt; &lt;code&gt;tf.random.experimental.Generator&lt;/code&gt; ,&lt;/a&gt;以获取一组新的有状态随机操作，这些操作使用外部变量来管理其状态。</target>
        </trans-unit>
        <trans-unit id="08568c721bd015f102f6c3ec1b535d6153171292" translate="yes" xml:space="preserve">
          <source>The second dict contains the feature_list key/values.</source>
          <target state="translated">第二个dict包含feature_list键/值。</target>
        </trans-unit>
        <trans-unit id="55c0e48b341890b90c7222213ebdf6711ed127c5" translate="yes" xml:space="preserve">
          <source>The second distribution.</source>
          <target state="translated">第二次分配。</target>
        </trans-unit>
        <trans-unit id="73916e07f6ea34c81abd799027349571045df1c6" translate="yes" xml:space="preserve">
          <source>The second innermost dimension of &lt;code&gt;diagonal&lt;/code&gt; has double meaning. When &lt;code&gt;k&lt;/code&gt; is scalar or &lt;code&gt;k[0] == k[1]&lt;/code&gt;, &lt;code&gt;M&lt;/code&gt; is part of the batch size [I, J, ..., M], and the output tensor is:</source>
          <target state="translated">&lt;code&gt;diagonal&lt;/code&gt; 的第二个最里面的维度具有双重含义。当 &lt;code&gt;k&lt;/code&gt; 是标量或 &lt;code&gt;k[0] == k[1]&lt;/code&gt; ， &lt;code&gt;M&lt;/code&gt; 是批处理大小[I，J，...，M]的一部分，并且输出张量是：</target>
        </trans-unit>
        <trans-unit id="897c8f258e58c08596b32bdbc51f703aa19e32cb" translate="yes" xml:space="preserve">
          <source>The second list to compare.</source>
          <target state="translated">第二张清单来比较。</target>
        </trans-unit>
        <trans-unit id="07d2101b8340d508be1c61741652a76d9deb3b07" translate="yes" xml:space="preserve">
          <source>The second operand; &lt;code&gt;SparseTensor&lt;/code&gt; or &lt;code&gt;Tensor&lt;/code&gt;. At least one operand must be sparse.</source>
          <target state="translated">第二个操作数； &lt;code&gt;SparseTensor&lt;/code&gt; 或 &lt;code&gt;Tensor&lt;/code&gt; 。至少一个操作数必须是稀疏的。</target>
        </trans-unit>
        <trans-unit id="eb28a5566b72083d75d5c15273e66594bb2b10e4" translate="yes" xml:space="preserve">
          <source>The second sequence to compare.</source>
          <target state="translated">第二序列来比较。</target>
        </trans-unit>
        <trans-unit id="f44476a07dabc329393369e874eaf7a10133de21" translate="yes" xml:space="preserve">
          <source>The second set to compare.</source>
          <target state="translated">第二套来比较。</target>
        </trans-unit>
        <trans-unit id="f193b83c375843f2a04bc17c95ee85c91007fcdb" translate="yes" xml:space="preserve">
          <source>The second structure to compare.</source>
          <target state="translated">第二种结构来比较。</target>
        </trans-unit>
        <trans-unit id="d1bef3c10ad61ff65c7724ce0d24e0698479866c" translate="yes" xml:space="preserve">
          <source>The second tuple to compare.</source>
          <target state="translated">第二个元组进行比较。</target>
        </trans-unit>
        <trans-unit id="b310dae88696133dc7ed85369e494d665ee6beaf" translate="yes" xml:space="preserve">
          <source>The second variant is compatible with CuDNNGRU (GPU-only) and allows inference on CPU. Thus it has separate biases for &lt;code&gt;kernel&lt;/code&gt; and &lt;code&gt;recurrent_kernel&lt;/code&gt;. To use this variant, set &lt;code&gt;'reset_after'=True&lt;/code&gt; and &lt;code&gt;recurrent_activation='sigmoid'&lt;/code&gt;.</source>
          <target state="translated">第二种变体与CuDNNGRU（仅GPU）兼容，并允许在CPU上进行推理。因此，它对 &lt;code&gt;kernel&lt;/code&gt; 和 &lt;code&gt;recurrent_kernel&lt;/code&gt; 具有不同的偏见。要使用此变体，请设置 &lt;code&gt;'reset_after'=True&lt;/code&gt; 和 &lt;code&gt;recurrent_activation='sigmoid'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="44fd41a3b2fd550cb5e331da22b4349e8495f086" translate="yes" xml:space="preserve">
          <source>The second variant is compatible with CuDNNGRU (GPU-only) and allows inference on CPU. Thus it has separate biases for &lt;code&gt;kernel&lt;/code&gt; and &lt;code&gt;recurrent_kernel&lt;/code&gt;. Use &lt;code&gt;'reset_after'=True&lt;/code&gt; and &lt;code&gt;recurrent_activation='sigmoid'&lt;/code&gt;.</source>
          <target state="translated">第二种变体与CuDNNGRU（仅GPU）兼容，并允许在CPU上进行推理。因此，它对 &lt;code&gt;kernel&lt;/code&gt; 和 &lt;code&gt;recurrent_kernel&lt;/code&gt; 具有不同的偏见。使用 &lt;code&gt;'reset_after'=True&lt;/code&gt; 和 &lt;code&gt;recurrent_activation='sigmoid'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5ef054429664e25cebbffadf1c7499db194568c6" translate="yes" xml:space="preserve">
          <source>The second way is through a callable function that does not accept any arguments.</source>
          <target state="translated">第二种方式是通过一个不接受任何参数的可调用函数。</target>
        </trans-unit>
        <trans-unit id="9b1ff7b195e25bb1e934a1faddac3cb31e9dda14" translate="yes" xml:space="preserve">
          <source>The selected tensor.</source>
          <target state="translated">选定的张量。</target>
        </trans-unit>
        <trans-unit id="6d81fff9b6102438ec592417def3c2112c0846b7" translate="yes" xml:space="preserve">
          <source>The semantics of the input tensor depends on tensor_debug_mode. In typical usage, the input tensor comes directly from the user computation only when graph_debug_mode is FULL_TENSOR (see protobuf/debug_event.proto for a list of all the possible values of graph_debug_mode). For the other debug modes, the input tensor should be produced by an additional op or subgraph that computes summary information about one or more tensors.</source>
          <target state="translated">输入张量的语义取决于tensor_debug_mode。在典型的使用中,只有当graph_debug_mode为FULL_TENSOR时,输入张量才直接来自用户的计算(参见protobuf/debug_event.proto查看graph_debug_mode的所有可能值的列表)。对于其他的调试模式,输入的张量应该由一个额外的运算或子图产生,计算一个或多个张量的摘要信息。</target>
        </trans-unit>
        <trans-unit id="1450544c35748cfa003192a74e09ceb10f67e4f7" translate="yes" xml:space="preserve">
          <source>The separator string used between ngram elements. Must be a string constant, not a Tensor.</source>
          <target state="translated">ngram元素之间的分隔符.必须是一个字符串常数,而不是Tensor。</target>
        </trans-unit>
        <trans-unit id="c84f4757d0db527182bb06d2afdc87b1042d6fea" translate="yes" xml:space="preserve">
          <source>The sequence in which to look for prefix.</source>
          <target state="translated">寻找前缀的顺序。</target>
        </trans-unit>
        <trans-unit id="14448741add9013596276b373f9b2b4fe8b71b2b" translate="yes" xml:space="preserve">
          <source>The sequence of &lt;code&gt;Tensor&lt;/code&gt; objects representing the data inputs of this op.</source>
          <target state="translated">&lt;code&gt;Tensor&lt;/code&gt; 对象序列，表示此op的数据输入。</target>
        </trans-unit>
        <trans-unit id="ad56b1734b2aec94010201da62b3c74ae59652ee" translate="yes" xml:space="preserve">
          <source>The sequence that we are testing.</source>
          <target state="translated">我们正在测试的序列。</target>
        </trans-unit>
        <trans-unit id="9c721fd10cd508d808b1312a2194dc7e4890a7fc" translate="yes" xml:space="preserve">
          <source>The serialized &lt;code&gt;GraphDef&lt;/code&gt; can be imported into another &lt;code&gt;Graph&lt;/code&gt; (using &lt;a href=&quot;graph_util/import_graph_def&quot;&gt;&lt;code&gt;tf.import_graph_def&lt;/code&gt;&lt;/a&gt;) or used with the &lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/cc/index&quot;&gt;C++ Session API&lt;/a&gt;.</source>
          <target state="translated">序列化的 &lt;code&gt;GraphDef&lt;/code&gt; 可以导入到另一个 &lt;code&gt;Graph&lt;/code&gt; 中（使用&lt;a href=&quot;graph_util/import_graph_def&quot;&gt; &lt;code&gt;tf.import_graph_def&lt;/code&gt; &lt;/a&gt;）或与&lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/cc/index&quot;&gt;C ++ Session API一起使用&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="73859165e320015abdcf466de88e9720ebb22082" translate="yes" xml:space="preserve">
          <source>The serialized &lt;code&gt;GraphDef&lt;/code&gt; can be imported into another &lt;code&gt;Graph&lt;/code&gt; (using &lt;a href=&quot;graph_util/import_graph_def&quot;&gt;&lt;code&gt;tf.import_graph_def&lt;/code&gt;&lt;/a&gt;) or used with the &lt;a href=&quot;https://www.tensorflow.org/versions/r2.3/api_docs/api_docs/cc/index&quot;&gt;C++ Session API&lt;/a&gt;.</source>
          <target state="translated">序列化的 &lt;code&gt;GraphDef&lt;/code&gt; 可以导入到另一个 &lt;code&gt;Graph&lt;/code&gt; 中（使用&lt;a href=&quot;graph_util/import_graph_def&quot;&gt; &lt;code&gt;tf.import_graph_def&lt;/code&gt; &lt;/a&gt;）或与&lt;a href=&quot;https://www.tensorflow.org/versions/r2.3/api_docs/api_docs/cc/index&quot;&gt;C ++ Session API一起使用&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="766157bb0379c94a2b06eab4d5860dfc7335b141" translate="yes" xml:space="preserve">
          <source>The session to evaluate variables in. Ignored when executing eagerly. If not provided when graph building, the default session is used.</source>
          <target state="translated">要评估变量的会话。在急于执行时忽略。如果建图时没有提供,则使用默认会话。</target>
        </trans-unit>
        <trans-unit id="4a72456f672e80ba24848fd6a912b31242ea6b05" translate="yes" xml:space="preserve">
          <source>The session to use to evaluate this variable. If none, the default session is used.</source>
          <target state="translated">用于评估这个变量的会话。如果没有,则使用默认会话。</target>
        </trans-unit>
        <trans-unit id="ec73351af802bdf6746d29fb3d40b6d4c532bba4" translate="yes" xml:space="preserve">
          <source>The set of absent/default values may be specified using a vector of lengths or a padding value (but not both). If &lt;code&gt;lengths&lt;/code&gt; is specified, then the output tensor will satisfy &lt;code&gt;output[row] = tensor[row][:lengths[row]]&lt;/code&gt;. If 'lengths' is a list of lists or tuple of lists, those lists will be used as nested row lengths. If &lt;code&gt;padding&lt;/code&gt; is specified, then any row &lt;em&gt;suffix&lt;/em&gt; consisting entirely of &lt;code&gt;padding&lt;/code&gt; will be excluded from the returned &lt;code&gt;RaggedTensor&lt;/code&gt;. If neither &lt;code&gt;lengths&lt;/code&gt; nor &lt;code&gt;padding&lt;/code&gt; is specified, then the returned &lt;code&gt;RaggedTensor&lt;/code&gt; will have no absent/default values.</source>
          <target state="translated">可以使用长度向量或填充值（但不能同时使用两者）来指定缺少/默认值的集合。如果指定了 &lt;code&gt;lengths&lt;/code&gt; ，则输出张量将满足 &lt;code&gt;output[row] = tensor[row][:lengths[row]]&lt;/code&gt; 。如果&amp;ldquo;长度&amp;rdquo;是列表列表或列表元组，则这些列表将用作嵌套行长度。如果 &lt;code&gt;padding&lt;/code&gt; 被指定，则任一行&lt;em&gt;后缀&lt;/em&gt;完全由 &lt;code&gt;padding&lt;/code&gt; 将被排除从返回 &lt;code&gt;RaggedTensor&lt;/code&gt; 。如果既未指定 &lt;code&gt;lengths&lt;/code&gt; 也未指定 &lt;code&gt;padding&lt;/code&gt; ，则返回的 &lt;code&gt;RaggedTensor&lt;/code&gt; 将没有缺席/默认值。</target>
        </trans-unit>
        <trans-unit id="0c1258f12879c9bd1b917a210f7db35cdd522088" translate="yes" xml:space="preserve">
          <source>The set of ops to be run as part of the main op upon the load operation.</source>
          <target state="translated">负载操作时,作为主操作的一部分运行的一组操作。</target>
        </trans-unit>
        <trans-unit id="b8a942e9ca1e31cdbe28c47ba81e62ec5b4daae6" translate="yes" xml:space="preserve">
          <source>The set of variable names to convert (by default, all variables are converted).</source>
          <target state="translated">要转换的一组变量名称(默认情况下,所有变量都会被转换)。</target>
        </trans-unit>
        <trans-unit id="708ef0da29666e679f2d004edd19c8b18d49eb0b" translate="yes" xml:space="preserve">
          <source>The set of variable names to omit converting to constants.</source>
          <target state="translated">省略转换为常量的变量名的集合。</target>
        </trans-unit>
        <trans-unit id="4fc94cf8836a9ff2b4e0118d0144216371d4137e" translate="yes" xml:space="preserve">
          <source>The shape and dtype of any intermediate or output tensors in the computation of &lt;code&gt;fn&lt;/code&gt; should not depend on the input to &lt;code&gt;fn&lt;/code&gt;.</source>
          <target state="translated">在计算 &lt;code&gt;fn&lt;/code&gt; 时，任何中间或输出张量的形状和dtype不应取决于 &lt;code&gt;fn&lt;/code&gt; 的输入。</target>
        </trans-unit>
        <trans-unit id="915b2e710b3b023da7cd3921f1c34459542c194c" translate="yes" xml:space="preserve">
          <source>The shape for individual flat values in the &lt;code&gt;RaggedTensor&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;RaggedTensor&lt;/code&gt; 中单个平面值的形状。</target>
        </trans-unit>
        <trans-unit id="b1218538acb5cc473fe08e57d6e893aaca21938d" translate="yes" xml:space="preserve">
          <source>The shape format of the &lt;code&gt;inputs&lt;/code&gt; Tensors. If True, these &lt;code&gt;Tensors&lt;/code&gt; must be shaped &lt;code&gt;[max_time, batch_size, num_classes]&lt;/code&gt;. If False, these &lt;code&gt;Tensors&lt;/code&gt; must be shaped &lt;code&gt;[batch_size, max_time, num_classes]&lt;/code&gt;. Using &lt;code&gt;time_major = True&lt;/code&gt; (default) is a bit more efficient because it avoids transposes at the beginning of the ctc_loss calculation. However, most TensorFlow data is batch-major, so by this function also accepts inputs in batch-major form.</source>
          <target state="translated">&lt;code&gt;inputs&lt;/code&gt; 张量的形状格式。如果为True，则这些 &lt;code&gt;Tensors&lt;/code&gt; 必须为 &lt;code&gt;[max_time, batch_size, num_classes]&lt;/code&gt; 形状。如果为False，则这些 &lt;code&gt;Tensors&lt;/code&gt; 必须为 &lt;code&gt;[batch_size, max_time, num_classes]&lt;/code&gt; 形状。使用 &lt;code&gt;time_major = True&lt;/code&gt; （默认值）会更有效率，因为它避免了ctc_loss计算开始时的转置。但是，大多数TensorFlow数据都是批量生产的，因此通过此功能还可以接受批量生产形式的输入。</target>
        </trans-unit>
        <trans-unit id="cd03f61b772d8cecf2105a6fa68f431df30d2490" translate="yes" xml:space="preserve">
          <source>The shape format of the &lt;code&gt;inputs&lt;/code&gt; and &lt;code&gt;outputs&lt;/code&gt; Tensors. If true, these &lt;code&gt;Tensors&lt;/code&gt; must be shaped &lt;code&gt;[max_time, batch_size, depth]&lt;/code&gt;. If false, these &lt;code&gt;Tensors&lt;/code&gt; must be shaped &lt;code&gt;[batch_size, max_time, depth]&lt;/code&gt;. Using &lt;code&gt;time_major = True&lt;/code&gt; is a bit more efficient because it avoids transposes at the beginning and end of the RNN calculation. However, most TensorFlow data is batch-major, so by default this function accepts input and emits output in batch-major form.</source>
          <target state="translated">&lt;code&gt;inputs&lt;/code&gt; 和 &lt;code&gt;outputs&lt;/code&gt; 张量的形状格式。如果为真，这些 &lt;code&gt;Tensors&lt;/code&gt; 的形状必须 &lt;code&gt;[max_time, batch_size, depth]&lt;/code&gt; 。如果为false，则这些 &lt;code&gt;Tensors&lt;/code&gt; 必须为 &lt;code&gt;[batch_size, max_time, depth]&lt;/code&gt; 形状。使用 &lt;code&gt;time_major = True&lt;/code&gt; 会更有效率，因为它避免了RNN计算开始和结束时的转置。但是，大多数TensorFlow数据都是批量生产的，因此默认情况下，此函数接受输入并以批量生产的形式发出输出。</target>
        </trans-unit>
        <trans-unit id="e4fa5d6d5a2b3f7febe8c6a21da6334423de8f46" translate="yes" xml:space="preserve">
          <source>The shape format of the &lt;code&gt;inputs&lt;/code&gt; and &lt;code&gt;outputs&lt;/code&gt; tensors. If True, the inputs and outputs will be in shape &lt;code&gt;(timesteps, batch, ...)&lt;/code&gt;, whereas in the False case, it will be &lt;code&gt;(batch, timesteps, ...)&lt;/code&gt;. Using &lt;code&gt;time_major = True&lt;/code&gt; is a bit more efficient because it avoids transposes at the beginning and end of the RNN calculation. However, most TensorFlow data is batch-major, so by default this function accepts input and emits output in batch-major form.</source>
          <target state="translated">&lt;code&gt;inputs&lt;/code&gt; 和 &lt;code&gt;outputs&lt;/code&gt; 张量的形状格式。如果为True，则输入和输出的形状将为 &lt;code&gt;(timesteps, batch, ...)&lt;/code&gt; ，而在False情况下，其输入形状将为 &lt;code&gt;(batch, timesteps, ...)&lt;/code&gt; 。使用 &lt;code&gt;time_major = True&lt;/code&gt; 会更有效率，因为它避免了RNN计算开始和结束时的转置。但是，大多数TensorFlow数据都是批量生产的，因此默认情况下，此函数接受输入并以批量生产的形式发出输出。</target>
        </trans-unit>
        <trans-unit id="1ca9d07b430e38f671c59fb716e05624f9098d82" translate="yes" xml:space="preserve">
          <source>The shape format of the &lt;code&gt;inputs&lt;/code&gt; and &lt;code&gt;outputs&lt;/code&gt; tensors. If True, the inputs and outputs will be in shape &lt;code&gt;[timesteps, batch, feature]&lt;/code&gt;, whereas in the False case, it will be &lt;code&gt;[batch, timesteps, feature]&lt;/code&gt;. Using &lt;code&gt;time_major = True&lt;/code&gt; is a bit more efficient because it avoids transposes at the beginning and end of the RNN calculation. However, most TensorFlow data is batch-major, so by default this function accepts input and emits output in batch-major form.</source>
          <target state="translated">&lt;code&gt;inputs&lt;/code&gt; 和 &lt;code&gt;outputs&lt;/code&gt; 张量的形状格式。如果为True，则输入和输出将为 &lt;code&gt;[timesteps, batch, feature]&lt;/code&gt; 形状，而为False时，则为 &lt;code&gt;[batch, timesteps, feature]&lt;/code&gt; 形状。使用 &lt;code&gt;time_major = True&lt;/code&gt; 会更有效率，因为它避免了RNN计算开始和结束时的转置。但是，大多数TensorFlow数据都是批量生产的，因此默认情况下，此函数接受输入并以批量生产的形式发出输出。</target>
        </trans-unit>
        <trans-unit id="9705c68d850f3729f14c8b0e86c4e7a7c9b8a9c5" translate="yes" xml:space="preserve">
          <source>The shape inference functions propagate shapes to the extent possible:</source>
          <target state="translated">形状推理函数尽可能地传播形状。</target>
        </trans-unit>
        <trans-unit id="b9cb3dea60cfc74ebbae7817c2108c229a9e9930" translate="yes" xml:space="preserve">
          <source>The shape invariants for the loop variables.</source>
          <target state="translated">循环变量的形状不变量。</target>
        </trans-unit>
        <trans-unit id="081af2a7657fa36a381b1785e67936a8d191debb" translate="yes" xml:space="preserve">
          <source>The shape is computed using shape inference functions that are registered for each &lt;a href=&quot;operation&quot;&gt;&lt;code&gt;tf.Operation&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">使用为每个&lt;a href=&quot;operation&quot;&gt; &lt;code&gt;tf.Operation&lt;/code&gt; &lt;/a&gt;注册的形状推断函数来计算形状。</target>
        </trans-unit>
        <trans-unit id="7bc1bd93e711b8be01e066d838cfcf34409f0c38" translate="yes" xml:space="preserve">
          <source>The shape is computed using shape inference functions that are registered in the Op for each &lt;code&gt;Operation&lt;/code&gt;. See &lt;a href=&quot;tensorshape&quot;&gt;&lt;code&gt;tf.TensorShape&lt;/code&gt;&lt;/a&gt; for more details of what a shape represents.</source>
          <target state="translated">使用每个 &lt;code&gt;Operation&lt;/code&gt; 在Op中注册的形状推断函数计算形状。有关形状表示的更多详细信息，请参见&lt;a href=&quot;tensorshape&quot;&gt; &lt;code&gt;tf.TensorShape&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="5319f783b1231c02b92342d27602eef300f0a629" translate="yes" xml:space="preserve">
          <source>The shape of arguments to &lt;code&gt;__init__&lt;/code&gt;, &lt;code&gt;cdf&lt;/code&gt;, &lt;code&gt;log_cdf&lt;/code&gt;, &lt;code&gt;prob&lt;/code&gt;, and &lt;code&gt;log_prob&lt;/code&gt; reflect this broadcasting, as does the return value of &lt;code&gt;sample&lt;/code&gt; and &lt;code&gt;sample_n&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;__init__&lt;/code&gt; ， &lt;code&gt;cdf&lt;/code&gt; ， &lt;code&gt;log_cdf&lt;/code&gt; ， &lt;code&gt;prob&lt;/code&gt; 和 &lt;code&gt;log_prob&lt;/code&gt; 的参数的形状反映了这种广播， &lt;code&gt;sample&lt;/code&gt; 和 &lt;code&gt;sample_n&lt;/code&gt; 的返回值也是如此。</target>
        </trans-unit>
        <trans-unit id="c9539ceccae379560bc28e4de427220124dc1b7f" translate="yes" xml:space="preserve">
          <source>The shape of each element in the &lt;code&gt;TensorArray&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;TensorArray&lt;/code&gt; 中每个元素的形状。</target>
        </trans-unit>
        <trans-unit id="019adcd66453633d864b6a68c6e8a1ef2fb39d18" translate="yes" xml:space="preserve">
          <source>The shape of the &lt;code&gt;indices&lt;/code&gt; component, which indicates how many slices are in the &lt;code&gt;IndexedSlices&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;indices&lt;/code&gt; 组件的形状，它指示 &lt;code&gt;IndexedSlices&lt;/code&gt; 中有多少个切片。</target>
        </trans-unit>
        <trans-unit id="2a83199f6a8139b3838bc185a252e39d72f892d5" translate="yes" xml:space="preserve">
          <source>The shape of the RaggedTensor, or &lt;code&gt;None&lt;/code&gt; to allow any shape. If a shape is specified, then all ragged dimensions must have size &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">RaggedTensor的形状，或&amp;ldquo; &lt;code&gt;None&lt;/code&gt; 以允许任何形状。如果指定了形状，则所有参差不齐的尺寸都必须具有大小 &lt;code&gt;None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="aed84e9082260c1530ba06836ab583c0743099df" translate="yes" xml:space="preserve">
          <source>The shape of the elements of the given list, as a tensor.</source>
          <target state="translated">给定列表中元素的形状,作为张量。</target>
        </trans-unit>
        <trans-unit id="7ae769cd7a979ed862a9572cd30406fef8fee2dc" translate="yes" xml:space="preserve">
          <source>The shape of the input data per sequence id. E.g. if &lt;code&gt;shape=(2,)&lt;/code&gt;, each example must contain &lt;code&gt;2 * sequence_length&lt;/code&gt; values.</source>
          <target state="translated">每个序列ID的输入数据的形状。例如，如果 &lt;code&gt;shape=(2,)&lt;/code&gt; ，则每个示例必须包含 &lt;code&gt;2 * sequence_length&lt;/code&gt; 值。</target>
        </trans-unit>
        <trans-unit id="3720998b3d73539e8c9661d288914e2cdaa82ba7" translate="yes" xml:space="preserve">
          <source>The shape of the output tensor is:</source>
          <target state="translated">输出张量的形状是:</target>
        </trans-unit>
        <trans-unit id="b54a3296d32e764f7303629c8f4e133de6a9a7b0" translate="yes" xml:space="preserve">
          <source>The shape of the output will be:</source>
          <target state="translated">输出的形状将是。</target>
        </trans-unit>
        <trans-unit id="ac04b940314214f5e17141730326fbc95d00e6e3" translate="yes" xml:space="preserve">
          <source>The shape of the resulting dense tensor. In particular, &lt;code&gt;result.shape[i]&lt;/code&gt; is &lt;code&gt;shape[i]&lt;/code&gt; (if &lt;code&gt;shape[i]&lt;/code&gt; is not None), or &lt;code&gt;self.bounding_shape(i)&lt;/code&gt; (otherwise).&lt;code&gt;shape.rank&lt;/code&gt; must be &lt;code&gt;None&lt;/code&gt; or equal to &lt;code&gt;self.rank&lt;/code&gt;.</source>
          <target state="translated">生成的密集张量的形状。特别是， &lt;code&gt;result.shape[i]&lt;/code&gt; 是 &lt;code&gt;shape[i]&lt;/code&gt; （如果 &lt;code&gt;shape[i]&lt;/code&gt; 不是None），或者是 &lt;code&gt;self.bounding_shape(i)&lt;/code&gt; （否则）。 &lt;code&gt;shape.rank&lt;/code&gt; 必须为 &lt;code&gt;None&lt;/code&gt; 或等于 &lt;code&gt;self.rank&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2c9a810e455caa2b0b6f686bf9c200a962c54ef3" translate="yes" xml:space="preserve">
          <source>The shape of the state is algorithm-specific.</source>
          <target state="translated">状态的形状是特定的算法。</target>
        </trans-unit>
        <trans-unit id="3389c6b2701837ccb66fc16af6826d909f63b514" translate="yes" xml:space="preserve">
          <source>The shape of the tensor to be fed (optional). If the shape is not specified, you can feed a sparse tensor of any shape.</source>
          <target state="translated">要输入的张量的形状(可选)。如果没有指定形状,你可以输入任何形状的稀疏张量。</target>
        </trans-unit>
        <trans-unit id="376d2cc370b5ba232fdf37d951603c116bb04786" translate="yes" xml:space="preserve">
          <source>The shape of the tensor to be fed (optional). If the shape is not specified, you can feed a tensor of any shape.</source>
          <target state="translated">要输入的张量的形状(可选)。如果没有指定形状,你可以输入任何形状的张量。</target>
        </trans-unit>
        <trans-unit id="b04d4e03100c6f081b5e785ea772cca017db1933" translate="yes" xml:space="preserve">
          <source>The shapes of the two operands must match: broadcasting is not supported.</source>
          <target state="translated">两个操作数的形状必须匹配:不支持广播。</target>
        </trans-unit>
        <trans-unit id="348113de328d02812f9ca2ed7f6d52b929c19ce9" translate="yes" xml:space="preserve">
          <source>The simplest form of RNN network generated is:</source>
          <target state="translated">RNN网络生成的最简单形式是。</target>
        </trans-unit>
        <trans-unit id="ff2f171ddc511ae06abeb2ff57b6d8a5b4678a63" translate="yes" xml:space="preserve">
          <source>The simplest form of scatter is to insert individual elements in a tensor by index. For example, say we want to insert 4 scattered elements in a rank-1 tensor with 8 elements.</source>
          <target state="translated">最简单的散布形式是在张量中按索引插入单个元素。例如,假设我们想在一个有8个元素的rank-1张量中插入4个散点元素。</target>
        </trans-unit>
        <trans-unit id="f366cdfba69e2fc90954206f2645c89f45fbcd17" translate="yes" xml:space="preserve">
          <source>The simplest form of tensor_scatter_add is to add individual elements to a tensor by index. For example, say we want to add 4 elements in a rank-1 tensor with 8 elements.</source>
          <target state="translated">tensor_scatter_add最简单的形式是将单个元素按索引添加到一个张量中。例如,假设我们想在一个有8个元素的rank-1张量中添加4个元素。</target>
        </trans-unit>
        <trans-unit id="3af9f04f82cbb797b19d70da5fb65e10e6f6ef91" translate="yes" xml:space="preserve">
          <source>The simplest form of tensor_scatter_sub is to subtract individual elements from a tensor by index. For example, say we want to insert 4 scattered elements in a rank-1 tensor with 8 elements.</source>
          <target state="translated">tensor_scatter_sub最简单的形式是按索引从张量中减去单个元素。例如,假设我们想在一个有8个元素的rank-1张量中插入4个分散的元素。</target>
        </trans-unit>
        <trans-unit id="802d517fc471afa54d770d15cdbc678a89e0c7a9" translate="yes" xml:space="preserve">
          <source>The simplest version of &lt;code&gt;map_fn&lt;/code&gt; repeatedly applies the callable &lt;code&gt;fn&lt;/code&gt; to a sequence of elements from first to last. The elements are made of the tensors unpacked from &lt;code&gt;elems&lt;/code&gt;. &lt;code&gt;dtype&lt;/code&gt; is the data type of the return value of &lt;code&gt;fn&lt;/code&gt;. Users must provide &lt;code&gt;dtype&lt;/code&gt; if it is different from the data type of &lt;code&gt;elems&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;map_fn&lt;/code&gt; 的最简单版本将可调用 &lt;code&gt;fn&lt;/code&gt; 从头到尾重复应用于一系列元素。这些元素由从元 &lt;code&gt;elems&lt;/code&gt; 解压缩的张量组成。 &lt;code&gt;dtype&lt;/code&gt; 是 &lt;code&gt;fn&lt;/code&gt; 返回值的数据类型。用户必须提供 &lt;code&gt;dtype&lt;/code&gt; ，如果它是从数据类型不同 &lt;code&gt;elems&lt;/code&gt; 的。</target>
        </trans-unit>
        <trans-unit id="d7f6d20e0852a13ae739990f6fedc34688b995c0" translate="yes" xml:space="preserve">
          <source>The simplest version of &lt;code&gt;scan&lt;/code&gt; repeatedly applies the callable &lt;code&gt;fn&lt;/code&gt; to a sequence of elements from first to last. The elements are made of the tensors unpacked from &lt;code&gt;elems&lt;/code&gt; on dimension 0. The callable fn takes two tensors as arguments. The first argument is the accumulated value computed from the preceding invocation of fn, and the second is the value at the current position of &lt;code&gt;elems&lt;/code&gt;. If &lt;code&gt;initializer&lt;/code&gt; is None, &lt;code&gt;elems&lt;/code&gt; must contain at least one element, and its first element is used as the initializer.</source>
          <target state="translated">最简单的 &lt;code&gt;scan&lt;/code&gt; 版本将可调用 &lt;code&gt;fn&lt;/code&gt; 从头到尾重复应用于一系列元素。该元件是由从解压缩的张量的 &lt;code&gt;elems&lt;/code&gt; 的上尺寸0.可调用FN采用两个张量作为参数。第一个参数是从前一次调用fn计算得出的累加值，第二个参数是 &lt;code&gt;elems&lt;/code&gt; 当前位置的值。如果 &lt;code&gt;initializer&lt;/code&gt; 是None， &lt;code&gt;elems&lt;/code&gt; 的必须包含至少一个元件，并且它的第一元件被用作初始化。</target>
        </trans-unit>
        <trans-unit id="7a38981091aad7a606ea4d4d2a567d64ac905976" translate="yes" xml:space="preserve">
          <source>The simplest way to create a dataset is to create it from a python &lt;code&gt;list&lt;/code&gt;:</source>
          <target state="translated">创建数据集的最简单方法是从python &lt;code&gt;list&lt;/code&gt; 创建它：</target>
        </trans-unit>
        <trans-unit id="8c730f2755df0951b225a0138c6c5b2d885cf90a" translate="yes" xml:space="preserve">
          <source>The size of &lt;code&gt;tensor_names&lt;/code&gt; must match the number of tensors in &lt;code&gt;data&lt;/code&gt;. &lt;code&gt;data[i]&lt;/code&gt; is written to &lt;code&gt;filename&lt;/code&gt; with name &lt;code&gt;tensor_names[i]&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;tensor_names&lt;/code&gt; 的大小必须与 &lt;code&gt;data&lt;/code&gt; 中的张量数量匹配。 &lt;code&gt;data[i]&lt;/code&gt; 被写入名称为 &lt;code&gt;tensor_names[i]&lt;/code&gt; &lt;code&gt;filename&lt;/code&gt; 名中。</target>
        </trans-unit>
        <trans-unit id="ab51731901ff080d9eff54a48d6859d0f1323c1a" translate="yes" xml:space="preserve">
          <source>The size of the resulting dataset will match the size of the smallest input dataset, and no error will be raised if input datasets have different sizes.</source>
          <target state="translated">生成的数据集的大小将与最小的输入数据集的大小相匹配,如果输入数据集的大小不同,则不会出现错误。</target>
        </trans-unit>
        <trans-unit id="017e6c560666c8b8adcda856df4242248fd294ae" translate="yes" xml:space="preserve">
          <source>The sizes of the pooling regions are generated randomly but are fairly uniform. For example, let's look at the height dimension, and the constraints on the list of rows that will be pool boundaries.</source>
          <target state="translated">池化区域的尺寸是随机生成的,但相当统一。例如,让我们看看高度维度,以及对将成为池边界的行列表的约束。</target>
        </trans-unit>
        <trans-unit id="3c43eb48e9249082c6dab3dc37ecd5648014a1b5" translate="yes" xml:space="preserve">
          <source>The snapshot API allows users to transparently persist the output of their preprocessing pipeline to disk, and materialize the pre-processed data on a different training run.</source>
          <target state="translated">快照API允许用户透明地将其预处理管道的输出持久化到磁盘上,并在不同的训练运行中实现预处理数据的具体化。</target>
        </trans-unit>
        <trans-unit id="50a231b5d401442573df3e17bbb0c7335e8e5d44" translate="yes" xml:space="preserve">
          <source>The softmax of each vector x is calculated by &lt;code&gt;exp(x)/tf.reduce_sum(exp(x))&lt;/code&gt;. The input values in are the log-odds of the resulting probability.</source>
          <target state="translated">每个向量x的softmax由 &lt;code&gt;exp(x)/tf.reduce_sum(exp(x))&lt;/code&gt; 计算。输入值是结果概率的对数。</target>
        </trans-unit>
        <trans-unit id="887fc2159220b03235a84826c28912f3876b1988" translate="yes" xml:space="preserve">
          <source>The softmax of each vector x is computed as &lt;code&gt;exp(x) / tf.reduce_sum(exp(x))&lt;/code&gt;.</source>
          <target state="translated">每个向量x的softmax计算为 &lt;code&gt;exp(x) / tf.reduce_sum(exp(x))&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b6d8793457c083f0d11acb319bb58de8dcf61657" translate="yes" xml:space="preserve">
          <source>The softplus activation: &lt;code&gt;log(exp(x) + 1)&lt;/code&gt;.</source>
          <target state="translated">softplus激活： &lt;code&gt;log(exp(x) + 1)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="a35f61726860bd793a42b2269f7f12bfff5f5061" translate="yes" xml:space="preserve">
          <source>The softplus activation: &lt;code&gt;x / (abs(x) + 1)&lt;/code&gt;.</source>
          <target state="translated">softplus激活： &lt;code&gt;x / (abs(x) + 1)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="6cdf43d3dd10a8cc3aa784daa0a9e1312644b77d" translate="yes" xml:space="preserve">
          <source>The softsign activation: &lt;code&gt;x / (abs(x) + 1)&lt;/code&gt;.</source>
          <target state="translated">软签名激活： &lt;code&gt;x / (abs(x) + 1)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="6542d7e39f0c310801daa6538837ad6138ffae82" translate="yes" xml:space="preserve">
          <source>The solution is to ensure any access to the underlying resource &lt;code&gt;v&lt;/code&gt; is only processed through a critical section:</source>
          <target state="translated">解决方案是确保仅通过关键部分来处理对基础资源 &lt;code&gt;v&lt;/code&gt; 的任何访问：</target>
        </trans-unit>
        <trans-unit id="48c0d2fec393318121a3ea291f2dd8dccd7b38fa" translate="yes" xml:space="preserve">
          <source>The solution is to identify which gradient call this particular TensorArray gradient is being called in. This is performed by identifying a unique string (e.g. &quot;gradients&quot;, &quot;gradients_1&quot;, ...) from the input gradient Tensor's name. This string is used as a suffix when creating the TensorArray gradient object here (the attribute &lt;code&gt;source&lt;/code&gt;).</source>
          <target state="translated">解决方案是识别正在调用此特定TensorArray梯度的调用哪个梯度。这是通过从输入梯度Tensor的名称中标识唯一的字符串（例如，&amp;ldquo; gradients&amp;rdquo;，&amp;ldquo; gradients_1&amp;rdquo;，...）来执行的。在此处创建TensorArray渐变对象（属性 &lt;code&gt;source&lt;/code&gt; ）时，此字符串用作后缀。</target>
        </trans-unit>
        <trans-unit id="8a9c87c3bbd16608d71f6244bf102c899fcfe323" translate="yes" xml:space="preserve">
          <source>The solution is to wrap the model construction and execution in a keras-style scope:</source>
          <target state="translated">解决方法是将模型的构建和执行包裹在keras式的作用域中。</target>
        </trans-unit>
        <trans-unit id="e590f462584ca8d9fda3b6ea3ab70cb8db99fef3" translate="yes" xml:space="preserve">
          <source>The source of the non-determinism will be platform- and time-dependent.</source>
          <target state="translated">非决定性的来源将取决于平台和时间。</target>
        </trans-unit>
        <trans-unit id="7b94d68698f376ae68caf545ac2ad9ff65451535" translate="yes" xml:space="preserve">
          <source>The sparse implementation of this algorithm (used when the gradient is an IndexedSlices object, typically because of &lt;a href=&quot;../../../gather&quot;&gt;&lt;code&gt;tf.gather&lt;/code&gt;&lt;/a&gt; or an embedding lookup in the forward pass) does apply momentum to variable slices even if they were not used in the forward pass (meaning they have a gradient equal to zero). Momentum decay (beta1) is also applied to the entire momentum accumulator. This means that the sparse behavior is equivalent to the dense behavior (in contrast to some momentum implementations which ignore momentum unless a variable slice was actually used).</source>
          <target state="translated">此算法的稀疏实现（当渐变是IndexedSlices对象时使用，通常是由于&lt;a href=&quot;../../../gather&quot;&gt; &lt;code&gt;tf.gather&lt;/code&gt; &lt;/a&gt;或在前向遍历中嵌入查找而使用），即使在前向遍历中未使用可变切片也不会将动量应用于变量梯度等于零）。动量衰减（beta1）也适用于整个动量累加器。这意味着稀疏行为等同于密集行为（与某些动量实现相反，除非实际使用可变切片，否则某些动量实现会忽略动量）。</target>
        </trans-unit>
        <trans-unit id="2c28505e1992f5a2d21a399d52aaf1d49a6228f7" translate="yes" xml:space="preserve">
          <source>The sparse implementation of this algorithm (used when the gradient is an IndexedSlices object, typically because of &lt;a href=&quot;../../gather&quot;&gt;&lt;code&gt;tf.gather&lt;/code&gt;&lt;/a&gt; or an embedding lookup in the forward pass) does apply momentum to variable slices even if they were not used in the forward pass (meaning they have a gradient equal to zero). Momentum decay (beta1) is also applied to the entire momentum accumulator. This means that the sparse behavior is equivalent to the dense behavior (in contrast to some momentum implementations which ignore momentum unless a variable slice was actually used).</source>
          <target state="translated">此算法的稀疏实现（当渐变是IndexedSlices对象时使用，通常是由于&lt;a href=&quot;../../gather&quot;&gt; &lt;code&gt;tf.gather&lt;/code&gt; &lt;/a&gt;或在前向遍历中嵌入查找而使用），即使在前向遍历中未使用可变切片也不会将动量应用于变量梯度等于零）。动量衰减（beta1）也适用于整个动量累加器。这意味着稀疏行为等同于密集行为（与某些动量实现相反，除非实际使用可变切片，否则某些动量实现会忽略动量）。</target>
        </trans-unit>
        <trans-unit id="e5bdcabcaa194b28393438e39e1590c0f77ff2c8" translate="yes" xml:space="preserve">
          <source>The sparse matrix product may have numeric (non-structural) zeros.</source>
          <target state="translated">稀疏矩阵积可以有数字(非结构)零。</target>
        </trans-unit>
        <trans-unit id="dedfba1684c4375e13d104a45eb8d51735e2b834" translate="yes" xml:space="preserve">
          <source>The sparse tensor to convert. Must have rank 2.</source>
          <target state="translated">要转换的稀疏张量。必须有2级。</target>
        </trans-unit>
        <trans-unit id="b54e597f7fe46cc2667d4b65cc738c9c9db301ba" translate="yes" xml:space="preserve">
          <source>The specified output type of the operation (&lt;code&gt;int32&lt;/code&gt; or &lt;code&gt;int64&lt;/code&gt;). Defaults to &lt;a href=&quot;../tf#int32&quot;&gt;&lt;code&gt;tf.int32&lt;/code&gt;&lt;/a&gt;(optional).</source>
          <target state="translated">操作的指定输出类型（ &lt;code&gt;int32&lt;/code&gt; 或 &lt;code&gt;int64&lt;/code&gt; ）。默认为&lt;a href=&quot;../tf#int32&quot;&gt; &lt;code&gt;tf.int32&lt;/code&gt; &lt;/a&gt;（可选）。</target>
        </trans-unit>
        <trans-unit id="8b88004b19728b58b00753256fbdb980b735e8d2" translate="yes" xml:space="preserve">
          <source>The split indices for the ragged tensor value.</source>
          <target state="translated">粗糙张量值的分割指数。</target>
        </trans-unit>
        <trans-unit id="f49d81f2a44d35cf534ae7fd8c45debcfaf469ae" translate="yes" xml:space="preserve">
          <source>The split information is the best threshold (bucket id), gains and left/right node contributions per node for each feature.</source>
          <target state="translated">分割信息是每个特征的最佳阈值(bucket id)、收益和每个节点的左/右节点贡献。</target>
        </trans-unit>
        <trans-unit id="25fdcf5ad4499cdb6d4201c3ecb0186f730f52b9" translate="yes" xml:space="preserve">
          <source>The standard &lt;code&gt;segment_*&lt;/code&gt; functions assert that the segment indices are sorted. If you have unsorted indices use the equivalent &lt;code&gt;unsorted_segment_&lt;/code&gt; function. These functions take an additional argument &lt;code&gt;num_segments&lt;/code&gt; so that the output tensor can be efficiently allocated.</source>
          <target state="translated">标准的 &lt;code&gt;segment_*&lt;/code&gt; 函数断言，对分段索引进行了排序。如果您有未排序的索引，请使用等效的 &lt;code&gt;unsorted_segment_&lt;/code&gt; 函数。这些函数采用附加参数 &lt;code&gt;num_segments&lt;/code&gt; ,以便可以有效分配输出张量。</target>
        </trans-unit>
        <trans-unit id="b29667fc1f908940634484906fb6441d097a55ad" translate="yes" xml:space="preserve">
          <source>The standard &lt;code&gt;segment_*&lt;/code&gt; functions assert that the segment indices are sorted. If you have unsorted indices use the equivalent &lt;code&gt;unsorted_segment_&lt;/code&gt; function. Thses functions take an additional argument &lt;code&gt;num_segments&lt;/code&gt; so that the output tensor can be efficiently allocated.</source>
          <target state="translated">标准的 &lt;code&gt;segment_*&lt;/code&gt; 函数断言，分段索引已排序。如果您有未排序的索引，请使用等效的 &lt;code&gt;unsorted_segment_&lt;/code&gt; 函数。这些函数采用附加参数 &lt;code&gt;num_segments&lt;/code&gt; ,以便可以有效分配输出张量。</target>
        </trans-unit>
        <trans-unit id="6cf74e13a5dc987612c9d705626f37de29ad545f" translate="yes" xml:space="preserve">
          <source>The standard library uses various well-known names to collect and retrieve values associated with a graph. For example, the &lt;code&gt;tf.Optimizer&lt;/code&gt; subclasses default to optimizing the variables collected under &lt;code&gt;tf.GraphKeys.TRAINABLE_VARIABLES&lt;/code&gt; if none is specified, but it is also possible to pass an explicit list of variables.</source>
          <target state="translated">标准库使用各种众所周知的名称来收集和检索与图形关联的值。例如，如果未指定 &lt;code&gt;tf.Optimizer&lt;/code&gt; 子类，则默认情况下将优化在 &lt;code&gt;tf.GraphKeys.TRAINABLE_VARIABLES&lt;/code&gt; 下收集的变量，但也可以传递显式变量列表。</target>
        </trans-unit>
        <trans-unit id="ce8c36f190ecab29f1604c69bb1fdd2396d6f9bd" translate="yes" xml:space="preserve">
          <source>The standard pattern for updating variables is to:</source>
          <target state="translated">更新变量的标准模式是。</target>
        </trans-unit>
        <trans-unit id="1df550e9ac2ff1efc86381fe4007d40151e6c36c" translate="yes" xml:space="preserve">
          <source>The started thread is added to the list of threads managed by the supervisor so it does not need to be passed to the &lt;code&gt;stop()&lt;/code&gt; method.</source>
          <target state="translated">已启动的线程将添加到主管所管理的线程列表中，因此不需要将其传递给 &lt;code&gt;stop()&lt;/code&gt; 方法。</target>
        </trans-unit>
        <trans-unit id="22b0c0a61720b2b49f3bdb46b68ab2ffe3bd7116" translate="yes" xml:space="preserve">
          <source>The started thread.</source>
          <target state="translated">开始的线。</target>
        </trans-unit>
        <trans-unit id="04fb16ca663bd50a2194064568a76d4c96b84f17" translate="yes" xml:space="preserve">
          <source>The starting value for accumulators. Only zero or positive values are allowed.</source>
          <target state="translated">蓄能器的起始值。只允许零或正值。</target>
        </trans-unit>
        <trans-unit id="c25d85454a9ea543e59596d91af95b242b42d7a4" translate="yes" xml:space="preserve">
          <source>The state of the RNG after &lt;code&gt;rng_skip(n)&lt;/code&gt; will be the same as that after &lt;code&gt;stateful_uniform([n])&lt;/code&gt; (or any other distribution). The actual increment added to the counter is an unspecified implementation detail.</source>
          <target state="translated">&lt;code&gt;rng_skip(n)&lt;/code&gt; 之后的RNG状态将与 &lt;code&gt;stateful_uniform([n])&lt;/code&gt; （或任何其他分布）之后的状态相同。添加到计数器的实际增量是未指定的实现细节。</target>
        </trans-unit>
        <trans-unit id="dd546c154f1294509a35cef6daf7e05272419f55" translate="yes" xml:space="preserve">
          <source>The state of the optimizer, allowing to resume training exactly where you left off.</source>
          <target state="translated">优化器的状态,允许在你离开的地方继续训练。</target>
        </trans-unit>
        <trans-unit id="712f8a6f55bf7c5b4fd684dab030efe40f46ccee" translate="yes" xml:space="preserve">
          <source>The statically known shape of this ragged tensor.</source>
          <target state="translated">静态上已知这个粗糙张量的形状。</target>
        </trans-unit>
        <trans-unit id="05eef5966a4733b45681c7bb2c1cfa7fcd7e3ded" translate="yes" xml:space="preserve">
          <source>The statistics options associated with the dataset. See &lt;a href=&quot;experimental/statsoptions&quot;&gt;&lt;code&gt;tf.data.experimental.StatsOptions&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">与数据集关联的统计信息选项。有关更多详细信息，请参见&lt;a href=&quot;experimental/statsoptions&quot;&gt; &lt;code&gt;tf.data.experimental.StatsOptions&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="a8c6e46a80c3223660eded889659ea2c4138b476" translate="yes" xml:space="preserve">
          <source>The step set by &lt;a href=&quot;set_step&quot;&gt;&lt;code&gt;tf.summary.experimental.set_step()&lt;/code&gt;&lt;/a&gt; if one has been set, otherwise None.</source>
          <target state="translated">如果已设置，则由&lt;a href=&quot;set_step&quot;&gt; &lt;code&gt;tf.summary.experimental.set_step()&lt;/code&gt; &lt;/a&gt;设置的步骤，否则为None。</target>
        </trans-unit>
        <trans-unit id="5dff7b910b7ad17251b2c5a9c0f91aaacc64e975" translate="yes" xml:space="preserve">
          <source>The str() operator of a 'FlagValues' object provides help for all of the registered 'Flag' objects.</source>
          <target state="translated">FlagValues &quot;对象的str()操作符为所有注册的 &quot;Flag &quot;对象提供帮助。</target>
        </trans-unit>
        <trans-unit id="13be6068510e1bba9bcaba7458adf0809e711454" translate="yes" xml:space="preserve">
          <source>The strategy may choose to put the variable on multiple devices, like mirrored variables, but unlike mirrored variables we don't synchronize the updates to them to make sure they have the same value. Instead, the synchronization is performed when reading in cross-replica context. In a replica context, reads and writes are performed on the local copy (we allow reads so you can write code like &lt;code&gt;v = 0.9*v + 0.1*update&lt;/code&gt;). We don't allow operations like &lt;code&gt;v.assign_add&lt;/code&gt; in a cross-replica context for sync on read variables; right now we don't have a use case for such updates and depending on the aggregation mode such updates may not be sensible.</source>
          <target state="translated">该策略可能选择将变量放置在多个设备上，例如镜像变量，但是与镜像变量不同，我们不将更新同步到它们以确保它们具有相同的值。而是在跨副本上下文中读取时执行同步。在副本环境中，读写操作是在本地副本上执行的（我们允许读取操作，因此您可以编写类似 &lt;code&gt;v = 0.9*v + 0.1*update&lt;/code&gt; ）。我们不允许在跨副本上下文中使用 &lt;code&gt;v.assign_add&lt;/code&gt; 之类的操作来同步读取变量；目前，我们没有此类更新的用例，并且根据聚合模式，此类更新可能并不明智。</target>
        </trans-unit>
        <trans-unit id="d3fd5c3cf46ac06d0260b1f659bcc1a0b81cadee" translate="yes" xml:space="preserve">
          <source>The stream whose writes should be captured. This stream must have a file descriptor, support writing via using that file descriptor, and must have a &lt;code&gt;.flush()&lt;/code&gt; method.</source>
          <target state="translated">应捕获其写入的流。此流必须具有文件描述符，支持使用该文件描述符进行写入，并且必须具有 &lt;code&gt;.flush()&lt;/code&gt; 方法。</target>
        </trans-unit>
        <trans-unit id="644f3a81fca55d1677b34a373591148f236e0ab3" translate="yes" xml:space="preserve">
          <source>The string &quot;tensorflow&quot;.</source>
          <target state="translated">字符串 &quot;tensorflow&quot;。</target>
        </trans-unit>
        <trans-unit id="cc503fc832fae0582037d46d99a283b2143a5687" translate="yes" xml:space="preserve">
          <source>The string &lt;code&gt;-&lt;/code&gt; meaning that the slice covers all indices of this dimension</source>
          <target state="translated">字符串 &lt;code&gt;-&lt;/code&gt; 表示切片覆盖了该维度的所有索引</target>
        </trans-unit>
        <trans-unit id="8540ba2c73157ed83e7a8b8d1ba5f66009b9e41f" translate="yes" xml:space="preserve">
          <source>The string name of a job in this cluster.</source>
          <target state="translated">该集群中的作业的字符串名称。</target>
        </trans-unit>
        <trans-unit id="56132600b50269345758de3a44957eaf5ba36a10" translate="yes" xml:space="preserve">
          <source>The string name of the device to which this op has been assigned, or an empty string if it has not been assigned to a device.</source>
          <target state="translated">本操作被分配到的设备的字符串名称,如果没有分配到设备,则为空字符串。</target>
        </trans-unit>
        <trans-unit id="8992e1dfe2880c9a4de6beb1ac8e0230422a7a79" translate="yes" xml:space="preserve">
          <source>The string name of the underlying Queue.</source>
          <target state="translated">底层队列的字符串名称。</target>
        </trans-unit>
        <trans-unit id="a96aa4c9d9595a33bb0f9789d55b573d138edebb" translate="yes" xml:space="preserve">
          <source>The string name of this tensor.</source>
          <target state="translated">该张量的字符串名称。</target>
        </trans-unit>
        <trans-unit id="c67de356b037fd5e8be6d855afae37205aa00794" translate="yes" xml:space="preserve">
          <source>The string path to the exported directory or &lt;code&gt;None&lt;/code&gt; if export is skipped.</source>
          <target state="translated">导出目录的字符串路径；如果跳过导出，则为 &lt;code&gt;None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e5efe1c4c830550f08c9bea1e979d7c9a7668a12" translate="yes" xml:space="preserve">
          <source>The string path to the exported directory.</source>
          <target state="translated">输出目录的字符串路径。</target>
        </trans-unit>
        <trans-unit id="e6d17d88d9f96bec8835a8d4d00d7ee284382edf" translate="yes" xml:space="preserve">
          <source>The string representation of a persistent tensor handle.</source>
          <target state="translated">持久性张量句柄的字符串表示。</target>
        </trans-unit>
        <trans-unit id="a8d628e9b8c8f44ad4c03facb818cd42847e7780" translate="yes" xml:space="preserve">
          <source>The string to use to separate the inputs. Defaults to &quot; &quot;.</source>
          <target state="translated">用来分隔输入的字符串。默认值为&quot;&quot;。</target>
        </trans-unit>
        <trans-unit id="b4d0576ccdc2458c02c95d244e31139154833425" translate="yes" xml:space="preserve">
          <source>The string type of an operation. This corresponds to the &lt;code&gt;OpDef.name&lt;/code&gt; field for the proto that defines the operation.</source>
          <target state="translated">操作的字符串类型。这对应于定义操作的原型的 &lt;code&gt;OpDef.name&lt;/code&gt; 字段。</target>
        </trans-unit>
        <trans-unit id="a17bd3da8a69a3dada0980459ac9d927bbc1c342" translate="yes" xml:space="preserve">
          <source>The string we try to match with the items in regexes.</source>
          <target state="translated">我们试图与regexes中的项目进行匹配的字符串。</target>
        </trans-unit>
        <trans-unit id="bb20559dca432fe77c06ffe1211357d7726b77ab" translate="yes" xml:space="preserve">
          <source>The structure of the components of this optional.</source>
          <target state="translated">这个可选的组件的结构。</target>
        </trans-unit>
        <trans-unit id="636182e5686aa0b143279a1925716c9803dd42dc" translate="yes" xml:space="preserve">
          <source>The stubbing is using the builtin getattr and setattr. So, the &lt;strong&gt;get&lt;/strong&gt; and &lt;strong&gt;set&lt;/strong&gt; will be called when stubbing ( probably be to manipulate obj.&lt;strong&gt;dict&lt;/strong&gt; instead of getattr() and setattr()).</source>
          <target state="translated">存根使用内置的getattr和setattr。因此，在存根时将调用&lt;strong&gt;get&lt;/strong&gt;和&lt;strong&gt;set&lt;/strong&gt;（可能是操纵&lt;strong&gt;obj.dict&lt;/strong&gt;而不是getattr（）和setattr（））。</target>
        </trans-unit>
        <trans-unit id="10e607f82b834bedf8082612b2f31785cf1834ed" translate="yes" xml:space="preserve">
          <source>The stubbing is using the builtin getattr and setattr. So, the &lt;strong&gt;get&lt;/strong&gt; and &lt;strong&gt;set&lt;/strong&gt; will be called when stubbing (TODO: A better idea would probably be to manipulate obj.&lt;strong&gt;dict&lt;/strong&gt; instead of getattr() and setattr()).</source>
          <target state="translated">存根使用内置的getattr和setattr。因此，在存根时将调用&lt;strong&gt;get&lt;/strong&gt;和&lt;strong&gt;set&lt;/strong&gt;（TODO：一个更好的主意可能是操纵&lt;strong&gt;obj.dict&lt;/strong&gt;而不是getattr（）和setattr（））。</target>
        </trans-unit>
        <trans-unit id="e3a83e1ccdb64a4c6aff88c02b12dcc686b2bf3b" translate="yes" xml:space="preserve">
          <source>The suffix for the variable that keeps the gradient squared accumulator. If not present, defaults to name.</source>
          <target state="translated">保持梯度平方累积器的变量的后缀。如果不存在,默认为name。</target>
        </trans-unit>
        <trans-unit id="628f884b08eaa26944d28d228cad39f4b6e377f6" translate="yes" xml:space="preserve">
          <source>The suffix for the variable that keeps the linear gradient accumulator. If not present, defaults to name + &quot;&lt;em&gt;1&quot;. &lt;/em&gt;</source>
          <target state="translated">保留线性梯度累加器的变量的后缀。如果不存在，则默认为名称+&amp;ldquo; &lt;em&gt;1&amp;rdquo;。&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="0b87677b2e7091b18f84e0a5a53ce47bf2fbc17a" translate="yes" xml:space="preserve">
          <source>The sum of the squared distance from each point in the first batch of inputs to its nearest cluster center.</source>
          <target state="translated">第一批输入的每个点到其最近的簇中心的平方距离之和。</target>
        </trans-unit>
        <trans-unit id="b87e105d01203888c35de69e0d8cabac2d7238b3" translate="yes" xml:space="preserve">
          <source>The summary has up to &lt;code&gt;max_images&lt;/code&gt; summary values containing images. The images are built from &lt;code&gt;tensor&lt;/code&gt; which must be 4-D with shape &lt;code&gt;[batch_size, height, width, channels]&lt;/code&gt; and where &lt;code&gt;channels&lt;/code&gt; can be:</source>
          <target state="translated">摘要最多包含包含图像的 &lt;code&gt;max_images&lt;/code&gt; 摘要值。图像由 &lt;code&gt;tensor&lt;/code&gt; 构建，张量必须为4D，形状为 &lt;code&gt;[batch_size, height, width, channels]&lt;/code&gt; ， &lt;code&gt;channels&lt;/code&gt; 可以是：</target>
        </trans-unit>
        <trans-unit id="f3f45fd11021d0fbbacbdd0bd0b9e463916250a5" translate="yes" xml:space="preserve">
          <source>The summary has up to &lt;code&gt;max_outputs&lt;/code&gt; summary values containing audio. The audio is built from &lt;code&gt;tensor&lt;/code&gt; which must be 3-D with shape &lt;code&gt;[batch_size, frames, channels]&lt;/code&gt; or 2-D with shape &lt;code&gt;[batch_size, frames]&lt;/code&gt;. The values are assumed to be in the range of &lt;code&gt;[-1.0, 1.0]&lt;/code&gt; with a sample rate of &lt;code&gt;sample_rate&lt;/code&gt;.</source>
          <target state="translated">摘要最多包含音频的 &lt;code&gt;max_outputs&lt;/code&gt; 摘要值。音频是由 &lt;code&gt;tensor&lt;/code&gt; 构建的，张量必须为形状为 &lt;code&gt;[batch_size, frames, channels]&lt;/code&gt; 3-D或形状为 &lt;code&gt;[batch_size, frames]&lt;/code&gt; 2-D 。假设这些值在 &lt;code&gt;[-1.0, 1.0]&lt;/code&gt; 范围内，采样率为 &lt;code&gt;sample_rate&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4c8749a05b6c387d61ef6d60497d42b829e20f04" translate="yes" xml:space="preserve">
          <source>The summary has up to &lt;code&gt;max_outputs&lt;/code&gt; summary values containing images. The images are built from &lt;code&gt;tensor&lt;/code&gt; which must be 4-D with shape &lt;code&gt;[batch_size, height, width, channels]&lt;/code&gt; and where &lt;code&gt;channels&lt;/code&gt; can be:</source>
          <target state="translated">摘要最多包含包含图像的 &lt;code&gt;max_outputs&lt;/code&gt; 摘要值。图像由 &lt;code&gt;tensor&lt;/code&gt; 构建，张量必须为4D，形状为 &lt;code&gt;[batch_size, height, width, channels]&lt;/code&gt; ， &lt;code&gt;channels&lt;/code&gt; 可以是：</target>
        </trans-unit>
        <trans-unit id="45ad74c802e798c2d2739d6fd3d4c8a9a95f1067" translate="yes" xml:space="preserve">
          <source>The summary stats contains gradients and hessians accumulated for each node, bucket and dimension id.</source>
          <target state="translated">摘要统计包含了每个节点、桶和维度id积累的梯度和对数。</target>
        </trans-unit>
        <trans-unit id="3b4c4f21208a76ec80b0674286557919ec4d96bb" translate="yes" xml:space="preserve">
          <source>The summary stats contains gradients and hessians accumulated for each node, feature dimension id and bucket.</source>
          <target state="translated">摘要统计包含了每个节点、特征维度id和bucket积累的梯度和hesians。</target>
        </trans-unit>
        <trans-unit id="cdad9771b45b958101ba790c70a250dceed1de82" translate="yes" xml:space="preserve">
          <source>The summary stats contains gradients and hessians accumulated into the corresponding node and bucket for each example.</source>
          <target state="translated">摘要统计包含了每个例子积累到相应节点和桶中的梯度和恒定值。</target>
        </trans-unit>
        <trans-unit id="fa003ab56e1eb5342f9795fdbfba586bfe2f1900" translate="yes" xml:space="preserve">
          <source>The supervisor is notified of any exception raised by one of the services. After an exception is raised, &lt;code&gt;should_stop()&lt;/code&gt; returns &lt;code&gt;True&lt;/code&gt;. In that case the training loop should also stop. This is why the training loop has to check for &lt;code&gt;sv.should_stop()&lt;/code&gt;.</source>
          <target state="translated">如果其中一项服务引发任何异常，则会通知主管。引发异常后， &lt;code&gt;should_stop()&lt;/code&gt; 返回 &lt;code&gt;True&lt;/code&gt; 。在这种情况下，训练循环也应停止。这就是训练循环必须检查 &lt;code&gt;sv.should_stop()&lt;/code&gt; 的原因。</target>
        </trans-unit>
        <trans-unit id="50528fab5ce38e9d3b4fc6e5612ddf34e494bebf" translate="yes" xml:space="preserve">
          <source>The swish activation applied to &lt;code&gt;x&lt;/code&gt; (see reference paper for details).</source>
          <target state="translated">swish激活应用于 &lt;code&gt;x&lt;/code&gt; （有关详细信息，请参见参考文件）。</target>
        </trans-unit>
        <trans-unit id="c298d20320b239b254c85a68c7638e9b3d6999d9" translate="yes" xml:space="preserve">
          <source>The table initializer to use. See &lt;code&gt;HashTable&lt;/code&gt; kernel for supported key and value types.</source>
          <target state="translated">要使用的表初始化程序。有关支持的键和值类型，请参见 &lt;code&gt;HashTable&lt;/code&gt; 内核。</target>
        </trans-unit>
        <trans-unit id="b372ddff28f44f63c6e232f1c30d44e36e67c586" translate="yes" xml:space="preserve">
          <source>The table key dtype.</source>
          <target state="translated">表键dtype。</target>
        </trans-unit>
        <trans-unit id="92555821a111ce6704d66c3accb512005dcb54c2" translate="yes" xml:space="preserve">
          <source>The table to be initialized.</source>
          <target state="translated">要初始化的表格。</target>
        </trans-unit>
        <trans-unit id="284ce4f13ba5d1db3bbec94a52d693d3d2283853" translate="yes" xml:space="preserve">
          <source>The table to initialize.</source>
          <target state="translated">要初始化的表格。</target>
        </trans-unit>
        <trans-unit id="45ea7c0740084e6be8859e3f6c6a5a25999322a4" translate="yes" xml:space="preserve">
          <source>The table value dtype.</source>
          <target state="translated">表值dtype。</target>
        </trans-unit>
        <trans-unit id="8e9701fdc91c90f3e0a50b0354216b715e7e2b61" translate="yes" xml:space="preserve">
          <source>The tag name for this metadata.</source>
          <target state="translated">此元数据的标签名称。</target>
        </trans-unit>
        <trans-unit id="2e6370789b18c4cef6f8f91690bde182484827c1" translate="yes" xml:space="preserve">
          <source>The target value of comparison.</source>
          <target state="translated">比较的目标值。</target>
        </trans-unit>
        <trans-unit id="f6bc45ea60aa5301cecc35d1fbbe5b21036c5bda" translate="yes" xml:space="preserve">
          <source>The task index for this particular VM, within the GCE instance group. In particular, every single instance should be assigned a unique ordinal index within an instance group manually so that they can be distinguished from each other.</source>
          <target state="translated">在GCE实例组内,这个特定VM的任务索引。特别是,每个实例都应在实例组内手动分配一个唯一的序号,以便相互区分。</target>
        </trans-unit>
        <trans-unit id="39fc3d1afa951ce1fc0c6acdffe30bf9d05aaa6f" translate="yes" xml:space="preserve">
          <source>The task of an Enqueuer is to use parallelism to speed up preprocessing. This is done with processes or threads.</source>
          <target state="translated">Enqueuer的任务是利用并行性来加快预处理的速度。这是用进程或线程完成的。</target>
        </trans-unit>
        <trans-unit id="8d203e324545cf312d1311ac6ed39dbe8533902e" translate="yes" xml:space="preserve">
          <source>The temporary directory.</source>
          <target state="translated">临时目录。</target>
        </trans-unit>
        <trans-unit id="89760396be504814d1d16fffe685b4c4ed9d185b" translate="yes" xml:space="preserve">
          <source>The tensor &lt;code&gt;keys&lt;/code&gt; must be of the same type as the keys of the table. The tensor &lt;code&gt;values&lt;/code&gt; must be of the type of the table values.</source>
          <target state="translated">张量 &lt;code&gt;keys&lt;/code&gt; 的类型必须与表的键类型相同。张 &lt;code&gt;values&lt;/code&gt; 必须是表值的类型。</target>
        </trans-unit>
        <trans-unit id="ba629a56d84ff4136c8aeadbf5a5b5c5e14b4183" translate="yes" xml:space="preserve">
          <source>The tensor &lt;code&gt;keys&lt;/code&gt; must of the same type as the keys of the table. Keys not already in the table are silently ignored.</source>
          <target state="translated">张量 &lt;code&gt;keys&lt;/code&gt; 的类型必须与表的键相同。表中尚未存在的键将被静默忽略。</target>
        </trans-unit>
        <trans-unit id="784e799f8d450863febee3c764bd275192b18bd8" translate="yes" xml:space="preserve">
          <source>The tensor &lt;code&gt;keys&lt;/code&gt; must of the same type as the keys of the table. The output &lt;code&gt;values&lt;/code&gt; is of the type of the table values.</source>
          <target state="translated">张量 &lt;code&gt;keys&lt;/code&gt; 的类型必须与表的键相同。输出 &lt;code&gt;values&lt;/code&gt; 是表值的类型。</target>
        </trans-unit>
        <trans-unit id="ba8d97a1a9f6a392dad767f9f5f935fc3b942c5f" translate="yes" xml:space="preserve">
          <source>The tensor at index &lt;code&gt;index&lt;/code&gt;.</source>
          <target state="translated">索引 &lt;code&gt;index&lt;/code&gt; 处的张量。</target>
        </trans-unit>
        <trans-unit id="2316496cec696b46dadd19db7c5fa7648eaa2e07" translate="yes" xml:space="preserve">
          <source>The tensor for the keys.</source>
          <target state="translated">键的张量。</target>
        </trans-unit>
        <trans-unit id="0acc2ca709af133edb5533c7cf72d36eab353cfc" translate="yes" xml:space="preserve">
          <source>The tensor for the values.</source>
          <target state="translated">值的张量。</target>
        </trans-unit>
        <trans-unit id="1b8bac0316b66126d9a5f3d03ead635dc9c4e503" translate="yes" xml:space="preserve">
          <source>The tensor is shuffled along dimension 0, such that each &lt;code&gt;value[j]&lt;/code&gt; is mapped to one and only one &lt;code&gt;output[i]&lt;/code&gt;. For example, a mapping that might occur for a 3x2 tensor is:</source>
          <target state="translated">张量沿维度0混排，以便每个 &lt;code&gt;value[j]&lt;/code&gt; 映射到一个，只有一个 &lt;code&gt;output[i]&lt;/code&gt; 。例如，对于3x2张量可能发生的映射为：</target>
        </trans-unit>
        <trans-unit id="b2ebac51ffaed872fc6c58ab59a37320ff00d8b4" translate="yes" xml:space="preserve">
          <source>The tensor over which to pool. Must have rank 3.</source>
          <target state="translated">在其上汇集的张量。必须有3级。</target>
        </trans-unit>
        <trans-unit id="67d66a49c4d56f6740c34ad1abd2330c2a53d237" translate="yes" xml:space="preserve">
          <source>The tensor over which to pool. Must have rank 4.</source>
          <target state="translated">在其上汇集的张量。必须有4级。</target>
        </trans-unit>
        <trans-unit id="8d6751ac1718f3abff2c47f31b4ce194ddbe60a5" translate="yes" xml:space="preserve">
          <source>The tensor over which to pool. Must have rank 5.</source>
          <target state="translated">在其上汇集的张量。必须有5级。</target>
        </trans-unit>
        <trans-unit id="6ba96488585c7e70161cf56eacc0bc675e7c2bf5" translate="yes" xml:space="preserve">
          <source>The tensor returned by this operation is immutable.</source>
          <target state="translated">该操作返回的张量是不可改变的。</target>
        </trans-unit>
        <trans-unit id="374a08aed4e22028f8c4f1a7a5332f381742c546" translate="yes" xml:space="preserve">
          <source>The tensor to reduce. Should be of numeric type, &lt;code&gt;bool&lt;/code&gt;, or &lt;code&gt;string&lt;/code&gt;.</source>
          <target state="translated">张量减少。应该是数字类型， &lt;code&gt;bool&lt;/code&gt; 或 &lt;code&gt;string&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="af6cdbb07f5f2007c7664b9fc800d495933f2079" translate="yes" xml:space="preserve">
          <source>The tensor to reduce. Should have numeric type.</source>
          <target state="translated">要减少的张量。应该是数值型的。</target>
        </trans-unit>
        <trans-unit id="f76ee355f90903f0cc3c888c4e5cbb97172bd55d" translate="yes" xml:space="preserve">
          <source>The tensor to reduce. Should have real numeric type.</source>
          <target state="translated">要减少的张量。应该是实数型。</target>
        </trans-unit>
        <trans-unit id="c8f9620fdd17bd0ae7e2e11319433d1d510e064f" translate="yes" xml:space="preserve">
          <source>The tensor to reduce. Should have real or complex type.</source>
          <target state="translated">要减少的张量。应该是实型或复型。</target>
        </trans-unit>
        <trans-unit id="60a48c532dffc217f038e38531b427ddb3f91bda" translate="yes" xml:space="preserve">
          <source>The tensor to start from.</source>
          <target state="translated">要从张量开始。</target>
        </trans-unit>
        <trans-unit id="bc7b892140fc1cca57f3b6847beb68b7faa703c7" translate="yes" xml:space="preserve">
          <source>The tensor type for the result: one of &lt;code&gt;&quot;RaggedTensor&quot;&lt;/code&gt; or &lt;code&gt;&quot;SparseTensor&quot;&lt;/code&gt;.</source>
          <target state="translated">结果的张量类型： &lt;code&gt;&quot;RaggedTensor&quot;&lt;/code&gt; 或 &lt;code&gt;&quot;SparseTensor&quot;&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f1c73ec4612fec81093c6dca3603df5977d11e17" translate="yes" xml:space="preserve">
          <source>The tensor_shape to resize the input to.</source>
          <target state="translated">要调整输入大小的 tensor_shape。</target>
        </trans-unit>
        <trans-unit id="79aa982763ef4786022042e868a3556033a0c5ec" translate="yes" xml:space="preserve">
          <source>The tensors at corresponding positions in the three input lists (sample_indices, embedding_indices and aggregation_weights) must have the same shape, i.e. rank 1 with dim_size() equal to the total number of lookups into the table described by the corresponding feature.</source>
          <target state="translated">三个输入列表(sample_indices、embedding_indices和aggregation_weights)中相应位置上的时序必须具有相同的形状,即等级为1,dim_size()等于相应特征所描述的查表总数。</target>
        </trans-unit>
        <trans-unit id="9f41fba569c688c2ad85251ea878f2cfba46898a" translate="yes" xml:space="preserve">
          <source>The tensors at corresponding positions in the three input lists must have the same shape, i.e. rank 1 with dim_size() equal to the total number of lookups into the table described by the corresponding table_id.</source>
          <target state="translated">三个输入列表中相应位置的时序必须具有相同的形状,即等级为1,dim_size()等于相应table_id所描述的查入表的总次数。</target>
        </trans-unit>
        <trans-unit id="a5925b962b25493edc9ea9353afe8ff0989d3ebc" translate="yes" xml:space="preserve">
          <source>The tensors at corresponding positions in two of the input lists, embedding_indices and aggregation_weights, must have the same shape, i.e. rank 1 with dim_size() equal to the total number of lookups into the table described by the corresponding feature.</source>
          <target state="translated">embedding_indices和aggregation_weights这两个输入列表中对应位置的时序必须具有相同的形状,即等级为1,dim_size()等于相应特征所描述的查表总数。</target>
        </trans-unit>
        <trans-unit id="263280cf1684ff5fb9dc90aff492167c49c357e8" translate="yes" xml:space="preserve">
          <source>The tensors in the &lt;code&gt;TensorArray&lt;/code&gt; selected by &lt;code&gt;indices&lt;/code&gt;, packed into one tensor.</source>
          <target state="translated">通过 &lt;code&gt;indices&lt;/code&gt; 选择的 &lt;code&gt;TensorArray&lt;/code&gt; 中的张量，打包成一个张量。</target>
        </trans-unit>
        <trans-unit id="14d5f93ad26c02d132da873b0983deda8cc5ede0" translate="yes" xml:space="preserve">
          <source>The tensors returned by the callable identified by &lt;code&gt;branch_index&lt;/code&gt;, or those returned by &lt;code&gt;default&lt;/code&gt; if no key matches and &lt;code&gt;default&lt;/code&gt; was provided, or those returned by the max-keyed &lt;code&gt;branch_fn&lt;/code&gt; if no &lt;code&gt;default&lt;/code&gt; is provided.</source>
          <target state="translated">通过调用返回的张量鉴定 &lt;code&gt;branch_index&lt;/code&gt; ，或由返回 &lt;code&gt;default&lt;/code&gt; ，如果没有关键比赛和 &lt;code&gt;default&lt;/code&gt; 被提供，或者那些由最大键返回 &lt;code&gt;branch_fn&lt;/code&gt; 如果没有 &lt;code&gt;default&lt;/code&gt; 设置。</target>
        </trans-unit>
        <trans-unit id="0ea9d9d23c406f164fd2c56245bf91a63b2aa423" translate="yes" xml:space="preserve">
          <source>The tensors returned by the first pair whose predicate evaluated to True, or those returned by &lt;code&gt;default&lt;/code&gt; if none does.</source>
          <target state="translated">由第一对谓词评估为True的张量返回，或者如果没有，则 &lt;code&gt;default&lt;/code&gt; 返回的张量。</target>
        </trans-unit>
        <trans-unit id="62c909c71b66d6f5bfc11fa3e5279c6761971045" translate="yes" xml:space="preserve">
          <source>The tensors returned from &lt;code&gt;fn()&lt;/code&gt;.</source>
          <target state="translated">张量从 &lt;code&gt;fn()&lt;/code&gt; 返回。</target>
        </trans-unit>
        <trans-unit id="3ac69b5b20154b08a328b1d97feb7527a2ebab36" translate="yes" xml:space="preserve">
          <source>The tensors to print out if the condition is False. Defaults to error message and first few entries of &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">如果条件为False，则打印出张量。默认为错误消息和 &lt;code&gt;x&lt;/code&gt; ， &lt;code&gt;y&lt;/code&gt; 的前几个条目。</target>
        </trans-unit>
        <trans-unit id="de476c8a1f81cdd546d80c349f6129aaf3737e55" translate="yes" xml:space="preserve">
          <source>The tensors to print out if the condition is False. Defaults to error message and first few entries of &lt;code&gt;x&lt;/code&gt;.</source>
          <target state="translated">如果条件为False，则打印出张量。默认为错误消息和 &lt;code&gt;x&lt;/code&gt; 的前几个条目。</target>
        </trans-unit>
        <trans-unit id="d755f0b97c63b7eff4b6dbe33de5d6acf734d32c" translate="yes" xml:space="preserve">
          <source>The tensors to print out if the condition is False. Defaults to error message and first few entries of the violating tensor.</source>
          <target state="translated">如果条件为False,要打印出的张量。默认为错误信息和违规张量的前几个条目。</target>
        </trans-unit>
        <trans-unit id="41e71fc634a3e1532ea4fdaf908aef7f0776c5d2" translate="yes" xml:space="preserve">
          <source>The tensors to print out if the condition is False. Defaults to error message and the shape of &lt;code&gt;x&lt;/code&gt;.</source>
          <target state="translated">如果条件为False，则打印出张量。默认为错误消息和 &lt;code&gt;x&lt;/code&gt; 的形状。</target>
        </trans-unit>
        <trans-unit id="a3c87ea478ca9312b37363d8086e97ef6cac7d50" translate="yes" xml:space="preserve">
          <source>The tensors to print out when condition is false.</source>
          <target state="translated">当条件为假时,要打印出的张数。</target>
        </trans-unit>
        <trans-unit id="2f38446647c9bdb5f6c17c4098bf60d7f38504af" translate="yes" xml:space="preserve">
          <source>The tensors will be printed to the log, with &lt;code&gt;INFO&lt;/code&gt; severity. If you are not seeing the logs, you might want to add the following line after your imports:</source>
          <target state="translated">张量将以 &lt;code&gt;INFO&lt;/code&gt; 严重性打印到日志中。如果看不到日志，则可能要在导入后添加以下行：</target>
        </trans-unit>
        <trans-unit id="6922a3a9fd52a35b3e308cba8bec364b7d504e80" translate="yes" xml:space="preserve">
          <source>The tf.Graph in which tensors are looked up. If None, the current default graph is used.</source>
          <target state="translated">tf.Graph,在此图中查找张力。如果为None,则使用当前的默认图形。</target>
        </trans-unit>
        <trans-unit id="11bfdd9c3d39d4a32c1a375c3ce8bfd375bd0b18" translate="yes" xml:space="preserve">
          <source>The tf.tpu.Topology object for the topology of the TPU cluster.</source>
          <target state="translated">tf.tpu.Topology对象,用于TPU集群的拓扑结构。</target>
        </trans-unit>
        <trans-unit id="befdc8701902b7376e60ef81bed3d724c3bb2879" translate="yes" xml:space="preserve">
          <source>The the elements of the output vector are in range (0, 1) and sum to 1.</source>
          <target state="translated">输出向量的元素在(0,1)范围内,且和为1。</target>
        </trans-unit>
        <trans-unit id="e8a1b74b382d4bec7d008b052874fc8efea06387" translate="yes" xml:space="preserve">
          <source>The threading options associated with the dataset. See &lt;a href=&quot;experimental/threadingoptions&quot;&gt;&lt;code&gt;tf.data.experimental.ThreadingOptions&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">与数据集关联的线程选项。有关更多详细信息，请参见&lt;a href=&quot;experimental/threadingoptions&quot;&gt; &lt;code&gt;tf.data.experimental.ThreadingOptions&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="ed8f460f2dbd5613ea876b9d93397d3969bdc742" translate="yes" xml:space="preserve">
          <source>The thresholds used for evaluating AUC.</source>
          <target state="translated">用于评估AUC的阈值;</target>
        </trans-unit>
        <trans-unit id="4d718a76c8894d5837a42ef966440d43045adca5" translate="yes" xml:space="preserve">
          <source>The token representing an out-of-vocabulary value. Defaults to &quot;[UNK]&quot;.</source>
          <target state="translated">代表词汇外的值的标记。默认值为&quot;[UNK]&quot;。</target>
        </trans-unit>
        <trans-unit id="d3e4fd90c9e18807d72bafd4f4716080dd6bc34f" translate="yes" xml:space="preserve">
          <source>The total number of dimensions in a &lt;code&gt;RaggedTensor&lt;/code&gt; is called its &lt;em&gt;rank&lt;/em&gt;, and the number of ragged dimensions in a &lt;code&gt;RaggedTensor&lt;/code&gt; is called its &lt;em&gt;ragged-rank&lt;/em&gt;. A &lt;code&gt;RaggedTensor&lt;/code&gt;'s ragged-rank is fixed at graph creation time: it can't depend on the runtime values of &lt;code&gt;Tensor&lt;/code&gt;s, and can't vary dynamically for different session runs.</source>
          <target state="translated">&lt;code&gt;RaggedTensor&lt;/code&gt; 中的维度总数称为其&lt;em&gt;等级&lt;/em&gt;， &lt;code&gt;RaggedTensor&lt;/code&gt; 中的参差不齐维度的数量称为其Ragged &lt;em&gt;-rank&lt;/em&gt;。甲 &lt;code&gt;RaggedTensor&lt;/code&gt; 的参差不齐秩被固定在图创建时间：它可以不依赖于的运行时的值 &lt;code&gt;Tensor&lt;/code&gt; S，并不能针对不同的会话运行动态地变化。</target>
        </trans-unit>
        <trans-unit id="7366f216b0f07a06f98c4cfb72b6efd8edad2b69" translate="yes" xml:space="preserve">
          <source>The total variation is the sum of the absolute differences for neighboring pixel-values in the input images. This measures how much noise is in the images.</source>
          <target state="translated">总差异是输入图像中相邻像素值的绝对差异之和。这可以衡量图像中的噪声有多大。</target>
        </trans-unit>
        <trans-unit id="4cd5555b6721cf29a977af9cf8bb6114222ca75d" translate="yes" xml:space="preserve">
          <source>The total variation of &lt;code&gt;images&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;images&lt;/code&gt; 的总变化。</target>
        </trans-unit>
        <trans-unit id="b1be006e04763947d6204304381da8047ab403c6" translate="yes" xml:space="preserve">
          <source>The trace of input tensor.</source>
          <target state="translated">输入张量的轨迹。</target>
        </trans-unit>
        <trans-unit id="e883a1ea41a9a9c563ecd15d66e0cea02e0fd475" translate="yes" xml:space="preserve">
          <source>The transformation calls &lt;code&gt;reduce_func&lt;/code&gt; successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The &lt;code&gt;initial_state&lt;/code&gt; argument is used for the initial state and the final state is returned as the result.</source>
          <target state="translated">转换会在输入数据集的每个元素上依次调用 &lt;code&gt;reduce_func&lt;/code&gt; ，直到数据集用完为止，以其内部状态汇总信息。所述 &lt;code&gt;initial_state&lt;/code&gt; 参数被用于初始状态和最终状态作为结果返回。</target>
        </trans-unit>
        <trans-unit id="a76374d8db32ee668d78ea0cbd0b4fb2bd9d2baf" translate="yes" xml:space="preserve">
          <source>The transpose of &lt;code&gt;atrous_conv2d&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;atrous_conv2d&lt;/code&gt; 的转置。</target>
        </trans-unit>
        <trans-unit id="24fae844f58c6b39229d2519fcdd2125a204111b" translate="yes" xml:space="preserve">
          <source>The transpose of &lt;code&gt;conv1d&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;conv1d&lt;/code&gt; 的转置。</target>
        </trans-unit>
        <trans-unit id="e5584a45278a21e44f011bf21c57aa5501a09482" translate="yes" xml:space="preserve">
          <source>The transpose of &lt;code&gt;conv2d&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;conv2d&lt;/code&gt; 的转置。</target>
        </trans-unit>
        <trans-unit id="88458abd7b78099fd74249ee9fe81104261b47e7" translate="yes" xml:space="preserve">
          <source>The transpose of &lt;code&gt;conv3d&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;conv3d&lt;/code&gt; 的转置。</target>
        </trans-unit>
        <trans-unit id="4974672a5f0bdf2f1f4a3ac2ea5cc95568047305" translate="yes" xml:space="preserve">
          <source>The transpose of &lt;code&gt;convolution&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;convolution&lt;/code&gt; 的转置。</target>
        </trans-unit>
        <trans-unit id="1519a20ef4fe586180643d48497a05b2e0fad92f" translate="yes" xml:space="preserve">
          <source>The tuple of concatenated tensors that was dequeued.</source>
          <target state="translated">被dequeued的连词元组。</target>
        </trans-unit>
        <trans-unit id="947a12be95ae913828fb29965c58acb416f681d0" translate="yes" xml:space="preserve">
          <source>The tuple of tensors that was dequeued.</source>
          <target state="translated">被去掉的tensors元组。</target>
        </trans-unit>
        <trans-unit id="3ac8da8c53a0fe7729ca96dc99672e3deaee4160" translate="yes" xml:space="preserve">
          <source>The tutorials cover how to use &lt;a href=&quot;../../distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt; to do distributed training with native Keras APIs, custom training loops, and Esitmator APIs. They also cover how to save/load model when using &lt;a href=&quot;../../distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">这些教程介绍了如何使用&lt;a href=&quot;../../distribute/strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; &lt;/a&gt;通过本机Keras API，自定义训练循环和Esitmator API进行分布式训练。它们还介绍了使用&lt;a href=&quot;../../distribute/strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; &lt;/a&gt;时如何保存/加载模型。</target>
        </trans-unit>
        <trans-unit id="81263e37d187db94dad3d1abafbff5fd315364f1" translate="yes" xml:space="preserve">
          <source>The tutorials cover how to use &lt;a href=&quot;distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt; to do distributed training with native Keras APIs, custom training loops, and Esitmator APIs. They also cover how to save/load model when using &lt;a href=&quot;distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">这些教程介绍了如何使用&lt;a href=&quot;distribute/strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; &lt;/a&gt;通过本机Keras API，自定义训练循环和Esitmator API进行分布式训练。它们还介绍了使用&lt;a href=&quot;distribute/strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; &lt;/a&gt;时如何保存/加载模型。</target>
        </trans-unit>
        <trans-unit id="fcce8e7308a1f27fd06f367ce630638c93d290a0" translate="yes" xml:space="preserve">
          <source>The two arguments should be data trees consisting of trees of dicts and lists. They will be deeply compared by walking into the contents of dicts and lists; other items will be compared using the == operator. If the two structures differ in content, the failure message will indicate the location within the structures where the first difference is found. This may be helpful when comparing large structures.</source>
          <target state="translated">这两个参数应该是由dicts和list的树组成的数据树,它们将通过走进dicts和list的内容进行深度比较,其他项目将使用==操作符进行比较。它们将通过走进dicts和list的内容进行深度比较,其他项目将使用==操作符进行比较。如果两个结构在内容上有差异,失败消息将指出结构内发现第一个差异的位置。这在比较大型结构时可能会有帮助。</target>
        </trans-unit>
        <trans-unit id="c63ea59f28eeb3c5ef7f12904295f5acd2abba7d" translate="yes" xml:space="preserve">
          <source>The two optional lists, &lt;code&gt;shapes&lt;/code&gt; and &lt;code&gt;names&lt;/code&gt;, must be of the same length as &lt;code&gt;dtypes&lt;/code&gt; if provided. The values at a given index &lt;code&gt;i&lt;/code&gt; indicate the shape and name to use for the corresponding queue component in &lt;code&gt;dtypes&lt;/code&gt;.</source>
          <target state="translated">如果提供的话，两个可选列表 &lt;code&gt;shapes&lt;/code&gt; 和 &lt;code&gt;names&lt;/code&gt; 的长度必须与 &lt;code&gt;dtypes&lt;/code&gt; 相同。给定索引 &lt;code&gt;i&lt;/code&gt; 上的值指示 &lt;code&gt;dtypes&lt;/code&gt; 中用于相应队列组件的形状和名称。</target>
        </trans-unit>
        <trans-unit id="2509253fc683a9173057f5630f71d4971cacd9bd" translate="yes" xml:space="preserve">
          <source>The type of &lt;code&gt;values&lt;/code&gt; elements in the tensor to be fed.</source>
          <target state="translated">张量中 &lt;code&gt;values&lt;/code&gt; 元素的类型。</target>
        </trans-unit>
        <trans-unit id="7c1114eda45ad0aa9835c881c7ed41948119a5f6" translate="yes" xml:space="preserve">
          <source>The type of alpha, beta, and the output: &lt;code&gt;float16&lt;/code&gt;, &lt;code&gt;float32&lt;/code&gt;, or &lt;code&gt;float64&lt;/code&gt;.</source>
          <target state="translated">alpha，beta和输出的类型： &lt;code&gt;float16&lt;/code&gt; ， &lt;code&gt;float32&lt;/code&gt; 或 &lt;code&gt;float64&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="fcb9c58891241a3653f7ed23f1f4e9ce6c454d7f" translate="yes" xml:space="preserve">
          <source>The type of an element in the resulting &lt;code&gt;Tensor&lt;/code&gt;</source>
          <target state="translated">结果 &lt;code&gt;Tensor&lt;/code&gt; 元素的类型</target>
        </trans-unit>
        <trans-unit id="8cccbb2c85e31880b8682b2d0adfb113b78b64da" translate="yes" xml:space="preserve">
          <source>The type of compression for the record.</source>
          <target state="translated">记录的压缩类型。</target>
        </trans-unit>
        <trans-unit id="1b4dd7863bb3d0a86631168ad97e3c5ad4e8e603" translate="yes" xml:space="preserve">
          <source>The type of element in the resulting &lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="translated">结果 &lt;code&gt;Tensor&lt;/code&gt; 中元素的类型。</target>
        </trans-unit>
        <trans-unit id="94422215aeace7edcb47320386970071b5179d19" translate="yes" xml:space="preserve">
          <source>The type of elements for the returned &lt;code&gt;RaggedTensor&lt;/code&gt;. If not specified, then a default is chosen based on the scalar values in &lt;code&gt;pylist&lt;/code&gt;.</source>
          <target state="translated">返回的 &lt;code&gt;RaggedTensor&lt;/code&gt; 的元素类型。如果未指定，则根据 &lt;code&gt;pylist&lt;/code&gt; 中的标量值选择默认值。</target>
        </trans-unit>
        <trans-unit id="7a013615eda1f8f0ede1ff237cff4a624ad2cf8a" translate="yes" xml:space="preserve">
          <source>The type of elements in the tensor to be fed.</source>
          <target state="translated">要输入的张量元素的类型。</target>
        </trans-unit>
        <trans-unit id="ffc1d3e87a785b70bd5fa0c191f6b8f9a2dab411" translate="yes" xml:space="preserve">
          <source>The type of encoding for the file. Defaults to none.</source>
          <target state="translated">文件的编码类型。默认值为无。</target>
        </trans-unit>
        <trans-unit id="f00002f765021b74ae3866db29b33e5675d9c97a" translate="yes" xml:space="preserve">
          <source>The type of features. Only string and integer types are supported.</source>
          <target state="translated">特征的类型。只支持字符串和整数类型。</target>
        </trans-unit>
        <trans-unit id="405fc0a59f40a1f68ad3da9d2cbead3200ca93bb" translate="yes" xml:space="preserve">
          <source>The type of features. Only string and integer types are supported. If &lt;code&gt;None&lt;/code&gt;, it will be inferred from &lt;code&gt;vocabulary_list&lt;/code&gt;.</source>
          <target state="translated">功能类型。仅支持字符串和整数类型。如果为 &lt;code&gt;None&lt;/code&gt; ，它将从 &lt;code&gt;vocabulary_list&lt;/code&gt; 进行推断。</target>
        </trans-unit>
        <trans-unit id="0e39ef94a4f11a83b4f0f2df2ede7e0ef31ede65" translate="yes" xml:space="preserve">
          <source>The type of loss reduction used in the head.</source>
          <target state="translated">头中使用的减损类型。</target>
        </trans-unit>
        <trans-unit id="9e1c8635c975f4590fc90901ef34444f7e5713cc" translate="yes" xml:space="preserve">
          <source>The type of sharding that auto-shard should attempt. If this is set to FILE, then we will attempt to shard by files (each worker will get a set of files to process). If we cannot find a set of files to shard for at least one file per worker, we will error out. When this option is selected, make sure that you have enough files so that each worker gets at least one file. There will be a runtime error thrown if there are insufficient files. If this is set to DATA, then we will shard by elements produced by the dataset, and each worker will process the whole dataset and discard the portion that is not for itself. If this is set to OFF, then we will not autoshard, and each worker will receive a copy of the full dataset. This option is set to AUTO by default, AUTO will attempt to first shard by FILE, and fall back to sharding by DATA if we cannot find a set of files to shard.</source>
          <target state="translated">自动shard应该尝试的shard类型。如果设置为FILE,那么我们将尝试按文件进行碎片处理(每个worker将获得一组文件进行处理)。如果我们无法为每个 Worker 找到至少一个文件的文件集来进行碎片整理,我们就会出错。选择此选项时,请确保您有足够的文件,以便每个 Worker 至少得到一个文件。如果文件不足,会抛出一个运行时错误。如果将此选项设置为data,那么我们将按数据集产生的元素进行碎片处理,每个worker将处理整个数据集,并丢弃不属于自己的部分。如果将此选项设置为OFF,那么我们将不会自动shard,每个worker将收到完整数据集的副本。这个选项默认设置为AUTO,AUTO会首先尝试按FILE进行shard,如果我们找不到一组文件进行shard,就会退回到按DATA进行shard。</target>
        </trans-unit>
        <trans-unit id="1076c8ce1d97d2aae1dd52bec1be1eb1d7da0726" translate="yes" xml:space="preserve">
          <source>The type of the elements of the resulting tensor.</source>
          <target state="translated">张量元素的类型。</target>
        </trans-unit>
        <trans-unit id="4e8bb7b7574042b37e7c6afeec47ae6a75f4b513" translate="yes" xml:space="preserve">
          <source>The type of the elements of the resulting tensor. If not specified, then a value is chosen based on the other args.</source>
          <target state="translated">产生的张量元素的类型。如果没有指定,则根据其他参数选择一个值。</target>
        </trans-unit>
        <trans-unit id="aed6e689e6786fb190bf4135aa5cf05a31f4f0b3" translate="yes" xml:space="preserve">
          <source>The type of the event samples (default: int32).</source>
          <target state="translated">事件样本的类型(默认:int32)。</target>
        </trans-unit>
        <trans-unit id="5a0617255d963a3d9373238c1968c00abc24574b" translate="yes" xml:space="preserve">
          <source>The type of the event samples. &lt;code&gt;None&lt;/code&gt; implies no type-enforcement.</source>
          <target state="translated">事件样本的类型。 &lt;code&gt;None&lt;/code&gt; 表示没有类型强制。</target>
        </trans-unit>
        <trans-unit id="0e6ae3861b6b33f7202d6cb04f51d8a43f2ebb83" translate="yes" xml:space="preserve">
          <source>The type of the event samples. Default: &lt;code&gt;int32&lt;/code&gt;.</source>
          <target state="translated">事件样本的类型。默认： &lt;code&gt;int32&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e5ac8c9ef31246fab5b3b784f6fdb5ffec2b2eba" translate="yes" xml:space="preserve">
          <source>The type of the op (e.g. &lt;code&gt;&quot;MatMul&quot;&lt;/code&gt;).</source>
          <target state="translated">op的类型（例如 &lt;code&gt;&quot;MatMul&quot;&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="067097eae217bc20b817dae41bcd6adaa941c2bf" translate="yes" xml:space="preserve">
          <source>The type of the op that generated the tensor with bad numerics.</source>
          <target state="translated">生成不良数值张量的运算类型。</target>
        </trans-unit>
        <trans-unit id="08a9b8c589712a808dd41511f2b9583296c78486" translate="yes" xml:space="preserve">
          <source>The type of the output tensor.</source>
          <target state="translated">输出张量的类型。</target>
        </trans-unit>
        <trans-unit id="6ac7350774189c9f4f0f6d153487ae1c0df2dfef" translate="yes" xml:space="preserve">
          <source>The type of the output.</source>
          <target state="translated">输出的类型。</target>
        </trans-unit>
        <trans-unit id="da83b3257815d9c8a52c8c2fe774ab08e0d4a548" translate="yes" xml:space="preserve">
          <source>The type of the output. Default: tf.int32</source>
          <target state="translated">输出的类型,默认:tf.int32。默认值:tf.int32</target>
        </trans-unit>
        <trans-unit id="aa045fdeece95a6373419965f4b455817b39af60" translate="yes" xml:space="preserve">
          <source>The type of the output: &lt;code&gt;float16&lt;/code&gt;, &lt;code&gt;float32&lt;/code&gt;, &lt;code&gt;float64&lt;/code&gt;, &lt;code&gt;int32&lt;/code&gt; or &lt;code&gt;int64&lt;/code&gt;.</source>
          <target state="translated">输出的类型： &lt;code&gt;float16&lt;/code&gt; ， &lt;code&gt;float32&lt;/code&gt; ， &lt;code&gt;float64&lt;/code&gt; ， &lt;code&gt;int32&lt;/code&gt; 或 &lt;code&gt;int64&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f27b43df09bda0cc0e4bd074ae7a03dba23972ee" translate="yes" xml:space="preserve">
          <source>The type of the output: &lt;code&gt;float16&lt;/code&gt;, &lt;code&gt;float32&lt;/code&gt;, &lt;code&gt;float64&lt;/code&gt;, &lt;code&gt;int32&lt;/code&gt;, or &lt;code&gt;int64&lt;/code&gt;.</source>
          <target state="translated">输出的类型： &lt;code&gt;float16&lt;/code&gt; ， &lt;code&gt;float32&lt;/code&gt; ， &lt;code&gt;float64&lt;/code&gt; ， &lt;code&gt;int32&lt;/code&gt; 或 &lt;code&gt;int64&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f8cc4d05affc30251cb4e72ec85d99c502c39e2b" translate="yes" xml:space="preserve">
          <source>The type of the output: &lt;code&gt;float16&lt;/code&gt;, &lt;code&gt;float32&lt;/code&gt;, &lt;code&gt;float64&lt;/code&gt;, &lt;code&gt;int32&lt;/code&gt;, or &lt;code&gt;int64&lt;/code&gt;. For unbounded uniform ints (&lt;code&gt;minval&lt;/code&gt;, &lt;code&gt;maxval&lt;/code&gt; both &lt;code&gt;None&lt;/code&gt;), &lt;code&gt;uint32&lt;/code&gt; and &lt;code&gt;uint64&lt;/code&gt; may be used.</source>
          <target state="translated">输出的类型： &lt;code&gt;float16&lt;/code&gt; ， &lt;code&gt;float32&lt;/code&gt; ， &lt;code&gt;float64&lt;/code&gt; ， &lt;code&gt;int32&lt;/code&gt; 或 &lt;code&gt;int64&lt;/code&gt; 。对于无界统一整数（ &lt;code&gt;minval&lt;/code&gt; ， &lt;code&gt;maxval&lt;/code&gt; 均为 &lt;code&gt;None&lt;/code&gt; ），可以使用 &lt;code&gt;uint32&lt;/code&gt; 和 &lt;code&gt;uint64&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5b2e4f649b30bc99091c2da410cb8448c8ba873a" translate="yes" xml:space="preserve">
          <source>The type of the this &lt;code&gt;LinearOperator&lt;/code&gt;. Arguments to &lt;code&gt;matmul&lt;/code&gt; and &lt;code&gt;solve&lt;/code&gt; will have to be this type.</source>
          <target state="translated">此 &lt;code&gt;LinearOperator&lt;/code&gt; 的类型。 &lt;code&gt;matmul&lt;/code&gt; 和 &lt;code&gt;solve&lt;/code&gt; 的论点必须是这种类型。</target>
        </trans-unit>
        <trans-unit id="74e0737155eb6c648136eb7aa61f1f5f53eaee2c" translate="yes" xml:space="preserve">
          <source>The type of the variable. Defaults to &lt;code&gt;self.dtype&lt;/code&gt; or &lt;code&gt;float32&lt;/code&gt;.</source>
          <target state="translated">变量的类型。默认为 &lt;code&gt;self.dtype&lt;/code&gt; 或 &lt;code&gt;float32&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="89eb9b54cfc1eb5edbdf1804af86410f57c03f40" translate="yes" xml:space="preserve">
          <source>The type of values.</source>
          <target state="translated">值的类型。</target>
        </trans-unit>
        <trans-unit id="cc791524ffdf48de9024b17fb2e81e72b1743abc" translate="yes" xml:space="preserve">
          <source>The type specification of an element of &lt;a href=&quot;distributediterator&quot;&gt;&lt;code&gt;tf.distribute.DistributedIterator&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;distributediterator&quot;&gt; &lt;code&gt;tf.distribute.DistributedIterator&lt;/code&gt; &lt;/a&gt;元素的类型说明。</target>
        </trans-unit>
        <trans-unit id="586865cdc2362857bdac73e4d9d4aafd45add2e6" translate="yes" xml:space="preserve">
          <source>The type specification of an element of this &lt;a href=&quot;distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">此&lt;a href=&quot;distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt;的元素的类型规范。</target>
        </trans-unit>
        <trans-unit id="9f4d546108b4584207f8473dd28294356ddeede5" translate="yes" xml:space="preserve">
          <source>The type specification of an element of this dataset.</source>
          <target state="translated">该数据集元素的类型规范。</target>
        </trans-unit>
        <trans-unit id="31f5cdd5cadead57bcfc00f19cdc637b778a8dfa" translate="yes" xml:space="preserve">
          <source>The type specification of an element of this iterator.</source>
          <target state="translated">该迭代器元素的类型规范。</target>
        </trans-unit>
        <trans-unit id="31e1b2499af19cb707e87dbc9b32089b65e02169" translate="yes" xml:space="preserve">
          <source>The type specification of an element of this optional.</source>
          <target state="translated">该可选元素的类型规范。</target>
        </trans-unit>
        <trans-unit id="bce40406c73aa0072207096be99446cc37cd3334" translate="yes" xml:space="preserve">
          <source>The types of the tensors in &lt;code&gt;values&lt;/code&gt; must match the schema for the fields specified in &lt;code&gt;field_names&lt;/code&gt;. All the tensors in &lt;code&gt;values&lt;/code&gt; must have a common shape prefix, &lt;em&gt;batch_shape&lt;/em&gt;.</source>
          <target state="translated">&lt;code&gt;values&lt;/code&gt; 中的张量类型必须与 &lt;code&gt;field_names&lt;/code&gt; 中指定的字段的模式匹配。 &lt;code&gt;values&lt;/code&gt; 所有张量必须具有公共的形状前缀&lt;em&gt;batch_shape&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="e5381b2e579e07c748270f78ee35dd566e3eb442" translate="yes" xml:space="preserve">
          <source>The typical scenario for &lt;code&gt;ExponentialMovingAverage&lt;/code&gt; is to compute moving averages of variables during training, and restore the variables from the computed moving averages during evaluations.</source>
          <target state="translated">&lt;code&gt;ExponentialMovingAverage&lt;/code&gt; 的典型方案是在训练过程中计算变量的移动平均值，并在评估过程中从计算出的移动平均值中恢复变量。</target>
        </trans-unit>
        <trans-unit id="3da77fbceb0e840441ce8c39192555e97fa2928b" translate="yes" xml:space="preserve">
          <source>The underlying accumulator reference.</source>
          <target state="translated">基本的累加器参考。</target>
        </trans-unit>
        <trans-unit id="c25ef637b6290d9c594d8a4f22f2ec949aa1bd01" translate="yes" xml:space="preserve">
          <source>The underlying queue reference.</source>
          <target state="translated">底层队列引用。</target>
        </trans-unit>
        <trans-unit id="e903c075a80c182337230b41e8fc05a166057ed0" translate="yes" xml:space="preserve">
          <source>The unique &lt;code&gt;frame_name&lt;/code&gt; is used by the &lt;code&gt;Executor&lt;/code&gt; to identify frames. If &lt;code&gt;is_constant&lt;/code&gt; is true, &lt;code&gt;output&lt;/code&gt; is a constant in the child frame; otherwise it may be changed in the child frame. At most &lt;code&gt;parallel_iterations&lt;/code&gt; iterations are run in parallel in the child frame.</source>
          <target state="translated">独特的 &lt;code&gt;frame_name&lt;/code&gt; 用于由 &lt;code&gt;Executor&lt;/code&gt; 识别帧。如果 &lt;code&gt;is_constant&lt;/code&gt; 为true，则在子帧中 &lt;code&gt;output&lt;/code&gt; 为常量；否则，输出为false 。否则可能会在子框架中更改。最多 &lt;code&gt;parallel_iterations&lt;/code&gt; 迭代在子框架中并行运行。</target>
        </trans-unit>
        <trans-unit id="2570833b8fed9aa7ee0f6db077c7e434ab3d563b" translate="yes" xml:space="preserve">
          <source>The unpacked tuple, with &lt;code&gt;None&lt;/code&gt;s for &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;sample_weight&lt;/code&gt; if they are not provided.</source>
          <target state="translated">未打包的元组，如果未提供，则 &lt;code&gt;y&lt;/code&gt; 和 &lt;code&gt;sample_weight&lt;/code&gt; 分别为 &lt;code&gt;None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d25a2d45745d4308947942d9bddb3fe51f90e219" translate="yes" xml:space="preserve">
          <source>The update rule for &lt;code&gt;variable&lt;/code&gt; with gradient &lt;code&gt;g&lt;/code&gt; uses an optimization described at the end of section 2 of the paper:</source>
          <target state="translated">具有梯度 &lt;code&gt;g&lt;/code&gt; 的 &lt;code&gt;variable&lt;/code&gt; 的更新规则使用了本文第2节末尾描述的优化：</target>
        </trans-unit>
        <trans-unit id="dcaa87717690c893cddcbb06c2774de0d640ef88" translate="yes" xml:space="preserve">
          <source>The update rule for &lt;code&gt;variable&lt;/code&gt; with gradient &lt;code&gt;g&lt;/code&gt; uses an optimization described at the end of section 7.1 of the paper:</source>
          <target state="translated">梯度为 &lt;code&gt;g&lt;/code&gt; 的 &lt;code&gt;variable&lt;/code&gt; 的更新规则使用本文第7.1节末尾描述的优化：</target>
        </trans-unit>
        <trans-unit id="0efd76819f39cf3bcb5dd14bc65d8c8c72cef23d" translate="yes" xml:space="preserve">
          <source>The update rule for parameter &lt;code&gt;w&lt;/code&gt; with gradient &lt;code&gt;g&lt;/code&gt; is described at the end of section 7.1 of the paper:</source>
          <target state="translated">梯度为 &lt;code&gt;g&lt;/code&gt; 的参数 &lt;code&gt;w&lt;/code&gt; 的更新规则在本文第7.1节的末尾进行了描述：</target>
        </trans-unit>
        <trans-unit id="1a9901f8d5a239b98ad1f04b6bbdac6a25b5dbe0" translate="yes" xml:space="preserve">
          <source>The updated config has something needed to run a strategy, e.g. configuration to run collective ops, or device filters to improve distributed training performance.</source>
          <target state="translated">更新后的配置有一些运行策略所需要的东西,例如运行集体行动的配置,或者提高分布式训练性能的设备过滤器。</target>
        </trans-unit>
        <trans-unit id="60dd7fe2afadb5341f05d411ef207c2859229807" translate="yes" xml:space="preserve">
          <source>The updated copy of the &lt;code&gt;config_proto&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;config_proto&lt;/code&gt; 的更新副本。</target>
        </trans-unit>
        <trans-unit id="7d9e39fd60700bb347ed0039e54d9af3ef5a5100" translate="yes" xml:space="preserve">
          <source>The updated decorator. If decorator_func is not a tf_decorator, new_target is returned.</source>
          <target state="translated">更新后的decorator。如果 decorator_func 不是 tf_decorator,则返回 new_target。</target>
        </trans-unit>
        <trans-unit id="fa6ecd5172af3b96400529c50d0c7eb9f4adaa33" translate="yes" xml:space="preserve">
          <source>The updated variable.</source>
          <target state="translated">更新后的变量。</target>
        </trans-unit>
        <trans-unit id="bb0ce841932dc29d8f332a4acc96a1313d2133b7" translate="yes" xml:space="preserve">
          <source>The updated variable. If &lt;code&gt;read_value&lt;/code&gt; is false, instead returns None in Eager mode and the assign op in graph mode.</source>
          <target state="translated">更新的变量。如果 &lt;code&gt;read_value&lt;/code&gt; 为false，则在Eager模式下返回None，在图形模式下返回assign op。</target>
        </trans-unit>
        <trans-unit id="5d8daec55baf3c813ce2f2fc4a5eab596ea78fbf" translate="yes" xml:space="preserve">
          <source>The upper regularized incomplete Gamma function is defined as:</source>
          <target state="translated">上正则化不完全Gamma函数定义为:</target>
        </trans-unit>
        <trans-unit id="a16fb2ccec6d9e76f7b18eacf6123674146710e7" translate="yes" xml:space="preserve">
          <source>The user could also use &lt;code&gt;make_input_fn_iterator&lt;/code&gt; if they want to customize which input is fed to which replica/worker etc.</source>
          <target state="translated">如果用户要自定义将哪个输入馈送到哪个副本/工作人员等，则还可以使用 &lt;code&gt;make_input_fn_iterator&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ab789081815c49957f21bd695c2a3924b82c20ff" translate="yes" xml:space="preserve">
          <source>The user is given the option of raising an exception or returning &lt;code&gt;NaN&lt;/code&gt;.</source>
          <target state="translated">用户可以选择引发异常或返回 &lt;code&gt;NaN&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2d78e502adf2b1729b9f368b8135c69dbb97bac0" translate="yes" xml:space="preserve">
          <source>The usual cross-entropy cost is defined as:</source>
          <target state="translated">通常的交叉熵成本定义为:。</target>
        </trans-unit>
        <trans-unit id="55894b0693429bc52d5ca851ab5e7e50a45cb526" translate="yes" xml:space="preserve">
          <source>The valid keyword arguments in kwds are:</source>
          <target state="translated">kwds中有效的关键字参数是:。</target>
        </trans-unit>
        <trans-unit id="2e9dcb12c3ec9343c2b4d76ebee61442f18c708d" translate="yes" xml:space="preserve">
          <source>The value &lt;code&gt;delta&lt;/code&gt; is added to all components of the tensor &lt;code&gt;image&lt;/code&gt;. &lt;code&gt;image&lt;/code&gt; is converted to &lt;code&gt;float&lt;/code&gt; and scaled appropriately if it is in fixed-point representation, and &lt;code&gt;delta&lt;/code&gt; is converted to the same data type. For regular images, &lt;code&gt;delta&lt;/code&gt; should be in the range &lt;code&gt;[0,1)&lt;/code&gt;, as it is added to the image in floating point representation, where pixel values are in the &lt;code&gt;[0,1)&lt;/code&gt; range.</source>
          <target state="translated">值 &lt;code&gt;delta&lt;/code&gt; 将添加到张量 &lt;code&gt;image&lt;/code&gt; 所有分量。 &lt;code&gt;image&lt;/code&gt; 是定点表示形式，则将图像转换为 &lt;code&gt;float&lt;/code&gt; 并适当缩放，并将 &lt;code&gt;delta&lt;/code&gt; 转换为相同的数据类型。对于常规图像， &lt;code&gt;delta&lt;/code&gt; 应在 &lt;code&gt;[0,1)&lt;/code&gt; 范围内，因为它以浮点表示形式添加到图像中，其中像素值在 &lt;code&gt;[0,1)&lt;/code&gt; 范围内。</target>
        </trans-unit>
        <trans-unit id="a0cc40289aaf5ca63368c701a28edf746620f686" translate="yes" xml:space="preserve">
          <source>The value of &lt;code&gt;self.value &amp;gt; other.value&lt;/code&gt; if both are known, otherwise None.</source>
          <target state="translated">如果 &lt;code&gt;self.value &amp;gt; other.value&lt;/code&gt; 的值都已知，则为 None。</target>
        </trans-unit>
        <trans-unit id="28eee150e3c68dabbd6edcffc1df83846a31e392" translate="yes" xml:space="preserve">
          <source>The value of &lt;code&gt;self.value &amp;gt;= other.value&lt;/code&gt; if both are known, otherwise None.</source>
          <target state="translated">如果 &lt;code&gt;self.value &amp;gt;= other.value&lt;/code&gt; 的值都已知，则为 None。</target>
        </trans-unit>
        <trans-unit id="1ac388cf68a7917e640172de08cfda57213cf191" translate="yes" xml:space="preserve">
          <source>The value of &lt;code&gt;self.value &amp;lt; other.value&lt;/code&gt; if both are known, otherwise None.</source>
          <target state="translated">如果 &lt;code&gt;self.value &amp;lt; other.value&lt;/code&gt; 的值都是已知的，否则为None。</target>
        </trans-unit>
        <trans-unit id="b01cea77304d81c49aa6bfe991a86b9994cc913b" translate="yes" xml:space="preserve">
          <source>The value of &lt;code&gt;self.value &amp;lt;= other.value&lt;/code&gt; if both are known, otherwise None.</source>
          <target state="translated">如果 &lt;code&gt;self.value &amp;lt;= other.value&lt;/code&gt; 的值都是已知的，否则为None。</target>
        </trans-unit>
        <trans-unit id="97a93b3e6e5220aba9e50c43c510d2bbbc1a87e0" translate="yes" xml:space="preserve">
          <source>The value of such a flag is a list that contains the individual values from all the appearances of that flag on the command-line.</source>
          <target state="translated">这种标志的值是一个列表,其中包含了该标志在命令行上所有出现的单个值。</target>
        </trans-unit>
        <trans-unit id="a54451b41292d69deaf5cd8ce0771065680d134d" translate="yes" xml:space="preserve">
          <source>The value of the attr, as a Python object.</source>
          <target state="translated">attr的值,作为一个Python对象。</target>
        </trans-unit>
        <trans-unit id="6e7716dffe6353c157e4a2c3f2e655d155be6b4b" translate="yes" xml:space="preserve">
          <source>The value of the flag is always a list, even if the option was only supplied once, and even if the default value is a single value</source>
          <target state="translated">标志的值总是一个列表,即使选项只提供了一次,即使默认值是一个单一的值也是如此</target>
        </trans-unit>
        <trans-unit id="f28d312af2bea03bbf52afb2a569177bf8ca705a" translate="yes" xml:space="preserve">
          <source>The value of the flag, empty if the flag is not defined.</source>
          <target state="translated">标志的值,如果没有定义标志,则为空。</target>
        </trans-unit>
        <trans-unit id="bf8b3cb3b48dec54fd1680c59ba7cefb3dfc3602" translate="yes" xml:space="preserve">
          <source>The value of the variable after the update.</source>
          <target state="translated">更新后的变量值。</target>
        </trans-unit>
        <trans-unit id="d29c684e1056225784225bef1dc61fddd3c411d6" translate="yes" xml:space="preserve">
          <source>The value of this dimension, or None if it is unknown.</source>
          <target state="translated">该维度的值,如果未知,则为 &quot;无&quot;。</target>
        </trans-unit>
        <trans-unit id="dc40d49743dfe15cf0fa34edb80ecfa5afa70d65" translate="yes" xml:space="preserve">
          <source>The value or values returned by &lt;code&gt;map_func&lt;/code&gt; determine the structure of each element in the returned dataset.</source>
          <target state="translated">&lt;code&gt;map_func&lt;/code&gt; 返回的一个或多个值确定返回的数据集中每个元素的结构。</target>
        </trans-unit>
        <trans-unit id="277f6e101a6f4f13a9cc5f1fee440972491084f4" translate="yes" xml:space="preserve">
          <source>The value representing an out-of-vocabulary value. Defaults to -1.</source>
          <target state="translated">代表词汇外的值。默认值为-1。</target>
        </trans-unit>
        <trans-unit id="27a9bdb78e692ccca6e6eeacfe9cf8259c37e6d7" translate="yes" xml:space="preserve">
          <source>The value returned by &lt;code&gt;run()&lt;/code&gt; has the same shape as the &lt;code&gt;fetches&lt;/code&gt; argument, where the leaves are replaced by the corresponding values returned by TensorFlow.</source>
          <target state="translated">&lt;code&gt;run()&lt;/code&gt; 返回的值与 &lt;code&gt;fetches&lt;/code&gt; 参数具有相同的形状，其中叶子被TensorFlow返回的相应值替换。</target>
        </trans-unit>
        <trans-unit id="6455c45d9b49b1bb8f4ec9b5dff00531b9342ec2" translate="yes" xml:space="preserve">
          <source>The value returned by the &lt;code&gt;activity_regularizer&lt;/code&gt; is divided by the input batch size so that the relative weighting between the weight regularizers and the activity regularizers does not change with the batch size.</source>
          <target state="translated">&lt;code&gt;activity_regularizer&lt;/code&gt; 返回的值除以输入的批次大小，以使权重调整器和活动调整器之间的相对权重不会随批次大小而变化。</target>
        </trans-unit>
        <trans-unit id="e67ea60a1d86085a3a91a8df093188ea7d78231f" translate="yes" xml:space="preserve">
          <source>The value returned by this operation is guaranteed to be influenced by all the writes on which this operation depends directly or indirectly, and to not be influenced by any of the writes which depend directly or indirectly on this operation.</source>
          <target state="translated">保证该操作返回的值受到该操作直接或间接依赖的所有写的影响,不受任何直接或间接依赖该操作的写的影响。</target>
        </trans-unit>
        <trans-unit id="ca8a88dac26c441fe66426635dcf33f3ecddba85" translate="yes" xml:space="preserve">
          <source>The value to add to the collection.</source>
          <target state="translated">要添加到收藏中的价值。</target>
        </trans-unit>
        <trans-unit id="67488a526901c9a4f2769ad17a3b244c04775dbb" translate="yes" xml:space="preserve">
          <source>The value to add to the collections.</source>
          <target state="translated">要添加到收藏中的价值。</target>
        </trans-unit>
        <trans-unit id="0467f1eec9e713645aba39b382503cafa6ae6856" translate="yes" xml:space="preserve">
          <source>The value to fill for empty rows, with the same type as &lt;code&gt;sp_input.&lt;/code&gt;</source>
          <target state="translated">与 &lt;code&gt;sp_input.&lt;/code&gt; 相同类型的用于填充空行的值。</target>
        </trans-unit>
        <trans-unit id="6c9729e11273c726456e8e0c3c8e270a5166729e" translate="yes" xml:space="preserve">
          <source>The value to fill the area outside the specified diagonal band with. Default is 0.</source>
          <target state="translated">填充指定对角线带外的区域的值。默认值为0。</target>
        </trans-unit>
        <trans-unit id="53f548425b8c43bbee75c542b6da5327e5f5f507" translate="yes" xml:space="preserve">
          <source>The value to use if a key is missing in the table.</source>
          <target state="translated">如果表中缺少一个键,要使用的值。</target>
        </trans-unit>
        <trans-unit id="46cf38cbb0627d0763f0f1b40f1bcbf960cd2dde" translate="yes" xml:space="preserve">
          <source>The values generated are similar to values from a &lt;a href=&quot;randomnormal&quot;&gt;&lt;code&gt;tf.keras.initializers.RandomNormal&lt;/code&gt;&lt;/a&gt; initializer except that values more than two standard deviations from the mean are discarded and re-drawn.</source>
          <target state="translated">生成的值与&lt;a href=&quot;randomnormal&quot;&gt; &lt;code&gt;tf.keras.initializers.RandomNormal&lt;/code&gt; &lt;/a&gt;初始值设定项中的值相似，不同之处在于，与平均值相差两个以上标准差的值将被丢弃并重新绘制。</target>
        </trans-unit>
        <trans-unit id="c61a71eb4077cf30110803e381657708b49ae932" translate="yes" xml:space="preserve">
          <source>The values must include 0. There can be no duplicate values or negative values.</source>
          <target state="translated">值必须包括0。不能有重复值或负值。</target>
        </trans-unit>
        <trans-unit id="1050986413ba8aa09be08e1ada6c99bfa286c827" translate="yes" xml:space="preserve">
          <source>The values not defined in &lt;code&gt;sp_input&lt;/code&gt; don't participate in the reduce max, as opposed to be implicitly assumed 0 -- hence it can return negative values for sparse &lt;code&gt;axis&lt;/code&gt;. But, in case there are no values in &lt;code&gt;axis&lt;/code&gt;, it will reduce to 0. See second example below.</source>
          <target state="translated">与隐式假定为0相反，未在 &lt;code&gt;sp_input&lt;/code&gt; 中定义的值不参与reduce max，因此它可以为稀疏 &lt;code&gt;axis&lt;/code&gt; 返回负值。但是，如果 &lt;code&gt;axis&lt;/code&gt; 中没有值，它将减少为0。请参见下面的第二个示例。</target>
        </trans-unit>
        <trans-unit id="15f0cbbec41c8cec7a1600faa3fd0ba6c8267aeb" translate="yes" xml:space="preserve">
          <source>The values not defined in &lt;code&gt;sp_input&lt;/code&gt; don't participate in the reduce max, as opposed to be implicitly assumed 0 -- hence it can return negative values for sparse &lt;code&gt;reduction_axes&lt;/code&gt;. But, in case there are no values in &lt;code&gt;reduction_axes&lt;/code&gt;, it will reduce to 0. See second example below.</source>
          <target state="translated">与隐式假定为0相反，在 &lt;code&gt;sp_input&lt;/code&gt; 中未定义的值不参与reduce max，因此它可以为稀疏 &lt;code&gt;reduction_axes&lt;/code&gt; 返回负值。但是，如果 &lt;code&gt;reduction_axes&lt;/code&gt; 中没有值，它将减少为0。请参见下面的第二个示例。</target>
        </trans-unit>
        <trans-unit id="01b28056c2bd0c58a2e89c42fd350d1a9e497580" translate="yes" xml:space="preserve">
          <source>The values of &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;scale&lt;/code&gt; are chosen so that the mean and variance of the inputs are preserved between two consecutive layers as long as the weights are initialized correctly (see &lt;a href=&quot;../initializers/lecun_normal&quot;&gt;&lt;code&gt;lecun_normal&lt;/code&gt; initialization&lt;/a&gt;) and the number of inputs is &quot;large enough&quot; (see references for more information).</source>
          <target state="translated">选择 &lt;code&gt;alpha&lt;/code&gt; 和 &lt;code&gt;scale&lt;/code&gt; 的值，以便在权重正确初始化（请参阅&lt;a href=&quot;../initializers/lecun_normal&quot;&gt; &lt;code&gt;lecun_normal&lt;/code&gt; 初始化&lt;/a&gt;）并且输入的数量&amp;ldquo;足够大&amp;rdquo;（请参见参考资料）的同时，在两个连续的层之间保留输入的均值和方差。更多信息）。</target>
        </trans-unit>
        <trans-unit id="64ea94b6b8cce4972e532c22a39ade0ab1c04bbd" translate="yes" xml:space="preserve">
          <source>The values of &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;scale&lt;/code&gt; are chosen so that the mean and variance of the inputs are preserved between two consecutive layers as long as the weights are initialized correctly (see &lt;a href=&quot;../initializers/lecunnormal&quot;&gt;&lt;code&gt;tf.keras.initializers.LecunNormal&lt;/code&gt;&lt;/a&gt; initializer) and the number of input units is &quot;large enough&quot; (see reference paper for more information).</source>
          <target state="translated">选择 &lt;code&gt;alpha&lt;/code&gt; 和 &lt;code&gt;scale&lt;/code&gt; 的值，以便只要正确初始化权重（请参阅&lt;a href=&quot;../initializers/lecunnormal&quot;&gt; &lt;code&gt;tf.keras.initializers.LecunNormal&lt;/code&gt; &lt;/a&gt;初始值设定项）并且输入单位数为&amp;ldquo;足够大&amp;rdquo;（有关更多信息，请参见参考文件）。</target>
        </trans-unit>
        <trans-unit id="f733a02fe3b03768ab1ca9ec4da3805a1ad4b791" translate="yes" xml:space="preserve">
          <source>The values of &lt;code&gt;value&lt;/code&gt; are assigned to the positions in the tensor &lt;code&gt;input&lt;/code&gt; that are selected by the slice parameters. The slice parameters &lt;code&gt;begin&lt;/code&gt;&lt;code&gt;end&lt;/code&gt;&lt;code&gt;strides&lt;/code&gt; etc. work exactly as in &lt;code&gt;StridedSlice&lt;/code&gt;.</source>
          <target state="translated">值的 &lt;code&gt;value&lt;/code&gt; 分配给张量 &lt;code&gt;input&lt;/code&gt; 中由切片参数选择的位置。切片参数 &lt;code&gt;begin&lt;/code&gt; &lt;code&gt;end&lt;/code&gt; &lt;code&gt;strides&lt;/code&gt; 究竟等方面的工作，如 &lt;code&gt;StridedSlice&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f39cb348464cd40ba244caddc3b9020cc754ba4b" translate="yes" xml:space="preserve">
          <source>The values of &lt;code&gt;value&lt;/code&gt; are assigned to the positions in the variable &lt;code&gt;ref&lt;/code&gt; that are selected by the slice parameters. The slice parameters &lt;code&gt;begin,&lt;/code&gt;end&lt;code&gt;,&lt;/code&gt;strides&lt;code&gt;, etc. work exactly as in&lt;/code&gt;StridedSlice`.</source>
          <target state="translated">值的 &lt;code&gt;value&lt;/code&gt; 分配给变量 &lt;code&gt;ref&lt;/code&gt; 中由切片参数选择的位置。切片参数 &lt;code&gt;begin,&lt;/code&gt; 结束 &lt;code&gt;,&lt;/code&gt; 进展 &lt;code&gt;, etc. work exactly as in&lt;/code&gt; StridedSlice`。</target>
        </trans-unit>
        <trans-unit id="1f2d5c98a3c37925f071ae528d2a1426fcf8d79c" translate="yes" xml:space="preserve">
          <source>The values of &lt;code&gt;value&lt;/code&gt; are assigned to the positions in the variable &lt;code&gt;ref&lt;/code&gt; that are selected by the slice parameters. The slice parameters &lt;code&gt;begin&lt;/code&gt;, &lt;code&gt;end&lt;/code&gt;, &lt;code&gt;strides&lt;/code&gt;, etc. work exactly as in &lt;code&gt;StridedSlice&lt;/code&gt;.</source>
          <target state="translated">值的 &lt;code&gt;value&lt;/code&gt; 分配给变量 &lt;code&gt;ref&lt;/code&gt; 中由切片参数选择的位置。切片参数的 &lt;code&gt;begin&lt;/code&gt; ， &lt;code&gt;end&lt;/code&gt; ， &lt;code&gt;strides&lt;/code&gt; 等的工作方式与 &lt;code&gt;StridedSlice&lt;/code&gt; 中的完全相同。</target>
        </trans-unit>
        <trans-unit id="823a169cfff816da0e0ed79e7aa0fca33f12184b" translate="yes" xml:space="preserve">
          <source>The values to be used in the operation.</source>
          <target state="translated">操作中要使用的值。</target>
        </trans-unit>
        <trans-unit id="340b53ddcc61e1600456fa6a942b3a9db9dcd7f4" translate="yes" xml:space="preserve">
          <source>The variable &lt;code&gt;x&lt;/code&gt; updated.</source>
          <target state="translated">变量 &lt;code&gt;x&lt;/code&gt; 已更新。</target>
        </trans-unit>
        <trans-unit id="9b98520ded5c59855ccbb91e15451f218e4fed8a" translate="yes" xml:space="preserve">
          <source>The variable corresponding to &lt;code&gt;input_&lt;/code&gt; or None</source>
          <target state="translated">对应于 &lt;code&gt;input_&lt;/code&gt; 或None的变量</target>
        </trans-unit>
        <trans-unit id="b58067393f30493e389388e4808eed6d24490c13" translate="yes" xml:space="preserve">
          <source>The variable dtype of this policy, or None if the variable dtype should be inferred from the inputs.</source>
          <target state="translated">此策略的变量d类型,如果变量d类型应从输入中推断,则为None。</target>
        </trans-unit>
        <trans-unit id="bb822034c75bacc81fb53dec6e4b25c54f49ac48" translate="yes" xml:space="preserve">
          <source>The variable dtype of this policy.</source>
          <target state="translated">该政策的变量dtype。</target>
        </trans-unit>
        <trans-unit id="cf801794eaaba6ef2f8f7f433ed37a63d7d2327d" translate="yes" xml:space="preserve">
          <source>The variance for Student's T equals</source>
          <target state="translated">学生的T的方差等于</target>
        </trans-unit>
        <trans-unit id="d641778ea2685b7b78fd228569c0026aa6abdc9c" translate="yes" xml:space="preserve">
          <source>The vocabulary file name.</source>
          <target state="translated">词汇文件名。</target>
        </trans-unit>
        <trans-unit id="cde4cff8a5ecda6c3308cd587b5940a27adec93c" translate="yes" xml:space="preserve">
          <source>The vocabulary file should be in CSV-like format, with the last field being the weight associated with the word.</source>
          <target state="translated">词汇文件应该是类似CSV的格式,最后一个字段是与单词相关的权重。</target>
        </trans-unit>
        <trans-unit id="1063f666d548cfe042252b2f4239aa647a4b8506" translate="yes" xml:space="preserve">
          <source>The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers.</source>
          <target state="translated">一个层的权重代表了该层的状态。该函数以Numpy数组列表的形式返回与该层相关的可训练和不可训练的权重值,而这些权重值又可以用来将状态加载到类似参数化的层中。</target>
        </trans-unit>
        <trans-unit id="bf4e9211223da35fb0cefc91c559dd39cd92d9f5" translate="yes" xml:space="preserve">
          <source>The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer.</source>
          <target state="translated">一个层的权重代表了这个层的状态。这个函数从numpy数组中设置权重值。权重值应该按照层创建的顺序来传递。注意,在调用这个函数之前,必须先实例化层的权重,通过调用层。</target>
        </trans-unit>
        <trans-unit id="9c9c5e7f6fe29d5feb4e712a5efa9aacc5795ce1" translate="yes" xml:space="preserve">
          <source>The weights of an optimizer are its state (ie, variables). This function returns the weight values associated with this optimizer as a list of Numpy arrays. The first value is always the iterations count of the optimizer, followed by the optimizer's state variables in the order they were created. The returned list can in turn be used to load state into similarly parameterized optimizers.</source>
          <target state="translated">一个优化器的权重是它的状态(即变量)。这个函数以Numpy数组列表的形式返回与这个优化器相关的权重值。第一个值始终是优化器的迭代次数,后面是优化器的状态变量,按照它们被创建的顺序排列。返回的列表又可以用来将状态加载到类似参数化的优化器中。</target>
        </trans-unit>
        <trans-unit id="1dbc3d68ba3835fc93e47564b7ed4f1325c6e8cd" translate="yes" xml:space="preserve">
          <source>The weights of an optimizer are its state (ie, variables). This function takes the weight values associated with this optimizer as a list of Numpy arrays. The first value is always the iterations count of the optimizer, followed by the optimizer's state variables in the order they are created. The passed values are used to set the new state of the optimizer.</source>
          <target state="translated">一个优化器的权重就是它的状态(即变量)。这个函数将与这个优化器相关的权重值作为一个Numpy数组列表。第一个值总是优化器的迭代次数,然后是优化器的状态变量,按照它们被创建的顺序。传递的值用于设置优化器的新状态。</target>
        </trans-unit>
        <trans-unit id="15e91da427b6cf2b2fbc5ac9447fe8b24dd8aa82" translate="yes" xml:space="preserve">
          <source>The width the output tensor is &lt;code&gt;input_depth * block_size&lt;/code&gt;, whereas the height is &lt;code&gt;input_height * block_size&lt;/code&gt;.</source>
          <target state="translated">输出张量的宽度为 &lt;code&gt;input_depth * block_size&lt;/code&gt; ，而高度为 &lt;code&gt;input_height * block_size&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4a165b5ce96be1f52d9f7edf6d8a65c227dd997f" translate="yes" xml:space="preserve">
          <source>The width(s) of the ngrams to create. If this is a list or tuple, the op will return ngrams of all specified arities in list order. Values must be non-Tensor integers greater than 0.</source>
          <target state="translated">要创建的ngrams的宽度。如果是列表或元组,运算将按列表顺序返回所有指定数组的ngrams。值必须是大于0的非Tensor整数。</target>
        </trans-unit>
        <trans-unit id="78fda38eef982ddecbfd2142c00746b57a76815b" translate="yes" xml:space="preserve">
          <source>The word index dictionary.</source>
          <target state="translated">词条索引词典。</target>
        </trans-unit>
        <trans-unit id="524d27d9a78ab988dfdffc5a8ddf699fa93cbe93" translate="yes" xml:space="preserve">
          <source>The word index dictionary. Keys are word strings, values are their index.</source>
          <target state="translated">词条索引词典。键是单词字符串,值是它们的索引。</target>
        </trans-unit>
        <trans-unit id="273384d93f6363b2a2a4c34767e0268890f5d5af" translate="yes" xml:space="preserve">
          <source>The wrapped input tensor.</source>
          <target state="translated">封装的输入张量。</target>
        </trans-unit>
        <trans-unit id="e4a82e3bb086ae7e63715facddf72f40e0765006" translate="yes" xml:space="preserve">
          <source>The wrapped output tensor.</source>
          <target state="translated">包装后的输出张量。</target>
        </trans-unit>
        <trans-unit id="52457270e7a01e13ee00378b3108ea8fc12315e0" translate="yes" xml:space="preserve">
          <source>The wrapped value.</source>
          <target state="translated">包的价值。</target>
        </trans-unit>
        <trans-unit id="bb274ffefe3874a86b65898ba52aa9be06563172" translate="yes" xml:space="preserve">
          <source>The wrapper function.</source>
          <target state="translated">包装器功能。</target>
        </trans-unit>
        <trans-unit id="0c29d607aec40364caa8044d1b086cb311bf2d17" translate="yes" xml:space="preserve">
          <source>Theano-like behavior example</source>
          <target state="translated">类行为例</target>
        </trans-unit>
        <trans-unit id="c3d562c283b894723caae359b0851a3c4fb15dc1" translate="yes" xml:space="preserve">
          <source>Then calling &lt;code&gt;image_dataset_from_directory(main_directory, labels='inferred')&lt;/code&gt; will return a &lt;a href=&quot;../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; that yields batches of images from the subdirectories &lt;code&gt;class_a&lt;/code&gt; and &lt;code&gt;class_b&lt;/code&gt;, together with labels 0 and 1 (0 corresponding to &lt;code&gt;class_a&lt;/code&gt; and 1 corresponding to &lt;code&gt;class_b&lt;/code&gt;).</source>
          <target state="translated">然后调用 &lt;code&gt;image_dataset_from_directory(main_directory, labels='inferred')&lt;/code&gt; 将返回一个&lt;a href=&quot;../../data/dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt;，它从子目录 &lt;code&gt;class_a&lt;/code&gt; 和 &lt;code&gt;class_b&lt;/code&gt; 以及标签0和1（0对应于 &lt;code&gt;class_a&lt;/code&gt; 和1对应于 &lt;code&gt;class_b&lt;/code&gt; ）中产生一批图像。</target>
        </trans-unit>
        <trans-unit id="f5fae7664a9f8954314f95eaa134030803e80ccc" translate="yes" xml:space="preserve">
          <source>Then calling &lt;code&gt;text_dataset_from_directory(main_directory, labels='inferred')&lt;/code&gt; will return a &lt;a href=&quot;../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; that yields batches of texts from the subdirectories &lt;code&gt;class_a&lt;/code&gt; and &lt;code&gt;class_b&lt;/code&gt;, together with labels 0 and 1 (0 corresponding to &lt;code&gt;class_a&lt;/code&gt; and 1 corresponding to &lt;code&gt;class_b&lt;/code&gt;).</source>
          <target state="translated">然后调用 &lt;code&gt;text_dataset_from_directory(main_directory, labels='inferred')&lt;/code&gt; 将返回一个&lt;a href=&quot;../../data/dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt;，它从子目录 &lt;code&gt;class_a&lt;/code&gt; 和 &lt;code&gt;class_b&lt;/code&gt; 以及标签0和1（0对应于 &lt;code&gt;class_a&lt;/code&gt; 和1对应于 &lt;code&gt;class_b&lt;/code&gt; ）中产生大量文本。</target>
        </trans-unit>
        <trans-unit id="fb44a39f5e3304ba1e47cd9001107c1190200e02" translate="yes" xml:space="preserve">
          <source>Then output is &lt;code&gt;[2 x 2 x 3]&lt;/code&gt;:</source>
          <target state="translated">然后输出是 &lt;code&gt;[2 x 2 x 3]&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="70803c29e37a6cdd022ef83164306a19f47318ca" translate="yes" xml:space="preserve">
          <source>Then output is &lt;code&gt;[3 x 4]&lt;/code&gt;:</source>
          <target state="translated">然后输出是 &lt;code&gt;[3 x 4]&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="609a89b6f4ea61b7d43929c28cac3cb16c1a8704" translate="yes" xml:space="preserve">
          <source>Then output is &lt;code&gt;[4 x 3]&lt;/code&gt;:</source>
          <target state="translated">然后输出是 &lt;code&gt;[4 x 3]&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="b6380335743da8e76766d68f136c813dfdf80835" translate="yes" xml:space="preserve">
          <source>Then the final line will print out:</source>
          <target state="translated">然后将最后一行打印出来。</target>
        </trans-unit>
        <trans-unit id="d073abde2a1df4b5c62a21448d955ee1c50a80df" translate="yes" xml:space="preserve">
          <source>Then the output is a dictionary:</source>
          <target state="translated">然后输出的是一个字典。</target>
        </trans-unit>
        <trans-unit id="4c6244085178ac4f66fb4367c44b652758bf26c2" translate="yes" xml:space="preserve">
          <source>Then you can run a &lt;a href=&quot;../math/add&quot;&gt;&lt;code&gt;tf.add&lt;/code&gt;&lt;/a&gt; operation only on logical device 0.</source>
          <target state="translated">然后，您只能在逻辑设备0上运行&lt;a href=&quot;../math/add&quot;&gt; &lt;code&gt;tf.add&lt;/code&gt; &lt;/a&gt;操作。</target>
        </trans-unit>
        <trans-unit id="f77a2ae4d69da67530e4f97b9e84f16f29f79cdf" translate="yes" xml:space="preserve">
          <source>Then,</source>
          <target state="translated">Then,</target>
        </trans-unit>
        <trans-unit id="18c6dda9e31a83fdb5a90169c27005c658e8056f" translate="yes" xml:space="preserve">
          <source>Then, row_pooling_sequence should satisfy:</source>
          <target state="translated">那么,row_pooling_sequence应该满足。</target>
        </trans-unit>
        <trans-unit id="3debd4416d24691c45dc01b19b3489468194d8c2" translate="yes" xml:space="preserve">
          <source>There are a number of questions to ask in the decision process, including:</source>
          <target state="translated">在决策过程中要问一些问题,包括:</target>
        </trans-unit>
        <trans-unit id="6f58afb7ffb8a1e95bc14a4cccd59e26c144fddb" translate="yes" xml:space="preserve">
          <source>There are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).</source>
          <target state="translated">在作用域内需要发生的事情有很多要求。但是,在我们掌握了哪种策略的信息的地方,我们经常为用户输入作用域,所以他们不必明确地去做(即在作用域内或作用域外调用那些都可以)。</target>
        </trans-unit>
        <trans-unit id="f0dc8d396849561a849ade939c069a913c651243" translate="yes" xml:space="preserve">
          <source>There are different ways to quantize. This version uses only scaling, so 0.0 maps to 0.</source>
          <target state="translated">有不同的量化方式。这个版本只使用缩放,所以0.0映射到0。</target>
        </trans-unit>
        <trans-unit id="372d16d9e0a1868251f75c59e5f513839dddd179" translate="yes" xml:space="preserve">
          <source>There are many different ways to implement atrous convolution (see the refs above). The implementation here reduces</source>
          <target state="translated">有很多不同的方法来实现trtrous卷积(见上面的参考文献)。这里的实现减少了</target>
        </trans-unit>
        <trans-unit id="4b4c86e082be3a02799da3aa4833e74149042874" translate="yes" xml:space="preserve">
          <source>There are nodes like Identity and CheckNumerics that are only useful during training, and can be removed in graphs that will be used for nothing but inference. Here we identify and remove them, returning an equivalent graph. To be specific, CheckNumerics nodes are always removed, and Identity nodes that aren't involved in control edges are spliced out so that their input and outputs are directly connected.</source>
          <target state="translated">有一些像Identity和CheckNumerics这样的节点只在训练过程中有用,可以在图中删除,这些节点除了推理之外不会被使用。在这里,我们识别并删除它们,返回一个等价图。具体来说,CheckNumerics节点总是被移除,而不参与控制边的Identity节点则被拼接出来,使其输入和输出直接连接。</target>
        </trans-unit>
        <trans-unit id="a4fe5597954ba439b4679d614be0e1e5164b606b" translate="yes" xml:space="preserve">
          <source>There are several delicate issues when running multiple threads that way: closing the queues in sequence as the input is exhausted, correctly catching and reporting exceptions, etc.</source>
          <target state="translated">这样运行多个线程时,有几个微妙的问题:在输入耗尽时依次关闭队列,正确捕捉和报告异常等。</target>
        </trans-unit>
        <trans-unit id="20bc80fe9d0b075a12bc0575d2ba4a5d5fe1269d" translate="yes" xml:space="preserve">
          <source>There are several ways to run the conversion:</source>
          <target state="translated">有几种运行转换的方法。</target>
        </trans-unit>
        <trans-unit id="9a0c34e11b24dd273612b5e842b9ca3db6c2dc5f" translate="yes" xml:space="preserve">
          <source>There are three important concepts associated with TensorFlow Distributions shapes:</source>
          <target state="translated">有三个重要的概念与TensorFlow Distributions形状相关。</target>
        </trans-unit>
        <trans-unit id="d89ee0299ae366416b4bb7507b2e32a19b6a4278" translate="yes" xml:space="preserve">
          <source>There are two APIs to create a &lt;a href=&quot;distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; object: &lt;a href=&quot;strategy#experimental_distribute_dataset&quot;&gt;&lt;code&gt;tf.distribute.Strategy.experimental_distribute_dataset(dataset)&lt;/code&gt;&lt;/a&gt;and &lt;a href=&quot;strategy#experimental_distribute_datasets_from_function&quot;&gt;&lt;code&gt;tf.distribute.Strategy.experimental_distribute_datasets_from_function(dataset_fn)&lt;/code&gt;&lt;/a&gt;. &lt;em&gt;When to use which?&lt;/em&gt; When you have a &lt;a href=&quot;../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; instance, and the regular batch splitting (i.e. re-batch the input &lt;a href=&quot;../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; instance with a new batch size that is equal to the global batch size divided by the number of replicas in sync) and autosharding (i.e. the &lt;a href=&quot;../data/experimental/autoshardpolicy&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy&lt;/code&gt;&lt;/a&gt; options) work for you, use the former API. Otherwise, if you are &lt;em&gt;not&lt;/em&gt; using a canonical &lt;a href=&quot;../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; instance, or you would like to customize the batch splitting or sharding, you can wrap these logic in a &lt;code&gt;dataset_fn&lt;/code&gt; and use the latter API. Both API handles prefetch to device for the user. For more details and examples, follow the links to the APIs.</source>
          <target state="translated">有两个API可以创建&lt;a href=&quot;distributeddataset&quot;&gt; &lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt; &lt;/a&gt;对象：&lt;a href=&quot;strategy#experimental_distribute_dataset&quot;&gt; &lt;code&gt;tf.distribute.Strategy.experimental_distribute_dataset(dataset)&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;strategy#experimental_distribute_datasets_from_function&quot;&gt; &lt;code&gt;tf.distribute.Strategy.experimental_distribute_datasets_from_function(dataset_fn)&lt;/code&gt; &lt;/a&gt;。&lt;em&gt;什么时候使用？&lt;/em&gt;当您拥有&lt;a href=&quot;../data/dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt;实例并进行常规批量拆分时（即，使用新的批量大小重新批入输入的&lt;a href=&quot;../data/dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt;实例，该新批量大小等于全局批量大小除以同步副本数）和自动分片（即&lt;a href=&quot;../data/experimental/autoshardpolicy&quot;&gt; &lt;code&gt;tf.data.experimental.AutoShardPolicy&lt;/code&gt; &lt;/a&gt;选项）为您工作，请使用以前的API。否则，如果您&lt;em&gt;不&lt;/em&gt;使用规范&lt;a href=&quot;../data/dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt;实例，或者您想自定义批处理拆分或分片，可以将这些逻辑包装在 &lt;code&gt;dataset_fn&lt;/code&gt; 中,并使用后者的API。这两个API都为用户处理向设备的预取。有关更多详细信息和示例，请访问API的链接。</target>
        </trans-unit>
        <trans-unit id="6b8f13eb2103d67ac4feee46d41de0d462019931" translate="yes" xml:space="preserve">
          <source>There are two main usages of a &lt;code&gt;DistributedDataset&lt;/code&gt; object:</source>
          <target state="translated">&lt;code&gt;DistributedDataset&lt;/code&gt; 对象有两种主要用法：</target>
        </trans-unit>
        <trans-unit id="3536015a851856ea989836d31de530e9b064da8c" translate="yes" xml:space="preserve">
          <source>There are two means to control the logging verbosity:</source>
          <target state="translated">有两种手段可以控制记录的啰嗦。</target>
        </trans-unit>
        <trans-unit id="93dc11cfab5744c0fd203945e6587a775aa790bd" translate="yes" xml:space="preserve">
          <source>There are two modes under which the &lt;code&gt;TPUEmbedding&lt;/code&gt; class can used. This depends on if the class was created under a &lt;code&gt;TPUStrategy&lt;/code&gt; scope or not.</source>
          <target state="translated">&lt;code&gt;TPUEmbedding&lt;/code&gt; 类可以使用两种模式。这取决于该类是否在 &lt;code&gt;TPUStrategy&lt;/code&gt; 范围内创建。</target>
        </trans-unit>
        <trans-unit id="cfe77a90ab442852a7e0e410775f1f4aa154097a" translate="yes" xml:space="preserve">
          <source>There are two possible return values, &quot;google&quot; (when TensorFlow is running in a Google-internal environment) or an empty string (when TensorFlow is running elsewhere).</source>
          <target state="translated">有两个可能的返回值,&quot;google&quot;(当TensorFlow在Google内部环境中运行时)或一个空字符串(当TensorFlow在其他地方运行时)。</target>
        </trans-unit>
        <trans-unit id="c121ae8dd46f0cedd4d9939e53220e70d1795c85" translate="yes" xml:space="preserve">
          <source>There are two questions to ask in the decision process: Do you need gradients computed as sparse too? Is your sparse data represented as two &lt;code&gt;SparseTensor&lt;/code&gt;s: ids and values? There is more explanation about data format below. If you answer any of these questions as yes, consider using &lt;a href=&quot;../nn/embedding_lookup_sparse&quot;&gt;&lt;code&gt;tf.nn.embedding_lookup_sparse&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">在决策过程中有两个问题要问：是否也需要计算稀疏梯度？您的稀疏数据是否表示为两个 &lt;code&gt;SparseTensor&lt;/code&gt; ：ID和值？以下是有关数据格式的更多说明。如果您回答以上任一问题，请考虑使用&lt;a href=&quot;../nn/embedding_lookup_sparse&quot;&gt; &lt;code&gt;tf.nn.embedding_lookup_sparse&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0878716bd35f31f600ff16927cc66d2e1bdcca8d" translate="yes" xml:space="preserve">
          <source>There are two variants of the GRU implementation. The default one is based on &lt;a href=&quot;https://arxiv.org/abs/1406.1078v3&quot;&gt;v3&lt;/a&gt; and has reset gate applied to hidden state before matrix multiplication. The other one is based on &lt;a href=&quot;https://arxiv.org/abs/1406.1078v1&quot;&gt;original&lt;/a&gt; and has the order reversed.</source>
          <target state="translated">GRU实现有两种变体。默认值基于&lt;a href=&quot;https://arxiv.org/abs/1406.1078v3&quot;&gt;v3，&lt;/a&gt;并且在矩阵乘法之前将复位门应用于隐藏状态。另一个基于&lt;a href=&quot;https://arxiv.org/abs/1406.1078v1&quot;&gt;原始格式&lt;/a&gt;，并且顺序相反。</target>
        </trans-unit>
        <trans-unit id="f3d0da97f844d09b6bac8339000119f443c41934" translate="yes" xml:space="preserve">
          <source>There are two variants. The default one is based on 1406.1078v3 and has reset gate applied to hidden state before matrix multiplication. The other one is based on original 1406.1078v1 and has the order reversed.</source>
          <target state="translated">有两个变体。默认的是基于1406.1078v3,在矩阵乘法之前,将复位门应用于隐藏状态。另一个是基于原来的1406.1078v1,顺序相反。</target>
        </trans-unit>
        <trans-unit id="f1e5ef8034bae35c3455bef5218f7a32bbdb1aa7" translate="yes" xml:space="preserve">
          <source>There are two versions of the API: 1 or 2.</source>
          <target state="translated">API有两个版本:1或2。</target>
        </trans-unit>
        <trans-unit id="19c11322b50f362088a37650ccaf3b2e479b115a" translate="yes" xml:space="preserve">
          <source>There are two versions of the API: ExportSavedModelApiVersion.V1 and V2.</source>
          <target state="translated">有两个版本的API。ExportSavedModelApiVersion.V1和V2。</target>
        </trans-unit>
        <trans-unit id="c409af06e5c37c31d5d6e63ae53409968bb739b2" translate="yes" xml:space="preserve">
          <source>There are two ways to create decorators that TensorFlow can introspect into. This is important for documentation generation purposes, so that function signatures aren't obscured by the (*args, **kwds) signature that decorators often provide.</source>
          <target state="translated">有两种方法可以创建TensorFlow可以内视到的装饰器。这对于生成文档来说很重要,这样函数签名就不会被装饰符经常提供的(*args,**kwds)签名所掩盖。</target>
        </trans-unit>
        <trans-unit id="49196c8dfb0588201d8b9733a14917623170a741" translate="yes" xml:space="preserve">
          <source>There are two ways to instantiate a &lt;code&gt;Model&lt;/code&gt;:</source>
          <target state="translated">实例化 &lt;code&gt;Model&lt;/code&gt; 有两种方法：</target>
        </trans-unit>
        <trans-unit id="a3e498654dcff830e5be467e55fbbbbc20e2910f" translate="yes" xml:space="preserve">
          <source>There are two ways to use the moving averages for evaluations:</source>
          <target state="translated">有两种方法可以使用移动平均线进行评价。</target>
        </trans-unit>
        <trans-unit id="14b50269ad70f1441184d03f5b9e983ae0e09462" translate="yes" xml:space="preserve">
          <source>There is a special node with &lt;code&gt;task_type&lt;/code&gt; as &lt;code&gt;evaluator&lt;/code&gt;, which is not part of the (training) &lt;code&gt;cluster_spec&lt;/code&gt;. It handles the distributed evaluation job.</source>
          <target state="translated">有一个特殊的节点，具有 &lt;code&gt;task_type&lt;/code&gt; 作为 &lt;code&gt;evaluator&lt;/code&gt; ，它不属于（训练） &lt;code&gt;cluster_spec&lt;/code&gt; 。它处理分布式评估工作。</target>
        </trans-unit>
        <trans-unit id="ff5a342ad76b4f6dbc02bde4742c1939d0847ee4" translate="yes" xml:space="preserve">
          <source>There is also a global generator:</source>
          <target state="translated">还有一个全球发电机。</target>
        </trans-unit>
        <trans-unit id="c0a273890eb3544a6c0002e05e862f10bbfd245d" translate="yes" xml:space="preserve">
          <source>There is an equivalent description in terms of the [batch] spectrum &lt;code&gt;H&lt;/code&gt; and Fourier transforms. Here we consider &lt;code&gt;A.shape = [N, N]&lt;/code&gt; and ignore batch dimensions.</source>
          <target state="translated">关于[批光谱] &lt;code&gt;H&lt;/code&gt; 和傅立叶变换，存在等效描述。在这里，我们考虑 &lt;code&gt;A.shape = [N, N]&lt;/code&gt; 并忽略批处理尺寸。</target>
        </trans-unit>
        <trans-unit id="34b53209d83711d378deaa8b481a2f90355eb0e3" translate="yes" xml:space="preserve">
          <source>There is an equivalent description in terms of the [batch] spectrum &lt;code&gt;H&lt;/code&gt; and Fourier transforms. Here we consider &lt;code&gt;A.shape = [N, N]&lt;/code&gt; and ignore batch dimensions. Define the discrete Fourier transform (DFT) and its inverse by</source>
          <target state="translated">关于[批光谱] &lt;code&gt;H&lt;/code&gt; 和傅立叶变换，存在等效描述。在这里，我们考虑 &lt;code&gt;A.shape = [N, N]&lt;/code&gt; 并忽略批处理尺寸。定义离散傅里叶变换（DFT）及其逆</target>
        </trans-unit>
        <trans-unit id="70db84bf8987013d7aeb820bf2eec70af7a073ca" translate="yes" xml:space="preserve">
          <source>There is no need to delete the directory after the test.</source>
          <target state="translated">测试后不需要删除目录。</target>
        </trans-unit>
        <trans-unit id="897a9d5198f512dcaeb305f37bd31486ee120960" translate="yes" xml:space="preserve">
          <source>There is no transformation in the &lt;a href=&quot;../data&quot;&gt;&lt;code&gt;tf.data&lt;/code&gt;&lt;/a&gt; Python API for creating this dataset. Instead, it is created as a result of the &lt;code&gt;filter_with_random_uniform_fusion&lt;/code&gt; static optimization. Whether this optimization is performed is determined by the &lt;code&gt;experimental_optimization.filter_with_random_uniform_fusion&lt;/code&gt; option of &lt;a href=&quot;../data/options&quot;&gt;&lt;code&gt;tf.data.Options&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">还有就是在没有改造&lt;a href=&quot;../data&quot;&gt; &lt;code&gt;tf.data&lt;/code&gt; &lt;/a&gt;的Python API来创建此数据集。相反，它是根据 &lt;code&gt;filter_with_random_uniform_fusion&lt;/code&gt; 静态优化创建的。是否执行此优化取决于&lt;a href=&quot;../data/options&quot;&gt; &lt;code&gt;tf.data.Options&lt;/code&gt; &lt;/a&gt;的 &lt;code&gt;experimental_optimization.filter_with_random_uniform_fusion&lt;/code&gt; 选项。</target>
        </trans-unit>
        <trans-unit id="b246461630b3c81ed498d37ca7dc4b93263133aa" translate="yes" xml:space="preserve">
          <source>There is often a need to lift variable initialization ops out of control-flow scopes, function-building graphs, and gradient tapes. Entering an &lt;code&gt;init_scope&lt;/code&gt; is a mechanism for satisfying these desiderata. In particular, entering an &lt;code&gt;init_scope&lt;/code&gt; has three effects:</source>
          <target state="translated">通常需要将变量初始化操作移出控制流范围，功能构建图和渐变带。输入 &lt;code&gt;init_scope&lt;/code&gt; 是满足这些需求的机制。特别是，输入 &lt;code&gt;init_scope&lt;/code&gt; 具有三个效果：</target>
        </trans-unit>
        <trans-unit id="9600bed3adf3351d3b6785f43a000c7d51f5ddee" translate="yes" xml:space="preserve">
          <source>There is one exception: if the final (i.e., innermost) element(s) of &lt;code&gt;partitions&lt;/code&gt; are &lt;code&gt;UniformRowLength&lt;/code&gt;s, then the values are simply reshaped (as a higher-dimensional &lt;a href=&quot;../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt;), rather than being wrapped in a &lt;a href=&quot;../raggedtensor&quot;&gt;&lt;code&gt;tf.RaggedTensor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">有一个例外：如果 &lt;code&gt;partitions&lt;/code&gt; 的最后一个（即，最里面的）元素是 &lt;code&gt;UniformRowLength&lt;/code&gt; ，则值将被简单地重塑（作为高维&lt;a href=&quot;../tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt;），而不是被包装在&lt;a href=&quot;../raggedtensor&quot;&gt; &lt;code&gt;tf.RaggedTensor&lt;/code&gt; 中&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="b0e762e9fe47e7a8aaa65fed7f11b0b738414c89" translate="yes" xml:space="preserve">
          <source>There should be no data dependency between the different semantic invocations of &lt;code&gt;fn&lt;/code&gt;, i.e. it should be safe to map the elements of the inputs in any order.</source>
          <target state="translated">&lt;code&gt;fn&lt;/code&gt; 的不同语义调用之间应该没有数据依赖性，即以任何顺序映射输入的元素应该是安全的。</target>
        </trans-unit>
        <trans-unit id="69f7128e30301479f7dbfc0ac8f483f3d6d00a4d" translate="yes" xml:space="preserve">
          <source>Therefore we introduce some decoupling using a queue. The queue contains the work units and the Reader dequeues from the queue when it is asked to produce a record (via Read()) but it has finished the last work unit.</source>
          <target state="translated">因此我们使用队列引入一些解耦。队列中包含了工作单元,当Reader被要求产生一条记录(通过Read())但它已经完成了最后一个工作单元时,就会从队列中去掉队列。</target>
        </trans-unit>
        <trans-unit id="86cd25893d67c9049a975fb7c203728b1fcc6c8b" translate="yes" xml:space="preserve">
          <source>These are arguments passed to the optimizer subclass constructor (the &lt;code&gt;__init__&lt;/code&gt; method), and then passed to &lt;code&gt;self._set_hyper()&lt;/code&gt;. They can be either regular Python values (like 1.0), tensors, or callables. If they are callable, the callable will be called during &lt;code&gt;apply_gradients()&lt;/code&gt; to get the value for the hyper parameter.</source>
          <target state="translated">这些是传递给优化器子类构造函数（ &lt;code&gt;__init__&lt;/code&gt; 方法），然后传递给 &lt;code&gt;self._set_hyper()&lt;/code&gt; 的参数。它们可以是常规Python值（如1.0），张量或可调用值。如果它们是可调用的，则可调用对象将在 &lt;code&gt;apply_gradients()&lt;/code&gt; 期间被调用以获取hyper参数的值。</target>
        </trans-unit>
        <trans-unit id="d1eefae8eebd09f04dd361373c7c577f0cf1144d" translate="yes" xml:space="preserve">
          <source>These conversion options are experimental. They are subject to change without notice and offer no guarantees.</source>
          <target state="translated">这些转换选项是试验性的。如有变更,恕不另行通知,也不提供任何保证。</target>
        </trans-unit>
        <trans-unit id="c291964ed47da0b87e8a53387f07ca95b28e890c" translate="yes" xml:space="preserve">
          <source>These indices specify where the values for each row begin in &lt;code&gt;self.values&lt;/code&gt;. &lt;code&gt;rt.row_starts()&lt;/code&gt; is equal to &lt;code&gt;rt.row_splits[:-1]&lt;/code&gt;.</source>
          <target state="translated">这些索引指定每行的值从 &lt;code&gt;self.values&lt;/code&gt; 开始的位置。 &lt;code&gt;rt.row_starts()&lt;/code&gt; 等于 &lt;code&gt;rt.row_splits[:-1]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="99b0058dc30c77b214aebabddab907af2da4a216" translate="yes" xml:space="preserve">
          <source>These indices specify where the values for each row end in &lt;code&gt;self.values&lt;/code&gt;. &lt;code&gt;rt.row_limits(self)&lt;/code&gt; is equal to &lt;code&gt;rt.row_splits[:-1]&lt;/code&gt;.</source>
          <target state="translated">这些索引指定每行的值在 &lt;code&gt;self.values&lt;/code&gt; 中结束的位置。 &lt;code&gt;rt.row_limits(self)&lt;/code&gt; 等于 &lt;code&gt;rt.row_splits[:-1]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="23fdcaa94183d818b1595f88d3e5484cc7528a37" translate="yes" xml:space="preserve">
          <source>These layers expose 3 keyword arguments:</source>
          <target state="translated">这些层暴露了3个关键字参数。</target>
        </trans-unit>
        <trans-unit id="76c2c66d43654cd0b29da8b2ca646058c82ed16b" translate="yes" xml:space="preserve">
          <source>These might be stored sparsely in the following Example protos by storing only the feature ids (column number if the vectors are treated as a matrix) of the non-zero elements and the corresponding values:</source>
          <target state="translated">在下面的示例protos中,可以通过只存储非零元素的特征id(如果将向量视为矩阵,则列号)和相应的值来稀疏地存储这些内容。</target>
        </trans-unit>
        <trans-unit id="e47f03b2a63c4e509a81c6c7d3f37f7d1be1a857" translate="yes" xml:space="preserve">
          <source>These sufficient statistics are computed using the one pass algorithm on an input that's optionally shifted. See: &lt;a href=&quot;https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data&quot;&gt;https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data&lt;/a&gt;</source>
          <target state="translated">这些足够的统计信息是通过对可选移动的输入使用单次通过算法来计算的。请参阅：&lt;a href=&quot;https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data&quot;&gt;https&lt;/a&gt;：//en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data</target>
        </trans-unit>
        <trans-unit id="df0616278e0670c1f8a838c05616c2906a5ad628" translate="yes" xml:space="preserve">
          <source>These sufficient statistics are computed using the one pass algorithm on an input that's optionally shifted. See: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data</source>
          <target state="translated">这些足够的统计量是在一个有选择地移位的输入上使用一传算法计算出来的。参见:https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data</target>
        </trans-unit>
        <trans-unit id="5c983cdacd0a0593413a5e4ab438ec1b6414a3f5" translate="yes" xml:space="preserve">
          <source>These typically correspond to model heads.</source>
          <target state="translated">这些通常对应的是模型头。</target>
        </trans-unit>
        <trans-unit id="a2e13849b656ad081d144e8fb21e76af2be31973" translate="yes" xml:space="preserve">
          <source>These values are similar to values from a &lt;code&gt;random_normal_initializer&lt;/code&gt; except that values more than two standard deviations from the mean are discarded and re-drawn. This is the recommended initializer for neural network weights and filters.</source>
          <target state="translated">这些值与 &lt;code&gt;random_normal_initializer&lt;/code&gt; 中的值相似，不同之处在于，与均值相比有两个以上标准差的值将被丢弃并重新绘制。对于神经网络权重和过滤器，这是推荐的初始化程序。</target>
        </trans-unit>
        <trans-unit id="6f69d15ae681fb68548cb2f0cf5da97b8f2f416c" translate="yes" xml:space="preserve">
          <source>They are not resettable. Once a one-shot iterator reaches the end of its underlying dataset, subsequent &quot;IteratorGetNext&quot; operations on that iterator will always produce an &lt;code&gt;OutOfRange&lt;/code&gt; error.</source>
          <target state="translated">它们不可重置。一旦单次迭代器到达其基础数据集的末尾，该迭代器上的后续&amp;ldquo; IteratorGetNext&amp;rdquo;操作将始终产生 &lt;code&gt;OutOfRange&lt;/code&gt; 错误。</target>
        </trans-unit>
        <trans-unit id="82a7de6a736dfa8a46716669d941754c78b26714" translate="yes" xml:space="preserve">
          <source>They do not support parameterization: all logic for creating the underlying dataset must be bundled in the &lt;code&gt;dataset_factory&lt;/code&gt; function.</source>
          <target state="translated">它们不支持参数化：创建基础数据集的所有逻辑都必须捆绑在 &lt;code&gt;dataset_factory&lt;/code&gt; 函数中。</target>
        </trans-unit>
        <trans-unit id="035fa9e40dd64ae60fe7c5634de4cc5ea73a82db" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;LinearOperator&lt;/code&gt; is initialized to have shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; by providing &lt;code&gt;spectrum&lt;/code&gt;, a &lt;code&gt;[B1,...,Bb, N0, N1, N2]&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt; with &lt;code&gt;N0*N1*N2 = N&lt;/code&gt;.</source>
          <target state="translated">此 &lt;code&gt;LinearOperator&lt;/code&gt; 被初始化为具有形状 &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; 通过提供 &lt;code&gt;spectrum&lt;/code&gt; ，一个 &lt;code&gt;[B1,...,Bb, N0, N1, N2]&lt;/code&gt; &lt;code&gt;Tensor&lt;/code&gt; 与 &lt;code&gt;N0*N1*N2 = N&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7dc76893f409505eb4af8e3ca9d9b50f0907d32c" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;LinearOperator&lt;/code&gt; is initialized to have shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; by providing &lt;code&gt;spectrum&lt;/code&gt;, a &lt;code&gt;[B1,...,Bb, N0, N1]&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt; with &lt;code&gt;N0*N1 = N&lt;/code&gt;.</source>
          <target state="translated">此 &lt;code&gt;LinearOperator&lt;/code&gt; 被初始化为具有形状 &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; 通过提供 &lt;code&gt;spectrum&lt;/code&gt; ，一个 &lt;code&gt;[B1,...,Bb, N0, N1]&lt;/code&gt; &lt;code&gt;Tensor&lt;/code&gt; 与 &lt;code&gt;N0*N1 = N&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="45faa2260146699c38e2dafd91d1b8425180a30a" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;LinearOperator&lt;/code&gt; is initialized to have shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; by providing &lt;code&gt;spectrum&lt;/code&gt;, a &lt;code&gt;[B1,...,Bb, N]&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="translated">此 &lt;code&gt;LinearOperator&lt;/code&gt; 被初始化为具有形状 &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; 通过提供 &lt;code&gt;spectrum&lt;/code&gt; ，一个 &lt;code&gt;[B1,...,Bb, N]&lt;/code&gt; &lt;code&gt;Tensor&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="14d38c8190663bcf5aee0e569e1c9fa815f97d9a" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;LinearOperator&lt;/code&gt; is initialized with boolean flags of the form &lt;code&gt;is_X&lt;/code&gt;, for &lt;code&gt;X = non_singular, self_adjoint, positive_definite, square&lt;/code&gt;. These have the following meaning</source>
          <target state="translated">此 &lt;code&gt;LinearOperator&lt;/code&gt; 初始化与以下形式的布尔标志 &lt;code&gt;is_X&lt;/code&gt; ，对于 &lt;code&gt;X = non_singular, self_adjoint, positive_definite, square&lt;/code&gt; 。这些具有以下含义</target>
        </trans-unit>
        <trans-unit id="59e2aeacdf121218eb0206f7223d277271e20ff4" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;LinearOperator&lt;/code&gt; is initialized with boolean flags of the form &lt;code&gt;is_X&lt;/code&gt;, for &lt;code&gt;X = non_singular, self_adjoint, positive_definite, square&lt;/code&gt;. These have the following meaning:</source>
          <target state="translated">此 &lt;code&gt;LinearOperator&lt;/code&gt; 初始化与以下形式的布尔标志 &lt;code&gt;is_X&lt;/code&gt; ，对于 &lt;code&gt;X = non_singular, self_adjoint, positive_definite, square&lt;/code&gt; 。这些具有以下含义：</target>
        </trans-unit>
        <trans-unit id="c893a77a5cadc49b62c18f0fba68970cb1dcbaf6" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;LinearOperator&lt;/code&gt; is initialized with boolean flags of the form &lt;code&gt;is_X&lt;/code&gt;, for &lt;code&gt;X = non_singular&lt;/code&gt;, &lt;code&gt;self_adjoint&lt;/code&gt;, &lt;code&gt;positive_definite&lt;/code&gt;, &lt;code&gt;diag_update_positive&lt;/code&gt; and &lt;code&gt;square&lt;/code&gt;. These have the following meaning:</source>
          <target state="translated">此 &lt;code&gt;LinearOperator&lt;/code&gt; 初始化与以下形式的布尔标志 &lt;code&gt;is_X&lt;/code&gt; ，对于 &lt;code&gt;X = non_singular&lt;/code&gt; ， &lt;code&gt;self_adjoint&lt;/code&gt; ， &lt;code&gt;positive_definite&lt;/code&gt; ， &lt;code&gt;diag_update_positive&lt;/code&gt; 和 &lt;code&gt;square&lt;/code&gt; 。这些具有以下含义：</target>
        </trans-unit>
        <trans-unit id="1af767d14077ff11d4e7fc9b014e22bbf1d448de" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;Model&lt;/code&gt; has a dependency named &quot;input_transform&quot; on its &lt;code&gt;Dense&lt;/code&gt; layer, which in turn depends on its variables. As a result, saving an instance of &lt;code&gt;Regress&lt;/code&gt; using &lt;a href=&quot;../../../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; will also save all the variables created by the &lt;code&gt;Dense&lt;/code&gt; layer.</source>
          <target state="translated">该 &lt;code&gt;Model&lt;/code&gt; 在其 &lt;code&gt;Dense&lt;/code&gt; 层上具有一个名为&amp;ldquo; input_transform&amp;rdquo;的依赖关系，而该依赖关系又取决于其变量。其结果是，在保存的情况下 &lt;code&gt;Regress&lt;/code&gt; 使用&lt;a href=&quot;../../../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt;也将保存所有被创建的变量 &lt;code&gt;Dense&lt;/code&gt; 层。</target>
        </trans-unit>
        <trans-unit id="6bb0cfce4d3ef61c634cae7fd40aef46d6d26e65" translate="yes" xml:space="preserve">
          <source>This &lt;code&gt;Model&lt;/code&gt; has a dependency named &quot;input_transform&quot; on its &lt;code&gt;Dense&lt;/code&gt; layer, which in turn depends on its variables. As a result, saving an instance of &lt;code&gt;Regress&lt;/code&gt; using &lt;a href=&quot;checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; will also save all the variables created by the &lt;code&gt;Dense&lt;/code&gt; layer.</source>
          <target state="translated">该 &lt;code&gt;Model&lt;/code&gt; 在其 &lt;code&gt;Dense&lt;/code&gt; 层上具有一个名为&amp;ldquo; input_transform&amp;rdquo;的依赖关系，而该依赖关系又取决于其变量。其结果是，在保存的情况下 &lt;code&gt;Regress&lt;/code&gt; 使用&lt;a href=&quot;checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt;也将保存所有被创建的变量 &lt;code&gt;Dense&lt;/code&gt; 层。</target>
        </trans-unit>
        <trans-unit id="b5871012b5796b5a485fbd2ea90e6e96dc8a6361" translate="yes" xml:space="preserve">
          <source>This API allows querying the physical hardware resources prior to runtime initialization. Thus, giving an opportunity to call any additional configuration APIs. This is in contrast to &lt;a href=&quot;list_logical_devices&quot;&gt;&lt;code&gt;tf.config.list_logical_devices&lt;/code&gt;&lt;/a&gt;, which triggers runtime initialization in order to list the configured devices.</source>
          <target state="translated">该API允许在运行时初始化之前查询物理硬件资源。因此，有机会调用任何其他配置API。这与&lt;a href=&quot;list_logical_devices&quot;&gt; &lt;code&gt;tf.config.list_logical_devices&lt;/code&gt; &lt;/a&gt;相反，后者触发运行时初始化以列出已配置的设备。</target>
        </trans-unit>
        <trans-unit id="74aac56e82063765baca7f72b6ef73462943063c" translate="yes" xml:space="preserve">
          <source>This API enables repeated preprocessing steps to be consolidated, and allows re-use of already processed data, trading off disk storage and network bandwidth for freeing up more valuable CPU resources and accelerator compute time.</source>
          <target state="translated">这个API可以将重复的预处理步骤进行整合,并允许重复使用已经处理过的数据,以磁盘存储和网络带宽为代价,腾出更宝贵的CPU资源和加速器计算时间。</target>
        </trans-unit>
        <trans-unit id="762aea1a89e9b0e55d82633b7d3d567f5d2f187d" translate="yes" xml:space="preserve">
          <source>This API takes in a &lt;a href=&quot;../physicaldevice&quot;&gt;&lt;code&gt;tf.config.PhysicalDevice&lt;/code&gt;&lt;/a&gt; returned by &lt;a href=&quot;../list_physical_devices&quot;&gt;&lt;code&gt;tf.config.list_physical_devices&lt;/code&gt;&lt;/a&gt;. It returns a dict with string keys containing various details about the device. Each key is only supported by a subset of devices, so you should not assume the returned dict will have any particular key.</source>
          <target state="translated">此API接受由&lt;a href=&quot;../physicaldevice&quot;&gt; &lt;code&gt;tf.config.PhysicalDevice&lt;/code&gt; &lt;/a&gt;返回的&lt;a href=&quot;../list_physical_devices&quot;&gt; &lt;code&gt;tf.config.list_physical_devices&lt;/code&gt; &lt;/a&gt;。它返回带有字符串键的字典，该键包含有关设备的各种详细信息。每个密钥仅由一部分设备支持，因此您不应假定返回的字典将具有任何特定的密钥。</target>
        </trans-unit>
        <trans-unit id="22207bc9bfb65faecbe8df80de09b7c7ff71edfd" translate="yes" xml:space="preserve">
          <source>This API will connect to remote TPU cluster and initialize the TPU hardwares. Example usage:</source>
          <target state="translated">此 API 将连接到远程 TPU 集群,并初始化 TPU 硬体。使用示例</target>
        </trans-unit>
        <trans-unit id="fb9102764cadc1d75fc4b0e80481d3d9ed63d68c" translate="yes" xml:space="preserve">
          <source>This Estimator implements the following variants of the K-means algorithm:</source>
          <target state="translated">这个估计器实现了以下K-means算法的变体。</target>
        </trans-unit>
        <trans-unit id="80c1b7fb2eecacd5d699b9ee230de5704ca37c8d" translate="yes" xml:space="preserve">
          <source>This Multinomial distribution is parameterized by &lt;code&gt;probs&lt;/code&gt;, a (batch of) length-&lt;code&gt;K&lt;/code&gt;&lt;code&gt;prob&lt;/code&gt; (probability) vectors (&lt;code&gt;K &amp;gt; 1&lt;/code&gt;) such that &lt;code&gt;tf.reduce_sum(probs, -1) = 1&lt;/code&gt;, and a &lt;code&gt;total_count&lt;/code&gt; number of trials, i.e., the number of trials per draw from the Multinomial. It is defined over a (batch of) length-&lt;code&gt;K&lt;/code&gt; vector &lt;code&gt;counts&lt;/code&gt; such that &lt;code&gt;tf.reduce_sum(counts, -1) = total_count&lt;/code&gt;. The Multinomial is identically the Binomial distribution when &lt;code&gt;K = 2&lt;/code&gt;.</source>
          <target state="translated">该多项式分布由 &lt;code&gt;probs&lt;/code&gt; ，长度为 &lt;code&gt;K&lt;/code&gt; &lt;code&gt;prob&lt;/code&gt; （概率）向量（ &lt;code&gt;K &amp;gt; 1&lt;/code&gt; ）（批次）， &lt;code&gt;tf.reduce_sum(probs, -1) = 1&lt;/code&gt; 以及试验的 &lt;code&gt;total_count&lt;/code&gt; 个数（即多项式每次抽奖的尝试次数。它是在（一个）长度（ &lt;code&gt;K&lt;/code&gt; 个向量） &lt;code&gt;counts&lt;/code&gt; ，使得 &lt;code&gt;tf.reduce_sum(counts, -1) = total_count&lt;/code&gt; 。当 &lt;code&gt;K = 2&lt;/code&gt; 时，多项式等于二项分布。</target>
        </trans-unit>
        <trans-unit id="61586b843c2a7242a7ce5f726a415d728d094add" translate="yes" xml:space="preserve">
          <source>This Op also supports repeated indices in the output subscript, which is not supported by &lt;code&gt;numpy.einsum&lt;/code&gt;.</source>
          <target state="translated">此Op还支持输出下标中的重复索引， &lt;code&gt;numpy.einsum&lt;/code&gt; 不支持。</target>
        </trans-unit>
        <trans-unit id="d0c42e63acc7dcc7eea605a766b11674d1c53e82" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] != y[i]&lt;/code&gt; holds for every pair of (possibly broadcast) elements of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="translated">此Op检查 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 的每对（可能是广播的）元素 &lt;code&gt;x[i] != y[i]&lt;/code&gt; 成立。如果 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 都为空，则可以轻松满足。</target>
        </trans-unit>
        <trans-unit id="74ec1aa4f8491ea53037a0fc532d4741eb9218bd" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] &amp;gt; 0&lt;/code&gt; holds for every element of &lt;code&gt;x&lt;/code&gt;. If &lt;code&gt;x&lt;/code&gt; is empty, this is trivially satisfied.</source>
          <target state="translated">此Op检查 &lt;code&gt;x[i] &amp;gt; 0&lt;/code&gt; 是否对 &lt;code&gt;x&lt;/code&gt; 的每个元素成立。如果 &lt;code&gt;x&lt;/code&gt; 为空，这是很简单的。</target>
        </trans-unit>
        <trans-unit id="64284c5803db04a210d1a9048bfe82459a849a9d" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] &amp;gt; y[i]&lt;/code&gt; holds for every pair of (possibly broadcast) elements of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="translated">此Op检查 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 的每对（可能是广播的）元素 &lt;code&gt;x[i] &amp;gt; y[i]&lt;/code&gt; 成立。如果 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 都为空，则可以轻松满足。</target>
        </trans-unit>
        <trans-unit id="382c384f5bde2874efffdf9480af145ba234b875" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] &amp;gt;= 0&lt;/code&gt; holds for every element of &lt;code&gt;x&lt;/code&gt;. If &lt;code&gt;x&lt;/code&gt; is empty, this is trivially satisfied.</source>
          <target state="translated">此Op检查 &lt;code&gt;x[i] &amp;gt;= 0&lt;/code&gt; 是否对 &lt;code&gt;x&lt;/code&gt; 的每个元素成立。如果 &lt;code&gt;x&lt;/code&gt; 为空，这是很简单的。</target>
        </trans-unit>
        <trans-unit id="501dbba0dad9772b46dd6dbe6d32dfc34b3073e4" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] &amp;gt;= y[i]&lt;/code&gt; holds for every pair of (possibly broadcast) elements of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="translated">此Op检查 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 的每对（可能是广播的）元素 &lt;code&gt;x[i] &amp;gt;= y[i]&lt;/code&gt; 成立。如果 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 都为空，则可以轻松满足。</target>
        </trans-unit>
        <trans-unit id="86a98c771b136d5122e127f8b045988c2c9039d5" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] &amp;lt; 0&lt;/code&gt; holds for every element of &lt;code&gt;x&lt;/code&gt;. If &lt;code&gt;x&lt;/code&gt; is empty, this is trivially satisfied.</source>
          <target state="translated">此Op检查 &lt;code&gt;x[i] &amp;lt; 0&lt;/code&gt; 对于 &lt;code&gt;x&lt;/code&gt; 的每个元素是否成立。如果 &lt;code&gt;x&lt;/code&gt; 为空，这是很简单的。</target>
        </trans-unit>
        <trans-unit id="a26f9f0c29e64714922c2bda7e956bffe84f5c0c" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] &amp;lt; y[i]&lt;/code&gt; holds for every pair of (possibly broadcast) elements of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="translated">此Op检查 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 的每对（可能是广播的）元素 &lt;code&gt;x[i] &amp;lt; y[i]&lt;/code&gt; 成立。如果 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 都为空，则可以轻松满足。</target>
        </trans-unit>
        <trans-unit id="c63795d0aad1b74648e6a4624d3ec74f6c7d4a1e" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] &amp;lt;= 0&lt;/code&gt; holds for every element of &lt;code&gt;x&lt;/code&gt;. If &lt;code&gt;x&lt;/code&gt; is empty, this is trivially satisfied.</source>
          <target state="translated">此Op检查 &lt;code&gt;x[i] &amp;lt;= 0&lt;/code&gt; 适用于 &lt;code&gt;x&lt;/code&gt; 的每个元素。如果 &lt;code&gt;x&lt;/code&gt; 为空，这是很简单的。</target>
        </trans-unit>
        <trans-unit id="52d38297a91871d25885ecc0eb75b01217f176df" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] &amp;lt;= y[i]&lt;/code&gt; holds for every pair of (possibly broadcast) elements of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="translated">此Op检查 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 的每对（可能是广播的）元素 &lt;code&gt;x[i] &amp;lt;= y[i]&lt;/code&gt; 成立。如果 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 都为空，则可以轻松满足。</target>
        </trans-unit>
        <trans-unit id="a274dbd07debbab7fc4511fde87c45ce7dd5f11a" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] - y[i] &amp;lt; atol + rtol * tf.abs(y[i])&lt;/code&gt; holds for every pair of (possibly broadcast) elements of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="translated">此Op检查是否对 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 的每对（可能广播的）元素都保持 &lt;code&gt;x[i] - y[i] &amp;lt; atol + rtol * tf.abs(y[i])&lt;/code&gt; 成立。如果 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 都为空，则可以轻松满足。</target>
        </trans-unit>
        <trans-unit id="afe67f7444ede4a73d34ba9ae37a78f686f755dd" translate="yes" xml:space="preserve">
          <source>This Op checks that &lt;code&gt;x[i] == y[i]&lt;/code&gt; holds for every pair of (possibly broadcast) elements of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="translated">此Op检查 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 的每对（可能是广播的）元素 &lt;code&gt;x[i] == y[i]&lt;/code&gt; 成立。如果 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 都为空，则可以轻松满足。</target>
        </trans-unit>
        <trans-unit id="47fcdf77a806254f145cef6045f72f7418b87096" translate="yes" xml:space="preserve">
          <source>This Op checks that a collection of tensors shape relationships satisfies given constraints.</source>
          <target state="translated">这个Op检查一个时序器形状关系的集合是否满足给定的约束。</target>
        </trans-unit>
        <trans-unit id="70d8b32973294baa04c2b6c2343f69d0d35b251b" translate="yes" xml:space="preserve">
          <source>This Op checks that the rank of &lt;code&gt;x&lt;/code&gt; is equal to &lt;code&gt;rank&lt;/code&gt;.</source>
          <target state="translated">此Op检查 &lt;code&gt;x&lt;/code&gt; 的秩是否等于 &lt;code&gt;rank&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="45bf526016d2ed135926c16509b05dd90dd8b920" translate="yes" xml:space="preserve">
          <source>This Op checks that the rank of &lt;code&gt;x&lt;/code&gt; is greater or equal to &lt;code&gt;rank&lt;/code&gt;.</source>
          <target state="translated">此Op检查 &lt;code&gt;x&lt;/code&gt; 的秩是否大于或等于 &lt;code&gt;rank&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ef8abcb7ef7c82e56e48acf10aee7a915b661e8e" translate="yes" xml:space="preserve">
          <source>This Op checks that the rank of &lt;code&gt;x&lt;/code&gt; is in &lt;code&gt;ranks&lt;/code&gt;.</source>
          <target state="translated">此Op检查 &lt;code&gt;x&lt;/code&gt; 的秩是否为 &lt;code&gt;ranks&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="9bbbc71838cec9daa4b798c766e634988ae8eda0" translate="yes" xml:space="preserve">
          <source>This Op does not require &lt;code&gt;a_indices&lt;/code&gt; be sorted in standard lexicographic order.</source>
          <target state="translated">此操作不需要按标准字典顺序对 &lt;code&gt;a_indices&lt;/code&gt; 进行排序。</target>
        </trans-unit>
        <trans-unit id="995662dbb5ee7bd63655bfa102038ae50bbbb546" translate="yes" xml:space="preserve">
          <source>This Op does not support implicit form. (i.e. equations without &lt;code&gt;-&amp;gt;&lt;/code&gt;).</source>
          <target state="translated">该操作不支持隐式形式。（即不带 &lt;code&gt;-&amp;gt;&lt;/code&gt; 的方程式）。</target>
        </trans-unit>
        <trans-unit id="b037dc8de7a8b2184874f7694988339f20126019" translate="yes" xml:space="preserve">
          <source>This Op eases the porting of code that uses embedding_lookup_sparse(), although some Python preprocessing of the SparseTensor arguments to embedding_lookup_sparse() is required to produce the arguments to this Op, since only a single EnqueueTPUEmbeddingSparseBatch Op is allowed per training step.</source>
          <target state="translated">这个 Op 简化了使用 embedding_lookup_sparse()的代码的移植,尽管需要对 embedding_lookup_sparse()的 SparseTensor 参数进行一些 Python 预处理才能产生这个 Op 的参数,因为每个训练步骤只允许有一个 EnqueueTPUEmbeddingSparseBatch Op。</target>
        </trans-unit>
        <trans-unit id="b3230f00e0154436022761e6b1f967d770055298" translate="yes" xml:space="preserve">
          <source>This Op first attempts to find the V2 index file pointed to by &quot;prefix&quot;, and if found proceed to read it as a V2 checkpoint;</source>
          <target state="translated">该操作首先尝试查找 &quot;前缀 &quot;指向的V2索引文件,如果找到,则继续将其作为V2检查点读取。</target>
        </trans-unit>
        <trans-unit id="1ed5e982672256e8ea90d7bd0d72e83dbb1172c0" translate="yes" xml:space="preserve">
          <source>This Op only supports unary and binary forms of &lt;code&gt;numpy.einsum&lt;/code&gt;.</source>
          <target state="translated">该Op仅支持 &lt;code&gt;numpy.einsum&lt;/code&gt; 的一元和二进制形式。</target>
        </trans-unit>
        <trans-unit id="793a60433b6712b8062346cc2fb3747282971aed" translate="yes" xml:space="preserve">
          <source>This Op picks a random location in &lt;code&gt;image&lt;/code&gt; and crops a &lt;code&gt;height&lt;/code&gt; by &lt;code&gt;width&lt;/code&gt; rectangle from that location. The random location is picked so the cropped area will fit inside the original image.</source>
          <target state="translated">此Op在 &lt;code&gt;image&lt;/code&gt; 选择一个随机位置，并从该位置按 &lt;code&gt;width&lt;/code&gt; 矩形裁剪一个 &lt;code&gt;height&lt;/code&gt; 。选择随机位置，使裁切区域适合原始图像。</target>
        </trans-unit>
        <trans-unit id="214a679e138acba179edb8672f1201325c1e87b6" translate="yes" xml:space="preserve">
          <source>This Op produces a set of TPU cores (for warm-up) or a single TPU core (for regular inference) to execute the TPU program on. The output is consumed by TPUPartitionedCall.</source>
          <target state="translated">该操作产生一组 TPU 内核(用于预热)或单个 TPU 内核(用于常规推理)来执行 TPU 程序。输出由 TPUPartitionedCall 消耗。</target>
        </trans-unit>
        <trans-unit id="1956a13a2a0626040a0d05adca12d21c39b428f4" translate="yes" xml:space="preserve">
          <source>This Op takes a SparseTensor and is the sparse counterpart to &lt;a href=&quot;../../math/reduce_max&quot;&gt;&lt;code&gt;tf.reduce_max()&lt;/code&gt;&lt;/a&gt;. In contrast to SparseReduceSum, this Op returns a SparseTensor.</source>
          <target state="translated">该Op带有SparseTensor，并且是&lt;a href=&quot;../../math/reduce_max&quot;&gt; &lt;code&gt;tf.reduce_max()&lt;/code&gt; &lt;/a&gt;的稀疏副本。与SparseReduceSum相比，此Op返回一个SparseTensor。</target>
        </trans-unit>
        <trans-unit id="21543202b1cccf9199ac85ba0b6367bb37a43438" translate="yes" xml:space="preserve">
          <source>This Op takes a SparseTensor and is the sparse counterpart to &lt;a href=&quot;../../math/reduce_max&quot;&gt;&lt;code&gt;tf.reduce_max()&lt;/code&gt;&lt;/a&gt;. In particular, this Op also returns a dense &lt;code&gt;Tensor&lt;/code&gt; instead of a sparse one.</source>
          <target state="translated">该Op带有SparseTensor，并且是&lt;a href=&quot;../../math/reduce_max&quot;&gt; &lt;code&gt;tf.reduce_max()&lt;/code&gt; &lt;/a&gt;的稀疏副本。特别是，此Op还返回密集的 &lt;code&gt;Tensor&lt;/code&gt; ,而不是稀疏的Tensor。</target>
        </trans-unit>
        <trans-unit id="a92cf7c266fad63ba2ede16d013a7954c0a3ed23" translate="yes" xml:space="preserve">
          <source>This Op takes a SparseTensor and is the sparse counterpart to &lt;a href=&quot;../../math/reduce_sum&quot;&gt;&lt;code&gt;tf.reduce_sum()&lt;/code&gt;&lt;/a&gt;. In contrast to SparseReduceSum, this Op returns a SparseTensor.</source>
          <target state="translated">该Op带有SparseTensor，并且是&lt;a href=&quot;../../math/reduce_sum&quot;&gt; &lt;code&gt;tf.reduce_sum()&lt;/code&gt; &lt;/a&gt;的稀疏副本。与SparseReduceSum相比，此Op返回一个SparseTensor。</target>
        </trans-unit>
        <trans-unit id="eff3316c87e58a0e09844e295d654acad826f230" translate="yes" xml:space="preserve">
          <source>This Op takes a SparseTensor and is the sparse counterpart to &lt;a href=&quot;../../math/reduce_sum&quot;&gt;&lt;code&gt;tf.reduce_sum()&lt;/code&gt;&lt;/a&gt;. In particular, this Op also returns a dense &lt;code&gt;Tensor&lt;/code&gt; instead of a sparse one.</source>
          <target state="translated">该Op带有SparseTensor，并且是&lt;a href=&quot;../../math/reduce_sum&quot;&gt; &lt;code&gt;tf.reduce_sum()&lt;/code&gt; &lt;/a&gt;的稀疏副本。特别是，此Op还返回密集的 &lt;code&gt;Tensor&lt;/code&gt; ,而不是稀疏的Tensor。</target>
        </trans-unit>
        <trans-unit id="02bfe7eb6e17547c27337071dde6515aea09a633" translate="yes" xml:space="preserve">
          <source>This Op takes a SparseTensor and is the sparse counterpart to &lt;a href=&quot;../math/reduce_max&quot;&gt;&lt;code&gt;tf.reduce_max()&lt;/code&gt;&lt;/a&gt;. In contrast to SparseReduceMax, this Op returns a SparseTensor.</source>
          <target state="translated">该Op带有SparseTensor，并且是&lt;a href=&quot;../math/reduce_max&quot;&gt; &lt;code&gt;tf.reduce_max()&lt;/code&gt; &lt;/a&gt;的稀疏副本。与SparseReduceMax相比，此Op返回一个SparseTensor。</target>
        </trans-unit>
        <trans-unit id="7321159b3f380865d8a3ddbbd1ea9288e20a93c6" translate="yes" xml:space="preserve">
          <source>This Op takes a SparseTensor and is the sparse counterpart to &lt;a href=&quot;../math/reduce_max&quot;&gt;&lt;code&gt;tf.reduce_max()&lt;/code&gt;&lt;/a&gt;. In particular, this Op also returns a dense &lt;code&gt;Tensor&lt;/code&gt; if &lt;code&gt;output_is_sparse&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, or a &lt;code&gt;SparseTensor&lt;/code&gt; if &lt;code&gt;output_is_sparse&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">该Op带有SparseTensor，并且是&lt;a href=&quot;../math/reduce_max&quot;&gt; &lt;code&gt;tf.reduce_max()&lt;/code&gt; &lt;/a&gt;的稀疏副本。特别是，该作品也返回稠密 &lt;code&gt;Tensor&lt;/code&gt; 如果 &lt;code&gt;output_is_sparse&lt;/code&gt; 是 &lt;code&gt;False&lt;/code&gt; ，或 &lt;code&gt;SparseTensor&lt;/code&gt; 如果 &lt;code&gt;output_is_sparse&lt;/code&gt; 是 &lt;code&gt;True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b1103b246b6e1caad53d997c04c64c3898e6d3b5" translate="yes" xml:space="preserve">
          <source>This Op takes a SparseTensor and is the sparse counterpart to &lt;a href=&quot;../math/reduce_max&quot;&gt;&lt;code&gt;tf.reduce_max()&lt;/code&gt;&lt;/a&gt;. In particular, this Op also returns a dense &lt;code&gt;Tensor&lt;/code&gt; instead of a sparse one.</source>
          <target state="translated">该Op带有SparseTensor，并且是&lt;a href=&quot;../math/reduce_max&quot;&gt; &lt;code&gt;tf.reduce_max()&lt;/code&gt; &lt;/a&gt;的稀疏副本。特别是，此Op还返回密集的 &lt;code&gt;Tensor&lt;/code&gt; ,而不是稀疏的Tensor。</target>
        </trans-unit>
        <trans-unit id="a81865bba66b7cb3ccd5c843f29a627466d44c58" translate="yes" xml:space="preserve">
          <source>This Op takes a SparseTensor and is the sparse counterpart to &lt;a href=&quot;../math/reduce_sum&quot;&gt;&lt;code&gt;tf.reduce_sum()&lt;/code&gt;&lt;/a&gt;. In contrast to SparseReduceSum, this Op returns a SparseTensor.</source>
          <target state="translated">该Op带有SparseTensor，并且是&lt;a href=&quot;../math/reduce_sum&quot;&gt; &lt;code&gt;tf.reduce_sum()&lt;/code&gt; &lt;/a&gt;的稀疏副本。与SparseReduceSum相比，此Op返回一个SparseTensor。</target>
        </trans-unit>
        <trans-unit id="a574e60c745da5b56aeabab5fc189ca97095b4e6" translate="yes" xml:space="preserve">
          <source>This Op takes a SparseTensor and is the sparse counterpart to &lt;a href=&quot;../math/reduce_sum&quot;&gt;&lt;code&gt;tf.reduce_sum()&lt;/code&gt;&lt;/a&gt;. In particular, this Op also returns a dense &lt;code&gt;Tensor&lt;/code&gt; if &lt;code&gt;output_is_sparse&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, or a &lt;code&gt;SparseTensor&lt;/code&gt; if &lt;code&gt;output_is_sparse&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">该Op带有SparseTensor，并且是&lt;a href=&quot;../math/reduce_sum&quot;&gt; &lt;code&gt;tf.reduce_sum()&lt;/code&gt; &lt;/a&gt;的稀疏副本。特别是，该作品也返回稠密 &lt;code&gt;Tensor&lt;/code&gt; 如果 &lt;code&gt;output_is_sparse&lt;/code&gt; 是 &lt;code&gt;False&lt;/code&gt; ，或 &lt;code&gt;SparseTensor&lt;/code&gt; 如果 &lt;code&gt;output_is_sparse&lt;/code&gt; 是 &lt;code&gt;True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0611fe28cd2ae42a84cd683414a5531be62247ed" translate="yes" xml:space="preserve">
          <source>This Op takes a SparseTensor and is the sparse counterpart to &lt;a href=&quot;../math/reduce_sum&quot;&gt;&lt;code&gt;tf.reduce_sum()&lt;/code&gt;&lt;/a&gt;. In particular, this Op also returns a dense &lt;code&gt;Tensor&lt;/code&gt; instead of a sparse one.</source>
          <target state="translated">该Op带有SparseTensor，并且是&lt;a href=&quot;../math/reduce_sum&quot;&gt; &lt;code&gt;tf.reduce_sum()&lt;/code&gt; &lt;/a&gt;的稀疏副本。特别是，此Op还返回密集的 &lt;code&gt;Tensor&lt;/code&gt; ,而不是稀疏的Tensor。</target>
        </trans-unit>
        <trans-unit id="9513360fe89de6564516b0de822250d992516167" translate="yes" xml:space="preserve">
          <source>This adjusts the dynamic range of the gradient evaluation by scaling up the &lt;code&gt;loss&lt;/code&gt; value. The gradient values are then scaled back down by the recipricol of the loss scale. This is useful in reduced precision training where small gradient values would otherwise underflow the representable range.</source>
          <target state="translated">这通过按比例增加 &lt;code&gt;loss&lt;/code&gt; 值来调整梯度评估的动态范围。然后，通过损失标度的公式将梯度值按比例缩小。这在降低精度的训练中很有用，在该训练中，小梯度值会超出可表示的范围。</target>
        </trans-unit>
        <trans-unit id="f5e97086794cce37b1d5d739075a4835500e01f8" translate="yes" xml:space="preserve">
          <source>This adjusts the dynamic range of the gradient evaluation by scaling up the &lt;code&gt;loss&lt;/code&gt; value. The gradient values are then scaled back down by the reciprocal of the loss scale. This is useful in reduced precision training where small gradient values would otherwise underflow the representable range.</source>
          <target state="translated">这通过按比例增加 &lt;code&gt;loss&lt;/code&gt; 值来调整梯度评估的动态范围。然后，通过损失标度的倒数将梯度值按比例缩小。这在降低精度的训练中很有用，在该训练中，小梯度值会超出可表示的范围。</target>
        </trans-unit>
        <trans-unit id="33c2fc8a5963a345edc285e854dc6f61f0166cef" translate="yes" xml:space="preserve">
          <source>This allows 'names' which should be a list of names.</source>
          <target state="translated">这允许'名字'应该是一个名字列表。</target>
        </trans-unit>
        <trans-unit id="4742c30308cfd6082fa96c4df5f33b7fbb5afa83" translate="yes" xml:space="preserve">
          <source>This allows communication and coordination when there are multiple calls to the step_fn triggered by a call to &lt;code&gt;strategy.experimental_run_v2(step_fn, ...)&lt;/code&gt;.</source>
          <target state="translated">当对 &lt;code&gt;strategy.experimental_run_v2(step_fn, ...)&lt;/code&gt; 的调用触发了对step_fn的多次调用时，这允许进行通信和协调。</target>
        </trans-unit>
        <trans-unit id="54e8d42d088337b5915c0ad3a362c2e5bfd670e4" translate="yes" xml:space="preserve">
          <source>This allows communication and coordination when there are multiple calls to the step_fn triggered by a call to &lt;code&gt;strategy.run(step_fn, ...)&lt;/code&gt;.</source>
          <target state="translated">当对 &lt;code&gt;strategy.run(step_fn, ...)&lt;/code&gt; 的调用触发了对step_fn的多次调用时，这允许进行通信和协调。</target>
        </trans-unit>
        <trans-unit id="6406854276bccf2ce7a8330ff5c67c7e93bc6f24" translate="yes" xml:space="preserve">
          <source>This allows creating a sub-tensor from part of the current contents of a variable. See &lt;a href=&quot;../../tensor#__getitem__&quot;&gt;&lt;code&gt;tf.Tensor.&lt;strong&gt;getitem&lt;/strong&gt;&lt;/code&gt;&lt;/a&gt; for detailed examples of slicing.</source>
          <target state="translated">这允许从变量的当前内容的一部分创建子张量。参见&lt;a href=&quot;../../tensor#__getitem__&quot;&gt; &lt;code&gt;tf.Tensor.&lt;strong&gt;getitem&lt;/strong&gt;&lt;/code&gt; &lt;/a&gt;提供了切片的详细示例。</target>
        </trans-unit>
        <trans-unit id="3f9bd14a03a5b9808d578e330775308ae0518e1f" translate="yes" xml:space="preserve">
          <source>This allows creating a sub-tensor from part of the current contents of a variable. See &lt;a href=&quot;tensor#__getitem__&quot;&gt;&lt;code&gt;tf.Tensor.&lt;strong&gt;getitem&lt;/strong&gt;&lt;/code&gt;&lt;/a&gt; for detailed examples of slicing.</source>
          <target state="translated">这允许从变量的当前内容的一部分创建子张量。参见&lt;a href=&quot;tensor#__getitem__&quot;&gt; &lt;code&gt;tf.Tensor.&lt;strong&gt;getitem&lt;/strong&gt;&lt;/code&gt; &lt;/a&gt;提供了切片的详细示例。</target>
        </trans-unit>
        <trans-unit id="4a0031976f29ed8ed5b38f0545ffe09ca05a84a3" translate="yes" xml:space="preserve">
          <source>This allows reading and writing to this tensors w/o copies. This more closely mirrors the C++ Interpreter class interface's tensor() member, hence the name. Be careful to not hold these output references through calls to &lt;code&gt;allocate_tensors()&lt;/code&gt; and &lt;code&gt;invoke()&lt;/code&gt;. This function cannot be used to read intermediate results.</source>
          <target state="translated">这允许在没有张量的情况下对该张量进行读写。这更紧密地反映了C ++ Interpreter类接口的tensor（）成员，因此而得名。注意不要通过调用 &lt;code&gt;allocate_tensors()&lt;/code&gt; 和 &lt;code&gt;invoke()&lt;/code&gt; 来保存这些输出引用。此功能不能用于读取中间结果。</target>
        </trans-unit>
        <trans-unit id="aab58add7bb75e6c4119bf13ecc09894979d960d" translate="yes" xml:space="preserve">
          <source>This allows you to save the entirety of the state of a model in a single file.</source>
          <target state="translated">这允许您将模型的全部状态保存在一个文件中。</target>
        </trans-unit>
        <trans-unit id="083c822ae2751294f6b801ab1aec618144643d76" translate="yes" xml:space="preserve">
          <source>This also supports either output striding via the optional &lt;code&gt;strides&lt;/code&gt; parameter or atrous convolution (also known as convolution with holes or dilated convolution, based on the French word &quot;trous&quot; meaning holes in English) via the optional &lt;code&gt;dilation_rate&lt;/code&gt; parameter. Currently, however, output striding is not supported for atrous convolutions.</source>
          <target state="translated">这也支持两种输出通过可选的跨越式 &lt;code&gt;strides&lt;/code&gt; 参数或atrous卷积（也称为有孔或扩张的卷积卷积的基础上，法语单词&amp;ldquo;劈窗&amp;rdquo;在英语解释孔）通过可选的 &lt;code&gt;dilation_rate&lt;/code&gt; 参数。但是，目前，无速度卷积不支持输出跨步。</target>
        </trans-unit>
        <trans-unit id="0282c5f56f860d1be334e321248ec8429b6f156d" translate="yes" xml:space="preserve">
          <source>This also supports either output striding via the optional &lt;code&gt;strides&lt;/code&gt; parameter or atrous convolution (also known as convolution with holes or dilated convolution, based on the French word &quot;trous&quot; meaning holes in English) via the optional &lt;code&gt;dilations&lt;/code&gt; parameter. Currently, however, output striding is not supported for atrous convolutions.</source>
          <target state="translated">这也支持两种输出通过可选的跨越式 &lt;code&gt;strides&lt;/code&gt; 参数或atrous卷积（也称为有孔或扩张的卷积卷积的基础上，法语单词&amp;ldquo;劈窗&amp;rdquo;在英语解释孔）通过可选的 &lt;code&gt;dilations&lt;/code&gt; 参数。但是，目前，无速度卷积不支持输出跨步。</target>
        </trans-unit>
        <trans-unit id="40ecf12ea54cbb9c03cee235c92870bdadc05d0e" translate="yes" xml:space="preserve">
          <source>This assumes the input dictionary contains a &lt;code&gt;SparseTensor&lt;/code&gt; for key 'terms', and a &lt;code&gt;SparseTensor&lt;/code&gt; for key 'frequencies'. These 2 tensors must have the same indices and dense shape.</source>
          <target state="translated">这是假设输入字典包含 &lt;code&gt;SparseTensor&lt;/code&gt; 关键&amp;ldquo;条款&amp;rdquo;，以及 &lt;code&gt;SparseTensor&lt;/code&gt; 关键&amp;ldquo;频率&amp;rdquo;。这两个张量必须具有相同的索引和密集的形状。</target>
        </trans-unit>
        <trans-unit id="ec5e01502dedce9b5506d5284b5d16bed3dce6f8" translate="yes" xml:space="preserve">
          <source>This avoids adding &lt;code&gt;numpy_input&lt;/code&gt; as a large constant in the graph, and copies the data to the machine or machines that will be processing the input.</source>
          <target state="translated">这样可以避免将 &lt;code&gt;numpy_input&lt;/code&gt; 作为大常量添加到图形中，并将数据复制到将要处理输入的一台或多台计算机上。</target>
        </trans-unit>
        <trans-unit id="542cdf0657b556d7d4f13254bf996896da1af7f2" translate="yes" xml:space="preserve">
          <source>This behaves similarly to &lt;a href=&quot;../../name_scope&quot;&gt;&lt;code&gt;tf.name_scope&lt;/code&gt;&lt;/a&gt;, except that it returns a generated summary tag in addition to the scope name. The tag is structurally similar to the scope name - derived from the user-provided name, prefixed with enclosing name scopes if any - but we relax the constraint that it be uniquified, as well as the character set limitation (so the user-provided name can contain characters not legal for scope names; in the scope name these are removed).</source>
          <target state="translated">此行为类似于&lt;a href=&quot;../../name_scope&quot;&gt; &lt;code&gt;tf.name_scope&lt;/code&gt; &lt;/a&gt;，除了它除了作用域名称外还返回生成的摘要标记。该标签在结构上与作用域名称类似-从用户提供的名称派生，并带有封闭的名称范围（如果有的话）-但我们放宽了其唯一性的约束以及字符集限制（因此，用户提供的名称可以包含对范围名称不合法的字符；在范围名称中将这些字符删除）。</target>
        </trans-unit>
        <trans-unit id="5d690c9d397a6da08fd08939e695a0fb6ea89e4b" translate="yes" xml:space="preserve">
          <source>This behavior gives control to callers on what to do if checkpoints do not come fast enough or stop being generated. For example, if callers have a way to detect that the training has stopped and know that no new checkpoints will be generated, they can provide a &lt;code&gt;timeout_fn&lt;/code&gt; that returns &lt;code&gt;True&lt;/code&gt; when the training has stopped. If they know that the training is still going on they return &lt;code&gt;False&lt;/code&gt; instead.</source>
          <target state="translated">如果检查点未足够快地生成或停止生成，则此行为可以控制呼叫者。例如，如果呼叫者有办法检测到训练已停止并且知道不会生成新的检查点，则他们可以提供一个 &lt;code&gt;timeout_fn&lt;/code&gt; ，当训练停止时该值将返回 &lt;code&gt;True&lt;/code&gt; 。如果他们知道培训仍在继续，则返回 &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5d87911d57456c793352550a56288318dbdc2112" translate="yes" xml:space="preserve">
          <source>This behavior has been introduced in TensorFlow 2.0, in order to enable &lt;code&gt;layer.trainable = False&lt;/code&gt; to produce the most commonly expected behavior in the convnet fine-tuning use case.</source>
          <target state="translated">TensorFlow 2.0中已引入此行为，以使 &lt;code&gt;layer.trainable = False&lt;/code&gt; 可以在convnet微调用例中产生最普遍预期的行为。</target>
        </trans-unit>
        <trans-unit id="566da943133ad9f7ff89e0ae58ccee97f0942cd9" translate="yes" xml:space="preserve">
          <source>This behavior only occurs as of TensorFlow 2.0. In 1.*, setting &lt;code&gt;layer.trainable = False&lt;/code&gt; would freeze the layer but would not switch it to inference mode.</source>
          <target state="translated">仅从TensorFlow 2.0起才会出现此行为。在1. *中，设置 &lt;code&gt;layer.trainable = False&lt;/code&gt; 将冻结该层，但不会将其切换到推理模式。</target>
        </trans-unit>
        <trans-unit id="947e43a536a4f632b306b7d5c3e4805a08d13eec" translate="yes" xml:space="preserve">
          <source>This blocks the calling thread until the thread whose join() method is called terminates -- either normally or through an unhandled exception or until the optional timeout occurs.</source>
          <target state="translated">这将阻止调用线程,直到其join()方法被调用的线程终止--正常情况下或通过一个未处理的异常或直到可选的超时发生。</target>
        </trans-unit>
        <trans-unit id="f576e406288a87f269b5d6306d3d486ac32d4798" translate="yes" xml:space="preserve">
          <source>This boolean flag determines whether variables should be initialized as they are instantiated (default), or if the user should handle the initialization (e.g. via &lt;a href=&quot;../../compat/v1/initialize_all_variables&quot;&gt;&lt;code&gt;tf.compat.v1.initialize_all_variables()&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">该布尔值标志确定是否应在实例化变量时对其进行初始化（默认设置），或者用户是否应进行初始化（例如，通过&lt;a href=&quot;../../compat/v1/initialize_all_variables&quot;&gt; &lt;code&gt;tf.compat.v1.initialize_all_variables()&lt;/code&gt; &lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="4711e34ac0b0f311f0826db62648fbfab1ecfcb6" translate="yes" xml:space="preserve">
          <source>This boolean is True when this is an export in the end of training. It is False for the intermediate exports during the training. When passing &lt;code&gt;Exporter&lt;/code&gt; to &lt;a href=&quot;train_and_evaluate&quot;&gt;&lt;code&gt;tf.estimator.train_and_evaluate&lt;/code&gt;&lt;/a&gt;&lt;code&gt;is_the_final_export&lt;/code&gt; is always False if &lt;a href=&quot;trainspec#max_steps&quot;&gt;&lt;code&gt;TrainSpec.max_steps&lt;/code&gt;&lt;/a&gt; is &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">如果这是训练结束时的导出，则此布尔值为True。培训期间的中间出口是错误的。如果将&lt;a href=&quot;trainspec#max_steps&quot;&gt; &lt;code&gt;TrainSpec.max_steps&lt;/code&gt; &lt;/a&gt;为 &lt;code&gt;None&lt;/code&gt; ，&lt;a href=&quot;train_and_evaluate&quot;&gt; &lt;code&gt;tf.estimator.train_and_evaluate&lt;/code&gt; &lt;/a&gt; &lt;code&gt;Exporter&lt;/code&gt; 传递给tf.estimator.train_and_evaluate时， &lt;code&gt;is_the_final_export&lt;/code&gt; 始终为False 。</target>
        </trans-unit>
        <trans-unit id="0d320c46f5814a56c1f3f60058177b848b6fe981" translate="yes" xml:space="preserve">
          <source>This call blocks until a set of threads have terminated. The set of thread is the union of the threads passed in the &lt;code&gt;threads&lt;/code&gt; argument and the list of threads that registered with the coordinator by calling &lt;a href=&quot;coordinator#register_thread&quot;&gt;&lt;code&gt;Coordinator.register_thread()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">该调用将阻塞，直到一组线程终止。线程集是在 &lt;code&gt;threads&lt;/code&gt; 参数中传递的线程与通过调用&lt;a href=&quot;coordinator#register_thread&quot;&gt; &lt;code&gt;Coordinator.register_thread()&lt;/code&gt; &lt;/a&gt;向协调器注册的线程列表的并集。</target>
        </trans-unit>
        <trans-unit id="51c54c7d5726811d44822079b16e485771669c24" translate="yes" xml:space="preserve">
          <source>This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution).</source>
          <target state="translated">当启用急切执行时,这个调用会被忽略(在这种情况下,变量更新是在飞行中运行的,因此不需要为以后的执行进行跟踪)。</target>
        </trans-unit>
        <trans-unit id="062df39159f4972c7eb0c0bbc3c621a80385af7d" translate="yes" xml:space="preserve">
          <source>This callback is automatically applied to every Keras model.</source>
          <target state="translated">该回调自动应用于每个Keras模型。</target>
        </trans-unit>
        <trans-unit id="4a2209138c3b99588918296a296e9d5cba348ee9" translate="yes" xml:space="preserve">
          <source>This callback is automatically applied to every Keras model. The &lt;code&gt;History&lt;/code&gt; object gets returned by the &lt;code&gt;fit&lt;/code&gt; method of models.</source>
          <target state="translated">该回调将自动应用于每个Keras模型。该 &lt;code&gt;History&lt;/code&gt; 对象被用返回的 &lt;code&gt;fit&lt;/code&gt; 的模型方法。</target>
        </trans-unit>
        <trans-unit id="9a8fccab0666eafd61c1fb78dee506da35fb92ce" translate="yes" xml:space="preserve">
          <source>This callback is constructed with anonymous functions that will be called at the appropriate time. Note that the callbacks expects positional arguments, as:</source>
          <target state="translated">这个回调是用匿名函数构造的,将在适当的时候被调用。请注意,该回调期望使用位置参数,如。</target>
        </trans-unit>
        <trans-unit id="d5a28ff485e6238d1ad10ebb9f92b4945e5fbef4" translate="yes" xml:space="preserve">
          <source>This callback is not compatible with disabling eager execution.</source>
          <target state="translated">这个回调与禁用急切执行不兼容。</target>
        </trans-unit>
        <trans-unit id="d2fd53cd69211c630c172b830b110ef6741f78e7" translate="yes" xml:space="preserve">
          <source>This callback logs events for TensorBoard, including:</source>
          <target state="translated">这个回调记录TensorBoard的事件,包括。</target>
        </trans-unit>
        <trans-unit id="6cbedc26a564f8697fb303c927049b43625f8671" translate="yes" xml:space="preserve">
          <source>This can also be used in a &lt;a href=&quot;tableconfig&quot;&gt;&lt;code&gt;tf.tpu.experimental.embedding.TableConfig&lt;/code&gt;&lt;/a&gt; as the optimizer parameter to set a table specific optimizer. This will override the optimizer and parameters for global embedding optimizer defined above:</source>
          <target state="translated">也可以在&lt;a href=&quot;tableconfig&quot;&gt; &lt;code&gt;tf.tpu.experimental.embedding.TableConfig&lt;/code&gt; 中将其&lt;/a&gt;用作优化程序参数，以设置特定于表的优化程序。这将覆盖上面定义的全局嵌入优化器的优化器和参数：</target>
        </trans-unit>
        <trans-unit id="6c4d9ff06bd1c217d75cb9a57ebc28626f964add" translate="yes" xml:space="preserve">
          <source>This can always be checked statically, so this method returns nothing.</source>
          <target state="translated">这可以一直静态地检查,所以这个方法不返回任何东西。</target>
        </trans-unit>
        <trans-unit id="6f195f030f5d6a3952c927a303c10a3d51f131c5" translate="yes" xml:space="preserve">
          <source>This can be faster than multiple individual &lt;code&gt;reduce&lt;/code&gt;s because we can fuse several tensors into one or multiple packs before reduction.</source>
          <target state="translated">这可能比多个单独的 &lt;code&gt;reduce&lt;/code&gt; 快，因为我们可以在缩减之前将多个张量融合到一个或多个包中。</target>
        </trans-unit>
        <trans-unit id="ef728ae4ad68ee3a6531133cdd224d4406968fce" translate="yes" xml:space="preserve">
          <source>This can be passed to methods like &lt;code&gt;tf.distribute.get_replica_context().all_reduce()&lt;/code&gt; to optimize collective operation performance. Note that these are only hints, which may or may not change the actual behavior. Some options only apply to certain strategy and are ignored by others.</source>
          <target state="translated">可以将其传递给 &lt;code&gt;tf.distribute.get_replica_context().all_reduce()&lt;/code&gt; 以优化集合操作的性能。请注意，这些仅是提示，可能会或可能不会更改实际行为。有些选项仅适用于某些策略，而其他选项则忽略。</target>
        </trans-unit>
        <trans-unit id="f4c1e8ee1a694edb65fdf5329eae65afdec683d9" translate="yes" xml:space="preserve">
          <source>This can be used as a &quot;join&quot; mechanism for parallel computations: all the argument tensors can be computed in parallel, but the values of any tensor returned by &lt;code&gt;tuple&lt;/code&gt; are only available after all the parallel computations are done.</source>
          <target state="translated">这可以用作并行计算的&amp;ldquo;联接&amp;rdquo;机制：所有参数张量都可以并行计算，但是由 &lt;code&gt;tuple&lt;/code&gt; 返回的任何张量的值只有在完成所有并行计算后才可用。</target>
        </trans-unit>
        <trans-unit id="66b08400383b183cc1cba0b591ec90b5fe85d73d" translate="yes" xml:space="preserve">
          <source>This can be used as a loss-function during optimization so as to suppress noise in images. If you have a batch of images, then you should calculate the scalar loss-value as the sum: &lt;code&gt;loss = tf.reduce_sum(tf.image.total_variation(images))&lt;/code&gt;</source>
          <target state="translated">可以在优化过程中用作损失函数，以抑制图像中的噪声。如果您有一批图像，则应将标量损失值计算为总和： &lt;code&gt;loss = tf.reduce_sum(tf.image.total_variation(images))&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f9d5c2eebbfc8025f5c8da3b80769337a79a6187" translate="yes" xml:space="preserve">
          <source>This can be used to hold some strategy specific configs.</source>
          <target state="translated">这可以用来保存一些特定策略的配置。</target>
        </trans-unit>
        <trans-unit id="ff61da80eb1038583fadf6e588b06357b27a55d8" translate="yes" xml:space="preserve">
          <source>This can be useful for debugging or profiling. For example, let's say you implemented a simple iterative sqrt function, and you want to collect the intermediate values and plot the convergence. Appending the values to a list in &lt;code&gt;@tf.function&lt;/code&gt; normally wouldn't work since it will just record the Tensors being traced, not the values. Instead, you can do the following.</source>
          <target state="translated">这对于调试或分析很有用。例如，假设您实现了一个简单的迭代sqrt函数，并且想要收集中间值并绘制收敛性。将值追加到 &lt;code&gt;@tf.function&lt;/code&gt; 的列表通常将不起作用，因为它只会记录要跟踪的张量，而不是值。相反，您可以执行以下操作。</target>
        </trans-unit>
        <trans-unit id="5bc85a9ecd105989dec28b806afbbe4faa69f248" translate="yes" xml:space="preserve">
          <source>This can be useful if you want to log debug a training algorithm, report stats about the slots, etc.</source>
          <target state="translated">如果你想对训练算法进行日志调试,报告有关槽点的统计数据等,这可以很有用。</target>
        </trans-unit>
        <trans-unit id="de803dd8ed0f3487c59166443a170b97850a2c8d" translate="yes" xml:space="preserve">
          <source>This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...</source>
          <target state="translated">这个类允许对文本语料进行向量化,通过将每个文本转化为一个整数序列(每个整数是一个字典中的标记的索引)或者转化为一个向量,其中每个标记的系数可以是二进制的,基于字数,基于tf-idf......。</target>
        </trans-unit>
        <trans-unit id="c35c7cb7e9df11039724d697524b210395fe269e" translate="yes" xml:space="preserve">
          <source>This class assumes each worker is running the same code independently, but parameter servers are running a standard server. This means that while each worker will synchronously compute a single gradient update across all GPUs, updates between workers proceed asynchronously. Operations that occur only on the first replica (such as incrementing the global step), will occur on the first replica &lt;em&gt;of every worker&lt;/em&gt;.</source>
          <target state="translated">此类假定每个工作程序都在独立运行相同的代码，但是参数服务器在运行标准服务器。这意味着尽管每个工作人员将在所有GPU上同步计算单个渐变更新，但工作人员之间的更新将异步进行。仅在第一个副本上发生的操作（例如，增加全局步长）将在&lt;em&gt;每个worker&lt;/em&gt;的第一个副本&lt;em&gt;上&lt;/em&gt;发生。</target>
        </trans-unit>
        <trans-unit id="1ea4dadc6d5580b6a9567a54f238c77dab3d0a06" translate="yes" xml:space="preserve">
          <source>This class caches file writers, one per directory.</source>
          <target state="translated">该类缓存文件写入器,每个目录一个。</target>
        </trans-unit>
        <trans-unit id="b8684cfbd26f7ff4ba54e9a816ce8e5ac6de8988" translate="yes" xml:space="preserve">
          <source>This class can be used to support training large embeddings on TPU. When creating an instance of this class, you must specify the complete set of tables and features you expect to lookup in those tables. See the documentation of &lt;a href=&quot;tableconfig&quot;&gt;&lt;code&gt;tf.tpu.experimental.embedding.TableConfig&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;featureconfig&quot;&gt;&lt;code&gt;tf.tpu.experimental.embedding.FeatureConfig&lt;/code&gt;&lt;/a&gt; for more details on the complete set of options. We will cover the basic usage here.</source>
          <target state="translated">此类可用于支持在TPU上训练大型嵌入。创建此类的实例时，必须指定完整的表集和期望在这些表中查找的功能。有关完整选项集的更多详细信息，请参阅&lt;a href=&quot;tableconfig&quot;&gt; &lt;code&gt;tf.tpu.experimental.embedding.TableConfig&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;featureconfig&quot;&gt; &lt;code&gt;tf.tpu.experimental.embedding.FeatureConfig&lt;/code&gt; &lt;/a&gt;的文档。我们将在这里介绍基本用法。</target>
        </trans-unit>
        <trans-unit id="e9243b860278b8d7215f95a8eb101fc7a883ab5b" translate="yes" xml:space="preserve">
          <source>This class can create placeholders for tf.Tensors, tf.SparseTensors, and tf.RaggedTensors by choosing 'sparse=True' or 'ragged=True'.</source>
          <target state="translated">该类可以通过选择'sparse=True'或'ragged=True'来创建tf.Tensors、tf.SparseTensors和tf.RaggedTensors的占位符。</target>
        </trans-unit>
        <trans-unit id="805ec9c49684c57a889497d64617b10bca6259a1" translate="yes" xml:space="preserve">
          <source>This class can create placeholders for tf.Tensors, tf.SparseTensors, and tf.RaggedTensors by choosing 'sparse=True' or 'ragged=True'. Note that 'sparse' and 'ragged' can't be configured to True at same time. Usage:</source>
          <target state="translated">该类可以通过选择'sparse=True'或'ragged=True'为tf.Tensors、tf.SparseTensors和tf.RaggedTensors创建占位符。注意'sparse'和'ragged'不能同时配置为True。使用方法</target>
        </trans-unit>
        <trans-unit id="b93eb86d4b8b199ed8de190972226d76d3ddbda5" translate="yes" xml:space="preserve">
          <source>This class defines the API to add Ops to train a model. You never use this class directly, but instead instantiate one of its subclasses such as &lt;a href=&quot;sgd&quot;&gt;&lt;code&gt;tf.keras.optimizers.SGD&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;adam&quot;&gt;&lt;code&gt;tf.keras.optimizers.Adam&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">此类定义用于添加Ops来训练模型的API。您永远不会直接使用此类，而是实例化其子类之一，例如&lt;a href=&quot;sgd&quot;&gt; &lt;code&gt;tf.keras.optimizers.SGD&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;adam&quot;&gt; &lt;code&gt;tf.keras.optimizers.Adam&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="1921ba1f544676f9bbcb9030a711c397087751d1" translate="yes" xml:space="preserve">
          <source>This class defines the API to add Ops to train a model. You never use this class directly, but instead instantiate one of its subclasses such as &lt;code&gt;GradientDescentOptimizer&lt;/code&gt;, &lt;code&gt;AdagradOptimizer&lt;/code&gt;, or &lt;code&gt;MomentumOptimizer&lt;/code&gt;.</source>
          <target state="translated">此类定义用于添加Ops来训练模型的API。您永远不会直接使用此类，而是实例化其子类之一，例如 &lt;code&gt;GradientDescentOptimizer&lt;/code&gt; ， &lt;code&gt;AdagradOptimizer&lt;/code&gt; 或 &lt;code&gt;MomentumOptimizer&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f1b4934ebc6e87e946a8327b096ccaba6f902fbb" translate="yes" xml:space="preserve">
          <source>This class defines the key and value used for tf.lookup.TextFileInitializer.</source>
          <target state="translated">这个类定义了用于tf.lookup.TextFileInitializer的键和值。</target>
        </trans-unit>
        <trans-unit id="bbef232ee84dd43e03ff0bb31e2fff3d62ddb086" translate="yes" xml:space="preserve">
          <source>This class exports the serving graph and checkpoints at the end.</source>
          <target state="translated">这个类在最后导出服务图和检查点。</target>
        </trans-unit>
        <trans-unit id="2398f265a0682e91ce8fc30c31cc00a32b85cf94" translate="yes" xml:space="preserve">
          <source>This class exports the serving graph and checkpoints of the best models.</source>
          <target state="translated">该类导出最佳模型的服务图和检查点。</target>
        </trans-unit>
        <trans-unit id="f378b5fe5691dc39f9cd3d236136a8685104c4fd" translate="yes" xml:space="preserve">
          <source>This class has been deprecated. Please use &lt;a href=&quot;../../../lite/tfliteconverter&quot;&gt;&lt;code&gt;lite.TFLiteConverter&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">此类已弃用。请改用&lt;a href=&quot;../../../lite/tfliteconverter&quot;&gt; &lt;code&gt;lite.TFLiteConverter&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="56db13aef8a0368de7cd44eab17a2a6ce7b0dc42" translate="yes" xml:space="preserve">
          <source>This class has been deprecated. Please use &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter&quot;&gt;&lt;code&gt;lite.TFLiteConverter&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">此类已弃用。请改用&lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter&quot;&gt; &lt;code&gt;lite.TFLiteConverter&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="8616741a1ce86b957920807cdf7bdf7cd9ce000e" translate="yes" xml:space="preserve">
          <source>This class has two primary purposes:</source>
          <target state="translated">这门课有两个主要目的。</target>
        </trans-unit>
        <trans-unit id="42f7db2b92ba92041bfe671280afd36a3919a9f0" translate="yes" xml:space="preserve">
          <source>This class holds the configuration data for a single embedding feature. The main use is to assign features to &lt;a href=&quot;tableconfig&quot;&gt;&lt;code&gt;tf.tpu.experimental.embedding.TableConfig&lt;/code&gt;&lt;/a&gt;s via the table parameter:</source>
          <target state="translated">此类保存单个嵌入功能的配置数据。主要用途是通过table参数将功能分配给&lt;a href=&quot;tableconfig&quot;&gt; &lt;code&gt;tf.tpu.experimental.embedding.TableConfig&lt;/code&gt; &lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="3a9133b931fbc77d042cdefc7dee2428d484b0ed" translate="yes" xml:space="preserve">
          <source>This class holds the configuration data for a single embedding table. It is used as the &lt;code&gt;table&lt;/code&gt; parameter of a &lt;a href=&quot;featureconfig&quot;&gt;&lt;code&gt;tf.tpu.experimental.embedding.FeatureConfig&lt;/code&gt;&lt;/a&gt;. Multiple &lt;a href=&quot;featureconfig&quot;&gt;&lt;code&gt;tf.tpu.experimental.embedding.FeatureConfig&lt;/code&gt;&lt;/a&gt; objects can use the same &lt;a href=&quot;tableconfig&quot;&gt;&lt;code&gt;tf.tpu.experimental.embedding.TableConfig&lt;/code&gt;&lt;/a&gt; object. In this case a shared table will be created for those feature lookups.</source>
          <target state="translated">此类保存单个嵌入表的配置数据。它用作&lt;a href=&quot;featureconfig&quot;&gt; &lt;code&gt;tf.tpu.experimental.embedding.FeatureConfig&lt;/code&gt; &lt;/a&gt;的 &lt;code&gt;table&lt;/code&gt; 参数。多个&lt;a href=&quot;featureconfig&quot;&gt; &lt;code&gt;tf.tpu.experimental.embedding.FeatureConfig&lt;/code&gt; &lt;/a&gt;对象可以使用相同的&lt;a href=&quot;tableconfig&quot;&gt; &lt;code&gt;tf.tpu.experimental.embedding.TableConfig&lt;/code&gt; &lt;/a&gt;对象。在这种情况下，将为那些功能查找创建一个共享表。</target>
        </trans-unit>
        <trans-unit id="e41eb7b59b50f02eb388655630751957c627b643" translate="yes" xml:space="preserve">
          <source>This class implements &lt;code&gt;__enter__&lt;/code&gt; and &lt;code&gt;__exit__&lt;/code&gt;, and can be used in &lt;code&gt;with&lt;/code&gt; blocks like a normal file.</source>
          <target state="translated">此类实现 &lt;code&gt;__enter__&lt;/code&gt; 和 &lt;code&gt;__exit__&lt;/code&gt; ，并且可以 &lt;code&gt;with&lt;/code&gt; 普通文件之类的块一起使用。</target>
        </trans-unit>
        <trans-unit id="8e6a775ae22bedf95251724d923983db37089b28" translate="yes" xml:space="preserve">
          <source>This class implements &lt;code&gt;__enter__&lt;/code&gt; and &lt;code&gt;__exit__&lt;/code&gt;, and can be used in &lt;code&gt;with&lt;/code&gt; blocks like a normal file. (See the usage example above.)</source>
          <target state="translated">此类实现 &lt;code&gt;__enter__&lt;/code&gt; 和 &lt;code&gt;__exit__&lt;/code&gt; ，并且可以 &lt;code&gt;with&lt;/code&gt; 普通文件之类的块一起使用。（请参阅上面的用法示例。）</target>
        </trans-unit>
        <trans-unit id="61d78e6233b72d451051551f631a8d253b9c0d10" translate="yes" xml:space="preserve">
          <source>This class implements a simple mechanism to coordinate the termination of a set of threads.</source>
          <target state="translated">这个类实现了一个简单的机制来协调一组线程的终止。</target>
        </trans-unit>
        <trans-unit id="2c453470b9fb7d2dd4c8372805a2c5b45dcafe1a" translate="yes" xml:space="preserve">
          <source>This class in stateful and thread-compatible.</source>
          <target state="translated">这个类是有状态和线程兼容的。</target>
        </trans-unit>
        <trans-unit id="b6c71f2399c238428100228254e29a64f6c45e19" translate="yes" xml:space="preserve">
          <source>This class is a simple wrapper for a pair of &lt;code&gt;Tensor&lt;/code&gt; objects:</source>
          <target state="translated">此类是一对 &lt;code&gt;Tensor&lt;/code&gt; 对象的简单包装：</target>
        </trans-unit>
        <trans-unit id="aaddfb846b1d90d727149969bb3d79f9de0c7466" translate="yes" xml:space="preserve">
          <source>This class is a small wrapper that takes care of session creation and checkpoint recovery. It also provides functions that to facilitate coordination among multiple training threads or processes.</source>
          <target state="translated">该类是一个小型的包装器,负责会话创建和检查点恢复。它还提供了促进多个训练线程或进程之间协调的功能。</target>
        </trans-unit>
        <trans-unit id="17463f3d93317c6b5e6f0cc508b2a6f39e7358ea" translate="yes" xml:space="preserve">
          <source>This class is deprecated. For synchrononous training, please use &lt;a href=&quot;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute&quot;&gt;Distribution Strategies&lt;/a&gt;.</source>
          <target state="translated">此类已弃用。对于同步培训，请使用&amp;ldquo; &lt;a href=&quot;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute&quot;&gt;分发策略&amp;rdquo;&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="000d9e8de17ad9ab35fd8858d3f373310d7cf1c7" translate="yes" xml:space="preserve">
          <source>This class is deprecated. For synchronous training, please use &lt;a href=&quot;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute&quot;&gt;Distribution Strategies&lt;/a&gt;.</source>
          <target state="translated">此类已弃用。对于同步培训，请使用&amp;ldquo;&lt;a href=&quot;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute&quot;&gt;分发策略&amp;rdquo;&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4616d8f2e476585f5e22705937707332b5baeb5b" translate="yes" xml:space="preserve">
          <source>This class is deprecated. Please use &lt;a href=&quot;monitoredtrainingsession&quot;&gt;&lt;code&gt;tf.compat.v1.train.MonitoredTrainingSession&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">此类已弃用。请改用&lt;a href=&quot;monitoredtrainingsession&quot;&gt; &lt;code&gt;tf.compat.v1.train.MonitoredTrainingSession&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="42d3bc86be52a8736e6d7aa317815110401307ef" translate="yes" xml:space="preserve">
          <source>This class is heavily overloaded:</source>
          <target state="translated">这个班级严重超载。</target>
        </trans-unit>
        <trans-unit id="22170de053f3420fc08a96260e613e3fa8486018" translate="yes" xml:space="preserve">
          <source>This class is meant to be used with dynamic iteration primitives such as &lt;code&gt;while_loop&lt;/code&gt; and &lt;code&gt;map_fn&lt;/code&gt;. It supports gradient back-propagation via special &quot;flow&quot; control flow dependencies.</source>
          <target state="translated">此类应与动态迭代原语（如 &lt;code&gt;while_loop&lt;/code&gt; 和 &lt;code&gt;map_fn&lt;/code&gt; )一起使用。它通过特殊的&amp;ldquo;流&amp;rdquo;控制流相关性支持梯度反向传播。</target>
        </trans-unit>
        <trans-unit id="3c5c516c190cd8f8be6eb152047649a9bf6735af" translate="yes" xml:space="preserve">
          <source>This class is not thread-safe.</source>
          <target state="translated">这个类不是线程安全的。</target>
        </trans-unit>
        <trans-unit id="121c6274230c0f7d1b806b54f9d580ba8e2d228a" translate="yes" xml:space="preserve">
          <source>This class merges the output of multiple &lt;code&gt;Head&lt;/code&gt; objects. Specifically:</source>
          <target state="translated">此类合并多个 &lt;code&gt;Head&lt;/code&gt; 对象的输出。特别：</target>
        </trans-unit>
        <trans-unit id="7cd980fdc04e7da43f8abb3e3be6ffdf25b7685d" translate="yes" xml:space="preserve">
          <source>This class performs a model export everytime the new model is better than any existing model.</source>
          <target state="translated">该类每当新模型优于任何现有模型时,都会执行一次模型导出。</target>
        </trans-unit>
        <trans-unit id="391aed511e9154256ca236f23d6877cb0f9d37db" translate="yes" xml:space="preserve">
          <source>This class performs a single export at the end of training.</source>
          <target state="translated">该类在训练结束后进行一次导出。</target>
        </trans-unit>
        <trans-unit id="b83c252083ca3edbed9d1168a0ed9755da22c25f" translate="yes" xml:space="preserve">
          <source>This class performs a union given two or more existing ClusterResolvers. It merges the underlying ClusterResolvers, and returns one unified ClusterSpec when cluster_spec is called. The details of the merge function is documented in the cluster_spec function.</source>
          <target state="translated">这个类在给定两个或多个现有的ClusterResolvers的情况下执行一个联合。它合并了底层的ClusterResolvers,并在调用cluster_spec时返回一个统一的ClusterSpec。合并函数的细节在 cluster_spec 函数中得到了说明。</target>
        </trans-unit>
        <trans-unit id="631d6a58bb184da4fc166c1a1dbaf36a0e45d9c8" translate="yes" xml:space="preserve">
          <source>This class performs the softmax operation for you, so inputs should be e.g. linear projections of outputs by an LSTM.</source>
          <target state="translated">这个类为你执行softmax操作,所以输入应该是例如LSTM对输出的线性投影。</target>
        </trans-unit>
        <trans-unit id="d2810b13f641fd9987b2ce8b38f939b3369079e3" translate="yes" xml:space="preserve">
          <source>This class processes one step within the whole time sequence input, whereas &lt;code&gt;tf.keras.layer.GRU&lt;/code&gt; processes the whole sequence.</source>
          <target state="translated">此类处理整个时间序列输入中的一个步骤，而 &lt;code&gt;tf.keras.layer.GRU&lt;/code&gt; 处理整个序列。</target>
        </trans-unit>
        <trans-unit id="9a1d5a35670a2cc4cfac2401e85535a1c8acffb8" translate="yes" xml:space="preserve">
          <source>This class processes one step within the whole time sequence input, whereas &lt;code&gt;tf.keras.layer.LSTM&lt;/code&gt; processes the whole sequence.</source>
          <target state="translated">此类在整个时间序列输入中处理一个步骤，而 &lt;code&gt;tf.keras.layer.LSTM&lt;/code&gt; 处理整个序列。</target>
        </trans-unit>
        <trans-unit id="85aecc3ec7757c0f4286579ce3ab5119031dd197" translate="yes" xml:space="preserve">
          <source>This class processes one step within the whole time sequence input, whereas &lt;code&gt;tf.keras.layer.SimpleRNN&lt;/code&gt; processes the whole sequence.</source>
          <target state="translated">此类处理整个时序输入中的一个步骤，而 &lt;code&gt;tf.keras.layer.SimpleRNN&lt;/code&gt; 处理整个时序。</target>
        </trans-unit>
        <trans-unit id="c2cd1ab18438d633073fa2c3300d94950ce386b5" translate="yes" xml:space="preserve">
          <source>This class regularly exports the serving graph and checkpoints.</source>
          <target state="translated">该类定期导出服务图和检查点。</target>
        </trans-unit>
        <trans-unit id="44829f1dbd5fa6ec4b7c405e96fb0cb7473fedd5" translate="yes" xml:space="preserve">
          <source>This class specifies the configurations for an &lt;code&gt;Estimator&lt;/code&gt; run.</source>
          <target state="translated">此类指定 &lt;code&gt;Estimator&lt;/code&gt; 运行的配置。</target>
        </trans-unit>
        <trans-unit id="349b70f1528f5d483fc48af054599841c5669982" translate="yes" xml:space="preserve">
          <source>This class takes in a sequence of data-points gathered at equal intervals, along with time series parameters such as stride, length of history, etc., to produce batches for training/validation.</source>
          <target state="translated">该类采取以等间隔收集的数据点序列,以及时间序列参数,如步幅、历史长度等,生成训练/验证的批次。</target>
        </trans-unit>
        <trans-unit id="2020d8f19309c20263f69977f3eb5adfc93381e1" translate="yes" xml:space="preserve">
          <source>This class uses a &lt;a href=&quot;../variable&quot;&gt;&lt;code&gt;tf.Variable&lt;/code&gt;&lt;/a&gt; to manage its internal state. Every time random numbers are generated, the state of the generator will change. For example:</source>
          <target state="translated">此类使用&lt;a href=&quot;../variable&quot;&gt; &lt;code&gt;tf.Variable&lt;/code&gt; &lt;/a&gt;来管理其内部状态。每次生成随机数时，生成器的状态都会改变。例如：</target>
        </trans-unit>
        <trans-unit id="45bad515b0c24287a7857bc49730849b980995da" translate="yes" xml:space="preserve">
          <source>This classifier ignores feature values and will learn to predict the average value of each label. For single-label problems, this will predict the probability distribution of the classes as seen in the labels. For multi-label problems, this will predict the fraction of examples that are positive for each class.</source>
          <target state="translated">这个分类器忽略特征值,会学习预测每个标签的平均值。对于单标签问题,它将预测标签中看到的类的概率分布。对于多标签问题,它将预测每个类的正值例子的比例。</target>
        </trans-unit>
        <trans-unit id="1203741987f9f009e5e86b580142b5f23085017c" translate="yes" xml:space="preserve">
          <source>This computes the internal data stats related to the data-dependent transformations, based on an array of sample data.</source>
          <target state="translated">该功能根据样本数据数组计算与数据相关的变换相关的内部数据统计。</target>
        </trans-unit>
        <trans-unit id="38619017d4eec4e645fa2f91a523c788477cd3cb" translate="yes" xml:space="preserve">
          <source>This condition holds if for every pair of (possibly broadcast) elements &lt;code&gt;x[i]&lt;/code&gt;, &lt;code&gt;y[i]&lt;/code&gt;, we have</source>
          <target state="translated">对于每对（可能是广播的）元素 &lt;code&gt;x[i]&lt;/code&gt; ， &lt;code&gt;y[i]&lt;/code&gt; ，如果</target>
        </trans-unit>
        <trans-unit id="462aa0ba39503de33a78d3a4ed1c72d0eb71fa77" translate="yes" xml:space="preserve">
          <source>This condition holds if for every pair of (possibly broadcast) elements &lt;code&gt;x[i]&lt;/code&gt;, &lt;code&gt;y[i]&lt;/code&gt;, we have &lt;code&gt;x[i] != y[i]&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="translated">如果对于每对（可能广播的）元素 &lt;code&gt;x[i]&lt;/code&gt; ， &lt;code&gt;y[i]&lt;/code&gt; ，我们有 &lt;code&gt;x[i] != y[i]&lt;/code&gt; 则此条件成立。如果 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 都为空，则可以轻松满足。</target>
        </trans-unit>
        <trans-unit id="e25b6ae62c3ffc2ef4953d25e9698856f0a3b77a" translate="yes" xml:space="preserve">
          <source>This condition holds if for every pair of (possibly broadcast) elements &lt;code&gt;x[i]&lt;/code&gt;, &lt;code&gt;y[i]&lt;/code&gt;, we have &lt;code&gt;x[i] &amp;gt; y[i]&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="translated">如果对于每对（可能广播的）元素 &lt;code&gt;x[i]&lt;/code&gt; ， &lt;code&gt;y[i]&lt;/code&gt; ，我们有 &lt;code&gt;x[i] &amp;gt; y[i]&lt;/code&gt; 则此条件成立。如果 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 都为空，则可以轻松满足。</target>
        </trans-unit>
        <trans-unit id="ad801aabcdd92fbf94ec233e05dfeadd68d460b3" translate="yes" xml:space="preserve">
          <source>This condition holds if for every pair of (possibly broadcast) elements &lt;code&gt;x[i]&lt;/code&gt;, &lt;code&gt;y[i]&lt;/code&gt;, we have &lt;code&gt;x[i] &amp;gt;= y[i]&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="translated">如果对于每对（可能广播的）元素 &lt;code&gt;x[i]&lt;/code&gt; ， &lt;code&gt;y[i]&lt;/code&gt; ，我们有 &lt;code&gt;x[i] &amp;gt;= y[i]&lt;/code&gt; 则此条件成立。如果 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 都为空，则可以轻松满足。</target>
        </trans-unit>
        <trans-unit id="512ed74cbec98906ad1a4d1866d9484e4d2ad504" translate="yes" xml:space="preserve">
          <source>This condition holds if for every pair of (possibly broadcast) elements &lt;code&gt;x[i]&lt;/code&gt;, &lt;code&gt;y[i]&lt;/code&gt;, we have &lt;code&gt;x[i] &amp;lt; y[i]&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="translated">如果对于每对（可能广播的）元素 &lt;code&gt;x[i]&lt;/code&gt; ， &lt;code&gt;y[i]&lt;/code&gt; ，我们有 &lt;code&gt;x[i] &amp;lt; y[i]&lt;/code&gt; 则此条件成立。如果 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 都为空，则可以轻松满足。</target>
        </trans-unit>
        <trans-unit id="81434868d39629bbc35c170df36a994a92015b46" translate="yes" xml:space="preserve">
          <source>This condition holds if for every pair of (possibly broadcast) elements &lt;code&gt;x[i]&lt;/code&gt;, &lt;code&gt;y[i]&lt;/code&gt;, we have &lt;code&gt;x[i] &amp;lt;= y[i]&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="translated">如果对于每对（可能广播的）元素 &lt;code&gt;x[i]&lt;/code&gt; ， &lt;code&gt;y[i]&lt;/code&gt; ，我们有 &lt;code&gt;x[i] &amp;lt;= y[i]&lt;/code&gt; 则此条件成立。如果 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 都为空，则可以轻松满足。</target>
        </trans-unit>
        <trans-unit id="93ee9d4d5438fdc7dae43626edce4fb33cefa73b" translate="yes" xml:space="preserve">
          <source>This condition holds if for every pair of (possibly broadcast) elements &lt;code&gt;x[i]&lt;/code&gt;, &lt;code&gt;y[i]&lt;/code&gt;, we have &lt;code&gt;x[i] == y[i]&lt;/code&gt;. If both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are empty, this is trivially satisfied.</source>
          <target state="translated">如果对于每对（可能广播的）元素 &lt;code&gt;x[i]&lt;/code&gt; ， &lt;code&gt;y[i]&lt;/code&gt; ，我们有 &lt;code&gt;x[i] == y[i]&lt;/code&gt; 则此条件成立。如果 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 都为空，则可以轻松满足。</target>
        </trans-unit>
        <trans-unit id="1107421e32062956898c5a66e60e49bd95c7ed45" translate="yes" xml:space="preserve">
          <source>This constraint can be applied to any &lt;code&gt;Conv2D&lt;/code&gt; layer version, including &lt;code&gt;Conv2DTranspose&lt;/code&gt; and &lt;code&gt;SeparableConv2D&lt;/code&gt;, and with either &lt;code&gt;&quot;channels_last&quot;&lt;/code&gt; or &lt;code&gt;&quot;channels_first&quot;&lt;/code&gt; data format. The method assumes the weight tensor is of shape &lt;code&gt;(rows, cols, input_depth, output_depth)&lt;/code&gt;.</source>
          <target state="translated">这种约束可以应用于任何 &lt;code&gt;Conv2D&lt;/code&gt; 层版本，包括 &lt;code&gt;Conv2DTranspose&lt;/code&gt; 和 &lt;code&gt;SeparableConv2D&lt;/code&gt; ，并与任一 &lt;code&gt;&quot;channels_last&quot;&lt;/code&gt; 或 &lt;code&gt;&quot;channels_first&quot;&lt;/code&gt; 数据格式。该方法假定权重张量是形状 &lt;code&gt;(rows, cols, input_depth, output_depth)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e5e140df8d55549c17df75f17763f4c88b908cd9" translate="yes" xml:space="preserve">
          <source>This constructor creates both a &lt;code&gt;variable&lt;/code&gt; Op and an &lt;code&gt;assign&lt;/code&gt; Op to set the variable to its initial value.</source>
          <target state="translated">该构造函数同时创建 &lt;code&gt;variable&lt;/code&gt; Op和 &lt;code&gt;assign&lt;/code&gt; Op，以将变量设置为其初始值。</target>
        </trans-unit>
        <trans-unit id="0853b5f1d96849514913dcea4715e205fe7e9be9" translate="yes" xml:space="preserve">
          <source>This constructor is private -- please use one of the following ops to build &lt;code&gt;RaggedTensor&lt;/code&gt;s:</source>
          <target state="translated">此构造函数是私有的-请使用以下操作之一来构建 &lt;code&gt;RaggedTensor&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="fa9706d28ea0bf76646310aace552e8f79dee4fd" translate="yes" xml:space="preserve">
          <source>This constructor only applies if the algorithm is a counter-based algorithm. See method &lt;code&gt;key&lt;/code&gt; for the meaning of &quot;key&quot; and &quot;counter&quot;.</source>
          <target state="translated">仅当算法是基于计数器的算法时，此构造函数才适用。有关&amp;ldquo;密钥&amp;rdquo;和&amp;ldquo;计数器&amp;rdquo;的含义，请参见方法 &lt;code&gt;key&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="abd1f92b92080f9534139ad50b5578924ec063f6" translate="yes" xml:space="preserve">
          <source>This contains most of the synchronization implementation and also wraps the apply_gradients() from the real optimizer.</source>
          <target state="translated">这包含了大部分的同步实现,并且还封装了真正优化器中的apply_gradients()。</target>
        </trans-unit>
        <trans-unit id="dac62a0f0dbdc3c6a9231598871c80876ac21c3b" translate="yes" xml:space="preserve">
          <source>This context handler simplifies the exception handling. Use it as follows:</source>
          <target state="translated">这个上下文处理程序简化了异常处理。使用它的方法如下。</target>
        </trans-unit>
        <trans-unit id="92e2326270a118c9b1678bb4952443a859f4c559" translate="yes" xml:space="preserve">
          <source>This context manager can be used to override the gradient function that will be used for ops within the scope of the context.</source>
          <target state="translated">这个上下文管理器可以用来覆盖将用于上下文范围内的操作的梯度函数。</target>
        </trans-unit>
        <trans-unit id="790e879037e30f5e5546c1be4c06bf6476dca2dd" translate="yes" xml:space="preserve">
          <source>This context manager captures all writes to a given stream inside of a &lt;code&gt;CapturedWrites&lt;/code&gt; object. When this context manager is created, it yields the &lt;code&gt;CapturedWrites&lt;/code&gt; object. The captured contents can be accessed by calling &lt;code&gt;.contents()&lt;/code&gt; on the &lt;code&gt;CapturedWrites&lt;/code&gt;.</source>
          <target state="translated">该上下文管理器在 &lt;code&gt;CapturedWrites&lt;/code&gt; 对象内部捕获对给定流的所有写入。创建此上下文管理器后，将产生 &lt;code&gt;CapturedWrites&lt;/code&gt; 对象。可以通过在 &lt;code&gt;CapturedWrites&lt;/code&gt; 上调用 &lt;code&gt;.contents()&lt;/code&gt; 来访问捕获的内容。</target>
        </trans-unit>
        <trans-unit id="41c375fab79db6dca2a3052fe26a3bb1f0a48127" translate="yes" xml:space="preserve">
          <source>This context manager creates and automatically recovers a session. It optionally starts the standard services that handle checkpoints and summaries. It monitors exceptions raised from the &lt;code&gt;with&lt;/code&gt; block or from the services and stops the supervisor as needed.</source>
          <target state="translated">该上下文管理器创建并自动恢复会话。它可以选择启动处理检查点和摘要的标准服务。它监视从 &lt;code&gt;with&lt;/code&gt; 块或服务引发的异常，并根据需要停止主管。</target>
        </trans-unit>
        <trans-unit id="1dfc6367da2112861ba5a19ab74136e2bfa59bd7" translate="yes" xml:space="preserve">
          <source>This context manager pushes a name scope, which will make the name of all operations added within it have a prefix.</source>
          <target state="translated">这个上下文管理器会推送一个名称范围,会让所有添加在其中的操作名称都有一个前缀。</target>
        </trans-unit>
        <trans-unit id="cadf2e29564a69f6d9e78e18436e128fc8603a09" translate="yes" xml:space="preserve">
          <source>This context manager validates that the (optional) &lt;code&gt;values&lt;/code&gt; are from the same graph, ensures that graph is the default graph, and pushes a name scope and a variable scope.</source>
          <target state="translated">该上下文管理器验证（可选） &lt;code&gt;values&lt;/code&gt; 是否来自同一图，确保该图是默认图，并推送名称范围和变量范围。</target>
        </trans-unit>
        <trans-unit id="9fe8f8972b4d68b13082f0d96ff8de4b430acf3b" translate="yes" xml:space="preserve">
          <source>This context manager validates that the given &lt;code&gt;values&lt;/code&gt; are from the same graph, makes that graph the default graph, and pushes a name scope in that graph (see &lt;a href=&quot;../../../../graph#name_scope&quot;&gt;&lt;code&gt;tf.Graph.name_scope&lt;/code&gt;&lt;/a&gt; for more details on that).</source>
          <target state="translated">该上下文管理器验证给定的 &lt;code&gt;values&lt;/code&gt; 是否来自同一图，将该图&lt;a href=&quot;../../../../graph#name_scope&quot;&gt; &lt;code&gt;tf.Graph.name_scope&lt;/code&gt; &lt;/a&gt;默认图，并在该图中推送名称范围（有关此内容的更多详细信息，请参见tf.Graph.name_scope）。</target>
        </trans-unit>
        <trans-unit id="8bc9405ecd91e0aa7f29a32f0b001ccc17dfce1e" translate="yes" xml:space="preserve">
          <source>This convenience method requires a session where the graph containing this variable has been launched. If no session is passed, the default session is used. See &lt;a href=&quot;compat/v1/session&quot;&gt;&lt;code&gt;tf.compat.v1.Session&lt;/code&gt;&lt;/a&gt; for more information on launching a graph and on sessions.</source>
          <target state="translated">此便捷方法需要启动包含该变量的图形的会话。如果没有会话通过，则使用默认会话。有关启动图形和会话的更多信息，请参见&lt;a href=&quot;compat/v1/session&quot;&gt; &lt;code&gt;tf.compat.v1.Session&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="69597b8afaecf26bf780e17dbe96db99a176f9db" translate="yes" xml:space="preserve">
          <source>This convenience method requires a session where the graph containing this variable has been launched. If no session is passed, the default session is used. See &lt;a href=&quot;session&quot;&gt;&lt;code&gt;tf.compat.v1.Session&lt;/code&gt;&lt;/a&gt; for more information on launching a graph and on sessions.</source>
          <target state="translated">此便捷方法需要启动包含该变量的图形的会话。如果没有会话通过，则使用默认会话。有关启动图形和会话的更多信息，请参见&lt;a href=&quot;session&quot;&gt; &lt;code&gt;tf.compat.v1.Session&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="049bd1098346a19ff3fab486ff6a7b47c3fd7819" translate="yes" xml:space="preserve">
          <source>This creates a &lt;code&gt;LinearOperator&lt;/code&gt; of the form &lt;code&gt;A = L + U D V^H&lt;/code&gt;, with &lt;code&gt;L&lt;/code&gt; a &lt;code&gt;LinearOperator&lt;/code&gt;, &lt;code&gt;U, V&lt;/code&gt; both [batch] matrices, and &lt;code&gt;D&lt;/code&gt; a [batch] diagonal matrix.</source>
          <target state="translated">这就产生了一个 &lt;code&gt;LinearOperator&lt;/code&gt; 形式的 &lt;code&gt;A = L + U D V^H&lt;/code&gt; ，具有 &lt;code&gt;L&lt;/code&gt; 一个 &lt;code&gt;LinearOperator&lt;/code&gt; ， &lt;code&gt;U, V&lt;/code&gt; 两[批次]矩阵， &lt;code&gt;D&lt;/code&gt; A [批次]对角矩阵。</target>
        </trans-unit>
        <trans-unit id="20e733fb47526c81e38b7224224a075d45127bb0" translate="yes" xml:space="preserve">
          <source>This creates a named directory on disk that is isolated to this test, and will be properly cleaned up by the test. This avoids several pitfalls of creating temporary directories for test purposes, as well as makes it easier to setup directories and verify their contents.</source>
          <target state="translated">这将在磁盘上创建一个与本次测试隔离的命名目录,并将被测试适当清理。这样就避免了为测试目的创建临时目录的几个陷阱,同时也使设置目录和验证其内容变得更加容易。</target>
        </trans-unit>
        <trans-unit id="63a4f566bb13c25047e12750280d68497191b0d8" translate="yes" xml:space="preserve">
          <source>This creates a named directory on disk that is isolated to this test, and will be properly cleaned up by the test. This avoids several pitfalls of creating temporary directories for test purposes, as well as makes it easier to setup directories and verify their contents. For example:</source>
          <target state="translated">这将在磁盘上创建一个与本次测试隔离的命名目录,并将被测试适当清理。这样就避免了为测试目的而创建临时目录的几个陷阱,同时也使设置目录和验证其内容变得更加容易。比如说</target>
        </trans-unit>
        <trans-unit id="75714df8d9550841d3043fbd62dad500169275e6" translate="yes" xml:space="preserve">
          <source>This creates a named file on disk that is isolated to this test, and will be properly cleaned up by the test. This avoids several pitfalls of creating temporary files for test purposes, as well as makes it easier to setup files, their data, read them back, and inspect them when a test fails.</source>
          <target state="translated">这将在磁盘上创建一个与本次测试隔离的命名文件,并将被测试适当清理。这就避免了为测试目的创建临时文件的几个缺陷,同时也使设置文件、数据、回读文件以及在测试失败时检查文件变得更加容易。</target>
        </trans-unit>
        <trans-unit id="2f4f8ffa306c5f0499e24ea63e1e0e690360c7a3" translate="yes" xml:space="preserve">
          <source>This creates a named file on disk that is isolated to this test, and will be properly cleaned up by the test. This avoids several pitfalls of creating temporary files for test purposes, as well as makes it easier to setup files, their data, read them back, and inspect them when a test fails. For example:</source>
          <target state="translated">这将在磁盘上创建一个与本次测试隔离的命名文件,并将被测试适当清理。这样就避免了为测试目的创建临时文件的几个陷阱,同时也使设置文件、数据、读回文件以及在测试失败时检查文件更加容易。比如说</target>
        </trans-unit>
        <trans-unit id="42646afd71d42ede988399add1a74a5f6b4c93c2" translate="yes" xml:space="preserve">
          <source>This creates a tuple of tensors with the same values as the &lt;code&gt;tensors&lt;/code&gt; argument, except that the value of each tensor is only returned after the values of all tensors have been computed.</source>
          <target state="translated">这将创建一个具有与 &lt;code&gt;tensors&lt;/code&gt; 参数相同值的张量元组，只是每个张量的值仅在计算完所有张量的值之后才返回。</target>
        </trans-unit>
        <trans-unit id="89651a110f241aba282c0d3c4d5816a82b95a5ce" translate="yes" xml:space="preserve">
          <source>This dataset attempts to determine whether a valid snapshot exists at the &lt;code&gt;snapshot_path&lt;/code&gt;, and reads from the snapshot in lieu of using &lt;code&gt;input_dataset&lt;/code&gt;. If not, it will run the preprocessing pipeline as usual, and write out a snapshot of the data processed for future use.</source>
          <target state="translated">该数据集尝试确定在快照路径上是否存在有效的 &lt;code&gt;snapshot_path&lt;/code&gt; ，并代替使用 &lt;code&gt;input_dataset&lt;/code&gt; 从快照读取数据。如果没有，它将照常运行预处理管道，并写出已处理数据的快照以备将来使用。</target>
        </trans-unit>
        <trans-unit id="f9dc1c4d1a56233dc4a8e886da12f3f0a495623a" translate="yes" xml:space="preserve">
          <source>This dataset fills a buffer with &lt;code&gt;buffer_size&lt;/code&gt; elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.</source>
          <target state="translated">该数据集使用 &lt;code&gt;buffer_size&lt;/code&gt; 元素填充缓冲区，然后从该缓冲区中随机采样元素，将所选元素替换为新元素。为了实现完美的改组，需要缓冲区大小大于或等于数据集的完整大小。</target>
        </trans-unit>
        <trans-unit id="28c299276126ace248fef26b3267fca38b82ca7a" translate="yes" xml:space="preserve">
          <source>This dataset has been superseded by &lt;code&gt;ParallelInterleaveDatasetV2&lt;/code&gt;. New code should use &lt;code&gt;ParallelInterleaveDatasetV2&lt;/code&gt;.</source>
          <target state="translated">该数据集已被 &lt;code&gt;ParallelInterleaveDatasetV2&lt;/code&gt; 取代。新代码应使用 &lt;code&gt;ParallelInterleaveDatasetV2&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="97fff19a84ee410a08999fab8be4d199f9066fdc" translate="yes" xml:space="preserve">
          <source>This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset.</source>
          <target state="translated">在运行分布式训练时,这个数据集操作符非常有用,因为它允许每个worker读取一个独特的子集。</target>
        </trans-unit>
        <trans-unit id="0601b18d7846c9e12aa36064d4b2b93f04fe9903" translate="yes" xml:space="preserve">
          <source>This dataset will throw a NotFound error if we cannot shard the dataset automatically.</source>
          <target state="translated">如果我们不能自动shard数据集,这个数据集会抛出一个NotFound错误。</target>
        </trans-unit>
        <trans-unit id="38966f21ae4fd315566bc2e9e41902f2d11ad1fd" translate="yes" xml:space="preserve">
          <source>This decorator allows fine grained control over the gradients of a sequence for operations. This may be useful for multiple reasons, including providing a more efficient or numerically stable gradient for a sequence of operations.</source>
          <target state="translated">该装饰器允许对操作序列的梯度进行细粒度控制。这可能对多种原因有用,包括为一个操作序列提供更有效或数值上稳定的梯度。</target>
        </trans-unit>
        <trans-unit id="507c4c48f689d10862643a9c65b8ad1497b61251" translate="yes" xml:space="preserve">
          <source>This decorator injects the decorated class or function into the Keras custom object dictionary, so that it can be serialized and deserialized without needing an entry in the user-provided custom object dict. It also injects a function that Keras will call to get the object's serializable string key.</source>
          <target state="translated">这个装饰器将被装饰的类或函数注入到Keras自定义对象字典中,这样它就可以被序列化和反序列化,而不需要在用户提供的自定义对象dict中输入。它还注入了一个函数,Keras将调用它来获取对象的可序列化字符串键。</target>
        </trans-unit>
        <trans-unit id="99ba7a22516d6648f4353818a77d3239c06fe661" translate="yes" xml:space="preserve">
          <source>This decorator is only used when defining a new op type. For an op with &lt;code&gt;m&lt;/code&gt; inputs and &lt;code&gt;n&lt;/code&gt; outputs, the gradient function is a function that takes the original &lt;code&gt;Operation&lt;/code&gt; and &lt;code&gt;n&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt; objects (representing the gradients with respect to each output of the op), and returns &lt;code&gt;m&lt;/code&gt;&lt;code&gt;Tensor&lt;/code&gt; objects (representing the partial gradients with respect to each input of the op).</source>
          <target state="translated">仅在定义新的op类型时使用此装饰器。对于具有 &lt;code&gt;m&lt;/code&gt; 个输入和 &lt;code&gt;n&lt;/code&gt; 个输出的op ，梯度函数是一个函数，该函数采用原始的 &lt;code&gt;Operation&lt;/code&gt; 和 &lt;code&gt;n&lt;/code&gt; 个 &lt;code&gt;Tensor&lt;/code&gt; 对象（表示相对于op的每个输出的梯度），并返回 &lt;code&gt;m&lt;/code&gt; 个 &lt;code&gt;Tensor&lt;/code&gt; 对象（表示具有关于op的每个输入）。</target>
        </trans-unit>
        <trans-unit id="f69390eec43b9170084ec67f23f7df44213faadc" translate="yes" xml:space="preserve">
          <source>This defines the skeleton for all implementations of ClusterResolvers. ClusterResolvers are a way for TensorFlow to communicate with various cluster management systems (e.g. GCE, AWS, etc...) and gives TensorFlow necessary information to set up distributed training.</source>
          <target state="translated">这定义了ClusterResolvers的所有实现的骨架。ClusterResolvers是TensorFlow与各种集群管理系统(如GCE、AWS等......)进行通信的一种方式,并为TensorFlow提供必要的信息来设置分布式训练。</target>
        </trans-unit>
        <trans-unit id="31c4ff795b26f67c26110ef2a390028f451d7769" translate="yes" xml:space="preserve">
          <source>This defines the skeleton for all implementations of ClusterResolvers. ClusterResolvers are a way for TensorFlow to communicate with various cluster management systems (e.g. GCE, AWS, etc...).</source>
          <target state="translated">这定义了ClusterResolvers的所有实现的骨架。ClusterResolvers是TensorFlow与各种集群管理系统(如GCE、AWS等)通信的一种方式。</target>
        </trans-unit>
        <trans-unit id="0df31e40439c17db3321616e25b535cdb0316252" translate="yes" xml:space="preserve">
          <source>This definition of cell differs from the definition used in the literature. In the literature, 'cell' refers to an object with a single scalar output. This definition refers to a horizontal array of such units.</source>
          <target state="translated">单元格的这个定义与文献中的定义不同。在文献中,&quot;单元 &quot;指的是一个具有单一标量输出的对象。这个定义指的是这种单元的水平阵列。</target>
        </trans-unit>
        <trans-unit id="0725a72c209eb53df1440f5a39a25dbf2ab21a14" translate="yes" xml:space="preserve">
          <source>This distribution has parameters: degree of freedom &lt;code&gt;df&lt;/code&gt;, location &lt;code&gt;loc&lt;/code&gt;, and &lt;code&gt;scale&lt;/code&gt;.</source>
          <target state="translated">该分布具有参数：自由度 &lt;code&gt;df&lt;/code&gt; ，位置 &lt;code&gt;loc&lt;/code&gt; 和 &lt;code&gt;scale&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="11a222f9738197dadd6eb7c453e22ae24d0f3567" translate="yes" xml:space="preserve">
          <source>This does not close the session.</source>
          <target state="translated">这不会关闭会话。</target>
        </trans-unit>
        <trans-unit id="e3bddf2a09aebf2b7d2bb27cde671e8328a7dc27" translate="yes" xml:space="preserve">
          <source>This does not undo the effects of loss scaling. Any optimizers wrapped with a LossScaleOptimizer will continue to do loss scaling, although this loss scaling will no longer be useful if the optimizer is used in new Sessions, as the graph rewrite no longer converts the graph to use float16.</source>
          <target state="translated">这并不能撤销损失缩放的效果。任何用LossScaleOptimizer封装的优化器都会继续进行损失缩放,尽管如果优化器被用于新的Sessions,这个损失缩放将不再有用,因为图形重写不再将图形转换为使用float16。</target>
        </trans-unit>
        <trans-unit id="02c54112c729e547b6e3e03e55cab5ced9766b9c" translate="yes" xml:space="preserve">
          <source>This does not undo the effects of loss scaling. Any optimizers wrapped with a LossScaleOptimizer will continue to do loss scaling, although this loss scaling will no longer be useful, as the graph rewrite no longer converts tf.functions to use float16.</source>
          <target state="translated">这并不能撤销损失缩放的效果。任何用LossScaleOptimizer封装的优化器都将继续进行损失缩放,尽管这种损失缩放将不再有用,因为图形重写不再将tf.function转换为使用float16。</target>
        </trans-unit>
        <trans-unit id="3bfa5300d7e625917e03fb074107ed12abbdcbbe" translate="yes" xml:space="preserve">
          <source>This eliminates the overhead of &lt;code&gt;k-1&lt;/code&gt; calls to &lt;code&gt;space_to_batch_nd&lt;/code&gt; and &lt;code&gt;batch_to_space_nd&lt;/code&gt;.</source>
          <target state="translated">这消除了 &lt;code&gt;k-1&lt;/code&gt; 调用 &lt;code&gt;space_to_batch_nd&lt;/code&gt; 和 &lt;code&gt;batch_to_space_nd&lt;/code&gt; 的开销。</target>
        </trans-unit>
        <trans-unit id="4c1542adb4efbb08745cf8660782eb015be7e9a8" translate="yes" xml:space="preserve">
          <source>This enables the new behavior.</source>
          <target state="translated">这样就可以实现新的行为。</target>
        </trans-unit>
        <trans-unit id="e158c4e53eb8ef556e4d562ffa77e16f6fc696bb" translate="yes" xml:space="preserve">
          <source>This enables the user to close and release the resource in the middle of a step/run.</source>
          <target state="translated">这使用户能够在步骤/运行过程中关闭和释放资源。</target>
        </trans-unit>
        <trans-unit id="b70309885ebd2feded30214b2bff057bfcef522b" translate="yes" xml:space="preserve">
          <source>This enables variables to be read as bfloat16 type when using get_variable.</source>
          <target state="translated">当使用get_variable时,可以将变量读取为bfloat16类型。</target>
        </trans-unit>
        <trans-unit id="9b4ec5604b14a4a73674990062864327b53e189b" translate="yes" xml:space="preserve">
          <source>This enumeration represents optional conversion options.</source>
          <target state="translated">这个枚举代表了可选的转换选项。</target>
        </trans-unit>
        <trans-unit id="26d8315a3de6d7adfdf3bbd23a3ef83e0e870212" translate="yes" xml:space="preserve">
          <source>This estimator ignores feature values and will learn to predict the average value of each label. E.g. for single-label classification problems, this will predict the probability distribution of the classes as seen in the labels. For multi-label classification problems, it will predict the ratio of examples that contain each class.</source>
          <target state="translated">这个估计器忽略特征值,将学习预测每个标签的平均值。例如,对于单标签分类问题,它将预测标签中看到的类的概率分布。对于多标签分类问题,它将预测包含每个类的例子的比例。</target>
        </trans-unit>
        <trans-unit id="c0a3c93652b7acdf673bb2d9770fb27696ed0299" translate="yes" xml:space="preserve">
          <source>This example creates a lookup layer and generates the vocabulary by analyzing the dataset.</source>
          <target state="translated">这个例子创建了一个查找层,并通过分析数据集生成词汇。</target>
        </trans-unit>
        <trans-unit id="a9de20f542fbd733883e56dc873754dfb758c273" translate="yes" xml:space="preserve">
          <source>This example creates a lookup layer with a pre-existing vocabulary.</source>
          <target state="translated">这个例子创建了一个具有预先存在的词汇的查找层。</target>
        </trans-unit>
        <trans-unit id="c0b0f0177841bd64344b603ca5701c7a1c4bfeec" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to map indices to strings using this layer. (You can also use adapt() with inverse=True, but for simplicity we'll pass the vocab in this example.)</source>
          <target state="translated">这个例子演示了如何使用这一层将索引映射到字符串。(你也可以在inverse=True的情况下使用adapt(),但为了简单起见,我们将在本例中传递词汇。)</target>
        </trans-unit>
        <trans-unit id="3a755e66edb0249a30f62c3fbfa9da1107d08395" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to map indices to values using this layer. (You can also use adapt() with inverse=True, but for simplicity we'll pass the vocab in this example.)</source>
          <target state="translated">这个例子演示了如何使用这个层将索引映射到值。(你也可以在inverse=True的情况下使用adapt(),但为了简单起见,我们将在这个例子中传递词汇。)</target>
        </trans-unit>
        <trans-unit id="494bbe4febd153531076a3e5cddcbab4e936bc24" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to use a lookup layer with multiple OOV tokens. When a layer is created with more than one OOV token, any OOV values are hashed into the number of OOV buckets, distributing OOV values in a deterministic fashion across the set.</source>
          <target state="translated">这个例子演示了如何使用具有多个 OOV 标记的查找层。当使用多个OOV标记创建一个层时,任何OOV值都会被哈希到OOV桶的数量中,以确定性的方式在集合中分配OOV值。</target>
        </trans-unit>
        <trans-unit id="69a4365d2fc53f53ca98893f19670abd4d1d2032" translate="yes" xml:space="preserve">
          <source>This example demonstrates how to use the vocabulary of a standard lookup layer to create an inverse lookup layer.</source>
          <target state="translated">这个例子演示了如何使用标准查找层的词汇来创建一个反向查找层。</target>
        </trans-unit>
        <trans-unit id="a68786f5a17b7958b628ee0e8691b24f56ab38dc" translate="yes" xml:space="preserve">
          <source>This example gives binary output instead of counting the occurrence.</source>
          <target state="translated">这个例子给出了二进制输出,而不是对发生次数进行计数。</target>
        </trans-unit>
        <trans-unit id="3f03708fe58a6d112475001cb64ef56af4e257ac" translate="yes" xml:space="preserve">
          <source>This example instantiates a TextVectorization layer that lowercases text, splits on whitespace, strips punctuation, and outputs integer vocab indices.</source>
          <target state="translated">这个例子实例化了一个TextVectorization层,该层可以将文本小写,分割空白,剥离标点符号,并输出整数词汇索引。</target>
        </trans-unit>
        <trans-unit id="ee52f3666629477330cedb91ea1ff2368d33cc78" translate="yes" xml:space="preserve">
          <source>This example shows how to instantiate a layer that applies the same dense operation to every element in a sequence, but uses the ellipsis notation instead of specifying the batch and sequence dimensions.</source>
          <target state="translated">这个例子展示了如何实例化一个层,该层对序列中的每个元素应用相同的密集操作,但使用省略号符号而不是指定批次和序列尺寸。</target>
        </trans-unit>
        <trans-unit id="0d7af74c46a4fae38ce9ff64ebe601e504aada00" translate="yes" xml:space="preserve">
          <source>This example shows how to instantiate a layer that applies the same dense operation to every element in a sequence. Here, the 'output_shape' has two values (since there are two non-batch dimensions in the output); the first dimension in the output_shape is &lt;code&gt;None&lt;/code&gt;, because the sequence dimension &lt;code&gt;b&lt;/code&gt; has an unknown shape.</source>
          <target state="translated">本示例说明如何实例化对序列中的每个元素应用相同密集操作的层。这里，&amp;ldquo; output_shape&amp;rdquo;具有两个值（因为输出中有两个非批处理尺寸）；因此，&amp;ldquo; output_shape&amp;rdquo;具有两个值。output_shape中的第一个维度为 &lt;code&gt;None&lt;/code&gt; ，因为序列维度 &lt;code&gt;b&lt;/code&gt; 的形状未知。</target>
        </trans-unit>
        <trans-unit id="a7e9f20f12802c04c896151951f7fd5800246d6c" translate="yes" xml:space="preserve">
          <source>This example shows how to instantiate a standard Keras dense layer using einsum operations. This example is equivalent to &lt;a href=&quot;../dense&quot;&gt;&lt;code&gt;tf.keras.layers.Dense(64, use_bias=True)&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">本示例说明如何使用einsum操作实例化标准Keras密集层。此示例等效于&lt;a href=&quot;../dense&quot;&gt; &lt;code&gt;tf.keras.layers.Dense(64, use_bias=True)&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="bf32128894beae4e1d5986c66f68269f1ae0857f" translate="yes" xml:space="preserve">
          <source>This example takes a 2 dimensional input and returns a &lt;code&gt;Tensor&lt;/code&gt; with bincounting on each sample.</source>
          <target state="translated">本示例采用二维输入，并在每个样本上返回具有bincounting的 &lt;code&gt;Tensor&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="8f9c3d77008f03aede1949e1e8b86cf490084ce8" translate="yes" xml:space="preserve">
          <source>This example takes an input (which could be a Tensor, RaggedTensor, or SparseTensor) and returns a SparseTensor where (i,j) is 1 if the value j appears in batch i at least once and is 0 otherwise. Note that, even though some values (like 20 in batch 1 and 11 in batch 2) appear more than once, the 'values' tensor is all 1s.</source>
          <target state="translated">这个例子接受一个输入(可以是Tensor、RaggedTensor或SparseTensor),并返回一个SparseTensor,其中(i,j)是1,如果j值至少在第i批中出现一次,否则就是0。请注意,即使有些值(比如批号1中的20和批号2中的11)出现了不止一次,&quot;值 &quot;张量都是1。</target>
        </trans-unit>
        <trans-unit id="d3981b34919fca04a3be3649adee342abfe3505a" translate="yes" xml:space="preserve">
          <source>This example takes an input (which could be a Tensor, RaggedTensor, or SparseTensor) and returns a SparseTensor where the value of (i,j) is the number of times value j appears in batch i.</source>
          <target state="translated">这个例子接受一个输入(可以是Tensor、RaggedTensor或SparseTensor)并返回一个SparseTensor,其中(i,j)的值是值j在批次i中出现的次数。</target>
        </trans-unit>
        <trans-unit id="8dd04c77a84f7fb7b79c65a0782e87dc8901b29b" translate="yes" xml:space="preserve">
          <source>This example takes an input (which could be a Tensor, RaggedTensor, or SparseTensor) and returns a SparseTensor where the value of (i,j) is the number of times value j appears in batch i. However, all values of j above 'maxlength' are ignored. The dense_shape of the output sparse tensor is set to 'minlength'. Note that, while the input is identical to the example above, the value '10001' in batch item 2 is dropped, and the dense shape is [2, 500] instead of [2,10002] or [2, 102].</source>
          <target state="translated">这个例子接受一个输入(可以是Tensor、RaggedTensor或SparseTensor),并返回一个SparseTensor,其中(i,j)的值是j值在批处理i中出现的次数。然而,j的所有值超过'maxlength'都会被忽略。输出稀疏张量的dense_shape被设置为'minlength'。请注意,虽然输入与上面的例子相同,但批次项目2中的值'10001'被删除,密实形状是[2,500]而不是[2,10002]或[2,102]。</target>
        </trans-unit>
        <trans-unit id="9ad6c7816616b3241d748e89b2cfcff037903efa" translate="yes" xml:space="preserve">
          <source>This example takes two inputs - a values tensor and a weights tensor. These tensors must be identically shaped, and have the same row splits or indices in the case of RaggedTensors or SparseTensors. When performing a weighted count, the op will output a SparseTensor where the value of (i, j) is the sum of the values in the weight tensor's batch i in the locations where the values tensor has the value j. In this case, the output dtype is the same as the dtype of the weights tensor.</source>
          <target state="translated">这个例子需要两个输入--一个值张量和一个权重张量。这些张量必须形状相同,并且在RaggedTensors或SparseTensors的情况下具有相同的行分割或指数。当执行加权计数时,op将输出一个SparseTensor,其中(i,j)的值是权重张量的第i批值在值张量有j的位置的值的总和,在这种情况下,输出dtype与权重张量的dtype相同。</target>
        </trans-unit>
        <trans-unit id="3b8a0001b21c834c94ffc58110b29147780ed1bc" translate="yes" xml:space="preserve">
          <source>This exception is most commonly raised when running an operation that reads a &lt;a href=&quot;../variable&quot;&gt;&lt;code&gt;tf.Variable&lt;/code&gt;&lt;/a&gt; before it has been initialized.</source>
          <target state="translated">当运行一个在初始化之前读取&lt;a href=&quot;../variable&quot;&gt; &lt;code&gt;tf.Variable&lt;/code&gt; &lt;/a&gt;的操作时，通常会引发此异常。</target>
        </trans-unit>
        <trans-unit id="5c5a8c57c6d5ff0bef6c24b2e2bd3f673907515f" translate="yes" xml:space="preserve">
          <source>This exception is not currently used.</source>
          <target state="translated">目前还没有使用这种例外情况。</target>
        </trans-unit>
        <trans-unit id="f99d6b607eb42d98eb52dda612321114e3e91d9d" translate="yes" xml:space="preserve">
          <source>This exception is raised in &quot;end-of-file&quot; conditions, such as when a &lt;code&gt;tf.QueueBase.dequeue&lt;/code&gt; operation is blocked on an empty queue, and a &lt;code&gt;tf.QueueBase.close&lt;/code&gt; operation executes.</source>
          <target state="translated">在&amp;ldquo;文件结束&amp;rdquo;条件下（例如，在空队列上阻止 &lt;code&gt;tf.QueueBase.close&lt;/code&gt; 操作并执行 &lt;code&gt;tf.QueueBase.dequeue&lt;/code&gt; 操作时）会引发此异常。</target>
        </trans-unit>
        <trans-unit id="78d71c0636a2aa1fd087c8298d2d2e031276f9b2" translate="yes" xml:space="preserve">
          <source>This exception is raised when some invariant expected by the runtime has been broken. Catching this exception is not recommended.</source>
          <target state="translated">当运行时所期望的一些不变性被破坏时,会引发这个异常。不建议捕捉这个异常。</target>
        </trans-unit>
        <trans-unit id="7057b737e9dfe1ba0218e510ec51533edf1eab95" translate="yes" xml:space="preserve">
          <source>This exists primarily to support the definition of type-specific summary ops like scalar() and image(), and is not intended for direct use unless defining a new type-specific summary op.</source>
          <target state="translated">它的存在主要是为了支持特定类型的摘要操作的定义,如scalar()和image(),除非定义一个新的特定类型的摘要操作,否则不打算直接使用。</target>
        </trans-unit>
        <trans-unit id="42562f80c79b3f19581dfb21136c32f3ec5adea4" translate="yes" xml:space="preserve">
          <source>This facilitates a cleaner api around global state. Instead of</source>
          <target state="translated">这有利于在全局状态下实现更干净的api。取而代之的是</target>
        </trans-unit>
        <trans-unit id="b7cea24b85385b2c626baad7c000ac087d0ac17d" translate="yes" xml:space="preserve">
          <source>This file includes functions and constants from core (model_utils) and export.py</source>
          <target state="translated">这个文件包含了 core (model_utils)和 export.py 中的函数和常量。</target>
        </trans-unit>
        <trans-unit id="1ed5997b223ee14f22d4d6301133fe48a3becd70" translate="yes" xml:space="preserve">
          <source>This flag will have a value of None, True or False. None is possible if default=None and the user does not specify the flag on the command line.</source>
          <target state="translated">该标志的值为None、True或False。如果default=None,并且用户没有在命令行中指定该标志,则可以使用None。</target>
        </trans-unit>
        <trans-unit id="edc3e42fb098d2f82cf52f313f0a2bbc39d59acf" translate="yes" xml:space="preserve">
          <source>This foldl operator repeatedly applies the callable &lt;code&gt;fn&lt;/code&gt; to a sequence of elements from first to last. The elements are made of the tensors unpacked from &lt;code&gt;elems&lt;/code&gt; on dimension 0. The callable fn takes two tensors as arguments. The first argument is the accumulated value computed from the preceding invocation of fn, and the second is the value at the current position of &lt;code&gt;elems&lt;/code&gt;. If &lt;code&gt;initializer&lt;/code&gt; is None, &lt;code&gt;elems&lt;/code&gt; must contain at least one element, and its first element is used as the initializer.</source>
          <target state="translated">该foldl运算符从头到尾将可调用 &lt;code&gt;fn&lt;/code&gt; 重复应用于一系列元素。该元件是由从解压缩的张量的 &lt;code&gt;elems&lt;/code&gt; 的上尺寸0.可调用FN采用两个张量作为参数。第一个参数是从前一次调用fn计算得出的累加值，第二个参数是 &lt;code&gt;elems&lt;/code&gt; 当前位置的值。如果 &lt;code&gt;initializer&lt;/code&gt; 是None， &lt;code&gt;elems&lt;/code&gt; 的必须包含至少一个元件，并且它的第一元件被用作初始化。</target>
        </trans-unit>
        <trans-unit id="5c9efc4e60690682d054976cd526187b0ec74f90" translate="yes" xml:space="preserve">
          <source>This foldr operator repeatedly applies the callable &lt;code&gt;fn&lt;/code&gt; to a sequence of elements from last to first. The elements are made of the tensors unpacked from &lt;code&gt;elems&lt;/code&gt;. The callable fn takes two tensors as arguments. The first argument is the accumulated value computed from the preceding invocation of fn, and the second is the value at the current position of &lt;code&gt;elems&lt;/code&gt;. If &lt;code&gt;initializer&lt;/code&gt; is None, &lt;code&gt;elems&lt;/code&gt; must contain at least one element, and its first element is used as the initializer.</source>
          <target state="translated">该文件夹运算符将可调用的 &lt;code&gt;fn&lt;/code&gt; 重复应用于从最后到第一个元素的序列。这些元素由从元 &lt;code&gt;elems&lt;/code&gt; 解压缩的张量组成。可调用的fn以两个张量作为参数。第一个参数是从前一次调用fn计算得出的累加值，第二个参数是 &lt;code&gt;elems&lt;/code&gt; 当前位置的值。如果 &lt;code&gt;initializer&lt;/code&gt; 是None， &lt;code&gt;elems&lt;/code&gt; 的必须包含至少一个元件，并且它的第一元件被用作初始化。</target>
        </trans-unit>
        <trans-unit id="e47bc5be6551c40b285b99814572b5e2b249a8b5" translate="yes" xml:space="preserve">
          <source>This function adds operations to the current session. To compute the error using a particular device, such as a GPU, use the standard methods for setting a device (e.g. using with sess.graph.device() or setting a device function in the session constructor).</source>
          <target state="translated">该函数将操作添加到当前会话中。要使用特定的设备(如GPU)来计算错误,请使用设置设备的标准方法(如使用sess.graph.device()或在session构造函数中设置设备函数)。</target>
        </trans-unit>
        <trans-unit id="b1c988623a891fbccad5b0fbb3db51dcb83f9a90" translate="yes" xml:space="preserve">
          <source>This function adds the following to the current &lt;code&gt;Graph&lt;/code&gt;:</source>
          <target state="translated">此函数将以下内容添加到当前 &lt;code&gt;Graph&lt;/code&gt; 中：</target>
        </trans-unit>
        <trans-unit id="7391056d9f36003196c437fbb04f6186eed40dde" translate="yes" xml:space="preserve">
          <source>This function allows expressing computations in a TensorFlow graph as Python functions. In particular, it wraps a Python function &lt;code&gt;func&lt;/code&gt; in a once-differentiable TensorFlow operation that executes it with eager execution enabled. As a consequence, &lt;a href=&quot;py_function&quot;&gt;&lt;code&gt;tf.py_function&lt;/code&gt;&lt;/a&gt; makes it possible to express control flow using Python constructs (&lt;code&gt;if&lt;/code&gt;, &lt;code&gt;while&lt;/code&gt;, &lt;code&gt;for&lt;/code&gt;, etc.), instead of TensorFlow control flow constructs (&lt;a href=&quot;cond&quot;&gt;&lt;code&gt;tf.cond&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;while_loop&quot;&gt;&lt;code&gt;tf.while_loop&lt;/code&gt;&lt;/a&gt;). For example, you might use &lt;a href=&quot;py_function&quot;&gt;&lt;code&gt;tf.py_function&lt;/code&gt;&lt;/a&gt; to implement the log huber function:</source>
          <target state="translated">此功能允许将TensorFlow图中的计算表示为Python函数。特别是，它将Python函数 &lt;code&gt;func&lt;/code&gt; 包装在一次可区分的TensorFlow操作中，该操作在启用了急切执行的情况下执行该函数。因此，&lt;a href=&quot;py_function&quot;&gt; &lt;code&gt;tf.py_function&lt;/code&gt; &lt;/a&gt;使得可以使用Python构造（ &lt;code&gt;if&lt;/code&gt; ， &lt;code&gt;while&lt;/code&gt; ， &lt;code&gt;for&lt;/code&gt; 等）来表达控制流，而不是TensorFlow控制流构造（&lt;a href=&quot;cond&quot;&gt; &lt;code&gt;tf.cond&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;while_loop&quot;&gt; &lt;code&gt;tf.while_loop&lt;/code&gt; &lt;/a&gt;）。例如，您可以使用&lt;a href=&quot;py_function&quot;&gt; &lt;code&gt;tf.py_function&lt;/code&gt; &lt;/a&gt;来实现日志集线器功能：</target>
        </trans-unit>
        <trans-unit id="fa49b9c5fb3cc27a08b15ff4c8fbc286bffc2bb3" translate="yes" xml:space="preserve">
          <source>This function allows replacing a function wrapped by &lt;code&gt;decorator_func&lt;/code&gt;, assuming the decorator that wraps the function is written as described below.</source>
          <target state="translated">该函数允许替换由 &lt;code&gt;decorator_func&lt;/code&gt; 包装的函数，假定包装该函数的装饰器如下所述。</target>
        </trans-unit>
        <trans-unit id="f1af1fe4443bcb9edf1df9ce5ca5cc1b073069a3" translate="yes" xml:space="preserve">
          <source>This function also returns a &lt;code&gt;should_apply_gradients&lt;/code&gt; bool. If False, gradients should not be applied to the variables that step, as nonfinite gradients were found, and the loss scale has been be updated to reduce the chance of finding nonfinite gradients in the next step. Some loss scale classes will always return True, as they cannot adjust themselves in response to nonfinite gradients.</source>
          <target state="translated">此函数还返回 &lt;code&gt;should_apply_gradients&lt;/code&gt; bool。如果为False，则不应将梯度应用于该步骤的变量，因为找到了非有限梯度，并且损耗标度已更新，以减少在下一步中发现非有限梯度的机会。某些损耗标度类将始终返回True，因为它们无法响应非有限梯度来进行自我调整。</target>
        </trans-unit>
        <trans-unit id="a970dd1ee86d1ed9e7a0f4a5fcb3e14f9903fdf2" translate="yes" xml:space="preserve">
          <source>This function assumes that &lt;code&gt;img1&lt;/code&gt; and &lt;code&gt;img2&lt;/code&gt; are image batches, i.e. the last three dimensions are [height, width, channels].</source>
          <target state="translated">该函数假定 &lt;code&gt;img1&lt;/code&gt; 和 &lt;code&gt;img2&lt;/code&gt; 是图像批处理，即最后三个维度是[height，width，channels]。</target>
        </trans-unit>
        <trans-unit id="05e4418b517bd473a485966680131f4c4c444d83" translate="yes" xml:space="preserve">
          <source>This function attempts to partially evaluate the given tensor, and returns its value as a numpy ndarray if this succeeds.</source>
          <target state="translated">这个函数试图对给定的张量进行部分评估,如果成功,则以numpy ndarray的形式返回其值。</target>
        </trans-unit>
        <trans-unit id="cf86c424c057435bb07a468ed262a1bc87e5ff48" translate="yes" xml:space="preserve">
          <source>This function can be called at the beginning of the program (before &lt;code&gt;Tensors&lt;/code&gt;, &lt;code&gt;Graphs&lt;/code&gt; or other structures have been created, and before devices have been initialized. It switches all global behaviors that are different between TensorFlow 1.x and 2.x to behave as intended for 1.x.</source>
          <target state="translated">可以在程序开始时调用此函数（在创建 &lt;code&gt;Tensors&lt;/code&gt; ， &lt;code&gt;Graphs&lt;/code&gt; 或其他结构之前，以及在初始化设备之前）。它将在TensorFlow 1.x和2.x之间不同的所有全局行为切换为适用于1.x。</target>
        </trans-unit>
        <trans-unit id="10057d11c28687d408d56022580e8ea6fe2a793b" translate="yes" xml:space="preserve">
          <source>This function can be called at the beginning of the program (before &lt;code&gt;Tensors&lt;/code&gt;, &lt;code&gt;Graphs&lt;/code&gt; or other structures have been created, and before devices have been initialized. It switches all global behaviors that are different between TensorFlow 1.x and 2.x to behave as intended for 2.x.</source>
          <target state="translated">可以在程序开始时调用此函数（在创建 &lt;code&gt;Tensors&lt;/code&gt; ， &lt;code&gt;Graphs&lt;/code&gt; 或其他结构之前，以及在初始化设备之前）。它将在TensorFlow 1.x和2.x之间不同的所有全局行为切换为适用于2.x。</target>
        </trans-unit>
        <trans-unit id="b42e569daa67e0a1534f20b78d521ba43e71fa0e" translate="yes" xml:space="preserve">
          <source>This function can be used to calculate a suitable paddings argument for use with space_to_batch_nd and batch_to_space_nd.</source>
          <target state="translated">这个函数可以用来计算合适的paddings参数,用于space_to_batch_nd和batch_to_space_nd。</target>
        </trans-unit>
        <trans-unit id="c744362e6d55098ce01656aff7353b51e78a3f8e" translate="yes" xml:space="preserve">
          <source>This function can be useful when composing a new operation in Python (such as &lt;code&gt;my_func&lt;/code&gt; in the example above). All standard Python op constructors apply this function to each of their Tensor-valued inputs, which allows those ops to accept numpy arrays, Python lists, and scalars in addition to &lt;code&gt;Tensor&lt;/code&gt; objects.</source>
          <target state="translated">在Python中编写新操作时（例如上例中的 &lt;code&gt;my_func&lt;/code&gt; )，此函数很有用。所有标准的Python op构造函数都将此函数应用于其每个Tensor值输入，这使这些op除了 &lt;code&gt;Tensor&lt;/code&gt; 对象还接受numpy数组，Python列表和标量。</target>
        </trans-unit>
        <trans-unit id="217d0b2441a5002feaf4d45bd68846f61852099e" translate="yes" xml:space="preserve">
          <source>This function can compute several different vector norms (the 1-norm, the Euclidean or 2-norm, the inf-norm, and in general the p-norm for p &amp;gt; 0) and matrix norms (Frobenius, 1-norm, 2-norm and inf-norm).</source>
          <target state="translated">此函数可以计算几个不同的向量范数（1-范数，欧几里德或2-范数，INF范数，以及通常p&amp;gt; 0的p范数）和矩阵范式（Frobenius，1-范数，2-规范和inf规范）。</target>
        </trans-unit>
        <trans-unit id="3ea434ce2125739c9300b55f884e2363f68836bb" translate="yes" xml:space="preserve">
          <source>This function can only be called before any Graphs, Ops, or Tensors have been created. It can be used at the beginning of the program for complex migration projects from TensorFlow 1.x to 2.x.</source>
          <target state="translated">这个函数只能在创建任何Graphs、Ops或Tensors之前被调用,它可以在程序开始时用于从TensorFlow 1.x到2.x的复杂迁移项目。它可以在程序开始时用于从TensorFlow 1.x到2.x的复杂迁移项目。</target>
        </trans-unit>
        <trans-unit id="588718f27309c0577e10569b211613b30b1126d6" translate="yes" xml:space="preserve">
          <source>This function casts the input to &lt;code&gt;dtype&lt;/code&gt; without applying any scaling. If there is a danger that values would over or underflow in the cast, this op applies the appropriate clamping before the cast.</source>
          <target state="translated">此函数将输入强制转换为 &lt;code&gt;dtype&lt;/code&gt; 而不应用任何缩放。如果存在铸件上溢位或下溢的危险，则该操作在铸件前进行适当的夹紧。</target>
        </trans-unit>
        <trans-unit id="09decee67bf539dd150a70f2c58935c601acb068" translate="yes" xml:space="preserve">
          <source>This function computes the exponential of every element in the input tensor. i.e. &lt;code&gt;exp(x)&lt;/code&gt; or &lt;code&gt;e^(x)&lt;/code&gt;, where &lt;code&gt;x&lt;/code&gt; is the input tensor. &lt;code&gt;e&lt;/code&gt; denotes Euler's number and is approximately equal to 2.718281. Output is positive for any real input.</source>
          <target state="translated">此函数计算输入张量中每个元素的指数。即 &lt;code&gt;exp(x)&lt;/code&gt; 或 &lt;code&gt;e^(x)&lt;/code&gt; ，其中 &lt;code&gt;x&lt;/code&gt; 是输入张量。 &lt;code&gt;e&lt;/code&gt; 表示欧拉数，大约等于2.718281。任何实际输入的输出均为正。</target>
        </trans-unit>
        <trans-unit id="5fcc9bd16353155f925829234fcc96a9dbe0c86d" translate="yes" xml:space="preserve">
          <source>This function computes the exponential of the input tensor element-wise. i.e. &lt;a href=&quot;exp&quot;&gt;&lt;code&gt;math.exp(x)&lt;/code&gt;&lt;/a&gt; or \(e^x\), where &lt;code&gt;x&lt;/code&gt; is the input tensor. \(e\) denotes Euler's number and is approximately equal to 2.718281. Output is positive for any real input.</source>
          <target state="translated">此函数按元素计算输入张量的指数。即&lt;a href=&quot;exp&quot;&gt; &lt;code&gt;math.exp(x)&lt;/code&gt; &lt;/a&gt;或\（e ^ x \），其中 &lt;code&gt;x&lt;/code&gt; 是输入张量。\（e \）表示欧拉数，大约等于2.718281。任何实际输入的输出均为正。</target>
        </trans-unit>
        <trans-unit id="7c0a191a50e5e9af59c01d8569f306fc43d930b1" translate="yes" xml:space="preserve">
          <source>This function computes the matrix logarithm using the Schur-Parlett algorithm. Details of the algorithm can be found in Section 11.6.2 of: Nicholas J. Higham, Functions of Matrices: Theory and Computation, SIAM 2008. ISBN 978-0-898716-46-7.</source>
          <target state="translated">这个函数使用Schur-Parlett算法计算矩阵对数。算法的详细内容可以在《Nicholas J.Higham,矩阵函数:矩阵对数》第11.6.2节中找到。Nicholas J.Higham,矩阵函数:理论与计算》,SIAM 2008。ISBN 978-0-898716-46-7。</target>
        </trans-unit>
        <trans-unit id="be1f544dd8e61bda50e9ba11ee30c7fa32e6f8ea" translate="yes" xml:space="preserve">
          <source>This function converts Python objects of various types to &lt;code&gt;Tensor&lt;/code&gt; objects. It accepts &lt;code&gt;Tensor&lt;/code&gt; objects, numpy arrays, Python lists, and Python scalars. For example:</source>
          <target state="translated">该函数将各种类型的Python对象转换为 &lt;code&gt;Tensor&lt;/code&gt; 对象。它接受 &lt;code&gt;Tensor&lt;/code&gt; 对象，numpy数组，Python列表和Python标量。例如：</target>
        </trans-unit>
        <trans-unit id="e487da37eb7d13adb8d956491efcafe1ed6b6abd" translate="yes" xml:space="preserve">
          <source>This function creates a new Generator object (and the Variable object within), which does not work well with tf.function because (1) tf.function puts restrictions on Variable creation thus reset_global_generator can't be freely used inside tf.function; (2) redirecting a global variable to a new object is problematic with tf.function because the old object may be captured by a 'tf.function'ed function and still be used by it. A 'tf.function'ed function only keeps weak references to variables, so deleting a variable and then calling that function again may raise an error, as demonstrated by random_test.py/RandomTest.testResetGlobalGeneratorBadWithDefun .</source>
          <target state="translated">这个函数创建了一个新的Generator对象(以及其中的Variable对象),这在tf.function中不能很好的使用,因为(1)tf.function对Variable的创建有限制,因此reset_global_generator不能在tf.function中自由使用;(2)将一个全局变量重定向到一个新的对象在tf.function中是有问题的,因为旧的对象可能会被一个'tf.function'ed函数捕获,并且仍然被它使用。一个'tf.function'ed函数只保留对变量的弱引用,因此删除一个变量然后再次调用该函数可能会引发一个错误,如random_test.py/RandomTest.testResetGlobalGeneratorBadWithDefun所展示的那样。</target>
        </trans-unit>
        <trans-unit id="7bd8be377fdf00edb4e98ffd4a0d17d3f66c21a2" translate="yes" xml:space="preserve">
          <source>This function divides &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, forcing Python 2 semantics. That is, if &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are both integers then the result will be an integer. This is in contrast to Python 3, where division with &lt;code&gt;/&lt;/code&gt; is always a float while division with &lt;code&gt;//&lt;/code&gt; is always an integer.</source>
          <target state="translated">此函数将 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 分开，强制使用Python 2语义。也就是说，如果 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 都是整数，那么结果将是整数。这与Python 3相反，在Python 3中，用 &lt;code&gt;/&lt;/code&gt; 除总是一个浮点数，而用 &lt;code&gt;//&lt;/code&gt; 除总是一个整数。</target>
        </trans-unit>
        <trans-unit id="d24aa00a66ce40fc5a2092349ae7643f400f331a" translate="yes" xml:space="preserve">
          <source>This function enables you to use a &lt;a href=&quot;../dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; in a stateless &quot;tensor-in tensor-out&quot; expression, without creating a &lt;a href=&quot;../../compat/v1/data/iterator&quot;&gt;&lt;code&gt;tf.compat.v1.data.Iterator&lt;/code&gt;&lt;/a&gt;. This can be useful when your preprocessing transformations are expressed as a &lt;code&gt;Dataset&lt;/code&gt;, and you want to use the transformation at serving time. For example:</source>
          <target state="translated">此功能可让您使用&lt;a href=&quot;../dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt;在无状态&amp;ldquo;张量，张量出&amp;rdquo;表达，而无需创建一个&lt;a href=&quot;../../compat/v1/data/iterator&quot;&gt; &lt;code&gt;tf.compat.v1.data.Iterator&lt;/code&gt; &lt;/a&gt;。当您将预处理转换表示为 &lt;code&gt;Dataset&lt;/code&gt; 且要在服务时使用该转换时，此功能很有用。例如：</target>
        </trans-unit>
        <trans-unit id="dcd4ee76305c19cb19df5a4e6d70410c1e1185ec" translate="yes" xml:space="preserve">
          <source>This function enables you to use a &lt;a href=&quot;../dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; in a stateless &quot;tensor-in tensor-out&quot; expression, without creating an iterator. This can be useful when your preprocessing transformations are expressed as a &lt;code&gt;Dataset&lt;/code&gt;, and you want to use the transformation at serving time.</source>
          <target state="translated">此函数使您可以在无状态&amp;ldquo; tensor-in tensor-out&amp;rdquo;表达式中使用&lt;a href=&quot;../dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt;，而无需创建迭代器。当您将预处理转换表示为 &lt;code&gt;Dataset&lt;/code&gt; 且要在服务时使用该转换时，此功能很有用。</target>
        </trans-unit>
        <trans-unit id="43245fcf24b3e5f89c388ed18c02c0b4b9e1fd8d" translate="yes" xml:space="preserve">
          <source>This function enqueues a structure of features to be looked up in the embedding tables. We expect that the batch size of each of the tensors in features matches the per core batch size. This will automatically happen if your input dataset is batched to the global batch size and you use &lt;a href=&quot;../../../distribute/tpustrategy&quot;&gt;&lt;code&gt;tf.distribute.TPUStrategy&lt;/code&gt;&lt;/a&gt;'s &lt;code&gt;experimental_distribute_dataset&lt;/code&gt; or if you use &lt;code&gt;experimental_distribute_datasets_from_function&lt;/code&gt; and batch to the per core batch size computed by the context passed to your input function.</source>
          <target state="translated">该函数使要在嵌入表中查找的功能的结构排队。我们期望要素中每个张量的批大小与每个核心批大小匹配。如果输入数据集被批处理为全局批处理大小，并且您使用&lt;a href=&quot;../../../distribute/tpustrategy&quot;&gt; &lt;code&gt;tf.distribute.TPUStrategy&lt;/code&gt; &lt;/a&gt;的 &lt;code&gt;experimental_distribute_dataset&lt;/code&gt; ,或者如果您使用 &lt;code&gt;experimental_distribute_datasets_from_function&lt;/code&gt; 进行批处理并按传递给输入函数的上下文计算的每个核心批处理大小，则将自动发生这种情况。</target>
        </trans-unit>
        <trans-unit id="af17991dd15e2b7fa7774cc8e8187690043ab3dd" translate="yes" xml:space="preserve">
          <source>This function exists only for backwards compatibility purposes; new code should use &lt;code&gt;__floordiv__&lt;/code&gt; via the syntax &lt;code&gt;x // y&lt;/code&gt;. Using &lt;code&gt;x // y&lt;/code&gt; communicates clearly that the result rounds down, and is forward compatible to Python 3.</source>
          <target state="translated">此功能仅出于向后兼容的目的而存在。新代码应通过语法 &lt;code&gt;x // y&lt;/code&gt; 使用 &lt;code&gt;__floordiv__&lt;/code&gt; 。使用 &lt;code&gt;x // y&lt;/code&gt; 可以清楚地表明结果是向下取整的，并且与Python 3正向兼容。</target>
        </trans-unit>
        <trans-unit id="1a2f6439e571c218e4f990c2f6f9f31b011962aa" translate="yes" xml:space="preserve">
          <source>This function exists only to have a better error message. Instead of: &lt;code&gt;TypeError: unsupported operand type(s) for /: 'Dimension' and 'int'&lt;/code&gt;, this function will explicitly call for usage of &lt;code&gt;//&lt;/code&gt; instead.</source>
          <target state="translated">存在此功能只是为了获得更好的错误消息。代替： &lt;code&gt;TypeError: unsupported operand type(s) for /: 'Dimension' and 'int'&lt;/code&gt; ，该函数将显式调用 &lt;code&gt;//&lt;/code&gt; 的用法。</target>
        </trans-unit>
        <trans-unit id="6bf09410a9b47558afad7eb9fc6a9929885c6dd9" translate="yes" xml:space="preserve">
          <source>This function exists only to have a better error message. Instead of: &lt;code&gt;TypeError: unsupported operand type(s) for /: 'int' and 'Dimension'&lt;/code&gt;, this function will explicitly call for usage of &lt;code&gt;//&lt;/code&gt; instead.</source>
          <target state="translated">存在此功能只是为了获得更好的错误消息。代替： &lt;code&gt;TypeError: unsupported operand type(s) for /: 'int' and 'Dimension'&lt;/code&gt; ，此函数将显式调用 &lt;code&gt;//&lt;/code&gt; 的用法。</target>
        </trans-unit>
        <trans-unit id="7046a2ab2d223422699f2f1db88f8efbcff064ed" translate="yes" xml:space="preserve">
          <source>This function exports the graph, saver, and collection objects into &lt;code&gt;MetaGraphDef&lt;/code&gt; protocol buffer with the intention of it being imported at a later time or location to restart training, run inference, or be a subgraph.</source>
          <target state="translated">此函数将图形，保护程序和集合对象 &lt;code&gt;MetaGraphDef&lt;/code&gt; 到MetaGraphDef协议缓冲区中，目的是在以后的时间或位置将其导入以重新开始训练，运行推理或成为子图。</target>
        </trans-unit>
        <trans-unit id="7a855742752eb51788b391963ce908a091cbe366" translate="yes" xml:space="preserve">
          <source>This function follows the &lt;a href=&quot;http://htk.eng.cam.ac.uk/&quot;&gt;Hidden Markov Model Toolkit (HTK)&lt;/a&gt; convention, defining the mel scale in terms of a frequency in hertz according to the following formula:</source>
          <target state="translated">此函数遵循&amp;ldquo;&lt;a href=&quot;http://htk.eng.cam.ac.uk/&quot;&gt;隐马尔可夫模型工具包（HTK）&amp;rdquo;&lt;/a&gt;约定，根据以下公式，以赫兹频率定义梅尔音阶：</target>
        </trans-unit>
        <trans-unit id="6a27fb60b44269b202e3c908e7cb802642c2ac28" translate="yes" xml:space="preserve">
          <source>This function forces Python 3 division operator semantics where all integer arguments are cast to floating types first. This op is generated by normal &lt;code&gt;x / y&lt;/code&gt; division in Python 3 and in Python 2.7 with &lt;code&gt;from __future__ import division&lt;/code&gt;. If you want integer division that rounds down, use &lt;code&gt;x // y&lt;/code&gt; or &lt;code&gt;tf.math.floordiv&lt;/code&gt;.</source>
          <target state="translated">此函数强制使用Python 3除法运算符语义，其中所有整数参数都首先转换为浮点类型。该操作由Python 3和Python 2.7中的常规 &lt;code&gt;x / y&lt;/code&gt; 除法使用 &lt;code&gt;from __future__ import division&lt;/code&gt; 。如果要 &lt;code&gt;tf.math.floordiv&lt;/code&gt; 到整数除法，请使用 &lt;code&gt;x // y&lt;/code&gt; 或tf.math.floordiv。</target>
        </trans-unit>
        <trans-unit id="4d4720bce15c58196c687612f77168d6737d499a" translate="yes" xml:space="preserve">
          <source>This function generalizes the &lt;a href=&quot;non_max_suppression&quot;&gt;&lt;code&gt;tf.image.non_max_suppression&lt;/code&gt;&lt;/a&gt; op by also supporting a Soft-NMS (with Gaussian weighting) mode (c.f. Bodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score of other overlapping boxes instead of directly causing them to be pruned. Consequently, in contrast to &lt;a href=&quot;non_max_suppression&quot;&gt;&lt;code&gt;tf.image.non_max_suppression&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;non_max_suppression_padded&quot;&gt;&lt;code&gt;tf.image.non_max_suppression_padded&lt;/code&gt;&lt;/a&gt; returns the new scores of each input box in the second output, &lt;code&gt;selected_scores&lt;/code&gt;.</source>
          <target state="translated">此功能还通过支持Soft-NMS（具有高斯加权）模式（参见Bodla等人，https：//arxiv.org/abs/1704.04503）来概括&lt;a href=&quot;non_max_suppression&quot;&gt; &lt;code&gt;tf.image.non_max_suppression&lt;/code&gt; &lt;/a&gt; op，其中框会降低其他重叠框的得分而不是直接导致它们被修剪。因此，与&lt;a href=&quot;non_max_suppression&quot;&gt; &lt;code&gt;tf.image.non_max_suppression&lt;/code&gt; 相比&lt;/a&gt;，&lt;a href=&quot;non_max_suppression_padded&quot;&gt; &lt;code&gt;tf.image.non_max_suppression_padded&lt;/code&gt; &lt;/a&gt;返回第二个输出 &lt;code&gt;selected_scores&lt;/code&gt; 中每个输入框的新分数。</target>
        </trans-unit>
        <trans-unit id="b139bf71180b25aa620a27f39c22f066fe31fcf7" translate="yes" xml:space="preserve">
          <source>This function generalizes the &lt;a href=&quot;non_max_suppression&quot;&gt;&lt;code&gt;tf.image.non_max_suppression&lt;/code&gt;&lt;/a&gt; op by also supporting a Soft-NMS (with Gaussian weighting) mode (c.f. Bodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score of other overlapping boxes instead of directly causing them to be pruned. Consequently, in contrast to &lt;a href=&quot;non_max_suppression&quot;&gt;&lt;code&gt;tf.image.non_max_suppression&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;tf.image.non_max_suppression_v2&lt;/code&gt; returns the new scores of each input box in the second output, &lt;code&gt;selected_scores&lt;/code&gt;.</source>
          <target state="translated">此功能还通过支持Soft-NMS（具有高斯加权）模式（参见Bodla等人，https：//arxiv.org/abs/1704.04503）来概括&lt;a href=&quot;non_max_suppression&quot;&gt; &lt;code&gt;tf.image.non_max_suppression&lt;/code&gt; &lt;/a&gt; op，其中框会降低其他重叠框的得分而不是直接导致它们被修剪。因此，与&lt;a href=&quot;non_max_suppression&quot;&gt; &lt;code&gt;tf.image.non_max_suppression&lt;/code&gt; 相比&lt;/a&gt;， &lt;code&gt;tf.image.non_max_suppression_v2&lt;/code&gt; 返回第二个输出 &lt;code&gt;selected_scores&lt;/code&gt; 中每个输入框的新分数。</target>
        </trans-unit>
        <trans-unit id="1f78cfdbddd70d6ebd322125da902964ca2c0221" translate="yes" xml:space="preserve">
          <source>This function generates a weighted sum based on output dimension &lt;code&gt;units&lt;/code&gt;. Weighted sum refers to logits in classification problems. It refers to the prediction itself for linear regression problems.</source>
          <target state="translated">此函数根据输出尺寸 &lt;code&gt;units&lt;/code&gt; 生成加权和。加权和是指分类问题中的对数。它指的是线性回归问题的预测本身。</target>
        </trans-unit>
        <trans-unit id="b7164aa4227947230adf26e333ecf50d207f9d4e" translate="yes" xml:space="preserve">
          <source>This function ignores flags whose value is None. Each flag assignment is separated by a newline.</source>
          <target state="translated">此函数忽略值为None的标志。每个标志的分配都用换行符隔开。</target>
        </trans-unit>
        <trans-unit id="0c81aeff7442b1d7f71d30de1b5da89dfe187de7" translate="yes" xml:space="preserve">
          <source>This function in addition also allows assignment to a sliced range. This is similar to &lt;code&gt;__setitem__&lt;/code&gt; functionality in Python. However, the syntax is different so that the user can capture the assignment operation for grouping or passing to &lt;code&gt;sess.run()&lt;/code&gt;. For example,</source>
          <target state="translated">此外，该功能还允许分配给切片范围。这类似于Python中的 &lt;code&gt;__setitem__&lt;/code&gt; 功能。但是，语法不同，因此用户可以捕获分配操作以进行分组或传递给 &lt;code&gt;sess.run()&lt;/code&gt; 。例如，</target>
        </trans-unit>
        <trans-unit id="f477dc6f411c004d0b07e1741886385a8bad1741" translate="yes" xml:space="preserve">
          <source>This function is a more primitive version of &lt;code&gt;dynamic_rnn&lt;/code&gt; that provides more direct access to the inputs each iteration. It also provides more control over when to start and finish reading the sequence, and what to emit for the output.</source>
          <target state="translated">该函数是 &lt;code&gt;dynamic_rnn&lt;/code&gt; 的更原始版本，可在每次迭代时更直接地访问输入。它还提供了对何时开始和结束读取序列以及为输出发出什么信号的更多控制。</target>
        </trans-unit>
        <trans-unit id="b35cb926e7e56c5e0bb4984bfdd19c6c71696446" translate="yes" xml:space="preserve">
          <source>This function is a simpler wrapper around the more general &lt;a href=&quot;convolution&quot;&gt;&lt;code&gt;tf.nn.convolution&lt;/code&gt;&lt;/a&gt;, and exists only for backwards compatibility. You can use &lt;a href=&quot;convolution&quot;&gt;&lt;code&gt;tf.nn.convolution&lt;/code&gt;&lt;/a&gt; to perform 1-D, 2-D, or 3-D atrous convolution.</source>
          <target state="translated">此函数是更通用的&lt;a href=&quot;convolution&quot;&gt; &lt;code&gt;tf.nn.convolution&lt;/code&gt; &lt;/a&gt;的更简单包装，并且仅用于向后兼容。您可以使用&lt;a href=&quot;convolution&quot;&gt; &lt;code&gt;tf.nn.convolution&lt;/code&gt; &lt;/a&gt;执行1-D，2-D或3-D原子卷积。</target>
        </trans-unit>
        <trans-unit id="1fde94a598e8772eaea3cc47dd741a3aa23b5c85" translate="yes" xml:space="preserve">
          <source>This function is analogous to &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html&quot;&gt;&lt;code&gt;numpy.linalg.pinv&lt;/code&gt;&lt;/a&gt;. It differs only in default value of &lt;code&gt;rcond&lt;/code&gt;. In &lt;code&gt;numpy.linalg.pinv&lt;/code&gt;, the default &lt;code&gt;rcond&lt;/code&gt; is &lt;code&gt;1e-15&lt;/code&gt;. Here the default is &lt;code&gt;10. * max(num_rows, num_cols) * np.finfo(dtype).eps&lt;/code&gt;.</source>
          <target state="translated">该函数类似于&lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html&quot;&gt; &lt;code&gt;numpy.linalg.pinv&lt;/code&gt; &lt;/a&gt;。它仅在 &lt;code&gt;rcond&lt;/code&gt; 的默认值上有所不同。在 &lt;code&gt;numpy.linalg.pinv&lt;/code&gt; 中，默认 &lt;code&gt;rcond&lt;/code&gt; 为 &lt;code&gt;1e-15&lt;/code&gt; 。此处的默认值为 &lt;code&gt;10. * max(num_rows, num_cols) * np.finfo(dtype).eps&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c82b17de9fe920f72f7a5369e74c77b8a5f75d81" translate="yes" xml:space="preserve">
          <source>This function is based on the standard SSIM implementation from: Wang, Z., Bovik, A. C., Sheikh, H. R., &amp;amp; Simoncelli, E. P. (2004). Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing.</source>
          <target state="translated">此功能基于以下标准的SSIM实现：Wang，Z.，Bovik，AC，Sheikh，HR和Simoncelli，EP（2004）。图像质量评估：从错误可见性到结构相似性。IEEE图像处理事务。</target>
        </trans-unit>
        <trans-unit id="3f78f322021258197ee0401af332233d4fb6c937" translate="yes" xml:space="preserve">
          <source>This function is cached the first time &lt;a href=&quot;../model#evaluate&quot;&gt;&lt;code&gt;Model.evaluate&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../model#test_on_batch&quot;&gt;&lt;code&gt;Model.test_on_batch&lt;/code&gt;&lt;/a&gt; is called. The cache is cleared whenever &lt;a href=&quot;../model#compile&quot;&gt;&lt;code&gt;Model.compile&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="translated">第一次&lt;a href=&quot;../model#evaluate&quot;&gt; &lt;code&gt;Model.evaluate&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;../model#test_on_batch&quot;&gt; &lt;code&gt;Model.test_on_batch&lt;/code&gt; &lt;/a&gt;时，将缓存此函数。每当&lt;a href=&quot;../model#compile&quot;&gt; &lt;code&gt;Model.compile&lt;/code&gt; &lt;/a&gt;时，都会清除缓存。</target>
        </trans-unit>
        <trans-unit id="03d4062526248415274d1920f151d5925ea1d106" translate="yes" xml:space="preserve">
          <source>This function is cached the first time &lt;a href=&quot;../model#fit&quot;&gt;&lt;code&gt;Model.fit&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../model#train_on_batch&quot;&gt;&lt;code&gt;Model.train_on_batch&lt;/code&gt;&lt;/a&gt; is called. The cache is cleared whenever &lt;a href=&quot;../model#compile&quot;&gt;&lt;code&gt;Model.compile&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="translated">第一次调用&lt;a href=&quot;../model#fit&quot;&gt; &lt;code&gt;Model.fit&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;../model#train_on_batch&quot;&gt; &lt;code&gt;Model.train_on_batch&lt;/code&gt; &lt;/a&gt;时，将缓存此函数。每当&lt;a href=&quot;../model#compile&quot;&gt; &lt;code&gt;Model.compile&lt;/code&gt; &lt;/a&gt;时，都会清除缓存。</target>
        </trans-unit>
        <trans-unit id="a73ed236f67dee6103073403c6d29f26833958d9" translate="yes" xml:space="preserve">
          <source>This function is cached the first time &lt;a href=&quot;../model#predict&quot;&gt;&lt;code&gt;Model.predict&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../model#predict_on_batch&quot;&gt;&lt;code&gt;Model.predict_on_batch&lt;/code&gt;&lt;/a&gt; is called. The cache is cleared whenever &lt;a href=&quot;../model#compile&quot;&gt;&lt;code&gt;Model.compile&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="translated">第一次&lt;a href=&quot;../model#predict&quot;&gt; &lt;code&gt;Model.predict&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;../model#predict_on_batch&quot;&gt; &lt;code&gt;Model.predict_on_batch&lt;/code&gt; &lt;/a&gt;时将缓存此函数。每当&lt;a href=&quot;../model#compile&quot;&gt; &lt;code&gt;Model.compile&lt;/code&gt; &lt;/a&gt;时，都会清除缓存。</target>
        </trans-unit>
        <trans-unit id="9c02b98872d235ad42d3656b50ce22017eda7ff2" translate="yes" xml:space="preserve">
          <source>This function is cached the first time &lt;a href=&quot;model#evaluate&quot;&gt;&lt;code&gt;Model.evaluate&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;model#test_on_batch&quot;&gt;&lt;code&gt;Model.test_on_batch&lt;/code&gt;&lt;/a&gt; is called. The cache is cleared whenever &lt;a href=&quot;model#compile&quot;&gt;&lt;code&gt;Model.compile&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="translated">第一次&lt;a href=&quot;model#evaluate&quot;&gt; &lt;code&gt;Model.evaluate&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;model#test_on_batch&quot;&gt; &lt;code&gt;Model.test_on_batch&lt;/code&gt; &lt;/a&gt;时，将缓存此函数。每当&lt;a href=&quot;model#compile&quot;&gt; &lt;code&gt;Model.compile&lt;/code&gt; &lt;/a&gt;时，都会清除缓存。</target>
        </trans-unit>
        <trans-unit id="9d1d8a8ff69ef7a941ba68b453aec429bfe458e1" translate="yes" xml:space="preserve">
          <source>This function is cached the first time &lt;a href=&quot;model#fit&quot;&gt;&lt;code&gt;Model.fit&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;model#train_on_batch&quot;&gt;&lt;code&gt;Model.train_on_batch&lt;/code&gt;&lt;/a&gt; is called. The cache is cleared whenever &lt;a href=&quot;model#compile&quot;&gt;&lt;code&gt;Model.compile&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="translated">第一次调用&lt;a href=&quot;model#fit&quot;&gt; &lt;code&gt;Model.fit&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;model#train_on_batch&quot;&gt; &lt;code&gt;Model.train_on_batch&lt;/code&gt; &lt;/a&gt;时，将缓存此函数。每当&lt;a href=&quot;model#compile&quot;&gt; &lt;code&gt;Model.compile&lt;/code&gt; &lt;/a&gt;时，都会清除缓存。</target>
        </trans-unit>
        <trans-unit id="5d0db761a3ad75448d327cc87f61eb8b0729f377" translate="yes" xml:space="preserve">
          <source>This function is cached the first time &lt;a href=&quot;model#predict&quot;&gt;&lt;code&gt;Model.predict&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;model#predict_on_batch&quot;&gt;&lt;code&gt;Model.predict_on_batch&lt;/code&gt;&lt;/a&gt; is called. The cache is cleared whenever &lt;a href=&quot;model#compile&quot;&gt;&lt;code&gt;Model.compile&lt;/code&gt;&lt;/a&gt; is called.</source>
          <target state="translated">第一次&lt;a href=&quot;model#predict&quot;&gt; &lt;code&gt;Model.predict&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;model#predict_on_batch&quot;&gt; &lt;code&gt;Model.predict_on_batch&lt;/code&gt; &lt;/a&gt;时将缓存此函数。每当&lt;a href=&quot;model#compile&quot;&gt; &lt;code&gt;Model.compile&lt;/code&gt; &lt;/a&gt;时，都会清除缓存。</target>
        </trans-unit>
        <trans-unit id="ecacc04269db8f28f91780c2b346f24c2d101704" translate="yes" xml:space="preserve">
          <source>This function is called between epochs/steps, when a metric is evaluated during training.</source>
          <target state="translated">当在训练过程中评估度量时,该函数在两个纪元/步之间被调用。</target>
        </trans-unit>
        <trans-unit id="a75cbe1aa934dcb50e2537aae264b28dbab09250" translate="yes" xml:space="preserve">
          <source>This function is called by FLAGS(argv). It scans the input list for a flag that looks like: --flagfile=</source>
          <target state="translated">这个函数被FLAGS(argv)调用。它扫描输入列表,寻找一个类似的标志。--flagfile=</target>
        </trans-unit>
        <trans-unit id="e57b5dff2b7e230f6a359d3b07cb810778cc3047" translate="yes" xml:space="preserve">
          <source>This function is called in the main TensorFlow &lt;code&gt;__init__.py&lt;/code&gt; file, user should not need to call it, except during complex migrations.</source>
          <target state="translated">该函数在TensorFlow &lt;code&gt;__init__.py&lt;/code&gt; 主文件中调用，用户不需要调用它，除非在复杂的迁移过程中。</target>
        </trans-unit>
        <trans-unit id="0831cfa041ba84a9c1bb953a8575728ee9dbe8bf" translate="yes" xml:space="preserve">
          <source>This function is faster and numerically stabler than &lt;code&gt;bessel_i0(x)&lt;/code&gt;.</source>
          <target state="translated">该函数比 &lt;code&gt;bessel_i0(x)&lt;/code&gt; 更快并且在数值上更稳定。</target>
        </trans-unit>
        <trans-unit id="1f2e499db54e57708a372078c16af5df37036099" translate="yes" xml:space="preserve">
          <source>This function is faster and numerically stabler than &lt;code&gt;bessel_i1(x)&lt;/code&gt;.</source>
          <target state="translated">该函数比 &lt;code&gt;bessel_i1(x)&lt;/code&gt; 更快且在数值上更稳定。</target>
        </trans-unit>
        <trans-unit id="d7e374cf0f52d98689ddb22ec7b9e2a428cd4956" translate="yes" xml:space="preserve">
          <source>This function is implemented using a queue. A &lt;code&gt;QueueRunner&lt;/code&gt; for the queue is added to the current &lt;code&gt;Graph&lt;/code&gt;'s &lt;code&gt;QUEUE_RUNNER&lt;/code&gt; collection.</source>
          <target state="translated">此功能使用队列来实现。队列的 &lt;code&gt;QueueRunner&lt;/code&gt; 已添加到当前 &lt;code&gt;Graph&lt;/code&gt; 的 &lt;code&gt;QUEUE_RUNNER&lt;/code&gt; 集合中。</target>
        </trans-unit>
        <trans-unit id="4c4c3026638168daaa4ab1a3c114680f01d9d246" translate="yes" xml:space="preserve">
          <source>This function is more numerically stable than log(sum(exp(input))). It avoids overflows caused by taking the exp of large inputs and underflows caused by taking the log of small inputs.</source>
          <target state="translated">这个函数在数值上比log(sum(exp(input))更稳定。)它避免了因取大输入的exp而造成的溢出和因取小输入的log而造成的不足。</target>
        </trans-unit>
        <trans-unit id="80c0f02cfab4a0ee1574d2a1145f5fca28f12b11" translate="yes" xml:space="preserve">
          <source>This function is only available with the TensorFlow backend for the time being.</source>
          <target state="translated">这个功能暂时只在TensorFlow后端可用。</target>
        </trans-unit>
        <trans-unit id="11a7ac7f028baf06c483242a158777b17ee8b353" translate="yes" xml:space="preserve">
          <source>This function is only used when defining a new op type. It may be used for ops such as &lt;a href=&quot;size&quot;&gt;&lt;code&gt;tf.size()&lt;/code&gt;&lt;/a&gt; that are not differentiable. For example:</source>
          <target state="translated">仅在定义新的op类型时使用此功能。它可用于不可区分的操作，例如&lt;a href=&quot;size&quot;&gt; &lt;code&gt;tf.size()&lt;/code&gt; &lt;/a&gt;。例如：</target>
        </trans-unit>
        <trans-unit id="61969c39a3dfe9d632e8e43316d05aa062590efa" translate="yes" xml:space="preserve">
          <source>This function is part of the Keras serialization and deserialization framework. It maps objects to the string names associated with those objects for serialization/deserialization.</source>
          <target state="translated">该函数是Keras序列化和反序列化框架的一部分。它将对象映射到与这些对象相关联的字符串名称,以便进行序列化/反序列化。</target>
        </trans-unit>
        <trans-unit id="7aa4be9a74ac3f90821c5e3ef2a47e10c4dfdcb8" translate="yes" xml:space="preserve">
          <source>This function is part of the Keras serialization and deserialization framework. It maps strings to the objects associated with them for serialization/deserialization.</source>
          <target state="translated">该函数是Keras序列化和反序列化框架的一部分。它将字符串映射到与之相关联的对象上,以便进行序列化/反序列化。</target>
        </trans-unit>
        <trans-unit id="577a32736f43471cd2cd1266738343fb80596d48" translate="yes" xml:space="preserve">
          <source>This function is the canonical way to get/validate an object of one of the allowed types from an external argument reference in the Session API.</source>
          <target state="translated">这个函数是从Session API的外部参数引用中获取/验证一个允许类型的对象的规范方法。</target>
        </trans-unit>
        <trans-unit id="130785c66d51b0bf917ff1a83e8cb5fd0234052e" translate="yes" xml:space="preserve">
          <source>This function is to generate &lt;a href=&quot;../distributedvalues&quot;&gt;&lt;code&gt;tf.distribute.DistributedValues&lt;/code&gt;&lt;/a&gt; to pass into &lt;code&gt;run&lt;/code&gt;, &lt;code&gt;reduce&lt;/code&gt;, or other methods that take distributed values when not using datasets.</source>
          <target state="translated">此函数将生成&lt;a href=&quot;../distributedvalues&quot;&gt; &lt;code&gt;tf.distribute.DistributedValues&lt;/code&gt; &lt;/a&gt;传递给 &lt;code&gt;run&lt;/code&gt; ， &lt;code&gt;reduce&lt;/code&gt; 或其他不使用数据集时采用分布式值的方法。</target>
        </trans-unit>
        <trans-unit id="84ff67d2523742ed898675e46a84da4adae4f6b1" translate="yes" xml:space="preserve">
          <source>This function is to generate &lt;a href=&quot;distributedvalues&quot;&gt;&lt;code&gt;tf.distribute.DistributedValues&lt;/code&gt;&lt;/a&gt; to pass into &lt;code&gt;run&lt;/code&gt;, &lt;code&gt;reduce&lt;/code&gt;, or other methods that take distributed values when not using datasets.</source>
          <target state="translated">此函数将生成&lt;a href=&quot;distributedvalues&quot;&gt; &lt;code&gt;tf.distribute.DistributedValues&lt;/code&gt; &lt;/a&gt;传递给 &lt;code&gt;run&lt;/code&gt; ， &lt;code&gt;reduce&lt;/code&gt; 或其他不使用数据集时采用分布式值的方法。</target>
        </trans-unit>
        <trans-unit id="691cc7f8251e205cf948be38afb6e034f26a2e72" translate="yes" xml:space="preserve">
          <source>This function is used to perform parallel lookups on the list of tensors in &lt;code&gt;params&lt;/code&gt;. It is a generalization of &lt;a href=&quot;../../../gather&quot;&gt;&lt;code&gt;tf.gather&lt;/code&gt;&lt;/a&gt;, where &lt;code&gt;params&lt;/code&gt; is interpreted as a partitioning of a large embedding tensor. &lt;code&gt;params&lt;/code&gt; may be a &lt;code&gt;PartitionedVariable&lt;/code&gt; as returned by using &lt;a href=&quot;../get_variable&quot;&gt;&lt;code&gt;tf.compat.v1.get_variable()&lt;/code&gt;&lt;/a&gt; with a partitioner.</source>
          <target state="translated">此函数用于在 &lt;code&gt;params&lt;/code&gt; 中的张量列表上执行并行查找。它是&lt;a href=&quot;../../../gather&quot;&gt; &lt;code&gt;tf.gather&lt;/code&gt; &lt;/a&gt;的一般化，其中 &lt;code&gt;params&lt;/code&gt; 被解释为大嵌入张量的划分。 &lt;code&gt;params&lt;/code&gt; 可以是通过将&lt;a href=&quot;../get_variable&quot;&gt; &lt;code&gt;tf.compat.v1.get_variable()&lt;/code&gt; &lt;/a&gt;与分区程序一起使用返回的 &lt;code&gt;PartitionedVariable&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4938f0a72e046cd0e52c1383bdb6cd4186015a22" translate="yes" xml:space="preserve">
          <source>This function is used to perform parallel lookups on the list of tensors in &lt;code&gt;params&lt;/code&gt;. It is a generalization of &lt;a href=&quot;../gather&quot;&gt;&lt;code&gt;tf.gather&lt;/code&gt;&lt;/a&gt;, where &lt;code&gt;params&lt;/code&gt; is interpreted as a partitioning of a large embedding tensor.</source>
          <target state="translated">此函数用于在 &lt;code&gt;params&lt;/code&gt; 中的张量列表上执行并行查找。它是&lt;a href=&quot;../gather&quot;&gt; &lt;code&gt;tf.gather&lt;/code&gt; &lt;/a&gt;的一般化，其中 &lt;code&gt;params&lt;/code&gt; 被解释为大嵌入张量的划分。</target>
        </trans-unit>
        <trans-unit id="237e26d699d02ce05b0e4079a36fba68db8a8789" translate="yes" xml:space="preserve">
          <source>This function is used to perform parallel lookups on the list of tensors in &lt;code&gt;params&lt;/code&gt;. It is a generalization of &lt;a href=&quot;../gather&quot;&gt;&lt;code&gt;tf.gather&lt;/code&gt;&lt;/a&gt;, where &lt;code&gt;params&lt;/code&gt; is interpreted as a partitioning of a large embedding tensor. &lt;code&gt;params&lt;/code&gt; may be a &lt;code&gt;PartitionedVariable&lt;/code&gt; as returned by using &lt;a href=&quot;../compat/v1/get_variable&quot;&gt;&lt;code&gt;tf.compat.v1.get_variable()&lt;/code&gt;&lt;/a&gt; with a partitioner.</source>
          <target state="translated">此函数用于在 &lt;code&gt;params&lt;/code&gt; 中的张量列表上执行并行查找。它是&lt;a href=&quot;../gather&quot;&gt; &lt;code&gt;tf.gather&lt;/code&gt; &lt;/a&gt;的一般化，其中 &lt;code&gt;params&lt;/code&gt; 被解释为大嵌入张量的划分。 &lt;code&gt;params&lt;/code&gt; 可以是通过将&lt;a href=&quot;../compat/v1/get_variable&quot;&gt; &lt;code&gt;tf.compat.v1.get_variable()&lt;/code&gt; &lt;/a&gt;与分区程序一起使用返回的 &lt;code&gt;PartitionedVariable&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f577da20d8fc0c2379f7e51688987afe782aa9c2" translate="yes" xml:space="preserve">
          <source>This function is useful for unit testing. A unit test can test using the mixed precision graph rewrite, then disable it so future unit tests continue using float32.</source>
          <target state="translated">这个函数对单元测试很有用。单元测试可以使用混合精度图重写进行测试,然后禁用它,这样未来的单元测试就可以继续使用float32。</target>
        </trans-unit>
        <trans-unit id="d61d06f477fe487d45a847bca66c249257c10fe4" translate="yes" xml:space="preserve">
          <source>This function is useful for unit testing. A unit tests can test using the mixed precision graph rewrite, then disable it so future unit tests continue using float32. If this is done, unit tests should not share a single session, as &lt;code&gt;enable_mixed_precision_graph_rewrite&lt;/code&gt; and &lt;code&gt;disable_mixed_precision_graph_rewrite&lt;/code&gt; have no effect on existing sessions.</source>
          <target state="translated">此功能对于单元测试很有用。单元测试可以使用混合精度图形重写进行测试，然后将其禁用，因此以后的单元测试将继续使用float32。如果这样做，则单元测试不应共享单个会话，因为 &lt;code&gt;enable_mixed_precision_graph_rewrite&lt;/code&gt; 和 &lt;code&gt;disable_mixed_precision_graph_rewrite&lt;/code&gt; 对现有会话没有影响。</target>
        </trans-unit>
        <trans-unit id="929b9d5a5eeca32709da98b98c1fa20f11f6684a" translate="yes" xml:space="preserve">
          <source>This function may be used in the &lt;code&gt;options&lt;/code&gt; argument in functions that load a SavedModel (&lt;a href=&quot;load&quot;&gt;&lt;code&gt;tf.saved_model.load&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../keras/models/load_model&quot;&gt;&lt;code&gt;tf.keras.models.load_model&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">可以在加载SavedModel（&lt;a href=&quot;load&quot;&gt; &lt;code&gt;tf.saved_model.load&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;../keras/models/load_model&quot;&gt; &lt;code&gt;tf.keras.models.load_model&lt;/code&gt; &lt;/a&gt;）的函数的 &lt;code&gt;options&lt;/code&gt; 参数中使用此函数。</target>
        </trans-unit>
        <trans-unit id="d4e537bea1c76cb2cbcdaeafa3c9baedcb0e335e" translate="yes" xml:space="preserve">
          <source>This function may be used in the &lt;code&gt;options&lt;/code&gt; argument in functions that save a SavedModel (&lt;a href=&quot;save&quot;&gt;&lt;code&gt;tf.saved_model.save&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../keras/models/save_model&quot;&gt;&lt;code&gt;tf.keras.models.save_model&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">该函数可用于保存SavedModel的函数的 &lt;code&gt;options&lt;/code&gt; 参数中（&lt;a href=&quot;save&quot;&gt; &lt;code&gt;tf.saved_model.save&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;../keras/models/save_model&quot;&gt; &lt;code&gt;tf.keras.models.save_model&lt;/code&gt; &lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="23d4449e48af4e4a9efae86d49cf037eac266df3" translate="yes" xml:space="preserve">
          <source>This function only gets the device policy for the current thread. Any subsequently started thread will again use the default policy.</source>
          <target state="translated">这个函数只获取当前线程的设备策略。任何随后启动的线程将再次使用默认策略。</target>
        </trans-unit>
        <trans-unit id="69c8cb582945069c7d9496b069e6902700fc7e3d" translate="yes" xml:space="preserve">
          <source>This function only sets the device policy for the current thread. Any subsequently started thread will again use the default policy.</source>
          <target state="translated">该函数只为当前线程设置设备策略。任何随后启动的线程将再次使用默认策略。</target>
        </trans-unit>
        <trans-unit id="128baaf391e006514e7e2e442ed3dbb64e14a3c9" translate="yes" xml:space="preserve">
          <source>This function performs the equivalent of</source>
          <target state="translated">该功能的作用相当于</target>
        </trans-unit>
        <trans-unit id="fac08ba1f46139129a2f42247910a712f1085f49" translate="yes" xml:space="preserve">
          <source>This function prefixes the name with the current variable scope and performs reuse checks. See the &lt;a href=&quot;https://tensorflow.org/guide/variables&quot;&gt;Variable Scope How To&lt;/a&gt; for an extensive description of how reusing works. Here is a basic example:</source>
          <target state="translated">此函数在名称前加上当前变量作用域，并执行重用检查。有关重用的工作原理的详细说明，请参见&amp;ldquo; &lt;a href=&quot;https://tensorflow.org/guide/variables&quot;&gt;可变作用域&amp;rdquo;&lt;/a&gt;。这是一个基本示例：</target>
        </trans-unit>
        <trans-unit id="85bc4aa7e8b39b071c3a83bcd2bcb628b96948b5" translate="yes" xml:space="preserve">
          <source>This function produces signatures intended for use with the TensorFlow Serving Classify API (tensorflow_serving/apis/prediction_service.proto), and so constrains the input and output types to those allowed by TensorFlow Serving.</source>
          <target state="translated">该函数产生的签名旨在与TensorFlow Serving分类API(tensorflow_serving/apis/prediction_service.proto)一起使用,因此将输入和输出类型约束为TensorFlow Serving允许的类型。</target>
        </trans-unit>
        <trans-unit id="795898cb16fc0e553ecaa87cbe16e8417edef412" translate="yes" xml:space="preserve">
          <source>This function produces signatures intended for use with the TensorFlow Serving Predict API (tensorflow_serving/apis/prediction_service.proto). This API imposes no constraints on the input and output types.</source>
          <target state="translated">该函数产生旨在与 TensorFlow Serving Predict API (tensorflow_serving/apis/prediction_service.proto)一起使用的签名。这个API对输入和输出类型没有任何限制。</target>
        </trans-unit>
        <trans-unit id="0bd91f24173f9c6c06caccaa86fb2987286936b9" translate="yes" xml:space="preserve">
          <source>This function produces signatures intended for use with the TensorFlow Serving Regress API (tensorflow_serving/apis/prediction_service.proto), and so constrains the input and output types to those allowed by TensorFlow Serving.</source>
          <target state="translated">该函数产生的签名旨在用于TensorFlow Serving Regress API (tensorflow_serving/apis/prediction_service.proto),因此将输入和输出类型约束为TensorFlow Serving允许的类型。</target>
        </trans-unit>
        <trans-unit id="da2207550b053f2e41ed03fce80a539629a65e2e" translate="yes" xml:space="preserve">
          <source>This function provides a way to import a serialized TensorFlow &lt;a href=&quot;https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto&quot;&gt;&lt;code&gt;GraphDef&lt;/code&gt;&lt;/a&gt; protocol buffer, and extract individual objects in the &lt;code&gt;GraphDef&lt;/code&gt; as &lt;a href=&quot;../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../operation&quot;&gt;&lt;code&gt;tf.Operation&lt;/code&gt;&lt;/a&gt; objects. Once extracted, these objects are placed into the current default &lt;code&gt;Graph&lt;/code&gt;. See &lt;a href=&quot;../graph#as_graph_def&quot;&gt;&lt;code&gt;tf.Graph.as_graph_def&lt;/code&gt;&lt;/a&gt; for a way to create a &lt;code&gt;GraphDef&lt;/code&gt; proto.</source>
          <target state="translated">此功能提供了一种导入序列化TensorFlow &lt;a href=&quot;https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto&quot;&gt; &lt;code&gt;GraphDef&lt;/code&gt; &lt;/a&gt;协议缓冲区并将 &lt;code&gt;GraphDef&lt;/code&gt; 中的单个对象提取为&lt;a href=&quot;../tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../operation&quot;&gt; &lt;code&gt;tf.Operation&lt;/code&gt; &lt;/a&gt;对象的方法。提取后，将这些对象放入当前的默认 &lt;code&gt;Graph&lt;/code&gt; 中。有关创建 &lt;code&gt;GraphDef&lt;/code&gt; 原型的方法，请参见&lt;a href=&quot;../graph#as_graph_def&quot;&gt; &lt;code&gt;tf.Graph.as_graph_def&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="59bf9d9f9818c530570cd3baaed9172f0b655e0a" translate="yes" xml:space="preserve">
          <source>This function raises &lt;code&gt;ValueError&lt;/code&gt; unless it can be certain that the given &lt;code&gt;tensor&lt;/code&gt; is a scalar. &lt;code&gt;ValueError&lt;/code&gt; is also raised if the shape of &lt;code&gt;tensor&lt;/code&gt; is unknown.</source>
          <target state="translated">除非可以确定给定 &lt;code&gt;tensor&lt;/code&gt; 是一个标量，否则此函数将引发 &lt;code&gt;ValueError&lt;/code&gt; 。如果 &lt;code&gt;tensor&lt;/code&gt; 的形状未知，也会引发 &lt;code&gt;ValueError&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="22fe780c67f001ded49fb5e4133cb0847481dd48" translate="yes" xml:space="preserve">
          <source>This function receives as input a string of text and returns a list of encoded integers each corresponding to a word (or token) in the given input string.</source>
          <target state="translated">这个函数接收一串文字作为输入,并返回一个编码的整数列表,每个整数对应于给定输入字符串中的一个字(或标记)。</target>
        </trans-unit>
        <trans-unit id="3009d910bdcd32cff99d1458043403e12c1c254c" translate="yes" xml:space="preserve">
          <source>This function reinstantiates model state by:</source>
          <target state="translated">该函数通过以下方式对模型状态进行重述:</target>
        </trans-unit>
        <trans-unit id="a733b4ff78c36cb4d261221c5118416ce8061d8e" translate="yes" xml:space="preserve">
          <source>This function reinstantiates model state by: 1) loading model topology from json (this will eventually come from metagraph). 2) loading model weights from checkpoint.</source>
          <target state="translated">这个函数通过以下方式来重述模型状态。1)从json加载模型拓扑结构(最终会从metagraph加载)。2)从检查点加载模型权重。</target>
        </trans-unit>
        <trans-unit id="ff0b45b19346e757b8eb62e1bc7385c9d06f1d69" translate="yes" xml:space="preserve">
          <source>This function returns a tensor whose elements are defined by &lt;code&gt;equation&lt;/code&gt;, which is written in a shorthand form inspired by the Einstein summation convention. As an example, consider multiplying two matrices A and B to form a matrix C. The elements of C are given by:</source>
          <target state="translated">此函数返回一个张量，其张量由 &lt;code&gt;equation&lt;/code&gt; 定义，并以爱因斯坦求和惯例的启发以简写形式编写。例如，考虑将两个矩阵A和B相乘以形成矩阵C。C的元素由下式给出：</target>
        </trans-unit>
        <trans-unit id="0b7b7a286f6486a214583e48df98d413e08472ce" translate="yes" xml:space="preserve">
          <source>This function should &lt;em&gt;not&lt;/em&gt; be used for operations that have a well-defined gradient that is not yet implemented.</source>
          <target state="translated">此功能&lt;em&gt;不&lt;/em&gt;应用于具有尚未定义的定义明确的渐变的操作。</target>
        </trans-unit>
        <trans-unit id="1b640a4c1fd849556e2182a5a501c968b01e5603" translate="yes" xml:space="preserve">
          <source>This function should contain the mathemetical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates.</source>
          <target state="translated">该函数应包含一步评估的数学逻辑。这通常包括正向传递、损失计算和指标更新。</target>
        </trans-unit>
        <trans-unit id="48eea658706b62c655231b07e7ecfd12850950f0" translate="yes" xml:space="preserve">
          <source>This function should only be called during TRAIN mode.</source>
          <target state="translated">此功能只应在TRAIN模式下调用。</target>
        </trans-unit>
        <trans-unit id="92add72d0f856bfc3fd4e7bcbf6ab90dc2a5a26d" translate="yes" xml:space="preserve">
          <source>This function specifies the device to be used for ops created/executed in a particular context. Nested contexts will inherit and also create/execute their ops on the specified device. If a specific device is not required, consider not using this function so that a device can be automatically assigned. In general the use of this function is optional. &lt;code&gt;device_name&lt;/code&gt; can be fully specified, as in &quot;/job:worker/task:1/device:cpu:0&quot;, or partially specified, containing only a subset of the &quot;/&quot;-separated fields. Any fields which are specified will override device annotations from outer scopes.</source>
          <target state="translated">该函数指定要在特定上下文中创建/执行的操作所使用的设备。嵌套上下文将在指定设备上继承并创建/执行其操作。如果不需要特定设备，请考虑不使用此功能，以便可以自动分配设备。通常，此功能的使用是可选的。 &lt;code&gt;device_name&lt;/code&gt; 可以完全指定，例如在&amp;ldquo; / job：worker / task：1 / device：cpu：0&amp;rdquo;中，也可以部分指定，仅包含&amp;ldquo; /&amp;rdquo;分隔字段的子集。指定的任何字段都将覆盖外部作用域中的设备注释。</target>
        </trans-unit>
        <trans-unit id="c538dca8aca927a7d8bbd1b0bb2e590f6c5ad3a5" translate="yes" xml:space="preserve">
          <source>This function supports a subset of tf.gather, see tf.gather for details on usage.</source>
          <target state="translated">这个函数支持tf.gather的一个子集,详细用法请看tf.gather。</target>
        </trans-unit>
        <trans-unit id="e857d76db9c7bac209f1715cec1437cf523540d8" translate="yes" xml:space="preserve">
          <source>This function swaps half-spaces for all axes listed (defaults to all). Note that &lt;code&gt;y[0]&lt;/code&gt; is the Nyquist component only if &lt;code&gt;len(x)&lt;/code&gt; is even.</source>
          <target state="translated">此功能为列出的所有轴交换半角空格（默认为全部）。注意，仅当 &lt;code&gt;len(x)&lt;/code&gt; 为偶数时， &lt;code&gt;y[0]&lt;/code&gt; 是奈奎斯特分量。</target>
        </trans-unit>
        <trans-unit id="700dee7b88966a0547c5ef0b6e43b47147bea1dd" translate="yes" xml:space="preserve">
          <source>This function takes a &lt;code&gt;MetaGraphDef&lt;/code&gt; protocol buffer as input. If the argument is a file containing a &lt;code&gt;MetaGraphDef&lt;/code&gt; protocol buffer , it constructs a protocol buffer from the file content. The function then adds all the nodes from the &lt;code&gt;graph_def&lt;/code&gt; field to the current graph, recreates all the collections, and returns a saver constructed from the &lt;code&gt;saver_def&lt;/code&gt; field.</source>
          <target state="translated">此函数将 &lt;code&gt;MetaGraphDef&lt;/code&gt; 协议缓冲区作为输入。如果参数是一个包含 &lt;code&gt;MetaGraphDef&lt;/code&gt; 协议缓冲区的文件，则它将根据文件内容构造一个协议缓冲区。然后，该函数将 &lt;code&gt;graph_def&lt;/code&gt; 字段中的所有节点添加到当前图形中，重新创建所有集合，并返回从 &lt;code&gt;saver_def&lt;/code&gt; 字段构造的保护程序。</target>
        </trans-unit>
        <trans-unit id="fd08a12501f4bce4f0a38bb7f61e556860ac6186" translate="yes" xml:space="preserve">
          <source>This function takes in a sequence of data-points gathered at equal intervals, along with time series parameters such as length of the sequences/windows, spacing between two sequence/windows, etc., to produce batches of timeseries inputs and targets.</source>
          <target state="translated">该函数接收以等间隔收集的数据点序列,以及时间序列参数,如序列/窗口的长度、两个序列/窗口之间的间距等,从而产生一批时间序列输入和目标。</target>
        </trans-unit>
        <trans-unit id="f6d25a51ea520664ee90a7477aef16d45f240f73" translate="yes" xml:space="preserve">
          <source>This function transforms a list (of length &lt;code&gt;num_samples&lt;/code&gt;) of sequences (lists of integers) into a 2D Numpy array of shape &lt;code&gt;(num_samples, num_timesteps)&lt;/code&gt;. &lt;code&gt;num_timesteps&lt;/code&gt; is either the &lt;code&gt;maxlen&lt;/code&gt; argument if provided, or the length of the longest sequence in the list.</source>
          <target state="translated">此函数将序列（长度为 &lt;code&gt;num_samples&lt;/code&gt; ）列表（整数列表）转换为2D Numpy形状的数组 &lt;code&gt;(num_samples, num_timesteps)&lt;/code&gt; 。 &lt;code&gt;num_timesteps&lt;/code&gt; 是 &lt;code&gt;maxlen&lt;/code&gt; 参数（如果提供）或列表中最长序列的长度。</target>
        </trans-unit>
        <trans-unit id="94fc01032dc6978adba1e0b4d64fc3f4b8d2c710" translate="yes" xml:space="preserve">
          <source>This function transforms a list of &lt;code&gt;num_samples&lt;/code&gt; sequences (lists of integers) into a 2D Numpy array of shape &lt;code&gt;(num_samples, num_timesteps)&lt;/code&gt;. &lt;code&gt;num_timesteps&lt;/code&gt; is either the &lt;code&gt;maxlen&lt;/code&gt; argument if provided, or the length of the longest sequence otherwise.</source>
          <target state="translated">此函数将 &lt;code&gt;num_samples&lt;/code&gt; 序列列表（整数列表）转换为2D Numpy形状的数组 &lt;code&gt;(num_samples, num_timesteps)&lt;/code&gt; 。 &lt;code&gt;num_timesteps&lt;/code&gt; 是 &lt;code&gt;maxlen&lt;/code&gt; 参数（如果提供），或者是最长序列的长度。</target>
        </trans-unit>
        <trans-unit id="c60464500521000032d7d0dd44fb1270d9c6691c" translate="yes" xml:space="preserve">
          <source>This function transforms a sequence of word indexes (list of integers) into tuples of words of the form:</source>
          <target state="translated">该函数将单词索引序列(整数列表)转换为形式为单词的元组。</target>
        </trans-unit>
        <trans-unit id="109cf8b58e8d5422a5502fb692567a43d8008aa5" translate="yes" xml:space="preserve">
          <source>This function transforms a string of text into a list of words while ignoring &lt;code&gt;filters&lt;/code&gt; which include punctuations by default.</source>
          <target state="translated">此函数将文本字符串转换为单词列表，同时忽略默认情况下包括标点符号的 &lt;code&gt;filters&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0cf6cff1a07ce3f9dd6dd3457b8400fd6ba9faf8" translate="yes" xml:space="preserve">
          <source>This function uses substring matching, i.e. the matching succeeds if &lt;em&gt;any&lt;/em&gt; substring of the error message matches &lt;em&gt;any&lt;/em&gt; regex in the list. This is more convenient for the user than full-string matching.</source>
          <target state="translated">此函数使用子字符串匹配，即，如果错误消息的&lt;em&gt;任何子&lt;/em&gt;字符串与列表中的&lt;em&gt;任何&lt;/em&gt;正则表达式匹配，则匹配成功。对于用户而言，这比全字符串匹配更方便。</target>
        </trans-unit>
        <trans-unit id="6ea3c37a0724be70d72aba9456ec479ad3e1efd0" translate="yes" xml:space="preserve">
          <source>This function validates that &lt;code&gt;obj&lt;/code&gt; represents an element of this graph, and gives an informative error message if it is not.</source>
          <target state="translated">此函数验证 &lt;code&gt;obj&lt;/code&gt; 代表该图的元素，如果不是，则给出提示性错误消息。</target>
        </trans-unit>
        <trans-unit id="5535ae7e49ad8d44ba9e9cbd32412f5d3383ba9d" translate="yes" xml:space="preserve">
          <source>This function will check the outermost context for the program and see if it is in eager mode. It is useful comparing to &lt;a href=&quot;../../executing_eagerly&quot;&gt;&lt;code&gt;tf.executing_eagerly()&lt;/code&gt;&lt;/a&gt;, which checks the current context and will return &lt;code&gt;False&lt;/code&gt; within a &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; body. It can be used to build library that behave differently in eager runtime and v1 session runtime (deprecated).</source>
          <target state="translated">此功能将检查程序的最外部上下文，并查看它是否处于急切模式。与&lt;a href=&quot;../../executing_eagerly&quot;&gt; &lt;code&gt;tf.executing_eagerly()&lt;/code&gt; &lt;/a&gt;相比，它很有用，后者检查当前上下文并在&lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;主体中返回 &lt;code&gt;False&lt;/code&gt; 。它可以用来构建在急切的运行时和v1会话运行时中行为不同的库（不建议使用）。</target>
        </trans-unit>
        <trans-unit id="1b809c8513edcf65ff2e661e67bc392f776c9e7e" translate="yes" xml:space="preserve">
          <source>This function will create the global generator the first time it is called, and the generator will be placed at the default device at that time, so one needs to be careful when this function is first called. Using a generator placed on a less-ideal device will incur performance regression.</source>
          <target state="translated">这个函数第一次调用时会创建全局生成器,此时生成器会放置在默认设备上,所以第一次调用这个函数时需要注意。使用放置在不太理想的设备上的生成器会招致性能退步。</target>
        </trans-unit>
        <trans-unit id="89bde45a9d6e8e699bac43f2a09b2f06fe6c3d12" translate="yes" xml:space="preserve">
          <source>This function will modify the tensors passed in as it adds more operations and hence changing the consumers of the operations of the input tensors.</source>
          <target state="translated">这个函数将修改传入的时标,因为它增加了更多的操作,从而改变了输入时标的操作消费者。</target>
        </trans-unit>
        <trans-unit id="e4e12e3274ef470af1606308b5afb5383f0e4373" translate="yes" xml:space="preserve">
          <source>This function works on either a single image (&lt;code&gt;image&lt;/code&gt; is a 3-D Tensor), or a batch of images (&lt;code&gt;image&lt;/code&gt; is a 4-D Tensor).</source>
          <target state="translated">此功能适用于单个图像（ &lt;code&gt;image&lt;/code&gt; 是3-D张量）或一批图像（ &lt;code&gt;image&lt;/code&gt; 是4-D张量）。</target>
        </trans-unit>
        <trans-unit id="3bc4d91f81e39f3ea54e11412256f8f82c2a442a" translate="yes" xml:space="preserve">
          <source>This function wraps tensor placeholders in a supervised_receiver_fn with the expectation that the features and labels appear precisely as the model_fn expects them. Features and labels can therefore be dicts of tensors, or raw tensors.</source>
          <target state="translated">这个函数将张量占位符包装在supervised_receiver_fn中,期望特征和标签的出现与model_fn的期望一致。因此,特征和标签可以是张量的描述符,或者是原始张量。</target>
        </trans-unit>
        <trans-unit id="390f51be88bc027febcd4f2fc680ab5ff0ecb909" translate="yes" xml:space="preserve">
          <source>This functionality can be used to remap both row vocabularies (typically, features) and column vocabularies (typically, classes) from TensorFlow checkpoints. Note that the partitioning logic relies on contiguous vocabularies corresponding to div-partitioned variables. Moreover, the underlying remapping uses an IndexTable (as opposed to an inexact CuckooTable), so client code should use the corresponding index_table_from_file() as the FeatureColumn framework does (as opposed to tf.feature_to_id(), which uses a CuckooTable).</source>
          <target state="translated">这个功能可以用来重映射TensorFlow检查点的行词汇表(通常是特征)和列词汇表(通常是类)。需要注意的是,分区逻辑依赖于对应于div-partitioned变量的连续词汇表。此外,底层的重映射使用的是IndexTable(而不是不精确的CuckooTable),所以客户端代码应该像FeatureColumn框架一样使用相应的index_table_from_file()(而不是使用CuckooTable的tf.feature_to_id())。</target>
        </trans-unit>
        <trans-unit id="061aef760e659c96c1579128f6e2913f90a85e0a" translate="yes" xml:space="preserve">
          <source>This has the effect of transforming sliding window operations into the corresponding &quot;atrous&quot; operation in which the input is sampled at the specified &lt;code&gt;dilation_rate&lt;/code&gt;.</source>
          <target state="translated">这具有将滑动窗口操作转换为相应的&amp;ldquo;无用&amp;rdquo;操作的效果，其中以指定的 &lt;code&gt;dilation_rate&lt;/code&gt; 采样输入。</target>
        </trans-unit>
        <trans-unit id="10c41b6251ef0bcbec27e5d4c8ebb2292ccebcaa" translate="yes" xml:space="preserve">
          <source>This helper method provides a higher-level alternative to using &lt;code&gt;tf.contrib.summary.summary_writer_initializer_op&lt;/code&gt; and &lt;code&gt;tf.contrib.summary.graph&lt;/code&gt;.</source>
          <target state="translated">此辅助方法提供了使用 &lt;code&gt;tf.contrib.summary.summary_writer_initializer_op&lt;/code&gt; 和 &lt;code&gt;tf.contrib.summary.graph&lt;/code&gt; 的更高层次的替代方法。</target>
        </trans-unit>
        <trans-unit id="671c9eb768ee498e26815ceb625e7ee7c309f7e7" translate="yes" xml:space="preserve">
          <source>This hook delays execution until global step reaches to &lt;code&gt;wait_until_step&lt;/code&gt;. It is used to gradually start workers in distributed settings. One example usage would be setting &lt;code&gt;wait_until_step=int(K*log(task_id+1))&lt;/code&gt; assuming that task_id=0 is the chief.</source>
          <target state="translated">该钩子将延迟执行，直到全局步长达到 &lt;code&gt;wait_until_step&lt;/code&gt; 为止。它用于逐步启动分布式环境中的工作程序。一种示例用法是，假设task_id = 0为主要负责人，设置 &lt;code&gt;wait_until_step=int(K*log(task_id+1))&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1a0fa0f63117915fa2c817fa42b4c67a4f5c8b33" translate="yes" xml:space="preserve">
          <source>This hook requests stop after either a number of steps have been executed or a last step has been reached. Only one of the two options can be specified.</source>
          <target state="translated">这个钩子要求在执行了若干步骤或达到最后一步后停止。两个选项中只能指定一个。</target>
        </trans-unit>
        <trans-unit id="29326e19d0dc3f4c325692b2b9f948f424da0e4f" translate="yes" xml:space="preserve">
          <source>This hook saves the state of the iterators in the &lt;code&gt;Graph&lt;/code&gt; so that when training is resumed the input pipeline continues from where it left off. This could potentially avoid overfitting in certain pipelines where the number of training steps per eval are small compared to the dataset size or if the training pipeline is pre-empted.</source>
          <target state="translated">该挂钩将迭代器的状态保存在 &lt;code&gt;Graph&lt;/code&gt; 中,以便在恢复训练后，输入管道从中断处继续。这可以潜在地避免在某些管道中过拟合，在某些管道中，每个评估的训练步骤数量比数据集大小少，或者如果训练管道被抢占。</target>
        </trans-unit>
        <trans-unit id="362202ba29e8d6beed9c2059ae03ede13f3cd76f" translate="yes" xml:space="preserve">
          <source>This hook should be used if the input pipeline state needs to be saved separate from the model checkpoint. Doing so may be useful for a few reasons:</source>
          <target state="translated">如果需要将输入管道状态与模型检查点分开保存,则应使用该钩子。这样做可能有几个原因。</target>
        </trans-unit>
        <trans-unit id="b6ae1f6a6bb162ce4153046b8500ec1c1479c758" translate="yes" xml:space="preserve">
          <source>This identifies the replica that is part of a sync group. Currently we assume that all sync groups contain the same number of replicas. The value of the replica id can range from 0 to &lt;code&gt;num_replica_in_sync&lt;/code&gt; - 1.</source>
          <target state="translated">这标识了属于同步组的副本。当前，我们假定所有同步组都包含相同数量的副本。副本ID的值的范围为0到 &lt;code&gt;num_replica_in_sync&lt;/code&gt; -1。</target>
        </trans-unit>
        <trans-unit id="1b29036c26df39b1ea3c52cbf28f86f795fcf9f0" translate="yes" xml:space="preserve">
          <source>This implementation is based off of the Cephes math library.</source>
          <target state="translated">这个实现是基于Cephes数学库的。</target>
        </trans-unit>
        <trans-unit id="3f20df5d52794499c7d70f58fc604ea383aee58e" translate="yes" xml:space="preserve">
          <source>This implementation is to be used in conjunction of BlockLSTMV2.</source>
          <target state="translated">本实施例将与BlockLSTMV2配合使用。</target>
        </trans-unit>
        <trans-unit id="e4e20691c6271c3052db6329ff09bab3c3926f92" translate="yes" xml:space="preserve">
          <source>This implementation is to be used in conjunction of LSTMBlock.</source>
          <target state="translated">本实施例将与LSTMBlock配合使用。</target>
        </trans-unit>
        <trans-unit id="2023d65f880c26efdc064ba734473728a5acce91" translate="yes" xml:space="preserve">
          <source>This implementation is to be used in conjunction of LSTMBlockCell.</source>
          <target state="translated">本实施例将与LSTMBlockCell配合使用。</target>
        </trans-unit>
        <trans-unit id="ecd5efd797cabcd540fdce983afef271dcb8292a" translate="yes" xml:space="preserve">
          <source>This implementation of RMSprop uses plain momentum, not Nesterov momentum.</source>
          <target state="translated">RMSprop的这个实现使用的是普通动量,而不是Nesterov动量。</target>
        </trans-unit>
        <trans-unit id="32650114feea17cac9bd97ba7f3acc193158dc56" translate="yes" xml:space="preserve">
          <source>This implementation uses 1 weight matrix and 1 bias vector, and there's an optional peephole connection.</source>
          <target state="translated">本实施例使用1个权重矩阵和1个偏置向量,还有一个可选的窥视孔连接。</target>
        </trans-unit>
        <trans-unit id="69398f914b35c5353b93476810576462db6eec45" translate="yes" xml:space="preserve">
          <source>This implements the anisotropic 2-D version of the formula described here:</source>
          <target state="translated">这实现了这里所描述的各向异性2-D版本的公式。</target>
        </trans-unit>
        <trans-unit id="cd31200075bd4a5cc039e35e154eab86539a7999" translate="yes" xml:space="preserve">
          <source>This improves training speed. Please see &lt;code&gt;optimization_parameters.proto&lt;/code&gt; for details.</source>
          <target state="translated">这样可以提高训练速度。有关详细信息，请参见 &lt;code&gt;optimization_parameters.proto&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="eaa40933c0c7c31f3c54363539d17de16903a6ff" translate="yes" xml:space="preserve">
          <source>This includes ops from TF 2.0 tf.summary and TF 1.x tf.contrib.summary (except for &lt;code&gt;tf.contrib.summary.graph&lt;/code&gt; and &lt;code&gt;tf.contrib.summary.import_event&lt;/code&gt;), but does &lt;em&gt;not&lt;/em&gt; include TF 1.x tf.summary ops.</source>
          <target state="translated">这包括来自TF 2.0 tf.summary和TF的1.x tf.contrib.summary（除了OPS &lt;code&gt;tf.contrib.summary.graph&lt;/code&gt; 和 &lt;code&gt;tf.contrib.summary.import_event&lt;/code&gt; ），但并&lt;em&gt;不&lt;/em&gt;包括TF的1.x tf.summary行动。</target>
        </trans-unit>
        <trans-unit id="2cbffbf5b87e7d08406cd7f76d2f7936c22b5d07" translate="yes" xml:space="preserve">
          <source>This includes the operations to synchronize replicas: aggregate gradients, apply to variables, increment global step, insert tokens to token queue.</source>
          <target state="translated">这包括同步复制的操作:聚合梯度、应用于变量、增量全局步长、将代币插入代币队列。</target>
        </trans-unit>
        <trans-unit id="dc77baa2dba67576cde30b92064b3afa6abe84f1" translate="yes" xml:space="preserve">
          <source>This induces quasi-linear speedup on up to 8 GPUs.</source>
          <target state="translated">这在多达8个GPU上引起准线性加速。</target>
        </trans-unit>
        <trans-unit id="9e91bcd2748922b865fbdb3c28f16a3f92c87b0e" translate="yes" xml:space="preserve">
          <source>This initializer assigns one entry in the table for each line in the file.</source>
          <target state="translated">这个初始化器为文件中的每一行分配一个表项。</target>
        </trans-unit>
        <trans-unit id="93a30392533a2023857f78a57a8100de69db56b7" translate="yes" xml:space="preserve">
          <source>This initializes a new Kubernetes ClusterResolver. The ClusterResolver will attempt to talk to the Kubernetes master to retrieve all the instances of pods matching a label selector.</source>
          <target state="translated">这将初始化一个新的Kubernetes ClusterResolver。ClusterResolver将尝试与Kubernetes主站对话,以检索与标签选择器相匹配的pods的所有实例。</target>
        </trans-unit>
        <trans-unit id="ded15c2fc6dc030fcd9627bdc75c00460c2cb65d" translate="yes" xml:space="preserve">
          <source>This is (mostly) a special case of &lt;a href=&quot;../math/add&quot;&gt;&lt;code&gt;tf.add&lt;/code&gt;&lt;/a&gt; where &lt;code&gt;bias&lt;/code&gt; is restricted to 1-D. Broadcasting is supported, so &lt;code&gt;value&lt;/code&gt; may have any number of dimensions. Unlike &lt;a href=&quot;../math/add&quot;&gt;&lt;code&gt;tf.add&lt;/code&gt;&lt;/a&gt;, the type of &lt;code&gt;bias&lt;/code&gt; is allowed to differ from &lt;code&gt;value&lt;/code&gt; in the case where both types are quantized.</source>
          <target state="translated">这（多数情况下）是&lt;a href=&quot;../math/add&quot;&gt; &lt;code&gt;tf.add&lt;/code&gt; 的&lt;/a&gt;一种特殊情况，其中 &lt;code&gt;bias&lt;/code&gt; 仅限于1-D。支持广播，因此 &lt;code&gt;value&lt;/code&gt; 可以具有任意多个维度。与&lt;a href=&quot;../math/add&quot;&gt; &lt;code&gt;tf.add&lt;/code&gt; &lt;/a&gt;不同，在对两种类型进行量化的情况下，允许 &lt;code&gt;bias&lt;/code&gt; 的类型与 &lt;code&gt;value&lt;/code&gt; 不同。</target>
        </trans-unit>
        <trans-unit id="9f67d123191520f4a8994999dc8cd88f7fe320b7" translate="yes" xml:space="preserve">
          <source>This is EXPERIMENTAL and subject to change.</source>
          <target state="translated">这是实验性的,可能会有变化。</target>
        </trans-unit>
        <trans-unit id="02f8d625292f24a8f6fa6d8a843ae5c003159e97" translate="yes" xml:space="preserve">
          <source>This is a class method that describes what key/value arguments are required to instantiate the given &lt;code&gt;Distribution&lt;/code&gt; so that a particular shape is returned for that instance's call to &lt;code&gt;sample()&lt;/code&gt;.</source>
          <target state="translated">这是一个类方法，描述了实例化给定 &lt;code&gt;Distribution&lt;/code&gt; 所需的键/值参数，以便为该实例对 &lt;code&gt;sample()&lt;/code&gt; 的调用返回特定的形状。</target>
        </trans-unit>
        <trans-unit id="012a7836bd88d1e3c2c595506564d4812be83ba5" translate="yes" xml:space="preserve">
          <source>This is a class method that describes what key/value arguments are required to instantiate the given &lt;code&gt;Distribution&lt;/code&gt; so that a particular shape is returned for that instance's call to &lt;code&gt;sample()&lt;/code&gt;. Assumes that the sample's shape is known statically.</source>
          <target state="translated">这是一个类方法，描述了实例化给定 &lt;code&gt;Distribution&lt;/code&gt; 所需的键/值参数，以便为该实例对 &lt;code&gt;sample()&lt;/code&gt; 的调用返回特定的形状。假定样品的形状是静态已知的。</target>
        </trans-unit>
        <trans-unit id="fead40a760af2fb3753f3e33415c41be437e055a" translate="yes" xml:space="preserve">
          <source>This is a companion method to &lt;code&gt;add_queue_runner()&lt;/code&gt;. It just starts threads for all queue runners collected in the graph. It returns the list of all threads.</source>
          <target state="translated">这是 &lt;code&gt;add_queue_runner()&lt;/code&gt; 的辅助方法。它只是为图中收集的所有队列运行器启动线程。它返回所有线程的列表。</target>
        </trans-unit>
        <trans-unit id="9590afd024cc456ab84ee4d2f95cabc25b5ebe3a" translate="yes" xml:space="preserve">
          <source>This is a context class that is passed to the &lt;code&gt;value_fn&lt;/code&gt; in &lt;code&gt;strategy.experimental_distribute_values_from_function&lt;/code&gt; and contains information about the compute replicas. The &lt;code&gt;num_replicas_in_sync&lt;/code&gt; and &lt;code&gt;replica_id&lt;/code&gt; can be used to customize the value on each replica.</source>
          <target state="translated">这是传递给一个上下文类 &lt;code&gt;value_fn&lt;/code&gt; 在 &lt;code&gt;strategy.experimental_distribute_values_from_function&lt;/code&gt; ，包含有关计算副本的信息。的 &lt;code&gt;num_replicas_in_sync&lt;/code&gt; 和 &lt;code&gt;replica_id&lt;/code&gt; 可用于自定义在每个副本的值。</target>
        </trans-unit>
        <trans-unit id="309ae6e7b107ea04158b03fa5671cc2764a5cdf7" translate="yes" xml:space="preserve">
          <source>This is a context class that is passed to the user's input function and contains information about the compute replicas and input pipelines. The number of compute replicas (in sync training) helps compute the local batch size from the desired global batch size for each replica. The input pipeline information can be used to return a different subset of the input in each replica (for e.g. shard the input pipeline, use a different input source etc).</source>
          <target state="translated">这是一个传递给用户输入函数的上下文类,包含了计算副本和输入管道的信息。计算副本的数量(在同步训练中)有助于从每个副本的所需全局批次大小计算出本地批次大小。输入管道信息可用于在每个副本中返回不同的输入子集(如对输入管道进行碎片化处理,使用不同的输入源等)。</target>
        </trans-unit>
        <trans-unit id="1659a3286a3bedb8cf721b60523b4bfde4d51caf" translate="yes" xml:space="preserve">
          <source>This is a convenience method that converts RGB images to float representation, adjusts their brightness, and then converts them back to the original data type. If several adjustments are chained, it is advisable to minimize the number of redundant conversions.</source>
          <target state="translated">这是一种方便的方法,它将RGB图像转换为浮动表示,调整其亮度,然后将其转换回原始数据类型。如果几个调整是连锁的,建议尽量减少多余的转换次数。</target>
        </trans-unit>
        <trans-unit id="7bcc711030b8e6b303b05fbb2eadad7a51295960" translate="yes" xml:space="preserve">
          <source>This is a convenience method that converts RGB images to float representation, adjusts their contrast, and then converts them back to the original data type. If several adjustments are chained, it is advisable to minimize the number of redundant conversions.</source>
          <target state="translated">这是一种方便的方法,它将RGB图像转换为浮动表示,调整其对比度,然后将其转换回原始数据类型。如果几个调整是连锁的,建议尽量减少多余的转换次数。</target>
        </trans-unit>
        <trans-unit id="689582a22623e8b58ba6e0ed65682c48dc03d8f0" translate="yes" xml:space="preserve">
          <source>This is a convenience method that converts RGB images to float representation, converts them to HSV, add an offset to the saturation channel, converts back to RGB and then back to the original data type. If several adjustments are chained it is advisable to minimize the number of redundant conversions.</source>
          <target state="translated">这是一种方便的方法,它将RGB图像转换为浮动表示,将其转换为HSV,在饱和度通道上添加一个偏移,转换回RGB,然后回到原始数据类型。如果几个调整是连锁的,建议尽量减少多余的转换次数。</target>
        </trans-unit>
        <trans-unit id="154fc7928cd01964ea2a05b16a1fbc2cadc62c3b" translate="yes" xml:space="preserve">
          <source>This is a convenience method that converts RGB images to float representation, converts them to HSV, adds an offset to the saturation channel, converts back to RGB and then back to the original data type. If several adjustments are chained it is advisable to minimize the number of redundant conversions.</source>
          <target state="translated">这是一种方便的方法,它将RGB图像转换为浮动表示,将其转换为HSV,为饱和度通道添加一个偏移,转换回RGB,然后回到原始数据类型。如果几个调整是连锁的,建议尽量减少冗余转换的次数。</target>
        </trans-unit>
        <trans-unit id="157983bd7b4bf865fccf95ec9e2bc8ddd381392d" translate="yes" xml:space="preserve">
          <source>This is a convenience method that converts an RGB image to float representation, converts it to HSV, add an offset to the hue channel, converts back to RGB and then back to the original data type. If several adjustments are chained it is advisable to minimize the number of redundant conversions.</source>
          <target state="translated">这是一种方便的方法,它将RGB图像转换为浮动表示,将其转换为HSV,为色调通道添加一个偏移,转换回RGB,然后回到原始数据类型。如果几个调整是连锁的,建议尽量减少多余的转换次数。</target>
        </trans-unit>
        <trans-unit id="bc1addde10acbe587836014417c1c886ecf1e9cf" translate="yes" xml:space="preserve">
          <source>This is a convenience method that converts an RGB image to float representation, converts it to HSV, adds an offset to the hue channel, converts back to RGB and then back to the original data type. If several adjustments are chained it is advisable to minimize the number of redundant conversions.</source>
          <target state="translated">这是一种方便的方法,它将RGB图像转换为浮动表示,将其转换为HSV,为色调通道添加一个偏移,转换回RGB,然后回到原始数据类型。如果几个调整是连锁的,建议尽量减少多余的转换次数。</target>
        </trans-unit>
        <trans-unit id="8b85cb6fb2b17c1cfe1f8becbb4b4eeadf576adb" translate="yes" xml:space="preserve">
          <source>This is a convenience method that converts an image to uint8 representation, encodes it to jpeg with &lt;code&gt;jpeg_quality&lt;/code&gt;, decodes it, and then converts back to the original data type.</source>
          <target state="translated">这是一种方便的方法，可以将图像转换为uint8表示形式，使用 &lt;code&gt;jpeg_quality&lt;/code&gt; 将其编码为jpeg，对其进行解码，然后再转换回原始数据类型。</target>
        </trans-unit>
        <trans-unit id="040dbe98920d5a5c1090b8783413f490c3abe113" translate="yes" xml:space="preserve">
          <source>This is a convenience utility for packing data into the tuple formats that &lt;a href=&quot;../model#fit&quot;&gt;&lt;code&gt;Model.fit&lt;/code&gt;&lt;/a&gt; uses.</source>
          <target state="translated">这是一个方便实用的实用程序，用于将数据打包为&lt;a href=&quot;../model#fit&quot;&gt; &lt;code&gt;Model.fit&lt;/code&gt; &lt;/a&gt;使用的元组格式。</target>
        </trans-unit>
        <trans-unit id="177a96552c22761995fc2a02f86657fdf66dd73d" translate="yes" xml:space="preserve">
          <source>This is a convenience utility to be used when overriding &lt;a href=&quot;../model#train_step&quot;&gt;&lt;code&gt;Model.train_step&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../model#test_step&quot;&gt;&lt;code&gt;Model.test_step&lt;/code&gt;&lt;/a&gt;, or &lt;a href=&quot;../model#predict_step&quot;&gt;&lt;code&gt;Model.predict_step&lt;/code&gt;&lt;/a&gt;. This utility makes it easy to support data of the form &lt;code&gt;(x,)&lt;/code&gt;, &lt;code&gt;(x, y)&lt;/code&gt;, or &lt;code&gt;(x, y, sample_weight)&lt;/code&gt;.</source>
          <target state="translated">这是覆盖&lt;a href=&quot;../model#train_step&quot;&gt; &lt;code&gt;Model.train_step&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;../model#test_step&quot;&gt; &lt;code&gt;Model.test_step&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;../model#predict_step&quot;&gt; &lt;code&gt;Model.predict_step&lt;/code&gt; &lt;/a&gt;时使用的便捷实用程序。使用该实用程序可以轻松支持 &lt;code&gt;(x,)&lt;/code&gt; ， &lt;code&gt;(x, y)&lt;/code&gt; 或 &lt;code&gt;(x, y, sample_weight)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c0b259f9b8e5db6abf1f7ca2cc64744a095dde1b" translate="yes" xml:space="preserve">
          <source>This is a dataset of 11,228 newswires from Reuters, labeled over 46 topics.</source>
          <target state="translated">这是路透社11,228条新闻的数据集,标注了46个主题。</target>
        </trans-unit>
        <trans-unit id="768eb884c8ee5b48b0c147f90ce4407a6a9884da" translate="yes" xml:space="preserve">
          <source>This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer &quot;3&quot; encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: &quot;only consider the top 10,000 most common words, but eliminate the top 20 most common words&quot;.</source>
          <target state="translated">这是一个来自IMDB的25,000条电影评论的数据集,按情绪(正面/负面)进行标注。评论已被预处理,每条评论被编码为一个词索引列表(整数)。为方便起见,单词是按数据集中的整体频率进行索引的,因此,例如整数 &quot;3 &quot;编码的是数据中第3个最频繁的单词。这样就可以进行快速的过滤操作,比如。&quot;只考虑前10000个最常见的词,但去掉前20个最常见的词&quot;。</target>
        </trans-unit>
        <trans-unit id="e86bf0041eb6fb31b4e9ede482e55b7d43a98c80" translate="yes" xml:space="preserve">
          <source>This is a dataset of 50,000 32x32 color training images and 10,000 test images, labeled over 10 categories. See more info at the &lt;a href=&quot;https://www.cs.toronto.edu/%7Ekriz/cifar.html&quot;&gt;CIFAR homepage&lt;/a&gt;.</source>
          <target state="translated">这是50,000个32x32彩色训练图像和10,000个测试图像的数据集，标记了10个类别。在&lt;a href=&quot;https://www.cs.toronto.edu/%7Ekriz/cifar.html&quot;&gt;CIFAR主页上&lt;/a&gt;查看更多信息。</target>
        </trans-unit>
        <trans-unit id="435f80948690367d77e0f3784d30df76f7f3d98a" translate="yes" xml:space="preserve">
          <source>This is a dataset of 50,000 32x32 color training images and 10,000 test images, labeled over 100 fine-grained classes that are grouped into 20 coarse-grained classes. See more info at the &lt;a href=&quot;https://www.cs.toronto.edu/%7Ekriz/cifar.html&quot;&gt;CIFAR homepage&lt;/a&gt;.</source>
          <target state="translated">这是一个50,000个32x32彩色训练图像和10,000个测试图像的数据集，标记了100多个细粒度类别，这些细粒度类别被分组为20个粗粒度类别。在&lt;a href=&quot;https://www.cs.toronto.edu/%7Ekriz/cifar.html&quot;&gt;CIFAR主页上&lt;/a&gt;查看更多信息。</target>
        </trans-unit>
        <trans-unit id="33a8fb4dd54ee096f6f9e8b6100e0ded183914c6" translate="yes" xml:space="preserve">
          <source>This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The class labels are:</source>
          <target state="translated">这是一个包含10个时尚类别的60,000张28x28灰度图像的数据集,以及一个包含10,000张图像的测试集。该数据集可作为MNIST的替代产品。类别标签是:</target>
        </trans-unit>
        <trans-unit id="e30665bff921b2720a3a63831af563bf188d5b46" translate="yes" xml:space="preserve">
          <source>This is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. More info can be found at the &lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot;&gt;MNIST homepage&lt;/a&gt;.</source>
          <target state="translated">这是一个包含10位数的60,000张28x28灰度图像的数据集，以及10,000张图像的测试集。可以在&lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot;&gt;MNIST主页上&lt;/a&gt;找到更多信息。</target>
        </trans-unit>
        <trans-unit id="710ac221d7447de49c2c2452f75596e4bb685c1c" translate="yes" xml:space="preserve">
          <source>This is a dataset taken from the StatLib library which is maintained at Carnegie Mellon University.</source>
          <target state="translated">这是从卡耐基梅隆大学维护的StatLib库中提取的数据集。</target>
        </trans-unit>
        <trans-unit id="ab28ad9476b59e4ae1d47c731d1e80592c84261d" translate="yes" xml:space="preserve">
          <source>This is a deprecated version of &lt;code&gt;fractional_avg_pool&lt;/code&gt;.</source>
          <target state="translated">这是 &lt;code&gt;fractional_avg_pool&lt;/code&gt; 的已弃用版本。</target>
        </trans-unit>
        <trans-unit id="89fac60f109fd0365fd768e5b0c1efda980aac29" translate="yes" xml:space="preserve">
          <source>This is a deprecated version of &lt;code&gt;fractional_max_pool&lt;/code&gt;.</source>
          <target state="translated">这是 &lt;code&gt;fractional_max_pool&lt;/code&gt; 的不推荐使用的版本。</target>
        </trans-unit>
        <trans-unit id="ec0b5065eeff136feb43dbfad07e272612b79d8c" translate="yes" xml:space="preserve">
          <source>This is a deprecated version of BiasAdd and will be soon removed.</source>
          <target state="translated">这是BiasAdd的一个过时版本,很快就会被删除。</target>
        </trans-unit>
        <trans-unit id="066f7ab7935722a8bd3e79702ee438f7b2aad916" translate="yes" xml:space="preserve">
          <source>This is a difference between DatasetV1 and DatasetV2. DatasetV1 does not take anything in its constructor whereas in the DatasetV2, we expect subclasses to create a variant_tensor and pass it in to the super() call.</source>
          <target state="translated">这是DatasetV1和DatasetV2的区别。DatasetV1的构造函数中不包含任何内容,而在DatasetV2中,我们希望子类能够创建一个variant_tensor,并将其传递给super()调用。</target>
        </trans-unit>
        <trans-unit id="e7c85e6a460e7ff32a9089528b0d0f38042adaa0" translate="yes" xml:space="preserve">
          <source>This is a faster way to train a softmax classifier over a huge number of classes.</source>
          <target state="translated">这是一种在大量类上训练softmax分类器的较快方法。</target>
        </trans-unit>
        <trans-unit id="d7e11ee2edab133ca8b8374faafd78e7821ed539" translate="yes" xml:space="preserve">
          <source>This is a legacy behaviour of TensorFlow and is highly discouraged.</source>
          <target state="translated">这是TensorFlow的遗留行为,是非常不鼓励的。</target>
        </trans-unit>
        <trans-unit id="a3be19c517fb6a6f6742dcaed9be8bb79e5a6a4d" translate="yes" xml:space="preserve">
          <source>This is a legacy version of the more general BatchToSpaceND.</source>
          <target state="translated">这是更通用的BatchToSpaceND的遗留版本。</target>
        </trans-unit>
        <trans-unit id="c2584be3bd38ee90f3ac17e87b40fd5fb1908bdb" translate="yes" xml:space="preserve">
          <source>This is a legacy version of the more general SpaceToBatchND.</source>
          <target state="translated">这是更通用的SpaceToBatchND的遗留版本。</target>
        </trans-unit>
        <trans-unit id="c0ae3cb7ef651d4c8fb957d4b4823d4c0713533f" translate="yes" xml:space="preserve">
          <source>This is a low-level interface for creating an &lt;code&gt;Operation&lt;/code&gt;. Most programs will not call this method directly, and instead use the Python op constructors, such as &lt;a href=&quot;constant&quot;&gt;&lt;code&gt;tf.constant()&lt;/code&gt;&lt;/a&gt;, which add ops to the default graph.</source>
          <target state="translated">这是用于创建 &lt;code&gt;Operation&lt;/code&gt; 的底层接口。大多数程序不会直接调用此方法，而是使用Python op构造函数，例如&lt;a href=&quot;constant&quot;&gt; &lt;code&gt;tf.constant()&lt;/code&gt; &lt;/a&gt;，它们将ops添加到默认图形。</target>
        </trans-unit>
        <trans-unit id="1cd850dc655362d680df73debd3d92a42edd1883" translate="yes" xml:space="preserve">
          <source>This is a method that implementers of subclasses of &lt;code&gt;Layer&lt;/code&gt; or &lt;code&gt;Model&lt;/code&gt; can override if they need a state-creation step in-between layer instantiation and layer call.</source>
          <target state="translated">如果 &lt;code&gt;Layer&lt;/code&gt; 或 &lt;code&gt;Model&lt;/code&gt; 的子类的实现者需要在层实例化和层调用之间进行状态创建步骤，则可以重写此方法。</target>
        </trans-unit>
        <trans-unit id="62c4042c7b665a4296b8d1a943304048eb64aec7" translate="yes" xml:space="preserve">
          <source>This is a nonzero integer. See the get_ident() function. Thread identifiers may be recycled when a thread exits and another thread is created. The identifier is available even after the thread has exited.</source>
          <target state="translated">这是一个非零的整数。参见get_ident()函数。当一个线程退出并创建另一个线程时,线程标识符可以被回收。即使在线程退出后,该标识符仍可使用。</target>
        </trans-unit>
        <trans-unit id="31eb573dcb4f05e127842ce7a2c523b4f9fd2e61" translate="yes" xml:space="preserve">
          <source>This is a reduction created for Nvidia DGX-1 which assumes GPUs connects like that on DGX-1 machine. If you have different GPU inter-connections, it is likely that it would be slower than &lt;a href=&quot;reductiontoonedevice&quot;&gt;&lt;code&gt;tf.distribute.ReductionToOneDevice&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">这是为Nvidia DGX-1创建的缩减，它假定GPU的连接方式与DGX-1机器上的类似。如果您具有不同的GPU互连，则它可能会比&lt;a href=&quot;reductiontoonedevice&quot;&gt; &lt;code&gt;tf.distribute.ReductionToOneDevice&lt;/code&gt; &lt;/a&gt;慢。</target>
        </trans-unit>
        <trans-unit id="85f7e8c05b61af3e5451f6e510d0b0985383c1d8" translate="yes" xml:space="preserve">
          <source>This is a special case of &lt;a href=&quot;../math/add&quot;&gt;&lt;code&gt;tf.add&lt;/code&gt;&lt;/a&gt; where &lt;code&gt;bias&lt;/code&gt; is restricted to be 1-D. Broadcasting is supported, so &lt;code&gt;value&lt;/code&gt; may have any number of dimensions.</source>
          <target state="translated">这是&lt;a href=&quot;../math/add&quot;&gt; &lt;code&gt;tf.add&lt;/code&gt; &lt;/a&gt;的特殊情况，其中 &lt;code&gt;bias&lt;/code&gt; 限制为1-D。支持广播，因此 &lt;code&gt;value&lt;/code&gt; 可以具有任意多个维度。</target>
        </trans-unit>
        <trans-unit id="cbdbb1332709a943bc7d0db1d68caf76c3a99f69" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;a href=&quot;../../../random/categorical&quot;&gt;&lt;code&gt;tf.random.categorical&lt;/code&gt;&lt;/a&gt;: if run twice with the same seeds and shapes, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="translated">这是&lt;a href=&quot;../../../random/categorical&quot;&gt; &lt;code&gt;tf.random.categorical&lt;/code&gt; &lt;/a&gt;的无状态版本：如果使用相同的种子和形状运行两次，它将产生相同的伪随机数。在同一硬件上（以及CPU和GPU之间）的多次运行中，输出是一致的，但在TensorFlow的版本之间或在非CPU / GPU硬件上可能会有所变化。</target>
        </trans-unit>
        <trans-unit id="81cb44c6cca42a31a07fbae208fd1a7a56619cfa" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;a href=&quot;../../../random/categorical&quot;&gt;&lt;code&gt;tf.random.categorical&lt;/code&gt;&lt;/a&gt;: if run twice with the same seeds, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="translated">这是&lt;a href=&quot;../../../random/categorical&quot;&gt; &lt;code&gt;tf.random.categorical&lt;/code&gt; &lt;/a&gt;的无状态版本：如果使用相同的种子运行两次，它将产生相同的伪随机数。在同一硬件上（以及CPU和GPU之间）的多次运行中，输出是一致的，但在TensorFlow的版本之间或在非CPU / GPU硬件上可能会有所变化。</target>
        </trans-unit>
        <trans-unit id="de68a47b12e07130e567ec24bb61927f4f535092" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;a href=&quot;gamma&quot;&gt;&lt;code&gt;tf.random.gamma&lt;/code&gt;&lt;/a&gt;: if run twice with the same seeds and shapes, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="translated">这是&lt;a href=&quot;gamma&quot;&gt; &lt;code&gt;tf.random.gamma&lt;/code&gt; &lt;/a&gt;的无状态版本：如果使用相同的种子和形状运行两次，它将产生相同的伪随机数。在同一硬件上（以及CPU和GPU之间）的多次运行中，输出是一致的，但是在TensorFlow的版本之间或在非CPU / GPU硬件上可能会有所变化。</target>
        </trans-unit>
        <trans-unit id="d527e38ee04e494f8d4e9909ef23824c02f218d2" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;a href=&quot;generator#binomial&quot;&gt;&lt;code&gt;tf.random.Generator.binomial&lt;/code&gt;&lt;/a&gt;: if run twice with the same seeds and shapes, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="translated">这是&lt;a href=&quot;generator#binomial&quot;&gt; &lt;code&gt;tf.random.Generator.binomial&lt;/code&gt; &lt;/a&gt;的无状态版本：如果使用相同的种子和形状运行两次，它将产生相同的伪随机数。在同一硬件上（以及CPU和GPU之间）的多次运行中，输出是一致的，但是在TensorFlow的版本之间或在非CPU / GPU硬件上可能会有所变化。</target>
        </trans-unit>
        <trans-unit id="1f9d7559f26ce2def191894981d83e7101b4893f" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;a href=&quot;normal&quot;&gt;&lt;code&gt;tf.random.normal&lt;/code&gt;&lt;/a&gt;: if run twice with the same seeds and shapes, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="translated">这是&lt;a href=&quot;normal&quot;&gt; &lt;code&gt;tf.random.normal&lt;/code&gt; &lt;/a&gt;的无状态版本：如果使用相同的种子和形状运行两次，它将产生相同的伪随机数。在同一硬件上（以及CPU和GPU之间）的多次运行中，输出是一致的，但是在TensorFlow的版本之间或在非CPU / GPU硬件上可能会有所变化。</target>
        </trans-unit>
        <trans-unit id="812a072511f22cc831859b50e28ff4fe62133693" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;a href=&quot;normal&quot;&gt;&lt;code&gt;tf.random.normal&lt;/code&gt;&lt;/a&gt;: if run twice with the same seeds, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="translated">这是&lt;a href=&quot;normal&quot;&gt; &lt;code&gt;tf.random.normal&lt;/code&gt; &lt;/a&gt;的无状态版本：如果使用相同的种子运行两次，它将产生相同的伪随机数。在同一硬件上（以及CPU和GPU之间）的多次运行中，输出是一致的，但在TensorFlow的版本之间或在非CPU / GPU硬件上可能会有所变化。</target>
        </trans-unit>
        <trans-unit id="eb02c3777bd174779389bb97e4e8d5e8436a288c" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;a href=&quot;poisson&quot;&gt;&lt;code&gt;tf.random.poisson&lt;/code&gt;&lt;/a&gt;: if run twice with the same seeds and shapes, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware, but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="translated">这是&lt;a href=&quot;poisson&quot;&gt; &lt;code&gt;tf.random.poisson&lt;/code&gt; &lt;/a&gt;的无状态版本：如果使用相同的种子和形状运行两次，它将产生相同的伪随机数。在同一硬件上的多次运行中，输出是一致的，但在TensorFlow的版本之间或在非CPU / GPU硬件上可能会有所变化。</target>
        </trans-unit>
        <trans-unit id="4ca51d036bc29518d8577d5bfc5e3dc7a0421db6" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;a href=&quot;truncated_normal&quot;&gt;&lt;code&gt;tf.random.truncated_normal&lt;/code&gt;&lt;/a&gt;: if run twice with the same seeds and shapes, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="translated">这是&lt;a href=&quot;truncated_normal&quot;&gt; &lt;code&gt;tf.random.truncated_normal&lt;/code&gt; &lt;/a&gt;的无状态版本：如果使用相同的种子和形状运行两次，它将产生相同的伪随机数。在同一硬件上（以及CPU和GPU之间）的多次运行中，输出是一致的，但是在TensorFlow的版本之间或在非CPU / GPU硬件上可能会有所变化。</target>
        </trans-unit>
        <trans-unit id="dd6096e0d7eff9f42170cc05a1fe61ba85be91f4" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;a href=&quot;truncated_normal&quot;&gt;&lt;code&gt;tf.random.truncated_normal&lt;/code&gt;&lt;/a&gt;: if run twice with the same seeds, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="translated">这是&lt;a href=&quot;truncated_normal&quot;&gt; &lt;code&gt;tf.random.truncated_normal&lt;/code&gt; &lt;/a&gt;的无状态版本：如果使用相同的种子运行两次，它将产生相同的伪随机数。在同一硬件上（以及CPU和GPU之间）的多次运行中，输出是一致的，但在TensorFlow的版本之间或在非CPU / GPU硬件上可能会有所变化。</target>
        </trans-unit>
        <trans-unit id="ac72baa117d9b4a5bdf610d359eb9848fb6723ce" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;a href=&quot;uniform&quot;&gt;&lt;code&gt;tf.random.uniform&lt;/code&gt;&lt;/a&gt;: if run twice with the same seeds and shapes, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="translated">这是&lt;a href=&quot;uniform&quot;&gt; &lt;code&gt;tf.random.uniform&lt;/code&gt; &lt;/a&gt;的无状态版本：如果使用相同的种子和形状运行两次，它将产生相同的伪随机数。在同一硬件上（以及CPU和GPU之间）的多次运行中，输出是一致的，但是在TensorFlow的版本之间或在非CPU / GPU硬件上可能会有所变化。</target>
        </trans-unit>
        <trans-unit id="7b120804ac2994699c2b37663ac0990fd63dfd70" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;a href=&quot;uniform&quot;&gt;&lt;code&gt;tf.random.uniform&lt;/code&gt;&lt;/a&gt;: if run twice with the same seeds, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="translated">这是&lt;a href=&quot;uniform&quot;&gt; &lt;code&gt;tf.random.uniform&lt;/code&gt; &lt;/a&gt;的无状态版本：如果使用相同的种子运行两次，它将产生相同的伪随机数。在同一硬件上（以及CPU和GPU之间）的多次运行中，输出是一致的，但在TensorFlow的版本之间或在非CPU / GPU硬件上可能会有所变化。</target>
        </trans-unit>
        <trans-unit id="93abfee5707aabda372725092df92726179f4164" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;code&gt;tf.categorical&lt;/code&gt;: if run twice with the same seeds and shapes, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="translated">这是 &lt;code&gt;tf.categorical&lt;/code&gt; 的无状态版本：如果使用相同的种子和形状运行两次，它将产生相同的伪随机数。在同一硬件上（以及CPU和GPU之间）的多次运行中，输出是一致的，但是在TensorFlow的版本之间或在非CPU / GPU硬件上可能会有所变化。</target>
        </trans-unit>
        <trans-unit id="ada11b1aaca11d67319d958bd7d1da13f15ebb7f" translate="yes" xml:space="preserve">
          <source>This is a stateless version of &lt;code&gt;tf.categorical&lt;/code&gt;: if run twice with the same seeds, it will produce the same pseudorandom numbers. The output is consistent across multiple runs on the same hardware (and between CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU hardware.</source>
          <target state="translated">这是 &lt;code&gt;tf.categorical&lt;/code&gt; 的无状态版本：如果使用相同的种子运行两次，它将产生相同的伪随机数。在同一硬件上（以及CPU和GPU之间）的多次运行中，输出是一致的，但在TensorFlow的版本之间或在非CPU / GPU硬件上可能会有所变化。</target>
        </trans-unit>
        <trans-unit id="9603abb8f073d177d05702e0bc2ac4de5a04a74c" translate="yes" xml:space="preserve">
          <source>This is a wrapper to the &lt;code&gt;hashing_trick&lt;/code&gt; function using &lt;code&gt;hash&lt;/code&gt; as the hashing function; unicity of word to index mapping non-guaranteed.</source>
          <target state="translated">这是使用 &lt;code&gt;hash&lt;/code&gt; 作为哈希函数的 &lt;code&gt;hashing_trick&lt;/code&gt; 函数的包装；不保证单词到索引映射的唯一性。</target>
        </trans-unit>
        <trans-unit id="5b00a32a0d99275c6a4a177506328b41e8a5cbe9" translate="yes" xml:space="preserve">
          <source>This is almost identical to QuantizeAndDequantizeV2, except that num_bits is a tensor, so its value can change during training.</source>
          <target state="translated">这与QuantizeAndDequantizeV2几乎相同,除了num_bits是一个张量,所以它的值可以在训练过程中改变。</target>
        </trans-unit>
        <trans-unit id="ac7a813777192ceaa721f12e2b167eefa5ce19d7" translate="yes" xml:space="preserve">
          <source>This is always checked statically, so this method returns nothing.</source>
          <target state="translated">这始终是静态检查,所以本方法不返回任何内容。</target>
        </trans-unit>
        <trans-unit id="edf37ccd5d1846dec567da6f67dc930901f7907c" translate="yes" xml:space="preserve">
          <source>This is an abstract base class, so you cannot instantiate it directly. Instead, use one of its concrete subclasses:</source>
          <target state="translated">这是一个抽象的基类,所以你不能直接实例化它。相反,使用它的一个具体子类。</target>
        </trans-unit>
        <trans-unit id="acb8330008d86d94ea39351a6992d0b396765e55" translate="yes" xml:space="preserve">
          <source>This is an abstract class which allows extensions to TensorFlow's object-based checkpointing (see &lt;a href=&quot;../checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt;). For example a wrapper for NumPy arrays:</source>
          <target state="translated">这是一个抽象类，它允许扩展TensorFlow的基于对象的检查点（请参阅&lt;a href=&quot;../checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt;）。例如，NumPy数组的包装器：</target>
        </trans-unit>
        <trans-unit id="f0c3e68e0358a0f76609938022081ba5ccf573dd" translate="yes" xml:space="preserve">
          <source>This is an experimental op for internal use only and it is possible to use this op in unsafe ways. DO NOT USE unless you fully understand the risks.</source>
          <target state="translated">这是一个仅供内部使用的实验性操作,有可能在不安全的情况下使用这个操作。除非您完全了解风险,否则请勿使用。</target>
        </trans-unit>
        <trans-unit id="8bbaa33b6f8b11a0355ba861cdf03d3e08622a92" translate="yes" xml:space="preserve">
          <source>This is an identity op (behaves like &lt;a href=&quot;../../identity&quot;&gt;&lt;code&gt;tf.identity&lt;/code&gt;&lt;/a&gt;) with the side effect of printing &lt;code&gt;data&lt;/code&gt; when evaluating.</source>
          <target state="translated">这是一个身份操作（行为如&lt;a href=&quot;../../identity&quot;&gt; &lt;code&gt;tf.identity&lt;/code&gt; &lt;/a&gt;），具有评估时打印 &lt;code&gt;data&lt;/code&gt; 的副作用。</target>
        </trans-unit>
        <trans-unit id="5c9b3651662523b2ca2e5016d6f25cb9aff58541" translate="yes" xml:space="preserve">
          <source>This is an implementation of ClusterResolver for Slurm clusters. This allows the specification of jobs and task counts, number of tasks per node, number of GPUs on each node and number of GPUs for each task. It retrieves system attributes by Slurm environment variables, resolves allocated computing node names, constructs a cluster and returns a ClusterResolver object which can be used for distributed TensorFlow.</source>
          <target state="translated">这是Slurm集群的ClusterResolver的一个实现。它允许指定工作和任务数量、每个节点的任务数量、每个节点上的GPU数量以及每个任务的GPU数量。它通过Slurm环境变量检索系统属性,解析分配的计算节点名称,构建集群并返回一个ClusterResolver对象,该对象可用于分布式TensorFlow。</target>
        </trans-unit>
        <trans-unit id="04be17d14e8556e45ddfd971016c00de8c4f5796" translate="yes" xml:space="preserve">
          <source>This is an implementation of cluster resolvers for Kubernetes. When given the the Kubernetes namespace and label selector for pods, we will retrieve the pod IP addresses of all running pods matching the selector, and return a ClusterSpec based on that information.</source>
          <target state="translated">这是Kubernetes的集群解析器的实现。当给定Kubernetes命名空间和pod的标签选择器时,我们将检索与选择器相匹配的所有运行中的pod的IP地址,并根据这些信息返回一个ClusterSpec。</target>
        </trans-unit>
        <trans-unit id="fbfdd6a723bd3e94666f26590d4249971ef2fb48" translate="yes" xml:space="preserve">
          <source>This is an implementation of cluster resolvers for Slurm clusters. This allows the specification of jobs and task counts, number of tasks per node, number of GPUs on each node and number of GPUs for each task. It retrieves system attributes by Slurm environment variables, resolves allocated computing node names, constructs a cluster and returns a ClusterResolver object which can be use for distributed TensorFlow.</source>
          <target state="translated">这是Slurm集群的集群解析器的实现。它允许指定工作和任务数量、每个节点的任务数量、每个节点上的GPU数量以及每个任务的GPU数量。它通过Slurm环境变量检索系统属性,解析分配的计算节点名称,构建集群并返回一个ClusterResolver对象,该对象可用于分布式TensorFlow。</target>
        </trans-unit>
        <trans-unit id="6c7332a1bfb936db2e58085e5162712580a9aab8" translate="yes" xml:space="preserve">
          <source>This is an implementation of cluster resolvers for the Google Cloud TPU service.</source>
          <target state="translated">这是Google Cloud TPU服务的集群解析器的实现。</target>
        </trans-unit>
        <trans-unit id="71688349367e3a46bf633affe83f7f67a539cf79" translate="yes" xml:space="preserve">
          <source>This is an implementation of cluster resolvers for the Google Cloud TPU service. As Cloud TPUs are in alpha, you will need to specify a API definition file for this to consume, in addition to a list of Cloud TPUs in your Google Cloud Platform project.</source>
          <target state="translated">这是Google Cloud TPU服务的集群解析器的实现。由于 Cloud TPU 处于 alpha 状态,因此除了 Google Cloud Platform 项目中的 Cloud TPU 列表外,您还需要指定一个 API 定义文件,以便使用该 API。</target>
        </trans-unit>
        <trans-unit id="ad683e8f469fc31a1a6f2ec923965536267cfedf" translate="yes" xml:space="preserve">
          <source>This is an implementation of cluster resolvers for the Google Compute Engine instance group platform. By specifying a project, zone, and instance group, this will retrieve the IP address of all the instances within the instance group and return a ClusterResolver object suitable for use for distributed TensorFlow.</source>
          <target state="translated">这是Google Compute Engine实例组平台的集群解析器的实现。通过指定项目、区域和实例组,将检索实例组内所有实例的 IP 地址,并返回一个适合分布式 TensorFlow 使用的 ClusterResolver 对象。</target>
        </trans-unit>
        <trans-unit id="a371677e8e3150f82fcdcb41fbdf4b2aece50db2" translate="yes" xml:space="preserve">
          <source>This is an implementation of cluster resolvers when using TF_CONFIG to set information about the cluster. The cluster spec returned will be initialized from the TF_CONFIG environment variable.</source>
          <target state="translated">这是使用TF_CONFIG设置集群信息时集群解析器的实现。返回的集群规格将从TF_CONFIG环境变量中初始化。</target>
        </trans-unit>
        <trans-unit id="f2e4a40b99bc22ad76a6b8d488160787266d96b7" translate="yes" xml:space="preserve">
          <source>This is because evaluating the gradient graph does not require evaluating the constant(1) op created in the forward pass.</source>
          <target state="translated">这是因为评估梯度图不需要评估在正向传递中创建的 constant(1)op。</target>
        </trans-unit>
        <trans-unit id="b402b2ea93d17cfe8814c2e2dd9513661013af48" translate="yes" xml:space="preserve">
          <source>This is called to signal the hooks that a new session has been created. This has two essential differences with the situation in which &lt;code&gt;begin&lt;/code&gt; is called:</source>
          <target state="translated">调用此信号是为了向挂钩表明已创建了新会话。这与调用 &lt;code&gt;begin&lt;/code&gt; 的情况有两个本质区别：</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
