<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="tensorflow">
    <body>
      <group id="tensorflow">
        <trans-unit id="587ea8c1cdf68f3aaebe6d4c8fb0dc04c9f49662" translate="yes" xml:space="preserve">
          <source>User-written layers and models can achieve the same behavior with code that looks like:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7076897558f527fd7b476c32a2d7999a7bce1794" translate="yes" xml:space="preserve">
          <source>Users can call this method to get some facts of the TPU system, like total number of cores, number of TPU workers and the devices. E.g.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="67147ed6d1d38b4f4b015cf112ec870c25df3f0b" translate="yes" xml:space="preserve">
          <source>Users can pass strategy specific options to &lt;code&gt;options&lt;/code&gt; argument. An example to enable bucketizing dynamic shapes in &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy#run&quot;&gt;&lt;code&gt;TPUStrategy.run&lt;/code&gt;&lt;/a&gt; is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f72b22db9ab4d4d8bbc6f80831bcd32ed79c81e6" translate="yes" xml:space="preserve">
          <source>Users can specify various options to control the behavior of snapshot, including how snapshots are read from and written to by passing in user-defined functions to the &lt;code&gt;reader_func&lt;/code&gt; and &lt;code&gt;shard_func&lt;/code&gt; parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="157f011efb63e43996c3b578c255e7d5545ba313" translate="yes" xml:space="preserve">
          <source>Users can write it to file for offline analysis by tfprof commandline or graphical interface.</source>
          <target state="translated">用户可以通过tfprof命令行或图形界面将其写入文件进行离线分析。</target>
        </trans-unit>
        <trans-unit id="e5a32aab95198164a5ff787e69d28cbf95af2893" translate="yes" xml:space="preserve">
          <source>Users may want specify this function to control how snapshot files should be read from disk, including the amount of shuffling and parallelism.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6748ccb2ebd37a27cc38d08c0979684dcbf55e30" translate="yes" xml:space="preserve">
          <source>Users may want to specify this function to control how snapshot files should be written to disk. Below is an example of how a potential shard_func could be written.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e7108f46888ed24215d4773b328e7e496d911304" translate="yes" xml:space="preserve">
          <source>Users may write the following code to asynchronuously invoke &lt;code&gt;train_step_fn&lt;/code&gt; and log the &lt;code&gt;loss&lt;/code&gt; metric for every &lt;code&gt;num_steps&lt;/code&gt; steps in a training loop. &lt;code&gt;train_step_fn&lt;/code&gt; internally consumes data using &lt;code&gt;iterator.get_next()&lt;/code&gt;, and may throw OutOfRangeError when running out of data. In the case:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0ec32691eaba97311f1cccd2dad97641a499fa1" translate="yes" xml:space="preserve">
          <source>Users must not modify any collections used in nest while this function is running.</source>
          <target state="translated">在此函数运行时,用户不得修改任何在nest中使用的集合。</target>
        </trans-unit>
        <trans-unit id="06b0870bd9d2eea911ba0bf24a44afc23e3b6358" translate="yes" xml:space="preserve">
          <source>Users need to combine parsing spec of features with labels and weights (if any) since they are all parsed from same tf.Example instance. This utility combines these specs.</source>
          <target state="translated">用户需要将特征的解析规范与标签和权重(如果有的话)结合起来,因为它们都是从同一个 tf.Example 实例中解析出来的。这个工具将这些规范结合起来。</target>
        </trans-unit>
        <trans-unit id="182c1f1a391ebf26b9c0b763f027f3059f9e6bd2" translate="yes" xml:space="preserve">
          <source>Users of &lt;code&gt;step_fn&lt;/code&gt; may perform &lt;code&gt;run()&lt;/code&gt; calls without running hooks by accessing the &lt;code&gt;session&lt;/code&gt;. A &lt;code&gt;run()&lt;/code&gt; call with hooks may be performed using &lt;code&gt;run_with_hooks()&lt;/code&gt;. Computation flow can be interrupted using &lt;code&gt;request_stop()&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;step_fn&lt;/code&gt; 的用户可以通过访问 &lt;code&gt;session&lt;/code&gt; 来执行 &lt;code&gt;run()&lt;/code&gt; 调用，而无需运行钩子。甲 &lt;code&gt;run()&lt;/code&gt; 可以使用来执行与钩呼叫 &lt;code&gt;run_with_hooks()&lt;/code&gt; 。可以使用 &lt;code&gt;request_stop()&lt;/code&gt; 中断计算流程。</target>
        </trans-unit>
        <trans-unit id="2f188fa1f43efe8bdab97db20fb68ecadd592af6" translate="yes" xml:space="preserve">
          <source>Users will just instantiate a layer and then treat it as a callable.</source>
          <target state="translated">用户只需实例化一个图层,然后把它当作一个可调用的图层。</target>
        </trans-unit>
        <trans-unit id="c72c8f3ece626a4705405cb3a7b8af47ef729815" translate="yes" xml:space="preserve">
          <source>Uses &lt;code&gt;sigmoid_cross_entropy&lt;/code&gt; loss average over classes and weighted sum over the batch. Namely, if the input logits have shape &lt;code&gt;[batch_size, n_classes]&lt;/code&gt;, the loss is the average over &lt;code&gt;n_classes&lt;/code&gt; and the weighted sum over &lt;code&gt;batch_size&lt;/code&gt;.</source>
          <target state="translated">在类别上使用 &lt;code&gt;sigmoid_cross_entropy&lt;/code&gt; 损失平均值，在批次中使用加权和。即，如果输入logits具有 &lt;code&gt;[batch_size, n_classes]&lt;/code&gt; 形状，则损失为 &lt;code&gt;n_classes&lt;/code&gt; 的平均值和 &lt;code&gt;batch_size&lt;/code&gt; 的加权总和。</target>
        </trans-unit>
        <trans-unit id="11a9b701f656adfafd6806eb9e7ef756d209ed2e" translate="yes" xml:space="preserve">
          <source>Uses &lt;code&gt;sigmoid_cross_entropy_with_logits&lt;/code&gt; loss, which is the same as &lt;code&gt;BinaryClassHead&lt;/code&gt;. The differences compared to &lt;code&gt;BinaryClassHead&lt;/code&gt; are:</source>
          <target state="translated">使用 &lt;code&gt;sigmoid_cross_entropy_with_logits&lt;/code&gt; 损失，与 &lt;code&gt;BinaryClassHead&lt;/code&gt; 相同。与 &lt;code&gt;BinaryClassHead&lt;/code&gt; 相比，差异是：</target>
        </trans-unit>
        <trans-unit id="3d8209fd0815d68d4830b41bd06d14a5cb947aa2" translate="yes" xml:space="preserve">
          <source>Uses &lt;code&gt;sigmoid_cross_entropy_with_logits&lt;/code&gt; loss.</source>
          <target state="translated">使用 &lt;code&gt;sigmoid_cross_entropy_with_logits&lt;/code&gt; 损失。</target>
        </trans-unit>
        <trans-unit id="e004400a6e3754313301e3fd9261a2c189b3474a" translate="yes" xml:space="preserve">
          <source>Uses &lt;code&gt;sparse_softmax_cross_entropy&lt;/code&gt; loss.</source>
          <target state="translated">使用 &lt;code&gt;sparse_softmax_cross_entropy&lt;/code&gt; 损失。</target>
        </trans-unit>
        <trans-unit id="3e2b61f4b530febb6503deed927b41c373ddaaf3" translate="yes" xml:space="preserve">
          <source>Uses &lt;code&gt;str(value)&lt;/code&gt;, except for &lt;code&gt;bytes&lt;/code&gt; typed inputs, which are converted using &lt;code&gt;as_str&lt;/code&gt;.</source>
          <target state="translated">使用 &lt;code&gt;str(value)&lt;/code&gt; ，但 &lt;code&gt;bytes&lt;/code&gt; 类型输入除外，后者使用 &lt;code&gt;as_str&lt;/code&gt; 进行转换。</target>
        </trans-unit>
        <trans-unit id="2973e6aa554e21daa4adef06a74539c445b14d95" translate="yes" xml:space="preserve">
          <source>Uses utf-8 encoding for text by default.</source>
          <target state="translated">默认使用utf-8编码的文本。</target>
        </trans-unit>
        <trans-unit id="217a51c3037aab24f119676b38ba458afe249401" translate="yes" xml:space="preserve">
          <source>Using &lt;a href=&quot;../nn/embedding_lookup_sparse&quot;&gt;&lt;code&gt;tf.nn.embedding_lookup_sparse&lt;/code&gt;&lt;/a&gt; for sparse multiplication:</source>
          <target state="translated">使用&lt;a href=&quot;../nn/embedding_lookup_sparse&quot;&gt; &lt;code&gt;tf.nn.embedding_lookup_sparse&lt;/code&gt; &lt;/a&gt;进行稀疏乘法：</target>
        </trans-unit>
        <trans-unit id="159ac66854831fd41477c6614f26e3ff55888f88" translate="yes" xml:space="preserve">
          <source>Using &lt;code&gt;pos&lt;/code&gt; and &lt;code&gt;len&lt;/code&gt; with same shape as &lt;code&gt;input&lt;/code&gt;:</source>
          <target state="translated">使用 &lt;code&gt;pos&lt;/code&gt; 和 &lt;code&gt;len&lt;/code&gt; 具有与 &lt;code&gt;input&lt;/code&gt; 相同的形状：</target>
        </trans-unit>
        <trans-unit id="20acdd8f39a8fcbb172a5237734f68ee0d2be386" translate="yes" xml:space="preserve">
          <source>Using float64 is similar to mixed precision. Either the global policy can be set to float64, or &lt;code&gt;dtype='float64'&lt;/code&gt; can be passed to individual layers. For example, to set the global policy:</source>
          <target state="translated">使用float64类似于混合精度。可以将全局策略设置为float64，也可以将 &lt;code&gt;dtype='float64'&lt;/code&gt; 传递给各个层。例如，要设置全局策略：</target>
        </trans-unit>
        <trans-unit id="f6b2bb4ba02dab533aac4565623cbaf2d9684521" translate="yes" xml:space="preserve">
          <source>Using graphs directly (deprecated)</source>
          <target state="translated">直接使用图形(已废弃</target>
        </trans-unit>
        <trans-unit id="45c1897ccbe225e4b4bb0b5776304a91ccc424dd" translate="yes" xml:space="preserve">
          <source>Using scalar &lt;code&gt;pos&lt;/code&gt; and &lt;code&gt;len&lt;/code&gt;:</source>
          <target state="translated">使用标量 &lt;code&gt;pos&lt;/code&gt; 和 &lt;code&gt;len&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="55be0948f7db3ef4c4e1ab2c5b8f53c076ac5356" translate="yes" xml:space="preserve">
          <source>Using the &lt;code&gt;TensorBoard&lt;/code&gt; callback will work when eager execution is enabled, with the restriction that outputting histogram summaries of weights and gradients is not supported. Consequently, &lt;code&gt;histogram_freq&lt;/code&gt; will be ignored.</source>
          <target state="translated">启用急切执行时，使用 &lt;code&gt;TensorBoard&lt;/code&gt; 回调将起作用，但有一个限制，即不支持输出权重和渐变的直方图摘要。因此， &lt;code&gt;histogram_freq&lt;/code&gt; 将被忽略。</target>
        </trans-unit>
        <trans-unit id="10b4feb7db468a7b136fb7eedc7778e772ed7977" translate="yes" xml:space="preserve">
          <source>Using the &lt;code&gt;Uniform&lt;/code&gt; distribution as an example:</source>
          <target state="translated">以 &lt;code&gt;Uniform&lt;/code&gt; 分布为例：</target>
        </trans-unit>
        <trans-unit id="a70f4ea74dc28a0759892b77d899be11c4a630a4" translate="yes" xml:space="preserve">
          <source>Using the SavedModel format</source>
          <target state="translated">使用SavedModel格式</target>
        </trans-unit>
        <trans-unit id="2d5a6c95117b542db8bf88cf405c6beac37d2a3c" translate="yes" xml:space="preserve">
          <source>Using the above module would produce &lt;a href=&quot;variable&quot;&gt;&lt;code&gt;tf.Variable&lt;/code&gt;&lt;/a&gt;s and &lt;a href=&quot;tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt;s whose names included the module name:</source>
          <target state="translated">使用上面的模块将产生&lt;a href=&quot;variable&quot;&gt; &lt;code&gt;tf.Variable&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; ,&lt;/a&gt;其名称包括模块名称：</target>
        </trans-unit>
        <trans-unit id="7ad33ae89a6d279a16ce6134859a442eac2581c4" translate="yes" xml:space="preserve">
          <source>Using the default job_name of worker, you can schedule ops to run remotely as follows:</source>
          <target state="translated">使用worker的默认job_name,您可以按以下方式安排远程运行操作。</target>
        </trans-unit>
        <trans-unit id="5a6462b40df0780171c71c9fb37d8c4d54d01a02" translate="yes" xml:space="preserve">
          <source>Using this strategy will place any variables created in its scope on the specified device. Input distributed through this strategy will be prefetched to the specified device. Moreover, any functions called via &lt;code&gt;strategy.experimental_run_v2&lt;/code&gt; will also be placed on the specified device as well.</source>
          <target state="translated">使用此策略会将在其作用域中创建的所有变量放置在指定设备上。通过此策略分配的输入将被预取到指定的设备。此外，通过 &lt;code&gt;strategy.experimental_run_v2&lt;/code&gt; 调用的任何函数也将放置在指定的设备上。</target>
        </trans-unit>
        <trans-unit id="0921ab75268541486ce05861f2d20fe9143695fe" translate="yes" xml:space="preserve">
          <source>Using this strategy will place any variables created in its scope on the specified device. Input distributed through this strategy will be prefetched to the specified device. Moreover, any functions called via &lt;code&gt;strategy.run&lt;/code&gt; will also be placed on the specified device as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71b01f087dc4062f216f4b3e8e8c06a8b29e198c" translate="yes" xml:space="preserve">
          <source>Usually, this does not necessarily mean that the layer is run in inference mode (which is normally controlled by the &lt;code&gt;training&lt;/code&gt; argument that can be passed when calling a layer). &quot;Frozen state&quot; and &quot;inference mode&quot; are two separate concepts.</source>
          <target state="translated">通常，这并不一定意味着该层以推理模式运行（通常由调用层时可以传递的 &lt;code&gt;training&lt;/code&gt; 参数来控制）。&amp;ldquo;冻结状态&amp;rdquo;和&amp;ldquo;推理模式&amp;rdquo;是两个单独的概念。</target>
        </trans-unit>
        <trans-unit id="6cf4bb7b1492b783c6bc69633b0fe1693771c5d9" translate="yes" xml:space="preserve">
          <source>Utilities for ImageNet data preprocessing &amp;amp; prediction decoding.</source>
          <target state="translated">用于ImageNet数据预处理和预测解码的实用程序。</target>
        </trans-unit>
        <trans-unit id="08fb7be2421c2bdf2a78c1776aba074edf57a478" translate="yes" xml:space="preserve">
          <source>Utilities for preprocessing sequence data.</source>
          <target state="translated">用于预处理序列数据的实用程序。</target>
        </trans-unit>
        <trans-unit id="ae6bf25944794d7df424031b54eed9e8c6e9c537" translate="yes" xml:space="preserve">
          <source>Utilities for text input preprocessing.</source>
          <target state="translated">文本输入预处理的实用工具。</target>
        </trans-unit>
        <trans-unit id="41900282bff2287f65ac5f2919d6b167c32aa670" translate="yes" xml:space="preserve">
          <source>Utilities for writing compatible code</source>
          <target state="translated">用于编写兼容代码的工具</target>
        </trans-unit>
        <trans-unit id="4209295b2fe1eb250fb221ec6cd68f34f854faae" translate="yes" xml:space="preserve">
          <source>Utility class for generating batches of temporal data.</source>
          <target state="translated">用于生成一批时态数据的实用类。</target>
        </trans-unit>
        <trans-unit id="3f80dd65b77c781d8eb1f4f65d0db44d5b92a1de" translate="yes" xml:space="preserve">
          <source>Utility function to build TensorInfo proto from a Tensor. (deprecated)</source>
          <target state="translated">用于从一个Tensor建立TensorInfo proto的实用函数。(已废弃)</target>
        </trans-unit>
        <trans-unit id="d66171638f7d7dc0d33fb68081060b45caf826a1" translate="yes" xml:space="preserve">
          <source>Utility function to build a SignatureDef protocol buffer.</source>
          <target state="translated">构建SignatureDef协议缓冲区的实用函数。</target>
        </trans-unit>
        <trans-unit id="a1fc6fdd2556af786588697b8b4da900e88da0b4" translate="yes" xml:space="preserve">
          <source>Utility functions for building and inspecting SignatureDef protos.</source>
          <target state="translated">用于构建和检查SignatureDef protos的实用功能。</target>
        </trans-unit>
        <trans-unit id="41a205ecd48940b5d4f27244d92bd652c4f4f6d1" translate="yes" xml:space="preserve">
          <source>Utility functions to assist with setup and construction of the SavedModel proto.</source>
          <target state="translated">帮助设置和构建SavedModel原件的实用功能。</target>
        </trans-unit>
        <trans-unit id="25e1689ca50193bb7593b638ace3275be8fc9300" translate="yes" xml:space="preserve">
          <source>Utility methods to create simple input_fns.</source>
          <target state="translated">实用的方法来创建简单的输入_fns。</target>
        </trans-unit>
        <trans-unit id="98c6a5322edf6827b19ea21d2e974ec573fb37d2" translate="yes" xml:space="preserve">
          <source>V2 Compatibility</source>
          <target state="translated">V2兼容性</target>
        </trans-unit>
        <trans-unit id="b45432e089497cb1e4b6a50a251afeb8f8ec8634" translate="yes" xml:space="preserve">
          <source>V2 format specific: merges the metadata files of sharded checkpoints. The</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f4dce99bbaf012216e610738a930ee809e1a1dd" translate="yes" xml:space="preserve">
          <source>VGG16 model for Keras.</source>
          <target state="translated">VGG16型号为Keras。</target>
        </trans-unit>
        <trans-unit id="16578957bc13d8fcce797647de9c6287bbab95bd" translate="yes" xml:space="preserve">
          <source>VGG19 model for Keras.</source>
          <target state="translated">VGG19型号为Keras。</target>
        </trans-unit>
        <trans-unit id="b2ac8a00dcd90f10a8f8eab14df7c47a44d8bc39" translate="yes" xml:space="preserve">
          <source>Valid keyword args are:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b984695d87811bddc59458f1d995336a48f8d61" translate="yes" xml:space="preserve">
          <source>Valid values for whence are: 0: start of the file (default) 1: relative to the current position of the file 2: relative to the end of file. &lt;code&gt;offset&lt;/code&gt; is usually negative.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35d73c1e3e748be7af4094274c7c669844f7b39a" translate="yes" xml:space="preserve">
          <source>Validate and return float type based on &lt;code&gt;tensors&lt;/code&gt; and &lt;code&gt;dtype&lt;/code&gt;.</source>
          <target state="translated">根据 &lt;code&gt;tensors&lt;/code&gt; 和 &lt;code&gt;dtype&lt;/code&gt; 验证并返回float类型。</target>
        </trans-unit>
        <trans-unit id="7cfbc94b8562d35a1a624063af44283471348553" translate="yes" xml:space="preserve">
          <source>Validated type.</source>
          <target state="translated">审定类型:</target>
        </trans-unit>
        <trans-unit id="fbc4d5994f37368bc38796f9fcac6177e5b079ae" translate="yes" xml:space="preserve">
          <source>Value Error: If input contains string value</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b6107bdd0fa4996fbf7660de172ff5a7b7753b6" translate="yes" xml:space="preserve">
          <source>Value convertible to &lt;a href=&quot;dtypes/dtype&quot;&gt;&lt;code&gt;tf.DType&lt;/code&gt;&lt;/a&gt;. The type of the tensor values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15c75ed5285ab847c51c1d1b073c1ffd6af1e2d3" translate="yes" xml:space="preserve">
          <source>Value convertible to &lt;a href=&quot;tensorshape&quot;&gt;&lt;code&gt;tf.TensorShape&lt;/code&gt;&lt;/a&gt;. The shape of the tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe02012fec01937ecf4625a4bb543d51186a14cc" translate="yes" xml:space="preserve">
          <source>Value of new time step. Can be a variable or a constant</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf813cf306f9fe6920d27a32a73e1a8ab0b8ac28" translate="yes" xml:space="preserve">
          <source>Value of tensor to set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0d2a5aeb6f41dd496d2ccf306df13e6658541ae" translate="yes" xml:space="preserve">
          <source>Value to return if flagname is not defined. Defaults to None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4123cacba8af0210e8bc3a45f165b7510fbe81e" translate="yes" xml:space="preserve">
          <source>Value to set for indices not specified in &lt;code&gt;self&lt;/code&gt;. Defaults to zero. &lt;code&gt;default_value&lt;/code&gt; must be broadcastable to &lt;code&gt;self.shape[self.ragged_rank + 1:]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c7617def04f30bf9e69162b03566ab37d2f02c8" translate="yes" xml:space="preserve">
          <source>Value to set the tensor to, as a Numpy array (of the same shape).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44cdd81777401755111de56bb90c9f9e4f31fb50" translate="yes" xml:space="preserve">
          <source>ValueError if &lt;code&gt;num_packs&lt;/code&gt; is negative.</source>
          <target state="translated">如果 &lt;code&gt;num_packs&lt;/code&gt; 为负，则为 ValueError 。</target>
        </trans-unit>
        <trans-unit id="b03333735384a88c2a20365a2ab9d19d0855bf6a" translate="yes" xml:space="preserve">
          <source>ValueError if data format is unrecognized, if &lt;code&gt;value&lt;/code&gt; has less than two dimensions when &lt;code&gt;data_format&lt;/code&gt; is 'N..C'/&lt;code&gt;None&lt;/code&gt; or &lt;code&gt;value&lt;/code&gt; has less then three dimensions when &lt;code&gt;data_format&lt;/code&gt; is &lt;code&gt;NC..&lt;/code&gt;, if &lt;code&gt;bias&lt;/code&gt; does not have exactly one dimension (is a vector), or if the size of &lt;code&gt;bias&lt;/code&gt; does not match the size of the channel dimension of &lt;code&gt;value&lt;/code&gt;.</source>
          <target state="translated">如果ValueError错误数据格式是无法识别，如果 &lt;code&gt;value&lt;/code&gt; 具有小于二维时 &lt;code&gt;data_format&lt;/code&gt; 是&amp;ldquo;N..C&amp;rdquo; / &lt;code&gt;None&lt;/code&gt; 或 &lt;code&gt;value&lt;/code&gt; 已经小于三个维度时 &lt;code&gt;data_format&lt;/code&gt; 是 &lt;code&gt;NC..&lt;/code&gt; ，如果 &lt;code&gt;bias&lt;/code&gt; 不具有恰好一个维度（由向量），或者如果 &lt;code&gt;bias&lt;/code&gt; 的大小与 &lt;code&gt;value&lt;/code&gt; 的通道维度的大小不匹配。</target>
        </trans-unit>
        <trans-unit id="28e00764e50e68a9e4b435811b832e1599c2d7ff" translate="yes" xml:space="preserve">
          <source>ValueError when attempting to use experimental_compile, but XLA support is not enabled.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f7604a24858b57e5b1686ec99368d1d81fb1d5a4" translate="yes" xml:space="preserve">
          <source>ValueError: When set pad_to_max_output_size to False for batched input.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d1cad1ac50b9d58fe699af8d3719130c04596c12" translate="yes" xml:space="preserve">
          <source>ValueRowIds(key,)</source>
          <target state="translated">ValueRowIds(key,)</target>
        </trans-unit>
        <trans-unit id="2aa729287cefae889cb308c2489e301fd72ddfcc" translate="yes" xml:space="preserve">
          <source>Values are generated when TensorFlow is compiled, and are static for each TensorFlow package. The return value is a dictionary with string keys such as:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9989c8b816fd103fa17ba450218cc3f655b85003" translate="yes" xml:space="preserve">
          <source>Values are merged in order, so if an index appears in both &lt;code&gt;indices[m][i]&lt;/code&gt; and &lt;code&gt;indices[n][j]&lt;/code&gt; for &lt;code&gt;(m,i) &amp;lt; (n,j)&lt;/code&gt; the slice &lt;code&gt;data[n][j]&lt;/code&gt; will appear in the merged result. If you do not need this guarantee, ParallelDynamicStitch might perform better on some devices.</source>
          <target state="translated">值被合并，以便，因此，如果索引出现在两个 &lt;code&gt;indices[m][i]&lt;/code&gt; 和 &lt;code&gt;indices[n][j]&lt;/code&gt; 为 &lt;code&gt;(m,i) &amp;lt; (n,j)&lt;/code&gt; 切片 &lt;code&gt;data[n][j]&lt;/code&gt; 将出现在合并结果中。如果不需要此保证，则ParallelDynamicStitch在某些设备上可能会表现更好。</target>
        </trans-unit>
        <trans-unit id="31517cd57e02e0d5266cab70a1c38e47fcd19844" translate="yes" xml:space="preserve">
          <source>Values are not loaded immediately, but when the initializer is run (typically by running a &lt;a href=&quot;../global_variables_initializer&quot;&gt;&lt;code&gt;tf.compat.v1.global_variables_initializer&lt;/code&gt;&lt;/a&gt; op).</source>
          <target state="translated">不会立即加载值，而是在运行初始化程序时（通常通过运行&lt;a href=&quot;../global_variables_initializer&quot;&gt; &lt;code&gt;tf.compat.v1.global_variables_initializer&lt;/code&gt; &lt;/a&gt; op）来加载值。</target>
        </trans-unit>
        <trans-unit id="c7b1102521bf7adb2b3f3d054e8e942970be2d94" translate="yes" xml:space="preserve">
          <source>Values can also have the same locality as a variable, which is a mirrored value but residing on the same devices as the variable (as opposed to the compute devices). Such values may be passed to a call to &lt;a href=&quot;../../../distribute/strategyextended#update&quot;&gt;&lt;code&gt;tf.distribute.StrategyExtended.update&lt;/code&gt;&lt;/a&gt; to update the value of a variable. You may use &lt;a href=&quot;../../../distribute/strategyextended#colocate_vars_with&quot;&gt;&lt;code&gt;tf.distribute.StrategyExtended.colocate_vars_with&lt;/code&gt;&lt;/a&gt; to give a variable the same locality as another variable. This is useful, for example, for &quot;slot&quot; variables used by an optimizer for keeping track of statistics used to update a primary/model variable. You may convert a per-replica value to a variable's locality by using &lt;a href=&quot;../../../distribute/strategyextended#reduce_to&quot;&gt;&lt;code&gt;tf.distribute.StrategyExtended.reduce_to&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../../distribute/strategyextended#batch_reduce_to&quot;&gt;&lt;code&gt;tf.distribute.StrategyExtended.batch_reduce_to&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">值也可以与变量具有相同的局部性，变量是一个镜像值，但与变量位于同一设备上（与计算设备相反）。这样的值可以传递给对&lt;a href=&quot;../../../distribute/strategyextended#update&quot;&gt; &lt;code&gt;tf.distribute.StrategyExtended.update&lt;/code&gt; &lt;/a&gt;的调用以更新变量的值。您可以使用&lt;a href=&quot;../../../distribute/strategyextended#colocate_vars_with&quot;&gt; &lt;code&gt;tf.distribute.StrategyExtended.colocate_vars_with&lt;/code&gt; &lt;/a&gt;赋予变量与另一个变量相同的位置。例如，这对于优化器用于跟踪用于更新主要/模型变量的统计信息的&amp;ldquo;插槽&amp;rdquo;变量很有用。您可以使用&lt;a href=&quot;../../../distribute/strategyextended#reduce_to&quot;&gt; &lt;code&gt;tf.distribute.StrategyExtended.reduce_to&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;../../../distribute/strategyextended#batch_reduce_to&quot;&gt; &lt;code&gt;tf.distribute.StrategyExtended.batch_reduce_to&lt;/code&gt; &lt;/a&gt;将每个副本的值转换为变量的局部性。</target>
        </trans-unit>
        <trans-unit id="2143538d5a76006c2fb34d48a187c9999b4ce092" translate="yes" xml:space="preserve">
          <source>Values can also have the same locality as a variable, which is a mirrored value but residing on the same devices as the variable (as opposed to the compute devices). Such values may be passed to a call to &lt;a href=&quot;strategyextended#update&quot;&gt;&lt;code&gt;tf.distribute.StrategyExtended.update&lt;/code&gt;&lt;/a&gt; to update the value of a variable. You may use &lt;a href=&quot;strategyextended#colocate_vars_with&quot;&gt;&lt;code&gt;tf.distribute.StrategyExtended.colocate_vars_with&lt;/code&gt;&lt;/a&gt; to give a variable the same locality as another variable. This is useful, for example, for &quot;slot&quot; variables used by an optimizer for keeping track of statistics used to update a primary/model variable. You may convert a per-replica value to a variable's locality by using &lt;a href=&quot;strategyextended#reduce_to&quot;&gt;&lt;code&gt;tf.distribute.StrategyExtended.reduce_to&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;strategyextended#batch_reduce_to&quot;&gt;&lt;code&gt;tf.distribute.StrategyExtended.batch_reduce_to&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">值也可以与变量具有相同的局部性，变量是一个镜像值，但与变量位于同一设备上（与计算设备相反）。这样的值可以传递给对&lt;a href=&quot;strategyextended#update&quot;&gt; &lt;code&gt;tf.distribute.StrategyExtended.update&lt;/code&gt; &lt;/a&gt;的调用以更新变量的值。您可以使用&lt;a href=&quot;strategyextended#colocate_vars_with&quot;&gt; &lt;code&gt;tf.distribute.StrategyExtended.colocate_vars_with&lt;/code&gt; &lt;/a&gt;赋予变量与另一个变量相同的位置。例如，这对于优化器用于跟踪用于更新主要/模型变量的统计信息的&amp;ldquo;插槽&amp;rdquo;变量很有用。您可以使用&lt;a href=&quot;strategyextended#reduce_to&quot;&gt; &lt;code&gt;tf.distribute.StrategyExtended.reduce_to&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;strategyextended#batch_reduce_to&quot;&gt; &lt;code&gt;tf.distribute.StrategyExtended.batch_reduce_to&lt;/code&gt; &lt;/a&gt;将每个副本的值转换为变量的局部性。</target>
        </trans-unit>
        <trans-unit id="ee2d365c74f97141598697d434a691d20762875a" translate="yes" xml:space="preserve">
          <source>Values in &lt;code&gt;arr&lt;/code&gt; outside of the range [0, size) are ignored.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93fb0db8d97fc795e57ce06a9fa4d44911eba728" translate="yes" xml:space="preserve">
          <source>Values may be merged in parallel, so if an index appears in both &lt;code&gt;indices[m][i]&lt;/code&gt; and &lt;code&gt;indices[n][j]&lt;/code&gt;, the result may be invalid. This differs from the normal DynamicStitch operator that defines the behavior in that case.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d15a2f51d1f3c105d253f174b82490fef1d2d0a2" translate="yes" xml:space="preserve">
          <source>Values of the sparse gradient to be applied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="332770f6ea1035cc31b26b51a3ff43d26f36a8a6" translate="yes" xml:space="preserve">
          <source>Values returned by all methods, such as &lt;code&gt;matmul&lt;/code&gt; or &lt;code&gt;determinant&lt;/code&gt; will be cast to &lt;code&gt;DTYPE&lt;/code&gt;.</source>
          <target state="translated">所有方法（例如 &lt;code&gt;matmul&lt;/code&gt; 或 &lt;code&gt;determinant&lt;/code&gt; )返回的值将强制转换为 &lt;code&gt;DTYPE&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="472814b4275e6a3c35876afc6f85f152eeb4868b" translate="yes" xml:space="preserve">
          <source>Values to be associated with keys. Must be a tensor of the same shape as &lt;code&gt;keys&lt;/code&gt; and match the table's value type.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7e1d325bbb77e43c2a2a39b8f1ed0e1639c81e61" translate="yes" xml:space="preserve">
          <source>Values to pad with, passed to &lt;a href=&quot;../dataset#padded_batch&quot;&gt;&lt;code&gt;tf.data.Dataset.padded_batch&lt;/code&gt;&lt;/a&gt;. Defaults to padding with 0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="152b6b1ae03ca0564ec1afcfa23945d6679fe3af" translate="yes" xml:space="preserve">
          <source>Values to put in the TensorProto.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="771c4420b1aa778f676175f583b791c3f76deedd" translate="yes" xml:space="preserve">
          <source>VarHandleOp</source>
          <target state="translated">VarHandleOp</target>
        </trans-unit>
        <trans-unit id="ff83e8cfb2acccae7a99a5aa63151589e41e200d" translate="yes" xml:space="preserve">
          <source>VarIsInitializedOp</source>
          <target state="translated">VarIsInitializedOp</target>
        </trans-unit>
        <trans-unit id="19de69cb601f53a4ea7af22a65c71ae63251365c" translate="yes" xml:space="preserve">
          <source>Variable</source>
          <target state="translated">Variable</target>
        </trans-unit>
        <trans-unit id="8b550e406a084548380628fd2909bf25b85b7d1f" translate="yes" xml:space="preserve">
          <source>Variable Constraint</source>
          <target state="translated">变量限制</target>
        </trans-unit>
        <trans-unit id="dc8c6831002ca69f6ebeab63140354a283153e34" translate="yes" xml:space="preserve">
          <source>Variable Constraints</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="87bff689d1fa1dea41f1caabbfa56196f9dac995" translate="yes" xml:space="preserve">
          <source>Variable creation inside &lt;code&gt;scope&lt;/code&gt; is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like &lt;code&gt;MirroredStrategy&lt;/code&gt;, &lt;code&gt;TPUStrategy&lt;/code&gt; and &lt;code&gt;MultiWorkerMiroredStrategy&lt;/code&gt; create variables replicated on each replica, whereas &lt;code&gt;ParameterServerStrategy&lt;/code&gt; creates variables on the parameter servers. This is done using a custom &lt;a href=&quot;../../../../variable_creator_scope&quot;&gt;&lt;code&gt;tf.variable_creator_scope&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ebe6d40e82094b23daf6be3c6e12b5de702a887" translate="yes" xml:space="preserve">
          <source>Variable creation inside &lt;code&gt;scope&lt;/code&gt; is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like &lt;code&gt;MirroredStrategy&lt;/code&gt;, &lt;code&gt;TPUStrategy&lt;/code&gt; and &lt;code&gt;MultiWorkerMiroredStrategy&lt;/code&gt; create variables replicated on each replica, whereas &lt;code&gt;ParameterServerStrategy&lt;/code&gt; creates variables on the parameter servers. This is done using a custom &lt;a href=&quot;../../../variable_creator_scope&quot;&gt;&lt;code&gt;tf.variable_creator_scope&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ad8c28ec75b4fb97533632feee539c8715d6500d" translate="yes" xml:space="preserve">
          <source>Variable creation inside &lt;code&gt;scope&lt;/code&gt; is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like &lt;code&gt;MirroredStrategy&lt;/code&gt;, &lt;code&gt;TPUStrategy&lt;/code&gt; and &lt;code&gt;MultiWorkerMiroredStrategy&lt;/code&gt; create variables replicated on each replica, whereas &lt;code&gt;ParameterServerStrategy&lt;/code&gt; creates variables on the parameter servers. This is done using a custom &lt;a href=&quot;../../variable_creator_scope&quot;&gt;&lt;code&gt;tf.variable_creator_scope&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56732160b0c125a3a8776369974b1d8fe83bf273" translate="yes" xml:space="preserve">
          <source>Variable creation inside &lt;code&gt;scope&lt;/code&gt; is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like &lt;code&gt;MirroredStrategy&lt;/code&gt;, &lt;code&gt;TPUStrategy&lt;/code&gt; and &lt;code&gt;MultiWorkerMiroredStrategy&lt;/code&gt; create variables replicated on each replica, whereas &lt;code&gt;ParameterServerStrategy&lt;/code&gt; creates variables on the parameter servers. This is done using a custom &lt;a href=&quot;../variable_creator_scope&quot;&gt;&lt;code&gt;tf.variable_creator_scope&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7dd58eeb1186ca442949f2a63231fa7bc6934171" translate="yes" xml:space="preserve">
          <source>Variable name to use for the first structure in assertion messages.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4e78d0db2266afd0792266861e4c967b5ab1437" translate="yes" xml:space="preserve">
          <source>Variable name to use for the second structure.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66091c2f5a648f0c372811ab148bd71631287d1d" translate="yes" xml:space="preserve">
          <source>Variable name.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0be93ad0375437f556470057db6cb269fac15f01" translate="yes" xml:space="preserve">
          <source>Variable or tensor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d982c2fd72628882252beffffe7353608ac38df" translate="yes" xml:space="preserve">
          <source>Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing &lt;code&gt;losses&lt;/code&gt; under a &lt;a href=&quot;../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt; will propagate gradients back to the corresponding variables.</source>
          <target state="translated">当访问该属性创建变量正规化张量，所以它是渴望安全：访问 &lt;code&gt;losses&lt;/code&gt; 下&lt;a href=&quot;../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;将传播梯度回相应的变量。</target>
        </trans-unit>
        <trans-unit id="9becbe8191297fcb35681c2cb0fd8d5255180628" translate="yes" xml:space="preserve">
          <source>Variable scope allows you to create new variables and to share already created ones while providing checks to not create or share by accident. For details, see the &lt;a href=&quot;https://tensorflow.org/guide/variables&quot;&gt;Variable Scope How To&lt;/a&gt;, here we present only a few basic examples.</source>
          <target state="translated">变量作用域允许您创建新变量并共享已创建的变量，同时提供检查以防止意外创建或共享。有关详细信息，请参见&amp;ldquo; &lt;a href=&quot;https://tensorflow.org/guide/variables&quot;&gt;可变范围操作方法&amp;rdquo;&lt;/a&gt;，此处仅介绍一些基本示例。</target>
        </trans-unit>
        <trans-unit id="78a94c97c2babdd8664aa38c452645afd7c11def" translate="yes" xml:space="preserve">
          <source>Variable scope object to carry defaults to provide to &lt;code&gt;get_variable&lt;/code&gt;.</source>
          <target state="translated">携带默认值的变量范围对象提供给 &lt;code&gt;get_variable&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="656181555fa5b90a78a2907e7cdc150cdb7d9969" translate="yes" xml:space="preserve">
          <source>Variable semantics in TensorFlow 2 are eager execution friendly. The above code is roughly equivalent to:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31ae50eddd5315630d388c7fc7d3dd4ac0f0c588" translate="yes" xml:space="preserve">
          <source>Variable shape. Defaults to scalar if unspecified.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c9c726ae1bb253cefd1f8e18cd504aa6e24e0e2b" translate="yes" xml:space="preserve">
          <source>Variable to set to a new value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2fa42dfef00dd6f5716bd283c68f5a7c3df717eb" translate="yes" xml:space="preserve">
          <source>Variable, possibly mirrored to multiple devices, to operate on.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fad16e163b18a5adad4a6b1f9bafad182b1fc5e8" translate="yes" xml:space="preserve">
          <source>Variable-size shapes are allowed by setting the corresponding shape dimensions to 0 in the shape attr. In this case DequeueMany will pad up to the maximum size of any given element in the minibatch. See below for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f736193c89669392caf0903ec1e19730e643644" translate="yes" xml:space="preserve">
          <source>Variable. The number of training steps this Optimizer has run.</source>
          <target state="translated">变量。该优化器已运行的训练步骤数。</target>
        </trans-unit>
        <trans-unit id="fbd24bae4ab2c5fb1757b5c4452669855276459f" translate="yes" xml:space="preserve">
          <source>VariableScope for the created subgraph; defaults to &quot;bidirectional_rnn&quot;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a6f1a6e7f1ea3415ae9f018191ab201ea3b0fac" translate="yes" xml:space="preserve">
          <source>VariableScope for the created subgraph; defaults to &quot;rnn&quot;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3580913cdba362b6cc2d19d6b7d1f9e9219c622d" translate="yes" xml:space="preserve">
          <source>VariableShape</source>
          <target state="translated">VariableShape</target>
        </trans-unit>
        <trans-unit id="751393cca20cc522b2f5c1a18350f77fe8d88bea" translate="yes" xml:space="preserve">
          <source>VariableV2</source>
          <target state="translated">VariableV2</target>
        </trans-unit>
        <trans-unit id="f628de42a66fa9f6ab10a5ed8f37f3bfec4938d4" translate="yes" xml:space="preserve">
          <source>Variables are assigned to local CPU or the only GPU. If there is more than one GPU, compute operations (other than variable update operations) will be replicated across all GPUs.</source>
          <target state="translated">变量被分配给本地CPU或唯一的GPU。如果有一个以上的GPU,计算操作(除了变量更新操作)将在所有GPU上复制。</target>
        </trans-unit>
        <trans-unit id="70a30a529e57576d822cecd0f271a7b5d39f84b7" translate="yes" xml:space="preserve">
          <source>Variables are automatically tracked when assigned to attributes of types inheriting from &lt;a href=&quot;module&quot;&gt;&lt;code&gt;tf.Module&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">将变量分配给从&lt;a href=&quot;module&quot;&gt; &lt;code&gt;tf.Module&lt;/code&gt; &lt;/a&gt;继承的类型的属性时，将自动跟踪变量。</target>
        </trans-unit>
        <trans-unit id="1655acbcc407ca5c5cc17d4daaba10766362006a" translate="yes" xml:space="preserve">
          <source>Variables are often captured and manipulated by &lt;a href=&quot;function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;s. This works the same way the un-decorated function would have:</source>
          <target state="translated">变量通常由&lt;a href=&quot;function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;捕获和操纵。这与未装饰的函数具有相同的工作方式：</target>
        </trans-unit>
        <trans-unit id="9422ebfe912588a94b4db8ceb623ec6943b8ae1a" translate="yes" xml:space="preserve">
          <source>Variables created inside a &lt;a href=&quot;function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; must be owned outside the function and be created only once:</source>
          <target state="translated">在&lt;a href=&quot;function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;内部创建的变量必须在该函数外部拥有，并且只能创建一次：</target>
        </trans-unit>
        <trans-unit id="a4f9c7fa0b18154ca4691caf51f88ee40eea34cb" translate="yes" xml:space="preserve">
          <source>Variables created inside a &lt;code&gt;MirroredStrategy&lt;/code&gt; which is wrapped with a &lt;a href=&quot;../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; are still &lt;code&gt;MirroredVariables&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="29e458106bb18014f5f1ce0d551df4f487281340" translate="yes" xml:space="preserve">
          <source>Variables created inside a &lt;code&gt;MirroredStrategy&lt;/code&gt; which is wrapped with a &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; are still &lt;code&gt;MirroredVariables&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="198e5fbedc6e80bc4c09f5cf7cecc37f23ca5651" translate="yes" xml:space="preserve">
          <source>Variables created inside the strategy scope are &quot;owned&quot; by it:</source>
          <target state="translated">在策略范围内创建的变量由它 &quot;拥有&quot;。</target>
        </trans-unit>
        <trans-unit id="ecf6360262e709b4fb787134a5207269f78afa79" translate="yes" xml:space="preserve">
          <source>Variables created outside the strategy are not owned by it:</source>
          <target state="translated">在战略之外创建的变量不属于战略的所有者。</target>
        </trans-unit>
        <trans-unit id="4e85b51c74744b26c07cb066853860450f1f5376" translate="yes" xml:space="preserve">
          <source>Variables must be tracked by assigning them to an attribute of a tracked object or to an attribute of &lt;code&gt;obj&lt;/code&gt; directly. TensorFlow objects (e.g. layers from &lt;a href=&quot;../keras/layers&quot;&gt;&lt;code&gt;tf.keras.layers&lt;/code&gt;&lt;/a&gt;, optimizers from &lt;a href=&quot;../train&quot;&gt;&lt;code&gt;tf.train&lt;/code&gt;&lt;/a&gt;) track their variables automatically. This is the same tracking scheme that &lt;a href=&quot;../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; uses, and an exported &lt;code&gt;Checkpoint&lt;/code&gt; object may be restored as a training checkpoint by pointing &lt;a href=&quot;../train/checkpoint#restore&quot;&gt;&lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt;&lt;/a&gt; to the SavedModel's &quot;variables/&quot; subdirectory. Currently variables are the only stateful objects supported by &lt;a href=&quot;save&quot;&gt;&lt;code&gt;tf.saved_model.save&lt;/code&gt;&lt;/a&gt;, but others (e.g. tables) will be supported in the future.</source>
          <target state="translated">必须通过将变量直接分配给被跟踪对象的属性或 &lt;code&gt;obj&lt;/code&gt; 的属性来跟踪变量。 TensorFlow对象（例如&lt;a href=&quot;../keras/layers&quot;&gt; &lt;code&gt;tf.keras.layers&lt;/code&gt; 中的&lt;/a&gt;图层，tf.train中的优化&lt;a href=&quot;../train&quot;&gt; &lt;code&gt;tf.train&lt;/code&gt; &lt;/a&gt;）自动跟踪其变量。这与&lt;a href=&quot;../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt;使用的跟踪方案相同，通过将&lt;a href=&quot;../train/checkpoint#restore&quot;&gt; &lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt; &lt;/a&gt;指向SavedModel的&amp;ldquo; variables /&amp;rdquo;子目录，可以将导出的 &lt;code&gt;Checkpoint&lt;/code&gt; 对象恢复为训练检查点。当前，变量是&lt;a href=&quot;save&quot;&gt; &lt;code&gt;tf.saved_model.save&lt;/code&gt; &lt;/a&gt;支持的唯一有状态对象，但将来将支持其他对象（例如表）。</target>
        </trans-unit>
        <trans-unit id="e84fa29887783568cfad44fb6f314a273a26c8b5" translate="yes" xml:space="preserve">
          <source>Variables must be tracked by assigning them to an attribute of a tracked object or to an attribute of &lt;code&gt;obj&lt;/code&gt; directly. TensorFlow objects (e.g. layers from &lt;a href=&quot;../keras/layers&quot;&gt;&lt;code&gt;tf.keras.layers&lt;/code&gt;&lt;/a&gt;, optimizers from &lt;a href=&quot;../train&quot;&gt;&lt;code&gt;tf.train&lt;/code&gt;&lt;/a&gt;) track their variables automatically. This is the same tracking scheme that &lt;a href=&quot;../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; uses, and an exported &lt;code&gt;Checkpoint&lt;/code&gt; object may be restored as a training checkpoint by pointing &lt;a href=&quot;../train/checkpoint#restore&quot;&gt;&lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt;&lt;/a&gt; to the SavedModel's &quot;variables/&quot; subdirectory. Currently, variables are the only stateful objects supported by &lt;a href=&quot;save&quot;&gt;&lt;code&gt;tf.saved_model.save&lt;/code&gt;&lt;/a&gt;, but others (e.g. tables) will be supported in the future.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51a7e747ba069953d31cbdce4de5483d9446f49f" translate="yes" xml:space="preserve">
          <source>Variables, placeholders, and independent operations can also be stored, as shown in the following example.</source>
          <target state="translated">还可以存储变量、占位符和独立操作,如下例所示。</target>
        </trans-unit>
        <trans-unit id="612d2ee1679ef5637187e20c4629a406547bfef9" translate="yes" xml:space="preserve">
          <source>Variables:</source>
          <target state="translated">Variables:</target>
        </trans-unit>
        <trans-unit id="bb96e64e18e9a621cc7ceb2976a5e75548f771d4" translate="yes" xml:space="preserve">
          <source>Variance is defined as,</source>
          <target state="translated">差异的定义是:</target>
        </trans-unit>
        <trans-unit id="d282722043467708dfd05bebcd66f9f1d34aee3d" translate="yes" xml:space="preserve">
          <source>Variance of a tensor, alongside the specified axis.</source>
          <target state="translated">张量的方差,沿着指定的轴线。</target>
        </trans-unit>
        <trans-unit id="547d7dc902ba966407aa0728767694e953ad6b04" translate="yes" xml:space="preserve">
          <source>Variance of batch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="99189c611d030a5281d712eaa2b886309eb67a6b" translate="yes" xml:space="preserve">
          <source>Variance.</source>
          <target state="translated">Variance.</target>
        </trans-unit>
        <trans-unit id="108fd2238efb49f633daf84e4120ae9f47cfdec6" translate="yes" xml:space="preserve">
          <source>Various libraries built on top of the core TensorFlow library take care of creating some or all of these pieces and storing them in well known collections in the graph. The &lt;code&gt;Scaffold&lt;/code&gt; class helps pick these pieces from the graph collections, creating and adding them to the collections if needed.</source>
          <target state="translated">在核心TensorFlow库之上构建的各种库负责创建其中的一些或全部片段并将它们存储在图中的众所周知的集合中。该 &lt;code&gt;Scaffold&lt;/code&gt; 类有助于挑选从图收藏这些作品，创造并根据需要将它们添加到收藏。</target>
        </trans-unit>
        <trans-unit id="a6a6801f04fd08c4cf3ed0c985c50e0b3cc573dd" translate="yes" xml:space="preserve">
          <source>Vector length = Maximum element in vector &lt;code&gt;values&lt;/code&gt; is 5. Adding 1, which is 6 will be the vector length.</source>
          <target state="translated">向量长度=向 &lt;code&gt;values&lt;/code&gt; 最大元素为5。将1加为6将是向量长度。</target>
        </trans-unit>
        <trans-unit id="f5115b80229d0ab74c010431ae650645a075fb17" translate="yes" xml:space="preserve">
          <source>Vector of coordinatewise logits.</source>
          <target state="translated">坐标对数的向量。</target>
        </trans-unit>
        <trans-unit id="444c2b1f0bb53e9255195603b2fe9758758fc528" translate="yes" xml:space="preserve">
          <source>Vector of coordinatewise probabilities.</source>
          <target state="translated">坐标概率的向量。</target>
        </trans-unit>
        <trans-unit id="ce1a5d4c90bdb53d4d10dd069e9eaa942f080fdb" translate="yes" xml:space="preserve">
          <source>Vector or scalar &lt;code&gt;Tensor&lt;/code&gt;. Specifies the exclusive upper limits for each range.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a3f9cb652ec383e8e638e09260439b3155ece9c8" translate="yes" xml:space="preserve">
          <source>Vector or scalar &lt;code&gt;Tensor&lt;/code&gt;. Specifies the first entry for each range if &lt;code&gt;limits&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt;; otherwise, specifies the range limits, and the first entries default to &lt;code&gt;0&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6a78cb729e962d878d29fc681da48899c9408f1c" translate="yes" xml:space="preserve">
          <source>Vector or scalar &lt;code&gt;Tensor&lt;/code&gt;. Specifies the increment for each range. Defaults to &lt;code&gt;1&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca486267fdece68b69e71338f98f70bc8a117412" translate="yes" xml:space="preserve">
          <source>Verbosity mode, 0 (silent), 1 (verbose), 2 (semi-verbose)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89f3f7fe4c1640033ee74781a079e4dad0989210" translate="yes" xml:space="preserve">
          <source>Verbosity mode, 0 or 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec03954e111311ff95bd45ec3b8a12f4f9d9e291" translate="yes" xml:space="preserve">
          <source>Verifies whether all flags pass validation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf8d82dcaf9576f668b800d91e2cd8a9bd872212" translate="yes" xml:space="preserve">
          <source>Vertical coordinate of the top-left corner of the result in the input.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3cbf1afdceff80dfc15073d9ab60779a3946f421" translate="yes" xml:space="preserve">
          <source>View aliases</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31a7604a3c742ea454ff128dd267ca1e16472796" translate="yes" xml:space="preserve">
          <source>View source</source>
          <target state="translated">查看来源</target>
        </trans-unit>
        <trans-unit id="a11a63ecf581d6810e65e62968e93200e384c05b" translate="yes" xml:space="preserve">
          <source>View source on GitHub</source>
          <target state="translated">在GitHub上查看源码</target>
        </trans-unit>
        <trans-unit id="6737eca3bba31c47e227cdb4b10d88148b47097b" translate="yes" xml:space="preserve">
          <source>Visit the &lt;a href=&quot;https://www.tensorflow.org/tutorials/distribute/input&quot;&gt;tutorial&lt;/a&gt; on distributed input for more examples and caveats.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d07bb0bd533bc6a8f9cecf11b29b925415f286e8" translate="yes" xml:space="preserve">
          <source>Vocabulary information for warm-starting.</source>
          <target state="translated">暖启动的词汇信息。</target>
        </trans-unit>
        <trans-unit id="1ec053f91b0e4f43b0ff9856f1f431195070e64b" translate="yes" xml:space="preserve">
          <source>WARNING: Experimental interface, subject to change.</source>
          <target state="translated">警告:实验界面,可能会有变化。</target>
        </trans-unit>
        <trans-unit id="ec7aa071ac596c65b3d8973549288f00dd9060fe" translate="yes" xml:space="preserve">
          <source>WARNING: If &lt;code&gt;sloppy&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the order of produced elements is not deterministic.</source>
          <target state="translated">警告：如果 &lt;code&gt;sloppy&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则产生的元素的顺序不确定。</target>
        </trans-unit>
        <trans-unit id="20a9fc48c3697f74feddc8ad93eb4c90b0d2bc47" translate="yes" xml:space="preserve">
          <source>WARNING: This function is nondeterministic, since it starts a separate thread for each tensor.</source>
          <target state="translated">警告:这个函数是非确定性的,因为它为每个张量启动一个单独的线程。</target>
        </trans-unit>
        <trans-unit id="e747855abeb092a18ebf6b91688f8d9d0024a93d" translate="yes" xml:space="preserve">
          <source>WARNING: tf.Variable objects by default have a non-intuitive memory model. A Variable is represented internally as a mutable Tensor which can non-deterministically alias other Tensors in a graph. The set of operations which consume a Variable and can lead to aliasing is undetermined and can change across TensorFlow versions. Avoid writing code which relies on the value of a Variable either changing or not changing as other operations happen. For example, using Variable objects or simple functions thereof as predicates in a &lt;a href=&quot;../../cond&quot;&gt;&lt;code&gt;tf.cond&lt;/code&gt;&lt;/a&gt; is dangerous and error-prone:</source>
          <target state="translated">警告：tf.Variable对象默认情况下具有非直观的内存模型。变量在内部表示为可变张量，该张量可以不确定地别名图形中的其他张量。不确定使用变量的操作集并可能导致混淆的操作集，并且可以在TensorFlow版本之间进行更改。避免编写依赖于变量的值的代码，该代码在其他操作发生时会更改或不更改。例如，在&lt;a href=&quot;../../cond&quot;&gt; &lt;code&gt;tf.cond&lt;/code&gt; 中&lt;/a&gt;使用Variable对象或其简单函数作为谓词是危险且容易出错的：</target>
        </trans-unit>
        <trans-unit id="1d2a3e7fd14becda3ad79ab623ee02dcfb3c4815" translate="yes" xml:space="preserve">
          <source>WRONG:</source>
          <target state="translated">WRONG:</target>
        </trans-unit>
        <trans-unit id="b55f8ed036408ccdecc6c75f53cbf2cbe602c833" translate="yes" xml:space="preserve">
          <source>Wait for threads to terminate.</source>
          <target state="translated">等待线程终止。</target>
        </trans-unit>
        <trans-unit id="480279e397ed207ff6dcb2138e6d19a402c6acab" translate="yes" xml:space="preserve">
          <source>Wait till the Coordinator is told to stop.</source>
          <target state="translated">等到协调员被叫停的时候</target>
        </trans-unit>
        <trans-unit id="fd3c945de3bbe026196979524e2513d29b8047c4" translate="yes" xml:space="preserve">
          <source>Wait until the thread terminates.</source>
          <target state="translated">等到线程终止。</target>
        </trans-unit>
        <trans-unit id="5fcf196e2d36a225ff59aa7e988a3bab54a62688" translate="yes" xml:space="preserve">
          <source>Warm-start all TRAINABLE variables:</source>
          <target state="translated">热启动所有可训练变量。</target>
        </trans-unit>
        <trans-unit id="77a75fce3b44ee7c1b7330f94b51f223f12aced8" translate="yes" xml:space="preserve">
          <source>Warm-start all variables (including non-TRAINABLE):</source>
          <target state="translated">预热启动所有变量(包括非TRAINABLE)。</target>
        </trans-unit>
        <trans-unit id="385cb07f1382ec501efc556d03af6fccd0c3bc88" translate="yes" xml:space="preserve">
          <source>Warm-start all weights but the embedding parameters corresponding to &lt;code&gt;sc_vocab_file&lt;/code&gt; have a different vocab from the one used in the current model:</source>
          <target state="translated">热启动所有权重，但与 &lt;code&gt;sc_vocab_file&lt;/code&gt; 对应的嵌入参数具有与当前模型中使用的不同的vocab：</target>
        </trans-unit>
        <trans-unit id="584c6949a375264781ffa7a3f30012c7d4ac99ba" translate="yes" xml:space="preserve">
          <source>Warm-start all weights but the parameters corresponding to &lt;code&gt;sc_vocab_file&lt;/code&gt; have a different vocab from the one used in current checkpoint and the parameters corresponding to &lt;code&gt;sc_vocab_list&lt;/code&gt; have a different name from the current checkpoint:</source>
          <target state="translated">热启动所有权重，但与 &lt;code&gt;sc_vocab_file&lt;/code&gt; 对应的参数具有与当前检查点所用的不同的vocab，与 &lt;code&gt;sc_vocab_list&lt;/code&gt; 对应的参数与当前检查点所使用的名称不同：</target>
        </trans-unit>
        <trans-unit id="aee3196d9918b3cb93e150d394afe537d71e12b7" translate="yes" xml:space="preserve">
          <source>Warm-start all weights but the parameters corresponding to &lt;code&gt;sc_vocab_file&lt;/code&gt; have a different vocab from the one used in current checkpoint, and only 100 of those entries were used:</source>
          <target state="translated">热启动所有权重，但与 &lt;code&gt;sc_vocab_file&lt;/code&gt; 对应的参数具有与当前检查点所用的vocab不同的vocab，并且仅使用了其中100个条目：</target>
        </trans-unit>
        <trans-unit id="cdac2057c8ebeb309f57754501e81125700d9646" translate="yes" xml:space="preserve">
          <source>Warm-start all weights in the model (input layer and hidden weights). Either the directory or a specific checkpoint can be provided (in the case of the former, the latest checkpoint will be used):</source>
          <target state="translated">热启动模型中的所有权重(输入层和隐藏权重)。可以提供目录或特定的检查点(在前者的情况下,将使用最新的检查点)。</target>
        </trans-unit>
        <trans-unit id="5b13568e15d19ce108f966610dd72d6f433fdac5" translate="yes" xml:space="preserve">
          <source>Warm-start non-TRAINABLE variables &quot;v1&quot;, &quot;v1/Momentum&quot;, and &quot;v2&quot; but not &quot;v2/momentum&quot;:</source>
          <target state="translated">热启动非可训练变量 &quot;v1&quot;、&quot;v1/动量 &quot;和 &quot;v2&quot;,但不是 &quot;v2/动量&quot;。</target>
        </trans-unit>
        <trans-unit id="a2a6608bb90e8edcca0d24db3c7888d49894818d" translate="yes" xml:space="preserve">
          <source>Warm-start only &lt;code&gt;sc_vocab_file&lt;/code&gt; embeddings (and no other variables), which have a different vocab from the one used in the current model:</source>
          <target state="translated">仅热启动 &lt;code&gt;sc_vocab_file&lt;/code&gt; 嵌入（无其他变量），其嵌入词与当前模型中使用的嵌入词不同：</target>
        </trans-unit>
        <trans-unit id="8454c4aaa6884d63b21f47b182bea32dff06b1d9" translate="yes" xml:space="preserve">
          <source>Warm-start only the embeddings (input layer):</source>
          <target state="translated">只对嵌入物(输入层)进行暖启动。</target>
        </trans-unit>
        <trans-unit id="053dd1f00688a5e1c6f7bb11994384338437241d" translate="yes" xml:space="preserve">
          <source>Warm-starts a model using the given settings.</source>
          <target state="translated">使用给定的设置暖启动模型。</target>
        </trans-unit>
        <trans-unit id="a935669a6f94647cbab43dac31886ff159458161" translate="yes" xml:space="preserve">
          <source>Warning class expected to be triggered.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="005fd2adc416b2f84c9268603969a31365422ae0" translate="yes" xml:space="preserve">
          <source>We add forget_bias (default: 1) to the biases of the forget gate in order to reduce the scale of forgetting in the beginning of the training.</source>
          <target state="translated">我们在遗忘门的偏置上增加了 forget_bias(默认:1),以减少训练之初的遗忘规模。</target>
        </trans-unit>
        <trans-unit id="ea902771c0c609fef23987d09209f89de6ea6624" translate="yes" xml:space="preserve">
          <source>We assume that the word frequencies follow Zipf's law (s=1) to derive a numerical approximation of frequency(rank):</source>
          <target state="translated">我们假设词频遵循Zipf定律(s=1),得出频率(rank)的数字近似值。</target>
        </trans-unit>
        <trans-unit id="ee7344fa99f19a3db805ed04f05f653d28a573dc" translate="yes" xml:space="preserve">
          <source>We call it an 'accidental hit' when one of the target classes matches one of the sampled classes. This operation reports accidental hits as triples &lt;code&gt;(index, id, weight)&lt;/code&gt;, where &lt;code&gt;index&lt;/code&gt; represents the row number in &lt;code&gt;true_classes&lt;/code&gt;, &lt;code&gt;id&lt;/code&gt; represents the position in &lt;code&gt;sampled_candidates&lt;/code&gt;, and weight is &lt;code&gt;-FLOAT_MAX&lt;/code&gt;.</source>
          <target state="translated">当目标类别之一与采样类别之一匹配时，我们将其称为&amp;ldquo;意外命中&amp;rdquo;。此操作将意外命中报告为三元组 &lt;code&gt;(index, id, weight)&lt;/code&gt; ，其中 &lt;code&gt;index&lt;/code&gt; 代表 &lt;code&gt;true_classes&lt;/code&gt; 中的行号， &lt;code&gt;id&lt;/code&gt; 代表 &lt;code&gt;sampled_candidates&lt;/code&gt; 中的位置，权重为 &lt;code&gt;-FLOAT_MAX&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="3e90ccb70a2df6831216576e3e93ad8f84e29aaa" translate="yes" xml:space="preserve">
          <source>We can again draw the effect, this time using the symbols &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;+&lt;/code&gt; and &lt;code&gt;o&lt;/code&gt; to distinguish the patches:</source>
          <target state="translated">我们可以再次绘制效果，这次使用符号 &lt;code&gt;*&lt;/code&gt; ， &lt;code&gt;x&lt;/code&gt; ， &lt;code&gt;+&lt;/code&gt; 和 &lt;code&gt;o&lt;/code&gt; 来区分色块：</target>
        </trans-unit>
        <trans-unit id="3ec1d6719eaed84adca414b94403fe4b9bce8b6a" translate="yes" xml:space="preserve">
          <source>We can also, insert entire slices of a higher rank tensor all at once. For example, if we wanted to insert two slices in the first dimension of a rank-3 tensor with two matrices of new values.</source>
          <target state="translated">我们还可以,一次性插入高阶张量的整个切片。例如,如果我们想在一个3级张量的第一维插入两个新值矩阵的切片。</target>
        </trans-unit>
        <trans-unit id="75a6bd3ac0e29b2d99ddde37882f73b9484c4c89" translate="yes" xml:space="preserve">
          <source>We can compute the mean and variance of the batch</source>
          <target state="translated">我们可以计算批次的均值和方差</target>
        </trans-unit>
        <trans-unit id="141a486e547cdd8105ede80b487557001daeacfc" translate="yes" xml:space="preserve">
          <source>We can construct a CsvDataset from it as follows:</source>
          <target state="translated">我们可以用它构建一个CsvDataset,如下所示。</target>
        </trans-unit>
        <trans-unit id="798032cd2519c55922a86a65a97aa8223721d2bf" translate="yes" xml:space="preserve">
          <source>We can use arguments:</source>
          <target state="translated">我们可以使用参数。</target>
        </trans-unit>
        <trans-unit id="bfe0a32bb694084e323bdf39d8ca5b3bf5acf3b4" translate="yes" xml:space="preserve">
          <source>We first define two int64 tensors &lt;code&gt;paddings&lt;/code&gt; and &lt;code&gt;crops&lt;/code&gt; of shape &lt;code&gt;[num_spatial_dims, 2]&lt;/code&gt; based on the value of &lt;code&gt;padding&lt;/code&gt; and the spatial dimensions of the &lt;code&gt;input&lt;/code&gt;:</source>
          <target state="translated">我们首先定义两个的int64张量 &lt;code&gt;paddings&lt;/code&gt; 和 &lt;code&gt;crops&lt;/code&gt; 形状的 &lt;code&gt;[num_spatial_dims, 2]&lt;/code&gt; 基于所述值 &lt;code&gt;padding&lt;/code&gt; 和的空间尺寸 &lt;code&gt;input&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="c362e7cae577801d3a077b086c30ca6fb0a66da0" translate="yes" xml:space="preserve">
          <source>We first solve &lt;code&gt;x_0 = A_00.solve(y_0)&lt;/code&gt;. Proceeding inductively, we solve for &lt;code&gt;x_k&lt;/code&gt;, &lt;code&gt;k = 1..n&lt;/code&gt;, given &lt;code&gt;x_0..x_(k-1)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d6f96925005fe200b4a2292648a063218b208f4" translate="yes" xml:space="preserve">
          <source>We keep track of which flag is defined by which module so that we can later sort the flags by module.</source>
          <target state="translated">我们跟踪哪个标志是由哪个模块定义的,这样我们以后就可以按模块对标志进行分类。</target>
        </trans-unit>
        <trans-unit id="e09363c49b193f1ac1770f115d6d8136f2ddc43c" translate="yes" xml:space="preserve">
          <source>We next use the scale_factor to adjust min_range and max_range as follows:</source>
          <target state="translated">接下来我们使用scale_factor来调整min_range和max_range,具体如下。</target>
        </trans-unit>
        <trans-unit id="6d1134ee5b3f5d2c7da4505ac717468ba6967616" translate="yes" xml:space="preserve">
          <source>We presuppose that the &lt;code&gt;sampled_candidates&lt;/code&gt; are unique.</source>
          <target state="translated">我们假设 &lt;code&gt;sampled_candidates&lt;/code&gt; 是唯一的。</target>
        </trans-unit>
        <trans-unit id="f093997d3edad673438d9d10c06d984c5a8a5f76" translate="yes" xml:space="preserve">
          <source>We recommend that descendants of &lt;code&gt;Layer&lt;/code&gt; implement the following methods:</source>
          <target state="translated">我们建议 &lt;code&gt;Layer&lt;/code&gt; 的后代实现以下方法：</target>
        </trans-unit>
        <trans-unit id="bd2045e3591e6b6a7950e1ad85e808c50d3fcbe0" translate="yes" xml:space="preserve">
          <source>We recommend using &lt;a href=&quot;https://github.com/tensorflow/io&quot;&gt;https://github.com/tensorflow/io&lt;/a&gt; to load your HDF5 data into a tf.data Dataset and passing that dataset to Keras.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="664f0b931d5bb248f3df03fa1a45aa162e8bb7fc" translate="yes" xml:space="preserve">
          <source>We recommend using https://github.com/tensorflow/io to load your HDF5 data into a tf.data Dataset and passing that dataset to Keras.</source>
          <target state="translated">我们建议使用https://github.com/tensorflow/io,将您的HDF5数据加载到tf.dataset中,并将该数据集传递给Keras。</target>
        </trans-unit>
        <trans-unit id="e6b1e174afc80b9d454ce2447d6b106dd858888d" translate="yes" xml:space="preserve">
          <source>We retrieve the information from the GCE APIs every time this method is called.</source>
          <target state="translated">每次调用这个方法时,我们都会从GCE API中检索信息。</target>
        </trans-unit>
        <trans-unit id="2e18048f9e04e3012f6199dfe11668988e00f3d9" translate="yes" xml:space="preserve">
          <source>We retrieve the information from the Kubernetes master every time this method is called.</source>
          <target state="translated">每次调用这个方法时,我们都会从Kubernetes主站中检索信息。</target>
        </trans-unit>
        <trans-unit id="ee86e3990a38d113aeda3e686a64aed6a6c21a17" translate="yes" xml:space="preserve">
          <source>We specify the size-related attributes as:</source>
          <target state="translated">我们将尺寸相关的属性指定为。</target>
        </trans-unit>
        <trans-unit id="88a4bbb7071448507fb5ea156b09b64279b62233" translate="yes" xml:space="preserve">
          <source>We use the following notation for (complex) matrix and right-hand sides in the batch:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d371d0dcdb3a29109c4aec0bd9662ca4cc225c96" translate="yes" xml:space="preserve">
          <source>We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers).</source>
          <target state="translated">我们将假设输入的数据集是按全局批处理大小进行分批的。在这个假设下,我们将尽最大努力将每个批次划分给所有的复制体(一个或多个工作者)。</target>
        </trans-unit>
        <trans-unit id="c1e896bf769945f02ba4270d6713548acfff861f" translate="yes" xml:space="preserve">
          <source>Web-safe means that the encoder uses - and _ instead of + and /.</source>
          <target state="translated">网络安全意味着编码器使用-和_代替+和/。</target>
        </trans-unit>
        <trans-unit id="a5ecd420b68c6ca62b2880bcb67d127d7527fd45" translate="yes" xml:space="preserve">
          <source>Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, some entries in &lt;code&gt;layer.updates&lt;/code&gt; may be dependent on &lt;code&gt;a&lt;/code&gt; and some on &lt;code&gt;b&lt;/code&gt;. This method automatically keeps track of dependencies.</source>
          <target state="translated">权重更新（例如，BatchNormalization层中移动平均值和方差的更新）可能取决于调用层时传递的输入。因此，当在不同的输入 &lt;code&gt;a&lt;/code&gt; 和 &lt;code&gt;b&lt;/code&gt; 上重用同一层时， &lt;code&gt;layer.updates&lt;/code&gt; 中的某些条目可能依赖于 &lt;code&gt;a&lt;/code&gt; ,而某些依赖于 &lt;code&gt;b&lt;/code&gt; 。此方法自动跟踪依赖关系。</target>
        </trans-unit>
        <trans-unit id="012994d31dee39cea10e34d13b123636095aef6d" translate="yes" xml:space="preserve">
          <source>Weighted loss &lt;code&gt;Tensor&lt;/code&gt; of the same type as &lt;code&gt;logits&lt;/code&gt;. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;NONE&lt;/code&gt;, this has shape &lt;code&gt;[batch_size]&lt;/code&gt;; otherwise, it is scalar.</source>
          <target state="translated">与 &lt;code&gt;logits&lt;/code&gt; 类型相同的加权损失 &lt;code&gt;Tensor&lt;/code&gt; 。如果 &lt;code&gt;reduction&lt;/code&gt; 为 &lt;code&gt;NONE&lt;/code&gt; ，则其形状为 &lt;code&gt;[batch_size]&lt;/code&gt; ；否则，它是标量。</target>
        </trans-unit>
        <trans-unit id="9d1005c1318b48c01558d66a2ac013b81040339a" translate="yes" xml:space="preserve">
          <source>Weighted loss &lt;code&gt;Tensor&lt;/code&gt; of the same type as &lt;code&gt;logits&lt;/code&gt;. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;NONE&lt;/code&gt;, this has the same shape as &lt;code&gt;labels&lt;/code&gt;; otherwise, it is scalar.</source>
          <target state="translated">与 &lt;code&gt;logits&lt;/code&gt; 类型相同的加权损失 &lt;code&gt;Tensor&lt;/code&gt; 。如果 &lt;code&gt;reduction&lt;/code&gt; 为 &lt;code&gt;NONE&lt;/code&gt; ，则其形状与 &lt;code&gt;labels&lt;/code&gt; 相同；否则，它是标量。</target>
        </trans-unit>
        <trans-unit id="834de6c05706f24229c420c387a2f8167f0b45d5" translate="yes" xml:space="preserve">
          <source>Weighted loss &lt;code&gt;Tensor&lt;/code&gt; of the same type as &lt;code&gt;logits&lt;/code&gt;. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;NONE&lt;/code&gt;, this has the same shape as &lt;code&gt;logits&lt;/code&gt;; otherwise, it is scalar.</source>
          <target state="translated">与 &lt;code&gt;logits&lt;/code&gt; 类型相同的加权损失 &lt;code&gt;Tensor&lt;/code&gt; 。如果 &lt;code&gt;reduction&lt;/code&gt; 是 &lt;code&gt;NONE&lt;/code&gt; ，这具有相同的形状 &lt;code&gt;logits&lt;/code&gt; ; 否则，它是标量。</target>
        </trans-unit>
        <trans-unit id="b075652dc6120aaf1d41f8a4dc2daf694ffa4a69" translate="yes" xml:space="preserve">
          <source>Weighted loss &lt;code&gt;Tensor&lt;/code&gt; of the same type as &lt;code&gt;losses&lt;/code&gt;. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;NONE&lt;/code&gt;, this has the same shape as &lt;code&gt;losses&lt;/code&gt;; otherwise, it is scalar.</source>
          <target state="translated">与 &lt;code&gt;losses&lt;/code&gt; 同类型的加权损失 &lt;code&gt;Tensor&lt;/code&gt; 。如果 &lt;code&gt;reduction&lt;/code&gt; 为 &lt;code&gt;NONE&lt;/code&gt; ，则与 &lt;code&gt;losses&lt;/code&gt; 具有相同的形状; 否则，它是标量。</target>
        </trans-unit>
        <trans-unit id="f8f11d4c7a3da229788db34932231d37e05fbe0b" translate="yes" xml:space="preserve">
          <source>Weighted loss float &lt;code&gt;Tensor&lt;/code&gt;. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;NONE&lt;/code&gt;, this has shape &lt;code&gt;[batch_size, d0, .. dN-1]&lt;/code&gt;; otherwise, it is scalar. (Note &lt;code&gt;dN-1&lt;/code&gt; because all loss functions reduce by 1 dimension, usually axis=-1.)</source>
          <target state="translated">加权亏损浮动 &lt;code&gt;Tensor&lt;/code&gt; 。如果 &lt;code&gt;reduction&lt;/code&gt; 为 &lt;code&gt;NONE&lt;/code&gt; ，则其形状为 &lt;code&gt;[batch_size, d0, .. dN-1]&lt;/code&gt; ；否则，它是标量。（请注意 &lt;code&gt;dN-1&lt;/code&gt; ,因为所有损失函数都会减少1维，通常为axis = -1。）</target>
        </trans-unit>
        <trans-unit id="37a14c45a506771a19d570992ffb74e2a8ababd3" translate="yes" xml:space="preserve">
          <source>Weighted loss float &lt;code&gt;Tensor&lt;/code&gt;. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;NONE&lt;/code&gt;, this has the same shape as &lt;code&gt;labels&lt;/code&gt;; otherwise, it is scalar.</source>
          <target state="translated">加权亏损浮动 &lt;code&gt;Tensor&lt;/code&gt; 。如果 &lt;code&gt;reduction&lt;/code&gt; 为 &lt;code&gt;NONE&lt;/code&gt; ，则其形状与 &lt;code&gt;labels&lt;/code&gt; 相同；否则，它是标量。</target>
        </trans-unit>
        <trans-unit id="23f94c92282719a4052aebc7d476aba406e80f05" translate="yes" xml:space="preserve">
          <source>Weights values as a list of numpy arrays.</source>
          <target state="translated">将数值加权为numpy数组的列表。</target>
        </trans-unit>
        <trans-unit id="79699365f3dd7ea5c60a1201001103f0e4d8e414" translate="yes" xml:space="preserve">
          <source>What &lt;code&gt;master&lt;/code&gt; string to use</source>
          <target state="translated">使用什么 &lt;code&gt;master&lt;/code&gt; 字符串</target>
        </trans-unit>
        <trans-unit id="8126fc60320a74dd637a7bb9a583488f0b14222c" translate="yes" xml:space="preserve">
          <source>What happens in &lt;code&gt;adapt&lt;/code&gt;: Compute mean and variance of the data and store them as the layer's weights. &lt;code&gt;adapt&lt;/code&gt; should be called before &lt;code&gt;fit&lt;/code&gt;, &lt;code&gt;evaluate&lt;/code&gt;, or &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">什么发生在 &lt;code&gt;adapt&lt;/code&gt; ：计算平均值和数据的变化，并将它们存储为层的权重。 &lt;code&gt;adapt&lt;/code&gt; 应该在 &lt;code&gt;fit&lt;/code&gt; ， &lt;code&gt;evaluate&lt;/code&gt; 或 &lt;code&gt;predict&lt;/code&gt; 之前进行。</target>
        </trans-unit>
        <trans-unit id="f4112aa72544bbd54c25bceb58f7f58b932e32d8" translate="yes" xml:space="preserve">
          <source>What to return in test phase (tensor or callable that returns a tensor).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="17f24426650f12852c89205be844ef319f76644b" translate="yes" xml:space="preserve">
          <source>What to return in train phase (tensor or callable that returns a tensor).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="def8b9f5f06fb4c06699fb40ca04320258ae46d4" translate="yes" xml:space="preserve">
          <source>What to return otherwise (tensor or callable that returns a tensor).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e11f03a4f4c3dbc0b65542222647d127c17f0502" translate="yes" xml:space="preserve">
          <source>What's under the hood of this method, when we say the &lt;a href=&quot;../../../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; instance - &lt;code&gt;dataset&lt;/code&gt; - gets distributed? It depends on how you set the &lt;a href=&quot;../../../../data/experimental/autoshardpolicy&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy&lt;/code&gt;&lt;/a&gt; through &lt;a href=&quot;../../../../data/experimental/distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;. By default, it is set to &lt;a href=&quot;../../../../data/experimental/autoshardpolicy#AUTO&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy.AUTO&lt;/code&gt;&lt;/a&gt;. In a multi-worker setting, we will first attempt to distribute &lt;code&gt;dataset&lt;/code&gt; by detecting whether &lt;code&gt;dataset&lt;/code&gt; is being created out of reader datasets (e.g. &lt;a href=&quot;../../../../data/tfrecorddataset&quot;&gt;&lt;code&gt;tf.data.TFRecordDataset&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../../../data/textlinedataset&quot;&gt;&lt;code&gt;tf.data.TextLineDataset&lt;/code&gt;&lt;/a&gt;, etc.) and if so, try to shard the input files. Note that there has to be at least one input file per worker. If you have less than one input file per worker, we suggest that you disable dataset sharding across workers, by setting the &lt;a href=&quot;../../../../data/experimental/distributeoptions#auto_shard_policy&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions.auto_shard_policy&lt;/code&gt;&lt;/a&gt; to be &lt;a href=&quot;../../../../data/experimental/autoshardpolicy#OFF&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy.OFF&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f8e0522d952680a4b2c6fbddea797a2677b1547" translate="yes" xml:space="preserve">
          <source>What's under the hood of this method, when we say the &lt;a href=&quot;../../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; instance - &lt;code&gt;dataset&lt;/code&gt; - gets distributed? It depends on how you set the &lt;a href=&quot;../../../data/experimental/autoshardpolicy&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy&lt;/code&gt;&lt;/a&gt; through &lt;a href=&quot;../../../data/experimental/distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;. By default, it is set to &lt;a href=&quot;../../../data/experimental/autoshardpolicy#AUTO&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy.AUTO&lt;/code&gt;&lt;/a&gt;. In a multi-worker setting, we will first attempt to distribute &lt;code&gt;dataset&lt;/code&gt; by detecting whether &lt;code&gt;dataset&lt;/code&gt; is being created out of reader datasets (e.g. &lt;a href=&quot;../../../data/tfrecorddataset&quot;&gt;&lt;code&gt;tf.data.TFRecordDataset&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../../data/textlinedataset&quot;&gt;&lt;code&gt;tf.data.TextLineDataset&lt;/code&gt;&lt;/a&gt;, etc.) and if so, try to shard the input files. Note that there has to be at least one input file per worker. If you have less than one input file per worker, we suggest that you disable dataset sharding across workers, by setting the &lt;a href=&quot;../../../data/experimental/distributeoptions#auto_shard_policy&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions.auto_shard_policy&lt;/code&gt;&lt;/a&gt; to be &lt;a href=&quot;../../../data/experimental/autoshardpolicy#OFF&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy.OFF&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2a0d7396d4c78509ccd91f8c0cfaa89f6bac9680" translate="yes" xml:space="preserve">
          <source>What's under the hood of this method, when we say the &lt;a href=&quot;../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; instance - &lt;code&gt;dataset&lt;/code&gt; - gets distributed? It depends on how you set the &lt;a href=&quot;../../data/experimental/autoshardpolicy&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy&lt;/code&gt;&lt;/a&gt; through &lt;a href=&quot;../../data/experimental/distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;. By default, it is set to &lt;a href=&quot;../../data/experimental/autoshardpolicy#AUTO&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy.AUTO&lt;/code&gt;&lt;/a&gt;. In a multi-worker setting, we will first attempt to distribute &lt;code&gt;dataset&lt;/code&gt; by detecting whether &lt;code&gt;dataset&lt;/code&gt; is being created out of reader datasets (e.g. &lt;a href=&quot;../../data/tfrecorddataset&quot;&gt;&lt;code&gt;tf.data.TFRecordDataset&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../data/textlinedataset&quot;&gt;&lt;code&gt;tf.data.TextLineDataset&lt;/code&gt;&lt;/a&gt;, etc.) and if so, try to shard the input files. Note that there has to be at least one input file per worker. If you have less than one input file per worker, we suggest that you disable dataset sharding across workers, by setting the &lt;a href=&quot;../../data/experimental/distributeoptions#auto_shard_policy&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions.auto_shard_policy&lt;/code&gt;&lt;/a&gt; to be &lt;a href=&quot;../../data/experimental/autoshardpolicy#OFF&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy.OFF&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e73d3cb0a89b521b7e9847b33f0048e348f89d5" translate="yes" xml:space="preserve">
          <source>What's under the hood of this method, when we say the &lt;a href=&quot;../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; instance - &lt;code&gt;dataset&lt;/code&gt; - gets distributed? It depends on how you set the &lt;a href=&quot;../data/experimental/autoshardpolicy&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy&lt;/code&gt;&lt;/a&gt; through &lt;a href=&quot;../data/experimental/distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;. By default, it is set to &lt;a href=&quot;../data/experimental/autoshardpolicy#AUTO&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy.AUTO&lt;/code&gt;&lt;/a&gt;. In a multi-worker setting, we will first attempt to distribute &lt;code&gt;dataset&lt;/code&gt; by detecting whether &lt;code&gt;dataset&lt;/code&gt; is being created out of reader datasets (e.g. &lt;a href=&quot;../data/tfrecorddataset&quot;&gt;&lt;code&gt;tf.data.TFRecordDataset&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../data/textlinedataset&quot;&gt;&lt;code&gt;tf.data.TextLineDataset&lt;/code&gt;&lt;/a&gt;, etc.) and if so, try to shard the input files. Note that there has to be at least one input file per worker. If you have less than one input file per worker, we suggest that you disable dataset sharding across workers, by setting the &lt;a href=&quot;../data/experimental/distributeoptions#auto_shard_policy&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions.auto_shard_policy&lt;/code&gt;&lt;/a&gt; to be &lt;a href=&quot;../data/experimental/autoshardpolicy#OFF&quot;&gt;&lt;code&gt;tf.data.experimental.AutoShardPolicy.OFF&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4633647d41640957a472ab68c0262290690dfdb" translate="yes" xml:space="preserve">
          <source>When 'TF_CONFIG' environment variable is set, it parses cluster_spec, task_type and task_id from 'TF_CONFIG' and turns into a multi-worker strategy which mirrored models on GPUs of all machines in a cluster. In the current implementation, it uses all GPUs in a cluster and it assumes all workers have the same number of GPUs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="97233c5c3c8d20a5e9ffbdbe3b7ccb6a51db597b" translate="yes" xml:space="preserve">
          <source>When 'TF_CONFIG' environment variable is set, it parses cluster_spec, task_type and task_id from 'TF_CONFIG' and turns into a multi-worker strategy which mirrores models on GPUs of all machines in a cluster. In the current implementation, it uses all GPUs in a cluster and it assumes all workers have the same number of GPUs.</source>
          <target state="translated">当设置'TF_CONFIG'环境变量时,它会从'TF_CONFIG'中解析出cluster_spec、task_type和task_id,然后变成多工作者策略,将模型镜像到集群中所有机器的GPU上。在当前的实现中,它使用了集群中的所有GPU,并且假设所有工作者都拥有相同数量的GPU。</target>
        </trans-unit>
        <trans-unit id="f5e1d55c89f446271b947fe72a857c539201c7dc" translate="yes" xml:space="preserve">
          <source>When 'antialias' is true, the sampling filter will anti-alias the input image as well as interpolate. When downsampling an image with &lt;a href=&quot;https://en.wikipedia.org/wiki/Spatial_anti-aliasing&quot;&gt;anti-aliasing&lt;/a&gt; the sampling filter kernel is scaled in order to properly anti-alias the input image signal. 'antialias' has no effect when upsampling an image.</source>
          <target state="translated">当&amp;ldquo;抗锯齿&amp;rdquo;为true时，采样滤波器将对输入图像进行抗锯齿并进行插值。在使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Spatial_anti-aliasing&quot;&gt;抗锯齿&lt;/a&gt;对图像进行下采样时，将对采样滤波器内核进行缩放，以正确地对输入图像信号进行抗锯齿。对图像进行上采样时，&amp;ldquo;抗锯齿&amp;rdquo;无效。</target>
        </trans-unit>
        <trans-unit id="a125e391abcaadbe33801ab8eddcf23c35279113" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;True&lt;/code&gt;, additional assertions might be embedded in the graph. Default value: &lt;code&gt;False&lt;/code&gt; (i.e., no graph assertions are added).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="735a581f2af66e2fcb181fd57a9ddb799b2eb257" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;antialias&lt;/code&gt; is true, the sampling filter will anti-alias the input image as well as interpolate. When downsampling an image with &lt;a href=&quot;https://en.wikipedia.org/wiki/Spatial_anti-aliasing&quot;&gt;anti-aliasing&lt;/a&gt; the sampling filter kernel is scaled in order to properly anti-alias the input image signal. &lt;code&gt;antialias&lt;/code&gt; has no effect when upsampling an image:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9aacbccc1a065c95aea812e79f50b9fb0809ae76" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;distribute&lt;/code&gt; or &lt;code&gt;experimental_distribute.train_distribute&lt;/code&gt; and &lt;code&gt;experimental_distribute.remote_cluster&lt;/code&gt; is set, this method will start a client running on the current host which connects to the &lt;code&gt;remote_cluster&lt;/code&gt; for training and evaluation.</source>
          <target state="translated">当 &lt;code&gt;distribute&lt;/code&gt; 或 &lt;code&gt;experimental_distribute.train_distribute&lt;/code&gt; 和 &lt;code&gt;experimental_distribute.remote_cluster&lt;/code&gt; 设置，此方法将开始它连接到当前主机上运行的客户端 &lt;code&gt;remote_cluster&lt;/code&gt; 的培训和考核。</target>
        </trans-unit>
        <trans-unit id="01497840225ce1bb0dcac0f82d53eb47485d512b" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;enable&lt;/code&gt; is set to None, an appropriate value will be picked automatically. The value picked may change between TensorFlow releases.</source>
          <target state="translated">当 &lt;code&gt;enable&lt;/code&gt; 设置为None时，将自动选择适当的值。在TensorFlow版本之间可能会有所不同。</target>
        </trans-unit>
        <trans-unit id="3a92e9802f62e2ea66187929bf4db58022152e26" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;nesterov=False&lt;/code&gt;, this rule becomes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c64f1c94124dc71ee97e99b0200e5f2b295d3204" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;queues&lt;/code&gt; is not a list of &lt;code&gt;QueueBase&lt;/code&gt; objects, or when the data types of &lt;code&gt;queues&lt;/code&gt; are not all the same.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d965c5a7f6cd6a88315eeacabf229f5d784d0a97" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;rescale&lt;/code&gt; is set to a value, rescaling is applied to sample data before computing the internal data stats.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dda3a421cfde51cef62429c270613980d6f95f4a" translate="yes" xml:space="preserve">
          <source>When True, &lt;a href=&quot;function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; may generate fewer, graphs that are less specialized on input shapes.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d2203660cd3c839935256a662d4fa7b611fae192" translate="yes" xml:space="preserve">
          <source>When a &lt;a href=&quot;../../../../keras/model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt; is created inside a &lt;code&gt;strategy.scope&lt;/code&gt;, we capture this information. When high level training frameworks methods such as &lt;code&gt;model.compile&lt;/code&gt;, &lt;code&gt;model.fit&lt;/code&gt; etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in &lt;a href=&quot;https://www.tensorflow.org/tutorials/distribute/keras&quot;&gt;distributed keras tutorial&lt;/a&gt;. Note that simply calling the &lt;code&gt;model(..)&lt;/code&gt; is not impacted - only high level training framework APIs are. &lt;code&gt;model.compile&lt;/code&gt;, &lt;code&gt;model.fit&lt;/code&gt;, &lt;code&gt;model.evaluate&lt;/code&gt;, &lt;code&gt;model.predict&lt;/code&gt; and &lt;code&gt;model.save&lt;/code&gt; can all be called inside or outside the scope.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a77de95801a4cff5844b758a2ad8179bb09d2c1a" translate="yes" xml:space="preserve">
          <source>When a &lt;a href=&quot;../../../keras/model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt; is created inside a &lt;code&gt;strategy.scope&lt;/code&gt;, we capture this information. When high level training frameworks methods such as &lt;code&gt;model.compile&lt;/code&gt;, &lt;code&gt;model.fit&lt;/code&gt; etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in &lt;a href=&quot;https://www.tensorflow.org/tutorials/distribute/keras&quot;&gt;distributed keras tutorial&lt;/a&gt;. Note that simply calling the &lt;code&gt;model(..)&lt;/code&gt; is not impacted - only high level training framework APIs are. &lt;code&gt;model.compile&lt;/code&gt;, &lt;code&gt;model.fit&lt;/code&gt;, &lt;code&gt;model.evaluate&lt;/code&gt;, &lt;code&gt;model.predict&lt;/code&gt; and &lt;code&gt;model.save&lt;/code&gt; can all be called inside or outside the scope.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5bc23220d2b015520f09eca2cbd06a9bdbef9712" translate="yes" xml:space="preserve">
          <source>When a &lt;a href=&quot;../../keras/model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt; is created inside a &lt;code&gt;strategy.scope&lt;/code&gt;, we capture this information. When high level training frameworks methods such as &lt;code&gt;model.compile&lt;/code&gt;, &lt;code&gt;model.fit&lt;/code&gt; etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in &lt;a href=&quot;https://www.tensorflow.org/tutorials/distribute/keras&quot;&gt;distributed keras tutorial&lt;/a&gt;. Note that simply calling the &lt;code&gt;model(..)&lt;/code&gt; is not impacted - only high level training framework APIs are. &lt;code&gt;model.compile&lt;/code&gt;, &lt;code&gt;model.fit&lt;/code&gt;, &lt;code&gt;model.evaluate&lt;/code&gt;, &lt;code&gt;model.predict&lt;/code&gt; and &lt;code&gt;model.save&lt;/code&gt; can all be called inside or outside the scope.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8157664b0c676127bfe8a48270b035a599288e2e" translate="yes" xml:space="preserve">
          <source>When a &lt;a href=&quot;../keras/model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt; is created inside a &lt;code&gt;strategy.scope&lt;/code&gt;, we capture this information. When high level training frameworks methods such as &lt;code&gt;model.compile&lt;/code&gt;, &lt;code&gt;model.fit&lt;/code&gt; etc are then called on this model, we automatically enter the scope, as well as use this strategy to distribute the training etc. See detailed example in &lt;a href=&quot;https://www.tensorflow.org/tutorials/distribute/keras&quot;&gt;distributed keras tutorial&lt;/a&gt;. Note that simply calling the &lt;code&gt;model(..)&lt;/code&gt; is not impacted - only high level training framework APIs are. &lt;code&gt;model.compile&lt;/code&gt;, &lt;code&gt;model.fit&lt;/code&gt;, &lt;code&gt;model.evaluate&lt;/code&gt;, &lt;code&gt;model.predict&lt;/code&gt; and &lt;code&gt;model.save&lt;/code&gt; can all be called inside or outside the scope.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5d3c29ba7c1915daf444dfa9480a120b9d259cc0" translate="yes" xml:space="preserve">
          <source>When a DistributionStrategy is used, this function may only be called in a cross-replica context.</source>
          <target state="translated">当使用DistributionStrategy时,这个函数只能在交叉复制的上下文中调用。</target>
        </trans-unit>
        <trans-unit id="9ee3d0211a7961d1892863b309bfae8dc55d02c2" translate="yes" xml:space="preserve">
          <source>When a Summary op is instantiated, a SummaryDescription of associated metadata is stored in its NodeDef. This method retrieves the description.</source>
          <target state="translated">当一个 Summary op 被实例化时,相关元数据的 SummaryDescription 被存储在它的 NodeDef 中。这个方法可以检索描述。</target>
        </trans-unit>
        <trans-unit id="468fae491a93893feb36e0fc65b7a7ac2c612355" translate="yes" xml:space="preserve">
          <source>When a global &lt;a href=&quot;../../../../keras/mixed_precision/experimental/policy&quot;&gt;&lt;code&gt;tf.keras.mixed_precision.experimental.Policy&lt;/code&gt;&lt;/a&gt; is set, a Keras layer's dtype will default to the global policy instead of floatx. Layers will automatically cast inputs to the policy's compute_dtype.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="067c92a0436a2a055b3eb1915882349b4d833f6c" translate="yes" xml:space="preserve">
          <source>When a op's float-type output tensor contains any Infinity or NaN, an &lt;a href=&quot;../errors/invalidargumenterror&quot;&gt;&lt;code&gt;tf.errors.InvalidArgumentError&lt;/code&gt;&lt;/a&gt; will be thrown, with an error message that reveals the following information:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e2c0bd0de968bcd753736a8da6d2db14223d5cf" translate="yes" xml:space="preserve">
          <source>When a op's float-type output tensor contains any Infinity or NaN, an &lt;a href=&quot;../errors/invalidargumenterror&quot;&gt;&lt;code&gt;tf.errors.InvalidArgumentError&lt;/code&gt;&lt;/a&gt; will be thrown, with an error message that reveals the following information: - The type of the op that generated the tensor with bad numerics. - Data type (dtype) of the tensor. - Shape of the tensor (to the extent known at the time of eager execution or graph construction). - Name of the containing graph (if available). - (Graph mode only): The stack trace of the intra-graph op's creation, with a stack-height limit and a path-length limit for visual clarity. The stack frames that belong to the user's code (as opposed to tensorflow's internal code) are highlighted with a text arrow (&quot;-&amp;gt;&quot;). - (Eager mode only): How many of the offending tensor's elements are &lt;code&gt;Infinity&lt;/code&gt; and &lt;code&gt;NaN&lt;/code&gt;, respectively.</source>
          <target state="translated">当op的浮点型输出张量包含任何Infinity或NaN时，将引发&lt;a href=&quot;../errors/invalidargumenterror&quot;&gt; &lt;code&gt;tf.errors.InvalidArgumentError&lt;/code&gt; &lt;/a&gt;，并显示一条错误消息，其中显示以下信息：-生成带有错误数字的张量的op的类型。 -张量的数据类型（dtype）。 -张量的形状（达到急切执行或构建图时的已知程度）。 -包含图的名称（如果有）。 -（仅适用于图形模式）：图形内op的创建的堆栈跟踪，具有堆栈高度限制和路径长度限制，以使视觉清晰。属于用户代码（与tensorflow的内部代码相反）的堆栈框架以文本箭头（&amp;ldquo;-&amp;gt;&amp;rdquo;）突出显示。 -（仅适用于热切模式）：有多少张紧张量'的元素是 &lt;code&gt;Infinity&lt;/code&gt; 和 &lt;code&gt;NaN&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b898b5ad811100780dad4051c33f639fd70aa1be" translate="yes" xml:space="preserve">
          <source>When a tf.random operation is built with XLA, the implementation doesn't pass the user provided seed to the XLA compiler. As such, the XLA compiler generates a random number and uses it as a seed when compiling the operation. This implementation causes a violation of the Tensorflow defined semantics in two aspects. First, changing the value of the user defined seed doesn't change the numbers generated by the operation. Second, when a seed is not specified, running the program multiple times will generate the same numbers.</source>
          <target state="translated">当使用 XLA 构建 tf.random 操作时,实现不会将用户提供的种子传递给 XLA 编译器。因此,XLA编译器会生成一个随机数,并在编译操作时将其作为种子。这种实现在两个方面造成了对Tensorflow定义语义的违反。首先,改变用户定义的种子值并不会改变操作生成的数字。第二,当没有指定种子时,多次运行程序会产生相同的数字。</target>
        </trans-unit>
        <trans-unit id="493652c7a3597db99acb0cc5aae0cc36b4d15e47" translate="yes" xml:space="preserve">
          <source>When a trackable object is exported via &lt;a href=&quot;save&quot;&gt;&lt;code&gt;tf.saved_model.save()&lt;/code&gt;&lt;/a&gt;, all the &lt;code&gt;Asset&lt;/code&gt;s reachable from it are copied into the SavedModel assets directory. Upon loading, the assets and the serialized functions that depend on them will refer to the correct filepaths inside the SavedModel directory.</source>
          <target state="translated">通过&lt;a href=&quot;save&quot;&gt; &lt;code&gt;tf.saved_model.save()&lt;/code&gt; &lt;/a&gt;导出可跟踪对象时，从该对象可访问的所有 &lt;code&gt;Asset&lt;/code&gt; 都将被复制到SavedModel资产目录中。加载后，资产和依赖于它们的序列化函数将引用SavedModel目录内的正确文件路径。</target>
        </trans-unit>
        <trans-unit id="6f1183817b3726ec7266e15fa411e144c4d1a1bb" translate="yes" xml:space="preserve">
          <source>When accessing the value of a TensorShape dimension, use this utility, like this:</source>
          <target state="translated">当访问一个TensorShape维度的值时,使用这个实用程序,比如这样。</target>
        </trans-unit>
        <trans-unit id="5eeff351258cc34299c1acc2c35d15d6a807f5fe" translate="yes" xml:space="preserve">
          <source>When arguments have invalid value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e15681c5c4d660d694d5dcc65f824f2819119a6a" translate="yes" xml:space="preserve">
          <source>When attempting to multiply a nD tensor with a nD tensor, it reproduces the Theano behavior. (e.g. &lt;code&gt;(2, 3) * (4, 3, 5) -&amp;gt; (2, 4, 5)&lt;/code&gt;)</source>
          <target state="translated">尝试将nD张量与nD张量相乘时，它会重现Theano行为。（例如 &lt;code&gt;(2, 3) * (4, 3, 5) -&amp;gt; (2, 4, 5)&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="c84b6c98a486c1f1ed40714b3413c2d05ebb290e" translate="yes" xml:space="preserve">
          <source>When attempting to normalize on an empty ensemble or an ensemble of trees which have no splits. Or when attempting to normalize and feature importances have negative values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c06f27a3b19deaf8176a82559892b7885f9b161" translate="yes" xml:space="preserve">
          <source>When autotuning is enabled (through &lt;code&gt;autotune&lt;/code&gt;), determines the CPU budget to use. Values greater than the number of schedulable CPU cores are allowed but may result in CPU contention. If None, defaults to the number of schedulable CPU cores.</source>
          <target state="translated">启用自动调整功能后（通过 &lt;code&gt;autotune&lt;/code&gt; ），确定要使用的CPU预算。允许的值大于可调度CPU内核的数量，但可能导致CPU争用。如果为None，则默认为可调度CPU内核数。</target>
        </trans-unit>
        <trans-unit id="aa75a73b6c69b49d6e19ddace76d0dc9e6d20863" translate="yes" xml:space="preserve">
          <source>When autotuning is enabled (through &lt;code&gt;autotune&lt;/code&gt;), determines whether to also autotune buffer sizes for datasets with parallelism. If None, defaults to False.</source>
          <target state="translated">启用自动调整（通过 &lt;code&gt;autotune&lt;/code&gt; ）后，确定是否还自动调整具有并行性的数据集的缓冲区大小。如果为None，则默认为False。</target>
        </trans-unit>
        <trans-unit id="283f72551210c9459423ecb79011f0f8519629e6" translate="yes" xml:space="preserve">
          <source>When autotuning is enabled (through &lt;code&gt;autotune&lt;/code&gt;), identifies the algorithm to use for the autotuning optimization.</source>
          <target state="translated">启用自动调整功能后（通过 &lt;code&gt;autotune&lt;/code&gt; ），标识用于自动调整优化的算法。</target>
        </trans-unit>
        <trans-unit id="e6deb11b689fcef12dab268b644289f6586fd39c" translate="yes" xml:space="preserve">
          <source>When both &lt;code&gt;squeeze_dims&lt;/code&gt; and &lt;code&gt;axis&lt;/code&gt; are specified.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="32eb733d84fe74265b206fb4da02dbc0b66960e1" translate="yes" xml:space="preserve">
          <source>When building a complex model that uses many queues it is often difficult to gather all the queue runners that need to be run. This convenience function allows you to add a queue runner to a well known collection in the graph.</source>
          <target state="translated">当建立一个使用许多队列的复杂模型时,通常很难收集所有需要运行的队列运行器。这个方便的函数可以让你把一个队列运行器添加到图中的一个已知集合中。</target>
        </trans-unit>
        <trans-unit id="e5bcf632e0c577d3a09737c01de056496ad7c080" translate="yes" xml:space="preserve">
          <source>When building a machine learning model it is often convenient to distinguish between variables holding the trainable model parameters and other variables such as a &lt;code&gt;global step&lt;/code&gt; variable used to count training steps. To make this easier, the variable constructor supports a &lt;code&gt;trainable=&amp;lt;bool&amp;gt;&lt;/code&gt; parameter. If &lt;code&gt;True&lt;/code&gt;, the new variable is also added to the graph collection &lt;code&gt;GraphKeys.TRAINABLE_VARIABLES&lt;/code&gt;. The convenience function &lt;code&gt;trainable_variables()&lt;/code&gt; returns the contents of this collection. The various &lt;code&gt;Optimizer&lt;/code&gt; classes use this collection as the default list of variables to optimize.</source>
          <target state="translated">在构建机器学习模型时，通常很容易区分包含可训练模型参数的变量和其他变量，例如用于计算训练步骤的 &lt;code&gt;global step&lt;/code&gt; 变量。为了使之更容易，变量构造函数支持 &lt;code&gt;trainable=&amp;lt;bool&amp;gt;&lt;/code&gt; 参数。如果为 &lt;code&gt;True&lt;/code&gt; ，则还将新变量添加到图形集合 &lt;code&gt;GraphKeys.TRAINABLE_VARIABLES&lt;/code&gt; 中。便捷函数 &lt;code&gt;trainable_variables()&lt;/code&gt; 返回此集合的内容。各种 &lt;code&gt;Optimizer&lt;/code&gt; 类都将此集合用作要优化的默认变量列表。</target>
        </trans-unit>
        <trans-unit id="68a44f742db33e692724433083d54a9691c316e1" translate="yes" xml:space="preserve">
          <source>When building a machine learning model it is often convenient to distinguish between variables holding trainable model parameters and other variables such as a &lt;code&gt;step&lt;/code&gt; variable used to count training steps. To make this easier, the variable constructor supports a &lt;code&gt;trainable=&amp;lt;bool&amp;gt;&lt;/code&gt; parameter. &lt;a href=&quot;gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt; watches trainable variables by default:</source>
          <target state="translated">建立机器学习模型时，通常很容易区分具有可训练模型参数的变量和其他变量，例如用于计算训练步骤的 &lt;code&gt;step&lt;/code&gt; 变量。为了使之更容易，变量构造函数支持 &lt;code&gt;trainable=&amp;lt;bool&amp;gt;&lt;/code&gt; 参数。&lt;a href=&quot;gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;默认情况下监视可训练变量：</target>
        </trans-unit>
        <trans-unit id="5eef9f253a5e93ec36275a968646b63d028cda2b" translate="yes" xml:space="preserve">
          <source>When building an eager SparseTensor if &lt;code&gt;dense_shape&lt;/code&gt; is unknown or contains unknown elements (None or -1).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13d3fbe26af35daa9bb0d8a47a6e4b1e39a6da13" translate="yes" xml:space="preserve">
          <source>When building ops to compute gradients, the TensorFlow gradient system will return an error when trying to lookup the gradient of this op, because no gradient must ever be registered for this function. This op exists to prevent subtle bugs from silently returning unimplemented gradients in some corner cases.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="11e07c6fbcce2a9ab5d49a2c9fc58743be530d36" translate="yes" xml:space="preserve">
          <source>When building ops to compute gradients, this op prevents the contribution of its inputs to be taken into account. Normally, the gradient generator adds ops to a graph to compute the derivatives of a specified 'loss' by recursively finding out inputs that contributed to its computation. If you insert this op in the graph it inputs are masked from the gradient generator. They are not taken into account for computing gradients.</source>
          <target state="translated">当构建计算梯度的操作时,这个操作可以防止输入的贡献被考虑在内,通常情况下,梯度生成器在图上添加操作来计算指定 &quot;损失 &quot;的导数,通过递归找出对其计算有贡献的输入。通常情况下,梯度生成器通过递归地找出对其计算有贡献的输入来计算指定 &quot;损失 &quot;的导数,从而将运算添加到图中。如果你在图形中插入这个操作,它的输入就会被梯度发生器屏蔽。在计算梯度时,它们不会被考虑在内。</target>
        </trans-unit>
        <trans-unit id="0d35f4de41040fd9e01e658fd5001879a66a9c2d" translate="yes" xml:space="preserve">
          <source>When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to &lt;code&gt;.cache()&lt;/code&gt; will have no effect until the cache file is removed or the filename is changed.</source>
          <target state="translated">缓存到文件时，缓存的数据将在每次运行中保留。即使是第一次遍历数据，也将从缓存文件中读取。直到删除缓存文件或更改文件名，在调用 &lt;code&gt;.cache()&lt;/code&gt; 之前更改输入管道才有效。</target>
        </trans-unit>
        <trans-unit id="b38df97ed344603f1f1650ff0bfe2600641ba555" translate="yes" xml:space="preserve">
          <source>When calculating the gradient of a weighted loss contributions from both &lt;code&gt;losses&lt;/code&gt; and &lt;code&gt;weights&lt;/code&gt; are considered. If your &lt;code&gt;weights&lt;/code&gt; depend on some model parameters but you do not want this to affect the loss gradient, you need to apply &lt;a href=&quot;../../../stop_gradient&quot;&gt;&lt;code&gt;tf.stop_gradient&lt;/code&gt;&lt;/a&gt; to &lt;code&gt;weights&lt;/code&gt; before passing them to &lt;code&gt;compute_weighted_loss&lt;/code&gt;.</source>
          <target state="translated">在计算加权损失的梯度时，要考虑来自 &lt;code&gt;losses&lt;/code&gt; 和 &lt;code&gt;weights&lt;/code&gt; 贡献。如果 &lt;code&gt;weights&lt;/code&gt; 取决于某些模型参数，但又不希望它影响损耗梯度，则需要&lt;a href=&quot;../../../stop_gradient&quot;&gt; &lt;code&gt;tf.stop_gradient&lt;/code&gt; &lt;/a&gt;应用于 &lt;code&gt;weights&lt;/code&gt; 然后再将其传递给 &lt;code&gt;compute_weighted_loss&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="05f81ef4361feddebda74fb8dc805bcdaf94d0cf" translate="yes" xml:space="preserve">
          <source>When called inside a strategy.run call and input is not directly taken from the args of the &lt;code&gt;strategy.run&lt;/code&gt; call. Also if the size of any sequence in &lt;code&gt;features&lt;/code&gt; does not match corresponding sequence in &lt;code&gt;feature_config&lt;/code&gt;. Similarly for &lt;code&gt;weights&lt;/code&gt;, if not &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ebd806d2188a9c9df38773d42df546a4427672df" translate="yes" xml:space="preserve">
          <source>When called inside a strategy.run call and inside XLA control flow.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e31beadd763eb7afc2237f8c90a1ca311bfd077a" translate="yes" xml:space="preserve">
          <source>When called, the default graph is the one that will be launched in the session. The hook can modify the graph by adding new operations to it. After the &lt;code&gt;begin()&lt;/code&gt; call the graph will be finalized and the other callbacks can not modify the graph anymore. Second call of &lt;code&gt;begin()&lt;/code&gt; on the same graph, should not change the graph.</source>
          <target state="translated">调用时，默认图形是将在会话中启动的图形。挂钩可以通过向图形添加新操作来修改图形。在 &lt;code&gt;begin()&lt;/code&gt; 调用之后，该图将被完成，其他回调无法再修改该图。在同一图形上对 &lt;code&gt;begin()&lt;/code&gt; 的第二次调用不应更改图形。</target>
        </trans-unit>
        <trans-unit id="97ac3a446111c899472a5c5300f96b04aabb3a5c" translate="yes" xml:space="preserve">
          <source>When checkpointing your model, you should include your &lt;a href=&quot;tpuembedding&quot;&gt;&lt;code&gt;tf.tpu.experimental.embedding.TPUEmbedding&lt;/code&gt;&lt;/a&gt; object in the checkpoint. It is a trackable object and saving it will save the embedding tables and their optimizer slot variables:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f92e2d94d9415d9d31e53c6519e2640ef5146f3" translate="yes" xml:space="preserve">
          <source>When combining specs, &lt;code&gt;dev&lt;/code&gt; will take precedence over the current spec. So for instance:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d7e4d9de8d529939e37509a2b98e74b3e5ae079" translate="yes" xml:space="preserve">
          <source>When combining specs, &lt;code&gt;dev&lt;/code&gt; will take precidence over the current spec. So for instance:</source>
          <target state="translated">结合规格时， &lt;code&gt;dev&lt;/code&gt; 将优先考虑当前规格。因此，例如：</target>
        </trans-unit>
        <trans-unit id="02d1051ddba9ce90040906dafc1d04ba61223ccf" translate="yes" xml:space="preserve">
          <source>When constructed with a &lt;a href=&quot;../session&quot;&gt;&lt;code&gt;tf.compat.v1.Session&lt;/code&gt;&lt;/a&gt; parameter, a &lt;code&gt;FileWriter&lt;/code&gt; instead forms a compatibility layer over new graph-based summaries (&lt;code&gt;tf.contrib.summary&lt;/code&gt;) to facilitate the use of new summary writing with pre-existing code that expects a &lt;code&gt;FileWriter&lt;/code&gt; instance.</source>
          <target state="translated">当使用&lt;a href=&quot;../session&quot;&gt; &lt;code&gt;tf.compat.v1.Session&lt;/code&gt; &lt;/a&gt;参数构造时， &lt;code&gt;FileWriter&lt;/code&gt; 会在新的基于图的摘要（ &lt;code&gt;tf.contrib.summary&lt;/code&gt; ）上形成一个兼容层，以利于使用新的摘要编写以及期望 &lt;code&gt;FileWriter&lt;/code&gt; 的现有代码实例。</target>
        </trans-unit>
        <trans-unit id="e77d83f8aae8dd347eba34fe1096849a87e1d28d" translate="yes" xml:space="preserve">
          <source>When constructed with a &lt;a href=&quot;../session&quot;&gt;&lt;code&gt;tf.compat.v1.Session&lt;/code&gt;&lt;/a&gt; parameter, a &lt;code&gt;FileWriter&lt;/code&gt; instead forms a compatibility layer over new graph-based summaries to facilitate the use of new summary writing with pre-existing code that expects a &lt;code&gt;FileWriter&lt;/code&gt; instance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aa47f312128ac2a3285768f283bc01e3ade90f81" translate="yes" xml:space="preserve">
          <source>When consuming SavedModels asynchronously (the producer is a separate process), the SavedModel directory will appear before all files have been written, and &lt;a href=&quot;load&quot;&gt;&lt;code&gt;tf.saved_model.load&lt;/code&gt;&lt;/a&gt; will fail if pointed at an incomplete SavedModel. Rather than checking for the directory, check for &quot;saved_model_dir/saved_model.pb&quot;. This file is written atomically as the last &lt;a href=&quot;save&quot;&gt;&lt;code&gt;tf.saved_model.save&lt;/code&gt;&lt;/a&gt; file operation.</source>
          <target state="translated">异步使用SavedModels（生产者是一个单独的过程）时，SavedModel目录将在所有文件写入之前出现，并且&lt;a href=&quot;load&quot;&gt; &lt;code&gt;tf.saved_model.load&lt;/code&gt; &lt;/a&gt;如果指向不完整的SavedModel将会失败。而不是检查目录，而是检查&amp;ldquo; saved_model_dir / saved_model.pb&amp;rdquo;。该文件被自动写入为最后的&lt;a href=&quot;save&quot;&gt; &lt;code&gt;tf.saved_model.save&lt;/code&gt; &lt;/a&gt;文件操作。</target>
        </trans-unit>
        <trans-unit id="b17afd826ceac8e996a7c85541a3c3196c1b6382" translate="yes" xml:space="preserve">
          <source>When creating a distributed dataset that is to be passed to the enqueue operation a special input option must be specified:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2db902f77ce37ed9d0c7614f639ced1da648ed42" translate="yes" xml:space="preserve">
          <source>When desired_channels is set, if the input contains fewer channels than this then the last channel will be duplicated to give the requested number, else if the input has more channels than requested then the additional channels will be ignored.</source>
          <target state="translated">当设置了 desired_channels 时,如果输入的通道数少于此数,那么最后一个通道将被重复,以给出所要求的数量,否则如果输入的通道数多于所要求的数量,那么额外的通道将被忽略。</target>
        </trans-unit>
        <trans-unit id="87468be68aa798b87299469214116a53bdacd212" translate="yes" xml:space="preserve">
          <source>When documenting the shape of a RaggedTensor, ragged dimensions can be indicated by enclosing them in parentheses. For example, the shape of a 3-D &lt;code&gt;RaggedTensor&lt;/code&gt; that stores the fixed-size word embedding for each word in a sentence, for each sentence in a batch, could be written as &lt;code&gt;[num_sentences, (num_words), embedding_size]&lt;/code&gt;. The parentheses around &lt;code&gt;(num_words)&lt;/code&gt; indicate that dimension is ragged, and that the length of each element list in that dimension may vary for each item.</source>
          <target state="translated">在记录RaggedTensor的形状时，可以通过将其括在括号中来指示参差不齐的尺寸。例如，可以将针对句子中的每个单词（对于批处理中的每个句子）存储固定大小的单词嵌入的3-D &lt;code&gt;RaggedTensor&lt;/code&gt; 的形状写为 &lt;code&gt;[num_sentences, (num_words), embedding_size]&lt;/code&gt; 。圆括号 &lt;code&gt;(num_words)&lt;/code&gt; 表示维度参差不齐，并且该维度中每个元素列表的长度可能因各项而异。</target>
        </trans-unit>
        <trans-unit id="933ca78b0f0aab236f7c30d0b7d8200f4284ead1" translate="yes" xml:space="preserve">
          <source>When doing broadcasted operations such as multiplying a tensor by a scalar, broadcasting (usually) confers some time or space benefit, as the broadcasted tensor is never materialized.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bac64cb9d33552a3e872e7c5fb90f9f381853ebe" translate="yes" xml:space="preserve">
          <source>When doing log-odds NCE, the result of this op should be passed through a SparseToDense op, then added to the logits of the sampled candidates. This has the effect of 'removing' the sampled labels that match the true labels by making the classifier sure that they are sampled labels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd3848450984baae9593cd8c988bd98ec36231eb" translate="yes" xml:space="preserve">
          <source>When each worker has more than one GPU, operations will be replicated on all GPUs. Even though operations may be replicated, variables are not and each worker shares a common view for which parameter server a variable is assigned to.</source>
          <target state="translated">当每个worker有一个以上的GPU时,操作将被复制到所有GPU上。即使操作可能被复制,但变量不会被复制,每个worker共享一个共同的视图,以便变量被分配到哪个参数服务器。</target>
        </trans-unit>
        <trans-unit id="1dfe4b0bd2fe2cdea509ee0800c577901d8b6927" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, &lt;code&gt;gate_gradients&lt;/code&gt;, &lt;code&gt;aggregation_method&lt;/code&gt;, and &lt;code&gt;colocate_gradients_with_ops&lt;/code&gt; are ignored.</source>
          <target state="translated">当急于执行启用， &lt;code&gt;gate_gradients&lt;/code&gt; ， &lt;code&gt;aggregation_method&lt;/code&gt; 和 &lt;code&gt;colocate_gradients_with_ops&lt;/code&gt; 被忽略。</target>
        </trans-unit>
        <trans-unit id="148b464485213e809e25fd2427e6df10ddb7913a" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;momentum&lt;/code&gt; can each be a callable that takes no arguments and returns the actual value to use. This can be useful for changing these values across different invocations of optimizer functions.</source>
          <target state="translated">启用急切执行后， &lt;code&gt;learning_rate&lt;/code&gt; 和 &lt;code&gt;momentum&lt;/code&gt; 可以是不带任何参数并返回要使用的实际值的可调用对象。这对于在优化器函数的不同调用之间更改这些值很有用。</target>
        </trans-unit>
        <trans-unit id="6f483d1759bf06f09369c7c2c6504ed890b4f074" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, &lt;code&gt;learning_rate&lt;/code&gt; can be a callable that takes no arguments and returns the actual value to use. This can be useful for changing these values across different invocations of optimizer functions.</source>
          <target state="translated">启用急切执行后， &lt;code&gt;learning_rate&lt;/code&gt; 可以是不带任何参数并返回要使用的实际值的可调用对象。这对于在优化器函数的不同调用之间更改这些值很有用。</target>
        </trans-unit>
        <trans-unit id="6eb0adbeafdc09876e4693f8ca8e165f9c0c0985" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, &lt;code&gt;learning_rate&lt;/code&gt;, &lt;code&gt;beta_1&lt;/code&gt;, &lt;code&gt;beta_2&lt;/code&gt;, and &lt;code&gt;epsilon&lt;/code&gt; can each be a callable that takes no arguments and returns the actual value to use. This can be useful for changing these values across different invocations of optimizer functions.</source>
          <target state="translated">启用急切执行后， &lt;code&gt;learning_rate&lt;/code&gt; ， &lt;code&gt;beta_1&lt;/code&gt; ， &lt;code&gt;beta_2&lt;/code&gt; 和 &lt;code&gt;epsilon&lt;/code&gt; 都可以是不带任何参数并返回要使用的实际值的可调用对象。这对于在优化器函数的不同调用之间更改这些值很有用。</target>
        </trans-unit>
        <trans-unit id="88cf25c3162efb5267e479cbb3ca75d9e84020c2" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, &lt;code&gt;learning_rate&lt;/code&gt;, &lt;code&gt;decay&lt;/code&gt;, &lt;code&gt;momentum&lt;/code&gt;, and &lt;code&gt;epsilon&lt;/code&gt; can each be a callable that takes no arguments and returns the actual value to use. This can be useful for changing these values across different invocations of optimizer functions.</source>
          <target state="translated">启用急切执行后， &lt;code&gt;learning_rate&lt;/code&gt; ， &lt;code&gt;decay&lt;/code&gt; ， &lt;code&gt;momentum&lt;/code&gt; 和 &lt;code&gt;epsilon&lt;/code&gt; 都可以是可调用的，不带任何参数并返回要使用的实际值。这对于在优化器函数的不同调用之间更改这些值很有用。</target>
        </trans-unit>
        <trans-unit id="a51c69c938604c7d227cd40cf800aac6b43df507" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, &lt;code&gt;learning_rate&lt;/code&gt;, &lt;code&gt;rho&lt;/code&gt;, and &lt;code&gt;epsilon&lt;/code&gt; can each be a callable that takes no arguments and returns the actual value to use. This can be useful for changing these values across different invocations of optimizer functions.</source>
          <target state="translated">启用急切执行后， &lt;code&gt;learning_rate&lt;/code&gt; ， &lt;code&gt;rho&lt;/code&gt; 和 &lt;code&gt;epsilon&lt;/code&gt; 都可以是可调用的，不带任何参数并返回要使用的实际值。这对于在优化器函数的不同调用之间更改这些值很有用。</target>
        </trans-unit>
        <trans-unit id="78865de3a670b7422c397c1e12a0d6935fb3e356" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, &lt;code&gt;loss&lt;/code&gt; should be a Python function that takes no arguments and computes the value to be minimized. Minimization (and gradient computation) is done with respect to the elements of &lt;code&gt;var_list&lt;/code&gt; if not None, else with respect to any trainable variables created during the execution of the &lt;code&gt;loss&lt;/code&gt; function. &lt;code&gt;gate_gradients&lt;/code&gt;, &lt;code&gt;aggregation_method&lt;/code&gt;, &lt;code&gt;colocate_gradients_with_ops&lt;/code&gt; and &lt;code&gt;grad_loss&lt;/code&gt; are ignored when eager execution is enabled.</source>
          <target state="translated">启用急切执行后， &lt;code&gt;loss&lt;/code&gt; 应该是一个不带参数的Python函数，并计算要最小化的值。如果不是None，则对 &lt;code&gt;var_list&lt;/code&gt; 的元素进行最小化（和梯度计算），否则对 &lt;code&gt;loss&lt;/code&gt; 函数执行期间创建的任何可训练变量进行最小化。 &lt;code&gt;gate_gradients&lt;/code&gt; ， &lt;code&gt;aggregation_method&lt;/code&gt; ， &lt;code&gt;colocate_gradients_with_ops&lt;/code&gt; 和 &lt;code&gt;grad_loss&lt;/code&gt; 当启用急于执行被忽略。</target>
        </trans-unit>
        <trans-unit id="a739b33250bbc25214beaa9599eadabdb7bb4665" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, &lt;code&gt;var_list&lt;/code&gt; must specify a &lt;code&gt;list&lt;/code&gt; or &lt;code&gt;dict&lt;/code&gt; of variables to save. Otherwise, a &lt;code&gt;RuntimeError&lt;/code&gt; will be raised.</source>
          <target state="translated">启用急切执行后， &lt;code&gt;var_list&lt;/code&gt; 必须指定要保存的变量的 &lt;code&gt;list&lt;/code&gt; 或 &lt;code&gt;dict&lt;/code&gt; 。否则，将引发 &lt;code&gt;RuntimeError&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="58de3a1f5573eceefbc002055806cdf8a10b2832" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, any callable object in the &lt;code&gt;control_inputs&lt;/code&gt; list will be called.</source>
          <target state="translated">启用急切执行后，将调用 &lt;code&gt;control_inputs&lt;/code&gt; 列表中的所有可调用对象。</target>
        </trans-unit>
        <trans-unit id="91a58274042c41959ab2122ef94206a6a2a7792d" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, code inside an init_scope block runs with eager execution enabled even when tracing a &lt;a href=&quot;function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;. For example:</source>
          <target state="translated">启用急切执行后，即使跟踪&lt;a href=&quot;function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;，init_scope块中的代码也会启用急切执行。例如：</target>
        </trans-unit>
        <trans-unit id="7644973d82350f411122d3333c3da0b3f33b32bb" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, executes ops specified by &lt;code&gt;fn&lt;/code&gt; on each replica. Otherwise, builds a graph to execute the ops on each replica.</source>
          <target state="translated">启用急切执行后，将在每个副本上执行 &lt;code&gt;fn&lt;/code&gt; 指定的操作。否则，构建一个图以在每个副本上执行操作。</target>
        </trans-unit>
        <trans-unit id="f2c02abca5b8ce6f6c1965307850901a9a73db40" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, learning_rate can be a callable that takes no arguments and returns the actual value to use. This can be useful for changing these values across different invocations of optimizer functions.</source>
          <target state="translated">当启用急切执行时,learning_rate可以是一个不接受参数并返回实际使用值的可调用函数。这对于在优化器函数的不同调用中改变这些值是很有用的。</target>
        </trans-unit>
        <trans-unit id="cfdc83fece145a8a3ec7bfd53296a084c4dce787" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, returns &lt;code&gt;True&lt;/code&gt; in most cases. However, this API might return &lt;code&gt;False&lt;/code&gt; in the following use cases.</source>
          <target state="translated">启用急切执行后，在大多数情况下返回 &lt;code&gt;True&lt;/code&gt; 。但是，在以下使用情况下，此API可能返回 &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="fca6e6a28c0b1d4d30760e6ed2b604579bf812fc" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, the mixed precision graph rewrite is only enabled within &lt;a href=&quot;../../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;, as outside &lt;a href=&quot;../../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;, there is no graph.</source>
          <target state="translated">启用急切执行时，仅在&lt;a href=&quot;../../../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; 中&lt;/a&gt;启用混合精度图形重写，因为在&lt;a href=&quot;../../../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;外部没有图形。</target>
        </trans-unit>
        <trans-unit id="cd5cd16ed9249edf9d475e4c4f731cb6a6ee674b" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, the mixed precision graph rewrite is only enabled within &lt;a href=&quot;../../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;s, as outside &lt;a href=&quot;../../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;s, there is no graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6892adfb044f1fdf15d846e00b609e83b202685" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, the mixed precision graph rewrite is only enabled within &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;, as outside &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;, there is no graph.</source>
          <target state="translated">启用急切执行时，仅在&lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; 中&lt;/a&gt;启用混合精度图形重写，因为在&lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;外部没有图形。</target>
        </trans-unit>
        <trans-unit id="530f59ba986dbe23a70377c034d053b8d4dc39ba" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, the mixed precision graph rewrite is only enabled within &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;s, as outside &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;s, there is no graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4ced4e377065fa4ed9d5e9a06ab2254081c3e07" translate="yes" xml:space="preserve">
          <source>When eager execution is enabled, this function returns a function which in turn returns the decayed learning rate Tensor. This can be useful for changing the learning rate value across different invocations of optimizer functions.</source>
          <target state="translated">当启用急切执行时,这个函数会返回一个函数,而这个函数又会返回衰减的学习率Tensor。这对于在不同的优化器函数调用中改变学习率值是很有用的。</target>
        </trans-unit>
        <trans-unit id="d2e71dc36f6cf8dc4084a1f274669076eb23a06d" translate="yes" xml:space="preserve">
          <source>When enabled, TensorFlow runtime will collection information that can later be exported and consumed by TensorBoard. The trace is activated across the entire TensorFlow runtime and affects all threads of execution.</source>
          <target state="translated">启用后,TensorFlow运行时将收集信息,这些信息以后可以被TensorBoard导出和消费。跟踪会在整个TensorFlow运行时激活,并影响所有执行线程。</target>
        </trans-unit>
        <trans-unit id="4de00d923ffd1999737d2b1cd351b2302e234977" translate="yes" xml:space="preserve">
          <source>When enabled, the dtype of Keras layers defaults to floatx (which is typically float32) instead of None. In addition, layers will automatically cast floating-point inputs to the layer's dtype.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c296a7d5f98201bab11940527df0b91bbde60a8b" translate="yes" xml:space="preserve">
          <source>When enum_class is empty.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="261d0f228aa5675f9d37b49b2a148dd964803430" translate="yes" xml:space="preserve">
          <source>When enum_class is not a subclass of Enum.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2564a8f9143928f594ea2074e98db1c0c92dbe63" translate="yes" xml:space="preserve">
          <source>When enum_values is empty.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4382685a76810fa4eefafb6b927a319a12869925" translate="yes" xml:space="preserve">
          <source>When exactly one of &lt;code&gt;x&lt;/code&gt; or &lt;code&gt;y&lt;/code&gt; is non-None, or the shapes are not all broadcastable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="947feea8d5b04906f82595b03627acd7039c1568" translate="yes" xml:space="preserve">
          <source>When exactly one of &lt;code&gt;x&lt;/code&gt; or &lt;code&gt;y&lt;/code&gt; is non-None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f4e19c79dfb2ba307013a448936b4d81569267d0" translate="yes" xml:space="preserve">
          <source>When executed in a graph, this op outputs its input tensor as-is.</source>
          <target state="translated">当在图中执行时,这个操作按原样输出其输入张量。</target>
        </trans-unit>
        <trans-unit id="4cecfcec22d59fa24fae039affcbd309fee3e65f" translate="yes" xml:space="preserve">
          <source>When executed, the Tensor &lt;code&gt;a&lt;/code&gt; will have the name &lt;code&gt;MyOp/a&lt;/code&gt;.</source>
          <target state="translated">当执行时，张量 &lt;code&gt;a&lt;/code&gt; 将具有名称 &lt;code&gt;MyOp/a&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ae9202cb150768cbd21e4a56c23a5fe6d721d720" translate="yes" xml:space="preserve">
          <source>When executed, the Tensors &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, will have names &lt;code&gt;MyOp/a&lt;/code&gt;, &lt;code&gt;MyOp/b&lt;/code&gt;, and &lt;code&gt;MyOp/c&lt;/code&gt;.</source>
          <target state="translated">当执行时，张量 &lt;code&gt;a&lt;/code&gt; ， &lt;code&gt;b&lt;/code&gt; ， &lt;code&gt;c&lt;/code&gt; 将具有名称 &lt;code&gt;MyOp/a&lt;/code&gt; ， &lt;code&gt;MyOp/b&lt;/code&gt; 和 &lt;code&gt;MyOp/c&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="fe804bdbbf36ce79d174e2e04fa5a4b653cbe404" translate="yes" xml:space="preserve">
          <source>When executing eagerly, &lt;code&gt;map_fn&lt;/code&gt; does not execute in parallel even if &lt;code&gt;parallel_iterations&lt;/code&gt; is set to a value &amp;gt; 1. You can still get the performance benefits of running a function in parallel by using the &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; decorator:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b4b0c36f73c370281ff533b8718b05a1bf7fc23a" translate="yes" xml:space="preserve">
          <source>When executing eagerly, &lt;code&gt;map_fn&lt;/code&gt; does not execute in parallel even if &lt;code&gt;parallel_iterations&lt;/code&gt; is set to a value &amp;gt; 1. You can still get the performance benefits of running a function in parallel by using the &lt;a href=&quot;function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; decorator:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="50bb271b7e16cf32dee4edabc5aa3fa56b53275c" translate="yes" xml:space="preserve">
          <source>When executing eagerly, either assigns values immediately if variables to restore have been created already, or defers restoration until the variables are created. Dependencies added after this call will be matched if they have a corresponding object in the checkpoint (the restore request will queue in any trackable object waiting for the expected dependency to be added).</source>
          <target state="translated">当急切执行时,如果要还原的变量已经创建,则立即赋值,或者推迟还原,直到变量被创建。在此调用后添加的依赖关系将被匹配,如果它们在检查点中有对应的对象(还原请求将在任何可跟踪对象中排队等待预期的依赖关系被添加)。</target>
        </trans-unit>
        <trans-unit id="9f2dc68be8f34aeff876ac6aadeee84459d20f11" translate="yes" xml:space="preserve">
          <source>When executing eagerly, map_fn does not execute in parallel even if &lt;code&gt;parallel_iterations&lt;/code&gt; is set to a value &amp;gt; 1. You can still get the performance benefits of running a function in parallel by using the &lt;code&gt;tf.contrib.eager.defun&lt;/code&gt; decorator,</source>
          <target state="translated">急切执行时，即使将 &lt;code&gt;parallel_iterations&lt;/code&gt; 设置为值&amp;gt; 1 ，map_fn也不会并行执行。通过使用 &lt;code&gt;tf.contrib.eager.defun&lt;/code&gt; 装饰器，您仍然可以获得并行运行函数的性能优势，</target>
        </trans-unit>
        <trans-unit id="243113db76fb85d3b379322eb6df8fc099c87214" translate="yes" xml:space="preserve">
          <source>When executing in a &lt;a href=&quot;function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; or building a model using &lt;a href=&quot;keras/input&quot;&gt;&lt;code&gt;tf.keras.Input&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;tensor#shape&quot;&gt;&lt;code&gt;Tensor.shape&lt;/code&gt;&lt;/a&gt; may return a partial shape (including &lt;code&gt;None&lt;/code&gt; for unknown dimensions). See &lt;a href=&quot;tensorshape&quot;&gt;&lt;code&gt;tf.TensorShape&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ce10899f8aee013da9a14ad538ad1fde8148656" translate="yes" xml:space="preserve">
          <source>When executing in a &lt;a href=&quot;function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;, or building a model using &lt;a href=&quot;keras/input&quot;&gt;&lt;code&gt;tf.keras.Input&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;tensor#set_shape&quot;&gt;&lt;code&gt;Tensor.set_shape&lt;/code&gt;&lt;/a&gt; will &lt;em&gt;merge&lt;/em&gt; the given &lt;code&gt;shape&lt;/code&gt; with the current shape of this tensor, and set the tensor's shape to the merged value (see &lt;a href=&quot;tensorshape#merge_with&quot;&gt;&lt;code&gt;tf.TensorShape.merge_with&lt;/code&gt;&lt;/a&gt; for details):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="961e80919c470474bd25500c2cb03aaf480dc1f1" translate="yes" xml:space="preserve">
          <source>When feeding features into &lt;code&gt;embedding.enqueue&lt;/code&gt; they can be &lt;a href=&quot;../../../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt;s, &lt;a href=&quot;../../../sparse/sparsetensor&quot;&gt;&lt;code&gt;tf.SparseTensor&lt;/code&gt;&lt;/a&gt;s or &lt;a href=&quot;../../../raggedtensor&quot;&gt;&lt;code&gt;tf.RaggedTensor&lt;/code&gt;&lt;/a&gt;s. When the argument &lt;code&gt;max_sequence_length&lt;/code&gt; is 0, the default, you should expect a output of &lt;code&gt;embedding.dequeue&lt;/code&gt; for this feature of shape &lt;code&gt;(batch_size, dim)&lt;/code&gt;. If &lt;code&gt;max_sequence_length&lt;/code&gt; is greater than 0, the feature is embedded as a sequence and padded up to the given length. The shape of the output for this feature will be &lt;code&gt;(batch_size, max_sequence_length, dim)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d5aadb062c18b16c150d95636430095568153cdb" translate="yes" xml:space="preserve">
          <source>When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as &lt;code&gt;ON_READ&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f8e307109474cb2469b10f7555dc21ae2944eac" translate="yes" xml:space="preserve">
          <source>When graph building, &lt;code&gt;assert_consumed()&lt;/code&gt; indicates that all of the restore ops that will be created for this checkpoint have been created. They can be run via the &lt;code&gt;run_restore_ops()&lt;/code&gt; method of the status object:</source>
          <target state="translated">建立图形时， &lt;code&gt;assert_consumed()&lt;/code&gt; 表示已创建了将为此检查点创建的所有还原操作。可以通过状态对象的 &lt;code&gt;run_restore_ops()&lt;/code&gt; 方法运行它们：</target>
        </trans-unit>
        <trans-unit id="79f071b74a52cc1dc25696f12242191a62439461" translate="yes" xml:space="preserve">
          <source>When graph building, restoration ops are added to the graph but not run immediately.</source>
          <target state="translated">在建立图形时,恢复操作会被添加到图形中,但不会立即运行。</target>
        </trans-unit>
        <trans-unit id="50bb22b935fa898c607810aea861b6f9cab61dc5" translate="yes" xml:space="preserve">
          <source>When in TF V1 mode (that is, outside &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;) Assert needs a control dependency on the output to ensure the assertion executes:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c8e82ac3193c4ec5ccc1a2ed347b087c3eee96b" translate="yes" xml:space="preserve">
          <source>When indexing keyword argument is not one of &lt;code&gt;xy&lt;/code&gt; or &lt;code&gt;ij&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45d6ec71b2e803e49adc786399511f890dec3b98" translate="yes" xml:space="preserve">
          <source>When indices are not consistent.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3400d0e372dbb4d788999038a1fa59202da9146e" translate="yes" xml:space="preserve">
          <source>When indices is a 1D tensor, this operation is equivalent to &lt;a href=&quot;scatter_update&quot;&gt;&lt;code&gt;tf.compat.v1.scatter_update&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">当索引为一维张量时，此操作等效于&lt;a href=&quot;scatter_update&quot;&gt; &lt;code&gt;tf.compat.v1.scatter_update&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="d2e52528636ed4172136496c48a5c9c765f21cc1" translate="yes" xml:space="preserve">
          <source>When initializing a deep network, it is in principle advantageous to keep the scale of the input variance constant, so it does not explode or diminish by reaching the final layer. If the input is &lt;code&gt;x&lt;/code&gt; and the operation &lt;code&gt;x * W&lt;/code&gt;, and we want to initialize &lt;code&gt;W&lt;/code&gt; uniformly at random, we need to pick &lt;code&gt;W&lt;/code&gt; from</source>
          <target state="translated">初始化深层网络时，原则上保持输入方差的大小恒定是有利的，因此它不会到达最终层而爆炸或减小。如果输入的是 &lt;code&gt;x&lt;/code&gt; ，操作 &lt;code&gt;x * W&lt;/code&gt; 了，我们要初始化 &lt;code&gt;W&lt;/code&gt; &amp;macr;&amp;macr;均匀随机的，我们需要挑选 &lt;code&gt;W&lt;/code&gt; &amp;macr;&amp;macr;从</target>
        </trans-unit>
        <trans-unit id="12f77eb406d2a652e408c888d240e8eb2d9cb755" translate="yes" xml:space="preserve">
          <source>When invoking a signature in an exported SavedModel, &lt;code&gt;Tensor&lt;/code&gt; arguments are identified by name. These names will come from the Python function's argument names by default. They may be overridden by specifying a &lt;code&gt;name=...&lt;/code&gt; argument in the corresponding &lt;a href=&quot;../tensorspec&quot;&gt;&lt;code&gt;tf.TensorSpec&lt;/code&gt;&lt;/a&gt; object. Explicit naming is required if multiple &lt;code&gt;Tensor&lt;/code&gt;s are passed through a single argument to the Python function.</source>
          <target state="translated">在导出的SavedModel中调用签名时， &lt;code&gt;Tensor&lt;/code&gt; 参数由名称标识。默认情况下，这些名称将来自Python函数的参数名称。通过在相应的&lt;a href=&quot;../tensorspec&quot;&gt; &lt;code&gt;tf.TensorSpec&lt;/code&gt; &lt;/a&gt;对象中指定 &lt;code&gt;name=...&lt;/code&gt; 参数，可以覆盖它们。如果将多个 &lt;code&gt;Tensor&lt;/code&gt; 通过单个参数传递给Python函数，则需要显式命名。</target>
        </trans-unit>
        <trans-unit id="aaecdf3e3e9d1409f6d5971a54eef9da389f9db7" translate="yes" xml:space="preserve">
          <source>When loading a weight file in TensorFlow format, returns the same status object as &lt;a href=&quot;../../train/checkpoint#restore&quot;&gt;&lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt;&lt;/a&gt;. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from &lt;code&gt;Model&lt;/code&gt;, immediately if it is already built).</source>
          <target state="translated">当以TensorFlow格式加载重量文件时，返回与&lt;a href=&quot;../../train/checkpoint#restore&quot;&gt; &lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt; &lt;/a&gt;相同的状态对象。建立图形时，恢复操作将在网络建立后立即自动运行（第一次调用从 &lt;code&gt;Model&lt;/code&gt; 继承的用户定义类时（如果已经建立，则立即调用））。</target>
        </trans-unit>
        <trans-unit id="ad0500aa71cc2e13561fbb9f3ccb38e89738eb04" translate="yes" xml:space="preserve">
          <source>When loading a weight file in TensorFlow format, returns the same status object as &lt;a href=&quot;../train/checkpoint#restore&quot;&gt;&lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt;&lt;/a&gt;. When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from &lt;code&gt;Model&lt;/code&gt;, immediately if it is already built).</source>
          <target state="translated">当以TensorFlow格式加载重量文件时，返回与&lt;a href=&quot;../train/checkpoint#restore&quot;&gt; &lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt; &lt;/a&gt;相同的状态对象。建立图形时，恢复操作将在网络建立后立即自动运行（第一次调用从 &lt;code&gt;Model&lt;/code&gt; 继承的用户定义类时（如果已经建立，则立即调用））。</target>
        </trans-unit>
        <trans-unit id="a3a6d69d15b5f2b1e2641796ab481024196abbd1" translate="yes" xml:space="preserve">
          <source>When loading weights in HDF5 format, returns &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">以HDF5格式加载权重时，返回 &lt;code&gt;None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="279df71b03d4f1d2b500da71eaeb34af2e345a5e" translate="yes" xml:space="preserve">
          <source>When many instances of this Op are being run concurrently with the same container/shared_name in the same device, some will output zero-shaped Tensors and others will output Tensors of size up to max_batch_size.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="caebbca103b05daa9db25a1f91ee52187b92666d" translate="yes" xml:space="preserve">
          <source>When mixed precision training is used, most layers will instead have a float16 or bfloat16 compute dtype and a float32 variable dtype, and so the layer does not have a single dtype. See &lt;a href=&quot;https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html&quot;&gt;this link&lt;/a&gt; for more information on mixed precision training. When the variable dtype does not match the compute dtype, variables will be automatically casted to the compute dtype to avoid type errors. In this case, &lt;a href=&quot;../../layers/layer#dtype&quot;&gt;&lt;code&gt;tf.keras.layers.Layer.dtype&lt;/code&gt;&lt;/a&gt; refers to the variable dtype, not the compute dtype.</source>
          <target state="translated">使用混合精度训练时，大多数层将改为具有float16或bfloat16计算dtype和float32变量dtype，因此该层没有单个dtype。有关混合精度训练的更多信息，请参&lt;a href=&quot;https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html&quot;&gt;见此链接&lt;/a&gt;。当变量dtype与计算dtype不匹配时，变量将自动转换为计算dtype，以避免类型错误。在这种情况下，&lt;a href=&quot;../../layers/layer#dtype&quot;&gt; &lt;code&gt;tf.keras.layers.Layer.dtype&lt;/code&gt; &lt;/a&gt;引用变量dtype，而不是计算dtype。</target>
        </trans-unit>
        <trans-unit id="31e5d2d74d34d722ae24b0d3557c28c55ecaa6d6" translate="yes" xml:space="preserve">
          <source>When mixed precision training is used, most layers will instead have a float16 or bfloat16 compute dtype and a float32 variable dtype, and so the layer does not have a single dtype. When the variable dtype does not match the compute dtype, variables will be automatically casted to the compute dtype to avoid type errors. In this case, &lt;a href=&quot;../../layers/layer#dtype&quot;&gt;&lt;code&gt;tf.keras.layers.Layer.dtype&lt;/code&gt;&lt;/a&gt; refers to the variable dtype, not the compute dtype. See &lt;a href=&quot;https://www.tensorflow.org/guide/keras/mixed_precision&quot;&gt;the mixed precision guide&lt;/a&gt; for more information on how to use mixed precision.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="12dfe9c646058fc3b646723517e40b30a435cf72" translate="yes" xml:space="preserve">
          <source>When mode is not one of &quot;CONSTANT&quot;, &quot;REFLECT&quot;, or &quot;SYMMETRIC&quot;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e594a784be178d46ade99237614f3d81b376cb0c" translate="yes" xml:space="preserve">
          <source>When multiple identical random ops are wrapped in a &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;, their behaviors change because the ops no long share the same counter. For example:</source>
          <target state="translated">当多个相同的随机操作包装在&lt;a href=&quot;../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; 中时&lt;/a&gt;，它们的行为会发生变化，因为操作不再共享相同的计数器。例如：</target>
        </trans-unit>
        <trans-unit id="2fdc190461b8260acb4e4a0e40690fe99685019d" translate="yes" xml:space="preserve">
          <source>When multiple steps of profiles are available, select which step's profile to use. If -1, use average of all available steps.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fbe07c906de9609ba47c0d5dc628f10d68cc6dd7" translate="yes" xml:space="preserve">
          <source>When no keyword arguments (kwargs) are passed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5b5b5298719274d2e99620e5ca7a91daabe32b28" translate="yes" xml:space="preserve">
          <source>When not &lt;code&gt;None&lt;/code&gt;, the probability we will drop out a given coordinate.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f319b17be698e68a66d41691883dcc29fc7f9c6" translate="yes" xml:space="preserve">
          <source>When not None, the probability we will drop out a given coordinate.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f589095e3d5fab094abecc7eb9696d68f8a9c36d" translate="yes" xml:space="preserve">
          <source>When operating in a v1-style graph context, ops are not executed in the same order as specified in the code; TensorFlow will attempt to execute ops in parallel or in an order convienient to the result it is computing. &lt;a href=&quot;group&quot;&gt;&lt;code&gt;tf.group&lt;/code&gt;&lt;/a&gt; allows you to request that one or more results finish before execution continues.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c3fc5b0a3b9961cb77e88c8942ce57281e9ad832" translate="yes" xml:space="preserve">
          <source>When passed &lt;code&gt;trainable=True&lt;/code&gt;, the &lt;code&gt;Variable()&lt;/code&gt; constructor automatically adds new variables to the graph collection &lt;code&gt;GraphKeys.TRAINABLE_VARIABLES&lt;/code&gt;. This convenience function returns the contents of that collection.</source>
          <target state="translated">当传递 &lt;code&gt;trainable=True&lt;/code&gt; ， &lt;code&gt;Variable()&lt;/code&gt; 构造函数自动将新变量添加到图形集合 &lt;code&gt;GraphKeys.TRAINABLE_VARIABLES&lt;/code&gt; 。此便捷功能返回该集合的内容。</target>
        </trans-unit>
        <trans-unit id="5b6a28f5fad1b02dbaf2abf3b2bcae0d38a29c7b" translate="yes" xml:space="preserve">
          <source>When reading a single input file, you can shard elements as follows:</source>
          <target state="translated">读取单个输入文件时,可以按以下方式分片元素。</target>
        </trans-unit>
        <trans-unit id="5d6c412525d35ed49e0a016246bbab95a1ad4343" translate="yes" xml:space="preserve">
          <source>When run, it returns a 1-D tensor containing the names of uninitialized variables if there are any, or an empty array if there are none.</source>
          <target state="translated">当运行时,如果有未初始化的变量,它会返回一个包含这些变量名称的一维张量,如果没有,则返回一个空数组。</target>
        </trans-unit>
        <trans-unit id="4cd96e8d064773862a00f08fa46097675a613f49" translate="yes" xml:space="preserve">
          <source>When run, reports an &lt;code&gt;InvalidArgument&lt;/code&gt; error if &lt;code&gt;tensor&lt;/code&gt; has any values that are not a number (NaN) or infinity (Inf). Otherwise, passes &lt;code&gt;tensor&lt;/code&gt; as-is.</source>
          <target state="translated">运行时，如果 &lt;code&gt;tensor&lt;/code&gt; 具有非数字（NaN）或无穷大（Inf）的任何值，则报告 &lt;code&gt;InvalidArgument&lt;/code&gt; 错误。否则，按原样通过 &lt;code&gt;tensor&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="9a7fb4ad12e1e9b24a4a905d1465aa8fe6223f81" translate="yes" xml:space="preserve">
          <source>When run, reports an &lt;code&gt;InvalidArgument&lt;/code&gt; error if &lt;code&gt;tensor&lt;/code&gt; has any values that are not a number (NaN) or infinity (Inf). Otherwise, passes &lt;code&gt;tensor&lt;/code&gt; as-is. Unlike CheckNumerics (V1), CheckNumericsV2 distinguishes -Inf and +Inf in the errors it throws.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a08b94134a846722329d59d0e06daaabf9e81c0e" translate="yes" xml:space="preserve">
          <source>When run, the returned Op will raise the exception &lt;code&gt;FailedPreconditionError&lt;/code&gt; if any of the variables has not yet been initialized.</source>
          <target state="translated">运行时，如果尚未初始化任何变量，则返回的Op将引发异常 &lt;code&gt;FailedPreconditionError&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="612235a5e3b2319598b46d42d78d5e3939c685ba" translate="yes" xml:space="preserve">
          <source>When running in graph mode, you must evaluate the tensor returned by &lt;code&gt;tf.tables_initializer()&lt;/code&gt; before evaluating the tensor returned by this class's &lt;code&gt;lookup()&lt;/code&gt; method. Example usage in graph mode:</source>
          <target state="translated">在图形模式下运行时，必须先评估 &lt;code&gt;tf.tables_initializer()&lt;/code&gt; 返回的张量，然后再评估此类的 &lt;code&gt;lookup()&lt;/code&gt; 方法返回的张量。图形模式的用法示例：</target>
        </trans-unit>
        <trans-unit id="7eea4af19c3fd0977360ee69dfc63bed5794fb89" translate="yes" xml:space="preserve">
          <source>When running in graph mode, you should add a dependency on this operation to ensure that it runs. Example of adding a dependency to an operation:</source>
          <target state="translated">在图模式下运行时,你应该在这个操作上添加一个依赖关系,以确保它的运行。为一个操作添加依赖关系的例子。</target>
        </trans-unit>
        <trans-unit id="83f395cc48e2b9d73bdec46f8abf442be27de226" translate="yes" xml:space="preserve">
          <source>When saving in HDF5 format, the weight file has:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="23b4a5289feb34fa15683e70f83d2559b44972e8" translate="yes" xml:space="preserve">
          <source>When saving in HDF5 format, the weight file has: - &lt;code&gt;layer_names&lt;/code&gt; (attribute), a list of strings (ordered names of model layers). - For every layer, a &lt;code&gt;group&lt;/code&gt; named &lt;code&gt;layer.name&lt;/code&gt; - For every such layer group, a group attribute &lt;code&gt;weight_names&lt;/code&gt;, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor.</source>
          <target state="translated">以HDF5格式保存时，权重文件具有： &lt;code&gt;layer_names&lt;/code&gt; （属性），字符串列表（模型层的有序名称）。-对于每个图层，一个名为 &lt;code&gt;layer.name&lt;/code&gt; 的 &lt;code&gt;group&lt;/code&gt; -对于每个这样的图层组，一个group属性 &lt;code&gt;weight_names&lt;/code&gt; ，一个字符串列表（该图层的权重张量的有序名称）。-对于图层中的每个权重，一个存储权重值的数据集，以权重张量命名。</target>
        </trans-unit>
        <trans-unit id="c275f46f84d94fb1e86dc221ba0e2772a4b65944" translate="yes" xml:space="preserve">
          <source>When saving in TensorFlow format, all objects referenced by the network are saved in the same format as &lt;a href=&quot;../../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt;, including any &lt;code&gt;Layer&lt;/code&gt; instances or &lt;code&gt;Optimizer&lt;/code&gt; instances assigned to object attributes. For networks constructed from inputs and outputs using &lt;code&gt;tf.keras.Model(inputs, outputs)&lt;/code&gt;, &lt;code&gt;Layer&lt;/code&gt; instances used by the network are tracked/saved automatically. For user-defined classes which inherit from &lt;a href=&quot;../model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;Layer&lt;/code&gt; instances must be assigned to object attributes, typically in the constructor. See the documentation of &lt;a href=&quot;../../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">以TensorFlow格式保存时，网络引用的所有对象都以与&lt;a href=&quot;../../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt;相同的格式保存，包括分配给对象属性的所有 &lt;code&gt;Layer&lt;/code&gt; 实例或 &lt;code&gt;Optimizer&lt;/code&gt; 实例。用于使用输入和输出构成网络 &lt;code&gt;tf.keras.Model(inputs, outputs)&lt;/code&gt; ， &lt;code&gt;Layer&lt;/code&gt; 由网络使用的实例跟踪/自动保存。对于从继承用户定义的类&lt;a href=&quot;../model&quot;&gt; &lt;code&gt;tf.keras.Model&lt;/code&gt; &lt;/a&gt;， &lt;code&gt;Layer&lt;/code&gt; 实例必须被分配给对象的属性，典型地在构造。有关详细信息，请参见&lt;a href=&quot;../../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../model&quot;&gt; &lt;code&gt;tf.keras.Model&lt;/code&gt; &lt;/a&gt;的文档。</target>
        </trans-unit>
        <trans-unit id="58ff97a8bb50647c4ab49957faee2741efb3b467" translate="yes" xml:space="preserve">
          <source>When saving in TensorFlow format, all objects referenced by the network are saved in the same format as &lt;a href=&quot;../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt;, including any &lt;code&gt;Layer&lt;/code&gt; instances or &lt;code&gt;Optimizer&lt;/code&gt; instances assigned to object attributes. For networks constructed from inputs and outputs using &lt;code&gt;tf.keras.Model(inputs, outputs)&lt;/code&gt;, &lt;code&gt;Layer&lt;/code&gt; instances used by the network are tracked/saved automatically. For user-defined classes which inherit from &lt;a href=&quot;model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;Layer&lt;/code&gt; instances must be assigned to object attributes, typically in the constructor. See the documentation of &lt;a href=&quot;../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">以TensorFlow格式保存时，网络引用的所有对象都以与&lt;a href=&quot;../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt;相同的格式保存，包括分配给对象属性的所有 &lt;code&gt;Layer&lt;/code&gt; 实例或 &lt;code&gt;Optimizer&lt;/code&gt; 实例。用于使用输入和输出构成网络 &lt;code&gt;tf.keras.Model(inputs, outputs)&lt;/code&gt; ， &lt;code&gt;Layer&lt;/code&gt; 由网络使用的实例跟踪/自动保存。对于从继承用户定义的类&lt;a href=&quot;model&quot;&gt; &lt;code&gt;tf.keras.Model&lt;/code&gt; &lt;/a&gt;， &lt;code&gt;Layer&lt;/code&gt; 实例必须被分配给对象的属性，典型地在构造。有关详细信息，请参见&lt;a href=&quot;../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;model&quot;&gt; &lt;code&gt;tf.keras.Model&lt;/code&gt; &lt;/a&gt;的文档。</target>
        </trans-unit>
        <trans-unit id="784c3ca10c1f2c8309f633399d4061eef62d41bd" translate="yes" xml:space="preserve">
          <source>When shape_x and shape_y are Tensors representing shapes (i.e. the result of calling tf.shape on another Tensor) this computes a Tensor which is the shape of the result of a broadcasting op applied in tensors of shapes shape_x and shape_y.</source>
          <target state="translated">当shape_x和shape_y是代表形状的Tensors时(即在另一个Tensor上调用tf.shape的结果),这将计算一个Tensor,这个Tensor是在shape_x和shape_y的Tensors中应用广播操作的结果的形状。</target>
        </trans-unit>
        <trans-unit id="382f3819ea47b409f9da2d9eacd00a3f2dc7a1b2" translate="yes" xml:space="preserve">
          <source>When shape_x and shape_y are fully known TensorShapes this computes a TensorShape which is the shape of the result of a broadcasting op applied in tensors of shapes shape_x and shape_y.</source>
          <target state="translated">当shape_x和shape_y是完全已知的TensorShapes,这计算一个TensorShape,这是一个广播操作的结果的形状,应用在形状shape_x和shape_y的tensors。</target>
        </trans-unit>
        <trans-unit id="b25a4e208695ca5ecbc4dac0901769cfec692d0d" translate="yes" xml:space="preserve">
          <source>When sparse_delta.indices is a 1D tensor, this operation is equivalent to &lt;code&gt;scatter_update&lt;/code&gt;.</source>
          <target state="translated">当sparse_delta.indices是一维张量时，此操作等效于 &lt;code&gt;scatter_update&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d43e498906b2adf0ec7959c38a4a8cf0f9fa7d0b" translate="yes" xml:space="preserve">
          <source>When starting a dedicated tf.data dispatch process, use join() to block indefinitely after starting up the server.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="934351906a686e71e8e5a944d6c4af715197c11e" translate="yes" xml:space="preserve">
          <source>When starting a dedicated tf.data worker process, use join() to block indefinitely after starting up the server.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5fee3dffe2d54b65ce0a85843a0d39fb34dec340" translate="yes" xml:space="preserve">
          <source>When that Op is run it tries to increment the variable by &lt;code&gt;1&lt;/code&gt;. If incrementing the variable would bring it above &lt;code&gt;limit&lt;/code&gt; then the Op raises the exception &lt;code&gt;OutOfRangeError&lt;/code&gt;.</source>
          <target state="translated">运行该Op时，它将尝试将变量增加 &lt;code&gt;1&lt;/code&gt; 。如果增加变量会使其超出 &lt;code&gt;limit&lt;/code&gt; 则Op会引发 &lt;code&gt;OutOfRangeError&lt;/code&gt; 异常。</target>
        </trans-unit>
        <trans-unit id="a324d7f1e796820d05403b217fea01aa17af935a" translate="yes" xml:space="preserve">
          <source>When the &lt;code&gt;GraphDef&lt;/code&gt; is larger than 2GB.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1856bfeda6ff30a3afd3efdbb33b44e8e2b972ce" translate="yes" xml:space="preserve">
          <source>When the &lt;code&gt;LinearOperator&lt;/code&gt; is not hinted to be &lt;code&gt;non_singular&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56cd8af2612b32012d09c54f7e15310ad702ad5c" translate="yes" xml:space="preserve">
          <source>When the &lt;code&gt;LinearOperator&lt;/code&gt; is not hinted to be positive definite and self adjoint.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ae5f82f4e0944c8fdd12da34fb5e45708d0e8f6" translate="yes" xml:space="preserve">
          <source>When the CrossShardOptimizer is constructed with &lt;code&gt;reduction == losses.Reduction.MEAN&lt;/code&gt; (default), this function scales the loss by &lt;code&gt;1.0 / num_shards&lt;/code&gt; before computing the gradients. Assuming the optimizer uses the default implementation of &lt;code&gt;compute_gradients()&lt;/code&gt;, the gradients of the scaled loss are scaled by &lt;code&gt;1.0 / num_shards&lt;/code&gt; compared to the gradients of the original loss. This scaling factor is important because &lt;code&gt;apply_gradients()&lt;/code&gt; sums gradients across shards, rather than averaging them. However, the scaling factor must be taken into account when clipping the norm of the gradients or performing other postprocessing.</source>
          <target state="translated">当CrossShardOptimizer构造为 &lt;code&gt;reduction == losses.Reduction.MEAN&lt;/code&gt; （默认值）时，此函数 &lt;code&gt;1.0 / num_shards&lt;/code&gt; 在计算梯度之前按1.0 / num_shards的比例缩放损失。假设优化程序使用默认实现的 &lt;code&gt;compute_gradients()&lt;/code&gt; ，则与原始损失的梯度相比，缩放损失的梯度 &lt;code&gt;1.0 / num_shards&lt;/code&gt; 进行缩放。该比例因子很重要，因为 &lt;code&gt;apply_gradients()&lt;/code&gt; 会求和各个碎片的梯度，而不是取其平均值。但是，在裁剪梯度的范数或执行其他后处理时，必须考虑比例因子。</target>
        </trans-unit>
        <trans-unit id="62b085231fa47179e1be16442d315153db83ffe4" translate="yes" xml:space="preserve">
          <source>When the Op is run, it reports an &lt;code&gt;InvalidArgument&lt;/code&gt; error if multiple values in the summaries to merge use the same tag.</source>
          <target state="translated">运行Op时，如果要合并的摘要中的多个值使用相同的标记，则会报告 &lt;code&gt;InvalidArgument&lt;/code&gt; 错误。</target>
        </trans-unit>
        <trans-unit id="a17f60db047668212c07e424cf0f21e2e8e6e67e" translate="yes" xml:space="preserve">
          <source>When the RNN layer is not stateful.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8b5dd0fb66417bcf00267bf905ed39a50b9ed42" translate="yes" xml:space="preserve">
          <source>When the batch size of the RNN layer is unknown.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04c6456a46e3c6718694244e79e5c206d69bdf28" translate="yes" xml:space="preserve">
          <source>When the file to be loaded is not found.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c426d27facbfbf4fa665efb6cfbd82b969658b0" translate="yes" xml:space="preserve">
          <source>When the input numpy array is not compatible with the RNN layer state, either size wise or dtype wise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3584f2f08e6dde164e436c25cc19bcd86167e861" translate="yes" xml:space="preserve">
          <source>When the timeout argument is not present or None, the operation will block until the thread terminates.</source>
          <target state="translated">当超时参数不存在或None时,操作将被阻塞,直到线程终止。</target>
        </trans-unit>
        <trans-unit id="93008cafbefd82e0878a6b1b3873678eb838ef6d" translate="yes" xml:space="preserve">
          <source>When the timeout argument is present and not None, it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof). As join() always returns None, you must call isAlive() after join() to decide whether a timeout happened -- if the thread is still alive, the join() call timed out.</source>
          <target state="translated">当存在超时参数而不是None时,它应该是一个浮点数,指定操作的超时时间,单位是秒(或其零头)。由于join()总是返回None,你必须在join()之后调用isAlive()来决定是否发生了超时--如果线程还活着,那么join()调用就超时了。</target>
        </trans-unit>
        <trans-unit id="df9a68fd13d29e9ba4462924730a99e7d6cbe88d" translate="yes" xml:space="preserve">
          <source>When the timeout argument is present and not None, it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof). As join() always returns None, you must call is_alive() after join() to decide whether a timeout happened -- if the thread is still alive, the join() call timed out.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df29949db6d12bcc6440a4ff55b2a3b63f98638c" translate="yes" xml:space="preserve">
          <source>When the underlying interpreter fails raise ValueError.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79079716d44440c95fa5813292d42c940b3c76a3" translate="yes" xml:space="preserve">
          <source>When this function is used, gradients should be computed and applied with the returned optimizer, either by calling &lt;code&gt;opt.minimize()&lt;/code&gt; or &lt;code&gt;opt.compute_gradients()&lt;/code&gt; followed by &lt;code&gt;opt.apply_gradients()&lt;/code&gt;. If gradients are instead computed with &lt;a href=&quot;../../gradients&quot;&gt;&lt;code&gt;tf.gradients&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;, loss scaling will not be applied, which will likely cause your model not to converge due to float16 underflow problems. To apply lossing scaling with &lt;a href=&quot;../../gradients&quot;&gt;&lt;code&gt;tf.gradients&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../keras/mixed_precision/experimental/lossscaleoptimizer#get_scaled_loss&quot;&gt;&lt;code&gt;LossScaleOptimizer.get_scaled_loss&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../keras/mixed_precision/experimental/lossscaleoptimizer#get_unscaled_gradients&quot;&gt;&lt;code&gt;LossScaleOptimizer.get_unscaled_gradients&lt;/code&gt;&lt;/a&gt;. See &lt;a href=&quot;../../keras/mixed_precision/experimental/lossscaleoptimizer&quot;&gt;&lt;code&gt;keras.mixed_precision.experimental.LossScaleOptimizer&lt;/code&gt;&lt;/a&gt; for details how to do this.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5afcb3cf29a4af014bb340f2f0c516a9732b6d10" translate="yes" xml:space="preserve">
          <source>When this function is used, gradients should only be computed and applied with the returned optimizer, either by calling &lt;code&gt;opt.minimize()&lt;/code&gt; or &lt;code&gt;opt.compute_gradients()&lt;/code&gt; followed by &lt;code&gt;opt.apply_gradients()&lt;/code&gt;. Gradients should not be computed with &lt;a href=&quot;../../../../gradients&quot;&gt;&lt;code&gt;tf.gradients&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;. This is because the returned optimizer will apply loss scaling, and &lt;a href=&quot;../../../../gradients&quot;&gt;&lt;code&gt;tf.gradients&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt; will not. If you do directly use &lt;a href=&quot;../../../../gradients&quot;&gt;&lt;code&gt;tf.gradients&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;, your model may not converge due to float16 underflow problems.</source>
          <target state="translated">使用此函数时，应仅通过调用 &lt;code&gt;opt.minimize()&lt;/code&gt; 或 &lt;code&gt;opt.compute_gradients()&lt;/code&gt; 然后是 &lt;code&gt;opt.apply_gradients()&lt;/code&gt; 来计算梯度并使用返回的优化器应用梯度。不应使用&lt;a href=&quot;../../../../gradients&quot;&gt; &lt;code&gt;tf.gradients&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;../../../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;计算梯度。这是因为返回的优化器将应用损失缩放，而&lt;a href=&quot;../../../../gradients&quot;&gt; &lt;code&gt;tf.gradients&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;../../../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;将不会应用。如果您确实直接使用&lt;a href=&quot;../../../../gradients&quot;&gt; &lt;code&gt;tf.gradients&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;../../../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;，则由于float16下溢问题，您的模型可能无法收敛。</target>
        </trans-unit>
        <trans-unit id="caac8e6b5e9c55ec72ae9a078f806e80238a5f0b" translate="yes" xml:space="preserve">
          <source>When this function is used, gradients should only be computed and applied with the returned optimizer, either by calling &lt;code&gt;opt.minimize()&lt;/code&gt; or &lt;code&gt;opt.compute_gradients()&lt;/code&gt; followed by &lt;code&gt;opt.apply_gradients()&lt;/code&gt;. Gradients should not be computed with &lt;a href=&quot;../../gradients&quot;&gt;&lt;code&gt;tf.gradients&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;. This is because the returned optimizer will apply loss scaling, and &lt;a href=&quot;../../gradients&quot;&gt;&lt;code&gt;tf.gradients&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt; will not. If you do directly use &lt;a href=&quot;../../gradients&quot;&gt;&lt;code&gt;tf.gradients&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;, your model may not converge due to float16 underflow problems.</source>
          <target state="translated">使用此函数时，应仅通过调用 &lt;code&gt;opt.minimize()&lt;/code&gt; 或 &lt;code&gt;opt.compute_gradients()&lt;/code&gt; 然后是 &lt;code&gt;opt.apply_gradients()&lt;/code&gt; 来计算梯度并使用返回的优化器应用梯度。不应使用&lt;a href=&quot;../../gradients&quot;&gt; &lt;code&gt;tf.gradients&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;计算梯度。这是因为返回的优化器将应用损失缩放，而&lt;a href=&quot;../../gradients&quot;&gt; &lt;code&gt;tf.gradients&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;将不会应用。如果您确实直接使用&lt;a href=&quot;../../gradients&quot;&gt; &lt;code&gt;tf.gradients&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;，则由于float16下溢问题，您的模型可能无法收敛。</target>
        </trans-unit>
        <trans-unit id="00a169e018bcf148a01078627fb468a85b4f57d1" translate="yes" xml:space="preserve">
          <source>When this is called, the graph is finalized and ops can no longer be added to the graph.</source>
          <target state="translated">调用此功能后,图形被最终确定,不能再向图形中添加操作。</target>
        </trans-unit>
        <trans-unit id="69ace3a238de55ef5f17d0593a3426df7375ca43" translate="yes" xml:space="preserve">
          <source>When this is true, the Adam update formula is changed from &lt;code&gt;m / (sqrt(v) + epsilon)&lt;/code&gt; to &lt;code&gt;m / sqrt(v + epsilon**2)&lt;/code&gt;. This option improves the performance of TPU training and is not expected to harm model quality.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f06429721d1fc01498f2381f4946b62026a0283c" translate="yes" xml:space="preserve">
          <source>When this op finishes, all ops in &lt;code&gt;inputs&lt;/code&gt; have finished. This op has no output.</source>
          <target state="translated">该操作完成后， &lt;code&gt;inputs&lt;/code&gt; 所有操作均已完成。该操作没有输出。</target>
        </trans-unit>
        <trans-unit id="e93fcaa76656fcbb53b75ad595ffb1773a2d63ac" translate="yes" xml:space="preserve">
          <source>When tracing a function, no ops are being executed, shapes may be unknown. See the &lt;a href=&quot;https://www.tensorflow.org/guide/concrete_function&quot;&gt;Concrete Functions Guide&lt;/a&gt; for details.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="db72b6d879a1722ab8b371aaaee80aaed3e96f22" translate="yes" xml:space="preserve">
          <source>When training a model, it is often beneficial to maintain moving averages of the trained parameters. Evaluations that use averaged parameters sometimes produce significantly better results than the final trained values.</source>
          <target state="translated">训练模型时,保持训练参数的移动平均数通常是有益的。使用平均参数的评估有时会产生比最终训练值更好的结果。</target>
        </trans-unit>
        <trans-unit id="927a8a21e9e3520f84a7ae9e37d363926dbaebe1" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This function applies a cosine decay function to a provided initial learning rate. It requires a &lt;code&gt;global_step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">训练模型时，通常建议随着训练的进行降低学习率。此函数将余弦衰减函数应用于提供的初始学习率。它需要一个 &lt;code&gt;global_step&lt;/code&gt; 值来计算衰减的学习率。您只需传递一个TensorFlow变量，即可在每个训练步骤中增加该变量。</target>
        </trans-unit>
        <trans-unit id="8d86dcb91899a78227813e8c0d47037c6ee19dfd" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This function applies a cosine decay function with restarts to a provided initial learning rate. It requires a &lt;code&gt;global_step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">训练模型时，通常建议随着训练的进行降低学习率。此函数应用余弦衰减函数，并重新启动到提供的初始学习速率。它需要一个 &lt;code&gt;global_step&lt;/code&gt; 值来计算衰减的学习率。您只需传递一个TensorFlow变量，即可在每个训练步骤中增加该变量。</target>
        </trans-unit>
        <trans-unit id="90010651f41909cacf9403b614efcb8bd58e1369" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This function applies a linear cosine decay function to a provided initial learning rate. It requires a &lt;code&gt;global_step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">训练模型时，通常建议随着训练的进行降低学习率。此函数将线性余弦衰减函数应用于提供的初始学习速率。它需要一个 &lt;code&gt;global_step&lt;/code&gt; 值来计算衰减的学习率。您只需传递一个TensorFlow变量，即可在每个训练步骤中增加该变量。</target>
        </trans-unit>
        <trans-unit id="6a9c3656ff265f4bda198cf8e5dfd044fb933790" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This function applies a noisy linear cosine decay function to a provided initial learning rate. It requires a &lt;code&gt;global_step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">训练模型时，通常建议随着训练的进行降低学习率。此函数将噪声线性余弦衰减函数应用于提供的初始学习速率。它需要一个 &lt;code&gt;global_step&lt;/code&gt; 值来计算衰减的学习率。您只需传递一个TensorFlow变量，即可在每个训练步骤中增加该变量。</target>
        </trans-unit>
        <trans-unit id="64048e6ec72e37dd6f4daf26af9d33a56f72d8c7" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This function applies an exponential decay function to a provided initial learning rate. It requires a &lt;code&gt;global_step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">训练模型时，通常建议随着训练的进行降低学习率。此函数将指数衰减函数应用于提供的初始学习率。它需要一个 &lt;code&gt;global_step&lt;/code&gt; 值来计算衰减的学习率。您只需传递一个TensorFlow变量，即可在每个训练步骤中增加该变量。</target>
        </trans-unit>
        <trans-unit id="d181a155b50eadc32c09739aa94b40b51fe01a65" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This function applies an exponential decay function to a provided initial learning rate. It requires an &lt;code&gt;global_step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">训练模型时，通常建议随着训练的进行降低学习率。此函数将指数衰减函数应用于提供的初始学习率。它需要一个 &lt;code&gt;global_step&lt;/code&gt; 值来计算衰减的学习率。您只需传递一个TensorFlow变量，即可在每个训练步骤中增加该变量。</target>
        </trans-unit>
        <trans-unit id="4f6fc97dc42e006ec89048c76f7f47de1eeb3d9a" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This function applies an inverse decay function to a provided initial learning rate. It requires an &lt;code&gt;global_step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">训练模型时，通常建议随着训练的进行降低学习率。此函数将逆衰减函数应用于提供的初始学习速率。它需要一个 &lt;code&gt;global_step&lt;/code&gt; 值来计算衰减的学习率。您只需传递一个TensorFlow变量，即可在每个训练步骤中增加该变量。</target>
        </trans-unit>
        <trans-unit id="4ddff8c9e5ff83f95e35cc80fb672fdcfe082715" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This schedule applies a cosine decay function to an optimizer step, given a provided initial learning rate. It requires a &lt;code&gt;step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">训练模型时，通常建议随着训练的进行降低学习率。给定初始学习速率，此计划会将余弦衰减函数应用于优化器步骤。它需要一个 &lt;code&gt;step&lt;/code&gt; 值来计算衰减的学习率。您只需传递一个TensorFlow变量，即可在每个训练步骤中增加该变量。</target>
        </trans-unit>
        <trans-unit id="512b0ee6d1762189943ee9ddad5e78bda622dc14" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This schedule applies a cosine decay function with restarts to an optimizer step, given a provided initial learning rate. It requires a &lt;code&gt;step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">训练模型时，通常建议随着训练的进行降低学习率。给定初始学习速率后，此计划会将余弦衰减函数重新启动到优化器步骤。它需要一个 &lt;code&gt;step&lt;/code&gt; 值来计算衰减的学习率。您只需传递一个TensorFlow变量，即可在每个训练步骤中增加该变量。</target>
        </trans-unit>
        <trans-unit id="93d56e5e126f6377bdf1a06e2189c730d031c2b4" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This schedule applies a linear cosine decay function to an optimizer step, given a provided initial learning rate. It requires a &lt;code&gt;step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">训练模型时，通常建议随着训练的进行降低学习率。给定初始学习速率，此计划会将线性余弦衰减函数应用于优化程序步骤。它需要一个 &lt;code&gt;step&lt;/code&gt; 值来计算衰减的学习率。您只需传递一个TensorFlow变量，即可在每个训练步骤中增加该变量。</target>
        </trans-unit>
        <trans-unit id="57bf3cf4acea41746a7afc38b8058640564015fe" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This schedule applies a noisy linear cosine decay function to an optimizer step, given a provided initial learning rate. It requires a &lt;code&gt;step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">训练模型时，通常建议随着训练的进行降低学习率。给定初始学习速率后，此计划会将噪声线性余弦衰减函数应用于优化程序步骤。它需要一个 &lt;code&gt;step&lt;/code&gt; 值来计算衰减的学习率。您只需传递一个TensorFlow变量，即可在每个训练步骤中增加该变量。</target>
        </trans-unit>
        <trans-unit id="94deb77d4f75eb579f72f3db2e455ef2c44c44e1" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This schedule applies an exponential decay function to an optimizer step, given a provided initial learning rate.</source>
          <target state="translated">当训练一个模型时,通常建议随着训练的进行降低学习率。这个时间表对优化器步骤应用了一个指数衰减函数,给定了一个初始学习率。</target>
        </trans-unit>
        <trans-unit id="fb18244719acd2825faa9890d721d54e05f0a037" translate="yes" xml:space="preserve">
          <source>When training a model, it is often recommended to lower the learning rate as the training progresses. This schedule applies the inverse decay function to an optimizer step, given a provided initial learning rate. It requires a &lt;code&gt;step&lt;/code&gt; value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.</source>
          <target state="translated">训练模型时，通常建议随着训练的进行降低学习率。给定初始学习速率后，此计划会将逆衰减函数应用于优化器步骤。它需要一个 &lt;code&gt;step&lt;/code&gt; 值来计算衰减的学习率。您只需传递一个TensorFlow变量，即可在每个训练步骤中增加该变量。</target>
        </trans-unit>
        <trans-unit id="0cf6ec8113a82c824118b19bb4f2d871de8f3f5d" translate="yes" xml:space="preserve">
          <source>When used with &lt;a href=&quot;../../distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt;, outside of built-in training loops such as &lt;a href=&quot;../../keras&quot;&gt;&lt;code&gt;tf.keras&lt;/code&gt;&lt;/a&gt;&lt;code&gt;compile&lt;/code&gt; and &lt;code&gt;fit&lt;/code&gt;, please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error.</source>
          <target state="translated">当与&lt;a href=&quot;../../distribute/strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; 一起&lt;/a&gt;使用时，请在诸如&lt;a href=&quot;../../keras&quot;&gt; &lt;code&gt;tf.keras&lt;/code&gt; &lt;/a&gt;的内置训练循环内进行 &lt;code&gt;compile&lt;/code&gt; 和 &lt;code&gt;fit&lt;/code&gt; ，请使用'SUM'或'NONE'减少类型，并在训练循环中明确减少损失。使用&amp;ldquo; AUTO&amp;rdquo;或&amp;ldquo; SUM_OVER_BATCH_SIZE&amp;rdquo;将引发错误。</target>
        </trans-unit>
        <trans-unit id="c6a537963fdbb8e6b3429be0c961e7b0d2a0a449" translate="yes" xml:space="preserve">
          <source>When used, it overrides name_ and is not made unique. If a template of the same scope/unique_name already exists and reuse is false, an error is raised. Defaults to None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58c97e844aea42b09b6f861449f7ea173f99a04f" translate="yes" xml:space="preserve">
          <source>When using InputLayer with Keras Sequential model, it can be skipped by moving the input_shape parameter to the first layer after the InputLayer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4ecb7ac1c54db33d719cbd8ca1f72cb0bec98edc" translate="yes" xml:space="preserve">
          <source>When using a custom callable for &lt;code&gt;split&lt;/code&gt;, the data received by the callable will have the 1st dimension squeezed out - instead of &lt;code&gt;[[&quot;string to split&quot;], [&quot;another string to split&quot;]]&lt;/code&gt;, the Callable will see &lt;code&gt;[&quot;string to split&quot;, &quot;another string to split&quot;]&lt;/code&gt;. The callable should return a Tensor with the first dimension containing the split tokens - in this example, we should see something like &lt;code&gt;[[&quot;string&quot;, &quot;to&quot;, &quot;split], [&quot;another&quot;, &quot;string&quot;, &quot;to&quot;, &quot;split&quot;]]&lt;/code&gt;. This makes the callable site natively compatible with &lt;a href=&quot;../../../../strings/split&quot;&gt;&lt;code&gt;tf.strings.split()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4658ae77687cb1c8967c8186bb04e6986f0779aa" translate="yes" xml:space="preserve">
          <source>When using a custom callable for &lt;code&gt;standardize&lt;/code&gt;, the data received by the callable will be exactly as passed to this layer. The callable should return a tensor of the same shape as the input.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e0724c7bfb29f8f0c6cdcbe1954184f010886b9c" translate="yes" xml:space="preserve">
          <source>When using multiple critical sections on the same resources, there is no guarantee of exclusive access to those resources. This behavior is disallowed by default (but see the kwarg &lt;code&gt;exclusive_resource_access&lt;/code&gt;).</source>
          <target state="translated">在同一资源上使用多个关键部分时，不能保证对这些资源的独占访问。这种行为是默认不允许的（但看到kwarg &lt;code&gt;exclusive_resource_access&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="005ecd21a6428476fbceb142b210d5384fc2790a" translate="yes" xml:space="preserve">
          <source>When using the default, an appropriate policy will be picked automatically. The default policy may change over time.</source>
          <target state="translated">使用默认值时,会自动选择合适的策略。默认策略可能会随着时间的推移而改变。</target>
        </trans-unit>
        <trans-unit id="67d49a53dc96e389873c164cb198f138ecc7ff2f" translate="yes" xml:space="preserve">
          <source>When using these moments for batch normalization (see &lt;a href=&quot;../../../nn/batch_normalization&quot;&gt;&lt;code&gt;tf.nn.batch_normalization&lt;/code&gt;&lt;/a&gt;):</source>
          <target state="translated">在将这些时间用于批处理归一化时（请参阅&lt;a href=&quot;../../../nn/batch_normalization&quot;&gt; &lt;code&gt;tf.nn.batch_normalization&lt;/code&gt; &lt;/a&gt;）：</target>
        </trans-unit>
        <trans-unit id="591b74326af491d6abfab42a671eba7ebb5be83e" translate="yes" xml:space="preserve">
          <source>When using these moments for batch normalization (see &lt;a href=&quot;batch_normalization&quot;&gt;&lt;code&gt;tf.nn.batch_normalization&lt;/code&gt;&lt;/a&gt;):</source>
          <target state="translated">在将这些时间用于批处理归一化时（请参阅&lt;a href=&quot;batch_normalization&quot;&gt; &lt;code&gt;tf.nn.batch_normalization&lt;/code&gt; &lt;/a&gt;）：</target>
        </trans-unit>
        <trans-unit id="ea3d2206487c99a74b50234d9380045457ab56b2" translate="yes" xml:space="preserve">
          <source>When using this layer as the first layer in a model, provide an &lt;code&gt;input_shape&lt;/code&gt; argument (tuple of integers or &lt;code&gt;None&lt;/code&gt;, e.g. &lt;code&gt;(10, 128)&lt;/code&gt; for sequences of 10 vectors of 128-dimensional vectors, or &lt;code&gt;(None, 128)&lt;/code&gt; for variable-length sequences of 128-dimensional vectors.</source>
          <target state="translated">当将此层用作模型中的第一层时，请提供 &lt;code&gt;input_shape&lt;/code&gt; 参数（整数元组或 &lt;code&gt;None&lt;/code&gt; ，例如对于10个向量的128维向量的序列，为 &lt;code&gt;(10, 128)&lt;/code&gt; 10，128 &lt;code&gt;(None, 128)&lt;/code&gt; 对于可变长度，则为（None，128） 128维向量的序列。</target>
        </trans-unit>
        <trans-unit id="0a409624f20a1b12afa97b438af1e2c02851ec80" translate="yes" xml:space="preserve">
          <source>When using this layer as the first layer in a model, provide the keyword argument &lt;code&gt;input_shape&lt;/code&gt; (tuple of integers, does not include the sample axis), e.g. &lt;code&gt;input_shape=(128, 128, 128, 1)&lt;/code&gt; for 128x128x128 volumes with a single channel, in &lt;code&gt;data_format=&quot;channels_last&quot;&lt;/code&gt;.</source>
          <target state="translated">当将此层用作模型中的第一层时，请为单个通道的128x128x128卷提供关键字参数 &lt;code&gt;input_shape&lt;/code&gt; （整数元组，不包括采样轴），例如 &lt;code&gt;input_shape=(128, 128, 128, 1)&lt;/code&gt; 128，128，128，1 ），在 &lt;code&gt;data_format=&quot;channels_last&quot;&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="14d463d2a8afe47183522c1acdade8296e826653" translate="yes" xml:space="preserve">
          <source>When using this layer as the first layer in a model, provide the keyword argument &lt;code&gt;input_shape&lt;/code&gt; (tuple of integers, does not include the sample axis), e.g. &lt;code&gt;input_shape=(128, 128, 128, 3)&lt;/code&gt; for a 128x128x128 volume with 3 channels if &lt;code&gt;data_format=&quot;channels_last&quot;&lt;/code&gt;.</source>
          <target state="translated">当将此层用作模型中的第一层时，请提供关键字参数 &lt;code&gt;input_shape&lt;/code&gt; （整数元组，不包括采样轴），例如对于具有3个通道的128x128x128体积，请 &lt;code&gt;input_shape=(128, 128, 128, 3)&lt;/code&gt; &lt;code&gt;data_format=&quot;channels_last&quot;&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b700c0ba9e5af2d79f0ea705fd60f2846a05cd69" translate="yes" xml:space="preserve">
          <source>When using this layer as the first layer in a model, provide the keyword argument &lt;code&gt;input_shape&lt;/code&gt; (tuple of integers, does not include the sample axis), e.g. &lt;code&gt;input_shape=(128, 128, 3)&lt;/code&gt; for 128x128 RGB pictures in &lt;code&gt;data_format=&quot;channels_last&quot;&lt;/code&gt;.</source>
          <target state="translated">当在模型中使用该层作为第一层，提供关键字参数 &lt;code&gt;input_shape&lt;/code&gt; （整数的元组，不包括样品轴线），例如 &lt;code&gt;input_shape=(128, 128, 3)&lt;/code&gt; 在128&amp;times;128的RGB图像 &lt;code&gt;data_format=&quot;channels_last&quot;&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="39c55fa13fdb10b59035551c9a262071686d849a" translate="yes" xml:space="preserve">
          <source>When using this layer as the first layer in a model, provide the keyword argument &lt;code&gt;input_shape&lt;/code&gt; (tuple of integers, does not include the sample axis), e.g. &lt;code&gt;input_shape=(128, 3)&lt;/code&gt; for data with 128 time steps and 3 channels.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="90420d916accea41b90f4795021da2ee04f4f6d6" translate="yes" xml:space="preserve">
          <source>When variables are assigned to multiple workers, each worker writes its own section of the checkpoint. These sections are then merged/re-indexed to behave as a single checkpoint. This avoids copying all variables to one worker, but does require that all workers see a common filesystem.</source>
          <target state="translated">当变量被分配给多个工作者时,每个工作者都会写出自己的检查点部分。然后,这些部分被合并/重新索引,作为一个单一的检查点。这就避免了将所有变量复制到一个工作者身上,但需要所有工作者看到一个共同的文件系统。</target>
        </trans-unit>
        <trans-unit id="d43c37e91c1c99cab60a11a374e0ae141f677fad" translate="yes" xml:space="preserve">
          <source>When writing a TensorFlow program, the main object that is manipulated and passed around is the &lt;a href=&quot;tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5727fc2aeafda851fc9ab281a980ef4c906dc39b" translate="yes" xml:space="preserve">
          <source>When you build a model for training you usually need ops to initialize variables, a &lt;code&gt;Saver&lt;/code&gt; to checkpoint them, an op to collect summaries for the visualizer, and so on.</source>
          <target state="translated">在构建训练模型时，通常需要使用ops初始化变量，使用 &lt;code&gt;Saver&lt;/code&gt; 对其进行检查点，使用op来收集可视化工具的摘要，等等。</target>
        </trans-unit>
        <trans-unit id="2571f0d8bc64be0b275df26c714df2c7b618a158" translate="yes" xml:space="preserve">
          <source>When you iterate over a dataset containing the &lt;code&gt;distribute&lt;/code&gt; transformation, the tf.data service creates a &quot;job&quot; which produces data for the dataset iteration.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="39d490a1a6c041251d0a268b335edd8cf95e7a4b" translate="yes" xml:space="preserve">
          <source>When you later call the &lt;code&gt;create_threads()&lt;/code&gt; method, the &lt;code&gt;QueueRunner&lt;/code&gt; will create one thread for each op in &lt;code&gt;enqueue_ops&lt;/code&gt;. Each thread will run its enqueue op in parallel with the other threads. The enqueue ops do not have to all be the same op, but it is expected that they all enqueue tensors in &lt;code&gt;queue&lt;/code&gt;.</source>
          <target state="translated">当你以后调用 &lt;code&gt;create_threads()&lt;/code&gt; 方法，该 &lt;code&gt;QueueRunner&lt;/code&gt; 将创建一个线程在每个运算 &lt;code&gt;enqueue_ops&lt;/code&gt; 。每个线程将与其他线程并行运行其入队操作。入队操作不必全都相同，但可以预期它们都将张量排入 &lt;code&gt;queue&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d3a2e77e985cdd0c4ca04ebfb7ec15ae2eed6e37" translate="yes" xml:space="preserve">
          <source>When you launch the graph, variables have to be explicitly initialized before you can run Ops that use their value. You can initialize a variable by running its &lt;em&gt;initializer op&lt;/em&gt;, restoring the variable from a save file, or simply running an &lt;code&gt;assign&lt;/code&gt; Op that assigns a value to the variable. In fact, the variable &lt;em&gt;initializer op&lt;/em&gt; is just an &lt;code&gt;assign&lt;/code&gt; Op that assigns the variable's initial value to the variable itself.</source>
          <target state="translated">启动图形时，必须先明确初始化变量，然后才能运行使用其值的Ops。您可以通过运行变量的&lt;em&gt;初始值设定项op&lt;/em&gt;，从保存文件中恢复变量，或简单地运行为变量分配值的 &lt;code&gt;assign&lt;/code&gt; Op来&lt;em&gt;初始化&lt;/em&gt;变量。实际上，变量初始值 &lt;code&gt;assign&lt;/code&gt; &lt;em&gt;op&lt;/em&gt;只是将变量的初始值分配给变量本身的赋值 Op。</target>
        </trans-unit>
        <trans-unit id="36dcf90c997814649120f194530d033746d75e2c" translate="yes" xml:space="preserve">
          <source>Whenever &lt;code&gt;partial_pivoting&lt;/code&gt; is true and the backend is XLA.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2e2218d66173c9095f9d7c0c982dd0a7b7b1d0cb" translate="yes" xml:space="preserve">
          <source>Whenever possible, the session will raise a more specific subclass of &lt;code&gt;OpError&lt;/code&gt; from the &lt;a href=&quot;../errors&quot;&gt;&lt;code&gt;tf.errors&lt;/code&gt;&lt;/a&gt; module.</source>
          <target state="translated">只要有可能，会话都会从&lt;a href=&quot;../errors&quot;&gt; &lt;code&gt;tf.errors&lt;/code&gt; &lt;/a&gt;模块中 &lt;code&gt;OpError&lt;/code&gt; 的更特定子类。</target>
        </trans-unit>
        <trans-unit id="525f61b6729229b1a8854a9e50bcaa52ea96b131" translate="yes" xml:space="preserve">
          <source>Where</source>
          <target state="translated">Where</target>
        </trans-unit>
        <trans-unit id="a5a5757b0332ab74cd51749fc4f26f2282d43f68" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;j&lt;/code&gt; is the &lt;code&gt;i&lt;/code&gt;th &lt;code&gt;True&lt;/code&gt; entry of &lt;code&gt;mask[a1...aA]&lt;/code&gt;.</source>
          <target state="translated">其中 &lt;code&gt;j&lt;/code&gt; 是 &lt;code&gt;mask[a1...aA]&lt;/code&gt; 第 &lt;code&gt;i&lt;/code&gt; 个 &lt;code&gt;True&lt;/code&gt; 项。</target>
        </trans-unit>
        <trans-unit id="a9e5e6939cbf334efa1154ae5d005e3955d0f0b7" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;key&lt;/code&gt; is a feature key whose values are used to partition the values. Partitions are listed from outermost to innermost.</source>
          <target state="translated">其中 &lt;code&gt;key&lt;/code&gt; 是功能键，其值用于分区值。分区从最外面到最里面列出。</target>
        </trans-unit>
        <trans-unit id="17e66517ae2fa6c4b3f8472462247a694203eb5e" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;year&lt;/code&gt;, &lt;code&gt;month&lt;/code&gt;, and &lt;code&gt;day&lt;/code&gt; specify the date beyond which binaries that consume a model are expected to have been updated to include the new operations. This date is typically at least 3 weeks beyond the date the code that adds the new operation is committed.</source>
          <target state="translated">其中 &lt;code&gt;year&lt;/code&gt; ， &lt;code&gt;month&lt;/code&gt; 和 &lt;code&gt;day&lt;/code&gt; 指定日期，在该日期之后，预计将更新使用模型的二进制文件以包括新操作。该日期通常比提交新操作的代码提交的日期至少晚3周。</target>
        </trans-unit>
        <trans-unit id="743987e43173da0b07d4299cc71758449720108f" translate="yes" xml:space="preserve">
          <source>Where &lt;em&gt;N&lt;/em&gt; = &lt;code&gt;ndims(params)&lt;/code&gt;, &lt;em&gt;M&lt;/em&gt; = &lt;code&gt;ndims(indices)&lt;/code&gt;, and &lt;em&gt;B&lt;/em&gt; = &lt;code&gt;batch_dims&lt;/code&gt;. Note that &lt;code&gt;params.shape[:batch_dims]&lt;/code&gt; must be identical to &lt;code&gt;indices.shape[:batch_dims]&lt;/code&gt;.</source>
          <target state="translated">其中&lt;em&gt;N&lt;/em&gt; = &lt;code&gt;ndims(params)&lt;/code&gt; ，&lt;em&gt;M&lt;/em&gt; = &lt;code&gt;ndims(indices)&lt;/code&gt; 和&lt;em&gt;B&lt;/em&gt; = &lt;code&gt;batch_dims&lt;/code&gt; 。需要注意的是 &lt;code&gt;params.shape[:batch_dims]&lt;/code&gt; 必须与 &lt;code&gt;indices.shape[:batch_dims]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="13aa13f1ced8020c65b238cd2e5743aaab5f6a97" translate="yes" xml:space="preserve">
          <source>Where &lt;em&gt;N&lt;/em&gt; = &lt;code&gt;ndims(params)&lt;/code&gt;.</source>
          <target state="translated">其中&lt;em&gt;N&lt;/em&gt; = &lt;code&gt;ndims(params)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7e741bc3dcef0123eeda11543758853be2aac149" translate="yes" xml:space="preserve">
          <source>Where:</source>
          <target state="translated">Where:</target>
        </trans-unit>
        <trans-unit id="df5ee0a432194eaf058dafaec560ccdd5751f85a" translate="yes" xml:space="preserve">
          <source>Whereas in &lt;a href=&quot;../../gather&quot;&gt;&lt;code&gt;tf.gather&lt;/code&gt;&lt;/a&gt;&lt;code&gt;indices&lt;/code&gt; defines slices into the first dimension of &lt;code&gt;params&lt;/code&gt;, in &lt;a href=&quot;../../gather_nd&quot;&gt;&lt;code&gt;tf.gather_nd&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;indices&lt;/code&gt; defines slices into the first &lt;code&gt;N&lt;/code&gt; dimensions of &lt;code&gt;params&lt;/code&gt;, where &lt;code&gt;N = indices.shape[-1]&lt;/code&gt;.</source>
          <target state="translated">在&lt;a href=&quot;../../gather&quot;&gt; &lt;code&gt;tf.gather&lt;/code&gt; 中,&lt;/a&gt; &lt;code&gt;indices&lt;/code&gt; 将切片定义为 &lt;code&gt;params&lt;/code&gt; 的第一维，而在&lt;a href=&quot;../../gather_nd&quot;&gt; &lt;code&gt;tf.gather_nd&lt;/code&gt; 中&lt;/a&gt;， &lt;code&gt;indices&lt;/code&gt; 将切片定义为 &lt;code&gt;params&lt;/code&gt; 的前 &lt;code&gt;N&lt;/code&gt; 个维，其中 &lt;code&gt;N = indices.shape[-1]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="db72094af8e11b94ee83cae58cea0ebfb59e1a33" translate="yes" xml:space="preserve">
          <source>Whereas in &lt;a href=&quot;../gather&quot;&gt;&lt;code&gt;tf.gather&lt;/code&gt;&lt;/a&gt;&lt;code&gt;indices&lt;/code&gt; defines slices into the &lt;code&gt;axis&lt;/code&gt; dimension of &lt;code&gt;params&lt;/code&gt;, in &lt;a href=&quot;../gather_nd&quot;&gt;&lt;code&gt;tf.gather_nd&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;indices&lt;/code&gt; defines slices into the first &lt;code&gt;N&lt;/code&gt; dimensions of &lt;code&gt;params&lt;/code&gt;, where &lt;code&gt;N = indices.shape[-1]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="37006b5e3234b3a81d34b49c1399e20287f6f5fd" translate="yes" xml:space="preserve">
          <source>Whereas in &lt;a href=&quot;gather&quot;&gt;&lt;code&gt;tf.gather&lt;/code&gt;&lt;/a&gt;&lt;code&gt;indices&lt;/code&gt; defines slices into the first dimension of &lt;code&gt;params&lt;/code&gt;, in &lt;a href=&quot;gather_nd&quot;&gt;&lt;code&gt;tf.gather_nd&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;indices&lt;/code&gt; defines slices into the first &lt;code&gt;N&lt;/code&gt; dimensions of &lt;code&gt;params&lt;/code&gt;, where &lt;code&gt;N = indices.shape[-1]&lt;/code&gt;.</source>
          <target state="translated">在&lt;a href=&quot;gather&quot;&gt; &lt;code&gt;tf.gather&lt;/code&gt; 中,&lt;/a&gt; &lt;code&gt;indices&lt;/code&gt; 将切片定义为 &lt;code&gt;params&lt;/code&gt; 的第一维，而在&lt;a href=&quot;gather_nd&quot;&gt; &lt;code&gt;tf.gather_nd&lt;/code&gt; 中&lt;/a&gt;， &lt;code&gt;indices&lt;/code&gt; 将切片定义为 &lt;code&gt;params&lt;/code&gt; 的前 &lt;code&gt;N&lt;/code&gt; 个维，其中 &lt;code&gt;N = indices.shape[-1]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d4b7353826912b9cec703766decc544dcf83f10b" translate="yes" xml:space="preserve">
          <source>Whether &lt;code&gt;output&lt;/code&gt; is expected to be a logits tensor. By default, we consider that &lt;code&gt;output&lt;/code&gt; encodes a probability distribution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47c0b61274d31e65caf92de4ae562ae2c5114355" translate="yes" xml:space="preserve">
          <source>Whether &lt;code&gt;y_pred&lt;/code&gt; is expected to be a logits tensor. By default, we assume that &lt;code&gt;y_pred&lt;/code&gt; encodes a probability distribution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe485b5aa69407b2cec559631c713bb745bddf4c" translate="yes" xml:space="preserve">
          <source>Whether &lt;code&gt;y_pred&lt;/code&gt; is expected to be a logits tensor. By default, we assume that &lt;code&gt;y_pred&lt;/code&gt; encodes a probability distribution. **Note - Using from_logits=True may be more numerically stable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8c9ab93a2df9a01da2c64742b40fc42d6c91dfb" translate="yes" xml:space="preserve">
          <source>Whether &lt;code&gt;y_pred&lt;/code&gt; is expected to be a logits tensor. By default, we assume that &lt;code&gt;y_pred&lt;/code&gt; encodes a probability distribution. &lt;strong&gt;Note - Using from_logits=True is more numerically stable.&lt;/strong&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8fd73dec5012589d7460a67d8190f1604f4a14a8" translate="yes" xml:space="preserve">
          <source>Whether GPU-CPU memory swap is enabled for this loop.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d6717cbbf5b0f27136d029a9a5a4fe644c3d0bdb" translate="yes" xml:space="preserve">
          <source>Whether a &lt;code&gt;DType&lt;/code&gt; is unsigned.</source>
          <target state="translated">&lt;code&gt;DType&lt;/code&gt; 是否为无符号的。</target>
        </trans-unit>
        <trans-unit id="1e01d5432153fe1934c596d7a5942310bb55c381" translate="yes" xml:space="preserve">
          <source>Whether a &lt;code&gt;History&lt;/code&gt; callback should be added, if one does not already exist in the &lt;code&gt;callbacks&lt;/code&gt; list.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="224e3e261d520f419e922338344450c9f2dfc0d7" translate="yes" xml:space="preserve">
          <source>Whether a &lt;code&gt;ProgbarLogger&lt;/code&gt; callback should be added, if one does not already exist in the &lt;code&gt;callbacks&lt;/code&gt; list.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8aa3f3168594096d3d8b8e19131df9c4c2c6f451" translate="yes" xml:space="preserve">
          <source>Whether autograph should be applied on &lt;code&gt;func&lt;/code&gt; before tracing a graph. Data-dependent control flow requires &lt;code&gt;autograph=True&lt;/code&gt;. For more information, see the &lt;a href=&quot;https://www.tensorflow.org/guide/function&quot;&gt;tf.function and AutoGraph guide&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e52c3c2213cac47314d63bd666108cb982d0a526" translate="yes" xml:space="preserve">
          <source>Whether backprop is enabled for this while loop.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2a906b7b12a6aaefca1bb38dbd9c872d24af6915" translate="yes" xml:space="preserve">
          <source>Whether bias centering needs to occur. Bias centering refers to the first node in the very first tree returning the prediction that is aligned with the original labels distribution. For example, for regression problems, the first node will return the mean of the labels. For binary classification problems, it will return a logit for a prior probability of label 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="43c8b1e245711e513da5fb3224a94014bfbd2ea6" translate="yes" xml:space="preserve">
          <source>Whether checkpointing is needed.</source>
          <target state="translated">是否需要检查点。</target>
        </trans-unit>
        <trans-unit id="d4eda7288db97494a48ea76ccf902c7ac0f935a7" translate="yes" xml:space="preserve">
          <source>Whether each tensor in &lt;code&gt;tensor_list&lt;/code&gt; is a single example.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="95fd49cdfd4929bef2715197b8eb6fc30ab04a73" translate="yes" xml:space="preserve">
          <source>Whether each tensor in &lt;code&gt;tensor_list_list&lt;/code&gt; is a single example.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ccb1e098286182b19dc3309570dc9cdd8118f6d3" translate="yes" xml:space="preserve">
          <source>Whether each tensor in &lt;code&gt;tensors&lt;/code&gt; is a single example.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d6a6ce44ff2b603ee993694c9f88f39e13a58c1f" translate="yes" xml:space="preserve">
          <source>Whether initialization is needed.</source>
          <target state="translated">是否需要初始化。</target>
        </trans-unit>
        <trans-unit id="b62856eef3b197086ffc08e6c1915a9c8bdb4a27" translate="yes" xml:space="preserve">
          <source>Whether only account the statistics of displayed profiler nodes.</source>
          <target state="translated">是否只对显示的剖析器节点进行统计。</target>
        </trans-unit>
        <trans-unit id="b2a9b9493710259f1453769c41e2339bd8290c9f" translate="yes" xml:space="preserve">
          <source>Whether only weights are saved, or the whole model is saved.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="369fdea0194aadd9a179680eb99f4f5771059465" translate="yes" xml:space="preserve">
          <source>Whether operations should be dispatched synchronously. Valid values:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="584fa0c419a0180a4a9a48f62c36f8aa85cbba56" translate="yes" xml:space="preserve">
          <source>Whether or not the embedding is trainable. Default is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd70d2454f94f5a5bd23acd3b6d8d74be52ad65b" translate="yes" xml:space="preserve">
          <source>Whether or not the enum is to be case-sensitive.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5af2fcf03aaec3220bccfce000595207f5de5efd" translate="yes" xml:space="preserve">
          <source>Whether or not to clear the device field for an &lt;code&gt;Operation&lt;/code&gt; or &lt;code&gt;Tensor&lt;/code&gt; during export.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="541b0e69634db2eb96093b9893572ce18575fde6" translate="yes" xml:space="preserve">
          <source>Whether or not to clear the device field for an &lt;code&gt;Operation&lt;/code&gt; or &lt;code&gt;Tensor&lt;/code&gt; during import.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f2329674feeb4db85ce54cddb74a65691271f9d8" translate="yes" xml:space="preserve">
          <source>Whether or not to close all open fd's in the child after forking.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c16fc5a5fb92977ae5605b686e098e91fe724b8" translate="yes" xml:space="preserve">
          <source>Whether saving summaries is needed.</source>
          <target state="translated">是否需要保存总结。</target>
        </trans-unit>
        <trans-unit id="a67af11a445e7ad4ad271581ae16018da270fae5" translate="yes" xml:space="preserve">
          <source>Whether shape inference is enabled.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="213556493385771188ee3ad68ba9d0c3e3df86d3" translate="yes" xml:space="preserve">
          <source>Whether the &lt;code&gt;TensorArray&lt;/code&gt; can grow past its initial size.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b529b545844f1f5f6b86ac51e751e32c783eddaa" translate="yes" xml:space="preserve">
          <source>Whether the &lt;code&gt;input_bytes&lt;/code&gt; data is in little-endian format. Data will be converted into host byte order if necessary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="254e76001f48c136c6f2b62d065c4a944ffb37c6" translate="yes" xml:space="preserve">
          <source>Whether the Reader implementation can serialize its state.</source>
          <target state="translated">Reader的实现是否可以将其状态序列化。</target>
        </trans-unit>
        <trans-unit id="8e46d69392dfbc82012e7be9a6824ef93b52a84b" translate="yes" xml:space="preserve">
          <source>Whether the layer is dynamic (eager-only); set in the constructor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b6e0e7bc8fc1c73b7aaaf3cab72814f4b9d143dc" translate="yes" xml:space="preserve">
          <source>Whether the layer should be trained (boolean), i.e. whether its potentially-trainable weights should be returned as part of &lt;code&gt;layer.trainable_weights&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="decf851b03ac228e9313cade0b5957f7b666dae0" translate="yes" xml:space="preserve">
          <source>Whether the outputs need to be produced in deterministic order. If None, defaults to True.</source>
          <target state="translated">输出是否需要按确定性顺序产生。如果为None,默认为True。</target>
        </trans-unit>
        <trans-unit id="204afdb37d75d1615b12cbddd4b547cac2445374" translate="yes" xml:space="preserve">
          <source>Whether the resources required by &lt;code&gt;fn&lt;/code&gt; should be exclusive to this &lt;code&gt;CriticalSection&lt;/code&gt;. Default: &lt;code&gt;True&lt;/code&gt;. You may want to set this to &lt;code&gt;False&lt;/code&gt; if you will be accessing a resource in read-only mode in two different CriticalSections.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6ca153ec1da7024d082dd6c5ca6490e0c9cc967f" translate="yes" xml:space="preserve">
          <source>Whether the scaling parameter of the layer should be trainable. Defaults to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f124179378e71744cfb3660434035ba0a965f11a" translate="yes" xml:space="preserve">
          <source>Whether the strategy uses between-graph replication or not.</source>
          <target state="translated">该策略是否使用图间复制。</target>
        </trans-unit>
        <trans-unit id="7b189cbeed35b5f550585a0be2719b6e1b80cbe7" translate="yes" xml:space="preserve">
          <source>Whether the threads should be marked as &lt;code&gt;daemons&lt;/code&gt;, meaning they don't block program exit.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ffa50da689f93e998f89852cb17efdd428d40e8f" translate="yes" xml:space="preserve">
          <source>Whether this is the last update for the progress bar. If &lt;code&gt;None&lt;/code&gt;, defaults to &lt;code&gt;current &amp;gt;= self.target&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="844772b73edafce629bee3e7ae25983a4b946d09" translate="yes" xml:space="preserve">
          <source>Whether this layer supports computing a mask using &lt;code&gt;compute_mask&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc8aa6f730bdacc1ba24ab5a192fd71d34266e12" translate="yes" xml:space="preserve">
          <source>Whether to L2-normalize samples along the dot product axis before taking the dot product. If set to True, then the output of the dot product is the cosine proximity between the two samples.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="25debb8437d8df1e4a47ad52d47cdffdeaed2fdb" translate="yes" xml:space="preserve">
          <source>Whether to add latency measurements on all edges. Defaults to False.</source>
          <target state="translated">是否在所有边缘上添加延迟测量。默认值为False。</target>
        </trans-unit>
        <trans-unit id="ea1060a10c23d0b7356e53c3f78b04cde7c784c6" translate="yes" xml:space="preserve">
          <source>Whether to add python code trace information. Used to support &quot;code&quot; view.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd79c6c27494dd0eff20142fcddaa9263851ca92" translate="yes" xml:space="preserve">
          <source>Whether to allow the expansion in the non-concat dimensions. Defaulted to False.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="60ffb7d2f619b0cd73858e4eae2a324f1c86a19d" translate="yes" xml:space="preserve">
          <source>Whether to apply decay in a discrete staircase, as opposed to continuous, fashion.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="98be7901e41c6f7d3da8e520ead11178c148bd11" translate="yes" xml:space="preserve">
          <source>Whether to apply default graph optimizations. If False, only graph optimizations that have been explicitly enabled will be applied.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04a904d718701877a2e0c411c4fe23b4c4e3dd17" translate="yes" xml:space="preserve">
          <source>Whether to apply default static optimizations. If False, only static optimizations that have been explicitly enabled will be applied.</source>
          <target state="translated">是否应用默认的静态优化。如果为 &quot;False&quot;,则只应用已明确启用的静态优化。</target>
        </trans-unit>
        <trans-unit id="0dfe28cb36c9e4043cc3c258d3d2b5adbf5c6fcb" translate="yes" xml:space="preserve">
          <source>Whether to automatically tune performance knobs. If None, defaults to True.</source>
          <target state="translated">是否自动调整性能旋钮。如果无,默认为True。</target>
        </trans-unit>
        <trans-unit id="e49bbcda0833c641972e00b20f44fee93023536d" translate="yes" xml:space="preserve">
          <source>Whether to close the &lt;code&gt;summary_writer&lt;/code&gt;. Defaults to &lt;code&gt;True&lt;/code&gt; if the summary writer was created by the supervisor, &lt;code&gt;False&lt;/code&gt; otherwise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="48089f8ff308f6b2e1f3c7040fd63434bb0bf7a9" translate="yes" xml:space="preserve">
          <source>Whether to close the summary writer when closing the session. Defaults to True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06ba131eb14ad989473a1a1d258716794efd0b30" translate="yes" xml:space="preserve">
          <source>Whether to convert the comparison operators, like equality. This is soon to be deprecated as support is being added to the Tensor class.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e7f5fb7a744d7e22b32c2f61ab042329f9cf9f1e" translate="yes" xml:space="preserve">
          <source>Whether to eliminate no-op transformations. If None, defaults to True.</source>
          <target state="translated">是否消除无操作转换。如果为None,默认为True。</target>
        </trans-unit>
        <trans-unit id="55bb7493c6bec3c764e8919ffd2eed1794554953" translate="yes" xml:space="preserve">
          <source>Whether to enable JIT compilation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8c98d9351c340a8a2394b518e7da0cf49c193b3a" translate="yes" xml:space="preserve">
          <source>Whether to enable or disable compilation in the scope. Either a Python bool, or a callable that accepts the parameter &lt;code&gt;node_def&lt;/code&gt; and returns a python bool.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f04c00e059b1defa60bb4eed8c8d8bfc3869da01" translate="yes" xml:space="preserve">
          <source>Whether to enable soft placement.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f9d095bc84ce17a933a7edf15d114454108ce476" translate="yes" xml:space="preserve">
          <source>Whether to enabled device placement logging.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fa1073b5ec09137db6c46e9a17f5e29476dc90c4" translate="yes" xml:space="preserve">
          <source>Whether to expand nested models into clusters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="593fbed8bd96575db44e786eedbea4f61bf29dd7" translate="yes" xml:space="preserve">
          <source>Whether to follow symlinks inside class subdirectories (default: False).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ccd7cdfaf0055cafb944adeab223f3187d39f6f" translate="yes" xml:space="preserve">
          <source>Whether to fuse filter dataset that predicts random_uniform &amp;lt; rate into a sampling dataset. If None, defaults to False.</source>
          <target state="translated">是否将预测random_uniform &amp;lt;rate的过滤器数据集融合到采样数据集中。如果为None，则默认为False。</target>
        </trans-unit>
        <trans-unit id="3d46b05c402499222a82d4000612dbe3e69e3543" translate="yes" xml:space="preserve">
          <source>Whether to fuse filter transformations. If None, defaults to False.</source>
          <target state="translated">是否融合滤波器变换。如果无,默认为False。</target>
        </trans-unit>
        <trans-unit id="14622b8d2b800d5545010933d2dce355f7878e20" translate="yes" xml:space="preserve">
          <source>Whether to fuse map and batch transformations. If None, defaults to True.</source>
          <target state="translated">是否融合地图和批次转换。如果为None,默认为True。</target>
        </trans-unit>
        <trans-unit id="89d5416ec9ee3bbc766239f06ad6082f104072da" translate="yes" xml:space="preserve">
          <source>Whether to fuse map and filter transformations. If None, defaults to False.</source>
          <target state="translated">是否融合映射和滤镜变换。如果无,默认为False。</target>
        </trans-unit>
        <trans-unit id="ed238b5a1a92f1cdcc3146430339e440df04eeef" translate="yes" xml:space="preserve">
          <source>Whether to fuse map transformations. If None, defaults to False.</source>
          <target state="translated">是否要融合地图变换,如果无,默认为False。如果为None,默认为False。</target>
        </trans-unit>
        <trans-unit id="5bf1515795584c1b97bc7e2ce679a96ff89a3700" translate="yes" xml:space="preserve">
          <source>Whether to fuse shuffle and repeat transformations. If None, defaults to True.</source>
          <target state="translated">是否融合洗牌和重复变换。如果为None,默认为True。</target>
        </trans-unit>
        <trans-unit id="5d8df6452e0ce74559d9d9e2cb01ae9313b462e1" translate="yes" xml:space="preserve">
          <source>Whether to hoist &lt;code&gt;tf.random_uniform()&lt;/code&gt; ops out of map transformations. If None, defaults to False.</source>
          <target state="translated">是否将 &lt;code&gt;tf.random_uniform()&lt;/code&gt; 提升到地图转换之外。如果为None，则默认为False。</target>
        </trans-unit>
        <trans-unit id="3d0ff5f97e1e7bc8d1a39a62cacd607bcbc343f6" translate="yes" xml:space="preserve">
          <source>Whether to include the constant &lt;code&gt;log(z!)&lt;/code&gt; term in computing the poisson loss. See &lt;a href=&quot;../nn/log_poisson_loss&quot;&gt;&lt;code&gt;tf.nn.log_poisson_loss&lt;/code&gt;&lt;/a&gt; for the full documentation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="95573f5b8478c4ff858958db70f9484eb3ff2864" translate="yes" xml:space="preserve">
          <source>Whether to include the fully-connected layer at the top of the network.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1b6d4f7d016891ba7c3664590ae67883c943bcc2" translate="yes" xml:space="preserve">
          <source>Whether to include the fully-connected layer at the top of the network. Defaults to True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a96a6b7996a34915e32caf67d322039a64480302" translate="yes" xml:space="preserve">
          <source>Whether to interpret &lt;code&gt;y_pred&lt;/code&gt; as a tensor of &lt;a href=&quot;https://en.wikipedia.org/wiki/Logit&quot;&gt;logit&lt;/a&gt; values. By default, we assume that &lt;code&gt;y_pred&lt;/code&gt; contains probabilities (i.e., values in [0, 1]). **Note - Using from_logits=True may be more numerically stable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c54c4b54df643bc4abc643e55545e0c75e91b663" translate="yes" xml:space="preserve">
          <source>Whether to introduce 'slack' in the last &lt;code&gt;prefetch&lt;/code&gt; of the input pipeline, if it exists. This may reduce CPU contention with accelerator host-side activity at the start of a step. The slack frequency is determined by the number of devices attached to this input pipeline. If None, defaults to False.</source>
          <target state="translated">是否在输入管道的最后 &lt;code&gt;prefetch&lt;/code&gt; 中引入&amp;ldquo;松弛&amp;rdquo; （如果存在）。这可以在步骤开始时通过加速器主机端活动减少CPU争用。松弛频率由连接到此输入管道的设备数确定。如果为None，则默认为False。</target>
        </trans-unit>
        <trans-unit id="415f3117867c673985b61a262949466ab1564948" translate="yes" xml:space="preserve">
          <source>Whether to mark this name as being used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d1aefded232219f79087eafa756226ded0158a7" translate="yes" xml:space="preserve">
          <source>Whether to only keep the model that has achieved the &quot;best performance&quot; so far, or whether to save the model at the end of every epoch regardless of performance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="914df09e28d1c64c2b3ad87baeeb42bb6e13c04f" translate="yes" xml:space="preserve">
          <source>Whether to output all intermediates from functional control flow ops.</source>
          <target state="translated">是否输出所有功能控制流操作的中间物。</target>
        </trans-unit>
        <trans-unit id="1dadddd88645b104b7d6bbaecaa2b1fda6cebd75" translate="yes" xml:space="preserve">
          <source>Whether to pad the end of &lt;code&gt;signal&lt;/code&gt; with &lt;code&gt;pad_value&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b76369cf9f1df3341ba7b7b298f504b7086664d1" translate="yes" xml:space="preserve">
          <source>Whether to pad the end of &lt;code&gt;signals&lt;/code&gt; with zeros when the provided frame length and step produces a frame that lies partially past its end.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02821348292f56a84707e75ec632d05c62475235" translate="yes" xml:space="preserve">
          <source>Whether to parallelize copying of batch elements. If None, defaults to False.</source>
          <target state="translated">是否对批处理元素进行并行复制。如果无,默认为False。</target>
        </trans-unit>
        <trans-unit id="735760deb2a9c67ef0c7efaa6a50691fc4a6f0e0" translate="yes" xml:space="preserve">
          <source>Whether to parallelize stateless map transformations. If None, defaults to False.</source>
          <target state="translated">是否并行化无状态地图转换。如果为None,默认为False。</target>
        </trans-unit>
        <trans-unit id="8eda9b273c93d039ba7a242609f9446311332c6d" translate="yes" xml:space="preserve">
          <source>Whether to preserve the aspect ratio. If this is set, then &lt;code&gt;images&lt;/code&gt; will be resized to a size that fits in &lt;code&gt;size&lt;/code&gt; while preserving the aspect ratio of the original image. Scales up the image if &lt;code&gt;size&lt;/code&gt; is bigger than the current size of the &lt;code&gt;image&lt;/code&gt;. Defaults to False.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af633c59d5db9e3af600703e972fa65df59d21b3" translate="yes" xml:space="preserve">
          <source>Whether to recursively convert any functions that the converted function may call.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="331d800dc7aa1366aba63d94549fbc90867f575a" translate="yes" xml:space="preserve">
          <source>Whether to replace the C0 control characters &lt;code&gt;(U+0000 - U+001F)&lt;/code&gt; with the &lt;code&gt;replacement_char&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1f08303e15a79462237d5a6f5ab69bc0d741f795" translate="yes" xml:space="preserve">
          <source>Whether to rescale image values to be within &lt;code&gt;[0, 255]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d631ffee25c1c8d5962cb92c91bc7155c1b86dbc" translate="yes" xml:space="preserve">
          <source>Whether to rescale image values to be within &lt;code&gt;[0, 255]&lt;/code&gt;. Defaults to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af90abadf38a68efbae5eab044cc111e83bae73f" translate="yes" xml:space="preserve">
          <source>Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bf889dca2e1b92ac166e4e9099cdfdcbbe292012" translate="yes" xml:space="preserve">
          <source>Whether to save the GraphDef and MetaGraphDef to &lt;code&gt;checkpoint_dir&lt;/code&gt;. The GraphDef is saved after the session is created as &lt;code&gt;graph.pbtxt&lt;/code&gt;. MetaGraphDefs are saved out for every checkpoint as &lt;code&gt;model.ckpt-*.meta&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c75f3049cc336ca24bf2e733a248d74d9f9c0730" translate="yes" xml:space="preserve">
          <source>Whether to shuffle output samples, or instead draw them in chronological order.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34ea722ba14f3f99938856c7d83cff00e23dae81" translate="yes" xml:space="preserve">
          <source>Whether to shuffle the data (default: True) If set to False, sorts the data in alphanumeric order.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="248b22a81980263fa482dd9c28f486987117fdb6" translate="yes" xml:space="preserve">
          <source>Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cbcae28d63feefddbed17928b77788494d83948d" translate="yes" xml:space="preserve">
          <source>Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="22ee2250a0c89ccf48c596cca64f08347b735454" translate="yes" xml:space="preserve">
          <source>Whether to start the standard services and the queue runners.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cfad559685c1e01540d88fa4658891788a35c214" translate="yes" xml:space="preserve">
          <source>Whether to start the standard services, such as checkpoint, summary and step counter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4197ca406a3d4336b3bdffbd1a95b8c81b6a0d1" translate="yes" xml:space="preserve">
          <source>Whether to store intermediate values needed for gradients on the CPU instead of GPU.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f85068103e5372bea510e0ae9c69724c9d066b10" translate="yes" xml:space="preserve">
          <source>Whether to subtract &lt;code&gt;b&lt;/code&gt; from &lt;code&gt;a&lt;/code&gt;, vs vice versa.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="10f6a87284583be6f6880d0e9062e36a3a101464" translate="yes" xml:space="preserve">
          <source>Whether to sum gradients from different replicas in the presense of &lt;a href=&quot;../../../distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt;. If False, it's user responsibility to aggregate the gradients. Default to True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="72847fae2b2b235f6c8342af13371de425b6b16d" translate="yes" xml:space="preserve">
          <source>Whether to sum gradients from different replicas in the presense of &lt;a href=&quot;../../distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt;. If False, it's user responsibility to aggregate the gradients. Default to True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bcc78610692aa1b78a0b8fce45e7b1a35f60fedd" translate="yes" xml:space="preserve">
          <source>Whether to unroll the RNN or to use a symbolic &lt;code&gt;while_loop&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8319c980207aacd2a41a4c25d0f4519649627a11" translate="yes" xml:space="preserve">
          <source>Whether to use &lt;a href=&quot;https://arxiv.org/abs/1702.03275&quot;&gt;Batch Renormalization&lt;/a&gt;. This adds extra variables during training. The inference is the same for either value of this parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd041b7a31c14b8f4c0154e4f0e31652d226ed84" translate="yes" xml:space="preserve">
          <source>Whether to use &lt;code&gt;ResourceVariable&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b32a3b0296e56c0857535ebb732043e2ca01eb48" translate="yes" xml:space="preserve">
          <source>Whether to use Batch Renormalization (Ioffe, 2017). This adds extra variables during training. The inference is the same for either value of this parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b7ceb047bcc05fae84d262bcb0f0a4c42d80112e" translate="yes" xml:space="preserve">
          <source>Whether to use ChooseFastestBranchDataset with this transformation. If True, the pipeline picks between the vectorized and original segment at runtime based on their iterations speed. If None, defaults to False.</source>
          <target state="translated">是否在此变换中使用ChooseFastestBranchDataset。如果为True,管道在运行时根据它们的迭代速度在矢量化段和原始段之间进行选择。如果为None,则默认为False。</target>
        </trans-unit>
        <trans-unit id="b41a583b4e8ecad70d2b783981605d5fb07b5daa" translate="yes" xml:space="preserve">
          <source>Whether to use an anti-aliasing filter when downsampling an image.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d7f954867443ea6a778475601ca4e3cc6e2dabd" translate="yes" xml:space="preserve">
          <source>Whether to use anti-aliasing when resizing. See 'image.resize()'.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c343c5190311c54955a31561f20b178e46089ae4" translate="yes" xml:space="preserve">
          <source>Whether to use autograph to compile python and eager style code for efficient graph-mode execution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ee94bf5c9ec84d6138e0ac95db29424ae3a5d6b" translate="yes" xml:space="preserve">
          <source>Whether to use batch normalization after each hidden layer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66b047600fb7cdf1794d2271af3d1f8a45f50c04" translate="yes" xml:space="preserve">
          <source>Whether to validate the order and range of sparse indices in &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="77a0de4187b80648dee2790bc3e0723da8ae8a3a" translate="yes" xml:space="preserve">
          <source>Whether to validate the order and range of sparse indices in &lt;code&gt;a&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d4f7ae19a819559a645d06cbc4b8c196628f89ca" translate="yes" xml:space="preserve">
          <source>Whether to vectorize map transformations. If None, defaults to False.</source>
          <target state="translated">是否对地图变换进行矢量化。如果为None,默认为False。</target>
        </trans-unit>
        <trans-unit id="f123cab967534816dc9b84ddbe1b892a217e9a8b" translate="yes" xml:space="preserve">
          <source>Whether to visits subdirectories pointed to by symlinks. Defaults to False.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="081ce8e1451357f99365d88262e4c58272efd31e" translate="yes" xml:space="preserve">
          <source>Whether to wait for checkpoint to become available.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69219f557de4f53ff935206788ba0bcdb1a1ab26" translate="yes" xml:space="preserve">
          <source>Whether we should overwrite any existing model at the target location, or instead ask the user with a manual prompt.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="63419d87417ca59c7ed9321136943312244206c5" translate="yes" xml:space="preserve">
          <source>Whether we should wait for the availability of a checkpoint before creating Session. Defaults to False.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bbe339fdd2ca55a9615f6086f8ab79036c1a5e5d" translate="yes" xml:space="preserve">
          <source>Whether you are running on your machine or in the cluster you can use the following values for the --master flag:</source>
          <target state="translated">无论你是在你的机器上还是在集群中运行,你都可以为--master标志使用以下值。</target>
        </trans-unit>
        <trans-unit id="3c232ac685e13a6b93f5f74209d44fc6f31c57d9" translate="yes" xml:space="preserve">
          <source>Which axis to join along. The default behavior is to join all elements, producing a scalar.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f517d8a1e3e3925fb5d9497fd0a8aca7ed0a886e" translate="yes" xml:space="preserve">
          <source>Which profile step to use for profiling.</source>
          <target state="translated">剖析时使用哪个剖析步骤。</target>
        </trans-unit>
        <trans-unit id="d86a5675cc6478e7030415aaa83cd72801292548" translate="yes" xml:space="preserve">
          <source>While</source>
          <target state="translated">While</target>
        </trans-unit>
        <trans-unit id="1c51ff24d93bea432325c091aee3e33a6598af2c" translate="yes" xml:space="preserve">
          <source>While &lt;a href=&quot;../../../keras/model#save_weights&quot;&gt;&lt;code&gt;tf.keras.Model.save_weights&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../../train/checkpoint#save&quot;&gt;&lt;code&gt;tf.train.Checkpoint.save&lt;/code&gt;&lt;/a&gt; save in the same format, note that the root of the resulting checkpoint is the object the save method is attached to. This means saving a &lt;a href=&quot;../../../keras/model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt; using &lt;code&gt;save_weights&lt;/code&gt; and loading into a &lt;a href=&quot;../../../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; with a &lt;code&gt;Model&lt;/code&gt; attached (or vice versa) will not match the &lt;code&gt;Model&lt;/code&gt;'s variables. See the &lt;a href=&quot;https://www.tensorflow.org/guide/checkpoint&quot;&gt;guide to training checkpoints&lt;/a&gt; for details. Prefer &lt;a href=&quot;../../../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; over &lt;a href=&quot;../../../keras/model#save_weights&quot;&gt;&lt;code&gt;tf.keras.Model.save_weights&lt;/code&gt;&lt;/a&gt; for training checkpoints.</source>
          <target state="translated">尽管&lt;a href=&quot;../../../keras/model#save_weights&quot;&gt; &lt;code&gt;tf.keras.Model.save_weights&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../../../train/checkpoint#save&quot;&gt; &lt;code&gt;tf.train.Checkpoint.save&lt;/code&gt; &lt;/a&gt;以相同的格式保存，但是请注意，生成的检查点的根是保存方法所附加的对象。这意味着节约了&lt;a href=&quot;../../../keras/model&quot;&gt; &lt;code&gt;tf.keras.Model&lt;/code&gt; &lt;/a&gt;使用 &lt;code&gt;save_weights&lt;/code&gt; 和装载到&lt;a href=&quot;../../../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt;与 &lt;code&gt;Model&lt;/code&gt; 附接（或反之亦然）将不匹配 &lt;code&gt;Model&lt;/code&gt; 的变量。有关详细信息，请参见&lt;a href=&quot;https://www.tensorflow.org/guide/checkpoint&quot;&gt;培训检查点指南&lt;/a&gt;。与&lt;a href=&quot;../../../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;../../../keras/model#save_weights&quot;&gt; &lt;code&gt;tf.keras.Model.save_weights&lt;/code&gt; &lt;/a&gt;适合用于训练检查点。</target>
        </trans-unit>
        <trans-unit id="af5169034174d26fcbbcf3598b642bb6291299f0" translate="yes" xml:space="preserve">
          <source>While &lt;a href=&quot;../keras/model#save_weights&quot;&gt;&lt;code&gt;tf.keras.Model.save_weights&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;checkpoint#save&quot;&gt;&lt;code&gt;tf.train.Checkpoint.save&lt;/code&gt;&lt;/a&gt; save in the same format, note that the root of the resulting checkpoint is the object the save method is attached to. This means saving a &lt;a href=&quot;../keras/model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt; using &lt;code&gt;save_weights&lt;/code&gt; and loading into a &lt;a href=&quot;checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; with a &lt;code&gt;Model&lt;/code&gt; attached (or vice versa) will not match the &lt;code&gt;Model&lt;/code&gt;'s variables. See the &lt;a href=&quot;https://www.tensorflow.org/guide/checkpoint&quot;&gt;guide to training checkpoints&lt;/a&gt; for details. Prefer &lt;a href=&quot;checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; over &lt;a href=&quot;../keras/model#save_weights&quot;&gt;&lt;code&gt;tf.keras.Model.save_weights&lt;/code&gt;&lt;/a&gt; for training checkpoints.</source>
          <target state="translated">尽管&lt;a href=&quot;../keras/model#save_weights&quot;&gt; &lt;code&gt;tf.keras.Model.save_weights&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;checkpoint#save&quot;&gt; &lt;code&gt;tf.train.Checkpoint.save&lt;/code&gt; &lt;/a&gt;以相同的格式保存，但是请注意，生成的检查点的根是保存方法所附加的对象。这意味着节约了&lt;a href=&quot;../keras/model&quot;&gt; &lt;code&gt;tf.keras.Model&lt;/code&gt; &lt;/a&gt;使用 &lt;code&gt;save_weights&lt;/code&gt; 和装载到&lt;a href=&quot;checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt;与 &lt;code&gt;Model&lt;/code&gt; 附接（或反之亦然）将不匹配 &lt;code&gt;Model&lt;/code&gt; 的变量。有关详细信息，请参见&lt;a href=&quot;https://www.tensorflow.org/guide/checkpoint&quot;&gt;培训检查点指南&lt;/a&gt;。与&lt;a href=&quot;checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;../keras/model#save_weights&quot;&gt; &lt;code&gt;tf.keras.Model.save_weights&lt;/code&gt; &lt;/a&gt;适合用于训练检查点。</target>
        </trans-unit>
        <trans-unit id="df92ff2e348ec38c9d24bb6a65e3a867cbde6530" translate="yes" xml:space="preserve">
          <source>While &lt;code&gt;fn&lt;/code&gt; is running in the critical section, no other functions which wish to use this critical section may run.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="721826e30de58e9f292f39ea411e1bcf62df9289" translate="yes" xml:space="preserve">
          <source>While it is possible to use Variables with Lambda layers, this practice is discouraged as it can easily lead to bugs. For instance, consider the following layer:</source>
          <target state="translated">虽然可以在Lambda层中使用变量,但不鼓励这种做法,因为它很容易导致错误。例如,考虑以下图层。</target>
        </trans-unit>
        <trans-unit id="b7ab51f21fc0c3654c10e148c98ad2c3c102cbcf" translate="yes" xml:space="preserve">
          <source>While the formats are the same, do not mix &lt;code&gt;save_weights&lt;/code&gt; and &lt;a href=&quot;../../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt;. Checkpoints saved by &lt;a href=&quot;../model#save_weights&quot;&gt;&lt;code&gt;Model.save_weights&lt;/code&gt;&lt;/a&gt; should be loaded using &lt;a href=&quot;../model#load_weights&quot;&gt;&lt;code&gt;Model.load_weights&lt;/code&gt;&lt;/a&gt;. Checkpoints saved using &lt;a href=&quot;../../train/checkpoint#save&quot;&gt;&lt;code&gt;tf.train.Checkpoint.save&lt;/code&gt;&lt;/a&gt; should be restored using the corresponding &lt;a href=&quot;../../train/checkpoint#restore&quot;&gt;&lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt;&lt;/a&gt;. Prefer &lt;a href=&quot;../../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; over &lt;code&gt;save_weights&lt;/code&gt; for training checkpoints.</source>
          <target state="translated">格式相同时，请勿混合使用 &lt;code&gt;save_weights&lt;/code&gt; 和&lt;a href=&quot;../../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt;。由&lt;a href=&quot;../model#save_weights&quot;&gt; &lt;code&gt;Model.save_weights&lt;/code&gt; &lt;/a&gt;保存的检查点应使用&lt;a href=&quot;../model#load_weights&quot;&gt; &lt;code&gt;Model.load_weights&lt;/code&gt; &lt;/a&gt;加载。使用&lt;a href=&quot;../../train/checkpoint#save&quot;&gt; &lt;code&gt;tf.train.Checkpoint.save&lt;/code&gt; &lt;/a&gt;保存的检查点应使用相应的&lt;a href=&quot;../../train/checkpoint#restore&quot;&gt; &lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt; &lt;/a&gt;恢复。&lt;a href=&quot;../../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt;对于 &lt;code&gt;save_weights&lt;/code&gt; ，首选tf.train.Checkpoint作为训练检查点。</target>
        </trans-unit>
        <trans-unit id="4d3c899f37d2d7cda82e12e4a5abbb5d78c486a6" translate="yes" xml:space="preserve">
          <source>While the formats are the same, do not mix &lt;code&gt;save_weights&lt;/code&gt; and &lt;a href=&quot;../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt;. Checkpoints saved by &lt;a href=&quot;model#save_weights&quot;&gt;&lt;code&gt;Model.save_weights&lt;/code&gt;&lt;/a&gt; should be loaded using &lt;a href=&quot;model#load_weights&quot;&gt;&lt;code&gt;Model.load_weights&lt;/code&gt;&lt;/a&gt;. Checkpoints saved using &lt;a href=&quot;../train/checkpoint#save&quot;&gt;&lt;code&gt;tf.train.Checkpoint.save&lt;/code&gt;&lt;/a&gt; should be restored using the corresponding &lt;a href=&quot;../train/checkpoint#restore&quot;&gt;&lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt;&lt;/a&gt;. Prefer &lt;a href=&quot;../train/checkpoint&quot;&gt;&lt;code&gt;tf.train.Checkpoint&lt;/code&gt;&lt;/a&gt; over &lt;code&gt;save_weights&lt;/code&gt; for training checkpoints.</source>
          <target state="translated">格式相同时，请勿混合使用 &lt;code&gt;save_weights&lt;/code&gt; 和&lt;a href=&quot;../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt;。由&lt;a href=&quot;model#save_weights&quot;&gt; &lt;code&gt;Model.save_weights&lt;/code&gt; &lt;/a&gt;保存的检查点应使用&lt;a href=&quot;model#load_weights&quot;&gt; &lt;code&gt;Model.load_weights&lt;/code&gt; &lt;/a&gt;加载。使用&lt;a href=&quot;../train/checkpoint#save&quot;&gt; &lt;code&gt;tf.train.Checkpoint.save&lt;/code&gt; &lt;/a&gt;保存的检查点应使用相应的&lt;a href=&quot;../train/checkpoint#restore&quot;&gt; &lt;code&gt;tf.train.Checkpoint.restore&lt;/code&gt; &lt;/a&gt;恢复。&lt;a href=&quot;../train/checkpoint&quot;&gt; &lt;code&gt;tf.train.Checkpoint&lt;/code&gt; &lt;/a&gt;对于 &lt;code&gt;save_weights&lt;/code&gt; ，首选tf.train.Checkpoint作为训练检查点。</target>
        </trans-unit>
        <trans-unit id="0b0e77019b80f4349b00c82de8efee158e5fc102" translate="yes" xml:space="preserve">
          <source>While using distribution strategies, all the variable creation should be done within the strategy's scope. This will replicate the variables across all the replicas and keep them in sync using an all-reduce algorithm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f64f547104eca8ffa08df75b0d127da08fb96ecc" translate="yes" xml:space="preserve">
          <source>While using distribution strategies, the variables created within strategy's scope will be replicated across all the replicas and can be kept in sync using all-reduce algorithms.</source>
          <target state="translated">在使用分布式策略时,在策略范围内创建的变量将被复制到所有的副本中,可以使用全还原算法保持同步。</target>
        </trans-unit>
        <trans-unit id="3d98a83dc2837ad96d068975c35687e2b85ed52c" translate="yes" xml:space="preserve">
          <source>While using distribution strategies, the variables created within the strategy's scope will be replicated across all the replicas and can be kept in sync using all-reduce algorithms.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1d21e39cec1dbe57a37b90b0658bf80c4c7b4e18" translate="yes" xml:space="preserve">
          <source>WholeFileReader</source>
          <target state="translated">WholeFileReader</target>
        </trans-unit>
        <trans-unit id="c10e00cc1061d25c1e75d6a6c9f37b8de149d520" translate="yes" xml:space="preserve">
          <source>WholeFileReaderV2</source>
          <target state="translated">WholeFileReaderV2</target>
        </trans-unit>
        <trans-unit id="4bc1c4e835b1b69225a6dcb4af4e28a2c06f52c5" translate="yes" xml:space="preserve">
          <source>Wide &amp;amp; Deep Model for regression and classification problems.</source>
          <target state="translated">回归和分类问题的宽模型和深模型。</target>
        </trans-unit>
        <trans-unit id="ce79ee760dc1e0dcf6f5314aeb909e856d4d894b" translate="yes" xml:space="preserve">
          <source>Width Multiplier (alpha) | ImageNet Acc | Multiply-Adds (M) | Params (M)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="681d52e4c1304a1ea24152480eaca88c3ee9ae33" translate="yes" xml:space="preserve">
          <source>Width of output image.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c58a6d122599315399444067e820708cd4baecf" translate="yes" xml:space="preserve">
          <source>Width of the result.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="664add438097fbd4307f814de8e62a10f8905588" translate="yes" xml:space="preserve">
          <source>Wikipedia</source>
          <target state="translated">Wikipedia</target>
        </trans-unit>
        <trans-unit id="f1e4a70a64ee6594b6f0a462b10bce93878f9218" translate="yes" xml:space="preserve">
          <source>Will NOT work in 2.x:</source>
          <target state="translated">不会在2.x中工作。</target>
        </trans-unit>
        <trans-unit id="1c0b2946d9427a7aeee307da8f37197ad5dcc849" translate="yes" xml:space="preserve">
          <source>Will dequeue a work unit from queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).</source>
          <target state="translated">如有必要,将从队列中删除一个工作单元(例如,当阅读器需要从一个新的文件开始阅读,因为它已经完成了前一个文件)。</target>
        </trans-unit>
        <trans-unit id="a8c3a8bad2b347c018d978650b0ad1f0a7153646" translate="yes" xml:space="preserve">
          <source>Will dequeue a work unit from queue if necessary (e.g., when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than num_records even before the last batch.</source>
          <target state="translated">必要时将从队列中删除一个工作单元(例如,当Reader需要从一个新的文件开始读取时,因为它已经完成了前一个文件)。即使在最后一个批次之前,它也可能返回少于num_records的数据。</target>
        </trans-unit>
        <trans-unit id="4ba78cccf43ec749a3b3f63b99ca41d71764e130" translate="yes" xml:space="preserve">
          <source>Will dequeue from the input queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f6ac5cb110e37a221155d9f3ad0cb5d4f454d263" translate="yes" xml:space="preserve">
          <source>Will dequeue from the input queue if necessary (e.g. when the Reader needs to start reading from a new file since it has finished with the previous file). It may return less than &lt;code&gt;num_records&lt;/code&gt; even before the last batch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="261f27a1f3bf63caf50878a078d18d17744de09c" translate="yes" xml:space="preserve">
          <source>Will make devices on the cluster available to use. Note that calling this more than once will work, but will invalidate any tensor handles on the old remote devices.</source>
          <target state="translated">将使集群上的设备可以使用。请注意,调用此函数不止一次,但会使旧的远程设备上的任何张量句柄失效。</target>
        </trans-unit>
        <trans-unit id="f6897db1eb6c38899798f94d9b426756d03b49e8" translate="yes" xml:space="preserve">
          <source>Will make devices on the remote host available to use. Note that calling this more than once will work, but will invalidate any tensor handles on the old remote devices.</source>
          <target state="translated">将使远程主机上的设备可以使用。请注意,调用此函数不止一次,但会使旧远程设备上的任何张量句柄失效。</target>
        </trans-unit>
        <trans-unit id="b4a06c844340471315b54b0dfa65c9f440e60872" translate="yes" xml:space="preserve">
          <source>Will the SparseTensor &lt;code&gt;A&lt;/code&gt; fit in memory if densified?</source>
          <target state="translated">如果密集 &lt;code&gt;A&lt;/code&gt; 话，SparseTensor A是否适合存储？</target>
        </trans-unit>
        <trans-unit id="0922711fb24b94894ef90c6a73b66b5cfaf62cb6" translate="yes" xml:space="preserve">
          <source>Will work in 1.x and 2.x (though deprecated in 2.x):</source>
          <target state="translated">可以在1.x和2.x中使用(尽管在2.x中已被废弃)。</target>
        </trans-unit>
        <trans-unit id="ce21a08c1a63e9b77f2a1c614615c252ca550863" translate="yes" xml:space="preserve">
          <source>WindowDataset</source>
          <target state="translated">WindowDataset</target>
        </trans-unit>
        <trans-unit id="7969fd6214a6335899a24dac6e8f7bd3aba9e39c" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;A&lt;/code&gt; the dense representation of this &lt;code&gt;Operator&lt;/code&gt;,</source>
          <target state="translated">使用 &lt;code&gt;A&lt;/code&gt; 表示该 &lt;code&gt;Operator&lt;/code&gt; ，</target>
        </trans-unit>
        <trans-unit id="ff9e0aefc629ac53bd25ea34ae417e2aeaf0766f" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;distribution=&quot;truncated_normal&quot; or &quot;untruncated_normal&quot;&lt;/code&gt;, samples are drawn from a truncated/untruncated normal distribution with a mean of zero and a standard deviation (after truncation, if used) &lt;code&gt;stddev = sqrt(scale / n)&lt;/code&gt; where n is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="234aca2d96fedfdf359061dabe09edd1bc5db7b3" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;distribution=&quot;truncated_normal&quot; or &quot;untruncated_normal&quot;&lt;/code&gt;, samples are drawn from a truncated/untruncated normal distribution with a mean of zero and a standard deviation (after truncation, if used) &lt;code&gt;stddev = sqrt(scale / n)&lt;/code&gt; where n is: - number of input units in the weight tensor, if mode = &quot;fan_in&quot; - number of output units, if mode = &quot;fan_out&quot; - average of the numbers of input and output units, if mode = &quot;fan_avg&quot;</source>
          <target state="translated">如果使用 &lt;code&gt;distribution=&quot;truncated_normal&quot; or &quot;untruncated_normal&quot;&lt;/code&gt; ，则从均值为零且标准差为标准差（如果使用了截断后，则为标准截断后）的截断/未截断正态分布中抽取 &lt;code&gt;stddev = sqrt(scale / n)&lt;/code&gt; ，其中n为：-如果mode =&amp;ldquo; fan_in&amp;rdquo;，则输入重量单位张数；如果mode =&amp;ldquo; fan_out&amp;rdquo;，则输出单位数目；如果mode =&amp;ldquo; fan_avg&amp;rdquo;，则输入和输出单位数的平均值</target>
        </trans-unit>
        <trans-unit id="4f723d75a7c2fa1bd113ada83e2655f70f9292fe" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;distribution=&quot;truncated_normal&quot; or &quot;untruncated_normal&quot;&lt;/code&gt;, samples are drawn from a truncated/untruncated normal distribution with a mean of zero and a standard deviation (after truncation, if used) &lt;code&gt;stddev = sqrt(scale / n)&lt;/code&gt;, where &lt;code&gt;n&lt;/code&gt; is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ff32194c2a7d6162d325ca83d7afa7acc313404" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;distribution=&quot;uniform&quot;&lt;/code&gt;, samples are drawn from a uniform distribution within &lt;code&gt;[-limit, limit]&lt;/code&gt;, where &lt;code&gt;limit = sqrt(3 * scale / n)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f5835e02899f12405b436a06a34c5acc97b07515" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;distribution=&quot;uniform&quot;&lt;/code&gt;, samples are drawn from a uniform distribution within [-limit, limit], with &lt;code&gt;limit = sqrt(3 * scale / n)&lt;/code&gt;.</source>
          <target state="translated">使用 &lt;code&gt;distribution=&quot;uniform&quot;&lt;/code&gt; ，样本是从[-limit，limit]内的均匀分布中提取的，其中 &lt;code&gt;limit = sqrt(3 * scale / n)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f8c1f4cf8df7060458ea559fb96747e733cc6916" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;height_shift_range=2&lt;/code&gt; possible values are integers &lt;code&gt;[-1, 0, +1]&lt;/code&gt;, same as with &lt;code&gt;height_shift_range=[-1, 0, +1]&lt;/code&gt;, while with &lt;code&gt;height_shift_range=1.0&lt;/code&gt; possible values are floats in the interval [-1.0, +1.0).</source>
          <target state="translated">与 &lt;code&gt;height_shift_range=2&lt;/code&gt; &lt;code&gt;[-1, 0, +1]&lt;/code&gt; 相同，对于height_shift_range = 2来说，可能的值是整数[-1，0，+ &lt;code&gt;height_shift_range=[-1, 0, +1]&lt;/code&gt; ，而对于 &lt;code&gt;height_shift_range=1.0&lt;/code&gt; 来说,可能是整数 [-1.0，+ 1.0）。</target>
        </trans-unit>
        <trans-unit id="8d71d2aae030803f505395187c99c592d13d4623" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;preserve_aspect_ratio=True&lt;/code&gt;, the aspect ratio is preserved, so &lt;code&gt;size&lt;/code&gt; is the maximum for each dimension:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f0be93f34340dbd9dbce5872c897c11744e5be0" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;width_shift_range=2&lt;/code&gt; possible values are integers &lt;code&gt;[-1, 0, +1]&lt;/code&gt;, same as with &lt;code&gt;width_shift_range=[-1, 0, +1]&lt;/code&gt;, while with &lt;code&gt;width_shift_range=1.0&lt;/code&gt; possible values are floats in the interval [-1.0, +1.0).</source>
          <target state="translated">与 &lt;code&gt;width_shift_range=2&lt;/code&gt; 个可能的值是整数 &lt;code&gt;[-1, 0, +1]&lt;/code&gt; ，同与 &lt;code&gt;width_shift_range=[-1, 0, +1]&lt;/code&gt; ，同时用 &lt;code&gt;width_shift_range=1.0&lt;/code&gt; 可能的值是浮在区间[-1.0，+ 1.0）。</target>
        </trans-unit>
        <trans-unit id="b3c5ce075d1d1fd6a0d1aad83b3d8c72b5bbfb7b" translate="yes" xml:space="preserve">
          <source>With a 1 in 2 chance, outputs the contents of &lt;code&gt;image&lt;/code&gt; flipped along the first dimension, which is &lt;code&gt;height&lt;/code&gt;. Otherwise output the image as-is. When passing a batch of images, each image will be randomly flipped independent of other images.</source>
          <target state="translated">几分之一的机会输出沿第一个维度（即 &lt;code&gt;height&lt;/code&gt; )翻转的 &lt;code&gt;image&lt;/code&gt; 的内容。否则，按原样输出图像。通过一批图像时，每个图像将独立于其他图像随机翻转。</target>
        </trans-unit>
        <trans-unit id="40d888cff933beb09db3d4f6596d9067e2878308" translate="yes" xml:space="preserve">
          <source>With a 1 in 2 chance, outputs the contents of &lt;code&gt;image&lt;/code&gt; flipped along the first dimension, which is &lt;code&gt;height&lt;/code&gt;. Otherwise, output the image as-is. When passing a batch of images, each image will be randomly flipped independent of other images.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dafbf9ee96525bc588fdd1a819189c2c04be4c99" translate="yes" xml:space="preserve">
          <source>With a 1 in 2 chance, outputs the contents of &lt;code&gt;image&lt;/code&gt; flipped along the second dimension, which is &lt;code&gt;width&lt;/code&gt;. Otherwise output the image as-is. When passing a batch of images, each image will be randomly flipped independent of other images.</source>
          <target state="translated">机会为2分之一，输出沿第二维（即 &lt;code&gt;width&lt;/code&gt; )翻转的 &lt;code&gt;image&lt;/code&gt; 的内容。否则，按原样输出图像。通过一批图像时，每个图像将独立于其他图像随机翻转。</target>
        </trans-unit>
        <trans-unit id="eb0883b4bf2a7901f51949844c31237c43d86ecb" translate="yes" xml:space="preserve">
          <source>With a &lt;code&gt;Coordinator&lt;/code&gt;, exceptions are reported to the coordinator and forgotten by the &lt;code&gt;QueueRunner&lt;/code&gt;.</source>
          <target state="translated">使用 &lt;code&gt;Coordinator&lt;/code&gt; ， &lt;code&gt;QueueRunner&lt;/code&gt; 异常报告给协调器，并由QueueRunner忘记。</target>
        </trans-unit>
        <trans-unit id="bf97c0b53fe731e0899e5aa3d7d1c5c2588e5d9e" translate="yes" xml:space="preserve">
          <source>With default values, it returns element-wise &lt;code&gt;max(x, 0)&lt;/code&gt;.</source>
          <target state="translated">使用默认值，它将返回逐元素的 &lt;code&gt;max(x, 0)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="db85e495eb55636b020a2a32657d103899296f2f" translate="yes" xml:space="preserve">
          <source>With default values, this returns the standard ReLU activation: &lt;code&gt;max(x, 0)&lt;/code&gt;, the element-wise maximum of 0 and the input tensor.</source>
          <target state="translated">使用默认值时，这将返回标准ReLU激活： &lt;code&gt;max(x, 0)&lt;/code&gt; ，逐个元素的最大值0和输入张量。</target>
        </trans-unit>
        <trans-unit id="8620f83bce4d323fd62431b72a3e3cd320c09979" translate="yes" xml:space="preserve">
          <source>With eager execution disabled (by default in TensorFlow 1.x and by calling disable_eager_execution() in TensorFlow 2.x), the following syntax can be used:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e7517aaf4d18e2c4d14a26fcfe70e076851f68b1" translate="yes" xml:space="preserve">
          <source>With eager execution this is a shape assertion, that returns the input:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f568c7dfa8797dfe677f7dd6f0a4ee1b0b5cbdba" translate="yes" xml:space="preserve">
          <source>With eager execution this operates as a shape assertion. Here the shapes match:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8b2addb5c48d67261f0b9d05eab9810322d19c59" translate="yes" xml:space="preserve">
          <source>With forwardprop, we specify a length-three vector in advance which multiplies the Jacobian. The &lt;code&gt;primals&lt;/code&gt; constructor argument is the parameter (a &lt;a href=&quot;../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../variable&quot;&gt;&lt;code&gt;tf.Variable&lt;/code&gt;&lt;/a&gt;) we're specifying a vector for, and the &lt;code&gt;tangents&lt;/code&gt; argument is the &quot;vector&quot; in Jacobian-vector product. If our goal is to compute the entire Jacobian matrix, forwardprop computes one column at a time while backprop computes one row at a time. Since the Jacobian in the linear regression example has only one row, backprop requires fewer invocations:</source>
          <target state="translated">使用forwardprop，我们预先指定一个三长度向量，该向量乘以Jacobian。该 &lt;code&gt;primals&lt;/code&gt; 构造函数的参数是参数（&lt;a href=&quot;../tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;../variable&quot;&gt; &lt;code&gt;tf.Variable&lt;/code&gt; &lt;/a&gt;）我们指定了一个矢量，而 &lt;code&gt;tangents&lt;/code&gt; 说法是在雅克比矢量产品的&amp;ldquo;载体&amp;rdquo;。如果我们的目标是计算整个雅可比矩阵，则前向运算符一次只计算一列，而后向运算符一次只计算一行。由于线性回归示例中的雅可比行只有一行，因此反向传播需要更少的调用：</target>
        </trans-unit>
        <trans-unit id="81932d1ace29ea95c92392af364f6a28c2c74781" translate="yes" xml:space="preserve">
          <source>With this definition, the gradient at x=100 will be correctly evaluated as 1.0.</source>
          <target state="translated">根据这个定义,x=100时的梯度将被正确地评估为1.0。</target>
        </trans-unit>
        <trans-unit id="317a1dfdd7bd7e17378c7a66cabfcee42b3896df" translate="yes" xml:space="preserve">
          <source>With y = f(x), computes the theoretical and numeric Jacobian dy/dx.</source>
          <target state="translated">在y=f(x)的情况下,计算理论和数值上的Jacobian dy/dx。</target>
        </trans-unit>
        <trans-unit id="659abe1496bd7876fc022d3e2b6b9cead33d30d8" translate="yes" xml:space="preserve">
          <source>Within a particular block, exactly one of these two things will be true:</source>
          <target state="translated">在一个特定的区块内,这两件事中恰恰有一件会是真的。</target>
        </trans-unit>
        <trans-unit id="13fe9d84fb92bd3745e88227dc1de58a44afddd1" translate="yes" xml:space="preserve">
          <source>Within a training loop, this argument sets how often host calls are performed during training. Host calls will be evaluated every n steps within a training loop where n is the value of this argument.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e7aeeddf89020677af5b4472b077bda01df8909" translate="yes" xml:space="preserve">
          <source>Within each worker, we will also split the data among all the worker devices (if more than one a present), and this will happen even if multi-worker sharding is disabled using the method above.</source>
          <target state="translated">在每个worker中,我们还将在所有worker设备中分割数据(如果存在多个设备),即使使用上述方法禁用多worker sharding,也会发生这种情况。</target>
        </trans-unit>
        <trans-unit id="4d99119d2f84477c4512754d5e0fcbfbe0f69b29" translate="yes" xml:space="preserve">
          <source>Within the &lt;code&gt;with sv.managed_session()&lt;/code&gt; block all variables in the graph have been initialized. In addition, a few services have been started to checkpoint the model and add summaries to the event log.</source>
          <target state="translated">在 &lt;code&gt;with sv.managed_session()&lt;/code&gt; 块内，图中的所有变量均已初始化。此外，已经启动了一些服务来检查模型并向事件日志中添加摘要。</target>
        </trans-unit>
        <trans-unit id="ab6e550ba7dbaa5e409cc2c91f32bbe98623ef14" translate="yes" xml:space="preserve">
          <source>Without &lt;a href=&quot;set_seed&quot;&gt;&lt;code&gt;tf.random.set_seed&lt;/code&gt;&lt;/a&gt; but with a &lt;code&gt;seed&lt;/code&gt; argument is specified, small changes to function graphs or previously executed operations will change the returned value. See &lt;a href=&quot;set_seed&quot;&gt;&lt;code&gt;tf.random.set_seed&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">如果没有&lt;a href=&quot;set_seed&quot;&gt; &lt;code&gt;tf.random.set_seed&lt;/code&gt; &lt;/a&gt;但指定了 &lt;code&gt;seed&lt;/code&gt; 参数，则对函数图或先前执行的操作进行小的更改将更改返回的值。有关详细信息，请参见&lt;a href=&quot;set_seed&quot;&gt; &lt;code&gt;tf.random.set_seed&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c8ccb2ad531bd87f8226e54b9b6a39a6b2307357" translate="yes" xml:space="preserve">
          <source>Without a &lt;code&gt;Coordinator&lt;/code&gt;, exceptions are captured by the &lt;code&gt;QueueRunner&lt;/code&gt; and made available in this &lt;code&gt;exceptions_raised&lt;/code&gt; property.</source>
          <target state="translated">如果没有 &lt;code&gt;Coordinator&lt;/code&gt; ，则 &lt;code&gt;QueueRunner&lt;/code&gt; 会捕获异常，并在此 &lt;code&gt;exceptions_raised&lt;/code&gt; 属性中将其变为可用。</target>
        </trans-unit>
        <trans-unit id="e696e263864301b6aa5902a80a4d0bff70cd1786" translate="yes" xml:space="preserve">
          <source>Word embeddings</source>
          <target state="translated">词语嵌入</target>
        </trans-unit>
        <trans-unit id="d98a4a13cdc22212a308bf1d89d6b7b049c03e2f" translate="yes" xml:space="preserve">
          <source>Worker devices vs. parameter devices: Most replica computations will happen on worker devices. Since we don't yet support model parallelism, there will be one worker device per replica. When using parameter servers or central storage, the set of devices holding variables may be different, otherwise the parameter devices might match the worker devices.</source>
          <target state="translated">工作者设备与参数设备。大部分的副本计算将在工作设备上进行。由于我们还不支持模型并行,所以每个副本会有一个worker设备。当使用参数服务器或中央存储时,持有变量的设备集可能是不同的,否则参数设备可能会与工人设备相匹配。</target>
        </trans-unit>
        <trans-unit id="e84ec097b7a7143f51f1fbc334ced683171b4c6c" translate="yes" xml:space="preserve">
          <source>Worker heartbeat op.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d20a3ffc32dcd238f452807ec295544fb73ee86b" translate="yes" xml:space="preserve">
          <source>WorkerHeartbeat</source>
          <target state="translated">WorkerHeartbeat</target>
        </trans-unit>
        <trans-unit id="398f69182b10972e971055c07ca464bed84b735b" translate="yes" xml:space="preserve">
          <source>Working with Bounding Boxes</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="51eeae9fe36f031b9bbc4e7ce891844c997b1b08" translate="yes" xml:space="preserve">
          <source>WrapDatasetVariant</source>
          <target state="translated">WrapDatasetVariant</target>
        </trans-unit>
        <trans-unit id="7a41e51968848ca6ebbc48f555f3248a673bee90" translate="yes" xml:space="preserve">
          <source>Wrapped inputs (identity standins that have additional metadata). These are also are also tf.Tensor's.</source>
          <target state="translated">Wrapped inputs(具有附加元数据的身份standins)。这些也是tf.Tensor的。</target>
        </trans-unit>
        <trans-unit id="384f837341c0fc0fa71de4eb8e929b11e7daf27f" translate="yes" xml:space="preserve">
          <source>Wrapped outputs (identity standins that have additional metadata). These are also tf.Tensor's.</source>
          <target state="translated">Wrapped outputs (具有附加元数据的身份代表)。这些也是tf.Tensor的。</target>
        </trans-unit>
        <trans-unit id="a298562930ec81f39bad36c204ae1b1059782591" translate="yes" xml:space="preserve">
          <source>Wrapped values: In order to represent values parallel across devices (either replicas or the devices associated with a particular value), we wrap them in a &quot;PerReplica&quot; or &quot;Mirrored&quot; object that contains a map from replica id to values. &quot;PerReplica&quot; is used when the value may be different across replicas, and &quot;Mirrored&quot; when the value are the same.</source>
          <target state="translated">包裹的值。为了在不同的设备(无论是复制体还是与特定值相关联的设备)之间平行地表示值,我们将它们包裹在一个 &quot;PerReplica &quot;或 &quot;Mirrored &quot;对象中,该对象包含一个从复制体ID到值的映射。&quot;PerReplica &quot;是在值在各个副本中可能不同时使用的,而 &quot;Mirrored &quot;是在值相同时使用的。</target>
        </trans-unit>
        <trans-unit id="890d066e219e9b18bb1c24edca8341903adbf397" translate="yes" xml:space="preserve">
          <source>Wrapper allowing a stack of RNN cells to behave as a single cell.</source>
          <target state="translated">允许RNN单元堆栈表现为单个单元的封装器。</target>
        </trans-unit>
        <trans-unit id="b4013106a25f53b1f217b9c3881270bfb6839489" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;../../graph#add_to_collection&quot;&gt;&lt;code&gt;Graph.add_to_collection()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">使用默认图形的&lt;a href=&quot;../../graph#add_to_collection&quot;&gt; &lt;code&gt;Graph.add_to_collection()&lt;/code&gt; 的&lt;/a&gt;包装。</target>
        </trans-unit>
        <trans-unit id="caa4198da3fc275ac76081f0b3d1bfe03cdd54fe" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;../../graph#add_to_collections&quot;&gt;&lt;code&gt;Graph.add_to_collections()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">使用默认图形的&lt;a href=&quot;../../graph#add_to_collections&quot;&gt; &lt;code&gt;Graph.add_to_collections()&lt;/code&gt; 的&lt;/a&gt;包装。</target>
        </trans-unit>
        <trans-unit id="264584356d25721fe348ac95a81a23f4cca709f4" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;../../graph#container&quot;&gt;&lt;code&gt;Graph.container()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">使用默认图形的&lt;a href=&quot;../../graph#container&quot;&gt; &lt;code&gt;Graph.container()&lt;/code&gt; 的&lt;/a&gt;包装。</target>
        </trans-unit>
        <trans-unit id="55f30bd6389eac4a761d1daf54731878db305f29" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;../../graph#device&quot;&gt;&lt;code&gt;Graph.device()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">使用默认图形的&lt;a href=&quot;../../graph#device&quot;&gt; &lt;code&gt;Graph.device()&lt;/code&gt; 的&lt;/a&gt;包装。</target>
        </trans-unit>
        <trans-unit id="0d1e4b32d773cedbb0330530a2382ac0bddfaba8" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;../../graph#get_collection&quot;&gt;&lt;code&gt;Graph.get_collection()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">使用默认图形的&lt;a href=&quot;../../graph#get_collection&quot;&gt; &lt;code&gt;Graph.get_collection()&lt;/code&gt; 的&lt;/a&gt;包装。</target>
        </trans-unit>
        <trans-unit id="190d7f80a83427a732312702b0799aa4fd4270f6" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;../../graph#get_collection_ref&quot;&gt;&lt;code&gt;Graph.get_collection_ref()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">使用默认图形的&lt;a href=&quot;../../graph#get_collection_ref&quot;&gt; &lt;code&gt;Graph.get_collection_ref()&lt;/code&gt; 的&lt;/a&gt;包装。</target>
        </trans-unit>
        <trans-unit id="89aa6df88d5fbd573c88df4510194a5615a938d5" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;graph#control_dependencies&quot;&gt;&lt;code&gt;Graph.control_dependencies()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="translated">使用默认图的&lt;a href=&quot;graph#control_dependencies&quot;&gt; &lt;code&gt;Graph.control_dependencies()&lt;/code&gt; &lt;/a&gt;包装。</target>
        </trans-unit>
        <trans-unit id="495b2dd43c40b93e9963a63ddff35ea9979905aa" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#add_to_collection&quot;&gt;&lt;code&gt;Graph.add_to_collection()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d79bebe0b09c5426685f0e539708d5d444124cc5" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#add_to_collections&quot;&gt;&lt;code&gt;Graph.add_to_collections()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="128db28adf836bfc21dd5fc549881d12855001a9" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#container&quot;&gt;&lt;code&gt;Graph.container()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="189fdde0fe51377c9c7df91b00d623f673fa3f34" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#device&quot;&gt;&lt;code&gt;Graph.device()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="04af0f377456628e691bf3f85dc843ed001f5c1b" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#get_collection&quot;&gt;&lt;code&gt;Graph.get_collection()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bed252c3eb66e3bb7424c1c2a3122d3cfea4dfb8" translate="yes" xml:space="preserve">
          <source>Wrapper for &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/Graph#get_collection_ref&quot;&gt;&lt;code&gt;Graph.get_collection_ref()&lt;/code&gt;&lt;/a&gt; using the default graph.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f37a810bede5b1390f81fea4b6be44aec37fbe5" translate="yes" xml:space="preserve">
          <source>Wrapper for using the Scikit-Learn API with Keras models.</source>
          <target state="translated">用于在Keras模型中使用Scikit-Learn API的封装器。</target>
        </trans-unit>
        <trans-unit id="1fff217e51e30156f66782f095344b6ef97a4794" translate="yes" xml:space="preserve">
          <source>Wrappers for primitive Neural Net (NN) Operations.</source>
          <target state="translated">原始神经网(NN)操作的封装器。</target>
        </trans-unit>
        <trans-unit id="7837b0cc643e2f001702979a842efc255ca9da70" translate="yes" xml:space="preserve">
          <source>Wrappers take another layer and augment it in various ways. Do not use this class as a layer, it is only an abstract base class. Two usable wrappers are the &lt;code&gt;TimeDistributed&lt;/code&gt; and &lt;code&gt;Bidirectional&lt;/code&gt; wrappers.</source>
          <target state="translated">包装器采用了另一层，并以各种方式对其进行了扩充。不要将此类用作层，它只是一个抽象基类。两个可用的包装器是 &lt;code&gt;TimeDistributed&lt;/code&gt; 和 &lt;code&gt;Bidirectional&lt;/code&gt; 包装器。</target>
        </trans-unit>
        <trans-unit id="3381242ca6a26b413c24d45326c44a72bdf78124" translate="yes" xml:space="preserve">
          <source>Wraps &lt;code&gt;call&lt;/code&gt;, applying pre- and post-processing steps.</source>
          <target state="translated">包装 &lt;code&gt;call&lt;/code&gt; ，应用预处理和后处理步骤。</target>
        </trans-unit>
        <trans-unit id="c62cdbd485fcae9d06ca69b5b8397cfed846cb17" translate="yes" xml:space="preserve">
          <source>Wraps a given text to a maximum line length and returns it.</source>
          <target state="translated">将给定的文本包装成最大行长并返回。</target>
        </trans-unit>
        <trans-unit id="8829b71bb0967a49348c7e7adf87bb1886403153" translate="yes" xml:space="preserve">
          <source>Wraps a python function and uses it as a TensorFlow op.</source>
          <target state="translated">封装一个python函数,并将其作为一个TensorFlow操作使用。</target>
        </trans-unit>
        <trans-unit id="8abaa30206e6292e5e70829b82947753e40ac335" translate="yes" xml:space="preserve">
          <source>Wraps a python function into a TensorFlow op that executes it eagerly.</source>
          <target state="translated">将一个python函数封装到一个TensorFlow op中,并急切地执行它。</target>
        </trans-unit>
        <trans-unit id="82ab6329b627d91bd83e44d4fe219ae9ac70441c" translate="yes" xml:space="preserve">
          <source>Wraps a value that may/may not be present at runtime.</source>
          <target state="translated">包裹一个在运行时可能/可能不存在的值。</target>
        </trans-unit>
        <trans-unit id="19c9eb1de0862700ff8502bf5e2ae450855f9133" translate="yes" xml:space="preserve">
          <source>Wraps arbitrary expressions as a &lt;code&gt;Layer&lt;/code&gt; object.</source>
          <target state="translated">将任意表达式包装为 &lt;code&gt;Layer&lt;/code&gt; 对象。</target>
        </trans-unit>
        <trans-unit id="352659d98f226f42ae5f9c0345757253d7e85e6e" translate="yes" xml:space="preserve">
          <source>Wraps the TF 1.x function fn into a graph function.</source>
          <target state="translated">将TF 1.x函数fn包装成一个图形函数。</target>
        </trans-unit>
        <trans-unit id="20b643b52957c38a95449d4001fccba2e0bcd12a" translate="yes" xml:space="preserve">
          <source>Write &lt;code&gt;value&lt;/code&gt; into index &lt;code&gt;index&lt;/code&gt; of the TensorArray.</source>
          <target state="translated">将 &lt;code&gt;value&lt;/code&gt; 写入TensorArray的索引 &lt;code&gt;index&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="483cdae75a267d35fd6e83b5653511d5a80cff0f" translate="yes" xml:space="preserve">
          <source>Write a customized optimizer.</source>
          <target state="translated">写一个定制的优化器。</target>
        </trans-unit>
        <trans-unit id="c8c630765322886f9847c22284b5fa8d2851f2ce" translate="yes" xml:space="preserve">
          <source>Write a histogram summary.</source>
          <target state="translated">写一个直方图总结。</target>
        </trans-unit>
        <trans-unit id="b0075d853115d3caa9b112367f26a9151c343826" translate="yes" xml:space="preserve">
          <source>Write a scalar summary.</source>
          <target state="translated">写一篇标量总结。</target>
        </trans-unit>
        <trans-unit id="76fe5c36d30ae305e5a6503ba46502eb4a13331d" translate="yes" xml:space="preserve">
          <source>Write a string record to the file.</source>
          <target state="translated">向文件中写入一个字符串记录。</target>
        </trans-unit>
        <trans-unit id="5b53b6d51e993b431470b37c217c59753ff1cc50" translate="yes" xml:space="preserve">
          <source>Write a text summary.</source>
          <target state="translated">写一篇文字摘要。</target>
        </trans-unit>
        <trans-unit id="d21f1a898ccfeda0fe2e731850a28cfafae9bb2b" translate="yes" xml:space="preserve">
          <source>Write an audio summary.</source>
          <target state="translated">写一篇音频摘要。</target>
        </trans-unit>
        <trans-unit id="21a6eeb1ed26a2be90d3ee27390e8e2bda8c0962" translate="yes" xml:space="preserve">
          <source>Write an image summary.</source>
          <target state="translated">写一篇形象总结。</target>
        </trans-unit>
        <trans-unit id="61f330bc93ccdaf24823ff722556ec64642374a9" translate="yes" xml:space="preserve">
          <source>Write data via Write and read via Read or Pack.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b36ad4c95f8d96567af24f9c3edf891e090ad84" translate="yes" xml:space="preserve">
          <source>Write the serialized data to one or more files</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2a6db3a59303fe109b5b5137b804a970c912e147" translate="yes" xml:space="preserve">
          <source>Write this:</source>
          <target state="translated">写这个。</target>
        </trans-unit>
        <trans-unit id="6226f3d4ff16e7406bb0cdaedd7746abc5236636" translate="yes" xml:space="preserve">
          <source>WriteAudioSummary</source>
          <target state="translated">WriteAudioSummary</target>
        </trans-unit>
        <trans-unit id="94dee2fafe1d10d5ce6db659eb0c0fcb3d5842be" translate="yes" xml:space="preserve">
          <source>WriteFile</source>
          <target state="translated">WriteFile</target>
        </trans-unit>
        <trans-unit id="1ea63605e83eb910f0b5d75fe28edb99127b24d1" translate="yes" xml:space="preserve">
          <source>WriteGraphSummary</source>
          <target state="translated">WriteGraphSummary</target>
        </trans-unit>
        <trans-unit id="e5637c5d46d26a2b2c82fac03d3b096952d4bf28" translate="yes" xml:space="preserve">
          <source>WriteHistogramSummary</source>
          <target state="translated">WriteHistogramSummary</target>
        </trans-unit>
        <trans-unit id="6b51491885b3d25ccb268be4c798a6b10e3d5b9d" translate="yes" xml:space="preserve">
          <source>WriteImageSummary</source>
          <target state="translated">WriteImageSummary</target>
        </trans-unit>
        <trans-unit id="4a572e40863e9455f5578aec02c3c8842448412b" translate="yes" xml:space="preserve">
          <source>WriteRawProtoSummary</source>
          <target state="translated">WriteRawProtoSummary</target>
        </trans-unit>
        <trans-unit id="88f81acb7e965e0b6b73b00739698ed9296482c6" translate="yes" xml:space="preserve">
          <source>WriteScalarSummary</source>
          <target state="translated">WriteScalarSummary</target>
        </trans-unit>
        <trans-unit id="cb73c41075bf3f01d3eb7791f75623bd77006f1a" translate="yes" xml:space="preserve">
          <source>WriteSummary</source>
          <target state="translated">WriteSummary</target>
        </trans-unit>
        <trans-unit id="29a4cf60a26ded7a2c3aed4855011f0c8495cfe3" translate="yes" xml:space="preserve">
          <source>Writes &lt;code&gt;MetaGraphDef&lt;/code&gt; to save_path/filename.</source>
          <target state="translated">将 &lt;code&gt;MetaGraphDef&lt;/code&gt; 写入save_path / filename。</target>
        </trans-unit>
        <trans-unit id="9fa9b02b6070f8899e280b451f66b6bd6b575f93" translate="yes" xml:space="preserve">
          <source>Writes &lt;code&gt;Summary&lt;/code&gt; protocol buffers to event files.</source>
          <target state="translated">将 &lt;code&gt;Summary&lt;/code&gt; 协议缓冲区写入事件文件。</target>
        </trans-unit>
        <trans-unit id="babca584f4aae5499f0f80472fec83895337e06c" translate="yes" xml:space="preserve">
          <source>Writes a &lt;code&gt;SavedModel&lt;/code&gt; protocol buffer to disk.</source>
          <target state="translated">将 &lt;code&gt;SavedModel&lt;/code&gt; 协议缓冲区写入磁盘。</target>
        </trans-unit>
        <trans-unit id="ec531d54bcd6301e96265e18d2ce6d1e6c9a4e3b" translate="yes" xml:space="preserve">
          <source>Writes a dataset to a TFRecord file.</source>
          <target state="translated">将一个数据集写入TFRecord文件。</target>
        </trans-unit>
        <trans-unit id="233cc84a3671355de983f6c125f1b2c0c8c7fc01" translate="yes" xml:space="preserve">
          <source>Writes a generic summary to the default SummaryWriter if one exists.</source>
          <target state="translated">如果存在一个默认的SummaryWriter,则将一个通用的摘要写入其中。</target>
        </trans-unit>
        <trans-unit id="d15430e669e23755ec2b520aa11323169d62858a" translate="yes" xml:space="preserve">
          <source>Writes a graph proto to a file.</source>
          <target state="translated">将一个图形原件写入文件。</target>
        </trans-unit>
        <trans-unit id="9976fe10182472bc6e870e0863e6f6d04fac02a5" translate="yes" xml:space="preserve">
          <source>Writes a set of weights into the opaque params buffer so they can be used in upcoming training or inferences.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0faccf4f9a4005aa59d77d68c1f6677070d11c1f" translate="yes" xml:space="preserve">
          <source>Writes a summary using raw &lt;a href=&quot;../../compat/v1/summary&quot;&gt;&lt;code&gt;tf.compat.v1.Summary&lt;/code&gt;&lt;/a&gt; protocol buffers.</source>
          <target state="translated">使用原始的&lt;a href=&quot;../../compat/v1/summary&quot;&gt; &lt;code&gt;tf.compat.v1.Summary&lt;/code&gt; &lt;/a&gt;协议缓冲区编写摘要。</target>
        </trans-unit>
        <trans-unit id="5adf0b1064881763cf04269a02e41a3ea90a5906" translate="yes" xml:space="preserve">
          <source>Writes a training checkpoint.</source>
          <target state="translated">写一个训练检查点。</target>
        </trans-unit>
        <trans-unit id="f2b611268944160d96a5b9a8fcf79249e74eb83f" translate="yes" xml:space="preserve">
          <source>Writes contents to the file at input filename. Creates file and recursively</source>
          <target state="translated">在输入文件名时将内容写入文件。创建文件并递归</target>
        </trans-unit>
        <trans-unit id="7fb1d95dfee1bc896877fefd9cbe8b4b9c44e0bf" translate="yes" xml:space="preserve">
          <source>Writes file_content to the file. Appends to the end of the file.</source>
          <target state="translated">将file_content写入文件。添加到文件的末尾。</target>
        </trans-unit>
        <trans-unit id="a2ce6c85367b8678f1f61797d0dc74828b05ea19" translate="yes" xml:space="preserve">
          <source>Writes new value to variable's memory. Doesn't add ops to the graph.</source>
          <target state="translated">将新的值写入变量的内存。不在图形中添加操作。</target>
        </trans-unit>
        <trans-unit id="99b81ac6b0204ef18bee769984926a4ea6b12daf" translate="yes" xml:space="preserve">
          <source>Writes the given dataset to the given file using the TFRecord format.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="45f84d521a5351104bc075acf3695dd6bad6898e" translate="yes" xml:space="preserve">
          <source>Writing custom layers and models with Keras</source>
          <target state="translated">使用Keras编写自定义图层和模型</target>
        </trans-unit>
        <trans-unit id="c6a0c4f8b902d47bac80ddaf6bf34b2fbf4f9666" translate="yes" xml:space="preserve">
          <source>Xception V1 model for Keras.</source>
          <target state="translated">Xception V1型号为Keras。</target>
        </trans-unit>
        <trans-unit id="1b60eef1486f042121cb1abb403bfa5983083402" translate="yes" xml:space="preserve">
          <source>Xdivy</source>
          <target state="translated">Xdivy</target>
        </trans-unit>
        <trans-unit id="9ed9396da589e127b120b5abc619086b34774295" translate="yes" xml:space="preserve">
          <source>Xlog1py</source>
          <target state="translated">Xlog1py</target>
        </trans-unit>
        <trans-unit id="12f9191163bb7e9e63172afc062b7b9642ca43a1" translate="yes" xml:space="preserve">
          <source>Xlogy</source>
          <target state="translated">Xlogy</target>
        </trans-unit>
        <trans-unit id="bccf09988d791dbda2da42e387167025b7d54ccb" translate="yes" xml:space="preserve">
          <source>YAML string or open file encoding a model configuration.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dfea80b5fe13bab375ae2dddcdd3dbb43f1a3633" translate="yes" xml:space="preserve">
          <source>Yann LeCun and Corinna Cortes hold the copyright of MNIST dataset, which is a derivative work from original NIST datasets. MNIST dataset is made available under the terms of the &lt;a href=&quot;https://creativecommons.org/licenses/by-sa/3.0/&quot;&gt;Creative Commons Attribution-Share Alike 3.0 license.&lt;/a&gt;</source>
          <target state="translated">Yann LeCun和Corinna Cortes拥有MNIST数据集的版权，这是原始NIST数据集的衍生作品。MNIST数据集根据&lt;a href=&quot;https://creativecommons.org/licenses/by-sa/3.0/&quot;&gt;知识共享署名-相同方式共享3.0许可的条款提供。&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="738a2b66281e5ca4973cbceebc923d1996e03dad" translate="yes" xml:space="preserve">
          <source>Yields</source>
          <target state="translated">Yields</target>
        </trans-unit>
        <trans-unit id="f0d948a8bdb9eb6bb818e3240fb3b2534756d8f7" translate="yes" xml:space="preserve">
          <source>Yields predictions for given features.</source>
          <target state="translated">产生给定特征的预测。</target>
        </trans-unit>
        <trans-unit id="c970e3f1e790a2a4cd28b40401902501b9bc2d74" translate="yes" xml:space="preserve">
          <source>Yields:</source>
          <target state="translated">Yields:</target>
        </trans-unit>
        <trans-unit id="e930f451f4aa0e180bfec9e3ca9b3c51172a0d23" translate="yes" xml:space="preserve">
          <source>You can access a layer's regularization penalties by calling &lt;code&gt;layer.losses&lt;/code&gt; after calling the layer on inputs.</source>
          <target state="translated">您可以在输入上调用层后调用 &lt;code&gt;layer.losses&lt;/code&gt; 来访问层的正则化惩罚。</target>
        </trans-unit>
        <trans-unit id="59874a67ef5f7bd864b9ef3d3bb9393f8444ef02" translate="yes" xml:space="preserve">
          <source>You can access the raw &lt;a href=&quot;../session&quot;&gt;&lt;code&gt;tf.compat.v1.Session&lt;/code&gt;&lt;/a&gt; object used by &lt;code&gt;SingularMonitoredSession&lt;/code&gt;, whereas in MonitoredSession the raw session is private. This can be used:</source>
          <target state="translated">您可以访问 &lt;code&gt;SingularMonitoredSession&lt;/code&gt; 使用的原始&lt;a href=&quot;../session&quot;&gt; &lt;code&gt;tf.compat.v1.Session&lt;/code&gt; &lt;/a&gt;对象，而在MonitoredSession中，原始会话是私有的。可以使用：</target>
        </trans-unit>
        <trans-unit id="6ccb3baa3a0cbd712e9c239e9968fd1fb9ad400a" translate="yes" xml:space="preserve">
          <source>You can add an outer &lt;code&gt;batch&lt;/code&gt; axis by passing &lt;code&gt;axis=0&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e7c18d1ca2444e96db7357ab3243c23ba7a401f2" translate="yes" xml:space="preserve">
          <source>You can also pass a &lt;a href=&quot;../../../../distribute/cluster_resolver/clusterresolver&quot;&gt;&lt;code&gt;distribute.cluster_resolver.ClusterResolver&lt;/code&gt;&lt;/a&gt; instance when instantiating the strategy. The task_type, task_id etc. will be parsed from the resolver instance instead of from the &lt;code&gt;TF_CONFIG&lt;/code&gt; env var.</source>
          <target state="translated">实例化策略时，您还可以传递&lt;a href=&quot;../../../../distribute/cluster_resolver/clusterresolver&quot;&gt; &lt;code&gt;distribute.cluster_resolver.ClusterResolver&lt;/code&gt; &lt;/a&gt;实例。将从解析器实例而不是从 &lt;code&gt;TF_CONFIG&lt;/code&gt; env var 解析task_type，task_id等。</target>
        </trans-unit>
        <trans-unit id="6e5fb70392b78f99083230f0a97aa193ef7fda3b" translate="yes" xml:space="preserve">
          <source>You can also pass a &lt;a href=&quot;../cluster_resolver/clusterresolver&quot;&gt;&lt;code&gt;distribute.cluster_resolver.ClusterResolver&lt;/code&gt;&lt;/a&gt; instance when instantiating the strategy. The task_type, task_id etc. will be parsed from the resolver instance instead of from the &lt;code&gt;TF_CONFIG&lt;/code&gt; env var.</source>
          <target state="translated">实例化策略时，您还可以传递&lt;a href=&quot;../cluster_resolver/clusterresolver&quot;&gt; &lt;code&gt;distribute.cluster_resolver.ClusterResolver&lt;/code&gt; &lt;/a&gt;实例。将从解析器实例而不是从 &lt;code&gt;TF_CONFIG&lt;/code&gt; env var 解析task_type，task_id等。</target>
        </trans-unit>
        <trans-unit id="5e1ab8575cb5168a5735bdcf646fbc24611fcf94" translate="yes" xml:space="preserve">
          <source>You can also pass a &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver&quot;&gt;&lt;code&gt;distribute.cluster_resolver.ClusterResolver&lt;/code&gt;&lt;/a&gt; instance when instantiating the strategy. The task_type, task_id etc. will be parsed from the resolver instance instead of from the &lt;code&gt;TF_CONFIG&lt;/code&gt; env var.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ea8ecb649a2869edbe22ed0fa4b60b444f6b3240" translate="yes" xml:space="preserve">
          <source>You can also pass the following additional pieces to the constructor:</source>
          <target state="translated">你也可以向构造函数传递以下附加部分。</target>
        </trans-unit>
        <trans-unit id="beae5b5e28d39839a95e5dcf5642d815e8f5ff5c" translate="yes" xml:space="preserve">
          <source>You can also specify &lt;code&gt;config&lt;/code&gt; of the loss to this function by passing dict containing &lt;code&gt;class_name&lt;/code&gt; and &lt;code&gt;config&lt;/code&gt; as an identifier. Also note that the &lt;code&gt;class_name&lt;/code&gt; must map to a &lt;code&gt;Loss&lt;/code&gt; class</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c69555354a0bc222fe10d4f7bc39675e8b5f8158" translate="yes" xml:space="preserve">
          <source>You can also specify &lt;code&gt;config&lt;/code&gt; of the metric to this function by passing dict containing &lt;code&gt;class_name&lt;/code&gt; and &lt;code&gt;config&lt;/code&gt; as an identifier. Also note that the &lt;code&gt;class_name&lt;/code&gt; must map to a &lt;code&gt;Metric&lt;/code&gt; class</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="52dc5982a3012b85661499626a8a8038241b7ffd" translate="yes" xml:space="preserve">
          <source>You can also use &lt;a href=&quot;py_function&quot;&gt;&lt;code&gt;tf.py_function&lt;/code&gt;&lt;/a&gt; to debug your models at runtime using Python tools, i.e., you can isolate portions of your code that you want to debug, wrap them in Python functions and insert &lt;code&gt;pdb&lt;/code&gt; tracepoints or print statements as desired, and wrap those functions in &lt;a href=&quot;py_function&quot;&gt;&lt;code&gt;tf.py_function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">您还可以使用&lt;a href=&quot;py_function&quot;&gt; &lt;code&gt;tf.py_function&lt;/code&gt; &lt;/a&gt;在运行时使用Python工具调试模型，即，可以隔离要调试的代码部分，将其包装在Python函数中，并根据需要插入 &lt;code&gt;pdb&lt;/code&gt; 跟踪点或打印语句，然后包装这些&lt;a href=&quot;py_function&quot;&gt; &lt;code&gt;tf.py_function&lt;/code&gt; 中的&lt;/a&gt;函数。</target>
        </trans-unit>
        <trans-unit id="6039b0dfd851cbc8275eabb888826394ae1b2ab1" translate="yes" xml:space="preserve">
          <source>You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../../../../distribute/distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; instance returned by this API to query the &lt;a href=&quot;../../../../typespec&quot;&gt;&lt;code&gt;tf.TypeSpec&lt;/code&gt;&lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d6714d05f3ec58957845e4e02d40676c59a816f4" translate="yes" xml:space="preserve">
          <source>You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../../../../distribute/distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; returned by this API to query the &lt;a href=&quot;../../../../typespec&quot;&gt;&lt;code&gt;tf.TypeSpec&lt;/code&gt;&lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="153ea4bbc50ebf5e0a8403f29bb6788447deb11c" translate="yes" xml:space="preserve">
          <source>You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../../../distribute/distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; instance returned by this API to query the &lt;a href=&quot;../../../typespec&quot;&gt;&lt;code&gt;tf.TypeSpec&lt;/code&gt;&lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ac6704fae664e72c4f0645536563ff0de5afe80" translate="yes" xml:space="preserve">
          <source>You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../../../distribute/distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; returned by this API to query the &lt;a href=&quot;../../../typespec&quot;&gt;&lt;code&gt;tf.TypeSpec&lt;/code&gt;&lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dcaf3f8131e1ea14866dcfb7fa41b1b0194e1d80" translate="yes" xml:space="preserve">
          <source>You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; instance returned by this API to query the &lt;a href=&quot;../../typespec&quot;&gt;&lt;code&gt;tf.TypeSpec&lt;/code&gt;&lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3385c4f6d32325dcf782cc9941db6f2f275241a0" translate="yes" xml:space="preserve">
          <source>You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;../distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; returned by this API to query the &lt;a href=&quot;../../typespec&quot;&gt;&lt;code&gt;tf.TypeSpec&lt;/code&gt;&lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68d5075223b70a3c64aec568d9f4b72b9521b3e8" translate="yes" xml:space="preserve">
          <source>You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; instance returned by this API to query the &lt;a href=&quot;../typespec&quot;&gt;&lt;code&gt;tf.TypeSpec&lt;/code&gt;&lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3234565ee9ebae226cb6498a83d504945b3da8d1" translate="yes" xml:space="preserve">
          <source>You can also use the &lt;code&gt;element_spec&lt;/code&gt; property of the &lt;a href=&quot;distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; returned by this API to query the &lt;a href=&quot;../typespec&quot;&gt;&lt;code&gt;tf.TypeSpec&lt;/code&gt;&lt;/a&gt; of the elements returned by the iterator. This can be used to set the &lt;code&gt;input_signature&lt;/code&gt; property of a &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9bf7e26d5c0cf51e4e4917f1eb23c3d37328691" translate="yes" xml:space="preserve">
          <source>You can cast a Keras variable but it still returns a Keras tensor.</source>
          <target state="translated">你可以铸造一个Keras变量,但它仍然返回一个Keras张量。</target>
        </trans-unit>
        <trans-unit id="3a3ea187f9997f3359ee54a5e203867aab3b8b27" translate="yes" xml:space="preserve">
          <source>You can create a &lt;a href=&quot;distributediterator&quot;&gt;&lt;code&gt;tf.distribute.DistributedIterator&lt;/code&gt;&lt;/a&gt; by calling &lt;code&gt;iter&lt;/code&gt; on a &lt;a href=&quot;distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt; or creating a python loop over a &lt;a href=&quot;distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c77d6bc9128a9a471ae009d158974ccffab599c2" translate="yes" xml:space="preserve">
          <source>You can disable dataset sharding across workers using the &lt;code&gt;auto_shard&lt;/code&gt; option in &lt;a href=&quot;../../../../data/experimental/distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">您可以禁用整个使用工数据集分片 &lt;code&gt;auto_shard&lt;/code&gt; 在选项&lt;a href=&quot;../../../../data/experimental/distributeoptions&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="abcd404b6ec568dc3b6a192e74ff39fd2a467efc" translate="yes" xml:space="preserve">
          <source>You can disable dataset sharding across workers using the &lt;code&gt;auto_shard&lt;/code&gt; option in &lt;a href=&quot;../../../data/experimental/distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">您可以禁用整个使用工数据集分片 &lt;code&gt;auto_shard&lt;/code&gt; 在选项&lt;a href=&quot;../../../data/experimental/distributeoptions&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="755b5985c40de671a4eb4bfb403b9e1a2cab5a03" translate="yes" xml:space="preserve">
          <source>You can disable dataset sharding across workers using the &lt;code&gt;auto_shard&lt;/code&gt; option in &lt;a href=&quot;../../data/experimental/distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">您可以禁用整个使用工数据集分片 &lt;code&gt;auto_shard&lt;/code&gt; 在选项&lt;a href=&quot;../../data/experimental/distributeoptions&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="823e308b708edca072a13279a53e3cee2ec00199" translate="yes" xml:space="preserve">
          <source>You can disable dataset sharding across workers using the &lt;code&gt;auto_shard&lt;/code&gt; option in &lt;a href=&quot;../data/experimental/distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">您可以禁用整个使用工数据集分片 &lt;code&gt;auto_shard&lt;/code&gt; 在选项&lt;a href=&quot;../data/experimental/distributeoptions&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="77348208b8f388d909966c4b5fc03169d2df6ba3" translate="yes" xml:space="preserve">
          <source>You can find more information about TensorBoard &lt;a href=&quot;https://www.tensorflow.org/get_started/summaries_and_tensorboard&quot;&gt;here&lt;/a&gt;.</source>
          <target state="translated">您可以&lt;a href=&quot;https://www.tensorflow.org/get_started/summaries_and_tensorboard&quot;&gt;在此处&lt;/a&gt;找到有关TensorBoard的更多信息。</target>
        </trans-unit>
        <trans-unit id="0be849f93096dd779e7a88c6ab36a4f32c9e43a7" translate="yes" xml:space="preserve">
          <source>You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like:</source>
          <target state="translated">你可以使用全局批量大小实现'SUM_OVER_BATCH_SIZE',比如。</target>
        </trans-unit>
        <trans-unit id="afa5195343dbaa120879b06f025ea84ad87e6a32" translate="yes" xml:space="preserve">
          <source>You can modify the operations in place, but modifications to the list such as inserts/delete have no effect on the list of operations known to the graph.</source>
          <target state="translated">你可以在原地修改操作,但对列表的修改,如插入/删除,对图中已知的操作列表没有影响。</target>
        </trans-unit>
        <trans-unit id="1c3d65193eb861cca9fdcde225478e0055db490e" translate="yes" xml:space="preserve">
          <source>You can now use table in functions like &lt;a href=&quot;../../../nn/embedding_lookup&quot;&gt;&lt;code&gt;tf.nn.embedding_lookup&lt;/code&gt;&lt;/a&gt; to perform your embedding lookup and pass to your model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c02260f75018990666dbe2089789cfb29e9e718" translate="yes" xml:space="preserve">
          <source>You can pass None to clear the control dependencies:</source>
          <target state="translated">你可以通过None来清除控件的依赖关系。</target>
        </trans-unit>
        <trans-unit id="215f1df4bc1261bbb6f65d825790eea599447ab5" translate="yes" xml:space="preserve">
          <source>You can pass any of the returned values to &lt;code&gt;restore()&lt;/code&gt;.</source>
          <target state="translated">您可以将任何返回值传递给 &lt;code&gt;restore()&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ac470d84ff347998661ececab5ea81ac9cc6a909" translate="yes" xml:space="preserve">
          <source>You can pass the result of evaluating any summary op, using &lt;code&gt;tf.Session.run&lt;/code&gt; or &lt;a href=&quot;../../../tensor#eval&quot;&gt;&lt;code&gt;tf.Tensor.eval&lt;/code&gt;&lt;/a&gt;, to this function. Alternatively, you can pass a &lt;a href=&quot;../summary&quot;&gt;&lt;code&gt;tf.compat.v1.Summary&lt;/code&gt;&lt;/a&gt; protocol buffer that you populate with your own data. The latter is commonly done to report evaluation results in event files.</source>
          <target state="translated">您可以使用 &lt;code&gt;tf.Session.run&lt;/code&gt; 或&lt;a href=&quot;../../../tensor#eval&quot;&gt; &lt;code&gt;tf.Tensor.eval&lt;/code&gt; &lt;/a&gt;将评估任何摘要操作的结果传递给此函数。或者，您可以传递使用自己的数据填充的&lt;a href=&quot;../summary&quot;&gt; &lt;code&gt;tf.compat.v1.Summary&lt;/code&gt; &lt;/a&gt;协议缓冲区。后者通常用于在事件文件中报告评估结果。</target>
        </trans-unit>
        <trans-unit id="69155a67a44012b4f51b2dc8d0ccd534aa7907a1" translate="yes" xml:space="preserve">
          <source>You can pass this schedule directly into a &lt;a href=&quot;../optimizer&quot;&gt;&lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt;&lt;/a&gt; as the learning rate. Example: Fit a Keras model when decaying 1/t with a rate of 0.5:</source>
          <target state="translated">您可以将此计划直接作为学习率传递到&lt;a href=&quot;../optimizer&quot;&gt; &lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt; &lt;/a&gt;中。示例：当以0.5的速率衰减1 / t时，拟合Keras模型：</target>
        </trans-unit>
        <trans-unit id="48f5a8f4038134ce54531b0e709a77dbf5681703" translate="yes" xml:space="preserve">
          <source>You can pass this schedule directly into a &lt;a href=&quot;../optimizer&quot;&gt;&lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt;&lt;/a&gt; as the learning rate. Example: Fit a model while decaying from 0.1 to 0.01 in 10000 steps using sqrt (i.e. power=0.5):</source>
          <target state="translated">您可以将此计划直接作为学习率传递到&lt;a href=&quot;../optimizer&quot;&gt; &lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt; &lt;/a&gt;中。示例：使用sqrt（即幂= 0.5）以10000步从0.1衰减到0.01时拟合模型：</target>
        </trans-unit>
        <trans-unit id="40b491f9b541bdf48952295bf5d5a0ca64ae94b3" translate="yes" xml:space="preserve">
          <source>You can pass this schedule directly into a &lt;a href=&quot;../optimizer&quot;&gt;&lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt;&lt;/a&gt; as the learning rate. Example: When fitting a Keras model, decay every 100000 steps with a base of 0.96:</source>
          <target state="translated">您可以将此计划直接作为学习率传递到&lt;a href=&quot;../optimizer&quot;&gt; &lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt; &lt;/a&gt;中。示例：拟合Keras模型时，每100000步以0.96为底进行衰减：</target>
        </trans-unit>
        <trans-unit id="573311646e6161dd40916f132e58fdf271d90b7d" translate="yes" xml:space="preserve">
          <source>You can pass this schedule directly into a &lt;a href=&quot;../optimizer&quot;&gt;&lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt;&lt;/a&gt; as the learning rate. The learning rate schedule is also serializable and deserializable using &lt;a href=&quot;serialize&quot;&gt;&lt;code&gt;tf.keras.optimizers.schedules.serialize&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;deserialize&quot;&gt;&lt;code&gt;tf.keras.optimizers.schedules.deserialize&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">您可以将此计划直接作为学习率传递到&lt;a href=&quot;../optimizer&quot;&gt; &lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt; &lt;/a&gt;中。学习率进度表也可以使用&lt;a href=&quot;serialize&quot;&gt; &lt;code&gt;tf.keras.optimizers.schedules.serialize&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;deserialize&quot;&gt; &lt;code&gt;tf.keras.optimizers.schedules.deserialize&lt;/code&gt; 进行&lt;/a&gt;序列化和反序列化。</target>
        </trans-unit>
        <trans-unit id="5458dcea309d742dc1ae87c297fbd433242a68c9" translate="yes" xml:space="preserve">
          <source>You can pass this schedule directly into a &lt;a href=&quot;../optimizers/optimizer&quot;&gt;&lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt;&lt;/a&gt; as the learning rate. The learning rate schedule is also serializable and deserializable using &lt;a href=&quot;../optimizers/schedules/serialize&quot;&gt;&lt;code&gt;tf.keras.optimizers.schedules.serialize&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../optimizers/schedules/deserialize&quot;&gt;&lt;code&gt;tf.keras.optimizers.schedules.deserialize&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">您可以将此计划直接作为学习率传递到&lt;a href=&quot;../optimizers/optimizer&quot;&gt; &lt;code&gt;tf.keras.optimizers.Optimizer&lt;/code&gt; &lt;/a&gt;中。学习率进度表也可以使用&lt;a href=&quot;../optimizers/schedules/serialize&quot;&gt; &lt;code&gt;tf.keras.optimizers.schedules.serialize&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../optimizers/schedules/deserialize&quot;&gt; &lt;code&gt;tf.keras.optimizers.schedules.deserialize&lt;/code&gt; 进行&lt;/a&gt;序列化和反序列化。</target>
        </trans-unit>
        <trans-unit id="154d72e5d23674ce441818a8ed845bb4df350073" translate="yes" xml:space="preserve">
          <source>You can provide logits of classes as &lt;code&gt;y_pred&lt;/code&gt;, since argmax of logits and probabilities are same.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0779a4d81e795874a050ef63a764491cc356c1c7" translate="yes" xml:space="preserve">
          <source>You can return from this call a &lt;code&gt;SessionRunArgs&lt;/code&gt; object indicating ops or tensors to add to the upcoming &lt;code&gt;run()&lt;/code&gt; call. These ops/tensors will be run together with the ops/tensors originally passed to the original run() call. The run args you return can also contain feeds to be added to the run() call.</source>
          <target state="translated">您可以从此调用返回一个 &lt;code&gt;SessionRunArgs&lt;/code&gt; 对象，该对象指示要添加到即将到来的 &lt;code&gt;run()&lt;/code&gt; 调用中的操作或张量。这些操作/张量将与最初传递给原始run（）调用的操作/张量一起运行。您返回的运行参数还可以包含要添加到run（）调用中的提要。</target>
        </trans-unit>
        <trans-unit id="9ae0a59fa6ec6a5af526342b6817b045dd0db4e8" translate="yes" xml:space="preserve">
          <source>You can set the distribution options of a dataset through the &lt;code&gt;experimental_distribute&lt;/code&gt; property of &lt;a href=&quot;../options&quot;&gt;&lt;code&gt;tf.data.Options&lt;/code&gt;&lt;/a&gt;; the property is an instance of &lt;a href=&quot;distributeoptions&quot;&gt;&lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">您可以通过&lt;a href=&quot;../options&quot;&gt; &lt;code&gt;tf.data.Options&lt;/code&gt; &lt;/a&gt;的 &lt;code&gt;experimental_distribute&lt;/code&gt; 属性设置数据集的分布选项；该属性是&lt;a href=&quot;distributeoptions&quot;&gt; &lt;code&gt;tf.data.experimental.DistributeOptions&lt;/code&gt; &lt;/a&gt;的实例。</target>
        </trans-unit>
        <trans-unit id="38287133e2e375cb065b8aff1223591e0429addd" translate="yes" xml:space="preserve">
          <source>You can set the optimization options of a dataset through the &lt;code&gt;experimental_optimization&lt;/code&gt; property of &lt;a href=&quot;../options&quot;&gt;&lt;code&gt;tf.data.Options&lt;/code&gt;&lt;/a&gt;; the property is an instance of &lt;a href=&quot;optimizationoptions&quot;&gt;&lt;code&gt;tf.data.experimental.OptimizationOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">您可以通过&lt;a href=&quot;../options&quot;&gt; &lt;code&gt;tf.data.Options&lt;/code&gt; &lt;/a&gt;的 &lt;code&gt;experimental_optimization&lt;/code&gt; 属性设置数据集的优化选项；该属性是&lt;a href=&quot;optimizationoptions&quot;&gt; &lt;code&gt;tf.data.experimental.OptimizationOptions&lt;/code&gt; &lt;/a&gt;的实例。</target>
        </trans-unit>
        <trans-unit id="a4942db36ae427216d9ed0a514b6b4124827857d" translate="yes" xml:space="preserve">
          <source>You can set the stats options of a dataset through the &lt;code&gt;experimental_stats&lt;/code&gt; property of &lt;a href=&quot;../options&quot;&gt;&lt;code&gt;tf.data.Options&lt;/code&gt;&lt;/a&gt;; the property is an instance of &lt;a href=&quot;statsoptions&quot;&gt;&lt;code&gt;tf.data.experimental.StatsOptions&lt;/code&gt;&lt;/a&gt;. For example, to collect latency stats on all dataset edges, use the following pattern:</source>
          <target state="translated">您可以通过&lt;a href=&quot;../options&quot;&gt; &lt;code&gt;tf.data.Options&lt;/code&gt; &lt;/a&gt;的 &lt;code&gt;experimental_stats&lt;/code&gt; 属性设置数据集的stats选项；该属性是&lt;a href=&quot;statsoptions&quot;&gt; &lt;code&gt;tf.data.experimental.StatsOptions&lt;/code&gt; &lt;/a&gt;的实例。例如，要收集所有数据集边上的延迟统计信息，请使用以下模式：</target>
        </trans-unit>
        <trans-unit id="0286d405fc4b694404c86890a3b0233533e44d1a" translate="yes" xml:space="preserve">
          <source>You can set the threading options of a dataset through the &lt;code&gt;experimental_threading&lt;/code&gt; property of &lt;a href=&quot;../options&quot;&gt;&lt;code&gt;tf.data.Options&lt;/code&gt;&lt;/a&gt;; the property is an instance of &lt;a href=&quot;threadingoptions&quot;&gt;&lt;code&gt;tf.data.experimental.ThreadingOptions&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">您可以通过&lt;a href=&quot;../options&quot;&gt; &lt;code&gt;tf.data.Options&lt;/code&gt; &lt;/a&gt;的 &lt;code&gt;experimental_threading&lt;/code&gt; 属性设置数据集的线程选项；该属性是&lt;a href=&quot;threadingoptions&quot;&gt; &lt;code&gt;tf.data.experimental.ThreadingOptions&lt;/code&gt; &lt;/a&gt;的实例。</target>
        </trans-unit>
        <trans-unit id="5821880102274e82922a0313317c1a0e00262ef7" translate="yes" xml:space="preserve">
          <source>You can specify the initial state of RNN layers numerically by calling &lt;code&gt;reset_states&lt;/code&gt; with the keyword argument &lt;code&gt;states&lt;/code&gt;. The value of &lt;code&gt;states&lt;/code&gt; should be a numpy array or list of numpy arrays representing the initial state of the RNN layer.</source>
          <target state="translated">您可以通过使用关键字参数 &lt;code&gt;states&lt;/code&gt; 调用 &lt;code&gt;reset_states&lt;/code&gt; 来以数字方式指定RNN层的初始状态。 &lt;code&gt;states&lt;/code&gt; 的值应该是代表RNN层初始状态的numpy数组或numpy数组的列表。</target>
        </trans-unit>
        <trans-unit id="7903e8eb7e3c0e1f135ecaca1aabba76f1186224" translate="yes" xml:space="preserve">
          <source>You can then use &lt;code&gt;TimeDistributed&lt;/code&gt; to apply a &lt;code&gt;Conv2D&lt;/code&gt; layer to each of the 10 timesteps, independently:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0934b024d096c8c6f1a29dcdebc76afd185fc96d" translate="yes" xml:space="preserve">
          <source>You can then use &lt;code&gt;TimeDistributed&lt;/code&gt; to apply a &lt;code&gt;Dense&lt;/code&gt; layer to each of the 10 timesteps, independently:</source>
          <target state="translated">然后，您可以使用 &lt;code&gt;TimeDistributed&lt;/code&gt; 将 &lt;code&gt;Dense&lt;/code&gt; 图层分别应用于10个时间步长中的每个时间步长：</target>
        </trans-unit>
        <trans-unit id="49b9bca1daa199f82797585539058cf0bff378d8" translate="yes" xml:space="preserve">
          <source>You can use &lt;a href=&quot;get_replica_context&quot;&gt;&lt;code&gt;tf.distribute.get_replica_context&lt;/code&gt;&lt;/a&gt; to get an instance of &lt;code&gt;ReplicaContext&lt;/code&gt;. This should be inside your replicated step function, such as in a &lt;a href=&quot;strategy#experimental_run_v2&quot;&gt;&lt;code&gt;tf.distribute.Strategy.experimental_run_v2&lt;/code&gt;&lt;/a&gt; call.</source>
          <target state="translated">您可以使用&lt;a href=&quot;get_replica_context&quot;&gt; &lt;code&gt;tf.distribute.get_replica_context&lt;/code&gt; &lt;/a&gt;获得的实例 &lt;code&gt;ReplicaContext&lt;/code&gt; 。这应该在您复制的步骤函数中，例如在&lt;a href=&quot;strategy#experimental_run_v2&quot;&gt; &lt;code&gt;tf.distribute.Strategy.experimental_run_v2&lt;/code&gt; &lt;/a&gt;调用中。</target>
        </trans-unit>
        <trans-unit id="f52e741d1756d64c06640ac0efef1ab8aa73c071" translate="yes" xml:space="preserve">
          <source>You can use &lt;a href=&quot;get_replica_context&quot;&gt;&lt;code&gt;tf.distribute.get_replica_context&lt;/code&gt;&lt;/a&gt; to get an instance of &lt;code&gt;ReplicaContext&lt;/code&gt;. This should be inside your replicated step function, such as in a &lt;a href=&quot;strategy#run&quot;&gt;&lt;code&gt;tf.distribute.Strategy.run&lt;/code&gt;&lt;/a&gt; call.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bdb676d9df499211a1b26621113c16cf595d42c7" translate="yes" xml:space="preserve">
          <source>You can use the &lt;code&gt;reduce&lt;/code&gt; API to aggregate results across replicas and use this as a return value from one iteration over a &lt;a href=&quot;distributeddataset&quot;&gt;&lt;code&gt;tf.distribute.DistributedDataset&lt;/code&gt;&lt;/a&gt;. Or you can use &lt;a href=&quot;../keras/metrics&quot;&gt;&lt;code&gt;tf.keras.metrics&lt;/code&gt;&lt;/a&gt; (such as loss, accuracy, etc.) to accumulate metrics across steps in a given epoch.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cbd80087e0ae69c397a70f9c2b3b4a4570f57c0a" translate="yes" xml:space="preserve">
          <source>You can use the &lt;code&gt;reduce&lt;/code&gt; API to aggregate results across replicas and use this as a return value from one iteration over the distributed dataset. Or you can use &lt;a href=&quot;../keras/metrics&quot;&gt;&lt;code&gt;tf.keras.metrics&lt;/code&gt;&lt;/a&gt; (such as loss, accuracy, etc.) to accumulate metrics across steps in a given epoch.</source>
          <target state="translated">您可以使用 &lt;code&gt;reduce&lt;/code&gt; API汇总跨副本的结果，并将其用作分布式数据集上一次迭代的返回值。或者，您可以使用&lt;a href=&quot;../keras/metrics&quot;&gt; &lt;code&gt;tf.keras.metrics&lt;/code&gt; &lt;/a&gt;（例如损失，准确性等）在给定时期跨步骤累积指标。</target>
        </trans-unit>
        <trans-unit id="849e0f628a638e3510c8fe215b32ce36d057b4e1" translate="yes" xml:space="preserve">
          <source>You can use the Dense layer as you would expect:</source>
          <target state="translated">你可以像你所期望的那样使用Dense层。</target>
        </trans-unit>
        <trans-unit id="ffad7cddd33e66d73417586d2382f64196240d65" translate="yes" xml:space="preserve">
          <source>You can use this function to read events written to an event file. It returns a Python iterator that yields &lt;code&gt;Event&lt;/code&gt; protocol buffers.</source>
          <target state="translated">您可以使用此功能读取写入事件文件的事件。它返回一个Python迭代器，该迭代器产生 &lt;code&gt;Event&lt;/code&gt; 协议缓冲区。</target>
        </trans-unit>
        <trans-unit id="8746f4908e5bf18e74cecb0263457015fb63a862" translate="yes" xml:space="preserve">
          <source>You could also use vocabulary lookup before crossing:</source>
          <target state="translated">你也可以在穿越前使用词汇查询。</target>
        </trans-unit>
        <trans-unit id="de365a46ec65ae586c7c15b2194ec9eca625cd28" translate="yes" xml:space="preserve">
          <source>You could simply do:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2314a9b2a621f208986f86ed4dc02fe23da32fe1" translate="yes" xml:space="preserve">
          <source>You may override this method in a subclass. The standard run() method invokes the callable object passed to the object's constructor as the target argument, if any, with sequential and keyword arguments taken from the args and kwargs arguments, respectively.</source>
          <target state="translated">你可以在子类中重写这个方法。标准的run()方法调用作为目标参数传递给对象的构造函数的可调用对象(如果有的话),顺序参数和关键字参数分别取自args和kwargs参数。</target>
        </trans-unit>
        <trans-unit id="a9f39d075c01a25e9820f6a67df1a4ab21c825d3" translate="yes" xml:space="preserve">
          <source>You may pass descendant of &lt;a href=&quot;strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt; to &lt;a href=&quot;../estimator/runconfig&quot;&gt;&lt;code&gt;tf.estimator.RunConfig&lt;/code&gt;&lt;/a&gt; to specify how a &lt;a href=&quot;../estimator/estimator&quot;&gt;&lt;code&gt;tf.estimator.Estimator&lt;/code&gt;&lt;/a&gt; should distribute its computation. See &lt;a href=&quot;https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_estimator_limited_support&quot;&gt;guide&lt;/a&gt;.</source>
          <target state="translated">您可以将&lt;a href=&quot;strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; 的&lt;/a&gt;后代传递给&lt;a href=&quot;../estimator/runconfig&quot;&gt; &lt;code&gt;tf.estimator.RunConfig&lt;/code&gt; ,&lt;/a&gt;以指定&lt;a href=&quot;../estimator/estimator&quot;&gt; &lt;code&gt;tf.estimator.Estimator&lt;/code&gt; &lt;/a&gt;应如何分配其计算。请参阅&lt;a href=&quot;https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_estimator_limited_support&quot;&gt;指南&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4caa6c70be3989fa593907ab37426f9ba5e5385e" translate="yes" xml:space="preserve">
          <source>You may provide either a constant &lt;code&gt;window_size&lt;/code&gt; or a window size determined by the key through &lt;code&gt;window_size_func&lt;/code&gt;.</source>
          <target state="translated">你可以提供任一恒定 &lt;code&gt;window_size&lt;/code&gt; 或者通过键所确定的窗口大小 &lt;code&gt;window_size_func&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="246729df98351fdf262d7a7f6c3229e8ff176c5b" translate="yes" xml:space="preserve">
          <source>You must have set the task_type and task_id object properties before calling this function, or pass in the &lt;code&gt;task_type&lt;/code&gt; and &lt;code&gt;task_id&lt;/code&gt; parameters when using this function. If you do both, the function parameters will override the object properties.</source>
          <target state="translated">您必须先设置task_type和task_id对象属性，然后才能调用此函数，或者在使用此函数时传入 &lt;code&gt;task_type&lt;/code&gt; 和 &lt;code&gt;task_id&lt;/code&gt; 参数。如果两者都执行，则函数参数将覆盖对象属性。</target>
        </trans-unit>
        <trans-unit id="843e6067b5e147efe5ecbea6ee6b24b54cc1cca5" translate="yes" xml:space="preserve">
          <source>You number checkpoint filenames by passing a value to the optional &lt;code&gt;global_step&lt;/code&gt; argument to &lt;code&gt;save()&lt;/code&gt;:</source>
          <target state="translated">您可以通过将值传递给 &lt;code&gt;save()&lt;/code&gt; 的可选 &lt;code&gt;global_step&lt;/code&gt; 参数来为检查点文件名编号：</target>
        </trans-unit>
        <trans-unit id="47945af2dd44ab9b9f3ac220d707a317fa8bb12b" translate="yes" xml:space="preserve">
          <source>You should not use this class directly, but instead instantiate one of its subclasses such as &lt;a href=&quot;sgd&quot;&gt;&lt;code&gt;tf.keras.optimizers.SGD&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;adam&quot;&gt;&lt;code&gt;tf.keras.optimizers.Adam&lt;/code&gt;&lt;/a&gt;, etc.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8d943ec0774ebc46f132a6e012d0ef8ae2735a29" translate="yes" xml:space="preserve">
          <source>You should use this instead of the variable itself to initialize another variable with a value that depends on the value of this variable.</source>
          <target state="translated">你应该用它来代替变量本身来初始化另一个变量,其值取决于这个变量的值。</target>
        </trans-unit>
        <trans-unit id="5bcb3cc3036b6c1787fd4b43072cd172e65fd9dd" translate="yes" xml:space="preserve">
          <source>You typically pass looper threads to the supervisor &lt;code&gt;Join()&lt;/code&gt; method.</source>
          <target state="translated">通常，您需要将弯针线程传递给主管 &lt;code&gt;Join()&lt;/code&gt; 方法。</target>
        </trans-unit>
        <trans-unit id="866c1e20e145360ff1c2d3fac51cd421f0959538" translate="yes" xml:space="preserve">
          <source>You usually do not need to call this method as all ops that need the value of the variable call it automatically through a &lt;code&gt;convert_to_tensor()&lt;/code&gt; call.</source>
          <target state="translated">通常，您不需要调用此方法，因为所有需要变量值的操作都会通过 &lt;code&gt;convert_to_tensor()&lt;/code&gt; 调用自动调用该方法。</target>
        </trans-unit>
        <trans-unit id="28b1f658f43504f27e0b8dad06c688c5569a5b43" translate="yes" xml:space="preserve">
          <source>You want os.path.exists() to always return true during testing.</source>
          <target state="translated">你希望 os.path.exces()在测试过程中总是返回 true。</target>
        </trans-unit>
        <trans-unit id="558865a16feb9f751b8bcebf46a954afbeba0b24" translate="yes" xml:space="preserve">
          <source>YouTube</source>
          <target state="translated">YouTube</target>
        </trans-unit>
        <trans-unit id="743b60db1fe7f2ede7836bb4e7bd6e58b1ef9754" translate="yes" xml:space="preserve">
          <source>Zeiler, 2012</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8cf5dacc30bc54f166bd31cec0b947c674cc3e7e" translate="yes" xml:space="preserve">
          <source>Zero or more tensors to group.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c152968210a9e92278d1a3f141b891e56657c41" translate="yes" xml:space="preserve">
          <source>Zero-pad the start and end of dimensions &lt;code&gt;[1, ..., M]&lt;/code&gt; of the input according to &lt;code&gt;paddings&lt;/code&gt; to produce &lt;code&gt;padded&lt;/code&gt; of shape &lt;code&gt;padded_shape&lt;/code&gt;.</source>
          <target state="translated">根据 &lt;code&gt;paddings&lt;/code&gt; 对输入的尺寸 &lt;code&gt;[1, ..., M]&lt;/code&gt; 的开始和结束进行零填充，以生成形状为 &lt;code&gt;padded_shape&lt;/code&gt; 的 &lt;code&gt;padded&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ec7ae133fd762cea93146fc3c7001608432a59c7" translate="yes" xml:space="preserve">
          <source>Zero-padding layer for 1D input (e.g. temporal sequence).</source>
          <target state="translated">1D输入的零填充层(如时间序列)。</target>
        </trans-unit>
        <trans-unit id="9b78ee89fd119fbc58af59c013afefa4ada9d7ed" translate="yes" xml:space="preserve">
          <source>Zero-padding layer for 2D input (e.g. picture).</source>
          <target state="translated">二维输入(如图片)的零填充层。</target>
        </trans-unit>
        <trans-unit id="81f93ad811573d34a01e1dda72548f7bcac0984e" translate="yes" xml:space="preserve">
          <source>Zero-padding layer for 3D data (spatial or spatio-temporal).</source>
          <target state="translated">3D数据(空间或时空)的零填充层。</target>
        </trans-unit>
        <trans-unit id="4650f7edf1724b78bd849b1c2cb32632cbf26f09" translate="yes" xml:space="preserve">
          <source>Zero-pads and then rearranges (permutes) blocks of spatial data into batch. More specifically, this op outputs a copy of the input tensor where values from the &lt;code&gt;height&lt;/code&gt; and &lt;code&gt;width&lt;/code&gt; dimensions are moved to the &lt;code&gt;batch&lt;/code&gt; dimension. After the zero-padding, both &lt;code&gt;height&lt;/code&gt; and &lt;code&gt;width&lt;/code&gt; of the input must be divisible by the block size.</source>
          <target state="translated">零填充，然后重新排列（置换）空间数据块成批。更具体地说，此op输出输入张量的副本，其中 &lt;code&gt;height&lt;/code&gt; 和 &lt;code&gt;width&lt;/code&gt; 尺寸的值将移动到 &lt;code&gt;batch&lt;/code&gt; 尺寸。零填充后，输入的 &lt;code&gt;height&lt;/code&gt; 和 &lt;code&gt;width&lt;/code&gt; 都必须可被块大小整除。</target>
        </trans-unit>
        <trans-unit id="dfee31bddce3aa2ae0eb9ab0a81354d098ee985f" translate="yes" xml:space="preserve">
          <source>ZerosLike</source>
          <target state="translated">ZerosLike</target>
        </trans-unit>
        <trans-unit id="d4dde75ca731d6afffc873406bc9b30fd639401e" translate="yes" xml:space="preserve">
          <source>Zeta</source>
          <target state="translated">Zeta</target>
        </trans-unit>
        <trans-unit id="37a40c343b1c25e2aa4647448f2479d812035f3e" translate="yes" xml:space="preserve">
          <source>ZipDataset</source>
          <target state="translated">ZipDataset</target>
        </trans-unit>
        <trans-unit id="cbfaea632aa3eb5b0ee3bf0fe5b02a033aca96d1" translate="yes" xml:space="preserve">
          <source>Zone of the GCE instance group.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b808978f0a72336077098f027b8f727415da270" translate="yes" xml:space="preserve">
          <source>Zone where the TPUs are located. If omitted or empty, we will assume that the zone of the TPU is the same as the zone of the GCE VM, which we will try to discover from the GCE metadata service.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="690066f3139e111a254b9b22702420163ce9e6c3" translate="yes" xml:space="preserve">
          <source>[-128, 127] for signed, num_bits = 8, or</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f0f7b84b517d1011bde2787a1af440ee2991478" translate="yes" xml:space="preserve">
          <source>[0, 255] for unsigned, num_bits = 8.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd22950e542e583caceeef78dc9ae8586fc6a2c9" translate="yes" xml:space="preserve">
          <source>[1] Nicholas J. Higham (2002). Accuracy and Stability of Numerical Algorithms: Second Edition. SIAM. p. 175. ISBN 978-0-89871-802-7.</source>
          <target state="translated">[1] Nicholas J. Higham（2002）。数值算法的准确性和稳定性：第二版。暹。p。175.ISBN 978-0-89871-802-7。</target>
        </trans-unit>
        <trans-unit id="b99fc78b4d7545fcd138b54f0e14096e6f4551ca" translate="yes" xml:space="preserve">
          <source>[1] http://en.wikipedia.org/wiki/Gamma_correction</source>
          <target state="translated">[1] http://en.wikipedia.org/wiki/Gamma_correction</target>
        </trans-unit>
        <trans-unit id="8b402dbbbcbda420a8c8a62323dde3dbd63d7a5e" translate="yes" xml:space="preserve">
          <source>[1]: G. Strang. 'Linear Algebra and Its Applications, 2nd Ed.' Academic Press, Inc., 1980, pp. 139-142.</source>
          <target state="translated">[1]：G。Strang。&amp;ldquo;线性代数及其应用，第二版。&amp;rdquo; Academic Press，Inc.，1980，第139-142页。</target>
        </trans-unit>
        <trans-unit id="3431dfde47f5f77802dc8c1e589fe801ed9dd223" translate="yes" xml:space="preserve">
          <source>[Flag], a new list of Flag instances. Caller may update this list as</source>
          <target state="translated">[Flag]，标志实例的新列表。呼叫者可以将此列表更新为</target>
        </trans-unit>
        <trans-unit id="8a492c08a6fb4ab5ad781dcaefb3c630980c2a92" translate="yes" xml:space="preserve">
          <source>[Optional] Dict of variable names (strings) to &lt;a href=&quot;../../../estimator/vocabinfo&quot;&gt;&lt;code&gt;tf.estimator.VocabInfo&lt;/code&gt;&lt;/a&gt;. The variable names should be &quot;full&quot; variables, not the names of the partitions. If not explicitly provided, the variable is assumed to have no (changes to) vocabulary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="040ab6c4e7ceb839df30c29d661e2122a9ea0ad3" translate="yes" xml:space="preserve">
          <source>[Optional] Dict of variable names (strings) to &lt;a href=&quot;vocabinfo&quot;&gt;&lt;code&gt;tf.estimator.VocabInfo&lt;/code&gt;&lt;/a&gt;. The variable names should be &quot;full&quot; variables, not the names of the partitions. If not explicitly provided, the variable is assumed to have no (changes to) vocabulary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ccfc72b7041b89963d68b36a1571281cdd9debc" translate="yes" xml:space="preserve">
          <source>[Optional] Dict of variable names (strings) to name of the previously-trained variable in &lt;code&gt;ckpt_to_initialize_from&lt;/code&gt;. If not explicitly provided, the name of the variable is assumed to be same between previous checkpoint and current model. Note that this has no effect on the set of variables that is warm-started, and only controls name mapping (use &lt;code&gt;vars_to_warm_start&lt;/code&gt; for controlling what variables to warm-start).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88dac723b45d7ec9ed5d2521d06d37e6754801f3" translate="yes" xml:space="preserve">
          <source>[Optional] One of the following:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56fcc4459de1190d856f1a05d4fa018ace19dfbd" translate="yes" xml:space="preserve">
          <source>[Required] A string specifying the directory with checkpoint file(s) or path to checkpoint from which to warm-start the model parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8ee2e7d87ae9f9bdabd7b4bb725a904d7d206cb7" translate="yes" xml:space="preserve">
          <source>[[w(1, 0), w(1, 2), 0.5], [w(0, 0), w(0, 2), -0.5], [0.25, -0.25, 42]]</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7fbe9b43ff0aeb17fee44c25d05877c8cdce9b59" translate="yes" xml:space="preserve">
          <source>[batch * prod(block_shape)] + [padded_shape[1] / block_shape[0], ..., padded_shape[M] / block_shape[M-1]] + remaining_shape</source>
          <target state="translated">[批处理*产品（block_shape）] + [papped_shape [1] / block_shape [0]，...，padded_shape [M] / block_shape [M-1]] +剩余形状</target>
        </trans-unit>
        <trans-unit id="cbf73d5213642961c32b5b788775c8bd0559cdc7" translate="yes" xml:space="preserve">
          <source>[batch, height - 2 * (filter_width - 1), width - 2 * (filter_height - 1), out_channels].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4f44d9fad75faea26dd9cc4bc783f18b2b9a3245" translate="yes" xml:space="preserve">
          <source>[batch, height, width, out_channels].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="69883aca3ab9b1512329fe1f31a31a2297068297" translate="yes" xml:space="preserve">
          <source>[batch&lt;em&gt;block_size&lt;/em&gt;block_size, height_pad/block_size, width_pad/block_size, depth]</source>
          <target state="translated">[批次&lt;em&gt;BLOCK_SIZE&lt;/em&gt; BLOCK_SIZE，height_pad / BLOCK_SIZE，width_pad / BLOCK_SIZE，深度]</target>
        </trans-unit>
        <trans-unit id="a99f9eae6edb2aeb1e4c6a1434cfbcae41146183" translate="yes" xml:space="preserve">
          <source>[batch] + [padded_shape[1] / block_shape[0], block_shape[0], ..., padded_shape[M] / block_shape[M-1], block_shape[M-1]] + remaining_shape</source>
          <target state="translated">[批量] + [填充形状[1] /块形状[0]，块形状[0]，...，填充形状[M] /块形状[M-1]，块形状[M-1]] +剩余形状</target>
        </trans-unit>
        <trans-unit id="a018842c23398495c71954bf778c4b46f8056996" translate="yes" xml:space="preserve">
          <source>[batch_size, num_channels] + output_spatial_shape</source>
          <target state="translated">[batch_size，num_channels] + output_spatial_shape</target>
        </trans-unit>
        <trans-unit id="51a4c96f9f2ac53566fd73823e05a471562bcdfc" translate="yes" xml:space="preserve">
          <source>[filename1, filename2, ... filenameN] as strings</source>
          <target state="translated">[filename1，filename2，... filenameN]作为字符串</target>
        </trans-unit>
        <trans-unit id="d8a02b4024d1f91043f88e51a21c86cf6a2b1f8b" translate="yes" xml:space="preserve">
          <source>[input_min, input_max] are scalar floats that specify the range for the float interpretation of the 'input' data. For example, if input_min is -1.0f and input_max is 1.0f, and we are dealing with quint16 quantized data, then a 0 value in the 16-bit data should be interpreted as -1.0f, and a 65535 means 1.0f.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="143b12af78e15f28dfa0f3a68b2560666f6d5995" translate="yes" xml:space="preserve">
          <source>[min_range, max_range] are scalar floats that specify the range for the 'input' data. The 'mode' attribute controls exactly which calculations are used to convert the float values to their quantized equivalents. The 'round_mode' attribute controls which rounding tie-breaking algorithm is used when rounding float values to their quantized equivalents.</source>
          <target state="translated">[min_range，max_range]是标量浮点数，用于指定&amp;ldquo;输入&amp;rdquo;数据的范围。&amp;ldquo;模式&amp;rdquo;属性精确控制要使用哪些计算将浮点值转换为其量化的等效值。&amp;ldquo; round_mode&amp;rdquo;属性控制在将float值四舍五入到其量化的等效项时使用哪种四舍五入的平局决胜算法。</target>
        </trans-unit>
        <trans-unit id="1fa2da75f60f4f3a22685794db19c7508c0c1baa" translate="yes" xml:space="preserve">
          <source>[min_range, max_range] are scalar floats that specify the range for the output. The 'mode' attribute controls exactly which calculations are used to convert the float values to their quantized equivalents.</source>
          <target state="translated">[min_range，max_range]是标量浮点数，用于指定输出范围。&amp;ldquo;模式&amp;rdquo;属性精确控制要使用哪些计算将浮点值转换为其量化的等效值。</target>
        </trans-unit>
        <trans-unit id="8bebaab027f4bff5b0c289cd914950701e571abd" translate="yes" xml:space="preserve">
          <source>[num_batches, input_spatial_shape[0], ..., input_spatial_shape[N-1], num_input_channels],</source>
          <target state="translated">[num_batches，input_spatial_shape [0]，...，input_spatial_shape [N-1]，num_input_channels]，</target>
        </trans-unit>
        <trans-unit id="8f8157d4fe8eda67ef4b3a3b791fa8bb98678ccc" translate="yes" xml:space="preserve">
          <source>[spatial_filter_shape[0], ..., spatial_filter_shape[N-1], num_input_channels, num_output_channels],</source>
          <target state="translated">[spatial_filter_shape [0]，...，spatial_filter_shape [N-1]，num_input_channels，num_output_channels]，</target>
        </trans-unit>
        <trans-unit id="ccfaace27f3147695ce3d29304da376491a69186" translate="yes" xml:space="preserve">
          <source>[str], a list of strings, usually sys.argv[1:], which may contain one or more flagfile directives of the form --flagfile=&quot;./filename&quot;. Note that the name of the program (sys.argv[0]) should be omitted.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a05d1e5d9cec162aa920d1c81e7d248fa2814404" translate="yes" xml:space="preserve">
          <source>[str], a list of the flag names to be checked.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c26fe712edb3b507e5043f8b806fc26990923cd" translate="yes" xml:space="preserve">
          <source>[str], a non-empty list of string values in the enum.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3cd81efe3c17628b43be50a6bfd943ed33227190" translate="yes" xml:space="preserve">
          <source>[str], a non-empty list of strings with the possible values for the flag.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7dbf8f891a3cf1f1fcd98f054581759232646314" translate="yes" xml:space="preserve">
          <source>[str], names of the flags.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0305e4e8314312d7aab76df54a2e11f81f096064" translate="yes" xml:space="preserve">
          <source>[str], the parsed flag value.</source>
          <target state="translated">[str]，已解析的标志值。</target>
        </trans-unit>
        <trans-unit id="06a4ee1356819500657e1855f1838410080d3362" translate="yes" xml:space="preserve">
          <source>\( c_{jklm} = \sum_i a_{ijk} b_{lmi} \).</source>
          <target state="translated">\（c_ {jklm} = \ sum_i a_ {ijk} b_ {lmi} \）。</target>
        </trans-unit>
        <trans-unit id="7c88dd9089928bec50c06f1f5a9b847c1bc1189e" translate="yes" xml:space="preserve">
          <source>\(B(x; a, b) = \int_0^x t^{a-1} (1 - t)^{b-1} dt\)</source>
          <target state="translated">\（B（x; a，b）= \ int_0 ^ xt ^ {a-1}（1-t）^ {b-1} dt \）</target>
        </trans-unit>
        <trans-unit id="5dd8170c96a11e0fac54775aa5e03b07d872abb3" translate="yes" xml:space="preserve">
          <source>\(Gamma(a, x) = int_{x}^{\infty} t^{a-1} exp(-t) dt\)</source>
          <target state="translated">\（Gamma（a，x）= int_ {x} ^ {\ infty} t ^ {a-1} exp（-t）dt \）</target>
        </trans-unit>
        <trans-unit id="a4f68edbf74b4238178695327b403faa736fbba4" translate="yes" xml:space="preserve">
          <source>\(I_x(a, b) = \frac{B(x; a, b)}{B(a, b)}\)</source>
          <target state="translated">\（I_x（a，b）= \ frac {B（x; a，b）} {B（a，b）} \）</target>
        </trans-unit>
        <trans-unit id="9b977890dfd27609aefecb3c22cf9edc2b5b6c7c" translate="yes" xml:space="preserve">
          <source>\(P(a, x) = gamma(a, x) / Gamma(a) = 1 - Q(a, x)\)</source>
          <target state="translated">\（P（a，x）=伽玛（a，x）/伽玛（a）= 1-Q（a，x）\）</target>
        </trans-unit>
        <trans-unit id="3c43fa9908d595b3f6dccdce0d125247c097b9f4" translate="yes" xml:space="preserve">
          <source>\(Q(a, x) = Gamma(a, x) / Gamma(a) = 1 - P(a, x)\)</source>
          <target state="translated">\（Q（a，x）= Gamma（a，x）/ Gamma（a）= 1-P（a，x）\）</target>
        </trans-unit>
        <trans-unit id="37291deb0e6026c081430c96a800990fc3edcaa7" translate="yes" xml:space="preserve">
          <source>\(\beta\)</source>
          <target state="translated">\(\beta\)</target>
        </trans-unit>
        <trans-unit id="ff41b6103716ee7c998d6e81784bdf83320ba7f2" translate="yes" xml:space="preserve">
          <source>\(\ell_1\,\,penalty =\ell_1\sum_{i=0}^n|x_i|\)</source>
          <target state="translated">\（\ ell_1 \，\，penalty = \ ell_1 \ sum_ {i = 0} ^ n | x_i | \）</target>
        </trans-unit>
        <trans-unit id="9335deefcc4fff06a8f47ea88b57b74222c6ee2a" translate="yes" xml:space="preserve">
          <source>\(\ell_2\,\,penalty =\ell_2\sum_{i=0}^nx_i^2\)</source>
          <target state="translated">\（\ ell_2 \，\，惩罚= \ ell_2 \ sum_ {i = 0} ^ nx_i ^ 2 \）</target>
        </trans-unit>
        <trans-unit id="c62e7aed7aab85e0bc7679b7b1654447c10071b7" translate="yes" xml:space="preserve">
          <source>\(\frac{\gamma(x-\mu)}{\sigma}+\beta\)</source>
          <target state="translated">\(\frac{\gamma(x-\mu)}{\sigma}+\beta\)</target>
        </trans-unit>
        <trans-unit id="4a0660a89253bc5ba688c6d45badc9f8ddf6380e" translate="yes" xml:space="preserve">
          <source>\(\psi^{(a)}(x) = \frac{d^a}{dx^a} \psi(x)\)</source>
          <target state="translated">\（\ psi ^ {（a）}（x）= \ frac {d ^ a} {dx ^ a} \ psi（x）\）</target>
        </trans-unit>
        <trans-unit id="57cfcb3ce0118881ab066bb534ef79062b9ae613" translate="yes" xml:space="preserve">
          <source>\(\sigma_{t,i} = (\sqrt{n_{t,i}} - \sqrt{n_{t-1,i}}) / \alpha\)</source>
          <target state="translated">\（\ sigma_ {t，i} =（\ sqrt {n_ {t，i}}-\ sqrt {n_ {t-1，i}}}）/ \ alpha \）</target>
        </trans-unit>
        <trans-unit id="c402d7a692489cd97ce624829b28b4af2556f395" translate="yes" xml:space="preserve">
          <source>\(\zeta(x, q) = \sum_{n=0}^{\infty} (q + n)^{-x}\)</source>
          <target state="translated">\（\ zeta（x，q）= \ sum_ {n = 0} ^ {\ infty}（q + n）^ {-x} \）</target>
        </trans-unit>
        <trans-unit id="ea1119f555233a8d758e9686e809fbc51e48a520" translate="yes" xml:space="preserve">
          <source>\(gamma(a, x) = \\int_{0}^{x} t^{a-1} exp(-t) dt\)</source>
          <target state="translated">\（gamma（a，x）= \\ int_ {0} ^ {x} t ^ {a-1} exp（-t）dt \）</target>
        </trans-unit>
        <trans-unit id="553f6c65213b82afcc0c043d8233d0d411b11d70" translate="yes" xml:space="preserve">
          <source>\(i\)</source>
          <target state="translated">\(i\)</target>
        </trans-unit>
        <trans-unit id="cad97ed2e69a3cdad8082dd06f3ff6c16e2928db" translate="yes" xml:space="preserve">
          <source>\(lbeta(x)[i1, ..., in] = Log(|Beta(x[i1, ..., in, :])|)\)</source>
          <target state="translated">\（lbeta（x）[i1，...，in] = Log（| Beta（x [i1，...，in，：]）|）\）</target>
        </trans-unit>
        <trans-unit id="e122b351d32a233056a0dc92eef090ec82833970" translate="yes" xml:space="preserve">
          <source>\(log(exp(A)) = A\)</source>
          <target state="translated">\（log（exp（A））= A \）</target>
        </trans-unit>
        <trans-unit id="a6e6475c1d10a33b250fca653ee8cfd040c5e589" translate="yes" xml:space="preserve">
          <source>\(lr_t := \text{learning\_rate} * \sqrt{1 - beta_2^t} / (1 - beta_1^t)\)</source>
          <target state="translated">\（lr_t：= \ text {learning \ _rate} * \ sqrt {1-beta_2 ^ t} /（1-beta_1 ^ t）\）</target>
        </trans-unit>
        <trans-unit id="9f6d1fd07abb052ec6fac4e52945c2c9c481cefa" translate="yes" xml:space="preserve">
          <source>\(m_0 := 0 \text{(Initialize initial 1st moment vector)}\)</source>
          <target state="translated">\（m_0：= 0 \ text {（初始化初始的第一矩向量）} \）</target>
        </trans-unit>
        <trans-unit id="e3dada6e7ecd7d65376b81a16036f8e723616d85" translate="yes" xml:space="preserve">
          <source>\(m_t := beta_1 * m_{t-1} + (1 - beta_1) * g\)</source>
          <target state="translated">\（m_t：= beta_1 * m_ {t-1} +（1-beta_1）* g \）</target>
        </trans-unit>
        <trans-unit id="36cf9049b0822fc2658dd5393471d4a9bc14521f" translate="yes" xml:space="preserve">
          <source>\(n_{t,i} = n_{t-1,i} + g_{t,i}^{2}\)</source>
          <target state="translated">\（n_ {t，i} = n_ {t-1，i} + g_ {t，i} ^ {2} \）</target>
        </trans-unit>
        <trans-unit id="e58cad3c2a2e7d2a68a0cec41e11be1d3da882e6" translate="yes" xml:space="preserve">
          <source>\(output_i = 1/N_i \sum_{j...} data[j...]\) where the sum is over tuples &lt;code&gt;j...&lt;/code&gt; such that &lt;code&gt;segment_ids[j...] == i&lt;/code&gt; with \N_i\ being the number of occurrences of id \i\.</source>
          <target state="translated">\（output_i = 1 / N_i \ sum_ {j ...} data [j ...] \）其中总和在元组 &lt;code&gt;j...&lt;/code&gt; ，使得 &lt;code&gt;segment_ids[j...] == i&lt;/code&gt; 与\ N_i \是ID \ i \的出现次数。</target>
        </trans-unit>
        <trans-unit id="eff44ad7c5ba3196137c591dc7d87b8984229557" translate="yes" xml:space="preserve">
          <source>\(output_i = 1/sqrt(N_i) \sum_{j...} data[j...]\) where the sum is over tuples &lt;code&gt;j...&lt;/code&gt; such that &lt;code&gt;segment_ids[j...] == i&lt;/code&gt; with \N_i\ being the number of occurrences of id \i\.</source>
          <target state="translated">\（output_i = 1 / sqrt（N_i）\ sum_ {j ...} data [j ...] \）其中总和超过元组 &lt;code&gt;j...&lt;/code&gt; 使得 &lt;code&gt;segment_ids[j...] == i&lt;/code&gt; 与\ N_i \是ID \ i \的出现次数。</target>
        </trans-unit>
        <trans-unit id="fd27c9aa6e45dbe63e023f46fe5fdca3d93fbc19" translate="yes" xml:space="preserve">
          <source>\(output_i = \max_{j...} data[j...]\) where max is over tuples &lt;code&gt;j...&lt;/code&gt; such that &lt;code&gt;segment_ids[j...] == i&lt;/code&gt;.</source>
          <target state="translated">\（output_i = \ max_ {j ...} data [j ...] \）其中max在元组 &lt;code&gt;j...&lt;/code&gt; ，使得 &lt;code&gt;segment_ids[j...] == i&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="529b95e92276e5f1ac9b067c9474b5798a2da7c5" translate="yes" xml:space="preserve">
          <source>\(output_i = \min_{j...} data_[j...]\) where min is over tuples &lt;code&gt;j...&lt;/code&gt; such that &lt;code&gt;segment_ids[j...] == i&lt;/code&gt;.</source>
          <target state="translated">\（output_i = \ min_ {j ...} data_ [j ...] \）其中min位于元组 &lt;code&gt;j...&lt;/code&gt; 之上，从而 &lt;code&gt;segment_ids[j...] == i&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1bf71665be24061201465110b49af7b8c6429f1f" translate="yes" xml:space="preserve">
          <source>\(output_i = \prod_{j...} data[j...]\) where the product is over tuples &lt;code&gt;j...&lt;/code&gt; such that &lt;code&gt;segment_ids[j...] == i&lt;/code&gt;.</source>
          <target state="translated">\（output_i = \ prod_ {j ...} data [j ...] \）产品在元组 &lt;code&gt;j...&lt;/code&gt; ，使得 &lt;code&gt;segment_ids[j...] == i&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="bcf88899f36fe32859ba4eebac881718864b3a68" translate="yes" xml:space="preserve">
          <source>\(predictions_i\) be the predictions for all classes for example &lt;code&gt;i&lt;/code&gt;, \(targets_i\) be the target class for example &lt;code&gt;i&lt;/code&gt;, \(out_i\) be the output for example &lt;code&gt;i&lt;/code&gt;,</source>
          <target state="translated">\（predictions_i \）是所有类别的预测，例如 &lt;code&gt;i&lt;/code&gt; ，\（targets_i \）是目标类别，例如 &lt;code&gt;i&lt;/code&gt; ，\（out_i \）是例如 &lt;code&gt;i&lt;/code&gt; 的输出，</target>
        </trans-unit>
        <trans-unit id="eb6c03745499b6a9eb903031a775345057a8d436" translate="yes" xml:space="preserve">
          <source>\(t := 0 \text{(Initialize timestep)}\)</source>
          <target state="translated">\（t：= 0 \ text {（初始化时间步长）} \）</target>
        </trans-unit>
        <trans-unit id="467d92f4cfe1c3cd183f798d119891376eebfa6b" translate="yes" xml:space="preserve">
          <source>\(t := t + 1\)</source>
          <target state="translated">\（t：= t + 1 \）</target>
        </trans-unit>
        <trans-unit id="e5334cf7b7c8be8a554d2684840ca9468e2d3597" translate="yes" xml:space="preserve">
          <source>\(t = t + 1\)</source>
          <target state="translated">\（t = t + 1 \）</target>
        </trans-unit>
        <trans-unit id="1a4815b823d6a6bd996f5174a896a936100c82c4" translate="yes" xml:space="preserve">
          <source>\(v_0 := 0 \text{(Initialize initial 2nd moment vector)}\)</source>
          <target state="translated">\（v_0：= 0 \ text {（初始化初始第二矩向量）} \）</target>
        </trans-unit>
        <trans-unit id="fe2806cab7064b169c451375ea09662f2387835c" translate="yes" xml:space="preserve">
          <source>\(v_hat_0 := 0 \text{(Initialize initial 2nd moment vector)}\)</source>
          <target state="translated">\（v_hat_0：= 0 \ text {（初始化初始第二时刻矢量）} \）</target>
        </trans-unit>
        <trans-unit id="95246ed68fccf6f4caddd1734e3779b6b21f4b80" translate="yes" xml:space="preserve">
          <source>\(v_hat_t := max(v_hat_{t-1}, v_t)\)</source>
          <target state="translated">\（v_hat_t：= max（v_hat_ {t-1}，v_t）\）</target>
        </trans-unit>
        <trans-unit id="4f4d3c48642e5d2288a5d23ba51224f14bb68e11" translate="yes" xml:space="preserve">
          <source>\(v_t := beta_2 * v_{t-1} + (1 - beta_2) * g * g\)</source>
          <target state="translated">\（v_t：= beta_2 * v_ {t-1} +（1-beta_2）* g * g \）</target>
        </trans-unit>
        <trans-unit id="ceb29146344acce0db4068ea0c682c74bad08207" translate="yes" xml:space="preserve">
          <source>\(variable := variable - lr_t * m_t / (\sqrt{v_hat_t} + \epsilon)\)</source>
          <target state="translated">\（变量：=变量-lr_t * m_t /（\ sqrt {v_hat_t} + \ epsilon）\）</target>
        </trans-unit>
        <trans-unit id="2a5065284239af79ffad5fd4634aca805131acd5" translate="yes" xml:space="preserve">
          <source>\(variable := variable - lr_t * m_t / (\sqrt{v_t} + \epsilon)\)</source>
          <target state="translated">\（变量：=变量-lr_t * m_t /（\ sqrt {v_t} + \ epsilon）\）</target>
        </trans-unit>
        <trans-unit id="9c42d66b235e7446c048f4defb1b795d3ee38b09" translate="yes" xml:space="preserve">
          <source>\(w_{i}\)</source>
          <target state="translated">\(w_{i}\)</target>
        </trans-unit>
        <trans-unit id="3b25b3312f2ef7cacd25276352e73b5ab48846cf" translate="yes" xml:space="preserve">
          <source>\(w_{t,i} = - ((\beta+\sqrt{n+{t}}) / \alpha + \lambda_{2})^{-1} * (z_{i} - sgn(z_{i}) * \lambda_{1}) if \abs{z_{i}} &amp;gt; \lambda_{i} else 0\)</source>
          <target state="translated">\（w_ {t，i} =-（（\ beta + \ sqrt {n + {t}}）/ \ alpha + \ lambda_ {2}）^ {-1} *（z_ {i}-sgn（z_ {i }）* \ lambda_ {1}），如果\ abs {z_ {i}}&amp;gt; \ lambda_ {i}则为0 \）</target>
        </trans-unit>
        <trans-unit id="ba90f7c8012e09317d8c902b37294fee9c1c8163" translate="yes" xml:space="preserve">
          <source>\(y = \beta + \sum_{i=1}^{N} w_{i} * x_{i}\)</source>
          <target state="translated">\（y = \ beta + \ sum_ {i = 1} ^ {N} w_ {i} * x_ {i} \）</target>
        </trans-unit>
        <trans-unit id="2dec4db0fcb32e5c79e8ecb6c6414661d0289001" translate="yes" xml:space="preserve">
          <source>\(z_{t,i} = z_{t-1,i} + g_{t,i} - \sigma_{t,i} * w_{t,i}\)</source>
          <target state="translated">\（z_ {t，i} = z_ {t-1，i} + g_ {t，i}-\ sigma_ {t，i} * w_ {t，i} \）</target>
        </trans-unit>
        <trans-unit id="31d5b9df6c8b26532e6975198879e8066c930cd4" translate="yes" xml:space="preserve">
          <source>], 'bias': [</source>
          <target state="translated">]，'bias'：[</target>
        </trans-unit>
        <trans-unit id="ebb0f535eb870cf683880d2973390bc12e206de5" translate="yes" xml:space="preserve">
          <source>], _NumericColumn( key='numeric_feature2', shape=(2,)): [</source>
          <target state="translated">]，_ NumericColumn（key ='numeric_feature2'，shape =（2，））：[</target>
        </trans-unit>
        <trans-unit id="a961fbc1cc31eed81a5be94725a3b10dfcfb3f0e" translate="yes" xml:space="preserve">
          <source>]} If a column creates no variables, its value will be an empty list. Note that cols_to_vars will also contain a string key 'bias' that maps to a list of Variables.</source>
          <target state="translated">]}如果一列不创建任何变量，则其值将为空列表。请注意，cols_to_vars还将包含一个映射到变量列表的字符串键&amp;ldquo; bias&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="82253180c6e96af25f2a21fff7bcfa0218b5a1e9" translate="yes" xml:space="preserve">
          <source>_normal_initializer</source>
          <target state="translated">_normal_initializer</target>
        </trans-unit>
        <trans-unit id="87ea43bbd5b9352fbaeea30b2cc056ed63741cfd" translate="yes" xml:space="preserve">
          <source>_uniform_initializer</source>
          <target state="translated">_uniform_initializer</target>
        </trans-unit>
        <trans-unit id="d5a25e2ec3739e3d8ae17e7c5a3f6cbea4f5abd6" translate="yes" xml:space="preserve">
          <source>a (major,minor) pair that indicates the minimum CUDA compute capability required, or None if no requirement.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd7aed71c0916f4e514b1b87472e1aaec7a3d81c" translate="yes" xml:space="preserve">
          <source>a 1-D numpy array whose size depends on the algorithm.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="09cc8a0e53d6d87d6a7d95e6a2118c05a24fa639" translate="yes" xml:space="preserve">
          <source>a 1-D tensor whose size depends on the algorithm.</source>
          <target state="translated">一个一维张量,其大小取决于算法。</target>
        </trans-unit>
        <trans-unit id="93178b091b08f4efbffb2adcc54e5c4fc936daed" translate="yes" xml:space="preserve">
          <source>a 1D tensor. Dimensions: out_units</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="60f13d546f38be6d76d9d2976a24c54009d2eb5a" translate="yes" xml:space="preserve">
          <source>a 2D tensor. Dimensions typically: batch, in_units</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="abdaa4ef567c5801d72513ba2ff400234446d877" translate="yes" xml:space="preserve">
          <source>a 2D tensor. Dimensions typically: in_units, out_units</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0832663c1e8c4bc4a737a50ba43d01171dfb1b52" translate="yes" xml:space="preserve">
          <source>a &lt;a href=&quot;../dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; whose elements are to be written to a file</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="63da673dc60479d55d81b4dac49f99e2addfa764" translate="yes" xml:space="preserve">
          <source>a &lt;a href=&quot;../model&quot;&gt;&lt;code&gt;tf.keras.Model&lt;/code&gt;&lt;/a&gt;, its output must match the output of the linear model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="23a2b4256e67b7643d6e462770d9d1ceff2c6a95" translate="yes" xml:space="preserve">
          <source>a &lt;a href=&quot;layer&quot;&gt;&lt;code&gt;tf.keras.layers.Layer&lt;/code&gt;&lt;/a&gt; instance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b7b7f1e42a3452df01c243bc4ff9f9c0a1e957a" translate="yes" xml:space="preserve">
          <source>a &lt;a href=&quot;options&quot;&gt;&lt;code&gt;tf.data.Options&lt;/code&gt;&lt;/a&gt; to merge with</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc2f84a127d1432687ba858bc82ffee5c3694cbb" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;DeviceSpec&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="532617217fec8780a205ebf785c1bcfcd14f51e0" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;DeviceSpec&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a0723a300b3e5a3037c5eec9b6314d294f925e8" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;SavedModel&lt;/code&gt; proto containing the Tensorflow backend graph. Separate graphs are saved for prediction (serving), train, and evaluation. If the model has not been compiled, then only the graph computing predictions will be exported.</source>
          <target state="translated">一个包含Tensorflow后端图的 &lt;code&gt;SavedModel&lt;/code&gt; 原型。保存单独的图形以进行预测（服务），训练和评估。如果尚未编译模型，则将仅导出图形计算预测。</target>
        </trans-unit>
        <trans-unit id="2fe7e88159041491b684ee0b2eef3649a559ee4b" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;SaverDef&lt;/code&gt; protocol buffer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe9334150fba500e634d08a0fd10f09c0e5c3458" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;SparseTensor&lt;/code&gt; operand whose dtype is real, and indices lexicographically ordered.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89fda5dd98ad8f5391967d857c89af91abadfb1e" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;Tensor&lt;/code&gt; of shape &lt;code&gt;sample_shape(x) + self.batch_shape&lt;/code&gt; with values of type &lt;code&gt;self.dtype&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="814d7841b7f08b3882cd124daea00a4d94c3529b" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;Tensor&lt;/code&gt; of shape &lt;code&gt;tf.concat([shape, tf.shape(alpha + beta)], axis=0)&lt;/code&gt; with values of type &lt;code&gt;dtype&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="28f44ad32a3986701c510c4b3dcbe94a32df1ffe" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;Tensor&lt;/code&gt; of shape &lt;code&gt;tf.concat([shape, tf.shape(lam)], axis=0)&lt;/code&gt; with values of type &lt;code&gt;dtype&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7a5aa938284d01a870254921f425995a35654dd9" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;Tensor&lt;/code&gt; with prepended dimensions &lt;code&gt;sample_shape&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4c4f77ba88c28c41bf33a3d3025e3b1e6cda5e8" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;Tensor&lt;/code&gt;, or a dict of string to &lt;code&gt;Tensor&lt;/code&gt;, specifying input nodes that will be fed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cfb18628ed3de27b4a1a234981f499e9dd433d38" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;str&lt;/code&gt; describing the contraction, in the same format as &lt;code&gt;numpy.einsum&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a4f5c14667db94a7f69f7b3c58a35864d1d15ebd" translate="yes" xml:space="preserve">
          <source>a &lt;code&gt;tf.ConfigProto&lt;/code&gt; object.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fd5e709c8e1b6963fd07bdd51f6a8e7d5b1a115e" translate="yes" xml:space="preserve">
          <source>a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3]) a # 2-D tensor</source>
          <target state="translated">a=tf.constant([1,2,3,4,5,6],shape=[2,3])a#二维张量器</target>
        </trans-unit>
        <trans-unit id="d3508e6212f2bb1a5bb507a6ad9f56dc1d60653b" translate="yes" xml:space="preserve">
          <source>a = tf.constant([[1, 2], [3, 4]]) tf.reduce_min(a)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b60dce959a5722be8d9ace37f6ea58387cc7a978" translate="yes" xml:space="preserve">
          <source>a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3]) a # 3-D tensor</source>
          <target state="translated">a=tf.constant(np.arrange(1,13,dtype=np.int32),shape=[2,2,3])a#三维张量器</target>
        </trans-unit>
        <trans-unit id="d5554b43b2c1e709f84d89ff59708b34ac43de5d" translate="yes" xml:space="preserve">
          <source>a ClusterResolver</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="12fec67c34e62bc4fe609123ff4e68b7b09d5716" translate="yes" xml:space="preserve">
          <source>a ConfigProto used to set session parameters, or &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cac282a286b145e90bbae0cb74e2d64c49b7f387" translate="yes" xml:space="preserve">
          <source>a GraphNodeProto that records the results.</source>
          <target state="translated">一个记录结果的GraphNodeProto。</target>
        </trans-unit>
        <trans-unit id="02fdc18cf71e7d138c4670ddd2b89252810f0b7c" translate="yes" xml:space="preserve">
          <source>a Mirrored object.</source>
          <target state="translated">a 镜像对象。</target>
        </trans-unit>
        <trans-unit id="6a65d1b63a2aa1329f486140387a944abe2d35ab" translate="yes" xml:space="preserve">
          <source>a MultiGraphNodeProto that records the results.</source>
          <target state="translated">一个记录结果的MultiGraphNodeProto。</target>
        </trans-unit>
        <trans-unit id="3f756fe3a0f877dbf55d583613842de50b650bca" translate="yes" xml:space="preserve">
          <source>a Tensor or list of Tensors.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="44a87c17dbe409e98a432e4764a0ca67503ad529" translate="yes" xml:space="preserve">
          <source>a TrtConversionParams instance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6adb18be2c65016a8dd91dcca0917351f5c5ea50" translate="yes" xml:space="preserve">
          <source>a boolean indicating whether the input boxes and scores are sorted in descending order by the score.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ae8444bacbe1b98ca2a1d9caa1bde03aa7aa8447" translate="yes" xml:space="preserve">
          <source>a callable taking two parameters, a variable and a list of slot names to create for it. This function should return a dict with the slot names as keys and the created variables as values. When set to None (the default), uses the built-in variable creation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b5462387fade93b713b97cb5dd916b0db334352" translate="yes" xml:space="preserve">
          <source>a callable that takes a single &lt;code&gt;DType&lt;/code&gt; argument and returns a Python &lt;code&gt;boolean&lt;/code&gt; indicating whether the dtype is to be included in the data dumping. Examples:</source>
          <target state="translated">一个带有单个 &lt;code&gt;DType&lt;/code&gt; 参数并返回一个Python &lt;code&gt;boolean&lt;/code&gt; 指示该dtype是否包含在数据转储中）的Callable 。例子：</target>
        </trans-unit>
        <trans-unit id="dadf9b05790e565e376db89427e055d3cb91682a" translate="yes" xml:space="preserve">
          <source>a checkpoint containing the model weights.</source>
          <target state="translated">一个包含模型权重的检查点。</target>
        </trans-unit>
        <trans-unit id="7cbfb1c56d61348ae16c7cd2face5f9db9cab352" translate="yes" xml:space="preserve">
          <source>a dict of string to &lt;code&gt;Tensor&lt;/code&gt; or &lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b740bb539e2724c4812a91601e69e1c90a91fbd" translate="yes" xml:space="preserve">
          <source>a dict of string to &lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79c49bc5c338207d05434ecb9d28173a38bb6f68" translate="yes" xml:space="preserve">
          <source>a dict of string to &lt;code&gt;VarLenFeature&lt;/code&gt;/&lt;code&gt;FixedLenFeature&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="813fdedf351b9814fc880a7e57d9c99855ec1cd5" translate="yes" xml:space="preserve">
          <source>a dict of string to additional groups of receiver tensors, each of which may be a &lt;code&gt;Tensor&lt;/code&gt;, &lt;code&gt;SparseTensor&lt;/code&gt;, or dict of string to &lt;code&gt;Tensor&lt;/code&gt; or&lt;code&gt;SparseTensor&lt;/code&gt;. These named receiver tensor alternatives generate additional serving signatures, which may be used to feed inputs at different points within the input receiver subgraph. A typical usage is to allow feeding raw feature &lt;code&gt;Tensor&lt;/code&gt;s &lt;em&gt;downstream&lt;/em&gt; of the tf.parse_example() op. Defaults to None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7bd9e9cef7c898d18e42b1f2f9c5375410bd03be" translate="yes" xml:space="preserve">
          <source>a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. &lt;a href=&quot;https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional&quot;&gt;Here are details&lt;/a&gt; about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8c591343285301a08eb1eb57b9c63187b713f4d7" translate="yes" xml:space="preserve">
          <source>a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the &lt;a href=&quot;https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional&quot;&gt;details&lt;/a&gt; about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8527fc8500abdc7bc32e70d3d9218debeaf5253f" translate="yes" xml:space="preserve">
          <source>a feature with &lt;code&gt;key=column.name&lt;/code&gt; whose &lt;code&gt;value&lt;/code&gt; is a &lt;code&gt;SparseTensor&lt;/code&gt;.</source>
          <target state="translated">与特征 &lt;code&gt;key=column.name&lt;/code&gt; 其 &lt;code&gt;value&lt;/code&gt; 是一个 &lt;code&gt;SparseTensor&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ded58018cf323ca8b228f31fb7eeecbe3e4bab57" translate="yes" xml:space="preserve">
          <source>a float &lt;code&gt;Tensor&lt;/code&gt; giving the predicted values. Required.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca1e3709c53c6fef2ab454a19e61ce4ffd76640f" translate="yes" xml:space="preserve">
          <source>a float &lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8fa1eefb32219e9ccd295492ff2de693d41b5058" translate="yes" xml:space="preserve">
          <source>a float represented as fraction of 2pi, or a tuple of size 2 representing lower and upper bound for rotating clockwise and counter-clockwise. A positive values means rotating counter clock-wise, while a negative value means clock-wise. When represented as a single float, this value is used for both the upper and lower bound. For instance, &lt;code&gt;factor=(-0.2, 0.3)&lt;/code&gt; results in an output rotation by a random amount in the range &lt;code&gt;[-20% * 2pi, 30% * 2pi]&lt;/code&gt;. &lt;code&gt;factor=0.2&lt;/code&gt; results in an output rotating by a random amount in the range &lt;code&gt;[-20% * 2pi, 20% * 2pi]&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f704c1b4eddb94e2f57ea74f007a87f33d697ca" translate="yes" xml:space="preserve">
          <source>a float represented as fraction of value, or a tuple of size 2 representing lower and upper bound for shifting horizontally. A negative value means shifting image left, while a positive value means shifting image right. When represented as a single positive float, this value is used for both the upper and lower bound. For instance, &lt;code&gt;width_factor=(-0.2, 0.3)&lt;/code&gt; results in an output shifted left by 20%, and shifted right by 30%. &lt;code&gt;width_factor=0.2&lt;/code&gt; results in an output height shifted left or right by 20%.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91c973265bac7ff051653d0165345d52a97d0e23" translate="yes" xml:space="preserve">
          <source>a float represented as fraction of value, or a tuple of size 2 representing lower and upper bound for shifting vertically. A negative value means shifting image up, while a positive value means shifting image down. When represented as a single positive float, this value is used for both the upper and lower bound. For instance, &lt;code&gt;height_factor=(-0.2, 0.3)&lt;/code&gt; results in an output shifted by a random amount in the range [-20%, +30%]. &lt;code&gt;height_factor=0.2&lt;/code&gt; results in an output height shifted by a random amount in the range [-20%, +20%].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7049c0b7f85f9f5d90a665c9a2c18068cc1f50bb" translate="yes" xml:space="preserve">
          <source>a float represented as fraction of value, or a tuple of size 2 representing lower and upper bound for zooming horizontally. When represented as a single float, this value is used for both the upper and lower bound. For instance, &lt;code&gt;width_factor=(0.2, 0.3)&lt;/code&gt; result in an output zooming out between 20% to 30%. &lt;code&gt;width_factor=(-0.3, -0.2)&lt;/code&gt; result in an output zooming in between 20% to 30%. Defaults to &lt;code&gt;None&lt;/code&gt;, i.e., zooming vertical and horizontal directions by preserving the aspect ratio.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2ec50d28cf2adbe8d493bb972ad6ae6b32140707" translate="yes" xml:space="preserve">
          <source>a float represented as fraction of value, or a tuple of size 2 representing lower and upper bound for zooming vertically. When represented as a single float, this value is used for both the upper and lower bound. A positive value means zooming out, while a negative value means zooming in. For instance, &lt;code&gt;height_factor=(0.2, 0.3)&lt;/code&gt; result in an output zoomed out by a random amount in the range [+20%, +30%]. &lt;code&gt;height_factor=(-0.3, -0.2)&lt;/code&gt; result in an output zoomed in by a random amount in the range [+20%, +30%].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="134107ca0c019b55908b80f4021d2430226ea2bd" translate="yes" xml:space="preserve">
          <source>a float representing the threshold for box scores. Boxes with a score that is not larger than this threshold will be suppressed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5f94882e30fda2ab68c451ff7e157ffcd9c9395b" translate="yes" xml:space="preserve">
          <source>a float representing the threshold for deciding whether boxes overlap too much with respect to IoU (intersection over union).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb70148942a0d807c9b8eab06234b1263dd9f73d" translate="yes" xml:space="preserve">
          <source>a float value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="95a34729e004ba21708276447a0c52f66f075797" translate="yes" xml:space="preserve">
          <source>a float. The maximum absolute difference allowed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8cade3d579ad24100c211eef3761440915ebda65" translate="yes" xml:space="preserve">
          <source>a floating point value. The learning rate.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="96653b90a10cbc5bd2bee44ca5eac4e329ab5a1e" translate="yes" xml:space="preserve">
          <source>a function that compares two evaluation results and returns true if current evaluation result is better. Follows the signature:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a9a62fd00059c46a517d9a5d09090fa30f1d118d" translate="yes" xml:space="preserve">
          <source>a function that does accumulation. If None, then &lt;a href=&quot;../math/add_n&quot;&gt;&lt;code&gt;tf.math.add_n&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e212799b3b3be2bf8bb202585b8b934660586c8" translate="yes" xml:space="preserve">
          <source>a function that takes an epoch index (integer, indexed from 0) and current learning rate (float) as inputs and returns a new learning rate as output (float).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e12532b57f71320a99fe67d455119ce2516227a2" translate="yes" xml:space="preserve">
          <source>a function that takes no arguments and returns a &lt;code&gt;ServingInputReceiver&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1bd914955f1635d519f5b5015a7943bdbe1e789c" translate="yes" xml:space="preserve">
          <source>a generator function that yields input data as a list or tuple, which will be used to execute the converted signature for calibration. All the returned input data should have the same shape. Example: &lt;code&gt;def input_fn(): yield input1, input2, input3&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b35ea09d187677febf5398dd21d3f485aef04a83" translate="yes" xml:space="preserve">
          <source>a generator function that yields input data as a list or tuple, which will be used to execute the converted signature to generate TRT engines. Example: `def input_fn():</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1862cc4d53bc4e8a702743354cc37627a768a2f8" translate="yes" xml:space="preserve">
          <source>a generator function which yields data</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="98c108f990a30bbb49abae38f009c82ac5f3a790" translate="yes" xml:space="preserve">
          <source>a generator to be copied from.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="420880ced69a6abdc0537ffab2bcb1d1ec757577" translate="yes" xml:space="preserve">
          <source>a handle to defined flag.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9fc2e7b4a4a43e13919ca0d29891f8252c3bdeb" translate="yes" xml:space="preserve">
          <source>a keras.Model instance.</source>
          <target state="translated">a keras.Model实例。</target>
        </trans-unit>
        <trans-unit id="825f328ed6cb35d3a0907c277ea200144eea6b05" translate="yes" xml:space="preserve">
          <source>a list of Mirrored objects.</source>
          <target state="translated">Mirrored对象的列表。</target>
        </trans-unit>
        <trans-unit id="8af7fc543dbba60d65534412fdeec31fcf0fd35c" translate="yes" xml:space="preserve">
          <source>a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of &lt;code&gt;get_weights&lt;/code&gt;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="64df7eba5e731a4dc70dbacc22e2fd9fd9e1d032" translate="yes" xml:space="preserve">
          <source>a list of checkpoint paths, typically the results of &lt;code&gt;Saver.save()&lt;/code&gt; or those of &lt;a href=&quot;../../../train/latest_checkpoint&quot;&gt;&lt;code&gt;tf.train.latest_checkpoint()&lt;/code&gt;&lt;/a&gt;, regardless of sharded/non-sharded or V1/V2.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cfa77622762529e62cce47be75b5d7ef8f04b09b" translate="yes" xml:space="preserve">
          <source>a list of checkpoint paths.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8652d1441c07c34c2491a5fef88f30f328252a67" translate="yes" xml:space="preserve">
          <source>a list of device strings such as &lt;code&gt;['/gpu:0', '/gpu:1']&lt;/code&gt;. If &lt;code&gt;None&lt;/code&gt;, all available GPUs are used. If no GPUs are found, CPU is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5dd923854fce47fe5a5fbe46663ed04995594696" translate="yes" xml:space="preserve">
          <source>a list of float values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3cae41a96c3b2e908d36cbc657c22fa2a8eb5b02" translate="yes" xml:space="preserve">
          <source>a list of gradients, one for each element of target. Defaults to None.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ca7a0932035e5c9de07f92110d3540ab4ff578d3" translate="yes" xml:space="preserve">
          <source>a list of loss tensors.</source>
          <target state="translated">损失定子清单;</target>
        </trans-unit>
        <trans-unit id="8466476c20c2b4575b81f48bbef14bb81277fd56" translate="yes" xml:space="preserve">
          <source>a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f3cccd7e469ea448da74293963e6a99fb093741" translate="yes" xml:space="preserve">
          <source>a list of prediction keys. Key can be either the class variable of prediction_keys.PredictionKeys or its string value, such as: prediction_keys.PredictionKeys.LOGITS or 'logits'.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe1e44e58043e3bac62424bc232c0f828907d045" translate="yes" xml:space="preserve">
          <source>a list of tuples &lt;code&gt;(tensor, value)&lt;/code&gt;. &lt;code&gt;value&lt;/code&gt; should be a Numpy array.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2e03752a3549be3776b9abdbd4707084bba9047" translate="yes" xml:space="preserve">
          <source>a list of variables that need to be averaged. Only needed if variable_averages is passed in.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3eb0f04e7fc6ee40cdb193ac62baee8bb1ca6431" translate="yes" xml:space="preserve">
          <source>a list of variables that require to use of the moving average variable name to be restored. If None, it will default to variables.moving_average_variables() + variables.trainable_variables()</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91a77371e3824445e17865c21a8864e288e638bc" translate="yes" xml:space="preserve">
          <source>a list or nested structure of Tensors (or IndexedSlices, or None), one for each element in &lt;code&gt;sources&lt;/code&gt;. Returned structure is the same as the structure of &lt;code&gt;sources&lt;/code&gt;.</source>
          <target state="translated">张量的列表或嵌套结构（或IndexedSlices或None）， &lt;code&gt;sources&lt;/code&gt; 每个元素一个。返回的结构与 &lt;code&gt;sources&lt;/code&gt; 的结构相同。</target>
        </trans-unit>
        <trans-unit id="7c876daa0f22003807573109c8a715b9d71802ff" translate="yes" xml:space="preserve">
          <source>a list or nested structure of Tensors or Variables to be differentiated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4276967efbeb15913c139768efe32fe80016a5bd" translate="yes" xml:space="preserve">
          <source>a list or nested structure of Tensors or Variables. &lt;code&gt;target&lt;/code&gt; will be differentiated against elements in &lt;code&gt;sources&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2b8519688d6e08f0ef3f1f53563b370c82f252c" translate="yes" xml:space="preserve">
          <source>a list or tuple of &lt;code&gt;DType&lt;/code&gt; objects or strings that can be converted to &lt;code&gt;DType&lt;/code&gt; objects via &lt;a href=&quot;../../dtypes/as_dtype&quot;&gt;&lt;code&gt;tf.as_dtype()&lt;/code&gt;&lt;/a&gt;. Examples:</source>
          <target state="translated">可以通过&lt;a href=&quot;../../dtypes/as_dtype&quot;&gt; &lt;code&gt;tf.as_dtype()&lt;/code&gt; &lt;/a&gt;转换为 &lt;code&gt;DType&lt;/code&gt; 对象的 &lt;code&gt;DType&lt;/code&gt; 对象或字符串的列表或元组。例子：</target>
        </trans-unit>
        <trans-unit id="4a6bdafb64b93a495b672af20865dc481b0ed23a" translate="yes" xml:space="preserve">
          <source>a list or tuple of prediction keys. Each key can be either the class variable of prediction_keys.PredictionKeys or its string value, such as: prediction_keys.PredictionKeys.CLASSES or 'classes'. If not specified, it will return the predictions for all valid keys.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e062962fff519e51826af13e2686d1b25c61f1c" translate="yes" xml:space="preserve">
          <source>a name for the op that creates the writer.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="82f9e1f9d52a6496cd6e1fbefeba7b546e34e817" translate="yes" xml:space="preserve">
          <source>a nest of NumPy input arrays that will be converted into a dataset. Note that the NumPy arrays are stacked, as that is normal &lt;a href=&quot;../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; behavior.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef8e153138ea9a5535d4306bcbaa35a18fc052c8" translate="yes" xml:space="preserve">
          <source>a nest of NumPy input arrays that will be converted into a dataset. Note that the NumPy arrays are stacked, as that is normal &lt;a href=&quot;../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; behavior.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="beb20ec17e12b3a92d15870c8fb928b39be9384e" translate="yes" xml:space="preserve">
          <source>a new instance of &lt;code&gt;RunConfig&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;RunConfig&lt;/code&gt; 的新实例。</target>
        </trans-unit>
        <trans-unit id="7ebfcefc6e47ac4ed9512e51f768d3cbe5c823cb" translate="yes" xml:space="preserve">
          <source>a numpy array.</source>
          <target state="translated">一个numpy数组。</target>
        </trans-unit>
        <trans-unit id="583166dd6bf07228d6040841ccf402891050a5e4" translate="yes" xml:space="preserve">
          <source>a numpy ndarray.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d253db5a4761b53c7ffde3781cb4f03e7cb07091" translate="yes" xml:space="preserve">
          <source>a path relative to tensorflow root. e.g. &quot;core/platform&quot;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d434acfa4631c87791b616c82b7e6083ce9a28b9" translate="yes" xml:space="preserve">
          <source>a positive float represented as fraction of value, or a tuple of size 2 representing lower and upper bound. When represented as a single float, lower = upper. The contrast factor will be randomly picked between [1.0 - lower, 1.0 + upper].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b17aca81e79c7530a3bc5fdc0659c98cdbd45a6" translate="yes" xml:space="preserve">
          <source>a premade LinearModel, its output must match the output of the dnn model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="259c33a51ca2823379036c66fbe8b38a134e8892" translate="yes" xml:space="preserve">
          <source>a python scalar or a scalar tensor. Mean of the random values to generate.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5d4da2cd944c3f7f7ef6b9224fdc61a3e90e225" translate="yes" xml:space="preserve">
          <source>a python scalar or a scalar tensor. Standard deviation of the random values to generate.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3267c112f6d4d3c0519780ece91f770b5ea07cc2" translate="yes" xml:space="preserve">
          <source>a rank (N+2) &lt;code&gt;filter&lt;/code&gt; Tensor of shape</source>
          <target state="translated">等级（N + 2） &lt;code&gt;filter&lt;/code&gt; 的形状张量</target>
        </trans-unit>
        <trans-unit id="567bea64cd23153635d3d86fe4dc29e83b999e55" translate="yes" xml:space="preserve">
          <source>a rank (N+2) &lt;code&gt;filters&lt;/code&gt; Tensor of shape</source>
          <target state="translated">等级（N + 2） &lt;code&gt;filters&lt;/code&gt; 形状的张量</target>
        </trans-unit>
        <trans-unit id="a9d06e247ca03db1069906ff93f237957d9b8b9a" translate="yes" xml:space="preserve">
          <source>a scalar integer &lt;code&gt;Tensor&lt;/code&gt; representing the maximum number of boxes to be selected by non max suppression.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3fca9faa64f5b46a4d7ce176156b7c88545b1baa" translate="yes" xml:space="preserve">
          <source>a shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e3fab1cb04b3c63a683f54ad32d8e23b5b601ed" translate="yes" xml:space="preserve">
          <source>a single argument or a list of arguments (typically a list of default values); a single argument is converted internally into a list containing one item.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7cbfc6ae061d8de0cb83e913ad03f6acad9950c0" translate="yes" xml:space="preserve">
          <source>a single data type (float32, int32, or string, for example)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d6b451769762c48595037c2a99820e1960c8d664" translate="yes" xml:space="preserve">
          <source>a single or a list the remote server addr in host-port format.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3b5a8f6cdc727f3954a46a20a08afdbba957f018" translate="yes" xml:space="preserve">
          <source>a size entry is interpreted as &lt;em&gt;any&lt;/em&gt; size if it is None or '.'.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1598d219bbf50d48513b4d681ac2e664678a0fb2" translate="yes" xml:space="preserve">
          <source>a size entry is interpreted as an explicit size if it can be parsed as an integer primitive.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc48ba5f69c2b3b1cd2c64475359fd4da8d724e2" translate="yes" xml:space="preserve">
          <source>a string added between each string being joined.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b3901c8cf09bba87b3019c11d2d00ae2d940cb1a" translate="yes" xml:space="preserve">
          <source>a string for the name of the executor to be used to execute functions defined by tf.contrib.eager.defun.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f317a91fbf6fd20ea76003a27e77d226e69d5e9f" translate="yes" xml:space="preserve">
          <source>a string of the form /job:</source>
          <target state="translated">a 形式为/job的字符串。</target>
        </trans-unit>
        <trans-unit id="592805594d7acec2e035ccb78cd57e76d0c89267" translate="yes" xml:space="preserve">
          <source>a string path indicating where to write the TFRecord data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="28879ce16025e8aea78e0eed31c3828842400532" translate="yes" xml:space="preserve">
          <source>a string resource path relative to tensorflow/</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf98d147fdc1a4586c1261348ed08e3ea9d55344" translate="yes" xml:space="preserve">
          <source>a string resource path relative to tensorflow/.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="382e66fb08d6f4af34d47af5189e64c302e25ab8" translate="yes" xml:space="preserve">
          <source>a string specifying the directory in which to write an event file.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8d4b27e20cdf3399f6ff473e703a75399bc6632" translate="yes" xml:space="preserve">
          <source>a string specifying the path to an existing SavedModel.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c782ca359f58ff03f46b2cba60080dcab6928ea" translate="yes" xml:space="preserve">
          <source>a string specifying the path to the SavedModel directory.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e66ef56dd71685a637ed8bde76e00efab58fe5da" translate="yes" xml:space="preserve">
          <source>a string-type Tensor to summarize.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="56c95190656de2a73fe1c13de36287330da4d209" translate="yes" xml:space="preserve">
          <source>a string. The address of the master to use for eval. Defaults to master if not set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c2db3d823dea26a64286f31522b26f56802a1985" translate="yes" xml:space="preserve">
          <source>a string. The address of the master to use for training.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a00a84ed543b41fac41116a0ecb87c993c7cda7" translate="yes" xml:space="preserve">
          <source>a summary_pb2.SummaryDescription</source>
          <target state="translated">a summary_pb2.SummaryDescription。</target>
        </trans-unit>
        <trans-unit id="45f3aa41e8ac0fd197acaacc063a6ad881fe4154" translate="yes" xml:space="preserve">
          <source>a tensor</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e4ae58d9fb87a25f2deeed86b25ef281f7e84171" translate="yes" xml:space="preserve">
          <source>a tensor of rank 1 or higher with a shape of [..., num_boxes].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1b4fd33d17d7c9e469c896528f1317da39655996" translate="yes" xml:space="preserve">
          <source>a tensor of rank 2 or higher with a shape of [..., num_boxes, 4]. Dimensions except the last two are batch dimensions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cb28536b86a99e6d9c4639674cb74b760427a486" translate="yes" xml:space="preserve">
          <source>a tensor or list of tensors</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="640aa7ca6a8766d889609357ce01db334a546544" translate="yes" xml:space="preserve">
          <source>a tuple of (&lt;code&gt;sampled_candidates&lt;/code&gt;, &lt;code&gt;true_expected_count&lt;/code&gt;, &lt;code&gt;sampled_expected_count&lt;/code&gt;) returned by a &lt;code&gt;*_candidate_sampler&lt;/code&gt; function. (if None, we default to &lt;code&gt;log_uniform_candidate_sampler&lt;/code&gt;)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fcfc12d63102b45a3e6037e53f6854edb7f5b381" translate="yes" xml:space="preserve">
          <source>a tuple of 2 integers, specifying the strides of the convolution along the width and height.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d183d11d0d54a060d7009b34542317f6fc714e19" translate="yes" xml:space="preserve">
          <source>a tuple of 2 integers, specifying the width and height of the 2D convolution window.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57a238277396d1dc9a17a850fb1ad6b9aa782459" translate="yes" xml:space="preserve">
          <source>a tuple of a single integer, specifying the length of the 1D convolution window.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="143bed0bba33eacbf68b60d1ca77684c689464bf" translate="yes" xml:space="preserve">
          <source>a tuple of a single integer, specifying the stride length of the convolution.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ab7dc476556d1061e224a2a7a8aa11501a5707ed" translate="yes" xml:space="preserve">
          <source>a tuple of strings, which describes all the TPU devices in the system.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aea224a6c5fec351ebdbae08e0bd3ba1b0c63d75" translate="yes" xml:space="preserve">
          <source>a tuple with (output_row, output_col).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8afd66969dd06ed1eba4fe64521e14d7727d738b" translate="yes" xml:space="preserve">
          <source>a tuple/list of strings.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="47ed70eec68109426c23a40515035b914c0da955" translate="yes" xml:space="preserve">
          <source>a value which can either hold 'none' or 'zero' and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b8ebd3b7d7e693185180008e1a8aaa05435c3d8c" translate="yes" xml:space="preserve">
          <source>a vector of dtype STATE_TYPE representing the initial counter for the RNG, whose length is algorithm-specific.,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0cfcb777f5630e0744dd02bb3be535bf23c0558" translate="yes" xml:space="preserve">
          <source>a vector of dtype STATE_TYPE representing the initial state of the RNG, whose length and semantics are algorithm-specific. If it's a variable, the generator will reuse it instead of creating a new variable.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af97be5a0dcfe7dd967ed814e5c51b34d26e758a" translate="yes" xml:space="preserve">
          <source>a) If a loop variable is a SparseTensor, the shape invariant must be TensorShape([r]) where r is the rank of the dense tensor represented by the sparse tensor. It means the shapes of the three tensors of the SparseTensor are ([None], [None, r], [r]). NOTE: The shape invariant here is the shape of the SparseTensor.dense_shape property. It must be the shape of a vector.</source>
          <target state="translated">a)如果一个循环变量是一个SparseTensor,形状不变式必须是TensorShape([r]),其中r是稀疏张量所代表的密实张量的等级。这意味着SparseTensor的三个张量的形状是([无],[无,r],[r])。注意:这里的形状不变是SparseTensor.dense_shape属性的形状。它必须是矢量的形状。</target>
        </trans-unit>
        <trans-unit id="addb12381e6be0fb67d7d9c7e9625d221a558d6e" translate="yes" xml:space="preserve">
          <source>a: A &lt;code&gt;CSRSparseMatrix&lt;/code&gt;. b: A &lt;code&gt;CSRSparseMatrix&lt;/code&gt; with the same type and rank as &lt;code&gt;a&lt;/code&gt;. type: The type of both &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. transpose_a: If True, &lt;code&gt;a&lt;/code&gt; transposed before multiplication. transpose_b: If True, &lt;code&gt;b&lt;/code&gt; transposed before multiplication. adjoint_a: If True, &lt;code&gt;a&lt;/code&gt; adjointed before multiplication. adjoint_b: If True, &lt;code&gt;b&lt;/code&gt; adjointed before multiplication.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a35ea6647dc95b90fe453d98f66e284fc5f0c506" translate="yes" xml:space="preserve">
          <source>a[0] = 0 : the first value of the sequence is 0</source>
          <target state="translated">a[0]=0:序列的第一个值是0。</target>
        </trans-unit>
        <trans-unit id="deda22cbcad48bb3a1413dc8ff28a54f1e5592e8" translate="yes" xml:space="preserve">
          <source>a[end] = input_row_length : the last value of the sequence is the size</source>
          <target state="translated">a[end]=input_row_length:序列的最后一个值的大小是什么?</target>
        </trans-unit>
        <trans-unit id="eb8f95bc156db1900a569b1735ddc3da656d19bc" translate="yes" xml:space="preserve">
          <source>about sharing states in tensorflow.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f0f846898da4ec61cf83b3e2c8495d2978a0200c" translate="yes" xml:space="preserve">
          <source>absolute tolerance for bfloat16.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ae04d0b7afda8def3fdb79e35983e0773b0864f" translate="yes" xml:space="preserve">
          <source>absolute tolerance for float16.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3cfa07cb0663ab1cde306433e352a29097f65419" translate="yes" xml:space="preserve">
          <source>absolute tolerance for float32.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4fd56eead7560ee583f1b8caad28fbd463bee25" translate="yes" xml:space="preserve">
          <source>absolute tolerance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e1f32f51354c2febc7caaaf4ffeef8cbb8f07cf" translate="yes" xml:space="preserve">
          <source>accum += grad * grad prox_v = var - lr * grad * (1 / sqrt(accum)) var = sign(prox_v)/(1+lr*l2) * max{|prox_v|-lr*l1,0}</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26c2261a427773f28da9c962c61401a44f52fafa" translate="yes" xml:space="preserve">
          <source>accum += grad * grad var -= lr * grad * (1 / (sqrt(accum) + epsilon))</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b39d4103e267bfff6b19f03a0c61954510b506e" translate="yes" xml:space="preserve">
          <source>accum += grad * grad var -= lr * grad * (1 / sqrt(accum))</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fbdff22455472e83bbec28e270efc5acd84cec93" translate="yes" xml:space="preserve">
          <source>accum = accum * momentum + grad var -= lr * accum</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cabf3744d4cc537b306d1c2d4e5c98eab9321c1c" translate="yes" xml:space="preserve">
          <source>accum = accum * momentum - lr * grad var += accum</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="77aa7bbfc2a1935f26fc03cc068c50ab9de9da1b" translate="yes" xml:space="preserve">
          <source>accum = rho() * accum + (1 - rho()) * grad.square(); update = (update_accum + epsilon).sqrt() * (accum + epsilon()).rsqrt() * grad; update_accum = rho() * update_accum + (1 - rho()) * update.square(); var -= update;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e072c3a42e30a463ae763379bfd4971f6786c3db" translate="yes" xml:space="preserve">
          <source>accum_new = accum + grad * grad linear += grad - (accum_new^(-lr_power) - accum^(-lr_power)) / lr * var quadratic = 1.0 / (accum_new^(lr_power) * lr) + 2 * l2 var = (sign(linear) * l1 - linear) / quadratic if |linear| &amp;gt; l1 else 0.0 accum = accum_new</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9207a4d89e0769ab8d4e61e6e2f82d2491a3631b" translate="yes" xml:space="preserve">
          <source>actual distribution of the values to maximize the usage of the lower bit depth and adjusting the output min and max ranges accordingly.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="336367a90c28a1ba840266e48935292b8c0ec99a" translate="yes" xml:space="preserve">
          <source>add and relu and requantize fusion.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3eebca9963a4e9d6f0fcc02adef3bd6f1fb5966b" translate="yes" xml:space="preserve">
          <source>add and relu fusion.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="577f333710522bc0627398bf6ba75e178aa283d7" translate="yes" xml:space="preserve">
          <source>add.</source>
          <target state="translated">add.</target>
        </trans-unit>
        <trans-unit id="b1b7d0394cceca9bd35a4316061103e356401833" translate="yes" xml:space="preserve">
          <source>additional keyword arguments to be passed to the underlying &lt;code&gt;assertAllClose&lt;/code&gt; call.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9d0442ecdcd49f18c5f26a6c7ac65eb7852e6a16" translate="yes" xml:space="preserve">
          <source>adjoints (conjugate transposes).</source>
          <target state="translated">邻接(共轭转位)。</target>
        </trans-unit>
        <trans-unit id="f0e72c8db0ec39595db76cd9ab3ea0b01fa0a54e" translate="yes" xml:space="preserve">
          <source>adjusted_dilation_rate is an int64 tensor of shape [max(spatial&lt;em&gt;dims)], adjusted&lt;/em&gt;{paddings,crops} are int64 tensors of shape [max(spatial_dims), 2]</source>
          <target state="translated">adjusted_dilation_rate是形状的一个Int64张量[最大值（空间&lt;em&gt;变暗）]，调整&lt;/em&gt; {填补处理，作物}是形状的int64张量[MAX（spatial_dims），2]</target>
        </trans-unit>
        <trans-unit id="05ab368bb50c30f0ad36866b483ced3c24cef7fa" translate="yes" xml:space="preserve">
          <source>adjusted_dilation_rate[spatial_dims[i] - 1] = dilation_rate[i] adjusted_paddings[spatial_dims[i] - 1, :] = paddings[i, :] adjusted_crops[spatial_dims[i] - 1, :] = crops[i, :]</source>
          <target state="translated">adjusted_dilation_rate[spatial_dims[i]-1]=dilation_rate[i]adjusted_paddings[spatial_dims[i]-1,:]=paddings[i,:]adjusted_crops[spatial_dims[i]-1,:]=crops[i,]。=crops[i,:]</target>
        </trans-unit>
        <trans-unit id="a4cbf8207b85c3ddcaf0003b5cf461ae49770af7" translate="yes" xml:space="preserve">
          <source>after each call to &lt;code&gt;Saver.save()&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="10582f7ebb86265685f2d94ac9bf45194c26c416" translate="yes" xml:space="preserve">
          <source>aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class &lt;a href=&quot;../../variableaggregation&quot;&gt;&lt;code&gt;tf.VariableAggregation&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">聚合：指示将如何聚合分布式变量。可接受的值是在&lt;a href=&quot;../../variableaggregation&quot;&gt; &lt;code&gt;tf.VariableAggregation&lt;/code&gt; &lt;/a&gt;类中定义的常量。</target>
        </trans-unit>
        <trans-unit id="95142e8db6f0e670c6cdd282ebda2a8dc3235286" translate="yes" xml:space="preserve">
          <source>aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class &lt;a href=&quot;variableaggregation&quot;&gt;&lt;code&gt;tf.VariableAggregation&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">聚合：指示将如何聚合分布式变量。可接受的值是在&lt;a href=&quot;variableaggregation&quot;&gt; &lt;code&gt;tf.VariableAggregation&lt;/code&gt; &lt;/a&gt;类中定义的常量。</target>
        </trans-unit>
        <trans-unit id="0b9a9b6a555fda3290e62d85d68e6dc900f6aa72" translate="yes" xml:space="preserve">
          <source>alias for &quot;input&quot; argument.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="40ab33a302eea4103a7a9fe7699eb8c64bfbebd3" translate="yes" xml:space="preserve">
          <source>alias for expand_nonconcat_dim</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b47a9fb6a6098b56fa005c9bf092d1ede95a90d" translate="yes" xml:space="preserve">
          <source>alpha = input_row_length / output_row_length : our reduction ratio</source>
          <target state="translated">alpha=input_lrow_length/output_lrow_length:我们的缩减率。</target>
        </trans-unit>
        <trans-unit id="13c37d0b0868b315be5754e60b93df3ddf6e1515" translate="yes" xml:space="preserve">
          <source>amount of weight decay to apply; None means that the weights are not decayed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="584cfc3585963e88777945fd0c74c8fed91e27d7" translate="yes" xml:space="preserve">
          <source>amount of weight decay to apply; None means that the weights are not decayed. Weights are decayed by multiplying the weight by this factor each step.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6be75b3f1c53a534ff3d3d88d27d62dd86a8b144" translate="yes" xml:space="preserve">
          <source>an &lt;code&gt;int32&lt;/code&gt; or &lt;code&gt;int64&lt;/code&gt; scalar representing data to be folded in to the seed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="21498301c88a4012ed64c1b4b7225d60e8644258" translate="yes" xml:space="preserve">
          <source>an &lt;code&gt;int&lt;/code&gt; shows until which global step should we wait.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="60271ab37cee858c4218f052e21a7603d914d273" translate="yes" xml:space="preserve">
          <source>an OrderedDict, where the keys are the feature column names and the values are importances. It is sorted by importance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e7cd4b2e10dade102ff7156179e9913d87e9c02" translate="yes" xml:space="preserve">
          <source>an RNG seed (a tensor with shape [2] and dtype &lt;code&gt;int32&lt;/code&gt; or &lt;code&gt;int64&lt;/code&gt;). (When using XLA, only &lt;code&gt;int32&lt;/code&gt; is allowed.)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f4a60ab5b3884b1dd6263e0c0406bab2fcc7011" translate="yes" xml:space="preserve">
          <source>an RNNCell, a projection to output_size is added to it.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4795fec293585f2b8f245ac28ac09c142cad999f" translate="yes" xml:space="preserve">
          <source>an approximation of the area under the P-R curve.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d4d758ce3efd3e79d82b2ead6ef75b7be77f612b" translate="yes" xml:space="preserve">
          <source>an arbitrarily nested structure.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4b9296a58496856781b45283ebf3905a9774cf6" translate="yes" xml:space="preserve">
          <source>an arbitrarily nested structure. Note, numpy arrays are considered atoms and are not flattened.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78bf9b0c50970fcea95462ac064c671c66592d16" translate="yes" xml:space="preserve">
          <source>an enum value of &lt;a href=&quot;../../../../distribute/inputreplicationmode&quot;&gt;&lt;code&gt;tf.distribute.InputReplicationMode&lt;/code&gt;&lt;/a&gt;. Only &lt;code&gt;PER_WORKER&lt;/code&gt; is supported currently, which means there will be a single call to &lt;code&gt;input_fn&lt;/code&gt; per worker. Replicas will dequeue from the local &lt;a href=&quot;../../../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; on their worker.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c1d9638fbc99061f41f5f1718d95aa9d9b2ba446" translate="yes" xml:space="preserve">
          <source>an enum value of &lt;a href=&quot;../../../distribute/inputreplicationmode&quot;&gt;&lt;code&gt;tf.distribute.InputReplicationMode&lt;/code&gt;&lt;/a&gt;. Only &lt;code&gt;PER_WORKER&lt;/code&gt; is supported currently, which means there will be a single call to &lt;code&gt;input_fn&lt;/code&gt; per worker. Replicas will dequeue from the local &lt;a href=&quot;../../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; on their worker.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3759390b842509817496ad6964a87f97c5843a65" translate="yes" xml:space="preserve">
          <source>an input generator that can be used to generate input samples for the model. This must be a callable object that returns an object that supports the &lt;code&gt;iter()&lt;/code&gt; protocol (e.g. a generator function). The elements generated must have same type and shape as inputs to the model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8feb6baf60936cf60390ee049e7a07e3ffd4392a" translate="yes" xml:space="preserve">
          <source>an input sequence.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2d46e772e042b91ec9dff778f5110d720bafa374" translate="yes" xml:space="preserve">
          <source>an instance of &lt;a href=&quot;../configproto&quot;&gt;&lt;code&gt;tf.compat.v1.ConfigProto&lt;/code&gt;&lt;/a&gt; proto used to configure the session. It's the &lt;code&gt;config&lt;/code&gt; argument of constructor of &lt;a href=&quot;../session&quot;&gt;&lt;code&gt;tf.compat.v1.Session&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1a37f2503f46094f2d360fd3f7fc596a57b2fd3f" translate="yes" xml:space="preserve">
          <source>an instance of &lt;a href=&quot;topology&quot;&gt;&lt;code&gt;tf.tpu.experimental.Topology&lt;/code&gt;&lt;/a&gt;, which describes the physical topology of TPU system.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd841e8ca3fc9c05fb813b0d2f5d9103dc9c0ba8" translate="yes" xml:space="preserve">
          <source>an instance of &lt;code&gt;tf.train.experimental/ClusterDeviceFilters&lt;/code&gt; that specify device filters to the remote tasks in cluster.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e2d70312bfddfcd3ddb72da0a20ed277ffcd67f" translate="yes" xml:space="preserve">
          <source>an integer or 1-D numpy array.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2085d387df6482fce1af122d6acdb0118638ce0b" translate="yes" xml:space="preserve">
          <source>an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any &lt;code&gt;dilation_rate&lt;/code&gt; value != 1 is incompatible with specifying any stride value != 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="12a0347d0bf66b603b4cf339883f33c0d26b06b7" translate="yes" xml:space="preserve">
          <source>an integer or tuple/list of 3 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any &lt;code&gt;dilation_rate&lt;/code&gt; value != 1 is incompatible with specifying any stride value != 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3402e6e4b4d2964f62630d5d955c60e92608fd4" translate="yes" xml:space="preserve">
          <source>an integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any &lt;code&gt;dilation_rate&lt;/code&gt; value != 1 is incompatible with specifying any &lt;code&gt;strides&lt;/code&gt; value != 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="58266c81cd572b21187fd9645689ccfc5a6f9adf" translate="yes" xml:space="preserve">
          <source>an integer representing the number of boxes in a tile, i.e., the maximum number of boxes per image that can be used to suppress other boxes in parallel; larger tile_size means larger parallelism and potentially more redundant work.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="12f87a898c564b834112575d56e2166669cc6140" translate="yes" xml:space="preserve">
          <source>an integer, specifying the dilation rate to use for dilated convolution. Currently, specifying a &lt;code&gt;dilation_rate&lt;/code&gt; value != 1 is incompatible with specifying a stride value != 1.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="36442510bd24021bdca61b0550d53fc4b7f41549" translate="yes" xml:space="preserve">
          <source>an integer: 1 or 2. 1 corresponds to V1, 2 corresponds to V2. (Defaults to V1). With V1, &lt;code&gt;export_saved_model()&lt;/code&gt; adds rewrite() and TPUPartitionedCallOp() for user; while in v2, user is expected to add rewrite(), TPUPartitionedCallOp() etc in their model_fn. A helper function &lt;code&gt;inference_on_tpu&lt;/code&gt; is provided for V2. brn_tpu_estimator.py includes examples for both versions i.e. TPUEstimatorExportTest and TPUEstimatorExportV2Test.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15bd4e656cd87110cb6d06a7f5b3f1e300030cdd" translate="yes" xml:space="preserve">
          <source>an optional &lt;code&gt;dilation_rate&lt;/code&gt; tensor of shape &lt;a href=&quot;defaulting%20to%20%5b1%5d*n&quot;&gt;N&lt;/a&gt; specifying the filter upsampling/input downsampling rate, and an optional list of N &lt;code&gt;strides&lt;/code&gt; (defaulting [1]*N), this computes for each N-D spatial output position (x[0], ..., x[N-1]):</source>
          <target state="translated">指定形状&lt;a href=&quot;defaulting%20to%20%5b1%5d*n&quot;&gt;N&lt;/a&gt;的可选 &lt;code&gt;dilation_rate&lt;/code&gt; 张量张量，指定滤波器的上采样/输入下采样率，以及可选的N &lt;code&gt;strides&lt;/code&gt; 列表（默认为[1] * N），这将为每个ND空间输出位置（x [0]，...， x [N-1]）：</target>
        </trans-unit>
        <trans-unit id="0ee24f38b8e109fbd5639cc2b47bec50e32475ed" translate="yes" xml:space="preserve">
          <source>an optional &lt;code&gt;dilations&lt;/code&gt; tensor of shape &lt;a href=&quot;defaulting%20to%20%5b1%5d*n&quot;&gt;N&lt;/a&gt; specifying the filter upsampling/input downsampling rate, and an optional list of N &lt;code&gt;strides&lt;/code&gt; (defaulting [1]*N), this computes for each N-D spatial output position (x[0], ..., x[N-1]):</source>
          <target state="translated">形状为&lt;a href=&quot;defaulting%20to%20%5b1%5d*n&quot;&gt;N&lt;/a&gt;的可选 &lt;code&gt;dilations&lt;/code&gt; 张量（指定滤波器上采样/输入下采样率）以及可选的N个 &lt;code&gt;strides&lt;/code&gt; 列表（默认为[1] * N），用于计算每个ND空间输出位置（x [0]，...， x [N-1]）：</target>
        </trans-unit>
        <trans-unit id="ab989fce67e1a28670d6c810e49317fa6212a61a" translate="yes" xml:space="preserve">
          <source>an optional name for the operation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3f33599b0487789dd89792f29c86c4b410cef6e4" translate="yes" xml:space="preserve">
          <source>an optional string of the form /job:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cffa50a32cb13a240d705317bcec65dd1f31b6ad" translate="yes" xml:space="preserve">
          <source>and</source>
          <target state="translated">and</target>
        </trans-unit>
        <trans-unit id="ebd46f618c63c3e212b1d778f0a25f4660c23598" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;SparseFeature&lt;/code&gt; config with 2 &lt;code&gt;index_key&lt;/code&gt;s</source>
          <target state="translated">和 &lt;code&gt;SparseFeature&lt;/code&gt; 配置有2 &lt;code&gt;index_key&lt;/code&gt; 小号</target>
        </trans-unit>
        <trans-unit id="6140381926bf0d082343ace25ade2e3cf221f627" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;default_value&lt;/code&gt; is &lt;code&gt;x&lt;/code&gt;, then the output will be a dense &lt;code&gt;[3, 5]&lt;/code&gt; string tensor with values:</source>
          <target state="translated">并且 &lt;code&gt;default_value&lt;/code&gt; 是 &lt;code&gt;x&lt;/code&gt; ，那么输出将是一个密集的 &lt;code&gt;[3, 5]&lt;/code&gt; 字符串张量，其值如下：</target>
        </trans-unit>
        <trans-unit id="90e193944d6b8498e0257c00b2820369a0de3479" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;ids&lt;/code&gt; is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b2d759c667e68226ece3acd5aa4e1cc6620b0ca0" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;max&lt;/code&gt; to 'outputs' tensor of same shape as &lt;code&gt;inputs&lt;/code&gt;.</source>
          <target state="translated">和 &lt;code&gt;max&lt;/code&gt; 到与 &lt;code&gt;inputs&lt;/code&gt; 相同形状的&amp;ldquo;输出&amp;rdquo;张量。</target>
        </trans-unit>
        <trans-unit id="78fbe88a4e8420d083776e826c4b0cd5bb232375" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;shape&lt;/code&gt; is &lt;code&gt;[9, -1]&lt;/code&gt;, then the output will be a &lt;code&gt;SparseTensor&lt;/code&gt; of shape &lt;code&gt;[9, 4]&lt;/code&gt; and &lt;code&gt;indices&lt;/code&gt; / &lt;code&gt;values&lt;/code&gt;:</source>
          <target state="translated">并且 &lt;code&gt;shape&lt;/code&gt; 为 &lt;code&gt;[9, -1]&lt;/code&gt; ，则输出将为形状为 &lt;code&gt;[9, 4]&lt;/code&gt; &lt;code&gt;SparseTensor&lt;/code&gt; ]和 &lt;code&gt;indices&lt;/code&gt; / &lt;code&gt;values&lt;/code&gt; 的SparseTensor：</target>
        </trans-unit>
        <trans-unit id="23cce7bb518f569f4a001bd92a904ee1f95e7599" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;to_retain = [True, False, False, True]&lt;/code&gt;, then the output will be a &lt;code&gt;SparseTensor&lt;/code&gt; of shape &lt;code&gt;[4, 5]&lt;/code&gt; with 2 non-empty values:</source>
          <target state="translated">和 &lt;code&gt;to_retain = [True, False, False, True]&lt;/code&gt; ，则输出将是形状为 &lt;code&gt;[4, 5]&lt;/code&gt; 的 &lt;code&gt;SparseTensor&lt;/code&gt; ，具有2个非空值：</target>
        </trans-unit>
        <trans-unit id="3b2da781f92810fe2701eb25993573d612b9c1c1" translate="yes" xml:space="preserve">
          <source>and &lt;code&gt;vocab_size = 200&lt;/code&gt;, then the output will be a &lt;code&gt;[2, 3, 200]&lt;/code&gt; dense bool tensor with False everywhere except at positions</source>
          <target state="translated">并且 &lt;code&gt;vocab_size = 200&lt;/code&gt; ，那么输出将是一个 &lt;code&gt;[2, 3, 200]&lt;/code&gt; 2，3，200 ]密集的布尔张量，除位置以外的所有地方均为 False</target>
        </trans-unit>
        <trans-unit id="fb47192727f17143a048825f4ab10f7ac6f7e0a3" translate="yes" xml:space="preserve">
          <source>and False elsewhere in &lt;code&gt;output&lt;/code&gt;.</source>
          <target state="translated">在 &lt;code&gt;output&lt;/code&gt; 其他地方为False 。</target>
        </trans-unit>
        <trans-unit id="41290a0f5cbc0ce6a4dfe0924ac8b26a039d1f24" translate="yes" xml:space="preserve">
          <source>and concatenates them into a Tensor of shape:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c7d1a416a2e3a434f40699d9880f90f9fb160f7e" translate="yes" xml:space="preserve">
          <source>and having size</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2223100c859ddb72353f03c35f828882929aa04b" translate="yes" xml:space="preserve">
          <source>and if &lt;code&gt;M = N&lt;/code&gt;,</source>
          <target state="translated">如果 &lt;code&gt;M = N&lt;/code&gt; ，</target>
        </trans-unit>
        <trans-unit id="1eb3d95c362b4e4d700bf804104fec0c967f459d" translate="yes" xml:space="preserve">
          <source>and process 2 prints</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0ba895928ad073edb53326fb61cbb432a1464bec" translate="yes" xml:space="preserve">
          <source>and that &lt;code&gt;value&lt;/code&gt; has shape</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="34d19665daa7f1410ef46ef88c82079153b1d866" translate="yes" xml:space="preserve">
          <source>and then compute a normalized (x), including a small factor ({\epsilon}) for numerical stability.</source>
          <target state="translated">然后计算一个归一化的(x),包括一个小的因子({/epsilon})以保证数值的稳定性。</target>
        </trans-unit>
        <trans-unit id="d8047f0fdc4d1e4560c970ac0b82f51065da8044" translate="yes" xml:space="preserve">
          <source>and then compute a normalized &lt;code&gt;x_i_normalized&lt;/code&gt;, including a small factor &lt;code&gt;epsilon&lt;/code&gt; for numerical stability.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6cc52111330674297a42724a1642fd203137a2d3" translate="yes" xml:space="preserve">
          <source>and therefore</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="78e8b25cc130b65509b44b60a2a5f53ed883a7db" translate="yes" xml:space="preserve">
          <source>append(self: tensorflow.python._tf_stack.StackSummary, x: tensorflow.python._tf_stack.FrameSummary) -&amp;gt; None</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="30ff52b5b667d11b2ffe908d3a6ddb3a80cc4c30" translate="yes" xml:space="preserve">
          <source>arbitrary function</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9aafd247e69aac8e786d351d47d5cdf462d0745f" translate="yes" xml:space="preserve">
          <source>arithmetic_optimization: Simplify arithmetic ops with common sub-expression elimination and arithmetic simplification.</source>
          <target state="translated">算术_优化。通过消除常见的子表达式和简化算术操作来简化算术操作。</target>
        </trans-unit>
        <trans-unit id="25c002c4154dbe462f9f8f1755f73522b0671750" translate="yes" xml:space="preserve">
          <source>array([[ 0., 0., 0.], [ 0., 0., 0.]], dtype=float32)</source>
          <target state="translated">数组([[0.,0.,0.],[0.,0.,0.]],dtype=float32)</target>
        </trans-unit>
        <trans-unit id="50893921378b303e1550a97d489bc7de28064118" translate="yes" xml:space="preserve">
          <source>array-like, shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt; Test samples where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples and &lt;code&gt;n_features&lt;/code&gt; is the number of features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89cdb649416fd6a0dc9e7f39d49b6ab03d5879e8" translate="yes" xml:space="preserve">
          <source>array-like, shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt; Training samples where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples and &lt;code&gt;n_features&lt;/code&gt; is the number of features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="850565e7e77632b17d2cc1ddc239a6bfe2cae1b3" translate="yes" xml:space="preserve">
          <source>array-like, shape &lt;code&gt;(n_samples, n_outputs)&lt;/code&gt; Class probability estimates. In the case of binary classification, to match the scikit-learn API, will return an array of shape &lt;code&gt;(n_samples, 2)&lt;/code&gt; (instead of &lt;code&gt;(n_sample, 1)&lt;/code&gt; as in Keras).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="65baf57d96f3cc1470e57342810a1de5f6a27f59" translate="yes" xml:space="preserve">
          <source>array-like, shape &lt;code&gt;(n_samples,)&lt;/code&gt; Class predictions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ae0244ecc60195106b25820595b8db35a60ed5ec" translate="yes" xml:space="preserve">
          <source>array-like, shape &lt;code&gt;(n_samples,)&lt;/code&gt; Predictions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3416fc16abdc6956ef8e2ee3753937f82806270c" translate="yes" xml:space="preserve">
          <source>array-like, shape &lt;code&gt;(n_samples,)&lt;/code&gt; True labels for &lt;code&gt;x&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c4e9940ae53eb744fbac597e70feee59a95b69b8" translate="yes" xml:space="preserve">
          <source>array-like, shape &lt;code&gt;(n_samples,)&lt;/code&gt; or &lt;code&gt;(n_samples, n_outputs)&lt;/code&gt; True labels for &lt;code&gt;x&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c0c679f2bbcaf59c6d5d58d588516277aa10113f" translate="yes" xml:space="preserve">
          <source>as cpu and gpu are mutually exclusive. All entries are optional.</source>
          <target state="translated">因为cpu和gpu是相互排斥的。所有条目都是可选的。</target>
        </trans-unit>
        <trans-unit id="d81b6defc8e7983d824c066efff71d103284c806" translate="yes" xml:space="preserve">
          <source>as inputs.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="307d5c7893e1f246cb5e563745a0402addd02464" translate="yes" xml:space="preserve">
          <source>assertSameElements([1, 1, 1, 0, 0, 0], [0, 1]) # Doesn't raise an AssertionError</source>
          <target state="translated">assertSameElements([1,1,1,0,0,0],[0,1])#不会引发断言错误。</target>
        </trans-unit>
        <trans-unit id="2454abb84d63a230ebc92ebe78cf3aff74dea1e0" translate="yes" xml:space="preserve">
          <source>assertSetEqual uses ducktyping to support different types of sets, and is optimized for sets specifically (parameters must support a difference method).</source>
          <target state="translated">assertSetEqual使用ducktyping来支持不同类型的集合,并专门针对集合进行了优化(参数必须支持差异方法)。</target>
        </trans-unit>
        <trans-unit id="223c6148b033dfedd56732627314f568565b952f" translate="yes" xml:space="preserve">
          <source>assertTotallyOrdered will check that instances can be ordered correctly. For example,</source>
          <target state="translated">assertTotallyOrdered将检查实例是否可以正确排序。例如</target>
        </trans-unit>
        <trans-unit id="260d4f1366ba09adb6172ca8fd634126e0e09014" translate="yes" xml:space="preserve">
          <source>associative container. Elements are ordered by key.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="88d718f8abfa216a9d9fcc60c2c44acc25a8cb82" translate="yes" xml:space="preserve">
          <source>at &lt;code&gt;ckpt_path&lt;/code&gt; and potentially reorders its rows and columns using the specified remappings.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b644d3d1649aa456171b6c0036601fd209ffbd9d" translate="yes" xml:space="preserve">
          <source>at the end of session</source>
          <target state="new"/>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
