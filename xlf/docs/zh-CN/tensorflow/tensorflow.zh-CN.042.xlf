<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="tensorflow">
    <body>
      <group id="tensorflow">
        <trans-unit id="e53a92d847e73238072fa6528b3217c26234a931" translate="yes" xml:space="preserve">
          <source>This is completely equivalent to the slightly longer code:</source>
          <target state="translated">这完全相当于稍长的代码。</target>
        </trans-unit>
        <trans-unit id="bb94386b1c4a4c290298db5397c611953054ddc2" translate="yes" xml:space="preserve">
          <source>This is convenient in interactive shells and &lt;a href=&quot;http://ipython.org&quot;&gt;IPython notebooks&lt;/a&gt;, as it avoids having to pass an explicit &lt;code&gt;Session&lt;/code&gt; object to run ops.</source>
          <target state="translated">在交互式shell和&lt;a href=&quot;http://ipython.org&quot;&gt;IPython笔记本中&lt;/a&gt;，这很方便，因为它避免了必须传递显式 &lt;code&gt;Session&lt;/code&gt; 对象来运行ops。</target>
        </trans-unit>
        <trans-unit id="9c4ed05585b78549393533536c5eaedd31668459" translate="yes" xml:space="preserve">
          <source>This is different from &lt;code&gt;get_collection()&lt;/code&gt; which always returns a copy of the collection list if it exists and never creates an empty collection.</source>
          <target state="translated">这与 &lt;code&gt;get_collection()&lt;/code&gt; 不同，后者总是返回集合列表的副本（如果存在），并且从不创建空集合。</target>
        </trans-unit>
        <trans-unit id="6e7866f4c4c548ff00d81ad9995e04b7c1f8a6da" translate="yes" xml:space="preserve">
          <source>This is different from &lt;code&gt;get_collection_ref()&lt;/code&gt; which always returns the actual collection list if it exists in that it returns a new list each time it is called.</source>
          <target state="translated">这与 &lt;code&gt;get_collection_ref()&lt;/code&gt; 不同，后者总是返回实际的集合列表（如果存在），因为它每次调用时都会返回一个新列表。</target>
        </trans-unit>
        <trans-unit id="c1f83f54b0650a6fa73b0409ff1516763b5fe7d4" translate="yes" xml:space="preserve">
          <source>This is equivalent to applying LSTMBlockCell in a loop, like so:</source>
          <target state="translated">这相当于在循环中应用LSTMBlockCell,像这样。</target>
        </trans-unit>
        <trans-unit id="731ffc6237ea1b1cd740ef79a9e5ef7a26a2d7c5" translate="yes" xml:space="preserve">
          <source>This is essentially a shortcut for &lt;code&gt;assign(self, value)&lt;/code&gt;.</source>
          <target state="translated">本质上，这是 &lt;code&gt;assign(self, value)&lt;/code&gt; 的快捷方式。</target>
        </trans-unit>
        <trans-unit id="0bbe53c86ac89d70d42d35929c4d3967934b3af3" translate="yes" xml:space="preserve">
          <source>This is essentially a shortcut for &lt;code&gt;assign_add(self, delta)&lt;/code&gt;.</source>
          <target state="translated">这本质上是 &lt;code&gt;assign_add(self, delta)&lt;/code&gt; 的快捷方式。</target>
        </trans-unit>
        <trans-unit id="13da478f41797a8113b6021c13a3264c6404bfc3" translate="yes" xml:space="preserve">
          <source>This is essentially a shortcut for &lt;code&gt;assign_sub(self, delta)&lt;/code&gt;.</source>
          <target state="translated">这本质上是 &lt;code&gt;assign_sub(self, delta)&lt;/code&gt; 的快捷方式。</target>
        </trans-unit>
        <trans-unit id="e2b79a6063d963599f65675c034b474bb3cf4380" translate="yes" xml:space="preserve">
          <source>This is essentially a shortcut for &lt;code&gt;count_up_to(self, limit)&lt;/code&gt;.</source>
          <target state="translated">这本质上是 &lt;code&gt;count_up_to(self, limit)&lt;/code&gt; 的快捷方式。</target>
        </trans-unit>
        <trans-unit id="1f513c169143d5db244b63790e45478adc90df06" translate="yes" xml:space="preserve">
          <source>This is expected to return a constant value that will not be changed throughout its life cycle.</source>
          <target state="translated">预计这将返回一个在其生命周期内不会改变的恒定值。</target>
        </trans-unit>
        <trans-unit id="cf3c566ed68759dd2e1f8f0b989dfbaaf3ac89b7" translate="yes" xml:space="preserve">
          <source>This is for example useful if you want to save to a local directory, such as &quot;/tmp&quot; when running in a distributed setting. In that case pass a device for the host where the &quot;/tmp&quot; directory is accessible.</source>
          <target state="translated">例如,如果你想保存到本地目录,例如在分布式环境中运行时的&quot;/tmp&quot;,这就很有用。在这种情况下,传递给主机一个可以访问&quot;/tmp &quot;目录的设备。</target>
        </trans-unit>
        <trans-unit id="cbd5047389c2714c42e18e715b6900b97495f191" translate="yes" xml:space="preserve">
          <source>This is for use with models that expect a single &lt;code&gt;Tensor&lt;/code&gt; or &lt;code&gt;SparseTensor&lt;/code&gt; as an input feature, as opposed to a dict of features.</source>
          <target state="translated">与期望将单个 &lt;code&gt;Tensor&lt;/code&gt; 或 &lt;code&gt;SparseTensor&lt;/code&gt; 用作输入特征的模型一起使用，而不是功能的决定。</target>
        </trans-unit>
        <trans-unit id="76d3e8dd92950ccc6782125edfc083318d1af070" translate="yes" xml:space="preserve">
          <source>This is implemented as a generalized linear model, see &lt;a href=&quot;https://en.wikipedia.org/wiki/Generalized_linear_model&quot;&gt;https://en.wikipedia.org/wiki/Generalized_linear_model&lt;/a&gt;</source>
          <target state="translated">这是作为广义线性模型实现的，请参见&lt;a href=&quot;https://en.wikipedia.org/wiki/Generalized_linear_model&quot;&gt;https://en.wikipedia.org/wiki/Generalized_linear_model&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4f78d4311c49702fc3e064ee729929a77dc6e0a0" translate="yes" xml:space="preserve">
          <source>This is implemented as a generalized linear model, see https://en.wikipedia.org/wiki/Generalized_linear_model.</source>
          <target state="translated">这是以广义线性模型的形式实现的,见https://en.wikipedia.org/wiki/Generalized_linear_model。</target>
        </trans-unit>
        <trans-unit id="7a79baaefeeabffec27522a961ac347462293274" translate="yes" xml:space="preserve">
          <source>This is important for the RNN layer to invoke this in it call() method so that the cached mask is cleared before calling the cell.call(). The mask should be cached across the timestep within the same batch, but shouldn't be cached between batches. Otherwise it will introduce unreasonable bias against certain index of data within the batch.</source>
          <target state="translated">这对于RNN层来说很重要,在它的call()方法中调用这个,这样在调用cell.call()之前,缓存的掩码就会被清除。掩码应该在同一批次内跨时间步长进行缓存,但不应该在批次之间进行缓存。否则会引入不合理的偏向,不利于批内某些数据的索引。</target>
        </trans-unit>
        <trans-unit id="b5e2c7b524f309c97d9b307ab3f360df187084ea" translate="yes" xml:space="preserve">
          <source>This is intended to be used on signals (or images). Produces a PSNR value for each image in batch.</source>
          <target state="translated">这是要用于信号(或图像)。为每个批次的图像产生一个PSNR值。</target>
        </trans-unit>
        <trans-unit id="5c8364ff06f44a0d2f95291cd9a883d7572dfead" translate="yes" xml:space="preserve">
          <source>This is just a shortcut for &lt;code&gt;variables_initializer(global_variables())&lt;/code&gt;</source>
          <target state="translated">这只是 &lt;code&gt;variables_initializer(global_variables())&lt;/code&gt; 的快捷方式</target>
        </trans-unit>
        <trans-unit id="df7f814ad895f2447cfabb704fb289f9c5058c9f" translate="yes" xml:space="preserve">
          <source>This is just a shortcut for &lt;code&gt;variables_initializer(local_variables())&lt;/code&gt;</source>
          <target state="translated">这只是 &lt;code&gt;variables_initializer(local_variables())&lt;/code&gt; 的快捷方式</target>
        </trans-unit>
        <trans-unit id="1f3a1e67435c460ebf6c9461373717a014d4077f" translate="yes" xml:space="preserve">
          <source>This is like &lt;code&gt;Restore&lt;/code&gt; except that restored tensor can be listed as filling only a slice of a larger tensor. &lt;code&gt;shape_and_slice&lt;/code&gt; specifies the shape of the larger tensor and the slice that the restored tensor covers.</source>
          <target state="translated">就像 &lt;code&gt;Restore&lt;/code&gt; 一样，除了恢复的张量可以列为仅填充较大张量的一个切片之外。 &lt;code&gt;shape_and_slice&lt;/code&gt; 指定较大张量的形状以及恢复的张量覆盖的切片。</target>
        </trans-unit>
        <trans-unit id="9a946a8e53161857f667092e8b20dfd7b65825b0" translate="yes" xml:space="preserve">
          <source>This is like &lt;code&gt;Save&lt;/code&gt; except that tensors can be listed in the saved file as being a slice of a larger tensor. &lt;code&gt;shapes_and_slices&lt;/code&gt; specifies the shape of the larger tensor and the slice that this tensor covers. &lt;code&gt;shapes_and_slices&lt;/code&gt; must have as many elements as &lt;code&gt;tensor_names&lt;/code&gt;.</source>
          <target state="translated">这类似于&amp;ldquo; &lt;code&gt;Save&lt;/code&gt; 不同之处在于张量可以在已保存的文件中列为较大张量的一部分。 &lt;code&gt;shapes_and_slices&lt;/code&gt; 指定较大张量的形状以及该张量覆盖的切片。 &lt;code&gt;shapes_and_slices&lt;/code&gt; 必须有一样多的元素 &lt;code&gt;tensor_names&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="3f6a76852476cdaa13e9bdd3f2afe687e9e8f273" translate="yes" xml:space="preserve">
          <source>This is like &lt;code&gt;sigmoid_cross_entropy_with_logits()&lt;/code&gt; except that &lt;code&gt;pos_weight&lt;/code&gt;, allows one to trade off recall and precision by up- or down-weighting the cost of a positive error relative to a negative error.</source>
          <target state="translated">类似于 &lt;code&gt;sigmoid_cross_entropy_with_logits()&lt;/code&gt; ,不同之处在于 &lt;code&gt;pos_weight&lt;/code&gt; 允许通过相对于负误差增加或减小正误差的成本来权衡召回率和精度。</target>
        </trans-unit>
        <trans-unit id="45f8d315bd404d0acf0d22377d0fcd8dbfde7b6a" translate="yes" xml:space="preserve">
          <source>This is mathematically equivalent to the classic formula below, but the use of an &lt;code&gt;assign_sub&lt;/code&gt; op (the &lt;code&gt;&quot;-=&quot;&lt;/code&gt; in the formula) allows concurrent lockless updates to the variables:</source>
          <target state="translated">这在数学上等同于下面的经典公式，但是使用 &lt;code&gt;assign_sub&lt;/code&gt; op（公式中的 &lt;code&gt;&quot;-=&quot;&lt;/code&gt; ）允许并发无锁更新变量：</target>
        </trans-unit>
        <trans-unit id="a748c5d3b3b745591c3f3c629fe6702e2487d204" translate="yes" xml:space="preserve">
          <source>This is matrix product, not element-wise product.</source>
          <target state="translated">这是矩阵积,不是元素积。</target>
        </trans-unit>
        <trans-unit id="e3ce6465dd501a9c64e67f9787b8dbacdd54e8b7" translate="yes" xml:space="preserve">
          <source>This is matrix-vector product, not element-wise product.</source>
          <target state="translated">这是矩阵向量积,不是元素向量积。</target>
        </trans-unit>
        <trans-unit id="8c5b780da30371ec04532fe08dfb27ab4f7f20a6" translate="yes" xml:space="preserve">
          <source>This is more efficient than using separate &lt;a href=&quot;../reverse&quot;&gt;&lt;code&gt;tf.reverse&lt;/code&gt;&lt;/a&gt; ops.</source>
          <target state="translated">这比使用单独的&lt;a href=&quot;../reverse&quot;&gt; &lt;code&gt;tf.reverse&lt;/code&gt; &lt;/a&gt; ops 更有效。</target>
        </trans-unit>
        <trans-unit id="eabc0e8c9ba11bd382b9ff6e31e4cc504e020ab9" translate="yes" xml:space="preserve">
          <source>This is more efficient than using separate &lt;a href=&quot;../reverse&quot;&gt;&lt;code&gt;tf.reverse&lt;/code&gt;&lt;/a&gt; ops. The &lt;code&gt;reverse&lt;/code&gt; and &lt;code&gt;exclusive&lt;/code&gt; kwargs can also be combined:</source>
          <target state="translated">这比使用单独的&lt;a href=&quot;../reverse&quot;&gt; &lt;code&gt;tf.reverse&lt;/code&gt; &lt;/a&gt; ops 更有效。的 &lt;code&gt;reverse&lt;/code&gt; 和 &lt;code&gt;exclusive&lt;/code&gt; kwargs也可以组合：</target>
        </trans-unit>
        <trans-unit id="80ea1c276768e8dc61eb8fd686066ffd896d6dd9" translate="yes" xml:space="preserve">
          <source>This is not a graph construction method, it does not add ops to the graph.</source>
          <target state="translated">这不是一个图的构造方法,它不会给图增加操作。</target>
        </trans-unit>
        <trans-unit id="afbcb9b1912c3cf36d0b4a3b3d79a1accb7231d9" translate="yes" xml:space="preserve">
          <source>This is particularly useful for creating a critical section when used in conjunction with &lt;code&gt;MutexLockIdentity&lt;/code&gt;:</source>
          <target state="translated">与 &lt;code&gt;MutexLockIdentity&lt;/code&gt; 结合使用时，这对于创建关键部分特别有用：</target>
        </trans-unit>
        <trans-unit id="be0ccfef7a257148d1bd487cc01dfdee2f9dcbb7" translate="yes" xml:space="preserve">
          <source>This is similar to &lt;code&gt;embedding_column&lt;/code&gt;, except that it produces a list of embedding columns that share the same embedding weights.</source>
          <target state="translated">这类似于 &lt;code&gt;embedding_column&lt;/code&gt; ，除了它会产生共享相同嵌入权重的嵌入列的列表。</target>
        </trans-unit>
        <trans-unit id="16ef5940d98e86cc048c2b1d955cc28ebe55e81d" translate="yes" xml:space="preserve">
          <source>This is supposed to be executed in the beginning of the chief/sync thread so that even if the total_num_replicas is less than replicas_to_aggregate, the model can still proceed as the replicas can compute multiple steps per variable update. Make sure: &lt;code&gt;num_tokens &amp;gt;= replicas_to_aggregate - total_num_replicas&lt;/code&gt;.</source>
          <target state="translated">假定此操作将在Chief / sync线程的开头执行，这样，即使total_num_replicas小于copys_to_aggregate，该模型仍可以继续进行，因为副本可以为每个变量更新计算多个步骤。确保： &lt;code&gt;num_tokens &amp;gt;= replicas_to_aggregate - total_num_replicas&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b0eb2e902e8ddf903b8c32989be7bc5b84650328" translate="yes" xml:space="preserve">
          <source>This is the Python 2.x counterpart to &lt;code&gt;__bool__()&lt;/code&gt; above.</source>
          <target state="translated">这是上面 &lt;code&gt;__bool__()&lt;/code&gt; 的Python 2.x对应版本。</target>
        </trans-unit>
        <trans-unit id="a177ea545dfd38bd683f27da57349ab86dcbee74" translate="yes" xml:space="preserve">
          <source>This is the V1 version of this layer that uses variable_scope's or partitioner to create variables which works well with PartitionedVariables. Variable scopes are deprecated in V2, so the V2 version uses name_scopes instead. But currently that lacks support for partitioned variables. Use this if you need partitioned variables. Use the partitioner argument if you have a Keras model and uses &lt;a href=&quot;../estimator/model_to_estimator&quot;&gt;&lt;code&gt;tf.compat.v1.keras.estimator.model_to_estimator&lt;/code&gt;&lt;/a&gt; for training.</source>
          <target state="translated">这是此层的V1版本，它使用variable_scope的分区程序或分区程序来创建与PartitionedVariables一起使用的变量。在V2中不推荐使用变量作用域，因此V2版本使用name_scopes代替。但是目前尚缺乏对分区变量的支持。如果需要分区变量，请使用此选项。如果您有&lt;a href=&quot;../estimator/model_to_estimator&quot;&gt; &lt;code&gt;tf.compat.v1.keras.estimator.model_to_estimator&lt;/code&gt; &lt;/a&gt;模型并使用tf.compat.v1.keras.estimator.model_to_estimator进行训练，请使用partitioner参数。</target>
        </trans-unit>
        <trans-unit id="8d8b6a8f2029ea1fc60a937a0c43797bc06e4fc9" translate="yes" xml:space="preserve">
          <source>This is the V1 version of this layer that uses variable_scope's to create variables which works well with PartitionedVariables. Variable scopes are deprecated in V2, so the V2 version uses name_scopes instead. But currently that lacks support for partitioned variables. Use this if you need partitioned variables.</source>
          <target state="translated">这是该层的V1版本,它使用variable_scope的来创建变量,这与PartitionedVariables配合得很好。变量作用域在V2中被废弃了,所以V2版本使用name_scopes代替。但目前这缺乏对分区变量的支持。如果你需要分区变量,请使用这个。</target>
        </trans-unit>
        <trans-unit id="f355bc341013b9a7925572a2dcb6ac68143ef849" translate="yes" xml:space="preserve">
          <source>This is the V2 version of this layer that uses name_scopes to create variables instead of variable_scopes. But this approach currently lacks support for partitioned variables. In that case, use the V1 version instead.</source>
          <target state="translated">这是该层的V2版本,使用name_scopes来创建变量,而不是variable_scopes。但是这种方法目前缺乏对分区变量的支持。在这种情况下,请使用V1版本代替。</target>
        </trans-unit>
        <trans-unit id="45048f38a2e249e88fc3cdaca3ce8480b40e2329" translate="yes" xml:space="preserve">
          <source>This is the angle ( \theta \in [-\pi, \pi] ) such that [ x = r \cos(\theta) ] and [ y = r \sin(\theta) ] where (r = \sqrt(x^2 + y^2) ).</source>
          <target state="translated">这是角(\theta \in [-\pi,\pi]),使[x=r \cos(\theta)]和[y=r \sin(\theta)]其中(r=\sqrt(x^2+y^2))。</target>
        </trans-unit>
        <trans-unit id="d2e50218603262fb1a544e117d169f38db78b601" translate="yes" xml:space="preserve">
          <source>This is the base class for implementing RNN cells with custom behavior.</source>
          <target state="translated">这是实现具有自定义行为的RNN单元的基础类。</target>
        </trans-unit>
        <trans-unit id="9e013372239f70ba2378635884ba4bb573e7d626" translate="yes" xml:space="preserve">
          <source>This is the class from which all layers inherit.</source>
          <target state="translated">这是所有层继承的类。</target>
        </trans-unit>
        <trans-unit id="ff34740f3f60fdf70d3491fdb28647e761a272fb" translate="yes" xml:space="preserve">
          <source>This is the correct way to perform gradient clipping (Pascanu et al., 2012).</source>
          <target state="translated">这是执行梯度剪裁的正确方法(Pascanu等人,2012)。</target>
        </trans-unit>
        <trans-unit id="8fa4ff7f2578477f255b6f02a47ae45758b90edc" translate="yes" xml:space="preserve">
          <source>This is the correct way to perform gradient clipping (for example, see &lt;a href=&quot;http://arxiv.org/abs/1211.5063&quot;&gt;Pascanu et al., 2012&lt;/a&gt; (&lt;a href=&quot;http://arxiv.org/pdf/1211.5063.pdf&quot;&gt;pdf&lt;/a&gt;)).</source>
          <target state="translated">这是执行梯度裁剪的正确方法（例如，请参见&lt;a href=&quot;http://arxiv.org/abs/1211.5063&quot;&gt;Pascanu等人，2012&lt;/a&gt;（&lt;a href=&quot;http://arxiv.org/pdf/1211.5063.pdf&quot;&gt;pdf&lt;/a&gt;））。</target>
        </trans-unit>
        <trans-unit id="76694ede23dda167a78335c0c1bd3e85284fad75" translate="yes" xml:space="preserve">
          <source>This is the crossentropy metric class to be used when there are multiple label classes (2 or more). Here we assume that labels are given as a &lt;code&gt;one_hot&lt;/code&gt; representation. eg., When labels values are [2, 0, 1], &lt;code&gt;y_true&lt;/code&gt; = [[0, 0, 1], [1, 0, 0], [0, 1, 0]].</source>
          <target state="translated">当存在多个标签类（2个或更多）时，将使用交叉熵度量类。在这里，我们假设标签以 &lt;code&gt;one_hot&lt;/code&gt; 表示形式给出。例如，当标签值为[ &lt;code&gt;y_true&lt;/code&gt; ]时，y_true = [[ 0，0，1 ]，[ 1，0，0 ]，[ 0，1，0 ]]。</target>
        </trans-unit>
        <trans-unit id="d187cef3999e110bfd14733d1bc7141db46b4d0d" translate="yes" xml:space="preserve">
          <source>This is the crossentropy metric class to be used when there are only two label classes (0 and 1).</source>
          <target state="translated">这是只有两个标签类(0和1)时要使用的交叉熵度量类。</target>
        </trans-unit>
        <trans-unit id="fb90fff0e52ece653003dbcf512572b6437c5795" translate="yes" xml:space="preserve">
          <source>This is the dtype layers will create their variables in, unless a layer explicitly chooses a different dtype. If this is different than &lt;a href=&quot;policy#compute_dtype&quot;&gt;&lt;code&gt;Policy.compute_dtype&lt;/code&gt;&lt;/a&gt;, Layers will cast variables to the compute dtype to avoid type errors.</source>
          <target state="translated">这是dtype层将在其中创建变量的方法，除非某个层明确选择其他dtype。如果这与&lt;a href=&quot;policy#compute_dtype&quot;&gt; &lt;code&gt;Policy.compute_dtype&lt;/code&gt; &lt;/a&gt;不同，则Layers会将变量强制转换为compute dtype，以避免类型错误。</target>
        </trans-unit>
        <trans-unit id="08b035fd1ef991c0ac17661834339a93b3d11556" translate="yes" xml:space="preserve">
          <source>This is the dtype layers will do their computations in.</source>
          <target state="translated">这是dtype层进行计算的地方。</target>
        </trans-unit>
        <trans-unit id="5e888c610bf02b0ac69c58288afbe898eebab389" translate="yes" xml:space="preserve">
          <source>This is the first part of &lt;code&gt;minimize()&lt;/code&gt;. It returns a list of (gradient, variable) pairs where &quot;gradient&quot; is the gradient for &quot;variable&quot;. Note that &quot;gradient&quot; can be a &lt;code&gt;Tensor&lt;/code&gt;, an &lt;code&gt;IndexedSlices&lt;/code&gt;, or &lt;code&gt;None&lt;/code&gt; if there is no gradient for the given variable.</source>
          <target state="translated">这是 &lt;code&gt;minimize()&lt;/code&gt; 的第一部分。它返回（梯度，变量）对的列表，其中&amp;ldquo; gradient&amp;rdquo;是&amp;ldquo; variable&amp;rdquo;的梯度。请注意，如果给定变量没有梯度，则&amp;ldquo; gradient&amp;rdquo;可以是 &lt;code&gt;Tensor&lt;/code&gt; ， &lt;code&gt;IndexedSlices&lt;/code&gt; 或 &lt;code&gt;None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d316c810415832d0f55adcb10b604f9c47cbd2e8" translate="yes" xml:space="preserve">
          <source>This is the number of train steps running in TPU system before returning to CPU host for each &lt;code&gt;Session.run&lt;/code&gt;. This means global step is increased &lt;code&gt;iterations_per_loop&lt;/code&gt; times in one &lt;code&gt;Session.run&lt;/code&gt;. It is recommended to be set as number of global steps for next checkpoint. Note that in evaluation don't use this value, instead we run total eval &lt;code&gt;steps&lt;/code&gt; on TPU for a single &lt;code&gt;Session.run&lt;/code&gt;. [Experimental]: &lt;code&gt;iterations_per_loop&lt;/code&gt; can be specified as a time interval. To specify N seconds in one &lt;code&gt;Session.run&lt;/code&gt;, one can specify it as &lt;code&gt;Ns&lt;/code&gt; and substitute the N with the N with the number of desired seconds. Alternatively, the unit of time can also be specified in minutes or hours, e.g. &lt;code&gt;3600s&lt;/code&gt; or &lt;code&gt;60m&lt;/code&gt; or &lt;code&gt;1h&lt;/code&gt;.</source>
          <target state="translated">这是在每个 &lt;code&gt;Session.run&lt;/code&gt; 返回CPU主机之前，在TPU系统中运行的训练步骤的数量。这意味着全局步长在一个 &lt;code&gt;Session.run&lt;/code&gt; 中增加了 &lt;code&gt;iterations_per_loop&lt;/code&gt; 次数。建议将其设置为下一个检查点的全局步骤数。请注意，在评估中不要使用该值，相反，我们在TPU上为单个 &lt;code&gt;Session.run&lt;/code&gt; 运行总评估 &lt;code&gt;steps&lt;/code&gt; 。 [实验]： &lt;code&gt;iterations_per_loop&lt;/code&gt; 可以指定为时间间隔。要在一个 &lt;code&gt;Session.run&lt;/code&gt; 中指定N秒，可以将其指定为 &lt;code&gt;Ns&lt;/code&gt; 并用所需秒数的N替换N。或者，时间单位也可以以分钟或小时为单位，例如 &lt;code&gt;3600s&lt;/code&gt; 或 &lt;code&gt;60m&lt;/code&gt; 或 &lt;code&gt;1h&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="a40023f76ab90114029e1bcc91749b2e4391f638" translate="yes" xml:space="preserve">
          <source>This is the opposite of &lt;code&gt;pack&lt;/code&gt;.</source>
          <target state="translated">这与 &lt;code&gt;pack&lt;/code&gt; 相反。</target>
        </trans-unit>
        <trans-unit id="c3ea6e7a75310a4a77e1b1f56fba56b3219204d9" translate="yes" xml:space="preserve">
          <source>This is the opposite of &lt;code&gt;unpack&lt;/code&gt;.</source>
          <target state="translated">这与 &lt;code&gt;unpack&lt;/code&gt; 相反。</target>
        </trans-unit>
        <trans-unit id="b03c235347a9cd23cdcbcc99319938e9531d2ad0" translate="yes" xml:space="preserve">
          <source>This is the opposite of stack.</source>
          <target state="translated">这与堆栈相反。</target>
        </trans-unit>
        <trans-unit id="2e4bca768b327e659a826ca7018b5b658cdefd12" translate="yes" xml:space="preserve">
          <source>This is the opposite of unstack. The numpy equivalent is</source>
          <target state="translated">这与unstack相反。numpy的等价物是</target>
        </trans-unit>
        <trans-unit id="40e4198c7947cb171a27f0f2b96b5e41f088c377" translate="yes" xml:space="preserve">
          <source>This is the opposite of unstack. The numpy equivalent is &lt;code&gt;np.stack&lt;/code&gt;</source>
          <target state="translated">这与取消堆叠相反。numpy等效为 &lt;code&gt;np.stack&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="97cb38281f3bf589012ea6423fb54fdf27897538" translate="yes" xml:space="preserve">
          <source>This is the recommended way to check if a checkpoint exists, since it takes into account the naming difference between V1 and V2 formats.</source>
          <target state="translated">这是检查检查点是否存在的推荐方法,因为它考虑到了V1和V2格式之间的命名差异。</target>
        </trans-unit>
        <trans-unit id="1ffa5c6451bfed59ad42e7f927c856cf80634c8e" translate="yes" xml:space="preserve">
          <source>This is the recommended way to get the mtimes, since it takes into account the naming difference between V1 and V2 formats.</source>
          <target state="translated">这是推荐的获取mtimes的方法,因为它考虑到了V1和V2格式之间的命名差异。</target>
        </trans-unit>
        <trans-unit id="e10b2d35c35f3fc21ee9a235cd260123a0e63325" translate="yes" xml:space="preserve">
          <source>This is the same as the number of Read executions that have succeeded.</source>
          <target state="translated">这与成功的Read执行次数相同。</target>
        </trans-unit>
        <trans-unit id="85a191debc0e75e9aae4867ba1a3abb350e8cdbd" translate="yes" xml:space="preserve">
          <source>This is the same as the number of ReaderRead executions that have succeeded.</source>
          <target state="translated">这与ReaderRead执行成功的次数相同。</target>
        </trans-unit>
        <trans-unit id="f339ca988bca6265b32a0422f47f6bd298db3ffc" translate="yes" xml:space="preserve">
          <source>This is the second part of &lt;code&gt;minimize()&lt;/code&gt;. It returns an &lt;code&gt;Operation&lt;/code&gt; that applies gradients.</source>
          <target state="translated">这是 &lt;code&gt;minimize()&lt;/code&gt; 的第二部分。它返回一个应用渐变的 &lt;code&gt;Operation&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="929a719e4fd77bb16e29532f382d40f922f3c1e7" translate="yes" xml:space="preserve">
          <source>This is the second part of &lt;code&gt;minimize()&lt;/code&gt;. It returns an &lt;code&gt;Operation&lt;/code&gt; that conditionally applies gradients if all gradient values are finite. Otherwise no update is performed (nor is &lt;code&gt;global_step&lt;/code&gt; incremented).</source>
          <target state="translated">这是 &lt;code&gt;minimize()&lt;/code&gt; 的第二部分。如果所有梯度值都是有限的，它将返回一个有条件地应用梯度的 &lt;code&gt;Operation&lt;/code&gt; 。否则，将不会执行任何更新（也不会增加 &lt;code&gt;global_step&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="e8ee3ea42d612b02e009f45c4fa84d1cc896a5de" translate="yes" xml:space="preserve">
          <source>This is true if the variable dtype is not the same as the compute dtype.</source>
          <target state="translated">如果变量dtype与compute dtype不一样,则为真。</target>
        </trans-unit>
        <trans-unit id="91423d96c0eed87b88e95950664bbf721548ff0b" translate="yes" xml:space="preserve">
          <source>This is typically used by gradient computations for a broadcasting operation.</source>
          <target state="translated">这通常被广播操作的梯度计算所使用。</target>
        </trans-unit>
        <trans-unit id="a06d1207d293755800555755a98a415474a76700" translate="yes" xml:space="preserve">
          <source>This is typically used by gradient computations for a concat operation.</source>
          <target state="translated">这通常被梯度计算用于协整运算。</target>
        </trans-unit>
        <trans-unit id="599e1941b7c0f4f9a0022f3bce79b85ba3adcf41" translate="yes" xml:space="preserve">
          <source>This is typically used to create the weights of &lt;code&gt;Layer&lt;/code&gt; subclasses.</source>
          <target state="translated">通常用于创建 &lt;code&gt;Layer&lt;/code&gt; 子类的权重。</target>
        </trans-unit>
        <trans-unit id="c448d69e13a9acb17ee778d9369e5a1d63cab697" translate="yes" xml:space="preserve">
          <source>This is used for converting legacy Theano-saved model files.</source>
          <target state="translated">这用于转换传统的Theano保存的模型文件。</target>
        </trans-unit>
        <trans-unit id="05a2eba266c663d9424df49b8db51e4cc8c2df9e" translate="yes" xml:space="preserve">
          <source>This is used only for TfLite, it provides hints and it also makes the variables in the desired for the tflite ops (transposed and seaparated).</source>
          <target state="translated">这只用于TfLite,它提供了提示,它也使变量在所需的tflite操作(转置和seaparated)。</target>
        </trans-unit>
        <trans-unit id="48e3efb832cfd45bd13919fdfde1ca19835cc159" translate="yes" xml:space="preserve">
          <source>This is used only for TfLite, it provides hints and it also makes the variables in the desired for the tflite ops (transposed and separated).</source>
          <target state="translated">这只用于TfLite,它提供了提示,也使tflite操作所需的变量(转置和分离)。</target>
        </trans-unit>
        <trans-unit id="ad15eb7677663d9a7242cb23213706bc03ee0734" translate="yes" xml:space="preserve">
          <source>This is used only for TfLite, it provides hints and it also makes the variables in the desired for the tflite ops.</source>
          <target state="translated">这只用于TfLite,它提供了提示,也使tflite操作所需的变量。</target>
        </trans-unit>
        <trans-unit id="aa63c8ad524a0fffe274fd0f59f47f9f37029b1b" translate="yes" xml:space="preserve">
          <source>This is used to convert from a TensorFlow GraphDef, SavedModel or tf.keras model into either a TFLite FlatBuffer or graph visualization.</source>
          <target state="translated">这用于将TensorFlow GraphDef、SavedModel或tf.keras模型转换为TFLite FlatBuffer或图形可视化。</target>
        </trans-unit>
        <trans-unit id="42aa887e1a89f1c6a2cebd74bb895f76b927be77" translate="yes" xml:space="preserve">
          <source>This is used to decide whether loss should be scaled in optimizer (used only for estimator + v1 optimizer use case).</source>
          <target state="translated">这是用来决定是否应该在优化器中对损失进行缩放(仅用于估计器+v1优化器的用例)。</target>
        </trans-unit>
        <trans-unit id="b17cf82d54ada528ca921a0efa90b52815fa4b91" translate="yes" xml:space="preserve">
          <source>This is used to prepare for toco conversion of complex intrinsic usages. Note: only one of session or graph_def should be used, not both.</source>
          <target state="translated">这是为toco转换复杂的固有用法做准备。注意:session或graph_def中只能使用一个,不能同时使用。</target>
        </trans-unit>
        <trans-unit id="c7bb1bcc7e14a92bfe2b3cdc7e1ae060a3c52e44" translate="yes" xml:space="preserve">
          <source>This is useful any time you want to compute a value with TensorFlow but need to pretend that the value was a constant. Some examples include:</source>
          <target state="translated">在任何时候,当你想用TensorFlow计算一个值,但需要假装这个值是一个常数时,这个功能都很有用。一些例子包括</target>
        </trans-unit>
        <trans-unit id="615f9e8a59ff4f1c3fc7f3ec9b1dd3650508efec" translate="yes" xml:space="preserve">
          <source>This is useful as a placeholder in code that expects a context manager.</source>
          <target state="translated">这在需要上下文管理器的代码中作为占位符很有用。</target>
        </trans-unit>
        <trans-unit id="150d8f70445d33c34b068178e2af6c26bf08bf36" translate="yes" xml:space="preserve">
          <source>This is useful for debugging and providing early errors. For example, when tracing a &lt;a href=&quot;function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;, no ops are being executed, shapes may be unknown (See the &lt;a href=&quot;https://www.tensorflow.org/guide/concrete_function&quot;&gt;Concrete Functions Guide&lt;/a&gt; for details).</source>
          <target state="translated">这对于调试和提供早期错误很有用。例如，当追踪&lt;a href=&quot;function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; 时&lt;/a&gt;，没有执行任何操作，形状可能是未知的（有关详细信息，请参见《&lt;a href=&quot;https://www.tensorflow.org/guide/concrete_function&quot;&gt;具体功能指南&lt;/a&gt;》）。</target>
        </trans-unit>
        <trans-unit id="b9563699660ae889442bbf807a27eb9beba27100" translate="yes" xml:space="preserve">
          <source>This is useful for separating training updates and state updates, e.g. when we need to update a layer's internal state during prediction.</source>
          <target state="translated">这对于分离训练更新和状态更新很有用,例如当我们在预测过程中需要更新一个层的内部状态时。</target>
        </trans-unit>
        <trans-unit id="946e89dc12a7c9a5e1ede20576867301b896367c" translate="yes" xml:space="preserve">
          <source>This is useful for sequence tasks in which the elements have variable length. Grouping together elements that have similar lengths reduces the total fraction of padding in a batch which increases training step efficiency.</source>
          <target state="translated">这对于元素具有可变长度的序列任务很有用。将长度相近的元素组合在一起,可以减少批次中填充的总分数,从而提高训练步骤的效率。</target>
        </trans-unit>
        <trans-unit id="750a87642623af70551e01a6394d91315e9bfbb7" translate="yes" xml:space="preserve">
          <source>This is useful if you don't want to exit the context manager for the tape, or can't because the desired reset point is inside a control flow construct:</source>
          <target state="translated">如果你不想退出磁带的上下文管理器,或者因为所需的复位点在控制流结构内而不能退出,这就很有用。</target>
        </trans-unit>
        <trans-unit id="1c4209d707783ab42b6537361cd0e771e2838f9d" translate="yes" xml:space="preserve">
          <source>This is useful in summaries to measure and report sparsity. For example,</source>
          <target state="translated">这在摘要中很有用,可以衡量和报告疏密程度。例如:</target>
        </trans-unit>
        <trans-unit id="af423eb943d44142b4ba2df380bb45f9370ab357" translate="yes" xml:space="preserve">
          <source>This is useful to eliminate per-test boilerplate when context managers are used. For example, instead of decorating every test with &lt;code&gt;@mock.patch&lt;/code&gt;, simply do &lt;code&gt;self.foo = self.enter_context(mock.patch(...))' in&lt;/code&gt;setUp()`.</source>
          <target state="translated">使用上下文管理器时，这对于消除每次测试样板非常有用。例如，不用用 &lt;code&gt;@mock.patch&lt;/code&gt; 装饰每个测试，只需 &lt;code&gt;self.foo = self.enter_context(mock.patch(...))' in&lt;/code&gt; setUp（）`中执行self.foo = self.enter_context（mock.patch（...））'。</target>
        </trans-unit>
        <trans-unit id="2f67d0f15d4bd847d95e4812f46c68a7b2ce669c" translate="yes" xml:space="preserve">
          <source>This is useful to mitigate overfitting (you could see it as a form of random data augmentation). Gaussian Noise (GS) is a natural choice as corruption process for real valued inputs.</source>
          <target state="translated">这对于缓解过拟合是很有用的(你可以把它看作是随机数据增强的一种形式)。高斯噪声(GS)作为实值输入的腐败过程是一个自然的选择。</target>
        </trans-unit>
        <trans-unit id="0a10808c724fd5007563891cae49997723f67e26" translate="yes" xml:space="preserve">
          <source>This is useful when starting a dedicated dispatch process.</source>
          <target state="translated">这在启动专用调度流程时很有用。</target>
        </trans-unit>
        <trans-unit id="16e0781bd49ec3a98f90bdf48daf84789ac4527c" translate="yes" xml:space="preserve">
          <source>This is useful when starting a dedicated worker process.</source>
          <target state="translated">这在启动专工流程时很有用。</target>
        </trans-unit>
        <trans-unit id="d38a2f7346a2a0766d59c6174961d13ece4817fb" translate="yes" xml:space="preserve">
          <source>This is useful when validating the result of a broadcasting operation when the tensors do not have statically known shapes.</source>
          <target state="translated">在验证广播操作的结果时,当张力器没有静态已知形状时,这很有用。</target>
        </trans-unit>
        <trans-unit id="50937613d6e3934b243fbeba16769c301a7b4618" translate="yes" xml:space="preserve">
          <source>This is useful when validating the result of a broadcasting operation when the tensors have statically known shapes.</source>
          <target state="translated">这在验证广播操作的结果时很有用,因为张力器的形状是静态已知的。</target>
        </trans-unit>
        <trans-unit id="d56f48520fc92b32fe411b251279af631392c0d0" translate="yes" xml:space="preserve">
          <source>This is useful when you need to extract a subset of slices in an &lt;code&gt;IndexedSlices&lt;/code&gt; object.</source>
          <target state="translated">当您需要在 &lt;code&gt;IndexedSlices&lt;/code&gt; 对象中提取切片的子集时，这很有用。</target>
        </trans-unit>
        <trans-unit id="7324163610c415fe69e9edf14d1514614e0096c2" translate="yes" xml:space="preserve">
          <source>This is where the layer's logic lives.</source>
          <target state="translated">这就是该层的逻辑所在。</target>
        </trans-unit>
        <trans-unit id="51b266b637bd972731e9b8ded931808387760c03" translate="yes" xml:space="preserve">
          <source>This iterator-constructing method can be used to create an iterator that is reusable with many different datasets.</source>
          <target state="translated">这个迭代器构造方法可以用来创建一个可重复使用许多不同数据集的迭代器。</target>
        </trans-unit>
        <trans-unit id="e4b84ee23970a9ba1fae231b7f3281c7ddfd3c7d" translate="yes" xml:space="preserve">
          <source>This kernel op implements the following mathematical equations:</source>
          <target state="translated">这个内核运算实现了以下数学公式。</target>
        </trans-unit>
        <trans-unit id="6651501cdbb63552c7881c732a891c9b596e4cd4" translate="yes" xml:space="preserve">
          <source>This layer can add rows and columns of zeros at the top, bottom, left and right side of an image tensor.</source>
          <target state="translated">该层可以在图像张量的顶部、底部、左侧和右侧添加行和列的零。</target>
        </trans-unit>
        <trans-unit id="c8f7de7a90e40a1acb832cfeb3e997b1fdd42909" translate="yes" xml:space="preserve">
          <source>This layer can be called multiple times with different features.</source>
          <target state="translated">这一层可以多次调用不同的功能。</target>
        </trans-unit>
        <trans-unit id="b23ebdf3426b29c2bb4017b933209c527b0b2eaf" translate="yes" xml:space="preserve">
          <source>This layer can only be used as the first layer in a model.</source>
          <target state="translated">此层只能作为模型中的第一层。</target>
        </trans-unit>
        <trans-unit id="8d2bf436fb7b0b2292ba74c82ed9e0b6359c6446" translate="yes" xml:space="preserve">
          <source>This layer can perform einsum calculations of arbitrary dimensionality.</source>
          <target state="translated">该层可以进行任意维度的einsum计算。</target>
        </trans-unit>
        <trans-unit id="28df12740051102a73475796ca5f477471a88b56" translate="yes" xml:space="preserve">
          <source>This layer concatenates multiple categorical inputs into a single categorical output (similar to Cartesian product). The output dtype is string.</source>
          <target state="translated">该层将多个分类输入连接成一个分类输出(类似于笛卡尔积)。输出dtype是字符串。</target>
        </trans-unit>
        <trans-unit id="fbd759fae81bda98338ee6eb24eac5b449a08594" translate="yes" xml:space="preserve">
          <source>This layer creates a convolution kernel that is convolved (actually cross-correlated) with the layer input to produce a tensor of outputs. If &lt;code&gt;use_bias&lt;/code&gt; is True (and a &lt;code&gt;bias_initializer&lt;/code&gt; is provided), a bias vector is created and added to the outputs. Finally, if &lt;code&gt;activation&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt;, it is applied to the outputs as well.</source>
          <target state="translated">该层创建一个卷积内核，该卷积内核与该层输入进行卷积（实际上是互相关）以产生输出张量。如果 &lt;code&gt;use_bias&lt;/code&gt; 为True（并提供了 &lt;code&gt;bias_initializer&lt;/code&gt; ），则会创建一个偏差向量并将其添加到输出中。最后，如果 &lt;code&gt;activation&lt;/code&gt; 不是 &lt;code&gt;None&lt;/code&gt; ，它也将应用于输出。</target>
        </trans-unit>
        <trans-unit id="a9977284bd794456f41f5d8f8d36aed1d3801d36" translate="yes" xml:space="preserve">
          <source>This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. If &lt;code&gt;use_bias&lt;/code&gt; is True, a bias vector is created and added to the outputs. Finally, if &lt;code&gt;activation&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt;, it is applied to the outputs as well.</source>
          <target state="translated">该层创建一个卷积核，该卷积核与该层输入在单个空间（或时间）维度上进行卷积，以生成输出张量。如果 &lt;code&gt;use_bias&lt;/code&gt; 为True，则会创建一个偏差矢量并将其添加到输出中。最后，如果 &lt;code&gt;activation&lt;/code&gt; 不是 &lt;code&gt;None&lt;/code&gt; ，它也将应用于输出。</target>
        </trans-unit>
        <trans-unit id="d01bf01ba17a42cbf8ad8412a4cd7bfe52f00c78" translate="yes" xml:space="preserve">
          <source>This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If &lt;code&gt;use_bias&lt;/code&gt; is True, a bias vector is created and added to the outputs. Finally, if &lt;code&gt;activation&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt;, it is applied to the outputs as well.</source>
          <target state="translated">该层创建一个卷积内核，该卷积内核与该层输入进行卷积以产生输出张量。如果 &lt;code&gt;use_bias&lt;/code&gt; 为True，则会创建一个偏差矢量并将其添加到输出中。最后，如果 &lt;code&gt;activation&lt;/code&gt; 不是 &lt;code&gt;None&lt;/code&gt; ，它也将应用于输出。</target>
        </trans-unit>
        <trans-unit id="884d29443e655c08f257cc412193f76628fd846b" translate="yes" xml:space="preserve">
          <source>This layer has basic options for managing text in a Keras model. It transforms a batch of strings (one sample = one string) into either a list of token indices (one sample = 1D tensor of integer token indices) or a dense representation (one sample = 1D tensor of float values representing data about the sample's tokens).</source>
          <target state="translated">该层具有管理Keras模型中文本的基本选项。它将一批字符串(一个样本=一个字符串)转换为标记索引列表(一个样本=整数标记索引的1D张量)或密集表示(一个样本=代表样本标记数据的浮点数的1D张量)。</target>
        </trans-unit>
        <trans-unit id="0b7b099051eae116bfa4a2ca036f64c30b769c6d" translate="yes" xml:space="preserve">
          <source>This layer implements a mapping from input space to a space with &lt;code&gt;output_dim&lt;/code&gt; dimensions, which approximates shift-invariant kernels. A kernel function &lt;code&gt;K(x, y)&lt;/code&gt; is shift-invariant if &lt;code&gt;K(x, y) == k(x - y)&lt;/code&gt; for some function &lt;code&gt;k&lt;/code&gt;. Many popular Radial Basis Functions (RBF), including Gaussian and Laplacian kernels, are shift-invariant.</source>
          <target state="translated">该层实现了从输入空间到具有 &lt;code&gt;output_dim&lt;/code&gt; 维度的空间的映射，该映射近似于不变位移的内核。如果某个函数 &lt;code&gt;k&lt;/code&gt; 的 &lt;code&gt;K(x, y) == k(x - y)&lt;/code&gt; ，则核函数 &lt;code&gt;K(x, y)&lt;/code&gt; 是平移不变的。包括高斯和拉普拉斯内核在内的许多流行的径向基函数（RBF）都是平移不变的。</target>
        </trans-unit>
        <trans-unit id="bca79ddc4abf3d361de2fa2d5bdf3c1e02faac7e" translate="yes" xml:space="preserve">
          <source>This layer implements the operation: &lt;code&gt;outputs = activation(inputs * kernel + bias)&lt;/code&gt; Where &lt;code&gt;activation&lt;/code&gt; is the activation function passed as the &lt;code&gt;activation&lt;/code&gt; argument (if not &lt;code&gt;None&lt;/code&gt;), &lt;code&gt;kernel&lt;/code&gt; is a weights matrix created by the layer, and &lt;code&gt;bias&lt;/code&gt; is a bias vector created by the layer (only if &lt;code&gt;use_bias&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;).</source>
          <target state="translated">该层实现以下操作： &lt;code&gt;outputs = activation(inputs * kernel + bias)&lt;/code&gt; 其中， &lt;code&gt;activation&lt;/code&gt; 是作为 &lt;code&gt;activation&lt;/code&gt; 参数传递的激活函数（如果不是 &lt;code&gt;None&lt;/code&gt; ）， &lt;code&gt;kernel&lt;/code&gt; 是由该层创建的权重矩阵，而 &lt;code&gt;bias&lt;/code&gt; 是创建的偏向矢量按层（仅当 &lt;code&gt;use_bias&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; 时）。</target>
        </trans-unit>
        <trans-unit id="35e71e883bb0be18fa94fd4d0a457376805923f0" translate="yes" xml:space="preserve">
          <source>This layer performs a depthwise convolution that acts separately on channels, followed by a pointwise convolution that mixes channels. If &lt;code&gt;use_bias&lt;/code&gt; is True and a bias initializer is provided, it adds a bias vector to the output. It then optionally applies an activation function to produce the final output.</source>
          <target state="translated">该层执行分别作用于通道的深度卷积，然后是混合通道的逐点卷积。如果 &lt;code&gt;use_bias&lt;/code&gt; 为True并提供了一个偏差初始值设定项，则它将偏差向量添加到输出中。然后，它可选地应用激活函数以产生最终输出。</target>
        </trans-unit>
        <trans-unit id="886b50c9aa90bb61796488bc8efaa41bf9f78825" translate="yes" xml:space="preserve">
          <source>This layer provides options for condensing data into a categorical encoding. It accepts integer values as inputs and outputs a dense representation (one sample = 1-index tensor of float values representing data about the sample's tokens) of those inputs.</source>
          <target state="translated">该层提供了将数据浓缩为分类编码的选项。它接受整数值作为输入,并输出这些输入的密集表示(一个样本=1-index的浮动值张量,表示关于样本的标记的数据)。</target>
        </trans-unit>
        <trans-unit id="47be8fc04afe99661c4ed5ab6b6d7ef9885d2eff" translate="yes" xml:space="preserve">
          <source>This layer provides options for condensing input data into denser representations. It accepts either integer values or strings as inputs, allows users to map those inputs into a contiguous integer space, and outputs either those integer values (one sample = 1D tensor of integer token indices) or a dense representation (one sample = 1D tensor of float values representing data about the sample's tokens).</source>
          <target state="translated">该层提供了将输入数据压缩成更密集表示的选项。它接受整数值或字符串作为输入,允许用户将这些输入映射到一个连续的整数空间中,并输出这些整数值(一个样本=整数标记指数的1D张量)或密集表示(一个样本=代表样本标记数据的浮点数的1D张量)。</target>
        </trans-unit>
        <trans-unit id="344f6b570b33a446c2eb6dc9ce40efd08ad87ee8" translate="yes" xml:space="preserve">
          <source>This layer supports masking for input data with a variable number of timesteps. To introduce masks to your data, use an [tf.keras.layers.Embedding] layer with the &lt;code&gt;mask_zero&lt;/code&gt; parameter set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">该层支持对具有可变数量的时间步长的输入数据进行屏蔽。要将掩码引入数据，请使用 &lt;code&gt;mask_zero&lt;/code&gt; 参数设置为 &lt;code&gt;True&lt;/code&gt; 的[tf.keras.layers.Embedding]层。</target>
        </trans-unit>
        <trans-unit id="f1e7b36e17487480357995623584b1cdfb115de4" translate="yes" xml:space="preserve">
          <source>This layer transforms single or multiple categorical inputs to hashed output. It converts a sequence of int or string to a sequence of int. The stable hash function uses tensorflow::ops::Fingerprint to produce universal output that is consistent across platforms.</source>
          <target state="translated">该层将单个或多个分类输入转换为哈希输出。它将一个int或string的序列转换为int的序列。稳定的哈希函数使用 tensorflow::ops::Fingerprint 产生跨平台一致的通用输出。</target>
        </trans-unit>
        <trans-unit id="d1b7379fe2dc54f750afe7d401d80af36bad9408" translate="yes" xml:space="preserve">
          <source>This layer translates a set of arbitrary integers into an integer output via a table-based lookup, with optional out-of-vocabulary handling.</source>
          <target state="translated">该层通过基于表的查找将一组任意整数转化为整数输出,并可选择进行词汇外的处理。</target>
        </trans-unit>
        <trans-unit id="bbcf3f69a06cbdc5f32560a16983b32541e31aaf" translate="yes" xml:space="preserve">
          <source>This layer translates a set of arbitrary strings into an integer output via a table-based lookup, with optional out-of-vocabulary handling.</source>
          <target state="translated">该层通过基于表格的查找,将一组任意字符串翻译成整数输出,并可选择进行词汇外的处理。</target>
        </trans-unit>
        <trans-unit id="f87526b4a527a8b2c085b4914d68a929b0016554" translate="yes" xml:space="preserve">
          <source>This layer uses &lt;a href=&quot;https://github.com/google/farmhash&quot;&gt;FarmHash64&lt;/a&gt; by default, which provides a consistent hashed output across different platforms and is stable across invocations, regardless of device and context, by mixing the input bits thoroughly.</source>
          <target state="translated">该层默认情况下使用&lt;a href=&quot;https://github.com/google/farmhash&quot;&gt;FarmHash64&lt;/a&gt;，它通过完全混合输入位，在不同平台上提供一致的哈希输出，并且无论设备和上下文如何，在调用中均保持稳定。</target>
        </trans-unit>
        <trans-unit id="951c6b01ab8359114ccb88b8a93d70dd8a953954" translate="yes" xml:space="preserve">
          <source>This layer will coerce its inputs into a distribution centered around 0 with standard deviation 1. It accomplishes this by precomputing the mean and variance of the data, and calling (input-mean)/sqrt(var) at runtime.</source>
          <target state="translated">这一层将把它的输入胁迫到一个以0为中心,标准差为1的分布。它通过预先计算数据的均值和方差,并在运行时调用(input-mean)/sqrt(var)来实现。</target>
        </trans-unit>
        <trans-unit id="fe1b0f3431a8ec7d138b525163f2ad9cf543ffa7" translate="yes" xml:space="preserve">
          <source>This layer will coerce its inputs into a normal distribution centered around 0 with standard deviation 1. It accomplishes this by precomputing the mean and variance of the data, and calling (input-mean)/sqrt(var) at runtime.</source>
          <target state="translated">这一层将把它的输入胁迫成以0为中心、标准差为1的正态分布。它通过预先计算数据的均值和方差,并在运行时调用(input-mean)/sqrt(var)来实现。</target>
        </trans-unit>
        <trans-unit id="bf78e85c045cf32b4df68b19ac70fe5f256ed641" translate="yes" xml:space="preserve">
          <source>This layer will crop all the images in the same batch to the same cropping location. By default, random cropping is only applied during training. At inference time, the images will be first rescaled to preserve the shorter side, and center cropped. If you need to apply random cropping at inference time, set &lt;code&gt;training&lt;/code&gt; to True when calling the layer.</source>
          <target state="translated">该层会将同一批中的所有图像裁剪到相同的裁剪位置。默认情况下，随机裁剪仅在训练期间应用。在推断时，将首先重新缩放图像以保留较短的一面，然后进行中心裁剪。如果需要在推理时应用随机裁剪，请在调用图层时将 &lt;code&gt;training&lt;/code&gt; 设置为True。</target>
        </trans-unit>
        <trans-unit id="bc9e9462205fcd047a049604a7917fbea53a1a7d" translate="yes" xml:space="preserve">
          <source>This layer will flip the images based on the &lt;code&gt;mode&lt;/code&gt; attribute. During inference time, the output will be identical to input. Call the layer with &lt;code&gt;training=True&lt;/code&gt; to flip the input.</source>
          <target state="translated">该层将基于 &lt;code&gt;mode&lt;/code&gt; 属性翻转图像。在推断时间内，输出将与输入相同。使用 &lt;code&gt;training=True&lt;/code&gt; 调用图层以翻转输入。</target>
        </trans-unit>
        <trans-unit id="a3d352f01824e505c14a980e99a24ac05a3a55ba" translate="yes" xml:space="preserve">
          <source>This layer will place each element of its input data into one of several contiguous ranges and output an integer index indicating which range each element was placed in.</source>
          <target state="translated">该层将把输入数据中的每个元素放置到几个连续的范围中,并输出一个整数索引,表示每个元素被放置在哪个范围中。</target>
        </trans-unit>
        <trans-unit id="fbf50d844249a2e7240d57251d949994666050a2" translate="yes" xml:space="preserve">
          <source>This library contains all implementations of ClusterResolvers. ClusterResolvers are a way of specifying cluster information for distributed execution. Built on top of existing &lt;code&gt;ClusterSpec&lt;/code&gt; framework, ClusterResolvers are a way for TensorFlow to communicate with various cluster management systems (e.g. GCE, AWS, etc...).</source>
          <target state="translated">该库包含ClusterResolvers的所有实现。ClusterResolvers是一种为分布式执行指定集群信息的方法。建立在现有 &lt;code&gt;ClusterSpec&lt;/code&gt; 框架之上的ClusterResolvers是TensorFlow与各种集群管理系统（例如GCE，AWS等）进行通信的一种方式。</target>
        </trans-unit>
        <trans-unit id="b6b2d72a41e6a38bd121d03e086a0c141b5bab2a" translate="yes" xml:space="preserve">
          <source>This makes the TensorFlow Lite interpreter accessible in Python. It is possible to use this interpreter in a multithreaded Python environment, but you must be sure to call functions of a particular instance from only one thread at a time. So if you want to have 4 threads running different inferences simultaneously, create an interpreter for each one as thread-local data. Similarly, if you are calling invoke() in one thread on a single interpreter but you want to use tensor() on another thread once it is done, you must use a synchronization primitive between the threads to ensure invoke has returned before calling tensor().</source>
          <target state="translated">这使得TensorFlow Lite解释器可以在Python中使用。可以在多线程的Python环境中使用这个解释器,但必须确保每次只从一个线程调用特定实例的函数。因此,如果你想让4个线程同时运行不同的推理,就为每个线程创建一个解释器作为线程本地数据。同样,如果你在一个线程中调用invoke()在一个解释器上,但你想在另一个线程上使用tensor()完成后,你必须在线程之间使用同步基元,以确保invoke在调用tensor()之前已经返回。</target>
        </trans-unit>
        <trans-unit id="6846fc96931fb6bb57a7f6152c9caf47e33ab2da" translate="yes" xml:space="preserve">
          <source>This makes the summary tag more predictable and consistent for the user.</source>
          <target state="translated">这使得摘要标签对用户来说更具有可预测性和一致性。</target>
        </trans-unit>
        <trans-unit id="ac535677c50e98c6e0ce0364a9b0ce341c3f0a28" translate="yes" xml:space="preserve">
          <source>This matches the behavior of If and While for determining if a tensor counts as true/false for a branch condition.</source>
          <target state="translated">这与If和While的行为相匹配,用于确定一个张量是否为分支条件的真/假。</target>
        </trans-unit>
        <trans-unit id="99fc0b3a4e959624cfdd7190647db3380c4e327c" translate="yes" xml:space="preserve">
          <source>This may be useful for checking HTML output.</source>
          <target state="translated">这可能对检查HTML输出有用。</target>
        </trans-unit>
        <trans-unit id="381d4e4134c335f7e7e4d713df90044462182059" translate="yes" xml:space="preserve">
          <source>This may occur, for example, if an operation receives an input tensor that has an invalid value or shape. For example, the &lt;a href=&quot;../linalg/matmul&quot;&gt;&lt;code&gt;tf.matmul&lt;/code&gt;&lt;/a&gt; op will raise this error if it receives an input that is not a matrix, and the &lt;a href=&quot;../reshape&quot;&gt;&lt;code&gt;tf.reshape&lt;/code&gt;&lt;/a&gt; op will raise this error if the new shape does not match the number of elements in the input tensor.</source>
          <target state="translated">例如，如果某个操作接收到具有无效值或形状的输入张量，则可能会发生这种情况。例如，如果&lt;a href=&quot;../linalg/matmul&quot;&gt; &lt;code&gt;tf.matmul&lt;/code&gt; &lt;/a&gt; op收到的输入不是矩阵，则将引发此错误；如果新形状与输入张量中的元素数量不匹配，则&lt;a href=&quot;../reshape&quot;&gt; &lt;code&gt;tf.reshape&lt;/code&gt; &lt;/a&gt; op将引发此错误。</target>
        </trans-unit>
        <trans-unit id="5c451c9f3d5af265bc5daacf860461f46903194b" translate="yes" xml:space="preserve">
          <source>This may only be used inside &lt;code&gt;self.scope()&lt;/code&gt;.</source>
          <target state="translated">这只能在 &lt;code&gt;self.scope()&lt;/code&gt; 内部使用。</target>
        </trans-unit>
        <trans-unit id="8407349f1434fe18f1d4f684e5cf94ad12755be8" translate="yes" xml:space="preserve">
          <source>This means that the result of matrix multiplication &lt;code&gt;v = Au&lt;/code&gt; has &lt;code&gt;Lth&lt;/code&gt; column given circular convolution between &lt;code&gt;h&lt;/code&gt; with the &lt;code&gt;Lth&lt;/code&gt; column of &lt;code&gt;u&lt;/code&gt;.</source>
          <target state="translated">这意味着矩阵乘积 &lt;code&gt;v = Au&lt;/code&gt; 的结果使 &lt;code&gt;Lth&lt;/code&gt; 列具有 &lt;code&gt;h&lt;/code&gt; 与 &lt;code&gt;u&lt;/code&gt; 的 &lt;code&gt;Lth&lt;/code&gt; 列之间的圆形卷积。</target>
        </trans-unit>
        <trans-unit id="54a591701bef9e5baac7a364b34d92ce1a314391" translate="yes" xml:space="preserve">
          <source>This means the layout when converted and saved as an image is rotated 90 degrees clockwise from a typical spectrogram. Time is descending down the Y axis, and the frequency decreases from left to right.</source>
          <target state="translated">这意味着当转换和保存为图像时,布局与典型的光谱图顺时针旋转90度。时间沿Y轴下降,频率从左到右递减。</target>
        </trans-unit>
        <trans-unit id="7bac1b11600b86163afb8ed50f9d2fd7c08c803e" translate="yes" xml:space="preserve">
          <source>This method allows you to define a &quot;feedable&quot; iterator where you can choose between concrete iterators by feeding a value in a &lt;code&gt;tf.Session.run&lt;/code&gt; call. In that case, &lt;code&gt;string_handle&lt;/code&gt; would be a &lt;a href=&quot;../placeholder&quot;&gt;&lt;code&gt;tf.compat.v1.placeholder&lt;/code&gt;&lt;/a&gt;, and you would feed it with the value of &lt;code&gt;tf.data.Iterator.string_handle&lt;/code&gt; in each step.</source>
          <target state="translated">通过此方法，您可以定义&amp;ldquo;可填充&amp;rdquo;迭代器，在其中可以通过在 &lt;code&gt;tf.Session.run&lt;/code&gt; 调用中输入值来在具体迭代器之间进行选择。在这种情况下， &lt;code&gt;string_handle&lt;/code&gt; 将是一个&lt;a href=&quot;../placeholder&quot;&gt; &lt;code&gt;tf.compat.v1.placeholder&lt;/code&gt; &lt;/a&gt;，您将在每个步骤 &lt;code&gt;tf.data.Iterator.string_handle&lt;/code&gt; 的值提供给它。</target>
        </trans-unit>
        <trans-unit id="3ed29b40d37b37daebb7172d60a5cb7452fb1937" translate="yes" xml:space="preserve">
          <source>This method also allows multi-arity &lt;code&gt;elems&lt;/code&gt; and accumulator. If &lt;code&gt;elems&lt;/code&gt; is a (possibly nested) list or tuple of tensors, then each of these tensors must have a matching first (unpack) dimension. The second argument of &lt;code&gt;fn&lt;/code&gt; must match the structure of &lt;code&gt;elems&lt;/code&gt;.</source>
          <target state="translated">此方法还允许使用多arar &lt;code&gt;elems&lt;/code&gt; 和累加器。如果 &lt;code&gt;elems&lt;/code&gt; 是张量的（可能是嵌套的）列表或元组，则每个张量必须具有匹配的第一（解包）维。 &lt;code&gt;fn&lt;/code&gt; 的第二个参数必须匹配 &lt;code&gt;elems&lt;/code&gt; 的结构。</target>
        </trans-unit>
        <trans-unit id="11d1041ad958b2d24d55eb0ab2e6d83f39085f6b" translate="yes" xml:space="preserve">
          <source>This method also allows multi-arity &lt;code&gt;elems&lt;/code&gt; and output of &lt;code&gt;fn&lt;/code&gt;. If &lt;code&gt;elems&lt;/code&gt; is a (possibly nested) list or tuple of tensors, then each of these tensors must have a matching first (unpack) dimension. The signature of &lt;code&gt;fn&lt;/code&gt; may match the structure of &lt;code&gt;elems&lt;/code&gt;. That is, if &lt;code&gt;elems&lt;/code&gt; is &lt;code&gt;(t1, [t2, t3, [t4, t5]])&lt;/code&gt;, then an appropriate signature for &lt;code&gt;fn&lt;/code&gt; is: &lt;code&gt;fn = lambda (t1, [t2, t3, [t4, t5]]):&lt;/code&gt;.</source>
          <target state="translated">此方法还允许多arar &lt;code&gt;elems&lt;/code&gt; 和 &lt;code&gt;fn&lt;/code&gt; 的输出。如果 &lt;code&gt;elems&lt;/code&gt; 是张量的（可能是嵌套的）列表或元组，则每个张量必须具有匹配的第一（解包）维。 &lt;code&gt;fn&lt;/code&gt; 的签名可能与 &lt;code&gt;elems&lt;/code&gt; 的结构匹配。也就是说，如果 &lt;code&gt;elems&lt;/code&gt; 的是 &lt;code&gt;(t1, [t2, t3, [t4, t5]])&lt;/code&gt; ，然后一个合适的签名 &lt;code&gt;fn&lt;/code&gt; 是： &lt;code&gt;fn = lambda (t1, [t2, t3, [t4, t5]]):&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="766a4862e3d658751f25e8ca0813b9af0856b88d" translate="yes" xml:space="preserve">
          <source>This method behaves differently than self.session(): for performance reasons &lt;code&gt;cached_session&lt;/code&gt; will by default reuse the same session within the same test. The session returned by this function will only be closed at the end of the test (in the TearDown function).</source>
          <target state="translated">此方法的行为与self.session（）不同：出于性能原因， &lt;code&gt;cached_session&lt;/code&gt; 默认情况下将在同一测试中重用同一会话。此函数返回的会话仅在测试结束时（在TearDown函数中）关闭。</target>
        </trans-unit>
        <trans-unit id="c7294d73f60860f63726337f7edc669a47fd434f" translate="yes" xml:space="preserve">
          <source>This method builds a new graph by first calling the &lt;code&gt;serving_input_receiver_fn&lt;/code&gt; to obtain feature &lt;code&gt;Tensor&lt;/code&gt;s, and then calling this &lt;code&gt;Estimator&lt;/code&gt;'s &lt;code&gt;model_fn&lt;/code&gt; to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given &lt;code&gt;export_dir_base&lt;/code&gt;, and writes a &lt;code&gt;SavedModel&lt;/code&gt; into it containing a single &lt;code&gt;tf.MetaGraphDef&lt;/code&gt; saved from this session.</source>
          <target state="translated">该方法通过首先调用 &lt;code&gt;serving_input_receiver_fn&lt;/code&gt; 以获取特征 &lt;code&gt;Tensor&lt;/code&gt; ，然后调用此 &lt;code&gt;Estimator&lt;/code&gt; 的 &lt;code&gt;model_fn&lt;/code&gt; 以基于这些特征生成模型图来构建新图。它将在新的会话中将给定的检查点（或者缺少该检查点）还原到该图中。最后，它在给定的 &lt;code&gt;export_dir_base&lt;/code&gt; 下创建一个带时间戳的导出目录，并向其中写入一个 &lt;code&gt;SavedModel&lt;/code&gt; ，其中包含一个从此会话中保存的 &lt;code&gt;tf.MetaGraphDef&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c6ee7778196b7d2b41326a073f8b56a7c8a9a62d" translate="yes" xml:space="preserve">
          <source>This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's &lt;code&gt;Input&lt;/code&gt;s. These losses become part of the model's topology and are tracked in &lt;code&gt;get_config&lt;/code&gt;.</source>
          <target state="translated">在构造过程中，也可以在功能模型上直接调用此方法。在这种情况下，传递给该模型的任何张量损失必须是符号性的，并且能够追溯到模型的 &lt;code&gt;Input&lt;/code&gt; 。这些损失成为模型拓扑的一部分，并在 &lt;code&gt;get_config&lt;/code&gt; 中进行跟踪。</target>
        </trans-unit>
        <trans-unit id="3e9d9d9b289f8d837ef3c68f7420c8739fcddb82" translate="yes" xml:space="preserve">
          <source>This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's &lt;code&gt;Input&lt;/code&gt;s. These metrics become part of the model's topology and are tracked when you save the model via &lt;code&gt;save()&lt;/code&gt;.</source>
          <target state="translated">在构造过程中，也可以在功能模型上直接调用此方法。在这种情况下，传递给此Model的任何张量都必须是符号性的，并且能够追溯到模型的 &lt;code&gt;Input&lt;/code&gt; 。这些度量标准成为模型拓扑的一部分，并在通过 &lt;code&gt;save()&lt;/code&gt; 保存模型时进行跟踪。</target>
        </trans-unit>
        <trans-unit id="72b1f5316c49fd45d1ca1593424ee77777a4e789" translate="yes" xml:space="preserve">
          <source>This method can be called multiple times, and will merge the given &lt;code&gt;shape&lt;/code&gt; with the current shape of this tensor. It can be used to provide additional information about the shape of this tensor that cannot be inferred from the graph alone. For example, this can be used to provide additional information about the shapes of images:</source>
          <target state="translated">可以多次调用此方法，并将给定 &lt;code&gt;shape&lt;/code&gt; 与该张量的当前形状合并。它可用于提供有关该张量的形状的其他信息，这些信息无法仅通过图形来推断。例如，这可以用于提供有关图像形状的其他信息：</target>
        </trans-unit>
        <trans-unit id="312e4a6b05fc4ebb6ca2b98fbc58b54373467b4c" translate="yes" xml:space="preserve">
          <source>This method can be overridden to support custom evaluation logic. This method is called by &lt;a href=&quot;../model#evaluate&quot;&gt;&lt;code&gt;Model.evaluate&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../model#test_on_batch&quot;&gt;&lt;code&gt;Model.test_on_batch&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以重写此方法以支持自定义评估逻辑。该方法由&lt;a href=&quot;../model#evaluate&quot;&gt; &lt;code&gt;Model.evaluate&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../model#test_on_batch&quot;&gt; &lt;code&gt;Model.test_on_batch&lt;/code&gt; 调用&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="ec15a46dc0420f052e7d9fc2485824bcb8d59546" translate="yes" xml:space="preserve">
          <source>This method can be overridden to support custom evaluation logic. This method is called by &lt;a href=&quot;../model#make_test_function&quot;&gt;&lt;code&gt;Model.make_test_function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以重写此方法以支持自定义评估逻辑。此方法由&lt;a href=&quot;../model#make_test_function&quot;&gt; &lt;code&gt;Model.make_test_function&lt;/code&gt; 调用&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="24b263a87142b52faa1e21dbe53ee04255d04a75" translate="yes" xml:space="preserve">
          <source>This method can be overridden to support custom evaluation logic. This method is called by &lt;a href=&quot;model#evaluate&quot;&gt;&lt;code&gt;Model.evaluate&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;model#test_on_batch&quot;&gt;&lt;code&gt;Model.test_on_batch&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以重写此方法以支持自定义评估逻辑。该方法由&lt;a href=&quot;model#evaluate&quot;&gt; &lt;code&gt;Model.evaluate&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;model#test_on_batch&quot;&gt; &lt;code&gt;Model.test_on_batch&lt;/code&gt; 调用&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="00b8ab943979a68d8e7d563dec5f7eda81840f4f" translate="yes" xml:space="preserve">
          <source>This method can be overridden to support custom evaluation logic. This method is called by &lt;a href=&quot;model#make_test_function&quot;&gt;&lt;code&gt;Model.make_test_function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以重写此方法以支持自定义评估逻辑。此方法由&lt;a href=&quot;model#make_test_function&quot;&gt; &lt;code&gt;Model.make_test_function&lt;/code&gt; 调用&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4740c061fca2f7ba4333f6609ad3d2119d60a6e9" translate="yes" xml:space="preserve">
          <source>This method can be overridden to support custom inference logic. This method is called by &lt;a href=&quot;../model#make_predict_function&quot;&gt;&lt;code&gt;Model.make_predict_function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以重写此方法以支持自定义推理逻辑。此方法由&lt;a href=&quot;../model#make_predict_function&quot;&gt; &lt;code&gt;Model.make_predict_function&lt;/code&gt; 调用&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="9231f9e258188fbf7d82d2b745ff3bd729ba7207" translate="yes" xml:space="preserve">
          <source>This method can be overridden to support custom inference logic. This method is called by &lt;a href=&quot;../model#predict&quot;&gt;&lt;code&gt;Model.predict&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../model#predict_on_batch&quot;&gt;&lt;code&gt;Model.predict_on_batch&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以重写此方法以支持自定义推理逻辑。该方法由&lt;a href=&quot;../model#predict&quot;&gt; &lt;code&gt;Model.predict&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../model#predict_on_batch&quot;&gt; &lt;code&gt;Model.predict_on_batch&lt;/code&gt; 调用&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="d12957750990229d94e78212f50e83f538705ddf" translate="yes" xml:space="preserve">
          <source>This method can be overridden to support custom inference logic. This method is called by &lt;a href=&quot;model#make_predict_function&quot;&gt;&lt;code&gt;Model.make_predict_function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以重写此方法以支持自定义推理逻辑。此方法由&lt;a href=&quot;model#make_predict_function&quot;&gt; &lt;code&gt;Model.make_predict_function&lt;/code&gt; 调用&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4ecd8d8943012dca870b082ac2069dbad2ddff8d" translate="yes" xml:space="preserve">
          <source>This method can be overridden to support custom inference logic. This method is called by &lt;a href=&quot;model#predict&quot;&gt;&lt;code&gt;Model.predict&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;model#predict_on_batch&quot;&gt;&lt;code&gt;Model.predict_on_batch&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以重写此方法以支持自定义推理逻辑。该方法由&lt;a href=&quot;model#predict&quot;&gt; &lt;code&gt;Model.predict&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;model#predict_on_batch&quot;&gt; &lt;code&gt;Model.predict_on_batch&lt;/code&gt; 调用&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="52dfa555331eece2b509081a1f3acc2117560cba" translate="yes" xml:space="preserve">
          <source>This method can be overridden to support custom training logic. This method is called by &lt;a href=&quot;../model#fit&quot;&gt;&lt;code&gt;Model.fit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../model#train_on_batch&quot;&gt;&lt;code&gt;Model.train_on_batch&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以重写此方法以支持自定义训练逻辑。该方法由&lt;a href=&quot;../model#fit&quot;&gt; &lt;code&gt;Model.fit&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../model#train_on_batch&quot;&gt; &lt;code&gt;Model.train_on_batch&lt;/code&gt; 调用&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="655f9f9c1b7ccd7db3e5c665ed02881a9dad1b2f" translate="yes" xml:space="preserve">
          <source>This method can be overridden to support custom training logic. This method is called by &lt;a href=&quot;../model#make_train_function&quot;&gt;&lt;code&gt;Model.make_train_function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以重写此方法以支持自定义训练逻辑。此方法由&lt;a href=&quot;../model#make_train_function&quot;&gt; &lt;code&gt;Model.make_train_function&lt;/code&gt; 调用&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="678799c210cac42289c8d3efd485d53c6086509d" translate="yes" xml:space="preserve">
          <source>This method can be overridden to support custom training logic. This method is called by &lt;a href=&quot;model#fit&quot;&gt;&lt;code&gt;Model.fit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;model#train_on_batch&quot;&gt;&lt;code&gt;Model.train_on_batch&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以重写此方法以支持自定义训练逻辑。该方法由&lt;a href=&quot;model#fit&quot;&gt; &lt;code&gt;Model.fit&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;model#train_on_batch&quot;&gt; &lt;code&gt;Model.train_on_batch&lt;/code&gt; 调用&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="ab33fe36b3e6b63aa5f9d2cddc383b1eeb7bd2db" translate="yes" xml:space="preserve">
          <source>This method can be overridden to support custom training logic. This method is called by &lt;a href=&quot;model#make_train_function&quot;&gt;&lt;code&gt;Model.make_train_function&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以重写此方法以支持自定义训练逻辑。此方法由&lt;a href=&quot;model#make_train_function&quot;&gt; &lt;code&gt;Model.make_train_function&lt;/code&gt; 调用&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="a7f74b2952869b5420047da9ad72bf8f755d938b" translate="yes" xml:space="preserve">
          <source>This method can be used after a call to &lt;a href=&quot;enable_check_numerics&quot;&gt;&lt;code&gt;tf.debugging.enable_check_numerics()&lt;/code&gt;&lt;/a&gt; to disable the numerics-checking mechanism that catches infinity and NaN values output by ops executed eagerly or in tf.function-compiled graphs.</source>
          <target state="translated">可以在调用&lt;a href=&quot;enable_check_numerics&quot;&gt; &lt;code&gt;tf.debugging.enable_check_numerics()&lt;/code&gt; &lt;/a&gt;之后使用此方法，以禁用捕获由急切执行的操作或在tf.function编译图中输出的无穷大和NaN值的数字检查机制。</target>
        </trans-unit>
        <trans-unit id="7667fea48f6e7bf7add0c54e02f390885694a1d8" translate="yes" xml:space="preserve">
          <source>This method can be used after a call to &lt;a href=&quot;enable_check_numerics&quot;&gt;&lt;code&gt;tf.debugging.enable_check_numerics()&lt;/code&gt;&lt;/a&gt; to disable the numerics-checking mechanism that catches inifnity and NaN values output by ops executed eagerly or in tf.function-compiled graphs.</source>
          <target state="translated">可以在调用&lt;a href=&quot;enable_check_numerics&quot;&gt; &lt;code&gt;tf.debugging.enable_check_numerics()&lt;/code&gt; &lt;/a&gt;之后使用此方法，以禁用捕获由急切执行的操作或在tf.function编译图中输出的无穷和NaN值的数字检查机制。</target>
        </trans-unit>
        <trans-unit id="ab134bc99ff08cb5e34c3578ec66ab8ee325437d" translate="yes" xml:space="preserve">
          <source>This method can be used for several purposes. For example, where &lt;code&gt;experimental_distribute_dataset&lt;/code&gt; is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in &lt;code&gt;experimental_distribute_dataset&lt;/code&gt;). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed. &lt;code&gt;experimental_distribute_dataset&lt;/code&gt; may also sometimes fail to split the batch across replicas on a worker. In that case, this method can be used where that limitation does not exist.</source>
          <target state="translated">该方法可以用于多种目的。例如，在 &lt;code&gt;experimental_distribute_dataset&lt;/code&gt; 无法分片输入文件的情况下，可以使用此方法手动分片数据集（避免 &lt;code&gt;experimental_distribute_dataset&lt;/code&gt; 中的缓慢回退行为）。如果数据集是无限的，则可以通过创建仅随机种子不同的数据集副本来完成此分片。 &lt;code&gt;experimental_distribute_dataset&lt;/code&gt; 有时有时也无法将批处理拆分到工作服务器上的副本之间。在那种情况下，可以在不存在限制的情况下使用此方法。</target>
        </trans-unit>
        <trans-unit id="4d445a1a8ccfc13d2cbb47a0cd8852bd14137cba" translate="yes" xml:space="preserve">
          <source>This method can be used inside a subclassed layer or model's &lt;code&gt;call&lt;/code&gt; function, in which case &lt;code&gt;losses&lt;/code&gt; should be a Tensor or list of Tensors.</source>
          <target state="translated">此方法可以在子类图层或模型的 &lt;code&gt;call&lt;/code&gt; 函数中使用，在这种情况下， &lt;code&gt;losses&lt;/code&gt; 应该是张量或张量列表。</target>
        </trans-unit>
        <trans-unit id="1d54c9015867d4032542a340799e853563723710" translate="yes" xml:space="preserve">
          <source>This method can be used inside the &lt;code&gt;call()&lt;/code&gt; method of a subclassed layer or model.</source>
          <target state="translated">可以在子类化图层或模型的 &lt;code&gt;call()&lt;/code&gt; 方法内部使用此方法。</target>
        </trans-unit>
        <trans-unit id="0a08143c632d6d9a6c3bb58abdfe351796aa7d61" translate="yes" xml:space="preserve">
          <source>This method can be used to assert that there exists a shape that both &lt;code&gt;self&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; represent.</source>
          <target state="translated">此方法可用于断言存在一个 &lt;code&gt;self&lt;/code&gt; 和 &lt;code&gt;other&lt;/code&gt; 表示的形状。</target>
        </trans-unit>
        <trans-unit id="311f5dd67cb7916aeab132002e12e94284e8fdb8" translate="yes" xml:space="preserve">
          <source>This method can be used to create &lt;code&gt;RaggedTensor&lt;/code&gt;s with multiple uniform outer dimensions. For example, a &lt;code&gt;RaggedTensor&lt;/code&gt; with shape &lt;code&gt;[2, 2, None]&lt;/code&gt; can be constructed with this method from a &lt;code&gt;RaggedTensor&lt;/code&gt; values with shape &lt;code&gt;[4, None]&lt;/code&gt;:</source>
          <target state="translated">此方法可用于创建具有多个统一外部尺寸的 &lt;code&gt;RaggedTensor&lt;/code&gt; 。例如， &lt;code&gt;RaggedTensor&lt;/code&gt; 具有形状 &lt;code&gt;[2, 2, None]&lt;/code&gt; 可以与来自该方法来构建 &lt;code&gt;RaggedTensor&lt;/code&gt; 值与形状 &lt;code&gt;[4, None]&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="53a52d98446448532e361c5ab0b1a6a4bc35f1f2" translate="yes" xml:space="preserve">
          <source>This method can be used to merge partitions created by &lt;code&gt;dynamic_partition&lt;/code&gt; as illustrated on the following example:</source>
          <target state="translated">此方法可用于合并由 &lt;code&gt;dynamic_partition&lt;/code&gt; 创建的分区，如以下示例所示：</target>
        </trans-unit>
        <trans-unit id="d34cdddad9212e7710a82e27967cd30b8fc5e14d" translate="yes" xml:space="preserve">
          <source>This method can be used to run a step function for training a number of times using input from a dataset.</source>
          <target state="translated">该方法可以使用数据集的输入来运行步进函数进行多次训练。</target>
        </trans-unit>
        <trans-unit id="9c709bdcba30505f75d83241dd8ba3817ec876a1" translate="yes" xml:space="preserve">
          <source>This method currently blocks forever.</source>
          <target state="translated">这种方法目前永远封锁。</target>
        </trans-unit>
        <trans-unit id="0f39f81e0d455fd570ceb61e61e1722fce44270e" translate="yes" xml:space="preserve">
          <source>This method enables setting metadata in a trace event after it is created.</source>
          <target state="translated">此方法可以在创建跟踪事件后,在该事件中设置元数据。</target>
        </trans-unit>
        <trans-unit id="bc0fd4004c973da3102a123f7521a19f6c88fb03" translate="yes" xml:space="preserve">
          <source>This method generalizes to higher-dimensions by simply providing a list for both the sp_ids as well as the vocab_size. In this case the resulting &lt;code&gt;SparseTensor&lt;/code&gt; has the following properties:</source>
          <target state="translated">通过简单地提供sp_id和vocab_size的列表，此方法可以推广到更高维度。在这种情况下，生成的 &lt;code&gt;SparseTensor&lt;/code&gt; 具有以下属性：</target>
        </trans-unit>
        <trans-unit id="a08e7549042177e7c94f4230a7e19c5cac25a2de" translate="yes" xml:space="preserve">
          <source>This method generalizes to higher-dimensions by simply providing a list for both the sp_ids as well as the vocab_size. In this case the resulting &lt;code&gt;SparseTensor&lt;/code&gt; has the following properties: - &lt;code&gt;indices&lt;/code&gt; is equivalent to &lt;code&gt;sp_ids[0].indices&lt;/code&gt; with the last dimension discarded and concatenated with &lt;code&gt;sp_ids[0].values, sp_ids[1].values, ...&lt;/code&gt;. - &lt;code&gt;values&lt;/code&gt; is simply &lt;code&gt;sp_values.values&lt;/code&gt;. - If &lt;code&gt;sp_ids.dense_shape = [D0, D1, ..., Dn, K]&lt;/code&gt;, then &lt;code&gt;output.shape = [D0, D1, ..., Dn] + vocab_size&lt;/code&gt;.</source>
          <target state="translated">通过简单地提供sp_id和vocab_size的列表，此方法可以推广到更高维度。在这种情况下，生成的 &lt;code&gt;SparseTensor&lt;/code&gt; 具有以下属性：- &lt;code&gt;indices&lt;/code&gt; 等效于 &lt;code&gt;sp_ids[0].indices&lt;/code&gt; ，最后一个维度被丢弃并与 &lt;code&gt;sp_ids[0].values, sp_ids[1].values, ...&lt;/code&gt; 。 - &lt;code&gt;values&lt;/code&gt; 仅仅是 &lt;code&gt;sp_values.values&lt;/code&gt; 。 -如果 &lt;code&gt;sp_ids.dense_shape = [D0, D1, ..., Dn, K]&lt;/code&gt; ，则 &lt;code&gt;output.shape = [D0, D1, ..., Dn] + vocab_size&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e7e44aa7677b60789a9ebef54c74a9714ec4de2a" translate="yes" xml:space="preserve">
          <source>This method has similar semantics to the built-in &lt;code&gt;zip()&lt;/code&gt; function in Python, with the main difference being that the &lt;code&gt;datasets&lt;/code&gt; argument can be an arbitrary nested structure of &lt;code&gt;Dataset&lt;/code&gt; objects.</source>
          <target state="translated">此方法的语义与Python 内置的 &lt;code&gt;zip()&lt;/code&gt; 函数相似，主要区别在于 &lt;code&gt;datasets&lt;/code&gt; 参数可以是 &lt;code&gt;Dataset&lt;/code&gt; 对象的任意嵌套结构。</target>
        </trans-unit>
        <trans-unit id="1925370323ce05b6dc0cdc8787e9c0d6ebf8403a" translate="yes" xml:space="preserve">
          <source>This method is a convenience wrapper for creating a &lt;a href=&quot;server&quot;&gt;&lt;code&gt;tf.distribute.Server&lt;/code&gt;&lt;/a&gt; with a &lt;a href=&quot;../train/serverdef&quot;&gt;&lt;code&gt;tf.train.ServerDef&lt;/code&gt;&lt;/a&gt; that specifies a single-process cluster containing a single task in a job called &lt;code&gt;&quot;local&quot;&lt;/code&gt;.</source>
          <target state="translated">此方法是用于使用&lt;a href=&quot;server&quot;&gt; &lt;code&gt;tf.distribute.Server&lt;/code&gt; &lt;/a&gt;创建tf.distribute.Server的便捷包装，该&lt;a href=&quot;../train/serverdef&quot;&gt; &lt;code&gt;tf.train.ServerDef&lt;/code&gt; &lt;/a&gt;指定包含在名为 &lt;code&gt;&quot;local&quot;&lt;/code&gt; 的作业中的单个任务的单进程集群。</target>
        </trans-unit>
        <trans-unit id="05c81c4b69f5f8cdea4eba6f7d8bb00b2d7cd37f" translate="yes" xml:space="preserve">
          <source>This method is automatically called when the StubOutForTesting() object is deleted; there is no need to call it explicitly.</source>
          <target state="translated">当StubOutForTesting()对象被删除时,这个方法会被自动调用,不需要显式调用。</target>
        </trans-unit>
        <trans-unit id="992fe7b39731079182b90eaf498e1ed5e5a20148" translate="yes" xml:space="preserve">
          <source>This method is completely compatible with the &lt;code&gt;tf.Session.run()&lt;/code&gt; method.</source>
          <target state="translated">此方法与 &lt;code&gt;tf.Session.run()&lt;/code&gt; 方法完全兼容。</target>
        </trans-unit>
        <trans-unit id="9074766f6510dcf3975a88d8985dc7d0e99f0a13" translate="yes" xml:space="preserve">
          <source>This method is deprecated, use is_alive() instead.</source>
          <target state="translated">这个方法已经废弃,请使用is_alive()代替。</target>
        </trans-unit>
        <trans-unit id="a744f7951fcb63381ecb2ea91dcc061d08e7d4a0" translate="yes" xml:space="preserve">
          <source>This method is exposed in TensorFlow's API so that library developers can register dispatching for &lt;a href=&quot;tensor#__eq__&quot;&gt;&lt;code&gt;Tensor.&lt;strong&gt;eq&lt;/strong&gt;&lt;/code&gt;&lt;/a&gt; to allow it to handle custom composite tensors &amp;amp; other custom objects.</source>
          <target state="translated">TensorFlow的API中公开了此方法，以便库开发人员可以注册&lt;a href=&quot;tensor#__eq__&quot;&gt; &lt;code&gt;Tensor.&lt;strong&gt;eq&lt;/strong&gt;&lt;/code&gt; &lt;/a&gt;调度。&lt;strong&gt;eq&lt;/strong&gt;以使其能够处理自定义复合张量和其他自定义对象。</target>
        </trans-unit>
        <trans-unit id="8a699437e1c2aa279788c76128e6e5f624c7c0a7" translate="yes" xml:space="preserve">
          <source>This method is exposed in TensorFlow's API so that library developers can register dispatching for &lt;a href=&quot;tensor#__getitem__&quot;&gt;&lt;code&gt;Tensor.&lt;strong&gt;getitem&lt;/strong&gt;&lt;/code&gt;&lt;/a&gt; to allow it to handle custom composite tensors &amp;amp; other custom objects.</source>
          <target state="translated">TensorFlow的API中公开了此方法，以便库开发人员可以注册&lt;a href=&quot;tensor#__getitem__&quot;&gt; &lt;code&gt;Tensor.&lt;strong&gt;getitem&lt;/strong&gt;&lt;/code&gt; &lt;/a&gt;调度。&lt;strong&gt;getitem&lt;/strong&gt;允许它处理自定义复合张量和其他自定义对象。</target>
        </trans-unit>
        <trans-unit id="ca897118350393caa56db53d55d7ff23f3ea66bf" translate="yes" xml:space="preserve">
          <source>This method is exposed in TensorFlow's API so that library developers can register dispatching for &lt;a href=&quot;tensor#__ne__&quot;&gt;&lt;code&gt;Tensor.&lt;strong&gt;ne&lt;/strong&gt;&lt;/code&gt;&lt;/a&gt; to allow it to handle custom composite tensors &amp;amp; other custom objects.</source>
          <target state="translated">TensorFlow的API中公开了此方法，以便库开发人员可以注册&lt;a href=&quot;tensor#__ne__&quot;&gt; &lt;code&gt;Tensor.&lt;strong&gt;ne&lt;/strong&gt;&lt;/code&gt; &lt;/a&gt;调度。&lt;strong&gt;NE&lt;/strong&gt;以允许它处理自定义复合张量与其他自定义对象。</target>
        </trans-unit>
        <trans-unit id="a7dd7ea7386a00c65842a1da48fad6c58e2f712f" translate="yes" xml:space="preserve">
          <source>This method is for use by TestCase subclasses that need to register their own type equality functions to provide nicer error messages.</source>
          <target state="translated">这个方法供TestCase子类使用,这些子类需要注册自己的类型平等函数来提供更好的错误信息。</target>
        </trans-unit>
        <trans-unit id="66283121e6883c956c7613fffa84d220528690f4" translate="yes" xml:space="preserve">
          <source>This method is idempotent. Calling it multiple times has the same effect as calling it once.</source>
          <target state="translated">这个方法是幂等的。多次调用与一次调用的效果相同。</target>
        </trans-unit>
        <trans-unit id="26e25dbe646e6bb109fc819d323736b4d761ed81" translate="yes" xml:space="preserve">
          <source>This method is intended to be used to load checkpoints created by &lt;code&gt;save()&lt;/code&gt;. For checkpoints created by &lt;code&gt;write()&lt;/code&gt; use the &lt;code&gt;read()&lt;/code&gt; method which does not expect the &lt;code&gt;save_counter&lt;/code&gt; variable added by &lt;code&gt;save()&lt;/code&gt;.</source>
          <target state="translated">此方法旨在用于加载 &lt;code&gt;save()&lt;/code&gt; 创建的检查点。对于由 &lt;code&gt;write()&lt;/code&gt; 创建的检查点，请使用 &lt;code&gt;read()&lt;/code&gt; 方法，该方法不希望 &lt;code&gt;save()&lt;/code&gt; 添加了 &lt;code&gt;save_counter&lt;/code&gt; 变量。</target>
        </trans-unit>
        <trans-unit id="887c8b37e88faa620b24a828f5dc104598906d79" translate="yes" xml:space="preserve">
          <source>This method is just like &lt;code&gt;restore()&lt;/code&gt; but does not expect the &lt;code&gt;save_counter&lt;/code&gt; variable in the checkpoint. It only restores the objects that the checkpoint already depends on.</source>
          <target state="translated">此方法与 &lt;code&gt;restore()&lt;/code&gt; 一样,但是不希望在检查点中使用 &lt;code&gt;save_counter&lt;/code&gt; 变量。它仅还原检查点已经依赖的对象。</target>
        </trans-unit>
        <trans-unit id="d1d2fc5aba91c121cb755ded927621c464c711dd" translate="yes" xml:space="preserve">
          <source>This method is only needed if you compute gradients manually, e.g. with &lt;a href=&quot;../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;. In that case, call this method to scale the loss before passing the loss to &lt;a href=&quot;../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;. If you use &lt;a href=&quot;../../optimizers/optimizer#minimize&quot;&gt;&lt;code&gt;LossScaleOptimizer.minimize&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;lossscaleoptimizer#get_gradients&quot;&gt;&lt;code&gt;LossScaleOptimizer.get_gradients&lt;/code&gt;&lt;/a&gt;, loss scaling is automatically applied and this method is unneeded.</source>
          <target state="translated">仅当您手动计算渐变（例如，使用&lt;a href=&quot;../../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;）时才需要此方法。在这种情况下，在将损失传递给&lt;a href=&quot;../../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;之前，请调用此方法缩放损失。如果使用&lt;a href=&quot;../../optimizers/optimizer#minimize&quot;&gt; &lt;code&gt;LossScaleOptimizer.minimize&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;lossscaleoptimizer#get_gradients&quot;&gt; &lt;code&gt;LossScaleOptimizer.get_gradients&lt;/code&gt; &lt;/a&gt;，则会自动应用损失定标，并且不需要此方法。</target>
        </trans-unit>
        <trans-unit id="2ffe062127379160c3973cd4f84270b4f4f8ccf9" translate="yes" xml:space="preserve">
          <source>This method is only needed if you compute gradients manually, e.g. with &lt;a href=&quot;../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;. In that case, call this method to unscale the gradients after computing them with &lt;a href=&quot;../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt;. If you use &lt;a href=&quot;../../optimizers/optimizer#minimize&quot;&gt;&lt;code&gt;LossScaleOptimizer.minimize&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;lossscaleoptimizer#get_gradients&quot;&gt;&lt;code&gt;LossScaleOptimizer.get_gradients&lt;/code&gt;&lt;/a&gt;, loss scaling is automatically applied and this method is unneeded.</source>
          <target state="translated">仅当您手动计算渐变（例如，使用&lt;a href=&quot;../../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;）时才需要此方法。在那种情况下，在使用&lt;a href=&quot;../../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;计算梯度之后，调用此方法以取消梯度的缩放。如果使用&lt;a href=&quot;../../optimizers/optimizer#minimize&quot;&gt; &lt;code&gt;LossScaleOptimizer.minimize&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;lossscaleoptimizer#get_gradients&quot;&gt; &lt;code&gt;LossScaleOptimizer.get_gradients&lt;/code&gt; &lt;/a&gt;，则会自动应用损失定标，并且不需要此方法。</target>
        </trans-unit>
        <trans-unit id="2d15ade5009ad723801c2c48bded790a3845587f" translate="yes" xml:space="preserve">
          <source>This method is optional if you are just training and executing models, exporting to and from SavedModels, or using weight checkpoints.</source>
          <target state="translated">如果您只是训练和执行模型,从SavedModels导出到和从SavedModels导出,或者使用权重检查点,这个方法是可选的。</target>
        </trans-unit>
        <trans-unit id="2ed4e772dabffd01cfde2b74159ef69c9f3e04f5" translate="yes" xml:space="preserve">
          <source>This method is required for Keras &lt;code&gt;model_to_estimator&lt;/code&gt;, saving and loading models to HDF5 formats, Keras model cloning, some visualization utilities, and exporting models to and from JSON.</source>
          <target state="translated">Keras &lt;code&gt;model_to_estimator&lt;/code&gt; ，将模型保存并加载为HDF5格式，Keras模型克隆，某些可视化实用程序以及将模型与JSON导出以及从JSON导出模型均需要此方法。</target>
        </trans-unit>
        <trans-unit id="6fdad9c5ed22f16c319e244d8261ea4ee3d279dd" translate="yes" xml:space="preserve">
          <source>This method is smart and works at the module, class, and instance level while preserving proper inheritance. It will not stub out C types however unless that has been explicitly allowed by the type.</source>
          <target state="translated">这个方法很聪明,在模块、类和实例级别工作,同时保留了适当的继承。然而,它不会将C类型存根化,除非该类型明确允许这样做。</target>
        </trans-unit>
        <trans-unit id="feecb6a599cedcf2305768be36a750d8591e9329" translate="yes" xml:space="preserve">
          <source>This method is the reverse of &lt;code&gt;get_config&lt;/code&gt;, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by &lt;code&gt;set_weights&lt;/code&gt;).</source>
          <target state="translated">此方法与 &lt;code&gt;get_config&lt;/code&gt; 相反，它能够从config字典中实例化同一层。它不处理层连接性（由网络处理），也不处理权重（由 &lt;code&gt;set_weights&lt;/code&gt; 处理）。</target>
        </trans-unit>
        <trans-unit id="422559cd09a8b37855dfa1becf9159996c55a88c" translate="yes" xml:space="preserve">
          <source>This method is the reverse of &lt;code&gt;get_config&lt;/code&gt;, capable of instantiating the same optimizer from the config dictionary.</source>
          <target state="translated">此方法与 &lt;code&gt;get_config&lt;/code&gt; 相反，它能够从config字典中实例化相同的优化器。</target>
        </trans-unit>
        <trans-unit id="c7d6ed78b790787a16f4abeccc0ecd949df6cc2b" translate="yes" xml:space="preserve">
          <source>This method is the reverse of &lt;code&gt;get_config&lt;/code&gt;, capable of instantiating the same regularizer from the config dictionary.</source>
          <target state="translated">此方法与 &lt;code&gt;get_config&lt;/code&gt; 相反，它能够从config字典中实例化相同的正则化器。</target>
        </trans-unit>
        <trans-unit id="64a729de3228c199e6f42ec9b13a781c1672f7d1" translate="yes" xml:space="preserve">
          <source>This method is the same as setting &lt;code&gt;minval&lt;/code&gt; and &lt;code&gt;maxval&lt;/code&gt; to &lt;code&gt;None&lt;/code&gt; in the &lt;code&gt;uniform&lt;/code&gt; method.</source>
          <target state="translated">此方法与 &lt;code&gt;uniform&lt;/code&gt; 方法 &lt;code&gt;minval&lt;/code&gt; 和 &lt;code&gt;maxval&lt;/code&gt; 设置为 &lt;code&gt;None&lt;/code&gt; 相同。</target>
        </trans-unit>
        <trans-unit id="188de6cc0c368ed4ab40eeabc0c2f0a4587200cc" translate="yes" xml:space="preserve">
          <source>This method is thread-safe.</source>
          <target state="translated">这个方法是线程安全的。</target>
        </trans-unit>
        <trans-unit id="430e31afd2b8ce36dbd30c0726de058dc4716f34" translate="yes" xml:space="preserve">
          <source>This method is used by Keras &lt;code&gt;model_to_estimator&lt;/code&gt;, saving and loading models to HDF5 formats, Keras model cloning, some visualization utilities, and exporting models to and from JSON.</source>
          <target state="translated">Keras &lt;code&gt;model_to_estimator&lt;/code&gt; 使用此方法，将模型保存并加载为HDF5格式，Keras模型克隆，某些可视化实用程序，以及将模型与JSON相互导出。</target>
        </trans-unit>
        <trans-unit id="457bbe494ff8a7cc3168e5b2acb3fc1d7df09a01" translate="yes" xml:space="preserve">
          <source>This method is used to convert a dictionary into a sequence of parameters for a binary that parses arguments using this module.</source>
          <target state="translated">本方法用于将字典转换为使用本模块解析参数的二进制的参数序列。</target>
        </trans-unit>
        <trans-unit id="5f40fabbeaa1bcaea8e9654a487713070e7848ee" translate="yes" xml:space="preserve">
          <source>This method is useful for recovering the &quot;self._last_checkpoints&quot; state.</source>
          <target state="translated">这个方法对于恢复 &quot;self._last_checkpoints &quot;状态很有用。</target>
        </trans-unit>
        <trans-unit id="5eb2940a1b8924b3a2b4c6c370fcb87bd404d70c" translate="yes" xml:space="preserve">
          <source>This method may be called concurrently from multiple threads.</source>
          <target state="translated">这个方法可以从多个线程中并发调用。</target>
        </trans-unit>
        <trans-unit id="879b4ac0bfff915fe8733219e840864c530b0386" translate="yes" xml:space="preserve">
          <source>This method must be used as a context manager, and will yield a recording object with two attributes: &lt;code&gt;output&lt;/code&gt; and &lt;code&gt;records&lt;/code&gt;. At the end of the context manager, the &lt;code&gt;output&lt;/code&gt; attribute will be a list of the matching formatted log messages and the &lt;code&gt;records&lt;/code&gt; attribute will be a list of the corresponding LogRecord objects.</source>
          <target state="translated">此方法必须用作上下文管理器，并将产生具有两个属性的记录对象： &lt;code&gt;output&lt;/code&gt; 和 &lt;code&gt;records&lt;/code&gt; 。在上下文管理器的末尾， &lt;code&gt;output&lt;/code&gt; 属性将是匹配格式的日志消息的列表，而 &lt;code&gt;records&lt;/code&gt; 属性将是对应的LogRecord对象的列表。</target>
        </trans-unit>
        <trans-unit id="45b78faa33c30a4ca2de4d4a2a7e0ea54433a6aa" translate="yes" xml:space="preserve">
          <source>This method overrides unittest.TestCase.shortDescription(), which only returns the first line of the docstring, obscuring the name of the test upon failure.</source>
          <target state="translated">这个方法覆盖了untest.TestCase.shortDescription(),它只返回docstring的第一行,在失败时掩盖了测试的名称。</target>
        </trans-unit>
        <trans-unit id="ee5a67ef5f4ab3abd49e8e0aa6ab37b4c495718e" translate="yes" xml:space="preserve">
          <source>This method promotes a completely unknown shape to one with a known rank.</source>
          <target state="translated">这种方法将一个完全未知的形状推广到一个已知等级的形状。</target>
        </trans-unit>
        <trans-unit id="f08536c5c4bd5b0bf81a621ac1af31ff012f949c" translate="yes" xml:space="preserve">
          <source>This method requires a session in which the graph was launched. It creates a list of threads, optionally starting them. There is one thread for each op passed in &lt;code&gt;enqueue_ops&lt;/code&gt;.</source>
          <target state="translated">此方法需要启动图形的会话。它创建线程列表，可以选择启动它们。 &lt;code&gt;enqueue_ops&lt;/code&gt; 中传递的每个op都有一个线程。</target>
        </trans-unit>
        <trans-unit id="ed37572237615338abb9afe9f97f9e51444c9050" translate="yes" xml:space="preserve">
          <source>This method requires that you are running in eager mode and the dataset's element_spec contains only &lt;code&gt;TensorSpec&lt;/code&gt; components.</source>
          <target state="translated">此方法要求您以急切模式运行，并且数据集的element_spec仅包含 &lt;code&gt;TensorSpec&lt;/code&gt; 组件。</target>
        </trans-unit>
        <trans-unit id="ba66428fee92af2921958313c4abbe6a414d3356" translate="yes" xml:space="preserve">
          <source>This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use &lt;a href=&quot;../../../../data/dataset#cardinality&quot;&gt;&lt;code&gt;tf.data.Dataset.cardinality&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">此方法要求您以渴望模式运行，并且数据集的长度是已知的并且是非无限的。当长度可能是未知的或无限的时，或者如果您在图形模式下运行，请改用&lt;a href=&quot;../../../../data/dataset#cardinality&quot;&gt; &lt;code&gt;tf.data.Dataset.cardinality&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c229102fa2fa8b3a631fa2db6ca01f20dd5c2adc" translate="yes" xml:space="preserve">
          <source>This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use &lt;a href=&quot;../../../data/dataset#cardinality&quot;&gt;&lt;code&gt;tf.data.Dataset.cardinality&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">此方法要求您以渴望模式运行，并且数据集的长度是已知的并且是非无限的。当长度可能是未知的或无限的时，或者如果您在图形模式下运行，请改用&lt;a href=&quot;../../../data/dataset#cardinality&quot;&gt; &lt;code&gt;tf.data.Dataset.cardinality&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="2bff5648ed0712c0feea6c45e6290da036f8f217" translate="yes" xml:space="preserve">
          <source>This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use &lt;a href=&quot;../dataset#cardinality&quot;&gt;&lt;code&gt;tf.data.Dataset.cardinality&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">此方法要求您以渴望模式运行，并且数据集的长度是已知的并且是非无限的。当长度可能是未知的或无限的时，或者如果您在图形模式下运行，请改用&lt;a href=&quot;../dataset#cardinality&quot;&gt; &lt;code&gt;tf.data.Dataset.cardinality&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4d8d6fb6d8be4a1c2d9cfd5ec8103395810dc8de" translate="yes" xml:space="preserve">
          <source>This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use &lt;a href=&quot;dataset#cardinality&quot;&gt;&lt;code&gt;tf.data.Dataset.cardinality&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">此方法要求您以渴望模式运行，并且数据集的长度是已知的并且是非无限的。当长度可能是未知的或无限的时，或者如果您在图形模式下运行，请改用&lt;a href=&quot;dataset#cardinality&quot;&gt; &lt;code&gt;tf.data.Dataset.cardinality&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="df938127180a900a9c0ee2c326f70990ca05bad0" translate="yes" xml:space="preserve">
          <source>This method returns True just before the run() method starts until just after the run() method terminates. The module function enumerate() returns a list of all alive threads.</source>
          <target state="translated">这个方法在run()方法开始之前返回True,直到run()方法终止之后。模块函数enumerate()返回所有活着的线程的列表。</target>
        </trans-unit>
        <trans-unit id="843186b661a979f57d76fd131bdb6daf25004a63" translate="yes" xml:space="preserve">
          <source>This method returns a context manager, and is used as follows:</source>
          <target state="translated">该方法返回一个上下文管理器,使用方法如下。</target>
        </trans-unit>
        <trans-unit id="67c5a6af4d2a477a788bf93645d98e3fc1667925" translate="yes" xml:space="preserve">
          <source>This method runs one &quot;step&quot; of TensorFlow computation, by running the necessary graph fragment to execute every &lt;code&gt;Operation&lt;/code&gt; and evaluate every &lt;code&gt;Tensor&lt;/code&gt; in &lt;code&gt;fetches&lt;/code&gt;, substituting the values in &lt;code&gt;feed_dict&lt;/code&gt; for the corresponding input values.</source>
          <target state="translated">此方法运行一个TensorFlow计算的&amp;ldquo;步骤&amp;rdquo;，通过运行必要图形片段来执行每一个 &lt;code&gt;Operation&lt;/code&gt; 和评价每 &lt;code&gt;Tensor&lt;/code&gt; 在 &lt;code&gt;fetches&lt;/code&gt; ，在代入值 &lt;code&gt;feed_dict&lt;/code&gt; 为相应的输入值。</target>
        </trans-unit>
        <trans-unit id="7ef4f95365ffd49356edce6309d3da564feb3d3b" translate="yes" xml:space="preserve">
          <source>This method runs the ops added by the constructor for restoring variables. It requires a session in which the graph was launched. The variables to restore do not have to have been initialized, as restoring is itself a way to initialize variables.</source>
          <target state="translated">本方法运行由构造函数添加的用于恢复变量的操作。它需要一个启动了图形的会话。要恢复的变量不一定要被初始化,因为恢复本身就是初始化变量的一种方式。</target>
        </trans-unit>
        <trans-unit id="6a0628c028c0b695f5ff3c6935b2aad50149fac6" translate="yes" xml:space="preserve">
          <source>This method runs the ops added by the constructor for saving variables. It requires a session in which the graph was launched. The variables to save must also have been initialized.</source>
          <target state="translated">本方法运行构造函数为保存变量而添加的操作。它需要一个启动了图形的会话。要保存的变量也必须被初始化。</target>
        </trans-unit>
        <trans-unit id="9df50570004b7fc9f261d6b98cab329e6a66afe0" translate="yes" xml:space="preserve">
          <source>This method sets the vocabulary and DF data for this layer directly, instead of analyzing a dataset through 'adapt'. It should be used whenever the vocab (and optionally document frequency) information is already known. If vocabulary data is already present in the layer, this method will either replace it, if 'append' is set to False, or append to it (if 'append' is set to True).</source>
          <target state="translated">该方法直接为该层设置词汇和DF数据,而不是通过 &quot;adapt &quot;分析数据集。当词汇(以及可选的文档频率)信息已经知道时,就应该使用这个方法。如果词汇数据已经存在于该层中,如果'append'被设置为False,本方法将替换它,或者追加到它上面(如果'append'被设置为True)。</target>
        </trans-unit>
        <trans-unit id="98a596750a8e57f7c4dd92233c96d160228e7339" translate="yes" xml:space="preserve">
          <source>This method sets the vocabulary and DF data for this layer directly, instead of analyzing a dataset through 'adapt'. It should be used whenever the vocab (and optionally document frequency) information is already known. If vocabulary data is already present in the layer, this method will replace it.</source>
          <target state="translated">该方法直接为该层设置词汇和DF数据,而不是通过 &quot;adapt &quot;分析数据集。当词汇(以及可选的文档频率)信息已经知道时,就应该使用这个方法。如果层中已经存在词汇数据,本方法将替换它。</target>
        </trans-unit>
        <trans-unit id="f39e288c36cc30e58aa384f98451d54174eefdc7" translate="yes" xml:space="preserve">
          <source>This method sets the vocabulary for this layer directly, instead of analyzing a dataset through 'adapt'. It should be used whenever the vocab information is already known. If vocabulary data is already present in the layer, this method will either replace it</source>
          <target state="translated">这个方法直接设置本层的词汇,而不是通过'adapt'分析数据集。只要词汇信息已经知道,就应该使用它。如果层中已经存在词汇数据,本方法将替换它</target>
        </trans-unit>
        <trans-unit id="381d5d74ef1ba876c849eaf0c748814799df311b" translate="yes" xml:space="preserve">
          <source>This method should be used if you want to create multiple graphs in the same process. For convenience, a global default graph is provided, and all ops will be added to this graph if you do not create a new graph explicitly.</source>
          <target state="translated">如果你想在同一个过程中创建多个图形,应该使用这个方法。为方便起见,提供了一个全局默认的图形,如果不明确创建新的图形,所有的操作都会被添加到这个图形中。</target>
        </trans-unit>
        <trans-unit id="801d04a3f8f77fe9f713a4af6184910c6103cec6" translate="yes" xml:space="preserve">
          <source>This method should be used to create all threads in test cases, as otherwise there is a risk that a thread will silently fail, and/or assertions made in the thread will not be respected.</source>
          <target state="translated">这个方法应该用于在测试用例中创建所有的线程,否则线程有可能默默地失败,和/或在线程中做出的断言不被尊重。</target>
        </trans-unit>
        <trans-unit id="0840a5953e72a61137caa721b5cce35c70dbd3d8" translate="yes" xml:space="preserve">
          <source>This method should contain the mathemetical logic for one step of inference. This typically includes the forward pass.</source>
          <target state="translated">这个方法应该包含一步推理的数学逻辑。这通常包括前传。</target>
        </trans-unit>
        <trans-unit id="eebfd3cd848c16a7d254cfeece5ae46941b7f986" translate="yes" xml:space="preserve">
          <source>This method should contain the mathemetical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates.</source>
          <target state="translated">这个方法应该包含一步训练的数学逻辑。这通常包括正向传递、损失计算、反向传播和度量更新。</target>
        </trans-unit>
        <trans-unit id="84ddde0d41fb950f979365b1dd4b04e51a4464b1" translate="yes" xml:space="preserve">
          <source>This method simply combines calls &lt;code&gt;compute_gradients()&lt;/code&gt; and &lt;code&gt;apply_gradients()&lt;/code&gt;. If you want to process the gradient before applying them call &lt;code&gt;compute_gradients()&lt;/code&gt; and &lt;code&gt;apply_gradients()&lt;/code&gt; explicitly instead of using this function.</source>
          <target state="translated">该方法仅将调用 &lt;code&gt;compute_gradients()&lt;/code&gt; 和 &lt;code&gt;apply_gradients()&lt;/code&gt; 组合在一起。如果要在应用渐变之前对其进行处理，请显式调用而不是不使用此函数，而是调用 &lt;code&gt;compute_gradients()&lt;/code&gt; 和 &lt;code&gt;apply_gradients()&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5f6d383158cc853c41c5bc430e4c2e31944ed6aa" translate="yes" xml:space="preserve">
          <source>This method simply computes gradient using &lt;a href=&quot;../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt; and calls &lt;code&gt;apply_gradients()&lt;/code&gt;. If you want to process the gradient before applying then call &lt;a href=&quot;../../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;apply_gradients()&lt;/code&gt; explicitly instead of using this function.</source>
          <target state="translated">此方法仅使用&lt;a href=&quot;../../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;计算梯度并调用 &lt;code&gt;apply_gradients()&lt;/code&gt; 。如果要在应用之前处理渐变，请显式调用&lt;a href=&quot;../../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;和 &lt;code&gt;apply_gradients()&lt;/code&gt; 而不是使用此函数。</target>
        </trans-unit>
        <trans-unit id="7c45540d5f88e8d63143978a302ef3be7b4897a3" translate="yes" xml:space="preserve">
          <source>This method simply computes gradient using &lt;a href=&quot;../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt; and calls &lt;code&gt;apply_gradients()&lt;/code&gt;. If you want to process the gradient before applying then call &lt;a href=&quot;../../gradienttape&quot;&gt;&lt;code&gt;tf.GradientTape&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;apply_gradients()&lt;/code&gt; explicitly instead of using this function.</source>
          <target state="translated">此方法仅使用&lt;a href=&quot;../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;计算梯度并调用 &lt;code&gt;apply_gradients()&lt;/code&gt; 。如果要在应用之前处理渐变，请显式调用&lt;a href=&quot;../../gradienttape&quot;&gt; &lt;code&gt;tf.GradientTape&lt;/code&gt; &lt;/a&gt;和 &lt;code&gt;apply_gradients()&lt;/code&gt; 而不是使用此函数。</target>
        </trans-unit>
        <trans-unit id="9fbf54940adbbc3b34153fb181199a19b56bd888" translate="yes" xml:space="preserve">
          <source>This method supports the case where attr_name is a staticmethod or a classmethod of obj.</source>
          <target state="translated">本方法支持attr_name是obj的staticmethod或classmethod的情况。</target>
        </trans-unit>
        <trans-unit id="b1e95fe0c7a12cd011cb442b54315c64dbbef85c" translate="yes" xml:space="preserve">
          <source>This method supports the case where child_name is a staticmethod or a classmethod of parent.</source>
          <target state="translated">本方法支持child_name是静态方法或父类方法的情况。</target>
        </trans-unit>
        <trans-unit id="3c5357e5e49a58f1873923787332f7877e186b5d" translate="yes" xml:space="preserve">
          <source>This method takes effect only on the thread in which it is called.</source>
          <target state="translated">本方法只在被调用的线程中生效。</target>
        </trans-unit>
        <trans-unit id="dc69067a62ba3479c811e1ffb95925d4cb95f817" translate="yes" xml:space="preserve">
          <source>This method takes the same args as the DeviceSpec constructor</source>
          <target state="translated">本方法的参数与DeviceSpec构造函数相同。</target>
        </trans-unit>
        <trans-unit id="7cd6047c37611c4ef1b1add7ec4d3db22681db24" translate="yes" xml:space="preserve">
          <source>This method will also be called as a result of recovering a wrapped session, not only at the beginning of the overall session.</source>
          <target state="translated">这个方法也会因为恢复一个被封装的会话而被调用,而不仅仅是在整体会话的开头。</target>
        </trans-unit>
        <trans-unit id="b524a4798bc6e14abf1530810ef4205d0b748932" translate="yes" xml:space="preserve">
          <source>This method will block caller thread until it receives tracing result. This method supports CPU, GPU, and Cloud TPU. This method supports profiling a single host for CPU, GPU, TPU, as well as multiple TPU workers. The profiled results will be saved to your specified TensorBoard log directory (e.g. the directory you save your model checkpoints). Use the TensorBoard profile plugin to view the visualization and analysis results.</source>
          <target state="translated">本方法将阻塞调用者线程,直到收到追踪结果。本方法支持CPU、GPU和云TPU。本方法支持对CPU、GPU、TPU的单个主机以及多个TPU工作者进行剖析。剖析结果将保存到您指定的 TensorBoard 日志目录(例如,您保存模型检查点的目录)。使用TensorBoard剖面插件来查看可视化和分析结果。</target>
        </trans-unit>
        <trans-unit id="80c9b1eb0c3b33d3a7e2274259517a077b4c895e" translate="yes" xml:space="preserve">
          <source>This method will first try to restore from the most recent checkpoint in &lt;code&gt;directory&lt;/code&gt;. If no checkpoints exist in &lt;code&gt;directory&lt;/code&gt;, and &lt;code&gt;init_fn&lt;/code&gt; is specified, this method will call &lt;code&gt;init_fn&lt;/code&gt; to do customized initialization. This can be used to support initialization from pretrained models.</source>
          <target state="translated">此方法将首先尝试从 &lt;code&gt;directory&lt;/code&gt; 中的最新检查点还原。如果 &lt;code&gt;directory&lt;/code&gt; 中没有检查点，并且指定了 &lt;code&gt;init_fn&lt;/code&gt; ，则此方法将调用 &lt;code&gt;init_fn&lt;/code&gt; 进行自定义初始化。这可以用来支持来自预训练模型的初始化。</target>
        </trans-unit>
        <trans-unit id="72f0ec32b5d6b0d5f31ec5a44977c2b73ae22a24" translate="yes" xml:space="preserve">
          <source>This method will raise a RuntimeError if called more than once on the same thread object.</source>
          <target state="translated">如果在同一个线程对象上调用不止一次,该方法将引发RuntimeError。</target>
        </trans-unit>
        <trans-unit id="5450168f56def10b3805f1b87bf923f81daadea9" translate="yes" xml:space="preserve">
          <source>This method works similar to tf.map_fn but is optimized to run much faster, possibly with a much larger memory footprint. The speedups are obtained by vectorization (see &lt;a href=&quot;https://arxiv.org/pdf/1903.04243.pdf&quot;&gt;https://arxiv.org/pdf/1903.04243.pdf&lt;/a&gt;). The idea behind vectorization is to semantically launch all the invocations of &lt;code&gt;fn&lt;/code&gt; in parallel and fuse corresponding operations across all these invocations. This fusion is done statically at graph generation time and the generated code is often similar in performance to a manually fused version.</source>
          <target state="translated">此方法的工作方式类似于tf.map_fn，但已优化为运行速度更快，可能占用的内存更大。加速是通过矢量化获得的（请参阅&lt;a href=&quot;https://arxiv.org/pdf/1903.04243.pdf&quot;&gt;https://arxiv.org/pdf/1903.04243.pdf&lt;/a&gt;）。向量化背后的想法是在语义上并行启动 &lt;code&gt;fn&lt;/code&gt; 的所有调用，并在所有这些调用中融合相应的操作。这种融合是在图生成时静态完成的，并且生成的代码在性能上通常与手动融合的版本相似。</target>
        </trans-unit>
        <trans-unit id="218a6eaea58d633bb4c07cebec2cce2a54164fc8" translate="yes" xml:space="preserve">
          <source>This method works similar to tf.map_fn but is optimized to run much faster, possibly with a much larger memory footprint. The speedups are obtained by vectorization (see https://arxiv.org/pdf/1903.04243.pdf). The idea behind vectorization is to semantically launch all the invocations of &lt;code&gt;fn&lt;/code&gt; in parallel and fuse corresponding operations across all these invocations. This fusion is done statically at graph generation time and the generated code is often similar in performance to a manually fused version.</source>
          <target state="translated">此方法的工作方式类似于tf.map_fn，但经过优化可运行得更快，可能占用更多的内存。加速是通过矢量化获得的（请参阅https://arxiv.org/pdf/1903.04243.pdf）。向量化背后的想法是在语义上并行启动 &lt;code&gt;fn&lt;/code&gt; 的所有调用，并在所有这些调用中融合相应的操作。这种融合是在图生成时静态完成的，并且生成的代码在性能上通常与手动融合的版本相似。</target>
        </trans-unit>
        <trans-unit id="fe5b3fca46a696592f5d3933d9a6c2e14a59d105" translate="yes" xml:space="preserve">
          <source>This method wraps the provided session in an &lt;code&gt;Event&lt;/code&gt; protocol buffer and adds it to the event file.</source>
          <target state="translated">此方法将提供的会话包装在 &lt;code&gt;Event&lt;/code&gt; 协议缓冲区中，并将其添加到事件文件中。</target>
        </trans-unit>
        <trans-unit id="0b654fd9b9bcfd951b8584e5c172fdf88b6f87e9" translate="yes" xml:space="preserve">
          <source>This method wraps the provided summary in an &lt;code&gt;Event&lt;/code&gt; protocol buffer and adds it to the event file.</source>
          <target state="translated">此方法将提供的摘要包装在 &lt;code&gt;Event&lt;/code&gt; 协议缓冲区中，并将其添加到事件文件中。</target>
        </trans-unit>
        <trans-unit id="e573192331e3d216aaf00fffe24f089e83e9c9ad" translate="yes" xml:space="preserve">
          <source>This method, unlike assertCountEqual, doesn't care about any duplicates in the expected and actual sequences.</source>
          <target state="translated">这个方法与 assertCountEqual 不同,不关心预期序列和实际序列中的任何重复。</target>
        </trans-unit>
        <trans-unit id="0b693febd3a0c9fc69fb14a0f914449cb9729097" translate="yes" xml:space="preserve">
          <source>This metric creates four local variables, &lt;code&gt;true_positives&lt;/code&gt;, &lt;code&gt;true_negatives&lt;/code&gt;, &lt;code&gt;false_positives&lt;/code&gt; and &lt;code&gt;false_negatives&lt;/code&gt; that are used to compute the AUC. To discretize the AUC curve, a linearly spaced set of thresholds is used to compute pairs of recall and precision values. The area under the ROC-curve is therefore computed using the height of the recall values by the false positive rate, while the area under the PR-curve is the computed using the height of the precision values by the recall.</source>
          <target state="translated">此度量标准创建用于计算AUC的四个局部变量 &lt;code&gt;true_positives&lt;/code&gt; ， &lt;code&gt;true_negatives&lt;/code&gt; ， &lt;code&gt;false_positives&lt;/code&gt; 和 &lt;code&gt;false_negatives&lt;/code&gt; 。为了离散化AUC曲线，使用一组线性间隔的阈值来计算成对的召回率和精度值。因此，ROC曲线下方的面积是通过假阳性率使用召回值的高度来计算的，而PR曲线下方的面积是通过召回率使用精度值的高度来计算的。</target>
        </trans-unit>
        <trans-unit id="8310e01ff09f3ad784585cc77cdb8fe7f1ac0213" translate="yes" xml:space="preserve">
          <source>This metric creates four local variables, &lt;code&gt;true_positives&lt;/code&gt;, &lt;code&gt;true_negatives&lt;/code&gt;, &lt;code&gt;false_positives&lt;/code&gt; and &lt;code&gt;false_negatives&lt;/code&gt; that are used to compute the precision at the given recall. The threshold for the given recall value is computed and used to evaluate the corresponding precision.</source>
          <target state="translated">此度量标准创建四个局部变量 &lt;code&gt;true_positives&lt;/code&gt; ， &lt;code&gt;true_negatives&lt;/code&gt; ， &lt;code&gt;false_positives&lt;/code&gt; 和 &lt;code&gt;false_negatives&lt;/code&gt; 用于在给定召回率下计算精度。计算给定召回值的阈值，并将其用于评估相应的精度。</target>
        </trans-unit>
        <trans-unit id="cd97f6783f485e9e31fe2474cb3ca5414d0a738b" translate="yes" xml:space="preserve">
          <source>This metric creates four local variables, &lt;code&gt;true_positives&lt;/code&gt;, &lt;code&gt;true_negatives&lt;/code&gt;, &lt;code&gt;false_positives&lt;/code&gt; and &lt;code&gt;false_negatives&lt;/code&gt; that are used to compute the recall at the given precision. The threshold for the given precision value is computed and used to evaluate the corresponding recall.</source>
          <target state="translated">此度量标准创建四个局部变量 &lt;code&gt;true_positives&lt;/code&gt; ， &lt;code&gt;true_negatives&lt;/code&gt; ， &lt;code&gt;false_positives&lt;/code&gt; 和 &lt;code&gt;false_negatives&lt;/code&gt; 用于以给定精度计算召回率。计算给定精度值的阈值，并将其用于评估相应的召回率。</target>
        </trans-unit>
        <trans-unit id="a469d6ce618a21614949a4828cde8383fb0456e0" translate="yes" xml:space="preserve">
          <source>This metric creates four local variables, &lt;code&gt;true_positives&lt;/code&gt;, &lt;code&gt;true_negatives&lt;/code&gt;, &lt;code&gt;false_positives&lt;/code&gt; and &lt;code&gt;false_negatives&lt;/code&gt; that are used to compute the sensitivity at the given specificity. The threshold for the given specificity value is computed and used to evaluate the corresponding sensitivity.</source>
          <target state="translated">此度量标准创建四个局部变量 &lt;code&gt;true_positives&lt;/code&gt; ， &lt;code&gt;true_negatives&lt;/code&gt; ， &lt;code&gt;false_positives&lt;/code&gt; 和 &lt;code&gt;false_negatives&lt;/code&gt; 用于计算给定特异性下的灵敏度。计算给定特异性值的阈值，并将其用于评估相应的灵敏度。</target>
        </trans-unit>
        <trans-unit id="ff9252130946c80b57ae4c5ee9b47938ca02b839" translate="yes" xml:space="preserve">
          <source>This metric creates four local variables, &lt;code&gt;true_positives&lt;/code&gt;, &lt;code&gt;true_negatives&lt;/code&gt;, &lt;code&gt;false_positives&lt;/code&gt; and &lt;code&gt;false_negatives&lt;/code&gt; that are used to compute the specificity at the given sensitivity. The threshold for the given sensitivity value is computed and used to evaluate the corresponding specificity.</source>
          <target state="translated">此度量标准创建四个局部变量 &lt;code&gt;true_positives&lt;/code&gt; ， &lt;code&gt;true_negatives&lt;/code&gt; ， &lt;code&gt;false_positives&lt;/code&gt; 和 &lt;code&gt;false_negatives&lt;/code&gt; 用于在给定灵敏度下计算特异性。计算给定灵敏度值的阈值，并将其用于评估相应的特异性。</target>
        </trans-unit>
        <trans-unit id="fb652b4471c79604ae7dfb8ff87ca222abc482c6" translate="yes" xml:space="preserve">
          <source>This metric creates one variable, &lt;code&gt;total&lt;/code&gt;, that is used to compute the sum of &lt;code&gt;values&lt;/code&gt;. This is ultimately returned as &lt;code&gt;sum&lt;/code&gt;.</source>
          <target state="translated">此度量标准创建一个 &lt;code&gt;total&lt;/code&gt; 变量，用于计算 &lt;code&gt;values&lt;/code&gt; 的总和。最终将其作为 &lt;code&gt;sum&lt;/code&gt; 返回。</target>
        </trans-unit>
        <trans-unit id="77a69992763ecf548ab84fef5482d353d7a9cba3" translate="yes" xml:space="preserve">
          <source>This metric creates two local variables, &lt;code&gt;total&lt;/code&gt; and &lt;code&gt;count&lt;/code&gt; that are used to compute the frequency with which &lt;code&gt;y_pred&lt;/code&gt; matches &lt;code&gt;y_true&lt;/code&gt;. This frequency is ultimately returned as &lt;code&gt;binary accuracy&lt;/code&gt;: an idempotent operation that simply divides &lt;code&gt;total&lt;/code&gt; by &lt;code&gt;count&lt;/code&gt;.</source>
          <target state="translated">该度量创建两个局部变量， &lt;code&gt;total&lt;/code&gt; 和 &lt;code&gt;count&lt;/code&gt; ，用于计算 &lt;code&gt;y_pred&lt;/code&gt; 与 &lt;code&gt;y_true&lt;/code&gt; 匹配的频率。该频率最终以 &lt;code&gt;binary accuracy&lt;/code&gt; 返回：幂等运算，将 &lt;code&gt;total&lt;/code&gt; 除以 &lt;code&gt;count&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="10a549b6038018b3cebc025343c39cb24e67e040" translate="yes" xml:space="preserve">
          <source>This metric creates two local variables, &lt;code&gt;total&lt;/code&gt; and &lt;code&gt;count&lt;/code&gt; that are used to compute the frequency with which &lt;code&gt;y_pred&lt;/code&gt; matches &lt;code&gt;y_true&lt;/code&gt;. This frequency is ultimately returned as &lt;code&gt;categorical accuracy&lt;/code&gt;: an idempotent operation that simply divides &lt;code&gt;total&lt;/code&gt; by &lt;code&gt;count&lt;/code&gt;.</source>
          <target state="translated">该度量创建两个局部变量， &lt;code&gt;total&lt;/code&gt; 和 &lt;code&gt;count&lt;/code&gt; ，用于计算 &lt;code&gt;y_pred&lt;/code&gt; 与 &lt;code&gt;y_true&lt;/code&gt; 匹配的频率。该频率最终以 &lt;code&gt;categorical accuracy&lt;/code&gt; 返回：幂等运算，简单地将 &lt;code&gt;total&lt;/code&gt; 除以 &lt;code&gt;count&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2d4e9dd5712eaf45fc10b998b8c6d4caf2e34bae" translate="yes" xml:space="preserve">
          <source>This metric creates two local variables, &lt;code&gt;total&lt;/code&gt; and &lt;code&gt;count&lt;/code&gt; that are used to compute the frequency with which &lt;code&gt;y_pred&lt;/code&gt; matches &lt;code&gt;y_true&lt;/code&gt;. This frequency is ultimately returned as &lt;code&gt;sparse categorical accuracy&lt;/code&gt;: an idempotent operation that simply divides &lt;code&gt;total&lt;/code&gt; by &lt;code&gt;count&lt;/code&gt;.</source>
          <target state="translated">该度量创建两个局部变量， &lt;code&gt;total&lt;/code&gt; 和 &lt;code&gt;count&lt;/code&gt; ，用于计算 &lt;code&gt;y_pred&lt;/code&gt; 与 &lt;code&gt;y_true&lt;/code&gt; 匹配的频率。该频率最终以 &lt;code&gt;sparse categorical accuracy&lt;/code&gt; 返回：幂等运算，简单地将 &lt;code&gt;total&lt;/code&gt; 除以 &lt;code&gt;count&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="300536328f19b08910251bd6b961009b9a37a3b7" translate="yes" xml:space="preserve">
          <source>This metric creates two local variables, &lt;code&gt;total&lt;/code&gt; and &lt;code&gt;count&lt;/code&gt; that are used to compute the mean relative absolute error. This average is weighted by &lt;code&gt;sample_weight&lt;/code&gt;, and it is ultimately returned as &lt;code&gt;mean_relative_error&lt;/code&gt;: an idempotent operation that simply divides &lt;code&gt;total&lt;/code&gt; by &lt;code&gt;count&lt;/code&gt;.</source>
          <target state="translated">该度量创建两个局部变量， &lt;code&gt;total&lt;/code&gt; 和 &lt;code&gt;count&lt;/code&gt; ，用于计算平均相对绝对误差。该平均值由 &lt;code&gt;sample_weight&lt;/code&gt; 加权，最终以 &lt;code&gt;mean_relative_error&lt;/code&gt; 的形式返回：幂等运算，将 &lt;code&gt;total&lt;/code&gt; 除以 &lt;code&gt;count&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="aea998ffd50f56931370c8f2e5952f5d92b92150" translate="yes" xml:space="preserve">
          <source>This metric creates two local variables, &lt;code&gt;total&lt;/code&gt; and &lt;code&gt;count&lt;/code&gt; that are used to compute the mean relative error. This is weighted by &lt;code&gt;sample_weight&lt;/code&gt;, and it is ultimately returned as &lt;code&gt;mean_relative_error&lt;/code&gt;: an idempotent operation that simply divides &lt;code&gt;total&lt;/code&gt; by &lt;code&gt;count&lt;/code&gt;.</source>
          <target state="translated">该度量创建两个局部变量， &lt;code&gt;total&lt;/code&gt; 和 &lt;code&gt;count&lt;/code&gt; ，用于计算平均相对误差。这由 &lt;code&gt;sample_weight&lt;/code&gt; 加权，最终返回为 &lt;code&gt;mean_relative_error&lt;/code&gt; ：一个幂等运算，将 &lt;code&gt;total&lt;/code&gt; 除以 &lt;code&gt;count&lt;/code&gt; 即可。</target>
        </trans-unit>
        <trans-unit id="0bf885c318184cc4d7f8536f5a3a6cfc0be45917" translate="yes" xml:space="preserve">
          <source>This metric creates two local variables, &lt;code&gt;true_positives&lt;/code&gt; and &lt;code&gt;false_negatives&lt;/code&gt;, that are used to compute the recall. This value is ultimately returned as &lt;code&gt;recall&lt;/code&gt;, an idempotent operation that simply divides &lt;code&gt;true_positives&lt;/code&gt; by the sum of &lt;code&gt;true_positives&lt;/code&gt; and &lt;code&gt;false_negatives&lt;/code&gt;.</source>
          <target state="translated">此度量标准创建两个局部变量 &lt;code&gt;true_positives&lt;/code&gt; 和 &lt;code&gt;false_negatives&lt;/code&gt; ，用于计算召回率。该值最终作为 &lt;code&gt;recall&lt;/code&gt; 返回，这是一个幂等运算，将 &lt;code&gt;true_positives&lt;/code&gt; 除以true_positives和 &lt;code&gt;true_positives&lt;/code&gt; 和 &lt;code&gt;false_negatives&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="093560f8538560280d9ed1cd9a3cab29a6cbb355" translate="yes" xml:space="preserve">
          <source>This metric creates two variables, &lt;code&gt;total&lt;/code&gt; and &lt;code&gt;count&lt;/code&gt; that are used to compute the average of &lt;code&gt;values&lt;/code&gt;. This average is ultimately returned as &lt;code&gt;mean&lt;/code&gt; which is an idempotent operation that simply divides &lt;code&gt;total&lt;/code&gt; by &lt;code&gt;count&lt;/code&gt;.</source>
          <target state="translated">该指标创建两个变量， &lt;code&gt;total&lt;/code&gt; 和 &lt;code&gt;count&lt;/code&gt; ，用于计算 &lt;code&gt;values&lt;/code&gt; 的平均值。该平均最终返回作为 &lt;code&gt;mean&lt;/code&gt; 是等幂操作，简单地划分 &lt;code&gt;total&lt;/code&gt; 通过 &lt;code&gt;count&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0c9b94b35f1d2e4eb96d2ecbbedfd0249ca4dde6" translate="yes" xml:space="preserve">
          <source>This metric keeps the average cosine similarity between &lt;code&gt;predictions&lt;/code&gt; and &lt;code&gt;labels&lt;/code&gt; over a stream of data.</source>
          <target state="translated">此度量标准可在数据流中保持 &lt;code&gt;predictions&lt;/code&gt; 和 &lt;code&gt;labels&lt;/code&gt; 之间的平均余弦相似度。</target>
        </trans-unit>
        <trans-unit id="418d70f20044dbd3397385f2c468b1eaa3216144" translate="yes" xml:space="preserve">
          <source>This model accepts sparse float inputs as well:</source>
          <target state="translated">这个模型也接受稀疏的浮点数输入。</target>
        </trans-unit>
        <trans-unit id="3a4737a2c04c7eb94694e1e82664a0b0d934f20d" translate="yes" xml:space="preserve">
          <source>This model approximates the following function:</source>
          <target state="translated">这个模型近似于以下函数:</target>
        </trans-unit>
        <trans-unit id="82e7fe60bedc5bc6c52d1131a24d660c1d1e5baf" translate="yes" xml:space="preserve">
          <source>This model jointly train a linear and a dnn model.</source>
          <target state="translated">该模型联合训练一个线性和一个dnn模型。</target>
        </trans-unit>
        <trans-unit id="e8ea575479c33e1d3b54efb7336850606e620ed3" translate="yes" xml:space="preserve">
          <source>This module contains experimental &lt;code&gt;Dataset&lt;/code&gt; sources and transformations that can be used in conjunction with the &lt;a href=&quot;../../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; API. Note that the &lt;a href=&quot;../../../data/experimental&quot;&gt;&lt;code&gt;tf.data.experimental&lt;/code&gt;&lt;/a&gt; API is not subject to the same backwards compatibility guarantees as &lt;a href=&quot;../../../data&quot;&gt;&lt;code&gt;tf.data&lt;/code&gt;&lt;/a&gt;, but we will provide deprecation advice in advance of removing existing functionality.</source>
          <target state="translated">此模块包含可与&lt;a href=&quot;../../../data/dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt; API 结合使用的实验 &lt;code&gt;Dataset&lt;/code&gt; 源和转换。请注意，&lt;a href=&quot;../../../data/experimental&quot;&gt; &lt;code&gt;tf.data.experimental&lt;/code&gt; &lt;/a&gt; API是不是受到相同的向后兼容性保证为&lt;a href=&quot;../../../data&quot;&gt; &lt;code&gt;tf.data&lt;/code&gt; &lt;/a&gt;，但我们会提前消除现有功能的提供弃用建议。</target>
        </trans-unit>
        <trans-unit id="048da49888cbf092642d30c4a165ddde49bb4a0b" translate="yes" xml:space="preserve">
          <source>This module contains experimental &lt;code&gt;Dataset&lt;/code&gt; sources and transformations that can be used in conjunction with the &lt;a href=&quot;dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; API. Note that the &lt;a href=&quot;experimental&quot;&gt;&lt;code&gt;tf.data.experimental&lt;/code&gt;&lt;/a&gt; API is not subject to the same backwards compatibility guarantees as &lt;a href=&quot;../data&quot;&gt;&lt;code&gt;tf.data&lt;/code&gt;&lt;/a&gt;, but we will provide deprecation advice in advance of removing existing functionality.</source>
          <target state="translated">此模块包含可与&lt;a href=&quot;dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt; API 结合使用的实验 &lt;code&gt;Dataset&lt;/code&gt; 源和转换。请注意，&lt;a href=&quot;experimental&quot;&gt; &lt;code&gt;tf.data.experimental&lt;/code&gt; &lt;/a&gt; API是不是受到相同的向后兼容性保证为&lt;a href=&quot;../data&quot;&gt; &lt;code&gt;tf.data&lt;/code&gt; &lt;/a&gt;，但我们会提前消除现有功能的提供弃用建议。</target>
        </trans-unit>
        <trans-unit id="458fef549ca5bb1cdd37d04e9f0b5b8e3b96d46e" translate="yes" xml:space="preserve">
          <source>This must be called by the constructors of subclasses.</source>
          <target state="translated">这必须被子类的构造函数所调用。</target>
        </trans-unit>
        <trans-unit id="b854be37a7975d22746ea61f4632ebd3ee101333" translate="yes" xml:space="preserve">
          <source>This must be called by the constructors of subclasses. Note that Optimizer instances should not bind to a single graph, and so shouldn't keep Tensors as member variables. Generally you should be able to use the _set_hyper()/state.get_hyper() facility instead.</source>
          <target state="translated">这必须由子类的构造函数调用。请注意,Optimizer实例不应该绑定到一个单一的图形上,因此不应该将Tensors作为成员变量保留。一般来说,你应该能够使用_set_hyper()/state.get_hyper()设施来代替。</target>
        </trans-unit>
        <trans-unit id="10ca194e65aab7cdbd21fa158a845dc67377d2ce" translate="yes" xml:space="preserve">
          <source>This must be set before start() is called, otherwise RuntimeError is raised. Its initial value is inherited from the creating thread; the main thread is not a daemon thread and therefore all threads created in the main thread default to daemon = False.</source>
          <target state="translated">必须在调用start()之前设置,否则会引发RuntimeError。它的初始值继承自创建线程,主线程不是守护进程线程,因此所有在主线程中创建的线程都默认为daemon=False。</target>
        </trans-unit>
        <trans-unit id="0329801ef1692f0ddc2ae7e5af20ed6a131c6c7c" translate="yes" xml:space="preserve">
          <source>This only ensures that the data has made its way out of the process without any guarantees on whether it's written to disk. This means that the data would survive an application crash but not necessarily an OS crash.</source>
          <target state="translated">这只能确保数据已经走出了进程,而不能保证数据是否写入磁盘。这意味着数据可以在应用程序崩溃后存活,但不一定能在操作系统崩溃后存活。</target>
        </trans-unit>
        <trans-unit id="739d09cc347d37887487c132a76071a68cf15e0c" translate="yes" xml:space="preserve">
          <source>This op accepts a ragged tensor with 1 ragged dimension containing only strings and outputs a ragged tensor with 1 ragged dimension containing ngrams of that string, joined along the innermost axis.</source>
          <target state="translated">这个操作接受一个只包含字符串的1个粗糙维度的粗糙张量,并输出一个包含该字符串的ngrams的1个粗糙维度的粗糙张量,沿最内轴连接。</target>
        </trans-unit>
        <trans-unit id="93e1dc6b2d1103564814280c522ceb3f62da8aaa" translate="yes" xml:space="preserve">
          <source>This op adds entries with the specified &lt;code&gt;default_value&lt;/code&gt; at index &lt;code&gt;[row, 0]&lt;/code&gt; for any row in the input that does not already have a value.</source>
          <target state="translated">此操作会为索引中没有值的任何行在索引 &lt;code&gt;[row, 0]&lt;/code&gt; 处添加具有指定 &lt;code&gt;default_value&lt;/code&gt; 的条目。</target>
        </trans-unit>
        <trans-unit id="5fa1114f375b9880e62fbc5a662e7acc5ce57fbe" translate="yes" xml:space="preserve">
          <source>This op also returns an indicator vector shaped &lt;code&gt;[dense_shape[0]]&lt;/code&gt; such that</source>
          <target state="translated">该op还返回形状为 &lt;code&gt;[dense_shape[0]]&lt;/code&gt; 的指标向量，使得</target>
        </trans-unit>
        <trans-unit id="9c96c116d1d672bbaf2e116ec518227e07309624" translate="yes" xml:space="preserve">
          <source>This op also returns an indicator vector such that</source>
          <target state="translated">这个操作也返回一个指标向量,如</target>
        </trans-unit>
        <trans-unit id="63eba777b467e98e8d9a395e0dc5bdd68e038be1" translate="yes" xml:space="preserve">
          <source>This op also supports decoding JPEGs and PNGs, though it is cleaner to use &lt;a href=&quot;../io/decode_image&quot;&gt;&lt;code&gt;tf.io.decode_image&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">该操作还支持解码JPEG和PNG，尽管使用&lt;a href=&quot;../io/decode_image&quot;&gt; &lt;code&gt;tf.io.decode_image&lt;/code&gt; 更为干净&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="f0a295b7a7559c4062aef820bad409706b47254e" translate="yes" xml:space="preserve">
          <source>This op also supports decoding JPEGs and PNGs, though it is cleaner to use &lt;a href=&quot;decode_image&quot;&gt;&lt;code&gt;tf.image.decode_image&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">该操作还支持解码JPEG和PNG，尽管使用&lt;a href=&quot;decode_image&quot;&gt; &lt;code&gt;tf.image.decode_image&lt;/code&gt; 更为干净&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="483bb07454239e8342211e5a15919059fc6155be" translate="yes" xml:space="preserve">
          <source>This op also supports decoding JPEGs and PNGs, though it is cleaner to use &lt;a href=&quot;decode_image&quot;&gt;&lt;code&gt;tf.io.decode_image&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">该操作还支持解码JPEG和PNG，尽管使用&lt;a href=&quot;decode_image&quot;&gt; &lt;code&gt;tf.io.decode_image&lt;/code&gt; 更为干净&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="18fa4a39fc572b40f8f4dacf2470cf5be8ddff15" translate="yes" xml:space="preserve">
          <source>This op also supports decoding JPEGs and non-animated GIFs since the interface is the same, though it is cleaner to use &lt;a href=&quot;../io/decode_image&quot;&gt;&lt;code&gt;tf.io.decode_image&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">由于接口相同，该操作还支持解码JPEG和非动画GIF，尽管使用&lt;a href=&quot;../io/decode_image&quot;&gt; &lt;code&gt;tf.io.decode_image&lt;/code&gt; 更为干净&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="7b09a5c52da344b1eb6cdb8c06bc391b1939c62f" translate="yes" xml:space="preserve">
          <source>This op also supports decoding JPEGs and non-animated GIFs since the interface is the same, though it is cleaner to use &lt;a href=&quot;decode_image&quot;&gt;&lt;code&gt;tf.image.decode_image&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">由于接口相同，该操作还支持解码JPEG和非动画GIF，尽管使用&lt;a href=&quot;decode_image&quot;&gt; &lt;code&gt;tf.image.decode_image&lt;/code&gt; 更为干净&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c9a007b21ccb02db71e5f851437184affd13ed0e" translate="yes" xml:space="preserve">
          <source>This op also supports decoding JPEGs and non-animated GIFs since the interface is the same, though it is cleaner to use &lt;a href=&quot;decode_image&quot;&gt;&lt;code&gt;tf.io.decode_image&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">由于接口相同，该操作还支持解码JPEG和非动画GIF，尽管使用&lt;a href=&quot;decode_image&quot;&gt; &lt;code&gt;tf.io.decode_image&lt;/code&gt; 更为干净&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="5122fab2d6910f76eb99cdafcfcf3c05b7df8003" translate="yes" xml:space="preserve">
          <source>This op also supports decoding PNGs and non-animated GIFs since the interface is the same, though it is cleaner to use &lt;a href=&quot;../io/decode_image&quot;&gt;&lt;code&gt;tf.io.decode_image&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">由于接口相同，该操作还支持解码PNG和非动画GIF，尽管使用&lt;a href=&quot;../io/decode_image&quot;&gt; &lt;code&gt;tf.io.decode_image&lt;/code&gt; 更为干净&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="42b9c8de7bcd9a44756c5acb1ef87661725abb7d" translate="yes" xml:space="preserve">
          <source>This op also supports decoding PNGs and non-animated GIFs since the interface is the same, though it is cleaner to use &lt;a href=&quot;decode_image&quot;&gt;&lt;code&gt;tf.image.decode_image&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">由于接口相同，该操作还支持解码PNG和非动画GIF，尽管使用&lt;a href=&quot;decode_image&quot;&gt; &lt;code&gt;tf.image.decode_image&lt;/code&gt; 更为干净&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4126fd2328e7484c2f21d905e8ae694800311e37" translate="yes" xml:space="preserve">
          <source>This op also supports decoding PNGs and non-animated GIFs since the interface is the same, though it is cleaner to use &lt;a href=&quot;decode_image&quot;&gt;&lt;code&gt;tf.io.decode_image&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">由于接口相同，该操作还支持解码PNG和非动画GIF，尽管使用&lt;a href=&quot;decode_image&quot;&gt; &lt;code&gt;tf.io.decode_image&lt;/code&gt; 更为干净&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="fd4491d1a6601d1e84570bcd363e85b0e7d5c12e" translate="yes" xml:space="preserve">
          <source>This op assumes that there is at least one id for each row in the dense tensor represented by sp_ids (i.e. there are no rows with empty features), and that all the indices of sp_ids are in canonical row-major order.</source>
          <target state="translated">这个操作假设在sp_ids所代表的密张量中,每行至少有一个id(即没有空特征的行),而且sp_ids的所有索引都是按规范的行主序排列。</target>
        </trans-unit>
        <trans-unit id="ff99bdd028bae5711c127316ae24a743c30a9349" translate="yes" xml:space="preserve">
          <source>This op can be substantially more efficient than &lt;a href=&quot;case&quot;&gt;&lt;code&gt;tf.case&lt;/code&gt;&lt;/a&gt; when exactly one branch will be selected. &lt;a href=&quot;switch_case&quot;&gt;&lt;code&gt;tf.switch_case&lt;/code&gt;&lt;/a&gt; is more like a C++ switch/case statement than &lt;a href=&quot;case&quot;&gt;&lt;code&gt;tf.case&lt;/code&gt;&lt;/a&gt;, which is more like an if/elif/elif/else chain.</source>
          <target state="translated">当只选择一个分支时，此op可能比&lt;a href=&quot;case&quot;&gt; &lt;code&gt;tf.case&lt;/code&gt; &lt;/a&gt;效率更高。&lt;a href=&quot;switch_case&quot;&gt; &lt;code&gt;tf.switch_case&lt;/code&gt; &lt;/a&gt;比&lt;a href=&quot;case&quot;&gt; &lt;code&gt;tf.case&lt;/code&gt; &lt;/a&gt;更像是C ++ switch / case语句，后者更像是if / elif / elif / else链。</target>
        </trans-unit>
        <trans-unit id="88d4e65ada667e4924bd61aa913d0b719a3489bc" translate="yes" xml:space="preserve">
          <source>This op can be used to override the gradient for complicated functions. For example, suppose y = f(x) and we wish to apply a custom function g for backprop such that dx = g(dy). In Python,</source>
          <target state="translated">这个操作可以用来覆盖复杂函数的梯度。例如,假设 y=f(x),我们希望为 backprop 应用一个自定义函数 g,使 dx=g(dy)。在 Python 中。</target>
        </trans-unit>
        <trans-unit id="a70d4bf94e1aa31fec2daa089308a302909ea19e" translate="yes" xml:space="preserve">
          <source>This op collects patches from the input image, as if applying a convolution. All extracted patches are stacked in the depth (last) dimension of the output.</source>
          <target state="translated">这个操作从输入图像中收集补丁,就像应用卷积一样。所有提取的补丁都在输出的深度(最后)维度上堆叠。</target>
        </trans-unit>
        <trans-unit id="b607b039f446accd9cc9d66850f0e1a0f18bc805" translate="yes" xml:space="preserve">
          <source>This op consumes a lock created by &lt;code&gt;MutexLock&lt;/code&gt;.</source>
          <target state="translated">该操作使用 &lt;code&gt;MutexLock&lt;/code&gt; 创建的锁。</target>
        </trans-unit>
        <trans-unit id="6c18dad5d5d6899466a10670d53475ca56625c85" translate="yes" xml:space="preserve">
          <source>This op converts between data types, scaling the values appropriately before casting.</source>
          <target state="translated">这个操作可以在数据类型之间进行转换,在投射之前适当地缩放数值。</target>
        </trans-unit>
        <trans-unit id="cc622534ae3fa62514818ea7bebb60dccd64c26d" translate="yes" xml:space="preserve">
          <source>This op creates a &lt;a href=&quot;https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto&quot;&gt;&lt;code&gt;Summary&lt;/code&gt;&lt;/a&gt; protocol buffer that contains the union of all the values in the input summaries.</source>
          <target state="translated">此操作创建一个&lt;a href=&quot;https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto&quot;&gt; &lt;code&gt;Summary&lt;/code&gt; &lt;/a&gt;协议缓冲区，其中包含输入摘要中所有值的并集。</target>
        </trans-unit>
        <trans-unit id="9d0ac8a3b320dffdcce993580747b2ce812c1930" translate="yes" xml:space="preserve">
          <source>This op creates a hash table, specifying the type of its keys and values. Before using the table you will have to initialize it. After initialization the table will be immutable.</source>
          <target state="translated">这个操作可以创建一个哈希表,指定其键和值的类型。在使用该表之前,你必须先初始化它。初始化后,该表将是不可更改的。</target>
        </trans-unit>
        <trans-unit id="656267a02d31eb1fe3e2f93fe32ae0ecba89ac70" translate="yes" xml:space="preserve">
          <source>This op creates a mutable hash table, specifying the type of its keys and values. Each value must be a scalar. Data can be inserted into the table using the insert operations. It does not support the initialization operation.</source>
          <target state="translated">这个操作创建了一个可突变的哈希表,指定其键和值的类型。每个值必须是一个标量。可以使用插入操作将数据插入到表中。它不支持初始化操作。</target>
        </trans-unit>
        <trans-unit id="889ef924ddf97837ebe886303d19b68a45247ddd" translate="yes" xml:space="preserve">
          <source>This op creates a mutable hash table, specifying the type of its keys and values. Each value must be a vector. Data can be inserted into the table using the insert operations. It does not support the initialization operation.</source>
          <target state="translated">这个操作创建了一个可突变的哈希表,指定其键和值的类型。每个值必须是一个向量。可以使用插入操作将数据插入到表中。它不支持初始化操作。</target>
        </trans-unit>
        <trans-unit id="26981876c06a781e2a944a631697cc4482c0e90e" translate="yes" xml:space="preserve">
          <source>This op cuts a rectangular part out of &lt;code&gt;image&lt;/code&gt;. The top-left corner of the returned image is at &lt;code&gt;offset_height, offset_width&lt;/code&gt; in &lt;code&gt;image&lt;/code&gt;, and its lower-right corner is at &lt;code&gt;offset_height + target_height, offset_width + target_width&lt;/code&gt;.</source>
          <target state="translated">该操作会从 &lt;code&gt;image&lt;/code&gt; 切出一个矩形部分。返回的图像的左上角是 &lt;code&gt;offset_height, offset_width&lt;/code&gt; 的 &lt;code&gt;image&lt;/code&gt; ，它的右下角是 &lt;code&gt;offset_height + target_height, offset_width + target_width&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b6b4a71613ffb1856f3dff1fb3fb7c9c68b7515a" translate="yes" xml:space="preserve">
          <source>This op decompresses each element of the &lt;code&gt;bytes&lt;/code&gt; input &lt;code&gt;Tensor&lt;/code&gt;, which is assumed to be compressed using the given &lt;code&gt;compression_type&lt;/code&gt;.</source>
          <target state="translated">此op解压缩 &lt;code&gt;bytes&lt;/code&gt; 输入 &lt;code&gt;Tensor&lt;/code&gt; 的每个元素，假定使用给定的 &lt;code&gt;compression_type&lt;/code&gt; 对其进行压缩。</target>
        </trans-unit>
        <trans-unit id="3ca6b920060424a35214647a956bfc883925dbdc" translate="yes" xml:space="preserve">
          <source>This op determines the maximum scale_factor that would map the initial [input_min, input_max] range to a range that lies within the representable quantized range.</source>
          <target state="translated">这个操作决定了最大的scale_factor,它可以将初始[input_min,input_max]范围映射到位于可表示量化范围内的范围。</target>
        </trans-unit>
        <trans-unit id="9e56e06059315b7dda1620664824a84ff8d7bc3c" translate="yes" xml:space="preserve">
          <source>This op does not &lt;a href=&quot;https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html&quot;&gt;broadcast&lt;/a&gt; its inputs. If you need broadcasting, use &lt;a href=&quot;add&quot;&gt;&lt;code&gt;tf.math.add&lt;/code&gt;&lt;/a&gt; (or the &lt;code&gt;+&lt;/code&gt; operator) instead.</source>
          <target state="translated">该操作不&lt;a href=&quot;https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html&quot;&gt;广播&lt;/a&gt;其输入。如果需要广播，请改用&lt;a href=&quot;add&quot;&gt; &lt;code&gt;tf.math.add&lt;/code&gt; &lt;/a&gt;（或 &lt;code&gt;+&lt;/code&gt; 运算符）。</target>
        </trans-unit>
        <trans-unit id="32ac8411f8421961cf71053e4e574031251a142f" translate="yes" xml:space="preserve">
          <source>This op does nothing if &lt;code&gt;offset_*&lt;/code&gt; is zero and the image already has size &lt;code&gt;target_height&lt;/code&gt; by &lt;code&gt;target_width&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;offset_*&lt;/code&gt; 为零且图像的大小为 &lt;code&gt;target_height&lt;/code&gt; x &lt;code&gt;target_width&lt;/code&gt; ,则此操作不执行任何操作。</target>
        </trans-unit>
        <trans-unit id="59b86b1f9adf8ca0901c6f9e9aeaabfe44733a8c" translate="yes" xml:space="preserve">
          <source>This op exists to consume a tensor created by &lt;code&gt;MutexLock&lt;/code&gt; (other than direct control dependencies). It should be the only that consumes the tensor, and will raise an error if it is not. Its only purpose is to keep the mutex lock tensor alive until it is consumed by this op.</source>
          <target state="translated">存在此操作是为了消耗 &lt;code&gt;MutexLock&lt;/code&gt; 创建的张量（除了直接控制依赖项之外）。它应该是唯一消耗张量的，如果不消耗张量，则会引发错误。它的唯一目的是保持互斥锁张量保持活动状态，直到被该op占用为止。</target>
        </trans-unit>
        <trans-unit id="fd2475b0227c59a4b9b579ebadb0eb003bb5c048" translate="yes" xml:space="preserve">
          <source>This op expects to receive audio data as an input, stored as floats in the range -1 to 1, together with a window width in samples, and a stride specifying how far to move the window between slices. From this it generates a three dimensional output. The first dimension is for the channels in the input, so a stereo audio input would have two here for example. The second dimension is time, with successive frequency slices. The third dimension has an amplitude value for each frequency during that time slice.</source>
          <target state="translated">该操作期望接收音频数据作为输入,存储为-1到1范围内的浮点数,以及以采样为单位的窗口宽度和指定窗口在切片之间移动多远的步幅。由此产生一个三维的输出。第一个维度是输入的通道,例如立体声音频输入会有两个通道。第二个维度是时间,有连续的频率切片。第三个维度是该时间片内每个频率的振幅值。</target>
        </trans-unit>
        <trans-unit id="25438ddc9615e500386687f63662f19536c3cd65" translate="yes" xml:space="preserve">
          <source>This op first slices &lt;code&gt;input&lt;/code&gt; along the dimension &lt;code&gt;batch_axis&lt;/code&gt;, and for each slice &lt;code&gt;i&lt;/code&gt;, reverses the first &lt;code&gt;seq_lengths[i]&lt;/code&gt; elements along the dimension &lt;code&gt;seq_axis&lt;/code&gt;.</source>
          <target state="translated">此运算第一切片 &lt;code&gt;input&lt;/code&gt; 沿着维度 &lt;code&gt;batch_axis&lt;/code&gt; ，并且对于每个切片 &lt;code&gt;i&lt;/code&gt; ，反转第一 &lt;code&gt;seq_lengths[i]&lt;/code&gt; 沿着维度元素 &lt;code&gt;seq_axis&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5c2da47231c9225efc906681c480ffa9fdcb8257" translate="yes" xml:space="preserve">
          <source>This op first slices &lt;code&gt;input&lt;/code&gt; along the dimension &lt;code&gt;batch_dim&lt;/code&gt;, and for each slice &lt;code&gt;i&lt;/code&gt;, reverses the first &lt;code&gt;seq_lengths[i]&lt;/code&gt; elements along the dimension &lt;code&gt;seq_dim&lt;/code&gt;.</source>
          <target state="translated">此运算第一切片 &lt;code&gt;input&lt;/code&gt; 沿着维度 &lt;code&gt;batch_dim&lt;/code&gt; ，并且对于每个切片 &lt;code&gt;i&lt;/code&gt; ，反转第一 &lt;code&gt;seq_lengths[i]&lt;/code&gt; 沿着维度元素 &lt;code&gt;seq_dim&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4fb605b6d0eb10d714ac2254180881c3166d465a" translate="yes" xml:space="preserve">
          <source>This op implements the CTC loss as presented in (Graves et al., 2006).</source>
          <target state="translated">该操作实现了Graves等人在2006年提出的CTC损失。</target>
        </trans-unit>
        <trans-unit id="0d8869736a8a0caa172690fd78df7a0c80acf40c" translate="yes" xml:space="preserve">
          <source>This op implements the CTC loss as presented in (Graves et al., 2016).</source>
          <target state="translated">这个操作实现了(Graves等人,2016)中提出的CTC损失。</target>
        </trans-unit>
        <trans-unit id="d7225b3ea22ec4deb1adfd4a695bd1a7623e154b" translate="yes" xml:space="preserve">
          <source>This op implements the CTC loss as presented in the article:</source>
          <target state="translated">该操作实现了文章中介绍的四氯化碳损失。</target>
        </trans-unit>
        <trans-unit id="6fcde341bc16f91f21275b0c9eb0d836f06faca1" translate="yes" xml:space="preserve">
          <source>This op inserts a single entry for every row that doesn't have any values. The index is created as &lt;code&gt;[row, 0, ..., 0]&lt;/code&gt; and the inserted value is &lt;code&gt;default_value&lt;/code&gt;.</source>
          <target state="translated">该操作为没有任何值的每一行插入一个条目。索引创建为 &lt;code&gt;[row, 0, ..., 0]&lt;/code&gt; ，插入的值为 &lt;code&gt;default_value&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2221b86f1776b0a82d12fa22eaebb9f19fcdaeda" translate="yes" xml:space="preserve">
          <source>This op is a convenience wrapper around &lt;code&gt;sparse_to_dense&lt;/code&gt; for &lt;code&gt;SparseTensor&lt;/code&gt;s.</source>
          <target state="translated">此操作是 &lt;code&gt;sparse_to_dense&lt;/code&gt; 的 &lt;code&gt;SparseTensor&lt;/code&gt; 的便利包装。</target>
        </trans-unit>
        <trans-unit id="9bcf7c2af4594b54e144cbf824cb1c395a0dff7e" translate="yes" xml:space="preserve">
          <source>This op is being phased out in favor of TensorSummaryV2, which lets callers pass a tag as well as a serialized SummaryMetadata proto string that contains plugin-specific data. We will keep this op to maintain backwards compatibility.</source>
          <target state="translated">这个操作将被淘汰,转而使用TensorSummaryV2,它允许调用者传递一个标签以及一个序列化的包含插件特定数据的SummaryMetadata proto字符串。我们将保留这个操作以保持向后的兼容性。</target>
        </trans-unit>
        <trans-unit id="498581ecde1f847b44ae5faacdf090686c5a82aa" translate="yes" xml:space="preserve">
          <source>This op is conceptually identical to,</source>
          <target state="translated">该行动在概念上与:</target>
        </trans-unit>
        <trans-unit id="e11c482ba528e17a7aaf9d69ed66f29f71a93708" translate="yes" xml:space="preserve">
          <source>This op is deprecated and will be removed in the future. Prefer &lt;a href=&quot;../nn/batch_normalization&quot;&gt;&lt;code&gt;tf.nn.batch_normalization&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">此操作已弃用，以后将被删除。更喜欢&lt;a href=&quot;../nn/batch_normalization&quot;&gt; &lt;code&gt;tf.nn.batch_normalization&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="939958da045f886ccce510657fd3f21292de978d" translate="yes" xml:space="preserve">
          <source>This op is deprecated. Prefer &lt;a href=&quot;../nn/batch_normalization&quot;&gt;&lt;code&gt;tf.nn.batch_normalization&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">此操作已弃用。更喜欢&lt;a href=&quot;../nn/batch_normalization&quot;&gt; &lt;code&gt;tf.nn.batch_normalization&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="d1fddd748b0417b58ea1729816edada8cf15be10" translate="yes" xml:space="preserve">
          <source>This op is deprecated. See &lt;a href=&quot;../../../nn/batch_normalization&quot;&gt;&lt;code&gt;tf.nn.batch_normalization&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">不推荐使用此操作。请参阅&lt;a href=&quot;../../../nn/batch_normalization&quot;&gt; &lt;code&gt;tf.nn.batch_normalization&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="faec970c7a84c7f84287e5993964bd296d88974e" translate="yes" xml:space="preserve">
          <source>This op is deprecated. See &lt;a href=&quot;../nn/batch_normalization&quot;&gt;&lt;code&gt;tf.nn.batch_normalization&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">此操作已弃用。请参阅&lt;a href=&quot;../nn/batch_normalization&quot;&gt; &lt;code&gt;tf.nn.batch_normalization&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="350e179bf73709c36862e34c13946cb30d6be41a" translate="yes" xml:space="preserve">
          <source>This op is deprecated. See &lt;a href=&quot;batch_normalization&quot;&gt;&lt;code&gt;tf.nn.batch_normalization&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">不推荐使用此操作。请参阅&lt;a href=&quot;batch_normalization&quot;&gt; &lt;code&gt;tf.nn.batch_normalization&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="f5f21db00ede271aa47c92ca1afb2548ebae0215" translate="yes" xml:space="preserve">
          <source>This op is equivalent to applying the normal &lt;a href=&quot;../nn/softmax&quot;&gt;&lt;code&gt;tf.nn.softmax()&lt;/code&gt;&lt;/a&gt; to each innermost logical submatrix with shape &lt;code&gt;[B, C]&lt;/code&gt;, but with the catch that &lt;em&gt;the implicitly zero elements do not participate&lt;/em&gt;. Specifically, the algorithm is equivalent to the following:</source>
          <target state="translated">此操作等效于将普通的&lt;a href=&quot;../nn/softmax&quot;&gt; &lt;code&gt;tf.nn.softmax()&lt;/code&gt; &lt;/a&gt;应用于形状为 &lt;code&gt;[B, C]&lt;/code&gt; 每个最里面的逻辑子矩阵，但要注意的&lt;em&gt;是隐式零元素不参与&lt;/em&gt;。具体来说，该算法等效于以下内容：</target>
        </trans-unit>
        <trans-unit id="bfe73bfb3165baeb1cd799227e5df39e5edc40e8" translate="yes" xml:space="preserve">
          <source>This op is equivalent to applying the normal &lt;a href=&quot;../nn/softmax&quot;&gt;&lt;code&gt;tf.nn.softmax()&lt;/code&gt;&lt;/a&gt; to each innermost logical submatrix with shape &lt;code&gt;[B, C]&lt;/code&gt;, but with the catch that &lt;em&gt;the implicitly zero elements do not participate&lt;/em&gt;. Specifically, the algorithm is equivalent to:</source>
          <target state="translated">此操作等效于将普通的&lt;a href=&quot;../nn/softmax&quot;&gt; &lt;code&gt;tf.nn.softmax()&lt;/code&gt; &lt;/a&gt;应用于形状为 &lt;code&gt;[B, C]&lt;/code&gt; 每个最里面的逻辑子矩阵，但要注意的&lt;em&gt;是隐式零元素不参与&lt;/em&gt;。具体来说，该算法等效于：</target>
        </trans-unit>
        <trans-unit id="9f3c57195c3a48d6087d2de6a616b3e7ac7825fb" translate="yes" xml:space="preserve">
          <source>This op is hidden from public in Python. It is used by TensorFlow Debugger to register gradient tensors for gradient debugging. This op operates on non-reference-type tensors.</source>
          <target state="translated">这个操作在Python中是不公开的。它被TensorFlow Debugger用来注册梯度时值,用于梯度调试。这个操作对非参考类型的渐变器进行操作。</target>
        </trans-unit>
        <trans-unit id="5208058dc424b56d5683c09cc9ca79f806771d57" translate="yes" xml:space="preserve">
          <source>This op is hidden from public in Python. It is used by TensorFlow Debugger to register gradient tensors for gradient debugging. This op operates on reference-type tensors.</source>
          <target state="translated">这个操作在Python中是不公开的。它被TensorFlow Debugger用来注册梯度时值,用于梯度调试。这个操作对参考型的渐变器进行操作。</target>
        </trans-unit>
        <trans-unit id="42f43d3073e8761b09396618e2f19e046d1dc61b" translate="yes" xml:space="preserve">
          <source>This op is meant only for debugging / testing, and its interface is not expected to be stable.</source>
          <target state="translated">此操作仅用于调试/测试,其接口预计不会稳定。</target>
        </trans-unit>
        <trans-unit id="cf3e5eda158a5f12c2f2da1b9b93833d5ff71185" translate="yes" xml:space="preserve">
          <source>This op is only defined for complex matrices. If A is positive-definite and real, then casting to a complex matrix, taking the logarithm and casting back to a real matrix will give the correct result.</source>
          <target state="translated">这个操作只定义在复数矩阵上。如果A是正定数且为实数,那么将A转换为复数矩阵,取对数后再转换为实数矩阵,将得到正确的结果。</target>
        </trans-unit>
        <trans-unit id="b4088dd89f372d9134efcda4f21dccb2a4e43e51" translate="yes" xml:space="preserve">
          <source>This op is similar to &lt;code&gt;tf.strings.decode(...)&lt;/code&gt;, but it also returns the start offset for each character in its respective string. This information can be used to align the characters with the original byte sequence.</source>
          <target state="translated">此操作类似于 &lt;code&gt;tf.strings.decode(...)&lt;/code&gt; ，但它还会返回其各自字符串中每个字符的起始偏移量。此信息可用于将字符与原始字节序列对齐。</target>
        </trans-unit>
        <trans-unit id="231193f912eb9ecede37b6d2bd24ac7965dab82e" translate="yes" xml:space="preserve">
          <source>This op is used as a placeholder in If branch functions. It doesn't provide a</source>
          <target state="translated">这个操作在If分支函数中作为占位符使用。它不提供一个</target>
        </trans-unit>
        <trans-unit id="30dc65726d5913cc5cf094798e27be8207b68832" translate="yes" xml:space="preserve">
          <source>This op is used during session initialization when a Scaffold is initialized without specifying the local_init_op arg. It includes &lt;a href=&quot;../local_variables_initializer&quot;&gt;&lt;code&gt;tf.compat.v1.local_variables_initializer&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../tables_initializer&quot;&gt;&lt;code&gt;tf.compat.v1.tables_initializer&lt;/code&gt;&lt;/a&gt;, and also initializes local session resources.</source>
          <target state="translated">在初始化脚手架而不指定local_init_op arg的会话初始化期间，将使用此op。它包括&lt;a href=&quot;../local_variables_initializer&quot;&gt; &lt;code&gt;tf.compat.v1.local_variables_initializer&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;../tables_initializer&quot;&gt; &lt;code&gt;tf.compat.v1.tables_initializer&lt;/code&gt; &lt;/a&gt;，并且还初始化本地会话资源。</target>
        </trans-unit>
        <trans-unit id="27d759b10e3031b72c69cecb21d0a5f2a1e56b50" translate="yes" xml:space="preserve">
          <source>This op is used together with &lt;code&gt;Exit&lt;/code&gt; to create loops in the graph. The unique &lt;code&gt;frame_name&lt;/code&gt; is used by the &lt;code&gt;Executor&lt;/code&gt; to identify frames. If &lt;code&gt;is_constant&lt;/code&gt; is true, &lt;code&gt;output&lt;/code&gt; is a constant in the child frame; otherwise it may be changed in the child frame. At most &lt;code&gt;parallel_iterations&lt;/code&gt; iterations are run in parallel in the child frame.</source>
          <target state="translated">此op与 &lt;code&gt;Exit&lt;/code&gt; 一起使用可在图形中创建循环。独特的 &lt;code&gt;frame_name&lt;/code&gt; 用于由 &lt;code&gt;Executor&lt;/code&gt; 识别帧。如果 &lt;code&gt;is_constant&lt;/code&gt; 为true，则在子帧中 &lt;code&gt;output&lt;/code&gt; 为常量；否则，输出为false 。否则可能会在子框架中更改。最多 &lt;code&gt;parallel_iterations&lt;/code&gt; 迭代在子框架中并行运行。</target>
        </trans-unit>
        <trans-unit id="4d539ed4cc2d3820ae7fed70f08ee95780da8755" translate="yes" xml:space="preserve">
          <source>This op may use some OS-provided source of non-determinism (e.g. an RNG), so each execution will give different results.</source>
          <target state="translated">这个操作可能会使用一些操作系统提供的非确定性源(如RNG),所以每次执行都会得到不同的结果。</target>
        </trans-unit>
        <trans-unit id="7494eb2e4d00b6f1a2a80a896b2a97c11ea84bbb" translate="yes" xml:space="preserve">
          <source>This op only parses the image header, so it is much faster than DecodeJpeg.</source>
          <target state="translated">这个操作只解析图像头,所以比DecodeJpeg快很多。</target>
        </trans-unit>
        <trans-unit id="256678fd9ba7a89e98c94dad29bebb27865a6123" translate="yes" xml:space="preserve">
          <source>This op parses a serialized sequence example into a tuple of dictionaries, each mapping keys to &lt;code&gt;Tensor&lt;/code&gt; and &lt;code&gt;SparseTensor&lt;/code&gt; objects. The first dictionary contains mappings for keys appearing in &lt;code&gt;context_features&lt;/code&gt;, and the second dictionary contains mappings for keys appearing in &lt;code&gt;sequence_features&lt;/code&gt;.</source>
          <target state="translated">该操作将序列化的序列示例解析为字典的元组，每个映射键都映射到 &lt;code&gt;Tensor&lt;/code&gt; 和 &lt;code&gt;SparseTensor&lt;/code&gt; 对象。第一个字典包含出现在 &lt;code&gt;context_features&lt;/code&gt; 中的键的映射，第二个字典包含出现在 &lt;code&gt;sequence_features&lt;/code&gt; 中的键的映射。</target>
        </trans-unit>
        <trans-unit id="b74d19b6cde7e379b7fc76473e78e58968efe064" translate="yes" xml:space="preserve">
          <source>This op parses serialized examples into a dictionary mapping keys to &lt;code&gt;Tensor&lt;/code&gt;, &lt;code&gt;SparseTensor&lt;/code&gt;, and &lt;code&gt;RaggedTensor&lt;/code&gt; objects. &lt;code&gt;features&lt;/code&gt; is a dict from keys to &lt;code&gt;VarLenFeature&lt;/code&gt;, &lt;code&gt;RaggedFeature&lt;/code&gt;, &lt;code&gt;SparseFeature&lt;/code&gt;, and &lt;code&gt;FixedLenFeature&lt;/code&gt; objects. Each &lt;code&gt;VarLenFeature&lt;/code&gt; and &lt;code&gt;SparseFeature&lt;/code&gt; is mapped to a &lt;code&gt;SparseTensor&lt;/code&gt;; each &lt;code&gt;RaggedFeature&lt;/code&gt; is mapped to a &lt;code&gt;RaggedTensor&lt;/code&gt;; and each &lt;code&gt;FixedLenFeature&lt;/code&gt; is mapped to a &lt;code&gt;Tensor&lt;/code&gt;. See &lt;a href=&quot;../../io/parse_example&quot;&gt;&lt;code&gt;tf.io.parse_example&lt;/code&gt;&lt;/a&gt; for more details about feature dictionaries.</source>
          <target state="translated">该操作将序列化的示例解析为字典，将键映射到 &lt;code&gt;Tensor&lt;/code&gt; ， &lt;code&gt;SparseTensor&lt;/code&gt; 和 &lt;code&gt;RaggedTensor&lt;/code&gt; 对象。 &lt;code&gt;features&lt;/code&gt; 是从 &lt;code&gt;VarLenFeature&lt;/code&gt; ， &lt;code&gt;RaggedFeature&lt;/code&gt; ， &lt;code&gt;SparseFeature&lt;/code&gt; 和 &lt;code&gt;FixedLenFeature&lt;/code&gt; 对象的键决定的。每个 &lt;code&gt;VarLenFeature&lt;/code&gt; 和 &lt;code&gt;SparseFeature&lt;/code&gt; 都映射到一个 &lt;code&gt;SparseTensor&lt;/code&gt; ; 每个 &lt;code&gt;RaggedFeature&lt;/code&gt; 都映射到一个 &lt;code&gt;RaggedTensor&lt;/code&gt; ；并且每个 &lt;code&gt;FixedLenFeature&lt;/code&gt; 都映射到一个 &lt;code&gt;Tensor&lt;/code&gt; 。参见&lt;a href=&quot;../../io/parse_example&quot;&gt; &lt;code&gt;tf.io.parse_example&lt;/code&gt; &lt;/a&gt; 有关功能词典的更多详细信息。</target>
        </trans-unit>
        <trans-unit id="845b399d2e9c5818b169dacff650d84da1b8a72a" translate="yes" xml:space="preserve">
          <source>This op parses serialized examples into a dictionary mapping keys to &lt;code&gt;Tensor&lt;/code&gt;&lt;code&gt;SparseTensor&lt;/code&gt;, and &lt;code&gt;RaggedTensor&lt;/code&gt; objects. &lt;code&gt;features&lt;/code&gt; is a dict from keys to &lt;code&gt;VarLenFeature&lt;/code&gt;, &lt;code&gt;SparseFeature&lt;/code&gt;, &lt;code&gt;RaggedFeature&lt;/code&gt;, and &lt;code&gt;FixedLenFeature&lt;/code&gt; objects. Each &lt;code&gt;VarLenFeature&lt;/code&gt; and &lt;code&gt;SparseFeature&lt;/code&gt; is mapped to a &lt;code&gt;SparseTensor&lt;/code&gt;; each &lt;code&gt;FixedLenFeature&lt;/code&gt; is mapped to a &lt;code&gt;Tensor&lt;/code&gt;; and each &lt;code&gt;RaggedFeature&lt;/code&gt; is mapped to a &lt;code&gt;RaggedTensor&lt;/code&gt;.</source>
          <target state="translated">该操作将序列化的示例解析为字典，将键映射到 &lt;code&gt;Tensor&lt;/code&gt; &lt;code&gt;SparseTensor&lt;/code&gt; 和 &lt;code&gt;RaggedTensor&lt;/code&gt; 对象。 &lt;code&gt;features&lt;/code&gt; 是从 &lt;code&gt;VarLenFeature&lt;/code&gt; ， &lt;code&gt;SparseFeature&lt;/code&gt; ， &lt;code&gt;RaggedFeature&lt;/code&gt; 和 &lt;code&gt;FixedLenFeature&lt;/code&gt; 对象的键决定的。每个 &lt;code&gt;VarLenFeature&lt;/code&gt; 和 &lt;code&gt;SparseFeature&lt;/code&gt; 都映射到一个 &lt;code&gt;SparseTensor&lt;/code&gt; ;每个 &lt;code&gt;FixedLenFeature&lt;/code&gt; 都映射到一个 &lt;code&gt;Tensor&lt;/code&gt; ；并且每个 &lt;code&gt;RaggedFeature&lt;/code&gt; 都映射到 &lt;code&gt;RaggedTensor&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="787b46fef668ea2442d8cf59e886173e8f1beeb1" translate="yes" xml:space="preserve">
          <source>This op parses serialized sequence examples into a tuple of dictionaries, each mapping keys to &lt;code&gt;Tensor&lt;/code&gt; and &lt;code&gt;SparseTensor&lt;/code&gt; objects. The first dictionary contains mappings for keys appearing in &lt;code&gt;context_features&lt;/code&gt;, and the second dictionary contains mappings for keys appearing in &lt;code&gt;sequence_features&lt;/code&gt;.</source>
          <target state="translated">该操作将序列化的序列示例解析为字典的元组，每个映射键均指向 &lt;code&gt;Tensor&lt;/code&gt; 和 &lt;code&gt;SparseTensor&lt;/code&gt; 对象。第一个字典包含出现在 &lt;code&gt;context_features&lt;/code&gt; 中的键的映射，第二个字典包含出现在 &lt;code&gt;sequence_features&lt;/code&gt; 中的键的映射。</target>
        </trans-unit>
        <trans-unit id="4fe7f65a801aad08671f34a64293fcb18c9780f3" translate="yes" xml:space="preserve">
          <source>This op produces Region of Interests from given bounding boxes(bbox_deltas) encoded wrt anchors according to eq.2 in arXiv:1506.01497</source>
          <target state="translated">该操作根据arXiv:1506.01497中的eq.2,从给定的边界盒(bbox_deltas)中产生利益区域,并对锚进行编码。</target>
        </trans-unit>
        <trans-unit id="4ca1b1701aa5c064253c44ce1aad741cb653df7b" translate="yes" xml:space="preserve">
          <source>This op reports an &lt;code&gt;InvalidArgument&lt;/code&gt; error if any value is not finite.</source>
          <target state="translated">如果任何值不是有限的，此操作会报告一个 &lt;code&gt;InvalidArgument&lt;/code&gt; 错误。</target>
        </trans-unit>
        <trans-unit id="a524bdef26722689a646035ff3a30a89b59c896f" translate="yes" xml:space="preserve">
          <source>This op runs in &lt;code&gt;O(M log M)&lt;/code&gt; time, where &lt;code&gt;M&lt;/code&gt; is the total number of non-empty values across all inputs. This is due to the need for an internal sort in order to concatenate efficiently across an arbitrary dimension.</source>
          <target state="translated">该运算以 &lt;code&gt;O(M log M)&lt;/code&gt; 时间运行，其中 &lt;code&gt;M&lt;/code&gt; 是所有输入中非空值的总数。这是由于需要内部排序以便在任意维度上有效地进行串联。</target>
        </trans-unit>
        <trans-unit id="7aad9bcdb175c316d0cd0c518023c08cbf3d7ccf" translate="yes" xml:space="preserve">
          <source>This op simply returns its first input, which is assumed to have been sliced from the Tensors returned by TPUEmbeddingDequeueActivations. The presence of this op, and its first argument being a trainable Variable, enables automatic differentiation of graphs containing embeddings via the TPU Embedding Python libraries.</source>
          <target state="translated">这个操作简单地返回它的第一个输入,它被认为是从 TPUEmbeddingDequeueActivations 返回的 Tensors 中切分出来的。这个 op 的存在,以及它的第一个参数是一个可训练的变量,可以通过 TPU Embedding Python 库自动区分包含嵌入的图。</target>
        </trans-unit>
        <trans-unit id="d483631861e7a19423d30b1c89ca7bd9af9deb77" translate="yes" xml:space="preserve">
          <source>This op simulates the precision loss from the quantized forward pass by:</source>
          <target state="translated">该操作模拟量化前传的精度损失。</target>
        </trans-unit>
        <trans-unit id="82f92515415e66dd8fe55e82e86a429d52767b5f" translate="yes" xml:space="preserve">
          <source>This op takes an N-dimensional &lt;code&gt;Tensor&lt;/code&gt;, &lt;code&gt;RaggedTensor&lt;/code&gt;, or &lt;code&gt;SparseTensor&lt;/code&gt;, and returns an N-dimensional int64 SparseTensor where element &lt;code&gt;[i0...i[axis], j]&lt;/code&gt; contains the number of times the value &lt;code&gt;j&lt;/code&gt; appears in slice &lt;code&gt;[i0...i[axis], :]&lt;/code&gt; of the input tensor. Currently, only N=0 and N=-1 are supported.</source>
          <target state="translated">此op接受N维 &lt;code&gt;Tensor&lt;/code&gt; ， &lt;code&gt;RaggedTensor&lt;/code&gt; 或 &lt;code&gt;SparseTensor&lt;/code&gt; ，并返回N维int64 SparseTensor，其中元素 &lt;code&gt;[i0...i[axis], j]&lt;/code&gt; 包含值 &lt;code&gt;j&lt;/code&gt; 出现在切片 &lt;code&gt;[i0...i[axis], :]&lt;/code&gt; ]中的次数。 ..i [轴]，：]输入张量。当前，仅支持N = 0和N = -1。</target>
        </trans-unit>
        <trans-unit id="ad5fa7632826618c2d76f2d4a4592c97b627433a" translate="yes" xml:space="preserve">
          <source>This op takes in the upstream gradient w.r.t. non-empty values of the sliced &lt;code&gt;SparseTensor&lt;/code&gt;, and outputs the gradients w.r.t. the non-empty values of input &lt;code&gt;SparseTensor&lt;/code&gt;.</source>
          <target state="translated">此op接受切片的 &lt;code&gt;SparseTensor&lt;/code&gt; 的上游梯度wrt的非空值，并输出梯度 &lt;code&gt;SparseTensor&lt;/code&gt; 到输入SparseTensor的非空值。</target>
        </trans-unit>
        <trans-unit id="edecba4e86376242d273ba622ca63b2965a5a438" translate="yes" xml:space="preserve">
          <source>This op translates a tensor containing Example records, encoded using the &lt;a href=&quot;https://developers.google.com/protocol-buffers/docs/proto3#json&quot;&gt;standard JSON mapping&lt;/a&gt;, into a tensor containing the same records encoded as binary protocol buffers. The resulting tensor can then be fed to any of the other Example-parsing ops.</source>
          <target state="translated">此操作将包含使用&lt;a href=&quot;https://developers.google.com/protocol-buffers/docs/proto3#json&quot;&gt;标准JSON映射&lt;/a&gt;编码的示例记录的张量转换为包含与二进制协议缓冲区编码相同记录的张量。然后可以将所得张量馈送到其他任何示例解析操作。</target>
        </trans-unit>
        <trans-unit id="aed6dcb7e60458715d6ef5c3c8eddc95029d6141" translate="yes" xml:space="preserve">
          <source>This op uses the algorithm by Marsaglia et al. to acquire samples via transformation-rejection from pairs of uniform and normal random variables. See &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=358414&quot;&gt;http://dl.acm.org/citation.cfm?id=358414&lt;/a&gt;</source>
          <target state="translated">该操作使用Marsaglia等人的算法。通过变换拒绝从成对的均匀和正常随机变量中获取样本。参见&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=358414&quot;&gt;http://dl.acm.org/citation.cfm?id=358414&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1c4201e75e2046cbb40d84791ef5b741ce6ab4e0" translate="yes" xml:space="preserve">
          <source>This op uses the algorithm by Marsaglia et al. to acquire samples via transformation-rejection from pairs of uniform and normal random variables. See &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=358414&quot;&gt;http://dl.acm.org/citation.cfm?id=358414&lt;/a&gt;</source>
          <target state="translated">该操作使用Marsaglia等人的算法。通过变换拒绝从成对的均匀和正常随机变量中获取样本。参见&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=358414&quot;&gt;http://dl.acm.org/citation.cfm?id=358414&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c381caf80ba3167793d4166edbbb6bfbc8d1b9b9" translate="yes" xml:space="preserve">
          <source>This op uses two algorithms, depending on rate. If rate &amp;gt;= 10, then the algorithm by Hormann is used to acquire samples via transformation-rejection. See &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/0167668793909974&quot;&gt;http://www.sciencedirect.com/science/article/pii/0167668793909974&lt;/a&gt;</source>
          <target state="translated">此操作根据速率使用两种算法。如果比率&amp;gt; = 10，则使用Hormann的算法通过变换拒绝来获取样本。参见&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/0167668793909974&quot;&gt;http://www.sciencedirect.com/science/article/pii/0167668793909974&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="29fa7aa36b378088557fbcf4052dee5582b27940" translate="yes" xml:space="preserve">
          <source>This op uses two algorithms, depending on rate. If rate &amp;gt;= 10, then the algorithm by Hormann is used to acquire samples via transformation-rejection. See &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/0167668793909974&quot;&gt;http://www.sciencedirect.com/science/article/pii/0167668793909974&lt;/a&gt;</source>
          <target state="translated">此操作根据速率使用两种算法。如果比率&amp;gt; = 10，则使用Hormann的算法通过变换拒绝来获取样本。参见&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/0167668793909974&quot;&gt;http://www.sciencedirect.com/science/article/pii/0167668793909974&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="49b534f6ddaaf2bd4e7772b31b4be9cf92fcfbd7" translate="yes" xml:space="preserve">
          <source>This operation blocks until that finishes.</source>
          <target state="translated">这个操作会被阻止,直到完成。</target>
        </trans-unit>
        <trans-unit id="14d876872d62a220ab5753fd3b852a5a6bcb76ef" translate="yes" xml:space="preserve">
          <source>This operation can be used with &lt;code&gt;output_encoding = input_encoding&lt;/code&gt; to enforce correct formatting for inputs even if they are already in the desired encoding.</source>
          <target state="translated">此操作可以与 &lt;code&gt;output_encoding = input_encoding&lt;/code&gt; 一起使用，以对输入实施正确的格式设置，即使它们已经采用所需的编码了。</target>
        </trans-unit>
        <trans-unit id="bf8af72c7ec5c03a0a302901ae0b02f8840934f4" translate="yes" xml:space="preserve">
          <source>This operation computes</source>
          <target state="translated">这个操作计算的是</target>
        </trans-unit>
        <trans-unit id="df2c8f127a35805203dd552ccd88657669212f89" translate="yes" xml:space="preserve">
          <source>This operation computes the inverse of an index permutation. It takes a 1-D integer tensor &lt;code&gt;x&lt;/code&gt;, which represents the indices of a zero-based array, and swaps each value with its index position. In other words, for an output tensor &lt;code&gt;y&lt;/code&gt; and an input tensor &lt;code&gt;x&lt;/code&gt;, this operation computes the following:</source>
          <target state="translated">此操作计算索引排列的倒数。它采用一维整数张量 &lt;code&gt;x&lt;/code&gt; ，它表示从零开始的数组的索引，并将每个值与其索引位置交换。换句话说，对于输出张量 &lt;code&gt;y&lt;/code&gt; 和输入张量 &lt;code&gt;x&lt;/code&gt; ，此操作计算以下内容：</target>
        </trans-unit>
        <trans-unit id="9ce96baa1d2aec98a8f7f3b28803c0d5bf74ec4e" translate="yes" xml:space="preserve">
          <source>This operation concatenates completed-element component tensors along the 0th dimension to make a single component tensor.</source>
          <target state="translated">这个操作将沿0维的完形元素分量张量连接起来,形成一个单一的分量张量。</target>
        </trans-unit>
        <trans-unit id="d8b4d24b16bc4a84c7216a06caa8665f602e4b2d" translate="yes" xml:space="preserve">
          <source>This operation concatenates queue-element component tensors along the 0th dimension to make a single component tensor. All of the components in the dequeued tuple will have size &lt;code&gt;n&lt;/code&gt; in the 0th dimension.</source>
          <target state="translated">该操作将沿第0维的队列元素分量张量连接起来以形成单个分量张量。已出队元组中的所有组件在第0维上的大小为 &lt;code&gt;n&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="45d38aafbd1acaa44920f79c8c0c3ecc6bbcdeda" translate="yes" xml:space="preserve">
          <source>This operation concatenates queue-element component tensors along the 0th dimension to make a single component tensor. All of the components in the dequeued tuple will have size n in the 0th dimension.</source>
          <target state="translated">这个操作将队列元素的分量张量沿0维连接起来,形成一个分量张量。在去队列元组中的所有分量在第0维中的大小为n。</target>
        </trans-unit>
        <trans-unit id="50e9d3be55bb6834daac8ca12d0b390851428378" translate="yes" xml:space="preserve">
          <source>This operation concatenates queue-element component tensors along the 0th dimension to make a single component tensor. If the queue has not been closed, all of the components in the dequeued tuple will have size &lt;code&gt;n&lt;/code&gt; in the 0th dimension.</source>
          <target state="translated">该操作将沿第0维的队列元素分量张量连接起来以形成单个分量张量。如果尚未关闭队列，则出队元组中的所有组件的第0维的大小为 &lt;code&gt;n&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="15bf39234c37f046f148fb34f3d92a5f21991cd5" translate="yes" xml:space="preserve">
          <source>This operation converts Unicode code points to script codes corresponding to each code point. Script codes correspond to International Components for Unicode (ICU) UScriptCode values. See &lt;a href=&quot;http://icu-project.org/apiref/icu4c/uscript_8h.html&quot;&gt;http://icu-project.org/apiref/icu4c/uscript_8h.html&lt;/a&gt; Returns -1 (USCRIPT_INVALID_CODE) for invalid codepoints. Output shape will match input shape.</source>
          <target state="translated">此操作将Unicode代码点转换为与每个代码点相对应的脚本代码。脚本代码对应于Unicode（ICU）UScriptCode值的国际组件。请参阅&lt;a href=&quot;http://icu-project.org/apiref/icu4c/uscript_8h.html&quot;&gt;http://icu-project.org/apiref/icu4c/uscript_8h.html&lt;/a&gt;对于无效的代码点，返回-1（USCRIPT_INVALID_CODE）。输出形状将与输入形状匹配。</target>
        </trans-unit>
        <trans-unit id="dd35b090ebd1aab32ffe1d4d8ebd3a96d1bfbf70" translate="yes" xml:space="preserve">
          <source>This operation converts Unicode code points to script codes corresponding to each code point. Script codes correspond to International Components for Unicode (ICU) UScriptCode values. See &lt;a href=&quot;https://ssl.icu-project.org/apiref/icu4c/uscript_8h.html&quot;&gt;http://icu-project.org/apiref/icu4c/uscript_8h.html&lt;/a&gt; Returns -1 (USCRIPT_INVALID_CODE) for invalid codepoints. Output shape will match input shape.</source>
          <target state="translated">此操作将Unicode代码点转换为与每个代码点相对应的脚本代码。脚本代码对应于Unicode（ICU）UScriptCode值的国际组件。请参阅&lt;a href=&quot;https://ssl.icu-project.org/apiref/icu4c/uscript_8h.html&quot;&gt;http://icu-project.org/apiref/icu4c/uscript_8h.html&lt;/a&gt;对于无效的代码点，返回-1（USCRIPT_INVALID_CODE）。输出形状将与输入形状匹配。</target>
        </trans-unit>
        <trans-unit id="45295c6769989505efcf8c49b52125eb620acebf" translate="yes" xml:space="preserve">
          <source>This operation converts Unicode code points to script codes corresponding to each code point. Script codes correspond to International Components for Unicode (ICU) UScriptCode values. See http://icu-project.org/apiref/icu4c/uscript_8h.html. Returns -1 (USCRIPT_INVALID_CODE) for invalid codepoints. Output shape will match input shape.</source>
          <target state="translated">该操作将Unicode码点转换为对应每个码点的脚本代码。脚本代码对应于Unicode国际组件(ICU)的UScriptCode值。参见http://icu-project.org/apiref/icu4c/uscript_8h.html。对于无效的代码点,返回-1 (USCRIPT_INVALID_CODE)。输出形状将与输入形状匹配。</target>
        </trans-unit>
        <trans-unit id="46e590d89bb172f52098449de9dab107e8dbb7d2" translate="yes" xml:space="preserve">
          <source>This operation corresponds to &lt;code&gt;numpy.tensordot(a, b, axes)&lt;/code&gt;.</source>
          <target state="translated">此操作对应于 &lt;code&gt;numpy.tensordot(a, b, axes)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="11cc84360970a3caa94b4965487c085edd1841b7" translate="yes" xml:space="preserve">
          <source>This operation creates a new tensor by adding sparse &lt;code&gt;updates&lt;/code&gt; to the passed in &lt;code&gt;tensor&lt;/code&gt;. This operation is very similar to &lt;code&gt;tf.scatter_nd_add&lt;/code&gt;, except that the updates are added onto an existing tensor (as opposed to a variable). If the memory for the existing tensor cannot be re-used, a copy is made and updated.</source>
          <target state="translated">此操作通过将稀疏 &lt;code&gt;updates&lt;/code&gt; 添加到传入的 &lt;code&gt;tensor&lt;/code&gt; 来创建新的张量。此操作与 &lt;code&gt;tf.scatter_nd_add&lt;/code&gt; 非常相似，除了将更新添加到现有张量（而不是变量）上。如果无法重新使用现有张量的内存，则进行复制并更新。</target>
        </trans-unit>
        <trans-unit id="ffa4c89af08b8794e18efa910261002f2ad8a60e" translate="yes" xml:space="preserve">
          <source>This operation creates a new tensor by applying sparse &lt;code&gt;updates&lt;/code&gt; to the passed in &lt;code&gt;tensor&lt;/code&gt;. This operation is very similar to &lt;a href=&quot;../scatter_nd&quot;&gt;&lt;code&gt;tf.scatter_nd&lt;/code&gt;&lt;/a&gt;, except that the updates are scattered onto an existing tensor (as opposed to a zero-tensor). If the memory for the existing tensor cannot be re-used, a copy is made and updated.</source>
          <target state="translated">此操作通过将稀疏 &lt;code&gt;updates&lt;/code&gt; 应用于传入的 &lt;code&gt;tensor&lt;/code&gt; 来创建新的张量。此操作与&lt;a href=&quot;../scatter_nd&quot;&gt; &lt;code&gt;tf.scatter_nd&lt;/code&gt; &lt;/a&gt;非常相似，不同之处在于更新分散在现有张量（与零张量相对）上。如果无法重新使用现有张量的内存，则进行复制并更新。</target>
        </trans-unit>
        <trans-unit id="c390e65b4cc57f97f7e73aea3a4e81f629906485" translate="yes" xml:space="preserve">
          <source>This operation creates a new tensor by applying sparse &lt;code&gt;updates&lt;/code&gt; to the passed in &lt;code&gt;tensor&lt;/code&gt;. This operation is very similar to &lt;a href=&quot;scatter_nd&quot;&gt;&lt;code&gt;tf.scatter_nd&lt;/code&gt;&lt;/a&gt;, except that the updates are scattered onto an existing tensor (as opposed to a zero-tensor). If the memory for the existing tensor cannot be re-used, a copy is made and updated.</source>
          <target state="translated">此操作通过将稀疏 &lt;code&gt;updates&lt;/code&gt; 应用于传入的 &lt;code&gt;tensor&lt;/code&gt; 来创建新的张量。此操作与&lt;a href=&quot;scatter_nd&quot;&gt; &lt;code&gt;tf.scatter_nd&lt;/code&gt; &lt;/a&gt;非常相似，不同之处在于更新分散在现有张量上（而不是零张量）。如果无法重新使用现有张量的内存，则进行复制并更新。</target>
        </trans-unit>
        <trans-unit id="35ada26abd351b9c5bf627e9862c2252c700b1b5" translate="yes" xml:space="preserve">
          <source>This operation creates a new tensor by replicating &lt;code&gt;input&lt;/code&gt;&lt;code&gt;multiples&lt;/code&gt; times. The output tensor's i'th dimension has &lt;code&gt;input.dims(i) * multiples[i]&lt;/code&gt; elements, and the values of &lt;code&gt;input&lt;/code&gt; are replicated &lt;code&gt;multiples[i]&lt;/code&gt; times along the 'i'th dimension. For example, tiling &lt;code&gt;[a b c d]&lt;/code&gt; by &lt;code&gt;[2]&lt;/code&gt; produces &lt;code&gt;[a b c d a b c d]&lt;/code&gt;.</source>
          <target state="translated">此操作创建通过复制一个新的张量 &lt;code&gt;input&lt;/code&gt; &lt;code&gt;multiples&lt;/code&gt; 倍。输出张量的第i个维度具有 &lt;code&gt;input.dims(i) * multiples[i]&lt;/code&gt; 元素，并且 &lt;code&gt;input&lt;/code&gt; 值沿第i个维度重复了multi &lt;code&gt;multiples[i]&lt;/code&gt; 次。例如，将 &lt;code&gt;[a b c d]&lt;/code&gt; 乘以 &lt;code&gt;[2]&lt;/code&gt; 会产生 &lt;code&gt;[a b c d a b c d]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5c90c8c96fbb8b355bef6addc97e2ca668679eab" translate="yes" xml:space="preserve">
          <source>This operation creates a new tensor by subtracting sparse &lt;code&gt;updates&lt;/code&gt; from the passed in &lt;code&gt;tensor&lt;/code&gt;. This operation is very similar to &lt;code&gt;tf.scatter_nd_sub&lt;/code&gt;, except that the updates are subtracted from an existing tensor (as opposed to a variable). If the memory for the existing tensor cannot be re-used, a copy is made and updated.</source>
          <target state="translated">此操作通过从传入的 &lt;code&gt;tensor&lt;/code&gt; 减去稀疏 &lt;code&gt;updates&lt;/code&gt; 来创建新的张量。此操作与 &lt;code&gt;tf.scatter_nd_sub&lt;/code&gt; 非常相似，不同之处在于，从现有张量（与变量相反）中减去更新。如果无法重新使用现有张量的内存，则进行复制并更新。</target>
        </trans-unit>
        <trans-unit id="52687eaf04e092c6367552afd7b0ecbe304d63d1" translate="yes" xml:space="preserve">
          <source>This operation creates a sequence of numbers that begins at &lt;code&gt;start&lt;/code&gt; and extends by increments of &lt;code&gt;delta&lt;/code&gt; up to but not including &lt;code&gt;limit&lt;/code&gt;.</source>
          <target state="translated">此操作创建开始于数字的序列 &lt;code&gt;start&lt;/code&gt; ，并通过增量扩展 &lt;code&gt;delta&lt;/code&gt; 直到但不包括 &lt;code&gt;limit&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="92df0de1d5e9953fcab8fedab1e1918021371c50" translate="yes" xml:space="preserve">
          <source>This operation creates a tensor of &lt;code&gt;shape&lt;/code&gt; and &lt;code&gt;dtype&lt;/code&gt;.</source>
          <target state="translated">此操作将创建 &lt;code&gt;shape&lt;/code&gt; 和 &lt;code&gt;dtype&lt;/code&gt; 的张量。</target>
        </trans-unit>
        <trans-unit id="0aaf5e994752ce058c39711d4638a06e40443f56" translate="yes" xml:space="preserve">
          <source>This operation creates a tensor of shape &lt;code&gt;dims&lt;/code&gt; and fills it with &lt;code&gt;value&lt;/code&gt;.</source>
          <target state="translated">此操作将创建一个形状 &lt;code&gt;dims&lt;/code&gt; 的张量，并用 &lt;code&gt;value&lt;/code&gt; 填充它。</target>
        </trans-unit>
        <trans-unit id="2acd7b755c2b5e055f5081527ae8768ab15af691" translate="yes" xml:space="preserve">
          <source>This operation divides &quot;spatial&quot; dimensions &lt;code&gt;[1, ..., M]&lt;/code&gt; of the input into a grid of blocks of shape &lt;code&gt;block_shape&lt;/code&gt;, and interleaves these blocks with the &quot;batch&quot; dimension (0) such that in the output, the spatial dimensions &lt;code&gt;[1, ..., M]&lt;/code&gt; correspond to the position within the grid, and the batch dimension combines both the position within a spatial block and the original batch position. Prior to division into blocks, the spatial dimensions of the input are optionally zero padded according to &lt;code&gt;paddings&lt;/code&gt;. See below for a precise description.</source>
          <target state="translated">此操作将输入的&amp;ldquo;空间&amp;rdquo;尺寸 &lt;code&gt;[1, ..., M]&lt;/code&gt; 划分为 &lt;code&gt;block_shape&lt;/code&gt; 形状的块的网格，并将这些块与&amp;ldquo; batch&amp;rdquo;尺寸（0）交错，以便在输出中获得空间尺寸 &lt;code&gt;[1, ..., M]&lt;/code&gt; 对应于网格内的位置，并且批次尺寸将空间块内的位置和原始批次位置结合在一起。在划分为块之前，可以根据 &lt;code&gt;paddings&lt;/code&gt; 将输入的空间尺寸可选地零填充。有关详细说明，请参见下文。</target>
        </trans-unit>
        <trans-unit id="41e0f15b392354d26d2756845f1cfcd72200eb6d" translate="yes" xml:space="preserve">
          <source>This operation either returns a tensor &lt;code&gt;y&lt;/code&gt; containing unique elements along the &lt;code&gt;axis&lt;/code&gt; of a tensor. The returned unique elements is sorted in the same order as they occur along &lt;code&gt;axis&lt;/code&gt; in &lt;code&gt;x&lt;/code&gt;. This operation also returns a tensor &lt;code&gt;idx&lt;/code&gt; and a tensor &lt;code&gt;count&lt;/code&gt; that are the same size as the number of the elements in &lt;code&gt;x&lt;/code&gt; along the &lt;code&gt;axis&lt;/code&gt; dimension. The &lt;code&gt;idx&lt;/code&gt; contains the index in the unique output &lt;code&gt;y&lt;/code&gt; and the &lt;code&gt;count&lt;/code&gt; contains the count in the unique output &lt;code&gt;y&lt;/code&gt;. In other words, for an &lt;code&gt;1-D&lt;/code&gt; tensor &lt;code&gt;x&lt;/code&gt; with `axis = None:</source>
          <target state="translated">该操作要么返回一个张量 &lt;code&gt;y&lt;/code&gt; ，该张量y沿着张量的 &lt;code&gt;axis&lt;/code&gt; 包含唯一元素。当它们沿着发生返回的唯一的元件以相同的顺序进行排序 &lt;code&gt;axis&lt;/code&gt; 在 &lt;code&gt;x&lt;/code&gt; 。此操作还返回张量 &lt;code&gt;idx&lt;/code&gt; 和张量 &lt;code&gt;count&lt;/code&gt; ，其大小与 &lt;code&gt;x&lt;/code&gt; 沿 &lt;code&gt;axis&lt;/code&gt; 尺寸的元素数相同。该 &lt;code&gt;idx&lt;/code&gt; 包含了独特的输出指标 &lt;code&gt;y&lt;/code&gt; 和 &lt;code&gt;count&lt;/code&gt; 包含了独特的输出计数 &lt;code&gt;y&lt;/code&gt; 。换句话说，对于一 &lt;code&gt;1-D&lt;/code&gt; 张量 &lt;code&gt;x&lt;/code&gt; 其中`axis = None：</target>
        </trans-unit>
        <trans-unit id="2fb9300ba4c7f909238dc4fbd9e13292c10f4740" translate="yes" xml:space="preserve">
          <source>This operation either returns a tensor &lt;code&gt;y&lt;/code&gt; containing unique elements along the &lt;code&gt;axis&lt;/code&gt; of a tensor. The returned unique elements is sorted in the same order as they occur along &lt;code&gt;axis&lt;/code&gt; in &lt;code&gt;x&lt;/code&gt;. This operation also returns a tensor &lt;code&gt;idx&lt;/code&gt; that is the same size as the number of the elements in &lt;code&gt;x&lt;/code&gt; along the &lt;code&gt;axis&lt;/code&gt; dimension. It contains the index in the unique output &lt;code&gt;y&lt;/code&gt;. In other words, for an &lt;code&gt;1-D&lt;/code&gt; tensor &lt;code&gt;x&lt;/code&gt; with `axis = None:</source>
          <target state="translated">该操作要么返回一个张量 &lt;code&gt;y&lt;/code&gt; ，该张量y沿着张量的 &lt;code&gt;axis&lt;/code&gt; 包含唯一元素。当它们沿着发生返回的唯一的元件以相同的顺序进行排序 &lt;code&gt;axis&lt;/code&gt; 在 &lt;code&gt;x&lt;/code&gt; 。此操作还返回一个张量 &lt;code&gt;idx&lt;/code&gt; ，其大小与 &lt;code&gt;x&lt;/code&gt; 沿 &lt;code&gt;axis&lt;/code&gt; 尺寸的元素数相同。它包含唯一输出 &lt;code&gt;y&lt;/code&gt; 中的索引。换句话说，对于轴为&amp;ldquo;无&amp;rdquo;的 &lt;code&gt;1-D&lt;/code&gt; 张量 &lt;code&gt;x&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="a631f52c0283f6c97e9168ae9c2c1920fc673e57" translate="yes" xml:space="preserve">
          <source>This operation ensures the underlying data memory is ready when returns.</source>
          <target state="translated">此操作确保返回时底层数据存储器已准备就绪。</target>
        </trans-unit>
        <trans-unit id="5d3e7305e25146c306e9ca9dac7912d7b2d4ded2" translate="yes" xml:space="preserve">
          <source>This operation extracts a slice of size &lt;code&gt;size&lt;/code&gt; from a tensor &lt;code&gt;input_&lt;/code&gt; starting at the location specified by &lt;code&gt;begin&lt;/code&gt;. The slice &lt;code&gt;size&lt;/code&gt; is represented as a tensor shape, where &lt;code&gt;size[i]&lt;/code&gt; is the number of elements of the 'i'th dimension of &lt;code&gt;input_&lt;/code&gt; that you want to slice. The starting location (&lt;code&gt;begin&lt;/code&gt;) for the slice is represented as an offset in each dimension of &lt;code&gt;input_&lt;/code&gt;. In other words, &lt;code&gt;begin[i]&lt;/code&gt; is the offset into the i'th dimension of &lt;code&gt;input_&lt;/code&gt; that you want to slice from.</source>
          <target state="translated">此操作从张量 &lt;code&gt;input_&lt;/code&gt; 处从 &lt;code&gt;begin&lt;/code&gt; 指定的位置开始提取大小为 &lt;code&gt;size&lt;/code&gt; 的切片。切片 &lt;code&gt;size&lt;/code&gt; 表示为张量形状，其中 &lt;code&gt;size[i]&lt;/code&gt; 是要切片的 &lt;code&gt;input_&lt;/code&gt; 的第i个维度的元素数。切片的开始位置（ &lt;code&gt;begin&lt;/code&gt; ）表示为 &lt;code&gt;input_&lt;/code&gt; 各个维度中的偏移量。换句话说， &lt;code&gt;begin[i]&lt;/code&gt; 是 &lt;code&gt;input_&lt;/code&gt; 的第i个维度的偏移量。</target>
        </trans-unit>
        <trans-unit id="29ab911f3d597d49d14cf454f1533a4b19e5e15b" translate="yes" xml:space="preserve">
          <source>This operation extracts the specified region from the tensor. The notation is similar to NumPy with the restriction that currently only support basic indexing. That means that using a non-scalar tensor as input is not currently allowed.</source>
          <target state="translated">该操作从张量中提取指定区域。这个符号类似于NumPy,但限制是目前只支持基本的索引。这意味着目前不允许使用非标量张量作为输入。</target>
        </trans-unit>
        <trans-unit id="c488d4bcca92a0ac54244fdbb1f9925da2e5d927" translate="yes" xml:space="preserve">
          <source>This operation folds the padded areas of &lt;code&gt;input&lt;/code&gt; by &lt;code&gt;MirrorPad&lt;/code&gt; according to the &lt;code&gt;paddings&lt;/code&gt; you specify. &lt;code&gt;paddings&lt;/code&gt; must be the same as &lt;code&gt;paddings&lt;/code&gt; argument given to the corresponding &lt;code&gt;MirrorPad&lt;/code&gt; op.</source>
          <target state="translated">此操作折叠的填充区域 &lt;code&gt;input&lt;/code&gt; 由 &lt;code&gt;MirrorPad&lt;/code&gt; 根据 &lt;code&gt;paddings&lt;/code&gt; 指定。 &lt;code&gt;paddings&lt;/code&gt; 必须与提供给相应 &lt;code&gt;MirrorPad&lt;/code&gt; op的 &lt;code&gt;paddings&lt;/code&gt; 参数相同。</target>
        </trans-unit>
        <trans-unit id="2f4879a7545e74fa85acacd4cf01d74137ecf234" translate="yes" xml:space="preserve">
          <source>This operation has &lt;code&gt;k&lt;/code&gt; outputs, where &lt;code&gt;k&lt;/code&gt; is the number of components in the tuples stored in the given queue, and output &lt;code&gt;i&lt;/code&gt; is the ith component of the dequeued tuple.</source>
          <target state="translated">此操作有 &lt;code&gt;k&lt;/code&gt; 个输出，其中 &lt;code&gt;k&lt;/code&gt; 是存储在给定队列中的元组中的组件数，而输出 &lt;code&gt;i&lt;/code&gt; 是出队元组的第i个组件。</target>
        </trans-unit>
        <trans-unit id="2d3ec2c39ae0a5c24c1de4e22eea70e184fc371a" translate="yes" xml:space="preserve">
          <source>This operation has a gradient and thus allows for training &lt;code&gt;min&lt;/code&gt; and &lt;code&gt;max&lt;/code&gt; values.</source>
          <target state="translated">此操作具有梯度，因此可以训练 &lt;code&gt;min&lt;/code&gt; 和 &lt;code&gt;max&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ac2e08c9a87871e0d33d02bc30d3bfc7e328d76d" translate="yes" xml:space="preserve">
          <source>This operation has k outputs, where &lt;code&gt;k&lt;/code&gt; is the number of components in the tuples stored in the given queue, and output &lt;code&gt;i&lt;/code&gt; is the ith component of the dequeued tuple.</source>
          <target state="translated">此操作有k个输出，其中 &lt;code&gt;k&lt;/code&gt; 是存储在给定队列中的元组中的组件数，而输出 &lt;code&gt;i&lt;/code&gt; 是出队元组的第i个组件。</target>
        </trans-unit>
        <trans-unit id="701111c6d7bceddee66d3b31a146ad5d2ba5dd35" translate="yes" xml:space="preserve">
          <source>This operation has k outputs, where k is the number of components in the tuples stored in the given queue, and output i is the ith component of the dequeued tuple.</source>
          <target state="translated">这个操作有k个输出,其中k是存储在给定队列中的元组中的成分数,输出i是去队列元组的第i个成分。</target>
        </trans-unit>
        <trans-unit id="b78d17d35c34babe04bca7ab5360f7b9d4aad4d2" translate="yes" xml:space="preserve">
          <source>This operation has the same semantics as &lt;code&gt;reshape&lt;/code&gt; on the represented dense tensor. The indices of non-empty values in &lt;code&gt;sp_input&lt;/code&gt; are recomputed based on the new dense shape, and a new &lt;code&gt;SparseTensor&lt;/code&gt; is returned containing the new indices and new shape. The order of non-empty values in &lt;code&gt;sp_input&lt;/code&gt; is unchanged.</source>
          <target state="translated">此操作的语义与在表示的密集张量上 &lt;code&gt;reshape&lt;/code&gt; 语义相同。根据新的密集形状重新计算 &lt;code&gt;sp_input&lt;/code&gt; 中非空值的索引，并返回一个包含新索引和新形状的新 &lt;code&gt;SparseTensor&lt;/code&gt; 。 &lt;code&gt;sp_input&lt;/code&gt; 中非空值的顺序不变。</target>
        </trans-unit>
        <trans-unit id="920eea1003570a2934a5442842f0411a9537123b" translate="yes" xml:space="preserve">
          <source>This operation has the same semantics as reshape on the represented dense tensor. The &lt;code&gt;input_indices&lt;/code&gt; are recomputed based on the requested &lt;code&gt;new_shape&lt;/code&gt;.</source>
          <target state="translated">此操作的语义与在表示的密集张量上重塑的语义相同。该 &lt;code&gt;input_indices&lt;/code&gt; 是基于请求重新计算 &lt;code&gt;new_shape&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f987eeda9053c7636415898332c77d1e39acd9d4" translate="yes" xml:space="preserve">
          <source>This operation holds a replicated input to a &lt;code&gt;tpu.replicate()&lt;/code&gt; computation subgraph. Each replicated input has the same shape and type alongside the output.</source>
          <target state="translated">此操作将复制的输入保存到 &lt;code&gt;tpu.replicate()&lt;/code&gt; 计算子图。每个复制的输入在输出旁边具有相同的形状和类型。</target>
        </trans-unit>
        <trans-unit id="8d663f608fa1c649097379c4a16273931656d413" translate="yes" xml:space="preserve">
          <source>This operation holds a replicated output from a &lt;code&gt;tpu.replicate()&lt;/code&gt; computation subgraph. Each replicated output has the same shape and type alongside the input.</source>
          <target state="translated">此操作保留 &lt;code&gt;tpu.replicate()&lt;/code&gt; 计算子图的复制输出。每个复制的输出在输入旁边具有相同的形状和类型。</target>
        </trans-unit>
        <trans-unit id="ddc1067cff13fb19a5da325542d16f1e694e62d1" translate="yes" xml:space="preserve">
          <source>This operation holds the metadata common to operations of a &lt;code&gt;tpu.replicate()&lt;/code&gt; computation subgraph.</source>
          <target state="translated">此操作保存 &lt;code&gt;tpu.replicate()&lt;/code&gt; 计算子图的操作所共有的元数据。</target>
        </trans-unit>
        <trans-unit id="1bd3f36c5bf6b7e281276b8f3d298543774510a1" translate="yes" xml:space="preserve">
          <source>This operation is a no-op when executing eagerly.</source>
          <target state="translated">这个操作在急于执行的时候是没有办法的。</target>
        </trans-unit>
        <trans-unit id="7c3a2eaa25ce85f5bc726adb4e2f6e321fc3ad96" translate="yes" xml:space="preserve">
          <source>This operation is a synchronous version IteratorGetNext. It should only be used in situations where the iterator does not block the calling thread, or where the calling thread is not a member of the thread pool used to execute parallel operations (e.g. in eager mode).</source>
          <target state="translated">这个操作是同步版本的IteratorGetNext。只有在迭代器不阻塞调用线程,或者调用线程不是用于执行并行操作的线程池成员的情况下(例如在急切模式下),才应该使用该操作。</target>
        </trans-unit>
        <trans-unit id="834adca0b6e8190426dd82e3dc679bdfc3901af6" translate="yes" xml:space="preserve">
          <source>This operation is considered stateful. For a stateless version, see PyFuncStateless.</source>
          <target state="translated">这个操作被认为是有状态的。关于无状态版本,请参见PyFuncStateless。</target>
        </trans-unit>
        <trans-unit id="c77c133cb2aa87b66c50cea9b24338cf94df91e2" translate="yes" xml:space="preserve">
          <source>This operation is equivalent to the following steps:</source>
          <target state="translated">此操作相当于以下步骤。</target>
        </trans-unit>
        <trans-unit id="a85336f9b64b0941e75fed8873dd343470ffb664" translate="yes" xml:space="preserve">
          <source>This operation is for training only. It is generally an underestimate of the full softmax loss.</source>
          <target state="translated">此操作仅用于训练。一般来说,它是低估了全部软最大值的损失。</target>
        </trans-unit>
        <trans-unit id="197a06d7811b56e8d6bce80b3c815e4a4c56a4a7" translate="yes" xml:space="preserve">
          <source>This operation is not supported by all queues. If a queue does not support DequeueUpTo, then an Unimplemented error is returned.</source>
          <target state="translated">并非所有的队列都支持此操作,如果一个队列不支持DequeueUpTo,那么将返回一个 &quot;未执行 &quot;错误。如果一个队列不支持DequeueUpTo,那么将返回一个Unimplemented错误。</target>
        </trans-unit>
        <trans-unit id="6db0a8622252241b3dd8e4da9fc3d6ba84977a9c" translate="yes" xml:space="preserve">
          <source>This operation is related to &lt;code&gt;squeeze()&lt;/code&gt;, which removes dimensions of size 1.</source>
          <target state="translated">此操作与 &lt;code&gt;squeeze()&lt;/code&gt; 有关，后者会删除尺寸为1的尺寸。</target>
        </trans-unit>
        <trans-unit id="49c94ac01402b2bd4ac870cdfc937202b7e69050" translate="yes" xml:space="preserve">
          <source>This operation is related to:</source>
          <target state="translated">这一行动与:</target>
        </trans-unit>
        <trans-unit id="f8a4568939439baca88acb2c04e8494963c44c55" translate="yes" xml:space="preserve">
          <source>This operation is significantly more numerically stable than the equivalent tensorflow operation &lt;code&gt;tf.math.log(tf.math.cumsum(tf.math.exp(x)))&lt;/code&gt;, although computes the same result given infinite numerical precision. However, note that in some cases, it may be less stable than &lt;a href=&quot;reduce_logsumexp&quot;&gt;&lt;code&gt;tf.math.reduce_logsumexp&lt;/code&gt;&lt;/a&gt; for a given element, as it applies the &quot;log-sum-exp trick&quot; in a different way.</source>
          <target state="translated">此操作在数值上比等效的张量流操作 &lt;code&gt;tf.math.log(tf.math.cumsum(tf.math.exp(x)))&lt;/code&gt; 显着更稳定，尽管在给定无限数值精度的情况下可以计算出相同的结果。但是，请注意，在某些情况下，对于给定元素而言，它可能不如&lt;a href=&quot;reduce_logsumexp&quot;&gt; &lt;code&gt;tf.math.reduce_logsumexp&lt;/code&gt; &lt;/a&gt;稳定，因为它以不同的方式应用&amp;ldquo; log-sum-exp技巧&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="c9fb91f09ebc4c3acce5ac821344d6140f7e24da" translate="yes" xml:space="preserve">
          <source>This operation is similar to tensor_scatter_add, except that the tensor is zero-initialized. Calling &lt;a href=&quot;../scatter_nd&quot;&gt;&lt;code&gt;tf.scatter_nd(indices, values, shape)&lt;/code&gt;&lt;/a&gt; is identical to &lt;code&gt;tensor_scatter_add(tf.zeros(shape, values.dtype), indices, values)&lt;/code&gt;</source>
          <target state="translated">该操作类似于tensor_scatter_add，不同之处在于张量是零初始化的。调用&lt;a href=&quot;../scatter_nd&quot;&gt; &lt;code&gt;tf.scatter_nd(indices, values, shape)&lt;/code&gt; &lt;/a&gt;与 &lt;code&gt;tensor_scatter_add(tf.zeros(shape, values.dtype), indices, values)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="0cce4836efcc5d3f1c90cad3d4974cf4b16512f4" translate="yes" xml:space="preserve">
          <source>This operation is similar to tensor_scatter_add, except that the tensor is zero-initialized. Calling &lt;a href=&quot;scatter_nd&quot;&gt;&lt;code&gt;tf.scatter_nd(indices, values, shape)&lt;/code&gt;&lt;/a&gt; is identical to &lt;code&gt;tensor_scatter_add(tf.zeros(shape, values.dtype), indices, values)&lt;/code&gt;</source>
          <target state="translated">该操作类似于tensor_scatter_add，不同之处在于张量是零初始化的。调用&lt;a href=&quot;scatter_nd&quot;&gt; &lt;code&gt;tf.scatter_nd(indices, values, shape)&lt;/code&gt; &lt;/a&gt;与 &lt;code&gt;tensor_scatter_add(tf.zeros(shape, values.dtype), indices, values)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="3ef068c49d35ae40fdbd6e0290914a410da40aff" translate="yes" xml:space="preserve">
          <source>This operation is sometimes called &quot;deconvolution&quot; after (Zeiler et al., 2010), but is actually the transpose (gradient) of &lt;code&gt;conv1d&lt;/code&gt; rather than an actual deconvolution.</source>
          <target state="translated">此操作有时在之后称为&amp;ldquo;反卷积&amp;rdquo;（Zeiler等人，2010），但实际上是 &lt;code&gt;conv1d&lt;/code&gt; 的转置（梯度），而不是实际的反卷积。</target>
        </trans-unit>
        <trans-unit id="f5e02776737c55d9c6ad653f1cc95f3511e5d57b" translate="yes" xml:space="preserve">
          <source>This operation is sometimes called &quot;deconvolution&quot; after (Zeiler et al., 2010), but is really the transpose (gradient) of &lt;code&gt;atrous_conv2d&lt;/code&gt; rather than an actual deconvolution.</source>
          <target state="translated">此操作有时在之后称为&amp;ldquo;反卷积&amp;rdquo;（Zeiler等人，2010），但实际上是 &lt;code&gt;atrous_conv2d&lt;/code&gt; 的转置（梯度），而不是实际的反卷积。</target>
        </trans-unit>
        <trans-unit id="00a748a99cc55d86cf7d8d7f88c1133466c7c32f" translate="yes" xml:space="preserve">
          <source>This operation is sometimes called &quot;deconvolution&quot; after (Zeiler et al., 2010), but is really the transpose (gradient) of &lt;code&gt;conv2d&lt;/code&gt; rather than an actual deconvolution.</source>
          <target state="translated">此操作有时在之后称为&amp;ldquo;反卷积&amp;rdquo;（Zeiler等人，2010），但实际上是 &lt;code&gt;conv2d&lt;/code&gt; 的转置（梯度），而不是实际的反卷积。</target>
        </trans-unit>
        <trans-unit id="c47ed45caa79111f8c5fa46b428a6d5a1fe8d370" translate="yes" xml:space="preserve">
          <source>This operation is sometimes called &quot;deconvolution&quot; after (Zeiler et al., 2010), but is really the transpose (gradient) of &lt;code&gt;conv3d&lt;/code&gt; rather than an actual deconvolution.</source>
          <target state="translated">此操作有时在之后称为&amp;ldquo;反卷积&amp;rdquo;（Zeiler等人，2010），但实际上是 &lt;code&gt;conv3d&lt;/code&gt; 的转置（梯度），而不是实际的反卷积。</target>
        </trans-unit>
        <trans-unit id="50f836730593fffe361ee867e1bf79d15e8af917" translate="yes" xml:space="preserve">
          <source>This operation is sometimes called &quot;deconvolution&quot; after &lt;a href=&quot;http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf&quot;&gt;Deconvolutional Networks&lt;/a&gt;, but is actually the transpose (gradient) of &lt;code&gt;conv2d&lt;/code&gt; rather than an actual deconvolution.</source>
          <target state="translated">在&lt;a href=&quot;http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf&quot;&gt;反卷积网络之后&lt;/a&gt;，此操作有时称为&amp;ldquo;反卷积&amp;rdquo; ，但实际上是 &lt;code&gt;conv2d&lt;/code&gt; 的转置（渐变），而不是实际的反卷积。</target>
        </trans-unit>
        <trans-unit id="cdd40bbfb18d0876cdd3d1bf655721101051d082" translate="yes" xml:space="preserve">
          <source>This operation is sometimes called &quot;deconvolution&quot; after &lt;a href=&quot;http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf&quot;&gt;Deconvolutional Networks&lt;/a&gt;, but is actually the transpose (gradient) of &lt;code&gt;convolution&lt;/code&gt; rather than an actual deconvolution.</source>
          <target state="translated">在&lt;a href=&quot;http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf&quot;&gt;反卷积网络之后&lt;/a&gt;，此操作有时称为&amp;ldquo;反卷积&amp;rdquo; ，但实际上是 &lt;code&gt;convolution&lt;/code&gt; 的转置（梯度），而不是实际的反卷积。</target>
        </trans-unit>
        <trans-unit id="a80e63d8e3617ef22a7c11bc4ac5ce2aac92b83c" translate="yes" xml:space="preserve">
          <source>This operation is sometimes called &quot;deconvolution&quot; after &lt;a href=&quot;https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf&quot;&gt;Deconvolutional Networks&lt;/a&gt;, but is really the transpose (gradient) of &lt;code&gt;atrous_conv2d&lt;/code&gt; rather than an actual deconvolution.</source>
          <target state="translated">在&lt;a href=&quot;https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf&quot;&gt;反卷积网络之后&lt;/a&gt;，此操作有时称为&amp;ldquo;反卷积&amp;rdquo; ，但实际上是 &lt;code&gt;atrous_conv2d&lt;/code&gt; 的转置（渐变），而不是实际的反卷积。</target>
        </trans-unit>
        <trans-unit id="3e16e89f92d27118cbc90336d321edbdccabf257" translate="yes" xml:space="preserve">
          <source>This operation is sometimes called &quot;deconvolution&quot; after &lt;a href=&quot;https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf&quot;&gt;Deconvolutional Networks&lt;/a&gt;, but is really the transpose (gradient) of &lt;code&gt;conv1d&lt;/code&gt; rather than an actual deconvolution.</source>
          <target state="translated">在&lt;a href=&quot;https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf&quot;&gt;反卷积网络之后&lt;/a&gt;，此操作有时称为&amp;ldquo;反卷积&amp;rdquo; ，但实际上是 &lt;code&gt;conv1d&lt;/code&gt; 的转置（梯度），而不是实际的反卷积。</target>
        </trans-unit>
        <trans-unit id="74c54a1240dd3ebac671477622bdae5730c68b14" translate="yes" xml:space="preserve">
          <source>This operation is sometimes called &quot;deconvolution&quot; after &lt;a href=&quot;https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf&quot;&gt;Deconvolutional Networks&lt;/a&gt;, but is really the transpose (gradient) of &lt;code&gt;conv2d&lt;/code&gt; rather than an actual deconvolution.</source>
          <target state="translated">在&lt;a href=&quot;https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf&quot;&gt;反卷积网络之后&lt;/a&gt;，此操作有时称为&amp;ldquo;反卷积&amp;rdquo; ，但实际上是 &lt;code&gt;conv2d&lt;/code&gt; 的转置（渐变），而不是实际的反卷积。</target>
        </trans-unit>
        <trans-unit id="78632da26e5444bf46256b6e1c192a603e6c3c00" translate="yes" xml:space="preserve">
          <source>This operation is sometimes called &quot;deconvolution&quot; after &lt;a href=&quot;https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf&quot;&gt;Deconvolutional Networks&lt;/a&gt;, but is really the transpose (gradient) of &lt;code&gt;conv3d&lt;/code&gt; rather than an actual deconvolution.</source>
          <target state="translated">在&lt;a href=&quot;https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf&quot;&gt;反卷积网络之后&lt;/a&gt;，此操作有时称为&amp;ldquo;反卷积&amp;rdquo; ，但实际上是 &lt;code&gt;conv3d&lt;/code&gt; 的转置（渐变），而不是实际的反卷积。</target>
        </trans-unit>
        <trans-unit id="20f728b5bb5e16f6250024eb3cf614e43cfbbe07" translate="yes" xml:space="preserve">
          <source>This operation is typically used to clip gradients before applying them with an optimizer.</source>
          <target state="translated">此操作通常用于在使用优化器应用渐变之前对其进行剪辑。</target>
        </trans-unit>
        <trans-unit id="7b06c412ab3afb65378a71bcff3a6e6664ba2503" translate="yes" xml:space="preserve">
          <source>This operation is typically used to clip gradients before applying them with an optimizer. Most gradient data is a collection of different shaped tensors for different parts of the model. Thus, this is a common usage:</source>
          <target state="translated">这个操作通常用于在使用优化器应用梯度之前对其进行剪辑。大多数梯度数据是模型不同部分的不同形状渐变的集合。因此,这是一种常见的用法。</target>
        </trans-unit>
        <trans-unit id="a39ff6aff1d832bca041bbe0ecd1ce428c1522da" translate="yes" xml:space="preserve">
          <source>This operation is useful for Locality-Sensitive-Hashing (LSH) and other algorithms that use hashing approximations of cosine and &lt;code&gt;L2&lt;/code&gt; distances; codes can be generated from an input via:</source>
          <target state="translated">此操作对局部敏感哈希（LSH）和其他使用余弦和 &lt;code&gt;L2&lt;/code&gt; 距离的哈希近似的算法很有用；可以通过以下方式从输入生成代码：</target>
        </trans-unit>
        <trans-unit id="5397809ea0e6b96d049101c13960159515304d67" translate="yes" xml:space="preserve">
          <source>This operation is useful for resizing the activations between convolutions (but keeping all data), e.g. instead of pooling. It is also useful for training purely convolutional models.</source>
          <target state="translated">这个操作对于调整卷积之间的激活量(但保留所有数据)很有用,例如,代替池化。它对于训练纯卷积模型也很有用。</target>
        </trans-unit>
        <trans-unit id="e6cbba91c74639c8cb40de16428e2bef0563e905" translate="yes" xml:space="preserve">
          <source>This operation is useful if you want to add a batch dimension to a single element. For example, if you have a single image of shape &lt;code&gt;[height, width, channels]&lt;/code&gt;, you can make it a batch of 1 image with &lt;code&gt;expand_dims(image, 0)&lt;/code&gt;, which will make the shape &lt;code&gt;[1, height, width, channels]&lt;/code&gt;.</source>
          <target state="translated">如果要将批次尺寸添加到单个元素，此操作很有用。例如，如果您有一个形状为 &lt;code&gt;[height, width, channels]&lt;/code&gt; ，则可以使用 &lt;code&gt;expand_dims(image, 0)&lt;/code&gt; 使其成为1张图像的批处理，这将使形状为 &lt;code&gt;[1, height, width, channels]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="6316e455129ed9fa859e28aacc9886a66315035c" translate="yes" xml:space="preserve">
          <source>This operation is useful if you want to add a batch dimension to a single element. For example, if you have a single image of shape &lt;code&gt;[height, width, channels]&lt;/code&gt;, you can make it a batch of one image with &lt;code&gt;expand_dims(image, 0)&lt;/code&gt;, which will make the shape &lt;code&gt;[1, height, width, channels]&lt;/code&gt;.</source>
          <target state="translated">如果要将批次尺寸添加到单个元素，此操作很有用。例如，如果您有一个形状为 &lt;code&gt;[height, width, channels]&lt;/code&gt; ，则可以使用 &lt;code&gt;expand_dims(image, 0)&lt;/code&gt; 使其成为一个图像的一批，这将使形状为 &lt;code&gt;[1, height, width, channels]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e351d2492bb6908fd7d460e61be31a9eca71f43d" translate="yes" xml:space="preserve">
          <source>This operation is useful to:</source>
          <target state="translated">这个操作的作用是:</target>
        </trans-unit>
        <trans-unit id="183fa485124f0558dfb1f2c40c10151a91905ce9" translate="yes" xml:space="preserve">
          <source>This operation may be executed multiple times. Each execution will reset the iterator in &lt;code&gt;iterator&lt;/code&gt; to the first element of &lt;code&gt;dataset&lt;/code&gt;.</source>
          <target state="translated">该操作可以执行多次。每次执行都会将迭代器中的 &lt;code&gt;iterator&lt;/code&gt; 重置为 &lt;code&gt;dataset&lt;/code&gt; 的第一个元素。</target>
        </trans-unit>
        <trans-unit id="082051a9c25e40abb9ca51ba30204e50cec28855" translate="yes" xml:space="preserve">
          <source>This operation outputs &quot;ref&quot; after the assignment is done. This makes it easier to chain operations that need to use the reset value.</source>
          <target state="translated">该操作在赋值完成后输出 &quot;ref&quot;。这使得需要使用重置值的操作更容易连锁。</target>
        </trans-unit>
        <trans-unit id="45e5d0f78a3a33704d46216d94caf563cfac246a" translate="yes" xml:space="preserve">
          <source>This operation outputs &quot;ref&quot; after the update is done. This makes it easier to chain operations that need to use the reset value.</source>
          <target state="translated">该操作在更新完成后输出 &quot;ref&quot;。这使得需要使用重置值的操作更容易连锁。</target>
        </trans-unit>
        <trans-unit id="7d1d3aca1842cf510032065cfce3509b5c793096" translate="yes" xml:space="preserve">
          <source>This operation outputs &quot;ref&quot; after the update is done. This makes it easier to chain operations that need to use the reset value. Unlike &lt;a href=&quot;../../math/add&quot;&gt;&lt;code&gt;tf.math.add&lt;/code&gt;&lt;/a&gt;, this op does not broadcast. &lt;code&gt;ref&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt; must have the same shape.</source>
          <target state="translated">更新完成后，此操作将输出&amp;ldquo; ref&amp;rdquo;。这样可以更轻松地链接需要使用重置值的操作。与&lt;a href=&quot;../../math/add&quot;&gt; &lt;code&gt;tf.math.add&lt;/code&gt; &lt;/a&gt;不同，此操作不会广播。 &lt;code&gt;ref&lt;/code&gt; 和 &lt;code&gt;value&lt;/code&gt; 必须具有相同的形状。</target>
        </trans-unit>
        <trans-unit id="7e16eef80cf37c2d9c02fad5e714a6e559684854" translate="yes" xml:space="preserve">
          <source>This operation outputs &lt;code&gt;ref&lt;/code&gt; after the update is done. This makes it easier to chain operations that need to use the reset value.</source>
          <target state="translated">更新完成后，此操作输出 &lt;code&gt;ref&lt;/code&gt; 。这样可以更轻松地链接需要使用重置值的操作。</target>
        </trans-unit>
        <trans-unit id="80080e1227c1f588b580293eb11f01d8754cc2fa" translate="yes" xml:space="preserve">
          <source>This operation outputs &lt;code&gt;ref&lt;/code&gt; after the update is done. This makes it easier to chain operations that need to use the reset value. Unlike &lt;a href=&quot;../../math/subtract&quot;&gt;&lt;code&gt;tf.math.subtract&lt;/code&gt;&lt;/a&gt;, this op does not broadcast. &lt;code&gt;ref&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt; must have the same shape.</source>
          <target state="translated">更新完成后，此操作输出 &lt;code&gt;ref&lt;/code&gt; 。这样可以更轻松地链接需要使用重置值的操作。与&lt;a href=&quot;../../math/subtract&quot;&gt; &lt;code&gt;tf.math.subtract&lt;/code&gt; &lt;/a&gt;不同，此操作不会广播。 &lt;code&gt;ref&lt;/code&gt; 和 &lt;code&gt;value&lt;/code&gt; 必须具有相同的形状。</target>
        </trans-unit>
        <trans-unit id="211cf88f878719f0fdbc3eba7700cfd25f62b94c" translate="yes" xml:space="preserve">
          <source>This operation outputs &lt;code&gt;ref&lt;/code&gt; after the update is done. This makes it easier to chain operations that need to use the updated value. Duplicate entries are handled correctly: if multiple &lt;code&gt;indices&lt;/code&gt; reference the same location, their contributions add.</source>
          <target state="translated">更新完成后，此操作输出 &lt;code&gt;ref&lt;/code&gt; 。这使得链接需要使用更新值的操作变得更加容易。重复项的处理正确：如果多个 &lt;code&gt;indices&lt;/code&gt; 引用相同的位置，则它们的贡献将相加。</target>
        </trans-unit>
        <trans-unit id="f7141134307e0a174b2d64cd3ba50899cf746d15" translate="yes" xml:space="preserve">
          <source>This operation outputs a Tensor that holds the new value of &lt;code&gt;ref&lt;/code&gt; after the value has been assigned. This makes it easier to chain operations that need to use the reset value.</source>
          <target state="translated">分配值后，此操作将输出一个Tensor，其中包含 &lt;code&gt;ref&lt;/code&gt; 的新值。这样可以更轻松地链接需要使用重置值的操作。</target>
        </trans-unit>
        <trans-unit id="ff9194e8e7be6d4ad4f022b42bf00a1d9dcc3aad" translate="yes" xml:space="preserve">
          <source>This operation pads &lt;code&gt;input&lt;/code&gt; according to the &lt;code&gt;paddings&lt;/code&gt; and &lt;code&gt;constant_values&lt;/code&gt; you specify. &lt;code&gt;paddings&lt;/code&gt; is an integer tensor with shape &lt;code&gt;[Dn, 2]&lt;/code&gt;, where n is the rank of &lt;code&gt;input&lt;/code&gt;. For each dimension D of &lt;code&gt;input&lt;/code&gt;, &lt;code&gt;paddings[D, 0]&lt;/code&gt; indicates how many padding values to add before the contents of &lt;code&gt;input&lt;/code&gt; in that dimension, and &lt;code&gt;paddings[D, 1]&lt;/code&gt; indicates how many padding values to add after the contents of &lt;code&gt;input&lt;/code&gt; in that dimension. &lt;code&gt;constant_values&lt;/code&gt; is a scalar tensor of the same type as &lt;code&gt;input&lt;/code&gt; that indicates the value to use for padding &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">此操作根据您指定的 &lt;code&gt;paddings&lt;/code&gt; 和 &lt;code&gt;constant_values&lt;/code&gt; 填充 &lt;code&gt;input&lt;/code&gt; 。 &lt;code&gt;paddings&lt;/code&gt; 是形状为 &lt;code&gt;[Dn, 2]&lt;/code&gt; 的整数张量，其中n是 &lt;code&gt;input&lt;/code&gt; 的秩。对于 &lt;code&gt;input&lt;/code&gt; 每个维度D ， &lt;code&gt;paddings[D, 0]&lt;/code&gt; 指示在该维度中要在 &lt;code&gt;input&lt;/code&gt; 内容之前添加多少填充值，而 &lt;code&gt;paddings[D, 1]&lt;/code&gt; 指示在该维度中要 &lt;code&gt;input&lt;/code&gt; 的内容之后添加多少填充值。方面。 &lt;code&gt;constant_values&lt;/code&gt; 是与 &lt;code&gt;input&lt;/code&gt; 类型相同的标量张量指示用于填充 &lt;code&gt;input&lt;/code&gt; 的值。</target>
        </trans-unit>
        <trans-unit id="c7dac094042ba8225a6a7449892b847dafe951a8" translate="yes" xml:space="preserve">
          <source>This operation pads a &lt;code&gt;input&lt;/code&gt; with mirrored values according to the &lt;code&gt;paddings&lt;/code&gt; you specify. &lt;code&gt;paddings&lt;/code&gt; is an integer tensor with shape &lt;code&gt;[n, 2]&lt;/code&gt;, where n is the rank of &lt;code&gt;input&lt;/code&gt;. For each dimension D of &lt;code&gt;input&lt;/code&gt;, &lt;code&gt;paddings[D, 0]&lt;/code&gt; indicates how many values to add before the contents of &lt;code&gt;input&lt;/code&gt; in that dimension, and &lt;code&gt;paddings[D, 1]&lt;/code&gt; indicates how many values to add after the contents of &lt;code&gt;input&lt;/code&gt; in that dimension. Both &lt;code&gt;paddings[D, 0]&lt;/code&gt; and &lt;code&gt;paddings[D, 1]&lt;/code&gt; must be no greater than &lt;code&gt;input.dim_size(D)&lt;/code&gt; (or &lt;code&gt;input.dim_size(D) - 1&lt;/code&gt;) if &lt;code&gt;copy_border&lt;/code&gt; is true (if false, respectively).</source>
          <target state="translated">此操作垫 &lt;code&gt;input&lt;/code&gt; 与根据镜像值 &lt;code&gt;paddings&lt;/code&gt; 指定。 &lt;code&gt;paddings&lt;/code&gt; 是形状为 &lt;code&gt;[n, 2]&lt;/code&gt; 的整数张量，其中n是 &lt;code&gt;input&lt;/code&gt; 的等级。对于 &lt;code&gt;input&lt;/code&gt; 每个维D ， &lt;code&gt;paddings[D, 0]&lt;/code&gt; 指示在该维中 &lt;code&gt;input&lt;/code&gt; 内容之前要添加多少个值，而 &lt;code&gt;paddings[D, 1]&lt;/code&gt; 指示在该维 &lt;code&gt;input&lt;/code&gt; 内容之后要添加多少个值。两个 &lt;code&gt;paddings[D, 0]&lt;/code&gt; 和 &lt;code&gt;paddings[D, 1]&lt;/code&gt; 必须不大于 &lt;code&gt;input.dim_size(D)&lt;/code&gt; （或 &lt;code&gt;input.dim_size(D) - 1&lt;/code&gt; 如果 &lt;code&gt;copy_border&lt;/code&gt; 为true（分别为false ），则为input.dim_size（D）-1）。</target>
        </trans-unit>
        <trans-unit id="98dc6d3604c67365b92527ae5f26da565365534d" translate="yes" xml:space="preserve">
          <source>This operation pads a &lt;code&gt;input&lt;/code&gt; with zeros according to the &lt;code&gt;paddings&lt;/code&gt; you specify. &lt;code&gt;paddings&lt;/code&gt; is an integer tensor with shape &lt;code&gt;[Dn, 2]&lt;/code&gt;, where n is the rank of &lt;code&gt;input&lt;/code&gt;. For each dimension D of &lt;code&gt;input&lt;/code&gt;, &lt;code&gt;paddings[D, 0]&lt;/code&gt; indicates how many zeros to add before the contents of &lt;code&gt;input&lt;/code&gt; in that dimension, and &lt;code&gt;paddings[D, 1]&lt;/code&gt; indicates how many zeros to add after the contents of &lt;code&gt;input&lt;/code&gt; in that dimension.</source>
          <target state="translated">此操作根据您指定的 &lt;code&gt;paddings&lt;/code&gt; 将 &lt;code&gt;input&lt;/code&gt; 填充为零。 &lt;code&gt;paddings&lt;/code&gt; 是形状为 &lt;code&gt;[Dn, 2]&lt;/code&gt; 的整数张量，其中n是 &lt;code&gt;input&lt;/code&gt; 的秩。对于 &lt;code&gt;input&lt;/code&gt; 每个维D ， &lt;code&gt;paddings[D, 0]&lt;/code&gt; 指示在该维中 &lt;code&gt;input&lt;/code&gt; 内容之前要添加多少个零，而 &lt;code&gt;paddings[D, 1]&lt;/code&gt; 指示在该维 &lt;code&gt;input&lt;/code&gt; 内容之后要添加多少个零。</target>
        </trans-unit>
        <trans-unit id="d8dcb952f1fd946dc8a99ab12f18c6c9f3bad230" translate="yes" xml:space="preserve">
          <source>This operation pads a &lt;code&gt;tensor&lt;/code&gt; according to the &lt;code&gt;paddings&lt;/code&gt; you specify. &lt;code&gt;paddings&lt;/code&gt; is an integer tensor with shape &lt;code&gt;[n, 2]&lt;/code&gt;, where n is the rank of &lt;code&gt;tensor&lt;/code&gt;. For each dimension D of &lt;code&gt;input&lt;/code&gt;, &lt;code&gt;paddings[D, 0]&lt;/code&gt; indicates how many values to add before the contents of &lt;code&gt;tensor&lt;/code&gt; in that dimension, and &lt;code&gt;paddings[D, 1]&lt;/code&gt; indicates how many values to add after the contents of &lt;code&gt;tensor&lt;/code&gt; in that dimension. If &lt;code&gt;mode&lt;/code&gt; is &quot;REFLECT&quot; then both &lt;code&gt;paddings[D, 0]&lt;/code&gt; and &lt;code&gt;paddings[D, 1]&lt;/code&gt; must be no greater than &lt;code&gt;tensor.dim_size(D) - 1&lt;/code&gt;. If &lt;code&gt;mode&lt;/code&gt; is &quot;SYMMETRIC&quot; then both &lt;code&gt;paddings[D, 0]&lt;/code&gt; and &lt;code&gt;paddings[D, 1]&lt;/code&gt; must be no greater than &lt;code&gt;tensor.dim_size(D)&lt;/code&gt;.</source>
          <target state="translated">此操作根据您指定的 &lt;code&gt;paddings&lt;/code&gt; &lt;code&gt;tensor&lt;/code&gt; 。 &lt;code&gt;paddings&lt;/code&gt; 是形状为 &lt;code&gt;[n, 2]&lt;/code&gt; 的整数张量，其中n是 &lt;code&gt;tensor&lt;/code&gt; 的秩。对于 &lt;code&gt;input&lt;/code&gt; 每个维D ， &lt;code&gt;paddings[D, 0]&lt;/code&gt; 指示在该维中 &lt;code&gt;tensor&lt;/code&gt; 的内容之前要添加多少值，而 &lt;code&gt;paddings[D, 1]&lt;/code&gt; 指示在该维中 &lt;code&gt;tensor&lt;/code&gt; 的内容之后要添加多少值。如果 &lt;code&gt;mode&lt;/code&gt; 为&amp;ldquo; REFLECT&amp;rdquo;，则 &lt;code&gt;paddings[D, 0]&lt;/code&gt; 和 &lt;code&gt;paddings[D, 1]&lt;/code&gt; 都不得大于 &lt;code&gt;tensor.dim_size(D) - 1&lt;/code&gt; 。如果 &lt;code&gt;mode&lt;/code&gt; 为&amp;ldquo; SYMMETRIC&amp;rdquo;，则 &lt;code&gt;paddings[D, 0]&lt;/code&gt; 和 &lt;code&gt;paddings[D, 1]&lt;/code&gt; 都不得大于 &lt;code&gt;tensor.dim_size(D)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ece6bb1bca3b5a4f71fcb2a4c7bb7def98bb6471" translate="yes" xml:space="preserve">
          <source>This operation performs non_max_suppression on the inputs per batch, across all classes. Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Note that this algorithm is agnostic to where the origin is in the coordinate system. Also note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm. The output of this operation is the final boxes, scores and classes tensor returned after performing non_max_suppression.</source>
          <target state="translated">此操作对每批所有类别的输入执行非最大抑制。修剪掉与之前选择的方框有较高交集重叠的方框。边界框以[y1,x1,y2,x2]的形式提供,其中(y1,x1)和(y2,x2)是任意一对对角线框角的坐标,坐标可以是归一化(即位于[0,1]区间)或绝对值。请注意,该算法与坐标系中原点的位置无关。还请注意,该算法对坐标系的正交变换和平移是不变的;因此,坐标系的平移或反射会导致算法选择相同的方框。此操作的输出是执行non_max_suppression后返回的最终盒子、分数和类张量。</target>
        </trans-unit>
        <trans-unit id="92bf8506361b0c3190382a71e7f126a1495f07cd" translate="yes" xml:space="preserve">
          <source>This operation randomly samples a tensor of sampled classes (&lt;code&gt;sampled_candidates&lt;/code&gt;) from the range of integers &lt;code&gt;[0, range_max)&lt;/code&gt;.</source>
          <target state="translated">该操作随机样本采样类（的张量 &lt;code&gt;sampled_candidates&lt;/code&gt; 从整数的范围内） &lt;code&gt;[0, range_max)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ef1ed12213e6ff487e80edd0040d70890d84414e" translate="yes" xml:space="preserve">
          <source>This operation requires that &lt;code&gt;axis&lt;/code&gt; is a valid index for &lt;code&gt;input.shape&lt;/code&gt;, following Python indexing rules:</source>
          <target state="translated">此操作要求 &lt;code&gt;axis&lt;/code&gt; 是遵循Python索引规则的 &lt;code&gt;input.shape&lt;/code&gt; 的有效索引：</target>
        </trans-unit>
        <trans-unit id="d5bc7ae5c1b93f8f5a6a390a4d228223c7c191dd" translate="yes" xml:space="preserve">
          <source>This operation requires that &lt;code&gt;axis&lt;/code&gt; is a valid index for &lt;code&gt;input.shape&lt;/code&gt;, following python indexing rules:</source>
          <target state="translated">此操作要求 &lt;code&gt;axis&lt;/code&gt; 是 &lt;code&gt;input.shape&lt;/code&gt; 的有效索引，遵循python索引规则：</target>
        </trans-unit>
        <trans-unit id="2656ea13a552cbb0836e0031c2afa8bfe371a2e1" translate="yes" xml:space="preserve">
          <source>This operation requires that:</source>
          <target state="translated">这一操作要求:</target>
        </trans-unit>
        <trans-unit id="b27b171e2cb8132634ecffed1d0dfa123640e238" translate="yes" xml:space="preserve">
          <source>This operation reshapes the &quot;batch&quot; dimension 0 into &lt;code&gt;M + 1&lt;/code&gt; dimensions of shape &lt;code&gt;block_shape + [batch]&lt;/code&gt;, interleaves these blocks back into the grid defined by the spatial dimensions &lt;code&gt;[1, ..., M]&lt;/code&gt;, to obtain a result with the same rank as the input. The spatial dimensions of this intermediate result are then optionally cropped according to &lt;code&gt;crops&lt;/code&gt; to produce the output. This is the reverse of SpaceToBatch (see &lt;a href=&quot;space_to_batch&quot;&gt;&lt;code&gt;tf.space_to_batch&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">此操作将&amp;ldquo;批&amp;rdquo;尺寸0整形为 &lt;code&gt;M + 1&lt;/code&gt; &lt;code&gt;block_shape + [batch]&lt;/code&gt; 形状的尺寸，将这些块交织回由空间尺寸 &lt;code&gt;[1, ..., M]&lt;/code&gt; 定义的网格中，以得到结果。与输入相同的等级。然后，根据 &lt;code&gt;crops&lt;/code&gt; ，可以选择裁剪此中间结果的空间尺寸，以产生输出。这与SpaceToBatch相反（请参阅&lt;a href=&quot;space_to_batch&quot;&gt; &lt;code&gt;tf.space_to_batch&lt;/code&gt; &lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="85455e262fab5e8f9fc55d0016d69887ed46e6fb" translate="yes" xml:space="preserve">
          <source>This operation reshapes the &quot;batch&quot; dimension 0 into &lt;code&gt;M + 1&lt;/code&gt; dimensions of shape &lt;code&gt;block_shape + [batch]&lt;/code&gt;, interleaves these blocks back into the grid defined by the spatial dimensions &lt;code&gt;[1, ..., M]&lt;/code&gt;, to obtain a result with the same rank as the input. The spatial dimensions of this intermediate result are then optionally cropped according to &lt;code&gt;crops&lt;/code&gt; to produce the output. This is the reverse of SpaceToBatch. See below for a precise description.</source>
          <target state="translated">此操作将&amp;ldquo;批&amp;rdquo;尺寸0 整形为 &lt;code&gt;M + 1&lt;/code&gt; &lt;code&gt;block_shape + [batch]&lt;/code&gt; 形状的尺寸，将这些块交织回由空间尺寸 &lt;code&gt;[1, ..., M]&lt;/code&gt; 定义的网格中，以得到结果。与输入的等级相同。然后，根据 &lt;code&gt;crops&lt;/code&gt; ，可以选择裁剪此中间结果的空间尺寸，以产生输出。这与SpaceToBatch相反。有关详细说明，请参见下文。</target>
        </trans-unit>
        <trans-unit id="9858054bc42ba5d711abfe7b471684870f12eb28" translate="yes" xml:space="preserve">
          <source>This operation returns N 1-D integer tensors representing shape of &lt;code&gt;input[i]s&lt;/code&gt;.</source>
          <target state="translated">此操作返回代表 &lt;code&gt;input[i]s&lt;/code&gt; 形状的N个一维整数张量。</target>
        </trans-unit>
        <trans-unit id="fe8665b789d92e174b87e3b8d8b3131e0c5a95d4" translate="yes" xml:space="preserve">
          <source>This operation returns a 1-D integer tensor representing the shape of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">此操作返回表示 &lt;code&gt;input&lt;/code&gt; 形状的一维整数张量。</target>
        </trans-unit>
        <trans-unit id="7a59463d2fe92b4018e2beec28525b7ad3bbf202" translate="yes" xml:space="preserve">
          <source>This operation returns a tensor &lt;code&gt;y&lt;/code&gt; containing all of the unique elements of &lt;code&gt;x&lt;/code&gt; sorted in the same order that they occur in &lt;code&gt;x&lt;/code&gt;. This operation also returns a tensor &lt;code&gt;idx&lt;/code&gt; the same size as &lt;code&gt;x&lt;/code&gt; that contains the index of each value of &lt;code&gt;x&lt;/code&gt; in the unique output &lt;code&gt;y&lt;/code&gt;. Finally, it returns a third tensor &lt;code&gt;count&lt;/code&gt; that contains the count of each element of &lt;code&gt;y&lt;/code&gt; in &lt;code&gt;x&lt;/code&gt;. In other words:</source>
          <target state="translated">此操作返回一个张量 &lt;code&gt;y&lt;/code&gt; 包含所有的独特元素 &lt;code&gt;x&lt;/code&gt; 以相同的顺序进行排序，它们发生在 &lt;code&gt;x&lt;/code&gt; 。此操作还返回一个张量 &lt;code&gt;idx&lt;/code&gt; ,其大小与 &lt;code&gt;x&lt;/code&gt; 相同，其中包含唯一输出 &lt;code&gt;y&lt;/code&gt; 中 &lt;code&gt;x&lt;/code&gt; 的每个值的索引。最后，它返回一第三张量 &lt;code&gt;count&lt;/code&gt; 包含的各要素的计数 &lt;code&gt;y&lt;/code&gt; 在 &lt;code&gt;x&lt;/code&gt; 。换一种说法：</target>
        </trans-unit>
        <trans-unit id="536871e049e74061332eb50e7568b95429f16ba6" translate="yes" xml:space="preserve">
          <source>This operation returns a tensor &lt;code&gt;y&lt;/code&gt; containing all of the unique elements of &lt;code&gt;x&lt;/code&gt; sorted in the same order that they occur in &lt;code&gt;x&lt;/code&gt;; &lt;code&gt;x&lt;/code&gt; does not need to be sorted. This operation also returns a tensor &lt;code&gt;idx&lt;/code&gt; the same size as &lt;code&gt;x&lt;/code&gt; that contains the index of each value of &lt;code&gt;x&lt;/code&gt; in the unique output &lt;code&gt;y&lt;/code&gt;. In other words:</source>
          <target state="translated">此操作返回一个张量 &lt;code&gt;y&lt;/code&gt; 包含所有的独特元素 &lt;code&gt;x&lt;/code&gt; 以相同的顺序进行排序，它们发生在 &lt;code&gt;x&lt;/code&gt; ; &lt;code&gt;x&lt;/code&gt; 不需要排序。此操作还返回一个张量 &lt;code&gt;idx&lt;/code&gt; ,其大小与 &lt;code&gt;x&lt;/code&gt; 相同，其中包含唯一输出 &lt;code&gt;y&lt;/code&gt; 中 &lt;code&gt;x&lt;/code&gt; 的每个值的索引。换一种说法：</target>
        </trans-unit>
        <trans-unit id="08bfd9d4b0608003900efe52523c1412fe2e10f1" translate="yes" xml:space="preserve">
          <source>This operation returns a tensor of type &lt;code&gt;dtype&lt;/code&gt; with shape &lt;code&gt;shape&lt;/code&gt; and all elements set to one.</source>
          <target state="translated">此操作将返回 &lt;code&gt;dtype&lt;/code&gt; 类型的张量，其形状为 &lt;code&gt;shape&lt;/code&gt; 且所有元素均设置为1。</target>
        </trans-unit>
        <trans-unit id="46403ed383d4150a23258a456b0faa0e79d56dbe" translate="yes" xml:space="preserve">
          <source>This operation returns a tensor of type &lt;code&gt;dtype&lt;/code&gt; with shape &lt;code&gt;shape&lt;/code&gt; and all elements set to zero.</source>
          <target state="translated">此操作返回类型的张量 &lt;code&gt;dtype&lt;/code&gt; 具有形状 &lt;code&gt;shape&lt;/code&gt; 和设置为零的所有元素。</target>
        </trans-unit>
        <trans-unit id="476cbc978c015f266a3353221077ac3bad3036de" translate="yes" xml:space="preserve">
          <source>This operation returns a tensor with the &lt;code&gt;diagonal&lt;/code&gt; part of the &lt;code&gt;input&lt;/code&gt;. The &lt;code&gt;diagonal&lt;/code&gt; part is computed as follows:</source>
          <target state="translated">此操作返回带有 &lt;code&gt;input&lt;/code&gt; &lt;code&gt;diagonal&lt;/code&gt; 部分的张量。的 &lt;code&gt;diagonal&lt;/code&gt; 部分计算如下：</target>
        </trans-unit>
        <trans-unit id="4aefd218f88fe56a44f84be48601325a8bfff9a5" translate="yes" xml:space="preserve">
          <source>This operation returns a tensor with the &lt;code&gt;diagonal&lt;/code&gt; part of the batched &lt;code&gt;input&lt;/code&gt;. The &lt;code&gt;diagonal&lt;/code&gt; part is computed as follows:</source>
          <target state="translated">此操作将返回一个张量，该张量具有成批 &lt;code&gt;input&lt;/code&gt; 的 &lt;code&gt;diagonal&lt;/code&gt; 部分。的 &lt;code&gt;diagonal&lt;/code&gt; 部分计算如下：</target>
        </trans-unit>
        <trans-unit id="19c9a08587b8ac6d36c602ec8cf419295278168f" translate="yes" xml:space="preserve">
          <source>This operation returns an integer representing the number of elements in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">此操作返回一个整数，该整数表示 &lt;code&gt;input&lt;/code&gt; 中的元素数。</target>
        </trans-unit>
        <trans-unit id="283ea59f55046522de219f0f2dc075ad8e2f3a65" translate="yes" xml:space="preserve">
          <source>This operation returns an integer representing the rank of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">此操作返回一个整数，该整数表示 &lt;code&gt;input&lt;/code&gt; 的等级。</target>
        </trans-unit>
        <trans-unit id="c7e2028f3f49e171315c3ed603528d9f2314c768" translate="yes" xml:space="preserve">
          <source>This operation returns the coordinates of true elements in &lt;code&gt;condition&lt;/code&gt;. The coordinates are returned in a 2-D tensor where the first dimension (rows) represents the number of true elements, and the second dimension (columns) represents the coordinates of the true elements. Keep in mind, the shape of the output tensor can vary depending on how many true values there are in &lt;code&gt;condition&lt;/code&gt;. Indices are output in row-major order.</source>
          <target state="translated">此操作返回 &lt;code&gt;condition&lt;/code&gt; 中真实元素的坐标。坐标以二维张量返回，其中第一维（行）代表真实元素的数量，第二维（列）代表真实元素的坐标。请记住，输出张量的形状可能会根据 &lt;code&gt;condition&lt;/code&gt; 有多少个真实值而变化。索引以行优先顺序输出。</target>
        </trans-unit>
        <trans-unit id="9768f017e85a1cd0efa99d0dd71cd4a51ae98fd1" translate="yes" xml:space="preserve">
          <source>This operation returns the result of a TPU compilation as a serialized CompilationResultProto, which holds a status and an error message if an error occurred during compilation.</source>
          <target state="translated">该操作将 TPU 编译的结果以序列化的 CompilationResultProto 的形式返回,如果在编译过程中发生了错误,它将保存一个状态和错误信息。</target>
        </trans-unit>
        <trans-unit id="e2b8bbf455ec7dda7bf033c368dcb34b158ee06e" translate="yes" xml:space="preserve">
          <source>This operation returns the same result as the C++ std::nextafter function.</source>
          <target state="translated">这个操作返回与C++std::nextafter函数相同的结果。</target>
        </trans-unit>
        <trans-unit id="cba2fef602700f559d54f64ca39679c7fe5a39cf" translate="yes" xml:space="preserve">
          <source>This operation returns true if the queue is closed and false if the queue is open.</source>
          <target state="translated">如果队列关闭,该操作返回true,如果队列打开,则返回false。</target>
        </trans-unit>
        <trans-unit id="dc131f95b04b052706f9dc5a2e0863ccce212278" translate="yes" xml:space="preserve">
          <source>This operation signals that no more elements will be enqueued in the given queue. Subsequent &lt;code&gt;enqueue&lt;/code&gt; and &lt;code&gt;enqueue_many&lt;/code&gt; operations will fail. Subsequent &lt;code&gt;dequeue&lt;/code&gt; and &lt;code&gt;dequeue_many&lt;/code&gt; operations will continue to succeed if sufficient elements remain in the queue. Subsequently dequeue and dequeue_many operations that would otherwise block waiting for more elements (if close hadn't been called) will now fail immediately.</source>
          <target state="translated">此操作表明不再有任何元素排入给定队列。随后的 &lt;code&gt;enqueue&lt;/code&gt; 和 &lt;code&gt;enqueue_many&lt;/code&gt; 操作将失败。如果队列中剩余足够的元素，则后续的 &lt;code&gt;dequeue&lt;/code&gt; 和 &lt;code&gt;dequeue_many&lt;/code&gt; 操作将继续成功。随后的dequeue和dequeue_many操作本来会立即失败，否则这些操作将阻塞等待更多的元素（如果尚未调用close）。</target>
        </trans-unit>
        <trans-unit id="03d601007c00ecaaf281c99e7ccdb7f989fc3f40" translate="yes" xml:space="preserve">
          <source>This operation signals that no more elements will be enqueued in the given queue. Subsequent Enqueue(Many) operations will fail. Subsequent Dequeue(Many) operations will continue to succeed if sufficient elements remain in the queue. Subsequent Dequeue(Many) operations that would block will fail immediately.</source>
          <target state="translated">这个操作标志着在给定的队列中不会再有元素被enqueued,随后的Enqueue(Many)操作将失败。随后的Enqueue(Many)操作将失败。如果队列中仍有足够的元素,随后的Dequeue(Many)操作将继续成功。随后的Dequeue(Many)操作如果会阻塞,将立即失败。</target>
        </trans-unit>
        <trans-unit id="5ec52846420c7cbfc81445413d50c93e2addb827" translate="yes" xml:space="preserve">
          <source>This operation signals that no more new elements will be inserted in the given barrier. Subsequent InsertMany that try to introduce a new key will fail. Subsequent InsertMany operations that just add missing components to already existing elements will continue to succeed. Subsequent TakeMany operations will continue to succeed if sufficient completed elements remain in the barrier. Subsequent TakeMany operations that would block will fail immediately.</source>
          <target state="translated">这个操作标志着在给定的障碍中不会再插入新的元素。随后的InsertMany如果试图引入一个新的键,将会失败。随后的InsertMany操作如果只是将缺少的元素添加到已经存在的元素中,将继续成功。如果在屏障中保留了足够多的已完成元素,后续的TakeMany操作将继续成功。随后的TakeMany操作如果会阻塞,将立即失败。</target>
        </trans-unit>
        <trans-unit id="ed8ef290710f5b794405ab349bc8048a526b3051" translate="yes" xml:space="preserve">
          <source>This operation slices each component tensor along the 0th dimension to make multiple queue elements. All of the tensors in &lt;code&gt;vals&lt;/code&gt; must have the same size in the 0th dimension.</source>
          <target state="translated">此操作沿第0维对每个分量张量进行切片，以形成多个队列元素。所有在张量的 &lt;code&gt;vals&lt;/code&gt; 必须在第0尺寸大小相同。</target>
        </trans-unit>
        <trans-unit id="b7a9602cce6855f9551ec64964df2f6660343d5e" translate="yes" xml:space="preserve">
          <source>This operation slices each component tensor along the 0th dimension to make multiple queue elements. All of the tuple components must have the same size in the 0th dimension.</source>
          <target state="translated">该操作沿第0维度对每个分量张量进行切片,使之成为多个队列元素。所有的元组分量在第0维度上必须具有相同的大小。</target>
        </trans-unit>
        <trans-unit id="b06acef6473406f9a6972f0bdd65c856a80109e7" translate="yes" xml:space="preserve">
          <source>This operation takes variable-length sequences (&lt;code&gt;hypothesis&lt;/code&gt; and &lt;code&gt;truth&lt;/code&gt;), each provided as a &lt;code&gt;SparseTensor&lt;/code&gt;, and computes the Levenshtein distance. You can normalize the edit distance by length of &lt;code&gt;truth&lt;/code&gt; by setting &lt;code&gt;normalize&lt;/code&gt; to true.</source>
          <target state="translated">此操作采用可变长度序列（ &lt;code&gt;hypothesis&lt;/code&gt; 和 &lt;code&gt;truth&lt;/code&gt; ），每个序列 &lt;code&gt;SparseTensor&lt;/code&gt; 提供，并计算Levenshtein距离。您可以通过标准化的长度编辑距离 &lt;code&gt;truth&lt;/code&gt; 通过设置 &lt;code&gt;normalize&lt;/code&gt; 为true。</target>
        </trans-unit>
        <trans-unit id="4b844fb17a610af4c2dfbda43ce1ac8c2ea894a7" translate="yes" xml:space="preserve">
          <source>This operation tends to perform well when &lt;code&gt;A&lt;/code&gt; is more sparse, if the column size of the product is small (e.g. matrix-vector multiplication), if &lt;code&gt;sp_a.dense_shape&lt;/code&gt; takes on large values.</source>
          <target state="translated">如果乘积的列大小较小（例如，矩阵向量乘法），并且 &lt;code&gt;sp_a.dense_shape&lt;/code&gt; 取大值，则当 &lt;code&gt;A&lt;/code&gt; 较稀疏时，此操作往往会执行良好。</target>
        </trans-unit>
        <trans-unit id="656ae1e1362cbc36310acc8deee39f6903aad8f9" translate="yes" xml:space="preserve">
          <source>This operation will block indefinitely until data is available.</source>
          <target state="translated">此操作将无限期地封锁,直到有数据可用。</target>
        </trans-unit>
        <trans-unit id="ca0db1beb7905bc51f7f1596562bcd98102255d3" translate="yes" xml:space="preserve">
          <source>This operation will block indefinitely until data is available. Output &lt;code&gt;i&lt;/code&gt; corresponds to XLA tuple element &lt;code&gt;i&lt;/code&gt;.</source>
          <target state="translated">此操作将无限期阻塞，直到有可用数据为止。输出 &lt;code&gt;i&lt;/code&gt; 对应于XLA元组元素 &lt;code&gt;i&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1051e1cb11d005837380c93974db8cb36d834ab5" translate="yes" xml:space="preserve">
          <source>This operation will generate a string suitable to be saved out to create a .wav audio file. It will be encoded in the 16-bit PCM format. It takes in float values in the range -1.0f to 1.0f, and any outside that value will be clamped to that range.</source>
          <target state="translated">此操作将产生一个适合保存的字符串,以创建一个.wav音频文件。它将以16位PCM格式编码。它接收的浮点数范围为-1.0f至1.0f,任何超出该值的数值将被钳制在该范围内。</target>
        </trans-unit>
        <trans-unit id="bf6b9f0d6051cd83422b6ad2cb1617b5b6c44a65" translate="yes" xml:space="preserve">
          <source>This operation will output a tensor of shape &lt;code&gt;[1, 1, 1, 4]&lt;/code&gt;:</source>
          <target state="translated">此操作将输出形状为 &lt;code&gt;[1, 1, 1, 4]&lt;/code&gt; 1，1，1，4]的张量：</target>
        </trans-unit>
        <trans-unit id="01c0abdcd1d63eef5d525eb4d621b9072cafca4f" translate="yes" xml:space="preserve">
          <source>This operation will output a tensor of shape &lt;code&gt;[1, 2, 2, 1]&lt;/code&gt;:</source>
          <target state="translated">此操作将输出形状为 &lt;code&gt;[1, 2, 2, 1]&lt;/code&gt; 1、2、2、1]的张量：</target>
        </trans-unit>
        <trans-unit id="f9c1bb170834121c155816075bc30535b34c3026" translate="yes" xml:space="preserve">
          <source>This operation would return the following:</source>
          <target state="translated">该操作将返回以下内容:</target>
        </trans-unit>
        <trans-unit id="c09d355015c640623e774187c625203da78fb3a6" translate="yes" xml:space="preserve">
          <source>This operation would return:</source>
          <target state="translated">这个操作会返回。</target>
        </trans-unit>
        <trans-unit id="6585b279d1753d7936601f5ce821925656bfd5b2" translate="yes" xml:space="preserve">
          <source>This operation, for block size of 2, will return the following tensor of shape &lt;code&gt;[1, 2, 2, 3]&lt;/code&gt;</source>
          <target state="translated">对于块大小为2的此操作，将返回以下形状的张量 &lt;code&gt;[1, 2, 2, 3]&lt;/code&gt; 1,2,2,3 ]</target>
        </trans-unit>
        <trans-unit id="123e5c864ddc20e423825601cb576b64af53b67e" translate="yes" xml:space="preserve">
          <source>This operation, for block_size of 2, will return the following tensor of shape &lt;code&gt;[1, 1, 1, 12]&lt;/code&gt;</source>
          <target state="translated">对于block_size为2的此操作，将返回以下形状为 &lt;code&gt;[1, 1, 1, 12]&lt;/code&gt; 1，1，1，12]的张量</target>
        </trans-unit>
        <trans-unit id="2a0a130eb7e8314a609a2db135e21936c4a0de8b" translate="yes" xml:space="preserve">
          <source>This operator acts like a (batch) matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, M, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;m x n&lt;/code&gt; matrix. Again, this matrix &lt;code&gt;A&lt;/code&gt; may not be materialized, but for purposes of identifying and working with compatible arguments the shape is relevant.</source>
          <target state="translated">这个操作符的作用就像一个（批）矩阵 &lt;code&gt;A&lt;/code&gt; 具有形状 &lt;code&gt;[B1,...,Bb, M, N]&lt;/code&gt; 对于某些 &lt;code&gt;b &amp;gt;= 0&lt;/code&gt; 。前 &lt;code&gt;b&lt;/code&gt; 个索引对批处理成员进行索引。对于每个批次索引 &lt;code&gt;(i1,...,ib)&lt;/code&gt; ， &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; 是一个 &lt;code&gt;m x n&lt;/code&gt; 矩阵。同样，此矩阵 &lt;code&gt;A&lt;/code&gt; 可能不会实现，但出于识别和使用兼容参数的目的，形状是相关的。</target>
        </trans-unit>
        <trans-unit id="9624d2140d2af8604c19c56292430ec7141d96ff" translate="yes" xml:space="preserve">
          <source>This operator acts like a [batch] Toeplitz matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x N&lt;/code&gt; matrix. This matrix &lt;code&gt;A&lt;/code&gt; is not materialized, but for purposes of broadcasting this shape will be relevant.</source>
          <target state="translated">这个操作符的作用就像一个[批次]托普利兹矩阵 &lt;code&gt;A&lt;/code&gt; 具有形状 &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; 对于某些 &lt;code&gt;b &amp;gt;= 0&lt;/code&gt; 。前 &lt;code&gt;b&lt;/code&gt; 个索引对批处理成员进行索引。对于每个批次索引 &lt;code&gt;(i1,...,ib)&lt;/code&gt; ， &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; 是一个 &lt;code&gt;N x N&lt;/code&gt; 矩阵。该矩阵 &lt;code&gt;A&lt;/code&gt; 尚未实现，但出于广播目的，此形状将是相关的。</target>
        </trans-unit>
        <trans-unit id="ae991b61103c038a5ab6e26593ecbc7152e117f2" translate="yes" xml:space="preserve">
          <source>This operator acts like a [batch] diagonal matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x N&lt;/code&gt; matrix. This matrix &lt;code&gt;A&lt;/code&gt; is not materialized, but for purposes of broadcasting this shape will be relevant.</source>
          <target state="translated">这个操作符的作用就像一个[批次]对角矩阵 &lt;code&gt;A&lt;/code&gt; 具有形状 &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; 对于某些 &lt;code&gt;b &amp;gt;= 0&lt;/code&gt; 。前 &lt;code&gt;b&lt;/code&gt; 个索引对批处理成员进行索引。对于每个批次索引 &lt;code&gt;(i1,...,ib)&lt;/code&gt; ， &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; 是一个 &lt;code&gt;N x N&lt;/code&gt; 矩阵。该矩阵 &lt;code&gt;A&lt;/code&gt; 尚未实现，但出于广播目的，此形状将是相关的。</target>
        </trans-unit>
        <trans-unit id="5eea0437af7326077ca375b7627f553585750e30" translate="yes" xml:space="preserve">
          <source>This operator acts like a [batch] identity matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x N&lt;/code&gt; matrix. This matrix &lt;code&gt;A&lt;/code&gt; is not materialized, but for purposes of broadcasting this shape will be relevant.</source>
          <target state="translated">这个操作符的作用就像一个[批次]同一性矩阵 &lt;code&gt;A&lt;/code&gt; 具有形状 &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; 对于某些 &lt;code&gt;b &amp;gt;= 0&lt;/code&gt; 。前 &lt;code&gt;b&lt;/code&gt; 个索引对批处理成员进行索引。对于每个批次索引 &lt;code&gt;(i1,...,ib)&lt;/code&gt; ， &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; 是一个 &lt;code&gt;N x N&lt;/code&gt; 矩阵。该矩阵 &lt;code&gt;A&lt;/code&gt; 尚未实现，但出于广播目的，此形状将是相关的。</target>
        </trans-unit>
        <trans-unit id="7036bf9c433fed406d0eb267aee69b2fb875c75f" translate="yes" xml:space="preserve">
          <source>This operator acts like a [batch] lower triangular matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x N&lt;/code&gt; matrix.</source>
          <target state="translated">对于某些 &lt;code&gt;b &amp;gt;= 0&lt;/code&gt; &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; 此算子的作用类似于[batch]下三角矩阵 &lt;code&gt;A&lt;/code&gt; ，其形状为[B1，...，Bb，N，N]。前 &lt;code&gt;b&lt;/code&gt; 个索引对批处理成员进行索引。对于每个批次索引 &lt;code&gt;(i1,...,ib)&lt;/code&gt; ， &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; 是一个 &lt;code&gt;N x N&lt;/code&gt; 矩阵。</target>
        </trans-unit>
        <trans-unit id="0d5825859d807d72eb10161ef6b5b940c6fe0b9b" translate="yes" xml:space="preserve">
          <source>This operator acts like a [batch] matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, M, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;M x N&lt;/code&gt; matrix.</source>
          <target state="translated">这个操作符的作用就像一个[批次]矩阵 &lt;code&gt;A&lt;/code&gt; 具有形状 &lt;code&gt;[B1,...,Bb, M, N]&lt;/code&gt; 对于某些 &lt;code&gt;b &amp;gt;= 0&lt;/code&gt; 。前 &lt;code&gt;b&lt;/code&gt; 个索引对批处理成员进行索引。对于每个批次索引 &lt;code&gt;(i1,...,ib)&lt;/code&gt; ， &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; 是一个 &lt;code&gt;M x N&lt;/code&gt; 矩阵。</target>
        </trans-unit>
        <trans-unit id="6f99131310964b3eb6cd5147dace10a254e9fff2" translate="yes" xml:space="preserve">
          <source>This operator acts like a [batch] of householder reflections with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x N&lt;/code&gt; matrix. This matrix &lt;code&gt;A&lt;/code&gt; is not materialized, but for purposes of broadcasting this shape will be relevant.</source>
          <target state="translated">对于某些 &lt;code&gt;b &amp;gt;= 0&lt;/code&gt; &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; 此算子的作用类似于形状为[B1，...，Bb，N，N]的家用反射的[batch] 。前 &lt;code&gt;b&lt;/code&gt; 个索引对批处理成员进行索引。对于每个批次索引 &lt;code&gt;(i1,...,ib)&lt;/code&gt; ， &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; 是一个 &lt;code&gt;N x N&lt;/code&gt; 矩阵。该矩阵 &lt;code&gt;A&lt;/code&gt; 尚未实现，但出于广播目的，此形状将是相关的。</target>
        </trans-unit>
        <trans-unit id="2c9497726784db48f14f606b75baa3af89ad92c3" translate="yes" xml:space="preserve">
          <source>This operator acts like a [batch] of permutations with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x N&lt;/code&gt; matrix. This matrix &lt;code&gt;A&lt;/code&gt; is not materialized, but for purposes of broadcasting this shape will be relevant.</source>
          <target state="translated">对于某些 &lt;code&gt;b &amp;gt;= 0&lt;/code&gt; &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; 此运算符的作用类似于形状为[B1，...，Bb，N，N]的排列的[batch] 。前 &lt;code&gt;b&lt;/code&gt; 个索引对批处理成员进行索引。对于每个批次索引 &lt;code&gt;(i1,...,ib)&lt;/code&gt; ， &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; 是一个 &lt;code&gt;N x N&lt;/code&gt; 矩阵。该矩阵 &lt;code&gt;A&lt;/code&gt; 尚未实现，但出于广播目的，此形状将是相关的。</target>
        </trans-unit>
        <trans-unit id="583ba682dbf34a68e239431a7d0b3d3bb383323a" translate="yes" xml:space="preserve">
          <source>This operator acts like a [batch] square tridiagonal matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x M&lt;/code&gt; matrix. This matrix &lt;code&gt;A&lt;/code&gt; is not materialized, but for purposes of broadcasting this shape will be relevant.</source>
          <target state="translated">这个操作符的作用就像一个[批次]平方三对角矩阵 &lt;code&gt;A&lt;/code&gt; 具有形状 &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; 对于某些 &lt;code&gt;b &amp;gt;= 0&lt;/code&gt; 。前 &lt;code&gt;b&lt;/code&gt; 个索引对批处理成员进行索引。对于每个批次索引 &lt;code&gt;(i1,...,ib)&lt;/code&gt; ， &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; 是一个 &lt;code&gt;N x M&lt;/code&gt; 矩阵。该矩阵 &lt;code&gt;A&lt;/code&gt; 尚未实现，但出于广播目的，此形状将是相关的。</target>
        </trans-unit>
        <trans-unit id="400eb5597c413c726e34a9f1e1bb7b69ea62bb78" translate="yes" xml:space="preserve">
          <source>This operator acts like a [batch] zero matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, N, M]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x M&lt;/code&gt; matrix. This matrix &lt;code&gt;A&lt;/code&gt; is not materialized, but for purposes of broadcasting this shape will be relevant.</source>
          <target state="translated">这个操作符的作用就像一个[批次]零矩阵 &lt;code&gt;A&lt;/code&gt; 具有形状 &lt;code&gt;[B1,...,Bb, N, M]&lt;/code&gt; 对于某些 &lt;code&gt;b &amp;gt;= 0&lt;/code&gt; 。前 &lt;code&gt;b&lt;/code&gt; 个索引对批处理成员进行索引。对于每个批次索引 &lt;code&gt;(i1,...,ib)&lt;/code&gt; ， &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; 是一个 &lt;code&gt;N x M&lt;/code&gt; 矩阵。该矩阵 &lt;code&gt;A&lt;/code&gt; 尚未实现，但出于广播目的，此形状将是相关的。</target>
        </trans-unit>
        <trans-unit id="b7102c53ecb6c13af874ea168315014a6e7dc84b" translate="yes" xml:space="preserve">
          <source>This operator acts like a block circulant matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x N&lt;/code&gt; matrix. This matrix &lt;code&gt;A&lt;/code&gt; is not materialized, but for purposes of broadcasting this shape will be relevant.</source>
          <target state="translated">这个操作符的作用就像一个块循环矩阵 &lt;code&gt;A&lt;/code&gt; 具有形状 &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; 对于某些 &lt;code&gt;b &amp;gt;= 0&lt;/code&gt; 。前 &lt;code&gt;b&lt;/code&gt; 个索引对批处理成员进行索引。对于每个批次索引 &lt;code&gt;(i1,...,ib)&lt;/code&gt; ， &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; 是一个 &lt;code&gt;N x N&lt;/code&gt; 矩阵。该矩阵 &lt;code&gt;A&lt;/code&gt; 尚未实现，但出于广播目的，此形状将是相关的。</target>
        </trans-unit>
        <trans-unit id="4c216274d20b8ffa480f3dfffdabad95ff0cda74" translate="yes" xml:space="preserve">
          <source>This operator acts like a circulant matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;N x N&lt;/code&gt; matrix. This matrix &lt;code&gt;A&lt;/code&gt; is not materialized, but for purposes of broadcasting this shape will be relevant.</source>
          <target state="translated">这个操作符的作用就像一个循环矩阵 &lt;code&gt;A&lt;/code&gt; 具有形状 &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; 对于某些 &lt;code&gt;b &amp;gt;= 0&lt;/code&gt; 。前 &lt;code&gt;b&lt;/code&gt; 个索引对批处理成员进行索引。对于每个批次索引 &lt;code&gt;(i1,...,ib)&lt;/code&gt; ， &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; 是一个 &lt;code&gt;N x N&lt;/code&gt; 矩阵。该矩阵 &lt;code&gt;A&lt;/code&gt; 尚未实现，但出于广播目的，此形状将是相关的。</target>
        </trans-unit>
        <trans-unit id="0f0a750eb3c4adb095723453eef766bb4bcd89f8" translate="yes" xml:space="preserve">
          <source>This operator acts like a scaled [batch] identity matrix &lt;code&gt;A&lt;/code&gt; with shape &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is a scaled version of the &lt;code&gt;N x N&lt;/code&gt; identity matrix.</source>
          <target state="translated">这个操作符的作用就像一个缩放[批次]同一性矩阵 &lt;code&gt;A&lt;/code&gt; 具有形状 &lt;code&gt;[B1,...,Bb, N, N]&lt;/code&gt; 对于某些 &lt;code&gt;b &amp;gt;= 0&lt;/code&gt; 。前 &lt;code&gt;b&lt;/code&gt; 个索引对批处理成员进行索引。对于每个批次索引 &lt;code&gt;(i1,...,ib)&lt;/code&gt; ， &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; 是 &lt;code&gt;N x N&lt;/code&gt; 单位矩阵的缩放版本。</target>
        </trans-unit>
        <trans-unit id="8b1442fc44774d80a4540791883d0146d00de836" translate="yes" xml:space="preserve">
          <source>This operator acts on [batch] matrix with compatible shape. &lt;code&gt;x&lt;/code&gt; is a batch matrix with compatible shape for &lt;code&gt;matmul&lt;/code&gt; and &lt;code&gt;solve&lt;/code&gt; if</source>
          <target state="translated">该运算符作用于形状兼容的[batch]矩阵。 &lt;code&gt;x&lt;/code&gt; 是形状与 &lt;code&gt;matmul&lt;/code&gt; 兼容的批处理矩阵，并 &lt;code&gt;solve&lt;/code&gt; 是否</target>
        </trans-unit>
        <trans-unit id="a4840abea8050987eeb44a958dc938a96dc5772f" translate="yes" xml:space="preserve">
          <source>This operator acts on batch matrices with compatible shape. FILL IN WHAT IS MEANT BY COMPATIBLE SHAPE</source>
          <target state="translated">这个运算符作用于具有兼容形状的批处理矩阵。请填写兼容形状的含义。</target>
        </trans-unit>
        <trans-unit id="5cfb7dc84da5837b6c81afa0e3c1dd415d37bda4" translate="yes" xml:space="preserve">
          <source>This operator broadcasts the batch dimensions of &lt;code&gt;bands&lt;/code&gt; and the batch dimensions of &lt;code&gt;rhs&lt;/code&gt;.</source>
          <target state="translated">该运算符广播 &lt;code&gt;bands&lt;/code&gt; 的批处理尺寸和 &lt;code&gt;rhs&lt;/code&gt; 的批处理尺寸。</target>
        </trans-unit>
        <trans-unit id="53228a3fac3cf2f50966b1ada24ef3b0466deb1e" translate="yes" xml:space="preserve">
          <source>This operator combines one or more linear operators &lt;code&gt;[op1,...,opJ]&lt;/code&gt;, building a new &lt;code&gt;LinearOperator&lt;/code&gt;, whose underlying matrix representation is square and has each operator &lt;code&gt;opi&lt;/code&gt; on the main diagonal, and zero's elsewhere.</source>
          <target state="translated">该运算符组合一个或多个线性运算符 &lt;code&gt;[op1,...,opJ]&lt;/code&gt; ，构建一个新的 &lt;code&gt;LinearOperator&lt;/code&gt; ，其基础矩阵表示为正方形，每个运算符 &lt;code&gt;opi&lt;/code&gt; 在主对角线上，其他位置为零。</target>
        </trans-unit>
        <trans-unit id="564367f646774e8a861547adb060150a45779d5f" translate="yes" xml:space="preserve">
          <source>This operator composes one or more linear operators &lt;code&gt;[op1,...,opJ]&lt;/code&gt;, building a new &lt;code&gt;LinearOperator&lt;/code&gt; representing the Kronecker product: &lt;code&gt;op1 x op2 x .. opJ&lt;/code&gt; (we omit parentheses as the Kronecker product is associative).</source>
          <target state="translated">该运算符组成一个或多个线性运算符 &lt;code&gt;[op1,...,opJ]&lt;/code&gt; ，构建一个表示Kronecker产品的新 &lt;code&gt;LinearOperator&lt;/code&gt; ： &lt;code&gt;op1 x op2 x .. opJ&lt;/code&gt; （由于Kronecker产品是关联的，因此省略了括号）。</target>
        </trans-unit>
        <trans-unit id="0c0d85ebac206e74618bf7317ca7c96fff15c835" translate="yes" xml:space="preserve">
          <source>This operator composes one or more linear operators &lt;code&gt;[op1,...,opJ]&lt;/code&gt;, building a new &lt;code&gt;LinearOperator&lt;/code&gt; with action defined by:</source>
          <target state="translated">此运算符组成一个或多个线性运算符 &lt;code&gt;[op1,...,opJ]&lt;/code&gt; ，以定义以下操作来构建新的 &lt;code&gt;LinearOperator&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="8e7241eaa8846c5c4338cb18a140b07ab3b606ae" translate="yes" xml:space="preserve">
          <source>This operator corresponds to a real matrix if and only if &lt;code&gt;H&lt;/code&gt; is Hermitian.</source>
          <target state="translated">当且仅当 &lt;code&gt;H&lt;/code&gt; 为Hermitian时，此算符对应于实矩阵。</target>
        </trans-unit>
        <trans-unit id="6f9242e838252063460c0276fd01dfac0ef01ff4" translate="yes" xml:space="preserve">
          <source>This operator corresponds to a real-valued matrix if and only if its spectrum is Hermitian.</source>
          <target state="translated">如果且仅当其频谱为赫米特时,这个算子对应于实值矩阵。</target>
        </trans-unit>
        <trans-unit id="3bcff2453a32e7e855e1bf28e92cc09521b4d919" translate="yes" xml:space="preserve">
          <source>This operator has two modes: in one mode both &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are provided, in another mode neither are provided. &lt;code&gt;condition&lt;/code&gt; is always expected to be a &lt;a href=&quot;tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt; of type &lt;code&gt;bool&lt;/code&gt;.</source>
          <target state="translated">该运算符有两种模式：在一种模式下， &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 都提供，在另一种模式下，都不提供。通常认为 &lt;code&gt;condition&lt;/code&gt; 是 &lt;code&gt;bool&lt;/code&gt; 类型的&lt;a href=&quot;tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c618e51b272c823eef5139b9cc3a948f981f98e8" translate="yes" xml:space="preserve">
          <source>This operator is able to broadcast the leading (batch) dimensions, which sometimes requires copying data. If &lt;code&gt;batch_shape&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, the operator can take arguments of any batch shape without copying. See examples.</source>
          <target state="translated">该操作员能够广播前导（批量）尺寸，有时这需要复制数据。如果 &lt;code&gt;batch_shape&lt;/code&gt; 为 &lt;code&gt;None&lt;/code&gt; ，则运算符可以采用任何批处理形状的参数，而无需复制。参见示例。</target>
        </trans-unit>
        <trans-unit id="66cb218ad1e2e8f68e038b90d3bfb45bd6e74e63" translate="yes" xml:space="preserve">
          <source>This operator is able to broadcast the leading (batch) dimensions.</source>
          <target state="translated">该运算符能够广播领先(批量)的尺寸。</target>
        </trans-unit>
        <trans-unit id="348a23cb6559c85829c515ac4244685e0efb46f2" translate="yes" xml:space="preserve">
          <source>This operator is considered non-singular if</source>
          <target state="translated">这个运算符在以下情况下被认为是非歌唱性的</target>
        </trans-unit>
        <trans-unit id="b5e0ce71bd6146dbac079f3d21ad0a2a2a8c2054" translate="yes" xml:space="preserve">
          <source>This operator is initialized with a nested list of linear operators, which are combined into a new &lt;code&gt;LinearOperator&lt;/code&gt; whose underlying matrix representation is square and has each operator on or below the main diagonal, and zero's elsewhere. Each element of the outer list is a list of &lt;code&gt;LinearOperators&lt;/code&gt; corresponding to a row-partition of the blockwise structure. The number of &lt;code&gt;LinearOperator&lt;/code&gt;s in row-partion &lt;code&gt;i&lt;/code&gt; must be equal to &lt;code&gt;i&lt;/code&gt;.</source>
          <target state="translated">该运算符由嵌套的线性运算符列表初始化，这些嵌套的线性运算符组合成一个新的 &lt;code&gt;LinearOperator&lt;/code&gt; ,其基础矩阵表示为正方形，并且每个运算符位于主对角线上或下方，其他位置为零。外部列表的每个元素 &lt;code&gt;LinearOperators&lt;/code&gt; 对应于块结构的行分区的LinearOperators列表。行部分 &lt;code&gt;i&lt;/code&gt; 中的 &lt;code&gt;LinearOperator&lt;/code&gt; s的数量必须等于 &lt;code&gt;i&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d06a5054c0e8f5a488a0e50140ca1317d8ee8b76" translate="yes" xml:space="preserve">
          <source>This operator is positive definite if and only if &lt;code&gt;Real{H} &amp;gt; 0&lt;/code&gt;.</source>
          <target state="translated">当且仅当 &lt;code&gt;Real{H} &amp;gt; 0&lt;/code&gt; 此运算符才为正定。</target>
        </trans-unit>
        <trans-unit id="fa787331edbdbf77b6543a837e62e8acfa555fdd" translate="yes" xml:space="preserve">
          <source>This operator is self-adjoint if and only if &lt;code&gt;H&lt;/code&gt; is real.</source>
          <target state="translated">当且仅当 &lt;code&gt;H&lt;/code&gt; 为实数时，该算子是自伴的。</target>
        </trans-unit>
        <trans-unit id="0434efb4cb6fe37aceabe23ccb563dea7d82d49a" translate="yes" xml:space="preserve">
          <source>This operator is similar to the unsorted segment sum operator found &lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;(here)&lt;/a&gt;. Instead of computing the sum over segments, it computes the maximum such that:</source>
          <target state="translated">该运算符类似于找到的未排序的段和运算符&lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;（此处）&lt;/a&gt;。而不是计算各段的总和，而是计算最大值，以便：</target>
        </trans-unit>
        <trans-unit id="f0203b5b1fedf802a969aad64e740ba2ad439f1c" translate="yes" xml:space="preserve">
          <source>This operator is similar to the unsorted segment sum operator found &lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;(here)&lt;/a&gt;. Instead of computing the sum over segments, it computes the minimum such that:</source>
          <target state="translated">该运算符类似于找到的未排序的段和运算符&lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;（此处）&lt;/a&gt;。它不计算分段的总和，而是计算最小值，使得：</target>
        </trans-unit>
        <trans-unit id="9b525fce235ebc8f800bcb2ded78049fa74f120a" translate="yes" xml:space="preserve">
          <source>This operator is similar to the unsorted segment sum operator found &lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;(here)&lt;/a&gt;. Instead of computing the sum over segments, it computes the product of all entries belonging to a segment such that:</source>
          <target state="translated">该运算符类似于找到的未排序的段和运算符&lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;（此处）&lt;/a&gt;。它不是计算分段的总和，而是计算属于一个分段的所有条目的乘积，使得：</target>
        </trans-unit>
        <trans-unit id="59fd9e153dcbd12bffedb241b5f88000e9237c11" translate="yes" xml:space="preserve">
          <source>This operator is similar to the unsorted segment sum operator found &lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;here&lt;/a&gt;. Additionally to computing the sum over segments, it divides the results by sqrt(N).</source>
          <target state="translated">该运算符类似于&lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;此处&lt;/a&gt;找到的未排序的段和运算符。除了计算各段之和外，它还会将结果除以sqrt（N）。</target>
        </trans-unit>
        <trans-unit id="a4a7cb1afea6269197bd76bbd4732c1dbecdf7fc" translate="yes" xml:space="preserve">
          <source>This operator is similar to the unsorted segment sum operator found &lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;here&lt;/a&gt;. Instead of computing the sum over segments, it computes the mean of all entries belonging to a segment such that:</source>
          <target state="translated">该运算符类似于&lt;a href=&quot;https://www.tensorflow.org/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;此处&lt;/a&gt;找到的未排序的段和运算符。它不计算段上的总和，而是计算属于段的所有条目的平均值，从而：</target>
        </trans-unit>
        <trans-unit id="52ed4dad04ede72e0a76f0501a2814cdc539eef1" translate="yes" xml:space="preserve">
          <source>This operator is similar to the unsorted segment sum operator found &lt;a href=&quot;https://www.tensorflow.org/versions/r2.3/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;(here)&lt;/a&gt;. Instead of computing the sum over segments, it computes the maximum such that:</source>
          <target state="translated">该运算符类似于找到的未排序的段和运算符&lt;a href=&quot;https://www.tensorflow.org/versions/r2.3/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;（在此）&lt;/a&gt;。而不是计算各段的总和，而是计算最大值，以便：</target>
        </trans-unit>
        <trans-unit id="d5555cd5610bd14e5cd5be64e806dbe517b909cf" translate="yes" xml:space="preserve">
          <source>This operator is similar to the unsorted segment sum operator found &lt;a href=&quot;https://www.tensorflow.org/versions/r2.3/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;(here)&lt;/a&gt;. Instead of computing the sum over segments, it computes the minimum such that:</source>
          <target state="translated">该运算符类似于找到的未排序的段和运算符&lt;a href=&quot;https://www.tensorflow.org/versions/r2.3/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;（在此）&lt;/a&gt;。它不计算分段的总和，而是计算最小值，使得：</target>
        </trans-unit>
        <trans-unit id="5626f77bd1ee157ca8ad5f5950e88d3290b6e7a5" translate="yes" xml:space="preserve">
          <source>This operator is similar to the unsorted segment sum operator found &lt;a href=&quot;https://www.tensorflow.org/versions/r2.3/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;(here)&lt;/a&gt;. Instead of computing the sum over segments, it computes the product of all entries belonging to a segment such that:</source>
          <target state="translated">该运算符类似于找到的未排序的段和运算符&lt;a href=&quot;https://www.tensorflow.org/versions/r2.3/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;（在此）&lt;/a&gt;。它不是计算分段的总和，而是计算属于一个分段的所有条目的乘积，使得：</target>
        </trans-unit>
        <trans-unit id="b4490e7220bb7e840d8af406c98df4879f8c31ca" translate="yes" xml:space="preserve">
          <source>This operator is similar to the unsorted segment sum operator found &lt;a href=&quot;https://www.tensorflow.org/versions/r2.3/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;here&lt;/a&gt;. Additionally to computing the sum over segments, it divides the results by sqrt(N).</source>
          <target state="translated">该运算符类似于&lt;a href=&quot;https://www.tensorflow.org/versions/r2.3/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;此处&lt;/a&gt;找到的未排序的段总和运算符。除了计算分段上的总和外，它还会将结果除以sqrt（N）。</target>
        </trans-unit>
        <trans-unit id="5fcc2d9bddedbecbae737fa85ec20cea5d89c2f2" translate="yes" xml:space="preserve">
          <source>This operator is similar to the unsorted segment sum operator found &lt;a href=&quot;https://www.tensorflow.org/versions/r2.3/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;here&lt;/a&gt;. Instead of computing the sum over segments, it computes the mean of all entries belonging to a segment such that:</source>
          <target state="translated">该运算符类似于&lt;a href=&quot;https://www.tensorflow.org/versions/r2.3/api_docs/api_docs/python/math_ops#UnsortedSegmentSum&quot;&gt;此处&lt;/a&gt;找到的未排序的段总和运算符。它不计算段上的总和，而是计算属于段的所有条目的均值，使得：</target>
        </trans-unit>
        <trans-unit id="83929e057e5aae77fa1c5e82493e5f81ffd7c7d5" translate="yes" xml:space="preserve">
          <source>This operator represents the adjoint of another operator.</source>
          <target state="translated">这个运算符代表另一个运算符的邻接。</target>
        </trans-unit>
        <trans-unit id="011faa1fcbdac7f7db43d7798ee8f420d25df1c3" translate="yes" xml:space="preserve">
          <source>This operator represents the inverse of another operator.</source>
          <target state="translated">这个运算符表示另一个运算符的反。</target>
        </trans-unit>
        <trans-unit id="d243b49a16809ba508894c3eb53aaf2744d8ff3a" translate="yes" xml:space="preserve">
          <source>This operator represents the loop termination condition used by the &quot;pivot&quot; switches of a loop.</source>
          <target state="translated">这个运算符表示循环的 &quot;枢轴 &quot;开关所使用的循环终止条件。</target>
        </trans-unit>
        <trans-unit id="efbc958735ecb1b79fddfae66e71db4081b1ceb7" translate="yes" xml:space="preserve">
          <source>This operator takes the given &lt;code&gt;SparseTensor&lt;/code&gt; and adds it to a container object (a &lt;code&gt;SparseTensorsMap&lt;/code&gt;). A unique key within this container is generated in the form of an &lt;code&gt;int64&lt;/code&gt;, and this is the value that is returned.</source>
          <target state="translated">此运算符采用给定的 &lt;code&gt;SparseTensor&lt;/code&gt; 并将其添加到容器对象（ &lt;code&gt;SparseTensorsMap&lt;/code&gt; ）。此容器内的唯一键以 &lt;code&gt;int64&lt;/code&gt; 的形式生成，这是返回的值。</target>
        </trans-unit>
        <trans-unit id="8beab818ce538eb5919197a98807ea337b127bd0" translate="yes" xml:space="preserve">
          <source>This operator tries to squeeze as much precision as possible into an output with a lower bit depth by calculating the actual min and max values found in the data. For example, maybe that quint16 input has no values lower than 16,384 and none higher than 49,152. That means only half the range is actually needed, all the float interpretations are between -0.5f and 0.5f, so if we want to compress the data into a quint8 output, we can use that range rather than the theoretical -1.0f to 1.0f that is suggested by the input min and max.</source>
          <target state="translated">这个运算符试图通过计算数据中的实际最小值和最大值,将尽可能多的精度压缩到一个较低位深的输出中。例如,可能那个quint16输入没有低于16,384的值,也没有高于49,152的值。这意味着实际只需要一半的范围,所有的浮点数解释都在-0.5f到0.5f之间,所以如果我们想把数据压缩成quint8输出,我们可以使用这个范围,而不是输入最小值和最大值所建议的理论上的-1.0f到1.0f。</target>
        </trans-unit>
        <trans-unit id="caac5d3bbf4d0945a7bd985970cd4106cce90915" translate="yes" xml:space="preserve">
          <source>This operator wraps a [batch] matrix &lt;code&gt;A&lt;/code&gt; (which is a &lt;code&gt;Tensor&lt;/code&gt;) with shape &lt;code&gt;[B1,...,Bb, M, N]&lt;/code&gt; for some &lt;code&gt;b &amp;gt;= 0&lt;/code&gt;. The first &lt;code&gt;b&lt;/code&gt; indices index a batch member. For every batch index &lt;code&gt;(i1,...,ib)&lt;/code&gt;, &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; is an &lt;code&gt;M x N&lt;/code&gt; matrix.</source>
          <target state="translated">该运算符包装形状为 &lt;code&gt;[B1,...,Bb, M, N]&lt;/code&gt; 的[batch]矩阵 &lt;code&gt;A&lt;/code&gt; （ &lt;code&gt;Tensor&lt;/code&gt; ），其中 &lt;code&gt;b &amp;gt;= 0&lt;/code&gt; 。前 &lt;code&gt;b&lt;/code&gt; 个索引对批处理成员进行索引。对于每个批次索引 &lt;code&gt;(i1,...,ib)&lt;/code&gt; ， &lt;code&gt;A[i1,...,ib, : :]&lt;/code&gt; 是一个 &lt;code&gt;M x N&lt;/code&gt; 矩阵。</target>
        </trans-unit>
        <trans-unit id="b7a64029550dddacbaa84a5c9b1e8fcd91b9538b" translate="yes" xml:space="preserve">
          <source>This optimizer class is &lt;a href=&quot;../../distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt; aware, which means it automatically sums gradients across all replicas. To average gradients, you divide your loss by the global batch size, which is done automatically if you use &lt;a href=&quot;../../keras&quot;&gt;&lt;code&gt;tf.keras&lt;/code&gt;&lt;/a&gt; built-in training or evaluation loops. See the &lt;code&gt;reduction&lt;/code&gt; argument of your loss which should be set to &lt;a href=&quot;../losses/reduction#SUM_OVER_BATCH_SIZE&quot;&gt;&lt;code&gt;tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE&lt;/code&gt;&lt;/a&gt; for averaging or &lt;a href=&quot;../losses/reduction#SUM&quot;&gt;&lt;code&gt;tf.keras.losses.Reduction.SUM&lt;/code&gt;&lt;/a&gt; for not.</source>
          <target state="translated">该优化器类是&lt;a href=&quot;../../distribute/strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; &lt;/a&gt;感知的，这意味着它将自动对所有副本之间的梯度求和。要对梯度进行平均，可以将损失除以全局批次大小，如果使用&lt;a href=&quot;../../keras&quot;&gt; &lt;code&gt;tf.keras&lt;/code&gt; &lt;/a&gt;内置的训练或评估循环，则会自动完成。见 &lt;code&gt;reduction&lt;/code&gt; 您的损失，应设置的参数&lt;a href=&quot;../losses/reduction#SUM_OVER_BATCH_SIZE&quot;&gt; &lt;code&gt;tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE&lt;/code&gt; &lt;/a&gt;的平均或&lt;a href=&quot;../losses/reduction#SUM&quot;&gt; &lt;code&gt;tf.keras.losses.Reduction.SUM&lt;/code&gt; &lt;/a&gt;不。</target>
        </trans-unit>
        <trans-unit id="f9ca3ab79a1177cb06784d426253ffabbbc2a6a4" translate="yes" xml:space="preserve">
          <source>This optimizer takes care of regularization of unseen features in a mini batch by updating them when they are seen with a closed form update rule that is equivalent to having updated them on every mini-batch.</source>
          <target state="translated">这个优化器通过一个闭合形式的更新规则来处理迷你批中未见特征的正则化,当看到这些特征时,更新它们,相当于在每个迷你批上都更新了它们。</target>
        </trans-unit>
        <trans-unit id="9a8b5c268bde39d241894c5b95721103ebdbc425" translate="yes" xml:space="preserve">
          <source>This optimizer wraps another optimizer and applies loss scaling to it via a &lt;code&gt;LossScale&lt;/code&gt;. Loss scaling is applied whenever gradients are computed, either through &lt;code&gt;minimize()&lt;/code&gt; or &lt;code&gt;get_gradients()&lt;/code&gt;. The loss scale is updated via &lt;a href=&quot;../../../mixed_precision/experimental/lossscale#update&quot;&gt;&lt;code&gt;LossScale.update()&lt;/code&gt;&lt;/a&gt; whenever gradients are applied, either through &lt;code&gt;minimize()&lt;/code&gt; or &lt;code&gt;apply_gradients()&lt;/code&gt;. For example:</source>
          <target state="translated">该优化器包装另一个优化器，并通过 &lt;code&gt;LossScale&lt;/code&gt; 对其施加损失缩放。损失应用缩放每当梯度被计算，或者通过 &lt;code&gt;minimize()&lt;/code&gt; 或 &lt;code&gt;get_gradients()&lt;/code&gt; 。损失尺度经由更新&lt;a href=&quot;../../../mixed_precision/experimental/lossscale#update&quot;&gt; &lt;code&gt;LossScale.update()&lt;/code&gt; &lt;/a&gt;每当被施加梯度，无论是通过 &lt;code&gt;minimize()&lt;/code&gt; 或 &lt;code&gt;apply_gradients()&lt;/code&gt; 。例如：</target>
        </trans-unit>
        <trans-unit id="a13f36128710145f1603618c3c0bcd6462911f2d" translate="yes" xml:space="preserve">
          <source>This optimizer wraps another optimizer and applies loss scaling to it via a &lt;code&gt;LossScale&lt;/code&gt;. Loss scaling is applied whenever gradients are computed, such as through &lt;code&gt;minimize()&lt;/code&gt;.</source>
          <target state="translated">该优化器包装另一个优化器，并通过 &lt;code&gt;LossScale&lt;/code&gt; 对其施加损失缩放。每当计算梯度时（例如通过 &lt;code&gt;minimize()&lt;/code&gt; ，都会应用损耗定标。</target>
        </trans-unit>
        <trans-unit id="64e321c394397e96aca6614583438da557ccee0e" translate="yes" xml:space="preserve">
          <source>This option can be used to override the default policy for how to handle external state when serializing a dataset or checkpointing its iterator. There are three settings available - IGNORE: in which we completely ignore any state; WARN: We warn the user that some state might be thrown away; FAIL: We fail if any state is being captured.</source>
          <target state="translated">这个选项可以用来覆盖默认的策略,以便在序列化一个数据集或检查点其迭代器时,如何处理外部状态。有三种设置可供选择--IGNORE:其中我们完全忽略任何状态;WARN:我们警告用户某些状态可能会被丢弃;FAIL:我们警告用户某些状态可能会被丢弃;FAIL:我们警告用户某些状态可能会被丢弃。我们警告用户一些状态可能会被丢弃;FAIL:如果任何状态被捕获,我们会失败。如果有任何状态被捕获,我们就会失败。</target>
        </trans-unit>
        <trans-unit id="fa77eb4223231696da9576378f71e5fd1b4c5fa1" translate="yes" xml:space="preserve">
          <source>This outputs a &lt;code&gt;batch_size&lt;/code&gt; bool array, an entry &lt;code&gt;out[i]&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt; if the prediction for the target class is among the top &lt;code&gt;k&lt;/code&gt; predictions among all predictions for example &lt;code&gt;i&lt;/code&gt;. Note that the behavior of &lt;code&gt;InTopK&lt;/code&gt; differs from the &lt;code&gt;TopK&lt;/code&gt; op in its handling of ties; if multiple classes have the same prediction value and straddle the top-&lt;code&gt;k&lt;/code&gt; boundary, all of those classes are considered to be in the top &lt;code&gt;k&lt;/code&gt;.</source>
          <target state="translated">这将输出 &lt;code&gt;batch_size&lt;/code&gt; bool数组，如果目标类的预测在所有预测（例如 &lt;code&gt;i&lt;/code&gt; ）中的前 &lt;code&gt;k&lt;/code&gt; 个预测之中，则入口 &lt;code&gt;out[i]&lt;/code&gt; 为 &lt;code&gt;true&lt;/code&gt; 。请注意， &lt;code&gt;InTopK&lt;/code&gt; 的行为与 &lt;code&gt;TopK&lt;/code&gt; op的关系处理不同。如果多个类具有相同的预测值并且跨越了前 &lt;code&gt;k&lt;/code&gt; 个边界，则所有这些类均被视为在前 &lt;code&gt;k&lt;/code&gt; 个中。</target>
        </trans-unit>
        <trans-unit id="43b5cb445e044712f2e69a5344f02f5be0ddda93" translate="yes" xml:space="preserve">
          <source>This outputs a &lt;code&gt;batch_size&lt;/code&gt; bool array, an entry &lt;code&gt;out[i]&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt; if the prediction for the target class is finite (not inf, -inf, or nan) and among the top &lt;code&gt;k&lt;/code&gt; predictions among all predictions for example &lt;code&gt;i&lt;/code&gt;. Note that the behavior of &lt;code&gt;InTopK&lt;/code&gt; differs from the &lt;code&gt;TopK&lt;/code&gt; op in its handling of ties; if multiple classes have the same prediction value and straddle the top-&lt;code&gt;k&lt;/code&gt; boundary, all of those classes are considered to be in the top &lt;code&gt;k&lt;/code&gt;.</source>
          <target state="translated">这将输出 &lt;code&gt;batch_size&lt;/code&gt; bool数组，如果目标类的预测是有限的（不是inf，-inf或nan），并且在所有预测（例如 &lt;code&gt;i&lt;/code&gt; ）中前 &lt;code&gt;k&lt;/code&gt; 个预测中，则 &lt;code&gt;out[i]&lt;/code&gt; 为 &lt;code&gt;true&lt;/code&gt; 。请注意， &lt;code&gt;InTopK&lt;/code&gt; 的行为与 &lt;code&gt;TopK&lt;/code&gt; op的关系处理不同。如果多个类别具有相同的预测值并跨越了前 &lt;code&gt;k&lt;/code&gt; 个边界，则所有这些类别均被视为在前 &lt;code&gt;k&lt;/code&gt; 个中。</target>
        </trans-unit>
        <trans-unit id="345380820e023cc35c074a4e6039205226041719" translate="yes" xml:space="preserve">
          <source>This overload raises a &lt;code&gt;TypeError&lt;/code&gt; when the user inadvertently treats a &lt;code&gt;Tensor&lt;/code&gt; as a boolean (most commonly in an &lt;code&gt;if&lt;/code&gt; or &lt;code&gt;while&lt;/code&gt; statement), in code that was not converted by AutoGraph. For example:</source>
          <target state="translated">当用户在未由AutoGraph转换的代码中无意中将 &lt;code&gt;Tensor&lt;/code&gt; 视为布尔值时（最常见于 &lt;code&gt;if&lt;/code&gt; 或 &lt;code&gt;while&lt;/code&gt; 语句中），此重载会引发 &lt;code&gt;TypeError&lt;/code&gt; 。例如：</target>
        </trans-unit>
        <trans-unit id="5f63c68c6b6d06032eeae666e8d05a28d8b6e750" translate="yes" xml:space="preserve">
          <source>This package defines ops for manipulating ragged tensors (&lt;a href=&quot;../../raggedtensor&quot;&gt;&lt;code&gt;tf.RaggedTensor&lt;/code&gt;&lt;/a&gt;), which are tensors with non-uniform shapes. In particular, each &lt;code&gt;RaggedTensor&lt;/code&gt; has one or more &lt;em&gt;ragged dimensions&lt;/em&gt;, which are dimensions whose slices may have different lengths. For example, the inner (column) dimension of &lt;code&gt;rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []]&lt;/code&gt; is ragged, since the column slices (&lt;code&gt;rt[0, :]&lt;/code&gt;, ..., &lt;code&gt;rt[4, :]&lt;/code&gt;) have different lengths. For a more detailed description of ragged tensors, see the &lt;a href=&quot;../../raggedtensor&quot;&gt;&lt;code&gt;tf.RaggedTensor&lt;/code&gt;&lt;/a&gt; class documentation and the &lt;a href=&quot;https://www.tensorflow.org/guide/ragged_tensors&quot;&gt;Ragged Tensor Guide&lt;/a&gt;.</source>
          <target state="translated">该程序包定义用于操作&lt;a href=&quot;../../raggedtensor&quot;&gt; &lt;code&gt;tf.RaggedTensor&lt;/code&gt; &lt;/a&gt;张量（tf.RaggedTensor）的操作，参差不齐的张量具有不均匀的形状。具体而言，每个 &lt;code&gt;RaggedTensor&lt;/code&gt; 具有一个或多个参差不齐的&lt;em&gt;尺寸&lt;/em&gt;，这些尺寸是其切片可能具有不同长度的尺寸。例如，由于列切片（ &lt;code&gt;rt[0, :]&lt;/code&gt; &lt;code&gt;rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []]&lt;/code&gt; 的内部（列）维是参差不齐的[0，：]，...， &lt;code&gt;rt[4, :]&lt;/code&gt; ）具有不同的长度。有关&lt;a href=&quot;https://www.tensorflow.org/guide/ragged_tensors&quot;&gt;参差不齐的张量&lt;/a&gt;的更详细描述，请参见&lt;a href=&quot;../../raggedtensor&quot;&gt; &lt;code&gt;tf.RaggedTensor&lt;/code&gt; &lt;/a&gt;类文档和参差不齐的Tensor Guide。</target>
        </trans-unit>
        <trans-unit id="3e4f0989d73eb8b679bcdf7fff408eece47a531f" translate="yes" xml:space="preserve">
          <source>This package defines ops for manipulating ragged tensors (&lt;a href=&quot;raggedtensor&quot;&gt;&lt;code&gt;tf.RaggedTensor&lt;/code&gt;&lt;/a&gt;), which are tensors with non-uniform shapes. In particular, each &lt;code&gt;RaggedTensor&lt;/code&gt; has one or more &lt;em&gt;ragged dimensions&lt;/em&gt;, which are dimensions whose slices may have different lengths. For example, the inner (column) dimension of &lt;code&gt;rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []]&lt;/code&gt; is ragged, since the column slices (&lt;code&gt;rt[0, :]&lt;/code&gt;, ..., &lt;code&gt;rt[4, :]&lt;/code&gt;) have different lengths. For a more detailed description of ragged tensors, see the &lt;a href=&quot;raggedtensor&quot;&gt;&lt;code&gt;tf.RaggedTensor&lt;/code&gt;&lt;/a&gt; class documentation and the &lt;a href=&quot;https://www.tensorflow.org/guide/ragged_tensors&quot;&gt;Ragged Tensor Guide&lt;/a&gt;.</source>
          <target state="translated">该程序包定义用于操作&lt;a href=&quot;raggedtensor&quot;&gt; &lt;code&gt;tf.RaggedTensor&lt;/code&gt; &lt;/a&gt;张量（tf.RaggedTensor）的操作，参差不齐的张量具有不均匀的形状。具体而言，每个 &lt;code&gt;RaggedTensor&lt;/code&gt; 具有一个或多个参差不齐的&lt;em&gt;尺寸&lt;/em&gt;，这些尺寸是其切片可能具有不同长度的尺寸。例如，由于列切片（ &lt;code&gt;rt[0, :]&lt;/code&gt; &lt;code&gt;rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []]&lt;/code&gt; 的内部（列）维是参差不齐的[0，：]，...， &lt;code&gt;rt[4, :]&lt;/code&gt; ）具有不同的长度。有关&lt;a href=&quot;https://www.tensorflow.org/guide/ragged_tensors&quot;&gt;参差不齐的张量&lt;/a&gt;的更详细描述，请参见&lt;a href=&quot;raggedtensor&quot;&gt; &lt;code&gt;tf.RaggedTensor&lt;/code&gt; &lt;/a&gt;类文档和参差不齐的Tensor Guide。</target>
        </trans-unit>
        <trans-unit id="ca381bc7587c1d86ccea71cf77a3f51f18c01785" translate="yes" xml:space="preserve">
          <source>This partitioner will shard a Variable along one axis, attempting to keep the maximum shard size below &lt;code&gt;max_shard_bytes&lt;/code&gt;. In practice, this is not always possible when sharding along only one axis. When this happens, this axis is sharded as much as possible (i.e., every dimension becomes a separate shard).</source>
          <target state="translated">该分区程序将沿一个轴分片变量，尝试将最大分片大小保持在 &lt;code&gt;max_shard_bytes&lt;/code&gt; 以下。实际上，当仅沿一个轴进行分片时，这并不总是可能的。发生这种情况时，将尽可能多地对该轴进行分片（即，每个尺寸都变成单独的分片）。</target>
        </trans-unit>
        <trans-unit id="fd457ac6c886ad4d6a9b1c2c9ed70e07ea8ad467" translate="yes" xml:space="preserve">
          <source>This produces files called &quot;timeline-</source>
          <target state="translated">这将产生名为 &quot;时间线 &quot;的文件。</target>
        </trans-unit>
        <trans-unit id="9f7b45ed5e5688563844dafc256f1f022fd16e33" translate="yes" xml:space="preserve">
          <source>This property only works when the &lt;code&gt;TPUEmbedding&lt;/code&gt; object is created under a non-TPU strategy. This is intended to be used to for CPU based lookup when creating a serving checkpoint.</source>
          <target state="translated">仅当在非TPU策略下创建 &lt;code&gt;TPUEmbedding&lt;/code&gt; 对象时，此属性才起作用。它旨在用于在创建服务检查点时用于基于CPU的查找。</target>
        </trans-unit>
        <trans-unit id="650852868a3688b5e43945d526af2b7a7abb7659" translate="yes" xml:space="preserve">
          <source>This regressor ignores feature values and will learn to predict the average value of each label.</source>
          <target state="translated">该回归器忽略特征值,将学习预测每个标签的平均值。</target>
        </trans-unit>
        <trans-unit id="6b4e11e2fc93d180cb8cb79e60f96113138df290" translate="yes" xml:space="preserve">
          <source>This returns a ClusterSpec object for use based on information from the specified initialization parameters and Slurm environment variables. The cluster specification is resolved each time this function is called. The resolver extract hostnames of nodes by scontrol and pack tasks in that order until a node a has number of tasks that is equal to specification. GPUs on nodes are allocated to tasks by specification through setting CUDA_VISIBLE_DEVICES environment variable.</source>
          <target state="translated">这将返回一个ClusterSpec对象,用于根据指定的初始化参数和Slurm环境变量的信息使用。每次调用该函数时,集群规范都会被解析。解析器通过scontrol提取节点的主机名,并按顺序打包任务,直到节点a的任务数等于规范。节点上的GPU通过设置CUDA_VISIBLE_DEVICES环境变量按规格分配给任务。</target>
        </trans-unit>
        <trans-unit id="ce42bdd47f55adee51dc4ae146a85efc32785746" translate="yes" xml:space="preserve">
          <source>This returns a ClusterSpec object for use based on information from the specified instance group. We will retrieve the information from the GCE APIs every time this method is called.</source>
          <target state="translated">这将根据指定实例组的信息返回一个ClusterSpec对象供使用。每次调用这个方法时,我们都会从GCE API中检索信息。</target>
        </trans-unit>
        <trans-unit id="d4e36425f3d932370ce3f31b5b37e6bd2c5dbb8b" translate="yes" xml:space="preserve">
          <source>This returns a function outputting &lt;code&gt;features&lt;/code&gt; and &lt;code&gt;targets&lt;/code&gt; based on the dict of numpy arrays. The dict &lt;code&gt;features&lt;/code&gt; has the same keys as the &lt;code&gt;x&lt;/code&gt;. The dict &lt;code&gt;targets&lt;/code&gt; has the same keys as the &lt;code&gt;y&lt;/code&gt; if &lt;code&gt;y&lt;/code&gt; is a dict.</source>
          <target state="translated">这将返回一个函数，该函数根据numpy数组的字典输出 &lt;code&gt;features&lt;/code&gt; 和 &lt;code&gt;targets&lt;/code&gt; 。dict &lt;code&gt;features&lt;/code&gt; 具有与 &lt;code&gt;x&lt;/code&gt; 相同的键。如果 &lt;code&gt;y&lt;/code&gt; 是字典，则dict &lt;code&gt;targets&lt;/code&gt; 具有与 &lt;code&gt;y&lt;/code&gt; 相同的键。</target>
        </trans-unit>
        <trans-unit id="091f487524e5d0cdc7f2f7122f0e27fecc0512d2" translate="yes" xml:space="preserve">
          <source>This returns the job name and task index for the process which calls this function according to its rank and cluster specification. The job name and task index are set after a cluster is constructed by cluster_spec otherwise defaults to None.</source>
          <target state="translated">该函数根据调用该函数的进程的等级和集群规格返回该进程的工作名称和任务索引。工作名称和任务索引是在 cluster_spec 构建集群后设置的,否则默认为 None。</target>
        </trans-unit>
        <trans-unit id="9e86e167f9dfaedec2287e6304deddcf9a677504" translate="yes" xml:space="preserve">
          <source>This returns the number of accelerator cores (such as GPUs and TPUs) available per worker.</source>
          <target state="translated">这将返回每个worker可用的加速器核数(如GPU和TPU)。</target>
        </trans-unit>
        <trans-unit id="43f97f599a6e5c9f5b6d870e441e77232b9c4ee2" translate="yes" xml:space="preserve">
          <source>This sampler is useful when the target classes approximately follow such a distribution - for example, if the classes represent words in a lexicon sorted in decreasing order of frequency. If your classes are not ordered by decreasing frequency, do not use this op.</source>
          <target state="translated">当目标类近似于这样的分布时,这个采样器是很有用的--例如,如果类代表词库中按频率递减排序的词。如果您的类不是按频率递减排序的,就不要使用这个操作。</target>
        </trans-unit>
        <trans-unit id="f4116f5a13b723775fce909b3d142c6ace24a3f4" translate="yes" xml:space="preserve">
          <source>This set may grow over time, so it's important the signature of creators is as mentioned above.</source>
          <target state="translated">这个套路可能会随着时间的推移而成长,所以创作者的签名就像上面提到的那样,很重要。</target>
        </trans-unit>
        <trans-unit id="642d4b5b813040e42f63e66f6c2fcb5de77a52c5" translate="yes" xml:space="preserve">
          <source>This should only be used when the if then/else body functions do not have stateful ops.</source>
          <target state="translated">只有当if then/else体函数没有状态操作时,才应该使用这个功能。</target>
        </trans-unit>
        <trans-unit id="5fc235c555c96c210745b72880b680aa6b17dbdb" translate="yes" xml:space="preserve">
          <source>This should only be used when the while condition and body functions do not have stateful ops.</source>
          <target state="translated">只有当while条件和体函数没有状态操作时,才应该使用这个功能。</target>
        </trans-unit>
        <trans-unit id="6fe6f784df295b3702cbf845894c09377130127e" translate="yes" xml:space="preserve">
          <source>This simply wraps &lt;code&gt;compute_gradients()&lt;/code&gt; from the real optimizer. The gradients will be aggregated in &lt;code&gt;apply_gradients()&lt;/code&gt; so that user can modify the gradients like clipping with per replica global norm if needed. The global norm with aggregated gradients can be bad as one replica's huge gradients can hurt the gradients from other replicas.</source>
          <target state="translated">这只是包装来自实际优化器的 &lt;code&gt;compute_gradients()&lt;/code&gt; 。渐变将在 &lt;code&gt;apply_gradients()&lt;/code&gt; 中聚合，以便用户可以根据需要使用每个副本全局规范修改渐变，例如剪切。具有汇总渐变的全局规范可能很糟糕，因为一个副本的巨大渐变会损害其他副本的渐变。</target>
        </trans-unit>
        <trans-unit id="8231e710a02b190ee62057d8ce9e6843f584733f" translate="yes" xml:space="preserve">
          <source>This simply wraps the compute_gradients() from the real optimizer. The gradients will be aggregated in the apply_gradients() so that user can modify the gradients like clipping with per replica global norm if needed. The global norm with aggregated gradients can be bad as one replica's huge gradients can hurt the gradients from other replicas.</source>
          <target state="translated">这只是简单地封装了真正优化器中的compute_gradients()。梯度将在apply_gradients()中被聚合,这样用户就可以根据需要修改梯度,比如用每个副本的全局规范进行剪切。聚合梯度的全局规范可能是不好的,因为一个副本的巨大梯度可能会影响其他副本的梯度。</target>
        </trans-unit>
        <trans-unit id="9535c2c156931877173be6ce0ef4fcc7c39ade4d" translate="yes" xml:space="preserve">
          <source>This simply wraps the get_slot() from the actual optimizer.</source>
          <target state="translated">这只是简单地包装了实际优化器中的get_slot()。</target>
        </trans-unit>
        <trans-unit id="627b6a914c45360f47c7270bf7239e5630834ac1" translate="yes" xml:space="preserve">
          <source>This simply wraps the get_slot_names() from the actual optimizer.</source>
          <target state="translated">这只是简单地封装了实际优化器中的get_slot_names()。</target>
        </trans-unit>
        <trans-unit id="481aba790f7180557002a29e34e41b2834d82d24" translate="yes" xml:space="preserve">
          <source>This starts services in the background. The services started depend on the parameters to the constructor and may include:</source>
          <target state="translated">这将在后台启动服务。启动的服务取决于构造函数的参数,可能包括:</target>
        </trans-unit>
        <trans-unit id="61ba97b856d34fcc84e3cd4a4201db78d7018540" translate="yes" xml:space="preserve">
          <source>This strategy implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to &lt;a href=&quot;../../../../distribute/mirroredstrategy&quot;&gt;&lt;code&gt;tf.distribute.MirroredStrategy&lt;/code&gt;&lt;/a&gt;, it creates copies of all variables in the model on each device across all workers.</source>
          <target state="translated">该策略可在多个可能具有多个GPU的工作人员之间实施同步的分布式培训。与&lt;a href=&quot;../../../../distribute/mirroredstrategy&quot;&gt; &lt;code&gt;tf.distribute.MirroredStrategy&lt;/code&gt; &lt;/a&gt;相似，它在所有工作器上的每个设备上的模型中创建所有变量的副本。</target>
        </trans-unit>
        <trans-unit id="f19046d3b0cb6c64ecb2b4f8764cc9d33c4854f1" translate="yes" xml:space="preserve">
          <source>This strategy implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to &lt;a href=&quot;../mirroredstrategy&quot;&gt;&lt;code&gt;tf.distribute.MirroredStrategy&lt;/code&gt;&lt;/a&gt;, it creates copies of all variables in the model on each device across all workers.</source>
          <target state="translated">该策略可在多个可能具有多个GPU的工作人员之间实施同步的分布式培训。与&lt;a href=&quot;../mirroredstrategy&quot;&gt; &lt;code&gt;tf.distribute.MirroredStrategy&lt;/code&gt; &lt;/a&gt;相似，它在所有工作器上的每个设备上的模型中创建所有变量的副本。</target>
        </trans-unit>
        <trans-unit id="04f8a8d0444c29ec6e24f3fc468a55853c986597" translate="yes" xml:space="preserve">
          <source>This strategy is typically used for training on one machine with multiple GPUs. For TPUs, use &lt;a href=&quot;../../../distribute/tpustrategy&quot;&gt;&lt;code&gt;tf.distribute.TPUStrategy&lt;/code&gt;&lt;/a&gt;. To use &lt;code&gt;MirroredStrategy&lt;/code&gt; with multiple workers, please refer to &lt;a href=&quot;../../../distribute/experimental/multiworkermirroredstrategy&quot;&gt;&lt;code&gt;tf.distribute.experimental.MultiWorkerMirroredStrategy&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">该策略通常用于在一台具有多个GPU的机器上的训练。对于TPU，请使用&lt;a href=&quot;../../../distribute/tpustrategy&quot;&gt; &lt;code&gt;tf.distribute.TPUStrategy&lt;/code&gt; &lt;/a&gt;。要将 &lt;code&gt;MirroredStrategy&lt;/code&gt; 与多个工作人员一起使用，请参阅&lt;a href=&quot;../../../distribute/experimental/multiworkermirroredstrategy&quot;&gt; &lt;code&gt;tf.distribute.experimental.MultiWorkerMirroredStrategy&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="7c008960642435f7f796dcc4d6d07c58ad8a9370" translate="yes" xml:space="preserve">
          <source>This strategy is typically used for training on one machine with multiple GPUs. For TPUs, use &lt;a href=&quot;tpustrategy&quot;&gt;&lt;code&gt;tf.distribute.TPUStrategy&lt;/code&gt;&lt;/a&gt;. To use &lt;code&gt;MirroredStrategy&lt;/code&gt; with multiple workers, please refer to &lt;a href=&quot;experimental/multiworkermirroredstrategy&quot;&gt;&lt;code&gt;tf.distribute.experimental.MultiWorkerMirroredStrategy&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">该策略通常用于在一台具有多个GPU的机器上的训练。对于TPU，请使用&lt;a href=&quot;tpustrategy&quot;&gt; &lt;code&gt;tf.distribute.TPUStrategy&lt;/code&gt; &lt;/a&gt;。要将 &lt;code&gt;MirroredStrategy&lt;/code&gt; 与多个工作人员一起使用，请参阅&lt;a href=&quot;experimental/multiworkermirroredstrategy&quot;&gt; &lt;code&gt;tf.distribute.experimental.MultiWorkerMirroredStrategy&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="648f5d6b8ceebd17f689c7c0ab8ef16660a7747c" translate="yes" xml:space="preserve">
          <source>This strategy requires two jobs: workers and parameter servers. Variables and updates to those variables will be assigned to parameter servers and other operations are assigned to workers.</source>
          <target state="translated">这个策略需要两个工作:工人和参数服务器。变量和对这些变量的更新将被分配给参数服务器,其他操作则被分配给工人。</target>
        </trans-unit>
        <trans-unit id="e44a68c24a5012e2f287321dc3d6da6b35bca174" translate="yes" xml:space="preserve">
          <source>This strategy requires two roles: workers and parameter servers. Variables and updates to those variables will be assigned to parameter servers and other operations are assigned to workers.</source>
          <target state="translated">这种策略需要两个角色:工人和参数服务器。变量和对这些变量的更新将被分配给参数服务器,其他操作则被分配给工人。</target>
        </trans-unit>
        <trans-unit id="609b539c1ba8e5fb73c25fa137b65336a8f05b90" translate="yes" xml:space="preserve">
          <source>This strategy uses one replica per device and sync replication for its multi-GPU version.</source>
          <target state="translated">这种策略每台设备使用一个副本,并对其多GPU版本进行同步复制。</target>
        </trans-unit>
        <trans-unit id="a028b1379398bc12b06afc7b36060473f028c25d" translate="yes" xml:space="preserve">
          <source>This symbol is also exported to v2 in tf.estimator namespace. See &lt;a href=&quot;https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/hooks/basic_session_run_hooks.py&quot;&gt;https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/hooks/basic_session_run_hooks.py&lt;/a&gt;</source>
          <target state="translated">此符号也已导出到tf.estimator命名空间中的v2。参见&lt;a href=&quot;https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/hooks/basic_session_run_hooks.py&quot;&gt;https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/hooks/basic_session_run_hooks.py&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f77a1c7f27ca177eb5967fdc02f021a08479ef96" translate="yes" xml:space="preserve">
          <source>This symbol is also exported to v2 in tf.estimator namespace. See https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/hooks/basic_session_run_hooks.py</source>
          <target state="translated">这个符号也被导出到tf.估计器命名空间的v2中。参见https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/hooks/basic_session_run_hooks.py。</target>
        </trans-unit>
        <trans-unit id="ee30a785484036470f77ee9c09ff132832518ed4" translate="yes" xml:space="preserve">
          <source>This takes an ordinary &lt;code&gt;dataset&lt;/code&gt; and &lt;code&gt;replica_fn&lt;/code&gt; and runs it distributed using a particular &lt;a href=&quot;strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt; named &lt;code&gt;my_strategy&lt;/code&gt; above. Any variables created in &lt;code&gt;replica_fn&lt;/code&gt; are created using &lt;code&gt;my_strategy&lt;/code&gt;'s policy, and library functions called by &lt;code&gt;replica_fn&lt;/code&gt; can use the &lt;code&gt;get_replica_context()&lt;/code&gt; API to implement distributed-specific behavior.</source>
          <target state="translated">这需要一个普通的 &lt;code&gt;dataset&lt;/code&gt; 和 &lt;code&gt;replica_fn&lt;/code&gt; 并使用上面名为 &lt;code&gt;my_strategy&lt;/code&gt; 的特定&lt;a href=&quot;strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; &lt;/a&gt;对其进行分布式运行。在创建的任何变量 &lt;code&gt;replica_fn&lt;/code&gt; 使用创建 &lt;code&gt;my_strategy&lt;/code&gt; 的政策，和库函数的调用方式 &lt;code&gt;replica_fn&lt;/code&gt; 可以使用 &lt;code&gt;get_replica_context()&lt;/code&gt; API来实现分布式的特定行为。</target>
        </trans-unit>
        <trans-unit id="f43e50d6d041bdda53ec0ccc184fa3940b822601" translate="yes" xml:space="preserve">
          <source>This takes in a few parameters and creates a GCEClusterResolver project. It will then use these parameters to query the GCE API for the IP addresses of each instance in the instance group.</source>
          <target state="translated">这需要一些参数,并创建一个GCEClusterResolver项目,然后它将使用这些参数来查询GCE API中每个实例组的IP地址。然后它将使用这些参数来查询GCE API中实例组中每个实例的IP地址。</target>
        </trans-unit>
        <trans-unit id="dcbb15c135f93b202dd4afc6d928c452a899b686" translate="yes" xml:space="preserve">
          <source>This takes in parameters and creates a SlurmClusterResolver object. It uses those parameters to check which nodes will processes reside on and resolves their hostnames. With the number of the GPUs on each node and number of GPUs for each task it offsets the port number for each process and allocates GPUs to tasks by setting environment variables. The resolver currently supports homogeneous tasks and default Slurm process allocation.</source>
          <target state="translated">它接收参数并创建一个 SlurmClusterResolver 对象。它使用这些参数来检查进程将驻留在哪些节点上,并解析它们的主机名。有了每个节点上的GPU数量和每个任务的GPU数量,它为每个进程偏移了端口号,并通过设置环境变量为任务分配GPU。目前该解析器支持同质任务和默认的Slurm进程分配。</target>
        </trans-unit>
        <trans-unit id="c32e52f6772f69af41778414a120795e360f2147" translate="yes" xml:space="preserve">
          <source>This thread class is intended to be used with a &lt;code&gt;Coordinator&lt;/code&gt;. It repeatedly runs code specified either as &lt;code&gt;target&lt;/code&gt; and &lt;code&gt;args&lt;/code&gt; or by the &lt;code&gt;run_loop()&lt;/code&gt; method.</source>
          <target state="translated">该线程类旨在与 &lt;code&gt;Coordinator&lt;/code&gt; 一起使用。它反复运行指定为 &lt;code&gt;target&lt;/code&gt; 和 &lt;code&gt;args&lt;/code&gt; 或 &lt;code&gt;run_loop()&lt;/code&gt; 方法指定的代码。</target>
        </trans-unit>
        <trans-unit id="cbe44aae232eb7beccd1bb717d150f53dfed09d2" translate="yes" xml:space="preserve">
          <source>This tracking then allows saving variable values to &lt;a href=&quot;https://www.tensorflow.org/guide/checkpoint&quot;&gt;training checkpoints&lt;/a&gt;, or to &lt;a href=&quot;https://www.tensorflow.org/guide/saved_model&quot;&gt;SavedModels&lt;/a&gt; which include serialized TensorFlow graphs.</source>
          <target state="translated">然后，此跟踪允许将变量值保存到&lt;a href=&quot;https://www.tensorflow.org/guide/checkpoint&quot;&gt;训练检查点&lt;/a&gt;或保存到包含序列化TensorFlow图的&lt;a href=&quot;https://www.tensorflow.org/guide/saved_model&quot;&gt;SavedModels&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="84d3d178d35882538e5cb6113338132ed1567a47" translate="yes" xml:space="preserve">
          <source>This transformation applies &lt;code&gt;map_func&lt;/code&gt; to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. &lt;code&gt;map_func&lt;/code&gt; can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components.</source>
          <target state="translated">此转换将 &lt;code&gt;map_func&lt;/code&gt; 应用于此数据集的每个元素，并返回一个包含已转换元素的新数据集，其顺序与它们在输入中出现的顺序相同。 &lt;code&gt;map_func&lt;/code&gt; 可用于更改数据集元素的值和结构。例如，向每个元素加1，或投影元素组件的子集。</target>
        </trans-unit>
        <trans-unit id="ba79b19e287af6ccbf373678af86969066d66587" translate="yes" xml:space="preserve">
          <source>This transformation checks whether the camel-case names (i.e. &quot;FlatMap&quot;, not &quot;flat_map&quot;) of the transformations following this transformation match the list of names in the &lt;code&gt;transformations&lt;/code&gt; argument. If there is a mismatch, the transformation raises an exception.</source>
          <target state="translated">此转换检查此转换之后的转换的驼峰式名称（即&amp;ldquo; FlatMap&amp;rdquo;而不是&amp;ldquo; flat_map&amp;rdquo;）是否与 &lt;code&gt;transformations&lt;/code&gt; 参数中的名称列表匹配。如果存在不匹配，则转换会引发异常。</target>
        </trans-unit>
        <trans-unit id="b91ea223c9ec8adff0faaecc5c9b88a252c4af69" translate="yes" xml:space="preserve">
          <source>This transformation combines multiple consecutive elements of the input dataset into a single element.</source>
          <target state="translated">这种转换将输入数据集的多个连续元素合并为一个元素。</target>
        </trans-unit>
        <trans-unit id="35f6c3c6efa792b9215faf94c8c00f35d19886a0" translate="yes" xml:space="preserve">
          <source>This transformation is a stateful relative of &lt;a href=&quot;../dataset#map&quot;&gt;&lt;code&gt;tf.data.Dataset.map&lt;/code&gt;&lt;/a&gt;. In addition to mapping &lt;code&gt;scan_func&lt;/code&gt; across the elements of the input dataset, &lt;code&gt;scan()&lt;/code&gt; accumulates one or more state tensors, whose initial values are &lt;code&gt;initial_state&lt;/code&gt;.</source>
          <target state="translated">此转换是&lt;a href=&quot;../dataset#map&quot;&gt; &lt;code&gt;tf.data.Dataset.map&lt;/code&gt; &lt;/a&gt;的有状态相对。除了跨输入数据集的元素映射 &lt;code&gt;scan_func&lt;/code&gt; 之外， &lt;code&gt;scan()&lt;/code&gt; 还会累积一个或多个状态张量，其初始值为 &lt;code&gt;initial_state&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="8e7258027c5f693626ed2b2e68a04576c8a8c285" translate="yes" xml:space="preserve">
          <source>This transformation maps each consecutive element in a dataset to a key using &lt;code&gt;key_func&lt;/code&gt; and groups the elements by key. It then applies &lt;code&gt;reduce_func&lt;/code&gt; to at most &lt;code&gt;window_size_func(key)&lt;/code&gt; elements matching the same key. All except the final window for each key will contain &lt;code&gt;window_size_func(key)&lt;/code&gt; elements; the final window may be smaller.</source>
          <target state="translated">此转换使用 &lt;code&gt;key_func&lt;/code&gt; 将数据集中的每个连续元素映射到一个键，并按键对元素进行分组。然后， &lt;code&gt;reduce_func&lt;/code&gt; 最多应用于与同一键匹配的 &lt;code&gt;window_size_func(key)&lt;/code&gt; 元素。除每个键的最终窗口外，所有窗口都将包含 &lt;code&gt;window_size_func(key)&lt;/code&gt; 元素；最终窗口可能会更小。</target>
        </trans-unit>
        <trans-unit id="a21aaa6c0a7b4d11f0bde650d661805100a01ad7" translate="yes" xml:space="preserve">
          <source>This transformation maps element of a dataset to a key using &lt;code&gt;key_func&lt;/code&gt; and groups the elements by key. The &lt;code&gt;reducer&lt;/code&gt; is used to process each group; its &lt;code&gt;init_func&lt;/code&gt; is used to initialize state for each group when it is created, the &lt;code&gt;reduce_func&lt;/code&gt; is used to update the state every time an element is mapped to the matching group, and the &lt;code&gt;finalize_func&lt;/code&gt; is used to map the final state to an output value.</source>
          <target state="translated">此转换使用 &lt;code&gt;key_func&lt;/code&gt; 将数据集的元素映射到键，并按键对元素进行分组。的 &lt;code&gt;reducer&lt;/code&gt; 用于处理每个组; 它的 &lt;code&gt;init_func&lt;/code&gt; 用于在创建每个组时初始化状态， &lt;code&gt;reduce_func&lt;/code&gt; 用于在每次将元素映射到匹配组时更新状态，而 &lt;code&gt;finalize_func&lt;/code&gt; 用于将最终状态映射到输出值。</target>
        </trans-unit>
        <trans-unit id="24ab6194aaa04fc157672b906f20ba2e1b1ec8cc" translate="yes" xml:space="preserve">
          <source>This updates the checkpoint file containing a CheckpointState proto.</source>
          <target state="translated">这将更新包含CheckpointState proto的检查点文件。</target>
        </trans-unit>
        <trans-unit id="5bb7cb617dbb67bdd4c7f55f6d436fa2f1b33e3c" translate="yes" xml:space="preserve">
          <source>This uses &lt;a href=&quot;../norm&quot;&gt;&lt;code&gt;tf.linalg.norm&lt;/code&gt;&lt;/a&gt; to compute the norm along &lt;code&gt;axis&lt;/code&gt;.</source>
          <target state="translated">这使用&lt;a href=&quot;../norm&quot;&gt; &lt;code&gt;tf.linalg.norm&lt;/code&gt; &lt;/a&gt;来计算 &lt;code&gt;axis&lt;/code&gt; 的范数。</target>
        </trans-unit>
        <trans-unit id="7945ebd1b4ae121cdd796a53759129e7e59a0846" translate="yes" xml:space="preserve">
          <source>This usually returns the master from the first ClusterResolver passed in, but you can override this by specifying the task_type and task_id.</source>
          <target state="translated">这通常会从传入的第一个ClusterResolver中返回主站,但你可以通过指定task_type和task_id来覆盖它。</target>
        </trans-unit>
        <trans-unit id="4285af5e3616d9c051e0b5382ab0649693cc39b6" translate="yes" xml:space="preserve">
          <source>This utility function provides consistent behavior for both local (non-distributed) and distributed configurations. The default distribution configuration is parameter server-based between-graph replication. For other types of distribution configurations such as all-reduce training, please use &lt;a href=&quot;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute&quot;&gt;DistributionStrategies&lt;/a&gt;.</source>
          <target state="translated">该实用程序功能为本地（非分布式）和分布式配置提供一致的行为。默认的分发配置是基于参数服务器的图形间复制。对于其他类型的分发配置，例如全缩减培训，请使用&lt;a href=&quot;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute&quot;&gt;DistributionStrategies&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="50e5c41b60a6dacb228de3d2a27ac083ca26570e" translate="yes" xml:space="preserve">
          <source>This utility function trains, evaluates, and (optionally) exports the model by using the given &lt;code&gt;estimator&lt;/code&gt;. All training related specification is held in &lt;code&gt;train_spec&lt;/code&gt;, including training &lt;code&gt;input_fn&lt;/code&gt; and training max steps, etc. All evaluation and export related specification is held in &lt;code&gt;eval_spec&lt;/code&gt;, including evaluation &lt;code&gt;input_fn&lt;/code&gt;, steps, etc.</source>
          <target state="translated">该实用程序功能使用给定的 &lt;code&gt;estimator&lt;/code&gt; 来训练，评估和（可选）导出模型。所有与培训相关的规范都保存在 &lt;code&gt;train_spec&lt;/code&gt; 中，包括培训 &lt;code&gt;input_fn&lt;/code&gt; 和最大培训步数等。所有与评估和导出相关的规范都包含在 &lt;code&gt;eval_spec&lt;/code&gt; 中，包括评估 &lt;code&gt;input_fn&lt;/code&gt; 和步骤等。</target>
        </trans-unit>
        <trans-unit id="370bf5deaf68ca405b51cda0b005ed741251919f" translate="yes" xml:space="preserve">
          <source>This utility method replaces the deprecated-in-V2 &lt;code&gt;tf.compat.v1.Dataset.output_classes&lt;/code&gt; property.</source>
          <target state="translated">此实用程序方法替换了V2中不推荐使用的 &lt;code&gt;tf.compat.v1.Dataset.output_classes&lt;/code&gt; 属性。</target>
        </trans-unit>
        <trans-unit id="46213afe2d7d2a75cd43b97252d79901207a3be4" translate="yes" xml:space="preserve">
          <source>This utility method replaces the deprecated-in-V2 &lt;code&gt;tf.compat.v1.Dataset.output_shapes&lt;/code&gt; property.</source>
          <target state="translated">此实用程序方法替换了不推荐使用的V2 &lt;code&gt;tf.compat.v1.Dataset.output_shapes&lt;/code&gt; 属性。</target>
        </trans-unit>
        <trans-unit id="917e4af53cd9619ae0480dec3aee8643df47493a" translate="yes" xml:space="preserve">
          <source>This utility method replaces the deprecated-in-V2 &lt;code&gt;tf.compat.v1.Dataset.output_types&lt;/code&gt; property.</source>
          <target state="translated">此实用程序方法替换了不推荐使用的V2 &lt;code&gt;tf.compat.v1.Dataset.output_types&lt;/code&gt; 属性。</target>
        </trans-unit>
        <trans-unit id="e1997fea43a5c765a33c5bb8c3d18f8bd998811c" translate="yes" xml:space="preserve">
          <source>This value is ultimately returned as &lt;code&gt;auc&lt;/code&gt;, an idempotent operation that computes the area under a discretized curve of precision versus recall values (computed using the aforementioned variables). The &lt;code&gt;num_thresholds&lt;/code&gt; variable controls the degree of discretization with larger numbers of thresholds more closely approximating the true AUC. The quality of the approximation may vary dramatically depending on &lt;code&gt;num_thresholds&lt;/code&gt;.</source>
          <target state="translated">该值最终以 &lt;code&gt;auc&lt;/code&gt; 形式返回，这是一个幂等运算，用于计算精度与召回值（使用上述变量计算）的离散曲线下的面积。该 &lt;code&gt;num_thresholds&lt;/code&gt; 变量控制离散化具有较大的阈值更紧密地逼近真实AUC的数量程度。近似的质量可能会因 &lt;code&gt;num_thresholds&lt;/code&gt; 而有很大差异。</target>
        </trans-unit>
        <trans-unit id="65677d697851bf6819d55d4ff4251a12696e0771" translate="yes" xml:space="preserve">
          <source>This value is ultimately returned as &lt;code&gt;auc&lt;/code&gt;, an idempotent operation that computes the area under a discretized curve of precision versus recall values (computed using the aforementioned variables). The &lt;code&gt;num_thresholds&lt;/code&gt; variable controls the degree of discretization with larger numbers of thresholds more closely approximating the true AUC. The quality of the approximation may vary dramatically depending on &lt;code&gt;num_thresholds&lt;/code&gt;. The &lt;code&gt;thresholds&lt;/code&gt; parameter can be used to manually specify thresholds which split the predictions more evenly.</source>
          <target state="translated">该值最终以 &lt;code&gt;auc&lt;/code&gt; 形式返回，这是一个幂等运算，用于计算精度与召回值（使用上述变量计算）的离散曲线下的面积。该 &lt;code&gt;num_thresholds&lt;/code&gt; 变量控制离散化具有较大的阈值更紧密地逼近真实AUC的数量程度。近似的质量可能会因 &lt;code&gt;num_thresholds&lt;/code&gt; 而有很大差异。 &lt;code&gt;thresholds&lt;/code&gt; 参数可用于手动指定阈值，这些阈值可以更均匀地划分预测。</target>
        </trans-unit>
        <trans-unit id="ca17fcf3f560f3e845deeb7ae0f38bd32e476e10" translate="yes" xml:space="preserve">
          <source>This version enqueues a different list of tensors in different threads. It adds the following to the current &lt;code&gt;Graph&lt;/code&gt;:</source>
          <target state="translated">此版本在不同线程中排队不同的张量列表。它将以下内容添加到当前 &lt;code&gt;Graph&lt;/code&gt; 中：</target>
        </trans-unit>
        <trans-unit id="3dd9104e524c4af8048275ef1cca44df52153ae4" translate="yes" xml:space="preserve">
          <source>This version has support for both online L2 (McMahan et al., 2013) and shrinkage-type L2, which is the addition of an L2 penalty to the loss function.</source>
          <target state="translated">这个版本既支持在线L2(McMahan等人,2013),也支持收缩型L2,即在损失函数中加入L2惩罚。</target>
        </trans-unit>
        <trans-unit id="56d946ffec0b3da9d0ec8b7171e5c9d7eac7532e" translate="yes" xml:space="preserve">
          <source>This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead.</source>
          <target state="translated">这个版本执行与Dropout相同的功能,但是它丢弃的是整个一维特征图而不是单个元素。如果特征图内的相邻帧具有很强的相关性(通常是早期卷积层的情况),那么常规的dropout将不会对激活进行规整,否则只会导致有效的学习率下降。在这种情况下,SpatialDropout1D将有助于促进特征图之间的独立性,应改用。</target>
        </trans-unit>
        <trans-unit id="a36f7273ef31d9cbeb00b22a3f221c68cd1e40a0" translate="yes" xml:space="preserve">
          <source>This version performs the same function as Dropout, however it drops entire 2D feature maps instead of individual elements. If adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout2D will help promote independence between feature maps and should be used instead.</source>
          <target state="translated">这个版本执行与Dropout相同的功能,但是它丢弃的是整个2D特征图而不是单个元素。如果特征图内的相邻像素具有很强的相关性(通常在早期卷积层中是这样的),那么常规的dropout将不会使激活规律化,否则只会导致有效学习率下降。在这种情况下,SpatialDropout2D将有助于促进特征图之间的独立性,应该改用。</target>
        </trans-unit>
        <trans-unit id="d0ef991814863883d725a0aaf896bf4a46fb3d16" translate="yes" xml:space="preserve">
          <source>This version performs the same function as Dropout, however it drops entire 3D feature maps instead of individual elements. If adjacent voxels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout3D will help promote independence between feature maps and should be used instead.</source>
          <target state="translated">这个版本执行与Dropout相同的功能,但是它丢弃的是整个3D特征图而不是单个元素。如果特征图内相邻的体素有很强的相关性(通常在早期卷积层中是这样),那么常规的 dropout 将不会使激活规律化,否则只会导致有效学习率下降。在这种情况下,SpatialDropout3D将有助于促进特征图之间的独立性,应该使用它来代替。</target>
        </trans-unit>
        <trans-unit id="3b754cc618ce29bdbcce941232103d369933ab97" translate="yes" xml:space="preserve">
          <source>This version performs the same function as Dropout, however, it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead.</source>
          <target state="translated">这个版本执行与Dropout相同的功能,但是,它丢弃的是整个一维特征图而不是单个元素。如果特征图内的相邻帧具有很强的相关性(通常在早期卷积层中是这样),那么常规的 dropout 将不会使激活规律化,否则只会导致有效学习率下降。在这种情况下,SpatialDropout1D将有助于促进特征图之间的独立性,应改用。</target>
        </trans-unit>
        <trans-unit id="669d1fce226733b310b6c063bbb1478bd8960685" translate="yes" xml:space="preserve">
          <source>This version performs the same function as Dropout, however, it drops entire 2D feature maps instead of individual elements. If adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout2D will help promote independence between feature maps and should be used instead.</source>
          <target state="translated">这个版本执行与Dropout相同的功能,但是,它丢弃的是整个2D特征图而不是单个元素。如果特征图内的相邻像素具有很强的相关性(通常在早期卷积层中是这样的),那么常规的dropout将不会使激活规律化,否则只会导致有效学习率下降。在这种情况下,SpatialDropout2D将有助于促进特征图之间的独立性,应该改用。</target>
        </trans-unit>
        <trans-unit id="ef292e83a5c48c6c48f427ef4821dec22b8cd18d" translate="yes" xml:space="preserve">
          <source>This version performs the same function as Dropout, however, it drops entire 3D feature maps instead of individual elements. If adjacent voxels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout3D will help promote independence between feature maps and should be used instead.</source>
          <target state="translated">这个版本执行与Dropout相同的功能,但是,它丢弃的是整个3D特征图而不是单个元素。如果特征图内相邻的体素有很强的相关性(通常在早期卷积层中是这样),那么常规的 dropout 将不会使激活规律化,否则只会导致有效学习率下降。在这种情况下,SpatialDropout3D将有助于促进特征图之间的独立性,应该使用它来代替。</target>
        </trans-unit>
        <trans-unit id="28fed8917e83f005a3a2493f518294b2e184da80" translate="yes" xml:space="preserve">
          <source>This was originally generated by parsing and preprocessing the classic Reuters-21578 dataset, but the preprocessing code is no longer packaged with Keras. See this &lt;a href=&quot;https://github.com/keras-team/keras/issues/12072&quot;&gt;github discussion&lt;/a&gt; for more info.</source>
          <target state="translated">它最初是通过解析和预处理经典的Reuters-21578数据集生成的，但是预处理代码不再与Keras打包在一起。有关更多信息，请参&lt;a href=&quot;https://github.com/keras-team/keras/issues/12072&quot;&gt;见此github讨论&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4d54f43064a89a76630348aa955f910392f5b82d" translate="yes" xml:space="preserve">
          <source>This will clear all caches, even those that are maintained through sequential calls to tf.tpu.experimental.initialize_tpu_system, such as the compilation cache.</source>
          <target state="translated">这将清除所有缓存,即使是那些通过连续调用 tf.tpu.experimental.initialize_tpu_system 来维护的缓存,比如编译缓存。</target>
        </trans-unit>
        <trans-unit id="d806ab1c15562f7cef5ecfb73dfc54f37420bf0f" translate="yes" xml:space="preserve">
          <source>This will match and replace multiple sig defs iff tags is None (i.e when multiple &lt;code&gt;MetaGraph&lt;/code&gt;s have a signature_def with the same key). If tags is not None, this will only replace a single signature_def in the &lt;code&gt;MetaGraph&lt;/code&gt; with matching tags.</source>
          <target state="translated">这将匹配并替换多个sig def，如果标记为None（即，当多个 &lt;code&gt;MetaGraph&lt;/code&gt; 的signature_def具有相同的键时）。如果标签不是None，这只会用匹配的标签替换 &lt;code&gt;MetaGraph&lt;/code&gt; 中的单个signature_def 。</target>
        </trans-unit>
        <trans-unit id="8c497916c950017d377f8b22ce65b7642e7fe656" translate="yes" xml:space="preserve">
          <source>This works for both single worker and multi-worker mode, only MirroredStrategy and MultiWorkerMirroredStrategy are supported for now.</source>
          <target state="translated">这适用于单工和多工模式,目前只支持MirroredStrategy和MultiWorkerMirroredStrategy。</target>
        </trans-unit>
        <trans-unit id="d1d5a90de0626fc107829e4415b87374a7e5143d" translate="yes" xml:space="preserve">
          <source>This wrapper allows to apply a layer to every temporal slice of an input.</source>
          <target state="translated">这个包装器允许将一个层应用到输入的每一个时间片上。</target>
        </trans-unit>
        <trans-unit id="e4911cf935e00a37c587b392c63c4d1259cd5359" translate="yes" xml:space="preserve">
          <source>This wraps &lt;code&gt;func_&lt;/code&gt; in a Template and partially evaluates it. Templates are functions that create variables the first time they are called and reuse them thereafter. In order for &lt;code&gt;func_&lt;/code&gt; to be compatible with a &lt;code&gt;Template&lt;/code&gt; it must have the following properties:</source>
          <target state="translated">这 &lt;code&gt;func_&lt;/code&gt; 包装在模板中并对其进行部分评估。模板是在第一次调用变量时创建变量，然后重新使用它们的函数。为了使 &lt;code&gt;func_&lt;/code&gt; 与 &lt;code&gt;Template&lt;/code&gt; 兼容，它必须具有以下属性：</target>
        </trans-unit>
        <trans-unit id="9934a34210519520663a7f89fe0d027dd3ebea0c" translate="yes" xml:space="preserve">
          <source>This wraps &lt;code&gt;variables()&lt;/code&gt; from the actual optimizer. It does not include the &lt;code&gt;SyncReplicasOptimizer&lt;/code&gt;'s local step.</source>
          <target state="translated">这包装了实际优化器中的 &lt;code&gt;variables()&lt;/code&gt; 。它不包括 &lt;code&gt;SyncReplicasOptimizer&lt;/code&gt; 的本地步骤。</target>
        </trans-unit>
        <trans-unit id="cea56f6fda7d999d314aba39c0fdf92a26a4a3d0" translate="yes" xml:space="preserve">
          <source>Thread Compatibility</source>
          <target state="translated">线程兼容性</target>
        </trans-unit>
        <trans-unit id="c707a1c58a3c28cb78f0eb8786b82d358ba76091" translate="yes" xml:space="preserve">
          <source>Thread code:</source>
          <target state="translated">主题代码:</target>
        </trans-unit>
        <trans-unit id="a1bfe9290c9416a7b4254caec7ab391081602a4c" translate="yes" xml:space="preserve">
          <source>Thread identifier of this thread or None if it has not been started.</source>
          <target state="translated">本线程的线程标识符,如果没有启动,则为 &quot;无&quot;。</target>
        </trans-unit>
        <trans-unit id="51781b51dab5c68c5071966ab81a91a116a5c345" translate="yes" xml:space="preserve">
          <source>ThreadPoolDataset</source>
          <target state="translated">ThreadPoolDataset</target>
        </trans-unit>
        <trans-unit id="c1e81f3f498c6f28feb25121d03bcd7897368f6a" translate="yes" xml:space="preserve">
          <source>ThreadPoolHandle</source>
          <target state="translated">ThreadPoolHandle</target>
        </trans-unit>
        <trans-unit id="b296a2588708c5eab00cc272e74d54b936f28305" translate="yes" xml:space="preserve">
          <source>ThreadUnsafeUnigramCandidateSampler</source>
          <target state="translated">ThreadUnsafeUnigramCandidateSampler</target>
        </trans-unit>
        <trans-unit id="673246c2e28b595a5660cad8b7dc6e0bcb64f903" translate="yes" xml:space="preserve">
          <source>Threshold below which the singular value is counted as 'zero'. Default value: &lt;code&gt;None&lt;/code&gt; (i.e., &lt;code&gt;eps * max(rows, cols) * max(singular_val)&lt;/code&gt;).</source>
          <target state="translated">阈值，低于该阈值的奇异值将被计为&amp;ldquo;零&amp;rdquo;。默认值： &lt;code&gt;None&lt;/code&gt; （即 &lt;code&gt;eps * max(rows, cols) * max(singular_val)&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="bf8ab928262e89058d10a418722762edfa599e46" translate="yes" xml:space="preserve">
          <source>Thresholded Rectified Linear Unit.</source>
          <target state="translated">阈值整流线性单元。</target>
        </trans-unit>
        <trans-unit id="9c1b3e348c5e658c9f04296e21ad3e2315006119" translate="yes" xml:space="preserve">
          <source>Throws:</source>
          <target state="translated">Throws:</target>
        </trans-unit>
        <trans-unit id="19453fe8b327db6c16c490630fc6fcb997c1883b" translate="yes" xml:space="preserve">
          <source>Thus the saved model can be reinstantiated in the exact same state, without any of the code used for model definition or training.</source>
          <target state="translated">因此,所保存的模型可以在完全相同的状态下进行重述,而无需任何用于模型定义或训练的代码。</target>
        </trans-unit>
        <trans-unit id="2dd2c660983753cf7cd7ed0bfc6dd2c1219c9bc2" translate="yes" xml:space="preserve">
          <source>Tile</source>
          <target state="translated">Tile</target>
        </trans-unit>
        <trans-unit id="04d349170a16840309d778c9a6bc280d1695cf69" translate="yes" xml:space="preserve">
          <source>TileGrad</source>
          <target state="translated">TileGrad</target>
        </trans-unit>
        <trans-unit id="ec15d67c587aeda01fdee12aac8f67ec1f6fc946" translate="yes" xml:space="preserve">
          <source>Time (in seconds) to wait for process cleanup to propagate.</source>
          <target state="translated">等待进程清理传播的时间(秒)。</target>
        </trans-unit>
        <trans-unit id="c5071117f474d7a0d851f7b8b532519d4fe0b2f5" translate="yes" xml:space="preserve">
          <source>Time boundaries at which to call Run(), or None if it should be called back to back.</source>
          <target state="translated">调用Run()的时间界限,如果要背靠背调用则为None。</target>
        </trans-unit>
        <trans-unit id="ea13a52ff3b11f3a3cff17d94eddba0db805c4f1" translate="yes" xml:space="preserve">
          <source>Time series forecasting</source>
          <target state="translated">时间序列预测</target>
        </trans-unit>
        <trans-unit id="3e5a5aa6d16549aefb0642b61016bd76ed1a572d" translate="yes" xml:space="preserve">
          <source>Time step at which the gradient was computed.</source>
          <target state="translated">计算梯度的时间步骤。</target>
        </trans-unit>
        <trans-unit id="d96b16498245359e1fc62fcf077cd9c20e575ff4" translate="yes" xml:space="preserve">
          <source>Timer that triggers at most once every N seconds or once every N steps.</source>
          <target state="translated">最多每N秒或每N步触发一次的定时器。</target>
        </trans-unit>
        <trans-unit id="19eabc961735d78f12fc7be906ffcb033853cf85" translate="yes" xml:space="preserve">
          <source>Timestamp</source>
          <target state="translated">Timestamp</target>
        </trans-unit>
        <trans-unit id="b0bee40a202555900d8a3e86b930a56b855407cd" translate="yes" xml:space="preserve">
          <source>To &lt;code&gt;run&lt;/code&gt; without hooks.</source>
          <target state="translated">要 &lt;code&gt;run&lt;/code&gt; 无钩。</target>
        </trans-unit>
        <trans-unit id="40336213cb93ad585f6a3a6e3ff018601cc59a2a" translate="yes" xml:space="preserve">
          <source>To achieve a performance improvement, you can also wrap the &lt;code&gt;strategy.run&lt;/code&gt; call with a &lt;a href=&quot;../range&quot;&gt;&lt;code&gt;tf.range&lt;/code&gt;&lt;/a&gt; inside a &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;. This runs multiple steps in a &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;. Autograph will convert it to a &lt;a href=&quot;../while_loop&quot;&gt;&lt;code&gt;tf.while_loop&lt;/code&gt;&lt;/a&gt; on the worker. However, it is less flexible comparing with running a single step inside &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;. For example, you cannot run things eagerly or arbitrary python code within the steps.</source>
          <target state="translated">为了提高性能，您还可以在&lt;a href=&quot;../range&quot;&gt; &lt;code&gt;tf.range&lt;/code&gt; &lt;/a&gt;内使用&lt;a href=&quot;../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;来包装对 &lt;code&gt;strategy.run&lt;/code&gt; 的调用。这将在&lt;a href=&quot;../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; 中&lt;/a&gt;运行多个步骤。签名将其转换为&lt;a href=&quot;../while_loop&quot;&gt; &lt;code&gt;tf.while_loop&lt;/code&gt; &lt;/a&gt;上的tf.while_loop。但是，与在&lt;a href=&quot;../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; 中&lt;/a&gt;运行单个步骤相比，它的灵活性较差。例如，您不能在这些步骤中急切地运行某些程序或任意python代码。</target>
        </trans-unit>
        <trans-unit id="f375d91c07ec3d845eaa0b13fb5e2121d93801bc" translate="yes" xml:space="preserve">
          <source>To add an inner vector length axis to a tensor of scalars.</source>
          <target state="translated">在标量的张量上增加一个内向量长度轴。</target>
        </trans-unit>
        <trans-unit id="bd6eda6c08067795995f955381394f94ccb8826e" translate="yes" xml:space="preserve">
          <source>To aggregate gradients yourself, call &lt;code&gt;apply_gradients&lt;/code&gt; with &lt;code&gt;experimental_aggregate_gradients&lt;/code&gt; set to False. This is useful if you need to process aggregated gradients.</source>
          <target state="translated">聚集梯度自己，呼叫 &lt;code&gt;apply_gradients&lt;/code&gt; 与 &lt;code&gt;experimental_aggregate_gradients&lt;/code&gt; 设置为False。如果您需要处理汇总的渐变，这将很有用。</target>
        </trans-unit>
        <trans-unit id="010eecd6ae7f583a0121a3a0180c941339655bd9" translate="yes" xml:space="preserve">
          <source>To apply a functional operation to the nonzero elements of a SparseTensor one of the following methods is recommended. First, if the function is expressible as TensorFlow ops, use</source>
          <target state="translated">要将函数运算应用于 SparseTensor 的非零元素,推荐使用以下方法之一。首先,如果函数可表达为 TensorFlow 运算,使用</target>
        </trans-unit>
        <trans-unit id="3eb084feee66304e2da16fe914a839b30b34c9ff" translate="yes" xml:space="preserve">
          <source>To associate a &lt;code&gt;StatsAggregator&lt;/code&gt; with a &lt;a href=&quot;../../../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; object, use the following pattern:</source>
          <target state="translated">要将 &lt;code&gt;StatsAggregator&lt;/code&gt; 与&lt;a href=&quot;../../../../data/dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt;对象相关联，请使用以下模式：</target>
        </trans-unit>
        <trans-unit id="c154cd080332e723a840721f93b48724110f7c06" translate="yes" xml:space="preserve">
          <source>To associate a &lt;code&gt;StatsAggregator&lt;/code&gt; with a &lt;a href=&quot;../dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt; object, use the following pattern:</source>
          <target state="translated">要将 &lt;code&gt;StatsAggregator&lt;/code&gt; 与&lt;a href=&quot;../dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt;对象相关联，请使用以下模式：</target>
        </trans-unit>
        <trans-unit id="936085b5545471dc42df6c7c7d391847c9751475" translate="yes" xml:space="preserve">
          <source>To avoid copies, if the consumer of the returned value is on the same device as the variable, this actually returns the live value of the variable, not a copy. Updates to the variable are seen by the consumer. If the consumer is on a different device it will get a copy of the variable.</source>
          <target state="translated">为了避免拷贝,如果返回值的消费者与变量在同一个设备上,这实际上返回的是变量的实时值,而不是拷贝。变量的更新会被消费者看到。如果消费者在不同的设备上,它将得到一个变量的副本。</target>
        </trans-unit>
        <trans-unit id="7c1421196dd351f3e8d18229f5901c13e59818be" translate="yes" xml:space="preserve">
          <source>To avoid this operation one can looping over the first &lt;code&gt;ndims&lt;/code&gt; of the variable and using &lt;code&gt;scatter_update&lt;/code&gt; on the subtensors that result of slicing the first dimension. This is a valid option for &lt;code&gt;ndims = 1&lt;/code&gt;, but less efficient than this implementation.</source>
          <target state="translated">为了避免此操作，可以循环遍历变量的第一个 &lt;code&gt;ndims&lt;/code&gt; ，并在对第一个维度进行切片的次张量上使用 &lt;code&gt;scatter_update&lt;/code&gt; 。这是 &lt;code&gt;ndims = 1&lt;/code&gt; 的有效选项，但效率低于此实现。</target>
        </trans-unit>
        <trans-unit id="cfcb9f867b92956487a659195717febb57a22acc" translate="yes" xml:space="preserve">
          <source>To avoid this operation there would be 2 alternatives:</source>
          <target state="translated">为了避免这种操作,将有2种选择。</target>
        </trans-unit>
        <trans-unit id="d93bb75610f7b39567f5b306f3397077993ff8e1" translate="yes" xml:space="preserve">
          <source>To avoid this operation there would be 2 alternatives: 1) Reshaping the variable by merging the first &lt;code&gt;ndims&lt;/code&gt; dimensions. However, this is not possible because &lt;a href=&quot;../../reshape&quot;&gt;&lt;code&gt;tf.reshape&lt;/code&gt;&lt;/a&gt; returns a Tensor, which we cannot use &lt;a href=&quot;scatter_update&quot;&gt;&lt;code&gt;tf.compat.v1.scatter_update&lt;/code&gt;&lt;/a&gt; on. 2) Looping over the first &lt;code&gt;ndims&lt;/code&gt; of the variable and using &lt;a href=&quot;scatter_update&quot;&gt;&lt;code&gt;tf.compat.v1.scatter_update&lt;/code&gt;&lt;/a&gt; on the subtensors that result of slicing the first dimension. This is a valid option for &lt;code&gt;ndims = 1&lt;/code&gt;, but less efficient than this implementation.</source>
          <target state="translated">为避免此操作，有两种选择：1）通过合并第一个 &lt;code&gt;ndims&lt;/code&gt; 维来重塑变量。但是，这是不可能的，因为&lt;a href=&quot;../../reshape&quot;&gt; &lt;code&gt;tf.reshape&lt;/code&gt; &lt;/a&gt;返回一个Tensor，我们不能在其上使用&lt;a href=&quot;scatter_update&quot;&gt; &lt;code&gt;tf.compat.v1.scatter_update&lt;/code&gt; &lt;/a&gt;。2）循环遍历变量的第一个 &lt;code&gt;ndims&lt;/code&gt; ，并在切片第一个维度的结果的次张量上使用&lt;a href=&quot;scatter_update&quot;&gt; &lt;code&gt;tf.compat.v1.scatter_update&lt;/code&gt; &lt;/a&gt;。这是 &lt;code&gt;ndims = 1&lt;/code&gt; 的有效选项，但效率低于此实现。</target>
        </trans-unit>
        <trans-unit id="c9183a300f45fff79988522f16235dc4e2de646c" translate="yes" xml:space="preserve">
          <source>To be implemented by subclasses:</source>
          <target state="translated">要由子类实现。</target>
        </trans-unit>
        <trans-unit id="52f2d1556ade9e7f9e4af5a180af2690b66cec6f" translate="yes" xml:space="preserve">
          <source>To be used together with &lt;code&gt;initializer = tf.variance_scaling_initializer(factor=1.0, mode='FAN_IN')&lt;/code&gt;. For correct dropout, use &lt;code&gt;tf.contrib.nn.alpha_dropout&lt;/code&gt;.</source>
          <target state="translated">与 &lt;code&gt;initializer = tf.variance_scaling_initializer(factor=1.0, mode='FAN_IN')&lt;/code&gt; 。要正确退出，请使用 &lt;code&gt;tf.contrib.nn.alpha_dropout&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ea58afe1c10fb928517e860a0db2a381e664fd3c" translate="yes" xml:space="preserve">
          <source>To be used together with the &lt;a href=&quot;../initializers/lecunnormal&quot;&gt;&lt;code&gt;tf.keras.initializers.LecunNormal&lt;/code&gt;&lt;/a&gt; initializer.</source>
          <target state="translated">与&lt;a href=&quot;../initializers/lecunnormal&quot;&gt; &lt;code&gt;tf.keras.initializers.LecunNormal&lt;/code&gt; &lt;/a&gt;初始化程序一起使用。</target>
        </trans-unit>
        <trans-unit id="067aa09e87c256fc4280bb7886d1747104dcfb5d" translate="yes" xml:space="preserve">
          <source>To be used together with the dropout variant &lt;a href=&quot;../layers/alphadropout&quot;&gt;&lt;code&gt;tf.keras.layers.AlphaDropout&lt;/code&gt;&lt;/a&gt; (not regular dropout).</source>
          <target state="translated">与&lt;a href=&quot;../layers/alphadropout&quot;&gt; &lt;code&gt;tf.keras.layers.AlphaDropout&lt;/code&gt; &lt;/a&gt;变体tf.keras.layers.AlphaDropout（非常规dropout）一起使用。</target>
        </trans-unit>
        <trans-unit id="53dd8873954bfa79dfd0bea43d6ef54fe742261a" translate="yes" xml:space="preserve">
          <source>To build a SavedModel, the first meta graph must be saved with variables. Subsequent meta graphs will simply be saved with their graph definitions. If assets need to be saved and written or copied to disk, they can be provided when the meta graph def is added. If multiple meta graph defs are associated an asset of the same name, only the first version is retained.</source>
          <target state="translated">要建立一个SavedModel,第一个元图必须和变量一起保存。随后的元图将简单地保存其图形定义。如果需要保存资产并将其写入或复制到磁盘,可以在添加元图def时提供。如果多个元图定义与同名资产相关联,则只保留第一个版本。</target>
        </trans-unit>
        <trans-unit id="44fe360fd9bf0e087ab467407a9221a32654e234" translate="yes" xml:space="preserve">
          <source>To construct a TPUStrategy object, you need to run the initialization code as below:</source>
          <target state="translated">构建TPUStrategy对象时,需要运行如下的初始化代码。</target>
        </trans-unit>
        <trans-unit id="83c7623c3022948b14096264ce7d526cb9408c74" translate="yes" xml:space="preserve">
          <source>To consume the statistics, associate a &lt;code&gt;StatsAggregator&lt;/code&gt; with the output dataset.</source>
          <target state="translated">要使用统计信息， &lt;code&gt;StatsAggregator&lt;/code&gt; 与输出数据集相关联。</target>
        </trans-unit>
        <trans-unit id="d2287264f43a3ef6c22b5d232bbf5118319287d4" translate="yes" xml:space="preserve">
          <source>To create a &lt;a href=&quot;../compat/v1/session&quot;&gt;&lt;code&gt;tf.compat.v1.Session&lt;/code&gt;&lt;/a&gt; that connects to this server, use the following snippet:</source>
          <target state="translated">要创建连接到该服务器的&lt;a href=&quot;../compat/v1/session&quot;&gt; &lt;code&gt;tf.compat.v1.Session&lt;/code&gt; &lt;/a&gt;，请使用以下代码段：</target>
        </trans-unit>
        <trans-unit id="d6382ce12824513eb2d0429e1935eb788cf98fff" translate="yes" xml:space="preserve">
          <source>To create a cluster with two jobs and five tasks, you specify the mapping from job names to lists of network addresses (typically hostname-port pairs).</source>
          <target state="translated">要创建一个具有两个作业和五个任务的群集,您可以指定从作业名称到网络地址列表(通常是主机名称-端口对)的映射。</target>
        </trans-unit>
        <trans-unit id="162a52ef0e892f8de2d469fbf8a8ede583aeb2c2" translate="yes" xml:space="preserve">
          <source>To create a dataset of all files matching a pattern, use &lt;a href=&quot;dataset#list_files&quot;&gt;&lt;code&gt;tf.data.Dataset.list_files&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">要创建与模式匹配的所有文件的数据集，请使用&lt;a href=&quot;dataset#list_files&quot;&gt; &lt;code&gt;tf.data.Dataset.list_files&lt;/code&gt; &lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="d3f4ab00aa69fb40cd693922a5c483368cf34082" translate="yes" xml:space="preserve">
          <source>To customize the estimator &lt;code&gt;eval_metric_ops&lt;/code&gt; names, you can pass in the &lt;code&gt;metric_names_map&lt;/code&gt; dictionary mapping the keras model output metric names to the custom names as follows:</source>
          <target state="translated">要自定义估算器 &lt;code&gt;eval_metric_ops&lt;/code&gt; 名称，可以传入 &lt;code&gt;metric_names_map&lt;/code&gt; 字典，将keras模型输出度量标准名称映射到自定义名称，如下所示：</target>
        </trans-unit>
        <trans-unit id="baed8fb95bde1dbbdafd79f549daf02185ceb494" translate="yes" xml:space="preserve">
          <source>To enable a public method, subclasses should implement the leading-underscore version of the method. The argument signature should be identical except for the omission of &lt;code&gt;name=&quot;...&quot;&lt;/code&gt;. For example, to enable &lt;code&gt;matmul(x, adjoint=False, name=&quot;matmul&quot;)&lt;/code&gt; a subclass should implement &lt;code&gt;_matmul(x, adjoint=False)&lt;/code&gt;.</source>
          <target state="translated">要启用公共方法，子类应实现该方法的前导下划线版本。除了省略 &lt;code&gt;name=&quot;...&quot;&lt;/code&gt; 外，参数签名应相同。例如，要启用 &lt;code&gt;matmul(x, adjoint=False, name=&quot;matmul&quot;)&lt;/code&gt; ，子类应实现 &lt;code&gt;_matmul(x, adjoint=False)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="a64f441e5dd84fd06f79adb0f1f3a053a7155dea" translate="yes" xml:space="preserve">
          <source>To enable and control broadcasting, use an ellipsis. For example, to perform batch matrix multiplication with NumPy-style broadcasting across the batch dimensions, use:</source>
          <target state="translated">要启用和控制广播,请使用省略号。例如,要用NumPy式的批量矩阵乘法执行跨批量维度的广播,请使用。</target>
        </trans-unit>
        <trans-unit id="afa60615bea13e940acd80b65497cda8c8de7ef9" translate="yes" xml:space="preserve">
          <source>To enable statefulness:</source>
          <target state="translated">要启用状态性。</target>
        </trans-unit>
        <trans-unit id="01b6fd9e2269c63a7f1c5eddbf780087a139a252" translate="yes" xml:space="preserve">
          <source>To enable statefulness: - Specify &lt;code&gt;stateful=True&lt;/code&gt; in the layer constructor. - Specify a fixed batch size for your model, by passing If sequential model: &lt;code&gt;batch_input_shape=(...)&lt;/code&gt; to the first layer in your model. Else for functional model with 1 or more Input layers: &lt;code&gt;batch_shape=(...)&lt;/code&gt; to all the first layers in your model. This is the expected shape of your inputs &lt;em&gt;including the batch size&lt;/em&gt;. It should be a tuple of integers, e.g. &lt;code&gt;(32, 10, 100)&lt;/code&gt;. - Specify &lt;code&gt;shuffle=False&lt;/code&gt; when calling fit().</source>
          <target state="translated">要启用状态性：- 在图层构造函数中指定 &lt;code&gt;stateful=True&lt;/code&gt; 。 -通过将If顺序模型： &lt;code&gt;batch_input_shape=(...)&lt;/code&gt; 传递到模型的第一层，为模型指定固定的批量大小。对于具有1个或多个输入层的功能模型，则为其他：对模型中所有第一层的 &lt;code&gt;batch_shape=(...)&lt;/code&gt; 。这是您期望的输入形状，&lt;em&gt;包括批量大小&lt;/em&gt;。它应该是整数的元组，例如 &lt;code&gt;(32, 10, 100)&lt;/code&gt; 。 - 调用fit（）时指定 &lt;code&gt;shuffle=False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d5e380bc52096caf61cf4a0e2ca23ca08f41f03b" translate="yes" xml:space="preserve">
          <source>To enable this Soft-NMS mode, set the &lt;code&gt;soft_nms_sigma&lt;/code&gt; parameter to be larger than 0. When &lt;code&gt;soft_nms_sigma&lt;/code&gt; equals 0, the behavior of &lt;a href=&quot;non_max_suppression_padded&quot;&gt;&lt;code&gt;tf.image.non_max_suppression_padded&lt;/code&gt;&lt;/a&gt; is identical to that of &lt;a href=&quot;non_max_suppression&quot;&gt;&lt;code&gt;tf.image.non_max_suppression&lt;/code&gt;&lt;/a&gt; (except for the extra output) both in function and in running time.</source>
          <target state="translated">为了使此软NMS模式，设定 &lt;code&gt;soft_nms_sigma&lt;/code&gt; 参数为大于0。当 &lt;code&gt;soft_nms_sigma&lt;/code&gt; 等于0，的行为&lt;a href=&quot;non_max_suppression_padded&quot;&gt; &lt;code&gt;tf.image.non_max_suppression_padded&lt;/code&gt; &lt;/a&gt;是相同的的&lt;a href=&quot;non_max_suppression&quot;&gt; &lt;code&gt;tf.image.non_max_suppression&lt;/code&gt; &lt;/a&gt;无论是在（除额外输出）功能和运行时间。</target>
        </trans-unit>
        <trans-unit id="63aaf92714daf46639926647a3d0a69e133b8f2a" translate="yes" xml:space="preserve">
          <source>To enable this Soft-NMS mode, set the &lt;code&gt;soft_nms_sigma&lt;/code&gt; parameter to be larger than 0. When &lt;code&gt;soft_nms_sigma&lt;/code&gt; equals 0, the behavior of &lt;code&gt;tf.image.non_max_suppression_v2&lt;/code&gt; is identical to that of &lt;a href=&quot;non_max_suppression&quot;&gt;&lt;code&gt;tf.image.non_max_suppression&lt;/code&gt;&lt;/a&gt; (except for the extra output) both in function and in running time.</source>
          <target state="translated">为了使此软NMS模式，设定 &lt;code&gt;soft_nms_sigma&lt;/code&gt; 参数为大于0。当 &lt;code&gt;soft_nms_sigma&lt;/code&gt; 等于0，的行为 &lt;code&gt;tf.image.non_max_suppression_v2&lt;/code&gt; 是相同的的&lt;a href=&quot;non_max_suppression&quot;&gt; &lt;code&gt;tf.image.non_max_suppression&lt;/code&gt; &lt;/a&gt;（除了额外输出）都在功能和运行时间。</target>
        </trans-unit>
        <trans-unit id="30dc95cd6fb2fe82cb6bde586b190076a23204d3" translate="yes" xml:space="preserve">
          <source>To ensure forward compatibility of generated graphs (see &lt;code&gt;forward_compatible&lt;/code&gt;) with older binaries, new features can be gated with:</source>
          <target state="translated">为了确保生成的图形（请参见 &lt;code&gt;forward_compatible&lt;/code&gt; ）与旧的二进制文件具有向前兼容性，可以使用以下新功能来设置新功能：</target>
        </trans-unit>
        <trans-unit id="2a6dfd40e1c087316d123358f0bfd9c6696a8b43" translate="yes" xml:space="preserve">
          <source>To ensure that loading is complete and no more assignments will take place, use the &lt;code&gt;assert_consumed()&lt;/code&gt; method of the status object returned by &lt;code&gt;restore()&lt;/code&gt;:</source>
          <target state="translated">为了确保加载完成后并没有更多的分配将发生，使用 &lt;code&gt;assert_consumed()&lt;/code&gt; 返回的状态对象的方法， &lt;code&gt;restore()&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="6fdc8715d19df576a528e05d54600b3b8b7e951d" translate="yes" xml:space="preserve">
          <source>To ensure that loading is complete and no more assignments will take place, use the &lt;code&gt;assert_consumed()&lt;/code&gt; method of the status object returned by &lt;code&gt;restore&lt;/code&gt;:</source>
          <target state="translated">为了确保加载完成后并没有更多的分配将发生，使用 &lt;code&gt;assert_consumed()&lt;/code&gt; 返回的状态对象的方法 &lt;code&gt;restore&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="7f9135f3ba27360a5d60098a844664f0e165caf9" translate="yes" xml:space="preserve">
          <source>To extend, inherit from this class; from the subclass &lt;strong&gt;init&lt;/strong&gt;, call</source>
          <target state="translated">要扩展，请从此类继承；从子类&lt;strong&gt;init&lt;/strong&gt;调用</target>
        </trans-unit>
        <trans-unit id="a5d8cbeafaa3eeb0c6165bfa56b4aafe15d6a9db" translate="yes" xml:space="preserve">
          <source>To generate different sequences across sessions, set neither graph-level nor op-level seeds:</source>
          <target state="translated">要在不同的会话中生成不同的序列,既不要设置图级种子,也不要设置运算级种子。</target>
        </trans-unit>
        <trans-unit id="8b8fda9f12ffa5af323a889062c3aa890309e2cb" translate="yes" xml:space="preserve">
          <source>To generate the same repeatable sequence for an op across sessions, set the seed for the op:</source>
          <target state="translated">要为一个操作在不同会话中生成相同的可重复序列,请为操作设置种子。</target>
        </trans-unit>
        <trans-unit id="ff3af5efb2174b5bcd82487648e50b8ae5b6afb0" translate="yes" xml:space="preserve">
          <source>To get a more intuitive and visual look at what this operation does, you can run tensorflow/examples/wav_to_spectrogram to read in an audio file and save out the resulting spectrogram as a PNG image.</source>
          <target state="translated">为了更直观、更形象地了解这个操作,你可以运行tensorflow/examples/wav_to_spectrogram来读取音频文件,并将生成的谱图保存为PNG图像。</target>
        </trans-unit>
        <trans-unit id="ec9fe6e91399876e841634556ab27d6bee162948" translate="yes" xml:space="preserve">
          <source>To get a protocol buffer summary of the currently aggregated statistics, use the &lt;code&gt;StatsAggregator.get_summary()&lt;/code&gt; tensor. The easiest way to do this is to add the returned tensor to the &lt;code&gt;tf.GraphKeys.SUMMARIES&lt;/code&gt; collection, so that the summaries will be included with any existing summaries.</source>
          <target state="translated">要获取当前聚合统计信息的协议缓冲区摘要，请使用 &lt;code&gt;StatsAggregator.get_summary()&lt;/code&gt; 张量。最简单的方法是将返回的张量添加到 &lt;code&gt;tf.GraphKeys.SUMMARIES&lt;/code&gt; 集合中，这样，这些摘要将包含在任何现有摘要中。</target>
        </trans-unit>
        <trans-unit id="eecdfb3013e20388a17f33fbb93597b30d4ef349" translate="yes" xml:space="preserve">
          <source>To get the current default session, use &lt;a href=&quot;get_default_session&quot;&gt;&lt;code&gt;tf.compat.v1.get_default_session&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">要获取当前的默认会话，请使用&lt;a href=&quot;get_default_session&quot;&gt; &lt;code&gt;tf.compat.v1.get_default_session&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="273ca78fffe894af5651288f4a1f4c051d209749" translate="yes" xml:space="preserve">
          <source>To illustrate the user-visible effects, consider these examples:</source>
          <target state="translated">为了说明用户可见的效果,请考虑这些例子。</target>
        </trans-unit>
        <trans-unit id="ce5890e8d9ded5f833a93abef51a8a4c147da863" translate="yes" xml:space="preserve">
          <source>To instead reorder the data to rearrange the dimensions of a tensor, see &lt;a href=&quot;transpose&quot;&gt;&lt;code&gt;tf.transpose&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">要改为对数据重新排序以重新排列张量的尺寸，请参见&lt;a href=&quot;transpose&quot;&gt; &lt;code&gt;tf.transpose&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="58521b85b5941fed07e375b8c580b661cb67962d" translate="yes" xml:space="preserve">
          <source>To load a network from a JSON save file, use &lt;a href=&quot;../models/model_from_json&quot;&gt;&lt;code&gt;keras.models.model_from_json(json_string, custom_objects={})&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">要从JSON保存文件加载网络，请使用&lt;a href=&quot;../models/model_from_json&quot;&gt; &lt;code&gt;keras.models.model_from_json(json_string, custom_objects={})&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="fc1f35de200c65966b97907804a085417fa2674d" translate="yes" xml:space="preserve">
          <source>To load a network from a JSON save file, use &lt;a href=&quot;models/model_from_json&quot;&gt;&lt;code&gt;keras.models.model_from_json(json_string, custom_objects={})&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">要从JSON保存文件加载网络，请使用&lt;a href=&quot;models/model_from_json&quot;&gt; &lt;code&gt;keras.models.model_from_json(json_string, custom_objects={})&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="171ed2725b41e726794d8676fc06308b4d649fcf" translate="yes" xml:space="preserve">
          <source>To load a network from a yaml save file, use &lt;a href=&quot;../models/model_from_yaml&quot;&gt;&lt;code&gt;keras.models.model_from_yaml(yaml_string, custom_objects={})&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">要从yaml保存文件加载网络，请使用&lt;a href=&quot;../models/model_from_yaml&quot;&gt; &lt;code&gt;keras.models.model_from_yaml(yaml_string, custom_objects={})&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="32acfa401b4c952f2dcfef233deb7f737a491a64" translate="yes" xml:space="preserve">
          <source>To load a network from a yaml save file, use &lt;a href=&quot;models/model_from_yaml&quot;&gt;&lt;code&gt;keras.models.model_from_yaml(yaml_string, custom_objects={})&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">要从yaml保存文件加载网络，请使用&lt;a href=&quot;models/model_from_yaml&quot;&gt; &lt;code&gt;keras.models.model_from_yaml(yaml_string, custom_objects={})&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="84dbf2d04bfcc507efa1aac8fcfb0ed50455e091" translate="yes" xml:space="preserve">
          <source>To make the random sequences generated by all ops be repeatable across sessions, set a graph-level seed:</source>
          <target state="translated">为了使所有操作产生的随机序列在各个会话中都可以重复,设置一个图级种子。</target>
        </trans-unit>
        <trans-unit id="8bca5a7e3515ab70d1734fcf9290f94f57a539f3" translate="yes" xml:space="preserve">
          <source>To mimic the behavior of &lt;code&gt;np.flatten&lt;/code&gt; (which flattens all dimensions), use &lt;code&gt;rt.merge_dims(0, -1). To mimic the behavior of&lt;/code&gt;tf.layers.Flatten&lt;code&gt;(which flattens all dimensions except the outermost batch dimension), use&lt;/code&gt;rt.merge_dims(1, -1)`.</source>
          <target state="translated">若要模拟 &lt;code&gt;np.flatten&lt;/code&gt; 的行为（会展平所有尺寸），请使用 &lt;code&gt;rt.merge_dims(0, -1). To mimic the behavior of&lt;/code&gt; tf.layers.Flatten &lt;code&gt;(which flattens all dimensions except the outermost batch dimension), use&lt;/code&gt; rt.merge_dims（1，-1）`。</target>
        </trans-unit>
        <trans-unit id="f9a3e8831e7d974549f1a0383e7928bbabcfacd7" translate="yes" xml:space="preserve">
          <source>To obtain an individual graph, use the &lt;code&gt;get_concrete_function&lt;/code&gt; method of the callable created by &lt;a href=&quot;function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;. It can be called with the same arguments as &lt;code&gt;func&lt;/code&gt; and returns a special &lt;a href=&quot;graph&quot;&gt;&lt;code&gt;tf.Graph&lt;/code&gt;&lt;/a&gt; object:</source>
          <target state="translated">为了获得一个单独的图形中，使用 &lt;code&gt;get_concrete_function&lt;/code&gt; 通过创建可调用的方法&lt;a href=&quot;function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;。可以使用与 &lt;code&gt;func&lt;/code&gt; 相同的参数来调用它，并返回一个特殊的&lt;a href=&quot;graph&quot;&gt; &lt;code&gt;tf.Graph&lt;/code&gt; &lt;/a&gt;对象：</target>
        </trans-unit>
        <trans-unit id="9e83f95895428f42e71becadca9982d2894dba04" translate="yes" xml:space="preserve">
          <source>To pass sample weights when training or evaluating the Estimator, the first item returned by the input function should be a dictionary with keys &lt;code&gt;features&lt;/code&gt; and &lt;code&gt;sample_weights&lt;/code&gt;. Example below:</source>
          <target state="translated">要在训练或评估Estimator时传递样本权重，输入功能返回的第一项应该是带有关键 &lt;code&gt;features&lt;/code&gt; 和 &lt;code&gt;sample_weights&lt;/code&gt; 的字典。下面的例子：</target>
        </trans-unit>
        <trans-unit id="a0110a07b243d67a47edf557bd54d79fd2de9674" translate="yes" xml:space="preserve">
          <source>To perform the clipping, the values &lt;code&gt;t_list[i]&lt;/code&gt; are set to:</source>
          <target state="translated">为了执行裁剪，将值 &lt;code&gt;t_list[i]&lt;/code&gt; 设置为：</target>
        </trans-unit>
        <trans-unit id="5e552f3cce7d0e03051d89f4e0255d1c49f37d1b" translate="yes" xml:space="preserve">
          <source>To prevent accidental sharing of variables, we raise an exception when getting an existing variable in a non-reusing scope.</source>
          <target state="translated">为了防止变量的意外共享,当我们在一个非重复使用的作用域中获取一个现有变量时,我们会引发一个异常。</target>
        </trans-unit>
        <trans-unit id="b90b39244fd5711de404a1ed79326b6635b08950" translate="yes" xml:space="preserve">
          <source>To process lines from files, use &lt;a href=&quot;textlinedataset&quot;&gt;&lt;code&gt;tf.data.TextLineDataset&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">要处理文件中的行，请使用&lt;a href=&quot;textlinedataset&quot;&gt; &lt;code&gt;tf.data.TextLineDataset&lt;/code&gt; &lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="aa139b98ea790b72d4805462346af0f7c690db65" translate="yes" xml:space="preserve">
          <source>To process records written in the &lt;code&gt;TFRecord&lt;/code&gt; format, use &lt;code&gt;TFRecordDataset&lt;/code&gt;:</source>
          <target state="translated">要处理以 &lt;code&gt;TFRecord&lt;/code&gt; 格式编写的记录，请使用 &lt;code&gt;TFRecordDataset&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="862cc9660449fcff05a0a9d8dd83a651e63fd705" translate="yes" xml:space="preserve">
          <source>To provide an API that is close to Python's file I/O objects, and</source>
          <target state="translated">提供一个接近Python的文件I/O对象的API,以及</target>
        </trans-unit>
        <trans-unit id="e93ce7e6602bc5d8a646cce42b0e2f27a7ee545d" translate="yes" xml:space="preserve">
          <source>To provide an implementation based on TensorFlow's C++ FileSystem API.</source>
          <target state="translated">提供一个基于TensorFlow的C++FileSystem API的实现。</target>
        </trans-unit>
        <trans-unit id="b683767243ad0f37ed1467c8125bf95f601a1fc4" translate="yes" xml:space="preserve">
          <source>To read back the elements, use &lt;code&gt;TFRecordDataset&lt;/code&gt;.</source>
          <target state="translated">要读回元素，请使用 &lt;code&gt;TFRecordDataset&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ff235c20472c16fd213bae4a4af01054d920d420" translate="yes" xml:space="preserve">
          <source>To reconstruct an original waveform, a complementary window function should be used with &lt;code&gt;inverse_stft&lt;/code&gt;. Such a window function can be constructed with &lt;a href=&quot;inverse_stft_window_fn&quot;&gt;&lt;code&gt;tf.signal.inverse_stft_window_fn&lt;/code&gt;&lt;/a&gt;. Example:</source>
          <target state="translated">要重建原始波形，应将互补窗口函数与 &lt;code&gt;inverse_stft&lt;/code&gt; 一起使用。可以使用&lt;a href=&quot;inverse_stft_window_fn&quot;&gt; &lt;code&gt;tf.signal.inverse_stft_window_fn&lt;/code&gt; &lt;/a&gt;构造这样的窗口函数。例：</target>
        </trans-unit>
        <trans-unit id="a1cda1853dc2b4c5ea41c6c734bd8df45e09a74c" translate="yes" xml:space="preserve">
          <source>To reconstruct an original waveform, the same window function should be used with &lt;code&gt;mdct&lt;/code&gt; and &lt;code&gt;inverse_mdct&lt;/code&gt;.</source>
          <target state="translated">要重建原始波形，应将相同的窗口函数与 &lt;code&gt;mdct&lt;/code&gt; 和 &lt;code&gt;inverse_mdct&lt;/code&gt; 一起使用。</target>
        </trans-unit>
        <trans-unit id="ca5c9dd44204672cbbbbaf3d7ffdd096d2e60573" translate="yes" xml:space="preserve">
          <source>To record statistics, use one of the custom transformation functions defined in this module when defining your &lt;a href=&quot;../../../../data/dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt;. All statistics will be aggregated by the &lt;code&gt;StatsAggregator&lt;/code&gt; that is associated with a particular iterator (see below). For example, to record the latency of producing each element by iterating over a dataset:</source>
          <target state="translated">要记录统计信息，请在定义&lt;a href=&quot;../../../../data/dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt;时使用此模块中定义的自定义转换函数之一。所有统计信息将由与特定迭代器关联的 &lt;code&gt;StatsAggregator&lt;/code&gt; 聚合（请参见下文）。例如，要通过迭代数据集来记录产生每个元素的等待时间：</target>
        </trans-unit>
        <trans-unit id="84004f640543424d2c71289d24eec0f0b8df9f20" translate="yes" xml:space="preserve">
          <source>To record statistics, use one of the custom transformation functions defined in this module when defining your &lt;a href=&quot;../dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt;. All statistics will be aggregated by the &lt;code&gt;StatsAggregator&lt;/code&gt; that is associated with a particular iterator (see below). For example, to record the latency of producing each element by iterating over a dataset:</source>
          <target state="translated">要记录统计信息，请在定义&lt;a href=&quot;../dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; &lt;/a&gt;时使用此模块中定义的自定义转换函数之一。所有统计信息将由与特定迭代器关联的 &lt;code&gt;StatsAggregator&lt;/code&gt; 聚合（请参见下文）。例如，要通过迭代数据集来记录产生每个元素的等待时间：</target>
        </trans-unit>
        <trans-unit id="6601eea42af302d30ea25dfb68ead4d332e25000" translate="yes" xml:space="preserve">
          <source>To rescale an input in the &lt;code&gt;[0, 255]&lt;/code&gt; range to be in the &lt;code&gt;[-1, 1]&lt;/code&gt; range, you would pass &lt;code&gt;scale=1./127.5, offset=-1&lt;/code&gt;.</source>
          <target state="translated">要将 &lt;code&gt;[0, 255]&lt;/code&gt; 范围内的输入重新缩放为 &lt;code&gt;[-1, 1]&lt;/code&gt; 范围内，您可以传递 &lt;code&gt;scale=1./127.5, offset=-1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="878d90bc13f047ae3b3be5ee736a5ab7a5fd6624" translate="yes" xml:space="preserve">
          <source>To rescale an input in the &lt;code&gt;[0, 255]&lt;/code&gt; range to be in the &lt;code&gt;[0, 1]&lt;/code&gt; range, you would pass &lt;code&gt;scale=1./255&lt;/code&gt;.</source>
          <target state="translated">重新缩放的输入在 &lt;code&gt;[0, 255]&lt;/code&gt; 范围是在 &lt;code&gt;[0, 1]&lt;/code&gt; 的范围，你会通过 &lt;code&gt;scale=1./255&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="9bf5571681c9436431c1ed224b52ea46ea226e1b" translate="yes" xml:space="preserve">
          <source>To reset the states of your model, call &lt;code&gt;.reset_states()&lt;/code&gt; on either a specific layer, or on your entire model.</source>
          <target state="translated">要重置模型的状态，请在特定层或整个模型上调用 &lt;code&gt;.reset_states()&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5f2fb526cb8dbaa2bd872bcdf1bffcdc847f4f4f" translate="yes" xml:space="preserve">
          <source>To restore variables, you have to know the name of the shadow variables. That name and the original variable can then be passed to a &lt;code&gt;Saver()&lt;/code&gt; object to restore the variable from the moving average value with: &lt;code&gt;saver = tf.compat.v1.train.Saver({ema.average_name(var): var})&lt;/code&gt;</source>
          <target state="translated">要还原变量，您必须知道阴影变量的名称。然后可以将该名称和原始变量传递给 &lt;code&gt;Saver()&lt;/code&gt; 对象，以使用以下方法从移动平均值中恢复变量： &lt;code&gt;saver = tf.compat.v1.train.Saver({ema.average_name(var): var})&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="61b9174bc024e647c5c9bf1631b4141790c76fd9" translate="yes" xml:space="preserve">
          <source>To run TF2 programs on TPUs, you can either use &lt;code&gt;.compile&lt;/code&gt; and &lt;code&gt;.fit&lt;/code&gt; APIs in &lt;a href=&quot;../../keras&quot;&gt;&lt;code&gt;tf.keras&lt;/code&gt;&lt;/a&gt; with TPUStrategy, or write your own customized training loop by calling &lt;code&gt;strategy.experimental_run_v2&lt;/code&gt; directly. Note that TPUStrategy doesn't support pure eager execution, so please make sure the function passed into &lt;code&gt;strategy.experimental_run_v2&lt;/code&gt; is a &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; or &lt;code&gt;strategy.experimental_run_v2&lt;/code&gt; us called inside a &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; if running in eager mode.</source>
          <target state="translated">要在TPU上运行TF2程序，您可以将 &lt;code&gt;.compile&lt;/code&gt; 和 &lt;code&gt;.fit&lt;/code&gt; API与&lt;a href=&quot;../../keras&quot;&gt; &lt;code&gt;tf.keras&lt;/code&gt; &lt;/a&gt;一起使用，或者直接调用 &lt;code&gt;strategy.experimental_run_v2&lt;/code&gt; 编写自己的自定义训练循环。需要注意的是TPUStrategy不支持纯急于执行，所以请确保传递到函数 &lt;code&gt;strategy.experimental_run_v2&lt;/code&gt; 是&lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;或 &lt;code&gt;strategy.experimental_run_v2&lt;/code&gt; 我们称为内部&lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;如果急于模式下运行。</target>
        </trans-unit>
        <trans-unit id="75d6af2437dac9afdbd259194ca306ea0df7ba35" translate="yes" xml:space="preserve">
          <source>To run TF2 programs on TPUs, you can either use &lt;code&gt;.compile&lt;/code&gt; and &lt;code&gt;.fit&lt;/code&gt; APIs in &lt;a href=&quot;../../keras&quot;&gt;&lt;code&gt;tf.keras&lt;/code&gt;&lt;/a&gt; with TPUStrategy, or write your own customized training loop by calling &lt;code&gt;strategy.run&lt;/code&gt; directly. Note that TPUStrategy doesn't support pure eager execution, so please make sure the function passed into &lt;code&gt;strategy.run&lt;/code&gt; is a &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; or &lt;code&gt;strategy.run&lt;/code&gt; is called inside a &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; if eager behavior is enabled.</source>
          <target state="translated">要在TPU上运行TF2程序，您可以将 &lt;code&gt;.compile&lt;/code&gt; 和 &lt;code&gt;.fit&lt;/code&gt; API与&lt;a href=&quot;../../keras&quot;&gt; &lt;code&gt;tf.keras&lt;/code&gt; &lt;/a&gt;一起使用，或者通过直接调用 &lt;code&gt;strategy.run&lt;/code&gt; 编写自己的自定义训练循环。需要注意的是TPUStrategy不支持纯急于执行，所以请确保传递到函数 &lt;code&gt;strategy.run&lt;/code&gt; 是&lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;或 &lt;code&gt;strategy.run&lt;/code&gt; 被称为内&lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;如果急于行为被启用。</target>
        </trans-unit>
        <trans-unit id="1d76e97ea1150910b642e3424cedffc8e064a46d" translate="yes" xml:space="preserve">
          <source>To run TF2 programs on TPUs, you can either use &lt;code&gt;.compile&lt;/code&gt; and &lt;code&gt;.fit&lt;/code&gt; APIs in &lt;a href=&quot;../keras&quot;&gt;&lt;code&gt;tf.keras&lt;/code&gt;&lt;/a&gt; with TPUStrategy, or write your own customized training loop by calling &lt;code&gt;strategy.run&lt;/code&gt; directly. Note that TPUStrategy doesn't support pure eager execution, so please make sure the function passed into &lt;code&gt;strategy.run&lt;/code&gt; is a &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; or &lt;code&gt;strategy.run&lt;/code&gt; is called inside a &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; if eager behavior is enabled. See more details in https://www.tensorflow.org/guide/tpu.</source>
          <target state="translated">要在TPU上运行TF2程序，您可以将 &lt;code&gt;.compile&lt;/code&gt; 和 &lt;code&gt;.fit&lt;/code&gt; API与&lt;a href=&quot;../keras&quot;&gt; &lt;code&gt;tf.keras&lt;/code&gt; &lt;/a&gt;一起使用，或者通过直接调用 &lt;code&gt;strategy.run&lt;/code&gt; 编写自己的自定义训练循环。需要注意的是TPUStrategy不支持纯急于执行，所以请确保传递到函数 &lt;code&gt;strategy.run&lt;/code&gt; 是&lt;a href=&quot;../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;或 &lt;code&gt;strategy.run&lt;/code&gt; 被称为内&lt;a href=&quot;../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;如果急于行为被启用。在https://www.tensorflow.org/guide/tpu中查看更多详细信息。</target>
        </trans-unit>
        <trans-unit id="e990fa899ccbe4b80cb8729867536ff2b0542458" translate="yes" xml:space="preserve">
          <source>To save and restore.</source>
          <target state="translated">要保存和恢复。</target>
        </trans-unit>
        <trans-unit id="b35673692b203b1580cc459164c684b7c71fe1e4" translate="yes" xml:space="preserve">
          <source>To shard a &lt;code&gt;dataset&lt;/code&gt; across multiple TFRecord files:</source>
          <target state="translated">要跨多个TFRecord文件分片 &lt;code&gt;dataset&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="fc6bb2051a5897d4a8d4dac929c8a069356b9e0c" translate="yes" xml:space="preserve">
          <source>To simplify the thread implementation, the Coordinator provides a context handler &lt;code&gt;stop_on_exception()&lt;/code&gt; that automatically requests a stop if an exception is raised. Using the context handler the thread code above can be written as:</source>
          <target state="translated">为了简化线程的实现，协调器提供了一个上下文处理程序 &lt;code&gt;stop_on_exception()&lt;/code&gt; ，如果引发异常，该处理程序将自动请求停止。使用上下文处理程序，以上线程代码可以编写为：</target>
        </trans-unit>
        <trans-unit id="a72b5fda5da9fdf55b3bf54113912820e04f4679" translate="yes" xml:space="preserve">
          <source>To stop the trace and export the collected information, use &lt;a href=&quot;trace_export&quot;&gt;&lt;code&gt;tf.summary.trace_export&lt;/code&gt;&lt;/a&gt;. To stop the trace without exporting, use &lt;a href=&quot;trace_off&quot;&gt;&lt;code&gt;tf.summary.trace_off&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">要停止跟踪并导出收集的信息，请使用&lt;a href=&quot;trace_export&quot;&gt; &lt;code&gt;tf.summary.trace_export&lt;/code&gt; &lt;/a&gt;。要停止跟踪而不导出，请使用&lt;a href=&quot;trace_off&quot;&gt; &lt;code&gt;tf.summary.trace_off&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="25b078e238adc2f2bf2f225cb3a76dafd2b5b18d" translate="yes" xml:space="preserve">
          <source>To take the transpose of the matrices in dimension-0 (such as when you are transposing matrices where 0 is the batch dimesnion), you would set &lt;code&gt;perm=[0,2,1]&lt;/code&gt;.</source>
          <target state="translated">要对维度为0的矩阵进行转置（例如，当对矩阵进行转置时，批次dimesnion的值为0），则应设置 &lt;code&gt;perm=[0,2,1]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="78edbe912a6ec7dfbde2fa6ca3dbd293be354b5e" translate="yes" xml:space="preserve">
          <source>To train with replicas you deploy the same program in a &lt;code&gt;Cluster&lt;/code&gt;. One of the tasks must be identified as the &lt;em&gt;chief&lt;/em&gt;: the task that handles initialization, checkpoints, summaries, and recovery. The other tasks depend on the &lt;em&gt;chief&lt;/em&gt; for these services.</source>
          <target state="translated">要训​​练副本，您需要在 &lt;code&gt;Cluster&lt;/code&gt; 部署相同的程序。必须确定其中一项任务为&lt;em&gt;负责人&lt;/em&gt;：负责初始化，检查点，摘要和恢复的任务。其他任务取决于&lt;em&gt;首席&lt;/em&gt;这些服务。</target>
        </trans-unit>
        <trans-unit id="0a01c3edc32585755d57328d40fa48f8fdb3ab82" translate="yes" xml:space="preserve">
          <source>To treat a sparse input as dense, provide &lt;code&gt;allow_missing=True&lt;/code&gt;; otherwise, the parse functions will fail on any examples missing this feature.</source>
          <target state="translated">要将稀疏输入视为密集输入，请提供 &lt;code&gt;allow_missing=True&lt;/code&gt; ；否则，解析函数将在缺少此功能的任何示例上失败。</target>
        </trans-unit>
        <trans-unit id="f70a87293b647de36d39cf376105ebfad1b65755" translate="yes" xml:space="preserve">
          <source>To treat sparse input as dense, provide a &lt;code&gt;default_value&lt;/code&gt;; otherwise, the parse functions will fail on any examples missing this feature.</source>
          <target state="translated">要将稀疏输入视为密集输入，请提供 &lt;code&gt;default_value&lt;/code&gt; ；否则，解析函数将在缺少此功能的任何示例上失败。</target>
        </trans-unit>
        <trans-unit id="2cf2713de328f732331e0697fff7bd058371175a" translate="yes" xml:space="preserve">
          <source>To use &lt;code&gt;MirroredStrategy&lt;/code&gt; with multiple workers, please refer to &lt;code&gt;tf.distribute.MultiWorkerMirroredStrategy&lt;/code&gt;.</source>
          <target state="translated">要将 &lt;code&gt;MirroredStrategy&lt;/code&gt; 与多个工作程序一起使用，请参阅 &lt;code&gt;tf.distribute.MultiWorkerMirroredStrategy&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="12770f5e5f15720c161e4a87129f9d9bb0774bc6" translate="yes" xml:space="preserve">
          <source>To use SyncReplicasOptimizer with an &lt;code&gt;Estimator&lt;/code&gt;, you need to send sync_replicas_hook while calling the fit.</source>
          <target state="translated">要将SyncReplicasOptimizer与 &lt;code&gt;Estimator&lt;/code&gt; 一起使用，您需要在调用fit时发送sync_replicas_hook。</target>
        </trans-unit>
        <trans-unit id="783180a85bf045e00f72afe9cf4aeba0310e67f8" translate="yes" xml:space="preserve">
          <source>To use a listener, implement a class and pass the listener to a &lt;code&gt;CheckpointSaverHook&lt;/code&gt;, as in this example:</source>
          <target state="translated">要使用侦听器，请实现一个类，并将侦听器传递给 &lt;code&gt;CheckpointSaverHook&lt;/code&gt; ，如本示例所示：</target>
        </trans-unit>
        <trans-unit id="dc7556a3e1ddbbfb55342d1204bfb7cd99d3d15a" translate="yes" xml:space="preserve">
          <source>To use another kernel, just replace the layer creation line with:</source>
          <target state="translated">如果要使用另一个内核,只需将创建层的一行替换为。</target>
        </trans-unit>
        <trans-unit id="82a8f0d7938dca39dd9baa444dbabf12a6beff73" translate="yes" xml:space="preserve">
          <source>To use crossed column in DNN model, you need to add it in an embedding column as in this example:</source>
          <target state="translated">要在DNN模型中使用交叉列,需要在嵌入列中添加,如本例。</target>
        </trans-unit>
        <trans-unit id="df232f0149a3304cb55f19e9533d29d033c88a07" translate="yes" xml:space="preserve">
          <source>To use it with Keras &lt;code&gt;compile&lt;/code&gt;/&lt;code&gt;fit&lt;/code&gt;, &lt;a href=&quot;https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_keras&quot;&gt;please read&lt;/a&gt;.</source>
          <target state="translated">要将它与Keras &lt;code&gt;compile&lt;/code&gt; / &lt;code&gt;fit&lt;/code&gt; ，&lt;a href=&quot;https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_keras&quot;&gt;请阅读&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="52761ad57b4e88f15825661793f52089d3153688" translate="yes" xml:space="preserve">
          <source>To use mixed precision in a Keras model, the &lt;code&gt;'mixed_float16'&lt;/code&gt; or &lt;code&gt;'mixed_bfloat16'&lt;/code&gt; policy can be used. &lt;a href=&quot;set_policy&quot;&gt;&lt;code&gt;tf.keras.mixed_precision.experimental.set_policy&lt;/code&gt;&lt;/a&gt; can be used to set the default policy for layers if no policy is passed to them. For example:</source>
          <target state="translated">要在 &lt;code&gt;'mixed_float16'&lt;/code&gt; 模型中使用混合精度，可以使用'mixed_float16'或 &lt;code&gt;'mixed_bfloat16'&lt;/code&gt; 策略。如果没有将策略传递给图层，则可以使用&lt;a href=&quot;set_policy&quot;&gt; &lt;code&gt;tf.keras.mixed_precision.experimental.set_policy&lt;/code&gt; &lt;/a&gt;设置默认策略。例如：</target>
        </trans-unit>
        <trans-unit id="e3bc90b596c6a06641f646fd21855a01a15f891a" translate="yes" xml:space="preserve">
          <source>To use partial execution, a user first calls &lt;code&gt;partial_run_setup()&lt;/code&gt; and then a sequence of &lt;code&gt;partial_run()&lt;/code&gt;. &lt;code&gt;partial_run_setup&lt;/code&gt; specifies the list of feeds and fetches that will be used in the subsequent &lt;code&gt;partial_run&lt;/code&gt; calls.</source>
          <target state="translated">要使用部分执行，用户首先调用 &lt;code&gt;partial_run_setup()&lt;/code&gt; ，然后序列 &lt;code&gt;partial_run()&lt;/code&gt; 。 &lt;code&gt;partial_run_setup&lt;/code&gt; 指定将在随后的 &lt;code&gt;partial_run&lt;/code&gt; 调用中使用的提要和获取的列表。</target>
        </trans-unit>
        <trans-unit id="0486110c8c71e38a21a240cd6d1c94b9eb3a8e49" translate="yes" xml:space="preserve">
          <source>To use the pprof file:</source>
          <target state="translated">要使用pprof文件。</target>
        </trans-unit>
        <trans-unit id="77972f247631f66b18016dae59b6ae8450d1266a" translate="yes" xml:space="preserve">
          <source>To use the replacement for variables which does not have these issues:</source>
          <target state="translated">要使用不存在这些问题的变量的替换。</target>
        </trans-unit>
        <trans-unit id="49e82f46abbf85f6d932a9d996d53b01fa3132b2" translate="yes" xml:space="preserve">
          <source>To use this API on TPU you should use a custom training loop. Below is an example of a training and evaluation step:</source>
          <target state="translated">要在 TPU 上使用该 API,您应该使用自定义训练循环。下面是一个训练和评估步骤的例子。</target>
        </trans-unit>
        <trans-unit id="d9fed22a474957718b6e8cceaa345a94b692ff2f" translate="yes" xml:space="preserve">
          <source>To use, enqueue filenames in a Queue. The output of Read will be a filename (key) and the contents of that file (value).</source>
          <target state="translated">使用时,在队列中查询文件名。Read的输出将是一个文件名(key)和该文件的内容(value)。</target>
        </trans-unit>
        <trans-unit id="22500ae27d7e8f143eaf335c0b5e17ab00f9b916" translate="yes" xml:space="preserve">
          <source>To use, enqueue filenames in a Queue. The output of ReaderRead will be a filename (key) and the contents of that file (value).</source>
          <target state="translated">使用时,在Queue中枚举文件名。ReaderRead的输出将是一个文件名(键)和该文件的内容(值)。</target>
        </trans-unit>
        <trans-unit id="936bb6e71e849ce05ab2a85d8c98f04120f3012d" translate="yes" xml:space="preserve">
          <source>To use, enqueue strings in a Queue. Read will take the front work string and output (work, work).</source>
          <target state="translated">使用时,在Queue中enqueue字符串。读取会取前面的工作字符串并输出(工作,工作)。</target>
        </trans-unit>
        <trans-unit id="c154b7f61f883ef570a45770d3ff51fd323c7da3" translate="yes" xml:space="preserve">
          <source>To use, enqueue strings in a Queue. ReaderRead will take the front work string and output (work, work).</source>
          <target state="translated">使用时,在Queue中enqueue字符串。ReaderRead将取前面的工作字符串并输出(work,工作)。</target>
        </trans-unit>
        <trans-unit id="3b5c6be676ffa47f64b992a379fa3a687addb427" translate="yes" xml:space="preserve">
          <source>To warm-start an &lt;code&gt;Estimator&lt;/code&gt;:</source>
          <target state="translated">要热启动 &lt;code&gt;Estimator&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="4e303367b1554b64249c563b8cdde01d47543e39" translate="yes" xml:space="preserve">
          <source>ToBool</source>
          <target state="translated">ToBool</target>
        </trans-unit>
        <trans-unit id="487247adcb71f2786b8b26a837b3a8f982eb5eb2" translate="yes" xml:space="preserve">
          <source>Toeplitz and Circulant Matrices - A Review: &lt;a href=&quot;https://www.nowpublishers.com/article/Details/CIT-006&quot;&gt;Gray, 2006&lt;/a&gt; (&lt;a href=&quot;https://ee.stanford.edu/%7Egray/toeplitz.pdf&quot;&gt;pdf&lt;/a&gt;)</source>
          <target state="translated">托普利茨和循环矩阵-评论：&lt;a href=&quot;https://www.nowpublishers.com/article/Details/CIT-006&quot;&gt;Gray，2006年&lt;/a&gt;（&lt;a href=&quot;https://ee.stanford.edu/%7Egray/toeplitz.pdf&quot;&gt;pdf&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="33d27663923e948257deb0ac3f48e158f539a021" translate="yes" xml:space="preserve">
          <source>Toeplitz means that &lt;code&gt;A&lt;/code&gt; has constant diagonals. Hence, &lt;code&gt;A&lt;/code&gt; can be generated with two vectors. One represents the first column of the matrix, and the other represents the first row.</source>
          <target state="translated">Toeplitz表示 &lt;code&gt;A&lt;/code&gt; 具有恒定的对角线。因此，可以用两个向量生成 &lt;code&gt;A&lt;/code&gt; 。一个代表矩阵的第一列，另一个代表第一行。</target>
        </trans-unit>
        <trans-unit id="0b92ef92f904efdaf4c897b9d82c8d8bc711a6dd" translate="yes" xml:space="preserve">
          <source>Top K categorical accuracy value.</source>
          <target state="translated">最高K分类准确值。</target>
        </trans-unit>
        <trans-unit id="00a0204649835b513c94e7dcd17a04a18748c48f" translate="yes" xml:space="preserve">
          <source>TopK</source>
          <target state="translated">TopK</target>
        </trans-unit>
        <trans-unit id="9be91f59904e4556becc788662db35f00e8e37f9" translate="yes" xml:space="preserve">
          <source>TopKV2</source>
          <target state="translated">TopKV2</target>
        </trans-unit>
        <trans-unit id="ecc5f00336dd7a221505890680888796ae7ad79b" translate="yes" xml:space="preserve">
          <source>Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes).</source>
          <target state="translated">打印行的总长度(例如,设置该值以适应不同终端窗口尺寸的显示)。</target>
        </trans-unit>
        <trans-unit id="18b8da40b59609823f34b2eae31c92eb13b8a1df" translate="yes" xml:space="preserve">
          <source>Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of &lt;code&gt;None&lt;/code&gt;. If x is a &lt;a href=&quot;../../data&quot;&gt;&lt;code&gt;tf.data&lt;/code&gt;&lt;/a&gt; dataset and &lt;code&gt;steps&lt;/code&gt; is None, &lt;code&gt;predict&lt;/code&gt; will run until the input dataset is exhausted.</source>
          <target state="translated">在宣布预测回合之前完成的步骤总数（样本批次）。忽略默认值 &lt;code&gt;None&lt;/code&gt; 。如果x是&lt;a href=&quot;../../data&quot;&gt; &lt;code&gt;tf.data&lt;/code&gt; &lt;/a&gt;数据集，而 &lt;code&gt;steps&lt;/code&gt; 为None，则 &lt;code&gt;predict&lt;/code&gt; 将运行直到输入数据集用尽。</target>
        </trans-unit>
        <trans-unit id="d64736c6b92cc8a7d61eecd538f81edd562b347c" translate="yes" xml:space="preserve">
          <source>Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of &lt;code&gt;None&lt;/code&gt;. If x is a &lt;a href=&quot;../data&quot;&gt;&lt;code&gt;tf.data&lt;/code&gt;&lt;/a&gt; dataset and &lt;code&gt;steps&lt;/code&gt; is None, &lt;code&gt;predict&lt;/code&gt; will run until the input dataset is exhausted.</source>
          <target state="translated">在宣布预测回合之前完成的步骤总数（样本批次）。忽略默认值 &lt;code&gt;None&lt;/code&gt; 。如果x是&lt;a href=&quot;../data&quot;&gt; &lt;code&gt;tf.data&lt;/code&gt; &lt;/a&gt;数据集，而 &lt;code&gt;steps&lt;/code&gt; 为None，则 &lt;code&gt;predict&lt;/code&gt; 将运行直到输入数据集用尽。</target>
        </trans-unit>
        <trans-unit id="f9b71747c87f2120b712f35eff6cd2daf9f890ef" translate="yes" xml:space="preserve">
          <source>Total number of steps expected, None if unknown.</source>
          <target state="translated">预期步骤总数,未知时为无。</target>
        </trans-unit>
        <trans-unit id="2b273e7fcad30a2510d0545ec0ed37f53fcc2226" translate="yes" xml:space="preserve">
          <source>Total number of tasks/workers/replicas, could be different from replicas_to_aggregate. If total_num_replicas &amp;gt; replicas_to_aggregate: it is backup_replicas + replicas_to_aggregate. If total_num_replicas &amp;lt; replicas_to_aggregate: Replicas compute multiple batches per update to variables.</source>
          <target state="translated">任务/工作人员/副本的总数可能与plicates_to_aggregate不同。如果total_num_replicas&amp;gt;复制副本到聚合集合：则为backup_replicas +复制副本到聚合集合。如果total_num_replicas &amp;lt;plicates_to_aggregate：副本每次更新变量将计算多个批处理。</target>
        </trans-unit>
        <trans-unit id="3c766217ff2603fec7b0d19fd0e734ed0a36b0a6" translate="yes" xml:space="preserve">
          <source>Trace events are created only when the profiler is enabled. More information on how to use the profiler can be found at &lt;a href=&quot;https://tensorflow.org/guide/profiler&quot;&gt;https://tensorflow.org/guide/profiler&lt;/a&gt;</source>
          <target state="translated">仅在启用了探查器时才创建跟踪事件。有关如何使用分析器的更多信息，请参见&lt;a href=&quot;https://tensorflow.org/guide/profiler&quot;&gt;https://tensorflow.org/guide/profiler&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="121fe412af2d5e916662ca3ec0794ef40b1e2f67" translate="yes" xml:space="preserve">
          <source>Trace of the linear operator, equal to sum of &lt;code&gt;self.diag_part()&lt;/code&gt;.</source>
          <target state="translated">线性运算符的迹线，等于 &lt;code&gt;self.diag_part()&lt;/code&gt; 的总和。</target>
        </trans-unit>
        <trans-unit id="865739f4547ef1507e787549027075c5e994644c" translate="yes" xml:space="preserve">
          <source>Trace the function</source>
          <target state="translated">追踪功能</target>
        </trans-unit>
        <trans-unit id="36823bd4f0f4f117aef52940f9d3cfb4969f8d54" translate="yes" xml:space="preserve">
          <source>Trace the function, see the &lt;a href=&quot;https://www.tensorflow.org/guide/concrete_function&quot;&gt;Concrete Functions Guide&lt;/a&gt; for details.</source>
          <target state="translated">跟踪函数，有关详细信息，请参见《&lt;a href=&quot;https://www.tensorflow.org/guide/concrete_function&quot;&gt;具体函数指南&lt;/a&gt;》。</target>
        </trans-unit>
        <trans-unit id="a3cd08d7e53167c39ff4df8ac6fd7573ac51ea11" translate="yes" xml:space="preserve">
          <source>Traces argument information at compilation time.</source>
          <target state="translated">在编译时追踪参数信息。</target>
        </trans-unit>
        <trans-unit id="0eed61bd40a0f60d630ca02fa135c76ba2b343a8" translate="yes" xml:space="preserve">
          <source>Tracing may fail if a shape missmatch can be detected:</source>
          <target state="translated">如果可以检测到形状不匹配,跟踪可能会失败。</target>
        </trans-unit>
        <trans-unit id="235a91222dd28e8e16c0e133dc2cba6c6d12742f" translate="yes" xml:space="preserve">
          <source>Train a linear model to classify instances into one of multiple possible classes. When number of possible classes is 2, this is binary classification.</source>
          <target state="translated">训练一个线性模型,将实例划分为多个可能的类之一。当可能的类数为2时,这就是二元分类。</target>
        </trans-unit>
        <trans-unit id="ee3c9790cb66e90ecfd717511a371fde17bac07c" translate="yes" xml:space="preserve">
          <source>Train a linear regression model to predict label value given observation of feature values.</source>
          <target state="translated">训练一个线性回归模型来预测标签值,给定观察到的特征值。</target>
        </trans-unit>
        <trans-unit id="3efa73c7139b833a96e690294f1f5dee0c8dafcf" translate="yes" xml:space="preserve">
          <source>Train and evaluate the &lt;code&gt;estimator&lt;/code&gt;.</source>
          <target state="translated">训练和评估 &lt;code&gt;estimator&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="beb3053fd8366acd014d9b2f3ece6098b4b72e55" translate="yes" xml:space="preserve">
          <source>Train and evaluate with Keras</source>
          <target state="translated">用Keras进行培训和评估</target>
        </trans-unit>
        <trans-unit id="cc6f15e9d8d0df93c841a4bfbdce1ebcc8b8e338" translate="yes" xml:space="preserve">
          <source>Trainable variables (created by &lt;a href=&quot;variable&quot;&gt;&lt;code&gt;tf.Variable&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;compat/v1/get_variable&quot;&gt;&lt;code&gt;tf.compat.v1.get_variable&lt;/code&gt;&lt;/a&gt;, where &lt;code&gt;trainable=True&lt;/code&gt; is default in both cases) are automatically watched. Tensors can be manually watched by invoking the &lt;code&gt;watch&lt;/code&gt; method on this context manager.</source>
          <target state="translated">自动监视 &lt;code&gt;trainable=True&lt;/code&gt; 变量（由&lt;a href=&quot;variable&quot;&gt; &lt;code&gt;tf.Variable&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;compat/v1/get_variable&quot;&gt; &lt;code&gt;tf.compat.v1.get_variable&lt;/code&gt; &lt;/a&gt;创建，其中两种情况下均默认为trainable = True）。通过在此上下文管理器上调用 &lt;code&gt;watch&lt;/code&gt; 方法，可以手动监视张量。</target>
        </trans-unit>
        <trans-unit id="d85aa0db75043b2fc7e6d076a658d6108e846757" translate="yes" xml:space="preserve">
          <source>Training checkpoints</source>
          <target state="translated">训练检查站</target>
        </trans-unit>
        <trans-unit id="10de55a426007543588aad0c26e9034f2ddc3a47" translate="yes" xml:space="preserve">
          <source>Training graph visualization</source>
          <target state="translated">训练图的可视化</target>
        </trans-unit>
        <trans-unit id="7db0d855207cb66583c78145b39525c5633639da" translate="yes" xml:space="preserve">
          <source>Training helper that restores from checkpoint and creates session.</source>
          <target state="translated">从检查点恢复并创建会话的训练助手。</target>
        </trans-unit>
        <trans-unit id="fb6f2aff4ed7bb3a59f319dbd5466e071cc53fba" translate="yes" xml:space="preserve">
          <source>Training loss &lt;code&gt;Tensor&lt;/code&gt;. Must be either scalar, or with shape &lt;code&gt;[1]&lt;/code&gt;.</source>
          <target state="translated">训练损失 &lt;code&gt;Tensor&lt;/code&gt; 。必须为标量或具有形状 &lt;code&gt;[1]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7079a2991739ec717b2df9e546127d4ec5f23f66" translate="yes" xml:space="preserve">
          <source>Training step to trigger on.</source>
          <target state="translated">训练步骤,以触发上。</target>
        </trans-unit>
        <trans-unit id="bf6759c44ad41b3091516c81e1bc49fed1c443ea" translate="yes" xml:space="preserve">
          <source>Trains a model given training data &lt;code&gt;input_fn&lt;/code&gt;.</source>
          <target state="translated">给定训练数据 &lt;code&gt;input_fn&lt;/code&gt; 训练模型。</target>
        </trans-unit>
        <trans-unit id="3b7a7a43991418e3ac5adb5f5a2111146e234e48" translate="yes" xml:space="preserve">
          <source>Trains a recurrent neural network model to classify instances into one of multiple classes.</source>
          <target state="translated">训练一个循环神经网络模型,将实例分为多个类中的一个。</target>
        </trans-unit>
        <trans-unit id="03d8eb1e5694e7e09ad702ee89cbe5988334450c" translate="yes" xml:space="preserve">
          <source>Trains the model for a fixed number of epochs (iterations on a dataset).</source>
          <target state="translated">训练模型的固定次数(数据集的迭代)。</target>
        </trans-unit>
        <trans-unit id="e5238d35744e76f113cfe81d17fbfa29f295ac05" translate="yes" xml:space="preserve">
          <source>Transcode the input text from a source encoding to a destination encoding.</source>
          <target state="translated">将输入的文本从源编码转码到目标编码。</target>
        </trans-unit>
        <trans-unit id="983dff86700d17b79a819dfaab7edfc9d23e3e22" translate="yes" xml:space="preserve">
          <source>Transfer learning with TensorFlow Hub</source>
          <target state="translated">用TensorFlow Hub转移学习</target>
        </trans-unit>
        <trans-unit id="db752c46f6cefcfd15e6ab5a7acbda3b3ec8bdb7" translate="yes" xml:space="preserve">
          <source>Transfer learning with a pretrained ConvNet</source>
          <target state="translated">用预先训练好的ConvNet转移学习。</target>
        </trans-unit>
        <trans-unit id="d3f4cdda163a694c3d7aafb554148d5d45b68f92" translate="yes" xml:space="preserve">
          <source>Transform [batch] matrix &lt;code&gt;x&lt;/code&gt; with left multiplication: &lt;code&gt;x --&amp;gt; Ax&lt;/code&gt;.</source>
          <target state="translated">用左乘法变换[batch]矩阵 &lt;code&gt;x&lt;/code&gt; ： &lt;code&gt;x --&amp;gt; Ax&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="42db4158efb02ff54237870e3243b8f49a10cb88" translate="yes" xml:space="preserve">
          <source>Transform [batch] vector &lt;code&gt;x&lt;/code&gt; with left multiplication: &lt;code&gt;x --&amp;gt; Ax&lt;/code&gt;.</source>
          <target state="translated">用左乘法变换[batch]向量 &lt;code&gt;x&lt;/code&gt; ： &lt;code&gt;x --&amp;gt; Ax&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d685ad7c59aeb1945ef867b52857539b62c1b81c" translate="yes" xml:space="preserve">
          <source>Transformations</source>
          <target state="translated">Transformations</target>
        </trans-unit>
        <trans-unit id="74bb10df9a38f2595c0a343bb271e614fdcbbc79" translate="yes" xml:space="preserve">
          <source>Transformations:</source>
          <target state="translated">Transformations:</target>
        </trans-unit>
        <trans-unit id="a65cfd47338728767e1d8fe389f325154f22e3d2" translate="yes" xml:space="preserve">
          <source>Transformer model for language understanding</source>
          <target state="translated">语言理解的转化模型</target>
        </trans-unit>
        <trans-unit id="c6c688b7b4f9c9a55098909d8e4bbea04f915bb0" translate="yes" xml:space="preserve">
          <source>Transforms &lt;code&gt;elems&lt;/code&gt; by applying &lt;code&gt;fn&lt;/code&gt; to each element unstacked on axis 0. (deprecated arguments)</source>
          <target state="translated">变换 &lt;code&gt;elems&lt;/code&gt; 的通过施加 &lt;code&gt;fn&lt;/code&gt; 到拆散轴线上0（不建议使用的参数）的每个元素</target>
        </trans-unit>
        <trans-unit id="33d02a85618ebba62b643cdb3a4e3f69d88e4866" translate="yes" xml:space="preserve">
          <source>Transforms &lt;code&gt;input_dataset&lt;/code&gt; containing &lt;code&gt;Example&lt;/code&gt; protos as vectors of DT_STRING into a dataset of &lt;code&gt;Tensor&lt;/code&gt; or &lt;code&gt;SparseTensor&lt;/code&gt; objects representing the parsed features.</source>
          <target state="translated">将包含 &lt;code&gt;Example&lt;/code&gt; 原型（作为DT_STRING的向量）的 &lt;code&gt;input_dataset&lt;/code&gt; 转换为表示已解析特征的 &lt;code&gt;Tensor&lt;/code&gt; 或 &lt;code&gt;SparseTensor&lt;/code&gt; 对象的数据集。</target>
        </trans-unit>
        <trans-unit id="ae192d97c8bb43adae60e73f077c7a262798214c" translate="yes" xml:space="preserve">
          <source>Transforms a Tensor into a serialized TensorProto proto.</source>
          <target state="translated">将一个Tensor转换为一个序列化的TensorProto proto。</target>
        </trans-unit>
        <trans-unit id="4999b7a71f3c11d96be3e3808c9e8fc9eb261a51" translate="yes" xml:space="preserve">
          <source>Transforms a scalar brain.SequenceExample proto (as strings) into typed tensors.</source>
          <target state="translated">将一个标量的brain.SequenceExample proto(作为字符串)转换为类型化的tensors。</target>
        </trans-unit>
        <trans-unit id="3fe37f8377ecffffde4f3405645d41e0b3b001bd" translate="yes" xml:space="preserve">
          <source>Transforms a serialized tensorflow.TensorProto proto into a Tensor.</source>
          <target state="translated">将序列化的tensorflow.TensorProto proto转换为Tensor.TensorProto Proto。</target>
        </trans-unit>
        <trans-unit id="f6ffa885d4348734b93da08e4df5eb979eae50df" translate="yes" xml:space="preserve">
          <source>Transforms a spectrogram into a form that's useful for speech recognition.</source>
          <target state="translated">将谱图转换为对语音识别有用的形式。</target>
        </trans-unit>
        <trans-unit id="bd2f17793a2563570a330d4671f70e89b2937951" translate="yes" xml:space="preserve">
          <source>Transforms a tf.Example proto (as a string) into typed tensors.</source>
          <target state="translated">将一个 tf.Example proto (作为一个字符串)转化为类型化的 tensors。</target>
        </trans-unit>
        <trans-unit id="0b8677520a5e74e9b4b4788602fe7bc1e6fe1360" translate="yes" xml:space="preserve">
          <source>Transforms a vector of brain.Example protos (as strings) into typed tensors.</source>
          <target state="translated">将脑.示例protos(作为字符串)的向量转换为类型化的tensors。</target>
        </trans-unit>
        <trans-unit id="be5a4de021d7eadbe941c3409ada3c7b1dabaea1" translate="yes" xml:space="preserve">
          <source>Transforms a vector of brain.SequenceExample protos (as strings) into typed tensors.</source>
          <target state="translated">将 brain.SequenceExample protos(字符串)的向量转换为类型化的 tensors。</target>
        </trans-unit>
        <trans-unit id="eb363dfb8e50cc87a12a26568447baf08e0619fb" translate="yes" xml:space="preserve">
          <source>Transforms a vector of tf.Example protos (as strings) into typed tensors.</source>
          <target state="translated">将tf.Example protos(字符串)的向量转换为类型化的tensors。</target>
        </trans-unit>
        <trans-unit id="db88575dec31c88470bf932dd1fed9370a7d6c45" translate="yes" xml:space="preserve">
          <source>Transforms a vector of tf.io.SequenceExample protos (as strings) into</source>
          <target state="translated">将tf.io.SequenceExample protos(字符串)向量转换为</target>
        </trans-unit>
        <trans-unit id="655ffb734eb7a488c9c872982dd743f0f56654c3" translate="yes" xml:space="preserve">
          <source>Transforms each input point to its distances to all cluster centers.</source>
          <target state="translated">将每个输入点转换为其到所有簇中心的距离。</target>
        </trans-unit>
        <trans-unit id="fcd8235ad6fe846d496fe3b5ab9ef987b5075ab1" translate="yes" xml:space="preserve">
          <source>Transforms each sequence in &lt;code&gt;sequences&lt;/code&gt; to a list of texts(strings).</source>
          <target state="translated">转换成在每个序列 &lt;code&gt;sequences&lt;/code&gt; 到的文本（字符串）的列表。</target>
        </trans-unit>
        <trans-unit id="e5d4ddf915c1a321118b7e1927e1abe917834a51" translate="yes" xml:space="preserve">
          <source>Transforms each sequence into a list of text.</source>
          <target state="translated">将每个序列转换为文本的列表。</target>
        </trans-unit>
        <trans-unit id="15870f729495d9cc692bfd46436e8e2286a88257" translate="yes" xml:space="preserve">
          <source>Transforms each text in &lt;code&gt;texts&lt;/code&gt; to a sequence of integers.</source>
          <target state="translated">转换成在每个文本 &lt;code&gt;texts&lt;/code&gt; 到整数序列。</target>
        </trans-unit>
        <trans-unit id="86add4b4e27d9c231884be5bf0ca73d7ca67d517" translate="yes" xml:space="preserve">
          <source>Transforms each text in texts to a sequence of integers.</source>
          <target state="translated">将文本中的每个文本转换为整数序列。</target>
        </trans-unit>
        <trans-unit id="71016bf2a8086e5adaa42aba0ed39ec30d8c82a3" translate="yes" xml:space="preserve">
          <source>Transparently swap the tensors produced in forward inference but needed for back prop from GPU to CPU. This allows training RNNs which would typically not fit on a single GPU, with very minimal (or no) performance penalty.</source>
          <target state="translated">透明地将前向推理中产生的但需要后向支撑的时序从GPU交换到CPU。这就允许训练通常不适合在单个GPU上的RNN,而性能上的损失非常小(或没有)。</target>
        </trans-unit>
        <trans-unit id="fa257429d8539a08df2ef78919507d4169cac2c8" translate="yes" xml:space="preserve">
          <source>Transpose</source>
          <target state="translated">Transpose</target>
        </trans-unit>
        <trans-unit id="c6f3940520a66558ccbf4deb29b8ad759ce2893f" translate="yes" xml:space="preserve">
          <source>Transpose image(s) by swapping the height and width dimension.</source>
          <target state="translated">通过交换高度和宽度尺寸来转换图像。</target>
        </trans-unit>
        <trans-unit id="aaef5878a51828ef045b24256f979251b2fa2f09" translate="yes" xml:space="preserve">
          <source>Transposed 2D convolution layer (sometimes called 2D Deconvolution).</source>
          <target state="translated">移植的二维卷积层(有时也称为二维解卷)。</target>
        </trans-unit>
        <trans-unit id="20b55b4579cc2c500b14b1401912fc234412af61" translate="yes" xml:space="preserve">
          <source>Transposed 3D convolution layer (sometimes called 3D Deconvolution).</source>
          <target state="translated">转移的3D卷积层(有时也称为3D解卷)。</target>
        </trans-unit>
        <trans-unit id="8af454b225f62d0dcbcdc7d510f5d2a829dab734" translate="yes" xml:space="preserve">
          <source>Transposed convolution layer (sometimes called Deconvolution).</source>
          <target state="translated">转化卷积层(有时也叫解卷积)。</target>
        </trans-unit>
        <trans-unit id="c1808bb1215e7e8c12e0e53a29ab899fd06887f1" translate="yes" xml:space="preserve">
          <source>Transposes &lt;code&gt;a&lt;/code&gt;, where &lt;code&gt;a&lt;/code&gt; is a Tensor.</source>
          <target state="translated">转置 &lt;code&gt;a&lt;/code&gt; ，其中 &lt;code&gt;a&lt;/code&gt; 是张量。</target>
        </trans-unit>
        <trans-unit id="4559d9f372084e43321f8c8b67aa21b4b3be2db7" translate="yes" xml:space="preserve">
          <source>Transposes &lt;code&gt;a&lt;/code&gt;.</source>
          <target state="translated">转置 &lt;code&gt;a&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5f6a95a7cfa0d75e4317493ed12e7c428f7e9164" translate="yes" xml:space="preserve">
          <source>Transposes a &lt;code&gt;SparseTensor&lt;/code&gt;</source>
          <target state="translated">转 &lt;code&gt;SparseTensor&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="2262bf9a92872de90cc8f31b3040779445c53825" translate="yes" xml:space="preserve">
          <source>Transposes a tensor and returns it.</source>
          <target state="translated">转置一个张量并返回它。</target>
        </trans-unit>
        <trans-unit id="52d7d3690c76e45b3f4ac6d7704301666eae2d1f" translate="yes" xml:space="preserve">
          <source>Transposes last two dimensions of tensor &lt;code&gt;a&lt;/code&gt;.</source>
          <target state="translated">转置张量 &lt;code&gt;a&lt;/code&gt; 的最后两个维度。</target>
        </trans-unit>
        <trans-unit id="9979dd1e79bbad4d151836984b737dcb27347af2" translate="yes" xml:space="preserve">
          <source>Transposes the inner (matrix) dimensions of a CSRSparseMatrix.</source>
          <target state="translated">转置一个CSRSparseMatrix的内部(矩阵)维度。</target>
        </trans-unit>
        <trans-unit id="3011a7a9922254d6bc94cbf852ba13e1faec6347" translate="yes" xml:space="preserve">
          <source>Transposes the inner (matrix) dimensions of a SparseMatrix and optionally conjugates its values.</source>
          <target state="translated">转置一个 SparseMatrix 的内部(矩阵)维度,并可选择将其值共轭。</target>
        </trans-unit>
        <trans-unit id="ad508b5e8ca6e24e73ed31c06d7f5a032986b9fa" translate="yes" xml:space="preserve">
          <source>Transposes the last two dimensions of and conjugates tensor &lt;code&gt;matrix&lt;/code&gt;.</source>
          <target state="translated">转置张量 &lt;code&gt;matrix&lt;/code&gt; 和的最后两个维度。</target>
        </trans-unit>
        <trans-unit id="50adcae727c1540c5f8c2710b6a31d5f9d861f46" translate="yes" xml:space="preserve">
          <source>TridiagonalMatMul</source>
          <target state="translated">TridiagonalMatMul</target>
        </trans-unit>
        <trans-unit id="90d5236e355acbf1f60d52716bcd6f0402f00405" translate="yes" xml:space="preserve">
          <source>TridiagonalSolve</source>
          <target state="translated">TridiagonalSolve</target>
        </trans-unit>
        <trans-unit id="06d45be632bf282d211af84cbad96e7a33a03b23" translate="yes" xml:space="preserve">
          <source>Trouser</source>
          <target state="translated">Trouser</target>
        </trans-unit>
        <trans-unit id="afd4d25dce087075230a2ce40818eed7a8b7c821" translate="yes" xml:space="preserve">
          <source>True and exponential_avg_factor != 1.0: Mean must be a &lt;code&gt;Tensor&lt;/code&gt; of the same shape as scale containing the exponential running mean.</source>
          <target state="translated">True和exponential_avg_factor！= 1.0：均值必须是与包含指数运行均值的标度具有相同形状的 &lt;code&gt;Tensor&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b0904ca48cae02b60d23262abfe2ce903cfefb1e" translate="yes" xml:space="preserve">
          <source>True and exponential_avg_factor == 1.0: Mean must be None. is_training</source>
          <target state="translated">True和exponential_avg_factor ==1.0。平均数必须为None。 is_training</target>
        </trans-unit>
        <trans-unit id="3c2872020c8add34a2cf2fcba50775a603f0d0e2" translate="yes" xml:space="preserve">
          <source>True if &lt;code&gt;v&lt;/code&gt; was created inside the scope, False if not.</source>
          <target state="translated">如果在范围内创建 &lt;code&gt;v&lt;/code&gt; ，则为True，否则为False。</target>
        </trans-unit>
        <trans-unit id="ea6d3038da7212df43580d700019b440b6df39f7" translate="yes" xml:space="preserve">
          <source>True if a GPU device of the requested kind is available.</source>
          <target state="translated">如果请求的GPU设备可用,则为真。</target>
        </trans-unit>
        <trans-unit id="0182552244a71267208d89b4ec5f7f593b52459d" translate="yes" xml:space="preserve">
          <source>True if a Tensor of the &lt;code&gt;other&lt;/code&gt;&lt;code&gt;DType&lt;/code&gt; will be implicitly converted to this &lt;code&gt;DType&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;other&lt;/code&gt; &lt;code&gt;DType&lt;/code&gt; 的张量将隐式转换为此 &lt;code&gt;DType&lt;/code&gt; ,则为True。</target>
        </trans-unit>
        <trans-unit id="d65dfa1082053356f76fa7e32f60ffc009e106a5" translate="yes" xml:space="preserve">
          <source>True if a stop was requested.</source>
          <target state="translated">如果要求停止,则为真。</target>
        </trans-unit>
        <trans-unit id="fba54694bed7e972eba04ddb43445b6835518a56" translate="yes" xml:space="preserve">
          <source>True if inside a &lt;code&gt;with strategy.scope():&lt;/code&gt;.</source>
          <target state="translated">如果在 &lt;code&gt;with strategy.scope():&lt;/code&gt; 中,则为true 。</target>
        </trans-unit>
        <trans-unit id="0d356c240016a0c32cb15a7f51543e8af63f4aad" translate="yes" xml:space="preserve">
          <source>True if spec_or_tensor is compatible with self.</source>
          <target state="translated">如果spec_or_tensor与self兼容,则为true。</target>
        </trans-unit>
        <trans-unit id="f031713f493f44a87f541097d9a6c4633a7c4174" translate="yes" xml:space="preserve">
          <source>True if the Coordinator is told stop, False if the timeout expired.</source>
          <target state="translated">如果告诉协调员停止,则为真;如果超时,则为假。</target>
        </trans-unit>
        <trans-unit id="40e3b3363df1be71fab94fcf563145f01a88073c" translate="yes" xml:space="preserve">
          <source>True if the caller can expect that serialized TensorFlow graphs produced can be consumed by programs that are compiled with the TensorFlow library source code after (year, month, day).</source>
          <target state="translated">如果调用者可以期望产生的序列化TensorFlow图可以被(年、月、日)后用TensorFlow库源码编译的程序消耗,则为真。</target>
        </trans-unit>
        <trans-unit id="5bee156ad7270160152bb6dbabfb7781157fb705" translate="yes" xml:space="preserve">
          <source>True if the constructor is being called by one of the factory methods. If false, an exception will be raised.</source>
          <target state="translated">如果构造函数被某个工厂方法调用,则为真。如果为false,则会引发异常。</target>
        </trans-unit>
        <trans-unit id="4faef914883a1a10aa15778c49f35fb98be8732e" translate="yes" xml:space="preserve">
          <source>True if the coordinator was told to stop, False otherwise.</source>
          <target state="translated">如果协调人被叫停,则为真,否则为假。</target>
        </trans-unit>
        <trans-unit id="41bb9eed11faf580e5cf2243f1284401f224c42a" translate="yes" xml:space="preserve">
          <source>True if the difference between the current time and the time of the last trigger exceeds &lt;code&gt;every_secs&lt;/code&gt;, or if the difference between the current step and the last triggered step exceeds &lt;code&gt;every_steps&lt;/code&gt;. False otherwise.</source>
          <target state="translated">如果当前时间和最后一个触发时间之间的时差超过 &lt;code&gt;every_secs&lt;/code&gt; ，或者当前步骤和最后一个触发步骤之间的时差超过every_steps，则为 &lt;code&gt;every_steps&lt;/code&gt; 。否则为假。</target>
        </trans-unit>
        <trans-unit id="bfc13f68b704941e80142d04a3ed7c6629e6c664" translate="yes" xml:space="preserve">
          <source>True if the export directory contains SavedModel files, False otherwise.</source>
          <target state="translated">如果导出目录中包含SavedModel文件,则为true,否则为False。</target>
        </trans-unit>
        <trans-unit id="6a900b62e1edba1c0e27907e12139238da6afb94" translate="yes" xml:space="preserve">
          <source>True if the given node must run on CPU, otherwise False.</source>
          <target state="translated">如果给定节点必须在CPU上运行,则为true,否则为False。</target>
        </trans-unit>
        <trans-unit id="e5ad15e25e43eb5bd9cd393c44c117efba29c6f6" translate="yes" xml:space="preserve">
          <source>True if the path exists, whether it's a file or a directory. False if the path does not exist and there are no filesystem errors.</source>
          <target state="translated">如果路径存在,无论是文件还是目录,则为真。如果路径不存在且没有文件系统错误,则为false。</target>
        </trans-unit>
        <trans-unit id="2817ab94bf3761a7902a64b969f4ce8bf858694f" translate="yes" xml:space="preserve">
          <source>True if the quantization is signed or unsigned.</source>
          <target state="translated">如果量化是有符号或无符号,则为真。</target>
        </trans-unit>
        <trans-unit id="12521f184e32516a21dbe08fda414904f64f4716" translate="yes" xml:space="preserve">
          <source>True if the queue is closed and false if the queue is open.</source>
          <target state="translated">如果队列关闭,则为真,如果队列打开,则为假。</target>
        </trans-unit>
        <trans-unit id="8a6738c31462b3c1cdd66322c6255be278594bf2" translate="yes" xml:space="preserve">
          <source>True if the reader implementation can serialize its state.</source>
          <target state="translated">如果阅读器的实现可以序列化其状态,则为真。</target>
        </trans-unit>
        <trans-unit id="b3d692e2a1a03f9a580cbe6af7887c076366f28a" translate="yes" xml:space="preserve">
          <source>True if the sequence is a not a string and is a collections.abc.Sequence or a dict.</source>
          <target state="translated">如果序列不是字符串,而是集合.abc.Sequence或dict,则为真。</target>
        </trans-unit>
        <trans-unit id="e846d2ffe93240497692f8c335b561f9a3b5ca4b" translate="yes" xml:space="preserve">
          <source>True if this Dimension and &lt;code&gt;other&lt;/code&gt; are compatible.</source>
          <target state="translated">如果此Dimension与 &lt;code&gt;other&lt;/code&gt; 尺寸兼容，则为true 。</target>
        </trans-unit>
        <trans-unit id="1af9f77f297353d13cc2052ce5add84a77143e29" translate="yes" xml:space="preserve">
          <source>True if this graph has been finalized.</source>
          <target state="translated">如果此图已定型,则为真。</target>
        </trans-unit>
        <trans-unit id="13ac567ddd78dbff91bfa15ccdc8c27002125b9d" translate="yes" xml:space="preserve">
          <source>True iff &lt;code&gt;self&lt;/code&gt; is compatible with &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">真正的iff &lt;code&gt;self&lt;/code&gt; 与 &lt;code&gt;other&lt;/code&gt; 兼容。</target>
        </trans-unit>
        <trans-unit id="3ea2041726102a7cb24768511dc474dfc2d2086e" translate="yes" xml:space="preserve">
          <source>True iff at most one predicate is allowed to evaluate to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">如果最多只有一个谓词，则允许将 &lt;code&gt;True&lt;/code&gt; 评估为True。</target>
        </trans-unit>
        <trans-unit id="84ec63366604b21f0a1cd748c6c806b8cc035002" translate="yes" xml:space="preserve">
          <source>True on success, or false if no summary was emitted because no default summary writer was available.</source>
          <target state="translated">成功时为真,如果因为没有默认的摘要编写器而没有发出摘要,则为假。</target>
        </trans-unit>
        <trans-unit id="fc591d62fe392ab5c3a30eaf5e21967f39695530" translate="yes" xml:space="preserve">
          <source>True on success, or false if no summary was written because no default summary writer was available.</source>
          <target state="translated">成功时为真,如果没有写出摘要,则为假,因为没有默认的摘要编写器。</target>
        </trans-unit>
        <trans-unit id="c5ce5b285e7cceb24195ca6aafbc51395c9eec5d" translate="yes" xml:space="preserve">
          <source>True tries extracting the file as an Archive, like tar or zip.</source>
          <target state="translated">True尝试将文件解压缩为一个档案,如tar或zip。</target>
        </trans-unit>
        <trans-unit id="64eb5b8ece15c07598a0a8b6e044d88bd144975f" translate="yes" xml:space="preserve">
          <source>True, False or None. None restores the default behavior.</source>
          <target state="translated">True、False或None。无恢复默认行为。</target>
        </trans-unit>
        <trans-unit id="f16aacc072df96e8b74050ff74d241c633ad1f94" translate="yes" xml:space="preserve">
          <source>True, if the path is a directory; False otherwise</source>
          <target state="translated">如果路径是一个目录,则为true;否则为False。</target>
        </trans-unit>
        <trans-unit id="b14f591e5a76ce927e3d51f5d369ecadec7b9175" translate="yes" xml:space="preserve">
          <source>True, if variables should be casted.</source>
          <target state="translated">True,如果变量应该被铸成。</target>
        </trans-unit>
        <trans-unit id="0398d49e6ea66afbb212b36d04e2616800d64ac3" translate="yes" xml:space="preserve">
          <source>True: executes each operation synchronously.</source>
          <target state="translated">True:同步执行每个操作。</target>
        </trans-unit>
        <trans-unit id="4a2f0a78e05a23faa43333403bbc8e7bb4a127d9" translate="yes" xml:space="preserve">
          <source>TruncateDiv</source>
          <target state="translated">TruncateDiv</target>
        </trans-unit>
        <trans-unit id="88d5fe1e339e14bfa5e83a1a1041f877f78fa654" translate="yes" xml:space="preserve">
          <source>TruncateMod</source>
          <target state="translated">TruncateMod</target>
        </trans-unit>
        <trans-unit id="5b6f413272cd888b5af448b50b30ef69c71b431f" translate="yes" xml:space="preserve">
          <source>TruncatedNormal</source>
          <target state="translated">TruncatedNormal</target>
        </trans-unit>
        <trans-unit id="4d9fd66a7778c40e3dba2083356a599b52f25d88" translate="yes" xml:space="preserve">
          <source>Truncation designates that negative numbers will round fractional quantities toward zero. I.e. -7 / 5 = -1. This matches C semantics but it is different than Python semantics. See &lt;code&gt;FloorDiv&lt;/code&gt; for a division function that matches Python Semantics.</source>
          <target state="translated">截断表示负数会将小数舍入为零。即-7 / 5 = -1。这与C语义匹配，但是与Python语义不同。有关与Python语义匹配的除法功能，请参见 &lt;code&gt;FloorDiv&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="aa758b468b210aad3c052427b42a0c7ac48798f6" translate="yes" xml:space="preserve">
          <source>Tuple in the format used in &lt;a href=&quot;../model#fit&quot;&gt;&lt;code&gt;Model.fit&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">具有&lt;a href=&quot;../model#fit&quot;&gt; &lt;code&gt;Model.fit&lt;/code&gt; 中&lt;/a&gt;使用的格式的元组。</target>
        </trans-unit>
        <trans-unit id="3c85382570b1156fafff522dc38424cdd757186e" translate="yes" xml:space="preserve">
          <source>Tuple of 2 integers, how many zeros to add at the start and end of dim 1.</source>
          <target state="translated">2个整数的元组,在dim 1的开头和结尾加多少个0。</target>
        </trans-unit>
        <trans-unit id="d878205ddde8c656cd0c2ec819ef9a66c6db3326" translate="yes" xml:space="preserve">
          <source>Tuple of 2 integers.</source>
          <target state="translated">2个整数的元组。</target>
        </trans-unit>
        <trans-unit id="9e902bb2fc5cab147bc391e40439b73d1a642c42" translate="yes" xml:space="preserve">
          <source>Tuple of 2 tuples, padding pattern.</source>
          <target state="translated">2个元组的元组,填充模式。</target>
        </trans-unit>
        <trans-unit id="3e0b6b48776191185766960be076280c767a6bcd" translate="yes" xml:space="preserve">
          <source>Tuple of 3 integers, factors by which to downscale (dim1, dim2, dim3). &lt;code&gt;(2, 2, 2)&lt;/code&gt; will halve the size of the 3D input in each dimension.</source>
          <target state="translated">由3个整数组成的元组，用于缩减的因子（dim1，dim2，dim3）。 &lt;code&gt;(2, 2, 2)&lt;/code&gt; 2、2、2 ）将使每个维度中3D输入的大小减半。</target>
        </trans-unit>
        <trans-unit id="a2a3aa7cb19c709e2aeca6ab2c5fbdce9372f47c" translate="yes" xml:space="preserve">
          <source>Tuple of 3 tuples, padding pattern.</source>
          <target state="translated">3个元组的元组,填充模式。</target>
        </trans-unit>
        <trans-unit id="3b36302f1d8a1cd0c40fa6d16b6fe2aaaa964f68" translate="yes" xml:space="preserve">
          <source>Tuple of Numpy arrays: &lt;code&gt;(x_train, y_train), (x_test, y_test)&lt;/code&gt;.</source>
          <target state="translated">Numpy数组的元组： &lt;code&gt;(x_train, y_train), (x_test, y_test)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e5fcb3189bbbad2089b9134f58fe7fca941f8a70" translate="yes" xml:space="preserve">
          <source>Tuple of integers &lt;code&gt;(height, width)&lt;/code&gt;, defaults to &lt;code&gt;(256, 256)&lt;/code&gt;. The dimensions to which all images found will be resized.</source>
          <target state="translated">整数 &lt;code&gt;(height, width)&lt;/code&gt; 元组，默认为 &lt;code&gt;(256, 256)&lt;/code&gt; 。找到的所有图像的尺寸将被调整为尺寸。</target>
        </trans-unit>
        <trans-unit id="80e3a7c1e14386c2b20631926430b4d65ddfff1f" translate="yes" xml:space="preserve">
          <source>Tuple of integers representing (min, max) range values for all arrays without a specified range. Intended for experimenting with quantization via &quot;dummy quantization&quot;. (default None)</source>
          <target state="translated">整数元组,代表所有没有指定范围的数组的(最小,最大)范围值。用于通过 &quot;虚拟量化 &quot;进行量化实验。(默认为无)</target>
        </trans-unit>
        <trans-unit id="7e26526855fdb022129c7cbbd982fef1427fddb1" translate="yes" xml:space="preserve">
          <source>Tuple of integers, shape of returned Keras variable.</source>
          <target state="translated">整数的元组,返回Keras变量的形状。</target>
        </trans-unit>
        <trans-unit id="b8ec74a912c043af498fdebd5ada0cb4d060e6f5" translate="yes" xml:space="preserve">
          <source>Tuple of integers. Permutation pattern does not include the samples dimension. Indexing starts at 1. For instance, &lt;code&gt;(2, 1)&lt;/code&gt; permutes the first and second dimensions of the input.</source>
          <target state="translated">整数元组。排列模式不包括样本维。索引从1开始。例如， &lt;code&gt;(2, 1)&lt;/code&gt; 置换输入的第一维和第二维。</target>
        </trans-unit>
        <trans-unit id="4d1f41be26f53eaa6f970fca60630a6f21855e44" translate="yes" xml:space="preserve">
          <source>Tuple of strings representing input tensor names and list of integers representing input shapes (e.g., [(&quot;foo&quot; : [1, 16, 16, 3])]). Use only when graph cannot be loaded into TensorFlow and when &lt;code&gt;input_tensors&lt;/code&gt; and &lt;code&gt;output_tensors&lt;/code&gt; are None. (default None)</source>
          <target state="translated">表示输入张量名称的字符串的元组和表示输入形状的整数列表（例如[[&amp;ldquo; foo&amp;rdquo;：[1，16，16，3]）]）。仅在无法将图形加载到TensorFlow中且 &lt;code&gt;input_tensors&lt;/code&gt; 和 &lt;code&gt;output_tensors&lt;/code&gt; 为None时使用。（默认为无）</target>
        </trans-unit>
        <trans-unit id="4cbc0064b79f9aa3d7ea238c4e18ad22b86f3acb" translate="yes" xml:space="preserve">
          <source>Tuple or list of integers with target dimensions, or single integer. The sizes of &lt;code&gt;x.shape[axes[0]]&lt;/code&gt; and &lt;code&gt;y.shape[axes[1]]&lt;/code&gt; should be equal.</source>
          <target state="translated">具有目标尺寸的元组或整数列表，或单个整数。的尺寸 &lt;code&gt;x.shape[axes[0]]&lt;/code&gt; 和 &lt;code&gt;y.shape[axes[1]]&lt;/code&gt; 应该相等。</target>
        </trans-unit>
        <trans-unit id="082793c3bba9dd17881797f57bdfd7fbb926fad6" translate="yes" xml:space="preserve">
          <source>Tuple or list of integers, shape of returned Keras variable</source>
          <target state="translated">整数的元组或列表,返回的Keras变量的形状。</target>
        </trans-unit>
        <trans-unit id="4531a58c8012cfb70552795a6d42bd391095a246" translate="yes" xml:space="preserve">
          <source>Tuple or list of two floats. Range for picking a brightness shift value from.</source>
          <target state="translated">Tuple或两个浮点数的列表。用于选择亮度移动值的范围。</target>
        </trans-unit>
        <trans-unit id="fcd91650680c7044999f313873fe057ab50db675" translate="yes" xml:space="preserve">
          <source>Tuple or list with positional arguments for &lt;code&gt;fn&lt;/code&gt;.</source>
          <target state="translated">具有 &lt;code&gt;fn&lt;/code&gt; 位置参数的元组或列表。</target>
        </trans-unit>
        <trans-unit id="b33b3b3fb1f8b255b7161c80b27363774e8f88ed" translate="yes" xml:space="preserve">
          <source>Tuple or list. Additional positional arguments to pass to &lt;code&gt;fn()&lt;/code&gt;.</source>
          <target state="translated">元组或列表。传递给 &lt;code&gt;fn()&lt;/code&gt; 的其他位置参数。</target>
        </trans-unit>
        <trans-unit id="2613ea839d069df0d5c8d9c5f47854631d09fc20" translate="yes" xml:space="preserve">
          <source>Tuple used by LSTM Cells for &lt;code&gt;state_size&lt;/code&gt;, &lt;code&gt;zero_state&lt;/code&gt;, and output state.</source>
          <target state="translated">LSTM单元用于 &lt;code&gt;state_size&lt;/code&gt; ， &lt;code&gt;zero_state&lt;/code&gt; 和输出状态的元组。</target>
        </trans-unit>
        <trans-unit id="5b48f912499203c94d01ac7a5a26aea7a05b21fa" translate="yes" xml:space="preserve">
          <source>Turn a nD tensor into a 2D tensor with same 0th dimension.</source>
          <target state="translated">把一个nD张量变成一个相同0维的2D张量。</target>
        </trans-unit>
        <trans-unit id="84c0c370c16aa336bb6f7381c737e4c2aba96f9e" translate="yes" xml:space="preserve">
          <source>Turns positive integers (indexes) into dense vectors of fixed size.</source>
          <target state="translated">将正整数(索引)变成固定大小的密集向量。</target>
        </trans-unit>
        <trans-unit id="2c47d5c1924debfd2ddf97975d083f8b91fc55b1" translate="yes" xml:space="preserve">
          <source>Turns the serialized form of a Keras object back into an actual object.</source>
          <target state="translated">将Keras对象的序列化形式转回实际对象。</target>
        </trans-unit>
        <trans-unit id="0312b13c624e55c7e9bb5fdcfe0c38b37f8b9c61" translate="yes" xml:space="preserve">
          <source>Tutorials and examples can be found in: &lt;a href=&quot;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/python_api.md&quot;&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/python_api.md&lt;/a&gt;</source>
          <target state="translated">教程和示例可在以下位置找到：&lt;a href=&quot;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/python_api.md&quot;&gt;https&lt;/a&gt; : //github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/python_api.md</target>
        </trans-unit>
        <trans-unit id="e1970692bb6c07bd87f9682c04edd6a7dc8b64ed" translate="yes" xml:space="preserve">
          <source>Tutorials and examples can be found in: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/README.md</source>
          <target state="translated">教程和例子可以在:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/README.md。</target>
        </trans-unit>
        <trans-unit id="5392c950bdde4be7e5f5b8fdc6a1ca5f21e905cf" translate="yes" xml:space="preserve">
          <source>Twitter</source>
          <target state="translated">Twitter</target>
        </trans-unit>
        <trans-unit id="698a7f116a9d0145fa7461b0cd8612ceab6c65c3" translate="yes" xml:space="preserve">
          <source>Two 2-d numpy arrays representing the theoretical and numerical Jacobian for dy/dx. Each has &quot;x_size&quot; rows and &quot;y_size&quot; columns where &quot;x_size&quot; is the number of elements in x and &quot;y_size&quot; is the number of elements in y. If x is a list, returns a list of two numpy arrays.</source>
          <target state="translated">两个二维的numpy数组代表dy/dx的理论和数值雅各布。每个数组都有 &quot;x_size &quot;行和 &quot;y_size &quot;列,其中 &quot;x_size &quot;是x中的元素数,&quot;y_size &quot;是y中的元素数。如果x是一个列表,返回两个numpy数组的列表。</target>
        </trans-unit>
        <trans-unit id="ba15ec3492dd4bf582b4e766b61d26647d5ffcdc" translate="yes" xml:space="preserve">
          <source>Two &lt;a href=&quot;../../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt; objects of type &lt;code&gt;bool&lt;/code&gt; of the same shape. In this case, the result will be the element-wise logical AND of the two input tensors.</source>
          <target state="translated">两个&lt;a href=&quot;../../tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt;对象类型的 &lt;code&gt;bool&lt;/code&gt; 形状相同。在这种情况下，结果将是两个输入张量的按元素逻辑与。</target>
        </trans-unit>
        <trans-unit id="e2f8646d6f565d2ad7fc58a15043d6c2087ee7b6" translate="yes" xml:space="preserve">
          <source>Two &lt;a href=&quot;../../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt; objects of type &lt;code&gt;bool&lt;/code&gt; of the same shape. In this case, the result will be the element-wise logical XOR of the two input tensors.</source>
          <target state="translated">两个&lt;a href=&quot;../../tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt;对象类型的 &lt;code&gt;bool&lt;/code&gt; 形状相同。在这种情况下，结果将是两个输入张量的按元素逻辑异或。</target>
        </trans-unit>
        <trans-unit id="0e476c2f4924caa7e379cac892bc43188804bebb" translate="yes" xml:space="preserve">
          <source>Two &lt;a href=&quot;../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt; objects of type &lt;code&gt;bool&lt;/code&gt; of the same shape. In this case, the result will be the element-wise logical AND of the two input tensors.</source>
          <target state="translated">两个&lt;a href=&quot;../tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt;对象类型的 &lt;code&gt;bool&lt;/code&gt; 形状相同。在这种情况下，结果将是两个输入张量的按元素逻辑与。</target>
        </trans-unit>
        <trans-unit id="34ab6873a8334c48d1673369412e608cd7417e10" translate="yes" xml:space="preserve">
          <source>Two &lt;a href=&quot;../tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt; objects of type &lt;code&gt;bool&lt;/code&gt; of the same shape. In this case, the result will be the element-wise logical XOR of the two input tensors.</source>
          <target state="translated">两个&lt;a href=&quot;../tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt;对象类型的 &lt;code&gt;bool&lt;/code&gt; 形状相同。在这种情况下，结果将是两个输入张量的按元素逻辑异或。</target>
        </trans-unit>
        <trans-unit id="31988105748d2b29509a18eb03ac4bcf2e98951c" translate="yes" xml:space="preserve">
          <source>Two &lt;a href=&quot;tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt; objects of type &lt;code&gt;bool&lt;/code&gt; of the same shape. In this case, the result will be the element-wise logical AND of the two input tensors.</source>
          <target state="translated">两个&lt;a href=&quot;tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt;对象类型的 &lt;code&gt;bool&lt;/code&gt; 形状相同。在这种情况下，结果将是两个输入张量的按元素逻辑与。</target>
        </trans-unit>
        <trans-unit id="322bef22d5ff10923c851f4887f9752d9171cff1" translate="yes" xml:space="preserve">
          <source>Two &lt;a href=&quot;tensor&quot;&gt;&lt;code&gt;tf.Tensor&lt;/code&gt;&lt;/a&gt; objects of type &lt;code&gt;bool&lt;/code&gt; of the same shape. In this case, the result will be the element-wise logical XOR of the two input tensors.</source>
          <target state="translated">两个&lt;a href=&quot;tensor&quot;&gt; &lt;code&gt;tf.Tensor&lt;/code&gt; &lt;/a&gt;对象类型的 &lt;code&gt;bool&lt;/code&gt; 形状相同。在这种情况下，结果将是两个输入张量的按元素逻辑异或。</target>
        </trans-unit>
        <trans-unit id="e61ae5fa9a8a338e450ea4521d3c90e37b3fce55" translate="yes" xml:space="preserve">
          <source>Two &lt;code&gt;Tensor&lt;/code&gt; objects: &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;variance&lt;/code&gt;.</source>
          <target state="translated">两个 &lt;code&gt;Tensor&lt;/code&gt; 对象： &lt;code&gt;mean&lt;/code&gt; 和 &lt;code&gt;variance&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="18f9e0ca8006912df25de0a98edea35700ad81f2" translate="yes" xml:space="preserve">
          <source>Two accumulation steps are required: 1) the accumulation of gradients squared, 2) the accumulation of updates squared.</source>
          <target state="translated">需要两个积累步骤。1)积累梯度的平方,2)积累更新的平方。</target>
        </trans-unit>
        <trans-unit id="a1d8ed408945a366f45477724e8c9686614ca2d7" translate="yes" xml:space="preserve">
          <source>Two different templates are guaranteed to be unique, unless you reenter the same variable scope as the initial definition of a template and redefine it. An examples of this exception:</source>
          <target state="translated">两个不同的模板保证是唯一的,除非你重新进入与模板初始定义相同的变量范围并重新定义它。这种例外的例子。</target>
        </trans-unit>
        <trans-unit id="641506872a34040c078fdb14eeef6b25c62c6fc9" translate="yes" xml:space="preserve">
          <source>Two generators are independent of each other in the sense that the random-number streams they generate don't have statistically detectable correlations. The new generators are also independent of the old one. The old generator's state will be changed (like other random-number generating methods), so two calls of &lt;code&gt;split&lt;/code&gt; will return different new generators.</source>
          <target state="translated">两个生成器彼此独立，因为它们生成的随机数流没有统计上可检测的相关性。新发电机也独立于旧发电机。旧生成器的状态将被更改（就像其他随机数生成方法一样），因此两次 &lt;code&gt;split&lt;/code&gt; 调用将返回不同的新生成器。</target>
        </trans-unit>
        <trans-unit id="609ab801a6e4c68686c97569df58fbb61201051d" translate="yes" xml:space="preserve">
          <source>Two known Dimensions are compatible if they have the same value. An unknown Dimension is compatible with all other Dimensions.</source>
          <target state="translated">如果两个已知的尺寸具有相同的值,则它们是兼容的。未知尺寸与所有其他尺寸兼容。</target>
        </trans-unit>
        <trans-unit id="6133911db30a0e3197c09499734a8919b0cbd3bf" translate="yes" xml:space="preserve">
          <source>Two or more words may be assigned to the same index, due to possible collisions by the hashing function. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Birthday_problem#Probability_table&quot;&gt;probability&lt;/a&gt; of a collision is in relation to the dimension of the hashing space and the number of distinct objects.</source>
          <target state="translated">由于哈希函数可能发生冲突，可能会将两个或多个单词分配给同一索引。冲突的&lt;a href=&quot;https://en.wikipedia.org/wiki/Birthday_problem#Probability_table&quot;&gt;可能性&lt;/a&gt;与散列空间的维数和不同对象的数量有关。</target>
        </trans-unit>
        <trans-unit id="aa46f48d3d26a8be473e57169e56d3f2b7f57692" translate="yes" xml:space="preserve">
          <source>Two possibly-partially-defined shapes are compatible if there exists a fully-defined shape that both shapes can represent. Thus, compatibility allows the shape inference code to reason about partially-defined shapes. For example:</source>
          <target state="translated">如果存在一个完全定义的形状,这两个可能是部分定义的形状是兼容的,这两个形状可以表示。因此,兼容性允许形状推理代码对部分定义的形状进行推理。例如</target>
        </trans-unit>
        <trans-unit id="b556440bce1c841c1d10f79b38663d9dfe8ce6f6" translate="yes" xml:space="preserve">
          <source>Two single elements of type &lt;code&gt;bool&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;bool&lt;/code&gt; 型的两个单个元素</target>
        </trans-unit>
        <trans-unit id="8b0a6ae6ca7d62abe9867da93517fe68a65fffe7" translate="yes" xml:space="preserve">
          <source>Two static instances exist in the distributions library, signifying one of two possible properties for samples from a distribution:</source>
          <target state="translated">分布库中存在两个静态实例,表示分布样本的两种可能属性之一。</target>
        </trans-unit>
        <trans-unit id="304880baecfd0af0b3d1da16c178a413930af3d7" translate="yes" xml:space="preserve">
          <source>Two tensors are considered compatible if they have the same dtype and their shapes are compatible (see &lt;a href=&quot;tensorshape#is_compatible_with&quot;&gt;&lt;code&gt;tf.TensorShape.is_compatible_with&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">如果两个张量具有相同的dtype并且它们的形状兼容（请参阅&lt;a href=&quot;tensorshape#is_compatible_with&quot;&gt; &lt;code&gt;tf.TensorShape.is_compatible_with&lt;/code&gt; &lt;/a&gt;），则认为它们是兼容的。</target>
        </trans-unit>
        <trans-unit id="6c808e735cf1d9acc205c12bc34a1db874936c96" translate="yes" xml:space="preserve">
          <source>Two tensors: &lt;code&gt;weighted_mean&lt;/code&gt; and &lt;code&gt;weighted_variance&lt;/code&gt;.</source>
          <target state="translated">两个张量： &lt;code&gt;weighted_mean&lt;/code&gt; 和 &lt;code&gt;weighted_variance&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="197f1a152813e8754c3ab4ada0aee8e8eb0ab0e1" translate="yes" xml:space="preserve">
          <source>Type collections</source>
          <target state="translated">收藏品类型</target>
        </trans-unit>
        <trans-unit id="70190875a976d04b6dbbdb76a76b8abab15ed20d" translate="yes" xml:space="preserve">
          <source>Type of reduction to apply to loss.</source>
          <target state="translated">适用于损失的扣减类型:</target>
        </trans-unit>
        <trans-unit id="580ea2f11b5081d9834cac3a14574bfc02323a10" translate="yes" xml:space="preserve">
          <source>Type of the new or existing variable (defaults to &lt;code&gt;DT_FLOAT&lt;/code&gt;).</source>
          <target state="translated">新变量或现有变量的类型（默认为 &lt;code&gt;DT_FLOAT&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="eda6e626972cf9e8cb577094d8e71478f4d87dc1" translate="yes" xml:space="preserve">
          <source>Type of the variables. Ignored if &lt;code&gt;initializer&lt;/code&gt; is a &lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="translated">变量的类型。如果 &lt;code&gt;initializer&lt;/code&gt; 是 &lt;code&gt;Tensor&lt;/code&gt; ,则忽略该方法。</target>
        </trans-unit>
        <trans-unit id="8cb8d838a08c5043e95b8affed6f234ffe7a8fca" translate="yes" xml:space="preserve">
          <source>Type of weights, such as &lt;a href=&quot;../../tf#float32&quot;&gt;&lt;code&gt;tf.float32&lt;/code&gt;&lt;/a&gt;. Only float and integer weights are supported.</source>
          <target state="translated">权重的类型，例如&lt;a href=&quot;../../tf#float32&quot;&gt; &lt;code&gt;tf.float32&lt;/code&gt; &lt;/a&gt;。仅支持浮点和整数权重。</target>
        </trans-unit>
        <trans-unit id="89bccb1027fad55abb521d6f7f2c0ac6a074c8f5" translate="yes" xml:space="preserve">
          <source>Type specification for &lt;a href=&quot;dataset&quot;&gt;&lt;code&gt;tf.data.Dataset&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;dataset&quot;&gt; &lt;code&gt;tf.data.Dataset&lt;/code&gt; 的&lt;/a&gt;类型规范。</target>
        </trans-unit>
        <trans-unit id="712d9ffcd0e88abdb9eab9798af5f280ec9181bd" translate="yes" xml:space="preserve">
          <source>Type specification for &lt;a href=&quot;experimental/optional&quot;&gt;&lt;code&gt;tf.experimental.Optional&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;experimental/optional&quot;&gt; &lt;code&gt;tf.experimental.Optional&lt;/code&gt; 的&lt;/a&gt;类型规范。</target>
        </trans-unit>
        <trans-unit id="546c3b422414d2acfe0b7b6df33ddda2e98f189c" translate="yes" xml:space="preserve">
          <source>Type specification for &lt;a href=&quot;iterator&quot;&gt;&lt;code&gt;tf.data.Iterator&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;iterator&quot;&gt; &lt;code&gt;tf.data.Iterator&lt;/code&gt; 的&lt;/a&gt;类型规范。</target>
        </trans-unit>
        <trans-unit id="7ffc6a6652502645af83819deceaf071d1bee7ae" translate="yes" xml:space="preserve">
          <source>Type specification for a &lt;a href=&quot;indexedslices&quot;&gt;&lt;code&gt;tf.IndexedSlices&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;indexedslices&quot;&gt; &lt;code&gt;tf.IndexedSlices&lt;/code&gt; 的&lt;/a&gt;类型规范。</target>
        </trans-unit>
        <trans-unit id="db38e64c6790f1fa6a4046e1608589e1baafdb4f" translate="yes" xml:space="preserve">
          <source>Type specification for a &lt;a href=&quot;raggedtensor&quot;&gt;&lt;code&gt;tf.RaggedTensor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;raggedtensor&quot;&gt; &lt;code&gt;tf.RaggedTensor&lt;/code&gt; 的&lt;/a&gt;类型规范。</target>
        </trans-unit>
        <trans-unit id="506637460ca723fc98125655b0c45f16d689f183" translate="yes" xml:space="preserve">
          <source>Type specification for a &lt;a href=&quot;sparse/sparsetensor&quot;&gt;&lt;code&gt;tf.SparseTensor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;sparse/sparsetensor&quot;&gt; &lt;code&gt;tf.SparseTensor&lt;/code&gt; 的&lt;/a&gt;类型规范。</target>
        </trans-unit>
        <trans-unit id="f8dc08024793f24e03dca43c23eba2de392c0936" translate="yes" xml:space="preserve">
          <source>Type specification for a &lt;a href=&quot;sparse/sparsetensor&quot;&gt;&lt;code&gt;tf.sparse.SparseTensor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;sparse/sparsetensor&quot;&gt; &lt;code&gt;tf.sparse.SparseTensor&lt;/code&gt; 的&lt;/a&gt;类型规范。</target>
        </trans-unit>
        <trans-unit id="097420656a3bb835a98ade8d3c1d8d6e161e4abc" translate="yes" xml:space="preserve">
          <source>Type specification for a &lt;a href=&quot;tensorarray&quot;&gt;&lt;code&gt;tf.TensorArray&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;tensorarray&quot;&gt; &lt;code&gt;tf.TensorArray&lt;/code&gt; 的&lt;/a&gt;类型规范。</target>
        </trans-unit>
        <trans-unit id="0fa80254c7cf732908a6ef92e8bcb16a8cbc3031" translate="yes" xml:space="preserve">
          <source>TypeError if &lt;code&gt;cluster&lt;/code&gt; is not a dictionary or &lt;code&gt;ClusterDef&lt;/code&gt; protocol buffer, or if &lt;code&gt;ps_strategy&lt;/code&gt; is provided but not a callable.</source>
          <target state="translated">如果 &lt;code&gt;cluster&lt;/code&gt; 不是字典或 &lt;code&gt;ClusterDef&lt;/code&gt; 协议缓冲区，或者提供了 &lt;code&gt;ps_strategy&lt;/code&gt; 但不是可调用的，则为TypeError。</target>
        </trans-unit>
        <trans-unit id="7a7a3cb39cba799635ae97a5b59c3c35fb2a8e05" translate="yes" xml:space="preserve">
          <source>TypeError.</source>
          <target state="translated">TypeError.</target>
        </trans-unit>
        <trans-unit id="5bdcc3d1ee1c7b80e26206fa1de63397c5a2add9" translate="yes" xml:space="preserve">
          <source>TypeError: If the slice indices aren't int, slice, ellipsis, tf.newaxis or int32/int64 tensors.</source>
          <target state="translated">类型错误。如果切片指数不是int,slice,ellipsis,tf.newaxis或int32/int64 tensors.</target>
        </trans-unit>
        <trans-unit id="0d6c4c2eeadfc7d013e2b2fb8940e520aa96954c" translate="yes" xml:space="preserve">
          <source>Types I, II, III and IV are supported. Type I is implemented using a length &lt;code&gt;2N&lt;/code&gt; padded &lt;a href=&quot;rfft&quot;&gt;&lt;code&gt;tf.signal.rfft&lt;/code&gt;&lt;/a&gt;. Type II is implemented using a length &lt;code&gt;2N&lt;/code&gt; padded &lt;a href=&quot;rfft&quot;&gt;&lt;code&gt;tf.signal.rfft&lt;/code&gt;&lt;/a&gt;, as described here: &lt;a href=&quot;https://dsp.stackexchange.com/a/10606&quot;&gt;Type 2 DCT using 2N FFT padded (Makhoul)&lt;/a&gt;. Type III is a fairly straightforward inverse of Type II (i.e. using a length &lt;code&gt;2N&lt;/code&gt; padded &lt;a href=&quot;irfft&quot;&gt;&lt;code&gt;tf.signal.irfft&lt;/code&gt;&lt;/a&gt;). Type IV is calculated through 2N length DCT2 of padded signal and picking the odd indices.</source>
          <target state="translated">支持类型I，II，III和IV。I型是用长度来实现 &lt;code&gt;2N&lt;/code&gt; 填充&lt;a href=&quot;rfft&quot;&gt; &lt;code&gt;tf.signal.rfft&lt;/code&gt; &lt;/a&gt;。使用长度 &lt;code&gt;2N&lt;/code&gt; 填充的&lt;a href=&quot;rfft&quot;&gt; &lt;code&gt;tf.signal.rfft&lt;/code&gt; &lt;/a&gt;实现类型II ，如下所述：&lt;a href=&quot;https://dsp.stackexchange.com/a/10606&quot;&gt;使用2N FFT填充的2型DCT（Makhoul）&lt;/a&gt;。类型III是类型II的一种非常简单的逆（即，使用长度为 &lt;code&gt;2N&lt;/code&gt; 的填充&lt;a href=&quot;irfft&quot;&gt; &lt;code&gt;tf.signal.irfft&lt;/code&gt; &lt;/a&gt;）。通过填充信号的2N长度DCT2并选择奇数索引来计算IV型。</target>
        </trans-unit>
        <trans-unit id="c9e719623f3a5418b899a4ace2cf4d169670e417" translate="yes" xml:space="preserve">
          <source>Types of loss reduction.</source>
          <target state="translated">减少损失的类型;</target>
        </trans-unit>
        <trans-unit id="aa3c047f418cf4b489027ae070f266970331772f" translate="yes" xml:space="preserve">
          <source>Typical usage example:</source>
          <target state="translated">典型的使用实例。</target>
        </trans-unit>
        <trans-unit id="f86aad613fa5c3ee99a3b5451654d3e3136022f7" translate="yes" xml:space="preserve">
          <source>Typical usage for the &lt;code&gt;SavedModelBuilder&lt;/code&gt;:</source>
          <target state="translated">&lt;code&gt;SavedModelBuilder&lt;/code&gt; 的典型用法：</target>
        </trans-unit>
        <trans-unit id="1dc96cc06e4c88360c7be195760a73e5c44fb2f7" translate="yes" xml:space="preserve">
          <source>Typical usage of this strategy could be testing your code with the tf.distribute.Strategy API before switching to other strategies which actually distribute to multiple devices/machines.</source>
          <target state="translated">这个策略的典型用法可以是在切换到其他策略之前,先用tf.distribution.Strategy API测试你的代码,因为这些策略实际上是分发到多个设备/机器上的。</target>
        </trans-unit>
        <trans-unit id="9f433de3a09f2cf5cd13a827c57725457d725e39" translate="yes" xml:space="preserve">
          <source>Typical usage:</source>
          <target state="translated">典型用途:</target>
        </trans-unit>
        <trans-unit id="86f2b70049641c1127233f5a7f23b12cf7d72298" translate="yes" xml:space="preserve">
          <source>Typical usages of the &lt;code&gt;MethodNameUpdater&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;MethodNameUpdater&lt;/code&gt; 的典型用法</target>
        </trans-unit>
        <trans-unit id="0271771c7069a116c3373c269a63be4c6aca6198" translate="yes" xml:space="preserve">
          <source>Typical users will use one of the more specialized DEFINE_xxx functions, such as DEFINE_string or DEFINE_integer. But developers who need to create Flag objects themselves should use this function to register their flags.</source>
          <target state="translated">典型的用户会使用一个更专业的DEFINE_xxx函数,比如DEFINE_string或DEFINE_integer。但是需要自己创建Flag对象的开发者应该使用这个函数来注册他们的Flag。</target>
        </trans-unit>
        <trans-unit id="2637b9cc80067f35b6391fac2edcbf9be59d7ab4" translate="yes" xml:space="preserve">
          <source>Typically only used in a cross-replica context:</source>
          <target state="translated">通常只在交叉复制的情况下使用。</target>
        </trans-unit>
        <trans-unit id="770e50ed1c5b312d1ae6d0b8292c12f6aec34dd4" translate="yes" xml:space="preserve">
          <source>Typically this function is used to convert from TensorFlow GraphDef to TFLite. Conversion can be customized by providing arguments that are forwarded to &lt;code&gt;build_toco_convert_protos&lt;/code&gt; (see documentation for details). This function has been deprecated. Please use &lt;a href=&quot;../../../lite/tfliteconverter&quot;&gt;&lt;code&gt;lite.TFLiteConverter&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">通常，此函数用于将TensorFlow GraphDef转换为TFLite。可以通过提供转发到 &lt;code&gt;build_toco_convert_protos&lt;/code&gt; 的参数来定制转换（有关详细信息，请参阅文档）。不建议使用此功能。请改用&lt;a href=&quot;../../../lite/tfliteconverter&quot;&gt; &lt;code&gt;lite.TFLiteConverter&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="3fe4dd1700ccac29f3d0ae3fa02b865b65b1c45f" translate="yes" xml:space="preserve">
          <source>Typically this function is used to convert from TensorFlow GraphDef to TFLite. Conversion can be customized by providing arguments that are forwarded to &lt;code&gt;build_toco_convert_protos&lt;/code&gt; (see documentation for details). This function has been deprecated. Please use &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter&quot;&gt;&lt;code&gt;lite.TFLiteConverter&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">通常，此函数用于将TensorFlow GraphDef转换为TFLite。可以通过提供转发到 &lt;code&gt;build_toco_convert_protos&lt;/code&gt; 的参数来定制转换（有关详细信息，请参阅文档）。此功能已被弃用。请改用&lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter&quot;&gt; &lt;code&gt;lite.TFLiteConverter&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="9f937cb551b8e8625cac21b8a54df64070d57507" translate="yes" xml:space="preserve">
          <source>Typically, constructing a file writer creates a new event file in &lt;code&gt;logdir&lt;/code&gt;. This event file will contain &lt;code&gt;Event&lt;/code&gt; protocol buffers constructed when you call one of the following functions: &lt;code&gt;add_summary()&lt;/code&gt;, &lt;code&gt;add_session_log()&lt;/code&gt;, &lt;code&gt;add_event()&lt;/code&gt;, or &lt;code&gt;add_graph()&lt;/code&gt;.</source>
          <target state="translated">通常，构造文件 &lt;code&gt;logdir&lt;/code&gt; 会在logdir中创建一个新的事件文件。当您调用以下函数之一时，此事件文件将包含 &lt;code&gt;Event&lt;/code&gt; 协议缓冲区： &lt;code&gt;add_summary()&lt;/code&gt; ， &lt;code&gt;add_session_log()&lt;/code&gt; ， &lt;code&gt;add_event()&lt;/code&gt; 或 &lt;code&gt;add_graph()&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="aff7b1a518fb0d16f85d64959ece4d247dd8cf1c" translate="yes" xml:space="preserve">
          <source>Typically, different numerical approximations can be used for the log survival function, which are more accurate than &lt;code&gt;1 - cdf(x)&lt;/code&gt; when &lt;code&gt;x &amp;gt;&amp;gt; 1&lt;/code&gt;.</source>
          <target state="translated">通常，可以将不同的数值近似值用于对数生存函数，当 &lt;code&gt;x &amp;gt;&amp;gt; 1&lt;/code&gt; 时，它们比 &lt;code&gt;1 - cdf(x)&lt;/code&gt; 更精确。</target>
        </trans-unit>
        <trans-unit id="dc11d475cf6eeccc4ccad2799b660daf47f6e97d" translate="yes" xml:space="preserve">
          <source>Typically, this is used for contiguous ranges of integer indexes, but it doesn't have to be. This might be inefficient, however, if many of IDs are unused. Consider &lt;code&gt;categorical_column_with_hash_bucket&lt;/code&gt; in that case.</source>
          <target state="translated">通常，它用于整数索引的连续范围，但不一定如此。但是，如果许多ID未使用，则效率可能较低。在这种情况下，请考虑 &lt;code&gt;categorical_column_with_hash_bucket&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="642d93bf7af9445a97651535e05ec594d4e074d2" translate="yes" xml:space="preserve">
          <source>Typically, this method directly controls &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt; settings, and delegates the actual evaluation logic to &lt;a href=&quot;../model#predict_step&quot;&gt;&lt;code&gt;Model.predict_step&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">通常，此方法直接控制&lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../../distribute/strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; &lt;/a&gt;设置，并将实际的评估逻辑委托给&lt;a href=&quot;../model#predict_step&quot;&gt; &lt;code&gt;Model.predict_step&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e09b882e09baa741494589bf82363a386761e573" translate="yes" xml:space="preserve">
          <source>Typically, this method directly controls &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt; settings, and delegates the actual evaluation logic to &lt;a href=&quot;../model#test_step&quot;&gt;&lt;code&gt;Model.test_step&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">通常，此方法直接控制&lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../../distribute/strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; &lt;/a&gt;设置，并将实际的评估逻辑委托给&lt;a href=&quot;../model#test_step&quot;&gt; &lt;code&gt;Model.test_step&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="b89bd069f03bea0bc8a7420d63726498425c37ea" translate="yes" xml:space="preserve">
          <source>Typically, this method directly controls &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt; settings, and delegates the actual training logic to &lt;a href=&quot;../model#train_step&quot;&gt;&lt;code&gt;Model.train_step&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">通常，此方法直接控制&lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../../distribute/strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; &lt;/a&gt;设置，并将实际的训练逻辑委托给&lt;a href=&quot;../model#train_step&quot;&gt; &lt;code&gt;Model.train_step&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6a9ed424e025572fe99563e115ac58ef9f0eecef" translate="yes" xml:space="preserve">
          <source>Typically, this method directly controls &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt; settings, and delegates the actual evaluation logic to &lt;a href=&quot;model#predict_step&quot;&gt;&lt;code&gt;Model.predict_step&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">通常，此方法直接控制&lt;a href=&quot;../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../distribute/strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; &lt;/a&gt;设置，并将实际的评估逻辑委托给&lt;a href=&quot;model#predict_step&quot;&gt; &lt;code&gt;Model.predict_step&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="88e682fbb9f04bb6f808d62f5cfd3cba250f9355" translate="yes" xml:space="preserve">
          <source>Typically, this method directly controls &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt; settings, and delegates the actual evaluation logic to &lt;a href=&quot;model#test_step&quot;&gt;&lt;code&gt;Model.test_step&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">通常，此方法直接控制&lt;a href=&quot;../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../distribute/strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; &lt;/a&gt;设置，并将实际的评估逻辑委托给&lt;a href=&quot;model#test_step&quot;&gt; &lt;code&gt;Model.test_step&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="39425f208b695839de1ae797673c90196ac3923f" translate="yes" xml:space="preserve">
          <source>Typically, this method directly controls &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../distribute/strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt; settings, and delegates the actual training logic to &lt;a href=&quot;model#train_step&quot;&gt;&lt;code&gt;Model.train_step&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">通常，此方法直接控制&lt;a href=&quot;../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../distribute/strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; &lt;/a&gt;设置，并将实际的训练逻辑委托给&lt;a href=&quot;model#train_step&quot;&gt; &lt;code&gt;Model.train_step&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4872edff50fbbe6a9f917cabaf4b95b3b51cfbf5" translate="yes" xml:space="preserve">
          <source>UUID of function that this tracks arguments for.</source>
          <target state="translated">追踪参数的函数的UUID。</target>
        </trans-unit>
        <trans-unit id="028563b1e54555afd3216dd205c2179bff62edca" translate="yes" xml:space="preserve">
          <source>Unbatch</source>
          <target state="translated">Unbatch</target>
        </trans-unit>
        <trans-unit id="18de537cd188f9aa6703c0bcca0bdd3aa030d834" translate="yes" xml:space="preserve">
          <source>UnbatchDataset</source>
          <target state="translated">UnbatchDataset</target>
        </trans-unit>
        <trans-unit id="282c6a33f97fdaf72f77ce51ca56ef49ffc77747" translate="yes" xml:space="preserve">
          <source>UnbatchGrad</source>
          <target state="translated">UnbatchGrad</target>
        </trans-unit>
        <trans-unit id="0ed10c799418646f55b928fd78fe57ac0388bf1c" translate="yes" xml:space="preserve">
          <source>UncompressElement</source>
          <target state="translated">UncompressElement</target>
        </trans-unit>
        <trans-unit id="4b7dbe0386bf95cabb32b5ed231c8f56aae99b96" translate="yes" xml:space="preserve">
          <source>Uncompresses a compressed dataset element.</source>
          <target state="translated">解压压缩的数据集元素。</target>
        </trans-unit>
        <trans-unit id="315f69cd0de49a2c6013b3acdede0a3586eb743f" translate="yes" xml:space="preserve">
          <source>Under &lt;code&gt;TPUStrategy&lt;/code&gt;, we allow access to the method &lt;code&gt;enqueue&lt;/code&gt;, &lt;code&gt;dequeue&lt;/code&gt; and &lt;code&gt;apply_gradients&lt;/code&gt;. We will show examples below of how to use these to train and evaluate your model. Under CPU, we only access to the &lt;code&gt;embedding_tables&lt;/code&gt; property which allow access to the embedding tables so that you can use them to run model evaluation/prediction on CPU.</source>
          <target state="translated">在 &lt;code&gt;TPUStrategy&lt;/code&gt; 下，我们允许访问方法 &lt;code&gt;enqueue&lt;/code&gt; ， &lt;code&gt;dequeue&lt;/code&gt; 和 &lt;code&gt;apply_gradients&lt;/code&gt; 。我们将在下面显示一些示例，说明如何使用这些示例来训练和评估您的模型。在CPU下，我们仅访问 &lt;code&gt;embedding_tables&lt;/code&gt; 属性，该属性允许访问嵌入表，以便您可以使用它们在CPU上运行模型评估/预测。</target>
        </trans-unit>
        <trans-unit id="e3e018efacc4788ac4991e79f9e0eac32efd113e" translate="yes" xml:space="preserve">
          <source>Under a scope &lt;code&gt;with custom_object_scope(objects_dict)&lt;/code&gt;, Keras methods such as &lt;a href=&quot;../models/load_model&quot;&gt;&lt;code&gt;tf.keras.models.load_model&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../models/model_from_config&quot;&gt;&lt;code&gt;tf.keras.models.model_from_config&lt;/code&gt;&lt;/a&gt; will be able to deserialize any custom object referenced by a saved config (e.g. a custom layer or metric).</source>
          <target state="translated">在 &lt;code&gt;with custom_object_scope(objects_dict)&lt;/code&gt; 的作用域下，诸如&lt;a href=&quot;../models/load_model&quot;&gt; &lt;code&gt;tf.keras.models.load_model&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;../models/model_from_config&quot;&gt; &lt;code&gt;tf.keras.models.model_from_config&lt;/code&gt; 之&lt;/a&gt;类的Keras方法将能够反序列化已保存的配置引用的任何自定义对象（例如，自定义层或度量）。</target>
        </trans-unit>
        <trans-unit id="505df01a0bd35a5e05151b2e08abe84fb66596dc" translate="yes" xml:space="preserve">
          <source>Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units: &lt;a href=&quot;http://proceedings.mlr.press/v48/shang16&quot;&gt;Shang et al., 2016&lt;/a&gt; (&lt;a href=&quot;http://proceedings.mlr.press/v48/shang16.pdf&quot;&gt;pdf&lt;/a&gt;)</source>
          <target state="translated">通过级联的整流线性单元理解和改进卷积神经网络：&lt;a href=&quot;http://proceedings.mlr.press/v48/shang16&quot;&gt;Shang等，2016&lt;/a&gt;（&lt;a href=&quot;http://proceedings.mlr.press/v48/shang16.pdf&quot;&gt;pdf&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="1f2505b6c39bb7db973660c096d38f1739c74197" translate="yes" xml:space="preserve">
          <source>Undoes all SmartSet() &amp;amp; Set() calls, restoring original definitions.</source>
          <target state="translated">撤消所有SmartSet（）和Set（）调用，恢复原始定义。</target>
        </trans-unit>
        <trans-unit id="1956e966c40ea031fb68e7fe6e402465c17b477c" translate="yes" xml:space="preserve">
          <source>Unicode encoding that should be used to encode each codepoint sequence. Can be &lt;code&gt;&quot;UTF-8&quot;&lt;/code&gt;, &lt;code&gt;&quot;UTF-16-BE&quot;&lt;/code&gt;, or &lt;code&gt;&quot;UTF-32-BE&quot;&lt;/code&gt;.</source>
          <target state="translated">应该用于对每个代码点序列进行编码的Unicode编码。可以是 &lt;code&gt;&quot;UTF-8&quot;&lt;/code&gt; ， &lt;code&gt;&quot;UTF-16-BE&quot;&lt;/code&gt; 或 &lt;code&gt;&quot;UTF-32-BE&quot;&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="90ac7bb3b01b32e952ee295aa92163d24d25615d" translate="yes" xml:space="preserve">
          <source>Unicode strings</source>
          <target state="translated">统一码字符串</target>
        </trans-unit>
        <trans-unit id="5e0684d6d7d272631c8bbe44f41cfa3f5076b1d4" translate="yes" xml:space="preserve">
          <source>UnicodeDecode</source>
          <target state="translated">UnicodeDecode</target>
        </trans-unit>
        <trans-unit id="d7d59980b4ae5cf39a350faf2c470f5fc69d74d8" translate="yes" xml:space="preserve">
          <source>UnicodeDecodeWithOffsets</source>
          <target state="translated">UnicodeDecodeWithOffsets</target>
        </trans-unit>
        <trans-unit id="05ae41cc8fec2efee56c1fd3fc455b9ee887c661" translate="yes" xml:space="preserve">
          <source>UnicodeEncode</source>
          <target state="translated">UnicodeEncode</target>
        </trans-unit>
        <trans-unit id="d48a266454de5c99b64f1535df01a2aa732eb493" translate="yes" xml:space="preserve">
          <source>UnicodeScript</source>
          <target state="translated">UnicodeScript</target>
        </trans-unit>
        <trans-unit id="c03f6044710be2f868218b1b8ef3d1a0cb70d8e9" translate="yes" xml:space="preserve">
          <source>UnicodeTranscode</source>
          <target state="translated">UnicodeTranscode</target>
        </trans-unit>
        <trans-unit id="976c215ff49db1df66b6e17319495979c35769ef" translate="yes" xml:space="preserve">
          <source>Uniform Inner Dimensions</source>
          <target state="translated">统一内径</target>
        </trans-unit>
        <trans-unit id="8560175804cd279f78f66d9a95299a6ca618dbfc" translate="yes" xml:space="preserve">
          <source>Uniform Outer Dimensions</source>
          <target state="translated">统一外形尺寸</target>
        </trans-unit>
        <trans-unit id="877e52ac4e7248f2800c12c1d91696ac0ce4e843" translate="yes" xml:space="preserve">
          <source>Uniform and ragged outer dimensions may be interleaved, meaning that a tensor with any combination of ragged and uniform dimensions may be created. For example, a RaggedTensor &lt;code&gt;t4&lt;/code&gt; with shape &lt;code&gt;[3, None, 4, 8, None, 2]&lt;/code&gt; could be constructed as follows:</source>
          <target state="translated">可以交错均匀和参差不齐的外部尺寸，这意味着可以创建具有参差不齐和统一尺寸的任意组合的张量。例如，形状为 &lt;code&gt;[3, None, 4, 8, None, 2]&lt;/code&gt; 4，4，8 ，None，2]的RaggedTensor &lt;code&gt;t4&lt;/code&gt; 可以构造如下：</target>
        </trans-unit>
        <trans-unit id="9c91421a9e87411dcbfdcaf48e2c1f3430455339" translate="yes" xml:space="preserve">
          <source>Uniform dimensions are encoded using multidimensional numpy &lt;code&gt;array&lt;/code&gt;s. In the following example, the value returned by &lt;a href=&quot;raggedtensor#numpy&quot;&gt;&lt;code&gt;RaggedTensor.numpy()&lt;/code&gt;&lt;/a&gt; contains a single numpy &lt;code&gt;array&lt;/code&gt; object, with &lt;code&gt;rank=2&lt;/code&gt; and &lt;code&gt;dtype=int64&lt;/code&gt;:</source>
          <target state="translated">统一尺寸是使用多维numpy &lt;code&gt;array&lt;/code&gt; s编码的。在以下示例中，&lt;a href=&quot;raggedtensor#numpy&quot;&gt; &lt;code&gt;RaggedTensor.numpy()&lt;/code&gt; &lt;/a&gt;返回的值包含单个numpy &lt;code&gt;array&lt;/code&gt; 对象，其 &lt;code&gt;rank=2&lt;/code&gt; 和 &lt;code&gt;dtype=int64&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="f358f22f84035b210a806ebde2b7ba64470c6103" translate="yes" xml:space="preserve">
          <source>Uniform distribution on an integer type's entire range.</source>
          <target state="translated">整数类型整个范围的统一分布。</target>
        </trans-unit>
        <trans-unit id="5a636a3f76b9605e2629209f45efcb9cace24843" translate="yes" xml:space="preserve">
          <source>Uniform distribution with &lt;code&gt;low&lt;/code&gt; and &lt;code&gt;high&lt;/code&gt; parameters.</source>
          <target state="translated">均匀分布 &lt;code&gt;low&lt;/code&gt; 和 &lt;code&gt;high&lt;/code&gt; 参数。</target>
        </trans-unit>
        <trans-unit id="2408554486ca40a7df4bc74cb3619a293981c6a8" translate="yes" xml:space="preserve">
          <source>UniformCandidateSampler</source>
          <target state="translated">UniformCandidateSampler</target>
        </trans-unit>
        <trans-unit id="82b9b64659267074d1c1ba85abf6981423a4e4d2" translate="yes" xml:space="preserve">
          <source>UniformRowLength(length,)</source>
          <target state="translated">UniformRowLength(length,)</target>
        </trans-unit>
        <trans-unit id="338a58853d5b589d79285d729f26c7d598bf74eb" translate="yes" xml:space="preserve">
          <source>Union[Iterable[Enum], Iterable[Text], Enum, Text, None], the default value of the flag; see &lt;code&gt;DEFINE_multi&lt;/code&gt;; only differences are documented here. If the value is a single Enum, it is treated as a single-item list of that Enum value. If it is an iterable, text values within the iterable will be converted to the equivalent Enum objects.</source>
          <target state="translated">Union [Iterable [Enum]，Iterable [Text]，Enum，Text，None]，标志的默认值；参见 &lt;code&gt;DEFINE_multi&lt;/code&gt; ; 此处仅记录差异。如果该值为单个Enum，则将其视为该Enum值的单项列表。如果它是可迭代的，则该可迭代范围内的文本值将转换为等效的Enum对象。</target>
        </trans-unit>
        <trans-unit id="04886f2c2934fa1074408cc7cc12b5f99af25706" translate="yes" xml:space="preserve">
          <source>Union[Iterable[T], Text, None], the default value of the flag. If the value is text, it will be parsed as if it was provided from the command line. If the value is a non-string iterable, it will be iterated over to create a shallow copy of the values. If it is None, it is left as-is.</source>
          <target state="translated">Union[Iterable[T],Text,None],该标志的默认值。如果值是文本,它将被解析,就像它是由命令行提供的一样。如果值是非字符串迭代,它将被迭代以创建一个值的浅层副本。如果是None,则保持原样。</target>
        </trans-unit>
        <trans-unit id="737c59cf13411337a19af29f48783402b8ed06de" translate="yes" xml:space="preserve">
          <source>Union[Iterable[Text], Text, None], the default value of the flag; see &lt;code&gt;DEFINE_multi&lt;/code&gt;.</source>
          <target state="translated">Union [Iterable [Text]，Text，None]，标志的默认值；参见 &lt;code&gt;DEFINE_multi&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="6428fc2f5260465f511164c37261c5570e275cd5" translate="yes" xml:space="preserve">
          <source>Union[Iterable[float], Text, None], the default value of the flag; see &lt;code&gt;DEFINE_multi&lt;/code&gt;.</source>
          <target state="translated">Union [Iterable [float]，Text，None]，标志的默认值；参见 &lt;code&gt;DEFINE_multi&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c19e99ab715ea5a714cc70c9ca1419999c89628b" translate="yes" xml:space="preserve">
          <source>Union[Iterable[int], Text, None], the default value of the flag; see &lt;code&gt;DEFINE_multi&lt;/code&gt;.</source>
          <target state="translated">Union [Iterable [int]，Text，None]，标志的默认值；参见 &lt;code&gt;DEFINE_multi&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="87c6f410754d2f5d42cbccc33576be0d5edc865c" translate="yes" xml:space="preserve">
          <source>Unique</source>
          <target state="translated">Unique</target>
        </trans-unit>
        <trans-unit id="0dd84a267eb06050e1d51577f84d2e5f9d140c27" translate="yes" xml:space="preserve">
          <source>Unique integer ID.</source>
          <target state="translated">唯一的整数ID。</target>
        </trans-unit>
        <trans-unit id="b898bebda42c40e1bfed797240e3b6330402424d" translate="yes" xml:space="preserve">
          <source>UniqueDataset</source>
          <target state="translated">UniqueDataset</target>
        </trans-unit>
        <trans-unit id="936d3125f6165a80c34df14b2afdc4601cd8b763" translate="yes" xml:space="preserve">
          <source>UniqueV2</source>
          <target state="translated">UniqueV2</target>
        </trans-unit>
        <trans-unit id="6d7fb951e6346ddabc82f570397d0fd46ed74844" translate="yes" xml:space="preserve">
          <source>UniqueWithCounts</source>
          <target state="translated">UniqueWithCounts</target>
        </trans-unit>
        <trans-unit id="1c7a5d24b663759b635f7eb14541a1e553019f23" translate="yes" xml:space="preserve">
          <source>UniqueWithCountsV2</source>
          <target state="translated">UniqueWithCountsV2</target>
        </trans-unit>
        <trans-unit id="3716d64264f900396f1af7fc16d7b86fdf992a50" translate="yes" xml:space="preserve">
          <source>Unknown error.</source>
          <target state="translated">未知的错误。</target>
        </trans-unit>
        <trans-unit id="36714cf4410eee18e8f50df2ffea0a7fa2ac86eb" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;../../../../data/dataset#batch&quot;&gt;&lt;code&gt;tf.data.Dataset.batch&lt;/code&gt;&lt;/a&gt;, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in &lt;code&gt;padded_shapes&lt;/code&gt;. The &lt;code&gt;padded_shapes&lt;/code&gt; argument determines the resulting shape for each dimension of each component in an output element:</source>
          <target state="translated">与&lt;a href=&quot;../../../../data/dataset#batch&quot;&gt; &lt;code&gt;tf.data.Dataset.batch&lt;/code&gt; &lt;/a&gt;不同，要批处理的输入元素可能具有不同的形状，并且此转换会将每个组件填充为 &lt;code&gt;padded_shapes&lt;/code&gt; 中的相应形状。所述 &lt;code&gt;padded_shapes&lt;/code&gt; 参数确定用于在输出元件的每个组件的每个维度产生的形状：</target>
        </trans-unit>
        <trans-unit id="9ecf9df1b9c446def763a4926348afbeccb07217" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;../../../../data/dataset#batch&quot;&gt;&lt;code&gt;tf.data.Dataset.batch&lt;/code&gt;&lt;/a&gt;, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in &lt;code&gt;padding_shapes&lt;/code&gt;. The &lt;code&gt;padding_shapes&lt;/code&gt; argument determines the resulting shape for each dimension of each component in an output element:</source>
          <target state="translated">与&lt;a href=&quot;../../../../data/dataset#batch&quot;&gt; &lt;code&gt;tf.data.Dataset.batch&lt;/code&gt; &lt;/a&gt;不同，要批处理的输入元素可能具有不同的形状，并且此转换会将每个组件填充为 &lt;code&gt;padding_shapes&lt;/code&gt; 中的相应形状。所述 &lt;code&gt;padding_shapes&lt;/code&gt; 参数确定用于在输出元件的每个组件的每个维度产生的形状：</target>
        </trans-unit>
        <trans-unit id="576d359833afce4c0260f4d74f22330fd90213de" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;../../../data/dataset#batch&quot;&gt;&lt;code&gt;tf.data.Dataset.batch&lt;/code&gt;&lt;/a&gt;, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in &lt;code&gt;padded_shapes&lt;/code&gt;. The &lt;code&gt;padded_shapes&lt;/code&gt; argument determines the resulting shape for each dimension of each component in an output element:</source>
          <target state="translated">与&lt;a href=&quot;../../../data/dataset#batch&quot;&gt; &lt;code&gt;tf.data.Dataset.batch&lt;/code&gt; &lt;/a&gt;不同，要批处理的输入元素可能具有不同的形状，并且此转换会将每个组件填充为 &lt;code&gt;padded_shapes&lt;/code&gt; 中的相应形状。所述 &lt;code&gt;padded_shapes&lt;/code&gt; 参数确定用于在输出元件的每个组件的每个维度产生的形状：</target>
        </trans-unit>
        <trans-unit id="cb9a4a5a4e9492ece2558573a0acc8a789ddc313" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;../../../data/dataset#batch&quot;&gt;&lt;code&gt;tf.data.Dataset.batch&lt;/code&gt;&lt;/a&gt;, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in &lt;code&gt;padding_shapes&lt;/code&gt;. The &lt;code&gt;padding_shapes&lt;/code&gt; argument determines the resulting shape for each dimension of each component in an output element:</source>
          <target state="translated">与&lt;a href=&quot;../../../data/dataset#batch&quot;&gt; &lt;code&gt;tf.data.Dataset.batch&lt;/code&gt; &lt;/a&gt;不同，要批处理的输入元素可能具有不同的形状，并且此转换会将每个组件填充为 &lt;code&gt;padding_shapes&lt;/code&gt; 中的相应形状。所述 &lt;code&gt;padding_shapes&lt;/code&gt; 参数确定用于在输出元件的每个组件的每个维度产生的形状：</target>
        </trans-unit>
        <trans-unit id="e4abe35be36c6363497bfcbdede20ee0472e70d3" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;to_graph&lt;/code&gt; is a low-level transpiler that converts Python code to TensorFlow graph code. It does not implement any caching, variable management or create any actual ops, and is best used where greater control over the generated TensorFlow graph is desired. Another difference from &lt;a href=&quot;../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; is that &lt;code&gt;to_graph&lt;/code&gt; will not wrap the graph into a TensorFlow function or a Python callable. Internally, &lt;a href=&quot;../../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; uses &lt;code&gt;to_graph&lt;/code&gt;.</source>
          <target state="translated">与&lt;a href=&quot;../../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;不同， &lt;code&gt;to_graph&lt;/code&gt; 是一个低级编译器，可将Python代码转换为TensorFlow图形代码。它不执行任何缓存，变量管理或创建任何实际操作，并且最好用于需要对生成的TensorFlow图进行更大控制的位置。与&lt;a href=&quot;../../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; 的&lt;/a&gt;另一个区别是 &lt;code&gt;to_graph&lt;/code&gt; 不会将图形包装到TensorFlow函数或Python可调用中。在内部，&lt;a href=&quot;../../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;使用 &lt;code&gt;to_graph&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c289f8aae64df7cd4ff7fdb8457061e6c012cdc6" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;../../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;wrap_function&lt;/code&gt; will only trace the Python function once. As with placeholders in TF 1.x, shapes and dtypes must be provided to &lt;code&gt;wrap_function&lt;/code&gt;'s &lt;code&gt;signature&lt;/code&gt; argument.</source>
          <target state="translated">与&lt;a href=&quot;../../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;不同， &lt;code&gt;wrap_function&lt;/code&gt; 只会跟踪Python函数一次。与TF 1.x中的占位符一样，必须为 &lt;code&gt;wrap_function&lt;/code&gt; 的 &lt;code&gt;signature&lt;/code&gt; 参数提供shape和dtypes 。</target>
        </trans-unit>
        <trans-unit id="704bf993fd1f302cbdbbea9b08e8e2b996cf4dcb" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;../dataset#batch&quot;&gt;&lt;code&gt;tf.data.Dataset.batch&lt;/code&gt;&lt;/a&gt;, the input elements to be batched may have different shapes, and each batch will be encoded as a &lt;a href=&quot;../../raggedtensor&quot;&gt;&lt;code&gt;tf.RaggedTensor&lt;/code&gt;&lt;/a&gt;. Example:</source>
          <target state="translated">与&lt;a href=&quot;../dataset#batch&quot;&gt; &lt;code&gt;tf.data.Dataset.batch&lt;/code&gt; &lt;/a&gt;不同，要批处理的输入元素可以具有不同的形状，并且每个批处理都将被编码为&lt;a href=&quot;../../raggedtensor&quot;&gt; &lt;code&gt;tf.RaggedTensor&lt;/code&gt; &lt;/a&gt;。例：</target>
        </trans-unit>
        <trans-unit id="ab3fab341c82646fe8f085450f444bec807918c5" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;../dataset#batch&quot;&gt;&lt;code&gt;tf.data.Dataset.batch&lt;/code&gt;&lt;/a&gt;, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in &lt;code&gt;padded_shapes&lt;/code&gt;. The &lt;code&gt;padded_shapes&lt;/code&gt; argument determines the resulting shape for each dimension of each component in an output element:</source>
          <target state="translated">与&lt;a href=&quot;../dataset#batch&quot;&gt; &lt;code&gt;tf.data.Dataset.batch&lt;/code&gt; &lt;/a&gt;不同，要批处理的输入元素可能具有不同的形状，并且此转换会将每个组件填充为 &lt;code&gt;padded_shapes&lt;/code&gt; 中的相应形状。所述 &lt;code&gt;padded_shapes&lt;/code&gt; 参数确定用于在输出元件的每个组件的每个维度产生的形状：</target>
        </trans-unit>
        <trans-unit id="1ebb4bc2458197f2fb3172235869b74c91d350b2" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;../dataset#batch&quot;&gt;&lt;code&gt;tf.data.Dataset.batch&lt;/code&gt;&lt;/a&gt;, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in &lt;code&gt;padding_shapes&lt;/code&gt;. The &lt;code&gt;padding_shapes&lt;/code&gt; argument determines the resulting shape for each dimension of each component in an output element:</source>
          <target state="translated">与&lt;a href=&quot;../dataset#batch&quot;&gt; &lt;code&gt;tf.data.Dataset.batch&lt;/code&gt; &lt;/a&gt;不同，要批处理的输入元素可能具有不同的形状，并且此转换会将每个组件填充为 &lt;code&gt;padding_shapes&lt;/code&gt; 中的相应形状。所述 &lt;code&gt;padding_shapes&lt;/code&gt; 参数确定用于在输出元件的每个组件的每个维度产生的形状：</target>
        </trans-unit>
        <trans-unit id="0d34e57a1aa18be3c8484d7069cc1c8ad2db64a5" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;../dataset#batch&quot;&gt;&lt;code&gt;tf.data.Dataset.batch&lt;/code&gt;&lt;/a&gt;, the input elements to be batched may have different shapes:</source>
          <target state="translated">与&lt;a href=&quot;../dataset#batch&quot;&gt; &lt;code&gt;tf.data.Dataset.batch&lt;/code&gt; &lt;/a&gt;不同，要批处理的输入元素可能具有不同的形状：</target>
        </trans-unit>
        <trans-unit id="69eb42951cde2b887407e0d83e6a95dc1668220b" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;to_graph&lt;/code&gt; is a low-level transpiler that converts Python code to TensorFlow graph code. It does not implement any caching, variable management or create any actual ops, and is best used where greater control over the generated TensorFlow graph is desired. Another difference from &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; is that &lt;code&gt;to_graph&lt;/code&gt; will not wrap the graph into a TensorFlow function or a Python callable. Internally, &lt;a href=&quot;../function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt; uses &lt;code&gt;to_graph&lt;/code&gt;.</source>
          <target state="translated">与&lt;a href=&quot;../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;不同， &lt;code&gt;to_graph&lt;/code&gt; 是一个低级编译器，可将Python代码转换为TensorFlow图形代码。它不执行任何缓存，变量管理或创建任何实际操作，并且最好用于需要对生成的TensorFlow图进行更大控制的位置。与&lt;a href=&quot;../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; 的&lt;/a&gt;另一个区别是 &lt;code&gt;to_graph&lt;/code&gt; 不会将图形包装到TensorFlow函数或Python可调用中。在内部，&lt;a href=&quot;../function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;使用 &lt;code&gt;to_graph&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="de7b7f502adc89ee96e98e3c06014fa1c272dc91" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;dataset#batch&quot;&gt;&lt;code&gt;tf.data.Dataset.batch&lt;/code&gt;&lt;/a&gt;, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in &lt;code&gt;padded_shapes&lt;/code&gt;. The &lt;code&gt;padded_shapes&lt;/code&gt; argument determines the resulting shape for each dimension of each component in an output element:</source>
          <target state="translated">与&lt;a href=&quot;dataset#batch&quot;&gt; &lt;code&gt;tf.data.Dataset.batch&lt;/code&gt; &lt;/a&gt;不同，要批处理的输入元素可能具有不同的形状，并且此转换会将每个组件填充为 &lt;code&gt;padded_shapes&lt;/code&gt; 中的相应形状。所述 &lt;code&gt;padded_shapes&lt;/code&gt; 参数确定用于在输出元件的每个组件的每个维度产生的形状：</target>
        </trans-unit>
        <trans-unit id="dcde43ba32f97c1ff2549287396cb94e24747c3e" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;dataset#batch&quot;&gt;&lt;code&gt;tf.data.Dataset.batch&lt;/code&gt;&lt;/a&gt;, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in &lt;code&gt;padding_shapes&lt;/code&gt;. The &lt;code&gt;padding_shapes&lt;/code&gt; argument determines the resulting shape for each dimension of each component in an output element:</source>
          <target state="translated">与&lt;a href=&quot;dataset#batch&quot;&gt; &lt;code&gt;tf.data.Dataset.batch&lt;/code&gt; &lt;/a&gt;不同，要批处理的输入元素可能具有不同的形状，并且此转换会将每个组件填充为 &lt;code&gt;padding_shapes&lt;/code&gt; 中的相应形状。所述 &lt;code&gt;padding_shapes&lt;/code&gt; 参数确定用于在输出元件的每个组件的每个维度产生的形状：</target>
        </trans-unit>
        <trans-unit id="e86a43f4e75dabc2059ed1937b3c30cb8a924b23" translate="yes" xml:space="preserve">
          <source>Unlike &lt;code&gt;SoftmaxCrossEntropyWithLogits&lt;/code&gt;, this operation does not accept a matrix of label probabilities, but rather a single label per row of features. This label is considered to have probability 1.0 for the given row.</source>
          <target state="translated">与 &lt;code&gt;SoftmaxCrossEntropyWithLogits&lt;/code&gt; 不同，此操作不接受标签概率矩阵，而是每行要素一个标签。对于给定的行，该标签被认为具有1.0的概率。</target>
        </trans-unit>
        <trans-unit id="c42cd1138a7b09e8aab4b48a275bce9fecd055c5" translate="yes" xml:space="preserve">
          <source>Unlike &lt;code&gt;mean_squared_error&lt;/code&gt;, which is a measure of the differences between corresponding elements of &lt;code&gt;predictions&lt;/code&gt; and &lt;code&gt;labels&lt;/code&gt;, &lt;code&gt;mean_pairwise_squared_error&lt;/code&gt; is a measure of the differences between pairs of corresponding elements of &lt;code&gt;predictions&lt;/code&gt; and &lt;code&gt;labels&lt;/code&gt;.</source>
          <target state="translated">与 &lt;code&gt;mean_squared_error&lt;/code&gt; 用来度量 &lt;code&gt;predictions&lt;/code&gt; 和 &lt;code&gt;labels&lt;/code&gt; 对应元素之间的差异不同， &lt;code&gt;mean_pairwise_squared_error&lt;/code&gt; 是度量 &lt;code&gt;predictions&lt;/code&gt; 和 &lt;code&gt;labels&lt;/code&gt; 的对应元素对之间的差异的度量。</target>
        </trans-unit>
        <trans-unit id="54b3e19f1658856fdd01905f7b1d827a5e2c7da2" translate="yes" xml:space="preserve">
          <source>Unlike &lt;code&gt;stack&lt;/code&gt;, &lt;code&gt;parallel_stack&lt;/code&gt; does NOT support backpropagation.</source>
          <target state="translated">与 &lt;code&gt;stack&lt;/code&gt; 不同， &lt;code&gt;parallel_stack&lt;/code&gt; 不支持反向传播。</target>
        </trans-unit>
        <trans-unit id="3d3e8c9f8d19935be11b8a97738c865b48f3a40a" translate="yes" xml:space="preserve">
          <source>Unlike FractionalMaxPoolGrad, we don't need to find arg_max for FractionalAvgPoolGrad, we just need to evenly back-propagate each element of out_backprop to those indices that form the same pooling cell. Therefore, we just need to know the shape of original input tensor, instead of the whole tensor.</source>
          <target state="translated">与FractionalMaxPoolGrad不同的是,我们不需要为FractionalAvgPoolGrad找到arg_max,我们只需要将out_backprop中的每个元素均匀地反推到那些构成同一池化单元的指数上。因此,我们只需要知道原始输入张量的形状,而不是整个张量。</target>
        </trans-unit>
        <trans-unit id="591f5490b4f3510f244c0007610068c1ceac25d0" translate="yes" xml:space="preserve">
          <source>Unlike MapDataset, the &lt;code&gt;f&lt;/code&gt; in FlatMapDataset is expected to return a Dataset variant, and FlatMapDataset will flatten successive results into a single Dataset.</source>
          <target state="translated">与MapDataset不同，FlatMapDataset中的 &lt;code&gt;f&lt;/code&gt; 应该返回Dataset变体，FlatMapDataset将连续的结果展平为单个Dataset。</target>
        </trans-unit>
        <trans-unit id="c453e83691ce84ff295e7a2c1ef516446c3ab8d7" translate="yes" xml:space="preserve">
          <source>Unlike MapDataset, the &lt;code&gt;f&lt;/code&gt; in InterleaveDataset is expected to return a Dataset variant, and InterleaveDataset will flatten successive results into a single Dataset. Unlike FlatMapDataset, InterleaveDataset will interleave sequences of up to &lt;code&gt;block_length&lt;/code&gt; consecutive elements from &lt;code&gt;cycle_length&lt;/code&gt; input elements.</source>
          <target state="translated">与MapDataset不同，InterleaveDataset中的 &lt;code&gt;f&lt;/code&gt; 预期会返回Dataset变体，并且InterleaveDataset会将连续的结果展平为单个Dataset。与FlatMapDataset不同，InterleaveDataset将对来自 &lt;code&gt;cycle_length&lt;/code&gt; 个输入元素的最多 &lt;code&gt;block_length&lt;/code&gt; 个连续元素的序列进行交织。</target>
        </trans-unit>
        <trans-unit id="9c4c90486ba0091b8ea290c9a282ca7b97ffe78f" translate="yes" xml:space="preserve">
          <source>Unlike a &quot;MapDataset&quot;, which applies &lt;code&gt;f&lt;/code&gt; sequentially, this dataset invokes up to &lt;code&gt;batch_size * num_parallel_batches&lt;/code&gt; copies of &lt;code&gt;f&lt;/code&gt; in parallel.</source>
          <target state="translated">与顺序应用 &lt;code&gt;f&lt;/code&gt; 的&amp;ldquo; MapDataset&amp;rdquo;不同，此数据集最多并行调用 &lt;code&gt;f&lt;/code&gt; 的 &lt;code&gt;batch_size * num_parallel_batches&lt;/code&gt; 副本。</target>
        </trans-unit>
        <trans-unit id="88b9771a4d40bd229bd2768494cd1206b15de019" translate="yes" xml:space="preserve">
          <source>Unlike a &quot;MapDataset&quot;, which applies &lt;code&gt;f&lt;/code&gt; sequentially, this dataset invokes up to &lt;code&gt;num_parallel_calls&lt;/code&gt; copies of &lt;code&gt;f&lt;/code&gt; in parallel.</source>
          <target state="translated">与顺序应用 &lt;code&gt;f&lt;/code&gt; 的&amp;ldquo; MapDataset&amp;rdquo;不同，此数据集并行调用最多 &lt;code&gt;f&lt;/code&gt; 的 &lt;code&gt;num_parallel_calls&lt;/code&gt; 个副本。</target>
        </trans-unit>
        <trans-unit id="13fc8fd7e6240efedf349e3dbe4446fea97e3c13" translate="yes" xml:space="preserve">
          <source>Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use &lt;code&gt;compute_output_shape&lt;/code&gt;, and will assume that the output dtype matches the input dtype.</source>
          <target state="translated">与TensorShape对象不同，TensorSpec对象包含张量的形状和dtype信息。如果此方法与输入dtype不同，则该方法允许层提供输出dtype信息。对于任何未实现此功能的层，该框架将转为使用 &lt;code&gt;compute_output_shape&lt;/code&gt; ，并假定输出dtype与输入dtype匹配。</target>
        </trans-unit>
        <trans-unit id="43b00e58be5bd1f34d6a3d43cebfb8995414356d" translate="yes" xml:space="preserve">
          <source>Unlike assertRaisesRegex, this method takes a literal string, not a regular expression.</source>
          <target state="translated">与 assertRaisesRegex 不同的是,本方法接受的是一个字面字符串,而不是一个正则表达式。</target>
        </trans-unit>
        <trans-unit id="b677c0df05da91282dc6309d244c4aedc109a5da" translate="yes" xml:space="preserve">
          <source>Unlike the Copy Op, this op has HostMemory constraint on its input or output.</source>
          <target state="translated">与复制操作不同的是,这个操作对其输入或输出有HostMemory约束。</target>
        </trans-unit>
        <trans-unit id="d5f7d3fe36850e7fa5811bed8af5b2d92d808d37" translate="yes" xml:space="preserve">
          <source>Unlike the CopyHost Op, this op does not have HostMemory constraint on its input or output.</source>
          <target state="translated">与CopyHost操作不同的是,这个操作的输入或输出没有HostMemory约束。</target>
        </trans-unit>
        <trans-unit id="780bb9592c9dfb9fae30e2efc18623daa3e2cff4" translate="yes" xml:space="preserve">
          <source>Unlike the older op &lt;a href=&quot;compat/v1/squeeze&quot;&gt;&lt;code&gt;tf.compat.v1.squeeze&lt;/code&gt;&lt;/a&gt;, this op does not accept a deprecated &lt;code&gt;squeeze_dims&lt;/code&gt; argument.</source>
          <target state="translated">与旧版op &lt;a href=&quot;compat/v1/squeeze&quot;&gt; &lt;code&gt;tf.compat.v1.squeeze&lt;/code&gt; &lt;/a&gt;不同，此op不接受不推荐使用的 &lt;code&gt;squeeze_dims&lt;/code&gt; 参数。</target>
        </trans-unit>
        <trans-unit id="33c0f5ca96c96a12269cd4ff95c4f773357fb44e" translate="yes" xml:space="preserve">
          <source>Unlike the original &lt;code&gt;accumulate_n&lt;/code&gt;, &lt;code&gt;accumulate_n_v2&lt;/code&gt; is differentiable.</source>
          <target state="translated">与原始的 &lt;code&gt;accumulate_n&lt;/code&gt; 不同， &lt;code&gt;accumulate_n_v2&lt;/code&gt; 是可微的。</target>
        </trans-unit>
        <trans-unit id="9828410fab6a8798616092f30a5aa2ae3611617b" translate="yes" xml:space="preserve">
          <source>Unordered dictionaries are not supported in eager mode when &lt;code&gt;exclusive=False&lt;/code&gt;. Use a list of tuples instead.</source>
          <target state="translated">当 &lt;code&gt;exclusive=False&lt;/code&gt; 时，在急切模式下不支持无序词典。请改用元组列表。</target>
        </trans-unit>
        <trans-unit id="e2231a91d542441b9a855a9095ab4fe46e316699" translate="yes" xml:space="preserve">
          <source>Unpack</source>
          <target state="translated">Unpack</target>
        </trans-unit>
        <trans-unit id="0657883ef20cc657b888740e46664440ff735ff1" translate="yes" xml:space="preserve">
          <source>Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the &lt;code&gt;x&lt;/code&gt; argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. &lt;code&gt;({&quot;x0&quot;: x0, &quot;x1&quot;: x1}, y)&lt;/code&gt;. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: &lt;code&gt;namedtuple(&quot;example_tuple&quot;, [&quot;y&quot;, &quot;x&quot;])&lt;/code&gt; it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: &lt;code&gt;namedtuple(&quot;other_tuple&quot;, [&quot;x&quot;, &quot;y&quot;, &quot;z&quot;])&lt;/code&gt; where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to &lt;code&gt;x&lt;/code&gt;. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.)</source>
          <target state="translated">类似于迭代器的输入的拆包行为：一种常见的模式是将tf.data.Dataset，generator或tf.keras.utils.Sequence传递给fit的 &lt;code&gt;x&lt;/code&gt; 参数，这实际上不仅会产生特征（x），还会产生特征（可选）目标（y）和样本权重。 Keras要求此类类似迭代器的输出必须明确。迭代器应返回长度为1、2或3的元组，其中可选的第二和第三元素将分别用于y和sample_weight。提供的任何其他类型将被包裹在一个元组的长度中，从而将所有内容有效地视为&amp;ldquo; x&amp;rdquo;。产生命令时，它们仍应遵循顶级元组结构。例如 &lt;code&gt;({&quot;x0&quot;: x0, &quot;x1&quot;: x1}, y)&lt;/code&gt; 。 Keras不会尝试从单个字典的键中分离特征，目标和权重。值得注意的不受支持的数据类型是namedtuple。原因是它的行为类似于有序数据类型（元组）和映射数据类型（dict）。因此，给定形式的 &lt;code&gt;namedtuple(&quot;example_tuple&quot;, [&quot;y&quot;, &quot;x&quot;])&lt;/code&gt; ：namedtuple（&amp;ldquo; example_tuple&amp;rdquo;，[&amp;ldquo; y&amp;rdquo;，&amp;ldquo; x&amp;rdquo;]），在解释值时是否反转元素的顺序是不明确的。更糟糕的是以下形式的元组： &lt;code&gt;namedtuple(&quot;other_tuple&quot;, [&quot;x&quot;, &quot;y&quot;, &quot;z&quot;])&lt;/code&gt; 在哪里，不清楚该元组是否打算拆箱成x，y和sample_weight或传递通过作为 &lt;code&gt;x&lt;/code&gt; 的单个元素。结果，如果数据处理代码遇到一个命名元组，它将仅引发ValueError。（以及纠正该问题的说明。）</target>
        </trans-unit>
        <trans-unit id="a70d7a46a185b9dd6d1158e08b8b8c0950e01eaa" translate="yes" xml:space="preserve">
          <source>Unpacks &lt;code&gt;num&lt;/code&gt; tensors from &lt;code&gt;value&lt;/code&gt; by chipping it along the &lt;code&gt;axis&lt;/code&gt; dimension. For example, given a tensor of shape &lt;code&gt;(A, B, C, D)&lt;/code&gt;;</source>
          <target state="translated">解压缩 &lt;code&gt;num&lt;/code&gt; 从张量 &lt;code&gt;value&lt;/code&gt; 由沿崩裂它 &lt;code&gt;axis&lt;/code&gt; 维度。例如，给定一个形状为 &lt;code&gt;(A, B, C, D)&lt;/code&gt; 的张量；</target>
        </trans-unit>
        <trans-unit id="243ed91dcd10a0424df6d36deb350e6391df8548" translate="yes" xml:space="preserve">
          <source>Unpacks &lt;code&gt;num&lt;/code&gt; tensors from &lt;code&gt;value&lt;/code&gt; by chipping it along the &lt;code&gt;axis&lt;/code&gt; dimension. If &lt;code&gt;num&lt;/code&gt; is not specified (the default), it is inferred from &lt;code&gt;value&lt;/code&gt;'s shape. If &lt;code&gt;value.shape[axis]&lt;/code&gt; is not known, &lt;code&gt;ValueError&lt;/code&gt; is raised.</source>
          <target state="translated">解压缩 &lt;code&gt;num&lt;/code&gt; 从张量 &lt;code&gt;value&lt;/code&gt; 由沿崩裂它 &lt;code&gt;axis&lt;/code&gt; 维度。如果未指定 &lt;code&gt;num&lt;/code&gt; （默认值），则根据 &lt;code&gt;value&lt;/code&gt; 的形状进行推断。如果不知道 &lt;code&gt;value.shape[axis]&lt;/code&gt; ，则引发 &lt;code&gt;ValueError&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="eb445a131ff25a35a36ff7e142e017c24f785cd1" translate="yes" xml:space="preserve">
          <source>Unpacks a given dimension of a rank-&lt;code&gt;R&lt;/code&gt; tensor into &lt;code&gt;num&lt;/code&gt; rank-&lt;code&gt;(R-1)&lt;/code&gt; tensors.</source>
          <target state="translated">将给定维度的等级 &lt;code&gt;R&lt;/code&gt; 张量解压缩为 &lt;code&gt;num&lt;/code&gt; 等级 &lt;code&gt;(R-1)&lt;/code&gt; 张量。</target>
        </trans-unit>
        <trans-unit id="48918b5563a8ceafb76129f6bc60ab73e49603cf" translate="yes" xml:space="preserve">
          <source>Unpacks the given dimension of a rank-&lt;code&gt;R&lt;/code&gt; tensor into rank-&lt;code&gt;(R-1)&lt;/code&gt; tensors.</source>
          <target state="translated">将给定等级的 &lt;code&gt;R&lt;/code&gt; 张量解压缩为等级 &lt;code&gt;(R-1)&lt;/code&gt; 张量。</target>
        </trans-unit>
        <trans-unit id="d65a83d865bb52fdcfa278aa190e57e05dce3f63" translate="yes" xml:space="preserve">
          <source>Unpacks user-provided data tuple.</source>
          <target state="translated">解开用户提供的数据元组。</target>
        </trans-unit>
        <trans-unit id="05dc9402d9793ff470926b16aca6821cb0999026" translate="yes" xml:space="preserve">
          <source>Unparses all flags to the point before any FLAGS(argv) was called.</source>
          <target state="translated">解除所有flags(argv)被调用之前的标志。</target>
        </trans-unit>
        <trans-unit id="2c8018d2ec1edd743fc94f4ac501f4979aa67070" translate="yes" xml:space="preserve">
          <source>UnravelIndex</source>
          <target state="translated">UnravelIndex</target>
        </trans-unit>
        <trans-unit id="5abd8671c3b3e7c85742a7fbdf4fc8baadcb9032" translate="yes" xml:space="preserve">
          <source>UnrecognizedFlagError: if the referenced flag doesn't exist. DuplicateFlagError: if the alias name has been used by some existing flag.</source>
          <target state="translated">UnrecognizedFlagError:如果引用的标志不存在。DuplicateFlagError:如果别名被某个现有的标志使用了。</target>
        </trans-unit>
        <trans-unit id="d01190ad612aa7505976a06d2cc1e8836b163992" translate="yes" xml:space="preserve">
          <source>Unscaled log probabilities of shape &lt;code&gt;[d_0, d_1, ..., d_{r-1}, num_classes]&lt;/code&gt; and dtype &lt;code&gt;float16&lt;/code&gt;, &lt;code&gt;float32&lt;/code&gt; or &lt;code&gt;float64&lt;/code&gt;.</source>
          <target state="translated">形状为 &lt;code&gt;[d_0, d_1, ..., d_{r-1}, num_classes]&lt;/code&gt; 和 &lt;code&gt;float16&lt;/code&gt; ， &lt;code&gt;float32&lt;/code&gt; 或 &lt;code&gt;float64&lt;/code&gt; 的未缩放日志概率。</target>
        </trans-unit>
        <trans-unit id="6b9238f8e545d4a2d2b6a4160c436067e5463ca7" translate="yes" xml:space="preserve">
          <source>Unscaled log probabilities of shape &lt;code&gt;[d_0, d_1, ..., d_{r-1}, num_classes]&lt;/code&gt; and dtype &lt;code&gt;float16&lt;/code&gt;, &lt;code&gt;float32&lt;/code&gt;, or &lt;code&gt;float64&lt;/code&gt;.</source>
          <target state="translated">形状为 &lt;code&gt;[d_0, d_1, ..., d_{r-1}, num_classes]&lt;/code&gt; 和 &lt;code&gt;float16&lt;/code&gt; ， &lt;code&gt;float32&lt;/code&gt; 或 &lt;code&gt;float64&lt;/code&gt; 的未缩放日志概率。</target>
        </trans-unit>
        <trans-unit id="6fd64ffd66a09162278b73ad3a2e108855905b4f" translate="yes" xml:space="preserve">
          <source>Unscaled log probabilities.</source>
          <target state="translated">无标度对数概率。</target>
        </trans-unit>
        <trans-unit id="9e5da72ba27bb718f2dac7fc946f26f49a76ad43" translate="yes" xml:space="preserve">
          <source>Unscales the gradients by the loss scale.</source>
          <target state="translated">按损失比例对梯度进行解调。</target>
        </trans-unit>
        <trans-unit id="9329b6002fc399b1c6e7eb106d68b0ce50955b52" translate="yes" xml:space="preserve">
          <source>UnsortedSegmentJoin</source>
          <target state="translated">UnsortedSegmentJoin</target>
        </trans-unit>
        <trans-unit id="0982334ce0fca73c3a5e3ea913a7e0313a459f82" translate="yes" xml:space="preserve">
          <source>UnsortedSegmentMax</source>
          <target state="translated">UnsortedSegmentMax</target>
        </trans-unit>
        <trans-unit id="366fa15ecb43d76a152b506219741a7b3f5f0bcd" translate="yes" xml:space="preserve">
          <source>UnsortedSegmentMin</source>
          <target state="translated">UnsortedSegmentMin</target>
        </trans-unit>
        <trans-unit id="6ef6286b2679e6c430cc36837f66e9730b079a8c" translate="yes" xml:space="preserve">
          <source>UnsortedSegmentProd</source>
          <target state="translated">UnsortedSegmentProd</target>
        </trans-unit>
        <trans-unit id="88334ca532d2e8466fab68a79d76f33ec76acf9a" translate="yes" xml:space="preserve">
          <source>UnsortedSegmentSum</source>
          <target state="translated">UnsortedSegmentSum</target>
        </trans-unit>
        <trans-unit id="c32922004f1cab6d2b368005f373dc639dc0003a" translate="yes" xml:space="preserve">
          <source>Unspecified run-time error.</source>
          <target state="translated">未说明的运行时错误。</target>
        </trans-unit>
        <trans-unit id="b986cea6fdf9453b34548d5133226ff149db996f" translate="yes" xml:space="preserve">
          <source>Unstack the values of a &lt;code&gt;Tensor&lt;/code&gt; in the TensorArray.</source>
          <target state="translated">在TensorArray中解叠 &lt;code&gt;Tensor&lt;/code&gt; 的值。</target>
        </trans-unit>
        <trans-unit id="7298d7ed988425fb6199864e49341c509f32d027" translate="yes" xml:space="preserve">
          <source>Unstage</source>
          <target state="translated">Unstage</target>
        </trans-unit>
        <trans-unit id="c5093a3d1578793a3f4e74f50d7ce016ac60ef3d" translate="yes" xml:space="preserve">
          <source>Untested. Very likely will not learn to output repeated classes.</source>
          <target state="translated">未经测试。很可能不会学习输出重复的类。</target>
        </trans-unit>
        <trans-unit id="5a0115166e5bef4246c231811f6b2ed301118eb7" translate="yes" xml:space="preserve">
          <source>Until the release of TF 2.0, we need the legacy behavior of &lt;code&gt;TensorShape&lt;/code&gt; to coexist with the new behavior. This utility is a bridge between the two.</source>
          <target state="translated">在TF 2.0发布之前，我们需要 &lt;code&gt;TensorShape&lt;/code&gt; 的旧行为与新行为共存。该实用程序是两者之间的桥梁。</target>
        </trans-unit>
        <trans-unit id="f13be7738e1389f72c0f88aa84d17719f746bf4a" translate="yes" xml:space="preserve">
          <source>Unused.</source>
          <target state="translated">Unused.</target>
        </trans-unit>
        <trans-unit id="673367f4f1a26218e41caccf074dc2728214600f" translate="yes" xml:space="preserve">
          <source>UnwrapDatasetVariant</source>
          <target state="translated">UnwrapDatasetVariant</target>
        </trans-unit>
        <trans-unit id="45306172633156360fe411da89aad86bcba27d77" translate="yes" xml:space="preserve">
          <source>Unwrapping and merging: Consider calling a function &lt;code&gt;fn&lt;/code&gt; on multiple replicas, like &lt;code&gt;experimental_run_v2(fn, args=[w])&lt;/code&gt; with an argument &lt;code&gt;w&lt;/code&gt; that is a wrapped value. This means &lt;code&gt;w&lt;/code&gt; will have a map taking replica id &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;w0&lt;/code&gt;, replica id &lt;code&gt;11&lt;/code&gt; to &lt;code&gt;w1&lt;/code&gt;, etc. &lt;code&gt;experimental_run_v2()&lt;/code&gt; unwraps &lt;code&gt;w&lt;/code&gt; before calling &lt;code&gt;fn&lt;/code&gt;, so it calls &lt;code&gt;fn(w0)&lt;/code&gt; on &lt;code&gt;d0&lt;/code&gt;, &lt;code&gt;fn(w1)&lt;/code&gt; on &lt;code&gt;d1&lt;/code&gt;, etc. It then merges the return values from &lt;code&gt;fn()&lt;/code&gt;, which can possibly result in wrapped values. For example, let's say &lt;code&gt;fn()&lt;/code&gt; returns a tuple with three components: &lt;code&gt;(x, a, v0)&lt;/code&gt; from replica 0, &lt;code&gt;(x, b, v1)&lt;/code&gt; on replica 1, etc. If the first component is the same object &lt;code&gt;x&lt;/code&gt; from every replica, then the first component of the merged result will also be &lt;code&gt;x&lt;/code&gt;. If the second component is different (&lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, ...) from each replica, then the merged value will have a wrapped map from replica device to the different values. If the third component is the members of a mirrored variable (&lt;code&gt;v&lt;/code&gt; maps &lt;code&gt;d0&lt;/code&gt; to &lt;code&gt;v0&lt;/code&gt;, &lt;code&gt;d1&lt;/code&gt; to &lt;a href=&quot;../../v1&quot;&gt;&lt;code&gt;v1&lt;/code&gt;&lt;/a&gt;, etc.), then the merged result will be that mirrored variable (&lt;code&gt;v&lt;/code&gt;).</source>
          <target state="translated">展开和合并：考虑在多个副本上调用函数 &lt;code&gt;fn&lt;/code&gt; ，例如 &lt;code&gt;experimental_run_v2(fn, args=[w])&lt;/code&gt; ，其参数 &lt;code&gt;w&lt;/code&gt; 是被包装的值。这意味着 &lt;code&gt;w&lt;/code&gt; 将具有将副本ID &lt;code&gt;0&lt;/code&gt; 到 &lt;code&gt;w0&lt;/code&gt; ，副本ID &lt;code&gt;11&lt;/code&gt; 到 &lt;code&gt;w1&lt;/code&gt; 的映射，等等。experimental_run_v2 &lt;code&gt;experimental_run_v2()&lt;/code&gt; 在调用 &lt;code&gt;fn&lt;/code&gt; 之前解包 &lt;code&gt;w&lt;/code&gt; ，因此它在 &lt;code&gt;d0&lt;/code&gt; 上调用 &lt;code&gt;fn(w0)&lt;/code&gt; ，在 &lt;code&gt;d1&lt;/code&gt; 上调用 &lt;code&gt;fn(w1)&lt;/code&gt; 等等。然后合并 &lt;code&gt;fn()&lt;/code&gt; 的返回值，这可能会导致包装值。例如，假设 &lt;code&gt;fn()&lt;/code&gt; 返回具有三个成分的元组：副本0的 &lt;code&gt;(x, a, v0)&lt;/code&gt; ，副本1的 &lt;code&gt;(x, b, v1)&lt;/code&gt; ，等等。如果第一个成分是每个对象的相同对象 &lt;code&gt;x&lt;/code&gt; 复制品，则合并结果的第一个成分也将是 &lt;code&gt;x&lt;/code&gt; 。如果第二个组件与每个副本不同（ &lt;code&gt;a&lt;/code&gt; ， &lt;code&gt;b&lt;/code&gt; ，...），则合并值将具有从副本设备到不同值的包装映射。如果第三个组件是镜像变量的成员（ &lt;code&gt;v&lt;/code&gt; 将 &lt;code&gt;d0&lt;/code&gt; 映射到 &lt;code&gt;v0&lt;/code&gt; ，将 &lt;code&gt;d1&lt;/code&gt; 映射到&lt;a href=&quot;../../v1&quot;&gt; &lt;code&gt;v1&lt;/code&gt; &lt;/a&gt;等），则合并后的结果将是该镜像变量（ &lt;code&gt;v&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="452d19a44e1939a79c6b1e46110b7518ec2e32f4" translate="yes" xml:space="preserve">
          <source>Unwrapping and merging: Consider calling a function &lt;code&gt;fn&lt;/code&gt; on multiple replicas, like &lt;code&gt;experimental_run_v2(fn, args=[w])&lt;/code&gt; with an argument &lt;code&gt;w&lt;/code&gt; that is a wrapped value. This means &lt;code&gt;w&lt;/code&gt; will have a map taking replica id &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;w0&lt;/code&gt;, replica id &lt;code&gt;11&lt;/code&gt; to &lt;code&gt;w1&lt;/code&gt;, etc. &lt;code&gt;experimental_run_v2()&lt;/code&gt; unwraps &lt;code&gt;w&lt;/code&gt; before calling &lt;code&gt;fn&lt;/code&gt;, so it calls &lt;code&gt;fn(w0)&lt;/code&gt; on &lt;code&gt;d0&lt;/code&gt;, &lt;code&gt;fn(w1)&lt;/code&gt; on &lt;code&gt;d1&lt;/code&gt;, etc. It then merges the return values from &lt;code&gt;fn()&lt;/code&gt;, which can possibly result in wrapped values. For example, let's say &lt;code&gt;fn()&lt;/code&gt; returns a tuple with three components: &lt;code&gt;(x, a, v0)&lt;/code&gt; from replica 0, &lt;code&gt;(x, b, v1)&lt;/code&gt; on replica 1, etc. If the first component is the same object &lt;code&gt;x&lt;/code&gt; from every replica, then the first component of the merged result will also be &lt;code&gt;x&lt;/code&gt;. If the second component is different (&lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, ...) from each replica, then the merged value will have a wrapped map from replica device to the different values. If the third component is the members of a mirrored variable (&lt;code&gt;v&lt;/code&gt; maps &lt;code&gt;d0&lt;/code&gt; to &lt;code&gt;v0&lt;/code&gt;, &lt;code&gt;d1&lt;/code&gt; to &lt;a href=&quot;../compat/v1&quot;&gt;&lt;code&gt;v1&lt;/code&gt;&lt;/a&gt;, etc.), then the merged result will be that mirrored variable (&lt;code&gt;v&lt;/code&gt;).</source>
          <target state="translated">展开和合并：考虑在多个副本上调用函数 &lt;code&gt;fn&lt;/code&gt; ，例如 &lt;code&gt;experimental_run_v2(fn, args=[w])&lt;/code&gt; ，其参数 &lt;code&gt;w&lt;/code&gt; 是被包装的值。这意味着 &lt;code&gt;w&lt;/code&gt; 将具有将副本ID &lt;code&gt;0&lt;/code&gt; 到 &lt;code&gt;w0&lt;/code&gt; ，副本ID &lt;code&gt;11&lt;/code&gt; 到 &lt;code&gt;w1&lt;/code&gt; 的映射，等等。experimental_run_v2 &lt;code&gt;experimental_run_v2()&lt;/code&gt; 在调用 &lt;code&gt;fn&lt;/code&gt; 之前解包 &lt;code&gt;w&lt;/code&gt; ，因此它在 &lt;code&gt;d0&lt;/code&gt; 上调用 &lt;code&gt;fn(w0)&lt;/code&gt; ，在 &lt;code&gt;d1&lt;/code&gt; 上调用 &lt;code&gt;fn(w1)&lt;/code&gt; 等等。然后合并 &lt;code&gt;fn()&lt;/code&gt; 的返回值，这可能会导致包装值。例如，假设 &lt;code&gt;fn()&lt;/code&gt; 返回具有三个成分的元组：副本0的 &lt;code&gt;(x, a, v0)&lt;/code&gt; ，副本1的 &lt;code&gt;(x, b, v1)&lt;/code&gt; ，等等。如果第一个成分是每个对象的相同对象 &lt;code&gt;x&lt;/code&gt; 复制品，则合并结果的第一个成分也将是 &lt;code&gt;x&lt;/code&gt; 。如果第二个组件与每个副本不同（ &lt;code&gt;a&lt;/code&gt; ， &lt;code&gt;b&lt;/code&gt; ，...），则合并值将具有从副本设备到不同值的包装映射。如果第三个组件是镜像变量的成员（ &lt;code&gt;v&lt;/code&gt; 将 &lt;code&gt;d0&lt;/code&gt; 映射到 &lt;code&gt;v0&lt;/code&gt; ，将 &lt;code&gt;d1&lt;/code&gt; 映射到&lt;a href=&quot;../compat/v1&quot;&gt; &lt;code&gt;v1&lt;/code&gt; &lt;/a&gt;等），则合并后的结果将是该镜像变量（ &lt;code&gt;v&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="447e8380a0660bcbaf0b5a0132d4263bb7475a27" translate="yes" xml:space="preserve">
          <source>Unwraps an object into a list of TFDecorators and a final target.</source>
          <target state="translated">将一个对象解包成一个TFDecorators列表和一个最终目标。</target>
        </trans-unit>
        <trans-unit id="f84ad806586080c9477df4b9148217324f31d72e" translate="yes" xml:space="preserve">
          <source>Up-to-date gradients (i.e., time step at which gradient was computed is equal to the accumulator's time step) are added to the accumulator.</source>
          <target state="translated">最新的梯度(即计算梯度的时间步数等于累加器的时间步数)被添加到累加器中。</target>
        </trans-unit>
        <trans-unit id="82d0fcae0670e010a42fb65be9daf931bbfd317f" translate="yes" xml:space="preserve">
          <source>Update '*var' according to the AdaMax algorithm.</source>
          <target state="translated">根据AdaMax算法更新'*var'。</target>
        </trans-unit>
        <trans-unit id="e38d308cee1cac8d9c2edb27c5ab629ef7fac7d9" translate="yes" xml:space="preserve">
          <source>Update '*var' according to the Adam algorithm.</source>
          <target state="translated">根据亚当算法更新'*var'。</target>
        </trans-unit>
        <trans-unit id="1b43e05874006f2166053362d39efb24c1da40cb" translate="yes" xml:space="preserve">
          <source>Update '*var' according to the AddSign update.</source>
          <target state="translated">根据AddSign的更新,更新'*var'。</target>
        </trans-unit>
        <trans-unit id="52d1d116ee95e78e3bbfd1ce841dc358b8929317" translate="yes" xml:space="preserve">
          <source>Update '*var' according to the Ftrl-proximal scheme.</source>
          <target state="translated">根据Ftrl-proximal方案更新'*var'。</target>
        </trans-unit>
        <trans-unit id="471071d3477d079bfcbcf7383ae1d699f1fecd50" translate="yes" xml:space="preserve">
          <source>Update '*var' according to the RMSProp algorithm.</source>
          <target state="translated">根据RMSProp算法更新'*var'。</target>
        </trans-unit>
        <trans-unit id="ff81ee48686edeb07f152d5d3567bb2505faedfb" translate="yes" xml:space="preserve">
          <source>Update '*var' according to the adadelta scheme.</source>
          <target state="translated">根据adadelta方案更新'*var'。</target>
        </trans-unit>
        <trans-unit id="5d5ebbbd0b1837c20103e258d32dcbcdda20ba7d" translate="yes" xml:space="preserve">
          <source>Update '*var' according to the adagrad scheme.</source>
          <target state="translated">根据adagrad方案更新'*var'。</target>
        </trans-unit>
        <trans-unit id="868d113999a0543f3fad5d44d43245c1668a6d37" translate="yes" xml:space="preserve">
          <source>Update '*var' according to the centered RMSProp algorithm.</source>
          <target state="translated">根据居中的RMSProp算法更新'*var'。</target>
        </trans-unit>
        <trans-unit id="e800b9c7484bc8710e98c151f2fff6a2fd5d03ac" translate="yes" xml:space="preserve">
          <source>Update '*var' according to the momentum scheme.</source>
          <target state="translated">根据动量方案更新'*var'。</target>
        </trans-unit>
        <trans-unit id="9d77d08a52fed9d23da57628bfa85afe8cc236bc" translate="yes" xml:space="preserve">
          <source>Update '*var' according to the proximal adagrad scheme.</source>
          <target state="translated">根据近端adagrad方案更新'*var'。</target>
        </trans-unit>
        <trans-unit id="787674ac15a7f58c7a9bd7dc691b153215067ce5" translate="yes" xml:space="preserve">
          <source>Update '*var' as FOBOS algorithm with fixed learning rate.</source>
          <target state="translated">更新'*var'为FOBOS算法,学习率固定。</target>
        </trans-unit>
        <trans-unit id="abf5d76205f9777481707bb7900ee4d7f09321a8" translate="yes" xml:space="preserve">
          <source>Update '*var' by subtracting 'alpha' * 'delta' from it.</source>
          <target state="translated">更新'*var',从中减去'alpha'*'delta'。</target>
        </trans-unit>
        <trans-unit id="9efdd46b9ea48522d76a8280bde64699fbd0f6a5" translate="yes" xml:space="preserve">
          <source>Update '&lt;em&gt;var' and '&lt;/em&gt;accum' according to FOBOS with Adagrad learning rate.</source>
          <target state="translated">根据FOBOS使用Adagrad学习率更新' &lt;em&gt;var'和'&lt;/em&gt; accum'。</target>
        </trans-unit>
        <trans-unit id="fe20dc33f34485fdead6adf9d07e849e8a936d69" translate="yes" xml:space="preserve">
          <source>Update 'ref' by adding 'value' to it.</source>
          <target state="translated">通过添加 &quot;值 &quot;来更新 &quot;ref&quot;。</target>
        </trans-unit>
        <trans-unit id="c9257701af71d4f765da34dcd01e8df105b8aa44" translate="yes" xml:space="preserve">
          <source>Update 'ref' by assigning 'value' to it.</source>
          <target state="translated">通过赋予'值'来更新'ref'。</target>
        </trans-unit>
        <trans-unit id="29365e08f06cd2686f25f0d377b66ff5415b759c" translate="yes" xml:space="preserve">
          <source>Update 'ref' by subtracting 'value' from it.</source>
          <target state="translated">更新'ref',从中减去'value'。</target>
        </trans-unit>
        <trans-unit id="6fe8e246cbfb66ec49e8326c839db8c07c5c7366" translate="yes" xml:space="preserve">
          <source>Update (</source>
          <target state="translated">更新(</target>
        </trans-unit>
        <trans-unit id="6509bc67c069e61a7a1a3a68aa4391deff81b6d2" translate="yes" xml:space="preserve">
          <source>Update &lt;code&gt;ref&lt;/code&gt; by adding &lt;code&gt;value&lt;/code&gt; to it.</source>
          <target state="translated">通过添加 &lt;code&gt;ref&lt;/code&gt; &lt;code&gt;value&lt;/code&gt; 来更新参考值。</target>
        </trans-unit>
        <trans-unit id="05a75e8ad72ec7a17916b0418183b2ae49757686" translate="yes" xml:space="preserve">
          <source>Update &lt;code&gt;ref&lt;/code&gt; by assigning &lt;code&gt;value&lt;/code&gt; to it.</source>
          <target state="translated">通过为其分配 &lt;code&gt;value&lt;/code&gt; 来更新 &lt;code&gt;ref&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0da9a4ecf7a09eb5b3d6a68070c44c71f746dcb9" translate="yes" xml:space="preserve">
          <source>Update &lt;code&gt;ref&lt;/code&gt; by subtracting &lt;code&gt;value&lt;/code&gt; from it.</source>
          <target state="translated">通过从中减去 &lt;code&gt;value&lt;/code&gt; 来更新 &lt;code&gt;ref&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="04cf114f82db9c03d560972fce4e303ff3f2a1fd" translate="yes" xml:space="preserve">
          <source>Update entries in '&lt;em&gt;var' and '&lt;/em&gt;accum' according to the proximal adagrad scheme.</source>
          <target state="translated">根据近端的adagrad方案更新' &lt;em&gt;var'和'&lt;/em&gt; accum'中的条目。</target>
        </trans-unit>
        <trans-unit id="8d6d9dee5acd7031de9e9744560f0e2f29a28a2c" translate="yes" xml:space="preserve">
          <source>Update op.</source>
          <target state="translated">更新执行部分</target>
        </trans-unit>
        <trans-unit id="447e87cca9a900913e2b362703c44b530d8ad36e" translate="yes" xml:space="preserve">
          <source>Update relevant entries in '*var' according to the Ftrl-proximal scheme.</source>
          <target state="translated">根据Ftrl-proximal方案更新'*var'中的相关条目。</target>
        </trans-unit>
        <trans-unit id="97803936ee3881b955dd554c9048e1fb8c7cee65" translate="yes" xml:space="preserve">
          <source>Update relevant entries in '&lt;em&gt;var' and '&lt;/em&gt;accum' according to the adagrad scheme.</source>
          <target state="translated">根据adagrad方案更新' &lt;em&gt;var'和'&lt;/em&gt; accum'中的相关条目。</target>
        </trans-unit>
        <trans-unit id="d6438974b08aca642d6bcdb677ce1d3cc168f6c3" translate="yes" xml:space="preserve">
          <source>Update relevant entries in '&lt;em&gt;var' and '&lt;/em&gt;accum' according to the momentum scheme.</source>
          <target state="translated">根据动量方案更新&amp;ldquo; &lt;em&gt;var&amp;rdquo;和&amp;ldquo;&lt;/em&gt; accum&amp;rdquo;中的相关条目。</target>
        </trans-unit>
        <trans-unit id="3f3b6f8527ee9f9f2b48e9baa56537bfbb1fe75d" translate="yes" xml:space="preserve">
          <source>Update rule for parameter &lt;code&gt;w&lt;/code&gt; with gradient &lt;code&gt;g&lt;/code&gt; when &lt;code&gt;momentum&lt;/code&gt; is 0:</source>
          <target state="translated">&lt;code&gt;momentum&lt;/code&gt; 为0时使用梯度 &lt;code&gt;g&lt;/code&gt; 更新参数 &lt;code&gt;w&lt;/code&gt; 的规则：</target>
        </trans-unit>
        <trans-unit id="63b0c5665a5cecb00d89f2c99aeee6da821e96bf" translate="yes" xml:space="preserve">
          <source>Update rule when &lt;code&gt;momentum&lt;/code&gt; is larger than 0:</source>
          <target state="translated">&lt;code&gt;momentum&lt;/code&gt; 大于0时更新规则：</target>
        </trans-unit>
        <trans-unit id="d3f52bac13b63cbb6e97bf073a84b716bbe078b6" translate="yes" xml:space="preserve">
          <source>Update step:</source>
          <target state="translated">更新步骤:</target>
        </trans-unit>
        <trans-unit id="a77038db17ca8f9cacdb34920018f09eddc93c47" translate="yes" xml:space="preserve">
          <source>Update the last triggered time and step number.</source>
          <target state="translated">更新上次触发的时间和步数。</target>
        </trans-unit>
        <trans-unit id="d386d7ccaa88b6dedf1b5040c55b451c526442d6" translate="yes" xml:space="preserve">
          <source>Update the value of &lt;code&gt;x&lt;/code&gt; by adding &lt;code&gt;increment&lt;/code&gt;.</source>
          <target state="translated">通过增加 &lt;code&gt;increment&lt;/code&gt; 更新 &lt;code&gt;x&lt;/code&gt; 的值。</target>
        </trans-unit>
        <trans-unit id="bbd7754fb14c25638e64d5d064bb72937445b6c4" translate="yes" xml:space="preserve">
          <source>Update the value of &lt;code&gt;x&lt;/code&gt; by subtracting &lt;code&gt;decrement&lt;/code&gt;.</source>
          <target state="translated">通过减去 &lt;code&gt;decrement&lt;/code&gt; 更新 &lt;code&gt;x&lt;/code&gt; 的值。</target>
        </trans-unit>
        <trans-unit id="bdd024c3635252f3a95af60168a7a02c85468f31" translate="yes" xml:space="preserve">
          <source>Updated base class for optimizers.</source>
          <target state="translated">更新了优化器的基类。</target>
        </trans-unit>
        <trans-unit id="f63d62e4ed45a361aebb6397e1fccc02f07b3ec2" translate="yes" xml:space="preserve">
          <source>Updates eval metrics. See &lt;code&gt;base_head.Head&lt;/code&gt; for details.</source>
          <target state="translated">更新评估指标。有关详细信息，请参见 &lt;code&gt;base_head.Head&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="25882cbb9d1b53825b652d00bf42872915dbaa9e" translate="yes" xml:space="preserve">
          <source>Updates internal vocabulary based on a list of sequences.</source>
          <target state="translated">根据序列列表更新内部词汇。</target>
        </trans-unit>
        <trans-unit id="852460ee15870b6e90ad0684cbc07e2e2a1a46d1" translate="yes" xml:space="preserve">
          <source>Updates internal vocabulary based on a list of texts.</source>
          <target state="translated">根据文本列表更新内部词汇。</target>
        </trans-unit>
        <trans-unit id="bf38595a5f74f710192dab7d4c5a5b9f35f64f69" translate="yes" xml:space="preserve">
          <source>Updates loss scale based on if gradients are finite in current step.</source>
          <target state="translated">根据当前步骤中梯度是否有限来更新损失比例。</target>
        </trans-unit>
        <trans-unit id="3ff6b49a83b568897a31a71591847bcb981cfe8c" translate="yes" xml:space="preserve">
          <source>Updates metric objects and returns a &lt;code&gt;dict&lt;/code&gt; of the updated metrics.</source>
          <target state="translated">更新度量标准对象并返回更新后的度量标准的 &lt;code&gt;dict&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c1444a8720b007d3bc3efc14cd0416a718bdfe20" translate="yes" xml:space="preserve">
          <source>Updates specified rows 'i' with values 'v'.</source>
          <target state="translated">用'v'更新指定的行'i'。</target>
        </trans-unit>
        <trans-unit id="da9e8385ef1cee6fd5a3ef89050fbd2471b13c5e" translate="yes" xml:space="preserve">
          <source>Updates the accumulator with a new value for global_step.</source>
          <target state="translated">用 global_step 的新值更新累加器。</target>
        </trans-unit>
        <trans-unit id="04014643289292114c04f072ec1d42a7ffcf535f" translate="yes" xml:space="preserve">
          <source>Updates the content of the 'checkpoint' file. (deprecated)</source>
          <target state="translated">更新'检查点'文件的内容。(已废弃)</target>
        </trans-unit>
        <trans-unit id="e27ee7c5bc27400dd7ff516cc28fe06e87e352bf" translate="yes" xml:space="preserve">
          <source>Updates the method name(s) of the SavedModel stored in the given path.</source>
          <target state="translated">更新存储在给定路径中的SavedModel的方法名。</target>
        </trans-unit>
        <trans-unit id="114215d83f25c29dd880192b62cf9c4d735d8aca" translate="yes" xml:space="preserve">
          <source>Updates the progress bar.</source>
          <target state="translated">更新进度条。</target>
        </trans-unit>
        <trans-unit id="f3aa5183ae2c5fffe19c95fc5154e5548921fe2d" translate="yes" xml:space="preserve">
          <source>Updates the shape of a tensor and checks at runtime that the shape holds.</source>
          <target state="translated">更新张量的形状,并在运行时检查形状是否保持。</target>
        </trans-unit>
        <trans-unit id="d45fc3ae1d9fcc9b117583faaa29b9c68525b6d1" translate="yes" xml:space="preserve">
          <source>Updates the shape of this tensor.</source>
          <target state="translated">更新此张量的形状。</target>
        </trans-unit>
        <trans-unit id="f590327e9570d8c3a01e570e103d03bfc643e1c5" translate="yes" xml:space="preserve">
          <source>Updates the table to associates keys with values.</source>
          <target state="translated">更新表,将键与值关联起来。</target>
        </trans-unit>
        <trans-unit id="cfb5af8bff1bdbad2b00e834b01497c1e3817b80" translate="yes" xml:space="preserve">
          <source>Updates the tree ensemble by adding a layer to the last tree being grown</source>
          <target state="translated">通过给最后一棵正在生长的树添加一个图层来更新树的集合。</target>
        </trans-unit>
        <trans-unit id="e6834941215f45b3c876acacc8e8c580f1e32c40" translate="yes" xml:space="preserve">
          <source>Updates the tree ensemble by either adding a layer to the last tree being grown</source>
          <target state="translated">通过在最后一棵树上添加一个图层来更新树的集合。</target>
        </trans-unit>
        <trans-unit id="dd283fb80513334be997d84fc1ce654a3740f2f1" translate="yes" xml:space="preserve">
          <source>Updates the value of the loss scale.</source>
          <target state="translated">更新损失等级的值。</target>
        </trans-unit>
        <trans-unit id="56d5a06c3dd0023752b72f3ca770e1ae3effe80d" translate="yes" xml:space="preserve">
          <source>Updates this variable with the max of &lt;a href=&quot;../../indexedslices&quot;&gt;&lt;code&gt;tf.IndexedSlices&lt;/code&gt;&lt;/a&gt; and itself.</source>
          <target state="translated">用最大&lt;a href=&quot;../../indexedslices&quot;&gt; &lt;code&gt;tf.IndexedSlices&lt;/code&gt; &lt;/a&gt;及其本身更新此变量。</target>
        </trans-unit>
        <trans-unit id="0b0e1aa88bedb98a31d52dc0db2aabdbdc628dd2" translate="yes" xml:space="preserve">
          <source>Updates this variable with the max of &lt;a href=&quot;indexedslices&quot;&gt;&lt;code&gt;tf.IndexedSlices&lt;/code&gt;&lt;/a&gt; and itself.</source>
          <target state="translated">用最大&lt;a href=&quot;indexedslices&quot;&gt; &lt;code&gt;tf.IndexedSlices&lt;/code&gt; &lt;/a&gt;及其本身更新此变量。</target>
        </trans-unit>
        <trans-unit id="729d65b580f1272ed91207fc8a10aa3312f5c72c" translate="yes" xml:space="preserve">
          <source>Updates this variable with the min of &lt;a href=&quot;../../indexedslices&quot;&gt;&lt;code&gt;tf.IndexedSlices&lt;/code&gt;&lt;/a&gt; and itself.</source>
          <target state="translated">用&lt;a href=&quot;../../indexedslices&quot;&gt; &lt;code&gt;tf.IndexedSlices&lt;/code&gt; &lt;/a&gt;及其自身的最小值更新此变量。</target>
        </trans-unit>
        <trans-unit id="ee9c6cf89fa6d5d3bc7214b300a7628504d2e372" translate="yes" xml:space="preserve">
          <source>Updates this variable with the min of &lt;a href=&quot;indexedslices&quot;&gt;&lt;code&gt;tf.IndexedSlices&lt;/code&gt;&lt;/a&gt; and itself.</source>
          <target state="translated">用&lt;a href=&quot;indexedslices&quot;&gt; &lt;code&gt;tf.IndexedSlices&lt;/code&gt; &lt;/a&gt;及其自身的最小值更新此变量。</target>
        </trans-unit>
        <trans-unit id="b62b62db1c38cdd8d87bd06d704758f1b3977490" translate="yes" xml:space="preserve">
          <source>Updating and clearing custom objects using &lt;code&gt;custom_object_scope&lt;/code&gt; is preferred, but &lt;code&gt;get_custom_objects&lt;/code&gt; can be used to directly access &lt;code&gt;_GLOBAL_CUSTOM_OBJECTS&lt;/code&gt;.</source>
          <target state="translated">首选使用 &lt;code&gt;custom_object_scope&lt;/code&gt; 更新和清除自定义对象，但是可以使用 &lt;code&gt;get_custom_objects&lt;/code&gt; 直接访问 &lt;code&gt;_GLOBAL_CUSTOM_OBJECTS&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="cd72d1503f9d3fb82ce4c9572b6d9a18a62eb0ba" translate="yes" xml:space="preserve">
          <source>Updating and clearing custom objects using &lt;code&gt;custom_object_scope&lt;/code&gt; is preferred, but &lt;code&gt;get_custom_objects&lt;/code&gt; can be used to directly access the current collection of custom objects.</source>
          <target state="translated">首选使用 &lt;code&gt;custom_object_scope&lt;/code&gt; 更新和清除自定义对象，但是可以使用 &lt;code&gt;get_custom_objects&lt;/code&gt; 直接访问当前的自定义对象集合。</target>
        </trans-unit>
        <trans-unit id="281cba0fb3149b6c3774ed01f37649638874e882" translate="yes" xml:space="preserve">
          <source>Upon a load, the subset of variables and assets supplied as part of the specific meta graph def, will be restored into the supplied session. The values of the variables though will correspond to the saved values from the first meta graph added to the SavedModel using &lt;code&gt;add_meta_graph_and_variables(...)&lt;/code&gt; in &lt;code&gt;builder.py&lt;/code&gt;.</source>
          <target state="translated">加载后，作为特定元图def的一部分提供的变量和资产的子集将恢复到提供的会话中。变量的值，但将对应于保存的值从使用添加到SavedModel所述第一超图 &lt;code&gt;add_meta_graph_and_variables(...)&lt;/code&gt; 在 &lt;code&gt;builder.py&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="363a67d86f68374ba8c4a4cfbabbae29d79775ae" translate="yes" xml:space="preserve">
          <source>Upon removal from the active set, a checkpoint will be preserved if it has been at least &lt;code&gt;keep_checkpoint_every_n_hours&lt;/code&gt; since the last preserved checkpoint. The default setting of &lt;code&gt;None&lt;/code&gt; does not preserve any checkpoints in this way.</source>
          <target state="translated">从活动集中删除后，如果检查点距离上一个保留的检查点至少 &lt;code&gt;keep_checkpoint_every_n_hours&lt;/code&gt; ，则将保留该检查点。默认设置&amp;ldquo; &lt;code&gt;None&lt;/code&gt; 不会以这种方式保留任何检查点。</target>
        </trans-unit>
        <trans-unit id="3ceee3a3a486ed4178c6d9fbac07e5800c6e5b47" translate="yes" xml:space="preserve">
          <source>Upper bound on the number of partitions. Defaults to 1.</source>
          <target state="translated">分区数量的上界。默认为1。</target>
        </trans-unit>
        <trans-unit id="5e44dc51c175ff5c0db1e82d8dfe21d3d95479e8" translate="yes" xml:space="preserve">
          <source>Upper boundary of the output interval.</source>
          <target state="translated">输出区间的上界。</target>
        </trans-unit>
        <trans-unit id="19aecdfa7cdf02c6ca74a44e34017ebb74fcec55" translate="yes" xml:space="preserve">
          <source>UpperBound</source>
          <target state="translated">UpperBound</target>
        </trans-unit>
        <trans-unit id="c31d78903813b3bcaeecbae756ddce1af8f35bae" translate="yes" xml:space="preserve">
          <source>Upsampling layer for 1D inputs.</source>
          <target state="translated">1D输入的取样层。</target>
        </trans-unit>
        <trans-unit id="7b0d1a97658971e96b3f9be123da52362174eefa" translate="yes" xml:space="preserve">
          <source>Upsampling layer for 2D inputs.</source>
          <target state="translated">2D输入的取样层。</target>
        </trans-unit>
        <trans-unit id="d8e99e181057faa2a85bca621f7cd639c1901c16" translate="yes" xml:space="preserve">
          <source>Upsampling layer for 3D inputs.</source>
          <target state="translated">3D输入的取样层。</target>
        </trans-unit>
        <trans-unit id="0bb18642b70b9f8a9c12ccf39487328f306b8e19" translate="yes" xml:space="preserve">
          <source>Usage</source>
          <target state="translated">Usage</target>
        </trans-unit>
        <trans-unit id="9629fc5be7eebf84f66a1a4fe14d7eee29fbc498" translate="yes" xml:space="preserve">
          <source>Usage Example:</source>
          <target state="translated">用例。</target>
        </trans-unit>
        <trans-unit id="c697e1f98f6b38e0ebf986cdde5209243dc5a1fb" translate="yes" xml:space="preserve">
          <source>Usage example with &lt;a href=&quot;../mobilenet&quot;&gt;&lt;code&gt;applications.MobileNet&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">&lt;a href=&quot;../mobilenet&quot;&gt; &lt;code&gt;applications.MobileNet&lt;/code&gt; &lt;/a&gt;用法示例：</target>
        </trans-unit>
        <trans-unit id="7878482cb2d732b2ccb44cbc91378bf291581491" translate="yes" xml:space="preserve">
          <source>Usage example with &lt;a href=&quot;../strategy&quot;&gt;&lt;code&gt;tf.distribute.Strategy&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">&lt;a href=&quot;../strategy&quot;&gt; &lt;code&gt;tf.distribute.Strategy&lt;/code&gt; 的&lt;/a&gt;用法示例：</target>
        </trans-unit>
        <trans-unit id="b925d1a88d8e7cc54989fe4e8d7270088645c5c3" translate="yes" xml:space="preserve">
          <source>Usage example with tf.distribute.Strategy:</source>
          <target state="translated">tf.distribution.Strategy.用法示例。</target>
        </trans-unit>
        <trans-unit id="fec43ce445f974147bd0eb223a50147e7fb7202d" translate="yes" xml:space="preserve">
          <source>Usage example:</source>
          <target state="translated">用例。</target>
        </trans-unit>
        <trans-unit id="77817d0a0c43f5e8811381e684f28870ad0caaef" translate="yes" xml:space="preserve">
          <source>Usage in a &lt;a href=&quot;function&quot;&gt;&lt;code&gt;tf.function&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">在&lt;a href=&quot;function&quot;&gt; &lt;code&gt;tf.function&lt;/code&gt; &lt;/a&gt;中的用法：</target>
        </trans-unit>
        <trans-unit id="801e743874a5f137b8515158c4ad1c682d3d5c9a" translate="yes" xml:space="preserve">
          <source>Usage in a functional model:</source>
          <target state="translated">在功能模型中的使用。</target>
        </trans-unit>
        <trans-unit id="f55d9b06976fb1e7cdaee8fa9d5e54cd7e87ed03" translate="yes" xml:space="preserve">
          <source>Usage in custom training loops</source>
          <target state="translated">在自定义训练循环中的使用</target>
        </trans-unit>
        <trans-unit id="f7229b161af4f523b13ed9ebb0ca67db64df129d" translate="yes" xml:space="preserve">
          <source>Usage with &lt;code&gt;compile()&lt;/code&gt; API:</source>
          <target state="translated">与 &lt;code&gt;compile()&lt;/code&gt; API结合使用：</target>
        </trans-unit>
        <trans-unit id="e990e47405f0ff9e5047d2be7faad8352983c3d7" translate="yes" xml:space="preserve">
          <source>Usage with a canned estimator:</source>
          <target state="translated">与罐装估计器一起使用。</target>
        </trans-unit>
        <trans-unit id="b7f68eccfb39ab6d5044b1fe94d8202e173daf81" translate="yes" xml:space="preserve">
          <source>Usage with distribution strategy and custom training loop:</source>
          <target state="translated">配合分销策略和自定义培训循环使用。</target>
        </trans-unit>
        <trans-unit id="e394973fc15ca8e6a0ab7b5e15b6537c110fcd61" translate="yes" xml:space="preserve">
          <source>Usage with tf.keras API:</source>
          <target state="translated">与 tf.keras API 一起使用。</target>
        </trans-unit>
        <trans-unit id="b672bfd407587ac4347c7d14f539f0167c943965" translate="yes" xml:space="preserve">
          <source>Usage with the &lt;a href=&quot;../../keras&quot;&gt;&lt;code&gt;tf.keras&lt;/code&gt;&lt;/a&gt; API:</source>
          <target state="translated">&lt;a href=&quot;../../keras&quot;&gt; &lt;code&gt;tf.keras&lt;/code&gt; &lt;/a&gt; API的用法：</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
