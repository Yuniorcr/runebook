<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="360796ce4b88f8a72813e93a376d34c0228713ee" translate="yes" xml:space="preserve">
          <source>Add histogram to summary.</source>
          <target state="translated">在摘要中添加直方图。</target>
        </trans-unit>
        <trans-unit id="1b6b7d6099858bcb255707e61a495522a0414994" translate="yes" xml:space="preserve">
          <source>Add image data to summary.</source>
          <target state="translated">将图像数据添加到摘要中。</target>
        </trans-unit>
        <trans-unit id="a392c90962df4acfb05e6d0ec46fd5a47980ff9d" translate="yes" xml:space="preserve">
          <source>Add meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check &lt;a href=&quot;https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene&quot;&gt;https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene&lt;/a&gt; for advanced usage.</source>
          <target state="translated">将网格或3D点云添加到TensorBoard。可视化基于Three.js，因此它允许用户与渲染的对象进行交互。除了诸如顶点，脸上的基本定义，用户可以进一步提供摄像机参数，照明条件等，请检查&lt;a href=&quot;https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene&quot;&gt;https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene&lt;/a&gt;为高级用法。</target>
        </trans-unit>
        <trans-unit id="1a9584fc26e356ef39ba9497c010651d8eae52d2" translate="yes" xml:space="preserve">
          <source>Add observer for the leaf child of the module.</source>
          <target state="translated">为模块的叶子增加观察者。</target>
        </trans-unit>
        <trans-unit id="701a19440eea643652da7e69f517b8c3d995eacc" translate="yes" xml:space="preserve">
          <source>Add scalar data to summary.</source>
          <target state="translated">在摘要中添加标量数据。</target>
        </trans-unit>
        <trans-unit id="7a54b0b6b6b02f66bd6beaecffa1d99a5f7eb192" translate="yes" xml:space="preserve">
          <source>Add text data to summary.</source>
          <target state="translated">在摘要中添加文本数据。</target>
        </trans-unit>
        <trans-unit id="c77f8dadb528de130d984e34a450fb3582dda9aa" translate="yes" xml:space="preserve">
          <source>Add video data to summary.</source>
          <target state="translated">将视频数据添加到摘要中。</target>
        </trans-unit>
        <trans-unit id="8afe6f3185ed76e023f80393638df4f157d48f52" translate="yes" xml:space="preserve">
          <source>Adding export support for operators is an &lt;em&gt;advance usage&lt;/em&gt;.</source>
          <target state="translated">为运营商增加出口支持是一种&lt;em&gt;先行&lt;/em&gt;做法。</target>
        </trans-unit>
        <trans-unit id="4c9c6ec239352be639e5bda7f0d0b398caeb9237" translate="yes" xml:space="preserve">
          <source>Adding support for operators</source>
          <target state="translated">增加对运营商的支持</target>
        </trans-unit>
        <trans-unit id="4e032cc35826dbb2a4a560c47d8b0573e59463ab" translate="yes" xml:space="preserve">
          <source>Additional args:</source>
          <target state="translated">额外的参数。</target>
        </trans-unit>
        <trans-unit id="b02695e3612016d107e21b0824bdec94963550bb" translate="yes" xml:space="preserve">
          <source>Additionally accepts an optional &lt;code&gt;reduce&lt;/code&gt; argument that allows specification of an optional reduction operation, which is applied to all values in the tensor &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indicies specified in the &lt;code&gt;index&lt;/code&gt;. For each value in &lt;code&gt;src&lt;/code&gt;, the reduction operation is applied to an index in &lt;code&gt;self&lt;/code&gt; which is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt;.</source>
          <target state="translated">此外接受可选 &lt;code&gt;reduce&lt;/code&gt; 参数，可任选的缩小操作，其被应用到在张量的所有值的规范 &lt;code&gt;src&lt;/code&gt; 到 &lt;code&gt;self&lt;/code&gt; 在指定的indicies &lt;code&gt;index&lt;/code&gt; 。对于 &lt;code&gt;src&lt;/code&gt; 中的每个值，将减少操作应用于 &lt;code&gt;self&lt;/code&gt; 中的索引，该索引由其在 &lt;code&gt;src&lt;/code&gt; 中的索引对于 &lt;code&gt;dimension != dim&lt;/code&gt; 的值和由在 &lt;code&gt;index&lt;/code&gt; 中的对应值指定的 &lt;code&gt;dimension = dim&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d9baa44eaf376e383c39686ddc11f7bd15a1ea88" translate="yes" xml:space="preserve">
          <source>Adds a buffer to the module.</source>
          <target state="translated">为模块添加一个缓冲区。</target>
        </trans-unit>
        <trans-unit id="677c1d2632a65843e9a01046b892b48b9f5176ac" translate="yes" xml:space="preserve">
          <source>Adds a child module to the current module.</source>
          <target state="translated">在当前模块中添加一个子模块。</target>
        </trans-unit>
        <trans-unit id="96b77db46874289e2dd12b6bc9489ce00f46440a" translate="yes" xml:space="preserve">
          <source>Adds a child pruning &lt;code&gt;method&lt;/code&gt; to the container.</source>
          <target state="translated">向容器添加子修剪 &lt;code&gt;method&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f9bccaca3fe9322a4122d15d5e4fdb25202c0320" translate="yes" xml:space="preserve">
          <source>Adds a parameter to the module.</source>
          <target state="translated">为模块添加一个参数。</target>
        </trans-unit>
        <trans-unit id="7fc7b19e96d1e48b76f041d88e6e8c8791a06fa6" translate="yes" xml:space="preserve">
          <source>Adds all values from the tensor &lt;code&gt;other&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indices specified in the &lt;code&gt;index&lt;/code&gt; tensor in a similar fashion as &lt;a href=&quot;#torch.Tensor.scatter_&quot;&gt;&lt;code&gt;scatter_()&lt;/code&gt;&lt;/a&gt;. For each value in &lt;code&gt;src&lt;/code&gt;, it is added to an index in &lt;code&gt;self&lt;/code&gt; which is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt;.</source>
          <target state="translated">以类似于&lt;a href=&quot;#torch.Tensor.scatter_&quot;&gt; &lt;code&gt;scatter_()&lt;/code&gt; 的&lt;/a&gt;方式，将张量 &lt;code&gt;other&lt;/code&gt; 所有值添加到 &lt;code&gt;index&lt;/code&gt; 张量中指定的索引处的 &lt;code&gt;self&lt;/code&gt; 中。对于中的每个值 &lt;code&gt;src&lt;/code&gt; ，它被添加到索引中的 &lt;code&gt;self&lt;/code&gt; 其通过它的索引中指定 &lt;code&gt;src&lt;/code&gt; 为 &lt;code&gt;dimension != dim&lt;/code&gt; ，并通过在相应的值 &lt;code&gt;index&lt;/code&gt; 为 &lt;code&gt;dimension = dim&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="8562c008b76c53309d3a0973a409f19d43babb11" translate="yes" xml:space="preserve">
          <source>Adds many scalar data to summary.</source>
          <target state="translated">在摘要中添加许多标量数据。</target>
        </trans-unit>
        <trans-unit id="8058a9f23e00e9bb14b6abf55b56e8a8543f9c29" translate="yes" xml:space="preserve">
          <source>Adds precision recall curve. Plotting a precision-recall curve lets you understand your model&amp;rsquo;s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.</source>
          <target state="translated">添加精度召回曲线。绘制精确调用曲线可让您了解在不同阈值设置下模型的性能。使用此功能，您可以为每个目标提供地面真相标签（T / F）和预测置信度（通常是模型的输出）。TensorBoard UI将允许您交互式地选择阈值。</target>
        </trans-unit>
        <trans-unit id="fd54948ee53264250bb1b351262b4ccb8986d94e" translate="yes" xml:space="preserve">
          <source>Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.</source>
          <target state="translated">增加了前向预钩,可以在飞行中进行修剪,并根据原始张量和修剪掩模对张量进行重新测量。</target>
        </trans-unit>
        <trans-unit id="ab685d43fc5e49c46e526f76f07f3d59e811972e" translate="yes" xml:space="preserve">
          <source>Adds the scalar &lt;code&gt;other&lt;/code&gt; to each element of the input &lt;code&gt;input&lt;/code&gt; and returns a new resulting tensor.</source>
          <target state="translated">将标量 &lt;code&gt;other&lt;/code&gt; 加到输入 &lt;code&gt;input&lt;/code&gt; 每个元素上，并返回一个新的结果张量。</target>
        </trans-unit>
        <trans-unit id="b2b204082818c243f404c5bfdc3dc16bbc32640c" translate="yes" xml:space="preserve">
          <source>After a class is defined, it can be used in both TorchScript and Python interchangeably like any other TorchScript type:</source>
          <target state="translated">在定义了一个类之后,它可以像其他TorchScript类型一样在TorchScript和Python中互换使用。</target>
        </trans-unit>
        <trans-unit id="cd74631d206c5c267345dcc56490ab6ea1c83f20" translate="yes" xml:space="preserve">
          <source>After an enum is defined, it can be used in both TorchScript and Python interchangeably like any other TorchScript type. The type of the values of an enum must be &lt;code&gt;int&lt;/code&gt;, &lt;code&gt;float&lt;/code&gt;, or &lt;code&gt;str&lt;/code&gt;. All values must be of the same type; heterogenous types for enum values are not supported.</source>
          <target state="translated">定义枚举后，它可以像在任何其他TorchScript类型中一样在TorchScript和Python中互换使用。枚举值的类型必须为 &lt;code&gt;int&lt;/code&gt; ， &lt;code&gt;float&lt;/code&gt; 或 &lt;code&gt;str&lt;/code&gt; 。所有值都必须是同一类型；不支持枚举值的异构类型。</target>
        </trans-unit>
        <trans-unit id="587806098e59106c8c783b918f92e7fc29e4d2ab" translate="yes" xml:space="preserve">
          <source>After fetching a list of samples using the indices from sampler, the function passed as the &lt;code&gt;collate_fn&lt;/code&gt; argument is used to collate lists of samples into batches.</source>
          <target state="translated">使用来自采样器的索引获取样本列表后，作为 &lt;code&gt;collate_fn&lt;/code&gt; 参数传递的函数用于将样本列表整理为批次。</target>
        </trans-unit>
        <trans-unit id="c52a739b9d98878e44d92e91256235a6d294431c" translate="yes" xml:space="preserve">
          <source>After the call &lt;code&gt;tensor&lt;/code&gt; is going to be bitwise identical in all processes.</source>
          <target state="translated">调用之后， &lt;code&gt;tensor&lt;/code&gt; 将在所有过程中按位相同。</target>
        </trans-unit>
        <trans-unit id="f3dd860a8580060d82d526ad04c5badbd57df94f" translate="yes" xml:space="preserve">
          <source>After the call, all 16 tensors on the two nodes will have the all-reduced value of 16</source>
          <target state="translated">调用后,两个节点上的16个时序都会有16的全减值。</target>
        </trans-unit>
        <trans-unit id="e03f01ec1cb89a34bde4816ebeb0fe756b9b7364" translate="yes" xml:space="preserve">
          <source>After the call, all &lt;code&gt;tensor&lt;/code&gt; in &lt;code&gt;tensor_list&lt;/code&gt; is going to be bitwise identical in all processes.</source>
          <target state="translated">通话结束后，所有的 &lt;code&gt;tensor&lt;/code&gt; 在 &lt;code&gt;tensor_list&lt;/code&gt; 将是在所有进程按位相同。</target>
        </trans-unit>
        <trans-unit id="9ae05a9ce240652b1911853b65ddd62f9764383c" translate="yes" xml:space="preserve">
          <source>AlexNet</source>
          <target state="translated">AlexNet</target>
        </trans-unit>
        <trans-unit id="ca5a1956913984160d31d9bf92ec542323ba8065" translate="yes" xml:space="preserve">
          <source>AlexNet model architecture from the &lt;a href=&quot;https://arxiv.org/abs/1404.5997&quot;&gt;&amp;ldquo;One weird trick&amp;hellip;&amp;rdquo;&lt;/a&gt; paper.</source>
          <target state="translated">AlexNet模型体系结构摘自&lt;a href=&quot;https://arxiv.org/abs/1404.5997&quot;&gt;&amp;ldquo;一个怪异的窍门&amp;rdquo;一&lt;/a&gt;文。</target>
        </trans-unit>
        <trans-unit id="52fc4196dd42d00268d82ad7f69953e231b88729" translate="yes" xml:space="preserve">
          <source>Alexnet</source>
          <target state="translated">Alexnet</target>
        </trans-unit>
        <trans-unit id="67e15eb99dc0e473c962d6a494d00e048e9f6fc6" translate="yes" xml:space="preserve">
          <source>Algorithms</source>
          <target state="translated">Algorithms</target>
        </trans-unit>
        <trans-unit id="c4836f5ef10696c1523f7a542472a979fa86940f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;#torch.Tensor.clamp&quot;&gt;&lt;code&gt;clamp()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">clip &lt;a href=&quot;#torch.Tensor.clamp&quot;&gt; &lt;code&gt;clamp()&lt;/code&gt; &lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="251f8be41a3289fe0ff13ca86e83184aaa6d1685" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;#torch.Tensor.clamp_&quot;&gt;&lt;code&gt;clamp_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.clamp_&quot;&gt; &lt;code&gt;clamp_()&lt;/code&gt; &lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="8b1674d7e68b7ce19ae1eac32e5f2d61ba39a3fa" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;#torch.Tensor.dim&quot;&gt;&lt;code&gt;dim()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.dim&quot;&gt; &lt;code&gt;dim()&lt;/code&gt; 的&lt;/a&gt;别名</target>
        </trans-unit>
        <trans-unit id="b806d3eb44bb5995d21b017162c333c30909bfd1" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;#torch.Tensor.numel&quot;&gt;&lt;code&gt;numel()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.numel&quot;&gt; &lt;code&gt;numel()&lt;/code&gt; 的&lt;/a&gt;别名</target>
        </trans-unit>
        <trans-unit id="8003ce6f6a4824ac7e86312aee5a809819964394" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt;&lt;code&gt;abs()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt; &lt;code&gt;abs()&lt;/code&gt; 的&lt;/a&gt;别名</target>
        </trans-unit>
        <trans-unit id="ae0811ab2275dcf323f302291b8578862167855b" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt;&lt;code&gt;torch.abs()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt; &lt;code&gt;torch.abs()&lt;/code&gt; 的&lt;/a&gt;别名</target>
        </trans-unit>
        <trans-unit id="dff47f32b9bc388203bf7e97dfa096239e9aa4fa" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.acos#torch.acos&quot;&gt;&lt;code&gt;torch.acos()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.acos#torch.acos&quot;&gt; &lt;code&gt;torch.acos()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="27f71b5c7ea505c6db6e7c3f268907cac51983c8" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.acosh#torch.acosh&quot;&gt;&lt;code&gt;torch.acosh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.acosh#torch.acosh&quot;&gt; &lt;code&gt;torch.acosh()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="8915eae6d7b9ee1bf0b6f73f5bcf701208d83a87" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.asin#torch.asin&quot;&gt;&lt;code&gt;torch.asin()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.asin#torch.asin&quot;&gt; &lt;code&gt;torch.asin()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="12a1bcfe361d7554bc235392cdf072ae77235de7" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.asinh#torch.asinh&quot;&gt;&lt;code&gt;torch.asinh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.asinh#torch.asinh&quot;&gt; &lt;code&gt;torch.asinh()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="21de09801d295b9e55c5ca95a97e6b707bdd91c4" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.atan#torch.atan&quot;&gt;&lt;code&gt;torch.atan()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.atan#torch.atan&quot;&gt; &lt;code&gt;torch.atan()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="f42b48abe60c1061b77c3d37b15698be8fee9c90" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.atanh#torch.atanh&quot;&gt;&lt;code&gt;torch.atanh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.atanh#torch.atanh&quot;&gt; &lt;code&gt;torch.atanh()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="b913a66219f4d1a88ba91e403c72efb7f3ef3abb" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.clamp#torch.clamp&quot;&gt;&lt;code&gt;torch.clamp()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.clamp#torch.clamp&quot;&gt; &lt;code&gt;torch.clamp()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="4ba0bdfda926bfb5d618f4ee65334d6fb8eb1eee" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.div#torch.div&quot;&gt;&lt;code&gt;torch.div()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.div#torch.div&quot;&gt; &lt;code&gt;torch.div()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="346a30cebac1d8f7c67c1c3a7f4f3bc3613f3f76" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.ge#torch.ge&quot;&gt;&lt;code&gt;torch.ge()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.ge#torch.ge&quot;&gt; &lt;code&gt;torch.ge()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="b313b53bc520608d37513795d2fb79da1dd596f1" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.gt#torch.gt&quot;&gt;&lt;code&gt;torch.gt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.gt#torch.gt&quot;&gt; &lt;code&gt;torch.gt()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="fd6c12495884b27e735c199934fdc7b613fa577d" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.le#torch.le&quot;&gt;&lt;code&gt;torch.le()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.le#torch.le&quot;&gt; &lt;code&gt;torch.le()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="982d40969f8de3053b6a7ba2029aed4446d9284b" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.lt#torch.lt&quot;&gt;&lt;code&gt;torch.lt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.lt#torch.lt&quot;&gt; &lt;code&gt;torch.lt()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="2d9a228af380b06ff04ec79aacc440723157779f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.mul#torch.mul&quot;&gt;&lt;code&gt;torch.mul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.mul#torch.mul&quot;&gt; &lt;code&gt;torch.mul()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="93282bf10a2917c15fe551a51f5161e90b61877f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.ne#torch.ne&quot;&gt;&lt;code&gt;torch.ne()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.ne#torch.ne&quot;&gt; &lt;code&gt;torch.ne()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="5c53bd5a9189ae9c36a9718c39849a71cf4795d9" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.neg#torch.neg&quot;&gt;&lt;code&gt;torch.neg()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.neg#torch.neg&quot;&gt; &lt;code&gt;torch.neg()&lt;/code&gt; 的&lt;/a&gt;别名</target>
        </trans-unit>
        <trans-unit id="84a5bea4d6c5059fe64ceca353dc77a1a8d6f0c7" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.sub#torch.sub&quot;&gt;&lt;code&gt;torch.sub()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.sub#torch.sub&quot;&gt; &lt;code&gt;torch.sub()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="07f60a052509a04e4581fb6abfc90937c2c71d1e" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.trunc#torch.trunc&quot;&gt;&lt;code&gt;torch.trunc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.trunc#torch.trunc&quot;&gt; &lt;code&gt;torch.trunc()&lt;/code&gt; 的&lt;/a&gt;别名</target>
        </trans-unit>
        <trans-unit id="d26757f4ad01130675ba8ea2672c12a2db5b8aab" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.abs#torch.abs&quot;&gt;&lt;code&gt;torch.abs()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;torch.abs#torch.abs&quot;&gt; &lt;code&gt;torch.abs()&lt;/code&gt; 的&lt;/a&gt;别名</target>
        </trans-unit>
        <trans-unit id="f1445190c40314f7662b67191fe5caef59adeec0" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.acos#torch.acos&quot;&gt;&lt;code&gt;torch.acos()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.acos#torch.acos&quot;&gt; &lt;code&gt;torch.acos()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="b7d58a750debce958ceee5c25ad01617693d3ea2" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.acosh#torch.acosh&quot;&gt;&lt;code&gt;torch.acosh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.acosh#torch.acosh&quot;&gt; &lt;code&gt;torch.acosh()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="3aec840f94f18b783a10543e6249d4a95dd33cf1" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.asin#torch.asin&quot;&gt;&lt;code&gt;torch.asin()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.asin#torch.asin&quot;&gt; &lt;code&gt;torch.asin()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="606d0a5ac071a495928d8903629e0d3c2c53055d" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.asinh#torch.asinh&quot;&gt;&lt;code&gt;torch.asinh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.asinh#torch.asinh&quot;&gt; &lt;code&gt;torch.asinh()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="dee63cafa3f7f5b85ddbc8fc58438f4ee2396691" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.atan#torch.atan&quot;&gt;&lt;code&gt;torch.atan()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.atan#torch.atan&quot;&gt; &lt;code&gt;torch.atan()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="8c0014cc8ab298138389abb57c1b96765b473b42" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.atanh#torch.atanh&quot;&gt;&lt;code&gt;torch.atanh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.atanh#torch.atanh&quot;&gt; &lt;code&gt;torch.atanh()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="23c4daa625a3c02d837a48a43fcee80a5cf020be" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.clamp#torch.clamp&quot;&gt;&lt;code&gt;torch.clamp()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.clamp#torch.clamp&quot;&gt; &lt;code&gt;torch.clamp()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="2cc182b7bdf6fcacb622e0e829ca673368ffde7e" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.div#torch.div&quot;&gt;&lt;code&gt;torch.div()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.div#torch.div&quot;&gt; &lt;code&gt;torch.div()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="caa996e0ac1f8de7d67f7455447758713ba8feee" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.ge#torch.ge&quot;&gt;&lt;code&gt;torch.ge()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.ge#torch.ge&quot;&gt; &lt;code&gt;torch.ge()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="392a801bbfecc8b02cebb4cb713410918e491a2f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.gt#torch.gt&quot;&gt;&lt;code&gt;torch.gt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.gt#torch.gt&quot;&gt; &lt;code&gt;torch.gt()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="9b13ac85fa2f8f4adaad80131c925608d0b684eb" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.le#torch.le&quot;&gt;&lt;code&gt;torch.le()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.le#torch.le&quot;&gt; &lt;code&gt;torch.le()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="6b6841b18d2b1421ff7068e9ab0cf0387e0beeda" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.lt#torch.lt&quot;&gt;&lt;code&gt;torch.lt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.lt#torch.lt&quot;&gt; &lt;code&gt;torch.lt()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="4bb2a8621505158fea7a3e7e83a0736217506d1f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.mul#torch.mul&quot;&gt;&lt;code&gt;torch.mul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.mul#torch.mul&quot;&gt; &lt;code&gt;torch.mul()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="3eaccfa1051feb57b0d0c06ee1f7f0ca18ed2f48" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.ne#torch.ne&quot;&gt;&lt;code&gt;torch.ne()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.ne#torch.ne&quot;&gt; &lt;code&gt;torch.ne()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="80bfac967c271e4287120446d7e735504a25deee" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.neg#torch.neg&quot;&gt;&lt;code&gt;torch.neg()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;torch.neg#torch.neg&quot;&gt; &lt;code&gt;torch.neg()&lt;/code&gt; 的&lt;/a&gt;别名</target>
        </trans-unit>
        <trans-unit id="63adb787982c4f010005f5042a7176ac57a5c3dd" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.sub#torch.sub&quot;&gt;&lt;code&gt;torch.sub()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.sub#torch.sub&quot;&gt; &lt;code&gt;torch.sub()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="004a1c4ca4c726826c71b8d3f3c668a30c19eca5" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.trunc#torch.trunc&quot;&gt;&lt;code&gt;torch.trunc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;torch.trunc#torch.trunc&quot;&gt; &lt;code&gt;torch.trunc()&lt;/code&gt; 的&lt;/a&gt;别名</target>
        </trans-unit>
        <trans-unit id="f7b4372e3de7ce07bac66d661704328db6f955a4" translate="yes" xml:space="preserve">
          <source>Alias for field number 0</source>
          <target state="translated">字段号0的别称</target>
        </trans-unit>
        <trans-unit id="bbef6b362c3d3509157f18014e4e5a25eb4e07ea" translate="yes" xml:space="preserve">
          <source>Alias for field number 1</source>
          <target state="translated">字段1的别称</target>
        </trans-unit>
        <trans-unit id="cb7d09e2006a3aec07c13d82496b6f2adb24a1f7" translate="yes" xml:space="preserve">
          <source>Alias for field number 2</source>
          <target state="translated">字段2的别称</target>
        </trans-unit>
        <trans-unit id="2116d748feb69a3af8d3d3f32852bff649bb421e" translate="yes" xml:space="preserve">
          <source>Alias for field number 3</source>
          <target state="translated">字段号3的别称</target>
        </trans-unit>
        <trans-unit id="c8d021883b0a18d7d212863e386fe9e4590e884d" translate="yes" xml:space="preserve">
          <source>Alias of &lt;a href=&quot;generated/torch.det#torch.det&quot;&gt;&lt;code&gt;torch.det()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.det#torch.det&quot;&gt; &lt;code&gt;torch.det()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="50e05f8a73a607a4d8a0fae45b49062e73a2e308" translate="yes" xml:space="preserve">
          <source>Alias of &lt;a href=&quot;generated/torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.outer#torch.outer&quot;&gt; &lt;code&gt;torch.outer()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="47e1617de19cb0d4a15c8e0fcabf777055dd3bb7" translate="yes" xml:space="preserve">
          <source>Alias of &lt;a href=&quot;torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.outer#torch.outer&quot;&gt; &lt;code&gt;torch.outer()&lt;/code&gt; 的&lt;/a&gt;别名。</target>
        </trans-unit>
        <trans-unit id="9e010f721821118294a0c5b2474248233557907a" translate="yes" xml:space="preserve">
          <source>All &lt;code&gt;Tensor&lt;/code&gt; s keep track of in-place operations applied to them, and if the implementation detects that a tensor was saved for backward in one of the functions, but it was modified in-place afterwards, an error will be raised once backward pass is started. This ensures that if you&amp;rsquo;re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct.</source>
          <target state="translated">所有 &lt;code&gt;Tensor&lt;/code&gt; 都跟踪对其应用的就地操作，并且如果实现检测到某个张量在某个函数中被保存用于向后，但随后又对其进行了修改，则一旦向后传递就将产生错误。开始。这可以确保，如果您使用的是就地函数并且没有看到任何错误，则可以确保计算出的梯度是正确的。</target>
        </trans-unit>
        <trans-unit id="199b1c80b5c9c67ba6a90eb5f4a37f1d39d57d4e" translate="yes" xml:space="preserve">
          <source>All CUDA kernels queued within its context will be enqueued on a selected stream.</source>
          <target state="translated">所有在其上下文中排队的CUDA内核都将在选定的流上被enqueued。</target>
        </trans-unit>
        <trans-unit id="7d50263b9330ad372189eff67c5c8da79c613de2" translate="yes" xml:space="preserve">
          <source>All RNN modules accept packed sequences as inputs.</source>
          <target state="translated">所有RNN模块都接受打包序列作为输入。</target>
        </trans-unit>
        <trans-unit id="0378c5b2171bc1b88405ce471d2eb658bdba38cd" translate="yes" xml:space="preserve">
          <source>All Tensors that have &lt;a href=&quot;#torch.Tensor.requires_grad&quot;&gt;&lt;code&gt;requires_grad&lt;/code&gt;&lt;/a&gt; which is &lt;code&gt;False&lt;/code&gt; will be leaf Tensors by convention.</source>
          <target state="translated">所有的张量有&lt;a href=&quot;#torch.Tensor.requires_grad&quot;&gt; &lt;code&gt;requires_grad&lt;/code&gt; &lt;/a&gt;这是 &lt;code&gt;False&lt;/code&gt; 将按照惯例叶张量。</target>
        </trans-unit>
        <trans-unit id="bcd80e614fd3639e154329146ec9d72471b9fa7b" translate="yes" xml:space="preserve">
          <source>All Tensors that have &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt;&lt;code&gt;requires_grad&lt;/code&gt;&lt;/a&gt; which is &lt;code&gt;False&lt;/code&gt; will be leaf Tensors by convention.</source>
          <target state="translated">所有的张量有&lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt; &lt;code&gt;requires_grad&lt;/code&gt; &lt;/a&gt;这是 &lt;code&gt;False&lt;/code&gt; 将按照惯例叶张量。</target>
        </trans-unit>
        <trans-unit id="fae256c49cab7960ddc648f6bae5910cd2d3503e" translate="yes" xml:space="preserve">
          <source>All TorchVision models, except for quantized versions, are exportable to ONNX. More details can be found in &lt;a href=&quot;torchvision/models&quot;&gt;TorchVision&lt;/a&gt;.</source>
          <target state="translated">除量化版本外，所有TorchVision模型均可导出到ONNX。可以在&lt;a href=&quot;torchvision/models&quot;&gt;TorchVision中&lt;/a&gt;找到更多详细信息。</target>
        </trans-unit>
        <trans-unit id="5dce89529d95dc69f4c1f1ca05bfc2111d81383b" translate="yes" xml:space="preserve">
          <source>All arguments are forwarded to the &lt;code&gt;setuptools.Extension&lt;/code&gt; constructor.</source>
          <target state="translated">所有参数都转发到 &lt;code&gt;setuptools.Extension&lt;/code&gt; 构造函数。</target>
        </trans-unit>
        <trans-unit id="11ff807dac1aacbdf6348e8f37bb396417fac326" translate="yes" xml:space="preserve">
          <source>All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite &lt;code&gt;__getitem__()&lt;/code&gt;, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite &lt;code&gt;__len__()&lt;/code&gt;, which is expected to return the size of the dataset by many &lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt;&lt;code&gt;Sampler&lt;/code&gt;&lt;/a&gt; implementations and the default options of &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">代表从键到数据样本的映射的所有数据集都应将其子类化。所有子类都应覆盖 &lt;code&gt;__getitem__()&lt;/code&gt; ，支持获取给定键的数据样本。子类还可以选择覆盖 &lt;code&gt;__len__()&lt;/code&gt; ，这有望通过许多&lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt; &lt;code&gt;Sampler&lt;/code&gt; &lt;/a&gt;实现和&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt;的默认选项返回数据集的大小。</target>
        </trans-unit>
        <trans-unit id="82dec6743296a8c3a95df2231d58c66c783ae2bd" translate="yes" xml:space="preserve">
          <source>All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream.</source>
          <target state="translated">所有表示数据样本的可迭代的数据集都应该对它进行子类化。当数据来自于流时,这种形式的数据集特别有用。</target>
        </trans-unit>
        <trans-unit id="7c7fdd1a2c6981ce3b8e5aa9cb3e2ddd838235f0" translate="yes" xml:space="preserve">
          <source>All dimension names of &lt;code&gt;self&lt;/code&gt; must be present in &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt; may contain additional names that are not in &lt;code&gt;self.names&lt;/code&gt;; the output tensor has a size-one dimension for each of those new names.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 的所有维名称都必须出现在&lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt;。&lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt;可能包含不在 &lt;code&gt;self.names&lt;/code&gt; 中的其他名称; 对于这些新名称中的每一个，输出张量都具有一个一维的尺寸。</target>
        </trans-unit>
        <trans-unit id="1576ac96be05f9aa6b0251d2e542e4b0f488bd8b" translate="yes" xml:space="preserve">
          <source>All dimension names of &lt;code&gt;self&lt;/code&gt; must be present in &lt;code&gt;other.names&lt;/code&gt;. &lt;code&gt;other&lt;/code&gt; may contain named dimensions that are not in &lt;code&gt;self.names&lt;/code&gt;; the output tensor has a size-one dimension for each of those new names.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 的所有维度名称都必须存在于 &lt;code&gt;other.names&lt;/code&gt; 中。 &lt;code&gt;other&lt;/code&gt; 可能包含不在 &lt;code&gt;self.names&lt;/code&gt; 中的命名维度; 对于这些新名称中的每一个，输出张量都具有一个一维的尺寸。</target>
        </trans-unit>
        <trans-unit id="91188b536d6c07689225e8b91d26aa659eef6d49" translate="yes" xml:space="preserve">
          <source>All elements must be greater than</source>
          <target state="translated">所有元素必须大于</target>
        </trans-unit>
        <trans-unit id="fa3a1109ed903de03a2630b0856e9124664ffc41" translate="yes" xml:space="preserve">
          <source>All functions must be valid TorchScript functions (including &lt;code&gt;__init__()&lt;/code&gt;).</source>
          <target state="translated">所有函数必须是有效的TorchScript函数（包括 &lt;code&gt;__init__()&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="e460db881180d9a3276146d738dac2c41672c148" translate="yes" xml:space="preserve">
          <source>All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.</source>
          <target state="translated">未来所有提交到这个流的工作将等到调用时提交到给定流的所有内核完成。</target>
        </trans-unit>
        <trans-unit id="d5ff4adfb04564464a64a43c245685fb2b5d3919" translate="yes" xml:space="preserve">
          <source>All inputs should have matching shapes, dtype, and layout. The output tensor will be of the same shape, dtype, and layout.</source>
          <target state="translated">所有的输入都应该有匹配的形状、dtype和布局。输出张量将具有相同的形状、dtype和布局。</target>
        </trans-unit>
        <trans-unit id="8102d7ae18041c98a7f876840a19500755dde277" translate="yes" xml:space="preserve">
          <source>All modules, no matter their device, are always loaded onto the CPU during loading. This is different from &lt;a href=&quot;torch.load#torch.load&quot;&gt;&lt;code&gt;torch.load()&lt;/code&gt;&lt;/a&gt;&amp;rsquo;s semantics and may change in the future.</source>
          <target state="translated">所有模块，无论使用哪种设备，都始终在加载期间加载到CPU中。这与&lt;a href=&quot;torch.load#torch.load&quot;&gt; &lt;code&gt;torch.load()&lt;/code&gt; &lt;/a&gt;的语义不同，并且将来可能会更改。</target>
        </trans-unit>
        <trans-unit id="391810af4c5adb65a3bbe96665345a54a154a4ff" translate="yes" xml:space="preserve">
          <source>All of &lt;code&gt;dims&lt;/code&gt; must be consecutive in order in the &lt;code&gt;self&lt;/code&gt; tensor, but not necessary contiguous in memory.</source>
          <target state="translated">所有 &lt;code&gt;dims&lt;/code&gt; 必须在 &lt;code&gt;self&lt;/code&gt; 张量中按顺序连续，但在内存中不必是连续的。</target>
        </trans-unit>
        <trans-unit id="6c24f14499bb0bd88c18f05a4fe7875666f28f6d" translate="yes" xml:space="preserve">
          <source>All of the dims of &lt;code&gt;self&lt;/code&gt; must be named in order to use this method. The resulting tensor is a view on the original tensor.</source>
          <target state="translated">为了使用此方法，必须对 &lt;code&gt;self&lt;/code&gt; 的所有暗点进行命名。生成的张量是原始张量的视图。</target>
        </trans-unit>
        <trans-unit id="8d9325f5f71bde68b50e39d98dd2b2fc4b9b3f89" translate="yes" xml:space="preserve">
          <source>All operations that support named tensors propagate names.</source>
          <target state="translated">所有支持命名时序的操作都会传播名称。</target>
        </trans-unit>
        <trans-unit id="b60bcfd2d0b2dce7638e7cd3bf4d7879477608de" translate="yes" xml:space="preserve">
          <source>All optimizers implement a &lt;a href=&quot;#torch.optim.Optimizer.step&quot;&gt;&lt;code&gt;step()&lt;/code&gt;&lt;/a&gt; method, that updates the parameters. It can be used in two ways:</source>
          <target state="translated">所有优化器都实现一个&lt;a href=&quot;#torch.optim.Optimizer.step&quot;&gt; &lt;code&gt;step()&lt;/code&gt; &lt;/a&gt;方法，该方法更新参数。它可以以两种方式使用：</target>
        </trans-unit>
        <trans-unit id="c773fd48da7cc8c1f4621a11671c67f1f6303a0c" translate="yes" xml:space="preserve">
          <source>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using &lt;code&gt;mean = [0.485, 0.456, 0.406]&lt;/code&gt; and &lt;code&gt;std = [0.229, 0.224, 0.225]&lt;/code&gt;. You can use the following transform to normalize:</source>
          <target state="translated">所有经过预训练的模型都希望输入图像以相同的方式归一化，即形状为（3 x H x W）的3通道RGB图像的迷你批，其中H和W至少应为224。将其加载到[0，1]的范围内，然后使用 &lt;code&gt;mean = [0.485, 0.456, 0.406]&lt;/code&gt; 和 &lt;code&gt;std = [0.229, 0.224, 0.225]&lt;/code&gt; ]进行归一化。您可以使用以下转换进行规范化：</target>
        </trans-unit>
        <trans-unit id="ec2e3e9eabe115169e1deab63808ca48a84df052" translate="yes" xml:space="preserve">
          <source>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB videos of shape (3 x T x H x W), where H and W are expected to be 112, and T is a number of video frames in a clip. The images have to be loaded in to a range of [0, 1] and then normalized using &lt;code&gt;mean = [0.43216, 0.394666, 0.37645]&lt;/code&gt; and &lt;code&gt;std = [0.22803, 0.22145, 0.216989]&lt;/code&gt;.</source>
          <target state="translated">所有经过预训练的模型都希望输入图像以相同的方式进行归一化，即形状为（3 x T x H x W）的3通道RGB视频的迷你批，其中H和W期望为112，T为剪辑中的视频帧数。图像必须加载到[0，1]范围内，然后使用 &lt;code&gt;mean = [0.43216, 0.394666, 0.37645]&lt;/code&gt; 和 &lt;code&gt;std = [0.22803, 0.22145, 0.216989]&lt;/code&gt; ]进行归一化。</target>
        </trans-unit>
        <trans-unit id="7463404aa8619413edebf6ff4257cb99add42cba" translate="yes" xml:space="preserve">
          <source>All previously saved modules, no matter their device, are first loaded onto CPU, and then are moved to the devices they were saved from. If this fails (e.g. because the run time system doesn&amp;rsquo;t have certain devices), an exception is raised.</source>
          <target state="translated">之前保存的所有模块，无论使用何种设备，都首先加载到CPU中，然后再移动到保存它们的设备上。如果失败（例如因为运行时系统没有某些设备），则会引发异常。</target>
        </trans-unit>
        <trans-unit id="1b8a41c0d78fe02aeede8e4c39fabb850893cedd" translate="yes" xml:space="preserve">
          <source>All subclasses should overwrite &lt;code&gt;__iter__()&lt;/code&gt;, which would return an iterator of samples in this dataset.</source>
          <target state="translated">所有子类都应覆盖 &lt;code&gt;__iter__()&lt;/code&gt; ，它将返回此数据集中的样本迭代器。</target>
        </trans-unit>
        <trans-unit id="f4fb70a59932061a3d80fc551911560fa69384f6" translate="yes" xml:space="preserve">
          <source>All summed &lt;code&gt;dim&lt;/code&gt; are squeezed (see &lt;a href=&quot;generated/torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting an output tensor having &lt;code&gt;dim&lt;/code&gt; fewer dimensions than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">所有求和 &lt;code&gt;dim&lt;/code&gt; 被挤压（见&lt;a href=&quot;generated/torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt;），从而产生具有输出张量 &lt;code&gt;dim&lt;/code&gt; 不是较少尺寸 &lt;code&gt;input&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="6bd30243e5faf7879695a6c25ee7d928afafccc6" translate="yes" xml:space="preserve">
          <source>All tensors need to be of the same size.</source>
          <target state="translated">所有的张力器都需要相同的尺寸。</target>
        </trans-unit>
        <trans-unit id="647c455337c3b30ca33d7810c4e9c4f5a9641226" translate="yes" xml:space="preserve">
          <source>All the weights and biases are initialized from</source>
          <target state="translated">所有的权重和偏置都是通过以下方式初始化的</target>
        </trans-unit>
        <trans-unit id="970c0ae90b9eb34e0cb6a356cf47964705868146" translate="yes" xml:space="preserve">
          <source>Allows the model to jointly attend to information from different representation subspaces.</source>
          <target state="translated">允许模型共同关注来自不同表示子空间的信息。</target>
        </trans-unit>
        <trans-unit id="5702cc92d2bcf7e780dcdf99f53eee4b5e88684e" translate="yes" xml:space="preserve">
          <source>Allows the model to jointly attend to information from different representation subspaces. See reference: Attention Is All You Need</source>
          <target state="translated">允许模型共同关注来自不同表示子空间的信息。参见参考文献。注意就是你需要的一切</target>
        </trans-unit>
        <trans-unit id="bcb917dff27ba02ff781ce514dcf9c4e2df73ba5" translate="yes" xml:space="preserve">
          <source>Alpha Dropout is a type of Dropout that maintains the self-normalizing property. For an input with zero mean and unit standard deviation, the output of Alpha Dropout maintains the original mean and standard deviation of the input. Alpha Dropout goes hand-in-hand with SELU activation function, which ensures that the outputs have zero mean and unit standard deviation.</source>
          <target state="translated">Alpha Dropout是一种保持自正态化特性的Dropout。对于一个均值和单位标准差为零的输入,Alpha Dropout的输出保持了输入的原始均值和标准差。Alpha Dropout与SELU激活功能并驾齐驱,确保输出的均值和单位标准差为零。</target>
        </trans-unit>
        <trans-unit id="468dc142cbb278c344ed57a0c2341d7dabf6044d" translate="yes" xml:space="preserve">
          <source>AlphaDropout</source>
          <target state="translated">AlphaDropout</target>
        </trans-unit>
        <trans-unit id="697ae46b13c17cf9f965bf8eeb7388f2f2599380" translate="yes" xml:space="preserve">
          <source>Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default &lt;code&gt;momentum&lt;/code&gt; of 0.1.</source>
          <target state="translated">同样默认情况下，在训练过程中，该层会继续对其计算的均值和方差进行估算，然后将其用于评估期间的标准化。保持运行估算，默认 &lt;code&gt;momentum&lt;/code&gt; 为0.1。</target>
        </trans-unit>
        <trans-unit id="8bf20b1915a4ba9029b2263ed8aac08cdcf55753" translate="yes" xml:space="preserve">
          <source>Also functions as a decorator. (Make sure to instantiate with parenthesis.)</source>
          <target state="translated">也可作为装饰器使用。(请务必用括号将其实例化。)</target>
        </trans-unit>
        <trans-unit id="51924beeca0c98eba05c19c63804e09424b9e355" translate="yes" xml:space="preserve">
          <source>Also known as Glorot initialization.</source>
          <target state="translated">又称Glorot初始化。</target>
        </trans-unit>
        <trans-unit id="99d08174dc1487c12f409bf77bbf4a6a16dc84ad" translate="yes" xml:space="preserve">
          <source>Also known as He initialization.</source>
          <target state="translated">又称何初始化。</target>
        </trans-unit>
        <trans-unit id="9319f57915da9aa68da62d35edfd64135450d0d5" translate="yes" xml:space="preserve">
          <source>Also note that &lt;code&gt;len(input_tensor_lists)&lt;/code&gt;, and the size of each element in &lt;code&gt;input_tensor_lists&lt;/code&gt; (each element is a list, therefore &lt;code&gt;len(input_tensor_lists[i])&lt;/code&gt;) need to be the same for all the distributed processes calling this function.</source>
          <target state="translated">另外请注意， &lt;code&gt;len(input_tensor_lists)&lt;/code&gt; ，并且每个元件的在大小 &lt;code&gt;input_tensor_lists&lt;/code&gt; （每个元素是一个列表，因此 &lt;code&gt;len(input_tensor_lists[i])&lt;/code&gt; ）需要对于所有分布式进程调用此函数是相同的。</target>
        </trans-unit>
        <trans-unit id="3a0c3f7b9176b92a14d4b6b5cc0f2c401a31947c" translate="yes" xml:space="preserve">
          <source>Also note that &lt;code&gt;len(output_tensor_lists)&lt;/code&gt;, and the size of each element in &lt;code&gt;output_tensor_lists&lt;/code&gt; (each element is a list, therefore &lt;code&gt;len(output_tensor_lists[i])&lt;/code&gt;) need to be the same for all the distributed processes calling this function.</source>
          <target state="translated">另外请注意， &lt;code&gt;len(output_tensor_lists)&lt;/code&gt; ，并且每个元件的在大小 &lt;code&gt;output_tensor_lists&lt;/code&gt; （每个元素是一个列表，因此 &lt;code&gt;len(output_tensor_lists[i])&lt;/code&gt; ）需要对于所有分布式进程调用此函数是相同的。</target>
        </trans-unit>
        <trans-unit id="0c8280a06795f9ac5918243b5469526506807f31" translate="yes" xml:space="preserve">
          <source>Although CUDA versions &amp;gt;= 11 support more than two levels of priorities, in PyTorch, we only support two levels of priorities.</source>
          <target state="translated">尽管CUDA版本&amp;gt; = 11支持超过两个级别的优先级，但是在PyTorch中，我们仅支持两个级别的优先级。</target>
        </trans-unit>
        <trans-unit id="043751522414e7c9aaee8aec5948d07968c78350" translate="yes" xml:space="preserve">
          <source>Although the recipe for forward pass needs to be defined within this function, one should call the &lt;a href=&quot;#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</source>
          <target state="translated">尽管需要在此函数中定义向前传递的方法，但应随后调用&lt;a href=&quot;#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt;实例，而不是调用此实例，因为前者负责运行已注册的钩子，而后者则静默地忽略它们。</target>
        </trans-unit>
        <trans-unit id="a16ed36f1048b1b7e767b2e1655943a9aa7dfbb6" translate="yes" xml:space="preserve">
          <source>An &lt;code&gt;RRef&lt;/code&gt; (Remote REFerence) is a reference to a value of some type &lt;code&gt;T&lt;/code&gt; (e.g. &lt;code&gt;Tensor&lt;/code&gt;) on a remote worker. This handle keeps the referenced remote value alive on the owner, but there is no implication that the value will be transferred to the local worker in the future. RRefs can be used in multi-machine training by holding references to &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Module&quot;&gt;nn.Modules&lt;/a&gt; that exist on other workers, and calling the appropriate functions to retrieve or modify their parameters during training. See &lt;a href=&quot;rpc/rref#remote-reference-protocol&quot;&gt;Remote Reference Protocol&lt;/a&gt; for more details.</source>
          <target state="translated">一个 &lt;code&gt;RRef&lt;/code&gt; （远程引用）是某种类型的值的参考 &lt;code&gt;T&lt;/code&gt; （例如 &lt;code&gt;Tensor&lt;/code&gt; 上的远程工作者）。该句柄使引用的远程值在所有者上保持活动状态，但不暗示该值将来会转移给本地工作人员。通过保留对其他工人上存在的&lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Module&quot;&gt;nn.Module的&lt;/a&gt;引用，并在培训期间调用适当的函数来检索或修改其参数，可以将RRef用于多机培训。有关更多详细信息，请参见&lt;a href=&quot;rpc/rref#remote-reference-protocol&quot;&gt;远程参考协议&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="366e57ddf274ce33b825366c8f324bc7720a8851" translate="yes" xml:space="preserve">
          <source>An Elman RNN cell with tanh or ReLU non-linearity.</source>
          <target state="translated">一个具有tanh或ReLU非线性的Elman RNN细胞。</target>
        </trans-unit>
        <trans-unit id="0a352f479f8560e5a2c03ce7b4e08fa3e11c8ded" translate="yes" xml:space="preserve">
          <source>An Elman RNN cell with tanh or ReLU non-linearity. A dynamic quantized RNNCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as &lt;code&gt;torch.nn.RNNCell&lt;/code&gt;, please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell&lt;/a&gt; for documentation.</source>
          <target state="translated">具有tanh或ReLU非线性的Elman RNN单元。具有浮点张量作为输入和输出的动态量化RNNCell模块。权重量化为8位。我们采用与 &lt;code&gt;torch.nn.RNNCell&lt;/code&gt; 相同的接口，有关文档，请参阅&lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="ecda50e6fe26ce22d999e9c65fbfaac432da6710" translate="yes" xml:space="preserve">
          <source>An EventList containing FunctionEventAvg objects.</source>
          <target state="translated">一个包含FunctionEventAvg对象的EventList。</target>
        </trans-unit>
        <trans-unit id="89afc3eab800778902526ed4786c65d7f4ca6494" translate="yes" xml:space="preserve">
          <source>An abstract class representing a &lt;a href=&quot;#torch.utils.data.Dataset&quot;&gt;&lt;code&gt;Dataset&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">表示&lt;a href=&quot;#torch.utils.data.Dataset&quot;&gt; &lt;code&gt;Dataset&lt;/code&gt; &lt;/a&gt;的抽象类。</target>
        </trans-unit>
        <trans-unit id="cda80fd86e845c48c7faf0e81a98aca3de57ab49" translate="yes" xml:space="preserve">
          <source>An abstract structure encapsulating the options passed into the RPC backend. An instance of this class can be passed in to &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt;&lt;code&gt;init_rpc()&lt;/code&gt;&lt;/a&gt; in order to initialize RPC with specific configurations, such as the RPC timeout and &lt;code&gt;init_method&lt;/code&gt; to be used.</source>
          <target state="translated">封装传递到RPC后端的选项的抽象结构。可以将此类的实例传递到&lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt; &lt;code&gt;init_rpc()&lt;/code&gt; &lt;/a&gt;中，以便使用特定配置（例如，要使用的RPC超时和 &lt;code&gt;init_method&lt;/code&gt; ）来初始化RPC 。</target>
        </trans-unit>
        <trans-unit id="34f08a969a81be9fad81ba44cfa1144a898067e4" translate="yes" xml:space="preserve">
          <source>An additional dimension of size &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt; is appended in the returned tensor.</source>
          <target state="translated">在返回的张量中附加了一个size&lt;a href=&quot;#torch.Tensor.size&quot;&gt; &lt;code&gt;size&lt;/code&gt; &lt;/a&gt;的附加维。</target>
        </trans-unit>
        <trans-unit id="619ad847534358910127729afc029b7953abcecf" translate="yes" xml:space="preserve">
          <source>An empty dict is assumed have type &lt;code&gt;Dict[str, Tensor]&lt;/code&gt;. The types of other dict literals are derived from the type of the members. See &lt;a href=&quot;#default-types&quot;&gt;Default Types&lt;/a&gt; for more details.</source>
          <target state="translated">假定一个空dict具有 &lt;code&gt;Dict[str, Tensor]&lt;/code&gt; 。其他dict文字的类型是从成员的类型派生的。有关更多详细信息，请参见&lt;a href=&quot;#default-types&quot;&gt;默认类型&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c1a0eecf8a5a6cdc72f4e0da994b510481da5a4d" translate="yes" xml:space="preserve">
          <source>An empty list is assumed have type &lt;code&gt;List[Tensor]&lt;/code&gt;. The types of other list literals are derived from the type of the members. See &lt;a href=&quot;#default-types&quot;&gt;Default Types&lt;/a&gt; for more details.</source>
          <target state="translated">假定一个空列表的类型为 &lt;code&gt;List[Tensor]&lt;/code&gt; 。其他列表文字的类型是从成员的类型派生的。有关更多详细信息，请参见&lt;a href=&quot;#default-types&quot;&gt;默认类型&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="d84c564cad7cf9fb4e3caa453c3a49e8beeae54d" translate="yes" xml:space="preserve">
          <source>An empty list is assumed to be &lt;code&gt;List[Tensor]&lt;/code&gt; and empty dicts &lt;code&gt;Dict[str, Tensor]&lt;/code&gt;. To instantiate an empty list or dict of other types, use &lt;code&gt;Python 3 type hints&lt;/code&gt;.</source>
          <target state="translated">假定空列表为 &lt;code&gt;List[Tensor]&lt;/code&gt; 而空字典为 &lt;code&gt;Dict[str, Tensor]&lt;/code&gt; 。要实例化其他类型的空列表或字典，请使用 &lt;code&gt;Python 3 type hints&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="fc24c0a7e765c257ec2c1deb32cf9b9b1bc99fd5" translate="yes" xml:space="preserve">
          <source>An empty sparse tensor can be constructed by specifying its size:</source>
          <target state="translated">可以通过指定其大小来构造一个空的稀疏张量。</target>
        </trans-unit>
        <trans-unit id="17f91bb7684b57d410e71c667849b986b1e265c6" translate="yes" xml:space="preserve">
          <source>An enum class of available backends.</source>
          <target state="translated">可用后端的枚举类。</target>
        </trans-unit>
        <trans-unit id="a60c11218c3c0643831cc9fba424f6f68723b504" translate="yes" xml:space="preserve">
          <source>An enum-like class for available reduction operations: &lt;code&gt;SUM&lt;/code&gt;, &lt;code&gt;PRODUCT&lt;/code&gt;, &lt;code&gt;MIN&lt;/code&gt;, &lt;code&gt;MAX&lt;/code&gt;, &lt;code&gt;BAND&lt;/code&gt;, &lt;code&gt;BOR&lt;/code&gt;, and &lt;code&gt;BXOR&lt;/code&gt;.</source>
          <target state="translated">类似于枚举的类，用于可用的约简操作： &lt;code&gt;SUM&lt;/code&gt; ， &lt;code&gt;PRODUCT&lt;/code&gt; ， &lt;code&gt;MIN&lt;/code&gt; ， &lt;code&gt;MAX&lt;/code&gt; ， &lt;code&gt;BAND&lt;/code&gt; ， &lt;code&gt;BOR&lt;/code&gt; 和 &lt;code&gt;BXOR&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c5d6691fbcb10d6ab33410657c5bcc031006a7f4" translate="yes" xml:space="preserve">
          <source>An enum-like class of available backends: GLOO, NCCL, MPI, and other registered backends.</source>
          <target state="translated">一个类似枚举的可用后端类。GLOO、NCCL、MPI和其他注册后端。</target>
        </trans-unit>
        <trans-unit id="c399f6bcf61d242f357b89e04dfba538f2cb230b" translate="yes" xml:space="preserve">
          <source>An example for the usage of &lt;a href=&quot;#torch.distributions.transformed_distribution.TransformedDistribution&quot;&gt;&lt;code&gt;TransformedDistribution&lt;/code&gt;&lt;/a&gt; would be:</source>
          <target state="translated">使用&lt;a href=&quot;#torch.distributions.transformed_distribution.TransformedDistribution&quot;&gt; &lt;code&gt;TransformedDistribution&lt;/code&gt; 的&lt;/a&gt;一个示例是：</target>
        </trans-unit>
        <trans-unit id="519ac03d4f41ca6837697ea6c98bf8c921545893" translate="yes" xml:space="preserve">
          <source>An example of such normalization can be found in the imagenet example &lt;a href=&quot;https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101&quot;&gt;here&lt;/a&gt;</source>
          <target state="translated">可以在&lt;a href=&quot;https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101&quot;&gt;此处&lt;/a&gt;的imagenet示例中找到此类规范化的示例</target>
        </trans-unit>
        <trans-unit id="3c1ceedb1dedcde4e24919af77558ac6f1600e4d" translate="yes" xml:space="preserve">
          <source>An example where &lt;code&gt;transform_to&lt;/code&gt; and &lt;code&gt;biject_to&lt;/code&gt; differ is &lt;code&gt;constraints.simplex&lt;/code&gt;: &lt;code&gt;transform_to(constraints.simplex)&lt;/code&gt; returns a &lt;a href=&quot;#torch.distributions.transforms.SoftmaxTransform&quot;&gt;&lt;code&gt;SoftmaxTransform&lt;/code&gt;&lt;/a&gt; that simply exponentiates and normalizes its inputs; this is a cheap and mostly coordinate-wise operation appropriate for algorithms like SVI. In contrast, &lt;code&gt;biject_to(constraints.simplex)&lt;/code&gt; returns a &lt;a href=&quot;#torch.distributions.transforms.StickBreakingTransform&quot;&gt;&lt;code&gt;StickBreakingTransform&lt;/code&gt;&lt;/a&gt; that bijects its input down to a one-fewer-dimensional space; this a more expensive less numerically stable transform but is needed for algorithms like HMC.</source>
          <target state="translated">其中一个例子 &lt;code&gt;transform_to&lt;/code&gt; 和 &lt;code&gt;biject_to&lt;/code&gt; 不同是 &lt;code&gt;constraints.simplex&lt;/code&gt; ： &lt;code&gt;transform_to(constraints.simplex)&lt;/code&gt; 返回一个&lt;a href=&quot;#torch.distributions.transforms.SoftmaxTransform&quot;&gt; &lt;code&gt;SoftmaxTransform&lt;/code&gt; &lt;/a&gt;简单地exponentiates和标准化它的输入; 这是一种便宜的方法，通常适合于像SVI这样的算法进行协调操作。相反， &lt;code&gt;biject_to(constraints.simplex)&lt;/code&gt; 返回一个&lt;a href=&quot;#torch.distributions.transforms.StickBreakingTransform&quot;&gt; &lt;code&gt;StickBreakingTransform&lt;/code&gt; &lt;/a&gt;，将其输入向下投影到一维空间以下的空间；这是一个更昂贵的数值稳定的变换，但是像HMC这样的算法需要它。</target>
        </trans-unit>
        <trans-unit id="44802d0d5256af16bcf81baadaa0893f0644e7bc" translate="yes" xml:space="preserve">
          <source>An integral output tensor cannot accept a floating point tensor.</source>
          <target state="translated">积分输出张量不能接受浮点张量。</target>
        </trans-unit>
        <trans-unit id="2d42c640da9c089ad44c78deb2cd826d6b0df3f8" translate="yes" xml:space="preserve">
          <source>An iterable Dataset.</source>
          <target state="translated">一个可迭代的数据集。</target>
        </trans-unit>
        <trans-unit id="498090423a5e476802043e5808dfa44461e23184" translate="yes" xml:space="preserve">
          <source>An iterable-style dataset is an instance of a subclass of &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt;&lt;code&gt;IterableDataset&lt;/code&gt;&lt;/a&gt; that implements the &lt;code&gt;__iter__()&lt;/code&gt; protocol, and represents an iterable over data samples. This type of datasets is particularly suitable for cases where random reads are expensive or even improbable, and where the batch size depends on the fetched data.</source>
          <target state="translated">可迭代样式的数据集是&lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt; &lt;code&gt;IterableDataset&lt;/code&gt; &lt;/a&gt;子类的实例，该子类实现 &lt;code&gt;__iter__()&lt;/code&gt; 协议，并表示数据样本上的可迭代对象。这种类型的数据集特别适用于随机读取价格昂贵甚至不可能的情况，并且批处理大小取决于所获取的数据。</target>
        </trans-unit>
        <trans-unit id="ada1c75faa68087ab2e8ec52c71f9a5f98fa8946" translate="yes" xml:space="preserve">
          <source>An torch.Generator object.</source>
          <target state="translated">一个Torch.Generator对象。</target>
        </trans-unit>
        <trans-unit id="042787a30d514e644feadd2d89eacccefc21c5e7" translate="yes" xml:space="preserve">
          <source>Anomaly detection</source>
          <target state="translated">异常检测</target>
        </trans-unit>
        <trans-unit id="ef94106a43404d867533b831248eaa8963699ea0" translate="yes" xml:space="preserve">
          <source>Another initialization method makes use of a file system that is shared and visible from all machines in a group, along with a desired &lt;code&gt;world_size&lt;/code&gt;. The URL should start with &lt;code&gt;file://&lt;/code&gt; and contain a path to a non-existent file (in an existing directory) on a shared file system. File-system initialization will automatically create that file if it doesn&amp;rsquo;t exist, but will not delete the file. Therefore, it is your responsibility to make sure that the file is cleaned up before the next &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; call on the same file path/name.</source>
          <target state="translated">另一种初始化方法利用了一个文件系统，该文件系统与所需的 &lt;code&gt;world_size&lt;/code&gt; 一起在组中的所有计算机上共享并可见。URL应以 &lt;code&gt;file://&lt;/code&gt; 开头，并包含指向共享文件系统上不存在的文件（在现有目录中）的路径。文件系统初始化将自动创建该文件（如果该文件不存在），但不会删除该文件。因此，您有责任在相同文件路径/名称的下一个&lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt;调用之前，确保已清理文件。</target>
        </trans-unit>
        <trans-unit id="f5194d861e34e463cbb383549f2be7e4afdfdb33" translate="yes" xml:space="preserve">
          <source>Any functions executed during the backward pass are also decorated with &lt;code&gt;seq=&amp;lt;N&amp;gt;&lt;/code&gt;. During default backward (with &lt;code&gt;create_graph=False&lt;/code&gt;) this information is irrelevant, and in fact, &lt;code&gt;N&lt;/code&gt; may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects&amp;rsquo; &lt;code&gt;apply()&lt;/code&gt; methods are useful, as a way to correlate these Function objects with the earlier forward pass.</source>
          <target state="translated">在向后传递过程中执行的所有函数也都用 &lt;code&gt;seq=&amp;lt;N&amp;gt;&lt;/code&gt; 装饰。在默认的向后（ &lt;code&gt;create_graph=False&lt;/code&gt; ）期间，此信息无关紧要，并且实际上，对于所有此类函数， &lt;code&gt;N&lt;/code&gt; 都可以简单地为0。只有与向后功能对象的 &lt;code&gt;apply()&lt;/code&gt; 方法关联的顶级范围才有用，可以将这些功能对象与更早的向前传递相关联。</target>
        </trans-unit>
        <trans-unit id="1eec009a2337c1a488dd1b74c1aff7cc1d139bf1" translate="yes" xml:space="preserve">
          <source>Any other functionality from the &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt;&lt;code&gt;typing&lt;/code&gt;&lt;/a&gt; module not explitily listed in this documentation is unsupported.</source>
          <target state="translated">不支持该文档中未明确列出的&lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt; &lt;code&gt;typing&lt;/code&gt; &lt;/a&gt;模块的任何其他功能。</target>
        </trans-unit>
        <trans-unit id="9267fbcc5ac03c112f491ee9bc7bbb21bb5ae1c5" translate="yes" xml:space="preserve">
          <source>Append the given callback function to this &lt;code&gt;Future&lt;/code&gt;, which will be run when the &lt;code&gt;Future&lt;/code&gt; is completed. Multiple callbacks can be added to the same &lt;code&gt;Future&lt;/code&gt;, and will be invoked in the same order as they were added. The callback must take one argument, which is the reference to this &lt;code&gt;Future&lt;/code&gt;. The callback function can use the &lt;code&gt;Future.wait()&lt;/code&gt; API to get the value.</source>
          <target state="translated">将给定的回调函数附加到此 &lt;code&gt;Future&lt;/code&gt; ，该函数将在 &lt;code&gt;Future&lt;/code&gt; 完成后运行。可以将多个回调添加到同一 &lt;code&gt;Future&lt;/code&gt; 中，并且将按照添加时的相同顺序进行调用。回调必须接受一个参数，这是对此 &lt;code&gt;Future&lt;/code&gt; 的引用。回调函数可以使用 &lt;code&gt;Future.wait()&lt;/code&gt; API来获取值。</target>
        </trans-unit>
        <trans-unit id="f3f60044b06335eb6c4230e3c39f5e0218445878" translate="yes" xml:space="preserve">
          <source>Appendix</source>
          <target state="translated">Appendix</target>
        </trans-unit>
        <trans-unit id="c87f37c9f3693b628e726dc6f101341b952d29db" translate="yes" xml:space="preserve">
          <source>Appends a given module to the end of the list.</source>
          <target state="translated">将一个模块添加到列表的末尾。</target>
        </trans-unit>
        <trans-unit id="379011982516fbdbffaea6138d56d674a94f5e18" translate="yes" xml:space="preserve">
          <source>Appends a given parameter at the end of the list.</source>
          <target state="translated">在列表末尾添加一个给定的参数。</target>
        </trans-unit>
        <trans-unit id="9b249ec6f74bf6c17017438ecc23770483a688cc" translate="yes" xml:space="preserve">
          <source>Appends modules from a Python iterable to the end of the list.</source>
          <target state="translated">将Python迭代表中的模块添加到列表的末尾。</target>
        </trans-unit>
        <trans-unit id="0e68b1f2c86ef40318501ff9b078b446cee7c813" translate="yes" xml:space="preserve">
          <source>Appends parameters from a Python iterable to the end of the list.</source>
          <target state="translated">将Python迭代表中的参数添加到列表的末尾。</target>
        </trans-unit>
        <trans-unit id="378aa8a7eda72c52086938707b31878d8a635ee4" translate="yes" xml:space="preserve">
          <source>Applied element-wise, as:</source>
          <target state="translated">从要素上应用,如。</target>
        </trans-unit>
        <trans-unit id="47ea67db7d24ea56bb6efc03577734231d9fa58e" translate="yes" xml:space="preserve">
          <source>Applies 2D average-pooling operation in</source>
          <target state="translated">应用二维平均拼合操作,在此基础上再进行二维平均拼合操作。</target>
        </trans-unit>
        <trans-unit id="26a3eb8c50568931de48e1e7fa354baa87e37c44" translate="yes" xml:space="preserve">
          <source>Applies 3D average-pooling operation in</source>
          <target state="translated">应用3D平均池操作在</target>
        </trans-unit>
        <trans-unit id="897491bacd3642b7742fd85de111604a2b1e2139" translate="yes" xml:space="preserve">
          <source>Applies &lt;code&gt;callable&lt;/code&gt; for each element in &lt;code&gt;self&lt;/code&gt; tensor and the given &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; and stores the results in &lt;code&gt;self&lt;/code&gt; tensor. &lt;code&gt;self&lt;/code&gt; tensor and the given &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">适用于 &lt;code&gt;self&lt;/code&gt; 张量和给定&lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt;量中的每个元素 &lt;code&gt;callable&lt;/code&gt; ，并将结果存储在 &lt;code&gt;self&lt;/code&gt; 张量中。 &lt;code&gt;self&lt;/code&gt; 张量和给定&lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt;必须是可&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;广播的&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="854151a01bba98f938fa936327548dc77fb47b3f" translate="yes" xml:space="preserve">
          <source>Applies &lt;code&gt;fn&lt;/code&gt; recursively to every submodule (as returned by &lt;code&gt;.children()&lt;/code&gt;) as well as self. Typical use includes initializing the parameters of a model (see also &lt;a href=&quot;../nn.init#nn-init-doc&quot;&gt;torch.nn.init&lt;/a&gt;).</source>
          <target state="translated">将 &lt;code&gt;fn&lt;/code&gt; 递归应用于每个子模块（由 &lt;code&gt;.children()&lt;/code&gt; 返回）以及self。典型的用法包括初始化模型的参数（另请参见&lt;a href=&quot;../nn.init#nn-init-doc&quot;&gt;torch.nn.init&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="f3ce1d7a64849414112d779b273ff91d45596516" translate="yes" xml:space="preserve">
          <source>Applies Alpha Dropout over the input.</source>
          <target state="translated">在输入上应用Alpha Dropout。</target>
        </trans-unit>
        <trans-unit id="13e7aada5fd7c3d78fcf6b9c0d54d77028725d8c" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization for each channel across a batch of data.</source>
          <target state="translated">对每批数据的每个通道进行批次归一化。</target>
        </trans-unit>
        <trans-unit id="29f9114f73a8742a5df31f7a4732b3ef2d2fa5ed" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; .</source>
          <target state="translated">如论文&amp;ldquo;&lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;批量归一化：通过减少内部协变量偏移来加速深度网络训练&amp;rdquo;中&lt;/a&gt;所述，对2D或3D输入（具有可选附加通道尺寸的1D输入的小批量）应用批量归一化。</target>
        </trans-unit>
        <trans-unit id="77df134b3c1dec023ae7933dc984fdf0b9a37d0b" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; .</source>
          <target state="translated">如论文&amp;ldquo;&lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;批量归一化：通过减少内部协变量偏移加速深层网络训练&amp;rdquo;中&lt;/a&gt;所述，对4D输入（具有附加通道尺寸的2D输入的小批量）应用批量归一化。</target>
        </trans-unit>
        <trans-unit id="d5008d35280faa701906a5c7bb40531dff590ddb" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; .</source>
          <target state="translated">如论文&amp;ldquo;&lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;批量归一化：通过减少内部协变量偏移加速深层网络训练&amp;rdquo;中&lt;/a&gt;所述，对5D输入（具有附加通道尺寸的3D输入的小批量）应用批量归一化。</target>
        </trans-unit>
        <trans-unit id="ebba8e6ee8c679b795f34839517e1330fe0f6cd0" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; .</source>
          <target state="translated">如论文&amp;ldquo;&lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;批量归一化：通过减少内部协变量偏移来加速深度网络训练&amp;rdquo;中&lt;/a&gt;所述，对N维输入（具有附加通道维的[N-2] D输入的小批量）应用批量归一化。</target>
        </trans-unit>
        <trans-unit id="4c0b063a3d01c57a39497f55e2c1c93c68f9edde" translate="yes" xml:space="preserve">
          <source>Applies Group Normalization over a mini-batch of inputs as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1803.08494&quot;&gt;Group Normalization&lt;/a&gt;</source>
          <target state="translated">如论文&amp;ldquo;&lt;a href=&quot;https://arxiv.org/abs/1803.08494&quot;&gt;组归一化&amp;rdquo;中&lt;/a&gt;所述，将组归一化应用于一小批输入</target>
        </trans-unit>
        <trans-unit id="6ddcab60a314dad6ac4e8205f6d0bbe42d633f04" translate="yes" xml:space="preserve">
          <source>Applies Instance Normalization for each channel in each data sample in a batch.</source>
          <target state="translated">对批次中每个数据样本的每个通道应用实例归一化。</target>
        </trans-unit>
        <trans-unit id="8eba97c1edce80ebe5a8f2d8c4ea438567d2707c" translate="yes" xml:space="preserve">
          <source>Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization: The Missing Ingredient for Fast Stylization&lt;/a&gt;.</source>
          <target state="translated">如论文&lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;实例规范化：快速样式化的缺失&lt;/a&gt;要素所述，将实例规范化应用于3D输入（具有可选的附加通道尺寸的1D输入的微型批处理）。</target>
        </trans-unit>
        <trans-unit id="1c1378db79ff8ef5bf285d891bdde624c5ed8210" translate="yes" xml:space="preserve">
          <source>Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization: The Missing Ingredient for Fast Stylization&lt;/a&gt;.</source>
          <target state="translated">如论文&lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;实例化规范：快速样式化的缺失成分中&lt;/a&gt;所述，对4D输入（带有附加通道尺寸的2D输入的微型批处理）应用实例化规范。</target>
        </trans-unit>
        <trans-unit id="331da363e42c71d65e839afe96ad5dce770a5331" translate="yes" xml:space="preserve">
          <source>Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization: The Missing Ingredient for Fast Stylization&lt;/a&gt;.</source>
          <target state="translated">如文章&lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;实例化规范：快速样式化的缺失&lt;/a&gt;要素中所述，对5D输入（具有附加通道尺寸的3D输入的微型批处理）应用实例化规范。</target>
        </trans-unit>
        <trans-unit id="b97189b49bbea1acf1c1f0194b9ab27dea9973b9" translate="yes" xml:space="preserve">
          <source>Applies Layer Normalization for last certain number of dimensions.</source>
          <target state="translated">对最后一定数量的尺寸进行层归一化。</target>
        </trans-unit>
        <trans-unit id="835e00e544806b9f13082cf66ead874e6019f00b" translate="yes" xml:space="preserve">
          <source>Applies Layer Normalization over a mini-batch of inputs as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1607.06450&quot;&gt;Layer Normalization&lt;/a&gt;</source>
          <target state="translated">如论文&amp;ldquo;&lt;a href=&quot;https://arxiv.org/abs/1607.06450&quot;&gt;层归一化&amp;rdquo;中&lt;/a&gt;所述，将层归一化应用于一小批输入</target>
        </trans-unit>
        <trans-unit id="aeb29e205e308a59093be0d4988fa5cd74bac086" translate="yes" xml:space="preserve">
          <source>Applies SoftMax over features to each spatial location.</source>
          <target state="translated">将SoftMax应用于每个空间位置的特征上。</target>
        </trans-unit>
        <trans-unit id="68727ddc338094e31b998534ceb0a7cd1e1f8321" translate="yes" xml:space="preserve">
          <source>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</source>
          <target state="translated">在由多个输入平面组成的输入信号上应用一维自适应平均池化。</target>
        </trans-unit>
        <trans-unit id="0d6edcf464a5fa2406d025e84b2ed153de9c75ee" translate="yes" xml:space="preserve">
          <source>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</source>
          <target state="translated">在由多个输入平面组成的输入信号上应用一维自适应最大池化。</target>
        </trans-unit>
        <trans-unit id="b8f3289d5daff81c1311cb6f2ca1629e42cafa4f" translate="yes" xml:space="preserve">
          <source>Applies a 1D average pooling over an input signal composed of several input planes.</source>
          <target state="translated">对由多个输入平面组成的输入信号进行一维平均池化。</target>
        </trans-unit>
        <trans-unit id="3c27388a69cba70f821eaba1e90c8524f7a152d3" translate="yes" xml:space="preserve">
          <source>Applies a 1D convolution over a quantized 1D input composed of several input planes.</source>
          <target state="translated">在由多个输入平面组成的量化一维输入上应用一维卷积。</target>
        </trans-unit>
        <trans-unit id="322ebd47033ac7c9c030a4056a89e60da48e7a7e" translate="yes" xml:space="preserve">
          <source>Applies a 1D convolution over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">对由多个量化输入平面组成的量化输入信号进行一维卷积。</target>
        </trans-unit>
        <trans-unit id="dcfb7d54383b0ff98c06cfb1f40c5d85baf1b8d7" translate="yes" xml:space="preserve">
          <source>Applies a 1D convolution over an input signal composed of several input planes.</source>
          <target state="translated">对由多个输入平面组成的输入信号进行一维卷积。</target>
        </trans-unit>
        <trans-unit id="ba598dc972597d5f42702090e4175518b59bd582" translate="yes" xml:space="preserve">
          <source>Applies a 1D max pooling over an input signal composed of several input planes.</source>
          <target state="translated">在一个由多个输入平面组成的输入信号上应用1D最大池化。</target>
        </trans-unit>
        <trans-unit id="6e7aac7c6517399757e4cbb358ae6fceeed56898" translate="yes" xml:space="preserve">
          <source>Applies a 1D power-average pooling over an input signal composed of several input planes.</source>
          <target state="translated">在一个由多个输入平面组成的输入信号上应用1D功率平均池。</target>
        </trans-unit>
        <trans-unit id="8bfc842196843fc0db2f2d55709b870df2c8219f" translate="yes" xml:space="preserve">
          <source>Applies a 1D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of &lt;code&gt;p&lt;/code&gt; is zero, the gradient is set to zero as well.</source>
          <target state="translated">在由多个输入平面组成的输入信号上应用一维功率平均池。如果 &lt;code&gt;p&lt;/code&gt; 的幂的所有输入的总和为零，则梯度也设置为零。</target>
        </trans-unit>
        <trans-unit id="96ca07ea2e411b1ba9e96092e82956074c9632be" translate="yes" xml:space="preserve">
          <source>Applies a 1D transposed convolution operator over an input image composed of several input planes.</source>
          <target state="translated">在由多个输入平面组成的输入图像上应用一维转置卷积算子。</target>
        </trans-unit>
        <trans-unit id="4b460f2239dd693e787bc347edd22db345631a88" translate="yes" xml:space="preserve">
          <source>Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called &amp;ldquo;deconvolution&amp;rdquo;.</source>
          <target state="translated">在由多个输入平面组成的输入信号上应用一维转置卷积运算符，有时也称为&amp;ldquo;反卷积&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="aa89da022829347e9eb20aeec1fec7371b510238" translate="yes" xml:space="preserve">
          <source>Applies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">在由多个量化输入平面组成的量化输入信号上应用二维自适应平均池化。</target>
        </trans-unit>
        <trans-unit id="2990176eebf3f395560769ef6ba618123ba3cbdc" translate="yes" xml:space="preserve">
          <source>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</source>
          <target state="translated">对由多个输入平面组成的输入信号进行二维自适应平均池化。</target>
        </trans-unit>
        <trans-unit id="8f9f8906e50a1a40da69b6542725773d5f97b476" translate="yes" xml:space="preserve">
          <source>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</source>
          <target state="translated">对由多个输入平面组成的输入信号进行二维自适应最大池化。</target>
        </trans-unit>
        <trans-unit id="d576b7f4a24ec8fd4a5244faa85b001a714e4b2b" translate="yes" xml:space="preserve">
          <source>Applies a 2D average pooling over an input signal composed of several input planes.</source>
          <target state="translated">对由多个输入平面组成的输入信号进行二维平均汇集。</target>
        </trans-unit>
        <trans-unit id="4eef15eebd1e3c58ec128ae4ac04428c46e00ff5" translate="yes" xml:space="preserve">
          <source>Applies a 2D bilinear upsampling to an input signal composed of several input channels.</source>
          <target state="translated">对由多个输入通道组成的输入信号进行二维双线上采样。</target>
        </trans-unit>
        <trans-unit id="47468ad50007cdb83f448361561cc024fef32584" translate="yes" xml:space="preserve">
          <source>Applies a 2D convolution over a quantized 2D input composed of several input planes.</source>
          <target state="translated">对由多个输入平面组成的量化二维输入进行二维卷积。</target>
        </trans-unit>
        <trans-unit id="3f8605d5715138e74b1fe2f63f6110cb337b8e9e" translate="yes" xml:space="preserve">
          <source>Applies a 2D convolution over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">对由多个量化输入平面组成的量化输入信号进行二维卷积。</target>
        </trans-unit>
        <trans-unit id="01f53bdcdfe76a89f9da101eca78388d0e3db80b" translate="yes" xml:space="preserve">
          <source>Applies a 2D convolution over an input image composed of several input planes.</source>
          <target state="translated">对由多个输入平面组成的输入图像进行二维卷积。</target>
        </trans-unit>
        <trans-unit id="4d7022a0fdb1e618b639247856e6903aab3855e6" translate="yes" xml:space="preserve">
          <source>Applies a 2D convolution over an input signal composed of several input planes.</source>
          <target state="translated">对由多个输入平面组成的输入信号进行二维卷积。</target>
        </trans-unit>
        <trans-unit id="d5648bdd82e29b0ec766dab1d7ea5bd15fb57316" translate="yes" xml:space="preserve">
          <source>Applies a 2D fractional max pooling over an input signal composed of several input planes.</source>
          <target state="translated">在由多个输入平面组成的输入信号上应用二维小数最大池化。</target>
        </trans-unit>
        <trans-unit id="4dcf07079bf5dd6c28cbfd49dde4f39b4bf6b35b" translate="yes" xml:space="preserve">
          <source>Applies a 2D max pooling over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">在由多个量化输入平面组成的量化输入信号上应用2D最大池化。</target>
        </trans-unit>
        <trans-unit id="a476032b7da8421b8000aa56e627562cba0b8e78" translate="yes" xml:space="preserve">
          <source>Applies a 2D max pooling over an input signal composed of several input planes.</source>
          <target state="translated">在一个由多个输入平面组成的输入信号上应用2D最大池化。</target>
        </trans-unit>
        <trans-unit id="16be7a5fb7dca84029d9fea2edb8b4f2f0ee57f8" translate="yes" xml:space="preserve">
          <source>Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.</source>
          <target state="translated">对由多个输入通道组成的输入信号进行二维近邻上采样。</target>
        </trans-unit>
        <trans-unit id="41e06a057945b6ec3fbb4344ac26327e886f585c" translate="yes" xml:space="preserve">
          <source>Applies a 2D power-average pooling over an input signal composed of several input planes.</source>
          <target state="translated">对由多个输入平面组成的输入信号进行二维功率平均池化。</target>
        </trans-unit>
        <trans-unit id="d69ec22c35e226c6aa2b1af68d48e3fa60401801" translate="yes" xml:space="preserve">
          <source>Applies a 2D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of &lt;code&gt;p&lt;/code&gt; is zero, the gradient is set to zero as well.</source>
          <target state="translated">在由多个输入平面组成的输入信号上应用2D功率平均池。如果 &lt;code&gt;p&lt;/code&gt; 的幂的所有输入的总和为零，则梯度也设置为零。</target>
        </trans-unit>
        <trans-unit id="aa4b7ef410c54190539113762f6eeb2a66bf418c" translate="yes" xml:space="preserve">
          <source>Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called &amp;ldquo;deconvolution&amp;rdquo;.</source>
          <target state="translated">在由多个输入平面组成的输入图像上应用二维转置卷积运算符，有时也称为&amp;ldquo;反卷积&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="b43e206b10ee89d84a5a812651aa838b543004e7" translate="yes" xml:space="preserve">
          <source>Applies a 2D transposed convolution operator over an input image composed of several input planes.</source>
          <target state="translated">在由多个输入平面组成的输入图像上应用二维转置卷积算子。</target>
        </trans-unit>
        <trans-unit id="e5d643fce9e39e07cff6793421bc79eff19c89aa" translate="yes" xml:space="preserve">
          <source>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</source>
          <target state="translated">对由多个输入平面组成的输入信号进行三维自适应平均池化。</target>
        </trans-unit>
        <trans-unit id="ae56276da0a49f342cfd4b640808d55b08117aa7" translate="yes" xml:space="preserve">
          <source>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</source>
          <target state="translated">在由多个输入平面组成的输入信号上应用3D自适应最大池化。</target>
        </trans-unit>
        <trans-unit id="37570017fc80764b6133f223b871fd527ef1696c" translate="yes" xml:space="preserve">
          <source>Applies a 3D average pooling over an input signal composed of several input planes.</source>
          <target state="translated">对由多个输入平面组成的输入信号进行三维平均汇集。</target>
        </trans-unit>
        <trans-unit id="af8bbac2d382374f13d6575cf2d20a9ea3e1524d" translate="yes" xml:space="preserve">
          <source>Applies a 3D convolution over a quantized 3D input composed of several input planes.</source>
          <target state="translated">对由多个输入平面组成的量化三维输入进行三维卷积。</target>
        </trans-unit>
        <trans-unit id="c2ec5baf08b4d035b1c172c872a14f1141d860e8" translate="yes" xml:space="preserve">
          <source>Applies a 3D convolution over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">对由多个量化输入平面组成的量化输入信号进行三维卷积。</target>
        </trans-unit>
        <trans-unit id="f6f649a7867c44dcfc76acf042b1d45c5f149502" translate="yes" xml:space="preserve">
          <source>Applies a 3D convolution over an input image composed of several input planes.</source>
          <target state="translated">对由多个输入平面组成的输入图像进行三维卷积。</target>
        </trans-unit>
        <trans-unit id="51acb800dbe635481a7193d943f2c4fdf6f9b633" translate="yes" xml:space="preserve">
          <source>Applies a 3D convolution over an input signal composed of several input planes.</source>
          <target state="translated">对由多个输入平面组成的输入信号进行三维卷积。</target>
        </trans-unit>
        <trans-unit id="a5c65698b496aec798c667f51e238522d8ca0853" translate="yes" xml:space="preserve">
          <source>Applies a 3D max pooling over an input signal composed of several input planes.</source>
          <target state="translated">在由多个输入平面组成的输入信号上应用3D最大池化。</target>
        </trans-unit>
        <trans-unit id="330c7c56dc877c7532afe521797eaa31d71f0cd0" translate="yes" xml:space="preserve">
          <source>Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called &amp;ldquo;deconvolution&amp;rdquo;</source>
          <target state="translated">在由多个输入平面组成的输入图像上应用3D转置卷积运算符，有时也称为&amp;ldquo;反卷积&amp;rdquo;</target>
        </trans-unit>
        <trans-unit id="8ceaf63b96a331eabb115beb8c3e00540f6c5ffc" translate="yes" xml:space="preserve">
          <source>Applies a 3D transposed convolution operator over an input image composed of several input planes.</source>
          <target state="translated">在由多个输入平面组成的输入图像上应用3D转置卷积运算符。</target>
        </trans-unit>
        <trans-unit id="65ce4bde72c1f78502a6398b7865adc72e50ac02" translate="yes" xml:space="preserve">
          <source>Applies a 3D transposed convolution operator over an input image composed of several input planes. The transposed convolution operator multiplies each input value element-wise by a learnable kernel, and sums over the outputs from all input feature planes.</source>
          <target state="translated">在由多个输入平面组成的输入图像上应用3D转置卷积算子。转置卷积算子将每个输入值按元素乘以一个可学习的内核,并对所有输入特征平面的输出进行求和。</target>
        </trans-unit>
        <trans-unit id="66c150b86ae084c85a0e9289376927637ec64e05" translate="yes" xml:space="preserve">
          <source>Applies a bilinear transformation to the incoming data:</source>
          <target state="translated">对输入的数据进行双线变换。</target>
        </trans-unit>
        <trans-unit id="6ff756a7c7cfe8cec72e517aee9af4136e1371be" translate="yes" xml:space="preserve">
          <source>Applies a linear transformation to the incoming data:</source>
          <target state="translated">对输入的数据进行线性变换。</target>
        </trans-unit>
        <trans-unit id="8ff337dc0db982c6291a92a0d7e5b13fd2dbe747" translate="yes" xml:space="preserve">
          <source>Applies a linear transformation to the incoming quantized data:</source>
          <target state="translated">对输入的量化数据进行线性变换。</target>
        </trans-unit>
        <trans-unit id="a2436507aea812ee14f1556396ee7b6b4cbdbe1d" translate="yes" xml:space="preserve">
          <source>Applies a multi-layer Elman RNN with</source>
          <target state="translated">应用多层埃尔曼RNN与</target>
        </trans-unit>
        <trans-unit id="bc4223d58e32410e3d29e09ef9a1db009d298cd1" translate="yes" xml:space="preserve">
          <source>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</source>
          <target state="translated">将多层门控循环单元(GRU)RNN应用于输入序列。</target>
        </trans-unit>
        <trans-unit id="02da21c27e34ce09b75309eefe898e142b0a5cb5" translate="yes" xml:space="preserve">
          <source>Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.</source>
          <target state="translated">将多层长短期记忆(LSTM)RNN应用于输入序列。</target>
        </trans-unit>
        <trans-unit id="7660fbeb32f57e21a2dce8c33a454664d3adf995" translate="yes" xml:space="preserve">
          <source>Applies a softmax followed by a logarithm.</source>
          <target state="translated">应用一个软最大值,然后是一个对数。</target>
        </trans-unit>
        <trans-unit id="16d6a0fbab19784fd6044224005fc862384c1a6c" translate="yes" xml:space="preserve">
          <source>Applies a softmax function.</source>
          <target state="translated">应用软最大值功能。</target>
        </trans-unit>
        <trans-unit id="3511c68e1b0c335caf44167160bac43a1d7fe51a" translate="yes" xml:space="preserve">
          <source>Applies a softmin function.</source>
          <target state="translated">应用软敏功能。</target>
        </trans-unit>
        <trans-unit id="cfc3d07eb38a8451b3b1a0481d05e3ce6d35b93e" translate="yes" xml:space="preserve">
          <source>Applies alpha dropout to the input.</source>
          <target state="translated">将alpha dropout应用于输入。</target>
        </trans-unit>
        <trans-unit id="d7a232c41e3f6477e97dc6f5c3a6a2571c32c1b2" translate="yes" xml:space="preserve">
          <source>Applies element-wise</source>
          <target state="translated">逐项应用</target>
        </trans-unit>
        <trans-unit id="113342e492fe7e12d769f5fd51f90c782c13aba3" translate="yes" xml:space="preserve">
          <source>Applies element-wise the function</source>
          <target state="translated">按元素顺序应用函数</target>
        </trans-unit>
        <trans-unit id="f558b6f91b1db6ba6cd7e44d550b3b8856e30ab7" translate="yes" xml:space="preserve">
          <source>Applies element-wise,</source>
          <target state="translated">适用于元素方面。</target>
        </trans-unit>
        <trans-unit id="bdcea2507c8f3d67607511bd4053b83de34dceec" translate="yes" xml:space="preserve">
          <source>Applies element-wise, the function</source>
          <target state="translated">对元素进行应用,函数</target>
        </trans-unit>
        <trans-unit id="e6f0771cea3151fdc337ea7709543dfa37058eea" translate="yes" xml:space="preserve">
          <source>Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.</source>
          <target state="translated">对由多个输入平面组成的输入信号进行局部响应归一化,其中通道占第二维。</target>
        </trans-unit>
        <trans-unit id="5dfd4a4a091440af428452b6ccfc1c2f2eb1887c" translate="yes" xml:space="preserve">
          <source>Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.</source>
          <target state="translated">对由多个输入平面组成的输入信号进行局部响应归一化,其中通道占据第二维。适用于跨通道的归一化。</target>
        </trans-unit>
        <trans-unit id="4ed90484b4c8624d14fcf12a0e8edf257e35f7b9" translate="yes" xml:space="preserve">
          <source>Applies pruning reparametrization to the tensor corresponding to the parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; without actually pruning any units.</source>
          <target state="translated">将修剪重新参数化应用于与 &lt;code&gt;module&lt;/code&gt; 名为 &lt;code&gt;name&lt;/code&gt; 的参数相对应的张量，而无需实际修剪任何单位。</target>
        </trans-unit>
        <trans-unit id="459126de8057e720ba86989e3d80f97bc3223f0e" translate="yes" xml:space="preserve">
          <source>Applies quantized rectified linear unit function element-wise:</source>
          <target state="translated">应用量化整流线性单位函数元素化。</target>
        </trans-unit>
        <trans-unit id="fdc304cf2ec7d610cd24075ed2e43e1a79d41759" translate="yes" xml:space="preserve">
          <source>Applies spectral normalization to a parameter in the given module.</source>
          <target state="translated">对给定模块中的一个参数进行光谱归一化。</target>
        </trans-unit>
        <trans-unit id="2134b29fbca7ca9ac873ada1558b714c901876b0" translate="yes" xml:space="preserve">
          <source>Applies the</source>
          <target state="translated">适用于</target>
        </trans-unit>
        <trans-unit id="a78621f5f34b400e9542d53a0ce88e233a0027bb" translate="yes" xml:space="preserve">
          <source>Applies the Gaussian Error Linear Units function:</source>
          <target state="translated">应用高斯误差线性单位函数。</target>
        </trans-unit>
        <trans-unit id="26cda85cebacaa2f55fa0de1b622a487073b568a" translate="yes" xml:space="preserve">
          <source>Applies the HardTanh function element-wise</source>
          <target state="translated">对HardTanh函数进行逐个元素的应用。</target>
        </trans-unit>
        <trans-unit id="12f634982a2632889413c5e4a92650f28c9bdade" translate="yes" xml:space="preserve">
          <source>Applies the HardTanh function element-wise. See &lt;a href=&quot;generated/torch.nn.hardtanh#torch.nn.Hardtanh&quot;&gt;&lt;code&gt;Hardtanh&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">逐个应用HardTanh函数。有关更多详细信息，请参见&lt;a href=&quot;generated/torch.nn.hardtanh#torch.nn.Hardtanh&quot;&gt; &lt;code&gt;Hardtanh&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="95b3d284066a600a911ef55c57442d7f4b86507f" translate="yes" xml:space="preserve">
          <source>Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.</source>
          <target state="translated">将Softmax函数应用于一个n维输入Tensor,对它们进行重新缩放,使n维输出Tensor的元素位于[0,1]的范围内,并且和为1。</target>
        </trans-unit>
        <trans-unit id="01834f69d8e2aaf053d85470251564527cc332f1" translate="yes" xml:space="preserve">
          <source>Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range &lt;code&gt;[0, 1]&lt;/code&gt; and sum to 1.</source>
          <target state="translated">将Softmin函数应用于n维输入Tensor，对其进行重新缩放，以使n维输出Tensor的元素位于 &lt;code&gt;[0, 1]&lt;/code&gt; 范围内，总和为1。</target>
        </trans-unit>
        <trans-unit id="e581807715bdab40e3c6fa2d722c3f49a2fef542" translate="yes" xml:space="preserve">
          <source>Applies the element-wise function</source>
          <target state="translated">应用元素增量函数</target>
        </trans-unit>
        <trans-unit id="c91f96c06647ec0aa46ecdb47707667cb0cb30c3" translate="yes" xml:space="preserve">
          <source>Applies the element-wise function:</source>
          <target state="translated">应用元素轮换功能。</target>
        </trans-unit>
        <trans-unit id="20e17ca8da87d2e89b6ddbe8ec999295cc6de103" translate="yes" xml:space="preserve">
          <source>Applies the function &lt;code&gt;callable&lt;/code&gt; to each element in the tensor, replacing each element with the value returned by &lt;code&gt;callable&lt;/code&gt;.</source>
          <target state="translated">将 &lt;code&gt;callable&lt;/code&gt; 的函数应用于张量中的每个元素，将每个元素替换为 &lt;code&gt;callable&lt;/code&gt; 返回的值。</target>
        </trans-unit>
        <trans-unit id="4f92865eb288d8379f17f76a886fec1e420e13a3" translate="yes" xml:space="preserve">
          <source>Applies the hard shrinkage function element-wise</source>
          <target state="translated">逐元素应用硬收缩函数。</target>
        </trans-unit>
        <trans-unit id="56fce9ee97b2fb62286ad99c1ca046ac1d4d2b2c" translate="yes" xml:space="preserve">
          <source>Applies the hard shrinkage function element-wise:</source>
          <target state="translated">将硬性收缩函数逐一应用。</target>
        </trans-unit>
        <trans-unit id="ec160fe4b0b150b142f63859285de9f24ee55b53" translate="yes" xml:space="preserve">
          <source>Applies the hardswish function, element-wise, as described in the paper:</source>
          <target state="translated">应用艰辛函数,元素方面,如文中所述。</target>
        </trans-unit>
        <trans-unit id="936f4f75c2b9f6e3826efbb88a4e51ae2ab84f70" translate="yes" xml:space="preserve">
          <source>Applies the latest &lt;code&gt;method&lt;/code&gt; by computing the new partial masks and returning its combination with the &lt;code&gt;default_mask&lt;/code&gt;. The new partial mask should be computed on the entries or channels that were not zeroed out by the &lt;code&gt;default_mask&lt;/code&gt;. Which portions of the tensor &lt;code&gt;t&lt;/code&gt; the new mask will be calculated from depends on the &lt;code&gt;PRUNING_TYPE&lt;/code&gt; (handled by the type handler):</source>
          <target state="translated">通过计算新的局部掩码并返回其与 &lt;code&gt;default_mask&lt;/code&gt; 的组合来应用最新 &lt;code&gt;method&lt;/code&gt; 。应该在没有被 &lt;code&gt;default_mask&lt;/code&gt; 归零的条目或通道上计算新的部分掩码。将根据新的掩码计算张量 &lt;code&gt;t&lt;/code&gt; 的哪一部分取决于 &lt;code&gt;PRUNING_TYPE&lt;/code&gt; （由类型处理程序处理）：</target>
        </trans-unit>
        <trans-unit id="7d91ea3d6a32f01c8496b5becb21a5b21016d757" translate="yes" xml:space="preserve">
          <source>Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper:</source>
          <target state="translated">应用论文中所述的随机泄漏整流衬垫单元函数,逐元。</target>
        </trans-unit>
        <trans-unit id="82d1c6b2a1ee5d5cde8c89bdcc2746329c59171c" translate="yes" xml:space="preserve">
          <source>Applies the rectified linear unit function element-wise. See &lt;a href=&quot;#torch.nn.quantized.ReLU&quot;&gt;&lt;code&gt;ReLU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">按元素应用整流线性单位函数。有关更多详细信息，请参见&lt;a href=&quot;#torch.nn.quantized.ReLU&quot;&gt; &lt;code&gt;ReLU&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="bbd19df946d0f4bb7fc4682300e73774ba71feab" translate="yes" xml:space="preserve">
          <source>Applies the rectified linear unit function element-wise. See &lt;a href=&quot;generated/torch.nn.relu#torch.nn.ReLU&quot;&gt;&lt;code&gt;ReLU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">按元素应用整流线性单位函数。有关更多详细信息，请参见&lt;a href=&quot;generated/torch.nn.relu#torch.nn.ReLU&quot;&gt; &lt;code&gt;ReLU&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="78f0af302507bc331ec744dca018a734a0627b7b" translate="yes" xml:space="preserve">
          <source>Applies the rectified linear unit function element-wise:</source>
          <target state="translated">对整流后的线性单位函数进行逐元应用。</target>
        </trans-unit>
        <trans-unit id="43edbfe54e1d38b1a754c8ddb026e3655a93da7b" translate="yes" xml:space="preserve">
          <source>Applies the silu function, element-wise.</source>
          <target state="translated">应用silu函数,按元素排列。</target>
        </trans-unit>
        <trans-unit id="f0e7dde2fc1c92ff1943988e21d9ce05bb8c8f04" translate="yes" xml:space="preserve">
          <source>Applies the soft shrinkage function elementwise</source>
          <target state="translated">从元素上应用软收缩功能。</target>
        </trans-unit>
        <trans-unit id="7e5fad00eee592f1c4a5342fdc927299a5993172" translate="yes" xml:space="preserve">
          <source>Applies the soft shrinkage function elementwise:</source>
          <target state="translated">从元素上应用软收缩功能。</target>
        </trans-unit>
        <trans-unit id="4f3fa5f7feb89ef4634370ab877a48de7ffe1ed7" translate="yes" xml:space="preserve">
          <source>Applies weight normalization to a parameter in the given module.</source>
          <target state="translated">对给定模块中的参数进行权重归一化。</target>
        </trans-unit>
        <trans-unit id="adb3943666d494e8bd63152770abcc75d5223bd5" translate="yes" xml:space="preserve">
          <source>Applying &lt;a href=&quot;torch.diag_embed#torch.diag_embed&quot;&gt;&lt;code&gt;torch.diag_embed()&lt;/code&gt;&lt;/a&gt; to the output of this function with the same arguments yields a diagonal matrix with the diagonal entries of the input. However, &lt;a href=&quot;torch.diag_embed#torch.diag_embed&quot;&gt;&lt;code&gt;torch.diag_embed()&lt;/code&gt;&lt;/a&gt; has different default dimensions, so those need to be explicitly specified.</source>
          <target state="translated">使用相同的参数将&lt;a href=&quot;torch.diag_embed#torch.diag_embed&quot;&gt; &lt;code&gt;torch.diag_embed()&lt;/code&gt; &lt;/a&gt;应用于此函数的输出会产生一个对角矩阵，其中包含输入的对角线条目。但是，&lt;a href=&quot;torch.diag_embed#torch.diag_embed&quot;&gt; &lt;code&gt;torch.diag_embed()&lt;/code&gt; &lt;/a&gt;具有不同的默认尺寸，因此需要明确指定这些尺寸。</target>
        </trans-unit>
        <trans-unit id="09ebc50e43a0da169dba1646717349a905de80e9" translate="yes" xml:space="preserve">
          <source>Applying &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;torch.diagonal()&lt;/code&gt;&lt;/a&gt; to the output of this function with the same arguments yields a matrix identical to input. However, &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;torch.diagonal()&lt;/code&gt;&lt;/a&gt; has different default dimensions, so those need to be explicitly specified.</source>
          <target state="translated">使用相同的参数将&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;torch.diagonal()&lt;/code&gt; &lt;/a&gt;应用于此函数的输出将产生与输入相同的矩阵。但是，&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;torch.diagonal()&lt;/code&gt; &lt;/a&gt;具有不同的默认尺寸，因此需要明确指定这些尺寸。</target>
        </trans-unit>
        <trans-unit id="d22103e4c9027e5c172cbf43f3dd7371a41d32a7" translate="yes" xml:space="preserve">
          <source>Arbitrary positional and keyword inputs are allowed to be passed into DataParallel but some types are specially handled. tensors will be &lt;strong&gt;scattered&lt;/strong&gt; on dim specified (default 0). tuple, list and dict types will be shallow copied. The other types will be shared among different threads and can be corrupted if written to in the model&amp;rsquo;s forward pass.</source>
          <target state="translated">允许将任意位置和关键字输入传递到DataParallel中，但某些类型经过特殊处理。张量将在指定的dim上&lt;strong&gt;分散&lt;/strong&gt;（默认为0）。元组，列表和字典类型将被浅表复制。其他类型将在不同线程之间共享，并且如果写入模型的前向传递中，则可能会损坏其他类型。</target>
        </trans-unit>
        <trans-unit id="b6075aed00d09ffa3057c99d13847e6be9606a66" translate="yes" xml:space="preserve">
          <source>Args:</source>
          <target state="translated">Args:</target>
        </trans-unit>
        <trans-unit id="511f2c74f69da56453fcefe6e26731eae720fb15" translate="yes" xml:space="preserve">
          <source>Args: &lt;code&gt;mod&lt;/code&gt; a float module, either produced by torch.quantization utilities or directly from user</source>
          <target state="translated">参数数量： &lt;code&gt;mod&lt;/code&gt; 浮子模块，或者通过torch.quantization实用程序或从用户产生直接</target>
        </trans-unit>
        <trans-unit id="155bff5864cb1f24be13ad5ed169f780a903d97c" translate="yes" xml:space="preserve">
          <source>Arguments can also be &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">参数也可以为 &lt;code&gt;None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="a0478ca5f4c068ca3ac12f9474f6a15865ce9053" translate="yes" xml:space="preserve">
          <source>Arguments:</source>
          <target state="translated">Arguments:</target>
        </trans-unit>
        <trans-unit id="3839352701cd3d321f8924d7921c49d951abf8f7" translate="yes" xml:space="preserve">
          <source>Arguments::</source>
          <target state="translated">Arguments::</target>
        </trans-unit>
        <trans-unit id="6104f39ed22a2cd32e98536a3447a01c4b9f4781" translate="yes" xml:space="preserve">
          <source>Arithmetic Operators</source>
          <target state="translated">算术运算符</target>
        </trans-unit>
        <trans-unit id="c0e74bbb76aa26e443aa08680a9aad05da89267b" translate="yes" xml:space="preserve">
          <source>Art B. Owen. Scrambling Sobol and Niederreiter-Xing points. Journal of Complexity, 14(4):466-489, December 1998.</source>
          <target state="translated">Art B.Owen.Scrambling Sobol and Niederreiter-Xing points.Journal of Complexity,14(4):466-489,December 1998.</target>
        </trans-unit>
        <trans-unit id="44e38ee54f654b08e43c3403586961f50c567585" translate="yes" xml:space="preserve">
          <source>As a result of these changes, the following items are considered deprecated and should not appear in new code:</source>
          <target state="translated">由于这些变化,以下项目被认为是过时的,不应该出现在新的代码中。</target>
        </trans-unit>
        <trans-unit id="9f1d874b6f85d0af8d36d263ad9bfb71343abc57" translate="yes" xml:space="preserve">
          <source>As a special case, when &lt;code&gt;input&lt;/code&gt; has zero dimensions and a nonzero scalar value, it is treated as a one-dimensional tensor with one element.</source>
          <target state="translated">作为一种特殊情况，当 &lt;code&gt;input&lt;/code&gt; 具有零维且标量值非零时，会将其视为具有一个元素的一维张量。</target>
        </trans-unit>
        <trans-unit id="50ba4ee86165b9263342121969588460e8e3d7d0" translate="yes" xml:space="preserve">
          <source>As a subset of Python, any valid TorchScript function is also a valid Python function. This makes it possible to &lt;code&gt;disable TorchScript&lt;/code&gt; and debug the function using standard Python tools like &lt;code&gt;pdb&lt;/code&gt;. The reverse is not true: there are many valid Python programs that are not valid TorchScript programs. Instead, TorchScript focuses specifically on the features of Python that are needed to represent neural network models in PyTorch.</source>
          <target state="translated">作为Python的子集，任何有效的TorchScript函数也是有效的Python函数。这样就可以使用标准Python工具（如 &lt;code&gt;pdb&lt;/code&gt; ) &lt;code&gt;disable TorchScript&lt;/code&gt; 并调试功能。反之则不成立：有许多有效的Python程序不是有效的TorchScript程序。相反，TorchScript特别专注于表示PyTorch中的神经网络模型所需的Python功能。</target>
        </trans-unit>
        <trans-unit id="b6cd63f503882dea2536baff15dc121bd94a50a6" translate="yes" xml:space="preserve">
          <source>As above, but the sample points are spaced uniformly at a distance of &lt;code&gt;dx&lt;/code&gt;.</source>
          <target state="translated">如上所述，但是采样点以 &lt;code&gt;dx&lt;/code&gt; 的距离均匀地间隔开。</target>
        </trans-unit>
        <trans-unit id="4514250ef4bd2637a52c6977efbe9cd977631d0f" translate="yes" xml:space="preserve">
          <source>As described in the paper &lt;a href=&quot;https://arxiv.org/abs/1411.4280&quot;&gt;Efficient Object Localization Using Convolutional Networks&lt;/a&gt; , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.</source>
          <target state="translated">如论文《&lt;a href=&quot;https://arxiv.org/abs/1411.4280&quot;&gt;使用卷积网络进行有效对象定位》所述&lt;/a&gt;，如果特征图中的相邻像素高度相关（通常是早期卷积层的情况），那么iid缺失将不会使激活规律化，否则只会导致有效的学习率减少。</target>
        </trans-unit>
        <trans-unit id="99099b94ea4fc98fc5c41ec54e22d892cee1ab67" translate="yes" xml:space="preserve">
          <source>As of 0.4, this function does not support an &lt;code&gt;out&lt;/code&gt; keyword. As an alternative, the old &lt;code&gt;torch.ones_like(input, out=output)&lt;/code&gt; is equivalent to &lt;code&gt;torch.ones(input.size(), out=output)&lt;/code&gt;.</source>
          <target state="translated">从0.4开始，此功能不支持 &lt;code&gt;out&lt;/code&gt; 关键字。作为替代方案，旧的 &lt;code&gt;torch.ones_like(input, out=output)&lt;/code&gt; 等效于 &lt;code&gt;torch.ones(input.size(), out=output)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="3fb950c139664617530977c05541f892b77d98f6" translate="yes" xml:space="preserve">
          <source>As of 0.4, this function does not support an &lt;code&gt;out&lt;/code&gt; keyword. As an alternative, the old &lt;code&gt;torch.zeros_like(input, out=output)&lt;/code&gt; is equivalent to &lt;code&gt;torch.zeros(input.size(), out=output)&lt;/code&gt;.</source>
          <target state="translated">从0.4开始，此功能不支持 &lt;code&gt;out&lt;/code&gt; 关键字。作为替代方案，旧的 &lt;code&gt;torch.zeros_like(input, out=output)&lt;/code&gt; 等效于 &lt;code&gt;torch.zeros(input.size(), out=output)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5d301eba7fb8fca354fa028e6aad9db923bee3a2" translate="yes" xml:space="preserve">
          <source>As of PyTorch v1.7, Windows support for the distributed package only covers collective communications with Gloo backend, &lt;code&gt;FileStore&lt;/code&gt;, and &lt;code&gt;DistributedDataParallel&lt;/code&gt;. Therefore, the &lt;code&gt;init_method&lt;/code&gt; argument in &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; must point to a file. This works for both local and shared file systems:</source>
          <target state="translated">从PyTorch v1.7开始，Windows对分布式软件包的支持仅涵盖与Gloo后端， &lt;code&gt;FileStore&lt;/code&gt; 和 &lt;code&gt;DistributedDataParallel&lt;/code&gt; 的集体通信。因此，&lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; 中&lt;/a&gt;的 &lt;code&gt;init_method&lt;/code&gt; 参数必须指向文件。这适用于本地和共享文件系统：</target>
        </trans-unit>
        <trans-unit id="80184e7d134febd3ee09d8016449ea572c67e780" translate="yes" xml:space="preserve">
          <source>As with &lt;a href=&quot;torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt;&lt;code&gt;NLLLoss&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;input&lt;/code&gt; given is expected to contain &lt;em&gt;log-probabilities&lt;/em&gt; and is not restricted to a 2D Tensor. The targets are interpreted as &lt;em&gt;probabilities&lt;/em&gt; by default, but could be considered as &lt;em&gt;log-probabilities&lt;/em&gt; with &lt;code&gt;log_target&lt;/code&gt; set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">与&lt;a href=&quot;torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt; &lt;code&gt;NLLLoss&lt;/code&gt; 一样&lt;/a&gt;，给定的 &lt;code&gt;input&lt;/code&gt; 应包含&lt;em&gt;对数概率，&lt;/em&gt;并且不限于2D张量。这些目标解释为&lt;em&gt;概率&lt;/em&gt;默认，但可被视为&lt;em&gt;对数概率&lt;/em&gt;与 &lt;code&gt;log_target&lt;/code&gt; 设置为 &lt;code&gt;True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="99266bbca47ea9ff5e29020debec6a9680d5bf7a" translate="yes" xml:space="preserve">
          <source>As with image classification models, all pre-trained models expect input images normalized in the same way. The images have to be loaded in to a range of &lt;code&gt;[0, 1]&lt;/code&gt; and then normalized using &lt;code&gt;mean = [0.485, 0.456, 0.406]&lt;/code&gt; and &lt;code&gt;std = [0.229, 0.224, 0.225]&lt;/code&gt;. They have been trained on images resized such that their minimum size is 520.</source>
          <target state="translated">与图像分类模型一样，所有预先训练的模型都希望输入图像以相同的方式归一化。图像必须加载到 &lt;code&gt;[0, 1]&lt;/code&gt; 的范围内，然后使用 &lt;code&gt;mean = [0.485, 0.456, 0.406]&lt;/code&gt; 和 &lt;code&gt;std = [0.229, 0.224, 0.225]&lt;/code&gt; ]进行归一化。已对它们进行了尺寸调整后的图像训练，使其最小尺寸为520。</target>
        </trans-unit>
        <trans-unit id="8b593995c88f61a609044a01e37e8e4ccc22e065" translate="yes" xml:space="preserve">
          <source>Assumptions</source>
          <target state="translated">Assumptions</target>
        </trans-unit>
        <trans-unit id="855f8e40c74ae8a51a99a06c8a2a032f398b5ed4" translate="yes" xml:space="preserve">
          <source>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</source>
          <target state="translated">异步工作句柄,如果async_op被设置为True。如果不是async_op或不属于该组,则为None。</target>
        </trans-unit>
        <trans-unit id="a78338cab2cb5c76c6ab7df23e81a35449f6df4f" translate="yes" xml:space="preserve">
          <source>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.</source>
          <target state="translated">如果async_op设置为True,则为异步工作句柄。如果不是async_op或者不是组的一部分,则为None。</target>
        </trans-unit>
        <trans-unit id="4042af2d84df423621c1b7387da3334c7f85d192" translate="yes" xml:space="preserve">
          <source>Async work handle, if async_op is set to True. None, otherwise</source>
          <target state="translated">如果async_op设置为True,则为异步工作句柄。无,否则</target>
        </trans-unit>
        <trans-unit id="1889aec46759ffe70c5c9ddb3fee3b5ee5713f7b" translate="yes" xml:space="preserve">
          <source>At groups= &lt;code&gt;in_channels&lt;/code&gt;, each input channel is convolved with its own set of filters (of size</source>
          <target state="translated">在groups = &lt;code&gt;in_channels&lt;/code&gt; 时，每个输入通道都与自己的一组过滤器（大小相同）进行卷积</target>
        </trans-unit>
        <trans-unit id="6510deaa2781bdfd17b677974666275b9af08554" translate="yes" xml:space="preserve">
          <source>At groups= &lt;code&gt;in_channels&lt;/code&gt;, each input channel is convolved with its own set of filters, of size</source>
          <target state="translated">在groups = &lt;code&gt;in_channels&lt;/code&gt; 时，每个输入通道都与自己的一组过滤器卷积，其大小</target>
        </trans-unit>
        <trans-unit id="9d3f19004934696d7b2d6e8d06f0e3a963b23772" translate="yes" xml:space="preserve">
          <source>At groups= &lt;code&gt;in_channels&lt;/code&gt;, each input channel is convolved with its own set of filters, of size:</source>
          <target state="translated">在groups = &lt;code&gt;in_channels&lt;/code&gt; 处，每个输入通道都与自己的一组过滤器卷积，其大小为：</target>
        </trans-unit>
        <trans-unit id="bcb1bde7e11ed9170bfe340f3dbc5867346695da" translate="yes" xml:space="preserve">
          <source>At groups=1, all inputs are convolved to all outputs.</source>
          <target state="translated">当组数=1时,所有输入都被卷积到所有输出。</target>
        </trans-unit>
        <trans-unit id="f9d6240328fee5db3b7a1921b6420e8a8acf15ff" translate="yes" xml:space="preserve">
          <source>At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.</source>
          <target state="translated">当组数=2时,操作变成相当于有两个conv层并排,各看到一半的输入通道,并产生一半的输出通道,随后都会被连通。</target>
        </trans-unit>
        <trans-unit id="da760faa855b591ad5bebacae023465206f25e52" translate="yes" xml:space="preserve">
          <source>At p =</source>
          <target state="translated">在p =时</target>
        </trans-unit>
        <trans-unit id="ac85559223d37e290d1c1660fd99f2a4cf5904d9" translate="yes" xml:space="preserve">
          <source>At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)</source>
          <target state="translated">当p=1时,可以得到Sum Pooling(与Average Pooling成正比)。</target>
        </trans-unit>
        <trans-unit id="9dbb94515aba090ad21fcfa70bd96411862efa24" translate="yes" xml:space="preserve">
          <source>At p = 1, one gets Sum Pooling (which is proportional to average pooling)</source>
          <target state="translated">当p=1时,可以得到Sum Pooling(与平均集合度成正比)。</target>
        </trans-unit>
        <trans-unit id="ba60e6b895acf11e04420dfecb408950d41fdcb0" translate="yes" xml:space="preserve">
          <source>At the heart of PyTorch data loading utility is the &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;torch.utils.data.DataLoader&lt;/code&gt;&lt;/a&gt; class. It represents a Python iterable over a dataset, with support for</source>
          <target state="translated">PyTorch数据加载实用程序的核心是&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;torch.utils.data.DataLoader&lt;/code&gt; &lt;/a&gt;类。它表示可在数据集上迭代的Python，并支持</target>
        </trans-unit>
        <trans-unit id="74e0b9c80dca267a78a89ad68a2f6c73241b5973" translate="yes" xml:space="preserve">
          <source>Attention</source>
          <target state="translated">Attention</target>
        </trans-unit>
        <trans-unit id="9b0aef7ef75761256026f8850aa6308137966a97" translate="yes" xml:space="preserve">
          <source>Attribute Lookup On Python Modules</source>
          <target state="translated">Python模块上的属性查询</target>
        </trans-unit>
        <trans-unit id="a6652617f2c799eb11ee727b16c5646c48af6905" translate="yes" xml:space="preserve">
          <source>Attributes</source>
          <target state="translated">Attributes</target>
        </trans-unit>
        <trans-unit id="cde0319b4a4a6d08df1fd51fb49e58f04a817a7e" translate="yes" xml:space="preserve">
          <source>Attributes of a ScriptModule can be marked constant by annotating them with &lt;code&gt;Final[T]&lt;/code&gt;</source>
          <target state="translated">可以通过用 &lt;code&gt;Final[T]&lt;/code&gt; 注释ScriptModule的属性来将其标记为常量。</target>
        </trans-unit>
        <trans-unit id="effc3797a92fafe90a0f719052ebdd67ab00baf2" translate="yes" xml:space="preserve">
          <source>Attributes: Same as torch.nn.quantized.Conv3d</source>
          <target state="translated">属性。与Torch.nn.quantized.Conv3d相同。</target>
        </trans-unit>
        <trans-unit id="e326babbae1150bc97146fd07419454c68ba8c11" translate="yes" xml:space="preserve">
          <source>Autocast Op Reference</source>
          <target state="translated">自动播报操作参考</target>
        </trans-unit>
        <trans-unit id="9b3dcde733ca512dd4a6b9b1a49fa5f26623a298" translate="yes" xml:space="preserve">
          <source>Autocasting</source>
          <target state="translated">Autocasting</target>
        </trans-unit>
        <trans-unit id="78965d92bd603a457514f5c8bd35fc06957dafd3" translate="yes" xml:space="preserve">
          <source>Autograd currently supports named tensors in a limited manner: autograd ignores names on all tensors. Gradient computation is still correct but we lose the safety that names give us.</source>
          <target state="translated">Autograd目前对命名时序的支持是有限的:autograd会忽略所有时序上的名称。梯度计算仍然是正确的,但我们失去了名字给我们带来的安全性。</target>
        </trans-unit>
        <trans-unit id="03567b4fb2f83cc2c2baad9124a8e9bc1b24aeed" translate="yes" xml:space="preserve">
          <source>Autograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU. There are two modes implemented at the moment - CPU-only using &lt;a href=&quot;#torch.autograd.profiler.profile&quot;&gt;&lt;code&gt;profile&lt;/code&gt;&lt;/a&gt;. and nvprof based (registers both CPU and GPU activity) using &lt;a href=&quot;#torch.autograd.profiler.emit_nvtx&quot;&gt;&lt;code&gt;emit_nvtx&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Autograd包括一个探查器，可让您检查模型中不同运算符的成本-在CPU和GPU上均如此。目前有两种模式-仅使用&lt;a href=&quot;#torch.autograd.profiler.profile&quot;&gt; &lt;code&gt;profile&lt;/code&gt; &lt;/a&gt;使用CPU 。和基于nvprof的（注册了CPU和GPU活动），使用了&lt;a href=&quot;#torch.autograd.profiler.emit_nvtx&quot;&gt; &lt;code&gt;emit_nvtx&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="1cead4310cfb62879040be96cb0226da15307414" translate="yes" xml:space="preserve">
          <source>Autograd is supported, see &lt;a href=&quot;#named-tensors-autograd-doc&quot;&gt;Autograd support&lt;/a&gt;. Because gradients are currently unnamed, optimizers may work but are untested.</source>
          <target state="translated">支持&lt;a href=&quot;#named-tensors-autograd-doc&quot;&gt;Autograd&lt;/a&gt;，请参阅Autograd支持。由于当前未命名渐变，因此优化程序可能有效，但未经测试。</target>
        </trans-unit>
        <trans-unit id="1fc85b24b64820e5c1a91ec86a7fdeb96b65e6ed" translate="yes" xml:space="preserve">
          <source>Autograd mechanics</source>
          <target state="translated">自动机械师</target>
        </trans-unit>
        <trans-unit id="a6a465a4bfe05284d5084187d5372a6d8da9b829" translate="yes" xml:space="preserve">
          <source>Autograd recording during the forward pass</source>
          <target state="translated">前传过程中的自签记录</target>
        </trans-unit>
        <trans-unit id="7807d41b89e768682ae049c6166592980a550e48" translate="yes" xml:space="preserve">
          <source>Autograd support</source>
          <target state="translated">签名支持</target>
        </trans-unit>
        <trans-unit id="9b8005aa270f8147e0440468e241bd00f96800bc" translate="yes" xml:space="preserve">
          <source>Automatic Mixed Precision examples</source>
          <target state="translated">自动混合精度实例</target>
        </trans-unit>
        <trans-unit id="56280d27e59e4f9a79ed74f7e14cd8c0e581e855" translate="yes" xml:space="preserve">
          <source>Automatic Mixed Precision package - torch.cuda.amp</source>
          <target state="translated">自动混合精度包-torch.cuda.amp。</target>
        </trans-unit>
        <trans-unit id="0f3dc77bfd956e56fcd5b8e898b3fabd32b08034" translate="yes" xml:space="preserve">
          <source>Automatic Trace Checking</source>
          <target state="translated">自动跟踪检查</target>
        </trans-unit>
        <trans-unit id="16c312b664ed41d040921c5ac59416706fca528b" translate="yes" xml:space="preserve">
          <source>Automatic batching (default)</source>
          <target state="translated">自动分批(默认</target>
        </trans-unit>
        <trans-unit id="6b27a774aecaec8b1c217a82557d2af8c10933cc" translate="yes" xml:space="preserve">
          <source>Automatic differentiation package - torch.autograd</source>
          <target state="translated">自动区分包--Torch.autograd。</target>
        </trans-unit>
        <trans-unit id="b29f65429a1d343b4fb7640c7ef2b8fc40202a32" translate="yes" xml:space="preserve">
          <source>Available for Python &amp;gt;= 3.4.</source>
          <target state="translated">适用于Python&amp;gt; = 3.4。</target>
        </trans-unit>
        <trans-unit id="feed24a7c6df28036011e42f91dd99e7f26aa307" translate="yes" xml:space="preserve">
          <source>Averages all events.</source>
          <target state="translated">所有事件的平均值。</target>
        </trans-unit>
        <trans-unit id="85ece30f6d8af988432cf40fa8753c609179aa8c" translate="yes" xml:space="preserve">
          <source>Averages all function events over their keys.</source>
          <target state="translated">将所有的函数事件平均到它们的键上。</target>
        </trans-unit>
        <trans-unit id="1def506ac3e846cb4c938843152cd0b9bba71135" translate="yes" xml:space="preserve">
          <source>AvgPool1d</source>
          <target state="translated">AvgPool1d</target>
        </trans-unit>
        <trans-unit id="c673cdd08db86374f9ff97d13da9945ad47b1180" translate="yes" xml:space="preserve">
          <source>AvgPool2d</source>
          <target state="translated">AvgPool2d</target>
        </trans-unit>
        <trans-unit id="fc7fc296c7e1f04112232fe8a9e0e82d5c7f190f" translate="yes" xml:space="preserve">
          <source>AvgPool3d</source>
          <target state="translated">AvgPool3d</target>
        </trans-unit>
        <trans-unit id="6da0341102c44a220c312de3110fd1209e71eaf5" translate="yes" xml:space="preserve">
          <source>Ax = b</source>
          <target state="translated">Ax=b</target>
        </trans-unit>
        <trans-unit id="ae4f281df5a5d0ff3cad6371f76d5c29b6d953ec" translate="yes" xml:space="preserve">
          <source>B</source>
          <target state="translated">B</target>
        </trans-unit>
        <trans-unit id="a9d087c32e7ca877fdba43990aa26038f88f00de" translate="yes" xml:space="preserve">
          <source>B \times P \times M</source>
          <target state="translated">B/times P/times M.</target>
        </trans-unit>
        <trans-unit id="db6d439058d0e22e68b29c790c70d29e6828be28" translate="yes" xml:space="preserve">
          <source>B \times P \times R</source>
          <target state="translated">B/times P/times R.</target>
        </trans-unit>
        <trans-unit id="2dfbff36e0eaece5eff122157b4277846a87f5f4" translate="yes" xml:space="preserve">
          <source>B \times R \times M</source>
          <target state="translated">B/times R/times M.</target>
        </trans-unit>
        <trans-unit id="45bdfdeb5092f65ada4a9a1c95c08cee2ffa142d" translate="yes" xml:space="preserve">
          <source>BAND</source>
          <target state="translated">BAND</target>
        </trans-unit>
        <trans-unit id="0ef786803f54f9a9e02024a654989e60c7f87b66" translate="yes" xml:space="preserve">
          <source>BCELoss</source>
          <target state="translated">BCELoss</target>
        </trans-unit>
        <trans-unit id="c174768efb833ed7a3edebffb9950a4f61c4c940" translate="yes" xml:space="preserve">
          <source>BCEWithLogitsLoss</source>
          <target state="translated">BCEWithLogitsLoss</target>
        </trans-unit>
        <trans-unit id="2c89bbb2577ddfe977123efaba3737f659629fff" translate="yes" xml:space="preserve">
          <source>BLAS and LAPACK Operations</source>
          <target state="translated">BLAS和LAPACK业务</target>
        </trans-unit>
        <trans-unit id="4aefcf5c188e5bba658f180d7b47a0379fa5e70c" translate="yes" xml:space="preserve">
          <source>BOR</source>
          <target state="translated">BOR</target>
        </trans-unit>
        <trans-unit id="bd606f52a5ad5bc6c6cc894b3accba4125210f66" translate="yes" xml:space="preserve">
          <source>BXOR</source>
          <target state="translated">BXOR</target>
        </trans-unit>
        <trans-unit id="e758ca64563fdd62965a2b97b76d555b1a70d938" translate="yes" xml:space="preserve">
          <source>Backend</source>
          <target state="translated">Backend</target>
        </trans-unit>
        <trans-unit id="b3776d63ad7b7a84bfe20d9c6d4a53a2b25d0e43" translate="yes" xml:space="preserve">
          <source>Backends</source>
          <target state="translated">Backends</target>
        </trans-unit>
        <trans-unit id="82921ea0c75d2b9e5a6aaafaf19c2a51a8ee3c26" translate="yes" xml:space="preserve">
          <source>Backends that come with PyTorch</source>
          <target state="translated">PyTorch附带的后端</target>
        </trans-unit>
        <trans-unit id="64dd60fe1a049fe6db3eb1369dec2e42bf428e21" translate="yes" xml:space="preserve">
          <source>Background</source>
          <target state="translated">Background</target>
        </trans-unit>
        <trans-unit id="4c7660dd341e52619271ac35d19b4aa963dcdc89" translate="yes" xml:space="preserve">
          <source>Backward through &lt;a href=&quot;#torch.det&quot;&gt;&lt;code&gt;det()&lt;/code&gt;&lt;/a&gt; internally uses SVD results when &lt;code&gt;input&lt;/code&gt; is not invertible. In this case, double backward through &lt;a href=&quot;#torch.det&quot;&gt;&lt;code&gt;det()&lt;/code&gt;&lt;/a&gt; will be unstable in when &lt;code&gt;input&lt;/code&gt; doesn&amp;rsquo;t have distinct singular values. See &lt;a href=&quot;torch.svd#torch.svd&quot;&gt;&lt;code&gt;svd()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">当 &lt;code&gt;input&lt;/code&gt; 不可逆时，通过&lt;a href=&quot;#torch.det&quot;&gt; &lt;code&gt;det()&lt;/code&gt; &lt;/a&gt;向后进行内部使用SVD结果。在这种情况下，如果 &lt;code&gt;input&lt;/code&gt; 没有明显的奇异值，则通过&lt;a href=&quot;#torch.det&quot;&gt; &lt;code&gt;det()&lt;/code&gt; &lt;/a&gt;向后翻倍将变得不稳定。有关详细信息，请参见&lt;a href=&quot;torch.svd#torch.svd&quot;&gt; &lt;code&gt;svd()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c28a70ba3b665ff380843011cb9bf4004fe6ce27" translate="yes" xml:space="preserve">
          <source>Backward through &lt;a href=&quot;#torch.logdet&quot;&gt;&lt;code&gt;logdet()&lt;/code&gt;&lt;/a&gt; internally uses SVD results when &lt;code&gt;input&lt;/code&gt; is not invertible. In this case, double backward through &lt;a href=&quot;#torch.logdet&quot;&gt;&lt;code&gt;logdet()&lt;/code&gt;&lt;/a&gt; will be unstable in when &lt;code&gt;input&lt;/code&gt; doesn&amp;rsquo;t have distinct singular values. See &lt;a href=&quot;torch.svd#torch.svd&quot;&gt;&lt;code&gt;svd()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">当 &lt;code&gt;input&lt;/code&gt; 不可逆时，通过&lt;a href=&quot;#torch.logdet&quot;&gt; &lt;code&gt;logdet()&lt;/code&gt; &lt;/a&gt;向后进行内部使用SVD结果。在这种情况下，当 &lt;code&gt;input&lt;/code&gt; 没有明显的奇异值时，通过&lt;a href=&quot;#torch.logdet&quot;&gt; &lt;code&gt;logdet()&lt;/code&gt; &lt;/a&gt;进行两次向后倒转将是不稳定的。有关详细信息，请参见&lt;a href=&quot;torch.svd#torch.svd&quot;&gt; &lt;code&gt;svd()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="28943ec2b2ba443d9dbd186c1f96eafcfe632579" translate="yes" xml:space="preserve">
          <source>Backward through &lt;a href=&quot;#torch.slogdet&quot;&gt;&lt;code&gt;slogdet()&lt;/code&gt;&lt;/a&gt; internally uses SVD results when &lt;code&gt;input&lt;/code&gt; is not invertible. In this case, double backward through &lt;a href=&quot;#torch.slogdet&quot;&gt;&lt;code&gt;slogdet()&lt;/code&gt;&lt;/a&gt; will be unstable in when &lt;code&gt;input&lt;/code&gt; doesn&amp;rsquo;t have distinct singular values. See &lt;a href=&quot;torch.svd#torch.svd&quot;&gt;&lt;code&gt;svd()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">当 &lt;code&gt;input&lt;/code&gt; 不可逆时，向后通过&lt;a href=&quot;#torch.slogdet&quot;&gt; &lt;code&gt;slogdet()&lt;/code&gt; &lt;/a&gt;内部使用SVD结果。在这种情况下，当 &lt;code&gt;input&lt;/code&gt; 没有明显的奇异值时，通过&lt;a href=&quot;#torch.slogdet&quot;&gt; &lt;code&gt;slogdet()&lt;/code&gt; &lt;/a&gt;向后翻倍将变得不稳定。有关详细信息，请参见&lt;a href=&quot;torch.svd#torch.svd&quot;&gt; &lt;code&gt;svd()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="bb44593f48eb0328822a985f5aa285528658fb23" translate="yes" xml:space="preserve">
          <source>Bartlett window function.</source>
          <target state="translated">巴特利特窗口功能。</target>
        </trans-unit>
        <trans-unit id="919b23578508277c7be92184e7175d68ef74ae7e" translate="yes" xml:space="preserve">
          <source>Base class for all Samplers.</source>
          <target state="translated">所有采样器的基类。</target>
        </trans-unit>
        <trans-unit id="8f6ce4f7d8508e7be7e4cda0e650d850c9905e8a" translate="yes" xml:space="preserve">
          <source>Base class for all neural network modules.</source>
          <target state="translated">所有神经网络模块的基础类。</target>
        </trans-unit>
        <trans-unit id="aa2e98de019e1d429d4b048d107e89660df3590b" translate="yes" xml:space="preserve">
          <source>Base class for all optimizers.</source>
          <target state="translated">所有优化器的基础类。</target>
        </trans-unit>
        <trans-unit id="3756aaad75fa5f15b1e3570c38539a1b4fa27093" translate="yes" xml:space="preserve">
          <source>Base class for all store implementations, such as the 3 provided by PyTorch distributed: (&lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt;&lt;code&gt;TCPStore&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt;&lt;code&gt;FileStore&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt;&lt;code&gt;HashStore&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">所有商店实现的基类，例如PyTorch所提供的3类分发的基类：（&lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt; &lt;code&gt;TCPStore&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;#torch.distributed.FileStore&quot;&gt; &lt;code&gt;FileStore&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;#torch.distributed.HashStore&quot;&gt; &lt;code&gt;HashStore&lt;/code&gt; &lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="7fa48cad7c036a5d05a3d1ee3ac94a5b6b79fa80" translate="yes" xml:space="preserve">
          <source>Base observer Module. Any observer implementation should derive from this class.</source>
          <target state="translated">基础观察者模块。任何观察者的实现都应该从这个类派生出来。</target>
        </trans-unit>
        <trans-unit id="fb7f5bac791ae3c9dcca099683932046b2428b2e" translate="yes" xml:space="preserve">
          <source>BasePruningMethod</source>
          <target state="translated">BasePruningMethod</target>
        </trans-unit>
        <trans-unit id="173d42bdad4a08f4dc4a342a93191085b664cf61" translate="yes" xml:space="preserve">
          <source>Bases: &lt;a href=&quot;#torch.distributions.distribution.Distribution&quot;&gt;&lt;code&gt;torch.distributions.distribution.Distribution&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">基础：&lt;a href=&quot;#torch.distributions.distribution.Distribution&quot;&gt; &lt;code&gt;torch.distributions.distribution.Distribution&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f6b8b8e70456b968792e96e115e64852d9f96501" translate="yes" xml:space="preserve">
          <source>Bases: &lt;a href=&quot;#torch.distributions.exp_family.ExponentialFamily&quot;&gt;&lt;code&gt;torch.distributions.exp_family.ExponentialFamily&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">基础：&lt;a href=&quot;#torch.distributions.exp_family.ExponentialFamily&quot;&gt; &lt;code&gt;torch.distributions.exp_family.ExponentialFamily&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a86b5e13aaeb2e61798bec3653ac860d7b6fac71" translate="yes" xml:space="preserve">
          <source>Bases: &lt;a href=&quot;#torch.distributions.gamma.Gamma&quot;&gt;&lt;code&gt;torch.distributions.gamma.Gamma&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">基础：&lt;a href=&quot;#torch.distributions.gamma.Gamma&quot;&gt; &lt;code&gt;torch.distributions.gamma.Gamma&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f37aa8ac66020a1c6c2b5aa83188590e63bd8613" translate="yes" xml:space="preserve">
          <source>Bases: &lt;a href=&quot;#torch.distributions.transformed_distribution.TransformedDistribution&quot;&gt;&lt;code&gt;torch.distributions.transformed_distribution.TransformedDistribution&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">基础：&lt;a href=&quot;#torch.distributions.transformed_distribution.TransformedDistribution&quot;&gt; &lt;code&gt;torch.distributions.transformed_distribution.TransformedDistribution&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="68ca77158d1716ec91be76868bd9ad42dd2154e8" translate="yes" xml:space="preserve">
          <source>Bases: &lt;a href=&quot;https://docs.python.org/3/library/functions.html#object&quot;&gt;&lt;code&gt;object&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">基础：&lt;a href=&quot;https://docs.python.org/3/library/functions.html#object&quot;&gt; &lt;code&gt;object&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7070665795a3b04470b95d5347795e00aee639f7" translate="yes" xml:space="preserve">
          <source>Basic name inference rules</source>
          <target state="translated">基本名称推断规则</target>
        </trans-unit>
        <trans-unit id="5fcebeefad3cdbbf8733aa928160dec7dc90c1a1" translate="yes" xml:space="preserve">
          <source>Basics</source>
          <target state="translated">Basics</target>
        </trans-unit>
        <trans-unit id="16f46715e1716fa628885ccd95ddf80183d01012" translate="yes" xml:space="preserve">
          <source>Batch sizes represent the number elements at each sequence step in the batch, not the varying sequence lengths passed to &lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt;&lt;code&gt;pack_padded_sequence()&lt;/code&gt;&lt;/a&gt;. For instance, given data &lt;code&gt;abc&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt; the &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;PackedSequence&lt;/code&gt;&lt;/a&gt; would contain data &lt;code&gt;axbc&lt;/code&gt; with &lt;code&gt;batch_sizes=[2,1,1]&lt;/code&gt;.</source>
          <target state="translated">批次大小表示批次中每个序列步骤的数量元素，而不是传递给&lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt; &lt;code&gt;pack_padded_sequence()&lt;/code&gt; &lt;/a&gt;的变化序列长度。例如，给定数据 &lt;code&gt;abc&lt;/code&gt; 和 &lt;code&gt;x&lt;/code&gt; ，&lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt; &lt;code&gt;PackedSequence&lt;/code&gt; &lt;/a&gt;将包含具有 &lt;code&gt;batch_sizes=[2,1,1]&lt;/code&gt; 的数据 &lt;code&gt;axbc&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="640ed9b96cb507644eb24c180e159849e7025eaa" translate="yes" xml:space="preserve">
          <source>BatchNorm</source>
          <target state="translated">BatchNorm</target>
        </trans-unit>
        <trans-unit id="c7c537e5e7d31a4a94ee4f8e5ebc01c365e0771a" translate="yes" xml:space="preserve">
          <source>BatchNorm1d</source>
          <target state="translated">BatchNorm1d</target>
        </trans-unit>
        <trans-unit id="539a37ce419a6050de06321e52ac8941c6a520dd" translate="yes" xml:space="preserve">
          <source>BatchNorm2d</source>
          <target state="translated">BatchNorm2d</target>
        </trans-unit>
        <trans-unit id="4036bb17e086d06a41c9d03825932c8325ec759f" translate="yes" xml:space="preserve">
          <source>BatchNorm3d</source>
          <target state="translated">BatchNorm3d</target>
        </trans-unit>
        <trans-unit id="58a700d3fc7db1c024d5e6e31878faaaa556887d" translate="yes" xml:space="preserve">
          <source>Because named tensors can coexist with unnamed tensors, refining names gives a nice way to write named-tensor-aware code that works with both named and unnamed tensors.</source>
          <target state="translated">因为命名的张量可以与未命名的张量共存,所以完善名称提供了一种很好的方法来编写命名张量感知代码,这种代码可以同时适用于命名和未命名的张量。</target>
        </trans-unit>
        <trans-unit id="0a3a2aefc4920597d8297a1ff6ee9586d1d76c12" translate="yes" xml:space="preserve">
          <source>Because of the similarity of APIs we do not document most of this package contents, and we recommend referring to very good docs of the original module.</source>
          <target state="translated">由于API的相似性,我们不记录这个包的大部分内容,建议参考原模块的非常好的文档。</target>
        </trans-unit>
        <trans-unit id="85ad6aa0a04066bce002e177e5b44f0e8ada2c9c" translate="yes" xml:space="preserve">
          <source>Because the Batch Normalization is done for each channel in the &lt;code&gt;C&lt;/code&gt; dimension, computing statistics on &lt;code&gt;(N, +)&lt;/code&gt; slices, it&amp;rsquo;s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.</source>
          <target state="translated">因为对 &lt;code&gt;C&lt;/code&gt; 维中的每个通道都进行了批量归一化，计算了 &lt;code&gt;(N, +)&lt;/code&gt; 切片的统计信息，所以通常将这种体积批量归一化或时空批量归一化称为术语。</target>
        </trans-unit>
        <trans-unit id="fd602c285d639d22638df01e41486fc6be835af0" translate="yes" xml:space="preserve">
          <source>Because the Batch Normalization is done over the &lt;code&gt;C&lt;/code&gt; dimension, computing statistics on &lt;code&gt;(N, D, H, W)&lt;/code&gt; slices, it&amp;rsquo;s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.</source>
          <target state="translated">因为批处理规范化是在 &lt;code&gt;C&lt;/code&gt; 维上完成的，计算 &lt;code&gt;(N, D, H, W)&lt;/code&gt; 切片的统计信息，所以通常将其称为&amp;ldquo;体积批处理规范化&amp;rdquo;或&amp;ldquo;时空批处理规范化&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="e3a6337703c27cf2e6ddd618d4baf0da4204696b" translate="yes" xml:space="preserve">
          <source>Because the Batch Normalization is done over the &lt;code&gt;C&lt;/code&gt; dimension, computing statistics on &lt;code&gt;(N, H, W)&lt;/code&gt; slices, it&amp;rsquo;s common terminology to call this Spatial Batch Normalization.</source>
          <target state="translated">因为批处理规范化是在 &lt;code&gt;C&lt;/code&gt; 维上完成的，计算 &lt;code&gt;(N, H, W)&lt;/code&gt; 切片的统计信息，所以通常将其称为&amp;ldquo;空间批处理规范化&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="288d84de2a904a705b5e7ec68955d71169bacf13" translate="yes" xml:space="preserve">
          <source>Because the Batch Normalization is done over the &lt;code&gt;C&lt;/code&gt; dimension, computing statistics on &lt;code&gt;(N, L)&lt;/code&gt; slices, it&amp;rsquo;s common terminology to call this Temporal Batch Normalization.</source>
          <target state="translated">因为批处理规范化是在 &lt;code&gt;C&lt;/code&gt; 维上完成的，计算 &lt;code&gt;(N, L)&lt;/code&gt; 切片的统计信息，所以通常将其称为&amp;ldquo;时间批处理规范化&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="0da3b2b883ef3b23c37366196cd69afd8c187cdd" translate="yes" xml:space="preserve">
          <source>Because the signal is Hermitian in the time-domain, the result will be real in the frequency domain. Note that some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in &lt;code&gt;input[0]&lt;/code&gt; would result in one or more complex frequency terms which cannot be represented in a real output and so will always be ignored.</source>
          <target state="translated">因为信号在时域中是Hermitian，所以结果在频域中是真实的。请注意，某些输入频率必须为实数才能满足Hermitian属性。在这些情况下，虚部将被忽略。例如， &lt;code&gt;input[0]&lt;/code&gt; 任何虚部都会导致一个或多个复杂的频率项，这些项无法在实际输出中表示，因此将始终被忽略。</target>
        </trans-unit>
        <trans-unit id="2e29f922f6839735f3a19bbe923a21ab693afacc" translate="yes" xml:space="preserve">
          <source>Because your script will be profiled, please ensure that it exits in a finite amount of time.</source>
          <target state="translated">因为你的脚本将被剖析,请确保它在有限的时间内退出。</target>
        </trans-unit>
        <trans-unit id="956a144b3d9996462b7b18cbedf9997fc2011650" translate="yes" xml:space="preserve">
          <source>Before going further, more details on TensorBoard can be found at &lt;a href=&quot;https://www.tensorflow.org/tensorboard/&quot;&gt;https://www.tensorflow.org/tensorboard/&lt;/a&gt;</source>
          <target state="translated">在继续之前，可以在&lt;a href=&quot;https://www.tensorflow.org/tensorboard/&quot;&gt;https://www.tensorflow.org/tensorboard/&lt;/a&gt;上找到有关TensorBoard的更多详细信息。</target>
        </trans-unit>
        <trans-unit id="8b23ce245f346d21fc21404045956dd5e7ec60de" translate="yes" xml:space="preserve">
          <source>Before using RPC and distributed autograd primitives, initialization must take place. To initialize the RPC framework we need to use &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt;&lt;code&gt;init_rpc()&lt;/code&gt;&lt;/a&gt; which would initialize the RPC framework, RRef framework and distributed autograd.</source>
          <target state="translated">在使用RPC和分布式autograd原语之前，必须进行初始化。要初始化RPC框架，我们需要使用&lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt; &lt;code&gt;init_rpc()&lt;/code&gt; &lt;/a&gt;来初始化RPC框架，RRef框架和分布式autograd。</target>
        </trans-unit>
        <trans-unit id="3e921d24df994f442d57e133126d10fb2ac6e163" translate="yes" xml:space="preserve">
          <source>Below is an example of running a TorchScript function using RPC.</source>
          <target state="translated">下面是一个使用RPC运行TorchScript函数的例子。</target>
        </trans-unit>
        <trans-unit id="dc8bffef30a767db701efa64dcad8d3b22a3cf6c" translate="yes" xml:space="preserve">
          <source>Bernoulli</source>
          <target state="translated">Bernoulli</target>
        </trans-unit>
        <trans-unit id="ad9db732260b1d62e536980667c87db0f86fe3e4" translate="yes" xml:space="preserve">
          <source>Bernoulli trials, the first</source>
          <target state="translated">伯努利试验</target>
        </trans-unit>
        <trans-unit id="01f8c251cc6790b547148a802d152b484b1bf377" translate="yes" xml:space="preserve">
          <source>Besides the GLOO/MPI/NCCL backends, PyTorch distributed supports third-party backends through a run-time register mechanism. For references on how to develop a third-party backend through C++ Extension, please refer to &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_extension.html&quot;&gt;Tutorials - Custom C++ and CUDA Extensions&lt;/a&gt; and &lt;code&gt;test/cpp_extensions/cpp_c10d_extension.cpp&lt;/code&gt;. The capability of third-party backends are decided by their own implementations.</source>
          <target state="translated">除了GLOO / MPI / NCCL后端外，PyTorch分布式还通过运行时注册机制支持第三方后端。有关如何通过C ++ Extension开发第三方后端的参考，请参考&lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_extension.html&quot;&gt;教程-自定义C ++和CUDA扩展&lt;/a&gt;以及 &lt;code&gt;test/cpp_extensions/cpp_c10d_extension.cpp&lt;/code&gt; 。第三方后端的功能由它们自己的实现决定。</target>
        </trans-unit>
        <trans-unit id="f03b60f7e52b7ce49ed1e4f9fa511c452a2185bb" translate="yes" xml:space="preserve">
          <source>Beta</source>
          <target state="translated">Beta</target>
        </trans-unit>
        <trans-unit id="7190fb74ee81a83c8faf5a8c92b9ef9ebbf6afa2" translate="yes" xml:space="preserve">
          <source>Beta distribution parameterized by &lt;a href=&quot;#torch.distributions.beta.Beta.concentration1&quot;&gt;&lt;code&gt;concentration1&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.distributions.beta.Beta.concentration0&quot;&gt;&lt;code&gt;concentration0&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Beta 2分布由&lt;a href=&quot;#torch.distributions.beta.Beta.concentration1&quot;&gt; &lt;code&gt;concentration1&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;#torch.distributions.beta.Beta.concentration0&quot;&gt; &lt;code&gt;concentration0&lt;/code&gt; &lt;/a&gt;参数化。</target>
        </trans-unit>
        <trans-unit id="e26ae344044922af518669ed7912f3779c4b00f9" translate="yes" xml:space="preserve">
          <source>Bias:</source>
          <target state="translated">Bias:</target>
        </trans-unit>
        <trans-unit id="a8b51aa01c82ba019a69245f557ba6ce284edd3e" translate="yes" xml:space="preserve">
          <source>Bilinear</source>
          <target state="translated">Bilinear</target>
        </trans-unit>
        <trans-unit id="054debc367aa35026cc5f23e63b04983a105cd4a" translate="yes" xml:space="preserve">
          <source>Binary arithmetic ops: &lt;a href=&quot;name_inference#unifies-names-from-inputs-doc&quot;&gt;Unifies names from inputs&lt;/a&gt;</source>
          <target state="translated">二进制算术运算：&lt;a href=&quot;name_inference#unifies-names-from-inputs-doc&quot;&gt;统一输入中的名称&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f22fc461ba15be26425f3495add8c55aef51c91e" translate="yes" xml:space="preserve">
          <source>Binomial</source>
          <target state="translated">Binomial</target>
        </trans-unit>
        <trans-unit id="5200f1acde5b24e6b432770b7da7700cd60a0c9b" translate="yes" xml:space="preserve">
          <source>Blackman window function.</source>
          <target state="translated">布莱克曼窗口功能。</target>
        </trans-unit>
        <trans-unit id="0f52a562e394a39a60e84e9b3ac335fc3bd698d8" translate="yes" xml:space="preserve">
          <source>Block until the value of this &lt;code&gt;Future&lt;/code&gt; is ready.</source>
          <target state="translated">阻塞直到此 &lt;code&gt;Future&lt;/code&gt; 的值准备好为止。</target>
        </trans-unit>
        <trans-unit id="c2959d258f8603010c5e64b30d6b389ff4f7e543" translate="yes" xml:space="preserve">
          <source>Blocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.</source>
          <target state="translated">阻断调用,将RRef的值从所有者复制到本地节点并返回。如果当前节点是所有者,返回对本地值的引用。</target>
        </trans-unit>
        <trans-unit id="b76ff4906f33c2dd97ddd929b9662ba8cac6174c" translate="yes" xml:space="preserve">
          <source>Boolean</source>
          <target state="translated">Boolean</target>
        </trans-unit>
        <trans-unit id="8bdc4a42a8aebafd81e1c9ebefc552e2af4c86e0" translate="yes" xml:space="preserve">
          <source>Both &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; must have integer types.</source>
          <target state="translated">无论 &lt;code&gt;input&lt;/code&gt; 及 &lt;code&gt;other&lt;/code&gt; 必须有整数类型。</target>
        </trans-unit>
        <trans-unit id="1f1c331482420b1dcc09832ffc768fcc272a654b" translate="yes" xml:space="preserve">
          <source>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.</source>
          <target state="translated">包括参数和持久缓冲区(如运行平均值)。键是相应的参数和缓冲区名称。</target>
        </trans-unit>
        <trans-unit id="fbb16819919dac69a8f12c76f83bc60666b08a4a" translate="yes" xml:space="preserve">
          <source>Break and Continue</source>
          <target state="translated">休息和继续</target>
        </trans-unit>
        <trans-unit id="7263f9de457f4107fd587921961d9e2a1123f9c7" translate="yes" xml:space="preserve">
          <source>Broadcasting semantics</source>
          <target state="translated">广播语义</target>
        </trans-unit>
        <trans-unit id="7fa441232eb833209908a5491f939a27366f7f1a" translate="yes" xml:space="preserve">
          <source>Broadcasts a sequence tensors to the specified GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations.</source>
          <target state="translated">向指定的GPU广播一个序列时序。小的 tensors 首先会被凝聚到一个缓冲区中,以减少同步的次数。</target>
        </trans-unit>
        <trans-unit id="1e2b086a05d88dd9ed966250318444cafc6508f3" translate="yes" xml:space="preserve">
          <source>Broadcasts a tensor to specified GPU devices.</source>
          <target state="translated">向指定的GPU设备广播张量。</target>
        </trans-unit>
        <trans-unit id="c0e07d9c87aab6412703456e09b86450b239d9d4" translate="yes" xml:space="preserve">
          <source>Broadcasts the given tensors according to &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;Broadcasting semantics&lt;/a&gt;.</source>
          <target state="translated">根据&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;广播语义&lt;/a&gt;广播给定张量。</target>
        </trans-unit>
        <trans-unit id="3ef2cb9c843900d814506b00e103fcfa6e5a1790" translate="yes" xml:space="preserve">
          <source>Broadcasts the tensor to the whole group with multiple GPU tensors per node.</source>
          <target state="translated">向整个组广播张量,每个节点有多个GPU张量。</target>
        </trans-unit>
        <trans-unit id="8cd98b60116d9de3db535a744915ab8ea6c752e2" translate="yes" xml:space="preserve">
          <source>Broadcasts the tensor to the whole group.</source>
          <target state="translated">向整个组广播张量。</target>
        </trans-unit>
        <trans-unit id="9d0d383693b784a550085e06cd3563d333282d5c" translate="yes" xml:space="preserve">
          <source>Buffers can be accessed as attributes using given names.</source>
          <target state="translated">缓冲区可以作为属性使用给定的名称进行访问。</target>
        </trans-unit>
        <trans-unit id="072f59c8f878e682443386c6e7570c90195dbc31" translate="yes" xml:space="preserve">
          <source>Built-in Functions and Modules</source>
          <target state="translated">内置功能和模块</target>
        </trans-unit>
        <trans-unit id="2faa9e18e8bd0f32f4bea2db29afed6e0ae12775" translate="yes" xml:space="preserve">
          <source>By default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. &lt;a href=&quot;#torch.distributed.new_group&quot;&gt;&lt;code&gt;new_group()&lt;/code&gt;&lt;/a&gt; function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a &lt;code&gt;group&lt;/code&gt; argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns).</source>
          <target state="translated">默认情况下，集合体在默认组（也称为世界）上运行，并要求所有进程进入分布式函数调用。但是，某些工作负载可以从更细粒度的通信中受益。这是分布式组发挥作用的地方。&lt;a href=&quot;#torch.distributed.new_group&quot;&gt; &lt;code&gt;new_group()&lt;/code&gt; &lt;/a&gt;函数可用于创建具有所有进程的任意子集的新组。它返回一个不透明的组句柄，该句柄可以作为 &lt;code&gt;group&lt;/code&gt; 参数提供给所有集合（集合是分布式函数，用于以某些众所周知的编程模式交换信息）。</target>
        </trans-unit>
        <trans-unit id="aa6f8066d9ec2654575e0a2dde8cde52f9a324b9" translate="yes" xml:space="preserve">
          <source>By default, &lt;code&gt;dim&lt;/code&gt; is the last dimension of the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">默认情况下， &lt;code&gt;dim&lt;/code&gt; 是 &lt;code&gt;input&lt;/code&gt; 张量的最后一个维度。</target>
        </trans-unit>
        <trans-unit id="06e2dff3c96eaf47ae009ba6e54dc225888930ea" translate="yes" xml:space="preserve">
          <source>By default, &lt;code&gt;torch.optim.swa_utils.AveragedModel&lt;/code&gt; computes a running equal average of the parameters that you provide, but you can also use custom averaging functions with the &lt;code&gt;avg_fn&lt;/code&gt; parameter. In the following example &lt;code&gt;ema_model&lt;/code&gt; computes an exponential moving average.</source>
          <target state="translated">默认情况下， &lt;code&gt;torch.optim.swa_utils.AveragedModel&lt;/code&gt; 计算您提供的参数的运行平均值，但是您也可以将自定义平均函数与 &lt;code&gt;avg_fn&lt;/code&gt; 参数一起使用。在以下示例中， &lt;code&gt;ema_model&lt;/code&gt; 计算指数移动平均值。</target>
        </trans-unit>
        <trans-unit id="fbd6c7669b0b5fa4b44c256ffe1ad2c51d96bac8" translate="yes" xml:space="preserve">
          <source>By default, all parameters to a TorchScript function are assumed to be Tensor. To specify that an argument to a TorchScript function is another type, it is possible to use MyPy-style type annotations using the types listed above.</source>
          <target state="translated">默认情况下,TorchScript函数的所有参数都被假定为Tensor。要指定TorchScript函数的参数是其他类型,可以使用上面列出的MyPy风格的类型注释。</target>
        </trans-unit>
        <trans-unit id="3269c80ab5ee7ca74422e9e598672c0d5563eb7c" translate="yes" xml:space="preserve">
          <source>By default, both the NCCL and Gloo backends will try to find the right network interface to use. If the automatically detected interface is not correct, you can override it using the following environment variables (applicable to the respective backend):</source>
          <target state="translated">默认情况下,NCCL和Gloo后端都会尝试寻找正确的网络接口来使用。如果自动检测到的接口不正确,您可以使用以下环境变量(适用于各自的后端)来覆盖它。</target>
        </trans-unit>
        <trans-unit id="c4858611fd19a2d108e1171bb6e36531524a98b7" translate="yes" xml:space="preserve">
          <source>By default, each worker will have its PyTorch seed set to &lt;code&gt;base_seed + worker_id&lt;/code&gt;, where &lt;code&gt;base_seed&lt;/code&gt; is a long generated by main process using its RNG (thereby, consuming a RNG state mandatorily). However, seeds for other libraries may be duplicated upon initializing workers (e.g., NumPy), causing each worker to return identical random numbers. (See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/faq.html#dataloader-workers-random-seed&quot;&gt;this section&lt;/a&gt; in FAQ.).</source>
          <target state="translated">默认情况下，每个工作程序的PyTorch种子设置为 &lt;code&gt;base_seed + worker_id&lt;/code&gt; ，其中 &lt;code&gt;base_seed&lt;/code&gt; 是主进程使用其RNG生成的长整数（因此，强制使用RNG状态）。但是，初始化工作程序（例如NumPy）时，可能会复制其他库的种子，从而使每个工作程序返回相同的随机数。（请参阅&amp;ldquo;常见问题解答&amp;rdquo;中的&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/faq.html#dataloader-workers-random-seed&quot;&gt;本节&lt;/a&gt;。）。</target>
        </trans-unit>
        <trans-unit id="a47868a7f38c8661f0bf65ea44d4c34b8a4bf540" translate="yes" xml:space="preserve">
          <source>By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the &lt;code&gt;MAX_JOBS&lt;/code&gt; environment variable to a non-negative number.</source>
          <target state="translated">默认情况下，Ninja后端使用#CPUS + 2个工作程序来构建扩展。这可能会在某些系统上消耗过多的资源。可以通过将 &lt;code&gt;MAX_JOBS&lt;/code&gt; 环境变量设置为非负数来控制工作程序数。</target>
        </trans-unit>
        <trans-unit id="0d1c0495670db55861d9901a2985f3df3c09c53d" translate="yes" xml:space="preserve">
          <source>By default, the directory to which the build file is emitted and the resulting library compiled to is &lt;code&gt;&amp;lt;tmp&amp;gt;/torch_extensions/&amp;lt;name&amp;gt;&lt;/code&gt;, where &lt;code&gt;&amp;lt;tmp&amp;gt;&lt;/code&gt; is the temporary folder on the current platform and &lt;code&gt;&amp;lt;name&amp;gt;&lt;/code&gt; the name of the extension. This location can be overridden in two ways. First, if the &lt;code&gt;TORCH_EXTENSIONS_DIR&lt;/code&gt; environment variable is set, it replaces &lt;code&gt;&amp;lt;tmp&amp;gt;/torch_extensions&lt;/code&gt; and all extensions will be compiled into subfolders of this directory. Second, if the &lt;code&gt;build_directory&lt;/code&gt; argument to this function is supplied, it overrides the entire path, i.e. the library will be compiled into that folder directly.</source>
          <target state="translated">默认情况下，生成文件的发布目录和生成的库编译到的 &lt;code&gt;&amp;lt;tmp&amp;gt;/torch_extensions/&amp;lt;name&amp;gt;&lt;/code&gt; 为&amp;lt;tmp&amp;gt; / torch_extensions / &amp;lt;name&amp;gt;，其中 &lt;code&gt;&amp;lt;tmp&amp;gt;&lt;/code&gt; 是当前平台上的临时文件夹，而 &lt;code&gt;&amp;lt;name&amp;gt;&lt;/code&gt; 是扩展名。可以通过两种方式覆盖此位置。首先，如果设置了 &lt;code&gt;TORCH_EXTENSIONS_DIR&lt;/code&gt; 环境变量，它将替换 &lt;code&gt;&amp;lt;tmp&amp;gt;/torch_extensions&lt;/code&gt; ，所有扩展名都将编译到该目录的子文件夹中。其次，如果提供了此函数的 &lt;code&gt;build_directory&lt;/code&gt; 参数，它将覆盖整个路径，即库将直接编译到该文件夹​​中。</target>
        </trans-unit>
        <trans-unit id="836a1bf0b7e01bddc495fceffa973ccc50f2cdd2" translate="yes" xml:space="preserve">
          <source>By default, this layer uses instance statistics computed from input data in both training and evaluation modes.</source>
          <target state="translated">默认情况下,该层使用在训练和评估模式下从输入数据计算的实例统计。</target>
        </trans-unit>
        <trans-unit id="e0d34afff9ec6083c3b3dad548be8255d68d92ca" translate="yes" xml:space="preserve">
          <source>By default, this returns the peak allocated memory since the beginning of this program. &lt;code&gt;reset_peak_stats()&lt;/code&gt; can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.</source>
          <target state="translated">默认情况下，这将返回自此程序开始以来的峰值分配内存。 &lt;code&gt;reset_peak_stats()&lt;/code&gt; 可用于重置跟踪该指标的起点。例如，这两个函数可以测量训练循环中每个迭代的峰值分配内存使用量。</target>
        </trans-unit>
        <trans-unit id="27248360b06b99273d17a1100cf62c5ed9fc9c4e" translate="yes" xml:space="preserve">
          <source>By default, this returns the peak cached memory since the beginning of this program. &lt;code&gt;reset_peak_stats()&lt;/code&gt; can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop.</source>
          <target state="translated">默认情况下，此函数返回自此程序开始以来的峰值缓存内存。 &lt;code&gt;reset_peak_stats()&lt;/code&gt; 可用于重置跟踪该指标的起点。例如，这两个函数可以测量训练循环中每次迭代的峰值缓存内存量。</target>
        </trans-unit>
        <trans-unit id="260db48b6c19b12ad8aaa2c223fbe53f044c91eb" translate="yes" xml:space="preserve">
          <source>By default, we decode byte strings as &lt;code&gt;utf-8&lt;/code&gt;. This is to avoid a common error case &lt;code&gt;UnicodeDecodeError: 'ascii' codec can't decode byte 0x...&lt;/code&gt; when loading files saved by Python 2 in Python 3. If this default is incorrect, you may use an extra &lt;code&gt;encoding&lt;/code&gt; keyword argument to specify how these objects should be loaded, e.g., &lt;code&gt;encoding='latin1'&lt;/code&gt; decodes them to strings using &lt;code&gt;latin1&lt;/code&gt; encoding, and &lt;code&gt;encoding='bytes'&lt;/code&gt; keeps them as byte arrays which can be decoded later with &lt;code&gt;byte_array.decode(...)&lt;/code&gt;.</source>
          <target state="translated">默认情况下，我们将字节字符串解码为 &lt;code&gt;utf-8&lt;/code&gt; 。这是为了避免一个常见的错误情况 &lt;code&gt;UnicodeDecodeError: 'ascii' codec can't decode byte 0x...&lt;/code&gt; 当被Python 2在Python 3中保存加载文件如果这个默认是不正确，你可以使用一个额外的 &lt;code&gt;encoding&lt;/code&gt; 关键字参数指定如何加载这些对象，例如， &lt;code&gt;encoding='latin1'&lt;/code&gt; 使用 &lt;code&gt;latin1&lt;/code&gt; 编码将其解码为字符串，而 &lt;code&gt;encoding='bytes'&lt;/code&gt; 将其保留为字节数组，稍后可使用 &lt;code&gt;byte_array.decode(...)&lt;/code&gt; 对其进行解码。</target>
        </trans-unit>
        <trans-unit id="5b3e53bd55890d668be711af0b27d6b4e4085bb4" translate="yes" xml:space="preserve">
          <source>By default, we don&amp;rsquo;t clean up files after loading it. Hub uses the cache by default if it already exists in the directory returned by &lt;a href=&quot;#torch.hub.get_dir&quot;&gt;&lt;code&gt;get_dir()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">默认情况下，我们不会在加载文件后清理文件。如果缓存已经存在于&lt;a href=&quot;#torch.hub.get_dir&quot;&gt; &lt;code&gt;get_dir()&lt;/code&gt; &lt;/a&gt;返回的目录中，则默认情况下Hub使用缓存。</target>
        </trans-unit>
        <trans-unit id="4d31b2b62d5a7017f58599071410bf4ed9eb8d68" translate="yes" xml:space="preserve">
          <source>By default, with &lt;code&gt;dim=0&lt;/code&gt;, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use &lt;code&gt;dim=None&lt;/code&gt;.</source>
          <target state="translated">默认情况下，当 &lt;code&gt;dim=0&lt;/code&gt; 时，将针对每个输出通道/平面独立计算范数。要计算整个重量张量的范数，请使用 &lt;code&gt;dim=None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="32096c2e0eff33d844ee6d675407ace18289357d" translate="yes" xml:space="preserve">
          <source>C</source>
          <target state="translated">C</target>
        </trans-unit>
        <trans-unit id="eaef64b3192bf8d5502eee309124b228be2379ec" translate="yes" xml:space="preserve">
          <source>C = \log(\pi) \times \frac{p (p - 1)}{4}</source>
          <target state="translated">C=log(pi)xtimes frac{p (p-1)}{4}</target>
        </trans-unit>
        <trans-unit id="4ee9f8327170df1bedff8fda035491204285f3c3" translate="yes" xml:space="preserve">
          <source>C = \text{number of classes (including blank)}</source>
          <target state="translated">C=text{类数(包括空白)}。</target>
        </trans-unit>
        <trans-unit id="f46c4b42af8b2196cc344f07319e812268c3c0dd" translate="yes" xml:space="preserve">
          <source>C \times \prod(\text{kernel\_size})</source>
          <target state="translated">C (times \prod(text{kernel_size})</target>
        </trans-unit>
        <trans-unit id="fc2b4216164cfb01ac45112054b3fedda8b56c86" translate="yes" xml:space="preserve">
          <source>C++</source>
          <target state="translated">C++</target>
        </trans-unit>
        <trans-unit id="a3d883aa22c9b3cbe1562cc3d62c063468b9a196" translate="yes" xml:space="preserve">
          <source>C=\text{num\_channels}</source>
          <target state="translated">C=\text{num\_channels}</target>
        </trans-unit>
        <trans-unit id="798a57343d5fc6a0cf122924f6ca62852b64f4a6" translate="yes" xml:space="preserve">
          <source>CELU</source>
          <target state="translated">CELU</target>
        </trans-unit>
        <trans-unit id="ff221d4752ce05f5a91bbf1d28b78a7bf7e2ddaa" translate="yes" xml:space="preserve">
          <source>CPU</source>
          <target state="translated">CPU</target>
        </trans-unit>
        <trans-unit id="d2b3baf18b41b52a701d4019f7ffaa284e02f53a" translate="yes" xml:space="preserve">
          <source>CPU hosts with Ethernet interconnect</source>
          <target state="translated">带以太网互连的CPU主机</target>
        </trans-unit>
        <trans-unit id="bfea39d4ef79843c8c035774adc370d11a93b4e8" translate="yes" xml:space="preserve">
          <source>CPU hosts with InfiniBand interconnect</source>
          <target state="translated">带InfiniBand互连的CPU主机</target>
        </trans-unit>
        <trans-unit id="aca0030d1b8e86f8e968a622d4b61c5e238ad1fa" translate="yes" xml:space="preserve">
          <source>CPU tensor</source>
          <target state="translated">CPU张量</target>
        </trans-unit>
        <trans-unit id="1adcc1c5aae9ba0bc68cde14e7017456258e8921" translate="yes" xml:space="preserve">
          <source>CPU threading and TorchScript inference</source>
          <target state="translated">CPU线程和TorchScript推理</target>
        </trans-unit>
        <trans-unit id="5c435b722f2bc9152d11b1225cc3099efa162504" translate="yes" xml:space="preserve">
          <source>CTCLoss</source>
          <target state="translated">CTCLoss</target>
        </trans-unit>
        <trans-unit id="b793fde9f05fa7373de2fa5fbd4dcb147eafd14e" translate="yes" xml:space="preserve">
          <source>CUDA events are synchronization markers that can be used to monitor the device&amp;rsquo;s progress, to accurately measure timing, and to synchronize CUDA streams.</source>
          <target state="translated">CUDA事件是同步标记，可用于监视设备的进度，准确测量时序并同步CUDA流。</target>
        </trans-unit>
        <trans-unit id="1ab6d957380a70ab72c7926a9d4fae8cf48ecf35" translate="yes" xml:space="preserve">
          <source>CUDA semantics</source>
          <target state="translated">CUDA语义</target>
        </trans-unit>
        <trans-unit id="3f5da429aac783fd0a3a0da898da1913e428656f" translate="yes" xml:space="preserve">
          <source>CUDA support with mixed compilation is provided. Simply pass CUDA source files (&lt;code&gt;.cu&lt;/code&gt; or &lt;code&gt;.cuh&lt;/code&gt;) along with other sources. Such files will be detected and compiled with nvcc rather than the C++ compiler. This includes passing the CUDA lib64 directory as a library directory, and linking &lt;code&gt;cudart&lt;/code&gt;. You can pass additional flags to nvcc via &lt;code&gt;extra_cuda_cflags&lt;/code&gt;, just like with &lt;code&gt;extra_cflags&lt;/code&gt; for C++. Various heuristics for finding the CUDA install directory are used, which usually work fine. If not, setting the &lt;code&gt;CUDA_HOME&lt;/code&gt; environment variable is the safest option.</source>
          <target state="translated">提供了具有混合编译的CUDA支持。只需将CUDA源文件（ &lt;code&gt;.cu&lt;/code&gt; 或 &lt;code&gt;.cuh&lt;/code&gt; ）与其他源一起传递。将使用nvcc而不是C ++编译器检测并编译此类文件。这包括将CUDA lib64目录作为库目录传递，并链接 &lt;code&gt;cudart&lt;/code&gt; 。您可以通过 &lt;code&gt;extra_cuda_cflags&lt;/code&gt; 将其他标志传递给nvcc ，就像C ++的 &lt;code&gt;extra_cflags&lt;/code&gt; 一样。使用各种启发式方法来查找CUDA安装目录，通常可以正常工作。如果不是，那么设置 &lt;code&gt;CUDA_HOME&lt;/code&gt; 环境变量是最安全的选择。</target>
        </trans-unit>
        <trans-unit id="56a1c421d119a488bb14d44dd6891b51aea087b0" translate="yes" xml:space="preserve">
          <source>Caching is useful for transforms whose inverses are either expensive or numerically unstable. Note that care must be taken with memoized values since the autograd graph may be reversed. For example while the following works with or without caching:</source>
          <target state="translated">缓存对于那些反演昂贵或数值不稳定的变换很有用。需要注意的是,对于记忆值必须小心,因为autograd图可能会被反转。例如,下面的例子无论是否有缓存都可以使用。</target>
        </trans-unit>
        <trans-unit id="5abf962ed164e31df7bbd04bfe694578c153b51d" translate="yes" xml:space="preserve">
          <source>Caching logic</source>
          <target state="translated">缓存逻辑</target>
        </trans-unit>
        <trans-unit id="e8a73f7e1b281ff72f0cd1f11baf202568648a12" translate="yes" xml:space="preserve">
          <source>Calculates determinant of a square matrix or batches of square matrices.</source>
          <target state="translated">计算一个平方矩阵或一批平方矩阵的行列式。</target>
        </trans-unit>
        <trans-unit id="b469ab85777119e331c0d2b40b68cd19ac71da49" translate="yes" xml:space="preserve">
          <source>Calculates log determinant of a square matrix or batches of square matrices.</source>
          <target state="translated">计算一个平方矩阵或一批平方矩阵的对数行列式。</target>
        </trans-unit>
        <trans-unit id="dbc9fd03c3e295de099741d45013bec1bd5703f7" translate="yes" xml:space="preserve">
          <source>Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the probability of possible alignments of input to target, producing a loss value which is differentiable with respect to each input node. The alignment of input to target is assumed to be &amp;ldquo;many-to-one&amp;rdquo;, which limits the length of the target sequence such that it must be</source>
          <target state="translated">计算连续（非分段）时间序列与目标序列之间的损失。CTCLoss计算输入与目标可能对齐的概率之和，从而产生相对于每个输入节点可微分的损耗值。假定输入与目标的比对是&amp;ldquo;多对一&amp;rdquo;，这限制了目标序列的长度，因此必须</target>
        </trans-unit>
        <trans-unit id="8b95dc9e720785341df91a709dd18edb57e422e8" translate="yes" xml:space="preserve">
          <source>Calculates pointwise</source>
          <target state="translated">按点计算</target>
        </trans-unit>
        <trans-unit id="19a076a6c7df9171545ad04fdc2cc5f305e06b38" translate="yes" xml:space="preserve">
          <source>Calculates the learning rate at batch index. This function treats &lt;code&gt;self.last_epoch&lt;/code&gt; as the last batch index.</source>
          <target state="translated">计算批次索引的学习率。此函数将 &lt;code&gt;self.last_epoch&lt;/code&gt; 视为最后一批索引。</target>
        </trans-unit>
        <trans-unit id="3f73e5660ccb505df5953e1fbed92e6c8a064be1" translate="yes" xml:space="preserve">
          <source>Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor.</source>
          <target state="translated">计算二维张量的伪逆(也称为摩尔-彭罗斯逆)。</target>
        </trans-unit>
        <trans-unit id="afbe6768e28de20bd0669bfb42db4b0e5335e016" translate="yes" xml:space="preserve">
          <source>Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor. Please look at &lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse&quot;&gt;Moore-Penrose inverse&lt;/a&gt; for more details</source>
          <target state="translated">计算2D张量的拟逆（也称为Moore-Penrose逆）。请查看&lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse&quot;&gt;Moore-Penrose逆&lt;/a&gt;以获取更多详细信息</target>
        </trans-unit>
        <trans-unit id="bf3fa1ed9287f3a31249968ae80488bbbc90709d" translate="yes" xml:space="preserve">
          <source>Calculates the sign and log absolute value of the determinant(s) of a square matrix or batches of square matrices.</source>
          <target state="translated">计算一个或多个平方矩阵的行列式的符号和对数绝对值。</target>
        </trans-unit>
        <trans-unit id="bc5cae9078b78a08c10f30abb3ba9b5b22cfef54" translate="yes" xml:space="preserve">
          <source>Callables prefixed with underscore are considered as helper functions which won&amp;rsquo;t show up in &lt;a href=&quot;#torch.hub.list&quot;&gt;&lt;code&gt;torch.hub.list()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">带有下划线前缀的可调用对象被视为辅助函数，这些函数不会显示在&lt;a href=&quot;#torch.hub.list&quot;&gt; &lt;code&gt;torch.hub.list()&lt;/code&gt; 中&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="9c42beef7db382c9ae158419c3ebc5c8310023f1" translate="yes" xml:space="preserve">
          <source>Calling &lt;code&gt;hub.set_dir(&amp;lt;PATH_TO_HUB_DIR&amp;gt;)&lt;/code&gt;</source>
          <target state="translated">呼叫 &lt;code&gt;hub.set_dir(&amp;lt;PATH_TO_HUB_DIR&amp;gt;)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="265655dd73dcd39520c5cd9a954e45b04d35e8a1" translate="yes" xml:space="preserve">
          <source>Calling &lt;code&gt;torch.kaiser_window(L, B, periodic=True)&lt;/code&gt; is equivalent to calling &lt;code&gt;torch.kaiser_window(L + 1, B, periodic=False)[:-1])&lt;/code&gt;. The &lt;code&gt;periodic&lt;/code&gt; argument is intended as a helpful shorthand to produce a periodic window as input to functions like &lt;a href=&quot;torch.stft#torch.stft&quot;&gt;&lt;code&gt;torch.stft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">调用 &lt;code&gt;torch.kaiser_window(L, B, periodic=True)&lt;/code&gt; 等效于调用 &lt;code&gt;torch.kaiser_window(L + 1, B, periodic=False)[:-1])&lt;/code&gt; 。的 &lt;code&gt;periodic&lt;/code&gt; 参数旨在作为一个有用的速记产生类似的周期性窗口作为输入功能&lt;a href=&quot;torch.stft#torch.stft&quot;&gt; &lt;code&gt;torch.stft()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0d622de7b9cec6af9b50c591c0762f95f6cf23f1" translate="yes" xml:space="preserve">
          <source>Calling a submodule directly (e.g. &lt;code&gt;self.resnet(input)&lt;/code&gt;) is equivalent to calling its &lt;code&gt;forward&lt;/code&gt; method (e.g. &lt;code&gt;self.resnet.forward(input)&lt;/code&gt;).</source>
          <target state="translated">直接调用子模块（例如 &lt;code&gt;self.resnet(input)&lt;/code&gt; ）等效于调用其 &lt;code&gt;forward&lt;/code&gt; 方法（例如 &lt;code&gt;self.resnet.forward(input)&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="1b7d84e5b4532eb537ef53414226540774bd94f3" translate="yes" xml:space="preserve">
          <source>Calling the backward transform (&lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">以相同的归一化模式调用后向变换（&lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt;）将在两个变换之间应用 &lt;code&gt;1/n&lt;/code&gt; 的总体归一化。这是使&lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt;完全相反的要求。</target>
        </trans-unit>
        <trans-unit id="57076e248ecf298e1219a0fa6ecf299084bfedd8" translate="yes" xml:space="preserve">
          <source>Calling the backward transform (&lt;a href=&quot;#torch.fft.ihfft&quot;&gt;&lt;code&gt;ihfft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ihfft&quot;&gt;&lt;code&gt;ihfft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">以相同的归一化模式调用后向变换（&lt;a href=&quot;#torch.fft.ihfft&quot;&gt; &lt;code&gt;ihfft()&lt;/code&gt; &lt;/a&gt;）将在两个变换之间应用 &lt;code&gt;1/n&lt;/code&gt; 的总体归一化。这是使&lt;a href=&quot;#torch.fft.ihfft&quot;&gt; &lt;code&gt;ihfft()&lt;/code&gt; &lt;/a&gt;完全相反的要求。</target>
        </trans-unit>
        <trans-unit id="6550668f3134865d4fd7ecb672c8b13821a60616" translate="yes" xml:space="preserve">
          <source>Calling the backward transform (&lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">以相同的归一化模式调用后向变换（&lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt;）将在两个变换之间应用 &lt;code&gt;1/n&lt;/code&gt; 的总体归一化。这是使&lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt;完全相反的要求。</target>
        </trans-unit>
        <trans-unit id="393935e7b8f54e6c891a458a9a0a5b90620581df" translate="yes" xml:space="preserve">
          <source>Calling the forward transform (&lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">以相同的归一化模式调用前向变换（&lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt;）将在两个变换之间应用 &lt;code&gt;1/n&lt;/code&gt; 的总体归一化。这是使&lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt;完全相反的要求。</target>
        </trans-unit>
        <trans-unit id="3c4696c3c77b6fbb0767ee074a0b08401ad5fd2b" translate="yes" xml:space="preserve">
          <source>Calling the forward transform (&lt;a href=&quot;#torch.fft.hfft&quot;&gt;&lt;code&gt;hfft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ihfft&quot;&gt;&lt;code&gt;ihfft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">以相同的归一化模式调用前向变换（&lt;a href=&quot;#torch.fft.hfft&quot;&gt; &lt;code&gt;hfft()&lt;/code&gt; &lt;/a&gt;）将在两个变换之间应用 &lt;code&gt;1/n&lt;/code&gt; 的总体归一化。这是使&lt;a href=&quot;#torch.fft.ihfft&quot;&gt; &lt;code&gt;ihfft()&lt;/code&gt; &lt;/a&gt;完全相反的要求。</target>
        </trans-unit>
        <trans-unit id="a0f807127c82b5c379fa18d9be9a714272f46e17" translate="yes" xml:space="preserve">
          <source>Calling the forward transform (&lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">以相同的归一化模式调用前向变换（&lt;a href=&quot;#torch.fft.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt;）将在两个变换之间应用 &lt;code&gt;1/n&lt;/code&gt; 的总体归一化。这是使&lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt;完全相反的要求。</target>
        </trans-unit>
        <trans-unit id="76582af9585743776e20d4bdf66734ecbe7e7ff9" translate="yes" xml:space="preserve">
          <source>Calls to &lt;code&gt;builtin functions&lt;/code&gt;</source>
          <target state="translated">调用 &lt;code&gt;builtin functions&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="eb07741ad617617e9abda7e3d52fee63a305a121" translate="yes" xml:space="preserve">
          <source>Calls to methods of builtin types like tensor: &lt;code&gt;x.mm(y)&lt;/code&gt;</source>
          <target state="translated">调用诸如张量之类的内置类型的方法： &lt;code&gt;x.mm(y)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="fc7e70542d9259610f72ca817bd3eb6584edd2c8" translate="yes" xml:space="preserve">
          <source>Calls to other script functions:</source>
          <target state="translated">对其他脚本函数的调用。</target>
        </trans-unit>
        <trans-unit id="b6867b70db2065294481ad42aed53ba49e6355b8" translate="yes" xml:space="preserve">
          <source>Can also be used for higher dimension inputs, such as 2D images, by providing an input of size</source>
          <target state="translated">也可用于更高维度的输入,如2D图像,通过提供尺寸为</target>
        </trans-unit>
        <trans-unit id="221c9205ab30df3bd1004805355ca477c455a15b" translate="yes" xml:space="preserve">
          <source>Can only be called once and before any inter-op parallel work is started (e.g. JIT execution).</source>
          <target state="translated">只能被调用一次,并且在任何操作间的并行工作开始之前(例如JIT执行)。</target>
        </trans-unit>
        <trans-unit id="3805fd56c2af8071d51bc53ae0a92fdcb58807db" translate="yes" xml:space="preserve">
          <source>Casting Examples:</source>
          <target state="translated">铸造实例。</target>
        </trans-unit>
        <trans-unit id="e7500c883cdd17fa4172ea83911ebf91a32625de" translate="yes" xml:space="preserve">
          <source>Casts</source>
          <target state="translated">Casts</target>
        </trans-unit>
        <trans-unit id="26538798d0973ae83ca7715b676794ed3b172f33" translate="yes" xml:space="preserve">
          <source>Casts all floating point parameters and buffers to &lt;code&gt;bfloat16&lt;/code&gt; datatype.</source>
          <target state="translated">将所有浮点参数和缓冲区 &lt;code&gt;bfloat16&lt;/code&gt; 为bfloat16数据类型。</target>
        </trans-unit>
        <trans-unit id="b6c764881902912eb6ba271fb55ce5179af34673" translate="yes" xml:space="preserve">
          <source>Casts all floating point parameters and buffers to &lt;code&gt;double&lt;/code&gt; datatype.</source>
          <target state="translated">将所有浮点参数和缓冲区强制转换为 &lt;code&gt;double&lt;/code&gt; 数据类型。</target>
        </trans-unit>
        <trans-unit id="78461ae0a43c2d54b7c9efa684628bba9ad1c1ed" translate="yes" xml:space="preserve">
          <source>Casts all floating point parameters and buffers to &lt;code&gt;half&lt;/code&gt; datatype.</source>
          <target state="translated">将所有浮点参数和缓冲区强制转换为 &lt;code&gt;half&lt;/code&gt; 数据类型。</target>
        </trans-unit>
        <trans-unit id="661d47897b8e53cbd52a45424e86044636652304" translate="yes" xml:space="preserve">
          <source>Casts all floating point parameters and buffers to float datatype.</source>
          <target state="translated">将所有的浮点参数和缓冲区转换为浮点数据类型。</target>
        </trans-unit>
        <trans-unit id="4724f395a64ded08f676d2e9e0393dcb0f712246" translate="yes" xml:space="preserve">
          <source>Casts all parameters and buffers to &lt;code&gt;dst_type&lt;/code&gt;.</source>
          <target state="translated">将所有参数和缓冲区 &lt;code&gt;dst_type&lt;/code&gt; 转换为dst_type。</target>
        </trans-unit>
        <trans-unit id="79f02a31265abcb1bb988f26d0c498d397fdd789" translate="yes" xml:space="preserve">
          <source>Casts this storage to bfloat16 type</source>
          <target state="translated">将此存储空间转换为bfloat16类型。</target>
        </trans-unit>
        <trans-unit id="bd487dabbd2c7187cd079ec504def8aaf3a7b571" translate="yes" xml:space="preserve">
          <source>Casts this storage to bool type</source>
          <target state="translated">将此存储方式转换为bool类型</target>
        </trans-unit>
        <trans-unit id="6694cf255316b894b3ad2697cc3b38ec9452ca42" translate="yes" xml:space="preserve">
          <source>Casts this storage to byte type</source>
          <target state="translated">将该存储空间转换为字节类型</target>
        </trans-unit>
        <trans-unit id="151ff3570c6883e5b0af6409a5ae071d0e154dbc" translate="yes" xml:space="preserve">
          <source>Casts this storage to char type</source>
          <target state="translated">将此存储空间转换为char类型</target>
        </trans-unit>
        <trans-unit id="d3e9e6a46615473165fa7e0ec3e61fb30d1e8866" translate="yes" xml:space="preserve">
          <source>Casts this storage to complex double type</source>
          <target state="translated">将该存储空间转换为复杂的双类型</target>
        </trans-unit>
        <trans-unit id="510b5fa8a83cea216c27cd3b246f5ff67c971a64" translate="yes" xml:space="preserve">
          <source>Casts this storage to complex float type</source>
          <target state="translated">将此存储空间转换为复杂的float类型</target>
        </trans-unit>
        <trans-unit id="2e339cd7fb3f62430cb2f45b27e9e325c9fd432f" translate="yes" xml:space="preserve">
          <source>Casts this storage to double type</source>
          <target state="translated">将该存储空间转换为双倍类型</target>
        </trans-unit>
        <trans-unit id="46bf8db49512235eab20518ddd006bc28b925a2c" translate="yes" xml:space="preserve">
          <source>Casts this storage to float type</source>
          <target state="translated">将该存储空间转换为浮点类型</target>
        </trans-unit>
        <trans-unit id="63e25d02d152bfc3b6809c0d00d52a9786a45699" translate="yes" xml:space="preserve">
          <source>Casts this storage to half type</source>
          <target state="translated">将此存储空间转换为半类型</target>
        </trans-unit>
        <trans-unit id="8dd4507b920a808cec77926ae75ebeb4c2c63230" translate="yes" xml:space="preserve">
          <source>Casts this storage to int type</source>
          <target state="translated">将该存储空间转换为int类型</target>
        </trans-unit>
        <trans-unit id="afab2e0bade340b8cecdf6e13b386611e22e2730" translate="yes" xml:space="preserve">
          <source>Casts this storage to long type</source>
          <target state="translated">将此存储空间转换为长类型</target>
        </trans-unit>
        <trans-unit id="d4ce7dd1afcf534c203c4afd8fc54fe512dfe75d" translate="yes" xml:space="preserve">
          <source>Casts this storage to short type</source>
          <target state="translated">将此存储空间转换为短类型</target>
        </trans-unit>
        <trans-unit id="8528ae94e2e148afb1dc7ce9973ecc62392671c1" translate="yes" xml:space="preserve">
          <source>Categorical</source>
          <target state="translated">Categorical</target>
        </trans-unit>
        <trans-unit id="0b7b85a2968dc04c734a5094885d58f3c06a57c0" translate="yes" xml:space="preserve">
          <source>Cauchy</source>
          <target state="translated">Cauchy</target>
        </trans-unit>
        <trans-unit id="795a0c324ff1f130803359d377842d67e3339ceb" translate="yes" xml:space="preserve">
          <source>Change if autograd should record operations on parameters in this module.</source>
          <target state="translated">更改autograd是否应该记录本模块中参数的操作。</target>
        </trans-unit>
        <trans-unit id="771633aa9e4ffd6ed00dfc406fa17e09d36a9380" translate="yes" xml:space="preserve">
          <source>Change if autograd should record operations on this tensor: sets this tensor&amp;rsquo;s &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt;&lt;code&gt;requires_grad&lt;/code&gt;&lt;/a&gt; attribute in-place. Returns this tensor.</source>
          <target state="translated">改变，如果autograd应在此张记录操作：设置这个张量的&lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt; &lt;code&gt;requires_grad&lt;/code&gt; &lt;/a&gt;就地属性。返回此张量。</target>
        </trans-unit>
        <trans-unit id="df974d2905d5cd175e8ce062cca16506fc69a1da" translate="yes" xml:space="preserve">
          <source>Channel dim is the 2nd dim of input. When input has dims &amp;lt; 2, then there is no channel dim and the number of channels = 1.</source>
          <target state="translated">通道暗淡是输入的第二暗淡。当输入的dims &amp;lt;2时，则没有通道暗淡，通道数= 1。</target>
        </trans-unit>
        <trans-unit id="239e4420c7d4084b23404d4d0c56239ffc4eca05" translate="yes" xml:space="preserve">
          <source>Check gradients computed via small finite differences against analytical gradients w.r.t. tensors in &lt;code&gt;inputs&lt;/code&gt; that are of floating point or complex type and with &lt;code&gt;requires_grad=True&lt;/code&gt;.</source>
          <target state="translated">检查通过小有限差分计算的梯度与浮点或复杂类型且具有 &lt;code&gt;requires_grad=True&lt;/code&gt; &lt;code&gt;inputs&lt;/code&gt; 中的张量的分析梯度。</target>
        </trans-unit>
        <trans-unit id="82aa07bbbf68e1780a5f4ead49b824c7cc4f5648" translate="yes" xml:space="preserve">
          <source>Check gradients of gradients computed via small finite differences against analytical gradients w.r.t. tensors in &lt;code&gt;inputs&lt;/code&gt; and &lt;code&gt;grad_outputs&lt;/code&gt; that are of floating point or complex type and with &lt;code&gt;requires_grad=True&lt;/code&gt;.</source>
          <target state="translated">检查通过小有限差分计算的梯度的梯度与浮点或复杂类型并具有 &lt;code&gt;grad_outputs&lt;/code&gt; &lt;code&gt;requires_grad=True&lt;/code&gt; &lt;code&gt;inputs&lt;/code&gt; 和grad_output中的张量的分析梯度。</target>
        </trans-unit>
        <trans-unit id="56ff021269456a017773120e4ed3d4af95c8ae59" translate="yes" xml:space="preserve">
          <source>Check whether &lt;code&gt;module&lt;/code&gt; is pruned by looking for &lt;code&gt;forward_pre_hooks&lt;/code&gt; in its modules that inherit from the &lt;a href=&quot;torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod&quot;&gt;&lt;code&gt;BasePruningMethod&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">通过在从&lt;a href=&quot;torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod&quot;&gt; &lt;code&gt;BasePruningMethod&lt;/code&gt; &lt;/a&gt;继承的模块中查找 &lt;code&gt;forward_pre_hooks&lt;/code&gt; 来检查 &lt;code&gt;module&lt;/code&gt; 是否被修剪。</target>
        </trans-unit>
        <trans-unit id="e4c8690ed4ddc9de7d901e88e48d1d54b0026ac9" translate="yes" xml:space="preserve">
          <source>Check whether &lt;code&gt;module&lt;/code&gt; is pruned by looking for &lt;code&gt;forward_pre_hooks&lt;/code&gt; in its modules that inherit from the &lt;code&gt;BasePruningMethod&lt;/code&gt;.</source>
          <target state="translated">通过在从 &lt;code&gt;BasePruningMethod&lt;/code&gt; 继承的模块中查找 &lt;code&gt;forward_pre_hooks&lt;/code&gt; 来检查 &lt;code&gt;module&lt;/code&gt; 是否被修剪。</target>
        </trans-unit>
        <trans-unit id="b3a48ae8d9eebbbcfe68f114073dd271c96bf314" translate="yes" xml:space="preserve">
          <source>Check whether it&amp;rsquo;s in the middle of the ONNX export. This function returns True in the middle of torch.onnx.export(). torch.onnx.export should be executed with single thread.</source>
          <target state="translated">检查它是否在ONNX导出的中间。此函数在torch.onnx.export（）的中间返回True。torch.onnx.export应该使用单线程执行。</target>
        </trans-unit>
        <trans-unit id="e151686a765ce214247b375cc4d7aa1c97e4e14f" translate="yes" xml:space="preserve">
          <source>Checking if the default process group has been initialized</source>
          <target state="translated">检查默认进程组是否已被初始化。</target>
        </trans-unit>
        <trans-unit id="b37b3cf9158406adf7b90792ec9193ec0e0c1df6" translate="yes" xml:space="preserve">
          <source>Checkpoint a model or part of the model</source>
          <target state="translated">检查点一个模型或模型的一部分</target>
        </trans-unit>
        <trans-unit id="95fac7ec70656fe8c623fe8fbf3c5c04c9b16115" translate="yes" xml:space="preserve">
          <source>Checkpointing doesn&amp;rsquo;t work with &lt;a href=&quot;autograd#torch.autograd.grad&quot;&gt;&lt;code&gt;torch.autograd.grad()&lt;/code&gt;&lt;/a&gt;, but only with &lt;a href=&quot;autograd#torch.autograd.backward&quot;&gt;&lt;code&gt;torch.autograd.backward()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">检查点不适用于&lt;a href=&quot;autograd#torch.autograd.grad&quot;&gt; &lt;code&gt;torch.autograd.grad()&lt;/code&gt; &lt;/a&gt;，而仅适用于&lt;a href=&quot;autograd#torch.autograd.backward&quot;&gt; &lt;code&gt;torch.autograd.backward()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="f51f71651c6e78eb334ceb82bf5f372ed3b9e12e" translate="yes" xml:space="preserve">
          <source>Checkpointing is implemented by rerunning a forward-pass segment for each checkpointed segment during backward. This can cause persistent states like the RNG state to be advanced than they would without checkpointing. By default, checkpointing includes logic to juggle the RNG state such that checkpointed passes making use of RNG (through dropout for example) have deterministic output as compared to non-checkpointed passes. The logic to stash and restore RNG states can incur a moderate performance hit depending on the runtime of checkpointed operations. If deterministic output compared to non-checkpointed passes is not required, supply &lt;code&gt;preserve_rng_state=False&lt;/code&gt; to &lt;code&gt;checkpoint&lt;/code&gt; or &lt;code&gt;checkpoint_sequential&lt;/code&gt; to omit stashing and restoring the RNG state during each checkpoint.</source>
          <target state="translated">通过在反向过程中为每个检查点段重新运行向前通过段来实现检查点。这可能导致像RNG状态这样的持久状态要比没有检查点的状态更先进。默认情况下，检查点包括处理RNG状态的逻辑，以便与非检查点通过相比，使用RNG（例如，通过辍学）的检查点通过具有确定性的输出。存储和还原RNG状态的逻辑可能会导致性能下降，具体取决于检查点操作的运行时间。如果不要求比非检查点通行证确定性的输出，电源 &lt;code&gt;preserve_rng_state=False&lt;/code&gt; 到 &lt;code&gt;checkpoint&lt;/code&gt; 或 &lt;code&gt;checkpoint_sequential&lt;/code&gt; 在每个检查点期间忽略隐藏和恢复RNG状态。</target>
        </trans-unit>
        <trans-unit id="332b6971e8e3a29969ff874b4073e4272d2a843f" translate="yes" xml:space="preserve">
          <source>Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does &lt;strong&gt;not&lt;/strong&gt; save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model.</source>
          <target state="translated">检查点通过将计算交换为内存来工作。检查点部分不保存整个计算图的所有中间激活以进行向后计算，而&lt;strong&gt;没有&lt;/strong&gt;保存中间激活，而是在向后传递时重新计算它们。它可以应用于模型的任何部分。</target>
        </trans-unit>
        <trans-unit id="893f0f558202bc0518489f883166ffe50bb8bd4d" translate="yes" xml:space="preserve">
          <source>Checks if all the work submitted has been completed.</source>
          <target state="translated">检查是否已完成所有提交的工作。</target>
        </trans-unit>
        <trans-unit id="945f252428f14d3ff3238e766e3368cedac5589e" translate="yes" xml:space="preserve">
          <source>Checks if all work currently captured by event has completed.</source>
          <target state="translated">检查事件当前捕获的所有工作是否已经完成。</target>
        </trans-unit>
        <trans-unit id="635284a98c9a3f2f5e6091ab87ff16c33d35a6cc" translate="yes" xml:space="preserve">
          <source>Checks if any sent CUDA tensors could be cleaned from the memory. Force closes shared memory file used for reference counting if there is no active counters. Useful when the producer process stopped actively sending tensors and want to release unused memory.</source>
          <target state="translated">检查是否有任何发送的 CUDA tensors 可以从内存中清除。如果没有活动的计数器,强制关闭用于参考计数的共享内存文件。当生产者进程停止主动发送Tensors,并希望释放未使用的内存时,该功能非常有用。</target>
        </trans-unit>
        <trans-unit id="39f6e80dd8c8ac31e71117491def641d00a288d1" translate="yes" xml:space="preserve">
          <source>Checks if tensor is in shared memory.</source>
          <target state="translated">检查张量是否在共享内存中。</target>
        </trans-unit>
        <trans-unit id="19004c9ecdb08358c5474afc574d3f10f8cf3c71" translate="yes" xml:space="preserve">
          <source>Checks if the MPI backend is available.</source>
          <target state="translated">检查MPI后端是否可用。</target>
        </trans-unit>
        <trans-unit id="f2d9da2e3f6bd1dd91d87846cd5b419112d24146" translate="yes" xml:space="preserve">
          <source>Checks if the NCCL backend is available.</source>
          <target state="translated">检查NCCL后台是否可用。</target>
        </trans-unit>
        <trans-unit id="1f4ac26af6f73e2c8bc1f384efba8f7bd4839e87" translate="yes" xml:space="preserve">
          <source>Chi2</source>
          <target state="translated">Chi2</target>
        </trans-unit>
        <trans-unit id="56268a6e36c10e47601c6c33d1ee9618154a6c67" translate="yes" xml:space="preserve">
          <source>Choosing the network interface to use</source>
          <target state="translated">选择要使用的网络接口</target>
        </trans-unit>
        <trans-unit id="d9eaceff91c313f7cab180625e8b4dd09baebd23" translate="yes" xml:space="preserve">
          <source>Clamp all elements in &lt;code&gt;input&lt;/code&gt; into the range &lt;code&gt;[&lt;/code&gt;&lt;a href=&quot;generated/torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;&lt;code&gt;]&lt;/code&gt; and return a resulting tensor:</source>
          <target state="translated">将 &lt;code&gt;input&lt;/code&gt; 所有元素限制在 &lt;code&gt;[&lt;/code&gt; &lt;a href=&quot;generated/torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt; &lt;code&gt;]&lt;/code&gt; 范围内，并返回结果张量：</target>
        </trans-unit>
        <trans-unit id="378e42a306697a4dd63aa3dd31b3c55c600db5d2" translate="yes" xml:space="preserve">
          <source>Clamp all elements in &lt;code&gt;input&lt;/code&gt; into the range &lt;code&gt;[&lt;/code&gt;&lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;&lt;code&gt;]&lt;/code&gt; and return a resulting tensor:</source>
          <target state="translated">将 &lt;code&gt;input&lt;/code&gt; 所有元素限制在 &lt;code&gt;[&lt;/code&gt; &lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt; &lt;code&gt;]&lt;/code&gt; 范围内，并返回结果张量：</target>
        </trans-unit>
        <trans-unit id="cfe592b543afcbd470b6fcfa84bfb4061fbeb434" translate="yes" xml:space="preserve">
          <source>Clamps all elements in &lt;code&gt;input&lt;/code&gt; to be larger or equal &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">将 &lt;code&gt;input&lt;/code&gt; 所有元素限制为大于或等于&lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e7bab3a95a3c2fe6a5a776d67024bda46c058f02" translate="yes" xml:space="preserve">
          <source>Clamps all elements in &lt;code&gt;input&lt;/code&gt; to be smaller or equal &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">将 &lt;code&gt;input&lt;/code&gt; 所有元素限制为小于或等于&lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="a8943ee6c6160fec8979faa9096fac2e19ce7bfd" translate="yes" xml:space="preserve">
          <source>Classes must be new-style classes, as we use &lt;code&gt;__new__()&lt;/code&gt; to construct them with pybind11.</source>
          <target state="translated">类必须是新型类，因为我们使用 &lt;code&gt;__new__()&lt;/code&gt; 与pybind11构造它们。</target>
        </trans-unit>
        <trans-unit id="9b6b05a782b6d150aaf1e98e0fdbbcbc865ca9be" translate="yes" xml:space="preserve">
          <source>Classes that inherit from &lt;code&gt;torch.jit.ScriptModule&lt;/code&gt;</source>
          <target state="translated">从 &lt;code&gt;torch.jit.ScriptModule&lt;/code&gt; 继承的类</target>
        </trans-unit>
        <trans-unit id="94c2a3189e7f7885455350c4c7a8df2d0d6ad1d1" translate="yes" xml:space="preserve">
          <source>Classification</source>
          <target state="translated">Classification</target>
        </trans-unit>
        <trans-unit id="140c2943a67faf9ddd529fa9ef19fca05ebb9209" translate="yes" xml:space="preserve">
          <source>Clears the cuFFT plan cache.</source>
          <target state="translated">清理cuFFT计划缓存。</target>
        </trans-unit>
        <trans-unit id="026a5307ef4b381234e9524521b9341950a6f9cf" translate="yes" xml:space="preserve">
          <source>Clip acc@1</source>
          <target state="translated">Clip acc@1</target>
        </trans-unit>
        <trans-unit id="a7ba9ed0f0f5ac74d1bfd4abe70aaeb95921f947" translate="yes" xml:space="preserve">
          <source>Clip acc@5</source>
          <target state="translated">Clip acc@5</target>
        </trans-unit>
        <trans-unit id="75ccb1e878d6b2fcd6f1473f8a7c353d6b2e4806" translate="yes" xml:space="preserve">
          <source>Clips gradient norm of an iterable of parameters.</source>
          <target state="translated">剪切参数迭代的梯度法线。</target>
        </trans-unit>
        <trans-unit id="990a2e75895b3d0ccfe4b20a82d20496b3e8ee0f" translate="yes" xml:space="preserve">
          <source>Clips gradient of an iterable of parameters at specified value.</source>
          <target state="translated">以指定的值剪切参数迭代的梯度。</target>
        </trans-unit>
        <trans-unit id="d4abb632898cd5f62fe263afaf95d739031ecab2" translate="yes" xml:space="preserve">
          <source>Closure use is not currently supported.</source>
          <target state="translated">目前不支持封闭使用。</target>
        </trans-unit>
        <trans-unit id="2b8f8ed2e1db909d591f68c446a99d83144eb890" translate="yes" xml:space="preserve">
          <source>Code running on Node 0</source>
          <target state="translated">在节点0上运行的代码</target>
        </trans-unit>
        <trans-unit id="4a4d07ff7beaaa363e77cbea9319ab7a99ad01d0" translate="yes" xml:space="preserve">
          <source>Code running on Node 1</source>
          <target state="translated">在节点1上运行的代码</target>
        </trans-unit>
        <trans-unit id="bfa7c14251111856333ef843853b74b24bdf052b" translate="yes" xml:space="preserve">
          <source>Collective functions</source>
          <target state="translated">集体职能</target>
        </trans-unit>
        <trans-unit id="43092611315cdcde3137662c72b15d7a3a889e75" translate="yes" xml:space="preserve">
          <source>Collects the provided &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; objects into a single combined &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; that is completed when all of the sub-futures are completed.</source>
          <target state="translated">将提供的&lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;对象收集到单个合并的&lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; 中&lt;/a&gt;，当所有子期货都完成时，该组合就完成了。</target>
        </trans-unit>
        <trans-unit id="ce079595058d15f2351c808e0ae1b284bea0c578" translate="yes" xml:space="preserve">
          <source>Combines an array of sliding local blocks into a large containing tensor.</source>
          <target state="translated">将滑动局部块的数组组合成一个大的包含张量。</target>
        </trans-unit>
        <trans-unit id="46488d0a373ab0b6babd0f93cccfb474321ae981" translate="yes" xml:space="preserve">
          <source>Combining Distributed DataParallel with Distributed RPC Framework</source>
          <target state="translated">分布式DataParallel与分布式RPC框架的结合。</target>
        </trans-unit>
        <trans-unit id="c170217f39333026f4f3aa0323fa905552560ec9" translate="yes" xml:space="preserve">
          <source>Common environment variables</source>
          <target state="translated">常见环境变量</target>
        </trans-unit>
        <trans-unit id="086cb07d3ecde2b840e1f458bfa9fda481174a5f" translate="yes" xml:space="preserve">
          <source>Common linear algebra operations.</source>
          <target state="translated">常见的线性代数运算。</target>
        </trans-unit>
        <trans-unit id="b4f955c58ceb08791953e5d03b53e986ad69907d" translate="yes" xml:space="preserve">
          <source>Commonly used ones include the following for debugging purposes:</source>
          <target state="translated">常用的有以下几种,用于调试。</target>
        </trans-unit>
        <trans-unit id="4beeb1b54292a1c6c080f3d414574b83eca3b2f1" translate="yes" xml:space="preserve">
          <source>Communication collectives</source>
          <target state="translated">通信集体</target>
        </trans-unit>
        <trans-unit id="bfd58ee3a270f3a931009900e1008d549bbd7453" translate="yes" xml:space="preserve">
          <source>Community</source>
          <target state="translated">Community</target>
        </trans-unit>
        <trans-unit id="e0c1faa1db1f49518cfef07ea955debfa4db607f" translate="yes" xml:space="preserve">
          <source>Compare against the full output from &lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">与&lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt;的完整输出进行比较：</target>
        </trans-unit>
        <trans-unit id="0ccc8d4186f8b576455c6bf986c275fa4e1ca508" translate="yes" xml:space="preserve">
          <source>Compare against the full output from &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">与&lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt;的完整输出进行比较：</target>
        </trans-unit>
        <trans-unit id="d9194e4e84d8475d85c26bc6d3d0aabdb9a3cfba" translate="yes" xml:space="preserve">
          <source>Compared against the full output from &lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt;, we have all elements up to the Nyquist frequency.</source>
          <target state="translated">与来自&lt;a href=&quot;#torch.fft.fftn&quot;&gt; &lt;code&gt;fftn()&lt;/code&gt; &lt;/a&gt;的完整输出相比，我们拥有的所有元件均达到奈奎斯特频率。</target>
        </trans-unit>
        <trans-unit id="25acda77b0bc6b058bd349dd6f4a271e2a967bfe" translate="yes" xml:space="preserve">
          <source>Comparison Operators</source>
          <target state="translated">比较运算符</target>
        </trans-unit>
        <trans-unit id="64e945410bd38eb8bc541fa4fe0d1a5601a0a7ba" translate="yes" xml:space="preserve">
          <source>Comparison Ops</source>
          <target state="translated">比较行动</target>
        </trans-unit>
        <trans-unit id="c73def212afdc811169afd7e77aebfbeecb5facc" translate="yes" xml:space="preserve">
          <source>Complex Numbers</source>
          <target state="translated">复数</target>
        </trans-unit>
        <trans-unit id="258ac8f61c6506a35c0dfc4029a4818ba9555f2a" translate="yes" xml:space="preserve">
          <source>Complex values are infinite when their real or imaginary part is infinite.</source>
          <target state="translated">当其实部或虚部为无限时,复值为无限。</target>
        </trans-unit>
        <trans-unit id="20602cb78c6ade6e08a8e69fafec22e07a7a725f" translate="yes" xml:space="preserve">
          <source>Complex-to-complex Discrete Fourier Transform.</source>
          <target state="translated">复杂到复杂的离散傅立叶变换。</target>
        </trans-unit>
        <trans-unit id="a7075fa71ba5aa25f75795b49f1a3a1c5779af40" translate="yes" xml:space="preserve">
          <source>Complex-to-complex Inverse Discrete Fourier Transform.</source>
          <target state="translated">复杂到复杂的反离散傅里叶变换。</target>
        </trans-unit>
        <trans-unit id="cfb4fe1390cd2aa35fb926daaca343f331617fd7" translate="yes" xml:space="preserve">
          <source>Complex-to-real Inverse Discrete Fourier Transform.</source>
          <target state="translated">复杂到真实的反离散傅里叶变换。</target>
        </trans-unit>
        <trans-unit id="5269e04efa4004aaf8887ad9e476bf6a875cd697" translate="yes" xml:space="preserve">
          <source>Composes multiple transforms in a chain. The transforms being composed are responsible for caching.</source>
          <target state="translated">在一个链中组合多个变换。被组合的变换负责缓存。</target>
        </trans-unit>
        <trans-unit id="27aacbfb281b15263d3c0a945d248f0f178b79f9" translate="yes" xml:space="preserve">
          <source>Compute Kullback-Leibler divergence</source>
          <target state="translated">计算Kullback-Leibler偏离度</target>
        </trans-unit>
        <trans-unit id="49a3dcb8dae28aaaa8933d10f9d1fc99995fdb01" translate="yes" xml:space="preserve">
          <source>Compute combinations of length</source>
          <target state="translated">计算长度的组合</target>
        </trans-unit>
        <trans-unit id="1c47a0f87804b817e8011c46890b198b5bc4d108" translate="yes" xml:space="preserve">
          <source>Compute the scale and zero point the same way as in the</source>
          <target state="translated">计算比例尺和零点的方法与在本例中相同。</target>
        </trans-unit>
        <trans-unit id="ad88b5c709893b3c20a622a8af743326fc15e581" translate="yes" xml:space="preserve">
          <source>Computes</source>
          <target state="translated">Computes</target>
        </trans-unit>
        <trans-unit id="b29ddfa9b93c0c21e2cd9673bf929a7688cb085a" translate="yes" xml:space="preserve">
          <source>Computes &lt;code&gt;input&lt;/code&gt; divided by &lt;code&gt;other&lt;/code&gt;, elementwise, and rounds each quotient towards zero. Equivalently, it truncates the quotient(s):</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 除以 &lt;code&gt;other&lt;/code&gt; ，按元素划分的值，并将每个商都舍入为零。等效地，它会截断商：</target>
        </trans-unit>
        <trans-unit id="d3181f8cc8e8fa272eef8d4d3ff7058798e00f63" translate="yes" xml:space="preserve">
          <source>Computes a QR decomposition of &lt;code&gt;input&lt;/code&gt;, but without constructing</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 的QR分解，但不构造</target>
        </trans-unit>
        <trans-unit id="f69a85669ba4f546c3f6c75d8d8a225191c83e6a" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;a href=&quot;torch.nn.maxpool1d#torch.nn.MaxPool1d&quot;&gt;&lt;code&gt;MaxPool1d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">计算&lt;a href=&quot;torch.nn.maxpool1d#torch.nn.MaxPool1d&quot;&gt; &lt;code&gt;MaxPool1d&lt;/code&gt; &lt;/a&gt;的局部逆。</target>
        </trans-unit>
        <trans-unit id="bf04c1ac66a51bee90d0426e3c8106a0e4185416" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;a href=&quot;torch.nn.maxpool2d#torch.nn.MaxPool2d&quot;&gt;&lt;code&gt;MaxPool2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">计算&lt;a href=&quot;torch.nn.maxpool2d#torch.nn.MaxPool2d&quot;&gt; &lt;code&gt;MaxPool2d&lt;/code&gt; &lt;/a&gt;的局部逆。</target>
        </trans-unit>
        <trans-unit id="f360cd6902e1a0579d80b170c5497449c1afe016" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;a href=&quot;torch.nn.maxpool3d#torch.nn.MaxPool3d&quot;&gt;&lt;code&gt;MaxPool3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">计算&lt;a href=&quot;torch.nn.maxpool3d#torch.nn.MaxPool3d&quot;&gt; &lt;code&gt;MaxPool3d&lt;/code&gt; &lt;/a&gt;的局部逆。</target>
        </trans-unit>
        <trans-unit id="26e6d25fb90ed6ad09263d6546b96e18a94d0c3e" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;code&gt;MaxPool1d&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;MaxPool1d&lt;/code&gt; 的局部逆。</target>
        </trans-unit>
        <trans-unit id="18c31c026887e211a21e3dc7927a9d793872976e" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;code&gt;MaxPool2d&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;MaxPool2d&lt;/code&gt; 的局部逆。</target>
        </trans-unit>
        <trans-unit id="85ab6bb517a0364e7f136e092fa61b3c016286ff" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;code&gt;MaxPool3d&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;MaxPool3d&lt;/code&gt; 的局部逆。</target>
        </trans-unit>
        <trans-unit id="b7cc19759037b27ca2d7fb79591e62353c0937a7" translate="yes" xml:space="preserve">
          <source>Computes and returns a mask for the input tensor &lt;code&gt;t&lt;/code&gt;. Starting from a base &lt;code&gt;default_mask&lt;/code&gt; (which should be a mask of ones if the tensor has not been pruned yet), generate a mask to apply on top of the &lt;code&gt;default_mask&lt;/code&gt; by zeroing out the channels along the specified dim with the lowest Ln-norm.</source>
          <target state="translated">计算并返回输入张量 &lt;code&gt;t&lt;/code&gt; 的掩码。从基本 &lt;code&gt;default_mask&lt;/code&gt; （如果尚未修剪张量，应为1的掩码）开始，通过将沿Ln范数最低的指定暗角的通道归零，生成一个掩码以应用在 &lt;code&gt;default_mask&lt;/code&gt; 之上。</target>
        </trans-unit>
        <trans-unit id="40d80ee71e511c62ded894cfa5f39bc5b20875f0" translate="yes" xml:space="preserve">
          <source>Computes and returns a mask for the input tensor &lt;code&gt;t&lt;/code&gt;. Starting from a base &lt;code&gt;default_mask&lt;/code&gt; (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the &lt;code&gt;default_mask&lt;/code&gt; according to the specific pruning method recipe.</source>
          <target state="translated">计算并返回输入张量 &lt;code&gt;t&lt;/code&gt; 的掩码。从基本 &lt;code&gt;default_mask&lt;/code&gt; （如果尚未修剪张量，应为1的掩码）开始，根据特定的修剪方法配方，生成一个随机掩码以应用到 &lt;code&gt;default_mask&lt;/code&gt; 之上。</target>
        </trans-unit>
        <trans-unit id="65434598b41da121b29ca65831d67382801ae0c7" translate="yes" xml:space="preserve">
          <source>Computes and returns a mask for the input tensor &lt;code&gt;t&lt;/code&gt;. Starting from a base &lt;code&gt;default_mask&lt;/code&gt; (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the &lt;code&gt;default_mask&lt;/code&gt; by randomly zeroing out channels along the specified dim of the tensor.</source>
          <target state="translated">计算并返回输入张量 &lt;code&gt;t&lt;/code&gt; 的掩码。从基本 &lt;code&gt;default_mask&lt;/code&gt; （如果尚未修剪张量，应为1的掩码）开始，通过沿张量的指定暗部随机将通道清零，生成一个随机掩码以应用于 &lt;code&gt;default_mask&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="180bc7b0d8de7293424fe248fc89cd2c995f979c" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">根据&lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; 中&lt;/a&gt;指定的修剪规则计算并返回输入张量 &lt;code&gt;t&lt;/code&gt; 的修剪版本。</target>
        </trans-unit>
        <trans-unit id="7a3fb50e3449e8908990f0c00a969ef20d83c411" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;a href=&quot;#torch.nn.utils.prune.LnStructured.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">根据&lt;a href=&quot;#torch.nn.utils.prune.LnStructured.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; 中&lt;/a&gt;指定的修剪规则计算并返回输入张量 &lt;code&gt;t&lt;/code&gt; 的修剪版本。</target>
        </trans-unit>
        <trans-unit id="9efff4b9b21781ca7329ebc9ab81c9ec882d85a1" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;a href=&quot;#torch.nn.utils.prune.PruningContainer.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">根据&lt;a href=&quot;#torch.nn.utils.prune.PruningContainer.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; 中&lt;/a&gt;指定的修剪规则计算并返回输入张量 &lt;code&gt;t&lt;/code&gt; 的修剪版本。</target>
        </trans-unit>
        <trans-unit id="f99800562bd522721e9ea1fa3102c89a24cca75f" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;a href=&quot;#torch.nn.utils.prune.RandomStructured.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">根据&lt;a href=&quot;#torch.nn.utils.prune.RandomStructured.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; 中&lt;/a&gt;指定的修剪规则计算并返回输入张量 &lt;code&gt;t&lt;/code&gt; 的修剪版本。</target>
        </trans-unit>
        <trans-unit id="28c95a2d472ee12c714c4dae6bc27475d57d2a6e" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;code&gt;compute_mask()&lt;/code&gt;.</source>
          <target state="translated">根据 &lt;code&gt;compute_mask()&lt;/code&gt; 中指定的修剪规则计算并返回输入张量 &lt;code&gt;t&lt;/code&gt; 的修剪版本。</target>
        </trans-unit>
        <trans-unit id="7251032ff2b6d46923b585e413559af8bf29d269" translate="yes" xml:space="preserve">
          <source>Computes and returns the sum of gradients of outputs w.r.t. the inputs.</source>
          <target state="translated">计算并返回输出与输入的梯度之和。</target>
        </trans-unit>
        <trans-unit id="2ad7888f31836fca25fd59e8bf8deaf0775bd8bb" translate="yes" xml:space="preserve">
          <source>Computes batched the p-norm distance between each pair of the two collections of row vectors.</source>
          <target state="translated">计算每对行向量集合之间的p-norm距离。</target>
        </trans-unit>
        <trans-unit id="f2e6dd93c4230ac985c6eeef22bb5e42799d351c" translate="yes" xml:space="preserve">
          <source>Computes element-wise equality</source>
          <target state="translated">计算元素的平等性</target>
        </trans-unit>
        <trans-unit id="23c55c6419438b7d4003d50aab0fa866dc6b754c" translate="yes" xml:space="preserve">
          <source>Computes log probabilities for all</source>
          <target state="translated">计算所有的对数概率</target>
        </trans-unit>
        <trans-unit id="bffd347218e0b14edb14dc921db35164acedc246" translate="yes" xml:space="preserve">
          <source>Computes sums or means of &amp;lsquo;bags&amp;rsquo; of embeddings, without instantiating the intermediate embeddings.</source>
          <target state="translated">计算嵌入&amp;ldquo;袋&amp;rdquo;的总和或方法，而无需实例化中间嵌入。</target>
        </trans-unit>
        <trans-unit id="a7d9c1eb8347c9d1cc0af2b2b967716bd8c4ae77" translate="yes" xml:space="preserve">
          <source>Computes sums, means or maxes of &lt;code&gt;bags&lt;/code&gt; of embeddings, without instantiating the intermediate embeddings.</source>
          <target state="translated">计算嵌入 &lt;code&gt;bags&lt;/code&gt; 的总和，平均值或最大值，而无需实例化中间嵌入。</target>
        </trans-unit>
        <trans-unit id="b71f4915a8b78a90bf71f4feb80ae5bf3e75096c" translate="yes" xml:space="preserve">
          <source>Computes the</source>
          <target state="translated">计算</target>
        </trans-unit>
        <trans-unit id="ff498bc657a9c74488608c0ce73fd38063feeafe" translate="yes" xml:space="preserve">
          <source>Computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_gamma_function&quot;&gt;multivariate log-gamma function&lt;/a&gt;) with dimension</source>
          <target state="translated">用维度计算&lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_gamma_function&quot;&gt;多元对数-伽马函数&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="76557cba93a25c6c86c33cc0d82d4b6080c0ca6f" translate="yes" xml:space="preserve">
          <source>Computes the Cholesky decomposition of a symmetric positive-definite matrix</source>
          <target state="translated">计算对称正定矩阵的Cholesky分解。</target>
        </trans-unit>
        <trans-unit id="81b53a3d90c63815ad24683a8c1c69a90d47d01e" translate="yes" xml:space="preserve">
          <source>Computes the Heaviside step function for each element in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 中每个元素的Heaviside步进函数。</target>
        </trans-unit>
        <trans-unit id="5d07d50a47d9ca8fe0d585f7aa291a49089343e4" translate="yes" xml:space="preserve">
          <source>Computes the Heaviside step function for each element in &lt;code&gt;input&lt;/code&gt;. The Heaviside step function is defined as:</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 中每个元素的Heaviside步进函数。Heaviside阶跃函数定义为：</target>
        </trans-unit>
        <trans-unit id="275e74bec95b0ab85103e19b6d6479c703706e12" translate="yes" xml:space="preserve">
          <source>Computes the Kaiser window with window length &lt;code&gt;window_length&lt;/code&gt; and shape parameter &lt;code&gt;beta&lt;/code&gt;.</source>
          <target state="translated">使用窗口长度 &lt;code&gt;window_length&lt;/code&gt; 和shape参数 &lt;code&gt;beta&lt;/code&gt; 计算Kaiser窗口。</target>
        </trans-unit>
        <trans-unit id="622fda98680f4721949b3e2bf9b78a14c34deb65" translate="yes" xml:space="preserve">
          <source>Computes the LU factorization of a matrix or batches of matrices &lt;code&gt;A&lt;/code&gt;.</source>
          <target state="translated">计算矩阵或矩阵 &lt;code&gt;A&lt;/code&gt; 的批次的LU分解。</target>
        </trans-unit>
        <trans-unit id="8ec07270d301f5370c046b7066eb8663a8f402a1" translate="yes" xml:space="preserve">
          <source>Computes the LU factorization of a matrix or batches of matrices &lt;code&gt;A&lt;/code&gt;. Returns a tuple containing the LU factorization and pivots of &lt;code&gt;A&lt;/code&gt;. Pivoting is done if &lt;code&gt;pivot&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">计算矩阵或矩阵 &lt;code&gt;A&lt;/code&gt; 的批次的LU分解。返回一个包含LU分解和 &lt;code&gt;A&lt;/code&gt; 的枢轴的元组。如果将 &lt;code&gt;pivot&lt;/code&gt; 设置为 &lt;code&gt;True&lt;/code&gt; ,则完成透视。</target>
        </trans-unit>
        <trans-unit id="0c3fba3f6161937ff4ae86bdb0e983d0c0fdda0b" translate="yes" xml:space="preserve">
          <source>Computes the N dimensional discrete Fourier transform of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">计算N个维离散付里叶变换的 &lt;code&gt;input&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5852120d66c32f7c318de497ae26eeb74f1de763" translate="yes" xml:space="preserve">
          <source>Computes the N dimensional inverse discrete Fourier transform of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">计算N个维离散傅立叶逆变换的 &lt;code&gt;input&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="efd57d1e36575205366c8a3b4753ee576bd1a231" translate="yes" xml:space="preserve">
          <source>Computes the N-dimensional discrete Fourier transform of real &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">计算实际 &lt;code&gt;input&lt;/code&gt; 的N维离散傅里叶变换。</target>
        </trans-unit>
        <trans-unit id="a21392e07d8e832fdaaf1fc616e555a310a70752" translate="yes" xml:space="preserve">
          <source>Computes the QR decomposition of a matrix or a batch of matrices &lt;code&gt;input&lt;/code&gt;, and returns a namedtuple (Q, R) of tensors such that</source>
          <target state="translated">计算矩阵或一批矩阵 &lt;code&gt;input&lt;/code&gt; 的QR分解，并返回张量的命名元组（Q，R），使得</target>
        </trans-unit>
        <trans-unit id="d476f44296e977553528b2efe494b3789b4d6cc8" translate="yes" xml:space="preserve">
          <source>Computes the absolute value of each element in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 中每个元素的绝对值。</target>
        </trans-unit>
        <trans-unit id="da0e0333ca7d69a3bf51d86eddafcf5537f32ee1" translate="yes" xml:space="preserve">
          <source>Computes the base two exponential function of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 的基二指数函数。</target>
        </trans-unit>
        <trans-unit id="833d30840f6f79f8ac884e63624d72a29fe2aedb" translate="yes" xml:space="preserve">
          <source>Computes the batchwise pairwise distance between vectors</source>
          <target state="translated">计算向量之间的批量对距离。</target>
        </trans-unit>
        <trans-unit id="b81b9d06afde312e3c1de6fa3003c1826f20a144" translate="yes" xml:space="preserve">
          <source>Computes the bitwise AND of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;other&lt;/code&gt; 的按位与。</target>
        </trans-unit>
        <trans-unit id="724c0661558717da97da9f13301714db62123db8" translate="yes" xml:space="preserve">
          <source>Computes the bitwise AND of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical AND.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;other&lt;/code&gt; 的按位与。输入张量必须是整数或布尔类型。对于布尔张量，它计算逻辑与。</target>
        </trans-unit>
        <trans-unit id="4c385fff8361fd76237dd3d0d310601e4ded675d" translate="yes" xml:space="preserve">
          <source>Computes the bitwise NOT of the given input tensor.</source>
          <target state="translated">计算给定输入张量的位非。</target>
        </trans-unit>
        <trans-unit id="d34002186774eda6ff1544d9bb235cb5b5e464f1" translate="yes" xml:space="preserve">
          <source>Computes the bitwise NOT of the given input tensor. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical NOT.</source>
          <target state="translated">计算给定输入张量的位非。输入张量必须是积分或布尔类型。对于布尔型张量,它计算逻辑NOT。</target>
        </trans-unit>
        <trans-unit id="1676d4cae5deb51721032154a09c9a5c26471fe8" translate="yes" xml:space="preserve">
          <source>Computes the bitwise OR of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;other&lt;/code&gt; 的按位或。</target>
        </trans-unit>
        <trans-unit id="db7eb5bd0ae610915cc8f5aaa985b610903472f9" translate="yes" xml:space="preserve">
          <source>Computes the bitwise OR of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical OR.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;other&lt;/code&gt; 的按位或。输入张量必须是整数或布尔类型。对于布尔张量，它计算逻辑或。</target>
        </trans-unit>
        <trans-unit id="2a7aa82e188d57e1be91dd1e59e101918adf2efe" translate="yes" xml:space="preserve">
          <source>Computes the bitwise XOR of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;other&lt;/code&gt; 的按位XOR 。</target>
        </trans-unit>
        <trans-unit id="88579c44bdc54edf5679b9534b4247fa5a02a043" translate="yes" xml:space="preserve">
          <source>Computes the bitwise XOR of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical XOR.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;other&lt;/code&gt; 的按位XOR 。输入张量必须是整数或布尔类型。对于布尔张量，它计算逻辑XOR。</target>
        </trans-unit>
        <trans-unit id="8a3e45b925774a34d85da6c0c7b80c7c01e6ba8d" translate="yes" xml:space="preserve">
          <source>Computes the complementary error function of each element of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 的每个元素的互补误差函数。</target>
        </trans-unit>
        <trans-unit id="159a12e467808284be4f45386c2f19ec971818e9" translate="yes" xml:space="preserve">
          <source>Computes the complementary error function of each element of &lt;code&gt;input&lt;/code&gt;. The complementary error function is defined as follows:</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 的每个元素的互补误差函数。互补误差函数定义如下：</target>
        </trans-unit>
        <trans-unit id="e8d67586d06368678f0e647fdffb8c00b8bde78b" translate="yes" xml:space="preserve">
          <source>Computes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution.</source>
          <target state="translated">通过反转变换和计算基本分布的得分来计算累积分布函数。</target>
        </trans-unit>
        <trans-unit id="1983c92beb407d56255dcc00b5fe7b66872fc6d6" translate="yes" xml:space="preserve">
          <source>Computes the dot product (inner product) of two tensors.</source>
          <target state="translated">计算两个正弦的点积(内积)。</target>
        </trans-unit>
        <trans-unit id="9a5b212f2d4dd5a8260512297de68c3095e68cab" translate="yes" xml:space="preserve">
          <source>Computes the dot product (inner product) of two tensors. The vdot(a, b) function handles complex numbers differently than dot(a, b). If the first argument is complex, the complex conjugate of the first argument is used for the calculation of the dot product.</source>
          <target state="translated">计算两个正弦的点积(内积).vdot(a,b)函数处理复数的方法与dot(a,b)不同.vdot(a,b)函数处理复数的方式与dot(a,b)不同。如果第一个参数是复数,则使用第一个参数的复数共轭来计算点积。</target>
        </trans-unit>
        <trans-unit id="e16561fbcf4a6c2f2dd9afb54001a93a98731fdc" translate="yes" xml:space="preserve">
          <source>Computes the eigenvalues and eigenvectors of a real square matrix.</source>
          <target state="translated">计算实方矩阵的特征值和特征向量。</target>
        </trans-unit>
        <trans-unit id="52f9bd141b84eda92242b06f7f6d290a253966b4" translate="yes" xml:space="preserve">
          <source>Computes the element-wise angle (in radians) of the given &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">计算给定 &lt;code&gt;input&lt;/code&gt; 张量的元素方向角度（以弧度为单位）。</target>
        </trans-unit>
        <trans-unit id="47dc21f5f4986e27abb977788d16b590a07a71d6" translate="yes" xml:space="preserve">
          <source>Computes the element-wise conjugate of the given &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">计算给定 &lt;code&gt;input&lt;/code&gt; 张量的逐元素共轭。</target>
        </trans-unit>
        <trans-unit id="9a964097a08f63a42878b22585751fd92a8682df" translate="yes" xml:space="preserve">
          <source>Computes the element-wise conjugate of the given &lt;code&gt;input&lt;/code&gt; tensor. If :attr`input` has a non-complex dtype, this function just returns &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">计算给定 &lt;code&gt;input&lt;/code&gt; 张量的逐元素共轭。如果：attr`input`具有非复杂dtype，则此函数仅返回 &lt;code&gt;input&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="8e315d5a6fd3a46bc729f0eda3f3fb92b601c50b" translate="yes" xml:space="preserve">
          <source>Computes the element-wise greatest common divisor (GCD) of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;other&lt;/code&gt; 的按元素的最大公约数（GCD）。</target>
        </trans-unit>
        <trans-unit id="91ee032a4ff000ec84d35b04317442f4984e6937" translate="yes" xml:space="preserve">
          <source>Computes the element-wise least common multiple (LCM) of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;other&lt;/code&gt; 的按元素的最小公倍数（LCM）。</target>
        </trans-unit>
        <trans-unit id="8e7b021553644e46362536880a5431b72a0587b9" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical AND of the given input tensors.</source>
          <target state="translated">计算给定输入时序的元素逻辑AND。</target>
        </trans-unit>
        <trans-unit id="68b2bf2409a8b5d2be8906244e0a49294d2e9679" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical AND of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">计算给定输入张量的按元素逻辑与。零被视为 &lt;code&gt;False&lt;/code&gt; ，非零被视为 &lt;code&gt;True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="a94b7ab9f10124804e19d264d7207840dc6728d2" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical NOT of the given input tensor.</source>
          <target state="translated">计算给定输入张量的元素逻辑NOT。</target>
        </trans-unit>
        <trans-unit id="117610e848f679548feba89108c832e6e9a20124" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical NOT of the given input tensor. If not specified, the output tensor will have the bool dtype. If the input tensor is not a bool tensor, zeros are treated as &lt;code&gt;False&lt;/code&gt; and non-zeros are treated as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">计算给定输入张量的按元素逻辑非。如果未指定，则输出张量将具有bool dtype。如果输入张量不是布尔张量，则将零视为 &lt;code&gt;False&lt;/code&gt; ，将非零视为 &lt;code&gt;True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="9572b2dd91b0d03ca02c1394c350df1174a646b0" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical OR of the given input tensors.</source>
          <target state="translated">计算给定输入时序的元素逻辑OR。</target>
        </trans-unit>
        <trans-unit id="256665a803b61ee6a430a86e174d4bca0ee69abe" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical OR of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">计算给定输入张量的按元素逻辑或。零被视为 &lt;code&gt;False&lt;/code&gt; ，非零被视为 &lt;code&gt;True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c172a20dd69ad22bb7b0a4de9500f73f3c3f98b9" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical XOR of the given input tensors.</source>
          <target state="translated">计算给定输入 tensors 的元素逻辑 XOR。</target>
        </trans-unit>
        <trans-unit id="c8837d2e60640bc8199344497981535b4ce62699" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical XOR of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">计算给定输入张量的逐元素逻辑XOR。零被视为 &lt;code&gt;False&lt;/code&gt; ，非零被视为 &lt;code&gt;True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b73dd1c77581f85619be2c0a672fa472582b7ef0" translate="yes" xml:space="preserve">
          <source>Computes the element-wise maximum of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;other&lt;/code&gt; 的逐元素最大值。</target>
        </trans-unit>
        <trans-unit id="c821156c5be7a7c6ec010c82496a8862539fc670" translate="yes" xml:space="preserve">
          <source>Computes the element-wise minimum of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;other&lt;/code&gt; 的按元素的最小值。</target>
        </trans-unit>
        <trans-unit id="6b3693910cf57f242c4267e044d56aef8263e01c" translate="yes" xml:space="preserve">
          <source>Computes the element-wise remainder of division.</source>
          <target state="translated">计算除法的元素余数。</target>
        </trans-unit>
        <trans-unit id="9003dd6b7c6711ac836cacfed24f4e0435a56aec" translate="yes" xml:space="preserve">
          <source>Computes the error function of each element.</source>
          <target state="translated">计算每个元素的误差函数。</target>
        </trans-unit>
        <trans-unit id="29774d569920339acd042c100fd75031d60d53ac" translate="yes" xml:space="preserve">
          <source>Computes the error function of each element. The error function is defined as follows:</source>
          <target state="translated">计算每个元素的误差函数。误差函数定义如下:</target>
        </trans-unit>
        <trans-unit id="2434be214f6e5985b87d4c052539ff329a7f0314" translate="yes" xml:space="preserve">
          <source>Computes the fractional portion of each element in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 中每个元素的小数部分。</target>
        </trans-unit>
        <trans-unit id="cb83638df4a6cb7d822fc5fb54ba573a3cd29240" translate="yes" xml:space="preserve">
          <source>Computes the gradient of current tensor w.r.t. graph leaves.</source>
          <target state="translated">计算当前张量与图叶的梯度。</target>
        </trans-unit>
        <trans-unit id="ce4470aa5bf338d4ad1d1c606fbb7a81df3b9429" translate="yes" xml:space="preserve">
          <source>Computes the histogram of a tensor.</source>
          <target state="translated">计算张量的直方图。</target>
        </trans-unit>
        <trans-unit id="28063e348dc010ab497d4b723f1ef5afa57d8d1a" translate="yes" xml:space="preserve">
          <source>Computes the inverse cosine of each element in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 中每个元素的反余弦值。</target>
        </trans-unit>
        <trans-unit id="e87d84d689f558075500c51085bf75e2883ed115" translate="yes" xml:space="preserve">
          <source>Computes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution.</source>
          <target state="translated">使用变换计算逆累积分布函数,并计算基础分布的得分。</target>
        </trans-unit>
        <trans-unit id="a8f61c7d805f4a069d9c46e57aa8a4eda42a714f" translate="yes" xml:space="preserve">
          <source>Computes the inverse error function of each element of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 的每个元素的反误差函数。</target>
        </trans-unit>
        <trans-unit id="ac7da6502603da8eebb2a12646d0b0cbe100d072" translate="yes" xml:space="preserve">
          <source>Computes the inverse error function of each element of &lt;code&gt;input&lt;/code&gt;. The inverse error function is defined in the range</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 的每个元素的反误差函数。逆误差函数在以下范围内定义</target>
        </trans-unit>
        <trans-unit id="e0fc6a3f3385283ab40e97e09d370d8da0903b9c" translate="yes" xml:space="preserve">
          <source>Computes the inverse of &lt;a href=&quot;#torch.fft.hfft&quot;&gt;&lt;code&gt;hfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">计算&lt;a href=&quot;#torch.fft.hfft&quot;&gt; &lt;code&gt;hfft()&lt;/code&gt; &lt;/a&gt;的逆。</target>
        </trans-unit>
        <trans-unit id="f42895543a1b54cd19094b1bf3f3033fd72a6293" translate="yes" xml:space="preserve">
          <source>Computes the inverse of &lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">计算&lt;a href=&quot;#torch.fft.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt;的逆。</target>
        </trans-unit>
        <trans-unit id="8157bbb847ce3db3aa43a43004aeed9105153b04" translate="yes" xml:space="preserve">
          <source>Computes the inverse of &lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">计算&lt;a href=&quot;#torch.fft.rfftn&quot;&gt; &lt;code&gt;rfftn()&lt;/code&gt; &lt;/a&gt;的逆。</target>
        </trans-unit>
        <trans-unit id="751c20273a8826ac2930fa4c66f27dae7e42581e" translate="yes" xml:space="preserve">
          <source>Computes the inverse of a symmetric positive-definite matrix</source>
          <target state="translated">计算对称正定矩阵的反。</target>
        </trans-unit>
        <trans-unit id="fa6955d637d20e846385cffc851f7eba38902835" translate="yes" xml:space="preserve">
          <source>Computes the log det jacobian &lt;code&gt;log |dy/dx|&lt;/code&gt; given input and output.</source>
          <target state="translated">计算log det jacobian &lt;code&gt;log |dy/dx|&lt;/code&gt; 给定输入和输出。</target>
        </trans-unit>
        <trans-unit id="5ad2395a52ab42ac3844b6eab85a745737496c2b" translate="yes" xml:space="preserve">
          <source>Computes the logarithm of the gamma function on &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 上的伽马函数的对数。</target>
        </trans-unit>
        <trans-unit id="0ab463de8fae2192880ba9572b3595b33254233d" translate="yes" xml:space="preserve">
          <source>Computes the logarithmic derivative of the gamma function on &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">计算 &lt;code&gt;input&lt;/code&gt; 上的伽马函数的对数导数。</target>
        </trans-unit>
        <trans-unit id="53d62470ba220fb7afc0e820b9e1d137675f35a7" translate="yes" xml:space="preserve">
          <source>Computes the one dimensional Fourier transform of real-valued &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">计算实值 &lt;code&gt;input&lt;/code&gt; 的一维傅立叶变换。</target>
        </trans-unit>
        <trans-unit id="df866b02f2fffb4ec82fb366d91cc209748d5c59" translate="yes" xml:space="preserve">
          <source>Computes the one dimensional discrete Fourier transform of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">计算一个维离散傅立叶变换的 &lt;code&gt;input&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e2bd9739508dd66799ff1e51b1381c47e1c1143b" translate="yes" xml:space="preserve">
          <source>Computes the one dimensional discrete Fourier transform of a Hermitian symmetric &lt;code&gt;input&lt;/code&gt; signal.</source>
          <target state="translated">计算Hermitian对称 &lt;code&gt;input&lt;/code&gt; 信号的一维离散傅里叶变换。</target>
        </trans-unit>
        <trans-unit id="1c827fe5b069aea41e126efde791986643cae21d" translate="yes" xml:space="preserve">
          <source>Computes the one dimensional inverse discrete Fourier transform of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">计算一个维离散傅立叶逆变换的 &lt;code&gt;input&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b12e2ad844db4fa1d8dc61bc76b5cc7fede08ebb" translate="yes" xml:space="preserve">
          <source>Computes the orthogonal matrix &lt;code&gt;Q&lt;/code&gt; of a QR factorization, from the &lt;code&gt;(input, input2)&lt;/code&gt; tuple returned by &lt;a href=&quot;generated/torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">计算正交矩阵 &lt;code&gt;Q&lt;/code&gt; 一QR分解的，从 &lt;code&gt;(input, input2)&lt;/code&gt; 通过返回元组&lt;a href=&quot;generated/torch.geqrf#torch.geqrf&quot;&gt; &lt;code&gt;torch.geqrf()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="9a50e0c97fa82cb476bece9bdcb5b778a06bbe5b" translate="yes" xml:space="preserve">
          <source>Computes the orthogonal matrix &lt;code&gt;Q&lt;/code&gt; of a QR factorization, from the &lt;code&gt;(input, input2)&lt;/code&gt; tuple returned by &lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">计算正交矩阵 &lt;code&gt;Q&lt;/code&gt; 一QR分解的，从 &lt;code&gt;(input, input2)&lt;/code&gt; 通过返回元组&lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt; &lt;code&gt;torch.geqrf()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c4aeca69a4a2b0bea72290f1b36d08fba25dfffc" translate="yes" xml:space="preserve">
          <source>Computes the p-norm distance between every pair of row vectors in the input. This is identical to the upper triangular portion, excluding the diagonal, of &lt;code&gt;torch.norm(input[:, None] - input, dim=2, p=p)&lt;/code&gt;. This function will be faster if the rows are contiguous.</source>
          <target state="translated">计算输入中每​​对行向量之间的p范数距离。这与 &lt;code&gt;torch.norm(input[:, None] - input, dim=2, p=p)&lt;/code&gt; 的对角线部分相同，不包括对角线。如果行是连续的，此功能将更快。</target>
        </trans-unit>
        <trans-unit id="52fb62cb631b335790a2488186d42d7739c05544" translate="yes" xml:space="preserve">
          <source>Computes the solution to the least squares and least norm problems for a full rank matrix</source>
          <target state="translated">计算全秩矩阵的最小二乘和最小正则问题的解。</target>
        </trans-unit>
        <trans-unit id="cc93b1c0376d6ee7ab14503b3b1f5781ffb751f1" translate="yes" xml:space="preserve">
          <source>Computes the sum of gradients of given tensors w.r.t. graph leaves.</source>
          <target state="translated">计算给定tensors与图叶的梯度之和。</target>
        </trans-unit>
        <trans-unit id="cf99caa72d6917de8396ddac94469751042f2b03" translate="yes" xml:space="preserve">
          <source>Computes the zeroth order modified Bessel function of the first kind for each element of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">为 &lt;code&gt;input&lt;/code&gt; 每个元素计算第一类零阶修改的Bessel函数。</target>
        </trans-unit>
        <trans-unit id="cbcbb3162b2e52eeda82ef5d679b6c19cdfaf1b5" translate="yes" xml:space="preserve">
          <source>Computing dependencies</source>
          <target state="translated">计算依赖性</target>
        </trans-unit>
        <trans-unit id="6ad79ab6353b1eee8ebbc085e10d17c4fcfb024f" translate="yes" xml:space="preserve">
          <source>Concat</source>
          <target state="translated">Concat</target>
        </trans-unit>
        <trans-unit id="4ddddf59160aed751a5f07007a03c044d9753ba8" translate="yes" xml:space="preserve">
          <source>Concatenates a sequence of tensors along a new dimension.</source>
          <target state="translated">沿着一个新的维度连接一个序列的张力。</target>
        </trans-unit>
        <trans-unit id="1b32473fe5755da0d831ffcfc10b7cdd982c9092" translate="yes" xml:space="preserve">
          <source>Concatenates the given sequence of &lt;code&gt;seq&lt;/code&gt; tensors in the given dimension.</source>
          <target state="translated">它们连接起来，给定的顺序 &lt;code&gt;seq&lt;/code&gt; 在给定尺寸张量。</target>
        </trans-unit>
        <trans-unit id="a9ec200f382eb7e2b6f899132d934e4d406c3bcb" translate="yes" xml:space="preserve">
          <source>Concatenates the given sequence of &lt;code&gt;seq&lt;/code&gt; tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</source>
          <target state="translated">它们连接起来，给定的顺序 &lt;code&gt;seq&lt;/code&gt; 在给定尺寸张量。所有张量必须具有相同的形状（在连接维中除外）或为空。</target>
        </trans-unit>
        <trans-unit id="c06ff30c05c7117f8cd03376fe398c72a9caaba3" translate="yes" xml:space="preserve">
          <source>Concrete observers should follow the same API. In forward, they will update the statistics of the observed Tensor. And they should provide a &lt;code&gt;calculate_qparams&lt;/code&gt; function that computes the quantization parameters given the collected statistics.</source>
          <target state="translated">具体的观察者应遵循相同的API。向前，他们将更新观测到的Tensor的统计信息。并且他们应该提供一个 &lt;code&gt;calculate_qparams&lt;/code&gt; 函数，该函数根据给定的统计信息来计算量化参数。</target>
        </trans-unit>
        <trans-unit id="1f8d364de32d6d9b96e9b370a225d6dab6d594c7" translate="yes" xml:space="preserve">
          <source>Concurrent calls to &lt;a href=&quot;#torch.distributed.optim.DistributedOptimizer.step&quot;&gt;&lt;code&gt;step()&lt;/code&gt;&lt;/a&gt;, either from the same or different clients, will be serialized on each worker &amp;ndash; as each worker&amp;rsquo;s optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers.</source>
          <target state="translated">来自同一客户端或不同客户端的对&lt;a href=&quot;#torch.distributed.optim.DistributedOptimizer.step&quot;&gt; &lt;code&gt;step()&lt;/code&gt; 的&lt;/a&gt;并发调用将在每个工作程序上序列化-因为每个工作程序的优化器一次只能处理一组渐变。但是，不能保证完整的前向后向优化程序序列将一次为一个客户端执行。这意味着所应用的渐变可能不对应于在给定工人上执行的最新前向通过。此外，也不能保证在所有工作人员之间都可以订购。</target>
        </trans-unit>
        <trans-unit id="30e59e381077a379cb9607bde3a4a42eb3f45ab9" translate="yes" xml:space="preserve">
          <source>Consider a batched &lt;code&gt;input&lt;/code&gt; tensor containing sliding local blocks, e.g., patches of images, of shape</source>
          <target state="translated">考虑包含滑动局部块（例如形状的图像块）的批处理 &lt;code&gt;input&lt;/code&gt; 张量</target>
        </trans-unit>
        <trans-unit id="7f6c99c148a673c2e06016f795e6fb113b6d25b3" translate="yes" xml:space="preserve">
          <source>Consider a batched &lt;code&gt;input&lt;/code&gt; tensor of shape</source>
          <target state="translated">考虑形状的批量 &lt;code&gt;input&lt;/code&gt; 张量</target>
        </trans-unit>
        <trans-unit id="41c82c00063d59cec7ac15ce301400bc6cba5a1e" translate="yes" xml:space="preserve">
          <source>Considering the specific case of Momentum, the update can be written as</source>
          <target state="translated">考虑到动量的特殊情况,更新可以写为</target>
        </trans-unit>
        <trans-unit id="a3d8f578f82ef706b4638b32cc8ac9d832c7f7c7" translate="yes" xml:space="preserve">
          <source>ConstantPad1d</source>
          <target state="translated">ConstantPad1d</target>
        </trans-unit>
        <trans-unit id="998c295145e82549d2f17c1a6ba6c23bef09837b" translate="yes" xml:space="preserve">
          <source>ConstantPad2d</source>
          <target state="translated">ConstantPad2d</target>
        </trans-unit>
        <trans-unit id="4a15d68828e68d36dcfe86ffe64b4e4f826c7f8a" translate="yes" xml:space="preserve">
          <source>ConstantPad3d</source>
          <target state="translated">ConstantPad3d</target>
        </trans-unit>
        <trans-unit id="0a41b38808acdf43af00c5eb932dd87575946224" translate="yes" xml:space="preserve">
          <source>ConstantPadNd</source>
          <target state="translated">ConstantPadNd</target>
        </trans-unit>
        <trans-unit id="0f386d7e7881b32fa39cb7b62bdb15c0f3a4c0e1" translate="yes" xml:space="preserve">
          <source>Constants</source>
          <target state="translated">Constants</target>
        </trans-unit>
        <trans-unit id="cba7185d08d214544898ab239c21fb5414c4fc69" translate="yes" xml:space="preserve">
          <source>Constants can be marked with a &lt;code&gt;Final&lt;/code&gt; class annotation instead of adding the name of the member to &lt;code&gt;__constants__&lt;/code&gt;.</source>
          <target state="translated">可以用 &lt;code&gt;Final&lt;/code&gt; 类注解标记常量，而不是将成员的名称添加到 &lt;code&gt;__constants__&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="3e93ee8b7e4549e7e136ce0f0147ddef90f905dc" translate="yes" xml:space="preserve">
          <source>Construct 18 layer Resnet3D model as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</source>
          <target state="translated">如&lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;那样构建18层Resnet3D模型</target>
        </trans-unit>
        <trans-unit id="58c6f5a95316e94c0311dfa74a05b94384191ff6" translate="yes" xml:space="preserve">
          <source>Constructing averaged models</source>
          <target state="translated">构建平均模型</target>
        </trans-unit>
        <trans-unit id="33ba182b915c42fbc2d5adcf6ef39b2978c20cad" translate="yes" xml:space="preserve">
          <source>Constructing it</source>
          <target state="translated">构造</target>
        </trans-unit>
        <trans-unit id="14faa516e57e5e69781808c4221cf6c62dc83c10" translate="yes" xml:space="preserve">
          <source>Constructor for 18 layer Mixed Convolution network as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248中的&lt;/a&gt;18层混合卷积网络的构造函数</target>
        </trans-unit>
        <trans-unit id="243abd8ffcbf23837c06014609b82a14c78192f1" translate="yes" xml:space="preserve">
          <source>Constructor for the 18 layer deep R(2+1)D network as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248中&lt;/a&gt;的18层深度R（2 + 1）D网络的构造函数</target>
        </trans-unit>
        <trans-unit id="25ef738c9d86e2fc8c4436c90a8dc11de37e7492" translate="yes" xml:space="preserve">
          <source>Constructor, forward method, and differentiation of the output (or a function of the output of this module) are distributed synchronization points. Take that into account in case different processes might be executing different code.</source>
          <target state="translated">构造函数、前向方法、输出(或本模块输出的函数)的分化是分布式同步点。要考虑到这一点,以防不同进程可能执行不同的代码。</target>
        </trans-unit>
        <trans-unit id="e3591588e6daf80e91783781c9de4873426a6c50" translate="yes" xml:space="preserve">
          <source>Constructs a DeepLabV3 model with a ResNet-101 backbone.</source>
          <target state="translated">以ResNet-101为骨干构建DeepLabV3模型。</target>
        </trans-unit>
        <trans-unit id="0b9d4f7f98afc347ec7829b73a659d20e47f4656" translate="yes" xml:space="preserve">
          <source>Constructs a DeepLabV3 model with a ResNet-50 backbone.</source>
          <target state="translated">以ResNet-50为骨干构建DeepLabV3模型。</target>
        </trans-unit>
        <trans-unit id="a8efeef3cc601befcf295dbb51e707b10b91e402" translate="yes" xml:space="preserve">
          <source>Constructs a Faster R-CNN model with a ResNet-50-FPN backbone.</source>
          <target state="translated">构建一个以ResNet-50-FPN为骨干的Faster R-CNN模型。</target>
        </trans-unit>
        <trans-unit id="63a7411b909e98e9e117a9221e16319404fd9e0b" translate="yes" xml:space="preserve">
          <source>Constructs a Fully-Convolutional Network model with a ResNet-101 backbone.</source>
          <target state="translated">以ResNet-101为骨干构建全卷积网络模型。</target>
        </trans-unit>
        <trans-unit id="40a8ca8c7bf720c8a0f63021a990b502e7b406b0" translate="yes" xml:space="preserve">
          <source>Constructs a Fully-Convolutional Network model with a ResNet-50 backbone.</source>
          <target state="translated">以ResNet-50为骨干构建全卷积网络模型。</target>
        </trans-unit>
        <trans-unit id="c44ff1a6ea4f78ec6e55eb5a4dd0b0381511fd66" translate="yes" xml:space="preserve">
          <source>Constructs a Keypoint R-CNN model with a ResNet-50-FPN backbone.</source>
          <target state="translated">以ResNet-50-FPN为骨干构建一个Keypoint R-CNN模型。</target>
        </trans-unit>
        <trans-unit id="0bac8f59a0445e0c89aaa35b2ecaf7957f6cbc76" translate="yes" xml:space="preserve">
          <source>Constructs a Mask R-CNN model with a ResNet-50-FPN backbone.</source>
          <target state="translated">构建一个以ResNet-50-FPN为骨架的Mask R-CNN模型。</target>
        </trans-unit>
        <trans-unit id="ed68461c0a66e54bb8577296b733b09ee8628e1f" translate="yes" xml:space="preserve">
          <source>Constructs a MobileNetV2 architecture from &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;&amp;ldquo;MobileNetV2: Inverted Residuals and Linear Bottlenecks&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">从&lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;&amp;ldquo; MobileNetV2：残差和线性瓶颈&amp;rdquo;&lt;/a&gt;构建MobileNetV2体系结构。</target>
        </trans-unit>
        <trans-unit id="fa58d20a53a578ca582a51e6082babe2633522d9" translate="yes" xml:space="preserve">
          <source>Constructs a RetinaNet model with a ResNet-50-FPN backbone.</source>
          <target state="translated">以ResNet-50-FPN为骨架构建RetinaNet模型。</target>
        </trans-unit>
        <trans-unit id="c060b450d2f139169d01201742e2de6bd6813c33" translate="yes" xml:space="preserve">
          <source>Constructs a ShuffleNetV2 with 0.5x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">构造具有0.5x输出通道的ShuffleNetV2，如&lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo; ShuffleNet V2：高效CNN架构设计实用指南&amp;rdquo;中所述&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c1b1411f7ad7f2a9785e3f797d1ed1dca55c0522" translate="yes" xml:space="preserve">
          <source>Constructs a ShuffleNetV2 with 1.0x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">构造具有1.0x输出通道的ShuffleNetV2，如&lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo; ShuffleNet V2：高效CNN架构设计实用指南&amp;rdquo;中所述&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="946788267a37fd704ae9ab9be9bb919471421254" translate="yes" xml:space="preserve">
          <source>Constructs a ShuffleNetV2 with 1.5x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">构造具有1.5x输出通道的ShuffleNetV2，如&lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo; ShuffleNet V2：高效CNN架构设计实用指南&amp;rdquo;中所述&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c3079515cd84378d124e9d68610779cdd6cd48ea" translate="yes" xml:space="preserve">
          <source>Constructs a ShuffleNetV2 with 2.0x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">构造具有2.0x输出通道的ShuffleNetV2，如&lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo; ShuffleNet V2：高效CNN架构设计实用指南&amp;rdquo;中所述&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="17f894712d4813d2bb2bfed6f1251b3bc7ad7f28" translate="yes" xml:space="preserve">
          <source>Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt;&lt;code&gt;abs&lt;/code&gt;&lt;/a&gt; and angle &lt;a href=&quot;generated/torch.angle#torch.angle&quot;&gt;&lt;code&gt;angle&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">构造一个复数张量，其元素为与绝对值&lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt; &lt;code&gt;abs&lt;/code&gt; &lt;/a&gt;和角度&lt;a href=&quot;generated/torch.angle#torch.angle&quot;&gt; &lt;code&gt;angle&lt;/code&gt; &lt;/a&gt;的极坐标相对应的笛卡尔坐标。</target>
        </trans-unit>
        <trans-unit id="21791c63e7cf531ba91fa49b8e8790ebd26c8e28" translate="yes" xml:space="preserve">
          <source>Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value &lt;a href=&quot;torch.abs#torch.abs&quot;&gt;&lt;code&gt;abs&lt;/code&gt;&lt;/a&gt; and angle &lt;a href=&quot;torch.angle#torch.angle&quot;&gt;&lt;code&gt;angle&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">构造一个复数张量，其元素为与绝对值&lt;a href=&quot;torch.abs#torch.abs&quot;&gt; &lt;code&gt;abs&lt;/code&gt; &lt;/a&gt;和角度&lt;a href=&quot;torch.angle#torch.angle&quot;&gt; &lt;code&gt;angle&lt;/code&gt; &lt;/a&gt;的极坐标相对应的笛卡尔坐标。</target>
        </trans-unit>
        <trans-unit id="3f22df20de166bd700f665def840237abc20c0b1" translate="yes" xml:space="preserve">
          <source>Constructs a complex tensor with its real part equal to &lt;a href=&quot;generated/torch.real#torch.real&quot;&gt;&lt;code&gt;real&lt;/code&gt;&lt;/a&gt; and its imaginary part equal to &lt;a href=&quot;generated/torch.imag#torch.imag&quot;&gt;&lt;code&gt;imag&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">构造一个复数张量，其实部等于&lt;a href=&quot;generated/torch.real#torch.real&quot;&gt; &lt;code&gt;real&lt;/code&gt; &lt;/a&gt;，虚部等于&lt;a href=&quot;generated/torch.imag#torch.imag&quot;&gt; &lt;code&gt;imag&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="7cbc2c2e6f7677b79f4e49073177abc0eaf5c360" translate="yes" xml:space="preserve">
          <source>Constructs a complex tensor with its real part equal to &lt;a href=&quot;torch.real#torch.real&quot;&gt;&lt;code&gt;real&lt;/code&gt;&lt;/a&gt; and its imaginary part equal to &lt;a href=&quot;torch.imag#torch.imag&quot;&gt;&lt;code&gt;imag&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">构造一个复数张量，其实部等于&lt;a href=&quot;torch.real#torch.real&quot;&gt; &lt;code&gt;real&lt;/code&gt; &lt;/a&gt;，虚部等于&lt;a href=&quot;torch.imag#torch.imag&quot;&gt; &lt;code&gt;imag&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="f93f080d3b4b5a3e3dc0939299e73f9681c6cfa7" translate="yes" xml:space="preserve">
          <source>Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given &lt;code&gt;indices&lt;/code&gt; with the given &lt;code&gt;values&lt;/code&gt;.</source>
          <target state="translated">在给定 &lt;code&gt;indices&lt;/code&gt; 使用给定 &lt;code&gt;values&lt;/code&gt; 以非零元素构造COO（rdinate）格式的稀疏张量。</target>
        </trans-unit>
        <trans-unit id="be72a7a131ec1a761dd654077e838e6f0e7ea737" translate="yes" xml:space="preserve">
          <source>Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given &lt;code&gt;indices&lt;/code&gt; with the given &lt;code&gt;values&lt;/code&gt;. A sparse tensor can be &lt;code&gt;uncoalesced&lt;/code&gt;, in that case, there are duplicate coordinates in the indices, and the value at that index is the sum of all duplicate value entries: &lt;a href=&quot;https://pytorch.org/docs/stable/sparse.html&quot;&gt;torch.sparse&lt;/a&gt;.</source>
          <target state="translated">在给定 &lt;code&gt;indices&lt;/code&gt; 使用给定 &lt;code&gt;values&lt;/code&gt; 以非零元素构造COO（rdinate）格式的稀疏张量。稀疏张量可以是 &lt;code&gt;uncoalesced&lt;/code&gt; ，在这种情况下，索引中存在重复的坐标，并且该索引处的值是所有重复值条目的总和：&lt;a href=&quot;https://pytorch.org/docs/stable/sparse.html&quot;&gt;torch.sparse&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0e69f365d19e40e598e945db7b7eaf81602d0597" translate="yes" xml:space="preserve">
          <source>Constructs a tensor with &lt;code&gt;data&lt;/code&gt;.</source>
          <target state="translated">使用 &lt;code&gt;data&lt;/code&gt; 构造张量。</target>
        </trans-unit>
        <trans-unit id="26d1a380017a1ee5db1ae9f2be051aa31298d8a4" translate="yes" xml:space="preserve">
          <source>Container holding a sequence of pruning methods for iterative pruning.</source>
          <target state="translated">容器中存放着用于迭代修剪的修剪方法序列。</target>
        </trans-unit>
        <trans-unit id="ecedca4e6711cc4546829104ac643201429f438a" translate="yes" xml:space="preserve">
          <source>Container holding a sequence of pruning methods for iterative pruning. Keeps track of the order in which pruning methods are applied and handles combining successive pruning calls.</source>
          <target state="translated">为迭代修剪而持有修剪方法序列的容器。跟踪应用修剪方法的顺序,并处理连续修剪调用的组合。</target>
        </trans-unit>
        <trans-unit id="e040a458f46532a90ec69fa0b4bfc33ba151c98b" translate="yes" xml:space="preserve">
          <source>Containers</source>
          <target state="translated">Containers</target>
        </trans-unit>
        <trans-unit id="a5f7ef3dcfb494670b1f12c053712004a30a5030" translate="yes" xml:space="preserve">
          <source>Containers are assumed to have type &lt;code&gt;Tensor&lt;/code&gt; and be non-optional (see &lt;code&gt;Default Types&lt;/code&gt; for more information). Previously, &lt;code&gt;torch.jit.annotate&lt;/code&gt; was used to tell the TorchScript compiler what the type should be. Python 3 style type hints are now supported.</source>
          <target state="translated">假定容器具有 &lt;code&gt;Tensor&lt;/code&gt; 类型，并且是非可选的（有关更多信息，请参见 &lt;code&gt;Default Types&lt;/code&gt; ）。以前， &lt;code&gt;torch.jit.annotate&lt;/code&gt; 用来告诉TorchScript编译器类型是什么。现在支持Python 3样式类型提示。</target>
        </trans-unit>
        <trans-unit id="c965d220340a70f3a925cd031cea31e23574727b" translate="yes" xml:space="preserve">
          <source>Context manager that makes every autograd operation emit an NVTX range.</source>
          <target state="translated">上下文管理器,使每一个autograd操作发出一个NVTX范围。</target>
        </trans-unit>
        <trans-unit id="67a3ce3e7cefceac3d4c664550d90fc678be467d" translate="yes" xml:space="preserve">
          <source>Context manager that manages autograd profiler state and holds a summary of results. Under the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. Note: profiler is thread local and is automatically propagated into the async tasks</source>
          <target state="translated">上下文管理器管理autograd profiler的状态并保存结果摘要。从表面上看,它只是记录在 C++中执行的函数事件,并将这些事件暴露给 Python。您可以将任何代码包入其中,而且它只会报告 PyTorch 函数的运行时间。注意:剖析器是线程本地的,会自动传播到异步任务中。</target>
        </trans-unit>
        <trans-unit id="1b40e97ec64197b261ed2c352a763baa4c6268be" translate="yes" xml:space="preserve">
          <source>Context method mixins</source>
          <target state="translated">上下文方法混搭</target>
        </trans-unit>
        <trans-unit id="69cef769a134b0d4a56c19cba61f8a8ed316d60c" translate="yes" xml:space="preserve">
          <source>Context object to wrap forward and backward passes when using distributed autograd. The &lt;code&gt;context_id&lt;/code&gt; generated in the &lt;code&gt;with&lt;/code&gt; statement is required to uniquely identify a distributed backward pass on all workers. Each worker stores metadata associated with this &lt;code&gt;context_id&lt;/code&gt;, which is required to correctly execute a distributed autograd pass.</source>
          <target state="translated">使用分布式autograd时要包装前进和后退传递的上下文对象。在 &lt;code&gt;with&lt;/code&gt; 语句中生成的 &lt;code&gt;context_id&lt;/code&gt; 是唯一标识所有工作程序上的分布式后向传递所必需的。每个工作程序都存储与此 &lt;code&gt;context_id&lt;/code&gt; 相关联的元数据，这是正确执行分布式autograd pass所必需的。</target>
        </trans-unit>
        <trans-unit id="34579070906cfe3afb9639eadd970c899029c8ca" translate="yes" xml:space="preserve">
          <source>Context-manager that changes the current device to that of given object.</source>
          <target state="translated">将当前设备改为给定对象的上下文管理器。</target>
        </trans-unit>
        <trans-unit id="4802d234013bb6f1339b5d9be21cf85859570968" translate="yes" xml:space="preserve">
          <source>Context-manager that changes the selected device.</source>
          <target state="translated">改变所选设备的上下文管理器。</target>
        </trans-unit>
        <trans-unit id="05fa9ad5bb0946657dbbab246393aea9c7883c54" translate="yes" xml:space="preserve">
          <source>Context-manager that disabled gradient calculation.</source>
          <target state="translated">禁用梯度计算的上下文管理器。</target>
        </trans-unit>
        <trans-unit id="10cdc0b206fae92706049769655993622c2c5fd2" translate="yes" xml:space="preserve">
          <source>Context-manager that enable anomaly detection for the autograd engine.</source>
          <target state="translated">Context-manager,使autograd引擎的异常检测成为可能。</target>
        </trans-unit>
        <trans-unit id="98ceadd98235bb51cb8307cf83dd7c6d79e59720" translate="yes" xml:space="preserve">
          <source>Context-manager that enables gradient calculation.</source>
          <target state="translated">可以进行梯度计算的上下文管理器。</target>
        </trans-unit>
        <trans-unit id="da467a4306f8a33fc27d4f7cbea22fc9876c9e5b" translate="yes" xml:space="preserve">
          <source>Context-manager that selects a given stream.</source>
          <target state="translated">选择给定流的上下文管理器。</target>
        </trans-unit>
        <trans-unit id="8479c988b868dfa3b4c887d081e83a92acf0bb43" translate="yes" xml:space="preserve">
          <source>Context-manager that sets gradient calculation to on or off.</source>
          <target state="translated">设置梯度计算开启或关闭的上下文管理器。</target>
        </trans-unit>
        <trans-unit id="dc43ee1ff31732f8dd825fad1558c5ab44a47fa2" translate="yes" xml:space="preserve">
          <source>Context-manager that sets the anomaly detection for the autograd engine on or off.</source>
          <target state="translated">上下文管理器,设置自动识别引擎的异常检测开启或关闭。</target>
        </trans-unit>
        <trans-unit id="2fee61d743d1ceafef226afc45bfff28d600d1ec" translate="yes" xml:space="preserve">
          <source>ContinuousBernoulli</source>
          <target state="translated">ContinuousBernoulli</target>
        </trans-unit>
        <trans-unit id="4fbe3836d5db9ff249ce993c595d3f0f979273c3" translate="yes" xml:space="preserve">
          <source>Conv</source>
          <target state="translated">Conv</target>
        </trans-unit>
        <trans-unit id="0f579280c2328a913d6d725180603d671f003e1c" translate="yes" xml:space="preserve">
          <source>Conv1d</source>
          <target state="translated">Conv1d</target>
        </trans-unit>
        <trans-unit id="40b3c3b8c860add6637a732716bb59fe0a472372" translate="yes" xml:space="preserve">
          <source>Conv2d</source>
          <target state="translated">Conv2d</target>
        </trans-unit>
        <trans-unit id="11775e3bcd7c158060f3c37b634846211f59b0a8" translate="yes" xml:space="preserve">
          <source>Conv3d</source>
          <target state="translated">Conv3d</target>
        </trans-unit>
        <trans-unit id="7bd21924a004fee82a9b59ee3da28ba6cf10e0cd" translate="yes" xml:space="preserve">
          <source>ConvBn1d</source>
          <target state="translated">ConvBn1d</target>
        </trans-unit>
        <trans-unit id="b49eb764a64698e64c0e874c6a40f0bd980f5854" translate="yes" xml:space="preserve">
          <source>ConvBn2d</source>
          <target state="translated">ConvBn2d</target>
        </trans-unit>
        <trans-unit id="f2a4cf1ba0285a54525c0f0bced1d81a716af040" translate="yes" xml:space="preserve">
          <source>ConvBnReLU1d</source>
          <target state="translated">ConvBnReLU1d</target>
        </trans-unit>
        <trans-unit id="9a62f54f222bf04d2c6e425b49c602b198a2f54e" translate="yes" xml:space="preserve">
          <source>ConvBnReLU2d</source>
          <target state="translated">ConvBnReLU2d</target>
        </trans-unit>
        <trans-unit id="02be55e7a33c6df1aae2e7f342fb637e9ac77c6b" translate="yes" xml:space="preserve">
          <source>ConvReLU1d</source>
          <target state="translated">ConvReLU1d</target>
        </trans-unit>
        <trans-unit id="ee43a211652ce7b140d6ac846ff3539f57bfe4ce" translate="yes" xml:space="preserve">
          <source>ConvReLU2d</source>
          <target state="translated">ConvReLU2d</target>
        </trans-unit>
        <trans-unit id="9c9f7992fc4e6c43d3ef3ba68725dbe4ae51d5e7" translate="yes" xml:space="preserve">
          <source>ConvReLU3d</source>
          <target state="translated">ConvReLU3d</target>
        </trans-unit>
        <trans-unit id="afca9fffa388b0f4407644f89fc3d2572519c33f" translate="yes" xml:space="preserve">
          <source>ConvTranspose1d</source>
          <target state="translated">ConvTranspose1d</target>
        </trans-unit>
        <trans-unit id="fc42d54df990ea8e2fd96576309fbe506686556e" translate="yes" xml:space="preserve">
          <source>ConvTranspose2d</source>
          <target state="translated">ConvTranspose2d</target>
        </trans-unit>
        <trans-unit id="aa70f25fef0f0928f03585abe233d18e1b7a43bf" translate="yes" xml:space="preserve">
          <source>ConvTranspose3d</source>
          <target state="translated">ConvTranspose3d</target>
        </trans-unit>
        <trans-unit id="bf6b6d744534af31ad6085d41b063c70edf02466" translate="yes" xml:space="preserve">
          <source>Convenience method that creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; with the bare minimum (but often sufficient) arguments to build a C++ extension.</source>
          <target state="translated">创建 &lt;code&gt;setuptools.Extension&lt;/code&gt; 的便捷方法，使用最少（但通常足够）的参数来构建C ++扩展。</target>
        </trans-unit>
        <trans-unit id="6e35177ef47547c230ba4dcc90eda6d927bb9e5a" translate="yes" xml:space="preserve">
          <source>Convenience method that creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library.</source>
          <target state="translated">创建 &lt;code&gt;setuptools.Extension&lt;/code&gt; 的便捷方法，使用最少的（但通常是足够的）参数来构建CUDA / C ++扩展。这包括CUDA包含路径，库路径和运行时库。</target>
        </trans-unit>
        <trans-unit id="5a0770a286742ba2629162d473ed7de8b93bc405" translate="yes" xml:space="preserve">
          <source>Convert one vector to the parameters</source>
          <target state="translated">将一个向量转换为参数</target>
        </trans-unit>
        <trans-unit id="be41e2cca6d10c1b2760ba9429806df5479848bc" translate="yes" xml:space="preserve">
          <source>Convert parameters to one vector</source>
          <target state="translated">将参数转换为一个向量</target>
        </trans-unit>
        <trans-unit id="01733ec2af89cb05f3440f813cca55cd11cb2b18" translate="yes" xml:space="preserve">
          <source>Convert the data into a &lt;code&gt;torch.Tensor&lt;/code&gt;.</source>
          <target state="translated">将数据转换为 &lt;code&gt;torch.Tensor&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="6906a54aac4ab7e793a9985a2b6c90ca5f574d9a" translate="yes" xml:space="preserve">
          <source>Convert the data into a &lt;code&gt;torch.Tensor&lt;/code&gt;. If the data is already a &lt;code&gt;Tensor&lt;/code&gt; with the same &lt;code&gt;dtype&lt;/code&gt; and &lt;code&gt;device&lt;/code&gt;, no copy will be performed, otherwise a new &lt;code&gt;Tensor&lt;/code&gt; will be returned with computational graph retained if data &lt;code&gt;Tensor&lt;/code&gt; has &lt;code&gt;requires_grad=True&lt;/code&gt;. Similarly, if the data is an &lt;code&gt;ndarray&lt;/code&gt; of the corresponding &lt;code&gt;dtype&lt;/code&gt; and the &lt;code&gt;device&lt;/code&gt; is the cpu, no copy will be performed.</source>
          <target state="translated">将数据转换为 &lt;code&gt;torch.Tensor&lt;/code&gt; 。如果数据已经是具有相同 &lt;code&gt;dtype&lt;/code&gt; 和 &lt;code&gt;device&lt;/code&gt; 的 &lt;code&gt;Tensor&lt;/code&gt; ，则不会执行任何复制，否则，如果数据 &lt;code&gt;Tensor&lt;/code&gt; 具有 &lt;code&gt;requires_grad=True&lt;/code&gt; 则将返回保留计算图的新 &lt;code&gt;Tensor&lt;/code&gt; 。类似地，如果数据是一个 &lt;code&gt;ndarray&lt;/code&gt; 相应的 &lt;code&gt;dtype&lt;/code&gt; 和 &lt;code&gt;device&lt;/code&gt; 是CPU，没有复制将被执行。</target>
        </trans-unit>
        <trans-unit id="294b48683ec8d036ef223113fe45f4c4409c00f1" translate="yes" xml:space="preserve">
          <source>Converts a float model to dynamic (i.e. weights-only) quantized model.</source>
          <target state="translated">将浮动模型转换为动态(即仅有权重)量化模型。</target>
        </trans-unit>
        <trans-unit id="5490b3bea11fc41ac534c3f667531da495228f6a" translate="yes" xml:space="preserve">
          <source>Converts a float tensor to a per-channel quantized tensor with given scales and zero points.</source>
          <target state="translated">将一个浮动张量转换为一个给定比例和零点的每通道量化张量。</target>
        </trans-unit>
        <trans-unit id="a8473a264e544b1bee1239b9dbf9ad1a93b701b5" translate="yes" xml:space="preserve">
          <source>Converts a float tensor to a quantized tensor with given scale and zero point.</source>
          <target state="translated">将浮动张量转换为给定比例和零点的量化张量。</target>
        </trans-unit>
        <trans-unit id="7e8ec30d2295e5cc0c009208c53acb2a3f05bb56" translate="yes" xml:space="preserve">
          <source>Converts submodules in input module to a different module according to &lt;code&gt;mapping&lt;/code&gt; by calling &lt;code&gt;from_float&lt;/code&gt; method on the target module class. And remove qconfig at the end if remove_qconfig is set to True.</source>
          <target state="translated">通过在目标模块类上调用 &lt;code&gt;from_float&lt;/code&gt; 方法，根据 &lt;code&gt;mapping&lt;/code&gt; 将输入模块中的子模块转换为其他模块。如果remove_qconfig设置为True，则最后删除qconfig。</target>
        </trans-unit>
        <trans-unit id="7757dbf24e068e638ad997ae9c4d2e07adceb8c8" translate="yes" xml:space="preserve">
          <source>Convolution Layers</source>
          <target state="translated">卷积层</target>
        </trans-unit>
        <trans-unit id="be084db8daa3740ea18a2c2e59749d4c0010cf94" translate="yes" xml:space="preserve">
          <source>Convolution functions</source>
          <target state="translated">卷积函数</target>
        </trans-unit>
        <trans-unit id="989f24495d4e5092a0b9ea78596dc97ffbe13f9f" translate="yes" xml:space="preserve">
          <source>Conv{1,2,3}D</source>
          <target state="translated">Conv{1,2,3}D</target>
        </trans-unit>
        <trans-unit id="1737f5913c8956ac817ace6ab2658a389afc44d8" translate="yes" xml:space="preserve">
          <source>Copies elements from &lt;code&gt;source&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; tensor at positions where the &lt;code&gt;mask&lt;/code&gt; is True. The shape of &lt;code&gt;mask&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the shape of the underlying tensor. The &lt;code&gt;source&lt;/code&gt; should have at least as many elements as the number of ones in &lt;code&gt;mask&lt;/code&gt;</source>
          <target state="translated">在 &lt;code&gt;mask&lt;/code&gt; 为True的位置将元素从 &lt;code&gt;source&lt;/code&gt; 复制到 &lt;code&gt;self&lt;/code&gt; 张量。 &lt;code&gt;mask&lt;/code&gt; 的形状必须与基础张量的形状一起&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;广播&lt;/a&gt;。的 &lt;code&gt;source&lt;/code&gt; 应当具有至少一样多的元素的人在数量 &lt;code&gt;mask&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="0ffea1905ca53aaa0c2e1dd04c44cfd1d1d610e9" translate="yes" xml:space="preserve">
          <source>Copies parameters and buffers from &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the keys of &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">将参数和缓冲区从&lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;复制到此模块及其后代中。如果 &lt;code&gt;strict&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则&lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;的键必须与该模块的&lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt;函数返回的键完全匹配。</target>
        </trans-unit>
        <trans-unit id="b20573e90f9dd7fefb1d1ba5fccbebf327c4350e" translate="yes" xml:space="preserve">
          <source>Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the keys of &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">将参数和缓冲区从&lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;复制到此模块及其后代中。如果 &lt;code&gt;strict&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则&lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;的键必须与该模块的&lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt;函数返回的键完全匹配。</target>
        </trans-unit>
        <trans-unit id="2aa7f8afed131bd721ebda279fdf8b30aaa83be9" translate="yes" xml:space="preserve">
          <source>Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the keys of &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">将参数和缓冲区从&lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;复制到此模块及其后代中。如果 &lt;code&gt;strict&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则&lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;的键必须与该模块的&lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt;函数返回的键完全匹配。</target>
        </trans-unit>
        <trans-unit id="3fbe05d7c19e34a099d5b27aecd7e50cc995055f" translate="yes" xml:space="preserve">
          <source>Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the keys of &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">将参数和缓冲区从&lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;复制到此模块及其后代中。如果 &lt;code&gt;strict&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则&lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;的键必须与该模块的&lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt;函数返回的键完全匹配。</target>
        </trans-unit>
        <trans-unit id="c58511363c704a11c0530ab36e516cead0a26399" translate="yes" xml:space="preserve">
          <source>Copies the elements from &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; into the positions specified by indices. For the purpose of indexing, the &lt;code&gt;self&lt;/code&gt; tensor is treated as if it were a 1-D tensor.</source>
          <target state="translated">将元素从&lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt;复制到索引指定的位置。出于索引的目的，将 &lt;code&gt;self&lt;/code&gt; 张量视为一维张量。</target>
        </trans-unit>
        <trans-unit id="31514897cedc5e7c028b472351d6c9c6b226eacf" translate="yes" xml:space="preserve">
          <source>Copies the elements from &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; tensor and returns &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">将 &lt;code&gt;src&lt;/code&gt; 中的元素复制到 &lt;code&gt;self&lt;/code&gt; 张量中，并返回 &lt;code&gt;self&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b302a58928c86985fd5405fb9b12e6f3b385a56d" translate="yes" xml:space="preserve">
          <source>Copies the elements of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; into the &lt;code&gt;self&lt;/code&gt; tensor by selecting the indices in the order given in &lt;code&gt;index&lt;/code&gt;. For example, if &lt;code&gt;dim == 0&lt;/code&gt; and &lt;code&gt;index[i] == j&lt;/code&gt;, then the &lt;code&gt;i&lt;/code&gt;th row of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; is copied to the &lt;code&gt;j&lt;/code&gt;th row of &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">拷贝的元件&lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt;到 &lt;code&gt;self&lt;/code&gt; 通过选择在给定的顺序的索引张量 &lt;code&gt;index&lt;/code&gt; 。例如，如果 &lt;code&gt;dim == 0&lt;/code&gt; 且 &lt;code&gt;index[i] == j&lt;/code&gt; ，则将&lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt;第 &lt;code&gt;i&lt;/code&gt; 行复制到 &lt;code&gt;self&lt;/code&gt; 的第 &lt;code&gt;j&lt;/code&gt; 行。</target>
        </trans-unit>
        <trans-unit id="b91c805903c6cb0b3510a18d72ca193bc741d244" translate="yes" xml:space="preserve">
          <source>Copies the storage to pinned memory, if it&amp;rsquo;s not already pinned.</source>
          <target state="translated">如果尚未固定存储，则将其复制到固定的内存中。</target>
        </trans-unit>
        <trans-unit id="278e4ebde33e7e777180bcca75800abdc23470b8" translate="yes" xml:space="preserve">
          <source>Copies the tensor to pinned memory, if it&amp;rsquo;s not already pinned.</source>
          <target state="translated">将张量复制到固定的内存（如果尚未固定）。</target>
        </trans-unit>
        <trans-unit id="361dead9429b7e599e4930ab20d7e7621cc46eae" translate="yes" xml:space="preserve">
          <source>Core statistics:</source>
          <target state="translated">核心统计数据:</target>
        </trans-unit>
        <trans-unit id="bbf794aba20b276c8548169896a15590fe109e14" translate="yes" xml:space="preserve">
          <source>CosineEmbeddingLoss</source>
          <target state="translated">CosineEmbeddingLoss</target>
        </trans-unit>
        <trans-unit id="38519b5b3e654f4e1b794c35fd7670bb326f9738" translate="yes" xml:space="preserve">
          <source>CosineSimilarity</source>
          <target state="translated">CosineSimilarity</target>
        </trans-unit>
        <trans-unit id="09bf9d0f08c3cee1a60852d335fb4cf2f77d1281" translate="yes" xml:space="preserve">
          <source>Count the frequency of each value in an array of non-negative ints.</source>
          <target state="translated">计算非负数组中每个值的频率。</target>
        </trans-unit>
        <trans-unit id="fa674450dd813dec8a084d834fc97c369ca9a763" translate="yes" xml:space="preserve">
          <source>Counts the number of non-zero values in the tensor &lt;code&gt;input&lt;/code&gt; along the given &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">计算沿给定 &lt;code&gt;dim&lt;/code&gt; 的张量 &lt;code&gt;input&lt;/code&gt; 非零值的数量。</target>
        </trans-unit>
        <trans-unit id="e0da99e01ab08d505f8e3aacba31f722f60ab5af" translate="yes" xml:space="preserve">
          <source>Counts the number of non-zero values in the tensor &lt;code&gt;input&lt;/code&gt; along the given &lt;code&gt;dim&lt;/code&gt;. If no dim is specified then all non-zeros in the tensor are counted.</source>
          <target state="translated">计算沿给定 &lt;code&gt;dim&lt;/code&gt; 的张量 &lt;code&gt;input&lt;/code&gt; 非零值的数量。如果未指定任何暗淡，则将张量中的所有非零计数。</target>
        </trans-unit>
        <trans-unit id="8bd108ea71b4b5faec418d13c1eaaf112af7d5fc" translate="yes" xml:space="preserve">
          <source>Create a block diagonal matrix from provided tensors.</source>
          <target state="translated">从提供的张力中创建一个块状对角矩阵。</target>
        </trans-unit>
        <trans-unit id="c794a7007c1ced08b83ca3f0fb4e8db7a3266ce0" translate="yes" xml:space="preserve">
          <source>Create a dynamic quantized module from a float module or qparams_dict</source>
          <target state="translated">从float模块或qparams_dict创建一个动态量化模块。</target>
        </trans-unit>
        <trans-unit id="661a797fdd966423deb47c56a841ce6d565626a3" translate="yes" xml:space="preserve">
          <source>Create a helper proxy to easily launch a &lt;code&gt;remote&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.remote().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</source>
          <target state="translated">创建一个帮助代理，以使用RRef的所有者作为目标来轻松启动 &lt;code&gt;remote&lt;/code&gt; 以在该RRef引用的对象上运行功能。更具体地说， &lt;code&gt;rref.remote().func_name(*args, **kwargs)&lt;/code&gt; 与以下内容相同：</target>
        </trans-unit>
        <trans-unit id="b873bdb21fd500fbd503a0fa2ae31f161f0a0486" translate="yes" xml:space="preserve">
          <source>Create a helper proxy to easily launch an &lt;code&gt;rpc_async&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.rpc_async().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</source>
          <target state="translated">创建一个帮助代理，以使用 &lt;code&gt;rpc_async&lt;/code&gt; 的所有者作为目标来轻松启动rpc_async，以在该RRef引用的对象上运行功能。更具体地说， &lt;code&gt;rref.rpc_async().func_name(*args, **kwargs)&lt;/code&gt; 与以下内容相同：</target>
        </trans-unit>
        <trans-unit id="8b2c6a8315871636ff748acb6e7ca0ce33cac24a" translate="yes" xml:space="preserve">
          <source>Create a helper proxy to easily launch an &lt;code&gt;rpc_sync&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.rpc_sync().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</source>
          <target state="translated">创建一个帮助代理，以使用 &lt;code&gt;rpc_sync&lt;/code&gt; 的所有者作为目标来轻松启动rpc_sync，以在该RRef引用的对象上运行功能。更具体地说， &lt;code&gt;rref.rpc_sync().func_name(*args, **kwargs)&lt;/code&gt; 与以下内容相同：</target>
        </trans-unit>
        <trans-unit id="8c6bb577e26b7162d7d6c68e8046a3fec50e1ed2" translate="yes" xml:space="preserve">
          <source>Create a qat module from a float module or qparams_dict</source>
          <target state="translated">从float模块或qparams_dict创建一个qat模块。</target>
        </trans-unit>
        <trans-unit id="608ebd71f80f5c1a22f515709f259847e91c3798" translate="yes" xml:space="preserve">
          <source>Create a quantized module from a float module or qparams_dict</source>
          <target state="translated">从float模块或qparams_dict创建一个量化模块。</target>
        </trans-unit>
        <trans-unit id="25224bf405304e1ab57eac3f4ba2017fad81ac0a" translate="yes" xml:space="preserve">
          <source>Create a symbolic function named &lt;code&gt;symbolic&lt;/code&gt; in the corresponding Function class.</source>
          <target state="translated">在相应的Function类中创建一个名为 &lt;code&gt;symbolic&lt;/code&gt; 的符号函数。</target>
        </trans-unit>
        <trans-unit id="05e63a314a968dccb93df86ac820277d35c3b513" translate="yes" xml:space="preserve">
          <source>Create a view of an existing &lt;code&gt;torch.Tensor&lt;/code&gt;&lt;code&gt;input&lt;/code&gt; with specified &lt;code&gt;size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt; and &lt;code&gt;storage_offset&lt;/code&gt;.</source>
          <target state="translated">创建具有指定 &lt;code&gt;size&lt;/code&gt; ， &lt;code&gt;stride&lt;/code&gt; 和 &lt;code&gt;storage_offset&lt;/code&gt; 的现有 &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;code&gt;input&lt;/code&gt; 的视图。</target>
        </trans-unit>
        <trans-unit id="b77a1bd9ce30851f58a1b21fe4ac8853dde6d2a8" translate="yes" xml:space="preserve">
          <source>Create special chart by collecting charts tags in &amp;lsquo;scalars&amp;rsquo;. Note that this function can only be called once for each SummaryWriter() object. Because it only provides metadata to tensorboard, the function can be called before or after the training loop.</source>
          <target state="translated">通过收集&amp;ldquo;标量&amp;rdquo;中的图表标签来创建特殊图表。请注意，此函数只能为每个SummaryWriter（）对象调用一次。由于它仅向张量板提供元数据，因此可以在训练循环之前或之后调用该函数。</target>
        </trans-unit>
        <trans-unit id="ad7ae06b82cbf41685a6d0a8499d7c2be99cffa5" translate="yes" xml:space="preserve">
          <source>Create the histogram of the incoming inputs.</source>
          <target state="translated">创建输入的直方图。</target>
        </trans-unit>
        <trans-unit id="a4594b741f646db69f78e37c46423c0ff117e55e" translate="yes" xml:space="preserve">
          <source>Creates Embedding instance from given 2-dimensional FloatTensor.</source>
          <target state="translated">从给定的二维FloatTensor创建Embedding实例。</target>
        </trans-unit>
        <trans-unit id="a6e25ee28f09c0ec7810655de85d0843eaddac29" translate="yes" xml:space="preserve">
          <source>Creates EmbeddingBag instance from given 2-dimensional FloatTensor.</source>
          <target state="translated">从给定的二维FloatTensor创建EmbeddingBag实例。</target>
        </trans-unit>
        <trans-unit id="7ab70bdf8d978808ae2341155ab591efacf83cbe" translate="yes" xml:space="preserve">
          <source>Creates a &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;&lt;code&gt;Tensor&lt;/code&gt;&lt;/a&gt; from a &lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray&quot;&gt;&lt;code&gt;numpy.ndarray&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">从&lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray&quot;&gt; &lt;code&gt;numpy.ndarray&lt;/code&gt; &lt;/a&gt;创建一个&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt; &lt;code&gt;Tensor&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="380a5b530b50dcf602f5ae20b20833a1876b83b8" translate="yes" xml:space="preserve">
          <source>Creates a &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;Tensor&lt;/code&gt;&lt;/a&gt; from a &lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray&quot;&gt;&lt;code&gt;numpy.ndarray&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">从&lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray&quot;&gt; &lt;code&gt;numpy.ndarray&lt;/code&gt; &lt;/a&gt;创建一个&lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;Tensor&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="8ff7109193ed0423f307bf24dbf6b88c15e440c1" translate="yes" xml:space="preserve">
          <source>Creates a &lt;code&gt;SummaryWriter&lt;/code&gt; that will write out events and summaries to the event file.</source>
          <target state="translated">创建一个 &lt;code&gt;SummaryWriter&lt;/code&gt; ，它将事件和摘要写到事件文件中。</target>
        </trans-unit>
        <trans-unit id="ba6567ea492c95adff11560e78e65d6f2fa6dfe4" translate="yes" xml:space="preserve">
          <source>Creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; for C++.</source>
          <target state="translated">创建用于C ++的 &lt;code&gt;setuptools.Extension&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b766aaed745dd6ce4f2529c6188fca66de086dd4" translate="yes" xml:space="preserve">
          <source>Creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; for CUDA/C++.</source>
          <target state="translated">创建用于CUDA / C ++的 &lt;code&gt;setuptools.Extension&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f3354489ddc8254e60320b0bc37db8d7b82c9d9c" translate="yes" xml:space="preserve">
          <source>Creates a Bernoulli distribution parameterized by &lt;a href=&quot;#torch.distributions.bernoulli.Bernoulli.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.bernoulli.Bernoulli.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt; (but not both).</source>
          <target state="translated">通过创建参数化的伯努利分布&lt;a href=&quot;#torch.distributions.bernoulli.Bernoulli.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;#torch.distributions.bernoulli.Bernoulli.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; &lt;/a&gt;（但不能同时）。</target>
        </trans-unit>
        <trans-unit id="58f0d733886ad515c8d42aceb08240a1f3daf7d2" translate="yes" xml:space="preserve">
          <source>Creates a Binomial distribution parameterized by &lt;code&gt;total_count&lt;/code&gt; and either &lt;a href=&quot;#torch.distributions.binomial.Binomial.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.binomial.Binomial.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt; (but not both). &lt;code&gt;total_count&lt;/code&gt; must be broadcastable with &lt;a href=&quot;#torch.distributions.binomial.Binomial.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt;/&lt;a href=&quot;#torch.distributions.binomial.Binomial.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">创建由参数化的二项分布 &lt;code&gt;total_count&lt;/code&gt; 并且或者&lt;a href=&quot;#torch.distributions.binomial.Binomial.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;#torch.distributions.binomial.Binomial.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; &lt;/a&gt;（但不是两者）。 &lt;code&gt;total_count&lt;/code&gt; 必须可以通过&lt;a href=&quot;#torch.distributions.binomial.Binomial.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt; / &lt;a href=&quot;#torch.distributions.binomial.Binomial.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; &lt;/a&gt;广播。</target>
        </trans-unit>
        <trans-unit id="719b609f965a9077ede6ab6190b76d01527929f0" translate="yes" xml:space="preserve">
          <source>Creates a Chi2 distribution parameterized by shape parameter &lt;a href=&quot;#torch.distributions.chi2.Chi2.df&quot;&gt;&lt;code&gt;df&lt;/code&gt;&lt;/a&gt;. This is exactly equivalent to &lt;code&gt;Gamma(alpha=0.5*df, beta=0.5)&lt;/code&gt;</source>
          <target state="translated">创建由形状参数&lt;a href=&quot;#torch.distributions.chi2.Chi2.df&quot;&gt; &lt;code&gt;df&lt;/code&gt; &lt;/a&gt;参数化的Chi2分布。这完全等于 &lt;code&gt;Gamma(alpha=0.5*df, beta=0.5)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="1aa6f05fdf77d536ae15b48713329d9043bf42ff" translate="yes" xml:space="preserve">
          <source>Creates a Dirichlet distribution parameterized by concentration &lt;code&gt;concentration&lt;/code&gt;.</source>
          <target state="translated">创建以浓度 &lt;code&gt;concentration&lt;/code&gt; 参数的Dirichlet分布。</target>
        </trans-unit>
        <trans-unit id="9d283176d9c3cbc5838d89065f1a04eb5cd11ce7" translate="yes" xml:space="preserve">
          <source>Creates a Exponential distribution parameterized by &lt;code&gt;rate&lt;/code&gt;.</source>
          <target state="translated">创建一个由 &lt;code&gt;rate&lt;/code&gt; 参数化的指数分布。</target>
        </trans-unit>
        <trans-unit id="99602cffe1f166e20c0a8e58347d9e329ce816ad" translate="yes" xml:space="preserve">
          <source>Creates a Fisher-Snedecor distribution parameterized by &lt;code&gt;df1&lt;/code&gt; and &lt;code&gt;df2&lt;/code&gt;.</source>
          <target state="translated">创建由 &lt;code&gt;df1&lt;/code&gt; 和 &lt;code&gt;df2&lt;/code&gt; 参数化的Fisher-Snedecor分布。</target>
        </trans-unit>
        <trans-unit id="d042259ebf83e1f39612d0d46ecb014811b94bfd" translate="yes" xml:space="preserve">
          <source>Creates a Gamma distribution parameterized by shape &lt;code&gt;concentration&lt;/code&gt; and &lt;code&gt;rate&lt;/code&gt;.</source>
          <target state="translated">创建由形状 &lt;code&gt;concentration&lt;/code&gt; 和 &lt;code&gt;rate&lt;/code&gt; 参数化的Gamma分布。</target>
        </trans-unit>
        <trans-unit id="9fcc365d0f990b9958aa880a339a219e17e17273" translate="yes" xml:space="preserve">
          <source>Creates a Geometric distribution parameterized by &lt;a href=&quot;#torch.distributions.geometric.Geometric.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt;, where &lt;a href=&quot;#torch.distributions.geometric.Geometric.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; is the probability of success of Bernoulli trials. It represents the probability that in</source>
          <target state="translated">创建一个由&lt;a href=&quot;#torch.distributions.geometric.Geometric.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt;参数化的几何分布，其中&lt;a href=&quot;#torch.distributions.geometric.Geometric.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt;是伯努利试验成功的概率。它代表了</target>
        </trans-unit>
        <trans-unit id="f986bba11240572671f561e08bcb8f571d37c585" translate="yes" xml:space="preserve">
          <source>Creates a Laplace distribution parameterized by &lt;code&gt;loc&lt;/code&gt; and &lt;code&gt;scale&lt;/code&gt;.</source>
          <target state="translated">创建一个由 &lt;code&gt;loc&lt;/code&gt; 和 &lt;code&gt;scale&lt;/code&gt; 参数化的Laplace分布。</target>
        </trans-unit>
        <trans-unit id="4adb5e5fdc76653b710744984dd086c968ee39fd" translate="yes" xml:space="preserve">
          <source>Creates a LogitRelaxedBernoulli distribution parameterized by &lt;a href=&quot;#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt; (but not both), which is the logit of a RelaxedBernoulli distribution.</source>
          <target state="translated">创建LogitRelaxedBernoulli分布由参数&lt;a href=&quot;#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; &lt;/a&gt;（但不是两者），这是一个RelaxedBernoulli分布的分对数。</target>
        </trans-unit>
        <trans-unit id="15a0c995e1894ed35a375aec8d89951825601652" translate="yes" xml:space="preserve">
          <source>Creates a Multinomial distribution parameterized by &lt;code&gt;total_count&lt;/code&gt; and either &lt;a href=&quot;#torch.distributions.multinomial.Multinomial.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.multinomial.Multinomial.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt; (but not both). The innermost dimension of &lt;a href=&quot;#torch.distributions.multinomial.Multinomial.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; indexes over categories. All other dimensions index over batches.</source>
          <target state="translated">创建一个多项分布参数由 &lt;code&gt;total_count&lt;/code&gt; 并且或者&lt;a href=&quot;#torch.distributions.multinomial.Multinomial.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;#torch.distributions.multinomial.Multinomial.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; &lt;/a&gt;（但不是两者）。&lt;a href=&quot;#torch.distributions.multinomial.Multinomial.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt;的最内部维度针对类别进行索引。所有其他尺寸均按批次编制索引。</target>
        </trans-unit>
        <trans-unit id="e3924f533e522b7cb260639c31a007a0e461a496" translate="yes" xml:space="preserve">
          <source>Creates a Negative Binomial distribution, i.e. distribution of the number of successful independent and identical Bernoulli trials before &lt;code&gt;total_count&lt;/code&gt; failures are achieved. The probability of success of each Bernoulli trial is &lt;a href=&quot;#torch.distributions.negative_binomial.NegativeBinomial.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">创建负二项式分布，即在达到 &lt;code&gt;total_count&lt;/code&gt; 失败之前，成功的独立且相同的Bernoulli试验次数的分布。每个伯努利试验的成功的概率是&lt;a href=&quot;#torch.distributions.negative_binomial.NegativeBinomial.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="500755127f28d6a568f776f9aef009f0a35b4dde" translate="yes" xml:space="preserve">
          <source>Creates a Poisson distribution parameterized by &lt;code&gt;rate&lt;/code&gt;, the rate parameter.</source>
          <target state="translated">创建由参数泊松分布 &lt;code&gt;rate&lt;/code&gt; ，速率参数。</target>
        </trans-unit>
        <trans-unit id="a83a7cc2ac442a11cbf1dc1e134a50d3c260f42e" translate="yes" xml:space="preserve">
          <source>Creates a RelaxedBernoulli distribution, parametrized by &lt;a href=&quot;#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature&quot;&gt;&lt;code&gt;temperature&lt;/code&gt;&lt;/a&gt;, and either &lt;a href=&quot;#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt; (but not both). This is a relaxed version of the &lt;code&gt;Bernoulli&lt;/code&gt; distribution, so the values are in (0, 1), and has reparametrizable samples.</source>
          <target state="translated">创建RelaxedBernoulli分布，通过参数化&lt;a href=&quot;#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature&quot;&gt; &lt;code&gt;temperature&lt;/code&gt; &lt;/a&gt;，并且或者&lt;a href=&quot;#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; &lt;/a&gt;（但不是两者）。这是 &lt;code&gt;Bernoulli&lt;/code&gt; 分布的宽松版本，因此值在（0，1）中，并且具有可重新设置参数的样本。</target>
        </trans-unit>
        <trans-unit id="f1a742732148726c845a736e718e0b42ae326d5a" translate="yes" xml:space="preserve">
          <source>Creates a RelaxedOneHotCategorical distribution parametrized by &lt;a href=&quot;#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature&quot;&gt;&lt;code&gt;temperature&lt;/code&gt;&lt;/a&gt;, and either &lt;a href=&quot;#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt;. This is a relaxed version of the &lt;code&gt;OneHotCategorical&lt;/code&gt; distribution, so its samples are on simplex, and are reparametrizable.</source>
          <target state="translated">创建RelaxedOneHotCategorical分布由参数化&lt;a href=&quot;#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature&quot;&gt; &lt;code&gt;temperature&lt;/code&gt; &lt;/a&gt;，并且或者&lt;a href=&quot;#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; &lt;/a&gt;。这是 &lt;code&gt;OneHotCategorical&lt;/code&gt; 发行版的宽松版本，因此其示例位于单形上，并且可以重新设置参数。</target>
        </trans-unit>
        <trans-unit id="7c20c969a8d2e6e99e36b8641b0c9149ce07f5be" translate="yes" xml:space="preserve">
          <source>Creates a Student&amp;rsquo;s t-distribution parameterized by degree of freedom &lt;code&gt;df&lt;/code&gt;, mean &lt;code&gt;loc&lt;/code&gt; and scale &lt;code&gt;scale&lt;/code&gt;.</source>
          <target state="translated">创建一个以自由度 &lt;code&gt;df&lt;/code&gt; ，均值 &lt;code&gt;loc&lt;/code&gt; 和scale &lt;code&gt;scale&lt;/code&gt; 为参数的学生t分布。</target>
        </trans-unit>
        <trans-unit id="956c67cb0f1c70fdb976dabf9f583b2950425b18" translate="yes" xml:space="preserve">
          <source>Creates a categorical distribution parameterized by either &lt;a href=&quot;#torch.distributions.categorical.Categorical.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.categorical.Categorical.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt; (but not both).</source>
          <target state="translated">创建由任一参数化的分类分配&lt;a href=&quot;#torch.distributions.categorical.Categorical.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;#torch.distributions.categorical.Categorical.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; &lt;/a&gt;（但不是两者）。</target>
        </trans-unit>
        <trans-unit id="20a8f67a767e2c70110f0eacd788bdca79dafc4f" translate="yes" xml:space="preserve">
          <source>Creates a continuous Bernoulli distribution parameterized by &lt;a href=&quot;#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt; (but not both).</source>
          <target state="translated">创建由参数化的连续贝努利分布&lt;a href=&quot;#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; &lt;/a&gt;（但不是两者）。</target>
        </trans-unit>
        <trans-unit id="0ccb6b54dc9399c2c85ab56d56ee522676eba886" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the Binary Cross Entropy between the target and the output:</source>
          <target state="translated">创建一个衡量目标和输出之间二进制交叉熵的标准。</target>
        </trans-unit>
        <trans-unit id="3d34756a1f2c85c7f54f427b38b59a7f138b4091" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the loss given input tensors</source>
          <target state="translated">创建一个标准,用于测量给定输入张力的损失。</target>
        </trans-unit>
        <trans-unit id="b30adb8efde8fc483debf3f678fe0a50c0f3a700" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the loss given inputs</source>
          <target state="translated">创建一个标准,衡量给定输入的损失。</target>
        </trans-unit>
        <trans-unit id="db68393122ce4917cd40e5341fb476ef56ce47be" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the mean absolute error (MAE) between each element in the input</source>
          <target state="translated">创建一个标准,衡量输入中每个元素之间的平均绝对误差(MAE)。</target>
        </trans-unit>
        <trans-unit id="dc0bceb74569c8f166528464397135db651de1e9" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input</source>
          <target state="translated">创建一个标准,衡量输入中每个元素之间的平均平方误差(平方L2规范)。</target>
        </trans-unit>
        <trans-unit id="c567813437a56bca1c0ee05ac4bbbc7a9a0f0b86" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the triplet loss given an input tensors</source>
          <target state="translated">创建一个标准,用于测量给定输入的三倍频损耗。</target>
        </trans-unit>
        <trans-unit id="54c0c219984f4a50b8cfc1212d879c396ddab80a" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the triplet loss given input tensors</source>
          <target state="translated">创建一个标准,测量给定输入时的三倍频损耗。</target>
        </trans-unit>
        <trans-unit id="3ab7e1fd5db78a2e9eadeb7c1fe0ccb243aebcd4" translate="yes" xml:space="preserve">
          <source>Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input</source>
          <target state="translated">创建一个准则,优化输入和输出之间的多类分类铰链损失(基于边缘的损失)。</target>
        </trans-unit>
        <trans-unit id="0f6e83829c9a9db048daa12ae2ef6905382cf541" translate="yes" xml:space="preserve">
          <source>Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input</source>
          <target state="translated">创建一个准则,优化输入之间的多类多分类铰链损失(基于边缘的损失)。</target>
        </trans-unit>
        <trans-unit id="74a30cc1dfda2e2ca6b72fcb42e123c4afa11917" translate="yes" xml:space="preserve">
          <source>Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input</source>
          <target state="translated">创建一个准则,基于最大熵优化多标签的单对多损失,在输入和输入之间的单对多损失。</target>
        </trans-unit>
        <trans-unit id="38862788d574ffd77ba587c5570ecdb676e04e5f" translate="yes" xml:space="preserve">
          <source>Creates a criterion that optimizes a two-class classification logistic loss between input tensor</source>
          <target state="translated">创建一个准则,优化输入张量之间的两类分类逻辑损失。</target>
        </trans-unit>
        <trans-unit id="d4196579876a20368dab571bd9df29f488cfb987" translate="yes" xml:space="preserve">
          <source>Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.</source>
          <target state="translated">创建一个标准,如果绝对要素误差低于β,则使用平方项,否则使用L1项。</target>
        </trans-unit>
        <trans-unit id="416d9c5bdd3061ae8e8917cecda26e9fe1f6412a" translate="yes" xml:space="preserve">
          <source>Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. It is less sensitive to outliers than the &lt;code&gt;MSELoss&lt;/code&gt; and in some cases prevents exploding gradients (e.g. see &lt;code&gt;Fast R-CNN&lt;/code&gt; paper by Ross Girshick). Also known as the Huber loss:</source>
          <target state="translated">创建一个使用平方项的条件，如果逐元素的绝对误差低于beta，则使用平方项；否则，使用L1项。它对异常值的敏感性不如 &lt;code&gt;MSELoss&lt;/code&gt; ,并且在某些情况下可以防止梯度爆炸（例如，请参阅Ross Girshick的 &lt;code&gt;Fast R-CNN&lt;/code&gt; 论文）。也称为胡贝尔损耗：</target>
        </trans-unit>
        <trans-unit id="1f5f49121121fa1e6abb3edd3d6d453965003b41" translate="yes" xml:space="preserve">
          <source>Creates a half-Cauchy distribution parameterized by &lt;code&gt;scale&lt;/code&gt; where:</source>
          <target state="translated">创建按 &lt;code&gt;scale&lt;/code&gt; 参数化的半Cauchy分布，其中：</target>
        </trans-unit>
        <trans-unit id="4deb8f0f5867e5b62231478b068c257636c7c149" translate="yes" xml:space="preserve">
          <source>Creates a half-normal distribution parameterized by &lt;code&gt;scale&lt;/code&gt; where:</source>
          <target state="translated">创建按 &lt;code&gt;scale&lt;/code&gt; 参数化的半正态分布，其中：</target>
        </trans-unit>
        <trans-unit id="7674fe00de59675dc7589881d3e4f40957fcf6b1" translate="yes" xml:space="preserve">
          <source>Creates a log-normal distribution parameterized by &lt;a href=&quot;#torch.distributions.log_normal.LogNormal.loc&quot;&gt;&lt;code&gt;loc&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.distributions.log_normal.LogNormal.scale&quot;&gt;&lt;code&gt;scale&lt;/code&gt;&lt;/a&gt; where:</source>
          <target state="translated">创建由&lt;a href=&quot;#torch.distributions.log_normal.LogNormal.loc&quot;&gt; &lt;code&gt;loc&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;#torch.distributions.log_normal.LogNormal.scale&quot;&gt; &lt;code&gt;scale&lt;/code&gt; &lt;/a&gt;参数化的对数正态分布，其中：</target>
        </trans-unit>
        <trans-unit id="abf49ecc79a512b11706abef3bf65667a788d341" translate="yes" xml:space="preserve">
          <source>Creates a multivariate normal (also called Gaussian) distribution parameterized by a mean vector and a covariance matrix.</source>
          <target state="translated">创建一个由均值向量和协方差矩阵参数化的多变量正态分布(也称为高斯)。</target>
        </trans-unit>
        <trans-unit id="3e6e658a2c1f4f9bb64225bec1cd132cf3dc92f9" translate="yes" xml:space="preserve">
          <source>Creates a multivariate normal distribution with covariance matrix having a low-rank form parameterized by &lt;code&gt;cov_factor&lt;/code&gt; and &lt;code&gt;cov_diag&lt;/code&gt;:</source>
          <target state="translated">创建具有协方差矩阵的多元正态分布，协方差矩阵具有由 &lt;code&gt;cov_factor&lt;/code&gt; 和 &lt;code&gt;cov_diag&lt;/code&gt; 参数化的低秩形式：</target>
        </trans-unit>
        <trans-unit id="1ebf98c1eaeed448f01f072825db935178bbb990" translate="yes" xml:space="preserve">
          <source>Creates a new distributed group.</source>
          <target state="translated">创建一个新的分布式组。</target>
        </trans-unit>
        <trans-unit id="cb830598128131448f4401b8c4fa79f95497434a" translate="yes" xml:space="preserve">
          <source>Creates a normal (also called Gaussian) distribution parameterized by &lt;code&gt;loc&lt;/code&gt; and &lt;code&gt;scale&lt;/code&gt;.</source>
          <target state="translated">创建一个由 &lt;code&gt;loc&lt;/code&gt; 和 &lt;code&gt;scale&lt;/code&gt; 参数化的正态（也称为高斯）分布。</target>
        </trans-unit>
        <trans-unit id="073fb44c832f84ccfa42ff94accd75b4dd72e7da" translate="yes" xml:space="preserve">
          <source>Creates a one-dimensional tensor of size &lt;code&gt;steps&lt;/code&gt; whose values are evenly spaced from</source>
          <target state="translated">创建尺寸 &lt;code&gt;steps&lt;/code&gt; 的一维张量，其值与</target>
        </trans-unit>
        <trans-unit id="350152525d5dacc5decf599c92018f7fa5823d28" translate="yes" xml:space="preserve">
          <source>Creates a one-dimensional tensor of size &lt;code&gt;steps&lt;/code&gt; whose values are evenly spaced from &lt;code&gt;start&lt;/code&gt; to &lt;code&gt;end&lt;/code&gt;, inclusive.</source>
          <target state="translated">创建尺寸 &lt;code&gt;steps&lt;/code&gt; 的一维张量，其值从 &lt;code&gt;start&lt;/code&gt; 到 &lt;code&gt;end&lt;/code&gt; （包括两端）均等间隔。</target>
        </trans-unit>
        <trans-unit id="9f41c69b98f8c4bce082aab055bcac8455947a3e" translate="yes" xml:space="preserve">
          <source>Creates a one-dimensional tensor of size &lt;code&gt;steps&lt;/code&gt; whose values are evenly spaced from &lt;code&gt;start&lt;/code&gt; to &lt;code&gt;end&lt;/code&gt;, inclusive. That is, the value are:</source>
          <target state="translated">创建尺寸 &lt;code&gt;steps&lt;/code&gt; 的一维张量，其值从 &lt;code&gt;start&lt;/code&gt; 到 &lt;code&gt;end&lt;/code&gt; （包括两端）均等间隔。即，值是：</target>
        </trans-unit>
        <trans-unit id="c05b957d45d9e4908a05a11e87943ceb8347e92e" translate="yes" xml:space="preserve">
          <source>Creates a one-hot categorical distribution parameterized by &lt;a href=&quot;#torch.distributions.one_hot_categorical.OneHotCategorical.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.one_hot_categorical.OneHotCategorical.logits&quot;&gt;&lt;code&gt;logits&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">创建由参数化的独热分类分布&lt;a href=&quot;#torch.distributions.one_hot_categorical.OneHotCategorical.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;#torch.distributions.one_hot_categorical.OneHotCategorical.logits&quot;&gt; &lt;code&gt;logits&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="9962e59b2ab133d8c3b2267ae38553f28f8a79bd" translate="yes" xml:space="preserve">
          <source>Creates a quantized module from a float module or qparams_dict.</source>
          <target state="translated">从一个float模块或qparams_dict创建一个量化模块。</target>
        </trans-unit>
        <trans-unit id="d2147eb4da2cad834986bf746aeef9a006067d0f" translate="yes" xml:space="preserve">
          <source>Creates a tensor of size &lt;code&gt;size&lt;/code&gt; filled with &lt;code&gt;fill_value&lt;/code&gt;.</source>
          <target state="translated">创建大小的张量 &lt;code&gt;size&lt;/code&gt; 充满 &lt;code&gt;fill_value&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ed2b3c6c1dcd8ed54d69945a6e4acc25f855c267" translate="yes" xml:space="preserve">
          <source>Creates a tensor of size &lt;code&gt;size&lt;/code&gt; filled with &lt;code&gt;fill_value&lt;/code&gt;. The tensor&amp;rsquo;s dtype is inferred from &lt;code&gt;fill_value&lt;/code&gt;.</source>
          <target state="translated">创建大小的张量 &lt;code&gt;size&lt;/code&gt; 充满 &lt;code&gt;fill_value&lt;/code&gt; 。张量的 &lt;code&gt;fill_value&lt;/code&gt; 是从fill_value推断出来的。</target>
        </trans-unit>
        <trans-unit id="12f064b3e975b58c920e1056343b0cd452f5ecab" translate="yes" xml:space="preserve">
          <source>Creates a tensor whose diagonals of certain 2D planes (specified by &lt;code&gt;dim1&lt;/code&gt; and &lt;code&gt;dim2&lt;/code&gt;) are filled by &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">创建一个张量，其特定2D平面（由 &lt;code&gt;dim1&lt;/code&gt; 和 &lt;code&gt;dim2&lt;/code&gt; 指定）的对角线由 &lt;code&gt;input&lt;/code&gt; 填充。</target>
        </trans-unit>
        <trans-unit id="a4e06d7745c58e373370f1d2833461757426df92" translate="yes" xml:space="preserve">
          <source>Creates a tensor whose diagonals of certain 2D planes (specified by &lt;code&gt;dim1&lt;/code&gt; and &lt;code&gt;dim2&lt;/code&gt;) are filled by &lt;code&gt;input&lt;/code&gt;. To facilitate creating batched diagonal matrices, the 2D planes formed by the last two dimensions of the returned tensor are chosen by default.</source>
          <target state="translated">创建一个张量，其特定2D平面（由 &lt;code&gt;dim1&lt;/code&gt; 和 &lt;code&gt;dim2&lt;/code&gt; 指定）的对角线由 &lt;code&gt;input&lt;/code&gt; 填充。为了便于创建成批的对角矩阵，默认情况下选择由返回张量的最后两个维度形成的2D平面。</target>
        </trans-unit>
        <trans-unit id="54d06cd8eb770a4665e06b480e41c18b35882c33" translate="yes" xml:space="preserve">
          <source>Creates an asynchronous task executing &lt;code&gt;func&lt;/code&gt; and a reference to the value of the result of this execution.</source>
          <target state="translated">创建一个执行 &lt;code&gt;func&lt;/code&gt; 的异步任务，并引用该执行结果的值。</target>
        </trans-unit>
        <trans-unit id="a528098d6266957ce08cc5436471c4e687e167c2" translate="yes" xml:space="preserve">
          <source>Creates an asynchronous task executing &lt;code&gt;func&lt;/code&gt; and a reference to the value of the result of this execution. &lt;code&gt;fork&lt;/code&gt; will return immediately, so the return value of &lt;code&gt;func&lt;/code&gt; may not have been computed yet. To force completion of the task and access the return value invoke &lt;code&gt;torch.jit.wait&lt;/code&gt; on the Future. &lt;code&gt;fork&lt;/code&gt; invoked with a &lt;code&gt;func&lt;/code&gt; which returns &lt;code&gt;T&lt;/code&gt; is typed as &lt;code&gt;torch.jit.Future[T]&lt;/code&gt;. &lt;code&gt;fork&lt;/code&gt; calls can be arbitrarily nested, and may be invoked with positional and keyword arguments. Asynchronous execution will only occur when run in TorchScript. If run in pure python, &lt;code&gt;fork&lt;/code&gt; will not execute in parallel. &lt;code&gt;fork&lt;/code&gt; will also not execute in parallel when invoked while tracing, however the &lt;code&gt;fork&lt;/code&gt; and &lt;code&gt;wait&lt;/code&gt; calls will be captured in the exported IR Graph. .. warning:</source>
          <target state="translated">创建一个执行 &lt;code&gt;func&lt;/code&gt; 的异步任务，并引用该执行结果的值。 &lt;code&gt;fork&lt;/code&gt; 将立即返回，因此 &lt;code&gt;func&lt;/code&gt; 的返回值可能尚未计算。要强制完成任务并访问返回值，请在Future上调用 &lt;code&gt;torch.jit.wait&lt;/code&gt; 。用返回 &lt;code&gt;T&lt;/code&gt; 的 &lt;code&gt;func&lt;/code&gt; 调用的 &lt;code&gt;fork&lt;/code&gt; 的类型为 &lt;code&gt;torch.jit.Future[T]&lt;/code&gt; 。 &lt;code&gt;fork&lt;/code&gt; 调用可以任意嵌套，并且可以使用位置和关键字参数来调用。异步执行仅在TorchScript中运行时才会发生。如果在纯python中运行， &lt;code&gt;fork&lt;/code&gt; 将不会并行执行。 &lt;code&gt;fork&lt;/code&gt; 在跟踪时调用fork时，fork也不会并行执行，但是 &lt;code&gt;fork&lt;/code&gt; 和 &lt;code&gt;wait&lt;/code&gt; 调用将在导出的IR Graph中捕获。.. 警告：</target>
        </trans-unit>
        <trans-unit id="5c73fcf2c3b7a88652c30df05d4a49be7a9a3a82" translate="yes" xml:space="preserve">
          <source>Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.</source>
          <target state="translated">创建并返回一个生成器对象,管理产生伪随机数的算法状态。</target>
        </trans-unit>
        <trans-unit id="7bf69af350942e6f19285c4071e0d9bc1d0390fb" translate="yes" xml:space="preserve">
          <source>Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers. Used as a keyword argument in many &lt;a href=&quot;../torch#inplace-random-sampling&quot;&gt;In-place random sampling&lt;/a&gt; functions.</source>
          <target state="translated">创建并返回一个生成器对象，该对象管理生成伪随机数的算法的状态。在许多&lt;a href=&quot;../torch#inplace-random-sampling&quot;&gt;就地随机采样&lt;/a&gt;函数中用作关键字参数。</target>
        </trans-unit>
        <trans-unit id="6db27514dc4a87b77455deb1f68ac2812f51084a" translate="yes" xml:space="preserve">
          <source>Creating TorchScript Code</source>
          <target state="translated">创建TorchScript代码</target>
        </trans-unit>
        <trans-unit id="14dc9f6dca53e0df6aaf4e5eb53a7efa6593aab3" translate="yes" xml:space="preserve">
          <source>Creating named tensors</source>
          <target state="translated">创建命名时序</target>
        </trans-unit>
        <trans-unit id="733cbb78a0d286a0935c6c571466400a11828907" translate="yes" xml:space="preserve">
          <source>Creation Ops</source>
          <target state="translated">创作行动</target>
        </trans-unit>
        <trans-unit id="d66fef2dd7f330ad8c7b2de8229080af12ec8719" translate="yes" xml:space="preserve">
          <source>Creation of this class requires that &lt;code&gt;torch.distributed&lt;/code&gt; to be already initialized, by calling &lt;a href=&quot;../distributed#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">此类的创建要求 &lt;code&gt;torch.distributed&lt;/code&gt; 已通过调用&lt;a href=&quot;../distributed#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt; &lt;/a&gt;进行了初始化。</target>
        </trans-unit>
        <trans-unit id="711f625ceb8a65f44028c4e7b7e4e818fc7445a7" translate="yes" xml:space="preserve">
          <source>CrossEntropyLoss</source>
          <target state="translated">CrossEntropyLoss</target>
        </trans-unit>
        <trans-unit id="5eec0b5e11526f784758a0afbe48baade8e71e9e" translate="yes" xml:space="preserve">
          <source>Current implementation of &lt;a href=&quot;#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; introduces memory overhead, thus it might lead to unexpectedly high memory usage in the applications with many tiny tensors. If this is your case, consider using one large structure.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;当前实现引入了内存开销，因此，在具有许多微小张量的应用程序中，它可能导致意外的高内存使用率。如果是这种情况，请考虑使用一种大型结构。</target>
        </trans-unit>
        <trans-unit id="96c1a2df528a935c7c808490ba7a2921d055775f" translate="yes" xml:space="preserve">
          <source>Current implementation packs weights on every call, which has penalty on performance. If you want to avoid the overhead, use &lt;a href=&quot;#torch.nn.quantized.Linear&quot;&gt;&lt;code&gt;Linear&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">当前的实现在每个调用中都包含权重，这会降低性能。如果要避免开销，请使用&lt;a href=&quot;#torch.nn.quantized.Linear&quot;&gt; &lt;code&gt;Linear&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="392192a4ae05351b1f7b21e538ff3e8ca534e67d" translate="yes" xml:space="preserve">
          <source>Currently &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;SyncBatchNorm&lt;/code&gt;&lt;/a&gt; only supports &lt;code&gt;DistributedDataParallel&lt;/code&gt; (DDP) with single GPU per process. Use &lt;a href=&quot;#torch.nn.SyncBatchNorm.convert_sync_batchnorm&quot;&gt;&lt;code&gt;torch.nn.SyncBatchNorm.convert_sync_batchnorm()&lt;/code&gt;&lt;/a&gt; to convert &lt;code&gt;BatchNorm*D&lt;/code&gt; layer to &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;SyncBatchNorm&lt;/code&gt;&lt;/a&gt; before wrapping Network with DDP.</source>
          <target state="translated">当前，&lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt; &lt;code&gt;SyncBatchNorm&lt;/code&gt; &lt;/a&gt;仅支持每个进程具有单个GPU的 &lt;code&gt;DistributedDataParallel&lt;/code&gt; （DDP）。使用DDP包装网络之前，请使用&lt;a href=&quot;#torch.nn.SyncBatchNorm.convert_sync_batchnorm&quot;&gt; &lt;code&gt;torch.nn.SyncBatchNorm.convert_sync_batchnorm()&lt;/code&gt; &lt;/a&gt;将 &lt;code&gt;BatchNorm*D&lt;/code&gt; 层转换为&lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt; &lt;code&gt;SyncBatchNorm&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="a78ac4f002658abaaa21040baac2a600bf544edc" translate="yes" xml:space="preserve">
          <source>Currently in the CUDA implementation and the CPU implementation when dim is specified, &lt;code&gt;torch.unique&lt;/code&gt; always sort the tensor at the beginning regardless of the &lt;code&gt;sort&lt;/code&gt; argument. Sorting could be slow, so if your input tensor is already sorted, it is recommended to use &lt;a href=&quot;torch.unique_consecutive#torch.unique_consecutive&quot;&gt;&lt;code&gt;torch.unique_consecutive()&lt;/code&gt;&lt;/a&gt; which avoids the sorting.</source>
          <target state="translated">当前，在CUDA实现和CPU实现中，当指定dim时，无论 &lt;code&gt;sort&lt;/code&gt; 参数如何， &lt;code&gt;torch.unique&lt;/code&gt; 总是在开始时对张量进行排序。排序可能很慢，因此，如果您的输入张量已被排序，建议使用&lt;a href=&quot;torch.unique_consecutive#torch.unique_consecutive&quot;&gt; &lt;code&gt;torch.unique_consecutive()&lt;/code&gt; &lt;/a&gt;来避免排序。</target>
        </trans-unit>
        <trans-unit id="c8a61a0d816f8b21bf7f4b46f888c832b03676e2" translate="yes" xml:space="preserve">
          <source>Currently spatial and volumetric upsampling are supported (i.e. expected inputs are 4 or 5 dimensional).</source>
          <target state="translated">目前支持空间和体积上采样(即预期输入是4维或5维)。</target>
        </trans-unit>
        <trans-unit id="8a9d7a3e06793c263711ce33ba0ffdc49d409027" translate="yes" xml:space="preserve">
          <source>Currently supported operations and subsystems</source>
          <target state="translated">目前支持的业务和子系统</target>
        </trans-unit>
        <trans-unit id="3b05bf74a2646cf7882607d1b6e8f3921b3f2124" translate="yes" xml:space="preserve">
          <source>Currently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.</source>
          <target state="translated">目前支持时间采样、空间采样和体积采样,即预期的输入是三维、四维或五维的形状。</target>
        </trans-unit>
        <trans-unit id="af454dd72bdbaa9839806374d040bd77db038329" translate="yes" xml:space="preserve">
          <source>Currently temporal, spatial and volumetric upsampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.</source>
          <target state="translated">目前支持时间、空间和体积上采样,即预期输入是3-D、4-D或5-D的形状。</target>
        </trans-unit>
        <trans-unit id="d49c5e3f020c5093dc8a1df7be3b5947455c7de6" translate="yes" xml:space="preserve">
          <source>Currently three initialization methods are supported:</source>
          <target state="translated">目前支持三种初始化方法。</target>
        </trans-unit>
        <trans-unit id="ecab6b4c3bf958fdadbd30836155137da6ef98e2" translate="yes" xml:space="preserve">
          <source>Currently valid scalar and tensor combination are 1. Scalar of floating dtype and torch.double 2. Scalar of integral dtype and torch.long 3. Scalar of complex dtype and torch.complex128</source>
          <target state="translated">目前有效的标量和张量组合有:1.浮动d型的标量和torch.double 2.整体d型的标量和torch.long 3.复数d型的标量和torch.complex128的标量。</target>
        </trans-unit>
        <trans-unit id="0cf55afa31790d6a153e9124662c9ad715c264be" translate="yes" xml:space="preserve">
          <source>Currently, only 3-D output tensors (unfolded batched image-like tensors) are supported.</source>
          <target state="translated">目前,只支持三维输出时序(展开的分批图像式时序)。</target>
        </trans-unit>
        <trans-unit id="cf7422cb9205f4ada47421ece107faeeaf3ef476" translate="yes" xml:space="preserve">
          <source>Currently, only 4-D input tensors (batched image-like tensors) are supported.</source>
          <target state="translated">目前,只支持4-D输入时序(分批图像式时序)。</target>
        </trans-unit>
        <trans-unit id="804337292e10b528ce9da39728466ea21dd2d93d" translate="yes" xml:space="preserve">
          <source>Currently, only 4-D output tensors (batched image-like tensors) are supported.</source>
          <target state="translated">目前,只支持4-D输出时序(分批图像式时序)。</target>
        </trans-unit>
        <trans-unit id="55e4dc48ed51c13b29c9e279e5505c0a895f1cb1" translate="yes" xml:space="preserve">
          <source>Currently, only spatial (4-D) and volumetric (5-D) &lt;code&gt;input&lt;/code&gt; are supported.</source>
          <target state="translated">当前，仅支持空间（4-D）和体积（5-D） &lt;code&gt;input&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="16e0def02ff49d292a9c6c0ee9ab56bbab51c7e1" translate="yes" xml:space="preserve">
          <source>Custom averaging strategies</source>
          <target state="translated">定制平均战略</target>
        </trans-unit>
        <trans-unit id="b674ceb20a7250912a1c130e2165e5889d62f4ab" translate="yes" xml:space="preserve">
          <source>Custom operators</source>
          <target state="translated">自定义运营商</target>
        </trans-unit>
        <trans-unit id="3461dd173d30fce828765c425b88ef7516427e68" translate="yes" xml:space="preserve">
          <source>CustomFromMask</source>
          <target state="translated">CustomFromMask</target>
        </trans-unit>
        <trans-unit id="e93172f8decb4286fa3d5ed1a6b28a8e5bb1dd99" translate="yes" xml:space="preserve">
          <source>Cyclical learning rate policy changes the learning rate after every batch. &lt;code&gt;step&lt;/code&gt; should be called after a batch has been used for training.</source>
          <target state="translated">周期性学习率策略会在每批之后更改学习率。一批用于培训后，应调用 &lt;code&gt;step&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="50c9e8d5fc98727b4bbc93cf5d64a68db647f04f" translate="yes" xml:space="preserve">
          <source>D</source>
          <target state="translated">D</target>
        </trans-unit>
        <trans-unit id="eb3635e47f534f46cf856ff7cd245f64d37e6d18" translate="yes" xml:space="preserve">
          <source>DCGAN</source>
          <target state="translated">DCGAN</target>
        </trans-unit>
        <trans-unit id="17ba820b5bcd55a78cfd53d227f66dc3adcb9245" translate="yes" xml:space="preserve">
          <source>D_{out} = (D_{in} - 1) \times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}</source>
          <target state="translated">D_{out}=(D_{in}-1)xtimes \text{stride[0]}-2 xtimes \text{padding[0]}+\text{kernel_size[0]}</target>
        </trans-unit>
        <trans-unit id="11e96ca0ef0a7d1c0968f6c0aa661473e65963b1" translate="yes" xml:space="preserve">
          <source>D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1</source>
          <target state="translated">D_{out}=(D_{in}-1)遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数</target>
        </trans-unit>
        <trans-unit id="c8db0974bd119b8e244f7c9a0fbb5cd60340ca9b" translate="yes" xml:space="preserve">
          <source>D_{out} = D_{in} + \text{padding\_front} + \text{padding\_back}</source>
          <target state="translated">D_{out}=D_{in}+{text{padding\front}+{text{padding\back}。</target>
        </trans-unit>
        <trans-unit id="343f5280293229b193ab7d6cc2f6fecb4a2a501b" translate="yes" xml:space="preserve">
          <source>D_{out} = \left\lfloor D_{in} \times \text{scale\_factor} \right\rfloor</source>
          <target state="translated">D_{out}=D_{in}......................</target>
        </trans-unit>
        <trans-unit id="5daaf33316a5a1e996b64f9e865fa2c06a216412" translate="yes" xml:space="preserve">
          <source>D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor</source>
          <target state="translated">D_{out}=/left/lfloor/frac{D_{in}+2 /times /text{padding}[0]-/text{dilation}[0]/times (text{kernel_size}[0]-1)-1}{text{stride}[0]}+1right /rfloor</target>
        </trans-unit>
        <trans-unit id="a58eeddbe8ce176e982a4bd5546958165921baf4" translate="yes" xml:space="preserve">
          <source>D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor</source>
          <target state="translated">D_{out}=\left\lfloor/frac{D_{in}+2 \times \text{padding}[0]-\text{kernel_size}[0]}{text{stride}[0]}+1 \right\rfloor</target>
        </trans-unit>
        <trans-unit id="58d17582a05d81dbc970a2fb5dde4c86c385cf6c" translate="yes" xml:space="preserve">
          <source>Danger</source>
          <target state="translated">Danger</target>
        </trans-unit>
        <trans-unit id="543cf269e3398d54432632170f9d8a44ec8a82d9" translate="yes" xml:space="preserve">
          <source>Data Loading Order and Sampler</source>
          <target state="translated">数据加载顺序和采样器</target>
        </trans-unit>
        <trans-unit id="faa811b7a6757fc89dec83a4445037d0b6d29fe4" translate="yes" xml:space="preserve">
          <source>Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset.</source>
          <target state="translated">数据加载器。结合一个数据集和一个采样器,并在给定的数据集上提供一个可迭代的数据。</target>
        </trans-unit>
        <trans-unit id="ee503fe5765b8d9456bf21def7f9e06d2b12ebb6" translate="yes" xml:space="preserve">
          <source>Data type</source>
          <target state="translated">数据类型</target>
        </trans-unit>
        <trans-unit id="5e3a2e3c18839bb47fb3084db81d051ea2d9e572" translate="yes" xml:space="preserve">
          <source>DataParallel</source>
          <target state="translated">DataParallel</target>
        </trans-unit>
        <trans-unit id="f41afa14a2956bd6dee2529d81450b9fde84c7bb" translate="yes" xml:space="preserve">
          <source>DataParallel Layers (multi-GPU, distributed)</source>
          <target state="translated">数据并行层(多GPU,分布式)</target>
        </trans-unit>
        <trans-unit id="2c2866ebc153b9ba5ee12510871be4ea8e2d66ad" translate="yes" xml:space="preserve">
          <source>DataParallel functions (multi-GPU, distributed)</source>
          <target state="translated">DataParallel函数(多GPU,分布式)</target>
        </trans-unit>
        <trans-unit id="6f795ae6a697849179693a4d0d952f51dfe274a9" translate="yes" xml:space="preserve">
          <source>Dataset Types</source>
          <target state="translated">数据集类型</target>
        </trans-unit>
        <trans-unit id="a759fa5cf358fa4e839fa020614e6a71debce953" translate="yes" xml:space="preserve">
          <source>Dataset as a concatenation of multiple datasets.</source>
          <target state="translated">数据集作为多个数据集的连接。</target>
        </trans-unit>
        <trans-unit id="3eac6174468e8a6ce464f86a0d629040e97fe409" translate="yes" xml:space="preserve">
          <source>Dataset for chainning multiple &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt;&lt;code&gt;IterableDataset&lt;/code&gt;&lt;/a&gt; s.</source>
          <target state="translated">用于链接多个&lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt; &lt;code&gt;IterableDataset&lt;/code&gt; &lt;/a&gt;的数据集。</target>
        </trans-unit>
        <trans-unit id="63231ef80d6022f27be0ddfb59b1c8c16d68db48" translate="yes" xml:space="preserve">
          <source>Dataset is assumed to be of constant size.</source>
          <target state="translated">假设数据集的大小不变。</target>
        </trans-unit>
        <trans-unit id="6935d46588944d8c25616ca6972db5257d686341" translate="yes" xml:space="preserve">
          <source>Dataset wrapping tensors.</source>
          <target state="translated">数据集包装张力。</target>
        </trans-unit>
        <trans-unit id="75c05e642a4a82474f0ded1d73a7d77a7cab17de" translate="yes" xml:space="preserve">
          <source>DeQuantize</source>
          <target state="translated">DeQuantize</target>
        </trans-unit>
        <trans-unit id="895b27c88016513d278a0ce3dc0663fae3829d58" translate="yes" xml:space="preserve">
          <source>Debugging</source>
          <target state="translated">Debugging</target>
        </trans-unit>
        <trans-unit id="2d9ba523f6bd6f1366ef31447c6896de739c5ebc" translate="yes" xml:space="preserve">
          <source>Debugging this script with &lt;code&gt;pdb&lt;/code&gt; works except for when we invoke the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt; function. We can globally disable JIT, so that we can call the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt; function as a normal Python function and not compile it. If the above script is called &lt;code&gt;disable_jit_example.py&lt;/code&gt;, we can invoke it like so:</source>
          <target state="translated">使用 &lt;code&gt;pdb&lt;/code&gt; 调试此脚本的工作原理除外，除非我们调用&lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;@torch.jit.script&lt;/code&gt; &lt;/a&gt;函数。我们可以全局禁用JIT，以便我们可以将&lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;@torch.jit.script&lt;/code&gt; &lt;/a&gt;函数作为普通的Python函数调用而不进行编译。如果上面的脚本称为 &lt;code&gt;disable_jit_example.py&lt;/code&gt; ，我们可以这样调用它：</target>
        </trans-unit>
        <trans-unit id="20487077b8cc9537e212490b180a9c77a92fedaf" translate="yes" xml:space="preserve">
          <source>Debugging utilities</source>
          <target state="translated">调试工具</target>
        </trans-unit>
        <trans-unit id="cdeed230ed1d975e42532ba4eb68e61538b14c37" translate="yes" xml:space="preserve">
          <source>Decays the learning rate of each parameter group by gamma every epoch. When last_epoch=-1, sets initial lr as lr.</source>
          <target state="translated">每一个纪元将每个参数组的学习率以gamma的方式递减,当last_epoch=-1时,将初始lr设置为lr。当last_epoch=-1时,设置初始lr为lr。</target>
        </trans-unit>
        <trans-unit id="68df60584336645f5d49c32e82cf477c01c234ca" translate="yes" xml:space="preserve">
          <source>Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.</source>
          <target state="translated">每隔一个step_size epochs,将每个参数组的学习率按gamma衰减。请注意,这种衰减可以与其他来自这个调度器外部的学习率变化同时发生。当last_epoch=-1时,设置初始lr为lr。</target>
        </trans-unit>
        <trans-unit id="65627d1ff3ccfff17f931ea2b13d4278cbbe2474" translate="yes" xml:space="preserve">
          <source>Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.</source>
          <target state="translated">一旦纪元数达到其中一个里程碑,每个参数组的学习率就会按gamma衰减。请注意,这种衰减可以与其他来自这个调度器外部的学习率变化同时发生。当last_epoch=-1时,设置初始lr为lr。</target>
        </trans-unit>
        <trans-unit id="9f7b680b643589b9ed8668bf524f99287df6399c" translate="yes" xml:space="preserve">
          <source>Decodes a DLPack to a tensor.</source>
          <target state="translated">将一个DLPack解码为一个张量。</target>
        </trans-unit>
        <trans-unit id="dceee5b011b77632a7ffb29ac384f66f1112b455" translate="yes" xml:space="preserve">
          <source>Decorator to register a pairwise function with &lt;a href=&quot;#torch.distributions.kl.kl_divergence&quot;&gt;&lt;code&gt;kl_divergence()&lt;/code&gt;&lt;/a&gt;. Usage:</source>
          <target state="translated">装饰器使用&lt;a href=&quot;#torch.distributions.kl.kl_divergence&quot;&gt; &lt;code&gt;kl_divergence()&lt;/code&gt; &lt;/a&gt;注册成对函数。用法：</target>
        </trans-unit>
        <trans-unit id="bbe3208131f8562d117c292d268d2f8db716693e" translate="yes" xml:space="preserve">
          <source>DeepLabV3</source>
          <target state="translated">DeepLabV3</target>
        </trans-unit>
        <trans-unit id="67529023df524563878c2ac2647a317ad8500070" translate="yes" xml:space="preserve">
          <source>DeepLabV3 ResNet101</source>
          <target state="translated">DeepLabV3 ResNet101</target>
        </trans-unit>
        <trans-unit id="617f04b5a03ab100dd0d3e0e5532b5eb50903269" translate="yes" xml:space="preserve">
          <source>DeepLabV3 ResNet50</source>
          <target state="translated">DeepLabV3 ResNet50</target>
        </trans-unit>
        <trans-unit id="d2b8f3a731cc6350b5d6b3c7afddb1acf6006314" translate="yes" xml:space="preserve">
          <source>DeepLabV3 ResNet50, ResNet101</source>
          <target state="translated">DeepLabV3 ResNet50、ResNet101</target>
        </trans-unit>
        <trans-unit id="8c8189f5b90e5b1272f55bc887fde7008a4bafdf" translate="yes" xml:space="preserve">
          <source>Default Types</source>
          <target state="translated">默认类型</target>
        </trans-unit>
        <trans-unit id="2d116b23402cf7d0664bd12bb48874727780a4ac" translate="yes" xml:space="preserve">
          <source>Default evaluation function takes a torch.utils.data.Dataset or a list of input Tensors and run the model on the dataset</source>
          <target state="translated">默认的评估函数接收一个 torch.utils.data.Dataset 或一个输入 Tensors 列表,并在数据集上运行模型。</target>
        </trans-unit>
        <trans-unit id="f410bf7d7a6034879436d17d9eeb5c8f8ee302f3" translate="yes" xml:space="preserve">
          <source>Default gradient layouts</source>
          <target state="translated">默认梯度布局</target>
        </trans-unit>
        <trans-unit id="e51881f10b051af0af29bc3e379261ed831a6098" translate="yes" xml:space="preserve">
          <source>Default is &lt;code&gt;&quot;backward&quot;&lt;/code&gt; (no normalization).</source>
          <target state="translated">默认值为 &lt;code&gt;&quot;backward&quot;&lt;/code&gt; （不规范化）。</target>
        </trans-unit>
        <trans-unit id="0e0fda91e6a2713611da6dc3cb2473efcd25b651" translate="yes" xml:space="preserve">
          <source>Default is &lt;code&gt;&quot;backward&quot;&lt;/code&gt; (normalize by &lt;code&gt;1/n&lt;/code&gt;).</source>
          <target state="translated">默认值为 &lt;code&gt;&quot;backward&quot;&lt;/code&gt; （标准化为 &lt;code&gt;1/n&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="8e4f64360906e7f1452e75f1085dc507b2bff801" translate="yes" xml:space="preserve">
          <source>Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="translated">默认值： &lt;code&gt;None&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="be8bdbcf95f1b2c543f6b5f2c72d836f7c5b5704" translate="yes" xml:space="preserve">
          <source>Defaults to zero if not provided. where</source>
          <target state="translated">如果没有提供,默认为0。</target>
        </trans-unit>
        <trans-unit id="0c57e9446082424cd876b20c90b07486b438b9b9" translate="yes" xml:space="preserve">
          <source>Define the symbolic function in &lt;code&gt;torch/onnx/symbolic_opset&amp;lt;version&amp;gt;.py&lt;/code&gt;, for example &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py&quot;&gt;torch/onnx/symbolic_opset9.py&lt;/a&gt;. Make sure the function has the same name as the ATen operator/function defined in &lt;code&gt;VariableType.h&lt;/code&gt;.</source>
          <target state="translated">在 &lt;code&gt;torch/onnx/symbolic_opset&amp;lt;version&amp;gt;.py&lt;/code&gt; 定义符号功能，例如&lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py&quot;&gt;torch / onnx / symbolic_opset9.py&lt;/a&gt;。确保函数具有与 &lt;code&gt;VariableType.h&lt;/code&gt; 中定义的ATen运算符/函数相同的名称。</target>
        </trans-unit>
        <trans-unit id="4cec63288c88c513e15dd89936be8dc37543d14b" translate="yes" xml:space="preserve">
          <source>Defines a formula for differentiating the operation.</source>
          <target state="translated">定义了一个区分操作的公式。</target>
        </trans-unit>
        <trans-unit id="7773324d0fbb19d2b097125eeec23bba37fd9fae" translate="yes" xml:space="preserve">
          <source>Defines the computation performed at every call.</source>
          <target state="translated">定义每次调用时执行的计算。</target>
        </trans-unit>
        <trans-unit id="9fdfda877e82e2aeb2ae17e08e28fcbd6d18a121" translate="yes" xml:space="preserve">
          <source>Deletes the key-value pair associated with &lt;code&gt;key&lt;/code&gt; from the store. Returns &lt;code&gt;true&lt;/code&gt; if the key was successfully deleted, and &lt;code&gt;false&lt;/code&gt; if it was not.</source>
          <target state="translated">从商店中删除与键关联的 &lt;code&gt;key&lt;/code&gt; 对。如果成功删除密钥，则返回 &lt;code&gt;true&lt;/code&gt; ;否则，返回 &lt;code&gt;false&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="fa7b4a50c0e0f2cffa01451426972ab2f487a9dd" translate="yes" xml:space="preserve">
          <source>DenseNet</source>
          <target state="translated">DenseNet</target>
        </trans-unit>
        <trans-unit id="8b2ff8d2942b70f38d615ed1e52ba52e2285edab" translate="yes" xml:space="preserve">
          <source>Densenet-121</source>
          <target state="translated">Densenet-121</target>
        </trans-unit>
        <trans-unit id="d26e787bd56b9c7aab1c534a7f7ead9ab4b12d73" translate="yes" xml:space="preserve">
          <source>Densenet-121 model from &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;&amp;ldquo;Densely Connected Convolutional Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&amp;ldquo;密集&lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;连接的卷积网络&amp;rdquo;中的&lt;/a&gt;Densenet-121模型</target>
        </trans-unit>
        <trans-unit id="dc79947b23872b66c941b9c7a01bd0904908a8d9" translate="yes" xml:space="preserve">
          <source>Densenet-161</source>
          <target state="translated">Densenet-161</target>
        </trans-unit>
        <trans-unit id="8079b2939275d34b5b7b870ad8e0a1f7fe8edd0d" translate="yes" xml:space="preserve">
          <source>Densenet-161 model from &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;&amp;ldquo;Densely Connected Convolutional Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&amp;ldquo;密集&lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;连接的卷积网络&amp;rdquo;中的&lt;/a&gt;Densenet-161模型</target>
        </trans-unit>
        <trans-unit id="f8e6fd5af55a790890421b436f68cbc8406a0b19" translate="yes" xml:space="preserve">
          <source>Densenet-169</source>
          <target state="translated">Densenet-169</target>
        </trans-unit>
        <trans-unit id="7480a230cf176343f7e3077f6904ede5855a19d8" translate="yes" xml:space="preserve">
          <source>Densenet-169 model from &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;&amp;ldquo;Densely Connected Convolutional Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&amp;ldquo;密集&lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;连接的卷积网络&amp;rdquo;中的&lt;/a&gt;Densenet-169模型</target>
        </trans-unit>
        <trans-unit id="fc80ee1a1bf823e3bf3be2c3a3194f577392f377" translate="yes" xml:space="preserve">
          <source>Densenet-201</source>
          <target state="translated">Densenet-201</target>
        </trans-unit>
        <trans-unit id="7124ecd6d83a80564dc1ba72e45a576f118ac991" translate="yes" xml:space="preserve">
          <source>Densenet-201 model from &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;&amp;ldquo;Densely Connected Convolutional Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&amp;ldquo;密集&lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;连接的卷积网络&amp;rdquo;中的&lt;/a&gt;Densenet-201模型</target>
        </trans-unit>
        <trans-unit id="7a89d9b0077197cdfbba3a66cddb06012c599011" translate="yes" xml:space="preserve">
          <source>Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;cross-correlation&lt;/a&gt;, and not a full &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;cross-correlation&lt;/a&gt;. It is up to the user to add proper padding.</source>
          <target state="translated">根据内核的大小，输入的（最后一列）几列可能会丢失，因为这是有效的&lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;互相关&lt;/a&gt;，而不是完全&lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;互相关&lt;/a&gt;。由用户决定是否添加适当的填充。</target>
        </trans-unit>
        <trans-unit id="7a0685deb3b8e4521a47904d7970aabb4610a92a" translate="yes" xml:space="preserve">
          <source>Depending on the custom operator, you can export it as one or a combination of existing ONNX ops. You can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain and version (custom opset) using the &lt;code&gt;custom_opsets&lt;/code&gt; dictionary at export. If not explicitly specified, the custom opset version is set to 1 by default. Using custom ONNX ops, you will need to extend the backend of your choice with matching custom ops implementation, e.g. &lt;a href=&quot;https://caffe2.ai/docs/custom-operators.html&quot;&gt;Caffe2 custom ops&lt;/a&gt;, &lt;a href=&quot;https://github.com/microsoft/onnxruntime/blob/master/docs/AddingCustomOp.md&quot;&gt;ONNX Runtime custom ops&lt;/a&gt;.</source>
          <target state="translated">根据自定义运算符的不同，您可以将其导出为现有ONNX操作之一或组合。您也可以将其导出为ONNX中的自定义操作。在这种情况下，可以在导出时使用 &lt;code&gt;custom_opsets&lt;/code&gt; 词典指定自定义域和版本（自定义opset）。如果未明确指定，则默认情况下将自定义opset版本设置为1。使用自定义ONNX操作，您将需要通过匹配的自定义操作实现扩展选择的后端，例如&lt;a href=&quot;https://caffe2.ai/docs/custom-operators.html&quot;&gt;Caffe2自定义操作&lt;/a&gt;，&lt;a href=&quot;https://github.com/microsoft/onnxruntime/blob/master/docs/AddingCustomOp.md&quot;&gt;ONNX运行时自定义操作&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0ab688461ca113091fd169e55e7c9e66deb886f0" translate="yes" xml:space="preserve">
          <source>Deprecated enum-like class for reduction operations: &lt;code&gt;SUM&lt;/code&gt;, &lt;code&gt;PRODUCT&lt;/code&gt;, &lt;code&gt;MIN&lt;/code&gt;, and &lt;code&gt;MAX&lt;/code&gt;.</source>
          <target state="translated">减少操作的不推荐枚举类： &lt;code&gt;SUM&lt;/code&gt; ， &lt;code&gt;PRODUCT&lt;/code&gt; ， &lt;code&gt;MIN&lt;/code&gt; 和 &lt;code&gt;MAX&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4995eb29b7f6d3dc7cac42d9f08dad19e95a9dfa" translate="yes" xml:space="preserve">
          <source>Deprecated; see &lt;a href=&quot;#torch.cuda.max_memory_reserved&quot;&gt;&lt;code&gt;max_memory_reserved()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">不推荐使用；参见&lt;a href=&quot;#torch.cuda.max_memory_reserved&quot;&gt; &lt;code&gt;max_memory_reserved()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e046da60da747b4d87f8d67259afaf6b9fe07bf5" translate="yes" xml:space="preserve">
          <source>Deprecated; see &lt;a href=&quot;#torch.cuda.memory_reserved&quot;&gt;&lt;code&gt;memory_reserved()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">不推荐使用；参见&lt;a href=&quot;#torch.cuda.memory_reserved&quot;&gt; &lt;code&gt;memory_reserved()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="15cdb030ac617cabaf72e8f3099d5d7b07f0d492" translate="yes" xml:space="preserve">
          <source>Dequantize stub module, before calibration, this is same as identity, this will be swapped as &lt;code&gt;nnq.DeQuantize&lt;/code&gt; in &lt;code&gt;convert&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;nnq.DeQuantize&lt;/code&gt; 存根模块，在校准之前，这与标识相同，将在 &lt;code&gt;convert&lt;/code&gt; 中交换为nnq.DeQuantize。</target>
        </trans-unit>
        <trans-unit id="44fd5a225ad67410d4630aa610f55ced5a7a7c71" translate="yes" xml:space="preserve">
          <source>Dequantizes an incoming tensor</source>
          <target state="translated">对传入的张量进行去量化</target>
        </trans-unit>
        <trans-unit id="2a6b986891e70c721e528fb3141e3feb4bc76505" translate="yes" xml:space="preserve">
          <source>Derived classes should implement one or both of &lt;code&gt;_call()&lt;/code&gt; or &lt;code&gt;_inverse()&lt;/code&gt;. Derived classes that set &lt;code&gt;bijective=True&lt;/code&gt; should also implement &lt;a href=&quot;#torch.distributions.transforms.Transform.log_abs_det_jacobian&quot;&gt;&lt;code&gt;log_abs_det_jacobian()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">派生类应实现 &lt;code&gt;_call()&lt;/code&gt; 或 &lt;code&gt;_inverse()&lt;/code&gt; 之一或两者。设置&lt;a href=&quot;#torch.distributions.transforms.Transform.log_abs_det_jacobian&quot;&gt; &lt;code&gt;log_abs_det_jacobian()&lt;/code&gt; &lt;/a&gt; &lt;code&gt;bijective=True&lt;/code&gt; 派生类也应实现log_abs_det_jacobian（）。</target>
        </trans-unit>
        <trans-unit id="516c22b47b1fe91311a530710a5d3d144b2ada06" translate="yes" xml:space="preserve">
          <source>Describe an instantaneous event that occurred at some point.</source>
          <target state="translated">描述在某个时刻发生的瞬间事件。</target>
        </trans-unit>
        <trans-unit id="8e0a2537499a2a7aaa95e9b6ce35838ddd92db58" translate="yes" xml:space="preserve">
          <source>Describes how to dynamically quantize a layer or a part of the network by providing settings (observer classes) for weights.</source>
          <target state="translated">描述了如何通过提供权重的设置(观察者类)来动态量化一层或网络的一部分。</target>
        </trans-unit>
        <trans-unit id="fd0e609a210b25ce30494d69de9681e62976242f" translate="yes" xml:space="preserve">
          <source>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</source>
          <target state="translated">描述了如何通过为激活和权重分别提供设置(观察者类)来量化一层或网络的一部分。</target>
        </trans-unit>
        <trans-unit id="55f8ebc805e65b5b71ddafdae390e3be2bcd69af" translate="yes" xml:space="preserve">
          <source>Description</source>
          <target state="translated">Description</target>
        </trans-unit>
        <trans-unit id="5966dbc5c4d197355b50f3dc66783c2fa78a5226" translate="yes" xml:space="preserve">
          <source>Design Notes</source>
          <target state="translated">设计说明</target>
        </trans-unit>
        <trans-unit id="fbe406308e1df41bfda74c701d4a66a5af72c8b6" translate="yes" xml:space="preserve">
          <source>Design Reasoning</source>
          <target state="translated">设计推理</target>
        </trans-unit>
        <trans-unit id="91441a05e70cda570458aad59617766bb3ed7236" translate="yes" xml:space="preserve">
          <source>Detaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place.</source>
          <target state="translated">将张量图从创建它的图中分离出来,使其成为叶子。视图不能就地分离。</target>
        </trans-unit>
        <trans-unit id="0e01337242e0297823f889a52bc9dad47f00c9ad" translate="yes" xml:space="preserve">
          <source>Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;documentation&lt;/a&gt;.</source>
          <target state="translated">确定在类型提升&lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;文档中&lt;/a&gt;描述的PyTorch转换规则下是否允许类型转换。</target>
        </trans-unit>
        <trans-unit id="d2244fb2618c9d05384c145a55a7a5a37bf99078" translate="yes" xml:space="preserve">
          <source>Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion &lt;a href=&quot;tensor_attributes#type-promotion-doc&quot;&gt;documentation&lt;/a&gt;.</source>
          <target state="translated">确定在类型提升&lt;a href=&quot;tensor_attributes#type-promotion-doc&quot;&gt;文档中&lt;/a&gt;描述的PyTorch转换规则下是否允许类型转换。</target>
        </trans-unit>
        <trans-unit id="a5a74a6df09278b88cb6ea23b7d7f2570c33babf" translate="yes" xml:space="preserve">
          <source>Device</source>
          <target state="translated">Device</target>
        </trans-unit>
        <trans-unit id="1c1f67e2f072a5af5bf17679821b62f151e69437" translate="yes" xml:space="preserve">
          <source>Dict Construction</source>
          <target state="translated">口述建筑</target>
        </trans-unit>
        <trans-unit id="4461566599c5b88c3897f351e4ae4b0ddc655116" translate="yes" xml:space="preserve">
          <source>Different from the standard SVD, the size of returned matrices depend on the specified rank and q values as follows:</source>
          <target state="translated">与标准的SVD不同,返回矩阵的大小取决于指定的秩和q值,具体如下。</target>
        </trans-unit>
        <trans-unit id="abe2291b9a77a8ae7585fd7e4b1df0a06016b998" translate="yes" xml:space="preserve">
          <source>Dimension names may contain characters or underscore. Furthermore, a dimension name must be a valid Python variable name (i.e., does not start with underscore).</source>
          <target state="translated">维度名称可以包含字符或下划线。此外,维度名称必须是有效的Python变量名 (即,不以下划线开始)。</target>
        </trans-unit>
        <trans-unit id="a6f1bd13e9cbff9087ce39dcba569ea9ce48c700" translate="yes" xml:space="preserve">
          <source>Dirichlet</source>
          <target state="translated">Dirichlet</target>
        </trans-unit>
        <trans-unit id="e1cf09d41f0116a87479fbe7f5ef39aeac24297b" translate="yes" xml:space="preserve">
          <source>Disable JIT for Debugging</source>
          <target state="translated">禁用JIT进行调试</target>
        </trans-unit>
        <trans-unit id="35a71645b1a4bfec2d328e91748bd5d190545dea" translate="yes" xml:space="preserve">
          <source>Disable automatic batching</source>
          <target state="translated">禁用自动批处理</target>
        </trans-unit>
        <trans-unit id="dae5511126cad0d55eede647161bef3c8ecf5358" translate="yes" xml:space="preserve">
          <source>Disables denormal floating numbers on CPU.</source>
          <target state="translated">禁用CPU上的反常态浮点数。</target>
        </trans-unit>
        <trans-unit id="c513ea041a153d26658bb6cf95f8c77cf39ac2f0" translate="yes" xml:space="preserve">
          <source>Disabling gradient calculation is useful for inference, when you are sure that you will not call &lt;a href=&quot;../autograd#torch.Tensor.backward&quot;&gt;&lt;code&gt;Tensor.backward()&lt;/code&gt;&lt;/a&gt;. It will reduce memory consumption for computations that would otherwise have &lt;code&gt;requires_grad=True&lt;/code&gt;.</source>
          <target state="translated">当您确定不会调用&lt;a href=&quot;../autograd#torch.Tensor.backward&quot;&gt; &lt;code&gt;Tensor.backward()&lt;/code&gt; &lt;/a&gt;时，禁用梯度计算对于推断很有用。它将减少原本 &lt;code&gt;requires_grad=True&lt;/code&gt; 计算的内存消耗。</target>
        </trans-unit>
        <trans-unit id="d2f39154ff5c9f1589f29bf3daf21678e81210ff" translate="yes" xml:space="preserve">
          <source>Disabling gradient calculation is useful for inference, when you are sure that you will not call &lt;code&gt;Tensor.backward()&lt;/code&gt;. It will reduce memory consumption for computations that would otherwise have &lt;code&gt;requires_grad=True&lt;/code&gt;.</source>
          <target state="translated">当您确定不会调用 &lt;code&gt;Tensor.backward()&lt;/code&gt; 时，禁用梯度计算对于推断很有用。它将减少原本 &lt;code&gt;requires_grad=True&lt;/code&gt; 计算的内存消耗。</target>
        </trans-unit>
        <trans-unit id="f9c83b5259f966caf231a71b3ad8e69e880665ed" translate="yes" xml:space="preserve">
          <source>Discrete Fourier transforms and related functions.</source>
          <target state="translated">离散傅立叶变换及相关函数。</target>
        </trans-unit>
        <trans-unit id="938004e21f3f3e82099132266fc1891a7f13de20" translate="yes" xml:space="preserve">
          <source>Distance Functions</source>
          <target state="translated">距离函数</target>
        </trans-unit>
        <trans-unit id="9cde45ececcca1e1adb41659d3d28a0420087495" translate="yes" xml:space="preserve">
          <source>Distance functions</source>
          <target state="translated">距离功能</target>
        </trans-unit>
        <trans-unit id="303a9c010db654973c68f36cc8c79b5ce73fe308" translate="yes" xml:space="preserve">
          <source>Distributed Autograd Context</source>
          <target state="translated">分布式自动识别上下文</target>
        </trans-unit>
        <trans-unit id="310713a581eec3788cf900c68063e38885f001bd" translate="yes" xml:space="preserve">
          <source>Distributed Autograd Design</source>
          <target state="translated">分布式自创设计</target>
        </trans-unit>
        <trans-unit id="fa35316634dcb2108118798dd9f61f3d3f7fbb66" translate="yes" xml:space="preserve">
          <source>Distributed Autograd Framework</source>
          <target state="translated">分布式Autograd框架</target>
        </trans-unit>
        <trans-unit id="ec9dd95759950a78e8ade825f089a889c2eb7fab" translate="yes" xml:space="preserve">
          <source>Distributed Backward Pass</source>
          <target state="translated">分布式后向传球</target>
        </trans-unit>
        <trans-unit id="aba915801825e438dbb9e75721a94e96b234ba7d" translate="yes" xml:space="preserve">
          <source>Distributed Data Parallel</source>
          <target state="translated">分布式数据并行</target>
        </trans-unit>
        <trans-unit id="ef10fda1ae05d95a37d1371a4a918aad1148a2da" translate="yes" xml:space="preserve">
          <source>Distributed Key-Value Store</source>
          <target state="translated">分布式键值存储</target>
        </trans-unit>
        <trans-unit id="34eb6d25d260c57a8faa37ee402539b363513315" translate="yes" xml:space="preserve">
          <source>Distributed Optimizer</source>
          <target state="translated">分布式优化器</target>
        </trans-unit>
        <trans-unit id="75f572c75699c1008b8a3d33982f73a57479f298" translate="yes" xml:space="preserve">
          <source>Distributed Pipeline Parallel</source>
          <target state="translated">分布式管道并行</target>
        </trans-unit>
        <trans-unit id="5dfffd4675ffa868d517cf4ad2d7b981131e1dbd" translate="yes" xml:space="preserve">
          <source>Distributed RPC Framework</source>
          <target state="translated">分布式RPC框架</target>
        </trans-unit>
        <trans-unit id="e3de08dea5da9f0cffd62432ca7da9726c39e218" translate="yes" xml:space="preserve">
          <source>Distributed communication package - torch.distributed</source>
          <target state="translated">分布式通信包-torch.distributed</target>
        </trans-unit>
        <trans-unit id="67b250f513d5c0c4bc692b9447f357fc7ef369ea" translate="yes" xml:space="preserve">
          <source>DistributedDataParallel</source>
          <target state="translated">DistributedDataParallel</target>
        </trans-unit>
        <trans-unit id="ec463333beac30f92377ae176f21cc4eb4b46520" translate="yes" xml:space="preserve">
          <source>DistributedOptimizer takes remote references to parameters scattered across workers and applies the given optimizer locally for each parameter.</source>
          <target state="translated">分布式优化器(DistributedOptimizer)接收分散在各个工位上的参数的远程引用,并为每个参数在本地应用给定的优化器。</target>
        </trans-unit>
        <trans-unit id="1d3c457cbe3e35739086346e0a4048653efa42e7" translate="yes" xml:space="preserve">
          <source>Distribution</source>
          <target state="translated">Distribution</target>
        </trans-unit>
        <trans-unit id="c0fd2f6b48b7ad3974231443cb830eddeebb455e" translate="yes" xml:space="preserve">
          <source>Distribution is the abstract base class for probability distributions.</source>
          <target state="translated">分布是概率分布的抽象基类。</target>
        </trans-unit>
        <trans-unit id="56af6c440851565952f28972260908c667af4940" translate="yes" xml:space="preserve">
          <source>Divides (&amp;ldquo;unscales&amp;rdquo;) the optimizer&amp;rsquo;s gradient tensors by the scale factor.</source>
          <target state="translated">将优化程序的梯度张量除（&amp;ldquo;无标度&amp;rdquo;）除以比例因子。</target>
        </trans-unit>
        <trans-unit id="e4468b3425489b998cab9185522d9f737628cd1d" translate="yes" xml:space="preserve">
          <source>Divides each element of the input &lt;code&gt;input&lt;/code&gt; by the corresponding element of &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">将输入 &lt;code&gt;input&lt;/code&gt; 的每个元素除以 &lt;code&gt;other&lt;/code&gt; 的相应元素。</target>
        </trans-unit>
        <trans-unit id="33968e4e64500a95f8035e27727c823cadd48d34" translate="yes" xml:space="preserve">
          <source>Do cartesian product of the given sequence of tensors.</source>
          <target state="translated">做给定序列的卡提斯积。</target>
        </trans-unit>
        <trans-unit id="721f0d8eda69e29b574fd64fe0f6c7674a1a6f89" translate="yes" xml:space="preserve">
          <source>Do cartesian product of the given sequence of tensors. The behavior is similar to python&amp;rsquo;s &lt;code&gt;itertools.product&lt;/code&gt;.</source>
          <target state="translated">给定张量序列的笛卡尔积。行为类似于python的 &lt;code&gt;itertools.product&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c4d30aee7a62cbfdaa3d84814f5fc6534da714e2" translate="yes" xml:space="preserve">
          <source>Do quantization aware training and output a quantized model</source>
          <target state="translated">做量化意识训练,输出量化模型。</target>
        </trans-unit>
        <trans-unit id="fcb94671187d8e10ddb359c9decbb94311e20300" translate="yes" xml:space="preserve">
          <source>Docstring of the function works as a help message. It explains what does the model do and what are the allowed positional/keyword arguments. It&amp;rsquo;s highly recommended to add a few examples here.</source>
          <target state="translated">该函数的文档字符串用作帮助消息。它解释了模型做什么以及允许的位置/关键字参数是什么。强烈建议在此处添加一些示例。</target>
        </trans-unit>
        <trans-unit id="96f57a29074dff5e315a68cd23e9cc22e3995e9f" translate="yes" xml:space="preserve">
          <source>Does a linear interpolation of two tensors &lt;code&gt;start&lt;/code&gt; (given by &lt;code&gt;input&lt;/code&gt;) and &lt;code&gt;end&lt;/code&gt; based on a scalar or tensor &lt;code&gt;weight&lt;/code&gt; and returns the resulting &lt;code&gt;out&lt;/code&gt; tensor.</source>
          <target state="translated">做两张量的线性内插 &lt;code&gt;start&lt;/code&gt; （由下式给出 &lt;code&gt;input&lt;/code&gt; ），并 &lt;code&gt;end&lt;/code&gt; 基于标量或张量 &lt;code&gt;weight&lt;/code&gt; 并返回生成的 &lt;code&gt;out&lt;/code&gt; 张量。</target>
        </trans-unit>
        <trans-unit id="8782c3b2687a1a4eba3fb6aace40c73bfe021afc" translate="yes" xml:space="preserve">
          <source>Does nothing if the CUDA state is already initialized.</source>
          <target state="translated">如果CUDA状态已经被初始化,则不做任何操作。</target>
        </trans-unit>
        <trans-unit id="525c9a2be6af3422648e9f02a6746e9df12f182d" translate="yes" xml:space="preserve">
          <source>Don&amp;rsquo;t pass received tensors.</source>
          <target state="translated">不要通过接收到的张量。</target>
        </trans-unit>
        <trans-unit id="e06ffaf662b8ea46a067dea9fb380262a87db6a2" translate="yes" xml:space="preserve">
          <source>Down/up samples the input to either the given &lt;code&gt;size&lt;/code&gt; or the given &lt;code&gt;scale_factor&lt;/code&gt;</source>
          <target state="translated">向下/向上采样输入到给定 &lt;code&gt;size&lt;/code&gt; 或给定的 &lt;code&gt;scale_factor&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="e61af53453cf013b5be43bae97307de95bad250e" translate="yes" xml:space="preserve">
          <source>Download object at the given URL to a local path.</source>
          <target state="translated">将给定URL中的对象下载到本地路径。</target>
        </trans-unit>
        <trans-unit id="361a0dc66d29b16c754d93724204f2ac9044c37d" translate="yes" xml:space="preserve">
          <source>Draws binary random numbers (0 or 1) from a Bernoulli distribution.</source>
          <target state="translated">从伯努利分布中抽取二进制随机数(0或1)。</target>
        </trans-unit>
        <trans-unit id="99a0d2b3ff3d729e26380413abca3d7df775cdde" translate="yes" xml:space="preserve">
          <source>Dropout</source>
          <target state="translated">Dropout</target>
        </trans-unit>
        <trans-unit id="132aa5f9595506ef0ca296d6be570b1e4aa7acb2" translate="yes" xml:space="preserve">
          <source>Dropout Layers</source>
          <target state="translated">辍学层</target>
        </trans-unit>
        <trans-unit id="15a66e933b5615cb5433ce9102df020458b34bcb" translate="yes" xml:space="preserve">
          <source>Dropout functions</source>
          <target state="translated">辍学功能</target>
        </trans-unit>
        <trans-unit id="cfe20a4021b31e2ea913c19efd2987a230ac723c" translate="yes" xml:space="preserve">
          <source>Dropout2d</source>
          <target state="translated">Dropout2d</target>
        </trans-unit>
        <trans-unit id="830971c9b2da3b705ee96a71f9d0d4e5128e9bb7" translate="yes" xml:space="preserve">
          <source>Dropout3d</source>
          <target state="translated">Dropout3d</target>
        </trans-unit>
        <trans-unit id="a7c8e330062301237ad2aac63813f57bd9bb1e14" translate="yes" xml:space="preserve">
          <source>Due to limited dynamic range of half datatype, performing this operation in half precision may cause the first element of result to overflow for certain inputs.</source>
          <target state="translated">由于半数据类型的动态范围有限,在半精度下执行此操作可能会导致某些输入的结果的第一个元素溢出。</target>
        </trans-unit>
        <trans-unit id="36b5672884c133b8c278456fa603d4322ce8f54a" translate="yes" xml:space="preserve">
          <source>Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time reports the amount of time used to launch the kernels but does not include the time the kernel spent executing on a GPU unless the operation does a synchronize. Ops that do synchronize appear to be extremely expensive under regular CPU-mode profilers. In these case where timings are incorrect, the CUDA-mode autograd profiler may be helpful.</source>
          <target state="translated">由于CUDA内核的异步性,当针对CUDA代码运行时,cProfile输出和CPU模式的autograd剖析器可能不会显示正确的时间:报告的CPU时间报告了用于启动内核的时间,但不包括内核在GPU上执行的时间,除非操作做了同步。在常规的CPU模式剖析器下,做同步的操作显得非常昂贵。在这些计时不正确的情况下,CUDA模式下的autograd剖析器可能会有帮助。</target>
        </trans-unit>
        <trans-unit id="aa6e674ff3a01dbf141329517674a02a51f4e055" translate="yes" xml:space="preserve">
          <source>Due to the conjugate symmetry, &lt;code&gt;input&lt;/code&gt; do not need to contain the full complex frequency values. Roughly half of the values will be sufficient, as is the case when &lt;code&gt;input&lt;/code&gt; is given by &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;rfft(signal, onesided=True)&lt;/code&gt;. In such case, set the &lt;code&gt;onesided&lt;/code&gt; argument of this method to &lt;code&gt;True&lt;/code&gt;. Moreover, the original signal shape information can sometimes be lost, optionally set &lt;code&gt;signal_sizes&lt;/code&gt; to be the size of the original signal (without the batch dimensions if in batched mode) to recover it with correct shape.</source>
          <target state="translated">由于共轭对称性， &lt;code&gt;input&lt;/code&gt; 不需要包含完整的复数频率值。大约一半的值就足够了，就像 &lt;code&gt;input&lt;/code&gt; 由&lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt;和 &lt;code&gt;rfft(signal, onesided=True)&lt;/code&gt; 给出输入的情况一样。在这种情况下，请将此方法的 &lt;code&gt;onesided&lt;/code&gt; 参数设置为 &lt;code&gt;True&lt;/code&gt; 。此外，原始信号形状信息有时可能会丢失，可以选择将 &lt;code&gt;signal_sizes&lt;/code&gt; 设置为原始信号的大小（如果处于批处理模式，则没有批处理尺寸），以恢复正确的形状。</target>
        </trans-unit>
        <trans-unit id="3f628f956c51a607f601ca3a9c65aebac60f6d34" translate="yes" xml:space="preserve">
          <source>Duplicate modules are returned only once. In the following example, &lt;code&gt;l&lt;/code&gt; will be returned only once.</source>
          <target state="translated">重复的模块仅返回一次。在下面的示例中， &lt;code&gt;l&lt;/code&gt; 将仅返回一次。</target>
        </trans-unit>
        <trans-unit id="50fa35c3d84d578bb945e995330eef7c63dcf91c" translate="yes" xml:space="preserve">
          <source>During backward, only gradients at &lt;code&gt;nnz&lt;/code&gt; locations of &lt;code&gt;input&lt;/code&gt; will propagate back. Note that the gradients of &lt;code&gt;input&lt;/code&gt; is coalesced.</source>
          <target state="translated">在向后期间，仅 &lt;code&gt;input&lt;/code&gt; &lt;code&gt;nnz&lt;/code&gt; 位置处的梯度将向后传播。请注意， &lt;code&gt;input&lt;/code&gt; 的渐变是合并的。</target>
        </trans-unit>
        <trans-unit id="8e08ad58135bf66f71d67cdf9e3eb42be9cdd416" translate="yes" xml:space="preserve">
          <source>During evaluation the module simply computes an identity function.</source>
          <target state="translated">在评估过程中,该模块只需计算一个身份函数。</target>
        </trans-unit>
        <trans-unit id="c42cc58336b9bff21da008fbb38ac485f6938f2c" translate="yes" xml:space="preserve">
          <source>During inference, the model requires only the input tensors, and returns the post-processed predictions as a &lt;code&gt;List[Dict[Tensor]]&lt;/code&gt;, one for each input image. The fields of the &lt;code&gt;Dict&lt;/code&gt; are as follows:</source>
          <target state="translated">在推理期间，模型仅需要输入张量，并以 &lt;code&gt;List[Dict[Tensor]]&lt;/code&gt; 返回后处理的预测，每个输入图像一个。 &lt;code&gt;Dict&lt;/code&gt; 的字段如下：</target>
        </trans-unit>
        <trans-unit id="8d654e950cd9f5e68da4c08caa15327df6055306" translate="yes" xml:space="preserve">
          <source>During the forward pass, each function range is decorated with &lt;code&gt;seq=&amp;lt;N&amp;gt;&lt;/code&gt;. &lt;code&gt;seq&lt;/code&gt; is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the &lt;code&gt;seq=&amp;lt;N&amp;gt;&lt;/code&gt; annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Function&amp;rsquo;s &lt;code&gt;apply()&lt;/code&gt; call is decorated with &lt;code&gt;stashed seq=&amp;lt;M&amp;gt;&lt;/code&gt;. &lt;code&gt;M&lt;/code&gt; is the sequence number that the backward object was created with. By comparing &lt;code&gt;stashed seq&lt;/code&gt; numbers in backward with &lt;code&gt;seq&lt;/code&gt; numbers in forward, you can track down which forward op created each backward Function.</source>
          <target state="translated">在前进过程中，每个功能范围都用 &lt;code&gt;seq=&amp;lt;N&amp;gt;&lt;/code&gt; 装饰。 &lt;code&gt;seq&lt;/code&gt; 是一个运行中的计数器，每次创建新的向后功能对象时都会递增，并藏起来以便向后倾斜。因此，与每个正向函数范围关联的 &lt;code&gt;seq=&amp;lt;N&amp;gt;&lt;/code&gt; 注释告诉您，如果此正向函数创建了一个反向函数对象，则该反向对象将接收序列号N。在反向传递过程中，顶级范围换行每个C ++向后函数的 &lt;code&gt;apply()&lt;/code&gt; 调用均以 &lt;code&gt;stashed seq=&amp;lt;M&amp;gt;&lt;/code&gt; 装饰。 &lt;code&gt;M&lt;/code&gt; 是创建向后对象的序列号。通过比较 &lt;code&gt;stashed seq&lt;/code&gt; 编号与 &lt;code&gt;seq&lt;/code&gt; 的向后 向前的数字，您可以跟踪哪个正向操作创建了每个向后的功能。</target>
        </trans-unit>
        <trans-unit id="899a3b765d6ee8d823d5852dbea555348e22fa78" translate="yes" xml:space="preserve">
          <source>During training, it randomly masks some of the elements of the input tensor with probability &lt;em&gt;p&lt;/em&gt; using samples from a bernoulli distribution. The elements to masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit standard deviation.</source>
          <target state="translated">在训练期间，它使用伯努利分布中的样本以概率&lt;em&gt;p&lt;/em&gt;随机掩盖输入张量的某些元素。在每个前向调用中，要屏蔽的元素都会随机化，并进行缩放和移位以保持零均值和单位标准差。</target>
        </trans-unit>
        <trans-unit id="1ddb72e8fb2bca4bbdd9cee1bd0f4124ba674fe1" translate="yes" xml:space="preserve">
          <source>During training, randomly zeroes some of the elements of the input tensor with probability &lt;code&gt;p&lt;/code&gt; using samples from a Bernoulli distribution.</source>
          <target state="translated">在训练过程中，使用伯努利分布的样本以概率 &lt;code&gt;p&lt;/code&gt; 将输入张量的某些元素随机置零。</target>
        </trans-unit>
        <trans-unit id="7e07f3de383a6b909b32c2c8420ab783da80d74b" translate="yes" xml:space="preserve">
          <source>During training, randomly zeroes some of the elements of the input tensor with probability &lt;code&gt;p&lt;/code&gt; using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.</source>
          <target state="translated">在训练过程中，使用伯努利分布的样本以概率 &lt;code&gt;p&lt;/code&gt; 将输入张量的某些元素随机置零。在每个前向呼叫中，每个通道将独立清零。</target>
        </trans-unit>
        <trans-unit id="e72931d7ae530922c7186be0d18873ff29d42369" translate="yes" xml:space="preserve">
          <source>During training, the model expects both the input tensors, as well as a targets (list of dictionary), containing:</source>
          <target state="translated">在训练过程中,模型既希望输入的腾博会登录,也希望有一个目标(字典列表),包含。</target>
        </trans-unit>
        <trans-unit id="e0184adedf913b076626646d3f52c3b49c39ad6d" translate="yes" xml:space="preserve">
          <source>E</source>
          <target state="translated">E</target>
        </trans-unit>
        <trans-unit id="a466f9bd6da8a16be528d1b97b22f8b80c50fc61" translate="yes" xml:space="preserve">
          <source>E (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;)</source>
          <target state="translated">E（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="db547439274fed58f2b9ee57841dc8931b457c07" translate="yes" xml:space="preserve">
          <source>ELU</source>
          <target state="translated">ELU</target>
        </trans-unit>
        <trans-unit id="c4cf619f3157e4a678ac53e564134cffe6210613" translate="yes" xml:space="preserve">
          <source>Each &lt;code&gt;torch.Tensor&lt;/code&gt; has a &lt;a href=&quot;#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.torch.layout&quot;&gt;&lt;code&gt;torch.layout&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">每个 &lt;code&gt;torch.Tensor&lt;/code&gt; 都有一个&lt;a href=&quot;#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;#torch.torch.layout&quot;&gt; &lt;code&gt;torch.layout&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="1c0427e3aafaddc6d9583e9cefbad6140b6d07cf" translate="yes" xml:space="preserve">
          <source>Each element of the tensor &lt;code&gt;input&lt;/code&gt; is multiplied by the corresponding element of the Tensor &lt;code&gt;other&lt;/code&gt;. The resulting tensor is returned.</source>
          <target state="translated">张量 &lt;code&gt;input&lt;/code&gt; 每个元素都与张量 &lt;code&gt;other&lt;/code&gt; 的对应元素相乘。返回结果张量。</target>
        </trans-unit>
        <trans-unit id="728738a25fa4324927c05f66f2cfc30782cc84df" translate="yes" xml:space="preserve">
          <source>Each element of the tensor &lt;code&gt;other&lt;/code&gt; is multiplied by the scalar &lt;code&gt;alpha&lt;/code&gt; and added to each element of the tensor &lt;code&gt;input&lt;/code&gt;. The resulting tensor is returned.</source>
          <target state="translated">张量 &lt;code&gt;other&lt;/code&gt; 每个元素乘以标量 &lt;code&gt;alpha&lt;/code&gt; 并加到张量 &lt;code&gt;input&lt;/code&gt; 每个元素。返回结果张量。</target>
        </trans-unit>
        <trans-unit id="6ea1bfa1407efaeb9ea4478b91f07efe596058d3" translate="yes" xml:space="preserve">
          <source>Each element will be masked independently on every forward call with probability &lt;code&gt;p&lt;/code&gt; using samples from a Bernoulli distribution. The elements to be masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit variance.</source>
          <target state="translated">使用伯努利分布中的样本，每个元素将在每次前向呼叫中以概率 &lt;code&gt;p&lt;/code&gt; 独立屏蔽。待屏蔽的元素在每个前向调用中均被随机化，并进行缩放和移位以保持零均值和单位方差。</target>
        </trans-unit>
        <trans-unit id="8e969c5beedf78b6dd5c2c5d126246b66ca00cf2" translate="yes" xml:space="preserve">
          <source>Each parameter&amp;rsquo;s gradient (&lt;code&gt;.grad&lt;/code&gt; attribute) should be unscaled before the optimizer updates the parameters, so the scale factor does not interfere with the learning rate.</source>
          <target state="translated">在优化程序更新参数之前，应取消缩放每个参数的梯度（ &lt;code&gt;.grad&lt;/code&gt; 属性），因此缩放因子不会干扰学习率。</target>
        </trans-unit>
        <trans-unit id="04229e567ec03d81b5712793e639928468412520" translate="yes" xml:space="preserve">
          <source>Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and &amp;ldquo;GIL-thrashing&amp;rdquo; that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.</source>
          <target state="translated">每个进程都包含一个独立的Python解释器，从而消除了由单个Python进程驱动多个执行线程，模型副本或GPU所带来的额外解释器开销和&amp;ldquo; GIL颠簸&amp;rdquo;。这对于大量使用Python运行时的模型尤其重要，包括具有循环层或许多小组件的模型。</target>
        </trans-unit>
        <trans-unit id="3c11c2aae826827df9743912100e743d6a724614" translate="yes" xml:space="preserve">
          <source>Each process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes.</source>
          <target state="translated">每个进程都维护自己的优化器,并在每次迭代时执行一个完整的优化步骤。虽然这看起来可能是多余的,但由于梯度已经被收集在一起,并在各个进程中进行平均,因此每个进程的梯度都是相同的,这意味着不需要参数广播步骤,减少了在节点之间传输张力的时间。</target>
        </trans-unit>
        <trans-unit id="25ddc73f3c685d39e655eadbdd24861cccbd855c" translate="yes" xml:space="preserve">
          <source>Each process scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.</source>
          <target state="translated">每个进程将输入的时序列表分散到一个组中的所有进程,并在输出列表中返回收集的时序列表。</target>
        </trans-unit>
        <trans-unit id="e01a4afade34e5c383597dd828cf5ce98ca5f574" translate="yes" xml:space="preserve">
          <source>Each process will receive exactly one tensor and store its data in the &lt;code&gt;tensor&lt;/code&gt; argument.</source>
          <target state="translated">每个进程将只接收一个张量并将其数据存储在 &lt;code&gt;tensor&lt;/code&gt; 参数中。</target>
        </trans-unit>
        <trans-unit id="14f825720650e629375c55e6473f7e4c1fd8e6fb" translate="yes" xml:space="preserve">
          <source>Each sample will be retrieved by indexing tensors along the first dimension.</source>
          <target state="translated">每个样本将通过沿第一维度的索引时序来检索。</target>
        </trans-unit>
        <trans-unit id="a930fc3f8ffe54e36efcacdd772fc5e7328c868c" translate="yes" xml:space="preserve">
          <source>Each tensor has an associated &lt;code&gt;torch.Storage&lt;/code&gt;, which holds its data. The tensor class also provides multi-dimensional, &lt;a href=&quot;https://en.wikipedia.org/wiki/Stride_of_an_array&quot;&gt;strided&lt;/a&gt; view of a storage and defines numeric operations on it.</source>
          <target state="translated">每个张量都有一个关联的 &lt;code&gt;torch.Storage&lt;/code&gt; ，用于保存其数据。张量类还提供存储的多维，&lt;a href=&quot;https://en.wikipedia.org/wiki/Stride_of_an_array&quot;&gt;跨步&lt;/a&gt;视图，并在其上定义数字运算。</target>
        </trans-unit>
        <trans-unit id="0df7dd6db96e14d83140e8da4324d11663fe359e" translate="yes" xml:space="preserve">
          <source>Each tensor in &lt;code&gt;output_tensor_list&lt;/code&gt; should reside on a separate GPU, as should each list of tensors in &lt;code&gt;input_tensor_lists&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;output_tensor_list&lt;/code&gt; 中的每个张量应驻留在单独的GPU上， &lt;code&gt;input_tensor_lists&lt;/code&gt; 中的每个张量列表也应驻留在单独的GPU上。</target>
        </trans-unit>
        <trans-unit id="96aad5e1712347cedb5784e57172eb768752a304" translate="yes" xml:space="preserve">
          <source>Efficient softmax approximation as described in &lt;a href=&quot;https://arxiv.org/abs/1609.04309&quot;&gt;Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss&amp;eacute;, David Grangier, and Herv&amp;eacute; J&amp;eacute;gou&lt;/a&gt;.</source>
          <target state="translated">如&lt;a href=&quot;https://arxiv.org/abs/1609.04309&quot;&gt;Edouard Grave，Armand Joulin，MoustaphaCiss&amp;eacute;，David Grangier和Herv&amp;eacute;J&amp;eacute;gou的GPU的&lt;/a&gt;高效软最大逼近中所述，高效软最大逼近。</target>
        </trans-unit>
        <trans-unit id="f39713bdb9925bfb39803109379a2aa9f81732f8" translate="yes" xml:space="preserve">
          <source>Either the inplace modified module with submodules wrapped in &lt;code&gt;QuantWrapper&lt;/code&gt; based on qconfig or a new &lt;code&gt;QuantWrapper&lt;/code&gt; module which wraps the input module, the latter case only happens when the input module is a leaf module and we want to quantize it.</source>
          <target state="translated">要么是基于qconfig的带有 &lt;code&gt;QuantWrapper&lt;/code&gt; 的子模块的就地修改模块，要么是包装了输入模块的新 &lt;code&gt;QuantWrapper&lt;/code&gt; 模块，后一种情况仅在输入模块是叶模块并且我们要对其进行量化时发生。</target>
        </trans-unit>
        <trans-unit id="c8078d924b19f64b223a6df8142a489104063237" translate="yes" xml:space="preserve">
          <source>Either:</source>
          <target state="translated">Either:</target>
        </trans-unit>
        <trans-unit id="cba1a5641f37d31826ff43a90ac27303cd2de000" translate="yes" xml:space="preserve">
          <source>Element-wise arctangent of</source>
          <target state="translated">的元直切</target>
        </trans-unit>
        <trans-unit id="c0ce0d75b4a69f8d83fed56dda584bc104824e33" translate="yes" xml:space="preserve">
          <source>Elements lower than min and higher than max are ignored.</source>
          <target state="translated">小于最小值和高于最大值的元素将被忽略。</target>
        </trans-unit>
        <trans-unit id="279868256307295ed763af9335611205d2a9e0e9" translate="yes" xml:space="preserve">
          <source>Eliminates all but the first element from every consecutive group of equivalent elements.</source>
          <target state="translated">从每一个连续的等价元素组中消除除第一个元素以外的所有元素。</target>
        </trans-unit>
        <trans-unit id="65e872502ba89d481aa8b75804d63958258d47d9" translate="yes" xml:space="preserve">
          <source>Embedding</source>
          <target state="translated">Embedding</target>
        </trans-unit>
        <trans-unit id="fb52acf7809d84b49f20c1e0c2aeb4d9421e883d" translate="yes" xml:space="preserve">
          <source>Embedding (no optional arguments supported)</source>
          <target state="translated">嵌入(不支持可选参数</target>
        </trans-unit>
        <trans-unit id="ad4c45718740fb7fd09a983448014fcde632287c" translate="yes" xml:space="preserve">
          <source>EmbeddingBag</source>
          <target state="translated">EmbeddingBag</target>
        </trans-unit>
        <trans-unit id="f25f65444d58a4000e445a3586674f2f0a1c519b" translate="yes" xml:space="preserve">
          <source>EmbeddingBag also supports per-sample weights as an argument to the forward pass. This scales the output of the Embedding before performing a weighted reduction as specified by &lt;code&gt;mode&lt;/code&gt;. If &lt;code&gt;per_sample_weights`&lt;/code&gt; is passed, the only supported &lt;code&gt;mode&lt;/code&gt; is &lt;code&gt;&quot;sum&quot;&lt;/code&gt;, which computes a weighted sum according to &lt;code&gt;per_sample_weights&lt;/code&gt;.</source>
          <target state="translated">EmbeddingBag还支持按样本权重作为正向传递的参数。在执行 &lt;code&gt;mode&lt;/code&gt; 指定的加权缩减之前，这将缩放Embedding的输出。如果通过了 &lt;code&gt;per_sample_weights`&lt;/code&gt; ，则唯一支持的 &lt;code&gt;mode&lt;/code&gt; 是 &lt;code&gt;&quot;sum&quot;&lt;/code&gt; ，它根据 &lt;code&gt;per_sample_weights&lt;/code&gt; 计算一个加权和。</target>
        </trans-unit>
        <trans-unit id="7a184f78683775965d53e63c45e77c29a82cb431" translate="yes" xml:space="preserve">
          <source>Enables .grad attribute for non-leaf Tensors.</source>
          <target state="translated">对非叶型Tensors启用.grad属性。</target>
        </trans-unit>
        <trans-unit id="97e857cbb6b3eaaffa99455ab0a6561f4d2333be" translate="yes" xml:space="preserve">
          <source>Enables gradient calculation, if it has been disabled via &lt;a href=&quot;#torch.autograd.no_grad&quot;&gt;&lt;code&gt;no_grad&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.autograd.set_grad_enabled&quot;&gt;&lt;code&gt;set_grad_enabled&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">如果已通过&lt;a href=&quot;#torch.autograd.no_grad&quot;&gt; &lt;code&gt;no_grad&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;#torch.autograd.set_grad_enabled&quot;&gt; &lt;code&gt;set_grad_enabled&lt;/code&gt; &lt;/a&gt;禁用了梯度计算，则启用它。</target>
        </trans-unit>
        <trans-unit id="4ba6db1c18e6b89f8ae8769b517165aebdeeb5e7" translate="yes" xml:space="preserve">
          <source>Enables gradient calculation, if it has been disabled via &lt;a href=&quot;torch.no_grad#torch.no_grad&quot;&gt;&lt;code&gt;no_grad&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.set_grad_enabled#torch.set_grad_enabled&quot;&gt;&lt;code&gt;set_grad_enabled&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">如果已通过&lt;a href=&quot;torch.no_grad#torch.no_grad&quot;&gt; &lt;code&gt;no_grad&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;torch.set_grad_enabled#torch.set_grad_enabled&quot;&gt; &lt;code&gt;set_grad_enabled&lt;/code&gt; &lt;/a&gt;禁用了梯度计算，则启用它。</target>
        </trans-unit>
        <trans-unit id="129b2e35235ca3917459fbda73dccc7c7ba4bb9f" translate="yes" xml:space="preserve">
          <source>Ensures that the tensor memory is not reused for another tensor until all current work queued on &lt;code&gt;stream&lt;/code&gt; are complete.</source>
          <target state="translated">确保在 &lt;code&gt;stream&lt;/code&gt; 上排队的所有当前工作完成之前，张量内存不会被其他张量重用。</target>
        </trans-unit>
        <trans-unit id="02f525cd5d7591c6c78960a007006cf272db0f53" translate="yes" xml:space="preserve">
          <source>Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers.</source>
          <target state="translated">Entrypoint函数可以返回一个model(nn.module),也可以返回辅助工具,让用户的工作流程更加顺畅,比如tokenizers。</target>
        </trans-unit>
        <trans-unit id="b0dd14c2fc526f82985ca8c8a3a6e1c3b85b6488" translate="yes" xml:space="preserve">
          <source>Environment variable initialization</source>
          <target state="translated">环境变量初始化</target>
        </trans-unit>
        <trans-unit id="089481a55a3bf2c4a369d848036bccebf8abe33b" translate="yes" xml:space="preserve">
          <source>Equivalent to input[:,::-1]. Requires the array to be at least 2-D.</source>
          <target state="translated">相当于输入[:,::-1]。要求数组至少是二维的。</target>
        </trans-unit>
        <trans-unit id="3b4933d2950181258ba9867fa0e789a6fac6c0e2" translate="yes" xml:space="preserve">
          <source>Equivalent to input[::-1,&amp;hellip;]. Requires the array to be at least 1-D.</source>
          <target state="translated">等效于input [::-1，&amp;hellip;]。要求数组至少为一维。</target>
        </trans-unit>
        <trans-unit id="9c1eedde4e72567b3c48e7393929af1241088b7e" translate="yes" xml:space="preserve">
          <source>Errors such as timeouts for the &lt;code&gt;remote&lt;/code&gt; API are handled on a best-effort basis. This means that when remote calls initiated by &lt;code&gt;remote&lt;/code&gt; fail, such as with a timeout error, we take a best-effort approach to error handling. This means that errors are handled and set on the resulting RRef on an asynchronous basis. If the RRef has not been used by the application before this handling (such as &lt;code&gt;to_here&lt;/code&gt; or fork call), then future uses of the &lt;code&gt;RRef&lt;/code&gt; will appropriately raise errors. However, it is possible that the user application will use the &lt;code&gt;RRef&lt;/code&gt; before the errors are handled. In this case, errors may not be raised as they have not yet been handled.</source>
          <target state="translated">诸如 &lt;code&gt;remote&lt;/code&gt; API超时之类的错误将尽力而为。这意味着，当由 &lt;code&gt;remote&lt;/code&gt; 发起的远程调用失败时，例如出现超时错误，我们将采取尽力而为的方法进行错误处理。这意味着将以异步方式在生成的RRef上处理和设置错误。如果此处理之前应用程序尚未使用 &lt;code&gt;to_here&lt;/code&gt; （例如to_here或fork调用），则 &lt;code&gt;RRef&lt;/code&gt; 的未来使用将适当地引发错误。但是，用户应用程序可能会在处理错误之前使用 &lt;code&gt;RRef&lt;/code&gt; 。在这种情况下，由于尚未处理错误，因此可能不会引发错误。</target>
        </trans-unit>
        <trans-unit id="2352230ae4369b2400cd9a8b59625450e193a06a" translate="yes" xml:space="preserve">
          <source>Estimate</source>
          <target state="translated">Estimate</target>
        </trans-unit>
        <trans-unit id="9f5d3b9ce4cb19a213d647b445b4900659d33a6d" translate="yes" xml:space="preserve">
          <source>Evaluates module(input) in parallel across the GPUs given in device_ids.</source>
          <target state="translated">在device_ids中给定的GPU上并行评估模块(输入)。</target>
        </trans-unit>
        <trans-unit id="5b74f76cd70090d77d5d968780f896243c537f02" translate="yes" xml:space="preserve">
          <source>Every &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; has a corresponding storage of the same data type.</source>
          <target state="translated">每个&lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;都有对应的相同数据类型的存储。</target>
        </trans-unit>
        <trans-unit id="6c95c4e1c7a1060f504484945d50cad58c391f52" translate="yes" xml:space="preserve">
          <source>Every Sampler subclass has to provide an &lt;code&gt;__iter__()&lt;/code&gt; method, providing a way to iterate over indices of dataset elements, and a &lt;code&gt;__len__()&lt;/code&gt; method that returns the length of the returned iterators.</source>
          <target state="translated">每个Sampler子类都必须提供 &lt;code&gt;__iter__()&lt;/code&gt; 方法（提供对数据集元素的索引进行迭代的方法 &lt;code&gt;__len__()&lt;/code&gt; 和__len __（）方法（可返回返回的迭代器的长度）。</target>
        </trans-unit>
        <trans-unit id="88cdbf2ac191cecaae2230a964d90524ead7b613" translate="yes" xml:space="preserve">
          <source>Every collective operation function supports the following two kinds of operations:</source>
          <target state="translated">每个集体操作函数都支持以下两种操作。</target>
        </trans-unit>
        <trans-unit id="25ee794140785458589b1f0dd87ed4991a86d45f" translate="yes" xml:space="preserve">
          <source>Every operation performed on &lt;code&gt;Tensor&lt;/code&gt; s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies (&lt;code&gt;input &amp;lt;- output&lt;/code&gt;). Then, when backward is called, the graph is processed in the topological ordering, by calling &lt;a href=&quot;#torch.autograd.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt; methods of each &lt;a href=&quot;#torch.autograd.Function&quot;&gt;&lt;code&gt;Function&lt;/code&gt;&lt;/a&gt; object, and passing returned gradients on to next &lt;a href=&quot;#torch.autograd.Function&quot;&gt;&lt;code&gt;Function&lt;/code&gt;&lt;/a&gt; s.</source>
          <target state="translated">在 &lt;code&gt;Tensor&lt;/code&gt; 上执行的每个操作都会创建一个新的函数对象，该对象执行计算并记录其发生。历史记录以DAG函数形式保留，其边缘表示数据依赖性（ &lt;code&gt;input &amp;lt;- output&lt;/code&gt; ）。然后，当调用&lt;a href=&quot;#torch.autograd.backward&quot;&gt; &lt;code&gt;backward()&lt;/code&gt; &lt;/a&gt;，通过调用每个&lt;a href=&quot;#torch.autograd.Function&quot;&gt; &lt;code&gt;Function&lt;/code&gt; &lt;/a&gt;对象的向后（）方法并将返回的梯度传递给下一个&lt;a href=&quot;#torch.autograd.Function&quot;&gt; &lt;code&gt;Function&lt;/code&gt; &lt;/a&gt;，以拓扑顺序处理图。</target>
        </trans-unit>
        <trans-unit id="8d3ee82a911f61025ee027bbb14528d9be81be1b" translate="yes" xml:space="preserve">
          <source>Every tensor that&amp;rsquo;s been modified in-place in a call to &lt;code&gt;forward()&lt;/code&gt; should be given to this function, to ensure correctness of our checks. It doesn&amp;rsquo;t matter whether the function is called before or after modification.</source>
          <target state="translated">应该在对 &lt;code&gt;forward()&lt;/code&gt; 的调用中就地修改的每个张量都赋予该函数，以确保检查的正确性。在修改之前或之后调用该函数都没有关系。</target>
        </trans-unit>
        <trans-unit id="cac312ad37989d865266909c5b6681a3176edc55" translate="yes" xml:space="preserve">
          <source>Everything in a user defined &lt;a href=&quot;torchscript-class&quot;&gt;TorchScript Class&lt;/a&gt; is exported by default, functions can be decorated with &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; if needed.</source>
          <target state="translated">默认情况下，将导出用户定义的&lt;a href=&quot;torchscript-class&quot;&gt;TorchScript类中的所有&lt;/a&gt;内容，如果需要，可以使用&lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt;装饰函数。</target>
        </trans-unit>
        <trans-unit id="e1753c80d644f128cefa2284b014d868b991b9d9" translate="yes" xml:space="preserve">
          <source>Exactly one of &lt;code&gt;devices&lt;/code&gt; and &lt;code&gt;out&lt;/code&gt; must be specified.</source>
          <target state="translated">正好一个 &lt;code&gt;devices&lt;/code&gt; 和 &lt;code&gt;out&lt;/code&gt; 必须指定。</target>
        </trans-unit>
        <trans-unit id="502159c89a8ff9154570557bd71a328efcce5727" translate="yes" xml:space="preserve">
          <source>Exactly one of &lt;code&gt;devices&lt;/code&gt; and &lt;code&gt;out&lt;/code&gt; must be specified. When &lt;code&gt;out&lt;/code&gt; is specified, &lt;code&gt;chunk_sizes&lt;/code&gt; must not be specified and will be inferred from sizes of &lt;code&gt;out&lt;/code&gt;.</source>
          <target state="translated">正好一个 &lt;code&gt;devices&lt;/code&gt; 和 &lt;code&gt;out&lt;/code&gt; 必须指定。当 &lt;code&gt;out&lt;/code&gt; 指定， &lt;code&gt;chunk_sizes&lt;/code&gt; ，不得指定，将被从大小推断 &lt;code&gt;out&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0f01ed56a1e32a05e5ef96e4d779f34784af9a96" translate="yes" xml:space="preserve">
          <source>Example</source>
          <target state="translated">Example</target>
        </trans-unit>
        <trans-unit id="b3c7ccbaed13d7cf475ac0a8540d9af04a1abfba" translate="yes" xml:space="preserve">
          <source>Example (a type mismatch)</source>
          <target state="translated">例子(类型不匹配</target>
        </trans-unit>
        <trans-unit id="fb18992fbb684cb9203c712439d9448a7d50b7c9" translate="yes" xml:space="preserve">
          <source>Example (an exported and ignored method in a module):</source>
          <target state="translated">例子(模块中的导出和忽略的方法)。</target>
        </trans-unit>
        <trans-unit id="3fb80256910d817ce9141fdd4451ee10894ec35c" translate="yes" xml:space="preserve">
          <source>Example (calling a script function in a traced function):</source>
          <target state="translated">示例(在跟踪函数中调用脚本函数)。</target>
        </trans-unit>
        <trans-unit id="b5bcda1a3bfb1af96df6dd020d0588805d88feba" translate="yes" xml:space="preserve">
          <source>Example (calling a traced function in script):</source>
          <target state="translated">例子(在脚本中调用一个跟踪函数)。</target>
        </trans-unit>
        <trans-unit id="9488ed97f58e814938180bd3de0fc03a37f8d8c1" translate="yes" xml:space="preserve">
          <source>Example (fork a free function):</source>
          <target state="translated">例子(fork一个自由函数)。</target>
        </trans-unit>
        <trans-unit id="8d40b7fb2ebedc8ff715bb44861bcfbdc43fa817" translate="yes" xml:space="preserve">
          <source>Example (fork a module method):</source>
          <target state="translated">例子(fork一个模块方法)。</target>
        </trans-unit>
        <trans-unit id="c61fd1aa74a0dc25c5318d75c242307960c7041c" translate="yes" xml:space="preserve">
          <source>Example (refining types on parameters and locals):</source>
          <target state="translated">例子(在参数和locals上完善类型)。</target>
        </trans-unit>
        <trans-unit id="9f3b97fb80872ca69d304fba5bcdeedcdbea8e85" translate="yes" xml:space="preserve">
          <source>Example (scripting a function):</source>
          <target state="translated">例子(函数脚本)。</target>
        </trans-unit>
        <trans-unit id="de4e7f540085eb6296ec8260ef53372b15064ca1" translate="yes" xml:space="preserve">
          <source>Example (scripting a module with traced submodules):</source>
          <target state="translated">例子(用跟踪的子模块编写脚本)。</target>
        </trans-unit>
        <trans-unit id="c5b0acb6ce867883478ae6d5579cfda80cbb9404" translate="yes" xml:space="preserve">
          <source>Example (scripting a simple module with a Parameter):</source>
          <target state="translated">例子(用参数编写一个简单的模块脚本)。</target>
        </trans-unit>
        <trans-unit id="98fb0963dfe59db2efb10e03278c046397333fb9" translate="yes" xml:space="preserve">
          <source>Example (tracing a function):</source>
          <target state="translated">例子(跟踪函数)。</target>
        </trans-unit>
        <trans-unit id="0dad6a76da0fc25a19704212a257d6379a12034b" translate="yes" xml:space="preserve">
          <source>Example (tracing a module with multiple methods):</source>
          <target state="translated">例子(跟踪一个有多个方法的模块)。</target>
        </trans-unit>
        <trans-unit id="07f0e6f11243b3ae74d74b1d28cef8c785cb16fa" translate="yes" xml:space="preserve">
          <source>Example (tracing an existing module):</source>
          <target state="translated">例子(追踪现有模块):</target>
        </trans-unit>
        <trans-unit id="68ede4837476671e511c48828e0de48f7b06d426" translate="yes" xml:space="preserve">
          <source>Example (type annotations for Python 3):</source>
          <target state="translated">示例 (Python 3 的类型注释)。</target>
        </trans-unit>
        <trans-unit id="87a104ad2463311906e75683079bbad94a4fd5c2" translate="yes" xml:space="preserve">
          <source>Example (using &lt;code&gt;@torch.jit.export&lt;/code&gt; on a method):</source>
          <target state="translated">示例（在方法上使用 &lt;code&gt;@torch.jit.export&lt;/code&gt; ）：</target>
        </trans-unit>
        <trans-unit id="d1c7a00dbf049f876c9759d8441a029ed8e2f4ac" translate="yes" xml:space="preserve">
          <source>Example (using &lt;code&gt;@torch.jit.ignore(drop=True)&lt;/code&gt; on a method):</source>
          <target state="translated">示例（在方法上使用 &lt;code&gt;@torch.jit.ignore(drop=True)&lt;/code&gt; ）：</target>
        </trans-unit>
        <trans-unit id="f45fe2e88f095f6547360c930d8119bf8c200149" translate="yes" xml:space="preserve">
          <source>Example (using &lt;code&gt;@torch.jit.ignore&lt;/code&gt; on a method):</source>
          <target state="translated">示例（在方法上使用 &lt;code&gt;@torch.jit.ignore&lt;/code&gt; ）：</target>
        </trans-unit>
        <trans-unit id="ee3db4aefed96eb801a5613278d0970a19c709cc" translate="yes" xml:space="preserve">
          <source>Example (using &lt;code&gt;@torch.jit.unused&lt;/code&gt; on a method):</source>
          <target state="translated">示例（在方法上使用 &lt;code&gt;@torch.jit.unused&lt;/code&gt; ）：</target>
        </trans-unit>
        <trans-unit id="15234a3935f9814be4e208815ad355afc291f504" translate="yes" xml:space="preserve">
          <source>Example (using a traced module):</source>
          <target state="translated">例子(使用跟踪模块)。</target>
        </trans-unit>
        <trans-unit id="73cb14b980b72004cbdbfb86fac47cf40133628b" translate="yes" xml:space="preserve">
          <source>Example 1: splitting workload across all workers in &lt;code&gt;__iter__()&lt;/code&gt;:</source>
          <target state="translated">示例1：在 &lt;code&gt;__iter__()&lt;/code&gt; 工作负载分配给所有工作人员：</target>
        </trans-unit>
        <trans-unit id="877513517da6d71b899179494c7d8cfcf56d5921" translate="yes" xml:space="preserve">
          <source>Example 2: splitting workload across all workers using &lt;code&gt;worker_init_fn&lt;/code&gt;:</source>
          <target state="translated">示例2：使用 &lt;code&gt;worker_init_fn&lt;/code&gt; 在所有工作人员之间分配工作负载：</target>
        </trans-unit>
        <trans-unit id="49895a4623d5b4a9344031509b184da5a2c67818" translate="yes" xml:space="preserve">
          <source>Example. if we have the following shape for inputs and outputs:</source>
          <target state="translated">例如.如果我们的输入和输出的形状如下。</target>
        </trans-unit>
        <trans-unit id="c63737abd7347a7ae582cb9fbdf37d6c0e5b251e" translate="yes" xml:space="preserve">
          <source>Example:</source>
          <target state="translated">Example:</target>
        </trans-unit>
        <trans-unit id="8ec20a84a991e14aeedf10c8e4e9247bf2c9315c" translate="yes" xml:space="preserve">
          <source>Example: End-to-end AlexNet from PyTorch to ONNX</source>
          <target state="translated">实例:从PyTorch到ONNX的端到端AlexNet 从PyTorch到ONNX的端到端AlexNet。</target>
        </trans-unit>
        <trans-unit id="038d2f8486ff39be2d765514d254dcc770c02da8" translate="yes" xml:space="preserve">
          <source>Example: Suppose the last window is: &lt;code&gt;[17, 18, 0, 0, 0]&lt;/code&gt; vs &lt;code&gt;[18, 0, 0, 0, 0]&lt;/code&gt;</source>
          <target state="translated">示例：假设最后一个窗口是： &lt;code&gt;[17, 18, 0, 0, 0]&lt;/code&gt; vs &lt;code&gt;[18, 0, 0, 0, 0]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="6104a08ed7eb335488d3ea3f93c6210a1cc4746c" translate="yes" xml:space="preserve">
          <source>Example::</source>
          <target state="translated">Example::</target>
        </trans-unit>
        <trans-unit id="eb01bf04c9a0e8a71c45816513df424f1c7ffedb" translate="yes" xml:space="preserve">
          <source>Examples</source>
          <target state="translated">Examples</target>
        </trans-unit>
        <trans-unit id="fb3447b632f6a431215776dcf254a01001a40c4f" translate="yes" xml:space="preserve">
          <source>Examples:</source>
          <target state="translated">Examples:</target>
        </trans-unit>
        <trans-unit id="de3e24010b1050a8265462ab7e8f3a95c00717ea" translate="yes" xml:space="preserve">
          <source>Examples::</source>
          <target state="translated">Examples::</target>
        </trans-unit>
        <trans-unit id="8cc1b49841e2eac10bc692d1dee6f79c0d6dc0f7" translate="yes" xml:space="preserve">
          <source>Expand this tensor to the same size as &lt;code&gt;other&lt;/code&gt;. &lt;code&gt;self.expand_as(other)&lt;/code&gt; is equivalent to &lt;code&gt;self.expand(other.size())&lt;/code&gt;.</source>
          <target state="translated">将此张量扩展为与 &lt;code&gt;other&lt;/code&gt; 张量相同的大小。 &lt;code&gt;self.expand_as(other)&lt;/code&gt; 等同于 &lt;code&gt;self.expand(other.size())&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7381344dd49d389b0f60204fb4dbd8a7ee76371e" translate="yes" xml:space="preserve">
          <source>Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the &lt;code&gt;stride&lt;/code&gt; to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.</source>
          <target state="translated">扩展张量不会分配新的内存，而只会在现有张量上创建一个新视图，其中通过将 &lt;code&gt;stride&lt;/code&gt; 设置为0将大小为1的维扩展为更大的大小。大小为1的任何维都可以扩展为任意值而不分配新的内存。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
