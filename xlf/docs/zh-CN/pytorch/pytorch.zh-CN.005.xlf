<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="00b13dec03b43a1e31572c6206c49d66e2649772" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;non_blocking&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt; and this copy is between CPU and GPU, the copy may occur asynchronously with respect to the host. For other cases, this argument has no effect.</source>
          <target state="translated">&lt;strong&gt;non_blocking&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ,并且此副本位于CPU和GPU之间，则该副本可能相对于主机异步发生。在其他情况下，此参数无效。</target>
        </trans-unit>
        <trans-unit id="23e61115abf60d0f6a77464558c090db5540aa9e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;non_leaf_module_list&lt;/strong&gt; &amp;ndash; list of non-leaf modules we want to add observer</source>
          <target state="translated">&lt;strong&gt;non_leaf_module_list&lt;/strong&gt; &amp;ndash;我们要添加观察者的非叶子模块的列表</target>
        </trans-unit>
        <trans-unit id="d97e40b345e0eaa999d1b92e40db509bc22da78b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;nondet_tol&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance.</source>
          <target state="translated">&lt;strong&gt;nondet_tol&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;对不确定性的容忍度。通过微分运行相同的输入时，结果必须完全匹配（默认值为0.0）或在此公差范围内。</target>
        </trans-unit>
        <trans-unit id="8f5856c3c6589a1314d66382282476615b5a82fb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;nondet_tol&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance. Note that a small amount of nondeterminism in the gradient will lead to larger inaccuracies in the second derivative.</source>
          <target state="translated">&lt;strong&gt;nondet_tol&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;对不确定性的容忍度。通过微分运行相同的输入时，结果必须完全匹配（默认值为0.0）或在此公差范围内。请注意，梯度中的少量不确定性将导致二阶导数的较大误差。</target>
        </trans-unit>
        <trans-unit id="dc977ce4adf32dbf0bccc2e0e481f93caf29accf" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;nonlinearity&lt;/strong&gt; &amp;ndash; The non-linearity to use. Can be either &lt;code&gt;'tanh'&lt;/code&gt; or &lt;code&gt;'relu'&lt;/code&gt;. Default: &lt;code&gt;'tanh'&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;非线性&lt;/strong&gt;&amp;ndash;使用的非线性。可以是 &lt;code&gt;'tanh'&lt;/code&gt; 或 &lt;code&gt;'relu'&lt;/code&gt; 。默认值： &lt;code&gt;'tanh'&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="3130318f0a7fea9d363cd7ece82482a7951ecf83" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;nonlinearity&lt;/strong&gt; &amp;ndash; the non-linear function (&lt;code&gt;nn.functional&lt;/code&gt; name)</source>
          <target state="translated">&lt;strong&gt;非线性&lt;/strong&gt;&amp;ndash;非线性函数（ &lt;code&gt;nn.functional&lt;/code&gt; 功能名称）</target>
        </trans-unit>
        <trans-unit id="4aa37508fa49cde50a4584a0a4a2daca12ff537e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;nonlinearity&lt;/strong&gt; &amp;ndash; the non-linear function (&lt;code&gt;nn.functional&lt;/code&gt; name), recommended to use only with &lt;code&gt;'relu'&lt;/code&gt; or &lt;code&gt;'leaky_relu'&lt;/code&gt; (default).</source>
          <target state="translated">&lt;strong&gt;非线性&lt;/strong&gt;-非线性函数（ &lt;code&gt;nn.functional&lt;/code&gt; 名称为nn），建议仅与 &lt;code&gt;'relu'&lt;/code&gt; 或 &lt;code&gt;'leaky_relu'&lt;/code&gt; （默认值）一起使用。</target>
        </trans-unit>
        <trans-unit id="af1aae5aa50bc026c9cb830597267216d05aa8f1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;norm&lt;/strong&gt; &amp;ndash; the layer normalization component (optional).</source>
          <target state="translated">&lt;strong&gt;规范&lt;/strong&gt;&amp;ndash;图层归一化组件（可选）。</target>
        </trans-unit>
        <trans-unit id="042826a4724ce41d943ad1739990aba3a85a4a51" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;norm&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;规范&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="25b7d7d86b234b69e97817612f3d8411fbdc3831" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;norm_type&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; type of the used p-norm. Can be &lt;code&gt;'inf'&lt;/code&gt; for infinity norm.</source>
          <target state="translated">&lt;strong&gt;norm_type&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;使用的p范数的类型。对于无限范数可以为 &lt;code&gt;'inf'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5fbeccb6b749ceedad0227342ff93bd8d1b3c38e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;norm_type&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; See module initialization documentation. Default &lt;code&gt;2&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;norm_type&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;请参阅模块初始化文档。默认值 &lt;code&gt;2&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="67a00acec5420f77031cf95c030b9e478dabf5ff" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;norm_type&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The &lt;code&gt;p&lt;/code&gt; in the &lt;code&gt;p&lt;/code&gt;-norm to compute for the &lt;code&gt;max_norm&lt;/code&gt; option. Default &lt;code&gt;2&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;norm_type&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;浮动&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;） -的 &lt;code&gt;p&lt;/code&gt; 在 &lt;code&gt;p&lt;/code&gt; 范数来计算用于 &lt;code&gt;max_norm&lt;/code&gt; 选项。默认值 &lt;code&gt;2&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="6c0575ab7282d8575f927d745ef2b2ba33374a8d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;norm_type&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The p of the p-norm to compute for the &lt;code&gt;max_norm&lt;/code&gt; option. Default &lt;code&gt;2&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;norm_type&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;为 &lt;code&gt;max_norm&lt;/code&gt; 选项计算的p范数的p 。默认值 &lt;code&gt;2&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f7e673690158062d96a0d2ab65a7cef662314e39" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;normalized&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Whether the STFT was normalized. (Default: &lt;code&gt;False&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;标准化&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;布尔&lt;/a&gt;）&amp;ndash; STFT是否标准化。（默认： &lt;code&gt;False&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="e814d9b9d8871174bb71b430fdad8deb479f898c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;normalized&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; controls whether to return normalized results. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;标准化&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;控制是否返回标准化结果。默认值： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="55c2c1e33d4fa16ec0ad8d020fa793df29564524" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;normalized&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; controls whether to return the normalized STFT results Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;标准化&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;控制是否返回标准化的STFT结果默认： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="46fbb1084d5f3c7db6c165371d7dc2e34cf14b9f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;normalized_shape&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;torch.Size&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;normalized_shape&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;torch.Size&lt;/em&gt;）&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="34ddc3ee75c6e422834e70e579e7d1f48d9225b6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;nprocs&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Number of processes to spawn.</source>
          <target state="translated">&lt;strong&gt;nprocs&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;产生的进程数。</target>
        </trans-unit>
        <trans-unit id="0893590e108f46268f24f1a5dcb2dddba9556031" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_channels&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; number of channels expected in input</source>
          <target state="translated">&lt;strong&gt;num_channels&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;输入中预期的通道数</target>
        </trans-unit>
        <trans-unit id="5cfb2a3b4581e8308eef2a0a6f8caba338ba2eb1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_classes&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Total number of classes. If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.</source>
          <target state="translated">&lt;strong&gt;num_classes&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;类的总数。如果设置为-1，则类别数将推断为比输入张量中的最大类别值大一。</target>
        </trans-unit>
        <trans-unit id="0cedc9f268bdd45102c976aafc252937bfb35224" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_classes&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; number of output classes of the model (including the background)</source>
          <target state="translated">&lt;strong&gt;num_classes&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;模型的输出类的数量（包括背景）</target>
        </trans-unit>
        <trans-unit id="b89059d982eca861114c424da98eed8e996d7cf1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_decoder_layers&lt;/strong&gt; &amp;ndash; the number of sub-decoder-layers in the decoder (default=6).</source>
          <target state="translated">&lt;strong&gt;num_decoder_layers&lt;/strong&gt; &amp;ndash;解码器中子解码器层的数量（默认为6）。</target>
        </trans-unit>
        <trans-unit id="849f5a2c21cc33c1cacad354c0a79d6a1f17b0c2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_embeddings&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; size of the dictionary of embeddings</source>
          <target state="translated">&lt;strong&gt;num_embeddings&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;嵌入字典的大小</target>
        </trans-unit>
        <trans-unit id="8181be00d6cb90d97280a1382ec1a1a05ca6c394" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_encoder_layers&lt;/strong&gt; &amp;ndash; the number of sub-encoder-layers in the encoder (default=6).</source>
          <target state="translated">&lt;strong&gt;num_encoder_layers&lt;/strong&gt; &amp;ndash;编码器中子编码器层的数量（默认为6）。</target>
        </trans-unit>
        <trans-unit id="fb50075c2dd76286e5338792fc9e620415adf047" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_features&lt;/strong&gt; &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;num_features&lt;/strong&gt; &amp;ndash;</target>
        </trans-unit>
        <trans-unit id="b606c4e5166178414dc02bf646ef98468d9aef12" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_groups&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; number of groups to separate the channels into</source>
          <target state="translated">&lt;strong&gt;num_groups&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;将通道分隔为的组数</target>
        </trans-unit>
        <trans-unit id="93f81723e37eb2609764e3fe95a3bac369d5a4b0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_heads&lt;/strong&gt; &amp;ndash; parallel attention heads.</source>
          <target state="translated">&lt;strong&gt;num_heads&lt;/strong&gt; &amp;ndash;平行注意头。</target>
        </trans-unit>
        <trans-unit id="716e2a8f4f7ad74c1067fb5b8a90a00f71da6a83" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_layers&lt;/strong&gt; &amp;ndash; Number of recurrent layers. E.g., setting &lt;code&gt;num_layers=2&lt;/code&gt; would mean stacking two GRUs together to form a &lt;code&gt;stacked GRU&lt;/code&gt;, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1</source>
          <target state="translated">&lt;strong&gt;num_layers&lt;/strong&gt; &amp;ndash;循环图层数。例如，设置 &lt;code&gt;num_layers=2&lt;/code&gt; 意味着将两个 &lt;code&gt;stacked GRU&lt;/code&gt; 在一起以形成堆叠的GRU，而第二个GRU则接收第一个GRU的输出并计算最终结果。默认值：1</target>
        </trans-unit>
        <trans-unit id="848be87c245b176a092f816c28a51ec302bfbd9d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_layers&lt;/strong&gt; &amp;ndash; Number of recurrent layers. E.g., setting &lt;code&gt;num_layers=2&lt;/code&gt; would mean stacking two LSTMs together to form a &lt;code&gt;stacked LSTM&lt;/code&gt;, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1</source>
          <target state="translated">&lt;strong&gt;num_layers&lt;/strong&gt; &amp;ndash;循环图层数。例如，设置 &lt;code&gt;num_layers=2&lt;/code&gt; 意味着将两个 &lt;code&gt;stacked LSTM&lt;/code&gt; 在一起以形成堆叠的LSTM，而第二个LSTM接收第一个LSTM的输出并计算最终结果。默认值：1</target>
        </trans-unit>
        <trans-unit id="3f20f56b362366655c5f5d500d3983ccb38896b4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_layers&lt;/strong&gt; &amp;ndash; Number of recurrent layers. E.g., setting &lt;code&gt;num_layers=2&lt;/code&gt; would mean stacking two RNNs together to form a &lt;code&gt;stacked RNN&lt;/code&gt;, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1</source>
          <target state="translated">&lt;strong&gt;num_layers&lt;/strong&gt; &amp;ndash;循环图层数。例如，设置 &lt;code&gt;num_layers=2&lt;/code&gt; 意味着将两个 &lt;code&gt;stacked RNN&lt;/code&gt; 在一起以形成一个堆叠的RNN，而第二个RNN接收第一个RNN的输出并计算最终结果。默认值：1</target>
        </trans-unit>
        <trans-unit id="80d0325c2a7e14f3b87f5fbe564e95166bea3899" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_layers&lt;/strong&gt; &amp;ndash; the number of sub-decoder-layers in the decoder (required).</source>
          <target state="translated">&lt;strong&gt;num_layers&lt;/strong&gt; &amp;ndash;解码器中子解码器层的数量（必需）。</target>
        </trans-unit>
        <trans-unit id="6981d7fa46aee8b590e66abd7ac32bb99332f4b9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_layers&lt;/strong&gt; &amp;ndash; the number of sub-encoder-layers in the encoder (required).</source>
          <target state="translated">&lt;strong&gt;num_layers&lt;/strong&gt; &amp;ndash;编码器中子编码器层的数量（必填）。</target>
        </trans-unit>
        <trans-unit id="1c6217d241d0c6e30110c21e501846e588f977b1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_parameters&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; number of</source>
          <target state="translated">&lt;strong&gt;num_parameters&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;的数量</target>
        </trans-unit>
        <trans-unit id="400bd8b77c00061e5eae688001b6628633196c98" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_replicas&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Number of processes participating in distributed training. By default, &lt;code&gt;rank&lt;/code&gt; is retrieved from the current distributed group.</source>
          <target state="translated">&lt;strong&gt;num_replicas&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;参与分布式培训的进程数。默认情况下，从当前分布式组中检索 &lt;code&gt;rank&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="217f93399a3e49b6a211efe4dfeb93b9b074c795" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_samples&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; number of samples to draw</source>
          <target state="translated">&lt;strong&gt;num_samples&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;要绘制的样本数</target>
        </trans-unit>
        <trans-unit id="7c74e9e010f947a1d1f639420c7586d124de415e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_samples&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; number of samples to draw, default=`len(dataset)`. This argument is supposed to be specified only when &lt;code&gt;replacement&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;num_samples&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;要绘制的样本数，默认为len（dataset）。仅当 &lt;code&gt;replacement&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; 时才应指定此参数。</target>
        </trans-unit>
        <trans-unit id="261b75892c45e73ce2bcf393c665d4350903ddd0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_send_recv_threads&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The number of threads in the thread-pool used by &lt;code&gt;ProcessGroupAgent&lt;/code&gt; (default: 4).</source>
          <target state="translated">&lt;strong&gt;num_send_recv_threads&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash; &lt;code&gt;ProcessGroupAgent&lt;/code&gt; 使用的线程池中的线程数（默认值：4）。</target>
        </trans-unit>
        <trans-unit id="9e30ae31cb163bad5dd2dce2717b84e6e7ebd501" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_thresholds&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Number of thresholds used to draw the curve.</source>
          <target state="translated">&lt;strong&gt;num_thresholds&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;用于绘制曲线的阈值数。</target>
        </trans-unit>
        <trans-unit id="b15859c381baa669db57383f7570015c6f11a1eb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_worker_threads&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The number of threads in the thread-pool used by &lt;code&gt;TensorPipeAgent&lt;/code&gt; to execute requests (default: 16).</source>
          <target state="translated">&lt;strong&gt;num_worker_threads&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash; &lt;code&gt;TensorPipeAgent&lt;/code&gt; 用于执行请求的线程池中的线程数（默认值：16）。</target>
        </trans-unit>
        <trans-unit id="8bf5929cbf77d6b6e96064b0308a982665a52cec" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;num_workers&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; how many subprocesses to use for data loading. &lt;code&gt;0&lt;/code&gt; means that the data will be loaded in the main process. (default: &lt;code&gt;0&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;num_workers&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;要用于数据加载的子&lt;strong&gt;进程数&lt;/strong&gt;。 &lt;code&gt;0&lt;/code&gt; 表示将在主进程中加载​​数据。（默认值： &lt;code&gt;0&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="f2a5564d5b72bbb54db0dee38713bf01705f62e8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;obj&lt;/strong&gt; &amp;ndash; saved object</source>
          <target state="translated">&lt;strong&gt;obj&lt;/strong&gt; &amp;ndash;保存的对象</target>
        </trans-unit>
        <trans-unit id="339eb04d43f579222956fff0d0593550e205956a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;obj&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Storage&lt;/em&gt;) &amp;ndash; object allocated on the selected device.</source>
          <target state="translated">&lt;strong&gt;obj&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;Storage&lt;/em&gt;）&amp;ndash;在所选设备上分配的对象。</target>
        </trans-unit>
        <trans-unit id="9df8cf76692544b5e3907cc50ab40d5bd8586e94" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;obj&lt;/strong&gt; (&lt;em&gt;Object&lt;/em&gt;) &amp;ndash; Object to test</source>
          <target state="translated">&lt;strong&gt;obj&lt;/strong&gt;（&lt;em&gt;Object&lt;/em&gt;）&amp;ndash;要测试的对象</target>
        </trans-unit>
        <trans-unit id="1efcc773a7594d495f38b29dce91f4e0fcb2d526" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;obj&lt;/strong&gt; (callable, class, or &lt;code&gt;nn.Module&lt;/code&gt;) &amp;ndash; The &lt;code&gt;nn.Module&lt;/code&gt;, function, or class type to compile.</source>
          <target state="translated">&lt;strong&gt;obj&lt;/strong&gt;（callable，class或 &lt;code&gt;nn.Module&lt;/code&gt; ）&amp;ndash;要编译的 &lt;code&gt;nn.Module&lt;/code&gt; ，函数或类类型。</target>
        </trans-unit>
        <trans-unit id="d6c4e0bbeea69d8599a25da4f144962160ea1886" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;observer&lt;/strong&gt; (&lt;em&gt;module&lt;/em&gt;) &amp;ndash; Module for observing statistics on input tensors and calculating scale and zero-point.</source>
          <target state="translated">&lt;strong&gt;观察者&lt;/strong&gt;（&lt;em&gt;模块&lt;/em&gt;）&amp;ndash;用于观察输入张量的统计数据并计算刻度和零点的模块。</target>
        </trans-unit>
        <trans-unit id="8ba4f41b8381dd854ee440f11080a46465c4c0e9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;observer_kwargs&lt;/strong&gt; (&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Arguments for the observer module</source>
          <target state="translated">&lt;strong&gt;viewer_kwargs&lt;/strong&gt;（&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;观察者模块的参数</target>
        </trans-unit>
        <trans-unit id="964cfe304a42815568c84ad629c3feba66c4a36b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;observer_non_leaf_module_list&lt;/strong&gt; &amp;ndash; list of non-leaf modules we want to add observer</source>
          <target state="translated">&lt;strong&gt;reader_non_leaf_module_list&lt;/strong&gt; &amp;ndash;我们要添加观察者的非叶子模块的列表</target>
        </trans-unit>
        <trans-unit id="4d631610135b6d78e1caf59354dfcdfe0e55f151" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;offset&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the diagonal to consider. Default: 0 (main diagonal).</source>
          <target state="translated">&lt;strong&gt;offset&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;要考虑的对角线。默认值：0（主对角线）。</target>
        </trans-unit>
        <trans-unit id="a23fd1c737ac34111ea6878cbb466517bbf61204" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;offset&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; which diagonal to consider. Default: 0 (main diagonal).</source>
          <target state="translated">&lt;strong&gt;offset&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;要考虑的对角线。默认值：0（主对角线）。</target>
        </trans-unit>
        <trans-unit id="b094d10e5ad29ad4b91fe1094a0e0451c35935ce" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;offset&lt;/strong&gt; (&lt;code&gt;int&lt;/code&gt;) &amp;ndash; diagonal offset from the main diagonal. Default: if not provided, 0.</source>
          <target state="translated">&lt;strong&gt;offset&lt;/strong&gt;（ &lt;code&gt;int&lt;/code&gt; ）&amp;ndash;与主对角线的对角线偏移量。默认值：如果未提供，则为0。</target>
        </trans-unit>
        <trans-unit id="59680b913232bfc3f5ca8eefc5ea0a3e439f606d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;offsets&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Only used when &lt;code&gt;input&lt;/code&gt; is 1D. &lt;code&gt;offsets&lt;/code&gt; determines the starting index position of each bag (sequence) in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;偏移量&lt;/strong&gt;（&lt;em&gt;LongTensor &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;仅在 &lt;code&gt;input&lt;/code&gt; 为1D时使用。 &lt;code&gt;offsets&lt;/code&gt; 确定 &lt;code&gt;input&lt;/code&gt; 中每个袋（序列）的起始索引位置。</target>
        </trans-unit>
        <trans-unit id="f3eeb1215829aa860254719c6ed490994631af72" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;onesided&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; controls whether &lt;code&gt;input&lt;/code&gt; was halfed to avoid redundancy, e.g., by &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;oneside&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;布尔型&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;控制 &lt;code&gt;input&lt;/code&gt; 是否被二分之一以避免冗余，例如通过&lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt;。默认值： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c3828107f07c747332fcb7869a55bac90d7123f1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;onesided&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; controls whether to return half of results to avoid redundancy for real inputs. Default: &lt;code&gt;True&lt;/code&gt; for real &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;window&lt;/code&gt;, &lt;code&gt;False&lt;/code&gt; otherwise.</source>
          <target state="translated">&lt;strong&gt;单边&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;控制是否返回一半结果，以避免对实际输入造成冗余。默认值：对于实际 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;window&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，否则为 &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b8c4c667951698c100009eb45a650b471858d1c1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;onesided&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; controls whether to return half of results to avoid redundancy. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;单边&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;控制是否返回一半结果以避免重复。默认值： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="227c5f582067ca753f45fef8e7cb4cc34373476c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;onesided&lt;/strong&gt; (&lt;em&gt;Optional&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; Whether the STFT was onesided. (Default: &lt;code&gt;True&lt;/code&gt; if &lt;code&gt;n_fft != fft_size&lt;/code&gt; in the input size)</source>
          <target state="translated">&lt;strong&gt;单边&lt;/strong&gt;（&lt;em&gt;可选&lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash; STFT是否单边。（默认值：如果输入大小中的 &lt;code&gt;n_fft != fft_size&lt;/code&gt; ，则为 &lt;code&gt;True&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="b0dae8ba515d13e7989bf49ea6f91f0aa98dcae4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;op&lt;/strong&gt; (&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; One of the values from &lt;code&gt;torch.distributed.ReduceOp&lt;/code&gt; enum. Specifies an operation used for element-wise reductions.</source>
          <target state="translated">&lt;strong&gt;op&lt;/strong&gt;（&lt;em&gt;可选&lt;/em&gt;）&amp;ndash; &lt;code&gt;torch.distributed.ReduceOp&lt;/code&gt; 枚举中的值之一。指定用于逐元素精简的操作。</target>
        </trans-unit>
        <trans-unit id="ca9e3fe2123059286c3e6e03e1667d5bcbe86c04" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;operands&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The operands to compute the Einstein sum of.</source>
          <target state="translated">&lt;strong&gt;操作数&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;计算爱因斯坦总和的操作数。</target>
        </trans-unit>
        <trans-unit id="64c84dfc01211648b09d190f92dcde0b67c4d354" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;operator_export_type&lt;/strong&gt; (&lt;em&gt;enum&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default OperatorExportTypes.ONNX&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;operator_export_type&lt;/strong&gt;（&lt;em&gt;枚举&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;默认为OperatorExportTypes.ONNX&lt;/em&gt;）&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="ae2e7fa69f17a2f7d525734c74794c386a5f53af" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;opset_version&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default is 9&lt;/em&gt;) &amp;ndash; by default we export the model to the opset version of the onnx submodule. Since ONNX&amp;rsquo;s latest opset may evolve before next stable release, by default we export to one stable opset version. Right now, supported stable opset version is 9. The opset_version must be _onnx_master_opset or in _onnx_stable_opsets which are defined in torch/onnx/symbolic_helper.py</source>
          <target state="translated">&lt;strong&gt;opset_version&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;默认为9&lt;/em&gt;）&amp;ndash;默认情况下，我们将模型导出到onnx子模块的opset版本。由于ONNX的最新opset可能会在下一个稳定版本之前发展，因此默认情况下，我们会导出到一个稳定的opset版本。目前，受支持的稳定opset版本为9。opset_version必须为_onnx_master_opset或在torch / onnx / symbolic_helper.py中定义的_onnx_stable_opsets中。</target>
        </trans-unit>
        <trans-unit id="eeac5b5644cfe9f927a8580dad78015f44b4e5df" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;optimization options.&lt;/strong&gt; (&lt;em&gt;specific&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;优化选项。&lt;/strong&gt;（&lt;em&gt;具体&lt;/em&gt;）&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="341b800a5d451a2739a7cf894c4169dfd8a8ab69" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;optimization_blocklist&lt;/strong&gt; &amp;ndash; A set with type of MobileOptimizerType. When set is not passed, optimization method will run all the optimizer pass; otherwise, optimizer method will run the optimization pass that is not included inside optimization_blocklist.</source>
          <target state="translated">&lt;strong&gt;Optimization_blocklist&lt;/strong&gt; &amp;ndash;具有MobileOptimizerType类型的集合。当未传递set时，优化方法将运行所有优化器传递；否则，optimizer方法将运行不包含在optimization_blocklist中的优化过程。</target>
        </trans-unit>
        <trans-unit id="e384b8bc869084e0aad2704cab3a0c3783273b65" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;optimizer&lt;/strong&gt; (&lt;a href=&quot;#torch.optim.Optimizer&quot;&gt;Optimizer&lt;/a&gt;) &amp;ndash; Wrapped optimizer.</source>
          <target state="translated">&lt;strong&gt;优化器&lt;/strong&gt;（&lt;a href=&quot;#torch.optim.Optimizer&quot;&gt;Optimizer&lt;/a&gt;）&amp;ndash;包装的优化器。</target>
        </trans-unit>
        <trans-unit id="1207cb8cc2010faa25bd487265a929700cd7ee0c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;optimizer&lt;/strong&gt; (&lt;a href=&quot;optim#torch.optim.Optimizer&quot;&gt;torch.optim.Optimizer&lt;/a&gt;) &amp;ndash; Optimizer that applies the gradients.</source>
          <target state="translated">&lt;strong&gt;优化器&lt;/strong&gt;（&lt;a href=&quot;optim#torch.optim.Optimizer&quot;&gt;torch.optim.Optimizer&lt;/a&gt;）&amp;ndash;应用渐变的优化器。</target>
        </trans-unit>
        <trans-unit id="7c10e05231937bf3ab042ec5af55f70548f71ca1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;optimizer&lt;/strong&gt; (&lt;a href=&quot;optim#torch.optim.Optimizer&quot;&gt;torch.optim.Optimizer&lt;/a&gt;) &amp;ndash; Optimizer that owns the gradients to be unscaled.</source>
          <target state="translated">&lt;strong&gt;优化器&lt;/strong&gt;（&lt;a href=&quot;optim#torch.optim.Optimizer&quot;&gt;torch.optim.Optimizer&lt;/a&gt;）&amp;ndash;拥有要缩放的渐变的优化器。</target>
        </trans-unit>
        <trans-unit id="3eb51832b6aa0f276cb8df9ff35933a83aafdcb4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;optimizer_class&lt;/strong&gt; (&lt;a href=&quot;optim#torch.optim.Optimizer&quot;&gt;optim.Optimizer&lt;/a&gt;) &amp;ndash; the class of optimizer to instantiate on each worker.</source>
          <target state="translated">&lt;strong&gt;optimizer_class&lt;/strong&gt;（&lt;a href=&quot;optim#torch.optim.Optimizer&quot;&gt;optim.Optimizer&lt;/a&gt;）&amp;ndash;在每个worker上实例化的优化器的类。</target>
        </trans-unit>
        <trans-unit id="d4b6fe40a78147f10f4276a3fff8e0f880f89f66" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ord&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;inf&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;-inf&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;'fro'&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;'nuc'&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;ord&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;inf &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;-inf &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;'fro' &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;'nuc' &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="c278df20ef2016b751d8544cf7c0d4206d20cb20" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ortho_fparams, ortho_bparams&lt;/strong&gt; (&lt;em&gt;ortho_iparams&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;) &amp;ndash; various parameters to LOBPCG algorithm when using &lt;code&gt;method=&amp;rdquo;ortho&amp;rdquo;&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;ortho_fparams，ortho_bparams&lt;/strong&gt;（&lt;em&gt;ortho_iparams &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;）&amp;ndash;使用 &lt;code&gt;method=&amp;rdquo;ortho&amp;rdquo;&lt;/code&gt; 时LOBPCG算法的各种参数。</target>
        </trans-unit>
        <trans-unit id="3de7c1b5054808fbf162ba527fa5af49d0d5b2a5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; &amp;ndash; the second input tensor</source>
          <target state="translated">&lt;strong&gt;其他&lt;/strong&gt;&amp;ndash;第二个输入张量</target>
        </trans-unit>
        <trans-unit id="9830ed943f8f4c46a3eaf71558a02ad41461cee1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;a href=&quot;#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt;) &amp;ndash; The result tensor has the same shape as &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;other&lt;/strong&gt;（&lt;a href=&quot;#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;）&amp;ndash;结果张量具有与 &lt;code&gt;other&lt;/code&gt; 相同的形状。</target>
        </trans-unit>
        <trans-unit id="d83937e2055c79b7eeec7c9e6cd7007e8c6ba7b8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;a href=&quot;#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt;) &amp;ndash; The result tensor has the same size as &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;other&lt;/strong&gt;（&lt;a href=&quot;#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;）&amp;ndash;结果张量具有与 &lt;code&gt;other&lt;/code&gt; 相同的大小。</target>
        </trans-unit>
        <trans-unit id="575d583c4e3da2e5f40414a1c2ac1f8053858442" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; second tensor in the dot product.</source>
          <target state="translated">&lt;strong&gt;其他&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;点积中的第二张量。</target>
        </trans-unit>
        <trans-unit id="9683962d0d021c44b4feb57563050d15e672f921" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; second tensor to compare</source>
          <target state="translated">&lt;strong&gt;其他&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;要比较的第二张量</target>
        </trans-unit>
        <trans-unit id="1c95b6612a9c23b59a8184a8bc3fac93a55448ec" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the Right-hand-side input tensor</source>
          <target state="translated">&lt;strong&gt;其他&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;右侧输入张量</target>
        </trans-unit>
        <trans-unit id="03ddfd5bad09382b4da8e30aba277bed53e1d92c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the second input tensor</source>
          <target state="translated">&lt;strong&gt;其他&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;第二个输入张量</target>
        </trans-unit>
        <trans-unit id="7a7c2bdc488731ec762bebe69342927cf0cc7a4e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the second multiplicand tensor</source>
          <target state="translated">&lt;strong&gt;其他&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;第二个被乘张量</target>
        </trans-unit>
        <trans-unit id="006601c4fdb977844e03c057c807959c8ed0607e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the second tensor to be multiplied</source>
          <target state="translated">&lt;strong&gt;other&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;要相乘的第二张量</target>
        </trans-unit>
        <trans-unit id="669f8aaad77167734f2e1b5bbdc071b3c8cca4fa" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to compute AND with</source>
          <target state="translated">&lt;strong&gt;other&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;要计算AND的张量</target>
        </trans-unit>
        <trans-unit id="812d6aefc371af303841ae4e35623a96b151d7af" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to compute OR with</source>
          <target state="translated">&lt;strong&gt;other&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;用于计算OR的张量</target>
        </trans-unit>
        <trans-unit id="cc9311b9409e52a798efcdc865571b70ee24d69a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to compute XOR with</source>
          <target state="translated">&lt;strong&gt;other&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;用于计算XOR的张量</target>
        </trans-unit>
        <trans-unit id="1dc58021bcaef0b4c84bf5fbd8eac7072cf21ee6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the divisor that may be either a number or a Tensor of the same shape as the dividend</source>
          <target state="translated">&lt;strong&gt;其他&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;浮点数&lt;/a&gt;）&amp;ndash;除数可以是数字或与被除数形状相同的张量</target>
        </trans-unit>
        <trans-unit id="4af20d1a0478746ed342d723899ffae24e31673f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the divisor, which may be either a number or a tensor of the same shape as the dividend</source>
          <target state="translated">&lt;strong&gt;其他&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;浮点数&lt;/a&gt;）&amp;ndash;除数，可以是与被除数相同形状的数字或张量</target>
        </trans-unit>
        <trans-unit id="5f7bb660268bfc1f1ce99fb1f5d6144452802639" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the tensor or value to compare</source>
          <target state="translated">&lt;strong&gt;其他&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;浮点数&lt;/a&gt;）&amp;ndash;要比较的张量或值</target>
        </trans-unit>
        <trans-unit id="2083d9580c98436be500de8f122860e994452f50" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; the divisor</source>
          <target state="translated">&lt;strong&gt;其他&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;数字&lt;/em&gt;）&amp;ndash;除数</target>
        </trans-unit>
        <trans-unit id="24c486bb19e0b203a2a62ea6e87e873812911826" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Scalar&lt;/em&gt;) &amp;ndash; the tensor or scalar to subtract from &lt;code&gt;input&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;其他&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;标量&lt;/em&gt;）&amp;ndash;要从 &lt;code&gt;input&lt;/code&gt; 减去的张量或标量</target>
        </trans-unit>
        <trans-unit id="6897c0ab62af0e375bdc00c90649a3563fa23660" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Scalar&lt;/em&gt;) &amp;ndash; the tensor or value to compare</source>
          <target state="translated">&lt;strong&gt;其他&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;标量&lt;/em&gt;）&amp;ndash;要比较的张量或值</target>
        </trans-unit>
        <trans-unit id="1ca11bab125e2ff340132dc741c4418bbb9be48f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;other&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; the number to be multiplied to each element of &lt;code&gt;input&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;other&lt;/strong&gt;（&lt;em&gt;Number&lt;/em&gt;）&amp;ndash;要与 &lt;code&gt;input&lt;/code&gt; 每个元素相乘的数字</target>
        </trans-unit>
        <trans-unit id="8bbe00078fcdd90156cc48ebd3d452934d098736" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output tensor</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;输出张量</target>
        </trans-unit>
        <trans-unit id="4909287a5578c3077500a804d4cacd409ba1cbb4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; If the inputs are &lt;code&gt;torch.float32&lt;/code&gt;, must be &lt;code&gt;torch.complex64&lt;/code&gt;. If the inputs are &lt;code&gt;torch.float64&lt;/code&gt;, must be &lt;code&gt;torch.complex128&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;如果输入为 &lt;code&gt;torch.float32&lt;/code&gt; ，则必须为 &lt;code&gt;torch.complex64&lt;/code&gt; 。如果输入为 &lt;code&gt;torch.float64&lt;/code&gt; ，则必须为 &lt;code&gt;torch.complex128&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="49191fc918c7fbbf6b67857e145e1c13e69615dd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The output tensor</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;输出张量</target>
        </trans-unit>
        <trans-unit id="865f26f7ba269d26b55b78b56f8c40704e8bdc68" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; optional output matrix</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;可选输出矩阵</target>
        </trans-unit>
        <trans-unit id="c4d5ed5877e75a302ce827729fa9cc1a243e1951" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the destination tensor</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;目标张量</target>
        </trans-unit>
        <trans-unit id="c65d5e1626eb8eac6c27cd8f3a19ef6cfca82d2b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output matrix</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;输出矩阵</target>
        </trans-unit>
        <trans-unit id="dc670798304fb38fad30b85d22d05a85c1854899" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output tensor</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;输出张量</target>
        </trans-unit>
        <trans-unit id="2913addd49fe786d1b73d0259fcdb7a5929ea3da" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output tensor for &lt;code&gt;c&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash; &lt;code&gt;c&lt;/code&gt; 的输出张量</target>
        </trans-unit>
        <trans-unit id="14f81c54913f4111d9238607d8ce61fae4bd2cc6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output tensor for &lt;code&gt;inv&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash; &lt;code&gt;inv&lt;/code&gt; 的输出张量</target>
        </trans-unit>
        <trans-unit id="485744ee6c684eac51b20a24c837baecaa778471" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output tensor, must be the same size as &lt;code&gt;input&lt;/code&gt; if provided.</source>
          <target state="translated">&lt;strong&gt;出&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;） -输出张量，必须是大小相同的 &lt;code&gt;input&lt;/code&gt; ，如果提供。</target>
        </trans-unit>
        <trans-unit id="3ebd6deba9d17676dd19fc0c1b7635fe63639228" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output tensor, must be the same size as &lt;code&gt;values&lt;/code&gt; if provided.</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;输出张量必须与 &lt;code&gt;values&lt;/code&gt; 相同（如果提供）。</target>
        </trans-unit>
        <trans-unit id="964eb34dca913baad53314e60e02dd63488de605" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output tensor.</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;输出张量。</target>
        </trans-unit>
        <trans-unit id="fa34d018743c897888089d8a79ee1afc57d1450c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output tensor. Ignored if &lt;code&gt;dim&lt;/code&gt; = &lt;code&gt;None&lt;/code&gt; and &lt;code&gt;out&lt;/code&gt; = &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;输出张量。如果 &lt;code&gt;dim&lt;/code&gt; = &lt;code&gt;None&lt;/code&gt; 且 &lt;code&gt;out&lt;/code&gt; = &lt;code&gt;None&lt;/code&gt; 则忽略。</target>
        </trans-unit>
        <trans-unit id="606bdb70440f1c778aed44d5a2f91dc6db6f4c37" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; optional output tuple. If &lt;code&gt;get_infos&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If &lt;code&gt;get_infos&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then the elements in the tuple are Tensor, IntTensor. Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;元组&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;可选输出元组。如果 &lt;code&gt;get_infos&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则元组中的元素为Tensor，IntTensor和IntTensor。如果 &lt;code&gt;get_infos&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; ，则元组中的元素为Tensor，IntTensor。默认值： &lt;code&gt;None&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="b8efb0d2ac488f256c5bbb46000794a0044cbe5c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the optional destination tensor</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;元组&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;可选目标张量</target>
        </trans-unit>
        <trans-unit id="5381b6d62cf34413978a3eab8706a0bde8040c25" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output tensors</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;元组&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;输出张量</target>
        </trans-unit>
        <trans-unit id="f069527b4f11758b1081d0eb36c8c58ea9e99e0d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output tuple of (&lt;code&gt;Tensor&lt;/code&gt;, &lt;code&gt;LongTensor&lt;/code&gt;) that can be optionally given to be used as output buffers</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;元组&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;（ &lt;code&gt;Tensor&lt;/code&gt; ， &lt;code&gt;LongTensor&lt;/code&gt; ）的输出元组，可以选择将其用作输出缓冲区</target>
        </trans-unit>
        <trans-unit id="e270a7a389c717e8d14893d5b4ccc1bd8f03693e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;（Tensor，LongTensor）的输出元组可以有选择地用作输出缓冲区</target>
        </trans-unit>
        <trans-unit id="1ffc56309f4997d2adc30f856fbd91f3447214a8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;（Tensor，LongTensor）的输出元组，可以选择将其用作输出缓冲区</target>
        </trans-unit>
        <trans-unit id="489596f4185eee384c4b282ccfc92c2a9977b741" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output tuple of (Tensor, Tensor)</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;（Tensor，Tensor）的输出元组</target>
        </trans-unit>
        <trans-unit id="d316ec30ac2f8ba6b2289f13dd8a11cf7021e946" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output tuple of tensors</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;元组&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;张量的输出元组</target>
        </trans-unit>
        <trans-unit id="7708a683bac6a7fafb6df65ba8e0784d087454b8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the result tuple of two output tensors (max, max_indices)</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;两个输出张量的结果元组（max，max_indices）</target>
        </trans-unit>
        <trans-unit id="6a9bb0824abf9ea55c48cd268133b1146f437fd0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the result tuple of two output tensors (values, indices)</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;元组&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;两个输出张量（值，索引）的结果元组</target>
        </trans-unit>
        <trans-unit id="c901d01fb46bbe38d29cc46eda894f717e98cbb5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the tuple of two output tensors (min, min_indices)</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;两个输出张量的元组（min，min_indices）</target>
        </trans-unit>
        <trans-unit id="197a7a690b00cbe491d5fbf6fc25fcca33fdffa6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; tuple of &lt;code&gt;Q&lt;/code&gt; and &lt;code&gt;R&lt;/code&gt; tensors satisfying &lt;code&gt;input = torch.matmul(Q, R)&lt;/code&gt;. The dimensions of &lt;code&gt;Q&lt;/code&gt; and &lt;code&gt;R&lt;/code&gt; are</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;元组&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;满足 &lt;code&gt;input = torch.matmul(Q, R)&lt;/code&gt; 的 &lt;code&gt;Q&lt;/code&gt; 和 &lt;code&gt;R&lt;/code&gt; 张量的元组。的尺寸 &lt;code&gt;Q&lt;/code&gt; 和 &lt;code&gt;R&lt;/code&gt; 是</target>
        </trans-unit>
        <trans-unit id="b5510b7d23f0333ea37fb16d363c66ea86b78cc1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The output tensor. Ignored if &lt;code&gt;None&lt;/code&gt;. Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;输出张量。如果 &lt;code&gt;None&lt;/code&gt; 忽略。默认值： &lt;code&gt;None&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="11b25d060a0bb91a3a6bd4eb9234921ee682e10d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output tensor. If &lt;code&gt;out&lt;/code&gt; is used, this operation won&amp;rsquo;t be differentiable.</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;输出张量。如果使用 &lt;code&gt;out&lt;/code&gt; ，则此操作将不可区分。</target>
        </trans-unit>
        <trans-unit id="e4b8ca618e12fa653720e70948c720d1330c823f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;keyword-only&lt;/em&gt;) &amp;ndash; the tensor to store gather result. Its sizes must match those of &lt;code&gt;tensors&lt;/code&gt;, except for &lt;code&gt;dim&lt;/code&gt;, where the size must equal &lt;code&gt;sum(tensor.size(dim) for tensor in tensors)&lt;/code&gt;. Can be on CPU or CUDA.</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选的&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;仅关键字&lt;/em&gt;）&amp;ndash;用于存储收集结果的张量。它的大小必须与 &lt;code&gt;tensors&lt;/code&gt; 大小匹配，除了 &lt;code&gt;dim&lt;/code&gt; 之外，后者的大小必须等于 &lt;code&gt;sum(tensor.size(dim) for tensor in tensors)&lt;/code&gt; 。可以在CPU或CUDA上。</target>
        </trans-unit>
        <trans-unit id="d7bbe5a42e951041522cf9dd2a446fdcc33325b9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;em&gt;(&lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;)&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; optional output tuple.</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;em&gt;（&lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;）&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;可选的输出元组。</target>
        </trans-unit>
        <trans-unit id="209dfca5f1c305b375517af898ffea87b3b235b7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the output tensor containing indices</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;em&gt;LongTensor &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;包含索引的输出张量</target>
        </trans-unit>
        <trans-unit id="32fca21fc6c66cdf66c39e78522f6ed5d3485149" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;em&gt;Sequence&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;keyword-only&lt;/em&gt;) &amp;ndash; the GPU tensors to store output results.</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;em&gt;Sequence &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;] &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;仅关键字&lt;/em&gt;）&amp;ndash;用于存储输出结果的GPU张量。</target>
        </trans-unit>
        <trans-unit id="d075f0b19db2420856dfc3cd7bd9e5e606444306" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out&lt;/strong&gt; (&lt;em&gt;Sequence&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;keyword-only&lt;/em&gt;) &amp;ndash; the GPU tensors to store output results. Sizes of these tensors must match that of &lt;code&gt;tensor&lt;/code&gt;, except for &lt;code&gt;dim&lt;/code&gt;, where the total size must sum to &lt;code&gt;tensor.size(dim)&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;out&lt;/strong&gt;（&lt;em&gt;Sequence &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;] &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;仅关键字&lt;/em&gt;）&amp;ndash;用于存储输出结果的GPU张量。这些张量的大小必须匹配的 &lt;code&gt;tensor&lt;/code&gt; ，除了 &lt;code&gt;dim&lt;/code&gt; ，其中总尺寸之和必须为 &lt;code&gt;tensor.size(dim)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="fbf9c4879b754e5485d4adfd6cf789f5297488df" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out_channels&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Number of channels produced by the convolution</source>
          <target state="translated">&lt;strong&gt;out_channels&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;卷积产生的通道数</target>
        </trans-unit>
        <trans-unit id="cb2ef6ac4d28b8ca3c953e0855d6ae16d965ac4f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out_features&lt;/strong&gt; &amp;ndash; size of each output sample</source>
          <target state="translated">&lt;strong&gt;out_features&lt;/strong&gt; &amp;ndash;每个输出样本的大小</target>
        </trans-unit>
        <trans-unit id="d7de66fc4b74ea02276b09800cfd2ae532e3bc76" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;out_int32&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; indicate the output data type. torch.int32 if True, torch.int64 otherwise. Default value is False, i.e. default output data type is torch.int64.</source>
          <target state="translated">&lt;strong&gt;out_int32&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;指示输出数据类型。如果为True，则为torch.int32；否则，为torch.int64。默认值为False，即默认输出数据类型为torch.int64。</target>
        </trans-unit>
        <trans-unit id="d4c83d604511fcf859161efc385f906819b96864" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Output tensor.</source>
          <target state="translated">&lt;strong&gt;输出&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;输出张量。</target>
        </trans-unit>
        <trans-unit id="54e1f470236ead45903663caa70f84c32cb8d0d4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): the output list of unique scalar elements.</source>
          <target state="translated">&lt;strong&gt;output&lt;/strong&gt;（&lt;em&gt;Tensor&lt;/em&gt;）：唯一标量元素的输出列表。</target>
        </trans-unit>
        <trans-unit id="8042cc420b9244c9f4146afa6f88da5858ea8729" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output&lt;/strong&gt; is a Tensor of size &lt;code&gt;N&lt;/code&gt; containing computed target log probabilities for each example</source>
          <target state="translated">&lt;strong&gt;输出&lt;/strong&gt;是大小为 &lt;code&gt;N&lt;/code&gt; 的张量，其中包含每个示例的计算目标对数概率</target>
        </trans-unit>
        <trans-unit id="ee40fd7a70a6bf8cc86c927f28159d12e52c4fcd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output&lt;/strong&gt; of shape &lt;code&gt;(seq_len, batch, num_directions * hidden_size)&lt;/code&gt;: tensor containing the output features (&lt;code&gt;h_t&lt;/code&gt;) from the last layer of the RNN, for each &lt;code&gt;t&lt;/code&gt;. If a &lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;torch.nn.utils.rnn.PackedSequence&lt;/code&gt;&lt;/a&gt; has been given as the input, the output will also be a packed sequence.</source>
          <target state="translated">&lt;strong&gt;&lt;/strong&gt;形状的&lt;strong&gt;输出&lt;/strong&gt; &lt;code&gt;(seq_len, batch, num_directions * hidden_size)&lt;/code&gt; ：每个 &lt;code&gt;t&lt;/code&gt; 都包含来自RNN的最后一层的输出特征（ &lt;code&gt;h_t&lt;/code&gt; ）的张量。如果已给定&lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt; &lt;code&gt;torch.nn.utils.rnn.PackedSequence&lt;/code&gt; &lt;/a&gt;作为输入，则输出也将是打包序列。</target>
        </trans-unit>
        <trans-unit id="1dc2c70a675e8e584ab85a82d21f959742d0aaf7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output&lt;/strong&gt; of shape &lt;code&gt;(seq_len, batch, num_directions * hidden_size)&lt;/code&gt;: tensor containing the output features &lt;code&gt;(h_t)&lt;/code&gt; from the last layer of the LSTM, for each &lt;code&gt;t&lt;/code&gt;. If a &lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;torch.nn.utils.rnn.PackedSequence&lt;/code&gt;&lt;/a&gt; has been given as the input, the output will also be a packed sequence.</source>
          <target state="translated">&lt;strong&gt;&lt;/strong&gt;形状的&lt;strong&gt;输出&lt;/strong&gt; &lt;code&gt;(seq_len, batch, num_directions * hidden_size)&lt;/code&gt; ：张量，包含每个 &lt;code&gt;t&lt;/code&gt; 的LSTM最后一层的输出特征 &lt;code&gt;(h_t)&lt;/code&gt; 。如果已给定&lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt; &lt;code&gt;torch.nn.utils.rnn.PackedSequence&lt;/code&gt; &lt;/a&gt;作为输入，则输出也将是打包序列。</target>
        </trans-unit>
        <trans-unit id="44bac24d4e0073747a76bcde5580e1630789e3cd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output&lt;/strong&gt; of shape &lt;code&gt;(seq_len, batch, num_directions * hidden_size)&lt;/code&gt;: tensor containing the output features h_t from the last layer of the GRU, for each &lt;code&gt;t&lt;/code&gt;. If a &lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;torch.nn.utils.rnn.PackedSequence&lt;/code&gt;&lt;/a&gt; has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using &lt;code&gt;output.view(seq_len, batch, num_directions, hidden_size)&lt;/code&gt;, with forward and backward being direction &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; respectively.</source>
          <target state="translated">&lt;strong&gt;&lt;/strong&gt;形状的&lt;strong&gt;输出&lt;/strong&gt; &lt;code&gt;(seq_len, batch, num_directions * hidden_size)&lt;/code&gt; ：每个 &lt;code&gt;t&lt;/code&gt; 都包含来自GRU的最后一层的输出特征h_t的张量。如果已给定&lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt; &lt;code&gt;torch.nn.utils.rnn.PackedSequence&lt;/code&gt; &lt;/a&gt;作为输入，则输出也将是打包序列。对于未 &lt;code&gt;output.view(seq_len, batch, num_directions, hidden_size)&lt;/code&gt; 情况，可以使用output.view（seq_len，batch，num_directions，hidden_​​size）分隔方向，前进和后退分别是方向 &lt;code&gt;0&lt;/code&gt; 和 &lt;code&gt;1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="255f1d7d987dadd4223a022923279a9324511b84" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_device&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;) &amp;ndash; Device location of output for single-device CUDA modules. For multi-device modules and CPU modules, it must be &lt;code&gt;None&lt;/code&gt;, and the module itself dictates the output location. (default: &lt;code&gt;device_ids[0]&lt;/code&gt; for single-device modules)</source>
          <target state="translated">&lt;strong&gt;output_device&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;）&amp;ndash;单设备CUDA模块的输出的设备位置。对于多设备模块和CPU模块，它必须为 &lt;code&gt;None&lt;/code&gt; ，并且模块本身指定输出位置。（对于单设备模块，默认值： &lt;code&gt;device_ids[0]&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="8b601fc16628c28a84e5091b7164b5ba737b9327" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_device&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;) &amp;ndash; device location of output (default: device_ids[0])</source>
          <target state="translated">&lt;strong&gt;output_device&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;）&amp;ndash;输出的设备位置（默认：device_ids [0]）</target>
        </trans-unit>
        <trans-unit id="2e98efe0558eb0f88016b3110b664c00e2dd175e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_device&lt;/strong&gt; (&lt;em&gt;list of python:int&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device&lt;/a&gt;) &amp;ndash; GPU location of the output Use -1 to indicate the CPU. (default: device_ids[0])</source>
          <target state="translated">&lt;strong&gt;output_device&lt;/strong&gt;（&lt;em&gt;python：int&lt;/em&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;torch.device的&lt;/a&gt;&lt;em&gt;列表&lt;/em&gt;）&amp;ndash;输出的GPU位置使用-1表示CPU。（默认值：device_ids [0]）</target>
        </trans-unit>
        <trans-unit id="cd583af9b2638b2ed135367bf25e08abf6423093" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_names&lt;/strong&gt; (&lt;em&gt;list of strings&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default empty list&lt;/em&gt;) &amp;ndash; names to assign to the output nodes of the graph, in order</source>
          <target state="translated">&lt;strong&gt;output_names&lt;/strong&gt;（&lt;em&gt;字符串列表&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;默认为空列表&lt;/em&gt;）&amp;ndash;按顺序分配给图的输出节点的名称</target>
        </trans-unit>
        <trans-unit id="f491b241ade48cc378ec5122b4841e99e9754e6b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_padding&lt;/strong&gt; &amp;ndash; additional size added to one side of each dimension in the output shape. Can be a single number or a tuple &lt;code&gt;(out_padH, out_padW)&lt;/code&gt;. Default: 0</source>
          <target state="translated">&lt;strong&gt;output_padding&lt;/strong&gt; &amp;ndash;在输出形状的每个尺寸的一侧添加了额外的尺寸。可以是单个数字或元组 &lt;code&gt;(out_padH, out_padW)&lt;/code&gt; 。默认值：0</target>
        </trans-unit>
        <trans-unit id="734ccdbb1e5796890980cb4649cd409e247b53a2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_padding&lt;/strong&gt; &amp;ndash; additional size added to one side of each dimension in the output shape. Can be a single number or a tuple &lt;code&gt;(out_padT, out_padH, out_padW)&lt;/code&gt;. Default: 0</source>
          <target state="translated">&lt;strong&gt;output_padding&lt;/strong&gt; &amp;ndash;在输出形状的每个尺寸的一侧添加了额外的尺寸。可以是单个数字或元组 &lt;code&gt;(out_padT, out_padH, out_padW)&lt;/code&gt; 。默认值：0</target>
        </trans-unit>
        <trans-unit id="658b917621e239d640e2b03f43964b708d2c55ad" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_padding&lt;/strong&gt; &amp;ndash; additional size added to one side of each dimension in the output shape. Can be a single number or a tuple &lt;code&gt;(out_padW)&lt;/code&gt;. Default: 0</source>
          <target state="translated">&lt;strong&gt;output_padding&lt;/strong&gt; &amp;ndash;在输出形状的每个尺寸的一侧添加了额外的尺寸。可以是单个数字或元组 &lt;code&gt;(out_padW)&lt;/code&gt; 。默认值：0</target>
        </trans-unit>
        <trans-unit id="f16b08ff1eb258821023064b32148058b9079ce3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_padding&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Additional size added to one side of each dimension in the output shape. Default: 0</source>
          <target state="translated">&lt;strong&gt;output_padding&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;在输出形状的每个尺寸的一侧添加的附加大小。默认值：0</target>
        </trans-unit>
        <trans-unit id="a4a46c3866c2c1510433f2a4903dbbbe5fde896f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_padding&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Additional size added to one side of the output shape. Default: 0</source>
          <target state="translated">&lt;strong&gt;output_padding&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;在输出形状的一侧添加的附加大小。默认值：0</target>
        </trans-unit>
        <trans-unit id="2d697a1fa9ed20cfea2be8860a73c26c65201a48" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_ratio&lt;/strong&gt; &amp;ndash; If one wants to have an output size as a ratio of the input size, this option can be given. This has to be a number or tuple in the range (0, 1)</source>
          <target state="translated">&lt;strong&gt;output_ratio&lt;/strong&gt; &amp;ndash;如果希望输出大小与输入大小的比率，则可以指定此选项。这必须是范围为（0，1）的数字或元组</target>
        </trans-unit>
        <trans-unit id="d139558d8497f0a45efe69a095d8ca0b00a79bb1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_size&lt;/strong&gt; &amp;ndash; the target output size (single integer or double-integer tuple)</source>
          <target state="translated">&lt;strong&gt;output_size&lt;/strong&gt; &amp;ndash;目标输出大小（单整数或双整数元组）</target>
        </trans-unit>
        <trans-unit id="9aef33d381ddbb64a977de5875d434c15130e1b3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_size&lt;/strong&gt; &amp;ndash; the target output size (single integer or triple-integer tuple)</source>
          <target state="translated">&lt;strong&gt;output_size&lt;/strong&gt; &amp;ndash;目标输出大小（单整数或三整数元组）</target>
        </trans-unit>
        <trans-unit id="d7553c6c16b6821129bbbbffa2b2ab7a88d48c73" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_size&lt;/strong&gt; &amp;ndash; the target output size (single integer)</source>
          <target state="translated">&lt;strong&gt;output_size&lt;/strong&gt; &amp;ndash;目标输出大小（单个整数）</target>
        </trans-unit>
        <trans-unit id="5b92d42ba70c23bb2437e683e87b4a32323b7656" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_size&lt;/strong&gt; &amp;ndash; the target output size H</source>
          <target state="translated">&lt;strong&gt;output_size&lt;/strong&gt; &amp;ndash;目标输出大小H</target>
        </trans-unit>
        <trans-unit id="72273a2ad460dccb1f289d60806321a98b8260ff" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_size&lt;/strong&gt; &amp;ndash; the target output size of the form D x H x W. Can be a tuple (D, H, W) or a single number D for a cube D x D x D. D, H and W can be either a &lt;code&gt;int&lt;/code&gt;, or &lt;code&gt;None&lt;/code&gt; which means the size will be the same as that of the input.</source>
          <target state="translated">&lt;strong&gt;output_size&lt;/strong&gt; &amp;ndash;目标输出大小，格式为D &lt;strong&gt;xH xW&lt;/strong&gt;。可以是元组（D，H，W），也可以是多维数据集D &lt;strong&gt;xD&lt;/strong&gt; x D的单个数字D。D，H和W可以是 &lt;code&gt;int&lt;/code&gt; 或&amp;ldquo; &lt;code&gt;None&lt;/code&gt; ，这意味着大小将与输入的大小相同。</target>
        </trans-unit>
        <trans-unit id="ffbcc66c67e1dc1781d70637ab1a7011a5a86b0c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_size&lt;/strong&gt; &amp;ndash; the target output size of the image of the form &lt;code&gt;oH x oW&lt;/code&gt;. Can be a tuple &lt;code&gt;(oH, oW)&lt;/code&gt; or a single number oH for a square image &lt;code&gt;oH x oH&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;output_size&lt;/strong&gt; &amp;ndash; &lt;code&gt;oH x oW&lt;/code&gt; 形式的图像的目标输出大小。可以是元组 &lt;code&gt;(oH, oW)&lt;/code&gt; 也可以是正方形图像 &lt;code&gt;oH x oH&lt;/code&gt; 的一个数字oH</target>
        </trans-unit>
        <trans-unit id="2b13fdfcd89a724ec673724cf570c1569f4c6c2e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_size&lt;/strong&gt; &amp;ndash; the target output size of the image of the form D x H x W. Can be a tuple (D, H, W) or a single D for a cube D x D x D. D, H and W can be either a &lt;code&gt;int&lt;/code&gt;, or &lt;code&gt;None&lt;/code&gt; which means the size will be the same as that of the input.</source>
          <target state="translated">&lt;strong&gt;output_size&lt;/strong&gt; &amp;ndash;格式为D &lt;strong&gt;xH&lt;/strong&gt; x W的图像的目标输出大小。可以是元组（D，H，W），也可以是多维数据集D xD x D的单个D。D，H和W可以是一个 &lt;code&gt;int&lt;/code&gt; ，即 &lt;code&gt;None&lt;/code&gt; ，它的大小将与输入的大小相同。</target>
        </trans-unit>
        <trans-unit id="3da4b11e13dd7cb4717e94a2c74219954c4fc38b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_size&lt;/strong&gt; &amp;ndash; the target output size of the image of the form H x W. Can be a tuple (H, W) or a single H for a square image H x H. H and W can be either a &lt;code&gt;int&lt;/code&gt;, or &lt;code&gt;None&lt;/code&gt; which means the size will be the same as that of the input.</source>
          <target state="translated">&lt;strong&gt;output_size&lt;/strong&gt; &amp;ndash; H x W形式的图像的目标输出尺寸。可以是元组（H，W），也可以是正方形图像H x H的单个H。H和W可以是 &lt;code&gt;int&lt;/code&gt; ，也可以是 &lt;code&gt;None&lt;/code&gt; ，这意味着大小将与输入的大小相同。</target>
        </trans-unit>
        <trans-unit id="94b03ab4769532cfd586a9f93960f9dbdb397633" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; the shape of the spatial dimensions of the output (i.e., &lt;code&gt;output.sizes()[2:]&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;output_size&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;）&amp;ndash;输出的空间尺寸形状（即 &lt;code&gt;output.sizes()[2:]&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="a3faf432bc4aa5f657bed43eb96ddf6068193374" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_tensor_list&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; List of tensors to be gathered one per rank.</source>
          <target state="translated">&lt;strong&gt;output_tensor_list&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list &lt;/a&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;每个等级要收集的张量列表。</target>
        </trans-unit>
        <trans-unit id="859379055368818b2ccae1f2f63e2ee0a75004d4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_tensor_list&lt;/strong&gt; (&lt;em&gt;List&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;output_tensor_list&lt;/strong&gt; (&lt;em&gt;List&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash;</target>
        </trans-unit>
        <trans-unit id="0978f0a55922d996ca9a0fd7211f8e99295fc05a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;output_tensor_lists&lt;/strong&gt; (&lt;em&gt;List&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;em&gt;List&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;output_tensor_lists&lt;/strong&gt; (&lt;em&gt;List&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;em&gt;List&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash;</target>
        </trans-unit>
        <trans-unit id="088213e8c0ac027cdebbe4c6a7f1f41158f63b10" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;outputs&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;iterable of Tensors&lt;/em&gt;) &amp;ndash; Outputs to scale.</source>
          <target state="translated">&lt;strong&gt;输出&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;张量的&lt;em&gt;可迭代&lt;/em&gt;）&amp;ndash;按比例缩放的输出。</target>
        </trans-unit>
        <trans-unit id="a07f83b85bbf21108c57977c33c5fd41bf4b861e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;outputs&lt;/strong&gt; (&lt;em&gt;sequence of Tensor&lt;/em&gt;) &amp;ndash; outputs of the differentiated function.</source>
          <target state="translated">&lt;strong&gt;输出&lt;/strong&gt;（&lt;em&gt;张量序列&lt;/em&gt;）&amp;ndash;微分函数的输出。</target>
        </trans-unit>
        <trans-unit id="79584f8c8990b1c876dbadf460c2b02b547a0a20" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;p&lt;/strong&gt; &amp;ndash; dropout probability of a channel to be zeroed. Default: 0.5</source>
          <target state="translated">&lt;strong&gt;p&lt;/strong&gt; &amp;ndash;要归零的通道的丢失概率。默认值：0.5</target>
        </trans-unit>
        <trans-unit id="bd6aaa726614b383781b34c1c9759a061af7eef1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;p&lt;/strong&gt; &amp;ndash; p value for the p-norm distance to calculate between each vector pair</source>
          <target state="translated">&lt;strong&gt;p&lt;/strong&gt; &amp;ndash; p值的p范数距离，用于计算每个向量对之间的距离</target>
        </trans-unit>
        <trans-unit id="60e05b10c2aa191fd0f512b35139ad3f5166ee85" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;p&lt;/strong&gt; &amp;ndash; probability of a channel to be zeroed. Default: 0.5</source>
          <target state="translated">&lt;strong&gt;p&lt;/strong&gt; &amp;ndash;信道归零的概率。默认值：0.5</target>
        </trans-unit>
        <trans-unit id="055d3b3dc7e2825662e52c7899ba890592b92a5b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;p&lt;/strong&gt; &amp;ndash; probability of an element to be zeroed. Default: 0.5</source>
          <target state="translated">&lt;strong&gt;p&lt;/strong&gt; &amp;ndash;元素归零的概率。默认值：0.5</target>
        </trans-unit>
        <trans-unit id="ed3ed17d9e98cf2b21b9f8991f3865f41f4dc9da" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;p&lt;/strong&gt; (&lt;a href=&quot;#torch.distributions.distribution.Distribution&quot;&gt;Distribution&lt;/a&gt;) &amp;ndash; A &lt;code&gt;Distribution&lt;/code&gt; object.</source>
          <target state="translated">&lt;strong&gt;p&lt;/strong&gt;（&lt;a href=&quot;#torch.distributions.distribution.Distribution&quot;&gt;Distribution&lt;/a&gt;）&amp;ndash;一个 &lt;code&gt;Distribution&lt;/code&gt; 对象。</target>
        </trans-unit>
        <trans-unit id="fc61e4898026558df455ea228711212cc5f6bf2e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;p&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; probability of an element to be dropped. Default: 0.5</source>
          <target state="translated">&lt;strong&gt;p&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;元素被删除的概率。默认值：0.5</target>
        </trans-unit>
        <trans-unit id="be2c8a91ba679dbe9334ded6489b8da7b91810a8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;p&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the exponent value in the norm formulation. Default: 2</source>
          <target state="translated">&lt;strong&gt;p&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;范数公式中的指数值。默认值：2</target>
        </trans-unit>
        <trans-unit id="8ba0723e1b20dae5871248bdf08777de23e09a35" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;p&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the power for the norm computation</source>
          <target state="translated">&lt;strong&gt;p&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;范数计算的幂</target>
        </trans-unit>
        <trans-unit id="e35faf459e929b30d01b57525c4b3b7ff20ddb4b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;p&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; probability of an element to be zero-ed.</source>
          <target state="translated">&lt;strong&gt;p&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;元素归零的概率。</target>
        </trans-unit>
        <trans-unit id="c610f90195dd6efe31a7edae7cac213bdfe97ff2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;p&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; probability of an element to be zeroed.</source>
          <target state="translated">&lt;strong&gt;p&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;元素归零的概率。</target>
        </trans-unit>
        <trans-unit id="b58fa037642d7f64f2e095fa9889db70e6bb817b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;p&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the norm to be computed</source>
          <target state="translated">&lt;strong&gt;p&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;要计算的范数</target>
        </trans-unit>
        <trans-unit id="d02eaf36455552ed32ca3c35a59e5ff0754e7bc2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;p&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the number of dimensions</source>
          <target state="translated">&lt;strong&gt;p&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;维数</target>
        </trans-unit>
        <trans-unit id="94c331619b61acdfddc0975afbf5ab2cd627bfa1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;p&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;inf&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;-inf&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;'fro'&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;'nuc'&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;p&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;inf &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;-inf &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;'fro' &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;'nuc' &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="5885620cdb9d6e6007a7bfaa1257ea3fcbf35862" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;p&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Has a default value of</source>
          <target state="translated">&lt;strong&gt;p&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;默认值为</target>
        </trans-unit>
        <trans-unit id="eca1775966b053a849d07171e8c7fca4bc68e061" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;p&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The norm degree for pairwise distance. Default:</source>
          <target state="translated">&lt;strong&gt;p&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;成对距离的范数。默认：</target>
        </trans-unit>
        <trans-unit id="fd42949d40f645424a960ea51a5b0155e9547aae" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;p&lt;/strong&gt; (&lt;em&gt;real&lt;/em&gt;) &amp;ndash; the norm degree. Default: 2</source>
          <target state="translated">&lt;strong&gt;p&lt;/strong&gt;（&lt;em&gt;实数&lt;/em&gt;）&amp;ndash;范数。默认值：2</target>
        </trans-unit>
        <trans-unit id="2a7801e112cc1d15948e989881ab27456d36b8d7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pad&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; m-elements tuple, where</source>
          <target state="translated">&lt;strong&gt;填充&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;元组&lt;/a&gt;）&amp;ndash; m元素元组，其中</target>
        </trans-unit>
        <trans-unit id="f2e07cdbfd62345d8b7cd60d6940e8a0fbab4ae2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pad_mode&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; controls the padding method used when &lt;code&gt;center&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;. Default: &lt;code&gt;&quot;reflect&quot;&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;pad_mode&lt;/strong&gt;（&lt;em&gt;字符串&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;控制 &lt;code&gt;center&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; 时使用的填充方法。默认值： &lt;code&gt;&quot;reflect&quot;&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="8a9ca37fdb632d5816c23727858cdabb73430e65" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash; &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple &lt;code&gt;(padH, padW)&lt;/code&gt;. Default: 0</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash; &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; 零填充将添加到输入中每个维度的两侧。可以是单个数字或元组 &lt;code&gt;(padH, padW)&lt;/code&gt; 。默认值：0</target>
        </trans-unit>
        <trans-unit id="7562e0bf8b2fbf8f0612a3b04ddb3e0c77c8e2de" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash; &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple &lt;code&gt;(padT, padH, padW)&lt;/code&gt;. Default: 0</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash; &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; 零填充将添加到输入中每个维度的两侧。可以是单个数字或元组 &lt;code&gt;(padT, padH, padW)&lt;/code&gt; 。默认值：0</target>
        </trans-unit>
        <trans-unit id="448222bb6d7290e9d682e5dc18b926f4f3f2a062" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash; &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple &lt;code&gt;(padW,)&lt;/code&gt;. Default: 0</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash; &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; 零填充将添加到输入中每个维度的两侧。可以是一个数字或一个元组 &lt;code&gt;(padW,)&lt;/code&gt; 。默认值：0</target>
        </trans-unit>
        <trans-unit id="c82509396ecabb06e2ab93e27c0d96e3494c85e1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash; Implicit negative infinity padding to be added on both sides, must be &amp;gt;= 0 and &amp;lt;= kernel_size / 2.</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash;在两侧都添加隐式负无穷大填充，必须为&amp;gt; = 0和&amp;lt;= kernel_size / 2。</target>
        </trans-unit>
        <trans-unit id="035775c3f31b9fdf2b9087a4ab72feb97800345a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash; implicit paddings on both sides of the input. Can be a single number or a one-element tuple &lt;code&gt;(padW,)&lt;/code&gt;. Default: 0</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash;输入两侧的隐式填充。可以是一个数字或一个元素的元组 &lt;code&gt;(padW,)&lt;/code&gt; 。默认值：0</target>
        </trans-unit>
        <trans-unit id="eb115b29f2d4e8986dbb7361d1bde22d523aa2fe" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash; implicit paddings on both sides of the input. Can be a single number or a tuple &lt;code&gt;(padD, padH, padW)&lt;/code&gt;. Default: 0</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash;输入两侧的隐式填充。可以是单个数字或元组 &lt;code&gt;(padD, padH, padW)&lt;/code&gt; 。默认值：0</target>
        </trans-unit>
        <trans-unit id="c77720bd3be235f43a31a948921e71d65c1c2909" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash; implicit paddings on both sides of the input. Can be a single number or a tuple &lt;code&gt;(padH, padW)&lt;/code&gt;. Default: 0</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash;输入两侧的隐式填充。可以是单个数字或元组 &lt;code&gt;(padH, padW)&lt;/code&gt; 。默认值：0</target>
        </trans-unit>
        <trans-unit id="3615da94c4b802be1f5c636b261921ec43ca9b61" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash; implicit paddings on both sides of the input. Can be a single number or a tuple &lt;code&gt;(padT, padH, padW)&lt;/code&gt;. Default: 0</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash;输入两侧的隐式填充。可以是单个数字或元组 &lt;code&gt;(padT, padH, padW)&lt;/code&gt; 。默认值：0</target>
        </trans-unit>
        <trans-unit id="0da72c244db3c1c00f8e0e0ec331cec0af62c835" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash; implicit paddings on both sides of the input. Can be a single number or a tuple &lt;code&gt;(padW,)&lt;/code&gt;. Default: 0</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash;输入两侧的隐式填充。可以是一个数字或一个元组 &lt;code&gt;(padW,)&lt;/code&gt; 。默认值：0</target>
        </trans-unit>
        <trans-unit id="d76aaa6cddce6d9de011fab349f55959decabc91" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash; implicit zero padding to be added on all three sides</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash;在所有三个面上都添加隐式零填充</target>
        </trans-unit>
        <trans-unit id="c6eb557a7d88c437df862c9c9d77547955ec2e9c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash; implicit zero padding to be added on both sides</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash;在两边都添加隐式零填充</target>
        </trans-unit>
        <trans-unit id="5b5113170b2a3f7e706888c5ccdb6b94ee870598" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash; implicit zero paddings on both sides of the input. Can be a single number or a tuple &lt;code&gt;(padH, padW)&lt;/code&gt;. Default: 0</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash;输入两侧的隐式零填充。可以是单个数字或元组 &lt;code&gt;(padH, padW)&lt;/code&gt; 。默认值：0</target>
        </trans-unit>
        <trans-unit id="85e6b037c865499fe258c3d24a4079141415f86f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash; implicit zero paddings on both sides of the input. Can be a single number or a tuple &lt;code&gt;(padT, padH, padW)&lt;/code&gt;, Default: 0</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash;输入两侧的隐式零填充。可以是单个数字或元组 &lt;code&gt;(padT, padH, padW)&lt;/code&gt; ，默认值：0</target>
        </trans-unit>
        <trans-unit id="fe955794273a21194f7067e1ae48e4951e5e799b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash; implicit zero paddings on both sides of the input. Can be a single number or a tuple &lt;code&gt;(padW,)&lt;/code&gt;. Default: 0</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt; &amp;ndash;输入两侧的隐式零填充。可以是一个数字或一个元组 &lt;code&gt;(padW,)&lt;/code&gt; 。默认值：0</target>
        </trans-unit>
        <trans-unit id="1772edf760a9f7e7c4c981c7145fff686e883581" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; Padding that was added to the input</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;）&amp;ndash;已添加到输入中的填充</target>
        </trans-unit>
        <trans-unit id="9378d98a5c97c1044f08880972e01ac3393dbb72" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; zero-padding will be added to both sides of each dimension in the input. Default: 0</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash; &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; 零填充将添加到输入中每个维度的两侧。默认值：0</target>
        </trans-unit>
        <trans-unit id="6bab43419d41e24875998f0b0d4b8567ba1bd7b1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; zero-padding will be added to both sides of the input. Default: 0</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash; &lt;code&gt;dilation * (kernel_size - 1) - padding&lt;/code&gt; 零填充将添加到输入的两侧。默认值：0</target>
        </trans-unit>
        <trans-unit id="198506b9fb9056c9654540b4487b630e49f578c2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Zero-padding added to all three sides of the input. Default: 0</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;将零填充添加到输入的所有三个侧面。默认值：0</target>
        </trans-unit>
        <trans-unit id="71ff1d7f40410d7f42df028ec0f884ae0b40ef54" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Zero-padding added to both sides of the input. Default: 0</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;将零填充添加到输入的两侧。默认值：0</target>
        </trans-unit>
        <trans-unit id="acde30c2a2360aeb1bb9367ebe202c70986d7598" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; implicit zero padding to be added on both sides of input. Default: 0</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;要在输入的两侧添加隐式零填充。默认值：0</target>
        </trans-unit>
        <trans-unit id="661ddd64fdf787533b21bb87f5850bcd7ca62102" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; the size of the padding. If is &lt;code&gt;int&lt;/code&gt;, uses the same padding in all boundaries. If a 2-&lt;code&gt;tuple&lt;/code&gt;, uses (</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;）&amp;ndash;填充的大小。如果为 &lt;code&gt;int&lt;/code&gt; ，则在所有边界中使用相同的填充。如果是2 &lt;code&gt;tuple&lt;/code&gt; ，则使用（</target>
        </trans-unit>
        <trans-unit id="f8c452117fc928955dd2bd55760ff8548c5d6e4c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; the size of the padding. If is &lt;code&gt;int&lt;/code&gt;, uses the same padding in all boundaries. If a 4-&lt;code&gt;tuple&lt;/code&gt;, uses (</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;）&amp;ndash;填充的大小。如果为 &lt;code&gt;int&lt;/code&gt; ，则在所有边界中使用相同的填充。如果是4 &lt;code&gt;tuple&lt;/code&gt; ，则使用（</target>
        </trans-unit>
        <trans-unit id="0c871849a3c90385e2d2dfb4fade7f685e558919" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; the size of the padding. If is &lt;code&gt;int&lt;/code&gt;, uses the same padding in all boundaries. If a 6-&lt;code&gt;tuple&lt;/code&gt;, uses (</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;）&amp;ndash;填充的大小。如果为 &lt;code&gt;int&lt;/code&gt; ，则在所有边界中使用相同的填充。如果是6 &lt;code&gt;tuple&lt;/code&gt; ，则使用（</target>
        </trans-unit>
        <trans-unit id="de0fe1f500b814cb0dab544c02e18d3f627fc075" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; the size of the padding. If is &lt;code&gt;int&lt;/code&gt;, uses the same padding in both boundaries. If a 2-&lt;code&gt;tuple&lt;/code&gt;, uses (</source>
          <target state="translated">&lt;strong&gt;padding&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;）&amp;ndash;填充的大小。如果为 &lt;code&gt;int&lt;/code&gt; ，则在两个边界中使用相同的填充。如果是2 &lt;code&gt;tuple&lt;/code&gt; ，则使用（</target>
        </trans-unit>
        <trans-unit id="9401caef7f5afbf352224bad95b3fe76778191fe" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding_idx&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If given, pads the output with the embedding vector at &lt;code&gt;padding_idx&lt;/code&gt; (initialized to zeros) whenever it encounters the index.</source>
          <target state="translated">&lt;strong&gt;padding_idx&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;如果给定，则在遇到索引时在 &lt;code&gt;padding_idx&lt;/code&gt; （初始化为零）处使用嵌入矢量填充输出。</target>
        </trans-unit>
        <trans-unit id="9ada542978419d7c3da9e87c076d2586fc884e6e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding_idx&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; See module initialization documentation.</source>
          <target state="translated">&lt;strong&gt;padding_idx&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;请参阅模块初始化文档。</target>
        </trans-unit>
        <trans-unit id="3943fa317748711d3f74fb28e963a67dc6ae1023" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding_mode&lt;/strong&gt; &amp;ndash; the padding mode to use. Only &amp;ldquo;zeros&amp;rdquo; is supported for quantized convolution at the moment. Default: &amp;ldquo;zeros&amp;rdquo;</source>
          <target state="translated">&lt;strong&gt;padding_mode&lt;/strong&gt; &amp;ndash;要使用的填充模式。目前仅对量化卷积支持&amp;ldquo;零&amp;rdquo;。默认值：&amp;ldquo;零&amp;rdquo;</target>
        </trans-unit>
        <trans-unit id="d48e37505fe88a204a4698afd9bd53c6178eb622" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding_mode&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; padding mode for outside grid values &lt;code&gt;'zeros'&lt;/code&gt; | &lt;code&gt;'border'&lt;/code&gt; | &lt;code&gt;'reflection'&lt;/code&gt;. Default: &lt;code&gt;'zeros'&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;padding_mode&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;）&amp;ndash;外部网格值 &lt;code&gt;'zeros'&lt;/code&gt; 填充模式| &lt;code&gt;'border'&lt;/code&gt; | &lt;code&gt;'reflection'&lt;/code&gt; 。默认值： &lt;code&gt;'zeros'&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="4afe9a05f840c246091966bf0c12fea8eff4ade1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding_mode&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; &lt;code&gt;'zeros'&lt;/code&gt;, &lt;code&gt;'reflect'&lt;/code&gt;, &lt;code&gt;'replicate'&lt;/code&gt; or &lt;code&gt;'circular'&lt;/code&gt;. Default: &lt;code&gt;'zeros'&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;padding_mode&lt;/strong&gt;（&lt;em&gt;字符串&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash; &lt;code&gt;'zeros'&lt;/code&gt; ， &lt;code&gt;'reflect'&lt;/code&gt; ， &lt;code&gt;'replicate'&lt;/code&gt; 或 &lt;code&gt;'circular'&lt;/code&gt; 。默认值： &lt;code&gt;'zeros'&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="1c8930299bc472b0bf01b43e4876b43bc57b0afd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding_value&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; value for padded elements. Default: 0.</source>
          <target state="translated">&lt;strong&gt;padding_value&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;填充元素的值。默认值：0</target>
        </trans-unit>
        <trans-unit id="e46a4080234de5b81dc28aa5cf6494e583b2dfc4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;padding_value&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; values for padded elements.</source>
          <target state="translated">&lt;strong&gt;padding_value&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;填充元素的值。</target>
        </trans-unit>
        <trans-unit id="1e944f55958d7c17e7297fead1ace3158d4a22b0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;param&lt;/strong&gt; &amp;ndash; optional parameter for the non-linear function</source>
          <target state="translated">&lt;strong&gt;参数&lt;/strong&gt;&amp;ndash;非线性函数的可选参数</target>
        </trans-unit>
        <trans-unit id="2a439f504eecae5ec1487c4ee52295b7169b16a5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;param&lt;/strong&gt; (&lt;a href=&quot;torch.nn.parameter.parameter#torch.nn.parameter.Parameter&quot;&gt;Parameter&lt;/a&gt;) &amp;ndash; parameter to be added to the module.</source>
          <target state="translated">&lt;strong&gt;param&lt;/strong&gt;（&lt;a href=&quot;torch.nn.parameter.parameter#torch.nn.parameter.Parameter&quot;&gt;Parameter&lt;/a&gt;）&amp;ndash;要添加到模块的参数。</target>
        </trans-unit>
        <trans-unit id="a6e15f4d36c2b934c8b9075fb6dc299572c2c953" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;param_group&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;) &amp;ndash; Specifies what Tensors should be optimized along with group</source>
          <target state="translated">&lt;strong&gt;param_group&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;）&amp;ndash;指定应与组一起优化哪些张量</target>
        </trans-unit>
        <trans-unit id="f75a00d4efb185ff63b6a63af312bfa294122453" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;parameter&lt;/strong&gt; (&lt;em&gt;nn.Parameter&lt;/em&gt;) &amp;ndash; parameter to append</source>
          <target state="translated">&lt;strong&gt;参数&lt;/strong&gt;（&lt;em&gt;nn.Parameter&lt;/em&gt;）&amp;ndash;要附加的参数</target>
        </trans-unit>
        <trans-unit id="489497c61c6c4ca41d36025fee2d4513e38423ff" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;parameters&lt;/strong&gt; (&lt;em&gt;Iterable of&lt;/em&gt;&lt;em&gt; (&lt;/em&gt;&lt;em&gt;module&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;name&lt;/em&gt;&lt;em&gt;) &lt;/em&gt;&lt;em&gt;tuples&lt;/em&gt;) &amp;ndash; parameters of the model to prune in a global fashion, i.e. by aggregating all weights prior to deciding which ones to prune. module must be of type &lt;code&gt;nn.Module&lt;/code&gt;, and name must be a string.</source>
          <target state="translated">&lt;strong&gt;参数&lt;/strong&gt;（&lt;em&gt;（&lt;/em&gt;&lt;em&gt;模块&lt;/em&gt;&lt;em&gt;（&lt;/em&gt;&lt;em&gt;名称，&lt;/em&gt;&lt;em&gt;元组&lt;/em&gt;）的可&lt;em&gt;迭代&lt;/em&gt;&lt;em&gt;）&lt;/em&gt;&lt;em&gt;元组&lt;/em&gt;）&amp;ndash;以全局方式修剪的模型参数，即在决定要修剪的权重之前，先汇总所有权重。模块的类型必须为 &lt;code&gt;nn.Module&lt;/code&gt; ，名称必须为字符串。&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="d320f6d02ed2f210bc2bb94d849c5a4a0333a948" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;parameters&lt;/strong&gt; (&lt;em&gt;Iterable&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;] or &lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; an iterable of Tensors or a single Tensor that will have gradients normalized</source>
          <target state="translated">&lt;strong&gt;参数&lt;/strong&gt;（&lt;em&gt;Iterable &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;]或&lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;张量或单个Tensor的可迭代量，将对梯度进行归一化</target>
        </trans-unit>
        <trans-unit id="088332766231b5fb1e917d8a22e9be51803d3339" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;parameters&lt;/strong&gt; (&lt;em&gt;Iterable&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; an iterator of Tensors that are the parameters of a model.</source>
          <target state="translated">&lt;strong&gt;参数&lt;/strong&gt;（&lt;em&gt;Iterable &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;作为模型参数的Tensors的迭代器。</target>
        </trans-unit>
        <trans-unit id="22d3c60c713a7260e180dbfeec107c5a3c8c8e37" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;parameters&lt;/strong&gt; (&lt;em&gt;iterable&lt;/em&gt;) &amp;ndash; a mapping (dictionary) from string to &lt;code&gt;Parameter&lt;/code&gt;, or an iterable of key-value pairs of type (string, &lt;code&gt;Parameter&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;参数&lt;/strong&gt;（&lt;em&gt;可迭代&lt;/em&gt;）&amp;ndash;从字符串到 &lt;code&gt;Parameter&lt;/code&gt; 的映射（字典），或类型（字符串， &lt;code&gt;Parameter&lt;/code&gt; ）的键值对的可迭代</target>
        </trans-unit>
        <trans-unit id="b029593b0723b3ed9d1778d6c527b9cc13f2e3dd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;parameters&lt;/strong&gt; (&lt;em&gt;iterable&lt;/em&gt;) &amp;ndash; iterable of parameters to append</source>
          <target state="translated">&lt;strong&gt;参数&lt;/strong&gt;（&lt;em&gt;可迭代&lt;/em&gt;）&amp;ndash;可追加的参数可迭代</target>
        </trans-unit>
        <trans-unit id="b756d03d964186ac93672f10364095cbcb790305" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;parameters&lt;/strong&gt; (&lt;em&gt;iterable&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a mapping (dictionary) of (string : &lt;code&gt;Parameter&lt;/code&gt;) or an iterable of key-value pairs of type (string, &lt;code&gt;Parameter&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;参数&lt;/strong&gt;（&lt;em&gt;iterable &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;（字符串： &lt;code&gt;Parameter&lt;/code&gt; ）的映射（字典）或类型为（字符串， &lt;code&gt;Parameter&lt;/code&gt; ）的键-值对的可迭代</target>
        </trans-unit>
        <trans-unit id="7a1a7adbc39a4daf48449acfbbb118da27774d19" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;parameters&lt;/strong&gt; (&lt;em&gt;iterable&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; an iterable of &lt;code&gt;Parameter&lt;/code&gt; to add</source>
          <target state="translated">&lt;strong&gt;参数&lt;/strong&gt;（&lt;em&gt;iterable &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;要添加的 &lt;code&gt;Parameter&lt;/code&gt; 的可迭代项</target>
        </trans-unit>
        <trans-unit id="115933d529771be89fc68336db02fcf00bb79520" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;params&lt;/strong&gt; (&lt;em&gt;iterable&lt;/em&gt;) &amp;ndash; an iterable of &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; s or &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/a&gt; s. Specifies what Tensors should be optimized.</source>
          <target state="translated">&lt;strong&gt;参数&lt;/strong&gt;（&lt;em&gt;可迭代&lt;/em&gt;）&amp;ndash;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;的可迭代。Tensor或&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt; &lt;code&gt;dict&lt;/code&gt; &lt;/a&gt;。指定应该优化哪些张量。</target>
        </trans-unit>
        <trans-unit id="01f57d38e895cf47e995315469bf1de4745eabc0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;params&lt;/strong&gt; (&lt;em&gt;iterable&lt;/em&gt;) &amp;ndash; iterable of parameters to optimize or dicts defining parameter groups</source>
          <target state="translated">&lt;strong&gt;参数&lt;/strong&gt;（&lt;em&gt;可迭代&lt;/em&gt;）&amp;ndash;参数的可迭代以优化或命令定义参数组</target>
        </trans-unit>
        <trans-unit id="88c32fc1811dc3014215ab77e8ab52e5f171a72c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;params_rref&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;RRef&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; list of RRefs to local or remote parameters to optimize.</source>
          <target state="translated">&lt;strong&gt;params_rref&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list &lt;/a&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;RRef &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;要优化的本地或远程参数的RRef列表。</target>
        </trans-unit>
        <trans-unit id="aa7a53279c1837507c2fe5c94e103220a1d49e82" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;parts&lt;/strong&gt; (list of &lt;a href=&quot;#torch.distributions.transforms.Transform&quot;&gt;&lt;code&gt;Transform&lt;/code&gt;&lt;/a&gt;) &amp;ndash; A list of transforms to compose.</source>
          <target state="translated">&lt;strong&gt;零件&lt;/strong&gt;（&amp;ldquo;&lt;a href=&quot;#torch.distributions.transforms.Transform&quot;&gt; &lt;code&gt;Transform&lt;/code&gt; &lt;/a&gt;&amp;rdquo;列表）&amp;ndash;要组成的变换列表。</target>
        </trans-unit>
        <trans-unit id="bed12dc0de75cc0eb2c41cb6816108cf95e6ea9a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;path&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; Path where the trace will be written.</source>
          <target state="translated">&lt;strong&gt;path&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;）&amp;ndash;跟踪将被写入的路径。</target>
        </trans-unit>
        <trans-unit id="f3bc0f1d455d09bb05991fbe5db3a4cb4d74742b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;path&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; path to nvprof trace</source>
          <target state="translated">&lt;strong&gt;path&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;）&amp;ndash; nvprof跟踪的路径</target>
        </trans-unit>
        <trans-unit id="05386d14aa6eff4c85da6a56408e676b2a826d6f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;patience&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Number of epochs with no improvement after which learning rate will be reduced. For example, if &lt;code&gt;patience = 2&lt;/code&gt;, then we will ignore the first 2 epochs with no improvement, and will only decrease the LR after the 3rd epoch if the loss still hasn&amp;rsquo;t improved then. Default: 10.</source>
          <target state="translated">&lt;strong&gt;耐心&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;没有改善的时期数，此后学习率将降低。例如，如果 &lt;code&gt;patience = 2&lt;/code&gt; ，那么我们将忽略前两个时期而没有任何改善，并且如果损失仍然没有改善，则只会在第三个时期之后降低LR。默认值：10</target>
        </trans-unit>
        <trans-unit id="acf92cc771b32560798f3f952f36649dd693efa9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pct_start&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; The percentage of the cycle (in number of steps) spent increasing the learning rate. Default: 0.3</source>
          <target state="translated">&lt;strong&gt;pct_start&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;花费的周期百分比（以步数为单位），以提高学习率。默认值：0.3</target>
        </trans-unit>
        <trans-unit id="c73980a2dbe97d6cc5f31cd9778490dcbce3fd01" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;per_sample_weights&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a tensor of float / double weights, or None to indicate all weights should be taken to be 1. If specified, &lt;code&gt;per_sample_weights&lt;/code&gt; must have exactly the same shape as input and is treated as having the same &lt;code&gt;offsets&lt;/code&gt;, if those are not None.</source>
          <target state="translated">&lt;strong&gt;per_sample_weights&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;浮点数/双权重的张量，或者无以指示所有权重应为1。如果指定， &lt;code&gt;per_sample_weights&lt;/code&gt; 必须具有与输入完全相同的形状，并且被视为具有相同的 &lt;code&gt;offsets&lt;/code&gt; ，如果那些不是没有。</target>
        </trans-unit>
        <trans-unit id="1e8c363177f0382a2ca52911b44cd26dbb02b01b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;periodic&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If True, returns a periodic window suitable for use in spectral analysis. If False, returns a symmetric window suitable for use in filter design.</source>
          <target state="translated">&lt;strong&gt;周期&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为True，则返回一个适用于光谱分析的周期窗口。如果为False，则返回一个适用于过滤器设计的对称窗口。</target>
        </trans-unit>
        <trans-unit id="9dc935030f617a99daedb64a9e2141255a78596a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;periodic&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If True, returns a window to be used as periodic function. If False, return a symmetric window.</source>
          <target state="translated">&lt;strong&gt;周期&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为True，则返回一个用作周期函数的窗口。如果为False，则返回一个对称窗口。</target>
        </trans-unit>
        <trans-unit id="10ac8ca2d66e3e1712e04c036b9bf70de5e5b9a7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;perserved_methods&lt;/strong&gt; &amp;ndash; A list of methods that needed to be preserved when freeze_module pass is invoked</source>
          <target state="translated">&lt;strong&gt;perserved_methods&lt;/strong&gt; &amp;ndash;调用freeze_module传递时需要保留的方法的列表</target>
        </trans-unit>
        <trans-unit id="27ea671e2a5ede173ebe44a6fbf6a5dfc8e07a55" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;persistent&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; whether the buffer is part of this module&amp;rsquo;s &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;strong&gt;持久&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;缓冲区是否是此模块的&lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;的一部分。</target>
        </trans-unit>
        <trans-unit id="00ab29ea4c85832585125194a8d3ce432d70b86b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;persistent&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; whether the buffer is part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;strong&gt;持久&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;缓冲区是否是此模块的&lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;的一部分。</target>
        </trans-unit>
        <trans-unit id="d9ed1cdbc115cc96411a0714a99ba4f42fb9ffaf" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;persistent&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; whether the buffer is part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;strong&gt;持久&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;缓冲区是否是此模块的&lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;的一部分。</target>
        </trans-unit>
        <trans-unit id="730eade83c492cb9c9d61b0d4e948adf920f85d5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;persistent&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; whether the buffer is part of this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;strong&gt;持久&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;缓冲区是否是此模块的&lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;的一部分。</target>
        </trans-unit>
        <trans-unit id="cb3bb81fbe6b31c4f4a56e96a20f9e5651de1195" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;persistent_workers&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers &lt;code&gt;Dataset&lt;/code&gt; instances alive. (default: &lt;code&gt;False&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;sistence_workers&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则数据集使用一次后，数据加载器将不会关闭工作进程。这样可以使Worker &lt;code&gt;Dataset&lt;/code&gt; 实例保持活动状态。（默认： &lt;code&gt;False&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="6616e16bfa20c63814b50ea16a8dbe20de09b074" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pickle_load_args&lt;/strong&gt; &amp;ndash; (Python 3 only) optional keyword arguments passed over to &lt;code&gt;pickle_module.load()&lt;/code&gt; and &lt;code&gt;pickle_module.Unpickler()&lt;/code&gt;, e.g., &lt;code&gt;errors=...&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;pickle_load_args&lt;/strong&gt; - （Python的3只）可选的关键字参数传递到 &lt;code&gt;pickle_module.load()&lt;/code&gt; 和 &lt;code&gt;pickle_module.Unpickler()&lt;/code&gt; ，例如， &lt;code&gt;errors=...&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="80a816c0ab01c996cd7ea1aa403c557f4fa9c91f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pickle_module&lt;/strong&gt; &amp;ndash; module used for pickling metadata and objects</source>
          <target state="translated">&lt;strong&gt;pickle_module&lt;/strong&gt; &amp;ndash;用于腌制元数据和对象的模块</target>
        </trans-unit>
        <trans-unit id="2f4bba92d9ab262c68a9aa63c263e3ecc1546639" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pickle_module&lt;/strong&gt; &amp;ndash; module used for unpickling metadata and objects (has to match the &lt;code&gt;pickle_module&lt;/code&gt; used to serialize file)</source>
          <target state="translated">&lt;strong&gt;pickle_module&lt;/strong&gt; &amp;ndash;用于&lt;strong&gt;解开&lt;/strong&gt;元数据和对象的模块（必须与用于序列化文件的 &lt;code&gt;pickle_module&lt;/code&gt; 匹配）</target>
        </trans-unit>
        <trans-unit id="d45a398e45519ff88900ce547f84f72e8159bc02" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pickle_protocol&lt;/strong&gt; &amp;ndash; can be specified to override the default protocol</source>
          <target state="translated">&lt;strong&gt;pickle_protocol&lt;/strong&gt; &amp;ndash;可以指定为覆盖默认协议</target>
        </trans-unit>
        <trans-unit id="93bc0635305d11284034a5a8601f085bc3597090" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pin_memory&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your &lt;code&gt;collate_fn&lt;/code&gt; returns a batch that is a custom type, see the example below.</source>
          <target state="translated">&lt;strong&gt;pin_memory&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则数据加载器在将张量返回之前将其复制到CUDA固定的内存中。如果您的数据元素是自定义类型，或者您的 &lt;code&gt;collate_fn&lt;/code&gt; 返回的是一个自定义类型的批处理，请参见下面的示例。</target>
        </trans-unit>
        <trans-unit id="158a245072a4f4a11829d7283abb48b4e1e9eb19" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pin_memory&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;pin_memory&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果设置，返回的张量将分配在固定的内存中。仅适用于CPU张量。默认值： &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="30a109270dbf3f6ec8bac6d1654792ed9100f8cb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pivot&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; controls whether pivoting is done. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;枢轴&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;控制是否完成枢轴。默认值： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="cd32ee50a83eb126fa1a1c22993913d7830c61d9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pivots&lt;/strong&gt; (&lt;em&gt;IntTensor&lt;/em&gt;): the pivots of size</source>
          <target state="translated">&lt;strong&gt;枢轴&lt;/strong&gt;（&lt;em&gt;IntTensor&lt;/em&gt;）：大小的枢轴</target>
        </trans-unit>
        <trans-unit id="f2e2c490dcdc08b15ee53f75826edaf1f53d7fd1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;port&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The port on which the server store should listen for incoming requests.</source>
          <target state="translated">&lt;strong&gt;port&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;服务器存储应在其上侦听传入请求的端口。</target>
        </trans-unit>
        <trans-unit id="a7e3a169027105e593b1192ecd8a9853f1788650" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pos_weight&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a weight of positive examples. Must be a vector with length equal to the number of classes.</source>
          <target state="translated">&lt;strong&gt;pos_weight&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;正例的权重。必须是长度等于类数的向量。</target>
        </trans-unit>
        <trans-unit id="5d9e32c819f28a6d6eb1c4b5e3bf66285ee0131e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pos_weight&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a weight of positive examples. Must be a vector with length equal to the number of classes.</source>
          <target state="translated">&lt;strong&gt;pos_weight&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;正例的权重。必须是长度等于类数的向量。</target>
        </trans-unit>
        <trans-unit id="63a4d9260aa840607a810cbe8d0437c578111576" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;precision&lt;/strong&gt; &amp;ndash; Number of digits of precision for floating point output (default = 4).</source>
          <target state="translated">&lt;strong&gt;precision&lt;/strong&gt; &amp;ndash;浮点输出的精度位数（默认= 4）。</target>
        </trans-unit>
        <trans-unit id="b7887c4f9ed7cd09ae84c628834259b5f7580508" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;precision_matrix&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; positive-definite precision matrix</source>
          <target state="translated">&lt;strong&gt;precision_matrix&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;正定精度矩阵</target>
        </trans-unit>
        <trans-unit id="abaf92d6b3ab9531ecb8cfe273227a30db2aba37" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;predictions&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;numpy.array&lt;/em&gt;&lt;em&gt;, or &lt;/em&gt;&lt;em&gt;string/blobname&lt;/em&gt;) &amp;ndash; The probability that an element be classified as true. Value should in [0, 1]</source>
          <target state="translated">&lt;strong&gt;预测&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;numpy.array&lt;/em&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;string / blobname&lt;/em&gt;）&amp;ndash;元素被归类为true的概率。值应为[0，1]</target>
        </trans-unit>
        <trans-unit id="6fb3354ed439843dd8ef32205babe7ce9688e7c7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;prefetch_factor&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;keyword-only arg&lt;/em&gt;) &amp;ndash; Number of sample loaded in advance by each worker. &lt;code&gt;2&lt;/code&gt; means there will be a total of 2 * num_workers samples prefetched across all workers. (default: &lt;code&gt;2&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;prefetch_factor&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;仅关键字arg&lt;/em&gt;）&amp;ndash;每个工作人员预先加载的样本数。 &lt;code&gt;2&lt;/code&gt; 表示将在所有工作人员中预取总共2 * num_workers个样本。（默认： &lt;code&gt;2&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="739bd144f8a0f654bdebf94781331327df2092bf" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;prefix&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; The prefix string that is prepended to each key before being inserted into the store.</source>
          <target state="translated">&lt;strong&gt;prefix&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;）&amp;ndash;在插入商店之前，每个键之前的前缀字符串。</target>
        </trans-unit>
        <trans-unit id="fa8d87d0a691a6a353587f6a788059707272207e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;prefix&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; prefix to prepend to all buffer names.</source>
          <target state="translated">&lt;strong&gt;prefix&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;）&amp;ndash;前缀为所有缓冲区名称的前缀。</target>
        </trans-unit>
        <trans-unit id="a986463f78b9961525aa208023a9f037690aad13" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;prefix&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; prefix to prepend to all parameter names.</source>
          <target state="translated">&lt;strong&gt;prefix&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;）&amp;ndash;前缀为所有参数名称的前缀。</target>
        </trans-unit>
        <trans-unit id="9ada4cefd9a3c036b0c581d2f7f607e59a4d3f95" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;prehook&lt;/strong&gt; &amp;ndash; observer we want to add to forward_pre_hook</source>
          <target state="translated">&lt;strong&gt;prehook&lt;/strong&gt; &amp;ndash;我们要添加到forward_pre_hook的观察者</target>
        </trans-unit>
        <trans-unit id="87f996b18f2064a1da55736eae49c39b0a0515d6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;preserve_rng_state&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default=True&lt;/em&gt;) &amp;ndash; Omit stashing and restoring the RNG state during each checkpoint.</source>
          <target state="translated">&lt;strong&gt;prepare_rng_state&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;default = True&lt;/em&gt;）&amp;ndash;省略在每个检查点期间的隐藏和恢复RNG状态。</target>
        </trans-unit>
        <trans-unit id="7c2241fe546f8b691ed0b5ac683a6b12c7593208" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pretrained&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If True, returns a model pre-trained on COCO train2017</source>
          <target state="translated">&lt;strong&gt;pretrained&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;如果为True，则返回在COCO train2017上进行预训练的模型</target>
        </trans-unit>
        <trans-unit id="4130c1fbf426acf115174024390b36c99e17bb23" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pretrained&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If True, returns a model pre-trained on COCO train2017 which contains the same classes as Pascal VOC</source>
          <target state="translated">&lt;strong&gt;pretrained&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;如果为True，则返回在COCO train2017上进行预训练的模型，该模型包含与Pascal VOC相同的类</target>
        </trans-unit>
        <trans-unit id="3368d37398eeaa3adc0a4b2005b7ee2eb4eae69a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pretrained&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If True, returns a model pre-trained on ImageNet</source>
          <target state="translated">&lt;strong&gt;预训练&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;布尔&lt;/a&gt;） -如果为true，返回预先训练上ImageNet模型</target>
        </trans-unit>
        <trans-unit id="f82401ac0221d2cb1ccfd88239494a6d6471b218" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pretrained&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If True, returns a model pre-trained on Kinetics-400</source>
          <target state="translated">&lt;strong&gt;预训练&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;布尔&lt;/a&gt;） -如果为true，返回上预先训练动力学-400模型</target>
        </trans-unit>
        <trans-unit id="7c650eeb51be2510d223d56911e3f6d5151fafd9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pretrained_backbone&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If True, returns a model with backbone pre-trained on Imagenet</source>
          <target state="translated">&lt;strong&gt;pretrained_backbone&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;如果为True，则返回一个在Imagenet上经过预训练的骨干模型</target>
        </trans-unit>
        <trans-unit id="a012d454021e986a52416ed8bd22349eed031564" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;priority&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; priority of the stream. Can be either -1 (high priority) or 0 (low priority). By default, streams have priority 0.</source>
          <target state="translated">&lt;strong&gt;优先级&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;流的优先级。可以是-1（高优先级）或0（低优先级）。默认情况下，流的优先级为0。</target>
        </trans-unit>
        <trans-unit id="836a0ae5aaa2a4b5ebe789fe8a6b8eba4a2503f5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;probs&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Event probabilities</source>
          <target state="translated">&lt;strong&gt;probs&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;） -事件的概率</target>
        </trans-unit>
        <trans-unit id="9a547f3ca966c92d1b1c810aec9fbec6b5bae1da" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;probs&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Event probabilities of success in the half open interval [0, 1)</source>
          <target state="translated">&lt;strong&gt;probs&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;） -在半开区间成功的事件概率[0，1）</target>
        </trans-unit>
        <trans-unit id="57d78570b63b97e29d6e38d0d65aae44fc55c4ef" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;probs&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; event probabilities</source>
          <target state="translated">&lt;strong&gt;probs&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;） -事件的概率</target>
        </trans-unit>
        <trans-unit id="0ac25c99cbf653bc1bdacc6809f8131f7843f062" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;probs&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; (0,1) valued parameters</source>
          <target state="translated">&lt;strong&gt;概率&lt;/strong&gt;（&lt;em&gt;数字&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;（0,1）值参数</target>
        </trans-unit>
        <trans-unit id="c1fdcacad1373d6a9e0e8771ad4a3e2671dab2a3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;probs&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the probability of sampling &lt;code&gt;1&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;probs&lt;/strong&gt;（&lt;em&gt;号码&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;） -抽样的概率 &lt;code&gt;1&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="63137d647ab82091cd7358c87c0ba51703ae7f65" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;probs&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the probability of sampling &lt;code&gt;1&lt;/code&gt;. Must be in range (0, 1]</source>
          <target state="translated">&lt;strong&gt;probs&lt;/strong&gt;（&lt;em&gt;号码&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;） -抽样的概率 &lt;code&gt;1&lt;/code&gt; 。必须在范围内（0，1]</target>
        </trans-unit>
        <trans-unit id="c9bb8392d89004ba45474c8414a4e0ac5c0dec7d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;process_group&lt;/strong&gt; &amp;ndash; The process group to be used for distributed data all-reduction. If &lt;code&gt;None&lt;/code&gt;, the default process group, which is created by &lt;a href=&quot;../distributed#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt;, will be used. (default: &lt;code&gt;None&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;process_group&lt;/strong&gt; &amp;ndash;用于减少所有分布式数据的进程组。如果为 &lt;code&gt;None&lt;/code&gt; ，则将使用由&lt;a href=&quot;../distributed#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt; &lt;/a&gt;创建的默认进程组。（默认值： &lt;code&gt;None&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="ada49fdca36ec13c35249cb76cecbfce751dce94" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;process_group&lt;/strong&gt; &amp;ndash; synchronization of stats happen within each process group individually. Default behavior is synchronization across the whole world</source>
          <target state="translated">&lt;strong&gt;process_group&lt;/strong&gt; &amp;ndash;统计信息的同步分别在每个进程组内发生。默认行为是在整个世界范围内同步</target>
        </trans-unit>
        <trans-unit id="ab933fbb9c2c9f17574f6d3222ab8887ecfcdd68" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;process_group&lt;/strong&gt; (&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; process group to scope synchronization, default is the whole world</source>
          <target state="translated">&lt;strong&gt;process_group&lt;/strong&gt;（&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;进程组到范围的同步，默认为整个世界</target>
        </trans-unit>
        <trans-unit id="63cde1066d72754dbf25cd05b0ce992cd2c0bf1a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;profile&lt;/strong&gt; &amp;ndash; Sane defaults for pretty printing. Can override with any of the above options. (any one of &lt;code&gt;default&lt;/code&gt;, &lt;code&gt;short&lt;/code&gt;, &lt;code&gt;full&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;配置文件&lt;/strong&gt;&amp;ndash; Sane默认使用漂亮的打印。可以使用以上任何选项覆盖。（ &lt;code&gt;default&lt;/code&gt; ， &lt;code&gt;short&lt;/code&gt; ， &lt;code&gt;full&lt;/code&gt; 之一）</target>
        </trans-unit>
        <trans-unit id="4cce827d14fdd2e9caa71382821e2ec163703757" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;profile_memory&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Whether to report memory usage, default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;profile_memory&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;是否报告内存使用情况，默认值： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="4b15a59aebfcfe93a0fb0b923c3d5e9329cf089d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;progress&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If True, displays a progress bar of the download to stderr</source>
          <target state="translated">&lt;strong&gt;progress&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;如果为True，则显示下载到stderr的进度条</target>
        </trans-unit>
        <trans-unit id="fb2a87f24d269533a9f43524829d450e3c20d9a7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;progress&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether or not to display a progress bar to stderr Default: True</source>
          <target state="translated">&lt;strong&gt;progress&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;是否显示stderr的进度条默认值：True</target>
        </trans-unit>
        <trans-unit id="1f4a965676c83faefeff0a640a396640b206ce90" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;progress&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether or not to display a progress bar to stderr. Default: True</source>
          <target state="translated">&lt;strong&gt;progress&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;是否显示stderr的进度条。默认值：True</target>
        </trans-unit>
        <trans-unit id="904f28013662ab6c545488f5c421363d39caa951" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;pruning_method&lt;/strong&gt; (&lt;em&gt;function&lt;/em&gt;) &amp;ndash; a valid pruning function from this module, or a custom one implemented by the user that satisfies the implementation guidelines and has &lt;code&gt;PRUNING_TYPE='unstructured'&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;pruning_method&lt;/strong&gt;（&lt;em&gt;函数&lt;/em&gt;）&amp;ndash;该模块中的有效修剪函数，或者是由用户实施的，满足实施准则并且具有 &lt;code&gt;PRUNING_TYPE='unstructured'&lt;/code&gt; 的自定义函数。</target>
        </trans-unit>
        <trans-unit id="e2d5c802a24580a7de621d4fd574838b9fee79a5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;purge_step&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; When logging crashes at step</source>
          <target state="translated">&lt;strong&gt;purge_step&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;当日志记录在步骤崩溃时</target>
        </trans-unit>
        <trans-unit id="2c77c29b32dbddf5df2161d33068a2c919a7f28a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;q&lt;/strong&gt; (&lt;a href=&quot;#torch.distributions.distribution.Distribution&quot;&gt;Distribution&lt;/a&gt;) &amp;ndash; A &lt;code&gt;Distribution&lt;/code&gt; object.</source>
          <target state="translated">&lt;strong&gt;q&lt;/strong&gt;（&lt;a href=&quot;#torch.distributions.distribution.Distribution&quot;&gt;Distribution&lt;/a&gt;）&amp;ndash; &lt;code&gt;Distribution&lt;/code&gt; 对象。</target>
        </trans-unit>
        <trans-unit id="d1cf443a5e0dc3febcb5f16e5dcd17ebe5190074" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;q&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; a scalar or 1D tensor of quantile values in the range [0, 1]</source>
          <target state="translated">&lt;strong&gt;q&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;分位数的标量或一维张量，范围为[0，1]</target>
        </trans-unit>
        <trans-unit id="a79e6547c6d8a86b6b38364e5726183256af0cff" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;q&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a slightly overestimated rank of</source>
          <target state="translated">&lt;strong&gt;q&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;的等级被高估了</target>
        </trans-unit>
        <trans-unit id="19980c41df6565f1cebbe94e3136eaac2d85dd26" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;qconfig&lt;/strong&gt; &amp;ndash; quantization configuration for the tensor, if qconfig is not provided, we will get qconfig from parent modules</source>
          <target state="translated">&lt;strong&gt;qconfig&lt;/strong&gt; &amp;ndash;张量的量化配置，如果未提供qconfig，我们将从父模块获取qconfig</target>
        </trans-unit>
        <trans-unit id="2d06744fd3fe91170a1358d5cbdbabaeaa95f37e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;qconfig_dict&lt;/strong&gt; &amp;ndash; dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)</source>
          <target state="translated">&lt;strong&gt;qconfig_dict&lt;/strong&gt; &amp;ndash;从子模块的名称或类型映射到量化配置的字典，除非指定了子模块的qconfig（当子模块已经具有qconfig属性时），qconfig适用于给定模块的所有子模块</target>
        </trans-unit>
        <trans-unit id="d8ae7bd422f6a67dd76f0faf46879a1ac2e30be1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;qconfig_spec&lt;/strong&gt; &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;qconfig_spec&lt;/strong&gt; &amp;ndash;</target>
        </trans-unit>
        <trans-unit id="513991e27786f7e8f557b7f26358994ddf0d7171" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;qscheme&lt;/strong&gt; &amp;ndash; Quantization scheme to be used</source>
          <target state="translated">&lt;strong&gt;qscheme&lt;/strong&gt; &amp;ndash;要使用的量化方案</target>
        </trans-unit>
        <trans-unit id="a0b208a2eac673c5afa2a571d2f7b43dbc1b7802" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;quant_max&lt;/strong&gt; &amp;ndash; Maximum quantization value. If unspecified, it will follow the 8-bit setup.</source>
          <target state="translated">&lt;strong&gt;quant_max&lt;/strong&gt; &amp;ndash;最大量化值。如果未指定，它将遵循8位设置。</target>
        </trans-unit>
        <trans-unit id="13aeba16f14baca22ce7389ada7786093da7b81d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;quant_max&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The maximum allowable quantized value.</source>
          <target state="translated">&lt;strong&gt;Quantum_max&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;允许的最大量化值。</target>
        </trans-unit>
        <trans-unit id="53b5cb54c65a69d34cae88c78e6cf1d327781d3a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;quant_min&lt;/strong&gt; &amp;ndash; Minimum quantization value. If unspecified, it will follow the 8-bit setup.</source>
          <target state="translated">&lt;strong&gt;quant_min&lt;/strong&gt; &amp;ndash;最小量化值。如果未指定，它将遵循8位设置。</target>
        </trans-unit>
        <trans-unit id="4457923d474c9d812a47035e795d3a26ec0aede5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;quant_min&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The minimum allowable quantized value.</source>
          <target state="translated">&lt;strong&gt;quant_min&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;最小允许量化值。</target>
        </trans-unit>
        <trans-unit id="2b03c96abe488a3ad5cb0d36a695b5ff1baba3a5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;r&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; number of elements to combine</source>
          <target state="translated">&lt;strong&gt;r&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;要组合的元素数</target>
        </trans-unit>
        <trans-unit id="8b6f1d94cb019e9c8d76124f3064ce78117f6ad1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;raise_exception&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks.</source>
          <target state="translated">&lt;strong&gt;raise_exception&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;指示如果检查失败，是否引发异常。该异常提供有关故障确切性质的更多信息。这在调试gradcheck时很有用。</target>
        </trans-unit>
        <trans-unit id="46b43eec40a9385ec340e004393520434a0a5eff" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;rank&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; a globally unique id/rank of this node.</source>
          <target state="translated">&lt;strong&gt;rank&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;此节点的全局唯一ID /等级。</target>
        </trans-unit>
        <trans-unit id="1b9f3f2fe3816f64c8a2d72a1f782fb24e29f8c2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;rank&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Rank of the current process within &lt;code&gt;num_replicas&lt;/code&gt;. By default, &lt;code&gt;rank&lt;/code&gt; is retrieved from the current distributed group.</source>
          <target state="translated">&lt;strong&gt;rank&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;整数&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;当前进程在 &lt;code&gt;num_replicas&lt;/code&gt; 中的等级。默认情况下，从当前分布式组中检索 &lt;code&gt;rank&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="213d6776c2e063ecf8780712ac1852b9cd773740" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;rank&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Rank of the current process. Required if &lt;code&gt;store&lt;/code&gt; is specified.</source>
          <target state="translated">&lt;strong&gt;rank&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;当前进程的等级。如果指定了 &lt;code&gt;store&lt;/code&gt; 则为必需。</target>
        </trans-unit>
        <trans-unit id="eded7c6be7a599ab9e04149f80d8815426799286" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;ranks&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; List of ranks of group members. If &lt;code&gt;None&lt;/code&gt;, will be set to all ranks. Default is &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;ranks&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list &lt;/a&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;组成员的等级列表。如果为 &lt;code&gt;None&lt;/code&gt; ，将设置为所有等级。默认值为 &lt;code&gt;None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="8172a5e10a4de560b57298957c2be9e6c1d992c9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;rate&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; rate = 1 / scale of the distribution</source>
          <target state="translated">&lt;strong&gt;比率&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;浮动&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;比率= 1 /分配比例</target>
        </trans-unit>
        <trans-unit id="eb69dd0379b97bdbdf3170bc41f3f933f59297ce" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;rate&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; rate = 1 / scale of the distribution (often referred to as beta)</source>
          <target state="translated">&lt;strong&gt;费率&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;浮动&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;费率= 1 /分配比例（通常称为beta）</target>
        </trans-unit>
        <trans-unit id="c13c2129dc5528a3975452686d875d982aa4e939" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;rate&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the rate parameter</source>
          <target state="translated">&lt;strong&gt;rate&lt;/strong&gt;（&lt;em&gt;Number &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;速率参数</target>
        </trans-unit>
        <trans-unit id="75f1a5c80836b92712f937ef8445c423fd9ea3f0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;rcond&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; A floating point value to determine the cutoff for small singular values. Default: 1e-15</source>
          <target state="translated">&lt;strong&gt;rcond&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;一个浮点值，用于确定小的奇异值的临界值。默认值：1e-15</target>
        </trans-unit>
        <trans-unit id="af581fa7827e69e59f0e4da3fc56082b354bc569" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;real&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The real part of the complex tensor. Must be float or double.</source>
          <target state="translated">&lt;strong&gt;实数&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;复数张量的实数部分。必须为float或double。</target>
        </trans-unit>
        <trans-unit id="d062ec2db1e149329c436d3f85532b1325d11a04" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;recompute_scale_factor&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; recompute the scale_factor for use in the interpolation calculation. When &lt;code&gt;scale_factor&lt;/code&gt; is passed as a parameter, it is used to compute the &lt;code&gt;output_size&lt;/code&gt;. If &lt;code&gt;recompute_scale_factor&lt;/code&gt; is &lt;code&gt;`False&lt;/code&gt; or not specified, the passed-in &lt;code&gt;scale_factor&lt;/code&gt; will be used in the interpolation computation. Otherwise, a new &lt;code&gt;scale_factor&lt;/code&gt; will be computed based on the output and input sizes for use in the interpolation computation (i.e. the computation will be identical to if the computed &lt;code&gt;output_size&lt;/code&gt; were passed-in explicitly). Note that when &lt;code&gt;scale_factor&lt;/code&gt; is floating-point, the recomputed scale_factor may differ from the one passed in due to rounding and precision issues.</source>
          <target state="translated">&lt;strong&gt;recompute_scale_factor&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;重新计算在插值计算中使用的scale_factor。当 &lt;code&gt;scale_factor&lt;/code&gt; 作为参数传递时，将用于计算 &lt;code&gt;output_size&lt;/code&gt; 。如果 &lt;code&gt;recompute_scale_factor&lt;/code&gt; 为 &lt;code&gt;`False&lt;/code&gt; 或未指定，则传入的 &lt;code&gt;scale_factor&lt;/code&gt; 将用于插值计算。否则，将基于插值计算中使用的输出和输入大小来计算新的 &lt;code&gt;scale_factor&lt;/code&gt; （即，计算将与是否显式传入所计算的 &lt;code&gt;output_size&lt;/code&gt; 相同）。注意，当 &lt;code&gt;scale_factor&lt;/code&gt; 由于是浮点数，由于四舍五入和精度问题，重新计算的scale_factor可能与传递的scale_factor不同。</target>
        </trans-unit>
        <trans-unit id="6d147c57163aacc18c731dbb3194a3e546b62f51" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;record_shapes&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If shapes recording is set, information about input dimensions will be collected. This allows one to see which dimensions have been used under the hood and further group by them using prof.key_averages(group_by_input_shape=True). Please note that shape recording might skew your profiling data. It is recommended to use separate runs with and without shape recording to validate the timing. Most likely the skew will be negligible for bottom most events (in a case of nested function calls). But for higher level functions the total self cpu time might be artificially increased because of the shape collection.</source>
          <target state="translated">&lt;strong&gt;record_shapes&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果设置了形状记录，将收集有关输入尺寸的信息。这样一来，您可以查看引擎盖下使用了哪些尺寸，并使用prof.key_averages（group_by_input_shape = True）将它们进一步分组。请注意，形状记录可能会歪曲您的轮廓数据。建议使用带有和不带有形状记录的单独运行来验证计时。对于最底层的事件（在嵌套函数调用的情况下），偏斜很可能会忽略不计。但是对于较高级别的功能，由于形状收集，可能会人为地增加总的自体cpu时间。</target>
        </trans-unit>
        <trans-unit id="b9bb0a7673177fd2fbc7643ab228a01c936fa373" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;record_shapes&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default=False&lt;/em&gt;) &amp;ndash; If &lt;code&gt;record_shapes=True&lt;/code&gt;, the nvtx range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: &lt;code&gt;[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]&lt;/code&gt; Non-tensor arguments will be represented by &lt;code&gt;[]&lt;/code&gt;. Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of nvtx range creation.</source>
          <target state="translated">&lt;strong&gt;record_shapes&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;default = False&lt;/em&gt;）&amp;ndash;如果 &lt;code&gt;record_shapes=True&lt;/code&gt; ，则包装每个autograd op的nvtx范围将以以下格式附加有关该op接收到的Tensor参数的大小的信息： &lt;code&gt;[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]&lt;/code&gt; 非张量参数将由 &lt;code&gt;[]&lt;/code&gt; 表示。参数将按照后端操作接收到的顺序列出。请注意，此顺序可能与在Python端传递这些参数的顺序不匹配。还要注意，形状记录可能会增加nvtx范围创建的开销。</target>
        </trans-unit>
        <trans-unit id="66b7e6f37f61ed7bcaf736cc00b93f71e3e9d94f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;recurse&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</source>
          <target state="translated">&lt;strong&gt;recurse&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;如果为True，则产生此模块和所有子模块的缓冲区。否则，仅产生作为该模块直接成员的缓冲区。</target>
        </trans-unit>
        <trans-unit id="644d5b1b0bd86304bca7dd80429d10befe174346" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;recurse&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</source>
          <target state="translated">&lt;strong&gt;recurse&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;如果为True，则产生该模块和所有子模块的参数。否则，仅产生作为该模块直接成员的参数。</target>
        </trans-unit>
        <trans-unit id="9d6ed3cdc238c90248d062e02e2aeb40a792fd6a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;reduce&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Deprecated (see &lt;code&gt;reduction&lt;/code&gt;). By default, the losses are averaged or summed over observations for each minibatch depending on &lt;code&gt;size_average&lt;/code&gt;. When &lt;code&gt;reduce&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, returns a loss per batch element instead and ignores &lt;code&gt;size_average&lt;/code&gt;. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;reduce&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;不推荐使用（请参见 &lt;code&gt;reduction&lt;/code&gt; ）。默认情况下，根据 &lt;code&gt;size_average&lt;/code&gt; ,对每个小批量的观测值平均损失或求和。当 &lt;code&gt;reduce&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; 时，将返回每批元素损失，并忽略 &lt;code&gt;size_average&lt;/code&gt; 。默认值： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="ec48eb52fa8aaaf96330bc31ea80796de3009600" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;reduce&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; reduction operation to apply, can be either &amp;lsquo;add&amp;rsquo; or &amp;lsquo;multiply&amp;rsquo;.</source>
          <target state="translated">&lt;strong&gt;reduce&lt;/strong&gt;（&lt;em&gt;string&lt;/em&gt;）&amp;ndash;应用的归约运算，可以是&amp;ldquo; add&amp;rdquo;或&amp;ldquo; multiply&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="df9ff944cb4c0142267f05a92f7850f1e92dc9ac" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;reduce_range&lt;/strong&gt; &amp;ndash; Reduces the range of the quantized data type by 1 bit</source>
          <target state="translated">&lt;strong&gt;reduce_range&lt;/strong&gt; &amp;ndash;将量化数据类型的范围缩小1位</target>
        </trans-unit>
        <trans-unit id="999d2f01d581c18979dd9cf3c2b7eed8f5feaf65" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;reduction&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Specifies the (optional) reduction to apply to the output: &lt;code&gt;'none'&lt;/code&gt; | &lt;code&gt;'mean'&lt;/code&gt; | &lt;code&gt;'sum'&lt;/code&gt;. &lt;code&gt;'none'&lt;/code&gt;: no reduction will be applied, &lt;code&gt;'mean'&lt;/code&gt;: the sum of the output will be divided by the number of elements in the output, &lt;code&gt;'sum'&lt;/code&gt;: the output will be summed. Default: &lt;code&gt;'mean'&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;减少&lt;/strong&gt;（&lt;em&gt;字符串&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;指定要应用于输出的（可选）减少： &lt;code&gt;'none'&lt;/code&gt; | &lt;code&gt;'mean'&lt;/code&gt; | &lt;code&gt;'sum'&lt;/code&gt; 。 &lt;code&gt;'none'&lt;/code&gt; ：不应用任何减少， &lt;code&gt;'mean'&lt;/code&gt; ：输出的总和除以输出中元素的数量， &lt;code&gt;'sum'&lt;/code&gt; ：输出的总和。默认值： &lt;code&gt;'mean'&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="be1520205497ebdaeafa4280ed8c7e38f481ddbb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;reduction&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Specifies the reduction to apply to the output: &lt;code&gt;'none'&lt;/code&gt; | &lt;code&gt;'batchmean'&lt;/code&gt; | &lt;code&gt;'sum'&lt;/code&gt; | &lt;code&gt;'mean'&lt;/code&gt;. &lt;code&gt;'none'&lt;/code&gt;: no reduction will be applied &lt;code&gt;'batchmean'&lt;/code&gt;: the sum of the output will be divided by the batchsize &lt;code&gt;'sum'&lt;/code&gt;: the output will be summed &lt;code&gt;'mean'&lt;/code&gt;: the output will be divided by the number of elements in the output Default: &lt;code&gt;'mean'&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;reduction&lt;/strong&gt;（&lt;em&gt;字符串&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;指定要应用于输出的缩减： &lt;code&gt;'none'&lt;/code&gt; | &lt;code&gt;'batchmean'&lt;/code&gt; | &lt;code&gt;'sum'&lt;/code&gt; | &lt;code&gt;'mean'&lt;/code&gt; 。 &lt;code&gt;'none'&lt;/code&gt; ：不应用减少 &lt;code&gt;'batchmean'&lt;/code&gt; ：输出的总和除以批处理大小 &lt;code&gt;'sum'&lt;/code&gt; ：输出的总和 &lt;code&gt;'mean'&lt;/code&gt; ：输出将除以输出中元素的数量默认值： &lt;code&gt;'mean'&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c4b4f64c1d6c7c3276f90a766a11f80a1ea5e149" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;reduction&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Specifies the reduction to apply to the output: &lt;code&gt;'none'&lt;/code&gt; | &lt;code&gt;'batchmean'&lt;/code&gt; | &lt;code&gt;'sum'&lt;/code&gt; | &lt;code&gt;'mean'&lt;/code&gt;. &lt;code&gt;'none'&lt;/code&gt;: no reduction will be applied. &lt;code&gt;'batchmean'&lt;/code&gt;: the sum of the output will be divided by batchsize. &lt;code&gt;'sum'&lt;/code&gt;: the output will be summed. &lt;code&gt;'mean'&lt;/code&gt;: the output will be divided by the number of elements in the output. Default: &lt;code&gt;'mean'&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;reduction&lt;/strong&gt;（&lt;em&gt;字符串&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;指定要应用于输出的缩减： &lt;code&gt;'none'&lt;/code&gt; | &lt;code&gt;'batchmean'&lt;/code&gt; | &lt;code&gt;'sum'&lt;/code&gt; | &lt;code&gt;'mean'&lt;/code&gt; 。 &lt;code&gt;'none'&lt;/code&gt; ：不减少任何费用。 &lt;code&gt;'batchmean'&lt;/code&gt; ：输出的总和将除以batchsize。 &lt;code&gt;'sum'&lt;/code&gt; ：将对输出求和。 &lt;code&gt;'mean'&lt;/code&gt; ：输出将除以输出中元素的数量。默认值： &lt;code&gt;'mean'&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="550432cc430b06db5fb2ea76d0d512f8f5fde312" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;reduction&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Specifies the reduction to apply to the output: &lt;code&gt;'none'&lt;/code&gt; | &lt;code&gt;'mean'&lt;/code&gt; | &lt;code&gt;'sum'&lt;/code&gt;. &lt;code&gt;'none'&lt;/code&gt;: no reduction will be applied, &lt;code&gt;'mean'&lt;/code&gt;: the output losses will be divided by the target lengths and then the mean over the batch is taken, &lt;code&gt;'sum'&lt;/code&gt;: the output will be summed. Default: &lt;code&gt;'mean'&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;reduction&lt;/strong&gt;（&lt;em&gt;字符串&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;指定要应用于输出的缩减： &lt;code&gt;'none'&lt;/code&gt; | &lt;code&gt;'mean'&lt;/code&gt; | &lt;code&gt;'sum'&lt;/code&gt; 。 &lt;code&gt;'none'&lt;/code&gt; ：不减少任何值， &lt;code&gt;'mean'&lt;/code&gt; ：将输出损失除以目标长度，然后取该批次的平均值， &lt;code&gt;'sum'&lt;/code&gt; ：将输出相加。默认值： &lt;code&gt;'mean'&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="fab0f58a76f5ab9a119f22747e3687485076ce27" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;reduction&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Specifies the reduction to apply to the output: &lt;code&gt;'none'&lt;/code&gt; | &lt;code&gt;'mean'&lt;/code&gt; | &lt;code&gt;'sum'&lt;/code&gt;. &lt;code&gt;'none'&lt;/code&gt;: no reduction will be applied, &lt;code&gt;'mean'&lt;/code&gt;: the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: &lt;code&gt;'mean'&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;reduction&lt;/strong&gt;（&lt;em&gt;字符串&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;指定要应用于输出的缩减： &lt;code&gt;'none'&lt;/code&gt; | &lt;code&gt;'mean'&lt;/code&gt; | &lt;code&gt;'sum'&lt;/code&gt; 。 &lt;code&gt;'none'&lt;/code&gt; ：不减少， &lt;code&gt;'mean'&lt;/code&gt; ：将输出损失除以目标长度，然后取该批次的平均值。默认值： &lt;code&gt;'mean'&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="2038b00821d8c29dc11b468c90bec48a089c2703" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;reduction&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Specifies the reduction to apply to the output: &lt;code&gt;'none'&lt;/code&gt; | &lt;code&gt;'mean'&lt;/code&gt; | &lt;code&gt;'sum'&lt;/code&gt;. &lt;code&gt;'none'&lt;/code&gt;: no reduction will be applied, &lt;code&gt;'mean'&lt;/code&gt;: the sum of the output will be divided by the number of elements in the output, &lt;code&gt;'sum'&lt;/code&gt;: the output will be summed. Note: &lt;code&gt;size_average&lt;/code&gt; and &lt;code&gt;reduce&lt;/code&gt; are in the process of being deprecated, and in the meantime, specifying either of those two args will override &lt;code&gt;reduction&lt;/code&gt;. Default: &lt;code&gt;'mean'&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;reduction&lt;/strong&gt;（&lt;em&gt;字符串&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;指定要应用于输出的缩减： &lt;code&gt;'none'&lt;/code&gt; | &lt;code&gt;'mean'&lt;/code&gt; | &lt;code&gt;'sum'&lt;/code&gt; 。 &lt;code&gt;'none'&lt;/code&gt; ：不应用任何减少， &lt;code&gt;'mean'&lt;/code&gt; ：输出的总和除以输出中元素的数量， &lt;code&gt;'sum'&lt;/code&gt; ：输出的总和。注： &lt;code&gt;size_average&lt;/code&gt; 和 &lt;code&gt;reduce&lt;/code&gt; 处于被淘汰，而在此期间，指定是这两个参数的个数将覆盖 &lt;code&gt;reduction&lt;/code&gt; 。默认值： &lt;code&gt;'mean'&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="eeb5334dd27eff929df501296bde2d45292263ba" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;reduction&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Specifies the reduction to apply to the output: &lt;code&gt;'none'&lt;/code&gt; | &lt;code&gt;'mean'&lt;/code&gt; | &lt;code&gt;'sum'&lt;/code&gt;. &lt;code&gt;'none'&lt;/code&gt;: no reduction will be applied, &lt;code&gt;'mean'&lt;/code&gt;: the weighted mean of the output is taken, &lt;code&gt;'sum'&lt;/code&gt;: the output will be summed. Note: &lt;code&gt;size_average&lt;/code&gt; and &lt;code&gt;reduce&lt;/code&gt; are in the process of being deprecated, and in the meantime, specifying either of those two args will override &lt;code&gt;reduction&lt;/code&gt;. Default: &lt;code&gt;'mean'&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;reduction&lt;/strong&gt;（&lt;em&gt;字符串&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;指定要应用于输出的缩减： &lt;code&gt;'none'&lt;/code&gt; | &lt;code&gt;'mean'&lt;/code&gt; | &lt;code&gt;'sum'&lt;/code&gt; 。 &lt;code&gt;'none'&lt;/code&gt; ：不减少任何值， &lt;code&gt;'mean'&lt;/code&gt; ：得出输出的加权平均值， &lt;code&gt;'sum'&lt;/code&gt; ：得出输出的总和。注： &lt;code&gt;size_average&lt;/code&gt; 和 &lt;code&gt;reduce&lt;/code&gt; 处于被淘汰，而在此期间，指定是这两个参数的个数将覆盖 &lt;code&gt;reduction&lt;/code&gt; 。默认值： &lt;code&gt;'mean'&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="26f5663a719cfdd4fa447aac8908e9938b53db2d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;reinterpreted_batch_ndims&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the number of batch dims to reinterpret as event dims</source>
          <target state="translated">&lt;strong&gt;reinterpreted_batch_ndims&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;要重新解释为事件暗淡的批暗淡的数量</target>
        </trans-unit>
        <trans-unit id="6a3f6bd6ab2f6e4081336546b9b09a6627d4bd3b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;repeats&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The number of repetitions for each element. repeats is broadcasted to fit the shape of the given axis.</source>
          <target state="translated">&lt;strong&gt;重复次数&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;每个元素的重复次数。重复播放以适合给定轴的形状。</target>
        </trans-unit>
        <trans-unit id="b1b614c9ba8db64b7bf312b7a13b2a9bb7e47954" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;replacement&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row.</source>
          <target state="translated">&lt;strong&gt;替换&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则抽取替换样本。如果不是，它们将被绘制而不会被替换，这意味着当为一行绘制样本索引时，将无法为该行再次绘制它。</target>
        </trans-unit>
        <trans-unit id="c742f71bde47f3a77d49c3447369afe1d00535bb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;replacement&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; samples are drawn on-demand with replacement if &lt;code&gt;True&lt;/code&gt;, default=``False``</source>
          <target state="translated">&lt;strong&gt;替换&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则按需抽取样本，如果为True，则默认为=&amp;ldquo; False``</target>
        </trans-unit>
        <trans-unit id="624d56544251a5adb1f3f92beef5fbb3779fffe6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;replacement&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to draw with replacement or not</source>
          <target state="translated">&lt;strong&gt;替换&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;布尔&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;是否用替换绘制</target>
        </trans-unit>
        <trans-unit id="c9636def42aa5ea6379407cbf86e341937067fa5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;repo_or_dir&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; repo name (&lt;code&gt;repo_owner/repo_name[:tag_name]&lt;/code&gt;), if &lt;code&gt;source = 'github'&lt;/code&gt;; or a path to a local directory, if &lt;code&gt;source = 'local'&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;repo_or_dir&lt;/strong&gt;（&lt;em&gt;字符串&lt;/em&gt;）&amp;ndash;如果 &lt;code&gt;source = 'github'&lt;/code&gt; ，则为回购名称（ &lt;code&gt;repo_owner/repo_name[:tag_name]&lt;/code&gt; ）；或本地目录的路径（如果 &lt;code&gt;source = 'local'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="9d8769220135033f3f9eb757c4af510a8fde0c5e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;requires_grad&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If autograd should record operations on this tensor. Default: &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;requires_grad&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;布尔&lt;/a&gt;） -如果autograd应在此张记录操作。默认值： &lt;code&gt;True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4b55034c998f6c870737977254bec184876385d6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;requires_grad&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; whether autograd should record operations on parameters in this module. Default: &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;requires_grad&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;布尔&lt;/a&gt;） - autograd是否应在此模块中的参数记录等操作。默认值： &lt;code&gt;True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7483a3c1ab04560df78ce5fcc47dc422c5b6fb38" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;requires_grad&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If autograd should record operations on the returned tensor. Default: &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;requires_grad&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;布尔&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;） -如果autograd应返回的记录张操作。默认值： &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4f8da86e61fff9872b332d5a83e86f2da79513dc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;requires_grad&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if the parameter requires gradient. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/autograd.html#excluding-subgraphs&quot;&gt;Excluding subgraphs from backward&lt;/a&gt; for more details. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;require_grad&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果参数需要渐变。有关更多详细信息，请参见&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/autograd.html#excluding-subgraphs&quot;&gt;从后面排除子图&lt;/a&gt;。默认值： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="50f6783d1e07f9c1440a696e4976ac787e5b69d9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;result&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#object&quot;&gt;object&lt;/a&gt;) &amp;ndash; the result object of this &lt;code&gt;Future&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;result&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#object&quot;&gt;对象&lt;/a&gt;）&amp;ndash;此 &lt;code&gt;Future&lt;/code&gt; 的结果对象。</target>
        </trans-unit>
        <trans-unit id="6760018ae15219fda8ff14952fb1cf636108f831" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;retain_graph&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;False&lt;/code&gt;, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to &lt;code&gt;True&lt;/code&gt; is not needed and often can be worked around in a much more efficient way. Defaults to the value of &lt;code&gt;create_graph&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;keep_graph&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为 &lt;code&gt;False&lt;/code&gt; ，则用于计算grad的图形将被释放。请注意，几乎在所有情况下都不需要将此选项设置为 &lt;code&gt;True&lt;/code&gt; ，并且通常可以以更有效的方式解决它。默认为 &lt;code&gt;create_graph&lt;/code&gt; 的值。</target>
        </trans-unit>
        <trans-unit id="90a602776d12962951cc0385731c01ed3bcab28d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;retain_graph&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;False&lt;/code&gt;, the graph used to compute the grads will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of &lt;code&gt;create_graph&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;keep_graph&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为 &lt;code&gt;False&lt;/code&gt; ，则将释放用于计算毕业&lt;strong&gt;人数&lt;/strong&gt;的图表。请注意，几乎在所有情况下都不需要将此选项设置为True，并且通常可以以更有效的方式解决它。默认为 &lt;code&gt;create_graph&lt;/code&gt; 的值。</target>
        </trans-unit>
        <trans-unit id="2cfcdef9f1e81b511c2a831483836e1d2395c58f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;retain_graph&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to True to run backward multiple times.</source>
          <target state="translated">&lt;strong&gt;keep_graph&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为False，将释放用于计算grad的图形。请注意，几乎在所有情况下都不需要将此选项设置为True，并且通常可以以更有效的方式解决它。通常，您需要将其设置为True才能向后运行多次。</target>
        </trans-unit>
        <trans-unit id="a53068eb20e1703e6fae54e116c28d41c909b9cb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;return_complex&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to return a complex tensor, or a real tensor with an extra last dimension for the real and imaginary components.</source>
          <target state="translated">&lt;strong&gt;return_complex&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;是返回一个复数张量，还是返回具有实数和虚数分量的额外最后维度的实数张量。</target>
        </trans-unit>
        <trans-unit id="6e3ba44aedb23f32452a877f1431281949acdfbc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;return_complex&lt;/strong&gt; (&lt;em&gt;Optional&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; Whether the output should be complex, or if the input should be assumed to derive from a real signal and window. Note that this is incompatible with &lt;code&gt;onesided=True&lt;/code&gt;. (Default: &lt;code&gt;False&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;return_complex&lt;/strong&gt;（&lt;em&gt;可选&lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;输出应该是复杂的，还是应该假定输入是从真实信号和窗口派生的。请注意，这与 &lt;code&gt;onesided=True&lt;/code&gt; 不兼容。（默认： &lt;code&gt;False&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="451a8ec8a7c215fb8c9eb2bcdfe93c4066ee62d4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;return_counts&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Whether to also return the counts for each unique element.</source>
          <target state="translated">&lt;strong&gt;return_counts&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;是否还返回每个唯一元素的计数。</target>
        </trans-unit>
        <trans-unit id="be0b737a57d022be99ca3806d10ba96bda8a2e6c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;return_indices&lt;/strong&gt; &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, will return the argmax along with the max values. Useful for &lt;a href=&quot;torch.nn.maxunpool1d#torch.nn.MaxUnpool1d&quot;&gt;&lt;code&gt;torch.nn.MaxUnpool1d&lt;/code&gt;&lt;/a&gt; later</source>
          <target state="translated">&lt;strong&gt;return_indices&lt;/strong&gt; &amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则将返回argmax以及最大值。以后对&lt;a href=&quot;torch.nn.maxunpool1d#torch.nn.MaxUnpool1d&quot;&gt; &lt;code&gt;torch.nn.MaxUnpool1d&lt;/code&gt; &lt;/a&gt;有用</target>
        </trans-unit>
        <trans-unit id="dfccddd5be08bd3e21421b7d578619f610a4ebf2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;return_indices&lt;/strong&gt; &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, will return the indices along with the outputs. Useful to pass to &lt;code&gt;nn.MaxUnpool2d()&lt;/code&gt;. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;return_indices&lt;/strong&gt; &amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则将返回索引以及输出。传递给 &lt;code&gt;nn.MaxUnpool2d()&lt;/code&gt; 很有用。默认值： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="24f9f4b06fa0e2c9a270a8c0bf40171f36526e9d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;return_indices&lt;/strong&gt; &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool1d. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;return_indices&lt;/strong&gt; &amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则将返回索引以及输出。传递给nn.MaxUnpool1d很有用。默认值： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="3acdc3b1a33b53e4798f96fee93430c34612b83e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;return_indices&lt;/strong&gt; &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;return_indices&lt;/strong&gt; &amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则将返回索引以及输出。传递给nn.MaxUnpool2d很有用。默认值： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="dc9007ec5c0f693cbc0478e2780a9eeb50ea22b3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;return_indices&lt;/strong&gt; &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool3d. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;return_indices&lt;/strong&gt; &amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则将返回索引以及输出。传递给nn.MaxUnpool3d很有用。默认值： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="e2e8bb04f084571c1e2d5a7885f2d614f58c1003" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;return_indices&lt;/strong&gt; &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, will return the max indices along with the outputs. Useful for &lt;a href=&quot;torch.nn.maxunpool2d#torch.nn.MaxUnpool2d&quot;&gt;&lt;code&gt;torch.nn.MaxUnpool2d&lt;/code&gt;&lt;/a&gt; later</source>
          <target state="translated">&lt;strong&gt;return_indices&lt;/strong&gt; &amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则将返回最大索引以及输出。以后对&lt;a href=&quot;torch.nn.maxunpool2d#torch.nn.MaxUnpool2d&quot;&gt; &lt;code&gt;torch.nn.MaxUnpool2d&lt;/code&gt; &lt;/a&gt;有用</target>
        </trans-unit>
        <trans-unit id="cb3879f13bacb4ac2075aca61af136eb87bb632e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;return_indices&lt;/strong&gt; &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, will return the max indices along with the outputs. Useful for &lt;a href=&quot;torch.nn.maxunpool3d#torch.nn.MaxUnpool3d&quot;&gt;&lt;code&gt;torch.nn.MaxUnpool3d&lt;/code&gt;&lt;/a&gt; later</source>
          <target state="translated">&lt;strong&gt;return_indices&lt;/strong&gt; &amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则将返回最大索引以及输出。以后对&lt;a href=&quot;torch.nn.maxunpool3d#torch.nn.MaxUnpool3d&quot;&gt; &lt;code&gt;torch.nn.MaxUnpool3d&lt;/code&gt; &lt;/a&gt;有用</target>
        </trans-unit>
        <trans-unit id="c2d9bba5ad78165f7e64c1456856426896f737bb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;return_indices&lt;/strong&gt; &amp;ndash; whether to return pooling indices. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;return_indices&lt;/strong&gt; &amp;ndash;是否返回池索引。默认值： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="0ccd823978e457f89c779b1144535100d72c2c1d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;return_inverse&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Whether to also return the indices for where elements in the original input ended up in the returned unique list.</source>
          <target state="translated">&lt;strong&gt;return_inverse&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;是否还返回原始输入中元素在返回的唯一列表中所处位置的索引。</target>
        </trans-unit>
        <trans-unit id="1e938f9292b067d102f23035466dfb0011287d87" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;rho&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; coefficient used for computing a running average of squared gradients (default: 0.9)</source>
          <target state="translated">&lt;strong&gt;rho&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;用于计算平方梯度的移动平均值的系数（默认值：0.9）</target>
        </trans-unit>
        <trans-unit id="dac774a90d09e020fc94b8cf488f1c4dac29e906" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;right&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if False, return the first suitable location that is found. If True, return the last such index. If no suitable index found, return 0 for non-numerical value (eg. nan, inf) or the size of &lt;code&gt;boundaries&lt;/code&gt; (one pass the last index). In other words, if False, gets the lower bound index for each value in &lt;code&gt;input&lt;/code&gt; from &lt;code&gt;boundaries&lt;/code&gt;. If True, gets the upper bound index instead. Default value is False.</source>
          <target state="translated">&lt;strong&gt;right&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为False，则返回找到的第一个合适的位置。如果为True，则返回最后一个这样的索引。如果找不到合适的索引，则对于非数字值（例如nan，inf）或 &lt;code&gt;boundaries&lt;/code&gt; 大小（一个通过最后一个索引）返回0 。换句话说，如果为False，则从 &lt;code&gt;boundaries&lt;/code&gt; 获取 &lt;code&gt;input&lt;/code&gt; 每个值的下限索引。如果为True，则改为获取上限索引。默认值为False。</target>
        </trans-unit>
        <trans-unit id="5f99ca7c7e81085a92ca3b1ee3f61f4f25afa5c2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;right&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if False, return the first suitable location that is found. If True, return the last such index. If no suitable index found, return 0 for non-numerical value (eg. nan, inf) or the size of &lt;em&gt;innermost&lt;/em&gt; dimension within &lt;code&gt;sorted_sequence&lt;/code&gt; (one pass the last index of the &lt;em&gt;innermost&lt;/em&gt; dimension). In other words, if False, gets the lower bound index for each value in &lt;code&gt;values&lt;/code&gt; on the corresponding &lt;em&gt;innermost&lt;/em&gt; dimension of the &lt;code&gt;sorted_sequence&lt;/code&gt;. If True, gets the upper bound index instead. Default value is False.</source>
          <target state="translated">&lt;strong&gt;right&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为False，则返回找到的第一个合适的位置。如果为True，则返回最后一个这样的索引。如果找不到合适的索引，则对于非数值（例如nan，inf）或 &lt;code&gt;sorted_sequence&lt;/code&gt; 中&lt;em&gt;最里面的&lt;/em&gt;维度的大小（一个传递&lt;em&gt;最里面的&lt;/em&gt;维度的最后一个索引），返回0 。换句话说，如果假时，获取在每个值下界索引 &lt;code&gt;values&lt;/code&gt; 在相应的&lt;em&gt;最里面&lt;/em&gt;的尺寸 &lt;code&gt;sorted_sequence&lt;/code&gt; 。如果为True，则改为获取上限索引。默认值为False。&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="8af0242748d4ccc71f6d117d72964f9f1a6dfc76" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;roots&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;) &amp;ndash; Tensors which represent the roots of the autograd computation. All the tensors should be scalars.</source>
          <target state="translated">&lt;strong&gt;roots&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;）&amp;ndash;代表自动梯度计算的根的张量。所有张量应为标量。</target>
        </trans-unit>
        <trans-unit id="6e703cfc7b66dc3da520fdcf8d4940424403970a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;row&lt;/strong&gt; (&lt;code&gt;int&lt;/code&gt;) &amp;ndash; number of rows in the 2-D matrix.</source>
          <target state="translated">&lt;strong&gt;row&lt;/strong&gt;（ &lt;code&gt;int&lt;/code&gt; ）&amp;ndash;二维矩阵中的行数。</target>
        </trans-unit>
        <trans-unit id="df8808dbd32331cc487f9561ab89f6497d98b06d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;rpc_backend_options&lt;/strong&gt; (&lt;a href=&quot;#torch.distributed.rpc.RpcBackendOptions&quot;&gt;RpcBackendOptions&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The options passed to the RpcAgent constructor. It must be an agent-specific subclass of &lt;a href=&quot;#torch.distributed.rpc.RpcBackendOptions&quot;&gt;&lt;code&gt;RpcBackendOptions&lt;/code&gt;&lt;/a&gt; and contains agent-specific initialization configurations. By default, for all agents, it sets the default timeout to 60 seconds and performs the rendezvous with an underlying process group initialized using &lt;code&gt;init_method = &quot;env://&quot;&lt;/code&gt;, meaning that environment variables &lt;code&gt;MASTER_ADDR&lt;/code&gt; and &lt;code&gt;MASTER_PORT&lt;/code&gt; need to be set properly. See &lt;a href=&quot;#rpc-backends&quot;&gt;Backends&lt;/a&gt; for more information and find which options are available.</source>
          <target state="translated">&lt;strong&gt;rpc_backend_options&lt;/strong&gt;（&lt;a href=&quot;#torch.distributed.rpc.RpcBackendOptions&quot;&gt;RpcBackendOptions &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;传递给RpcAgent构造函数的选项。它必须是&lt;a href=&quot;#torch.distributed.rpc.RpcBackendOptions&quot;&gt; &lt;code&gt;RpcBackendOptions&lt;/code&gt; &lt;/a&gt;的特定于代理的子类，并且包含特定于代理的初始化配置。默认情况下，对于所有代理，它将默认超时设置为60秒，并与使用 &lt;code&gt;init_method = &quot;env://&quot;&lt;/code&gt; 初始化的基础进程组执行集合，这意味着需要正确设置环境变量 &lt;code&gt;MASTER_ADDR&lt;/code&gt; 和 &lt;code&gt;MASTER_PORT&lt;/code&gt; 。有关更多信息，请参见&lt;a href=&quot;#rpc-backends&quot;&gt;后端&lt;/a&gt;，并找到可用的选项。</target>
        </trans-unit>
        <trans-unit id="fa0b9defbfa21ac977266023b0b5c46f0b2f3e8a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;rpc_timeout&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The default timeout, in seconds, for RPC requests (default: 60 seconds). If the RPC has not completed in this timeframe, an exception indicating so will be raised. Callers can override this timeout for individual RPCs in &lt;a href=&quot;#torch.distributed.rpc.rpc_sync&quot;&gt;&lt;code&gt;rpc_sync()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt; if necessary.</source>
          <target state="translated">&lt;strong&gt;rpc_timeout&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash; RPC请求的默认超时（以秒为单位）（默认：60秒）。如果RPC在此时间范围内尚未完成，则会引发一个异常提示。如有必要，&lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt; &lt;code&gt;rpc_async()&lt;/code&gt; &lt;/a&gt;者可以为&lt;a href=&quot;#torch.distributed.rpc.rpc_sync&quot;&gt; &lt;code&gt;rpc_sync()&lt;/code&gt; &lt;/a&gt;和rpc_async（）中的各个RPC覆盖此超时。</target>
        </trans-unit>
        <trans-unit id="a9b4c9479c732d9f30ea164524b3a33731786f26" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;rtol&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; relative tolerance</source>
          <target state="translated">&lt;strong&gt;rtol&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;相对公差</target>
        </trans-unit>
        <trans-unit id="bacc9f0e60fd24a901f52d44a4638e9e04c80978" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;rtol&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; relative tolerance. Default: 1e-05</source>
          <target state="translated">&lt;strong&gt;rtol&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;相对公差。默认值：1e-05</target>
        </trans-unit>
        <trans-unit id="68516e2a3f22a5073a1b61d78f57c2742a450d43" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;run_args&lt;/strong&gt; &amp;ndash; positional arguments for &lt;code&gt;run_fn&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;run_args&lt;/strong&gt; -用于定位参数 &lt;code&gt;run_fn&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="bd1dab5b3aea5e89f1c99964e19bebd1e28fff57" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;run_fn&lt;/strong&gt; &amp;ndash; a calibration function for calibrating the prepared model</source>
          <target state="translated">&lt;strong&gt;run_fn&lt;/strong&gt; &amp;ndash;用于校准准备好的模型的校准功能</target>
        </trans-unit>
        <trans-unit id="931e0227dde3971f2dd3e3a902f46ff3b3f327d7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;run_fn&lt;/strong&gt; &amp;ndash; a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop</source>
          <target state="translated">&lt;strong&gt;run_fn&lt;/strong&gt; &amp;ndash;用于评估准备好的模型的函数，可以是仅运行准备好的模型的函数或训练循环</target>
        </trans-unit>
        <trans-unit id="4b07538c76cf919ebc72b7a60c06e46f59f69d90" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;run_name&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; Name of the run, to be included as part of the logdir. If unspecified, will use current timestamp.</source>
          <target state="translated">&lt;strong&gt;run_name&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;）&amp;ndash;运行的名称，将包含在logdir中。如果未指定，将使用当前时间戳。</target>
        </trans-unit>
        <trans-unit id="d393489b76cacef3a8a7486c7be0cd8c3544afb0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;s&lt;/strong&gt; (&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Signal size in the transformed dimensions. If given, each dimension &lt;code&gt;dim[i]&lt;/code&gt; will either be zero-padded or trimmed to the length &lt;code&gt;s[i]&lt;/code&gt; before computing the FFT. If a length &lt;code&gt;-1&lt;/code&gt; is specified, no padding is done in that dimension. Default: &lt;code&gt;s = [input.size(d) for d in dim]&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;s&lt;/strong&gt;（&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;] &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;转换后的尺寸中的信号尺寸。如果给定，则在计算FFT之前，将每个维 &lt;code&gt;dim[i]&lt;/code&gt; 补零或修整为长度 &lt;code&gt;s[i]&lt;/code&gt; 。如果指定长度为 &lt;code&gt;-1&lt;/code&gt; ，则不会在该维度上进行填充。默认值： &lt;code&gt;s = [input.size(d) for d in dim]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="383f45f0226b4e5e81f81f93c4ac71001b0283d4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;s&lt;/strong&gt; (&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Signal size in the transformed dimensions. If given, each dimension &lt;code&gt;dim[i]&lt;/code&gt; will either be zero-padded or trimmed to the length &lt;code&gt;s[i]&lt;/code&gt; before computing the IFFT. If a length &lt;code&gt;-1&lt;/code&gt; is specified, no padding is done in that dimension. Default: &lt;code&gt;s = [input.size(d) for d in dim]&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;s&lt;/strong&gt;（&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;] &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;转换后的尺寸中的信号尺寸。如果给定，则在计算IFFT之前，将每个维度 &lt;code&gt;dim[i]&lt;/code&gt; 补零或修整为长度 &lt;code&gt;s[i]&lt;/code&gt; 。如果指定长度为 &lt;code&gt;-1&lt;/code&gt; ，则不会在该维度上进行填充。默认值： &lt;code&gt;s = [input.size(d) for d in dim]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="3025f4149f55dd0264a95f1c44c06d665d4e4023" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;s&lt;/strong&gt; (&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Signal size in the transformed dimensions. If given, each dimension &lt;code&gt;dim[i]&lt;/code&gt; will either be zero-padded or trimmed to the length &lt;code&gt;s[i]&lt;/code&gt; before computing the real FFT. If a length &lt;code&gt;-1&lt;/code&gt; is specified, no padding is done in that dimension. Default: &lt;code&gt;s = [input.size(d) for d in dim]&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;s&lt;/strong&gt;（&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;] &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;转换后的尺寸中的信号尺寸。如果给定，则在计算实数FFT之前，每个维 &lt;code&gt;dim[i]&lt;/code&gt; 将被补零或修整为长度 &lt;code&gt;s[i]&lt;/code&gt; 。如果指定长度为 &lt;code&gt;-1&lt;/code&gt; ，则不会在该维度上进行填充。默认值： &lt;code&gt;s = [input.size(d) for d in dim]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="7febbe2af643e6ca772e051b519b3c65a06281d0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;s&lt;/strong&gt; (&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Signal size in the transformed dimensions. If given, each dimension &lt;code&gt;dim[i]&lt;/code&gt; will either be zero-padded or trimmed to the length &lt;code&gt;s[i]&lt;/code&gt; before computing the real FFT. If a length &lt;code&gt;-1&lt;/code&gt; is specified, no padding is done in that dimension. Defaults to even output in the last dimension: &lt;code&gt;s[-1] = 2*(input.size(dim[-1]) - 1)&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;s&lt;/strong&gt;（&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;] &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;转换后的尺寸中的信号尺寸。如果给定，则在计算实数FFT之前，每个维 &lt;code&gt;dim[i]&lt;/code&gt; 将被补零或修整为长度 &lt;code&gt;s[i]&lt;/code&gt; 。如果指定长度为 &lt;code&gt;-1&lt;/code&gt; ，则不会在该维度上进行填充。默认为最后一个维度的偶数输出： &lt;code&gt;s[-1] = 2*(input.size(dim[-1]) - 1)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e00929ae140d39c48352cf6fa3194ee39c66f5ab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sample_rate&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; sample rate in Hz</source>
          <target state="translated">&lt;strong&gt;sample_rate&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;采样率，单位Hz</target>
        </trans-unit>
        <trans-unit id="ede0ae581174ad5f881503114e9fd37062ebd9c3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sampler&lt;/strong&gt; (&lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt;Sampler&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Iterable&lt;/em&gt;) &amp;ndash; Base sampler. Can be any iterable object</source>
          <target state="translated">&lt;strong&gt;采样器&lt;/strong&gt;（&lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt;Sampler&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;Iterable&lt;/em&gt;）&amp;ndash;基本采样器。可以是任何可迭代的对象</target>
        </trans-unit>
        <trans-unit id="e17b89346bc0115c104f4ad35f54ceb6abdcd713" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sampler&lt;/strong&gt; (&lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt;Sampler&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Iterable&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; defines the strategy to draw samples from the dataset. Can be any &lt;code&gt;Iterable&lt;/code&gt; with &lt;code&gt;__len__&lt;/code&gt; implemented. If specified, &lt;code&gt;shuffle&lt;/code&gt; must not be specified.</source>
          <target state="translated">&lt;strong&gt;采样器&lt;/strong&gt;（&lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt;Sampler&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;Iterable &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;定义从数据集中抽取样本的策略。可以与实现 &lt;code&gt;__len__&lt;/code&gt; 的任何 &lt;code&gt;Iterable&lt;/code&gt; 一起使用。如果指定，则不得指定 &lt;code&gt;shuffle&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0f88769c8058a6eff5bd8fee7b065e6e5f50592a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scalar_value&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;string/blobname&lt;/em&gt;) &amp;ndash; Value to save</source>
          <target state="translated">&lt;strong&gt;scalar_value&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;string / blobname&lt;/em&gt;）&amp;ndash;要保存的值</target>
        </trans-unit>
        <trans-unit id="70829693ef66298f30df8e08fe38a1e8755c2ac6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale&lt;/strong&gt; &amp;ndash; quantization scale for the output. Default: 1.0</source>
          <target state="translated">&lt;strong&gt;标度&lt;/strong&gt;&amp;ndash;输出的量化标度。默认值：1.0</target>
        </trans-unit>
        <trans-unit id="8194d48378537f087c73ec0589aa42997e1c5c42" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale&lt;/strong&gt; &amp;ndash; quantization scale of the output tensor</source>
          <target state="translated">&lt;strong&gt;标度&lt;/strong&gt;&amp;ndash;输出张量的量化标度</target>
        </trans-unit>
        <trans-unit id="0a965b18684d8dfdbd37cc4f8057ea12da1baa61" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale&lt;/strong&gt; &amp;ndash; scale of the output Quantized Tensor</source>
          <target state="translated">&lt;strong&gt;标度&lt;/strong&gt;-输出&lt;strong&gt;标&lt;/strong&gt;度量化张量</target>
        </trans-unit>
        <trans-unit id="d171e9b1743bffba5ac11a6cc0337364c0f09257" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; scale to apply in quantization formula</source>
          <target state="translated">&lt;strong&gt;标度&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;浮点数&lt;/a&gt;）&amp;ndash;适用于量化公式的标度</target>
        </trans-unit>
        <trans-unit id="a5475858e5269ec567bda19595cd62989e64c5f2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Scale parameter of distribution (lambda).</source>
          <target state="translated">&lt;strong&gt;scale&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;分布的比例参数（lambda）。</target>
        </trans-unit>
        <trans-unit id="0b6af0ea65b40800b2fae6da73738fab11b8f539" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Scale parameter of the distribution</source>
          <target state="translated">&lt;strong&gt;scale&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;分布的比例参数</target>
        </trans-unit>
        <trans-unit id="5cf6bbf14633821ae6932495b773f589768f9883" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; half width at half maximum.</source>
          <target state="translated">&lt;strong&gt;标尺&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;浮点&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;一半为一半，最大值为一半。</target>
        </trans-unit>
        <trans-unit id="8e3168db0f835585afe526c7a2207657fd63567b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; scale of the distribution</source>
          <target state="translated">&lt;strong&gt;比例尺&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;浮动&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;分配比例尺</target>
        </trans-unit>
        <trans-unit id="e80c10e47ee88300a9ec3b9e4150ab361d1b234b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; scale of the full Cauchy distribution</source>
          <target state="translated">&lt;strong&gt;标度&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;浮点数&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;整个柯西分布的标度</target>
        </trans-unit>
        <trans-unit id="e3e27d0068eebb99b1c5c41e219ae176a56ddfb6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; scale of the full Normal distribution</source>
          <target state="translated">&lt;strong&gt;标度&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;浮点数&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;完整正态分布的标度</target>
        </trans-unit>
        <trans-unit id="17769021d37f4a1162d81775831adc53c9dbb866" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; standard deviation of log of the distribution</source>
          <target state="translated">&lt;strong&gt;标度&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;浮点&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;分布的对数的标准偏差</target>
        </trans-unit>
        <trans-unit id="0762f9d704810cff600f854a8a17f9eac019fd19" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; standard deviation of the distribution (often referred to as sigma)</source>
          <target state="translated">&lt;strong&gt;标度&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;浮点&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;分布的标准偏差（通常称为sigma）</target>
        </trans-unit>
        <trans-unit id="8dcf78ac62be4f4d6912e826c211335f116f32ae" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; Scale parameter.</source>
          <target state="translated">&lt;strong&gt;标度&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;浮点数&lt;/a&gt;）&amp;ndash;标度参数。</target>
        </trans-unit>
        <trans-unit id="7bf27f94abfecc48294933aebaa97dc03946f28b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale&lt;/strong&gt; (&lt;em&gt;double&lt;/em&gt;) &amp;ndash; output scale. If None, derived from the input scale</source>
          <target state="translated">&lt;strong&gt;标度&lt;/strong&gt;（&lt;em&gt;double&lt;/em&gt;）&amp;ndash;输出标度。如果为无，则从输入比例中得出</target>
        </trans-unit>
        <trans-unit id="2b336660f6fee25a7f04192c1f5d9cf1fa688414" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale&lt;/strong&gt; - quantization scale of the output, type: double.</source>
          <target state="translated">&lt;strong&gt;规模&lt;/strong&gt;-输出，输入的量化尺度：双。</target>
        </trans-unit>
        <trans-unit id="a683de9da2c79075bf71043aa5f98cf77b5329fb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale_factor&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; multiplier for spatial size.</source>
          <target state="translated">&lt;strong&gt;scale_factor&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;] &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;空间大小的乘数。</target>
        </trans-unit>
        <trans-unit id="87bbe589969398126c66e3c05dc45cc47c8a68bd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale_factor&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;] or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;] or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; multiplier for spatial size. Has to match input size if it is a tuple.</source>
          <target state="translated">&lt;strong&gt;scale_factor&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;]或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;]或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;] &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;空间大小的乘数。如果是元组，则必须匹配输入大小。</target>
        </trans-unit>
        <trans-unit id="0990d26d84ad85e506c48292c1fe24a540c4e742" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale_factor&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; multiplier for spatial size. Has to be an integer.</source>
          <target state="translated">&lt;strong&gt;scale_factor&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;空间大小的乘数。必须是整数。</target>
        </trans-unit>
        <trans-unit id="c6a7cf25445a5d29d088501a3080baadf196fbe1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale_factor&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; multiplier for spatial size. Has to match input size if it is a tuple.</source>
          <target state="translated">&lt;strong&gt;scale_factor&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;空间大小的乘数。如果是元组，则必须匹配输入大小。</target>
        </trans-unit>
        <trans-unit id="9aec3e62b7b949348936978b7c1cb99d932c0d1d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale_factor&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; multiplier for spatial size. Has to be an integer.</source>
          <target state="translated">&lt;strong&gt;scale_factor&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;空间大小的乘数。必须是整数。</target>
        </trans-unit>
        <trans-unit id="74af9ed5d3017b743baa3848deaf6665c7f9adf4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale_factor&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; multiplier for spatial size</source>
          <target state="translated">&lt;strong&gt;scale_factor&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;空间大小的乘数</target>
        </trans-unit>
        <trans-unit id="4f035dbf64185002cfc63d6f96931c6065065b6e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale_fn&lt;/strong&gt; (&lt;em&gt;function&lt;/em&gt;) &amp;ndash; Custom scaling policy defined by a single argument lambda function, where 0 &amp;lt;= scale_fn(x) &amp;lt;= 1 for all x &amp;gt;= 0. If specified, then &amp;lsquo;mode&amp;rsquo; is ignored. Default: None</source>
          <target state="translated">&lt;strong&gt;scale_fn&lt;/strong&gt;（&lt;em&gt;函数&lt;/em&gt;）&amp;ndash;由单个参数lambda函数定义的自定义缩放策略，其中对于所有x&amp;gt; = 0，0 &amp;lt;= scale_fn（x）&amp;lt;=1。如果指定，则忽略&amp;ldquo; mode&amp;rdquo;。默认值：无</target>
        </trans-unit>
        <trans-unit id="dc440c6b67388bb8565432e11af1c8daf58557ee" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale_grad_by_freq&lt;/strong&gt; (&lt;em&gt;boolean&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;scale_grad_by_freq&lt;/strong&gt;（&lt;em&gt;boolean &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果给定，将按小批量中单词频率的倒数来缩放梯度。默认为 &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1a079c4c0b1c3e38cde6fc8a641cfa06c56a1010" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale_grad_by_freq&lt;/strong&gt; (&lt;em&gt;boolean&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; See module initialization documentation. Default &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;scale_grad_by_freq&lt;/strong&gt;（&lt;em&gt;boolean &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;请参阅模块初始化文档。默认为 &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="85cf53fa92b9f2533942327656544089d09a9765" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale_grad_by_freq&lt;/strong&gt; (&lt;em&gt;boolean&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default &lt;code&gt;False&lt;/code&gt;. Note: this option is not supported when &lt;code&gt;mode=&quot;max&quot;&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;scale_grad_by_freq&lt;/strong&gt;（&lt;em&gt;boolean &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;如果给定的话，它将按照小批量中单词频率的倒数来缩放梯度。默认为 &lt;code&gt;False&lt;/code&gt; 。注意：当 &lt;code&gt;mode=&quot;max&quot;&lt;/code&gt; 时不支持此选项。</target>
        </trans-unit>
        <trans-unit id="cd7a2b15683af22c973bad97482e50efa4fda201" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale_mode&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; {&amp;lsquo;cycle&amp;rsquo;, &amp;lsquo;iterations&amp;rsquo;}. Defines whether scale_fn is evaluated on cycle number or cycle iterations (training iterations since start of cycle). Default: &amp;lsquo;cycle&amp;rsquo;</source>
          <target state="translated">&lt;strong&gt;scale_mode&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;）&amp;ndash; {'cycle'，'iterations'}。定义是否在循环数或循环迭代（自循环开始以来的训练迭代）上评估scale_fn。默认值：&amp;ldquo;循环&amp;rdquo;</target>
        </trans-unit>
        <trans-unit id="bb349689664c276bbe5b79256f79b9d60afa7249" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scale_tril&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; lower-triangular factor of covariance, with positive-valued diagonal</source>
          <target state="translated">&lt;strong&gt;scale_tril&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;协方差的下三角因子，对角线为正值</target>
        </trans-unit>
        <trans-unit id="1979e70804c1eb78b8633c391b664926b49b8921" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scales&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; float 1D tensor of scales to use, size should match &lt;code&gt;input.size(axis)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;标度&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;浮动的要使用的1D标度张量，大小应与 &lt;code&gt;input.size(axis)&lt;/code&gt; 相匹配</target>
        </trans-unit>
        <trans-unit id="97926b134c895c01bbe8f6c26f328a2eb031fb49" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scatter_list&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; List of tensors to scatter (default is None, must be specified on the source rank)</source>
          <target state="translated">&lt;strong&gt;scatter_list&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list &lt;/a&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;要分散的张量的列表（默认为None，必须在源等级上指定）</target>
        </trans-unit>
        <trans-unit id="95f54acc947d1c59dde2c9179fd3865f972d8dfa" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sci_mode&lt;/strong&gt; &amp;ndash; Enable (True) or disable (False) scientific notation. If None (default) is specified, the value is defined by &lt;code&gt;torch._tensor_str._Formatter&lt;/code&gt;. This value is automatically chosen by the framework.</source>
          <target state="translated">&lt;strong&gt;sci_mode&lt;/strong&gt; &amp;ndash;启用（正确）或禁用（错误）科学计数法。如果指定None（默认），则该值由 &lt;code&gt;torch._tensor_str._Formatter&lt;/code&gt; 定义。该值由框架自动选择。</target>
        </trans-unit>
        <trans-unit id="31b95efb697eb9d32d8d0513d989798a369d679f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;scramble&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Setting this to &lt;code&gt;True&lt;/code&gt; will produce scrambled Sobol sequences. Scrambling is capable of producing better Sobol sequences. Default: &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;scramble&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;将其设置为 &lt;code&gt;True&lt;/code&gt; 将产生加扰的Sobol序列。加扰能够产生更好的Sobol序列。默认值： &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0ace9de306213673d4dcea0e235e9069d516de2c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;script_module&lt;/strong&gt; &amp;ndash; An instance of torch script module with type of ScriptModule.</source>
          <target state="translated">&lt;strong&gt;script_module&lt;/strong&gt; &amp;ndash;火炬脚本模块的实例，其类型为ScriptModule。</target>
        </trans-unit>
        <trans-unit id="6cbb6e6c8f68076ceee85d8b653f1446d37b3d55" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;seed&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The desired seed.</source>
          <target state="translated">&lt;strong&gt;seed&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;所需的种子。</target>
        </trans-unit>
        <trans-unit id="9556506326cc7e5aba9d2bab7c9e945c1aa33011" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;seed&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The desired seed. Value must be within the inclusive range &lt;code&gt;[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]&lt;/code&gt;. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula &lt;code&gt;0xffff_ffff_ffff_ffff + seed&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;seed&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;所需的种子。值必须在包含范围内 &lt;code&gt;[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]&lt;/code&gt; 。否则，将引发RuntimeError。使用公式 &lt;code&gt;0xffff_ffff_ffff_ffff + seed&lt;/code&gt; 将负输入重新映射为正值。</target>
        </trans-unit>
        <trans-unit id="67900e91c8cb909f9e66451d9b54da1a81002763" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;seed&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; random seed used to shuffle the sampler if &lt;code&gt;shuffle=True&lt;/code&gt;. This number should be identical across all processes in the distributed group. Default: &lt;code&gt;0&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;seed&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;整数&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果 &lt;code&gt;shuffle=True&lt;/code&gt; ,则用于随机播放采样器的随机种子。在分布式组中的所有进程中，此数字应相同。默认： &lt;code&gt;0&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="42e3c476b10a593814d704cb92704cdb7628a925" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;seed&lt;/strong&gt; (&lt;em&gt;Int&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; This is the seed for the scrambling. The seed of the random number generator is set to this, if specified. Otherwise, it uses a random seed. Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;seed&lt;/strong&gt;（&lt;em&gt;Int &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;这是加扰的种子。如果指定，则将随机数生成器的种子设置为此。否则，它将使用随机种子。默认值： &lt;code&gt;None&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="34b993f82c0bff4c8876e51f86a4d0cd94041cd4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;segments&lt;/strong&gt; &amp;ndash; Number of chunks to create in the model</source>
          <target state="translated">&lt;strong&gt;segments&lt;/strong&gt; &amp;ndash;在模型中创建的块数</target>
        </trans-unit>
        <trans-unit id="b234df8ca1796df94acd4f2582e16d322c4d7b53" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;self&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the scalar base value for the power operation</source>
          <target state="translated">&lt;strong&gt;self&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;幂运算的标量基值</target>
        </trans-unit>
        <trans-unit id="3db3b9e470815a2d029e3df80be582d9373bb440" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sequence&lt;/strong&gt; (&lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt;PackedSequence&lt;/a&gt;) &amp;ndash; batch to pad</source>
          <target state="translated">&lt;strong&gt;序列&lt;/strong&gt;（&lt;a href=&quot;torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence&quot;&gt;PackedSequence&lt;/a&gt;）&amp;ndash;批量填充</target>
        </trans-unit>
        <trans-unit id="b266c49abe7b1dae13bdcaca6267d611f6a53c0f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sequences&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; A list of sequences of decreasing length.</source>
          <target state="translated">&lt;strong&gt;序列&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;列表&lt;/a&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;长度递减的序列列表。</target>
        </trans-unit>
        <trans-unit id="d2e643683d77e2e4974f3cca173d349dcef11874" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sequences&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; list of variable length sequences.</source>
          <target state="translated">&lt;strong&gt;序列&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;列表&lt;/a&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;可变长度序列的列表。</target>
        </trans-unit>
        <trans-unit id="516d3113df1d3fc366757f1c749d6916067ab846" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;set_to_none&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; instead of setting to zero, set the grads to None. See &lt;a href=&quot;../optim#torch.optim.Optimizer.zero_grad&quot;&gt;&lt;code&gt;torch.optim.Optimizer.zero_grad()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">&lt;strong&gt;set_to_none&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;将等级设置为&amp;ldquo;无&amp;rdquo;，而不是设置为零。有关详细信息，请参见&lt;a href=&quot;../optim#torch.optim.Optimizer.zero_grad&quot;&gt; &lt;code&gt;torch.optim.Optimizer.zero_grad()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="ae5372adcf73544efed6303bb63476d781673d62" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;set_to_none&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; instead of setting to zero, set the grads to None. This is will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests &lt;code&gt;zero_grad(set_to_none=True)&lt;/code&gt; followed by a backward pass, &lt;code&gt;.grad&lt;/code&gt;s are guaranteed to be None for params that did not receive a gradient. 3. &lt;code&gt;torch.optim&lt;/code&gt; optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether).</source>
          <target state="translated">&lt;strong&gt;set_to_none&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;将等级设置为&amp;ldquo;无&amp;rdquo;，而不是设置为零。通常，这将具有较低的内存占用量，并且可以适度提高性能。但是，它会更改某些行为。例如：1.当用户尝试访问渐变并对其执行手动操作时，None属性或全0的Tensor的行为将有所不同。2.如果用户请求 &lt;code&gt;zero_grad(set_to_none=True)&lt;/code&gt; ,然后向后传递，则对于未接收到渐变的参数， &lt;code&gt;.grad&lt;/code&gt; 保证为None。3.如果渐变为0或无，则 &lt;code&gt;torch.optim&lt;/code&gt; 优化器具有不同的行为（在一种情况下，它以0的梯度执行步骤，而在另一种情况下，则完全跳过该步骤）。</target>
        </trans-unit>
        <trans-unit id="1192414a9f920287eff5dbc4ec8c50b8a59e87d8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;shape&lt;/strong&gt; (&lt;em&gt;torch.Size&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;int...&lt;/em&gt;) &amp;ndash; the desired size</source>
          <target state="translated">&lt;strong&gt;形状&lt;/strong&gt;（&lt;em&gt;torch.Size&lt;/em&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;int ...&lt;/em&gt;）&amp;ndash;所需的大小</target>
        </trans-unit>
        <trans-unit id="dcd529c43883015420ed4381022949480a774183" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;shape&lt;/strong&gt; (&lt;em&gt;tuple of python:ints&lt;/em&gt;) &amp;ndash; the new shape</source>
          <target state="translated">&lt;strong&gt;shape&lt;/strong&gt;（&lt;em&gt;python：ints的元组&lt;/em&gt;）&amp;ndash;新形状</target>
        </trans-unit>
        <trans-unit id="afa1bc3f88d2613db86fb8ef6b22d240a6775e5f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;shape&lt;/strong&gt; (&lt;em&gt;tuple of python:ints&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;int...&lt;/em&gt;) &amp;ndash; the desired shape</source>
          <target state="translated">&lt;strong&gt;shape&lt;/strong&gt;（&lt;em&gt;python：ints&lt;/em&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;int ...的&lt;/em&gt;&lt;em&gt;元组&lt;/em&gt;）&amp;ndash;所需的形状</target>
        </trans-unit>
        <trans-unit id="182b2a09bf2ce9bed6ebdfb84b2291a8e31953a1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;shared&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; whether to share memory</source>
          <target state="translated">&lt;strong&gt;shared&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;是否共享内存</target>
        </trans-unit>
        <trans-unit id="6bb7f0e432956024dc35ef632a85dc75136860f3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;shifts&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of python:ints&lt;/em&gt;) &amp;ndash; The number of places by which the elements of the tensor are shifted. If shifts is a tuple, dims must be a tuple of the same size, and each dimension will be rolled by the corresponding value</source>
          <target state="translated">&lt;strong&gt;shifts&lt;/strong&gt;（&lt;em&gt;python：ints的&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;元组&lt;/em&gt;）&amp;ndash;张量元素移位的位置数。如果shifts是一个元组，则dims必须是相同大小的元组，并且每个维度将滚动相应的值</target>
        </trans-unit>
        <trans-unit id="a87a9278041bab20a7726487ef393bef15581381" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;shuffle&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt; (default), sampler will shuffle the indices.</source>
          <target state="translated">&lt;strong&gt;shuffle&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; （默认值），采样器将对索引进行随机排序。</target>
        </trans-unit>
        <trans-unit id="b2afec4ec297d37812ebe2a9e2d39534f54d4b8e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;shuffle&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; set to &lt;code&gt;True&lt;/code&gt; to have the data reshuffled at every epoch (default: &lt;code&gt;False&lt;/code&gt;).</source>
          <target state="translated">&lt;strong&gt;随机播放&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;设置为 &lt;code&gt;True&lt;/code&gt; 可使数据在每个时期都重新&lt;strong&gt;随机播放&lt;/strong&gt;（默认值： &lt;code&gt;False&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="03ea4cf5be76330e494785ae14e7e331672f9049" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;signal_ndim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the number of dimensions in each signal. &lt;code&gt;signal_ndim&lt;/code&gt; can only be 1, 2 or 3</source>
          <target state="translated">&lt;strong&gt;signal_ndim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;每个信号的维数。 &lt;code&gt;signal_ndim&lt;/code&gt; 只能是1、2或3</target>
        </trans-unit>
        <trans-unit id="3943ffb53f48c82b94eeb325494fc3c504f9b9b5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;signal_sizes&lt;/strong&gt; (list or &lt;code&gt;torch.Size&lt;/code&gt;, optional) &amp;ndash; the size of the original signal (without batch dimension). Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;signal_sizes&lt;/strong&gt;（list或 &lt;code&gt;torch.Size&lt;/code&gt; ，可选）&amp;ndash;原始信号的大小（无批处理尺寸）。默认值： &lt;code&gt;None&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="50e5115f0007f7707ab429cdfe68e39549183aab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; &amp;ndash; amount of neighbouring channels used for normalization</source>
          <target state="translated">&lt;strong&gt;大小&lt;/strong&gt;&amp;ndash;用于标准化的相邻信道的数量</target>
        </trans-unit>
        <trans-unit id="8fa4eb29dfbddf8f29442f7879d71c63e1cbeecf" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; number of elements in the storage</source>
          <target state="translated">&lt;strong&gt;size&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;存储中的元素数量</target>
        </trans-unit>
        <trans-unit id="275c9f306dba8511bb3cfa93aa9598de38ef32ca" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the size of each slice that is unfolded</source>
          <target state="translated">&lt;strong&gt;size&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;展开的每个切片的大小</target>
        </trans-unit>
        <trans-unit id="0b59659707a6fdfb9c664448cffe7da20582f359" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;] or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; output spatia size.</source>
          <target state="translated">&lt;strong&gt;大小&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;]或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;输出空间大小。</target>
        </trans-unit>
        <trans-unit id="cdf514b8ff20b8c4739f70c9f9e484a81b944c49" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;] or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; output spatial size.</source>
          <target state="translated">&lt;strong&gt;size&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;]或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;输出空间大小。</target>
        </trans-unit>
        <trans-unit id="149915349ce02ffdf5ceea93fe5bb247fae568e2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; output spatial size.</source>
          <target state="translated">&lt;strong&gt;size&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;输出空间大小。</target>
        </trans-unit>
        <trans-unit id="16964d2ffbdb9acd4dd1156889d58d0dbd349b50" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; output spatial sizes</source>
          <target state="translated">&lt;strong&gt;size&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;] &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;输出空间大小</target>
        </trans-unit>
        <trans-unit id="ec88c2bc82655addd9428024abd5b8f71a7f9cae" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;] or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;] or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; output spatial size.</source>
          <target state="translated">&lt;strong&gt;size&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;]或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;]或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;输出空间大小。</target>
        </trans-unit>
        <trans-unit id="efc98ce302823923f910c2ca140408828b2cf385" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;] or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;] or &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; output spatial sizes</source>
          <target state="translated">&lt;strong&gt;size&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;]或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;]或&lt;/em&gt;&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;] &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;输出空间大小</target>
        </trans-unit>
        <trans-unit id="0f7b874897c486cf1ce74a993c5f615d99fa025b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; a tuple defining the shape of the output tensor.</source>
          <target state="translated">&lt;strong&gt;size&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;）&amp;ndash;定义输出张量形状的元组。</target>
        </trans-unit>
        <trans-unit id="03a0a29e015390c8b6d2b6595aefea70b182be7a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;ints&lt;/em&gt;) &amp;ndash; the shape of the output tensor</source>
          <target state="translated">&lt;strong&gt;大小&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;元组&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;整数&lt;/em&gt;）&amp;ndash;输出张量的形状</target>
        </trans-unit>
        <trans-unit id="26a6ff6e605560d0327d5d701a4a8866e0a1e65d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; (&lt;em&gt;int...&lt;/em&gt;) &amp;ndash; a list, tuple, or &lt;code&gt;torch.Size&lt;/code&gt; of integers defining the shape of the output tensor.</source>
          <target state="translated">&lt;strong&gt;size&lt;/strong&gt;（&lt;em&gt;int ...&lt;/em&gt;）&amp;ndash;列表，元组或 &lt;code&gt;torch.Size&lt;/code&gt; 。定义输出张量形状的整数的大小。</target>
        </trans-unit>
        <trans-unit id="1a4f563f5d02f780ce83e0cc48b42515c2124c45" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; (&lt;em&gt;int...&lt;/em&gt;) &amp;ndash; a sequence of integers defining the shape of the output tensor.</source>
          <target state="translated">&lt;strong&gt;size&lt;/strong&gt;（&lt;em&gt;int ...&lt;/em&gt;）&amp;ndash;定义输出张量形状的整数序列。</target>
        </trans-unit>
        <trans-unit id="6f9a10015b06b51485833aee1a18eb0d6ca9ec1b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; (&lt;em&gt;int...&lt;/em&gt;) &amp;ndash; a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</source>
          <target state="translated">&lt;strong&gt;size&lt;/strong&gt;（&lt;em&gt;int ...&lt;/em&gt;）&amp;ndash;定义输出张量形状的整数序列。可以是可变数量的参数，也可以是列表或元组之类的集合。</target>
        </trans-unit>
        <trans-unit id="94baf289fa3f083898ae05024e21746311170982" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; (&lt;em&gt;torch.Size&lt;/em&gt;) &amp;ndash; the target output image size. (</source>
          <target state="translated">&lt;strong&gt;size&lt;/strong&gt;（&lt;em&gt;torch.Size&lt;/em&gt;）&amp;ndash;目标输出图像的大小。（</target>
        </trans-unit>
        <trans-unit id="f072e677d05eac98b80bb810fa352b0b054f140e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; (&lt;em&gt;torch.Size&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the desired size. Defaults to the size of the source.</source>
          <target state="translated">&lt;strong&gt;size&lt;/strong&gt;（&lt;em&gt;torch.Size &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;所需的大小。默认为源大小。</target>
        </trans-unit>
        <trans-unit id="3fc5235f279542b6d70b7bd49613ee59572fce51" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; (&lt;em&gt;tuple of python:ints&lt;/em&gt;) &amp;ndash; the shape of the output tensor</source>
          <target state="translated">&lt;strong&gt;size&lt;/strong&gt;（&lt;em&gt;python：ints的元组&lt;/em&gt;）&amp;ndash;输出张量的形状</target>
        </trans-unit>
        <trans-unit id="bd9344ee97a013a9847fea7b4e8e18307f4e8e61" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size&lt;/strong&gt; (list, tuple, or &lt;code&gt;torch.Size&lt;/code&gt;, optional) &amp;ndash; Size of the sparse tensor. If not provided the size will be inferred as the minimum size big enough to hold all non-zero elements.</source>
          <target state="translated">&lt;strong&gt;size&lt;/strong&gt;（列表，元组或 &lt;code&gt;torch.Size&lt;/code&gt; ，可选）&amp;ndash;稀疏张量的大小。如果未提供，则将推断大小为足以容纳所有非零元素的最小大小。</target>
        </trans-unit>
        <trans-unit id="7c9af3450b1512ce0b9ac6b4af383477e20316fa" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size_average&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Deprecated (see &lt;code&gt;reduction&lt;/code&gt;). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field &lt;code&gt;size_average&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt;, the losses are instead summed for each minibatch. Ignored when reduce is &lt;code&gt;False&lt;/code&gt;. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;size_average&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;已弃用（请参见 &lt;code&gt;reduction&lt;/code&gt; ）。默认情况下，损失是批次中每个损失元素的平均数。请注意，对于某些损失，每个样本有多个元素。如果将字段 &lt;code&gt;size_average&lt;/code&gt; 设置为 &lt;code&gt;False&lt;/code&gt; ，则对每个小批量将损失相加。当reduce为 &lt;code&gt;False&lt;/code&gt; 时被忽略。默认值： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a285cc358e12c5f4409aa06b08afb278f4b8c558" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;size_average&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Deprecated (see &lt;code&gt;reduction&lt;/code&gt;). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field &lt;code&gt;size_average&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt;, the losses are instead summed for each minibatch. Ignored when reduce is &lt;code&gt;False&lt;/code&gt;. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;size_average&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;已弃用（请参见 &lt;code&gt;reduction&lt;/code&gt; ）。默认情况下，损失是批次中每个损失元素的平均数。请注意，对于某些损失，每个样本有多个元素。如果将字段 &lt;code&gt;size_average&lt;/code&gt; 设置为 &lt;code&gt;False&lt;/code&gt; ，则对每个小批量将损失相加。当reduce为 &lt;code&gt;False&lt;/code&gt; 时被忽略。默认值： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="5308eb7679205686160791ca8161c5d58dbfe2c3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sizes&lt;/strong&gt; (&lt;em&gt;Union&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;] or &lt;/em&gt;&lt;em&gt;torch.Size&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; New shape of the unflattened dimension</source>
          <target state="translated">&lt;strong&gt;尺寸&lt;/strong&gt;（&lt;em&gt;联盟&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;em&gt;元组&lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;INT &lt;/a&gt;&lt;em&gt;]或&lt;/em&gt;&lt;em&gt;torch.Size &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;元组&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;em&gt;元组&lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;STR &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;INT &lt;/a&gt;&lt;em&gt;] &lt;/em&gt;&lt;em&gt;] &lt;/em&gt;&lt;em&gt;]&lt;/em&gt;） -的不平尺寸的新形状</target>
        </trans-unit>
        <trans-unit id="e1ebc0e50c25e5537cd83715368f39f3ae4ecbfc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sizes&lt;/strong&gt; (&lt;em&gt;torch.Size&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;int...&lt;/em&gt;) &amp;ndash; The number of times to repeat this tensor along each dimension</source>
          <target state="translated">&lt;strong&gt;尺寸&lt;/strong&gt;（&lt;em&gt;torch.Size&lt;/em&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;int ...&lt;/em&gt;）&amp;ndash;沿每个尺寸重复此张量的次数</target>
        </trans-unit>
        <trans-unit id="0a54d8fbd33de9da7dc0fc42b5e2c08cb4d412fd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sizes&lt;/strong&gt; (&lt;em&gt;torch.Size&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;int...&lt;/em&gt;) &amp;ndash; the desired size</source>
          <target state="translated">&lt;strong&gt;尺寸&lt;/strong&gt;（&lt;em&gt;torch.Size&lt;/em&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;int ...&lt;/em&gt;）&amp;ndash;所需的尺寸</target>
        </trans-unit>
        <trans-unit id="3ed38299ca2e29a9d9250d3fd4689c245a1c9056" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;snd_tensor&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; Sound data</source>
          <target state="translated">&lt;strong&gt;snd_tensor&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;）&amp;ndash;声音数据</target>
        </trans-unit>
        <trans-unit id="04df36410cc80bf7688b9b2775248dad717b281c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;solution&lt;/strong&gt; (&lt;em&gt;Tensor&lt;/em&gt;): the least squares solution</source>
          <target state="translated">&lt;strong&gt;解&lt;/strong&gt;（&lt;em&gt;张量&lt;/em&gt;）：最小二乘解</target>
        </trans-unit>
        <trans-unit id="a7ea50e0998478f8190ddf227de9fc8ccad28e5a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;some&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Set to &lt;code&gt;True&lt;/code&gt; for reduced QR decomposition and &lt;code&gt;False&lt;/code&gt; for complete QR decomposition.</source>
          <target state="translated">&lt;strong&gt;some&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;设置为 &lt;code&gt;True&lt;/code&gt; 以减少QR分解，设置为 &lt;code&gt;False&lt;/code&gt; 进行完整的QR分解。</target>
        </trans-unit>
        <trans-unit id="07a0dcb8d5e99634c38c0e2c2522103284a69856" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;some&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; controls the shape of returned &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;some&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;控制返回的 &lt;code&gt;U&lt;/code&gt; 和 &lt;code&gt;V&lt;/code&gt; 的形状</target>
        </trans-unit>
        <trans-unit id="815339fca61a4eec429f6db55252b5c5d2c244f7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sort_by&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Attribute used to sort entries. By default they are printed in the same order as they were registered. Valid keys include: &lt;code&gt;cpu_time&lt;/code&gt;, &lt;code&gt;cuda_time&lt;/code&gt;, &lt;code&gt;cpu_time_total&lt;/code&gt;, &lt;code&gt;cuda_time_total&lt;/code&gt;, &lt;code&gt;cpu_memory_usage&lt;/code&gt;, &lt;code&gt;cuda_memory_usage&lt;/code&gt;, &lt;code&gt;self_cpu_memory_usage&lt;/code&gt;, &lt;code&gt;self_cuda_memory_usage&lt;/code&gt;, &lt;code&gt;count&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;sort_by&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;用于对条目进行排序的属性。默认情况下，它们以与注册时相同的顺序打印。有效键包括： &lt;code&gt;cpu_time&lt;/code&gt; ， &lt;code&gt;cuda_time&lt;/code&gt; ， &lt;code&gt;cpu_time_total&lt;/code&gt; ， &lt;code&gt;cuda_time_total&lt;/code&gt; ， &lt;code&gt;cpu_memory_usage&lt;/code&gt; ， &lt;code&gt;cuda_memory_usage&lt;/code&gt; ， &lt;code&gt;self_cpu_memory_usage&lt;/code&gt; ， &lt;code&gt;self_cuda_memory_usage&lt;/code&gt; ， &lt;code&gt;count&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c964615183041f78000d335f6006659de8eaa42a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sorted&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Whether to sort the unique elements in ascending order before returning as output.</source>
          <target state="translated">&lt;strong&gt;sorted&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;在返回为输出之前是否按升序对唯一元素进行排序。</target>
        </trans-unit>
        <trans-unit id="d9cd0c6e3bf7a9c26be9ab6eb578de1df1d27078" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sorted&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; controls whether to return the elements in sorted order</source>
          <target state="translated">&lt;strong&gt;sorted&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;控制是否按排序顺序返回元素</target>
        </trans-unit>
        <trans-unit id="ba3bc3fc862fb1d978c7e8f26f45ac4560b6cbb6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sorted_sequence&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; N-D or 1-D tensor, containing monotonically increasing sequence on the &lt;em&gt;innermost&lt;/em&gt; dimension.</source>
          <target state="translated">&lt;strong&gt;sorted_sequence&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash; ND或1-D张量，在&lt;em&gt;最里面的&lt;/em&gt;维度上包含单调递增的序列。</target>
        </trans-unit>
        <trans-unit id="ff092f5a8d7513d4041f8ec05bf6db9a6433a55a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;source&lt;/strong&gt; (&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to copy from</source>
          <target state="translated">&lt;strong&gt;source&lt;/strong&gt;（&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;从中复制的张量</target>
        </trans-unit>
        <trans-unit id="d3e050c1ccd84c4a56619568303f30198e45f1ff" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;source&lt;/strong&gt; (&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Storage&lt;/em&gt;) &amp;ndash; the tensor or storage to use</source>
          <target state="translated">&lt;strong&gt;源&lt;/strong&gt;（&lt;a href=&quot;#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;存储&lt;/em&gt;）&amp;ndash;要使用的张量或存储</target>
        </trans-unit>
        <trans-unit id="537a3683bbbd7cb37d1a1e68afc20c93f4950810" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;source&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tuple of python:ints&lt;/em&gt;) &amp;ndash; Original positions of the dims to move. These must be unique.</source>
          <target state="translated">&lt;strong&gt;source&lt;/strong&gt;（&lt;em&gt;python：ints的&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;元组&lt;/em&gt;）&amp;ndash;要移动的暗点的原始位置。这些必须是唯一的。</target>
        </trans-unit>
        <trans-unit id="736b5df270b468d2d70fee90f0c6304038935e20" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;source&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; &lt;code&gt;'github'&lt;/code&gt; | &lt;code&gt;'local'&lt;/code&gt;. Specifies how &lt;code&gt;repo_or_dir&lt;/code&gt; is to be interpreted. Default is &lt;code&gt;'github'&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;源&lt;/strong&gt;（&lt;em&gt;字符串&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash; &lt;code&gt;'github'&lt;/code&gt; | &lt;code&gt;'local'&lt;/code&gt; 。指定如何解释 &lt;code&gt;repo_or_dir&lt;/code&gt; 。默认是 &lt;code&gt;'github'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7e187210f431543be9a1749f439d018aa4b11087" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sources&lt;/strong&gt; &amp;ndash; A list of relative or absolute paths to C++ source files.</source>
          <target state="translated">&lt;strong&gt;源&lt;/strong&gt;&amp;ndash; C ++源文件的相对或绝对路径的列表。</target>
        </trans-unit>
        <trans-unit id="16b83bb63832023cd48fcbf7cf2e3c51644ad4e7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sparse&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, gradient w.r.t. &lt;code&gt;weight&lt;/code&gt; matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.</source>
          <target state="translated">&lt;strong&gt;sparse&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则梯度wrt &lt;code&gt;weight&lt;/code&gt; 矩阵将为稀疏张量。有关稀疏渐变的更多详细信息，请参见注释。</target>
        </trans-unit>
        <trans-unit id="d82071676e26d482c374d70ee42605a5252d8f79" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sparse&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, gradient w.r.t. &lt;code&gt;weight&lt;/code&gt; will be a sparse tensor. See Notes under &lt;a href=&quot;generated/torch.nn.embedding#torch.nn.Embedding&quot;&gt;&lt;code&gt;torch.nn.Embedding&lt;/code&gt;&lt;/a&gt; for more details regarding sparse gradients.</source>
          <target state="translated">&lt;strong&gt;sparse&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则渐变 &lt;code&gt;weight&lt;/code&gt; 将为稀疏张量。有关稀疏渐变的更多详细信息，请参见&lt;a href=&quot;generated/torch.nn.embedding#torch.nn.Embedding&quot;&gt; &lt;code&gt;torch.nn.Embedding&lt;/code&gt; &lt;/a&gt;下的注释。</target>
        </trans-unit>
        <trans-unit id="490e7f9dc42b376ca2804c6155a64ad37564fb2c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sparse&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; See module initialization documentation.</source>
          <target state="translated">&lt;strong&gt;sparse&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;请参阅模块初始化文档。</target>
        </trans-unit>
        <trans-unit id="ef9a73bc582ac8360689c39a1c63c78ddde0dd12" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sparse&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; See module initialization documentation. Default: &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;sparse&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;请参阅模块初始化文档。默认值： &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ac1945361265423208febe431a5add209b0405e6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sparse&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, gradient w.r.t. &lt;code&gt;weight&lt;/code&gt; matrix will be a sparse tensor. See Notes for more details regarding sparse gradients. Note: this option is not supported when &lt;code&gt;mode=&quot;max&quot;&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;sparse&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则梯度wrt &lt;code&gt;weight&lt;/code&gt; 矩阵将为稀疏张量。有关稀疏渐变的更多详细信息，请参见注释。注意：当 &lt;code&gt;mode=&quot;max&quot;&lt;/code&gt; 时不支持此选项。</target>
        </trans-unit>
        <trans-unit id="264742d3a555bb6302cc2b99bcbdbaea90e73571" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sparse&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if &lt;code&gt;True&lt;/code&gt;, gradient w.r.t. &lt;code&gt;weight&lt;/code&gt; will be a sparse tensor. See Notes under &lt;a href=&quot;generated/torch.nn.embedding#torch.nn.Embedding&quot;&gt;&lt;code&gt;torch.nn.Embedding&lt;/code&gt;&lt;/a&gt; for more details regarding sparse gradients. Note: this option is not supported when &lt;code&gt;mode=&quot;max&quot;&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;sparse&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则渐变 &lt;code&gt;weight&lt;/code&gt; 将为稀疏张量。有关稀疏渐变的更多详细信息，请参见&lt;a href=&quot;generated/torch.nn.embedding#torch.nn.Embedding&quot;&gt; &lt;code&gt;torch.nn.Embedding&lt;/code&gt; &lt;/a&gt;下的注释。注意：当 &lt;code&gt;mode=&quot;max&quot;&lt;/code&gt; 时不支持此选项。</target>
        </trans-unit>
        <trans-unit id="94981b1994886031e5de3502e8d05e4753cd4159" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sparseDims&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the number of sparse dimensions to include in the new sparse tensor</source>
          <target state="translated">&lt;strong&gt;sparseDims&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;要包括在新的稀疏张量中的稀疏维数</target>
        </trans-unit>
        <trans-unit id="4b18e538096fb082b8177bc1560caf9a9d7bc9a7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sparse_grad&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, gradient w.r.t. &lt;code&gt;input&lt;/code&gt; will be a sparse tensor.</source>
          <target state="translated">&lt;strong&gt;sparse_grad&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则梯度wrt &lt;code&gt;input&lt;/code&gt; 将为稀疏张量。</target>
        </trans-unit>
        <trans-unit id="b37daeb607924ba93a0bda59b58e026064bd654d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;sparsity&lt;/strong&gt; &amp;ndash; The fraction of elements in each column to be set to zero</source>
          <target state="translated">&lt;strong&gt;稀疏度&lt;/strong&gt;&amp;ndash;将每一列中的元素比例设置为零</target>
        </trans-unit>
        <trans-unit id="e30145ed58f140ce4c89e2f335125ba650c4f507" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;split_size_or_sections&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;) or &lt;/em&gt;&lt;em&gt;(&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;&lt;em&gt;(&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;)&lt;/em&gt;) &amp;ndash; size of a single chunk or list of sizes for each chunk</source>
          <target state="translated">&lt;strong&gt;split_size_or_sections&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;）或&lt;/em&gt;&lt;em&gt;（&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list &lt;/a&gt;&lt;em&gt;（&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;）&lt;/em&gt;）&amp;ndash;单个块的大小或每个块的大小列表</target>
        </trans-unit>
        <trans-unit id="9b97dbfc339fde5bc389bd8ab93d7f7cb7c44dfc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;src&lt;/strong&gt; &amp;ndash; the sequence to the encoder (required).</source>
          <target state="translated">&lt;strong&gt;src&lt;/strong&gt; &amp;ndash;编码器的顺序（必需）。</target>
        </trans-unit>
        <trans-unit id="78fa5919448d76e6a64f4bd45287924a3a34df33" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;src&lt;/strong&gt; &amp;ndash; the sequence to the encoder layer (required).</source>
          <target state="translated">&lt;strong&gt;src&lt;/strong&gt; &amp;ndash;编码器层的序列（必需）。</target>
        </trans-unit>
        <trans-unit id="3cbdd89942669cde945cfe86a26dcfcd5645519c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;src&lt;/strong&gt; (&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the source element(s) to scatter, incase &lt;code&gt;value&lt;/code&gt; is not specified</source>
          <target state="translated">&lt;strong&gt;src&lt;/strong&gt;（&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;要分散的源元素，如果未指定 &lt;code&gt;value&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c00ac345411ae3480e897e5cbdd023e0d52ba5ba" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;src&lt;/strong&gt; (&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the source elements to scatter and add</source>
          <target state="translated">&lt;strong&gt;src&lt;/strong&gt;（&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;分散和添加的源元素</target>
        </trans-unit>
        <trans-unit id="cca60735cb66c0fd24e9daf6bd9a2d6affdd8b18" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;src&lt;/strong&gt; (&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the source tensor to copy from</source>
          <target state="translated">&lt;strong&gt;src&lt;/strong&gt;（&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;要复制的源张量</target>
        </trans-unit>
        <trans-unit id="2605c126b88196f5cf7d630fe3b356d67096670b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;src&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Source rank (default is 0)</source>
          <target state="translated">&lt;strong&gt;src&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;源排名（默认为0）</target>
        </trans-unit>
        <trans-unit id="5422756d7624b629fb6d789ff63552dac7a631ab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;src&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Source rank.</source>
          <target state="translated">&lt;strong&gt;src&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;源排名。</target>
        </trans-unit>
        <trans-unit id="6ecfc113b4210a133571a1ba6eee75db6462f3ac" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;src&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Source rank. Will receive from any process if unspecified.</source>
          <target state="translated">&lt;strong&gt;src&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;源排名。如果未指定，将从任何进程中接收。</target>
        </trans-unit>
        <trans-unit id="1bf7cb7495026465a5e025f645be215970d3c3f0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;src_key_padding_mask&lt;/strong&gt; &amp;ndash; the ByteTensor mask for src keys per batch (optional).</source>
          <target state="translated">&lt;strong&gt;src_key_padding_mask&lt;/strong&gt; &amp;ndash;每批src密钥的ByteTensor掩码（可选）。</target>
        </trans-unit>
        <trans-unit id="20d1aac54a28167b80b444f3e283804c33610581" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;src_key_padding_mask&lt;/strong&gt; &amp;ndash; the mask for the src keys per batch (optional).</source>
          <target state="translated">&lt;strong&gt;src_key_padding_mask&lt;/strong&gt; &amp;ndash;每批src密钥的掩码（可选）。</target>
        </trans-unit>
        <trans-unit id="2e4d7f0b7f046f61667e190129080811355c102a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;src_mask&lt;/strong&gt; &amp;ndash; the additive mask for the src sequence (optional).</source>
          <target state="translated">&lt;strong&gt;src_mask&lt;/strong&gt; &amp;ndash; src序列的附加掩码（可选）。</target>
        </trans-unit>
        <trans-unit id="b8ff899002109344c4f233e16bca9c3f67cf5740" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;src_mask&lt;/strong&gt; &amp;ndash; the mask for the src sequence (optional).</source>
          <target state="translated">&lt;strong&gt;src_mask&lt;/strong&gt; &amp;ndash; src序列的掩码（可选）。</target>
        </trans-unit>
        <trans-unit id="480ed9f7356f0f1438f0c3e67548752e2e88b19d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;src_tensor&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Source tensor rank within &lt;code&gt;tensor_list&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;src_tensor&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash; &lt;code&gt;tensor_list&lt;/code&gt; 中的源张量等级</target>
        </trans-unit>
        <trans-unit id="6c54e581b4a64152f841eae4b7132eca48a8ce38" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;start&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the starting value for the set of points</source>
          <target state="translated">&lt;strong&gt;start&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;点集的起始值</target>
        </trans-unit>
        <trans-unit id="fd9b8604b8d10a61d30a016be638c8f89e228f17" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;start&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the starting value for the set of points. Default: &lt;code&gt;0&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;start&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;点集的起始值。默认： &lt;code&gt;0&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="61cea1b64e6864ce3fd4c5cdb9ba97526597a0e9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;start&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the starting dimension</source>
          <target state="translated">&lt;strong&gt;start&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;起始尺寸</target>
        </trans-unit>
        <trans-unit id="017f9db566260fcc32ff561f6f41f549f1ab1b1b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;start&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; the starting value for the set of points. Default: &lt;code&gt;0&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;start&lt;/strong&gt;（&lt;em&gt;Number&lt;/em&gt;）&amp;ndash;点集的起始值。默认： &lt;code&gt;0&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="db305563060fce9be4b380f28e1127159859b2ac" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;start_dim&lt;/strong&gt; &amp;ndash; first dim to flatten (default = 1).</source>
          <target state="translated">&lt;strong&gt;start_dim&lt;/strong&gt; &amp;ndash;首先变暗以展平（默认= 1）。</target>
        </trans-unit>
        <trans-unit id="1aa7a3bfd2813f5dbf3d94b9c4043556308d2159" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;start_dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the first dim to flatten</source>
          <target state="translated">&lt;strong&gt;start_dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;第一个变暗的像素</target>
        </trans-unit>
        <trans-unit id="13b544bfd8927b59e44add059240511dff37c576" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;start_method&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; (deprecated) this method will always use &lt;code&gt;spawn&lt;/code&gt; as the start method. To use a different start method use &lt;code&gt;start_processes()&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;start_method&lt;/strong&gt;（&lt;em&gt;string&lt;/em&gt;）&amp;ndash;（不建议使用）此方法将始终使用 &lt;code&gt;spawn&lt;/code&gt; 作为start方法。要使用其他启动方法，请使用 &lt;code&gt;start_processes()&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e706afdee5bbb0a79b5a2f832271401d69b23519" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;state_dict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;) &amp;ndash; a dict containing parameters and persistent buffers.</source>
          <target state="translated">&lt;strong&gt;state_dict&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;）&amp;ndash;包含参数和持久缓冲区的字典。</target>
        </trans-unit>
        <trans-unit id="6d5556d76adbdb45cbe68476db8185b5492374d9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;state_dict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;) &amp;ndash; optimizer state. Should be an object returned from a call to &lt;a href=&quot;#torch.optim.Optimizer.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;strong&gt;state_dict&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;）&amp;ndash;优化器状态。应该是从调用&lt;a href=&quot;#torch.optim.Optimizer.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt;返回的对象。</target>
        </trans-unit>
        <trans-unit id="5cbbb29e1fe7881fd5844bacc7ca16ca6b9024eb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;state_dict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;) &amp;ndash; scaler state. Should be an object returned from a call to &lt;a href=&quot;#torch.cuda.amp.GradScaler.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;strong&gt;state_dict&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;）&amp;ndash;定标器状态。应该是从调用&lt;a href=&quot;#torch.cuda.amp.GradScaler.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt;返回的对象。</target>
        </trans-unit>
        <trans-unit id="66750c76d7c187af01e3a824d960ff4b84685f61" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;state_dict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;) &amp;ndash; scheduler state. Should be an object returned from a call to &lt;a href=&quot;#torch.optim.lr_scheduler.LambdaLR.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;strong&gt;state_dict&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;）&amp;ndash;调度程序状态。应该是从调用&lt;a href=&quot;#torch.optim.lr_scheduler.LambdaLR.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt;返回的对象。</target>
        </trans-unit>
        <trans-unit id="b56f7ed2e459a991898c4628d117292ad6733aef" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;state_dict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;) &amp;ndash; scheduler state. Should be an object returned from a call to &lt;a href=&quot;#torch.optim.lr_scheduler.MultiplicativeLR.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;strong&gt;state_dict&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;）&amp;ndash;调度程序状态。应该是从调用&lt;a href=&quot;#torch.optim.lr_scheduler.MultiplicativeLR.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt;返回的对象。</target>
        </trans-unit>
        <trans-unit id="397f739ec24999cef4aedd3f271bf305d11784f1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;std&lt;/strong&gt; &amp;ndash; the standard deviation of the normal distribution</source>
          <target state="translated">&lt;strong&gt;std&lt;/strong&gt; &amp;ndash;正态分布的标准偏差</target>
        </trans-unit>
        <trans-unit id="19af3df9428cf525a7500c9af96c43b782849a34" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;std&lt;/strong&gt; &amp;ndash; the standard deviation of the normal distribution used to generate the non-zero values</source>
          <target state="translated">&lt;strong&gt;std&lt;/strong&gt; &amp;ndash;用于生成非零值的正态分布的标准偏差</target>
        </trans-unit>
        <trans-unit id="8fd58ae2ac219c0c179d6b0c321bc9877f3d5c08" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;std&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor of per-element standard deviations</source>
          <target state="translated">&lt;strong&gt;std&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;每个元素的标准偏差的张量</target>
        </trans-unit>
        <trans-unit id="cc539f1fbf4c4bdbab694cd3aa4e1e85f66c6288" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;std&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the standard deviation for all distributions</source>
          <target state="translated">&lt;strong&gt;std&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;所有分布的标准偏差</target>
        </trans-unit>
        <trans-unit id="5c558f61587c7e53fc6d918f3a17c5e6dd6e0602" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;std&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the standard deviation for all distributions</source>
          <target state="translated">&lt;strong&gt;std&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;所有发行版的标准偏差</target>
        </trans-unit>
        <trans-unit id="baf9f5309ab39bb6c1581c5f3b05b5bdbedb7c06" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;step&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the gap between each pair of adjacent points. Default: &lt;code&gt;1&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;步长&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;每对相邻点之间的间隙。默认值： &lt;code&gt;1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1abac8c895387b0b9cd2795f79c2431234a95f31" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;step&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the step between each slice</source>
          <target state="translated">&lt;strong&gt;step&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;每个切片之间的步骤</target>
        </trans-unit>
        <trans-unit id="55d5b2b9ad99b0efd35850f714bb971bc246e22f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;step&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; the gap between each pair of adjacent points. Default: &lt;code&gt;1&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;step&lt;/strong&gt;（&lt;em&gt;Number&lt;/em&gt;）&amp;ndash;每对相邻点之间的间隙。默认值： &lt;code&gt;1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f83940981c3f69f7e6548653f4f568bae12ff7dc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;step_size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Period of learning rate decay.</source>
          <target state="translated">&lt;strong&gt;step_size&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;学习率衰减的时间段。</target>
        </trans-unit>
        <trans-unit id="f24616594618270e9b38c9591ee4052979020811" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;step_size_down&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Number of training iterations in the decreasing half of a cycle. If step_size_down is None, it is set to step_size_up. Default: None</source>
          <target state="translated">&lt;strong&gt;step_size_down&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;循环的递减一半中的训练迭代次数。如果step_size_down为None，则将其设置为step_size_up。默认值：无</target>
        </trans-unit>
        <trans-unit id="a4590b69559f14282bc9131a5417b5b46f2b757a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;step_size_up&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Number of training iterations in the increasing half of a cycle. Default: 2000</source>
          <target state="translated">&lt;strong&gt;step_size_up&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;循环的一半增加中的训练迭代次数。默认值：2000</target>
        </trans-unit>
        <trans-unit id="1c7814a208624eebc76b6a820bea42f2dd649e23" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;step_sizes&lt;/strong&gt; (&lt;em&gt;Tuple&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a pair of minimal and maximal allowed step sizes (default: (1e-6, 50))</source>
          <target state="translated">&lt;strong&gt;step_sizes&lt;/strong&gt;（&lt;em&gt;Tuple &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;] &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;一对最小和最大允许步长（默认值：（1e- &lt;strong&gt;6，50&lt;/strong&gt;））</target>
        </trans-unit>
        <trans-unit id="a166db3ce9222d4bcbe13cb6bde4fbbfc394f117" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;steps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; size of the constructed tensor</source>
          <target state="translated">&lt;strong&gt;steps&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;构造张量的大小</target>
        </trans-unit>
        <trans-unit id="bb9aa8a045e47f15c5cda85f9343f2cff9c54503" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;steps_per_epoch&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The number of steps per epoch to train for. This is used along with epochs in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Default: None</source>
          <target state="translated">&lt;strong&gt;steps_per_epoch&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;每个纪元要训练的步数。如果未提供total_steps的值，则将其与历元一起使用以推断循环中的总步数。默认值：无</target>
        </trans-unit>
        <trans-unit id="2401eb00fd1cd4eb0e88575af75256fc34e93df2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;storage_offset&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the offset in the storage</source>
          <target state="translated">&lt;strong&gt;storage_offset&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;存储中的偏移量</target>
        </trans-unit>
        <trans-unit id="a464ae09605bcd29232df8491b551330fa7b201e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;storage_offset&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the offset in the underlying storage of the output tensor</source>
          <target state="translated">&lt;strong&gt;storage_offset&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;输出张量在基础存储中的偏移量</target>
        </trans-unit>
        <trans-unit id="36921320f19d087af835850826666bf4b078eaa7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;store&lt;/strong&gt; (&lt;a href=&quot;#torch.distributed.Store&quot;&gt;Store&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Key/value store accessible to all workers, used to exchange connection/address information. Mutually exclusive with &lt;code&gt;init_method&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;store&lt;/strong&gt;（&lt;a href=&quot;#torch.distributed.Store&quot;&gt;商店&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;所有工作人员均可访问的键/值存储，用于交换连接/地址信息。与 &lt;code&gt;init_method&lt;/code&gt; 互斥。</target>
        </trans-unit>
        <trans-unit id="4dc03df96ec79acaac3452a59e6207bd786c9d6a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;store&lt;/strong&gt; (&lt;em&gt;torch.distributed.store&lt;/em&gt;) &amp;ndash; A store object that forms the underlying key-value store.</source>
          <target state="translated">&lt;strong&gt;store&lt;/strong&gt;（&lt;em&gt;torch.distributed.store&lt;/em&gt;）&amp;ndash;构成基础键值存储的存储对象。</target>
        </trans-unit>
        <trans-unit id="3808a536d61ce3eeaf910236762d7fc972e3843c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stream&lt;/strong&gt; (&lt;a href=&quot;#torch.cuda.Stream&quot;&gt;Stream&lt;/a&gt;) &amp;ndash; a stream to synchronize.</source>
          <target state="translated">&lt;strong&gt;stream&lt;/strong&gt;（&lt;a href=&quot;#torch.cuda.Stream&quot;&gt;Stream&lt;/a&gt;）&amp;ndash;要同步的流。</target>
        </trans-unit>
        <trans-unit id="44c2c07b80e8e091417520bc0794619cec3517a3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stream&lt;/strong&gt; (&lt;a href=&quot;#torch.cuda.Stream&quot;&gt;Stream&lt;/a&gt;) &amp;ndash; selected stream. This manager is a no-op if it&amp;rsquo;s &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;stream&lt;/strong&gt;（&lt;a href=&quot;#torch.cuda.Stream&quot;&gt;Stream&lt;/a&gt;）&amp;ndash;选定的流。如果为 &lt;code&gt;None&lt;/code&gt; ,则此经理为空手。</target>
        </trans-unit>
        <trans-unit id="0d198b00164cdb2d75aad580963cb14be1623fbe" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;streams&lt;/strong&gt; (&lt;em&gt;Iterable&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;#torch.cuda.Stream&quot;&gt;Stream&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; an iterable of Streams, among which to execute the scatter. If not specified, the default stream will be utilized.</source>
          <target state="translated">&lt;strong&gt;streams&lt;/strong&gt;（&lt;em&gt;Iterable &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;#torch.cuda.Stream&quot;&gt;Stream &lt;/a&gt;&lt;em&gt;] &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash; Streams的可迭代项，在其中执行分散。如果未指定，将使用默认流。</target>
        </trans-unit>
        <trans-unit id="15a9a33618c8145e65db0df141897cb687b7f41b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;strict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If &lt;code&gt;False&lt;/code&gt;, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value. Defaults to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;严格&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则当我们检测到存在一个输入以使所有输出独立于该输入时，将引发错误。如果为 &lt;code&gt;False&lt;/code&gt; ，则返回零张量作为所述输入的粗麻布，这是预期的数学值。默认为 &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="521e1bbc2df156a33be4ed0a762f4b6dc1c8ad68" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;strict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If &lt;code&gt;False&lt;/code&gt;, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value. Defaults to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;严格&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则当我们检测到存在一个输入以使所有输出独立于该输入时，将引发错误。如果为 &lt;code&gt;False&lt;/code&gt; ，则返回零张量作为上述输入的hvp，这是预期的数学值。默认为 &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c862109158a12033724c85557e4c10782bcd3e2a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;strict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If &lt;code&gt;False&lt;/code&gt;, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value. Defaults to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;严格&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则当我们检测到存在一个输入以使所有输出独立于该输入时，将引发错误。如果为 &lt;code&gt;False&lt;/code&gt; ，则返回零张量作为上述输入的雅可比，这是期望的数学值。默认为 &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d3196a755940ba28a032a950660ced9d1132f5ff" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;strict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If &lt;code&gt;False&lt;/code&gt;, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value. Defaults to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;严格&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则当我们检测到存在一个输入以使所有输出独立于该输入时，将引发错误。如果为 &lt;code&gt;False&lt;/code&gt; ，则返回零张量作为上述输入的jvp，这是预期的数学值。默认为 &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0cf1dbbdc41dad0369c4ebb8aa27533a686a95a2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;strict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If &lt;code&gt;False&lt;/code&gt;, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value. Defaults to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;严格&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则当我们检测到存在一个输入以使所有输出独立于该输入时，将引发错误。如果为 &lt;code&gt;False&lt;/code&gt; ，则返回零张量作为上述输入的vhp，这是预期的数学值。默认为 &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ddeb9357a7bf045b78eba2cf1739a79dd06a4609" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;strict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If &lt;code&gt;False&lt;/code&gt;, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value. Defaults to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;严格&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则当我们检测到存在一个输入以使所有输出独立于该输入时，将引发错误。如果为 &lt;code&gt;False&lt;/code&gt; ，则返回零张量作为上述输入的vjp，这是期望的数学值。默认为 &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="aef94a04df5ea5876b7e12de6f0824f58976f3e1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;strict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to strictly enforce that the keys in &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;strict&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;是否严格要求&lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;中的键与该模块的&lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt;函数返回的键匹配。默认值： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="33b205deb049448da3c0a8dc2d460e0783955775" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;strict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to strictly enforce that the keys in &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;strict&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;是否严格要求&lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;中的键与该模块的&lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt;函数返回的键匹配。默认值： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="ea9289c76d8286090b8a4f3dd767eca0a4c5e0c2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;strict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to strictly enforce that the keys in &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;strict&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;是否严格要求&lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;中的键与该模块的&lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt;函数返回的键匹配。默认值： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="1544925b842cb8d8b87627d19af858e20b780645" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;strict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to strictly enforce that the keys in &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;strict&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;是否严格要求&lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt;中的键与该模块的&lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt;函数返回的键匹配。默认值： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="685790b38e543d30d0b76c6ee8fcd86be2ab18e1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;strict&lt;/strong&gt; (&lt;code&gt;bool&lt;/code&gt;, optional) &amp;ndash; run the tracer in a strict mode or not (default: &lt;code&gt;True&lt;/code&gt;). Only turn this off when you want the tracer to record your mutable container types (currently &lt;code&gt;list&lt;/code&gt;/&lt;code&gt;dict&lt;/code&gt;) and you are sure that the container you are using in your problem is a &lt;code&gt;constant&lt;/code&gt; structure and does not get used as control flow (if, for) conditions.</source>
          <target state="translated">&lt;strong&gt;严格&lt;/strong&gt;（ &lt;code&gt;bool&lt;/code&gt; ，可选）&amp;ndash;是否在严格模式下运行跟踪器（默认值： &lt;code&gt;True&lt;/code&gt; ）。仅当您希望跟踪器记录可变容器类型（当前为 &lt;code&gt;list&lt;/code&gt; / &lt;code&gt;dict&lt;/code&gt; ）并且您确定问题中使用的容器是 &lt;code&gt;constant&lt;/code&gt; 结构且不会用作控制流时，才将其关闭（如果） 情况。</target>
        </trans-unit>
        <trans-unit id="d42c648d3610293a7cf248220e9664c5134177bd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; &amp;ndash; The stride of the sliding window, must be &amp;gt; 0. Default value is &lt;code&gt;kernel_size&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;stride&lt;/strong&gt; &amp;ndash;滑动窗口的步幅，必须大于0。默认值为 &lt;code&gt;kernel_size&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ece50bfaa1c42101ecdb71c0cdd588d8acf212c1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; &amp;ndash; a single int, the stride of the window. Default value is &lt;code&gt;kernel_size&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;跨度&lt;/strong&gt;&amp;ndash;一个int，即窗口的跨度。默认值为 &lt;code&gt;kernel_size&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="3d66128b0d5b6f8a04eda5e5cafb00185fe896d9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; &amp;ndash; stride of the pooling operation. Can be a single number or a tuple &lt;code&gt;(sH, sW)&lt;/code&gt;. Default: &lt;code&gt;kernel_size&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;跨步&lt;/strong&gt;&amp;ndash;合并操作的跨步。可以是单个数字或元组 &lt;code&gt;(sH, sW)&lt;/code&gt; 。默认值： &lt;code&gt;kernel_size&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="214b0bd7f37429dbe8ebc6dded03c3882c664098" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; &amp;ndash; stride of the pooling operation. Can be a single number or a tuple &lt;code&gt;(sT, sH, sW)&lt;/code&gt;. Default: &lt;code&gt;kernel_size&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;跨步&lt;/strong&gt;&amp;ndash;合并操作的跨步。可以是单个数字或元组 &lt;code&gt;(sT, sH, sW)&lt;/code&gt; 。默认值： &lt;code&gt;kernel_size&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c0738f57cba9ff50e90a91a07935ba7bfbd70a60" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; &amp;ndash; the stride of the convolving kernel. Can be a single number or a one-element tuple &lt;code&gt;(sW,)&lt;/code&gt;. Default: 1</source>
          <target state="translated">&lt;strong&gt;步幅&lt;/strong&gt;&amp;ndash;不断发展的内核步幅。可以是一个整数或一个元素的元组 &lt;code&gt;(sW,)&lt;/code&gt; 。默认值：1</target>
        </trans-unit>
        <trans-unit id="a8bfc0051811863634e34e362ca2dc7ac04c0a76" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; &amp;ndash; the stride of the convolving kernel. Can be a single number or a tuple &lt;code&gt;(sD, sH, sW)&lt;/code&gt;. Default: 1</source>
          <target state="translated">&lt;strong&gt;步幅&lt;/strong&gt;&amp;ndash;不断发展的内核步幅。可以是单个数字或元组 &lt;code&gt;(sD, sH, sW)&lt;/code&gt; 。默认值：1</target>
        </trans-unit>
        <trans-unit id="e99fe233d3379e77fe12415abc7f2b4a4414ea6b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; &amp;ndash; the stride of the convolving kernel. Can be a single number or a tuple &lt;code&gt;(sH, sW)&lt;/code&gt;. Default: 1</source>
          <target state="translated">&lt;strong&gt;步幅&lt;/strong&gt;&amp;ndash;不断发展的内核步幅。可以是单个数字或元组 &lt;code&gt;(sH, sW)&lt;/code&gt; 。默认值：1</target>
        </trans-unit>
        <trans-unit id="dc03b1c42fd2881680cf96ca6f9def163d1c788c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; &amp;ndash; the stride of the convolving kernel. Can be a single number or a tuple &lt;code&gt;(sT, sH, sW)&lt;/code&gt;. Default: 1</source>
          <target state="translated">&lt;strong&gt;步幅&lt;/strong&gt;&amp;ndash;不断发展的内核步幅。可以是单个数字或元组 &lt;code&gt;(sT, sH, sW)&lt;/code&gt; 。默认值：1</target>
        </trans-unit>
        <trans-unit id="1e07709d5ef30fc3327e2bffbfec50c789ae30d7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; &amp;ndash; the stride of the convolving kernel. Can be a single number or a tuple &lt;code&gt;(sW,)&lt;/code&gt;. Default: 1</source>
          <target state="translated">&lt;strong&gt;步幅&lt;/strong&gt;&amp;ndash;不断发展的内核步幅。可以是单个数字或元组 &lt;code&gt;(sW,)&lt;/code&gt; 。默认值：1</target>
        </trans-unit>
        <trans-unit id="6a3d3677bf8aef87647950a0175bf46a290a6881" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; &amp;ndash; the stride of the window. Can be a single number or a tuple &lt;code&gt;(sW,)&lt;/code&gt;. Default: &lt;code&gt;kernel_size&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;步幅&lt;/strong&gt;&amp;ndash;窗口的步幅。可以是单个数字或元组 &lt;code&gt;(sW,)&lt;/code&gt; 。默认值： &lt;code&gt;kernel_size&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="4d1ac3de99275d24a51b19deff5f49a4c626918a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; &amp;ndash; the stride of the window. Default value is &lt;code&gt;kernel_size&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;步幅&lt;/strong&gt;&amp;ndash;窗口的步幅。默认值为 &lt;code&gt;kernel_size&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="0e83b71423e0fab8d3fec82e9b62f9e213048e07" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; Stride of the max pooling window. It is set to &lt;code&gt;kernel_size&lt;/code&gt; by default.</source>
          <target state="translated">&lt;strong&gt;步幅&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;整数&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;元组&lt;/a&gt;）&amp;ndash;最大池窗口的步幅。默认情况下，它设置为 &lt;code&gt;kernel_size&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d642049bdaf0da3e7ac2a9bb747d7157b42b5143" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;) &amp;ndash; the stride of the sliding blocks in the input spatial dimensions. Default: 1</source>
          <target state="translated">&lt;strong&gt;跨度&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;整数&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;元组&lt;/a&gt;）&amp;ndash;滑块在输入空间维度上的跨度。默认值：1</target>
        </trans-unit>
        <trans-unit id="32f6fe46ead4028a3ff5fec9da1140af45a6f195" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Stride of the convolution. Default: 1</source>
          <target state="translated">&lt;strong&gt;步幅&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;卷积的步幅。默认值：1</target>
        </trans-unit>
        <trans-unit id="c28631e28f33721680bff83305d7dc712c049afc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the stride of the sliding blocks in the input spatial dimensions. Default: 1</source>
          <target state="translated">&lt;strong&gt;跨度&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;滑块在输入空间维度上的跨度。默认值：1</target>
        </trans-unit>
        <trans-unit id="5286e3c0bfd8223f9b20165f169fcac7223712b4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;ints&lt;/em&gt;) &amp;ndash; the stride of the output tensor</source>
          <target state="translated">&lt;strong&gt;跨度&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;元组&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;整数&lt;/em&gt;）&amp;ndash;输出张量的跨度</target>
        </trans-unit>
        <trans-unit id="6af3c97a48669b717945a12fdf9a7cc81362788e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;tuple&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the desired stride. Defaults to C-contiguous strides.</source>
          <target state="translated">&lt;strong&gt;步幅&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;元组&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;所需的步幅。默认为C连续跨步。</target>
        </trans-unit>
        <trans-unit id="9ed1797b9bf63c6b5db820aab38cf98e6f4f5b0d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;stride&lt;/strong&gt; (&lt;em&gt;tuple of python:ints&lt;/em&gt;) &amp;ndash; the strides of the output tensor</source>
          <target state="translated">&lt;strong&gt;步幅&lt;/strong&gt;（&lt;em&gt;python：ints的元组&lt;/em&gt;）&amp;ndash;输出张量的步幅</target>
        </trans-unit>
        <trans-unit id="b449245dfa29c41483eaf98f7744e5e9162391f6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;strip_doc_string&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default True&lt;/em&gt;) &amp;ndash; if True, strips the field &amp;ldquo;doc_string&amp;rdquo; from the exported model, which information about the stack trace.</source>
          <target state="translated">&lt;strong&gt;strip_doc_string&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;默认为True&lt;/em&gt;）&amp;ndash;如果为True，则从导出的模型中删除字段&amp;ldquo; doc_string&amp;rdquo;，其中包含有关堆栈跟踪的信息。</target>
        </trans-unit>
        <trans-unit id="19ac5a27007bad74f30226abe0c536a26c0a089d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;swap&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; The distance swap is described in detail in the paper &lt;code&gt;Learning shallow convolutional feature descriptors with triplet losses&lt;/code&gt; by V. Balntas, E. Riba et al. Default: &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;swap&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash; V. Balntas，E。Riba等人在《 &lt;code&gt;Learning shallow convolutional feature descriptors with triplet losses&lt;/code&gt; 的浅卷积特征描述符》中详细描述了距离交换。默认值： &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b154c5f2ebad9bb907ebc07611f820bf1efab1c1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;swap&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Whether to use the distance swap described in the paper &lt;code&gt;Learning shallow convolutional feature descriptors with triplet losses&lt;/code&gt; by V. Balntas, E. Riba et al. If True, and if the positive example is closer to the negative example than the anchor is, swaps the positive example and the anchor in the loss computation. Default: &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;swap&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;是否使用V. Balntas，E。Riba等人的论文《 &lt;code&gt;Learning shallow convolutional feature descriptors with triplet losses&lt;/code&gt; 的浅层卷积特征描述符》中所述的距离交换。如果为True，并且正例比锚点更接近负例，则在损失计算中交换正例和锚点。默认值： &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="65dc3057d66834aa80e6916cc4680ab2424ee9dc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;symmetric&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; indicates whether &lt;code&gt;input&lt;/code&gt; is symmetric. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;对称&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;指示 &lt;code&gt;input&lt;/code&gt; 是否对称。默认值： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="4fa21981bc63a0d2a7702caf8a85ccba75b7cbe3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;t0&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; point at which to start averaging (default: 1e6)</source>
          <target state="translated">&lt;strong&gt;t0&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;开始平均的点（默认值：1e6）</target>
        </trans-unit>
        <trans-unit id="0da7b9236fa7ec34aca8677755a15ff355bf2e5e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;t&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; tensor representing the parameter to prune</source>
          <target state="translated">&lt;strong&gt;t&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;）&amp;ndash;表示修剪参数的张量</target>
        </trans-unit>
        <trans-unit id="11deee4e01f144b32149bba99489b147b2b2f8c3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;t&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; tensor representing the parameter to prune (of same dimensions as &lt;code&gt;default_mask&lt;/code&gt;).</source>
          <target state="translated">&lt;strong&gt;t&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;）&amp;ndash;表示要修剪的参数的张量（与 &lt;code&gt;default_mask&lt;/code&gt; 的尺寸相同）。</target>
        </trans-unit>
        <trans-unit id="4a7620705c26f4724dfdfc18ad935e7dca1d4ba2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;t&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; tensor to prune (of same dimensions as &lt;code&gt;default_mask&lt;/code&gt;).</source>
          <target state="translated">&lt;strong&gt;t&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;）&amp;ndash;修剪的张量（与 &lt;code&gt;default_mask&lt;/code&gt; 相同的尺寸）。</target>
        </trans-unit>
        <trans-unit id="bad38a299dffdc28afcdbc450487e34252d433d3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;t&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#type&quot;&gt;type&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;string&lt;/em&gt;) &amp;ndash; the floating point tensor type or its name</source>
          <target state="translated">&lt;strong&gt;t&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#type&quot;&gt;类型&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;字符串&lt;/em&gt;）&amp;ndash;浮点张量类型或其名称</target>
        </trans-unit>
        <trans-unit id="148b2bfd250a2dd94c21a77dac7daca25f087bca" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tag&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Tag to match recv with remote send</source>
          <target state="translated">&lt;strong&gt;tag&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;标记以使recv与远程发送匹配</target>
        </trans-unit>
        <trans-unit id="f3d72f85bf8966156f98c3d0b833f53240d28797" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tag&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Tag to match send with remote recv</source>
          <target state="translated">&lt;strong&gt;标签&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;标签以匹配与远程recv发送</target>
        </trans-unit>
        <trans-unit id="24131f44dbc1a3c7ca0a89f0809f0ba1b31deb1d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tag&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; Data identifier</source>
          <target state="translated">&lt;strong&gt;标签&lt;/strong&gt;（&lt;em&gt;字符串&lt;/em&gt;）&amp;ndash;数据标识符</target>
        </trans-unit>
        <trans-unit id="1e47865c46d7c3937ed182d2bce842b69d717584" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tag&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; Name for the embedding</source>
          <target state="translated">&lt;strong&gt;标签&lt;/strong&gt;（&lt;em&gt;字符串&lt;/em&gt;）&amp;ndash;嵌入的名称</target>
        </trans-unit>
        <trans-unit id="6d1b78b5bcbeb528ec42020fb51c475acd78d770" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tag_scalar_dict&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;) &amp;ndash; Key-value pair storing the tag and corresponding values</source>
          <target state="translated">&lt;strong&gt;tag_scalar_dict&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;dict&lt;/a&gt;）&amp;ndash;存储标签和对应值的键值对</target>
        </trans-unit>
        <trans-unit id="8f30aad2305773e67a4ac76efb57c166bcbc5811" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;target&lt;/strong&gt; &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;目标&lt;/strong&gt;&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="719f030afe3ca4c13961bb8f325f84d710260d20" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;target&lt;/strong&gt; &amp;ndash; Tensor of the same shape as input</source>
          <target state="translated">&lt;strong&gt;目标&lt;/strong&gt;&amp;ndash;与输入形状相同的张量</target>
        </trans-unit>
        <trans-unit id="9206762f88088e88aeb5e77137d3fbb745052050" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;target&lt;/strong&gt; &amp;ndash; random sample</source>
          <target state="translated">&lt;strong&gt;目标&lt;/strong&gt;&amp;ndash;随机样本</target>
        </trans-unit>
        <trans-unit id="e65a2d46ca874e2e756d7350a646d869ef4c730d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;target&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;目标&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="72a23b074b2288d4e0ffc008a1ed4dc002ba4f88" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;target_lengths&lt;/strong&gt; &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;target_lengths&lt;/strong&gt; &amp;ndash;</target>
        </trans-unit>
        <trans-unit id="c98a50b7621481810ef1add02c8fb6b879a9c206" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;targets&lt;/strong&gt; &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;目标&lt;/strong&gt;&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="1e8d2e93abbf10103deb46414323861cf338c14e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tau&lt;/strong&gt; &amp;ndash; non-negative scalar temperature</source>
          <target state="translated">&lt;strong&gt;tau&lt;/strong&gt; &amp;ndash;非负标量温度</target>
        </trans-unit>
        <trans-unit id="d3e4cee8df27fa35e5fc4ea9723fd730a3218d61" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;temperature&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; relaxation temperature</source>
          <target state="translated">&lt;strong&gt;温度&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;松弛温度</target>
        </trans-unit>
        <trans-unit id="582dba7550e8ec990794ee9bb0c943f9584d6a9a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor1&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the numerator tensor</source>
          <target state="translated">&lt;strong&gt;tensor1&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;分子张量</target>
        </trans-unit>
        <trans-unit id="fe51c36bff59a01f2f78ad03d8e073164d1af5b1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor1&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to be multiplied</source>
          <target state="translated">&lt;strong&gt;tensor1&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;要相乘的张量</target>
        </trans-unit>
        <trans-unit id="d7c8f48497c84ad5e4208e1eddc9d137b9867057" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor1&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; an input tensor or number</source>
          <target state="translated">&lt;strong&gt;tensor1&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;数字&lt;/em&gt;）&amp;ndash;输入张量或数字</target>
        </trans-unit>
        <trans-unit id="0ddefe13e55f57fbd638c942ea4f7390bcbffe28" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor2&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the denominator tensor</source>
          <target state="translated">&lt;strong&gt;tensor2&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;分母张量</target>
        </trans-unit>
        <trans-unit id="41bea45291973dd63ddd884b78f07e00d3ab7d86" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor2&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor to be multiplied</source>
          <target state="translated">&lt;strong&gt;tensor2&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;要相乘的张量</target>
        </trans-unit>
        <trans-unit id="b7570a5e6d003c7758abda29ee8e97e855d23054" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor2&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; an input tensor or number</source>
          <target state="translated">&lt;strong&gt;tensor2&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;数字&lt;/em&gt;）&amp;ndash;输入张量或数字</target>
        </trans-unit>
        <trans-unit id="8f71579bdc06e7c9c71d37a5240b5de48b5701ac" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; &amp;ndash; a 2-dimensional &lt;code&gt;torch.Tensor&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;&amp;ndash;二维 &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;strong&gt;张量&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="1efb28f257d49ada4a2b28823d609173cc47778d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; &amp;ndash; a tensor to be exported</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;&amp;ndash;要导出的张量</target>
        </trans-unit>
        <trans-unit id="ebd185db4dcd0096fd7b5c3f6d3e9f89f2fe9c11" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; &amp;ndash; a {3, 4, 5}-dimensional &lt;code&gt;torch.Tensor&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;&amp;ndash; {3，4，5}维 &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;strong&gt;张量&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="38120be427a851f124349ce7081bb1e1d17f8071" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; &amp;ndash; an n-dimensional &lt;code&gt;torch.Tensor&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;&amp;ndash; n维 &lt;code&gt;torch.Tensor&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="9324a2a3b49e2c9c5b1f421b1679082153eb02c1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; &amp;ndash; an n-dimensional &lt;code&gt;torch.Tensor&lt;/code&gt;, where</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;&amp;ndash; n维 &lt;code&gt;torch.Tensor&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="fd9f9b08ebf1b11c3c6c5c2342c4729faa2bd202" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor containing values to add</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;包含要添加值的张量</target>
        </trans-unit>
        <trans-unit id="f130d0f3db6962333521550a2aac19265d9b05ae" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor containing values to copy</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;包含要复制值的张量</target>
        </trans-unit>
        <trans-unit id="48b5c5ac8e5e9b826ebcd51ac110f06caead74d9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor containing values to copy from</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;包含要从中复制值的张量</target>
        </trans-unit>
        <trans-unit id="db831c557616296e8156131ded5d0c40f87ca1c3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the tensor which has the desired type</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;具有所需类型的张量</target>
        </trans-unit>
        <trans-unit id="43e7c30902c9179fb0b37459c8ea94f10d65684d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; A quantized Tensor</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;量化的张量</target>
        </trans-unit>
        <trans-unit id="595262930a1db5ff17082aa694d1b0f8665e76ef" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; buffer to be registered.</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;要注册的缓冲区。</target>
        </trans-unit>
        <trans-unit id="daa8d39d1206451461ce1d0b71e54ecbc3499778" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; tensor to split.</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;张量分裂。</target>
        </trans-unit>
        <trans-unit id="c5119c9d3c84560f70044a06335745fbc7690c5e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</source>
          <target state="translated">&lt;strong&gt;tensor&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;）&amp;ndash;张量，其dtype和device是此模块中所有参数和缓冲区的所需dtype和device</target>
        </trans-unit>
        <trans-unit id="4ea925f35e05eeec0ce6c6ede21012597e80ba1d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Data to be sent if &lt;code&gt;src&lt;/code&gt; is the rank of current process, and tensor to be used to save received data otherwise.</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;如果 &lt;code&gt;src&lt;/code&gt; 是当前进程的等级，则发送数据，否则使用张量保存接收到的数据。</target>
        </trans-unit>
        <trans-unit id="0ea5bf92870a7e63a9dc7b73483e6447ea902554" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Input and output of the collective. The function operates in-place.</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;集合的输入和输出。该功能就地运行。</target>
        </trans-unit>
        <trans-unit id="871bd0fcd2a298c26c4d81bf5750fd9c2750ed91" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Input tensor.</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;输入张量。</target>
        </trans-unit>
        <trans-unit id="4086bda3a1c5df109757d3e351db5cbbd66ea1c7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Output tensor.</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;输出张量。</target>
        </trans-unit>
        <trans-unit id="d003bad4eb45ea45fc9aa2947e6f3d43bd663f2d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Tensor to be broadcast from current process.</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;从当前过程中广播的张量。</target>
        </trans-unit>
        <trans-unit id="8057a0b7b9e31fa19b74af90d185d7232521ede8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Tensor to fill with received data.</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;张量以填充接收到的数据。</target>
        </trans-unit>
        <trans-unit id="c5e9e88f62d8e8a8f1bab1c4e5838b8d932d6e95" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Tensor to send.</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;要发送的张量。</target>
        </trans-unit>
        <trans-unit id="c96308f3675384050b175ef61d2607c351010e84" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; tensor to broadcast. Can be on CPU or GPU.</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;要广播的张量。可以在CPU或GPU上。</target>
        </trans-unit>
        <trans-unit id="a96d9a8bda9ef562edf641df4347b4b73d26cbdb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; tensor to scatter. Can be on CPU or GPU.</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;张量散布。可以在CPU或GPU上。</target>
        </trans-unit>
        <trans-unit id="6ed36b8ea65f06efab795d8ef00bc8f1d04ff81c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor&lt;/strong&gt; (&lt;em&gt;LongTensor&lt;/em&gt;) &amp;ndash; class values of any shape.</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;em&gt;LongTensor&lt;/em&gt;）&amp;ndash;任何形状的类值。</target>
        </trans-unit>
        <trans-unit id="771c4af8e8d1e14069a64799aa2b6b2856c84f0a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor_list&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list&lt;/a&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; Output list. It should contain correctly-sized tensors to be used for output of the collective.</source>
          <target state="translated">&lt;strong&gt;tensor_list&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;list &lt;/a&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;输出列表。它应包含正确大小的张量以用于集合的输出。</target>
        </trans-unit>
        <trans-unit id="0529bcde8eee4077490565e65382a12d0014ef8d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor_list&lt;/strong&gt; (&lt;em&gt;List&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; Input and output GPU tensors of the collective. The function operates in-place. You also need to make sure that &lt;code&gt;len(tensor_list)&lt;/code&gt; is the same for all the distributed processes calling this function.</source>
          <target state="translated">&lt;strong&gt;tensor_list&lt;/strong&gt;（&lt;em&gt;List &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;集合的输入和输出GPU张量。该功能就地运行。您还需要确保所有调用此函数的分布式进程的 &lt;code&gt;len(tensor_list)&lt;/code&gt; 都相同。</target>
        </trans-unit>
        <trans-unit id="fd42e4e4f033181e34931a8527efa00ab289b6ad" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensor_list&lt;/strong&gt; (&lt;em&gt;List&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; Tensors that participate in the collective operation. If &lt;code&gt;src&lt;/code&gt; is the rank, then the specified &lt;code&gt;src_tensor&lt;/code&gt; element of &lt;code&gt;tensor_list&lt;/code&gt; (&lt;code&gt;tensor_list[src_tensor]&lt;/code&gt;) will be broadcast to all other tensors (on different GPUs) in the src process and all tensors in &lt;code&gt;tensor_list&lt;/code&gt; of other non-src processes. You also need to make sure that &lt;code&gt;len(tensor_list)&lt;/code&gt; is the same for all the distributed processes calling this function.</source>
          <target state="translated">&lt;strong&gt;tensor_list&lt;/strong&gt;（&lt;em&gt;List &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;参与集体操作的张量。如果 &lt;code&gt;src&lt;/code&gt; 为等级，则 &lt;code&gt;tensor_list&lt;/code&gt; （ &lt;code&gt;tensor_list[src_tensor]&lt;/code&gt; ）中指定的 &lt;code&gt;src_tensor&lt;/code&gt; 元素将被广播到src进程中的所有其他张量（在不同的GPU上），以及其他非src进程的 &lt;code&gt;tensor_list&lt;/code&gt; 中的所有张量。您还需要确保所有调用此函数的分布式进程的 &lt;code&gt;len(tensor_list)&lt;/code&gt; 都相同。</target>
        </trans-unit>
        <trans-unit id="11c199d5a721aff7dadaa91e201fd37a621320be" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensors&lt;/strong&gt; (&lt;em&gt;Iterable&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; an iterable of tensors to gather. Tensor sizes in all dimensions other than &lt;code&gt;dim&lt;/code&gt; have to match.</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;em&gt;可迭代&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;） -可迭代张量来收集。比其他所有尺寸规格张 &lt;code&gt;dim&lt;/code&gt; 必须匹配。</target>
        </trans-unit>
        <trans-unit id="251ca5120d14a95efd17bbf7ac86f2e8dcd31a74" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensors&lt;/strong&gt; (&lt;em&gt;sequence of Tensor&lt;/em&gt;) &amp;ndash; Tensors of which the derivative will be computed.</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;strong&gt;张量&lt;/strong&gt;&lt;em&gt;序列&lt;/em&gt;）&amp;ndash;将计算其导数的张量。</target>
        </trans-unit>
        <trans-unit id="6c11521362c09d9790c6e10205b4f5e9d4d0f8d2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensors&lt;/strong&gt; (&lt;em&gt;sequence of Tensors&lt;/em&gt;) &amp;ndash; A list of quantized Tensors</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;strong&gt;张量&lt;/strong&gt;&lt;em&gt;序列&lt;/em&gt;）&amp;ndash;量化张量列表</target>
        </trans-unit>
        <trans-unit id="56d11e41f90bcf5265e9b581f446c4e0dd727f96" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensors&lt;/strong&gt; (&lt;em&gt;sequence of Tensors&lt;/em&gt;) &amp;ndash; any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension.</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;strong&gt;张量&lt;/strong&gt;&lt;em&gt;序列&lt;/em&gt;）&amp;ndash;任何类型的python张量序列。提供的非空张量必须具有相同的形状，但猫的尺寸除外。</target>
        </trans-unit>
        <trans-unit id="17a8b4a65e76b16fe0001dce91103bc1f0d95788" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensors&lt;/strong&gt; (&lt;em&gt;sequence of Tensors&lt;/em&gt;) &amp;ndash; sequence of tensors to concatenate</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;strong&gt;张量&lt;/strong&gt;&lt;em&gt;序列&lt;/em&gt;）&amp;ndash;连接的张量序列</target>
        </trans-unit>
        <trans-unit id="c9c0ad52006da24004a15eb5f4a77cf9446867a1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tensors&lt;/strong&gt; (&lt;em&gt;sequence&lt;/em&gt;) &amp;ndash; tensors to broadcast. Must be on the same device, either CPU or GPU.</source>
          <target state="translated">&lt;strong&gt;张量&lt;/strong&gt;（&lt;em&gt;序列&lt;/em&gt;）&amp;ndash;要广播的张量。必须在同一设备（CPU或GPU）上。</target>
        </trans-unit>
        <trans-unit id="eac06248fef5ff01ab7df814f04387fca91e048c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;text_string&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; String to save</source>
          <target state="translated">&lt;strong&gt;text_string&lt;/strong&gt;（&lt;em&gt;string&lt;/em&gt;）&amp;ndash;要保存的字符串</target>
        </trans-unit>
        <trans-unit id="23bc89e013fe710997ceb4e02308dc814861dd46" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tgt&lt;/strong&gt; &amp;ndash; the sequence to the decoder (required).</source>
          <target state="translated">&lt;strong&gt;tgt&lt;/strong&gt; &amp;ndash;解码器的序列（必需）。</target>
        </trans-unit>
        <trans-unit id="a97b7e4ae84d91f24d47d3e533654ba0c637aeda" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tgt&lt;/strong&gt; &amp;ndash; the sequence to the decoder layer (required).</source>
          <target state="translated">&lt;strong&gt;tgt&lt;/strong&gt; &amp;ndash;解码器层的序列（必需）。</target>
        </trans-unit>
        <trans-unit id="46691f105bd3761d4561a84637a0dc690d56304c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tgt_key_padding_mask&lt;/strong&gt; &amp;ndash; the ByteTensor mask for tgt keys per batch (optional).</source>
          <target state="translated">&lt;strong&gt;tgt_key_padding_mask&lt;/strong&gt; &amp;ndash;每批tgt密钥的ByteTensor掩码（可选）。</target>
        </trans-unit>
        <trans-unit id="c272aa66d6d78c6319db9bd44d86112843bcbd0f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tgt_key_padding_mask&lt;/strong&gt; &amp;ndash; the mask for the tgt keys per batch (optional).</source>
          <target state="translated">&lt;strong&gt;tgt_key_padding_mask&lt;/strong&gt; &amp;ndash;每批tgt密钥的掩码（可选）。</target>
        </trans-unit>
        <trans-unit id="ed8a93d25be2a772ab9ad79f5fd98f7466a44647" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tgt_mask&lt;/strong&gt; &amp;ndash; the additive mask for the tgt sequence (optional).</source>
          <target state="translated">&lt;strong&gt;tgt_mask&lt;/strong&gt; &amp;ndash; tgt序列的附加掩码（可选）。</target>
        </trans-unit>
        <trans-unit id="132d016583670fccbcfcac0d15e9840ebbe1f4a6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tgt_mask&lt;/strong&gt; &amp;ndash; the mask for the tgt sequence (optional).</source>
          <target state="translated">&lt;strong&gt;tgt_mask&lt;/strong&gt; &amp;ndash; tgt序列的掩码（可选）。</target>
        </trans-unit>
        <trans-unit id="5601a92236ec47fd0c06d7f0fed5b5b6c8fcc29b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;that need to be respected after the new mask is&lt;/strong&gt; (&lt;em&gt;iterations&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;那之后的新面具是需要得到尊重&lt;/strong&gt;（&lt;em&gt;迭代&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;） -</target>
        </trans-unit>
        <trans-unit id="ae41ef1091d9aad2a5c98f17e3318f399d0289bd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;the best candidates for quantization&lt;/strong&gt; (&lt;em&gt;choosing&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;量化的最佳人选&lt;/strong&gt;（&lt;em&gt;选择&lt;/em&gt;）&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="6712ad21050e25aa58857f430d28e66ca2b72d67" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;theta&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; input batch of affine matrices with shape (</source>
          <target state="translated">&lt;strong&gt;theta&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;输入一批仿射矩阵，其形状为（</target>
        </trans-unit>
        <trans-unit id="1f97d9939a572f78a3fab174d3aace7d899626d1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;threshold&lt;/strong&gt; &amp;ndash; The value to threshold at</source>
          <target state="translated">&lt;strong&gt;阈值&lt;/strong&gt;&amp;ndash;阈值的值</target>
        </trans-unit>
        <trans-unit id="4e72f74774d99b133267f9808f645ba3f90baa42" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;threshold&lt;/strong&gt; &amp;ndash; Total number of array elements which trigger summarization rather than full &lt;code&gt;repr&lt;/code&gt; (default = 1000).</source>
          <target state="translated">&lt;strong&gt;阈值&lt;/strong&gt;&amp;ndash;触发汇总而不是完整 &lt;code&gt;repr&lt;/code&gt; 的数组元素总数（默认= 1000）。</target>
        </trans-unit>
        <trans-unit id="9beeeb50002c6e9c428d419f497bfffeb0df1d1b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;threshold&lt;/strong&gt; &amp;ndash; values above this revert to a linear function. Default: 20</source>
          <target state="translated">&lt;strong&gt;阈值&lt;/strong&gt;&amp;ndash;高于此值将恢复为线性函数。默认值：20</target>
        </trans-unit>
        <trans-unit id="6cb0c42969ee401db173a6b40a7eca999d5919d0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;threshold&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; Threshold for measuring the new optimum, to only focus on significant changes. Default: 1e-4.</source>
          <target state="translated">&lt;strong&gt;阈值&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;浮点数&lt;/a&gt;）&amp;ndash;用于测量新的最佳阈值，仅关注重大变化。默认值：1e-4。</target>
        </trans-unit>
        <trans-unit id="2d0eb62fc3c7ef60a698a936e990467a02a92a1c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;threshold_mode&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; One of &lt;code&gt;rel&lt;/code&gt;, &lt;code&gt;abs&lt;/code&gt;. In &lt;code&gt;rel&lt;/code&gt; mode, dynamic_threshold = best * ( 1 + threshold ) in &amp;lsquo;max&amp;rsquo; mode or best * ( 1 - threshold ) in &lt;code&gt;min&lt;/code&gt; mode. In &lt;code&gt;abs&lt;/code&gt; mode, dynamic_threshold = best + threshold in &lt;code&gt;max&lt;/code&gt; mode or best - threshold in &lt;code&gt;min&lt;/code&gt; mode. Default: &amp;lsquo;rel&amp;rsquo;.</source>
          <target state="translated">&lt;strong&gt;threshold_mode&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;）&amp;ndash; &lt;code&gt;rel&lt;/code&gt; ， &lt;code&gt;abs&lt;/code&gt; 之一。在 &lt;code&gt;rel&lt;/code&gt; 模式下，dynamic_threshold =&amp;ldquo; max&amp;rdquo;模式下的best *（1 + threshold）或 &lt;code&gt;min&lt;/code&gt; 模式下的best *（1-threshold）。在 &lt;code&gt;abs&lt;/code&gt; 模式下，dynamic_threshold =最佳+ &lt;code&gt;max&lt;/code&gt; 模式下的阈值或最佳- &lt;code&gt;min&lt;/code&gt; 模式下的阈值。默认值：&amp;ldquo; rel&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="c6464c5c07bfa189e529ba00b647f718ccc94239" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;timeout&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; Wait this long before giving up on waiting.</source>
          <target state="translated">&lt;strong&gt;超时&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;等待很长一段时间，然后放弃等待。</target>
        </trans-unit>
        <trans-unit id="a4158143cd3f539f6f9f4f75136861c57fa1f337" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;timeout&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Timeout for &lt;code&gt;to_here&lt;/code&gt;. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used.</source>
          <target state="translated">&lt;strong&gt;超时&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;浮动&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash; &lt;code&gt;to_here&lt;/code&gt; 的超时。如果呼叫未在此时间段内完成，则会引发一个异常指示。如果未提供此参数，则将使用默认的RPC超时（60s）。</target>
        </trans-unit>
        <trans-unit id="e0ca20ba1ac1f4702011bcb3b44292663520556f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;timeout&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; timeout in seconds for this remote call. If the creation of this &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;&lt;code&gt;RRef&lt;/code&gt;&lt;/a&gt; on worker &lt;code&gt;to&lt;/code&gt; is not successfully processed on this worker within this timeout, then the next time there is an attempt to use the RRef (such as &lt;code&gt;to_here()&lt;/code&gt;), a timeout will be raised indicating this failure. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with &lt;code&gt;_set_rpc_timeout&lt;/code&gt; is used.</source>
          <target state="translated">&lt;strong&gt;超时&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;此远程呼叫的超时（以秒为单位）。如果此的创建&lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt; &lt;code&gt;RRef&lt;/code&gt; &lt;/a&gt;对工人 &lt;code&gt;to&lt;/code&gt; 未成功这个超时时间内对这个工人处理，则下一个时间存在使用所述器RRef尝试（如 &lt;code&gt;to_here()&lt;/code&gt; ），超时将升高指示该故障。值为0表示无限超时，即永远不会发生超时错误。如果未提供，则使用初始化期间或通过 &lt;code&gt;_set_rpc_timeout&lt;/code&gt; 设置的默认值。</target>
        </trans-unit>
        <trans-unit id="cb09a1d9765bc72d97aa2aa954e843c0cc306ebd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;timeout&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; timeout in seconds to use for this RPC. If the RPC does not complete in this amount of time, an exception indicating it has timed out will be raised. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with &lt;code&gt;_set_rpc_timeout&lt;/code&gt; is used.</source>
          <target state="translated">&lt;strong&gt;超时&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;用于此RPC的超时（以秒为单位）。如果RPC在这段时间内没有完成，则将引发异常，表明RPC已超时。值为0表示无限超时，即永远不会发生超时错误。如果未提供，则使用初始化期间或通过 &lt;code&gt;_set_rpc_timeout&lt;/code&gt; 设置的默认值。</target>
        </trans-unit>
        <trans-unit id="d38206a16efc702de4b546c7ebb66bce20140dc1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;timeout&lt;/strong&gt; (&lt;em&gt;numeric&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: &lt;code&gt;0&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;超时&lt;/strong&gt;（&lt;em&gt;数字&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为正，则为从工作人员收集批次的超时值。应始终为非负数。（默认值： &lt;code&gt;0&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="9273f5efc9a2168581a1563dc88271535c09449c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;timeout&lt;/strong&gt; (&lt;em&gt;timedelta&lt;/em&gt;) &amp;ndash; Time to wait for the keys to be added before throwing an exception.</source>
          <target state="translated">&lt;strong&gt;timeout&lt;/strong&gt;（&lt;em&gt;timedelta&lt;/em&gt;）&amp;ndash;在引发异常之前等待添加键的时间。</target>
        </trans-unit>
        <trans-unit id="52d6695e842a69c4f1f1155be378bf5f945bf62e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;timeout&lt;/strong&gt; (&lt;em&gt;timedelta&lt;/em&gt;) &amp;ndash; Timeout used by the store during initialization and for methods such as &lt;code&gt;get()&lt;/code&gt; and &lt;code&gt;wait()&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;timeout&lt;/strong&gt;（&lt;em&gt;timedelta&lt;/em&gt;）&amp;ndash;初始化期间商店使用的超时，并且用于诸如 &lt;code&gt;get()&lt;/code&gt; 和 &lt;code&gt;wait()&lt;/code&gt; 之类的方法。</target>
        </trans-unit>
        <trans-unit id="8232ad0889858559e2f3203547e409d87becb68e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;timeout&lt;/strong&gt; (&lt;em&gt;timedelta&lt;/em&gt;) &amp;ndash; timeout to be set in the store.</source>
          <target state="translated">&lt;strong&gt;超时&lt;/strong&gt;（&lt;em&gt;timedelta&lt;/em&gt;）&amp;ndash;要在商店中设置的超时。</target>
        </trans-unit>
        <trans-unit id="2751f8d0a7ffb7b1eea973ad31d6029c6cf7b2ad" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;timeout&lt;/strong&gt; (&lt;em&gt;timedelta&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Timeout for operations executed against the process group. Default value equals 30 minutes. This is applicable for the &lt;code&gt;gloo&lt;/code&gt; backend. For &lt;code&gt;nccl&lt;/code&gt;, this is applicable only if the environment variable &lt;code&gt;NCCL_BLOCKING_WAIT&lt;/code&gt; or &lt;code&gt;NCCL_ASYNC_ERROR_HANDLING&lt;/code&gt; is set to 1. When &lt;code&gt;NCCL_BLOCKING_WAIT&lt;/code&gt; is set, this is the duration for which the process will block and wait for collectives to complete before throwing an exception. When &lt;code&gt;NCCL_ASYNC_ERROR_HANDLING&lt;/code&gt; is set, this is the duration after which collectives will be aborted asynchronously and the process will crash. &lt;code&gt;NCCL_BLOCKING_WAIT&lt;/code&gt; will provide errors to the user which can be caught and handled, but due to its blocking nature, it has a performance overhead. On the other hand, &lt;code&gt;NCCL_ASYNC_ERROR_HANDLING&lt;/code&gt; has little performance overhead, but crashes the process on errors. This is done since CUDA execution is async and it is no longer safe to continue executing user code since failed async NCCL operations might result in subsequent CUDA operations to run on corrupted data. Only one of these two environment variables should be set.</source>
          <target state="translated">&lt;strong&gt;超时&lt;/strong&gt;（&lt;em&gt;timedelta &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;针对流程组执行的操作的超时。默认值等于30分钟。这适用于 &lt;code&gt;gloo&lt;/code&gt; 后端。对于 &lt;code&gt;nccl&lt;/code&gt; ，仅当环境变量 &lt;code&gt;NCCL_BLOCKING_WAIT&lt;/code&gt; 或 &lt;code&gt;NCCL_ASYNC_ERROR_HANDLING&lt;/code&gt; 设置为1时，此方法才适用。设置 &lt;code&gt;NCCL_BLOCKING_WAIT&lt;/code&gt; 时，这是进程将阻塞并等待集合完成并引发异常之前的持续时间。当 &lt;code&gt;NCCL_ASYNC_ERROR_HANDLING&lt;/code&gt; 设置，这是后集体将被异步中止，进程会崩溃的持续时间。 &lt;code&gt;NCCL_BLOCKING_WAIT&lt;/code&gt; 将向用户提供可以捕获和处理的错误，但是由于其阻塞性质，因此会产生性能开销。另一方面， &lt;code&gt;NCCL_ASYNC_ERROR_HANDLING&lt;/code&gt; 的性能开销很小，但是由于错误而使进程崩溃。这样做是因为CUDA执行是异步的，并且继续执行用户代码不再安全，因为异步NCCL操作失败可能会导致后续的CUDA操作在损坏的数据上运行。只能设置这两个环境变量之一。</target>
        </trans-unit>
        <trans-unit id="00a4cf1534a34a3e1bdbf78c948e2ba0bebb6650" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;timeout&lt;/strong&gt; (&lt;em&gt;timedelta&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Timeout for operations executed against the process group. Default value equals 30 minutes. This is only applicable for the &lt;code&gt;gloo&lt;/code&gt; backend.</source>
          <target state="translated">&lt;strong&gt;超时&lt;/strong&gt;（&lt;em&gt;timedelta &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;针对流程组执行的操作的超时。默认值等于30分钟。这仅适用于 &lt;code&gt;gloo&lt;/code&gt; 后端。</target>
        </trans-unit>
        <trans-unit id="02c882e856aedbce8abb8c0907459899847d1718" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;to&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;#torch.distributed.rpc.WorkerInfo&quot;&gt;WorkerInfo&lt;/a&gt;) &amp;ndash; id or name of the destination worker.</source>
          <target state="translated">&lt;strong&gt;到&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;#torch.distributed.rpc.WorkerInfo&quot;&gt;WorkerInfo&lt;/a&gt;）&amp;ndash;目标工作者的ID或名称。</target>
        </trans-unit>
        <trans-unit id="ea9f5f30be6e298274025992da4c6c4ec845554e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;to&lt;/strong&gt; (&lt;em&gt;dpython:type&lt;/em&gt;) &amp;ndash; The target &lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;strong&gt;to&lt;/strong&gt;（&lt;em&gt;dpython：type&lt;/em&gt;）&amp;ndash;目标&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e52fdd2bd38561ad6a0c5c61e130e89b4aada3ce" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tol&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; residual tolerance for stopping criterion. Default is &lt;code&gt;feps ** 0.5&lt;/code&gt; where &lt;code&gt;feps&lt;/code&gt; is smallest non-zero floating-point number of the given input tensor &lt;code&gt;A&lt;/code&gt; data type.</source>
          <target state="translated">&lt;strong&gt;tol&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）-停止标准的剩余公差。默认值为 &lt;code&gt;feps ** 0.5&lt;/code&gt; ，其中 &lt;code&gt;feps&lt;/code&gt; 是给定输入张量 &lt;code&gt;A&lt;/code&gt; 数据类型的最小非零浮点数。</target>
        </trans-unit>
        <trans-unit id="ebb57295704495b7e390dfbf06821990d8f2e8d3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tol&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the tolerance value. Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;tol&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）-公差值。默认值： &lt;code&gt;None&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="bd525427c04eccdee85fc4cee3c55e31a24cb304" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tolerance_change&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; termination tolerance on function value/parameter changes (default: 1e-9).</source>
          <target state="translated">&lt;strong&gt;tolerance_change&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;函数值/参数更改时的终止公差（默认值：1e-9）。</target>
        </trans-unit>
        <trans-unit id="3c197525a443a87cf9f094716c1dcb003057eef0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tolerance_grad&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; termination tolerance on first order optimality (default: 1e-5).</source>
          <target state="translated">&lt;strong&gt;tolerance_grad&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;一阶最优的端接公差（默认值：1e-5）。</target>
        </trans-unit>
        <trans-unit id="407c445fafb074d2e67a14d4a7333561e68a55d0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;top_level_events_only&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Boolean flag to determine the selection of events to display. If true, the profiler will only display events at top level like top-level invocation of python &lt;code&gt;lstm&lt;/code&gt;, python &lt;code&gt;add&lt;/code&gt; or other functions, nested events like low-level cpu/cuda ops events are omitted for profiler result readability.</source>
          <target state="translated">&lt;strong&gt;top_level_events_only&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;布尔标志，用于确定要显示的事件的选择。如果为true，则探查器将仅在顶级事件（例如python &lt;code&gt;lstm&lt;/code&gt; 的顶级调用，python &lt;code&gt;add&lt;/code&gt; 或其他函数）上显示事件，为简化探查器结果的可读性，将省略嵌套事件（例如低级cpu / cuda ops事件）。</target>
        </trans-unit>
        <trans-unit id="abe1f21ed4cea1ee1d4b3f7ed2f79c15752c40b1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;total_count&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; non-negative number of negative Bernoulli trials to stop, although the distribution is still valid for real valued count</source>
          <target state="translated">&lt;strong&gt;total_count&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;要停止的伯努利阴性试验的非负数，尽管该分布对于实际值计数仍然有效</target>
        </trans-unit>
        <trans-unit id="d26e0c4265279fa280cd914ce50f04f7c4aa40eb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;total_count&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; number of trials</source>
          <target state="translated">&lt;strong&gt;total_count&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;试用次数</target>
        </trans-unit>
        <trans-unit id="077375196e8792ce500b9544f90a99bec0c48113" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;total_count&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; number of Bernoulli trials</source>
          <target state="translated">&lt;strong&gt;total_count&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;伯努利试验次数</target>
        </trans-unit>
        <trans-unit id="c6231da05b67c6d40cc466aa082bc7aa1fceb57f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;total_length&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; if not &lt;code&gt;None&lt;/code&gt;, the output will be padded to have length &lt;code&gt;total_length&lt;/code&gt;. This method will throw &lt;a href=&quot;https://docs.python.org/3/library/exceptions.html#ValueError&quot;&gt;&lt;code&gt;ValueError&lt;/code&gt;&lt;/a&gt; if &lt;code&gt;total_length&lt;/code&gt; is less than the max sequence length in &lt;code&gt;sequence&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;total_length&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果不为 &lt;code&gt;None&lt;/code&gt; ，则输出将被填充为具有 &lt;code&gt;total_length&lt;/code&gt; 的长度。此方法将抛出&lt;a href=&quot;https://docs.python.org/3/library/exceptions.html#ValueError&quot;&gt; &lt;code&gt;ValueError&lt;/code&gt; 异常&lt;/a&gt;如果 &lt;code&gt;total_length&lt;/code&gt; 小于最大序列长度 &lt;code&gt;sequence&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="13b2d7d1150a6055778df0dc94b65e3f358e1ead" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;total_steps&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The total number of steps in the cycle. Note that if a value is not provided here, then it must be inferred by providing a value for epochs and steps_per_epoch. Default: None</source>
          <target state="translated">&lt;strong&gt;total_steps&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;循环中的总步数。请注意，如果此处未提供值，则必须通过为epochs和steps_per_epoch提供值来进行推断。默认值：无</target>
        </trans-unit>
        <trans-unit id="07c5deb90e808db58c55e72b715383a8fbfd19d6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;track_running_stats&lt;/strong&gt; &amp;ndash; a boolean value that when set to &lt;code&gt;True&lt;/code&gt;, this module tracks the running mean and variance, and when set to &lt;code&gt;False&lt;/code&gt;, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;track_running_stats&lt;/strong&gt; &amp;ndash;一个布尔值，当设置为 &lt;code&gt;True&lt;/code&gt; 时，此模块跟踪运行平均值和方差；当设置为 &lt;code&gt;False&lt;/code&gt; 时，此模块不跟踪此类统计信息，并且始终在训练和评估模式下使用批处理统计信息。默认值： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="3efa70ce9f7603a628298aac133f6c2ebf2aea12" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;track_running_stats&lt;/strong&gt; &amp;ndash; a boolean value that when set to &lt;code&gt;True&lt;/code&gt;, this module tracks the running mean and variance, and when set to &lt;code&gt;False&lt;/code&gt;, this module does not track such statistics, and initializes statistics buffers &lt;code&gt;running_mean&lt;/code&gt; and &lt;code&gt;running_var&lt;/code&gt; as &lt;code&gt;None&lt;/code&gt;. When these buffers are &lt;code&gt;None&lt;/code&gt;, this module always uses batch statistics. in both training and eval modes. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;track_running_stats&lt;/strong&gt; &amp;ndash;一个布尔值，当设置为 &lt;code&gt;True&lt;/code&gt; 时，此模块跟踪运行平均值和方差；当设置为 &lt;code&gt;False&lt;/code&gt; 时，此模块不跟踪此类统计信息，并将统计信息缓冲区 &lt;code&gt;running_mean&lt;/code&gt; 和 &lt;code&gt;running_var&lt;/code&gt; 初始化为 &lt;code&gt;None&lt;/code&gt; 。当这些缓冲区为 &lt;code&gt;None&lt;/code&gt; 时，此模块将始终使用批处理统计信息。在训练和评估模式下都可以。默认值： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="83e1567d195f61030af1691e6408ec395faa36b6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;tracker&lt;/strong&gt; (&lt;em&gt;callable&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;跟踪器&lt;/strong&gt;（&lt;em&gt;可调用&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="cae553a329b41a39ac2ef28a1128e47f2aff1a79" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;trainable_backbone_layers&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; number of trainable (not frozen) resnet layers starting from final block. Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable.</source>
          <target state="translated">&lt;strong&gt;trainable_backbone_layers&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;从最后一个块开始的可训练（未冻结）resnet层数。有效值在0到5之间，其中5表示所有骨干层都是可训练的。</target>
        </trans-unit>
        <trans-unit id="1918b886e2cf75d47f208888e3bf63f2ca3faa0e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;training&lt;/strong&gt; &amp;ndash; apply dropout if is &lt;code&gt;True&lt;/code&gt;. Default: &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;培训&lt;/strong&gt;&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ,则申请辍学。默认值： &lt;code&gt;True&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="9b04f39ddad8101f7db51608eea54f5bf896eeb7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;training&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Boolean represents whether this module is in training or evaluation mode.</source>
          <target state="translated">&lt;strong&gt;training&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;布尔值，表示此模块是处于培训还是评估模式。</target>
        </trans-unit>
        <trans-unit id="51af50b75452e3c060a548901f1c86020bc06d08" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;training&lt;/strong&gt; (&lt;em&gt;enum&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default TrainingMode.EVAL&lt;/em&gt;) &amp;ndash; TrainingMode.EVAL: export the model in inference mode. TrainingMode.PRESERVE: export the model in inference mode if model.training is False and to a training friendly mode if model.training is True. TrainingMode.TRAINING: export the model in a training friendly mode.</source>
          <target state="translated">&lt;strong&gt;training&lt;/strong&gt;（&lt;em&gt;枚举&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;默认为TrainingMode.EVAL&lt;/em&gt;）&amp;ndash; TrainingMode.EVAL：以推理模式导出模型。TrainingMode.PRESERVE：如果model.training为False，则以推理模式导出模型，如果model.training为True，则导出为训练友好模式。TrainingMode.TRAINING：以训练友好模式导出模型。</target>
        </trans-unit>
        <trans-unit id="6b489d8a5fd1b48c8841b422261d73f6fc9f5019" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;transform_input&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If True, preprocesses the input according to the method with which it was trained on ImageNet. Default: &lt;em&gt;False&lt;/em&gt;</source>
          <target state="translated">&lt;strong&gt;transform_input&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;如果为True，则根据在ImageNet上训练输入的方法对输入进行预处理。默认值：&lt;em&gt;False&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="0f7f01bd2f696782296281656611b9d7f0e11af1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;transpose&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether</source>
          <target state="translated">&lt;strong&gt;转置&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;是否</target>
        </trans-unit>
        <trans-unit id="5d7f3ae8d3896d551417a34cad2154f4876e37e8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;type1&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;type1&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;) &amp;ndash;</target>
        </trans-unit>
        <trans-unit id="db83431c42acbc4313b6844960f9f9b62081f0cb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;type2&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;type2&lt;/strong&gt; (&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;) &amp;ndash;</target>
        </trans-unit>
        <trans-unit id="a0c7d87b9ea55c63dfca094efac9fe724ed34848" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;type_p&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#type&quot;&gt;type&lt;/a&gt;) &amp;ndash; A subclass of &lt;code&gt;Distribution&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;type_p&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#type&quot;&gt;类型&lt;/a&gt;）&amp;ndash; &lt;code&gt;Distribution&lt;/code&gt; 的子类。</target>
        </trans-unit>
        <trans-unit id="5169a079777e56a56ce65c29ef02ee71884331c3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;type_q&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#type&quot;&gt;type&lt;/a&gt;) &amp;ndash; A subclass of &lt;code&gt;Distribution&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;type_q&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#type&quot;&gt;类型&lt;/a&gt;）&amp;ndash; &lt;code&gt;Distribution&lt;/code&gt; 的子类。</target>
        </trans-unit>
        <trans-unit id="2ebcbce8f74baa28dc73e78635230701acf753b9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;unbiased&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; whether to use the unbiased estimation or not</source>
          <target state="translated">&lt;strong&gt;无偏&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;布尔&lt;/a&gt;）&amp;ndash;是否使用无偏估计</target>
        </trans-unit>
        <trans-unit id="2e13564fc77541067b4971002e5a668623c64ae3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;unexpected_keys&lt;/strong&gt; is a list of str containing the unexpected keys</source>
          <target state="translated">&lt;strong&gt;excellent_keys&lt;/strong&gt;是包含意外密钥的str的列表</target>
        </trans-unit>
        <trans-unit id="40c65479b90b1228aa4509bb18c068347e1a8396" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;unflattened_size&lt;/strong&gt; (&lt;em&gt;Union&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;em&gt;torch.Size&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;NamedShape&lt;/em&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; New shape of the unflattened dimension</source>
          <target state="translated">&lt;strong&gt;unflattened_size&lt;/strong&gt;（&lt;em&gt;Union &lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;em&gt;torch.Size &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;NamedShape &lt;/em&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;未展平尺寸的新形状</target>
        </trans-unit>
        <trans-unit id="6577f53f07c6b82453a5e4d8ee1cdce173f1095c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;unitriangular&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether</source>
          <target state="translated">&lt;strong&gt;unitriangular&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;是否</target>
        </trans-unit>
        <trans-unit id="f6abb29d9132a1cca0511aeb0026c03c9d5e185c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;unpack_data&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; flag indicating if the data should be unpacked</source>
          <target state="translated">&lt;strong&gt;unpack_data&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;指示是否应该解压缩数据的标志</target>
        </trans-unit>
        <trans-unit id="04e96028a5908db9b44aee6aad14f15f640b0004" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;unpack_pivots&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; flag indicating if the pivots should be unpacked</source>
          <target state="translated">&lt;strong&gt;unpack_pivots&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;指示是否应打开枢轴包装的标志</target>
        </trans-unit>
        <trans-unit id="f545e5ba1e2fb058ee8bbcf81cc46f57b737b0f7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;upper&lt;/strong&gt; &amp;ndash; upper bound of the uniform distribution. Default:</source>
          <target state="translated">&lt;strong&gt;上限&lt;/strong&gt;&amp;ndash;均匀分布的上限。默认：</target>
        </trans-unit>
        <trans-unit id="078b557c79445664413d140b19483eb01fc4a40b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;upper&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; flag that indicates whether to return a upper or lower triangular matrix. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;upper&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;指示是否返回上三角矩阵或下三角矩阵的标志。默认值： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f6e194e2ffa34563a9da2cd5695fbad2b195aeb3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;upper&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to consider the Cholesky factor as a lower or upper triangular matrix. Default: &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;upper&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;是将Cholesky因子视为下部三角形矩阵还是上部三角形矩阵。默认值： &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e4c027c4ea86e34928bea3afd23c0befc2ae84dc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;upper&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to return a lower (default) or upper triangular matrix</source>
          <target state="translated">&lt;strong&gt;upper&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;是返回下三角矩阵（默认）还是上三角矩阵</target>
        </trans-unit>
        <trans-unit id="e817dde6e0d6a551d34fc3800e9ab1a2430679a8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;upper&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to solve the upper-triangular system of equations (default) or the lower-triangular system of equations. Default: &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;upper&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;是求解方程的上三角系统（默认值）还是求解方程的下三角系统。默认值： &lt;code&gt;True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f6bd6cb43a30358db4258c20658d93f8d71b69f5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;upper&lt;/strong&gt; (&lt;em&gt;boolean&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; controls whether to consider upper-triangular or lower-triangular region</source>
          <target state="translated">&lt;strong&gt;upper&lt;/strong&gt;（&lt;em&gt;布尔值&lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;控制是考虑上三角区域还是下三角区域</target>
        </trans-unit>
        <trans-unit id="f5cb009f2bb1f037cbc1a012ac4ce862506a7d43" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;upsample_rate&lt;/strong&gt; &amp;ndash; Factor by which the histograms are upsampled, this is used to interpolate histograms with varying ranges across observations</source>
          <target state="translated">&lt;strong&gt;upsample_rate&lt;/strong&gt; &amp;ndash;直方图被上采样的因子，用于对观测值变化范围内的直方图进行插值</target>
        </trans-unit>
        <trans-unit id="f8ed541c4bbf8acbeeefe806b08d772cdb213ba6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;upscale_factor&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; factor to increase spatial resolution by</source>
          <target state="translated">&lt;strong&gt;upscale_factor&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;增加空间分辨率的因数</target>
        </trans-unit>
        <trans-unit id="dbbba1d51581a2e6efdbd392cc9848e659d8b9c2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;url&lt;/strong&gt; (&lt;em&gt;string&lt;/em&gt;) &amp;ndash; URL of the object to download</source>
          <target state="translated">&lt;strong&gt;url&lt;/strong&gt;（&lt;em&gt;字符串&lt;/em&gt;）&amp;ndash;要下载的对象的URL</target>
        </trans-unit>
        <trans-unit id="0f0afdb93eece512a7ce83a3e5f3b3330df3e43c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;use_cuda&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Enables timing of CUDA events as well using the cudaEvent API. Adds approximately 4us of overhead to each tensor operation. Default: &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;use_cuda&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;以及使用cudaEvent API启用CUDA事件的计时。每个张量操作会增加大约4us的开销。默认值： &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="03fa705d67fc134339599e4e459d34c6a562acc7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;v&lt;/strong&gt; (&lt;em&gt;tuple of Tensors&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The vector for which the Hessian vector product is computed. Must be the same size as the input of &lt;code&gt;func&lt;/code&gt;. This argument is optional when &lt;code&gt;func&lt;/code&gt;&amp;rsquo;s input contains a single element and (if it is not provided) will be set as a Tensor containing a single &lt;code&gt;1&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;v&lt;/strong&gt;（&lt;em&gt;张量&lt;/em&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量的&lt;/a&gt;&lt;em&gt;元组&lt;/em&gt;）&amp;ndash;要为其计算Hessian向量积的向量。必须与 &lt;code&gt;func&lt;/code&gt; 的输入大小相同。当 &lt;code&gt;func&lt;/code&gt; 的输入包含单个元素并且（如果未提供）将被设置为包含单个 &lt;code&gt;1&lt;/code&gt; 的张量时，此参数是可选的。</target>
        </trans-unit>
        <trans-unit id="13f694a2701ec74a73a5d0f00ebff38c907669d6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;v&lt;/strong&gt; (&lt;em&gt;tuple of Tensors&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The vector for which the Jacobian vector product is computed. Must be the same size as the input of &lt;code&gt;func&lt;/code&gt;. This argument is optional when the input to &lt;code&gt;func&lt;/code&gt; contains a single element and (if it is not provided) will be set as a Tensor containing a single &lt;code&gt;1&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;v&lt;/strong&gt;（&lt;em&gt;张量&lt;/em&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量的&lt;/a&gt;&lt;em&gt;元组&lt;/em&gt;）&amp;ndash;计算雅可比矢量积的矢量。必须与 &lt;code&gt;func&lt;/code&gt; 的输入大小相同。当 &lt;code&gt;func&lt;/code&gt; 的输入包含单个元素并且（如果未提供）将设置为包含单个 &lt;code&gt;1&lt;/code&gt; 的Tensor时，此参数是可选的。</target>
        </trans-unit>
        <trans-unit id="11eb112c28f57578089a428d40281d86b3149af8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;v&lt;/strong&gt; (&lt;em&gt;tuple of Tensors&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The vector for which the vector Hessian product is computed. Must be the same size as the input of &lt;code&gt;func&lt;/code&gt;. This argument is optional when &lt;code&gt;func&lt;/code&gt;&amp;rsquo;s input contains a single element and (if it is not provided) will be set as a Tensor containing a single &lt;code&gt;1&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;v&lt;/strong&gt;（&lt;em&gt;张量&lt;/em&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量的&lt;/a&gt;&lt;em&gt;元组&lt;/em&gt;）&amp;ndash;要为其计算矢量Hessian乘积的矢量。必须与 &lt;code&gt;func&lt;/code&gt; 的输入大小相同。当 &lt;code&gt;func&lt;/code&gt; 的输入包含单个元素并且（如果未提供）将被设置为包含单个 &lt;code&gt;1&lt;/code&gt; 的张量时，此参数是可选的。</target>
        </trans-unit>
        <trans-unit id="d835c6d1aa6fb4ff923c35566662ff986f45e13f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;v&lt;/strong&gt; (&lt;em&gt;tuple of Tensors&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The vector for which the vector Jacobian product is computed. Must be the same size as the output of &lt;code&gt;func&lt;/code&gt;. This argument is optional when the output of &lt;code&gt;func&lt;/code&gt; contains a single element and (if it is not provided) will be set as a Tensor containing a single &lt;code&gt;1&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;v&lt;/strong&gt;（&lt;em&gt;张量&lt;/em&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量的&lt;/a&gt;&lt;em&gt;元组&lt;/em&gt;）&amp;ndash;要为其计算矢量雅可比积的矢量。必须与 &lt;code&gt;func&lt;/code&gt; 的输出大小相同。当 &lt;code&gt;func&lt;/code&gt; 的输出包含单个元素并且（如果未提供）将被设置为包含单个 &lt;code&gt;1&lt;/code&gt; 的Tensor时，此参数是可选的。</target>
        </trans-unit>
        <trans-unit id="98118f8f62fd734993f566a38ff617a7ee9952b9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;val&lt;/strong&gt; &amp;ndash; the value to fill the tensor with</source>
          <target state="translated">&lt;strong&gt;val&lt;/strong&gt; &amp;ndash;用张量填充的值</target>
        </trans-unit>
        <trans-unit id="7e6a7de4d7910c74c696408d838f32027dedb43c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;val&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the value to fill with</source>
          <target state="translated">&lt;strong&gt;val&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;要填充的值</target>
        </trans-unit>
        <trans-unit id="e1ad2ed4dd7f0bf60e90e62bbcab68ef4dd2b46f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;value&lt;/strong&gt; &amp;ndash; The value to replace with</source>
          <target state="translated">&lt;strong&gt;值&lt;/strong&gt;&amp;ndash;替换为的值</target>
        </trans-unit>
        <trans-unit id="5ea89c83b46186a326f0e2a0c5a74bf1b7ed29c3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;value&lt;/strong&gt; &amp;ndash; fill value for &lt;code&gt;'constant'&lt;/code&gt; padding. Default: &lt;code&gt;0&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;value&lt;/strong&gt; &amp;ndash;填充 &lt;code&gt;'constant'&lt;/code&gt; 填充的值。默认值： &lt;code&gt;0&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="7b65f448578fca2733acf40a459ffeecb2eddae8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;value&lt;/strong&gt; (&lt;a href=&quot;#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; tensor of same dtype as &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;值&lt;/strong&gt;（&lt;a href=&quot;#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;与 &lt;code&gt;self&lt;/code&gt; 相同dtype的张量。</target>
        </trans-unit>
        <trans-unit id="1d73cf9df184ed1151607fad31d5ce6f0c1674e1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;value&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the source element(s) to scatter, incase &lt;code&gt;src&lt;/code&gt; is not specified</source>
          <target state="translated">&lt;strong&gt;value&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;要分散的源元素，如果未指定 &lt;code&gt;src&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="2231cde0326d2e7cbe7a063bea9dd6db2ca090b8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;value&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; the value to fill in with</source>
          <target state="translated">&lt;strong&gt;value&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;要填写的值</target>
        </trans-unit>
        <trans-unit id="9b8e6c78f63bfe21f3c21ce8282ea0ac124b1636" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;value&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; The value associated with &lt;code&gt;key&lt;/code&gt; to be added to the store.</source>
          <target state="translated">&lt;strong&gt;value&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;）&amp;ndash;与要添加到存储中的 &lt;code&gt;key&lt;/code&gt; 关联的值。</target>
        </trans-unit>
        <trans-unit id="58297fdeed23352e6cee0f60735b074f80fc01b0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;value&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;值&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="dbe8188f9a9c1f457d68930c71ccb9451d1315c0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;value&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;) &amp;ndash; the number to be added to each element of &lt;code&gt;input&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;value&lt;/strong&gt;（&lt;em&gt;Number&lt;/em&gt;）&amp;ndash;要添加到 &lt;code&gt;input&lt;/code&gt; 每个元素的数字</target>
        </trans-unit>
        <trans-unit id="b0865065739e87d295963220665d5169b7839527" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;value&lt;/strong&gt; (&lt;em&gt;Number&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; multiplier for</source>
          <target state="translated">&lt;strong&gt;值&lt;/strong&gt;（&lt;em&gt;Number &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;乘数</target>
        </trans-unit>
        <trans-unit id="83dafd80344b2148557bddd1e22227cc205b7bf0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;values&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The values to use where &lt;code&gt;input&lt;/code&gt; is zero.</source>
          <target state="translated">&lt;strong&gt;values&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash; &lt;code&gt;input&lt;/code&gt; 为零时要使用的值。</target>
        </trans-unit>
        <trans-unit id="a0a6ed69f7bb031d32442c8207e6351c28728f97" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;values&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Scalar&lt;/em&gt;) &amp;ndash; N-D tensor or a Scalar containing the search value(s).</source>
          <target state="translated">&lt;strong&gt;值&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;标量&lt;/em&gt;）&amp;ndash; ND张量或包含搜索值的标量。</target>
        </trans-unit>
        <trans-unit id="068141ee62e0c741721a1ca247da7418337fa905" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;values&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;numpy.array&lt;/em&gt;&lt;em&gt;, or &lt;/em&gt;&lt;em&gt;string/blobname&lt;/em&gt;) &amp;ndash; Values to build histogram</source>
          <target state="translated">&lt;strong&gt;值&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;numpy.array&lt;/em&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;字符串/ blobname&lt;/em&gt;）&amp;ndash;用于构建直方图的值</target>
        </trans-unit>
        <trans-unit id="81346c9c646d8fdc4c0b129427baa5e0d2452356" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;values&lt;/strong&gt; (&lt;em&gt;array_like&lt;/em&gt;) &amp;ndash; Initial values for the tensor. Can be a list, tuple, NumPy &lt;code&gt;ndarray&lt;/code&gt;, scalar, and other types.</source>
          <target state="translated">&lt;strong&gt;values&lt;/strong&gt;（&lt;em&gt;array_like&lt;/em&gt;）&amp;ndash;张量的初始值。可以是列表，元组，NumPy &lt;code&gt;ndarray&lt;/code&gt; ，标量和其他类型。</target>
        </trans-unit>
        <trans-unit id="2514f73a2f02b616f12aa53311fb41936e3843bc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;vdim&lt;/strong&gt; &amp;ndash; total number of features in value. Default: None.</source>
          <target state="translated">&lt;strong&gt;vdim&lt;/strong&gt; &amp;ndash;价值&lt;strong&gt;要素&lt;/strong&gt;总数。默认值：无。</target>
        </trans-unit>
        <trans-unit id="819b9c6fd109a7c509127d79322ef0512ba65bd2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;vec1&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the first vector of the outer product</source>
          <target state="translated">&lt;strong&gt;vec1&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;外积的第一个向量</target>
        </trans-unit>
        <trans-unit id="4e73eb0cbf6f2a91979ca7e439e5aa7d715d0300" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;vec2&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; 1-D input vector</source>
          <target state="translated">&lt;strong&gt;vec2&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;一维输入向量</target>
        </trans-unit>
        <trans-unit id="131877debbb3166e16ce26108f5a3eed6fc2f019" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;vec2&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the second vector of the outer product</source>
          <target state="translated">&lt;strong&gt;vec2&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;外积的第二向量</target>
        </trans-unit>
        <trans-unit id="4d0559907de880bfcd717f1f78869a200f9027f0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;vec&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; a single vector represents the parameters of a model.</source>
          <target state="translated">&lt;strong&gt;vec&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;单个向量表示模型的参数。</target>
        </trans-unit>
        <trans-unit id="daede4c661612057da3d485c36e667e3f8d4dad0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;vec&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; vector to be multiplied</source>
          <target state="translated">&lt;strong&gt;vec&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;要相乘的向量</target>
        </trans-unit>
        <trans-unit id="ff9a7e40dc82a226dc8f29f776ec07c36e2e317b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;verbose&lt;/strong&gt; &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, turns on verbose logging of load steps.</source>
          <target state="translated">&lt;strong&gt;verbose（详细）&lt;/strong&gt; &amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则打开加载步骤的详细日志记录。</target>
        </trans-unit>
        <trans-unit id="3c7c98a393594108f301ee2c897c52950fcdeb33" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;verbose&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; If &lt;code&gt;True&lt;/code&gt;, prints a message to stdout for each update. Default: &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;verbose&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;如果为 &lt;code&gt;True&lt;/code&gt; ，则每次更新都会向stdout打印一条消息。默认值： &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2094bc9935f0b3c92d97f3cf874c37a89b5048a1" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;verbose&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Whether to print graph structure in console.</source>
          <target state="translated">&lt;strong&gt;详细&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;是否在控制台中打印图形结构。</target>
        </trans-unit>
        <trans-unit id="c171e7c8da79938ba578ce62f6d80ce02e508b76" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;verbose&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;default False&lt;/em&gt;) &amp;ndash; if specified, we will print out a debug description of the trace being exported.</source>
          <target state="translated">&lt;strong&gt;详细&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;默认为False&lt;/em&gt;）&amp;ndash;如果指定，我们将打印出正在导出的跟踪的调试描述。</target>
        </trans-unit>
        <trans-unit id="f3599691f4621301d1623dd0c2e7a30947cc6548" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;verbose&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If &lt;code&gt;False&lt;/code&gt;, mute messages about hitting local caches. Note that the message about first download cannot be muted. Does not have any effect if &lt;code&gt;source = 'local'&lt;/code&gt;. Default is &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;详细&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;如果为 &lt;code&gt;False&lt;/code&gt; ，则忽略有关命中本地缓存的消息。请注意，有关第一次下载的消息不能被静音。如果 &lt;code&gt;source = 'local'&lt;/code&gt; 则没有任何效果。默认值为 &lt;code&gt;True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c849d403c5b92168af96b652290de46caebea368" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;vertices&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; List of the 3D coordinates of vertices.</source>
          <target state="translated">&lt;strong&gt;顶点&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;）&amp;ndash;顶点的3D坐标列表。</target>
        </trans-unit>
        <trans-unit id="f13839df85806a7d212ed83a81c31127bf2c2237" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;vid_tensor&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;) &amp;ndash; Video data</source>
          <target state="translated">&lt;strong&gt;vid_tensor&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;）&amp;ndash;视频数据</target>
        </trans-unit>
        <trans-unit id="dc84a7d8a07edfd6dc468d5b20a74d4a8841fdbe" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;walltime&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; Optional override default walltime (time.time()) seconds after epoch of event</source>
          <target state="translated">&lt;strong&gt;walltime&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;可选的覆盖默认的walltime（time.time（））秒</target>
        </trans-unit>
        <trans-unit id="f41030565ca27ea1362cda461c42d37902d0ff5c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;walltime&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;) &amp;ndash; Optional override default walltime (time.time()) with seconds after epoch of event</source>
          <target state="translated">&lt;strong&gt;walltime&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;）&amp;ndash;可选，以事件发生后的秒数覆盖默认的walltime（time.time（））</target>
        </trans-unit>
        <trans-unit id="8173b0dc1b32bad0b60a75ecb20d002cd08f4f9e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;we want to quantize&lt;/strong&gt; (&lt;em&gt;that&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;我们想要量化&lt;/strong&gt;（&lt;em&gt;那个&lt;/em&gt;）&amp;ndash;</target>
        </trans-unit>
        <trans-unit id="f9d74e109cb0a14286b98abd2e21c589f1814826" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;weight&lt;/strong&gt; &amp;ndash; filters of shape</source>
          <target state="translated">&lt;strong&gt;重量&lt;/strong&gt;&amp;ndash;形状过滤器</target>
        </trans-unit>
        <trans-unit id="f9a334f0285a62a48b2313f31b6d8a7b66efc571" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;weight&lt;/strong&gt; &amp;ndash; quantized filters of shape</source>
          <target state="translated">&lt;strong&gt;重量&lt;/strong&gt;&amp;ndash;量化的形状过滤器</target>
        </trans-unit>
        <trans-unit id="ab4ea073acb19584f6540d88c6edee778c2582cc" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;weight&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a manual rescaling weight given to each class. If given, has to be a Tensor of size &lt;code&gt;C&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;重量&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;为每个班级分配的手动缩放重量。如果给定，则必须为 &lt;code&gt;C&lt;/code&gt; 的张量</target>
        </trans-unit>
        <trans-unit id="70e8975c00d113cb45edf143611e73c84cc5ce44" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;weight&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a manual rescaling weight given to each class. If given, it has to be a Tensor of size &lt;code&gt;C&lt;/code&gt;. Otherwise, it is treated as if having all ones.</source>
          <target state="translated">&lt;strong&gt;权重&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;分配给每个班级的手动缩放重量。如果给出的话，它必须是大小为 &lt;code&gt;C&lt;/code&gt; 的张量。否则，将其视为拥有全部。</target>
        </trans-unit>
        <trans-unit id="954e4537a940f3ba78802ce2a539fbef33a42e08" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;weight&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size &lt;code&gt;nbatch&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;重量&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;对每个批次元素的损失赋予的手动重新缩放重量。如果给定，则必须是大小为 &lt;code&gt;nbatch&lt;/code&gt; 的张量。</target>
        </trans-unit>
        <trans-unit id="a1f17f82ec381f64b173aaca9f8effdf46eb1689" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;weight&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;tensor&lt;/em&gt;) &amp;ndash; the weight for the interpolation formula</source>
          <target state="translated">&lt;strong&gt;权重&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;浮点&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;张量&lt;/em&gt;）&amp;ndash;插值公式的权重</target>
        </trans-unit>
        <trans-unit id="22154f27393e88b905a5d41b51a84b23ab22932b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;weight&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Quantized weight of type &lt;code&gt;torch.qint8&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;重量&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash; &lt;code&gt;torch.qint8&lt;/code&gt; 类型的量化重量</target>
        </trans-unit>
        <trans-unit id="c4cf9889c18f266792e6d5b0d19e359c339fffa5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;weight&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size</source>
          <target state="translated">&lt;strong&gt;weight&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;行数等于最大可能索引+ 1，列数等于嵌入大小的嵌入矩阵</target>
        </trans-unit>
        <trans-unit id="0b17239462ca378c70577124e3fcde25d087b526" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;weight&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a manual rescaling weight given to each class. If given, has to be a Tensor of size &lt;code&gt;C&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;重量&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;为每个班级分配的手动缩放重量。如果给定，则必须为 &lt;code&gt;C&lt;/code&gt; 的张量</target>
        </trans-unit>
        <trans-unit id="ceb298880b72c5ea4b42cc8ffeba86de3610e3c0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;weight&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; a manual rescaling weight if provided it&amp;rsquo;s repeated to match input tensor shape</source>
          <target state="translated">&lt;strong&gt;weight&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;手动调整比例的权重（如果重复进行以匹配输入张量形状）</target>
        </trans-unit>
        <trans-unit id="4ae1b6ef1e362f74c30172fdf4ed58e3642a6d88" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;weight_decay&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; weight decay (L2 penalty) (default: 0)</source>
          <target state="translated">&lt;strong&gt;weight_decay&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;重量衰减（L2惩罚）（默认值：0）</target>
        </trans-unit>
        <trans-unit id="0c1b82ff67c5795bc63f72d0dc340e41b6ccd345" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;weight_decay&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; weight decay coefficient (default: 1e-2)</source>
          <target state="translated">&lt;strong&gt;weight_decay&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#float&quot;&gt;float &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;权重衰减系数（默认值：1e-2）</target>
        </trans-unit>
        <trans-unit id="30220330ec8f9548f71ee65df724e11d72d2b160" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;weights&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; optional, weight for each value in the input tensor. Should be of same size as input tensor.</source>
          <target state="translated">&lt;strong&gt;weights&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;可选，输入张量中每个值的权重。应具有与输入张量相同的大小。</target>
        </trans-unit>
        <trans-unit id="98654312450d33fec9fb0161447c6249d66d1d84" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;weights&lt;/strong&gt; (&lt;em&gt;sequence&lt;/em&gt;) &amp;ndash; a sequence of weights, not necessary summing up to one</source>
          <target state="translated">&lt;strong&gt;权重&lt;/strong&gt;（&lt;em&gt;序列&lt;/em&gt;）&amp;ndash;&lt;strong&gt;权重&lt;/strong&gt;&lt;em&gt;序列&lt;/em&gt;，不必累加一个</target>
        </trans-unit>
        <trans-unit id="1dacae6023a61e8210dc97a03d769ec15092e68e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;win_length&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the size of window frame and STFT filter. Default: &lt;code&gt;None&lt;/code&gt; (treated as equal to &lt;code&gt;n_fft&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;win_length&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;窗口框架和STFT过滤器的大小。默认值： &lt;code&gt;None&lt;/code&gt; （视为等于 &lt;code&gt;n_fft&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="6b1bf34b632ccb28e0c558a94dd1b77ac54ce12b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;win_length&lt;/strong&gt; (&lt;em&gt;Optional&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; The size of window frame and STFT filter. (Default: &lt;code&gt;n_fft&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;win_length&lt;/strong&gt;（&lt;em&gt;可选&lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;窗口框架和STFT过滤器的大小。（默认值： &lt;code&gt;n_fft&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="7d9a9a55c05cd5b2885fb65d4e15a0e9d0340b49" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;window&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; the optional window function. Default: &lt;code&gt;None&lt;/code&gt; (treated as window of all</source>
          <target state="translated">&lt;strong&gt;window&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;可选的窗口功能。默认值： &lt;code&gt;None&lt;/code&gt; （视为所有窗口</target>
        </trans-unit>
        <trans-unit id="f0f614133f5e416c9d9f06c46de251960734b001" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;window&lt;/strong&gt; (&lt;em&gt;Optional&lt;/em&gt;&lt;em&gt;[&lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor&lt;/a&gt;&lt;em&gt;]&lt;/em&gt;) &amp;ndash; The optional window function. (Default: &lt;code&gt;torch.ones(win_length)&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;window&lt;/strong&gt;（&lt;em&gt;可选&lt;/em&gt;&lt;em&gt;[ &lt;/em&gt;&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;torch.Tensor &lt;/a&gt;&lt;em&gt;]&lt;/em&gt;）&amp;ndash;可选的窗口功能。（默认： &lt;code&gt;torch.ones(win_length)&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="49b7cd3881023ff5bbecb687d60192ac07560fe8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;window_length&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; length of the window.</source>
          <target state="translated">&lt;strong&gt;window_length&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;窗口的长度。</target>
        </trans-unit>
        <trans-unit id="4e3bec63120999c913a0f66ab321260fd1dfb2c9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;window_length&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; the size of returned window</source>
          <target state="translated">&lt;strong&gt;window_length&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;返回窗口的大小</target>
        </trans-unit>
        <trans-unit id="9ccfcc0cdca44901901265e154f3e79d9b261692" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;with_cuda&lt;/strong&gt; &amp;ndash; Determines whether CUDA headers and libraries are added to the build. If set to &lt;code&gt;None&lt;/code&gt; (default), this value is automatically determined based on the existence of &lt;code&gt;.cu&lt;/code&gt; or &lt;code&gt;.cuh&lt;/code&gt; in &lt;code&gt;sources&lt;/code&gt;. Set it to &lt;code&gt;True`&lt;/code&gt; to force CUDA headers and libraries to be included.</source>
          <target state="translated">&lt;strong&gt;with_cuda&lt;/strong&gt; &amp;ndash;确定是否将CUDA标头和库添加到构建中。如果设置为 &lt;code&gt;None&lt;/code&gt; （默认值），该值被自动根据的存在与否决定 &lt;code&gt;.cu&lt;/code&gt; 或 &lt;code&gt;.cuh&lt;/code&gt; 在 &lt;code&gt;sources&lt;/code&gt; 。将其设置为 &lt;code&gt;True`&lt;/code&gt; 以强制包含CUDA标头和库。</target>
        </trans-unit>
        <trans-unit id="bd3ed9cacd1f8bdf1897ff6845942937dc2aa6cd" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;with_cuda&lt;/strong&gt; &amp;ndash; Determines whether CUDA headers and libraries are added to the build. If set to &lt;code&gt;None&lt;/code&gt; (default), this value is automatically determined based on whether &lt;code&gt;cuda_sources&lt;/code&gt; is provided. Set it to &lt;code&gt;True&lt;/code&gt; to force CUDA headers and libraries to be included.</source>
          <target state="translated">&lt;strong&gt;with_cuda&lt;/strong&gt; &amp;ndash;确定是否将CUDA标头和库添加到构建中。如果设置为 &lt;code&gt;None&lt;/code&gt; （默认），则根据是否提供 &lt;code&gt;cuda_sources&lt;/code&gt; 自动确定此值。将其设置为 &lt;code&gt;True&lt;/code&gt; 可强制包含CUDA标头和库。</target>
        </trans-unit>
        <trans-unit id="4815336ed54522c0846013f45d4ba87f409a2251" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;with_pytorch_error_handling&lt;/strong&gt; &amp;ndash; Determines whether pytorch error and warning macros are handled by pytorch instead of pybind. To do this, each function &lt;code&gt;foo&lt;/code&gt; is called via an intermediary &lt;code&gt;_safe_foo&lt;/code&gt; function. This redirection might cause issues in obscure cases of cpp. This flag should be set to &lt;code&gt;False&lt;/code&gt; when this redirect causes issues.</source>
          <target state="translated">&lt;strong&gt;with_pytorch_error_handling&lt;/strong&gt; &amp;ndash;确定pytorch而不是pybind处理pytorch错误和警告宏。为此，每个函数 &lt;code&gt;foo&lt;/code&gt; 都通过中介 &lt;code&gt;_safe_foo&lt;/code&gt; 函数调用。这种重定向在cpp晦涩的情况下可能会引起问题。当此重定向导致问题时，应将此标志设置为 &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4a19e10a55ff5cbaf67b76afd814820dc5724ee4" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;with_replacement&lt;/strong&gt; (&lt;em&gt;boolean&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; whether to allow duplication in combination</source>
          <target state="translated">&lt;strong&gt;with_replacement&lt;/strong&gt;（&lt;em&gt;boolean &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;是否允许组合复制</target>
        </trans-unit>
        <trans-unit id="5415451ab70a55f73cd0137592447b005d436e95" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;with_stack&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; record source information (file and line number) for the ops</source>
          <target state="translated">&lt;strong&gt;with_stack&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;记录操作的源信息（文件和行号）</target>
        </trans-unit>
        <trans-unit id="4dc6dc49be43948f81126e0018a7bfcb14810907" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;worker_init_fn&lt;/strong&gt; (&lt;em&gt;callable&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; If not &lt;code&gt;None&lt;/code&gt;, this will be called on each worker subprocess with the worker id (an int in &lt;code&gt;[0, num_workers - 1]&lt;/code&gt;) as input, after seeding and before data loading. (default: &lt;code&gt;None&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;worker_init_fn&lt;/strong&gt;（&lt;em&gt;callable &lt;/em&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;）&amp;ndash;如果不是 &lt;code&gt;None&lt;/code&gt; ，则将在播种之后和数据加载之前，在每个工作程序子 &lt;code&gt;[0, num_workers - 1]&lt;/code&gt; 上以工作程序ID（在[0，num_workers-1]中的int ）作为输入来调用此方法。（默认值： &lt;code&gt;None&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="d520029ad1a21541a6983cd1bbd1d13e71672aaf" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;worker_name&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;) &amp;ndash; the string name of a worker. If &lt;code&gt;None&lt;/code&gt;, return the the id of the current worker. (default &lt;code&gt;None&lt;/code&gt;)</source>
          <target state="translated">&lt;strong&gt;worker_name&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#str&quot;&gt;str&lt;/a&gt;）&amp;ndash;工人的字符串名称。如果为 &lt;code&gt;None&lt;/code&gt; ，则返回当前工作程序的ID。（默认为 &lt;code&gt;None&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="bb05a633103b92865e059e320074b5a31b7d0338" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;world_size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The number of workers in the group.</source>
          <target state="translated">&lt;strong&gt;world_size&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;组中的工人数。</target>
        </trans-unit>
        <trans-unit id="8944fd0aa7e0a543d0eb0d57230c998dddc0f01f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;world_size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The total number of processes using the store</source>
          <target state="translated">&lt;strong&gt;world_size&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;使用商店的进程总数</target>
        </trans-unit>
        <trans-unit id="4b40d216b7322f8dca0f99cbebec725fe60f6e0b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;world_size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; The total number of store users (number of clients + 1 for the server).</source>
          <target state="translated">&lt;strong&gt;world_size&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;商店用户总数（客户端数+服务器的1）。</target>
        </trans-unit>
        <trans-unit id="d09ea2fd888d7766224d3eb255ea7484752c0c9d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;world_size&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Number of processes participating in the job. Required if &lt;code&gt;store&lt;/code&gt; is specified.</source>
          <target state="translated">&lt;strong&gt;world_size&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;参与作业的进程数。如果指定了 &lt;code&gt;store&lt;/code&gt; 则为必需。</target>
        </trans-unit>
        <trans-unit id="e6ab46733a3cf79484335cfec30ffadc77125f0b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;wrap&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; the diagonal &amp;lsquo;wrapped&amp;rsquo; after N columns for tall matrices.</source>
          <target state="translated">&lt;strong&gt;wrap&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）-高矩阵在N列后对角地&amp;ldquo;包裹&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="039b003cafbdb956a808db4a8f6a45acd6067df8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;x1&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; input tensor of shape</source>
          <target state="translated">&lt;strong&gt;x1&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;形状的输入张量</target>
        </trans-unit>
        <trans-unit id="4e34f43957ff13fc9b613484e5941ff19da8a3ab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;x1&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; First input.</source>
          <target state="translated">&lt;strong&gt;x1&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;第一个输入。</target>
        </trans-unit>
        <trans-unit id="8c4e3cfe4db7965d9ad32d9b63db999f3ce5916d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;x2&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; input tensor of shape</source>
          <target state="translated">&lt;strong&gt;x2&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;形状的输入张量</target>
        </trans-unit>
        <trans-unit id="3da2f4cab51d1d5ba9a574263a3bb5682b0875ae" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;x2&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Second input (of size matching x1).</source>
          <target state="translated">&lt;strong&gt;x2&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;第二个输入（大小匹配x1）。</target>
        </trans-unit>
        <trans-unit id="b14756a562cc7fda7e1454cc0eb57d083473e78a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;x&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; 1-D input tensor.</source>
          <target state="translated">&lt;strong&gt;x&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;一维输入张量。</target>
        </trans-unit>
        <trans-unit id="8604fc9ce7b9564333699bb4a5095a672bbc3ef3" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;x&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The points at which the function &lt;code&gt;y&lt;/code&gt; is sampled. If &lt;code&gt;x&lt;/code&gt; is not in ascending order, intervals on which it is decreasing contribute negatively to the estimated integral (i.e., the convention</source>
          <target state="translated">&lt;strong&gt;x&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;）&amp;ndash;对函数 &lt;code&gt;y&lt;/code&gt; 进行采样的点。如果 &lt;code&gt;x&lt;/code&gt; 不按升序排列，则x减小的间隔会对估计的积分产生负面影响（即，惯例</target>
        </trans-unit>
        <trans-unit id="e7876837160f02de9f8a91a92f6b6e1b84b26b5d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;x&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Scalar&lt;/em&gt;) &amp;ndash; value (if :attr:x is a scalar) or values selected at indices where &lt;code&gt;condition&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;x&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;标量&lt;/em&gt;）&amp;ndash;值（如果：attr：x是标量）或在 &lt;code&gt;condition&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; 的索引处选择的值</target>
        </trans-unit>
        <trans-unit id="c68f34fbda8cdacd4cc4e099cfcd5fcf6cc52209" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;y&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; The values of the function to integrate</source>
          <target state="translated">&lt;strong&gt;y&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;要积分的函数的值</target>
        </trans-unit>
        <trans-unit id="72163565538ba72a52711498c5bfc9bb0d1c872a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;y&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;Scalar&lt;/em&gt;) &amp;ndash; value (if :attr:x is a scalar) or values selected at indices where &lt;code&gt;condition&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;y&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;em&gt;标量&lt;/em&gt;）&amp;ndash;值（如果：attr：x是标量）或在 &lt;code&gt;condition&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; 的索引处选择的值</target>
        </trans-unit>
        <trans-unit id="3ba6c96ac6975a47208ccc9a0b52c75bb531c910" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;zero_infinity&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Whether to zero infinite losses and the associated gradients. Default: &lt;code&gt;False&lt;/code&gt; Infinite losses mainly occur when the inputs are too short to be aligned to the targets.</source>
          <target state="translated">&lt;strong&gt;zero_infinity&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;是否将无限大损失和相关的梯度归零。默认值： &lt;code&gt;False&lt;/code&gt; 无限损失主要发生在输入太短而无法与目标对齐时。</target>
        </trans-unit>
        <trans-unit id="8638d6633ab7a4cf4da345720cc4bf2ac60e6815" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;zero_point, dtype&lt;/strong&gt; (&lt;em&gt;`scale`&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;) &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;zero_point, dtype&lt;/strong&gt; (&lt;em&gt;`scale`&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;) &amp;ndash;</target>
        </trans-unit>
        <trans-unit id="a1615df751082d4be9fb7cb31b4bb34695adfb0b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;zero_point&lt;/strong&gt; &amp;ndash; quantization zero point of the output tensor</source>
          <target state="translated">&lt;strong&gt;zero_point&lt;/strong&gt; &amp;ndash;输出张量的量化零点</target>
        </trans-unit>
        <trans-unit id="7247c88b2e908aea42f2ec3b7339d9a256569c09" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;zero_point&lt;/strong&gt; &amp;ndash; quantization zero_point for the output. Default: 0</source>
          <target state="translated">&lt;strong&gt;zero_point&lt;/strong&gt; &amp;ndash;输出的量化zero_point。默认值：0</target>
        </trans-unit>
        <trans-unit id="c2f7a803cb5fc582e2a288ee7168564bc33aa879" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;zero_point&lt;/strong&gt; &amp;ndash; zero_point of output Quantized Tensor</source>
          <target state="translated">&lt;strong&gt;zero_point&lt;/strong&gt; &amp;ndash;输出量化张量的zero_point</target>
        </trans-unit>
        <trans-unit id="223a725fe4e09c854eee949c752211f3d9820057" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;zero_point&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; offset in integer value that maps to float zero</source>
          <target state="translated">&lt;strong&gt;zero_point&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;以整数值表示的偏移量，该值映射为浮点数零</target>
        </trans-unit>
        <trans-unit id="4989efaac53b9d7d2d13c26d99d3def10531a636" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;zero_point&lt;/strong&gt; (&lt;em&gt;long&lt;/em&gt;) &amp;ndash; output zero point. If None, derived from the input zero_point</source>
          <target state="translated">&lt;strong&gt;zero_point&lt;/strong&gt;（&lt;em&gt;long&lt;/em&gt;）&amp;ndash;输出零点。如果为None，则从输入zero_point派生</target>
        </trans-unit>
        <trans-unit id="429279404949d69c49587e68892b656c79b73f46" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;zero_point&lt;/strong&gt; - quantization zero point of the output, type: long.</source>
          <target state="translated">&lt;strong&gt;zero_point-&lt;/strong&gt;输出的量化零点，键入：long。</target>
        </trans-unit>
        <trans-unit id="ffac724f17c0594d2ca1937a69bcfaf236dd1829" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;zero_points&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; integer 1D tensor of offset to use, size should match &lt;code&gt;input.size(axis)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;zero_points&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;）&amp;ndash;要使用的整数一维偏移量张量，大小应与 &lt;code&gt;input.size(axis)&lt;/code&gt; 匹配</target>
        </trans-unit>
        <trans-unit id="47893dd3bbacac551e16f1ea5a7f8a26eeaf9113" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;{input}&lt;/strong&gt; &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;{input}&lt;/strong&gt; &amp;ndash;</target>
        </trans-unit>
        <trans-unit id="20c4529cd2a28cc535afcdbc8ecd226b45340d70" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;{out}&lt;/strong&gt; &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;{out}&lt;/strong&gt; &amp;ndash;</target>
        </trans-unit>
        <trans-unit id="b7c764a4f83bf4672db31e3ba964ce2216c4e083" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Bilinear.bias&lt;/strong&gt; &amp;ndash; the learnable bias of the module of shape</source>
          <target state="translated">&lt;strong&gt;〜Bilinear.bias&lt;/strong&gt; &amp;ndash;形状模块的可学习偏差</target>
        </trans-unit>
        <trans-unit id="f89b33be0a086eb9b0633fea112be0da1cf2bd2d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Bilinear.weight&lt;/strong&gt; &amp;ndash; the learnable weights of the module of shape</source>
          <target state="translated">&lt;strong&gt;〜Bilinear.weight&lt;/strong&gt; &amp;ndash;形状模块的可学习权重</target>
        </trans-unit>
        <trans-unit id="786e19415c7aa959d92f4e6681a37e6161c762ce" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Conv1d.bias&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the learnable bias of the module of shape (out_channels). If &lt;code&gt;bias&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the values of these weights are sampled from</source>
          <target state="translated">&lt;strong&gt;〜Conv1d.bias&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;形状模块（out_channels）的可学习偏差。如果 &lt;code&gt;bias&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则从这些样本中采样这些权重的值。</target>
        </trans-unit>
        <trans-unit id="e06e33e99b955e617b52597dc55cd653ebf70f55" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Conv1d.scale&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; scalar for the output scale</source>
          <target state="translated">&lt;strong&gt;〜Conv1d.scale&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;输出比例的标量</target>
        </trans-unit>
        <trans-unit id="54e6f2689f69e48b8f9ee81ce716072bcf839a90" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Conv1d.weight&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the learnable weights of the module of shape</source>
          <target state="translated">&lt;strong&gt;〜Conv1d.weight&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;形状模块的可学习重量</target>
        </trans-unit>
        <trans-unit id="2b1b80d43979766fbec56a2cf03434689febed6b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Conv1d.weight&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; packed tensor derived from the learnable weight parameter.</source>
          <target state="translated">&lt;strong&gt;〜Conv1d.weight&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;从可学习的权重参数派生的压缩张量。</target>
        </trans-unit>
        <trans-unit id="7fe271f7c736601cbe58a60730d1ba367c3b713a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Conv1d.zero_point&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; scalar for the output zero point</source>
          <target state="translated">&lt;strong&gt;〜Conv1d.zero_point&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;输出零点的标量</target>
        </trans-unit>
        <trans-unit id="cab585d0b3b7a1e8311f27f86e9c9020f1f22fd7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Conv2d.bias&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the learnable bias of the module of shape (out_channels). If &lt;code&gt;bias&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the values of these weights are sampled from</source>
          <target state="translated">&lt;strong&gt;〜Conv2d.bias&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;形状模块（out_channels）的可学习偏差。如果 &lt;code&gt;bias&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则从这些样本中采样这些权重的值。</target>
        </trans-unit>
        <trans-unit id="491e2d85ab2736d7aafefcdd0aaf9951c3be3bd0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Conv2d.scale&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; scalar for the output scale</source>
          <target state="translated">&lt;strong&gt;〜Conv2d.scale&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;输出比例的标量</target>
        </trans-unit>
        <trans-unit id="1dbb931cba576b031f6690c20c53e86c7d5dd799" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Conv2d.weight&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the learnable weights of the module of shape</source>
          <target state="translated">&lt;strong&gt;〜Conv2d.weight&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;形状模块的可学习重量</target>
        </trans-unit>
        <trans-unit id="bbb22808d75f21da185917b5905c1df4c0e208f0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Conv2d.weight&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; packed tensor derived from the learnable weight parameter.</source>
          <target state="translated">&lt;strong&gt;〜Conv2d.weight&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;从可学习的权重参数派生的压缩张量。</target>
        </trans-unit>
        <trans-unit id="b3438ce5dca3a7cf14dc3bcf4aa9bbd6b0f92e36" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Conv2d.weight_fake_quant&lt;/strong&gt; &amp;ndash; fake quant module for weight</source>
          <target state="translated">&lt;strong&gt;〜Conv2d.weight_fake_quant&lt;/strong&gt; &amp;ndash;伪造的重量&lt;strong&gt;量化&lt;/strong&gt;模块</target>
        </trans-unit>
        <trans-unit id="7b8c7c333216763559ed33e7c4b13fa4f678413a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Conv2d.zero_point&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; scalar for the output zero point</source>
          <target state="translated">&lt;strong&gt;〜Conv2d.zero_point&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;输出零点的标量</target>
        </trans-unit>
        <trans-unit id="e2b641a6ffee3b2c43dbc8fe09c7d0c721cd8b22" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Conv3d.bias&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the learnable bias of the module of shape (out_channels). If &lt;code&gt;bias&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the values of these weights are sampled from</source>
          <target state="translated">&lt;strong&gt;〜Conv3d.bias&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;形状模块（out_channels）的可学习偏差。如果 &lt;code&gt;bias&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则从这些样本中采样这些权重的值。</target>
        </trans-unit>
        <trans-unit id="aa18d65289a64a7ca20ed8be2d410df1bdc536a6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Conv3d.scale&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; scalar for the output scale</source>
          <target state="translated">&lt;strong&gt;〜Conv3d.scale&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;输出比例的标量</target>
        </trans-unit>
        <trans-unit id="a72fbf59ce9ea685622e291418b9e3268a772b76" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Conv3d.weight&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the learnable weights of the module of shape</source>
          <target state="translated">&lt;strong&gt;〜Conv3d.weight&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;形状模块的可学习重量</target>
        </trans-unit>
        <trans-unit id="a3ad7c44a3e4e0fbec86bf00ce697cd744bb9a39" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Conv3d.weight&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; packed tensor derived from the learnable weight parameter.</source>
          <target state="translated">&lt;strong&gt;〜Conv3d.weight&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;从可学习的weight参数派生的压缩张量。</target>
        </trans-unit>
        <trans-unit id="99396f2687699c53375347ac0faddcdc798b2e6a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Conv3d.zero_point&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; scalar for the output zero point</source>
          <target state="translated">&lt;strong&gt;〜Conv3d.zero_point&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;输出零点的标量</target>
        </trans-unit>
        <trans-unit id="378d1f43230566f1da52f54e6a88d20a5d7790c7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~ConvBn2d.freeze_bn&lt;/strong&gt; &amp;ndash;</source>
          <target state="translated">&lt;strong&gt;~ConvBn2d.freeze_bn&lt;/strong&gt; &amp;ndash;</target>
        </trans-unit>
        <trans-unit id="d3bca18d068926bc6367e11701b6f03abca19647" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~ConvBn2d.weight_fake_quant&lt;/strong&gt; &amp;ndash; fake quant module for weight</source>
          <target state="translated">&lt;strong&gt;〜ConvBn2d.weight_fake_quant&lt;/strong&gt; &amp;ndash;伪造的重量&lt;strong&gt;量化&lt;/strong&gt;模块</target>
        </trans-unit>
        <trans-unit id="a55571977e6df390a36d65d847d2bb319eff0223" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~ConvBnReLU2d.weight_fake_quant&lt;/strong&gt; &amp;ndash; fake quant module for weight</source>
          <target state="translated">&lt;strong&gt;〜ConvBnReLU2d.weight_fake_quant&lt;/strong&gt; &amp;ndash;伪造的重量&lt;strong&gt;量化&lt;/strong&gt;模块</target>
        </trans-unit>
        <trans-unit id="ebcdaceff86165d2388fbceb90c8330fe2a56cad" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~ConvReLU2d.weight_fake_quant&lt;/strong&gt; &amp;ndash; fake quant module for weight</source>
          <target state="translated">&lt;strong&gt;〜ConvReLU2d.weight_fake_quant&lt;/strong&gt; &amp;ndash;伪造的重量&lt;strong&gt;量化&lt;/strong&gt;模块</target>
        </trans-unit>
        <trans-unit id="e9364c34d7f64012a1bdca3fb740fa3d412f113d" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~ConvTranspose1d.bias&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the learnable bias of the module of shape (out_channels). If &lt;code&gt;bias&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the values of these weights are sampled from</source>
          <target state="translated">&lt;strong&gt;〜ConvTranspose1d.bias&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;形状模块（out_channels）的可学习偏差。如果 &lt;code&gt;bias&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则从这些样本中采样这些权重的值。</target>
        </trans-unit>
        <trans-unit id="a79a77bf8698fc3e5d49fe813734ef802e085354" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~ConvTranspose1d.weight&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the learnable weights of the module of shape</source>
          <target state="translated">&lt;strong&gt;〜ConvTranspose1d.weight&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;形状模块的可学习重量</target>
        </trans-unit>
        <trans-unit id="aa3f13f16be4b75d562347e054c32c5197265a17" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~ConvTranspose2d.bias&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the learnable bias of the module of shape (out_channels) If &lt;code&gt;bias&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the values of these weights are sampled from</source>
          <target state="translated">&lt;strong&gt;〜ConvTranspose2d.bias&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;形状模块的可学习偏差（out_channels）如果 &lt;code&gt;bias&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则从以下位置采样这些权重的值</target>
        </trans-unit>
        <trans-unit id="53cb49c11f8899c6db6dd4eb8c1f8305369a4456" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~ConvTranspose2d.weight&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the learnable weights of the module of shape</source>
          <target state="translated">&lt;strong&gt;〜ConvTranspose2d.weight&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;形状模块的可学习权重</target>
        </trans-unit>
        <trans-unit id="114696aaba144dbc47af743af9b2db6c274492f0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~ConvTranspose3d.bias&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the learnable bias of the module of shape (out_channels) If &lt;code&gt;bias&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the values of these weights are sampled from</source>
          <target state="translated">&lt;strong&gt;〜ConvTranspose3d.bias&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;形状模块的可学习偏差（out_channels）如果 &lt;code&gt;bias&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则这些权重的值将从</target>
        </trans-unit>
        <trans-unit id="b02b6fb26cf3d035db5c31efc0adbe6ef4663d46" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~ConvTranspose3d.weight&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the learnable weights of the module of shape</source>
          <target state="translated">&lt;strong&gt;〜ConvTranspose3d.weight&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;形状模块的可学习权重</target>
        </trans-unit>
        <trans-unit id="97aab65ac8b5ca1756a2ee2ae4334c7bb0f4c4cb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~DataParallel.module&lt;/strong&gt; (&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;Module&lt;/a&gt;) &amp;ndash; the module to be parallelized</source>
          <target state="translated">&lt;strong&gt;〜DataParallel.module&lt;/strong&gt;（&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;Module&lt;/a&gt;）&amp;ndash;要并行化的模块</target>
        </trans-unit>
        <trans-unit id="7badf98694d23be19ae92b4f0b3ca4ed320885bb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~DistributedDataParallel.module&lt;/strong&gt; (&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;Module&lt;/a&gt;) &amp;ndash; the module to be parallelized.</source>
          <target state="translated">&lt;strong&gt;〜DistributedDataParallel.module&lt;/strong&gt;（&lt;a href=&quot;torch.nn.module#torch.nn.Module&quot;&gt;Module&lt;/a&gt;）&amp;ndash;要并行化的模块。</target>
        </trans-unit>
        <trans-unit id="4a07a3b857af99d25c9450da6034aa156ec65dd7" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Embedding.weight&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from</source>
          <target state="translated">&lt;strong&gt;〜Embedding.weight&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;） -形状的模块的可学习权重（num_embeddings，embedding_dim）从初始化</target>
        </trans-unit>
        <trans-unit id="4792bf3140649d4ebb921e27eb721fd115e1989e" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~EmbeddingBag.weight&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the learnable weights of the module of shape &lt;code&gt;(num_embeddings, embedding_dim)&lt;/code&gt; initialized from</source>
          <target state="translated">&lt;strong&gt;〜EmbeddingBag.weight&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;） -形状的模块的可学习权重 &lt;code&gt;(num_embeddings, embedding_dim)&lt;/code&gt; 从初始化</target>
        </trans-unit>
        <trans-unit id="469ff81b0263f1ef2386e2d3a21c2873edb34512" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~FakeQuantize.observer&lt;/strong&gt; (&lt;a href=&quot;generated/torch.nn.module#torch.nn.Module&quot;&gt;Module&lt;/a&gt;) &amp;ndash; User provided module that collects statistics on the input tensor and provides a method to calculate scale and zero-point.</source>
          <target state="translated">&lt;strong&gt;〜FakeQuantize.observer&lt;/strong&gt;（&lt;a href=&quot;generated/torch.nn.module#torch.nn.Module&quot;&gt;模块&lt;/a&gt;）&amp;ndash;用户提供的模块，该模块收集输入张量的统计信息，并提供一种计算标度和零点的方法。</target>
        </trans-unit>
        <trans-unit id="486db63a3b0eb7b18fc654f2a679c9e1761d4529" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~GRU.bias_hh_l[k]&lt;/strong&gt; &amp;ndash; the learnable hidden-hidden bias of the</source>
          <target state="translated">&lt;strong&gt;〜GRU.bias_hh_l [k]&lt;/strong&gt; &amp;ndash;的可学习的隐藏偏见</target>
        </trans-unit>
        <trans-unit id="568c0d45242b2e051763f666ab66932f82363330" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~GRU.bias_ih_l[k]&lt;/strong&gt; &amp;ndash; the learnable input-hidden bias of the</source>
          <target state="translated">&lt;strong&gt;〜GRU.bias_ih_l [k]&lt;/strong&gt; &amp;ndash;可学习的输入隐藏偏差</target>
        </trans-unit>
        <trans-unit id="b93874bd3ab7dfbc56dd2f71474633014b2b0a34" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~GRU.weight_hh_l[k]&lt;/strong&gt; &amp;ndash; the learnable hidden-hidden weights of the</source>
          <target state="translated">&lt;strong&gt;〜GRU.weight_hh_l [k]&lt;/strong&gt; &amp;ndash;的可学习的隐藏权重</target>
        </trans-unit>
        <trans-unit id="9a37d1757288e74e92d126a36c7070cd51271da9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~GRU.weight_ih_l[k]&lt;/strong&gt; &amp;ndash; the learnable input-hidden weights of the</source>
          <target state="translated">&lt;strong&gt;〜GRU.weight_ih_l [k]&lt;/strong&gt; &amp;ndash;可学习的输入隐藏权重</target>
        </trans-unit>
        <trans-unit id="397fce9905ce991267cadb4abd4e3c29554129d2" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~GRUCell.bias_hh&lt;/strong&gt; &amp;ndash; the learnable hidden-hidden bias, of shape &lt;code&gt;(3*hidden_size)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;〜GRUCell.bias_hh&lt;/strong&gt; &amp;ndash;可学习的隐藏偏差，形状为 &lt;code&gt;(3*hidden_size)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="98348f5219246cc6f34a9fbbfd7a43291f274e65" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~GRUCell.bias_ih&lt;/strong&gt; &amp;ndash; the learnable input-hidden bias, of shape &lt;code&gt;(3*hidden_size)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;〜GRUCell.bias_ih&lt;/strong&gt; &amp;ndash;可学习的输入隐藏偏差，形状为 &lt;code&gt;(3*hidden_size)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="4b4ebd2844961149a79509e03328089d66209c3c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~GRUCell.weight_hh&lt;/strong&gt; &amp;ndash; the learnable hidden-hidden weights, of shape &lt;code&gt;(3*hidden_size, hidden_size)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;〜GRUCell.weight_hh&lt;/strong&gt; &amp;ndash;可学习的隐藏重量，形状为 &lt;code&gt;(3*hidden_size, hidden_size)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="3cae575d5f85368dddcd51b697fe12eaeafcac2f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~GRUCell.weight_ih&lt;/strong&gt; &amp;ndash; the learnable input-hidden weights, of shape &lt;code&gt;(3*hidden_size, input_size)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;〜GRUCell.weight_ih&lt;/strong&gt; &amp;ndash;可学习的输入隐藏权重，形状为 &lt;code&gt;(3*hidden_size, input_size)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="67b82474914eb62986326c4401fa4d0253bcf5b9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~LSTM.bias_hh_l[k]&lt;/strong&gt; &amp;ndash; the learnable hidden-hidden bias of the</source>
          <target state="translated">&lt;strong&gt;〜LSTM.bias_hh_l [k]&lt;/strong&gt; &amp;ndash;的可学习的隐藏偏见</target>
        </trans-unit>
        <trans-unit id="418dca198dc36225a9737c2df2779e304936c820" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~LSTM.bias_ih_l[k]&lt;/strong&gt; &amp;ndash; the learnable input-hidden bias of the</source>
          <target state="translated">&lt;strong&gt;〜LSTM.bias_ih_l [k]&lt;/strong&gt; &amp;ndash;可学习的输入隐藏偏差</target>
        </trans-unit>
        <trans-unit id="761d7bd5940b869c800a8cec2a3765c298134c32" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~LSTM.weight_hh_l[k]&lt;/strong&gt; &amp;ndash; the learnable hidden-hidden weights of the</source>
          <target state="translated">&lt;strong&gt;〜LSTM.weight_hh_l [k]&lt;/strong&gt; &amp;ndash;可以学习的隐藏权重</target>
        </trans-unit>
        <trans-unit id="8ba84e472b18b379b7629198d242e16cefb87b3a" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~LSTM.weight_ih_l[k]&lt;/strong&gt; &amp;ndash; the learnable input-hidden weights of the</source>
          <target state="translated">&lt;strong&gt;〜LSTM.weight_ih_l [k]&lt;/strong&gt; &amp;ndash;的可学习的隐藏输入权重</target>
        </trans-unit>
        <trans-unit id="e9139487e553f7287b9b802b6302d1d46e37ac6c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~LSTMCell.bias_hh&lt;/strong&gt; &amp;ndash; the learnable hidden-hidden bias, of shape &lt;code&gt;(4*hidden_size)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;〜LSTMCell.bias_hh&lt;/strong&gt; &amp;ndash;可学习的隐藏偏差，形状为 &lt;code&gt;(4*hidden_size)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="896732c137e496c5eb6b474667fdc650df4d1ba0" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~LSTMCell.bias_ih&lt;/strong&gt; &amp;ndash; the learnable input-hidden bias, of shape &lt;code&gt;(4*hidden_size)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;〜LSTMCell.bias_ih&lt;/strong&gt; &amp;ndash;可学习的输入隐藏偏差，形状为 &lt;code&gt;(4*hidden_size)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="83260aa680a5b73dc2123fca87588a53fe5ecabb" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~LSTMCell.weight_hh&lt;/strong&gt; &amp;ndash; the learnable hidden-hidden weights, of shape &lt;code&gt;(4*hidden_size, hidden_size)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;〜LSTMCell.weight_hh&lt;/strong&gt; &amp;ndash;可学习的隐藏重量，形状为 &lt;code&gt;(4*hidden_size, hidden_size)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="ef0a5b92b7bdcb40d6493138f43418d1354cc221" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~LSTMCell.weight_ih&lt;/strong&gt; &amp;ndash; the learnable input-hidden weights, of shape &lt;code&gt;(4*hidden_size, input_size)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;〜LSTMCell.weight_ih&lt;/strong&gt; &amp;ndash;可学习的输入隐藏权重，形状为 &lt;code&gt;(4*hidden_size, input_size)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="d0bff85ad6b7e10c45ded2385264495acab8f4ab" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Linear.bias&lt;/strong&gt; &amp;ndash; the learnable bias of the module of shape</source>
          <target state="translated">&lt;strong&gt;〜Linear.bias&lt;/strong&gt; &amp;ndash;形状模块的可学习偏差</target>
        </trans-unit>
        <trans-unit id="ae7d6b9dea3f23c1bf0ec9b92c65617e42b54758" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Linear.bias&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the non-learnable bias of the module of shape</source>
          <target state="translated">&lt;strong&gt;〜Linear.bias&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;形状模块的不可学习的偏差</target>
        </trans-unit>
        <trans-unit id="80ff6fec5931df84e49c65ccc4fcb004cef4385f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Linear.bias&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the non-learnable floating point bias of the module of shape</source>
          <target state="translated">&lt;strong&gt;〜Linear.bias&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;形状模块的不可学习的浮点偏差</target>
        </trans-unit>
        <trans-unit id="fb2839a507719a2a7e1c7f3c45c3a78738935aff" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Linear.scale&lt;/strong&gt; &amp;ndash; &lt;code&gt;scale&lt;/code&gt; parameter of output Quantized Tensor, type: double</source>
          <target state="translated">&lt;strong&gt;〜Linear.scale&lt;/strong&gt; &amp;ndash;输出量化张量的 &lt;code&gt;scale&lt;/code&gt; 参数，类型：double</target>
        </trans-unit>
        <trans-unit id="aa119f3519a628ea703e857ef172d9d75ef8b21f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Linear.weight&lt;/strong&gt; &amp;ndash; fake quant module for weight</source>
          <target state="translated">&lt;strong&gt;〜Linear.weight&lt;/strong&gt; &amp;ndash;伪造的重量&lt;strong&gt;量化&lt;/strong&gt;模块</target>
        </trans-unit>
        <trans-unit id="315c79b44e7a362abc428a093029318a5cb6ebc6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Linear.weight&lt;/strong&gt; &amp;ndash; the learnable weights of the module of shape</source>
          <target state="translated">&lt;strong&gt;〜Linear.weight&lt;/strong&gt; &amp;ndash;形状模块的可学习权重</target>
        </trans-unit>
        <trans-unit id="74afca0120ba709897a38dc272bc9981208e2d0f" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Linear.weight&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the non-learnable quantized weights of the module of shape</source>
          <target state="translated">&lt;strong&gt;〜Linear.weight&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;形状模块的不可学习的量化权重</target>
        </trans-unit>
        <trans-unit id="64fb9620d600935b122442b73d2593dbde83a4d5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Linear.weight&lt;/strong&gt; (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the non-learnable quantized weights of the module which are of shape</source>
          <target state="translated">&lt;strong&gt;〜Linear.weight&lt;/strong&gt;（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;形状的模块的不可学习的量化权重</target>
        </trans-unit>
        <trans-unit id="3afcb3e4388b0cbfaa40fe5fcad92e725f9bf434" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Linear.zero_point&lt;/strong&gt; &amp;ndash; &lt;code&gt;zero_point&lt;/code&gt; parameter for output Quantized Tensor, type: long</source>
          <target state="translated">&lt;strong&gt;〜Linear.zero_point&lt;/strong&gt; &amp;ndash;输出量化张量的 &lt;code&gt;zero_point&lt;/code&gt; 参数，类型：long</target>
        </trans-unit>
        <trans-unit id="522c0534c3e108071d6f81950228abd00552637b" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~LinearReLU.weight&lt;/strong&gt; &amp;ndash; fake quant module for weight</source>
          <target state="translated">&lt;strong&gt;〜LinearReLU.weight&lt;/strong&gt; &amp;ndash;伪造的重量&lt;strong&gt;量化&lt;/strong&gt;模块</target>
        </trans-unit>
        <trans-unit id="8895a1b0c2e6107ada5018ca16d43d982c7d6188" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~PReLU.weight&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; the learnable weights of shape (&lt;code&gt;num_parameters&lt;/code&gt;).</source>
          <target state="translated">&lt;strong&gt;〜PReLU.weight&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;形状的可学习权重（ &lt;code&gt;num_parameters&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="cc864c7bcceabd4bb9ba42bcf609c115c46dde97" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~PackedSequence.batch_sizes&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Tensor of integers holding information about the batch size at each sequence step</source>
          <target state="translated">&lt;strong&gt;〜PackedSequence.batch_sizes&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;在每个序列步骤中保存有关批次大小信息的整数的张量</target>
        </trans-unit>
        <trans-unit id="2e648b81d63ca51e2c59bfb2670172ffa0bdcd12" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~PackedSequence.data&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; Tensor containing packed sequence</source>
          <target state="translated">&lt;strong&gt;〜PackedSequence.data&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;包含压缩序列的张量</target>
        </trans-unit>
        <trans-unit id="21b5da5ef8600388a81b311dc469708b1a243874" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~PackedSequence.sorted_indices&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Tensor of integers holding how this &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;PackedSequence&lt;/code&gt;&lt;/a&gt; is constructed from sequences.</source>
          <target state="translated">&lt;strong&gt;〜PackedSequence.sorted_indices&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;整数的张量，用于保存如何从序列构造&lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt; &lt;code&gt;PackedSequence&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="2f61547990dd4c37a589c3d1c949f534ce964d34" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~PackedSequence.unsorted_indices&lt;/strong&gt; (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;optional&lt;/em&gt;) &amp;ndash; Tensor of integers holding how this to recover the original sequences with correct order.</source>
          <target state="translated">&lt;strong&gt;〜PackedSequence.unsorted_indices&lt;/strong&gt;（&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor &lt;/a&gt;&lt;em&gt;，&lt;/em&gt;&lt;em&gt;可选&lt;/em&gt;）&amp;ndash;整数张量，用于保存如何以正确的顺序恢复原始序列。</target>
        </trans-unit>
        <trans-unit id="235904bb81615face6c03f7fdd0762f01a44c648" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~RNN.bias_hh_l[k]&lt;/strong&gt; &amp;ndash; the learnable hidden-hidden bias of the k-th layer, of shape &lt;code&gt;(hidden_size)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;〜RNN.bias_hh_l [k]&lt;/strong&gt; &amp;ndash;第k层的可学习的隐藏偏差，形状为 &lt;code&gt;(hidden_size)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="e8ba0fa05da47543bfecbc4b3d0f4f691b034e37" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~RNN.bias_ih_l[k]&lt;/strong&gt; &amp;ndash; the learnable input-hidden bias of the k-th layer, of shape &lt;code&gt;(hidden_size)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;〜RNN.bias_ih_l [k]&lt;/strong&gt; &amp;ndash;第k层的可学习的输入隐藏偏差，形状为 &lt;code&gt;(hidden_size)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="fe5a76d8bdd2bfbda4a600d6f968bab8b67aadc6" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~RNN.weight_hh_l[k]&lt;/strong&gt; &amp;ndash; the learnable hidden-hidden weights of the k-th layer, of shape &lt;code&gt;(hidden_size, hidden_size)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;〜RNN.weight_hh_l [k]&lt;/strong&gt; &amp;ndash;第k层的可学习的隐藏权重，形状 &lt;code&gt;(hidden_size, hidden_size)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="6b55d7debcf00b4a9aace4971974309ba4880f3c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~RNN.weight_ih_l[k]&lt;/strong&gt; &amp;ndash; the learnable input-hidden weights of the k-th layer, of shape &lt;code&gt;(hidden_size, input_size)&lt;/code&gt; for &lt;code&gt;k = 0&lt;/code&gt;. Otherwise, the shape is &lt;code&gt;(hidden_size, num_directions * hidden_size)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;〜RNN.weight_ih_l [k]&lt;/strong&gt; -第k层的可学习的输入隐藏权重，形状为 &lt;code&gt;(hidden_size, input_size)&lt;/code&gt; ，其中 &lt;code&gt;k = 0&lt;/code&gt; 。否则，形状为 &lt;code&gt;(hidden_size, num_directions * hidden_size)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="2d2a5ac7fe67f4afaa29c61a8d22a1372b50c310" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~RNNCell.bias_hh&lt;/strong&gt; &amp;ndash; the learnable hidden-hidden bias, of shape &lt;code&gt;(hidden_size)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;〜RNNCell.bias_hh&lt;/strong&gt; &amp;ndash;可学习的隐藏偏差，形状为 &lt;code&gt;(hidden_size)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="ca09dd35f6974547104a869f45041f8189618781" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~RNNCell.bias_ih&lt;/strong&gt; &amp;ndash; the learnable input-hidden bias, of shape &lt;code&gt;(hidden_size)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;〜RNNCell.bias_ih&lt;/strong&gt; &amp;ndash;可学习的输入隐藏偏差，形状为 &lt;code&gt;(hidden_size)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="5b9525f25b880e8e32703e40cb26509bc4d00f20" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~RNNCell.weight_hh&lt;/strong&gt; &amp;ndash; the learnable hidden-hidden weights, of shape &lt;code&gt;(hidden_size, hidden_size)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;〜RNNCell.weight_hh&lt;/strong&gt; &amp;ndash;可学习的隐藏权重，形状 &lt;code&gt;(hidden_size, hidden_size)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="6fdb4b833f363ac5882e1551bb76b1d609be001c" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~RNNCell.weight_ih&lt;/strong&gt; &amp;ndash; the learnable input-hidden weights, of shape &lt;code&gt;(hidden_size, input_size)&lt;/code&gt;</source>
          <target state="translated">&lt;strong&gt;〜RNNCell.weight_ih&lt;/strong&gt; &amp;ndash;可学习的输入隐藏权重，形状 &lt;code&gt;(hidden_size, input_size)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="e82390ad46fb58d4732ab24417e49c04a40e1ca9" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Transform.bijective&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;) &amp;ndash; Whether this transform is bijective. A transform &lt;code&gt;t&lt;/code&gt; is bijective iff &lt;code&gt;t.inv(t(x)) == x&lt;/code&gt; and &lt;code&gt;t(t.inv(y)) == y&lt;/code&gt; for every &lt;code&gt;x&lt;/code&gt; in the domain and &lt;code&gt;y&lt;/code&gt; in the codomain. Transforms that are not bijective should at least maintain the weaker pseudoinverse properties &lt;code&gt;t(t.inv(t(x)) == t(x)&lt;/code&gt; and &lt;code&gt;t.inv(t(t.inv(y))) == t.inv(y)&lt;/code&gt;.</source>
          <target state="translated">&lt;strong&gt;〜Transform.bijective&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;bool&lt;/a&gt;）&amp;ndash;此变换是否是双射的。对于域中的每个 &lt;code&gt;x&lt;/code&gt; 和共域中的 &lt;code&gt;y&lt;/code&gt; ，变换 &lt;code&gt;t&lt;/code&gt; 是双射的iff &lt;code&gt;t.inv(t(x)) == x&lt;/code&gt; 和 &lt;code&gt;t(t.inv(y)) == y&lt;/code&gt; 。非双射变换至少应保持较弱的伪逆特性 &lt;code&gt;t(t.inv(t(x)) == t(x)&lt;/code&gt; 和 &lt;code&gt;t.inv(t(t.inv(y))) == t.inv(y)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="824ccd97fb865047d77b970e039164f07cd61cc5" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Transform.codomain&lt;/strong&gt; (&lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt;&lt;code&gt;Constraint&lt;/code&gt;&lt;/a&gt;) &amp;ndash; The constraint representing valid outputs to this transform which are inputs to the inverse transform.</source>
          <target state="translated">&lt;strong&gt;〜Transform.codomain&lt;/strong&gt;（&lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt; &lt;code&gt;Constraint&lt;/code&gt; &lt;/a&gt;）&amp;ndash;表示此变换的有效输出的约束，该约束是逆变换的输入。</target>
        </trans-unit>
        <trans-unit id="db2c67871aa1327e4e09c9aa493aef93bdaba1a8" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Transform.domain&lt;/strong&gt; (&lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt;&lt;code&gt;Constraint&lt;/code&gt;&lt;/a&gt;) &amp;ndash; The constraint representing valid inputs to this transform.</source>
          <target state="translated">&lt;strong&gt;〜Transform.domain&lt;/strong&gt;（&lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt; &lt;code&gt;Constraint&lt;/code&gt; &lt;/a&gt;）&amp;ndash;表示此转换有效输入的约束。</target>
        </trans-unit>
        <trans-unit id="ee9340a9cb2c6500b85b8240b95a54b6ea0ed558" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Transform.event_dim&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;) &amp;ndash; Number of dimensions that are correlated together in the transform &lt;code&gt;event_shape&lt;/code&gt;. This should be 0 for pointwise transforms, 1 for transforms that act jointly on vectors, 2 for transforms that act jointly on matrices, etc.</source>
          <target state="translated">&lt;strong&gt;〜Transform.event_dim&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;INT&lt;/a&gt;） -这是在变换相关一起维数 &lt;code&gt;event_shape&lt;/code&gt; 。对于逐点变换，应为0；对于联合作用于矢量的变换，应为1；对于联合作用于矩阵的变换，其应为2。</target>
        </trans-unit>
        <trans-unit id="df7985766a96947f6970e908f5ca42cbfd70b948" translate="yes" xml:space="preserve">
          <source>&lt;strong&gt;~Transform.sign&lt;/strong&gt; (&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt; or &lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;) &amp;ndash; For bijective univariate transforms, this should be +1 or -1 depending on whether transform is monotone increasing or decreasing.</source>
          <target state="translated">&lt;strong&gt;〜Transform.sign&lt;/strong&gt;（&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;int&lt;/a&gt;&lt;em&gt;或&lt;/em&gt;&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;）&amp;ndash;对于双射单变量变换，此值应为+1或-1，具体取决于变换是单调递增还是递减。</target>
        </trans-unit>
        <trans-unit id="4363d72fe0148f9fd5bda59b03b2597b475ecb3d" translate="yes" xml:space="preserve">
          <source>= &lt;code&gt;hidden_size&lt;/code&gt; Defaults to zero if not provided.</source>
          <target state="translated">= &lt;code&gt;hidden_size&lt;/code&gt; 如果未提供，则默认为零。</target>
        </trans-unit>
        <trans-unit id="76c6be0a47b9e469e03f3d72ce03df30479e07c5" translate="yes" xml:space="preserve">
          <source>= &lt;code&gt;input_size&lt;/code&gt;</source>
          <target state="translated">= &lt;code&gt;input_size&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="22b30cde9b5535b0a766c1d74e242d402d5dfdfd" translate="yes" xml:space="preserve">
          <source>= &lt;code&gt;signal_ndim&lt;/code&gt; is number of dimensions for the signal, and</source>
          <target state="translated">= &lt;code&gt;signal_ndim&lt;/code&gt; 是信号的维数，并且</target>
        </trans-unit>
        <trans-unit id="74a544e408284dc448cde186b7b615c6eac7a62c" translate="yes" xml:space="preserve">
          <source>= &lt;code&gt;signal_ndim&lt;/code&gt;. &lt;code&gt;onesided&lt;/code&gt; flag controls whether to avoid redundancy in the output results. If set to &lt;code&gt;True&lt;/code&gt; (default), the output will not be full complex result of shape</source>
          <target state="translated">= &lt;code&gt;signal_ndim&lt;/code&gt; 。 &lt;code&gt;onesided&lt;/code&gt; 标志控制是否避免输出结果中的冗余。如果设置为 &lt;code&gt;True&lt;/code&gt; （默认），则输出将不是完整的形状结果</target>
        </trans-unit>
        <trans-unit id="f9c16f2184526910dd910a4f6192e4c698a5444c" translate="yes" xml:space="preserve">
          <source>≠</source>
          <target state="translated">≠</target>
        </trans-unit>
        <trans-unit id="6dcd4ce23d88e2ee9568ba546c007c63d9131c1b" translate="yes" xml:space="preserve">
          <source>A</source>
          <target state="translated">A</target>
        </trans-unit>
        <trans-unit id="0ba2b2257c8b8d78f6250b41bc82133cdb8d0d85" translate="yes" xml:space="preserve">
          <source>A (Tensor): the input tensor of size</source>
          <target state="translated">A(张量):输入的张量大小为</target>
        </trans-unit>
        <trans-unit id="c771998369e681962e03e40a977496f10f66e4ab" translate="yes" xml:space="preserve">
          <source>A - M</source>
          <target state="translated">A-M</target>
        </trans-unit>
        <trans-unit id="163f6731aec462243f49bce7b3f447cfca092a3d" translate="yes" xml:space="preserve">
          <source>A 1-D tensor of size</source>
          <target state="translated">大小的一维张量</target>
        </trans-unit>
        <trans-unit id="5ea1abe027131df5fe0f08815006944c348e7434" translate="yes" xml:space="preserve">
          <source>A 2 dimensional tensor with all the input tensors arranged in</source>
          <target state="translated">一个二维的张量,所有的输入张量都排列在一起。</target>
        </trans-unit>
        <trans-unit id="2da7899e802f2306f976e1aaf402515d02db4967" translate="yes" xml:space="preserve">
          <source>A 2-D tensor with ones on the diagonal and zeros elsewhere</source>
          <target state="translated">二维张量,对角线上为1,其他地方为0。</target>
        </trans-unit>
        <trans-unit id="94dae2aa29f01378af45a02523af98deeb730db7" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; is a multi-dimensional matrix containing elements of a single data type.</source>
          <target state="translated">甲&lt;a href=&quot;#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;是包含单一数据类型的元素的多维矩阵。</target>
        </trans-unit>
        <trans-unit id="30862db2302d182cfad7095752f0185b9568d784" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; can be constructed via a string or via a string and device ordinal</source>
          <target state="translated">甲&lt;a href=&quot;#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;可以经由串或经由串和设备序号来构成</target>
        </trans-unit>
        <trans-unit id="24d39a42c40922c085cc28a510764783ea381734" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; is an object representing the device on which a &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; is or will be allocated.</source>
          <target state="translated">甲&lt;a href=&quot;#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;是表示在其上的设备的对象&lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;被或将被分配的。</target>
        </trans-unit>
        <trans-unit id="194543d5c9873e7986896f48770c7b948114ce57" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; is an object that represents the data type of a &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt;. PyTorch has twelve different data types:</source>
          <target state="translated">甲&lt;a href=&quot;#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;是一个表示的数据类型的对象&lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;。PyTorch具有十二种不同的数据类型：</target>
        </trans-unit>
        <trans-unit id="38e208177382a27c53d72149eef4e9baaa2dc1d2" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;#torch.torch.finfo&quot;&gt;&lt;code&gt;torch.finfo&lt;/code&gt;&lt;/a&gt; is an object that represents the numerical properties of a floating point &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, (i.e. &lt;code&gt;torch.float32&lt;/code&gt;, &lt;code&gt;torch.float64&lt;/code&gt;, and &lt;code&gt;torch.float16&lt;/code&gt;). This is similar to &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.finfo.html&quot;&gt;numpy.finfo&lt;/a&gt;.</source>
          <target state="translated">甲&lt;a href=&quot;#torch.torch.finfo&quot;&gt; &lt;code&gt;torch.finfo&lt;/code&gt; &lt;/a&gt;是表示一个浮点的数值属性的对象&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;，（即 &lt;code&gt;torch.float32&lt;/code&gt; ， &lt;code&gt;torch.float64&lt;/code&gt; ，和 &lt;code&gt;torch.float16&lt;/code&gt; ）。这类似于&lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.finfo.html&quot;&gt;numpy.finfo&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e9b5788e7f1ba02d4ce5b044c5845d26ea320e06" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;#torch.torch.finfo&quot;&gt;&lt;code&gt;torch.finfo&lt;/code&gt;&lt;/a&gt; provides the following attributes:</source>
          <target state="translated">甲&lt;a href=&quot;#torch.torch.finfo&quot;&gt; &lt;code&gt;torch.finfo&lt;/code&gt; &lt;/a&gt;提供以下属性：</target>
        </trans-unit>
        <trans-unit id="dc99c23c305b6eee6243eadd6e2b8739deea9e14" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;#torch.torch.iinfo&quot;&gt;&lt;code&gt;torch.iinfo&lt;/code&gt;&lt;/a&gt; is an object that represents the numerical properties of a integer &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; (i.e. &lt;code&gt;torch.uint8&lt;/code&gt;, &lt;code&gt;torch.int8&lt;/code&gt;, &lt;code&gt;torch.int16&lt;/code&gt;, &lt;code&gt;torch.int32&lt;/code&gt;, and &lt;code&gt;torch.int64&lt;/code&gt;). This is similar to &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.iinfo.html&quot;&gt;numpy.iinfo&lt;/a&gt;.</source>
          <target state="translated">甲&lt;a href=&quot;#torch.torch.iinfo&quot;&gt; &lt;code&gt;torch.iinfo&lt;/code&gt; &lt;/a&gt;是一个对象，它表示一个整数的数值性质&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;（即 &lt;code&gt;torch.uint8&lt;/code&gt; ， &lt;code&gt;torch.int8&lt;/code&gt; ， &lt;code&gt;torch.int16&lt;/code&gt; ， &lt;code&gt;torch.int32&lt;/code&gt; ，和 &lt;code&gt;torch.int64&lt;/code&gt; ）。这类似于&lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.iinfo.html&quot;&gt;numpy.iinfo&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="ea287a33429f2668f3a29b9f646857cd3589f455" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;#torch.torch.iinfo&quot;&gt;&lt;code&gt;torch.iinfo&lt;/code&gt;&lt;/a&gt; provides the following attributes:</source>
          <target state="translated">甲&lt;a href=&quot;#torch.torch.iinfo&quot;&gt; &lt;code&gt;torch.iinfo&lt;/code&gt; &lt;/a&gt;提供以下属性：</target>
        </trans-unit>
        <trans-unit id="c2142ddfec581443f517442f35bb15b0be1e58bb" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;#torch.torch.layout&quot;&gt;&lt;code&gt;torch.layout&lt;/code&gt;&lt;/a&gt; is an object that represents the memory layout of a &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt;. Currently, we support &lt;code&gt;torch.strided&lt;/code&gt; (dense Tensors) and have beta support for &lt;code&gt;torch.sparse_coo&lt;/code&gt; (sparse COO Tensors).</source>
          <target state="translated">甲&lt;a href=&quot;#torch.torch.layout&quot;&gt; &lt;code&gt;torch.layout&lt;/code&gt; &lt;/a&gt;是一个表示的存储器布局的对象&lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;。目前，我们支持 &lt;code&gt;torch.strided&lt;/code&gt; （密集张量），并具有 &lt;code&gt;torch.sparse_coo&lt;/code&gt; （稀疏COO张量）的beta支持。</target>
        </trans-unit>
        <trans-unit id="de355853c58ee99175ccddd4a1b61f2e88daa33a" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;#torch.torch.memory_format&quot;&gt;&lt;code&gt;torch.memory_format&lt;/code&gt;&lt;/a&gt; is an object representing the memory format on which a &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; is or will be allocated.</source>
          <target state="translated">甲&lt;a href=&quot;#torch.torch.memory_format&quot;&gt; &lt;code&gt;torch.memory_format&lt;/code&gt; &lt;/a&gt;是表示存储器格式在其上的对象&lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;被或将被分配的。</target>
        </trans-unit>
        <trans-unit id="c2fda6a0424485527dc39199cf94f24420a7588b" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; uses single-process data loading by default.</source>
          <target state="translated">一&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt;默认使用单进程的数据加载。</target>
        </trans-unit>
        <trans-unit id="10fbed407aac8b67f8332747c3e4586eb1dec2dc" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;#torchscript-class&quot;&gt;TorchScript Class&lt;/a&gt;</source>
          <target state="translated">一&lt;a href=&quot;#torchscript-class&quot;&gt;TorchScript类&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="87a00529d12368e11ecb0951f6020b25b804bf4c" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;#torchscript-enum&quot;&gt;TorchScript Enum&lt;/a&gt;</source>
          <target state="translated">一个&lt;a href=&quot;#torchscript-enum&quot;&gt;TorchScript枚举&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7404dcf6cb59834c3f935967f5f2cff903cab0d1" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; with a single &lt;code&gt;forward&lt;/code&gt; method will have an attribute &lt;code&gt;code&lt;/code&gt;, which you can use to inspect the &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;&amp;rsquo;s code. If the &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; has more than one method, you will need to access &lt;code&gt;.code&lt;/code&gt; on the method itself and not the module. We can inspect the code of a method named &lt;code&gt;foo&lt;/code&gt; on a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; by accessing &lt;code&gt;.foo.code&lt;/code&gt;. The example above produces this output:</source>
          <target state="translated">一个&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;用单 &lt;code&gt;forward&lt;/code&gt; 方法将有一个属性 &lt;code&gt;code&lt;/code&gt; ，你可以用它来检查&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;的代码。如果&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;具有多个方法，则需要在方法本身而非模块上访问 &lt;code&gt;.code&lt;/code&gt; 。我们可以通过访问 &lt;code&gt;.foo.code&lt;/code&gt; 来检查&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;上名为 &lt;code&gt;foo&lt;/code&gt; 的方法的代码。上面的示例产生以下输出：</target>
        </trans-unit>
        <trans-unit id="fe55e59d1093f23c98c38d66836a27b73eecda96" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;https://docs.python.org/3/library/collections.html#collections.namedtuple&quot;&gt;&lt;code&gt;collections.namedtuple&lt;/code&gt;&lt;/a&gt; tuple type</source>
          <target state="translated">一个&lt;a href=&quot;https://docs.python.org/3/library/collections.html#collections.namedtuple&quot;&gt; &lt;code&gt;collections.namedtuple&lt;/code&gt; &lt;/a&gt;元组类型</target>
        </trans-unit>
        <trans-unit id="52f3bfdc6708921215b778992e2b2023ce0cf17c" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/a&gt; that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat-32(TF32) on Ampere devices&lt;/a&gt;.</source>
          <target state="translated">甲&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt; &lt;code&gt;bool&lt;/code&gt; &lt;/a&gt;但如TensorFloat-32张量核可以在cuDNN卷积使用安培或更高的GPU控制。请参阅&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;Ampere设备上的TensorFloat-32（TF32）&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="ee7cf0dafdb20b0870f4ba1b8b3c1c9c0b0961ae" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/a&gt; that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;TensorFloat-32(TF32) on Ampere devices&lt;/a&gt;.</source>
          <target state="translated">一个&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt; &lt;code&gt;bool&lt;/code&gt; &lt;/a&gt;，用于控制TensorFloat-32张量核是否可用于安培或更新GPU上的矩阵乘法。请参阅&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#tf32-on-ampere&quot;&gt;Ampere设备上的TensorFloat-32（TF32）&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="132439c11835b26ffb69dbea90cbf5ac7df424e2" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/a&gt; that controls whether cuDNN is enabled.</source>
          <target state="translated">一个&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt; &lt;code&gt;bool&lt;/code&gt; &lt;/a&gt;，用于控制是否启用cuDNN。</target>
        </trans-unit>
        <trans-unit id="f8f9daed2aeb319caf641cde5cc61730c1047610" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/a&gt; that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest.</source>
          <target state="translated">一个&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt; &lt;code&gt;bool&lt;/code&gt; &lt;/a&gt;，如果为true，导致cuDNN到基准多个卷积算法和选择最快的。</target>
        </trans-unit>
        <trans-unit id="ad8b39b030a50343fcfc5491aa61a98d87ef927a" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/a&gt; that, if True, causes cuDNN to only use deterministic convolution algorithms. See also &lt;a href=&quot;generated/torch.is_deterministic#torch.is_deterministic&quot;&gt;&lt;code&gt;torch.is_deterministic()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.set_deterministic#torch.set_deterministic&quot;&gt;&lt;code&gt;torch.set_deterministic()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">一个&lt;a href=&quot;https://docs.python.org/3/library/functions.html#bool&quot;&gt; &lt;code&gt;bool&lt;/code&gt; &lt;/a&gt;，如果为True，则导致cuDNN仅使用确定性卷积算法。另请参见&lt;a href=&quot;generated/torch.is_deterministic#torch.is_deterministic&quot;&gt; &lt;code&gt;torch.is_deterministic()&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/torch.set_deterministic#torch.set_deterministic&quot;&gt; &lt;code&gt;torch.set_deterministic()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="363a5bf37cd0bd5599a7ac4f74f15c234a572c9d" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/a&gt; that controls cache capacity of cuFFT plan.</source>
          <target state="translated">一个控制cuFFT计划的缓存容量的&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt; &lt;code&gt;int&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="15086d30a742ea962251815a98fb5f43c932d666" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt;&amp;rsquo;s device can be accessed via the &lt;a href=&quot;tensors#torch.Tensor.device&quot;&gt;&lt;code&gt;Tensor.device&lt;/code&gt;&lt;/a&gt; property.</source>
          <target state="translated">甲&lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;的设备可以通过访问&lt;a href=&quot;tensors#torch.Tensor.device&quot;&gt; &lt;code&gt;Tensor.device&lt;/code&gt; &lt;/a&gt;属性。</target>
        </trans-unit>
        <trans-unit id="669a8dfbb51e49ee0a4e2b45cc3fe7e68a5626a0" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; object with a single &lt;code&gt;forward&lt;/code&gt; method containing the traced code. When &lt;code&gt;func&lt;/code&gt; is a &lt;code&gt;torch.nn.Module&lt;/code&gt;, the returned &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; will have the same set of sub-modules and parameters as &lt;code&gt;func&lt;/code&gt;.</source>
          <target state="translated">一个&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;对象，具有包含跟踪代码的单个 &lt;code&gt;forward&lt;/code&gt; 方法。当 &lt;code&gt;func&lt;/code&gt; 是 &lt;code&gt;torch.nn.Module&lt;/code&gt; 时，返回的&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;将具有与 &lt;code&gt;func&lt;/code&gt; 相同的子模块和参数集。</target>
        </trans-unit>
        <trans-unit id="41acf1053b0302918ca7cea8e31ad650f225d642" translate="yes" xml:space="preserve">
          <source>A &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; object.</source>
          <target state="translated">一个&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;对象。</target>
        </trans-unit>
        <trans-unit id="7ba4764527a7c1083db1bd25ed8621fc70d8ebcb" translate="yes" xml:space="preserve">
          <source>A &lt;code&gt;dim&lt;/code&gt; value within the range &lt;code&gt;[-input.dim() - 1, input.dim() + 1)&lt;/code&gt; can be used. Negative &lt;code&gt;dim&lt;/code&gt; will correspond to &lt;a href=&quot;#torch.unsqueeze&quot;&gt;&lt;code&gt;unsqueeze()&lt;/code&gt;&lt;/a&gt; applied at &lt;code&gt;dim&lt;/code&gt; = &lt;code&gt;dim + input.dim() + 1&lt;/code&gt;.</source>
          <target state="translated">甲 &lt;code&gt;dim&lt;/code&gt; 的范围内的值 &lt;code&gt;[-input.dim() - 1, input.dim() + 1)&lt;/code&gt; 都可以使用。负 &lt;code&gt;dim&lt;/code&gt; 将对应于&lt;a href=&quot;#torch.unsqueeze&quot;&gt; &lt;code&gt;unsqueeze()&lt;/code&gt; &lt;/a&gt;在施加 &lt;code&gt;dim&lt;/code&gt; = &lt;code&gt;dim + input.dim() + 1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="a4042276aa29d077b69d47201952f51420b20aea" translate="yes" xml:space="preserve">
          <source>A &lt;code&gt;torch.ByteTensor&lt;/code&gt; which contains all the necessary bits to restore a Generator to a specific point in time.</source>
          <target state="translated">一个 &lt;code&gt;torch.ByteTensor&lt;/code&gt; ，其中包含将Generator还原到特定时间点的所有必需位。</target>
        </trans-unit>
        <trans-unit id="651aaa3c833905f7faa2adf76b8bd668ead8cca6" translate="yes" xml:space="preserve">
          <source>A &lt;code&gt;torch.Storage&lt;/code&gt; is a contiguous, one-dimensional array of a single data type.</source>
          <target state="translated">甲 &lt;code&gt;torch.Storage&lt;/code&gt; 是一个单独的数据类型的一个连续的，一维阵列。</target>
        </trans-unit>
        <trans-unit id="8d3fcf73542a1b9a5f63963e6df264f067c5be6b" translate="yes" xml:space="preserve">
          <source>A = LL^T</source>
          <target state="translated">A=LL^T</target>
        </trans-unit>
        <trans-unit id="9a98c44769eae9961b395c0bbcfcad7880070dff" translate="yes" xml:space="preserve">
          <source>A = U diag(S) V^T</source>
          <target state="translated">A=U diag(S)V^T</target>
        </trans-unit>
        <trans-unit id="e0c55a36f7072ee8c943df73addbc18e8cc62aa0" translate="yes" xml:space="preserve">
          <source>A = U^TU</source>
          <target state="translated">A=U^TU</target>
        </trans-unit>
        <trans-unit id="4ad39d5878705cc98deef8592636ef6df59eca22" translate="yes" xml:space="preserve">
          <source>A CUDA stream is a linear sequence of execution that belongs to a specific device, independent from other streams. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-semantics&quot;&gt;CUDA semantics&lt;/a&gt; for details.</source>
          <target state="translated">CUDA流是属于特定设备的线性执行序列，独立于其他流。有关详细信息，请参见&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-semantics&quot;&gt;CUDA语义&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="1c931baf3955ce6642a1157d462795c45a1e4f6a" translate="yes" xml:space="preserve">
          <source>A Conv2d module attached with FakeQuantize modules for weight, used for quantization aware training.</source>
          <target state="translated">一个Conv2d模块,附加FakeQuantize模块的权重,用于量化感知训练。</target>
        </trans-unit>
        <trans-unit id="72876496e2dc3bd4b4d55b0ee6148f21eda8c0c7" translate="yes" xml:space="preserve">
          <source>A ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with FakeQuantize modules for weight, used in quantization aware training.</source>
          <target state="translated">ConvBn2d模块是由Conv2d和BatchNorm2d融合而成的模块,附加FakeQuantize模块作为权重,用于量化感知训练。</target>
        </trans-unit>
        <trans-unit id="096a086e744907bafa054ddb3c822576842bca8b" translate="yes" xml:space="preserve">
          <source>A ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.</source>
          <target state="translated">ConvBnReLU2d模块是由Conv2d、BatchNorm2d和ReLU融合而成的模块,附加FakeQuantize模块作为权重,用于量化感知训练。</target>
        </trans-unit>
        <trans-unit id="6056ceba54b6d5f2f7479fdf0420885d4c9ee2d7" translate="yes" xml:space="preserve">
          <source>A ConvReLU2d module is a fused module of Conv2d and ReLU</source>
          <target state="translated">ConvReLU2d模块是Conv2d和ReLU的融合模块。</target>
        </trans-unit>
        <trans-unit id="2680c03e7de7a88804eb33c2fdac5905f71c50af" translate="yes" xml:space="preserve">
          <source>A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.</source>
          <target state="translated">ConvReLU2d模块是Conv2d和ReLU的融合模块,附加FakeQuantize模块的权重进行量化感知训练。</target>
        </trans-unit>
        <trans-unit id="6f481e2f39df1d5e690d89d168813ffb2098eecf" translate="yes" xml:space="preserve">
          <source>A ConvReLU3d module is a fused module of Conv3d and ReLU</source>
          <target state="translated">ConvReLU3d模块是Conv3d和ReLU的融合模块。</target>
        </trans-unit>
        <trans-unit id="2b27bbd839fbfbe69a98f451005ba9c4f4eacd76" translate="yes" xml:space="preserve">
          <source>A FunctionEventAvg object.</source>
          <target state="translated">一个FunctionEventAvg对象。</target>
        </trans-unit>
        <trans-unit id="c581294889f60e363c7316ff06c75be3343c616d" translate="yes" xml:space="preserve">
          <source>A LinearReLU module fused from Linear and ReLU modules</source>
          <target state="translated">由线性和ReLU模块融合而成的LinearReLU模块。</target>
        </trans-unit>
        <trans-unit id="d75cf067e6cff790d10b968c6dfc06b15a74c001" translate="yes" xml:space="preserve">
          <source>A LinearReLU module fused from Linear and ReLU modules, attached with FakeQuantize modules for weight, used in quantization aware training.</source>
          <target state="translated">由Linear和ReLU模块融合而成的LinearReLU模块,附加FakeQuantize模块作为权重,用于量化感知训练。</target>
        </trans-unit>
        <trans-unit id="f014741eaa3c28d99fb30934bea4ac8630cb0954" translate="yes" xml:space="preserve">
          <source>A PyTorch tensor of any dtype, dimension, or backend</source>
          <target state="translated">任何d类型、维度或后端的PyTorch张量。</target>
        </trans-unit>
        <trans-unit id="606edcfedaa2aed46ccbb45e395a66931ad76205" translate="yes" xml:space="preserve">
          <source>A TCP-based distributed key-value store implementation. The server store holds the data, while the client stores can connect to the server store over TCP and perform actions such as &lt;code&gt;set()&lt;/code&gt; to insert a key-value pair, &lt;code&gt;get()&lt;/code&gt; to retrieve a key-value pair, etc.</source>
          <target state="translated">一种基于TCP的分布式键值存储实现。服务器存储区保存数据，而客户端存储区可以通过TCP连接到服务器存储区，并执行诸如 &lt;code&gt;set()&lt;/code&gt; 插入键值对， &lt;code&gt;get()&lt;/code&gt; 检索键值对等操作。</target>
        </trans-unit>
        <trans-unit id="934bb2f6ce37592fed6983ebae538926df9a870a" translate="yes" xml:space="preserve">
          <source>A Tensor with the same shape as the input, except with &lt;code&gt;dim&lt;/code&gt; removed. Each element of the returned tensor represents the estimated integral</source>
          <target state="translated">一个与输入形状相同的张量，但去除了 &lt;code&gt;dim&lt;/code&gt; 。返回的张量的每个元素代表估计的积分</target>
        </trans-unit>
        <trans-unit id="ba88f3b238bb390167a791ceb442cf7b95e4e5ce" translate="yes" xml:space="preserve">
          <source>A \approx U diag(S) V^T</source>
          <target state="translated">Aapprox U diag(S)V^T</target>
        </trans-unit>
        <trans-unit id="e1ab8dd0ff33393a1f6a71bc1bf559f59b193357" translate="yes" xml:space="preserve">
          <source>A batch of KL divergences of shape &lt;code&gt;batch_shape&lt;/code&gt;.</source>
          <target state="translated">一批形状为 &lt;code&gt;batch_shape&lt;/code&gt; 的KL散度。</target>
        </trans-unit>
        <trans-unit id="4cb845308f633f9bb52d4d2c01f1eae33c0b43f8" translate="yes" xml:space="preserve">
          <source>A boolean indicating if all kernels in this stream are completed.</source>
          <target state="translated">表示该流中所有内核是否完成的布尔值。</target>
        </trans-unit>
        <trans-unit id="b74fae9955917e49531486c1dc48e2490ed86644" translate="yes" xml:space="preserve">
          <source>A boolean indicating if all work currently captured by event has completed.</source>
          <target state="translated">表示事件当前捕获的所有工作是否已经完成的布尔值。</target>
        </trans-unit>
        <trans-unit id="448d4ffe506866b86729dc826b5e940db7f330bc" translate="yes" xml:space="preserve">
          <source>A boolean output tensor cannot accept a non-boolean tensor.</source>
          <target state="translated">一个布尔输出张量不能接受一个非布尔张量。</target>
        </trans-unit>
        <trans-unit id="575d060b7c5e05a4efb184af85c6381a6c7e4517" translate="yes" xml:space="preserve">
          <source>A boolean tensor that is True where &lt;code&gt;input&lt;/code&gt; is NaN and False elsewhere</source>
          <target state="translated">一个布尔张量，在 &lt;code&gt;input&lt;/code&gt; 为NaN且其他位置为False时为True</target>
        </trans-unit>
        <trans-unit id="a32a86eb86fd42f1d26cffcd478d118053b7a4fa" translate="yes" xml:space="preserve">
          <source>A boolean tensor that is True where &lt;code&gt;input&lt;/code&gt; is equal to &lt;code&gt;other&lt;/code&gt; and False elsewhere</source>
          <target state="translated">一个布尔张量，在 &lt;code&gt;input&lt;/code&gt; 等于 &lt;code&gt;other&lt;/code&gt; 值的情况下为True，在其他地方为False</target>
        </trans-unit>
        <trans-unit id="158a2b9240f238280e9de635efb432409a1f50cc" translate="yes" xml:space="preserve">
          <source>A boolean tensor that is True where &lt;code&gt;input&lt;/code&gt; is finite and False elsewhere</source>
          <target state="translated">一个布尔张量，在 &lt;code&gt;input&lt;/code&gt; 是有限的情况下为True，在其他地方为False</target>
        </trans-unit>
        <trans-unit id="80aa5dd6aaf0df9d8b8247b87a9a03b74284fb4e" translate="yes" xml:space="preserve">
          <source>A boolean tensor that is True where &lt;code&gt;input&lt;/code&gt; is greater than &lt;code&gt;other&lt;/code&gt; and False elsewhere</source>
          <target state="translated">一个布尔张量，在 &lt;code&gt;input&lt;/code&gt; 大于 &lt;code&gt;other&lt;/code&gt; 为True，在其他地方为False</target>
        </trans-unit>
        <trans-unit id="0ab5ab93711aaebbc75e0a5f02857973e9b97aab" translate="yes" xml:space="preserve">
          <source>A boolean tensor that is True where &lt;code&gt;input&lt;/code&gt; is greater than or equal to &lt;code&gt;other&lt;/code&gt; and False elsewhere</source>
          <target state="translated">一个布尔张量，在 &lt;code&gt;input&lt;/code&gt; 大于或等于 &lt;code&gt;other&lt;/code&gt; 为True，在其他地方为False</target>
        </trans-unit>
        <trans-unit id="db5355275d060aecda9b99bd049b1fb3bb2b5999" translate="yes" xml:space="preserve">
          <source>A boolean tensor that is True where &lt;code&gt;input&lt;/code&gt; is infinite and False elsewhere</source>
          <target state="translated">布尔张量，在 &lt;code&gt;input&lt;/code&gt; 为无穷大的情况下为True，在其他地方为False</target>
        </trans-unit>
        <trans-unit id="d1a62ed7f7827621e58d59ed2687cad0159c8094" translate="yes" xml:space="preserve">
          <source>A boolean tensor that is True where &lt;code&gt;input&lt;/code&gt; is less than &lt;code&gt;other&lt;/code&gt; and False elsewhere</source>
          <target state="translated">一个布尔张量，在 &lt;code&gt;input&lt;/code&gt; 小于 &lt;code&gt;other&lt;/code&gt; 值的情况下为True，在其他地方为False</target>
        </trans-unit>
        <trans-unit id="3c0627ce37f19d66b3d1de89c44e210f0dac3318" translate="yes" xml:space="preserve">
          <source>A boolean tensor that is True where &lt;code&gt;input&lt;/code&gt; is less than or equal to &lt;code&gt;other&lt;/code&gt; and False elsewhere</source>
          <target state="translated">一个布尔张量，在 &lt;code&gt;input&lt;/code&gt; 小于或等于 &lt;code&gt;other&lt;/code&gt; 为True，在其他地方为False</target>
        </trans-unit>
        <trans-unit id="a1eaf30e5ae09b5f416d4f43e58dcb2543097d23" translate="yes" xml:space="preserve">
          <source>A boolean tensor that is True where &lt;code&gt;input&lt;/code&gt; is not equal to &lt;code&gt;other&lt;/code&gt; and False elsewhere</source>
          <target state="translated">一个布尔张量，在 &lt;code&gt;input&lt;/code&gt; 不等于 &lt;code&gt;other&lt;/code&gt; 为True，在其他地方为False</target>
        </trans-unit>
        <trans-unit id="8e7be5071d915aabc81f850d18a45204a8c662e6" translate="yes" xml:space="preserve">
          <source>A boolean tensor that is True where &lt;code&gt;input&lt;/code&gt; is real and False elsewhere</source>
          <target state="translated">一个布尔张量，在 &lt;code&gt;input&lt;/code&gt; 为实的情况下为True，在其他地方为False</target>
        </trans-unit>
        <trans-unit id="cd05f7a1045f3d5c6441bd51c29203b6005476c5" translate="yes" xml:space="preserve">
          <source>A boolean value</source>
          <target state="translated">一个布尔值</target>
        </trans-unit>
        <trans-unit id="b8ed5bafa84c3a25c2d663a29827741f80f0b589" translate="yes" xml:space="preserve">
          <source>A circular von Mises distribution.</source>
          <target state="translated">冯-米塞斯的循环分布。</target>
        </trans-unit>
        <trans-unit id="aa26eb075c756a3b55b4838f819fe3b9550e2c75" translate="yes" xml:space="preserve">
          <source>A common PyTorch convention is to save tensors using .pt file extension.</source>
          <target state="translated">常见的 PyTorch 惯例是使用 .pt 文件扩展名保存 tensors。</target>
        </trans-unit>
        <trans-unit id="9c7664a975132aa7b24f7761f786d39644daa8ba" translate="yes" xml:space="preserve">
          <source>A constraint object represents a region over which a variable is valid, e.g. within which a variable can be optimized.</source>
          <target state="translated">约束对象表示一个变量有效的区域,例如在这个区域内可以对变量进行优化。</target>
        </trans-unit>
        <trans-unit id="476d5990029ab426aac50c8ed35adab4427abf1f" translate="yes" xml:space="preserve">
          <source>A context manager to be used in conjunction with an instance of &lt;a href=&quot;#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt;&lt;/a&gt; to be able to train with uneven inputs across participating processes.</source>
          <target state="translated">上下文管理器将与&lt;a href=&quot;#torch.nn.parallel.DistributedDataParallel&quot;&gt; &lt;code&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt; &lt;/a&gt;的实例结合使用，从而能够在参与的过程中使用不均匀的输入进行训练。</target>
        </trans-unit>
        <trans-unit id="260ba8ca717d75756a968a13317edb397e545889" translate="yes" xml:space="preserve">
          <source>A context manager to disable gradient synchronizations across DDP processes. Within this context, gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass exiting the context.</source>
          <target state="translated">一个上下文管理器,用于禁用跨DDP进程的梯度同步。在此上下文中,梯度将累积在模块变量上,随后在退出上下文的第一个前向-后向通道中进行同步。</target>
        </trans-unit>
        <trans-unit id="f56526cfd84150a791f8131f272572b491286ff2" translate="yes" xml:space="preserve">
          <source>A context manager to temporarily set the training mode of &amp;lsquo;model&amp;rsquo; to &amp;lsquo;mode&amp;rsquo;, resetting it when we exit the with-block. A no-op if mode is None.</source>
          <target state="translated">上下文管理器可将&amp;ldquo;模型&amp;rdquo;的训练模式临时设置为&amp;ldquo;模式&amp;rdquo;，并在退出with块时将其重置。如果模式为&amp;ldquo;无&amp;rdquo;，则为无操作。</target>
        </trans-unit>
        <trans-unit id="2285737f700dc7bef014b28bc5b4f7a87acfec42" translate="yes" xml:space="preserve">
          <source>A custom &lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt;&lt;code&gt;Sampler&lt;/code&gt;&lt;/a&gt; that yields a list of batch indices at a time can be passed as the &lt;code&gt;batch_sampler&lt;/code&gt; argument. Automatic batching can also be enabled via &lt;code&gt;batch_size&lt;/code&gt; and &lt;code&gt;drop_last&lt;/code&gt; arguments. See &lt;a href=&quot;#loading-batched-and-non-batched-data&quot;&gt;the next section&lt;/a&gt; for more details on this.</source>
          <target state="translated">可以一次生成批次索引列表的自定义&lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt; &lt;code&gt;Sampler&lt;/code&gt; &lt;/a&gt;作为 &lt;code&gt;batch_sampler&lt;/code&gt; 参数传递。也可以通过 &lt;code&gt;batch_size&lt;/code&gt; 和 &lt;code&gt;drop_last&lt;/code&gt; 参数启用自动批处理。有关更多详细信息，请参见&lt;a href=&quot;#loading-batched-and-non-batched-data&quot;&gt;下一部分&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e3af495f9c511cb3a02634254a4e699d9e76ecf3" translate="yes" xml:space="preserve">
          <source>A custom &lt;code&gt;collate_fn&lt;/code&gt; can be used to customize collation, e.g., padding sequential data to max length of a batch. See &lt;a href=&quot;#dataloader-collate-fn&quot;&gt;this section&lt;/a&gt; on more about &lt;code&gt;collate_fn&lt;/code&gt;.</source>
          <target state="translated">自定义 &lt;code&gt;collate_fn&lt;/code&gt; 可用于自定义排序规则，例如，将顺序数据填充到批处理的最大长度。有关 &lt;code&gt;collate_fn&lt;/code&gt; 更多信息，请参见&lt;a href=&quot;#dataloader-collate-fn&quot;&gt;本节&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e87e0ea4a2441712bbddd88006ff260be6cf01e8" translate="yes" xml:space="preserve">
          <source>A custom &lt;code&gt;setuptools&lt;/code&gt; build extension .</source>
          <target state="translated">自定义 &lt;code&gt;setuptools&lt;/code&gt; 构建扩展。</target>
        </trans-unit>
        <trans-unit id="4879ed25bb5ad2e7dbdac9f4ce13e07cb540cd8d" translate="yes" xml:space="preserve">
          <source>A decorator for a function indicating that the return value of the function is guaranteed to be a &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; object and this function can run asynchronously on the RPC callee. More specifically, the callee extracts the &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; returned by the wrapped function and installs subsequent processing steps as a callback to that &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt;. The installed callback will read the value from the &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; when completed and send the value back as the RPC response. That also means the returned &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; only exists on the callee side and is never sent through RPC. This decorator is useful when the wrapped function&amp;rsquo;s (&lt;code&gt;fn&lt;/code&gt;) execution needs to pause and resume due to, e.g., containing &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt; or waiting for other signals.</source>
          <target state="translated">一个函数的装饰器，指示该函数的返回值保证是&lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;对象，并且该函数可以在RPC被调用方上异步运行。更具体地说，被调用方提取包装函数返回的&lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;，并安装后续处理步骤作为对该&lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;的回调。完成后，已安装的回调将从&lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; 中&lt;/a&gt;读取值，并将该值作为RPC响应发送回去。这也意味着返回的&lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;仅存在于被调用方，并且永远不会通过RPC发送。当包装函数的执行（ &lt;code&gt;fn&lt;/code&gt; ）由于例如包含&lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt; &lt;code&gt;rpc_async()&lt;/code&gt; &lt;/a&gt;而需要暂停和恢复时，此装饰器很有用 或等待其他信号。</target>
        </trans-unit>
        <trans-unit id="1802e436eb51e9216bfa3579e7b06f1deabcfc67" translate="yes" xml:space="preserve">
          <source>A dict with key type &lt;code&gt;K&lt;/code&gt; and value type &lt;code&gt;V&lt;/code&gt;. Only &lt;code&gt;str&lt;/code&gt;, &lt;code&gt;int&lt;/code&gt;, and &lt;code&gt;float&lt;/code&gt; are allowed as key types.</source>
          <target state="translated">键类型为 &lt;code&gt;K&lt;/code&gt; 且值类型为 &lt;code&gt;V&lt;/code&gt; 的字典。只能将 &lt;code&gt;str&lt;/code&gt; ， &lt;code&gt;int&lt;/code&gt; 和 &lt;code&gt;float&lt;/code&gt; 用作键类型。</target>
        </trans-unit>
        <trans-unit id="d6b44f652191a0739c60209548e762d3272e171e" translate="yes" xml:space="preserve">
          <source>A dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute). Entries in the dictionary need to be QConfigDynamic instances.</source>
          <target state="translated">一个从子模块的名称或类型映射到量化配置的字典,qconfig适用于一个给定模块的所有子模块,除非指定了子模块的qconfig(当子模块已经有qconfig属性时)。字典中的条目需要是QConfigDynamic实例。</target>
        </trans-unit>
        <trans-unit id="7a9fd856e77dfc3abad8cb7687ffa5e312c2b57f" translate="yes" xml:space="preserve">
          <source>A distributed request object. None, if not part of the group</source>
          <target state="translated">一个分布式请求对象。如果不属于该组,则为无。</target>
        </trans-unit>
        <trans-unit id="250c27944374a4fc6ecb0aa3bd87a6c9fac89495" translate="yes" xml:space="preserve">
          <source>A dynamic quantized GRUCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as &lt;code&gt;torch.nn.GRUCell&lt;/code&gt;, please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell&lt;/a&gt; for documentation.</source>
          <target state="translated">具有浮点张量作为输入和输出的动态量化GRUCell模块。权重量化为8位。我们采用与 &lt;code&gt;torch.nn.GRUCell&lt;/code&gt; 相同的接口，有关文档，请参阅&lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="963ab39ba50a4586010326f40e7fcf681f7e7ed2" translate="yes" xml:space="preserve">
          <source>A dynamic quantized LSTM module with floating point tensor as inputs and outputs. We adopt the same interface as &lt;code&gt;torch.nn.LSTM&lt;/code&gt;, please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM&lt;/a&gt; for documentation.</source>
          <target state="translated">具有浮点张量作为输入和输出的动态量化LSTM模块。我们采用与 &lt;code&gt;torch.nn.LSTM&lt;/code&gt; 相同的界面，有关文档，请参阅&lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="2bfcec11b612479566fbcc608de442edef3fec3d" translate="yes" xml:space="preserve">
          <source>A dynamic quantized LSTMCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as &lt;code&gt;torch.nn.LSTMCell&lt;/code&gt;, please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell&lt;/a&gt; for documentation.</source>
          <target state="translated">具有浮点张量作为输入和输出的动态量化LSTMCell模块。权重量化为8位。我们采用与 &lt;code&gt;torch.nn.LSTMCell&lt;/code&gt; 相同的接口，有关文档，请参阅&lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="5659ac53c7a777e77a4de94c531f76bef16a78a7" translate="yes" xml:space="preserve">
          <source>A dynamic quantized linear module with floating point tensor as inputs and outputs. We adopt the same interface as &lt;code&gt;torch.nn.Linear&lt;/code&gt;, please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&lt;/a&gt; for documentation.</source>
          <target state="translated">具有浮点张量作为输入和输出的动态量化线性模块。我们采用与 &lt;code&gt;torch.nn.Linear&lt;/code&gt; 相同的界面，有关文档，请参阅&lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="582df41a91e883c691f7f02b4e1fc90ccc670cb7" translate="yes" xml:space="preserve">
          <source>A float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out.</source>
          <target state="translated">表示所有RPC超时的浮点数。如果一个RPC没有在这个时间范围内完成,它将以一个异常的方式完成,表明它已经超时。</target>
        </trans-unit>
        <trans-unit id="a62b336a0d844d8b369172276cac660873819d03" translate="yes" xml:space="preserve">
          <source>A floating point scalar operand has dtype &lt;code&gt;torch.get_default_dtype()&lt;/code&gt; and an integral non-boolean scalar operand has dtype &lt;code&gt;torch.int64&lt;/code&gt;. Unlike numpy, we do not inspect values when determining the minimum &lt;code&gt;dtypes&lt;/code&gt; of an operand. Quantized and complex types are not yet supported.</source>
          <target state="translated">浮点标量操作数的 &lt;code&gt;torch.get_default_dtype()&lt;/code&gt; 而整数非布尔型标量操作数的 &lt;code&gt;torch.int64&lt;/code&gt; 。与numpy不同，在确定操作数的最小 &lt;code&gt;dtypes&lt;/code&gt; 时，我们不检查值。尚不支持量化和复杂类型。</target>
        </trans-unit>
        <trans-unit id="b9ed960bad6396547e811b3062f43d2d9d84c3b7" translate="yes" xml:space="preserve">
          <source>A gated recurrent unit (GRU) cell</source>
          <target state="translated">一个门控递归单元(GRU)细胞。</target>
        </trans-unit>
        <trans-unit id="6500e1e873505e9557317cbc126e48e477677230" translate="yes" xml:space="preserve">
          <source>A handful of CUDA operations are nondeterministic if the CUDA version is 10.2 or greater, unless the environment variable &lt;code&gt;CUBLAS_WORKSPACE_CONFIG=:4096:8&lt;/code&gt; or &lt;code&gt;CUBLAS_WORKSPACE_CONFIG=:16:8&lt;/code&gt; is set. See the CUDA documentation for more details: &lt;a href=&quot;https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility&quot;&gt;https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility&lt;/a&gt; If one of these environment variable configurations is not set, a &lt;a href=&quot;https://docs.python.org/3/library/exceptions.html#RuntimeError&quot;&gt;&lt;code&gt;RuntimeError&lt;/code&gt;&lt;/a&gt; will be raised from these operations when called with CUDA tensors:</source>
          <target state="translated">如果CUDA版本为10.2或更高，则少数CUDA操作不确定，除非设置了环境变量 &lt;code&gt;CUBLAS_WORKSPACE_CONFIG=:4096:8&lt;/code&gt; 或 &lt;code&gt;CUBLAS_WORKSPACE_CONFIG=:16:8&lt;/code&gt; 。有关更多详细信息，请参阅CUDA文档：&lt;a href=&quot;https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility&quot;&gt;https&lt;/a&gt; : //docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility如果未设置这些环境变量配置之一，则当使用CUDA调用时，这些操作将引发&lt;a href=&quot;https://docs.python.org/3/library/exceptions.html#RuntimeError&quot;&gt; &lt;code&gt;RuntimeError&lt;/code&gt; &lt;/a&gt;张量</target>
        </trans-unit>
        <trans-unit id="7749157cec3d641bd4967868e0fa47bf1bc04040" translate="yes" xml:space="preserve">
          <source>A handle of distributed group that can be given to collective calls.</source>
          <target state="translated">一个分布式组的句柄,可以给集体调用。</target>
        </trans-unit>
        <trans-unit id="206d2d1444ddd6b067f98b835b8bca7ba47b6c99" translate="yes" xml:space="preserve">
          <source>A helper function for checkpointing sequential models.</source>
          <target state="translated">一个用于检查顺序模型的辅助函数。</target>
        </trans-unit>
        <trans-unit id="3fa20b5ac9b07bb8e81b28cca1e0d1e80ac301dd" translate="yes" xml:space="preserve">
          <source>A kind of Tensor that is to be considered a module parameter.</source>
          <target state="translated">一种被认为是模块参数的Tensor。</target>
        </trans-unit>
        <trans-unit id="f9cebb2a15f669cb89a337fe94399d68f146852b" translate="yes" xml:space="preserve">
          <source>A known limitation that worth mentioning here is user &lt;strong&gt;CANNOT&lt;/strong&gt; load two different branches of the same repo in the &lt;strong&gt;same python process&lt;/strong&gt;. It&amp;rsquo;s just like installing two packages with the same name in Python, which is not good. Cache might join the party and give you surprises if you actually try that. Of course it&amp;rsquo;s totally fine to load them in separate processes.</source>
          <target state="translated">已知限制，即值得一提的是用户&lt;strong&gt;CAN NOT&lt;/strong&gt;加载相同回购的两个不同的分支在&lt;strong&gt;同一Python进程&lt;/strong&gt;。就像在Python中安装两个具有相同名称的软件包一样，这是不好的。快取可能会加入聚会，如果您实际尝试的话会给您带来惊喜。当然，将它们加载到单独的进程中完全可以。</target>
        </trans-unit>
        <trans-unit id="5b6768469e9cca4e490d35bd92c3d9b2169e1d91" translate="yes" xml:space="preserve">
          <source>A linear module attached with FakeQuantize modules for weight, used for quantization aware training.</source>
          <target state="translated">一个附加了FakeQuantize模块的线性模块,用于权重,用于量化感知训练。</target>
        </trans-unit>
        <trans-unit id="dc7b6582ee399c321872ca72686d97afecec5335" translate="yes" xml:space="preserve">
          <source>A list of include path strings.</source>
          <target state="translated">一个包含路径字符串的列表。</target>
        </trans-unit>
        <trans-unit id="670f1df273800a76faec2dce35ff3b26f07b45a5" translate="yes" xml:space="preserve">
          <source>A list of the completed &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; results. This method will throw an error if &lt;code&gt;wait&lt;/code&gt; on any &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; throws.</source>
          <target state="translated">已完成的&lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;结果列表。如果 &lt;code&gt;wait&lt;/code&gt; 任何&lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;抛出，则此方法将抛出错误。</target>
        </trans-unit>
        <trans-unit id="957409376e9513abfec84a7b922ff172b12a1b20" translate="yes" xml:space="preserve">
          <source>A list of which all members are type &lt;code&gt;T&lt;/code&gt;</source>
          <target state="translated">所有成员均为 &lt;code&gt;T&lt;/code&gt; 型的列表</target>
        </trans-unit>
        <trans-unit id="087cf02c32b2e05acb68308382032450a4a20257" translate="yes" xml:space="preserve">
          <source>A long short-term memory (LSTM) cell.</source>
          <target state="translated">一种长短期记忆(LSTM)细胞。</target>
        </trans-unit>
        <trans-unit id="f18e48f94bce7b9c4d48d2ece0fb5fb7d9efe971" translate="yes" xml:space="preserve">
          <source>A map where the key is the Tensor and the value is the associated gradient for that Tensor.</source>
          <target state="translated">一个映射,其中键是Tensor,值是该Tensor的相关梯度。</target>
        </trans-unit>
        <trans-unit id="57d19bf168213ea66d938124a89f136c6e9d301e" translate="yes" xml:space="preserve">
          <source>A map-style dataset is one that implements the &lt;code&gt;__getitem__()&lt;/code&gt; and &lt;code&gt;__len__()&lt;/code&gt; protocols, and represents a map from (possibly non-integral) indices/keys to data samples.</source>
          <target state="translated">映射样式的数据集是一种实现 &lt;code&gt;__getitem__()&lt;/code&gt; 和 &lt;code&gt;__len__()&lt;/code&gt; 协议的数据集，它表示从（可能是非整数）索引/键到数据样本的映射。</target>
        </trans-unit>
        <trans-unit id="96c11bff8b88f586e9213a9a4fa4c5da81e27dfc" translate="yes" xml:space="preserve">
          <source>A namedtuple (eigenvalues, eigenvectors) containing</source>
          <target state="translated">包含以下内容的命名tuple(特征值、特征向量)。</target>
        </trans-unit>
        <trans-unit id="728e8297f47bdf6af1113fc81a7338a6111e3fbb" translate="yes" xml:space="preserve">
          <source>A namedtuple (sign, logabsdet) containing the sign of the determinant, and the log value of the absolute determinant.</source>
          <target state="translated">一个命名的tuple(sign,logabsdet),包含行列式的符号和绝对行列式的对数值。</target>
        </trans-unit>
        <trans-unit id="f051d331990381212770691e09eaebbe9dbf0969" translate="yes" xml:space="preserve">
          <source>A namedtuple (solution, QR) containing:</source>
          <target state="translated">包含以下内容的命名tuple(溶液、QR):</target>
        </trans-unit>
        <trans-unit id="1c9b39e663c1fba635b37123383556801e4f83c6" translate="yes" xml:space="preserve">
          <source>A namedtuple &lt;code&gt;(solution, cloned_coefficient)&lt;/code&gt; where &lt;code&gt;cloned_coefficient&lt;/code&gt; is a clone of</source>
          <target state="translated">甲namedtuple &lt;code&gt;(solution, cloned_coefficient)&lt;/code&gt; 其中 &lt;code&gt;cloned_coefficient&lt;/code&gt; 是的克隆</target>
        </trans-unit>
        <trans-unit id="5a372625038d1c917e3aaa8e230582a3f57787f0" translate="yes" xml:space="preserve">
          <source>A namedtuple of (values, indices) is returned, where the &lt;code&gt;values&lt;/code&gt; are the sorted values and &lt;code&gt;indices&lt;/code&gt; are the indices of the elements in the original &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">返回（值，索引）的namedtuple，其中 &lt;code&gt;values&lt;/code&gt; 是排序后的值， &lt;code&gt;indices&lt;/code&gt; 是原始 &lt;code&gt;input&lt;/code&gt; 张量中元素的索引。</target>
        </trans-unit>
        <trans-unit id="604316c9ed5b0a04586de06d5b7c827e9c474f1c" translate="yes" xml:space="preserve">
          <source>A namedtuple of &lt;code&gt;(values, indices)&lt;/code&gt; is returned, where the &lt;code&gt;indices&lt;/code&gt; are the indices of the elements in the original &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">返回 &lt;code&gt;(values, indices)&lt;/code&gt; namedtuple ，其中 &lt;code&gt;indices&lt;/code&gt; 是原始 &lt;code&gt;input&lt;/code&gt; 张量中元素的索引。</target>
        </trans-unit>
        <trans-unit id="2869157b898ed95d4d84e8714207a533d4266c67" translate="yes" xml:space="preserve">
          <source>A new &lt;code&gt;Future&lt;/code&gt; object that holds the return value of the &lt;code&gt;callback&lt;/code&gt; and will be marked as completed when the given &lt;code&gt;callback&lt;/code&gt; finishes.</source>
          <target state="translated">一个新的 &lt;code&gt;Future&lt;/code&gt; 对象，它保存 &lt;code&gt;callback&lt;/code&gt; 的返回值，并在给定的 &lt;code&gt;callback&lt;/code&gt; 完成时被标记为完成。</target>
        </trans-unit>
        <trans-unit id="86f61ab629b1ba7d19f7dcc0bb8025724554b7cc" translate="yes" xml:space="preserve">
          <source>A new optimized torch script module</source>
          <target state="translated">一个新的优化的火炬脚本模块</target>
        </trans-unit>
        <trans-unit id="42ae237d21ac89e5671cbfe0c552756ff6818727" translate="yes" xml:space="preserve">
          <source>A newly quantized tensor</source>
          <target state="translated">新量化的张量</target>
        </trans-unit>
        <trans-unit id="0f1ef90ea70757aed597fc7161c74a26091c5cc9" translate="yes" xml:space="preserve">
          <source>A non-complex output tensor cannot accept a complex tensor</source>
          <target state="translated">非复杂的输出张量不能接受复杂的张量。</target>
        </trans-unit>
        <trans-unit id="e7960fd6039dfb2110c74872f4e093bd99c9b78f" translate="yes" xml:space="preserve">
          <source>A number of epochs (epochs) and a number of steps per epoch (steps_per_epoch) are provided. In this case, the number of total steps is inferred by total_steps = epochs * steps_per_epoch</source>
          <target state="translated">提供了若干个纪元(epochs)和每个纪元的步数(step_per_epoch)。在这种情况下,总步数由total_steps=epochs*steps_per_epoch推断。</target>
        </trans-unit>
        <trans-unit id="8860b725134278b328cae593df46234adf438aae" translate="yes" xml:space="preserve">
          <source>A placeholder identity operator that is argument-insensitive.</source>
          <target state="translated">一个对参数不敏感的占位符身份操作符。</target>
        </trans-unit>
        <trans-unit id="68cb032d3fee685e7e338c671eb8bac87e4a9de4" translate="yes" xml:space="preserve">
          <source>A quantized linear module with quantized tensor as inputs and outputs. We adopt the same interface as &lt;code&gt;torch.nn.Linear&lt;/code&gt;, please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&lt;/a&gt; for documentation.</source>
          <target state="translated">具有量化张量作为输入和输出的量化线性模块。我们采用与 &lt;code&gt;torch.nn.Linear&lt;/code&gt; 相同的界面，有关文档，请参阅&lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.Linear&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="90e1352af2e81f61b051ad3af1316a3695d6f809" translate="yes" xml:space="preserve">
          <source>A readonly &lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/a&gt; that shows the number of plans currently in the cuFFT plan cache.</source>
          <target state="translated">一个只读&lt;a href=&quot;https://docs.python.org/3/library/functions.html#int&quot;&gt; &lt;code&gt;int&lt;/code&gt; &lt;/a&gt;，显示cuFFT计划缓存中当前的计划数。</target>
        </trans-unit>
        <trans-unit id="4fd9c2aa54ec6edcb5667f155a92b1ad827136ed" translate="yes" xml:space="preserve">
          <source>A scalar floating point number</source>
          <target state="translated">一个标量浮点数</target>
        </trans-unit>
        <trans-unit id="ab6a03d7d6d156530f83f56670a446dfe49b0bed" translate="yes" xml:space="preserve">
          <source>A scalar integer</source>
          <target state="translated">一个标量整数</target>
        </trans-unit>
        <trans-unit id="f6cc25f2f216f722bfbd2ae25c5bbc684d2ee1fd" translate="yes" xml:space="preserve">
          <source>A sequential container.</source>
          <target state="translated">一个顺序容器。</target>
        </trans-unit>
        <trans-unit id="b8abdf9e14775972dca4bbd0c85df17f123918a1" translate="yes" xml:space="preserve">
          <source>A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ordered dict of modules can also be passed in.</source>
          <target state="translated">一个有序的容器。模块将按照它们在构造函数中传递的顺序被添加到容器中。或者,也可以传入一个有序的模块指令。</target>
        </trans-unit>
        <trans-unit id="d4fc542619d19643a87df58571b1a24f4ec2df18" translate="yes" xml:space="preserve">
          <source>A sequential or shuffled sampler will be automatically constructed based on the &lt;code&gt;shuffle&lt;/code&gt; argument to a &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt;. Alternatively, users may use the &lt;code&gt;sampler&lt;/code&gt; argument to specify a custom &lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt;&lt;code&gt;Sampler&lt;/code&gt;&lt;/a&gt; object that at each time yields the next index/key to fetch.</source>
          <target state="translated">基于&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt;的 &lt;code&gt;shuffle&lt;/code&gt; 参数，将自动构造一个顺序采样或混洗的采样器。或者，用户可以使用 &lt;code&gt;sampler&lt;/code&gt; 参数指定一个自定义的&lt;a href=&quot;#torch.utils.data.Sampler&quot;&gt; &lt;code&gt;Sampler&lt;/code&gt; &lt;/a&gt;对象，该对象每次都会产生要提取的下一个索引/键。</target>
        </trans-unit>
        <trans-unit id="b2ef395639e01521c4ce724db2e4291244bc2c91" translate="yes" xml:space="preserve">
          <source>A set of types and/or submodule names to apply dynamic quantization to, in which case the &lt;code&gt;dtype&lt;/code&gt; argument is used to specify the bit-width</source>
          <target state="translated">一组类型和/或子模块名称，用于对其进行动态量化，在这种情况下， &lt;code&gt;dtype&lt;/code&gt; 参数用于指定位宽</target>
        </trans-unit>
        <trans-unit id="1eb58c4c0d2493633735de8b616a63c6ada9e2fd" translate="yes" xml:space="preserve">
          <source>A simple lookup table that looks up embeddings in a fixed dictionary and size.</source>
          <target state="translated">一个简单的查找表,可以在固定的字典和大小中查找嵌入。</target>
        </trans-unit>
        <trans-unit id="28b7624a806a8a7fcea708395f29b1cdc6294a33" translate="yes" xml:space="preserve">
          <source>A simple lookup table that stores embeddings of a fixed dictionary and size.</source>
          <target state="translated">一个简单的查找表,存储固定字典和大小的嵌入。</target>
        </trans-unit>
        <trans-unit id="55e7985c56ca03421c43dc04192249c84e14dc6a" translate="yes" xml:space="preserve">
          <source>A single dimension may be -1, in which case it&amp;rsquo;s inferred from the remaining dimensions and the number of elements in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">单个尺寸可能为-1，在这种情况下，它是根据其余尺寸和 &lt;code&gt;input&lt;/code&gt; 中元素的数量推断出来的。</target>
        </trans-unit>
        <trans-unit id="6fd372cb1c5e72e14169cfb741de07d656f9f533" translate="yes" xml:space="preserve">
          <source>A sparse tensor is represented as a pair of dense tensors: a tensor of values and a 2D tensor of indices. A sparse tensor can be constructed by providing these two tensors, as well as the size of the sparse tensor (which cannot be inferred from these tensors!) Suppose we want to define a sparse tensor with the entry 3 at location (0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2). We would then write:</source>
          <target state="translated">一个稀疏张量被表示为一对密集张量:一个值的张量和一个指数的二维张量。通过提供这两个张量,以及稀疏张量的大小(不能从这些张量中推断),可以构造一个稀疏张量。 假设我们想定义一个稀疏张量,入口3在位置(0,2),入口4在位置(1,0),入口5在位置(1,2)。那么我们可以写道</target>
        </trans-unit>
        <trans-unit id="25e7a287394a0eef59197bb60e52ac1f6a84a5df" translate="yes" xml:space="preserve">
          <source>A store implementation that uses a file to store the underlying key-value pairs.</source>
          <target state="translated">一个使用文件来存储底层键值对的存储实现。</target>
        </trans-unit>
        <trans-unit id="6eb9ae0897420968aee10a2b095c52bec596e05c" translate="yes" xml:space="preserve">
          <source>A string</source>
          <target state="translated">一串</target>
        </trans-unit>
        <trans-unit id="8f1eb77185d4ddf9f3156ac797aef0e5bc38f488" translate="yes" xml:space="preserve">
          <source>A string containing the table.</source>
          <target state="translated">包含表格的字符串。</target>
        </trans-unit>
        <trans-unit id="62ebd3a50dba8122d0758d5d23ce37a93f24ab7e" translate="yes" xml:space="preserve">
          <source>A structure that encapsulates information of a worker in the system. Contains the name and ID of the worker. This class is not meant to be constructed directly, rather, an instance can be retrieved through &lt;a href=&quot;#torch.distributed.rpc.get_worker_info&quot;&gt;&lt;code&gt;get_worker_info()&lt;/code&gt;&lt;/a&gt; and the result can be passed in to functions such as &lt;a href=&quot;#torch.distributed.rpc.rpc_sync&quot;&gt;&lt;code&gt;rpc_sync()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.rpc.remote&quot;&gt;&lt;code&gt;remote()&lt;/code&gt;&lt;/a&gt; to avoid copying a string on every invocation.</source>
          <target state="translated">在系统中封装工作人员信息的结构。包含工作人员的名称和ID。此类并不旨在直接构造，而是可以通过&lt;a href=&quot;#torch.distributed.rpc.get_worker_info&quot;&gt; &lt;code&gt;get_worker_info()&lt;/code&gt; &lt;/a&gt;检索实例，并将结果传递给&lt;a href=&quot;#torch.distributed.rpc.rpc_sync&quot;&gt; &lt;code&gt;rpc_sync()&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt; &lt;code&gt;rpc_async()&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;#torch.distributed.rpc.remote&quot;&gt; &lt;code&gt;remote()&lt;/code&gt; 之类的&lt;/a&gt;函数，以避免在每个实例上复制字符串。调用。</target>
        </trans-unit>
        <trans-unit id="eb68c1bf6abfe77fe220468c23dfc4e2ccab3d1b" translate="yes" xml:space="preserve">
          <source>A tensor can be constructed from a Python &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;&lt;code&gt;list&lt;/code&gt;&lt;/a&gt; or sequence using the &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt; constructor:</source>
          <target state="translated">可以使用&lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;torch.tensor()&lt;/code&gt; &lt;/a&gt;构造函数从Python&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt; &lt;code&gt;list&lt;/code&gt; &lt;/a&gt;或序列构造张量：</target>
        </trans-unit>
        <trans-unit id="78690c5a267b02f430ea07580ff8566f6ba33315" translate="yes" xml:space="preserve">
          <source>A tensor can be created with &lt;code&gt;requires_grad=True&lt;/code&gt; so that &lt;a href=&quot;autograd#module-torch.autograd&quot;&gt;&lt;code&gt;torch.autograd&lt;/code&gt;&lt;/a&gt; records operations on them for automatic differentiation.</source>
          <target state="translated">可以使用&lt;a href=&quot;autograd#module-torch.autograd&quot;&gt; &lt;code&gt;torch.autograd&lt;/code&gt; &lt;/a&gt; &lt;code&gt;requires_grad=True&lt;/code&gt; 创建张量，以便torch.autograd记录其上的操作以进行自动微分。</target>
        </trans-unit>
        <trans-unit id="9f80380a23d77d5e0da6bb3c9300dccfb821b826" translate="yes" xml:space="preserve">
          <source>A tensor containing an elementwise sum of all inputs, placed on the &lt;code&gt;destination&lt;/code&gt; device.</source>
          <target state="translated">包含所有输入的元素和的张量，放置在 &lt;code&gt;destination&lt;/code&gt; 设备上。</target>
        </trans-unit>
        <trans-unit id="28d24ca39991219e32d7e5d9431b5638f6178577" translate="yes" xml:space="preserve">
          <source>A tensor containing the STFT result with shape described above</source>
          <target state="translated">包含上述形状的STFT结果的张量。</target>
        </trans-unit>
        <trans-unit id="4fa2d7d56cae6fbb5c5952f1e0d2ce41103f7c68" translate="yes" xml:space="preserve">
          <source>A tensor containing the complex-to-complex Fourier transform result</source>
          <target state="translated">包含复数到复数傅里叶变换结果的张量。</target>
        </trans-unit>
        <trans-unit id="f7a71af7c596a286496bba8f33f4ec3e6cafe018" translate="yes" xml:space="preserve">
          <source>A tensor containing the complex-to-complex inverse Fourier transform result</source>
          <target state="translated">包含复数到复数反傅里叶变换结果的张量。</target>
        </trans-unit>
        <trans-unit id="247163b1a472b2aa12f5bee5985796f6dfb72690" translate="yes" xml:space="preserve">
          <source>A tensor containing the complex-to-real inverse Fourier transform result</source>
          <target state="translated">包含复数到实数反傅里叶变换结果的张量。</target>
        </trans-unit>
        <trans-unit id="795a9e2ee149db6f297c73b4efe85421d1b43891" translate="yes" xml:space="preserve">
          <source>A tensor containing the real-to-complex Fourier transform result</source>
          <target state="translated">包含实数到复数傅立叶变换结果的张量。</target>
        </trans-unit>
        <trans-unit id="808c26c158b1b41512b2f49967eb9955a23d3ece" translate="yes" xml:space="preserve">
          <source>A tensor equivalent to converting all the input tensors into lists,</source>
          <target state="translated">一个张量相当于将所有输入的张量转换为列表。</target>
        </trans-unit>
        <trans-unit id="9f00a5099d29a70d5489d75e795a620e76bd2892" translate="yes" xml:space="preserve">
          <source>A tensor equivalent to converting all the input tensors into lists, do &lt;code&gt;itertools.combinations&lt;/code&gt; or &lt;code&gt;itertools.combinations_with_replacement&lt;/code&gt; on these lists, and finally convert the resulting list into tensor.</source>
          <target state="translated">等于将所有输入张量转换为列表的张量，在这些列表上执行 &lt;code&gt;itertools.combinations&lt;/code&gt; 或 &lt;code&gt;itertools.combinations_with_replacement&lt;/code&gt; ，最后将结果列表转换为张量。</target>
        </trans-unit>
        <trans-unit id="ffa7cd4e81e99e52c54f7880515bfb697dd56d45" translate="yes" xml:space="preserve">
          <source>A tensor of shape equal to the broadcasted shape of &lt;code&gt;condition&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;</source>
          <target state="translated">形状的张量等于所广播的形状 &lt;code&gt;condition&lt;/code&gt; ， &lt;code&gt;x&lt;/code&gt; ， &lt;code&gt;y&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="9f636fee4017e68defd8eca1418692d893e6ca2e" translate="yes" xml:space="preserve">
          <source>A tensor of specific data type can be constructed by passing a &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and/or a &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; to a constructor or tensor creation op:</source>
          <target state="translated">可以通过将&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;和/或&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;传递给构造函数或张量创建操作来构造特定数据类型的张量：</target>
        </trans-unit>
        <trans-unit id="56a6db01c3a1cc54c7399f9ada8907490b637b87" translate="yes" xml:space="preserve">
          <source>A tensor or a tuple of tensors containing</source>
          <target state="translated">包含以下内容的张量或张量的元组</target>
        </trans-unit>
        <trans-unit id="7015c602ad96676ce56c48693c2eb3931eef821e" translate="yes" xml:space="preserve">
          <source>A thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes.</source>
          <target state="translated">基于底层哈希图的线程安全存储实现。该存储可以在同一进程中使用(例如,被其他线程使用),但不能跨进程使用。</target>
        </trans-unit>
        <trans-unit id="fc0920364dd37ceba72cafbd19ccdfa555706749" translate="yes" xml:space="preserve">
          <source>A transformer model.</source>
          <target state="translated">一个变压器模型。</target>
        </trans-unit>
        <trans-unit id="6179bf6b8f65bc90ae5ee40cc1d9226f83588987" translate="yes" xml:space="preserve">
          <source>A transformer model. User is able to modify the attributes as needed. The architecture is based on the paper &amp;ldquo;Attention Is All You Need&amp;rdquo;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users can build the BERT(&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;) model with corresponding parameters.</source>
          <target state="translated">变压器模型。用户可以根据需要修改属性。该架构基于论文&amp;ldquo;注意就是您所需要的&amp;rdquo;。Ashish Vaswani，Noam Shazeer，Niki Parmar，Jakob Uszkoreit，Llion Jones，Aidan N Gomez，Lukasz Kaiser和Illia Polosukhin。2017年。您只需要关注即可。《神经信息处理系统的发展》，第6000-6010页。用户可以使用相应的参数构建BERT（&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;）模型。</target>
        </trans-unit>
        <trans-unit id="8d0f75196825a6a65fa8d7281e970ef1150c3424" translate="yes" xml:space="preserve">
          <source>A tuple containing copies of &lt;code&gt;tensor&lt;/code&gt;, placed on &lt;code&gt;devices&lt;/code&gt;.</source>
          <target state="translated">一个包含张 &lt;code&gt;tensor&lt;/code&gt; 副本的元组，放置在 &lt;code&gt;devices&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="8d66b15a6d20ea906de4e959af867fbbfb815b34" translate="yes" xml:space="preserve">
          <source>A tuple containing subtypes &lt;code&gt;T0&lt;/code&gt;, &lt;code&gt;T1&lt;/code&gt;, etc. (e.g. &lt;code&gt;Tuple[Tensor, Tensor]&lt;/code&gt;)</source>
          <target state="translated">包含子类型 &lt;code&gt;T0&lt;/code&gt; ， &lt;code&gt;T1&lt;/code&gt; 等的 &lt;code&gt;Tuple[Tensor, Tensor]&lt;/code&gt; （例如Tuple [Tensor，Tensor]）</target>
        </trans-unit>
        <trans-unit id="d1879bf0c8405911e4b464be498b79cafb324a20" translate="yes" xml:space="preserve">
          <source>A tuple of tensors containing</source>
          <target state="translated">一个包含以下内容的元组</target>
        </trans-unit>
        <trans-unit id="a2414a0a13983fc4726a965cde7c58a73e7199ce" translate="yes" xml:space="preserve">
          <source>A user &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;&lt;code&gt;RRef&lt;/code&gt;&lt;/a&gt; instance to the result value. Use the blocking API &lt;a href=&quot;#torch.distributed.rpc.RRef.to_here&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.to_here()&lt;/code&gt;&lt;/a&gt; to retrieve the result value locally.</source>
          <target state="translated">用户&lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt; &lt;code&gt;RRef&lt;/code&gt; &lt;/a&gt;实例的结果值。使用阻塞API &lt;a href=&quot;#torch.distributed.rpc.RRef.to_here&quot;&gt; &lt;code&gt;torch.distributed.rpc.RRef.to_here()&lt;/code&gt; &lt;/a&gt;在本地检索结果值。</target>
        </trans-unit>
        <trans-unit id="e92261305fa4f64f01209063cb7d69a18986cd91" translate="yes" xml:space="preserve">
          <source>A value for total_steps is explicitly provided.</source>
          <target state="translated">明确地提供了一个 total_steps 的值。</target>
        </trans-unit>
        <trans-unit id="327c954237e47bd5486826604abda3addea9acfd" translate="yes" xml:space="preserve">
          <source>A value which is either None or type &lt;code&gt;T&lt;/code&gt;</source>
          <target state="translated">无或为 &lt;code&gt;T&lt;/code&gt; 的值</target>
        </trans-unit>
        <trans-unit id="50a643ab21488e75a8cc3c466f11fce2f4ad673a" translate="yes" xml:space="preserve">
          <source>A wrapper around Python&amp;rsquo;s assert which is symbolically traceable.</source>
          <target state="translated">Python的assert的包装，可以用符号来跟踪。</target>
        </trans-unit>
        <trans-unit id="6c4e7377e95c980888bb30d4e414aed707349a68" translate="yes" xml:space="preserve">
          <source>A wrapper around any of the 3 key-value stores (&lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt;&lt;code&gt;TCPStore&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt;&lt;code&gt;FileStore&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt;&lt;code&gt;HashStore&lt;/code&gt;&lt;/a&gt;) that adds a prefix to each key inserted to the store.</source>
          <target state="translated">3个键值存储库（&lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt; &lt;code&gt;TCPStore&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;#torch.distributed.FileStore&quot;&gt; &lt;code&gt;FileStore&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;#torch.distributed.HashStore&quot;&gt; &lt;code&gt;HashStore&lt;/code&gt; &lt;/a&gt;）中的任何一个包装器，为每个插入到该存储库的键添加前缀。</target>
        </trans-unit>
        <trans-unit id="cad644a418935c3babf0ae5b058013172734574e" translate="yes" xml:space="preserve">
          <source>A wrapper class that wraps the input module, adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules.</source>
          <target state="translated">一个包装输入模块的包装器类,增加QuantStub和DeQuantStub,并在调用模块时包围调用quant和dequant模块。</target>
        </trans-unit>
        <trans-unit id="893fa7187a5a433ff31c6a646ba041165efd5e8a" translate="yes" xml:space="preserve">
          <source>A. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks: &lt;a href=&quot;https://www.cs.toronto.edu/~graves/icml_2006.pdf&quot;&gt;https://www.cs.toronto.edu/~graves/icml_2006.pdf&lt;/a&gt;</source>
          <target state="translated">A. Graves等人：Connectionist时间分类：使用递归神经网络标记未分段的序列数据：&lt;a href=&quot;https://www.cs.toronto.edu/~graves/icml_2006.pdf&quot;&gt;https&lt;/a&gt; ://www.cs.toronto.edu/~graves/icml_2006.pdf</target>
        </trans-unit>
        <trans-unit id="1eb4168992b5101108db3eeb3c96de61bbb45012" translate="yes" xml:space="preserve">
          <source>APIs in the RPC package are stable. There are multiple ongoing work items to improve performance and error handling, which will ship in future releases.</source>
          <target state="translated">RPC包中的API是稳定的。有多个正在进行的工作项目,以改善性能和错误处理,这些项目将在未来的版本中发布。</target>
        </trans-unit>
        <trans-unit id="bccb0000d0e87e05464132ae5d1a9fb1c8982b8a" translate="yes" xml:space="preserve">
          <source>ATen operators</source>
          <target state="translated">ATen运营商</target>
        </trans-unit>
        <trans-unit id="4bfc2c1b851857261545e0a7eb83c1db10ae6260" translate="yes" xml:space="preserve">
          <source>AX = B</source>
          <target state="translated">AX=B</target>
        </trans-unit>
        <trans-unit id="afc108ce2cbe35fd87218f8b08145cd05b2342e4" translate="yes" xml:space="preserve">
          <source>AX = b</source>
          <target state="translated">AX=b</target>
        </trans-unit>
        <trans-unit id="0d45206d15c0adf19b111143610d728deb04b811" translate="yes" xml:space="preserve">
          <source>A^T A / (m - 1)</source>
          <target state="translated">A^T A/(m-1)</target>
        </trans-unit>
        <trans-unit id="4c988bad76f6d7f308c6178b6dbc825504d4a7f8" translate="yes" xml:space="preserve">
          <source>Abstract base class for constraints.</source>
          <target state="translated">约束的抽象基类。</target>
        </trans-unit>
        <trans-unit id="086c7a89401b2c5d32553704f5cbdf2377564e0a" translate="yes" xml:space="preserve">
          <source>Abstract base class for creation of new pruning techniques.</source>
          <target state="translated">用于创建新的修剪技术的抽象基类。</target>
        </trans-unit>
        <trans-unit id="b96999daf7913ffe559be13e1a22b45ae0acedc9" translate="yes" xml:space="preserve">
          <source>Abstract class for invertable transformations with computable log det jacobians. They are primarily used in &lt;code&gt;torch.distributions.TransformedDistribution&lt;/code&gt;.</source>
          <target state="translated">可计算log det jacobians的可逆转换的抽象类。它们主要用于 &lt;code&gt;torch.distributions.TransformedDistribution&lt;/code&gt; 中。</target>
        </trans-unit>
        <trans-unit id="ce6150c1e37157c2b0ede38ac1f069f652c4bd27" translate="yes" xml:space="preserve">
          <source>Accepts as argument an instance of a BasePruningMethod or an iterable of them.</source>
          <target state="translated">接受BasePruningMethod的实例或它们的可迭代实例作为参数。</target>
        </trans-unit>
        <trans-unit id="f884a5a94ee360d4b70504f91c5e58e4219b6c1e" translate="yes" xml:space="preserve">
          <source>Accessing Module Parameters</source>
          <target state="translated">访问模块参数</target>
        </trans-unit>
        <trans-unit id="f2929dbc213f512c19707fd1ca48c5d74f64d113" translate="yes" xml:space="preserve">
          <source>Accumulate the elements of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; into the &lt;code&gt;self&lt;/code&gt; tensor by adding to the indices in the order given in &lt;code&gt;index&lt;/code&gt;. For example, if &lt;code&gt;dim == 0&lt;/code&gt; and &lt;code&gt;index[i] == j&lt;/code&gt;, then the &lt;code&gt;i&lt;/code&gt;th row of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; is added to the &lt;code&gt;j&lt;/code&gt;th row of &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">积累的元素&lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt;到 &lt;code&gt;self&lt;/code&gt; 通过添加到在给定的顺序的索引张量 &lt;code&gt;index&lt;/code&gt; 。例如，如果 &lt;code&gt;dim == 0&lt;/code&gt; 且 &lt;code&gt;index[i] == j&lt;/code&gt; ，则将第 &lt;code&gt;i&lt;/code&gt; 行&lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt;添加到 &lt;code&gt;self&lt;/code&gt; 的第 &lt;code&gt;j&lt;/code&gt; 行中。</target>
        </trans-unit>
        <trans-unit id="af92bf9a55b4e8ebc9cf19933cf41beac97277f2" translate="yes" xml:space="preserve">
          <source>Adaptive softmax is an approximate strategy for training models with large output spaces. It is most effective when the label distribution is highly imbalanced, for example in natural language modelling, where the word frequency distribution approximately follows the &lt;a href=&quot;https://en.wikipedia.org/wiki/Zipf%27s_law&quot;&gt;Zipf&amp;rsquo;s law&lt;/a&gt;.</source>
          <target state="translated">自适应softmax是用于训练具有大输出空间的模型的近似策略。当标签分布高度不平衡时，例如在自然语言建模中，单词频率分布大致遵循&lt;a href=&quot;https://en.wikipedia.org/wiki/Zipf%27s_law&quot;&gt;Zipf定律&lt;/a&gt;时，这是最有效的。</target>
        </trans-unit>
        <trans-unit id="6753d63f427d91531462240c7159228dba9dbe41" translate="yes" xml:space="preserve">
          <source>Adaptive softmax partitions the labels into several clusters, according to their frequency. These clusters may contain different number of targets each. Additionally, clusters containing less frequent labels assign lower dimensional embeddings to those labels, which speeds up the computation. For each minibatch, only clusters for which at least one target is present are evaluated.</source>
          <target state="translated">自适应softmax根据标签的频率,将标签划分为多个簇。这些聚类可以包含不同数量的目标,每个聚类的目标数量不同。此外,包含频率较低的标签的聚类为这些标签分配较低维度的嵌入,从而加快计算速度。对于每个小批量,只有至少有一个目标存在的聚类才会被评估。</target>
        </trans-unit>
        <trans-unit id="b7d62c155e1ec348bbbea519cad55c85951b3b6e" translate="yes" xml:space="preserve">
          <source>AdaptiveAvgPool1d</source>
          <target state="translated">AdaptiveAvgPool1d</target>
        </trans-unit>
        <trans-unit id="0068238690eed5246c61ee781eb3611fe6751168" translate="yes" xml:space="preserve">
          <source>AdaptiveAvgPool2d</source>
          <target state="translated">AdaptiveAvgPool2d</target>
        </trans-unit>
        <trans-unit id="65992cfd09d95132c3908bd45a21c959a32be4e1" translate="yes" xml:space="preserve">
          <source>AdaptiveAvgPool3d</source>
          <target state="translated">AdaptiveAvgPool3d</target>
        </trans-unit>
        <trans-unit id="a60f272cfc14a259a9716f4818eb79c22d95c973" translate="yes" xml:space="preserve">
          <source>AdaptiveLogSoftmaxWithLoss</source>
          <target state="translated">AdaptiveLogSoftmaxWithLoss</target>
        </trans-unit>
        <trans-unit id="84f10363ff1424545148abb336751c9a84b6497d" translate="yes" xml:space="preserve">
          <source>AdaptiveMaxPool1d</source>
          <target state="translated">AdaptiveMaxPool1d</target>
        </trans-unit>
        <trans-unit id="b001386de556a7b95313a763136a21be6a3d2d16" translate="yes" xml:space="preserve">
          <source>AdaptiveMaxPool2d</source>
          <target state="translated">AdaptiveMaxPool2d</target>
        </trans-unit>
        <trans-unit id="9315f841b98a9cfd3c84766eadb0e1b9b7bcbe32" translate="yes" xml:space="preserve">
          <source>AdaptiveMaxPool3d</source>
          <target state="translated">AdaptiveMaxPool3d</target>
        </trans-unit>
        <trans-unit id="86d178452c7b5ca14e4bbe65a1027bda69043129" translate="yes" xml:space="preserve">
          <source>Add a param group to the &lt;a href=&quot;#torch.optim.Optimizer&quot;&gt;&lt;code&gt;Optimizer&lt;/code&gt;&lt;/a&gt; s &lt;code&gt;param_groups&lt;/code&gt;.</source>
          <target state="translated">将参数组添加到&lt;a href=&quot;#torch.optim.Optimizer&quot;&gt; &lt;code&gt;Optimizer&lt;/code&gt; &lt;/a&gt;的 &lt;code&gt;param_groups&lt;/code&gt; 中。</target>
        </trans-unit>
        <trans-unit id="53b75d7b44ea18bab3550e79b5f19b4d01eb4d3d" translate="yes" xml:space="preserve">
          <source>Add a scalar or tensor to &lt;code&gt;self&lt;/code&gt; tensor. If both &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; are specified, each element of &lt;code&gt;other&lt;/code&gt; is scaled by &lt;code&gt;alpha&lt;/code&gt; before being used.</source>
          <target state="translated">在 &lt;code&gt;self&lt;/code&gt; 张量上添加标量或张量。如果两个 &lt;code&gt;alpha&lt;/code&gt; 和 &lt;code&gt;other&lt;/code&gt; 指定的每个元素 &lt;code&gt;other&lt;/code&gt; 通过缩放 &lt;code&gt;alpha&lt;/code&gt; 在使用前。</target>
        </trans-unit>
        <trans-unit id="363f75ca10e654de02076c9e812636b00b9240b1" translate="yes" xml:space="preserve">
          <source>Add a set of hyperparameters to be compared in TensorBoard.</source>
          <target state="translated">在TensorBoard中添加一组要比较的超参数。</target>
        </trans-unit>
        <trans-unit id="2981d9b125bbce3a8357f497272caa27d116b65e" translate="yes" xml:space="preserve">
          <source>Add audio data to summary.</source>
          <target state="translated">将音频数据添加到摘要中。</target>
        </trans-unit>
        <trans-unit id="378dceab0437ac530e9f4b067ecb1268bcc2db27" translate="yes" xml:space="preserve">
          <source>Add batched image data to summary.</source>
          <target state="translated">将批量图像数据添加到摘要中。</target>
        </trans-unit>
        <trans-unit id="b6c70cf24a9dc9ee0a583b95ed8a5b22e79a979d" translate="yes" xml:space="preserve">
          <source>Add embedding projector data to summary.</source>
          <target state="translated">在摘要中增加嵌入投影机数据。</target>
        </trans-unit>
        <trans-unit id="745fa53d610381d82d054b114f7c40ae6aaa4f61" translate="yes" xml:space="preserve">
          <source>Add graph data to summary.</source>
          <target state="translated">将图形数据添加到摘要中。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
