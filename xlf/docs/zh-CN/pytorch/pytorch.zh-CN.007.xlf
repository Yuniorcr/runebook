<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="76b27001feb75250d904bb68f5aaa46711a6a0f1" translate="yes" xml:space="preserve">
          <source>Expands the dimension &lt;a href=&quot;tensors#torch.Tensor.dim&quot;&gt;&lt;code&gt;dim&lt;/code&gt;&lt;/a&gt; of the &lt;code&gt;self&lt;/code&gt; tensor over multiple dimensions of sizes given by &lt;code&gt;sizes&lt;/code&gt;.</source>
          <target state="translated">扩展尺寸&lt;a href=&quot;tensors#torch.Tensor.dim&quot;&gt; &lt;code&gt;dim&lt;/code&gt; &lt;/a&gt;的的 &lt;code&gt;self&lt;/code&gt; 张量超过由下式给出尺寸的多个尺寸 &lt;code&gt;sizes&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="6e40e001bf1e0b420d657a639d6d67387b6d4923" translate="yes" xml:space="preserve">
          <source>Expected inputs are spatial (4 dimensional). Use &lt;code&gt;upsample_trilinear&lt;/code&gt; fo volumetric (5 dimensional) inputs.</source>
          <target state="translated">预期的输入是空间（4维）。使用 &lt;code&gt;upsample_trilinear&lt;/code&gt; 体积（5维）输入。</target>
        </trans-unit>
        <trans-unit id="551ccd41342efa881333f0f1e558164218f9345a" translate="yes" xml:space="preserve">
          <source>Expected result:</source>
          <target state="translated">预期结果:</target>
        </trans-unit>
        <trans-unit id="8439d6715059ee70b74368357e1335f786d20b96" translate="yes" xml:space="preserve">
          <source>Expects &lt;code&gt;input&lt;/code&gt; to be &amp;lt;= 2-D tensor and transposes dimensions 0 and 1.</source>
          <target state="translated">期望 &lt;code&gt;input&lt;/code&gt; 为&amp;lt;= 2-D张量并转置尺寸0和1。</target>
        </trans-unit>
        <trans-unit id="8133389ca86b79f9ac63f2057897dfbe1cda5cc8" translate="yes" xml:space="preserve">
          <source>Explicit alignment by names</source>
          <target state="translated">按名称明确排列</target>
        </trans-unit>
        <trans-unit id="34533450ac1f4cb8a131c86d48accad539bed843" translate="yes" xml:space="preserve">
          <source>Exponential</source>
          <target state="translated">Exponential</target>
        </trans-unit>
        <trans-unit id="7d2243304a874bff81f4d326427b9649a3f42d91" translate="yes" xml:space="preserve">
          <source>ExponentialFamily</source>
          <target state="translated">ExponentialFamily</target>
        </trans-unit>
        <trans-unit id="e8c6f63a2d8909014d6aec0ec531f11a54dcd1bb" translate="yes" xml:space="preserve">
          <source>ExponentialFamily is the abstract base class for probability distributions belonging to an exponential family, whose probability mass/density function has the form is defined below</source>
          <target state="translated">ExponentialFamily是属于指数族的概率分布的抽象基类,其概率质量/密度函数的形式定义如下。</target>
        </trans-unit>
        <trans-unit id="58b807aacff8abe3f97a111c7a1e5f71d192fe91" translate="yes" xml:space="preserve">
          <source>Export a model into ONNX format. This exporter runs your model once in order to get a trace of its execution to be exported; at the moment, it supports a limited set of dynamic models (e.g., RNNs.)</source>
          <target state="translated">将模型导出为ONNX格式。这个导出器会运行您的模型一次,以获得导出模型的执行痕迹;目前,它只支持一组有限的动态模型(例如,RNNs)。</target>
        </trans-unit>
        <trans-unit id="69c85a023a54b1f8defe44bdc84c4b1308b399c8" translate="yes" xml:space="preserve">
          <source>Exporting models with unsupported ONNX operators can be achieved using the &lt;code&gt;operator_export_type&lt;/code&gt; flag in export API. This flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX.</source>
          <target state="translated">可以使用导出API中的 &lt;code&gt;operator_export_type&lt;/code&gt; 标志来实现使用不受支持的ONNX运算符导出模型。当用户尝试导出未在ONNX中注册和支持的ATen和非ATen运算符时，此标志很有用。</target>
        </trans-unit>
        <trans-unit id="f1d6a87a24293323dbd3502f982e0c3f518eed8d" translate="yes" xml:space="preserve">
          <source>Exports an EventList as a Chrome tracing tools file.</source>
          <target state="translated">将事件列表导出为Chrome追踪工具文件。</target>
        </trans-unit>
        <trans-unit id="ae5fccd8dcd8fc317f8edfc8259af86cd2967a29" translate="yes" xml:space="preserve">
          <source>Expressions</source>
          <target state="translated">Expressions</target>
        </trans-unit>
        <trans-unit id="44bdb40abdeed26ffb35b097c6be620648eb56bc" translate="yes" xml:space="preserve">
          <source>Extending PyTorch</source>
          <target state="translated">扩展PyTorch</target>
        </trans-unit>
        <trans-unit id="9ff04a6a0866fa949548184e90e852102f7c5167" translate="yes" xml:space="preserve">
          <source>Extension of the Distribution class, which applies a sequence of Transforms to a base distribution. Let f be the composition of transforms applied:</source>
          <target state="translated">分布类的扩展,它将一连串的变换应用于一个基本分布。让f成为所应用的变换的组合。</target>
        </trans-unit>
        <trans-unit id="a505bba828df658cd19ca21d48f6b057b88e1432" translate="yes" xml:space="preserve">
          <source>Extra care needs to be taken when backward through &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; outputs. Such operation is really only stable when &lt;code&gt;input&lt;/code&gt; is full rank with all distinct singular values. Otherwise, &lt;code&gt;NaN&lt;/code&gt; can appear as the gradients are not properly defined. Also, notice that double backward will usually do an additional backward through &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; even if the original backward is only on &lt;code&gt;S&lt;/code&gt;.</source>
          <target state="translated">向后通过 &lt;code&gt;U&lt;/code&gt; 和 &lt;code&gt;V&lt;/code&gt; 输出时，需要格外小心。仅当 &lt;code&gt;input&lt;/code&gt; 具有所有不同的奇异值的完整等级时，此类操作才真正稳定。否则，由于梯度定义不正确，可能会出现 &lt;code&gt;NaN&lt;/code&gt; 。此外，请注意，即使原始后退仅出现在 &lt;code&gt;S&lt;/code&gt; 上，两次后退通常也会通过 &lt;code&gt;U&lt;/code&gt; 和 &lt;code&gt;V&lt;/code&gt; 进行额外的后退。</target>
        </trans-unit>
        <trans-unit id="7869f96c8dcb40e05a18db44c02551a0604c8dd8" translate="yes" xml:space="preserve">
          <source>Extra care needs to be taken when backward through outputs. Such operation is really only stable when all eigenvalues are distinct. Otherwise, &lt;code&gt;NaN&lt;/code&gt; can appear as the gradients are not properly defined.</source>
          <target state="translated">向后通过输出时，需要格外小心。只有当所有特征值都不同时，这种操作才真正稳定。否则，由于梯度定义不正确，可能会出现 &lt;code&gt;NaN&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="35e4d02caad3850761be7f0a939f52dc8e6d05c6" translate="yes" xml:space="preserve">
          <source>Extracts sliding local blocks from a batched input tensor.</source>
          <target state="translated">从分批输入张量中提取滑动局部块。</target>
        </trans-unit>
        <trans-unit id="545d81b806ede14bef050d6bbe02b72b7b5279d1" translate="yes" xml:space="preserve">
          <source>Extracts sliding local blocks from an batched input tensor.</source>
          <target state="translated">从分批输入张量中提取滑动局部块。</target>
        </trans-unit>
        <trans-unit id="e69f20e9f683920d3fb4329abd951e878b1f9372" translate="yes" xml:space="preserve">
          <source>F</source>
          <target state="translated">F</target>
        </trans-unit>
        <trans-unit id="bf9ce34c9cacb2121a3faa4bcb13d39153683bf1" translate="yes" xml:space="preserve">
          <source>F(\theta)</source>
          <target state="translated">F(\theta)</target>
        </trans-unit>
        <trans-unit id="d1636ed5d55d52fc51dab6a56c1fdb216178f4c5" translate="yes" xml:space="preserve">
          <source>FAST mode algorithm</source>
          <target state="translated">FAST模式算法</target>
        </trans-unit>
        <trans-unit id="5d8a2052196e0929b1ddeff4f9fa793509ac8f6c" translate="yes" xml:space="preserve">
          <source>FCN ResNet101</source>
          <target state="translated">FCN ResNet101</target>
        </trans-unit>
        <trans-unit id="d876bcc45100c189667e02c674ab45db60de2d8b" translate="yes" xml:space="preserve">
          <source>FCN ResNet50</source>
          <target state="translated">FCN ResNet50</target>
        </trans-unit>
        <trans-unit id="70deee53be1d417368b869145a93da9f61814dda" translate="yes" xml:space="preserve">
          <source>FCN ResNet50, ResNet101</source>
          <target state="translated">FCN ResNet50、ResNet101</target>
        </trans-unit>
        <trans-unit id="58296524e883e134fa6cc4840027902c76d7e637" translate="yes" xml:space="preserve">
          <source>Factory functions now take a new &lt;code&gt;names&lt;/code&gt; argument that associates a name with each dimension.</source>
          <target state="translated">现在，工厂函数采用了一个新的 &lt;code&gt;names&lt;/code&gt; 参数，该名称将名称与每个维度相关联。</target>
        </trans-unit>
        <trans-unit id="97cdbdc7feff827efb082a6b6dd2727237cd49fd" translate="yes" xml:space="preserve">
          <source>False</source>
          <target state="translated">False</target>
        </trans-unit>
        <trans-unit id="6bca42e6cb3531d60196488b0aafe02849dfad8a" translate="yes" xml:space="preserve">
          <source>False if the compiler is (likely) ABI-incompatible with PyTorch, else True.</source>
          <target state="translated">如果编译器与 PyTorch 不兼容(很可能),则为 False,否则为 True。</target>
        </trans-unit>
        <trans-unit id="f66134050db6d6f1871f0a392769d7d300bffc78" translate="yes" xml:space="preserve">
          <source>Faster R-CNN</source>
          <target state="translated">更快的R-CNN</target>
        </trans-unit>
        <trans-unit id="19fa372bf4511d5897a6678506a36d53384e83c1" translate="yes" xml:space="preserve">
          <source>Faster R-CNN ResNet-50 FPN</source>
          <target state="translated">更快的R-CNN ResNet-50 FPN。</target>
        </trans-unit>
        <trans-unit id="ceaa939a1707b8201f9f233e5c8d2c8a11872247" translate="yes" xml:space="preserve">
          <source>Faster R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.</source>
          <target state="translated">更快的R-CNN可输出到ONNX,输入固定大小的图像,并有固定的批量大小。</target>
        </trans-unit>
        <trans-unit id="b9a92e1a2a80529a37624caec86105f3856f6680" translate="yes" xml:space="preserve">
          <source>FeatureDropout (training mode not supported)</source>
          <target state="translated">特征缺失(不支持训练模式)。</target>
        </trans-unit>
        <trans-unit id="023ddfe2580672ca0eeb5f867eac27e114575b07" translate="yes" xml:space="preserve">
          <source>Features described in this documentation are classified by release status:</source>
          <target state="translated">本文档中描述的功能按发布状态分类。</target>
        </trans-unit>
        <trans-unit id="4ed223cb662e6eadab1b5774d593969ec64456ed" translate="yes" xml:space="preserve">
          <source>Features for large-scale deployments</source>
          <target state="translated">大规模部署的特点</target>
        </trans-unit>
        <trans-unit id="0e0100beb8ca4fc8e23a8eb611272bb7c779f1f4" translate="yes" xml:space="preserve">
          <source>File descriptor - &lt;code&gt;file_descriptor&lt;/code&gt;</source>
          <target state="translated">文件描述符 &lt;code&gt;file_descriptor&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f8affbccfbf014c7f4e4336ada91b93349f5adf6" translate="yes" xml:space="preserve">
          <source>File system - &lt;code&gt;file_system&lt;/code&gt;</source>
          <target state="translated">文件系统 &lt;code&gt;file_system&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="29fd83b7db12e4d9231f579c15fdaeb5d71086e4" translate="yes" xml:space="preserve">
          <source>Fill the main diagonal of a tensor that has at least 2-dimensions. When dims&amp;gt;2, all dimensions of input must be of equal length. This function modifies the input tensor in-place, and returns the input tensor.</source>
          <target state="translated">填充具有至少2维的张量的主对角线。当dims&amp;gt; 2时，输入的所有尺寸必须相等。此函数就地修改输入张量，并返回输入张量。</target>
        </trans-unit>
        <trans-unit id="46412e89386beda08d3ce994cbdd41f7b9cb6e05" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with elements drawn from the exponential distribution:</source>
          <target state="translated">用从指数分布中得出的元素填充 &lt;code&gt;self&lt;/code&gt; 张量：</target>
        </trans-unit>
        <trans-unit id="1aea402a861ce53378696f47d930c7de8244acde" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with elements drawn from the geometric distribution:</source>
          <target state="translated">用从几何分布中绘制的元素填充 &lt;code&gt;self&lt;/code&gt; 张量：</target>
        </trans-unit>
        <trans-unit id="1abd5eabced3b954f0b9c8f459ed264742cdc1be" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with elements samples from the normal distribution parameterized by &lt;a href=&quot;generated/torch.mean#torch.mean&quot;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.std#torch.std&quot;&gt;&lt;code&gt;std&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">用&lt;a href=&quot;generated/torch.mean#torch.mean&quot;&gt; &lt;code&gt;mean&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/torch.std#torch.std&quot;&gt; &lt;code&gt;std&lt;/code&gt; &lt;/a&gt;参数化的正态分布的元素样本填充 &lt;code&gt;self&lt;/code&gt; 张量。</target>
        </trans-unit>
        <trans-unit id="be7637b77a168dc9781dec5a6963103f27f1e666" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with numbers sampled from the continuous uniform distribution:</source>
          <target state="translated">用从连续均匀分布中采样的数字填充 &lt;code&gt;self&lt;/code&gt; 张量：</target>
        </trans-unit>
        <trans-unit id="963887f4ec9debd27ff138179ec127b4ceb1a324" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with numbers sampled from the discrete uniform distribution over &lt;code&gt;[from, to - 1]&lt;/code&gt;. If not specified, the values are usually only bounded by &lt;code&gt;self&lt;/code&gt; tensor&amp;rsquo;s data type. However, for floating point types, if unspecified, range will be &lt;code&gt;[0, 2^mantissa]&lt;/code&gt; to ensure that every value is representable. For example, &lt;code&gt;torch.tensor(1, dtype=torch.double).random_()&lt;/code&gt; will be uniform in &lt;code&gt;[0, 2^53]&lt;/code&gt;.</source>
          <target state="translated">用从 &lt;code&gt;[from, to - 1]&lt;/code&gt; 的离散均匀分布采样的数字填充 &lt;code&gt;self&lt;/code&gt; 张量。如果未指定，则这些值通常仅受 &lt;code&gt;self&lt;/code&gt; 张量的数据类型限制。但是，对于浮点类型，如果未指定，范围将为 &lt;code&gt;[0, 2^mantissa]&lt;/code&gt; 以确保每个值都是可表示的。例如， &lt;code&gt;torch.tensor(1, dtype=torch.double).random_()&lt;/code&gt; 在 &lt;code&gt;[0, 2^53]&lt;/code&gt; 中将是统一的。</target>
        </trans-unit>
        <trans-unit id="b7f10ef5f693feaeaa010e2aa027c2e16d3faf27" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with numbers samples from the log-normal distribution parameterized by the given mean</source>
          <target state="translated">用给定均值参数化的对数正态分布中的数字样本填充 &lt;code&gt;self&lt;/code&gt; 张量</target>
        </trans-unit>
        <trans-unit id="a537a70caec95c45887848e05dbf76327e57f2bf" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with the specified value.</source>
          <target state="translated">用指定值填充 &lt;code&gt;self&lt;/code&gt; 张量。</target>
        </trans-unit>
        <trans-unit id="84e59f1fa6c91fd2336c166329083335c07742c5" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with zeros.</source>
          <target state="translated">用零填充 &lt;code&gt;self&lt;/code&gt; 张量。</target>
        </trans-unit>
        <trans-unit id="c357bdf1d8cc36baaa85a26ceaf45fb123516806" translate="yes" xml:space="preserve">
          <source>Fills each location of &lt;code&gt;self&lt;/code&gt; with an independent sample from</source>
          <target state="translated">用来自以下位置的独立样本填充 &lt;code&gt;self&lt;/code&gt; 每个位置</target>
        </trans-unit>
        <trans-unit id="75c253cce2f7953023782f50f2d324ceb99f06ec" translate="yes" xml:space="preserve">
          <source>Fills elements of &lt;code&gt;self&lt;/code&gt; tensor with &lt;code&gt;value&lt;/code&gt; where &lt;code&gt;mask&lt;/code&gt; is True. The shape of &lt;code&gt;mask&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the shape of the underlying tensor.</source>
          <target state="translated">填充的元素 &lt;code&gt;self&lt;/code&gt; 张量 &lt;code&gt;value&lt;/code&gt; ，其中 &lt;code&gt;mask&lt;/code&gt; 是真。 &lt;code&gt;mask&lt;/code&gt; 的形状必须与基础张量的形状一起&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;广播&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="7f2c547e676c650b0294e152d6742725981b5e7b" translate="yes" xml:space="preserve">
          <source>Fills the 2-dimensional input &lt;code&gt;Tensor&lt;/code&gt; with the identity matrix. Preserves the identity of the inputs in &lt;code&gt;Linear&lt;/code&gt; layers, where as many inputs are preserved as possible.</source>
          <target state="translated">用单位矩阵填充二维输入 &lt;code&gt;Tensor&lt;/code&gt; 。保留 &lt;code&gt;Linear&lt;/code&gt; 层中输入的标识，在线性层中，将保留尽可能多的输入。</target>
        </trans-unit>
        <trans-unit id="5e10a9506b95f83d62c2104cb07cd55facde3766" translate="yes" xml:space="preserve">
          <source>Fills the 2D input &lt;code&gt;Tensor&lt;/code&gt; as a sparse matrix, where the non-zero elements will be drawn from the normal distribution</source>
          <target state="translated">将2D输入 &lt;code&gt;Tensor&lt;/code&gt; 填充为稀疏矩阵，其中非零元素将从正态分布中绘制</target>
        </trans-unit>
        <trans-unit id="5e66c6e2e841f8a8523fead84aa32e52990c1b4b" translate="yes" xml:space="preserve">
          <source>Fills the elements of the &lt;code&gt;self&lt;/code&gt; tensor with value &lt;code&gt;val&lt;/code&gt; by selecting the indices in the order given in &lt;code&gt;index&lt;/code&gt;.</source>
          <target state="translated">填充的元素 &lt;code&gt;self&lt;/code&gt; 同值张量 &lt;code&gt;val&lt;/code&gt; 通过选择在给定的顺序的索引 &lt;code&gt;index&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="3a21a74e5af8bd75efccfeb2dc749fe2483f779d" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with a (semi) orthogonal matrix, as described in &lt;code&gt;Exact solutions to the nonlinear dynamics of learning in deep linear neural networks&lt;/code&gt; - Saxe, A. et al. (2013). The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened.</source>
          <target state="translated">如（x）正交矩阵填充输入 &lt;code&gt;Tensor&lt;/code&gt; ，如 &lt;code&gt;Exact solutions to the nonlinear dynamics of learning in deep linear neural networks&lt;/code&gt; -Saxe，A.等。（2013）。输入张量必须至少具有2个维度，并且对于2个以上维度的张量，尾随尺寸将被展平。</target>
        </trans-unit>
        <trans-unit id="06242c8039ea176e183491ddfbaaf7c2d66c68e8" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with values according to the method described in &lt;code&gt;Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification&lt;/code&gt; - He, K. et al. (2015), using a normal distribution. The resulting tensor will have values sampled from</source>
          <target state="translated">根据 &lt;code&gt;Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification&lt;/code&gt; ，K.等人中描述的方法，用值填充输入 &lt;code&gt;Tensor&lt;/code&gt; 。（2015），使用正态分布。结果张量将具有从中采样的值</target>
        </trans-unit>
        <trans-unit id="342727420e0d4e4b77efe66611c6eb6db5be0acf" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with values according to the method described in &lt;code&gt;Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification&lt;/code&gt; - He, K. et al. (2015), using a uniform distribution. The resulting tensor will have values sampled from</source>
          <target state="translated">根据 &lt;code&gt;Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification&lt;/code&gt; ，K.等人中描述的方法，用值填充输入 &lt;code&gt;Tensor&lt;/code&gt; 。（2015），使用均匀分布。结果张量将具有从中采样的值</target>
        </trans-unit>
        <trans-unit id="71e3100123b6866d1997e6df9daeefd940ad80de" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with values according to the method described in &lt;code&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/code&gt; - Glorot, X. &amp;amp; Bengio, Y. (2010), using a normal distribution. The resulting tensor will have values sampled from</source>
          <target state="translated">根据正态分布，根据 &lt;code&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/code&gt; -Glorot，X.＆Bengio，Y.（2010）中描述的方法，用值填充输入 &lt;code&gt;Tensor&lt;/code&gt; 。结果张量将具有从中采样的值</target>
        </trans-unit>
        <trans-unit id="2313477ebbbc49bb7f4bf7d5c39c5ea0206267ac" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with values according to the method described in &lt;code&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/code&gt; - Glorot, X. &amp;amp; Bengio, Y. (2010), using a uniform distribution. The resulting tensor will have values sampled from</source>
          <target state="translated">根据 &lt;code&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/code&gt; -Glorot，X.＆Bengio，Y.（2010）中描述的方法，使用均匀分布，用值填充输入 &lt;code&gt;Tensor&lt;/code&gt; 。结果张量将具有从中采样的值</target>
        </trans-unit>
        <trans-unit id="afacae29047abc8bbea1f5a18cb9f7afddce8004" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with the scalar value &lt;code&gt;0&lt;/code&gt;.</source>
          <target state="translated">用标量值 &lt;code&gt;0&lt;/code&gt; 填充输入张量。</target>
        </trans-unit>
        <trans-unit id="0749fb4668d1e4b76f3a1ba9aa423e792bc38d58" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with the scalar value &lt;code&gt;1&lt;/code&gt;.</source>
          <target state="translated">用标量值 &lt;code&gt;1&lt;/code&gt; 填充输入张量。</target>
        </trans-unit>
        <trans-unit id="498455641766cf82c112342c938821f2f05dedf1" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with the value</source>
          <target state="translated">填充输入Tensor的值</target>
        </trans-unit>
        <trans-unit id="92513a6a0cb417782fe4aae045c4a1b66c923b2f" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with values drawn from the normal distribution</source>
          <target state="translated">用从正态分布中提取的值填充输入Tensor。</target>
        </trans-unit>
        <trans-unit id="6b3d75035cf3e14fc34e56b36e9d5379c6dd95d1" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with values drawn from the uniform distribution</source>
          <target state="translated">用从均匀分布中提取的值填充输入Tensor。</target>
        </trans-unit>
        <trans-unit id="53430dbd9b3ec1b5ed7bd9a57a201a7f2d1a60f2" translate="yes" xml:space="preserve">
          <source>Fills the tensor with numbers drawn from the Cauchy distribution:</source>
          <target state="translated">用从Cauchy分布中抽取的数字填充张量:</target>
        </trans-unit>
        <trans-unit id="c6632aebf9bc7adafe6c2b8a271e6b35dc1107ed" translate="yes" xml:space="preserve">
          <source>Fills the {3, 4, 5}-dimensional input &lt;code&gt;Tensor&lt;/code&gt; with the Dirac delta function. Preserves the identity of the inputs in &lt;code&gt;Convolutional&lt;/code&gt; layers, where as many input channels are preserved as possible. In case of groups&amp;gt;1, each group of channels preserves identity</source>
          <target state="translated">使用Dirac delta函数填充{3，4，5}维输入 &lt;code&gt;Tensor&lt;/code&gt; 。保留 &lt;code&gt;Convolutional&lt;/code&gt; 层中输入的标识，其中会保留尽可能多的输入通道。如果组&amp;gt; 1，则每组通道均保留身份</target>
        </trans-unit>
        <trans-unit id="695c574fe765006dd0a54d1a5cdae48d767984bd" translate="yes" xml:space="preserve">
          <source>Find the indices from the &lt;em&gt;innermost&lt;/em&gt; dimension of &lt;code&gt;sorted_sequence&lt;/code&gt; such that, if the corresponding values in &lt;code&gt;values&lt;/code&gt; were inserted before the indices, the order of the corresponding &lt;em&gt;innermost&lt;/em&gt; dimension within &lt;code&gt;sorted_sequence&lt;/code&gt; would be preserved.</source>
          <target state="translated">查找从指数&lt;em&gt;最里面&lt;/em&gt;的尺寸 &lt;code&gt;sorted_sequence&lt;/code&gt; 这样，如果在相应的值 &lt;code&gt;values&lt;/code&gt; 进行索引之前插入相应的顺序&lt;em&gt;最里面的&lt;/em&gt;内尺寸 &lt;code&gt;sorted_sequence&lt;/code&gt; 将被保留。</target>
        </trans-unit>
        <trans-unit id="bc20e9bbc443d3b0fd0b14b6ff920327eb09f1d2" translate="yes" xml:space="preserve">
          <source>Find the indices from the &lt;em&gt;innermost&lt;/em&gt; dimension of &lt;code&gt;sorted_sequence&lt;/code&gt; such that, if the corresponding values in &lt;code&gt;values&lt;/code&gt; were inserted before the indices, the order of the corresponding &lt;em&gt;innermost&lt;/em&gt; dimension within &lt;code&gt;sorted_sequence&lt;/code&gt; would be preserved. Return a new tensor with the same size as &lt;code&gt;values&lt;/code&gt;. If &lt;code&gt;right&lt;/code&gt; is False (default), then the left boundary of &lt;code&gt;sorted_sequence&lt;/code&gt; is closed. More formally, the returned index satisfies the following rules:</source>
          <target state="translated">查找从指数&lt;em&gt;最里面&lt;/em&gt;的尺寸 &lt;code&gt;sorted_sequence&lt;/code&gt; 这样，如果在相应的值 &lt;code&gt;values&lt;/code&gt; 进行索引之前插入相应的顺序&lt;em&gt;最里面的&lt;/em&gt;内尺寸 &lt;code&gt;sorted_sequence&lt;/code&gt; 将被保留。返回一个具有与 &lt;code&gt;values&lt;/code&gt; 相同大小的新张量。如果 &lt;code&gt;right&lt;/code&gt; 为False（默认值），则 &lt;code&gt;sorted_sequence&lt;/code&gt; 的左边界是封闭的。更正式地说，返回的索引满足以下规则：</target>
        </trans-unit>
        <trans-unit id="03215b19f0bffce926c83716ff9399ab447bdf55" translate="yes" xml:space="preserve">
          <source>Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods.</source>
          <target state="translated">利用无矩阵LOBPCG方法找到对称正定义广义特征值问题的k个最大(或最小)特征值和相应的特征向量。</target>
        </trans-unit>
        <trans-unit id="8cb7b9aabdd31afa308112ad36cdb297f62b4914" translate="yes" xml:space="preserve">
          <source>Fine grained control is possible with &lt;code&gt;qconfig&lt;/code&gt; and &lt;code&gt;mapping&lt;/code&gt; that act similarly to &lt;code&gt;quantize()&lt;/code&gt;. If &lt;code&gt;qconfig&lt;/code&gt; is provided, the &lt;code&gt;dtype&lt;/code&gt; argument is ignored.</source>
          <target state="translated">使用 &lt;code&gt;qconfig&lt;/code&gt; 和 &lt;code&gt;mapping&lt;/code&gt; 可以实现细粒度的控制，它们的作用类似于 &lt;code&gt;quantize()&lt;/code&gt; 。如果提供了 &lt;code&gt;qconfig&lt;/code&gt; ，则 &lt;code&gt;dtype&lt;/code&gt; 参数将被忽略。</target>
        </trans-unit>
        <trans-unit id="d6ca52cc281a8bbbaa3d9b53b49079e29eb878fa" translate="yes" xml:space="preserve">
          <source>First convert your model from GPU to CPU and then save it, like so:</source>
          <target state="translated">首先将你的模型从GPU转换到CPU,然后保存,像这样。</target>
        </trans-unit>
        <trans-unit id="ce44c84deade8daed8f8cfb3f091e34fe3a19e21" translate="yes" xml:space="preserve">
          <source>First it will prepare the model for calibration, then it calls &lt;code&gt;run_fn&lt;/code&gt; which will run the calibration step, after that we will convert the model to a quantized model.</source>
          <target state="translated">首先，它将准备要校准的模型，然后调用 &lt;code&gt;run_fn&lt;/code&gt; ，它将运行校准步骤，之后，我们将模型转换为量化模型。</target>
        </trans-unit>
        <trans-unit id="56b7d451e08bf1ccd9aa8267b497203df559b032" translate="yes" xml:space="preserve">
          <source>First, if you repeatedly perform an operation that can produce duplicate entries (e.g., &lt;a href=&quot;#torch.sparse.FloatTensor.add&quot;&gt;&lt;code&gt;torch.sparse.FloatTensor.add()&lt;/code&gt;&lt;/a&gt;), you should occasionally coalesce your sparse tensors to prevent them from growing too large.</source>
          <target state="translated">首先，如果您反复执行可能产生重复项的操作（例如&lt;a href=&quot;#torch.sparse.FloatTensor.add&quot;&gt; &lt;code&gt;torch.sparse.FloatTensor.add()&lt;/code&gt; &lt;/a&gt;），则有时应合并稀疏张量以防止它们变得太大。</target>
        </trans-unit>
        <trans-unit id="e3329b7e66ffa5f9cc2ce06f055a9b6ced15b8ee" translate="yes" xml:space="preserve">
          <source>FisherSnedecor</source>
          <target state="translated">FisherSnedecor</target>
        </trans-unit>
        <trans-unit id="0af14ddb20aabbe1bd98f28f2c2f744284ccedb3" translate="yes" xml:space="preserve">
          <source>Flatten</source>
          <target state="translated">Flatten</target>
        </trans-unit>
        <trans-unit id="557473442912b0bbbc1f4c6573c0fe9837b23ef2" translate="yes" xml:space="preserve">
          <source>Flattens &lt;code&gt;dims&lt;/code&gt; into a single dimension with name &lt;code&gt;out_dim&lt;/code&gt;.</source>
          <target state="translated">展平将 &lt;code&gt;dims&lt;/code&gt; 为一个名为 &lt;code&gt;out_dim&lt;/code&gt; 的单一维度。</target>
        </trans-unit>
        <trans-unit id="80a931c216b5d2a192745812d7ca953d11d59b70" translate="yes" xml:space="preserve">
          <source>Flattens a contiguous range of dims in a tensor.</source>
          <target state="translated">在张量中平坦化一个连续的dim范围。</target>
        </trans-unit>
        <trans-unit id="e2ed5b97e25777e5a578840b676a3b9e44b41cab" translate="yes" xml:space="preserve">
          <source>Flattens a contiguous range of dims into a tensor.</source>
          <target state="translated">将连续范围的dim平坦化为一个张量。</target>
        </trans-unit>
        <trans-unit id="bcfce6cdf44c8905f6faca75da235eedd5635aac" translate="yes" xml:space="preserve">
          <source>Flattens a contiguous range of dims into a tensor. For use with &lt;code&gt;Sequential&lt;/code&gt;.</source>
          <target state="translated">将连续范围的暗角展平为张量。与 &lt;code&gt;Sequential&lt;/code&gt; 一起使用。</target>
        </trans-unit>
        <trans-unit id="db0a60d5a36fb0f5467808a539ac8c0b01a8dadd" translate="yes" xml:space="preserve">
          <source>Flip array in the left/right direction, returning a new tensor.</source>
          <target state="translated">在左右方向翻转数组,返回一个新的张量。</target>
        </trans-unit>
        <trans-unit id="e9abeaf079a0d69d0b2ee4704e055dbacab017f3" translate="yes" xml:space="preserve">
          <source>Flip array in the up/down direction, returning a new tensor.</source>
          <target state="translated">在上下方向翻转数组,返回一个新的张量。</target>
        </trans-unit>
        <trans-unit id="d6b601389a5cfb7f115ef7332083ae3431e4e4e3" translate="yes" xml:space="preserve">
          <source>Flip the entries in each column in the up/down direction. Rows are preserved, but appear in a different order than before.</source>
          <target state="translated">按上/下方向翻转每列中的条目。行被保留,但显示顺序与之前不同。</target>
        </trans-unit>
        <trans-unit id="f14bb6de935d5a8cea7f1dc6e4323a38f4e11462" translate="yes" xml:space="preserve">
          <source>Flip the entries in each row in the left/right direction. Columns are preserved, but appear in a different order than before.</source>
          <target state="translated">将每行的条目向左/右方向翻转。列被保留,但出现的顺序与之前不同。</target>
        </trans-unit>
        <trans-unit id="23c9f78a2bf71d761dc5a430092962a70ec045f2" translate="yes" xml:space="preserve">
          <source>FloatFunctional</source>
          <target state="translated">FloatFunctional</target>
        </trans-unit>
        <trans-unit id="a2d156b66635ae5fe54c5766d1380c6080564834" translate="yes" xml:space="preserve">
          <source>Floating-point Tensors produced in an autocast-enabled region may be &lt;code&gt;float16&lt;/code&gt;. After returning to an autocast-disabled region, using them with floating-point Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s) produced in the autocast region back to &lt;code&gt;float32&lt;/code&gt; (or other dtype if desired). If a Tensor from the autocast region is already &lt;code&gt;float32&lt;/code&gt;, the cast is a no-op, and incurs no additional overhead. Example:</source>
          <target state="translated">在启用自动 &lt;code&gt;float16&lt;/code&gt; 区域中产生的浮点张量可以是float16。返回禁用自动广播的区域后，将它们与不同dtype的浮点张量一起使用可能会导致类型不匹配错误。如果是这样，则将自动广播区域中生成的张量转换回 &lt;code&gt;float32&lt;/code&gt; （或根据需要使用其他dtype ）。如果来自自动广播区域的张量已经是 &lt;code&gt;float32&lt;/code&gt; ，则强制转换为空操作，并且不会产生任何额外的开销。例子：</target>
        </trans-unit>
        <trans-unit id="0352ecfbeaceff6ef8214a3af42e602c1d2f0c6a" translate="yes" xml:space="preserve">
          <source>Flushes the event file to disk. Call this method to make sure that all pending events have been written to disk.</source>
          <target state="translated">将事件文件刷新到磁盘。调用此方法确保所有待处理的事件都已写入磁盘。</target>
        </trans-unit>
        <trans-unit id="b6ba0db1f814179114ec82fbf188b2eb5be2596e" translate="yes" xml:space="preserve">
          <source>Fold</source>
          <target state="translated">Fold</target>
        </trans-unit>
        <trans-unit id="ab2123970899470af7c3bcdbf9832cd5ea8344b1" translate="yes" xml:space="preserve">
          <source>Following this tutorial &lt;a href=&quot;https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html&quot;&gt;Extending TorchScript with Custom C++ Operators&lt;/a&gt;, you can create and register your own custom ops implementation in PyTorch. Here&amp;rsquo;s how to export such model to ONNX.:</source>
          <target state="translated">遵循本教程&amp;ldquo;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html&quot;&gt;使用自定义C ++运算符扩展TorchScript&amp;rdquo;&lt;/a&gt;之后，您可以在PyTorch中创建并注册自己的自定义ops实现。以下是将此类模型导出到ONNX的方法：</target>
        </trans-unit>
        <trans-unit id="662ac04d8757de24ad35f426f956e3ec58188dcf" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;#iterable-style-datasets&quot;&gt;iterable-style datasets&lt;/a&gt;, data loading order is entirely controlled by the user-defined iterable. This allows easier implementations of chunk-reading and dynamic batch size (e.g., by yielding a batched sample at each time).</source>
          <target state="translated">对于&lt;a href=&quot;#iterable-style-datasets&quot;&gt;可迭代样式的数据集&lt;/a&gt;，数据加载顺序完全由用户定义的可迭代样式控制。这样可以更轻松地实现块读取和动态批处理大小（例如，通过每次生成一个批处理的样本）。</target>
        </trans-unit>
        <trans-unit id="c762389bca1bb2b05603a57f1353b6b58529cac2" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; objects returned by &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;future.wait()&lt;/code&gt; should not be called after &lt;code&gt;shutdown()&lt;/code&gt;.</source>
          <target state="translated">对于&lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt; &lt;code&gt;rpc_async()&lt;/code&gt; &lt;/a&gt;返回的&lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;对象，在 &lt;code&gt;shutdown()&lt;/code&gt; 之后不应调用 &lt;code&gt;future.wait()&lt;/code&gt; （）。</target>
        </trans-unit>
        <trans-unit id="068880daf2a8a0b7a7463ef0178819889346c85b" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;N&lt;/code&gt;-dimensional padding, use &lt;a href=&quot;../nn.functional#torch.nn.functional.pad&quot;&gt;&lt;code&gt;torch.nn.functional.pad()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">对于 &lt;code&gt;N&lt;/code&gt; 维填充，请使用&lt;a href=&quot;../nn.functional#torch.nn.functional.pad&quot;&gt; &lt;code&gt;torch.nn.functional.pad()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0224a530bfe0946c7afd8a4140361f1cd0dcd86b" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;torch.nn.functional&lt;/code&gt; operators, we support the following:</source>
          <target state="translated">对于 &lt;code&gt;torch.nn.functional&lt;/code&gt; 运算符，我们支持以下内容：</target>
        </trans-unit>
        <trans-unit id="e1e6b2fc17a48e201a5b7dbea3ac88cd7be57af5" translate="yes" xml:space="preserve">
          <source>For CPU tensors, this method is currently only available with MKL. Use &lt;a href=&quot;../backends#torch.backends.mkl.is_available&quot;&gt;&lt;code&gt;torch.backends.mkl.is_available()&lt;/code&gt;&lt;/a&gt; to check if MKL is installed.</source>
          <target state="translated">对于CPU张量，此方法当前仅适用于MKL。使用&lt;a href=&quot;../backends#torch.backends.mkl.is_available&quot;&gt; &lt;code&gt;torch.backends.mkl.is_available()&lt;/code&gt; &lt;/a&gt;检查是否已安装MKL。</target>
        </trans-unit>
        <trans-unit id="5a9dc733d94431c6a4d80dbab50dc02413b91172" translate="yes" xml:space="preserve">
          <source>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same configuration. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cufft-plan-cache&quot;&gt;cuFFT plan cache&lt;/a&gt; for more details on how to monitor and control the cache.</source>
          <target state="translated">对于CUDA张量，LRU缓存用于cuFFT计划，以加快在具有相同配置的相同几何形状的张量上重复运行FFT方法的速度。有关如何监视和控制&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cufft-plan-cache&quot;&gt;缓存&lt;/a&gt;的更多详细信息，请参见cuFFT计划缓存。</target>
        </trans-unit>
        <trans-unit id="c3f7242e16cf3e0469ec63da054b257bd101a205" translate="yes" xml:space="preserve">
          <source>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. For CPU tensors, an error is thrown.</source>
          <target state="translated">对于CUDA tensors,该函数返回张量所在GPU的设备序号。对于CPU张量,会产生一个错误。</target>
        </trans-unit>
        <trans-unit id="0338c8903416c8af92041cbd33c12e566bca8ea7" translate="yes" xml:space="preserve">
          <source>For Tensors that have &lt;a href=&quot;#torch.Tensor.requires_grad&quot;&gt;&lt;code&gt;requires_grad&lt;/code&gt;&lt;/a&gt; which is &lt;code&gt;True&lt;/code&gt;, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so &lt;code&gt;grad_fn&lt;/code&gt; is None.</source>
          <target state="translated">对于有张量&lt;a href=&quot;#torch.Tensor.requires_grad&quot;&gt; &lt;code&gt;requires_grad&lt;/code&gt; &lt;/a&gt;这是 &lt;code&gt;True&lt;/code&gt; ，他们将叶张量，如果他们是由用户创建的。这意味着它们不是运算的结果，因此 &lt;code&gt;grad_fn&lt;/code&gt; 为&amp;ldquo;无&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="ce3ce048e3708a40dbc3057fc7dbd0788b47316f" translate="yes" xml:space="preserve">
          <source>For Tensors that have &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt;&lt;code&gt;requires_grad&lt;/code&gt;&lt;/a&gt; which is &lt;code&gt;True&lt;/code&gt;, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so &lt;code&gt;grad_fn&lt;/code&gt; is None.</source>
          <target state="translated">对于有张量&lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt; &lt;code&gt;requires_grad&lt;/code&gt; &lt;/a&gt;这是 &lt;code&gt;True&lt;/code&gt; ，他们将叶张量，如果他们是由用户创建的。这意味着它们不是运算的结果，因此 &lt;code&gt;grad_fn&lt;/code&gt; 为&amp;ldquo;无&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="8d2d3af0bd6ef80615c67c0f72bb823e693063c0" translate="yes" xml:space="preserve">
          <source>For a 3-D tensor the output is specified by:</source>
          <target state="translated">对于3-D张量来说,输出由以下方式指定。</target>
        </trans-unit>
        <trans-unit id="3f570fa1da407a7894b9ad9cdc01e54242454c59" translate="yes" xml:space="preserve">
          <source>For a 3-D tensor, &lt;code&gt;self&lt;/code&gt; is updated as:</source>
          <target state="translated">对于3-D张量， &lt;code&gt;self&lt;/code&gt; 更新为：</target>
        </trans-unit>
        <trans-unit id="dd4d8e8f7d7355e68f887ffe9f112d1d44ab2731" translate="yes" xml:space="preserve">
          <source>For a comprehensive list of name inference rules, see &lt;a href=&quot;name_inference#name-inference-reference-doc&quot;&gt;Named Tensors operator coverage&lt;/a&gt;. Here are two common operations that may be useful to go over:</source>
          <target state="translated">有关名称推断规则的完整列表，请参见&lt;a href=&quot;name_inference#name-inference-reference-doc&quot;&gt;命名张量运算符coverage&lt;/a&gt;。以下是两个可能有用的常见操作：</target>
        </trans-unit>
        <trans-unit id="f91f8919582617014f59317b7352c3f1154712ab" translate="yes" xml:space="preserve">
          <source>For a full listing of supported Python features, see &lt;a href=&quot;jit_python_reference#python-language-reference&quot;&gt;Python Language Reference Coverage&lt;/a&gt;.</source>
          <target state="translated">有关受支持的Python功能的完整列表，请参见&lt;a href=&quot;jit_python_reference#python-language-reference&quot;&gt;Python语言参考范围&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4cd19a97e1a392f82662e959d703403f2351a98b" translate="yes" xml:space="preserve">
          <source>For a gentle introduction to TorchScript, see the &lt;a href=&quot;https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html&quot;&gt;Introduction to TorchScript&lt;/a&gt; tutorial.</source>
          <target state="translated">有关TorchScript的简要介绍，请参阅《&lt;a href=&quot;https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html&quot;&gt;TorchScript简介》&lt;/a&gt;教程。</target>
        </trans-unit>
        <trans-unit id="8e0e8c59f7042dfe9b36d63277e5e7440cb658b4" translate="yes" xml:space="preserve">
          <source>For a tensor &lt;code&gt;input&lt;/code&gt; of sizes</source>
          <target state="translated">对于大小的张量 &lt;code&gt;input&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="7afccb9a65464232e6685319e5c664295be01466" translate="yes" xml:space="preserve">
          <source>For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see the &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_export.html&quot;&gt;Loading a PyTorch Model in C++&lt;/a&gt; tutorial.</source>
          <target state="translated">有关将PyTorch模型转换为TorchScript并在C ++中运行的端到端示例，请参阅在C ++中&lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_export.html&quot;&gt;加载PyTorch模型&lt;/a&gt;教程。</target>
        </trans-unit>
        <trans-unit id="c08a3397ebd78b324e2b03c935d901274f6914be" translate="yes" xml:space="preserve">
          <source>For bags of constant length and no &lt;code&gt;per_sample_weights&lt;/code&gt;, this class</source>
          <target state="translated">对于长度固定且没有 &lt;code&gt;per_sample_weights&lt;/code&gt; 的袋子，此类</target>
        </trans-unit>
        <trans-unit id="243fa0279fa6c67f52078764a1246089d77121bd" translate="yes" xml:space="preserve">
          <source>For complex functions, no notion of Jacobian exists. Gradcheck verifies if the numerical and analytical values of Wirtinger and Conjugate Wirtinger derivative are consistent. The gradient computation is done under the assumption that the overall function has a real valued output. For functions with complex output, gradcheck compares the numerical and analytical gradients for two values of &lt;code&gt;grad_output&lt;/code&gt;: 1 and 1j. For more details, check out &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/autograd.html#complex-autograd-doc&quot;&gt;Autograd for Complex Numbers&lt;/a&gt;.</source>
          <target state="translated">对于复杂函数，不存在雅可比行列式的概念。Gradcheck验证Wirtinger和共轭Wirtinger导数的数值和分析值是否一致。梯度计算是在整个函数具有实际值输出的假设下完成的。对于具有复杂的输出功能，gradcheck比较了的两个值的数字分析梯度 &lt;code&gt;grad_output&lt;/code&gt; ：1和1J。有关更多详细信息，请查看&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/autograd.html#complex-autograd-doc&quot;&gt;Autograd for Complex Numbers&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6f725f3358d3c963d673d4ca7969a99e89c8c101" translate="yes" xml:space="preserve">
          <source>For data loading, passing &lt;code&gt;pin_memory=True&lt;/code&gt; to a &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; will automatically put the fetched data Tensors in pinned memory, and thus enables faster data transfer to CUDA-enabled GPUs.</source>
          <target state="translated">对于数据加载，将 &lt;code&gt;pin_memory=True&lt;/code&gt; 传递给&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt;将自动将获取的数据张量放入固定内存中，从而能够更快地将数据传输到支持CUDA的GPU。</target>
        </trans-unit>
        <trans-unit id="ce5d81031fb4d7e97b33ce443ec1d33497cec1cd" translate="yes" xml:space="preserve">
          <source>For details on input arguments, parameters, and implementation see &lt;a href=&quot;generated/torch.nn.conv1d#torch.nn.Conv1d&quot;&gt;&lt;code&gt;Conv1d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">有关输入参数，参数和实现的详细信息，请参见&lt;a href=&quot;generated/torch.nn.conv1d#torch.nn.Conv1d&quot;&gt; &lt;code&gt;Conv1d&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="36b1210fa3359dec217e0f8625012720c3f10bc6" translate="yes" xml:space="preserve">
          <source>For details on input arguments, parameters, and implementation see &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">有关输入参数，参数和实现的详细信息，请参见&lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;Conv2d&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="17fc1bf0dec370ea37946985facbba42ffd22e0f" translate="yes" xml:space="preserve">
          <source>For details on input arguments, parameters, and implementation see &lt;a href=&quot;generated/torch.nn.conv3d#torch.nn.Conv3d&quot;&gt;&lt;code&gt;Conv3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">有关输入参数，参数和实现的详细信息，请参见&lt;a href=&quot;generated/torch.nn.conv3d#torch.nn.Conv3d&quot;&gt; &lt;code&gt;Conv3d&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="88fa6dcc55dbef1b20fcb850f35a6375f7b3938c" translate="yes" xml:space="preserve">
          <source>For each element in the input sequence, each layer computes the following function:</source>
          <target state="translated">对于输入序列中的每一个元素,每一层都要计算以下函数。</target>
        </trans-unit>
        <trans-unit id="d139363266e05a6b552e5defd18e04652bf83d44" translate="yes" xml:space="preserve">
          <source>For each mini-batch sample, the loss in terms of the 1D input</source>
          <target state="translated">对于每一个小批量的样品,在1D输入方面的损失。</target>
        </trans-unit>
        <trans-unit id="431645996fbd3fae9490db55c077050efdd9469b" translate="yes" xml:space="preserve">
          <source>For each output location &lt;code&gt;output[n, :, h, w]&lt;/code&gt;, the size-2 vector &lt;code&gt;grid[n, h, w]&lt;/code&gt; specifies &lt;code&gt;input&lt;/code&gt; pixel locations &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, which are used to interpolate the output value &lt;code&gt;output[n, :, h, w]&lt;/code&gt;. In the case of 5D inputs, &lt;code&gt;grid[n, d, h, w]&lt;/code&gt; specifies the &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, &lt;code&gt;z&lt;/code&gt; pixel locations for interpolating &lt;code&gt;output[n, :, d, h, w]&lt;/code&gt;. &lt;code&gt;mode&lt;/code&gt; argument specifies &lt;code&gt;nearest&lt;/code&gt; or &lt;code&gt;bilinear&lt;/code&gt; interpolation method to sample the input pixels.</source>
          <target state="translated">对于每个输出位置 &lt;code&gt;output[n, :, h, w]&lt;/code&gt; ，大小为2的矢量 &lt;code&gt;grid[n, h, w]&lt;/code&gt; 指定 &lt;code&gt;input&lt;/code&gt; 像素位置 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; ，用于对输出值 &lt;code&gt;output[n, :, h, w]&lt;/code&gt; 进行插值h，w]。在5D输入的情况下， &lt;code&gt;grid[n, d, h, w]&lt;/code&gt; 指定用于插值 &lt;code&gt;output[n, :, d, h, w]&lt;/code&gt; 的 &lt;code&gt;x&lt;/code&gt; ， &lt;code&gt;y&lt;/code&gt; 和 &lt;code&gt;z&lt;/code&gt; 像素位置。 &lt;code&gt;mode&lt;/code&gt; 参数指定 &lt;code&gt;nearest&lt;/code&gt; 或 &lt;code&gt;bilinear&lt;/code&gt; 插值方法以对输入像素进行采样。</target>
        </trans-unit>
        <trans-unit id="a707b704ee783d908df45582d74fb5eed5208cc2" translate="yes" xml:space="preserve">
          <source>For example, assigning to &lt;code&gt;self&lt;/code&gt; outside of the &lt;code&gt;__init__()&lt;/code&gt; method:</source>
          <target state="translated">例如，在 &lt;code&gt;__init__()&lt;/code&gt; 方法之外分配 &lt;code&gt;self&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="598cd3a9f173b17b51c024dfc8ec80175fdbbd3c" translate="yes" xml:space="preserve">
          <source>For example, if &lt;code&gt;input&lt;/code&gt; is a vector of size N, the result will also be a vector of size N, with elements.</source>
          <target state="translated">例如，如果 &lt;code&gt;input&lt;/code&gt; 是大小为N的向量，则结果也将是大小为N的向量（带有元素）。</target>
        </trans-unit>
        <trans-unit id="c5eb0f2bf9e3391a2f0f43ac5e99b77a4de61cf1" translate="yes" xml:space="preserve">
          <source>For example, if &lt;code&gt;input&lt;/code&gt; is of shape:</source>
          <target state="translated">例如，如果 &lt;code&gt;input&lt;/code&gt; 是形状：</target>
        </trans-unit>
        <trans-unit id="cb7354d4a324678f0e4b20e916940e161856f119" translate="yes" xml:space="preserve">
          <source>For example, if a dataset contains 100 positive and 300 negative examples of a single class, then &lt;code&gt;pos_weight&lt;/code&gt; for the class should be equal to</source>
          <target state="translated">例如，如果数据集包含单个类的100个正例和300个负例，则 &lt;code&gt;pos_weight&lt;/code&gt; 应等于</target>
        </trans-unit>
        <trans-unit id="ec89e688ce49781ad9518cc67374c388cc79a9c9" translate="yes" xml:space="preserve">
          <source>For example, if the system we use for distributed training has 2 nodes, each of which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would like to all-reduce. The following code can serve as a reference:</source>
          <target state="translated">例如,如果我们用于分布式训练的系统有2个节点,每个节点有8个GPU。在16个GPU中的每一个GPU上,都有一个我们想要全部还原的张量。下面的代码可以作为参考。</target>
        </trans-unit>
        <trans-unit id="b2aafe679a24a7e81a27fcc1b208b172dea562d1" translate="yes" xml:space="preserve">
          <source>For example, such a dataset, when accessed with &lt;code&gt;dataset[idx]&lt;/code&gt;, could read the &lt;code&gt;idx&lt;/code&gt;-th image and its corresponding label from a folder on the disk.</source>
          <target state="translated">例如，当使用 &lt;code&gt;dataset[idx]&lt;/code&gt; 访问时，这样的数据集可以从磁盘上的文件夹中读取第 &lt;code&gt;idx&lt;/code&gt; 张图像及其对应的标签。</target>
        </trans-unit>
        <trans-unit id="734a9ddb49bc44a14f3817f8ab3e399194fe7014" translate="yes" xml:space="preserve">
          <source>For example, such a dataset, when called &lt;code&gt;iter(dataset)&lt;/code&gt;, could return a stream of data reading from a database, a remote server, or even logs generated in real time.</source>
          <target state="translated">例如，这样的数据集，当称为 &lt;code&gt;iter(dataset)&lt;/code&gt; 时，可以返回从数据库，远程服务器甚至实时生成的日志中读取的数据流。</target>
        </trans-unit>
        <trans-unit id="b076a6f19f5c3f95d3197664aa2092f90ac3e4fd" translate="yes" xml:space="preserve">
          <source>For example, suppose that we wanted to implement an operator by operating directly on &lt;a href=&quot;#torch.sparse.FloatTensor._values&quot;&gt;&lt;code&gt;torch.sparse.FloatTensor._values()&lt;/code&gt;&lt;/a&gt;. Multiplication by a scalar can be implemented in the obvious way, as multiplication distributes over addition; however, square root cannot be implemented directly, since &lt;code&gt;sqrt(a + b) != sqrt(a) +
sqrt(b)&lt;/code&gt; (which is what would be computed if you were given an uncoalesced tensor.)</source>
          <target state="translated">例如，假设我们想通过直接在&lt;a href=&quot;#torch.sparse.FloatTensor._values&quot;&gt; &lt;code&gt;torch.sparse.FloatTensor._values()&lt;/code&gt; &lt;/a&gt;上进行操作来实现一个运算符。标量乘法可以很明显地实现，因为乘法分布在加法上。但是，平方根不能直接实现，因为 &lt;code&gt;sqrt(a + b) != sqrt(a) + sqrt(b)&lt;/code&gt; （如果给定非张量的张量，将计算出平方根）。</target>
        </trans-unit>
        <trans-unit id="287e119f8bc584b5b1a94e98cf1131a5d81ded2a" translate="yes" xml:space="preserve">
          <source>For example, this is very useful when one wants to specify per-layer learning rates:</source>
          <target state="translated">例如,当人们想指定每层的学习率时,这就非常有用。</target>
        </trans-unit>
        <trans-unit id="b3dee60b13728b6faa87ab6d08e758ac130cfdfa" translate="yes" xml:space="preserve">
          <source>For inputs of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, &lt;code&gt;value&lt;/code&gt; must be a real number, otherwise an integer.</source>
          <target state="translated">对于 &lt;code&gt;FloatTensor&lt;/code&gt; 或 &lt;code&gt;DoubleTensor&lt;/code&gt; 类型的输入， &lt;code&gt;value&lt;/code&gt; 必须为实数，否则为整数。</target>
        </trans-unit>
        <trans-unit id="a79d59fd72d10daf74904a50c1c53f2a33467eeb" translate="yes" xml:space="preserve">
          <source>For inputs of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, arguments &lt;code&gt;beta&lt;/code&gt; and &lt;code&gt;alpha&lt;/code&gt; must be real numbers, otherwise they should be integers</source>
          <target state="translated">对于 &lt;code&gt;FloatTensor&lt;/code&gt; 或 &lt;code&gt;DoubleTensor&lt;/code&gt; 类型的输入，参数 &lt;code&gt;beta&lt;/code&gt; 和 &lt;code&gt;alpha&lt;/code&gt; 必须为实数，否则它们应为整数</target>
        </trans-unit>
        <trans-unit id="c25cdbf09c7b02888e46dd28ab2af0cd145a33a9" translate="yes" xml:space="preserve">
          <source>For inputs of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, arguments &lt;code&gt;beta&lt;/code&gt; and &lt;code&gt;alpha&lt;/code&gt; must be real numbers, otherwise they should be integers.</source>
          <target state="translated">对于 &lt;code&gt;FloatTensor&lt;/code&gt; 或 &lt;code&gt;DoubleTensor&lt;/code&gt; 类型的输入，参数 &lt;code&gt;beta&lt;/code&gt; 和 &lt;code&gt;alpha&lt;/code&gt; 必须为实数，否则应为整数。</target>
        </trans-unit>
        <trans-unit id="f2524577d6ea6dbe6d56d704e00d4f3e91229b19" translate="yes" xml:space="preserve">
          <source>For instance, if each data sample consists of a 3-channel image and an integral class label, i.e., each element of the dataset returns a tuple &lt;code&gt;(image, class_index)&lt;/code&gt;, the default &lt;code&gt;collate_fn&lt;/code&gt; collates a list of such tuples into a single tuple of a batched image tensor and a batched class label Tensor. In particular, the default &lt;code&gt;collate_fn&lt;/code&gt; has the following properties:</source>
          <target state="translated">例如，如果每个数据样本都由一个3通道图像和一个整体类标签组成，即数据集的每个元素都返回一个元组 &lt;code&gt;(image, class_index)&lt;/code&gt; ，则默认的 &lt;code&gt;collate_fn&lt;/code&gt; 将此类元组的列表整理为一个元组批处理图像张量和批处理类标签Tensor。特别是，默认的 &lt;code&gt;collate_fn&lt;/code&gt; 具有以下属性：</target>
        </trans-unit>
        <trans-unit id="abc897209b2f98b7966665fa36a5eddbbc44f66d" translate="yes" xml:space="preserve">
          <source>For instance:</source>
          <target state="translated">例如:</target>
        </trans-unit>
        <trans-unit id="dc23cb2e2a00ac46f8d5395098475d4070a042d1" translate="yes" xml:space="preserve">
          <source>For iterable-style datasets, since each worker process gets a replica of the &lt;code&gt;dataset&lt;/code&gt; object, naive multi-process loading will often result in duplicated data. Using &lt;a href=&quot;#torch.utils.data.get_worker_info&quot;&gt;&lt;code&gt;torch.utils.data.get_worker_info()&lt;/code&gt;&lt;/a&gt; and/or &lt;code&gt;worker_init_fn&lt;/code&gt;, users may configure each replica independently. (See &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt;&lt;code&gt;IterableDataset&lt;/code&gt;&lt;/a&gt; documentations for how to achieve this. ) For similar reasons, in multi-process loading, the &lt;code&gt;drop_last&lt;/code&gt; argument drops the last non-full batch of each worker&amp;rsquo;s iterable-style dataset replica.</source>
          <target state="translated">对于可迭代样式的数据集，由于每个工作进程都获得了 &lt;code&gt;dataset&lt;/code&gt; 对象的副本，因此幼稚的多进程加载通常会导致数据重复。使用&lt;a href=&quot;#torch.utils.data.get_worker_info&quot;&gt; &lt;code&gt;torch.utils.data.get_worker_info()&lt;/code&gt; &lt;/a&gt;和/或 &lt;code&gt;worker_init_fn&lt;/code&gt; ，用户可以独立配置每个副本。（有关如何实现此操作，请参阅&lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt; &lt;code&gt;IterableDataset&lt;/code&gt; &lt;/a&gt;文档。）出于类似的原因，在多进程加载中， &lt;code&gt;drop_last&lt;/code&gt; 参数删除每个工作程序的可迭代样式数据集副本的最后一个非完整批次。</target>
        </trans-unit>
        <trans-unit id="22c3fe0b8bd316af1ba2f91851ec2f8f7995ee83" translate="yes" xml:space="preserve">
          <source>For legacy reasons, a device can be constructed via a single device ordinal, which is treated as a cuda device. This matches &lt;a href=&quot;tensors#torch.Tensor.get_device&quot;&gt;&lt;code&gt;Tensor.get_device()&lt;/code&gt;&lt;/a&gt;, which returns an ordinal for cuda tensors and is not supported for cpu tensors.</source>
          <target state="translated">出于遗留原因，可以通过单个设备序号（被视为cuda设备）构造设备。这与&lt;a href=&quot;tensors#torch.Tensor.get_device&quot;&gt; &lt;code&gt;Tensor.get_device()&lt;/code&gt; &lt;/a&gt;匹配，后者为cuda张量返回序数，而cpu张量不支持此序数。</target>
        </trans-unit>
        <trans-unit id="0aa3a8773fa895e1e6152468962406770d9b0977" translate="yes" xml:space="preserve">
          <source>For loops over constant nn.ModuleList</source>
          <target state="translated">对常数nn.ModuleList进行循环。</target>
        </trans-unit>
        <trans-unit id="d0ca847043fc995de69359f3596ce96734b521c7" translate="yes" xml:space="preserve">
          <source>For loops over tuples</source>
          <target state="translated">在元组上进行循环</target>
        </trans-unit>
        <trans-unit id="5aa0148bf5ef53de46ea15471ea81ef66c0693f4" translate="yes" xml:space="preserve">
          <source>For loops with range</source>
          <target state="translated">对于有范围的循环</target>
        </trans-unit>
        <trans-unit id="c492beb9d1c851a0661d50033a687a1b1aab0e7a" translate="yes" xml:space="preserve">
          <source>For map-style datasets, the main process generates the indices using &lt;code&gt;sampler&lt;/code&gt; and sends them to the workers. So any shuffle randomization is done in the main process which guides loading by assigning indices to load.</source>
          <target state="translated">对于地图样式的数据集，主要过程使用 &lt;code&gt;sampler&lt;/code&gt; 生成索引并将其发送给工作人员。因此，任何随机播放都是在主过程中完成的，该过程通过为索引分配索引来引导加载。</target>
        </trans-unit>
        <trans-unit id="0d65cf1ed6eabecd00c26213c93e0d17f7fcdf45" translate="yes" xml:space="preserve">
          <source>For more complicated uses of the profilers (like in a multi-GPU case), please see &lt;a href=&quot;https://docs.python.org/3/library/profile.html&quot;&gt;https://docs.python.org/3/library/profile.html&lt;/a&gt; or &lt;a href=&quot;autograd#torch.autograd.profiler.profile&quot;&gt;&lt;code&gt;torch.autograd.profiler.profile()&lt;/code&gt;&lt;/a&gt; for more information.</source>
          <target state="translated">有关探查器的更复杂用法（例如在多GPU情况下），请参阅&lt;a href=&quot;https://docs.python.org/3/library/profile.html&quot;&gt;https://docs.python.org/3/library/profile.html&lt;/a&gt;或&lt;a href=&quot;autograd#torch.autograd.profiler.profile&quot;&gt; &lt;code&gt;torch.autograd.profiler.profile()&lt;/code&gt; &lt;/a&gt;了解更多信息。</target>
        </trans-unit>
        <trans-unit id="676185286638fbcbfe90f50f9df7e7cdf2648359" translate="yes" xml:space="preserve">
          <source>For more examples, please look at the implementations of &lt;a href=&quot;#torch.distributions.gumbel.Gumbel&quot;&gt;&lt;code&gt;Gumbel&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributions.half_cauchy.HalfCauchy&quot;&gt;&lt;code&gt;HalfCauchy&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributions.half_normal.HalfNormal&quot;&gt;&lt;code&gt;HalfNormal&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributions.log_normal.LogNormal&quot;&gt;&lt;code&gt;LogNormal&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributions.pareto.Pareto&quot;&gt;&lt;code&gt;Pareto&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributions.weibull.Weibull&quot;&gt;&lt;code&gt;Weibull&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributions.relaxed_bernoulli.RelaxedBernoulli&quot;&gt;&lt;code&gt;RelaxedBernoulli&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical&quot;&gt;&lt;code&gt;RelaxedOneHotCategorical&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">有关更多示例，请查看&lt;a href=&quot;#torch.distributions.gumbel.Gumbel&quot;&gt; &lt;code&gt;Gumbel&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;#torch.distributions.half_cauchy.HalfCauchy&quot;&gt; &lt;code&gt;HalfCauchy&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;#torch.distributions.half_normal.HalfNormal&quot;&gt; &lt;code&gt;HalfNormal&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;#torch.distributions.log_normal.LogNormal&quot;&gt; &lt;code&gt;LogNormal&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;#torch.distributions.pareto.Pareto&quot;&gt; &lt;code&gt;Pareto&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;#torch.distributions.weibull.Weibull&quot;&gt; &lt;code&gt;Weibull&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;#torch.distributions.relaxed_bernoulli.RelaxedBernoulli&quot;&gt; &lt;code&gt;RelaxedBernoulli&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical&quot;&gt; &lt;code&gt;RelaxedOneHotCategorical&lt;/code&gt; 的实现。&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7b8781b8ea8a1a7fd16e315cb8caa2f0d8fd81ce" translate="yes" xml:space="preserve">
          <source>For more information on &lt;code&gt;torch.sparse_coo&lt;/code&gt; tensors, see &lt;a href=&quot;sparse#sparse-docs&quot;&gt;torch.sparse&lt;/a&gt;.</source>
          <target state="translated">有关 &lt;code&gt;torch.sparse_coo&lt;/code&gt; 张量的更多信息，请参见&lt;a href=&quot;sparse#sparse-docs&quot;&gt;torch.sparse&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="5a0ad772d1091f121cc7efb6b451798990b8911a" translate="yes" xml:space="preserve">
          <source>For more information on tensor views, see &lt;a href=&quot;tensor_view#tensor-view-doc&quot;&gt;Tensor Views&lt;/a&gt;.</source>
          <target state="translated">有关张量视图的更多信息，请参见&lt;a href=&quot;tensor_view#tensor-view-doc&quot;&gt;张量视图&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="1f3909dab7a0a74dfec923801d50d6afccbbd76f" translate="yes" xml:space="preserve">
          <source>For more information on the &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;tensor_attributes#torch.torch.layout&quot;&gt;&lt;code&gt;torch.layout&lt;/code&gt;&lt;/a&gt; attributes of a &lt;a href=&quot;#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt;, see &lt;a href=&quot;tensor_attributes#tensor-attributes-doc&quot;&gt;Tensor Attributes&lt;/a&gt;.</source>
          <target state="translated">有关更多信息&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;tensor_attributes#torch.torch.layout&quot;&gt; &lt;code&gt;torch.layout&lt;/code&gt; &lt;/a&gt;一个属性&lt;a href=&quot;#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;，见&lt;a href=&quot;tensor_attributes#tensor-attributes-doc&quot;&gt;张量属性&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e6568b8e5f7166a4f6114617787be1538e34997e" translate="yes" xml:space="preserve">
          <source>For now, normalization code can be found in &lt;code&gt;references/video_classification/transforms.py&lt;/code&gt;, see the &lt;code&gt;Normalize&lt;/code&gt; function there. Note that it differs from standard normalization for images because it assumes the video is 4d.</source>
          <target state="translated">现在，可以在 &lt;code&gt;references/video_classification/transforms.py&lt;/code&gt; 中找到规范化代码，请参阅此处的 &lt;code&gt;Normalize&lt;/code&gt; 函数。请注意，它与图像的标准规范化不同，因为它假定视频为4d。</target>
        </trans-unit>
        <trans-unit id="43d57f5d34139266a152ef339407d7c7bfa3e16a" translate="yes" xml:space="preserve">
          <source>For numerical stability the implementation reverts to the linear function when</source>
          <target state="translated">为了保证数值的稳定性,当以下情况时,实现将恢复到线性函数。</target>
        </trans-unit>
        <trans-unit id="e2c95cbc7cf538a8a02cb4f1bc38de8b5a3ed2b9" translate="yes" xml:space="preserve">
          <source>For object detection and instance segmentation, the pre-trained models return the predictions of the following classes:</source>
          <target state="translated">对于对象检测和实例分割,预训练的模型返回以下类的预测。</target>
        </trans-unit>
        <trans-unit id="0d60fa4ee1b04194485d841887bd2a9720912ce7" translate="yes" xml:space="preserve">
          <source>For one, if either</source>
          <target state="translated">首先,如果其中一个</target>
        </trans-unit>
        <trans-unit id="388c4450cdbff00300f1e66a7fe9bdbd7ad63df6" translate="yes" xml:space="preserve">
          <source>For person keypoint detection, the accuracies for the pre-trained models are as follows</source>
          <target state="translated">对于人的关键点检测,预训练模型的准确率如下所示</target>
        </trans-unit>
        <trans-unit id="50930fbd40ff369c0fa47ef8013fdffeda924fcf" translate="yes" xml:space="preserve">
          <source>For person keypoint detection, the pre-trained model return the keypoints in the following order:</source>
          <target state="translated">对于人的关键点检测,预训练的模型按照以下顺序返回关键点。</target>
        </trans-unit>
        <trans-unit id="1547488bbec68c16b5473f238ae370eab49bdca4" translate="yes" xml:space="preserve">
          <source>For references on how to use it, please refer to &lt;a href=&quot;https://github.com/pytorch/examples/tree/master/imagenet&quot;&gt;PyTorch example - ImageNet implementation&lt;/a&gt;</source>
          <target state="translated">有关如何使用它的参考，请参考&lt;a href=&quot;https://github.com/pytorch/examples/tree/master/imagenet&quot;&gt;PyTorch示例-ImageNet实现&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="35c8decc2166af07962152ef6f02d95b39a40bf1" translate="yes" xml:space="preserve">
          <source>For simplest usage provide &lt;code&gt;dtype&lt;/code&gt; argument that can be float16 or qint8. Weight-only quantization by default is performed for layers with large weights size - i.e. Linear and RNN variants.</source>
          <target state="translated">对于简单的使用提供了 &lt;code&gt;dtype&lt;/code&gt; 的论点，即可以float16或qint8。默认情况下，仅对具有较大权重大小的图层（即线性和RNN变体）执行仅权重量化。</target>
        </trans-unit>
        <trans-unit id="5b0de76315890608ccfec148ffc9eb96ab3d19d0" translate="yes" xml:space="preserve">
          <source>For summation index</source>
          <target state="translated">求和指数</target>
        </trans-unit>
        <trans-unit id="ae21afa5351a91a1f97daa42d064cca4e245e30f" translate="yes" xml:space="preserve">
          <source>For test time, we report the time for the model evaluation and postprocessing (including mask pasting in image), but not the time for computing the precision-recall.</source>
          <target state="translated">对于测试时间,我们报告了模型评估和后处理(包括图像中的掩模粘贴)的时间,但没有报告计算精度-回放的时间。</target>
        </trans-unit>
        <trans-unit id="294fafc65e707c55d30f92f6e6fa7597c4416793" translate="yes" xml:space="preserve">
          <source>For the case of two input spatial dimensions this operation is sometimes called &lt;code&gt;im2col&lt;/code&gt;.</source>
          <target state="translated">对于两个输入空间维度，此操作有时称为 &lt;code&gt;im2col&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4294b91105e53a9501af3f666602dc49b4fae690" translate="yes" xml:space="preserve">
          <source>For the case of two output spatial dimensions this operation is sometimes called &lt;code&gt;col2im&lt;/code&gt;.</source>
          <target state="translated">对于两个输出空间维度，此操作有时称为 &lt;code&gt;col2im&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="63464d394b40abaadcd5cd07925b101f22c85d5d" translate="yes" xml:space="preserve">
          <source>For the following examples:</source>
          <target state="translated">以下是一些例子:</target>
        </trans-unit>
        <trans-unit id="250e5319dc4351da49602e421d48ab9ddd21e40e" translate="yes" xml:space="preserve">
          <source>For the full list of NCCL environment variables, please refer to &lt;a href=&quot;https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html&quot;&gt;NVIDIA NCCL&amp;rsquo;s official documentation&lt;/a&gt;</source>
          <target state="translated">有关NCCL环境变量的完整列表，请参阅&lt;a href=&quot;https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html&quot;&gt;NVIDIA NCCL的官方文档。&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fbb3381cc7af3b7907b168edfc7c7438b66cd477" translate="yes" xml:space="preserve">
          <source>For the most part, you shouldn&amp;rsquo;t have to care whether or not a sparse tensor is coalesced or not, as most operations will work identically given a coalesced or uncoalesced sparse tensor. However, there are two cases in which you may need to care.</source>
          <target state="translated">在大多数情况下，您不必担心稀疏张量是否合并，因为在合并或未合并稀疏张量的情况下，大多数操作都可以相同地工作。但是，在两种情况下，您可能需要注意。</target>
        </trans-unit>
        <trans-unit id="b5ee688f7705f701edf82664a012c925ae4f0c34" translate="yes" xml:space="preserve">
          <source>For the unpacked case, the directions can be separated using &lt;code&gt;output.view(seq_len, batch, num_directions, hidden_size)&lt;/code&gt;, with forward and backward being direction &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; respectively. Similarly, the directions can be separated in the packed case.</source>
          <target state="translated">对于未 &lt;code&gt;output.view(seq_len, batch, num_directions, hidden_size)&lt;/code&gt; 情况，可以使用output.view（seq_len，batch，num_directions，hidden_​​size）分隔方向，前进和后退分别是方向 &lt;code&gt;0&lt;/code&gt; 和 &lt;code&gt;1&lt;/code&gt; 。同样，在包装好的情况下，方向也可以分开。</target>
        </trans-unit>
        <trans-unit id="c66bf21ad2ff938e858bedebd02eca91c5de881a" translate="yes" xml:space="preserve">
          <source>For these core statistics, values are broken down as follows.</source>
          <target state="translated">这些核心统计数字的数值细分如下:</target>
        </trans-unit>
        <trans-unit id="0b3352e82009813f0901947623d7383dc915b831" translate="yes" xml:space="preserve">
          <source>For unsorted sequences, use &lt;code&gt;enforce_sorted = False&lt;/code&gt;. If &lt;code&gt;enforce_sorted&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the sequences should be sorted by length in a decreasing order, i.e. &lt;code&gt;input[:,0]&lt;/code&gt; should be the longest sequence, and &lt;code&gt;input[:,B-1]&lt;/code&gt; the shortest one. &lt;code&gt;enforce_sorted = True&lt;/code&gt; is only necessary for ONNX export.</source>
          <target state="translated">对于未排序的序列，请使用 &lt;code&gt;enforce_sorted = False&lt;/code&gt; 。如果 &lt;code&gt;enforce_sorted&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则序列应按长度降序排列，即， &lt;code&gt;input[:,0]&lt;/code&gt; 应该是最长的序列，而 &lt;code&gt;input[:,B-1]&lt;/code&gt; 应该是最短的序列。 &lt;code&gt;enforce_sorted = True&lt;/code&gt; 仅对于ONNX导出是必需的。</target>
        </trans-unit>
        <trans-unit id="74aa708d44a325bbc3a2350d27d2ddf6e3e6b36a" translate="yes" xml:space="preserve">
          <source>For unsorted sequences, use &lt;code&gt;enforce_sorted = False&lt;/code&gt;. If &lt;code&gt;enforce_sorted&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the sequences should be sorted in the order of decreasing length. &lt;code&gt;enforce_sorted = True&lt;/code&gt; is only necessary for ONNX export.</source>
          <target state="translated">对于未排序的序列，请使用 &lt;code&gt;enforce_sorted = False&lt;/code&gt; 。如果 &lt;code&gt;enforce_sorted&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则应按长度减小的顺序对序列进行排序。 &lt;code&gt;enforce_sorted = True&lt;/code&gt; 仅对于ONNX导出是必需的。</target>
        </trans-unit>
        <trans-unit id="4f7d55db4ef6ba383bbd9077e4ad197526d0bc5b" translate="yes" xml:space="preserve">
          <source>Force collects GPU memory after it has been released by CUDA IPC.</source>
          <target state="translated">Force在CUDA IPC释放后收集GPU内存。</target>
        </trans-unit>
        <trans-unit id="65edc3b96f1e2f54f14a87eb92d4dea02b1b6e9b" translate="yes" xml:space="preserve">
          <source>Forces completion of a &lt;code&gt;torch.jit.Future[T]&lt;/code&gt; asynchronous task, returning the result of the task.</source>
          <target state="translated">强制完成 &lt;code&gt;torch.jit.Future[T]&lt;/code&gt; 异步任务，返回任务结果。</target>
        </trans-unit>
        <trans-unit id="dc619e95fe61cfbb5a61b8e23e153aa71a53f467" translate="yes" xml:space="preserve">
          <source>Forces completion of a &lt;code&gt;torch.jit.Future[T]&lt;/code&gt; asynchronous task, returning the result of the task. See &lt;a href=&quot;torch.jit.fork#torch.jit.fork&quot;&gt;&lt;code&gt;fork()&lt;/code&gt;&lt;/a&gt; for docs and examples. :param func: an asynchronous task reference, created through &lt;code&gt;torch.jit.fork&lt;/code&gt; :type func: torch.jit.Future[T]</source>
          <target state="translated">强制完成 &lt;code&gt;torch.jit.Future[T]&lt;/code&gt; 异步任务，返回任务结果。有关文档和示例，请参见&lt;a href=&quot;torch.jit.fork#torch.jit.fork&quot;&gt; &lt;code&gt;fork()&lt;/code&gt; &lt;/a&gt;。：param func：通过 &lt;code&gt;torch.jit.fork&lt;/code&gt; 创建的异步任务引用：type func：torch.jit.Future [T]</target>
        </trans-unit>
        <trans-unit id="7e827f4fab2c732110bd891aca164ea682ff2e51" translate="yes" xml:space="preserve">
          <source>Forks the RNG, so that when you return, the RNG is reset to the state that it was previously in.</source>
          <target state="translated">叉开RNG,所以当你返回时,RNG会被重置到之前的状态。</target>
        </trans-unit>
        <trans-unit id="e0b8013b454609aa16cd91b06faebccde13182b1" translate="yes" xml:space="preserve">
          <source>Forward and backward hooks defined on &lt;code&gt;module&lt;/code&gt; and its submodules will be invoked &lt;code&gt;len(device_ids)&lt;/code&gt; times, each with inputs located on a particular device. Particularly, the hooks are only guaranteed to be executed in correct order with respect to operations on corresponding devices. For example, it is not guaranteed that hooks set via &lt;a href=&quot;torch.nn.module#torch.nn.Module.register_forward_pre_hook&quot;&gt;&lt;code&gt;register_forward_pre_hook()&lt;/code&gt;&lt;/a&gt; be executed before &lt;code&gt;all&lt;/code&gt;&lt;code&gt;len(device_ids)&lt;/code&gt;&lt;a href=&quot;torch.nn.module#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; calls, but that each such hook be executed before the corresponding &lt;a href=&quot;torch.nn.module#torch.nn.Module.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; call of that device.</source>
          <target state="translated">在 &lt;code&gt;module&lt;/code&gt; 及其子模块上定义的向前和向后挂钩将被调用 &lt;code&gt;len(device_ids)&lt;/code&gt; 次，每次都有位于特定设备上的输入。特别是，仅保证在相应设备上的操作中，挂钩能够以正确的顺序执行。例如，不能保证在 &lt;code&gt;all&lt;/code&gt; &lt;code&gt;len(device_ids)&lt;/code&gt; &lt;a href=&quot;torch.nn.module#torch.nn.Module.forward&quot;&gt; &lt;code&gt;forward()&lt;/code&gt; &lt;/a&gt;调用之前执行通过&lt;a href=&quot;torch.nn.module#torch.nn.Module.register_forward_pre_hook&quot;&gt; &lt;code&gt;register_forward_pre_hook()&lt;/code&gt; &lt;/a&gt;设置的挂钩，但是不能保证每个这样的挂钩都在该设备的相应&lt;a href=&quot;torch.nn.module#torch.nn.Module.forward&quot;&gt; &lt;code&gt;forward()&lt;/code&gt; &lt;/a&gt;调用之前执行。</target>
        </trans-unit>
        <trans-unit id="6acf90721bbeda5f77e7073663e225cf7715008a" translate="yes" xml:space="preserve">
          <source>Forward and backward hooks defined on &lt;code&gt;module&lt;/code&gt; and its submodules won&amp;rsquo;t be invoked anymore, unless the hooks are initialized in the &lt;code&gt;forward()&lt;/code&gt; method.</source>
          <target state="translated">除非在 &lt;code&gt;forward()&lt;/code&gt; 方法中初始化了钩子，否则将不再调用在 &lt;code&gt;module&lt;/code&gt; 及其子模块上定义的向前和向后钩子。</target>
        </trans-unit>
        <trans-unit id="988135b5646708fe12c52e4f90092901a6ed112d" translate="yes" xml:space="preserve">
          <source>Fractional MaxPooling is described in detail in the paper &lt;a href=&quot;https://arxiv.org/abs/1412.6071&quot;&gt;Fractional MaxPooling&lt;/a&gt; by Ben Graham</source>
          <target state="translated">Ben Graham&lt;a href=&quot;https://arxiv.org/abs/1412.6071&quot;&gt;撰写&lt;/a&gt;的Fractional MaxPooling详细描述了Fractional MaxPooling</target>
        </trans-unit>
        <trans-unit id="2297da08c1909f5dca5e7ab8bf486b563fbe806e" translate="yes" xml:space="preserve">
          <source>FractionalMaxPool2d</source>
          <target state="translated">FractionalMaxPool2d</target>
        </trans-unit>
        <trans-unit id="d790b402d79ac1a723c790313bcd679999474630" translate="yes" xml:space="preserve">
          <source>Frequently Asked Questions</source>
          <target state="translated">常问问题</target>
        </trans-unit>
        <trans-unit id="8c7beab7a6d84d3a1004bcc75ccd72648a4866a0" translate="yes" xml:space="preserve">
          <source>Frobenius norm</source>
          <target state="translated">夫罗奔尼乌斯规范</target>
        </trans-unit>
        <trans-unit id="e553dabf708d383e58b9442ad6ace6c5038afc7a" translate="yes" xml:space="preserve">
          <source>From the &lt;code&gt;torch.nn.utils&lt;/code&gt; module</source>
          <target state="translated">来自 &lt;code&gt;torch.nn.utils&lt;/code&gt; 模块</target>
        </trans-unit>
        <trans-unit id="f92451195f62b1d7cd9731015aa1fa56d4159f9d" translate="yes" xml:space="preserve">
          <source>Fully Convolutional Networks</source>
          <target state="translated">完全卷积网络</target>
        </trans-unit>
        <trans-unit id="f1e410ad1472b42cb42cc98962428637290b6706" translate="yes" xml:space="preserve">
          <source>Function</source>
          <target state="translated">Function</target>
        </trans-unit>
        <trans-unit id="d8cdf10face49f05a0d7bce562c1cbcff9eeec04" translate="yes" xml:space="preserve">
          <source>Function Calls</source>
          <target state="translated">功能调用</target>
        </trans-unit>
        <trans-unit id="7c7552936b2d7609f5d81c59359e027294ef0188" translate="yes" xml:space="preserve">
          <source>Function is called as the entrypoint of the spawned process. This function must be defined at the top level of a module so it can be pickled and spawned. This is a requirement imposed by multiprocessing.</source>
          <target state="translated">函数作为被产卵过程的入口点被调用。这个函数必须定义在一个模块的顶层,这样它才能被腌制和产卵。这是多进程提出的要求。</target>
        </trans-unit>
        <trans-unit id="57608b0f76e1461b0dd084a18f9c93c533b79732" translate="yes" xml:space="preserve">
          <source>Function that computes the Hessian of a given scalar function.</source>
          <target state="translated">计算给定标量函数的切分函数。</target>
        </trans-unit>
        <trans-unit id="de45ff60cbfde5e2d453e6ab4052dd57e579a406" translate="yes" xml:space="preserve">
          <source>Function that computes the Jacobian of a given function.</source>
          <target state="translated">计算给定函数的Jacobian的函数。</target>
        </trans-unit>
        <trans-unit id="53e89b5d2efea7a989526e66020381aa4e865a00" translate="yes" xml:space="preserve">
          <source>Function that computes the dot product between a vector &lt;code&gt;v&lt;/code&gt; and the Hessian of a given scalar function at the point given by the inputs.</source>
          <target state="translated">在输入给定的点上计算向量 &lt;code&gt;v&lt;/code&gt; 与给定标量函数的Hessian之间的点积的函数。</target>
        </trans-unit>
        <trans-unit id="287af51397c51f28570adbfd1523afcb4677415b" translate="yes" xml:space="preserve">
          <source>Function that computes the dot product between a vector &lt;code&gt;v&lt;/code&gt; and the Jacobian of the given function at the point given by the inputs.</source>
          <target state="translated">在输入给定的点上计算向量 &lt;code&gt;v&lt;/code&gt; 与给定函数的雅可比行列之间的点积的函数。</target>
        </trans-unit>
        <trans-unit id="db56d8ae51d7ef133b6d7037c14d1d0f86dcbafc" translate="yes" xml:space="preserve">
          <source>Function that computes the dot product between the Hessian of a given scalar function and a vector &lt;code&gt;v&lt;/code&gt; at the point given by the inputs.</source>
          <target state="translated">计算给定标量函数的Hessian与输入给定点处的向量 &lt;code&gt;v&lt;/code&gt; 之间的点积的函数。</target>
        </trans-unit>
        <trans-unit id="62c588612b56212caded208831fd6675991af83c" translate="yes" xml:space="preserve">
          <source>Function that computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector &lt;code&gt;v&lt;/code&gt;.</source>
          <target state="translated">计算输入端给定点的给定函数的雅可比行列和向量 &lt;code&gt;v&lt;/code&gt; 之间的点积的函数。</target>
        </trans-unit>
        <trans-unit id="6eb78f68900c241e14e0ca55cb63779b55624f2b" translate="yes" xml:space="preserve">
          <source>Function that measures Binary Cross Entropy between target and output logits.</source>
          <target state="translated">测量目标和输出对数之间二元交叉熵的函数。</target>
        </trans-unit>
        <trans-unit id="7648ef7c5f8c3df8793ee3a78cea755f5210492d" translate="yes" xml:space="preserve">
          <source>Function that measures the Binary Cross Entropy between the target and the output.</source>
          <target state="translated">测量目标和输出之间二进制交叉熵的函数。</target>
        </trans-unit>
        <trans-unit id="9ffd252ff27f5e82a15818c0d16b964ae35b7608" translate="yes" xml:space="preserve">
          <source>Function that returns True when in compilation and False otherwise. This is useful especially with the @unused decorator to leave code in your model that is not yet TorchScript compatible. .. testcode:</source>
          <target state="translated">在编译时返回True,否则返回False的函数。这对于使用@未使用的装饰器在你的模型中留下还不兼容TorchScript的代码是非常有用的。...testcode:</target>
        </trans-unit>
        <trans-unit id="4f7128796586951f3b8d4c12bad19a0f376bcf68" translate="yes" xml:space="preserve">
          <source>Function that takes the mean element-wise absolute value difference.</source>
          <target state="translated">取平均元素的绝对值差的函数。</target>
        </trans-unit>
        <trans-unit id="092441d339641c913a709b12bd8927bee99f6a19" translate="yes" xml:space="preserve">
          <source>Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.</source>
          <target state="translated">函数,如果绝对的元素间误差低于β,则使用平方项,否则使用L1项。</target>
        </trans-unit>
        <trans-unit id="e285c73ee5b2c775ccf90093dc413f8d49580952" translate="yes" xml:space="preserve">
          <source>Function to draw a sequence of &lt;code&gt;n&lt;/code&gt; points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is</source>
          <target state="translated">从Sobol序列中绘制 &lt;code&gt;n&lt;/code&gt; 点序列的功能。请注意，样本取决于先前的样本。结果的大小是</target>
        </trans-unit>
        <trans-unit id="54711e443d02794b60e7868b9b77738a8238e18e" translate="yes" xml:space="preserve">
          <source>Function to fast-forward the state of the &lt;code&gt;SobolEngine&lt;/code&gt; by &lt;code&gt;n&lt;/code&gt; steps. This is equivalent to drawing &lt;code&gt;n&lt;/code&gt; samples without using the samples.</source>
          <target state="translated">通过 &lt;code&gt;n&lt;/code&gt; 步快进 &lt;code&gt;SobolEngine&lt;/code&gt; 的状态的功能。这等效于不使用样本就绘制 &lt;code&gt;n&lt;/code&gt; 个样本。</target>
        </trans-unit>
        <trans-unit id="ef0c07b5cc0a1093381723476ea2c34106682639" translate="yes" xml:space="preserve">
          <source>Function to reset the &lt;code&gt;SobolEngine&lt;/code&gt; to base state.</source>
          <target state="translated">将 &lt;code&gt;SobolEngine&lt;/code&gt; 重置为基本状态的功能。</target>
        </trans-unit>
        <trans-unit id="27655b57b5b53e117874eb4514d19d44a2fa2f28" translate="yes" xml:space="preserve">
          <source>Functional higher level API</source>
          <target state="translated">功能性的上层API</target>
        </trans-unit>
        <trans-unit id="f4400d33370b62a4424c92ac993690b6bf723355" translate="yes" xml:space="preserve">
          <source>Functional interface</source>
          <target state="translated">功能接口</target>
        </trans-unit>
        <trans-unit id="c75f6c5a3a8ea37a3aa2ef8f8e4a53197661b6a7" translate="yes" xml:space="preserve">
          <source>Functional interface (quantized).</source>
          <target state="translated">功能界面(量化)。</target>
        </trans-unit>
        <trans-unit id="c6f96173e459065a4f35775c868cf69352b58eaa" translate="yes" xml:space="preserve">
          <source>Functionally equivalent to a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;, but represents a single function and does not have any attributes or Parameters.</source>
          <target state="translated">功能上等效于&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;，但表示单个函数，并且没有任何属性或参数。</target>
        </trans-unit>
        <trans-unit id="1f72e9d093d3406e36decddb7c64c8207dc32272" translate="yes" xml:space="preserve">
          <source>Functionally equivalent to a &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;, but represents a single function and does not have any attributes or Parameters.</source>
          <target state="translated">功能上等效于&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;，但表示单个函数，并且没有任何属性或参数。</target>
        </trans-unit>
        <trans-unit id="2b961dea1dc0c60ddf9a2c8e9d090f6f7d082483" translate="yes" xml:space="preserve">
          <source>Functions</source>
          <target state="translated">Functions</target>
        </trans-unit>
        <trans-unit id="5bab26ebb87fa09b2599a41edb42c2be419cbfdb" translate="yes" xml:space="preserve">
          <source>Functions don&amp;rsquo;t change much, they can be decorated with &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/torch.jit.unused#torch.jit.unused&quot;&gt;&lt;code&gt;torch.jit.unused&lt;/code&gt;&lt;/a&gt; if needed.</source>
          <target state="translated">函数的变化不大，如果需要，可以使用&lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;generated/torch.jit.unused#torch.jit.unused&quot;&gt; &lt;code&gt;torch.jit.unused&lt;/code&gt; &lt;/a&gt;装饰它们。</target>
        </trans-unit>
        <trans-unit id="f08615c088136953c00202fd041b9b4df5e1dfea" translate="yes" xml:space="preserve">
          <source>Furthermore, if the &lt;code&gt;functions&lt;/code&gt; argument is supplied, bindings will be automatically generated for each function specified. &lt;code&gt;functions&lt;/code&gt; can either be a list of function names, or a dictionary mapping from function names to docstrings. If a list is given, the name of each function is used as its docstring.</source>
          <target state="translated">此外，如果提供 &lt;code&gt;functions&lt;/code&gt; 参数，则将为指定的每个函数自动生成绑定。 &lt;code&gt;functions&lt;/code&gt; 可以是函数名列表，也可以是从函数名到文档字符串的字典映射。如果给出了列表，则将每个函数的名称用作其文档字符串。</target>
        </trans-unit>
        <trans-unit id="5bace8a23b9deebac72cffe4881dc6eed9968755" translate="yes" xml:space="preserve">
          <source>Furthermore, the outputs are scaled by a factor of</source>
          <target state="translated">此外,产出的比例是按比例放大的。</target>
        </trans-unit>
        <trans-unit id="7bd79f69d6d1d8bfef4415bc3f9876396d21f772" translate="yes" xml:space="preserve">
          <source>Fuses a list of modules into a single module</source>
          <target state="translated">将一个模块列表融合成一个模块。</target>
        </trans-unit>
        <trans-unit id="936c5a8bd4c7984ab29544b2ca2086d83494e60f" translate="yes" xml:space="preserve">
          <source>Fuses only the following sequence of modules: conv, bn conv, bn, relu conv, relu linear, relu bn, relu All other sequences are left unchanged. For these sequences, replaces the first item in the list with the fused module, replacing the rest of the modules with identity.</source>
          <target state="translated">只融合以下模块序列:conv,bn conv,bn,relu conv,relu linear,relu bn,relu 所有其他序列保持不变。对于这些序列,用融合的模块替换列表中的第一项,用identity替换其余模块。</target>
        </trans-unit>
        <trans-unit id="5f71e96ba4fcb977ab1e675f428bf9d148fa7baf" translate="yes" xml:space="preserve">
          <source>GELU</source>
          <target state="translated">GELU</target>
        </trans-unit>
        <trans-unit id="1f9ad9bc561d09f2f445363cf93033cdd6037e9c" translate="yes" xml:space="preserve">
          <source>GLU</source>
          <target state="translated">GLU</target>
        </trans-unit>
        <trans-unit id="a6a6318544c9b361fc08c6ed94696c4d207d2748" translate="yes" xml:space="preserve">
          <source>GPU</source>
          <target state="translated">GPU</target>
        </trans-unit>
        <trans-unit id="954c88d52c93dd19e22542fcb20c7d583f7b8447" translate="yes" xml:space="preserve">
          <source>GPU hosts with Ethernet interconnect</source>
          <target state="translated">带以太网互连的GPU主机</target>
        </trans-unit>
        <trans-unit id="9238bee4212524d967f1550988f8d590152434a1" translate="yes" xml:space="preserve">
          <source>GPU hosts with InfiniBand interconnect</source>
          <target state="translated">带InfiniBand互连的GPU主机</target>
        </trans-unit>
        <trans-unit id="2b56fffb35167d283f21ccca2d0bc8fbd0a6fe43" translate="yes" xml:space="preserve">
          <source>GPU tensor</source>
          <target state="translated">GPU张量</target>
        </trans-unit>
        <trans-unit id="51c6274e38d61ebd2c2aa1fabf4fb11564b93c03" translate="yes" xml:space="preserve">
          <source>GRU</source>
          <target state="translated">GRU</target>
        </trans-unit>
        <trans-unit id="1e29d48c3333cba171b9e878119cbab38e34e4a1" translate="yes" xml:space="preserve">
          <source>GRUCell</source>
          <target state="translated">GRUCell</target>
        </trans-unit>
        <trans-unit id="cba508b12182b68f501d6af46c4f03f8fc5d2473" translate="yes" xml:space="preserve">
          <source>Gamma</source>
          <target state="translated">Gamma</target>
        </trans-unit>
        <trans-unit id="473631bf9e98f3b45650e597635bf741c36747b6" translate="yes" xml:space="preserve">
          <source>Gathers a list of tensors in a single process.</source>
          <target state="translated">在一个单一的过程中收集一个时序列表。</target>
        </trans-unit>
        <trans-unit id="bd67c5035db82b529f60e273fb31bc01d939b7bc" translate="yes" xml:space="preserve">
          <source>Gathers tensors from multiple GPU devices.</source>
          <target state="translated">收集多个GPU设备的张力。</target>
        </trans-unit>
        <trans-unit id="dfd7c23593ebd41b22c549217d736ad6cbc14502" translate="yes" xml:space="preserve">
          <source>Gathers tensors from the whole group in a list.</source>
          <target state="translated">将整个组中的tensors收集到一个列表中。</target>
        </trans-unit>
        <trans-unit id="a033b46254c69591de655bd57283ab1b1f241ecf" translate="yes" xml:space="preserve">
          <source>Gathers tensors from the whole group in a list. Each tensor in &lt;code&gt;tensor_list&lt;/code&gt; should reside on a separate GPU</source>
          <target state="translated">在列表中收集整个组的张量。 &lt;code&gt;tensor_list&lt;/code&gt; 中的每个张量应驻留在单独的GPU上</target>
        </trans-unit>
        <trans-unit id="d1ed345e7cd127b4f17c3621e8381fb806d9f267" translate="yes" xml:space="preserve">
          <source>Gathers values along an axis specified by &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">沿 &lt;code&gt;dim&lt;/code&gt; 所指定的轴收集值。</target>
        </trans-unit>
        <trans-unit id="83e2e53da9f325087f4bce1c008f1d1192e058ed" translate="yes" xml:space="preserve">
          <source>Generally speaking, input to this function should contain values following conjugate symmetry. Note that even if &lt;code&gt;onesided&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, often symmetry on some part is still needed. When this requirement is not satisfied, the behavior of &lt;a href=&quot;#torch.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt; is undefined. Since &lt;a href=&quot;../autograd#torch.autograd.gradcheck&quot;&gt;&lt;code&gt;torch.autograd.gradcheck()&lt;/code&gt;&lt;/a&gt; estimates numerical Jacobian with point perturbations, &lt;a href=&quot;#torch.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt; will almost certainly fail the check.</source>
          <target state="translated">通常，此函数的输入应包含遵循共轭对称性的值。请注意，即使 &lt;code&gt;onesided&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，仍然经常需要在某些部分上保持对称。当不满足此要求时，&lt;a href=&quot;#torch.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt;的行为是不确定的。由于&lt;a href=&quot;../autograd#torch.autograd.gradcheck&quot;&gt; &lt;code&gt;torch.autograd.gradcheck()&lt;/code&gt; &lt;/a&gt;估计带有点扰动的数字雅可比行列式，因此&lt;a href=&quot;#torch.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt;几乎肯定会失败。</target>
        </trans-unit>
        <trans-unit id="925a95fee15d092ef688c243751df4f4117845d7" translate="yes" xml:space="preserve">
          <source>Generate a square mask for the sequence. The masked positions are filled with float(&amp;lsquo;-inf&amp;rsquo;). Unmasked positions are filled with float(0.0).</source>
          <target state="translated">为该序列生成一个正方形蒙版。屏蔽的位置填充有float（'-inf'）。未屏蔽的位置填充有float（0.0）。</target>
        </trans-unit>
        <trans-unit id="05e4b9ca24a152cc1be23c09ff6368077d47b2b3" translate="yes" xml:space="preserve">
          <source>Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices &lt;code&gt;theta&lt;/code&gt;.</source>
          <target state="translated">给定一批仿射矩阵 &lt;code&gt;theta&lt;/code&gt; ，生成2D或3D流场（采样网格）。</target>
        </trans-unit>
        <trans-unit id="535a8bdc4a8b5e249e51bb72f3ff867e269f10d8" translate="yes" xml:space="preserve">
          <source>Generates a Vandermonde matrix.</source>
          <target state="translated">生成一个Vandermonde矩阵。</target>
        </trans-unit>
        <trans-unit id="a86ef66b9f9c0d2af98ab49a15285f8bd20e1d42" translate="yes" xml:space="preserve">
          <source>Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched.</source>
          <target state="translated">生成一个sample_shape形状的重参数化样本,如果分布参数是分批的,则生成一批sample_shape形状的重参数化样本。</target>
        </trans-unit>
        <trans-unit id="526c10ebcc2b4397308215e12b1bd4ed9189b980" translate="yes" xml:space="preserve">
          <source>Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies &lt;code&gt;transform()&lt;/code&gt; for every transform in the list.</source>
          <target state="translated">如果分配了分布参数，则生成一个sample_shape形状的重新参数化样本或sample_shape形状的一批重新参数化样本。首先从基本分布中采样，然后对列表中的每个转换应用 &lt;code&gt;transform()&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="15364dfe6049d908ff11f7465e040686a232a300" translate="yes" xml:space="preserve">
          <source>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</source>
          <target state="translated">生成一个sample_shape形的样本,如果分布参数是分批的,则生成一批sample_shape形的样本。</target>
        </trans-unit>
        <trans-unit id="581df05069d7c712c7a78fa21e0f7b76f4e89392" translate="yes" xml:space="preserve">
          <source>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies &lt;code&gt;transform()&lt;/code&gt; for every transform in the list.</source>
          <target state="translated">如果分配参数是批处理的，则生成sample_shape形状的样本或sample_shape形状的样本批。首先从基本分布中采样，然后对列表中的每个转换应用 &lt;code&gt;transform()&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b4909d3090c22e034c883a55c3857c6b650fef97" translate="yes" xml:space="preserve">
          <source>Generates n samples or n batches of samples if the distribution parameters are batched.</source>
          <target state="translated">如果分布参数是分批的,则生成n个样本或n个批次的样本。</target>
        </trans-unit>
        <trans-unit id="63b4637c6c6f7165bea01744025dd36ab607c841" translate="yes" xml:space="preserve">
          <source>Generates uniformly distributed random samples from the half-open interval &lt;code&gt;[low, high)&lt;/code&gt;.</source>
          <target state="translated">从半开间隔 &lt;code&gt;[low, high)&lt;/code&gt; 生成均匀分布的随机样本。</target>
        </trans-unit>
        <trans-unit id="1d20de03126b297e05c13a7d280f33e24c72c537" translate="yes" xml:space="preserve">
          <source>Generator</source>
          <target state="translated">Generator</target>
        </trans-unit>
        <trans-unit id="009c08ccefb98d44d31c65421a4320ecc2bb6f65" translate="yes" xml:space="preserve">
          <source>Generator.device -&amp;gt; device</source>
          <target state="translated">Generator.device-&amp;gt;设备</target>
        </trans-unit>
        <trans-unit id="a3e705cc61a19f33d7c9c030f107a70569966485" translate="yes" xml:space="preserve">
          <source>Generators</source>
          <target state="translated">Generators</target>
        </trans-unit>
        <trans-unit id="80dadd86173d0ff3979257793d4e45beb238b6a2" translate="yes" xml:space="preserve">
          <source>Generics</source>
          <target state="translated">Generics</target>
        </trans-unit>
        <trans-unit id="50911697822f06c09ce3f706d64281d2991753b3" translate="yes" xml:space="preserve">
          <source>Geometric</source>
          <target state="translated">Geometric</target>
        </trans-unit>
        <trans-unit id="43ac4a1c9cd4df862c76bc8567278e105813907d" translate="yes" xml:space="preserve">
          <source>Get &lt;a href=&quot;#torch.distributed.rpc.WorkerInfo&quot;&gt;&lt;code&gt;WorkerInfo&lt;/code&gt;&lt;/a&gt; of a given worker name. Use this &lt;a href=&quot;#torch.distributed.rpc.WorkerInfo&quot;&gt;&lt;code&gt;WorkerInfo&lt;/code&gt;&lt;/a&gt; to avoid passing an expensive string on every invocation.</source>
          <target state="translated">获取给定工人名称的&lt;a href=&quot;#torch.distributed.rpc.WorkerInfo&quot;&gt; &lt;code&gt;WorkerInfo&lt;/code&gt; &lt;/a&gt;。使用此&lt;a href=&quot;#torch.distributed.rpc.WorkerInfo&quot;&gt; &lt;code&gt;WorkerInfo&lt;/code&gt; &lt;/a&gt;可以避免在每次调用时传递昂贵的字符串。</target>
        </trans-unit>
        <trans-unit id="41265238a92192e4ad5f6debaeb489d9d1f1c823" translate="yes" xml:space="preserve">
          <source>Get the Torch Hub cache directory used for storing downloaded models &amp;amp; weights.</source>
          <target state="translated">获取用于存储下载的模型和权重的Torch Hub缓存目录。</target>
        </trans-unit>
        <trans-unit id="31e03005a5c777fa579a5d2e380424ed9ca0b4c8" translate="yes" xml:space="preserve">
          <source>Get the current default floating point &lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">获取当前的默认浮点&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="b8610c06d92827723a75fd0f1cce98077b3a70ce" translate="yes" xml:space="preserve">
          <source>Get the current default floating point &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">获取当前的默认浮点&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="5aca0eb2fa5dfc1cb36d36748698ea752a002636" translate="yes" xml:space="preserve">
          <source>Get the include paths required to build a C++ or CUDA extension.</source>
          <target state="translated">获取构建一个C++或CUDA扩展所需的包含路径。</target>
        </trans-unit>
        <trans-unit id="8c5303d4517efbd9f11eb88c60066d56bc4690f4" translate="yes" xml:space="preserve">
          <source>Get the k-th diagonal of a given matrix:</source>
          <target state="translated">获取给定矩阵的第k个对角线。</target>
        </trans-unit>
        <trans-unit id="d8e48ff0538ba6cbffe42300fa9158799c05f336" translate="yes" xml:space="preserve">
          <source>Get the square matrix where the input vector is the diagonal:</source>
          <target state="translated">获取输入向量为对角线的方阵。</target>
        </trans-unit>
        <trans-unit id="c3d9b9988129bd858a97b5e5dca9086f55e31cf1" translate="yes" xml:space="preserve">
          <source>Gets a non-deterministic random number from std::random_device or the current time and uses it to seed a Generator.</source>
          <target state="translated">从 std::random_device 或当前时间中获取一个非确定性的随机数,并将其作为 Generator 的种子。</target>
        </trans-unit>
        <trans-unit id="16cb932f5f31cf2818eda6bf06d30c94768bb98a" translate="yes" xml:space="preserve">
          <source>Gets the cuda capability of a device.</source>
          <target state="translated">获取设备的cuda功能。</target>
        </trans-unit>
        <trans-unit id="8c370c58908a07f2882a626fa97d9bcfd7bd61c5" translate="yes" xml:space="preserve">
          <source>Gets the current device of the generator.</source>
          <target state="translated">获取电源箱的电流装置。</target>
        </trans-unit>
        <trans-unit id="b9be3fb15dfe408dead8b567c5693ec335fb2ec0" translate="yes" xml:space="preserve">
          <source>Gets the name of a device.</source>
          <target state="translated">获取设备的名称。</target>
        </trans-unit>
        <trans-unit id="6a2e241a18985fac466002399c13c581b746dc24" translate="yes" xml:space="preserve">
          <source>Getting started with Distributed RPC Framework</source>
          <target state="translated">分布式RPC框架的入门</target>
        </trans-unit>
        <trans-unit id="c7e10d3aed3a471005914064e3e561dd9fce7d89" translate="yes" xml:space="preserve">
          <source>Given a 3-D tensor and reduction using the multiplication operation, &lt;code&gt;self&lt;/code&gt; is updated as:</source>
          <target state="translated">给定一个3-D张量并使用乘法操作进行归约，将 &lt;code&gt;self&lt;/code&gt; 更新为：</target>
        </trans-unit>
        <trans-unit id="cd7589a87267ad50da5e4440a08034e70102a4b2" translate="yes" xml:space="preserve">
          <source>Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor.</source>
          <target state="translated">给定一个通过线性(affine)每通道量化的Tensor,返回一个底层量化器的尺度Tensor。它的元素数量与张量的相应维度(来自q_per_channel_axis)相匹配。</target>
        </trans-unit>
        <trans-unit id="9a7e24af56b75cfda37479a07ba5602027c1c762" translate="yes" xml:space="preserve">
          <source>Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor.</source>
          <target state="translated">给定一个通过线性(affine)每通道量化的Tensor,返回一个底层量化器的零点张量。它的元素数量与张量的相应维度(来自q_per_channel_axis)相匹配。</target>
        </trans-unit>
        <trans-unit id="2ca023eeaa09e79566843e7d6c6a44e41c5d8038" translate="yes" xml:space="preserve">
          <source>Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.</source>
          <target state="translated">给定一个通过线性(仿射)每通道量化的Tensor,返回应用每通道量化的维度指数。</target>
        </trans-unit>
        <trans-unit id="488568401d49cd08f3be08869dd3fbd14d9e5113" translate="yes" xml:space="preserve">
          <source>Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().</source>
          <target state="translated">给定一个通过线性(affine)量化的Tensor,返回底层量化器()的尺度。</target>
        </trans-unit>
        <trans-unit id="70ceca1da9acd2d4b46a6674c2ea2f823b1ff770" translate="yes" xml:space="preserve">
          <source>Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().</source>
          <target state="translated">给定一个通过线性(affine)量化的Tensor,返回底层量化器()的零点。</target>
        </trans-unit>
        <trans-unit id="90c7e80a6051c57ef51f03a4a57294f2d936927c" translate="yes" xml:space="preserve">
          <source>Given a list of quantized Tensors, dequantize them and return a list of fp32 Tensors</source>
          <target state="translated">给定一个量化的Tensors列表,去量化它们并返回一个fp32 Tensors列表。</target>
        </trans-unit>
        <trans-unit id="275d4783ccc8a12c168de20cf782230c04ee1b4f" translate="yes" xml:space="preserve">
          <source>Given a quantized Tensor, &lt;code&gt;self.int_repr()&lt;/code&gt; returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.</source>
          <target state="translated">给 &lt;code&gt;self.int_repr()&lt;/code&gt; 化的Tensor，self.int_repr（）返回以uint8_t作为数据类型的CPU Tensor，该数据类型存储给定Tensor的基础uint8_t值。</target>
        </trans-unit>
        <trans-unit id="a07592b05f2ffe420d78857907dc0a868ad8e9ba" translate="yes" xml:space="preserve">
          <source>Given a quantized Tensor, dequantize it and return the dequantized float Tensor.</source>
          <target state="translated">给定一个量化的张量,对其进行去量化并返回去量化的浮动张量。</target>
        </trans-unit>
        <trans-unit id="df67c816ade50221be71f968c55c25fc9c22abad" translate="yes" xml:space="preserve">
          <source>Given an &lt;code&gt;input&lt;/code&gt; and a flow-field &lt;code&gt;grid&lt;/code&gt;, computes the &lt;code&gt;output&lt;/code&gt; using &lt;code&gt;input&lt;/code&gt; values and pixel locations from &lt;code&gt;grid&lt;/code&gt;.</source>
          <target state="translated">给定 &lt;code&gt;input&lt;/code&gt; 和流场 &lt;code&gt;grid&lt;/code&gt; ，使用 &lt;code&gt;input&lt;/code&gt; 值和 &lt;code&gt;grid&lt;/code&gt; 中的像素位置来计算 &lt;code&gt;output&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="993de1cd9d3c83efc214190a94f213ab60dcb670" translate="yes" xml:space="preserve">
          <source>Given running min/max as</source>
          <target state="translated">给定运行最小/最大值为</target>
        </trans-unit>
        <trans-unit id="1edda6e755754d8aedb4972c3cf98f079bb5938f" translate="yes" xml:space="preserve">
          <source>Given the legs of a right triangle, return its hypotenuse.</source>
          <target state="translated">给定一个直角三角形的腿,返回它的斜边。</target>
        </trans-unit>
        <trans-unit id="2507bf0b60a0a2f4a957fa33007787504bb4d58f" translate="yes" xml:space="preserve">
          <source>Gives us the following diagnostic information:</source>
          <target state="translated">给我们提供了以下诊断信息。</target>
        </trans-unit>
        <trans-unit id="d4b28b75d3d71965ffab4c160557a8512917b298" translate="yes" xml:space="preserve">
          <source>Globally prunes tensors corresponding to all parameters in &lt;code&gt;parameters&lt;/code&gt; by applying the specified &lt;code&gt;pruning_method&lt;/code&gt;.</source>
          <target state="translated">在全球范围内修剪对应于所有参数张量 &lt;code&gt;parameters&lt;/code&gt; 通过施加指定的 &lt;code&gt;pruning_method&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5b2d596c770a42eb120b6d78a8d729e895cf8f96" translate="yes" xml:space="preserve">
          <source>Globally prunes tensors corresponding to all parameters in &lt;code&gt;parameters&lt;/code&gt; by applying the specified &lt;code&gt;pruning_method&lt;/code&gt;. Modifies modules in place by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="translated">在全球范围内修剪对应于所有参数张量 &lt;code&gt;parameters&lt;/code&gt; 通过施加指定的 &lt;code&gt;pruning_method&lt;/code&gt; 。通过以下方式修改模块：1）添加一个名为 &lt;code&gt;name+'_mask'&lt;/code&gt; 的命名缓冲区，该缓冲区与通过修剪方法应用于参数 &lt;code&gt;name&lt;/code&gt; 的二进制掩码相对应。2）用已修剪的版本替换参数 &lt;code&gt;name&lt;/code&gt; ，而原始（未修剪的）参数存储在名为 &lt;code&gt;name+'_orig'&lt;/code&gt; 的新参数中。</target>
        </trans-unit>
        <trans-unit id="1c964494fd83a4fa2353af497be43d7d3c6b7430" translate="yes" xml:space="preserve">
          <source>Globally unique id to identify the worker.</source>
          <target state="translated">全球唯一的ID,用于识别工人。</target>
        </trans-unit>
        <trans-unit id="2dd8834e4e85debb567290745d3b3e1efdcd0e5d" translate="yes" xml:space="preserve">
          <source>Gloo has been hardened by years of extensive use in PyTorch and is thus very reliable. However, as it was designed to perform collective communication, it may not always be the best fit for RPC. For example, each networking operation is synchronous and blocking, which means that it cannot be run in parallel with others. Moreover, it opens a connection between all pairs of nodes, and brings down all of them when one fails, thus reducing the resiliency and the elasticity of the system.</source>
          <target state="translated">Gloo 经过多年在 PyTorch 中的广泛使用,已经变得非常坚固,因此非常可靠。然而,由于它是为执行集体通信而设计的,因此它可能并不总是最适合RPC。例如,每个网络操作都是同步和阻塞的,这意味着它不能与其他操作并行运行。此外,它在所有的节点对之间打开一个连接,当一个节点发生故障时,就会使所有的节点瘫痪,从而降低了系统的弹性和弹性。</target>
        </trans-unit>
        <trans-unit id="0493f8c5c9a4c1e6227e147f0649196be2a0314a" translate="yes" xml:space="preserve">
          <source>GoogLeNet</source>
          <target state="translated">GoogLeNet</target>
        </trans-unit>
        <trans-unit id="07ad0c4f37a421022704ed36cdcbf7e981dfe5e6" translate="yes" xml:space="preserve">
          <source>GoogLeNet (Inception v1) model architecture from &lt;a href=&quot;http://arxiv.org/abs/1409.4842&quot;&gt;&amp;ldquo;Going Deeper with Convolutions&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">GoogLeNet（Inception v1）的模型架构，来自&lt;a href=&quot;http://arxiv.org/abs/1409.4842&quot;&gt;&amp;ldquo;卷积更深&amp;rdquo;&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="2b233342b8e1172b316e6659198c51d93737c6f0" translate="yes" xml:space="preserve">
          <source>GoogLeNet (Inception v1) model architecture from &lt;a href=&quot;https://arxiv.org/abs/1409.4842&quot;&gt;&amp;ldquo;Going Deeper with Convolutions&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">GoogLeNet（Inception v1）的模型架构，来自&lt;a href=&quot;https://arxiv.org/abs/1409.4842&quot;&gt;&amp;ldquo;卷积更深&amp;rdquo;&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="a46ca2c8e8067233adba7afe56ba3ef36ef0ac6c" translate="yes" xml:space="preserve">
          <source>GoogleNet</source>
          <target state="translated">GoogleNet</target>
        </trans-unit>
        <trans-unit id="ede11b2557f474a2c7b9c843b53bc7eb91dcac40" translate="yes" xml:space="preserve">
          <source>Gradient Scaling</source>
          <target state="translated">梯度缩放</target>
        </trans-unit>
        <trans-unit id="6ff674c26d399b6074c452be450688d5b72da6db" translate="yes" xml:space="preserve">
          <source>Gradients are modified in-place.</source>
          <target state="translated">坡度就地修改。</target>
        </trans-unit>
        <trans-unit id="51f814ea6f476134875113c78aa4479584c4db98" translate="yes" xml:space="preserve">
          <source>Graphs can be inspected as shown to confirm that the computation described by a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; is correct, in both automated and manual fashion, as described below.</source>
          <target state="translated">如下图所示，可以检查图形以确认&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;所描述的计算是否正确（自动和手动）。</target>
        </trans-unit>
        <trans-unit id="db56247cfffad115bd03a34895c4144a2f602e2a" translate="yes" xml:space="preserve">
          <source>GroupNorm</source>
          <target state="translated">GroupNorm</target>
        </trans-unit>
        <trans-unit id="ae9629f4ebb82c6331c0809fa9a0e54b00e578e6" translate="yes" xml:space="preserve">
          <source>Groups</source>
          <target state="translated">Groups</target>
        </trans-unit>
        <trans-unit id="28391105a9b17b63b7212f323d98a19b7d728841" translate="yes" xml:space="preserve">
          <source>Gumbel</source>
          <target state="translated">Gumbel</target>
        </trans-unit>
        <trans-unit id="7cf184f4c67ad58283ecb19349720b0cae756829" translate="yes" xml:space="preserve">
          <source>H</source>
          <target state="translated">H</target>
        </trans-unit>
        <trans-unit id="e19beadee3375715c817358699b6b6b7c3b1c276" translate="yes" xml:space="preserve">
          <source>H=\text{embedding\_dim}</source>
          <target state="translated">H=\text{embedding\_dim}</target>
        </trans-unit>
        <trans-unit id="0daac27dde551de614ce1f8b22990869e521a767" translate="yes" xml:space="preserve">
          <source>H_{all}=\text{num\_directions} * \text{hidden\_size}</source>
          <target state="translated">H_{all}=/text{num_directions}*/text{hidden_size}</target>
        </trans-unit>
        <trans-unit id="5af2c1c42a9e7cbeea8d0ef376277c241943a820" translate="yes" xml:space="preserve">
          <source>H_{in1}=\text{in1\_features}</source>
          <target state="translated">H_{in1}=\text{in1\_features}</target>
        </trans-unit>
        <trans-unit id="553b3def323fb898bbf5a634729bc342796f0364" translate="yes" xml:space="preserve">
          <source>H_{in2}=\text{in2\_features}</source>
          <target state="translated">H_{in2}=\text{in2\_features}</target>
        </trans-unit>
        <trans-unit id="3bcb97c0c827228e3e76ce7ff65ced7d17f29099" translate="yes" xml:space="preserve">
          <source>H_{in}</source>
          <target state="translated">H_{in}</target>
        </trans-unit>
        <trans-unit id="208ac84ce5aa93dcc05f3ab66b20dc1d3775c246" translate="yes" xml:space="preserve">
          <source>H_{in} = \text{in\_features}</source>
          <target state="translated">H_{in}=text{in_features}(文本/特征)</target>
        </trans-unit>
        <trans-unit id="699846905d49c5bcf7b21460adb78513ef80b2fb" translate="yes" xml:space="preserve">
          <source>H_{in}=\text{input\_size}</source>
          <target state="translated">H_{in}=\text{input\_size}</target>
        </trans-unit>
        <trans-unit id="7530e9ad4c2fb5dd91aaec26c4ecbcf1a3a5ae05" translate="yes" xml:space="preserve">
          <source>H_{out}</source>
          <target state="translated">H_{out}</target>
        </trans-unit>
        <trans-unit id="29b7895de890ff23af55e3b18083accf3b3c9504" translate="yes" xml:space="preserve">
          <source>H_{out} = (H_{in} - 1) \times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}</source>
          <target state="translated">H_{out}=(H_{in}-1)遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数</target>
        </trans-unit>
        <trans-unit id="2ab9363dd5f0a667cae76fffcb24249e4641e2c1" translate="yes" xml:space="preserve">
          <source>H_{out} = (H_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}</source>
          <target state="translated">H_{out}=(H_{in}-1)xtimes \text{stride[1]}-2 xtimes \text{padding[1]}+\text{kernel\size[1]}</target>
        </trans-unit>
        <trans-unit id="eb1e8745c8404aa0257819793650f55ce47e4a32" translate="yes" xml:space="preserve">
          <source>H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1</source>
          <target state="translated">H_{out}=(H_{in}-1)遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 (遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数</target>
        </trans-unit>
        <trans-unit id="18b0ad397dc5e56655f52d98246617aead364a80" translate="yes" xml:space="preserve">
          <source>H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{kernel\_size}[0]</source>
          <target state="translated">H_{out}=(H_{in}-1)遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数</target>
        </trans-unit>
        <trans-unit id="79c01a18bf062cd324b81ee63feda955b8a67b5e" translate="yes" xml:space="preserve">
          <source>H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1</source>
          <target state="translated">H_{out}=(H_{in}-1)遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 (遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数</target>
        </trans-unit>
        <trans-unit id="092f027e0b0b6c6ccdb35d8eefd3fbd19f119f52" translate="yes" xml:space="preserve">
          <source>H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}</source>
          <target state="translated">H_{out}=H_{in}+text{padding\top}+text{padding\bottom}。</target>
        </trans-unit>
        <trans-unit id="42cf8f4ab52d42aa9e4d4c9b75634fb9ef976615" translate="yes" xml:space="preserve">
          <source>H_{out} = H_{in} \times \text{upscale\_factor}</source>
          <target state="translated">H_{out}=H_{in}遍地开花 遍地开花</target>
        </trans-unit>
        <trans-unit id="be48d8fe2e8d0b5e7eb05cfe3a720b22ad1fddb5" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor H_{in} \times \text{scale\_factor} \right\rfloor</source>
          <target state="translated">H_{out}=左右楼层 H_{in}左右楼层</target>
        </trans-unit>
        <trans-unit id="2df9cd2b82eb2979d700977c589d82b892fd325a" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding[0]} - \text{dilation[0]} \times (\text{kernel\_size[0]} - 1) - 1}{\text{stride[0]}} + 1\right\rfloor</source>
          <target state="translated">H_{out}=\left\lfloor\frac{H_{in}+2*\text{padding[0]}-\text{dilation[0]}\times (text{kernel\size[0]}-1)-1}{text{stride[0]}}+1\right\rfloor</target>
        </trans-unit>
        <trans-unit id="475e8a255144af15d34931776b25e492a4505dc7" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor</source>
          <target state="translated">H_{out}=左/右/楼面frac{H_{in}+2次 填充物}[0]-扩张物}[0]填充物(text{kernel_size}[0]-1)-1}{text{stride}[0]}+1次右/楼面frac{H_{in}</target>
        </trans-unit>
        <trans-unit id="017fc26768fffbdc1fef6a9416fcdd78e1cd6d94" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[0] - \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor</source>
          <target state="translated">H_{out}=\left\lfloor\frac{H_{in}+2 \times \text{padding}[0]-\text{kernel_size}[0]}{text{stride}[0]}+1 \right\rfloor</target>
        </trans-unit>
        <trans-unit id="d6788aaadc5cc718ef1733396e3d48f7aa8bc1e1" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="translated">H_{out}=左/右/楼面frac{H_{in}+2 遍 遍(遍)遍(遍)遍(遍)遍(遍)遍(遍)遍(遍)遍(遍)遍(遍)遍(遍)遍(遍)遍(遍)遍(遍)遍(遍)遍(遍)遍(遍)遍(遍)</target>
        </trans-unit>
        <trans-unit id="7eddc94bad71df818c92fb147993424e6e6351b5" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor</source>
          <target state="translated">H_{out}=\left\lfloor\frac{H_{in}+2 \times \text{padding}[1]-\text{kernel_size}[1]}{text{stride}[1]}+1 \right\rfloor</target>
        </trans-unit>
        <trans-unit id="fea56e0b8cb35cbb479b336782329a86326003b9" translate="yes" xml:space="preserve">
          <source>H_{out} = \left\lfloor\frac{H_{in} - \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor</source>
          <target state="translated">H_{out}=\left\lfloor\frac{H_{in}-\text{kernel\_size}[0]}{text{stride}[0]}+1right\rfloor</target>
        </trans-unit>
        <trans-unit id="563db076821ec71bea9cecf69cd0383ccd26729e" translate="yes" xml:space="preserve">
          <source>H_{out} = \text{out\_features}</source>
          <target state="translated">H_{out}=text{out_features}(文本{out_features})</target>
        </trans-unit>
        <trans-unit id="4a917c5666fd0c869393621972c028e97e193eb0" translate="yes" xml:space="preserve">
          <source>H_{out}=\text{hidden\_size}</source>
          <target state="translated">H_{out}=\text{hidden\_size}</target>
        </trans-unit>
        <trans-unit id="baeaafcb89405e03dc701698426b69d67220a62d" translate="yes" xml:space="preserve">
          <source>H_{out}=\text{out\_features}</source>
          <target state="translated">H_{out}=\text{out\_features}</target>
        </trans-unit>
        <trans-unit id="e3b87872ef89415e5807523e9374221302af4a0e" translate="yes" xml:space="preserve">
          <source>HalfCauchy</source>
          <target state="translated">HalfCauchy</target>
        </trans-unit>
        <trans-unit id="4fe2e4918ad02d73bac15c6c080f73da0de6891d" translate="yes" xml:space="preserve">
          <source>HalfNormal</source>
          <target state="translated">HalfNormal</target>
        </trans-unit>
        <trans-unit id="593e90ae43cc7b6082dfac4b6d583f5426501c8b" translate="yes" xml:space="preserve">
          <source>Hamming window function.</source>
          <target state="translated">汉明窗功能。</target>
        </trans-unit>
        <trans-unit id="69d7b15573065426b9ad5d17655afb90ef85c68f" translate="yes" xml:space="preserve">
          <source>Hann window function.</source>
          <target state="translated">汉恩窗口功能。</target>
        </trans-unit>
        <trans-unit id="9deb0b051207a937d2c2524ee2d17196f4c7e2ae" translate="yes" xml:space="preserve">
          <source>HardShrink</source>
          <target state="translated">HardShrink</target>
        </trans-unit>
        <trans-unit id="a1d971d31da1fbc25ba44171ebf839ffcca3d2d9" translate="yes" xml:space="preserve">
          <source>HardTanh</source>
          <target state="translated">HardTanh</target>
        </trans-unit>
        <trans-unit id="c9c68b0024efc57340d60b6a9fea397c5c6ea7cf" translate="yes" xml:space="preserve">
          <source>HardTanh is defined as:</source>
          <target state="translated">HardTanh的定义是:</target>
        </trans-unit>
        <trans-unit id="b2d44614503eb1489dff12f9f7cd6893be6f2de6" translate="yes" xml:space="preserve">
          <source>Hardshrink</source>
          <target state="translated">Hardshrink</target>
        </trans-unit>
        <trans-unit id="6da2cabc8d832449dfd76781dd5cab3e0527d812" translate="yes" xml:space="preserve">
          <source>Hardsigmoid</source>
          <target state="translated">Hardsigmoid</target>
        </trans-unit>
        <trans-unit id="34e8d8510ac49404bdd474758f835c671f823a90" translate="yes" xml:space="preserve">
          <source>Hardswish</source>
          <target state="translated">Hardswish</target>
        </trans-unit>
        <trans-unit id="c5343dec88b0ad5e872efa7466247b124ae44911" translate="yes" xml:space="preserve">
          <source>Hardtanh</source>
          <target state="translated">Hardtanh</target>
        </trans-unit>
        <trans-unit id="0698ac1b047809947767285ae1c3db0b44a90c83" translate="yes" xml:space="preserve">
          <source>Helper decorator for &lt;code&gt;forward&lt;/code&gt; methods of custom autograd functions (subclasses of &lt;a href=&quot;autograd#torch.autograd.Function&quot;&gt;&lt;code&gt;torch.autograd.Function&lt;/code&gt;&lt;/a&gt;). See the &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/amp_examples.html#amp-custom-examples&quot;&gt;example page&lt;/a&gt; for more detail.</source>
          <target state="translated">用于自定义autograd函数（&lt;a href=&quot;autograd#torch.autograd.Function&quot;&gt; &lt;code&gt;torch.autograd.Function&lt;/code&gt; 的&lt;/a&gt;子类）的 &lt;code&gt;forward&lt;/code&gt; 方法的Helper装饰器。有关更多详细信息，请参见&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/amp_examples.html#amp-custom-examples&quot;&gt;示例页面&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="862f59a03aa723c79d215e788d02145626feeb93" translate="yes" xml:space="preserve">
          <source>Helper decorator for backward methods of custom autograd functions (subclasses of &lt;a href=&quot;autograd#torch.autograd.Function&quot;&gt;&lt;code&gt;torch.autograd.Function&lt;/code&gt;&lt;/a&gt;). Ensures that &lt;code&gt;backward&lt;/code&gt; executes with the same autocast state as &lt;code&gt;forward&lt;/code&gt;. See the &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/amp_examples.html#amp-custom-examples&quot;&gt;example page&lt;/a&gt; for more detail.</source>
          <target state="translated">用于自定义autograd函数（&lt;a href=&quot;autograd#torch.autograd.Function&quot;&gt; &lt;code&gt;torch.autograd.Function&lt;/code&gt; 的&lt;/a&gt;子类）的后向方法的Helper装饰器。确保 &lt;code&gt;backward&lt;/code&gt; 执行与相同自动施放状态 &lt;code&gt;forward&lt;/code&gt; 。有关更多详细信息，请参见&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/amp_examples.html#amp-custom-examples&quot;&gt;示例页面&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e2828b0734fec8e5d21a67190b498444f3d1d6ac" translate="yes" xml:space="preserve">
          <source>Helper function to convert all &lt;code&gt;BatchNorm*D&lt;/code&gt; layers in the model to &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt;&lt;/a&gt; layers.</source>
          <target state="translated">辅助函数可将模型中的所有 &lt;code&gt;BatchNorm*D&lt;/code&gt; 图层转换为&lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt; &lt;code&gt;torch.nn.SyncBatchNorm&lt;/code&gt; &lt;/a&gt;图层。</target>
        </trans-unit>
        <trans-unit id="ecbcb0e424c40d9931c2c7c8b44c33c5f4e12465" translate="yes" xml:space="preserve">
          <source>Here</source>
          <target state="translated">Here</target>
        </trans-unit>
        <trans-unit id="2bcc88cec50d653f641638296cdbe26d969f54b5" translate="yes" xml:space="preserve">
          <source>Here are the summary of the accuracies for the models trained on the instances set of COCO train2017 and evaluated on COCO val2017.</source>
          <target state="translated">下面是在COCO train2017的实例集上训练的模型和在COCO val2017上评估的模型的精度总结。</target>
        </trans-unit>
        <trans-unit id="dab4abc36186b02c3f92677e0eb34f60ea19ee65" translate="yes" xml:space="preserve">
          <source>Here are the ways to call &lt;code&gt;to&lt;/code&gt;:</source>
          <target state="translated">下面是调用的方式 &lt;code&gt;to&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="445a6f9d83de6be3e9c47cd5d94f26ed51643163" translate="yes" xml:space="preserve">
          <source>Here is a code snippet specifies an entrypoint for &lt;code&gt;resnet18&lt;/code&gt; model if we expand the implementation in &lt;code&gt;pytorch/vision/hubconf.py&lt;/code&gt;. In most case importing the right function in &lt;code&gt;hubconf.py&lt;/code&gt; is sufficient. Here we just want to use the expanded version as an example to show how it works. You can see the full script in &lt;a href=&quot;https://github.com/pytorch/vision/blob/master/hubconf.py&quot;&gt;pytorch/vision repo&lt;/a&gt;</source>
          <target state="translated">如果我们在 &lt;code&gt;pytorch/vision/hubconf.py&lt;/code&gt; 中扩展实现，则以下代码片段指定了 &lt;code&gt;resnet18&lt;/code&gt; 模型的入口点。在大多数情况下，在 &lt;code&gt;hubconf.py&lt;/code&gt; 中导入正确的功能就足够了。在这里，我们仅以扩展版本为例来说明其工作原理。您可以在&lt;a href=&quot;https://github.com/pytorch/vision/blob/master/hubconf.py&quot;&gt;pytorch / vision repo中&lt;/a&gt;看到完整的脚本</target>
        </trans-unit>
        <trans-unit id="ae7bbf97b180351dca19b9329b791cc507f2b072" translate="yes" xml:space="preserve">
          <source>Here is a simple script which exports a pretrained AlexNet as defined in torchvision into ONNX. It runs a single round of inference and then saves the resulting traced model to &lt;code&gt;alexnet.onnx&lt;/code&gt;:</source>
          <target state="translated">这是一个简单的脚本，可以将Torchvision中定义的经过预训练的AlexNet导出到ONNX中。它运行一轮推断，然后将生成的跟踪模型保存到 &lt;code&gt;alexnet.onnx&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="92287b24e3387d7af034be3a29ae6b7bb430d9be" translate="yes" xml:space="preserve">
          <source>Here is an example of handling missing symbolic function for &lt;code&gt;elu&lt;/code&gt; operator. We try to export the model and see the error message as below:</source>
          <target state="translated">这是为 &lt;code&gt;elu&lt;/code&gt; 运算符处理缺少的符号函数的示例。我们尝试导出模型，并看到如下错误消息：</target>
        </trans-unit>
        <trans-unit id="e2d985f7b3554227167f11444cb5f1733937d03c" translate="yes" xml:space="preserve">
          <source>Here is another &lt;a href=&quot;https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html&quot;&gt;tutorial of exporting the SuperResolution model to ONNX.&lt;/a&gt;.</source>
          <target state="translated">这是&lt;a href=&quot;https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html&quot;&gt;将SuperResolution模型导出到ONNX的&lt;/a&gt;另一教程。。</target>
        </trans-unit>
        <trans-unit id="3314513abe3b45daef540f3d273944cf3438114e" translate="yes" xml:space="preserve">
          <source>Here the model &lt;code&gt;model&lt;/code&gt; can be an arbitrary &lt;a href=&quot;generated/torch.nn.module#torch.nn.Module&quot;&gt;&lt;code&gt;torch.nn.Module&lt;/code&gt;&lt;/a&gt; object. &lt;code&gt;swa_model&lt;/code&gt; will keep track of the running averages of the parameters of the &lt;code&gt;model&lt;/code&gt;. To update these averages, you can use the &lt;code&gt;update_parameters()&lt;/code&gt; function:</source>
          <target state="translated">在这里，模型 &lt;code&gt;model&lt;/code&gt; 可以是任意的&lt;a href=&quot;generated/torch.nn.module#torch.nn.Module&quot;&gt; &lt;code&gt;torch.nn.Module&lt;/code&gt; &lt;/a&gt;对象。 &lt;code&gt;swa_model&lt;/code&gt; 将跟踪 &lt;code&gt;model&lt;/code&gt; 参数的运行平均值。要更新这些平均值，可以使用 &lt;code&gt;update_parameters()&lt;/code&gt; 函数：</target>
        </trans-unit>
        <trans-unit id="fd181c7eb29dc3b0fda6e46bc8cb566ca4c818d9" translate="yes" xml:space="preserve">
          <source>Hessian (Tensor or a tuple of tuple of Tensors) if there are a single input,</source>
          <target state="translated">Hessian(Tensor或Tuple of Tensors的元组),如果有一个输入。</target>
        </trans-unit>
        <trans-unit id="3d3237992ce2b8eaf6bc76469f46fd9b7ad4ba34" translate="yes" xml:space="preserve">
          <source>HingeEmbeddingLoss</source>
          <target state="translated">HingeEmbeddingLoss</target>
        </trans-unit>
        <trans-unit id="144dc3699fae1cc6bf638916541c8cf0248b5ad0" translate="yes" xml:space="preserve">
          <source>Histogram represented as a tensor</source>
          <target state="translated">直方图用张量表示</target>
        </trans-unit>
        <trans-unit id="6abad81420de9ddfe06289f2d7475110d726e0f7" translate="yes" xml:space="preserve">
          <source>Holds parameters in a dictionary.</source>
          <target state="translated">在字典中保存参数。</target>
        </trans-unit>
        <trans-unit id="5b433389b7ec9270f0a561a21e07b6fafe96e6b4" translate="yes" xml:space="preserve">
          <source>Holds parameters in a list.</source>
          <target state="translated">在列表中保存参数。</target>
        </trans-unit>
        <trans-unit id="57c99ed1c82d3f1f9d82887040191b4b4abeeaa4" translate="yes" xml:space="preserve">
          <source>Holds submodules in a dictionary.</source>
          <target state="translated">在字典中保存子模块。</target>
        </trans-unit>
        <trans-unit id="26eb2ae7fc8977e64bf6bd3cc41f59552193cc67" translate="yes" xml:space="preserve">
          <source>Holds submodules in a list.</source>
          <target state="translated">在列表中保存子模块。</target>
        </trans-unit>
        <trans-unit id="e7b0c97307f7d5f337291cd7806994ff8fc4ef93" translate="yes" xml:space="preserve">
          <source>Holds the data and list of &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence.batch_sizes&quot;&gt;&lt;code&gt;batch_sizes&lt;/code&gt;&lt;/a&gt; of a packed sequence.</source>
          <target state="translated">保留打包序列的数据和&lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence.batch_sizes&quot;&gt; &lt;code&gt;batch_sizes&lt;/code&gt; &lt;/a&gt;列表。</target>
        </trans-unit>
        <trans-unit id="8419c96872dc0338c3c6bf4230e67b362cd3efd6" translate="yes" xml:space="preserve">
          <source>Holds the data and list of &lt;code&gt;batch_sizes&lt;/code&gt; of a packed sequence.</source>
          <target state="translated">保留打包序列的数据和 &lt;code&gt;batch_sizes&lt;/code&gt; 列表。</target>
        </trans-unit>
        <trans-unit id="0153f88746391d4a3323f9debd32f41ab51f78cf" translate="yes" xml:space="preserve">
          <source>Host to GPU copies are much faster when they originate from pinned (page-locked) memory. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-memory-pinning&quot;&gt;Use pinned memory buffers&lt;/a&gt; for more details on when and how to use pinned memory generally.</source>
          <target state="translated">主机到GPU副本源自固定（页面锁定）内存时，速度要快得多。请参阅&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-memory-pinning&quot;&gt;使用固定的内存缓冲区，&lt;/a&gt;以获取有关何时以及如何一般使用固定的内存的更多详细信息。</target>
        </trans-unit>
        <trans-unit id="63507ce15285f9aea8a61982f77258d3225e1e12" translate="yes" xml:space="preserve">
          <source>How to adjust learning rate</source>
          <target state="translated">如何调整学习率</target>
        </trans-unit>
        <trans-unit id="94d972a8a1e2dd3bfc19dfa88cfbe55c7c0c3689" translate="yes" xml:space="preserve">
          <source>How to implement an entrypoint?</source>
          <target state="translated">如何实现入口点?</target>
        </trans-unit>
        <trans-unit id="b1304bfd1ec08ee66df1618b8642dc42d805bfa0" translate="yes" xml:space="preserve">
          <source>How to use an optimizer</source>
          <target state="translated">如何使用优化器</target>
        </trans-unit>
        <trans-unit id="6310012cefe177ccf7e96e77a31017e3653da608" translate="yes" xml:space="preserve">
          <source>However the following will error when caching due to dependency reversal:</source>
          <target state="translated">但是,当缓存时,由于依赖性反转,会出现以下错误。</target>
        </trans-unit>
        <trans-unit id="25a9b274bc53945b3703ea4d8441695a628a5f50" translate="yes" xml:space="preserve">
          <source>However, &lt;a href=&quot;#torch.nn.EmbeddingBag&quot;&gt;&lt;code&gt;EmbeddingBag&lt;/code&gt;&lt;/a&gt; is much more time and memory efficient than using a chain of these operations.</source>
          <target state="translated">但是，与使用一系列操作相比，&lt;a href=&quot;#torch.nn.EmbeddingBag&quot;&gt; &lt;code&gt;EmbeddingBag&lt;/code&gt; 的&lt;/a&gt;时间和内存效率更高。</target>
        </trans-unit>
        <trans-unit id="0911b09b4e58b715dde3450d0531ebe9129437bf" translate="yes" xml:space="preserve">
          <source>However, &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence.batch_sizes&quot;&gt;&lt;code&gt;batch_sizes&lt;/code&gt;&lt;/a&gt; should always be a CPU &lt;code&gt;torch.int64&lt;/code&gt; tensor.</source>
          <target state="translated">但是，&lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence.batch_sizes&quot;&gt; &lt;code&gt;batch_sizes&lt;/code&gt; &lt;/a&gt;应该始终是CPU &lt;code&gt;torch.int64&lt;/code&gt; 张量。</target>
        </trans-unit>
        <trans-unit id="8db3600da518c9e30c26a5157e4e57a0a03f2dc3" translate="yes" xml:space="preserve">
          <source>However, if sharding results in multiple workers having incomplete last batches, this estimate can still be inaccurate, because (1) an otherwise complete batch can be broken into multiple ones and (2) more than one batch worth of samples can be dropped when &lt;code&gt;drop_last&lt;/code&gt; is set. Unfortunately, PyTorch can not detect such cases in general.</source>
          <target state="translated">但是，如果分片导致多个工作人员的最后一批不完整，则此估计仍然可能不准确，因为（1）否则可以将完整的一批分成多批，并且（2）当 &lt;code&gt;drop_last&lt;/code&gt; 时可以丢弃多于一批的样本设置好了。不幸的是，PyTorch通常无法检测到此类情况。</target>
        </trans-unit>
        <trans-unit id="ca73ab65568cd125c2d27a22bbd9e863c10b675d" translate="yes" xml:space="preserve">
          <source>I</source>
          <target state="translated">I</target>
        </trans-unit>
        <trans-unit id="17be3fac04b9eb29173ba1517552ac97aa85e862" translate="yes" xml:space="preserve">
          <source>I. M. Sobol. The distribution of points in a cube and the accurate evaluation of integrals. Zh. Vychisl. Mat. i Mat. Phys., 7:784-802, 1967.</source>
          <target state="translated">I.M.Sobol.立方体中点的分布和积分的精确评价.Zh.Vychisl.Mat.i Mat.Phys.,7:784-802,1967.</target>
        </trans-unit>
        <trans-unit id="c3eeaff2d8f8be416c4b7ba0cc9972581ba73c12" translate="yes" xml:space="preserve">
          <source>INDICES WITH CORRESPONDING NAMES:</source>
          <target state="translated">有相应名称的指数:</target>
        </trans-unit>
        <trans-unit id="7e5a975b6add84fd53e3710a9ceac15eb06663b7" translate="yes" xml:space="preserve">
          <source>Identity</source>
          <target state="translated">Identity</target>
        </trans-unit>
        <trans-unit id="751c68a3471b1c791efaee0a8e7c24ea0c266efd" translate="yes" xml:space="preserve">
          <source>If</source>
          <target state="translated">If</target>
        </trans-unit>
        <trans-unit id="632018a30643b60c95c40b0734c7a40ff76a740f" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;#torch.distributions.categorical.Categorical.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; is 1D with length-&lt;code&gt;K&lt;/code&gt;, each element is the relative probability of sampling the class at that index.</source>
          <target state="translated">如果&lt;a href=&quot;#torch.distributions.categorical.Categorical.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt;是长度为 &lt;code&gt;K&lt;/code&gt; 的1D ，则每个元素都是在该索引处采样该类的相对概率。</target>
        </trans-unit>
        <trans-unit id="0fbd2b7cbc8dbe46ae9eba9716b0dcf6af626a55" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;#torch.distributions.categorical.Categorical.probs&quot;&gt;&lt;code&gt;probs&lt;/code&gt;&lt;/a&gt; is 2D, it is treated as a batch of relative probability vectors.</source>
          <target state="translated">如果&lt;a href=&quot;#torch.distributions.categorical.Categorical.probs&quot;&gt; &lt;code&gt;probs&lt;/code&gt; &lt;/a&gt;是2D，它被处理为批量相对概率向量。</target>
        </trans-unit>
        <trans-unit id="e876238e2103f6c99bd92bbd23268dffc89d5e98" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;#torch.hub.set_dir&quot;&gt;&lt;code&gt;set_dir()&lt;/code&gt;&lt;/a&gt; is not called, default path is &lt;code&gt;$TORCH_HOME/hub&lt;/code&gt; where environment variable &lt;code&gt;$TORCH_HOME&lt;/code&gt; defaults to &lt;code&gt;$XDG_CACHE_HOME/torch&lt;/code&gt;. &lt;code&gt;$XDG_CACHE_HOME&lt;/code&gt; follows the X Design Group specification of the Linux filesystem layout, with a default value &lt;code&gt;~/.cache&lt;/code&gt; if the environment variable is not set.</source>
          <target state="translated">如果未调用&lt;a href=&quot;#torch.hub.set_dir&quot;&gt; &lt;code&gt;set_dir()&lt;/code&gt; &lt;/a&gt;，则默认路径为 &lt;code&gt;$TORCH_HOME/hub&lt;/code&gt; ，其中环境变量 &lt;code&gt;$TORCH_HOME&lt;/code&gt; 默认值为 &lt;code&gt;$XDG_CACHE_HOME/torch&lt;/code&gt; 。 &lt;code&gt;$XDG_CACHE_HOME&lt;/code&gt; 遵循Linux文件系统布局的X设计组规范，如果未设置环境变量，则默认值为 &lt;code&gt;~/.cache&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2365a78f9c70fd188f67372dcb4ead5e46155be1" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; &amp;gt; 0, it is above the main diagonal.</source>
          <target state="translated">如果&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt;&amp;gt; 0，则它在主对角线上方。</target>
        </trans-unit>
        <trans-unit id="3b581ac7d1874578962f7ebaf7dd7355265bfcb4" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; &amp;lt; 0, it is below the main diagonal.</source>
          <target state="translated">如果&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt;&amp;lt;0，则它在主对角线以下。</target>
        </trans-unit>
        <trans-unit id="d93aaba0d391e746ad390a6373a6f9bdbe62d27c" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;diagonal&lt;/code&gt;&lt;/a&gt; = 0, it is the main diagonal.</source>
          <target state="translated">如果&lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;diagonal&lt;/code&gt; &lt;/a&gt;= 0，则它是主对角线。</target>
        </trans-unit>
        <trans-unit id="a1496c02b09e4951bc82bcd52564a0a26f1a0976" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;(h_0, c_0)&lt;/code&gt; is not provided, both &lt;strong&gt;h_0&lt;/strong&gt; and &lt;strong&gt;c_0&lt;/strong&gt; default to zero.</source>
          <target state="translated">如果未提供 &lt;code&gt;(h_0, c_0)&lt;/code&gt; ，则&lt;strong&gt;h_0&lt;/strong&gt;和&lt;strong&gt;c_0均&lt;/strong&gt;默认为零。</target>
        </trans-unit>
        <trans-unit id="db361da536d20a314407fbca9d9b4b9534c18d89" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;Model&lt;/code&gt; is instantiated it will result in a compilation error since the compiler doesn&amp;rsquo;t know about &lt;code&gt;x&lt;/code&gt;. There are 4 ways to inform the compiler of attributes on &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">如果实例化 &lt;code&gt;Model&lt;/code&gt; ，则将导致编译错误，因为编译器不了解 &lt;code&gt;x&lt;/code&gt; 。有四种方法可以通知编译器&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;的属性：</target>
        </trans-unit>
        <trans-unit id="5fde3f46b9d751a46e85953fec7714aa67e2342e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;accumulate&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the elements in &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; are added to &lt;code&gt;self&lt;/code&gt;. If accumulate is &lt;code&gt;False&lt;/code&gt;, the behavior is undefined if indices contain duplicate elements.</source>
          <target state="translated">如果 &lt;code&gt;accumulate&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则将&lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt;中的元素添加到 &lt;code&gt;self&lt;/code&gt; 中。如果accumulate为 &lt;code&gt;False&lt;/code&gt; ，则在索引包含重复元素的情况下，行为是不确定的。</target>
        </trans-unit>
        <trans-unit id="2201c7c6a621b0e5c67fe66d2f987b652400f785" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;accumulate&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the elements in &lt;code&gt;value&lt;/code&gt; are added to &lt;code&gt;self&lt;/code&gt;. If accumulate is &lt;code&gt;False&lt;/code&gt;, the behavior is undefined if indices contain duplicate elements.</source>
          <target state="translated">如果 &lt;code&gt;accumulate&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则将 &lt;code&gt;value&lt;/code&gt; 中的元素添加到 &lt;code&gt;self&lt;/code&gt; 中。如果accumulate为 &lt;code&gt;False&lt;/code&gt; ，则在索引包含重复元素的情况下，行为是不确定的。</target>
        </trans-unit>
        <trans-unit id="989ae399047ba12cf5c5c1ad5cbe5e9842000a8a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;as_tuple&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, the output tensor containing indices. If &lt;code&gt;as_tuple&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, one 1-D tensor for each dimension, containing the indices of each nonzero element along that dimension.</source>
          <target state="translated">如果 &lt;code&gt;as_tuple&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; ，则包含索引的输出张量。如果 &lt;code&gt;as_tuple&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则每个维度一个一维张量，其中包含该维度上每个非零元素的索引。</target>
        </trans-unit>
        <trans-unit id="42d4ccb0a610720c59f54cf25fbe1940e041af3d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;batch1&lt;/code&gt; is a</source>
          <target state="translated">如果 &lt;code&gt;batch1&lt;/code&gt; 是一个</target>
        </trans-unit>
        <trans-unit id="f6f5f33da7723c89d0774922372d006d60d5610a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;beta&lt;/code&gt; is 0, then &lt;code&gt;input&lt;/code&gt; will be ignored, and &lt;code&gt;nan&lt;/code&gt; and &lt;code&gt;inf&lt;/code&gt; in it will not be propagated.</source>
          <target state="translated">如果 &lt;code&gt;beta&lt;/code&gt; 为0，则将忽略 &lt;code&gt;input&lt;/code&gt; ，并且不会传播其中的 &lt;code&gt;nan&lt;/code&gt; 和 &lt;code&gt;inf&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="dc51e352774290d1926f402524b258eb4db4275e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;center&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default), &lt;code&gt;input&lt;/code&gt; will be padded on both sides so that the</source>
          <target state="translated">如果 &lt;code&gt;center&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; （默认值），则将在两侧填充 &lt;code&gt;input&lt;/code&gt; 以便</target>
        </trans-unit>
        <trans-unit id="965719376b6d5fe07ada3da22176f7f46b7ed555" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;center&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then there will be padding e.g. &lt;code&gt;'constant'&lt;/code&gt;, &lt;code&gt;'reflect'&lt;/code&gt;, etc. Left padding can be trimmed off exactly because they can be calculated but right padding cannot be calculated without additional information.</source>
          <target state="translated">如果 &lt;code&gt;center&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，那么将存在填充，例如 &lt;code&gt;'constant'&lt;/code&gt; ， &lt;code&gt;'reflect'&lt;/code&gt; 等。可以精确地修剪左填充，因为可以计算出它们，但是如果没有其他信息，就不能计算出右填充。</target>
        </trans-unit>
        <trans-unit id="fc14ae1693b0f07d578cbfe76b09c86688e0df82" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;compute_uv&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, the returned &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; matrices will be zero matrices of shape</source>
          <target state="translated">如果 &lt;code&gt;compute_uv&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; ，则返回的 &lt;code&gt;U&lt;/code&gt; 和 &lt;code&gt;V&lt;/code&gt; 矩阵将是形状为零的矩阵</target>
        </trans-unit>
        <trans-unit id="8b121802d7ccc1f4651e3f4251de10bedc8ca4f7" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;create_graph=False&lt;/code&gt;, &lt;code&gt;backward()&lt;/code&gt; accumulates into &lt;code&gt;.grad&lt;/code&gt; in-place, which preserves its strides.</source>
          <target state="translated">如果 &lt;code&gt;create_graph=False&lt;/code&gt; ，则 &lt;code&gt;backward()&lt;/code&gt; 就地累积到 &lt;code&gt;.grad&lt;/code&gt; 中，这将保留其步幅。</target>
        </trans-unit>
        <trans-unit id="653e8379ac4a0083fba63e656d38b6380a7d066b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;create_graph=True&lt;/code&gt;, &lt;code&gt;backward()&lt;/code&gt; replaces &lt;code&gt;.grad&lt;/code&gt; with a new tensor &lt;code&gt;.grad + new grad&lt;/code&gt;, which attempts (but does not guarantee) matching the preexisting &lt;code&gt;.grad&lt;/code&gt;&amp;rsquo;s strides.</source>
          <target state="translated">如果 &lt;code&gt;create_graph=True&lt;/code&gt; ，则 &lt;code&gt;backward()&lt;/code&gt; 用新的张量 &lt;code&gt;.grad + new grad&lt;/code&gt; 替换 &lt;code&gt;.grad&lt;/code&gt; ，这会尝试（但不能保证）匹配先前存在的 &lt;code&gt;.grad&lt;/code&gt; 的步幅。</target>
        </trans-unit>
        <trans-unit id="65b33918f4984ada2bac2f4ee54a2728e16bb872" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;descending&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; then the elements are sorted in descending order by value.</source>
          <target state="translated">如果 &lt;code&gt;descending&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ,则元素将按值降序排列。</target>
        </trans-unit>
        <trans-unit id="f30e67b5addf429de66048f84b97eb2d56624113" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;dim&lt;/code&gt; is not given, it defaults to the first dimension found with the size 3. Note that this might be unexpected.</source>
          <target state="translated">如果未指定 &lt;code&gt;dim&lt;/code&gt; ，则默认为找到的第一个尺寸为3的尺寸。请注意，这可能是意外的。</target>
        </trans-unit>
        <trans-unit id="e52a79087756b4493bd3fe7986958f5739aaef82" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;dim&lt;/code&gt; is not given, the last dimension of the &lt;code&gt;input&lt;/code&gt; is chosen.</source>
          <target state="translated">如果未指定 &lt;code&gt;dim&lt;/code&gt; ，则选择 &lt;code&gt;input&lt;/code&gt; 的最后一个尺寸。</target>
        </trans-unit>
        <trans-unit id="ffe3df3fa3b821e3480cebbf887d0692dbdcfbb2" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;func&lt;/code&gt; is &lt;code&gt;nn.Module&lt;/code&gt; or &lt;code&gt;forward&lt;/code&gt; of &lt;code&gt;nn.Module&lt;/code&gt;, &lt;code&gt;trace&lt;/code&gt; returns a &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; object with a single &lt;code&gt;forward&lt;/code&gt; method containing the traced code. The returned &lt;code&gt;ScriptModule&lt;/code&gt; will have the same set of sub-modules and parameters as the original &lt;code&gt;nn.Module&lt;/code&gt;. If &lt;code&gt;func&lt;/code&gt; is a standalone function, &lt;code&gt;trace&lt;/code&gt; returns &lt;code&gt;ScriptFunction&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;func&lt;/code&gt; 是 &lt;code&gt;nn.Module&lt;/code&gt; 或 &lt;code&gt;forward&lt;/code&gt; 的 &lt;code&gt;nn.Module&lt;/code&gt; ， &lt;code&gt;trace&lt;/code&gt; 返回&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;与单个对象 &lt;code&gt;forward&lt;/code&gt; 包含跟踪代码方法。返回的 &lt;code&gt;ScriptModule&lt;/code&gt; 将具有与原始 &lt;code&gt;nn.Module&lt;/code&gt; 相同的子模块和参数集。如果 &lt;code&gt;func&lt;/code&gt; 是独立函数，则 &lt;code&gt;trace&lt;/code&gt; 返回 &lt;code&gt;ScriptFunction&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="12f27a06f15ef646d5a97006314eddc5168ac16a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;function&lt;/code&gt; invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won&amp;rsquo;t be equivalent, and unfortunately it can&amp;rsquo;t be detected.</source>
          <target state="translated">如果后退过程中的 &lt;code&gt;function&lt;/code&gt; 调用与前进过程中的函数调用没有任何区别，例如由于某个全局变量，则检查点版本将不等效，并且不幸的是，它无法被检测到。</target>
        </trans-unit>
        <trans-unit id="7fc30718c6826ba4f06aa25c6aae50a9ac61ea27" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;grid&lt;/code&gt; has values outside the range of &lt;code&gt;[-1, 1]&lt;/code&gt;, the corresponding outputs are handled as defined by &lt;code&gt;padding_mode&lt;/code&gt;. Options are</source>
          <target state="translated">如果 &lt;code&gt;grid&lt;/code&gt; 的值超出 &lt;code&gt;[-1, 1]&lt;/code&gt; 的范围，则按照 &lt;code&gt;padding_mode&lt;/code&gt; 的定义处理相应的输出。选项是</target>
        </trans-unit>
        <trans-unit id="56cdf52769f875d95bfc8ef6e8310ccf2d0ab6ef" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;hop_length&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; (default), it is treated as equal to &lt;code&gt;floor(n_fft / 4)&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;hop_length&lt;/code&gt; 为 &lt;code&gt;None&lt;/code&gt; （默认值），则将其视为 &lt;code&gt;floor(n_fft / 4)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="9a42f5f55bf046653a704f6db77ba0e3241e7e2f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; has</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 有</target>
        </trans-unit>
        <trans-unit id="b713735765f53d252a6982eaad4ca993659b98bf" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; has zero determinant, this returns &lt;code&gt;(0, -inf)&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 的行列式为零，则返回 &lt;code&gt;(0, -inf)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e500b7aaaa41be4614fc8206c1a0e4e07b1a2886" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is 1D of shape &lt;code&gt;(N)&lt;/code&gt;,</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 为形状 &lt;code&gt;(N)&lt;/code&gt; 的1D ，</target>
        </trans-unit>
        <trans-unit id="06376d4d271f3b9dcf5c2b5b33c61831ac5086a5" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is 2D of shape &lt;code&gt;(B, N)&lt;/code&gt;,</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 为 &lt;code&gt;(B, N)&lt;/code&gt; 2D形状，</target>
        </trans-unit>
        <trans-unit id="d721dee0fdf912ec6fe02d3c521f5fb3cf097977" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 是</target>
        </trans-unit>
        <trans-unit id="333f1219fe75814d72dd2dbd93db4f7d9396ef8a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a matrix (2-D tensor), then returns a 1-D tensor with the diagonal elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 是矩阵（2-D张量），则返回带有 &lt;code&gt;input&lt;/code&gt; 的对角线元素的1-D张量。</target>
        </trans-unit>
        <trans-unit id="63f74a4f9eae20a71d7f307933ef2802201be33d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a matrix with &lt;code&gt;m&lt;/code&gt; rows, &lt;code&gt;out&lt;/code&gt; is an matrix of shape</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 是 &lt;code&gt;m&lt;/code&gt; 行的矩阵，则 &lt;code&gt;out&lt;/code&gt; 是形状的矩阵</target>
        </trans-unit>
        <trans-unit id="27b6ec31f1f65b3c25640d5370bad846221e8b75" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a tensor with more than one dimension, then returns a 2-D tensor with diagonal elements equal to a flattened &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 是具有一个以上维的张量，则返回对角线元素等于flated &lt;code&gt;input&lt;/code&gt; 的2D张量。</target>
        </trans-unit>
        <trans-unit id="40b5681668244b650aeff8654a98b80d07f3df6a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a vector (1-D tensor), then returns a 2-D square tensor</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 是向量（一维张量），则返回二维平方张量</target>
        </trans-unit>
        <trans-unit id="35915a3eb4d935ad3d429e632e78a9274b0c797a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a vector (1-D tensor), then returns a 2-D square tensor with the elements of &lt;code&gt;input&lt;/code&gt; as the diagonal.</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 是向量（一维张量），则返回二维平方张量，其中 &lt;code&gt;input&lt;/code&gt; 元素为对角线。</target>
        </trans-unit>
        <trans-unit id="ad879c1e17f47008543d08a003ad2cbc09fcfa1b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is a vector, &lt;code&gt;out&lt;/code&gt; is a vector of size &lt;code&gt;num_samples&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 是一个向量，则 &lt;code&gt;out&lt;/code&gt; 是一个大小为 &lt;code&gt;num_samples&lt;/code&gt; 的向量。</target>
        </trans-unit>
        <trans-unit id="5b37263956a979da36ed328c86d1f8d5bafac1ea" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is an n-dimensional tensor with size</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 是具有尺寸的n维张量</target>
        </trans-unit>
        <trans-unit id="f77fab43f2855d468661a78ecf375280629b91c9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, &lt;code&gt;other&lt;/code&gt; should be a real number, otherwise it should be an integer</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 的类型为 &lt;code&gt;FloatTensor&lt;/code&gt; 或 &lt;code&gt;DoubleTensor&lt;/code&gt; ，则 &lt;code&gt;other&lt;/code&gt; 应该为实数，否则应为整数</target>
        </trans-unit>
        <trans-unit id="59214511303529b73d11d23823eff1c49a8f295c" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, &lt;code&gt;value&lt;/code&gt; should be a real number, otherwise it should be an integer.</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 的类型为 &lt;code&gt;FloatTensor&lt;/code&gt; 或 &lt;code&gt;DoubleTensor&lt;/code&gt; ，则 &lt;code&gt;value&lt;/code&gt; 应为实数，否则应为整数。</target>
        </trans-unit>
        <trans-unit id="7935f4c92d76a69f5c3018eeffd91bc6bda0cd48" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, args &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt; must be real numbers, otherwise they should be integers.</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 的类型为 &lt;code&gt;FloatTensor&lt;/code&gt; 或 &lt;code&gt;DoubleTensor&lt;/code&gt; ，则args &lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt;必须为实数，否则应为整数。</target>
        </trans-unit>
        <trans-unit id="74011e63fd6b8669cda7c1001070f1e13f3cf86f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;input&lt;/code&gt; is of type FloatTensor or DoubleTensor, &lt;code&gt;other&lt;/code&gt; must be a real number, otherwise it should be an integer.</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 的类型为FloatTensor或DoubleTensor，则 &lt;code&gt;other&lt;/code&gt; 必须为实数，否则应为整数。</target>
        </trans-unit>
        <trans-unit id="fd07eaa51527d175519a582a0f6f85c473a1308f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;is_python_module&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, returns the loaded PyTorch extension as a Python module. If &lt;code&gt;is_python_module&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; returns nothing (the shared library is loaded into the process as a side effect).</source>
          <target state="translated">如果 &lt;code&gt;is_python_module&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则将加载的PyTorch扩展作为Python模块返回。如果 &lt;code&gt;is_python_module&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; ,则不返回任何内容（共享库作为副作用加载到进程中）。</target>
        </trans-unit>
        <trans-unit id="150898b85fa40ffdbf63d6a8c5f8e7d9b416ba17" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim is ``True`&lt;/code&gt;, the output tensors are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension(s) &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim`s are squeezed (see :func:`torch.squeeze&lt;/code&gt;), resulting in the output tensors having fewer dimension than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;keepdim is ``True`&lt;/code&gt; ，则输出张量的大小与 &lt;code&gt;input&lt;/code&gt; 大小相同，只是尺寸为1的 &lt;code&gt;dim&lt;/code&gt; 的维数不同。否则， &lt;code&gt;dim`s are squeezed (see :func:`torch.squeeze&lt;/code&gt; ）导致输出张量的维数小于 &lt;code&gt;input&lt;/code&gt; 张量的维数。</target>
        </trans-unit>
        <trans-unit id="8dfb26e902cefe829de7730f64927f442131d727" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, both the &lt;code&gt;values&lt;/code&gt; and &lt;code&gt;indices&lt;/code&gt; tensors are the same size as &lt;code&gt;input&lt;/code&gt;, except in the dimension &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in both the &lt;code&gt;values&lt;/code&gt; and &lt;code&gt;indices&lt;/code&gt; tensors having 1 fewer dimension than the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">如果 &lt;code&gt;keepdim&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则 &lt;code&gt;values&lt;/code&gt; 和 &lt;code&gt;indices&lt;/code&gt; 张量都与 &lt;code&gt;input&lt;/code&gt; 相同，但尺寸 &lt;code&gt;dim&lt;/code&gt; 为1时除外。否则，将 &lt;code&gt;dim&lt;/code&gt; 压缩（请参阅&lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt;），从而得到 &lt;code&gt;values&lt;/code&gt; 和 &lt;code&gt;indices&lt;/code&gt; 张量的维数比 &lt;code&gt;input&lt;/code&gt; 张量少1 。</target>
        </trans-unit>
        <trans-unit id="4eef57842bb23530ec99ef24c142564e33a57efb" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output dimensions are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimensions being reduced (&lt;code&gt;dim&lt;/code&gt; or all if &lt;code&gt;dim&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;) where they have size 1. Otherwise, the dimensions being reduced are squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;). If &lt;code&gt;q&lt;/code&gt; is a 1D tensor, an extra dimension is prepended to the output tensor with the same size as &lt;code&gt;q&lt;/code&gt; which represents the quantiles.</source>
          <target state="translated">如果 &lt;code&gt;keepdim&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则输出尺寸与 &lt;code&gt;input&lt;/code&gt; 的尺寸相同，除了减小尺寸（ &lt;code&gt;dim&lt;/code&gt; 或如果 &lt;code&gt;dim&lt;/code&gt; 为 &lt;code&gt;None&lt;/code&gt; 时为全尺寸）时，其尺寸为1。否则，缩小的尺寸会受到挤压（请参见&lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt;）。如果 &lt;code&gt;q&lt;/code&gt; 是一维张量，则会在输出张量之前附加一个维，该维的大小与代表分位数的 &lt;code&gt;q&lt;/code&gt; 相同。</target>
        </trans-unit>
        <trans-unit id="8af1f9e4f1fca763580351e3fbd0001f54ae795e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensor is of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension &lt;code&gt;dim&lt;/code&gt; where it is of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;generated/torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in the output tensor having 1 fewer dimension than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;keepdim&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则输出张量的大小与 &lt;code&gt;input&lt;/code&gt; 大小相同，但尺寸为1的 &lt;code&gt;dim&lt;/code&gt; 除外。否则，将 &lt;code&gt;dim&lt;/code&gt; 压缩（请参见&lt;a href=&quot;generated/torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt;），从而使输出张量减少1个维度。比 &lt;code&gt;input&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="71fae5aef8701d9d46f893cd636d6b67ec5e6aab" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensor is of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension &lt;code&gt;dim&lt;/code&gt; where it is of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in the output tensor having 1 fewer dimension than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;keepdim&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则输出张量的大小与 &lt;code&gt;input&lt;/code&gt; 大小相同，但尺寸为1的 &lt;code&gt;dim&lt;/code&gt; 除外。否则，将 &lt;code&gt;dim&lt;/code&gt; 压缩（请参见&lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt;），从而使输出张量减少1个维度。比 &lt;code&gt;input&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5a9d71d5cbb1ac9891b1a744240b0f98bb2945c1" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensor is of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension(s) &lt;code&gt;dim&lt;/code&gt; where it is of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in the output tensor having 1 (or &lt;code&gt;len(dim)&lt;/code&gt;) fewer dimension(s).</source>
          <target state="translated">如果 &lt;code&gt;keepdim&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则输出张量的大小与 &lt;code&gt;input&lt;/code&gt; 大小相同，只是尺寸为1的 &lt;code&gt;dim&lt;/code&gt; 尺寸除外。否则，将 &lt;code&gt;dim&lt;/code&gt; 压缩（请参见&lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt;），从而使输出张量具有尺寸减少1（或 &lt;code&gt;len(dim)&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="7c2f11e03ffcd1caa4f43b25af60e0e5bd33899d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensors are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in the output tensors having 1 fewer dimension than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;keepdim&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则输出张量的大小与 &lt;code&gt;input&lt;/code&gt; 大小相同，但尺寸为1的 &lt;code&gt;dim&lt;/code&gt; 除外。否则，将挤压 &lt;code&gt;dim&lt;/code&gt; （请参见&lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt;），从而使输出张量的尺寸减少1个比 &lt;code&gt;input&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="21c9748f09f2547a22882ea8c179fcab7d38a94c" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensors are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim&lt;/code&gt; is squeezed (see &lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting in the outputs tensor having 1 fewer dimension than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;keepdim&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则输出张量的大小与 &lt;code&gt;input&lt;/code&gt; 大小相同，只是尺寸为1的 &lt;code&gt;dim&lt;/code&gt; 中的尺寸除外。否则，将 &lt;code&gt;dim&lt;/code&gt; 压缩（请参见&lt;a href=&quot;torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt;），从而使输出张量的尺寸减少1个比 &lt;code&gt;input&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="fd1cd26edf38a8fdee9967389f0e6dac29ec3c27" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;keepdim&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the output tensors are of the same size as &lt;code&gt;input&lt;/code&gt; except in the dimension(s) &lt;code&gt;dim&lt;/code&gt; where they are of size 1. Otherwise, &lt;code&gt;dim`s are squeezed (see :func:`torch.squeeze&lt;/code&gt;), resulting in the output tensors having fewer dimensions than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;keepdim&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则输出张量的大小与 &lt;code&gt;input&lt;/code&gt; 大小相同，只是在尺寸为1的 &lt;code&gt;dim&lt;/code&gt; 尺寸处。否则， &lt;code&gt;dim`s are squeezed (see :func:`torch.squeeze&lt;/code&gt; ），从而导致输出张量的尺寸小于 &lt;code&gt;input&lt;/code&gt; 张量。</target>
        </trans-unit>
        <trans-unit id="a5f40df338bf2f9f730257d36666627ccd6548fc" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;dilation&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt; or &lt;code&gt;stride&lt;/code&gt; is an int or a tuple of length 1, their values will be replicated across all spatial dimensions.</source>
          <target state="translated">如果 &lt;code&gt;kernel_size&lt;/code&gt; ， &lt;code&gt;dilation&lt;/code&gt; ， &lt;code&gt;padding&lt;/code&gt; 或 &lt;code&gt;stride&lt;/code&gt; 是长度为1的int或元组，则它们的值将在所有空间维度上复制。</target>
        </trans-unit>
        <trans-unit id="55b84dbfa8c62b3f0cac777d20c854c0409394ae" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;largest&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt; then the &lt;code&gt;k&lt;/code&gt; smallest elements are returned.</source>
          <target state="translated">如果 &lt;code&gt;largest&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; ,则返回 &lt;code&gt;k&lt;/code&gt; 个最小元素。</target>
        </trans-unit>
        <trans-unit id="fa798f0a6b2f55ce7a633ab807b16e5f24185685" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;map_location&lt;/code&gt; is a &lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; object or a string containing a device tag, it indicates the location where all tensors should be loaded.</source>
          <target state="translated">如果 &lt;code&gt;map_location&lt;/code&gt; 是&lt;a href=&quot;../tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;对象或包含设备标签的字符串，则它指示应加载所有张量的位置。</target>
        </trans-unit>
        <trans-unit id="048f95b6581ac17eb4c969ed9d078d249d78f4f2" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;map_location&lt;/code&gt; is a callable, it will be called once for each serialized storage with two arguments: storage and location. The storage argument will be the initial deserialization of the storage, residing on the CPU. Each serialized storage has a location tag associated with it which identifies the device it was saved from, and this tag is the second argument passed to &lt;code&gt;map_location&lt;/code&gt;. The builtin location tags are &lt;code&gt;'cpu'&lt;/code&gt; for CPU tensors and &lt;code&gt;'cuda:device_id'&lt;/code&gt; (e.g. &lt;code&gt;'cuda:2'&lt;/code&gt;) for CUDA tensors. &lt;code&gt;map_location&lt;/code&gt; should return either &lt;code&gt;None&lt;/code&gt; or a storage. If &lt;code&gt;map_location&lt;/code&gt; returns a storage, it will be used as the final deserialized object, already moved to the right device. Otherwise, &lt;a href=&quot;#torch.load&quot;&gt;&lt;code&gt;torch.load()&lt;/code&gt;&lt;/a&gt; will fall back to the default behavior, as if &lt;code&gt;map_location&lt;/code&gt; wasn&amp;rsquo;t specified.</source>
          <target state="translated">如果 &lt;code&gt;map_location&lt;/code&gt; 是可调用的，则将为每个序列化存储调用一次，并带有两个参数：storage和location。storage参数将是驻留在CPU上的存储的初始反序列化。每个序列化存储都有一个与之相关联的位置标签，该标签标识了从中进行保存的设备，该标签是传递给 &lt;code&gt;map_location&lt;/code&gt; 的第二个参数。内置的位置标记对于CPU张量为 &lt;code&gt;'cpu'&lt;/code&gt; ，对于CUDA张量为 &lt;code&gt;'cuda:device_id'&lt;/code&gt; （例如 &lt;code&gt;'cuda:2'&lt;/code&gt; ）。 &lt;code&gt;map_location&lt;/code&gt; 应该返回 &lt;code&gt;None&lt;/code&gt; 或存储。如果 &lt;code&gt;map_location&lt;/code&gt; 返回一个存储，它将用作最终反序列化的对象，该对象已移至正确的设备。否则，&lt;a href=&quot;#torch.load&quot;&gt; &lt;code&gt;torch.load()&lt;/code&gt; &lt;/a&gt;将退回到默认行为，就像未指定 &lt;code&gt;map_location&lt;/code&gt; 一样。</target>
        </trans-unit>
        <trans-unit id="0e6bc544f9847053786078b7293a8d56bcd4734d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;mat1&lt;/code&gt; is a</source>
          <target state="translated">如果 &lt;code&gt;mat1&lt;/code&gt; 是一个</target>
        </trans-unit>
        <trans-unit id="42dc8bdc5d9263d6983656748538a841cde8189e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;mat&lt;/code&gt; is a</source>
          <target state="translated">如果 &lt;code&gt;mat&lt;/code&gt; 是一个</target>
        </trans-unit>
        <trans-unit id="ebf39946f09dcba3bcd6988a72fb252101844e11" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;modules&lt;/code&gt; is an &lt;code&gt;OrderedDict&lt;/code&gt;, a &lt;a href=&quot;#torch.nn.ModuleDict&quot;&gt;&lt;code&gt;ModuleDict&lt;/code&gt;&lt;/a&gt;, or an iterable of key-value pairs, the order of new elements in it is preserved.</source>
          <target state="translated">如果 &lt;code&gt;modules&lt;/code&gt; 是 &lt;code&gt;OrderedDict&lt;/code&gt; ，&lt;a href=&quot;#torch.nn.ModuleDict&quot;&gt; &lt;code&gt;ModuleDict&lt;/code&gt; &lt;/a&gt;或键值对的可迭代对象，则将保留其中的新元素顺序。</target>
        </trans-unit>
        <trans-unit id="9da2da81b60822905c169e5c07b984ccee82a21a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n&lt;/code&gt; is negative, then the inverse of the matrix (if invertible) is raised to the power &lt;code&gt;n&lt;/code&gt;. For a batch of matrices, the batched inverse (if invertible) is raised to the power &lt;code&gt;n&lt;/code&gt;. If &lt;code&gt;n&lt;/code&gt; is 0, then an identity matrix is returned.</source>
          <target state="translated">如果 &lt;code&gt;n&lt;/code&gt; 为负，则矩阵的逆（如果是可逆的）提高到幂 &lt;code&gt;n&lt;/code&gt; 。对于一批矩阵，将成批的逆（如果可逆）提高到幂 &lt;code&gt;n&lt;/code&gt; 。如果 &lt;code&gt;n&lt;/code&gt; 为0，则返回一个单位矩阵。</target>
        </trans-unit>
        <trans-unit id="7a5b0d2161214cfbfdadaec7bb8185cf215fb328" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n&lt;/code&gt; is the number of dimensions in &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;x.T&lt;/code&gt; is equivalent to &lt;code&gt;x.permute(n-1, n-2, ..., 0)&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;n&lt;/code&gt; 是 &lt;code&gt;x&lt;/code&gt; 中的维数，则 &lt;code&gt;x.T&lt;/code&gt; 等于 &lt;code&gt;x.permute(n-1, n-2, ..., 0)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="64021abf601b2ef80dc11cb82f4cc12a6574725a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;nonlinearity&lt;/code&gt; is &lt;code&gt;&amp;lsquo;relu&amp;rsquo;&lt;/code&gt;, then ReLU is used in place of tanh.</source>
          <target state="translated">如果 &lt;code&gt;nonlinearity&lt;/code&gt; 为 &lt;code&gt;&amp;lsquo;relu&amp;rsquo;&lt;/code&gt; ，则使用ReLU代替tanh。</target>
        </trans-unit>
        <trans-unit id="14d766061a3de2fff996b1f5b1787217fd8c9c07" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalized&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default is &lt;code&gt;False&lt;/code&gt;), the function returns the normalized STFT results, i.e., multiplied by</source>
          <target state="translated">如果 &lt;code&gt;normalized&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; （默认为 &lt;code&gt;False&lt;/code&gt; ），则该函数返回归一化的STFT结果，即乘以</target>
        </trans-unit>
        <trans-unit id="00b342cfeb1bc29dc26ae492fe4fb871890e749d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;obj&lt;/code&gt; is &lt;code&gt;nn.Module&lt;/code&gt;, &lt;code&gt;script&lt;/code&gt; returns a &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; object. The returned &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; will have the same set of sub-modules and parameters as the original &lt;code&gt;nn.Module&lt;/code&gt;. If &lt;code&gt;obj&lt;/code&gt; is a standalone function, a &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; will be returned.</source>
          <target state="translated">如果 &lt;code&gt;obj&lt;/code&gt; 是 &lt;code&gt;nn.Module&lt;/code&gt; ，则 &lt;code&gt;script&lt;/code&gt; 返回&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;对象。返回的&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;将具有与原始 &lt;code&gt;nn.Module&lt;/code&gt; 相同的子模块和参数集。如果 &lt;code&gt;obj&lt;/code&gt; 是独立函数，则将返回&lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="a63f6cedcd0f8aaa3b09629c749bc8a65b8e3242" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;offset&lt;/code&gt; &amp;gt; 0, it is above the main diagonal.</source>
          <target state="translated">如果 &lt;code&gt;offset&lt;/code&gt; &amp;gt; 0，则它在主对角线上方。</target>
        </trans-unit>
        <trans-unit id="71789ffff3401f164bb9ff6ccd173e3d93708807" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;offset&lt;/code&gt; &amp;lt; 0, it is below the main diagonal.</source>
          <target state="translated">如果 &lt;code&gt;offset&lt;/code&gt; &amp;lt;0，则它在主对角线以下。</target>
        </trans-unit>
        <trans-unit id="68479729055a8e84ce055c5fd8fcdf5fc489269d" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;offset&lt;/code&gt; = 0, it is the main diagonal.</source>
          <target state="translated">如果 &lt;code&gt;offset&lt;/code&gt; = 0，则它是主对角线。</target>
        </trans-unit>
        <trans-unit id="4a6e80bcb7f10675f2ca57bde51217a2a4acefff" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;onesided&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default for real input), only values for</source>
          <target state="translated">如果 &lt;code&gt;onesided&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; （真实输入的默认值），则仅值</target>
        </trans-unit>
        <trans-unit id="d02abfd63ad40eea8ad8a0098542cd40d9fd90fd" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;only_inputs&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the function will only return a list of gradients w.r.t the specified inputs. If it&amp;rsquo;s &lt;code&gt;False&lt;/code&gt;, then gradient w.r.t. all remaining leaves will still be computed, and will be accumulated into their &lt;code&gt;.grad&lt;/code&gt; attribute.</source>
          <target state="translated">如果 &lt;code&gt;only_inputs&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则该函数将仅返回带有指定输入的渐变列表。如果它为 &lt;code&gt;False&lt;/code&gt; ，则仍将计算所有剩余叶子的梯度，并将其累积到其 &lt;code&gt;.grad&lt;/code&gt; 属性中。</target>
        </trans-unit>
        <trans-unit id="d55a2e244595c07c3e007024e25f845d2373549a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;other&lt;/code&gt; is of type FloatTensor or DoubleTensor, &lt;code&gt;alpha&lt;/code&gt; must be a real number, otherwise it should be an integer.</source>
          <target state="translated">如果 &lt;code&gt;other&lt;/code&gt; 为FloatTensor或DoubleTensor类型，则 &lt;code&gt;alpha&lt;/code&gt; 必须为实数，否则应为整数。</target>
        </trans-unit>
        <trans-unit id="d24ea9f71cf8ff7e189e7b233b073ed7e74bfb09" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;output_size&lt;/code&gt;, &lt;code&gt;kernel_size&lt;/code&gt;, &lt;code&gt;dilation&lt;/code&gt;, &lt;code&gt;padding&lt;/code&gt; or &lt;code&gt;stride&lt;/code&gt; is an int or a tuple of length 1 then their values will be replicated across all spatial dimensions.</source>
          <target state="translated">如果 &lt;code&gt;output_size&lt;/code&gt; ， &lt;code&gt;kernel_size&lt;/code&gt; ， &lt;code&gt;dilation&lt;/code&gt; ， &lt;code&gt;padding&lt;/code&gt; 或 &lt;code&gt;stride&lt;/code&gt; 是长度为1的int或元组，则它们的值将在所有空间维度上复制。</target>
        </trans-unit>
        <trans-unit id="bcbbd502f1fedcc0223634c589add98b568f5531" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;padding&lt;/code&gt; is non-zero, then the input is implicitly padded with negative infinity on both sides for &lt;code&gt;padding&lt;/code&gt; number of points. &lt;code&gt;dilation&lt;/code&gt; is the stride between the elements within the sliding window. This &lt;a href=&quot;https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md&quot;&gt;link&lt;/a&gt; has a nice visualization of the pooling parameters.</source>
          <target state="translated">如果 &lt;code&gt;padding&lt;/code&gt; 为非零值，则在输入的两边都用负无穷大隐式填充，以 &lt;code&gt;padding&lt;/code&gt; 点数。 &lt;code&gt;dilation&lt;/code&gt; 是滑动窗口内元素之间的跨度。该&lt;a href=&quot;https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md&quot;&gt;链接&lt;/a&gt;很好地展示了池化参数。</target>
        </trans-unit>
        <trans-unit id="a2d29b6dc8926940187a1fc2a87fbebaeec5e10f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;padding&lt;/code&gt; is non-zero, then the input is implicitly zero-padded on all three sides for &lt;code&gt;padding&lt;/code&gt; number of points.</source>
          <target state="translated">如果 &lt;code&gt;padding&lt;/code&gt; 为非零值，则输入的所有三侧都将被隐式填充为零，以 &lt;code&gt;padding&lt;/code&gt; 点数。</target>
        </trans-unit>
        <trans-unit id="a34bc02f3af80858de013a28d3f5075d29b733ee" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;padding&lt;/code&gt; is non-zero, then the input is implicitly zero-padded on both sides for &lt;code&gt;padding&lt;/code&gt; number of points.</source>
          <target state="translated">如果 &lt;code&gt;padding&lt;/code&gt; 不为零，则对于点的 &lt;code&gt;padding&lt;/code&gt; 数量，输入都将在两侧隐式地进行零填充。</target>
        </trans-unit>
        <trans-unit id="0789404550f41a9218f1f8f6d2d8802206a399d3" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;padding&lt;/code&gt; is non-zero, then the input is implicitly zero-padded on both sides for &lt;code&gt;padding&lt;/code&gt; number of points. &lt;code&gt;dilation&lt;/code&gt; controls the spacing between the kernel points. It is harder to describe, but this &lt;a href=&quot;https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md&quot;&gt;link&lt;/a&gt; has a nice visualization of what &lt;code&gt;dilation&lt;/code&gt; does.</source>
          <target state="translated">如果 &lt;code&gt;padding&lt;/code&gt; 不为零，则对于点的 &lt;code&gt;padding&lt;/code&gt; 数量，输入都将在两侧隐式地进行零填充。 &lt;code&gt;dilation&lt;/code&gt; 控制内核点之间的间距。很难描述，但是此&lt;a href=&quot;https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md&quot;&gt;链接&lt;/a&gt;很好地展示了 &lt;code&gt;dilation&lt;/code&gt; 作用。</target>
        </trans-unit>
        <trans-unit id="3fe50649c0b4459ddadacb46d032eecac188eedd" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;param.grad&lt;/code&gt; is initially &lt;code&gt;None&lt;/code&gt;:</source>
          <target state="translated">如果 &lt;code&gt;param.grad&lt;/code&gt; 最初为 &lt;code&gt;None&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="c202579e5b79a5ca47cce55bf1764d6590abf4d9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;param&lt;/code&gt; already has a non-sparse &lt;code&gt;.grad&lt;/code&gt; attribute:</source>
          <target state="translated">如果 &lt;code&gt;param&lt;/code&gt; 已经具有非稀疏的 &lt;code&gt;.grad&lt;/code&gt; 属性：</target>
        </trans-unit>
        <trans-unit id="b85948d2056bf712cd0ade9dc5602720a12d6ab2" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;param&lt;/code&gt;&amp;rsquo;s memory is non-overlapping and dense, &lt;code&gt;.grad&lt;/code&gt; is created with strides matching &lt;code&gt;param&lt;/code&gt; (thus matching &lt;code&gt;param&lt;/code&gt;&amp;rsquo;s layout).</source>
          <target state="translated">如果 &lt;code&gt;param&lt;/code&gt; 的内存不重叠且密集， &lt;code&gt;.grad&lt;/code&gt; 创建与步长匹配的 &lt;code&gt;param&lt;/code&gt; （因此匹配 &lt;code&gt;param&lt;/code&gt; 的布局）的.grad。</target>
        </trans-unit>
        <trans-unit id="5c75f70ae17fab5880e35d5beec80b653312b036" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;parameters&lt;/code&gt; is an &lt;code&gt;OrderedDict&lt;/code&gt;, a &lt;a href=&quot;#torch.nn.ParameterDict&quot;&gt;&lt;code&gt;ParameterDict&lt;/code&gt;&lt;/a&gt;, or an iterable of key-value pairs, the order of new elements in it is preserved.</source>
          <target state="translated">如果 &lt;code&gt;parameters&lt;/code&gt; 是 &lt;code&gt;OrderedDict&lt;/code&gt; ，&lt;a href=&quot;#torch.nn.ParameterDict&quot;&gt; &lt;code&gt;ParameterDict&lt;/code&gt; &lt;/a&gt;或键值对的可迭代对象，则将保留其中的新元素的顺序。</target>
        </trans-unit>
        <trans-unit id="fa4de2a8b20f68604caa5fc5c42d01d653f7619e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;reduction&lt;/code&gt; is not &lt;code&gt;'none'&lt;/code&gt; (default &lt;code&gt;'mean'&lt;/code&gt;), then:</source>
          <target state="translated">如果 &lt;code&gt;reduction&lt;/code&gt; 不是 &lt;code&gt;'none'&lt;/code&gt; （默认是 &lt;code&gt;'mean'&lt;/code&gt; ），则：</target>
        </trans-unit>
        <trans-unit id="e3f6cc94ec59a0b57a35c18da0e38bada6245d56" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;return_complex&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default if input is complex), the return is a &lt;code&gt;input.dim() + 1&lt;/code&gt; dimensional complex tensor. If &lt;code&gt;False&lt;/code&gt;, the output is a &lt;code&gt;input.dim() + 2&lt;/code&gt; dimensional real tensor where the last dimension represents the real and imaginary components.</source>
          <target state="translated">如果 &lt;code&gt;return_complex&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; （如果输入为复数则为默认值），则返回值为 &lt;code&gt;input.dim() + 1&lt;/code&gt; 维复数张量。如果为 &lt;code&gt;False&lt;/code&gt; ，则输出为 &lt;code&gt;input.dim() + 2&lt;/code&gt; 二维实数张量，其中最后一个维表示实部和虚部。</target>
        </trans-unit>
        <trans-unit id="f865281b07a3f9b21462c504abaff9fa28e7b147" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;self.cycle_momentum&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, this function has a side effect of updating the optimizer&amp;rsquo;s momentum.</source>
          <target state="translated">如果 &lt;code&gt;self.cycle_momentum&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则此函数具有更新优化器动量的副作用。</target>
        </trans-unit>
        <trans-unit id="86cbdc2f08a338f85c6f8af902eddf736f286a0f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;self&lt;/code&gt; is a sparse COO tensor (i.e., with &lt;code&gt;torch.sparse_coo&lt;/code&gt; layout), this returns a view of the contained indices tensor. Otherwise, this throws an error.</source>
          <target state="translated">如果 &lt;code&gt;self&lt;/code&gt; 是一个稀疏的COO张量（即，使用 &lt;code&gt;torch.sparse_coo&lt;/code&gt; 布局），则返回包含的索引张量的视图。否则，将引发错误。</target>
        </trans-unit>
        <trans-unit id="1a38a9c9f965f62380fce875a97d72dc47659bfb" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;self&lt;/code&gt; is a sparse COO tensor (i.e., with &lt;code&gt;torch.sparse_coo&lt;/code&gt; layout), this returns a view of the contained values tensor. Otherwise, this throws an error.</source>
          <target state="translated">如果 &lt;code&gt;self&lt;/code&gt; 是一个稀疏的COO张量（即，使用 &lt;code&gt;torch.sparse_coo&lt;/code&gt; 布局），则将返回包含的值张量的视图。否则，将引发错误。</target>
        </trans-unit>
        <trans-unit id="95842db2195edf44c14e3fcd797605bf3a42bb61" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;self&lt;/code&gt; is a sparse COO tensor (i.e., with &lt;code&gt;torch.sparse_coo&lt;/code&gt; layout), this returns the number of dense dimensions. Otherwise, this throws an error.</source>
          <target state="translated">如果 &lt;code&gt;self&lt;/code&gt; 是一个稀疏的COO张量（即，使用 &lt;code&gt;torch.sparse_coo&lt;/code&gt; 布局），则返回密集尺寸的数量。否则，将引发错误。</target>
        </trans-unit>
        <trans-unit id="0bcf04d576bbe19078f6b28ab84fc731b5408840" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;self&lt;/code&gt; is a sparse COO tensor (i.e., with &lt;code&gt;torch.sparse_coo&lt;/code&gt; layout), this returns the number of sparse dimensions. Otherwise, this throws an error.</source>
          <target state="translated">如果 &lt;code&gt;self&lt;/code&gt; 是一个稀疏的COO张量（即，使用 &lt;code&gt;torch.sparse_coo&lt;/code&gt; 布局），则返回稀疏维数。否则，将引发错误。</target>
        </trans-unit>
        <trans-unit id="ca7f952167ad5a99c7190c4496d9c80f7e0a4624" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;shared&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then memory is shared between all processes. All changes are written to the file. If &lt;code&gt;shared&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then the changes on the storage do not affect the file.</source>
          <target state="translated">如果 &lt;code&gt;shared&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则内存在所有进程之间共享。所有更改都将写入文件。如果 &lt;code&gt;shared&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; ，则存储上的更改不会影响该文件。</target>
        </trans-unit>
        <trans-unit id="015e3e2de9fc9120162aae17d21cac0467229fd9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;sizedim&lt;/code&gt; is the size of dimension &lt;code&gt;dimension&lt;/code&gt; for &lt;code&gt;self&lt;/code&gt;, the size of dimension &lt;code&gt;dimension&lt;/code&gt; in the returned tensor will be &lt;code&gt;(sizedim - size) / step + 1&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;sizedim&lt;/code&gt; 是维度的大小 &lt;code&gt;dimension&lt;/code&gt; 为 &lt;code&gt;self&lt;/code&gt; ，尺寸的大小 &lt;code&gt;dimension&lt;/code&gt; 在返回的张量将是 &lt;code&gt;(sizedim - size) / step + 1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e63fb825587df089e2f9c8a5d5a13bc6f9e0bcfa" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;some&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; (default), the method returns the reduced singular value decomposition i.e., if the last two dimensions of &lt;code&gt;input&lt;/code&gt; are &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;n&lt;/code&gt;, then the returned &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; matrices will contain only</source>
          <target state="translated">如果 &lt;code&gt;some&lt;/code&gt; 是 &lt;code&gt;True&lt;/code&gt; （默认值），则该方法返回降低奇异值分解，即，如果最后两个维度 &lt;code&gt;input&lt;/code&gt; 是 &lt;code&gt;m&lt;/code&gt; 和 &lt;code&gt;n&lt;/code&gt; ，则返回的 &lt;code&gt;U&lt;/code&gt; 和 &lt;code&gt;V&lt;/code&gt; 矩阵将只包含</target>
        </trans-unit>
        <trans-unit id="7e496e3c20e80698b4576d97cd438ec7eb85163b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;some&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then this function returns the thin (reduced) QR factorization. Otherwise, if &lt;code&gt;some&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, this function returns the complete QR factorization.</source>
          <target state="translated">如果 &lt;code&gt;some&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则此函数返回瘦（​​减少的）QR因式分解。否则，如果 &lt;code&gt;some&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; ，则此函数返回完整的QR因式分解。</target>
        </trans-unit>
        <trans-unit id="1ff1fe08ba98d042300c393b324450ec3743e612" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;source&lt;/code&gt; is &lt;code&gt;'github'&lt;/code&gt;, &lt;code&gt;repo_or_dir&lt;/code&gt; is expected to be of the form &lt;code&gt;repo_owner/repo_name[:tag_name]&lt;/code&gt; with an optional tag/branch.</source>
          <target state="translated">如果 &lt;code&gt;source&lt;/code&gt; 是 &lt;code&gt;'github'&lt;/code&gt; ，则 &lt;code&gt;repo_or_dir&lt;/code&gt; 的形式应为 &lt;code&gt;repo_owner/repo_name[:tag_name]&lt;/code&gt; 并带有可选的标记/分支。</target>
        </trans-unit>
        <trans-unit id="7ddb98294e27497103be69303e2d273125226255" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;source&lt;/code&gt; is &lt;code&gt;'local'&lt;/code&gt;, &lt;code&gt;repo_or_dir&lt;/code&gt; is expected to be a path to a local directory.</source>
          <target state="translated">如果 &lt;code&gt;source&lt;/code&gt; 是 &lt;code&gt;'local'&lt;/code&gt; ，则 &lt;code&gt;repo_or_dir&lt;/code&gt; 应该是本地目录的路径。</target>
        </trans-unit>
        <trans-unit id="8e46fd64e2d63e21f45b5d33db7806f0957cdf7b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;source&lt;/code&gt; is a &lt;code&gt;Storage&lt;/code&gt;, the method sets the underlying storage, offset, size, and stride.</source>
          <target state="translated">如果 &lt;code&gt;source&lt;/code&gt; 是 &lt;code&gt;Storage&lt;/code&gt; ，则该方法设置基础存储，偏移量，大小和跨度。</target>
        </trans-unit>
        <trans-unit id="cde3a8bb0ace5c85908b26369901162f2c89a71e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;split_size_or_sections&lt;/code&gt; is a list, then &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; will be split into &lt;code&gt;len(split_size_or_sections)&lt;/code&gt; chunks with sizes in &lt;code&gt;dim&lt;/code&gt; according to &lt;code&gt;split_size_or_sections&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;split_size_or_sections&lt;/code&gt; 是一个列表，然后&lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt;将被分成 &lt;code&gt;len(split_size_or_sections)&lt;/code&gt; 块与尺寸在 &lt;code&gt;dim&lt;/code&gt; 根据 &lt;code&gt;split_size_or_sections&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="a130a738825270ea8d38803841375a69a70f4847" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;split_size_or_sections&lt;/code&gt; is an integer type, then &lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension &lt;code&gt;dim&lt;/code&gt; is not divisible by &lt;code&gt;split_size&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;split_size_or_sections&lt;/code&gt; 是整数类型，则&lt;a href=&quot;torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt;将被分为相等大小的块（如果可能）。如果沿着给定维度 &lt;code&gt;dim&lt;/code&gt; 的张量大小不能被 &lt;code&gt;split_size&lt;/code&gt; 整除，则最后一个块将更小。</target>
        </trans-unit>
        <trans-unit id="cfc288bc3d95c2b0c35b45e151245ce8a50c4585" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;track_running_stats&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt;, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.</source>
          <target state="translated">如果 &lt;code&gt;track_running_stats&lt;/code&gt; 设置为 &lt;code&gt;False&lt;/code&gt; ，那么此层将不保留运行估计，而是在评估期间也使用批处理统计信息。</target>
        </trans-unit>
        <trans-unit id="12927d84c9ce948d1b220f95a6bfac95c27d8d68" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;track_running_stats&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default &lt;code&gt;momentum&lt;/code&gt; of 0.1.</source>
          <target state="translated">如果 &lt;code&gt;track_running_stats&lt;/code&gt; 设置为 &lt;code&gt;True&lt;/code&gt; ，则在训练过程中该层将继续对其计算的均值和方差进行连续估算，然后将其用于评估期间的标准化。保持运行估算，默认 &lt;code&gt;momentum&lt;/code&gt; 为0.1。</target>
        </trans-unit>
        <trans-unit id="07322ca8d48ec2924a49a2e5be17cc42cbbed920" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;tracker&lt;/code&gt; sets &lt;code&gt;bvars[&amp;ldquo;force_stop&amp;rdquo;] = True&lt;/code&gt;, the iteration process will be hard-stopped.</source>
          <target state="translated">如果 &lt;code&gt;tracker&lt;/code&gt; 将 &lt;code&gt;bvars[&amp;ldquo;force_stop&amp;rdquo;] = True&lt;/code&gt; ，则迭代过程将被严格停止。</target>
        </trans-unit>
        <trans-unit id="69139fd88bd2f170758129dcebe723655bc85791" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;unbiased&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel&amp;rsquo;s correction will be used.</source>
          <target state="translated">如果 &lt;code&gt;unbiased&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; ，则将通过有偏估计量计算标准差。否则，将使用贝塞尔校正。</target>
        </trans-unit>
        <trans-unit id="0054a72bc16c599350023586c7a4c1a0ef3cdabb" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;unbiased&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then the variance will be calculated via the biased estimator. Otherwise, Bessel&amp;rsquo;s correction will be used.</source>
          <target state="translated">如果 &lt;code&gt;unbiased&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; ，则将通过有偏估计量计算方差。否则，将使用贝塞尔校正。</target>
        </trans-unit>
        <trans-unit id="64c7abce64d159f698a9f17e0c36ec9abe8f3696" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;,</source>
          <target state="translated">如果 &lt;code&gt;upper&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; ，</target>
        </trans-unit>
        <trans-unit id="ea8581ad2e15c4757c07b341a196b7e3beb08404" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, the returned matrix &lt;code&gt;L&lt;/code&gt; is lower-triangular, and the decomposition has the form:</source>
          <target state="translated">如果 &lt;code&gt;upper&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; ，则返回的矩阵 &lt;code&gt;L&lt;/code&gt; 为下三角，分解形式为：</target>
        </trans-unit>
        <trans-unit id="64a08919bf7ea0ada24f0d42a92ac1bba95e2ff7" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, then lower triangular portion is used.</source>
          <target state="translated">如果 &lt;code&gt;upper&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; ，则使用下部三角形部分。</target>
        </trans-unit>
        <trans-unit id="16f312504df9d82f1149f9aeb9d6c3a7744b7863" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; or not provided,</source>
          <target state="translated">如果 &lt;code&gt;upper&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; 或未提供，</target>
        </trans-unit>
        <trans-unit id="0de37aa5dde3c55904d77c2e915431e5555caf52" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, and</source>
          <target state="translated">如果 &lt;code&gt;upper&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，并且</target>
        </trans-unit>
        <trans-unit id="d54780fe04a147f268bb2ed2fa8ab8ca14bbc641" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;upper&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the returned matrix &lt;code&gt;U&lt;/code&gt; is upper-triangular, and the decomposition has the form:</source>
          <target state="translated">如果 &lt;code&gt;upper&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，则返回的矩阵 &lt;code&gt;U&lt;/code&gt; 为上三角，分解形式为：</target>
        </trans-unit>
        <trans-unit id="f0b760bb14c0f3a83b45512f0cb97f37a555bad1" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;vec1&lt;/code&gt; is a vector of size &lt;code&gt;n&lt;/code&gt; and &lt;code&gt;vec2&lt;/code&gt; is a vector of size &lt;code&gt;m&lt;/code&gt;, then &lt;code&gt;input&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with a matrix of size</source>
          <target state="translated">如果 &lt;code&gt;vec1&lt;/code&gt; 是大小为 &lt;code&gt;n&lt;/code&gt; 的向量，而 &lt;code&gt;vec2&lt;/code&gt; 是大小为 &lt;code&gt;m&lt;/code&gt; 的向量，则 &lt;code&gt;input&lt;/code&gt; 必须是可&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;广播&lt;/a&gt;的，大小为矩阵</target>
        </trans-unit>
        <trans-unit id="3a620298db8d1a210d3df0072bbfa22f1c6b3dc1" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;win_length&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; (default), it is treated as equal to &lt;code&gt;n_fft&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;win_length&lt;/code&gt; 为 &lt;code&gt;None&lt;/code&gt; （默认值），则将其视为等于 &lt;code&gt;n_fft&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e41ff352c71d6d3681ae49796d3eaff91163d87a" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;window_length&lt;/code&gt;</source>
          <target state="translated">如果 &lt;code&gt;window_length&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="5ba4029b8d23bd264ef58b8c6c6f2f2dbcc50773" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;window_length&lt;/code&gt; is one, then the returned window is a single element tensor containing a one.</source>
          <target state="translated">如果 &lt;code&gt;window_length&lt;/code&gt; 为1，则返回的窗口是包含1的单个元素张量。</target>
        </trans-unit>
        <trans-unit id="36700c2c10655c29379465dfe1fd6e89caba46f9" translate="yes" xml:space="preserve">
          <source>If Statements</source>
          <target state="translated">如果声明</target>
        </trans-unit>
        <trans-unit id="37f71555dd25e508495629f5b40797b269bbe017" translate="yes" xml:space="preserve">
          <source>If True, all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph. If False, then initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs.</source>
          <target state="translated">如果为真,则导出图形中的所有初始化器(通常对应于参数)也将作为输入添加到图形中。如果为False,则初始化器不作为输入添加到图形中,只将非参数输入作为输入添加。</target>
        </trans-unit>
        <trans-unit id="a286c308254a9cdf1bc57c2c79179a955b02516d" translate="yes" xml:space="preserve">
          <source>If a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension which is expected to be of that specific size.</source>
          <target state="translated">如果使用单个整数,则将其作为单子列表处理,该模块将在最后一个维度上进行归一化处理,预计该维度为该特定大小。</target>
        </trans-unit>
        <trans-unit id="6cc6559de482ee416e97f4d6264cba5f02719534" translate="yes" xml:space="preserve">
          <source>If a zero-dimension tensor operand has a higher category than dimensioned operands, we promote to a type with sufficient size and category to hold all zero-dim tensor operands of that category.</source>
          <target state="translated">如果一个零维张量操作数比维操作数有更高的类别,我们就会推广到一个具有足够大小和类别的类型,以容纳该类别的所有零维张量操作数。</target>
        </trans-unit>
        <trans-unit id="4cbc07c033e4f3f50b5cdb0467c64644f527b6f8" translate="yes" xml:space="preserve">
          <source>If an op is unlisted, we assume it&amp;rsquo;s numerically stable in &lt;code&gt;float16&lt;/code&gt;. If you believe an unlisted op is numerically unstable in &lt;code&gt;float16&lt;/code&gt;, please file an issue.</source>
          <target state="translated">如果某个操作未列出，我们认为它在 &lt;code&gt;float16&lt;/code&gt; 中在数值上是稳定的。如果您认为未列出的操作在 &lt;code&gt;float16&lt;/code&gt; 中在数值上不稳定，请提出问题。</target>
        </trans-unit>
        <trans-unit id="ad31490b158e1b2eea07f572ad07acc51d64ff3a" translate="yes" xml:space="preserve">
          <source>If any checked tensor in &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;grad_outputs&lt;/code&gt; has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from &lt;code&gt;torch.expand()&lt;/code&gt;), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;grad_outputs&lt;/code&gt; 中任何已检查的张量具有重叠的内存，即指向同一内存地址的不同索引（例如，来自 &lt;code&gt;torch.expand()&lt;/code&gt; ），则此检查可能会失败，因为在这些索引处通过点扰动计算出的数值梯度将更改共享相同内存地址的所有其他索引的值。</target>
        </trans-unit>
        <trans-unit id="cc7adef2433e41254f246b09a9940775c5a1f72e" translate="yes" xml:space="preserve">
          <source>If any checked tensor in &lt;code&gt;input&lt;/code&gt; has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from &lt;code&gt;torch.expand()&lt;/code&gt;), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 任何已检查的张量具有重叠的内存，即，不同的索引指向相同的内存地址（例如，来自 &lt;code&gt;torch.expand()&lt;/code&gt; ），则此检查可能会失败，因为在这些索引处通过点扰动计算出的数值梯度将改变值共享相同内存地址的所有其他索引。</target>
        </trans-unit>
        <trans-unit id="3e818bb7352c628c52d3f8d4ba3508811d8bc58f" translate="yes" xml:space="preserve">
          <source>If any of these would help your use case, please &lt;a href=&quot;https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22module%3A+named+tensor%22&quot;&gt;search if an issue has already been filed&lt;/a&gt; and if not, &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/new/choose&quot;&gt;file one&lt;/a&gt;.</source>
          <target state="translated">如果以上任何一种都可以帮助您解决用例，请&lt;a href=&quot;https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22module%3A+named+tensor%22&quot;&gt;搜索是否已提交问题&lt;/a&gt;；如果没有，请&lt;a href=&quot;https://github.com/pytorch/pytorch/issues/new/choose&quot;&gt;提交一个&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="365d8ae01687e5fc5ba3a3c559e4b1609ec1de03" translate="yes" xml:space="preserve">
          <source>If any optimizer steps were skipped the scale is multiplied by &lt;code&gt;backoff_factor&lt;/code&gt; to reduce it. If &lt;code&gt;growth_interval&lt;/code&gt; unskipped iterations occurred consecutively, the scale is multiplied by &lt;code&gt;growth_factor&lt;/code&gt; to increase it.</source>
          <target state="translated">如果跳过了任何优化程序步骤，则将比例乘以 &lt;code&gt;backoff_factor&lt;/code&gt; 来减小比例。如果 &lt;code&gt;growth_interval&lt;/code&gt; 未跳过的迭代连续发生，则将比例乘以 &lt;code&gt;growth_factor&lt;/code&gt; 来增加它。</target>
        </trans-unit>
        <trans-unit id="1080160de99f6c4b9ac7827f3fbead61bb81527f" translate="yes" xml:space="preserve">
          <source>If both arguments are 2-dimensional, the matrix-matrix product is returned.</source>
          <target state="translated">如果两个参数都是二维的,则返回矩阵-矩阵积。</target>
        </trans-unit>
        <trans-unit id="d6fa8132ad71df4e876fdeaeb999a43daf3f4278" translate="yes" xml:space="preserve">
          <source>If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N &amp;gt; 2), then a batched matrix multiply is returned. If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. The non-matrix (i.e. batch) dimensions are &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcasted&lt;/a&gt; (and thus must be broadcastable). For example, if &lt;code&gt;input&lt;/code&gt; is a</source>
          <target state="translated">如果两个参数均为至少一维且至少一个参数为N维（其中N&amp;gt; 2），则返回批处理矩阵乘法。如果第一个自变量是一维的，则将1附加到其维的前面，以实现成批矩阵乘法并在之后将其删除。如果第二个自变量是一维的，则将其1附加到其维上，以实现成批矩阵倍数的目的，然后将其删除。非矩阵（即批处理）尺寸被&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;广播&lt;/a&gt;（因此必须是可广播的）。例如，如果 &lt;code&gt;input&lt;/code&gt; 是</target>
        </trans-unit>
        <trans-unit id="d89fe56ed688f10e95f166f512f223aa5ad0f1fe" translate="yes" xml:space="preserve">
          <source>If both tensors are 1-dimensional, the dot product (scalar) is returned.</source>
          <target state="translated">如果两个张力都是一维的,则返回点积(标量)。</target>
        </trans-unit>
        <trans-unit id="b74b95d5cdc716438937d19d93c000b85e6ccb8b" translate="yes" xml:space="preserve">
          <source>If checkpointed segment contains tensors detached from the computational graph by &lt;code&gt;detach()&lt;/code&gt; or &lt;code&gt;torch.no_grad()&lt;/code&gt;, the backward pass will raise an error. This is because &lt;code&gt;checkpoint&lt;/code&gt; makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the &lt;code&gt;checkpoint&lt;/code&gt; function.</source>
          <target state="translated">如果检查点线段包含通过 &lt;code&gt;detach()&lt;/code&gt; 或 &lt;code&gt;torch.no_grad()&lt;/code&gt; 与计算图分离的张量，则向后传递会引发错误。这是因为 &lt;code&gt;checkpoint&lt;/code&gt; 使所有输出都需要渐变，这在将张量定义为在模型中没有渐变时会引起问题。为了避免这种情况，请在 &lt;code&gt;checkpoint&lt;/code&gt; 功能之外分离张量。</target>
        </trans-unit>
        <trans-unit id="6148ee076450f28f4a8ddad929803f8c02667e14" translate="yes" xml:space="preserve">
          <source>If downloaded file is a zip file, it will be automatically decompressed.</source>
          <target state="translated">如果下载的文件是一个zip文件,将自动解压。</target>
        </trans-unit>
        <trans-unit id="3632499e9221774229ac5b4849f51f9b7ea5baa6" translate="yes" xml:space="preserve">
          <source>If input has shape</source>
          <target state="translated">如果输入的形状是</target>
        </trans-unit>
        <trans-unit id="6054f9e157471bc8a7a27391680f78509c758f80" translate="yes" xml:space="preserve">
          <source>If it is &lt;code&gt;False&lt;/code&gt;, only eigenvalues are computed. If it is &lt;code&gt;True&lt;/code&gt;, both eigenvalues and eigenvectors are computed.</source>
          <target state="translated">如果为 &lt;code&gt;False&lt;/code&gt; ，则仅计算特征值。如果为 &lt;code&gt;True&lt;/code&gt; ，则同时计算特征值和特征向量。</target>
        </trans-unit>
        <trans-unit id="c2b14334c440f3e0a03846297c6b858c19d59ebd" translate="yes" xml:space="preserve">
          <source>If neither is specified, &lt;code&gt;init_method&lt;/code&gt; is assumed to be &amp;ldquo;env://&amp;rdquo;.</source>
          <target state="translated">如果两者均未指定， &lt;code&gt;init_method&lt;/code&gt; 假定为&amp;ldquo; env：//&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="8af7fa7360ca7c6df177260376e8748ca36e7b2e" translate="yes" xml:space="preserve">
          <source>If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module&amp;rsquo;s &lt;code&gt;_load_from_state_dict&lt;/code&gt; method can compare the version number and do appropriate changes if the state dict is from before the change.</source>
          <target state="translated">如果从模块添加/删除了新的参数/缓冲区，则该数字将增加，并且模块的 &lt;code&gt;_load_from_state_dict&lt;/code&gt; 方法可以比较版本号，并且如果状态dict来自更改之前，则可以进行适当的更改。</target>
        </trans-unit>
        <trans-unit id="ab07e4375ba32a4af6a712b8e6818b3c0e5409bd" translate="yes" xml:space="preserve">
          <source>If no inf/NaN gradients are found, invokes &lt;code&gt;optimizer.step()&lt;/code&gt; using the unscaled gradients. Otherwise, &lt;code&gt;optimizer.step()&lt;/code&gt; is skipped to avoid corrupting the params.</source>
          <target state="translated">如果未找到inf / NaN渐变，请使用未缩放的渐变来调用 &lt;code&gt;optimizer.step()&lt;/code&gt; 。否则，将跳过 &lt;code&gt;optimizer.step()&lt;/code&gt; ，以避免破坏参数。</target>
        </trans-unit>
        <trans-unit id="35eb8efeed66172f49e69d3035c935e69cd09a9d" translate="yes" xml:space="preserve">
          <source>If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row.</source>
          <target state="translated">如果没有,则在没有替换的情况下进行绘制,也就是说,当为某行绘制样本索引时,不能再为该行绘制。</target>
        </trans-unit>
        <trans-unit id="8bad8088b028a7abefb044cb50129322a5504452" translate="yes" xml:space="preserve">
          <source>If one of the elements being compared is a NaN, then that element is returned. &lt;a href=&quot;#torch.maximum&quot;&gt;&lt;code&gt;maximum()&lt;/code&gt;&lt;/a&gt; is not supported for tensors with complex dtypes.</source>
          <target state="translated">如果要比较的元素之一是NaN，则返回该元素。具有复杂dtypes的张量不支持&lt;a href=&quot;#torch.maximum&quot;&gt; &lt;code&gt;maximum()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4d2edfb80af8b4c10b0daabd533575291d8f3436" translate="yes" xml:space="preserve">
          <source>If one of the elements being compared is a NaN, then that element is returned. &lt;a href=&quot;#torch.minimum&quot;&gt;&lt;code&gt;minimum()&lt;/code&gt;&lt;/a&gt; is not supported for tensors with complex dtypes.</source>
          <target state="translated">如果要比较的元素之一是NaN，则返回该元素。具有复杂dtypes的张量不支持&lt;a href=&quot;#torch.minimum&quot;&gt; &lt;code&gt;minimum()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c998486ad794fdb83e80c2ef35e8c9ca0a076827" translate="yes" xml:space="preserve">
          <source>If one of the processes exits with a non-zero exit status, the remaining processes are killed and an exception is raised with the cause of termination. In the case an exception was caught in the child process, it is forwarded and its traceback is included in the exception raised in the parent process.</source>
          <target state="translated">如果其中一个进程以非零的退出状态退出,那么其余的进程就会被杀死,并引发一个异常,说明终止的原因。如果异常是在子进程中发现的,则会被转发,其回溯包含在父进程引发的异常中。</target>
        </trans-unit>
        <trans-unit id="7af111fbf2a8bf97e37886e5888c11a588605b8f" translate="yes" xml:space="preserve">
          <source>If provided, the optional argument &lt;code&gt;weight&lt;/code&gt; should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.</source>
          <target state="translated">如果提供，则可选参数 &lt;code&gt;weight&lt;/code&gt; 应为一维张量，为每个类分配权重。当您的训练集不平衡时，此功能特别有用。</target>
        </trans-unit>
        <trans-unit id="969ca4bbce09cb3097e3db4dcee3ccac4fa12d57" translate="yes" xml:space="preserve">
          <source>If replacement is &lt;code&gt;True&lt;/code&gt;, samples are drawn with replacement.</source>
          <target state="translated">如果replacement为 &lt;code&gt;True&lt;/code&gt; ，则抽取替换后的样本。</target>
        </trans-unit>
        <trans-unit id="484e62a9e6c54b0aaa3a5007d8a946f20da7704f" translate="yes" xml:space="preserve">
          <source>If the &lt;code&gt;repeats&lt;/code&gt; is &lt;code&gt;tensor([n1, n2, n3, &amp;hellip;])&lt;/code&gt;, then the output will be &lt;code&gt;tensor([0, 0, &amp;hellip;, 1, 1, &amp;hellip;, 2, 2, &amp;hellip;, &amp;hellip;])&lt;/code&gt; where &lt;code&gt;0&lt;/code&gt; appears &lt;code&gt;n1&lt;/code&gt; times, &lt;code&gt;1&lt;/code&gt; appears &lt;code&gt;n2&lt;/code&gt; times, &lt;code&gt;2&lt;/code&gt; appears &lt;code&gt;n3&lt;/code&gt; times, etc.</source>
          <target state="translated">如果 &lt;code&gt;repeats&lt;/code&gt; 是 &lt;code&gt;tensor([n1, n2, n3, &amp;hellip;])&lt;/code&gt; ，则输出将为 &lt;code&gt;tensor([0, 0, &amp;hellip;, 1, 1, &amp;hellip;, 2, 2, &amp;hellip;, &amp;hellip;])&lt;/code&gt; 其中 &lt;code&gt;0&lt;/code&gt; 出现 &lt;code&gt;n1&lt;/code&gt; &lt;code&gt;1&lt;/code&gt; 次出现 &lt;code&gt;n2&lt;/code&gt; 次， &lt;code&gt;2&lt;/code&gt; 次出现 &lt;code&gt;n3&lt;/code&gt; 次，依此类推。</target>
        </trans-unit>
        <trans-unit id="5a469887feb69df132460464b530071b29b07521" translate="yes" xml:space="preserve">
          <source>If the &lt;code&gt;self.data&lt;/code&gt; Tensor already has the correct &lt;code&gt;torch.dtype&lt;/code&gt; and &lt;code&gt;torch.device&lt;/code&gt;, then &lt;code&gt;self&lt;/code&gt; is returned. Otherwise, returns a copy with the desired configuration.</source>
          <target state="translated">如果 &lt;code&gt;self.data&lt;/code&gt; Tensor已经具有正确的 &lt;code&gt;torch.dtype&lt;/code&gt; 和 &lt;code&gt;torch.device&lt;/code&gt; ，则返回 &lt;code&gt;self&lt;/code&gt; 。否则，返回具有所需配置的副本。</target>
        </trans-unit>
        <trans-unit id="5d823313d98ebbfc4cb390dd8145252b2ceb1eaf" translate="yes" xml:space="preserve">
          <source>If the &lt;code&gt;self&lt;/code&gt; Tensor already has the correct &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, then &lt;code&gt;self&lt;/code&gt; is returned. Otherwise, the returned tensor is a copy of &lt;code&gt;self&lt;/code&gt; with the desired &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">如果 &lt;code&gt;self&lt;/code&gt; Tensor已经具有正确的&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;，则返回 &lt;code&gt;self&lt;/code&gt; 。否则，返回的张量是具有所需&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;的 &lt;code&gt;self&lt;/code&gt; 的副本。</target>
        </trans-unit>
        <trans-unit id="9a55f7772c88fb21a34f619e7cbc1ddc996bd65e" translate="yes" xml:space="preserve">
          <source>If the &lt;code&gt;spawn&lt;/code&gt; start method is used, &lt;code&gt;worker_init_fn&lt;/code&gt; cannot be an unpicklable object, e.g., a lambda function. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/multiprocessing.html#multiprocessing-best-practices&quot;&gt;Multiprocessing best practices&lt;/a&gt; on more details related to multiprocessing in PyTorch.</source>
          <target state="translated">如果使用了 &lt;code&gt;spawn&lt;/code&gt; start方法， &lt;code&gt;worker_init_fn&lt;/code&gt; 不能是不可拾取的对象，例如lambda函数。有关PyTorch中与多处理有关的更多详细信息，请参见&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/multiprocessing.html#multiprocessing-best-practices&quot;&gt;多处理最佳实践&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6b8c8166f7ac4def7f8a41f990435d7b737da1e1" translate="yes" xml:space="preserve">
          <source>If the RNN is bidirectional, num_directions should be 2, else it should be 1.</source>
          <target state="translated">如果RNN是双向的,num_directions应该是2,否则应该是1。</target>
        </trans-unit>
        <trans-unit id="b65be36d1abe2727420269313c2df666e233f015" translate="yes" xml:space="preserve">
          <source>If the consumer process dies abnormally to a fatal signal, the shared tensor could be forever kept in memory as long as the sending process is running.</source>
          <target state="translated">如果消费者进程对致命信号异常死亡,只要发送进程在运行,共享张量就可能永远保存在内存中。</target>
        </trans-unit>
        <trans-unit id="191802e65e024d3ca3eb1bca5bd908ca2613d6eb" translate="yes" xml:space="preserve">
          <source>If the current node is the owner, returns a reference to the local value. Otherwise, throws an exception.</source>
          <target state="translated">如果当前节点是所有者,返回对本地值的引用。否则,抛出一个异常。</target>
        </trans-unit>
        <trans-unit id="c74db6eba0315e50d747303ae70dc492c51af6b7" translate="yes" xml:space="preserve">
          <source>If the decorated &lt;code&gt;forward&lt;/code&gt; is called outside an autocast-enabled region, &lt;a href=&quot;#torch.cuda.amp.custom_fwd&quot;&gt;&lt;code&gt;custom_fwd&lt;/code&gt;&lt;/a&gt; is a no-op and &lt;code&gt;cast_inputs&lt;/code&gt; has no effect.</source>
          <target state="translated">如果在启用自动广播的区域之外调用修饰的 &lt;code&gt;forward&lt;/code&gt; ，则&lt;a href=&quot;#torch.cuda.amp.custom_fwd&quot;&gt; &lt;code&gt;custom_fwd&lt;/code&gt; &lt;/a&gt;为无操作， &lt;code&gt;cast_inputs&lt;/code&gt; 不起作用。</target>
        </trans-unit>
        <trans-unit id="9bc28fe59df6be023786ded1b0d7a664101bb1a6" translate="yes" xml:space="preserve">
          <source>If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed.</source>
          <target state="translated">如果第一个参数是一维的,第二个参数是二维的,那么在矩阵乘法时,在它的维度前加1。矩阵乘法后,去掉预置的维度。</target>
        </trans-unit>
        <trans-unit id="805768ae7e7883adbd4eb9cfd347ff7553b939ce" translate="yes" xml:space="preserve">
          <source>If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned.</source>
          <target state="translated">如果第一个参数是二维的,第二个参数是一维的,则返回矩阵-向量积。</target>
        </trans-unit>
        <trans-unit id="603aa447ecbcf1ce44b939cf59f78b72d62d8d4f" translate="yes" xml:space="preserve">
          <source>If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype &lt;code&gt;torch.float16&lt;/code&gt; 4) V100 GPU is used, 5) input data is not in &lt;code&gt;PackedSequence&lt;/code&gt; format persistent algorithm can be selected to improve performance.</source>
          <target state="translated">如果满足以下条件：1）启用cudnn，2）输入数据在GPU上3）输入数据具有 &lt;code&gt;torch.float16&lt;/code&gt; 4）使用V100 GPU，5）输入数据不是 &lt;code&gt;PackedSequence&lt;/code&gt; 格式的持久算法，可以选择以提高性能。</target>
        </trans-unit>
        <trans-unit id="e2ec6b76ced3aa12445764a71f2f934d55e863cb" translate="yes" xml:space="preserve">
          <source>If the forward pass for a particular op has &lt;code&gt;float16&lt;/code&gt; inputs, the backward pass for that op will produce &lt;code&gt;float16&lt;/code&gt; gradients. Gradient values with small magnitudes may not be representable in &lt;code&gt;float16&lt;/code&gt;. These values will flush to zero (&amp;ldquo;underflow&amp;rdquo;), so the update for the corresponding parameters will be lost.</source>
          <target state="translated">如果特定操作的前向通过具有 &lt;code&gt;float16&lt;/code&gt; 输入，则该操作的后向通过将产生 &lt;code&gt;float16&lt;/code&gt; 梯度。在 &lt;code&gt;float16&lt;/code&gt; 中，小幅度的渐变值可能无法表示。这些值将刷新为零（&amp;ldquo;下溢&amp;rdquo;），因此将丢失相应参数的更新。</target>
        </trans-unit>
        <trans-unit id="787caec1c53ac744f6704113ef9317970b83ba1d" translate="yes" xml:space="preserve">
          <source>If the input argument is a tensor, but ONNX asks for a scalar, we have to explicitly do the conversion. The helper function &lt;code&gt;_scalar&lt;/code&gt; can convert a scalar tensor into a python scalar, and &lt;code&gt;_if_scalar_type_as&lt;/code&gt; can turn a Python scalar into a PyTorch tensor.</source>
          <target state="translated">如果输入参数是张量，但ONNX要求标量，则必须显式进行转换。辅助函数 &lt;code&gt;_scalar&lt;/code&gt; 可以将标量张量转换为python标量，而 &lt;code&gt;_if_scalar_type_as&lt;/code&gt; 可以将Python标量转换为PyTorch张量。</target>
        </trans-unit>
        <trans-unit id="9db31a4fe372bf1c4d439edd124d674c08f32005" translate="yes" xml:space="preserve">
          <source>If the main process exits abruptly (e.g. because of an incoming signal), Python&amp;rsquo;s &lt;code&gt;multiprocessing&lt;/code&gt; sometimes fails to clean up its children. It&amp;rsquo;s a known caveat, so if you&amp;rsquo;re seeing any resource leaks after interrupting the interpreter, it probably means that this has just happened to you.</source>
          <target state="translated">如果主进程突然退出（例如，由于传入信号），则Python的 &lt;code&gt;multiprocessing&lt;/code&gt; 有时无法清理其子进程。这是一个已知的警告，因此，如果您在中断解释器后看到任何资源泄漏，则可能意味着这只是发生在您身上。</target>
        </trans-unit>
        <trans-unit id="f988049101ff18d4e7918e66cf5ad5885aec7070" translate="yes" xml:space="preserve">
          <source>If the norm of a row is lower than &lt;code&gt;maxnorm&lt;/code&gt;, the row is unchanged</source>
          <target state="translated">如果某行的范数低于 &lt;code&gt;maxnorm&lt;/code&gt; ，则该行不变</target>
        </trans-unit>
        <trans-unit id="d44fada1fe05f824e96651b70e1c4a292ba8a891" translate="yes" xml:space="preserve">
          <source>If the object is already present in &lt;code&gt;model_dir&lt;/code&gt;, it&amp;rsquo;s deserialized and returned. The default value of &lt;code&gt;model_dir&lt;/code&gt; is &lt;code&gt;&amp;lt;hub_dir&amp;gt;/checkpoints&lt;/code&gt; where &lt;code&gt;hub_dir&lt;/code&gt; is the directory returned by &lt;a href=&quot;#torch.hub.get_dir&quot;&gt;&lt;code&gt;get_dir()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">如果该对象已经存在于 &lt;code&gt;model_dir&lt;/code&gt; 中，则将其反序列化并返回。的默认值 &lt;code&gt;model_dir&lt;/code&gt; 是 &lt;code&gt;&amp;lt;hub_dir&amp;gt;/checkpoints&lt;/code&gt; ，其中 &lt;code&gt;hub_dir&lt;/code&gt; 是由返回目录&lt;a href=&quot;#torch.hub.get_dir&quot;&gt; &lt;code&gt;get_dir()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="572431e231db065e224abcc587fb2e502c2edb94" translate="yes" xml:space="preserve">
          <source>If the object is already present in &lt;code&gt;model_dir&lt;/code&gt;, it&amp;rsquo;s deserialized and returned. The default value of &lt;code&gt;model_dir&lt;/code&gt; is &lt;code&gt;&amp;lt;hub_dir&amp;gt;/checkpoints&lt;/code&gt; where &lt;code&gt;hub_dir&lt;/code&gt; is the directory returned by &lt;a href=&quot;hub#torch.hub.get_dir&quot;&gt;&lt;code&gt;get_dir()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">如果该对象已经存在于 &lt;code&gt;model_dir&lt;/code&gt; 中，则将其反序列化并返回。的默认值 &lt;code&gt;model_dir&lt;/code&gt; 是 &lt;code&gt;&amp;lt;hub_dir&amp;gt;/checkpoints&lt;/code&gt; ，其中 &lt;code&gt;hub_dir&lt;/code&gt; 是由返回目录&lt;a href=&quot;hub#torch.hub.get_dir&quot;&gt; &lt;code&gt;get_dir()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e07c64dd92353460c82bc7c43c0298ff2498e3ef" translate="yes" xml:space="preserve">
          <source>If the operator is a non-ATen operator, the symbolic function has to be added in the corresponding PyTorch Function class. Please read the following instructions:</source>
          <target state="translated">如果运算符为非 ATen 运算符,则必须在相应的 PyTorch 函数类中添加符号函数。请阅读以下说明。</target>
        </trans-unit>
        <trans-unit id="8de0216c16416c65797272e400b10794b950ac17" translate="yes" xml:space="preserve">
          <source>If the operator is an ATen operator, which means you can find the declaration of the function in &lt;code&gt;torch/csrc/autograd/generated/VariableType.h&lt;/code&gt; (available in generated code in PyTorch install dir), you should add the symbolic function in &lt;code&gt;torch/onnx/symbolic_opset&amp;lt;version&amp;gt;.py&lt;/code&gt; and follow the instructions listed as below:</source>
          <target state="translated">如果该运算符是ATen运算符，则意味着您可以在 &lt;code&gt;torch/csrc/autograd/generated/VariableType.h&lt;/code&gt; 找到该函数的声明（在PyTorch安装目录的生成代码中可用），您应该在 &lt;code&gt;torch/onnx/symbolic_opset&amp;lt;version&amp;gt;.py&lt;/code&gt; 添加符号函数onnx / symbolic_opset &amp;lt;version&amp;gt; .py并按照以下说明进行操作：</target>
        </trans-unit>
        <trans-unit id="3f469a63a2d68271648256ec0018c99f6ecea6e3" translate="yes" xml:space="preserve">
          <source>If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0.</source>
          <target state="translated">如果运行的最小值等于运行的最大值,则将刻度和零点设置为1.0和0。</target>
        </trans-unit>
        <trans-unit id="f56fe0773d874b4f8f3b816d146b62978698c172" translate="yes" xml:space="preserve">
          <source>If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0.</source>
          <target state="translated">如果运行的最小值等于运行的最大值,则将刻度和零点设置为1.0和0。</target>
        </trans-unit>
        <trans-unit id="08987c5581da900d9050fec1b266346dd018ed86" translate="yes" xml:space="preserve">
          <source>If the sum to the power of &lt;code&gt;p&lt;/code&gt; is zero, the gradient of this function is not defined. This implementation will set the gradient to zero in this case.</source>
          <target state="translated">如果 &lt;code&gt;p&lt;/code&gt; 的幂的和为零，则此函数的梯度未定义。在这种情况下，此实现会将梯度设置为零。</target>
        </trans-unit>
        <trans-unit id="a6455b98cbf68564852f3d65be9e3d85724ba33b" translate="yes" xml:space="preserve">
          <source>If the targets are given as a 1d tensor that is the concatenation of individual targets, the target_lengths must add up to the total length of the tensor.</source>
          <target state="translated">如果给定的目标是一个1d的张量,是单个目标的连接,那么target_lengths必须加起来是张量的总长度。</target>
        </trans-unit>
        <trans-unit id="eb673583185e53bcc86746d0d9978c1392f01670" translate="yes" xml:space="preserve">
          <source>If the tensor has a batch dimension of size 1, then &lt;code&gt;squeeze(input)&lt;/code&gt; will also remove the batch dimension, which can lead to unexpected errors.</source>
          <target state="translated">如果张量的批量大小为1，则 &lt;code&gt;squeeze(input)&lt;/code&gt; 也将删除批量大小，这可能会导致意外错误。</target>
        </trans-unit>
        <trans-unit id="67a5ac190ccd238f6283765db614cb7a1d643567" translate="yes" xml:space="preserve">
          <source>If the torch.fft module is imported then &amp;ldquo;torch.fft&amp;rdquo; will refer to the module and not this function. Use &lt;a href=&quot;../tensors#torch.Tensor.fft&quot;&gt;&lt;code&gt;torch.Tensor.fft()&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">如果导入了torch.fft模块，则&amp;ldquo; torch.fft&amp;rdquo;将引用该模块，而不是此功能。请改用&lt;a href=&quot;../tensors#torch.Tensor.fft&quot;&gt; &lt;code&gt;torch.Tensor.fft()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="d413b6561145ebb882d59d64e1d5bb494eb94bfd" translate="yes" xml:space="preserve">
          <source>If the type of a scalar operand is of a higher category than tensor operands (where complex &amp;gt; floating &amp;gt; integral &amp;gt; boolean), we promote to a type with sufficient size to hold all scalar operands of that category.</source>
          <target state="translated">如果标量操作数的类型比张量操作数（其中复数&amp;gt;浮动&amp;gt;整数&amp;gt;布尔值）具有更高的类别，我们将提升为具有足够大小的类型以容纳该类别的所有标量操作数。</target>
        </trans-unit>
        <trans-unit id="1fdbc94ec5c6bbfaeab5c1df5a93f67f15cfdc84" translate="yes" xml:space="preserve">
          <source>If there are multiple maximal values in a reduced row then the indices of the first maximal value are returned.</source>
          <target state="translated">如果在缩小的行中有多个最大值,那么将返回第一个最大值的指数。</target>
        </trans-unit>
        <trans-unit id="dffbe07ea95c30719f442abdb52cf12ce069da58" translate="yes" xml:space="preserve">
          <source>If there are multiple minimal values in a reduced row then the indices of the first minimal value are returned.</source>
          <target state="translated">如果在缩小的行中有多个最小值,那么将返回第一个最小值的索引。</target>
        </trans-unit>
        <trans-unit id="c351680cf4894b25f49425f0d4d516d6d33a16ff" translate="yes" xml:space="preserve">
          <source>If there are multiple minimal values then the indices of the first minimal value are returned.</source>
          <target state="translated">如果有多个最小值,那么将返回第一个最小值的索引。</target>
        </trans-unit>
        <trans-unit id="7f03b96d2971ec564ed4f31e34caa43003898f1d" translate="yes" xml:space="preserve">
          <source>If there are no higher-category zero-dim operands, we promote to a type with sufficient size and category to hold all dimensioned operands.</source>
          <target state="translated">如果没有更高类别的零维操作数,我们就推广到一个有足够大小和类别的类型,以容纳所有的维操作数。</target>
        </trans-unit>
        <trans-unit id="5962a1d4338b99c456d76f53f49888023b22b5fe" translate="yes" xml:space="preserve">
          <source>If this instance is not enabled, returns an empty dict.</source>
          <target state="translated">如果该实例未启用,则返回一个空的dict。</target>
        </trans-unit>
        <trans-unit id="bc9e3883be4333b3f7486d77ab3655dd30a42ad7" translate="yes" xml:space="preserve">
          <source>If this is already of the correct type, no copy is performed and the original object is returned.</source>
          <target state="translated">如果这已经是正确的类型,则不进行复制,返回原始对象。</target>
        </trans-unit>
        <trans-unit id="ce68525cc568e97b080be8925035deeb766a0ee2" translate="yes" xml:space="preserve">
          <source>If this object is already in CPU memory and on the correct device, then no copy is performed and the original object is returned.</source>
          <target state="translated">如果这个对象已经在CPU内存中,并且在正确的设备上,那么就不会进行复制,而是返回原来的对象。</target>
        </trans-unit>
        <trans-unit id="6788146c6f952e177f58c27fec835e0c0d256c8a" translate="yes" xml:space="preserve">
          <source>If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.</source>
          <target state="translated">如果这个对象已经在CUDA内存中,并且在正确的设备上,那么就不进行复制,返回原始对象。</target>
        </trans-unit>
        <trans-unit id="242928789aba900d14b07ac7fbbbd1efba673985" translate="yes" xml:space="preserve">
          <source>If true, undefined output grad tensors will be expanded to tensors full of zeros prior to calling the &lt;code&gt;backward()&lt;/code&gt; method.</source>
          <target state="translated">如果为true，则在调用 &lt;code&gt;backward()&lt;/code&gt; 方法之前，未定义的输出grad张量将被扩展为充满零的张量。</target>
        </trans-unit>
        <trans-unit id="7c50d408ff8252bcf7756ca41661606361f0e20e" translate="yes" xml:space="preserve">
          <source>If x1 has shape</source>
          <target state="translated">如果x1的形状是</target>
        </trans-unit>
        <trans-unit id="c21dcebc3506f3faeef2282027b5db5536bb6293" translate="yes" xml:space="preserve">
          <source>If you are profiling CUDA code, the first profiler that &lt;code&gt;bottleneck&lt;/code&gt; runs (cProfile) will include the CUDA startup time (CUDA buffer allocation cost) in its time reporting. This should not matter if your bottlenecks result in code much slower than the CUDA startup time.</source>
          <target state="translated">如果要分析CUDA代码，则 &lt;code&gt;bottleneck&lt;/code&gt; 运行的第一个分析器（cProfile）将在其时间报告中包括CUDA启动时间（CUDA缓冲区分配成本）。如果瓶颈导致代码比CUDA启动时间慢得多，这无关紧要。</target>
        </trans-unit>
        <trans-unit id="bb09e45c509c395b596fc5d452b50e8440b313fe" translate="yes" xml:space="preserve">
          <source>If you are using DistributedDataParallel in conjunction with the &lt;a href=&quot;../rpc#distributed-rpc-framework&quot;&gt;Distributed RPC Framework&lt;/a&gt;, you should always use &lt;a href=&quot;../rpc#torch.distributed.autograd.backward&quot;&gt;&lt;code&gt;torch.distributed.autograd.backward()&lt;/code&gt;&lt;/a&gt; to compute gradients and &lt;a href=&quot;../rpc#torch.distributed.optim.DistributedOptimizer&quot;&gt;&lt;code&gt;torch.distributed.optim.DistributedOptimizer&lt;/code&gt;&lt;/a&gt; for optimizing parameters.</source>
          <target state="translated">如果将DistributedDataParallel与&lt;a href=&quot;../rpc#distributed-rpc-framework&quot;&gt;Distributed RPC Framework&lt;/a&gt;结合使用，则应始终使用&lt;a href=&quot;../rpc#torch.distributed.autograd.backward&quot;&gt; &lt;code&gt;torch.distributed.autograd.backward()&lt;/code&gt; &lt;/a&gt;来计算渐变，并始终使用&lt;a href=&quot;../rpc#torch.distributed.optim.DistributedOptimizer&quot;&gt; &lt;code&gt;torch.distributed.optim.DistributedOptimizer&lt;/code&gt; &lt;/a&gt;来优化参数。</target>
        </trans-unit>
        <trans-unit id="f2eaa5f7e49b18ce16b1e20ba98c586a98d8b30c" translate="yes" xml:space="preserve">
          <source>If you are working with a multi-GPU model, this function is insufficient to get determinism. To seed all GPUs, use &lt;a href=&quot;#torch.cuda.manual_seed_all&quot;&gt;&lt;code&gt;manual_seed_all()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">如果您使用的是多GPU模型，则此功能不足以获得确定性。要播种所有GPU，请使用&lt;a href=&quot;#torch.cuda.manual_seed_all&quot;&gt; &lt;code&gt;manual_seed_all()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="70fa782f705679e7b47a8754281b13145f0bdb95" translate="yes" xml:space="preserve">
          <source>If you are working with a multi-GPU model, this function will only initialize the seed on one GPU. To initialize all GPUs, use &lt;a href=&quot;#torch.cuda.seed_all&quot;&gt;&lt;code&gt;seed_all()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">如果您使用的是多GPU模型，则此功能将仅在一个GPU上初始化种子。要初始化所有GPU，请使用&lt;a href=&quot;#torch.cuda.seed_all&quot;&gt; &lt;code&gt;seed_all()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="79878b7e0f390005d12ab18a0ff2e5e3bf2df40c" translate="yes" xml:space="preserve">
          <source>If you have more than one GPU on each node, when using the NCCL and Gloo backend, &lt;a href=&quot;#torch.distributed.broadcast_multigpu&quot;&gt;&lt;code&gt;broadcast_multigpu()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;#torch.distributed.all_reduce_multigpu&quot;&gt;&lt;code&gt;all_reduce_multigpu()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;#torch.distributed.reduce_multigpu&quot;&gt;&lt;code&gt;reduce_multigpu()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;#torch.distributed.all_gather_multigpu&quot;&gt;&lt;code&gt;all_gather_multigpu()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.distributed.reduce_scatter_multigpu&quot;&gt;&lt;code&gt;reduce_scatter_multigpu()&lt;/code&gt;&lt;/a&gt; support distributed collective operations among multiple GPUs within each node. These functions can potentially improve the overall distributed training performance and be easily used by passing a list of tensors. Each Tensor in the passed tensor list needs to be on a separate GPU device of the host where the function is called. Note that the length of the tensor list needs to be identical among all the distributed processes. Also note that currently the multi-GPU collective functions are only supported by the NCCL backend.</source>
          <target state="translated">如果每个节点上有多个GPU，则在使用NCCL和Gloo后端时，&lt;a href=&quot;#torch.distributed.broadcast_multigpu&quot;&gt; &lt;code&gt;broadcast_multigpu()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;#torch.distributed.all_reduce_multigpu&quot;&gt; &lt;code&gt;all_reduce_multigpu()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;#torch.distributed.reduce_multigpu&quot;&gt; &lt;code&gt;reduce_multigpu()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;#torch.distributed.all_gather_multigpu&quot;&gt; &lt;code&gt;all_gather_multigpu()&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;#torch.distributed.reduce_scatter_multigpu&quot;&gt; &lt;code&gt;reduce_scatter_multigpu()&lt;/code&gt; &lt;/a&gt;支持在每个节点内的多个GPU之间进行分布式集体操作。这些功能可以潜在地改善整体分布式训练性能，并且可以通过传递张量列表来轻松使用。传递的张量列表中的每个张量必须位于调用该函数的主机的单独GPU设备上。请注意，在所有分布式过程中，张量列表的长度必须相同。另请注意，当前只有NCCL后端支持多GPU集合功能。</target>
        </trans-unit>
        <trans-unit id="318b82b9810de4a4d14eae5c37c587578c4b554f" translate="yes" xml:space="preserve">
          <source>If you need manual control over &lt;code&gt;.grad&lt;/code&gt;&amp;rsquo;s strides, assign &lt;code&gt;param.grad =&lt;/code&gt; a zeroed tensor with desired strides before the first &lt;code&gt;backward()&lt;/code&gt;, and never reset it to &lt;code&gt;None&lt;/code&gt;. 3 guarantees your layout is preserved as long as &lt;code&gt;create_graph=False&lt;/code&gt;. 4 indicates your layout is &lt;em&gt;likely&lt;/em&gt; preserved even if &lt;code&gt;create_graph=True&lt;/code&gt;.</source>
          <target state="translated">如果您需要手动控制 &lt;code&gt;.grad&lt;/code&gt; 的步幅，请在第一个 &lt;code&gt;backward()&lt;/code&gt; 之前为 &lt;code&gt;param.grad =&lt;/code&gt; 零张量分配所需的步幅，并且永远不要将其重置为 &lt;code&gt;None&lt;/code&gt; 。3确保只要 &lt;code&gt;create_graph=False&lt;/code&gt; 即可保留您的布局。4表示即使 &lt;code&gt;create_graph=True&lt;/code&gt; ,您的布局也&lt;em&gt;可能会&lt;/em&gt;保留。&lt;em&gt;&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="67bf24ac1005b3683fde52841b115c0871efc9f7" translate="yes" xml:space="preserve">
          <source>If you need to move a model to GPU via &lt;code&gt;.cuda()&lt;/code&gt;, please do so before constructing optimizers for it. Parameters of a model after &lt;code&gt;.cuda()&lt;/code&gt; will be different objects with those before the call.</source>
          <target state="translated">如果您需要通过 &lt;code&gt;.cuda()&lt;/code&gt; 将模型移至GPU ，请在为模型构建优化器之前执行此操作。 &lt;code&gt;.cuda()&lt;/code&gt; 之后的模型参数将是与调用之前的对象不同的对象。</target>
        </trans-unit>
        <trans-unit id="136147687478e135ab5df6494e85fb08cdedac29" translate="yes" xml:space="preserve">
          <source>If you plan on using this module with a &lt;code&gt;nccl&lt;/code&gt; backend or a &lt;code&gt;gloo&lt;/code&gt; backend (that uses Infiniband), together with a DataLoader that uses multiple workers, please change the multiprocessing start method to &lt;code&gt;forkserver&lt;/code&gt; (Python 3 only) or &lt;code&gt;spawn&lt;/code&gt;. Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will likely experience deadlocks if you don&amp;rsquo;t change this setting.</source>
          <target state="translated">如果您打算将此模块与 &lt;code&gt;nccl&lt;/code&gt; 后端或 &lt;code&gt;gloo&lt;/code&gt; 后端（使用Infiniband）一起使用，并与使用多个工作程序的DataLoader一起使用，请将多处理启动方法更改为 &lt;code&gt;forkserver&lt;/code&gt; （仅限Python 3）或 &lt;code&gt;spawn&lt;/code&gt; 。不幸的是，Gloo（使用Infiniband）和NCCL2都不安全，如果不更改此设置，您可能会遇到死锁。</target>
        </trans-unit>
        <trans-unit id="07ab34f8fafef28d79bd246f9789bf5105b86f8b" translate="yes" xml:space="preserve">
          <source>If you plan to backpropagate through QR, note that the current backward implementation is only well-defined when the first</source>
          <target state="translated">如果你打算通过QR进行反向传播,请注意,目前的反向实现只有当第一个</target>
        </trans-unit>
        <trans-unit id="1cd7b6977f27c5a4e9182f95ff2f859a518a8917" translate="yes" xml:space="preserve">
          <source>If you use &lt;code&gt;torch.save&lt;/code&gt; on one process to checkpoint the module, and &lt;code&gt;torch.load&lt;/code&gt; on some other processes to recover it, make sure that &lt;code&gt;map_location&lt;/code&gt; is configured properly for every process. Without &lt;code&gt;map_location&lt;/code&gt;, &lt;code&gt;torch.load&lt;/code&gt; would recover the module to devices where the module was saved from.</source>
          <target state="translated">如果您使用 &lt;code&gt;torch.save&lt;/code&gt; 一个过程，设置检查点模块，并 &lt;code&gt;torch.load&lt;/code&gt; 一些其他进程来恢复它，确保 &lt;code&gt;map_location&lt;/code&gt; 为每个进程正确配置。如果没有 &lt;code&gt;map_location&lt;/code&gt; ，则 &lt;code&gt;torch.load&lt;/code&gt; 会将模块恢复到保存模块的设备上。</target>
        </trans-unit>
        <trans-unit id="74c65669b6d4e5ce564cda9931810dc0c170c246" translate="yes" xml:space="preserve">
          <source>If you want downsampling/general resizing, you should use &lt;code&gt;interpolate()&lt;/code&gt;.</source>
          <target state="translated">如果要缩减采样/常规调整大小，则应使用 &lt;code&gt;interpolate()&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ace4de17f2e6daea2f622c0b5c24ff868ca712b5" translate="yes" xml:space="preserve">
          <source>If you wish to checkpoint the scaler&amp;rsquo;s state after a particular iteration, &lt;a href=&quot;#torch.cuda.amp.GradScaler.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; should be called after &lt;a href=&quot;#torch.cuda.amp.GradScaler.update&quot;&gt;&lt;code&gt;update()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">如果您希望在特定迭代后检查定标器的状态，&lt;a href=&quot;#torch.cuda.amp.GradScaler.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt;在&lt;a href=&quot;#torch.cuda.amp.GradScaler.update&quot;&gt; &lt;code&gt;update()&lt;/code&gt; &lt;/a&gt;之后调用state_dict （）。</target>
        </trans-unit>
        <trans-unit id="064ec7b53383c8d17c5751c9a34c0ac9daf7e8cd" translate="yes" xml:space="preserve">
          <source>If you&amp;rsquo;re using the Gloo backend, you can specify multiple interfaces by separating them by a comma, like this: &lt;code&gt;export GLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3&lt;/code&gt;. The backend will dispatch operations in a round-robin fashion across these interfaces. It is imperative that all processes specify the same number of interfaces in this variable.</source>
          <target state="translated">如果使用的是Gloo后端，则可以用逗号分隔多个接口，例如： &lt;code&gt;export GLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3&lt;/code&gt; 。后端将以循环方式在这些接口之间调度操作。至关重要的是，所有进程都必须在此变量中指定相同数量的接口。</target>
        </trans-unit>
        <trans-unit id="40a9a99bd41d3dd35eb2ee9f7f247c013023dc94" translate="yes" xml:space="preserve">
          <source>If your InfiniBand has enabled IP over IB, use Gloo, otherwise, use MPI instead. We are planning on adding InfiniBand support for Gloo in the upcoming releases.</source>
          <target state="translated">如果您的InfiniBand已经启用了IP over IB,请使用Gloo,否则请使用MPI。我们计划在即将发布的版本中增加对Gloo的InfiniBand支持。</target>
        </trans-unit>
        <trans-unit id="7ea2434c9b9d3a93397a8c29208e88de7aacbec3" translate="yes" xml:space="preserve">
          <source>If your use case is always 1-D sorted sequence, &lt;a href=&quot;torch.bucketize#torch.bucketize&quot;&gt;&lt;code&gt;torch.bucketize()&lt;/code&gt;&lt;/a&gt; is preferred, because it has fewer dimension checks resulting in slightly better performance.</source>
          <target state="translated">如果您的用例始终是一维排序序列，则首选&lt;a href=&quot;torch.bucketize#torch.bucketize&quot;&gt; &lt;code&gt;torch.bucketize()&lt;/code&gt; &lt;/a&gt;，因为它的维检查较少，从而导致性能稍好。</target>
        </trans-unit>
        <trans-unit id="52c2c0cd2584a6259a5f73e3f48627e08a00b06c" translate="yes" xml:space="preserve">
          <source>If, on the other hand, a backward pass with &lt;code&gt;create_graph=True&lt;/code&gt; is underway (in other words, if you are setting up for a double-backward), each function&amp;rsquo;s execution during backward is given a nonzero, useful &lt;code&gt;seq=&amp;lt;N&amp;gt;&lt;/code&gt;. Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. The relationship between backward and double-backward is conceptually the same as the relationship between forward and backward: The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects&amp;rsquo; &lt;code&gt;apply()&lt;/code&gt; ranges are still tagged with &lt;code&gt;stashed seq&lt;/code&gt; numbers, which can be compared to &lt;code&gt;seq&lt;/code&gt; numbers from the backward pass.</source>
          <target state="translated">另一方面，如果正在进行 &lt;code&gt;create_graph=True&lt;/code&gt; 的向后传递（换句话说，如果您要进行双向后设置），则向后执行过程中每个函数的执行将被赋予非零的，有用的 &lt;code&gt;seq=&amp;lt;N&amp;gt;&lt;/code&gt; 。这些函数本身可以创建Function对象，以便稍后在双向后执行时，就像向前传递中的原始函数一样。反向和双向后之间的关系在概念上与向前和向后之间的关系相同：这些函数仍会发出带有当前序列号标记的范围，它们创建的Function对象仍会存储这些序列号，并且在最终的double-向后，功能对象的 &lt;code&gt;apply()&lt;/code&gt; 范围仍用 &lt;code&gt;stashed seq&lt;/code&gt; 标记数字，可以将其与后向传递的 &lt;code&gt;seq&lt;/code&gt; 数字进行比较。</target>
        </trans-unit>
        <trans-unit id="eec64bb7b6af56628b6864bb7faf7664a4d5fe90" translate="yes" xml:space="preserve">
          <source>Ignoring the optional batch dimension, this method computes the following expression:</source>
          <target state="translated">忽略可选的批次维度,本方法计算以下表达式。</target>
        </trans-unit>
        <trans-unit id="d5e7e82cbad5c6c7282eb3f91a5d122dc9858f8e" translate="yes" xml:space="preserve">
          <source>ImageNet 1-crop error rates (224x224)</source>
          <target state="translated">ImageNet 1-crop错误率(224x224)</target>
        </trans-unit>
        <trans-unit id="8781d615fd77be9578225c40ac67b9471394cced" translate="yes" xml:space="preserve">
          <source>Implementation</source>
          <target state="translated">Implementation</target>
        </trans-unit>
        <trans-unit id="7aa6ad27b14b39704b337bb5273a54caf43dd27d" translate="yes" xml:space="preserve">
          <source>Implementation details: &lt;a href=&quot;https://arxiv.org/pdf/1806.08342.pdf&quot;&gt;https://arxiv.org/pdf/1806.08342.pdf&lt;/a&gt;</source>
          <target state="translated">实施细节：&lt;a href=&quot;https://arxiv.org/pdf/1806.08342.pdf&quot;&gt;https&lt;/a&gt;：//arxiv.org/pdf/1806.08342.pdf</target>
        </trans-unit>
        <trans-unit id="e2301206e5db52d9fb3a1373c4f2d128cbca7b09" translate="yes" xml:space="preserve">
          <source>Implementation details: &lt;a href=&quot;https://arxiv.org/pdf/1806.08342.pdf&quot;&gt;https://arxiv.org/pdf/1806.08342.pdf&lt;/a&gt; section 3.2.2</source>
          <target state="translated">实施细节：&lt;a href=&quot;https://arxiv.org/pdf/1806.08342.pdf&quot;&gt;https&lt;/a&gt; : //arxiv.org/pdf/1806.08342.pdf第3.2.2节</target>
        </trans-unit>
        <trans-unit id="123ad8e975b9151926be8910a7ca29c06e0a08b2" translate="yes" xml:space="preserve">
          <source>Implementing a Parameter Server using Distributed RPC Framework</source>
          <target state="translated">使用分布式RPC框架实现参数服务器</target>
        </trans-unit>
        <trans-unit id="44171a5d0f5320fd21c59c6c6516557d63659e70" translate="yes" xml:space="preserve">
          <source>Implementing batch RPC processing</source>
          <target state="translated">实现批量RPC处理</target>
        </trans-unit>
        <trans-unit id="138a4e92d697c7080168dd78d1fdd8142d73db58" translate="yes" xml:space="preserve">
          <source>Implements Adadelta algorithm.</source>
          <target state="translated">执行Adadelta算法。</target>
        </trans-unit>
        <trans-unit id="3f2f1bb826222edfff8f08a8965769571e6272e0" translate="yes" xml:space="preserve">
          <source>Implements Adagrad algorithm.</source>
          <target state="translated">执行Adagrad算法。</target>
        </trans-unit>
        <trans-unit id="f57315b0630de232938139c51a2a3c26106ba982" translate="yes" xml:space="preserve">
          <source>Implements Adam algorithm.</source>
          <target state="translated">执行亚当算法。</target>
        </trans-unit>
        <trans-unit id="2c77540f43659b4faff14d608e5c738dcda6383c" translate="yes" xml:space="preserve">
          <source>Implements AdamW algorithm.</source>
          <target state="translated">执行AdamW算法。</target>
        </trans-unit>
        <trans-unit id="8ee370fe200b91f2533d1c27faf2c64f231383a7" translate="yes" xml:space="preserve">
          <source>Implements Adamax algorithm (a variant of Adam based on infinity norm).</source>
          <target state="translated">实施Adamax算法(基于无穷大规范的Adam变体)。</target>
        </trans-unit>
        <trans-unit id="c03be935b69dc866871fdae88659292b8000bbfc" translate="yes" xml:space="preserve">
          <source>Implements Averaged Stochastic Gradient Descent.</source>
          <target state="translated">实施平均随机梯度下降法。</target>
        </trans-unit>
        <trans-unit id="05307e82588713f6b300d4819c8e92a2878569db" translate="yes" xml:space="preserve">
          <source>Implements L-BFGS algorithm, heavily inspired by &lt;code&gt;minFunc &amp;lt;https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html&amp;gt;&lt;/code&gt;.</source>
          <target state="translated">实现L-BFGS算法，该算法受 &lt;code&gt;minFunc &amp;lt;https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html&amp;gt;&lt;/code&gt; 的启发很大。</target>
        </trans-unit>
        <trans-unit id="bf2d15180174f2200375382c7122c5aecdd4ad8b" translate="yes" xml:space="preserve">
          <source>Implements RMSprop algorithm.</source>
          <target state="translated">执行RMSprop算法。</target>
        </trans-unit>
        <trans-unit id="9f29957e1ffbfbf9022787fc2c7cfe246141046c" translate="yes" xml:space="preserve">
          <source>Implements data parallelism at the module level.</source>
          <target state="translated">在模块级实现数据并行。</target>
        </trans-unit>
        <trans-unit id="247ad11e0e8ab7f7233963e6adbe7062bce23a44" translate="yes" xml:space="preserve">
          <source>Implements distributed data parallelism that is based on &lt;code&gt;torch.distributed&lt;/code&gt; package at the module level.</source>
          <target state="translated">在模块级别基于 &lt;code&gt;torch.distributed&lt;/code&gt; 包实现分布式数据并行性。</target>
        </trans-unit>
        <trans-unit id="dd0d130d28d26e9f00e5bf88ccace4711a7a11be" translate="yes" xml:space="preserve">
          <source>Implements lazy version of Adam algorithm suitable for sparse tensors.</source>
          <target state="translated">实施适用于稀疏张数的Adam算法的懒惰版本。</target>
        </trans-unit>
        <trans-unit id="a2f16c33652615d2d8893f5da3944e675b6f0334" translate="yes" xml:space="preserve">
          <source>Implements stochastic gradient descent (optionally with momentum).</source>
          <target state="translated">实现随机梯度下降(可选择动量)。</target>
        </trans-unit>
        <trans-unit id="83cda66c04c86732b26204a544bebe240466f333" translate="yes" xml:space="preserve">
          <source>Implements the resilient backpropagation algorithm.</source>
          <target state="translated">实施弹性反向传播算法。</target>
        </trans-unit>
        <trans-unit id="1ac98376b8dde644a48731cdda5a6c2f0a79cf77" translate="yes" xml:space="preserve">
          <source>Important Notice</source>
          <target state="translated">重要通知</target>
        </trans-unit>
        <trans-unit id="dbea8ebe841db621fdf156d683ca479b155b0a0f" translate="yes" xml:space="preserve">
          <source>Important consideration in the parameters &lt;code&gt;window&lt;/code&gt; and &lt;code&gt;center&lt;/code&gt; so that the envelop created by the summation of all the windows is never zero at certain point in time. Specifically,</source>
          <target state="translated">参数 &lt;code&gt;window&lt;/code&gt; 和 &lt;code&gt;center&lt;/code&gt; 重要考虑因素，以使所有窗口的总和所形成的包络在某个时间点永远不会为零。具体来说，</target>
        </trans-unit>
        <trans-unit id="f0e8d11c90c88612b519564a3a4ddc8195dcee1a" translate="yes" xml:space="preserve">
          <source>In &lt;code&gt;worker_init_fn&lt;/code&gt;, you may access the PyTorch seed set for each worker with either &lt;a href=&quot;#torch.utils.data.get_worker_info&quot;&gt;&lt;code&gt;torch.utils.data.get_worker_info().seed&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/torch.initial_seed#torch.initial_seed&quot;&gt;&lt;code&gt;torch.initial_seed()&lt;/code&gt;&lt;/a&gt;, and use it to seed other libraries before data loading.</source>
          <target state="translated">在 &lt;code&gt;worker_init_fn&lt;/code&gt; 中，您可以使用&lt;a href=&quot;#torch.utils.data.get_worker_info&quot;&gt; &lt;code&gt;torch.utils.data.get_worker_info().seed&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;generated/torch.initial_seed#torch.initial_seed&quot;&gt; &lt;code&gt;torch.initial_seed()&lt;/code&gt; &lt;/a&gt;访问每个工作人员的PyTorch种子集，并在加载数据之前使用它为其他库添加种子。</target>
        </trans-unit>
        <trans-unit id="1546cf194023a313eda4a32e23fe8da2e85133d5" translate="yes" xml:space="preserve">
          <source>In a multilayer GRU, the input</source>
          <target state="translated">在多层GRU中,输入的</target>
        </trans-unit>
        <trans-unit id="4ca4b7a5c3d05548dd2f62668a8daeab0e9ec06f" translate="yes" xml:space="preserve">
          <source>In a multilayer LSTM, the input</source>
          <target state="translated">在一个多层LSTM中,输入的</target>
        </trans-unit>
        <trans-unit id="e20dc20835fec339c1d3cc16c346bd578e3d90d9" translate="yes" xml:space="preserve">
          <source>In addition to bools, floats, ints, and Tensors can be used in a conditional and will be implicitly casted to a boolean.</source>
          <target state="translated">除了bools之外,floats、ints和Tensors也可以在条件中使用,并且会被隐式地转换为boolean。</target>
        </trans-unit>
        <trans-unit id="3a493aa393d90ed5e727839c89230e8cffd94159" translate="yes" xml:space="preserve">
          <source>In addition to the core statistics, we also provide some simple event counters:</source>
          <target state="translated">除了核心统计数据,我们还提供了一些简单的事件计数器。</target>
        </trans-unit>
        <trans-unit id="121bfc6001439fa824c41c891327b9bc48e17ca8" translate="yes" xml:space="preserve">
          <source>In addition, one can now create tensors with &lt;code&gt;requires_grad=True&lt;/code&gt; using factory methods such as &lt;a href=&quot;generated/torch.randn#torch.randn&quot;&gt;&lt;code&gt;torch.randn()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.zeros#torch.zeros&quot;&gt;&lt;code&gt;torch.zeros()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.ones#torch.ones&quot;&gt;&lt;code&gt;torch.ones()&lt;/code&gt;&lt;/a&gt;, and others like the following:</source>
          <target state="translated">此外，现在可以使用工厂方法（例如&lt;a href=&quot;generated/torch.randn#torch.randn&quot;&gt; &lt;code&gt;torch.randn()&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/torch.zeros#torch.zeros&quot;&gt; &lt;code&gt;torch.zeros()&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/torch.ones#torch.ones&quot;&gt; &lt;code&gt;torch.ones()&lt;/code&gt; &lt;/a&gt;以及其他类似方法）使用require_grad &lt;code&gt;requires_grad=True&lt;/code&gt; 创建张量。</target>
        </trans-unit>
        <trans-unit id="14b2f80199ae20e9c340149a30176ab138c9610a" translate="yes" xml:space="preserve">
          <source>In both cases of single-node distributed training or multi-node distributed training, this utility will launch the given number of processes per node (&lt;code&gt;--nproc_per_node&lt;/code&gt;). If used for GPU training, this number needs to be less or equal to the number of GPUs on the current system (&lt;code&gt;nproc_per_node&lt;/code&gt;), and each process will be operating on a single GPU from &lt;em&gt;GPU 0 to GPU (nproc_per_node - 1)&lt;/em&gt;.</source>
          <target state="translated">在单节点分布式训练或多节点分布式训练的两种情况下，此实用程序都会为每个节点启动给定数量的进程（ &lt;code&gt;--nproc_per_node&lt;/code&gt; ）。如果用于GPU训练，则此数目必须小于或等于当前系统上的GPU数目（ &lt;code&gt;nproc_per_node&lt;/code&gt; ），并且每个进程都将在从&lt;em&gt;GPU 0到GPU（nproc_per_node-1）&lt;/em&gt;的单个GPU上运行。</target>
        </trans-unit>
        <trans-unit id="0e876cc3bbe05952d2705985bee305b58baf29ac" translate="yes" xml:space="preserve">
          <source>In cases like these, tracing would not be appropriate and &lt;a href=&quot;torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;scripting&lt;/code&gt;&lt;/a&gt; is a better choice. If you trace such models, you may silently get incorrect results on subsequent invocations of the model. The tracer will try to emit warnings when doing something that may cause an incorrect trace to be produced.</source>
          <target state="translated">在这种情况下，跟踪是不合适的，&lt;a href=&quot;torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;scripting&lt;/code&gt; &lt;/a&gt;是一个更好的选择。如果跟踪此类模型，则可能在随后的模型调用中静默地得到不正确的结果。在执行可能导致产生不正确跟踪的操作时，跟踪器将尝试发出警告。</target>
        </trans-unit>
        <trans-unit id="2210461412c5abe9090cdb0e1f87af39668c1506" translate="yes" xml:space="preserve">
          <source>In certain cases, users may want to handle batching manually in dataset code, or simply load individual samples. For example, it could be cheaper to directly load batched data (e.g., bulk reads from a database or reading continuous chunks of memory), or the batch size is data dependent, or the program is designed to work on individual samples. Under these scenarios, it&amp;rsquo;s likely better to not use automatic batching (where &lt;code&gt;collate_fn&lt;/code&gt; is used to collate the samples), but let the data loader directly return each member of the &lt;code&gt;dataset&lt;/code&gt; object.</source>
          <target state="translated">在某些情况下，用户可能希望以数据集代码手动处理批处理，或仅加载单个样品。例如，直接加载批处理的数据（例如，从数据库中批量读取或读取连续的内存块）可能会比较便宜，或者批处理大小取决于数据，或者该程序设计为可以处理单个样本。在这些情况下，最好不要使用自动批处理（其中使用 &lt;code&gt;collate_fn&lt;/code&gt; 来整理样本），而是让数据加载器直接返回 &lt;code&gt;dataset&lt;/code&gt; 对象的每个成员。</target>
        </trans-unit>
        <trans-unit id="86e76c5e72e3158e53103708c732b0ca2fb77314" translate="yes" xml:space="preserve">
          <source>In default &lt;code&gt;reduction&lt;/code&gt; mode &lt;code&gt;'mean'&lt;/code&gt;, the losses are averaged for each minibatch over observations &lt;strong&gt;as well as&lt;/strong&gt; over dimensions. &lt;code&gt;'batchmean'&lt;/code&gt; mode gives the correct KL divergence where losses are averaged over batch dimension only. &lt;code&gt;'mean'&lt;/code&gt; mode&amp;rsquo;s behavior will be changed to the same as &lt;code&gt;'batchmean'&lt;/code&gt; in the next major release.</source>
          <target state="translated">在默认 &lt;code&gt;reduction&lt;/code&gt; 模式 &lt;code&gt;'mean'&lt;/code&gt; ，损失是对观测值&lt;strong&gt;和&lt;/strong&gt;维度上的每个小批量进行平均的。 &lt;code&gt;'batchmean'&lt;/code&gt; 模式可提供正确的KL散度，其中损失仅在批尺寸范围内平均。在下一个主要发行版中， &lt;code&gt;'mean'&lt;/code&gt; 模式的行为将更改为与 &lt;code&gt;'batchmean'&lt;/code&gt; 相同的行为。</target>
        </trans-unit>
        <trans-unit id="c3aa986e649ca9866807ff9f998ea701984440c5" translate="yes" xml:space="preserve">
          <source>In distributed mode, calling the &lt;code&gt;set_epoch()&lt;/code&gt; method at the beginning of each epoch &lt;strong&gt;before&lt;/strong&gt; creating the &lt;code&gt;DataLoader&lt;/code&gt; iterator is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be always used.</source>
          <target state="translated">在分布式模式下，&lt;strong&gt;在&lt;/strong&gt;创建 &lt;code&gt;DataLoader&lt;/code&gt; 迭代器&lt;strong&gt;之前&lt;/strong&gt;，需要在每个时期的开始处调用 &lt;code&gt;set_epoch()&lt;/code&gt; 方法，以使改组在多个时期中正常工作。否则，将始终使用相同的顺序。&lt;strong&gt;&lt;/strong&gt;</target>
        </trans-unit>
        <trans-unit id="d7469a9738cf72345e41f4a9561f6407dd3c1952" translate="yes" xml:space="preserve">
          <source>In each forward, &lt;code&gt;module&lt;/code&gt; is &lt;strong&gt;replicated&lt;/strong&gt; on each device, so any updates to the running module in &lt;code&gt;forward&lt;/code&gt; will be lost. For example, if &lt;code&gt;module&lt;/code&gt; has a counter attribute that is incremented in each &lt;code&gt;forward&lt;/code&gt;, it will always stay at the initial value because the update is done on the replicas which are destroyed after &lt;code&gt;forward&lt;/code&gt;. However, &lt;a href=&quot;#torch.nn.DataParallel&quot;&gt;&lt;code&gt;DataParallel&lt;/code&gt;&lt;/a&gt; guarantees that the replica on &lt;code&gt;device[0]&lt;/code&gt; will have its parameters and buffers sharing storage with the base parallelized &lt;code&gt;module&lt;/code&gt;. So &lt;strong&gt;in-place&lt;/strong&gt; updates to the parameters or buffers on &lt;code&gt;device[0]&lt;/code&gt; will be recorded. E.g., &lt;a href=&quot;torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt;&lt;code&gt;BatchNorm2d&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.nn.utils.spectral_norm#torch.nn.utils.spectral_norm&quot;&gt;&lt;code&gt;spectral_norm()&lt;/code&gt;&lt;/a&gt; rely on this behavior to update the buffers.</source>
          <target state="translated">在每个前， &lt;code&gt;module&lt;/code&gt; 被&lt;strong&gt;复制的&lt;/strong&gt;每个设备上，所以在任何更新到正在运行的模块 &lt;code&gt;forward&lt;/code&gt; 将会丢失。例如，如果 &lt;code&gt;module&lt;/code&gt; 的计数器属性在每个 &lt;code&gt;forward&lt;/code&gt; 递增，则它将始终保持在初始值，因为更新是在 &lt;code&gt;forward&lt;/code&gt; 之后销毁的副本上进行的。但是，&lt;a href=&quot;#torch.nn.DataParallel&quot;&gt; &lt;code&gt;DataParallel&lt;/code&gt; &lt;/a&gt;保证 &lt;code&gt;device[0]&lt;/code&gt; 上的副本具有其参数，并且缓冲区与基本并行化 &lt;code&gt;module&lt;/code&gt; 共享存储。因此&lt;strong&gt;，&lt;/strong&gt;将记录对 &lt;code&gt;device[0]&lt;/code&gt; 上的参数或缓冲区&lt;strong&gt;的就地&lt;/strong&gt;更新。例如，&lt;a href=&quot;torch.nn.batchnorm2d#torch.nn.BatchNorm2d&quot;&gt; &lt;code&gt;BatchNorm2d&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;torch.nn.utils.spectral_norm#torch.nn.utils.spectral_norm&quot;&gt; &lt;code&gt;spectral_norm()&lt;/code&gt; &lt;/a&gt;依赖此行为来更新缓冲区。</target>
        </trans-unit>
        <trans-unit id="985dc1f8616a9367ef67f7e8fb270d75af2e49e6" translate="yes" xml:space="preserve">
          <source>In fact, resetting all &lt;code&gt;.grad&lt;/code&gt;s to &lt;code&gt;None&lt;/code&gt; before each accumulation phase, e.g.:</source>
          <target state="translated">实际上，在每个累积阶段之前将所有 &lt;code&gt;.grad&lt;/code&gt; 重置为 &lt;code&gt;None&lt;/code&gt; ，例如：</target>
        </trans-unit>
        <trans-unit id="c4c7ae6e45efeeb88e3de47cc32025f3afeae3d6" translate="yes" xml:space="preserve">
          <source>In general, folding and unfolding operations are related as follows. Consider &lt;a href=&quot;#torch.nn.Fold&quot;&gt;&lt;code&gt;Fold&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;torch.nn.unfold#torch.nn.Unfold&quot;&gt;&lt;code&gt;Unfold&lt;/code&gt;&lt;/a&gt; instances created with the same parameters:</source>
          <target state="translated">通常，折叠和展开操作如下相关。考虑使用相同参数创建的&lt;a href=&quot;#torch.nn.Fold&quot;&gt; &lt;code&gt;Fold&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;torch.nn.unfold#torch.nn.Unfold&quot;&gt; &lt;code&gt;Unfold&lt;/code&gt; &lt;/a&gt;实例：</target>
        </trans-unit>
        <trans-unit id="14a82233b9f46d949d131d9066b4b1e345f4a14b" translate="yes" xml:space="preserve">
          <source>In general, folding and unfolding operations are related as follows. Consider &lt;a href=&quot;torch.nn.fold#torch.nn.Fold&quot;&gt;&lt;code&gt;Fold&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.nn.Unfold&quot;&gt;&lt;code&gt;Unfold&lt;/code&gt;&lt;/a&gt; instances created with the same parameters:</source>
          <target state="translated">通常，折叠和展开操作如下相关。考虑使用相同参数创建的&lt;a href=&quot;torch.nn.fold#torch.nn.Fold&quot;&gt; &lt;code&gt;Fold&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;#torch.nn.Unfold&quot;&gt; &lt;code&gt;Unfold&lt;/code&gt; &lt;/a&gt;实例：</target>
        </trans-unit>
        <trans-unit id="1855810e3cc280e259c5b815913beac56d322de4" translate="yes" xml:space="preserve">
          <source>In general, the basic method spends least time per iteration. However, the robust methods converge much faster and are more stable. So, the usage of the basic method is generally not recommended but there exist cases where the usage of the basic method may be preferred.</source>
          <target state="translated">一般来说,基本方法每次迭代花费的时间最少。然而,稳健方法的收敛速度更快,更稳定。因此,一般不推荐使用基本方法,但在某些情况下,可能更倾向于使用基本方法。</target>
        </trans-unit>
        <trans-unit id="5a8901639688080e9ddf9086d6c38d66aa6d29ff" translate="yes" xml:space="preserve">
          <source>In general, use the full-rank SVD implementation &lt;code&gt;torch.svd&lt;/code&gt; for dense matrices due to its 10-fold higher performance characteristics. The low-rank SVD will be useful for huge sparse matrices that &lt;code&gt;torch.svd&lt;/code&gt; cannot handle.</source>
          <target state="translated">通常，由于密集性能矩阵的性能提高了10倍， &lt;code&gt;torch.svd&lt;/code&gt; 对于密集型矩阵，请使用完整等级SVD实现torch.svd。低阶SVD对于 &lt;code&gt;torch.svd&lt;/code&gt; 无法处理的巨大稀疏矩阵很有用。</target>
        </trans-unit>
        <trans-unit id="f1278ae47dd736566667f86259def089c9bd6886" translate="yes" xml:space="preserve">
          <source>In general, you should make sure that optimized parameters live in consistent locations when optimizers are constructed and used.</source>
          <target state="translated">一般来说,在构造和使用优化器时,你应该确保优化参数活在一致的位置。</target>
        </trans-unit>
        <trans-unit id="809a8a0c78ca14c8503127d00dd103ee7f6c80b0" translate="yes" xml:space="preserve">
          <source>In many cases either tracing or scripting is an easier approach for converting a model to TorchScript. Tracing and scripting can be composed to suit the particular requirements of a part of a model.</source>
          <target state="translated">在许多情况下,跟踪或脚本是将模型转换为 TorchScript 的较简单方法。跟踪和脚本可以组成,以适应模型一部分的特殊要求。</target>
        </trans-unit>
        <trans-unit id="6485ce8afe7564cea5c8362029901ba50e16aea8" translate="yes" xml:space="preserve">
          <source>In order to spawn up multiple processes per node, you can use either &lt;code&gt;torch.distributed.launch&lt;/code&gt; or &lt;code&gt;torch.multiprocessing.spawn&lt;/code&gt;.</source>
          <target state="translated">为了在每个节点上产生多个进程，可以使用 &lt;code&gt;torch.distributed.launch&lt;/code&gt; 或 &lt;code&gt;torch.multiprocessing.spawn&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="40c808fe1c4d6bdfde7e41f404147333e778923e" translate="yes" xml:space="preserve">
          <source>In order to use CuDNN, the following must be satisfied: &lt;code&gt;targets&lt;/code&gt; must be in concatenated format, all &lt;code&gt;input_lengths&lt;/code&gt; must be &lt;code&gt;T&lt;/code&gt;.</source>
          <target state="translated">为了使用CuDNN，则必须满足以下条件： &lt;code&gt;targets&lt;/code&gt; 必须是串联的形式，所有 &lt;code&gt;input_lengths&lt;/code&gt; 必须是 &lt;code&gt;T&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5224913092c4a84680bc86a2c5b52f3a9390a2e0" translate="yes" xml:space="preserve">
          <source>In other words, for an input of size</source>
          <target state="translated">换句话说,对于一个大小为</target>
        </trans-unit>
        <trans-unit id="6f63e1b44e4587b6cbe87da3c99588530141677e" translate="yes" xml:space="preserve">
          <source>In particular, solves</source>
          <target state="translated">特别是,解决了</target>
        </trans-unit>
        <trans-unit id="43678d1fc6309b5815b7dedc1dd771218daaa610" translate="yes" xml:space="preserve">
          <source>In practice we would sample an action from the output of a network, apply this action in an environment, and then use &lt;code&gt;log_prob&lt;/code&gt; to construct an equivalent loss function. Note that we use a negative because optimizers use gradient descent, whilst the rule above assumes gradient ascent. With a categorical policy, the code for implementing REINFORCE would be as follows:</source>
          <target state="translated">在实践中，我们将从网络的输出中采样一个动作，将该动作应用于环境中，然后使用 &lt;code&gt;log_prob&lt;/code&gt; 构造一个等效的损失函数。请注意，我们使用负值，因为优化程序使用梯度下降，而以上规则假定梯度上升。使用分类策略，用于实现REINFORCE的代码如下：</target>
        </trans-unit>
        <trans-unit id="1cb4118455a2bb7d53d0e1417d8853260cb96078" translate="yes" xml:space="preserve">
          <source>In practice, when working with named tensors, one should avoid having unnamed dimensions because their handling can be complicated. It is recommended to lift all unnamed dimensions to be named dimensions by using &lt;a href=&quot;#torch.Tensor.refine_names&quot;&gt;&lt;code&gt;refine_names()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">在实践中，使用命名张量时，应避免使用未命名的维，因为它们的处理可能很复杂。建议通过使用&lt;a href=&quot;#torch.Tensor.refine_names&quot;&gt; &lt;code&gt;refine_names()&lt;/code&gt; &lt;/a&gt;将所有未命名的尺寸提升为已命名的尺寸。</target>
        </trans-unit>
        <trans-unit id="df5ee95eb608e3ef9c532ed1dab9ce8f7003c283" translate="yes" xml:space="preserve">
          <source>In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting &lt;code&gt;torch.backends.cudnn.deterministic =
True&lt;/code&gt;. Please see the notes on &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/randomness.html&quot;&gt;Reproducibility&lt;/a&gt; for background.</source>
          <target state="translated">在某些情况下，将CUDA后端与CuDNN一起使用时，该运算符可能会选择不确定的算法来提高性能。如果不希望这样做，可以通过设置 &lt;code&gt;torch.backends.cudnn.deterministic = True&lt;/code&gt; 来使操作具有确定性（可能以性能为代价）。请参阅有关可&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/randomness.html&quot;&gt;重现性&lt;/a&gt;的说明作为背景。</target>
        </trans-unit>
        <trans-unit id="e14a819b75469318610313772eb13bce47023e07" translate="yes" xml:space="preserve">
          <source>In the above example, aten::triu is not supported in ONNX, hence exporter falls back on this op. OperatorExportTypes.RAW: Export raw ir. OperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported in ONNX, fall through and export the operator as is, as a custom ONNX op. Using this mode, the op can be exported and implemented by the user for their runtime backend. Example graph:</source>
          <target state="translated">在上面的例子中,ONNX 不支持 aten::triu,因此出口商使用该操作。OperatorExportTypes.RAW:导出原始的ir。OperatorExportTypes.ONNX_FALLTHROUGH:如果 ONNX 不支持某个操作,则将其作为自定义的 ONNX 操作导出。使用这种模式,用户可以为其运行时的后台导出和实现操作。示例图。</target>
        </trans-unit>
        <trans-unit id="b3a5fd174642790bebe72eca626e159e3e5dfcd8" translate="yes" xml:space="preserve">
          <source>In the above example, prim::ListConstruct is not supported, hence exporter falls through.</source>
          <target state="translated">在上面的例子中,prim::ListConstruct不支持,因此exporter失败了。</target>
        </trans-unit>
        <trans-unit id="7c25e2887a622adc243b3c60db24b8cbea2277b3" translate="yes" xml:space="preserve">
          <source>In the case of batches of square matrices with size less or equal to 32 on a CUDA device, the LU factorization is repeated for singular matrices due to the bug in the MAGMA library (see magma issue 13).</source>
          <target state="translated">在CUDA设备上成批使用大小小于或等于32的正方形矩阵的情况下,由于MAGMA库的bug,对奇异矩阵重复进行LU分解(见magma问题13)。</target>
        </trans-unit>
        <trans-unit id="f0075edff8aa44b42fa791f6d655add581639408" translate="yes" xml:space="preserve">
          <source>In the example below, &lt;code&gt;swa_model&lt;/code&gt; is the SWA model that accumulates the averages of the weights. We train the model for a total of 300 epochs and we switch to the SWA learning rate schedule and start to collect SWA averages of the parameters at epoch 160:</source>
          <target state="translated">在下面的示例中， &lt;code&gt;swa_model&lt;/code&gt; 是累积权重平均值的SWA模型。我们对模型进行了总共300个纪元的训练，然后切换到SWA学习率时间表，并开始在纪元160收集参数的SWA平均值：</target>
        </trans-unit>
        <trans-unit id="7889605442c466f81697836a1b9ec1edd9e39e26" translate="yes" xml:space="preserve">
          <source>In the following table, we use 8 V100 GPUs, with CUDA 10.0 and CUDNN 7.4 to report the results. During training, we use a batch size of 2 per GPU, and during testing a batch size of 1 is used.</source>
          <target state="translated">在下表中,我们使用8个V100 GPU,使用CUDA 10.0和CUDNN 7.4来报告结果。在训练过程中,我们使用每个GPU的批处理量为2个,在测试过程中使用1个批处理量。</target>
        </trans-unit>
        <trans-unit id="33fa5776db9b9bbfad4e856c392d72e978768794" translate="yes" xml:space="preserve">
          <source>In the future, &lt;a href=&quot;#torch.conj&quot;&gt;&lt;code&gt;torch.conj()&lt;/code&gt;&lt;/a&gt; may return a non-writeable view for an &lt;code&gt;input&lt;/code&gt; of non-complex dtype. It&amp;rsquo;s recommended that programs not modify the tensor returned by &lt;a href=&quot;#torch.conj&quot;&gt;&lt;code&gt;torch.conj()&lt;/code&gt;&lt;/a&gt; when &lt;code&gt;input&lt;/code&gt; is of non-complex dtype to be compatible with this change.</source>
          <target state="translated">将来，&lt;a href=&quot;#torch.conj&quot;&gt; &lt;code&gt;torch.conj()&lt;/code&gt; &lt;/a&gt;可能会为非复杂dtype的 &lt;code&gt;input&lt;/code&gt; 返回不可写的视图。建议 &lt;code&gt;input&lt;/code&gt; 为非复杂&lt;a href=&quot;#torch.conj&quot;&gt; &lt;code&gt;torch.conj()&lt;/code&gt; &lt;/a&gt;时，程序不要修改torch.conj（）返回的张量，以与此更改兼容。</target>
        </trans-unit>
        <trans-unit id="99f1ee81dc5eba42240face3c5c428f4a78edd8a" translate="yes" xml:space="preserve">
          <source>In the future, there will be backends for other frameworks as well.</source>
          <target state="translated">未来,也会有其他框架的后端。</target>
        </trans-unit>
        <trans-unit id="d25cb11f14e2999086551374695bf04a7bfaf842" translate="yes" xml:space="preserve">
          <source>In the past, we were often asked: &amp;ldquo;which backend should I use?&amp;rdquo;.</source>
          <target state="translated">在过去，我们经常被问到：&amp;ldquo;我应该使用哪个后端？&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="0b0a51e5781bdbc893d9a5aff9fe6d5b37f0885c" translate="yes" xml:space="preserve">
          <source>In the returned &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;, operations that have different behaviors in &lt;code&gt;training&lt;/code&gt; and &lt;code&gt;eval&lt;/code&gt; modes will always behave as if it is in the mode it was in during tracing, no matter which mode the &lt;code&gt;ScriptModule&lt;/code&gt; is in.</source>
          <target state="translated">在返回的&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; 中&lt;/a&gt;，无论 &lt;code&gt;ScriptModule&lt;/code&gt; 处于哪种模式，在 &lt;code&gt;training&lt;/code&gt; 和 &lt;code&gt;eval&lt;/code&gt; 模式下具有不同行为的操作将始终像在跟踪过程中所处于的模式一样工作。</target>
        </trans-unit>
        <trans-unit id="b3daaa94feed4c186e9e9a97512618fa018aae7f" translate="yes" xml:space="preserve">
          <source>In the simplest case, the output value of the layer with input size</source>
          <target state="translated">在最简单的情况下,输入大小的层的输出值为</target>
        </trans-unit>
        <trans-unit id="239b7a59f23d668ac60e6721fee439268e244221" translate="yes" xml:space="preserve">
          <source>In the single-machine synchronous case, &lt;code&gt;torch.distributed&lt;/code&gt; or the &lt;a href=&quot;generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;torch.nn.parallel.DistributedDataParallel()&lt;/code&gt;&lt;/a&gt; wrapper may still have advantages over other approaches to data-parallelism, including &lt;a href=&quot;generated/torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt;&lt;code&gt;torch.nn.DataParallel()&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">在单机同步情况下， &lt;code&gt;torch.distributed&lt;/code&gt; 或&lt;a href=&quot;generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt; &lt;code&gt;torch.nn.parallel.DistributedDataParallel()&lt;/code&gt; &lt;/a&gt;包装器仍可能具有优于其他数据并行方法的优势，包括&lt;a href=&quot;generated/torch.nn.dataparallel#torch.nn.DataParallel&quot;&gt; &lt;code&gt;torch.nn.DataParallel()&lt;/code&gt; &lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="1e133f6f146acb51fdcd21b510ff4101dead133b" translate="yes" xml:space="preserve">
          <source>In the spatial (4-D) case, for &lt;code&gt;input&lt;/code&gt; with shape</source>
          <target state="translated">在空间（4-D）情况下，用于 &lt;code&gt;input&lt;/code&gt; 形状</target>
        </trans-unit>
        <trans-unit id="51ee81d82611ed15672cbe78ad3be970f77568fa" translate="yes" xml:space="preserve">
          <source>In the symbolic function, if the operator is already standardized in ONNX, we just need to create a node to represent the ONNX operator in the graph.</source>
          <target state="translated">在符号函数中,如果操作符已经在ONNX中标准化,我们只需要在图中创建一个节点来代表ONNX操作符。</target>
        </trans-unit>
        <trans-unit id="05f2edf0966166823005056e0975e4e593b57b37" translate="yes" xml:space="preserve">
          <source>In the symbolic function, if the operator is already standardized in ONNX, we only need to create a node to represent the ONNX operator in the graph.</source>
          <target state="translated">在符号函数中,如果操作符已经在ONNX中标准化,我们只需要在图中创建一个节点来表示ONNX操作符。</target>
        </trans-unit>
        <trans-unit id="84a6816bbe45c61bb1b6bef7593d5a6c612e3f5a" translate="yes" xml:space="preserve">
          <source>In these regions, CUDA ops run in an op-specific dtype chosen by autocast to improve performance while maintaining accuracy. See the &lt;a href=&quot;#autocast-op-reference&quot;&gt;Autocast Op Reference&lt;/a&gt; for details.</source>
          <target state="translated">在这些区域中，CUDA操作以自动广播选择的特定于操作的dtype运行，以提高性能并保持精度。有关详细信息，请参见《自动&lt;a href=&quot;#autocast-op-reference&quot;&gt;广播操作参考&lt;/a&gt;》。</target>
        </trans-unit>
        <trans-unit id="c41d2ba36e63a36e43970f33786546314d161009" translate="yes" xml:space="preserve">
          <source>In this case, &lt;code&gt;nn.Dropout2d()&lt;/code&gt; will help promote independence between feature maps and should be used instead.</source>
          <target state="translated">在这种情况下， &lt;code&gt;nn.Dropout2d()&lt;/code&gt; 将有助于促进要素图之间的独立性，应使用nn.Dropout2d（）代替。</target>
        </trans-unit>
        <trans-unit id="df2da228c2bd90611f132d29496ecf7b2c5d7aaa" translate="yes" xml:space="preserve">
          <source>In this case, &lt;code&gt;nn.Dropout3d()&lt;/code&gt; will help promote independence between feature maps and should be used instead.</source>
          <target state="translated">在这种情况下， &lt;code&gt;nn.Dropout3d()&lt;/code&gt; 将有助于促进要素图之间的独立性，应使用nn.Dropout3d（）代替。</target>
        </trans-unit>
        <trans-unit id="f8b3c9fa943c89a945d6617acdfae17b22a6204e" translate="yes" xml:space="preserve">
          <source>In this case, data-dependent control flow like this can be captured using &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;torch.jit.script()&lt;/code&gt;&lt;/a&gt; instead:</source>
          <target state="translated">在这种情况下，可以使用&lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;torch.jit.script()&lt;/code&gt; &lt;/a&gt;代替捕获类似数据的控制流：</target>
        </trans-unit>
        <trans-unit id="a5eaf16a960fd80b4157d86a90c080399cfb6b65" translate="yes" xml:space="preserve">
          <source>In this case, loading from a map-style dataset is roughly equivalent with:</source>
          <target state="translated">在这种情况下,从地图式数据集加载大致相当于与。</target>
        </trans-unit>
        <trans-unit id="0dd020ad3088741e2272795fe9b24fb663481ec2" translate="yes" xml:space="preserve">
          <source>In this mode, data fetching is done in the same process a &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; is initialized. Therefore, data loading may block computing. However, this mode may be preferred when resource(s) used for sharing data among processes (e.g., shared memory, file descriptors) is limited, or when the entire dataset is small and can be loaded entirely in memory. Additionally, single-process loading often shows more readable error traces and thus is useful for debugging.</source>
          <target state="translated">在这种模式下，数据获取是在初始化&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt;的同一过程中完成的。因此，数据加载可能会阻止计算。但是，当用于在进程之间共享数据的资源（例如共享内存，文件描述符）有限时，或者当整个数据集很小并且可以完全加载到内存中时，此模式可能是首选的。此外，单进程加载通常显示出更具可读性的错误跟踪，因此对于调试很有用。</target>
        </trans-unit>
        <trans-unit id="37db45d569ba92c31b1894c46ec1eedf759f889f" translate="yes" xml:space="preserve">
          <source>In this mode, each time an iterator of a &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; is created (e.g., when you call &lt;code&gt;enumerate(dataloader)&lt;/code&gt;), &lt;code&gt;num_workers&lt;/code&gt; worker processes are created. At this point, the &lt;code&gt;dataset&lt;/code&gt;, &lt;code&gt;collate_fn&lt;/code&gt;, and &lt;code&gt;worker_init_fn&lt;/code&gt; are passed to each worker, where they are used to initialize, and fetch data. This means that dataset access together with its internal IO, transforms (including &lt;code&gt;collate_fn&lt;/code&gt;) runs in the worker process.</source>
          <target state="translated">在这种模式下，每次创建&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt;的迭代器时（例如，当您调用 &lt;code&gt;enumerate(dataloader)&lt;/code&gt; 时）， &lt;code&gt;num_workers&lt;/code&gt; 创建num_workers工作进程。在这一点上，该 &lt;code&gt;dataset&lt;/code&gt; ， &lt;code&gt;collate_fn&lt;/code&gt; 和 &lt;code&gt;worker_init_fn&lt;/code&gt; 传递给每一个工人，在那里它们被用来初始化，并获取数据。这意味着数据集访问及其内部IO转换（包括 &lt;code&gt;collate_fn&lt;/code&gt; ）在工作进程中运行。</target>
        </trans-unit>
        <trans-unit id="85295238e8aebda5327b95ca72369c8df3c9eab9" translate="yes" xml:space="preserve">
          <source>In this mode, the result of every computation will have &lt;code&gt;requires_grad=False&lt;/code&gt;, even when the inputs have &lt;code&gt;requires_grad=True&lt;/code&gt;.</source>
          <target state="translated">在这种模式下，即使输入具有 &lt;code&gt;requires_grad=True&lt;/code&gt; ，每次计算的结果也将具有 &lt;code&gt;requires_grad=False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="91c8c0bce1c0356637c4656cf0a0cca8ed0c2a41" translate="yes" xml:space="preserve">
          <source>In this section please find the documentation for named tensor specific APIs. For a comprehensive reference for how names are propagated through other PyTorch operators, see &lt;a href=&quot;name_inference#name-inference-reference-doc&quot;&gt;Named Tensors operator coverage&lt;/a&gt;.</source>
          <target state="translated">在本节中，请找到特定于特定张量的API的文档。有关如何通过其他PyTorch运算符传播名称的全面参考，请参阅&lt;a href=&quot;name_inference#name-inference-reference-doc&quot;&gt;命名张量运算符coverage&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6458e074d9a813cc3dc5f6d0f9ad52155e927d58" translate="yes" xml:space="preserve">
          <source>In this variant, only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters.</source>
          <target state="translated">在这个变体中,只有出现在梯度中的时刻才会被更新,而且只有梯度的那些部分才会被应用到参数中。</target>
        </trans-unit>
        <trans-unit id="13102db919fdc00f95907c16ae125b4a1f45c313" translate="yes" xml:space="preserve">
          <source>In version 1.6 changed to this from set_training</source>
          <target state="translated">在1.6版本中,从set_training改成了这个。</target>
        </trans-unit>
        <trans-unit id="de12c6841ad61e706ed4075c85bb11d26176e1ff" translate="yes" xml:space="preserve">
          <source>In-place correctness checks</source>
          <target state="translated">就地检查正确性</target>
        </trans-unit>
        <trans-unit id="16a694343d462dfe3161b9f7e507c3e45c250203" translate="yes" xml:space="preserve">
          <source>In-place operations on Tensors</source>
          <target state="translated">对Tensors的就地操作</target>
        </trans-unit>
        <trans-unit id="90ef0f5bd340db15b402881b337b9e6fd1b54dd1" translate="yes" xml:space="preserve">
          <source>In-place random sampling</source>
          <target state="translated">就地随机抽样</target>
        </trans-unit>
        <trans-unit id="1723c07101158e28ed701a45820ffb2cbec954ae" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.abs&quot;&gt;&lt;code&gt;abs()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.abs&quot;&gt; &lt;code&gt;abs()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="24948f16983220b792808971db48b1bcdd5b3fa1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.absolute&quot;&gt;&lt;code&gt;absolute()&lt;/code&gt;&lt;/a&gt; Alias for &lt;a href=&quot;#torch.Tensor.abs_&quot;&gt;&lt;code&gt;abs_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.abs_&quot;&gt; &lt;code&gt;abs_()&lt;/code&gt; &lt;/a&gt;的&lt;a href=&quot;#torch.Tensor.absolute&quot;&gt; &lt;code&gt;absolute()&lt;/code&gt; &lt;/a&gt;别名</target>
        </trans-unit>
        <trans-unit id="dde70002d7610f16bccc15d518923eec4d107bba" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.acos&quot;&gt;&lt;code&gt;acos()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.acos&quot;&gt; &lt;code&gt;acos()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2f828fea297a593b936b38df78e2d795295f80f1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.acosh&quot;&gt;&lt;code&gt;acosh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.acosh&quot;&gt; &lt;code&gt;acosh()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6e2ef98fa9aa6aca8a1bdb5dad128434e35bceaa" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.add&quot;&gt;&lt;code&gt;add()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.add&quot;&gt; &lt;code&gt;add()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9afdb8ac221f8f75547e87484016d557ba50f247" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addbmm&quot;&gt;&lt;code&gt;addbmm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.addbmm&quot;&gt; &lt;code&gt;addbmm()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="dcf43a3561084dec90d7c3a2ecc1b6cc0c26f938" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addcdiv&quot;&gt;&lt;code&gt;addcdiv()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.addcdiv&quot;&gt; &lt;code&gt;addcdiv()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b52599a8d04e379d3f3c9ecc7af347c1222d473b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addcmul&quot;&gt;&lt;code&gt;addcmul()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.addcmul&quot;&gt; &lt;code&gt;addcmul()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="212bb18c46269c9b244d88318432c473793d6225" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addmm&quot;&gt;&lt;code&gt;addmm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.addmm&quot;&gt; &lt;code&gt;addmm()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7e04b090badbdcab058cb74254d5f97b547843c5" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addmv&quot;&gt;&lt;code&gt;addmv()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.addmv&quot;&gt; &lt;code&gt;addmv()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="714cae5e9e959037c53ed668822d40bf8774aa1b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.addr&quot;&gt;&lt;code&gt;addr()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.addr&quot;&gt; &lt;code&gt;addr()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="85808cd790e595c311410030bdf947098dfb3832" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arccos&quot;&gt;&lt;code&gt;arccos()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.arccos&quot;&gt; &lt;code&gt;arccos()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f222472fe1a711c429f88dfa21d735fc20269d70" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arccosh&quot;&gt;&lt;code&gt;arccosh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.arccosh&quot;&gt; &lt;code&gt;arccosh()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e5850fc97599dacb252ef7b71a0bc059e90ac832" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arcsin&quot;&gt;&lt;code&gt;arcsin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.arcsin&quot;&gt; &lt;code&gt;arcsin()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a92421400d2c919bc99b517f289d8339a3ae03e4" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arcsinh&quot;&gt;&lt;code&gt;arcsinh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.arcsinh&quot;&gt; &lt;code&gt;arcsinh()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ab6c8d62429cad7d186033198296a84d5c952598" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arctan&quot;&gt;&lt;code&gt;arctan()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.arctan&quot;&gt; &lt;code&gt;arctan()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e025d0bea89a2469024775370356f0fa69f44cc9" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.arctanh&quot;&gt;&lt;code&gt;arctanh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.arctanh&quot;&gt; &lt;code&gt;arctanh()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1eda7abb65e5c787ce66fb95749143a3f04888ed" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.asin&quot;&gt;&lt;code&gt;asin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.asin&quot;&gt; &lt;code&gt;asin()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0d258dbfd544c95327dd54668d3cb8823507d044" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.asinh&quot;&gt;&lt;code&gt;asinh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.asinh&quot;&gt; &lt;code&gt;asinh()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0e752ea6e17fecf2b883b074dbb3aa938a6f7d1c" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.atan&quot;&gt;&lt;code&gt;atan()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.atan&quot;&gt; &lt;code&gt;atan()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f00b83a4661950dbba55ca10e0c95538d6e98aaf" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.atan2&quot;&gt;&lt;code&gt;atan2()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.atan2&quot;&gt; &lt;code&gt;atan2()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9a9b90fa5b79edeea0ea7f804bf39ca483463ba1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.atanh&quot;&gt;&lt;code&gt;atanh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.atanh&quot;&gt; &lt;code&gt;atanh()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0cf21533d1ef19709a89c1226676212cc25b35e7" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.baddbmm&quot;&gt;&lt;code&gt;baddbmm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.baddbmm&quot;&gt; &lt;code&gt;baddbmm()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="723c45d84b8a47a2aac768faa389e89dbffdc312" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.bitwise_and&quot;&gt;&lt;code&gt;bitwise_and()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.bitwise_and&quot;&gt; &lt;code&gt;bitwise_and()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f23ac5a50549f7c4c93c7cc4af1a4a7fb14079c7" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.bitwise_not&quot;&gt;&lt;code&gt;bitwise_not()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.bitwise_not&quot;&gt; &lt;code&gt;bitwise_not()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1643902bef600d4d0d475acf131454814b25dc26" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.bitwise_or&quot;&gt;&lt;code&gt;bitwise_or()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.bitwise_or&quot;&gt; &lt;code&gt;bitwise_or()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fc90a184109b29c52ddf3915b630a1a54ae9634b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.bitwise_xor&quot;&gt;&lt;code&gt;bitwise_xor()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.bitwise_xor&quot;&gt; &lt;code&gt;bitwise_xor()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9b071498cdd10ed979300e38b8823b05d84b4e9a" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.ceil&quot;&gt;&lt;code&gt;ceil()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.ceil&quot;&gt; &lt;code&gt;ceil()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cd0a36194221856cef72e71a8bee0b50682fd547" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.clamp&quot;&gt;&lt;code&gt;clamp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.clamp&quot;&gt; &lt;code&gt;clamp()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="70bc593dd1c02b1636bd32a3b21498325eaaa85c" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.cos&quot;&gt;&lt;code&gt;cos()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.cos&quot;&gt; &lt;code&gt;cos()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="30b3045f3354737011026cef07419edca16e51b7" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.cosh&quot;&gt;&lt;code&gt;cosh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.cosh&quot;&gt; &lt;code&gt;cosh()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="85698a440de4a886c432db2c8ba93bb04b9783a5" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.digamma&quot;&gt;&lt;code&gt;digamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.digamma&quot;&gt; &lt;code&gt;digamma()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5b9d988d2d91968da3bc8e9addd60ca857a07078" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.div&quot;&gt;&lt;code&gt;div()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.div&quot;&gt; &lt;code&gt;div()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="40f813960ae0cb30555d66235642030e14081a78" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.divide&quot;&gt;&lt;code&gt;divide()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.divide&quot;&gt; &lt;code&gt;divide()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3593d29a80b1997092e2a7caab1ae4c89ff665ad" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.eq&quot;&gt;&lt;code&gt;eq()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.eq&quot;&gt; &lt;code&gt;eq()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a597279c8f26129b927bac941304ed95baa85e3a" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.erf&quot;&gt;&lt;code&gt;erf()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.erf&quot;&gt; &lt;code&gt;erf()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cbb6ddfcf060f64c41fb12dadf24c5f4104c8248" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.erfc&quot;&gt;&lt;code&gt;erfc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.erfc&quot;&gt; &lt;code&gt;erfc()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9d3f26f32a7d82b45bafe3bb5a09586684935c0d" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.erfinv&quot;&gt;&lt;code&gt;erfinv()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.erfinv&quot;&gt; &lt;code&gt;erfinv()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="843b7383da05ed2bdd2c3d50d63831578e83d793" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.exp&quot;&gt;&lt;code&gt;exp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.exp&quot;&gt; &lt;code&gt;exp()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="592c5733a1bb63ac93435062ef6cf042a3e0764b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.expm1&quot;&gt;&lt;code&gt;expm1()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.expm1&quot;&gt; &lt;code&gt;expm1()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d5d395d973d45d767241ba61182b45ea933b25b3" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.fix&quot;&gt;&lt;code&gt;fix()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.fix&quot;&gt; &lt;code&gt;fix()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a0ad235d8de2027be5bcf6998afc520cb85c9fc4" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.floor&quot;&gt;&lt;code&gt;floor()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.floor&quot;&gt; &lt;code&gt;floor()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1298ca9624e848339b14dbd7ac9e1cc72e8426d5" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.floor_divide&quot;&gt;&lt;code&gt;floor_divide()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本&lt;a href=&quot;#torch.Tensor.floor_divide&quot;&gt; &lt;code&gt;floor_divide()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ba34126e0ac2c59a401a9332bbc39d964a9f1676" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.fmod&quot;&gt;&lt;code&gt;fmod()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.fmod&quot;&gt; &lt;code&gt;fmod()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="05577f4319cd8a9afef3327e267c8259cd201bc1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.frac&quot;&gt;&lt;code&gt;frac()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.frac&quot;&gt; &lt;code&gt;frac()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2c9a5d890ffefe6a4582f4b8a23dc1a024530071" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.gcd&quot;&gt;&lt;code&gt;gcd()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.gcd&quot;&gt; &lt;code&gt;gcd()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="64f7412f381bbec829d7e064308cdeedbdb4239d" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.ge&quot;&gt;&lt;code&gt;ge()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.ge&quot;&gt; &lt;code&gt;ge()&lt;/code&gt; &lt;/a&gt;的就地版本。</target>
        </trans-unit>
        <trans-unit id="c4e0348ba557fd7f53f8d998e67417ad216e0ca1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.greater&quot;&gt;&lt;code&gt;greater()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.greater&quot;&gt; &lt;code&gt;greater()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c8558b60a835a6d4d07c498aab1d1c2aee7927ef" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.greater_equal&quot;&gt;&lt;code&gt;greater_equal()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.greater_equal&quot;&gt; &lt;code&gt;greater_equal()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="23ff3a9c46bb69e3bf80c84bd30a30c21ea9397e" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.gt&quot;&gt;&lt;code&gt;gt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.gt&quot;&gt; &lt;code&gt;gt()&lt;/code&gt; &lt;/a&gt;的就地版本。</target>
        </trans-unit>
        <trans-unit id="0101731065c7560702b5f46c4e91cdc9923c8847" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.hypot&quot;&gt;&lt;code&gt;hypot()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.hypot&quot;&gt; &lt;code&gt;hypot()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2ae74e50f4d266279d834bc1480fb185d3f5523f" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.i0&quot;&gt;&lt;code&gt;i0()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.i0&quot;&gt; &lt;code&gt;i0()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a9c7375b3919548ed0ffd6165fe0af4e23e758d4" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.lcm&quot;&gt;&lt;code&gt;lcm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.lcm&quot;&gt; &lt;code&gt;lcm()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c39b8eebf07573ad5eac2472d11ac00d3ec49609" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.le&quot;&gt;&lt;code&gt;le()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.le&quot;&gt; &lt;code&gt;le()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="de77dd1c89036ad69daa80304475b1011fb3301e" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.lerp&quot;&gt;&lt;code&gt;lerp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.lerp&quot;&gt; &lt;code&gt;lerp()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c58f50cdbebdcfb188e2bdfa9fcdc829890f845d" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.less&quot;&gt;&lt;code&gt;less()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.less&quot;&gt; &lt;code&gt;less()&lt;/code&gt; &lt;/a&gt;的就地版本。</target>
        </trans-unit>
        <trans-unit id="b9423b0d6372fd7e2833e62f8095c74ec7f8c46f" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.less_equal&quot;&gt;&lt;code&gt;less_equal()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.less_equal&quot;&gt; &lt;code&gt;less_equal()&lt;/code&gt; &lt;/a&gt;的就地版本。</target>
        </trans-unit>
        <trans-unit id="6e2671a3a2693a72154fb41d1a6e377bc20e43a8" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.lgamma&quot;&gt;&lt;code&gt;lgamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.lgamma&quot;&gt; &lt;code&gt;lgamma()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9af05e667e56282be6af4fe5b0246f9f857e3a1d" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.log&quot;&gt;&lt;code&gt;log()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.log&quot;&gt; &lt;code&gt;log()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c10939ba9c9c50755fefd56b392ef3407792f705" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.log10&quot;&gt;&lt;code&gt;log10()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.log10&quot;&gt; &lt;code&gt;log10()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1458e566ebdc0a91bfc06212d913d1e13187212c" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.log1p&quot;&gt;&lt;code&gt;log1p()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.log1p&quot;&gt; &lt;code&gt;log1p()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b4555443ad8a2acf3c819dcdfbbef49a10a2e972" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.log2&quot;&gt;&lt;code&gt;log2()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.log2&quot;&gt; &lt;code&gt;log2()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="433b0aeae51de4808da5b68a9beefef42daf02ce" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.logical_and&quot;&gt;&lt;code&gt;logical_and()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.logical_and&quot;&gt; &lt;code&gt;logical_and()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c70b5c1f8176e07af5e81b152a7bdfca820fb3f9" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.logical_not&quot;&gt;&lt;code&gt;logical_not()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.logical_not&quot;&gt; &lt;code&gt;logical_not()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d91ce8af603413f0aefc2d1b8b6eec1f114d044b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.logical_or&quot;&gt;&lt;code&gt;logical_or()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.logical_or&quot;&gt; &lt;code&gt;logical_or()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3109d25b88385322d682d606fa91db53903884b9" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.logical_xor&quot;&gt;&lt;code&gt;logical_xor()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.logical_xor&quot;&gt; &lt;code&gt;logical_xor()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="343ccb07f42d386eca14f79b12f59de018d60e86" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.logit&quot;&gt;&lt;code&gt;logit()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.logit&quot;&gt; &lt;code&gt;logit()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9d856395e389956dab5ecceb9942b3af92fe9ea1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.lt&quot;&gt;&lt;code&gt;lt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.lt&quot;&gt; &lt;code&gt;lt()&lt;/code&gt; &lt;/a&gt;的就地版本。</target>
        </trans-unit>
        <trans-unit id="23d64d86cc2e5f1327a6e010cdeb7c115938b177" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.mul&quot;&gt;&lt;code&gt;mul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.mul&quot;&gt; &lt;code&gt;mul()&lt;/code&gt; &lt;/a&gt;的就地版本。</target>
        </trans-unit>
        <trans-unit id="320ff1f84d1c7cfe2a1624b610ddcce8af66a764" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.multiply&quot;&gt;&lt;code&gt;multiply()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.multiply&quot;&gt; &lt;code&gt;multiply()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="d10598fcd6504f5f6bb4b9ddb4d89823cb2a72a4" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.mvlgamma&quot;&gt;&lt;code&gt;mvlgamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.mvlgamma&quot;&gt; &lt;code&gt;mvlgamma()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b3a59563ae4042d5dfe02b4b4d28142c18b881db" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.ne&quot;&gt;&lt;code&gt;ne()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.ne&quot;&gt; &lt;code&gt;ne()&lt;/code&gt; &lt;/a&gt;的就地版本。</target>
        </trans-unit>
        <trans-unit id="0933e5f6d727692e6688d039a1e1af9fcbe844b2" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.neg&quot;&gt;&lt;code&gt;neg()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.neg&quot;&gt; &lt;code&gt;neg()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6b0826ad65145f6f9f94c5d871b782df9fd07711" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.negative&quot;&gt;&lt;code&gt;negative()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.negative&quot;&gt; &lt;code&gt;negative()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9a9845a362319d27c686423e43aadf669c3c68aa" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.nextafter&quot;&gt;&lt;code&gt;nextafter()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就位版本的&lt;a href=&quot;#torch.Tensor.nextafter&quot;&gt; &lt;code&gt;nextafter()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="28a1d8909a6e02b4e7751f03f7b62a7e62637734" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.not_equal&quot;&gt;&lt;code&gt;not_equal()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.not_equal&quot;&gt; &lt;code&gt;not_equal()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="f16d420f4d416e8798bbbadc72fbaac73ed31ae3" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.polygamma&quot;&gt;&lt;code&gt;polygamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.polygamma&quot;&gt; &lt;code&gt;polygamma()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="59d753fe8af8e0e2fdbff27f828f4dc81a011e2f" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.pow&quot;&gt;&lt;code&gt;pow()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.pow&quot;&gt; &lt;code&gt;pow()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="52dd6f8c275f23765de5ee7462ecb62204684869" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.reciprocal&quot;&gt;&lt;code&gt;reciprocal()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.reciprocal&quot;&gt; &lt;code&gt;reciprocal()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ca45cdac88c7921f089b79a8f83e20d848c48d4a" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.remainder&quot;&gt;&lt;code&gt;remainder()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.remainder&quot;&gt; &lt;code&gt;remainder()&lt;/code&gt; &lt;/a&gt;版本的就地版本</target>
        </trans-unit>
        <trans-unit id="053eb7c5cd301fdd12904be2836f5fb0ec275646" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.rename&quot;&gt;&lt;code&gt;rename()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.rename&quot;&gt; &lt;code&gt;rename()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="818df20d8449f1acb432a28eac3f0b43768c2197" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.renorm&quot;&gt;&lt;code&gt;renorm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.renorm&quot;&gt; &lt;code&gt;renorm()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="53ce6f2d6ab6ae80005a428178e9b77aef382107" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.round&quot;&gt;&lt;code&gt;round()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.round&quot;&gt; &lt;code&gt;round()&lt;/code&gt; &lt;/a&gt;的就地版本</target>
        </trans-unit>
        <trans-unit id="b623bd7302eab7378ff509a5b3dd4f0470b21494" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.rsqrt&quot;&gt;&lt;code&gt;rsqrt()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.rsqrt&quot;&gt; &lt;code&gt;rsqrt()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1fe0cf7810c0bc56d540daa9ddb7cec1599e41ee" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sgn&quot;&gt;&lt;code&gt;sgn()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.sgn&quot;&gt; &lt;code&gt;sgn()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cdcdd0daf61ad10208f07b04eb0183a9d4d2ab44" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sigmoid&quot;&gt;&lt;code&gt;sigmoid()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.sigmoid&quot;&gt; &lt;code&gt;sigmoid()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9e2cd3c0abcebc8ed45bd6570406d8c9d4c33ea1" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sign&quot;&gt;&lt;code&gt;sign()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.sign&quot;&gt; &lt;code&gt;sign()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="69d6918f4d893ee927f13c625b9f268b4eea1c8b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sin&quot;&gt;&lt;code&gt;sin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.sin&quot;&gt; &lt;code&gt;sin()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7f906cf7cca8e4f9df35be1db35c6f3f24ecf45b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sinh&quot;&gt;&lt;code&gt;sinh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.sinh&quot;&gt; &lt;code&gt;sinh()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="cbe8e3f4ea9deb4f23b9e270df8f53c6c9f24e99" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sqrt&quot;&gt;&lt;code&gt;sqrt()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.sqrt&quot;&gt; &lt;code&gt;sqrt()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8821b663c3d7115389c6fb460a014cd6f46b31a6" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.square&quot;&gt;&lt;code&gt;square()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.square&quot;&gt; &lt;code&gt;square()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="331d4255e6beb57fd7b54b749ce35473d9c934d2" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.squeeze&quot;&gt;&lt;code&gt;squeeze()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.squeeze&quot;&gt; &lt;code&gt;squeeze()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9c0246ff36c2a3383754e9573043424c772cead3" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.sub&quot;&gt;&lt;code&gt;sub()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.sub&quot;&gt; &lt;code&gt;sub()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="afadc08d16c14a6f3ab63ac2e1777d4a0c639d3d" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.subtract&quot;&gt;&lt;code&gt;subtract()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.subtract&quot;&gt; &lt;code&gt;subtract()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6e9a157c014d95de0d85c9410108016dcf899ffc" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.t&quot;&gt;&lt;code&gt;t()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.t&quot;&gt; &lt;code&gt;t()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c2bca115f72f0be4030271dbbf4001e5646c23cd" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.tan&quot;&gt;&lt;code&gt;tan()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.tan&quot;&gt; &lt;code&gt;tan()&lt;/code&gt; &lt;/a&gt;的就地版本</target>
        </trans-unit>
        <trans-unit id="abae0cb9439d45a19abc17669152c5675c6b2f91" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.tanh&quot;&gt;&lt;code&gt;tanh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.tanh&quot;&gt; &lt;code&gt;tanh()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e2861f743f86145802065db61cf1289846a4dadc" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.transpose&quot;&gt;&lt;code&gt;transpose()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.transpose&quot;&gt; &lt;code&gt;transpose()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="45499df8d625edfd4a83589eb3c8afa20bd32bb9" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.tril&quot;&gt;&lt;code&gt;tril()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.tril&quot;&gt; &lt;code&gt;tril()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="065bb546251aa96738ef601a77cd6f37ec7d9659" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.triu&quot;&gt;&lt;code&gt;triu()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.triu&quot;&gt; &lt;code&gt;triu()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="212e370e8fdc9518bb1c8dd81fce7b074d0a6998" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.true_divide_&quot;&gt;&lt;code&gt;true_divide_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.true_divide_&quot;&gt; &lt;code&gt;true_divide_()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e29d58f0e79f4e7e5e919543670aacaa5b39af55" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.trunc&quot;&gt;&lt;code&gt;trunc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.trunc&quot;&gt; &lt;code&gt;trunc()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1e2464f4a6dc92f29b5fe948a1cb58f6a6f8ec52" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.Tensor.unsqueeze&quot;&gt;&lt;code&gt;unsqueeze()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.Tensor.unsqueeze&quot;&gt; &lt;code&gt;unsqueeze()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="40042c187d9c08cff699d8c486b17c8fc8e0f71b" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.elu&quot;&gt;&lt;code&gt;elu()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.nn.functional.elu&quot;&gt; &lt;code&gt;elu()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="fda2e5a23576045d8c887926b8f5f495cbb00502" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.hardtanh&quot;&gt;&lt;code&gt;hardtanh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.nn.functional.hardtanh&quot;&gt; &lt;code&gt;hardtanh()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4e00c79886a4c5fdfdd4fc0c5274b0108988ba92" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.leaky_relu&quot;&gt;&lt;code&gt;leaky_relu()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.nn.functional.leaky_relu&quot;&gt; &lt;code&gt;leaky_relu()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="eda3fb8c00a121092ce0d25f989c8256cb992365" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.relu&quot;&gt;&lt;code&gt;relu()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.nn.functional.relu&quot;&gt; &lt;code&gt;relu()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e3a76eb13d927dc0ac3b8f72061ef5fd006d9d68" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.rrelu&quot;&gt;&lt;code&gt;rrelu()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.nn.functional.rrelu&quot;&gt; &lt;code&gt;rrelu()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="cf12b7cfa392abc73035a860cd7f9148c1c352d9" translate="yes" xml:space="preserve">
          <source>In-place version of &lt;a href=&quot;#torch.nn.functional.threshold&quot;&gt;&lt;code&gt;threshold()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">就地版本的&lt;a href=&quot;#torch.nn.functional.threshold&quot;&gt; &lt;code&gt;threshold()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0b7ad9a8774865b3951b77d446d4e908b37c2715" translate="yes" xml:space="preserve">
          <source>Inception (warning: this model is highly sensitive to changes in operator implementation)</source>
          <target state="translated">启用(警告:该模型对操作者实施中的变化高度敏感)。</target>
        </trans-unit>
        <trans-unit id="86187099fc5837d4a16585e59250186b1bdbfd39" translate="yes" xml:space="preserve">
          <source>Inception v3</source>
          <target state="translated">Inception v3</target>
        </trans-unit>
        <trans-unit id="512c1383f2d8169cfead07824d0994b018f42ff3" translate="yes" xml:space="preserve">
          <source>Inception v3 model architecture from &lt;a href=&quot;http://arxiv.org/abs/1512.00567&quot;&gt;&amp;ldquo;Rethinking the Inception Architecture for Computer Vision&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;http://arxiv.org/abs/1512.00567&quot;&gt;&amp;ldquo;重新思考计算机视觉的初始架构&amp;rdquo;中的&lt;/a&gt;Inception v3模型架构。</target>
        </trans-unit>
        <trans-unit id="bd5fe5285febc19f7457223bee13ebc30e42633a" translate="yes" xml:space="preserve">
          <source>Inception v3 model architecture from &lt;a href=&quot;https://arxiv.org/abs/1512.00567&quot;&gt;&amp;ldquo;Rethinking the Inception Architecture for Computer Vision&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1512.00567&quot;&gt;&amp;ldquo;重新思考计算机视觉的初始架构&amp;rdquo;中的&lt;/a&gt;Inception v3模型架构。</target>
        </trans-unit>
        <trans-unit id="228505f85662d947dfd09fcfb9c5416638b8b2ae" translate="yes" xml:space="preserve">
          <source>Independent</source>
          <target state="translated">Independent</target>
        </trans-unit>
        <trans-unit id="c2df9b932637fe9d32a0f16da1c11873398f873d" translate="yes" xml:space="preserve">
          <source>Index</source>
          <target state="translated">Index</target>
        </trans-unit>
        <trans-unit id="6f4e6ca52449e10a302a5dfc3ebba098cd7e6757" translate="yes" xml:space="preserve">
          <source>Indexing, Slicing, Joining, Mutating Ops</source>
          <target state="translated">索引、切片、合并、突变操作。</target>
        </trans-unit>
        <trans-unit id="5e2d7833039dc978e6eb0d1055910e03a86a4609" translate="yes" xml:space="preserve">
          <source>Indices and tables</source>
          <target state="translated">指数和表格</target>
        </trans-unit>
        <trans-unit id="4eb1cf386795f140ff47153f7333aea4dda421e5" translate="yes" xml:space="preserve">
          <source>Indices are ordered from left to right according to when each was sampled (first samples are placed in first column).</source>
          <target state="translated">指数从左到右按照每个样本的取样时间排序(第一个样本放在第一列)。</target>
        </trans-unit>
        <trans-unit id="68fa16ffd48f366e4fa8d57fea78ff03fcab0191" translate="yes" xml:space="preserve">
          <source>Initialization</source>
          <target state="translated">Initialization</target>
        </trans-unit>
        <trans-unit id="8a509cef8fe7210eb4a6fc80831a2072673fceae" translate="yes" xml:space="preserve">
          <source>Initialize PyTorch&amp;rsquo;s CUDA state. You may need to call this explicitly if you are interacting with PyTorch via its C API, as Python bindings for CUDA functionality will not be available until this initialization takes place. Ordinary users should not need this, as all of PyTorch&amp;rsquo;s CUDA methods automatically initialize CUDA state on-demand.</source>
          <target state="translated">初始化PyTorch的CUDA状态。如果您通过PyTorch的C API与PyTorch进行交互，则可能需要显式调用此方法，因为在进行初始化之前，无法使用CUDA功能的Python绑定。普通用户不需要此，因为所有PyTorch的CUDA方法都会自动按需初始化CUDA状态。</target>
        </trans-unit>
        <trans-unit id="73e5b96b48f418d64840badaa42ca942b71c67be" translate="yes" xml:space="preserve">
          <source>Initializes RPC primitives such as the local RPC agent and distributed autograd, which immediately makes the current process ready to send and receive RPCs.</source>
          <target state="translated">初始化 RPC 基元,如本地 RPC 代理和分布式 autograd,使当前进程立即准备好发送和接收 RPC。</target>
        </trans-unit>
        <trans-unit id="bd0f315e911df950ab4124032aa329f129570289" translate="yes" xml:space="preserve">
          <source>Initializes the default distributed process group, and this will also initialize the distributed package.</source>
          <target state="translated">初始化默认的分布式进程组,这也将初始化分布式软件包。</target>
        </trans-unit>
        <trans-unit id="252c7bda7950bbedbb2a4bb4550ac0f5ef1aa6a8" translate="yes" xml:space="preserve">
          <source>Input lists. It should contain correctly-sized tensors on each GPU to be used for input of the collective, e.g. &lt;code&gt;input_tensor_lists[i]&lt;/code&gt; contains the reduce_scatter input that resides on the GPU of &lt;code&gt;output_tensor_list[i]&lt;/code&gt;.</source>
          <target state="translated">输入列表。它应该在每个GPU上包含正确大小的张量，以用于集合的输入，例如 &lt;code&gt;input_tensor_lists[i]&lt;/code&gt; 包含位于 &lt;code&gt;output_tensor_list[i]&lt;/code&gt; 的GPU上的reduce_scatter输入。</target>
        </trans-unit>
        <trans-unit id="5bba22431efd0a63a04193e3ddac4464b3801e67" translate="yes" xml:space="preserve">
          <source>Input1:</source>
          <target state="translated">Input1:</target>
        </trans-unit>
        <trans-unit id="feac2b2649c90f11d6d855888f274f7a0752b7d9" translate="yes" xml:space="preserve">
          <source>Input2:</source>
          <target state="translated">Input2:</target>
        </trans-unit>
        <trans-unit id="79d70dcb4f9ee8b7d94ed9539586cc73c0d399da" translate="yes" xml:space="preserve">
          <source>Input:</source>
          <target state="translated">Input:</target>
        </trans-unit>
        <trans-unit id="7b180c0fda0377ef1dc31c7cce34da732fc57c9e" translate="yes" xml:space="preserve">
          <source>Input: LongTensor of arbitrary shape containing the indices to extract</source>
          <target state="translated">Input.任意形状的LongTensor,包含要提取的指数。任意形状的LongTensor,包含要提取的指数。</target>
        </trans-unit>
        <trans-unit id="15c3ba090d23cb0ca95577aa6799755edeefe016" translate="yes" xml:space="preserve">
          <source>Input_lengths: Tuple or tensor of size</source>
          <target state="translated">Input_lengths:元组或张量,大小为</target>
        </trans-unit>
        <trans-unit id="fcadc5a2f2ce33e2ebb11bf51a1dd2322a5a729a" translate="yes" xml:space="preserve">
          <source>Inputs:</source>
          <target state="translated">Inputs:</target>
        </trans-unit>
        <trans-unit id="f969dd5ae0f7325be616e39a51b06a49fc60e816" translate="yes" xml:space="preserve">
          <source>Inputs: input, (h_0, c_0)</source>
          <target state="translated">输入:输入,(h_0,c_0)</target>
        </trans-unit>
        <trans-unit id="1be4f879f896bd3dcbeaf30b825be75ca856fcfb" translate="yes" xml:space="preserve">
          <source>Inputs: input, h_0</source>
          <target state="translated">输入:输入,h_0</target>
        </trans-unit>
        <trans-unit id="71bf746d2a9f5ff1025d4c04b2cf1fcfbd657691" translate="yes" xml:space="preserve">
          <source>Inputs: input, hidden</source>
          <target state="translated">输入:输入、隐藏</target>
        </trans-unit>
        <trans-unit id="7f3d3cd091d557867390efc2557a3f5b19292265" translate="yes" xml:space="preserve">
          <source>Insert a given module before a given index in the list.</source>
          <target state="translated">在列表中给定的模块前插入一个给定的索引。</target>
        </trans-unit>
        <trans-unit id="50d8055b5be81f5be9d6435d07e2a4ac2451cc98" translate="yes" xml:space="preserve">
          <source>Inserts the key-value pair into the store based on the supplied &lt;code&gt;key&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt;. If &lt;code&gt;key&lt;/code&gt; already exists in the store, it will overwrite the old value with the new supplied &lt;code&gt;value&lt;/code&gt;.</source>
          <target state="translated">根据提供的 &lt;code&gt;key&lt;/code&gt; 和 &lt;code&gt;value&lt;/code&gt; 将键值对插入商店。如果商店中已经存在 &lt;code&gt;key&lt;/code&gt; ，它将用新提供的 &lt;code&gt;value&lt;/code&gt; 覆盖旧值。</target>
        </trans-unit>
        <trans-unit id="52616e80a59172caf19bcc15fac4910844e40524" translate="yes" xml:space="preserve">
          <source>Inspecting Code</source>
          <target state="translated">检查代码</target>
        </trans-unit>
        <trans-unit id="9efecc53883d2ffb75b3f04810dbaac718af41ba" translate="yes" xml:space="preserve">
          <source>InstanceNorm1d</source>
          <target state="translated">InstanceNorm1d</target>
        </trans-unit>
        <trans-unit id="a1ce20a07b568c5cbed5860b3c884130e1c4f948" translate="yes" xml:space="preserve">
          <source>InstanceNorm2d</source>
          <target state="translated">InstanceNorm2d</target>
        </trans-unit>
        <trans-unit id="f8a9e223352c23318d808b3147734411baf32655" translate="yes" xml:space="preserve">
          <source>InstanceNorm3d</source>
          <target state="translated">InstanceNorm3d</target>
        </trans-unit>
        <trans-unit id="c54533fbd7a6f02423b7a436a597e8275c12dc4d" translate="yes" xml:space="preserve">
          <source>Instances of &lt;a href=&quot;#torch.cuda.amp.autocast&quot;&gt;&lt;code&gt;autocast&lt;/code&gt;&lt;/a&gt; serve as context managers or decorators that allow regions of your script to run in mixed precision.</source>
          <target state="translated">自动&lt;a href=&quot;#torch.cuda.amp.autocast&quot;&gt; &lt;code&gt;autocast&lt;/code&gt; &lt;/a&gt;实例充当上下文管理器或装饰器，使脚本的区域可以混合精度运行。</target>
        </trans-unit>
        <trans-unit id="8ab3500f668dc459ea6ccd8a8b7456bb53bb532b" translate="yes" xml:space="preserve">
          <source>Instances of this class should never be created manually. They are meant to be instantiated by functions like &lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt;&lt;code&gt;pack_padded_sequence()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">此类的实例永远不要手动创建。它们应通过诸如&lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt; &lt;code&gt;pack_padded_sequence()&lt;/code&gt; 之&lt;/a&gt;类的函数实例化。</target>
        </trans-unit>
        <trans-unit id="5a54ceeb7f75257e5faecf7dac25c3c19332eb52" translate="yes" xml:space="preserve">
          <source>Instancing a pre-trained model will download its weights to a cache directory. This directory can be set using the &lt;code&gt;TORCH_MODEL_ZOO&lt;/code&gt; environment variable. See &lt;a href=&quot;../model_zoo#torch.utils.model_zoo.load_url&quot;&gt;&lt;code&gt;torch.utils.model_zoo.load_url()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">实例化预训练的模型会将其权重下载到缓存目录中。可以使用 &lt;code&gt;TORCH_MODEL_ZOO&lt;/code&gt; 环境变量来设置此目录。有关详细信息，请参见&lt;a href=&quot;../model_zoo#torch.utils.model_zoo.load_url&quot;&gt; &lt;code&gt;torch.utils.model_zoo.load_url()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4156151a1b281cbc4bcd7e0576424486039cddfe" translate="yes" xml:space="preserve">
          <source>Integer division with addcdiv is no longer supported, and in a future release addcdiv will perform a true division of tensor1 and tensor2. The historic addcdiv behavior can be implemented as (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype) for integer inputs and as (input + value * tensor1 / tensor2) for float inputs. The future addcdiv behavior is just the latter implementation: (input + value * tensor1 / tensor2), for all dtypes.</source>
          <target state="translated">不再支持使用addcdiv进行整数除法,在未来的版本中,addcdiv将对tensor1和tensor2进行真正的除法。历史上的addcdiv行为对于整数输入可以实现为(input+value*torch.trunc(tensor1/tensor2)).to(input.dtype),对于浮点数输入可以实现为(input+value*tensor1/tensor2)。未来的addcdiv行为只是后者的实现。(input+value*tensor1/tensor2),适用于所有dtype。</target>
        </trans-unit>
        <trans-unit id="2c54682be64b87caa364aa75498c5eaafa212bed" translate="yes" xml:space="preserve">
          <source>Internally invokes &lt;code&gt;unscale_(optimizer)&lt;/code&gt; (unless &lt;a href=&quot;#torch.cuda.amp.GradScaler.unscale_&quot;&gt;&lt;code&gt;unscale_()&lt;/code&gt;&lt;/a&gt; was explicitly called for &lt;code&gt;optimizer&lt;/code&gt; earlier in the iteration). As part of the &lt;a href=&quot;#torch.cuda.amp.GradScaler.unscale_&quot;&gt;&lt;code&gt;unscale_()&lt;/code&gt;&lt;/a&gt;, gradients are checked for infs/NaNs.</source>
          <target state="translated">在内部调用 &lt;code&gt;unscale_(optimizer)&lt;/code&gt; （除非&lt;a href=&quot;#torch.cuda.amp.GradScaler.unscale_&quot;&gt; &lt;code&gt;unscale_()&lt;/code&gt; &lt;/a&gt;被明确要求 &lt;code&gt;optimizer&lt;/code&gt; 早些时候迭代）。作为&lt;a href=&quot;#torch.cuda.amp.GradScaler.unscale_&quot;&gt; &lt;code&gt;unscale_()&lt;/code&gt; 的&lt;/a&gt;一部分，将检查梯度的infs / NaNs。</target>
        </trans-unit>
        <trans-unit id="9f650ccc7d0d6be8b0fb7120cf085b2ff78498f0" translate="yes" xml:space="preserve">
          <source>Interpreting Graphs</source>
          <target state="translated">解读图表</target>
        </trans-unit>
        <trans-unit id="ab6b952b1b568a27a27bc4ef9eccfa0e4506f5e1" translate="yes" xml:space="preserve">
          <source>Interpreting the output of this function requires familiarity with the memory allocator internals.</source>
          <target state="translated">解释这个函数的输出需要熟悉内存分配器的内部结构。</target>
        </trans-unit>
        <trans-unit id="e42baee8c04a7fb48aeb23c27863b3e7f99c9a61" translate="yes" xml:space="preserve">
          <source>Inverse short time Fourier Transform.</source>
          <target state="translated">逆短时傅里叶变换。</target>
        </trans-unit>
        <trans-unit id="f2d2a372360fc80cd1c1832163878137a219bb55" translate="yes" xml:space="preserve">
          <source>Inverse short time Fourier Transform. This is expected to be the inverse of &lt;a href=&quot;torch.stft#torch.stft&quot;&gt;&lt;code&gt;stft()&lt;/code&gt;&lt;/a&gt;. It has the same parameters (+ additional optional parameter of &lt;code&gt;length&lt;/code&gt;) and it should return the least squares estimation of the original signal. The algorithm will check using the NOLA condition ( nonzero overlap).</source>
          <target state="translated">短时傅立叶逆变换。预期这是&lt;a href=&quot;torch.stft#torch.stft&quot;&gt; &lt;code&gt;stft()&lt;/code&gt; &lt;/a&gt;的反函数。它具有相同的参数（+ &lt;code&gt;length&lt;/code&gt; 的附加可选参数），并且应返回原始信号的最小二乘估计。该算法将使用NOLA条件（非零重叠）进行检查。</target>
        </trans-unit>
        <trans-unit id="4212bbbb75b521f6e55e2e79c4b2dab9598d7c21" translate="yes" xml:space="preserve">
          <source>Invoking &lt;code&gt;trace&lt;/code&gt; with a module&amp;rsquo;s method captures module parameters (which may require gradients) as &lt;strong&gt;constants&lt;/strong&gt;.</source>
          <target state="translated">使用模块的方法调用 &lt;code&gt;trace&lt;/code&gt; 会将模块参数（可能需要渐变）捕获为&lt;strong&gt;常量&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="0b1154e9d45e9b9a76119f9cdf7dd156a3cc44ad" translate="yes" xml:space="preserve">
          <source>Irrespective of the original strides, the returned matrices &lt;code&gt;solution&lt;/code&gt; and &lt;code&gt;LU&lt;/code&gt; will be transposed, i.e. with strides like &lt;code&gt;B.contiguous().transpose(-1, -2).stride()&lt;/code&gt; and &lt;code&gt;A.contiguous().transpose(-1, -2).stride()&lt;/code&gt; respectively.</source>
          <target state="translated">不管原始步幅如何，返回的矩阵 &lt;code&gt;solution&lt;/code&gt; 和 &lt;code&gt;LU&lt;/code&gt; 都将进行转置，即使用诸如 &lt;code&gt;B.contiguous().transpose(-1, -2).stride()&lt;/code&gt; 和 &lt;code&gt;A.contiguous().transpose(-1, -2).stride()&lt;/code&gt; ）之类的步幅。 -2）.stride（）。</target>
        </trans-unit>
        <trans-unit id="49986ad18b9e3b5d2fddd594cfccd882001d60e0" translate="yes" xml:space="preserve">
          <source>Irrespective of the original strides, the returned matrix &lt;code&gt;U&lt;/code&gt; will be transposed, i.e. with strides &lt;code&gt;U.contiguous().transpose(-2, -1).stride()&lt;/code&gt;</source>
          <target state="translated">不管原始步幅如何，返回的矩阵 &lt;code&gt;U&lt;/code&gt; 都会被转置，即步幅为 &lt;code&gt;U.contiguous().transpose(-2, -1).stride()&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="82269a6fafdcdbb86e718604476ce9b6c8e7d2a7" translate="yes" xml:space="preserve">
          <source>Irrespective of the original strides, the returned matrix &lt;code&gt;V&lt;/code&gt; will be transposed, i.e. with strides &lt;code&gt;V.contiguous().transpose(-1, -2).stride()&lt;/code&gt;.</source>
          <target state="translated">无论原始步幅如何，返回的矩阵 &lt;code&gt;V&lt;/code&gt; 都会被转置，即步幅为 &lt;code&gt;V.contiguous().transpose(-1, -2).stride()&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="01899cc116f34c67486ee354a88f582f6796cc36" translate="yes" xml:space="preserve">
          <source>Irrespective of the original strides, the returned tensors will be transposed, i.e. with strides like &lt;code&gt;input.contiguous().transpose(-2, -1).stride()&lt;/code&gt;</source>
          <target state="translated">不管原始步幅如何，返回的张量都将被转置，即具有 &lt;code&gt;input.contiguous().transpose(-2, -1).stride()&lt;/code&gt; ，例如input.contiguous（）。transpose（-2，-1）.stride（）</target>
        </trans-unit>
        <trans-unit id="2310615072fec2b4a5a875a4e0dd6ffebdaf12ad" translate="yes" xml:space="preserve">
          <source>Is &lt;code&gt;True&lt;/code&gt; if gradients need to be computed for this Tensor, &lt;code&gt;False&lt;/code&gt; otherwise.</source>
          <target state="translated">是 &lt;code&gt;True&lt;/code&gt; ，如果梯度需要计算该张量， &lt;code&gt;False&lt;/code&gt; 否则。</target>
        </trans-unit>
        <trans-unit id="aa669dbfc27ea4c75455d3b2508d4155513b175a" translate="yes" xml:space="preserve">
          <source>Is &lt;code&gt;True&lt;/code&gt; if the Tensor is a meta tensor, &lt;code&gt;False&lt;/code&gt; otherwise. Meta tensors are like normal tensors, but they carry no data.</source>
          <target state="translated">是 &lt;code&gt;True&lt;/code&gt; ，如果张量是元张， &lt;code&gt;False&lt;/code&gt; 否则。元张量类似于普通张量，但是它们不携带任何数据。</target>
        </trans-unit>
        <trans-unit id="adff4bd8499f965b85193b88135bc7439c31730a" translate="yes" xml:space="preserve">
          <source>Is &lt;code&gt;True&lt;/code&gt; if the Tensor is quantized, &lt;code&gt;False&lt;/code&gt; otherwise.</source>
          <target state="translated">是 &lt;code&gt;True&lt;/code&gt; ，如果张量进行量化， &lt;code&gt;False&lt;/code&gt; 否则。</target>
        </trans-unit>
        <trans-unit id="3a0d8439442e2aedd6e797ff733a51980c711c86" translate="yes" xml:space="preserve">
          <source>Is &lt;code&gt;True&lt;/code&gt; if the Tensor is stored on the GPU, &lt;code&gt;False&lt;/code&gt; otherwise.</source>
          <target state="translated">是 &lt;code&gt;True&lt;/code&gt; ，如果张量存储在GPU， &lt;code&gt;False&lt;/code&gt; 否则。</target>
        </trans-unit>
        <trans-unit id="e91f70a847754923183418ed052bcea694565f54" translate="yes" xml:space="preserve">
          <source>Is the &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; where this Tensor is.</source>
          <target state="translated">是该&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;所在的torch.device。</target>
        </trans-unit>
        <trans-unit id="4212d08cece9c425b48ec2c1e60e8be1d1e78b6c" translate="yes" xml:space="preserve">
          <source>Is this Tensor with its dimensions reversed.</source>
          <target state="translated">这个Tensor的尺寸是不是反过来了。</target>
        </trans-unit>
        <trans-unit id="6b5b4f414a6eb16cb4afeb48e5665cb815db584b" translate="yes" xml:space="preserve">
          <source>It achieves two things: - makes the output value exactly one-hot (since we add then subtract y_soft value) - makes the gradient equal to y_soft gradient (since we strip all other gradients)</source>
          <target state="translated">它实现了两点:--使输出值完全一热(因为我们先加再减y_soft值)--使梯度等于y_soft梯度(因为我们把其他梯度都去掉了)。</target>
        </trans-unit>
        <trans-unit id="3b6bfab9486b081f92c85bb7d32db79b3eceeba3" translate="yes" xml:space="preserve">
          <source>It always prepends a new dimension as the batch dimension.</source>
          <target state="translated">它总是预设一个新的维度作为批量维度。</target>
        </trans-unit>
        <trans-unit id="2f8d2c743873b5a3fbbb90a0d2c5cf8d6a8807aa" translate="yes" xml:space="preserve">
          <source>It automatically converts NumPy arrays and Python numerical values into PyTorch Tensors.</source>
          <target state="translated">它可以自动将NumPy数组和Python数值转换为PyTorch Tensors。</target>
        </trans-unit>
        <trans-unit id="49e4e2f8e005da183a111f2acc6be4571ffd2077" translate="yes" xml:space="preserve">
          <source>It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas.</source>
          <target state="translated">它包含了self.__dict__中每一个不是优化器的变量的条目。学习率lambda函数只有在它们是可调用对象时才会被保存,而不是在它们是函数或lambdas时。</target>
        </trans-unit>
        <trans-unit id="4a7efc8c6cf2fe612e155694f2110de8eb7b9450" translate="yes" xml:space="preserve">
          <source>It contains two entries:</source>
          <target state="translated">它包含两个条目。</target>
        </trans-unit>
        <trans-unit id="8e6db53c7cd43b88eb003448e0a5df6411ba7a87" translate="yes" xml:space="preserve">
          <source>It currently accepts &lt;code&gt;ndarray&lt;/code&gt; with dtypes of &lt;code&gt;numpy.float64&lt;/code&gt;, &lt;code&gt;numpy.float32&lt;/code&gt;, &lt;code&gt;numpy.float16&lt;/code&gt;, &lt;code&gt;numpy.complex64&lt;/code&gt;, &lt;code&gt;numpy.complex128&lt;/code&gt;, &lt;code&gt;numpy.int64&lt;/code&gt;, &lt;code&gt;numpy.int32&lt;/code&gt;, &lt;code&gt;numpy.int16&lt;/code&gt;, &lt;code&gt;numpy.int8&lt;/code&gt;, &lt;code&gt;numpy.uint8&lt;/code&gt;, and &lt;code&gt;numpy.bool&lt;/code&gt;.</source>
          <target state="translated">当前它接受 &lt;code&gt;ndarray&lt;/code&gt; 为 &lt;code&gt;numpy.float64&lt;/code&gt; ， &lt;code&gt;numpy.float32&lt;/code&gt; ， &lt;code&gt;numpy.float16&lt;/code&gt; ， &lt;code&gt;numpy.complex64&lt;/code&gt; ， &lt;code&gt;numpy.complex128&lt;/code&gt; ， &lt;code&gt;numpy.int64&lt;/code&gt; ， &lt;code&gt;numpy.int32&lt;/code&gt; ， &lt;code&gt;numpy.int16&lt;/code&gt; ， &lt;code&gt;numpy.int8&lt;/code&gt; ， &lt;code&gt;numpy.uint8&lt;/code&gt; 和 &lt;code&gt;numpy.bool&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="401f208e4a6ef5ef6ad2bc031ef0d0f97b3ebf14" translate="yes" xml:space="preserve">
          <source>It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability &amp;gt;= 3.0</source>
          <target state="translated">它具有CUDA对应项，使您能够在计算能力&amp;gt; = 3.0的NVIDIA GPU上运行张量计算</target>
        </trans-unit>
        <trans-unit id="730337db11f0b320af1c601c156987d99a95f2d4" translate="yes" xml:space="preserve">
          <source>It has been proposed in &lt;a href=&quot;http://jmlr.org/papers/v12/duchi11a.html&quot;&gt;Adaptive Subgradient Methods for Online Learning and Stochastic Optimization&lt;/a&gt;.</source>
          <target state="translated">在&lt;a href=&quot;http://jmlr.org/papers/v12/duchi11a.html&quot;&gt;在线学习和随机优化的自适应次梯度方法中&lt;/a&gt;提出了该方法。</target>
        </trans-unit>
        <trans-unit id="051e2d9e2a228306c26b3c61549e79ae202aec25" translate="yes" xml:space="preserve">
          <source>It has been proposed in &lt;a href=&quot;https://arxiv.org/abs/1212.5701&quot;&gt;ADADELTA: An Adaptive Learning Rate Method&lt;/a&gt;.</source>
          <target state="translated">它已在&lt;a href=&quot;https://arxiv.org/abs/1212.5701&quot;&gt;ADADELTA中&lt;/a&gt;提出：一种自适应学习率方法。</target>
        </trans-unit>
        <trans-unit id="09050a199eb7d53185c92e9f5b5a66dd6952d42f" translate="yes" xml:space="preserve">
          <source>It has been proposed in &lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam: A Method for Stochastic Optimization&lt;/a&gt;.</source>
          <target state="translated">它已在《&lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;亚当：一种随机优化方法》中&lt;/a&gt;提出。</target>
        </trans-unit>
        <trans-unit id="333ffdb98cee48515840fbcbba1813b5937b4a78" translate="yes" xml:space="preserve">
          <source>It has been proposed in &lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam: A Method for Stochastic Optimization&lt;/a&gt;. The implementation of the L2 penalty follows changes proposed in &lt;a href=&quot;https://arxiv.org/abs/1711.05101&quot;&gt;Decoupled Weight Decay Regularization&lt;/a&gt;.</source>
          <target state="translated">它已在《&lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;亚当：一种随机优化方法》中&lt;/a&gt;提出。L2惩罚的实现遵循&amp;ldquo;去&lt;a href=&quot;https://arxiv.org/abs/1711.05101&quot;&gt;耦权重衰减正则化&amp;rdquo;中&lt;/a&gt;提出的更改。</target>
        </trans-unit>
        <trans-unit id="01b731554c5955677b611c13e6a10743e819c4a9" translate="yes" xml:space="preserve">
          <source>It has been proposed in &lt;a href=&quot;https://arxiv.org/abs/1608.03983&quot;&gt;SGDR: Stochastic Gradient Descent with Warm Restarts&lt;/a&gt;.</source>
          <target state="translated">它已在&lt;a href=&quot;https://arxiv.org/abs/1608.03983&quot;&gt;SGDR中&lt;/a&gt;提出：带暖重启的随机梯度下降。</target>
        </trans-unit>
        <trans-unit id="2ff6bf54974d4bef1fd6f31a4e3985ebfbf18b7e" translate="yes" xml:space="preserve">
          <source>It has been proposed in &lt;a href=&quot;https://arxiv.org/abs/1608.03983&quot;&gt;SGDR: Stochastic Gradient Descent with Warm Restarts&lt;/a&gt;. Note that this only implements the cosine annealing part of SGDR, and not the restarts.</source>
          <target state="translated">它已在&lt;a href=&quot;https://arxiv.org/abs/1608.03983&quot;&gt;SGDR中&lt;/a&gt;提出：带暖重启的随机梯度下降。请注意，这仅实现了SGDR的余弦退火部分，而没有实现重启。</target>
        </trans-unit>
        <trans-unit id="459d6918fdc7170b0fc1dc7a88c04a7b59fb13a0" translate="yes" xml:space="preserve">
          <source>It has been proposed in &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=131098&quot;&gt;Acceleration of stochastic approximation by averaging&lt;/a&gt;.</source>
          <target state="translated">已经提出了&lt;a href=&quot;https://dl.acm.org/citation.cfm?id=131098&quot;&gt;通过平均&lt;/a&gt;来加速随机逼近。</target>
        </trans-unit>
        <trans-unit id="6563cef1ae60ef9f0cfc196eb441bdefa9d3c7aa" translate="yes" xml:space="preserve">
          <source>It has similar signature as &lt;a href=&quot;../tensors#torch.Tensor.to&quot;&gt;&lt;code&gt;torch.Tensor.to()&lt;/code&gt;&lt;/a&gt;, except optional arguments like &lt;code&gt;non_blocking&lt;/code&gt; and &lt;code&gt;copy&lt;/code&gt; should be passed as kwargs, not args, or they will not apply to the index tensors.</source>
          <target state="translated">它具有与&lt;a href=&quot;../tensors#torch.Tensor.to&quot;&gt; &lt;code&gt;torch.Tensor.to()&lt;/code&gt; &lt;/a&gt;类似的签名，除了诸如 &lt;code&gt;non_blocking&lt;/code&gt; 和 &lt;code&gt;copy&lt;/code&gt; 之类的可选参数应作为kwarg（而不是args）传递，否则它们将不适用于索引张量。</target>
        </trans-unit>
        <trans-unit id="2b37442b8edf941c9b64329b2dd98dad1125f396" translate="yes" xml:space="preserve">
          <source>It is also possible to annotate types with Python 3 type hints from the &lt;code&gt;typing&lt;/code&gt; module.</source>
          <target state="translated">还可以使用来自 &lt;code&gt;typing&lt;/code&gt; 模块的Python 3类型提示来注释类型。</target>
        </trans-unit>
        <trans-unit id="86993dc3f5b11fb441bb568843f0d884da0db7f4" translate="yes" xml:space="preserve">
          <source>It is an inverse operation to &lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt;&lt;code&gt;pack_padded_sequence()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">这是&lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt; &lt;code&gt;pack_padded_sequence()&lt;/code&gt; &lt;/a&gt;的逆运算。</target>
        </trans-unit>
        <trans-unit id="50cbae8d4910e6f042bec5701d96446fe97f36c9" translate="yes" xml:space="preserve">
          <source>It is applied to all slices along dim, and will re-scale them so that the elements lie in the range &lt;code&gt;[0, 1]&lt;/code&gt; and sum to 1.</source>
          <target state="translated">它应用于沿dim的所有切片，并将对其进行重新缩放，以使元素位于 &lt;code&gt;[0, 1]&lt;/code&gt; 范围内且总和为1。</target>
        </trans-unit>
        <trans-unit id="79324c046d8fb8ad0505c73e6f4ad6bf30dbe4de" translate="yes" xml:space="preserve">
          <source>It is equivalent to &lt;code&gt;`
ComposeTransform([AffineTransform(0., 2.), SigmoidTransform(), AffineTransform(-1., 2.)])
`&lt;/code&gt; However this might not be numerically stable, thus it is recommended to use &lt;code&gt;TanhTransform&lt;/code&gt; instead.</source>
          <target state="translated">它等效于 &lt;code&gt;` ComposeTransform([AffineTransform(0., 2.), SigmoidTransform(), AffineTransform(-1., 2.)]) `&lt;/code&gt; 但是，这在数值上可能不是稳定的，因此建议改用 &lt;code&gt;TanhTransform&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="88fdb36159da698a069b2cd0358110c7f72e3f52" translate="yes" xml:space="preserve">
          <source>It is equivalent to the distribution that &lt;a href=&quot;generated/torch.multinomial#torch.multinomial&quot;&gt;&lt;code&gt;torch.multinomial()&lt;/code&gt;&lt;/a&gt; samples from.</source>
          <target state="translated">它等效于&lt;a href=&quot;generated/torch.multinomial#torch.multinomial&quot;&gt; &lt;code&gt;torch.multinomial()&lt;/code&gt; &lt;/a&gt;采样所来自的分布。</target>
        </trans-unit>
        <trans-unit id="b26d4802e99ec71d25e10e9e447cc2865d7a8327" translate="yes" xml:space="preserve">
          <source>It is especially useful in conjunction with &lt;a href=&quot;generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt;&lt;/a&gt;. In such a case, each process can pass a &lt;code&gt;DistributedSampler&lt;/code&gt; instance as a &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; sampler, and load a subset of the original dataset that is exclusive to it.</source>
          <target state="translated">与&lt;a href=&quot;generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt; &lt;code&gt;torch.nn.parallel.DistributedDataParallel&lt;/code&gt; &lt;/a&gt;结合使用时特别有用。在这种情况下，每个进程都可以将 &lt;code&gt;DistributedSampler&lt;/code&gt; 实例作为&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt;采样器传递，并加载原始数据集的专有子集。</target>
        </trans-unit>
        <trans-unit id="a9c65d86833900305f35c9e1c934da02bcf964b7" translate="yes" xml:space="preserve">
          <source>It is generally not recommended to return CUDA tensors in multi-process loading because of many subtleties in using CUDA and sharing CUDA tensors in multiprocessing (see &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/multiprocessing.html#multiprocessing-cuda-note&quot;&gt;CUDA in multiprocessing&lt;/a&gt;). Instead, we recommend using &lt;a href=&quot;#memory-pinning&quot;&gt;automatic memory pinning&lt;/a&gt; (i.e., setting &lt;code&gt;pin_memory=True&lt;/code&gt;), which enables fast data transfer to CUDA-enabled GPUs.</source>
          <target state="translated">通常不建议在多进程加载中返回CUDA张量，因为在使用CUDA和在多处理中共享CUDA张量时存在许多微妙之处（请参阅&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/multiprocessing.html#multiprocessing-cuda-note&quot;&gt;多处理中的CUDA&lt;/a&gt;）。相反，我们建议使用&lt;a href=&quot;#memory-pinning&quot;&gt;自动内存固定&lt;/a&gt;（即，设置 &lt;code&gt;pin_memory=True&lt;/code&gt; ），这样可以将数据快速传输到支持CUDA的GPU。</target>
        </trans-unit>
        <trans-unit id="ec0c9b813cf0aa82dc727b897163ee4aef81fc00" translate="yes" xml:space="preserve">
          <source>It is lazily initialized, so you can always import it, and use &lt;a href=&quot;#torch.cuda.is_available&quot;&gt;&lt;code&gt;is_available()&lt;/code&gt;&lt;/a&gt; to determine if your system supports CUDA.</source>
          <target state="translated">它是延迟初始化的，因此您始终可以导入它，并使用&lt;a href=&quot;#torch.cuda.is_available&quot;&gt; &lt;code&gt;is_available()&lt;/code&gt; &lt;/a&gt;确定您的系统是否支持CUDA。</target>
        </trans-unit>
        <trans-unit id="5de635d4ec610b825189efbe466e3cacb9dcacd9" translate="yes" xml:space="preserve">
          <source>It is not possible to directly backpropagate through random samples. However, there are two main methods for creating surrogate functions that can be backpropagated through. These are the score function estimator/likelihood ratio estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly seen as the basis for policy gradient methods in reinforcement learning, and the pathwise derivative estimator is commonly seen in the reparameterization trick in variational autoencoders. Whilst the score function only requires the value of samples</source>
          <target state="translated">不可能直接通过随机样本进行反向传播。但是,有两种主要的方法可以创建代用函数,可以通过反向传播。这两种方法是得分函数估计器/似然比估计器/REINFORCE和路径导数估计器。REINFORCE通常被视为强化学习中政策梯度方法的基础,而路径导数估计器通常被视为变量自动编码器中的重参数化技巧。虽然得分函数只需要样本的值。</target>
        </trans-unit>
        <trans-unit id="7c6829a0a0e941307482f3fc9539e6effef1fbd8" translate="yes" xml:space="preserve">
          <source>It is recommended to use &lt;a href=&quot;torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt;&lt;code&gt;DistributedDataParallel&lt;/code&gt;&lt;/a&gt;, instead of this class, to do multi-GPU training, even if there is only a single node. See: &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-nn-ddp-instead&quot;&gt;Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/ddp.html#ddp&quot;&gt;Distributed Data Parallel&lt;/a&gt;.</source>
          <target state="translated">建议使用&lt;a href=&quot;torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel&quot;&gt; &lt;code&gt;DistributedDataParallel&lt;/code&gt; &lt;/a&gt;而不是此类来进行多GPU训练，即使只有单个节点也是如此。请参阅：&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cuda-nn-ddp-instead&quot;&gt;使用nn.parallel.DistributedDataParallel而不是多处理或nn.DataParallel&lt;/a&gt;和&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/ddp.html#ddp&quot;&gt;Distributed Data Parallel&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e66b3186074888a018543f3b0fff094946ea09d0" translate="yes" xml:space="preserve">
          <source>It is useful when running the program under nvprof:</source>
          <target state="translated">它在nvprof下运行程序时很有用。</target>
        </trans-unit>
        <trans-unit id="cc50a460e1a231d6b7914ab6f80ce6645cd28d1a" translate="yes" xml:space="preserve">
          <source>It is useful when training a classification problem with &lt;code&gt;C&lt;/code&gt; classes. If provided, the optional argument &lt;code&gt;weight&lt;/code&gt; should be a 1D &lt;code&gt;Tensor&lt;/code&gt; assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.</source>
          <target state="translated">在训练带有 &lt;code&gt;C&lt;/code&gt; 类的分类问题时很有用。如果提供，则可选参数 &lt;code&gt;weight&lt;/code&gt; 应为一维 &lt;code&gt;Tensor&lt;/code&gt; 量，为每个类分配权重。当您的训练集不平衡时，此功能特别有用。</target>
        </trans-unit>
        <trans-unit id="d3dd49ac73db5350741c77e9746cd12b789fc846" translate="yes" xml:space="preserve">
          <source>It must accept a context &lt;code&gt;ctx&lt;/code&gt; as the first argument, followed by as many outputs did &lt;a href=&quot;#torch.autograd.Function.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt; return, and it should return as many tensors, as there were inputs to &lt;a href=&quot;#torch.autograd.Function.forward&quot;&gt;&lt;code&gt;forward()&lt;/code&gt;&lt;/a&gt;. Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input.</source>
          <target state="translated">它必须接受上下文 &lt;code&gt;ctx&lt;/code&gt; 作为第一个参数，其后跟&lt;a href=&quot;#torch.autograd.Function.forward&quot;&gt; &lt;code&gt;forward()&lt;/code&gt; &lt;/a&gt;返回的输出一样多，并且它应该返回与&lt;a href=&quot;#torch.autograd.Function.forward&quot;&gt; &lt;code&gt;forward()&lt;/code&gt; 的&lt;/a&gt;输入一样多的张量。每个参数都是给定输出的梯度，每个返回值应该是对应输入的梯度。</target>
        </trans-unit>
        <trans-unit id="252cdfb99fc0852a5d679618d9953abd9399f85f" translate="yes" xml:space="preserve">
          <source>It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types).</source>
          <target state="translated">它必须接受一个上下文ctx作为第一个参数,然后是任意数量的参数(ensors或其他类型)。</target>
        </trans-unit>
        <trans-unit id="a5858d55aee32bb1f338ad58158a7863ffaa0e2a" translate="yes" xml:space="preserve">
          <source>It preserves the data structure, e.g., if each sample is a dictionary, it outputs a dictionary with the same set of keys but batched Tensors as values (or lists if the values can not be converted into Tensors). Same for &lt;code&gt;list&lt;/code&gt; s, &lt;code&gt;tuple&lt;/code&gt; s, &lt;code&gt;namedtuple&lt;/code&gt; s, etc.</source>
          <target state="translated">它保留数据结构，例如，如果每个样本都是字典，则输出具有相同键集但批处理过的张量作为值的字典（或列出无法将其转换为张量的列表）。与 &lt;code&gt;list&lt;/code&gt; ， &lt;code&gt;tuple&lt;/code&gt; ， &lt;code&gt;namedtuple&lt;/code&gt; 等等相同。</target>
        </trans-unit>
        <trans-unit id="99a91e5a4c5f62323cf29af98e5d69957ac9245f" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s like QConfig, but for dynamic quantization.</source>
          <target state="translated">它类似于QConfig，但用于动态量化。</target>
        </trans-unit>
        <trans-unit id="88103e98293e4d193ade88c8ec2a2e41a9017ce8" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s possible to trade off recall and precision by adding weights to positive examples. In the case of multi-label classification the loss can be described as:</source>
          <target state="translated">通过为正面示例增加权重，可以在召回率和准确性之间进行权衡。在多标签分类的情况下，损失可描述为：</target>
        </trans-unit>
        <trans-unit id="ecdda59aea5ee67d7d854c969ccf7f4f4b4a4c54" translate="yes" xml:space="preserve">
          <source>Item</source>
          <target state="translated">Item</target>
        </trans-unit>
        <trans-unit id="6a10d92f43c8da36e2a39d8ec8446f793702e774" translate="yes" xml:space="preserve">
          <source>Iterable-style datasets</source>
          <target state="translated">迭代式数据集</target>
        </trans-unit>
        <trans-unit id="4dd80eea3f6c51bf5b9c13a8a47609bd55b30e0e" translate="yes" xml:space="preserve">
          <source>Iterables</source>
          <target state="translated">Iterables</target>
        </trans-unit>
        <trans-unit id="069e2ae56a9eacaa6614576bc398f841898b1686" translate="yes" xml:space="preserve">
          <source>Its signature is similar to &lt;a href=&quot;../tensors#torch.Tensor.to&quot;&gt;&lt;code&gt;torch.Tensor.to()&lt;/code&gt;&lt;/a&gt;, but only accepts floating point desired &lt;code&gt;dtype&lt;/code&gt; s. In addition, this method will only cast the floating point parameters and buffers to &lt;code&gt;dtype&lt;/code&gt; (if given). The integral parameters and buffers will be moved &lt;code&gt;device&lt;/code&gt;, if that is given, but with dtypes unchanged. When &lt;code&gt;non_blocking&lt;/code&gt; is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</source>
          <target state="translated">它的签名是类似于&lt;a href=&quot;../tensors#torch.Tensor.to&quot;&gt; &lt;code&gt;torch.Tensor.to()&lt;/code&gt; &lt;/a&gt;，但只接受浮点期望 &lt;code&gt;dtype&lt;/code&gt; 秒。此外，此方法只会将浮点参数和缓冲区 &lt;code&gt;dtype&lt;/code&gt; 为dtype（如果有的话）。如果给定了整数参数和缓冲区，则将其移动到 &lt;code&gt;device&lt;/code&gt; ，但dtypes不变。当 &lt;code&gt;non_blocking&lt;/code&gt; 被设定，它会尝试转换/相对于异步移动到的主机，如果可能的，例如，移动CPU张量与固定内存到CUDA设备。</target>
        </trans-unit>
        <trans-unit id="eb5d2f4a58038c71155ddcf3cea35dc3c6d34501" translate="yes" xml:space="preserve">
          <source>JIT</source>
          <target state="translated">JIT</target>
        </trans-unit>
        <trans-unit id="ba8c07b6e8f75ea0f3fccd63588af034ee568ee6" translate="yes" xml:space="preserve">
          <source>Jacobian (&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt; or nested tuple of Tensors)</source>
          <target state="translated">Jacobian（&lt;a href=&quot;tensors#torch.Tensor&quot;&gt;张量&lt;/a&gt;或张量的嵌套元组）</target>
        </trans-unit>
        <trans-unit id="a26e63e0dd630bf0aa29409ebb2007f8ed0989a9" translate="yes" xml:space="preserve">
          <source>Javadoc</source>
          <target state="translated">Javadoc</target>
        </trans-unit>
        <trans-unit id="a7ee38bb7be4fc44198cb2685d9601dcf2b9f569" translate="yes" xml:space="preserve">
          <source>K</source>
          <target state="translated">K</target>
        </trans-unit>
        <trans-unit id="d1dd1d02265a63fb16a7c02869b20a2ff09b23ec" translate="yes" xml:space="preserve">
          <source>K \geq 1</source>
          <target state="translated">K geq 1</target>
        </trans-unit>
        <trans-unit id="d815c4b766be65370cc630500fbc7534d96810e3" translate="yes" xml:space="preserve">
          <source>KL(p \| q)</source>
          <target state="translated">KL(p\|q)</target>
        </trans-unit>
        <trans-unit id="3789bcc3ea87de653c85546427835735d21460e6" translate="yes" xml:space="preserve">
          <source>KL(p \| q) = \int p(x) \log\frac {p(x)} {q(x)} \,dx</source>
          <target state="translated">KL(p \| q)=\int p(x)\log\frac {p(x)}{q(x)}\ dx</target>
        </trans-unit>
        <trans-unit id="a0d38167f0e193a1ac968e4ff065116f3dd7d3e2" translate="yes" xml:space="preserve">
          <source>KLDivLoss</source>
          <target state="translated">KLDivLoss</target>
        </trans-unit>
        <trans-unit id="2dff5751295255d47b7ebb571ae0d72fa938cb72" translate="yes" xml:space="preserve">
          <source>Keep in mind that only a limited number of optimizers support sparse gradients: currently it&amp;rsquo;s &lt;code&gt;optim.SGD&lt;/code&gt; (&lt;code&gt;CUDA&lt;/code&gt; and &lt;code&gt;CPU&lt;/code&gt;), &lt;code&gt;optim.SparseAdam&lt;/code&gt; (&lt;code&gt;CUDA&lt;/code&gt; and &lt;code&gt;CPU&lt;/code&gt;) and &lt;code&gt;optim.Adagrad&lt;/code&gt; (&lt;code&gt;CPU&lt;/code&gt;)</source>
          <target state="translated">请记住，只有有限数量的优化程序支持稀疏渐变：当前为 &lt;code&gt;optim.SGD&lt;/code&gt; （ &lt;code&gt;CUDA&lt;/code&gt; 和 &lt;code&gt;CPU&lt;/code&gt; ）， &lt;code&gt;optim.SparseAdam&lt;/code&gt; （ &lt;code&gt;CUDA&lt;/code&gt; 和 &lt;code&gt;CPU&lt;/code&gt; ）和 &lt;code&gt;optim.Adagrad&lt;/code&gt; （ &lt;code&gt;CPU&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="61df1ec904a19beceda75adc97e889c2fd5cb69f" translate="yes" xml:space="preserve">
          <source>Keypoint R-CNN</source>
          <target state="translated">关键点R-CNN</target>
        </trans-unit>
        <trans-unit id="4dbced9709f5236c9b69e641ffff848fd0986a34" translate="yes" xml:space="preserve">
          <source>Keypoint R-CNN ResNet-50 FPN</source>
          <target state="translated">关键点 R-CNN ResNet-50 FPN</target>
        </trans-unit>
        <trans-unit id="f2815e1786777eee9275fa2f0b31c8b705f9edc6" translate="yes" xml:space="preserve">
          <source>Keypoint R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.</source>
          <target state="translated">Keypoint R-CNN可输出到ONNX,输入固定大小的图像,并有固定的批量大小。</target>
        </trans-unit>
        <trans-unit id="db32172db3cd21ae36313d815fba0dd39bd7127a" translate="yes" xml:space="preserve">
          <source>Keyword Arguments</source>
          <target state="translated">關鍵字參數</target>
        </trans-unit>
        <trans-unit id="1c524ac8953f745b9e5b79c69e1b4d39891c5ba1" translate="yes" xml:space="preserve">
          <source>Keyword arguments &lt;code&gt;min_value&lt;/code&gt; and &lt;code&gt;max_value&lt;/code&gt; have been deprecated in favor of &lt;code&gt;min_val&lt;/code&gt; and &lt;code&gt;max_val&lt;/code&gt;.</source>
          <target state="translated">关键字参数 &lt;code&gt;min_value&lt;/code&gt; 和 &lt;code&gt;max_value&lt;/code&gt; 都赞成被否决 &lt;code&gt;min_val&lt;/code&gt; 和 &lt;code&gt;max_val&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e48bd0570e2bf01526ed152277d5e60977339c96" translate="yes" xml:space="preserve">
          <source>Kicks off the distributed backward pass using the provided roots. This currently implements the &lt;a href=&quot;rpc/distributed_autograd#fast-mode-algorithm&quot;&gt;FAST mode algorithm&lt;/a&gt; which assumes all RPC messages sent in the same distributed autograd context across workers would be part of the autograd graph during the backward pass.</source>
          <target state="translated">使用提供的根启动分布式反向传递。当前，这实现了&lt;a href=&quot;rpc/distributed_autograd#fast-mode-algorithm&quot;&gt;FAST模式算法&lt;/a&gt;，该算法假定在反向传递过程中，跨工作程序在同一分布式autograd上下文中发送的所有RPC消息都是autograd图的一部分。</target>
        </trans-unit>
        <trans-unit id="41bb1570ae7ccc5980e78c5f1c45cc0aed0bdbed" translate="yes" xml:space="preserve">
          <source>Kinetics 1-crop accuracies for clip length 16 (16x112x112)</source>
          <target state="translated">夹子长度16(16x112x112)的动力学1-crop精度</target>
        </trans-unit>
        <trans-unit id="a5a5fafee83492d8b176cbbdd5bb0860a6fdbf28" translate="yes" xml:space="preserve">
          <source>Known limitations:</source>
          <target state="translated">已知的局限性。</target>
        </trans-unit>
        <trans-unit id="d160e0986aca4714714a16f29ec605af90be704d" translate="yes" xml:space="preserve">
          <source>L</source>
          <target state="translated">L</target>
        </trans-unit>
        <trans-unit id="80f4812ca21133a053b1e37ce0b7c69d130f72e6" translate="yes" xml:space="preserve">
          <source>L = \prod_d \left\lfloor\frac{\text{output\_size}[d] + 2 \times \text{padding}[d] % - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,</source>
          <target state="translated">L=leftlfloor/frac{text{output_size}[d]+2次/text{padding}[d]%-text{dilation}[d]次/text{kernel_size}[d]-1)-1}{text{stride}[d]}+1次/rightrfloor。</target>
        </trans-unit>
        <trans-unit id="c67a32aa874c2b17f9de746130e43c1407789f98" translate="yes" xml:space="preserve">
          <source>L = \prod_d \left\lfloor\frac{\text{spatial\_size}[d] + 2 \times \text{padding}[d] % - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,</source>
          <target state="translated">L=leftlfloor/frac{text{spatial_size}[d]+2 遍/text{padding}[d]%-遍/text{dilation}[d]遍(text{kernel_size}[d]-1)-1}{text{stride}[d]}+1遍/rightrfloor。</target>
        </trans-unit>
        <trans-unit id="c775d48eb4f58382ff520cfe8c90ab7190041ada" translate="yes" xml:space="preserve">
          <source>L = \{l_1,\dots,l_N\}^\top</source>
          <target state="translated">L={l_1,dots,l_N}^\top</target>
        </trans-unit>
        <trans-unit id="77bc26a3089aa1a1138008701f4240f1d24f7ec4" translate="yes" xml:space="preserve">
          <source>L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</source>
          <target state="translated">L(a,p,n)={d(a_i,p_i)-d(a_i,n_i)+{margin},0}。</target>
        </trans-unit>
        <trans-unit id="38b3e9960c6ed6d7fea333d02fe4a806a8e19d30" translate="yes" xml:space="preserve">
          <source>L1Loss</source>
          <target state="translated">L1Loss</target>
        </trans-unit>
        <trans-unit id="eea818c7023faaf096d89f5dc66b905458038bf3" translate="yes" xml:space="preserve">
          <source>L1Unstructured</source>
          <target state="translated">L1Unstructured</target>
        </trans-unit>
        <trans-unit id="ba6b42d0904cce73f214db608d06bfd30f824246" translate="yes" xml:space="preserve">
          <source>L=C \times \text{upscale\_factor}^2</source>
          <target state="translated">L=C xtimes text{upscale_factor}^2</target>
        </trans-unit>
        <trans-unit id="e08d4eaec3e70d3b11d0930088b84b2ff38e358f" translate="yes" xml:space="preserve">
          <source>LPPool1d</source>
          <target state="translated">LPPool1d</target>
        </trans-unit>
        <trans-unit id="2ea3c697cd5d5c5710f8ec9f1853900569f0680c" translate="yes" xml:space="preserve">
          <source>LPPool2d</source>
          <target state="translated">LPPool2d</target>
        </trans-unit>
        <trans-unit id="23757b375d1fdee5bda46fff218547176aed63b2" translate="yes" xml:space="preserve">
          <source>LSTM</source>
          <target state="translated">LSTM</target>
        </trans-unit>
        <trans-unit id="2f25bc39b85fe095b000d209878e94802865e367" translate="yes" xml:space="preserve">
          <source>LSTMCell</source>
          <target state="translated">LSTMCell</target>
        </trans-unit>
        <trans-unit id="ecfecac3ee4f2cb5286bbb038509a2e5de7c4c02" translate="yes" xml:space="preserve">
          <source>LU factorization with &lt;code&gt;pivot&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt; is not available for CPU, and attempting to do so will throw an error. However, LU factorization with &lt;code&gt;pivot&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt; is available for CUDA.</source>
          <target state="translated">&lt;code&gt;pivot&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt; 的LU因式分解不适用于CPU，尝试这样做会引发错误。但是，CUDA提供了 &lt;code&gt;pivot&lt;/code&gt; = &lt;code&gt;False&lt;/code&gt; 的LU因式分解。</target>
        </trans-unit>
        <trans-unit id="0deac72fe13e2e0b87977edeca8e509c0bcfa09e" translate="yes" xml:space="preserve">
          <source>L_p</source>
          <target state="translated">L_p</target>
        </trans-unit>
        <trans-unit id="06c5049b8951b65d33ef4a2ef830eb1114e69b8d" translate="yes" xml:space="preserve">
          <source>L_{out} = (L_{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{dilation} \times (\text{kernel\_size} - 1) + \text{output\_padding} + 1</source>
          <target state="translated">L_{out}=(L_{in}-1)遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数 遍数</target>
        </trans-unit>
        <trans-unit id="b805a9450351de83336b48fb7172bcc72a263dc2" translate="yes" xml:space="preserve">
          <source>L_{out} = \left\lfloor \frac{L_{in} + 2 \times \text{padding} - \text{dilation} \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor</source>
          <target state="translated">L_{out}=左/左/lfloor \frac{L_{in}+2 xtimes \text{padding}-\text{dilation}\times (text{kernel_size}-1)-1}{text{stride}}+1rightrfloor</target>
        </trans-unit>
        <trans-unit id="ceac6a4482e16b31e7e38fe1379ffb2a0580a4c8" translate="yes" xml:space="preserve">
          <source>L_{out} = \left\lfloor \frac{L_{in} + 2 \times \text{padding} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloor</source>
          <target state="translated">L_{out}=左右/lfloor \frac{L_{in}+2 xtimes text{padding}-text{kernel_size}}{text{stride}}+1rightrfloor</target>
        </trans-unit>
        <trans-unit id="87439fc8e93caa8a60f8765a05bec347953ce5dc" translate="yes" xml:space="preserve">
          <source>L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation} \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor</source>
          <target state="translated">L_{out}=左/左/lfloor/frac{L_{in}+2 时/text{padding}-语/text{dilation}语/times (text{kernel_size}-1)-1}{text{stride}}+1rightrfloor</target>
        </trans-unit>
        <trans-unit id="9c7d288e1c47c63e7fa88234abfbcbb629553bc4" translate="yes" xml:space="preserve">
          <source>L_{out} = \left\lfloor\frac{L_{in} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloor</source>
          <target state="translated">L_{out}=\left\lfloor\frac{L_{in}-\text{kernel\_size}}{text{stride}}+1right\rfloor</target>
        </trans-unit>
        <trans-unit id="82415f0bf390885258b6da20230789fced78fab6" translate="yes" xml:space="preserve">
          <source>Labels passed as inputs to this module should be sorted according to their frequency. This means that the most frequent label should be represented by the index &lt;code&gt;0&lt;/code&gt;, and the least frequent label should be represented by the index &lt;code&gt;n_classes - 1&lt;/code&gt;.</source>
          <target state="translated">传递给该模块的标签应根据其频率进行分类。这意味着最频繁的标签应由索引 &lt;code&gt;0&lt;/code&gt; 表示，最不频繁的标签应由索引 &lt;code&gt;n_classes - 1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e52f467c6824dfb11f12944d49910287d4e8acf9" translate="yes" xml:space="preserve">
          <source>Language Bindings</source>
          <target state="translated">语言绑定</target>
        </trans-unit>
        <trans-unit id="f7c1b0b04225275caa328bda30504fd6b8c37f10" translate="yes" xml:space="preserve">
          <source>Laplace</source>
          <target state="translated">Laplace</target>
        </trans-unit>
        <trans-unit id="1a13e309cb6089d8795a4086c303a44f1aa93eb0" translate="yes" xml:space="preserve">
          <source>Last chunk will be smaller if the tensor size along the given dimension &lt;code&gt;dim&lt;/code&gt; is not divisible by &lt;code&gt;chunks&lt;/code&gt;.</source>
          <target state="translated">如果沿着给定维度 &lt;code&gt;dim&lt;/code&gt; 的张量大小无法被 &lt;code&gt;chunks&lt;/code&gt; 整除，则最后一个块将更小。</target>
        </trans-unit>
        <trans-unit id="ea4ab66267252a04db528f49cff0f01eeac15249" translate="yes" xml:space="preserve">
          <source>Later, saved tensors can be accessed through the &lt;code&gt;saved_tensors&lt;/code&gt; attribute. Before returning them to the user, a check is made to ensure they weren&amp;rsquo;t used in any in-place operation that modified their content.</source>
          <target state="translated">以后，可以通过 &lt;code&gt;saved_tensors&lt;/code&gt; 属性访问已保存的张量。在将它们返回给用户之前，要进行检查以确保未在任何修改其内容的就地操作中使用它们。</target>
        </trans-unit>
        <trans-unit id="c11e8b0852264fedca1944abe570a02d58008f36" translate="yes" xml:space="preserve">
          <source>Launch utility</source>
          <target state="translated">启动实用程序</target>
        </trans-unit>
        <trans-unit id="3e7347f29c1c343ae08ddd8deb4a8c8c703257a6" translate="yes" xml:space="preserve">
          <source>LayerNorm</source>
          <target state="translated">LayerNorm</target>
        </trans-unit>
        <trans-unit id="ae8648006d573eefab4fc3a540fa844d3c24c709" translate="yes" xml:space="preserve">
          <source>Leaky Relu</source>
          <target state="translated">漏水的继电器</target>
        </trans-unit>
        <trans-unit id="f3272736528de0f5cd99ceafd394209cd4f0d9d4" translate="yes" xml:space="preserve">
          <source>LeakyRELU</source>
          <target state="translated">LeakyRELU</target>
        </trans-unit>
        <trans-unit id="fedab85703859b90b0ad1f8cd46b9b7ea93c8647" translate="yes" xml:space="preserve">
          <source>LeakyReLU</source>
          <target state="translated">LeakyReLU</target>
        </trans-unit>
        <trans-unit id="08736899d926026ffd2bed5a1ba2420f2fe34a6d" translate="yes" xml:space="preserve">
          <source>Learning rate scheduling should be applied after optimizer&amp;rsquo;s update; e.g., you should write your code this way:</source>
          <target state="translated">优化器更新后应应用学习率计划；例如，您应该以这种方式编写代码：</target>
        </trans-unit>
        <trans-unit id="7422cede6b29fe51f9334794bb938ed3c22c4e55" translate="yes" xml:space="preserve">
          <source>Least squares estimation of the original signal of size (&amp;hellip;, signal_length)</source>
          <target state="translated">大小（&amp;hellip;，signal_length）的原始信号的最小二乘估计</target>
        </trans-unit>
        <trans-unit id="81a9dc728fc2ca28b37b2c8b862e702ce61a20d2" translate="yes" xml:space="preserve">
          <source>Legacy Constructors</source>
          <target state="translated">遗产建筑公司</target>
        </trans-unit>
        <trans-unit id="e6252983d84535ed576c200c0613bfb5c53e74f6" translate="yes" xml:space="preserve">
          <source>Let I_0 be the zeroth order modified Bessel function of the first kind (see &lt;a href=&quot;torch.i0#torch.i0&quot;&gt;&lt;code&gt;torch.i0()&lt;/code&gt;&lt;/a&gt;) and &lt;code&gt;N = L - 1&lt;/code&gt; if &lt;code&gt;periodic&lt;/code&gt; is False and &lt;code&gt;L&lt;/code&gt; if &lt;code&gt;periodic&lt;/code&gt; is True, where &lt;code&gt;L&lt;/code&gt; is the &lt;code&gt;window_length&lt;/code&gt;. This function computes:</source>
          <target state="translated">让I_0是零阶修正的第一类贝塞尔函数（见&lt;a href=&quot;torch.i0#torch.i0&quot;&gt; &lt;code&gt;torch.i0()&lt;/code&gt; &lt;/a&gt;）和 &lt;code&gt;N = L - 1&lt;/code&gt; ，如果 &lt;code&gt;periodic&lt;/code&gt; 为False并且 &lt;code&gt;L&lt;/code&gt; 如果 &lt;code&gt;periodic&lt;/code&gt; 为True，其中 &lt;code&gt;L&lt;/code&gt; 是 &lt;code&gt;window_length&lt;/code&gt; 。该函数计算：</target>
        </trans-unit>
        <trans-unit id="c0f4fa4a2a001fbe73eaa9b243479e76bce8ec6b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s see how &lt;code&gt;match&lt;/code&gt; and &lt;code&gt;unify&lt;/code&gt; are used in name inference in the case of adding two one-dim tensors with no broadcasting.</source>
          <target state="translated">让我们看看在添加两个不广播的一维张量的情况下，如何在名称推断中使用 &lt;code&gt;match&lt;/code&gt; 和 &lt;code&gt;unify&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="27c968e6692b41bf6e1a241b870dc41ff1a51b17" translate="yes" xml:space="preserve">
          <source>Libraries</source>
          <target state="translated">Libraries</target>
        </trans-unit>
        <trans-unit id="538c09161b8497f998404cafc34964ed3a445575" translate="yes" xml:space="preserve">
          <source>Licensed under the 3-clause BSD License.</source>
          <target state="translated">采用3-clause BSD授权。</target>
        </trans-unit>
        <trans-unit id="7f5f45ae757c1886e79dc62c9c379591270427af" translate="yes" xml:space="preserve">
          <source>Like &lt;em&gt;output&lt;/em&gt;, the layers can be separated using &lt;code&gt;h_n.view(num_layers, num_directions, batch, hidden_size)&lt;/code&gt; and similarly for &lt;em&gt;c_n&lt;/em&gt;.</source>
          <target state="translated">像&lt;em&gt;输出&lt;/em&gt;一样，可以使用 &lt;code&gt;h_n.view(num_layers, num_directions, batch, hidden_size)&lt;/code&gt; 分隔图层，并且对于&lt;em&gt;c_n&lt;/em&gt;同样。</target>
        </trans-unit>
        <trans-unit id="db10501c95280d11842c24308a3aa239dbe03702" translate="yes" xml:space="preserve">
          <source>Like &lt;em&gt;output&lt;/em&gt;, the layers can be separated using &lt;code&gt;h_n.view(num_layers, num_directions, batch, hidden_size)&lt;/code&gt;.</source>
          <target state="translated">类似于&lt;em&gt;输出&lt;/em&gt;，可以使用 &lt;code&gt;h_n.view(num_layers, num_directions, batch, hidden_size)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e802831cc6f03739c21444458e82e5daba72cf20" translate="yes" xml:space="preserve">
          <source>Like with &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;, the output length must be given in order to recover an even length output:</source>
          <target state="translated">与&lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; 一样&lt;/a&gt;，必须指定输出长度才能恢复偶数长度的输出：</target>
        </trans-unit>
        <trans-unit id="a7c04c64ed3f2a9374590c76c50d3b7f1b18e3da" translate="yes" xml:space="preserve">
          <source>Limitations</source>
          <target state="translated">Limitations</target>
        </trans-unit>
        <trans-unit id="af502f2b37eea07ed9083c7daf40f34553f6fccd" translate="yes" xml:space="preserve">
          <source>Linear</source>
          <target state="translated">Linear</target>
        </trans-unit>
        <trans-unit id="52f5fe1a2af4bafc542342604407d9368527a7e4" translate="yes" xml:space="preserve">
          <source>Linear / Identity</source>
          <target state="translated">线性/身份</target>
        </trans-unit>
        <trans-unit id="462229a1c5dbd1fa15040281e0145c1fe6a072c6" translate="yes" xml:space="preserve">
          <source>Linear Layers</source>
          <target state="translated">线性分层</target>
        </trans-unit>
        <trans-unit id="e0948f7097c1b1ef0dfbf5426722f24d5359dce5" translate="yes" xml:space="preserve">
          <source>Linear functions</source>
          <target state="translated">线性函数</target>
        </trans-unit>
        <trans-unit id="034874347ef77609e931030e8022aaf9f02a5da3" translate="yes" xml:space="preserve">
          <source>LinearReLU</source>
          <target state="translated">LinearReLU</target>
        </trans-unit>
        <trans-unit id="2654ae5325afbc554a0ee441059fbb41cbaf1dbb" translate="yes" xml:space="preserve">
          <source>List Construction</source>
          <target state="translated">建筑清单</target>
        </trans-unit>
        <trans-unit id="92ca3314a6145558197b21fe52556246426b2528" translate="yes" xml:space="preserve">
          <source>List all entrypoints available in &lt;code&gt;github&lt;/code&gt; hubconf.</source>
          <target state="translated">列出 &lt;code&gt;github&lt;/code&gt; hubconf中可用的所有入口点。</target>
        </trans-unit>
        <trans-unit id="c7ba7dcf662374aafe652b03d75cb8f6024519d9" translate="yes" xml:space="preserve">
          <source>Literals</source>
          <target state="translated">Literals</target>
        </trans-unit>
        <trans-unit id="bd26911422165279d6cf54be6a2c3601b7d15a43" translate="yes" xml:space="preserve">
          <source>LnStructured</source>
          <target state="translated">LnStructured</target>
        </trans-unit>
        <trans-unit id="b0a48cb5954a9d7f0650f13e3620734ccd2bae05" translate="yes" xml:space="preserve">
          <source>Load a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; previously saved with &lt;a href=&quot;generated/torch.jit.save#torch.jit.save&quot;&gt;&lt;code&gt;torch.jit.save&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">加载&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt;先前用&lt;a href=&quot;generated/torch.jit.save#torch.jit.save&quot;&gt; &lt;code&gt;torch.jit.save&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="895bd8b16804ae34cfb0e2dc72b19fd2ce1623cf" translate="yes" xml:space="preserve">
          <source>Load a &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt; previously saved with &lt;a href=&quot;torch.jit.save#torch.jit.save&quot;&gt;&lt;code&gt;torch.jit.save&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">加载&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt;先前用&lt;a href=&quot;torch.jit.save#torch.jit.save&quot;&gt; &lt;code&gt;torch.jit.save&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6cb35151312e0bc2bc2c91990968d299f6b351f9" translate="yes" xml:space="preserve">
          <source>Load a model from a github repo or a local directory.</source>
          <target state="translated">从github repo或本地目录加载一个模型。</target>
        </trans-unit>
        <trans-unit id="bbfedf2587224caa5003ffee100938c3cce6187d" translate="yes" xml:space="preserve">
          <source>Loading Batched and Non-Batched Data</source>
          <target state="translated">加载批量和非批量数据</target>
        </trans-unit>
        <trans-unit id="08eab7f7a8aa4c317badff7188cc2c0bb5835fc2" translate="yes" xml:space="preserve">
          <source>Loading models from Hub</source>
          <target state="translated">从Hub加载模型</target>
        </trans-unit>
        <trans-unit id="1df3601943ceea2bf72fecc57231a5842fe092bb" translate="yes" xml:space="preserve">
          <source>Loads a PyTorch C++ extension just-in-time (JIT) from string sources.</source>
          <target state="translated">从字符串源中加载 PyTorch C++扩展的及时性 (JIT)。</target>
        </trans-unit>
        <trans-unit id="e0c28fcdf2966da4afbb699048a05d25a1e72589" translate="yes" xml:space="preserve">
          <source>Loads a PyTorch C++ extension just-in-time (JIT).</source>
          <target state="translated">及时加载PyTorch C++扩展(JIT)。</target>
        </trans-unit>
        <trans-unit id="b8172851e5d518987e24368975b30a3f3621f89a" translate="yes" xml:space="preserve">
          <source>Loads an object saved with &lt;a href=&quot;generated/torch.save#torch.save&quot;&gt;&lt;code&gt;torch.save()&lt;/code&gt;&lt;/a&gt; from a file.</source>
          <target state="translated">从文件加载用&lt;a href=&quot;generated/torch.save#torch.save&quot;&gt; &lt;code&gt;torch.save()&lt;/code&gt; &lt;/a&gt;保存的对象。</target>
        </trans-unit>
        <trans-unit id="abae7d5b43752c21e187437593ecfd6e103a310d" translate="yes" xml:space="preserve">
          <source>Loads an object saved with &lt;a href=&quot;torch.save#torch.save&quot;&gt;&lt;code&gt;torch.save()&lt;/code&gt;&lt;/a&gt; from a file.</source>
          <target state="translated">从文件加载用&lt;a href=&quot;torch.save#torch.save&quot;&gt; &lt;code&gt;torch.save()&lt;/code&gt; &lt;/a&gt;保存的对象。</target>
        </trans-unit>
        <trans-unit id="f2285e79b168630a8a2e0f5ae0c64959ab5ec6e8" translate="yes" xml:space="preserve">
          <source>Loads the Torch serialized object at the given URL.</source>
          <target state="translated">在给定的URL上加载Torch序列化对象。</target>
        </trans-unit>
        <trans-unit id="8e303a8f00a29e8f80582a95270441302240025d" translate="yes" xml:space="preserve">
          <source>Loads the optimizer state.</source>
          <target state="translated">加载优化器状态。</target>
        </trans-unit>
        <trans-unit id="7aaab562b127aeeb78317020a69983cd185a2928" translate="yes" xml:space="preserve">
          <source>Loads the scaler state. If this instance is disabled, &lt;a href=&quot;#torch.cuda.amp.GradScaler.load_state_dict&quot;&gt;&lt;code&gt;load_state_dict()&lt;/code&gt;&lt;/a&gt; is a no-op.</source>
          <target state="translated">加载缩放器状态。如果禁用此实例，则&lt;a href=&quot;#torch.cuda.amp.GradScaler.load_state_dict&quot;&gt; &lt;code&gt;load_state_dict()&lt;/code&gt; &lt;/a&gt;为无操作。</target>
        </trans-unit>
        <trans-unit id="1cfdfc410207837272e43ead5fb1f22980e56511" translate="yes" xml:space="preserve">
          <source>Loads the schedulers state.</source>
          <target state="translated">加载调度器的状态。</target>
        </trans-unit>
        <trans-unit id="bca03b832694245e7d76094e54b41aee61f32b8c" translate="yes" xml:space="preserve">
          <source>Local file system, &lt;code&gt;init_method=&quot;file:///d:/tmp/some_file&quot;&lt;/code&gt;</source>
          <target state="translated">本地文件系统 &lt;code&gt;init_method=&quot;file:///d:/tmp/some_file&quot;&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="9e2a7f51460fa624f29eb4d9345e24061c3314f3" translate="yes" xml:space="preserve">
          <source>LocalResponseNorm</source>
          <target state="translated">LocalResponseNorm</target>
        </trans-unit>
        <trans-unit id="0ca5c78f2cf1102c66c5583023caa00fc87c10db" translate="yes" xml:space="preserve">
          <source>Locally disabling gradient computation</source>
          <target state="translated">本地禁用梯度计算</target>
        </trans-unit>
        <trans-unit id="4029ee70e71d54ca0962e31cd594321368f86629" translate="yes" xml:space="preserve">
          <source>LogNormal</source>
          <target state="translated">LogNormal</target>
        </trans-unit>
        <trans-unit id="5f6a43bda80ee771a5be7b5a9a5b959803eff8b6" translate="yes" xml:space="preserve">
          <source>LogSigmoid</source>
          <target state="translated">LogSigmoid</target>
        </trans-unit>
        <trans-unit id="9fcfd80b59ec37c0d7c822778e2ede78ad5ef865" translate="yes" xml:space="preserve">
          <source>LogSoftmax</source>
          <target state="translated">LogSoftmax</target>
        </trans-unit>
        <trans-unit id="2be6f5f3dfe34ae207c15144d936b394d3342629" translate="yes" xml:space="preserve">
          <source>Log_probs: Tensor of size</source>
          <target state="translated">Log_probs:大小的张量</target>
        </trans-unit>
        <trans-unit id="3a29b9b1054fef950824734dd96565e57e322c96" translate="yes" xml:space="preserve">
          <source>Logarithm of the sum of exponentiations of the inputs in base-2.</source>
          <target state="translated">基数2中的投入指数之和的对数。</target>
        </trans-unit>
        <trans-unit id="273c857a62012f5708232560c688d9c8f7c28ba5" translate="yes" xml:space="preserve">
          <source>Logarithm of the sum of exponentiations of the inputs.</source>
          <target state="translated">输入的指数之和的对数。</target>
        </trans-unit>
        <trans-unit id="65eac6118ebdd55aa38b41f70efdf532e567ffd8" translate="yes" xml:space="preserve">
          <source>Logical Operators</source>
          <target state="translated">逻辑运算符</target>
        </trans-unit>
        <trans-unit id="ca7f4bd6623919e36ffd033c9c80f0e4b3c64f49" translate="yes" xml:space="preserve">
          <source>LogitRelaxedBernoulli</source>
          <target state="translated">LogitRelaxedBernoulli</target>
        </trans-unit>
        <trans-unit id="4e1da61dedde155219f888017b4095799b14e811" translate="yes" xml:space="preserve">
          <source>LongTensor or tuple of LongTensor</source>
          <target state="translated">长张量或长张量元组</target>
        </trans-unit>
        <trans-unit id="79894b78077b352e7913286a38c3c0b109363b18" translate="yes" xml:space="preserve">
          <source>LongTensor that has one more dimension with 1 values at the index of last dimension indicated by the input, and 0 everywhere else.</source>
          <target state="translated">LongTensor多了一个维度,在输入的最后一个维度的索引处有1个值,其他地方都是0。</target>
        </trans-unit>
        <trans-unit id="5889a83849e7452eb90474433b00a93e8ed08a2f" translate="yes" xml:space="preserve">
          <source>Look at the paper: &lt;a href=&quot;https://arxiv.org/abs/1609.05158&quot;&gt;Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network&lt;/a&gt; by Shi et. al (2016) for more details.</source>
          <target state="translated">看论文：Shi等人的论文，&lt;a href=&quot;https://arxiv.org/abs/1609.05158&quot;&gt;使用高效的亚像素卷积神经网络实现实时单图像和视频超分辨率&lt;/a&gt;。al（2016）了解更多详情。</target>
        </trans-unit>
        <trans-unit id="99663eada4ef450e2ffc9048b04317b33e9db27b" translate="yes" xml:space="preserve">
          <source>Lookup returns the most specific (type,type) match ordered by subclass. If the match is ambiguous, a &lt;code&gt;RuntimeWarning&lt;/code&gt; is raised. For example to resolve the ambiguous situation:</source>
          <target state="translated">查找返回子类排序的最特定的（类型，类型）匹配。如果匹配不明确， &lt;code&gt;RuntimeWarning&lt;/code&gt; 引发RuntimeWarning。例如解决模棱两可的情况：</target>
        </trans-unit>
        <trans-unit id="c0224525e6e81a08ff829f4d53c8aff4b15f7024" translate="yes" xml:space="preserve">
          <source>Loss Functions</source>
          <target state="translated">损失功能</target>
        </trans-unit>
        <trans-unit id="bf3496193ce471c7c1c432d246ad87b6a1823e9c" translate="yes" xml:space="preserve">
          <source>Loss functions</source>
          <target state="translated">损失功能</target>
        </trans-unit>
        <trans-unit id="85b9d89a2ed9217bacdf503e6a3c96f95e5750aa" translate="yes" xml:space="preserve">
          <source>Lots of information can be logged for one experiment. To avoid cluttering the UI and have better result clustering, we can group plots by naming them hierarchically. For example, &amp;ldquo;Loss/train&amp;rdquo; and &amp;ldquo;Loss/test&amp;rdquo; will be grouped together, while &amp;ldquo;Accuracy/train&amp;rdquo; and &amp;ldquo;Accuracy/test&amp;rdquo; will be grouped separately in the TensorBoard interface.</source>
          <target state="translated">一个实验可以记录很多信息。为了避免使UI混乱，并更好地进行结果聚类，我们可以通过对图进行分层命名来对图进行分组。例如，&amp;ldquo;损失/火车&amp;rdquo;和&amp;ldquo;损失/测试&amp;rdquo;将被分组在一起，而&amp;ldquo;准确性/火车&amp;rdquo;和&amp;ldquo;准确性/测试&amp;rdquo;将在TensorBoard界面中分别分组。</target>
        </trans-unit>
        <trans-unit id="a851f6b05f669f852f4c7d2a4117b59cc9a5c4f0" translate="yes" xml:space="preserve">
          <source>LowRankMultivariateNormal</source>
          <target state="translated">LowRankMultivariateNormal</target>
        </trans-unit>
        <trans-unit id="c63ae6dd4fc9f9dda66970e827d13f7c73fe841c" translate="yes" xml:space="preserve">
          <source>M</source>
          <target state="translated">M</target>
        </trans-unit>
        <trans-unit id="a452071172e433a963ff286fe987870a980f2d28" translate="yes" xml:space="preserve">
          <source>M (Tensor, optional): the input tensor&amp;rsquo;s mean of size</source>
          <target state="translated">M（张量，可选）：输入张量的均值</target>
        </trans-unit>
        <trans-unit id="7b186e235f284107df6b4dbe6060d2b6a5d9f1e5" translate="yes" xml:space="preserve">
          <source>MAX</source>
          <target state="translated">MAX</target>
        </trans-unit>
        <trans-unit id="0d96af233d36586b84359d8b5bd0b417787982e1" translate="yes" xml:space="preserve">
          <source>MC3 Network definition</source>
          <target state="translated">MC3网络定义</target>
        </trans-unit>
        <trans-unit id="04e66352aa8f9c4c5f26b71bf380973ada994760" translate="yes" xml:space="preserve">
          <source>MIN</source>
          <target state="translated">MIN</target>
        </trans-unit>
        <trans-unit id="351682e4dc04204d75a824bcbf8f8aa6acc15e71" translate="yes" xml:space="preserve">
          <source>MIXED MODE OF (1) and (2):</source>
          <target state="translated">(1)和(2)的混合模式。</target>
        </trans-unit>
        <trans-unit id="59ce6264cd26f13684de464b9aeb65d1d414e559" translate="yes" xml:space="preserve">
          <source>MNASNet</source>
          <target state="translated">MNASNet</target>
        </trans-unit>
        <trans-unit id="533bea2ee6c045fe09ac0124525af723616f331a" translate="yes" xml:space="preserve">
          <source>MNASNet 1.0</source>
          <target state="translated">MNASNet 1.0</target>
        </trans-unit>
        <trans-unit id="9a7425f07104beddfc0bce0274310b0a2e2ea359" translate="yes" xml:space="preserve">
          <source>MNASNet with depth multiplier of 0.5 from &lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo;MnasNet: Platform-Aware Neural Architecture Search for Mobile&amp;rdquo;&lt;/a&gt;. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</source>
          <target state="translated">深度乘数为0.5的MNASNet，来自&lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo; MnasNet：移动平台感知的神经体系结构搜索&amp;rdquo;&lt;/a&gt;。：param pretrained：如果为True，则返回在ImageNet上进行了预训练的模型：type pretrained：bool：param progress：如果为True，则显示下载到stderr的进度条：type progress：bool</target>
        </trans-unit>
        <trans-unit id="98e07c23513158f84ea34588dbe9369dbc7ac20e" translate="yes" xml:space="preserve">
          <source>MNASNet with depth multiplier of 0.75 from &lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo;MnasNet: Platform-Aware Neural Architecture Search for Mobile&amp;rdquo;&lt;/a&gt;. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</source>
          <target state="translated">MNASNet，其深度乘数为0.75，来自&lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo; MnasNet：针对移动设备的平台感知神经体系结构搜索&amp;rdquo;&lt;/a&gt;。：param pretrained：如果为True，则返回在ImageNet上进行了预训练的模型：type pretrained：bool：param progress：如果为True，则显示下载到stderr的进度条：type progress：bool</target>
        </trans-unit>
        <trans-unit id="71754b43c6a0483cdc892a42e6511e7a6757f6a2" translate="yes" xml:space="preserve">
          <source>MNASNet with depth multiplier of 1.0 from &lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo;MnasNet: Platform-Aware Neural Architecture Search for Mobile&amp;rdquo;&lt;/a&gt;. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</source>
          <target state="translated">深度倍数为1.0的MNASNet，来自&lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo; MnasNet：移动平台感知的神经体系结构搜索&amp;rdquo;&lt;/a&gt;。：param pretrained：如果为True，则返回在ImageNet上进行了预训练的模型：type pretrained：bool：param progress：如果为True，则显示下载到stderr的进度条：type progress：bool</target>
        </trans-unit>
        <trans-unit id="6f35e9dd2bd116147e38d35cef6e82d4b56bfc05" translate="yes" xml:space="preserve">
          <source>MNASNet with depth multiplier of 1.3 from &lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo;MnasNet: Platform-Aware Neural Architecture Search for Mobile&amp;rdquo;&lt;/a&gt;. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool</source>
          <target state="translated">深度倍数为1.3的MNASNet，来自&lt;a href=&quot;https://arxiv.org/pdf/1807.11626.pdf&quot;&gt;&amp;ldquo; MnasNet：针对移动设备的平台感知神经体系结构搜索&amp;rdquo;&lt;/a&gt;。：param pretrained：如果为True，则返回在ImageNet上进行了预训练的模型：type pretrained：bool：param progress：如果为True，则显示下载到stderr的进度条：type progress：bool</target>
        </trans-unit>
        <trans-unit id="53880fd71eeaa5e41a5d925ca0243b4befcdb092" translate="yes" xml:space="preserve">
          <source>MSELoss</source>
          <target state="translated">MSELoss</target>
        </trans-unit>
        <trans-unit id="76c7044fa78115fa06a42c399d17d51a9203e830" translate="yes" xml:space="preserve">
          <source>Make a blocking RPC call to run function &lt;code&gt;func&lt;/code&gt; on worker &lt;code&gt;to&lt;/code&gt;. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe.</source>
          <target state="translated">进行阻塞性RPC调用以在worker &lt;code&gt;to&lt;/code&gt; 运行函数 &lt;code&gt;func&lt;/code&gt; 。RPC消息的发送和接收与Python代码的执行并行。此方法是线程安全的。</target>
        </trans-unit>
        <trans-unit id="0d7820212a00c819e77e602a3ef50edfe2f50d52" translate="yes" xml:space="preserve">
          <source>Make a non-blocking RPC call to run function &lt;code&gt;func&lt;/code&gt; on worker &lt;code&gt;to&lt;/code&gt;. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe. This method will immediately return a &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; that can be awaited on.</source>
          <target state="translated">进行非阻塞RPC调用，以在worker &lt;code&gt;to&lt;/code&gt; 运行函数 &lt;code&gt;func&lt;/code&gt; 。RPC消息的发送和接收与Python代码的执行并行。此方法是线程安全的。此方法将立即返回可以等待的&lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="f221c8d616e55bc62ec21b79af5d8282f107f2d3" translate="yes" xml:space="preserve">
          <source>Make a remote call to run &lt;code&gt;func&lt;/code&gt; on worker &lt;code&gt;to&lt;/code&gt; and return an &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;&lt;code&gt;RRef&lt;/code&gt;&lt;/a&gt; to the result value immediately. Worker &lt;code&gt;to&lt;/code&gt; will be the owner of the returned &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;&lt;code&gt;RRef&lt;/code&gt;&lt;/a&gt;, and the worker calling &lt;code&gt;remote&lt;/code&gt; is a user. The owner manages the global reference count of its &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;&lt;code&gt;RRef&lt;/code&gt;&lt;/a&gt;, and the owner &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;&lt;code&gt;RRef&lt;/code&gt;&lt;/a&gt; is only destructed when globally there are no living references to it.</source>
          <target state="translated">做一个远程调用运行 &lt;code&gt;func&lt;/code&gt; 对工人 &lt;code&gt;to&lt;/code&gt; 和返回&lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt; &lt;code&gt;RRef&lt;/code&gt; &lt;/a&gt;立即的结果值。 &lt;code&gt;to&lt;/code&gt; 的工作人员将是返回的&lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt; &lt;code&gt;RRef&lt;/code&gt; &lt;/a&gt;的所有者，而调用 &lt;code&gt;remote&lt;/code&gt; 的工作人员则是用户。所有者管理其&lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt; &lt;code&gt;RRef&lt;/code&gt; &lt;/a&gt;的全局引用计数，并且仅在全局上没有活动引用时销毁所有者&lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt; &lt;code&gt;RRef&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="f909b867fd9b369b49e7c33ae5b4db26cd626b58" translate="yes" xml:space="preserve">
          <source>Make sure that &lt;code&gt;MASTER_ADDR&lt;/code&gt; and &lt;code&gt;MASTER_PORT&lt;/code&gt; are set properly on both workers. Refer to &lt;a href=&quot;distributed#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; API for more details. For example,</source>
          <target state="translated">确保在两个工作 &lt;code&gt;MASTER_ADDR&lt;/code&gt; 上都正确设置了MASTER_ADDR和 &lt;code&gt;MASTER_PORT&lt;/code&gt; 。有关更多详细信息，请参考&lt;a href=&quot;distributed#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt; API。例如，</target>
        </trans-unit>
        <trans-unit id="e23b19686007a5b5511fdca9b9d258395bc61451" translate="yes" xml:space="preserve">
          <source>Make sure that any custom &lt;code&gt;collate_fn&lt;/code&gt;, &lt;code&gt;worker_init_fn&lt;/code&gt; or &lt;code&gt;dataset&lt;/code&gt; code is declared as top level definitions, outside of the &lt;code&gt;__main__&lt;/code&gt; check. This ensures that they are available in worker processes. (this is needed since functions are pickled as references only, not &lt;code&gt;bytecode&lt;/code&gt;.)</source>
          <target state="translated">确保在 &lt;code&gt;__main__&lt;/code&gt; 检查之外将任何自定义 &lt;code&gt;collate_fn&lt;/code&gt; ， &lt;code&gt;worker_init_fn&lt;/code&gt; 或 &lt;code&gt;dataset&lt;/code&gt; 代码声明为顶级定义。这样可以确保它们在工作进程中可用。（这是必需的，因为将函数仅作为引用而不是 &lt;code&gt;bytecode&lt;/code&gt; 进行腌制）。</target>
        </trans-unit>
        <trans-unit id="3140663730440e28711adcbaaf1e5be8f31a832b" translate="yes" xml:space="preserve">
          <source>Makes a &lt;code&gt;cls&lt;/code&gt; instance with the same data pointer as &lt;code&gt;self&lt;/code&gt;. Changes in the output mirror changes in &lt;code&gt;self&lt;/code&gt;, and the output stays attached to the autograd graph. &lt;code&gt;cls&lt;/code&gt; must be a subclass of &lt;code&gt;Tensor&lt;/code&gt;.</source>
          <target state="translated">用与 &lt;code&gt;self&lt;/code&gt; 相同的数据指针创建一个 &lt;code&gt;cls&lt;/code&gt; 实例。输出镜像中的更改 &lt;code&gt;self&lt;/code&gt; 更改，并且输出保持附加到autograd图表上。 &lt;code&gt;cls&lt;/code&gt; 必须是 &lt;code&gt;Tensor&lt;/code&gt; 的子类。</target>
        </trans-unit>
        <trans-unit id="e810a7ac67a7f7afc66a0a46bf613aab777bf84d" translate="yes" xml:space="preserve">
          <source>Makes all future work submitted to the given stream wait for this event.</source>
          <target state="translated">使今后所有提交给给定流的工作都要等待这个事件。</target>
        </trans-unit>
        <trans-unit id="8bb52cb02033ef90d627dbaa4c9db7b7d77cc8ad" translate="yes" xml:space="preserve">
          <source>Makes all future work submitted to the stream wait for an event.</source>
          <target state="translated">让今后所有提交到流媒体的作品都要等待一个事件。</target>
        </trans-unit>
        <trans-unit id="49e2977180d59fe62d760df21fb871ec41183f71" translate="yes" xml:space="preserve">
          <source>Manipulating dimensions</source>
          <target state="translated">操作尺寸</target>
        </trans-unit>
        <trans-unit id="1b2e08cb686bf5745b4dbb6986d0dc5c2cab21bf" translate="yes" xml:space="preserve">
          <source>Manual gradient layouts</source>
          <target state="translated">手动梯度布局</target>
        </trans-unit>
        <trans-unit id="6d825496aa8e7368b658938db8b142e79e1f39ee" translate="yes" xml:space="preserve">
          <source>Many PyTorch functions, which return a view of a tensor, are internally implemented with this function. Those functions, like &lt;a href=&quot;../tensors#torch.Tensor.expand&quot;&gt;&lt;code&gt;torch.Tensor.expand()&lt;/code&gt;&lt;/a&gt;, are easier to read and are therefore more advisable to use.</source>
          <target state="translated">许多PyTorch函数可返回张量视图，并在此函数内部实现。这些功能，例如&lt;a href=&quot;../tensors#torch.Tensor.expand&quot;&gt; &lt;code&gt;torch.Tensor.expand()&lt;/code&gt; &lt;/a&gt;，更易于阅读，因此更可取。</target>
        </trans-unit>
        <trans-unit id="181be8aee3d1857e33b8c6d149cff923185af75a" translate="yes" xml:space="preserve">
          <source>Many models use a sigmoid layer right before the binary cross entropy layer. In this case, combine the two layers using &lt;a href=&quot;nn.functional#torch.nn.functional.binary_cross_entropy_with_logits&quot;&gt;&lt;code&gt;torch.nn.functional.binary_cross_entropy_with_logits()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/torch.nn.bcewithlogitsloss#torch.nn.BCEWithLogitsLoss&quot;&gt;&lt;code&gt;torch.nn.BCEWithLogitsLoss&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;binary_cross_entropy_with_logits&lt;/code&gt; and &lt;code&gt;BCEWithLogits&lt;/code&gt; are safe to autocast.</source>
          <target state="translated">许多模型在二元交叉熵层之前使用S形层。在这种情况下，请使用&lt;a href=&quot;nn.functional#torch.nn.functional.binary_cross_entropy_with_logits&quot;&gt; &lt;code&gt;torch.nn.functional.binary_cross_entropy_with_logits()&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;generated/torch.nn.bcewithlogitsloss#torch.nn.BCEWithLogitsLoss&quot;&gt; &lt;code&gt;torch.nn.BCEWithLogitsLoss&lt;/code&gt; &lt;/a&gt;组合这两层。 &lt;code&gt;binary_cross_entropy_with_logits&lt;/code&gt; 和 &lt;code&gt;BCEWithLogits&lt;/code&gt; 可以安全地自动广播。</target>
        </trans-unit>
        <trans-unit id="877edbec3bdc8c92a89c77b353c1bffa8a6d3f53" translate="yes" xml:space="preserve">
          <source>Many of Python&amp;rsquo;s &lt;a href=&quot;https://docs.python.org/3/library/functions.html&quot;&gt;built-in functions&lt;/a&gt; are supported in TorchScript. The &lt;a href=&quot;https://docs.python.org/3/library/math.html#module-math&quot;&gt;&lt;code&gt;math&lt;/code&gt;&lt;/a&gt; module is also supported (see &lt;a href=&quot;jit_builtin_functions#math-module&quot;&gt;math Module&lt;/a&gt; for details), but no other Python modules (built-in or third party) are supported.</source>
          <target state="translated">TorchScript支持Python的许多&lt;a href=&quot;https://docs.python.org/3/library/functions.html&quot;&gt;内置函数&lt;/a&gt;。该&lt;a href=&quot;https://docs.python.org/3/library/math.html#module-math&quot;&gt; &lt;code&gt;math&lt;/code&gt; &lt;/a&gt;模块还支持（见&lt;a href=&quot;jit_builtin_functions#math-module&quot;&gt;数学模块&lt;/a&gt;的详细信息），但没有其他Python模块（内置或第三方）的支持。</target>
        </trans-unit>
        <trans-unit id="9a0216a95943fa820f7459f0a77368a7416b7939" translate="yes" xml:space="preserve">
          <source>Map-style datasets</source>
          <target state="translated">地图式数据集</target>
        </trans-unit>
        <trans-unit id="8b9cac864fb0630d3e0375c972324ae14f0a7cbf" translate="yes" xml:space="preserve">
          <source>MarginRankingLoss</source>
          <target state="translated">MarginRankingLoss</target>
        </trans-unit>
        <trans-unit id="1373a08f797bf15732fbadb687eff12b2e03c08f" translate="yes" xml:space="preserve">
          <source>Marks given tensors as modified in an in-place operation.</source>
          <target state="translated">标记在原地操作中修改的给定时序。</target>
        </trans-unit>
        <trans-unit id="64d7f425f84dfa95b2a3d1450fcc8fbbce25eb01" translate="yes" xml:space="preserve">
          <source>Marks outputs as non-differentiable.</source>
          <target state="translated">将输出标记为无差别。</target>
        </trans-unit>
        <trans-unit id="3b3b6161e2be4020ea8045bf777a1b7a9a9ef9d7" translate="yes" xml:space="preserve">
          <source>Mask R-CNN</source>
          <target state="translated">屏蔽R-CNN</target>
        </trans-unit>
        <trans-unit id="de79ab812f56430c47ab7094aa9e5f95c476c9d5" translate="yes" xml:space="preserve">
          <source>Mask R-CNN ResNet-50 FPN</source>
          <target state="translated">掩膜R-CNN ResNet-50 FPN</target>
        </trans-unit>
        <trans-unit id="e283616925f71932f219908f30d53b3df610dd54" translate="yes" xml:space="preserve">
          <source>Mask R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.</source>
          <target state="translated">掩模R-CNN可输出到ONNX,输入图像大小固定的批量大小。</target>
        </trans-unit>
        <trans-unit id="370101c55e76bb32e62a76c0f757b67a5a083820" translate="yes" xml:space="preserve">
          <source>Math operations</source>
          <target state="translated">数学运算</target>
        </trans-unit>
        <trans-unit id="f266f996d2555bfc5ac129ce470aa82f278b2388" translate="yes" xml:space="preserve">
          <source>Matrix multiplication ops: &lt;a href=&quot;name_inference#contracts-away-dims-doc&quot;&gt;Contracts away dims&lt;/a&gt;</source>
          <target state="translated">矩阵乘法运算：&lt;a href=&quot;name_inference#contracts-away-dims-doc&quot;&gt;缩小暗淡&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7a75fc5d78987f3dab3a65065c4b1d4f0692e098" translate="yes" xml:space="preserve">
          <source>Matrix product of two tensors.</source>
          <target state="translated">矩阵乘积的两个腾博会登录。</target>
        </trans-unit>
        <trans-unit id="eccdf32fe79dd9c576c8118d98c630238fca6bb7" translate="yes" xml:space="preserve">
          <source>MaxPool1d</source>
          <target state="translated">MaxPool1d</target>
        </trans-unit>
        <trans-unit id="3b3d541959fe8a8dd18b510e5a3f48b2498a53b4" translate="yes" xml:space="preserve">
          <source>MaxPool2d</source>
          <target state="translated">MaxPool2d</target>
        </trans-unit>
        <trans-unit id="98908ed042b1a9862ea5e9d4da6ed687e958f368" translate="yes" xml:space="preserve">
          <source>MaxPool3d</source>
          <target state="translated">MaxPool3d</target>
        </trans-unit>
        <trans-unit id="4a5419bdd0aea2636e49e9d7ca08451ce7530629" translate="yes" xml:space="preserve">
          <source>MaxUnpool1d</source>
          <target state="translated">MaxUnpool1d</target>
        </trans-unit>
        <trans-unit id="dde49d17029a94b9195f0444462e31e369394942" translate="yes" xml:space="preserve">
          <source>MaxUnpool2d</source>
          <target state="translated">MaxUnpool2d</target>
        </trans-unit>
        <trans-unit id="cbb9a5d6b569b7fb0a0f22c9d9d9d31e12466046" translate="yes" xml:space="preserve">
          <source>MaxUnpool3d</source>
          <target state="translated">MaxUnpool3d</target>
        </trans-unit>
        <trans-unit id="fe4de202eb4956062e7cfb72ed4b684ffbb5bd30" translate="yes" xml:space="preserve">
          <source>Measures the element-wise mean squared error.</source>
          <target state="translated">测量元素的均方误差。</target>
        </trans-unit>
        <trans-unit id="09b86d975dd6b1f5a73e1c3517013bb23d540761" translate="yes" xml:space="preserve">
          <source>Measures the loss given an input tensor</source>
          <target state="translated">测量输入张量的损耗</target>
        </trans-unit>
        <trans-unit id="1d01e0fee07072aec458beb4c034b607f10f63a9" translate="yes" xml:space="preserve">
          <source>Members:</source>
          <target state="translated">Members:</target>
        </trans-unit>
        <trans-unit id="af23da357d652a02423313397c1adbd940726581" translate="yes" xml:space="preserve">
          <source>Memory Pinning</source>
          <target state="translated">记忆钉</target>
        </trans-unit>
        <trans-unit id="1c5536b98ab016fe61807564b4a34c80211a7613" translate="yes" xml:space="preserve">
          <source>Memory management</source>
          <target state="translated">内存管理</target>
        </trans-unit>
        <trans-unit id="7c5f0da2191ca27795b8631522c1ca879f1716c9" translate="yes" xml:space="preserve">
          <source>Method Calls</source>
          <target state="translated">方法调用</target>
        </trans-unit>
        <trans-unit id="fcf32dbe7b3f386bd441fb137d4760dd5708ad54" translate="yes" xml:space="preserve">
          <source>Method to compute the entropy using Bregman divergence of the log normalizer.</source>
          <target state="translated">利用对数归一化的Bregman发散计算熵的方法。</target>
        </trans-unit>
        <trans-unit id="b3dfdaa160857d292036f521c9ea3c58c5d52fbd" translate="yes" xml:space="preserve">
          <source>Methods such as &lt;code&gt;var.backward(), var.detach(), var.register_hook()&lt;/code&gt; now work on tensors with the same method names.</source>
          <target state="translated">诸如 &lt;code&gt;var.backward(), var.detach(), var.register_hook()&lt;/code&gt; 现在可以在具有相同方法名称的张量上使用。</target>
        </trans-unit>
        <trans-unit id="b69c14783d5324daa62920c742f93c096f643f4f" translate="yes" xml:space="preserve">
          <source>Methods which mutate a tensor are marked with an underscore suffix. For example, &lt;code&gt;torch.FloatTensor.abs_()&lt;/code&gt; computes the absolute value in-place and returns the modified tensor, while &lt;code&gt;torch.FloatTensor.abs()&lt;/code&gt; computes the result in a new tensor.</source>
          <target state="translated">改变张量的方法用下划线后缀标记。例如， &lt;code&gt;torch.FloatTensor.abs_()&lt;/code&gt; 计算就地绝对值并返回修改后的张量，而 &lt;code&gt;torch.FloatTensor.abs()&lt;/code&gt; 计算新张量的结果。</target>
        </trans-unit>
        <trans-unit id="9b7c339ea87ccc784ad1deadd85bf646248e9d73" translate="yes" xml:space="preserve">
          <source>Methods which take a device will generally accept a (properly formatted) string or (legacy) integer device ordinal, i.e. the following are all equivalent:</source>
          <target state="translated">接受设备的方法一般会接受(正确格式化的)字符串或(传统的)整数设备序数,即以下都是等价的。</target>
        </trans-unit>
        <trans-unit id="adfdb53a472af0c4b913a760ea8b9484fe72338d" translate="yes" xml:space="preserve">
          <source>Metric type:</source>
          <target state="translated">公制类型:</target>
        </trans-unit>
        <trans-unit id="e067f6663a0605e92a7219ee53ccfb43fa76eaf3" translate="yes" xml:space="preserve">
          <source>Migrating to PyTorch 1.2 Recursive Scripting API</source>
          <target state="translated">迁移至 PyTorch 1.2 递归脚本 API</target>
        </trans-unit>
        <trans-unit id="ff26bc094b922f3c06f27a4d1e69dcd28d4c0e3c" translate="yes" xml:space="preserve">
          <source>Mixing Tracing and Scripting</source>
          <target state="translated">混合跟踪和脚本</target>
        </trans-unit>
        <trans-unit id="6a693905dba81b2e3c99a490b1ec25a2a6253235" translate="yes" xml:space="preserve">
          <source>MixtureSameFamily</source>
          <target state="translated">MixtureSameFamily</target>
        </trans-unit>
        <trans-unit id="9ff690f5176e9e122a08292f5ff8b357a6c354e0" translate="yes" xml:space="preserve">
          <source>MobileNet V2</source>
          <target state="translated">MobileNet V2</target>
        </trans-unit>
        <trans-unit id="c901047dcc843db7018568e9b72b5dbf5a54540d" translate="yes" xml:space="preserve">
          <source>MobileNet v2</source>
          <target state="translated">MobileNet v2</target>
        </trans-unit>
        <trans-unit id="b8ff02892916ff59f7fbd4e617fccd01f6bca576" translate="yes" xml:space="preserve">
          <source>Module</source>
          <target state="translated">Module</target>
        </trans-unit>
        <trans-unit id="9c3f2aba6913de8da00faed8e2df2428bb1257b7" translate="yes" xml:space="preserve">
          <source>Module Attributes</source>
          <target state="translated">模块属性</target>
        </trans-unit>
        <trans-unit id="3cb22c5e4eabc0e7d3ccf2b60e66f53421b63149" translate="yes" xml:space="preserve">
          <source>Module Index</source>
          <target state="translated">模块索引</target>
        </trans-unit>
        <trans-unit id="0b2ce983642fdce8da88af1aafe61bc6a6ee4bfb" translate="yes" xml:space="preserve">
          <source>ModuleDict</source>
          <target state="translated">ModuleDict</target>
        </trans-unit>
        <trans-unit id="e12ef0c5e95f31ebc597e0b6fd821d6b604f3f95" translate="yes" xml:space="preserve">
          <source>ModuleList</source>
          <target state="translated">ModuleList</target>
        </trans-unit>
        <trans-unit id="04e9462c0ff02bb9032b92abd45881a3c7e15fb7" translate="yes" xml:space="preserve">
          <source>Modules</source>
          <target state="translated">Modules</target>
        </trans-unit>
        <trans-unit id="9dbae4788d0b2dea68c6e66c9e0ff2aed52f652e" translate="yes" xml:space="preserve">
          <source>Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:</source>
          <target state="translated">模块也可以包含其他模块,允许将它们嵌套在一个树形结构中。你可以将子模块分配为常规属性。</target>
        </trans-unit>
        <trans-unit id="4a649c7ebd14c451b9eadae1b57b3c6722590226" translate="yes" xml:space="preserve">
          <source>More Information about RPC Autograd</source>
          <target state="translated">更多关于RPC Autograd的信息</target>
        </trans-unit>
        <trans-unit id="9cb54dedb558ff4177e32b924117d76eb6014c23" translate="yes" xml:space="preserve">
          <source>More Information about RRef</source>
          <target state="translated">更多关于RRef的信息</target>
        </trans-unit>
        <trans-unit id="cea97ccd7b3c8bf2304a429a1f4a38f55fbcf7bc" translate="yes" xml:space="preserve">
          <source>More details can be found in the paper &lt;a href=&quot;https://arxiv.org/abs/1704.07483&quot;&gt;Continuously Differentiable Exponential Linear Units&lt;/a&gt; .</source>
          <target state="translated">更多细节可以在论文&lt;a href=&quot;https://arxiv.org/abs/1704.07483&quot;&gt;连续微分指数线性单位中找到&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4458c625976778e29149ea1b97ed51860f9fe8ca" translate="yes" xml:space="preserve">
          <source>More details can be found in the paper &lt;a href=&quot;https://arxiv.org/abs/1706.02515&quot;&gt;Self-Normalizing Neural Networks&lt;/a&gt; .</source>
          <target state="translated">更多细节可以在论文《&lt;a href=&quot;https://arxiv.org/abs/1706.02515&quot;&gt;自规范神经网络》中找到&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="8edf225c0a72c5fe13ece57cbf2f54c6b24e17b0" translate="yes" xml:space="preserve">
          <source>More than one element of a broadcasted tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.</source>
          <target state="translated">广播张量的一个以上的元素可能指的是一个单一的内存位置。因此,原地操作(特别是那些有向量的操作)可能会导致不正确的行为。如果你需要向张量写入,请先克隆它们。</target>
        </trans-unit>
        <trans-unit id="1d81819b85f2cb3da872c9da3d0ec33cdc7aecdb" translate="yes" xml:space="preserve">
          <source>More than one element of a created tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.</source>
          <target state="translated">所创建的张量的一个以上的元素可能指的是一个单一的内存位置。因此,原地操作(特别是那些有向量的操作)可能会导致不正确的行为。如果你需要向张量写入,请先克隆它们。</target>
        </trans-unit>
        <trans-unit id="304f7ec57645990453077c598f4fe8c37d2314ab" translate="yes" xml:space="preserve">
          <source>More than one element of an expanded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.</source>
          <target state="translated">扩展张量的一个以上的元素可能指的是一个单一的内存位置。因此,原地操作(特别是那些有向量的操作)可能会导致不正确的行为。如果你需要向张量写入,请先克隆它们。</target>
        </trans-unit>
        <trans-unit id="f38e0558c4bf9056c21ea63c5960cbf11fd1b539" translate="yes" xml:space="preserve">
          <source>More than one element of the created tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.</source>
          <target state="translated">所创建的张量中可能有一个以上的元素指向一个内存位置。因此,原地操作(特别是那些有向量的操作)可能会导致不正确的行为。如果你需要向张量写入,请先克隆它们。</target>
        </trans-unit>
        <trans-unit id="599668aa340279a3862bca37e7e4a6db462ba962" translate="yes" xml:space="preserve">
          <source>More than one element of the unfolded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensor, please clone it first.</source>
          <target state="translated">展开张量的一个以上的元素可能指的是一个单一的内存位置。因此,原地操作(特别是那些有向量的操作)可能会导致不正确的行为。如果你需要向张量写入,请先克隆它。</target>
        </trans-unit>
        <trans-unit id="805667fbfb7f6f1745944861662036a8e58f3c82" translate="yes" xml:space="preserve">
          <source>Moreover, as for &lt;a href=&quot;#torch.Tensor.gather&quot;&gt;&lt;code&gt;gather()&lt;/code&gt;&lt;/a&gt;, the values of &lt;code&gt;index&lt;/code&gt; must be between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;self.size(dim) - 1&lt;/code&gt; inclusive, and all values in a row along the specified dimension &lt;a href=&quot;#torch.Tensor.dim&quot;&gt;&lt;code&gt;dim&lt;/code&gt;&lt;/a&gt; must be unique.</source>
          <target state="translated">此外，对于&lt;a href=&quot;#torch.Tensor.gather&quot;&gt; &lt;code&gt;gather()&lt;/code&gt; &lt;/a&gt;，的值 &lt;code&gt;index&lt;/code&gt; 必须介于 &lt;code&gt;0&lt;/code&gt; 和 &lt;code&gt;self.size(dim) - 1&lt;/code&gt; 以下，并且在沿着所述规定尺寸的行的所有值&lt;a href=&quot;#torch.Tensor.dim&quot;&gt; &lt;code&gt;dim&lt;/code&gt; &lt;/a&gt;必须是唯一的。</target>
        </trans-unit>
        <trans-unit id="dbc5f1c387e0131e10dadc48c7f1ed13f16fda56" translate="yes" xml:space="preserve">
          <source>Most attribute types can be inferred, so &lt;code&gt;torch.jit.Attribute&lt;/code&gt; is not necessary. For empty container types, annotate their types using &lt;a href=&quot;https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations&quot;&gt;PEP 526-style&lt;/a&gt; class annotations.</source>
          <target state="translated">可以推断大多数属性类型，因此 &lt;code&gt;torch.jit.Attribute&lt;/code&gt; 。对于空容器类型，请使用&lt;a href=&quot;https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations&quot;&gt;PEP 526样式&lt;/a&gt;类注释对它们的类​​型进行注释。</target>
        </trans-unit>
        <trans-unit id="ed994a7cd5d0a3520430a627f9865ebc68ca6011" translate="yes" xml:space="preserve">
          <source>Moved to &lt;code&gt;torch.hub&lt;/code&gt;.</source>
          <target state="translated">移至 &lt;code&gt;torch.hub&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2b0f27b71a53a0f7447384ac613f55c4204b1204" translate="yes" xml:space="preserve">
          <source>Moves all model parameters and buffers to the CPU.</source>
          <target state="translated">将所有模型参数和缓冲区移动到CPU。</target>
        </trans-unit>
        <trans-unit id="e8a79081679af4528b212d87c458a607691d5b3e" translate="yes" xml:space="preserve">
          <source>Moves all model parameters and buffers to the GPU.</source>
          <target state="translated">将所有模型参数和缓冲区移动到GPU上。</target>
        </trans-unit>
        <trans-unit id="e874c62b11df47c2d692fb8935adeff7a382eea3" translate="yes" xml:space="preserve">
          <source>Moves and/or casts the parameters and buffers.</source>
          <target state="translated">移动和/或投掷参数和缓冲区。</target>
        </trans-unit>
        <trans-unit id="7729e2003ebf66fb44071aeaadd11a5376f40f12" translate="yes" xml:space="preserve">
          <source>Moves the dimension(s) of &lt;code&gt;input&lt;/code&gt; at the position(s) in &lt;code&gt;source&lt;/code&gt; to the position(s) in &lt;code&gt;destination&lt;/code&gt;.</source>
          <target state="translated">将 &lt;code&gt;source&lt;/code&gt; 位置上的 &lt;code&gt;input&lt;/code&gt; 维移动到 &lt;code&gt;destination&lt;/code&gt; 位置。</target>
        </trans-unit>
        <trans-unit id="35ac3446a4eebf31c31aeb5dad10efd9c64f3049" translate="yes" xml:space="preserve">
          <source>Moves the storage to shared memory.</source>
          <target state="translated">将存储移至共享内存。</target>
        </trans-unit>
        <trans-unit id="7b5555e73ea9cfb4e4bc00be9e2f28d1b533e2f3" translate="yes" xml:space="preserve">
          <source>Moves the underlying storage to shared memory.</source>
          <target state="translated">将底层存储移至共享内存。</target>
        </trans-unit>
        <trans-unit id="8a37b99360165eb79c6362b8c59ca260f24fc9e9" translate="yes" xml:space="preserve">
          <source>Multi-GPU collective functions</source>
          <target state="translated">多GPU集体功能</target>
        </trans-unit>
        <trans-unit id="f8c9042e53df42e885d4b0f01c778b6c31356f24" translate="yes" xml:space="preserve">
          <source>Multi-Node multi-process distributed training: (e.g. two nodes)</source>
          <target state="translated">多节点多进程分布式训练。(如:两个节点)</target>
        </trans-unit>
        <trans-unit id="cffbb32b31aa84528c7a48bc5418f9c2b5817403" translate="yes" xml:space="preserve">
          <source>Multi-process data loading</source>
          <target state="translated">多进程数据加载</target>
        </trans-unit>
        <trans-unit id="e8e85a354a72efff2947c00f8ff7c48fed5f6652" translate="yes" xml:space="preserve">
          <source>MultiHead</source>
          <target state="translated">MultiHead</target>
        </trans-unit>
        <trans-unit id="aaf9b486dc5c2543a40061716ec697ddb8e0fbae" translate="yes" xml:space="preserve">
          <source>MultiLabelMarginLoss</source>
          <target state="translated">MultiLabelMarginLoss</target>
        </trans-unit>
        <trans-unit id="0bd3257d9c850deeb628f346481873a6ea866f39" translate="yes" xml:space="preserve">
          <source>MultiLabelSoftMarginLoss</source>
          <target state="translated">MultiLabelSoftMarginLoss</target>
        </trans-unit>
        <trans-unit id="65baaa8172153705ce14a7161536175046e824b3" translate="yes" xml:space="preserve">
          <source>MultiMarginLoss</source>
          <target state="translated">MultiMarginLoss</target>
        </trans-unit>
        <trans-unit id="d53b7a9617160a0821d5bb5cb1a941ffdfcd6b08" translate="yes" xml:space="preserve">
          <source>MultiheadAttention</source>
          <target state="translated">MultiheadAttention</target>
        </trans-unit>
        <trans-unit id="4319c2711cce37df073896273f25e2e05b39cbdd" translate="yes" xml:space="preserve">
          <source>Multinomial</source>
          <target state="translated">Multinomial</target>
        </trans-unit>
        <trans-unit id="7814aed0b837819b215ee4613ccc9b3a4ba6040e" translate="yes" xml:space="preserve">
          <source>Multiple Assignments</source>
          <target state="translated">多重作业</target>
        </trans-unit>
        <trans-unit id="2f5facc713a0a585db79dc694e29dcb69110ceb7" translate="yes" xml:space="preserve">
          <source>Multiplies (&amp;lsquo;scales&amp;rsquo;) a tensor or list of tensors by the scale factor.</source>
          <target state="translated">将张量或张量列表乘以（&amp;ldquo;缩放&amp;rdquo;）比例因子。</target>
        </trans-unit>
        <trans-unit id="67620498370f67e43d30e9f331640d50be1f106c" translate="yes" xml:space="preserve">
          <source>Multiplies &lt;code&gt;mat&lt;/code&gt; (given by &lt;code&gt;input3&lt;/code&gt;) by the orthogonal &lt;code&gt;Q&lt;/code&gt; matrix of the QR factorization formed by &lt;a href=&quot;generated/torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt; that is represented by &lt;code&gt;(a, tau)&lt;/code&gt; (given by (&lt;code&gt;input&lt;/code&gt;, &lt;code&gt;input2&lt;/code&gt;)).</source>
          <target state="translated">将 &lt;code&gt;mat&lt;/code&gt; （由 &lt;code&gt;input3&lt;/code&gt; 给定）乘以&lt;a href=&quot;generated/torch.geqrf#torch.geqrf&quot;&gt; &lt;code&gt;torch.geqrf()&lt;/code&gt; &lt;/a&gt;形成的QR因式分解的正交 &lt;code&gt;Q&lt;/code&gt; 矩阵，该torch.geqrf（）由 &lt;code&gt;(a, tau)&lt;/code&gt; （由（ &lt;code&gt;input&lt;/code&gt; ， &lt;code&gt;input2&lt;/code&gt; ）给定）。</target>
        </trans-unit>
        <trans-unit id="76de5400e24dedf164da206bd33a31ebb079cf8f" translate="yes" xml:space="preserve">
          <source>Multiplies &lt;code&gt;mat&lt;/code&gt; (given by &lt;code&gt;input3&lt;/code&gt;) by the orthogonal &lt;code&gt;Q&lt;/code&gt; matrix of the QR factorization formed by &lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt; that is represented by &lt;code&gt;(a, tau)&lt;/code&gt; (given by (&lt;code&gt;input&lt;/code&gt;, &lt;code&gt;input2&lt;/code&gt;)).</source>
          <target state="translated">将 &lt;code&gt;mat&lt;/code&gt; （由 &lt;code&gt;input3&lt;/code&gt; 给定）乘以&lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt; &lt;code&gt;torch.geqrf()&lt;/code&gt; &lt;/a&gt;形成的QR因式分解的正交 &lt;code&gt;Q&lt;/code&gt; 矩阵，该torch.geqrf（）由 &lt;code&gt;(a, tau)&lt;/code&gt; （由（ &lt;code&gt;input&lt;/code&gt; ， &lt;code&gt;input2&lt;/code&gt; ）给定）。</target>
        </trans-unit>
        <trans-unit id="329fd1dd91c45edaf7c72f188795f5545aaec268" translate="yes" xml:space="preserve">
          <source>Multiplies each element of the input &lt;code&gt;input&lt;/code&gt; with the scalar &lt;code&gt;other&lt;/code&gt; and returns a new resulting tensor.</source>
          <target state="translated">将输入 &lt;code&gt;input&lt;/code&gt; 每个元素与标量 &lt;code&gt;other&lt;/code&gt; 相乘，并返回一个新的结果张量。</target>
        </trans-unit>
        <trans-unit id="a50721c699d328b377c86aaee9cd7e1759fd60d6" translate="yes" xml:space="preserve">
          <source>Multiply the learning rate of each parameter group by the factor given in the specified function. When last_epoch=-1, sets initial lr as lr.</source>
          <target state="translated">将每个参数组的学习率乘以指定函数中给定的系数,当 last_epoch=-1 时,设置初始 lr 为 lr。当last_epoch=-1时,设置初始lr为lr。</target>
        </trans-unit>
        <trans-unit id="4d2004ec3a414cc551a0c27958276999f72ee752" translate="yes" xml:space="preserve">
          <source>Multiprocessing best practices</source>
          <target state="translated">多重处理最佳做法</target>
        </trans-unit>
        <trans-unit id="70f967b7b383e533c089648a15d728ac8a059ce8" translate="yes" xml:space="preserve">
          <source>Multiprocessing package - torch.multiprocessing</source>
          <target state="translated">多处理包--Torch.multiprocessing。</target>
        </trans-unit>
        <trans-unit id="c1a37b0bcc3effc97da7c8ab9689abbfff92b501" translate="yes" xml:space="preserve">
          <source>MultivariateNormal</source>
          <target state="translated">MultivariateNormal</target>
        </trans-unit>
        <trans-unit id="b51a60734da64be0e618bacbea2865a8a7dcd669" translate="yes" xml:space="preserve">
          <source>N</source>
          <target state="translated">N</target>
        </trans-unit>
        <trans-unit id="129ca909e4d3ed97aba21ae73582b3bd13e114f8" translate="yes" xml:space="preserve">
          <source>N = \text{batch size}</source>
          <target state="translated">N=text{batch size}。</target>
        </trans-unit>
        <trans-unit id="9008c254267c7b8e353b34e82cc9a3ccdffad81e" translate="yes" xml:space="preserve">
          <source>N \times 2 \times 3</source>
          <target state="translated">N/times 2/times 3</target>
        </trans-unit>
        <trans-unit id="1b8b351ea25ef1c009076f028a05723a16aa0eb6" translate="yes" xml:space="preserve">
          <source>N \times 3 \times 4</source>
          <target state="translated">N/times 3/times 4</target>
        </trans-unit>
        <trans-unit id="004d23aba5b55baf9486f9aab3e50a94de7147b2" translate="yes" xml:space="preserve">
          <source>N \times C \times D \times H \times W</source>
          <target state="translated">N/times C/times D/times H/times W.</target>
        </trans-unit>
        <trans-unit id="14734700ec8d366b9e0bb39033413068e35b488f" translate="yes" xml:space="preserve">
          <source>N \times C \times H \times W</source>
          <target state="translated">N次 C次 H次 W次</target>
        </trans-unit>
        <trans-unit id="db16fdaa05353f31c7f5c303d74f40ba9971f144" translate="yes" xml:space="preserve">
          <source>N \times H \times W \times 2</source>
          <target state="translated">N/times H/times W/times 2</target>
        </trans-unit>
        <trans-unit id="7fd20744aa54e5cfb63458f679b6501918e1991e" translate="yes" xml:space="preserve">
          <source>N \times M</source>
          <target state="translated">N/times M</target>
        </trans-unit>
        <trans-unit id="c79a64aa7f7a8e49dfba455783b75f9a68074b19" translate="yes" xml:space="preserve">
          <source>N is the batch size, &lt;code&gt;*&lt;/code&gt; means any number of additional dimensions</source>
          <target state="translated">N是批次大小， &lt;code&gt;*&lt;/code&gt; 表示任意数量的附加尺寸</target>
        </trans-unit>
        <trans-unit id="72e211ac7f7abd7ef74bebbff3bdb5e0da1f0db9" translate="yes" xml:space="preserve">
          <source>N-D</source>
          <target state="translated">N-D</target>
        </trans-unit>
        <trans-unit id="7610334d74930225de51e4e4fce7e6de6c627870" translate="yes" xml:space="preserve">
          <source>NCCL has also provided a number of environment variables for fine-tuning purposes.</source>
          <target state="translated">NCCL还提供了一些环境变量用于微调。</target>
        </trans-unit>
        <trans-unit id="165a578116c3590cf83add849da91470c0378dc0" translate="yes" xml:space="preserve">
          <source>NLLLoss</source>
          <target state="translated">NLLLoss</target>
        </trans-unit>
        <trans-unit id="c63922691a03fc61bc5d30e1155494b0495ebce4" translate="yes" xml:space="preserve">
          <source>NN module forward passes have code that don&amp;rsquo;t support named tensors and will error out appropriately.</source>
          <target state="translated">NN模块正向传递的代码不支持命名张量，并且会适当地出错。</target>
        </trans-unit>
        <trans-unit id="fb2adaff54f80ebafd8d75fb8e20d7e6464bee58" translate="yes" xml:space="preserve">
          <source>NN module parameters are unnamed, so outputs may be partially named.</source>
          <target state="translated">NN模块参数未命名,所以输出可以部分命名。</target>
        </trans-unit>
        <trans-unit id="fb77d2b5cb211a1bafb61c44a7f17e0c98a57348" translate="yes" xml:space="preserve">
          <source>NN modules are currently unsupported. This can lead to the following when calling modules with named tensor inputs:</source>
          <target state="translated">目前不支持NN模块。当调用具有命名张量输入的模块时,这可能导致以下情况。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
