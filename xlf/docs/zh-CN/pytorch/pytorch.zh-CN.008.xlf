<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="0579bfd58f6bcb591b037d0d86676f26b53c86e8" translate="yes" xml:space="preserve">
          <source>NVIDIA Tools Extension (NVTX)</source>
          <target state="translated">NVIDIA工具扩展(NVTX)</target>
        </trans-unit>
        <trans-unit id="855336587fa59262965cdb9a2a6114933586800b" translate="yes" xml:space="preserve">
          <source>N_i</source>
          <target state="translated">N_i</target>
        </trans-unit>
        <trans-unit id="c543b3c390ed1ce750f7a99ce30092a4fe11d2d6" translate="yes" xml:space="preserve">
          <source>NaN values in &lt;code&gt;grid&lt;/code&gt; would be interpreted as &lt;code&gt;-1&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;grid&lt;/code&gt; NaN值将被解释为 &lt;code&gt;-1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="709a23220f2c3d64d1e1d6d18c4d5280f8d82fca" translate="yes" xml:space="preserve">
          <source>Name</source>
          <target state="translated">Name</target>
        </trans-unit>
        <trans-unit id="0585ffb115d09e675d597261b20f2d75a3eb03a7" translate="yes" xml:space="preserve">
          <source>Name propagation semantics</source>
          <target state="translated">名称传播语义</target>
        </trans-unit>
        <trans-unit id="817cc3c2ad5448f94a4d253bbcac8dd6cc2d1cf9" translate="yes" xml:space="preserve">
          <source>Named Tensors</source>
          <target state="translated">命名张力器</target>
        </trans-unit>
        <trans-unit id="e90e823237a9f4d7fe13c9da511106ba9aee2f03" translate="yes" xml:space="preserve">
          <source>Named Tensors allow users to give explicit names to tensor dimensions. In most cases, operations that take dimension parameters will accept dimension names, avoiding the need to track dimensions by position. In addition, named tensors use names to automatically check that APIs are being used correctly at runtime, providing extra safety. Names can also be used to rearrange dimensions, for example, to support &amp;ldquo;broadcasting by name&amp;rdquo; rather than &amp;ldquo;broadcasting by position&amp;rdquo;.</source>
          <target state="translated">命名张量允许用户给张量尺寸指定显式名称。在大多数情况下，采用尺寸参数的操作将接受尺寸名称，从而避免了按位置跟踪尺寸的需要。此外，命名张量使用名称来自动检查运行时是否正确使用了API，从而提供了额外的安全性。名称也可以用于重新排列尺寸，例如，支持&amp;ldquo;按名称广播&amp;rdquo;而不是&amp;ldquo;按位置广播&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="6e3e9a74ab4dd7c6ec7d57c873cd59674e123d29" translate="yes" xml:space="preserve">
          <source>Named Tensors operator coverage</source>
          <target state="translated">已命名的Tensors运营商覆盖率</target>
        </trans-unit>
        <trans-unit id="00f19d99a1a121465f53068cd3e37c47ef1fad72" translate="yes" xml:space="preserve">
          <source>Named Tuples</source>
          <target state="translated">已命名的图元组</target>
        </trans-unit>
        <trans-unit id="d906a9f233a4d60e7ff3266a36abd411378985f0" translate="yes" xml:space="preserve">
          <source>Named dimensions</source>
          <target state="translated">命名尺寸</target>
        </trans-unit>
        <trans-unit id="d69a85347956dd430a196f336efedfda312c73cf" translate="yes" xml:space="preserve">
          <source>Named dimensions, like regular Tensor dimensions, are ordered. &lt;code&gt;tensor.names[i]&lt;/code&gt; is the name of dimension &lt;code&gt;i&lt;/code&gt; of &lt;code&gt;tensor&lt;/code&gt;.</source>
          <target state="translated">命名尺寸（如常规Tensor尺寸）是有序的。 &lt;code&gt;tensor.names[i]&lt;/code&gt; 是维度的名称 &lt;code&gt;i&lt;/code&gt; 的 &lt;code&gt;tensor&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="87ade27d950d661d18340b5de9b3d2e0c2886884" translate="yes" xml:space="preserve">
          <source>Named tensor API reference</source>
          <target state="translated">已命名的张量API参考</target>
        </trans-unit>
        <trans-unit id="554d6e43a251766628f65b287dfa2b6bb568c6f8" translate="yes" xml:space="preserve">
          <source>Named tensors can coexist with unnamed tensors; named tensors are instances of &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt;. Unnamed tensors have &lt;code&gt;None&lt;/code&gt;-named dimensions. Named tensors do not require all dimensions to be named.</source>
          <target state="translated">命名张量可以与未命名张量共存；命名张量是&lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; 的&lt;/a&gt;实例。未命名的张量具有&amp;ldquo; &lt;code&gt;None&lt;/code&gt; 命名&amp;rdquo;维。命名张量不需要命名所有尺寸。</target>
        </trans-unit>
        <trans-unit id="9babc66ca4089f8e16cccf70d5e5125ac942af22" translate="yes" xml:space="preserve">
          <source>Named tensors use names to automatically check that APIs are being called correctly at runtime. This occurs in a process called &lt;em&gt;name inference&lt;/em&gt;. More formally, name inference consists of the following two steps:</source>
          <target state="translated">命名张量使用名称来自动检查运行时是否正确调用了API。这发生在称为&lt;em&gt;名称推断&lt;/em&gt;的过程中。更正式地说，名称推断包括以下两个步骤：</target>
        </trans-unit>
        <trans-unit id="bc702e382ad1ddf0605ed2f8380fd2e579529d82" translate="yes" xml:space="preserve">
          <source>Namely, joining processes sequentially implies they will terminate sequentially. If they don&amp;rsquo;t, and the first process does not terminate, the process termination will go unnoticed. Also, there are no native facilities for error propagation.</source>
          <target state="translated">即，加入过程按顺序表示它们将按顺序终止。如果它们没有这样做，并且第一个进程没有终止，则该进程终止将不会被注意到。而且，没有用于错误传播的本机工具。</target>
        </trans-unit>
        <trans-unit id="ce14504f9d8c7c9312b2871ff6009782be1ad451" translate="yes" xml:space="preserve">
          <source>Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions, arXiv:0909.4061 [math.NA; math.PR], 2009 (available at &lt;a href=&quot;https://arxiv.org/abs/0909.4061&quot;&gt;arXiv&lt;/a&gt;).</source>
          <target state="translated">内森&amp;middot;哈尔科（Nathan Halko），佩尔&amp;middot;古纳尔&amp;middot;马丁森（Per-Gunnar Martinsson）和乔尔&amp;middot;特罗普（Joel Tropp），《寻找具有随机性的结构：构建近似矩阵分解的概率算法》，arXiv：0909.4061 [math.NA; math.PR]，2009年（可从&lt;a href=&quot;https://arxiv.org/abs/0909.4061&quot;&gt;arXiv获得&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="1589b41367a72480492d7a5904aab37522c48d37" translate="yes" xml:space="preserve">
          <source>Negative log likelihood loss with Poisson distribution of target.</source>
          <target state="translated">负对数似然损失与目标的泊松分布。</target>
        </trans-unit>
        <trans-unit id="e2c678e1dc9e8fcb17d534c7469483a491efdcab" translate="yes" xml:space="preserve">
          <source>NegativeBinomial</source>
          <target state="translated">NegativeBinomial</target>
        </trans-unit>
        <trans-unit id="b7f2cb3940a28574771032f71b83c2599f143244" translate="yes" xml:space="preserve">
          <source>Neither &lt;code&gt;sampler&lt;/code&gt; nor &lt;code&gt;batch_sampler&lt;/code&gt; is compatible with iterable-style datasets, since such datasets have no notion of a key or an index.</source>
          <target state="translated">&lt;code&gt;sampler&lt;/code&gt; 和 &lt;code&gt;batch_sampler&lt;/code&gt; 都不与可迭代样式的数据集兼容，因为此类数据集没有键或索引的概念。</target>
        </trans-unit>
        <trans-unit id="9821f42c2d297ad4796746e3fa14e55cb3e99e22" translate="yes" xml:space="preserve">
          <source>Nesterov momentum is based on the formula from &lt;a href=&quot;http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf&quot;&gt;On the importance of initialization and momentum in deep learning&lt;/a&gt;.</source>
          <target state="translated">Nesterov动量基于&amp;ldquo;&lt;a href=&quot;http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf&quot;&gt;深度学习中初始化和动量的重要性&amp;rdquo;中&lt;/a&gt;的公式。</target>
        </trans-unit>
        <trans-unit id="53ebc572b4a44802ba114729f07bdaaf5409a9d7" translate="yes" xml:space="preserve">
          <source>Network</source>
          <target state="translated">Network</target>
        </trans-unit>
        <trans-unit id="813b5b686d397a3b63ce050a22e4bcaa735af1e8" translate="yes" xml:space="preserve">
          <source>New API:</source>
          <target state="translated">新的API。</target>
        </trans-unit>
        <trans-unit id="cc7e29ed592d2dbd4407e7babc4a099e18fb72ad" translate="yes" xml:space="preserve">
          <source>New distribution instance with batch dimensions expanded to &lt;code&gt;batch_size&lt;/code&gt;.</source>
          <target state="translated">具有批次尺寸的新分发实例已扩展为 &lt;code&gt;batch_size&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="540f7186c4d530bac579f69bb157fd5e121529cc" translate="yes" xml:space="preserve">
          <source>NewType</source>
          <target state="translated">NewType</target>
        </trans-unit>
        <trans-unit id="0013bf26fef8048809165bc5fc20da2af938e96c" translate="yes" xml:space="preserve">
          <source>No expressions except method definitions are allowed in the body of the class.</source>
          <target state="translated">除了方法定义外,类的主体中不允许有任何表达式。</target>
        </trans-unit>
        <trans-unit id="7a05537029cdbc40cf12ae1af3fd5798f2e0fb52" translate="yes" xml:space="preserve">
          <source>No support for inheritance or any other polymorphism strategy, except for inheriting from &lt;code&gt;object&lt;/code&gt; to specify a new-style class.</source>
          <target state="translated">不支持继承或任何其他多态策略，除了从 &lt;code&gt;object&lt;/code&gt; 继承以指定新样式的类外。</target>
        </trans-unit>
        <trans-unit id="16585665bdbf85c2a46ca04dcdf6f3b3c87a4c04" translate="yes" xml:space="preserve">
          <source>No, but the exporter will try to handle that part. Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars. However for cases that it failed to do so, you will need to manually provide the datatype information. This often happens with scripted models, where the datatypes are not recorded. We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future.</source>
          <target state="translated">没有,但出口商会尝试处理这部分。在ONNX中,标量被转换为常数。输出器会尝试为标量找出正确的数据类型。但是,如果它没有这样做,您需要手动提供数据类型信息。这种情况经常发生在没有记录数据类型的脚本模型中。我们正在努力改进导出器中的数据类型传播,以便将来不需要手动更改。</target>
        </trans-unit>
        <trans-unit id="3a122cf075b0ba371ed052ceb0f012b712c9d692" translate="yes" xml:space="preserve">
          <source>Node 1: &lt;em&gt;(IP: 192.168.1.1, and has a free port: 1234)&lt;/em&gt;</source>
          <target state="translated">节点1：&lt;em&gt;（IP：192.168.1.1，并具有可用端口：1234）&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="d7c20c7d93b71387bce45a0f532f94aceec659b1" translate="yes" xml:space="preserve">
          <source>Node 2:</source>
          <target state="translated">节点2:</target>
        </trans-unit>
        <trans-unit id="2200871a1ac28c0b5dd797edb44ce5069b3ba408" translate="yes" xml:space="preserve">
          <source>Nominal typing is in development, but structural typing is not</source>
          <target state="translated">名义上的类型化正在发展中,但结构上的类型化还没有发展起来</target>
        </trans-unit>
        <trans-unit id="56c04398e9678198426e682b1f9d96427b74b23b" translate="yes" xml:space="preserve">
          <source>Nominal vs structural subtyping</source>
          <target state="translated">名义与结构分型</target>
        </trans-unit>
        <trans-unit id="b89eadfaef5d8d82b020885869aa96878170cfaa" translate="yes" xml:space="preserve">
          <source>Non-ATen operators</source>
          <target state="translated">非ATen运营商</target>
        </trans-unit>
        <trans-unit id="05ac3d4369dff83cd256460b1c7ab61b3d6873b9" translate="yes" xml:space="preserve">
          <source>Non-linear Activations (other)</source>
          <target state="translated">非线性活动(其他)</target>
        </trans-unit>
        <trans-unit id="37e8f09f37b4ba586b29c8d43264a706f392c534" translate="yes" xml:space="preserve">
          <source>Non-linear Activations (weighted sum, nonlinearity)</source>
          <target state="translated">非线性活动(加权和、非线性)。</target>
        </trans-unit>
        <trans-unit id="90f4c5f6b363fba151454b4dc3811eace77006ea" translate="yes" xml:space="preserve">
          <source>Non-linear activation functions</source>
          <target state="translated">非线性激活函数</target>
        </trans-unit>
        <trans-unit id="32cfd0f165a52f907e24f0d0d41b54665a307df6" translate="yes" xml:space="preserve">
          <source>Non-local variables are resolved to Python values at compile time when the function is defined. These values are then converted into TorchScript values using the rules described in &lt;a href=&quot;#use-of-python-values&quot;&gt;Use of Python Values&lt;/a&gt;.</source>
          <target state="translated">定义函数时，非局部变量会在编译时解析为Python值。然后，&lt;a href=&quot;#use-of-python-values&quot;&gt;使用&amp;ldquo;使用Python值&amp;rdquo;中&lt;/a&gt;所述的规则将这些值转换为TorchScript值。</target>
        </trans-unit>
        <trans-unit id="6eef6648406c333a4035cd5e60d0bf2ecf2606d7" translate="yes" xml:space="preserve">
          <source>None</source>
          <target state="translated">None</target>
        </trans-unit>
        <trans-unit id="8acafd3007c1ab1f15fdb73fa725fb8e794923ba" translate="yes" xml:space="preserve">
          <source>None if &lt;code&gt;join&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, &lt;code&gt;ProcessContext&lt;/code&gt; if &lt;code&gt;join&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">如果 &lt;code&gt;join&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; 则为None ，如果 &lt;code&gt;join&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; 则为 &lt;code&gt;ProcessContext&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="ab71bc2480c32bc39f483a12c34cd4676886028d" translate="yes" xml:space="preserve">
          <source>None, module is modified inplace with added observer modules and forward_hooks</source>
          <target state="translated">无,模块在原地修改,增加了观察者模块和forward_hooks。</target>
        </trans-unit>
        <trans-unit id="e4fa32a75bd8c135dd060f289888f83fb70a2d47" translate="yes" xml:space="preserve">
          <source>None, module is modified inplace with qconfig attached</source>
          <target state="translated">无,模块在原地修改,附加qconfig。</target>
        </trans-unit>
        <trans-unit id="45e118d0563ea8581f830f46e85b60ae714faae4" translate="yes" xml:space="preserve">
          <source>Normal</source>
          <target state="translated">Normal</target>
        </trans-unit>
        <trans-unit id="46e38cc5e7e0b2b170857112460a732303fdb5e3" translate="yes" xml:space="preserve">
          <source>Normalization Layers</source>
          <target state="translated">归一化层</target>
        </trans-unit>
        <trans-unit id="746e3a4c0d98d2df15064221df23fa7793e56038" translate="yes" xml:space="preserve">
          <source>Normalization functions</source>
          <target state="translated">归一化功能</target>
        </trans-unit>
        <trans-unit id="c4639d9330cdd597089734b95310026444f4de7e" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the backward transform (&lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">归一化模式。对于后向变换（&lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt;），它们对应于：</target>
        </trans-unit>
        <trans-unit id="fae17255f7a6267cdf6f973a75edad225dcf4d82" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the backward transform (&lt;a href=&quot;#torch.fft.ifftn&quot;&gt;&lt;code&gt;ifftn()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">归一化模式。对于后向变换（&lt;a href=&quot;#torch.fft.ifftn&quot;&gt; &lt;code&gt;ifftn()&lt;/code&gt; &lt;/a&gt;），它们对应于：</target>
        </trans-unit>
        <trans-unit id="37141e0de089638ade05f2fc80c5c6e1a4b3e953" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the backward transform (&lt;a href=&quot;#torch.fft.ihfft&quot;&gt;&lt;code&gt;ihfft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">归一化模式。对于后向变换（&lt;a href=&quot;#torch.fft.ihfft&quot;&gt; &lt;code&gt;ihfft()&lt;/code&gt; &lt;/a&gt;），它们对应于：</target>
        </trans-unit>
        <trans-unit id="d960941bb7a853f66a4f14f14d7f53d0f7022466" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the backward transform (&lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">归一化模式。对于后向变换（&lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt;），它们对应于：</target>
        </trans-unit>
        <trans-unit id="7b81cd890e0116574dbf7776b7567225bd21e39c" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the backward transform (&lt;a href=&quot;#torch.fft.irfftn&quot;&gt;&lt;code&gt;irfftn()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">归一化模式。对于后向变换（&lt;a href=&quot;#torch.fft.irfftn&quot;&gt; &lt;code&gt;irfftn()&lt;/code&gt; &lt;/a&gt;），它们对应于：</target>
        </trans-unit>
        <trans-unit id="e18f2dcd89118b40d295cc2d0f570dae9ee63571" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the forward transform (&lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">归一化模式。对于前向变换（&lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt;），它们对应于：</target>
        </trans-unit>
        <trans-unit id="20885ecc112a0654f2fd1749899c2aade057f18f" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the forward transform (&lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">归一化模式。对于前向变换（&lt;a href=&quot;#torch.fft.fftn&quot;&gt; &lt;code&gt;fftn()&lt;/code&gt; &lt;/a&gt;），它们对应于：</target>
        </trans-unit>
        <trans-unit id="36f2290f1751dd4fd40961edc0b9d58c6d126bef" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the forward transform (&lt;a href=&quot;#torch.fft.hfft&quot;&gt;&lt;code&gt;hfft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">归一化模式。对于正向变换（&lt;a href=&quot;#torch.fft.hfft&quot;&gt; &lt;code&gt;hfft()&lt;/code&gt; &lt;/a&gt;），它们对应于：</target>
        </trans-unit>
        <trans-unit id="f07c249cb9e5f2751b249e4c52c89f4d38befc44" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the forward transform (&lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">归一化模式。对于正向变换（&lt;a href=&quot;#torch.fft.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt;），它们对应于：</target>
        </trans-unit>
        <trans-unit id="d3864acfcc3775dda73f1b742d452bc185e4b45c" translate="yes" xml:space="preserve">
          <source>Normalization mode. For the forward transform (&lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt;), these correspond to:</source>
          <target state="translated">归一化模式。对于前向变换（&lt;a href=&quot;#torch.fft.rfftn&quot;&gt; &lt;code&gt;rfftn()&lt;/code&gt; &lt;/a&gt;），它们对应于：</target>
        </trans-unit>
        <trans-unit id="290617bd60b4381fb793ad7f08e51d3f4c292b8a" translate="yes" xml:space="preserve">
          <source>Normally, the only way users interact with functions is by creating subclasses and defining new operations. This is a recommended way of extending torch.autograd.</source>
          <target state="translated">通常,用户与函数交互的唯一方式是创建子类并定义新的操作。这是扩展 torch.autograd 的推荐方式。</target>
        </trans-unit>
        <trans-unit id="ac15bca5bbea06cd32f056c8cbf634e4f1faea41" translate="yes" xml:space="preserve">
          <source>Not implemented</source>
          <target state="translated">未执行</target>
        </trans-unit>
        <trans-unit id="df8f8315e0c299c571aa4226e66410f1097a424b" translate="yes" xml:space="preserve">
          <source>Not providing a value for &lt;code&gt;steps&lt;/code&gt; is deprecated. For backwards compatibility, not providing a value for &lt;code&gt;steps&lt;/code&gt; will create a tensor with 100 elements. Note that this behavior is not reflected in the documented function signature and should not be relied on. In a future PyTorch release, failing to provide a value for &lt;code&gt;steps&lt;/code&gt; will throw a runtime error.</source>
          <target state="translated">不建议提供 &lt;code&gt;steps&lt;/code&gt; 值。为了向后兼容，不为 &lt;code&gt;steps&lt;/code&gt; 提供值将创建具有100个元素的张量。请注意，此行为未反映在已记录的函数签名中，因此不应依赖。在未来的PyTorch版本中，如果无法提供 &lt;code&gt;steps&lt;/code&gt; 值，则会引发运行时错误。</target>
        </trans-unit>
        <trans-unit id="2c924e3088204ee77ba681f72be3444357932fca" translate="yes" xml:space="preserve">
          <source>Note</source>
          <target state="translated">Note</target>
        </trans-unit>
        <trans-unit id="a73e4c5274c790b0c43ddacd3a38f0f1878acefd" translate="yes" xml:space="preserve">
          <source>Note also that the total number of steps in the cycle can be determined in one of two ways (listed in order of precedence):</source>
          <target state="translated">还需要注意的是,循环中的总步数可以通过两种方式之一来确定(按先后顺序排列)。</target>
        </trans-unit>
        <trans-unit id="da7038b6159ab37164aeb7442907c881108aff7e" translate="yes" xml:space="preserve">
          <source>Note that</source>
          <target state="translated">请注意</target>
        </trans-unit>
        <trans-unit id="5433b2e634d55f555a9239d7a13b88b55129e9b2" translate="yes" xml:space="preserve">
          <source>Note that &lt;a href=&quot;#torch.nn.ModuleDict.update&quot;&gt;&lt;code&gt;update()&lt;/code&gt;&lt;/a&gt; with other unordered mapping types (e.g., Python&amp;rsquo;s plain &lt;code&gt;dict&lt;/code&gt; before Python version 3.6) does not preserve the order of the merged mapping.</source>
          <target state="translated">请注意，具有其他无序映射类型（例如，Python 3.6版之前的Python的普通 &lt;code&gt;dict&lt;/code&gt; &lt;a href=&quot;#torch.nn.ModuleDict.update&quot;&gt; &lt;code&gt;update()&lt;/code&gt; &lt;/a&gt;不会保留合并映射的顺序。</target>
        </trans-unit>
        <trans-unit id="a1dc440ad60db846cad7f7d0a7ce1260c129ea8c" translate="yes" xml:space="preserve">
          <source>Note that &lt;a href=&quot;#torch.nn.ParameterDict.update&quot;&gt;&lt;code&gt;update()&lt;/code&gt;&lt;/a&gt; with other unordered mapping types (e.g., Python&amp;rsquo;s plain &lt;code&gt;dict&lt;/code&gt;) does not preserve the order of the merged mapping.</source>
          <target state="translated">请注意，具有其他无序映射类型（例如，Python的plain &lt;code&gt;dict&lt;/code&gt; ）的&lt;a href=&quot;#torch.nn.ParameterDict.update&quot;&gt; &lt;code&gt;update()&lt;/code&gt; &lt;/a&gt;不会保留合并映射的顺序。</target>
        </trans-unit>
        <trans-unit id="42c994d9673f83cc70c78e02c5d9a230c6566874" translate="yes" xml:space="preserve">
          <source>Note that &lt;code&gt;*args&lt;/code&gt; and &lt;code&gt;**kwargs&lt;/code&gt; in &lt;a href=&quot;#torch.hub.load&quot;&gt;&lt;code&gt;torch.hub.load()&lt;/code&gt;&lt;/a&gt; are used to &lt;strong&gt;instantiate&lt;/strong&gt; a model. After you have loaded a model, how can you find out what you can do with the model? A suggested workflow is</source>
          <target state="translated">需要注意的是 &lt;code&gt;*args&lt;/code&gt; 和 &lt;code&gt;**kwargs&lt;/code&gt; 在&lt;a href=&quot;#torch.hub.load&quot;&gt; &lt;code&gt;torch.hub.load()&lt;/code&gt; &lt;/a&gt;来&lt;strong&gt;实例化&lt;/strong&gt;一个模型。加载模型后，如何找出可以使用该模型的功能？建议的工作流程是</target>
        </trans-unit>
        <trans-unit id="2d1e0d8414eaec791533b2f0922dd4ade580351c" translate="yes" xml:space="preserve">
          <source>Note that &lt;code&gt;BAND&lt;/code&gt;, &lt;code&gt;BOR&lt;/code&gt;, and &lt;code&gt;BXOR&lt;/code&gt; reductions are not available when using the &lt;code&gt;NCCL&lt;/code&gt; backend.</source>
          <target state="translated">请注意，使用 &lt;code&gt;NCCL&lt;/code&gt; 后端时 &lt;code&gt;BAND&lt;/code&gt; ， &lt;code&gt;BOR&lt;/code&gt; 和 &lt;code&gt;BXOR&lt;/code&gt; 减少不可用。</target>
        </trans-unit>
        <trans-unit id="0bd438191e4b3887c615246e19e8361b84bd83a7" translate="yes" xml:space="preserve">
          <source>Note that &lt;code&gt;T[1] == T[-1].conj()&lt;/code&gt; and &lt;code&gt;T[2] == T[-2].conj()&lt;/code&gt; is redundant. We can thus compute the forward transform without considering negative frequencies:</source>
          <target state="translated">注意， &lt;code&gt;T[1] == T[-1].conj()&lt;/code&gt; 和 &lt;code&gt;T[2] == T[-2].conj()&lt;/code&gt; 是多余的。因此，我们可以在不考虑负频率的情况下计算正向变换：</target>
        </trans-unit>
        <trans-unit id="ee47ccc0fe1d0f9aca7dffbff0fc6bd81769f2e4" translate="yes" xml:space="preserve">
          <source>Note that &lt;code&gt;len(output_tensor_list)&lt;/code&gt; needs to be the same for all the distributed processes calling this function.</source>
          <target state="translated">请注意，对于调用此函数的所有分布式进程， &lt;code&gt;len(output_tensor_list)&lt;/code&gt; 必须相同。</target>
        </trans-unit>
        <trans-unit id="de81d60cb1e06955cef77dc7af826512edffc0f1" translate="yes" xml:space="preserve">
          <source>Note that &lt;code&gt;total_count&lt;/code&gt; need not be specified if only &lt;a href=&quot;#torch.distributions.multinomial.Multinomial.log_prob&quot;&gt;&lt;code&gt;log_prob()&lt;/code&gt;&lt;/a&gt; is called (see example below)</source>
          <target state="translated">请注意，如果仅&lt;a href=&quot;#torch.distributions.multinomial.Multinomial.log_prob&quot;&gt; &lt;code&gt;log_prob()&lt;/code&gt; &lt;/a&gt;则无需指定 &lt;code&gt;total_count&lt;/code&gt; （请参见下面的示例）</target>
        </trans-unit>
        <trans-unit id="7ad0240a1b6c1c4077cd6f27931fdf77ffb68b6b" translate="yes" xml:space="preserve">
          <source>Note that QConfig needs to contain observer &lt;strong&gt;classes&lt;/strong&gt; (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization preparation function will instantiate observers multiple times for each of the layers.</source>
          <target state="translated">请注意，QConfig需要包含观察者&lt;strong&gt;类&lt;/strong&gt;（如MinMaxObserver）或一个可调用的可调用对象，该类可在调用时返回实例，而不是具体的观察者实例本身。量化准备功能将为每个层多次实例化观察者。</target>
        </trans-unit>
        <trans-unit id="1ccf01e0451a8d6abc36c37e011e58952b42e20e" translate="yes" xml:space="preserve">
          <source>Note that QConfigDynamic needs to contain observer &lt;strong&gt;classes&lt;/strong&gt; (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization function will instantiate observers multiple times for each of the layers.</source>
          <target state="translated">请注意，QConfigDynamic需要包含观察者&lt;strong&gt;类&lt;/strong&gt;（例如MinMaxObserver）或一个可调用的可调用对象，该类可在调用时返回实例，而不是具体的观察者实例本身。量化功能将为每个层多次实例化观察者。</target>
        </trans-unit>
        <trans-unit id="d9706617db521f3c057495c1501777340ff2c154" translate="yes" xml:space="preserve">
          <source>Note that automatic rank assignment is not supported anymore in the latest distributed package and &lt;code&gt;group_name&lt;/code&gt; is deprecated as well.</source>
          <target state="translated">请注意，最新的分布式软件包不再支持自动等级分配，并且不建议使用 &lt;code&gt;group_name&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d90621e782b71803aaf8f760cde86adc6ccf606c" translate="yes" xml:space="preserve">
          <source>Note that deterministic operations tend to have worse performance than non-deterministic operations.</source>
          <target state="translated">请注意,确定性操作的性能往往比非确定性操作差。</target>
        </trans-unit>
        <trans-unit id="b7948c066b35cd06338c8b76a0036f57024ca522" translate="yes" xml:space="preserve">
          <source>Note that each element of &lt;code&gt;input_tensor_lists&lt;/code&gt; has the size of &lt;code&gt;world_size * len(output_tensor_list)&lt;/code&gt;, since the function scatters the result from every single GPU in the group. To interpret each element of &lt;code&gt;input_tensor_lists[i]&lt;/code&gt;, note that &lt;code&gt;output_tensor_list[j]&lt;/code&gt; of rank k receives the reduce-scattered result from &lt;code&gt;input_tensor_lists[i][k * world_size + j]&lt;/code&gt;</source>
          <target state="translated">请注意， &lt;code&gt;input_tensor_lists&lt;/code&gt; 的每个元素的大小为 &lt;code&gt;world_size * len(output_tensor_list)&lt;/code&gt; ，因为该函数分散了组中每个GPU的结果。要解释 &lt;code&gt;input_tensor_lists[i]&lt;/code&gt; 每个元素，请注意，排名k的 &lt;code&gt;output_tensor_list[j]&lt;/code&gt; 从 &lt;code&gt;input_tensor_lists[i][k * world_size + j]&lt;/code&gt; 于散乱的结果</target>
        </trans-unit>
        <trans-unit id="f1fa921e9c60fd6eb9bf5d36ace2910b625e1764" translate="yes" xml:space="preserve">
          <source>Note that each element of &lt;code&gt;output_tensor_lists&lt;/code&gt; has the size of &lt;code&gt;world_size * len(input_tensor_list)&lt;/code&gt;, since the function all gathers the result from every single GPU in the group. To interpret each element of &lt;code&gt;output_tensor_lists[i]&lt;/code&gt;, note that &lt;code&gt;input_tensor_list[j]&lt;/code&gt; of rank k will be appear in &lt;code&gt;output_tensor_lists[i][k * world_size + j]&lt;/code&gt;</source>
          <target state="translated">请注意， &lt;code&gt;output_tensor_lists&lt;/code&gt; 的每个元素的大小为 &lt;code&gt;world_size * len(input_tensor_list)&lt;/code&gt; ，因为该函数都从组中的每个GPU收集结果。要解释 &lt;code&gt;output_tensor_lists[i]&lt;/code&gt; 每个元素，请注意，等级k的 &lt;code&gt;input_tensor_list[j]&lt;/code&gt; 将出现在 &lt;code&gt;output_tensor_lists[i][k * world_size + j]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a779c7a4b4446c6b4a026c9792070b87e0802a90" translate="yes" xml:space="preserve">
          <source>Note that if there will be a lot of tensors shared, this strategy will keep a large number of file descriptors open most of the time. If your system has low limits for the number of open file descriptors, and you can&amp;rsquo;t raise them, you should use the &lt;code&gt;file_system&lt;/code&gt; strategy.</source>
          <target state="translated">请注意，如果将共享许多张量，则此策略将在大多数时间保持打开大量文件描述符。如果您的系统对打开的文件描述符的数量有较低的限制，并且您不能提高它们的限制，则应使用 &lt;code&gt;file_system&lt;/code&gt; 策略。</target>
        </trans-unit>
        <trans-unit id="261fa08253a2249def1244a03fcd9b8841a48f8d" translate="yes" xml:space="preserve">
          <source>Note that multicast address is not supported anymore in the latest distributed package. &lt;code&gt;group_name&lt;/code&gt; is deprecated as well.</source>
          <target state="translated">请注意，最新的分布式程序包中不再支持多播地址。同样不推荐使用 &lt;code&gt;group_name&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b2830ad219b13cbec38ff7f3f532533bf250586f" translate="yes" xml:space="preserve">
          <source>Note that non-integer &lt;code&gt;step&lt;/code&gt; is subject to floating point rounding errors when comparing against &lt;code&gt;end&lt;/code&gt;; to avoid inconsistency, we advise adding a small epsilon to &lt;code&gt;end&lt;/code&gt; in such cases.</source>
          <target state="translated">请注意，与 &lt;code&gt;end&lt;/code&gt; 进行比较时，非整数 &lt;code&gt;step&lt;/code&gt; 易受浮点舍入误差的影响。为了避免矛盾，我们建议增加一个小小量到 &lt;code&gt;end&lt;/code&gt; 在这种情况下。</target>
        </trans-unit>
        <trans-unit id="112ac3f4ac80a498de80217376157fc61531fc0a" translate="yes" xml:space="preserve">
          <source>Note that one should use &lt;code&gt;cache_size=1&lt;/code&gt; when it comes to &lt;code&gt;NaN/Inf&lt;/code&gt; values.</source>
          <target state="translated">请注意，对于 &lt;code&gt;NaN/Inf&lt;/code&gt; 值，应使用 &lt;code&gt;cache_size=1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0bc75f2496d9d7af4d1e5ae3151f63a061a03230" translate="yes" xml:space="preserve">
          <source>Note that the &lt;code&gt;.event_shape&lt;/code&gt; of a &lt;a href=&quot;#torch.distributions.transformed_distribution.TransformedDistribution&quot;&gt;&lt;code&gt;TransformedDistribution&lt;/code&gt;&lt;/a&gt; is the maximum shape of its base distribution and its transforms, since transforms can introduce correlations among events.</source>
          <target state="translated">注意， &lt;code&gt;.event_shape&lt;/code&gt; 一个的&lt;a href=&quot;#torch.distributions.transformed_distribution.TransformedDistribution&quot;&gt; &lt;code&gt;TransformedDistribution&lt;/code&gt; &lt;/a&gt;是其碱分布及其变换的最大形状，由于变换可以事件之间的相关性引入。</target>
        </trans-unit>
        <trans-unit id="fe417e45eca8b56b8a391ee5d65f646c54c00fc4" translate="yes" xml:space="preserve">
          <source>Note that the input to LongTensor is NOT a list of index tuples. If you want to write your indices this way, you should transpose before passing them to the sparse constructor:</source>
          <target state="translated">请注意,LongTensor的输入不是索引元组的列表。如果你想以这种方式写你的索引,你应该在将它们传递给稀疏构造函数之前进行转置。</target>
        </trans-unit>
        <trans-unit id="7e0f39780c8cac4e285c8ec3c405982d7cafc31f" translate="yes" xml:space="preserve">
          <source>Note that these cases may in fact be traceable in the future.</source>
          <target state="translated">需要注意的是,这些情况其实未来可能会有迹可循。</target>
        </trans-unit>
        <trans-unit id="59bdfd4be005baa7a143e9e7af0a324461ee7947" translate="yes" xml:space="preserve">
          <source>Note that this enumerates over all batched tensors in lock-step &lt;code&gt;[[0, 0], [1, 1], &amp;hellip;]&lt;/code&gt;. With &lt;code&gt;expand=False&lt;/code&gt;, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, &lt;code&gt;[[0], [1], ..&lt;/code&gt;.</source>
          <target state="translated">请注意，这枚举了所有以锁定步长 &lt;code&gt;[[0, 0], [1, 1], &amp;hellip;]&lt;/code&gt; 。与 &lt;code&gt;expand=False&lt;/code&gt; ，枚举沿着昏暗0发生，但与剩余的批次尺寸为单尺寸 &lt;code&gt;[[0], [1], ..&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="752392a2ff1e2a39cc0db5f6e4585276ad451ffb" translate="yes" xml:space="preserve">
          <source>Note that this function is simply doing &lt;code&gt;isinstance(obj, Tensor)&lt;/code&gt;. Using that &lt;code&gt;isinstance&lt;/code&gt; check is better for typechecking with mypy, and more explicit - so it&amp;rsquo;s recommended to use that instead of &lt;code&gt;is_tensor&lt;/code&gt;.</source>
          <target state="translated">请注意，此函数只是在执行 &lt;code&gt;isinstance(obj, Tensor)&lt;/code&gt; 。使用 &lt;code&gt;isinstance&lt;/code&gt; check比较适合与mypy进行类型检查，并且更显式-因此建议使用它代替 &lt;code&gt;is_tensor&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="a5b8b84abd5cbd1ff532fd7ae1e19baa9b3b1a28" translate="yes" xml:space="preserve">
          <source>Note that this function requires Python 3.4 or higher.</source>
          <target state="translated">注意这个函数需要Python 3.4或更高版本。</target>
        </trans-unit>
        <trans-unit id="e898adc76bed905448d3327e88134b4893e4bf98" translate="yes" xml:space="preserve">
          <source>Note that this requires the &lt;code&gt;matplotlib&lt;/code&gt; package.</source>
          <target state="translated">请注意，这需要使用 &lt;code&gt;matplotlib&lt;/code&gt; 软件包。</target>
        </trans-unit>
        <trans-unit id="ba4e51216b174df96efb807dbfef6adf290a07f2" translate="yes" xml:space="preserve">
          <source>Note that this requires the &lt;code&gt;moviepy&lt;/code&gt; package.</source>
          <target state="translated">请注意，这需要 &lt;code&gt;moviepy&lt;/code&gt; 软件包。</target>
        </trans-unit>
        <trans-unit id="da8c941a1c4dc21607ac70d896e4a7cb648a3554" translate="yes" xml:space="preserve">
          <source>Note that this requires the &lt;code&gt;pillow&lt;/code&gt; package.</source>
          <target state="translated">请注意，这需要 &lt;code&gt;pillow&lt;/code&gt; 包装。</target>
        </trans-unit>
        <trans-unit id="38c3af3645ee3d8c1e85f997b98d93ebdc11ac44" translate="yes" xml:space="preserve">
          <source>Note that when &lt;code&gt;tracker&lt;/code&gt; stores Tensor objects from the LOBPCG instance, it must make copies of these.</source>
          <target state="translated">请注意，当 &lt;code&gt;tracker&lt;/code&gt; 从LOBPCG实例存储Tensor对象时，它必须复制这些对象。</target>
        </trans-unit>
        <trans-unit id="76d32111f7bcbce3e8104b2ef5376761705e1942" translate="yes" xml:space="preserve">
          <source>Note: A full example to apply nn.Transformer module for the word language model is available in &lt;a href=&quot;https://github.com/pytorch/examples/tree/master/word_language_model&quot;&gt;https://github.com/pytorch/examples/tree/master/word_language_model&lt;/a&gt;</source>
          <target state="translated">注：完整的示例应用nn.Transformer模块的单词的语言模型提供&lt;a href=&quot;https://github.com/pytorch/examples/tree/master/word_language_model&quot;&gt;https://github.com/pytorch/examples/tree/master/word_language_model&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2acb81c5ebbb77d4e03260d44dbd15da614d8dba" translate="yes" xml:space="preserve">
          <source>Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode.</source>
          <target state="translated">注:由于变换器模型中的多头注意架构,变换器的输出序列长度与解码器的输入序列(即目标)长度相同。</target>
        </trans-unit>
        <trans-unit id="ea3f023c9638ee2f77985080cfee0c3149276897" translate="yes" xml:space="preserve">
          <source>Note: Loading a model is the typical use case, but this can also be used to for loading other objects such as tokenizers, loss functions, etc.</source>
          <target state="translated">注意:加载模型是典型的使用案例,但这也可以用于加载其他对象,如标记器、损失函数等。</target>
        </trans-unit>
        <trans-unit id="fec631db96f94a854d531dcf201c32e0e5b12fec" translate="yes" xml:space="preserve">
          <source>Note: When beta is set to 0, this is equivalent to &lt;a href=&quot;torch.nn.l1loss#torch.nn.L1Loss&quot;&gt;&lt;code&gt;L1Loss&lt;/code&gt;&lt;/a&gt;. Passing a negative value in for beta will result in an exception.</source>
          <target state="translated">注意：当beta设置为0时，这等效于&lt;a href=&quot;torch.nn.l1loss#torch.nn.L1Loss&quot;&gt; &lt;code&gt;L1Loss&lt;/code&gt; &lt;/a&gt;。为beta传递负值将导致异常。</target>
        </trans-unit>
        <trans-unit id="9b7c2d4ed1b493db790041907421b97fbf38eae9" translate="yes" xml:space="preserve">
          <source>Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with &lt;code&gt;True&lt;/code&gt; are not allowed to attend while &lt;code&gt;False&lt;/code&gt; values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of &lt;code&gt;True&lt;/code&gt; will be ignored while the position with the value of &lt;code&gt;False&lt;/code&gt; will be unchanged.</source>
          <target state="translated">注意：[src / tgt / memory] ​​_mask确保允许位置i进入未屏蔽的位置。如果提供了ByteTensor，则不允许零位参加，而零位将保持不变。如果提供了BoolTensor，则不允许参加 &lt;code&gt;True&lt;/code&gt; 职位，而 &lt;code&gt;False&lt;/code&gt; 值将保持不变。如果提供了FloatTensor，它将被添加到注意权重中。[src / tgt / memory] ​​_key_padding_mask在键中提供了要忽略的指定元素。如果提供了ByteTensor，则将忽略非零位置，而零位置将保持不变。如果提供了BoolTensor，则值为 &lt;code&gt;True&lt;/code&gt; 的位置将被忽略，而值为 &lt;code&gt;False&lt;/code&gt; 的位置将被忽略 将保持不变。</target>
        </trans-unit>
        <trans-unit id="70440046a3dc2e079f23ee1c57dfa76669b732aa" translate="yes" xml:space="preserve">
          <source>Notes</source>
          <target state="translated">Notes</target>
        </trans-unit>
        <trans-unit id="4425723195c4a3b16b476f69800fa3cf16abb9b4" translate="yes" xml:space="preserve">
          <source>Notice that if</source>
          <target state="translated">注意,如果</target>
        </trans-unit>
        <trans-unit id="d1656ebe55b849d2893f4a0cf92db35aea6025bc" translate="yes" xml:space="preserve">
          <source>Notice that operators can also have associated &lt;code&gt;blocks&lt;/code&gt;, namely the &lt;code&gt;prim::Loop&lt;/code&gt; and &lt;code&gt;prim::If&lt;/code&gt; operators. In the graph print-out, these operators are formatted to reflect their equivalent source code forms to facilitate easy debugging.</source>
          <target state="translated">请注意，运算符还可以具有关联的 &lt;code&gt;blocks&lt;/code&gt; ，即 &lt;code&gt;prim::Loop&lt;/code&gt; 和 &lt;code&gt;prim::If&lt;/code&gt; 运算符。在图形打印输出中，这些运算符被格式化以反映其等效的源代码形式，从而便于调试。</target>
        </trans-unit>
        <trans-unit id="04a5b9046bd473cb428ae07e5d6a160de265bc5d" translate="yes" xml:space="preserve">
          <source>Notice that the symmetric element &lt;code&gt;T[-1] == T[1].conj()&lt;/code&gt; is omitted. At the Nyquist frequency &lt;code&gt;T[-2] == T[2]&lt;/code&gt; is it&amp;rsquo;s own symmetric pair, and therefore must always be real-valued.</source>
          <target state="translated">请注意，省略了对称元素 &lt;code&gt;T[-1] == T[1].conj()&lt;/code&gt; 。在奈奎斯特频率 &lt;code&gt;T[-2] == T[2]&lt;/code&gt; 是它自己的对称对，因此必须始终为实值。</target>
        </trans-unit>
        <trans-unit id="1956334a42d3e178212344356997b21e347fa273" translate="yes" xml:space="preserve">
          <source>Now PyTorch is able to export &lt;code&gt;elu&lt;/code&gt; operator.</source>
          <target state="translated">现在，PyTorch能够导出 &lt;code&gt;elu&lt;/code&gt; 运算符。</target>
        </trans-unit>
        <trans-unit id="a30a0ffebbaef4efa26c2a00eda09cf59167dfe6" translate="yes" xml:space="preserve">
          <source>Now the exported ONNX graph becomes:</source>
          <target state="translated">现在导出的ONNX图变成了。</target>
        </trans-unit>
        <trans-unit id="27759972b02a15b63bf671d015c9407939f14fb3" translate="yes" xml:space="preserve">
          <source>Numerical gradient checking</source>
          <target state="translated">数值梯度检查</target>
        </trans-unit>
        <trans-unit id="08a914cde05039694ef0194d9ee79ff9a79dde33" translate="yes" xml:space="preserve">
          <source>O</source>
          <target state="translated">O</target>
        </trans-unit>
        <trans-unit id="58e9484683a456865672c8a4517b530e0f41cd11" translate="yes" xml:space="preserve">
          <source>ONLY INDICES:</source>
          <target state="translated">只有指标。</target>
        </trans-unit>
        <trans-unit id="b335243efa572b5f0beb01acb45b42a02be187b5" translate="yes" xml:space="preserve">
          <source>ONNX</source>
          <target state="translated">ONNX</target>
        </trans-unit>
        <trans-unit id="98099fd5d344320bb061ab40d4f82b1271288a2a" translate="yes" xml:space="preserve">
          <source>ONNX_ATEN</source>
          <target state="translated">ONNX_ATEN</target>
        </trans-unit>
        <trans-unit id="7fd76316b691135ff7ad4e41541a37a2b2ed8c3d" translate="yes" xml:space="preserve">
          <source>ONNX_ATEN_FALLBACK</source>
          <target state="translated">ONNX_ATEN_FALLBACK</target>
        </trans-unit>
        <trans-unit id="0976054bd35910dc334e574eb3e3c16bf4dc9b17" translate="yes" xml:space="preserve">
          <source>ONNX_FALLTHROUGH</source>
          <target state="translated">ONNX_FALLTHROUGH</target>
        </trans-unit>
        <trans-unit id="946938795b376f71c3a327dfe4ec5463d0103fa6" translate="yes" xml:space="preserve">
          <source>Object Detection, Instance Segmentation and Person Keypoint Detection</source>
          <target state="translated">对象检测、实例分割和人物关键点检测</target>
        </trans-unit>
        <trans-unit id="56b40c7447fb6cdfcf37a82cf91030f8b0adc145" translate="yes" xml:space="preserve">
          <source>Observer classes have usually reasonable default arguments, but they can be overwritten with &lt;code&gt;with_args&lt;/code&gt; method (that behaves like functools.partial):</source>
          <target state="translated">观察者类通常具有合理的默认参数，但是可以使用 &lt;code&gt;with_args&lt;/code&gt; 方法覆盖它们（其行为类似于functools.partial）：</target>
        </trans-unit>
        <trans-unit id="05df80fc8f6cd937ee6fca4db89715da3aeb05ee" translate="yes" xml:space="preserve">
          <source>Observer module for computing the quantization parameters based on the moving average of the min and max values.</source>
          <target state="translated">观察员模块,用于根据最小值和最大值的移动平均值计算量化参数。</target>
        </trans-unit>
        <trans-unit id="0e6416b8d542683480294d9aa23272830844da59" translate="yes" xml:space="preserve">
          <source>Observer module for computing the quantization parameters based on the running min and max values.</source>
          <target state="translated">观察者模块,用于根据运行的最小值和最大值计算量化参数。</target>
        </trans-unit>
        <trans-unit id="06100f7170e7f76076139e20082a1eb3afab8d09" translate="yes" xml:space="preserve">
          <source>Observer module for computing the quantization parameters based on the running per channel min and max values.</source>
          <target state="translated">观察者模块,用于计算基于每通道最小值和最大值的量化参数。</target>
        </trans-unit>
        <trans-unit id="77e203a6c2d75205e18b618179c387b4e62870c5" translate="yes" xml:space="preserve">
          <source>Observer that doesn&amp;rsquo;t do anything and just passes its configuration to the quantized module&amp;rsquo;s &lt;code&gt;.from_float()&lt;/code&gt;.</source>
          <target state="translated">观察者什么也不做，只是将其配置传递给量化模块的 &lt;code&gt;.from_float()&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0618a214d16e39be75c6315a11d086a7dcbea1b6" translate="yes" xml:space="preserve">
          <source>Observers</source>
          <target state="translated">Observers</target>
        </trans-unit>
        <trans-unit id="8bf3249a3a8cece6d4343af20c9431018c926b24" translate="yes" xml:space="preserve">
          <source>Obtaining log-probabilities in a neural network is easily achieved by adding a &lt;code&gt;LogSoftmax&lt;/code&gt; layer in the last layer of your network. You may use &lt;code&gt;CrossEntropyLoss&lt;/code&gt; instead, if you prefer not to add an extra layer.</source>
          <target state="translated">通过在网络的最后一层添加 &lt;code&gt;LogSoftmax&lt;/code&gt; 层，可以轻松获得神经网络中的对数概率。如果您不希望添加额外的图层，则可以改用 &lt;code&gt;CrossEntropyLoss&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b98c7204267732ffe5bc04ce20cbadbe8642e52b" translate="yes" xml:space="preserve">
          <source>Of course the reality is much more complicated and your script might not be in one of those two extremes depending on the part of the model you&amp;rsquo;re evaluating. If the profiler outputs don&amp;rsquo;t help, you could try looking at the result of &lt;a href=&quot;autograd#torch.autograd.profiler.emit_nvtx&quot;&gt;&lt;code&gt;torch.autograd.profiler.emit_nvtx()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;nvprof&lt;/code&gt;. However, please take into account that the NVTX overhead is very high and often gives a heavily skewed timeline.</source>
          <target state="translated">当然，实际情况要复杂得多，根据您要评估的模型部分，您的脚本可能不会处于这两种极端情况之一。如果探查器输出无济于事，则可以尝试使用 &lt;code&gt;nvprof&lt;/code&gt; 查看&lt;a href=&quot;autograd#torch.autograd.profiler.emit_nvtx&quot;&gt; &lt;code&gt;torch.autograd.profiler.emit_nvtx()&lt;/code&gt; &lt;/a&gt;的结果。但是，请考虑到NVTX开销非常高，并且通常会出现严重偏差的时间表。</target>
        </trans-unit>
        <trans-unit id="c37dc6bfd2614888384f13ab211221f9b141d421" translate="yes" xml:space="preserve">
          <source>Old API:</source>
          <target state="translated">旧的API。</target>
        </trans-unit>
        <trans-unit id="fe1a8467f796e691ae75cd019ca47d231225ebf5" translate="yes" xml:space="preserve">
          <source>On CUDA 10.1, set environment variable &lt;code&gt;CUDA_LAUNCH_BLOCKING=1&lt;/code&gt;. This may affect performance.</source>
          <target state="translated">在CUDA 10.1上，设置环境变量 &lt;code&gt;CUDA_LAUNCH_BLOCKING=1&lt;/code&gt; 。这可能会影响性能。</target>
        </trans-unit>
        <trans-unit id="87f5d31208b13882b13d7cef323f2a640539b60a" translate="yes" xml:space="preserve">
          <source>On CUDA 10.2 or later, set environment variable (note the leading colon symbol) &lt;code&gt;CUBLAS_WORKSPACE_CONFIG=:16:8&lt;/code&gt; or &lt;code&gt;CUBLAS_WORKSPACE_CONFIG=:4096:2&lt;/code&gt;.</source>
          <target state="translated">在CUDA 10.2或更高版本上，设置环境变量（注意前导冒号） &lt;code&gt;CUBLAS_WORKSPACE_CONFIG=:16:8&lt;/code&gt; 或 &lt;code&gt;CUBLAS_WORKSPACE_CONFIG=:4096:2&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="36e5819afd89f0fa826f96c64796ff1578b24bfd" translate="yes" xml:space="preserve">
          <source>On Unix, &lt;code&gt;fork()&lt;/code&gt; is the default &lt;a href=&quot;https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing&quot;&gt;&lt;code&gt;multiprocessing&lt;/code&gt;&lt;/a&gt; start method. Using &lt;code&gt;fork()&lt;/code&gt;, child workers typically can access the &lt;code&gt;dataset&lt;/code&gt; and Python argument functions directly through the cloned address space.</source>
          <target state="translated">在Unix上， &lt;code&gt;fork()&lt;/code&gt; 是默认的&lt;a href=&quot;https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing&quot;&gt; &lt;code&gt;multiprocessing&lt;/code&gt; &lt;/a&gt;启动方法。使用 &lt;code&gt;fork()&lt;/code&gt; ，子级工作人员通常可以直接通过克隆的地址空间访问 &lt;code&gt;dataset&lt;/code&gt; 和Python参数函数。</target>
        </trans-unit>
        <trans-unit id="7e2ae33ff3c9afba44a2228dce980df719a2e771" translate="yes" xml:space="preserve">
          <source>On Windows, &lt;code&gt;spawn()&lt;/code&gt; is the default &lt;a href=&quot;https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing&quot;&gt;&lt;code&gt;multiprocessing&lt;/code&gt;&lt;/a&gt; start method. Using &lt;code&gt;spawn()&lt;/code&gt;, another interpreter is launched which runs your main script, followed by the internal worker function that receives the &lt;code&gt;dataset&lt;/code&gt;, &lt;code&gt;collate_fn&lt;/code&gt; and other arguments through &lt;a href=&quot;https://docs.python.org/3/library/pickle.html#module-pickle&quot;&gt;&lt;code&gt;pickle&lt;/code&gt;&lt;/a&gt; serialization.</source>
          <target state="translated">在Windows上， &lt;code&gt;spawn()&lt;/code&gt; 是默认的&lt;a href=&quot;https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing&quot;&gt; &lt;code&gt;multiprocessing&lt;/code&gt; &lt;/a&gt;启动方法。使用 &lt;code&gt;spawn()&lt;/code&gt; ，将启动另一个解释器，该解释器运行您的主脚本，随后是内部工作程序函数，该函数通过&lt;a href=&quot;https://docs.python.org/3/library/pickle.html#module-pickle&quot;&gt; &lt;code&gt;pickle&lt;/code&gt; &lt;/a&gt;序列化接收 &lt;code&gt;dataset&lt;/code&gt; ， &lt;code&gt;collate_fn&lt;/code&gt; 和其他参数。</target>
        </trans-unit>
        <trans-unit id="b433d4e11b599fd15dbc04960281f9eb287632d1" translate="yes" xml:space="preserve">
          <source>On each window, the function computed is:</source>
          <target state="translated">在每个窗口上,计算出的函数是。</target>
        </trans-unit>
        <trans-unit id="a84bfeb86e4aa154caeb1f582b53ed62fd133141" translate="yes" xml:space="preserve">
          <source>On modules, methods must be compiled before they can be called. The TorchScript compiler recursively compiles methods it sees when compiling other methods. By default, compilation starts on the &lt;code&gt;forward&lt;/code&gt; method. Any methods called by &lt;code&gt;forward&lt;/code&gt; will be compiled, and any methods called by those methods, and so on. To start compilation at a method other than &lt;code&gt;forward&lt;/code&gt;, use the &lt;a href=&quot;jit#torch.jit.export&quot;&gt;&lt;code&gt;@torch.jit.export&lt;/code&gt;&lt;/a&gt; decorator (&lt;code&gt;forward&lt;/code&gt; implicitly is marked &lt;code&gt;@torch.jit.export&lt;/code&gt;).</source>
          <target state="translated">在模块上，必须先编译方法，然后才能调用它们。TorchScript编译器以递归方式编译在编译其他方法时看到的方法。默认情况下，编译从 &lt;code&gt;forward&lt;/code&gt; 方法开始。由 &lt;code&gt;forward&lt;/code&gt; 调用的任何方法都将被编译，由这些方法调用的任何方法都将被编译，依此类推。要使用 &lt;code&gt;forward&lt;/code&gt; 以外的方法开始编译，请使用&lt;a href=&quot;jit#torch.jit.export&quot;&gt; &lt;code&gt;@torch.jit.export&lt;/code&gt; &lt;/a&gt;装饰器（隐式将 &lt;code&gt;forward&lt;/code&gt; 标记为 &lt;code&gt;@torch.jit.export&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="a488ba8f17e355abbbc16eb50cb56f628e0ef099" translate="yes" xml:space="preserve">
          <source>On the other hand, invoking &lt;code&gt;trace&lt;/code&gt; with module&amp;rsquo;s instance (e.g. &lt;code&gt;my_module&lt;/code&gt;) creates a new module and correctly copies parameters into the new module, so they can accumulate gradients if required.</source>
          <target state="translated">另一方面，使用模块实例（例如 &lt;code&gt;my_module&lt;/code&gt; ）调用 &lt;code&gt;trace&lt;/code&gt; 会创建一个新模块，并将参数正确复制到该新模块中，以便在需要时它们可以累积梯度。</target>
        </trans-unit>
        <trans-unit id="b392f645cfdecae166313100197dfb1e06d02b4b" translate="yes" xml:space="preserve">
          <source>Once all DDP processes have joined, the context manager will broadcast the model corresponding to the last joined process to all processes to ensure the model is the same across all processes (which is guaranteed by DDP).</source>
          <target state="translated">当所有的DDP进程加入后,上下文管理器将向所有进程广播最后加入的进程所对应的模型,以确保所有进程的模型是相同的(这是DDP所保证的)。</target>
        </trans-unit>
        <trans-unit id="3ebcd2d39bb7401d6e980dad866025a71193ae15" translate="yes" xml:space="preserve">
          <source>Once these are installed, you can use the backend for Caffe2:</source>
          <target state="translated">安装好这些后,您就可以使用Caffe2的后台了。</target>
        </trans-unit>
        <trans-unit id="c7da4cd77b5400566e4f52a1d8995e48e44e2095" translate="yes" xml:space="preserve">
          <source>Once these are installed, you can use the backend for ONNX Runtime:</source>
          <target state="translated">安装好这些后,您就可以使用ONNX Runtime的后台了。</target>
        </trans-unit>
        <trans-unit id="181dde386ac4e48b01ea4c9ac2766b9fdca3442c" translate="yes" xml:space="preserve">
          <source>Once you&amp;rsquo;ve installed TensorBoard, these utilities let you log PyTorch models and metrics into a directory for visualization within the TensorBoard UI. Scalars, images, histograms, graphs, and embedding visualizations are all supported for PyTorch models and tensors as well as Caffe2 nets and blobs.</source>
          <target state="translated">安装TensorBoard后，这些实用程序使您可以将PyTorch模型和指标记录到目录中，以便在TensorBoard UI中进行可视化。PyTorch模型和张量以及Caffe2网络和Blob均支持标量，图像，直方图，图形和嵌入可视化。</target>
        </trans-unit>
        <trans-unit id="fa0ae9a8c41213fbda341d9975cb44fc985cb402" translate="yes" xml:space="preserve">
          <source>One can either give a &lt;code&gt;scale_factor&lt;/code&gt; or the target output &lt;code&gt;size&lt;/code&gt; to calculate the output size. (You cannot give both, as it is ambiguous)</source>
          <target state="translated">可以给出 &lt;code&gt;scale_factor&lt;/code&gt; 或目标输出 &lt;code&gt;size&lt;/code&gt; 来计算输出大小。（因为模棱两可，您不能同时给出两者）</target>
        </trans-unit>
        <trans-unit id="f84825776234c0c4615edbbd6631eafc8684c310" translate="yes" xml:space="preserve">
          <source>One cannot specify both positional args &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt; and keyword args &lt;code&gt;rename_map&lt;/code&gt;.</source>
          <target state="translated">不能同时指定位置args&lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt;和关键字args &lt;code&gt;rename_map&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f99ff2b17e8a666214d13b388ce8036710cb6b91" translate="yes" xml:space="preserve">
          <source>One way to automatically catch many errors in traces is by using &lt;code&gt;check_inputs&lt;/code&gt; on the &lt;code&gt;torch.jit.trace()&lt;/code&gt; API. &lt;code&gt;check_inputs&lt;/code&gt; takes a list of tuples of inputs that will be used to re-trace the computation and verify the results. For example:</source>
          <target state="translated">自动赶上痕迹许多错误的一种方法是使用 &lt;code&gt;check_inputs&lt;/code&gt; 在 &lt;code&gt;torch.jit.trace()&lt;/code&gt; API。 &lt;code&gt;check_inputs&lt;/code&gt; 获取一个输入元组的列表，该列表将用于重新跟踪计算并验证结果。例如：</target>
        </trans-unit>
        <trans-unit id="19248650fc8fc3134816a8981b1c84826b1d56cc" translate="yes" xml:space="preserve">
          <source>OneHotCategorical</source>
          <target state="translated">OneHotCategorical</target>
        </trans-unit>
        <trans-unit id="e9f428e38cd970cf00e186de12fa06466a0c70e3" translate="yes" xml:space="preserve">
          <source>Only 2D input is supported for quantized inputs</source>
          <target state="translated">量化输入只支持2D输入。</target>
        </trans-unit>
        <trans-unit id="e86d4094fa43c487d3762de38c606afefec7936e" translate="yes" xml:space="preserve">
          <source>Only 2D inputs are supported</source>
          <target state="translated">只支持2D输入</target>
        </trans-unit>
        <trans-unit id="daede8c2e196c09bd4c265e562b51d50f0840292" translate="yes" xml:space="preserve">
          <source>Only 2D/3D input is supported for quantized inputs</source>
          <target state="translated">量化输入只支持2D/3D输入。</target>
        </trans-unit>
        <trans-unit id="27966aeec004be97d5e6f5f602ef5deb9549007c" translate="yes" xml:space="preserve">
          <source>Only &lt;code&gt;torch.quint8&lt;/code&gt; is supported for the input data type.</source>
          <target state="translated">输入数据类型仅支持 &lt;code&gt;torch.quint8&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="04aa0322b01e7ae421d1285e24a5ecee9dd8da63" translate="yes" xml:space="preserve">
          <source>Only &lt;code&gt;zeros&lt;/code&gt; is supported for the &lt;code&gt;padding_mode&lt;/code&gt; argument.</source>
          <target state="translated">&lt;code&gt;padding_mode&lt;/code&gt; 参数仅支持 &lt;code&gt;zeros&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="63c087cd6938fb9b66dc8e1e4aed57dffdb285a6" translate="yes" xml:space="preserve">
          <source>Only CUDA ops are eligible for autocasting.</source>
          <target state="translated">只有CUDA操作才有资格进行自动播报。</target>
        </trans-unit>
        <trans-unit id="a1a79ba7e0818fab9dabcadf1f32212012a954c6" translate="yes" xml:space="preserve">
          <source>Only leaf Tensors will have their &lt;a href=&quot;#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; populated during a call to &lt;a href=&quot;#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt;. To get &lt;a href=&quot;#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; populated for non-leaf Tensors, you can use &lt;a href=&quot;#torch.Tensor.retain_grad&quot;&gt;&lt;code&gt;retain_grad()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">在调用&lt;a href=&quot;#torch.Tensor.backward&quot;&gt; &lt;code&gt;backward()&lt;/code&gt; &lt;/a&gt;期间，仅叶子张量的&lt;a href=&quot;#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; &lt;/a&gt;填充。要填充非叶张量的&lt;a href=&quot;#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; &lt;/a&gt;，可以使用&lt;a href=&quot;#torch.Tensor.retain_grad&quot;&gt; &lt;code&gt;retain_grad()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4e5b1e16b907585048a530098a587f0e967c17c8" translate="yes" xml:space="preserve">
          <source>Only leaf Tensors will have their &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; populated during a call to &lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt;&lt;code&gt;backward()&lt;/code&gt;&lt;/a&gt;. To get &lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt;&lt;code&gt;grad&lt;/code&gt;&lt;/a&gt; populated for non-leaf Tensors, you can use &lt;a href=&quot;autograd#torch.Tensor.retain_grad&quot;&gt;&lt;code&gt;retain_grad()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">在调用&lt;a href=&quot;autograd#torch.Tensor.backward&quot;&gt; &lt;code&gt;backward()&lt;/code&gt; &lt;/a&gt;期间，仅叶子张量的&lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; &lt;/a&gt;填充。要填充非叶张量的&lt;a href=&quot;autograd#torch.Tensor.grad&quot;&gt; &lt;code&gt;grad&lt;/code&gt; &lt;/a&gt;，可以使用&lt;a href=&quot;autograd#torch.Tensor.retain_grad&quot;&gt; &lt;code&gt;retain_grad()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="214baf780986ab4b5685dfad680eb394c92a2724" translate="yes" xml:space="preserve">
          <source>Only nccl and gloo backend are currently supported tensors should only be GPU tensors</source>
          <target state="translated">目前只支持nccl和gloo后台,tensors应该只支持GPU tensors。</target>
        </trans-unit>
        <trans-unit id="db8dc40130a88665cd5bf8f3be6d7ea249aff179" translate="yes" xml:space="preserve">
          <source>Only nccl and gloo backend is currently supported tensors should only be GPU tensors</source>
          <target state="translated">目前只支持nccl和gloo后端,tensors应该只有GPU tensors。</target>
        </trans-unit>
        <trans-unit id="df8cfb4a9929de36ffa3e1c6452d25a578580d0f" translate="yes" xml:space="preserve">
          <source>Only nccl backend is currently supported tensors should only be GPU tensors</source>
          <target state="translated">目前只支持nccl后端,tensors应该只有GPU tensors。</target>
        </trans-unit>
        <trans-unit id="d7f15fe4ed58d21e3568885b930a49d8cb28d993" translate="yes" xml:space="preserve">
          <source>Only one of &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix&quot;&gt;&lt;code&gt;covariance_matrix&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix&quot;&gt;&lt;code&gt;precision_matrix&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril&quot;&gt;&lt;code&gt;scale_tril&lt;/code&gt;&lt;/a&gt; can be specified.</source>
          <target state="translated">只能指定&lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix&quot;&gt; &lt;code&gt;covariance_matrix&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix&quot;&gt; &lt;code&gt;precision_matrix&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril&quot;&gt; &lt;code&gt;scale_tril&lt;/code&gt; 之一&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="02ff51aa952480752b675430ae860b4c527dade3" translate="yes" xml:space="preserve">
          <source>Only out-of-place ops and Tensor methods are eligible. In-place variants and calls that explicitly supply an &lt;code&gt;out=...&lt;/code&gt; Tensor are allowed in autocast-enabled regions, but won&amp;rsquo;t go through autocasting. For example, in an autocast-enabled region &lt;code&gt;a.addmm(b, c)&lt;/code&gt; can autocast, but &lt;code&gt;a.addmm_(b, c)&lt;/code&gt; and &lt;code&gt;a.addmm(b, c, out=d)&lt;/code&gt; cannot. For best performance and stability, prefer out-of-place ops in autocast-enabled regions.</source>
          <target state="translated">只有不合时宜的操作和Tensor方法才有资格。明确提供 &lt;code&gt;out=...&lt;/code&gt; 的就地变体和调用在启用自动广播的区域中允许张量，但不会进行自动广播。例如，在启用 &lt;code&gt;a.addmm(b, c)&lt;/code&gt; 广播的区域中，a.addmm（b，c）可以自动广播，但 &lt;code&gt;a.addmm_(b, c)&lt;/code&gt; 和 &lt;code&gt;a.addmm(b, c, out=d)&lt;/code&gt; 不能。为了获得最佳性能和稳定性，请在启用自动广播的区域中选择异地操作。</target>
        </trans-unit>
        <trans-unit id="6f7a0a50221e4f4092d5689875e749bacffbc3f1" translate="yes" xml:space="preserve">
          <source>Only the GPU of &lt;code&gt;tensor_list[dst_tensor]&lt;/code&gt; on the process with rank &lt;code&gt;dst&lt;/code&gt; is going to receive the final result.</source>
          <target state="translated">排名为 &lt;code&gt;dst&lt;/code&gt; 的进程上只有 &lt;code&gt;tensor_list[dst_tensor]&lt;/code&gt; 的GPU将接收最终结果。</target>
        </trans-unit>
        <trans-unit id="c024a1c0a5c1c600879124a8ffc9c018f3eeaa1c" translate="yes" xml:space="preserve">
          <source>Only the following modes are supported for the quantized inputs:</source>
          <target state="translated">量化输入只支持以下模式。</target>
        </trans-unit>
        <trans-unit id="4abce74e0ec5249c62cdd843742dc1fcd44cf957" translate="yes" xml:space="preserve">
          <source>Only the process with rank &lt;code&gt;dst&lt;/code&gt; is going to receive the final result.</source>
          <target state="translated">只有等级为 &lt;code&gt;dst&lt;/code&gt; 的进程才能收到最终结果。</target>
        </trans-unit>
        <trans-unit id="465e942c4c87f62f9475a2c554cc5cf2204bbefd" translate="yes" xml:space="preserve">
          <source>Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted but their usage is not recommended. Users need to verify their dict inputs carefully, and keep in mind that dynamic lookups are not available.</source>
          <target state="translated">只有元组、列表和变量被支持为JIT输入/输出。字典和字符串也被接受,但不建议使用。用户需要仔细验证他们的dict输入,并记住动态查找是不可用的。</target>
        </trans-unit>
        <trans-unit id="c63b5ae248ca6bf9edcb58a14545a2ec8ac9edff" translate="yes" xml:space="preserve">
          <source>Only works with &lt;code&gt;torch.per_tensor_affine&lt;/code&gt; quantization scheme.</source>
          <target state="translated">仅适用于 &lt;code&gt;torch.per_tensor_affine&lt;/code&gt; 量化方案。</target>
        </trans-unit>
        <trans-unit id="b82189b88bcf4f6d2bb2ec50d464f322cc6fbe77" translate="yes" xml:space="preserve">
          <source>Only works with &lt;code&gt;torch.per_tensor_symmetric&lt;/code&gt; quantization scheme</source>
          <target state="translated">仅适用于 &lt;code&gt;torch.per_tensor_symmetric&lt;/code&gt; 量化方案</target>
        </trans-unit>
        <trans-unit id="4315fcaa2c448f9408ac0b9bd62d96495ec80efb" translate="yes" xml:space="preserve">
          <source>Op Eligibility</source>
          <target state="translated">经营资格</target>
        </trans-unit>
        <trans-unit id="8e1bcca5493f841f86f549bb09efaabbc0b197b6" translate="yes" xml:space="preserve">
          <source>Op-Specific Behavior</source>
          <target state="translated">具体行为</target>
        </trans-unit>
        <trans-unit id="a14e13faab81577c3e1bd1a5724a666f83008292" translate="yes" xml:space="preserve">
          <source>Opens an nvprof trace file and parses autograd annotations.</source>
          <target state="translated">打开 nvprof 跟踪文件并解析 autograd 注释。</target>
        </trans-unit>
        <trans-unit id="d0f4a3ec3e4ad0aecc8e89f342dde91d8ebe6688" translate="yes" xml:space="preserve">
          <source>Operator Export Type</source>
          <target state="translated">运营商出口类型</target>
        </trans-unit>
        <trans-unit id="bb7373849dc3f047bbc13778dd3b1cb1da43b12b" translate="yes" xml:space="preserve">
          <source>OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops (with ONNX namespace). OperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops (with aten namespace). OperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported in ONNX or its symbolic is missing, fall back on ATen op. Registered ops are exported to ONNX regularly. Example graph:</source>
          <target state="translated">OperatorExportTypes.ONNX:所有操作都以常规的 ONNX 操作导出(使用 ONNX 命名空间)。OperatorExportTypes.ONNX_ATEN:所有操作都作为 ATen 操作输出(使用 aten 命名空间)。OperatorExportTypes.ONNX_ATEN_FALLBACK:如果 ONNX 不支持 ATen 操作,或缺少其符号,则使用 ATen 操作。注册的操作会定期导出到ONNX。示例图。</target>
        </trans-unit>
        <trans-unit id="e90414358dbfff0a68e4eb5d68a16978cf197d5a" translate="yes" xml:space="preserve">
          <source>Operators</source>
          <target state="translated">Operators</target>
        </trans-unit>
        <trans-unit id="ae7fe79b856a95e009187cbf46df9c67d852304a" translate="yes" xml:space="preserve">
          <source>Ops called with an explicit &lt;code&gt;dtype=...&lt;/code&gt; argument are not eligible, and will produce output that respects the &lt;code&gt;dtype&lt;/code&gt; argument.</source>
          <target state="translated">使用显式 &lt;code&gt;dtype=...&lt;/code&gt; 参数调用的操作不符合条件，并且将产生尊重 &lt;code&gt;dtype&lt;/code&gt; 参数的输出。</target>
        </trans-unit>
        <trans-unit id="f728860bd944be6f6762552490261e98b6019554" translate="yes" xml:space="preserve">
          <source>Ops not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they&amp;rsquo;re downstream from autocasted ops.</source>
          <target state="translated">以下未列出的操作不会进行自动广播。它们以输入定义的类型运行。但是，如果未列出的操作位于自动播放的操作的下游，则自动播放仍可以更改未列出操作的运行类型。</target>
        </trans-unit>
        <trans-unit id="1e2c18bb8c291433dee05fffb46fe3081033ce4f" translate="yes" xml:space="preserve">
          <source>Ops that can autocast to &lt;code&gt;float16&lt;/code&gt;</source>
          <target state="translated">可以自动投射到 &lt;code&gt;float16&lt;/code&gt; 的操作</target>
        </trans-unit>
        <trans-unit id="d41b349395acc2ee22829c823ba6e5272a03f36e" translate="yes" xml:space="preserve">
          <source>Ops that can autocast to &lt;code&gt;float32&lt;/code&gt;</source>
          <target state="translated">可以自动投射到 &lt;code&gt;float32&lt;/code&gt; 的操作</target>
        </trans-unit>
        <trans-unit id="de95d5ff9adfbfe0e08db9c9f3168e2ebb01f3f5" translate="yes" xml:space="preserve">
          <source>Ops that promote to the widest input type</source>
          <target state="translated">晋升到最广泛的输入类型的操作。</target>
        </trans-unit>
        <trans-unit id="c518b3a6d985b38fecd8953dbd43a11b6da0e4b9" translate="yes" xml:space="preserve">
          <source>Ops that run in &lt;code&gt;float64&lt;/code&gt; or non-floating-point dtypes are not eligible, and will run in these types whether or not autocast is enabled.</source>
          <target state="translated">以 &lt;code&gt;float64&lt;/code&gt; 或非浮点dtypes运行的操作不符合条件，并且无论是否启用了自动广播，都将以这些类型运行。</target>
        </trans-unit>
        <trans-unit id="326510735a447ec63da9abec360c7b0441bdd180" translate="yes" xml:space="preserve">
          <source>Optional Type Refinement</source>
          <target state="translated">可选类型精炼</target>
        </trans-unit>
        <trans-unit id="21826226f40329aa4cc3679f6cd2d7ad202e0147" translate="yes" xml:space="preserve">
          <source>Optional values &lt;code&gt;beta&lt;/code&gt; and &lt;code&gt;alpha&lt;/code&gt; are scaling factors on the outer product between &lt;code&gt;vec1&lt;/code&gt; and &lt;code&gt;vec2&lt;/code&gt; and the added matrix &lt;code&gt;input&lt;/code&gt; respectively.</source>
          <target state="translated">可选值 &lt;code&gt;beta&lt;/code&gt; 和 &lt;code&gt;alpha&lt;/code&gt; 是 &lt;code&gt;vec1&lt;/code&gt; 和 &lt;code&gt;vec2&lt;/code&gt; 之间的外部乘积以及所添加的矩阵 &lt;code&gt;input&lt;/code&gt; 之间的比例因子。</target>
        </trans-unit>
        <trans-unit id="14cf9a30a86cd6abc4768ba63268809b6a642a72" translate="yes" xml:space="preserve">
          <source>Optionally set the Torch Hub directory used to save downloaded models &amp;amp; weights.</source>
          <target state="translated">（可选）设置用于保存下载的模型和权重的Torch Hub目录。</target>
        </trans-unit>
        <trans-unit id="3cdb2fb789b07586dcdc33ff9c3ec35d1394e04a" translate="yes" xml:space="preserve">
          <source>Optionally, you can give non-equal weighting on the classes by passing a 1D &lt;code&gt;weight&lt;/code&gt; tensor into the constructor.</source>
          <target state="translated">（可选）您可以通过将一维权 &lt;code&gt;weight&lt;/code&gt; 张量传递到构造函数中来对类进行非等重加权。</target>
        </trans-unit>
        <trans-unit id="657998dbdb24b52285f38e822b9084533640dfff" translate="yes" xml:space="preserve">
          <source>Ordinarily, &amp;ldquo;automatic mixed precision training&amp;rdquo; uses &lt;a href=&quot;#torch.cuda.amp.autocast&quot;&gt;&lt;code&gt;torch.cuda.amp.autocast&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.cuda.amp.GradScaler&quot;&gt;&lt;code&gt;torch.cuda.amp.GradScaler&lt;/code&gt;&lt;/a&gt; together, as shown in the &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/amp_examples.html#amp-examples&quot;&gt;Automatic Mixed Precision examples&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html&quot;&gt;Automatic Mixed Precision recipe&lt;/a&gt;. However, &lt;a href=&quot;#torch.cuda.amp.autocast&quot;&gt;&lt;code&gt;autocast&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.cuda.amp.GradScaler&quot;&gt;&lt;code&gt;GradScaler&lt;/code&gt;&lt;/a&gt; are modular, and may be used separately if desired.</source>
          <target state="translated">通常，&amp;ldquo;自动混合精度训练&amp;rdquo;将&lt;a href=&quot;#torch.cuda.amp.autocast&quot;&gt; &lt;code&gt;torch.cuda.amp.autocast&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;#torch.cuda.amp.GradScaler&quot;&gt; &lt;code&gt;torch.cuda.amp.GradScaler&lt;/code&gt; &lt;/a&gt;一起使用，如&amp;ldquo;&lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/amp_examples.html#amp-examples&quot;&gt;自动混合精度&amp;rdquo;示例&lt;/a&gt;和&amp;ldquo;&lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html&quot;&gt;自动混合精度&amp;rdquo;配方中所示&lt;/a&gt;。但是，自动&lt;a href=&quot;#torch.cuda.amp.autocast&quot;&gt; &lt;code&gt;autocast&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;#torch.cuda.amp.GradScaler&quot;&gt; &lt;code&gt;GradScaler&lt;/code&gt; &lt;/a&gt;是模块化的，如果需要，可以分别使用。</target>
        </trans-unit>
        <trans-unit id="ae88acefda9740ca025e9b1e8d82500694df6312" translate="yes" xml:space="preserve">
          <source>Orphan</source>
          <target state="translated">Orphan</target>
        </trans-unit>
        <trans-unit id="6e6a6f2086bb5fe5dbfd17d8d5f502d48759834b" translate="yes" xml:space="preserve">
          <source>Other</source>
          <target state="translated">Other</target>
        </trans-unit>
        <trans-unit id="d51aeaedae53d1689ff4812f653d88f9f69f9530" translate="yes" xml:space="preserve">
          <source>Other NCCL environment variables</source>
          <target state="translated">其他NCCL环境变量</target>
        </trans-unit>
        <trans-unit id="b41b4ea22c0549444f4374445d8b5be41ed6c7a3" translate="yes" xml:space="preserve">
          <source>Other Operations</source>
          <target state="translated">其他业务</target>
        </trans-unit>
        <trans-unit id="b4fb6252944f2406950d09755e09e265942a94cb" translate="yes" xml:space="preserve">
          <source>Other dimensions of &lt;code&gt;input&lt;/code&gt; that are not explicitly moved remain in their original order and appear at the positions not specified in &lt;code&gt;destination&lt;/code&gt;.</source>
          <target state="translated">未显式移动的其他 &lt;code&gt;input&lt;/code&gt; 维将保持其原始顺序，并出现在 &lt;code&gt;destination&lt;/code&gt; 中未指定的位置。</target>
        </trans-unit>
        <trans-unit id="48ae709d89b00cce606c898dd9a79282ec9c0bb8" translate="yes" xml:space="preserve">
          <source>Otherwise, &lt;code&gt;.grad&lt;/code&gt; is created with rowmajor-contiguous strides.</source>
          <target state="translated">否则， &lt;code&gt;.grad&lt;/code&gt; 与rowmajor连续进步创建。</target>
        </trans-unit>
        <trans-unit id="9b1d762f3a800a1e6b0f118d4aa509412d043de1" translate="yes" xml:space="preserve">
          <source>Otherwise, if &lt;code&gt;map_location&lt;/code&gt; is a dict, it will be used to remap location tags appearing in the file (keys), to ones that specify where to put the storages (values).</source>
          <target state="translated">否则，如果 &lt;code&gt;map_location&lt;/code&gt; 是dict，它将用于将文件（键）中显示的位置标记重新映射到指定将存储位置（值）放置的位置标记。</target>
        </trans-unit>
        <trans-unit id="9f6ea88f9397a13956243e283a62e08c4833e59a" translate="yes" xml:space="preserve">
          <source>Otherwise, it will not be possible to view &lt;code&gt;self&lt;/code&gt; tensor as &lt;code&gt;shape&lt;/code&gt; without copying it (e.g., via &lt;a href=&quot;#torch.Tensor.contiguous&quot;&gt;&lt;code&gt;contiguous()&lt;/code&gt;&lt;/a&gt;). When it is unclear whether a &lt;a href=&quot;#torch.Tensor.view&quot;&gt;&lt;code&gt;view()&lt;/code&gt;&lt;/a&gt; can be performed, it is advisable to use &lt;a href=&quot;generated/torch.reshape#torch.reshape&quot;&gt;&lt;code&gt;reshape()&lt;/code&gt;&lt;/a&gt;, which returns a view if the shapes are compatible, and copies (equivalent to calling &lt;a href=&quot;#torch.Tensor.contiguous&quot;&gt;&lt;code&gt;contiguous()&lt;/code&gt;&lt;/a&gt;) otherwise.</source>
          <target state="translated">否则，将无法将 &lt;code&gt;self&lt;/code&gt; 张量视为 &lt;code&gt;shape&lt;/code&gt; 而不复制它（例如，通过&lt;a href=&quot;#torch.Tensor.contiguous&quot;&gt; &lt;code&gt;contiguous()&lt;/code&gt; &lt;/a&gt;）。当不清楚是否可以执行&lt;a href=&quot;#torch.Tensor.view&quot;&gt; &lt;code&gt;view()&lt;/code&gt; &lt;/a&gt;时，建议使用&lt;a href=&quot;generated/torch.reshape#torch.reshape&quot;&gt; &lt;code&gt;reshape()&lt;/code&gt; &lt;/a&gt;，如果形状兼容，则返回一个视图，否则复制（等效于调用&lt;a href=&quot;#torch.Tensor.contiguous&quot;&gt; &lt;code&gt;contiguous()&lt;/code&gt; &lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="5cbc6561d27e47c3ea5b5fa6d9459eb9629b7617" translate="yes" xml:space="preserve">
          <source>Otherwise:</source>
          <target state="translated">Otherwise:</target>
        </trans-unit>
        <trans-unit id="d386f5cc6857d8a2d7baf372e44b4d1117dfed1a" translate="yes" xml:space="preserve">
          <source>Our solution is that BCELoss clamps its log function outputs to be greater than or equal to -100. This way, we can always have a finite loss value and a linear backward method.</source>
          <target state="translated">我们的解决方案是,BCELoss将其对数函数的输出箝制为大于或等于-100。这样一来,我们就可以始终拥有一个有限的损失值和一个线性后退的方法。</target>
        </trans-unit>
        <trans-unit id="02600672b80ab0a1c4955d9a3e0f68bbce6d3eed" translate="yes" xml:space="preserve">
          <source>Our sparse tensor format permits &lt;em&gt;uncoalesced&lt;/em&gt; sparse tensors, where there may be duplicate coordinates in the indices; in this case, the interpretation is that the value at that index is the sum of all duplicate value entries. Uncoalesced tensors permit us to implement certain operators more efficiently.</source>
          <target state="translated">我们的稀疏张量格式允许使用&lt;em&gt;未分块的&lt;/em&gt;稀疏张量，其中索引中可能存在重复的坐标；在这种情况下，解释是该索引处的值是所有重复值条目的总和。张量张量允许我们更有效地实施某些运算符。</target>
        </trans-unit>
        <trans-unit id="48c4c6b206793564023af6432f79ea36266139f9" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.index_add_&quot;&gt;&lt;code&gt;torch.Tensor.index_add_()&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;tensor1&lt;/code&gt; corresponds to &lt;code&gt;self&lt;/code&gt; in &lt;a href=&quot;#torch.Tensor.index_add_&quot;&gt;&lt;code&gt;torch.Tensor.index_add_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.index_add_&quot;&gt; &lt;code&gt;torch.Tensor.index_add_()&lt;/code&gt; &lt;/a&gt;的替代版本。 &lt;code&gt;tensor1&lt;/code&gt; 对应于 &lt;code&gt;self&lt;/code&gt; 于&lt;a href=&quot;#torch.Tensor.index_add_&quot;&gt; &lt;code&gt;torch.Tensor.index_add_()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="99a0b833417509afcf74b89d81969fd3e6454e0f" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.index_copy_&quot;&gt;&lt;code&gt;torch.Tensor.index_copy_()&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;tensor1&lt;/code&gt; corresponds to &lt;code&gt;self&lt;/code&gt; in &lt;a href=&quot;#torch.Tensor.index_copy_&quot;&gt;&lt;code&gt;torch.Tensor.index_copy_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.index_copy_&quot;&gt; &lt;code&gt;torch.Tensor.index_copy_()&lt;/code&gt; &lt;/a&gt;的替代版本。 &lt;code&gt;tensor1&lt;/code&gt; 对应于 &lt;code&gt;self&lt;/code&gt; 于&lt;a href=&quot;#torch.Tensor.index_copy_&quot;&gt; &lt;code&gt;torch.Tensor.index_copy_()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="f02f9ec70b467240dbc6d49d1fba7b03c1442801" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.index_fill_&quot;&gt;&lt;code&gt;torch.Tensor.index_fill_()&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;tensor1&lt;/code&gt; corresponds to &lt;code&gt;self&lt;/code&gt; in &lt;a href=&quot;#torch.Tensor.index_fill_&quot;&gt;&lt;code&gt;torch.Tensor.index_fill_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.index_fill_&quot;&gt; &lt;code&gt;torch.Tensor.index_fill_()&lt;/code&gt; &lt;/a&gt;的替代版本。 &lt;code&gt;tensor1&lt;/code&gt; 对应于 &lt;code&gt;self&lt;/code&gt; 于&lt;a href=&quot;#torch.Tensor.index_fill_&quot;&gt; &lt;code&gt;torch.Tensor.index_fill_()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="5a8086fbdf63aa63eb1e982f564377b659f098f0" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.masked_fill_&quot;&gt;&lt;code&gt;torch.Tensor.masked_fill_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.masked_fill_&quot;&gt; &lt;code&gt;torch.Tensor.masked_fill_()&lt;/code&gt; &lt;/a&gt;的异地版本</target>
        </trans-unit>
        <trans-unit id="73076ad1c2f9be4b4d38d73b62e6c3e6c045bec2" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.masked_scatter_&quot;&gt;&lt;code&gt;torch.Tensor.masked_scatter_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.masked_scatter_&quot;&gt; &lt;code&gt;torch.Tensor.masked_scatter_()&lt;/code&gt; &lt;/a&gt;的异地版本</target>
        </trans-unit>
        <trans-unit id="e013688ad79fc825d3497306dc29bb552d0e34c4" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.scatter_&quot;&gt;&lt;code&gt;torch.Tensor.scatter_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.scatter_&quot;&gt; &lt;code&gt;torch.Tensor.scatter_()&lt;/code&gt; &lt;/a&gt;的异地版本</target>
        </trans-unit>
        <trans-unit id="725700a259ff1b2b017f60b4e74698aa49714d29" translate="yes" xml:space="preserve">
          <source>Out-of-place version of &lt;a href=&quot;#torch.Tensor.scatter_add_&quot;&gt;&lt;code&gt;torch.Tensor.scatter_add_()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.scatter_add_&quot;&gt; &lt;code&gt;torch.Tensor.scatter_add_()&lt;/code&gt; &lt;/a&gt;的异地版本</target>
        </trans-unit>
        <trans-unit id="6398fea054e2780d871397e980d3cf8c7e956e4c" translate="yes" xml:space="preserve">
          <source>Out-place version of &lt;a href=&quot;#torch.Tensor.index_put_&quot;&gt;&lt;code&gt;index_put_()&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;tensor1&lt;/code&gt; corresponds to &lt;code&gt;self&lt;/code&gt; in &lt;a href=&quot;#torch.Tensor.index_put_&quot;&gt;&lt;code&gt;torch.Tensor.index_put_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.Tensor.index_put_&quot;&gt; &lt;code&gt;index_put_()&lt;/code&gt; &lt;/a&gt;的替代版本。 &lt;code&gt;tensor1&lt;/code&gt; 对应于 &lt;code&gt;self&lt;/code&gt; 于&lt;a href=&quot;#torch.Tensor.index_put_&quot;&gt; &lt;code&gt;torch.Tensor.index_put_()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="33c36eb13432b5622fd7de9ba502b6b4e6d48ded" translate="yes" xml:space="preserve">
          <source>Outer product of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;vec2&lt;/code&gt;.</source>
          <target state="translated">的外积 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;vec2&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="615e0188218c976163bf2ec4e7c83bb336305665" translate="yes" xml:space="preserve">
          <source>Outer product of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;vec2&lt;/code&gt;. If &lt;code&gt;input&lt;/code&gt; is a vector of size</source>
          <target state="translated">的外积 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;vec2&lt;/code&gt; 。如果 &lt;code&gt;input&lt;/code&gt; 是大小的向量</target>
        </trans-unit>
        <trans-unit id="67a0062a0687696c335a2c7d0e459405eba90f74" translate="yes" xml:space="preserve">
          <source>Output lists. It should contain correctly-sized tensors on each GPU to be used for output of the collective, e.g. &lt;code&gt;output_tensor_lists[i]&lt;/code&gt; contains the all_gather result that resides on the GPU of &lt;code&gt;input_tensor_list[i]&lt;/code&gt;.</source>
          <target state="translated">输出列表。它应该在每个GPU上包含正确大小的张量，以用于集合的输出，例如 &lt;code&gt;output_tensor_lists[i]&lt;/code&gt; 包含位于 &lt;code&gt;input_tensor_list[i]&lt;/code&gt; 的GPU上的all_gather结果。</target>
        </trans-unit>
        <trans-unit id="1b572830da9a955103a98dbcea6dc96cd3c1bf56" translate="yes" xml:space="preserve">
          <source>Output of running &lt;code&gt;function&lt;/code&gt; on &lt;code&gt;*args&lt;/code&gt;</source>
          <target state="translated">在 &lt;code&gt;*args&lt;/code&gt; 上输出运行 &lt;code&gt;function&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="90de97722219af193315e02c2b185d1c1a5a4186" translate="yes" xml:space="preserve">
          <source>Output of running &lt;code&gt;functions&lt;/code&gt; sequentially on &lt;code&gt;*inputs&lt;/code&gt;</source>
          <target state="translated">在 &lt;code&gt;*inputs&lt;/code&gt; 上按顺序输出正在运行的 &lt;code&gt;functions&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="613a57f34002b7db7e7eb40fb879941fd5f0eb57" translate="yes" xml:space="preserve">
          <source>Output shape: &lt;code&gt;(B, embedding_dim)&lt;/code&gt;</source>
          <target state="translated">输出形状： &lt;code&gt;(B, embedding_dim)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f6a2e936babf4473798688e71160bef838804c5c" translate="yes" xml:space="preserve">
          <source>Output tensors (on different GPUs) to receive the result of the operation.</source>
          <target state="translated">输出腾博会登录(在不同的GPU上)来接收操作结果。</target>
        </trans-unit>
        <trans-unit id="be4dd5eb977c617cc4374f35ce4dcfe3424e6c52" translate="yes" xml:space="preserve">
          <source>Output1:</source>
          <target state="translated">Output1:</target>
        </trans-unit>
        <trans-unit id="099f2ae15e5d444369fa820ff7e04ecd1eccb38d" translate="yes" xml:space="preserve">
          <source>Output2:</source>
          <target state="translated">Output2:</target>
        </trans-unit>
        <trans-unit id="f3c8c95c5e534bcd2ea0034a0d83177efa6923f4" translate="yes" xml:space="preserve">
          <source>Output:</source>
          <target state="translated">Output:</target>
        </trans-unit>
        <trans-unit id="8063b576f684099c293104f0277e7a1383d61e53" translate="yes" xml:space="preserve">
          <source>Output: &lt;code&gt;(*, embedding_dim)&lt;/code&gt;, where &lt;code&gt;*&lt;/code&gt; is the input shape</source>
          <target state="translated">输出： &lt;code&gt;(*, embedding_dim)&lt;/code&gt; ，其中 &lt;code&gt;*&lt;/code&gt; 是输入形状</target>
        </trans-unit>
        <trans-unit id="7b14dfe86f44f01989354331a43a653a7f8d7f72" translate="yes" xml:space="preserve">
          <source>Output: A Tensor of shape</source>
          <target state="translated">输出。一个形状的张量</target>
        </trans-unit>
        <trans-unit id="4a4fbbd55c6c3ccbf6bf7220f45ac1790ff9bb33" translate="yes" xml:space="preserve">
          <source>Output: scalar by default. If :attr:&lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;'none'&lt;/code&gt;, then</source>
          <target state="translated">输出：默认情况下为标量。如果：attr： &lt;code&gt;reduction&lt;/code&gt; 为 &lt;code&gt;'none'&lt;/code&gt; ，则</target>
        </trans-unit>
        <trans-unit id="7166320ecb31bd6a3586040c60443c5f16f96651" translate="yes" xml:space="preserve">
          <source>Output: scalar by default. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;'none'&lt;/code&gt;, then</source>
          <target state="translated">输出：默认情况下为标量。如果 &lt;code&gt;reduction&lt;/code&gt; 为 &lt;code&gt;'none'&lt;/code&gt; ，则</target>
        </trans-unit>
        <trans-unit id="88a34ee124c80fdb97be4c7994066abce0b11c8e" translate="yes" xml:space="preserve">
          <source>Output: scalar. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;'none'&lt;/code&gt;, then</source>
          <target state="translated">输出：标量。如果 &lt;code&gt;reduction&lt;/code&gt; 为 &lt;code&gt;'none'&lt;/code&gt; ，则</target>
        </trans-unit>
        <trans-unit id="816914c299df40485ecf46a1e795a1eb404d91d8" translate="yes" xml:space="preserve">
          <source>Output: scalar. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;'none'&lt;/code&gt;, then same shape as the input</source>
          <target state="translated">输出：标量。如果 &lt;code&gt;reduction&lt;/code&gt; 为 &lt;code&gt;'none'&lt;/code&gt; ，则与输入的形状相同</target>
        </trans-unit>
        <trans-unit id="573c4c1298793f227765c23e85deebaee6066af6" translate="yes" xml:space="preserve">
          <source>Output: scalar. If &lt;code&gt;reduction&lt;/code&gt; is &lt;code&gt;'none'&lt;/code&gt;, then the same size as the target:</source>
          <target state="translated">输出：标量。如果 &lt;code&gt;reduction&lt;/code&gt; 为 &lt;code&gt;'none'&lt;/code&gt; ，则与目标大小相同：</target>
        </trans-unit>
        <trans-unit id="6ee76fa98a639a0ac269010b7032639b0a171647" translate="yes" xml:space="preserve">
          <source>Outputs:</source>
          <target state="translated">Outputs:</target>
        </trans-unit>
        <trans-unit id="19620445bec40ebeb61b840841c0effef04f34be" translate="yes" xml:space="preserve">
          <source>Outputs: (h_1, c_1)</source>
          <target state="translated">輸出端:(h_1,c_1)(h_1,c_1)</target>
        </trans-unit>
        <trans-unit id="9b4d142414484eb98f10e629a2f06df85748b811" translate="yes" xml:space="preserve">
          <source>Outputs: h&amp;rsquo;</source>
          <target state="translated">输出：h'</target>
        </trans-unit>
        <trans-unit id="511c97e64880de564b0676893af9fcc8a480a2e2" translate="yes" xml:space="preserve">
          <source>Outputs: output, (h_n, c_n)</source>
          <target state="translated">输出:输出,(h_n,c_n)</target>
        </trans-unit>
        <trans-unit id="a459eb30ab99f3ae5b662cb99c1f4215b146179c" translate="yes" xml:space="preserve">
          <source>Outputs: output, h_n</source>
          <target state="translated">输出:输出,h_n</target>
        </trans-unit>
        <trans-unit id="3454926b31857082d753c8156d0ffd5035b9d6b1" translate="yes" xml:space="preserve">
          <source>Overloaded function.</source>
          <target state="translated">超载功能。</target>
        </trans-unit>
        <trans-unit id="23672df91d237894c7bd8c7afdec0a1cd14fb6f6" translate="yes" xml:space="preserve">
          <source>Owner Share RRef with User</source>
          <target state="translated">业主与用户共享RRef</target>
        </trans-unit>
        <trans-unit id="511993d3c99719e38a6779073019dacd7178ddb9" translate="yes" xml:space="preserve">
          <source>P</source>
          <target state="translated">P</target>
        </trans-unit>
        <trans-unit id="4a6bac0b7fcbb3ba201bd5676355ccb5034a278a" translate="yes" xml:space="preserve">
          <source>P(x) = \dfrac{1}{\text{to} - \text{from}}</source>
          <target state="translated">P(x)=\dfrac{1}{text{to}-\text{from}})</target>
        </trans-unit>
        <trans-unit id="964bd5656c7a016ef0c9ada7382da7ca1fee27a4" translate="yes" xml:space="preserve">
          <source>PRODUCT</source>
          <target state="translated">PRODUCT</target>
        </trans-unit>
        <trans-unit id="768bfcbbf6df966e333cf0230ff5aeca88678ff9" translate="yes" xml:space="preserve">
          <source>PReLU</source>
          <target state="translated">PReLU</target>
        </trans-unit>
        <trans-unit id="c6672b1c1e9f2629ba013dde01df96129b495f92" translate="yes" xml:space="preserve">
          <source>PackedSequence</source>
          <target state="translated">PackedSequence</target>
        </trans-unit>
        <trans-unit id="c0577f7163ed4ff21523cd27bc5236f92de452c9" translate="yes" xml:space="preserve">
          <source>Packs a Tensor containing padded sequences of variable length.</source>
          <target state="translated">封装一个包含可变长度填充序列的Tensor。</target>
        </trans-unit>
        <trans-unit id="65d415f11a63c4c34ebb5de81f2e889c9328e53e" translate="yes" xml:space="preserve">
          <source>Packs a list of variable length Tensors</source>
          <target state="translated">封装一个可变长度的Tensors列表。</target>
        </trans-unit>
        <trans-unit id="019020b0e5b242a0cc1a8528059f3ad86084afa9" translate="yes" xml:space="preserve">
          <source>Pad a list of variable length Tensors with &lt;code&gt;padding_value&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;padding_value&lt;/code&gt; 填充可变长度张量的列表</target>
        </trans-unit>
        <trans-unit id="2560c7ddae8fa20bff911361bd8035aef865bce1" translate="yes" xml:space="preserve">
          <source>Padding Layers</source>
          <target state="translated">填充层</target>
        </trans-unit>
        <trans-unit id="119765b1620180622c6f6bb53002149703be9254" translate="yes" xml:space="preserve">
          <source>Padding mode:</source>
          <target state="translated">填充模式。</target>
        </trans-unit>
        <trans-unit id="c28c7b4763bce482c5c0e969cee27313a7b7a4b3" translate="yes" xml:space="preserve">
          <source>Padding size:</source>
          <target state="translated">衬垫大小。</target>
        </trans-unit>
        <trans-unit id="01c1aebd02a3849d24632e98070e328ff731c754" translate="yes" xml:space="preserve">
          <source>Pads a packed batch of variable length sequences.</source>
          <target state="translated">填充一批可变长度的序列。</target>
        </trans-unit>
        <trans-unit id="5f2ca3b7c48057f8a6249de795ad4677ba74df59" translate="yes" xml:space="preserve">
          <source>Pads tensor.</source>
          <target state="translated">垫张量。</target>
        </trans-unit>
        <trans-unit id="340e6f3d3040c075fcd53240ac5a1540bc713e60" translate="yes" xml:space="preserve">
          <source>Pads the input tensor boundaries with a constant value.</source>
          <target state="translated">用一个恒定的值填充输入张量边界。</target>
        </trans-unit>
        <trans-unit id="66887cd786c809b7e475e385b383407a6d7e005a" translate="yes" xml:space="preserve">
          <source>Pads the input tensor boundaries with zero.</source>
          <target state="translated">将输入张量边界垫为零。</target>
        </trans-unit>
        <trans-unit id="15f49157ae62175a9c8bfe59b0dd171af5f13c80" translate="yes" xml:space="preserve">
          <source>Pads the input tensor using replication of the input boundary.</source>
          <target state="translated">使用输入边界的复制对输入张量进行填充。</target>
        </trans-unit>
        <trans-unit id="2095464e9318d466281e389dabe7d857b2607c1d" translate="yes" xml:space="preserve">
          <source>Pads the input tensor using the reflection of the input boundary.</source>
          <target state="translated">利用输入边界的反射对输入张量进行填充。</target>
        </trans-unit>
        <trans-unit id="9b861161343780fb206bb4b2ff66ca3fba1cab61" translate="yes" xml:space="preserve">
          <source>PairwiseDistance</source>
          <target state="translated">PairwiseDistance</target>
        </trans-unit>
        <trans-unit id="1ad9f67d0f855f646efb3775c7a4778a3cc5a138" translate="yes" xml:space="preserve">
          <source>Parallelism</source>
          <target state="translated">Parallelism</target>
        </trans-unit>
        <trans-unit id="f699f295e5ae4ac633cfa18437fed38d028b3fdb" translate="yes" xml:space="preserve">
          <source>Parameter</source>
          <target state="translated">Parameter</target>
        </trans-unit>
        <trans-unit id="cb5ac90fcc6e04cb5c8cae65265e51f0ab35a0b7" translate="yes" xml:space="preserve">
          <source>Parameter names except the first must EXACTLY match the names in &lt;code&gt;forward&lt;/code&gt;.</source>
          <target state="translated">除第一个参数名称外，参数名称必须与 &lt;code&gt;forward&lt;/code&gt; 中的名称完全匹配。</target>
        </trans-unit>
        <trans-unit id="4891fc0302ab937e6c97e91f093534d4afb88b1e" translate="yes" xml:space="preserve">
          <source>Parameter ordering does NOT necessarily match what is in &lt;code&gt;VariableType.h&lt;/code&gt;, tensors (inputs) are always first, then non-tensor arguments.</source>
          <target state="translated">参数排序不一定与 &lt;code&gt;VariableType.h&lt;/code&gt; 中的匹配，张量（输入）始终是第一个，然后是非张量参数。</target>
        </trans-unit>
        <trans-unit id="88be3e0f29cc772e39dc2a268a55384b430d8f22" translate="yes" xml:space="preserve">
          <source>ParameterDict</source>
          <target state="translated">ParameterDict</target>
        </trans-unit>
        <trans-unit id="63777906465127a1bb8f4e009ca405717880743f" translate="yes" xml:space="preserve">
          <source>ParameterDict can be indexed like a regular Python dictionary, but parameters it contains are properly registered, and will be visible by all Module methods.</source>
          <target state="translated">ParameterDict可以像普通的Python字典一样进行索引,但它所包含的参数是被正确注册的,并且将被所有的Module方法看到。</target>
        </trans-unit>
        <trans-unit id="acdf35da4c553e1dfc2210109486c37c589cbf86" translate="yes" xml:space="preserve">
          <source>ParameterList</source>
          <target state="translated">ParameterList</target>
        </trans-unit>
        <trans-unit id="a975eea30db9fa05003e3b5097688bd49ec7e01b" translate="yes" xml:space="preserve">
          <source>Parameters</source>
          <target state="translated">Parameters</target>
        </trans-unit>
        <trans-unit id="b2ef1af91a76a3fb94590270b4864284563ecce3" translate="yes" xml:space="preserve">
          <source>Parameters are &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;&lt;code&gt;Tensor&lt;/code&gt;&lt;/a&gt; subclasses, that have a very special property when used with &lt;code&gt;Module&lt;/code&gt; s - when they&amp;rsquo;re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in &lt;code&gt;parameters()&lt;/code&gt; iterator. Assigning a Tensor doesn&amp;rsquo;t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as &lt;a href=&quot;#torch.nn.parameter.Parameter&quot;&gt;&lt;code&gt;Parameter&lt;/code&gt;&lt;/a&gt;, these temporaries would get registered too.</source>
          <target state="translated">参数是&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt; &lt;code&gt;Tensor&lt;/code&gt; &lt;/a&gt;子类，与 &lt;code&gt;Module&lt;/code&gt; 一起使用时具有非常特殊的属性-当将它们分配为Module属性时，它们会自动添加到其参数列表中，并会出现在例如 &lt;code&gt;parameters()&lt;/code&gt; 迭代器中。分配张量不会产生这种效果。这是因为可能要在模型中缓存一些临时状态，例如RNN的最后一个隐藏状态。如果没有诸如&lt;a href=&quot;#torch.nn.parameter.Parameter&quot;&gt; &lt;code&gt;Parameter&lt;/code&gt; &lt;/a&gt;这样的类，那么这些临时对象也将被注册。</target>
        </trans-unit>
        <trans-unit id="3df6923b905b81224f8a63a634516931831a9327" translate="yes" xml:space="preserve">
          <source>Parameters are never broadcast between processes. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all processes in the same way. Buffers (e.g. BatchNorm stats) are broadcast from the module in process of rank 0, to all other replicas in the system in every iteration.</source>
          <target state="translated">参数永远不会在进程间广播。模块在梯度上执行全还原步骤,并假设它们将在所有进程中被优化器以同样的方式修改。缓冲区(如BatchNorm统计)在每次迭代时都会从等级为0的进程中的模块广播到系统中的所有其他副本。</target>
        </trans-unit>
        <trans-unit id="794cc4ffc58cd8aa6cf549921b36b20e1749013e" translate="yes" xml:space="preserve">
          <source>Parameters need to be specified as collections that have a deterministic ordering that is consistent between runs. Examples of objects that don&amp;rsquo;t satisfy those properties are sets and iterators over values of dictionaries.</source>
          <target state="translated">需要将参数指定为具有确定性顺序的集合，这些顺序在两次运行之间是一致的。不满足这些属性的对象的示例是字典值的集合和迭代器。</target>
        </trans-unit>
        <trans-unit id="63ed4981dcd5565ccc67fe6943dae94af81704fd" translate="yes" xml:space="preserve">
          <source>Pareto</source>
          <target state="translated">Pareto</target>
        </trans-unit>
        <trans-unit id="a70edfd33f03d14b9304a9ca1fe474eaf0817702" translate="yes" xml:space="preserve">
          <source>Parsing the local_rank argument</source>
          <target state="translated">解析local_rank参数</target>
        </trans-unit>
        <trans-unit id="48a79b6f6f692b43d8485194d95ec3b0c6abe1d9" translate="yes" xml:space="preserve">
          <source>Pass the input through the encoder layer.</source>
          <target state="translated">通过编码器层的输入。</target>
        </trans-unit>
        <trans-unit id="74a6cfbb8f181d4db771aa9745afb6080e4ee4fe" translate="yes" xml:space="preserve">
          <source>Pass the input through the encoder layers in turn.</source>
          <target state="translated">依次通过编码器层的输入。</target>
        </trans-unit>
        <trans-unit id="4d0cc1f642a2a736b4718ae462ea2ac809ed76b7" translate="yes" xml:space="preserve">
          <source>Pass the inputs (and mask) through the decoder layer in turn.</source>
          <target state="translated">将输入(和掩码)依次通过解码层。</target>
        </trans-unit>
        <trans-unit id="8c65d8ab6bdf0e2280cb1ffcf102aefaac492303" translate="yes" xml:space="preserve">
          <source>Pass the inputs (and mask) through the decoder layer.</source>
          <target state="translated">将输入(和掩码)通过解码层。</target>
        </trans-unit>
        <trans-unit id="fd97c67f36d1e3c87f8ab45030bd4f215e7035cc" translate="yes" xml:space="preserve">
          <source>Passing -1 as the size for a dimension means not changing the size of that dimension.</source>
          <target state="translated">传递-1作为一个维度的大小意味着不改变该维度的大小。</target>
        </trans-unit>
        <trans-unit id="1ade9bd239bdf873c59edb2cc4d6f0dd2841681d" translate="yes" xml:space="preserve">
          <source>Passing &lt;code&gt;new_scale&lt;/code&gt; sets the scale directly.</source>
          <target state="translated">传递 &lt;code&gt;new_scale&lt;/code&gt; 直接设置比例。</target>
        </trans-unit>
        <trans-unit id="6f36a4168bfec1245f47f9820a4b54d62ffb6bc8" translate="yes" xml:space="preserve">
          <source>Pathwise derivative</source>
          <target state="translated">Pathwise derivative</target>
        </trans-unit>
        <trans-unit id="ea3c8179bd96f54267bf75b132502b2bb1730b13" translate="yes" xml:space="preserve">
          <source>Pattern Matching Assignments</source>
          <target state="translated">模式匹配作业</target>
        </trans-unit>
        <trans-unit id="246efe0491937cc86746284b090916b8754123a3" translate="yes" xml:space="preserve">
          <source>Per-parameter options</source>
          <target state="translated">按参数选项</target>
        </trans-unit>
        <trans-unit id="bbb4559fe386a24723c5dad9bdcdef77441cfdce" translate="yes" xml:space="preserve">
          <source>Perform a shutdown of the RPC agent, and then destroy the RPC agent. This stops the local agent from accepting outstanding requests, and shuts down the RPC framework by terminating all RPC threads. If &lt;code&gt;graceful=True&lt;/code&gt;, this will block until all local and remote RPC processes reach this method and wait for all outstanding work to complete. Otherwise, if &lt;code&gt;graceful=False&lt;/code&gt;, this is a local shutdown, and it does not wait for other RPC processes to reach this method.</source>
          <target state="translated">关闭RPC代理，然后销毁RPC代理。这将阻止本地代理接受未完成的请求，并通过终止所有RPC线程来关闭RPC框架。如果 &lt;code&gt;graceful=True&lt;/code&gt; ，它将阻塞直到所有本地和远程RPC进程都到达此方法并等待所有未完成的工作完成。否则，如果 &lt;code&gt;graceful=False&lt;/code&gt; ，则这是本地关闭，并且它不等待其他RPC进程到达此方法。</target>
        </trans-unit>
        <trans-unit id="000d790b0603ede49c70b8206287b63e59d6ce01" translate="yes" xml:space="preserve">
          <source>Performs</source>
          <target state="translated">Performs</target>
        </trans-unit>
        <trans-unit id="ab6a68d7b5ed065f1d2b1e10ab32e7ca4fa89132" translate="yes" xml:space="preserve">
          <source>Performs Tensor dtype and/or device conversion. A &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; are inferred from the arguments of &lt;code&gt;self.to(*args, **kwargs)&lt;/code&gt;.</source>
          <target state="translated">执行Tensor dtype和/或设备转换。甲&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;从的参数推断出 &lt;code&gt;self.to(*args, **kwargs)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="148ba906f736c7be8f3690226a95e0fc3751c601" translate="yes" xml:space="preserve">
          <source>Performs a &amp;ldquo;true&amp;rdquo; division like Python 3. See &lt;a href=&quot;torch.floor_divide#torch.floor_divide&quot;&gt;&lt;code&gt;torch.floor_divide()&lt;/code&gt;&lt;/a&gt; for floor division.</source>
          <target state="translated">执行类似于Python 3的&amp;ldquo;真实&amp;rdquo;划分。有关楼层划分，请参见&lt;a href=&quot;torch.floor_divide#torch.floor_divide&quot;&gt; &lt;code&gt;torch.floor_divide()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="ca1d45c075edfbcf89b6d16ec3eabd49d65b9ace" translate="yes" xml:space="preserve">
          <source>Performs a batch matrix-matrix product of matrices in &lt;code&gt;batch1&lt;/code&gt; and &lt;code&gt;batch2&lt;/code&gt;.</source>
          <target state="translated">执行在矩阵的批次矩阵矩阵乘积 &lt;code&gt;batch1&lt;/code&gt; 和 &lt;code&gt;batch2&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1d3416bea00d8ee005b5f0e7621baf85cbbbc8e4" translate="yes" xml:space="preserve">
          <source>Performs a batch matrix-matrix product of matrices in &lt;code&gt;batch1&lt;/code&gt; and &lt;code&gt;batch2&lt;/code&gt;. &lt;code&gt;input&lt;/code&gt; is added to the final result.</source>
          <target state="translated">执行在矩阵的批次矩阵矩阵乘积 &lt;code&gt;batch1&lt;/code&gt; 和 &lt;code&gt;batch2&lt;/code&gt; 。 &lt;code&gt;input&lt;/code&gt; 被添加到最终结果中。</target>
        </trans-unit>
        <trans-unit id="6629b05e96ef663c5b3cf02ed167a20014b205ec" translate="yes" xml:space="preserve">
          <source>Performs a batch matrix-matrix product of matrices stored in &lt;code&gt;batch1&lt;/code&gt; and &lt;code&gt;batch2&lt;/code&gt;, with a reduced add step (all matrix multiplications get accumulated along the first dimension).</source>
          <target state="translated">执行存储在矩阵的批次矩阵矩阵乘积 &lt;code&gt;batch1&lt;/code&gt; 和 &lt;code&gt;batch2&lt;/code&gt; ，具有减小的附加步骤（所有矩阵乘法获得沿着第一维度积累）。</target>
        </trans-unit>
        <trans-unit id="1710424b09b25bd3fe48dce1c221f445326275f1" translate="yes" xml:space="preserve">
          <source>Performs a batch matrix-matrix product of matrices stored in &lt;code&gt;batch1&lt;/code&gt; and &lt;code&gt;batch2&lt;/code&gt;, with a reduced add step (all matrix multiplications get accumulated along the first dimension). &lt;code&gt;input&lt;/code&gt; is added to the final result.</source>
          <target state="translated">执行存储在矩阵的批次矩阵矩阵乘积 &lt;code&gt;batch1&lt;/code&gt; 和 &lt;code&gt;batch2&lt;/code&gt; ，具有减小的附加步骤（所有矩阵乘法获得沿着第一维度积累）。 &lt;code&gt;input&lt;/code&gt; 被添加到最终结果中。</target>
        </trans-unit>
        <trans-unit id="6860592c3b78f43f0a520489d41db750bddd29e5" translate="yes" xml:space="preserve">
          <source>Performs a batch matrix-matrix product of matrices stored in &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;mat2&lt;/code&gt;.</source>
          <target state="translated">对 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;mat2&lt;/code&gt; 中存储的矩阵执行批处理矩阵矩阵乘积。</target>
        </trans-unit>
        <trans-unit id="df9a63e54344ddcdcb571d3c78f6a26c0d733eac" translate="yes" xml:space="preserve">
          <source>Performs a matrix multiplication of the matrices &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;mat2&lt;/code&gt;.</source>
          <target state="translated">执行矩阵 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;mat2&lt;/code&gt; 的矩阵乘法。</target>
        </trans-unit>
        <trans-unit id="e06b6aa87fc19869b8dd410c826fba4f794df727" translate="yes" xml:space="preserve">
          <source>Performs a matrix multiplication of the matrices &lt;code&gt;mat1&lt;/code&gt; and &lt;code&gt;mat2&lt;/code&gt;.</source>
          <target state="translated">执行矩阵 &lt;code&gt;mat1&lt;/code&gt; 和 &lt;code&gt;mat2&lt;/code&gt; 的矩阵乘法。</target>
        </trans-unit>
        <trans-unit id="bc47b86c1cbb06be939cda2d15d03119065f2793" translate="yes" xml:space="preserve">
          <source>Performs a matrix multiplication of the matrices &lt;code&gt;mat1&lt;/code&gt; and &lt;code&gt;mat2&lt;/code&gt;. The matrix &lt;code&gt;input&lt;/code&gt; is added to the final result.</source>
          <target state="translated">执行矩阵 &lt;code&gt;mat1&lt;/code&gt; 和 &lt;code&gt;mat2&lt;/code&gt; 的矩阵乘法。矩阵 &lt;code&gt;input&lt;/code&gt; 被添加到最终结果中。</target>
        </trans-unit>
        <trans-unit id="ec815028dc3febded0f07da8c4424505ed14884e" translate="yes" xml:space="preserve">
          <source>Performs a matrix multiplication of the sparse matrix &lt;code&gt;mat1&lt;/code&gt; and dense matrix &lt;code&gt;mat2&lt;/code&gt;. Similar to &lt;a href=&quot;generated/torch.mm#torch.mm&quot;&gt;&lt;code&gt;torch.mm()&lt;/code&gt;&lt;/a&gt;, If &lt;code&gt;mat1&lt;/code&gt; is a</source>
          <target state="translated">执行稀疏矩阵 &lt;code&gt;mat1&lt;/code&gt; 和密集矩阵 &lt;code&gt;mat2&lt;/code&gt; 的矩阵乘法。类似于&lt;a href=&quot;generated/torch.mm#torch.mm&quot;&gt; &lt;code&gt;torch.mm()&lt;/code&gt; &lt;/a&gt;，如果 &lt;code&gt;mat1&lt;/code&gt; 是</target>
        </trans-unit>
        <trans-unit id="0fcbec8807d0db14810ce48d50247513762338dc" translate="yes" xml:space="preserve">
          <source>Performs a matrix-vector product of the matrix &lt;code&gt;input&lt;/code&gt; and the vector &lt;code&gt;vec&lt;/code&gt;.</source>
          <target state="translated">执行矩阵 &lt;code&gt;input&lt;/code&gt; 和向量 &lt;code&gt;vec&lt;/code&gt; 的矩阵向量乘积。</target>
        </trans-unit>
        <trans-unit id="fd688cf71edd9657ad9f3dba62a61d9bb7db410e" translate="yes" xml:space="preserve">
          <source>Performs a matrix-vector product of the matrix &lt;code&gt;mat&lt;/code&gt; and the vector &lt;code&gt;vec&lt;/code&gt;.</source>
          <target state="translated">执行矩阵 &lt;code&gt;mat&lt;/code&gt; 和向量 &lt;code&gt;vec&lt;/code&gt; 的矩阵向量乘积。</target>
        </trans-unit>
        <trans-unit id="0d3f478da414f374637a9f8efee9102a8950a0aa" translate="yes" xml:space="preserve">
          <source>Performs a matrix-vector product of the matrix &lt;code&gt;mat&lt;/code&gt; and the vector &lt;code&gt;vec&lt;/code&gt;. The vector &lt;code&gt;input&lt;/code&gt; is added to the final result.</source>
          <target state="translated">执行矩阵 &lt;code&gt;mat&lt;/code&gt; 和向量 &lt;code&gt;vec&lt;/code&gt; 的矩阵向量乘积。向量 &lt;code&gt;input&lt;/code&gt; 将添加到最终结果中。</target>
        </trans-unit>
        <trans-unit id="0efe20233a22c5777e16f0f9f23f0594b1694e45" translate="yes" xml:space="preserve">
          <source>Performs a single optimization step (parameter update).</source>
          <target state="translated">执行单个优化步骤(参数更新)。</target>
        </trans-unit>
        <trans-unit id="eb4301873be53ea1f5d5d9d23cf8670a4bf294c6" translate="yes" xml:space="preserve">
          <source>Performs a single optimization step.</source>
          <target state="translated">执行单个优化步骤。</target>
        </trans-unit>
        <trans-unit id="097227fe1294d426da153667fbb89e50d7a799f6" translate="yes" xml:space="preserve">
          <source>Performs dtype and/or device conversion on &lt;code&gt;self.data&lt;/code&gt;.</source>
          <target state="translated">在 &lt;code&gt;self.data&lt;/code&gt; 上执行dtype和/或设备转换。</target>
        </trans-unit>
        <trans-unit id="50008568c0d385fc9c65f3ac3f0c22f33a0fd7e8" translate="yes" xml:space="preserve">
          <source>Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.</source>
          <target state="translated">对低阶矩阵、批量矩阵或稀疏矩阵进行线性主成分分析(PCA)。</target>
        </trans-unit>
        <trans-unit id="20ee59ad32c7c62194b8cc1470dab04b5aeb1063" translate="yes" xml:space="preserve">
          <source>Performs the element-wise division of &lt;code&gt;tensor1&lt;/code&gt; by &lt;code&gt;tensor2&lt;/code&gt;, multiply the result by the scalar &lt;code&gt;value&lt;/code&gt; and add it to &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">将 &lt;code&gt;tensor1&lt;/code&gt; 除以 &lt;code&gt;tensor2&lt;/code&gt; 进行逐元素除法，将结果乘以标 &lt;code&gt;value&lt;/code&gt; 然后将其添加到 &lt;code&gt;input&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="20dfc56a802418cc6cb883565e29a8149e17cc74" translate="yes" xml:space="preserve">
          <source>Performs the element-wise multiplication of &lt;code&gt;tensor1&lt;/code&gt; by &lt;code&gt;tensor2&lt;/code&gt;, multiply the result by the scalar &lt;code&gt;value&lt;/code&gt; and add it to &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">将 &lt;code&gt;tensor1&lt;/code&gt; 与tensor2进行 &lt;code&gt;tensor2&lt;/code&gt; 元素相乘，将结果与标 &lt;code&gt;value&lt;/code&gt; 相乘，然后将其加到 &lt;code&gt;input&lt;/code&gt; 上。</target>
        </trans-unit>
        <trans-unit id="5b3394929b52d1d2c8a062a547e9e7818f342005" translate="yes" xml:space="preserve">
          <source>Performs the operation.</source>
          <target state="translated">执行操作。</target>
        </trans-unit>
        <trans-unit id="b8d3d151e5b759fb242c35ceb1f1a9e038806294" translate="yes" xml:space="preserve">
          <source>Performs the outer-product of vectors &lt;code&gt;vec1&lt;/code&gt; and &lt;code&gt;vec2&lt;/code&gt; and adds it to the matrix &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">执行向量 &lt;code&gt;vec1&lt;/code&gt; 和 &lt;code&gt;vec2&lt;/code&gt; 的外积并将其添加到矩阵 &lt;code&gt;input&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7775a934f392a3fd32435cb3afba7f94cd7e9125" translate="yes" xml:space="preserve">
          <source>Permutes the dimensions of the &lt;code&gt;self&lt;/code&gt; tensor to match the dimension order in the &lt;code&gt;other&lt;/code&gt; tensor, adding size-one dims for any new names.</source>
          <target state="translated">排列 &lt;code&gt;self&lt;/code&gt; 张量的尺寸以匹配 &lt;code&gt;other&lt;/code&gt; 张量中的尺寸顺序，为任何新名称添加大小为1的暗淡。</target>
        </trans-unit>
        <trans-unit id="172d57479053b5b2d1caa6f27506a79abfa3d89b" translate="yes" xml:space="preserve">
          <source>Permutes the dimensions of the &lt;code&gt;self&lt;/code&gt; tensor to match the order specified in &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt;, adding size-one dims for any new names.</source>
          <target state="translated">排列 &lt;code&gt;self&lt;/code&gt; 张量的尺寸以匹配&lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt;指定的顺序，为任何新名称添加大小为1的暗调。</target>
        </trans-unit>
        <trans-unit id="89073f3c192cb17be2601e291aa1534b084f3029" translate="yes" xml:space="preserve">
          <source>PixelShuffle</source>
          <target state="translated">PixelShuffle</target>
        </trans-unit>
        <trans-unit id="c026434074e7feb786c10d360bdf80aa00a3000a" translate="yes" xml:space="preserve">
          <source>Platform-specific behaviors</source>
          <target state="translated">特定平台行为</target>
        </trans-unit>
        <trans-unit id="46d351806d607c75c00d9e7addd52a82379f8961" translate="yes" xml:space="preserve">
          <source>Please checkout &lt;a href=&quot;#tracing-vs-scripting&quot;&gt;Tracing vs Scripting&lt;/a&gt;.</source>
          <target state="translated">请签出&lt;a href=&quot;#tracing-vs-scripting&quot;&gt;跟踪与脚本&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="77243e0c917ac356b20b9bd68dd15bf839ebd531" translate="yes" xml:space="preserve">
          <source>Please ensure that &lt;code&gt;device_ids&lt;/code&gt; argument is set to be the only GPU device id that your code will be operating on. This is generally the local rank of the process. In other words, the &lt;code&gt;device_ids&lt;/code&gt; needs to be &lt;code&gt;[args.local_rank]&lt;/code&gt;, and &lt;code&gt;output_device&lt;/code&gt; needs to be &lt;code&gt;args.local_rank&lt;/code&gt; in order to use this utility</source>
          <target state="translated">请确保将 &lt;code&gt;device_ids&lt;/code&gt; 参数设置为您的代码将在其上运行的唯一GPU设备ID。这通常是该过程的本地等级。换句话说， &lt;code&gt;device_ids&lt;/code&gt; 必须为 &lt;code&gt;[args.local_rank]&lt;/code&gt; ，而 &lt;code&gt;output_device&lt;/code&gt; 必须为 &lt;code&gt;args.local_rank&lt;/code&gt; 才能使用此实用程序</target>
        </trans-unit>
        <trans-unit id="489042bcd064b62307292513642ef8adc6ce3919" translate="yes" xml:space="preserve">
          <source>Please refer to &lt;a href=&quot;https://pytorch.org/tutorials/beginner/dist_overview.html&quot;&gt;PyTorch Distributed Overview&lt;/a&gt; for a brief introduction to all features related to distributed training.</source>
          <target state="translated">请参阅&lt;a href=&quot;https://pytorch.org/tutorials/beginner/dist_overview.html&quot;&gt;PyTorch分布式概述&lt;/a&gt;，以简要了解与分布式培训相关的所有功能。</target>
        </trans-unit>
        <trans-unit id="17832db8653d0843df90a5d67009e3924ef34997" translate="yes" xml:space="preserve">
          <source>Please see &lt;a href=&quot;#torch.Tensor.expand&quot;&gt;&lt;code&gt;expand()&lt;/code&gt;&lt;/a&gt; for more information about &lt;code&gt;expand&lt;/code&gt;.</source>
          <target state="translated">请参阅&lt;a href=&quot;#torch.Tensor.expand&quot;&gt; &lt;code&gt;expand()&lt;/code&gt; &lt;/a&gt;了解有关 &lt;code&gt;expand&lt;/code&gt; 的更多信息。</target>
        </trans-unit>
        <trans-unit id="5ec18ad55b74e4a4762c499e962c30b802cc51eb" translate="yes" xml:space="preserve">
          <source>Please see &lt;a href=&quot;#torch.Tensor.view&quot;&gt;&lt;code&gt;view()&lt;/code&gt;&lt;/a&gt; for more information about &lt;code&gt;view&lt;/code&gt;.</source>
          <target state="translated">有关 &lt;code&gt;view&lt;/code&gt; 的更多信息，请参见&lt;a href=&quot;#torch.Tensor.view&quot;&gt; &lt;code&gt;view()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6ce3ed1431a0c41a218668fa1492f351b3c1863d" translate="yes" xml:space="preserve">
          <source>Please see &lt;a href=&quot;generated/torch.reshape#torch.reshape&quot;&gt;&lt;code&gt;reshape()&lt;/code&gt;&lt;/a&gt; for more information about &lt;code&gt;reshape&lt;/code&gt;.</source>
          <target state="translated">有关 &lt;code&gt;reshape&lt;/code&gt; 的更多信息，请参见&lt;a href=&quot;generated/torch.reshape#torch.reshape&quot;&gt; &lt;code&gt;reshape()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="037ad39d2570fc5f7861f69bb41699739bb0d316" translate="yes" xml:space="preserve">
          <source>Please see &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU&lt;/a&gt; for more documentation on ReLU.</source>
          <target state="translated">有关ReLU的更多文档，请参见&lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU&quot;&gt;https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4353aa1de0244f4e4d17336cf0860de21b27bcfd" translate="yes" xml:space="preserve">
          <source>Point-to-point communication</source>
          <target state="translated">点对点通信</target>
        </trans-unit>
        <trans-unit id="f2954403f1437b0d34c50ea2ba4d35acd2ad8ef0" translate="yes" xml:space="preserve">
          <source>Pointwise Ops</source>
          <target state="translated">点式操作</target>
        </trans-unit>
        <trans-unit id="7f6c43edfe5b4aaaf229cadb3d90c7f7c42e39d6" translate="yes" xml:space="preserve">
          <source>Poisson</source>
          <target state="translated">Poisson</target>
        </trans-unit>
        <trans-unit id="fa7dfe92888800753b97007c7d56433a35b2c1ff" translate="yes" xml:space="preserve">
          <source>Poisson negative log likelihood loss.</source>
          <target state="translated">Poisson负对数似然损失。</target>
        </trans-unit>
        <trans-unit id="080ef63a7d510bdbbbb0c01b547bfd9466aafda2" translate="yes" xml:space="preserve">
          <source>PoissonNLLLoss</source>
          <target state="translated">PoissonNLLLoss</target>
        </trans-unit>
        <trans-unit id="8774a7b82c80db34246fbe30ad918e5ef44c7d79" translate="yes" xml:space="preserve">
          <source>Pool type:</source>
          <target state="translated">泳池类型:</target>
        </trans-unit>
        <trans-unit id="58ef016bfefe7d0fa87dbd8a9397011926b50b05" translate="yes" xml:space="preserve">
          <source>Pooling functions</source>
          <target state="translated">汇集功能</target>
        </trans-unit>
        <trans-unit id="4f14f8bf99ac13b99d1b436dd62fcdf8537ddee0" translate="yes" xml:space="preserve">
          <source>Pooling layers</source>
          <target state="translated">汇集层</target>
        </trans-unit>
        <trans-unit id="0b3306afd19e4390fd2c7bf86d987f82d98b6188" translate="yes" xml:space="preserve">
          <source>Pops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended.</source>
          <target state="translated">从嵌套的范围堆栈中弹出一个范围。返回被结束的范围的零基深度。</target>
        </trans-unit>
        <trans-unit id="7280aa9b126e519bb5acdcab7417841a30bb930a" translate="yes" xml:space="preserve">
          <source>Possible values are:</source>
          <target state="translated">可能的数值是:</target>
        </trans-unit>
        <trans-unit id="1774bc5c596634bc97c87b3b60bbe553b38e8300" translate="yes" xml:space="preserve">
          <source>Prefer &lt;code&gt;binary_cross_entropy_with_logits&lt;/code&gt; over &lt;code&gt;binary_cross_entropy&lt;/code&gt;</source>
          <target state="translated">身高 &lt;code&gt;binary_cross_entropy_with_logits&lt;/code&gt; 超过 &lt;code&gt;binary_cross_entropy&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="1167b0995920e6f31e81abce0609fca20a13f6e0" translate="yes" xml:space="preserve">
          <source>Prepares a copy of the model for quantization calibration or quantization-aware training and converts it to quantized version.</source>
          <target state="translated">为量化校准或量化感知训练准备一份模型副本,并将其转换为量化版本。</target>
        </trans-unit>
        <trans-unit id="17bb7ea4855300bdce06f568a4c1e23392bcdee5" translate="yes" xml:space="preserve">
          <source>Prepares a copy of the model for quantization calibration or quantization-aware training.</source>
          <target state="translated">准备一份模型的副本,用于量化校准或量化感知训练。</target>
        </trans-unit>
        <trans-unit id="3c85cac74e37970ecc55d37819ce7376a3e2e8cd" translate="yes" xml:space="preserve">
          <source>Preparing model for quantization</source>
          <target state="translated">准备量化模型</target>
        </trans-unit>
        <trans-unit id="6175db74ea439762cf31ac47bf2800599471e037" translate="yes" xml:space="preserve">
          <source>Pretrained weights can either be stored locally in the github repo, or loadable by &lt;a href=&quot;#torch.hub.load_state_dict_from_url&quot;&gt;&lt;code&gt;torch.hub.load_state_dict_from_url()&lt;/code&gt;&lt;/a&gt;. If less than 2GB, it&amp;rsquo;s recommended to attach it to a &lt;a href=&quot;https://help.github.com/en/articles/distributing-large-binaries&quot;&gt;project release&lt;/a&gt; and use the url from the release. In the example above &lt;code&gt;torchvision.models.resnet.resnet18&lt;/code&gt; handles &lt;code&gt;pretrained&lt;/code&gt;, alternatively you can put the following logic in the entrypoint definition.</source>
          <target state="translated">预训练的权重既可以存储在github存储库中，也可以由&lt;a href=&quot;#torch.hub.load_state_dict_from_url&quot;&gt; &lt;code&gt;torch.hub.load_state_dict_from_url()&lt;/code&gt; &lt;/a&gt;加载。如果小于2GB，建议将其附加到&lt;a href=&quot;https://help.github.com/en/articles/distributing-large-binaries&quot;&gt;项目发行版中，&lt;/a&gt;并使用发行版中的url。在上面的示例中， &lt;code&gt;torchvision.models.resnet.resnet18&lt;/code&gt; 处理 &lt;code&gt;pretrained&lt;/code&gt; ，或者，您可以在入口点定义中添加以下逻辑。</target>
        </trans-unit>
        <trans-unit id="c189714d06cfa7c01d24df029a774f3f30e4981e" translate="yes" xml:space="preserve">
          <source>Primarily used for quantization to float16 which doesn&amp;rsquo;t require determining ranges.</source>
          <target state="translated">主要用于量化为float16，不需要确定范围。</target>
        </trans-unit>
        <trans-unit id="fd6f86c9793df0ba1099fc411b3e7ec52a6a1cfe" translate="yes" xml:space="preserve">
          <source>Print Statements</source>
          <target state="translated">打印报表</target>
        </trans-unit>
        <trans-unit id="fee7456cc782e1f8058fb173d31005a875e61eb8" translate="yes" xml:space="preserve">
          <source>Prints an EventList as a nicely formatted table.</source>
          <target state="translated">将EventList打印成一个格式良好的表格。</target>
        </trans-unit>
        <trans-unit id="06e3b604d03b7cb14c38bef03c1ff6819f9e40ec" translate="yes" xml:space="preserve">
          <source>Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before the optimizer&amp;rsquo;s update; 1.1.0 changed this behavior in a BC-breaking way. If you use the learning rate scheduler (calling &lt;code&gt;scheduler.step()&lt;/code&gt;) before the optimizer&amp;rsquo;s update (calling &lt;code&gt;optimizer.step()&lt;/code&gt;), this will skip the first value of the learning rate schedule. If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check if you are calling &lt;code&gt;scheduler.step()&lt;/code&gt; at the wrong time.</source>
          <target state="translated">在PyTorch 1.1.0之前，学习率调度程序应在优化程序更新之前被调用。1.1.0改变了这种行为，打破了传统文化。如果在更新 &lt;code&gt;optimizer.step()&lt;/code&gt; 调用optimizer.step（））之前使用学习率调度器（调用 &lt;code&gt;scheduler.step()&lt;/code&gt; ）），则会跳过学习率调度器的第一个值。如果升级到PyTorch 1.1.0后无法重现结果，请检查是否在错误的时间调用了 &lt;code&gt;scheduler.step()&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7444118490ce8aa2e0fbbbea8ea2ca65e38de32d" translate="yes" xml:space="preserve">
          <source>Probability distributions - torch.distributions</source>
          <target state="translated">概率分布-torch.distributions(分布)</target>
        </trans-unit>
        <trans-unit id="d36d84ef2381c76d6d53fffa7008829b5eaeb1fc" translate="yes" xml:space="preserve">
          <source>Process Group Backend</source>
          <target state="translated">流程组后端</target>
        </trans-unit>
        <trans-unit id="40e121339893a371c4a1b2c140642c9b6a34e9bd" translate="yes" xml:space="preserve">
          <source>Produces several warnings and a graph which simply returns the input:</source>
          <target state="translated">产生几个警告和一个简单返回输入的图形。</target>
        </trans-unit>
        <trans-unit id="31da1961bbfc7c4ddeb384d30515cc288fec6568" translate="yes" xml:space="preserve">
          <source>Profiler</source>
          <target state="translated">Profiler</target>
        </trans-unit>
        <trans-unit id="a445fe395c6c6d32b8f6c47c9d18f8e8ae1263f5" translate="yes" xml:space="preserve">
          <source>Profiling RPC-based Workloads</source>
          <target state="translated">剖析基于RPC的工作负载</target>
        </trans-unit>
        <trans-unit id="6f204147095385ad851c3a0b168d9ac27a7540b0" translate="yes" xml:space="preserve">
          <source>Promotion Examples:</source>
          <target state="translated">促销实例。</target>
        </trans-unit>
        <trans-unit id="d29f4bd5abe504fd6f4217c42fe5687ce1f0ffba" translate="yes" xml:space="preserve">
          <source>Propagate qconfig through the module hierarchy and assign &lt;code&gt;qconfig&lt;/code&gt; attribute on each leaf module</source>
          <target state="translated">通过模块层次结构传播qconfig并在每个叶子模块上分配 &lt;code&gt;qconfig&lt;/code&gt; 属性</target>
        </trans-unit>
        <trans-unit id="51f45d4a44ddb55152733d7d5ec938ec682205f1" translate="yes" xml:space="preserve">
          <source>Proposed by G. Hinton in his &lt;a href=&quot;https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&quot;&gt;course&lt;/a&gt;.</source>
          <target state="translated">G. Hinton在他的&lt;a href=&quot;https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&quot;&gt;课程中&lt;/a&gt;提出的建议。</target>
        </trans-unit>
        <trans-unit id="17a62537e52031c55b875c513ecbb2c241853032" translate="yes" xml:space="preserve">
          <source>Protocol Scenarios</source>
          <target state="translated">协议方案</target>
        </trans-unit>
        <trans-unit id="7a477b80f752e81ae51a71148e25e03d00004834" translate="yes" xml:space="preserve">
          <source>Provides a skeleton for customization requiring the overriding of methods such as &lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.apply&quot;&gt;&lt;code&gt;apply()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">提供用于进行自定义的框架，该自定义要求覆盖诸如&lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.apply&quot;&gt; &lt;code&gt;apply()&lt;/code&gt; 之类的方法&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="df76ae939d5d588eb6f5766984f7e5d52eb50216" translate="yes" xml:space="preserve">
          <source>Prune (currently unpruned) units in a tensor at random.</source>
          <target state="translated">随机修剪(当前未修剪)张量中的单位。</target>
        </trans-unit>
        <trans-unit id="368202ce04421132a9166203cfc137eb74e9e21c" translate="yes" xml:space="preserve">
          <source>Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.</source>
          <target state="translated">修剪(当前未修剪的)张量单位,将L1-norm最低的单位清零。</target>
        </trans-unit>
        <trans-unit id="5b5612215ed58f8a7a594acd4880fde95d901f0f" translate="yes" xml:space="preserve">
          <source>Prune entire (currently unpruned) channels in a tensor at random.</source>
          <target state="translated">随机修剪张量中的整个(当前未修剪的)通道。</target>
        </trans-unit>
        <trans-unit id="a802bde3db7f577abbb1b538eee51db0b51ddf30" translate="yes" xml:space="preserve">
          <source>Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.</source>
          <target state="translated">根据Ln-norm修剪张量中的整个(当前未修剪的)通道。</target>
        </trans-unit>
        <trans-unit id="2306b2b0d296e475c246073f14ea3e13b1639fc6" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by applying the pre-computed mask in &lt;code&gt;mask&lt;/code&gt;.</source>
          <target state="translated">张量对应于参数调用梅干 &lt;code&gt;name&lt;/code&gt; 在 &lt;code&gt;module&lt;/code&gt; 通过在将预先计算的掩模 &lt;code&gt;mask&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="8222739be42aac5d12f65e68e036f42e19a41197" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by applying the pre-computed mask in &lt;code&gt;mask&lt;/code&gt;. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="translated">张量对应于参数调用梅干 &lt;code&gt;name&lt;/code&gt; 在 &lt;code&gt;module&lt;/code&gt; 通过在将预先计算的掩模 &lt;code&gt;mask&lt;/code&gt; 。通过以下方式就地修改模块（并返回修改后的模块）：1）添加一个名为 &lt;code&gt;name+'_mask'&lt;/code&gt; 的命名缓冲区，该缓冲区与通过修剪方法应用于参数 &lt;code&gt;name&lt;/code&gt; 的二进制掩码相对应。2）用已修剪的版本替换参数 &lt;code&gt;name&lt;/code&gt; ，而原始（未修剪的）参数存储在名为 &lt;code&gt;name+'_orig'&lt;/code&gt; 的新参数中。</target>
        </trans-unit>
        <trans-unit id="12f145dd28935b207164778834f46d05e62d324e" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) channels along the specified &lt;code&gt;dim&lt;/code&gt; selected at random.</source>
          <target state="translated">张量对应于参数调用梅干 &lt;code&gt;name&lt;/code&gt; 在 &lt;code&gt;module&lt;/code&gt; 通过去除规定 &lt;code&gt;amount&lt;/code&gt; 的沿着指定的（当前未修剪的）信道 &lt;code&gt;dim&lt;/code&gt; 随机选择。</target>
        </trans-unit>
        <trans-unit id="1993775f18835672e6ea2ee6df5cc4b95f07d89b" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) channels along the specified &lt;code&gt;dim&lt;/code&gt; selected at random. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="translated">张量对应于参数调用梅干 &lt;code&gt;name&lt;/code&gt; 在 &lt;code&gt;module&lt;/code&gt; 通过去除规定 &lt;code&gt;amount&lt;/code&gt; 的沿着指定的（当前未修剪的）信道 &lt;code&gt;dim&lt;/code&gt; 随机选择。通过以下方式就地修改模块（并返回修改后的模块）：1）添加一个名为 &lt;code&gt;name+'_mask'&lt;/code&gt; 的命名缓冲区，该缓冲区与通过修剪方法应用于参数 &lt;code&gt;name&lt;/code&gt; 的二进制掩码相对应。2）用已修剪的版本替换参数 &lt;code&gt;name&lt;/code&gt; ，而原始（未修剪的）参数存储在名为 &lt;code&gt;name+'_orig'&lt;/code&gt; 的新参数中。</target>
        </trans-unit>
        <trans-unit id="e4cf8aaa8979161ded3d9362964d7e1da3ab5bd2" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) channels along the specified &lt;code&gt;dim&lt;/code&gt; with the lowest L``n``-norm.</source>
          <target state="translated">张量对应于参数调用梅干 &lt;code&gt;name&lt;/code&gt; 在 &lt;code&gt;module&lt;/code&gt; 通过去除规定 &lt;code&gt;amount&lt;/code&gt; 的沿着指定的（当前未修剪的）信道 &lt;code&gt;dim&lt;/code&gt; 具有最低L``n``范数。</target>
        </trans-unit>
        <trans-unit id="e5e671a44c6d84c3fbfdc3856808100dbb577dde" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) channels along the specified &lt;code&gt;dim&lt;/code&gt; with the lowest L``n``-norm. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="translated">张量对应于参数调用梅干 &lt;code&gt;name&lt;/code&gt; 在 &lt;code&gt;module&lt;/code&gt; 通过去除规定 &lt;code&gt;amount&lt;/code&gt; 的沿着指定的（当前未修剪的）信道 &lt;code&gt;dim&lt;/code&gt; 具有最低L``n``范数。通过以下方式就地修改模块（并返回修改后的模块）：1）添加一个名为 &lt;code&gt;name+'_mask'&lt;/code&gt; 的命名缓冲区，该缓冲区与通过修剪方法应用于参数 &lt;code&gt;name&lt;/code&gt; 的二进制掩码相对应。2）用已修剪的版本替换参数 &lt;code&gt;name&lt;/code&gt; ，而原始（未修剪的）参数存储在名为 &lt;code&gt;name+'_orig'&lt;/code&gt; 的新参数中。</target>
        </trans-unit>
        <trans-unit id="e414308272708bb6595a21b037cefd9b491135bf" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) units selected at random.</source>
          <target state="translated">梅干张量对应于参数调用 &lt;code&gt;name&lt;/code&gt; 在 &lt;code&gt;module&lt;/code&gt; 通过去除规定 &lt;code&gt;amount&lt;/code&gt; 的（当前未修剪）单元随机选择。</target>
        </trans-unit>
        <trans-unit id="0d1f0e7025ef360a1a8db642ffde843b0c686100" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) units selected at random. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="translated">梅干张量对应于参数调用 &lt;code&gt;name&lt;/code&gt; 在 &lt;code&gt;module&lt;/code&gt; 通过去除规定 &lt;code&gt;amount&lt;/code&gt; 的（当前未修剪）单元随机选择。通过以下方式就地修改模块（并返回修改后的模块）：1）添加一个名为 &lt;code&gt;name+'_mask'&lt;/code&gt; 的命名缓冲区，该缓冲区与通过修剪方法应用于参数 &lt;code&gt;name&lt;/code&gt; 的二进制掩码相对应。2）用已修剪的版本替换参数 &lt;code&gt;name&lt;/code&gt; ，而原始（未修剪的）参数存储在名为 &lt;code&gt;name+'_orig'&lt;/code&gt; 的新参数中。</target>
        </trans-unit>
        <trans-unit id="0e2ae92cb48940de59dcf3dcc3eee3fc2761f74b" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) units with the lowest L1-norm.</source>
          <target state="translated">梅干张量对应于参数调用 &lt;code&gt;name&lt;/code&gt; 在 &lt;code&gt;module&lt;/code&gt; 通过去除规定 &lt;code&gt;amount&lt;/code&gt; 的（当前未修剪）单元具有最低L1范数。</target>
        </trans-unit>
        <trans-unit id="3ceb680b02bb9fc923a4db9b23afd99bcf23bd57" translate="yes" xml:space="preserve">
          <source>Prunes tensor corresponding to parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; by removing the specified &lt;code&gt;amount&lt;/code&gt; of (currently unpruned) units with the lowest L1-norm. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called &lt;code&gt;name+'_mask'&lt;/code&gt; corresponding to the binary mask applied to the parameter &lt;code&gt;name&lt;/code&gt; by the pruning method. 2) replacing the parameter &lt;code&gt;name&lt;/code&gt; by its pruned version, while the original (unpruned) parameter is stored in a new parameter named &lt;code&gt;name+'_orig'&lt;/code&gt;.</source>
          <target state="translated">梅干张量对应于参数调用 &lt;code&gt;name&lt;/code&gt; 在 &lt;code&gt;module&lt;/code&gt; 通过去除规定 &lt;code&gt;amount&lt;/code&gt; 的（当前未修剪）单元具有最低L1范数。通过以下方式就地修改模块（并返回修改后的模块）：1）添加一个名为 &lt;code&gt;name+'_mask'&lt;/code&gt; 的命名缓冲区，该缓冲区与通过修剪方法应用于参数 &lt;code&gt;name&lt;/code&gt; 的二进制掩码相对应。2）用已修剪的版本替换参数 &lt;code&gt;name&lt;/code&gt; ，而原始（未修剪的）参数存储在名为 &lt;code&gt;name+'_orig'&lt;/code&gt; 的新参数中。</target>
        </trans-unit>
        <trans-unit id="8ee04a977907bca0b936aebe6227e9be8b0a5084" translate="yes" xml:space="preserve">
          <source>Pruning itself is NOT undone or reversed!</source>
          <target state="translated">修剪本身是不能撤销或逆转的!</target>
        </trans-unit>
        <trans-unit id="fae06ff3f9da62952a413ca3bf10b3455201b027" translate="yes" xml:space="preserve">
          <source>PruningContainer</source>
          <target state="translated">PruningContainer</target>
        </trans-unit>
        <trans-unit id="784e725830e98a189da61d3afccea45b6e5a9d0f" translate="yes" xml:space="preserve">
          <source>Publishing models</source>
          <target state="translated">出版模式</target>
        </trans-unit>
        <trans-unit id="ae8ea7cada42ba595cd41ec28d2ff26795c6d771" translate="yes" xml:space="preserve">
          <source>Pushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.</source>
          <target state="translated">将一个范围推到嵌套的范围跨度堆栈上。返回被启动的范围的基于零的深度。</target>
        </trans-unit>
        <trans-unit id="256db3aa08282f6add3cb3226d3554daf2bee937" translate="yes" xml:space="preserve">
          <source>Puts values from the tensor &lt;code&gt;value&lt;/code&gt; into the tensor &lt;code&gt;self&lt;/code&gt; using the indices specified in &lt;a href=&quot;#torch.Tensor.indices&quot;&gt;&lt;code&gt;indices&lt;/code&gt;&lt;/a&gt; (which is a tuple of Tensors). The expression &lt;code&gt;tensor.index_put_(indices, value)&lt;/code&gt; is equivalent to &lt;code&gt;tensor[indices] = value&lt;/code&gt;. Returns &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">使用索引中指定的&lt;a href=&quot;#torch.Tensor.indices&quot;&gt; &lt;code&gt;indices&lt;/code&gt; &lt;/a&gt;（张量的元组）将张 &lt;code&gt;value&lt;/code&gt; 放入张量 &lt;code&gt;self&lt;/code&gt; 。表达式 &lt;code&gt;tensor.index_put_(indices, value)&lt;/code&gt; 等效于 &lt;code&gt;tensor[indices] = value&lt;/code&gt; 。返回 &lt;code&gt;self&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c148aded3d47eba6bad059e92b2038d207b0a750" translate="yes" xml:space="preserve">
          <source>Putting it all together</source>
          <target state="translated">把它整合在一起</target>
        </trans-unit>
        <trans-unit id="98a2e30eeb85238d061519d74935e170e44c72cb" translate="yes" xml:space="preserve">
          <source>PyTorch</source>
          <target state="translated">PyTorch</target>
        </trans-unit>
        <trans-unit id="2837c35a89ca2ff1375acbd6ae4015154d808597" translate="yes" xml:space="preserve">
          <source>PyTorch Contribution Guide</source>
          <target state="translated">PyTorch 贡献指南</target>
        </trans-unit>
        <trans-unit id="0de6ec530c1988d0af9e4ce7eb73eef1189976b6" translate="yes" xml:space="preserve">
          <source>PyTorch Functions and Modules</source>
          <target state="translated">PyTorch 函数和模块</target>
        </trans-unit>
        <trans-unit id="6517675a341634e77bfecd3ddf631a647d857475" translate="yes" xml:space="preserve">
          <source>PyTorch Governance</source>
          <target state="translated">PyTorch治理</target>
        </trans-unit>
        <trans-unit id="d35b21e83a6b1a4ee2adc066dff9672eedab98a1" translate="yes" xml:space="preserve">
          <source>PyTorch Governance | Persons of Interest</source>
          <target state="translated">PyTorch 治理|利益相关者</target>
        </trans-unit>
        <trans-unit id="3a2bd7a8f704e6cdbe374466d0208d799f0651cc" translate="yes" xml:space="preserve">
          <source>PyTorch and ONNX backends(Caffe2, ONNX Runtime, etc) often have implementations of operators with some numeric differences. Depending on model structure, these differences may be negligible, but they can also cause major divergences in behavior (especially on untrained models.) We allow Caffe2 to call directly to Torch implementations of operators, to help you smooth over these differences when precision is important, and to also document these differences.</source>
          <target state="translated">PyTorch 和 ONNX 后端(Caffe2、ONNX Runtime 等)的运算符实现往往存在一些数值差异。根据模型结构的不同,这些差异可能可以忽略不计,但也会导致行为上的重大分歧(尤其是在未经训练的模型上)。 我们允许 Caffe2 直接调用 Torch 的运算符实现,以帮助您在精度很重要的情况下平滑这些差异,并记录这些差异。</target>
        </trans-unit>
        <trans-unit id="6148268ed386643e6b3a594818c58fc07ad9c665" translate="yes" xml:space="preserve">
          <source>PyTorch distributed package supports Linux (stable), MacOS (stable), and Windows (prototype). By default for Linux, the Gloo and NCCL backends are built and included in PyTorch distributed (NCCL only when building with CUDA). MPI is an optional backend that can only be included if you build PyTorch from source. (e.g.building PyTorch on a host that has MPI installed.)</source>
          <target state="translated">PyTorch 分布式软件包支持 Linux(稳定版)、MacOS(稳定版)和 Windows(原型)。对于 Linux,默认情况下,Gloo 和 NCCL 后端已构建并包含在 PyTorch 分布式包中(NCCL 仅在使用 CUDA 构建时提供)。MPI 是一个可选的后端,只有在您从源码中构建 PyTorch 时才会包含。(例如,在安装了 MPI 的主机上构建 PyTorch。)</target>
        </trans-unit>
        <trans-unit id="9456a74dbb38bfce73f52a1992187228bd2d00fa" translate="yes" xml:space="preserve">
          <source>PyTorch documentation</source>
          <target state="translated">PyTorch文档</target>
        </trans-unit>
        <trans-unit id="d19d474346c5b08d13d3339e3bf4feebcd91f549" translate="yes" xml:space="preserve">
          <source>PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.</source>
          <target state="translated">PyTorch是一个优化的张量库,用于使用GPU和CPU进行深度学习。</target>
        </trans-unit>
        <trans-unit id="899246dc869a567118284070768757e2fc424c84" translate="yes" xml:space="preserve">
          <source>PyTorch on XLA Devices</source>
          <target state="translated">XLA 设备上的 PyTorch</target>
        </trans-unit>
        <trans-unit id="c93898f1a89d832ed7049068dfa7cb16c03bf3ed" translate="yes" xml:space="preserve">
          <source>PyTorch preserves storage sharing across serialization. See &lt;code&gt;preserve-storage-sharing&lt;/code&gt; for more details.</source>
          <target state="translated">PyTorch保留序列化过程中的存储共享。有关更多详细信息，请参见 &lt;code&gt;preserve-storage-sharing&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="27b7e9176c7a6b31ba4779baa5f868bead4ef214" translate="yes" xml:space="preserve">
          <source>PyTorch provides two global &lt;a href=&quot;#torch.distributions.constraint_registry.ConstraintRegistry&quot;&gt;&lt;code&gt;ConstraintRegistry&lt;/code&gt;&lt;/a&gt; objects that link &lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt;&lt;code&gt;Constraint&lt;/code&gt;&lt;/a&gt; objects to &lt;a href=&quot;#torch.distributions.transforms.Transform&quot;&gt;&lt;code&gt;Transform&lt;/code&gt;&lt;/a&gt; objects. These objects both input constraints and return transforms, but they have different guarantees on bijectivity.</source>
          <target state="translated">PyTorch提供了两个全球&lt;a href=&quot;#torch.distributions.constraint_registry.ConstraintRegistry&quot;&gt; &lt;code&gt;ConstraintRegistry&lt;/code&gt; &lt;/a&gt;该链接的对象&lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt; &lt;code&gt;Constraint&lt;/code&gt; &lt;/a&gt;对象来&lt;a href=&quot;#torch.distributions.transforms.Transform&quot;&gt; &lt;code&gt;Transform&lt;/code&gt; &lt;/a&gt;对象。这些对象既有输入约束又有返回变换，但对双射性有不同的保证。</target>
        </trans-unit>
        <trans-unit id="ccb71e60b710059778503566d9b6733540438582" translate="yes" xml:space="preserve">
          <source>PyTorch ships with two builtin backends: &lt;code&gt;BackendType.TENSORPIPE&lt;/code&gt; and &lt;code&gt;BackendType.PROCESS_GROUP&lt;/code&gt;. Additional ones can be registered using the &lt;code&gt;register_backend()&lt;/code&gt; function.</source>
          <target state="translated">PyTorch附带了两个内置后端： &lt;code&gt;BackendType.TENSORPIPE&lt;/code&gt; 和 &lt;code&gt;BackendType.PROCESS_GROUP&lt;/code&gt; 。可以使用 &lt;code&gt;register_backend()&lt;/code&gt; 函数注册其他对象。</target>
        </trans-unit>
        <trans-unit id="2d5a1e0600b8abe6782e41ec54cae91372de9186" translate="yes" xml:space="preserve">
          <source>Python 2 does not support Ellipsis but one may use a string literal instead (&lt;code&gt;'...'&lt;/code&gt;).</source>
          <target state="translated">Python 2不支持Ellipsis，但是可以使用字符串文字代替（ &lt;code&gt;'...'&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="b5ec6c002217ad80d40ef02a1f4366196fae59b7" translate="yes" xml:space="preserve">
          <source>Python 3 type hints can be used in place of &lt;code&gt;torch.jit.annotate&lt;/code&gt;</source>
          <target state="translated">可以使用Python 3类型提示代替 &lt;code&gt;torch.jit.annotate&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="0711b02500c16372db7e037b0db241051fe28013" translate="yes" xml:space="preserve">
          <source>Python API</source>
          <target state="translated">蟒蛇API</target>
        </trans-unit>
        <trans-unit id="cad6fbd004ed44f2a0389e7ad1a4cfa89ba1d3fc" translate="yes" xml:space="preserve">
          <source>Python Functions and Modules</source>
          <target state="translated">Python函数和模块</target>
        </trans-unit>
        <trans-unit id="c9209054e624163b69a9eb976a32495c0a33865f" translate="yes" xml:space="preserve">
          <source>Python Language Reference Comparison</source>
          <target state="translated">Python语言参考比较</target>
        </trans-unit>
        <trans-unit id="e4ec2cbb65019f4837950da585dfbc7781b88d91" translate="yes" xml:space="preserve">
          <source>Python Language Reference Coverage</source>
          <target state="translated">Python语言参考范围</target>
        </trans-unit>
        <trans-unit id="fd58a092a5203baf1b73956b039089eaeff86afb" translate="yes" xml:space="preserve">
          <source>Python classes can be used in TorchScript if they are annotated with &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt;, similar to how you would declare a TorchScript function:</source>
          <target state="translated">如果Python类使用&lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;@torch.jit.script&lt;/code&gt; &lt;/a&gt;进行注释，则可以在TorchScript中使用它们，类似于声明TorchScript函数的方式：</target>
        </trans-unit>
        <trans-unit id="dcb69ff04adf584813a764fcff415e81178eed67" translate="yes" xml:space="preserve">
          <source>Python enums can be used in TorchScript without any extra annotation or code:</source>
          <target state="translated">Python枚举可以在TorchScript中使用,不需要任何额外的注释或代码。</target>
        </trans-unit>
        <trans-unit id="21aa60d14a5d3bf9941c64223102e72e43cd7db3" translate="yes" xml:space="preserve">
          <source>Python-defined Constants</source>
          <target state="translated">Python定义的常量</target>
        </trans-unit>
        <trans-unit id="bca513fe1a1ac6b508002e4d70e9b04a95755456" translate="yes" xml:space="preserve">
          <source>Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility.</source>
          <target state="translated">Pytorch Hub是一个预先训练的模型库,旨在促进研究的可重复性。</target>
        </trans-unit>
        <trans-unit id="4a3b71fd725a5570aa333bd5a258f51e9ad067c8" translate="yes" xml:space="preserve">
          <source>Pytorch Hub provides convenient APIs to explore all available models in hub through &lt;a href=&quot;#torch.hub.list&quot;&gt;&lt;code&gt;torch.hub.list()&lt;/code&gt;&lt;/a&gt;, show docstring and examples through &lt;a href=&quot;#torch.hub.help&quot;&gt;&lt;code&gt;torch.hub.help()&lt;/code&gt;&lt;/a&gt; and load the pre-trained models using &lt;a href=&quot;#torch.hub.load&quot;&gt;&lt;code&gt;torch.hub.load()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Pytorch集线器提供了方便的API通过探索所有可用的模型在毂&lt;a href=&quot;#torch.hub.list&quot;&gt; &lt;code&gt;torch.hub.list()&lt;/code&gt; &lt;/a&gt;，显示文档字符串并通过实例&lt;a href=&quot;#torch.hub.help&quot;&gt; &lt;code&gt;torch.hub.help()&lt;/code&gt; &lt;/a&gt;，并使用加载预训练的模型&lt;a href=&quot;#torch.hub.load&quot;&gt; &lt;code&gt;torch.hub.load()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="71ea6e31bfc2cea7da3ec5ce3389fe77a6457276" translate="yes" xml:space="preserve">
          <source>Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights) to a github repository by adding a simple &lt;code&gt;hubconf.py&lt;/code&gt; file;</source>
          <target state="translated">Pytorch Hub支持通过添加简单的 &lt;code&gt;hubconf.py&lt;/code&gt; 文件将预训练的模型（模型定义和预训练的权重）发布到github存储库；</target>
        </trans-unit>
        <trans-unit id="c3156e00d3c2588c639e0d3cf6821258b05761c7" translate="yes" xml:space="preserve">
          <source>Q</source>
          <target state="translated">Q</target>
        </trans-unit>
        <trans-unit id="817b3643a8c9250c5615063227303b8c5da71cbe" translate="yes" xml:space="preserve">
          <source>Q: Does ONNX support implicit scalar datatype casting?</source>
          <target state="translated">问:ONNX 是否支持隐式标量数据类型铸造?</target>
        </trans-unit>
        <trans-unit id="e999540844f438b9780d0892a8883a34284bbec7" translate="yes" xml:space="preserve">
          <source>Q: How do I store attributes on a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt;?</source>
          <target state="translated">问：如何在&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;上存储属性？</target>
        </trans-unit>
        <trans-unit id="71cd3f9caa34814ac682f01c8799e67876c37c7b" translate="yes" xml:space="preserve">
          <source>Q: How to export models with loops in it?</source>
          <target state="translated">问:如何导出有循环的模型?</target>
        </trans-unit>
        <trans-unit id="3ab78ce25486eb77e84e3dce117cf425f112dd6c" translate="yes" xml:space="preserve">
          <source>Q: I have exported my lstm model, but its input size seems to be fixed?</source>
          <target state="translated">问:我导出了我的lstm模型,但是它的输入大小好像是固定的?</target>
        </trans-unit>
        <trans-unit id="a12de1b8a4f8718726ec01c0631665899c7a3116" translate="yes" xml:space="preserve">
          <source>Q: I would like to trace module&amp;rsquo;s method but I keep getting this error:</source>
          <target state="translated">问：我想跟踪模块的方法，但我一直收到此错误：</target>
        </trans-unit>
        <trans-unit id="2b00cd8e47e4d783f5be1951f1dd20a7ce719f21" translate="yes" xml:space="preserve">
          <source>Q: I would like to train a model on GPU and do inference on CPU. What are the best practices?</source>
          <target state="translated">问:我想在GPU上训练一个模型,在CPU上做推理。最佳实践是什么?</target>
        </trans-unit>
        <trans-unit id="470067c814eb1ea3d524592caed0d6e62ae92c6f" translate="yes" xml:space="preserve">
          <source>Q: Is tensor in-place indexed assignment like &lt;code&gt;data[index] = new_data&lt;/code&gt; supported?</source>
          <target state="translated">问：是否支持张量就地索引分配，例如 &lt;code&gt;data[index] = new_data&lt;/code&gt; ？</target>
        </trans-unit>
        <trans-unit id="e79884b3f980f1ad19cca2c799bbefd52769c3b3" translate="yes" xml:space="preserve">
          <source>Q: Is tensor list exportable to ONNX?</source>
          <target state="translated">问:张量列表是否可以导出到ONNX?</target>
        </trans-unit>
        <trans-unit id="892976819297325dacfe2454b5f3511112ebff7a" translate="yes" xml:space="preserve">
          <source>QFunctional</source>
          <target state="translated">QFunctional</target>
        </trans-unit>
        <trans-unit id="3a8e72c5cc093d92b3cfbbcce054a2f1c557acd3" translate="yes" xml:space="preserve">
          <source>Q_\text{max}</source>
          <target state="translated">Q_\text{max}</target>
        </trans-unit>
        <trans-unit id="d92ca2583a7c56daca5b83e5f89026fff5289864" translate="yes" xml:space="preserve">
          <source>Q_\text{min}</source>
          <target state="translated">Q_\text{min}</target>
        </trans-unit>
        <trans-unit id="97f0eded6b0e44ee7ab93339b4fa67092a5342fa" translate="yes" xml:space="preserve">
          <source>Quantization</source>
          <target state="translated">Quantization</target>
        </trans-unit>
        <trans-unit id="7b80176f26e12114d8e0a0da100ea2dd78841526" translate="yes" xml:space="preserve">
          <source>Quantization configuration should be assigned preemptively to individual submodules in &lt;code&gt;.qconfig&lt;/code&gt; attribute.</source>
          <target state="translated">量化配置应优先分配给 &lt;code&gt;.qconfig&lt;/code&gt; 属性中的各个子模块。</target>
        </trans-unit>
        <trans-unit id="ac5d2b1d677d49a0bd86ef0862a7927fd9191c9e" translate="yes" xml:space="preserve">
          <source>Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to the &lt;a href=&quot;quantization#quantization-doc&quot;&gt;Quantization&lt;/a&gt; documentation.</source>
          <target state="translated">量化是指用于执行计算并以低于浮点精度的位宽存储张量的技术。PyTorch支持每个张量和每个通道非对称线性量化。要了解更多如何在PyTorch中使用量化函数的信息，请参阅&lt;a href=&quot;quantization#quantization-doc&quot;&gt;量化&lt;/a&gt;文档。</target>
        </trans-unit>
        <trans-unit id="18a85a5632065b99cfe45a2efad2f1ae99da1046" translate="yes" xml:space="preserve">
          <source>Quantize</source>
          <target state="translated">Quantize</target>
        </trans-unit>
        <trans-unit id="f8fde939a75393ebe92d80b644e2d353e60f89e3" translate="yes" xml:space="preserve">
          <source>Quantize stub module, before calibration, this is same as an observer, it will be swapped as &lt;code&gt;nnq.Quantize&lt;/code&gt; in &lt;code&gt;convert&lt;/code&gt;.</source>
          <target state="translated">量化存根模块，在校准之前，它与观察者相同，它将被 &lt;code&gt;convert&lt;/code&gt; 为 &lt;code&gt;nnq.Quantize&lt;/code&gt; 在convert中。</target>
        </trans-unit>
        <trans-unit id="567337a1768d995cea83bd61b738d0b3fde9158e" translate="yes" xml:space="preserve">
          <source>Quantize the input float model with post training static quantization.</source>
          <target state="translated">对输入的浮动模型进行量化,训练后静态量化。</target>
        </trans-unit>
        <trans-unit id="5dc78d5962844144dbb78dcd34c444f89d2360d5" translate="yes" xml:space="preserve">
          <source>Quantized Functions</source>
          <target state="translated">量化函数</target>
        </trans-unit>
        <trans-unit id="93bd0dbffb4b3754bf685731ec3211818537c4b4" translate="yes" xml:space="preserve">
          <source>Quantized model.</source>
          <target state="translated">量化模型。</target>
        </trans-unit>
        <trans-unit id="22040c5f1ace6596f37ed53e5feee71111db31d0" translate="yes" xml:space="preserve">
          <source>Quantizes an incoming tensor</source>
          <target state="translated">量化一个传入的张量</target>
        </trans-unit>
        <trans-unit id="7ad43145efa235468f5902d61cd285ed9b73e61e" translate="yes" xml:space="preserve">
          <source>Quasi-random sampling</source>
          <target state="translated">半随机抽样</target>
        </trans-unit>
        <trans-unit id="06576556d1ad802f247cad11ae748be47b70cd9c" translate="yes" xml:space="preserve">
          <source>R</source>
          <target state="translated">R</target>
        </trans-unit>
        <trans-unit id="a4a12b6d13143948a19488e0d1cd0864bcfcd487" translate="yes" xml:space="preserve">
          <source>R(2+1)D-18 network</source>
          <target state="translated">R(2+1)D-18网络</target>
        </trans-unit>
        <trans-unit id="6d51f7562aadcec1a3d0cd7c60d6435cbf2ea1a0" translate="yes" xml:space="preserve">
          <source>R3D-18 network</source>
          <target state="translated">R3D-18网络</target>
        </trans-unit>
        <trans-unit id="c5db7969dcd30635e5d7867040b6cc76158dd175" translate="yes" xml:space="preserve">
          <source>RAW</source>
          <target state="translated">RAW</target>
        </trans-unit>
        <trans-unit id="2fe8814f679602ef8677e9b846ae9cc63646e90c" translate="yes" xml:space="preserve">
          <source>RNN</source>
          <target state="translated">RNN</target>
        </trans-unit>
        <trans-unit id="419fa7ef1354a7471cfefa40385138f81850a613" translate="yes" xml:space="preserve">
          <source>RNNBase</source>
          <target state="translated">RNNBase</target>
        </trans-unit>
        <trans-unit id="22c95097b5426fe43997d6eb11be7ab8c5097b78" translate="yes" xml:space="preserve">
          <source>RNNCell</source>
          <target state="translated">RNNCell</target>
        </trans-unit>
        <trans-unit id="c3282cbbcba660116d62c007822229220838a1c6" translate="yes" xml:space="preserve">
          <source>RPC</source>
          <target state="translated">RPC</target>
        </trans-unit>
        <trans-unit id="b75c8242fd289be2b08bd6f1908eeb3affe74a47" translate="yes" xml:space="preserve">
          <source>RReLU</source>
          <target state="translated">RReLU</target>
        </trans-unit>
        <trans-unit id="e908d326bea7dd110e9821c1f738fe31ee60caf9" translate="yes" xml:space="preserve">
          <source>RRef</source>
          <target state="translated">RRef</target>
        </trans-unit>
        <trans-unit id="e98cb22d66aa1860c5d982443eb2d7e84c9630f1" translate="yes" xml:space="preserve">
          <source>RRef Lifetime</source>
          <target state="translated">RRef Lifetime</target>
        </trans-unit>
        <trans-unit id="c36d9458e0d1baae7403b5ce642a78b09e4acc7f" translate="yes" xml:space="preserve">
          <source>Raises</source>
          <target state="translated">Raises</target>
        </trans-unit>
        <trans-unit id="d92af57a28c421f8d8438229f0b21202604e4ae3" translate="yes" xml:space="preserve">
          <source>Raises &lt;code&gt;RuntimeError&lt;/code&gt; if &lt;a href=&quot;https://ninja-build.org/&quot;&gt;ninja&lt;/a&gt; build system is not available on the system, does nothing otherwise.</source>
          <target state="translated">如果&lt;a href=&quot;https://ninja-build.org/&quot;&gt;忍者&lt;/a&gt;构建系统在系统上不可用，则引发 &lt;code&gt;RuntimeError&lt;/code&gt; ，否则不执行任何操作。</target>
        </trans-unit>
        <trans-unit id="fe6245a65da1996031b65e42c6ba6186170ca87f" translate="yes" xml:space="preserve">
          <source>Raises ValueError if the value is not present.</source>
          <target state="translated">如果该值不存在,则引发ValueError。</target>
        </trans-unit>
        <trans-unit id="eef14c4912a222577af3c87a94916617431f0107" translate="yes" xml:space="preserve">
          <source>Random Number Generator</source>
          <target state="translated">随机数生成器</target>
        </trans-unit>
        <trans-unit id="aedcea91bbd64d179e04e09799d9cf9271d320c0" translate="yes" xml:space="preserve">
          <source>Random sampling</source>
          <target state="translated">随机抽样</target>
        </trans-unit>
        <trans-unit id="4a94ba04a9499be153f8aa33a992c875d347dc19" translate="yes" xml:space="preserve">
          <source>Random sampling creation ops are listed under &lt;a href=&quot;#random-sampling&quot;&gt;Random sampling&lt;/a&gt; and include: &lt;a href=&quot;generated/torch.rand#torch.rand&quot;&gt;&lt;code&gt;torch.rand()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;generated/torch.rand_like#torch.rand_like&quot;&gt;&lt;code&gt;torch.rand_like()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;generated/torch.randn#torch.randn&quot;&gt;&lt;code&gt;torch.randn()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;generated/torch.randn_like#torch.randn_like&quot;&gt;&lt;code&gt;torch.randn_like()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;generated/torch.randint#torch.randint&quot;&gt;&lt;code&gt;torch.randint()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;generated/torch.randint_like#torch.randint_like&quot;&gt;&lt;code&gt;torch.randint_like()&lt;/code&gt;&lt;/a&gt;&lt;a href=&quot;generated/torch.randperm#torch.randperm&quot;&gt;&lt;code&gt;torch.randperm()&lt;/code&gt;&lt;/a&gt; You may also use &lt;a href=&quot;generated/torch.empty#torch.empty&quot;&gt;&lt;code&gt;torch.empty()&lt;/code&gt;&lt;/a&gt; with the &lt;a href=&quot;#inplace-random-sampling&quot;&gt;In-place random sampling&lt;/a&gt; methods to create &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; s with values sampled from a broader range of distributions.</source>
          <target state="translated">随机抽样创建OPS被列在&lt;a href=&quot;#random-sampling&quot;&gt;随机抽样&lt;/a&gt;，包括：&lt;a href=&quot;generated/torch.rand#torch.rand&quot;&gt; &lt;code&gt;torch.rand()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;generated/torch.rand_like#torch.rand_like&quot;&gt; &lt;code&gt;torch.rand_like()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;generated/torch.randn#torch.randn&quot;&gt; &lt;code&gt;torch.randn()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;generated/torch.randn_like#torch.randn_like&quot;&gt; &lt;code&gt;torch.randn_like()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;generated/torch.randint#torch.randint&quot;&gt; &lt;code&gt;torch.randint()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;generated/torch.randint_like#torch.randint_like&quot;&gt; &lt;code&gt;torch.randint_like()&lt;/code&gt; &lt;/a&gt;&lt;a href=&quot;generated/torch.randperm#torch.randperm&quot;&gt; &lt;code&gt;torch.randperm()&lt;/code&gt; &lt;/a&gt;您可能还将&lt;a href=&quot;generated/torch.empty#torch.empty&quot;&gt; &lt;code&gt;torch.empty()&lt;/code&gt; &lt;/a&gt;与&lt;a href=&quot;#inplace-random-sampling&quot;&gt;就地随机采样&lt;/a&gt;方法一起使用，以从更广泛的分布范围中采样的值创建&lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="1033bbebd194535f79c8257b5bf0911dcfc67f5b" translate="yes" xml:space="preserve">
          <source>RandomStructured</source>
          <target state="translated">RandomStructured</target>
        </trans-unit>
        <trans-unit id="1bc526966e9f4a0fc0dc8703f5e03b9b477135e9" translate="yes" xml:space="preserve">
          <source>RandomUnstructured</source>
          <target state="translated">RandomUnstructured</target>
        </trans-unit>
        <trans-unit id="464ae7dc9e362bdfac200fc767cbef5d1f028f9c" translate="yes" xml:space="preserve">
          <source>Randomized leaky ReLU.</source>
          <target state="translated">随机泄漏的ReLU。</target>
        </trans-unit>
        <trans-unit id="9e67c8ff6a6fe1b402c9707f9ca4025ad7e992ff" translate="yes" xml:space="preserve">
          <source>Randomly masks out entire channels (a channel is a feature map, e.g. the</source>
          <target state="translated">随机屏蔽整个通道(一个通道是一个特征图,如</target>
        </trans-unit>
        <trans-unit id="6b76ae477338397aa80a17779eb1a87460d96c9e" translate="yes" xml:space="preserve">
          <source>Randomly split a dataset into non-overlapping new datasets of given lengths. Optionally fix the generator for reproducible results, e.g.:</source>
          <target state="translated">将一个数据集随机分割成给定长度的非重叠的新数据集。可选择固定生成器,以获得可重复的结果,例如:。</target>
        </trans-unit>
        <trans-unit id="f5ee3860ad6da586f04541bda24f379685849499" translate="yes" xml:space="preserve">
          <source>Randomly zero out entire channels (a channel is a 2D feature map, e.g., the</source>
          <target state="translated">随机清零整个通道(一个通道是一个二维特征图,如</target>
        </trans-unit>
        <trans-unit id="d25942032b09a691f7d96817937bb37b8fc61cbd" translate="yes" xml:space="preserve">
          <source>Randomly zero out entire channels (a channel is a 3D feature map, e.g., the</source>
          <target state="translated">随机清零整个通道(一个通道是一个3D特征图,如</target>
        </trans-unit>
        <trans-unit id="1e57b50527a8568c0445a4ecc1addecd8c4a65f8" translate="yes" xml:space="preserve">
          <source>Randomness in multi-process data loading</source>
          <target state="translated">多进程数据加载中的随机性</target>
        </trans-unit>
        <trans-unit id="e887e4a3161eab98177808f95ed573db9bd91832" translate="yes" xml:space="preserve">
          <source>Rank is a unique identifier assigned to each process within a distributed process group. They are always consecutive integers ranging from 0 to &lt;code&gt;world_size&lt;/code&gt;.</source>
          <target state="translated">等级是分配给分布式过程组中每个过程的唯一标识符。它们始终是从0到 &lt;code&gt;world_size&lt;/code&gt; 的连续整数。</target>
        </trans-unit>
        <trans-unit id="db193220cd72bbc96216458a492d5965637b4287" translate="yes" xml:space="preserve">
          <source>Rather, this directly calls the underlying LAPACK function &lt;code&gt;?geqrf&lt;/code&gt; which produces a sequence of &amp;lsquo;elementary reflectors&amp;rsquo;.</source>
          <target state="translated">而是直接调用底层的LAPACK函数 &lt;code&gt;?geqrf&lt;/code&gt; ，该函数生成一系列&amp;ldquo;基本反射器&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="5a864fcb6762d236bc276a85137cd0fcd44b3001" translate="yes" xml:space="preserve">
          <source>ReLU</source>
          <target state="translated">ReLU</target>
        </trans-unit>
        <trans-unit id="5290128df38d58d4be3bdda2dec90bacff114b1c" translate="yes" xml:space="preserve">
          <source>ReLU6</source>
          <target state="translated">ReLU6</target>
        </trans-unit>
        <trans-unit id="3294ad6b1eda298355d27264864a84bb153e4e5b" translate="yes" xml:space="preserve">
          <source>Real values are finite when they are not NaN, negative infinity, or infinity. Complex values are finite when both their real and imaginary parts are finite.</source>
          <target state="translated">当实值不是NaN、负无穷或无穷大时,它们是有限的。复值是有限的,当它们的实部和虚部都是有限的。</target>
        </trans-unit>
        <trans-unit id="6bf7f7c605a0ed647c478d22e23c8b7c103aaf78" translate="yes" xml:space="preserve">
          <source>Real-to-complex Discrete Fourier Transform.</source>
          <target state="translated">实到复杂的离散傅立叶变换。</target>
        </trans-unit>
        <trans-unit id="4cb84ee223d1d7f38f89544406d3d898f06cdafb" translate="yes" xml:space="preserve">
          <source>Rearranges elements in a tensor of shape</source>
          <target state="translated">将形状张量中的元素重新排列。</target>
        </trans-unit>
        <trans-unit id="d4d632c1063de7870f16352716202b1f2a8c81c1" translate="yes" xml:space="preserve">
          <source>Receives a tensor asynchronously.</source>
          <target state="translated">异步接收张量。</target>
        </trans-unit>
        <trans-unit id="2f89320a075e26566ba50731edf2f47338c4432f" translate="yes" xml:space="preserve">
          <source>Receives a tensor synchronously.</source>
          <target state="translated">同步接收张量。</target>
        </trans-unit>
        <trans-unit id="d3031eb6413ee9483cf91490238bc6c5cdb0c9c8" translate="yes" xml:space="preserve">
          <source>Reconstruct an event from an IPC handle on the given device.</source>
          <target state="translated">从给定设备上的IPC句柄重构一个事件。</target>
        </trans-unit>
        <trans-unit id="9f19550117b40c6d2db06a55311fcd7ca6f091f5" translate="yes" xml:space="preserve">
          <source>Recorded event.</source>
          <target state="translated">记录的事件:</target>
        </trans-unit>
        <trans-unit id="16d8e6844c3490f438ed2b3c2bf4939db0fa6454" translate="yes" xml:space="preserve">
          <source>Records an event.</source>
          <target state="translated">记录一个事件。</target>
        </trans-unit>
        <trans-unit id="8c8da4978a756400f2a253bb3ea49d3f8d06e10d" translate="yes" xml:space="preserve">
          <source>Records operation history and defines formulas for differentiating ops.</source>
          <target state="translated">记录操作历史并定义区分操作的公式。</target>
        </trans-unit>
        <trans-unit id="8417f5f78b257e51c68ac1ce329dbe8dc58722b1" translate="yes" xml:space="preserve">
          <source>Records the event in a given stream.</source>
          <target state="translated">在给定的流中记录该事件。</target>
        </trans-unit>
        <trans-unit id="6d3b40b37dfe8fdad50303f26863dfa5944ffb83" translate="yes" xml:space="preserve">
          <source>Recurrent Layers</source>
          <target state="translated">循环层</target>
        </trans-unit>
        <trans-unit id="437a7c4fe39d00898697f5eea2075a348d8b6eda" translate="yes" xml:space="preserve">
          <source>Reduce and scatter a list of tensors to the whole group. Only nccl backend is currently supported.</source>
          <target state="translated">将一个时序列表缩减并分散到整个组中。目前只支持nccl后端。</target>
        </trans-unit>
        <trans-unit id="62a542ed033585ebe95fd9e187a5f08c926cf3da" translate="yes" xml:space="preserve">
          <source>Reduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This scheduler reads a metrics quantity and if no improvement is seen for a &amp;lsquo;patience&amp;rsquo; number of epochs, the learning rate is reduced.</source>
          <target state="translated">当指标停止改进时，降低学习率。一旦学习停滞，模型通常会受益于将学习率降低2到10倍。该调度程序读取度量标准数量，如果对于&amp;ldquo;耐心&amp;rdquo;的时期没有看到改善，则学习率会降低。</target>
        </trans-unit>
        <trans-unit id="2e154454ad02bd98ccb4e04b626318e2ca6ecea3" translate="yes" xml:space="preserve">
          <source>Reduces the tensor data across all machines in such a way that all get the final result.</source>
          <target state="translated">将所有机器的张量数据以这样的方式减少,使所有机器都得到最终结果。</target>
        </trans-unit>
        <trans-unit id="0845d8a3b333d7e6bdeae11a45e93d821e227572" translate="yes" xml:space="preserve">
          <source>Reduces the tensor data across all machines in such a way that all get the final result. This function reduces a number of tensors on every node, while each tensor resides on different GPUs. Therefore, the input tensor in the tensor list needs to be GPU tensors. Also, each tensor in the tensor list needs to reside on a different GPU.</source>
          <target state="translated">减少所有机器上的张量数据,使所有机器都得到最终结果。这个函数在每个节点上都会减少一些张量,而每个张量驻留在不同的GPU上。因此,张量列表中输入的张量需要是GPU的张量。同时,张量列表中的每个张量需要驻留在不同的GPU上。</target>
        </trans-unit>
        <trans-unit id="68d09fcde5e8e7a3c4ca4ab69d90b3bd77b87c7d" translate="yes" xml:space="preserve">
          <source>Reduces the tensor data across all machines.</source>
          <target state="translated">减少所有机器的张量数据。</target>
        </trans-unit>
        <trans-unit id="c6916d4b854bb48bab1b470966a0bd89b2429552" translate="yes" xml:space="preserve">
          <source>Reduces the tensor data on multiple GPUs across all machines. Each tensor in &lt;code&gt;tensor_list&lt;/code&gt; should reside on a separate GPU</source>
          <target state="translated">减少所有计算机上多个GPU上的张量数据。 &lt;code&gt;tensor_list&lt;/code&gt; 中的每个张量应驻留在单独的GPU上</target>
        </trans-unit>
        <trans-unit id="72826c34e0b98307364b7bcd04a0655be2f7559a" translate="yes" xml:space="preserve">
          <source>Reduces, then scatters a list of tensors to all processes in a group.</source>
          <target state="translated">减少,然后将一个时序列表分散到一个组中的所有进程。</target>
        </trans-unit>
        <trans-unit id="44eef8c0a66c5ce295026ad03e889f17e08ed29c" translate="yes" xml:space="preserve">
          <source>Reducing with the addition operation is the same as using &lt;a href=&quot;#torch.Tensor.scatter_add_&quot;&gt;&lt;code&gt;scatter_add_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">使用加法操作进行缩减与使用&lt;a href=&quot;#torch.Tensor.scatter_add_&quot;&gt; &lt;code&gt;scatter_add_()&lt;/code&gt; &lt;/a&gt;相同。</target>
        </trans-unit>
        <trans-unit id="1b6d087fa2831e91be3ac6de64b3da6a95b4c118" translate="yes" xml:space="preserve">
          <source>Reduction Ops</source>
          <target state="translated">减少行动</target>
        </trans-unit>
        <trans-unit id="da9dd3248fef822f41edb238f214a556d8fa5e69" translate="yes" xml:space="preserve">
          <source>Reduction is not yet implemented for the CUDA backend.</source>
          <target state="translated">CUDA后端还没有实现还原。</target>
        </trans-unit>
        <trans-unit id="45c3dc1c7731c6185824876ed514e54f71bacb64" translate="yes" xml:space="preserve">
          <source>Reference:</source>
          <target state="translated">Reference:</target>
        </trans-unit>
        <trans-unit id="5d20d0fee3b91643dd8d272ac33d01ca95179d82" translate="yes" xml:space="preserve">
          <source>References</source>
          <target state="translated">References</target>
        </trans-unit>
        <trans-unit id="9d1e4e7d27b519b1da3d7266c9c87d7861741080" translate="yes" xml:space="preserve">
          <source>References:</source>
          <target state="translated">References:</target>
        </trans-unit>
        <trans-unit id="9368e2ec97261508237a4befedcf1e3dc15edd5f" translate="yes" xml:space="preserve">
          <source>References::</source>
          <target state="translated">References::</target>
        </trans-unit>
        <trans-unit id="ba0ffdee70f599751e1059c2737dfaa0253ed5bb" translate="yes" xml:space="preserve">
          <source>Refines the dimension names of &lt;code&gt;self&lt;/code&gt; according to &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">根据&lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt;细化 &lt;code&gt;self&lt;/code&gt; 的维度名称。</target>
        </trans-unit>
        <trans-unit id="fd83bfc89e8951867aebce2ddbcf9aebb755b1db" translate="yes" xml:space="preserve">
          <source>Refining is a special case of renaming that &amp;ldquo;lifts&amp;rdquo; unnamed dimensions. A &lt;code&gt;None&lt;/code&gt; dim can be refined to have any name; a named dim can only be refined to have the same name.</source>
          <target state="translated">精炼是重命名的特殊情况，可以&amp;ldquo;提升&amp;rdquo;未命名的尺寸。一个 &lt;code&gt;None&lt;/code&gt; 暗淡可以细化到有任何名称; 命名的dim只能精炼为具有相同的名称。</target>
        </trans-unit>
        <trans-unit id="5acc200962ac427c960517361870d2ca95708a0c" translate="yes" xml:space="preserve">
          <source>ReflectionPad1d</source>
          <target state="translated">ReflectionPad1d</target>
        </trans-unit>
        <trans-unit id="1970d048b8b11bdbb537bc4467b97dde9cbcbcf6" translate="yes" xml:space="preserve">
          <source>ReflectionPad2d</source>
          <target state="translated">ReflectionPad2d</target>
        </trans-unit>
        <trans-unit id="b0e2ded4176f1e93c24c6bb28a1f246d0f065d46" translate="yes" xml:space="preserve">
          <source>Registers a &lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt;&lt;code&gt;Constraint&lt;/code&gt;&lt;/a&gt; subclass in this registry. Usage:</source>
          <target state="translated">在此注册表中注册一个&lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt; &lt;code&gt;Constraint&lt;/code&gt; &lt;/a&gt;子类。用法：</target>
        </trans-unit>
        <trans-unit id="44e5f9796886b9f39a08e9c3d313df29a4afdc69" translate="yes" xml:space="preserve">
          <source>Registers a backward hook on the module.</source>
          <target state="translated">在模块上注册一个后向钩。</target>
        </trans-unit>
        <trans-unit id="780fedc50fbec8a5ee2093bca221ccb17c71d8a2" translate="yes" xml:space="preserve">
          <source>Registers a backward hook.</source>
          <target state="translated">注册一个倒钩。</target>
        </trans-unit>
        <trans-unit id="859ad11a7a9b00f2dc1648434120e2c30f756c3a" translate="yes" xml:space="preserve">
          <source>Registers a forward hook on the module.</source>
          <target state="translated">注册模块上的正向钩。</target>
        </trans-unit>
        <trans-unit id="c1cc6d46047e39e672054a7b43c380ab89aa516c" translate="yes" xml:space="preserve">
          <source>Registers a forward pre-hook on the module.</source>
          <target state="translated">注册模块上的正向预钩。</target>
        </trans-unit>
        <trans-unit id="d2f50deb4525028ab1c8e505fb5f42b3b1805265" translate="yes" xml:space="preserve">
          <source>Registry to link constraints to transforms.</source>
          <target state="translated">注册表将约束条件与变换联系起来。</target>
        </trans-unit>
        <trans-unit id="994be4aa8c70d405e9665f571a19642fba261dfd" translate="yes" xml:space="preserve">
          <source>Reinterprets some of the batch dims of a distribution as event dims.</source>
          <target state="translated">将分布中的一些批次dims重新解释为事件dims。</target>
        </trans-unit>
        <trans-unit id="c4e38689980bd0eb675729a3f5eeff33277a75da" translate="yes" xml:space="preserve">
          <source>RelaxedBernoulli</source>
          <target state="translated">RelaxedBernoulli</target>
        </trans-unit>
        <trans-unit id="17435d6989481bd254182b89c973e12b3560805c" translate="yes" xml:space="preserve">
          <source>RelaxedOneHotCategorical</source>
          <target state="translated">RelaxedOneHotCategorical</target>
        </trans-unit>
        <trans-unit id="017e050dc2c81440963780dd987527e102d00635" translate="yes" xml:space="preserve">
          <source>Release memory ASAP in the consumer.</source>
          <target state="translated">在消费者中尽快释放内存。</target>
        </trans-unit>
        <trans-unit id="c78aa00da732269786895ce2069aaa539d4c52f2" translate="yes" xml:space="preserve">
          <source>Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in &lt;code&gt;nvidia-smi&lt;/code&gt;.</source>
          <target state="translated">释放当前由缓存分配器保留的所有未占用的缓存内存，以便这些内存可以在其他GPU应用程序中使用，并在 &lt;code&gt;nvidia-smi&lt;/code&gt; 中可见。</target>
        </trans-unit>
        <trans-unit id="6aabe5246532aa5e226dacac93b7040def81f191" translate="yes" xml:space="preserve">
          <source>Remote Reference Protocol</source>
          <target state="translated">远程参考协议</target>
        </trans-unit>
        <trans-unit id="d2611051acb1fb592d77daa5cae8fbc141634b65" translate="yes" xml:space="preserve">
          <source>Remove all items from the ModuleDict.</source>
          <target state="translated">删除ModuleDict中的所有项目。</target>
        </trans-unit>
        <trans-unit id="19890e3b8941dee97b6730d3bdd4b5d7d4da71b3" translate="yes" xml:space="preserve">
          <source>Remove all items from the ParameterDict.</source>
          <target state="translated">删除ParameterDict中的所有项目。</target>
        </trans-unit>
        <trans-unit id="42e212ea49cffd4d66b50c00fb3332336dc13f4d" translate="yes" xml:space="preserve">
          <source>Remove key from the ModuleDict and return its module.</source>
          <target state="translated">从ModuleDict中取出钥匙,并返回其模块。</target>
        </trans-unit>
        <trans-unit id="1225726758ca9313c0c5ad3ef2ebcc4ac3f41d5b" translate="yes" xml:space="preserve">
          <source>Remove key from the ParameterDict and return its parameter.</source>
          <target state="translated">从ParameterDict中删除键并返回其参数。</target>
        </trans-unit>
        <trans-unit id="9168342afe82b61ecfcba245e5a3e85066d9cae2" translate="yes" xml:space="preserve">
          <source>Removes a tensor dimension.</source>
          <target state="translated">移除一个张量维。</target>
        </trans-unit>
        <trans-unit id="10551f8140eae7eec1c2f790aa2c398a8a2c225b" translate="yes" xml:space="preserve">
          <source>Removes the pruning reparameterization from a module and the pruning method from the forward hook.</source>
          <target state="translated">从模块中删除修剪重参数化,从前向钩子中删除修剪方法。</target>
        </trans-unit>
        <trans-unit id="4d39954d7304877c790bae592a897de9b12d0614" translate="yes" xml:space="preserve">
          <source>Removes the pruning reparameterization from a module and the pruning method from the forward hook. The pruned parameter named &lt;code&gt;name&lt;/code&gt; remains permanently pruned, and the parameter named &lt;code&gt;name+'_orig'&lt;/code&gt; is removed from the parameter list. Similarly, the buffer named &lt;code&gt;name+'_mask'&lt;/code&gt; is removed from the buffers.</source>
          <target state="translated">从模块中删除修剪重新参数化，并从前向挂钩中删除修剪方法。被修剪的 &lt;code&gt;name&lt;/code&gt; 为name的参数将被永久修剪，并且将从 &lt;code&gt;name+'_orig'&lt;/code&gt; 列表中删除名称为name +'_ orig'的参数。同样，将从缓冲区中删除名为 &lt;code&gt;name+'_mask'&lt;/code&gt; 的缓冲区。</target>
        </trans-unit>
        <trans-unit id="c59a012a002ebcdeb37a7a4b34651f264175236e" translate="yes" xml:space="preserve">
          <source>Removes the pruning reparameterization from a module. The pruned parameter named &lt;code&gt;name&lt;/code&gt; remains permanently pruned, and the parameter named &lt;code&gt;name+'_orig'&lt;/code&gt; is removed from the parameter list. Similarly, the buffer named &lt;code&gt;name+'_mask'&lt;/code&gt; is removed from the buffers.</source>
          <target state="translated">从模块中删除修剪重新参数化。被修剪的 &lt;code&gt;name&lt;/code&gt; 为name的参数将被永久修剪，并且将从 &lt;code&gt;name+'_orig'&lt;/code&gt; 列表中删除名称为name +'_ orig'的参数。同样，将从缓冲区中删除名为 &lt;code&gt;name+'_mask'&lt;/code&gt; 的缓冲区。</target>
        </trans-unit>
        <trans-unit id="dc2f6b46c8953513899753dc6f37ab21a947d81f" translate="yes" xml:space="preserve">
          <source>Removes the spectral normalization reparameterization from a module.</source>
          <target state="translated">删除模块中的光谱归一化重参数化。</target>
        </trans-unit>
        <trans-unit id="e6f3f85d165e444aba5b7212e4c107777c615fa6" translate="yes" xml:space="preserve">
          <source>Removes the weight normalization reparameterization from a module.</source>
          <target state="translated">删除模块中的权重归一化重新参数化。</target>
        </trans-unit>
        <trans-unit id="52a78431ddbd753c3acf927411e5dc4eadd51b45" translate="yes" xml:space="preserve">
          <source>Renames dimension names of &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">重命名 &lt;code&gt;self&lt;/code&gt; 的维度名称。</target>
        </trans-unit>
        <trans-unit id="1e15bfbedf002065481094064ab67496cead9c00" translate="yes" xml:space="preserve">
          <source>Render matplotlib figure into an image and add it to summary.</source>
          <target state="translated">将matplotlib图渲染成图像并添加到摘要中。</target>
        </trans-unit>
        <trans-unit id="d4424e2e484e1d0e2228176a1b9d133567150454" translate="yes" xml:space="preserve">
          <source>Repeat elements of a tensor.</source>
          <target state="translated">重复一个张量的元素。</target>
        </trans-unit>
        <trans-unit id="c71c8380a78ab9cf6beaa41ffbcea1de55f181da" translate="yes" xml:space="preserve">
          <source>Repeated tensor which has the same shape as input, except along the</source>
          <target state="translated">重复张量,其形状与输入相同,但沿以下方向除外</target>
        </trans-unit>
        <trans-unit id="fb8854cff80d6a6dd0521042f630c62a94943f9c" translate="yes" xml:space="preserve">
          <source>Repeats this tensor along the specified dimensions.</source>
          <target state="translated">沿着指定的尺寸重复这个张量。</target>
        </trans-unit>
        <trans-unit id="b48641d4be70735a27677d2b7a657eb8c5a36bbf" translate="yes" xml:space="preserve">
          <source>Replaces specified modules with dynamic weight-only quantized versions and output the quantized model.</source>
          <target state="translated">将指定的模块替换为动态纯重量量化版本,并输出量化模型。</target>
        </trans-unit>
        <trans-unit id="d717a0451eef9c7694fd101a42a92bb95d3c0a0f" translate="yes" xml:space="preserve">
          <source>ReplicationPad1d</source>
          <target state="translated">ReplicationPad1d</target>
        </trans-unit>
        <trans-unit id="7134ff298e79c741cf90df7d64ae3929d67c6005" translate="yes" xml:space="preserve">
          <source>ReplicationPad2d</source>
          <target state="translated">ReplicationPad2d</target>
        </trans-unit>
        <trans-unit id="b3ace69bd0bb68cc7d2ded9c8edec593b271b884" translate="yes" xml:space="preserve">
          <source>ReplicationPad3d</source>
          <target state="translated">ReplicationPad3d</target>
        </trans-unit>
        <trans-unit id="e41aa1fbbb423bb33022752fd330ea4ede8e16b6" translate="yes" xml:space="preserve">
          <source>Reproducibility</source>
          <target state="translated">Reproducibility</target>
        </trans-unit>
        <trans-unit id="f2e21cfb2562445f4e03149dcac964dc2b3217e6" translate="yes" xml:space="preserve">
          <source>ResNeXt</source>
          <target state="translated">ResNeXt</target>
        </trans-unit>
        <trans-unit id="135c48b06dbed678d0d867cebf0cf2de161d70cb" translate="yes" xml:space="preserve">
          <source>ResNeXt-101 32x8d model from &lt;a href=&quot;https://arxiv.org/pdf/1611.05431.pdf&quot;&gt;&amp;ldquo;Aggregated Residual Transformation for Deep Neural Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">ResNeXt-101 32x8d模型，来自&lt;a href=&quot;https://arxiv.org/pdf/1611.05431.pdf&quot;&gt;&amp;ldquo;深度神经网络的聚合残差转换&amp;rdquo;&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bd1a95f8f26475fcde5af625d8ad155d8daa61e2" translate="yes" xml:space="preserve">
          <source>ResNeXt-101-32x8d</source>
          <target state="translated">ResNeXt-101-32x8d</target>
        </trans-unit>
        <trans-unit id="6345fcacef60a6153337313629690096bdc6aa78" translate="yes" xml:space="preserve">
          <source>ResNeXt-50 32x4d model from &lt;a href=&quot;https://arxiv.org/pdf/1611.05431.pdf&quot;&gt;&amp;ldquo;Aggregated Residual Transformation for Deep Neural Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">ResNeXt-50 32x4d模型，来自&lt;a href=&quot;https://arxiv.org/pdf/1611.05431.pdf&quot;&gt;&amp;ldquo;深度神经网络的聚合残差转换&amp;rdquo;&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e6816178c1f62a28d2e7d31075a846209cc1ee81" translate="yes" xml:space="preserve">
          <source>ResNeXt-50-32x4d</source>
          <target state="translated">ResNeXt-50-32x4d</target>
        </trans-unit>
        <trans-unit id="78fa6ef9716ebb9b06e34fb7e8ef3e1ee1ff74a3" translate="yes" xml:space="preserve">
          <source>ResNet</source>
          <target state="translated">ResNet</target>
        </trans-unit>
        <trans-unit id="5e9c332cfed41849a28e63e75aa34789f743a723" translate="yes" xml:space="preserve">
          <source>ResNet (2+1)D</source>
          <target state="translated">ResNet (2+1)D</target>
        </trans-unit>
        <trans-unit id="870e5dde5df25034d3630c3aac9cb2c4bf4e76fc" translate="yes" xml:space="preserve">
          <source>ResNet 3D</source>
          <target state="translated">ResNet 3D</target>
        </trans-unit>
        <trans-unit id="da256605d586d089e9e7223940cb0055eadbd1ee" translate="yes" xml:space="preserve">
          <source>ResNet 3D 18</source>
          <target state="translated">ResNet 3D 18</target>
        </trans-unit>
        <trans-unit id="f8475b358157d99679b66bf1c03461ee4befbc51" translate="yes" xml:space="preserve">
          <source>ResNet MC 18</source>
          <target state="translated">ResNet MC 18</target>
        </trans-unit>
        <trans-unit id="1f7d19436196d570e8d470413429de7c74b3c3f2" translate="yes" xml:space="preserve">
          <source>ResNet Mixed Convolution</source>
          <target state="translated">ResNet混合卷积</target>
        </trans-unit>
        <trans-unit id="65817ac0c28cdf514ee310ccdae57eb30ff37866" translate="yes" xml:space="preserve">
          <source>ResNet-101</source>
          <target state="translated">ResNet-101</target>
        </trans-unit>
        <trans-unit id="6cdb79fa3540b98aee4fbbedd859f85dfa5878e1" translate="yes" xml:space="preserve">
          <source>ResNet-101 model from &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&amp;ldquo;Deep Residual Learning for Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">来自&lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&amp;ldquo;用于图像识别的深度残差学习&amp;rdquo;的&lt;/a&gt;ResNet-101模型</target>
        </trans-unit>
        <trans-unit id="ad50fc165163aca8aff90e2401f355f0d73cb36d" translate="yes" xml:space="preserve">
          <source>ResNet-152</source>
          <target state="translated">ResNet-152</target>
        </trans-unit>
        <trans-unit id="ce82bef112318844175b33dd6318fe126f6f7716" translate="yes" xml:space="preserve">
          <source>ResNet-152 model from &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&amp;ldquo;Deep Residual Learning for Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">来自&lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&amp;ldquo;用于图像识别的深度残差学习&amp;rdquo;的&lt;/a&gt;ResNet-152模型</target>
        </trans-unit>
        <trans-unit id="a95e0d1f4879bbdecde67b2d824acd91e0a2a593" translate="yes" xml:space="preserve">
          <source>ResNet-18</source>
          <target state="translated">ResNet-18</target>
        </trans-unit>
        <trans-unit id="0c0f62f5b13f506e60ccc5621518d2252a8fa85e" translate="yes" xml:space="preserve">
          <source>ResNet-18 model from &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&amp;ldquo;Deep Residual Learning for Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">来自&lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&amp;ldquo;用于图像识别的深度残差学习&amp;rdquo;的&lt;/a&gt;ResNet-18模型</target>
        </trans-unit>
        <trans-unit id="a7751035efc6144aca1732aa23112dd3aecb6bc5" translate="yes" xml:space="preserve">
          <source>ResNet-34</source>
          <target state="translated">ResNet-34</target>
        </trans-unit>
        <trans-unit id="e8def5ecc7cddff157e2b586142f5fdf856d05a5" translate="yes" xml:space="preserve">
          <source>ResNet-34 model from &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&amp;ldquo;Deep Residual Learning for Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">来自&lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&amp;ldquo;用于图像识别的深度残差学习&amp;rdquo;的&lt;/a&gt;ResNet-34模型</target>
        </trans-unit>
        <trans-unit id="27444a897bb1c912cdd3b04f0a83fd62a5d2c590" translate="yes" xml:space="preserve">
          <source>ResNet-50</source>
          <target state="translated">ResNet-50</target>
        </trans-unit>
        <trans-unit id="238ac2833cd54b6df3e18e9c24a824061fd3a21c" translate="yes" xml:space="preserve">
          <source>ResNet-50 model from &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&amp;ldquo;Deep Residual Learning for Image Recognition&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">来自&lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;&amp;ldquo;用于图像识别的深度残差学习&amp;rdquo;的&lt;/a&gt;ResNet-50模型</target>
        </trans-unit>
        <trans-unit id="6f2d9cd833caae8dbee1966d8ab0943fddb27fd0" translate="yes" xml:space="preserve">
          <source>ResNext</source>
          <target state="translated">ResNext</target>
        </trans-unit>
        <trans-unit id="d6989ed48031dfd0f342adca72920ca82d14ffc5" translate="yes" xml:space="preserve">
          <source>Resets parameter data pointer so that they can use faster code paths.</source>
          <target state="translated">重置参数数据指针,使他们可以使用更快的代码路径。</target>
        </trans-unit>
        <trans-unit id="48147200a8245c840b5cf1d24511816f59e2613d" translate="yes" xml:space="preserve">
          <source>Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.</source>
          <target state="translated">重置给定设备的缓存分配器管理的最大GPU内存的跟踪起点。</target>
        </trans-unit>
        <trans-unit id="055ff3b94f1675cb0c91b05c26e8f21a16e0aac4" translate="yes" xml:space="preserve">
          <source>Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.</source>
          <target state="translated">重置给定设备跟踪最大GPU内存占用的起始点。</target>
        </trans-unit>
        <trans-unit id="5146907f6db4f1b0f9869f7b411013406ae86ae8" translate="yes" xml:space="preserve">
          <source>Resizes &lt;code&gt;self&lt;/code&gt; tensor to the specified size. If the number of elements is larger than the current storage size, then the underlying storage is resized to fit the new number of elements. If the number of elements is smaller, the underlying storage is not changed. Existing elements are preserved but any new memory is uninitialized.</source>
          <target state="translated">将 &lt;code&gt;self&lt;/code&gt; 张量调整为指定大小。如果元素数量大于当前存储大小，那么将调整基础存储的大小以适合新的元素数量。如果元素数较小，则基础存储不会更改。现有元素将保留，但任何新内存均未初始化。</target>
        </trans-unit>
        <trans-unit id="70492132da5a47c3477a96c6627fc5933e85e1e1" translate="yes" xml:space="preserve">
          <source>Resizes the &lt;code&gt;self&lt;/code&gt; tensor to be the same size as the specified &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt;. This is equivalent to &lt;code&gt;self.resize_(tensor.size())&lt;/code&gt;.</source>
          <target state="translated">将 &lt;code&gt;self&lt;/code&gt; 张量调整为与指定&lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt;相同的大小。这等效于 &lt;code&gt;self.resize_(tensor.size())&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="dc2cdc93dc037b80e1ebd8434da768aef6d0fcaf" translate="yes" xml:space="preserve">
          <source>Result is &lt;code&gt;-inf&lt;/code&gt; if &lt;code&gt;input&lt;/code&gt; has zero log determinant, and is &lt;code&gt;nan&lt;/code&gt; if &lt;code&gt;input&lt;/code&gt; has negative determinant.</source>
          <target state="translated">结果是 &lt;code&gt;-inf&lt;/code&gt; 如果 &lt;code&gt;input&lt;/code&gt; 具有零数的决定因素，是 &lt;code&gt;nan&lt;/code&gt; ，如果 &lt;code&gt;input&lt;/code&gt; 具有负的决定因素。</target>
        </trans-unit>
        <trans-unit id="8ae09aed030eb1a83f83c301c7fc275b3c8a0647" translate="yes" xml:space="preserve">
          <source>RetinaNet</source>
          <target state="translated">RetinaNet</target>
        </trans-unit>
        <trans-unit id="a1e229ed770b9619ca67b1b0aef6ec1a40fa1776" translate="yes" xml:space="preserve">
          <source>RetinaNet ResNet-50 FPN</source>
          <target state="translated">RetinaNet ResNet-50 FPN</target>
        </trans-unit>
        <trans-unit id="2f7a9d4a0bc6f5ce0cbca61c540daaa5837025ee" translate="yes" xml:space="preserve">
          <source>Retrieves a map from Tensor to the appropriate gradient for that Tensor accumulated in the provided context corresponding to the given &lt;code&gt;context_id&lt;/code&gt; as part of the distributed autograd backward pass.</source>
          <target state="translated">从Tensor检索对应于给定 &lt;code&gt;context_id&lt;/code&gt; 累积的Tensor的适当梯度的映射，该上下文对应于给定context_id，作为分布式autograd向后传递的一部分。</target>
        </trans-unit>
        <trans-unit id="1ece76093eabcbca0a40b151ec0259b426ce567d" translate="yes" xml:space="preserve">
          <source>Retrieves the value associated with the given &lt;code&gt;key&lt;/code&gt; in the store. If &lt;code&gt;key&lt;/code&gt; is not present in the store, the function will wait for &lt;code&gt;timeout&lt;/code&gt;, which is defined when initializing the store, before throwing an exception.</source>
          <target state="translated">检索与商店中给定 &lt;code&gt;key&lt;/code&gt; 关联的值。如果存储中不存在 &lt;code&gt;key&lt;/code&gt; ，则该函数将在引发异常之前等待初始化存储时定义的 &lt;code&gt;timeout&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="24f096b221f9534bcad007f2b5a32b490950b5b5" translate="yes" xml:space="preserve">
          <source>Return</source>
          <target state="translated">Return</target>
        </trans-unit>
        <trans-unit id="9cae8b09f0118269a649c5008d17c8253bec9782" translate="yes" xml:space="preserve">
          <source>Return &lt;code&gt;True&lt;/code&gt; if this &lt;code&gt;Future&lt;/code&gt; is done. A &lt;code&gt;Future&lt;/code&gt; is done if it has a result or an exception.</source>
          <target state="translated">如果此 &lt;code&gt;Future&lt;/code&gt; 完成，则返回 &lt;code&gt;True&lt;/code&gt; 。一个 &lt;code&gt;Future&lt;/code&gt; ，如果它有一个结果或异常完成。</target>
        </trans-unit>
        <trans-unit id="402831e8533c0acc0b144ded8789c976ea34ab74" translate="yes" xml:space="preserve">
          <source>Return a tensor of elements selected from either &lt;code&gt;x&lt;/code&gt; or &lt;code&gt;y&lt;/code&gt;, depending on &lt;code&gt;condition&lt;/code&gt;.</source>
          <target state="translated">根据 &lt;code&gt;condition&lt;/code&gt; 返回从 &lt;code&gt;x&lt;/code&gt; 或 &lt;code&gt;y&lt;/code&gt; 中选择的元素的张量。</target>
        </trans-unit>
        <trans-unit id="91f0e9dea6ad89a9cf4c50eeccd2244407a0e3b6" translate="yes" xml:space="preserve">
          <source>Return an iterable of the ModuleDict key/value pairs.</source>
          <target state="translated">返回ModuleDict键/值对的迭代。</target>
        </trans-unit>
        <trans-unit id="53758c5a8d9838ee15c5a1139ddc298268e3f550" translate="yes" xml:space="preserve">
          <source>Return an iterable of the ModuleDict keys.</source>
          <target state="translated">返回ModuleDict键的迭代。</target>
        </trans-unit>
        <trans-unit id="75b2716bbd52d76a593565ce26e635f35a65eba3" translate="yes" xml:space="preserve">
          <source>Return an iterable of the ModuleDict values.</source>
          <target state="translated">返回ModuleDict值的迭代。</target>
        </trans-unit>
        <trans-unit id="39aa7eccaf084effaa1d95b2523664d87adadc63" translate="yes" xml:space="preserve">
          <source>Return an iterable of the ParameterDict key/value pairs.</source>
          <target state="translated">返回ParameterDict键/值对的迭代。</target>
        </trans-unit>
        <trans-unit id="36d08ddba7858cda4d75cdfaa632b92a0d271817" translate="yes" xml:space="preserve">
          <source>Return an iterable of the ParameterDict keys.</source>
          <target state="translated">返回ParameterDict键的迭代。</target>
        </trans-unit>
        <trans-unit id="0eb7abc74094e832f39c777a2b9fdb3353665e0c" translate="yes" xml:space="preserve">
          <source>Return an iterable of the ParameterDict values.</source>
          <target state="translated">返回ParameterDict值的迭代。</target>
        </trans-unit>
        <trans-unit id="6fb5e3008c47afba229895a4aa4b8cfa03845a94" translate="yes" xml:space="preserve">
          <source>Return the next floating-point value after &lt;code&gt;input&lt;/code&gt; towards &lt;code&gt;other&lt;/code&gt;, elementwise.</source>
          <target state="translated">之后返回下浮点值 &lt;code&gt;input&lt;/code&gt; 对 &lt;code&gt;other&lt;/code&gt; ，按元素。</target>
        </trans-unit>
        <trans-unit id="c457486fbf0726f2b8521c0379e30203a9c59a56" translate="yes" xml:space="preserve">
          <source>Return the recommended gain value for the given nonlinearity function. The values are as follows:</source>
          <target state="translated">返回给定非线性函数的推荐增益值。值如下:</target>
        </trans-unit>
        <trans-unit id="689416efbdc5cadbcaa00d5d52249a2a0b1ad5c6" translate="yes" xml:space="preserve">
          <source>Return the singular value decomposition &lt;code&gt;(U, S, V)&lt;/code&gt; of a matrix, batches of matrices, or a sparse matrix</source>
          <target state="translated">返回矩阵，矩阵批次或稀疏矩阵的奇异值分解 &lt;code&gt;(U, S, V)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="41b1fb407b7fa442b77381701968fb175362cf78" translate="yes" xml:space="preserve">
          <source>Return type</source>
          <target state="translated">返回类型</target>
        </trans-unit>
        <trans-unit id="1254906d712ed370ba1cdf6cdc5b06fa43eea6c3" translate="yes" xml:space="preserve">
          <source>Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as &lt;code&gt;resize_&lt;/code&gt; / &lt;code&gt;resize_as_&lt;/code&gt; / &lt;code&gt;set_&lt;/code&gt; / &lt;code&gt;transpose_&lt;/code&gt;) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as &lt;code&gt;zero_&lt;/code&gt; / &lt;code&gt;copy_&lt;/code&gt; / &lt;code&gt;add_&lt;/code&gt;) to the returned tensor will not update the original tensor anymore, and will instead trigger an error.</source>
          <target state="translated">返回的Tensor与原始Tensor共享相同的存储。可以看到对它们中的任何一个的就地修改，并且可能触发正确性检查中的错误。重要提示：此前，就地尺寸/步幅/存储更改（如 &lt;code&gt;resize_&lt;/code&gt; / &lt;code&gt;resize_as_&lt;/code&gt; / &lt;code&gt;set_&lt;/code&gt; / &lt;code&gt;transpose_&lt;/code&gt; ）返回的张量也更新原有的张量。现在，这些就地更改将不再更新原始张量，而将触发错误。对于稀疏张量：对返回的张量的就地索引/值更改（例如 &lt;code&gt;zero_&lt;/code&gt; / &lt;code&gt;copy_&lt;/code&gt; / &lt;code&gt;add_&lt;/code&gt; ）将不再更新原始张量，而是触发错误。</target>
        </trans-unit>
        <trans-unit id="24bbb3acca42b36ec251562d04ed070ce8e00154" translate="yes" xml:space="preserve">
          <source>Returned by &lt;a href=&quot;#torch.multiprocessing.spawn&quot;&gt;&lt;code&gt;spawn()&lt;/code&gt;&lt;/a&gt; when called with &lt;code&gt;join=False&lt;/code&gt;.</source>
          <target state="translated">当使用 &lt;code&gt;join=False&lt;/code&gt; 调用时，由&lt;a href=&quot;#torch.multiprocessing.spawn&quot;&gt; &lt;code&gt;spawn()&lt;/code&gt; &lt;/a&gt;返回。</target>
        </trans-unit>
        <trans-unit id="996cd4205ef23a24fd77a57ded3d3ea1bc26418b" translate="yes" xml:space="preserve">
          <source>Returned tensor</source>
          <target state="translated">返回的张量</target>
        </trans-unit>
        <trans-unit id="9582a02f141fc4b345b2936eba691cd0654efebc" translate="yes" xml:space="preserve">
          <source>Returns</source>
          <target state="translated">Returns</target>
        </trans-unit>
        <trans-unit id="bececec8df4be65f442e8a241d0d72275aefac53" translate="yes" xml:space="preserve">
          <source>Returns &lt;code&gt;True&lt;/code&gt; if all processes have been joined successfully, &lt;code&gt;False&lt;/code&gt; if there are more processes that need to be joined.</source>
          <target state="translated">如果所有进程都已成功加入，则返回 &lt;code&gt;True&lt;/code&gt; ；如果还有更多进程需要加入，则返回 &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="11cfd066b647ca21ea710fa73097f97cb102e1f7" translate="yes" xml:space="preserve">
          <source>Returns &lt;code&gt;True&lt;/code&gt; if the &lt;a href=&quot;https://ninja-build.org/&quot;&gt;ninja&lt;/a&gt; build system is available on the system, &lt;code&gt;False&lt;/code&gt; otherwise.</source>
          <target state="translated">如果&lt;a href=&quot;https://ninja-build.org/&quot;&gt;忍者&lt;/a&gt;构建系统在系统上可用，则返回 &lt;code&gt;True&lt;/code&gt; ,否则返回 &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="196db82d442085adba66ce72a40bf4c6b883e9c5" translate="yes" xml:space="preserve">
          <source>Returns &lt;code&gt;True&lt;/code&gt; if the distributed package is available. Otherwise, &lt;code&gt;torch.distributed&lt;/code&gt; does not expose any other APIs. Currently, &lt;code&gt;torch.distributed&lt;/code&gt; is available on Linux, MacOS and Windows. Set &lt;code&gt;USE_DISTRIBUTED=1&lt;/code&gt; to enable it when building PyTorch from source. Currently, the default value is &lt;code&gt;USE_DISTRIBUTED=1&lt;/code&gt; for Linux and Windows, &lt;code&gt;USE_DISTRIBUTED=0&lt;/code&gt; for MacOS.</source>
          <target state="translated">如果分布式程序包可用，则返回 &lt;code&gt;True&lt;/code&gt; 。否则， &lt;code&gt;torch.distributed&lt;/code&gt; 不会公开任何其他API。当前， &lt;code&gt;torch.distributed&lt;/code&gt; 在Linux，MacOS和Windows上可用。从源代码构建PyTorch时，设置 &lt;code&gt;USE_DISTRIBUTED=1&lt;/code&gt; 启用它。当前，对于Linux和Windows，默认值为 &lt;code&gt;USE_DISTRIBUTED=1&lt;/code&gt; ，对于MacOS ，默认值为 &lt;code&gt;USE_DISTRIBUTED=0&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="616465fba98082b69d414e38f74b13cab726f4ca" translate="yes" xml:space="preserve">
          <source>Returns &lt;code&gt;True&lt;/code&gt; if your system supports flushing denormal numbers and it successfully configures flush denormal mode. &lt;a href=&quot;#torch.set_flush_denormal&quot;&gt;&lt;code&gt;set_flush_denormal()&lt;/code&gt;&lt;/a&gt; is only supported on x86 architectures supporting SSE3.</source>
          <target state="translated">如果您的系统支持刷新非正规数并且已成功配置刷新非正规模式，则返回 &lt;code&gt;True&lt;/code&gt; 。仅在支持SSE3的x86体系结构上支持&lt;a href=&quot;#torch.set_flush_denormal&quot;&gt; &lt;code&gt;set_flush_denormal()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="65ca52a0d52509836247baf62510fc9195f808a9" translate="yes" xml:space="preserve">
          <source>Returns &lt;code&gt;self&lt;/code&gt; tensor as a NumPy &lt;code&gt;ndarray&lt;/code&gt;. This tensor and the returned &lt;code&gt;ndarray&lt;/code&gt; share the same underlying storage. Changes to &lt;code&gt;self&lt;/code&gt; tensor will be reflected in the &lt;code&gt;ndarray&lt;/code&gt; and vice versa.</source>
          <target state="translated">将 &lt;code&gt;self&lt;/code&gt; 张量作为NumPy &lt;code&gt;ndarray&lt;/code&gt; 返回。该张量和返回的 &lt;code&gt;ndarray&lt;/code&gt; 共享相同的基础存储。 &lt;code&gt;self&lt;/code&gt; 张量的变化将反映在 &lt;code&gt;ndarray&lt;/code&gt; 中，反之亦然。</target>
        </trans-unit>
        <trans-unit id="606042b612f01e1413a4be37fdcf4bc9fb55a2fd" translate="yes" xml:space="preserve">
          <source>Returns &lt;code&gt;self&lt;/code&gt; tensor&amp;rsquo;s offset in the underlying storage in terms of number of storage elements (not bytes).</source>
          <target state="translated">根据存储元素的数量（不是字节）返回基础存储中的 &lt;code&gt;self&lt;/code&gt; 张量的偏移量。</target>
        </trans-unit>
        <trans-unit id="f2368e1fd9ddecc9751151ce685ba540173983ea" translate="yes" xml:space="preserve">
          <source>Returns NVCC gencode flags this library were compiled with.</source>
          <target state="translated">Returns NVCC gencode flags this library were compiled with.</target>
        </trans-unit>
        <trans-unit id="bcee86aa4247be4fa57afaf338977b171a646e64" translate="yes" xml:space="preserve">
          <source>Returns True if &lt;code&gt;obj&lt;/code&gt; is a PyTorch storage object.</source>
          <target state="translated">如果 &lt;code&gt;obj&lt;/code&gt; 是PyTorch存储对象，则返回True 。</target>
        </trans-unit>
        <trans-unit id="0c0fb790a1f7d7b5df52c8eb6b44b136b1ef5094" translate="yes" xml:space="preserve">
          <source>Returns True if &lt;code&gt;obj&lt;/code&gt; is a PyTorch tensor.</source>
          <target state="translated">如果 &lt;code&gt;obj&lt;/code&gt; 是PyTorch张量，则返回True 。</target>
        </trans-unit>
        <trans-unit id="598b755e55cee356b7f7a395b6039f5d177fd41e" translate="yes" xml:space="preserve">
          <source>Returns True if &lt;code&gt;self&lt;/code&gt; tensor is contiguous in memory in the order specified by memory format.</source>
          <target state="translated">如果 &lt;code&gt;self&lt;/code&gt; 张量在内存中按内存格式指定的顺序连续，则返回True 。</target>
        </trans-unit>
        <trans-unit id="f80a083f5b5de080482f9bd320c35bee376edd5c" translate="yes" xml:space="preserve">
          <source>Returns True if all elements in each row of the tensor in the given dimension &lt;code&gt;dim&lt;/code&gt; are True, False otherwise.</source>
          <target state="translated">返回true如果在给定尺寸下张量的每一行中的所有元素 &lt;code&gt;dim&lt;/code&gt; 都是真实的，否则为false。</target>
        </trans-unit>
        <trans-unit id="10febb846509f6f2bc8c4b9a8a6d5f7a4d94b02e" translate="yes" xml:space="preserve">
          <source>Returns True if all elements in the tensor are True, False otherwise.</source>
          <target state="translated">如果张量中的所有元素都是True,则返回True,否则返回False。</target>
        </trans-unit>
        <trans-unit id="ec249a00c3df799b548b2828fcdbdce219624299" translate="yes" xml:space="preserve">
          <source>Returns True if any elements in each row of the tensor in the given dimension &lt;code&gt;dim&lt;/code&gt; are True, False otherwise.</source>
          <target state="translated">如果给定维度 &lt;code&gt;dim&lt;/code&gt; 中张量的每一行中的任何元素为True，则返回True ，否则为False。</target>
        </trans-unit>
        <trans-unit id="19164100a9af1a366aaf5bd12a4e88efb4270d73" translate="yes" xml:space="preserve">
          <source>Returns True if any elements in the tensor are True, False otherwise.</source>
          <target state="translated">如果张量中的任何元素为True,则返回True,否则返回False。</target>
        </trans-unit>
        <trans-unit id="93fef2bb2e23cd032c2bc66acb9ba2af786fd6c1" translate="yes" xml:space="preserve">
          <source>Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).</source>
          <target state="translated">如果两个 tensors 都指向相同的内存(相同的存储空间、偏移量、大小和步长),则返回 True。</target>
        </trans-unit>
        <trans-unit id="dfeaffdf583441339db6cd69904330215eafe368" translate="yes" xml:space="preserve">
          <source>Returns True if the &lt;code&gt;input&lt;/code&gt; is a single element tensor which is not equal to zero after type conversions.</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 是类型转换后不等于零的单个元素张量，则返回True 。</target>
        </trans-unit>
        <trans-unit id="c533fa6ff5203fb73d711ade459c89e619986758" translate="yes" xml:space="preserve">
          <source>Returns True if the &lt;code&gt;input&lt;/code&gt; is a single element tensor which is not equal to zero after type conversions. i.e. not equal to &lt;code&gt;torch.tensor([0.])&lt;/code&gt; or &lt;code&gt;torch.tensor([0])&lt;/code&gt; or &lt;code&gt;torch.tensor([False])&lt;/code&gt;. Throws a &lt;code&gt;RuntimeError&lt;/code&gt; if &lt;code&gt;torch.numel() != 1&lt;/code&gt; (even in case of sparse tensors).</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 是类型转换后不等于零的单个元素张量，则返回True 。即不等于 &lt;code&gt;torch.tensor([0.])&lt;/code&gt; 或 &lt;code&gt;torch.tensor([0])&lt;/code&gt; 或 &lt;code&gt;torch.tensor([False])&lt;/code&gt; 。如果 &lt;code&gt;torch.numel() != 1&lt;/code&gt; （即使在稀疏张量的情况下），也会引发 &lt;code&gt;RuntimeError&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4546d55dc55434836d7155a943fded11ace1b513" translate="yes" xml:space="preserve">
          <source>Returns True if the data type of &lt;code&gt;input&lt;/code&gt; is a complex data type i.e., one of &lt;code&gt;torch.complex64&lt;/code&gt;, and &lt;code&gt;torch.complex128&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 的数据类型是复杂数据类型，即 &lt;code&gt;torch.complex64&lt;/code&gt; 和 &lt;code&gt;torch.complex128&lt;/code&gt; 之一，则返回True 。</target>
        </trans-unit>
        <trans-unit id="5ed711035f4f45a68bdc409be57f591a1203d447" translate="yes" xml:space="preserve">
          <source>Returns True if the data type of &lt;code&gt;input&lt;/code&gt; is a floating point data type i.e., one of &lt;code&gt;torch.float64&lt;/code&gt;, &lt;code&gt;torch.float32&lt;/code&gt; and &lt;code&gt;torch.float16&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;input&lt;/code&gt; 的数据类型是浮点数据类型，即 &lt;code&gt;torch.float64&lt;/code&gt; ， &lt;code&gt;torch.float32&lt;/code&gt; 和 &lt;code&gt;torch.float16&lt;/code&gt; 之一，则返回True 。</target>
        </trans-unit>
        <trans-unit id="5ad196c93762cde87d752d5a63b5607aaf9d27a9" translate="yes" xml:space="preserve">
          <source>Returns True if the data type of &lt;code&gt;self&lt;/code&gt; is a complex data type.</source>
          <target state="translated">如果 &lt;code&gt;self&lt;/code&gt; 的数据类型是复杂数据类型，则返回True 。</target>
        </trans-unit>
        <trans-unit id="47b01dfd12ff21d88fce8263233ddd92fb4ed8af" translate="yes" xml:space="preserve">
          <source>Returns True if the data type of &lt;code&gt;self&lt;/code&gt; is a floating point data type.</source>
          <target state="translated">如果 &lt;code&gt;self&lt;/code&gt; 的数据类型是浮点数据类型，则返回True 。</target>
        </trans-unit>
        <trans-unit id="81de301f63a48cfa0447acb0616f4e3a8050d38a" translate="yes" xml:space="preserve">
          <source>Returns True if the data type of &lt;code&gt;self&lt;/code&gt; is a signed data type.</source>
          <target state="translated">如果 &lt;code&gt;self&lt;/code&gt; 的数据类型是带符号的数据类型，则返回True 。</target>
        </trans-unit>
        <trans-unit id="2b3d22229996fcea50652425a0ceef76726b9f91" translate="yes" xml:space="preserve">
          <source>Returns True if the global deterministic flag is turned on.</source>
          <target state="translated">如果全局确定性标志被打开,返回True。</target>
        </trans-unit>
        <trans-unit id="b5073df54cb09fc1a6407537c2e76e0147566896" translate="yes" xml:space="preserve">
          <source>Returns True if the global deterministic flag is turned on. Refer to &lt;a href=&quot;torch.set_deterministic#torch.set_deterministic&quot;&gt;&lt;code&gt;torch.set_deterministic()&lt;/code&gt;&lt;/a&gt; documentation for more details.</source>
          <target state="translated">如果全局确定性标志已打开，则返回True。有关更多详细信息，请参考&lt;a href=&quot;torch.set_deterministic#torch.set_deterministic&quot;&gt; &lt;code&gt;torch.set_deterministic()&lt;/code&gt; &lt;/a&gt;文档。</target>
        </trans-unit>
        <trans-unit id="c4032dc2622bbc19b42824ae9a76253643ed72f9" translate="yes" xml:space="preserve">
          <source>Returns a 1-D tensor of size</source>
          <target state="translated">返回大小为1-D的张量</target>
        </trans-unit>
        <trans-unit id="d2604008505b663501ba7ccd9df67158c244b1f4" translate="yes" xml:space="preserve">
          <source>Returns a 1-dimensional view of each input tensor with zero dimensions.</source>
          <target state="translated">返回每个输入张量的零维度的一维视图。</target>
        </trans-unit>
        <trans-unit id="e25d75bccca92cff61915e5211fa5ae4cd4e83b9" translate="yes" xml:space="preserve">
          <source>Returns a 1-dimensional view of each input tensor with zero dimensions. Input tensors with one or more dimensions are returned as-is.</source>
          <target state="translated">返回每个零维输入张量的一维视图。有一个或多个维度的输入张量将按原样返回。</target>
        </trans-unit>
        <trans-unit id="f9b98594557024de39affd4c070595a178dad181" translate="yes" xml:space="preserve">
          <source>Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.</source>
          <target state="translated">返回一个对角线为1,其他地方为0的二维张量。</target>
        </trans-unit>
        <trans-unit id="1a93e2128169bc51774f54bd53d2eaec67c329a8" translate="yes" xml:space="preserve">
          <source>Returns a 2-dimensional view of each each input tensor with zero dimensions.</source>
          <target state="translated">返回每个输入张量的零维度的二维视图。</target>
        </trans-unit>
        <trans-unit id="5d4b7116a250c6d886720ec99aa88d8cbeb63c35" translate="yes" xml:space="preserve">
          <source>Returns a 2-dimensional view of each each input tensor with zero dimensions. Input tensors with two or more dimensions are returned as-is. :param input: :type input: Tensor or list of Tensors</source>
          <target state="translated">返回每个输入张量的零维度的二维视图。有两个或更多维度的输入张量将按原样返回。:param input::type input.Tensor或List of Tensors。张量或张量列表</target>
        </trans-unit>
        <trans-unit id="5b208e9701580107f4e0baee12e48579cf84134e" translate="yes" xml:space="preserve">
          <source>Returns a 3-dimensional view of each each input tensor with zero dimensions.</source>
          <target state="translated">返回每个零维输入张量的三维视图。</target>
        </trans-unit>
        <trans-unit id="9be625c9292e0cee5fd9c2f9e9f3ae65376309fe" translate="yes" xml:space="preserve">
          <source>Returns a 3-dimensional view of each each input tensor with zero dimensions. Input tensors with three or more dimensions are returned as-is. :param input: :type input: Tensor or list of Tensors</source>
          <target state="translated">返回每个零维度输入张量的三维视图。有三个或更多维度的输入张量将按原样返回。:param input::type input.Tensor或Tensors列表。张量或张量列表</target>
        </trans-unit>
        <trans-unit id="ca90d78c7ebd1efe1d3973c488edc158aa0b5d0e" translate="yes" xml:space="preserve">
          <source>Returns a &lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt;&lt;code&gt;Constraint&lt;/code&gt;&lt;/a&gt; object representing this distribution&amp;rsquo;s support.</source>
          <target state="translated">返回表示此发行版支持的&lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt; &lt;code&gt;Constraint&lt;/code&gt; &lt;/a&gt;对象。</target>
        </trans-unit>
        <trans-unit id="ab61e9a8b5883304b4c24f113af9c2e2c089bc01" translate="yes" xml:space="preserve">
          <source>Returns a &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; object to a list of the passed in Futures.</source>
          <target state="translated">将&lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;对象返回到传递的Futures列表中。</target>
        </trans-unit>
        <trans-unit id="cff2424065898889ca4c8ceba9391c00be8f3c4e" translate="yes" xml:space="preserve">
          <source>Returns a &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; object that can be waited on. When completed, the return value of &lt;code&gt;func&lt;/code&gt; on &lt;code&gt;args&lt;/code&gt; and &lt;code&gt;kwargs&lt;/code&gt; can be retrieved from the &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; object.</source>
          <target state="translated">返回可以等待的&lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;对象。当完成时，返回值 &lt;code&gt;func&lt;/code&gt; 上 &lt;code&gt;args&lt;/code&gt; 和 &lt;code&gt;kwargs&lt;/code&gt; 可以从被检索&lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt;对象。</target>
        </trans-unit>
        <trans-unit id="3012b7f55211365d28ddf4ab3e32e9d56916febf" translate="yes" xml:space="preserve">
          <source>Returns a CPU copy of this storage if it&amp;rsquo;s not already on the CPU</source>
          <target state="translated">返回此存储的CPU副本（如果尚未在CPU上）</target>
        </trans-unit>
        <trans-unit id="310ef068d2b0176ea9cd4ecb440e8d929300b581" translate="yes" xml:space="preserve">
          <source>Returns a DLPack representing the tensor.</source>
          <target state="translated">返回一个代表张量的DLPack。</target>
        </trans-unit>
        <trans-unit id="d270dbf83984f38c1be89175b564ec60887173d4" translate="yes" xml:space="preserve">
          <source>Returns a Python float containing the current scale, or 1.0 if scaling is disabled.</source>
          <target state="translated">返回一个包含当前比例尺的Python float,如果禁用比例尺,则返回1.0。</target>
        </trans-unit>
        <trans-unit id="79f467145c77b5bb1f3f5bd6e7fabcaca51e20bd" translate="yes" xml:space="preserve">
          <source>Returns a Python float containing the scale backoff factor.</source>
          <target state="translated">Returns a Python float containing the scale backoff factor.</target>
        </trans-unit>
        <trans-unit id="fea0c1596123398a7d55be21927faed71a6ab193" translate="yes" xml:space="preserve">
          <source>Returns a Python float containing the scale growth factor.</source>
          <target state="translated">返回一个包含比例增长因子的Python float。</target>
        </trans-unit>
        <trans-unit id="7844351802a857a8df7b5d5cbf2c31ca4dbaa9b0" translate="yes" xml:space="preserve">
          <source>Returns a Python int containing the growth interval.</source>
          <target state="translated">返回一个包含生长区间的 Python int。</target>
        </trans-unit>
        <trans-unit id="8d50286946851edf8b261b9ee4ffcf514e96ce11" translate="yes" xml:space="preserve">
          <source>Returns a Tensor of size &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt; filled with &lt;code&gt;0&lt;/code&gt;. By default, the returned Tensor has the same &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; as this tensor.</source>
          <target state="translated">返回一个&lt;a href=&quot;#torch.Tensor.size&quot;&gt; &lt;code&gt;size&lt;/code&gt; &lt;/a&gt;为 &lt;code&gt;0&lt;/code&gt; 的张量的张量。默认情况下，返回的Tensor具有与此张量相同的&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0ab36bf75992b3e189a293c78e7fdb358c585560" translate="yes" xml:space="preserve">
          <source>Returns a Tensor of size &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt; filled with &lt;code&gt;1&lt;/code&gt;. By default, the returned Tensor has the same &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; as this tensor.</source>
          <target state="translated">返回大小为&lt;a href=&quot;#torch.Tensor.size&quot;&gt; &lt;code&gt;size&lt;/code&gt; &lt;/a&gt;的张量，并填充 &lt;code&gt;1&lt;/code&gt; 。默认情况下，返回的Tensor具有与此张量相同的&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="aff928dda115238bd0307d57f02acbba9a324cfb" translate="yes" xml:space="preserve">
          <source>Returns a Tensor of size &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt; filled with &lt;code&gt;fill_value&lt;/code&gt;. By default, the returned Tensor has the same &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; as this tensor.</source>
          <target state="translated">返回&lt;a href=&quot;#torch.Tensor.size&quot;&gt; &lt;code&gt;size&lt;/code&gt; &lt;/a&gt;为 &lt;code&gt;fill_value&lt;/code&gt; 的张量的张量。默认情况下，返回的Tensor具有与此张量相同的&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="545a5b97b3133069c0e8dcc8e1ce77397a1e785a" translate="yes" xml:space="preserve">
          <source>Returns a Tensor of size &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt; filled with uninitialized data. By default, the returned Tensor has the same &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; as this tensor.</source>
          <target state="translated">返回大小的张量&lt;a href=&quot;#torch.Tensor.size&quot;&gt; &lt;code&gt;size&lt;/code&gt; &lt;/a&gt;充满了未初始化的数据。默认情况下，返回的Tensor具有与此张量相同的&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e1bd30076d669876272e68e1e793c79f23bddd95" translate="yes" xml:space="preserve">
          <source>Returns a Tensor with same &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; as the Tensor &lt;code&gt;other&lt;/code&gt;. When &lt;code&gt;non_blocking&lt;/code&gt;, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When &lt;code&gt;copy&lt;/code&gt; is set, a new Tensor is created even when the Tensor already matches the desired conversion.</source>
          <target state="translated">返回张量相同&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;为张量 &lt;code&gt;other&lt;/code&gt; 。当 &lt;code&gt;non_blocking&lt;/code&gt; 时，如果可能，尝试相对于主机进行异步转换，例如，将具有固定内存的CPU Tensor转换为CUDA Tensor。当 &lt;code&gt;copy&lt;/code&gt; 设置，即使在张量已经匹配所需的转化创造了一个新的张量。</target>
        </trans-unit>
        <trans-unit id="7ff74c2d1bff3e0ce5952d69b71bd9a1e607f1aa" translate="yes" xml:space="preserve">
          <source>Returns a Tensor with the specified &lt;a href=&quot;#torch.Tensor.device&quot;&gt;&lt;code&gt;device&lt;/code&gt;&lt;/a&gt; and (optional) &lt;code&gt;dtype&lt;/code&gt;. If &lt;code&gt;dtype&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; it is inferred to be &lt;code&gt;self.dtype&lt;/code&gt;. When &lt;code&gt;non_blocking&lt;/code&gt;, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When &lt;code&gt;copy&lt;/code&gt; is set, a new Tensor is created even when the Tensor already matches the desired conversion.</source>
          <target state="translated">返回具有指定&lt;a href=&quot;#torch.Tensor.device&quot;&gt; &lt;code&gt;device&lt;/code&gt; &lt;/a&gt;和（可选） &lt;code&gt;dtype&lt;/code&gt; 的张量。如果 &lt;code&gt;dtype&lt;/code&gt; 为 &lt;code&gt;None&lt;/code&gt; ，则推断为 &lt;code&gt;self.dtype&lt;/code&gt; 。当 &lt;code&gt;non_blocking&lt;/code&gt; 时，如果可能，尝试相对于主机进行异步转换，例如，将具有固定内存的CPU Tensor转换为CUDA Tensor。当 &lt;code&gt;copy&lt;/code&gt; 设置，即使在张量已经匹配所需的转化创造了一个新的张量。</target>
        </trans-unit>
        <trans-unit id="da569f76cd483df9524efcb31fcf2ea99e390904" translate="yes" xml:space="preserve">
          <source>Returns a Tensor with the specified &lt;code&gt;dtype&lt;/code&gt;</source>
          <target state="translated">返回具有指定 &lt;code&gt;dtype&lt;/code&gt; 的张量</target>
        </trans-unit>
        <trans-unit id="da2702897f096fbbfeecf34312faf90efede2de3" translate="yes" xml:space="preserve">
          <source>Returns a bool indicating if CUDA is currently available.</source>
          <target state="translated">返回一个指示CUDA当前是否可用的bool。</target>
        </trans-unit>
        <trans-unit id="6724b6006e294e6e49650c83c7e2700124a4da5f" translate="yes" xml:space="preserve">
          <source>Returns a bool indicating if CUDNN is currently available.</source>
          <target state="translated">返回一个表示CUDNN当前是否可用的布尔。</target>
        </trans-unit>
        <trans-unit id="623f6de5f91be71aa002bb6ab0ba269d8348512a" translate="yes" xml:space="preserve">
          <source>Returns a bool indicating whether this instance is enabled.</source>
          <target state="translated">返回一个表示该实例是否启用的布尔。</target>
        </trans-unit>
        <trans-unit id="005b46932efde89028d44d377485c52b269ce9de" translate="yes" xml:space="preserve">
          <source>Returns a byte tensor of &lt;code&gt;sample_shape + batch_shape&lt;/code&gt; indicating whether each event in value satisfies this constraint.</source>
          <target state="translated">返回 &lt;code&gt;sample_shape + batch_shape&lt;/code&gt; 的字节张量，指示值中的每个事件是否满足此约束。</target>
        </trans-unit>
        <trans-unit id="e15fd6f4de5dc5fea9ffd2c5465a552199dc217b" translate="yes" xml:space="preserve">
          <source>Returns a contiguous in memory tensor containing the same data as &lt;code&gt;self&lt;/code&gt; tensor. If &lt;code&gt;self&lt;/code&gt; tensor is already in the specified memory format, this function returns the &lt;code&gt;self&lt;/code&gt; tensor.</source>
          <target state="translated">返回一个连续的内存张量，其中包含与 &lt;code&gt;self&lt;/code&gt; 张量相同的数据。如果 &lt;code&gt;self&lt;/code&gt; tensor已经是指定的内存格式，则此函数返回 &lt;code&gt;self&lt;/code&gt; tensor。</target>
        </trans-unit>
        <trans-unit id="27f4f3450faf371eb6306a54c86ee05877b30941" translate="yes" xml:space="preserve">
          <source>Returns a contraction of a and b over multiple dimensions.</source>
          <target state="translated">返回a和b在多个维度上的收缩。</target>
        </trans-unit>
        <trans-unit id="28370d731f31276aa7e934060964f2e577e96a76" translate="yes" xml:space="preserve">
          <source>Returns a copy of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 的副本。</target>
        </trans-unit>
        <trans-unit id="3318b58b5db87c6ad9827c719c9a9b9dbdc40b78" translate="yes" xml:space="preserve">
          <source>Returns a copy of the tensor in &lt;code&gt;torch.mkldnn&lt;/code&gt; layout.</source>
          <target state="translated">返回 &lt;code&gt;torch.mkldnn&lt;/code&gt; 布局中的张量的副本。</target>
        </trans-unit>
        <trans-unit id="f6881bd2839553790195cb237970005b22687467" translate="yes" xml:space="preserve">
          <source>Returns a copy of this object in CPU memory.</source>
          <target state="translated">返回CPU内存中这个对象的副本。</target>
        </trans-unit>
        <trans-unit id="bd445c8d669710f667e23d46ff8038e303167c04" translate="yes" xml:space="preserve">
          <source>Returns a copy of this object in CUDA memory.</source>
          <target state="translated">返回CUDA内存中这个对象的副本。</target>
        </trans-unit>
        <trans-unit id="2d7501c9a5a1714d3c3684f56c90bd8b61fff5d3" translate="yes" xml:space="preserve">
          <source>Returns a copy of this storage</source>
          <target state="translated">返回此存储的副本</target>
        </trans-unit>
        <trans-unit id="7d518eb5a72df432375d8394da8f628d15fd3b22" translate="yes" xml:space="preserve">
          <source>Returns a dictionary containing a whole state of the module.</source>
          <target state="translated">返回一个包含模块整体状态的字典。</target>
        </trans-unit>
        <trans-unit id="7da6f33bf2a479dba2b7480dac5819fb18cbbc2f" translate="yes" xml:space="preserve">
          <source>Returns a dictionary from argument names to &lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt;&lt;code&gt;Constraint&lt;/code&gt;&lt;/a&gt; objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict.</source>
          <target state="translated">从参数名称返回&lt;a href=&quot;#torch.distributions.constraints.Constraint&quot;&gt; &lt;code&gt;Constraint&lt;/code&gt; &lt;/a&gt;对象的字典，此分布的每个参数都应满足该对象。不是tensor的args不必出现在此dict中。</target>
        </trans-unit>
        <trans-unit id="e46b2212af694a636c66b6d3e83683486d2db4d8" translate="yes" xml:space="preserve">
          <source>Returns a dictionary of CUDA memory allocator statistics for a given device.</source>
          <target state="translated">Returns a dictionary of CUDA memory allocator statistics for a given device.</target>
        </trans-unit>
        <trans-unit id="915db01bc1c22d7e1017e2a0ebc05f0845ed5843" translate="yes" xml:space="preserve">
          <source>Returns a human-readable printout of the current memory allocator statistics for a given device.</source>
          <target state="translated">返回给定设备的当前内存分配器统计数据的可读打印输出。</target>
        </trans-unit>
        <trans-unit id="dbff7cc734b7142fc6847934bd6e1e33f653b796" translate="yes" xml:space="preserve">
          <source>Returns a list containing the elements of this storage</source>
          <target state="translated">返回一个包含该存储元素的列表</target>
        </trans-unit>
        <trans-unit id="97fd9a7e3427d9329387c86b5fc1fc50b3513d44" translate="yes" xml:space="preserve">
          <source>Returns a list of ByteTensor representing the random number states of all devices.</source>
          <target state="translated">返回代表所有设备随机数状态的ByteTensor列表。</target>
        </trans-unit>
        <trans-unit id="dd4681a72abe05d09c99ad2246c5c6f58da6423c" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the &lt;code&gt;k&lt;/code&gt; th smallest element of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">返回一个namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; ，其中 &lt;code&gt;values&lt;/code&gt; 是给定维 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每一行的第 &lt;code&gt;k&lt;/code&gt; 个最小元素。</target>
        </trans-unit>
        <trans-unit id="ee8ea74a50bcff655454904543731546a0784b94" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the &lt;code&gt;k&lt;/code&gt; th smallest element of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;. And &lt;code&gt;indices&lt;/code&gt; is the index location of each element found.</source>
          <target state="translated">返回一个namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; ，其中 &lt;code&gt;values&lt;/code&gt; 是给定维 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每一行的第 &lt;code&gt;k&lt;/code&gt; 个最小元素。而 &lt;code&gt;indices&lt;/code&gt; 是找到的每个元素的索引位置。</target>
        </trans-unit>
        <trans-unit id="1e386080072251f678a51090a4553c8d16a045fa" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the cumulative maximum of elements of &lt;code&gt;input&lt;/code&gt; in the dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">返回一个namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; ，其中 &lt;code&gt;values&lt;/code&gt; 是维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 元素的累积最大值。</target>
        </trans-unit>
        <trans-unit id="126e4b96929c50182f17c4bb4a58b7d0fe049898" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the cumulative maximum of elements of &lt;code&gt;input&lt;/code&gt; in the dimension &lt;code&gt;dim&lt;/code&gt;. And &lt;code&gt;indices&lt;/code&gt; is the index location of each maximum value found in the dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">返回一个namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; ，其中 &lt;code&gt;values&lt;/code&gt; 是维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 元素的累积最大值。而 &lt;code&gt;indices&lt;/code&gt; 在尺寸找到的每个最大值的索引位置 &lt;code&gt;dim&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0588c1c3069407fd276a9514a421bf82e6313f49" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the cumulative minimum of elements of &lt;code&gt;input&lt;/code&gt; in the dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">返回一个namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; ，其中 &lt;code&gt;values&lt;/code&gt; 是维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 元素的累计最小值。</target>
        </trans-unit>
        <trans-unit id="c882e37440105ee95efe87d7c7a08229fc1a4c09" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the cumulative minimum of elements of &lt;code&gt;input&lt;/code&gt; in the dimension &lt;code&gt;dim&lt;/code&gt;. And &lt;code&gt;indices&lt;/code&gt; is the index location of each maximum value found in the dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">返回一个namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; ，其中 &lt;code&gt;values&lt;/code&gt; 是维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 元素的累计最小值。而 &lt;code&gt;indices&lt;/code&gt; 在尺寸找到的每个最大值的索引位置 &lt;code&gt;dim&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="967b821b8f0b7a1f96a4f7d0f792de5f0185c446" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the maximum value of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;. And &lt;code&gt;indices&lt;/code&gt; is the index location of each maximum value found (argmax).</source>
          <target state="translated">返回一个namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; ，其中 &lt;code&gt;values&lt;/code&gt; 是给定维 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每一行的最大值。和 &lt;code&gt;indices&lt;/code&gt; 是每个最大值的索引位置找到（argmax）。</target>
        </trans-unit>
        <trans-unit id="340d25e61485b289574a04d557988b6aad2f299c" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the median value of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;. And &lt;code&gt;indices&lt;/code&gt; is the index location of each median value found.</source>
          <target state="translated">返回一个namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; ，其中 &lt;code&gt;values&lt;/code&gt; 是给定维 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每一行的中值。而 &lt;code&gt;indices&lt;/code&gt; 是找到的每个中值的索引位置。</target>
        </trans-unit>
        <trans-unit id="6b24dd1ba3e9b5a8840a923ed9f1831a7b9b04bb" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the minimum value of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;. And &lt;code&gt;indices&lt;/code&gt; is the index location of each minimum value found (argmin).</source>
          <target state="translated">返回一个namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; ，其中 &lt;code&gt;values&lt;/code&gt; 是给定维 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每一行的最小值。和 &lt;code&gt;indices&lt;/code&gt; 是每个最小值的索引位置找到（argmin）。</target>
        </trans-unit>
        <trans-unit id="bc8471de2c101d3651a6d7a76fd12e82843e6661" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the mode value of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;, i.e.</source>
          <target state="translated">返回一个namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; ，其中 &lt;code&gt;values&lt;/code&gt; 是给定维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每一行的模式值，即</target>
        </trans-unit>
        <trans-unit id="a4cc2f3a360a06df266083c92ef6ae91e2b318f1" translate="yes" xml:space="preserve">
          <source>Returns a namedtuple &lt;code&gt;(values, indices)&lt;/code&gt; where &lt;code&gt;values&lt;/code&gt; is the mode value of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;, i.e. a value which appears most often in that row, and &lt;code&gt;indices&lt;/code&gt; is the index location of each mode value found.</source>
          <target state="translated">返回一个命名元组 &lt;code&gt;(values, indices)&lt;/code&gt; ，其中 &lt;code&gt;values&lt;/code&gt; 是给定维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每一行的众数值，即该行中最常出现的值，而 &lt;code&gt;indices&lt;/code&gt; 是找到的每个众数值的索引位置。</target>
        </trans-unit>
        <trans-unit id="55daf74ff9a749b34582eb6dc5a7e9467d0b17d0" translate="yes" xml:space="preserve">
          <source>Returns a new 1-D tensor which indexes the &lt;code&gt;input&lt;/code&gt; tensor according to the boolean mask &lt;code&gt;mask&lt;/code&gt; which is a &lt;code&gt;BoolTensor&lt;/code&gt;.</source>
          <target state="translated">返回一个新的一维张量，该张量根据布尔型mask &lt;code&gt;mask&lt;/code&gt; (即 &lt;code&gt;BoolTensor&lt;/code&gt; )对 &lt;code&gt;input&lt;/code&gt; 张量进行索引。</target>
        </trans-unit>
        <trans-unit id="cc996281b6d8b730ea490f35416e253843087c8c" translate="yes" xml:space="preserve">
          <source>Returns a new SparseTensor with values from Tensor &lt;code&gt;input&lt;/code&gt; filtered by indices of &lt;code&gt;mask&lt;/code&gt; and values are ignored. &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;mask&lt;/code&gt; must have the same shape.</source>
          <target state="translated">返回一个新的SparseTensor，其Tensor &lt;code&gt;input&lt;/code&gt; 值通过 &lt;code&gt;mask&lt;/code&gt; 的索引过滤，并且值被忽略。 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;mask&lt;/code&gt; 必须具有相同的形状。</target>
        </trans-unit>
        <trans-unit id="650add904aa71e494da203b253a7e5daaa3f4c87" translate="yes" xml:space="preserve">
          <source>Returns a new Tensor with &lt;code&gt;data&lt;/code&gt; as the tensor data. By default, the returned Tensor has the same &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; as this tensor.</source>
          <target state="translated">返回以 &lt;code&gt;data&lt;/code&gt; 为张量数据的新Tensor 。默认情况下，返回的Tensor具有与此张量相同的&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6f5b88777af1b3fb2f4856550ea09e3d3ade90f2" translate="yes" xml:space="preserve">
          <source>Returns a new Tensor, detached from the current graph.</source>
          <target state="translated">返回一个新的Tensor,从当前图形中分离出来。</target>
        </trans-unit>
        <trans-unit id="adfa10a0d579afab7d4a1e2dd4d592e805a24bbd" translate="yes" xml:space="preserve">
          <source>Returns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to &lt;code&gt;batch_shape&lt;/code&gt;. This method calls &lt;a href=&quot;tensors#torch.Tensor.expand&quot;&gt;&lt;code&gt;expand&lt;/code&gt;&lt;/a&gt; on the distribution&amp;rsquo;s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in &lt;code&gt;__init__.py&lt;/code&gt;, when an instance is first created.</source>
          <target state="translated">返回一个新的分发实例（或填充派生类提供的现有实例），并将批次尺寸扩展为 &lt;code&gt;batch_shape&lt;/code&gt; 。此方法对发行版的参数调用&lt;a href=&quot;tensors#torch.Tensor.expand&quot;&gt; &lt;code&gt;expand&lt;/code&gt; &lt;/a&gt;。因此，这不会为扩展的分发实例分配新的内存。此外，首次创建实例时，此操作不会在 &lt;code&gt;__init__.py&lt;/code&gt; 中重复任何args检查或参数广播。</target>
        </trans-unit>
        <trans-unit id="b6cad31d8dba22f0073db8447536a302dee9ee8b" translate="yes" xml:space="preserve">
          <source>Returns a new tensor containing imaginary values of the &lt;code&gt;self&lt;/code&gt; tensor.</source>
          <target state="translated">返回一个新的张量，其中包含 &lt;code&gt;self&lt;/code&gt; 张量的虚值。</target>
        </trans-unit>
        <trans-unit id="a474356ccaab506872384349a8c7d3cc842edfca" translate="yes" xml:space="preserve">
          <source>Returns a new tensor containing imaginary values of the &lt;code&gt;self&lt;/code&gt; tensor. The returned tensor and &lt;code&gt;self&lt;/code&gt; share the same underlying storage.</source>
          <target state="translated">返回一个新的张量，其中包含 &lt;code&gt;self&lt;/code&gt; 张量的虚值。返回的张量和 &lt;code&gt;self&lt;/code&gt; 共享相同的基础存储。</target>
        </trans-unit>
        <trans-unit id="0417e03c0fc21d8aa9cfd0ad69033f35d283ab8f" translate="yes" xml:space="preserve">
          <source>Returns a new tensor containing real values of the &lt;code&gt;self&lt;/code&gt; tensor.</source>
          <target state="translated">返回一个新的张量，其中包含 &lt;code&gt;self&lt;/code&gt; 张量的实际值。</target>
        </trans-unit>
        <trans-unit id="ecd577300f229280c9c5bc1fe369db0f8475652e" translate="yes" xml:space="preserve">
          <source>Returns a new tensor containing real values of the &lt;code&gt;self&lt;/code&gt; tensor. The returned tensor and &lt;code&gt;self&lt;/code&gt; share the same underlying storage.</source>
          <target state="translated">返回一个新的张量，其中包含 &lt;code&gt;self&lt;/code&gt; 张量的实际值。返回的张量和 &lt;code&gt;self&lt;/code&gt; 共享相同的基础存储。</target>
        </trans-unit>
        <trans-unit id="84c3a649630732036f386759b3c1fe0f4082f284" translate="yes" xml:space="preserve">
          <source>Returns a new tensor that is a narrowed version of &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">返回一个新的张量，它是 &lt;code&gt;input&lt;/code&gt; 张量的缩小版本。</target>
        </trans-unit>
        <trans-unit id="446ae7f4205e67fe0ead00f78a291ab93a7e711a" translate="yes" xml:space="preserve">
          <source>Returns a new tensor that is a narrowed version of &lt;code&gt;input&lt;/code&gt; tensor. The dimension &lt;code&gt;dim&lt;/code&gt; is input from &lt;code&gt;start&lt;/code&gt; to &lt;code&gt;start + length&lt;/code&gt;. The returned tensor and &lt;code&gt;input&lt;/code&gt; tensor share the same underlying storage.</source>
          <target state="translated">返回一个新的张量，它是 &lt;code&gt;input&lt;/code&gt; 张量的缩小版本。维 &lt;code&gt;dim&lt;/code&gt; 是从输入 &lt;code&gt;start&lt;/code&gt; 到 &lt;code&gt;start + length&lt;/code&gt; 。返回的张量和 &lt;code&gt;input&lt;/code&gt; 张量共享相同的基础存储。</target>
        </trans-unit>
        <trans-unit id="d6897347512206bded239af0259e282422aa1908" translate="yes" xml:space="preserve">
          <source>Returns a new tensor which indexes the &lt;code&gt;input&lt;/code&gt; tensor along dimension &lt;code&gt;dim&lt;/code&gt; using the entries in &lt;code&gt;index&lt;/code&gt; which is a &lt;code&gt;LongTensor&lt;/code&gt;.</source>
          <target state="translated">返回一个新的张量，它使用 &lt;code&gt;index&lt;/code&gt; 中的条目 &lt;code&gt;LongTensor&lt;/code&gt; 沿维度 &lt;code&gt;dim&lt;/code&gt; 索引 &lt;code&gt;input&lt;/code&gt; 张量。</target>
        </trans-unit>
        <trans-unit id="d9c1e11a5f9f21aaf09f7c09b28d3efe9b79c1c9" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with a dimension of size one inserted at the specified position.</source>
          <target state="translated">返回在指定位置插入的尺寸为1的新张量。</target>
        </trans-unit>
        <trans-unit id="3224c262166ef14a2d46031572904d6b7dc8f710" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with boolean elements representing if each element is &lt;code&gt;finite&lt;/code&gt; or not.</source>
          <target state="translated">返回带有布尔元素的新张量，布尔元素表示每个元素是否 &lt;code&gt;finite&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="95e1f8a1ccb8a8da884d7e84169520f79117ec5e" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with boolean elements representing if each element of &lt;code&gt;input&lt;/code&gt; is &amp;ldquo;close&amp;rdquo; to the corresponding element of &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">返回带有布尔元素的新张量，布尔元素表示 &lt;code&gt;input&lt;/code&gt; 每个元素是否&amp;ldquo;接近&amp;rdquo; &lt;code&gt;other&lt;/code&gt; 的对应元素。</target>
        </trans-unit>
        <trans-unit id="4be234d2b824b3afbb8b6e3c023a9678a2fef898" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with boolean elements representing if each element of &lt;code&gt;input&lt;/code&gt; is &amp;ldquo;close&amp;rdquo; to the corresponding element of &lt;code&gt;other&lt;/code&gt;. Closeness is defined as:</source>
          <target state="translated">返回带有布尔元素的新张量，布尔元素表示 &lt;code&gt;input&lt;/code&gt; 每个元素是否&amp;ldquo;接近&amp;rdquo; &lt;code&gt;other&lt;/code&gt; 的对应元素。紧密度定义为：</target>
        </trans-unit>
        <trans-unit id="e9a96751e78a2d8505653806cfbbeebe8f0da987" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with boolean elements representing if each element of &lt;code&gt;input&lt;/code&gt; is NaN or not.</source>
          <target state="translated">返回带有布尔元素的新张量，布尔元素表示 &lt;code&gt;input&lt;/code&gt; 每个元素是否为NaN。</target>
        </trans-unit>
        <trans-unit id="f12a3acfcbbec29775876cc40c95d5a00ba9dd4a" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with boolean elements representing if each element of &lt;code&gt;input&lt;/code&gt; is NaN or not. Complex values are considered NaN when either their real and/or imaginary part is NaN.</source>
          <target state="translated">返回带有布尔元素的新张量，布尔元素表示 &lt;code&gt;input&lt;/code&gt; 每个元素是否为NaN。当复数值的实部和/或虚部为NaN时，它们被视为NaN。</target>
        </trans-unit>
        <trans-unit id="c183c7e7c59ca8cd11a1eb4078b588d714403acc" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with boolean elements representing if each element of &lt;code&gt;input&lt;/code&gt; is real-valued or not.</source>
          <target state="translated">返回带有布尔元素的新张量，布尔元素表示 &lt;code&gt;input&lt;/code&gt; 每个元素是否为实数值。</target>
        </trans-unit>
        <trans-unit id="1d49a6f7124bd5cd437a7a8fee3ecaa6dd817cb5" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with boolean elements representing if each element of &lt;code&gt;input&lt;/code&gt; is real-valued or not. All real-valued types are considered real. Complex values are considered real when their imaginary part is 0.</source>
          <target state="translated">返回带有布尔元素的新张量，布尔元素表示 &lt;code&gt;input&lt;/code&gt; 每个元素是否为实数值。所有实值类型都被视为实数。当复数值的虚部为0时，它们被认为是实数。</target>
        </trans-unit>
        <trans-unit id="fc57cc29526229bfe21bab4f3ac0fa280da67107" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with each of the elements of &lt;code&gt;input&lt;/code&gt; converted from angles in degrees to radians.</source>
          <target state="translated">返回一个新的张量，其中每个 &lt;code&gt;input&lt;/code&gt; 元素从以角度为单位的角度转换为弧度。</target>
        </trans-unit>
        <trans-unit id="e88584aef7be0fb19dcf3caab156e7c17acae22e" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with each of the elements of &lt;code&gt;input&lt;/code&gt; converted from angles in radians to degrees.</source>
          <target state="translated">返回一个新的张量，其中每个 &lt;code&gt;input&lt;/code&gt; 元素从弧度的角度转换为度。</target>
        </trans-unit>
        <trans-unit id="1fd6a07571fec21877bbfab5cd7f129520d284d3" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with each of the elements of &lt;code&gt;input&lt;/code&gt; rounded to the closest integer.</source>
          <target state="translated">返回一个新的张量，其中 &lt;code&gt;input&lt;/code&gt; 每个元素都舍入到最接近的整数。</target>
        </trans-unit>
        <trans-unit id="fbe9fedb3aa9a3f8d1c80fddb5c3023ad4eb0562" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the arcsine of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回具有 &lt;code&gt;input&lt;/code&gt; 元素的反正弦值的新张量。</target>
        </trans-unit>
        <trans-unit id="e7301245f47a696980faac9e988f16c20bfda8b7" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the arctangent of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回具有 &lt;code&gt;input&lt;/code&gt; 元素的反正切值的新张量。</target>
        </trans-unit>
        <trans-unit id="48ba42f2193edb51a142cab5570b31530effd843" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the ceil of the elements of &lt;code&gt;input&lt;/code&gt;, the smallest integer greater than or equal to each element.</source>
          <target state="translated">返回具有 &lt;code&gt;input&lt;/code&gt; 元素的ceil的新张量，该整数是大于或等于每个元素的最小整数。</target>
        </trans-unit>
        <trans-unit id="ec380dfec0f03433bc5e3eb0dc32821c6d58c884" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the cosine of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回具有 &lt;code&gt;input&lt;/code&gt; 元素的余弦值的新张量。</target>
        </trans-unit>
        <trans-unit id="b0f7755628d00f3662eb39d02532aaee3f952862" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the elements of &lt;code&gt;input&lt;/code&gt; at the given indices.</source>
          <target state="translated">返回一个新的张量，其中的 &lt;code&gt;input&lt;/code&gt; 元素在给定的索引处。</target>
        </trans-unit>
        <trans-unit id="1c910712ac5b779edd94af13b8675d26b2ec9292" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the elements of &lt;code&gt;input&lt;/code&gt; at the given indices. The input tensor is treated as if it were viewed as a 1-D tensor. The result takes the same shape as the indices.</source>
          <target state="translated">返回一个新的张量，其中的 &lt;code&gt;input&lt;/code&gt; 元素在给定的索引处。将输入张量视为视为一维张量。结果采用与索引相同的形状。</target>
        </trans-unit>
        <trans-unit id="303a77ef9e4e8ebea411b724b70cc880893ce1e5" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the exponential of the elements minus 1 of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回一个新的张量，其元素的指数为 &lt;code&gt;input&lt;/code&gt; 的负1 。</target>
        </trans-unit>
        <trans-unit id="fc8c5f6e874a9d0acbadee7550724c3fd5e88f46" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the exponential of the elements of the input tensor &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回带有输入张量 &lt;code&gt;input&lt;/code&gt; 的元素指数的新张量。</target>
        </trans-unit>
        <trans-unit id="66422619029217fb49c5540ea658003ab25d0251" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the floor of the elements of &lt;code&gt;input&lt;/code&gt;, the largest integer less than or equal to each element.</source>
          <target state="translated">返回一个新的张量，其张数为 &lt;code&gt;input&lt;/code&gt; 的元素的下限，即小于或等于每个元素的最大整数。</target>
        </trans-unit>
        <trans-unit id="54289275fc2999c18e519269b9c836fadd53ce09" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the hyperbolic cosine of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回具有 &lt;code&gt;input&lt;/code&gt; 元素的双曲余弦的新张量。</target>
        </trans-unit>
        <trans-unit id="60d4e278d41229d94159bb88a5a80098f0f17944" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the hyperbolic sine of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回具有 &lt;code&gt;input&lt;/code&gt; 元素的双曲正弦值的新张量。</target>
        </trans-unit>
        <trans-unit id="529a1d5ed854492ced22f20ca045b03cdb98828a" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the hyperbolic tangent of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回具有 &lt;code&gt;input&lt;/code&gt; 元素的双曲正切值的新张量。</target>
        </trans-unit>
        <trans-unit id="9966d85e500f92c101949dbcee56d3622b0b7c74" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the inverse hyperbolic cosine of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回具有的元素的反双曲余弦一个新的张量 &lt;code&gt;input&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="016516e7b6362bc2420ae10e09ad46e7409d58cb" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the inverse hyperbolic sine of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回具有的元件的反双曲正弦一个新的张量 &lt;code&gt;input&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="8624b8361034217c02a1c5e88fca1edddd5ce5dd" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the inverse hyperbolic tangent of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">与返回的元素的反双曲正切一个新的张量 &lt;code&gt;input&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="dfd8b16427d7265384f4ab1669de0b012956b1e0" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the logarithm to the base 10 of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回一个新的张量，其对数为 &lt;code&gt;input&lt;/code&gt; 元素的底数为10的对数。</target>
        </trans-unit>
        <trans-unit id="ca59edbe372ab054262e278a40d31de7be114996" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the logarithm to the base 2 of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回一个新的张量，其对数为 &lt;code&gt;input&lt;/code&gt; 元素的底数为2的对数。</target>
        </trans-unit>
        <trans-unit id="768981f99c3160fc5f2a6b954d047547f7f51a33" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the logit of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回一个具有 &lt;code&gt;input&lt;/code&gt; 元素logit的新张量。</target>
        </trans-unit>
        <trans-unit id="83c02ed817fb8ffcff4e801d08d12130cb9f1cab" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the logit of the elements of &lt;code&gt;input&lt;/code&gt;. &lt;code&gt;input&lt;/code&gt; is clamped to [eps, 1 - eps] when eps is not None. When eps is None and &lt;code&gt;input&lt;/code&gt; &amp;lt; 0 or &lt;code&gt;input&lt;/code&gt; &amp;gt; 1, the function will yields NaN.</source>
          <target state="translated">返回一个具有 &lt;code&gt;input&lt;/code&gt; 元素logit的新张量。当eps不为None时， &lt;code&gt;input&lt;/code&gt; 被钳位为[eps，1-eps]。当eps为None且 &lt;code&gt;input&lt;/code&gt; &amp;lt;0或 &lt;code&gt;input&lt;/code&gt; &amp;gt; 1时，该函数将产生NaN。</target>
        </trans-unit>
        <trans-unit id="743fb44188d39b917a57a969a7a2bdfa54f07c5c" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the natural logarithm of (1 + &lt;code&gt;input&lt;/code&gt;).</source>
          <target state="translated">返回自然对数为（1 + &lt;code&gt;input&lt;/code&gt; ）的新张量。</target>
        </trans-unit>
        <trans-unit id="7802c5deb620e1b95914f599e220dfe558ee03c0" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the natural logarithm of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回具有 &lt;code&gt;input&lt;/code&gt; 元素的自然对数的新张量。</target>
        </trans-unit>
        <trans-unit id="c9bebfac39f90f81173963e0ca1f126dea57da57" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the negative of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回带有 &lt;code&gt;input&lt;/code&gt; 元素负数的新张量。</target>
        </trans-unit>
        <trans-unit id="a0595e3a74577061b95243faf09a3387b6e217a7" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the reciprocal of the elements of &lt;code&gt;input&lt;/code&gt;</source>
          <target state="translated">返回带有 &lt;code&gt;input&lt;/code&gt; 元素倒数的新张量</target>
        </trans-unit>
        <trans-unit id="40e790541f395ea8a715d66e7db7815ecda1464f" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the reciprocal of the square-root of each of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回一个新张量，其张量为 &lt;code&gt;input&lt;/code&gt; 的每个元素的平方根。</target>
        </trans-unit>
        <trans-unit id="82f5d4bb888952c73ce51e1bac717329cb2869c0" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the same data as the &lt;code&gt;self&lt;/code&gt; tensor but of a different &lt;code&gt;shape&lt;/code&gt;.</source>
          <target state="translated">返回一个新张量，该张量具有与 &lt;code&gt;self&lt;/code&gt; 张量相同的数据，但 &lt;code&gt;shape&lt;/code&gt; 不同。</target>
        </trans-unit>
        <trans-unit id="736d1d1a205da1b8f4c4709de8f6235951658a7e" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the sigmoid of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回具有 &lt;code&gt;input&lt;/code&gt; 元素的S形的新张量。</target>
        </trans-unit>
        <trans-unit id="5e2b36eb81199cdbe569cc67684536ac9defc3a0" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the signs of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回带有 &lt;code&gt;input&lt;/code&gt; 元素的符号的新张量。</target>
        </trans-unit>
        <trans-unit id="47f02b09b0f6a0ab4b9c1f34438e6706d93ade59" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the sine of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回带有 &lt;code&gt;input&lt;/code&gt; 元素正弦值的新张量。</target>
        </trans-unit>
        <trans-unit id="75f7c6fa74ae5240454bfc9ef7f507262ffe9ed2" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the square of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回具有 &lt;code&gt;input&lt;/code&gt; 元素平方的新张量。</target>
        </trans-unit>
        <trans-unit id="b2f28004537b23138d7c295a5914cf3e65ab6fd0" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the square-root of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回具有 &lt;code&gt;input&lt;/code&gt; 元素的平方根的新张量。</target>
        </trans-unit>
        <trans-unit id="baf6cf4928d6ab113308458333184cd46b1dbec9" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the tangent of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回与 &lt;code&gt;input&lt;/code&gt; 的元素相切的新张量。</target>
        </trans-unit>
        <trans-unit id="c51c660d43995f5399bc591984337fa3aa3dd1db" translate="yes" xml:space="preserve">
          <source>Returns a new tensor with the truncated integer values of the elements of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回一个新的张量，该张量具有 &lt;code&gt;input&lt;/code&gt; 元素的截断整数值。</target>
        </trans-unit>
        <trans-unit id="71a9122ea30596ae8f8e0cfb84c73297b46f8b8e" translate="yes" xml:space="preserve">
          <source>Returns a new view of the &lt;code&gt;self&lt;/code&gt; tensor with singleton dimensions expanded to a larger size.</source>
          <target state="translated">返回单张量尺寸扩展为更大尺寸的 &lt;code&gt;self&lt;/code&gt; 张量的新视图。</target>
        </trans-unit>
        <trans-unit id="04f242c1b997d817d7997eb7a445d6e3823e9344" translate="yes" xml:space="preserve">
          <source>Returns a partial view of &lt;code&gt;input&lt;/code&gt; with the its diagonal elements with respect to &lt;code&gt;dim1&lt;/code&gt; and &lt;code&gt;dim2&lt;/code&gt; appended as a dimension at the end of the shape.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 的局部视图，其对角线元素相对于 &lt;code&gt;dim1&lt;/code&gt; 和 &lt;code&gt;dim2&lt;/code&gt; 追加为形状的结尾处的尺寸。</target>
        </trans-unit>
        <trans-unit id="d3032b88cf4f390e50cfecedba4f33edc6cf5043" translate="yes" xml:space="preserve">
          <source>Returns a pretty-printed representation (as valid Python syntax) of the internal graph for the &lt;code&gt;forward&lt;/code&gt; method. See &lt;a href=&quot;../jit#inspecting-code&quot;&gt;Inspecting Code&lt;/a&gt; for details.</source>
          <target state="translated">返回 &lt;code&gt;forward&lt;/code&gt; 方法的内部图的漂亮打印表示形式（作为有效的Python语法）。有关详细信息，请参见&lt;a href=&quot;../jit#inspecting-code&quot;&gt;检查代码&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="21aab95a18e188886df073751e7d106cd6f9db59" translate="yes" xml:space="preserve">
          <source>Returns a random permutation of integers from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;n - 1&lt;/code&gt;.</source>
          <target state="translated">返回从 &lt;code&gt;0&lt;/code&gt; 到 &lt;code&gt;n - 1&lt;/code&gt; 的整数的随机排列。</target>
        </trans-unit>
        <trans-unit id="af613cf5c56817cafaa6ac101470a6f4a4596731" translate="yes" xml:space="preserve">
          <source>Returns a result tensor where each</source>
          <target state="translated">Returns a result tensor where each</target>
        </trans-unit>
        <trans-unit id="38827e466d04bcb489dba158a3d8570409e939d7" translate="yes" xml:space="preserve">
          <source>Returns a set of sharing strategies supported on a current system.</source>
          <target state="translated">返回当前系统支持的一组共享策略。</target>
        </trans-unit>
        <trans-unit id="a40e726bb013fd09820ae8ca46b5c0798f7ebbbb" translate="yes" xml:space="preserve">
          <source>Returns a snapshot of the CUDA memory allocator state across all devices.</source>
          <target state="translated">返回所有设备的CUDA内存分配器状态的快照。</target>
        </trans-unit>
        <trans-unit id="a13c7d88cad174382717397efbd3cfcb039a6695" translate="yes" xml:space="preserve">
          <source>Returns a sparse copy of the tensor. PyTorch supports sparse tensors in &lt;a href=&quot;sparse#sparse-docs&quot;&gt;coordinate format&lt;/a&gt;.</source>
          <target state="translated">返回张量的稀疏副本。PyTorch支持&lt;a href=&quot;sparse#sparse-docs&quot;&gt;坐标格式的&lt;/a&gt;稀疏张量。</target>
        </trans-unit>
        <trans-unit id="d0a96d92340e65cc2a84c22d4d7b7ff9aa5fc4fe" translate="yes" xml:space="preserve">
          <source>Returns a string representation of the internal graph for the &lt;code&gt;forward&lt;/code&gt; method. See &lt;a href=&quot;../jit#interpreting-graphs&quot;&gt;Interpreting Graphs&lt;/a&gt; for details.</source>
          <target state="translated">返回 &lt;code&gt;forward&lt;/code&gt; 方法的内部图的字符串表示形式。有关详细信息，请参见&lt;a href=&quot;../jit#interpreting-graphs&quot;&gt;解释图&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="597e1ef5a6ebc351d6f9d2b405bdd9101ff81520" translate="yes" xml:space="preserve">
          <source>Returns a string representation of the internal graph for the &lt;code&gt;forward&lt;/code&gt; method. This graph will be preprocessed to inline all function and method calls. See &lt;a href=&quot;../jit#interpreting-graphs&quot;&gt;Interpreting Graphs&lt;/a&gt; for details.</source>
          <target state="translated">返回 &lt;code&gt;forward&lt;/code&gt; 方法的内部图的字符串表示形式。该图将被预处理以内联所有函数和方法调用。有关详细信息，请参见&lt;a href=&quot;../jit#interpreting-graphs&quot;&gt;解释图&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="bbb10017036639ab22d4a14313f02dc34ac957a3" translate="yes" xml:space="preserve">
          <source>Returns a tensor containing the indices of all non-zero elements of &lt;code&gt;input&lt;/code&gt;. Each row in the result contains the indices of a non-zero element in &lt;code&gt;input&lt;/code&gt;. The result is sorted lexicographically, with the last index changing the fastest (C-style).</source>
          <target state="translated">返回一个张量，其中包含 &lt;code&gt;input&lt;/code&gt; 的所有非零元素的索引。结果中的每一行都包含 &lt;code&gt;input&lt;/code&gt; 中非零元素的索引。结果按字典顺序排序，最后一个索引更改最快（C样式）。</target>
        </trans-unit>
        <trans-unit id="92387ca99fbc648117949c0806c35a077690e5b2" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with random integers generated uniformly between &lt;code&gt;low&lt;/code&gt; (inclusive) and &lt;code&gt;high&lt;/code&gt; (exclusive).</source>
          <target state="translated">返回一个张量，该张量填充在 &lt;code&gt;low&lt;/code&gt; （包含）和 &lt;code&gt;high&lt;/code&gt; （不含）之间均匀生成的随机整数。</target>
        </trans-unit>
        <trans-unit id="a3052366ee17d89fee66f156e0077cbbe80ebcf2" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with random numbers from a normal distribution with mean &lt;code&gt;0&lt;/code&gt; and variance &lt;code&gt;1&lt;/code&gt; (also called the standard normal distribution).</source>
          <target state="translated">从平均值为 &lt;code&gt;0&lt;/code&gt; 且方差为 &lt;code&gt;1&lt;/code&gt; 的正态分布（也称为标准正态分布）中返回一个填充有随机数的张量。</target>
        </trans-unit>
        <trans-unit id="071f27bb3cbfbdac0a3664bb5e1e9b570135a766" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with random numbers from a uniform distribution on the interval</source>
          <target state="translated">返回一个由区间上均匀分布的随机数填充的张量。</target>
        </trans-unit>
        <trans-unit id="f6ccb0d318d2aea6d35f4cd94e4134bbdd4f3636" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with the scalar value &lt;code&gt;0&lt;/code&gt;, with the same size as &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回一个填充有标量值 &lt;code&gt;0&lt;/code&gt; 的张量，其大小与 &lt;code&gt;input&lt;/code&gt; 相同。</target>
        </trans-unit>
        <trans-unit id="5053ad71aa6f4e2e9cdde0c5980a9be98d8e91ca" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with the scalar value &lt;code&gt;0&lt;/code&gt;, with the same size as &lt;code&gt;input&lt;/code&gt;. &lt;code&gt;torch.zeros_like(input)&lt;/code&gt; is equivalent to &lt;code&gt;torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt;.</source>
          <target state="translated">返回一个填充有标量值 &lt;code&gt;0&lt;/code&gt; 的张量，其大小与 &lt;code&gt;input&lt;/code&gt; 相同。 &lt;code&gt;torch.zeros_like(input)&lt;/code&gt; 等效于 &lt;code&gt;torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="53e8d496bf6d2306beb97ee488fa23a9b84ced51" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with the scalar value &lt;code&gt;0&lt;/code&gt;, with the shape defined by the variable argument &lt;code&gt;size&lt;/code&gt;.</source>
          <target state="translated">返回一个由标量值 &lt;code&gt;0&lt;/code&gt; 填充的张量，其形状由可变参数 &lt;code&gt;size&lt;/code&gt; 定义。</target>
        </trans-unit>
        <trans-unit id="19bc36fafde43e2e6ce27a396db27c69f93641b5" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with the scalar value &lt;code&gt;1&lt;/code&gt;, with the same size as &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回一个填充有标量值 &lt;code&gt;1&lt;/code&gt; 的张量，其大小与 &lt;code&gt;input&lt;/code&gt; 相同。</target>
        </trans-unit>
        <trans-unit id="45f89a095c06d9b0650d201a4622d88088b3f6da" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with the scalar value &lt;code&gt;1&lt;/code&gt;, with the same size as &lt;code&gt;input&lt;/code&gt;. &lt;code&gt;torch.ones_like(input)&lt;/code&gt; is equivalent to &lt;code&gt;torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt;.</source>
          <target state="translated">返回一个填充有标量值 &lt;code&gt;1&lt;/code&gt; 的张量，其大小与 &lt;code&gt;input&lt;/code&gt; 相同。 &lt;code&gt;torch.ones_like(input)&lt;/code&gt; 等效于 &lt;code&gt;torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="380e4f75952135cee58ac0cb34dec6379c82103a" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with the scalar value &lt;code&gt;1&lt;/code&gt;, with the shape defined by the variable argument &lt;code&gt;size&lt;/code&gt;.</source>
          <target state="translated">返回一个由标量值 &lt;code&gt;1&lt;/code&gt; 填充的张量，其形状由可变参数 &lt;code&gt;size&lt;/code&gt; 定义。</target>
        </trans-unit>
        <trans-unit id="2619f65f9f0c07b8f2554ca2a356bbee1fb49067" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with uninitialized data.</source>
          <target state="translated">返回一个充满未初始化数据的张量。</target>
        </trans-unit>
        <trans-unit id="6c9d253aa1bcbf44289c2a22a9efa2be86cdae22" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with uninitialized data. The shape and strides of the tensor is defined by the variable argument &lt;code&gt;size&lt;/code&gt; and &lt;code&gt;stride&lt;/code&gt; respectively. &lt;code&gt;torch.empty_strided(size, stride)&lt;/code&gt; is equivalent to &lt;code&gt;torch.empty(size).as_strided(size, stride)&lt;/code&gt;.</source>
          <target state="translated">返回填充有未初始化数据的张量。张量的形状和 &lt;code&gt;stride&lt;/code&gt; 分别由可变的参数 &lt;code&gt;size&lt;/code&gt; 和步幅定义。 &lt;code&gt;torch.empty_strided(size, stride)&lt;/code&gt; 等效于 &lt;code&gt;torch.empty(size).as_strided(size, stride)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="480a441694f9fb4e384f61d0c285c3346aba2867" translate="yes" xml:space="preserve">
          <source>Returns a tensor filled with uninitialized data. The shape of the tensor is defined by the variable argument &lt;code&gt;size&lt;/code&gt;.</source>
          <target state="translated">返回填充有未初始化数据的张量。张量的形状由可变的参数 &lt;code&gt;size&lt;/code&gt; 定义。</target>
        </trans-unit>
        <trans-unit id="5a9df9f8590e35dc409160c4e4da50e8473fc470" translate="yes" xml:space="preserve">
          <source>Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.</source>
          <target state="translated">返回一个随机数的张量,该随机数是从给定的均值和标准差的独立正态分布中提取的。</target>
        </trans-unit>
        <trans-unit id="ef9879439829f78c493eca54baa6bd3b9da4b6a9" translate="yes" xml:space="preserve">
          <source>Returns a tensor of the same size as &lt;code&gt;input&lt;/code&gt; with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in &lt;code&gt;input&lt;/code&gt; i.e.,</source>
          <target state="translated">返回与 &lt;code&gt;input&lt;/code&gt; 大小相同的张量，其中每个元素都是从泊松分布中采样的，其速率参数由 &lt;code&gt;input&lt;/code&gt; 相应元素给出，即</target>
        </trans-unit>
        <trans-unit id="dee1e5e90adcef58cad4a12a8016b3c535cc3f0d" translate="yes" xml:space="preserve">
          <source>Returns a tensor that is a transposed version of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回张量，它是 &lt;code&gt;input&lt;/code&gt; 的转置版本。</target>
        </trans-unit>
        <trans-unit id="3c3720e104640e1187905d7635a3f6f3703b6e8e" translate="yes" xml:space="preserve">
          <source>Returns a tensor that is a transposed version of &lt;code&gt;input&lt;/code&gt;. The given dimensions &lt;code&gt;dim0&lt;/code&gt; and &lt;code&gt;dim1&lt;/code&gt; are swapped.</source>
          <target state="translated">返回张量，它是 &lt;code&gt;input&lt;/code&gt; 的转置版本。给定的尺寸 &lt;code&gt;dim0&lt;/code&gt; 和 &lt;code&gt;dim1&lt;/code&gt; 被交换。</target>
        </trans-unit>
        <trans-unit id="bc2a3afc167973637c7f4b6c69bc81e1a7b7c261" translate="yes" xml:space="preserve">
          <source>Returns a tensor where each row contains &lt;code&gt;num_samples&lt;/code&gt; indices sampled from the multinomial probability distribution located in the corresponding row of tensor &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回一个张量，其中每行包含 &lt;code&gt;num_samples&lt;/code&gt; 个索引，该索引是从位于张量 &lt;code&gt;input&lt;/code&gt; 的相应行中的多项式概率分布中采样的。</target>
        </trans-unit>
        <trans-unit id="bfd4b9d9504c67da6e3c81783a008d55415316b9" translate="yes" xml:space="preserve">
          <source>Returns a tensor where each sub-tensor of &lt;code&gt;input&lt;/code&gt; along dimension &lt;code&gt;dim&lt;/code&gt; is normalized such that the &lt;code&gt;p&lt;/code&gt;-norm of the sub-tensor is lower than the value &lt;code&gt;maxnorm&lt;/code&gt;</source>
          <target state="translated">返回一个张量，其中沿着维度 &lt;code&gt;dim&lt;/code&gt; 的 &lt;code&gt;input&lt;/code&gt; 每个子张量均被规范化，从而子张量的 &lt;code&gt;p&lt;/code&gt; -norm小于 &lt;code&gt;maxnorm&lt;/code&gt; 值</target>
        </trans-unit>
        <trans-unit id="a3a6b5909320cbc074af2544b9626c6f4bda5986" translate="yes" xml:space="preserve">
          <source>Returns a tensor with all the dimensions of &lt;code&gt;input&lt;/code&gt; of size &lt;code&gt;1&lt;/code&gt; removed.</source>
          <target state="translated">返回一个张量，其中大小为 &lt;code&gt;1&lt;/code&gt; 的 &lt;code&gt;input&lt;/code&gt; 的所有维都已删除。</target>
        </trans-unit>
        <trans-unit id="d4dca01252775c3a510432e13b2a50bd68aaad47" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same data and number of elements as &lt;code&gt;input&lt;/code&gt;, but with the specified shape.</source>
          <target state="translated">返回具有与 &lt;code&gt;input&lt;/code&gt; 相同的数据和元素数量，但具有指定形状的张量。</target>
        </trans-unit>
        <trans-unit id="10ea5ae4aa9d17fa5857d769438a3aa3d3b7e12d" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same data and number of elements as &lt;code&gt;input&lt;/code&gt;, but with the specified shape. When possible, the returned tensor will be a view of &lt;code&gt;input&lt;/code&gt;. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.</source>
          <target state="translated">返回具有与 &lt;code&gt;input&lt;/code&gt; 相同的数据和元素数量，但具有指定形状的张量。如果可能，返回的张量将是 &lt;code&gt;input&lt;/code&gt; 的视图。否则，它将是副本。连续输入和具有兼容步幅的输入可以在不复制的情况下进行重塑，但是您不应该依赖复制与查看行为。</target>
        </trans-unit>
        <trans-unit id="859366eeae9c89a2d4641ddcda6c14627157bccd" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same data and number of elements as &lt;code&gt;self&lt;/code&gt; but with the specified shape. This method returns a view if &lt;code&gt;shape&lt;/code&gt; is compatible with the current shape. See &lt;a href=&quot;#torch.Tensor.view&quot;&gt;&lt;code&gt;torch.Tensor.view()&lt;/code&gt;&lt;/a&gt; on when it is possible to return a view.</source>
          <target state="translated">返回具有与 &lt;code&gt;self&lt;/code&gt; 相同的数据和元素数量但具有指定形状的张量。如果 &lt;code&gt;shape&lt;/code&gt; 与当前形状兼容，则此方法返回一个视图。何时可以返回视图，请参见&lt;a href=&quot;#torch.Tensor.view&quot;&gt; &lt;code&gt;torch.Tensor.view()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4ddaf7cc67581deb40ac49cd798acd828948f3ec" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same shape as Tensor &lt;code&gt;input&lt;/code&gt; filled with random integers generated uniformly between &lt;code&gt;low&lt;/code&gt; (inclusive) and &lt;code&gt;high&lt;/code&gt; (exclusive).</source>
          <target state="translated">返回具有与Tensor &lt;code&gt;input&lt;/code&gt; 相同形状的张量，其中填充了均匀地在 &lt;code&gt;low&lt;/code&gt; （包含）和 &lt;code&gt;high&lt;/code&gt; （不含）之间生成的随机整数。</target>
        </trans-unit>
        <trans-unit id="f4d7f431450f08dca6e6bf02ccea73d644849cfb" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same size as &lt;code&gt;input&lt;/code&gt; filled with &lt;code&gt;fill_value&lt;/code&gt;.</source>
          <target state="translated">返回与填充 &lt;code&gt;fill_value&lt;/code&gt; 的 &lt;code&gt;input&lt;/code&gt; 具有相同大小的张量。</target>
        </trans-unit>
        <trans-unit id="03ce7caeebb968b62c8d34b90b828d1ce8423f2f" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same size as &lt;code&gt;input&lt;/code&gt; filled with &lt;code&gt;fill_value&lt;/code&gt;. &lt;code&gt;torch.full_like(input, fill_value)&lt;/code&gt; is equivalent to &lt;code&gt;torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt;.</source>
          <target state="translated">返回与填充 &lt;code&gt;fill_value&lt;/code&gt; 的 &lt;code&gt;input&lt;/code&gt; 具有相同大小的张量。 &lt;code&gt;torch.full_like(input, fill_value)&lt;/code&gt; 等效于 &lt;code&gt;torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c4b1fea8767410865166f5ee9867a018bc11da8d" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same size as &lt;code&gt;input&lt;/code&gt; that is filled with random numbers from a normal distribution with mean 0 and variance 1.</source>
          <target state="translated">返回与 &lt;code&gt;input&lt;/code&gt; 具有相同大小的张量，该张量由均值0和方差1的正态分布的随机数填充。</target>
        </trans-unit>
        <trans-unit id="ea94290136057228a379eae661032b4dbc67bcaf" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same size as &lt;code&gt;input&lt;/code&gt; that is filled with random numbers from a normal distribution with mean 0 and variance 1. &lt;code&gt;torch.randn_like(input)&lt;/code&gt; is equivalent to &lt;code&gt;torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt;.</source>
          <target state="translated">返回具有相同大小的张量 &lt;code&gt;input&lt;/code&gt; 填充有随机数从正态分布均值为0，方差为1。 &lt;code&gt;torch.randn_like(input)&lt;/code&gt; 等效于 &lt;code&gt;torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0152430be2c09d5c5103cb904fde37d1ace3b55a" translate="yes" xml:space="preserve">
          <source>Returns a tensor with the same size as &lt;code&gt;input&lt;/code&gt; that is filled with random numbers from a uniform distribution on the interval</source>
          <target state="translated">返回与 &lt;code&gt;input&lt;/code&gt; 大小相同的张量，该张量由区间上均匀分布的随机数填充</target>
        </trans-unit>
        <trans-unit id="823c3c765d13781fa9b963395a22444e3b621c72" translate="yes" xml:space="preserve">
          <source>Returns a tuple of 1-D tensors, one for each dimension in &lt;code&gt;input&lt;/code&gt;, each containing the indices (in that dimension) of all non-zero elements of &lt;code&gt;input&lt;/code&gt; .</source>
          <target state="translated">返回的1-d张量，一个用于在每个维度上的元组 &lt;code&gt;input&lt;/code&gt; ，各自包含的所有非零元素的索引（该维度） &lt;code&gt;input&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="fcd5e2fc8161aab0c115280174b5bb6b68911439" translate="yes" xml:space="preserve">
          <source>Returns a tuple of all slices along a given dimension, already without it.</source>
          <target state="translated">返回一个沿给定维度的所有切片的元组,已经没有它。</target>
        </trans-unit>
        <trans-unit id="f9c24c4caad4a1ea9a709ef4d25a5add23c5abc6" translate="yes" xml:space="preserve">
          <source>Returns a tuple of tensors as &lt;code&gt;(the pivots, the L tensor, the U tensor)&lt;/code&gt;.</source>
          <target state="translated">返回张量的元组为 &lt;code&gt;(the pivots, the L tensor, the U tensor)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="cad07505f53debb40a34d79ccc9f58807d93a22d" translate="yes" xml:space="preserve">
          <source>Returns a tuple of:</source>
          <target state="translated">返回一个元组。</target>
        </trans-unit>
        <trans-unit id="48ec84de6060a777e4b133c0cc4cbb5c195b63f4" translate="yes" xml:space="preserve">
          <source>Returns a view of &lt;code&gt;input&lt;/code&gt; as a complex tensor.</source>
          <target state="translated">以复数张量形式返回 &lt;code&gt;input&lt;/code&gt; 的视图。</target>
        </trans-unit>
        <trans-unit id="cbd534c7cdca6e762bf6f1efb8dbc311c8d5604b" translate="yes" xml:space="preserve">
          <source>Returns a view of &lt;code&gt;input&lt;/code&gt; as a complex tensor. For an input complex tensor of &lt;code&gt;size&lt;/code&gt;</source>
          <target state="translated">以复数张量形式返回 &lt;code&gt;input&lt;/code&gt; 的视图。对于 &lt;code&gt;size&lt;/code&gt; 为输入的复数张量</target>
        </trans-unit>
        <trans-unit id="d5219316aa3ad16a6268bf6cb076669b071e63b0" translate="yes" xml:space="preserve">
          <source>Returns a view of &lt;code&gt;input&lt;/code&gt; as a real tensor.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 的视图作为真实张量。</target>
        </trans-unit>
        <trans-unit id="feec717f015dd3a3543847bb4922909ce0df9768" translate="yes" xml:space="preserve">
          <source>Returns a view of &lt;code&gt;input&lt;/code&gt; as a real tensor. For an input complex tensor of &lt;code&gt;size&lt;/code&gt;</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 的视图作为真实张量。对于 &lt;code&gt;size&lt;/code&gt; 为输入的复数张量</target>
        </trans-unit>
        <trans-unit id="b19c812d92695d71cc123c4309dfdfb970e00990" translate="yes" xml:space="preserve">
          <source>Returns a view of the original tensor which contains all slices of size &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt; from &lt;code&gt;self&lt;/code&gt; tensor in the dimension &lt;code&gt;dimension&lt;/code&gt;.</source>
          <target state="translated">返回原始张量的视图，其中包含来自维度 &lt;code&gt;dimension&lt;/code&gt; &lt;code&gt;self&lt;/code&gt; 张量的所有&lt;a href=&quot;#torch.Tensor.size&quot;&gt; &lt;code&gt;size&lt;/code&gt; &lt;/a&gt;为切片的切片。</target>
        </trans-unit>
        <trans-unit id="af681be32c7d06253ca1c1b67c18e2b4ecd6f876" translate="yes" xml:space="preserve">
          <source>Returns a view of the original tensor with its dimensions permuted.</source>
          <target state="translated">返回原始张量的视图,并将其尺寸进行了微调。</target>
        </trans-unit>
        <trans-unit id="92e60f922f107927ce27990d6a2076704c10ee26" translate="yes" xml:space="preserve">
          <source>Returns an IPC handle of this event. If not recorded yet, the event will use the current device.</source>
          <target state="translated">返回该事件的IPC句柄。如果还没有记录,事件将使用当前设备。</target>
        </trans-unit>
        <trans-unit id="c55cff2ceacf5489627870098b732610149ca753" translate="yes" xml:space="preserve">
          <source>Returns an fp32 Tensor by dequantizing a quantized Tensor</source>
          <target state="translated">通过去量化张量器返回一个fp32张量器。</target>
        </trans-unit>
        <trans-unit id="8a760468481f662aec69f0d66a0af21b8d01fcc9" translate="yes" xml:space="preserve">
          <source>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</source>
          <target state="translated">返回网络中所有模块的迭代器,产生模块的名称和模块本身。</target>
        </trans-unit>
        <trans-unit id="ac68eb8bdb577f36ea46f58e6159bc7bdc815476" translate="yes" xml:space="preserve">
          <source>Returns an iterator over all modules in the network.</source>
          <target state="translated">返回网络中所有模块的迭代器。</target>
        </trans-unit>
        <trans-unit id="94fc7e2a384a791953b1e205101832ccf08471ae" translate="yes" xml:space="preserve">
          <source>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</source>
          <target state="translated">返回对直系子模块的迭代器,产生模块的名称和模块本身。</target>
        </trans-unit>
        <trans-unit id="06d88a90e624553bccb897dec896158a9e8cbe10" translate="yes" xml:space="preserve">
          <source>Returns an iterator over immediate children modules.</source>
          <target state="translated">返回直系子模块的迭代器。</target>
        </trans-unit>
        <trans-unit id="710ada787bca81e41a3d40e51f40a7c77bf13dde" translate="yes" xml:space="preserve">
          <source>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</source>
          <target state="translated">返回模块缓冲区的迭代器,产生缓冲区的名称和缓冲区本身。</target>
        </trans-unit>
        <trans-unit id="6eff830e357832045671934c63ceb5cb2fcb5978" translate="yes" xml:space="preserve">
          <source>Returns an iterator over module buffers.</source>
          <target state="translated">返回模块缓冲区的迭代器。</target>
        </trans-unit>
        <trans-unit id="b1b975207d4ac5172ef30d4d877fa0c9ee98199a" translate="yes" xml:space="preserve">
          <source>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</source>
          <target state="translated">返回模块参数的迭代器,产生参数的名称和参数本身。</target>
        </trans-unit>
        <trans-unit id="aa5e4dda23bfdd55410c837a325f1f9ec0f370cb" translate="yes" xml:space="preserve">
          <source>Returns an iterator over module parameters.</source>
          <target state="translated">返回模块参数的迭代器。</target>
        </trans-unit>
        <trans-unit id="48fce4dac8987007e7e689346df86ef5aaaf8d0c" translate="yes" xml:space="preserve">
          <source>Returns an uninitialized tensor with the same size as &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">返回与 &lt;code&gt;input&lt;/code&gt; 大小相同的未初始化张量。</target>
        </trans-unit>
        <trans-unit id="25a01c60e025b196abbdae1e228f044292cc80b3" translate="yes" xml:space="preserve">
          <source>Returns an uninitialized tensor with the same size as &lt;code&gt;input&lt;/code&gt;. &lt;code&gt;torch.empty_like(input)&lt;/code&gt; is equivalent to &lt;code&gt;torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt;.</source>
          <target state="translated">返回与 &lt;code&gt;input&lt;/code&gt; 大小相同的未初始化张量。 &lt;code&gt;torch.empty_like(input)&lt;/code&gt; 等效于 &lt;code&gt;torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ab487b617ce0598b5cf2a7578485a234f9dec28c" translate="yes" xml:space="preserve">
          <source>Returns cosine similarity between</source>
          <target state="translated">之间的余弦相似度的结果。</target>
        </trans-unit>
        <trans-unit id="e36c4e3a7a9f1d367dee6c46085960ff3fa5e136" translate="yes" xml:space="preserve">
          <source>Returns cosine similarity between x1 and x2, computed along dim.</source>
          <target state="translated">Returns cosine similarity between x1 and x2,computed along dim。</target>
        </trans-unit>
        <trans-unit id="e9ee7bb84b7690f6d9a479786aae93fb8de0a027" translate="yes" xml:space="preserve">
          <source>Returns cublasHandle_t pointer to current cuBLAS handle</source>
          <target state="translated">返回指向当前cuBLAS句柄的cublasHandle_t指针。</target>
        </trans-unit>
        <trans-unit id="415079d9dd9bf0c3a5f12e73dab28b75c28128b7" translate="yes" xml:space="preserve">
          <source>Returns either a complex tensor of size</source>
          <target state="translated">返回一个大小为</target>
        </trans-unit>
        <trans-unit id="a4b4474f9674a58aa04ea930d85c7a5dc2c0abdc" translate="yes" xml:space="preserve">
          <source>Returns entropy of distribution, batched over batch_shape.</source>
          <target state="translated">Returns entropy of distribution,batched over batch_shape.</target>
        </trans-unit>
        <trans-unit id="dfc3ffdf3e62461b544a4b95b691f57d57a7486b" translate="yes" xml:space="preserve">
          <source>Returns list CUDA architectures this library was compiled for.</source>
          <target state="translated">Returns list CUDA architectures this library was compiled for.</target>
        </trans-unit>
        <trans-unit id="efcc116701cc62cfdac8737d78b2800ae566068b" translate="yes" xml:space="preserve">
          <source>Returns perplexity of distribution, batched over batch_shape.</source>
          <target state="translated">Returns perplexity of distribution,batched over batch_shape.</target>
        </trans-unit>
        <trans-unit id="bdbba2c4c769cf86fe1ade7dbe8c1a8c12ab8f3b" translate="yes" xml:space="preserve">
          <source>Returns scaled outputs. If this instance of &lt;a href=&quot;#torch.cuda.amp.GradScaler&quot;&gt;&lt;code&gt;GradScaler&lt;/code&gt;&lt;/a&gt; is not enabled, outputs are returned unmodified.</source>
          <target state="translated">返回缩放的输出。如果未启用此&lt;a href=&quot;#torch.cuda.amp.GradScaler&quot;&gt; &lt;code&gt;GradScaler&lt;/code&gt; &lt;/a&gt;实例，则返回的输出将保持不变。</target>
        </trans-unit>
        <trans-unit id="468f8e57e6feeaa4ca9cfb85917c23990eabfb8c" translate="yes" xml:space="preserve">
          <source>Returns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be &lt;code&gt;(cardinality,) + batch_shape + event_shape&lt;/code&gt; (where &lt;code&gt;event_shape = ()&lt;/code&gt; for univariate distributions).</source>
          <target state="translated">返回包含离散分布支持的所有值的张量。结果将在维度0上枚举，因此结果的形状将为 &lt;code&gt;(cardinality,) + batch_shape + event_shape&lt;/code&gt; （其中单变量分布的 &lt;code&gt;event_shape = ()&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="d89b2ba3d69dbf0c5536d8dba9d4a8221b96833a" translate="yes" xml:space="preserve">
          <source>Returns the &lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; that would result from performing an arithmetic operation on the provided input tensors. See type promotion &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;documentation&lt;/a&gt; for more information on the type promotion logic.</source>
          <target state="translated">返回在提供的张量上执行算术运算而得到的&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;。有关类型升级逻辑的更多信息，请参见类型升级&lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;文档&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="3a64cb8b4d560048c2d03eaad56133c0e7564a88" translate="yes" xml:space="preserve">
          <source>Returns the &lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; with the smallest size and scalar kind that is not smaller nor of lower kind than either &lt;code&gt;type1&lt;/code&gt; or &lt;code&gt;type2&lt;/code&gt;. See type promotion &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;documentation&lt;/a&gt; for more information on the type promotion logic.</source>
          <target state="translated">返回具有最小大小和标量种类的&lt;a href=&quot;../tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;，该种类和大小不小于 &lt;code&gt;type1&lt;/code&gt; 或 &lt;code&gt;type2&lt;/code&gt; 。有关类型升级逻辑的更多信息，请参见类型升级&lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;文档&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="615a0842a40834e6013343600f3be5a6a5475da4" translate="yes" xml:space="preserve">
          <source>Returns the &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; that would result from performing an arithmetic operation on the provided input tensors.</source>
          <target state="translated">返回在提供的张量上执行算术运算而得到的&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6d1fdb76adea0e220c4916182503a08888092e2a" translate="yes" xml:space="preserve">
          <source>Returns the &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; with the smallest size and scalar kind that is not smaller nor of lower kind than either &lt;code&gt;type1&lt;/code&gt; or &lt;code&gt;type2&lt;/code&gt;.</source>
          <target state="translated">返回具有最小大小和标量种类的&lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt;，该种类和大小不小于 &lt;code&gt;type1&lt;/code&gt; 或 &lt;code&gt;type2&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="bf82a1ecc16fe682cb2b7f0a986e6561051951df" translate="yes" xml:space="preserve">
          <source>Returns the &lt;code&gt;k&lt;/code&gt; largest elements of the given &lt;code&gt;input&lt;/code&gt; tensor along a given dimension.</source>
          <target state="translated">返回沿给定维度的给定 &lt;code&gt;input&lt;/code&gt; 张量的 &lt;code&gt;k&lt;/code&gt; 个最大元素。</target>
        </trans-unit>
        <trans-unit id="8bed4d210c184a7836660a5c8e790f223aef9984" translate="yes" xml:space="preserve">
          <source>Returns the Generator state as a &lt;code&gt;torch.ByteTensor&lt;/code&gt;.</source>
          <target state="translated">以 &lt;code&gt;torch.ByteTensor&lt;/code&gt; 的形式返回Generator的状态。</target>
        </trans-unit>
        <trans-unit id="b326f89b0009d9a68d7ef03ea15be0ad2d1ff47f" translate="yes" xml:space="preserve">
          <source>Returns the LU solve of the linear system</source>
          <target state="translated">返回线性系统的LU解</target>
        </trans-unit>
        <trans-unit id="f7e41a542d6bed1cb096c1ed65295fbd9d0b0caa" translate="yes" xml:space="preserve">
          <source>Returns the address of the first element of &lt;code&gt;self&lt;/code&gt; tensor.</source>
          <target state="translated">返回 &lt;code&gt;self&lt;/code&gt; 张量的第一个元素的地址。</target>
        </trans-unit>
        <trans-unit id="2dcc3de7b8c8581e1cdbe092b04332f6662ee7fa" translate="yes" xml:space="preserve">
          <source>Returns the backend of the given process group.</source>
          <target state="translated">返回给定流程组的后端。</target>
        </trans-unit>
        <trans-unit id="0ff2118aeefe8e264108f6bd90694f18e03d42e7" translate="yes" xml:space="preserve">
          <source>Returns the cross product of vectors in dimension &lt;code&gt;dim&lt;/code&gt; of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 和 &lt;code&gt;other&lt;/code&gt; 的尺寸为 &lt;code&gt;dim&lt;/code&gt; 的向量的叉积。</target>
        </trans-unit>
        <trans-unit id="ac3e83f04f1e5b5b04b0171c84cf8b30b1da0e51" translate="yes" xml:space="preserve">
          <source>Returns the cumulative density/mass function evaluated at &lt;code&gt;value&lt;/code&gt;.</source>
          <target state="translated">返回以 &lt;code&gt;value&lt;/code&gt; 评估的累积密度/质量函数。</target>
        </trans-unit>
        <trans-unit id="3ea80ae8957ba72ac8ddc6e085e6f544e8ae8c05" translate="yes" xml:space="preserve">
          <source>Returns the cumulative product of elements of &lt;code&gt;input&lt;/code&gt; in the dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">返回尺寸为 &lt;code&gt;dim&lt;/code&gt; 的 &lt;code&gt;input&lt;/code&gt; 元素的累积积。</target>
        </trans-unit>
        <trans-unit id="672aa95c90df23603089d78e5a986ac0c7ee6f37" translate="yes" xml:space="preserve">
          <source>Returns the cumulative sum of elements of &lt;code&gt;input&lt;/code&gt; in the dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">返回维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 元素的累积和。</target>
        </trans-unit>
        <trans-unit id="543b4e544f696735e42e91a48b1a05b0d0275c5e" translate="yes" xml:space="preserve">
          <source>Returns the current GPU memory managed by the caching allocator in bytes for a given device.</source>
          <target state="translated">Returns the current GPU memory managed by the caching allocator for a given device (以字节为单位).</target>
        </trans-unit>
        <trans-unit id="353af166fcd46553479e01ad58f7b83a7e79e6b9" translate="yes" xml:space="preserve">
          <source>Returns the current GPU memory occupied by tensors in bytes for a given device.</source>
          <target state="translated">Returns the current GPU memory occupied by tensors for a given device,bytes.</target>
        </trans-unit>
        <trans-unit id="96ed2eb11bac807a9796664f42b21924320e0776" translate="yes" xml:space="preserve">
          <source>Returns the current random seed of the current GPU.</source>
          <target state="translated">返回当前GPU的随机种子。</target>
        </trans-unit>
        <trans-unit id="e5e450695734676a7d659d903827db864e4c16d8" translate="yes" xml:space="preserve">
          <source>Returns the current strategy for sharing CPU tensors.</source>
          <target state="translated">返回当前共享CPU tensors的策略。</target>
        </trans-unit>
        <trans-unit id="67bc3c1de929ca35a480af3381de6be2f62d0bd8" translate="yes" xml:space="preserve">
          <source>Returns the currently selected &lt;a href=&quot;#torch.cuda.Stream&quot;&gt;&lt;code&gt;Stream&lt;/code&gt;&lt;/a&gt; for a given device.</source>
          <target state="translated">返回给定设备当前选择的&lt;a href=&quot;#torch.cuda.Stream&quot;&gt; &lt;code&gt;Stream&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="214e82956aaf7597b71c309353db304a3f55c7e8" translate="yes" xml:space="preserve">
          <source>Returns the default &lt;a href=&quot;#torch.cuda.Stream&quot;&gt;&lt;code&gt;Stream&lt;/code&gt;&lt;/a&gt; for a given device.</source>
          <target state="translated">返回给定设备的默认&lt;a href=&quot;#torch.cuda.Stream&quot;&gt; &lt;code&gt;Stream&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="59028083782bdbe4c0f1ae0141a2a0d44a986b01" translate="yes" xml:space="preserve">
          <source>Returns the index of a currently selected device.</source>
          <target state="translated">返回当前选定设备的索引。</target>
        </trans-unit>
        <trans-unit id="711b80d2744d16d4fcb6dea10d90a382b486d415" translate="yes" xml:space="preserve">
          <source>Returns the indices of the buckets to which each value in the &lt;code&gt;input&lt;/code&gt; belongs, where the boundaries of the buckets are set by &lt;code&gt;boundaries&lt;/code&gt;.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 每个值所属的存储桶的索引，存储桶的边界由 &lt;code&gt;boundaries&lt;/code&gt; 设置。</target>
        </trans-unit>
        <trans-unit id="8f4a40b7b47151439ff4a946635582981d1e9915" translate="yes" xml:space="preserve">
          <source>Returns the indices of the buckets to which each value in the &lt;code&gt;input&lt;/code&gt; belongs, where the boundaries of the buckets are set by &lt;code&gt;boundaries&lt;/code&gt;. Return a new tensor with the same size as &lt;code&gt;input&lt;/code&gt;. If &lt;code&gt;right&lt;/code&gt; is False (default), then the left boundary is closed. More formally, the returned index satisfies the following rules:</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 每个值所属的存储桶的索引，存储桶的边界由 &lt;code&gt;boundaries&lt;/code&gt; 设置。返回一个具有与 &lt;code&gt;input&lt;/code&gt; 相同大小的新张量。如果 &lt;code&gt;right&lt;/code&gt; 为False（默认值），则左侧边界关闭。更正式地说，返回的索引满足以下规则：</target>
        </trans-unit>
        <trans-unit id="8e89c9544d1eb7458a38f74393c854c8ecda06cb" translate="yes" xml:space="preserve">
          <source>Returns the indices of the lower triangular part of a &lt;code&gt;row&lt;/code&gt;-by- &lt;code&gt;col&lt;/code&gt; matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.</source>
          <target state="translated">返回的下三角部分的索引 &lt;code&gt;row&lt;/code&gt; -by- &lt;code&gt;col&lt;/code&gt; 矩阵在一个2-N张量，其中，第一行包含所有指数和第二行包含列坐标的行的坐标。</target>
        </trans-unit>
        <trans-unit id="2a481df57e472778d81eb9d047f3b776a831dacb" translate="yes" xml:space="preserve">
          <source>Returns the indices of the lower triangular part of a &lt;code&gt;row&lt;/code&gt;-by- &lt;code&gt;col&lt;/code&gt; matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates. Indices are ordered based on rows and then columns.</source>
          <target state="translated">返回的下三角部分的索引 &lt;code&gt;row&lt;/code&gt; -by- &lt;code&gt;col&lt;/code&gt; 矩阵在一个2-N张量，其中，第一行包含所有指数和第二行包含列坐标的行的坐标。索引是根据行然后按列进行排序的。</target>
        </trans-unit>
        <trans-unit id="36d1fcc471f42fd072f6a49d3437bd2fb723f49b" translate="yes" xml:space="preserve">
          <source>Returns the indices of the maximum value of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 张量中所有元素的最大值的索引。</target>
        </trans-unit>
        <trans-unit id="27e8f13c991f81ab6eaf5ea019cec0319d74f97f" translate="yes" xml:space="preserve">
          <source>Returns the indices of the maximum values of a tensor across a dimension.</source>
          <target state="translated">Returns the indices of the maximum values of a tensor across a dimension.Returns the indices of the maximum values across a dimension.</target>
        </trans-unit>
        <trans-unit id="02a6d25f17c18c20395b7dbfe38496f775ac3649" translate="yes" xml:space="preserve">
          <source>Returns the indices of the minimum value of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 张量中所有元素的最小值的索引。</target>
        </trans-unit>
        <trans-unit id="c7c9c6f3bffff8c19d01d21b7330abaf2fed0de4" translate="yes" xml:space="preserve">
          <source>Returns the indices of the minimum values of a tensor across a dimension.</source>
          <target state="translated">返回一个维度上张量的最小值的指数。</target>
        </trans-unit>
        <trans-unit id="6112a2a2de721ce77e0fbceb09a5d99e7c8b1b09" translate="yes" xml:space="preserve">
          <source>Returns the indices of the upper triangular part of a &lt;code&gt;row&lt;/code&gt; by &lt;code&gt;col&lt;/code&gt; matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.</source>
          <target state="translated">返回2乘N张量中按 &lt;code&gt;col&lt;/code&gt; 矩阵的 &lt;code&gt;row&lt;/code&gt; 的上三角部分的索引，其中第一行包含所有索引的行坐标，第二行包含列坐标。</target>
        </trans-unit>
        <trans-unit id="0c610a94f21c0e29b9389ef4636aaf00f7bcfcca" translate="yes" xml:space="preserve">
          <source>Returns the indices of the upper triangular part of a &lt;code&gt;row&lt;/code&gt; by &lt;code&gt;col&lt;/code&gt; matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates. Indices are ordered based on rows and then columns.</source>
          <target state="translated">返回2乘N张量中按 &lt;code&gt;col&lt;/code&gt; 矩阵的 &lt;code&gt;row&lt;/code&gt; 的上三角部分的索引，其中第一行包含所有索引的行坐标，第二行包含列坐标。索引是根据行然后按列进行排序的。</target>
        </trans-unit>
        <trans-unit id="c38cd5cab4c0cbb6855ee937ca43e344a716d2eb" translate="yes" xml:space="preserve">
          <source>Returns the indices that sort a tensor along a given dimension in ascending order by value.</source>
          <target state="translated">返回沿给定维度按升序排列张量值的指数。</target>
        </trans-unit>
        <trans-unit id="12d3b5fef61c4a9ef299d2984d5e7f942e44587a" translate="yes" xml:space="preserve">
          <source>Returns the information about the current &lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt;&lt;code&gt;DataLoader&lt;/code&gt;&lt;/a&gt; iterator worker process.</source>
          <target state="translated">返回有关当前&lt;a href=&quot;#torch.utils.data.DataLoader&quot;&gt; &lt;code&gt;DataLoader&lt;/code&gt; &lt;/a&gt;迭代器工作进程的信息。</target>
        </trans-unit>
        <trans-unit id="528f25a31d271ea8c283f1c3f903d83097bb316f" translate="yes" xml:space="preserve">
          <source>Returns the initial seed for generating random numbers as a Python &lt;code&gt;long&lt;/code&gt;.</source>
          <target state="translated">返回用于生成随机数的Python的 &lt;code&gt;long&lt;/code&gt; 的初始种子。</target>
        </trans-unit>
        <trans-unit id="b3583d38607467b25587c688245c573295c38448" translate="yes" xml:space="preserve">
          <source>Returns the initial seed for generating random numbers.</source>
          <target state="translated">返回生成随机数的初始种子。</target>
        </trans-unit>
        <trans-unit id="d3aa84ac404b56cbb6665579407061f3f1829f6d" translate="yes" xml:space="preserve">
          <source>Returns the inverse &lt;a href=&quot;#torch.distributions.transforms.Transform&quot;&gt;&lt;code&gt;Transform&lt;/code&gt;&lt;/a&gt; of this transform. This should satisfy &lt;code&gt;t.inv.inv is t&lt;/code&gt;.</source>
          <target state="translated">返回逆&lt;a href=&quot;#torch.distributions.transforms.Transform&quot;&gt; &lt;code&gt;Transform&lt;/code&gt; &lt;/a&gt;此变换。这应该满足 &lt;code&gt;t.inv.inv is t&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c1aef1f541873f46619d10b39465c4cebb0a9884" translate="yes" xml:space="preserve">
          <source>Returns the inverse cumulative density/mass function evaluated at &lt;code&gt;value&lt;/code&gt;.</source>
          <target state="translated">返回在 &lt;code&gt;value&lt;/code&gt; 处求值的逆累积密度/质量函数。</target>
        </trans-unit>
        <trans-unit id="f756dfcdc4b88f47cc8afe02c13bb828b05c63e7" translate="yes" xml:space="preserve">
          <source>Returns the log of summed exponentials of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">返回给定维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每一行的总指数对数。</target>
        </trans-unit>
        <trans-unit id="b6389ff7f37373c9ca57e71fc477f0c668bda055" translate="yes" xml:space="preserve">
          <source>Returns the log of summed exponentials of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;. The computation is numerically stabilized.</source>
          <target state="translated">返回给定维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每一行的总指数对数。该计算在数值上是稳定的。</target>
        </trans-unit>
        <trans-unit id="cf79c17bd23741493f06836ca1da65ec3e27f2d9" translate="yes" xml:space="preserve">
          <source>Returns the log of the probability density/mass function evaluated at &lt;code&gt;value&lt;/code&gt;.</source>
          <target state="translated">返回在 &lt;code&gt;value&lt;/code&gt; 处评估的概率密度/质量函数的对数。</target>
        </trans-unit>
        <trans-unit id="d7b57cf20e91ac970ff715302367553ab5d606f0" translate="yes" xml:space="preserve">
          <source>Returns the logarithm of the cumulative summation of the exponentiation of elements of &lt;code&gt;input&lt;/code&gt; in the dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">返回维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 元素的幂的累加总和的对数。</target>
        </trans-unit>
        <trans-unit id="ece733a42c60a3b1bb785f7b96e5ca90ed6c9354" translate="yes" xml:space="preserve">
          <source>Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices &lt;code&gt;input&lt;/code&gt;, the other elements of the result tensor &lt;code&gt;out&lt;/code&gt; are set to 0.</source>
          <target state="translated">返回矩阵（2-D张量）或矩阵 &lt;code&gt;input&lt;/code&gt; 批次的下三角部分，结果张量 &lt;code&gt;out&lt;/code&gt; 的其他元素设置为0。</target>
        </trans-unit>
        <trans-unit id="903bb5ad470677dd96718bed7dd424b4eea58c19" translate="yes" xml:space="preserve">
          <source>Returns the matrix exponential. Supports batched input. For a matrix &lt;code&gt;A&lt;/code&gt;, the matrix exponential is defined as</source>
          <target state="translated">返回矩阵指数。支持批量输入。对于矩阵 &lt;code&gt;A&lt;/code&gt; ，矩阵指数定义为</target>
        </trans-unit>
        <trans-unit id="57632c5d6ea2c48ab4c7ad8c0de751aa58bc8ad6" translate="yes" xml:space="preserve">
          <source>Returns the matrix norm or vector norm of a given tensor.</source>
          <target state="translated">返回给定张量的矩阵法线或矢量法线。</target>
        </trans-unit>
        <trans-unit id="aa8212443e4c775af178e39c5b9d95062b7f280a" translate="yes" xml:space="preserve">
          <source>Returns the matrix product of the</source>
          <target state="translated">Returns the matrix product of the</target>
        </trans-unit>
        <trans-unit id="2056ceddf487b6aeac66141f77f1593dd5d51a33" translate="yes" xml:space="preserve">
          <source>Returns the matrix raised to the power &lt;code&gt;n&lt;/code&gt; for square matrices.</source>
          <target state="translated">返回矩阵幂 &lt;code&gt;n&lt;/code&gt; 为方阵。</target>
        </trans-unit>
        <trans-unit id="7a715df6cd530cefa49cad9ca42363f5dcc48be7" translate="yes" xml:space="preserve">
          <source>Returns the matrix raised to the power &lt;code&gt;n&lt;/code&gt; for square matrices. For batch of matrices, each individual matrix is raised to the power &lt;code&gt;n&lt;/code&gt;.</source>
          <target state="translated">返回矩阵幂 &lt;code&gt;n&lt;/code&gt; 为方阵。对于一批矩阵，将每个单独的矩阵提高到幂 &lt;code&gt;n&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="03824d09bb5c54243f197f9c9ba3e110276ea20d" translate="yes" xml:space="preserve">
          <source>Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.</source>
          <target state="translated">Returns the maximum GPU memory managed by the caching allocator for a given device(以字节为单位)。</target>
        </trans-unit>
        <trans-unit id="0caf8ade3d9c0d603d539d45bc8b1a8d76a7591a" translate="yes" xml:space="preserve">
          <source>Returns the maximum GPU memory occupied by tensors in bytes for a given device.</source>
          <target state="translated">Returns the maximum GPU memory occupied bytes of tensors for a given device.</target>
        </trans-unit>
        <trans-unit id="2592d5f82bbbc822a97e96b68b786a9c78bb926c" translate="yes" xml:space="preserve">
          <source>Returns the maximum value of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 张量中所有元素的最大值。</target>
        </trans-unit>
        <trans-unit id="fa13bd7559d027b3477eb7ddbb196e1171a0bca9" translate="yes" xml:space="preserve">
          <source>Returns the maximum value of each slice of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension(s) &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">返回给定维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每个切片的最大值。</target>
        </trans-unit>
        <trans-unit id="bca8e17597f7810508efae907eae755779bfdb1a" translate="yes" xml:space="preserve">
          <source>Returns the mean of the distribution.</source>
          <target state="translated">返回分布的平均值。</target>
        </trans-unit>
        <trans-unit id="809ee94cff5a85854481052825bc00d99efb4eb6" translate="yes" xml:space="preserve">
          <source>Returns the mean value of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 张量中所有元素的平均值。</target>
        </trans-unit>
        <trans-unit id="b95b8ed42b947df36f019475ad19d862230385ee" translate="yes" xml:space="preserve">
          <source>Returns the mean value of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;. If &lt;code&gt;dim&lt;/code&gt; is a list of dimensions, reduce over all of them.</source>
          <target state="translated">返回给定维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每一行的平均值。如果 &lt;code&gt;dim&lt;/code&gt; 是尺寸列表，请缩小所有尺寸。</target>
        </trans-unit>
        <trans-unit id="f2abf9fb9cf56489f6a00a784d4d458d7895d090" translate="yes" xml:space="preserve">
          <source>Returns the median value of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 张量中所有元素的中值。</target>
        </trans-unit>
        <trans-unit id="8878f4d09befec1d421d046a625ff001883087d7" translate="yes" xml:space="preserve">
          <source>Returns the minimum value of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 张量中所有元素的最小值。</target>
        </trans-unit>
        <trans-unit id="0a0cee886c48025d366605a4d62185c2c9000596" translate="yes" xml:space="preserve">
          <source>Returns the minimum value of each slice of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension(s) &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">返回给定维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每个切片的最小值。</target>
        </trans-unit>
        <trans-unit id="1b3e9ec11a895a649216c83f8ca9adf589921f7c" translate="yes" xml:space="preserve">
          <source>Returns the number of GPUs available.</source>
          <target state="translated">Returns the number of GPU available.</target>
        </trans-unit>
        <trans-unit id="fd09e78e1b1393b57b6e81bd22e6b7089ac87df8" translate="yes" xml:space="preserve">
          <source>Returns the number of dimensions of &lt;code&gt;self&lt;/code&gt; tensor.</source>
          <target state="translated">返回 &lt;code&gt;self&lt;/code&gt; 张量的维数。</target>
        </trans-unit>
        <trans-unit id="074c1198678efc321484286e167225dc00032bc7" translate="yes" xml:space="preserve">
          <source>Returns the number of keys set in the store. Note that this number will typically be one greater than the number of keys added by &lt;code&gt;set()&lt;/code&gt; and &lt;code&gt;add()&lt;/code&gt; since one key is used to coordinate all the workers using the store.</source>
          <target state="translated">返回商店中设置的键数。请注意，此数字通常比 &lt;code&gt;set()&lt;/code&gt; 和 &lt;code&gt;add()&lt;/code&gt; 添加的键数大一，因为一个键用于协调使用商店的所有工作人员。</target>
        </trans-unit>
        <trans-unit id="77a1f23eab350f2cc1dff14c648c4ff6203e6c58" translate="yes" xml:space="preserve">
          <source>Returns the number of processes in the current process group</source>
          <target state="translated">返回当前进程组中的进程数量。</target>
        </trans-unit>
        <trans-unit id="c6ca89b8a3b13965452fa2d1cea6ac87f72d7a23" translate="yes" xml:space="preserve">
          <source>Returns the number of threads used for inter-op parallelism on CPU (e.g.</source>
          <target state="translated">Returns the number of threads used for inter-op parallelism on CPU (g.).</target>
        </trans-unit>
        <trans-unit id="d8b5e8f68adc8148c1f9e13acbeae8fd51bfefc0" translate="yes" xml:space="preserve">
          <source>Returns the number of threads used for inter-op parallelism on CPU (e.g. in JIT interpreter)</source>
          <target state="translated">Returns the number of threads used for inter-op parallelism on CPU (例如在JIT解释器中)</target>
        </trans-unit>
        <trans-unit id="7c423a746fa97a8c78c6c6a0bea67a7ee6cacc04" translate="yes" xml:space="preserve">
          <source>Returns the number of threads used for parallelizing CPU operations</source>
          <target state="translated">返回用于并行化CPU操作的线程数。</target>
        </trans-unit>
        <trans-unit id="76a9d76393683122be08a7059b22ee9487de8af7" translate="yes" xml:space="preserve">
          <source>Returns the numerical rank of a 2-D tensor.</source>
          <target state="translated">返回一个二维张量的数值等级。</target>
        </trans-unit>
        <trans-unit id="915a8c083943d2bf3a4491860c8c37370cb6d81a" translate="yes" xml:space="preserve">
          <source>Returns the numerical rank of a 2-D tensor. The method to compute the matrix rank is done using SVD by default. If &lt;code&gt;symmetric&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then &lt;code&gt;input&lt;/code&gt; is assumed to be symmetric, and the computation of the rank is done by obtaining the eigenvalues.</source>
          <target state="translated">返回二维张量的数字等级。默认情况下，使用SVD完成计算矩阵等级的方法。如果 &lt;code&gt;symmetric&lt;/code&gt; 是 &lt;code&gt;True&lt;/code&gt; ，那么 &lt;code&gt;input&lt;/code&gt; 被认为是对称的，排名的计算是通过获取特征值来完成。</target>
        </trans-unit>
        <trans-unit id="b9021b2a01727f502181983387c837330544dbe0" translate="yes" xml:space="preserve">
          <source>Returns the p-norm of (&lt;code&gt;input&lt;/code&gt; - &lt;code&gt;other&lt;/code&gt;)</source>
          <target state="translated">返回（ &lt;code&gt;input&lt;/code&gt; - &lt;code&gt;other&lt;/code&gt; ）的p范数</target>
        </trans-unit>
        <trans-unit id="0db967e67efe6e63a87c0ce474cb74d7038305af" translate="yes" xml:space="preserve">
          <source>Returns the product of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 张量中所有元素的乘积。</target>
        </trans-unit>
        <trans-unit id="a7140a19943936f9d48bbfc1f4b6d873ec50ed19" translate="yes" xml:space="preserve">
          <source>Returns the product of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">返回给定维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每一行的乘积。</target>
        </trans-unit>
        <trans-unit id="971be3e3aa0bf4f890c81c3791eba6b84f38ff83" translate="yes" xml:space="preserve">
          <source>Returns the q-th quantiles of all elements in the &lt;code&gt;input&lt;/code&gt; tensor, doing a linear interpolation when the q-th quantile lies between two data points.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 张量中所有元素的第q个分位数，当第q个分位数位于两个数据点之间时进行线性插值。</target>
        </trans-unit>
        <trans-unit id="1aa60514b18ba933ebc0d3b97778472f133a31e9" translate="yes" xml:space="preserve">
          <source>Returns the q-th quantiles of each row of the &lt;code&gt;input&lt;/code&gt; tensor along the dimension &lt;code&gt;dim&lt;/code&gt;, doing a linear interpolation when the q-th quantile lies between two data points. By default, &lt;code&gt;dim&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; resulting in the &lt;code&gt;input&lt;/code&gt; tensor being flattened before computation.</source>
          <target state="translated">返回沿维度 &lt;code&gt;dim&lt;/code&gt; 的 &lt;code&gt;input&lt;/code&gt; 张量的每一行的第q个分位数，当第q个分位数位于两个数据点之间时进行线性插值。默认情况下， &lt;code&gt;dim&lt;/code&gt; 为 &lt;code&gt;None&lt;/code&gt; 会导致在计算之前将 &lt;code&gt;input&lt;/code&gt; 张量展平。</target>
        </trans-unit>
        <trans-unit id="855eae4c3b5ff9439f790aa02a1e5edcb2f9666b" translate="yes" xml:space="preserve">
          <source>Returns the quantization scheme of a given QTensor.</source>
          <target state="translated">返回给定QTensor的量化方案。</target>
        </trans-unit>
        <trans-unit id="7d9c1cd994cab4ee1d9aadcaf26ce62a26fd9235" translate="yes" xml:space="preserve">
          <source>Returns the random number generator state as a &lt;code&gt;torch.ByteTensor&lt;/code&gt;.</source>
          <target state="translated">以 &lt;code&gt;torch.ByteTensor&lt;/code&gt; 的形式返回随机数生成器的状态。</target>
        </trans-unit>
        <trans-unit id="5e5816534688f7e46f4af65ebc7314df9d504c09" translate="yes" xml:space="preserve">
          <source>Returns the random number generator state of the specified GPU as a ByteTensor.</source>
          <target state="translated">以ByteTensor形式返回指定GPU的随机数发生器状态。</target>
        </trans-unit>
        <trans-unit id="627a3c7f5879a8778db877ff3b00bbcfbf03056f" translate="yes" xml:space="preserve">
          <source>Returns the rank of current process group</source>
          <target state="translated">返回当前进程组的等级</target>
        </trans-unit>
        <trans-unit id="4525ac2f89a3ac5d0c306f28833d2a44e3540a50" translate="yes" xml:space="preserve">
          <source>Returns the real and the imaginary parts together as one tensor of the same shape of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">将实部和虚部一起返回为 &lt;code&gt;input&lt;/code&gt; 形状相同的一个张量。</target>
        </trans-unit>
        <trans-unit id="7e3148edb8aa985705a099b8ae7ab9879b62393f" translate="yes" xml:space="preserve">
          <source>Returns the result of running &lt;code&gt;func&lt;/code&gt; with &lt;code&gt;args&lt;/code&gt; and &lt;code&gt;kwargs&lt;/code&gt;.</source>
          <target state="translated">返回使用 &lt;code&gt;args&lt;/code&gt; 和 &lt;code&gt;kwargs&lt;/code&gt; 运行 &lt;code&gt;func&lt;/code&gt; 的结果。</target>
        </trans-unit>
        <trans-unit id="64531174d404b10b93897ed2b88aba389572d7db" translate="yes" xml:space="preserve">
          <source>Returns the return value of &lt;code&gt;optimizer.step(*args, **kwargs)&lt;/code&gt;.</source>
          <target state="translated">返回 &lt;code&gt;optimizer.step(*args, **kwargs)&lt;/code&gt; 的返回值。</target>
        </trans-unit>
        <trans-unit id="6acc79cd313eeed62eea5f7e0ca05acecdcbc531" translate="yes" xml:space="preserve">
          <source>Returns the shape of a single sample (without batching).</source>
          <target state="translated">返回单个样本的形状(不分批)。</target>
        </trans-unit>
        <trans-unit id="4121b94d8bee0076d793dcc54679dd64a1c94a20" translate="yes" xml:space="preserve">
          <source>Returns the shape over which parameters are batched.</source>
          <target state="translated">Returns the shape over which parameters are batched。</target>
        </trans-unit>
        <trans-unit id="e4dd6ff47ca4bddd8efc777b8ef0c3a2c983568d" translate="yes" xml:space="preserve">
          <source>Returns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms.</source>
          <target state="translated">如果适用,返回雅各布行列式的符号。一般来说,这只对双项变换有意义。</target>
        </trans-unit>
        <trans-unit id="7d3d453cc3ee6cad8567bb601546cc4fa1aa1364" translate="yes" xml:space="preserve">
          <source>Returns the size in bytes of an individual element.</source>
          <target state="translated">返回单个元素的大小,以字节为单位。</target>
        </trans-unit>
        <trans-unit id="38820622e4a658519544ad686f76dee59368c29c" translate="yes" xml:space="preserve">
          <source>Returns the size of the &lt;code&gt;self&lt;/code&gt; tensor. The returned value is a subclass of &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt;&lt;code&gt;tuple&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">返回 &lt;code&gt;self&lt;/code&gt; 张量的大小。返回的值是&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#tuple&quot;&gt; &lt;code&gt;tuple&lt;/code&gt; &lt;/a&gt;的子类。</target>
        </trans-unit>
        <trans-unit id="11ff079b4e3d88cd650436812653217a81a77a46" translate="yes" xml:space="preserve">
          <source>Returns the standard deviation of the distribution.</source>
          <target state="translated">返回分布的标准差。</target>
        </trans-unit>
        <trans-unit id="159dfd0ce2a90b4d47c871e593a62731a632dc99" translate="yes" xml:space="preserve">
          <source>Returns the standard-deviation and mean of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 张量中所有元素的标准差和均值。</target>
        </trans-unit>
        <trans-unit id="b541928a3e9600b386da860747f66bb8139c4d0a" translate="yes" xml:space="preserve">
          <source>Returns the standard-deviation and mean of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the dimension &lt;code&gt;dim&lt;/code&gt;. If &lt;code&gt;dim&lt;/code&gt; is a list of dimensions, reduce over all of them.</source>
          <target state="translated">返回尺寸维 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每一行的标准偏差和均值。如果 &lt;code&gt;dim&lt;/code&gt; 是尺寸列表，请缩小所有尺寸。</target>
        </trans-unit>
        <trans-unit id="8bbd91280991e2a9737689547bbddca952d63287" translate="yes" xml:space="preserve">
          <source>Returns the standard-deviation of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 张量中所有元素的标准偏差。</target>
        </trans-unit>
        <trans-unit id="e297a06301ae289868c8331b63443bb83703e3b8" translate="yes" xml:space="preserve">
          <source>Returns the standard-deviation of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the dimension &lt;code&gt;dim&lt;/code&gt;. If &lt;code&gt;dim&lt;/code&gt; is a list of dimensions, reduce over all of them.</source>
          <target state="translated">返回尺寸为 &lt;code&gt;dim&lt;/code&gt; 的 &lt;code&gt;input&lt;/code&gt; 张量的每一行的标准偏差。如果 &lt;code&gt;dim&lt;/code&gt; 是尺寸列表，请缩小所有尺寸。</target>
        </trans-unit>
        <trans-unit id="8527f000d10d08462f55a7df699877bf3a2b4a09" translate="yes" xml:space="preserve">
          <source>Returns the state of the optimizer as a &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">以&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt; &lt;code&gt;dict&lt;/code&gt; 的形式&lt;/a&gt;返回优化器的状态。</target>
        </trans-unit>
        <trans-unit id="ddacfefa1e98c3901d53623a3db23fafb076c7fc" translate="yes" xml:space="preserve">
          <source>Returns the state of the scaler as a &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/a&gt;. It contains five entries:</source>
          <target state="translated">以&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt; &lt;code&gt;dict&lt;/code&gt; 的形式&lt;/a&gt;返回定标器的状态。它包含五个条目：</target>
        </trans-unit>
        <trans-unit id="cbae1c6724a7b43e4ce8327e553e968ab36fa3db" translate="yes" xml:space="preserve">
          <source>Returns the state of the scheduler as a &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">以&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#dict&quot;&gt; &lt;code&gt;dict&lt;/code&gt; 的形式&lt;/a&gt;返回调度程序的状态。</target>
        </trans-unit>
        <trans-unit id="1ebb6fa0d71baf28bcf461777f1c5e425a2744c2" translate="yes" xml:space="preserve">
          <source>Returns the stride of &lt;code&gt;self&lt;/code&gt; tensor.</source>
          <target state="translated">返回 &lt;code&gt;self&lt;/code&gt; 张量的步幅。</target>
        </trans-unit>
        <trans-unit id="b5f0fbc903ffee7b0004fd1f9bcd5996f44017b1" translate="yes" xml:space="preserve">
          <source>Returns the sum of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 张量中所有元素的总和。</target>
        </trans-unit>
        <trans-unit id="1863758d4c0f9cfda8c72c22853117ba70ebdee5" translate="yes" xml:space="preserve">
          <source>Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.</source>
          <target state="translated">返回所有元素的总和,将 &quot;非数字&quot;(NaNs)视为零。</target>
        </trans-unit>
        <trans-unit id="14e6166614886b7770d3ecf48dce1bfb59192744" translate="yes" xml:space="preserve">
          <source>Returns the sum of each row of SparseTensor &lt;code&gt;input&lt;/code&gt; in the given dimensions &lt;code&gt;dim&lt;/code&gt;. If &lt;code&gt;dim&lt;/code&gt; is a list of dimensions, reduce over all of them. When sum over all &lt;code&gt;sparse_dim&lt;/code&gt;, this method returns a Tensor instead of SparseTensor.</source>
          <target state="translated">返回给定维度 &lt;code&gt;dim&lt;/code&gt; 中每行SparseTensor &lt;code&gt;input&lt;/code&gt; 的总和。如果 &lt;code&gt;dim&lt;/code&gt; 是尺寸列表，请缩小所有尺寸。当对所有 &lt;code&gt;sparse_dim&lt;/code&gt; 求和时，此方法将返回一个Tensor而不是SparseTensor。</target>
        </trans-unit>
        <trans-unit id="bf49e8cc86ca6a6600ce29a25884a785a376abc6" translate="yes" xml:space="preserve">
          <source>Returns the sum of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;, treating Not a Numbers (NaNs) as zero. If &lt;code&gt;dim&lt;/code&gt; is a list of dimensions, reduce over all of them.</source>
          <target state="translated">返回给定维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每一行的总和，将非数字（NaNs）视为零。如果 &lt;code&gt;dim&lt;/code&gt; 是尺寸列表，请缩小所有尺寸。</target>
        </trans-unit>
        <trans-unit id="4d434d5c1c4d2f8c246ee3548fb97e54d9889e29" translate="yes" xml:space="preserve">
          <source>Returns the sum of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;. If &lt;code&gt;dim&lt;/code&gt; is a list of dimensions, reduce over all of them.</source>
          <target state="translated">返回给定维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每一行的总和。如果 &lt;code&gt;dim&lt;/code&gt; 是尺寸列表，请缩小所有尺寸。</target>
        </trans-unit>
        <trans-unit id="28abb051bc478158b8e05bc1054f16151a20a7cb" translate="yes" xml:space="preserve">
          <source>Returns the sum of the elements of the diagonal of the input 2-D matrix.</source>
          <target state="translated">返回输入的二维矩阵对角线的元素之和。</target>
        </trans-unit>
        <trans-unit id="628247234e04df2fd2dd0c458a3e5e95270ff4e1" translate="yes" xml:space="preserve">
          <source>Returns the tensor as a (nested) list. For scalars, a standard Python number is returned, just like with &lt;a href=&quot;#torch.Tensor.item&quot;&gt;&lt;code&gt;item()&lt;/code&gt;&lt;/a&gt;. Tensors are automatically moved to the CPU first if necessary.</source>
          <target state="translated">将张量作为（嵌套的）列表返回。对于标量，将返回标准的Python数字，就像&lt;a href=&quot;#torch.Tensor.item&quot;&gt; &lt;code&gt;item()&lt;/code&gt; 一样&lt;/a&gt;。如有必要，张量会先自动移至CPU。</target>
        </trans-unit>
        <trans-unit id="035dc69af946dc6fb060b51cabe8b5996eafc4da" translate="yes" xml:space="preserve">
          <source>Returns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded.</source>
          <target state="translated">Returns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded.</target>
        </trans-unit>
        <trans-unit id="94ea64072a9591f059633c957623237cd647f97a" translate="yes" xml:space="preserve">
          <source>Returns the total number of elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 张量中元素的总数。</target>
        </trans-unit>
        <trans-unit id="1d3970a743c03ce9d28802ddc4f6588b09bb7c8b" translate="yes" xml:space="preserve">
          <source>Returns the type if &lt;code&gt;dtype&lt;/code&gt; is not provided, else casts this object to the specified type.</source>
          <target state="translated">如果未提供 &lt;code&gt;dtype&lt;/code&gt; ,则返回类型，否则将此对象强制转换为指定的类型。</target>
        </trans-unit>
        <trans-unit id="2fea27b4bcbff9f0edb11c8bfcd39329450a9b92" translate="yes" xml:space="preserve">
          <source>Returns the type of the underlying storage.</source>
          <target state="translated">返回底层存储的类型。</target>
        </trans-unit>
        <trans-unit id="4ac509092325d5bcdffb0cc3612c7cf497735d46" translate="yes" xml:space="preserve">
          <source>Returns the underlying storage.</source>
          <target state="translated">返回底层存储。</target>
        </trans-unit>
        <trans-unit id="32fcdf754b9cb5a228ef02efc503187163097afb" translate="yes" xml:space="preserve">
          <source>Returns the unique elements of the input tensor.</source>
          <target state="translated">返回输入张量的唯一元素。</target>
        </trans-unit>
        <trans-unit id="26e4d829967fdd7e5612995ff5e9602634c0689a" translate="yes" xml:space="preserve">
          <source>Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices &lt;code&gt;input&lt;/code&gt;, the other elements of the result tensor &lt;code&gt;out&lt;/code&gt; are set to 0.</source>
          <target state="translated">返回矩阵（2-D张量）或矩阵 &lt;code&gt;input&lt;/code&gt; 批次的上三角部分，结果张量 &lt;code&gt;out&lt;/code&gt; 的其他元素设置为0。</target>
        </trans-unit>
        <trans-unit id="f17ab21227456440a4bca3f2fb221dbfad6b68d3" translate="yes" xml:space="preserve">
          <source>Returns the value of this tensor as a standard Python number. This only works for tensors with one element. For other cases, see &lt;a href=&quot;#torch.Tensor.tolist&quot;&gt;&lt;code&gt;tolist()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">返回此张量的值作为标准Python数。这仅适用于具有一个元素的张量。对于其他情况，请参见&lt;a href=&quot;#torch.Tensor.tolist&quot;&gt; &lt;code&gt;tolist()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="f1cc620c9c632e9d45f1d98e6550b6c3781efad4" translate="yes" xml:space="preserve">
          <source>Returns the variance and mean of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 张量中所有元素的方差和均值。</target>
        </trans-unit>
        <trans-unit id="9ff1d1aa2591afcd14de03b53682da2b82cb9cc7" translate="yes" xml:space="preserve">
          <source>Returns the variance and mean of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">返回给定维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每一行的方差和均值。</target>
        </trans-unit>
        <trans-unit id="af4e03a6f47c5a06e9dc95d8b7b0827b12d10bac" translate="yes" xml:space="preserve">
          <source>Returns the variance of all elements in the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">返回 &lt;code&gt;input&lt;/code&gt; 张量中所有元素的方差。</target>
        </trans-unit>
        <trans-unit id="0298d938b904aa1e59745be5e3318e09914487aa" translate="yes" xml:space="preserve">
          <source>Returns the variance of each row of the &lt;code&gt;input&lt;/code&gt; tensor in the given dimension &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">返回给定维度 &lt;code&gt;dim&lt;/code&gt; 中 &lt;code&gt;input&lt;/code&gt; 张量的每一行的方差。</target>
        </trans-unit>
        <trans-unit id="fdcaff1a07dc530f557ad4bcd05c34a11bb994e8" translate="yes" xml:space="preserve">
          <source>Returns the variance of the distribution.</source>
          <target state="translated">返回分布的方差。</target>
        </trans-unit>
        <trans-unit id="88e4f4506a554a4b89da7d55d5cfa7515f220fb9" translate="yes" xml:space="preserve">
          <source>Returns the version of cuDNN</source>
          <target state="translated">返回cuDNN的版本</target>
        </trans-unit>
        <trans-unit id="9c3ac5c69999b0a023804bb506ededc917b6c131" translate="yes" xml:space="preserve">
          <source>Returns this tensor as the same shape as &lt;code&gt;other&lt;/code&gt;. &lt;code&gt;self.reshape_as(other)&lt;/code&gt; is equivalent to &lt;code&gt;self.reshape(other.sizes())&lt;/code&gt;. This method returns a view if &lt;code&gt;other.sizes()&lt;/code&gt; is compatible with the current shape. See &lt;a href=&quot;#torch.Tensor.view&quot;&gt;&lt;code&gt;torch.Tensor.view()&lt;/code&gt;&lt;/a&gt; on when it is possible to return a view.</source>
          <target state="translated">以与 &lt;code&gt;other&lt;/code&gt; 相同的形状返回此张量。 &lt;code&gt;self.reshape_as(other)&lt;/code&gt; 等同于 &lt;code&gt;self.reshape(other.sizes())&lt;/code&gt; 。如果 &lt;code&gt;other.sizes()&lt;/code&gt; 与当前形状兼容，则此方法返回一个视图。何时可以返回视图，请参见&lt;a href=&quot;#torch.Tensor.view&quot;&gt; &lt;code&gt;torch.Tensor.view()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="586000b57d8c3b25d201cf41023bcdd3dcb6f713" translate="yes" xml:space="preserve">
          <source>Returns this tensor cast to the type of the given tensor.</source>
          <target state="translated">返回这个张量投射到给定张量的类型。</target>
        </trans-unit>
        <trans-unit id="ba9ebe14b855b8e57d04a421d5dedae584879e50" translate="yes" xml:space="preserve">
          <source>Returns total time spent on CPU obtained as a sum of all self times across all the events.</source>
          <target state="translated">Returns total time spent on CPU obtained as a sum of all self times across all events.</target>
        </trans-unit>
        <trans-unit id="88c329059251d00815688b5e671bb43e44ab8c23" translate="yes" xml:space="preserve">
          <source>Returns true if &lt;code&gt;self.data&lt;/code&gt; stored on a gpu</source>
          <target state="translated">如果 &lt;code&gt;self.data&lt;/code&gt; 存储在gpu上，则返回true</target>
        </trans-unit>
        <trans-unit id="69352252bee0873e3fb0a65734cef7e6482fff70" translate="yes" xml:space="preserve">
          <source>Returns true if &lt;code&gt;self.data&lt;/code&gt; stored on in pinned memory</source>
          <target state="translated">如果将 &lt;code&gt;self.data&lt;/code&gt; 存储在固定内存中，则返回true</target>
        </trans-unit>
        <trans-unit id="373aeaf23062f8c5ecaff73e9be00cb54c4e37f3" translate="yes" xml:space="preserve">
          <source>Returns true if this tensor resides in pinned memory.</source>
          <target state="translated">如果该张量驻留在pinned内存中,返回true。</target>
        </trans-unit>
        <trans-unit id="c8e3480d7d70ecfc7b3c25ecaed7108c83b90b62" translate="yes" xml:space="preserve">
          <source>Returns whether PyTorch is built with CUDA support. Note that this doesn&amp;rsquo;t necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it.</source>
          <target state="translated">返回PyTorch是否在CUDA支持下构建。请注意，这并不一定意味着CUDA可用。只是如果此PyTorch二进制文件运行在具有正常CUDA驱动程序和设备的计算机上，我们将能够使用它。</target>
        </trans-unit>
        <trans-unit id="98fd0f653522d8480237ac570476e6435a37e06f" translate="yes" xml:space="preserve">
          <source>Returns whether PyTorch is built with MKL support.</source>
          <target state="translated">Returns whether PyTorch is built with MKL support.</target>
        </trans-unit>
        <trans-unit id="620b15262eed97044ef3e21b5235ee100fef8185" translate="yes" xml:space="preserve">
          <source>Returns whether PyTorch is built with MKL-DNN support.</source>
          <target state="translated">返回 PyTorch 是否支持 MKL-DNN。</target>
        </trans-unit>
        <trans-unit id="957ab7585c4317f19aa9cfbd05dcb5165584e9c9" translate="yes" xml:space="preserve">
          <source>Returns whether PyTorch is built with OpenMP support.</source>
          <target state="translated">返回 PyTorch 是否支持 OpenMP。</target>
        </trans-unit>
        <trans-unit id="57fa332c1c053f80c3eff6f20003e44a174f7e50" translate="yes" xml:space="preserve">
          <source>Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1</source>
          <target state="translated">返回 PyTorch 是否以 _GLIBCXX_USE_CXX11_ABI=1 的方式构建。</target>
        </trans-unit>
        <trans-unit id="4f1dfa1c8e9678eac6dccc5f732ccd2cbc520cad" translate="yes" xml:space="preserve">
          <source>Returns whether PyTorch&amp;rsquo;s CUDA state has been initialized.</source>
          <target state="translated">返回PyTorch的CUDA状态是否已初始化。</target>
        </trans-unit>
        <trans-unit id="770f265f08ee812cf8c52d62b5ca51586f3291bd" translate="yes" xml:space="preserve">
          <source>Returns whether or not the current node is the owner of this &lt;code&gt;RRef&lt;/code&gt;.</source>
          <target state="translated">返回当前节点是否是此 &lt;code&gt;RRef&lt;/code&gt; 的所有者。</target>
        </trans-unit>
        <trans-unit id="65ba60da18a01009c75c582854b7076b39cbf39e" translate="yes" xml:space="preserve">
          <source>Returns whether this &lt;code&gt;RRef&lt;/code&gt; has been confirmed by the owner. &lt;code&gt;OwnerRRef&lt;/code&gt; always returns true, while &lt;code&gt;UserRRef&lt;/code&gt; only returns true when the owner knowns about this &lt;code&gt;UserRRef&lt;/code&gt;.</source>
          <target state="translated">返回此 &lt;code&gt;RRef&lt;/code&gt; 是否已被所有者确认。 &lt;code&gt;OwnerRRef&lt;/code&gt; 始终返回true，而 &lt;code&gt;UserRRef&lt;/code&gt; 仅在所有者知道此 &lt;code&gt;UserRRef&lt;/code&gt; 时才返回true 。</target>
        </trans-unit>
        <trans-unit id="a0f5e59205b44dcceb0ac6759e21e032f1501bc8" translate="yes" xml:space="preserve">
          <source>Returns worker information of the node that owns this &lt;code&gt;RRef&lt;/code&gt;.</source>
          <target state="translated">返回拥有此 &lt;code&gt;RRef&lt;/code&gt; 的节点的工作程序信息。</target>
        </trans-unit>
        <trans-unit id="4edad2b2ca35229024c67f3f0d3bff7d93093b61" translate="yes" xml:space="preserve">
          <source>Returns worker name of the node that owns this &lt;code&gt;RRef&lt;/code&gt;.</source>
          <target state="translated">返回拥有此 &lt;code&gt;RRef&lt;/code&gt; 的节点的工作者名称。</target>
        </trans-unit>
        <trans-unit id="7749fcf802c472b6c2f5bd0556805e456ffd5674" translate="yes" xml:space="preserve">
          <source>Returns:</source>
          <target state="translated">Returns:</target>
        </trans-unit>
        <trans-unit id="614439f097ad878a8635d81ef81858113ba66ffa" translate="yes" xml:space="preserve">
          <source>Returns: self</source>
          <target state="translated">返回:自我</target>
        </trans-unit>
        <trans-unit id="63fbd9687bea5bf2237b15e3392da627d72bce65" translate="yes" xml:space="preserve">
          <source>Reverse the order of a n-D tensor along given axis in dims.</source>
          <target state="translated">将n-D张量沿给定轴线的顺序反转,用dims表示。</target>
        </trans-unit>
        <trans-unit id="7787cde8d62a04058bf7bd3bc0e17bb68ebcb122" translate="yes" xml:space="preserve">
          <source>Right now all parameters have to be on a single device. This will be improved in the future.</source>
          <target state="translated">现在,所有的参数都必须在一个设备上。这一点将在未来得到改善。</target>
        </trans-unit>
        <trans-unit id="c79c0386989b48b4178b173f296df3e3c68180f2" translate="yes" xml:space="preserve">
          <source>Right now, this works only if the module is on the GPU and cuDNN is enabled. Otherwise, it&amp;rsquo;s a no-op.</source>
          <target state="translated">目前，只有在模块位于GPU上且启用了cuDNN的情况下，此方法才有效。否则，这是一个禁忌行动。</target>
        </trans-unit>
        <trans-unit id="bf85deb85c247cf504b5145a433d00570a94475a" translate="yes" xml:space="preserve">
          <source>Roll the tensor along the given dimension(s).</source>
          <target state="translated">沿着给定的维度滚动张量。</target>
        </trans-unit>
        <trans-unit id="61764ff63ec4f189d73879d14b23769ece78814a" translate="yes" xml:space="preserve">
          <source>Roll the tensor along the given dimension(s). Elements that are shifted beyond the last position are re-introduced at the first position. If a dimension is not specified, the tensor will be flattened before rolling and then restored to the original shape.</source>
          <target state="translated">沿着给定的维度滚动张量。超过最后位置的元素会在第一个位置重新引入。如果没有指定维度,张量将在滚动前被压平,然后恢复到原始形状。</target>
        </trans-unit>
        <trans-unit id="8f164f4fea8febd9cad272e82ab328161a3c6dcb" translate="yes" xml:space="preserve">
          <source>Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.</source>
          <target state="translated">在dims轴指定的平面上将n-D张量旋转90度。</target>
        </trans-unit>
        <trans-unit id="8dfaeb09d713bebac16f06e9df8c867ae92ec360" translate="yes" xml:space="preserve">
          <source>Rotate a n-D tensor by 90 degrees in the plane specified by dims axis. Rotation direction is from the first towards the second axis if k &amp;gt; 0, and from the second towards the first for k &amp;lt; 0.</source>
          <target state="translated">在调光轴指定的平面中将nD张量旋转90度。如果k&amp;gt; 0，则旋转方向是从第一个轴到第二个轴；对于k &amp;lt;0，旋转方向是从第二个轴到第一个轴。</target>
        </trans-unit>
        <trans-unit id="8f997250f8d2d3174e19a84c2938437affe12ee1" translate="yes" xml:space="preserve">
          <source>Rule of thumb</source>
          <target state="translated">经验法则</target>
        </trans-unit>
        <trans-unit id="f35ba84a98c82f892526356da88061a3501723bb" translate="yes" xml:space="preserve">
          <source>Run it on the command line with</source>
          <target state="translated">在命令行中运行</target>
        </trans-unit>
        <trans-unit id="312956888e10f6c9efa0be100898714e9d49a550" translate="yes" xml:space="preserve">
          <source>Running a loaded model:</source>
          <target state="translated">运行一个加载的模型。</target>
        </trans-unit>
        <trans-unit id="5359eea6ab0e87db9479836bb9761e7ad6a157fe" translate="yes" xml:space="preserve">
          <source>Runtime characteristics</source>
          <target state="translated">运行时特点</target>
        </trans-unit>
        <trans-unit id="02aa629c8b16cd17a44f3a0efec2feed43937642" translate="yes" xml:space="preserve">
          <source>S</source>
          <target state="translated">S</target>
        </trans-unit>
        <trans-unit id="d7c8ea15b98297abf6f0c48aa6d1cd297394d77c" translate="yes" xml:space="preserve">
          <source>S ** 2 / (m - 1)</source>
          <target state="translated">S**2/(m-1)</target>
        </trans-unit>
        <trans-unit id="dc71df7ac9f3ac27b7d12ac3903965de330de133" translate="yes" xml:space="preserve">
          <source>S = \text{max target length, if shape is } (N, S)</source>
          <target state="translated">S=text{最大目标长度,如果形状是}。(N,S)</target>
        </trans-unit>
        <trans-unit id="1678c8d5a5e1b27b84ba49edaa49f332453af05c" translate="yes" xml:space="preserve">
          <source>S=\text{num\_layers} * \text{num\_directions}</source>
          <target state="translated">S=text{num_layers}*text{num_direction}(文本)</target>
        </trans-unit>
        <trans-unit id="d4cd3df709d03592b7820bf91be2a174166e078a" translate="yes" xml:space="preserve">
          <source>SELU</source>
          <target state="translated">SELU</target>
        </trans-unit>
        <trans-unit id="bad45f89fe2643171a9f0af482912ef53c58c8d3" translate="yes" xml:space="preserve">
          <source>SMART mode algorithm</source>
          <target state="translated">SMART模式算法</target>
        </trans-unit>
        <trans-unit id="2c798885ebfbe383227dbd5c2205277f8af9d524" translate="yes" xml:space="preserve">
          <source>SUM</source>
          <target state="translated">SUM</target>
        </trans-unit>
        <trans-unit id="6f0eaf31574764cd755cbd6a055a6fcbc66e5dae" translate="yes" xml:space="preserve">
          <source>SWA has been proposed in &lt;a href=&quot;https://arxiv.org/abs/1803.05407&quot;&gt;Averaging Weights Leads to Wider Optima and Better Generalization&lt;/a&gt;.</source>
          <target state="translated">在&lt;a href=&quot;https://arxiv.org/abs/1803.05407&quot;&gt;平均权重导致更宽的最优性和更好的泛化&lt;/a&gt;方面提出了SWA 。</target>
        </trans-unit>
        <trans-unit id="f84f75a03bd05c4bba727eb6c0d6fe360d00d818" translate="yes" xml:space="preserve">
          <source>SWA learning rate schedules</source>
          <target state="translated">SWA学习率时间表</target>
        </trans-unit>
        <trans-unit id="2e4fbb0a9e252120bad1b2cf3ddef7f9a08b3c65" translate="yes" xml:space="preserve">
          <source>Same as &lt;a href=&quot;#torch.Tensor.narrow&quot;&gt;&lt;code&gt;Tensor.narrow()&lt;/code&gt;&lt;/a&gt; except returning a copy rather than shared storage. This is primarily for sparse tensors, which do not have a shared-storage narrow method. Calling &lt;code&gt;`narrow_copy&lt;/code&gt; with &lt;code&gt;`dimemsion &amp;gt; self.sparse_dim()`&lt;/code&gt; will return a copy with the relevant dense dimension narrowed, and &lt;code&gt;`self.shape`&lt;/code&gt; updated accordingly.</source>
          <target state="translated">与&lt;a href=&quot;#torch.Tensor.narrow&quot;&gt; &lt;code&gt;Tensor.narrow()&lt;/code&gt; &lt;/a&gt;相同，除了返回副本而不是共享存储。这主要用于稀疏张量，它们没有共享存储的窄方法。用 &lt;code&gt;`dimemsion &amp;gt; self.sparse_dim()`&lt;/code&gt; 调用 &lt;code&gt;`narrow_copy&lt;/code&gt; '将返回一个缩小了相关密集尺寸的副本，并 &lt;code&gt;`self.shape`&lt;/code&gt; 更新了`self.shape`。</target>
        </trans-unit>
        <trans-unit id="12f6402d6b42f6b20df7c7e5514d35048980475e" translate="yes" xml:space="preserve">
          <source>Sampled tensor of same shape as &lt;code&gt;logits&lt;/code&gt; from the Gumbel-Softmax distribution. If &lt;code&gt;hard=True&lt;/code&gt;, the returned samples will be one-hot, otherwise they will be probability distributions that sum to 1 across &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">与Gumbel-Softmax分布中的 &lt;code&gt;logits&lt;/code&gt; 形状相同的采样张量。如果 &lt;code&gt;hard=True&lt;/code&gt; ，则返回的样本将是一热点，否则它们将是对 &lt;code&gt;dim&lt;/code&gt; 求和为1的概率分布。</target>
        </trans-unit>
        <trans-unit id="0f11d31dedd1a06aeb7fd6c168cfb8046bb4af97" translate="yes" xml:space="preserve">
          <source>Sampler that restricts data loading to a subset of the dataset.</source>
          <target state="translated">采样器,将数据加载限制在数据集的一个子集。</target>
        </trans-unit>
        <trans-unit id="46c4eda8f0682c6a4eca16cae1bf03c979256658" translate="yes" xml:space="preserve">
          <source>Samples are binary (0 or 1). They take the value &lt;code&gt;1&lt;/code&gt; with probability &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;0&lt;/code&gt; with probability &lt;code&gt;1 - p&lt;/code&gt;.</source>
          <target state="translated">样本是二进制的（0或1）。他们采取的值 &lt;code&gt;1&lt;/code&gt; 的概率为 &lt;code&gt;p&lt;/code&gt; 和 &lt;code&gt;0&lt;/code&gt; 的概率为 &lt;code&gt;1 - p&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="290387605cb78533c282d0e9d73fda591d532885" translate="yes" xml:space="preserve">
          <source>Samples are integers from</source>
          <target state="translated">样品是整数,从</target>
        </trans-unit>
        <trans-unit id="58b74a9d8f020ad3d0017c06dac17a64698c94e6" translate="yes" xml:space="preserve">
          <source>Samples are logits of values in (0, 1). See [1] for more details.</source>
          <target state="translated">样本是在(0,1)中取值的对数。详见[1]。</target>
        </trans-unit>
        <trans-unit id="038b1821087530df9b32220979af778b78c446a1" translate="yes" xml:space="preserve">
          <source>Samples are non-negative integers [0,</source>
          <target state="translated">样本为非负整数[0,</target>
        </trans-unit>
        <trans-unit id="19b7a569829985718d97274f88b37fe1917c9e92" translate="yes" xml:space="preserve">
          <source>Samples are nonnegative integers, with a pmf given by</source>
          <target state="translated">样本为非负整数,pmf由以下公式给出</target>
        </trans-unit>
        <trans-unit id="c447ff4f78d86ac69f689268060727181fbe9807" translate="yes" xml:space="preserve">
          <source>Samples are one-hot coded vectors of size &lt;code&gt;probs.size(-1)&lt;/code&gt;.</source>
          <target state="translated">样本是大小为 &lt;code&gt;probs.size(-1)&lt;/code&gt; 的一键编码矢量。</target>
        </trans-unit>
        <trans-unit id="19104e555389bebae6c56e80fa764ee086be702a" translate="yes" xml:space="preserve">
          <source>Samples elements from &lt;code&gt;[0,..,len(weights)-1]&lt;/code&gt; with given probabilities (weights).</source>
          <target state="translated">以给定的概率（权重）对 &lt;code&gt;[0,..,len(weights)-1]&lt;/code&gt; 中的元素进行采样。</target>
        </trans-unit>
        <trans-unit id="9e9e089066c3d2af4277a5ab0054e52473f2c07f" translate="yes" xml:space="preserve">
          <source>Samples elements randomly from a given list of indices, without replacement.</source>
          <target state="translated">从给定的索引列表中随机抽取元素,不进行替换。</target>
        </trans-unit>
        <trans-unit id="24e35b463a3cf98fd449e886fc4e86f3a0dab5f2" translate="yes" xml:space="preserve">
          <source>Samples elements randomly. If without replacement, then sample from a shuffled dataset. If with replacement, then user can specify &lt;code&gt;num_samples&lt;/code&gt; to draw.</source>
          <target state="translated">随机采样元素。如果不进行替换，则从经过改组的数据集中采样。如果替换，则用户可以指定要绘制的 &lt;code&gt;num_samples&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="09f894ef8557b505ad9a0b93997e1c4496877755" translate="yes" xml:space="preserve">
          <source>Samples elements sequentially, always in the same order.</source>
          <target state="translated">按顺序取样,总是以相同的顺序取样。</target>
        </trans-unit>
        <trans-unit id="22779ddf6cbe4d8bc9ff5fafe1e2aa1b1ff778a6" translate="yes" xml:space="preserve">
          <source>Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of independent normally distributed random variables with means &lt;code&gt;0&lt;/code&gt; follows a Cauchy distribution.</source>
          <target state="translated">来自柯西（洛伦兹）分布的样本。独立均值为 &lt;code&gt;0&lt;/code&gt; 的正态分布随机变量的比率分布遵循柯西分布。</target>
        </trans-unit>
        <trans-unit id="2cb9d9b0dcc4dfe9421ee8a8fe988bef0860c270" translate="yes" xml:space="preserve">
          <source>Samples from a Gumbel Distribution.</source>
          <target state="translated">来自Gumbel分布的样本。</target>
        </trans-unit>
        <trans-unit id="f796da3db0834b82a9b597a027df4fe0a3710f34" translate="yes" xml:space="preserve">
          <source>Samples from a Pareto Type 1 distribution.</source>
          <target state="translated">来自帕累托1型分布的样本。</target>
        </trans-unit>
        <trans-unit id="e40b18d593f8292cd1929002560bba0658812862" translate="yes" xml:space="preserve">
          <source>Samples from a two-parameter Weibull distribution.</source>
          <target state="translated">从双参数Weibull分布的样本。</target>
        </trans-unit>
        <trans-unit id="870cb0523222c3bafa82fc03370a9998186a6c8a" translate="yes" xml:space="preserve">
          <source>Samples from the Gumbel-Softmax distribution (&lt;a href=&quot;https://arxiv.org/abs/1611.00712&quot;&gt;Link 1&lt;/a&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.01144&quot;&gt;Link 2&lt;/a&gt;) and optionally discretizes.</source>
          <target state="translated">来自Gumbel-Softmax分布（&lt;a href=&quot;https://arxiv.org/abs/1611.00712&quot;&gt;链接1&lt;/a&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.01144&quot;&gt;链接2&lt;/a&gt;）的样本，也可以离散化。</target>
        </trans-unit>
        <trans-unit id="bb9c2993371e955a4cc850805ced703ca0a14469" translate="yes" xml:space="preserve">
          <source>Save an offline version of this module for use in a separate process.</source>
          <target state="translated">保存此模块的离线版本,以便在单独的过程中使用。</target>
        </trans-unit>
        <trans-unit id="c01b4e6f82d950b9eb933dee7fd7ccb2a95a9e86" translate="yes" xml:space="preserve">
          <source>Save an offline version of this module for use in a separate process. The saved module serializes all of the methods, submodules, parameters, and attributes of this module. It can be loaded into the C++ API using &lt;code&gt;torch::jit::load(filename)&lt;/code&gt; or into the Python API with &lt;a href=&quot;torch.jit.load#torch.jit.load&quot;&gt;&lt;code&gt;torch.jit.load&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">保存此模块的脱机版本以在单独的过程中使用。保存的模块会序列化此模块的所有方法，子模块，参数和属性。它可以使用被加载到C ++ API &lt;code&gt;torch::jit::load(filename)&lt;/code&gt; 或与Python的API &lt;a href=&quot;torch.jit.load#torch.jit.load&quot;&gt; &lt;code&gt;torch.jit.load&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="ae2ed5816db3fac279ce9e6772770274eeecffb6" translate="yes" xml:space="preserve">
          <source>Saves an object to a disk file.</source>
          <target state="translated">将对象保存到磁盘文件中。</target>
        </trans-unit>
        <trans-unit id="4568659c737ca6dd6d7cb2b416c756b4fa3d4b2e" translate="yes" xml:space="preserve">
          <source>Saves given tensors for a future call to &lt;code&gt;backward()&lt;/code&gt;.</source>
          <target state="translated">保存给定的张量，以供将来调用 &lt;code&gt;backward()&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="122590ee88a0a7371836192fc7f0f566864870ca" translate="yes" xml:space="preserve">
          <source>Say we have a model like:</source>
          <target state="translated">说我们有一个模型,像。</target>
        </trans-unit>
        <trans-unit id="66da7860d1535f34a78cf5749db3d68876d558e1" translate="yes" xml:space="preserve">
          <source>Scatters a list of tensors to all processes in a group.</source>
          <target state="translated">将一个时序列表分散到一个组中的所有进程。</target>
        </trans-unit>
        <trans-unit id="106090197649a7e069056e240a9ad14c57a1cbfa" translate="yes" xml:space="preserve">
          <source>Scatters tensor across multiple GPUs.</source>
          <target state="translated">将张量分散到多个GPU上。</target>
        </trans-unit>
        <trans-unit id="b687feeb25ef199e522b82d65d8c03c4535af2a9" translate="yes" xml:space="preserve">
          <source>Score function</source>
          <target state="translated">得分功能</target>
        </trans-unit>
        <trans-unit id="ca23ef4c78c4b30679a7feed50e583d779b937a2" translate="yes" xml:space="preserve">
          <source>Scores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian.</source>
          <target state="translated">通过倒置变换和使用基分布的得分和对数abs det jacobian计算得分来对样本进行评分。</target>
        </trans-unit>
        <trans-unit id="8003f43b6ada6148476432121a9ade7662e59807" translate="yes" xml:space="preserve">
          <source>ScriptFunction</source>
          <target state="translated">ScriptFunction</target>
        </trans-unit>
        <trans-unit id="45d3892e0529a64b6123a79e03cc7e0f2432a98c" translate="yes" xml:space="preserve">
          <source>ScriptModule</source>
          <target state="translated">ScriptModule</target>
        </trans-unit>
        <trans-unit id="3fb95588d29ea3da9d0dc7585a2020bf65e56741" translate="yes" xml:space="preserve">
          <source>Scripted functions can call traced functions. This is particularly useful when you need to use control-flow around a simple feed-forward model. For instance the beam search of a sequence to sequence model will typically be written in script but can call an encoder module generated using tracing.</source>
          <target state="translated">脚本函数可以调用跟踪函数。当你需要围绕一个简单的前馈模型使用控制流时,这一点特别有用。例如,序列到序列模型的波束搜索通常用脚本编写,但可以调用使用跟踪生成的编码器模块。</target>
        </trans-unit>
        <trans-unit id="898752673b10b861d8ace49b60919a48cd4eac78" translate="yes" xml:space="preserve">
          <source>Scripting a function or &lt;code&gt;nn.Module&lt;/code&gt; will inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return a &lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">对函数或 &lt;code&gt;nn.Module&lt;/code&gt; 进行脚本编写将检查源代码，使用TorchScript编译器将其编译为TorchScript代码，然后返回&lt;a href=&quot;generated/torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;generated/torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="af454137fb34eaadd60d3d10157722e0668fe3b2" translate="yes" xml:space="preserve">
          <source>Scripting a function or &lt;code&gt;nn.Module&lt;/code&gt; will inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return a &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt;&lt;code&gt;ScriptFunction&lt;/code&gt;&lt;/a&gt;. TorchScript itself is a subset of the Python language, so not all features in Python work, but we provide enough functionality to compute on tensors and do control-dependent operations. For a complete guide, see the &lt;a href=&quot;../jit_language_reference#language-reference&quot;&gt;TorchScript Language Reference&lt;/a&gt;.</source>
          <target state="translated">对函数或 &lt;code&gt;nn.Module&lt;/code&gt; 进行脚本编写将检查源代码，使用TorchScript编译器将其编译为TorchScript代码，然后返回&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;torch.jit.scriptfunction#torch.jit.ScriptFunction&quot;&gt; &lt;code&gt;ScriptFunction&lt;/code&gt; &lt;/a&gt;。TorchScript本身是Python语言的子集，因此Python并非所有功能都能正常工作，但我们提供了足够的功能来在张量上进行计算并执行与控制有关的操作。有关完整指南，请参见《&lt;a href=&quot;../jit_language_reference#language-reference&quot;&gt;TorchScript语言参考》&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="033419356306d2d296e24d272874b31ebedcd66e" translate="yes" xml:space="preserve">
          <source>Scripting an &lt;code&gt;nn.Module&lt;/code&gt; by default will compile the &lt;code&gt;forward&lt;/code&gt; method and recursively compile any methods, submodules, and functions called by &lt;code&gt;forward&lt;/code&gt;. If a &lt;code&gt;nn.Module&lt;/code&gt; only uses features supported in TorchScript, no changes to the original module code should be necessary. &lt;code&gt;script&lt;/code&gt; will construct &lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt;&lt;code&gt;ScriptModule&lt;/code&gt;&lt;/a&gt; that has copies of the attributes, parameters, and methods of the original module.</source>
          <target state="translated">脚本的情况 &lt;code&gt;nn.Module&lt;/code&gt; 默认会编译 &lt;code&gt;forward&lt;/code&gt; 方法和递归编写的任何方法，子模块和功能通过所谓的 &lt;code&gt;forward&lt;/code&gt; 。如果 &lt;code&gt;nn.Module&lt;/code&gt; 仅使用TorchScript支持的功能，则无需更改原始模块代码。 &lt;code&gt;script&lt;/code&gt; 将构造&lt;a href=&quot;torch.jit.scriptmodule#torch.jit.ScriptModule&quot;&gt; &lt;code&gt;ScriptModule&lt;/code&gt; &lt;/a&gt;，该脚本具有原始模块的属性，参数和方法的副本。</target>
        </trans-unit>
        <trans-unit id="19791ed5c10ea9f7f4c0c92eb2869473896cff09" translate="yes" xml:space="preserve">
          <source>Search the distribution in the histogram for optimal min/max values.</source>
          <target state="translated">在直方图中搜索分布,寻找最佳的最小/最大值。</target>
        </trans-unit>
        <trans-unit id="2b7757dc0bce246d1f7d37c65a1b7323d32d2496" translate="yes" xml:space="preserve">
          <source>Second, some operators will produce different values depending on whether or not they are coalesced or not (e.g., &lt;a href=&quot;#torch.sparse.FloatTensor._values&quot;&gt;&lt;code&gt;torch.sparse.FloatTensor._values()&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#torch.sparse.FloatTensor._indices&quot;&gt;&lt;code&gt;torch.sparse.FloatTensor._indices()&lt;/code&gt;&lt;/a&gt;, as well as &lt;a href=&quot;tensors#torch.Tensor.sparse_mask&quot;&gt;&lt;code&gt;torch.Tensor.sparse_mask()&lt;/code&gt;&lt;/a&gt;). These operators are prefixed by an underscore to indicate that they reveal internal implementation details and should be used with care, since code that works with coalesced sparse tensors may not work with uncoalesced sparse tensors; generally speaking, it is safest to explicitly coalesce before working with these operators.</source>
          <target state="translated">其次，某些运算符将根据是否合并而产生不同的值（例如，&lt;a href=&quot;#torch.sparse.FloatTensor._values&quot;&gt; &lt;code&gt;torch.sparse.FloatTensor._values()&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;#torch.sparse.FloatTensor._indices&quot;&gt; &lt;code&gt;torch.sparse.FloatTensor._indices()&lt;/code&gt; &lt;/a&gt;以及&lt;a href=&quot;tensors#torch.Tensor.sparse_mask&quot;&gt; &lt;code&gt;torch.Tensor.sparse_mask()&lt;/code&gt; &lt;/a&gt;）。这些运算符以下划线作为前缀，表示它们揭示了内部实现细节，因此应谨慎使用，因为与合并的稀疏张量一起使用的代码可能不适用于未合并的稀疏张量；一般来说，与这些运营商合作之前，明确合并是最安全的。</target>
        </trans-unit>
        <trans-unit id="a158648517089dfe2e8140b70f8bef8fa8b3034b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#dataloader-collate-fn&quot;&gt;this section&lt;/a&gt; on more about &lt;code&gt;collate_fn&lt;/code&gt;.</source>
          <target state="translated">有关 &lt;code&gt;collate_fn&lt;/code&gt; 更多信息，请参见&lt;a href=&quot;#dataloader-collate-fn&quot;&gt;本节&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="36c23f7f90f14da5821a9b8b12151f9fc9d50660" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#dataset-types&quot;&gt;Dataset Types&lt;/a&gt; for more details on these two types of datasets and how &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt;&lt;code&gt;IterableDataset&lt;/code&gt;&lt;/a&gt; interacts with &lt;a href=&quot;#multi-process-data-loading&quot;&gt;Multi-process data loading&lt;/a&gt;.</source>
          <target state="translated">有关这两种类型的数据集以及&lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt; &lt;code&gt;IterableDataset&lt;/code&gt; &lt;/a&gt;如何与&lt;a href=&quot;#multi-process-data-loading&quot;&gt;多进程数据加载进行&lt;/a&gt;交互的更多详细信息，请参见&lt;a href=&quot;#dataset-types&quot;&gt;数据集类型&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="b547fa358f4d375a4ada3bd99e7498b8afae4202" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#module-torch.utils.data&quot;&gt;&lt;code&gt;torch.utils.data&lt;/code&gt;&lt;/a&gt; documentation page for more details.</source>
          <target state="translated">有关更多详细信息，请参见&lt;a href=&quot;#module-torch.utils.data&quot;&gt; &lt;code&gt;torch.utils.data&lt;/code&gt; &lt;/a&gt;文档页面。</target>
        </trans-unit>
        <trans-unit id="5b6dced15fa8a0213a82f02ae1effe79ccf48d16" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt; for restrictions on tensor names.</source>
          <target state="translated">请参阅&lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt;以了解对张量名称的限制。</target>
        </trans-unit>
        <trans-unit id="243a5526f4842ad1af50687b538ec9b7b7b4698f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.cuda.max_memory_allocated&quot;&gt;&lt;code&gt;max_memory_allocated()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">有关详细信息，请参见&lt;a href=&quot;#torch.cuda.max_memory_allocated&quot;&gt; &lt;code&gt;max_memory_allocated()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="18ad60e0a411bdc88fa0f09bd839bc7ff6f95e28" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.cuda.max_memory_cached&quot;&gt;&lt;code&gt;max_memory_cached()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">有关详细信息，请参见&lt;a href=&quot;#torch.cuda.max_memory_cached&quot;&gt; &lt;code&gt;max_memory_cached()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="d23b93420a80f6bb1829feef82f05084851c1675" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.nn.quantized.Conv1d&quot;&gt;&lt;code&gt;Conv1d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">有关详细信息和输出形状，请参见&lt;a href=&quot;#torch.nn.quantized.Conv1d&quot;&gt; &lt;code&gt;Conv1d&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="b27704ec42ef1503a1d419e6619ba79f9a13ebfb" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.nn.quantized.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">有关详细信息和输出形状，请参见&lt;a href=&quot;#torch.nn.quantized.Conv2d&quot;&gt; &lt;code&gt;Conv2d&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="56a1d3982f38299aa1d9205f01ea00cd34354fef" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.nn.quantized.Conv3d&quot;&gt;&lt;code&gt;Conv3d&lt;/code&gt;&lt;/a&gt; for details and output shape.</source>
          <target state="translated">有关详细信息和输出形状，请参见&lt;a href=&quot;#torch.nn.quantized.Conv3d&quot;&gt; &lt;code&gt;Conv3d&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="b4ccb661511817b74aa1b611837a5a3335536f1d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.utils.checkpoint.checkpoint&quot;&gt;&lt;code&gt;checkpoint()&lt;/code&gt;&lt;/a&gt; on how checkpointing works.</source>
          <target state="translated">有关&lt;a href=&quot;#torch.utils.checkpoint.checkpoint&quot;&gt; &lt;code&gt;checkpoint()&lt;/code&gt; &lt;/a&gt;如何工作的信息，请参见checkpoint（）。</target>
        </trans-unit>
        <trans-unit id="592148f1f545aa4c74a139446df1dc46e1b6f5ca" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt;&lt;code&gt;load()&lt;/code&gt;&lt;/a&gt; for a description of arguments omitted below.</source>
          <target state="translated">有关以下省略的参数的描述，请参见&lt;a href=&quot;#torch.utils.cpp_extension.load&quot;&gt; &lt;code&gt;load()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="3896f3dcf8c1d0bd4419fbc3e2eb2617d32af3a0" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.utils.data.Dataset&quot;&gt;&lt;code&gt;Dataset&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">有关更多详细信息，请参见&lt;a href=&quot;#torch.utils.data.Dataset&quot;&gt; &lt;code&gt;Dataset&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="1b587694da52c7658cba1e5d21c7a0bf9aad6739" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt;&lt;code&gt;IterableDataset&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">有关更多详细信息，请参见&lt;a href=&quot;#torch.utils.data.IterableDataset&quot;&gt; &lt;code&gt;IterableDataset&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="dd3e6f221b9ffca56a618c8ce4986b9d9a3b9a59" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;#variable-resolution&quot;&gt;Variable Resolution&lt;/a&gt; for how variables are resolved.</source>
          <target state="translated">有关如何解析变量的信息，请参见&lt;a href=&quot;#variable-resolution&quot;&gt;变量分辨率&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6663536c00122040c87a44bdc2b745caee1f1423" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../jit#inspecting-code&quot;&gt;Inspecting Code&lt;/a&gt; for details.</source>
          <target state="translated">有关详细信息，请参见&lt;a href=&quot;../jit#inspecting-code&quot;&gt;检查代码&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="eb8c27e43e09f15fd524c6c5a38cd014825e6c23" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;../tensors#torch.Tensor.view&quot;&gt;&lt;code&gt;torch.Tensor.view()&lt;/code&gt;&lt;/a&gt; on when it is possible to return a view.</source>
          <target state="translated">何时可以返回视图，请参见&lt;a href=&quot;../tensors#torch.Tensor.view&quot;&gt; &lt;code&gt;torch.Tensor.view()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0410d55962a7fc08aaf5d7b12288cb6edf33cd4b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt;&lt;code&gt;torch.abs()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt; &lt;code&gt;torch.abs()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ced4292c46e2d0f9084a0d4bbafd060bcaf8de87" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.acos#torch.acos&quot;&gt;&lt;code&gt;torch.acos()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.acos#torch.acos&quot;&gt; &lt;code&gt;torch.acos()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d557ccd80fb3258ddffbbb8174134a02d6cf179c" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.acosh#torch.acosh&quot;&gt;&lt;code&gt;torch.acosh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.acosh#torch.acosh&quot;&gt; &lt;code&gt;torch.acosh()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="789fcdc7d8a9ef51de9ddf2ef8954e307a7dd712" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.add#torch.add&quot;&gt;&lt;code&gt;torch.add()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.add#torch.add&quot;&gt; &lt;code&gt;torch.add()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="37a8639e7cd42640a040dc7726bd5f806eb0bd35" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.addbmm#torch.addbmm&quot;&gt;&lt;code&gt;torch.addbmm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.addbmm#torch.addbmm&quot;&gt; &lt;code&gt;torch.addbmm()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d031d4e4498ba97f52d6d80a33dad228fb9f2cbe" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.addcdiv#torch.addcdiv&quot;&gt;&lt;code&gt;torch.addcdiv()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.addcdiv#torch.addcdiv&quot;&gt; &lt;code&gt;torch.addcdiv()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="565ec468f6c035b17c0ac0bb82ee2cbcd8edaca7" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.addcmul#torch.addcmul&quot;&gt;&lt;code&gt;torch.addcmul()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.addcmul#torch.addcmul&quot;&gt; &lt;code&gt;torch.addcmul()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b2d04f2a20233d7e081134dcc2710d272865025a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.addmm#torch.addmm&quot;&gt;&lt;code&gt;torch.addmm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.addmm#torch.addmm&quot;&gt; &lt;code&gt;torch.addmm()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fbb6e45db46d337e88fb01997ddfd0d38b1e36e5" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.addmv#torch.addmv&quot;&gt;&lt;code&gt;torch.addmv()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.addmv#torch.addmv&quot;&gt; &lt;code&gt;torch.addmv()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0415452e192f39adc1dfb989387d7c4f3dd04593" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.addr#torch.addr&quot;&gt;&lt;code&gt;torch.addr()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.addr#torch.addr&quot;&gt; &lt;code&gt;torch.addr()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7040368b881cece8154506daeab47e7b26bc4465" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.allclose#torch.allclose&quot;&gt;&lt;code&gt;torch.allclose()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.allclose#torch.allclose&quot;&gt; &lt;code&gt;torch.allclose()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b5b8be1187577d5e5a0b32510f1f32129626cca8" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.amax#torch.amax&quot;&gt;&lt;code&gt;torch.amax()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.amax#torch.amax&quot;&gt; &lt;code&gt;torch.amax()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bac9b8922811f35c234bf464a4494dbef74933b2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.amin#torch.amin&quot;&gt;&lt;code&gt;torch.amin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.amin#torch.amin&quot;&gt; &lt;code&gt;torch.amin()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d5518b7da2603866919ecc93573fb7693cbe1d28" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.angle#torch.angle&quot;&gt;&lt;code&gt;torch.angle()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.angle#torch.angle&quot;&gt; &lt;code&gt;torch.angle()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6d6f36d28f663492799df5b5fcb97f2c82893849" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.arccos#torch.arccos&quot;&gt;&lt;code&gt;torch.arccos()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.arccos#torch.arccos&quot;&gt; &lt;code&gt;torch.arccos()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="95b1cd872f49253eed8e157b208ed8426d2a08f0" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.arccosh#torch.arccosh&quot;&gt;&lt;code&gt;torch.arccosh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.arccosh#torch.arccosh&quot;&gt; &lt;code&gt;torch.arccosh()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="20d7592e91b8ac1020ec98e06fb5bdf07f1891bc" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.arcsin#torch.arcsin&quot;&gt;&lt;code&gt;torch.arcsin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.arcsin#torch.arcsin&quot;&gt; &lt;code&gt;torch.arcsin()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="92063535c0e36748fc07d3768b27df3f9db98d33" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.arcsinh#torch.arcsinh&quot;&gt;&lt;code&gt;torch.arcsinh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.arcsinh#torch.arcsinh&quot;&gt; &lt;code&gt;torch.arcsinh()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f471a52f4701848a5c13052260382f54ba5f2bf2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.arctan#torch.arctan&quot;&gt;&lt;code&gt;torch.arctan()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.arctan#torch.arctan&quot;&gt; &lt;code&gt;torch.arctan()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6352e358921d4176f31796abd8e2e1d94c2c4807" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.arctanh#torch.arctanh&quot;&gt;&lt;code&gt;torch.arctanh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.arctanh#torch.arctanh&quot;&gt; &lt;code&gt;torch.arctanh()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8a3005660e169df440b195c1013e466fc0dc0f85" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.argmax#torch.argmax&quot;&gt;&lt;code&gt;torch.argmax()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.argmax#torch.argmax&quot;&gt; &lt;code&gt;torch.argmax()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9336c76dce7b82519130c38591a8f4bc511802b3" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.argmin#torch.argmin&quot;&gt;&lt;code&gt;torch.argmin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.argmin#torch.argmin&quot;&gt; &lt;code&gt;torch.argmin()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8db3f6696a6f968e7692af8c5ef5b20e20027f77" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.argsort#torch.argsort&quot;&gt;&lt;code&gt;torch.argsort()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.argsort#torch.argsort&quot;&gt; &lt;code&gt;torch.argsort()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4a2ff963efa23ea4ed3e29949ad7d7c3a624603f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.as_strided#torch.as_strided&quot;&gt;&lt;code&gt;torch.as_strided()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.as_strided#torch.as_strided&quot;&gt; &lt;code&gt;torch.as_strided()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="885b7e053cac4178633ab2cdae8c61527522af3a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.asin#torch.asin&quot;&gt;&lt;code&gt;torch.asin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.asin#torch.asin&quot;&gt; &lt;code&gt;torch.asin()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4b825056a4e3742071a4388ba1c9dd880e2f684d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.asinh#torch.asinh&quot;&gt;&lt;code&gt;torch.asinh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.asinh#torch.asinh&quot;&gt; &lt;code&gt;torch.asinh()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="85ed8161a1e7cf3d498c0e025910cad264dbb99e" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.atan#torch.atan&quot;&gt;&lt;code&gt;torch.atan()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.atan#torch.atan&quot;&gt; &lt;code&gt;torch.atan()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7c4e94754d2f650204d6da4aafe2bedeb278399d" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.atan2#torch.atan2&quot;&gt;&lt;code&gt;torch.atan2()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.atan2#torch.atan2&quot;&gt; &lt;code&gt;torch.atan2()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ed04a790c582bea4569ce9ba2a1f5fb49e96591b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.atanh#torch.atanh&quot;&gt;&lt;code&gt;torch.atanh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.atanh#torch.atanh&quot;&gt; &lt;code&gt;torch.atanh()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2f172a9267504e77b4582e306093325cd390dbe6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.baddbmm#torch.baddbmm&quot;&gt;&lt;code&gt;torch.baddbmm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.baddbmm#torch.baddbmm&quot;&gt; &lt;code&gt;torch.baddbmm()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0ee51350ddf5c14b20523cca547052313b6da137" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.bernoulli#torch.bernoulli&quot;&gt;&lt;code&gt;torch.bernoulli()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.bernoulli#torch.bernoulli&quot;&gt; &lt;code&gt;torch.bernoulli()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fa6bd5fd2fc0d4cbf209b32b685dcd15549e17a1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.bincount#torch.bincount&quot;&gt;&lt;code&gt;torch.bincount()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.bincount#torch.bincount&quot;&gt; &lt;code&gt;torch.bincount()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1330e38a3bff864a896452adaef91e316142ee7a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.bitwise_and#torch.bitwise_and&quot;&gt;&lt;code&gt;torch.bitwise_and()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.bitwise_and#torch.bitwise_and&quot;&gt; &lt;code&gt;torch.bitwise_and()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b232e0fb96bd322c2458574164c04bbfc68e696b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.bitwise_not#torch.bitwise_not&quot;&gt;&lt;code&gt;torch.bitwise_not()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.bitwise_not#torch.bitwise_not&quot;&gt; &lt;code&gt;torch.bitwise_not()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="49bd1ecf94a298ed7136ddaa7b129d79af6f4f58" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.bitwise_or#torch.bitwise_or&quot;&gt;&lt;code&gt;torch.bitwise_or()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.bitwise_or#torch.bitwise_or&quot;&gt; &lt;code&gt;torch.bitwise_or()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="74f2dbe9a2909c1557b61d4ca27fc1dfa0ee2a38" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.bitwise_xor#torch.bitwise_xor&quot;&gt;&lt;code&gt;torch.bitwise_xor()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.bitwise_xor#torch.bitwise_xor&quot;&gt; &lt;code&gt;torch.bitwise_xor()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="50e2d514fed6ae782003300fa73df0e3bb831323" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.bmm#torch.bmm&quot;&gt;&lt;code&gt;torch.bmm()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.bmm#torch.bmm&quot;&gt; &lt;code&gt;torch.bmm()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1c1407ae846f94e16c4554a9aed2a962bd7d5b41" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.ceil#torch.ceil&quot;&gt;&lt;code&gt;torch.ceil()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.ceil#torch.ceil&quot;&gt; &lt;code&gt;torch.ceil()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0ee9005d9f7612dda3673351dab212c7597eacf9" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cholesky#torch.cholesky&quot;&gt;&lt;code&gt;torch.cholesky()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.cholesky#torch.cholesky&quot;&gt; &lt;code&gt;torch.cholesky()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9234f12a7920c48c92b0b92789cbf7f520e6cce2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cholesky_inverse#torch.cholesky_inverse&quot;&gt;&lt;code&gt;torch.cholesky_inverse()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.cholesky_inverse#torch.cholesky_inverse&quot;&gt; &lt;code&gt;torch.cholesky_inverse()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="95412d4a68f7be2f448d2f6abdef3ba1039693ce" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cholesky_solve#torch.cholesky_solve&quot;&gt;&lt;code&gt;torch.cholesky_solve()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.cholesky_solve#torch.cholesky_solve&quot;&gt; &lt;code&gt;torch.cholesky_solve()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="70d52135d5fe5109fdf4b68140efc30ed3ff15ed" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.chunk#torch.chunk&quot;&gt;&lt;code&gt;torch.chunk()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.chunk#torch.chunk&quot;&gt; &lt;code&gt;torch.chunk()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="9d2a8fa141c274f1ecc888c9dee97f508028c310" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.clamp#torch.clamp&quot;&gt;&lt;code&gt;torch.clamp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.clamp#torch.clamp&quot;&gt; &lt;code&gt;torch.clamp()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="db643765ea3bdabe789a609a784f6f21e8b15ff6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.clone#torch.clone&quot;&gt;&lt;code&gt;torch.clone()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.clone#torch.clone&quot;&gt; &lt;code&gt;torch.clone()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2c5dee157b17c3fc64e10f3c111962a223627009" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.conj#torch.conj&quot;&gt;&lt;code&gt;torch.conj()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.conj#torch.conj&quot;&gt; &lt;code&gt;torch.conj()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="00b6992059c4a2ecf9c5ba83b4b4d6367e100b3a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cos#torch.cos&quot;&gt;&lt;code&gt;torch.cos()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.cos#torch.cos&quot;&gt; &lt;code&gt;torch.cos()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5f760d7b54a866c270d1f4f0fee69afc86260ce1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cosh#torch.cosh&quot;&gt;&lt;code&gt;torch.cosh()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.cosh#torch.cosh&quot;&gt; &lt;code&gt;torch.cosh()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2e6ca2ba899e5a5ed0308b11c6dec0f921c1c386" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.count_nonzero#torch.count_nonzero&quot;&gt;&lt;code&gt;torch.count_nonzero()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.count_nonzero#torch.count_nonzero&quot;&gt; &lt;code&gt;torch.count_nonzero()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3154a66df758bedd23c1ca8d445818980c877056" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cross#torch.cross&quot;&gt;&lt;code&gt;torch.cross()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.cross#torch.cross&quot;&gt; &lt;code&gt;torch.cross()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c33b1f8ea95f5374185b2beef8bdc0333006ca40" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cummax#torch.cummax&quot;&gt;&lt;code&gt;torch.cummax()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.cummax#torch.cummax&quot;&gt; &lt;code&gt;torch.cummax()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b7a6c4b7502006ffffe7f61bc361df0a8f5c3485" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cummin#torch.cummin&quot;&gt;&lt;code&gt;torch.cummin()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.cummin#torch.cummin&quot;&gt; &lt;code&gt;torch.cummin()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="175fd7991a76a77d1e7947c94b32c3900db2a22a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cumprod#torch.cumprod&quot;&gt;&lt;code&gt;torch.cumprod()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.cumprod#torch.cumprod&quot;&gt; &lt;code&gt;torch.cumprod()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ecda9d651f57ce0a80f553759df7118f9fc98f17" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.cumsum#torch.cumsum&quot;&gt;&lt;code&gt;torch.cumsum()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.cumsum#torch.cumsum&quot;&gt; &lt;code&gt;torch.cumsum()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e4c2bb557dabd8b8d6424a1086a8f0e49082dcc1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.deg2rad#torch.deg2rad&quot;&gt;&lt;code&gt;torch.deg2rad()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.deg2rad#torch.deg2rad&quot;&gt; &lt;code&gt;torch.deg2rad()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="74fe8f78b0f15bbe31849b729e7353cfc88dda04" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.det#torch.det&quot;&gt;&lt;code&gt;torch.det()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.det#torch.det&quot;&gt; &lt;code&gt;torch.det()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="87d829bc645704d604ad02215c0bb4c96717d9da" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.diag#torch.diag&quot;&gt;&lt;code&gt;torch.diag()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.diag#torch.diag&quot;&gt; &lt;code&gt;torch.diag()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d17eea2fe4310acb0e6689f3868245d146ae748a" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.diag_embed#torch.diag_embed&quot;&gt;&lt;code&gt;torch.diag_embed()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.diag_embed#torch.diag_embed&quot;&gt; &lt;code&gt;torch.diag_embed()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="07ddff47af47155cb8195d49fd74ab5f110f0de6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.diagflat#torch.diagflat&quot;&gt;&lt;code&gt;torch.diagflat()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.diagflat#torch.diagflat&quot;&gt; &lt;code&gt;torch.diagflat()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e9871639ca6ca069867ff9833f0d0bad21e81d3f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;torch.diagonal()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;torch.diagonal()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="fd7c0b43d807db12fbff5fc78c8898e1347cf1a1" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.digamma#torch.digamma&quot;&gt;&lt;code&gt;torch.digamma()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.digamma#torch.digamma&quot;&gt; &lt;code&gt;torch.digamma()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ec5b576fe8744daae4dc7030555211156b888d46" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.dist#torch.dist&quot;&gt;&lt;code&gt;torch.dist()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.dist#torch.dist&quot;&gt; &lt;code&gt;torch.dist()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8ab62666946a8333d7379a60a2d1d9d3ae225fa2" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.div#torch.div&quot;&gt;&lt;code&gt;torch.div()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.div#torch.div&quot;&gt; &lt;code&gt;torch.div()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="0cfd78ad9784ac679c7a2b9b5bca896325ba89a4" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.divide#torch.divide&quot;&gt;&lt;code&gt;torch.divide()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.divide#torch.divide&quot;&gt; &lt;code&gt;torch.divide()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="593f13a108fe5ed0675dcd7c508f6c35b99b0b10" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.dot#torch.dot&quot;&gt;&lt;code&gt;torch.dot()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.dot#torch.dot&quot;&gt; &lt;code&gt;torch.dot()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="59a8ed7a18a443621255817cd4ad0bf4a0e632ca" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.eig#torch.eig&quot;&gt;&lt;code&gt;torch.eig()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.eig#torch.eig&quot;&gt; &lt;code&gt;torch.eig()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5ab6c51079e17cb89630c1e4a91f1d3454d90b6f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.eq#torch.eq&quot;&gt;&lt;code&gt;torch.eq()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.eq#torch.eq&quot;&gt; &lt;code&gt;torch.eq()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4813e89ec9293f99dfa5025547b461fb432f69d3" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.equal#torch.equal&quot;&gt;&lt;code&gt;torch.equal()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.equal#torch.equal&quot;&gt; &lt;code&gt;torch.equal()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3f7360444c9397fc1d5c5b1bc067d9af3c6e9edd" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.erf#torch.erf&quot;&gt;&lt;code&gt;torch.erf()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.erf#torch.erf&quot;&gt; &lt;code&gt;torch.erf()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="08fb634ad4f6c40b3935c0b3378501a7339f8fbe" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.erfc#torch.erfc&quot;&gt;&lt;code&gt;torch.erfc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.erfc#torch.erfc&quot;&gt; &lt;code&gt;torch.erfc()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2bd6c425dc9b646c2b2bed4edf1c5a988917c21f" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.erfinv#torch.erfinv&quot;&gt;&lt;code&gt;torch.erfinv()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.erfinv#torch.erfinv&quot;&gt; &lt;code&gt;torch.erfinv()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ad29dad56d468c778a3c249cc44c87db316a44e6" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.exp#torch.exp&quot;&gt;&lt;code&gt;torch.exp()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.exp#torch.exp&quot;&gt; &lt;code&gt;torch.exp()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6b6fbfd00b7b75b48f9ccfcacaa40ee11ef98750" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.expm1#torch.expm1&quot;&gt;&lt;code&gt;torch.expm1()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.expm1#torch.expm1&quot;&gt; &lt;code&gt;torch.expm1()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="4bbb47f4968cd75aa8ca7739f121cb837ee30c39" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.fft#torch.fft&quot;&gt;&lt;code&gt;torch.fft()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.fft#torch.fft&quot;&gt; &lt;code&gt;torch.fft()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a9a77b8be814999a1e18a3bcfe6fdc2b3f1b3bf0" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.fix#torch.fix&quot;&gt;&lt;code&gt;torch.fix()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.fix#torch.fix&quot;&gt; &lt;code&gt;torch.fix()&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="aad979493ad2ea8c7660622a96be7811d0a81b87" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.flip#torch.flip&quot;&gt;&lt;code&gt;torch.flip()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.flip#torch.flip&quot;&gt; &lt;code&gt;torch.flip()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bad831a531ebee9d67a2612a1d0e6978cc7ed27b" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.fliplr#torch.fliplr&quot;&gt;&lt;code&gt;torch.fliplr()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.fliplr#torch.fliplr&quot;&gt; &lt;code&gt;torch.fliplr()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="87f6a8078b8a3f6f5a4247460381f4e94549ef39" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.flipud#torch.flipud&quot;&gt;&lt;code&gt;torch.flipud()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.flipud#torch.flipud&quot;&gt; &lt;code&gt;torch.flipud()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="399317938ad3be99c06e85c37e27d75701458b99" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.floor#torch.floor&quot;&gt;&lt;code&gt;torch.floor()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.floor#torch.floor&quot;&gt; &lt;code&gt;torch.floor()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="bcdf651ea00fbe14dab01c29ec405b9ced982377" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.floor_divide#torch.floor_divide&quot;&gt;&lt;code&gt;torch.floor_divide()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">参见&lt;a href=&quot;generated/torch.floor_divide#torch.floor_divide&quot;&gt; &lt;code&gt;torch.floor_divide()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2efd0d46fdaf4a4f7fc46d9063330efa363c69ce" translate="yes" xml:space="preserve">
          <source>See &lt;a href=&quot;generated/torch.fmod#torch.fmod&quot;&gt;&lt;code&gt;torch.fmod()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">见&lt;a href=&quot;generated/torch.fmod#torch.fmod&quot;&gt; &lt;code&gt;torch.fmod()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
