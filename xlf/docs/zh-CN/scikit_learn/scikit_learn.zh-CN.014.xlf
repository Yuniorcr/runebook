<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="94f937d118108bbf56d117b69cfda51f345777ab" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.StratifiedShuffleSplit&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.model_selection.StratifiedShuffleSplit&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="6d2a45b4a228c0ff26d151abb4398a317e97c2e2" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.TimeSeriesSplit&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.model_selection.TimeSeriesSplit&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="1169f405dfa22e87cb64248f2c5b612de695b324" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.cross_val_predict&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.model_selection.cross_val_predict&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="d1e3e38243b5c0275e9fe55822527a766daf84e5" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.cross_val_score&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.model_selection.cross_val_score&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="cd66ce686ba75391d858a270576652221e9366cb" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.cross_validate&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.model_selection.cross_validate&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="64bdbb73303147fbfb7b4c72e2c5b9f862e6846a" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.learning_curve&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.model_selection.learning_curve&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="058189237019f568402a807ff0f37d6659f730bd" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.permutation_test_score&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.model_selection.permutation_test_score&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="a35d588868114f2ff75fd130567f7e47c8648990" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.train_test_split&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.model_selection.train_test_split&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="c436bb41de4b1b0cbaf9f2a30c84739212eaf00c" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.model_selection.validation_curve&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.model_selection.validation_curve&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="8c90ea159f2ef33465cc5505a41ebcb13d8fa556" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="3b803ea3410b546ac8080c64d5a283e65161ec36" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.multioutput.ClassifierChain&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.multioutput.ClassifierChain&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="175a335aedce67fab5414504b5b10913957177f2" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.multioutput.MultiOutputRegressor&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.multioutput.MultiOutputRegressor&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="7e5276c814e14e2874f1abc67268ecba0a9384b5" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.naive_bayes.BernoulliNB&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.naive_bayes.BernoulliNB&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="07d68dd4fe4a915ea6b5e32e2eca6d299d857bb7" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.naive_bayes.ComplementNB&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.naive_bayes.ComplementNB&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="af3404bd8b46bee3672ba86cfbae1ef5ccc5adfa" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.naive_bayes.GaussianNB&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.naive_bayes.GaussianNB&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="141d21773d99f3975d70ea9e9ce62f28b143b5ad" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.naive_bayes.MultinomialNB&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.naive_bayes.MultinomialNB&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="5d999f3df66c782268c5cd0602cb79c507903292" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.KNeighborsClassifier&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.neighbors.KNeighborsClassifier&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="f26682456d970fbdab48470c970d2b2cf9d0be01" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.KNeighborsRegressor&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.neighbors.KNeighborsRegressor&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="da4ee64191a33554442daf81f4b75c328c16de8c" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.KNeighborsTransformer&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.neighbors.KNeighborsTransformer&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="2b0ca2d2cfcae04b6e8f648035a263800eba906a" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="135f507596f50f4128ab7f674fa719851e06ba93" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.LocalOutlierFactor&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.neighbors.LocalOutlierFactor&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="2d07ff0ebbbb5614d7d55c03e06e43f1f3f6d8b5" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.NearestCentroid&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.neighbors.NearestCentroid&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="0749621ad934aca64b6d4bb597f488ca85f63f5a" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.NeighborhoodComponentsAnalysis&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.neighbors.NeighborhoodComponentsAnalysis&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="9e409523b85d3bf8b35de4b9a79e58e19412df01" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="92b259d24e226f9725b7a2251b317d28f69ae1de" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neural_network.BernoulliRBM&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.neural_network.BernoulliRBM&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="bc26545aa247592ad5cae0c82982049aecb04005" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neural_network.MLPClassifier&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.neural_network.MLPClassifier&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="439d6a7e65183ef0435c18b6b323c74a5c51b635" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.neural_network.MLPRegressor&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.neural_network.MLPRegressor&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="aa6ee186b720b58c48c82ddfe44e7546cce24778" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.pipeline.FeatureUnion&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.pipeline.FeatureUnion&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="998ec3583b26e3fb4a4ed0aff8fa32c106d8d3fd" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="ce16abd7de545850afcb200375d7496f51a99f86" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.pipeline.make_pipeline&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.pipeline.make_pipeline&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="22b8f1c3009cd0b959254500e33f171fb13ddc5e" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.pipeline.make_union&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.pipeline.make_union&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="2542a7119b1badd4a5e17e03f5405355cc7a6b4e" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.FunctionTransformer&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.preprocessing.FunctionTransformer&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="c5ebd1b718c2b56f55f671f8d4b702c99b6f3049" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.KBinsDiscretizer&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.preprocessing.KBinsDiscretizer&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="c4381d9fd377759510c6b6b975b0aef8ecf5740c" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.MaxAbsScaler&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.preprocessing.MaxAbsScaler&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="6c598f0cbf4e71a390fc3aa76e817310e670b681" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.MinMaxScaler&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.preprocessing.MinMaxScaler&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="c1bc49d9937f0e2a5f4b0d4692671125b0d008b2" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.Normalizer&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.preprocessing.Normalizer&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="149136e5bc700b2e0a7747b2cd498f47536f44b4" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="11c249d2a654ff2f1c2cc110185680b161c2af36" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.OrdinalEncoder&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.preprocessing.OrdinalEncoder&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="56c4939b9d0afea5d1cebbe13cfe733258c34c00" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.PolynomialFeatures&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.preprocessing.PolynomialFeatures&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="3a155bf7a30ca61ce1f29447001d55507068d346" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.PowerTransformer&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.preprocessing.PowerTransformer&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="deb84a64d1425831a8777aa592bd6255271f75f8" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.QuantileTransformer&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.preprocessing.QuantileTransformer&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="6a98bd8bd28f139beba8d2222e8707ab286c5640" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.RobustScaler&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.preprocessing.RobustScaler&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="cc6025242e9ac27ec2143cf98f98385064f91436" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.StandardScaler&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.preprocessing.StandardScaler&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="be97d0bdf7a32b2c8b6fa3970798bb89bf783b34" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.label_binarize&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.preprocessing.label_binarize&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="36fa08e3b378f54ed6dffaaf0e419793c6ba101a" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.minmax_scale&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.preprocessing.minmax_scale&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="69e893f40e9fcd420f194c86a5ec119b1f003520" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.quantile_transform&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.preprocessing.quantile_transform&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="f1b108e5d448ff373a393340cccb912e8ffb72af" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.preprocessing.scale&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.preprocessing.scale&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="955e7e901699a2db4e4172aa9ab76068eea1b92f" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.random_projection.SparseRandomProjection&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.random_projection.SparseRandomProjection&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="4c4748cf4849c5fc863bc575791af8141a34cef5" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="88c1a4bb06d05e08ee20fd615bb4b53da175dc3d" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.semi_supervised.LabelSpreading&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.semi_supervised.LabelSpreading&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="737ca8cb1ccf3eeb065178263662a6be9c3d9649" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.set_config&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.set_config&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="7cd962b7c530c80e05b210277444c2bad820ca1e" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="edbf5922b6f27be5c5403e10197b447705e6f9cb" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.NuSVC&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.svm.NuSVC&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="12c9b218e18b941578bc9aca3c23747e42f54c1b" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.NuSVR&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.svm.NuSVR&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="dea2aede79f19fb95e5148ee5c11b3c7f223323a" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="6bc3b4bf10642b79be6b77851a23e142b0ca36cf" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.SVC&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.svm.SVC&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="c4dcb2fd7042cd90b5cd989b10ea50e9efa48d50" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.SVR&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.svm.SVR&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="2a2669f574a679afac893b53b03bdb1bd3060d05" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.svm.l1_min_c&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="15e7f4eb0d23fc3ecea22ca0e3a3ebdefcef3322" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.tree.DecisionTreeClassifier&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.tree.DecisionTreeClassifier&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="65c5a0a1817f47dc4a24c72d932486e31f108484" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.tree.DecisionTreeRegressor&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.tree.DecisionTreeRegressor&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="607ad6107d22cda5b789ada19b3e439fe3038b27" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.tree.plot_tree&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.tree.plot_tree&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="5b4d8799f6ce5100bf207b65e66619307f668cac" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.Bunch&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.utils.Bunch&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="668a018e39621dfc09781d5e1a018ed078089b72" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.Memory&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.utils.Memory&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="7213df901109f2c1652eb65fa0f35fe0ff18cd9d" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.check_random_state&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.utils.check_random_state&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="e9dab354869145a323333bb1a6f4abfc5f1cb7b1" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.estimator_checks.parametrize_with_checks&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.utils.estimator_checks.parametrize_with_checks&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="2865372d64705a746f4e393e76c01c0a1fae3b37" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.extmath.density&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.utils.extmath.density&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="da99fb3fb0ad21b853976b89d77d08cdf2ec4f84" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.gen_even_slices&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.utils.gen_even_slices&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="851edad00347cd3937bd7ea24424c348d471aca4" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.metaestimators.if_delegate_has_method&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.utils.metaestimators.if_delegate_has_method&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="08628ee68b552a536a7e40670b9091a9987d81dc" translate="yes" xml:space="preserve">
          <source>Examples using &lt;code&gt;sklearn.utils.shuffle&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;sklearn.utils.shuffle&lt;/code&gt; 的示例</target>
        </trans-unit>
        <trans-unit id="fb3447b632f6a431215776dcf254a01001a40c4f" translate="yes" xml:space="preserve">
          <source>Examples:</source>
          <target state="translated">Examples:</target>
        </trans-unit>
        <trans-unit id="2c25bc9aea0135b8a44078eaa686262d861b8d2c" translate="yes" xml:space="preserve">
          <source>Exception class to raise if estimator is used before fitting.</source>
          <target state="translated">如果在拟合前使用了估计器,则引发异常类。</target>
        </trans-unit>
        <trans-unit id="7233f2cfa7da608e08349effe3a0829359d064c0" translate="yes" xml:space="preserve">
          <source>Exception.with_traceback(tb) &amp;ndash; set self.__traceback__ to tb and return self.</source>
          <target state="translated">Exception.with_traceback（tb）&amp;ndash;将self .__ traceback__设置为tb并返回self。</target>
        </trans-unit>
        <trans-unit id="adb63819e55094a1c321051667ef69bc97910b3c" translate="yes" xml:space="preserve">
          <source>Exercise 1: Language identification</source>
          <target state="translated">练习一:语言识别</target>
        </trans-unit>
        <trans-unit id="f2d31e2f590f63884cdac3b6e0353c56e95636fd" translate="yes" xml:space="preserve">
          <source>Exercise 2: Sentiment Analysis on movie reviews</source>
          <target state="translated">练习二:电影评论的情感分析</target>
        </trans-unit>
        <trans-unit id="f076754d245dfa0bb055ce293bbe18bfa8d62689" translate="yes" xml:space="preserve">
          <source>Exercise 3: CLI text classification utility</source>
          <target state="translated">练习3:CLI文本分类工具</target>
        </trans-unit>
        <trans-unit id="4dc503dafcf231e8065504c4cd9f19a0dcdfc147" translate="yes" xml:space="preserve">
          <source>Exercises</source>
          <target state="translated">Exercises</target>
        </trans-unit>
        <trans-unit id="9b4e2cce8211934c05426343255dbbe40e6fc29d" translate="yes" xml:space="preserve">
          <source>Exercises for the tutorials</source>
          <target state="translated">教程的练习</target>
        </trans-unit>
        <trans-unit id="e172501d8e170f5e501dbb7e10f621b359347edb" translate="yes" xml:space="preserve">
          <source>Exhaustive search over specified parameter values for an estimator.</source>
          <target state="translated">对估计器的指定参数值进行彻底搜索。</target>
        </trans-unit>
        <trans-unit id="827e74ef83aecac9c6f9fc861a3a010bc5589266" translate="yes" xml:space="preserve">
          <source>Exp-Sine-Squared kernel (aka periodic kernel).</source>
          <target state="translated">Exp-Sine-Squared核(又称周期核)。</target>
        </trans-unit>
        <trans-unit id="a9607bbaaa4524856cf2140445f92e407ff55546" translate="yes" xml:space="preserve">
          <source>Exp-Sine-Squared kernel.</source>
          <target state="translated">Exp-Sine-Squared核。</target>
        </trans-unit>
        <trans-unit id="3b0c107b231b7c7d2dd1b666566c6604385763d3" translate="yes" xml:space="preserve">
          <source>Expected results for the top 5 most represented people in the dataset:</source>
          <target state="translated">数据集中最具代表性的前5名人员的预期结果。</target>
        </trans-unit>
        <trans-unit id="98312dc3857136b93e4f949a4e72a6712170156c" translate="yes" xml:space="preserve">
          <source>Explained variance regression score function</source>
          <target state="translated">解释方差回归得分函数</target>
        </trans-unit>
        <trans-unit id="31162dbfd1baf8644ae388945633979e4d4b742f" translate="yes" xml:space="preserve">
          <source>Explicit feature map approximation for RBF kernels</source>
          <target state="translated">RBF核的显式特征图近似法</target>
        </trans-unit>
        <trans-unit id="2139158fefd69dd7900f572f929357923b2e9c08" translate="yes" xml:space="preserve">
          <source>Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver=&amp;rsquo;adam&amp;rsquo;</source>
          <target state="translated">用于估计亚当中第一矩矢量的指数衰减率应为[0，1）。仅在solver ='adam'时使用</target>
        </trans-unit>
        <trans-unit id="3b517f2d5130d355e5abbd42b6b2d0de0b8022fe" translate="yes" xml:space="preserve">
          <source>Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver=&amp;rsquo;adam&amp;rsquo;</source>
          <target state="translated">用于估计亚当中第二矩矢量的指数衰减率应为[0，1）。仅在solver ='adam'时使用</target>
        </trans-unit>
        <trans-unit id="5be074c21803e5b45a92c260ef448b3876757aad" translate="yes" xml:space="preserve">
          <source>Exponential kernel (&lt;code&gt;kernel = 'exponential'&lt;/code&gt;)</source>
          <target state="translated">指数内核（ &lt;code&gt;kernel = 'exponential'&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="4e6d09cb3f7c099f340e3cd011063b05dd20e736" translate="yes" xml:space="preserve">
          <source>Exponential loss (&lt;code&gt;'exponential'&lt;/code&gt;): The same loss function as &lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt;&lt;code&gt;AdaBoostClassifier&lt;/code&gt;&lt;/a&gt;. Less robust to mislabeled examples than &lt;code&gt;'deviance'&lt;/code&gt;; can only be used for binary classification.</source>
          <target state="translated">指数损失（ &lt;code&gt;'exponential'&lt;/code&gt; ）：与&lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt; &lt;code&gt;AdaBoostClassifier&lt;/code&gt; &lt;/a&gt;相同的损失函数。对于标签错误的示例，其健壮性不如 &lt;code&gt;'deviance'&lt;/code&gt; ；只能用于二进制分类。</target>
        </trans-unit>
        <trans-unit id="82a5349d4a42ae4b58ce233c1fe754cf20695efd" translate="yes" xml:space="preserve">
          <source>Exponentiate kernel by given exponent.</source>
          <target state="translated">用给定的指数对内核进行指数化。</target>
        </trans-unit>
        <trans-unit id="6a1fb392b4816003f7cab8689b69b73b81fd13d1" translate="yes" xml:space="preserve">
          <source>Export a decision tree in DOT format.</source>
          <target state="translated">以DOT格式导出决策树。</target>
        </trans-unit>
        <trans-unit id="862ee3b17a826818107e616215cf3c3a954115aa" translate="yes" xml:space="preserve">
          <source>Exposure</source>
          <target state="translated">Exposure</target>
        </trans-unit>
        <trans-unit id="9eb0650b6756b55d46beda1492ea11acfe7e3431" translate="yes" xml:space="preserve">
          <source>Expresses to what extent the local structure is retained.</source>
          <target state="translated">表示当地结构的保留程度。</target>
        </trans-unit>
        <trans-unit id="03b256629a71913d7df02c71e2c067b3de2f6cf2" translate="yes" xml:space="preserve">
          <source>External Resources, Videos and Talks</source>
          <target state="translated">外部资源、视频和讲座</target>
        </trans-unit>
        <trans-unit id="6a53253fde7542b58764813563b294277a2499e0" translate="yes" xml:space="preserve">
          <source>External Tutorials</source>
          <target state="translated">外部教程</target>
        </trans-unit>
        <trans-unit id="d29859b915eecd05832dcd65405884b6cc0d4c40" translate="yes" xml:space="preserve">
          <source>Extra keyword arguments will be passed to matplotlib&amp;rsquo;s &lt;code&gt;plot&lt;/code&gt;.</source>
          <target state="translated">额外的关键字参数将传递给matplotlib的 &lt;code&gt;plot&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="8d6114c0a06ffd4fdec489d603f3c2293f085d8e" translate="yes" xml:space="preserve">
          <source>Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the &lt;code&gt;max_features&lt;/code&gt; randomly selected features and the best split among those is chosen. When &lt;code&gt;max_features&lt;/code&gt; is set 1, this amounts to building a totally random decision tree.</source>
          <target state="translated">额外树在构建方式上与经典决策树不同。当寻找最佳分割以将一个节点的样本分成两组时，将为每个 &lt;code&gt;max_features&lt;/code&gt; 随机选择的特征绘制随机分割，并在其中选择最佳分割。当 &lt;code&gt;max_features&lt;/code&gt; 设置为1时，这相当于建立一个完全随机的决策树。</target>
        </trans-unit>
        <trans-unit id="9955e456c4c0c464fbdd3775651693383fea52c1" translate="yes" xml:space="preserve">
          <source>Extract an ordered array of unique labels</source>
          <target state="translated">提取一个唯一标签的有序数组</target>
        </trans-unit>
        <trans-unit id="73640fcc3fc33a24884d5d906a4df64cbbe3523c" translate="yes" xml:space="preserve">
          <source>Extract token counts out of raw text documents using the vocabulary fitted with fit or the one provided to the constructor.</source>
          <target state="translated">使用适合的词汇或提供给构造函数的词汇,从原始文本文档中提取令牌数。</target>
        </trans-unit>
        <trans-unit id="752e876b40a502b1de5591e926917e4bc7914d7e" translate="yes" xml:space="preserve">
          <source>Extracting features from text files</source>
          <target state="translated">从文本文件中提取特征</target>
        </trans-unit>
        <trans-unit id="d28d7a25a071634693f533b81722275ae4a2006b" translate="yes" xml:space="preserve">
          <source>Extracting the clusters runs in linear time. Note that this results in &lt;code&gt;labels_&lt;/code&gt; which are close to a &lt;a href=&quot;sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;DBSCAN&lt;/code&gt;&lt;/a&gt; with similar settings and &lt;code&gt;eps&lt;/code&gt;, only if &lt;code&gt;eps&lt;/code&gt; is close to &lt;code&gt;max_eps&lt;/code&gt;.</source>
          <target state="translated">提取群集以线性时间运行。请注意，仅当 &lt;code&gt;eps&lt;/code&gt; 接近 &lt;code&gt;max_eps&lt;/code&gt; 时，这才导致 &lt;code&gt;labels_&lt;/code&gt; 接近具有相似设置和 &lt;code&gt;eps&lt;/code&gt; 的&lt;a href=&quot;sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt; &lt;code&gt;DBSCAN&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="f1c8b18c9f903ec74b12bbc8b3c4f7151c923f8b" translate="yes" xml:space="preserve">
          <source>Extracts an ordered list of points and reachability distances, and performs initial clustering using &lt;code&gt;max_eps&lt;/code&gt; distance specified at OPTICS object instantiation.</source>
          <target state="translated">提取点和可达距离的有序列表，并使用在OPTICS对象实例化时指定的 &lt;code&gt;max_eps&lt;/code&gt; 距离执行初始聚类。</target>
        </trans-unit>
        <trans-unit id="4d1d5b7b89c16041c19e08ba2433ff6ccea2c09b" translate="yes" xml:space="preserve">
          <source>Extracts patches from a collection of images</source>
          <target state="translated">从图像集合中提取补丁</target>
        </trans-unit>
        <trans-unit id="20b1f470ded66ddce46bb1f3957f49e776657ce0" translate="yes" xml:space="preserve">
          <source>F values of features.</source>
          <target state="translated">特征的F值。</target>
        </trans-unit>
        <trans-unit id="94361d25a9823c7799f46133208e1994c901281f" translate="yes" xml:space="preserve">
          <source>F-beta score of the positive class in binary classification or weighted average of the F-beta score of each class for the multiclass task.</source>
          <target state="translated">二元分类中正类的F-beta得分或多类任务中各类F-beta得分的加权平均值。</target>
        </trans-unit>
        <trans-unit id="b88bc0b15f26e6bb0b6f496fb42c5c9bb95eea78" translate="yes" xml:space="preserve">
          <source>F-value between label/feature for regression tasks.</source>
          <target state="translated">回归任务的标签/特征之间的F值。</target>
        </trans-unit>
        <trans-unit id="3ca92f821d80e2b3e5b34951989fdd6926d62950" translate="yes" xml:space="preserve">
          <source>F1 score of the positive class in binary classification or weighted average of the F1 scores of each class for the multiclass task.</source>
          <target state="translated">二元分类中正类的F1得分或多类任务中各类F1得分的加权平均值。</target>
        </trans-unit>
        <trans-unit id="03688ba6aa340b87549088aa5739944cb6b1dc73" translate="yes" xml:space="preserve">
          <source>FAQ</source>
          <target state="translated">FAQ</target>
        </trans-unit>
        <trans-unit id="761451a93e0c15b8090688d5167bdaf6518e982d" translate="yes" xml:space="preserve">
          <source>FPR test stands for False Positive Rate test. It controls the total amount of false detections.</source>
          <target state="translated">FPR测试是假阳性率测试的缩写。它控制着误检的总量。</target>
        </trans-unit>
        <trans-unit id="385b798ea2337dc2cc46e2c01984384fa2b4a883" translate="yes" xml:space="preserve">
          <source>F_beta</source>
          <target state="translated">F_beta</target>
        </trans-unit>
        <trans-unit id="272ad30c6a89ef4d06060249f8d92df03b3df406" translate="yes" xml:space="preserve">
          <source>Face completion with a multi-output estimators</source>
          <target state="translated">多输出估计器的面完成</target>
        </trans-unit>
        <trans-unit id="0507c7e4982962e832b85d06191a2b4d2bebd20a" translate="yes" xml:space="preserve">
          <source>Face recognition with eigenfaces</source>
          <target state="translated">利用特征面孔进行人脸识别</target>
        </trans-unit>
        <trans-unit id="2a0151d21d57d7f913fb01048c891608a6dbd1dd" translate="yes" xml:space="preserve">
          <source>Face, a 1024 x 768 size image of a raccoon face, is used here to illustrate how &lt;code&gt;k&lt;/code&gt;-means is used for vector quantization.</source>
          <target state="translated">人脸是浣熊脸的1024 x 768尺寸图像，在这里用于说明如何将 &lt;code&gt;k&lt;/code&gt; 均值用于矢量量化。</target>
        </trans-unit>
        <trans-unit id="e470f0e1bd32044d0ac0cbb19036da58e0de1f71" translate="yes" xml:space="preserve">
          <source>Faces dataset decompositions</source>
          <target state="translated">面部数据集分解</target>
        </trans-unit>
        <trans-unit id="05a6c8a2b029e9776dffd4a4e031ba78fd52aae9" translate="yes" xml:space="preserve">
          <source>Faces recognition example using eigenfaces and SVMs</source>
          <target state="translated">使用特征面和SVM进行人脸识别的例子</target>
        </trans-unit>
        <trans-unit id="1395bbf4ff3a0e8184103b7a1548ed436935fe5c" translate="yes" xml:space="preserve">
          <source>Factor Analysis (FA)</source>
          <target state="translated">因素分析(FA)</target>
        </trans-unit>
        <trans-unit id="ef94831c8deb9ad37fe90ab4fff5e44828bcfdae" translate="yes" xml:space="preserve">
          <source>Factor analysis &lt;em&gt;can&lt;/em&gt; produce similar components (the columns of its loading matrix) to &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt;. However, one can not make any general statements about these components (e.g. whether they are orthogonal):</source>
          <target state="translated">因子分析&lt;em&gt;可以&lt;/em&gt;产生与&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;相似的组件（其负载矩阵的列）。但是，不能对这些组件做出任何一般性声明（例如，它们是否正交）：</target>
        </trans-unit>
        <trans-unit id="d48954e3f19174382e76ae0104e93e844b15de73" translate="yes" xml:space="preserve">
          <source>FactorAnalysis performs a maximum likelihood estimate of the so-called &lt;code&gt;loading&lt;/code&gt; matrix, the transformation of the latent variables to the observed ones, using SVD based approach.</source>
          <target state="translated">FactorAnalysis使用基于SVD的方法对所谓的 &lt;code&gt;loading&lt;/code&gt; 矩阵进行最大似然估计，即将潜在变量转换为观测变量。</target>
        </trans-unit>
        <trans-unit id="6452044c700752d1602ef512b59b643cc8ae2400" translate="yes" xml:space="preserve">
          <source>FactorAnalysis performs a maximum likelihood estimate of the so-called &lt;code&gt;loading&lt;/code&gt; matrix, the transformation of the latent variables to the observed ones, using expectation-maximization (EM).</source>
          <target state="translated">FactorAnalysis 使用期望最大化（EM）对所谓的 &lt;code&gt;loading&lt;/code&gt; 矩阵执行最大似然估计，即将潜在变量转换为观测变量。</target>
        </trans-unit>
        <trans-unit id="cf3867c5acb3e93b6681ae294efcb69608ed1285" translate="yes" xml:space="preserve">
          <source>Factorization matrix, sometimes called &amp;lsquo;dictionary&amp;rsquo;.</source>
          <target state="translated">分解矩阵，有时称为&amp;ldquo;字典&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="c37fb3c7cf083e46f6a90c90a7e2709ff0a25468" translate="yes" xml:space="preserve">
          <source>False : never precompute distances</source>
          <target state="translated">假的:从不预先计算距离。</target>
        </trans-unit>
        <trans-unit id="4031377a355ac026bbc01c0a8c66abf927d5347f" translate="yes" xml:space="preserve">
          <source>False : never precompute distances.</source>
          <target state="translated">False:从不预先计算距离。</target>
        </trans-unit>
        <trans-unit id="9f59f0377a675667ec374342cc81b9550d388560" translate="yes" xml:space="preserve">
          <source>False positive rate.</source>
          <target state="translated">假阳性率。</target>
        </trans-unit>
        <trans-unit id="25cefd866e6e705dad47ce327a0915fc7b301c18" translate="yes" xml:space="preserve">
          <source>False when &lt;code&gt;y&lt;/code&gt;&amp;rsquo;s shape is (n_samples, ) or (n_samples, 1) during fit otherwise True.</source>
          <target state="translated">在拟合期间，当 &lt;code&gt;y&lt;/code&gt; 的形状为（n_samples，）或（n_samples，1）时为False，否则为True。</target>
        </trans-unit>
        <trans-unit id="3b5c093eaf163ad057d3c3eb085ce8a982ff3356" translate="yes" xml:space="preserve">
          <source>False: accept both np.inf and np.nan in X.</source>
          <target state="translated">假:同时接受X中的np.inf和np.nan。</target>
        </trans-unit>
        <trans-unit id="e8392e19182108180157ea67d85038a25937bf1d" translate="yes" xml:space="preserve">
          <source>False: accepts np.inf, np.nan, pd.NA in X.</source>
          <target state="translated">假:接受X中的np.inf、np.nan、pd.NA。</target>
        </trans-unit>
        <trans-unit id="ed9971d141108d2bfea275398777be51a58308ae" translate="yes" xml:space="preserve">
          <source>False: accepts np.inf, np.nan, pd.NA in array.</source>
          <target state="translated">False:接受数组中的np.inf、np.nan、pd.NA。</target>
        </trans-unit>
        <trans-unit id="804378d4f4edbfd6d3a924fd7282c07b3c8233b0" translate="yes" xml:space="preserve">
          <source>False: the results is casted to a signed int</source>
          <target state="translated">False:将结果转换为一个有符号的int。</target>
        </trans-unit>
        <trans-unit id="b73e8a734794b535117e86ab6a6ba9f9a65b9a31" translate="yes" xml:space="preserve">
          <source>Fan, Rong-En, et al., &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf&quot;&gt;&amp;ldquo;LIBLINEAR: A library for large linear classification.&amp;rdquo;&lt;/a&gt;, Journal of machine learning research 9.Aug (2008): 1871-1874.</source>
          <target state="translated">Fan，Rong-En等，&lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf&quot;&gt;&amp;ldquo; LIBLINEAR：用于大型线性分类的库。&amp;rdquo; &lt;/a&gt;，机器学习研究杂志9.Aug（2008）：1871-1874。</target>
        </trans-unit>
        <trans-unit id="c87c316f44a57817f64366d0bec154e1f9523fd3" translate="yes" xml:space="preserve">
          <source>Fancy token-level analysis such as stemming, lemmatizing, compound splitting, filtering based on part-of-speech, etc. are not included in the scikit-learn codebase, but can be added by customizing either the tokenizer or the analyzer. Here&amp;rsquo;s a &lt;code&gt;CountVectorizer&lt;/code&gt; with a tokenizer and lemmatizer using &lt;a href=&quot;http://www.nltk.org&quot;&gt;NLTK&lt;/a&gt;:</source>
          <target state="translated">scikit-learn代码库中不包含花式令牌级分析，例如词干，词根化，复合分解，基于词性的过滤等，但可以通过自定义令牌化器或分析器来添加。这是一个使用&lt;a href=&quot;http://www.nltk.org&quot;&gt;NLTK的&lt;/a&gt;带有分词器和 &lt;code&gt;CountVectorizer&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="47db1ac61ed9cca1bf16f757f65fe37b19da02d2" translate="yes" xml:space="preserve">
          <source>Fancy token-level analysis such as stemming, lemmatizing, compound splitting, filtering based on part-of-speech, etc. are not included in the scikit-learn codebase, but can be added by customizing either the tokenizer or the analyzer. Here&amp;rsquo;s a &lt;code&gt;CountVectorizer&lt;/code&gt; with a tokenizer and lemmatizer using &lt;a href=&quot;https://www.nltk.org/&quot;&gt;NLTK&lt;/a&gt;:</source>
          <target state="translated">scikit-learn代码库中不包含花式令牌级分析，例如词干，词根化，化合物分解，基于词性的过滤等，但可以通过自定义令牌化器或分析器来添加。这是一个使用&lt;a href=&quot;https://www.nltk.org/&quot;&gt;NLTK的&lt;/a&gt;带分词器和 &lt;code&gt;CountVectorizer&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="000599c940f606b6105f7d1916791a94094aca12" translate="yes" xml:space="preserve">
          <source>Fast computation of nearest neighbors is an active area of research in machine learning. The most naive neighbor search implementation involves the brute-force computation of distances between all pairs of points in the dataset: for \(N\) samples in \(D\) dimensions, this approach scales as \(O[D N^2]\). Efficient brute-force neighbors searches can be very competitive for small data samples. However, as the number of samples \(N\) grows, the brute-force approach quickly becomes infeasible. In the classes within &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt;, brute-force neighbors searches are specified using the keyword &lt;code&gt;algorithm = 'brute'&lt;/code&gt;, and are computed using the routines available in &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">最近邻的快速计算是机器学习研究的活跃领域。最幼稚的邻居搜索实现涉及数据集中所有点对之间距离的蛮力计算：对于\（D \）维中的\（N \）个样本，此方法的缩放比例为\（O [DN ^ 2] \）。对于小数据样本而言，有效的蛮力邻居搜索可能非常有竞争力。但是，随着样本数量\（N \）的增加，强力方法很快变得不可行。在&lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; &lt;/a&gt;内的类中，使用关键字 &lt;code&gt;algorithm = 'brute'&lt;/code&gt; 指定蛮力邻居搜索，并使用&lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; 中&lt;/a&gt;可用的例程进行计算。</target>
        </trans-unit>
        <trans-unit id="2e91ff193b9320036698a113daf46d8118a44e7d" translate="yes" xml:space="preserve">
          <source>FastICA on 2D point clouds</source>
          <target state="translated">二维点云上的FastICA</target>
        </trans-unit>
        <trans-unit id="6921319b009c74978f9e5104f25e9063c7cf2c6b" translate="yes" xml:space="preserve">
          <source>FastICA: a fast algorithm for Independent Component Analysis.</source>
          <target state="translated">FastICA:独立成分分析的快速算法。</target>
        </trans-unit>
        <trans-unit id="c2070bedc34b06cdbf740c2fed5e122d3efb93a7" translate="yes" xml:space="preserve">
          <source>Faster for large datasets</source>
          <target state="translated">更快地处理大型数据集</target>
        </trans-unit>
        <trans-unit id="6fa15d231f996464c9e743e456c3d6855ab0d8f6" translate="yes" xml:space="preserve">
          <source>Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition Letters, 2006, 27(8):861-874.</source>
          <target state="translated">Fawcett T.An introduction to ROC analysis[J].Pattern Recognition Letters,2006,27(8):861-874.</target>
        </trans-unit>
        <trans-unit id="0724b7da2e7893e2aa61b560332f137d5b0c3174" translate="yes" xml:space="preserve">
          <source>Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861-874.</source>
          <target state="translated">Fawcett,T.(2006)。ROC分析介绍。Pattern Recognition Letters,27(8),861-874.</target>
        </trans-unit>
        <trans-unit id="e6d44f9610f58495afc33ead5474258ef3212417" translate="yes" xml:space="preserve">
          <source>Fawcett, T., 2001. &lt;a href=&quot;http://ieeexplore.ieee.org/document/989510/&quot;&gt;Using rule sets to maximize ROC performance&lt;/a&gt; In Data Mining, 2001. Proceedings IEEE International Conference, pp. 131-138.</source>
          <target state="translated">Fawcett，T.，2001年。&lt;a href=&quot;http://ieeexplore.ieee.org/document/989510/&quot;&gt;使用规则集最大化ROC性能&lt;/a&gt;在Data Mining中，2001年。IEEE国际会议论文集，第131-138页。</target>
        </trans-unit>
        <trans-unit id="777dc3527e71dea0036f160bc11c11f47afad8e4" translate="yes" xml:space="preserve">
          <source>Fawcett, T., 2006. &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S016786550500303X&quot;&gt;An introduction to ROC analysis.&lt;/a&gt; Pattern Recognition Letters, 27(8), pp. 861-874.</source>
          <target state="translated">Fawcett，T.，2006&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S016786550500303X&quot;&gt;年。ROC分析简介。&lt;/a&gt;模式识别字母，第27（8）页，第861-874页。</target>
        </trans-unit>
        <trans-unit id="79246ec7915038316872f16646be2a17cce4452e" translate="yes" xml:space="preserve">
          <source>Fawcett, T., 2006. &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S016786550500303X&quot;&gt;An introduction to ROC analysis.&lt;/a&gt; Pattern Recognition Letters, 27(8), pp. 861-874.</source>
          <target state="translated">Fawcett，T.，2006&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S016786550500303X&quot;&gt;年。ROC分析简介。&lt;/a&gt;模式识别字母，第27（8）页，第861-874页。</target>
        </trans-unit>
        <trans-unit id="77834b29664ccb5e5cc9173718797e7ec674b1ed" translate="yes" xml:space="preserve">
          <source>Feature 0 (median income in a block) and feature 5 (number of households) of the &lt;a href=&quot;http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html&quot;&gt;California housing dataset&lt;/a&gt; have very different scales and contain some very large outliers. These two characteristics lead to difficulties to visualize the data and, more importantly, they can degrade the predictive performance of many machine learning algorithms. Unscaled data can also slow down or even prevent the convergence of many gradient-based estimators.</source>
          <target state="translated">&lt;a href=&quot;http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html&quot;&gt;加利福尼亚住房数据集的&lt;/a&gt;特征0（中位数收入）和特征5（家庭数）具有不同的尺度，并包含一些非常大的离群值。这两个特征导致难以可视化数据，更重要的是，它们会降低许多机器学习算法的预测性能。未缩放的数据也会减慢甚至阻止许多基于梯度的估计器的收敛。</target>
        </trans-unit>
        <trans-unit id="d43b2b4b53fe06f630d6e6fea5ae39ec97ef2464" translate="yes" xml:space="preserve">
          <source>Feature 0 (median income in a block) and feature 5 (number of households) of the &lt;a href=&quot;https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html&quot;&gt;California housing dataset&lt;/a&gt; have very different scales and contain some very large outliers. These two characteristics lead to difficulties to visualize the data and, more importantly, they can degrade the predictive performance of many machine learning algorithms. Unscaled data can also slow down or even prevent the convergence of many gradient-based estimators.</source>
          <target state="translated">&lt;a href=&quot;https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html&quot;&gt;加利福尼亚住房数据集的&lt;/a&gt;特征0（中位数收入）和特征5（家庭数）具有不同的尺度，并包含一些非常大的离群值。这两个特征导致难以可视化数据，更重要的是，它们会降低许多机器学习算法的预测性能。未缩放的数据还会减慢甚至阻止许多基于梯度的估计器的收敛。</target>
        </trans-unit>
        <trans-unit id="1e2bfc0d574ebb3c0866208d70d9a4defc9229e8" translate="yes" xml:space="preserve">
          <source>Feature Selection</source>
          <target state="translated">特征选择</target>
        </trans-unit>
        <trans-unit id="3290cbe981e79caf9ab6f89a8812507aa114a727" translate="yes" xml:space="preserve">
          <source>Feature agglomeration</source>
          <target state="translated">特征集聚</target>
        </trans-unit>
        <trans-unit id="81f8ec148b187c97182bf2c56b5ce15909bcbae4" translate="yes" xml:space="preserve">
          <source>Feature agglomeration vs. univariate selection</source>
          <target state="translated">特征集聚VS单变量选择</target>
        </trans-unit>
        <trans-unit id="49e1dcf68a4912e92e5e7f68710255b3ca05471e" translate="yes" xml:space="preserve">
          <source>Feature discretization</source>
          <target state="translated">特征离散化</target>
        </trans-unit>
        <trans-unit id="3c43171269f5f432d869bba89f82c3b54ca0debf" translate="yes" xml:space="preserve">
          <source>Feature extraction</source>
          <target state="translated">特征提取</target>
        </trans-unit>
        <trans-unit id="df5954aef1e3ea02a4e2fe61d1e0e10b9f726b5a" translate="yes" xml:space="preserve">
          <source>Feature extraction and normalization.</source>
          <target state="translated">特征提取和归一化。</target>
        </trans-unit>
        <trans-unit id="aedfee8f7e4b62695a0b89930709eed1e654b00f" translate="yes" xml:space="preserve">
          <source>Feature extraction is very different from &lt;a href=&quot;feature_selection#feature-selection&quot;&gt;Feature selection&lt;/a&gt;: the former consists in transforming arbitrary data, such as text or images, into numerical features usable for machine learning. The latter is a machine learning technique applied on these features.</source>
          <target state="translated">特征提取与&lt;a href=&quot;feature_selection#feature-selection&quot;&gt;特征选择&lt;/a&gt;有很大不同：特征选择包括将任意数据（例如文本或图像）转换为可用于机器学习的数字特征。后者是应用于这些功能的机器学习技术。</target>
        </trans-unit>
        <trans-unit id="4c3a870a4e311a31f848b5314c8f3949d8e31e78" translate="yes" xml:space="preserve">
          <source>Feature hashing can be employed in document classification, but unlike &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;text.CountVectorizer&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; does not do word splitting or any other preprocessing except Unicode-to-UTF-8 encoding; see &lt;a href=&quot;#hashing-vectorizer&quot;&gt;Vectorizing a large text corpus with the hashing trick&lt;/a&gt;, below, for a combined tokenizer/hasher.</source>
          <target state="translated">可以在文档分类中使用特征哈希，但与&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;text.CountVectorizer&lt;/code&gt; &lt;/a&gt;不同，&lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt;不会进行字分割或除Unicode到UTF-8编码外的任何其他预处理；请参阅下面的&lt;a href=&quot;#hashing-vectorizer&quot;&gt;使用哈希技巧对大型文本语料库进行向量化&lt;/a&gt;，以获取组合的标记器/哈希器。</target>
        </trans-unit>
        <trans-unit id="7bb8776f4e73559816ad10cf154be669f7df47cf" translate="yes" xml:space="preserve">
          <source>Feature importances with forests of trees</source>
          <target state="translated">树木森林的特点</target>
        </trans-unit>
        <trans-unit id="561e2555a0c1659bb31a6148c1abe250489ca708" translate="yes" xml:space="preserve">
          <source>Feature mappings for the samples in X.</source>
          <target state="translated">X中样本的特征映射。</target>
        </trans-unit>
        <trans-unit id="b96ff43d78f2a1f5a4e1d891c2cc36e4c380c32b" translate="yes" xml:space="preserve">
          <source>Feature matrix, for use with estimators or further transformers.</source>
          <target state="translated">特征矩阵,用于估计器或进一步的变换器。</target>
        </trans-unit>
        <trans-unit id="66d052b121f901629f4ce88124bd7cb2635fe497" translate="yes" xml:space="preserve">
          <source>Feature matrix.</source>
          <target state="translated">特征矩阵。</target>
        </trans-unit>
        <trans-unit id="c4587a739696f602e34f3bba74d9106ccf3fff49" translate="yes" xml:space="preserve">
          <source>Feature names corresponding to the indices in &lt;code&gt;features&lt;/code&gt;.</source>
          <target state="translated">特征对应于指数名称 &lt;code&gt;features&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="975d25794ea94f600d607356398714d33388d870" translate="yes" xml:space="preserve">
          <source>Feature names of type byte string are used as-is. Unicode strings are converted to UTF-8 first, but no Unicode normalization is done. Feature values must be (finite) numbers.</source>
          <target state="translated">字节串类型的特征名按原样使用。Unicode 字符串首先被转换为 UTF-8,但不进行 Unicode 归一化。特征值必须是(有限的)数字。</target>
        </trans-unit>
        <trans-unit id="9509bbe5f8817e2ca0562d6baac6f0ad1787e1d6" translate="yes" xml:space="preserve">
          <source>Feature ranking with recursive feature elimination and cross-validated selection of the best number of features.</source>
          <target state="translated">用递归特征消除和交叉验证选择最佳特征数进行特征排序。</target>
        </trans-unit>
        <trans-unit id="f706081a8ad25e74e9a88b9e19a48d5a46a52012" translate="yes" xml:space="preserve">
          <source>Feature ranking with recursive feature elimination.</source>
          <target state="translated">特征排序与递归特征消除。</target>
        </trans-unit>
        <trans-unit id="7f17a87c5de04e39e04129e9c0737edfeafaee92" translate="yes" xml:space="preserve">
          <source>Feature scaling through standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.</source>
          <target state="translated">通过标准化(或Z-score归一化)进行特征缩放可以成为许多机器学习算法的重要预处理步骤。标准化涉及到重新调整特征,使其具有标准正态分布的特性,其平均值为零,标准偏差为1。</target>
        </trans-unit>
        <trans-unit id="3ac99400fe4169f40a977ea7007419c98db61771" translate="yes" xml:space="preserve">
          <source>Feature scores between 0 and 1 for all values of the regularization parameter. The reference article suggests &lt;code&gt;scores_&lt;/code&gt; is the max of &lt;code&gt;all_scores_&lt;/code&gt;.</source>
          <target state="translated">对于正则化参数的所有值，特征得分在0和1之间。参考文章建议 &lt;code&gt;scores_&lt;/code&gt; 是最大 &lt;code&gt;all_scores_&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="aa75d746547685bebfdc2a7e366361e478fa27f0" translate="yes" xml:space="preserve">
          <source>Feature scores between 0 and 1.</source>
          <target state="translated">特征得分在0到1之间。</target>
        </trans-unit>
        <trans-unit id="afb880f1a910829f871ab84d5509ba79bf00b111" translate="yes" xml:space="preserve">
          <source>Feature selection is usually used as a pre-processing step before doing the actual learning. The recommended way to do this in scikit-learn is to use a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">在进行实际学习之前，通常将特征选择用作预处理步骤。推荐在scikit-learn中执行此操作的方法是使用&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="5a515063a6bfb2324e04a5cb7dd1fe0bf9c9b6c2" translate="yes" xml:space="preserve">
          <source>Feature selection mode.</source>
          <target state="translated">特征选择模式。</target>
        </trans-unit>
        <trans-unit id="c9b5854c848ad0aec62ba5b490b140e3c7a1bb7d" translate="yes" xml:space="preserve">
          <source>Feature selection using SelectFromModel and LassoCV</source>
          <target state="translated">使用SelectFromModel和LassoCV进行特征选择。</target>
        </trans-unit>
        <trans-unit id="d13bfd588897e09202c5ea9fa5c11da3f65c72ad" translate="yes" xml:space="preserve">
          <source>Feature selection with sparse data</source>
          <target state="translated">稀疏数据的特征选择</target>
        </trans-unit>
        <trans-unit id="8a9e4457b64c8313be96ab86a00a0aa32201d58b" translate="yes" xml:space="preserve">
          <source>Feature selector that removes all low-variance features.</source>
          <target state="translated">去除所有低方差特征的特征选择器。</target>
        </trans-unit>
        <trans-unit id="e1b0ac4f2f2d7f5b2a31bb18d47b68f8979e7db4" translate="yes" xml:space="preserve">
          <source>Feature transformations with ensembles of trees</source>
          <target state="translated">树群的特征变换</target>
        </trans-unit>
        <trans-unit id="9a6ded29908e935164e678c6c6abd840802b0c72" translate="yes" xml:space="preserve">
          <source>Feature values below or equal to this are replaced by 0, above it by 1. Threshold may not be less than 0 for operations on sparse matrices.</source>
          <target state="translated">特征值低于或等于此值的用0代替,高于此值的用1代替,对稀疏矩阵的操作,阈值不得小于0。</target>
        </trans-unit>
        <trans-unit id="0bf4742e81f7f65623aaef302013934b401b3eb5" translate="yes" xml:space="preserve">
          <source>Feature values in training data (also required for prediction)</source>
          <target state="translated">训练数据中的特征值(也是预测所需)</target>
        </trans-unit>
        <trans-unit id="3252ec9f22752fab5d0e2a9197cc3c5847a58e00" translate="yes" xml:space="preserve">
          <source>Feature vectors or other representations of training data (also required for prediction).</source>
          <target state="translated">训练数据的特征向量或其他表示方式(预测也需要)。</target>
        </trans-unit>
        <trans-unit id="1ed37149ccf99f3ffe537e50f5c181c973cd258e" translate="yes" xml:space="preserve">
          <source>Feature vectors or other representations of training data.</source>
          <target state="translated">特征向量或训练数据的其他表示方法。</target>
        </trans-unit>
        <trans-unit id="b7599b9ef6a2c582b4507f1b87929e181ce07276" translate="yes" xml:space="preserve">
          <source>Feature vectors; always 2-d.</source>
          <target state="translated">特征向量;总是二维的。</target>
        </trans-unit>
        <trans-unit id="8425332e36244b1448079ae72c3edeebff1ff3df" translate="yes" xml:space="preserve">
          <source>Feature-wise means</source>
          <target state="translated">从功能上看是指</target>
        </trans-unit>
        <trans-unit id="4dfce3abcd43918524b8cd8b96300d3efd01d1fb" translate="yes" xml:space="preserve">
          <source>Feature-wise transformation of the data.</source>
          <target state="translated">对数据进行特征化改造。</target>
        </trans-unit>
        <trans-unit id="5ce13d491431cc736ca395a68aa77c25024d518b" translate="yes" xml:space="preserve">
          <source>Feature-wise variances</source>
          <target state="translated">特征差异</target>
        </trans-unit>
        <trans-unit id="8c7ad0b456fa9bf3ae09e270340fb831f15c3f18" translate="yes" xml:space="preserve">
          <source>FeatureHasher and DictVectorizer Comparison</source>
          <target state="translated">FeatureHasher和DictVectorizer比较</target>
        </trans-unit>
        <trans-unit id="fc338f87a058158eb824b53705961801516a9460" translate="yes" xml:space="preserve">
          <source>Features</source>
          <target state="translated">Features</target>
        </trans-unit>
        <trans-unit id="7b144c2dee4c701c6fc61ec2c427f38a9b6c6529" translate="yes" xml:space="preserve">
          <source>Features 1 and 2 of the diabetes-dataset are fitted and plotted below. It illustrates that although feature 2 has a strong coefficient on the full model, it does not give us much regarding &lt;code&gt;y&lt;/code&gt; when compared to just feature 1</source>
          <target state="translated">拟合并绘制了糖尿病数据集的特征1和2。它说明了虽然特征2在完整模型上具有很强的系数，但是与仅特征1相比，它对 &lt;code&gt;y&lt;/code&gt; 的作用不大</target>
        </trans-unit>
        <trans-unit id="27bf161ea6af475b73d710eebe7f66e368f22dbd" translate="yes" xml:space="preserve">
          <source>Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.</source>
          <target state="translated">特征是从乳腺肿块的细针抽吸(FNA)数字化图像中计算出来的。它们描述了图像中存在的细胞核的特征。</target>
        </trans-unit>
        <trans-unit id="7dc73780307913b147af45ec74f5c7491dc77d15" translate="yes" xml:space="preserve">
          <source>Features got by optimizing the Huber loss.</source>
          <target state="translated">通过优化Huber损失得到的功能。</target>
        </trans-unit>
        <trans-unit id="0a06ba45f44f1716120206612e20d59fa0c2d9b4" translate="yes" xml:space="preserve">
          <source>Features that are deemed of &lt;strong&gt;low importance for a bad model&lt;/strong&gt; (low cross-validation score) could be &lt;strong&gt;very important for a good model&lt;/strong&gt;. Therefore it is always important to evaluate the predictive power of a model using a held-out set (or better with cross-validation) prior to computing importances. Permutation importance does not reflect to the intrinsic predictive value of a feature by itself but &lt;strong&gt;how important this feature is for a particular model&lt;/strong&gt;.</source>
          <target state="translated">被认为对&lt;strong&gt;不良模型不重要的功能&lt;/strong&gt;（较低的交叉验证分数）&lt;strong&gt;对于良好模型&lt;/strong&gt;可能&lt;strong&gt;非常重要&lt;/strong&gt;。因此，在计算重要性之前，使用保留集（或更好地进行交叉验证）评估模型的预测能力始终很重要。排列的重要性本身并不能反映特征的内在预测价值，而是&lt;strong&gt;该特征对于特定模型的重要性&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="285c7b098890786c959265d1fe4e4933df08392b" translate="yes" xml:space="preserve">
          <source>Features that do not occur in a sample (mapping) will have a zero value in the resulting array/matrix.</source>
          <target state="translated">在样本(映射)中没有出现的特征,在结果的数组/矩阵中会有一个零值。</target>
        </trans-unit>
        <trans-unit id="e9ec5e9ca7278a6df71a586ca47e81c93d4d7336" translate="yes" xml:space="preserve">
          <source>Features which contain all missing values at &lt;code&gt;fit&lt;/code&gt; are discarded upon &lt;code&gt;transform&lt;/code&gt;.</source>
          <target state="translated">包含所有丢失的 &lt;code&gt;fit&lt;/code&gt; 值的特征将在 &lt;code&gt;transform&lt;/code&gt; 丢弃。</target>
        </trans-unit>
        <trans-unit id="71724a67f7aa53e34d06b737e24512de12116bad" translate="yes" xml:space="preserve">
          <source>Features with a training-set variance lower than this threshold will be removed. The default is to keep all features with non-zero variance, i.e. remove the features that have the same value in all samples.</source>
          <target state="translated">训练集方差低于这个阈值的特征将被删除。默认情况是保留所有方差不为零的特征,即删除所有样本中数值相同的特征。</target>
        </trans-unit>
        <trans-unit id="c6023b69815535fce0c52cfebfa0b836b623efcf" translate="yes" xml:space="preserve">
          <source>Ferri, C&amp;egrave;sar &amp;amp; Hernandez-Orallo, Jose &amp;amp; Modroiu, R. (2009). &lt;a href=&quot;https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf&quot;&gt;An Experimental Comparison of Performance Measures for Classification.&lt;/a&gt; Pattern Recognition Letters. 30. 27-38.</source>
          <target state="translated">Ferri，C&amp;egrave;sar和Hernandez-Orallo，Jose＆Modroiu，R.（2009年）。&lt;a href=&quot;https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf&quot;&gt;分类性能指标的实验比较。&lt;/a&gt;模式识别字母。30. 27-38。</target>
        </trans-unit>
        <trans-unit id="7609c48291c2fa7cc57f1dc4a85a3683c0749b4f" translate="yes" xml:space="preserve">
          <source>Fetch an mldata.org data set</source>
          <target state="translated">获取一个mldata.org数据集。</target>
        </trans-unit>
        <trans-unit id="2b29d907c2ea7e85ef894ade96f5a9788aa198c9" translate="yes" xml:space="preserve">
          <source>Fetch dataset from openml by name or dataset id.</source>
          <target state="translated">通过名称或数据集id从openml中获取数据集。</target>
        </trans-unit>
        <trans-unit id="76c8d1e0086c4e650fc514d650a93664c1ba4aa6" translate="yes" xml:space="preserve">
          <source>Fevotte, C., &amp;amp; Idier, J. (2011). Algorithms for nonnegative matrix factorization with the beta-divergence. Neural Computation, 23(9).</source>
          <target state="translated">Fevotte，C.和Idier，J.（2011）。&amp;beta;散度的非负矩阵分解算法。神经计算，23（9）。</target>
        </trans-unit>
        <trans-unit id="07da69c9120fa48d1a8831ddfdb2e22d5f38a14c" translate="yes" xml:space="preserve">
          <source>Few clusters, even cluster size, non-flat geometry</source>
          <target state="translated">簇数少,甚至簇数少,非平面几何形状。</target>
        </trans-unit>
        <trans-unit id="ba82bf0fe9bdb74f2de9056ff08a208a0201f2e5" translate="yes" xml:space="preserve">
          <source>Field &lt;code&gt;support_vectors_&lt;/code&gt; is now empty, only indices of support vectors are stored in &lt;code&gt;support_&lt;/code&gt;</source>
          <target state="translated">字段 &lt;code&gt;support_vectors_&lt;/code&gt; 现在为空，仅支持向量的索引存储在 &lt;code&gt;support_&lt;/code&gt; 中</target>
        </trans-unit>
        <trans-unit id="e6b7eea98ef60c86409d4e4dd54b3796ed56341b" translate="yes" xml:space="preserve">
          <source>Figure containing partial dependence plots.</source>
          <target state="translated">图中包含部分依赖图。</target>
        </trans-unit>
        <trans-unit id="bf29ad109219929951ed38da3e672e781ba3fade" translate="yes" xml:space="preserve">
          <source>Figure containing the confusion matrix.</source>
          <target state="translated">包含混淆矩阵的图。</target>
        </trans-unit>
        <trans-unit id="65bafae58e0d2ce7524ef55e10fcc5ed53a13aa4" translate="yes" xml:space="preserve">
          <source>Figure containing the curve.</source>
          <target state="translated">包含曲线的图。</target>
        </trans-unit>
        <trans-unit id="63c41242bc3de30c7fa64bf90cc3aa34e5d08243" translate="yes" xml:space="preserve">
          <source>Filter: Select the p-values corresponding to Family-wise error rate</source>
          <target state="translated">滤波器。选择与家族误差率相对应的p值。</target>
        </trans-unit>
        <trans-unit id="d601967c960c718fae8a27ae10382e904525e519" translate="yes" xml:space="preserve">
          <source>Filter: Select the p-values for an estimated false discovery rate</source>
          <target state="translated">过滤器。选择估计错误发现率的p值。</target>
        </trans-unit>
        <trans-unit id="f9d7c51c4e21ffa778934e88c4ada6f78e753e59" translate="yes" xml:space="preserve">
          <source>Filter: Select the pvalues below alpha based on a FPR test.</source>
          <target state="translated">筛选。根据FPR测试选择低于alpha的p值。</target>
        </trans-unit>
        <trans-unit id="fa74263b1f44e0e368feef4eccc429e274602a1d" translate="yes" xml:space="preserve">
          <source>Final perplexity score on training set.</source>
          <target state="translated">训练集的最终迷惑性得分。</target>
        </trans-unit>
        <trans-unit id="e832a73dedeb0259ac793f9ac425f8e8ee172602" translate="yes" xml:space="preserve">
          <source>Finally it is possible to discover the main topics of a corpus by relaxing the hard assignment constraint of clustering, for instance by using &lt;a href=&quot;decomposition#nmf&quot;&gt;Non-negative matrix factorization (NMF or NNMF)&lt;/a&gt;:</source>
          <target state="translated">最后，可以通过放宽聚类的硬分配约束，例如通过使用&lt;a href=&quot;decomposition#nmf&quot;&gt;非负矩阵分解（NMF或NNMF）&lt;/a&gt;来发现语料库的主要主题：</target>
        </trans-unit>
        <trans-unit id="3879f3b348e8db024fb12750b7a6d38d6bbf41c3" translate="yes" xml:space="preserve">
          <source>Finally one can also observe that for some intermediate values of &lt;code&gt;gamma&lt;/code&gt; we get equally performing models when &lt;code&gt;C&lt;/code&gt; becomes very large: it is not necessary to regularize by enforcing a larger margin. The radius of the RBF kernel alone acts as a good structural regularizer. In practice though it might still be interesting to simplify the decision function with a lower value of &lt;code&gt;C&lt;/code&gt; so as to favor models that use less memory and that are faster to predict.</source>
          <target state="translated">最后，我们还可以观察到，对于某些中间值的 &lt;code&gt;gamma&lt;/code&gt; 当 &lt;code&gt;C&lt;/code&gt; 变得非常大时，我们得到的模型性能均相同：不必通过强制执行更大的余量来进行正则化。 RBF内核的半径本身就可以充当良好的结构调整器。实际上，用较低的 &lt;code&gt;C&lt;/code&gt; 值简化决策函数以支持使用较少内存且预测速度更快的模型可能仍然很有趣。</target>
        </trans-unit>
        <trans-unit id="023b5a681b65ffd6304d113bea1d054693e2974b" translate="yes" xml:space="preserve">
          <source>Finally one should highlight that the Compound Poisson Gamma model that is directly fit on the pure premium is operationally simpler to develop and maintain as it consists in a single scikit-learn estimator instead of a pair of models, each with its own set of hyperparameters.</source>
          <target state="translated">最后需要强调的是,在纯溢价上直接拟合的复合泊松伽马模型在操作上更容易开发和维护,因为它由一个scikit-learn估计器组成,而不是一对模型,每个模型都有自己的超参数集。</target>
        </trans-unit>
        <trans-unit id="132cf9317ef371444593d6b5ea568f7dd88cd93f" translate="yes" xml:space="preserve">
          <source>Finally we are going to visualize the score:</source>
          <target state="translated">最后我们要把分数可视化。</target>
        </trans-unit>
        <trans-unit id="7c7dc3c27466a01b4ea9d811b8dcf76d3f27d658" translate="yes" xml:space="preserve">
          <source>Finally we will plot the selected two features from the data.</source>
          <target state="translated">最后我们将从数据中选取两个特征进行绘制。</target>
        </trans-unit>
        <trans-unit id="968616cc558da3fce60b0fa70563b382cf880868" translate="yes" xml:space="preserve">
          <source>Finally, &lt;a href=&quot;#dummy-estimators&quot;&gt;Dummy estimators&lt;/a&gt; are useful to get a baseline value of those metrics for random predictions.</source>
          <target state="translated">最后，&lt;a href=&quot;#dummy-estimators&quot;&gt;虚拟估计器&lt;/a&gt;对于获取这些度量的基准值以进行随机预测很有用。</target>
        </trans-unit>
        <trans-unit id="f6d5bddccec3511395edb0437e03e1a9559b2146" translate="yes" xml:space="preserve">
          <source>Finally, as we will see next, computing partial dependence plots tree-based models is also orders of magnitude faster making it cheap to compute partial dependence plots for pairs of interacting features:</source>
          <target state="translated">最后,正如我们接下来将看到的那样,计算基于树状模型的部分依赖图也快了几个数量级,使得计算交互特征对的部分依赖图很便宜。</target>
        </trans-unit>
        <trans-unit id="bac8fe99c0a1d10b0495b7da851a20ddd76d68ac" translate="yes" xml:space="preserve">
          <source>Finally, for 3. we have a number of options inside scikit-learn. Although not all algorithms can learn incrementally (i.e. without seeing all the instances at once), all estimators implementing the &lt;code&gt;partial_fit&lt;/code&gt; API are candidates. Actually, the ability to learn incrementally from a mini-batch of instances (sometimes called &amp;ldquo;online learning&amp;rdquo;) is key to out-of-core learning as it guarantees that at any given time there will be only a small amount of instances in the main memory. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve some tuning &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;.</source>
          <target state="translated">最后，对于3.，我们在scikit-learn中有许多选项。尽管并非所有算法都可以增量学习（即，不能一次看到所有实例），但是所有实现 &lt;code&gt;partial_fit&lt;/code&gt; API的估计器都是候选者。实际上，从一小批实例进行增量学习的能力（有时称为&amp;ldquo;在线学习&amp;rdquo;）是核心学习的关键，因为它可以确保在任何给定的时间中仅存在少量实例。主内存。为平衡相关性和内存占用的微型批处理选择一个合适的大小可能需要一些调整&lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="8f153367033a26a7113472eddd66aa5516b2f784" translate="yes" xml:space="preserve">
          <source>Finally, for 3. we have a number of options inside scikit-learn. Although not all algorithms can learn incrementally (i.e. without seeing all the instances at once), all estimators implementing the &lt;code&gt;partial_fit&lt;/code&gt; API are candidates. Actually, the ability to learn incrementally from a mini-batch of instances (sometimes called &amp;ldquo;online learning&amp;rdquo;) is key to out-of-core learning as it guarantees that at any given time there will be only a small amount of instances in the main memory. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve some tuning &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">最后，对于3.，我们在scikit-learn中有许多选项。尽管并非所有算法都可以增量学习（即，不能一次看到所有实例），但是所有实现 &lt;code&gt;partial_fit&lt;/code&gt; API的估计器都是候选者。实际上，从一小批实例进行增量学习的能力（有时称为&amp;ldquo;在线学习&amp;rdquo;）是核心学习的关键，因为它可以确保在任何给定的时间中仅存在少量实例。主内存。为平衡相关性和内存占用的微型批处理选择一个合适的大小可能需要一些调整&lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="2ea772d9a2706e63fd635b713dfc5a265ad9f7ca" translate="yes" xml:space="preserve">
          <source>Finally, for the last data set, it is hard to say that one sample is more abnormal than another sample as they are uniformly distributed in a hypercube. Except for the &lt;a href=&quot;../../modules/generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; which overfits a little, all estimators present decent solutions for this situation. In such a case, it would be wise to look more closely at the scores of abnormality of the samples as a good estimator should assign similar scores to all the samples.</source>
          <target state="translated">最后，对于最后一个数据集，很难说一个样本比另一个样本异常得多，因为它们均匀分布在超立方体中。除了sklearn.svm.OneClassSVM有点过&lt;a href=&quot;../../modules/generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;sklearn.svm.OneClassSVM&lt;/code&gt; &lt;/a&gt;之外，所有估计器都针对这种情况提供了不错的解决方案。在这种情况下，明智的做法是仔细观察样本的异常分数，因为好的估算者应该为所有样本分配相似的分数。</target>
        </trans-unit>
        <trans-unit id="02251ecbb666e2a1b9f431635bb03c5b54eb0a09" translate="yes" xml:space="preserve">
          <source>Finally, for the last data set, it is hard to say that one sample is more abnormal than another sample as they are uniformly distributed in a hypercube. Except for the &lt;code&gt;svm.OneClassSVM&lt;/code&gt; which overfits a little, all estimators present decent solutions for this situation. In such a case, it would be wise to look more closely at the scores of abnormality of the samples as a good estimator should assign similar scores to all the samples.</source>
          <target state="translated">最后，对于最后一个数据集，很难说一个样本比另一个样本异常得多，因为它们均匀分布在超立方体中。除了svm.OneClassSVM有点过 &lt;code&gt;svm.OneClassSVM&lt;/code&gt; 之外，所有估计器都针对这种情况提供了不错的解决方案。在这种情况下，明智的做法是仔细观察样本的异常分数，因为好的估算者应该为所有样本分配相似的分数。</target>
        </trans-unit>
        <trans-unit id="ced44a1e1c24bc2a905bacd41c580fdb8b398c7b" translate="yes" xml:space="preserve">
          <source>Finally, if the centered data is expected to be small enough, explicitly converting the input to an array using the &lt;code&gt;toarray&lt;/code&gt; method of sparse matrices is another option.</source>
          <target state="translated">最后，如果期望居中的数据足够小，则使用稀疏矩阵的 &lt;code&gt;toarray&lt;/code&gt; 方法将输入显式转换为数组是另一种选择。</target>
        </trans-unit>
        <trans-unit id="3e454243da6a98ce545408a4ef381fa38d1b1d45" translate="yes" xml:space="preserve">
          <source>Finally, many parts of the implementation of &lt;a href=&quot;generated/sklearn.ensemble.histgradientboostingclassifier#sklearn.ensemble.HistGradientBoostingClassifier&quot;&gt;&lt;code&gt;HistGradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt;&lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; are parallelized.</source>
          <target state="translated">最后，将&lt;a href=&quot;generated/sklearn.ensemble.histgradientboostingclassifier#sklearn.ensemble.HistGradientBoostingClassifier&quot;&gt; &lt;code&gt;HistGradientBoostingClassifier&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt; &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;的实现的许多部分并行化。</target>
        </trans-unit>
        <trans-unit id="9b957bcce911d726f82907b30c1a02070fcadf91" translate="yes" xml:space="preserve">
          <source>Finally, note that parameters of the models have been here handpicked but that in practice they need to be adjusted. In the absence of labelled data, the problem is completely unsupervised so model selection can be a challenge.</source>
          <target state="translated">最后要注意的是,这里的模型的参数是经过精心挑选的,但在实际操作中需要调整。在没有标注数据的情况下,这个问题是完全无监督的,所以模型的选择是一个挑战。</target>
        </trans-unit>
        <trans-unit id="657149d8f47b73d795165e7f97ca51bcb04565c9" translate="yes" xml:space="preserve">
          <source>Finally, the precomputation can be performed by custom estimators to use different implementations, such as approximate nearest neighbors methods, or implementation with special data types. The precomputed neighbors &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;sparse graph&lt;/a&gt; needs to be formatted as in &lt;a href=&quot;generated/sklearn.neighbors.radius_neighbors_graph#sklearn.neighbors.radius_neighbors_graph&quot;&gt;&lt;code&gt;radius_neighbors_graph&lt;/code&gt;&lt;/a&gt; output:</source>
          <target state="translated">最后，预计算可以由自定义估算器执行，以使用不同的实现方式，例如近似最近邻方法或具有特殊数据类型的实现方式。预计算的邻居&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;稀疏图&lt;/a&gt;需要按照&lt;a href=&quot;generated/sklearn.neighbors.radius_neighbors_graph#sklearn.neighbors.radius_neighbors_graph&quot;&gt; &lt;code&gt;radius_neighbors_graph&lt;/code&gt; &lt;/a&gt;输出的格式进行格式化：</target>
        </trans-unit>
        <trans-unit id="5d0d4410d99fc712aa6cde836179153d36f8849c" translate="yes" xml:space="preserve">
          <source>Finally, the preprocessing pipeline is integrated in a full prediction pipeline using &lt;a href=&quot;../../modules/generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;, together with a simple classification model.</source>
          <target state="translated">最后，使用&lt;a href=&quot;../../modules/generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt;和简单的分类模型将预处理管道集成到完整的预测管道中。</target>
        </trans-unit>
        <trans-unit id="88dd3e3888504f660d94f91abf7147882c40ab2a" translate="yes" xml:space="preserve">
          <source>Finally, this module also features the parallel construction of the trees and the parallel computation of the predictions through the &lt;code&gt;n_jobs&lt;/code&gt; parameter. If &lt;code&gt;n_jobs=k&lt;/code&gt; then computations are partitioned into &lt;code&gt;k&lt;/code&gt; jobs, and run on &lt;code&gt;k&lt;/code&gt; cores of the machine. If &lt;code&gt;n_jobs=-1&lt;/code&gt; then all cores available on the machine are used. Note that because of inter-process communication overhead, the speedup might not be linear (i.e., using &lt;code&gt;k&lt;/code&gt; jobs will unfortunately not be &lt;code&gt;k&lt;/code&gt; times as fast). Significant speedup can still be achieved though when building a large number of trees, or when building a single tree requires a fair amount of time (e.g., on large datasets).</source>
          <target state="translated">最后，该模块还具有树的并行构造和通过 &lt;code&gt;n_jobs&lt;/code&gt; 参数的预测并行计算的功能。如果 &lt;code&gt;n_jobs=k&lt;/code&gt; ,则将计算划分为 &lt;code&gt;k&lt;/code&gt; 个作业，并在计算机的 &lt;code&gt;k&lt;/code&gt; 个内核上运行。如果 &lt;code&gt;n_jobs=-1&lt;/code&gt; ,则使用计算机上所有可用的内核。请注意，由于进程间的通信开销，加速可能不是线性的（即，不幸的是，使用 &lt;code&gt;k&lt;/code&gt; 个作业将不会是 &lt;code&gt;k&lt;/code&gt; 倍的速度）。尽管构建大量树或构建一棵树需要相当长的时间（例如，在大型数据集上），仍然可以实现显着的加速。</target>
        </trans-unit>
        <trans-unit id="ac4279287fdb399e92fa8f401fc1a571b7df3622" translate="yes" xml:space="preserve">
          <source>Finally, we can compare the two models using a plot of cumulated claims: for each model, the policyholders are ranked from safest to riskiest and the fraction of observed total cumulated claims is plotted on the y axis. This plot is often called the ordered Lorenz curve of the model.</source>
          <target state="translated">最后,我们可以使用累计索赔图来比较两个模型:对于每个模型,将保单持有人从最安全到最危险进行排序,并在y轴上绘制观察到的累计索赔总额的分数。这个图通常被称为模型的有序洛伦兹曲线。</target>
        </trans-unit>
        <trans-unit id="abb5440e13032f9c21233aa604593a79c839b358" translate="yes" xml:space="preserve">
          <source>Finally, we fit our pipeline on the training data and use it to predict topics for &lt;code&gt;X_test&lt;/code&gt;. Performance metrics of our pipeline are then printed.</source>
          <target state="translated">最后，我们根据训练数据调整流水线，并使用它来预测 &lt;code&gt;X_test&lt;/code&gt; 的主题。然后打印我们管道的性能指标。</target>
        </trans-unit>
        <trans-unit id="f50b898f991b3ec8dadce88abad6749cb3fd8140" translate="yes" xml:space="preserve">
          <source>Finally, we have a full-fledged example of &lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core classification of text documents&lt;/a&gt;. It is aimed at providing a starting point for people wanting to build out-of-core learning systems and demonstrates most of the notions discussed above.</source>
          <target state="translated">最后，我们有一个完整&lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;的文本文档核心分类&lt;/a&gt;示例。它旨在为希望构建核心学习系统的人们提供一个起点，并演示了上面讨论的大多数概念。</target>
        </trans-unit>
        <trans-unit id="67b1b42b3e66107ae3b505ef41aac41ee3327ff3" translate="yes" xml:space="preserve">
          <source>Finally, we will consider a non-linear model, namely Gradient Boosting Regression Trees. Tree-based models do not require the categorical data to be one-hot encoded: instead, we can encode each category label with an arbitrary integer using &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt;&lt;code&gt;OrdinalEncoder&lt;/code&gt;&lt;/a&gt;. With this encoding, the trees will treat the categorical features as ordered features, which might not be always a desired behavior. However this effect is limited for deep enough trees which are able to recover the categorical nature of the features. The main advantage of the &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt;&lt;code&gt;OrdinalEncoder&lt;/code&gt;&lt;/a&gt; over the &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;OneHotEncoder&lt;/code&gt;&lt;/a&gt; is that it will make training faster.</source>
          <target state="translated">最后，我们将考虑一个非线性模型，即梯度增强回归树。基于树的模型不需要对分类数据进行一次热编码：相反，我们可以使用&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt; &lt;code&gt;OrdinalEncoder&lt;/code&gt; &lt;/a&gt;使用任意整数对每个分类标签进行编码。通过这种编码，树将分类特征视为有序特征，这可能并不总是所希望的行为。但是，这种效果仅限于足够深的树，这些树能够恢复特征的分类性质。的主要优势&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt; &lt;code&gt;OrdinalEncoder&lt;/code&gt; &lt;/a&gt;在&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt; &lt;code&gt;OneHotEncoder&lt;/code&gt; &lt;/a&gt;是，它会使培训更快。</target>
        </trans-unit>
        <trans-unit id="15e943be721eab8a2c2612f45c392677826a2d6c" translate="yes" xml:space="preserve">
          <source>Finally, we will plot the predictions made by all models for comparison.</source>
          <target state="translated">最后,我们将绘制所有模型的预测结果进行比较。</target>
        </trans-unit>
        <trans-unit id="e9d6cf373d4d749e0936e6f64e1c533e8fd8ee41" translate="yes" xml:space="preserve">
          <source>Finally, we will visualize the 20 predictions. The red stars show the average prediction made by &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt;&lt;code&gt;VotingRegressor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">最后，我们将可视化20个预测。红星显示的是&lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt; &lt;code&gt;VotingRegressor&lt;/code&gt; &lt;/a&gt;做出的平均预测。</target>
        </trans-unit>
        <trans-unit id="9f642cdc0abe545be7ddd1dd012ea6198c659512" translate="yes" xml:space="preserve">
          <source>Finally, we will visualize the results. To do that we will first compute the test set deviance and then plot it against boosting iterations.</source>
          <target state="translated">最后,我们将把结果可视化。要做到这一点,我们将首先计算测试集偏差,然后绘制它与提升迭代的关系。</target>
        </trans-unit>
        <trans-unit id="a40631d3e46670650e2553ca58294d12cd46b8ee" translate="yes" xml:space="preserve">
          <source>Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches &lt;a href=&quot;#lg2012&quot; id=&quot;id4&quot;&gt;[LG2012]&lt;/a&gt;.</source>
          <target state="translated">最后，当基于样本和特征的子集建立基本估计量时，该方法称为随机补丁&lt;a href=&quot;#lg2012&quot; id=&quot;id4&quot;&gt;[LG2012]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="7185a548932a7d63b1a08d591559f9d8821b9404" translate="yes" xml:space="preserve">
          <source>Find a &amp;lsquo;safe&amp;rsquo; number of components to randomly project to</source>
          <target state="translated">查找&amp;ldquo;安全&amp;rdquo;数量的组件以随机投影到</target>
        </trans-unit>
        <trans-unit id="e40e118fb652d4dd06f307746b620728b2345e85" translate="yes" xml:space="preserve">
          <source>Find a good set of parameters using grid search.</source>
          <target state="translated">利用网格搜索找到一组好的参数。</target>
        </trans-unit>
        <trans-unit id="022a840c39dad4bac6c36ba2a156531a2e97ca25" translate="yes" xml:space="preserve">
          <source>Find importance of the features</source>
          <target state="translated">找出特征的重要性</target>
        </trans-unit>
        <trans-unit id="2ff57da6a2f73ea1d0dfc969be6516b83c61b137" translate="yes" xml:space="preserve">
          <source>Find out what the actual encoding of the text is. The file might come with a header or README that tells you the encoding, or there might be some standard encoding you can assume based on where the text comes from.</source>
          <target state="translated">找出文本的实际编码是什么。文件可能带有一个头或README,告诉你编码,或者可能有一些标准编码,你可以根据文本的来源来假设。</target>
        </trans-unit>
        <trans-unit id="c317277c718283dc0dcc7baf2d812226e24cc453" translate="yes" xml:space="preserve">
          <source>Find the minimum value of an array over positive values</source>
          <target state="translated">求一个数组在正值上的最小值。</target>
        </trans-unit>
        <trans-unit id="d95b4144f33d10b131b37a33f2e7ad7cc55860b7" translate="yes" xml:space="preserve">
          <source>Find the optimal separating hyperplane using an SVC for classes that are unbalanced.</source>
          <target state="translated">对于不平衡的类,使用SVC寻找最优分离超平面。</target>
        </trans-unit>
        <trans-unit id="1c17584736b51c43c88ae3dfa46ea2817cd1a339" translate="yes" xml:space="preserve">
          <source>Find two non-negative matrices (W, H) whose product approximates the non- negative matrix X. This factorization can be used for example for dimensionality reduction, source separation or topic extraction.</source>
          <target state="translated">找出两个非负矩阵(W,H),其乘积近似于非负矩阵X。这种因式化可用于维度减少、源分离或主题提取等。</target>
        </trans-unit>
        <trans-unit id="4daca12ed4dca9ca21325e05dde77793902b89f7" translate="yes" xml:space="preserve">
          <source>Finding a reasonable regularization parameter \(\alpha\) is best done using &lt;code&gt;GridSearchCV&lt;/code&gt;, usually in the range &lt;code&gt;10.0 ** -np.arange(1, 7)&lt;/code&gt;.</source>
          <target state="translated">使用 &lt;code&gt;GridSearchCV&lt;/code&gt; 最好找到合理的正则化参数\（\ alpha \），通常在 &lt;code&gt;10.0 ** -np.arange(1, 7)&lt;/code&gt; 范围内。</target>
        </trans-unit>
        <trans-unit id="d0410c73baea9b092d4edb9802ae4c10c09f4696" translate="yes" xml:space="preserve">
          <source>Finding a reasonable regularization term \(\alpha\) is best done using &lt;code&gt;GridSearchCV&lt;/code&gt;, usually in the range &lt;code&gt;10.0**-np.arange(1,7)&lt;/code&gt;.</source>
          <target state="translated">使用 &lt;code&gt;GridSearchCV&lt;/code&gt; 最好找到合理的正则化项\（\ alpha \），通常在 &lt;code&gt;10.0**-np.arange(1,7)&lt;/code&gt; 范围内。</target>
        </trans-unit>
        <trans-unit id="37e657d78a9dafeda1b142a17aaad5659fab59b7" translate="yes" xml:space="preserve">
          <source>Finding a reasonable regularization term \(\alpha\) is best done using automatic hyper-parameter search, e.g. &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt;&lt;code&gt;RandomizedSearchCV&lt;/code&gt;&lt;/a&gt;, usually in the range &lt;code&gt;10.0**-np.arange(1,7)&lt;/code&gt;.</source>
          <target state="translated">最好使用自动超参数搜索（例如&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt; &lt;code&gt;RandomizedSearchCV&lt;/code&gt; &lt;/a&gt;）找到合理的正则项\（\ alpha \），通常在 &lt;code&gt;10.0**-np.arange(1,7)&lt;/code&gt; 范围内。</target>
        </trans-unit>
        <trans-unit id="3c5a640c918941c9e6a6ecbfe984c3f4bfbed0d7" translate="yes" xml:space="preserve">
          <source>Finding help</source>
          <target state="translated">寻找帮助</target>
        </trans-unit>
        <trans-unit id="71cc727880d53ab3c238ec955595a0ded28610b7" translate="yes" xml:space="preserve">
          <source>Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) http://arxiv.org/pdf/0909.4061</source>
          <target state="translated">寻找随机性的结构。用于构建近似矩阵分解的随机算法 Halko等人,2009年(arXiv:909)http://arxiv.org/pdf/0909.4061</target>
        </trans-unit>
        <trans-unit id="de5cdfe8ddd63c8c4fb1f7f388d421a16cbbf828" translate="yes" xml:space="preserve">
          <source>Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf</source>
          <target state="translated">寻找随机性的结构。用于构建近似矩阵分解的随机算法 Halko等人,2009年(arXiv:909)https://arxiv.org/pdf/0909.4061.pdf</target>
        </trans-unit>
        <trans-unit id="50ccb6d4c8a3bfa0951293912c0c5132106a8c89" translate="yes" xml:space="preserve">
          <source>Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 &lt;a href=&quot;http://arxiv.org/abs/arXiv:0909.4061&quot;&gt;http://arxiv.org/abs/arXiv:0909.4061&lt;/a&gt;</source>
          <target state="translated">寻找具有随机性的结构：用于构造近似矩阵分解的随机算法Halko等，2009年&lt;a href=&quot;http://arxiv.org/abs/arXiv:0909.4061&quot;&gt;http://arxiv.org/abs/arXiv:0909.4061&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="09110546ddb637f3471f1e7efbbe318b3963484e" translate="yes" xml:space="preserve">
          <source>Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 &lt;a href=&quot;https://arxiv.org/abs/0909.4061&quot;&gt;https://arxiv.org/abs/0909.4061&lt;/a&gt;</source>
          <target state="translated">寻找具有随机性的结构：用于构造近似矩阵分解的随机算法Halko等，2009 &lt;a href=&quot;https://arxiv.org/abs/0909.4061&quot;&gt;https://arxiv.org/abs/0909.4061&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="26becafaa69af10fe9097be8fd6bf298839f5095" translate="yes" xml:space="preserve">
          <source>Finds a dictionary (a set of atoms) that can best be used to represent data using a sparse code.</source>
          <target state="translated">找到一个字典(一组原子),可以最好地使用稀疏代码来表示数据。</target>
        </trans-unit>
        <trans-unit id="bdd5a22a4b66cac66c670f6f1297e40cb6d2aae2" translate="yes" xml:space="preserve">
          <source>Finds a sparse representation of data against a fixed, precomputed dictionary.</source>
          <target state="translated">对照一个固定的、预先计算的字典,找到数据的稀疏表示。</target>
        </trans-unit>
        <trans-unit id="a5dbd88babc750758342dfe447e24f6c260c3097" translate="yes" xml:space="preserve">
          <source>Finds core samples of high density and expands clusters from them.</source>
          <target state="translated">找到高密度的核心样本,并从中扩展出集群。</target>
        </trans-unit>
        <trans-unit id="e0fb0de42fddb601e63b5dedf2b446e9b6fdd66e" translate="yes" xml:space="preserve">
          <source>Finds core samples of high density and expands clusters from them. This example uses data that is generated so that the clusters have different densities. The &lt;a href=&quot;../../modules/generated/sklearn.cluster.optics#sklearn.cluster.OPTICS&quot;&gt;&lt;code&gt;sklearn.cluster.OPTICS&lt;/code&gt;&lt;/a&gt; is first used with its Xi cluster detection method, and then setting specific thresholds on the reachability, which corresponds to &lt;a href=&quot;../../modules/generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;sklearn.cluster.DBSCAN&lt;/code&gt;&lt;/a&gt;. We can see that the different clusters of OPTICS&amp;rsquo;s Xi method can be recovered with different choices of thresholds in DBSCAN.</source>
          <target state="translated">查找高密度的核心样本并从中扩展聚类。本示例使用生成的数据，使群集具有不同的密度。所述&lt;a href=&quot;../../modules/generated/sklearn.cluster.optics#sklearn.cluster.OPTICS&quot;&gt; &lt;code&gt;sklearn.cluster.OPTICS&lt;/code&gt; &lt;/a&gt;首先用其僖群集检测方法中使用，然后在可达性，其对应于设置特定阈值&lt;a href=&quot;../../modules/generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt; &lt;code&gt;sklearn.cluster.DBSCAN&lt;/code&gt; &lt;/a&gt;。我们可以看到，通过DBSCAN中不同的阈值选择，可以恢复OPTICS Xi方法的不同集群。</target>
        </trans-unit>
        <trans-unit id="a0ac333cc4bd6a4e85a5691bb991f9cd4f114c12" translate="yes" xml:space="preserve">
          <source>Finds the K-neighbors of a point.</source>
          <target state="translated">找到一个点的K邻点。</target>
        </trans-unit>
        <trans-unit id="e7def6740e556221f1aa42003fab4e2365f3483e" translate="yes" xml:space="preserve">
          <source>Finds the K-neighbors of a point. Returns indices of and distances to the neighbors of each point.</source>
          <target state="translated">查找一个点的K-邻居。返回每个点的邻域指数和距离。</target>
        </trans-unit>
        <trans-unit id="011362db8e96e3edcb95f8fcdf5f1e0d92d5e3e2" translate="yes" xml:space="preserve">
          <source>Finds the best dictionary and the corresponding sparse code for approximating the data matrix X by solving:</source>
          <target state="translated">通过求解数据矩阵X的近似,找出最佳字典和相应的稀疏代码。</target>
        </trans-unit>
        <trans-unit id="b6f70d43205a979d509f018cfbe88e93d812b3ec" translate="yes" xml:space="preserve">
          <source>Finds the neighbors within a given radius of a point or points.</source>
          <target state="translated">在给定半径内找到一个或多个点的邻居。</target>
        </trans-unit>
        <trans-unit id="dd8ddfd214a6d88017dd4006d52b1774d72e0be7" translate="yes" xml:space="preserve">
          <source>Finds the set of sparse components that can optimally reconstruct the data. The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter alpha.</source>
          <target state="translated">找到能够最佳重建数据的稀疏成分集。稀疏度的大小由L1惩罚的系数控制,由参数alpha给出。</target>
        </trans-unit>
        <trans-unit id="9ce397e0f2c9f33bd1911b665e99384e26321100" translate="yes" xml:space="preserve">
          <source>Finite Gaussian mixture fit with EM.</source>
          <target state="translated">用EM进行有限高斯混合物拟合。</target>
        </trans-unit>
        <trans-unit id="fd9ae1ff1bfcac14b05b92d277d3f6074f078a31" translate="yes" xml:space="preserve">
          <source>First 10 columns are numeric predictive values</source>
          <target state="translated">前10列是数字预测值</target>
        </trans-unit>
        <trans-unit id="30250bc7520fcdac140dbcf0c3820bc484ca88d7" translate="yes" xml:space="preserve">
          <source>First example</source>
          <target state="translated">第一个例子</target>
        </trans-unit>
        <trans-unit id="7874170ffefbed5d0d1c4372827ea2aeb194c3ba" translate="yes" xml:space="preserve">
          <source>First fit an ensemble of trees (totally random trees, a random forest, or gradient boosted trees) on the training set. Then each leaf of each tree in the ensemble is assigned a fixed arbitrary feature index in a new feature space. These leaf indices are then encoded in a one-hot fashion.</source>
          <target state="translated">首先在训练集上拟合一个树的合集(完全随机树、随机森林或梯度提升树)。然后在新的特征空间中,给合集中每棵树的每片叶子分配一个固定的任意特征指数。然后将这些叶子指数以一热的方式进行编码。</target>
        </trans-unit>
        <trans-unit id="c9c47ba90ccfdfceca4eb5b8199939d35b815802" translate="yes" xml:space="preserve">
          <source>First note that the K means \(\mu_k\) are vectors in \(\mathcal{R}^d\), and they lie in an affine subspace \(H\) of dimension at least \(K - 1\) (2 points lie on a line, 3 points lie on a plane, etc).</source>
          <target state="translated">首先要注意,K的意思是说,(mathcal{R}^d/)中的向量,它们位于一个至少维度为(K-1)的亲和子空间(H)中(2点位于一条线上,3点位于一个平面上,等等)。</target>
        </trans-unit>
        <trans-unit id="5b7d36dc535373f63b2b8c97af1496035ebdccf8" translate="yes" xml:space="preserve">
          <source>First of all, we can take a look to the values of the coefficients of the regressor we have fitted.</source>
          <target state="translated">首先,我们可以看一下我们所拟合的回归者的系数值。</target>
        </trans-unit>
        <trans-unit id="af1667b67f8a74831d9cbccc10fec01c4bb8c75d" translate="yes" xml:space="preserve">
          <source>First we check which value of \(\alpha\) has been selected.</source>
          <target state="translated">首先,我们检查一下选择了哪个值。</target>
        </trans-unit>
        <trans-unit id="31c524afbf18465a1ae6f80767e0e696d536b875" translate="yes" xml:space="preserve">
          <source>First we create a data set of 9 samples from 3 classes, and plot the points in the original space. For this example, we focus on the classification of point no. 3. The thickness of a link between point no. 3 and another point is proportional to their distance.</source>
          <target state="translated">首先我们建立一个数据集,包含3个类的9个样本,并在原始空间中绘制点。在本例中,我们重点研究3号点的分类。3号点与另一个点之间的链接厚度与它们的距离成正比。</target>
        </trans-unit>
        <trans-unit id="66d3e97d3998fff9804546e983728130f9a1d48d" translate="yes" xml:space="preserve">
          <source>First we download the two datasets. Diabetes dataset is shipped with scikit-learn. It has 442 entries, each with 10 features. California Housing dataset is much larger with 20640 entries and 8 features. It needs to be downloaded. We will only use the first 400 entries for the sake of speeding up the calculations but feel free to use the whole dataset.</source>
          <target state="translated">首先我们下载两个数据集。糖尿病数据集是随scikit-learn一起提供的。它有442个条目,每个条目有10个特征。加利福尼亚住房数据集更大,有20640个条目和8个特征。它需要被下载。为了加快计算速度,我们将只使用前400个条目,但请随意使用整个数据集。</target>
        </trans-unit>
        <trans-unit id="c2e4b6f9c83c90558d9df87e3572e1d8ff690d21" translate="yes" xml:space="preserve">
          <source>First we need to load the data.</source>
          <target state="translated">首先我们需要加载数据。</target>
        </trans-unit>
        <trans-unit id="8d180e35a78f80e45c6959bdc2213230723a6ec9" translate="yes" xml:space="preserve">
          <source>First we verify which value of \(\alpha\) has been selected.</source>
          <target state="translated">首先,我们要验证一下选择了哪个数值。</target>
        </trans-unit>
        <trans-unit id="da6f0be97a3f9bc23cb9968f6dd764558635185b" translate="yes" xml:space="preserve">
          <source>First, let&amp;rsquo;s get some insights by looking at the variable distributions and at the pairwise relationships between them. Only numerical variables will be used. In the following plot, each dot represents a sample.</source>
          <target state="translated">首先，让我们通过查看变量分布及其之间的成对关系来获得一些见解。仅使用数字变量。在下图中，每个点代表一个样本。</target>
        </trans-unit>
        <trans-unit id="1c794cffa5e7e2eb61589bb3349c992e76e5c222" translate="yes" xml:space="preserve">
          <source>First, let&amp;rsquo;s load the diabetes dataset which is available from within sklearn. Then, we will look what features are collected for the diabates patients:</source>
          <target state="translated">首先，让我们加载可从sklearn中获得的糖尿病数据集。然后，我们将看看为糖尿病患者收集了哪些功能：</target>
        </trans-unit>
        <trans-unit id="cac3b241d42030e20b4b1109edf6cf6d51db7345" translate="yes" xml:space="preserve">
          <source>First, the precomputed graph can be re-used multiple times, for instance while varying a parameter of the estimator. This can be done manually by the user, or using the caching properties of the scikit-learn pipeline:</source>
          <target state="translated">首先,预计算的图可以被多次重复使用,例如在改变估计器的参数时。这可以由用户手动完成,或者使用scikit-learn管道的缓存属性。</target>
        </trans-unit>
        <trans-unit id="97be8aa0250f0cdac7301eb79bf3b2a478185ccc" translate="yes" xml:space="preserve">
          <source>First, three examplary classifiers are initialized (&lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt;&lt;code&gt;GaussianNB&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt;) and used to initialize a soft-voting &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; with weights &lt;code&gt;[1, 1, 5]&lt;/code&gt;, which means that the predicted probabilities of the &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt; count 5 times as much as the weights of the other classifiers when the averaged probability is calculated.</source>
          <target state="translated">首先，将初始化三个示例分类器（&lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;../../modules/generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt; &lt;code&gt;GaussianNB&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt; &lt;code&gt;RandomForestClassifier&lt;/code&gt; &lt;/a&gt;），并使用权重 &lt;code&gt;[1, 1, 5]&lt;/code&gt; &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt; &lt;code&gt;VotingClassifier&lt;/code&gt; &lt;/a&gt; ]初始化软投票VotingClassifier，这意味着&lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt; &lt;code&gt;RandomForestClassifier&lt;/code&gt; &lt;/a&gt;的预测概率计数为权重的5倍。计算平均概率时其他分类器的权重。</target>
        </trans-unit>
        <trans-unit id="16a8cd51addf2278ba02a88d4c3026fabfe281d9" translate="yes" xml:space="preserve">
          <source>First, three examplary classifiers are initialized (&lt;code&gt;LogisticRegression&lt;/code&gt;, &lt;code&gt;GaussianNB&lt;/code&gt;, and &lt;code&gt;RandomForestClassifier&lt;/code&gt;) and used to initialize a soft-voting &lt;code&gt;VotingClassifier&lt;/code&gt; with weights &lt;code&gt;[1, 1, 5]&lt;/code&gt;, which means that the predicted probabilities of the &lt;code&gt;RandomForestClassifier&lt;/code&gt; count 5 times as much as the weights of the other classifiers when the averaged probability is calculated.</source>
          <target state="translated">首先，将初始化三个示例分类器（ &lt;code&gt;LogisticRegression&lt;/code&gt; ， &lt;code&gt;GaussianNB&lt;/code&gt; 和 &lt;code&gt;RandomForestClassifier&lt;/code&gt; ），并使用权重 &lt;code&gt;[1, 1, 5]&lt;/code&gt; &lt;code&gt;VotingClassifier&lt;/code&gt; ]初始化软投票VotingClassifier，这意味着 &lt;code&gt;RandomForestClassifier&lt;/code&gt; 的预测概率计算为权重的 5倍。计算平均概率时其他分类器的权重。</target>
        </trans-unit>
        <trans-unit id="f50c60bee958db3de20cd6c08f2c3fe9b2fb6e88" translate="yes" xml:space="preserve">
          <source>First, three exemplary classifiers are initialized (&lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt;&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;) and used to initialize a soft-voting &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; with weights &lt;code&gt;[2,
1, 2]&lt;/code&gt;, which means that the predicted probabilities of the &lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt;&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; each count 2 times as much as the weights of the &lt;a href=&quot;../../modules/generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt; classifier when the averaged probability is calculated.</source>
          <target state="translated">首先，初始化三个示例性分类器（&lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt; &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;../../modules/generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt; &lt;code&gt;KNeighborsClassifier&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt;）并用于初始化权重为 &lt;code&gt;[2, 1, 2]&lt;/code&gt; &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt; &lt;code&gt;VotingClassifier&lt;/code&gt; &lt;/a&gt; ]的软投票VotingClassifier，这意味着&lt;a href=&quot;../../modules/generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt; &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt;的预测概率分别为2倍。与计算平均概率时的&lt;a href=&quot;../../modules/generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt; &lt;code&gt;KNeighborsClassifier&lt;/code&gt; &lt;/a&gt;分类器的权重相同。</target>
        </trans-unit>
        <trans-unit id="1ea180eeb1f820255090eaa2e746ff697919b622" translate="yes" xml:space="preserve">
          <source>First, three exemplary classifiers are initialized (&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;, &lt;code&gt;KNeighborsClassifier&lt;/code&gt;, and &lt;code&gt;SVC&lt;/code&gt;) and used to initialize a soft-voting &lt;code&gt;VotingClassifier&lt;/code&gt; with weights &lt;code&gt;[2, 1, 2]&lt;/code&gt;, which means that the predicted probabilities of the &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; and &lt;code&gt;SVC&lt;/code&gt; count 5 times as much as the weights of the &lt;code&gt;KNeighborsClassifier&lt;/code&gt; classifier when the averaged probability is calculated.</source>
          <target state="translated">首先，初始化三个示例性分类器（ &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; ， &lt;code&gt;KNeighborsClassifier&lt;/code&gt; 和 &lt;code&gt;SVC&lt;/code&gt; ）并用于初始化权重为 &lt;code&gt;[2, 1, 2]&lt;/code&gt; &lt;code&gt;VotingClassifier&lt;/code&gt; ]的软投票VotingClassifier，这意味着 &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; 和 &lt;code&gt;SVC&lt;/code&gt; 的预测概率计数为5倍作为计算平均概率时的 &lt;code&gt;KNeighborsClassifier&lt;/code&gt; 分类器的权重。</target>
        </trans-unit>
        <trans-unit id="5fdc5502e4f55e2ed52afffb452568aea16c1218" translate="yes" xml:space="preserve">
          <source>First, we fit the model.</source>
          <target state="translated">首先,我们对模型进行拟合。</target>
        </trans-unit>
        <trans-unit id="ef9c4b59f96926f6f0e5f166376f8408083d95db" translate="yes" xml:space="preserve">
          <source>First, we load the wine dataset and convert it to a binary classification problem. Then, we train a support vector classifier on a training dataset.</source>
          <target state="translated">首先,我们加载葡萄酒数据集并将其转换为二进制分类问题。然后,我们在训练数据集上训练一个支持向量分类器。</target>
        </trans-unit>
        <trans-unit id="9d07cc681f72e7314cfb570248652c85d831875b" translate="yes" xml:space="preserve">
          <source>First, we must understand the structure of our data. It has 100 randomly generated input datapoints, 3 classes split unevenly across datapoints, and 10 &amp;ldquo;groups&amp;rdquo; split evenly across datapoints.</source>
          <target state="translated">首先，我们必须了解数据的结构。它具有100个随机生成的输入数据点，3个类在数据点之间不均匀地划分以及10个&amp;ldquo;组&amp;rdquo;在数据点之间均匀地划分。</target>
        </trans-unit>
        <trans-unit id="cc0ba427a614a821f465ac72c31717d2cbd7abed" translate="yes" xml:space="preserve">
          <source>First, we train a decision tree and a multi-layer perceptron on the diabetes dataset.</source>
          <target state="translated">首先,我们在糖尿病数据集上训练一棵决策树和一个多层感知器。</target>
        </trans-unit>
        <trans-unit id="f123dfa67a1788d2150ed770bdb351ff3e4fb8cc" translate="yes" xml:space="preserve">
          <source>First, we train a random forest on the breast cancer dataset and evaluate its accuracy on a test set:</source>
          <target state="translated">首先,我们在乳腺癌数据集上训练一个随机森林,并在测试集上评估其准确性。</target>
        </trans-unit>
        <trans-unit id="2b8f70566f670b04c1ac522dc7b0f129462badb9" translate="yes" xml:space="preserve">
          <source>First, we want to estimate the score on the original data:</source>
          <target state="translated">首先,我们要对原始数据进行估分。</target>
        </trans-unit>
        <trans-unit id="fab091b4d5d2bc522f0ed86af86bd56d5c7a2d19" translate="yes" xml:space="preserve">
          <source>First, we will load the diabetes dataset and initiate a gradient boosting regressor, a random forest regressor and a linear regression. Next, we will use the 3 regressors to build the voting regressor:</source>
          <target state="translated">首先,我们将加载糖尿病数据集,并启动一个梯度提升回归器、一个随机森林回归器和一个线性回归。接下来,我们将使用这3个回归器来建立投票回归器。</target>
        </trans-unit>
        <trans-unit id="5918b4ffaa61bc5b2d97e424d8740b3562535f07" translate="yes" xml:space="preserve">
          <source>First, we would like a transformer that extracts the subject and body of each post. Since this is a stateless transformation (does not require state information from training data), we can define a function that performs the data transformation then use &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.functiontransformer#sklearn.preprocessing.FunctionTransformer&quot;&gt;&lt;code&gt;FunctionTransformer&lt;/code&gt;&lt;/a&gt; to create a scikit-learn transformer.</source>
          <target state="translated">首先，我们需要一个转换器来提取每个帖子的主题和正文。由于这是无状态转换（不需要训练数据中的状态信息），因此我们可以定义执行数据转换的函数，然后使用&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.functiontransformer#sklearn.preprocessing.FunctionTransformer&quot;&gt; &lt;code&gt;FunctionTransformer&lt;/code&gt; &lt;/a&gt;创建scikit-learn转换器。</target>
        </trans-unit>
        <trans-unit id="b01058ecfb06b2978590da1239a0a81618b1465b" translate="yes" xml:space="preserve">
          <source>Fisher transformation. Wikipedia. &lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher_transformation&quot;&gt;https://en.wikipedia.org/wiki/Fisher_transformation&lt;/a&gt;</source>
          <target state="translated">费舍尔变换。维基百科。&lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher_transformation&quot;&gt;https://zh.wikipedia.org/wiki/Fisher_transformation&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="75ec02df98bcfaf21ee3dd0abe872b47c664ba1c" translate="yes" xml:space="preserve">
          <source>Fisher, R.A. &amp;ldquo;The use of multiple measurements in taxonomic problems&amp;rdquo; Annual Eugenics, 7, Part II, 179-188 (1936); also in &amp;ldquo;Contributions to Mathematical Statistics&amp;rdquo; (John Wiley, NY, 1950).</source>
          <target state="translated">费舍尔，RA，&amp;ldquo;在分类学问题中使用多项测量&amp;rdquo;，《年度优生学》，第7部分，第179-188页，1936年；同样在&amp;ldquo;对数学统计的贡献&amp;rdquo;中（约翰&amp;middot;威利，纽约，1950年）。</target>
        </trans-unit>
        <trans-unit id="4d49737491d2fdff73fcfce78bfa3496db9aead4" translate="yes" xml:space="preserve">
          <source>Fit Gaussian Naive Bayes according to X, y</source>
          <target state="translated">根据X、y拟合高斯奈夫贝叶斯。</target>
        </trans-unit>
        <trans-unit id="5697d81dd45cef7c7ddb8bf2ab425085bcc0125d" translate="yes" xml:space="preserve">
          <source>Fit Gaussian process classification model</source>
          <target state="translated">拟合高斯过程分类模型</target>
        </trans-unit>
        <trans-unit id="87cac59f89b2fa13ce5de11220e3eeec4bd8c5cf" translate="yes" xml:space="preserve">
          <source>Fit Gaussian process regression model.</source>
          <target state="translated">拟合高斯过程回归模型。</target>
        </trans-unit>
        <trans-unit id="8eb1d823143ba03b3eb9b823b5dab8b0cf44511a" translate="yes" xml:space="preserve">
          <source>Fit Kernel Ridge regression model</source>
          <target state="translated">拟合Kernel Ridge回归模型</target>
        </trans-unit>
        <trans-unit id="62df27bb0a01d202786ab5c8f2652f91558551a4" translate="yes" xml:space="preserve">
          <source>Fit KernelCenterer</source>
          <target state="translated">Fit KernelCenter</target>
        </trans-unit>
        <trans-unit id="f97a5239f74bba940e2a0890df2d8120afcfa03c" translate="yes" xml:space="preserve">
          <source>Fit LSI model on training data X.</source>
          <target state="translated">在训练数据X上拟合LSI模型。</target>
        </trans-unit>
        <trans-unit id="c2f5d79cd7108f6536b92a8ab72454a376b81e9e" translate="yes" xml:space="preserve">
          <source>Fit LSI model to X and perform dimensionality reduction on X.</source>
          <target state="translated">将LSI模型拟合到X上,并对X进行降维。</target>
        </trans-unit>
        <trans-unit id="f79f31a7bd81775029053b8e229553093e280715" translate="yes" xml:space="preserve">
          <source>Fit LinearDiscriminantAnalysis model according to the given</source>
          <target state="translated">根据给定的LinearDiscriminantAnalysis模型拟合。</target>
        </trans-unit>
        <trans-unit id="5a914df7976c81fad1a03ac324a4632a19d9e602" translate="yes" xml:space="preserve">
          <source>Fit LinearDiscriminantAnalysis model according to the given training data and parameters.</source>
          <target state="translated">根据给定的训练数据和参数拟合LinearDiscriminantAnalysis模型。</target>
        </trans-unit>
        <trans-unit id="cb91a62a58b24a52b6e32fdd160e489a55cfbac1" translate="yes" xml:space="preserve">
          <source>Fit MultiTaskElasticNet model with coordinate descent</source>
          <target state="translated">用坐标下降法拟合MultiTaskElasticNet模型。</target>
        </trans-unit>
        <trans-unit id="07e793af99a651a8f0a47822a080e4cf194bac9b" translate="yes" xml:space="preserve">
          <source>Fit Naive Bayes classifier according to X, y</source>
          <target state="translated">根据X,y,拟合Naive Bayes分类器</target>
        </trans-unit>
        <trans-unit id="afc10844f54e485a835b4e52b36e0ebefe70c8de" translate="yes" xml:space="preserve">
          <source>Fit OneHotEncoder to X, then transform X.</source>
          <target state="translated">将OneHotEncoder适配到X上,然后对X进行变换。</target>
        </trans-unit>
        <trans-unit id="6cbc8c577214883d5f9bdf864d2a7d76e95b5a74" translate="yes" xml:space="preserve">
          <source>Fit OneHotEncoder to X.</source>
          <target state="translated">将OneHotEncoder装入X。</target>
        </trans-unit>
        <trans-unit id="7a3e15c6e4bf063bec4aa435d4748fb868286ed7" translate="yes" xml:space="preserve">
          <source>Fit Ridge and HuberRegressor on a dataset with outliers.</source>
          <target state="translated">在有离群值的数据集上拟合Ridge和HuberRegressor。</target>
        </trans-unit>
        <trans-unit id="9157b13407894777185a7ac8935a66fc19422ca1" translate="yes" xml:space="preserve">
          <source>Fit Ridge classifier model.</source>
          <target state="translated">契合岭分类器模型。</target>
        </trans-unit>
        <trans-unit id="e744f084cdfc69f98e0c6bdc1ef85d9a5fdf52b4" translate="yes" xml:space="preserve">
          <source>Fit Ridge classifier with cv.</source>
          <target state="translated">档岭</target>
        </trans-unit>
        <trans-unit id="8f8e3b0b4710ae51d90dcd69d91e33d123946d3a" translate="yes" xml:space="preserve">
          <source>Fit Ridge regression model</source>
          <target state="translated">拟合岭回归模型</target>
        </trans-unit>
        <trans-unit id="47da48f063cc42a3f008d1e6c416d90a29ea0591" translate="yes" xml:space="preserve">
          <source>Fit Ridge regression model with cv.</source>
          <target state="translated">拟合岭回归模型与cv。</target>
        </trans-unit>
        <trans-unit id="2e3e7ebdde04653523f01d0d804ccc0c533f3460" translate="yes" xml:space="preserve">
          <source>Fit Ridge regression model.</source>
          <target state="translated">拟合岭回归模型。</target>
        </trans-unit>
        <trans-unit id="74fbf563cb041ef637760be7f01def498fc1300e" translate="yes" xml:space="preserve">
          <source>Fit X into an embedded space and return that transformed output.</source>
          <target state="translated">将X拟合到一个嵌入式空间中,并返回该转换后的输出。</target>
        </trans-unit>
        <trans-unit id="97ca0bc1677dbeaefd600411062ca557eacc6754" translate="yes" xml:space="preserve">
          <source>Fit X into an embedded space.</source>
          <target state="translated">将X装入嵌入式空间。</target>
        </trans-unit>
        <trans-unit id="9b21c283121f9ec299302b1b3016dbcdb3391466" translate="yes" xml:space="preserve">
          <source>Fit a Bayesian ridge model and optimize the regularization parameters lambda (precision of the weights) and alpha (precision of the noise).</source>
          <target state="translated">拟合贝叶斯脊模型,优化正则化参数lambda(权重的精度)和alpha(噪声的精度)。</target>
        </trans-unit>
        <trans-unit id="dd6f625705b26516fc692deb07fa9f7046f6035b" translate="yes" xml:space="preserve">
          <source>Fit a Bayesian ridge model. See the Notes section for details on this implementation and the optimization of the regularization parameters lambda (precision of the weights) and alpha (precision of the noise).</source>
          <target state="translated">拟合一个贝叶斯岭模型。请参阅 &quot;注释 &quot;一节,了解本实施例和正则化参数lambda(权重的精度)和alpha(噪声的精度)的优化。</target>
        </trans-unit>
        <trans-unit id="dac39fc69b5b06ff2bcc2ce8d7b8984bce5d49fc" translate="yes" xml:space="preserve">
          <source>Fit a Generalized Linear Model.</source>
          <target state="translated">拟合一个广义线性模型。</target>
        </trans-unit>
        <trans-unit id="8bec80135f416f6c22f260fc3b1ac04b4cff59ba" translate="yes" xml:space="preserve">
          <source>Fit a model to the random subset (&lt;code&gt;base_estimator.fit&lt;/code&gt;) and check whether the estimated model is valid (see &lt;code&gt;is_model_valid&lt;/code&gt;).</source>
          <target state="translated">将模型拟合到随机子集（ &lt;code&gt;base_estimator.fit&lt;/code&gt; ），然后检查估计的模型是否有效（请参见 &lt;code&gt;is_model_valid&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="ebd8c04ad061ef008e394e38c512b2d8b58287fa" translate="yes" xml:space="preserve">
          <source>Fit a semi-supervised label propagation model based</source>
          <target state="translated">拟合半监督标签传播模型,基于</target>
        </trans-unit>
        <trans-unit id="deba03b3f9d00ab1e647b43aa626a17c324cec03" translate="yes" xml:space="preserve">
          <source>Fit all the transforms one after the other and transform the data, then fit the transformed data using the final estimator.</source>
          <target state="translated">将所有的变换一一拟合,并对数据进行变换,然后使用最终估计器对变换后的数据进行拟合。</target>
        </trans-unit>
        <trans-unit id="114f8ea42731308cb5abe991bcd972d570944b6a" translate="yes" xml:space="preserve">
          <source>Fit all transformers using X.</source>
          <target state="translated">用X装好所有的变压器。</target>
        </trans-unit>
        <trans-unit id="246e8826a6bd7a4f56f132c591c766db9ce2d6b0" translate="yes" xml:space="preserve">
          <source>Fit all transformers, transform the data and concatenate results.</source>
          <target state="translated">拟合所有的变换器,变换数据,并将结果连接起来。</target>
        </trans-unit>
        <trans-unit id="922528f3171c2d46b470779ae89446b3062cf8a4" translate="yes" xml:space="preserve">
          <source>Fit estimator and transform dataset.</source>
          <target state="translated">拟合估计器和转换数据集。</target>
        </trans-unit>
        <trans-unit id="54af94e49eb61f7670851d6ef57ae7193fdff2da" translate="yes" xml:space="preserve">
          <source>Fit estimator to data.</source>
          <target state="translated">对数据进行拟合估计。</target>
        </trans-unit>
        <trans-unit id="85fa5fe8b9795ca2b319f392f4a4756a8b584c81" translate="yes" xml:space="preserve">
          <source>Fit estimator using RANSAC algorithm.</source>
          <target state="translated">使用RANSAC算法进行拟合估计。</target>
        </trans-unit>
        <trans-unit id="820a840249b34711a41e746dbc8ea00e0bef6043" translate="yes" xml:space="preserve">
          <source>Fit estimator.</source>
          <target state="translated">拟合估计器。</target>
        </trans-unit>
        <trans-unit id="c7654cec30bb319e2781e513c9f9ca93708de9b2" translate="yes" xml:space="preserve">
          <source>Fit is on grid of alphas and best alpha estimated by cross-validation.</source>
          <target state="translated">拟合度是在阿尔法的网格上,通过交叉验证估计的最佳阿尔法。</target>
        </trans-unit>
        <trans-unit id="b8cbf7d1420444207c14a8b65b5fcf478860f130" translate="yes" xml:space="preserve">
          <source>Fit label binarizer</source>
          <target state="translated">合适的标签二值机</target>
        </trans-unit>
        <trans-unit id="e7ecedc7848a8ef424a3d78854bd272e8154c780" translate="yes" xml:space="preserve">
          <source>Fit label binarizer and transform multi-class labels to binary labels.</source>
          <target state="translated">拟合标签二值化器,并将多类标签转化为二进制标签。</target>
        </trans-unit>
        <trans-unit id="c75cef45e92464b24b2d996a50f9aed44b9ef31e" translate="yes" xml:space="preserve">
          <source>Fit label encoder</source>
          <target state="translated">安装标签编码器</target>
        </trans-unit>
        <trans-unit id="5de88deba19907fdf6f6eb3678730f7aacb3c312" translate="yes" xml:space="preserve">
          <source>Fit label encoder and return encoded labels</source>
          <target state="translated">安装标签编码器并返回编码标签</target>
        </trans-unit>
        <trans-unit id="cf615a74e4f9177f1a29f39484c75ecef454ecbe" translate="yes" xml:space="preserve">
          <source>Fit linear model with Passive Aggressive algorithm.</source>
          <target state="translated">用被动进取算法拟合线性模型。</target>
        </trans-unit>
        <trans-unit id="def5054532c40485109a181ee65c06aba2df1b53" translate="yes" xml:space="preserve">
          <source>Fit linear model with Stochastic Gradient Descent.</source>
          <target state="translated">用随机梯度下降法拟合线性模型。</target>
        </trans-unit>
        <trans-unit id="80dff5f041e36b3407c4f4e7528431510ae562cc" translate="yes" xml:space="preserve">
          <source>Fit linear model with coordinate descent</source>
          <target state="translated">用坐标下降法拟合线性模型</target>
        </trans-unit>
        <trans-unit id="4a42bd8bc00e1e2eb46153b9c0e14ea2a3d30f42" translate="yes" xml:space="preserve">
          <source>Fit linear model.</source>
          <target state="translated">拟合线性模型。</target>
        </trans-unit>
        <trans-unit id="596c5db56f9a987de5f7691971fcf687d77673ec" translate="yes" xml:space="preserve">
          <source>Fit model to data.</source>
          <target state="translated">根据数据拟合模型。</target>
        </trans-unit>
        <trans-unit id="7bad9b91d21434b63dea0481c80eaf84a7cbcb25" translate="yes" xml:space="preserve">
          <source>Fit model with coordinate descent.</source>
          <target state="translated">用坐标下降法拟合模型。</target>
        </trans-unit>
        <trans-unit id="8c69e1745d795a8f39d4e8e6661ab7afc8b0bcff" translate="yes" xml:space="preserve">
          <source>Fit regression model</source>
          <target state="translated">拟合回归模型</target>
        </trans-unit>
        <trans-unit id="dddcb769b9b69c71c174f417fcd2a333b19f55fd" translate="yes" xml:space="preserve">
          <source>Fit regression model with Bayesian Ridge Regression.</source>
          <target state="translated">用贝叶斯岭回归法拟合回归模型。</target>
        </trans-unit>
        <trans-unit id="dda67be7d4db5d3187deb44446a3404222c185a6" translate="yes" xml:space="preserve">
          <source>Fit the ARDRegression model according to the given training data and parameters.</source>
          <target state="translated">根据给定的训练数据和参数,拟合ARDRegression模型。</target>
        </trans-unit>
        <trans-unit id="b5a680830417593aa73dd7de7fcef69c6ed24e1b" translate="yes" xml:space="preserve">
          <source>Fit the EllipticEnvelope model.</source>
          <target state="translated">拟合EllipticEnvelope模型。</target>
        </trans-unit>
        <trans-unit id="b52ba2a596ae6c033bcc7f3cf20ecd5c58d00c2f" translate="yes" xml:space="preserve">
          <source>Fit the FactorAnalysis model to X using EM</source>
          <target state="translated">利用EM对X进行FactorAnalysis模型拟合。</target>
        </trans-unit>
        <trans-unit id="d35f22f8e1c64965b364c3173e1ec356759343b4" translate="yes" xml:space="preserve">
          <source>Fit the FactorAnalysis model to X using SVD based approach</source>
          <target state="translated">使用基于SVD的方法对X进行因子分析模型的拟合。</target>
        </trans-unit>
        <trans-unit id="6613ce48c67ddab345a10c60b764cb90b5d43667" translate="yes" xml:space="preserve">
          <source>Fit the Kernel Density model on the data.</source>
          <target state="translated">在数据上拟合内核密度模型。</target>
        </trans-unit>
        <trans-unit id="18de2bdae7f6b4aabf66891dec1f775662960310" translate="yes" xml:space="preserve">
          <source>Fit the LSH forest on the data.</source>
          <target state="translated">在数据上拟合LSH森林。</target>
        </trans-unit>
        <trans-unit id="12a1c0d798db89b07ae159bda7700f6de8a414b1" translate="yes" xml:space="preserve">
          <source>Fit the Ledoit-Wolf shrunk covariance model according to the given training data and parameters.</source>
          <target state="translated">根据给定的训练数据和参数,拟合Ledoit-Wolf收缩协方差模型。</target>
        </trans-unit>
        <trans-unit id="7e835684e570bb3989f14c8abec4c77362174b02" translate="yes" xml:space="preserve">
          <source>Fit the NearestCentroid model according to the given training data.</source>
          <target state="translated">根据给定的训练数据拟合NearestCentroid模型。</target>
        </trans-unit>
        <trans-unit id="faef2120c4a0d719909dbc882126884ed71710cf" translate="yes" xml:space="preserve">
          <source>Fit the Oracle Approximating Shrinkage covariance model according to the given training data and parameters.</source>
          <target state="translated">根据给定的训练数据和参数,拟合Oracle近似收缩协方差模型。</target>
        </trans-unit>
        <trans-unit id="f68da32a4110559ba3f4b03666c6374bbe1b67bc" translate="yes" xml:space="preserve">
          <source>Fit the OrdinalEncoder to X.</source>
          <target state="translated">将OrdinalEncoder适合X。</target>
        </trans-unit>
        <trans-unit id="9efb3f60cfc062ba641dafe57455afe5be189469" translate="yes" xml:space="preserve">
          <source>Fit the RFE model and automatically tune the number of selected</source>
          <target state="translated">拟合RFE模型,并自动调整选择的数量。</target>
        </trans-unit>
        <trans-unit id="a1f31e1eab4986ee8b13885092428f58b3a203e9" translate="yes" xml:space="preserve">
          <source>Fit the RFE model and automatically tune the number of selected features.</source>
          <target state="translated">拟合RFE模型并自动调整所选特征的数量。</target>
        </trans-unit>
        <trans-unit id="9813edcd03393c0187279f71f7b7e90f266a7ca4" translate="yes" xml:space="preserve">
          <source>Fit the RFE model and then the underlying estimator on the selected</source>
          <target state="translated">拟合RFE模型,然后在所选的RFE模型上拟合基本估计器。</target>
        </trans-unit>
        <trans-unit id="2e888db086bdb6861e2b6c27ddb4959896a661ee" translate="yes" xml:space="preserve">
          <source>Fit the RFE model and then the underlying estimator on the selected features.</source>
          <target state="translated">拟合RFE模型,然后对选定的特征进行基本估计。</target>
        </trans-unit>
        <trans-unit id="3e8e54a78ebc1bc30d8f6371731aa34e571e1cb7" translate="yes" xml:space="preserve">
          <source>Fit the SVM model according to the given training data.</source>
          <target state="translated">根据给定的训练数据来拟合SVM模型。</target>
        </trans-unit>
        <trans-unit id="e094bb2f16b3f77c4f0ded4998dd5acef469f424" translate="yes" xml:space="preserve">
          <source>Fit the SelectFromModel meta-transformer only once.</source>
          <target state="translated">只适合SelectFromModel元变换器一次。</target>
        </trans-unit>
        <trans-unit id="4f2b5174d874ffda16493a72eb6b7d9e49692dfa" translate="yes" xml:space="preserve">
          <source>Fit the SelectFromModel meta-transformer.</source>
          <target state="translated">契合SelectFromModel元变换器。</target>
        </trans-unit>
        <trans-unit id="48df38063d696d0aeb8b0988a010338565cac757" translate="yes" xml:space="preserve">
          <source>Fit the calibrated model</source>
          <target state="translated">拟合校准模型</target>
        </trans-unit>
        <trans-unit id="c57e47c49821e6802e2a740c9051d8c66c744ebb" translate="yes" xml:space="preserve">
          <source>Fit the clustering from features or affinity matrix, and return cluster labels.</source>
          <target state="translated">从特征或亲和矩阵拟合聚类,并返回聚类标签。</target>
        </trans-unit>
        <trans-unit id="d9687ab7d6ecd0d65550bf660d242c20bd8d898b" translate="yes" xml:space="preserve">
          <source>Fit the clustering from features, or affinity matrix.</source>
          <target state="translated">从特征中拟合聚类,或亲和矩阵。</target>
        </trans-unit>
        <trans-unit id="548ac10033a88a7f2f90c29b16d9b1eac7c9baf8" translate="yes" xml:space="preserve">
          <source>Fit the data from X, and returns the embedded coordinates</source>
          <target state="translated">从X中拟合数据,并返回嵌入的坐标。</target>
        </trans-unit>
        <trans-unit id="08758549856b6a0e509ce1e713cae02d47e4bedc" translate="yes" xml:space="preserve">
          <source>Fit the estimator.</source>
          <target state="translated">拟合估计器。</target>
        </trans-unit>
        <trans-unit id="855887273459477b0845aa915278722b2bfdc95e" translate="yes" xml:space="preserve">
          <source>Fit the estimators.</source>
          <target state="translated">拟合估算器。</target>
        </trans-unit>
        <trans-unit id="78a5f0fa7f9e8e8876e6f8fdd879d2a72169691e" translate="yes" xml:space="preserve">
          <source>Fit the gradient boosting model.</source>
          <target state="translated">拟合梯度提升模型。</target>
        </trans-unit>
        <trans-unit id="59e531372ee14a17e072823da12da9c216d03637" translate="yes" xml:space="preserve">
          <source>Fit the hierarchical clustering from features or distance matrix, and return cluster labels.</source>
          <target state="translated">从特征或距离矩阵拟合层次聚类,并返回聚类标签。</target>
        </trans-unit>
        <trans-unit id="5f5e34ffa1bdd98c15becac765f254aeb2f0a093" translate="yes" xml:space="preserve">
          <source>Fit the hierarchical clustering from features, or distance matrix.</source>
          <target state="translated">从特征中拟合层次聚类,或距离矩阵。</target>
        </trans-unit>
        <trans-unit id="6951b3b9c1e77870a95debb989a3776ac4c5d6a1" translate="yes" xml:space="preserve">
          <source>Fit the hierarchical clustering on the data</source>
          <target state="translated">在数据上拟合层次聚类。</target>
        </trans-unit>
        <trans-unit id="5b5563854bcbf3fe8e972427b6550f84e6f5e44a" translate="yes" xml:space="preserve">
          <source>Fit the imputer on X.</source>
          <target state="translated">在X上装上撞击器。</target>
        </trans-unit>
        <trans-unit id="f79ad350ada74918a25b6a18b9c98a44219aea81" translate="yes" xml:space="preserve">
          <source>Fit the label sets binarizer and transform the given label sets</source>
          <target state="translated">拟合标签集二值化器并转换给定的标签集。</target>
        </trans-unit>
        <trans-unit id="93ac61c2b8893a8a8dc44af4d06cf2252851b7b2" translate="yes" xml:space="preserve">
          <source>Fit the label sets binarizer, storing &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;</source>
          <target state="translated">适合标签集二值化器，存储&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="aff4cb7658d810f05c33be44cd22feded7d4bab1" translate="yes" xml:space="preserve">
          <source>Fit the label sets binarizer, storing &lt;code&gt;classes_&lt;/code&gt;</source>
          <target state="translated">适合标签集二值化器，存储 &lt;code&gt;classes_&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="8fe48671e323549fd93560341a4a3c7b625e4c7d" translate="yes" xml:space="preserve">
          <source>Fit the model</source>
          <target state="translated">拟合模型</target>
        </trans-unit>
        <trans-unit id="4a0fb954f184570eb54f89782d45ff6b62de8f93" translate="yes" xml:space="preserve">
          <source>Fit the model according to the given training data and parameters.</source>
          <target state="translated">根据给定的训练数据和参数来拟合模型。</target>
        </trans-unit>
        <trans-unit id="226810036d48f43519ac6362c835a5ebb75ac273" translate="yes" xml:space="preserve">
          <source>Fit the model according to the given training data.</source>
          <target state="translated">根据给定的训练数据来拟合模型。</target>
        </trans-unit>
        <trans-unit id="12dc3b0f35bd8c187f3d6ad72da28e55416ad4ec" translate="yes" xml:space="preserve">
          <source>Fit the model and recover the sources from X.</source>
          <target state="translated">拟合模型并从X中恢复来源。</target>
        </trans-unit>
        <trans-unit id="71e9ee1734e2bbf9ac4fe07ac1b6c03c04237b08" translate="yes" xml:space="preserve">
          <source>Fit the model and transform with the final estimator</source>
          <target state="translated">拟合模型,并用最终估计器进行变换。</target>
        </trans-unit>
        <trans-unit id="6b8aa0f161bb8a77dc4775ed1ee1fcfc2c406c82" translate="yes" xml:space="preserve">
          <source>Fit the model from data in X and transform X.</source>
          <target state="translated">从X中的数据拟合模型,并变换X。</target>
        </trans-unit>
        <trans-unit id="0aa179622924cc08ee70a31764d01ab49bbf6bcd" translate="yes" xml:space="preserve">
          <source>Fit the model from data in X.</source>
          <target state="translated">根据X中的数据拟合模型。</target>
        </trans-unit>
        <trans-unit id="09a244ac4f08853db4f27fa0f56ca2d8ae157c91" translate="yes" xml:space="preserve">
          <source>Fit the model to X.</source>
          <target state="translated">将模型拟合到X上。</target>
        </trans-unit>
        <trans-unit id="d8fc33348e2baabb167cc4df9e594abe384c1eb2" translate="yes" xml:space="preserve">
          <source>Fit the model to data matrix X and target y.</source>
          <target state="translated">将模型拟合到数据矩阵X和目标y上。</target>
        </trans-unit>
        <trans-unit id="a08fe1d5397b99194d7ea288b13912038b863a22" translate="yes" xml:space="preserve">
          <source>Fit the model to data matrix X and target(s) y.</source>
          <target state="translated">将模型拟合到数据矩阵X和目标y上。</target>
        </trans-unit>
        <trans-unit id="5260da8ec9e88ebba64d6137961aeb8e84b7f391" translate="yes" xml:space="preserve">
          <source>Fit the model to data matrix X and targets Y.</source>
          <target state="translated">将模型拟合到数据矩阵X和目标Y上。</target>
        </trans-unit>
        <trans-unit id="45ba17915e2e3031d1582da2d277da7f4931a4e1" translate="yes" xml:space="preserve">
          <source>Fit the model to data.</source>
          <target state="translated">根据数据拟合模型。</target>
        </trans-unit>
        <trans-unit id="afd434d49f80c8d01132ca406d4c79b805654e96" translate="yes" xml:space="preserve">
          <source>Fit the model to data. Fit a separate model for each output variable.</source>
          <target state="translated">根据数据拟合模型。为每个输出变量拟合一个单独的模型。</target>
        </trans-unit>
        <trans-unit id="cfdc299b35aa8a48c0af501e27ae7cce65e8f528" translate="yes" xml:space="preserve">
          <source>Fit the model to the data X which should contain a partial segment of the data.</source>
          <target state="translated">将模型拟合到数据X上,数据X应包含部分数据段。</target>
        </trans-unit>
        <trans-unit id="b93160749fdae6a7fa042ecb31b3787d54faa056" translate="yes" xml:space="preserve">
          <source>Fit the model to the data X.</source>
          <target state="translated">将模型与数据X拟合。</target>
        </trans-unit>
        <trans-unit id="22b95bc22aacaf67cf593094b402a5ffdf9c9f1a" translate="yes" xml:space="preserve">
          <source>Fit the model using X as training data</source>
          <target state="translated">用X作为训练数据拟合模型</target>
        </trans-unit>
        <trans-unit id="c8f2bc6ec471b46a95d660913e3db5351ccdd413" translate="yes" xml:space="preserve">
          <source>Fit the model using X as training data and y as target values</source>
          <target state="translated">将X作为训练数据,将y作为目标值来拟合模型。</target>
        </trans-unit>
        <trans-unit id="67d84010e7f659892bf337ed29c5877ed8354203" translate="yes" xml:space="preserve">
          <source>Fit the model using X as training data.</source>
          <target state="translated">以X作为训练数据拟合模型。</target>
        </trans-unit>
        <trans-unit id="3b6c346e2454cc9e9a61a017fe7c431d552c8c5b" translate="yes" xml:space="preserve">
          <source>Fit the model using X, y as training data.</source>
          <target state="translated">用X,y作为训练数据来拟合模型。</target>
        </trans-unit>
        <trans-unit id="5c7f073bf35ac36015511e86a701acdf9d1f18c6" translate="yes" xml:space="preserve">
          <source>Fit the model with X and apply the dimensionality reduction on X.</source>
          <target state="translated">用X拟合模型,并在X上应用维度降低。</target>
        </trans-unit>
        <trans-unit id="ea7ce544f3a486e6f2a49538bf4fe64bf5a8afde" translate="yes" xml:space="preserve">
          <source>Fit the model with X, using minibatches of size batch_size.</source>
          <target state="translated">用X拟合模型,使用大小为batch_size的minibatches。</target>
        </trans-unit>
        <trans-unit id="e7967bf329b3c1f757d75d92e374951929321ba7" translate="yes" xml:space="preserve">
          <source>Fit the model with X.</source>
          <target state="translated">用X来拟合模型。</target>
        </trans-unit>
        <trans-unit id="a5abbc09518b549e779e08db2e6e49f0ba32533d" translate="yes" xml:space="preserve">
          <source>Fit the random classifier.</source>
          <target state="translated">拟合随机分类器。</target>
        </trans-unit>
        <trans-unit id="30f7575e3bac8aaa2852370b548ad3f04b290586" translate="yes" xml:space="preserve">
          <source>Fit the random regressor.</source>
          <target state="translated">拟合随机回归器。</target>
        </trans-unit>
        <trans-unit id="4fae2d49ee8a5e8d8ea52343db3e2b05ff45988e" translate="yes" xml:space="preserve">
          <source>Fit the ridge classifier.</source>
          <target state="translated">契合山脊分级机。</target>
        </trans-unit>
        <trans-unit id="1e4446f620ec40ca22b46eba2b6f8ce6be3c57e2" translate="yes" xml:space="preserve">
          <source>Fit the shrunk covariance model according to the given training data and parameters.</source>
          <target state="translated">根据给定的训练数据和参数,拟合收缩协方差模型。</target>
        </trans-unit>
        <trans-unit id="2b96765d688b574c756064537ec2686c8ac36571" translate="yes" xml:space="preserve">
          <source>Fit the transformer on X.</source>
          <target state="translated">在X上安装变压器。</target>
        </trans-unit>
        <trans-unit id="acd485cd941415835a11d2180278c2146ce5888c" translate="yes" xml:space="preserve">
          <source>Fit the weights of a regression model, using an ARD prior. The weights of the regression model are assumed to be in Gaussian distributions. Also estimate the parameters lambda (precisions of the distributions of the weights) and alpha (precision of the distribution of the noise). The estimation is done by an iterative procedures (Evidence Maximization)</source>
          <target state="translated">使用ARD先验,拟合回归模型的权重。回归模型的权重被假定为高斯分布。同时估计参数lambda(权重分布的精度)和alpha(噪声分布的精度)。估计是通过一个迭代程序(证据最大化)来完成的。</target>
        </trans-unit>
        <trans-unit id="ea18404b90123396c3f41537d11afad54c507780" translate="yes" xml:space="preserve">
          <source>Fit to data, then transform it.</source>
          <target state="translated">对数据进行拟合,然后进行转换。</target>
        </trans-unit>
        <trans-unit id="c2cf341635a3875675d46dd618b9a1757d9a6c7c" translate="yes" xml:space="preserve">
          <source>Fit transformer by checking X.</source>
          <target state="translated">通过检查X安装变压器。</target>
        </trans-unit>
        <trans-unit id="eb75d7eb91b3dadf245e1b3bf8f4c37a27824085" translate="yes" xml:space="preserve">
          <source>Fit underlying estimators.</source>
          <target state="translated">契合基本估算器。</target>
        </trans-unit>
        <trans-unit id="c093c0cee7f3b9fa93ffb32acb026baea322889f" translate="yes" xml:space="preserve">
          <source>Fits a Minimum Covariance Determinant with the FastMCD algorithm.</source>
          <target state="translated">用FastMCD算法拟合一个最小协方差确定因子。</target>
        </trans-unit>
        <trans-unit id="f30506afc59d08eae550fdeb02ae856731ea996b" translate="yes" xml:space="preserve">
          <source>Fits all the transforms one after the other and transforms the data, then uses fit_transform on transformed data with the final estimator.</source>
          <target state="translated">将所有的变换一一拟合,并对数据进行变换,然后在变换后的数据上使用fit_transform与最终的估计器。</target>
        </trans-unit>
        <trans-unit id="c5c1cc9c0352ec3e2d5fcc0df63b71ce152f382f" translate="yes" xml:space="preserve">
          <source>Fits the GraphicalLasso covariance model to X.</source>
          <target state="translated">将GraphicalLasso协方差模型拟合到X上。</target>
        </trans-unit>
        <trans-unit id="268ae9ae0a9043d80b2e575c7e344f83f78e662d" translate="yes" xml:space="preserve">
          <source>Fits the GraphicalLasso model to X.</source>
          <target state="translated">将GraphicalLasso模型拟合到X上。</target>
        </trans-unit>
        <trans-unit id="643e04850030e1e1aeeaf619a88e7dab7e6a06be" translate="yes" xml:space="preserve">
          <source>Fits the Ledoit-Wolf shrunk covariance model according to the given training data and parameters.</source>
          <target state="translated">根据给定的训练数据和参数,拟合Ledoit-Wolf收缩协方差模型。</target>
        </trans-unit>
        <trans-unit id="60a2c06b8221e361c36d5fdce8ee644d8456bfce" translate="yes" xml:space="preserve">
          <source>Fits the Maximum Likelihood Estimator covariance model according to the given training data and parameters.</source>
          <target state="translated">根据给定的训练数据和参数,拟合最大似然估计器协方差模型。</target>
        </trans-unit>
        <trans-unit id="9b00c787fd8f13356df0bf86c478f2b7848ac4d7" translate="yes" xml:space="preserve">
          <source>Fits the Oracle Approximating Shrinkage covariance model according to the given training data and parameters.</source>
          <target state="translated">根据给定的训练数据和参数,拟合Oracle近似收缩协方差模型。</target>
        </trans-unit>
        <trans-unit id="2fb3d8548147a6429a7eeed9e3fbe0456049bc8c" translate="yes" xml:space="preserve">
          <source>Fits the estimator.</source>
          <target state="translated">契合估计器。</target>
        </trans-unit>
        <trans-unit id="3cea2475743e34d34ded723467b2a735e1203cfc" translate="yes" xml:space="preserve">
          <source>Fits the imputer on X and return self.</source>
          <target state="translated">在X上装上imputer并返回self。</target>
        </trans-unit>
        <trans-unit id="6bb471485459c492057aae581eaa0cf9c4592a1d" translate="yes" xml:space="preserve">
          <source>Fits the imputer on X and return the transformed X.</source>
          <target state="translated">对X进行拟合,并返回变换后的X。</target>
        </trans-unit>
        <trans-unit id="c13921c2e1a959ce7d05648d804f502aded299cf" translate="yes" xml:space="preserve">
          <source>Fits the model to the training set X and returns the labels.</source>
          <target state="translated">将模型拟合到训练集X,并返回标签。</target>
        </trans-unit>
        <trans-unit id="98f6beb1ac87a74b86b8c1fedd5ae0ed0ac89461" translate="yes" xml:space="preserve">
          <source>Fits the shrunk covariance model according to the given training data and parameters.</source>
          <target state="translated">根据给定的训练数据和参数来拟合收缩协方差模型。</target>
        </trans-unit>
        <trans-unit id="f072640da94e1ad56e67373f3632aeaebdd8b854" translate="yes" xml:space="preserve">
          <source>Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.</source>
          <target state="translated">用可选的参数fit_params对X和y进行拟合变换,并返回X的变换版本。</target>
        </trans-unit>
        <trans-unit id="06f59e2a32c90e6aa8aff9dfb100b6f03f2af9ff" translate="yes" xml:space="preserve">
          <source>Fitted classifier or a fitted &lt;a href=&quot;sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; in which the last estimator is a classifier.</source>
          <target state="translated">拟合的分类器或拟合的&lt;a href=&quot;sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt;，其中最后一个估计量是分类器。</target>
        </trans-unit>
        <trans-unit id="e0dea44d64bdac3b47bf2c46576cad767f74d843" translate="yes" xml:space="preserve">
          <source>Fitted estimator.</source>
          <target state="translated">合适的估算器。</target>
        </trans-unit>
        <trans-unit id="fd568de48dadd27cd4e3ca2395da082642e8f8ec" translate="yes" xml:space="preserve">
          <source>Fitted regressor.</source>
          <target state="translated">合适的回归者。</target>
        </trans-unit>
        <trans-unit id="f162f6bcf340dc521df64a1aae70440e61c389db" translate="yes" xml:space="preserve">
          <source>Fitted scaler.</source>
          <target state="translated">装配好的剥皮机。</target>
        </trans-unit>
        <trans-unit id="bccb0ee0062e477c480530940a0aca37551f6020" translate="yes" xml:space="preserve">
          <source>Fitted vectorizer.</source>
          <target state="translated">合适的矢量器。</target>
        </trans-unit>
        <trans-unit id="14c6da7cd39661e1b0c6468a3c6a5907d4f99a9c" translate="yes" xml:space="preserve">
          <source>Fitting transformers may be computationally expensive. With its &lt;code&gt;memory&lt;/code&gt; parameter set, &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; will cache each transformer after calling &lt;code&gt;fit&lt;/code&gt;. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical. A typical example is the case of a grid search in which the transformers can be fitted only once and reused for each configuration.</source>
          <target state="translated">安装变压器可能在计算上很昂贵。通过设置其 &lt;code&gt;memory&lt;/code&gt; 参数，&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt;将在调用 &lt;code&gt;fit&lt;/code&gt; 之后缓存每个转换器。如果参数和输入数据相同，则使用此功能可避免计算管道内的配合变压器。一个典型的例子是在电网搜索中，其中的变压器只能安装一次，并且对于每种配置都可以重复使用。</target>
        </trans-unit>
        <trans-unit id="01fe05c223cb56d84d085e38ca62de1932a87e50" translate="yes" xml:space="preserve">
          <source>Flag indicating if the cross-validation values corresponding to each alpha should be stored in the &lt;code&gt;cv_values_&lt;/code&gt; attribute (see below). This flag is only compatible with &lt;code&gt;cv=None&lt;/code&gt; (i.e. using Generalized Cross-Validation).</source>
          <target state="translated">指示是否应将与每个alpha对应的交叉验证值存储在 &lt;code&gt;cv_values_&lt;/code&gt; 属性中的标志（请参见下文）。该标志仅与 &lt;code&gt;cv=None&lt;/code&gt; 兼容（即使用通用交叉验证）。</target>
        </trans-unit>
        <trans-unit id="1f498759924682e2363e94f7b83282b97429fcf5" translate="yes" xml:space="preserve">
          <source>Flag indicating which strategy to use when performing Generalized Cross-Validation. Options are:</source>
          <target state="translated">标志表示在执行广义交叉验证时要使用的策略。选项有:</target>
        </trans-unit>
        <trans-unit id="3407c4421a1f6ede0cab565dc5123546e65ddde6" translate="yes" xml:space="preserve">
          <source>Flat geometry, good for density estimation</source>
          <target state="translated">平坦的几何形状,有利于密度估算</target>
        </trans-unit>
        <trans-unit id="748a38982c93bb25fbfeb18b34277c35439ac98c" translate="yes" xml:space="preserve">
          <source>Flavanoids</source>
          <target state="translated">Flavanoids</target>
        </trans-unit>
        <trans-unit id="f55beb472c3b08362b7861294963760ddd037d08" translate="yes" xml:space="preserve">
          <source>Flavanoids:</source>
          <target state="translated">Flavanoids:</target>
        </trans-unit>
        <trans-unit id="1bf94453d6aa9e9092828eefafa6394692100339" translate="yes" xml:space="preserve">
          <source>Flexible pickling control for the communication to and from the worker processes.</source>
          <target state="translated">灵活的酸洗控制,用于与工人流程之间的沟通。</target>
        </trans-unit>
        <trans-unit id="2d83a2dbf42ef510856c4fe5eb69b3efa4599763" translate="yes" xml:space="preserve">
          <source>Flow Chart</source>
          <target state="translated">流程图</target>
        </trans-unit>
        <trans-unit id="87698cca8f914c77b735bad53fe489d2af135e70" translate="yes" xml:space="preserve">
          <source>Folder to be used by the pool for memmapping large arrays for sharing memory with worker processes. If None, this will try in order:</source>
          <target state="translated">池使用的文件夹,用于memmapping大型数组,以便与工作进程共享内存。如果无,将按顺序尝试。</target>
        </trans-unit>
        <trans-unit id="c09a9bf27e9aabfc7b35dc309da81eb816c5e989" translate="yes" xml:space="preserve">
          <source>Following special cases exist,</source>
          <target state="translated">存在以下特殊情况:</target>
        </trans-unit>
        <trans-unit id="313fc38f449c398563f152e4416c92af47202923" translate="yes" xml:space="preserve">
          <source>Follows Algorithm 4.3 of Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) http://arxiv.org/pdf/0909.4061</source>
          <target state="translated">遵循《用随机性寻找结构》的算法4.3。构建近似矩阵分解的随机算法 Halko等人,2009年(arXiv:909)http://arxiv.org/pdf/0909.4061。</target>
        </trans-unit>
        <trans-unit id="061a7a165d75c1efa21f2a2a8a850415a95a5911" translate="yes" xml:space="preserve">
          <source>Follows Algorithm 4.3 of Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf</source>
          <target state="translated">遵循《用随机性寻找结构》的算法4.3。构建近似矩阵分解的随机算法 Halko等人,2009年(arXiv:909)https://arxiv.org/pdf/0909.4061.pdf。</target>
        </trans-unit>
        <trans-unit id="ec0c3b76630fd745381cc215a284820af75a683a" translate="yes" xml:space="preserve">
          <source>Footnotes</source>
          <target state="translated">Footnotes</target>
        </trans-unit>
        <trans-unit id="871453ce5a358112246d8fa103eff55b515e95a2" translate="yes" xml:space="preserve">
          <source>For &amp;ldquo;one-vs-rest&amp;rdquo; &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; the attributes &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; have the shape &lt;code&gt;(n_classes, n_features)&lt;/code&gt; and &lt;code&gt;(n_classes,)&lt;/code&gt; respectively. Each row of the coefficients corresponds to one of the &lt;code&gt;n_classes&lt;/code&gt; &amp;ldquo;one-vs-rest&amp;rdquo; classifiers and similar for the intercepts, in the order of the &amp;ldquo;one&amp;rdquo; class.</source>
          <target state="translated">对于&amp;ldquo;&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;休息&amp;rdquo; LinearSVC，属性 &lt;code&gt;coef_&lt;/code&gt; 和 &lt;code&gt;intercept_&lt;/code&gt; 分别具有 &lt;code&gt;(n_classes, n_features)&lt;/code&gt; 和 &lt;code&gt;(n_classes,)&lt;/code&gt; 的形状。系数的每一行对应于 &lt;code&gt;n_classes&lt;/code&gt; &amp;ldquo; one-vs-rest&amp;rdquo;分类器之一，并且按照&amp;ldquo; one&amp;rdquo;类的顺序对应于截距。</target>
        </trans-unit>
        <trans-unit id="41e2c901c13d4daacf7c9adad4d559c077a8e51e" translate="yes" xml:space="preserve">
          <source>For &amp;ldquo;one-vs-rest&amp;rdquo; &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; the attributes &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; have the shape &lt;code&gt;[n_class, n_features]&lt;/code&gt; and &lt;code&gt;[n_class]&lt;/code&gt; respectively. Each row of the coefficients corresponds to one of the &lt;code&gt;n_class&lt;/code&gt; many &amp;ldquo;one-vs-rest&amp;rdquo; classifiers and similar for the intercepts, in the order of the &amp;ldquo;one&amp;rdquo; class.</source>
          <target state="translated">对于&amp;ldquo;一对一休息&amp;rdquo;的&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;，属性 &lt;code&gt;coef_&lt;/code&gt; 和 &lt;code&gt;intercept_&lt;/code&gt; 分别具有 &lt;code&gt;[n_class, n_features]&lt;/code&gt; 和 &lt;code&gt;[n_class]&lt;/code&gt; 的形状。系数的每一行对应于 &lt;code&gt;n_class&lt;/code&gt; 的许多&amp;ldquo; one-vs-rest&amp;rdquo;分类器之一，并且对于截距，按&amp;ldquo; one&amp;rdquo;类的顺序类似。</target>
        </trans-unit>
        <trans-unit id="c6cc35fe8de003f58f84a7c02bbc9d4b652dc227" translate="yes" xml:space="preserve">
          <source>For &amp;ldquo;pairwise&amp;rdquo; metrics, between &lt;em&gt;samples&lt;/em&gt; and not estimators or predictions, see the &lt;a href=&quot;metrics#metrics&quot;&gt;Pairwise metrics, Affinities and Kernels&lt;/a&gt; section.</source>
          <target state="translated">对于&amp;ldquo;成对&amp;rdquo;度量，在&lt;em&gt;样本&lt;/em&gt;之间而不是估计量或预测之间，请参阅&lt;a href=&quot;metrics#metrics&quot;&gt;成对度量，亲和力和内核&lt;/a&gt;部分。</target>
        </trans-unit>
        <trans-unit id="935f9d2961eec1b64a8d3f62208c3a16e0e7ef63" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; of trees (e.g. RandomForest, GBT, ExtraTrees etc) the number of trees and their depth play the most important role. Latency and throughput should scale linearly with the number of trees. In this case we used directly the &lt;code&gt;n_estimators&lt;/code&gt; parameter of &lt;code&gt;sklearn.ensemble.gradient_boosting.GradientBoostingRegressor&lt;/code&gt;.</source>
          <target state="translated">对于树木的&lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt;（例如RandomForest，GBT，ExtraTrees等），树木的数量及其深度起着最重要的作用。延迟和吞吐量应与树的数量成线性比例。在这种情况下，我们直接使用 &lt;code&gt;n_estimators&lt;/code&gt; 的参数 &lt;code&gt;sklearn.ensemble.gradient_boosting.GradientBoostingRegressor&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c2e53d45d0579b4b39658069206cb04a03ac3808" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;classes#module-sklearn.linear_model&quot;&gt;&lt;code&gt;sklearn.linear_model&lt;/code&gt;&lt;/a&gt; (e.g. Lasso, ElasticNet, SGDClassifier/Regressor, Ridge &amp;amp; RidgeClassifier, PassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression&amp;hellip;) the decision function that is applied at prediction time is the same (a dot product) , so latency should be equivalent.</source>
          <target state="translated">对于&lt;a href=&quot;classes#module-sklearn.linear_model&quot;&gt; &lt;code&gt;sklearn.linear_model&lt;/code&gt; &lt;/a&gt;（例如Lasso，ElasticNet，SGDClassifier / Regressor，Ridge＆RidgeClassifier，PassiveAggressiveClassifier / Regressor，LinearSVC，LogisticRegression&amp;hellip;），在预测时应用的决策函数是相同的（点积），因此延迟应相等。</target>
        </trans-unit>
        <trans-unit id="b1a84d28126765870388f5c6f8149674d42fd858" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt;&lt;code&gt;StackingClassifier&lt;/code&gt;&lt;/a&gt;, note that the output of the &lt;code&gt;estimators&lt;/code&gt; is controlled by the parameter &lt;code&gt;stack_method&lt;/code&gt; and it is called by each estimator. This parameter is either a string, being estimator method names, or &lt;code&gt;'auto'&lt;/code&gt; which will automatically identify an available method depending on the availability, tested in the order of preference: &lt;code&gt;predict_proba&lt;/code&gt;, &lt;code&gt;decision_function&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">对于&lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt; &lt;code&gt;StackingClassifier&lt;/code&gt; &lt;/a&gt;，请注意， &lt;code&gt;estimators&lt;/code&gt; 的输出由参数 &lt;code&gt;stack_method&lt;/code&gt; 控制，并且每个估算器都调用它。此参数可以是一个字符串（是估算器方法名称），也可以是 &lt;code&gt;'auto'&lt;/code&gt; ，它会根据可用性自动确定可用方法，并按优先顺序进行测试： &lt;code&gt;predict_proba&lt;/code&gt; ， &lt;code&gt;decision_function&lt;/code&gt; 和 &lt;code&gt;predict&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d5bf64c1d60fc523cf9cf35c71f5018653089c51" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt;&lt;code&gt;StackingClassifier&lt;/code&gt;&lt;/a&gt;, when using &lt;code&gt;stack_method_='predict_proba'&lt;/code&gt;, the first column is dropped when the problem is a binary classification problem. Indeed, both probability columns predicted by each estimator are perfectly collinear.</source>
          <target state="translated">对于&lt;a href=&quot;generated/sklearn.ensemble.stackingclassifier#sklearn.ensemble.StackingClassifier&quot;&gt; &lt;code&gt;StackingClassifier&lt;/code&gt; &lt;/a&gt;，当使用 &lt;code&gt;stack_method_='predict_proba'&lt;/code&gt; 时，如果问题是二进制分类问题，则删除第一列。实际上，每个估计器预测的两个概率列都是完全共线的。</target>
        </trans-unit>
        <trans-unit id="6d0c4acadc1a3e244ad80f139e9b2149c5787665" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; (and &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;) any input passed as a numpy array will be copied and converted to the &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; internal sparse data representation (double precision floats and int32 indices of non-zero components). If you want to fit a large-scale linear classifier without copying a dense numpy C-contiguous double precision array as input, we suggest to use the &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; class instead. The objective function can be configured to be almost the same as the &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; model.</source>
          <target state="translated">对于&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;（和&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;），将复制作为numpy数组传递的任何输入并将其转换为&lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt;内部稀疏数据表示形式（双精度浮点数和非零分量的int32索引）。如果要适合大型线性分类器而不复制密集的numpy C连续双精度数组作为输入，则建议改用&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;类。可以将目标函数配置为与&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;模型几乎相同。</target>
        </trans-unit>
        <trans-unit id="8582a7ae6ed830b76bb2d9fd21363d4d3995f59c" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; (and &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;) any input passed as a numpy array will be copied and converted to the liblinear internal sparse data representation (double precision floats and int32 indices of non-zero components). If you want to fit a large-scale linear classifier without copying a dense numpy C-contiguous double precision array as input we suggest to use the &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; class instead. The objective function can be configured to be almost the same as the &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; model.</source>
          <target state="translated">对于&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;（和&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;），将复制作为numpy数组传递的任何输入并将其转换为liblinear内部稀疏数据表示形式（双精度浮点数和非零分量的int32索引）。如果要适合大型线性分类器而不复制密集的numpy C连续双精度数组作为输入，我们建议改用&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;类。可以将目标函数配置为与&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;模型几乎相同。</target>
        </trans-unit>
        <trans-unit id="2643eb1343d14a8cde9753d0546e2bd56743d7c1" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, the &amp;lsquo;recursion&amp;rsquo; method (used by default) will not account for the &lt;code&gt;init&lt;/code&gt; predictor of the boosting process. In practice, this will produce the same values as &amp;lsquo;brute&amp;rsquo; up to a constant offset in the target response, provided that &lt;code&gt;init&lt;/code&gt; is a constant estimator (which is the default). However, if &lt;code&gt;init&lt;/code&gt; is not a constant estimator, the partial dependence values are incorrect for &amp;lsquo;recursion&amp;rsquo; because the offset will be sample-dependent. It is preferable to use the &amp;lsquo;brute&amp;rsquo; method. Note that this only applies to &lt;a href=&quot;sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, not to &lt;a href=&quot;sklearn.ensemble.histgradientboostingclassifier#sklearn.ensemble.HistGradientBoostingClassifier&quot;&gt;&lt;code&gt;HistGradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt;&lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">对于&lt;a href=&quot;sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;，&amp;ldquo;递归&amp;rdquo;方法（默认情况下使用）将不考虑提升过程的 &lt;code&gt;init&lt;/code&gt; 预测变量。实际上，只要 &lt;code&gt;init&lt;/code&gt; 是一个常数估算器（默认值），它将在目标响应中产生与&amp;ldquo; brute&amp;rdquo;相同的值，直到一个恒定的偏移量。但是，如果 &lt;code&gt;init&lt;/code&gt; 不是常数估计器，则部分相关性值对于&amp;ldquo;递归&amp;rdquo;是不正确的，因为偏移量将取决于样本。最好使用&amp;ldquo;粗略&amp;rdquo;方法。请注意，这仅适用于&lt;a href=&quot;sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;，不适用于&lt;a href=&quot;sklearn.ensemble.histgradientboostingclassifier#sklearn.ensemble.HistGradientBoostingClassifier&quot;&gt; &lt;code&gt;HistGradientBoostingClassifier&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;sklearn.ensemble.histgradientboostingregressor#sklearn.ensemble.HistGradientBoostingRegressor&quot;&gt; &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="f9c95b352fd6ddba1a98d2caa12d747735ff41c3" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;0 &amp;lt; power &amp;lt; 1&lt;/code&gt;, no distribution exists.</source>
          <target state="translated">对于 &lt;code&gt;0 &amp;lt; power &amp;lt; 1&lt;/code&gt; ，不存在分布。</target>
        </trans-unit>
        <trans-unit id="e56c72d645a0047396221ed2eb5407914a9b6bf6" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;make_classification&lt;/code&gt;, three binary and two multi-class classification datasets are generated, with different numbers of informative features and clusters per class.</source>
          <target state="translated">对于 &lt;code&gt;make_classification&lt;/code&gt; ，将生成三个二进制和两个多类分类数据集，每个类具有不同数量的信息特征和聚类。</target>
        </trans-unit>
        <trans-unit id="dda7c631740b122861937f9ebbfdde73fd44b016" translate="yes" xml:space="preserve">
          <source>For Gaussian distributed data, the distance of an observation \(x_i\) to the mode of the distribution can be computed using its Mahalanobis distance: \(d_{(\mu,\Sigma)}(x_i)^2 = (x_i - \mu)'\Sigma^{-1}(x_i - \mu)\) where \(\mu\) and \(\Sigma\) are the location and the covariance of the underlying Gaussian distribution.</source>
          <target state="translated">对于高斯分布的数据,观测值/(x_i/)到分布模式的距离可以用它的马哈兰诺比斯距离来计算。\(d_{(\mu,\Sigma)}(x_i)^2=(x_i-\mu)'\Sigma^{-1}(x_i-\mu)\)其中,\(\mu)和\(\Sigma\)是基本高斯分布的位置和协方差。</target>
        </trans-unit>
        <trans-unit id="3fb903a20f5aad7e20f9123d2edfa2a0638dc6bc" translate="yes" xml:space="preserve">
          <source>For \(k\) clusters, the Calinski-Harabaz score \(s\) is given as the ratio of the between-clusters dispersion mean and the within-cluster dispersion:</source>
          <target state="translated">对于/(k)个聚类,Calinski-Harabaz得分/(s)是以聚类间离散度平均值和聚类内离散度之比给出的。</target>
        </trans-unit>
        <trans-unit id="d27bcc7c91650762beefd01dc3089ec6978d5c87" translate="yes" xml:space="preserve">
          <source>For a classification model, the predicted class for each sample in X is returned. For a regression model, the predicted value based on X is returned.</source>
          <target state="translated">对于分类模型,返回X中每个样本的预测类。对于回归模型,返回基于X的预测值。</target>
        </trans-unit>
        <trans-unit id="e6fd66f776dfd09a091bc857e8c9d10d50ac3ba8" translate="yes" xml:space="preserve">
          <source>For a comparison of the different scalers, transformers, and normalizers, see &lt;a href=&quot;../../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;examples/preprocessing/plot_all_scaling.py&lt;/a&gt;.</source>
          <target state="translated">有关不同缩放器，转换器和规范化器的比较，请参阅&lt;a href=&quot;../../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;examples / preprocessing / plot_all_scaling.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="33f57a2a03940da66a0808ac0a9d7e45bc98afe2" translate="yes" xml:space="preserve">
          <source>For a complete probabilistic model we also need a prior distribution for the latent variable \(h\). The most straightforward assumption (based on the nice properties of the Gaussian distribution) is \(h \sim \mathcal{N}(0, \mathbf{I})\). This yields a Gaussian as the marginal distribution of \(x\):</source>
          <target state="translated">对于一个完整的概率模型,我们还需要一个潜变量的先验分布(h)。最直接的假设(基于高斯分布的良好特性)是(h sim \mathcal{N}(0,\mathbf{I})\)。这就产生了一个高斯分布,作为边际分布的(x)。</target>
        </trans-unit>
        <trans-unit id="7e3b25cfbbacb17bf9ce066c3983b6610e3fab10" translate="yes" xml:space="preserve">
          <source>For a constant learning rate use &lt;code&gt;learning_rate='constant'&lt;/code&gt; and use &lt;code&gt;eta0&lt;/code&gt; to specify the learning rate.</source>
          <target state="translated">对于恒定的学习率，使用 &lt;code&gt;learning_rate='constant'&lt;/code&gt; 并使用 &lt;code&gt;eta0&lt;/code&gt; 指定学习率。</target>
        </trans-unit>
        <trans-unit id="76a9227cf28fa05c9bb19673e15a2774476a035c" translate="yes" xml:space="preserve">
          <source>For a description of the implementation and details of the algorithms used, please refer to</source>
          <target state="translated">关于实施的描述和所使用的算法的细节,请参见</target>
        </trans-unit>
        <trans-unit id="ccaff5036b34bf3986d666eb2f98e4ef943f3dfd" translate="yes" xml:space="preserve">
          <source>For a discussion and comparison of these algorithms, see the &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;manifold module page&lt;/a&gt;</source>
          <target state="translated">有关这些算法的讨论和比较，请参阅&lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;歧管模块页面&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7b7d7f03d534f8340da15e8579a79cb680de77bd" translate="yes" xml:space="preserve">
          <source>For a document generated from multiple topics, all topics are weighted equally in generating its bag of words.</source>
          <target state="translated">对于一个由多个主题生成的文档,在生成它的词袋时,所有主题的权重相同。</target>
        </trans-unit>
        <trans-unit id="b87483db50bfd800e7f61326ccd19592abcc3547" translate="yes" xml:space="preserve">
          <source>For a few of the best biclusters, its most common document categories and its ten most important words get printed. The best biclusters are determined by their normalized cut. The best words are determined by comparing their sums inside and outside the bicluster.</source>
          <target state="translated">对于几个最好的双联体,它最常见的文档类别和它的十个最重要的词得到印证。最佳双簇由其归一化切割决定。最佳词汇是通过比较双簇内外的总和来确定的。</target>
        </trans-unit>
        <trans-unit id="859f5c51d38da3ad244e41ebf28b507a4a99bc62" translate="yes" xml:space="preserve">
          <source>For a full code example that demonstrates using a &lt;a href=&quot;generated/sklearn.preprocessing.functiontransformer#sklearn.preprocessing.FunctionTransformer&quot;&gt;&lt;code&gt;FunctionTransformer&lt;/code&gt;&lt;/a&gt; to do custom feature selection, see &lt;a href=&quot;../auto_examples/preprocessing/plot_function_transformer#sphx-glr-auto-examples-preprocessing-plot-function-transformer-py&quot;&gt;Using FunctionTransformer to select columns&lt;/a&gt;</source>
          <target state="translated">有关演示使用&lt;a href=&quot;generated/sklearn.preprocessing.functiontransformer#sklearn.preprocessing.FunctionTransformer&quot;&gt; &lt;code&gt;FunctionTransformer&lt;/code&gt; &lt;/a&gt;进行自定义功能选择的完整代码示例，请参见&lt;a href=&quot;../auto_examples/preprocessing/plot_function_transformer#sphx-glr-auto-examples-preprocessing-plot-function-transformer-py&quot;&gt;使用FunctionTransformer选择列&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a29c02b5f33160de772eacbd74337a53f6625181" translate="yes" xml:space="preserve">
          <source>For a full-fledged example of out-of-core scaling in a text classification task see &lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core classification of text documents&lt;/a&gt;.</source>
          <target state="translated">有关文本分类任务中核外缩放的完整示例，请参见&lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;文本文档的核外分类&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="d78aafa74d01fdbbdb67982f22aabb8fb92e6131" translate="yes" xml:space="preserve">
          <source>For a given value of &lt;code&gt;n_components&lt;/code&gt;&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; is often less accurate as &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; is cheaper to compute, though, making use of larger feature spaces more efficient.</source>
          <target state="translated">对于一个给定的值 &lt;code&gt;n_components&lt;/code&gt; &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;常常是作为不太精确&lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;。但是，&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; 的&lt;/a&gt;计算成本较低，从而可以更有效地利用较大的特征空间。</target>
        </trans-unit>
        <trans-unit id="46cec00c813e8ea8e2f5bd58264262a28ac481cf" translate="yes" xml:space="preserve">
          <source>For a good choice of alpha, the &lt;a href=&quot;linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; can fully recover the exact set of non-zero variables using only few observations, provided certain specific conditions are met. In particular, the number of samples should be &amp;ldquo;sufficiently large&amp;rdquo;, or L1 models will perform at random, where &amp;ldquo;sufficiently large&amp;rdquo; depends on the number of non-zero coefficients, the logarithm of the number of features, the amount of noise, the smallest absolute value of non-zero coefficients, and the structure of the design matrix X. In addition, the design matrix must display certain specific properties, such as not being too correlated.</source>
          <target state="translated">为了很好地选择alpha ，只要满足某些特定条件，&lt;a href=&quot;linear_model#lasso&quot;&gt;套索&lt;/a&gt;就可以使用很少的观测值完全恢复非零变量的确切集合。特别是，样本数量应为&amp;ldquo;足够大&amp;rdquo;，否则L1模型将随机执行，其中&amp;ldquo;足够大&amp;rdquo;取决于非零系数的数量，特征数量的对数，噪声量，非零系数的最小绝对值，以及设计矩阵X的结构。此外，设计矩阵必须显示某些特定的属性，例如，不要过于相关。</target>
        </trans-unit>
        <trans-unit id="283fe9d87c4a4faac62d4b9cee8a3089a1cb638f" translate="yes" xml:space="preserve">
          <source>For a multi-label classification problem with N classes, N binary classifiers are assigned an integer between 0 and N-1. These integers define the order of models in the chain. Each classifier is then fit on the available training data plus the true labels of the classes whose models were assigned a lower number.</source>
          <target state="translated">对于一个有N个类的多标签分类问题,N个二元分类器被分配一个0到N-1之间的整数。这些整数定义了链中模型的顺序。然后,在现有的训练数据上对每个分类器进行拟合,再加上模型被赋予较低数字的类的真实标签。</target>
        </trans-unit>
        <trans-unit id="73e6ca6403df9166903acb4326e48adf2d2e8f55" translate="yes" xml:space="preserve">
          <source>For a multi_class problem, if multi_class is set to be &amp;ldquo;multinomial&amp;rdquo; the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes.</source>
          <target state="translated">对于multi_class问题，如果将multi_class设置为&amp;ldquo;多项式&amp;rdquo;，则使用softmax函数查找每个类别的预测概率。否则，使用一对多休息法，即使用逻辑函数，假设每个类别为正，则计算它们的概率。并在所有类别中标准化这些值。</target>
        </trans-unit>
        <trans-unit id="642c44d27e63bf2bd3e040832cd67d4f4b97be3d" translate="yes" xml:space="preserve">
          <source>For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.</source>
          <target state="translated">对于多类问题,每个类的超参数都是用在所有折线和类上并行进行onevs-rest得到的最佳分数来计算的。因此,这不是真正的多义损失。</target>
        </trans-unit>
        <trans-unit id="3c102da8b9e1a48c3e9639790d9170902789d884" translate="yes" xml:space="preserve">
          <source>For a new point entering the root, it is merged with the subcluster closest to it and the linear sum, squared sum and the number of samples of that subcluster are updated. This is done recursively till the properties of the leaf node are updated.</source>
          <target state="translated">对于进入根节点的一个新点,将其与最接近它的子簇合并,并更新该子簇的线性和、平方和以及样本数。这样反复进行,直到更新叶子节点的属性。</target>
        </trans-unit>
        <trans-unit id="14657655d860195eca6994d15ac16d17936616a4" translate="yes" xml:space="preserve">
          <source>For a one-class model, +1 or -1 is returned.</source>
          <target state="translated">对于单类模型,返回+1或-1。</target>
        </trans-unit>
        <trans-unit id="49c44661048e1cd2101f0d885d98fc8331d85aee" translate="yes" xml:space="preserve">
          <source>For a set of data \(E\) of size \(n_E\) which has been clustered into \(k\) clusters, the Calinski-Harabasz score \(s\) is defined as the ratio of the between-clusters dispersion mean and the within-cluster dispersion:</source>
          <target state="translated">对于一组大小为(n_E/)的数据聚类为(k)个聚类的数据,Calinski-Harabasz得分(s)被定义为聚类间离散度平均值和聚类内离散度的比率。</target>
        </trans-unit>
        <trans-unit id="d36945b7ba93218440d9a3fb3620ffe9bc7ebec1" translate="yes" xml:space="preserve">
          <source>For a similar example, where the methods are applied to a sphere dataset, see &lt;a href=&quot;plot_manifold_sphere#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py&quot;&gt;Manifold Learning methods on a severed sphere&lt;/a&gt;</source>
          <target state="translated">对于类似的示例（将方法应用于球体数据集的情况），请参见&lt;a href=&quot;plot_manifold_sphere#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py&quot;&gt;切割球体上的流形学习方法。&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="945c76e7faeb6fe6d013381d6851dfb972b0f8ae" translate="yes" xml:space="preserve">
          <source>For a similar example, where the methods are applied to the S-curve dataset, see &lt;a href=&quot;plot_compare_methods#sphx-glr-auto-examples-manifold-plot-compare-methods-py&quot;&gt;Comparison of Manifold Learning methods&lt;/a&gt;</source>
          <target state="translated">对于类似的示例，其中将方法应用于S曲线数据集，请参阅&lt;a href=&quot;plot_compare_methods#sphx-glr-auto-examples-manifold-plot-compare-methods-py&quot;&gt;流形学习方法比较&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5693da1a0a426bf5e6f357791de67cea3dcb709e" translate="yes" xml:space="preserve">
          <source>For an adaptively decreasing learning rate, use &lt;code&gt;learning_rate='adaptive'&lt;/code&gt; and use &lt;code&gt;eta0&lt;/code&gt; to specify the starting learning rate. When the stopping criterion is reached, the learning rate is divided by 5, and the algorithm does not stop. The algorithm stops when the learning rate goes below 1e-6.</source>
          <target state="translated">对于自适应降低的学习率，请使用 &lt;code&gt;learning_rate='adaptive'&lt;/code&gt; 并使用 &lt;code&gt;eta0&lt;/code&gt; 指定起始学习率。当达到停止标准时，学习率将除以5，并且算法不会停止。当学习率低于1e-6时，算法将停止。</target>
        </trans-unit>
        <trans-unit id="289eda38dfb97e5735805aabc918de776eb35066" translate="yes" xml:space="preserve">
          <source>For an estimator to be effective, you need the distance between neighboring points to be less than some value \(d\), which depends on the problem. In one dimension, this requires on average \(n \sim 1/d\) points. In the context of the above \(k\)-NN example, if the data is described by just one feature with values ranging from 0 to 1 and with \(n\) training observations, then new data will be no further away than \(1/n\). Therefore, the nearest neighbor decision rule will be efficient as soon as \(1/n\) is small compared to the scale of between-class feature variations.</source>
          <target state="translated">为了使估算器有效,你需要相邻点之间的距离小于某个值 \(d\),这取决于问题。在一个维度上,这需要平均的点(k/d)。在上面的上下文中,如果数据只用一个特征来描述,其值范围在0到1之间,并且有(n)个训练观测值,那么新数据的距离不会超过(1/n/)。因此,与类间特征变化的规模相比,只要 \(1/n\)小,最近邻决策规则就会有效。</target>
        </trans-unit>
        <trans-unit id="cc92ccd33be99f33389b25ab23e30ca5c4b36d12" translate="yes" xml:space="preserve">
          <source>For an example of using this dataset with scikit-learn, see &lt;a href=&quot;../../auto_examples/applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;examples/applications/plot_species_distribution_modeling.py&lt;/a&gt;.</source>
          <target state="translated">有关将此数据集与scikit-learn一起使用的示例，请参见&lt;a href=&quot;../../auto_examples/applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;examples / applications / plot_species_distribution_modeling.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="84cc57ff6bd3680e44c549367baf76fa37633107" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_affinity_propagation#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py&quot;&gt;examples/cluster/plot_affinity_propagation.py&lt;/a&gt;.</source>
          <target state="translated">有关示例，请参见&lt;a href=&quot;../../auto_examples/cluster/plot_affinity_propagation#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py&quot;&gt;examples / cluster / plot_affinity_propagation.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="550540d863fd72ef27788284d554fe3cf91d4d31" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_dbscan#sphx-glr-auto-examples-cluster-plot-dbscan-py&quot;&gt;examples/cluster/plot_dbscan.py&lt;/a&gt;.</source>
          <target state="translated">有关示例，请参见&lt;a href=&quot;../../auto_examples/cluster/plot_dbscan#sphx-glr-auto-examples-cluster-plot-dbscan-py&quot;&gt;examples / cluster / plot_dbscan.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="5a609c98d64d09ee64c2c225998c2e63797d885b" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_mean_shift#sphx-glr-auto-examples-cluster-plot-mean-shift-py&quot;&gt;examples/cluster/plot_mean_shift.py&lt;/a&gt;.</source>
          <target state="translated">有关示例，请参见&lt;a href=&quot;../../auto_examples/cluster/plot_mean_shift#sphx-glr-auto-examples-cluster-plot-mean-shift-py&quot;&gt;examples / cluster / plot_mean_shift.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0c7aeec7cbc3121a908ff21339ef38d8e3682ea4" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_ard#sphx-glr-auto-examples-linear-model-plot-ard-py&quot;&gt;examples/linear_model/plot_ard.py&lt;/a&gt;.</source>
          <target state="translated">有关示例，请参见&lt;a href=&quot;../../auto_examples/linear_model/plot_ard#sphx-glr-auto-examples-linear-model-plot-ard-py&quot;&gt;examples / linear_model / plot_ard.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="8606b3a4ce46a8dc7d8f3fc386175108176c1a2f" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_bayesian_ridge#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py&quot;&gt;examples/linear_model/plot_bayesian_ridge.py&lt;/a&gt;.</source>
          <target state="translated">有关示例，请参见&lt;a href=&quot;../../auto_examples/linear_model/plot_bayesian_ridge#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py&quot;&gt;examples / linear_model / plot_bayesian_ridge.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="1a253fcf6bd3baedce8e1812aafc9e481fd05853" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_coordinate_descent_path#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py&quot;&gt;examples/linear_model/plot_lasso_coordinate_descent_path.py&lt;/a&gt;.</source>
          <target state="translated">有关示例，请参见&lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_coordinate_descent_path#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py&quot;&gt;examples / linear_model / plot_lasso_coordinate_descent_path.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="dd893fae3c875581ed42afc5493c8a73ae57ea3a" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_model_selection#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py&quot;&gt;examples/linear_model/plot_lasso_model_selection.py&lt;/a&gt;.</source>
          <target state="translated">有关示例，请参见&lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_model_selection#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py&quot;&gt;examples / linear_model / plot_lasso_model_selection.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="7185c2666c1903f0809fab3c9082ec1324c6a9c7" translate="yes" xml:space="preserve">
          <source>For an introduction to Unicode and character encodings in general, see Joel Spolsky&amp;rsquo;s &lt;a href=&quot;http://www.joelonsoftware.com/articles/Unicode.html&quot;&gt;Absolute Minimum Every Software Developer Must Know About Unicode&lt;/a&gt;.</source>
          <target state="translated">有关Unicode和字符编码的一般介绍，请参见Joel Spolsky的&amp;ldquo; &lt;a href=&quot;http://www.joelonsoftware.com/articles/Unicode.html&quot;&gt;每个软件开发人员必须了解的Unicode最低要求&amp;rdquo;&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="28d13aab6e72bcde5f37b4278086b064720820de" translate="yes" xml:space="preserve">
          <source>For an introduction to Unicode and character encodings in general, see Joel Spolsky&amp;rsquo;s &lt;a href=&quot;https://www.joelonsoftware.com/articles/Unicode.html&quot;&gt;Absolute Minimum Every Software Developer Must Know About Unicode&lt;/a&gt;.</source>
          <target state="translated">有关Unicode和字符编码的一般介绍，请参见Joel Spolsky的&amp;ldquo;&lt;a href=&quot;https://www.joelonsoftware.com/articles/Unicode.html&quot;&gt;每个软件开发人员必须了解的Unicode最低要求&amp;rdquo;&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="3a88e794655306a2809e5d03a41b7ab87506c6aa" translate="yes" xml:space="preserve">
          <source>For an one-class model, +1 (inlier) or -1 (outlier) is returned.</source>
          <target state="translated">对于一个单类模型,返回+1(inlier)或-1(outlier)。</target>
        </trans-unit>
        <trans-unit id="6b041f627b95dafb713c53f369d3fb59c9505ee0" translate="yes" xml:space="preserve">
          <source>For an one-class model, +1 or -1 is returned.</source>
          <target state="translated">对于单类模型,返回+1或-1。</target>
        </trans-unit>
        <trans-unit id="90c9667034ee59a28024b8500c3a1c0772f75e0b" translate="yes" xml:space="preserve">
          <source>For an overview of available strategies in scikit-learn, see also the &lt;a href=&quot;computing#scaling-strategies&quot;&gt;out-of-core learning&lt;/a&gt; documentation.</source>
          <target state="translated">有关scikit-learn中可用策略的概述，另请参见&lt;a href=&quot;computing#scaling-strategies&quot;&gt;核心学习&lt;/a&gt;文档。</target>
        </trans-unit>
        <trans-unit id="bd83f9999f935e2f6fce3641b48e5b3393b97a71" translate="yes" xml:space="preserve">
          <source>For binary classification with a true label \(y \in \{0,1\}\) and a probability estimate \(p = \operatorname{Pr}(y = 1)\), the log loss per sample is the negative log-likelihood of the classifier given the true label:</source>
          <target state="translated">对于二元分类,如果有一个真标签(y \in \{0,1\}\)和一个概率估计(p=\operatorname{Pr}(y=1)\),每个样本的对数损失是给定真标签的分类器的负对数似然。</target>
        </trans-unit>
        <trans-unit id="23a4f6b8b8e57d58b02ac23ef6f32b82b6049d45" translate="yes" xml:space="preserve">
          <source>For binary classification, \(f(x)\) passes through the logistic function \(g(z)=1/(1+e^{-z})\) to obtain output values between zero and one. A threshold, set to 0.5, would assign samples of outputs larger or equal 0.5 to the positive class, and the rest to the negative class.</source>
          <target state="translated">对于二元分类,\(f(x)\)通过逻辑函数\(g(z)=1/(1+e^{-z})\)来获得0和1之间的输出值。将阈值设为0.5,则将大于或等于0.5的输出样本归入正级,其余的归入负级。</target>
        </trans-unit>
        <trans-unit id="883efcc2dc17e184b74392564fb44d2a6f9c0bf6" translate="yes" xml:space="preserve">
          <source>For binary problems, we can get counts of true negatives, false positives, false negatives and true positives as follows:</source>
          <target state="translated">对于二元问题,我们可以得到真否定、假阳性、假否定和真阳性的计数如下。</target>
        </trans-unit>
        <trans-unit id="7f954d6786e07c3ef37ff8e0245acb91efbb4b2a" translate="yes" xml:space="preserve">
          <source>For classification with &lt;code&gt;loss='deviance'&lt;/code&gt; the target response is logit(p).</source>
          <target state="translated">对于 &lt;code&gt;loss='deviance'&lt;/code&gt; 的分类，目标响应为logit（p）。</target>
        </trans-unit>
        <trans-unit id="4118e1de638d62fd337275c2f8d28f38f1e40db3" translate="yes" xml:space="preserve">
          <source>For classification with a logistic loss, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">对于具有逻辑损失的分类，具有平均策略的SGD的另一个变体与随机平均梯度（SAG）算法一起提供，可以在&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; 中&lt;/a&gt;用作求解器。</target>
        </trans-unit>
        <trans-unit id="7a750c34f262db1f2c0c844eb9774104416e6f5c" translate="yes" xml:space="preserve">
          <source>For classification you can think of it as the regression score before the link function.</source>
          <target state="translated">对于分类来说,你可以把它看作是链接函数之前的回归分数。</target>
        </trans-unit>
        <trans-unit id="6a3d9b2c887776af95639587c4c76f62b0d1c8f1" translate="yes" xml:space="preserve">
          <source>For classification, &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveclassifier#sklearn.linear_model.PassiveAggressiveClassifier&quot;&gt;&lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt;&lt;/a&gt; can be used with &lt;code&gt;loss='hinge'&lt;/code&gt; (PA-I) or &lt;code&gt;loss='squared_hinge'&lt;/code&gt; (PA-II). For regression, &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveregressor#sklearn.linear_model.PassiveAggressiveRegressor&quot;&gt;&lt;code&gt;PassiveAggressiveRegressor&lt;/code&gt;&lt;/a&gt; can be used with &lt;code&gt;loss='epsilon_insensitive'&lt;/code&gt; (PA-I) or &lt;code&gt;loss='squared_epsilon_insensitive'&lt;/code&gt; (PA-II).</source>
          <target state="translated">对于分类，&lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveclassifier#sklearn.linear_model.PassiveAggressiveClassifier&quot;&gt; &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; &lt;/a&gt;可以与 &lt;code&gt;loss='hinge'&lt;/code&gt; （PA-I）或 &lt;code&gt;loss='squared_hinge'&lt;/code&gt; （PA-II）一起使用。对于回归，&lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveregressor#sklearn.linear_model.PassiveAggressiveRegressor&quot;&gt; &lt;code&gt;PassiveAggressiveRegressor&lt;/code&gt; &lt;/a&gt;可以与 &lt;code&gt;loss='epsilon_insensitive'&lt;/code&gt; （PA-I）或 &lt;code&gt;loss='squared_epsilon_insensitive'&lt;/code&gt; （PA-II）一起使用。</target>
        </trans-unit>
        <trans-unit id="27898fb8346ff80dce087f616900965fd4196842" translate="yes" xml:space="preserve">
          <source>For classification, a somewhat important thing to note is that although a stateless feature extraction routine may be able to cope with new/unseen attributes, the incremental learner itself may be unable to cope with new/unseen targets classes. In this case you have to pass all the possible classes to the first &lt;code&gt;partial_fit&lt;/code&gt; call using the &lt;code&gt;classes=&lt;/code&gt; parameter.</source>
          <target state="translated">对于分类，需要注意的一点重要一点是，尽管无状态特征提取例程可能能够应对新的/看不见的属性，但是增量学习者本身可能无法应对新的/看不见的目标类。在这种情况下，您必须使用 &lt;code&gt;classes=&lt;/code&gt; 参数将所有可能的类传递给第一个 &lt;code&gt;partial_fit&lt;/code&gt; 调用。</target>
        </trans-unit>
        <trans-unit id="fda4c776ec3f2170d3e99301b50afa3144298d48" translate="yes" xml:space="preserve">
          <source>For classification, as in the labeling &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;iris&lt;/a&gt; task, linear regression is not the right approach as it will give too much weight to data far from the decision frontier. A linear approach is to fit a sigmoid function or &lt;strong&gt;logistic&lt;/strong&gt; function:</source>
          <target state="translated">对于分类，如在标记&lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;虹膜&lt;/a&gt;任务中一样，线性回归不是正确的方法，因为它将对远离决策边界的数据赋予过多的权重。线性方法是拟合S型或&lt;strong&gt;Logistic&lt;/strong&gt;函数：</target>
        </trans-unit>
        <trans-unit id="cda870024c615048609a38e867ca66fe1aaab764" translate="yes" xml:space="preserve">
          <source>For classification, the target response may be the probability of a class (the positive class for binary classification), or the decision function.</source>
          <target state="translated">对于分类来说,目标响应可以是一个类的概率(二元分类的正类),或者是决策函数。</target>
        </trans-unit>
        <trans-unit id="049512f622ab4a1900a694aa9ec5fcf56244d47a" translate="yes" xml:space="preserve">
          <source>For classification: &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt;&lt;code&gt;chi2&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.f_classif#sklearn.feature_selection.f_classif&quot;&gt;&lt;code&gt;f_classif&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt;&lt;code&gt;mutual_info_classif&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">对于分类：&lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt; &lt;code&gt;chi2&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.feature_selection.f_classif#sklearn.feature_selection.f_classif&quot;&gt; &lt;code&gt;f_classif&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt; &lt;code&gt;mutual_info_classif&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2b5c234d472222c920c562abfd3ef4ec80e6cfee" translate="yes" xml:space="preserve">
          <source>For comparison, a quantized image using a random codebook (colors picked up randomly) is also shown.</source>
          <target state="translated">为了进行比较,还显示了使用随机代码本(随机拾取的颜色)的量化图像。</target>
        </trans-unit>
        <trans-unit id="3be5878f7d3ebd4495804d5a6a55058ed71eceb1" translate="yes" xml:space="preserve">
          <source>For comparison, the documents are also clustered using MiniBatchKMeans. The document clusters derived from the biclusters achieve a better V-measure than clusters found by MiniBatchKMeans.</source>
          <target state="translated">为了比较,还使用MiniBatchKMeans对文档进行聚类。由双聚类得出的文档聚类比MiniBatchKMeans发现的聚类取得了更好的V-measure。</target>
        </trans-unit>
        <trans-unit id="9da7ddf96abc62edc6f61720c2e84583fe9d6b7c" translate="yes" xml:space="preserve">
          <source>For comparison, we also add the output from &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt;&lt;code&gt;QuantileTransformer&lt;/code&gt;&lt;/a&gt;. It can force any arbitrary distribution into a gaussian, provided that there are enough training samples (thousands). Because it is a non-parametric method, it is harder to interpret than the parametric ones (Box-Cox and Yeo-Johnson).</source>
          <target state="translated">为了进行比较，我们还添加了&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt; &lt;code&gt;QuantileTransformer&lt;/code&gt; &lt;/a&gt;的输出。只要有足够的训练样本（千个），它就可以将任意分布强加给高斯。因为它是非参数方法，所以比参数方法（Box-Cox和Yeo-Johnson）更难解释。</target>
        </trans-unit>
        <trans-unit id="6d421941474530194b10c6be8d7b89e2eb530191" translate="yes" xml:space="preserve">
          <source>For comparison, we also add the output from &lt;code&gt;preprocessing.QuantileTransformer&lt;/code&gt;. It can force any arbitrary distribution into a gaussian, provided that there are enough training samples (thousands). Because it is a non-parametric method, it is harder to interpret than the parametric ones (Box-Cox and Yeo-Johnson).</source>
          <target state="translated">为了进行比较，我们还添加了 &lt;code&gt;preprocessing.QuantileTransformer&lt;/code&gt; 的输出。只要有足够的训练样本（数千个），它就可以将任意分布强加给高斯。因为它是非参数方法，所以比参数方法（Box-Cox和Yeo-Johnson）更难解释。</target>
        </trans-unit>
        <trans-unit id="3fedc899dde91ba75e541f7b8d87d7a3665ca193" translate="yes" xml:space="preserve">
          <source>For compatibility, user code relying on this method should wrap its calls in &lt;code&gt;np.asarray&lt;/code&gt; to avoid type issues.</source>
          <target state="translated">为了兼容，依赖此方法的用户代码应将其调用包装在 &lt;code&gt;np.asarray&lt;/code&gt; 中,以避免类型问题。</target>
        </trans-unit>
        <trans-unit id="6f31aee2196032d492cf3c67f7f43ab3f6981996" translate="yes" xml:space="preserve">
          <source>For continuous parameters, such as &lt;code&gt;C&lt;/code&gt; above, it is important to specify a continuous distribution to take full advantage of the randomization. This way, increasing &lt;code&gt;n_iter&lt;/code&gt; will always lead to a finer search.</source>
          <target state="translated">对于连续参数，例如上面的 &lt;code&gt;C&lt;/code&gt; ，重要的是指定连续分布以充分利用随机化。这样，增加 &lt;code&gt;n_iter&lt;/code&gt; 总是会导致更好的搜索。</target>
        </trans-unit>
        <trans-unit id="4288bdf523218f188616117af5e7ab510c1e81a7" translate="yes" xml:space="preserve">
          <source>For cross-validation, we use 20-fold with 2 algorithms to compute the Lasso path: coordinate descent, as implemented by the LassoCV class, and Lars (least angle regression) as implemented by the LassoLarsCV class. Both algorithms give roughly the same results. They differ with regards to their execution speed and sources of numerical errors.</source>
          <target state="translated">对于交叉验证,我们使用20倍与2种算法来计算Lasso路径:LassoCV类实现的坐标下降和LassoLarsCV类实现的Lars(最小角度回归)。这两种算法给出的结果大致相同。它们的不同之处在于执行速度和数值误差的来源。</target>
        </trans-unit>
        <trans-unit id="c8cbf2457961ac615bed8ee5d0896fee418cd0e6" translate="yes" xml:space="preserve">
          <source>For custom messages if &amp;ldquo;%(name)s&amp;rdquo; is present in the message string, it is substituted for the estimator name.</source>
          <target state="translated">对于自定义消息，如果消息字符串中包含&amp;ldquo;％（name）s&amp;rdquo;，则将其替换为估算器名称。</target>
        </trans-unit>
        <trans-unit id="67e0a596cb4bf62edc844af1488251663768a571" translate="yes" xml:space="preserve">
          <source>For details on the precise mathematical formulation of the provided kernel functions and how &lt;code&gt;gamma&lt;/code&gt;, &lt;code&gt;coef0&lt;/code&gt; and &lt;code&gt;degree&lt;/code&gt; affect each other, see the corresponding section in the narrative documentation: &lt;a href=&quot;../svm#svm-kernels&quot;&gt;Kernel functions&lt;/a&gt;.</source>
          <target state="translated">有关所提供内核函数的精确数学公式以及 &lt;code&gt;gamma&lt;/code&gt; ， &lt;code&gt;coef0&lt;/code&gt; 和 &lt;code&gt;degree&lt;/code&gt; 如何相互影响的详细信息，请参见叙述性文档中的相应部分：&lt;a href=&quot;../svm#svm-kernels&quot;&gt;内核函数&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="885e0e96d0439bc0a3432bfe3535963e5dbd93a5" translate="yes" xml:space="preserve">
          <source>For each class k an array of shape (n_features, n_k), where &lt;code&gt;n_k = min(n_features, number of elements in class k)&lt;/code&gt; It is the rotation of the Gaussian distribution, i.e. its principal axis. It corresponds to &lt;code&gt;V&lt;/code&gt;, the matrix of eigenvectors coming from the SVD of &lt;code&gt;Xk = U S Vt&lt;/code&gt; where &lt;code&gt;Xk&lt;/code&gt; is the centered matrix of samples from class k.</source>
          <target state="translated">对于每个类别k，形状的数组（n_features，n_k），其中 &lt;code&gt;n_k = min(n_features, number of elements in class k)&lt;/code&gt; 它是高斯分布的旋转，即其主轴。它对应于 &lt;code&gt;V&lt;/code&gt; ，即来自 &lt;code&gt;Xk = U S Vt&lt;/code&gt; 的SVD的特征向量矩阵= US Vt，其中 &lt;code&gt;Xk&lt;/code&gt; 是来自k类的样本的居中矩阵。</target>
        </trans-unit>
        <trans-unit id="e4ed0f1e2f8642affc7014ca7518ae7bfac1b31c" translate="yes" xml:space="preserve">
          <source>For each class k an array of shape [n_features, n_k], with &lt;code&gt;n_k = min(n_features, number of elements in class k)&lt;/code&gt; It is the rotation of the Gaussian distribution, i.e. its principal axis.</source>
          <target state="translated">对于每个类别k，形状为[n_features，n_k]的形状的数组，其中 &lt;code&gt;n_k = min(n_features, number of elements in class k)&lt;/code&gt; 这是高斯分布的旋转，即其主轴。</target>
        </trans-unit>
        <trans-unit id="a5ae820e12dddee4457f060f6b705b304ca3a1e3" translate="yes" xml:space="preserve">
          <source>For each class k an array of shape [n_k]. It contains the scaling of the Gaussian distributions along its principal axes, i.e. the variance in the rotated coordinate system.</source>
          <target state="translated">对于每一类k,一个形状[n_k]的数组。它包含了沿其主轴的高斯分布的缩放,即旋转坐标系的方差。</target>
        </trans-unit>
        <trans-unit id="bb1ef71f090300eb5b67a4f2bd338bada7005d30" translate="yes" xml:space="preserve">
          <source>For each class of models we make the model complexity vary through the choice of relevant model parameters and measure the influence on both computational performance (latency) and predictive power (MSE or Hamming Loss).</source>
          <target state="translated">对于每一类模型,我们通过相关模型参数的选择使模型复杂度发生变化,并衡量对计算性能(延迟)和预测能力(MSE或汉明损失)的影响。</target>
        </trans-unit>
        <trans-unit id="21f3c14e6817e5773b09bd85149800940dffa132" translate="yes" xml:space="preserve">
          <source>For each class, contains the scaling of the Gaussian distributions along its principal axes, i.e. the variance in the rotated coordinate system. It corresponds to &lt;code&gt;S^2 /
(n_samples - 1)&lt;/code&gt;, where &lt;code&gt;S&lt;/code&gt; is the diagonal matrix of singular values from the SVD of &lt;code&gt;Xk&lt;/code&gt;, where &lt;code&gt;Xk&lt;/code&gt; is the centered matrix of samples from class k.</source>
          <target state="translated">对于每个类，包含沿其主轴的高斯分布的比例，即旋转坐标系中的方差。它对应于 &lt;code&gt;S^2 / (n_samples - 1)&lt;/code&gt; ，其中 &lt;code&gt;S&lt;/code&gt; 是 &lt;code&gt;Xk&lt;/code&gt; 的SVD的奇异值的对角矩阵，其中 &lt;code&gt;Xk&lt;/code&gt; 是k类的样本的中心矩阵。</target>
        </trans-unit>
        <trans-unit id="93f968263bf15ebdfa5ca6b61b7631f18bb2dedd" translate="yes" xml:space="preserve">
          <source>For each class, gives the covariance matrix estimated using the samples of that class. The estimations are unbiased. Only present if &lt;code&gt;store_covariance&lt;/code&gt; is True.</source>
          <target state="translated">对于每个类别，给出使用该类别的样本估计的协方差矩阵。估计是无偏见的。仅在 &lt;code&gt;store_covariance&lt;/code&gt; 为True时存在。</target>
        </trans-unit>
        <trans-unit id="51af5a1cbe5b213e1b6f97e9040e40d195d22222" translate="yes" xml:space="preserve">
          <source>For each component k, find the weights u, v that maximizes max corr(Xk u, Yk v), such that &lt;code&gt;|u| = |v| = 1&lt;/code&gt;</source>
          <target state="translated">对于每个分量k，找到使max corr（Xk u，Yk v）最大化的权重u，v，使得 &lt;code&gt;|u| = |v| = 1&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="0acc93a412fe0bc296f4de29ad2df21b9415e5fb" translate="yes" xml:space="preserve">
          <source>For each component k, find weights u, v that optimize:</source>
          <target state="translated">对于每个分量k,找到优化的权重u,v。</target>
        </trans-unit>
        <trans-unit id="34ffb7458447c8e84aeb0c0dcae78f73f1c9783e" translate="yes" xml:space="preserve">
          <source>For each component k, find weights u, v that optimizes: &lt;code&gt;max corr(Xk u, Yk v) * std(Xk u) std(Yk u)&lt;/code&gt;, such that &lt;code&gt;|u| = 1&lt;/code&gt;</source>
          <target state="translated">对于每个分量k，找到优化的权重u，v： &lt;code&gt;max corr(Xk u, Yk v) * std(Xk u) std(Yk u)&lt;/code&gt; ，使得 &lt;code&gt;|u| = 1&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f9db939b51ba3d78630796e1c0444915001bc251" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator.</source>
          <target state="translated">对于X中的每一个数据点x和合集中的每一棵树,返回叶子x在每个估计器中的索引。</target>
        </trans-unit>
        <trans-unit id="4571d700205de7840eb7f685393b9c35a449521a" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator. In the case of binary classification n_classes is 1.</source>
          <target state="translated">对于X中的每一个数据点x和合集中的每一棵树,返回叶子x在每个估计器中的索引。在二元分类的情况下,n_classes为1。</target>
        </trans-unit>
        <trans-unit id="44ac20da24a40ac490697d0897d69873261c6900" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in.</source>
          <target state="translated">对于X中的每一个数据点x和森林中的每一棵树,返回x最终所在的叶子的索引。</target>
        </trans-unit>
        <trans-unit id="d82941d49be2d46b8acb0305feded896c81f7a70" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered within &lt;code&gt;[0; self.tree_.node_count)&lt;/code&gt;, possibly with gaps in the numbering.</source>
          <target state="translated">对于X中的每个数据点x，返回以x结尾的叶子的索引。叶子的编号在 &lt;code&gt;[0; self.tree_.node_count)&lt;/code&gt; ，可能在编号上存在间隙。</target>
        </trans-unit>
        <trans-unit id="7b85d3d77de0d2c34f470b25ce92cd73fbf8293c" translate="yes" xml:space="preserve">
          <source>For each dataset, 15% of samples are generated as random uniform noise. This proportion is the value given to the nu parameter of the OneClassSVM and the contamination parameter of the other outlier detection algorithms. Decision boundaries between inliers and outliers are displayed in black except for Local Outlier Factor (LOF) as it has no predict method to be applied on new data when it is used for outlier detection.</source>
          <target state="translated">对于每个数据集,15%的样本被生成为随机均匀噪声。这个比例是给OneClassSVM的nu参数和其他离群检测算法的污染参数的值。除了局部离群值因子(LOF)外,离群值和离群值之间的决策边界用黑色显示,因为它在用于离群值检测时,没有预测方法应用于新数据。</target>
        </trans-unit>
        <trans-unit id="894d665cd020f2e7e68923ae49f3798173d1a959" translate="yes" xml:space="preserve">
          <source>For each document &lt;code&gt;#i&lt;/code&gt;, count the number of occurrences of each word &lt;code&gt;w&lt;/code&gt; and store it in &lt;code&gt;X[i, j]&lt;/code&gt; as the value of feature &lt;code&gt;#j&lt;/code&gt; where &lt;code&gt;j&lt;/code&gt; is the index of word &lt;code&gt;w&lt;/code&gt; in the dictionary.</source>
          <target state="translated">对于每个文档 &lt;code&gt;#i&lt;/code&gt; ，计算每个单词 &lt;code&gt;w&lt;/code&gt; 的出现次数，并将其存储在 &lt;code&gt;X[i, j]&lt;/code&gt; 作为特征 &lt;code&gt;#j&lt;/code&gt; 的值，其中 &lt;code&gt;j&lt;/code&gt; 是词典中单词 &lt;code&gt;w&lt;/code&gt; 的索引。</target>
        </trans-unit>
        <trans-unit id="086df8ffb1b70734dd37b88cb050e6a142a873b8" translate="yes" xml:space="preserve">
          <source>For each document \(d \in D\), draw the topic proportions \(\theta_d \sim \mathrm{Dirichlet}(\alpha)\). \(\alpha\) corresponds to &lt;code&gt;doc_topic_prior&lt;/code&gt;.</source>
          <target state="translated">对于每个文档\（d \ in D \），绘制主题比例\（\ theta_d \ sim \ mathrm {Dirichlet}（\ alpha）\）。\（\ alpha \）对应于 &lt;code&gt;doc_topic_prior&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f8df1081a030b15d2d8afea6cd05ff167080d1cd" translate="yes" xml:space="preserve">
          <source>For each document \(d\), draw \(\theta_d \sim \mathrm{Dirichlet}(\alpha), \: d=1...D\)</source>
          <target state="translated">对于每份文件(d),画出(theta_d)(sim mathrm{Dirichlet}(alpha),d=1...D)。</target>
        </trans-unit>
        <trans-unit id="6eb16a2ee99fe65f3a14878a1546f940e1e0bd2a" translate="yes" xml:space="preserve">
          <source>For each feature \(i\) in the training set \(X\), &lt;a href=&quot;generated/sklearn.naive_bayes.categoricalnb#sklearn.naive_bayes.CategoricalNB&quot;&gt;&lt;code&gt;CategoricalNB&lt;/code&gt;&lt;/a&gt; estimates a categorical distribution for each feature i of X conditioned on the class y. The index set of the samples is defined as \(J = \{ 1, \dots, m \}\), with \(m\) as the number of samples.</source>
          <target state="translated">对于训练集中\（X \）中的每个特征\（i \），&lt;a href=&quot;generated/sklearn.naive_bayes.categoricalnb#sklearn.naive_bayes.CategoricalNB&quot;&gt; &lt;code&gt;CategoricalNB&lt;/code&gt; &lt;/a&gt;估计以类y为条件的X的每个特征i的分类分布。样本的索引集定义为\（J = \ {1，\ dots，m \} \），其中样本数为\（m \）。</target>
        </trans-unit>
        <trans-unit id="9366e353cbe2cfb64854c09c8a8adb8df13a11b1" translate="yes" xml:space="preserve">
          <source>For each feature \(j\) (column of \(D\)):</source>
          <target state="translated">对于每个特征(J)(D(D)一栏):</target>
        </trans-unit>
        <trans-unit id="d88ce97fd881b7b3225357f99ece9e603e7378b5" translate="yes" xml:space="preserve">
          <source>For each observation, tells whether or not (+1 or -1) it should be considered as an inlier according to the fitted model.</source>
          <target state="translated">对于每个观测值,告诉人们根据拟合模型,是否应该将其视为(+1或-1)inlier。</target>
        </trans-unit>
        <trans-unit id="ceeb3b0129a3e252c3947f10f0ffdea6343158bd" translate="yes" xml:space="preserve">
          <source>For each pair of iris features, the decision tree learns decision boundaries made of combinations of simple thresholding rules inferred from the training samples.</source>
          <target state="translated">对于每一对虹膜特征,决策树学习由训练样本推断的简单阈值规则组合而成的决策边界。</target>
        </trans-unit>
        <trans-unit id="63ed3328d4a6b49da02c095fda15b05ff5f99ccf" translate="yes" xml:space="preserve">
          <source>For each repetition \(k\) in \({1, ..., K}\):</source>
          <target state="translated">对于每一次重复({1,...,K}中的(k))。</target>
        </trans-unit>
        <trans-unit id="ebc579ef4368e5d7216210ea27aaa16d7216a1cd" translate="yes" xml:space="preserve">
          <source>For each sample, the generative process is:</source>
          <target state="translated">对于每个样本,生成过程是。</target>
        </trans-unit>
        <trans-unit id="d05dbfe66f302738b6b31b31becd47c7eec37764" translate="yes" xml:space="preserve">
          <source>For each topic \(k \in K\), draw \(\beta_k \sim \mathrm{Dirichlet}(\eta)\). This provides a distribution over the words, i.e. the probability of a word appearing in topic \(k\). \(\eta\) corresponds to &lt;code&gt;topic_word_prior&lt;/code&gt;.</source>
          <target state="translated">对于每个主题\（k \ in K \），绘制\（\ beta_k \ sim \ mathrm {Dirichlet}（\ eta）\）。这提供了单词上的分布，即单词出现在主题\（k \）中的概率。\（\ eta \）对应于 &lt;code&gt;topic_word_prior&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="884540e9625ad90cb4316f6468437987a56adbd4" translate="yes" xml:space="preserve">
          <source>For each topic \(k\), draw \(\beta_k \sim \mathrm{Dirichlet}(\eta),\: k =1...K\)</source>
          <target state="translated">对于每一个主题(k),画出(beta_k)(sim mathrm{Dirichlet}(eta),\:k =1...\ K)</target>
        </trans-unit>
        <trans-unit id="ebc60a8584824b9b236f9d24a5aa9b01b10427f0" translate="yes" xml:space="preserve">
          <source>For each value of &lt;code&gt;n_components&lt;/code&gt;, we plot:</source>
          <target state="translated">对于 &lt;code&gt;n_components&lt;/code&gt; 的每个值，我们绘制：</target>
        </trans-unit>
        <trans-unit id="6ad9736839514923dd21aa4c5541f4da9ec0e497" translate="yes" xml:space="preserve">
          <source>For each value of the &amp;lsquo;target&amp;rsquo; features in the &lt;code&gt;grid&lt;/code&gt; the partial dependence function need to marginalize the predictions of a tree over all possible values of the &amp;lsquo;complement&amp;rsquo; features. In decision trees this function can be evaluated efficiently without reference to the training data. For each grid point a weighted tree traversal is performed: if a split node involves a &amp;lsquo;target&amp;rsquo; feature, the corresponding left or right branch is followed, otherwise both branches are followed, each branch is weighted by the fraction of training samples that entered that branch. Finally, the partial dependence is given by a weighted average of all visited leaves. For tree ensembles the results of each individual tree are again averaged.</source>
          <target state="translated">对于 &lt;code&gt;grid&lt;/code&gt; &amp;ldquo;目标&amp;rdquo;特征的每个值，部分依赖函数需要在&amp;ldquo;互补&amp;rdquo;特征的所有可能值上边缘化树的预测。在决策树中，无需参考训练数据即可有效评估此功能。对于每个网格点，将执行加权树遍历：如果拆分节点涉及&amp;ldquo;目标&amp;rdquo;特征，则遵循相应的左或右分支，否则遵循两个分支，每个分支均按输入的训练样本的分数加权科。最后，偏倚由所有访问过的叶子的加权平均值给出。对于树集合，再次将每棵树的结果平均。</target>
        </trans-unit>
        <trans-unit id="719d4a30c8dd97a24789edd5bdd1f3a14cdc8a6c" translate="yes" xml:space="preserve">
          <source>For each word \(i\) in document \(d\):</source>
          <target state="translated">对于文件中的每一个字(i),(d)。</target>
        </trans-unit>
        <trans-unit id="d13eb916a8aa10457ca38f56195d8da451f22b82" translate="yes" xml:space="preserve">
          <source>For efficiency reasons, the euclidean distance between a pair of row vector x and y is computed as:</source>
          <target state="translated">为了提高效率,一对行向量x和y之间的欧氏距离计算为:。</target>
        </trans-unit>
        <trans-unit id="9aa1f675d2607b44cb2ebebaba9400cb8fdf4c6d" translate="yes" xml:space="preserve">
          <source>For evaluating multiple metrics, either give a list of (unique) strings or a dict with names as keys and callables as values.</source>
          <target state="translated">为了评估多个指标,可以给出一个(唯一的)字符串列表或一个以名字作为键和可调用值的dict。</target>
        </trans-unit>
        <trans-unit id="091a4026feae279bd855844156c1035b747b54da" translate="yes" xml:space="preserve">
          <source>For example &lt;code&gt;average_precision&lt;/code&gt; or the area under the roc curve can not be computed using discrete predictions alone.</source>
          <target state="translated">例如，不能单独使用离散预测来计算 &lt;code&gt;average_precision&lt;/code&gt; 或roc曲线下的面积。</target>
        </trans-unit>
        <trans-unit id="d73a71b2256308d0d5e12341f8e7d64f3e849f98" translate="yes" xml:space="preserve">
          <source>For example try instead of the &lt;code&gt;SVC&lt;/code&gt;:</source>
          <target state="translated">例如，尝试代替 &lt;code&gt;SVC&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="541edf61321b8728dd0c7cafa11d713cadb8fb1e" translate="yes" xml:space="preserve">
          <source>For example, a less computationally intensive alternative to &lt;code&gt;LeavePGroupsOut(p=10)&lt;/code&gt; would be &lt;code&gt;GroupShuffleSplit(test_size=10, n_splits=100)&lt;/code&gt;.</source>
          <target state="translated">例如，可以使用 &lt;code&gt;GroupShuffleSplit(test_size=10, n_splits=100)&lt;/code&gt; 代替 &lt;code&gt;LeavePGroupsOut(p=10)&lt;/code&gt; 来减少计算量。</target>
        </trans-unit>
        <trans-unit id="65cec63d764ed66c423ae4b0636bcfb02973f7ba" translate="yes" xml:space="preserve">
          <source>For example, a simple linear regression can be extended by constructing &lt;strong&gt;polynomial features&lt;/strong&gt; from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data:</source>
          <target state="translated">例如，可以通过从系数构造&lt;strong&gt;多项式特征&lt;/strong&gt;来扩展简单的线性回归。在标准线性回归的情况下，对于二维数据，您可能具有一个类似于以下的模型：</target>
        </trans-unit>
        <trans-unit id="8a4469c2cb53cff6560ff1210e7df829ee4e673a" translate="yes" xml:space="preserve">
          <source>For example, classification of the properties &amp;ldquo;type of fruit&amp;rdquo; and &amp;ldquo;colour&amp;rdquo; for a set of images of fruit. The property &amp;ldquo;type of fruit&amp;rdquo; has the possible classes: &amp;ldquo;apple&amp;rdquo;, &amp;ldquo;pear&amp;rdquo; and &amp;ldquo;orange&amp;rdquo;. The property &amp;ldquo;colour&amp;rdquo; has the possible classes: &amp;ldquo;green&amp;rdquo;, &amp;ldquo;red&amp;rdquo;, &amp;ldquo;yellow&amp;rdquo; and &amp;ldquo;orange&amp;rdquo;. Each sample is an image of a fruit, a label is output for both properties and each label is one of the possible classes of the corresponding property.</source>
          <target state="translated">例如，对一组水果图像的属性&amp;ldquo;水果类型&amp;rdquo;和&amp;ldquo;颜色&amp;rdquo;进行分类。属性&amp;ldquo;水果类型&amp;rdquo;具有可能的类别：&amp;ldquo;苹果&amp;rdquo;，&amp;ldquo;梨&amp;rdquo;和&amp;ldquo;橙色&amp;rdquo;。属性&amp;ldquo;颜色&amp;rdquo;具有可能的类别：&amp;ldquo;绿色&amp;rdquo;，&amp;ldquo;红色&amp;rdquo;，&amp;ldquo;黄色&amp;rdquo;和&amp;ldquo;橙色&amp;rdquo;。每个样本都是水果的图像，两个属性的标签都将输出，每个标签是相应属性的可能类别之一。</target>
        </trans-unit>
        <trans-unit id="794a77a9ad99cd614e0490463df18e6667fa3c9c" translate="yes" xml:space="preserve">
          <source>For example, classification using features extracted from a set of images of fruit, where each image may either be of an orange, an apple, or a pear. Each image is one sample and is labelled as one of the 3 possible classes. Multiclass classification makes the assumption that each sample is assigned to one and only one label - one sample cannot, for example, be both a pear and an apple.</source>
          <target state="translated">例如,使用从一组水果图像中提取的特征进行分类,其中每张图像可能是橙子、苹果或梨的图像。每张图像是一个样本,并被标记为3个可能的类之一。多类分类假设每个样本被分配到一个且仅有一个标签--例如,一个样本不能同时是梨和苹果。</target>
        </trans-unit>
        <trans-unit id="f87725ef6020293ce39f30b54128c30f50570f0e" translate="yes" xml:space="preserve">
          <source>For example, if each point is just a single number (8 bytes), then an effective \(k\)-NN estimator in a paltry \(p \sim 20\) dimensions would require more training data than the current estimated size of the entire internet (&amp;plusmn;1000 Exabytes or so).</source>
          <target state="translated">例如，如果每个点仅是一个数字（8个字节），则在微不足道的\（p \ sim 20 \）中有效的\（k \）-NN估计量将需要比当前估计大小更多的训练数据。整个互联网（&amp;plusmn;1000艾字节左右）。</target>
        </trans-unit>
        <trans-unit id="5e44134c443036a12804aff41c3842c8f74c39ce" translate="yes" xml:space="preserve">
          <source>For example, in random projection, this warning is raised when the number of components, which quantifies the dimensionality of the target projection space, is higher than the number of features, which quantifies the dimensionality of the original source space, to imply that the dimensionality of the problem will not be reduced.</source>
          <target state="translated">例如,在随机投影中,当量化目标投影空间维度的分量数高于量化原始源空间维度的特征数时,就会发出这个警告,以暗示问题的维度不会降低。</target>
        </trans-unit>
        <trans-unit id="4bb7f297d1cf6895bcaff7553e1e2a2edd1164e9" translate="yes" xml:space="preserve">
          <source>For example, in the cases of multiple experiments, &lt;a href=&quot;generated/sklearn.model_selection.leaveonegroupout#sklearn.model_selection.LeaveOneGroupOut&quot;&gt;&lt;code&gt;LeaveOneGroupOut&lt;/code&gt;&lt;/a&gt; can be used to create a cross-validation based on the different experiments: we create a training set using the samples of all the experiments except one:</source>
          <target state="translated">例如，在多个实验的情况下，&lt;a href=&quot;generated/sklearn.model_selection.leaveonegroupout#sklearn.model_selection.LeaveOneGroupOut&quot;&gt; &lt;code&gt;LeaveOneGroupOut&lt;/code&gt; &lt;/a&gt;可用于基于不同的实验创建交叉验证：我们使用除一个实验之外的所有实验样本创建训练集：</target>
        </trans-unit>
        <trans-unit id="9dc98c27045ddaa13bc1efc30f0c70951a11ebf1" translate="yes" xml:space="preserve">
          <source>For example, let&amp;rsquo;s look at the results of a multinomial Naive Bayes classifier, which is fast to train and achieves a decent F-score:</source>
          <target state="translated">例如，让我们看一下多项朴素贝叶斯分类器的结果，该分类器可以快速训练并获得不错的F分数：</target>
        </trans-unit>
        <trans-unit id="85d6aa34dc31a086da5a0b5a46fa6367e968ccaf" translate="yes" xml:space="preserve">
          <source>For example, let&amp;rsquo;s say we&amp;rsquo;re dealing with a corpus of two documents: &lt;code&gt;['words', 'wprds']&lt;/code&gt;. The second document contains a misspelling of the word &amp;lsquo;words&amp;rsquo;. A simple bag of words representation would consider these two as very distinct documents, differing in both of the two possible features. A character 2-gram representation, however, would find the documents matching in 4 out of 8 features, which may help the preferred classifier decide better:</source>
          <target state="translated">例如，假设我们正在处理两个文档的主体： &lt;code&gt;['words', 'wprds']&lt;/code&gt; 。第二份文档包含单词&amp;ldquo; words&amp;rdquo;的拼写错误。一个简单的单词表示袋会将这两个文档视为非常不同的文档，两个可能的特征都不同。但是，字符2克表示法会在8个功能中的4个中找到匹配的文档，这可能有助于首选分类器更好地做出决定：</target>
        </trans-unit>
        <trans-unit id="a387381a97998b7238fd2bc704165170ce740115" translate="yes" xml:space="preserve">
          <source>For example, prediction of both wind speed and wind direction, in degrees, using data obtained at a certain location. Each sample would be data obtained at one location and both wind speed and direction would be output for each sample.</source>
          <target state="translated">例如,利用在某一地点获得的数据预测风速和风向,单位为度。每个样本都是在一个地点获得的数据,每个样本将输出风速和风向。</target>
        </trans-unit>
        <trans-unit id="0256c851cb28fec27d5dcefe2d74c45aece1b249" translate="yes" xml:space="preserve">
          <source>For example, prediction of the topics relevant to a text document or video. The document or video may be about one of &amp;lsquo;religion&amp;rsquo;, &amp;lsquo;politics&amp;rsquo;, &amp;lsquo;finance&amp;rsquo; or &amp;lsquo;education&amp;rsquo;, several of the topic classes or all of the topic classes.</source>
          <target state="translated">例如，预测与文本文档或视频相关的主题。该文档或视频可以是关于&amp;ldquo;宗教&amp;rdquo;，&amp;ldquo;政治&amp;rdquo;，&amp;ldquo;财务&amp;rdquo;或&amp;ldquo;教育&amp;rdquo;，几个主题类别或所有主题类别中的一种。</target>
        </trans-unit>
        <trans-unit id="a34fc3c98b3c662bed7829df5d3e68b291f14f22" translate="yes" xml:space="preserve">
          <source>For example, suppose that we have a first algorithm that extracts Part of Speech (PoS) tags that we want to use as complementary tags for training a sequence classifier (e.g. a chunker). The following dict could be such a window of features extracted around the word &amp;lsquo;sat&amp;rsquo; in the sentence &amp;lsquo;The cat sat on the mat.&amp;rsquo;:</source>
          <target state="translated">例如，假设我们有第一个算法提取词性（PoS）标签，我们希望将其用作训练序列分类器（例如，分块器）的补充标签。以下命令可能是这样的窗口：在&amp;ldquo;猫坐在垫子上&amp;rdquo;句子中的&amp;ldquo; sat&amp;rdquo;一词周围提取特征：</target>
        </trans-unit>
        <trans-unit id="5b46799167b8bb61c43e949f07a2335c6f7cd1fa" translate="yes" xml:space="preserve">
          <source>For example, the distance between &lt;code&gt;[3, na, na, 6]&lt;/code&gt; and &lt;code&gt;[1, na, 4, 5]&lt;/code&gt; is:</source>
          <target state="translated">例如， &lt;code&gt;[3, na, na, 6]&lt;/code&gt; 与 &lt;code&gt;[1, na, 4, 5]&lt;/code&gt; ]之间的距离为：</target>
        </trans-unit>
        <trans-unit id="875d6b4f0ebd92b4525ff9ff007e35a4ef09b992" translate="yes" xml:space="preserve">
          <source>For example, the following snippet uses &lt;code&gt;chardet&lt;/code&gt; (not shipped with scikit-learn, must be installed separately) to figure out the encoding of three texts. It then vectorizes the texts and prints the learned vocabulary. The output is not shown here.</source>
          <target state="translated">例如，以下代码段使用 &lt;code&gt;chardet&lt;/code&gt; （scikit-learn不附带，必须单独安装）确定三个文本的编码。然后对文本进行矢量化处理，并打印学习到的词汇。输出未在此处显示。</target>
        </trans-unit>
        <trans-unit id="a5b55b26c17fcbb577abf03053fdea355f9d1a85" translate="yes" xml:space="preserve">
          <source>For example, this warning may occur when the user</source>
          <target state="translated">例如,当用户在以下情况下可能会出现这个警告</target>
        </trans-unit>
        <trans-unit id="fa61683485e98ae724ab66c0cc502f9a28b6f341" translate="yes" xml:space="preserve">
          <source>For example, to download a dataset of gene expressions in mice brains:</source>
          <target state="translated">例如,要下载小鼠大脑的基因表达数据集。</target>
        </trans-unit>
        <trans-unit id="0fb165a7680e316154f88ea288da16adb651003b" translate="yes" xml:space="preserve">
          <source>For example, to use &lt;code&gt;n_jobs&lt;/code&gt; greater than 1 in the example below, &lt;code&gt;custom_scoring_function&lt;/code&gt; function is saved in a user-created module (&lt;code&gt;custom_scorer_module.py&lt;/code&gt;) and imported:</source>
          <target state="translated">例如，为了使用 &lt;code&gt;n_jobs&lt;/code&gt; 大于1在下面的例子中， &lt;code&gt;custom_scoring_function&lt;/code&gt; 功能被保存用户创建的模块（在 &lt;code&gt;custom_scorer_module.py&lt;/code&gt; ）和进口：</target>
        </trans-unit>
        <trans-unit id="bea8752fb35944e93422e8c1c3a159c63e531901" translate="yes" xml:space="preserve">
          <source>For example, we can compute the tf-idf of the first term in the first document in the &lt;code&gt;counts&lt;/code&gt; array as follows:</source>
          <target state="translated">例如，我们可以如下计算 &lt;code&gt;counts&lt;/code&gt; 数组中第一个文档中第一个词的tf-idf ：</target>
        </trans-unit>
        <trans-unit id="8e5e851ba9e40a81086c0f41a19109e3eeaaee54" translate="yes" xml:space="preserve">
          <source>For example, when dealing with boolean features, \(x_i^n = x_i\) for all \(n\) and is therefore useless; but \(x_i x_j\) represents the conjunction of two booleans. This way, we can solve the XOR problem with a linear classifier:</source>
          <target state="translated">例如,在处理布尔特征时,对于所有的(n/),\(x_i^n=x_i/\),因此是无用的;但\(x_i x_j/\)代表两个布尔的结合。这样,我们就可以用线性分类器解决XOR问题。</target>
        </trans-unit>
        <trans-unit id="dfe2d757676022996b9aa748350ec295d5357a7e" translate="yes" xml:space="preserve">
          <source>For example, when using a validation set, set the &lt;code&gt;test_fold&lt;/code&gt; to 0 for all samples that are part of the validation set, and to -1 for all other samples.</source>
          <target state="translated">例如，使用验证集时，将属于验证集的所有样本的 &lt;code&gt;test_fold&lt;/code&gt; 设置为0，将所有其他样本的test_fold设置为-1。</target>
        </trans-unit>
        <trans-unit id="98a47ab600b3d6adb5169d4d316e6fcca6b662b8" translate="yes" xml:space="preserve">
          <source>For examples on how it is to be used refer to the sections below.</source>
          <target state="translated">关于如何使用的例子,请参考以下章节。</target>
        </trans-unit>
        <trans-unit id="717701439dd44ea572c3fc98c89180aa00806795" translate="yes" xml:space="preserve">
          <source>For further details on bias-variance decomposition, see section 7.3 of &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;.</source>
          <target state="translated">有关偏差方差分解的更多详细信息，请参见第&lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt;节的7.3 。</target>
        </trans-unit>
        <trans-unit id="1fd7dfc113fdc87e0b1f446fd7d77e9da35e9457" translate="yes" xml:space="preserve">
          <source>For further details on bias-variance decomposition, see section 7.3 of &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">有关偏差方差分解的更多详细信息，请参见&lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]的&lt;/a&gt; 7.3节。</target>
        </trans-unit>
        <trans-unit id="2f1f137cd461ac2e31c6cc087610e0dfea901ed1" translate="yes" xml:space="preserve">
          <source>For further details, &amp;ldquo;How to Use t-SNE Effectively&amp;rdquo; &lt;a href=&quot;http://distill.pub/2016/misread-tsne/&quot;&gt;http://distill.pub/2016/misread-tsne/&lt;/a&gt; provides a good discussion of the effects of various parameters, as well as interactive plots to explore those effects.</source>
          <target state="translated">有关更多详细信息，&amp;ldquo;如何有效使用t-SNE&amp;rdquo; &lt;a href=&quot;http://distill.pub/2016/misread-tsne/&quot;&gt;http://distill.pub/2016/misread-tsne/&lt;/a&gt;提供了有关各种参数效果的良好讨论，并提供了交互式图来探索这些效果。</target>
        </trans-unit>
        <trans-unit id="0482a5c448a6c0f244b48ce337c6748a1bc4ca45" translate="yes" xml:space="preserve">
          <source>For further details, &amp;ldquo;How to Use t-SNE Effectively&amp;rdquo; &lt;a href=&quot;https://distill.pub/2016/misread-tsne/&quot;&gt;https://distill.pub/2016/misread-tsne/&lt;/a&gt; provides a good discussion of the effects of various parameters, as well as interactive plots to explore those effects.</source>
          <target state="translated">有关更多详细信息，&amp;ldquo;如何有效使用t-SNE&amp;rdquo; &lt;a href=&quot;https://distill.pub/2016/misread-tsne/&quot;&gt;https://distill.pub/2016/misread-tsne/&lt;/a&gt;提供了有关各种参数效果的很好讨论，并提供了交互式图来探索这些效果。</target>
        </trans-unit>
        <trans-unit id="0f1bbe6e8000be72ab1e63dd45896ee0e4b6eb7a" translate="yes" xml:space="preserve">
          <source>For greyscale image data where pixel values can be interpreted as degrees of blackness on a white background, like handwritten digit recognition, the Bernoulli Restricted Boltzmann machine model (&lt;a href=&quot;../../modules/generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt;) can perform effective non-linear feature extraction.</source>
          <target state="translated">对于像手写数字识别这样的像素值可以解释为白色背景上的黑度的灰度图像数据，Bernoulli Restricted Boltzmann机器模型（&lt;a href=&quot;../../modules/generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; &lt;/a&gt;）可以执行有效的非线性特征提取。</target>
        </trans-unit>
        <trans-unit id="0f8240ee0365c34e0de439585e58b6dfcdcc2ed3" translate="yes" xml:space="preserve">
          <source>For high-dimensional datasets with many collinear features, &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt; is most often preferable. However, &lt;a href=&quot;generated/sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV&quot;&gt;&lt;code&gt;LassoLarsCV&lt;/code&gt;&lt;/a&gt; has the advantage of exploring more relevant values of &lt;code&gt;alpha&lt;/code&gt; parameter, and if the number of samples is very small compared to the number of features, it is often faster than &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">对于具有许多共线特征的高维数据集，&lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt; &lt;code&gt;LassoCV&lt;/code&gt; &lt;/a&gt;通常是首选。但是，&lt;a href=&quot;generated/sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV&quot;&gt; &lt;code&gt;LassoLarsCV&lt;/code&gt; &lt;/a&gt;的优点是可以探索更相关的 &lt;code&gt;alpha&lt;/code&gt; 参数值，并且如果与特征数量相比样本数量非常少，则通常比&lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt; &lt;code&gt;LassoCV&lt;/code&gt; &lt;/a&gt;更快。</target>
        </trans-unit>
        <trans-unit id="866314f9db098f25a642d6cb6f4a133adac68ce1" translate="yes" xml:space="preserve">
          <source>For high-dimensional datasets with many collinear regressors, &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt; is most often preferable. However, &lt;a href=&quot;generated/sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV&quot;&gt;&lt;code&gt;LassoLarsCV&lt;/code&gt;&lt;/a&gt; has the advantage of exploring more relevant values of &lt;code&gt;alpha&lt;/code&gt; parameter, and if the number of samples is very small compared to the number of features, it is often faster than &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">对于具有许多共线性回归的高维数据集，&lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt; &lt;code&gt;LassoCV&lt;/code&gt; &lt;/a&gt;最常用。但是，&lt;a href=&quot;generated/sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV&quot;&gt; &lt;code&gt;LassoLarsCV&lt;/code&gt; &lt;/a&gt;的优势是可以探索更相关的 &lt;code&gt;alpha&lt;/code&gt; 参数值，并且如果与特征数量相比样本数量非常少，则通常比&lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt; &lt;code&gt;LassoCV&lt;/code&gt; &lt;/a&gt;更快。</target>
        </trans-unit>
        <trans-unit id="aff7e1aca1f3054316321a609c5b24bbfe0a02a6" translate="yes" xml:space="preserve">
          <source>For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994.</source>
          <target state="translated">关于NIST预处理程序的信息,见M.D.Garris,J.L.Blue,G.T.Candela,D.L.Dimmick,J.Geist,P.J.Grother,S.A.Janet,and C.L.Wilson,NIST Form-Based Handprint Recognition System,NISTIR 5469,1994。</target>
        </trans-unit>
        <trans-unit id="ee57e485cfe4c61d12541e3ea6e169aab79014e8" translate="yes" xml:space="preserve">
          <source>For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.</source>
          <target state="translated">例如,一个由10,000个短文文档(如电子邮件)组成的集合将使用一个总大小为100,000个独特单词的词汇表,而每个文档将单独使用100到1000个独特单词。</target>
        </trans-unit>
        <trans-unit id="4e9f2f3fee78ca296cddfe5ea5b4e060f79a0a4e" translate="yes" xml:space="preserve">
          <source>For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.</source>
          <target state="translated">例如学习算法的目标函数中使用的许多元素(如支持向量机的RBF核或线性模型的L1和L2正则化器)都假设所有特征都以0为中心,并且具有相同顺序的方差。如果一个特征的方差比其他特征的方差大一个数量级,它可能会主导目标函数,使估计器无法像预期的那样从其他特征中正确学习。</target>
        </trans-unit>
        <trans-unit id="9c54ff6618aa4505fc044efee7a1b7aa2d457afd" translate="yes" xml:space="preserve">
          <source>For instance the below given table</source>
          <target state="translated">例如下面的表格</target>
        </trans-unit>
        <trans-unit id="61fc71afaf6946d5d1cad0702c1f4030326fcb83" translate="yes" xml:space="preserve">
          <source>For instance the groups could be the year of collection of the samples and thus allow for cross-validation against time-based splits.</source>
          <target state="translated">例如,各组可以是收集样本的年份,从而允许对基于时间的分割进行交叉验证。</target>
        </trans-unit>
        <trans-unit id="c007161505dacd37b43b63ce0c84bfbd3ce25160" translate="yes" xml:space="preserve">
          <source>For instance, assuming that the inlier data are Gaussian distributed, it will estimate the inlier location and covariance in a robust way (i.e. without being influenced by outliers). The Mahalanobis distances obtained from this estimate is used to derive a measure of outlyingness. This strategy is illustrated below.</source>
          <target state="translated">例如,假设离群数据是高斯分布的,它将以一种稳健的方式(即不受离群值的影响)估计离群位置和协方差。从这个估计中得到的马哈兰诺比斯距离被用来推导出离群值的测量。这一策略在下文中进行了说明。</target>
        </trans-unit>
        <trans-unit id="7519828cefe279ed0b1f593fd20db7243c914832" translate="yes" xml:space="preserve">
          <source>For instance, given a matrix of shape &lt;code&gt;(10, 10)&lt;/code&gt;, one possible bicluster with three rows and two columns induces a submatrix of shape &lt;code&gt;(3, 2)&lt;/code&gt;:</source>
          <target state="translated">例如，给定一个形状为 &lt;code&gt;(10, 10)&lt;/code&gt; 的矩阵，一个具有三行两列的可能的双曲线会诱导一个形状为 &lt;code&gt;(3, 2)&lt;/code&gt; 的子矩阵：</target>
        </trans-unit>
        <trans-unit id="66b8e52235d720becae09b00e19696b279a13527" translate="yes" xml:space="preserve">
          <source>For instance, if \(p\) singular vectors were calculated, the \(q\) best are found as described, where \(q&amp;lt;p\). Let \(U\) be the matrix with columns the \(q\) best left singular vectors, and similarly \(V\) for the right. To partition the rows, the rows of \(A\) are projected to a \(q\) dimensional space: \(A * V\). Treating the \(m\) rows of this \(m \times q\) matrix as samples and clustering using k-means yields the row labels. Similarly, projecting the columns to \(A^{\top} * U\) and clustering this \(n \times q\) matrix yields the column labels.</source>
          <target state="translated">例如，如果计算了\（p \）奇异矢量，则按照描述找到最佳的\（q \），其中\（q &amp;lt;p \）。令\（U \）为具有（\ q \）最佳左奇异向量，右为\（V \）列的矩阵。为了对行进行分区，\（A \）的行被投影到\（q \）维空间：\（A * V \）。将此\（m \ times q \）矩阵的\（m \）行视为样本，并使用k-means进行聚类，得到行标签。同样，将列投影到\（A ^ {\ top} * U \）并对该\（n \ times q \）矩阵进行聚类将产生列标签。</target>
        </trans-unit>
        <trans-unit id="040e034a022032a75dc3b85f4ce9de7cff88a6fc" translate="yes" xml:space="preserve">
          <source>For instance, if we work with 64x64 pixel gray-level pictures for face recognition, the dimensionality of the data is 4096 and it is slow to train an RBF support vector machine on such wide data. Furthermore we know that the intrinsic dimensionality of the data is much lower than 4096 since all pictures of human faces look somewhat alike. The samples lie on a manifold of much lower dimension (say around 200 for instance). The PCA algorithm can be used to linearly transform the data while both reducing the dimensionality and preserve most of the explained variance at the same time.</source>
          <target state="translated">例如,如果我们用64x64像素的灰度级图片来进行人脸识别,数据的维度是4096,在这么宽的数据上训练RBF支持向量机是很慢的。另外我们知道数据的内在维度要比4096低得多,因为所有的人脸图片看起来都有些相似。样本位于一个维度更低的歧管上(比如说200左右)。PCA算法可以用来对数据进行线性变换,同时既降低了维度,又保留了大部分的解释方差。</target>
        </trans-unit>
        <trans-unit id="f676ab37a8d5ec2f850de1fcd3ee779d6ce55a52" translate="yes" xml:space="preserve">
          <source>For instance, in the case of the digits dataset, &lt;code&gt;digits.data&lt;/code&gt; gives access to the features that can be used to classify the digits samples:</source>
          <target state="translated">例如，对于digits数据集， &lt;code&gt;digits.data&lt;/code&gt; 可以访问可用于对digits样本进行分类的功能：</target>
        </trans-unit>
        <trans-unit id="0b72faa109feccaf14265d5a54672e188bc4b73a" translate="yes" xml:space="preserve">
          <source>For instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.</source>
          <target state="translated">例如,在下面的例子中,决策树从数据中学习,用一组if-then-else决策规则来逼近一条正弦曲线。树越深,决策规则越复杂,模型越适合。</target>
        </trans-unit>
        <trans-unit id="a42ba327206d2f6a371d1c7b16518c8946340f21" translate="yes" xml:space="preserve">
          <source>For instance, let&amp;rsquo;s compare the two predictions 1.0 and 100 that are both 50% of their corresponding true value.</source>
          <target state="translated">例如，让我们比较两个预测1.0和100，它们都是对应的真实值的50％。</target>
        </trans-unit>
        <trans-unit id="bb4b6181584be99d136bb8fd3d82dd09da37eec0" translate="yes" xml:space="preserve">
          <source>For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.</source>
          <target state="translated">例如,学习算法的目标函数中使用的许多元素(如支持向量机的RBF核或线性模型的l1和l2正则化器)假设所有特征都以零为中心,并且具有相同顺序的方差。如果一个特征的方差比其他特征的方差大一个数量级,它可能会主导目标函数,使估计器不能像预期的那样从其他特征中正确学习。</target>
        </trans-unit>
        <trans-unit id="2500a85d8a54066d44afc291418c2bdbdb2d8331" translate="yes" xml:space="preserve">
          <source>For instance, the following shows 16 sample portraits (centered around 0.0) from the Olivetti dataset. On the right hand side are the first 16 singular vectors reshaped as portraits. Since we only require the top 16 singular vectors of a dataset with size \(n_{samples} = 400\) and \(n_{features} = 64 \times 64 = 4096\), the computation time is less than 1s:</source>
          <target state="translated">例如,下面显示了来自Olivetti数据集的16个样本肖像(以0.0为中心)。右手边是前16个奇异向量被重塑为肖像画。由于我们只需要一个数据集的前16个奇异向量,其大小为 \(n_{samples}=400\)和 \(n_{features}=64 \times 64=4096\),计算时间小于1s。</target>
        </trans-unit>
        <trans-unit id="8f189274df939cb66095eb188cc3a399c86cb294" translate="yes" xml:space="preserve">
          <source>For instance, we can perform a \(\chi^2\) test to the samples to retrieve only the two best features as follows:</source>
          <target state="translated">例如,我们可以对样本进行如下测试,只检索出两个最好的特征。</target>
        </trans-unit>
        <trans-unit id="abc897209b2f98b7966665fa36a5eddbbc44f66d" translate="yes" xml:space="preserve">
          <source>For instance:</source>
          <target state="translated">例如:</target>
        </trans-unit>
        <trans-unit id="c017c696c4eba476debcc2637355a4ef09f7c0a3" translate="yes" xml:space="preserve">
          <source>For int/None inputs, &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="translated">对于int / None输入，使用 &lt;code&gt;KFold&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="560fc78b966a9c766c4d5505bc2466cc24122eb7" translate="yes" xml:space="preserve">
          <source>For int/None inputs, if the estimator is a classifier and &lt;code&gt;y&lt;/code&gt; is either binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. In all other cases, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">对于int / None输入，如果估计量是分类器，并且 &lt;code&gt;y&lt;/code&gt; 是二进制或多类，则将使用&lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt;。在所有其他情况下，都使用&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="21205df9d4ba13a75af14823666b84f64bd04084" translate="yes" xml:space="preserve">
          <source>For integer/None inputs &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="translated">对于整数/无输入，使用 &lt;code&gt;KFold&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f4cdf9352c6e062816193041b97f5514c42b421e" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="translated">对于整数/无输入，使用 &lt;code&gt;KFold&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f206c091dc56a7e693c1c1efe6b0899c57cec04a" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used, else, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">对于整数/无输入，如果 &lt;code&gt;y&lt;/code&gt; 是二进制或多类，则使用&lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt;，否则使用&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="84373ac49af10a751441a8470e060e3de62490b1" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. If &lt;code&gt;y&lt;/code&gt; is neither binary nor multiclass, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">对于整数/无输入，如果 &lt;code&gt;y&lt;/code&gt; 是二进制或多类，则使用&lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt;。如果 &lt;code&gt;y&lt;/code&gt; 既不是二进制也不是多类，则使用&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="8d4fea32021fed22e35126e0e01d0c620369dc78" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. If the estimator is a classifier or if &lt;code&gt;y&lt;/code&gt; is neither binary nor multiclass, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">对于整数/无输入，如果 &lt;code&gt;y&lt;/code&gt; 是二进制或多类，则使用&lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt;。如果估计器是分类器，或者 &lt;code&gt;y&lt;/code&gt; 既不是二进制也不是多类，则使用&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="68ad85210cd514ab63c161f2689020aa738ee186" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if classifier is True and &lt;code&gt;y&lt;/code&gt; is either binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. In all other cases, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">对于整数/无输入，如果分类器为True， &lt;code&gt;y&lt;/code&gt; 为二进制或多类，则使用&lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt;。在所有其他情况下，都使用&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="821cadb32f750528bd31875526972b81201437b9" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if the estimator is a classifier and &lt;code&gt;y&lt;/code&gt; is either binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. In all other cases, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">对于整数/无输入，如果估计量是分类器， &lt;code&gt;y&lt;/code&gt; 是二进制或多类，则使用&lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt;。在所有其他情况下，都使用&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="ec46cd7e35a119deeb6479ced2aa91071495e898" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, &lt;code&gt;StratifiedKFold&lt;/code&gt; is used. In all other cases, &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="translated">对于整数/无输入，如果估计量是分类器，y是二进制或多类，则使用 &lt;code&gt;StratifiedKFold&lt;/code&gt; 。在所有其他情况下，都使用 &lt;code&gt;KFold&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="cfc9bcb00c8530f8c99a68584e1330a4da8fc56a" translate="yes" xml:space="preserve">
          <source>For intermediate values, we can see on the second plot that good models can be found on a diagonal of &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt;. Smooth models (lower &lt;code&gt;gamma&lt;/code&gt; values) can be made more complex by increasing the importance of classifying each point correctly (larger &lt;code&gt;C&lt;/code&gt; values) hence the diagonal of good performing models.</source>
          <target state="translated">对于中间值，我们可以在第二个图上看到可以在 &lt;code&gt;C&lt;/code&gt; 和 &lt;code&gt;gamma&lt;/code&gt; 的对角线上找到好的模型。通过增加正确分类每个点的重要性（较大的 &lt;code&gt;C&lt;/code&gt; 值），可以提高平滑模型（较低的 &lt;code&gt;gamma&lt;/code&gt; 值）的复杂度，从而提高对等性能模型的对角线。</target>
        </trans-unit>
        <trans-unit id="eb96f16ecd15a6be088f1dc93fa28ca4ca7ecca5" translate="yes" xml:space="preserve">
          <source>For kernel=&amp;rdquo;precomputed&amp;rdquo;, the expected shape of X is (n_samples_test, n_samples_train).</source>
          <target state="translated">对于内核=&amp;ldquo;预先计算&amp;rdquo;，X的预期形状为（n_samples_test，n_samples_train）。</target>
        </trans-unit>
        <trans-unit id="93c16e02e4641d6fe7bdb8a83439c237817b53cd" translate="yes" xml:space="preserve">
          <source>For kernel=&amp;rdquo;precomputed&amp;rdquo;, the expected shape of X is [n_samples_test, n_samples_train]</source>
          <target state="translated">对于内核=&amp;ldquo;预先计算&amp;rdquo;，X的预期形状为[n_samples_test，n_samples_train]</target>
        </trans-unit>
        <trans-unit id="9cb299cfc771ccbc3a241c16ffeed37fabbcff7c" translate="yes" xml:space="preserve">
          <source>For large dataset, you may also consider using &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; with &amp;lsquo;log&amp;rsquo; loss.</source>
          <target state="translated">对于大型数据集，您也可以考虑使用&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;并避免 'log'丢失。</target>
        </trans-unit>
        <trans-unit id="3b24421d17b9758d0c0e99f847aff320121bf350" translate="yes" xml:space="preserve">
          <source>For large datasets, similar (but not identical) results can be obtained via &lt;a href=&quot;https://hdbscan.readthedocs.io&quot;&gt;HDBSCAN&lt;/a&gt;. The HDBSCAN implementation is multithreaded, and has better algorithmic runtime complexity than OPTICS, at the cost of worse memory scaling. For extremely large datasets that exhaust system memory using HDBSCAN, OPTICS will maintain &lt;em&gt;n&lt;/em&gt; (as opposed to &lt;em&gt;n^2&lt;/em&gt;) memory scaling; however, tuning of the &lt;code&gt;max_eps&lt;/code&gt; parameter will likely need to be used to give a solution in a reasonable amount of wall time.</source>
          <target state="translated">对于大型数据集，可以通过&lt;a href=&quot;https://hdbscan.readthedocs.io&quot;&gt;HDBSCAN&lt;/a&gt;获得相似（但不完全相同）的结果。HDBSCAN实现是多线程的，并且与OPTICS相比具有更好的算法运行时复杂性，但代价是内存扩展性更差。对于使用HDBSCAN耗尽系统内存的超大型数据集，OPTICS将维持&lt;em&gt;n&lt;/em&gt;（而不是&lt;em&gt;n ^ 2&lt;/em&gt;）的内存缩放；但是，可能需要使用 &lt;code&gt;max_eps&lt;/code&gt; 参数的调整以在合理的时间范围内给出解决方案。</target>
        </trans-unit>
        <trans-unit id="98187e1f181515ca77d41de7fa27ac44b69c7c11" translate="yes" xml:space="preserve">
          <source>For many estimators, including the SVMs, having datasets with unit standard deviation for each feature is important to get good prediction.</source>
          <target state="translated">对于许多估计器,包括SVMs,拥有每个特征的单位标准差的数据集对获得良好的预测很重要。</target>
        </trans-unit>
        <trans-unit id="f1359c1e0656157adbc7e3ee11ae253cb961bb70" translate="yes" xml:space="preserve">
          <source>For mono-output tasks it is:</source>
          <target state="translated">对于单项输出任务,它是:</target>
        </trans-unit>
        <trans-unit id="c24592da8118b35d1dd067bf2a75576669aef344" translate="yes" xml:space="preserve">
          <source>For more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) &amp;ldquo;Least Angle Regression,&amp;rdquo; Annals of Statistics (with discussion), 407-499. (&lt;a href=&quot;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&quot;&gt;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&lt;/a&gt;)</source>
          <target state="translated">有关更多信息，请参见：Bradley Efron，Trevor Hastie，Iain Johnstone和Robert Tibshirani（2004）&amp;ldquo;最小角度回归&amp;rdquo;，《统计年鉴》（带讨论），407-499。（&lt;a href=&quot;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&quot;&gt;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="d5e463f9f0eea6808a42462cec939570f71a16a6" translate="yes" xml:space="preserve">
          <source>For more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) &amp;ldquo;Least Angle Regression,&amp;rdquo; Annals of Statistics (with discussion), 407-499. (&lt;a href=&quot;https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&quot;&gt;https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&lt;/a&gt;)</source>
          <target state="translated">有关更多信息，请参见：Bradley Efron，Trevor Hastie，Iain Johnstone和Robert Tibshirani（2004）&amp;ldquo;最小角度回归&amp;rdquo;，《统计年鉴》（带讨论），407-499。（&lt;a href=&quot;https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&quot;&gt;https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="a4bc4c3f735998ec8c1614ec6127a37e3e7a02d8" translate="yes" xml:space="preserve">
          <source>For more information, see &lt;a href=&quot;../../modules/clustering#hierarchical-clustering&quot;&gt;Hierarchical clustering&lt;/a&gt;.</source>
          <target state="translated">有关更多信息，请参见&lt;a href=&quot;../../modules/clustering#hierarchical-clustering&quot;&gt;层次集群&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="b089e1ecd97ba592f22037a3c3fabf2924387c39" translate="yes" xml:space="preserve">
          <source>For more on usage see the &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">有关用法的更多信息，请参见《&lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;用户指南》&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="2e682df49d58d058f1f4b4c26ca6fb15a2f979d8" translate="yes" xml:space="preserve">
          <source>For multi-class classification, &lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt;&lt;code&gt;AdaBoostClassifier&lt;/code&gt;&lt;/a&gt; implements AdaBoost-SAMME and AdaBoost-SAMME.R &lt;a href=&quot;#zzrh2009&quot; id=&quot;id11&quot;&gt;[ZZRH2009]&lt;/a&gt;.</source>
          <target state="translated">对于多类分类，&lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt; &lt;code&gt;AdaBoostClassifier&lt;/code&gt; &lt;/a&gt;实现AdaBoost-SAMME和AdaBoost-SAMME.R &lt;a href=&quot;#zzrh2009&quot; id=&quot;id11&quot;&gt;[ZZRH2009]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="416ab9ed1829c79f2f091ddf9b13cfb5bb7486ce" translate="yes" xml:space="preserve">
          <source>For multi-class classification, n_class classifiers are trained in a one-versus-all approach. Concretely, this is implemented by taking advantage of the multi-variate response support in Ridge.</source>
          <target state="translated">对于多类分类,n_class分类器采用一比一的方式进行训练。具体来说,就是利用Ridge中的多变量响应支持来实现。</target>
        </trans-unit>
        <trans-unit id="f04898b2d6925a5dec9ef02339647623f6f19f8d" translate="yes" xml:space="preserve">
          <source>For multi-class classification, you need to set the class label for which the PDPs should be created via the &lt;code&gt;target&lt;/code&gt; argument:</source>
          <target state="translated">对于多类分类，您需要通过 &lt;code&gt;target&lt;/code&gt; 参数设置应为其创建PDP的类标签：</target>
        </trans-unit>
        <trans-unit id="65042013a5d26811a6a7088f4c47e70c6ddb0074" translate="yes" xml:space="preserve">
          <source>For multi-class models, you need to set the class label for which the PDPs should be created via the &lt;code&gt;label&lt;/code&gt; argument:</source>
          <target state="translated">对于多类模型，您需要通过 &lt;code&gt;label&lt;/code&gt; 参数设置应为其创建PDP的类标签：</target>
        </trans-unit>
        <trans-unit id="ccc2264ef7a998ec0c6ed9c92245080e0f0807c7" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, the scores for all the scorers are available in the &lt;code&gt;cv_results_&lt;/code&gt; dict at the keys ending with that scorer&amp;rsquo;s name (&lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt;) instead of &lt;code&gt;'_score'&lt;/code&gt; shown above. (&amp;lsquo;split0_test_precision&amp;rsquo;, &amp;lsquo;mean_train_precision&amp;rsquo; etc.)</source>
          <target state="translated">对于多指标评估，所有得分手的得分都可以在 &lt;code&gt;cv_results_&lt;/code&gt; dict中以该得分手的名字（ &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; ）结尾的键获得，而不是上面显示的 &lt;code&gt;'_score'&lt;/code&gt; 。（&amp;ldquo; split0_test_precision&amp;rdquo;，&amp;ldquo; mean_train_precision&amp;rdquo;等）</target>
        </trans-unit>
        <trans-unit id="6cd27769ef18013ec211b9a824f9bced7dd1ce74" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this attribute holds the validated &lt;code&gt;scoring&lt;/code&gt; dict which maps the scorer key to the scorer callable.</source>
          <target state="translated">对于多指标评估，此属性包含将 &lt;code&gt;scoring&lt;/code&gt; 者键映射到可调用得分者的已验证得分 dict。</target>
        </trans-unit>
        <trans-unit id="39c0f65b87a914c1f3244978c44bbc4d0d1190be" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this attribute is present only if &lt;code&gt;refit&lt;/code&gt; is specified.</source>
          <target state="translated">对于多指标评估，仅当指定 &lt;code&gt;refit&lt;/code&gt; ，才存在此属性。</target>
        </trans-unit>
        <trans-unit id="dd788cb84c37fa5cbd110321907573fb764ddce9" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this is not available if &lt;code&gt;refit&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;. See &lt;code&gt;refit&lt;/code&gt; parameter for more information.</source>
          <target state="translated">对于多指标评估，如果 &lt;code&gt;refit&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; ，则此功能不可用。有关更多信息，请参见 &lt;code&gt;refit&lt;/code&gt; 参数。</target>
        </trans-unit>
        <trans-unit id="4367b150d838b05eaff7170a1426f1dc4e6edc76" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this is present only if &lt;code&gt;refit&lt;/code&gt; is specified.</source>
          <target state="translated">对于多指标评估，仅当指定 &lt;code&gt;refit&lt;/code&gt; 时才存在。</target>
        </trans-unit>
        <trans-unit id="d328aa31182c6b57c5d921c1da1c7333c03486fe" translate="yes" xml:space="preserve">
          <source>For multi-output tasks it is:</source>
          <target state="translated">对于多输出任务,它是:</target>
        </trans-unit>
        <trans-unit id="22c12fa701004eab18f67dae2fb9f6cb3b68f428" translate="yes" xml:space="preserve">
          <source>For multi-output, the weights of each column of y will be multiplied.</source>
          <target state="translated">对于多输出,将y的每一列的权重相乘。</target>
        </trans-unit>
        <trans-unit id="77f8b599f18368a52e2142c2cada7c61eef9fbca" translate="yes" xml:space="preserve">
          <source>For multiclass classification with a &amp;ldquo;negative class&amp;rdquo;, it is possible to exclude some labels:</source>
          <target state="translated">对于带有&amp;ldquo;否定类&amp;rdquo;的多类分类，可以排除一些标签：</target>
        </trans-unit>
        <trans-unit id="dedd3af803ae0305b6a99c040827201493989ff1" translate="yes" xml:space="preserve">
          <source>For multiclass classification, K trees (for K classes) are built at each of the \(M\) iterations. The probability that \(x_i\) belongs to class k is modeled as a softmax of the \(F_{M,k}(x_i)\) values.</source>
          <target state="translated">对于多类分类,在每一次的迭代中都会建立K棵树(针对K类)。属于k类的概率被建模为一个软最大值(F_{M,k}(x_i)\)值。</target>
        </trans-unit>
        <trans-unit id="393c73f8bfb73747a6a85987ccf51d73c8d3636f" translate="yes" xml:space="preserve">
          <source>For multiclass problems, only &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo;, &amp;lsquo;saga&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; handle multinomial loss; &amp;lsquo;liblinear&amp;rsquo; is limited to one-versus-rest schemes.</source>
          <target state="translated">对于多类问题，只有'newton-cg'，'sag'，'saga'和'lbfgs'处理多项式损失。&amp;ldquo; liblinear&amp;rdquo;仅限于&amp;ldquo;一站式&amp;rdquo;计划。</target>
        </trans-unit>
        <trans-unit id="de227a68bb98af9d7f682ac4556427f028c04d66" translate="yes" xml:space="preserve">
          <source>For multiple labels per instance, use &lt;a href=&quot;generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt;&lt;code&gt;MultiLabelBinarizer&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">对于每个实例多个标签，请使用&lt;a href=&quot;generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt; &lt;code&gt;MultiLabelBinarizer&lt;/code&gt; &lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="36264d57db60ea0d287310ae67879ef2207922af" translate="yes" xml:space="preserve">
          <source>For multiple metric evaluation, this needs to be a &lt;code&gt;str&lt;/code&gt; denoting the scorer that would be used to find the best parameters for refitting the estimator at the end.</source>
          <target state="translated">对于多指标评估，这需要是一个表示计分器的 &lt;code&gt;str&lt;/code&gt; 该计分器将用于找到最佳参数以最终重新拟合估计器。</target>
        </trans-unit>
        <trans-unit id="088c3cd08ec3b30c1e8d705dc9f773b25266ffd1" translate="yes" xml:space="preserve">
          <source>For multiple metric evaluation, this needs to be a string denoting the scorer is used to find the best parameters for refitting the estimator at the end.</source>
          <target state="translated">对于多度量评价,这需要是一个字符串,表示评分器用于寻找最佳参数,以便在最后重新装配估计器。</target>
        </trans-unit>
        <trans-unit id="ab406c3bb6ddaeec6408e58ba4985d8a5097ee33" translate="yes" xml:space="preserve">
          <source>For multiple metric evaluation, this needs to be a string denoting the scorer that would be used to find the best parameters for refitting the estimator at the end.</source>
          <target state="translated">对于多重度量评估,这需要是一个字符串,表示将用于寻找最佳参数的评分器,以便在最后重新装配估计器。</target>
        </trans-unit>
        <trans-unit id="f895ac59b8264ca94c275f903e2d6c6c438b4c9c" translate="yes" xml:space="preserve">
          <source>For multiplicative-update (&amp;lsquo;mu&amp;rsquo;) solver, the Frobenius norm (0.5 * ||X - WH||_Fro^2) can be changed into another beta-divergence loss, by changing the beta_loss parameter.</source>
          <target state="translated">对于乘更新（'mu'）求解器，可以通过更改beta_loss参数将Frobenius范数（0.5 * || X-WH || _Fro ^ 2）更改为另一个beta发散损失。</target>
        </trans-unit>
        <trans-unit id="4d0bed9bc5aa3b36bb0ba6ad6bc592a5bb3e78af" translate="yes" xml:space="preserve">
          <source>For n_components == &amp;lsquo;mle&amp;rsquo;, this class uses the method of &lt;code&gt;Minka, T. P. &amp;ldquo;Automatic choice of dimensionality for PCA&amp;rdquo;. In NIPS, pp. 598-604&lt;/code&gt;</source>
          <target state="translated">对于n_components =='mle'，此类使用 &lt;code&gt;Minka, T. P. &amp;ldquo;Automatic choice of dimensionality for PCA&amp;rdquo;. In NIPS, pp. 598-604&lt;/code&gt; 的方法TP&amp;ldquo; PCA的尺寸自动选择&amp;rdquo;。在NIPS中，第598-604页</target>
        </trans-unit>
        <trans-unit id="bd307d11754e6296ef567a0e60c357948ac810da" translate="yes" xml:space="preserve">
          <source>For n_components == &amp;lsquo;mle&amp;rsquo;, this class uses the method of &lt;em&gt;Minka, T. P. &amp;ldquo;Automatic choice of dimensionality for PCA&amp;rdquo;. In NIPS, pp. 598-604&lt;/em&gt;</source>
          <target state="translated">对于n_components =='mle'，此类使用&lt;em&gt;Minka&lt;/em&gt;的方法&lt;em&gt;TP&amp;ldquo; PCA的尺寸自动选择&amp;rdquo;。在NIPS中，第598-604页&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="9f27cc961ae174b8e96b15764c2907159174c2c2" translate="yes" xml:space="preserve">
          <source>For non-sparse models, i.e. when there are not many zeros in &lt;code&gt;coef_&lt;/code&gt;, this may actually &lt;em&gt;increase&lt;/em&gt; memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with &lt;code&gt;(coef_ == 0).sum()&lt;/code&gt;, must be more than 50% for this to provide significant benefits.</source>
          <target state="translated">对于非稀疏模型，即 &lt;code&gt;coef_&lt;/code&gt; 中没有太多零时，这实际上可能会&lt;em&gt;增加&lt;/em&gt;内存使用量，因此请谨慎使用此方法。一条经验法则是，可以使用 &lt;code&gt;(coef_ == 0).sum()&lt;/code&gt; 计算的零元素数量必须大于50％，以提供显着的好处。</target>
        </trans-unit>
        <trans-unit id="ffa9a81349b558a734852a9e93a9e01e72e14f97" translate="yes" xml:space="preserve">
          <source>For normalized mutual information and adjusted mutual information, the normalizing value is typically some &lt;em&gt;generalized&lt;/em&gt; mean of the entropies of each clustering. Various generalized means exist, and no firm rules exist for preferring one over the others. The decision is largely a field-by-field basis; for instance, in community detection, the arithmetic mean is most common. Each normalizing method provides &amp;ldquo;qualitatively similar behaviours&amp;rdquo; &lt;a href=&quot;#yat2016&quot; id=&quot;id14&quot;&gt;[YAT2016]&lt;/a&gt;. In our implementation, this is controlled by the &lt;code&gt;average_method&lt;/code&gt; parameter.</source>
          <target state="translated">对于归一化的互信息和调整后的互信息，归一化值通常是每个聚类的熵的某个&lt;em&gt;广义&lt;/em&gt;均值。存在各种广义的手段，并且不存在确定一种偏于另一种的坚决规则。该决定在很大程度上是逐个领域进行的；例如，在社区检测中，算术平均值是最常见的。每种归一化方法都提供&amp;ldquo;定性相似的行为&amp;rdquo; &lt;a href=&quot;#yat2016&quot; id=&quot;id14&quot;&gt;[YAT2016]&lt;/a&gt;。在我们的实现中，这是由 &lt;code&gt;average_method&lt;/code&gt; 参数控制的。</target>
        </trans-unit>
        <trans-unit id="e62cf2ed735d43186d3b55660d3dd2856258814f" translate="yes" xml:space="preserve">
          <source>For normalized mutual information and adjusted mutual information, the normalizing value is typically some &lt;em&gt;generalized&lt;/em&gt; mean of the entropies of each clustering. Various generalized means exist, and no firm rules exist for preferring one over the others. The decision is largely a field-by-field basis; for instance, in community detection, the arithmetic mean is most common. Each normalizing method provides &amp;ldquo;qualitatively similar behaviours&amp;rdquo; [YAT2016]. In our implementation, this is controlled by the &lt;code&gt;average_method&lt;/code&gt; parameter.</source>
          <target state="translated">对于归一化的互信息和调整后的互信息，归一化值通常是每个聚类的熵的某个&lt;em&gt;广义&lt;/em&gt;均值。存在各种广义的手段，并且不存在确定一种优先于另一种的固定规则。该决定在很大程度上是逐个领域进行的；例如，在社区检测中，算术平均值最为常见。每种归一化方法都提供&amp;ldquo;质量上相似的行为&amp;rdquo; [YAT2016]。在我们的实现中，这是由 &lt;code&gt;average_method&lt;/code&gt; 参数控制的。</target>
        </trans-unit>
        <trans-unit id="8d22dd702102471017e490a2b7bc8b22b9add5e5" translate="yes" xml:space="preserve">
          <source>For now &amp;ldquo;auto&amp;rdquo; (kept for backward compatibiliy) chooses &amp;ldquo;elkan&amp;rdquo; but it might change in the future for a better heuristic.</source>
          <target state="translated">目前，&amp;ldquo; auto&amp;rdquo;（保持向后兼容性）选择&amp;ldquo; elkan&amp;rdquo;，但为了更好的启发式，将来可能会更改。</target>
        </trans-unit>
        <trans-unit id="5573de0c3d8eae2871980794db1007675aa56eed" translate="yes" xml:space="preserve">
          <source>For now, we will consider the estimator as a black box:</source>
          <target state="translated">现在,我们将把估计器视为一个黑盒子。</target>
        </trans-unit>
        <trans-unit id="436dc589cc421427188d0cca81de34e1e73f3c7d" translate="yes" xml:space="preserve">
          <source>For one sample, given the vector of continuous ground-truth values for each target \(y \in \mathbb{R}^{M}\), where \(M\) is the number of outputs, and the prediction \(\hat{y}\), which induces the ranking function \(f\), the DCG score is</source>
          <target state="translated">对于一个样本,给定每个目标的连续地面真值向量 ⑴(y ⑴in mathbb{R}^{M}),其中 ⑵(M)是产出的数量,以及预测 ⑶(hat{y}),它引起的排序函数 ⑷(f),DCG得分是</target>
        </trans-unit>
        <trans-unit id="09d6f289aa89a27e5d33a2c2e001f7d32a001ce5" translate="yes" xml:space="preserve">
          <source>For our dataset, again the model is not very predictive.</source>
          <target state="translated">对于我们的数据集,该模型同样没有很好的预测性。</target>
        </trans-unit>
        <trans-unit id="a7f8c21b68cc173c251c1992bff906fb9b13f276" translate="yes" xml:space="preserve">
          <source>For parameter estimation, the posterior distribution is:</source>
          <target state="translated">对于参数估计,后置分布为:。</target>
        </trans-unit>
        <trans-unit id="acaedbca04fb48c0d8436452cabe74f03629d275" translate="yes" xml:space="preserve">
          <source>For regression the default learning rate schedule is inverse scaling (&lt;code&gt;learning_rate='invscaling'&lt;/code&gt;), given by</source>
          <target state="translated">为了进行回归，默认学习率计划是反比例缩放（ &lt;code&gt;learning_rate='invscaling'&lt;/code&gt; ），由</target>
        </trans-unit>
        <trans-unit id="17d6bf85a6ee02e9c1e3f5f799f4a791f96ca437" translate="yes" xml:space="preserve">
          <source>For regression with a squared loss and a l2 penalty, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">对于具有平方损失和12罚分的回归，可以使用随机平均梯度（SAG）算法获得具有平均策略的SGD的另一个变体，该算法可作为&lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; 中&lt;/a&gt;的求解器使用。</target>
        </trans-unit>
        <trans-unit id="82e8631b28597b123d5803439222a08ee2e59047" translate="yes" xml:space="preserve">
          <source>For regression, &lt;a href=&quot;generated/sklearn.ensemble.adaboostregressor#sklearn.ensemble.AdaBoostRegressor&quot;&gt;&lt;code&gt;AdaBoostRegressor&lt;/code&gt;&lt;/a&gt; implements AdaBoost.R2 &lt;a href=&quot;#d1997&quot; id=&quot;id12&quot;&gt;[D1997]&lt;/a&gt;.</source>
          <target state="translated">对于回归，&lt;a href=&quot;generated/sklearn.ensemble.adaboostregressor#sklearn.ensemble.AdaBoostRegressor&quot;&gt; &lt;code&gt;AdaBoostRegressor&lt;/code&gt; &lt;/a&gt;实现AdaBoost.R2 &lt;a href=&quot;#d1997&quot; id=&quot;id12&quot;&gt;[D1997]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="faab950ebeb88e87f11e22c6efcd943439e07aea" translate="yes" xml:space="preserve">
          <source>For regression, MLP uses the Square Error loss function; written as,</source>
          <target state="translated">对于回归,MLP使用平方误差损失函数;写成:</target>
        </trans-unit>
        <trans-unit id="a8c842b7da02e24dac30b073413aa7112e52aecd" translate="yes" xml:space="preserve">
          <source>For regression: &lt;a href=&quot;generated/sklearn.feature_selection.f_regression#sklearn.feature_selection.f_regression&quot;&gt;&lt;code&gt;f_regression&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt;&lt;code&gt;mutual_info_regression&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">对于回归：&lt;a href=&quot;generated/sklearn.feature_selection.f_regression#sklearn.feature_selection.f_regression&quot;&gt; &lt;code&gt;f_regression&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt; &lt;code&gt;mutual_info_regression&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e8aa16ccbf6b94ae32b7c5b78e608f800f0eb6cd" translate="yes" xml:space="preserve">
          <source>For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was handled by returning a dense np.matrix instance. Going forward, np.ndarray returns an np.ndarray, as expected.</source>
          <target state="translated">在 scikit-learn 0.14.1 及以前的版本中,return_as=np.ndarray 是通过返回一个密集的 np.matrix 实例来处理的。今后,np.ndarray将按照预期返回一个np.ndarray。</target>
        </trans-unit>
        <trans-unit id="2410f1ccaa1a03065aaeec2b709967381feb9cea" translate="yes" xml:space="preserve">
          <source>For simple transformations, instead of a Transformer object, a pair of functions can be passed, defining the transformation and its inverse mapping:</source>
          <target state="translated">对于简单的变换,可以通过一对函数来代替Transformer对象,定义变换和它的反映射。</target>
        </trans-unit>
        <trans-unit id="2f72f7e3c1f68f97fc714ad1c06f3f5738fb15a6" translate="yes" xml:space="preserve">
          <source>For simplicity the equation above is written for a single training example. The gradient with respect to the weights is formed of two terms corresponding to the ones above. They are usually known as the positive gradient and the negative gradient, because of their respective signs. In this implementation, the gradients are estimated over mini-batches of samples.</source>
          <target state="translated">为了简单起见,上面的方程是针对单个训练实例写的。相对于权重的梯度是由两个项组成的,与上面的对应。它们通常被称为正梯度和负梯度,因为它们各自的符号。在本实施例中,梯度是通过微型样本批次来估计的。</target>
        </trans-unit>
        <trans-unit id="27c46746207f2a31ae25da1632ef3ccf3ef87e4d" translate="yes" xml:space="preserve">
          <source>For single metric evaluation, where the scoring parameter is a string, callable or None, the keys will be - &lt;code&gt;['test_score', 'fit_time', 'score_time']&lt;/code&gt;</source>
          <target state="translated">对于单指标评估，评分参数是字符串，可调用或无，则键将是- &lt;code&gt;['test_score', 'fit_time', 'score_time']&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a0a8bb77034843b440cd9ae1f7c55bd3aa47ece1" translate="yes" xml:space="preserve">
          <source>For small data sets (\(N\) less than 30 or so), \(\log(N)\) is comparable to \(N\), and brute force algorithms can be more efficient than a tree-based approach. Both &lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; address this through providing a &lt;em&gt;leaf size&lt;/em&gt; parameter: this controls the number of samples at which a query switches to brute-force. This allows both algorithms to approach the efficiency of a brute-force computation for small \(N\).</source>
          <target state="translated">对于较小的数据集（小于30的\（N \）），\（\ log（N）\）与\（N \）相当，并且蛮力算法可能比基于树的方法更有效。&lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt;都通过提供一个&lt;em&gt;叶子大小&lt;/em&gt;参数来解决此问题：这控制了查询切换为蛮力时的样本数。这使两种算法都可以针对小\（N \）接近蛮力计算的效率。</target>
        </trans-unit>
        <trans-unit id="4638d963661692a289a12b8cac2d92a9d2c758fa" translate="yes" xml:space="preserve">
          <source>For small datasets, &amp;lsquo;liblinear&amp;rsquo; is a good choice, whereas &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;saga&amp;rsquo; are faster for large ones.</source>
          <target state="translated">对于小型数据集，&amp;ldquo; liblinear&amp;rdquo;是一个不错的选择，而对于大型数据集，&amp;ldquo; sag&amp;rdquo;和&amp;ldquo; saga&amp;rdquo;则更快。</target>
        </trans-unit>
        <trans-unit id="dffce5e2239efe7c22f00e78e9cfce148c8ba698" translate="yes" xml:space="preserve">
          <source>For some applications the amount of examples, features (or both) and/or the speed at which they need to be processed are challenging for traditional approaches. In these cases scikit-learn has a number of options you can consider to make your system scale.</source>
          <target state="translated">对于一些应用来说,例子的数量、特征(或两者)和/或需要处理的速度对于传统的方法来说是一个挑战。在这些情况下,scikit-learn有很多选择,您可以考虑让您的系统进行扩展。</target>
        </trans-unit>
        <trans-unit id="d0fa030cdd6e029de147eaddac9b22a69b1ced78" translate="yes" xml:space="preserve">
          <source>For some applications the performance (mainly latency and throughput at prediction time) of estimators is crucial. It may also be of interest to consider the training throughput but this is often less important in a production setup (where it often takes place offline).</source>
          <target state="translated">对于某些应用来说,估计器的性能(主要是预测时的延迟和吞吐量)是至关重要的。考虑训练吞吐量可能也很有意义,但在生产设置中,这往往不那么重要(通常是离线进行)。</target>
        </trans-unit>
        <trans-unit id="7c42c316b39e1433fd7efc7ca18424a81519f374" translate="yes" xml:space="preserve">
          <source>For some business applications, we are interested in the ability of the model to rank the riskiest from the safest policyholders, irrespective of the absolute value of the prediction. In this case, the model evaluation would cast the problem as a ranking problem rather than a regression problem.</source>
          <target state="translated">对于一些商业应用,我们感兴趣的是模型从最安全的保单持有人中排列出最危险的保单持有人的能力,而不管预测的绝对值如何。在这种情况下,模型评估将把问题投向排名问题而不是回归问题。</target>
        </trans-unit>
        <trans-unit id="7400fa7073eb75f62370e5aadbb0f2aef8d5fc81" translate="yes" xml:space="preserve">
          <source>For some datasets, a pre-defined split of the data into training- and validation fold or into several cross-validation folds already exists. Using &lt;a href=&quot;generated/sklearn.model_selection.predefinedsplit#sklearn.model_selection.PredefinedSplit&quot;&gt;&lt;code&gt;PredefinedSplit&lt;/code&gt;&lt;/a&gt; it is possible to use these folds e.g. when searching for hyperparameters.</source>
          <target state="translated">对于某些数据集，已经预先定义了将数据分为训练折叠和验证折叠或几个交叉验证折叠的数据。使用&lt;a href=&quot;generated/sklearn.model_selection.predefinedsplit#sklearn.model_selection.PredefinedSplit&quot;&gt; &lt;code&gt;PredefinedSplit&lt;/code&gt; &lt;/a&gt;可以使用这些折叠，例如在搜索超参数时。</target>
        </trans-unit>
        <trans-unit id="694369dc091b24e34e3df33be9eef2601d0ef6d1" translate="yes" xml:space="preserve">
          <source>For some losses, e.g. the least absolute deviation (LAD) where the gradients are \(\pm 1\), the values predicted by a fitted \(h_m\) are not accurate enough: the tree can only output integer values. As a result, the leaves values of the tree \(h_m\) are modified once the tree is fitted, such that the leaves values minimize the loss \(L_m\). The update is loss-dependent: for the LAD loss, the value of a leaf is updated to the median of the samples in that leaf.</source>
          <target state="translated">对于一些损失,例如梯度为最小绝对偏差(LAD)的情况下,拟合的树所预测的值(h_m/)不够准确:树只能输出整数值。因此,一旦树被拟合,树的叶子值就会被修改,使叶子值的损失最小化(L_m/)。更新是依赖于损失的:对于LAD损失,叶子的值更新为该叶子中样本的中值。</target>
        </trans-unit>
        <trans-unit id="c50bf24a893de08f1d0809fe397202f1a031fb85" translate="yes" xml:space="preserve">
          <source>For some miscellaneous data such as images, videos, and audio, you may wish to refer to:</source>
          <target state="translated">对于一些杂项数据,如图片、视频和音频,你可以参考。</target>
        </trans-unit>
        <trans-unit id="303cfe0bd7811405e77868ed843c4904556ebde5" translate="yes" xml:space="preserve">
          <source>For sparse input the data is &lt;strong&gt;converted to the Compressed Sparse Rows representation&lt;/strong&gt; (see &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt;) before being fed to efficient Cython routines. To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.</source>
          <target state="translated">对于稀疏输入，在将数据馈入有效的Cython例程之前， &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; 其&lt;strong&gt;转换为Compressed Sparse Rows表示形式&lt;/strong&gt;（请参见scipy.sparse.csr_matrix）。为避免不必要的内存复制，建议选择上游的CSR表示形式。</target>
        </trans-unit>
        <trans-unit id="44ae99861a7942ab4350b3f16d27ddb33207e51f" translate="yes" xml:space="preserve">
          <source>For sparse input the data is &lt;strong&gt;converted to the Compressed Sparse Rows representation&lt;/strong&gt; (see &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt;). To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.</source>
          <target state="translated">对于稀疏输入，数据将&lt;strong&gt;转换为&amp;ldquo;压缩稀疏行&amp;rdquo;表示形式&lt;/strong&gt;（请参见 &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; ）。为避免不必要的内存复制，建议选择上游的CSR表示形式。</target>
        </trans-unit>
        <trans-unit id="a6ddfb481ebd700db5464677bebe705030e704c9" translate="yes" xml:space="preserve">
          <source>For speed and space efficiency reasons &lt;code&gt;scikit-learn&lt;/code&gt; loads the target attribute as an array of integers that corresponds to the index of the category name in the &lt;code&gt;target_names&lt;/code&gt; list. The category integer id of each sample is stored in the &lt;code&gt;target&lt;/code&gt; attribute:</source>
          <target state="translated">出于速度和空间效率的原因， &lt;code&gt;scikit-learn&lt;/code&gt; 将target属性加载为整数数组，该整数数组对应于 &lt;code&gt;target_names&lt;/code&gt; 列表中类别名称的索引。每个样本的类别整数id存储在 &lt;code&gt;target&lt;/code&gt; 属性中：</target>
        </trans-unit>
        <trans-unit id="7ace947ef3298ab26b0edee5253deecf977a3b02" translate="yes" xml:space="preserve">
          <source>For speed, all real work is done at the C level in function copy_predict (libsvm_helper.c).</source>
          <target state="translated">为了提高速度,所有真正的工作都是在函数copy_predict(libsvm_helper.c)的C层完成的。</target>
        </trans-unit>
        <trans-unit id="1c3a0f29bcc1c543ffc020478b77d5b706223ce4" translate="yes" xml:space="preserve">
          <source>For splitting the data according to explicit domain-specific stratification of the dataset.</source>
          <target state="translated">用于根据明确的特定领域分层对数据集进行拆分。</target>
        </trans-unit>
        <trans-unit id="0adf7a63adc917db1adffd6d4cf61e05de34a6e7" translate="yes" xml:space="preserve">
          <source>For splitting the data according to explicit, domain-specific stratification of the dataset.</source>
          <target state="translated">用于根据明确的、特定领域的数据集分层来分割数据。</target>
        </trans-unit>
        <trans-unit id="ab4a742934d8510715858d09854f728742beaaec" translate="yes" xml:space="preserve">
          <source>For svd_solver == &amp;lsquo;arpack&amp;rsquo;, refer to &lt;code&gt;scipy.sparse.linalg.svds&lt;/code&gt;.</source>
          <target state="translated">对于svd_solver =='arpack'，请参考 &lt;code&gt;scipy.sparse.linalg.svds&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="25caaa7ea914cf58c60f262a55964ee17d21e090" translate="yes" xml:space="preserve">
          <source>For svd_solver == &amp;lsquo;randomized&amp;rsquo;, see: &lt;code&gt;Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). &amp;ldquo;Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions&amp;rdquo;. SIAM review, 53(2), 217-288.&lt;/code&gt; and also &lt;code&gt;Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). &amp;ldquo;A randomized algorithm for the decomposition of matrices&amp;rdquo;. Applied and Computational Harmonic Analysis, 30(1), 47-68.&lt;/code&gt;</source>
          <target state="translated">有关svd_solver =='randomized'的信息，请参见： &lt;code&gt;Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). &amp;ldquo;Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions&amp;rdquo;. SIAM review, 53(2), 217-288.&lt;/code&gt; 以及 &lt;code&gt;Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). &amp;ldquo;A randomized algorithm for the decomposition of matrices&amp;rdquo;. Applied and Computational Harmonic Analysis, 30(1), 47-68.&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c853f7e3d4c36b0f8076402fc9583125688d75a1" translate="yes" xml:space="preserve">
          <source>For svd_solver == &amp;lsquo;randomized&amp;rsquo;, see: &lt;em&gt;Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). &amp;ldquo;Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions&amp;rdquo;. SIAM review, 53(2), 217-288.&lt;/em&gt; and also &lt;em&gt;Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). &amp;ldquo;A randomized algorithm for the decomposition of matrices&amp;rdquo;. Applied and Computational Harmonic Analysis, 30(1), 47-68.&lt;/em&gt;</source>
          <target state="translated">有关svd_solver =='randomized'的信息，请参见：&lt;em&gt;N。Halko，PG的Martinsson和JA的Tropp（2011）。 &amp;ldquo;随机发现结构：用于构造近似矩阵分解的概率算法&amp;rdquo;。 SIAM评论，53（2），217-288。&lt;/em&gt;以及&lt;em&gt;PG的Martinsson，V。的Rokhlin和M.的Tygert（2011）。 &amp;ldquo;矩阵分解的随机算法&amp;rdquo;。应用与计算谐波分析，30（1），47-68。&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="e7341be727234aad9fa4e331ba9d61b6fce122ff" translate="yes" xml:space="preserve">
          <source>For the &amp;lsquo;liblinear&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers set verbose to any positive number for verbosity.</source>
          <target state="translated">对于'liblinear'，'sag'和'lbfgs'求解器将冗长设置为冗长的任何正数。</target>
        </trans-unit>
        <trans-unit id="e0f6f55d7894824895d15fbf0dd2debb3188c403" translate="yes" xml:space="preserve">
          <source>For the &lt;a href=&quot;classes#module-sklearn.svm&quot;&gt;&lt;code&gt;sklearn.svm&lt;/code&gt;&lt;/a&gt; family of algorithms with a non-linear kernel, the latency is tied to the number of support vectors (the fewer the faster). Latency and throughput should (asymptotically) grow linearly with the number of support vectors in a SVC or SVR model. The kernel will also influence the latency as it is used to compute the projection of the input vector once per support vector. In the following graph the &lt;code&gt;nu&lt;/code&gt; parameter of &lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt;&lt;code&gt;sklearn.svm.NuSVR&lt;/code&gt;&lt;/a&gt; was used to influence the number of support vectors.</source>
          <target state="translated">对于具有非线性内核的&lt;a href=&quot;classes#module-sklearn.svm&quot;&gt; &lt;code&gt;sklearn.svm&lt;/code&gt; &lt;/a&gt;算法系列，延迟与支持向量的数量有关（越少越快）。延迟和吞吐量应（渐近地）随着SVC或SVR模型中支持向量的数量线性增长。内核还将影响等待时间，因为它用于每个支持向量计算一次输入向量的投影。在下图中，&lt;a href=&quot;generated/sklearn.svm.nusvr#sklearn.svm.NuSVR&quot;&gt; &lt;code&gt;sklearn.svm.NuSVR&lt;/code&gt; &lt;/a&gt;的 &lt;code&gt;nu&lt;/code&gt; 参数用于影响支持向量的数量。</target>
        </trans-unit>
        <trans-unit id="a081b6c50a07cf5457333031806f4c48e54ea42e" translate="yes" xml:space="preserve">
          <source>For the &lt;a href=&quot;classes#module-sklearn.svm&quot;&gt;&lt;code&gt;sklearn.svm&lt;/code&gt;&lt;/a&gt; family of algorithms with a non-linear kernel, the latency is tied to the number of support vectors (the fewer the faster). Latency and throughput should (asymptotically) grow linearly with the number of support vectors in a SVC or SVR model. The kernel will also influence the latency as it is used to compute the projection of the input vector once per support vector. In the following graph the &lt;code&gt;nu&lt;/code&gt; parameter of &lt;code&gt;sklearn.svm.classes.NuSVR&lt;/code&gt; was used to influence the number of support vectors.</source>
          <target state="translated">对于具有非线性内核的&lt;a href=&quot;classes#module-sklearn.svm&quot;&gt; &lt;code&gt;sklearn.svm&lt;/code&gt; &lt;/a&gt;算法系列，延迟与支持向量的数量有关（越少越快）。延迟和吞吐量应（渐近地）随着SVC或SVR模型中支持向量的数量线性增长。内核还将影响等待时间，因为它用于每个支持向量计算一次输入向量的投影。在下图中，使用了 &lt;code&gt;sklearn.svm.classes.NuSVR&lt;/code&gt; 的 &lt;code&gt;nu&lt;/code&gt; 参数来影响支持向量的数量。</target>
        </trans-unit>
        <trans-unit id="49dfac47eea992144c43ccc29f61713fabd0e5ae" translate="yes" xml:space="preserve">
          <source>For the &lt;code&gt;l2&lt;/code&gt; penalty case, the best result comes from the case where &lt;code&gt;C&lt;/code&gt; is not scaled.</source>
          <target state="translated">对于 &lt;code&gt;l2&lt;/code&gt; 罚金的情况，最好的结果来自于 &lt;code&gt;C&lt;/code&gt; 不缩放的情况。</target>
        </trans-unit>
        <trans-unit id="b64515e541acdc2619277e9fede8598b267eca89" translate="yes" xml:space="preserve">
          <source>For the coefficient analysis, scaling is not needed this time.</source>
          <target state="translated">对于系数分析,这次不需要缩放。</target>
        </trans-unit>
        <trans-unit id="73523e969cb224887344b79725c4bd74783daf1f" translate="yes" xml:space="preserve">
          <source>For the grid of &lt;code&gt;Cs&lt;/code&gt; values and &lt;code&gt;l1_ratios&lt;/code&gt; values, the best hyperparameter is selected by the cross-validator &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt;, but it can be changed using the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-cv&quot;&gt;cv&lt;/a&gt; parameter. The &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo;, &amp;lsquo;saga&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers can warm-start the coefficients (see &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;Glossary&lt;/a&gt;).</source>
          <target state="translated">对于 &lt;code&gt;Cs&lt;/code&gt; 值和 &lt;code&gt;l1_ratios&lt;/code&gt; 值的网格，交叉验证器&lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt;选择了最佳超参数，但可以使用&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-cv&quot;&gt;cv&lt;/a&gt;参数更改它。'newton-cg'，'sag'，'saga'和'lbfgs'求解器可以热启动系数（请参阅&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-warm-start&quot;&gt;词汇表&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="d9f81a56586341e43516abb99b238b1b5d6587c8" translate="yes" xml:space="preserve">
          <source>For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data.</source>
          <target state="translated">对于Cs值的网格(默认设置为1e-4和1e-4之间的10个对数值),最佳超参数由交叉验证器StratifiedKFold选择,但可以通过cv参数进行修改。在newton-cg和lbfgs求解器的情况下,我们沿着路径进行热启动,即把本次拟合的初始系数猜测为前一次拟合收敛后得到的系数,所以对于高维密集数据来说,应该是比较快的。</target>
        </trans-unit>
        <trans-unit id="a2e465567e94e51f5ccdac035f7ba7d95a9eeb84" translate="yes" xml:space="preserve">
          <source>For the lbfgs solver set verbose to any positive number for verbosity.</source>
          <target state="translated">对于lbfgs解算器,将verbose设置为任意正数作为verbosity。</target>
        </trans-unit>
        <trans-unit id="93f0b6841feed67e5fc00af0443562656921cce7" translate="yes" xml:space="preserve">
          <source>For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.</source>
          <target state="translated">对于liblinear和lbfgs解算器,将verbose设置为任意正数的verbosity。</target>
        </trans-unit>
        <trans-unit id="181a8f355e9076e45b2bb33dd2324e0459310c3b" translate="yes" xml:space="preserve">
          <source>For the linear case, the algorithm used in &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; by the &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; implementation is much more efficient than its &lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;-based &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; counterpart and can scale almost linearly to millions of samples and/or features.</source>
          <target state="translated">对于线性的情况下，在所使用的算法&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;由&lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt;实施是更有效的比其&lt;a href=&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;LIBSVM&lt;/a&gt;的基于&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt;对方，并且可以几乎线性地扩展到数百万样本和/或功能。</target>
        </trans-unit>
        <trans-unit id="4c4a7d0fb25ecd0231acfef000eb4ebb4024b077" translate="yes" xml:space="preserve">
          <source>For the most common use cases, you can designate a scorer object with the &lt;code&gt;scoring&lt;/code&gt; parameter; the table below shows all possible values. All scorer objects follow the convention that &lt;strong&gt;higher return values are better than lower return values&lt;/strong&gt;. Thus metrics which measure the distance between the model and the data, like &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;metrics.mean_squared_error&lt;/code&gt;&lt;/a&gt;, are available as neg_mean_squared_error which return the negated value of the metric.</source>
          <target state="translated">对于最常见的用例，您可以使用 &lt;code&gt;scoring&lt;/code&gt; 参数指定一个计分器对象。下表显示了所有可能的值。所有计分器对象均遵循以下约定：&lt;strong&gt;较高的返回值比较低的返回值更好&lt;/strong&gt;。因此，用于度量模型与数据之间距离的度量（如&lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;metrics.mean_squared_error&lt;/code&gt; )&lt;/a&gt;可用作neg_mean_squared_error，该度量返回度量的取反值。</target>
        </trans-unit>
        <trans-unit id="d789fec5338f15a1f2a15098b90094ce0875aa16" translate="yes" xml:space="preserve">
          <source>For the naive Bayes, both the validation score and the training score converge to a value that is quite low with increasing size of the training set. Thus, we will probably not benefit much from more training data.</source>
          <target state="translated">对于天真的贝叶斯来说,验证得分和训练得分都会随着训练集大小的增加而收敛到一个相当低的值。因此,我们可能不会从更多的训练数据中获益良多。</target>
        </trans-unit>
        <trans-unit id="5c5d7d9872e083b1cf44dccc9ef51ebf6d1fc473" translate="yes" xml:space="preserve">
          <source>For the rationale behind the names &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt;, i.e. naive Bayes as a linear classifier, see J. Rennie et al. (2003), Tackling the poor assumptions of naive Bayes text classifiers, ICML.</source>
          <target state="translated">有关名称 &lt;code&gt;coef_&lt;/code&gt; 和 &lt;code&gt;intercept_&lt;/code&gt; 的基本原理，即朴素的贝叶斯作为线性分类器，请参见J. Rennie等。（2003年），处理朴素的贝叶斯文本分类器的糟糕假设，ICML。</target>
        </trans-unit>
        <trans-unit id="1a1a7cda3c63aae62fc5939e46e880b718f959b0" translate="yes" xml:space="preserve">
          <source>For the remainder of this example, we remove the last element in &lt;code&gt;clfs&lt;/code&gt; and &lt;code&gt;ccp_alphas&lt;/code&gt;, because it is the trivial tree with only one node. Here we show that the number of nodes and tree depth decreases as alpha increases.</source>
          <target state="translated">在本示例的其余部分中，我们删除了 &lt;code&gt;clfs&lt;/code&gt; 和 &lt;code&gt;ccp_alphas&lt;/code&gt; 中的最后一个元素，因为它是只有一个节点的琐碎的树。在这里，我们显示节点的数量和树的深度随着alpha的增加而减少。</target>
        </trans-unit>
        <trans-unit id="08233f540a36ed45603c5cb01e2e4f593cd79c27" translate="yes" xml:space="preserve">
          <source>For the simple task of finding the nearest neighbors between two sets of data, the unsupervised algorithms within &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; can be used:</source>
          <target state="translated">对于查找两组数据之间最近的邻居的简单任务，可以使用&lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; 中&lt;/a&gt;的无监督算法：</target>
        </trans-unit>
        <trans-unit id="a0a1bdaba8df32e218fd25f1e35b96e97a68bb33" translate="yes" xml:space="preserve">
          <source>For this data, we might want to encode the &lt;code&gt;'city'&lt;/code&gt; column as a categorical variable using &lt;a href=&quot;generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;preprocessing.OneHotEncoder&lt;/code&gt;&lt;/a&gt; but apply a &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt; to the &lt;code&gt;'title'&lt;/code&gt; column. As we might use multiple feature extraction methods on the same column, we give each transformer a unique name, say &lt;code&gt;'city_category'&lt;/code&gt; and &lt;code&gt;'title_bow'&lt;/code&gt;. By default, the remaining rating columns are ignored (&lt;code&gt;remainder='drop'&lt;/code&gt;):</source>
          <target state="translated">对于此数据，我们可能希望使用&lt;a href=&quot;generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt; &lt;code&gt;preprocessing.OneHotEncoder&lt;/code&gt; &lt;/a&gt;将 &lt;code&gt;'city'&lt;/code&gt; 列编码为分类变量，但将一个&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt;应用于 &lt;code&gt;'title'&lt;/code&gt; 列。由于我们可能在同一列上使用多种特征提取方法，因此我们给每个转换器一个唯一的名称，例如 &lt;code&gt;'city_category'&lt;/code&gt; 和 &lt;code&gt;'title_bow'&lt;/code&gt; 。默认情况下，其余的评级列将被忽略（ &lt;code&gt;remainder='drop'&lt;/code&gt; ）：</target>
        </trans-unit>
        <trans-unit id="daed0c58d42835f25cc91f4ef37c8c2918d442fd" translate="yes" xml:space="preserve">
          <source>For this data, we might want to encode the &lt;code&gt;'city'&lt;/code&gt; column as a categorical variable, but apply a &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt; to the &lt;code&gt;'title'&lt;/code&gt; column. As we might use multiple feature extraction methods on the same column, we give each transformer a unique name, say &lt;code&gt;'city_category'&lt;/code&gt; and &lt;code&gt;'title_bow'&lt;/code&gt;. By default, the remaining rating columns are ignored (&lt;code&gt;remainder='drop'&lt;/code&gt;):</source>
          <target state="translated">对于此数据，我们可能希望将 &lt;code&gt;'city'&lt;/code&gt; 列编码为分类变量，但将一个&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt;应用于 &lt;code&gt;'title'&lt;/code&gt; 列。由于我们可能在同一列上使用多种特征提取方法，因此我们给每个转换器一个唯一的名称，例如 &lt;code&gt;'city_category'&lt;/code&gt; 和 &lt;code&gt;'title_bow'&lt;/code&gt; 。默认情况下，其余的评级列将被忽略（ &lt;code&gt;remainder='drop'&lt;/code&gt; ）：</target>
        </trans-unit>
        <trans-unit id="2e47bbc09921a29cf4a007e2d92242f5a8a9f3d8" translate="yes" xml:space="preserve">
          <source>For this example we will use the &lt;a href=&quot;http://mldata.org/repository/data/viewslug/yeast&quot;&gt;yeast&lt;/a&gt; dataset which contains 2417 datapoints each with 103 features and 14 possible labels. Each data point has at least one label. As a baseline we first train a logistic regression classifier for each of the 14 labels. To evaluate the performance of these classifiers we predict on a held-out test set and calculate the &lt;a href=&quot;../../modules/model_evaluation#jaccard-similarity-score&quot;&gt;jaccard similarity score&lt;/a&gt;.</source>
          <target state="translated">在此示例中，我们将使用&lt;a href=&quot;http://mldata.org/repository/data/viewslug/yeast&quot;&gt;酵母&lt;/a&gt;数据集，其中包含2417个数据点，每个数据点具有103个特征和14个可能的标签。每个数据点至少具有一个标签。作为基线，我们首先为14个标签中的每一个训练逻辑回归分类器。为了评估这些分类器的性能，我们在保留的测试集上进行预测并计算&lt;a href=&quot;../../modules/model_evaluation#jaccard-similarity-score&quot;&gt;jaccard相似度得分&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="ccd3bf49df3c855961b0d8c881b499df98dfa5eb" translate="yes" xml:space="preserve">
          <source>For this example we will use the &lt;a href=&quot;https://www.openml.org/d/40597&quot;&gt;yeast&lt;/a&gt; dataset which contains 2417 datapoints each with 103 features and 14 possible labels. Each data point has at least one label. As a baseline we first train a logistic regression classifier for each of the 14 labels. To evaluate the performance of these classifiers we predict on a held-out test set and calculate the &lt;a href=&quot;../../modules/model_evaluation#jaccard-similarity-score&quot;&gt;jaccard score&lt;/a&gt; for each sample.</source>
          <target state="translated">在此示例中，我们将使用&lt;a href=&quot;https://www.openml.org/d/40597&quot;&gt;酵母&lt;/a&gt;数据集，其中包含2417个数据点，每个数据点具有103个特征和14个可能的标签。每个数据点至少具有一个标签。作为基线，我们首先为14个标签中的每一个训练逻辑回归分类器。为了评估这些分类器的性能，我们对保留的测试集进行了预测，并计算了每个样本的&lt;a href=&quot;../../modules/model_evaluation#jaccard-similarity-score&quot;&gt;jaccard得分&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="50c8fe3ea12a1b37824eecf96be6564acb86b697" translate="yes" xml:space="preserve">
          <source>For this example, the impurity-based and permutation methods identify the same 2 strongly predictive features but not in the same order. The third most predictive feature, &amp;ldquo;bp&amp;rdquo;, is also the same for the 2 methods. The remaining features are less predictive and the error bars of the permutation plot show that they overlap with 0.</source>
          <target state="translated">对于此示例，基于杂质的方法和置换方法可识别相同的2个强预测特征，但顺序不相同。两种方法的第三个最可预测的特征&amp;ldquo; bp&amp;rdquo;也相同。其余特征的预测性较差，排列图的误差线表明它们与0重叠。</target>
        </trans-unit>
        <trans-unit id="a29265ef3f92733677c4dbea10c38e7e64e8fed9" translate="yes" xml:space="preserve">
          <source>For this example, we load a blood transfusion service center data set from &lt;code&gt;OpenML &amp;lt;https://www.openml.org/d/1464&amp;gt;&lt;/code&gt;. This is a binary classification problem where the target is whether an individual donated blood. Then the data is split into a train and test dataset and a logistic regression is fitted wtih the train dataset.</source>
          <target state="translated">对于此示例，我们从 &lt;code&gt;OpenML &amp;lt;https://www.openml.org/d/1464&amp;gt;&lt;/code&gt; 加载输血服务中心数据集。这是一个二进制分类问题，目标是个人是否献血。然后将数据分为训练和测试数据集，并通过训练数据集进行逻辑回归。</target>
        </trans-unit>
        <trans-unit id="6ab77ac3ad8536d0d4bc113a409f055607cd6e01" translate="yes" xml:space="preserve">
          <source>For this method, M may be a dense matrix, sparse matrix, or general linear operator. Warning: ARPACK can be unstable for some problems. It is best to try several random seeds in order to check results.</source>
          <target state="translated">对于这种方法,M可以是稠密矩阵、稀疏矩阵或一般线性算子。警告:对于某些问题,ARPACK可能不稳定。ARPACK对于某些问题可能是不稳定的.最好是尝试几个随机种子,以便检查结果。</target>
        </trans-unit>
        <trans-unit id="08c8de94919880222510697f43131d88196a6fc1" translate="yes" xml:space="preserve">
          <source>For this particular pattern of missing values we see that &lt;a href=&quot;../../modules/generated/sklearn.ensemble.extratreesregressor#sklearn.ensemble.ExtraTreesRegressor&quot;&gt;&lt;code&gt;sklearn.ensemble.ExtraTreesRegressor&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.linear_model.bayesianridge#sklearn.linear_model.BayesianRidge&quot;&gt;&lt;code&gt;sklearn.linear_model.BayesianRidge&lt;/code&gt;&lt;/a&gt; give the best results.</source>
          <target state="translated">对于这种特殊的缺失值模式，我们看到&lt;a href=&quot;../../modules/generated/sklearn.ensemble.extratreesregressor#sklearn.ensemble.ExtraTreesRegressor&quot;&gt; &lt;code&gt;sklearn.ensemble.ExtraTreesRegressor&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../../modules/generated/sklearn.linear_model.bayesianridge#sklearn.linear_model.BayesianRidge&quot;&gt; &lt;code&gt;sklearn.linear_model.BayesianRidge&lt;/code&gt; &lt;/a&gt;提供了最佳结果。</target>
        </trans-unit>
        <trans-unit id="201d282b655d8d026b78eb9f9255553e50678277" translate="yes" xml:space="preserve">
          <source>For this purpose, the estimators use a &amp;lsquo;connectivity&amp;rsquo; matrix, giving which samples are connected.</source>
          <target state="translated">为此，估计器使用&amp;ldquo;连接性&amp;rdquo;矩阵，给出了连接的样本。</target>
        </trans-unit>
        <trans-unit id="764ea2ccb7951b3db73f825ee916559c0e4bce1d" translate="yes" xml:space="preserve">
          <source>For this reason, the functions that load 20 Newsgroups data provide a parameter called &lt;strong&gt;remove&lt;/strong&gt;, telling it what kinds of information to strip out of each file. &lt;strong&gt;remove&lt;/strong&gt; should be a tuple containing any subset of &lt;code&gt;('headers', 'footers', 'quotes')&lt;/code&gt;, telling it to remove headers, signature blocks, and quotation blocks respectively.</source>
          <target state="translated">因此，加载20个新闻组数据的函数提供了一个称为&lt;strong&gt;remove&lt;/strong&gt;的参数，告诉它&lt;strong&gt;要从&lt;/strong&gt;每个文件中&lt;strong&gt;删除&lt;/strong&gt;哪些信息。&lt;strong&gt;remove&lt;/strong&gt;应该是一个包含 &lt;code&gt;('headers', 'footers', 'quotes')&lt;/code&gt; 任何子集的元组，告诉它分别删除标头，签名块和引号块。</target>
        </trans-unit>
        <trans-unit id="60d82b78a5294ae2dc0ada0318b904b11e85c403" translate="yes" xml:space="preserve">
          <source>For two clusters, SpectralClustering solves a convex relaxation of the &lt;a href=&quot;https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf&quot;&gt;normalised cuts&lt;/a&gt; problem on the similarity graph: cutting the graph in two so that the weight of the edges cut is small compared to the weights of the edges inside each cluster. This criteria is especially interesting when working on images, where graph vertices are pixels, and weights of the edges of the similarity graph are computed using a function of a gradient of the image.</source>
          <target state="translated">对于两个聚类，SpectralClustering解决了相似性图上&lt;a href=&quot;https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf&quot;&gt;归一化切割&lt;/a&gt;问题的凸松弛问题：将图切割成两个，以使所切边缘的权重比每个聚簇内边缘的权重小。当处理图形顶点为像素且使用图形梯度函数计算相似度图边缘的权重的图像时，此标准特别有趣。</target>
        </trans-unit>
        <trans-unit id="f2d2e6058597b408c702846b2d537e901630ce3a" translate="yes" xml:space="preserve">
          <source>For two clusters, it solves a convex relaxation of the &lt;a href=&quot;http://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf&quot;&gt;normalised cuts&lt;/a&gt; problem on the similarity graph: cutting the graph in two so that the weight of the edges cut is small compared to the weights of the edges inside each cluster. This criteria is especially interesting when working on images: graph vertices are pixels, and edges of the similarity graph are a function of the gradient of the image.</source>
          <target state="translated">对于两个聚类，它解决了相似性图上&lt;a href=&quot;http://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf&quot;&gt;归一化切割&lt;/a&gt;问题的凸松弛问题：将图切成两半，以使所切边缘的权重比每个聚类内边缘的权重小。在处理图像时，此标准特别有趣：图的顶点是像素，相似图的边缘是图像的梯度的函数。</target>
        </trans-unit>
        <trans-unit id="4092abbbb6ead577ab2b40e6704455f3cb4d3df5" translate="yes" xml:space="preserve">
          <source>For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data. See the &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;Glossary of Common Terms and API Elements&lt;/a&gt; entry on imputation.</source>
          <target state="translated">由于各种原因，许多现实世界的数据集包含缺失值，通常将其编码为空白，NaN或其他占位符。但是，此类数据集与scikit-learn估计器不兼容，后者假定数组中的所有值都是数字，并且都具有并具有含义。使用不完整数据集的基本策略是丢弃包含缺失值的整个行和/或列。但是，这是以丢失有价值的数据为代价的（即使数据不完整）。更好的策略是估算缺失值，即从数据的已知部分推断出缺失值。参见插补&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;的通用术语表和API元素&lt;/a&gt;条目。</target>
        </trans-unit>
        <trans-unit id="e2303c3ef75d0f928a4dd5ec4517d95f82d7d02b" translate="yes" xml:space="preserve">
          <source>For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data. See the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#glossary&quot;&gt;Glossary of Common Terms and API Elements&lt;/a&gt; entry on imputation.</source>
          <target state="translated">由于各种原因，许多现实世界的数据集包含缺失值，通常将其编码为空白，NaN或其他占位符。但是，此类数据集与scikit-learn估计器不兼容，后者假定数组中的所有值都是数字，并且都具有并具有含义。使用不完整数据集的基本策略是丢弃包含缺失值的整个行和/或列。但是，这是以丢失有价值的数据为代价的（即使数据不完整）。更好的策略是估算缺失值，即从数据的已知部分推断出缺失值。参见插补&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#glossary&quot;&gt;的&amp;ldquo;通用术语表&amp;rdquo;和&amp;ldquo; API元素&amp;rdquo;&lt;/a&gt;条目。</target>
        </trans-unit>
        <trans-unit id="7f12e7919ffa1c3009c9eefff46504b7c0642e13" translate="yes" xml:space="preserve">
          <source>For visualization purpose (which is the main use case of t-SNE), using the Barnes-Hut method is strongly recommended. The exact t-SNE method is useful for checking the theoretically properties of the embedding possibly in higher dimensional space but limit to small datasets due to computational constraints.</source>
          <target state="translated">对于可视化目的(这是t-SNE的主要用途),强烈建议使用Barnes-Hut方法。确切的t-SNE方法对于检查嵌入的理论属性是有用的,可能在更高维度的空间,但由于计算的限制,仅限于小数据集。</target>
        </trans-unit>
        <trans-unit id="e7a5b4b1244321faa67509dff73df9a23d7da1b3" translate="yes" xml:space="preserve">
          <source>For visualization purposes, given a bicluster, the rows and columns of the data matrix may be rearranged to make the bicluster contiguous.</source>
          <target state="translated">为了可视化的目的,给定一个双簇,数据矩阵的行和列可以重新排列,使双簇连续。</target>
        </trans-unit>
        <trans-unit id="d62d3122e2e4eef979e7c46fd629936aec0233be" translate="yes" xml:space="preserve">
          <source>For visualization purposes, we need to lay out the different symbols on a 2D canvas. For this we use &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;Manifold learning&lt;/a&gt; techniques to retrieve 2D embedding.</source>
          <target state="translated">出于可视化目的，我们需要在2D画布上布置不同的符号。为此，我们使用&lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;流形学习&lt;/a&gt;技术来检索2D嵌入。</target>
        </trans-unit>
        <trans-unit id="c502fd7960fae5affa9295a7a329adeddad6ab37" translate="yes" xml:space="preserve">
          <source>Force row-by-row generation by reducing &lt;code&gt;working_memory&lt;/code&gt;:</source>
          <target state="translated">通过减少 &lt;code&gt;working_memory&lt;/code&gt; 强制逐行生成：</target>
        </trans-unit>
        <trans-unit id="4bb98e5d778957b0dd66fa6aed87be22d170768c" translate="yes" xml:space="preserve">
          <source>Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.</source>
          <target state="translated">Forina,M.et al,PARVUS-An Extendible Package for Data Exploration,Classification and Correlation.Institute of Pharmaceutical and Food Analysis and Technologies,Via Brigata Salerno,16147 Genoa,Italy.</target>
        </trans-unit>
        <trans-unit id="35705e005c1f18ed14dab92df9e1435742858283" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the average precision is defined as</source>
          <target state="translated">从形式上看,给定一个二进制指标矩阵的地面真相标签/(y \in \left\{0,1 \right\}^{n_text{samples}\times n_text{labels}})和与每个标签相关的分数/(\hat{f}\in \mathbb{R}^{n_text{samples}\times n_text{labels}}/),平均精度定义为:1.</target>
        </trans-unit>
        <trans-unit id="4f395914b8fb9e643646835cc07cbc38c9742edc" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the coverage is defined as</source>
          <target state="translated">从形式上看,给定一个二进制指标矩阵的地面真相标签(y \in \left\{0,1 \right\}^{n_text{samples}\times n_text{labels}})和与每个标签相关的分数(\hat{f}\in \mathbb{R}^{n_text{samples}\times n_text{labels}}),覆盖率被定义为</target>
        </trans-unit>
        <trans-unit id="c175d46f254d733413b5b0ee831c9d600136a7b6" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the ranking loss is defined as</source>
          <target state="translated">从形式上看,给定一个二进制指标矩阵的地面真相标签/(y \in \left\{0,1 \right\}^{n_text{samples}\times n_text{labels}})和与每个标签相关的分数/(\hat{f}\in \mathbb{R}^{n_text{samples}\times n_text{labels}}/),排名损失定义如下</target>
        </trans-unit>
        <trans-unit id="c45b835372dc3641934de6a0e36f8d5df72bc091" translate="yes" xml:space="preserve">
          <source>Format specification for values in confusion matrix. If &lt;code&gt;None&lt;/code&gt;, the format specification is &amp;lsquo;d&amp;rsquo; or &amp;lsquo;.2g&amp;rsquo; whichever is shorter.</source>
          <target state="translated">混淆矩阵中值的格式规范。如果为 &lt;code&gt;None&lt;/code&gt; ，则格式规范为'd'或'.2g'（以较短者为准）。</target>
        </trans-unit>
        <trans-unit id="47fb5045fef615598469a37da8a59110352753ff" translate="yes" xml:space="preserve">
          <source>Forms an affinity matrix given by the specified function and applies spectral decomposition to the corresponding graph laplacian. The resulting transformation is given by the value of the eigenvectors for each data point.</source>
          <target state="translated">形成一个由指定函数给定的亲和矩阵,并对相应的图形拉普拉斯进行光谱分解。所产生的变换由每个数据点的特征向量值给出。</target>
        </trans-unit>
        <trans-unit id="638babaaa209a18fe959f40f19725a01af068351" translate="yes" xml:space="preserve">
          <source>Fortunately, &lt;strong&gt;most values in X will be zeros&lt;/strong&gt; since for a given document less than a few thousand distinct words will be used. For this reason we say that bags of words are typically &lt;strong&gt;high-dimensional sparse datasets&lt;/strong&gt;. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory.</source>
          <target state="translated">幸运的是，&lt;strong&gt;X中的大多数值将为零，&lt;/strong&gt;因为对于给定的文档，将使用少于数千个不同的单词。因此，我们说单词袋通常是&lt;strong&gt;高维稀疏数据集&lt;/strong&gt;。通过仅将特征向量的非零部分存储在内存中，我们可以节省大量内存。</target>
        </trans-unit>
        <trans-unit id="e399cc71dd11205217c77d4c7f1a914e719b462c" translate="yes" xml:space="preserve">
          <source>Frequency model &amp;ndash; Poisson distribution</source>
          <target state="translated">频率模型&amp;ndash;泊松分布</target>
        </trans-unit>
        <trans-unit id="659b18cdaec75234c8e955e09af5dc004ab6498a" translate="yes" xml:space="preserve">
          <source>Frequently asked questions about the project and contributing.</source>
          <target state="translated">关于项目和贡献的常见问题。</target>
        </trans-unit>
        <trans-unit id="c378e5372fbcc6968de3f23916b1cc385b9617be" translate="yes" xml:space="preserve">
          <source>Friedman et al, &lt;a href=&quot;http://biostatistics.oxfordjournals.org/content/9/3/432.short&quot;&gt;&amp;ldquo;Sparse inverse covariance estimation with the graphical lasso&amp;rdquo;&lt;/a&gt;, Biostatistics 9, pp 432, 2008</source>
          <target state="translated">Friedman等人，&lt;a href=&quot;http://biostatistics.oxfordjournals.org/content/9/3/432.short&quot;&gt;&amp;ldquo;使用图形套索进行稀疏逆协方差估计&amp;rdquo;&lt;/a&gt;，《生物统计》第9期，第432页，2008年</target>
        </trans-unit>
        <trans-unit id="563c92aa7d72e63e351914d407eb4e8a1bed4188" translate="yes" xml:space="preserve">
          <source>Friedman et al, &lt;a href=&quot;https://biostatistics.oxfordjournals.org/content/9/3/432.short&quot;&gt;&amp;ldquo;Sparse inverse covariance estimation with the graphical lasso&amp;rdquo;&lt;/a&gt;, Biostatistics 9, pp 432, 2008</source>
          <target state="translated">Friedman等人，&lt;a href=&quot;https://biostatistics.oxfordjournals.org/content/9/3/432.short&quot;&gt;&amp;ldquo;使用图形套索进行稀疏逆协方差估计&amp;rdquo;&lt;/a&gt;，《生物统计》第9期，第432页，2008年</target>
        </trans-unit>
        <trans-unit id="8438e27109208985a133518d65493568dedc6924" translate="yes" xml:space="preserve">
          <source>Friedman, &amp;ldquo;Stochastic Gradient Boosting&amp;rdquo;, 1999</source>
          <target state="translated">弗里德曼，&amp;ldquo;随机梯度提升&amp;rdquo;，1999年</target>
        </trans-unit>
        <trans-unit id="9fcad16d5a3614a8ac9a3dd3615a46004936d92d" translate="yes" xml:space="preserve">
          <source>Friedman, Stochastic Gradient Boosting, 1999</source>
          <target state="translated">弗里德曼,《随机梯度提升》,1999年。</target>
        </trans-unit>
        <trans-unit id="910211d4464f03643fe19d7b724011f366eafec5" translate="yes" xml:space="preserve">
          <source>Friedmann, Jerome H., 2007, &lt;a href=&quot;https://statweb.stanford.edu/~jhf/ftp/stobst.pdf&quot;&gt;&amp;ldquo;Stochastic Gradient Boosting&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">Friedmann，Jerome H.，2007年，&lt;a href=&quot;https://statweb.stanford.edu/~jhf/ftp/stobst.pdf&quot;&gt;&amp;ldquo;随机梯度提升&amp;rdquo;&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d93c5ad51427861e9927c0f93ba75f21fa7b3769" translate="yes" xml:space="preserve">
          <source>Frobenius norm of the matrix difference, or beta-divergence, between the training data &lt;code&gt;X&lt;/code&gt; and the reconstructed data &lt;code&gt;WH&lt;/code&gt; from the fitted model.</source>
          <target state="translated">训练数据 &lt;code&gt;X&lt;/code&gt; 与拟合模型中的重构数据 &lt;code&gt;WH&lt;/code&gt; 之间的矩阵差或&amp;beta;散度的Frobenius范数。</target>
        </trans-unit>
        <trans-unit id="c51f06e97e3098b2be5170a19f440afe2d031cf5" translate="yes" xml:space="preserve">
          <source>From features with fewest missing values to most.</source>
          <target state="translated">从缺失值最少的特征到最。</target>
        </trans-unit>
        <trans-unit id="42591b8a9cd574128a8414edcfb65dcf2e26248d" translate="yes" xml:space="preserve">
          <source>From features with most missing values to fewest.</source>
          <target state="translated">从缺失值最多到最少的特征。</target>
        </trans-unit>
        <trans-unit id="2a2bd03e6f160e636919837a5a755bde731a1eeb" translate="yes" xml:space="preserve">
          <source>From images</source>
          <target state="translated">来自图片</target>
        </trans-unit>
        <trans-unit id="5ff0ffd1e24dbd90ba4e307313dc3fed8b0cd6c4" translate="yes" xml:space="preserve">
          <source>From occurrences to frequencies</source>
          <target state="translated">从发生到频率</target>
        </trans-unit>
        <trans-unit id="4a6ea847ae49dd26abc66504268644d690f3206b" translate="yes" xml:space="preserve">
          <source>From scikit-learn: [&amp;lsquo;cityblock&amp;rsquo;, &amp;lsquo;cosine&amp;rsquo;, &amp;lsquo;euclidean&amp;rsquo;, &amp;lsquo;l1&amp;rsquo;, &amp;lsquo;l2&amp;rsquo;, &amp;lsquo;manhattan&amp;rsquo;]. These metrics support sparse matrix inputs.</source>
          <target state="translated">来自scikit-learn：['cityblock'，'cosine'，'euclidean'，'l1'，'l2'，'manhattan']。这些指标支持稀疏矩阵输入。</target>
        </trans-unit>
        <trans-unit id="e036581a0079e3a00bbe6bddf83817f4c4f30e31" translate="yes" xml:space="preserve">
          <source>From scikit-learn: [&amp;lsquo;cityblock&amp;rsquo;, &amp;lsquo;cosine&amp;rsquo;, &amp;lsquo;euclidean&amp;rsquo;, &amp;lsquo;l1&amp;rsquo;, &amp;lsquo;l2&amp;rsquo;, &amp;lsquo;manhattan&amp;rsquo;]. These metrics support sparse matrix inputs. [&amp;lsquo;nan_euclidean&amp;rsquo;] but it does not yet support sparse matrices.</source>
          <target state="translated">来自scikit-learn：['cityblock'，'cosine'，'euclidean'，'l1'，'l2'，'manhattan']。这些指标支持稀疏矩阵输入。['nan_euclidean']，但尚不支持稀疏矩阵。</target>
        </trans-unit>
        <trans-unit id="6d8d962b98fbbe50de709ee2f1e71db53d579c2e" translate="yes" xml:space="preserve">
          <source>From scipy.spatial.distance: [&amp;lsquo;braycurtis&amp;rsquo;, &amp;lsquo;canberra&amp;rsquo;, &amp;lsquo;chebyshev&amp;rsquo;, &amp;lsquo;correlation&amp;rsquo;, &amp;lsquo;dice&amp;rsquo;, &amp;lsquo;hamming&amp;rsquo;, &amp;lsquo;jaccard&amp;rsquo;, &amp;lsquo;kulsinski&amp;rsquo;, &amp;lsquo;mahalanobis&amp;rsquo;, &amp;lsquo;minkowski&amp;rsquo;, &amp;lsquo;rogerstanimoto&amp;rsquo;, &amp;lsquo;russellrao&amp;rsquo;, &amp;lsquo;seuclidean&amp;rsquo;, &amp;lsquo;sokalmichener&amp;rsquo;, &amp;lsquo;sokalsneath&amp;rsquo;, &amp;lsquo;sqeuclidean&amp;rsquo;, &amp;lsquo;yule&amp;rsquo;] See the documentation for scipy.spatial.distance for details on these metrics. These metrics do not support sparse matrix inputs.</source>
          <target state="translated">来自scipy.spatial.distance：['braycurtis'，'canberra'，'chebyshev'，'correlation'，'dice，'hamming'，'jaccard'，'kulsinski'，'mahalanobis'，'minkowski'，'rogerstanimoto '，'russellrao'，'seuclidean'，'sokalmichener'，'sokalsneath'，'sqeuclidean'，'yule']有关这些指标的详细信息，请参见scipy.spatial.distance的文档。这些度量标准不支持稀疏矩阵输入。</target>
        </trans-unit>
        <trans-unit id="d3990f36d057d6745fedc272447b2563e02193f7" translate="yes" xml:space="preserve">
          <source>From text</source>
          <target state="translated">来自文本</target>
        </trans-unit>
        <trans-unit id="b8b69c633940e44df1e4f7cf5b91c3ed0ce89b65" translate="yes" xml:space="preserve">
          <source>From the Wikipedia page for Discounted Cumulative Gain:</source>
          <target state="translated">来自维基百科的折扣累积收益页面。</target>
        </trans-unit>
        <trans-unit id="eea6e746f349894e725f780b7f19777ab0b59905" translate="yes" xml:space="preserve">
          <source>From the above formula, it is clear that LDA has a linear decision surface. In the case of QDA, there are no assumptions on the covariance matrices \(\Sigma_k\) of the Gaussians, leading to quadratic decision surfaces. See &lt;a href=&quot;#id5&quot; id=&quot;id2&quot;&gt;1&lt;/a&gt; for more details.</source>
          <target state="translated">从以上公式可以清楚地看出，LDA具有线性决策面。在QDA的情况下，没有关于高斯协方差矩阵\（\ Sigma_k \）的假设，从而导致二次决策面。有关更多详细信息，请参见&lt;a href=&quot;#id5&quot; id=&quot;id2&quot;&gt;1&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="13bce2493b501286a428cebcb4e0bc57e6083c63" translate="yes" xml:space="preserve">
          <source>From the implementation point of view, this is just plain Ordinary Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.</source>
          <target state="translated">从实现的角度来看,这只是普通的普通最小二乘法(scipy.linalg.lstsq),包装成一个预测对象。</target>
        </trans-unit>
        <trans-unit id="704f1c6d00ec317ee90c0b6672883e3dd205ae68" translate="yes" xml:space="preserve">
          <source>From the programming standpoint, it is interesting because it shows how to use the online API of the scikit-learn to process a very large dataset by chunks. The way we proceed is that we load an image at a time and extract randomly 50 patches from this image. Once we have accumulated 500 of these patches (using 10 images), we run the &lt;a href=&quot;../../modules/generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans.partial_fit&quot;&gt;&lt;code&gt;partial_fit&lt;/code&gt;&lt;/a&gt; method of the online KMeans object, MiniBatchKMeans.</source>
          <target state="translated">从编程的角度来看，这很有趣，因为它展示了如何使用scikit-learn的在线API逐块处理非常大的数据集。进行的方式是一次加载一张图像，并从该图像中随机抽取50个色块。一旦我们累积了500个这些补丁（使用10张图像），就可以运行在线KMeans对象MiniBatchKMeans的&lt;a href=&quot;../../modules/generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans.partial_fit&quot;&gt; &lt;code&gt;partial_fit&lt;/code&gt; &lt;/a&gt;方法。</target>
        </trans-unit>
        <trans-unit id="ae8e3bdf9c1967ed71af43f356a6c2d5d1712708" translate="yes" xml:space="preserve">
          <source>From the programming standpoint, it is interesting because it shows how to use the online API of the scikit-learn to process a very large dataset by chunks. The way we proceed is that we load an image at a time and extract randomly 50 patches from this image. Once we have accumulated 500 of these patches (using 10 images), we run the &lt;code&gt;partial_fit&lt;/code&gt; method of the online KMeans object, MiniBatchKMeans.</source>
          <target state="translated">从编程的角度来看，这很有趣，因为它显示了如何使用scikit-learn的在线API来按块处理非常大的数据集。进行的方式是一次加载一张图像，并从该图像中随机抽取50个色块。一旦我们累积了500个这些补丁（使用10张图像），便运行在线KMeans对象MiniBatchKMeans 的 &lt;code&gt;partial_fit&lt;/code&gt; 方法。</target>
        </trans-unit>
        <trans-unit id="f1e410ad1472b42cb42cc98962428637290b6706" translate="yes" xml:space="preserve">
          <source>Function</source>
          <target state="translated">Function</target>
        </trans-unit>
        <trans-unit id="9f410a9e5384dfe1720c4cd228fe7bb63965656b" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores. Default is f_classif (see below &amp;ldquo;See also&amp;rdquo;). The default function only works with classification tasks.</source>
          <target state="translated">函数采用两个数组X和y，并返回一对数组（分数，pvalue）或带分数的单个数组。默认值为f_classif（请参见下文&amp;ldquo;另请参见&amp;rdquo;）。默认功能仅适用于分类任务。</target>
        </trans-unit>
        <trans-unit id="a8696032e0adf35ffec7c9da28cd036adeb91c99" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below &amp;ldquo;See also&amp;rdquo;). The default function only works with classification tasks.</source>
          <target state="translated">函数接受两个数组X和y，并返回一对数组（分数，pvalue）。默认值为f_classif（请参见下文&amp;ldquo;另请参见&amp;rdquo;）。默认功能仅适用于分类任务。</target>
        </trans-unit>
        <trans-unit id="acf1f055cd0885a9fc7d245efda7d1c727fca691" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). For modes &amp;lsquo;percentile&amp;rsquo; or &amp;lsquo;kbest&amp;rsquo; it can return a single array scores.</source>
          <target state="translated">函数接受两个数组X和y，并返回一对数组（分数，pvalue）。对于&amp;ldquo;百分位数&amp;rdquo;或&amp;ldquo; kbest&amp;rdquo;模式，它可以返回单个数组分数。</target>
        </trans-unit>
        <trans-unit id="0c64f21c81859fb42c302c0d2cd301e40332c2c7" translate="yes" xml:space="preserve">
          <source>Function to apply to &lt;code&gt;y&lt;/code&gt; before passing to &lt;code&gt;fit&lt;/code&gt;. Cannot be set at the same time as &lt;code&gt;transformer&lt;/code&gt;. The function needs to return a 2-dimensional array. If &lt;code&gt;func&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, the function used will be the identity function.</source>
          <target state="translated">在传递给 &lt;code&gt;fit&lt;/code&gt; 之前应用于 &lt;code&gt;y&lt;/code&gt; 的函数。不能与 &lt;code&gt;transformer&lt;/code&gt; 同时设置。该函数需要返回一个二维数组。如果 &lt;code&gt;func&lt;/code&gt; 为 &lt;code&gt;None&lt;/code&gt; ，则使用的功能将是身份功能。</target>
        </trans-unit>
        <trans-unit id="f712e33ad68950dd5132b77ad3129994bf2cbbce" translate="yes" xml:space="preserve">
          <source>Function to apply to the prediction of the regressor. Cannot be set at the same time as &lt;code&gt;transformer&lt;/code&gt; as well. The function needs to return a 2-dimensional array. The inverse function is used to return predictions to the same space of the original training labels.</source>
          <target state="translated">应用于预测回归的功能。不能与 &lt;code&gt;transformer&lt;/code&gt; 同时设置。该函数需要返回一个二维数组。逆函数用于将预测返回到原始训练标签的相同空间。</target>
        </trans-unit>
        <trans-unit id="2b961dea1dc0c60ddf9a2c8e9d090f6f7d082483" translate="yes" xml:space="preserve">
          <source>Functions</source>
          <target state="translated">Functions</target>
        </trans-unit>
        <trans-unit id="c216053588b385d3de175b467017426b8b421912" translate="yes" xml:space="preserve">
          <source>Further discussion on the importance of centering and scaling data is available on this FAQ: &lt;a href=&quot;http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html&quot;&gt;Should I normalize/standardize/rescale the data?&lt;/a&gt;</source>
          <target state="translated">常见问题解答中提供了有关对数据进行居中和缩放的重要性的进一步讨论：&lt;a href=&quot;http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html&quot;&gt;是否应该对数据进行归一化/标准化/缩放？&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="606b0f774f1d5e4151969dbb768df62ebca8a20e" translate="yes" xml:space="preserve">
          <source>Further removes the linear correlation across features with &amp;lsquo;whiten=True&amp;rsquo;.</source>
          <target state="translated">进一步删除带有'whiten = True'的特征之间的线性相关性。</target>
        </trans-unit>
        <trans-unit id="ec9ba56eabfa3f70786eb84612f0623df80dfc4d" translate="yes" xml:space="preserve">
          <source>Further, the model supports &lt;a href=&quot;multiclass#multiclass&quot;&gt;multi-label classification&lt;/a&gt; in which a sample can belong to more than one class. For each class, the raw output passes through the logistic function. Values larger or equal to &lt;code&gt;0.5&lt;/code&gt; are rounded to &lt;code&gt;1&lt;/code&gt;, otherwise to &lt;code&gt;0&lt;/code&gt;. For a predicted output of a sample, the indices where the value is &lt;code&gt;1&lt;/code&gt; represents the assigned classes of that sample:</source>
          <target state="translated">此外，该模型支持&lt;a href=&quot;multiclass#multiclass&quot;&gt;多标签分类&lt;/a&gt;，其中样本可以属于一个以上的类别。对于每个类，原始输出通过逻辑函数传递。大于或等于 &lt;code&gt;0.5&lt;/code&gt; 的值将四舍五入为 &lt;code&gt;1&lt;/code&gt; ，否则为 &lt;code&gt;0&lt;/code&gt; 。对于样本的预测输出，值为 &lt;code&gt;1&lt;/code&gt; 的索引表示该样本的分配类别：</target>
        </trans-unit>
        <trans-unit id="cc118108875cca01a2724ce6e20debf4e124a846" translate="yes" xml:space="preserve">
          <source>Furthermore, &lt;a href=&quot;generated/sklearn.metrics.adjusted_rand_score#sklearn.metrics.adjusted_rand_score&quot;&gt;&lt;code&gt;adjusted_rand_score&lt;/code&gt;&lt;/a&gt; is &lt;strong&gt;symmetric&lt;/strong&gt;: swapping the argument does not change the score. It can thus be used as a &lt;strong&gt;consensus measure&lt;/strong&gt;:</source>
          <target state="translated">此外，&lt;a href=&quot;generated/sklearn.metrics.adjusted_rand_score#sklearn.metrics.adjusted_rand_score&quot;&gt; &lt;code&gt;adjusted_rand_score&lt;/code&gt; &lt;/a&gt;是&lt;strong&gt;对称的&lt;/strong&gt;：交换参数不会更改分数。因此，它可以用作&lt;strong&gt;共识措施&lt;/strong&gt;：</target>
        </trans-unit>
        <trans-unit id="f148ccef557451e9c13ec83bfface59d01588547" translate="yes" xml:space="preserve">
          <source>Furthermore, impurity-based feature importance for trees are &lt;strong&gt;strongly biased&lt;/strong&gt; and &lt;strong&gt;favor high cardinality features&lt;/strong&gt; (typically numerical features) over low cardinality features such as binary features or categorical variables with a small number of possible categories.</source>
          <target state="translated">此外，基于杂质的树对树的重要性&lt;strong&gt;很受偏见，&lt;/strong&gt;并且&lt;strong&gt;偏爱高基数的特征&lt;/strong&gt;（通常是数字特征），而不是低基数的特征（例如具有少量可能类别的二元特征或分类变量）。</target>
        </trans-unit>
        <trans-unit id="7218de362b7befd5a71b1a5a01365e3552aa1087" translate="yes" xml:space="preserve">
          <source>Furthermore, it also shows the evolution of the performance of different algorithms with the number of processed examples.</source>
          <target state="translated">此外,它还显示了不同算法的性能随处理实例数量的变化。</target>
        </trans-unit>
        <trans-unit id="17753e7322d4f150d032ddf1f2dbdf4fe6d38592" translate="yes" xml:space="preserve">
          <source>Furthermore, the default parameter &lt;code&gt;smooth_idf=True&lt;/code&gt; adds &amp;ldquo;1&amp;rdquo; to the numerator and denominator as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions:</source>
          <target state="translated">此外，默认参数 &lt;code&gt;smooth_idf=True&lt;/code&gt; 将&amp;ldquo; 1&amp;rdquo;添加到分子和分母，就好像看到一个额外的文档恰好包含一次集合中的每个术语，从而防止了零除法：</target>
        </trans-unit>
        <trans-unit id="2e93583dd7fd8dcf1f0371a9818f0db1fd3c80a7" translate="yes" xml:space="preserve">
          <source>Furthermore, the formulas used to compute tf and idf depend on parameter settings that correspond to the SMART notation used in IR as follows:</source>
          <target state="translated">此外,用于计算tf和idf的公式取决于与IR中使用的SMART符号相对应的参数设置,具体如下:</target>
        </trans-unit>
        <trans-unit id="f576c03970023ff0ede275b819158dc78e4bd414" translate="yes" xml:space="preserve">
          <source>Furthermore, the impurity-based feature importance of random forests suffers from being computed on statistics derived from the training dataset: the importances can be high even for features that are not predictive of the target variable, as long as the model has the capacity to use them to overfit.</source>
          <target state="translated">此外,随机森林的基于杂质的特征重要性受到了来自训练数据集的统计数据的影响:只要模型有能力使用它们进行过拟合,即使是对目标变量没有预测性的特征,其重要性也会很高。</target>
        </trans-unit>
        <trans-unit id="a28e3ed3e4426c3b74ba7f5c5c797d3018edc64c" translate="yes" xml:space="preserve">
          <source>Furthermore, when splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size &lt;code&gt;max_features&lt;/code&gt;. (See the &lt;a href=&quot;#random-forest-parameters&quot;&gt;parameter tuning guidelines&lt;/a&gt; for more details).</source>
          <target state="translated">此外，在树的构造过程中拆分每个节点时，可以从所有输入 &lt;code&gt;max_features&lt;/code&gt; 或大小为max_features的随机子集中找到最佳拆分。（有关更多详细信息，请参阅&lt;a href=&quot;#random-forest-parameters&quot;&gt;参数调整准则&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="2b6ca190d547b1e777d8fa3e93274ce6ad7c42b4" translate="yes" xml:space="preserve">
          <source>G. Brier, &lt;a href=&quot;ftp://ftp.library.noaa.gov/docs.lib/htdocs/rescue/mwr/078/mwr-078-01-0001.pdf&quot;&gt;Verification of forecasts expressed in terms of probability&lt;/a&gt;, Monthly weather review 78.1 (1950)</source>
          <target state="translated">G.布莱尔，&lt;a href=&quot;ftp://ftp.library.noaa.gov/docs.lib/htdocs/rescue/mwr/078/mwr-078-01-0001.pdf&quot;&gt;用概率表示的预报的确认&lt;/a&gt;，每月天气回顾78.1（1950）</target>
        </trans-unit>
        <trans-unit id="8ccf25498da17f5ff69133909511a6d98d2976f3" translate="yes" xml:space="preserve">
          <source>G. Celeux, M. El Anbari, J.-M. Marin, C. P. Robert, &amp;ldquo;Regularization in regression: comparing Bayesian and frequentist methods in a poorly informative situation&amp;rdquo;, 2009.</source>
          <target state="translated">G.Celeux，M.El Anbari，J.-M。Marin，CP罗伯特，&amp;ldquo;回归中的正则化：在信息匮乏的情况下比较贝叶斯方法和频繁主义者的方法&amp;rdquo;，2009年。</target>
        </trans-unit>
        <trans-unit id="623c67fa2cbbcf37d713401c722eceec0b680645" translate="yes" xml:space="preserve">
          <source>G. Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5, Section 5.4.4, pp. 252-253.</source>
          <target state="translated">G.Golub和C.Van Loan.矩阵计算,第三版,第5章,5.4.4节,第252-253页。</target>
        </trans-unit>
        <trans-unit id="755f0c9208b383f3b380dd0d2b1a156d6d5865c4" translate="yes" xml:space="preserve">
          <source>G. James, D. Witten, T. Hastie, R Tibshirani, &lt;a href=&quot;http://www-bcf.usc.edu/~gareth/ISL&quot;&gt;An Introduction to Statistical Learning&lt;/a&gt;, Springer 2013.</source>
          <target state="translated">G. James，D. Witten，T. Hastie，R Tibshirani，&lt;a href=&quot;http://www-bcf.usc.edu/~gareth/ISL&quot;&gt;《统计学习入门》&lt;/a&gt;，Springer 2013。</target>
        </trans-unit>
        <trans-unit id="3cb2eea23f004b3e761d528ddc6b2e3e79458d85" translate="yes" xml:space="preserve">
          <source>G. James, D. Witten, T. Hastie, R Tibshirani, &lt;a href=&quot;https://www-bcf.usc.edu/~gareth/ISL/&quot;&gt;An Introduction to Statistical Learning&lt;/a&gt;, Springer 2013.</source>
          <target state="translated">G.James，D.Witten，T.Hasttie，R Tibshirani，&lt;a href=&quot;https://www-bcf.usc.edu/~gareth/ISL/&quot;&gt;《统计学习概论》&lt;/a&gt;，施普林格2013年。</target>
        </trans-unit>
        <trans-unit id="28ef1689ee2219c624cfde5d7c88afdcae0138ec" translate="yes" xml:space="preserve">
          <source>G. Louppe and P. Geurts, &amp;ldquo;Ensembles on Random Patches&amp;rdquo;, Machine Learning and Knowledge Discovery in Databases, 346-361, 2012.</source>
          <target state="translated">G. Louppe和P. Geurts，&amp;ldquo;随机补丁集合&amp;rdquo;，数据库中的机器学习和知识发现，346-361，2012年。</target>
        </trans-unit>
        <trans-unit id="c722e87d5d9d7dbc54dd2b811a759cc621efb047" translate="yes" xml:space="preserve">
          <source>G. Louppe, &amp;ldquo;Understanding Random Forests: From Theory to Practice&amp;rdquo;, PhD Thesis, U. of Liege, 2014.</source>
          <target state="translated">G. Louppe，&amp;ldquo;理解随机森林：从理论到实践&amp;rdquo;，博士论文，列日大学，2014年。</target>
        </trans-unit>
        <trans-unit id="80030b72580197a6197c00418acf9f08511c2c49" translate="yes" xml:space="preserve">
          <source>G. Ridgeway, &amp;ldquo;Generalized Boosted Models: A guide to the gbm package&amp;rdquo;, 2007</source>
          <target state="translated">G. Ridgeway，&amp;ldquo;广义增强模型：gbm软件包指南&amp;rdquo;，2007年</target>
        </trans-unit>
        <trans-unit id="a8ed6bad205ec1f52f0b48e7f8377435663ec074" translate="yes" xml:space="preserve">
          <source>G.E.P. Box and D.R. Cox, &amp;ldquo;An Analysis of Transformations&amp;rdquo;, Journal of the Royal Statistical Society B, 26, 211-252 (1964).</source>
          <target state="translated">GEP Box和DR Cox，&amp;ldquo;转型分析&amp;rdquo;，皇家统计学会杂志B，第26期，第211-252页（1964年）。</target>
        </trans-unit>
        <trans-unit id="83c6052410f7be2971558c8f2b162b661b4a734b" translate="yes" xml:space="preserve">
          <source>GB builds an additive model in a forward stage-wise fashion. Regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.</source>
          <target state="translated">GB以正向阶段性的方式建立一个加法模型。回归树是在二项式或多项式偏差损失函数的负梯度上拟合的。二元分类是一种特殊情况,只诱导出一棵回归树。</target>
        </trans-unit>
        <trans-unit id="b8cb867b444fe174ab482df0a111ed147a9ceddf" translate="yes" xml:space="preserve">
          <source>GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage &lt;code&gt;n_classes_&lt;/code&gt; regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.</source>
          <target state="translated">GB以渐进的阶段方式建立加性模型；它允许优化任意微分损失函数。在每个阶段，将 &lt;code&gt;n_classes_&lt;/code&gt; 回归树拟合到二项式或多项式偏差损失函数的负梯度上。二进制分类是一种特殊情况，其中仅诱导单个回归树。</target>
        </trans-unit>
        <trans-unit id="80f39c4fc4a6461ea00d5d7be636d9c6f77055de" translate="yes" xml:space="preserve">
          <source>GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.</source>
          <target state="translated">GB以前向阶段性的方式建立一个加法模型;它允许对任意可微分损失函数进行优化。在每个阶段,都会在给定损失函数的负梯度上拟合一棵回归树。</target>
        </trans-unit>
        <trans-unit id="2c1af0078ebec6d87c6fe14b52a6ca7ecb93e0e6" translate="yes" xml:space="preserve">
          <source>GBRT considers additive models of the following form:</source>
          <target state="translated">GBRT考虑以下形式的加法模型。</target>
        </trans-unit>
        <trans-unit id="af90cc1188550654bd22990a09c9155ebaa04680" translate="yes" xml:space="preserve">
          <source>GBRT regressors are additive models whose prediction \(y_i\) for a given input \(x_i\) is of the following form:</source>
          <target state="translated">GBRT回归者是加法模型,其对给定输入的预测(y_i\)为以下形式:</target>
        </trans-unit>
        <trans-unit id="348ddf733ebe39c89fe60cc4aea0def489f0df0c" translate="yes" xml:space="preserve">
          <source>GMM covariances</source>
          <target state="translated">GMM协方差</target>
        </trans-unit>
        <trans-unit id="89a541e422be32f4e38c95b70a35778f6b3b29a5" translate="yes" xml:space="preserve">
          <source>G[i,j] gives the shortest distance from point i to point j along the graph.</source>
          <target state="translated">G[i,j]给出了从点i到点j沿图的最短距离。</target>
        </trans-unit>
        <trans-unit id="dc7da4ca9757d9015c0ba1d2228560006792966e" translate="yes" xml:space="preserve">
          <source>Gallery generated by Sphinx-Gallery</source>
          <target state="translated">画廊由斯芬克斯画廊生成</target>
        </trans-unit>
        <trans-unit id="cba508b12182b68f501d6af46c4f03f8fc5d2473" translate="yes" xml:space="preserve">
          <source>Gamma</source>
          <target state="translated">Gamma</target>
        </trans-unit>
        <trans-unit id="927ef8e7f274c04e9c8836a36352812cc37bb26c" translate="yes" xml:space="preserve">
          <source>Gamma deviance is equivalent to the Tweedie deviance with the power parameter &lt;code&gt;power=2&lt;/code&gt;. It is invariant to scaling of the target variable, and measures relative errors.</source>
          <target state="translated">伽玛偏差等于功率参数 &lt;code&gt;power=2&lt;/code&gt; 的Tweedie偏差。它对于目标变量的定标是不变的，并且可以测量相对误差。</target>
        </trans-unit>
        <trans-unit id="24f0f86d8b8da4a3eb66c5315b49fb7db14a0fa6" translate="yes" xml:space="preserve">
          <source>Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels. Interpretation of the default value is left to the kernel; see the documentation for sklearn.metrics.pairwise. Ignored by other kernels.</source>
          <target state="translated">RBF、laplacian、多项式、指数chi2和sigmoid内核的Gamma参数。默认值的解释由内核决定,参见sklearn.metrics.pairwise的文档。被其他内核忽略。</target>
        </trans-unit>
        <trans-unit id="8abb933fe9bd6d8a92eb104bdc2fd613c351d44f" translate="yes" xml:space="preserve">
          <source>Gamma parameter in rbf, poly and sigmoid kernels. Ignored by other kernels. 0.1 by default.</source>
          <target state="translated">rbf、poly和sigmoid核的gamma参数。其他内核忽略。默认为0.1。</target>
        </trans-unit>
        <trans-unit id="86050f4573c138fca290821e4b579d6d320e40d1" translate="yes" xml:space="preserve">
          <source>Gates, G.W. (1972) &amp;ldquo;The Reduced Nearest Neighbor Rule&amp;rdquo;. IEEE Transactions on Information Theory, May 1972, 431-433.</source>
          <target state="translated">盖茨（Gates）（1972）&amp;ldquo;减少的最近邻居规则&amp;rdquo;。IEEE Transactions on Information Theory，1972年5月，431-433。</target>
        </trans-unit>
        <trans-unit id="46a57bcdd34ea523f3417e94b431a41097b638e9" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Ellipsoids</source>
          <target state="translated">高斯混合物模型 椭圆体</target>
        </trans-unit>
        <trans-unit id="2f22bd1dad8340bd3d8973db40a56083b791482c" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Selection</source>
          <target state="translated">高斯混合物模型选择</target>
        </trans-unit>
        <trans-unit id="7ada59d703243073c5122ce20c200108df4cf582" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Sine Curve</source>
          <target state="translated">高斯混合模型 正弦曲线</target>
        </trans-unit>
        <trans-unit id="b8fb995e81cb89650fea0baec9d6ae8f98a9538f" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Models</source>
          <target state="translated">高斯混合模型</target>
        </trans-unit>
        <trans-unit id="662a25df4ddd527b4e6e6b4415fd19857fcb55fc" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture.</source>
          <target state="translated">高斯混合体。</target>
        </trans-unit>
        <trans-unit id="52d32c3ce740bd6bf6fa9b8c9a00c471e2b8ab61" translate="yes" xml:space="preserve">
          <source>Gaussian Naive Bayes (GaussianNB)</source>
          <target state="translated">高斯奈夫贝叶斯(GaussianNB)</target>
        </trans-unit>
        <trans-unit id="d854cefb413c2902ad18940be1c741ae3117e7e6" translate="yes" xml:space="preserve">
          <source>Gaussian Process for Machine Learning</source>
          <target state="translated">机器学习的高斯过程</target>
        </trans-unit>
        <trans-unit id="3e71cc209c706f89187660af28df9dbd656b7dfb" translate="yes" xml:space="preserve">
          <source>Gaussian Processes regression: basic introductory example</source>
          <target state="translated">高斯过程回归:基本的介绍性例子。</target>
        </trans-unit>
        <trans-unit id="7c9060d2e2a8ab44211d4b8690374c1230f1b7f2" translate="yes" xml:space="preserve">
          <source>Gaussian kernel (&lt;code&gt;kernel = 'gaussian'&lt;/code&gt;)</source>
          <target state="translated">高斯核（ &lt;code&gt;kernel = 'gaussian'&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="16bd9bbb5a5342036acd14278f2e03ad41c57f6a" translate="yes" xml:space="preserve">
          <source>Gaussian mixture model fit with a variational inference.</source>
          <target state="translated">高斯混合物模型拟合与变异推理。</target>
        </trans-unit>
        <trans-unit id="c4278ff51902cddfc2c28028add69085822b616d" translate="yes" xml:space="preserve">
          <source>Gaussian mixture models, useful for clustering, are described in &lt;a href=&quot;mixture#mixture&quot;&gt;another chapter of the documentation&lt;/a&gt; dedicated to mixture models. KMeans can be seen as a special case of Gaussian mixture model with equal covariance per component.</source>
          <target state="translated">高斯混合模型可用于聚类，在专用于混合模型&lt;a href=&quot;mixture#mixture&quot;&gt;的文档的另一章中进行了&lt;/a&gt;描述。KMeans可以看作是高斯混合模型的特例，每个分量具有相同的协方差。</target>
        </trans-unit>
        <trans-unit id="52102b8851b98924c7d8b1f347902fc1a6a2f6c4" translate="yes" xml:space="preserve">
          <source>Gaussian mixtures</source>
          <target state="translated">高斯混合物</target>
        </trans-unit>
        <trans-unit id="fb2ed046d4b5b73ab490df316744dbd7803b27c6" translate="yes" xml:space="preserve">
          <source>Gaussian process classification (GPC) based on Laplace approximation.</source>
          <target state="translated">基于拉普拉斯近似的高斯过程分类(GPC)。</target>
        </trans-unit>
        <trans-unit id="6022eb0f0e245ca9c1dcd7d4b4311ff01e4db354" translate="yes" xml:space="preserve">
          <source>Gaussian process classification (GPC) on iris dataset</source>
          <target state="translated">虹膜数据集上的高斯过程分类(GPC)</target>
        </trans-unit>
        <trans-unit id="21a63bbdb2d774ad21ffa6c87b635dc00ddbdcbd" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR) on Mauna Loa CO2 data.</source>
          <target state="translated">Mauna Loa二氧化碳数据的高斯过程回归(GPR);</target>
        </trans-unit>
        <trans-unit id="0c7b8e025d47923893c509b893c584646dec60f9" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR) with noise-level estimation</source>
          <target state="translated">高斯过程回归(GPR)与噪声水平估计。</target>
        </trans-unit>
        <trans-unit id="e020234a1ce464bccd79fd7ca6cd9571320c3263" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR).</source>
          <target state="translated">高斯过程回归(GPR)。</target>
        </trans-unit>
        <trans-unit id="81d5ab12411c6f249a3ae9ff3884b17e3d00399b" translate="yes" xml:space="preserve">
          <source>Gaussian processes on discrete data structures</source>
          <target state="translated">离散数据结构上的高斯过程</target>
        </trans-unit>
        <trans-unit id="3eef2758f8f04922436ba69e73f365c3b677d080" translate="yes" xml:space="preserve">
          <source>GaussianNaiveBayes tends to push probabilities to 0 or 1 (note the counts in the histograms). This is mainly because it makes the assumption that features are conditionally independent given the class, which is not the case in this dataset which contains 2 redundant features.</source>
          <target state="translated">GaussianNaiveBayes倾向于将概率推到0或1(注意直方图中的计数)。这主要是因为它做出了一个假设,即特征在给定类的情况下是有条件独立的,而在这个包含2个冗余特征的数据集中,情况并非如此。</target>
        </trans-unit>
        <trans-unit id="9ee50bfb8852bcfbfa07c7c7a246c842043563a2" translate="yes" xml:space="preserve">
          <source>General KDD structure :</source>
          <target state="translated">KDD的总体结构</target>
        </trans-unit>
        <trans-unit id="082e84b7a80940ab38b8fafffced3896fbf61a5f" translate="yes" xml:space="preserve">
          <source>General examples about classification algorithms.</source>
          <target state="translated">关于分类算法的一般例子。</target>
        </trans-unit>
        <trans-unit id="340183f53d5a585fe2f90b1573169f80622dc9bd" translate="yes" xml:space="preserve">
          <source>General-purpose, even cluster size, flat geometry, not too many clusters</source>
          <target state="translated">通用型,集群大小均匀,几何形状平坦,集群数量不多。</target>
        </trans-unit>
        <trans-unit id="5a99200d3c187d0fcefb7b4df6803366dc2748df" translate="yes" xml:space="preserve">
          <source>Generalized Linear Model with a Gamma distribution.</source>
          <target state="translated">具有Gamma分布的通用线性模型。</target>
        </trans-unit>
        <trans-unit id="0ed4c66ad535ba7380d74741129413d4f8c145bc" translate="yes" xml:space="preserve">
          <source>Generalized Linear Model with a Poisson distribution.</source>
          <target state="translated">泊松分布的广义线性模型。</target>
        </trans-unit>
        <trans-unit id="d682c681b1fa1783371317722a00ec63b80aa77c" translate="yes" xml:space="preserve">
          <source>Generalized Linear Model with a Tweedie distribution.</source>
          <target state="translated">兑迪分布的广义线性模型。</target>
        </trans-unit>
        <trans-unit id="b17d9222a12b9513aac695dd37d7bdc64c218d77" translate="yes" xml:space="preserve">
          <source>Generalized Linear Models</source>
          <target state="translated">广义线性模型</target>
        </trans-unit>
        <trans-unit id="bcbd479b250088e4214e337494acb2a2758516bc" translate="yes" xml:space="preserve">
          <source>Generalized Linear Models (GLM) extend linear models in two ways &lt;a href=&quot;#id33&quot; id=&quot;id31&quot;&gt;10&lt;/a&gt;. First, the predicted values \(\hat{y}\) are linked to a linear combination of the input variables \(X\) via an inverse link function \(h\) as</source>
          <target state="translated">广义线性模型（GLM）以两种方式扩展线性模型&lt;a href=&quot;#id33&quot; id=&quot;id31&quot;&gt;10&lt;/a&gt;。首先，将预测值\（\ hat {y} \）通过反链接函数\（h \）链接到输入变量\（X \）的线性组合，如下所示：</target>
        </trans-unit>
        <trans-unit id="050c76b497e038e0c5a06ed24dce47a6958dfb02" translate="yes" xml:space="preserve">
          <source>Generalized Linear Models, and Poisson loss for gradient boosting</source>
          <target state="translated">广义线性模型,以及梯度提升的泊松损失。</target>
        </trans-unit>
        <trans-unit id="194ee7e5ec30d094070f5f72a72c8597376dc276" translate="yes" xml:space="preserve">
          <source>Generalized linear models (GLM) for regression</source>
          <target state="translated">用于回归的广义线性模型(GLM)</target>
        </trans-unit>
        <trans-unit id="a807e718c7c2444084ecd599b5293f02618f18b0" translate="yes" xml:space="preserve">
          <source>Generally speaking, when model complexity increases, predictive power and latency are supposed to increase. Increasing predictive power is usually interesting, but for many applications we would better not increase prediction latency too much. We will now review this idea for different families of supervised models.</source>
          <target state="translated">一般来说,当模型复杂度增加时,预测能力和延迟都应该增加。增加预测能力通常是有趣的,但对于很多应用来说,我们最好不要过多地增加预测延迟。现在我们将针对不同系列的监督模型来回顾一下这个想法。</target>
        </trans-unit>
        <trans-unit id="af9887d0c879889fc0d4b97d28831fef1da0e335" translate="yes" xml:space="preserve">
          <source>Generate a distance matrix chunk by chunk with optional reduction</source>
          <target state="translated">逐块生成一个距离矩阵,并可选择还原。</target>
        </trans-unit>
        <trans-unit id="06c2a79c89c40ddc99e314455bfeabb348baaefc" translate="yes" xml:space="preserve">
          <source>Generate a mostly low rank matrix with bell-shaped singular values</source>
          <target state="translated">生成一个具有钟形奇异值的大部分低秩矩阵。</target>
        </trans-unit>
        <trans-unit id="c1825817fcf44112a4d64fe6f2acf131fceae396" translate="yes" xml:space="preserve">
          <source>Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].</source>
          <target state="translated">生成一个新的特征矩阵,由所有度数小于或等于指定度数的特征的多项式组合组成。例如,如果一个输入样本是二维的,形式是[a,b],那么度数2的多项式特征是[1,a,b,a^2,ab,b^2]。</target>
        </trans-unit>
        <trans-unit id="138afdc51f7a90d9b74b5dc5c84735ab7ad5ab97" translate="yes" xml:space="preserve">
          <source>Generate a random multilabel classification problem.</source>
          <target state="translated">生成一个随机多标签分类问题。</target>
        </trans-unit>
        <trans-unit id="6e53d56707f7eb93fc64a285e9e5b0c1571546a7" translate="yes" xml:space="preserve">
          <source>Generate a random n-class classification problem.</source>
          <target state="translated">生成一个随机的n类分类问题。</target>
        </trans-unit>
        <trans-unit id="45b70aa4bfe7b5254dd4845949fd163391dae828" translate="yes" xml:space="preserve">
          <source>Generate a random regression problem with sparse uncorrelated design</source>
          <target state="translated">生成一个具有稀疏非相关设计的随机回归问题。</target>
        </trans-unit>
        <trans-unit id="097811da2f026de1c67525043ab17d6d057450a6" translate="yes" xml:space="preserve">
          <source>Generate a random regression problem.</source>
          <target state="translated">生成一个随机回归问题。</target>
        </trans-unit>
        <trans-unit id="90aba5bbbbad8863550c06ced91ee520b1c0caff" translate="yes" xml:space="preserve">
          <source>Generate a random symmetric, positive-definite matrix.</source>
          <target state="translated">生成一个随机对称的正定矩阵。</target>
        </trans-unit>
        <trans-unit id="b303920886f4c442ac72ea67b8bd3cb1b7460430" translate="yes" xml:space="preserve">
          <source>Generate a signal as a sparse combination of dictionary elements.</source>
          <target state="translated">产生一个信号,作为字典元素的稀疏组合。</target>
        </trans-unit>
        <trans-unit id="035b22a208f9d34d7467f70a3f8e5a4c27edb9b2" translate="yes" xml:space="preserve">
          <source>Generate a sparse random projection matrix</source>
          <target state="translated">生成一个稀疏的随机投影矩阵。</target>
        </trans-unit>
        <trans-unit id="4dc557ac054fd2b6925cea078345560226a5469c" translate="yes" xml:space="preserve">
          <source>Generate a sparse symmetric definite positive matrix.</source>
          <target state="translated">生成一个稀疏的对称定正矩阵。</target>
        </trans-unit>
        <trans-unit id="2b6ed08a20bd86f602cf70906530ae751a13aa6a" translate="yes" xml:space="preserve">
          <source>Generate a swiss roll dataset.</source>
          <target state="translated">生成一个瑞士卷数据集。</target>
        </trans-unit>
        <trans-unit id="2f7e815b3b193bc1cd3e7e4a28307316625909c7" translate="yes" xml:space="preserve">
          <source>Generate an S curve dataset.</source>
          <target state="translated">生成一个S曲线数据集。</target>
        </trans-unit>
        <trans-unit id="a97cf86ca659bda28267893fc11990f8622b62e7" translate="yes" xml:space="preserve">
          <source>Generate an array with block checkerboard structure for biclustering.</source>
          <target state="translated">生成一个具有块状棋盘结构的数组进行双聚类。</target>
        </trans-unit>
        <trans-unit id="a9f13a8783d09446e6122b3e3234e1d6fcb95591" translate="yes" xml:space="preserve">
          <source>Generate an array with constant block diagonal structure for biclustering.</source>
          <target state="translated">生成一个具有恒定块对角线结构的数组进行双聚类。</target>
        </trans-unit>
        <trans-unit id="afeaee3f091598162e7eb33b08779a77e0e748f4" translate="yes" xml:space="preserve">
          <source>Generate cross-validated estimates for each input data point</source>
          <target state="translated">为每个输入数据点生成交叉验证的估计值</target>
        </trans-unit>
        <trans-unit id="99b9ba538a40d50737f63d924a3c7ce27d75993f" translate="yes" xml:space="preserve">
          <source>Generate datasets. We choose the size big enough to see the scalability of the algorithms, but not too big to avoid too long running times</source>
          <target state="translated">生成数据集。我们选择足够大的尺寸,以查看算法的可扩展性,但不要太大,以避免运行时间过长。</target>
        </trans-unit>
        <trans-unit id="c00dd920cc2725de42546dcb337634c4ac897029" translate="yes" xml:space="preserve">
          <source>Generate indices to split data into training and test set.</source>
          <target state="translated">生成指数,将数据分割成训练集和测试集。</target>
        </trans-unit>
        <trans-unit id="5107cc8a6ff57cac684ccce1f62420eaa4260507" translate="yes" xml:space="preserve">
          <source>Generate isotropic Gaussian and label samples by quantile</source>
          <target state="translated">生成各向同性高斯,并通过分位数标记样本</target>
        </trans-unit>
        <trans-unit id="8e89de3bc63d92fa78eda36337c27db80aab71fe" translate="yes" xml:space="preserve">
          <source>Generate isotropic Gaussian blobs for clustering.</source>
          <target state="translated">生成各向同性高斯布点进行聚类。</target>
        </trans-unit>
        <trans-unit id="37d03dbfefb10390fe483e5ed2d7b03c5a459fa1" translate="yes" xml:space="preserve">
          <source>Generate missing values indicator for X.</source>
          <target state="translated">生成X的缺失值指标。</target>
        </trans-unit>
        <trans-unit id="462cab2784077aa54955d18bb40a9de12e6edf3c" translate="yes" xml:space="preserve">
          <source>Generate polynomial and interaction features.</source>
          <target state="translated">生成多项式和交互特征。</target>
        </trans-unit>
        <trans-unit id="d1bba874447d3710a4261bda204e3775c6148149" translate="yes" xml:space="preserve">
          <source>Generate random samples from the fitted Gaussian distribution.</source>
          <target state="translated">从拟合的高斯分布中产生随机样本。</target>
        </trans-unit>
        <trans-unit id="ce67c2d91c83a1d56ab9a9ee35d822063af6506a" translate="yes" xml:space="preserve">
          <source>Generate random samples from the model.</source>
          <target state="translated">从模型中产生随机样本。</target>
        </trans-unit>
        <trans-unit id="077b466863e9f097ef6d30c373ea7fea91f90736" translate="yes" xml:space="preserve">
          <source>Generate test sets such that all contain the same distribution of classes, or as close as possible.</source>
          <target state="translated">生成测试集,使所有的测试集都包含相同的类分布,或者尽可能的接近。</target>
        </trans-unit>
        <trans-unit id="7afab3e6555db4edb28194e580e8ac980040a53c" translate="yes" xml:space="preserve">
          <source>Generate test sets where the smallest and largest differ by at most one sample.</source>
          <target state="translated">生成最小和最大相差最多一个样本的测试集。</target>
        </trans-unit>
        <trans-unit id="f4defed702b6f02ff908f1cd9f8b411c35ee40dd" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #1&amp;rdquo; regression problem</source>
          <target state="translated">生成&amp;ldquo; Friedman＃1&amp;rdquo;回归问题</target>
        </trans-unit>
        <trans-unit id="75088d435099809ee2a5f0ec830b6e2b26fb0500" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #2&amp;rdquo; regression problem</source>
          <target state="translated">生成&amp;ldquo; Friedman＃2&amp;rdquo;回归问题</target>
        </trans-unit>
        <trans-unit id="18ca02f4b303dec3c31289cd6db22246b19d8adb" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #3&amp;rdquo; regression problem</source>
          <target state="translated">生成&amp;ldquo; Friedman＃3&amp;rdquo;回归问题</target>
        </trans-unit>
        <trans-unit id="1526c84b2e9b495f9ed3216009ebf8b31d461518" translate="yes" xml:space="preserve">
          <source>Generates data for binary classification used in Hastie et al.</source>
          <target state="translated">生成用于Hastie等人的二元分类的数据。</target>
        </trans-unit>
        <trans-unit id="c2cb269fed6a06711794c0a014b9a89e92300ddb" translate="yes" xml:space="preserve">
          <source>Generates data for binary classification used in Hastie et al. 2009, Example 10.2.</source>
          <target state="translated">生成Hastie等人2009年使用的二元分类数据,例10.2。</target>
        </trans-unit>
        <trans-unit id="fbfd61fc35f16aea2f376426724b313bf45b644a" translate="yes" xml:space="preserve">
          <source>Generates indices to split data into training and test set.</source>
          <target state="translated">生成指数,将数据分割成训练集和测试集。</target>
        </trans-unit>
        <trans-unit id="9a963ad633fdf36ff4f1d429308e1f3d90a2ceea" translate="yes" xml:space="preserve">
          <source>Generates train/test indices based on predefined splits.</source>
          <target state="translated">根据预定义的分割生成训练/测试指数。</target>
        </trans-unit>
        <trans-unit id="4678269441c5cad2dec162c29e80b19e70944794" translate="yes" xml:space="preserve">
          <source>Generates train/test indices based on random permutation.</source>
          <target state="translated">基于随机排列生成训练/测试指数。</target>
        </trans-unit>
        <trans-unit id="025efadf6f18cb5d61732c8188dd311431f2fe8b" translate="yes" xml:space="preserve">
          <source>Generator on parameters sampled from given distributions.</source>
          <target state="translated">从给定分布中取样的参数生成器。</target>
        </trans-unit>
        <trans-unit id="6d76c76581c79bfcc7307e6698c50d3852025179" translate="yes" xml:space="preserve">
          <source>Generator that yields (estimator, check) tuples. Returned when &lt;code&gt;generate_only=True&lt;/code&gt;.</source>
          <target state="translated">生成（估计器，检查）元组的生成器。当 &lt;code&gt;generate_only=True&lt;/code&gt; 时返回。</target>
        </trans-unit>
        <trans-unit id="9008c79b50b6e856f48dd8a1acb75bd481c83565" translate="yes" xml:space="preserve">
          <source>Generator to create n_packs slices going up to n.</source>
          <target state="translated">生成n_packs切片,最多可生成n个。</target>
        </trans-unit>
        <trans-unit id="6a34af9aa1c17133e53bdde13fa952c7bcbcf3f6" translate="yes" xml:space="preserve">
          <source>Geometry (metric used)</source>
          <target state="translated">几何学(公制)</target>
        </trans-unit>
        <trans-unit id="e5f048789e3e59e8993091df470af502112331aa" translate="yes" xml:space="preserve">
          <source>George W Bush</source>
          <target state="translated">乔治-布什</target>
        </trans-unit>
        <trans-unit id="b583db923d23716d80d92ca8bb6a609aa1f738a2" translate="yes" xml:space="preserve">
          <source>Gerhard Schroeder</source>
          <target state="translated">格哈德-施罗德</target>
        </trans-unit>
        <trans-unit id="33868dad5f60b783d41cfb7c4e686fd5af82ea02" translate="yes" xml:space="preserve">
          <source>Get a list of all estimators from sklearn.</source>
          <target state="translated">从sklearn获取所有估算师的名单。</target>
        </trans-unit>
        <trans-unit id="c89b4f911ae16fa0b7caa09ce0c140306df6a7bd" translate="yes" xml:space="preserve">
          <source>Get a mask, or integer index, of the features selected</source>
          <target state="translated">获取所选特征的掩码或整数索引。</target>
        </trans-unit>
        <trans-unit id="72908cf84377de645c7534a22afeddeeaba91d9d" translate="yes" xml:space="preserve">
          <source>Get a scorer from string</source>
          <target state="translated">从字符串中获取一个得分器</target>
        </trans-unit>
        <trans-unit id="892dda63b5e110479cdb36e62f1ec2fd9807071b" translate="yes" xml:space="preserve">
          <source>Get a scorer from string.</source>
          <target state="translated">从字符串中获取一个得分器。</target>
        </trans-unit>
        <trans-unit id="9077d712fdd7764174aa9d64af32e58e63a20fd2" translate="yes" xml:space="preserve">
          <source>Get data and node arrays.</source>
          <target state="translated">获取数据和节点数组。</target>
        </trans-unit>
        <trans-unit id="45a250b2600ca82b0e59f392e6c981ee3cc2728d" translate="yes" xml:space="preserve">
          <source>Get feature names from all transformers.</source>
          <target state="translated">从所有变压器中获取特征名称。</target>
        </trans-unit>
        <trans-unit id="29212f8ab4fb514c62f70555a85d5fbb976ec617" translate="yes" xml:space="preserve">
          <source>Get number of calls.</source>
          <target state="translated">获取电话数量。</target>
        </trans-unit>
        <trans-unit id="4be0c520942fc8926cfd53e42cd4ae1d1cc70df9" translate="yes" xml:space="preserve">
          <source>Get parameters for this estimator.</source>
          <target state="translated">获取该估计器的参数。</target>
        </trans-unit>
        <trans-unit id="fe15f50ace10fe1b8c70139542f4a1796682abb3" translate="yes" xml:space="preserve">
          <source>Get parameters of this kernel.</source>
          <target state="translated">获取该内核的参数。</target>
        </trans-unit>
        <trans-unit id="1314abe875bac1db97b1a7155d7b4a8c13c230ee" translate="yes" xml:space="preserve">
          <source>Get predictions from each split of cross-validation for diagnostic purposes.</source>
          <target state="translated">从交叉验证的每个分割中获得预测,以达到诊断的目的。</target>
        </trans-unit>
        <trans-unit id="dd0a065fc935a1fd709e1a1d7d55ca6c3433dca5" translate="yes" xml:space="preserve">
          <source>Get the given distance metric from the string identifier.</source>
          <target state="translated">从字符串标识符中获取给定的距离度量。</target>
        </trans-unit>
        <trans-unit id="d1f7f6e0092ef00a29852a7f857762682fb899cd" translate="yes" xml:space="preserve">
          <source>Get the parameters of an estimator from the ensemble.</source>
          <target state="translated">从集合中获取估计器的参数。</target>
        </trans-unit>
        <trans-unit id="df2089c702273c8bc78b6842775813fe9702ad55" translate="yes" xml:space="preserve">
          <source>Get the parameters of the VotingClassifier</source>
          <target state="translated">获取VotingClassifier的参数。</target>
        </trans-unit>
        <trans-unit id="44fa9d84cdb2287aa5766955eab26611c0998b04" translate="yes" xml:space="preserve">
          <source>Get tree status.</source>
          <target state="translated">获取树的状态。</target>
        </trans-unit>
        <trans-unit id="1f6030226293d5ed7b4d4b045e215d6de20db61c" translate="yes" xml:space="preserve">
          <source>Getter for the precision matrix.</source>
          <target state="translated">精密矩阵的获取器。</target>
        </trans-unit>
        <trans-unit id="24670d1cd19283e4b5f2e1096ab423493375ec8f" translate="yes" xml:space="preserve">
          <source>Gibbs sampling from visible and hidden layers.</source>
          <target state="translated">从可见层和隐藏层进行吉布斯采样。</target>
        </trans-unit>
        <trans-unit id="53379a8bafa1cbd8bc5da14050f14d3817f01039" translate="yes" xml:space="preserve">
          <source>Given 2 multivariate covarying two-dimensional datasets, X, and Y, PLS extracts the &amp;lsquo;directions of covariance&amp;rsquo;, i.e. the components of each datasets that explain the most shared variance between both datasets. This is apparent on the &lt;strong&gt;scatterplot matrix&lt;/strong&gt; display: components 1 in dataset X and dataset Y are maximally correlated (points lie around the first diagonal). This is also true for components 2 in both dataset, however, the correlation across datasets for different components is weak: the point cloud is very spherical.</source>
          <target state="translated">给定2个多元协变二维数据集X和Y，PLS提取&amp;ldquo;协方差方向&amp;rdquo;，即解释两个数据集之间共享程度最大的每个数据集的组成部分。这在&lt;strong&gt;散点图矩阵&lt;/strong&gt;显示中很明显：数据集X和数据集Y中的分量1最大相关（点位于第一个对角线上）。这两个数据集中的分量2也是如此，但是，数据集中不同分量之间的相关性很弱：点云非常球形。</target>
        </trans-unit>
        <trans-unit id="16179644ab5a4c2a1f730ff634ab3d4d3a869791" translate="yes" xml:space="preserve">
          <source>Given a candidate centroid \(x_i\) for iteration \(t\), the candidate is updated according to the following equation:</source>
          <target state="translated">给定一个迭代的候选中心点(x_i/),候选中心点按以下公式更新。</target>
        </trans-unit>
        <trans-unit id="4368fa47ed8eb35b757e7b3d5aaf6d7ee1cd4ff6" translate="yes" xml:space="preserve">
          <source>Given a dataset with two features, we let the encoder find the unique values per feature and transform the data to a binary one-hot encoding.</source>
          <target state="translated">给定一个具有两个特征的数据集,我们让编码器找到每个特征的唯一值,并将数据转化为二进制一热编码。</target>
        </trans-unit>
        <trans-unit id="a65060cb3a96ad97e8800308b9076a9a49180060" translate="yes" xml:space="preserve">
          <source>Given a dataset with two features, we let the encoder find the unique values per feature and transform the data to an ordinal encoding.</source>
          <target state="translated">给定一个具有两个特征的数据集,我们让编码器找到每个特征的唯一值,并将数据转换为序数编码。</target>
        </trans-unit>
        <trans-unit id="6cc0cdb4252ae3fe585bd759a612161dfe7c6d85" translate="yes" xml:space="preserve">
          <source>Given a set of training examples \((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\) where \(x_i \in \mathbf{R}^n\) and \(y_i \in \{0, 1\}\), a one hidden layer one hidden neuron MLP learns the function \(f(x) = W_2 g(W_1^T x + b_1) + b_2\) where \(W_1 \in \mathbf{R}^m\) and \(W_2, b_1, b_2 \in \mathbf{R}\) are model parameters. \(W_1, W_2\) represent the weights of the input layer and hidden layer, respectively; and \(b_1, b_2\) represent the bias added to the hidden layer and the output layer, respectively. \(g(\cdot) : R \rightarrow R\) is the activation function, set by default as the hyperbolic tan. It is given as,</source>
          <target state="translated">给出一组训练例子((x_1,y_1),(x_2,y_2),(x_n,y_n))其中(x_i在mathbf{R}^n)和(y_i在{0,1}/))。一个一隐藏层一隐藏神经元MLP学习的函数/(f(x)=W_2 g(W_1^T x+b_1)+b_2/),其中/(W_1 in \mathbf{R}^m)和/(W_2,b_1,b_2 \in \mathbf{R}/)是模型参数。\(W_1,W_2\)分别代表输入层和隐藏层的权重;\(b_1,b_2\)分别代表加在隐藏层和输出层的偏置。\(g(\cdot):R \rightarrow R\)为激活函数,默认设置为双曲tan。它的给定值为:。</target>
        </trans-unit>
        <trans-unit id="c774496cc27fa29850b7be4385a4e837807fe19c" translate="yes" xml:space="preserve">
          <source>Given a set of training examples \((x_1, y_1), \ldots, (x_n, y_n)\) where \(x_i \in \mathbf{R}^m\) and \(y_i \in \mathcal{R}\) (\(y_i \in {-1, 1}\) for classification), our goal is to learn a linear scoring function \(f(x) = w^T x + b\) with model parameters \(w \in \mathbf{R}^m\) and intercept \(b \in \mathbf{R}\). In order to make predictions for binary classification, we simply look at the sign of \(f(x)\). To find the model parameters, we minimize the regularized training error given by</source>
          <target state="translated">给出一组训练例子((x_1,y_1),ldots,(x_n,y_n)),其中(x_i在mathbf{R}^m)和(y_i在mathcal{R})((y_i在{-1,1})进行分类)。我们的目标是学习一个线性计分函数/(f(x)=w^T x+b/)的模型参数/(w in \mathbf{R}^m\)和截距/(b in \mathbf{R})。为了进行二元分类的预测,我们只需观察/(f(x)/)的符号。为了找到模型参数,我们将正则化训练误差最小化,由以下公式给出。</target>
        </trans-unit>
        <trans-unit id="99b85508f1069fad6e9945b3624fea4140b5fbae" translate="yes" xml:space="preserve">
          <source>Given a set of training examples \((x_1, y_1), \ldots, (x_n, y_n)\) where \(x_i \in \mathbf{R}^m\) and \(y_i \in \{-1,1\}\), our goal is to learn a linear scoring function \(f(x) = w^T x + b\) with model parameters \(w \in \mathbf{R}^m\) and intercept \(b \in \mathbf{R}\). In order to make predictions, we simply look at the sign of \(f(x)\). A common choice to find the model parameters is by minimizing the regularized training error given by</source>
          <target state="translated">给出一组训练示例(((x_1,y_1),ldots,(x_n,y_n)\)其中 \(x_i \mathbf{R}^m)和 \(y_i \mathbf{R}^m)。我们的目标是学习一个线性评分函数(f(x)=w^T x+b\),其模型参数为(w \in \mathbf{R}^m\)和截距(b \in \mathbf{R})。为了进行预测,我们只需看一下\(f(x)\)的符号。寻找模型参数的一个常见选择是通过最小化正则化训练误差来寻找模型参数,给定的模型参数如下所示。</target>
        </trans-unit>
        <trans-unit id="f05ffd1dc56829aeb2ce3b1aa47183d5a5a71272" translate="yes" xml:space="preserve">
          <source>Given an exception, a callable to raise the exception, and a message string, tests that the correct exception is raised and that the message is a substring of the error thrown. Used to test that the specific message thrown during an exception is correct.</source>
          <target state="translated">给定一个异常、一个引发异常的可调用对象和一个消息字符串,测试是否引发了正确的异常,并且消息是抛出的错误的子串。用于测试在异常期间抛出的特定消息是否正确。</target>
        </trans-unit>
        <trans-unit id="d5588778e54082615cf481fafbc7dcf0b337d76d" translate="yes" xml:space="preserve">
          <source>Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (&lt;a href=&quot;generated/sklearn.feature_selection.rfe#sklearn.feature_selection.RFE&quot;&gt;&lt;code&gt;RFE&lt;/code&gt;&lt;/a&gt;) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a &lt;code&gt;coef_&lt;/code&gt; attribute or through a &lt;code&gt;feature_importances_&lt;/code&gt; attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.</source>
          <target state="translated">给定将权重分配给特征（例如线性模型的系数）的外部估计器，递归特征消除（&lt;a href=&quot;generated/sklearn.feature_selection.rfe#sklearn.feature_selection.RFE&quot;&gt; &lt;code&gt;RFE&lt;/code&gt; &lt;/a&gt;）将通过递归考虑越来越少的特征集来选择特征。首先，对估计器进行初始特征集训练，并通过 &lt;code&gt;coef_&lt;/code&gt; 属性或 &lt;code&gt;feature_importances_&lt;/code&gt; 属性获得每个特征的重要性。然后，从当前的一组特征中删除最不重要的特征，然后对该过程进行递归重复，直到最终达到所需的特征数量。</target>
        </trans-unit>
        <trans-unit id="0a3e62329db7e0582a525546102e4bc5a3e414ee" translate="yes" xml:space="preserve">
          <source>Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a &lt;code&gt;coef_&lt;/code&gt; attribute or through a &lt;code&gt;feature_importances_&lt;/code&gt; attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.</source>
          <target state="translated">给定一个将权重分配给特征（例如线性模型的系数）的外部估计器，递归特征消除（RFE）的目标是通过递归考虑越来越少的特征集来选择特征。首先，对估计器进行初始特征集训练，并通过 &lt;code&gt;coef_&lt;/code&gt; 属性或 &lt;code&gt;feature_importances_&lt;/code&gt; 属性获得每个特征的重要性。然后，从当前功能集中删除最不重要的功能。在修剪的集上递归地重复该过程，直到最终达到所需的要选择的特征数量。</target>
        </trans-unit>
        <trans-unit id="2e4a90e9413cabdb8d0d79c137af8efe3fbd16ef" translate="yes" xml:space="preserve">
          <source>Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the &lt;code&gt;init='k-means++'&lt;/code&gt; parameter). This initializes the centroids to be (generally) distant from each other, leading to provably better results than random initialization, as shown in the reference.</source>
          <target state="translated">给定足够的时间，K均值将始终收敛，但这可能是局部最小值。这在很大程度上取决于质心的初始化。结果，该计算通常使用质心的不同初始化进行多次。解决此问题的一种方法是k-means ++初始化方案，该方案已在scikit-learn中实现（使用 &lt;code&gt;init='k-means++'&lt;/code&gt; 参数）。如参考文献中所示，这将质心初始化为（通常）彼此远离，从而导致可证明比随机初始化更好的结果。</target>
        </trans-unit>
        <trans-unit id="74d4aecb20e2cdcd5c8865136aad914eecac7d61" translate="yes" xml:space="preserve">
          <source>Given the iris dataset, if we knew that there were 3 types of iris, but did not have access to a taxonomist to label them: we could try a &lt;strong&gt;clustering task&lt;/strong&gt;: split the observations into well-separated group called &lt;em&gt;clusters&lt;/em&gt;.</source>
          <target state="translated">给定虹膜数据集，如果我们知道虹膜有3种类型，但没有分类学家来标记它们的话：我们可以尝试&lt;strong&gt;聚类任务&lt;/strong&gt;：将观察结果分成分离良好的组，称为&lt;em&gt;clusters&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="7ffdaa4cdda4b54b62086a7f5ac68bd7ea3b5908" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments &lt;code&gt;labels_true&lt;/code&gt; and our clustering algorithm assignments of the same samples &lt;code&gt;labels_pred&lt;/code&gt;, the &lt;strong&gt;Mutual Information&lt;/strong&gt; is a function that measures the &lt;strong&gt;agreement&lt;/strong&gt; of the two assignments, ignoring permutations. Two different normalized versions of this measure are available, &lt;strong&gt;Normalized Mutual Information (NMI)&lt;/strong&gt; and &lt;strong&gt;Adjusted Mutual Information (AMI)&lt;/strong&gt;. NMI is often used in the literature, while AMI was proposed more recently and is &lt;strong&gt;normalized against chance&lt;/strong&gt;:</source>
          <target state="translated">有了基本事实类分配 &lt;code&gt;labels_true&lt;/code&gt; 和我们的相同样本 &lt;code&gt;labels_pred&lt;/code&gt; 的聚类算法分配的知识，&lt;strong&gt;互信息&lt;/strong&gt;就是一个函数，它测量两个分配的&lt;strong&gt;一致性&lt;/strong&gt;，而忽略排列。可以使用此度量的两个不同的规范化版本：&lt;strong&gt;规范化互信息（NMI）&lt;/strong&gt;和&lt;strong&gt;调整后的互信息（AMI）&lt;/strong&gt;。 NMI在文献中经常使用，而AMI是最近才提出的，并且&lt;strong&gt;针对偶然性进行了标准化&lt;/strong&gt;：</target>
        </trans-unit>
        <trans-unit id="943836cb04e0640667940c68f56d5deeb3e35898" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments &lt;code&gt;labels_true&lt;/code&gt; and our clustering algorithm assignments of the same samples &lt;code&gt;labels_pred&lt;/code&gt;, the &lt;strong&gt;adjusted Rand index&lt;/strong&gt; is a function that measures the &lt;strong&gt;similarity&lt;/strong&gt; of the two assignments, ignoring permutations and &lt;strong&gt;with chance normalization&lt;/strong&gt;:</source>
          <target state="translated">有了基本事实类分配 &lt;code&gt;labels_true&lt;/code&gt; 和我们的相同样本 &lt;code&gt;labels_pred&lt;/code&gt; 的聚类算法分配的知识，&lt;strong&gt;调整后的兰德指数&lt;/strong&gt;就是一个函数，它测量两个分配的&lt;strong&gt;相似性&lt;/strong&gt;，而忽略了排列并&lt;strong&gt;通过机会归一化&lt;/strong&gt;：</target>
        </trans-unit>
        <trans-unit id="3a989bbd6a98db5dab53799fee5637e2080ce141" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments of the samples, it is possible to define some intuitive metric using conditional entropy analysis.</source>
          <target state="translated">鉴于对样本的地真类赋值的了解,可以用条件熵分析来定义一些直观的度量。</target>
        </trans-unit>
        <trans-unit id="4d7a7b1af5c7c7276434270fce7100038c705add" translate="yes" xml:space="preserve">
          <source>Given these singular vectors, they are ranked according to which can be best approximated by a piecewise-constant vector. The approximations for each vector are found using one-dimensional k-means and scored using the Euclidean distance. Some subset of the best left and right singular vector are selected. Next, the data is projected to this best subset of singular vectors and clustered.</source>
          <target state="translated">给定这些奇异向量,根据哪个向量能被一个计件常数向量最好地逼近进行排序。使用一维k-means找到每个向量的近似值,并使用欧氏距离进行评分。选出一些最好的左、右奇异向量的子集。接下来,将数据投射到这个最佳子集的奇异向量上,并进行聚类。</target>
        </trans-unit>
        <trans-unit id="21675a464e2ca3b8f99eef191d00e106aa21c0dd" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in R^n\), i=1,&amp;hellip;, l and a label vector \(y \in R^l\), a decision tree recursively partitions the space such that the samples with the same labels are grouped together.</source>
          <target state="translated">给定训练向量\（x_i \ in R ^ n \），i = 1，&amp;hellip;，l和标签向量\（y \ in R ^ l \），决策树递归地划分空间，使得具有相同样本的样本标签分组在一起。</target>
        </trans-unit>
        <trans-unit id="02fd4db44c84fce9026584422f7727ba079bc40a" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in \mathbb{R}^p\), i=1,&amp;hellip;, n, and a vector \(y \in \mathbb{R}^n\)\(\varepsilon\)-SVR solves the following primal problem:</source>
          <target state="translated">给定训练向量\（x_i \ in \ mathbb {R} ^ p \），i = 1，&amp;hellip;，n和向量\（y \ in \ mathbb {R} ^ p \）\（\ varepsilon \）- SVR解决了以下主要问题：</target>
        </trans-unit>
        <trans-unit id="70e397398a5003e0a6b00de067e9804bfe571e70" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in \mathbb{R}^p\), i=1,&amp;hellip;, n, in two classes, and a vector \(y \in \{1, -1\}^n\), SVC solves the following primal problem:</source>
          <target state="translated">给定训练向量\（x_i \ in \ mathbb {R} ^ p \），在两个类别中i = 1，&amp;hellip;，n，以及向量\（y \ in \ {1，-1 \} ^ n \） ，SVC解决了以下主要问题：</target>
        </trans-unit>
        <trans-unit id="e43c2f871d10fa4875c4f15e109aeb5faf94fb18" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in \mathbb{R}^p\), i=1,&amp;hellip;, n, in two classes, and a vector \(y \in \{1, -1\}^n\), our goal is to find \(w \in \mathbb{R}^p\) and \(b \in \mathbb{R}\) such that the prediction given by \(\text{sign} (w^T\phi(x) + b)\) is correct for most samples.</source>
          <target state="translated">给定训练向量\（x_i \ in \ mathbb {R} ^ p \），在两个类别中i = 1，&amp;hellip;，n，以及向量\（y \ in \ {1，-1 \} ^ n \） ，我们的目标是找到\（w \ in \ mathbb {R} ^ p \）和\（b \ in \ mathbb {R} \）以使得\（\ text {sign}（w ^ T \ phi（x）+ b）\）对于大多数样本都是正确的。</target>
        </trans-unit>
        <trans-unit id="e44bf83eca8aa1cc0c5bdaa89da0afa702f51625" translate="yes" xml:space="preserve">
          <source>Gives the number of (complex) sampling points.</source>
          <target state="translated">给出(复杂的)采样点的数量。</target>
        </trans-unit>
        <trans-unit id="ac4e9c94eac5d688eef08c9122f5d38187b8f922" translate="yes" xml:space="preserve">
          <source>Global min and max average predictions, such that all plots will have the same scale and y limits. &lt;code&gt;pdp_lim[1]&lt;/code&gt; is the global min and max for single partial dependence curves. &lt;code&gt;pdp_lim[2]&lt;/code&gt; is the global min and max for two-way partial dependence curves.</source>
          <target state="translated">全局最小和最大平均预测，这样所有图将具有相同的比例和y限制。 &lt;code&gt;pdp_lim[1]&lt;/code&gt; 是单个部分相关曲线的全局最小值和最大值。 &lt;code&gt;pdp_lim[2]&lt;/code&gt; 是双向偏相关曲线的全局最小值和最大值。</target>
        </trans-unit>
        <trans-unit id="f36c7685daa8ebc7e1344aa0d6e3a7d679decebf" translate="yes" xml:space="preserve">
          <source>Global structure is not explicitly preserved. This is problem is mitigated by initializing points with PCA (using &lt;code&gt;init=&amp;rsquo;pca&amp;rsquo;&lt;/code&gt;).</source>
          <target state="translated">全局结构未明确保留。通过使用PCA初始化点（使用 &lt;code&gt;init=&amp;rsquo;pca&amp;rsquo;&lt;/code&gt; ）可以缓解此问题。</target>
        </trans-unit>
        <trans-unit id="01649050ef673ff19d8d011219103526d0d7370d" translate="yes" xml:space="preserve">
          <source>Global structure is not explicitly preserved. This problem is mitigated by initializing points with PCA (using &lt;code&gt;init='pca'&lt;/code&gt;).</source>
          <target state="translated">全局结构未明确保留。通过使用PCA初始化点（使用 &lt;code&gt;init='pca'&lt;/code&gt; ）可以缓解此问题。</target>
        </trans-unit>
        <trans-unit id="178c27bf7200da0534de904ea7e6ca7da842dbb5" translate="yes" xml:space="preserve">
          <source>Glorot, Xavier, and Yoshua Bengio. &amp;ldquo;Understanding the difficulty of</source>
          <target state="translated">Glorot，Xavier和Yoshua Bengio。&amp;ldquo;了解困难</target>
        </trans-unit>
        <trans-unit id="7427cf697be16a4ec1d916910128a59d920125e7" translate="yes" xml:space="preserve">
          <source>Glossary</source>
          <target state="translated">Glossary</target>
        </trans-unit>
        <trans-unit id="f7c22aaad44fb28f4ee8f06d6d4f4f14ac9ce899" translate="yes" xml:space="preserve">
          <source>Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,</source>
          <target state="translated">Golub和C.Van Loan.矩阵计算,第三版,第5章。</target>
        </trans-unit>
        <trans-unit id="1de5b736be2f9def46d07ed88549feeeea5a97b0" translate="yes" xml:space="preserve">
          <source>Gorodkin, (2004). Comparing two K-category assignments by a K-category correlation coefficient</source>
          <target state="translated">Gorodkin(2004年)。通过K类相关系数比较两个K类分配。</target>
        </trans-unit>
        <trans-unit id="46268d41f41f8e1954ca3d54fd29ddb1959ea6db" translate="yes" xml:space="preserve">
          <source>Gradient Boosting Out-of-Bag estimates</source>
          <target state="translated">梯度提升袋外估计值</target>
        </trans-unit>
        <trans-unit id="ff01958eb0f121764b2210794dcbe435f29dcc7a" translate="yes" xml:space="preserve">
          <source>Gradient Boosting Regression Trees for Poisson regression</source>
          <target state="translated">用于泊松回归的梯度提升回归树。</target>
        </trans-unit>
        <trans-unit id="9396c57fff04d750ce06a05cfd3c756b4f971532" translate="yes" xml:space="preserve">
          <source>Gradient Boosting also gives the possibility to fit the trees with a Poisson loss (with an implicit log-link function) instead of the default least-squares loss. Here we only fit trees with the Poisson loss to keep this example concise.</source>
          <target state="translated">梯度提升也提供了用泊松损失(用一个隐式对数链接函数)代替默认的最小二乘损失来拟合树木的可能性。在这里,我们只用泊松损失来拟合树木,以保持这个例子的简洁。</target>
        </trans-unit>
        <trans-unit id="3e3d95a92c5a33953c001956fd3fd6ac3b1082fa" translate="yes" xml:space="preserve">
          <source>Gradient Boosting attempts to solve this minimization problem numerically via steepest descent: The steepest descent direction is the negative gradient of the loss function evaluated at the current model \(F_{m-1}\) which can be calculated for any differentiable loss function:</source>
          <target state="translated">梯度提升法试图通过最陡下降法以数值方式解决这个最小化问题。最陡降方向是在当前模型上评估的损失函数的负梯度,可以计算出任何可微分损失函数。</target>
        </trans-unit>
        <trans-unit id="be45c92854a0f55592d6c3c1c28201cf75d59d94" translate="yes" xml:space="preserve">
          <source>Gradient Boosting for classification.</source>
          <target state="translated">分类的梯度提升。</target>
        </trans-unit>
        <trans-unit id="65fd480d2da13d80eb18643fd08c31b9e5239c9a" translate="yes" xml:space="preserve">
          <source>Gradient Boosting for regression.</source>
          <target state="translated">回归的梯度提升。</target>
        </trans-unit>
        <trans-unit id="23dcf8253cdacbdd915f0e5e69e684c3457ad1df" translate="yes" xml:space="preserve">
          <source>Gradient Boosting regression</source>
          <target state="translated">梯度提升回归</target>
        </trans-unit>
        <trans-unit id="33b1659de13c2a7e036f71b3c26eda1d552a4b1c" translate="yes" xml:space="preserve">
          <source>Gradient Boosting regularization</source>
          <target state="translated">梯度提升正则化</target>
        </trans-unit>
        <trans-unit id="a558a9ccdbbb397deb97e7223684a95578fb2ba7" translate="yes" xml:space="preserve">
          <source>Gradient boosting for classification is very similar to the regression case. However, the sum of the trees \(F_M(x_i) = \sum_m h_m(x_i)\) is not homogeneous to a prediction: it cannot be a class, since the trees predict continuous values.</source>
          <target state="translated">分类的梯度提升与回归的情况非常相似。然而,树的和 \(F_M(x_i)=\sum_m h_m(x_i)\)并不是同质的预测:它不可能是一个类,因为树预测的是连续值。</target>
        </trans-unit>
        <trans-unit id="cf9557c4e6e59de44aebd2e8b07221ff19e46958" translate="yes" xml:space="preserve">
          <source>Gradient boosting is an ensembling technique where several weak learners (regression trees) are combined to yield a powerful single model, in an iterative fashion.</source>
          <target state="translated">梯度提升是一种集合技术,将多个弱学习器(回归树)结合起来,以迭代的方式产生一个强大的单一模型。</target>
        </trans-unit>
        <trans-unit id="692996b3838fb57cde4b100ea1ec7f66fff47afe" translate="yes" xml:space="preserve">
          <source>Gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta. Only returned when &lt;code&gt;eval_gradient&lt;/code&gt; is True.</source>
          <target state="translated">对数边际可能性相对于位置theta处的核超参数的梯度。仅当 &lt;code&gt;eval_gradient&lt;/code&gt; 为True时返回。</target>
        </trans-unit>
        <trans-unit id="c4611f197e5e7430aa271445ae503720ad1cf3d4" translate="yes" xml:space="preserve">
          <source>Gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta. Only returned when eval_gradient is True.</source>
          <target state="translated">在theta位置上,相对于核超参数的对数边际似然的梯度。仅当eval_gradient为True时返回。</target>
        </trans-unit>
        <trans-unit id="e64c4914bb8a27678b7a6969455bd717adc62d09" translate="yes" xml:space="preserve">
          <source>GradientBoostingRegressor</source>
          <target state="translated">GradientBoostingRegressor</target>
        </trans-unit>
        <trans-unit id="e2fb5831cb5dd547c1703af3319394a3f8535468" translate="yes" xml:space="preserve">
          <source>Gram = np.dot(X.T * X).</source>
          <target state="translated">Gram=np.dot(X.T*X).</target>
        </trans-unit>
        <trans-unit id="f77edae6db0cdcd4449adeeb038c653af7406ea3" translate="yes" xml:space="preserve">
          <source>Gram Orthogonal Matching Pursuit (OMP)</source>
          <target state="translated">革兰氏正交匹配追求(OMP)</target>
        </trans-unit>
        <trans-unit id="10ef9123115df39a65f62ffa3d9d0e10899ca7cd" translate="yes" xml:space="preserve">
          <source>Gram matrix of the input data: X.T * X</source>
          <target state="translated">输入数据的Gram矩阵。X.T*X</target>
        </trans-unit>
        <trans-unit id="a83784084519ce853a92535121a74c85019c19b0" translate="yes" xml:space="preserve">
          <source>Graph distance (e.g. nearest-neighbor graph)</source>
          <target state="translated">图形距离(如最近邻图)</target>
        </trans-unit>
        <trans-unit id="8d5c9a04db77341319c1b38643f0d38066fc8710" translate="yes" xml:space="preserve">
          <source>Graph of the pixel-to-pixel connections</source>
          <target state="translated">像素与像素之间的连接图</target>
        </trans-unit>
        <trans-unit id="1b6f746d097f9fe3740f364d944363a7e3d991f9" translate="yes" xml:space="preserve">
          <source>Graph of the pixel-to-pixel gradient connections</source>
          <target state="translated">像素与像素之间的梯度连接图。</target>
        </trans-unit>
        <trans-unit id="a291a5c559f789dd92fe83af065257b966fe9953" translate="yes" xml:space="preserve">
          <source>Graph where A[i, j] is assigned the weight of edge that connects i to j. The matrix is of CSR format.</source>
          <target state="translated">图形,其中A[i,j]被赋予连接i和j的边的权重,矩阵为CSR格式。</target>
        </trans-unit>
        <trans-unit id="933bf21afdd55a0d2283845fed0e7bbdd1f5db49" translate="yes" xml:space="preserve">
          <source>Green</source>
          <target state="translated">Green</target>
        </trans-unit>
        <trans-unit id="9786dcbe8afbab8ac93bdfcd6653b6cd7aa7993b" translate="yes" xml:space="preserve">
          <source>Grid of Cs used for cross-validation.</source>
          <target state="translated">用于交叉验证的Cs网格。</target>
        </trans-unit>
        <trans-unit id="5bd85812ea7e2436359885d902fd71d10cd1c2d9" translate="yes" xml:space="preserve">
          <source>Grid of parameters with a discrete number of values for each.</source>
          <target state="translated">参数网格,每个参数都有一个离散的数值。</target>
        </trans-unit>
        <trans-unit id="4a6f9190abeab5c3ccde3d9c276bc4db019e7d38" translate="yes" xml:space="preserve">
          <source>Grid search can also be performed on the different preprocessing steps defined in the &lt;code&gt;ColumnTransformer&lt;/code&gt; object, together with the classifier&amp;rsquo;s hyperparameters as part of the &lt;code&gt;Pipeline&lt;/code&gt;. We will search for both the imputer strategy of the numeric preprocessing and the regularization parameter of the logistic regression using &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">网格搜索还可以在 &lt;code&gt;ColumnTransformer&lt;/code&gt; 对象中定义的不同预处理步骤以及作为 &lt;code&gt;Pipeline&lt;/code&gt; 一部分的分类器超参数中执行。我们将使用&lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt; &lt;/a&gt;搜索数值预处理的不合理策略和逻辑回归的正则化参数。</target>
        </trans-unit>
        <trans-unit id="71a1782f5aa6d2b7cc26a083f91eef66c1cf3aff" translate="yes" xml:space="preserve">
          <source>Grid-search</source>
          <target state="translated">Grid-search</target>
        </trans-unit>
        <trans-unit id="ed926e289de9aa5047e6b09f7b537df04bde4bbf" translate="yes" xml:space="preserve">
          <source>Grid-search and cross-validated estimators</source>
          <target state="translated">网格搜索和交叉验证的估计器。</target>
        </trans-unit>
        <trans-unit id="64ba146c44fdd8e95f622a314398320f76845aed" translate="yes" xml:space="preserve">
          <source>GridSearchCV implements a &amp;ldquo;fit&amp;rdquo; and a &amp;ldquo;score&amp;rdquo; method. It also implements &amp;ldquo;predict&amp;rdquo;, &amp;ldquo;predict_proba&amp;rdquo;, &amp;ldquo;decision_function&amp;rdquo;, &amp;ldquo;transform&amp;rdquo; and &amp;ldquo;inverse_transform&amp;rdquo; if they are implemented in the estimator used.</source>
          <target state="translated">GridSearchCV实现&amp;ldquo;拟合&amp;rdquo;和&amp;ldquo;得分&amp;rdquo;方法。如果在所使用的估计器中实现了&amp;ldquo;预测&amp;rdquo;，&amp;ldquo;预测_proba&amp;rdquo;，&amp;ldquo;决策功能&amp;rdquo;，&amp;ldquo;变换&amp;rdquo;和&amp;ldquo;逆变换&amp;rdquo;，则还可以实现它们。</target>
        </trans-unit>
        <trans-unit id="2e6f2bdd92d1c5e33352841cf6b10ed864b19fa7" translate="yes" xml:space="preserve">
          <source>Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification: An Overview. International Journal of Data Warehousing &amp;amp; Mining, 3(3), 1-13, July-September 2007.</source>
          <target state="translated">Grigorios Tsoumakas，Ioannis Katakis。多标签分类：概述。国际数据仓库与采矿杂志，2007年7月至9月，第3（3）页，第1-13页。</target>
        </trans-unit>
        <trans-unit id="796325c68f51f69a2afcc84a9fb61fa1d8420435" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) labels for n_samples samples.</source>
          <target state="translated">n_samples样本的地面真实(正确)标签。</target>
        </trans-unit>
        <trans-unit id="740dd68aa13d511b42941c79135a52aa5a0f5bc4" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) labels.</source>
          <target state="translated">地实(正确)标签。</target>
        </trans-unit>
        <trans-unit id="cf154969e860842a471602bf65b740057751e47b" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) target values.</source>
          <target state="translated">基本事实(正确)目标值。</target>
        </trans-unit>
        <trans-unit id="b29893e6134ceb0ae63100b750ea64cd6227af16" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) target values. Requires y_true &amp;gt; 0.</source>
          <target state="translated">基本事实（正确）目标值。要求y_true&amp;gt; 0。</target>
        </trans-unit>
        <trans-unit id="8dcf4476037b86142a6c723cc6d58c74ce6fa30f" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) target values. Requires y_true &amp;gt;= 0.</source>
          <target state="translated">基本事实（正确）目标值。要求y_true&amp;gt; = 0。</target>
        </trans-unit>
        <trans-unit id="691f624e8ff75b4d50175631f407699ccfb7e35d" translate="yes" xml:space="preserve">
          <source>Ground truth class labels to be used as a reference</source>
          <target state="translated">作为参考的地面真相类标签。</target>
        </trans-unit>
        <trans-unit id="2859baca63ac3255284d20bc28f887a4c54fefb4" translate="yes" xml:space="preserve">
          <source>Group labels for the samples used while splitting the dataset into train/test set.</source>
          <target state="translated">将数据集拆分为训练/测试集时使用的样本的组标签。</target>
        </trans-unit>
        <trans-unit id="da0d044e30ddccc2bad0f6e17da06a788ea2385a" translate="yes" xml:space="preserve">
          <source>Group labels for the samples used while splitting the dataset into train/test set. Only used in conjunction with a &amp;ldquo;Group&amp;rdquo; &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-cv&quot;&gt;cv&lt;/a&gt; instance (e.g., &lt;a href=&quot;sklearn.model_selection.groupkfold#sklearn.model_selection.GroupKFold&quot;&gt;&lt;code&gt;GroupKFold&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">将数据集拆分为训练/测试集时使用的样本的标签分组。仅与&amp;ldquo; Group&amp;rdquo; &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-cv&quot;&gt;cv&lt;/a&gt;实例（例如&lt;a href=&quot;sklearn.model_selection.groupkfold#sklearn.model_selection.GroupKFold&quot;&gt; &lt;code&gt;GroupKFold&lt;/code&gt; &lt;/a&gt;）结合使用。</target>
        </trans-unit>
        <trans-unit id="f5ee660cf40b3d432d2833cbe2c4255cb71b873d" translate="yes" xml:space="preserve">
          <source>Group labels for the samples used while splitting the dataset into train/test set. This &amp;lsquo;groups&amp;rsquo; parameter must always be specified to calculate the number of splits, though the other parameters can be omitted.</source>
          <target state="translated">将数据集拆分为训练/测试集时使用的样本的标签分组。尽管可以省略其他参数，但必须始终指定此&amp;ldquo; groups&amp;rdquo;参数来计算拆分数。</target>
        </trans-unit>
        <trans-unit id="2fe58cc1aca321453c1632eb3218b2ee2034ed27" translate="yes" xml:space="preserve">
          <source>Grow a tree with &lt;code&gt;max_leaf_nodes&lt;/code&gt; in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</source>
          <target state="translated">以最佳优先方式种植带有 &lt;code&gt;max_leaf_nodes&lt;/code&gt; 的树。最佳节点定义为杂质的相对减少。如果为None，则叶节点数不受限制。</target>
        </trans-unit>
        <trans-unit id="9f319cd9d13cdc03649579ff252c6b96c720508d" translate="yes" xml:space="preserve">
          <source>Grow trees with &lt;code&gt;max_leaf_nodes&lt;/code&gt; in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</source>
          <target state="translated">以最佳优先方式种植具有 &lt;code&gt;max_leaf_nodes&lt;/code&gt; 的树。最佳节点定义为杂质的相对减少。如果为None，则叶节点数不受限制。</target>
        </trans-unit>
        <trans-unit id="bf073fae640ded81eeb7a4cee70faff4a623c16c" translate="yes" xml:space="preserve">
          <source>Guide</source>
          <target state="translated">Guide</target>
        </trans-unit>
        <trans-unit id="1fd932db6b504d046b60a30c3273eb39ba2ac7a5" translate="yes" xml:space="preserve">
          <source>Guyon, I., Weston, J., Barnhill, S., &amp;amp; Vapnik, V., &amp;ldquo;Gene selection for cancer classification using support vector machines&amp;rdquo;, Mach. Learn., 46(1-3), 389&amp;ndash;422, 2002.</source>
          <target state="translated">马萨诸塞州，Guyon，I.，Weston，J.，Barnhill，S。，＆Vapnik，V。，&amp;ldquo;使用支持向量机进行癌症分类的基因选择&amp;rdquo;。Learn。，46（1-3），389-422，2002。</target>
        </trans-unit>
        <trans-unit id="dd4d457c816b0cb358c91f5b8813986bac26cb3d" translate="yes" xml:space="preserve">
          <source>H. Zhang (2004). &lt;a href=&quot;http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf&quot;&gt;The optimality of Naive Bayes.&lt;/a&gt; Proc. FLAIRS.</source>
          <target state="translated">张海（2004）。&lt;a href=&quot;http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf&quot;&gt;朴素贝叶斯的最优性。&lt;/a&gt;进程 天赋。</target>
        </trans-unit>
        <trans-unit id="ce3bde746d403806636c8d155597ef91ca7e1f03" translate="yes" xml:space="preserve">
          <source>H. Zhang (2004). &lt;a href=&quot;https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf&quot;&gt;The optimality of Naive Bayes.&lt;/a&gt; Proc. FLAIRS.</source>
          <target state="translated">张海（2004）。&lt;a href=&quot;https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf&quot;&gt;朴素贝叶斯的最优性。&lt;/a&gt;程序。天赋。</target>
        </trans-unit>
        <trans-unit id="de489f31c1f185d4a81f0399ead4e066a95b91be" translate="yes" xml:space="preserve">
          <source>HTML representation of &lt;code&gt;Pipeline&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;Pipeline&lt;/code&gt; HTML表示形式</target>
        </trans-unit>
        <trans-unit id="d930a6037b9120a42017959402d9dc27dd6bf69c" translate="yes" xml:space="preserve">
          <source>HTML representation of estimator.</source>
          <target state="translated">估计器的HTML表示。</target>
        </trans-unit>
        <trans-unit id="f5b6915b0e377ea69d7b62d27d3f027cc63657d7" translate="yes" xml:space="preserve">
          <source>Hagai Attias. (2000). &amp;ldquo;A Variational Bayesian Framework for Graphical Models&amp;rdquo;. In Advances in Neural Information Processing Systems 12.</source>
          <target state="translated">Hagai Attias。（2000）。&amp;ldquo;图形模型的变分贝叶斯框架&amp;rdquo;。神经信息处理系统进展12。</target>
        </trans-unit>
        <trans-unit id="8446ed65374f4c03b547ffebe7ab69437207be78" translate="yes" xml:space="preserve">
          <source>Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). &amp;ldquo;On Clustering Validation Techniques&amp;rdquo; Journal of Intelligent Information Systems, 17(2-3), 107-145. &lt;a href=&quot;http://dx.doi.org/10.1023/A:1012801612483&quot;&gt;doi:10.1023/A:1012801612483&lt;/a&gt;.</source>
          <target state="translated">玛丽亚&amp;middot;哈尔基迪（Halkidi）扬尼斯&amp;middot;巴蒂斯塔基斯；Vazirgiannis，Michalis（2001）。&amp;ldquo;关于聚类验证技术&amp;rdquo;，《智能信息系统杂志》，第17（2-3）页，第107-145页。&lt;a href=&quot;http://dx.doi.org/10.1023/A:1012801612483&quot;&gt;doi：10.1023 / A：1012801612483&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="959a6f4c185bd74a74d43205ed3cc9281eca4d45" translate="yes" xml:space="preserve">
          <source>Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). &amp;ldquo;On Clustering Validation Techniques&amp;rdquo; Journal of Intelligent Information Systems, 17(2-3), 107-145. &lt;a href=&quot;https://doi.org/10.1023/A:1012801612483&quot;&gt;doi:10.1023/A:1012801612483&lt;/a&gt;.</source>
          <target state="translated">玛丽亚&amp;middot;哈尔基迪（Halkidi）扬尼斯&amp;middot;巴蒂斯塔基斯（Batistakis）；Vazirgiannis，Michalis（2001）。&amp;ldquo;关于聚类验证技术&amp;rdquo;，《智能信息系统杂志》，第17（2-3）页，第107-145页。&lt;a href=&quot;https://doi.org/10.1023/A:1012801612483&quot;&gt;doi：10.1023 / A：1012801612483&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="57fe625410e680c160d128700bfb1af1b965809e" translate="yes" xml:space="preserve">
          <source>HammingDistance</source>
          <target state="translated">HammingDistance</target>
        </trans-unit>
        <trans-unit id="9757089e5251a143827d61ff72e389c7fd386869" translate="yes" xml:space="preserve">
          <source>Hand, D.J. and Till, R.J., (2001). &lt;a href=&quot;http://link.springer.com/article/10.1023/A:1010920819831&quot;&gt;A simple generalisation of the area under the ROC curve for multiple class classification problems.&lt;/a&gt; Machine learning, 45(2), pp.171-186.</source>
          <target state="translated">Hand，DJ和Till，RJ，（2001年）。&lt;a href=&quot;http://link.springer.com/article/10.1023/A:1010920819831&quot;&gt;针对多类分类问题的ROC曲线下面积的简单概括。&lt;/a&gt;机器学习，45（2），页171-186。</target>
        </trans-unit>
        <trans-unit id="9b654c73f6303f0d27e1fbee4658df85b5cd876f" translate="yes" xml:space="preserve">
          <source>Hand, D.J. and Till, R.J., (2001). &lt;a href=&quot;https://link.springer.com/article/10.1023/A:1010920819831&quot;&gt;A simple generalisation of the area under the ROC curve for multiple class classification problems.&lt;/a&gt; Machine learning, 45(2), pp.171-186.</source>
          <target state="translated">Hand，DJ和Till，RJ，（2001年）。&lt;a href=&quot;https://link.springer.com/article/10.1023/A:1010920819831&quot;&gt;针对多类分类问题的ROC曲线下面积的简单概括。&lt;/a&gt;机器学习，45（2），页171-186。</target>
        </trans-unit>
        <trans-unit id="88b6ef37ba2f9ba619bbf453c13dce1665119f21" translate="yes" xml:space="preserve">
          <source>Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems. Machine Learning, 45(2), 171-186.</source>
          <target state="translated">Hand,D.J.,Till,R.J.(2001).A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems.机器学习,45(2),171-186。</target>
        </trans-unit>
        <trans-unit id="dee17735ec3038cb9f5dda5413031eefdf59071a" translate="yes" xml:space="preserve">
          <source>Handle or name of the output file. If &lt;code&gt;None&lt;/code&gt;, the result is returned as a string.</source>
          <target state="translated">输出文件的句柄或名称。如果为 &lt;code&gt;None&lt;/code&gt; ，则结果以字符串形式返回。</target>
        </trans-unit>
        <trans-unit id="528b68d16981ccbe32f7d51bc822d75e077c8b80" translate="yes" xml:space="preserve">
          <source>Handling Multicollinear Features</source>
          <target state="translated">处理多线性特征</target>
        </trans-unit>
        <trans-unit id="077bb86f8a4736a0992a0b108c1d4b8e9298e04f" translate="yes" xml:space="preserve">
          <source>Hard constraint to select the backend. If set to &amp;lsquo;sharedmem&amp;rsquo;, the selected backend will be single-host and thread-based even if the user asked for a non-thread based backend with parallel_backend.</source>
          <target state="translated">很难选择后端。如果设置为&amp;ldquo; sharedmem&amp;rdquo;，则即使用户要求带有parallel_backend的非基于线程的后端，选定的后端也将是基于单主机和基于线程的后端。</target>
        </trans-unit>
        <trans-unit id="9b9156693e970a15a3c18a9425374c7bf2903574" translate="yes" xml:space="preserve">
          <source>Hard limit on iterations within solver, or -1 for no limit.</source>
          <target state="translated">对求解器内的迭代进行硬限制,如果没有限制,则为-1。</target>
        </trans-unit>
        <trans-unit id="73dd008516fbc283773051e5943e3b658488b1b1" translate="yes" xml:space="preserve">
          <source>Harrison, D. and Rubinfeld, D.L.</source>
          <target state="translated">Harrison,D.和Rubinfeld,D.L.</target>
        </trans-unit>
        <trans-unit id="c23f4e8aad7e2235e0ebdbc3c9d2bf9b602d6e3d" translate="yes" xml:space="preserve">
          <source>Hash function g(p,x) for a tree is an array of 32 randomly generated float arrays with the same dimension as the data set. This array is stored in GaussianRandomProjectionHash object and can be obtained from &lt;code&gt;components_&lt;/code&gt; attribute.</source>
          <target state="translated">树的哈希函数g（p，x）是由32个随机生成的float数组组成的数组，其维数与数据集的维数相同。该数组存储在GaussianRandomProjectionHash对象中，可以从 &lt;code&gt;components_&lt;/code&gt; attribute获取。</target>
        </trans-unit>
        <trans-unit id="8b5d87d4a16c0b826cb8988befd77a0e52c765c1" translate="yes" xml:space="preserve">
          <source>Hashing feature transformation using Totally Random Trees</source>
          <target state="translated">使用完全随机树进行哈希特征转换</target>
        </trans-unit>
        <trans-unit id="717a562588a8bf4bd25fb65069c4d3192c7a16dc" translate="yes" xml:space="preserve">
          <source>HashingVectorizer does not provide IDF weighting as this is a stateless model (the fit method does nothing). When IDF weighting is needed it can be added by pipelining its output to a TfidfTransformer instance.</source>
          <target state="translated">HashingVectorizer不提供IDF加权,因为这是一个无状态模型(拟合方法不做任何事情)。当需要IDF加权时,可以通过管道将其输出添加到TfidfTransformer实例中。</target>
        </trans-unit>
        <trans-unit id="d06cc92706967f16b8b9c95848cf5aff7ec1c456" translate="yes" xml:space="preserve">
          <source>HashingVectorizer hashes word occurrences to a fixed dimensional space, possibly with collisions. The word count vectors are then normalized to each have l2-norm equal to one (projected to the euclidean unit-ball) which seems to be important for k-means to work in high dimensional space.</source>
          <target state="translated">HashingVectorizer 将词的出现次数散列到一个固定的维度空间,可能会有碰撞。然后将词数向量归一化,使每个词数的l2-norm等于1(投射到欧氏单位球),这对于k-means在高维空间中的工作似乎很重要。</target>
        </trans-unit>
        <trans-unit id="28041ffc119d6685560d28cedcd34e917cd495e5" translate="yes" xml:space="preserve">
          <source>Hastie, R. Tibshirani and J. Friedman, &amp;ldquo;Elements of Statistical Learning Ed. 2&amp;rdquo;, Springer, 2009.</source>
          <target state="translated">Hastie，R。Tibshirani和J. Friedman，&amp;ldquo;统计学习的要素&amp;rdquo; 2&amp;rdquo;，施普林格，2009年。</target>
        </trans-unit>
        <trans-unit id="9803f456bd9f04731b6843d220c5ae89fa289aa2" translate="yes" xml:space="preserve">
          <source>Haussler, D. (1999). Convolution kernels on discrete structures (Vol. 646). Technical report, Department of Computer Science, University of California at Santa Cruz.</source>
          <target state="translated">Haussler,D.(1999).离散结构上的卷积核(第646卷)。技术报告,加州大学圣克鲁斯分校计算机科学系。</target>
        </trans-unit>
        <trans-unit id="b8dd0d155e19e8a71f19b1bbe40cdccabf151805" translate="yes" xml:space="preserve">
          <source>Have a look at the &lt;a href=&quot;../../modules/feature_extraction#hashing-vectorizer&quot;&gt;Hashing Vectorizer&lt;/a&gt; as a memory efficient alternative to &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">看看&lt;a href=&quot;../../modules/feature_extraction#hashing-vectorizer&quot;&gt;Hashing Vectorizer&lt;/a&gt;是&lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; 的&lt;/a&gt;一种内存有效替代方案。</target>
        </trans-unit>
        <trans-unit id="a899619755f5d06da20b9b2964b88739a1ab106e" translate="yes" xml:space="preserve">
          <source>Have a look at using &lt;a href=&quot;../../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core Classification&lt;/a&gt; to learn from data that would not fit into the computer main memory.</source>
          <target state="translated">看一看使用&lt;a href=&quot;../../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;核外分类&lt;/a&gt;从不适合计算机主内存的数据中学习。</target>
        </trans-unit>
        <trans-unit id="27a688f240efc4c1f8111e73298dc1d5dd7e9964" translate="yes" xml:space="preserve">
          <source>HaversineDistance</source>
          <target state="translated">HaversineDistance</target>
        </trans-unit>
        <trans-unit id="260c7f8bcac0cff0858b268328a3c57270e6d05b" translate="yes" xml:space="preserve">
          <source>He, Kaiming, et al. &amp;ldquo;Delving deep into rectifiers: Surpassing human-level</source>
          <target state="translated">他，Kaiming等。&amp;ldquo;深入研究整流器：超越人类水平</target>
        </trans-unit>
        <trans-unit id="2f8a00b4f7c2990e23253c9271642cb45a1f2224" translate="yes" xml:space="preserve">
          <source>Helper class for readable parallel mapping.</source>
          <target state="translated">可读并行映射的辅助类。</target>
        </trans-unit>
        <trans-unit id="15e3ecfce92d858c5fac5d21e2153dba45c36e72" translate="yes" xml:space="preserve">
          <source>Helper function to test the message raised in an exception.</source>
          <target state="translated">用于测试异常情况下发出的消息的辅助函数。</target>
        </trans-unit>
        <trans-unit id="e22b8152bb5ec7ad5480951d5d1692b1809abba4" translate="yes" xml:space="preserve">
          <source>Hence using random projections on the digits dataset which only has 64 features in the input space does not make sense: it does not allow for dimensionality reduction in this case.</source>
          <target state="translated">因此,在输入空间中只有64个特征的数字数据集上使用随机投影是没有意义的:在这种情况下,它不允许减少维度。</target>
        </trans-unit>
        <trans-unit id="858c4ba42a503184b8af0061cb8145e1add6548c" translate="yes" xml:space="preserve">
          <source>Hence words that were not seen in the training corpus will be completely ignored in future calls to the transform method:</source>
          <target state="translated">因此,在训练语料中没有看到的词,在未来调用转换方法时将完全被忽略。</target>
        </trans-unit>
        <trans-unit id="3fea43b2d3bbf05cef0fdbdec4ca7b01a2de9eb5" translate="yes" xml:space="preserve">
          <source>Hence, the None case results in:</source>
          <target state="translated">因此,&quot;无 &quot;的结果是:</target>
        </trans-unit>
        <trans-unit id="4fffc6a6ec537bad9c19e154df4fbf4c1dee1839" translate="yes" xml:space="preserve">
          <source>Here &lt;code&gt;func&lt;/code&gt; is a function which takes two one-dimensional numpy arrays, and returns a distance. Note that in order to be used within the BallTree, the distance must be a true metric: i.e. it must satisfy the following properties</source>
          <target state="translated">这里的 &lt;code&gt;func&lt;/code&gt; 是一个函数，它接受两个一维numpy数组，并返回一个距离。请注意，为了在BallTree中使用该距离，必须是一个真实的度量标准：即它必须满足以下属性</target>
        </trans-unit>
        <trans-unit id="8918252717f29fe05952e0490941948a7c1afcd2" translate="yes" xml:space="preserve">
          <source>Here a sine function is fit with a polynomial of order 3, for values close to zero.</source>
          <target state="translated">这里的正弦函数是用一个3阶的多项式来拟合的,数值接近于零。</target>
        </trans-unit>
        <trans-unit id="dd2684d229285b3b1a04454d9cd068cd1e054408" translate="yes" xml:space="preserve">
          <source>Here a small example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; function with a svm classifier in a binary class problem:</source>
          <target state="translated">这是一个小示例，该示例演示在二进制类问题中将&lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt;函数与svm分类器一起使用：</target>
        </trans-unit>
        <trans-unit id="5113795d86cf9b1916006aecdb0ceee73192da33" translate="yes" xml:space="preserve">
          <source>Here a small excerpt which illustrates how to use the Gaussian random projection transformer:</source>
          <target state="translated">这里摘录一个小片段,说明如何使用高斯随机投影变换器。</target>
        </trans-unit>
        <trans-unit id="d838251a264bfc0a86e50e7c2f5d9ef6f54d10aa" translate="yes" xml:space="preserve">
          <source>Here a small excerpt which illustrates how to use the sparse random projection transformer:</source>
          <target state="translated">这里摘录一小段,说明如何使用稀疏随机投影变换器。</target>
        </trans-unit>
        <trans-unit id="32f51b9dd909238771016da8eae995fa183bb752" translate="yes" xml:space="preserve">
          <source>Here are a few suggestions to help further your scikit-learn intuition upon the completion of this tutorial:</source>
          <target state="translated">以下是一些建议,帮助你在完成本教程后,进一步提升你的scikit-learn直觉。</target>
        </trans-unit>
        <trans-unit id="3dda6ea8d57e795e10f1bb02b3e190ba1eee1ee3" translate="yes" xml:space="preserve">
          <source>Here are some examples demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.multilabel_confusion_matrix#sklearn.metrics.multilabel_confusion_matrix&quot;&gt;&lt;code&gt;multilabel_confusion_matrix&lt;/code&gt;&lt;/a&gt; function to calculate recall (or sensitivity), specificity, fall out and miss rate for each class in a problem with multilabel indicator matrix input.</source>
          <target state="translated">以下是一些示例，这些示例演示了使用&lt;a href=&quot;generated/sklearn.metrics.multilabel_confusion_matrix#sklearn.metrics.multilabel_confusion_matrix&quot;&gt; &lt;code&gt;multilabel_confusion_matrix&lt;/code&gt; &lt;/a&gt;函数来计算多标签指示器矩阵输入问题中每个类别的召回率（或敏感性），特异性，掉落率和未命中率。</target>
        </trans-unit>
        <trans-unit id="8d2ed57227d29b030e11d07a6ad14619156d6baa" translate="yes" xml:space="preserve">
          <source>Here are some recommended ways to load standard columnar data into a format usable by scikit-learn:</source>
          <target state="translated">这里有一些推荐的方法可以将标准的列式数据加载到scikit-learn可用的格式中。</target>
        </trans-unit>
        <trans-unit id="6caa2e3f5fa319efda163f3ada59f70b9af4251d" translate="yes" xml:space="preserve">
          <source>Here are some small examples in binary classification:</source>
          <target state="translated">下面是二进制分类中的一些小例子。</target>
        </trans-unit>
        <trans-unit id="cad58f968788a0c8b830200526f46c2e8380af6d" translate="yes" xml:space="preserve">
          <source>Here is a list of incremental estimators for different tasks:</source>
          <target state="translated">以下是不同任务的增量估算器列表。</target>
        </trans-unit>
        <trans-unit id="e5cc3ef05cd44a377ff0113c5a0144a6cd05b3f4" translate="yes" xml:space="preserve">
          <source>Here is a sample output of a run on a quad-core machine:</source>
          <target state="translated">下面是在四核机上运行的输出示例。</target>
        </trans-unit>
        <trans-unit id="00dac27806e77f637d738445566eb627365e0881" translate="yes" xml:space="preserve">
          <source>Here is a sketch of a system designed to achieve this goal:</source>
          <target state="translated">下面是为实现这一目标而设计的一个系统的草图。</target>
        </trans-unit>
        <trans-unit id="c595619fa24f58ee3930e8429960f874f9b329e7" translate="yes" xml:space="preserve">
          <source>Here is a small example illustrating the usage of the &lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt;&lt;code&gt;matthews_corrcoef&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">这是一个小示例，说明了&lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt; &lt;code&gt;matthews_corrcoef&lt;/code&gt; &lt;/a&gt;函数的用法：</target>
        </trans-unit>
        <trans-unit id="423aaa3f8753fc630af578bc1fbb46728b5a06e5" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">这是一个&lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; &lt;/a&gt;变量用法的小例子：</target>
        </trans-unit>
        <trans-unit id="1875c837ecf1df623da5e8546dcb195bb9e83c64" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.max_error#sklearn.metrics.max_error&quot;&gt;&lt;code&gt;max_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">这是使用&lt;a href=&quot;generated/sklearn.metrics.max_error#sklearn.metrics.max_error&quot;&gt; &lt;code&gt;max_error&lt;/code&gt; &lt;/a&gt;函数的一个小示例：</target>
        </trans-unit>
        <trans-unit id="cb41f576a02130e8636700bae5b58c941781076b" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt;&lt;code&gt;mean_absolute_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">这是一个使用&lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt; &lt;code&gt;mean_absolute_error&lt;/code&gt; &lt;/a&gt;函数的小例子：</target>
        </trans-unit>
        <trans-unit id="7c0ba7d72bd4599fa8b6676ec86f846e3705f7da" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;mean_squared_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">这是一个使用&lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;mean_squared_error&lt;/code&gt; &lt;/a&gt;函数的小例子：</target>
        </trans-unit>
        <trans-unit id="0be0450f469be9534c036908ab2afdbd59b24548" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt;&lt;code&gt;mean_squared_log_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">这是一个使用&lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt; &lt;code&gt;mean_squared_log_error&lt;/code&gt; &lt;/a&gt;函数的小例子：</target>
        </trans-unit>
        <trans-unit id="f8da86e09b21d704ee9aa6f7fcb4b0cf6258a18d" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt;&lt;code&gt;median_absolute_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">这是使用&lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt; &lt;code&gt;median_absolute_error&lt;/code&gt; &lt;/a&gt;函数的一个小示例：</target>
        </trans-unit>
        <trans-unit id="e0060b4a19332fa9cdf176d47debc4e3de22af1f" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">这是使用&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt;函数的一个小示例：</target>
        </trans-unit>
        <trans-unit id="2121874e07dc9ac1fb205417370f94e728d5e5e6" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of this function:</source>
          <target state="translated">下面是这个函数使用的一个小例子。</target>
        </trans-unit>
        <trans-unit id="f03ea6f9a5b7db0b84376e166dbfe9d87d690fa9" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of this function::</source>
          <target state="translated">下面是这个函数使用的一个小例子:。</target>
        </trans-unit>
        <trans-unit id="da9291cb119f102218681b72119ede84a1e93115" translate="yes" xml:space="preserve">
          <source>Here is a usage example:</source>
          <target state="translated">下面是一个使用实例。</target>
        </trans-unit>
        <trans-unit id="ec46f6fe41e667dcb81fcf9a89e2aaf0a6763af5" translate="yes" xml:space="preserve">
          <source>Here is a visual representation of such a confusion matrix (this figure comes from the &lt;a href=&quot;../auto_examples/model_selection/plot_confusion_matrix#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py&quot;&gt;Confusion matrix&lt;/a&gt; example):</source>
          <target state="translated">这是这种混淆矩阵的直观表示（此图来自&lt;a href=&quot;../auto_examples/model_selection/plot_confusion_matrix#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py&quot;&gt;混淆矩阵&lt;/a&gt;示例）：</target>
        </trans-unit>
        <trans-unit id="876abdb2188ee5022ae84c77928e2082f05a478c" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior.</source>
          <target state="translated">下面是一个可视化的交叉验证行为。</target>
        </trans-unit>
        <trans-unit id="d5a8fd11bd11ae3f4eb764b39ba1acfee92579af" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior. Note that &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is not affected by classes or groups.</source>
          <target state="translated">这是交叉验证行为的可视化。请注意，&lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;不受类或组的影响。</target>
        </trans-unit>
        <trans-unit id="e10cd61d7e44ad9e6bb0d4cec30745248d4c4e93" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior. Note that &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt;&lt;code&gt;ShuffleSplit&lt;/code&gt;&lt;/a&gt; is not affected by classes or groups.</source>
          <target state="translated">这是交叉验证行为的可视化。请注意，&lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt; &lt;code&gt;ShuffleSplit&lt;/code&gt; &lt;/a&gt;不受类或组的影响。</target>
        </trans-unit>
        <trans-unit id="0b166c480658b240c273df0a43ce9ffa8405561c" translate="yes" xml:space="preserve">
          <source>Here is an example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; function with a svm classifier in a multiclass problem:</source>
          <target state="translated">这是一个示例，演示在多类问题中对svm分类器使用&lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt;函数：</target>
        </trans-unit>
        <trans-unit id="01baca5f1060ff615b57707b27b89349c3831f22" translate="yes" xml:space="preserve">
          <source>Here is an example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.multilabel_confusion_matrix#sklearn.metrics.multilabel_confusion_matrix&quot;&gt;&lt;code&gt;multilabel_confusion_matrix&lt;/code&gt;&lt;/a&gt; function with &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multiclass&quot;&gt;multiclass&lt;/a&gt; input:</source>
          <target state="translated">这是一个演示&lt;a href=&quot;generated/sklearn.metrics.multilabel_confusion_matrix#sklearn.metrics.multilabel_confusion_matrix&quot;&gt; &lt;code&gt;multilabel_confusion_matrix&lt;/code&gt; &lt;/a&gt;函数与&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multiclass&quot;&gt;多类&lt;/a&gt;输入的结合使用的示例：</target>
        </trans-unit>
        <trans-unit id="ac06b69a6bbd9ae081c446d18662dd3517d588fd" translate="yes" xml:space="preserve">
          <source>Here is an example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.multilabel_confusion_matrix#sklearn.metrics.multilabel_confusion_matrix&quot;&gt;&lt;code&gt;multilabel_confusion_matrix&lt;/code&gt;&lt;/a&gt; function with &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multilabel-indicator-matrix&quot;&gt;multilabel indicator matrix&lt;/a&gt; input:</source>
          <target state="translated">这是一个演示将&lt;a href=&quot;generated/sklearn.metrics.multilabel_confusion_matrix#sklearn.metrics.multilabel_confusion_matrix&quot;&gt; &lt;code&gt;multilabel_confusion_matrix&lt;/code&gt; &lt;/a&gt;函数与&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multilabel-indicator-matrix&quot;&gt;多标签指示符矩阵&lt;/a&gt;输入配合使用的示例：</target>
        </trans-unit>
        <trans-unit id="58bc8e3595ba3624485387b6def524d2d31bae65" translate="yes" xml:space="preserve">
          <source>Here is an example of &lt;code&gt;cross_validate&lt;/code&gt; using a single metric:</source>
          <target state="translated">以下是使用单个指标的 &lt;code&gt;cross_validate&lt;/code&gt; 示例：</target>
        </trans-unit>
        <trans-unit id="f7e50cdf4078c7823c206e72ce0bf5486f1e2a9f" translate="yes" xml:space="preserve">
          <source>Here is an example of applying this idea to one-dimensional data, using polynomial features of varying degrees:</source>
          <target state="translated">下面是一个将这一思想应用于一维数据的例子,使用不同程度的多项式特征。</target>
        </trans-unit>
        <trans-unit id="8375acd14d3c16b75f14ad4cf9799bf09154cba1" translate="yes" xml:space="preserve">
          <source>Here is an example of building custom scorers, and of using the &lt;code&gt;greater_is_better&lt;/code&gt; parameter:</source>
          <target state="translated">以下是构建自定义评分器以及使用 &lt;code&gt;greater_is_better&lt;/code&gt; 参数的示例：</target>
        </trans-unit>
        <trans-unit id="3be41bccb12847b90804b0be88468f33593d4dc5" translate="yes" xml:space="preserve">
          <source>Here is an example of stratified 3-fold cross-validation on a dataset with 50 samples from two unbalanced classes. We show the number of samples in each class and compare with &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">这是一个对数据集进行分层三折交叉验证的示例，该数据集包含来自两个不平衡类的50个样本。我们显示每个类别中的样本数量，并与&lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;进行比较。</target>
        </trans-unit>
        <trans-unit id="120ffb4ca9b2da814644df8eb634b8cef584b2a0" translate="yes" xml:space="preserve">
          <source>Here is an example to scale a toy data matrix to the &lt;code&gt;[0, 1]&lt;/code&gt; range:</source>
          <target state="translated">这是将玩具数据矩阵缩放到 &lt;code&gt;[0, 1]&lt;/code&gt; 范围的示例：</target>
        </trans-unit>
        <trans-unit id="04b3257d3ad37f9ca0ccbd79a367325c6e1ed5f4" translate="yes" xml:space="preserve">
          <source>Here is an example using &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt; with the &lt;code&gt;elasticnet&lt;/code&gt; penalty. The regularization strength is globally controlled by the &lt;code&gt;alpha&lt;/code&gt; parameter. With a sufficiently high &lt;code&gt;alpha&lt;/code&gt;, one can then increase the &lt;code&gt;l1_ratio&lt;/code&gt; parameter of &lt;code&gt;elasticnet&lt;/code&gt; to enforce various levels of sparsity in the model coefficients. Higher sparsity here is interpreted as less model complexity as we need fewer coefficients to describe it fully. Of course sparsity influences in turn the prediction time as the sparse dot-product takes time roughly proportional to the number of non-zero coefficients.</source>
          <target state="translated">这是将&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt; &lt;/a&gt;与 &lt;code&gt;elasticnet&lt;/code&gt; 罚分一起使用的示例。正则化强度由 &lt;code&gt;alpha&lt;/code&gt; 参数全局控制。具有足够高 &lt;code&gt;alpha&lt;/code&gt; ，可以再增加 &lt;code&gt;l1_ratio&lt;/code&gt; 的参数 &lt;code&gt;elasticnet&lt;/code&gt; 执行模型中的系数的稀疏性的各种级别。这里的稀疏度越高，模型的复杂度越低，因为我们需要更少的系数来完整地描述它。当然，稀疏性反过来会影响预测时间，因为稀疏点积花费的时间与非零系数的数量大致成比例。</target>
        </trans-unit>
        <trans-unit id="8f89ae42a83e786b17cd6f1b83024799754e5687" translate="yes" xml:space="preserve">
          <source>Here is an example using &lt;code&gt;sklearn.linear_model.stochastic_gradient.SGDClassifier&lt;/code&gt; with the &lt;code&gt;elasticnet&lt;/code&gt; penalty. The regularization strength is globally controlled by the &lt;code&gt;alpha&lt;/code&gt; parameter. With a sufficiently high &lt;code&gt;alpha&lt;/code&gt;, one can then increase the &lt;code&gt;l1_ratio&lt;/code&gt; parameter of &lt;code&gt;elasticnet&lt;/code&gt; to enforce various levels of sparsity in the model coefficients. Higher sparsity here is interpreted as less model complexity as we need fewer coefficients to describe it fully. Of course sparsity influences in turn the prediction time as the sparse dot-product takes time roughly proportional to the number of non-zero coefficients.</source>
          <target state="translated">这是将 &lt;code&gt;sklearn.linear_model.stochastic_gradient.SGDClassifier&lt;/code&gt; 与 &lt;code&gt;elasticnet&lt;/code&gt; 罚分一起使用的示例。正则化强度由 &lt;code&gt;alpha&lt;/code&gt; 参数全局控制。具有足够高 &lt;code&gt;alpha&lt;/code&gt; ，可以再增加 &lt;code&gt;l1_ratio&lt;/code&gt; 的参数 &lt;code&gt;elasticnet&lt;/code&gt; 执行模型中的系数的稀疏性的各种级别。此处，稀疏度越高，模型的复杂度越低，因为我们需要更少的系数来充分描述它。当然，稀疏性反过来会影响预测时间，因为稀疏点积花费的时间与非零系数的数量大致成比例。</target>
        </trans-unit>
        <trans-unit id="540ee2aaf7182c6dfc449b18e5accb694e3b0894" translate="yes" xml:space="preserve">
          <source>Here is an example:</source>
          <target state="translated">下面是一个例子。</target>
        </trans-unit>
        <trans-unit id="a1ce1cc95adf7777aaf8483ebc72e46f7e0c5dd5" translate="yes" xml:space="preserve">
          <source>Here is how to use the toy data from the previous example with this scaler:</source>
          <target state="translated">下面是如何用这个标尺使用前面例子中的玩具数据。</target>
        </trans-unit>
        <trans-unit id="da00252cb105e8e07c4719d8131543c2597c6b64" translate="yes" xml:space="preserve">
          <source>Here is sample code that illustrates the use of the &lt;code&gt;sparsify()&lt;/code&gt; method:</source>
          <target state="translated">以下示例代码说明了 &lt;code&gt;sparsify()&lt;/code&gt; 方法的用法：</target>
        </trans-unit>
        <trans-unit id="b1b76d97b9ed98e3661e06b53d247e6f552362c3" translate="yes" xml:space="preserve">
          <source>Here is sample code to test the sparsity of your input:</source>
          <target state="translated">下面是测试输入稀疏度的示例代码。</target>
        </trans-unit>
        <trans-unit id="7a4f1fdf399f62578619e41a5fba4a345597683a" translate="yes" xml:space="preserve">
          <source>Here is the list of models benefiting from the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) for automated model selection:</source>
          <target state="translated">以下是受益于Akaike信息准则(AIC)或贝叶斯信息准则(BIC)的自动模型选择的模型列表。</target>
        </trans-unit>
        <trans-unit id="24d46233c5b1cf5947d798926d1e317b272fc656" translate="yes" xml:space="preserve">
          <source>Here is the list of such models:</source>
          <target state="translated">以下是这类车型的清单。</target>
        </trans-unit>
        <trans-unit id="757c8807092bec583b3c00400f122f638dd1b02a" translate="yes" xml:space="preserve">
          <source>Here one can observe that the train accuracy is very high (the forest model has enough capacity to completely memorize the training set) but it can still generalize well enough to the test set thanks to the built-in bagging of random forests.</source>
          <target state="translated">在这里可以观察到,训练精度非常高(森林模型有足够的能力完全记忆训练集),但由于随机森林的内置袋装,它仍然可以对测试集有足够的泛化。</target>
        </trans-unit>
        <trans-unit id="15f3441e24a8e858a11c375d5c2fee3bc8aa09ba" translate="yes" xml:space="preserve">
          <source>Here our goal goal is to predict the expected value, i.e. the mean, of the total claim amount per exposure unit also referred to as the pure premium.</source>
          <target state="translated">在这里,我们的目标目标是预测每个风险单位的总索赔金额的预期值,即平均值,也称为纯溢价。</target>
        </trans-unit>
        <trans-unit id="785dc58756e0e128c51d97262d985997dfc75263" translate="yes" xml:space="preserve">
          <source>Here the &lt;code&gt;transform&lt;/code&gt; operation returns \(LX^T\), therefore its time complexity equals &lt;code&gt;n_components * n_features * n_samples_test&lt;/code&gt;. There is no added space complexity in the operation.</source>
          <target state="translated">这里的 &lt;code&gt;transform&lt;/code&gt; 操作返回\（LX ^ T \），因此其时间复杂度等于 &lt;code&gt;n_components * n_features * n_samples_test&lt;/code&gt; 。操作中不会增加空间复杂度。</target>
        </trans-unit>
        <trans-unit id="8029b08717fd12d596415c9951c99ad442611512" translate="yes" xml:space="preserve">
          <source>Here the computation is achieved thanks to Martinsson&amp;rsquo;s Randomized SVD algorithm implemented in scikit-learn.</source>
          <target state="translated">这里的计算是通过scikit-learn中实现的Martinsson的随机SVD算法实现的。</target>
        </trans-unit>
        <trans-unit id="baa6fd34087f3f3b80a068e5198c152eb2224084" translate="yes" xml:space="preserve">
          <source>Here the results are not as good as they could be as our choice for the regularization parameter C was not the best. In real life applications this parameter is usually chosen using &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt;.</source>
          <target state="translated">在这里，结果不如我们期望的那样好，因为正则化参数C并不是最好的选择。在实际应用中，通常使用&lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;调整估计器的超参数&lt;/a&gt;来选择此参数。</target>
        </trans-unit>
        <trans-unit id="e33a1b9fd8981a72cf8c17c638c489933e2535f4" translate="yes" xml:space="preserve">
          <source>Here we choose the SAGA solver because it can efficiently optimize for the Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.</source>
          <target state="translated">这里我们之所以选择SAGA求解器,是因为它可以有效地对Logistic回归损失进行非平稳、稀疏性诱导的L1惩罚优化。</target>
        </trans-unit>
        <trans-unit id="15d9d9f74de48f0968b756664a2f90e7435e3e3c" translate="yes" xml:space="preserve">
          <source>Here we choose the liblinear solver because it can efficiently optimize for the Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.</source>
          <target state="translated">在这里,我们选择liblinear求解器,因为它可以有效地优化Logistic回归损失,并具有非平滑的、稀疏性诱导的L1惩罚。</target>
        </trans-unit>
        <trans-unit id="d4668460d1dfffc9a12dd3f0ca645c8016c22599" translate="yes" xml:space="preserve">
          <source>Here we compare 3 approaches:</source>
          <target state="translated">这里我们比较3种方法。</target>
        </trans-unit>
        <trans-unit id="3aadfee7aa5bef8aeacf179790398b01b017dc93" translate="yes" xml:space="preserve">
          <source>Here we describe variational inference algorithms on Dirichlet process mixture. The Dirichlet process is a prior probability distribution on &lt;em&gt;clusterings with an infinite, unbounded, number of partitions&lt;/em&gt;. Variational techniques let us incorporate this prior structure on Gaussian mixture models at almost no penalty in inference time, comparing with a finite Gaussian mixture model.</source>
          <target state="translated">在这里，我们描述了Dirichlet过程混合的变分推理算法。Dirichlet过程是&lt;em&gt;具有无限，无界分区数的聚类&lt;/em&gt;的先验概率分布。与有限的高斯混合模型相比，变分技术使我们可以将这种先验结构并入高斯混合模型中，而在推理时间上几乎没有损失。</target>
        </trans-unit>
        <trans-unit id="19eba1946aa4b2b6c504a0a7a0b9c334a19205ae" translate="yes" xml:space="preserve">
          <source>Here we fit a multinomial logistic regression with L1 penalty on a subset of the MNIST digits classification task. We use the SAGA algorithm for this purpose: this a solver that is fast when the number of samples is significantly larger than the number of features and is able to finely optimize non-smooth objective functions which is the case with the l1-penalty. Test accuracy reaches &amp;gt; 0.8, while weight vectors remains &lt;em&gt;sparse&lt;/em&gt; and therefore more easily &lt;em&gt;interpretable&lt;/em&gt;.</source>
          <target state="translated">在这里，我们对MNIST数字分类任务的子集进行L1罚分的多项式逻辑回归。为此，我们使用SAGA算法：这是一种求解器，当样本数量明显大于特征数量时，它是快速的，并且能够很好地优化非平滑目标函数，这就是l1罚分的情况。测试精度达到&amp;gt; 0.8，而权重向量仍然&lt;em&gt;稀疏&lt;/em&gt;，因此更容易&lt;em&gt;解释&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="bc0bdb5bd44175832d7fcee805ba26710508e043" translate="yes" xml:space="preserve">
          <source>Here we have used &lt;code&gt;kernel='gaussian'&lt;/code&gt;, as seen above. Mathematically, a kernel is a positive function \(K(x;h)\) which is controlled by the bandwidth parameter \(h\). Given this kernel form, the density estimate at a point \(y\) within a group of points \(x_i; i=1\cdots N\) is given by:</source>
          <target state="translated">如上所示，这里我们使用 &lt;code&gt;kernel='gaussian'&lt;/code&gt; 。从数学上讲，内核是由带宽参数\（h \）控制的正函数\（K（x; h）\）。给定该内核形式，在一组点\（x_i; i = 1 \ cdots N \）内的点\（y \）处的密度估计为：</target>
        </trans-unit>
        <trans-unit id="9bc4f467db4b070f940a4ae98fc40a9d9951075c" translate="yes" xml:space="preserve">
          <source>Here we simulate independent sources using a highly non-Gaussian process, 2 student T with a low number of degrees of freedom (top left figure). We mix them to create observations (top right figure). In this raw observation space, directions identified by PCA are represented by orange vectors. We represent the signal in the PCA space, after whitening by the variance corresponding to the PCA vectors (lower left). Running ICA corresponds to finding a rotation in this space to identify the directions of largest non-Gaussianity (lower right).</source>
          <target state="translated">在这里,我们使用一个高度非高斯过程模拟独立的来源,2学生T与低自由度数(左上图)。我们将它们混合起来创建观测值(右上图)。在这个原始观测空间中,由PCA识别的方向由橙色向量表示。我们在PCA空间中表示信号,通过PCA向量对应的方差进行白化后(左下图)。运行ICA相当于在这个空间中找到一个旋转,以识别最大的非高斯性的方向(右下)。</target>
        </trans-unit>
        <trans-unit id="72e463d9d9e7402af61cb976f70a41a655f66554" translate="yes" xml:space="preserve">
          <source>Here we use the caching property of pipelines to cache the nearest neighbors graph between multiple fits of KNeighborsClassifier. The first call is slow since it computes the neighbors graph, while subsequent call are faster as they do not need to recompute the graph. Here the durations are small since the dataset is small, but the gain can be more substantial when the dataset grows larger, or when the grid of parameter to search is large.</source>
          <target state="translated">这里我们利用管道的缓存属性,在KNeighborsClassifier的多次拟合之间缓存最近邻图。第一次调用很慢,因为它计算的是邻域图,而后续的调用则更快,因为它们不需要重新计算图。这里由于数据集较小,所以持续时间较小,但当数据集变大,或者要搜索的参数网格较大时,收益会比较大。</target>
        </trans-unit>
        <trans-unit id="4efad2f65eb490631f07741acecbb3c6dbc45f0c" translate="yes" xml:space="preserve">
          <source>Here we use the l1 sparsity that trims the weights of not informative features to zero. This is good if the goal is to extract the strongly discriminative vocabulary of each class. If the goal is to get the best predictive accuracy, it is better to use the non sparsity-inducing l2 penalty instead.</source>
          <target state="translated">在这里,我们使用l1稀疏性,将非信息特征的权重修剪为零。如果目标是提取每个类的强区分性词汇,这是很好的。如果目标是获得最好的预测精度,最好使用非稀疏性诱导的l2惩罚来代替。</target>
        </trans-unit>
        <trans-unit id="0013bebf729b11b2c5dfc0313efdfb116ab3b0c5" translate="yes" xml:space="preserve">
          <source>Here we want to model the frequency &lt;code&gt;y = ClaimNb / Exposure&lt;/code&gt; conditionally on &lt;code&gt;X&lt;/code&gt; via a (scaled) Poisson distribution, and use &lt;code&gt;Exposure&lt;/code&gt; as &lt;code&gt;sample_weight&lt;/code&gt;.</source>
          <target state="translated">在这里，我们想通过（缩放的）泊松分布在 &lt;code&gt;X&lt;/code&gt; 上有条件地对频率 &lt;code&gt;y = ClaimNb / Exposure&lt;/code&gt; 建模，并使用 &lt;code&gt;Exposure&lt;/code&gt; 作为 &lt;code&gt;sample_weight&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="034b3c08e48de8757b6f1f86830d0869f87a8e39" translate="yes" xml:space="preserve">
          <source>Here, &lt;code&gt;&amp;lt;estimator&amp;gt;&lt;/code&gt; is the parameter name of the nested estimator, in this case &lt;code&gt;base_estimator&lt;/code&gt;. If the meta-estimator is constructed as a collection of estimators as in &lt;code&gt;pipeline.Pipeline&lt;/code&gt;, then &lt;code&gt;&amp;lt;estimator&amp;gt;&lt;/code&gt; refers to the name of the estimator, see &lt;a href=&quot;compose#pipeline-nested-parameters&quot;&gt;Nested parameters&lt;/a&gt;. In practice, there can be several levels of nesting:</source>
          <target state="translated">此处， &lt;code&gt;&amp;lt;estimator&amp;gt;&lt;/code&gt; 是嵌套估算器的参数名称，在本例中为 &lt;code&gt;base_estimator&lt;/code&gt; 。如果将meta-estimator构造为 &lt;code&gt;pipeline.Pipeline&lt;/code&gt; 中的估计量集合，则 &lt;code&gt;&amp;lt;estimator&amp;gt;&lt;/code&gt; 引用估计量的名称，请参见&lt;a href=&quot;compose#pipeline-nested-parameters&quot;&gt;嵌套参数&lt;/a&gt;。实际上，可以有多个级别的嵌套：</target>
        </trans-unit>
        <trans-unit id="0260adbe3be54cb933a36e08a92f87d76459f0fc" translate="yes" xml:space="preserve">
          <source>Here, \(\alpha \geq 0\) is a complexity parameter that controls the amount of shrinkage: the larger the value of \(\alpha\), the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.</source>
          <target state="translated">这里,\(\alpha \geq 0)\是一个控制收缩量的复杂性参数:\(\alpha \)的值越大,收缩量越大,因此系数对一致性的影响也越大。</target>
        </trans-unit>
        <trans-unit id="9412689bc5806775f9bf0419d0db25d5c39e9741" translate="yes" xml:space="preserve">
          <source>Here, the classifier is &lt;code&gt;fit()&lt;/code&gt; on a 2d binary label representation of &lt;code&gt;y&lt;/code&gt;, using the &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.labelbinarizer#sklearn.preprocessing.LabelBinarizer&quot;&gt;&lt;code&gt;LabelBinarizer&lt;/code&gt;&lt;/a&gt;. In this case &lt;code&gt;predict()&lt;/code&gt; returns a 2d array representing the corresponding multilabel predictions.</source>
          <target state="translated">在这里，分类器使用&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.labelbinarizer#sklearn.preprocessing.LabelBinarizer&quot;&gt; &lt;code&gt;LabelBinarizer&lt;/code&gt; &lt;/a&gt;在 &lt;code&gt;y&lt;/code&gt; 的2d二进制标签表示上进行 &lt;code&gt;fit()&lt;/code&gt; 。在这种情况下， &lt;code&gt;predict()&lt;/code&gt; 返回表示相应多标签预测的2d数组。</target>
        </trans-unit>
        <trans-unit id="e42ec4b790491f01a91defa6334fdc833c4f6019" translate="yes" xml:space="preserve">
          <source>Here, the default kernel &lt;code&gt;rbf&lt;/code&gt; is first changed to &lt;code&gt;linear&lt;/code&gt; via &lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC.set_params&quot;&gt;&lt;code&gt;SVC.set_params()&lt;/code&gt;&lt;/a&gt; after the estimator has been constructed, and changed back to &lt;code&gt;rbf&lt;/code&gt; to refit the estimator and to make a second prediction.</source>
          <target state="translated">这里，默认内核 &lt;code&gt;rbf&lt;/code&gt; 首先改变为 &lt;code&gt;linear&lt;/code&gt; 经由&lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC.set_params&quot;&gt; &lt;code&gt;SVC.set_params()&lt;/code&gt; &lt;/a&gt;估计器被构造之后和变回 &lt;code&gt;rbf&lt;/code&gt; 改装的估计器和使一个第二预测。</target>
        </trans-unit>
        <trans-unit id="379b23b33ba6458bba2f568a4c2b136ad7c827a5" translate="yes" xml:space="preserve">
          <source>Here, the first &lt;code&gt;predict()&lt;/code&gt; returns an integer array, since &lt;code&gt;iris.target&lt;/code&gt; (an integer array) was used in &lt;code&gt;fit&lt;/code&gt;. The second &lt;code&gt;predict()&lt;/code&gt; returns a string array, since &lt;code&gt;iris.target_names&lt;/code&gt; was for fitting.</source>
          <target state="translated">在这里，第一个 &lt;code&gt;predict()&lt;/code&gt; 返回一个整数数组，因为在 &lt;code&gt;fit&lt;/code&gt; 中使用了 &lt;code&gt;iris.target&lt;/code&gt; （一个整数数组）。由于 &lt;code&gt;iris.target_names&lt;/code&gt; 适合拟合，所以第二个 &lt;code&gt;predict()&lt;/code&gt; 返回一个字符串数组。</target>
        </trans-unit>
        <trans-unit id="21a97ae1e0557499a4ed4420c47199de8f6f0cde" translate="yes" xml:space="preserve">
          <source>Here, the number of samples is slightly larger than the number of dimensions, thus the empirical covariance is still invertible. However, as the observations are strongly correlated, the empirical covariance matrix is ill-conditioned and as a result its inverse &amp;ndash;the empirical precision matrix&amp;ndash; is very far from the ground truth.</source>
          <target state="translated">在这里，样本数略大于维度数，因此经验协方差仍然是可逆的。但是，由于观测值之间有很强的相关性，因此经验协方差矩阵条件不佳，因此其逆矩阵（经验精度矩阵）与基本事实相距甚远。</target>
        </trans-unit>
        <trans-unit id="2c6a31e993187ebfe932ff15824a46e0c83fd078" translate="yes" xml:space="preserve">
          <source>Here, the predicted class label is 2, since it has the highest average probability.</source>
          <target state="translated">这里,预测的类标签是2,因为它的平均概率最高。</target>
        </trans-unit>
        <trans-unit id="2ac251de8487bd51d665dc484131dc49cb351bf8" translate="yes" xml:space="preserve">
          <source>Here, the scores for the test data call for caution as they are significantly worse than for the training data indicating an overfit despite the strong regularization.</source>
          <target state="translated">在这里,测试数据的分数需要谨慎,因为它们比训练数据的分数要差很多,说明尽管有很强的正则化,但还是存在过拟合。</target>
        </trans-unit>
        <trans-unit id="e4ef491b953b35546ea7174e484d0bf1fa4f7b9b" translate="yes" xml:space="preserve">
          <source>Here, we are penalizing samples whose prediction is at least \(\varepsilon\) away from their true target. These samples penalize the objective by \(\zeta_i\) or \(\zeta_i^*\), depending on whether their predictions lie above or below the \(\varepsilon\) tube.</source>
          <target state="translated">在这里,我们惩罚那些预测与真实目标至少相差 %(varepsilon)的样本。这些样本对目标进行惩罚的方式是(zeta_i/)或(zeta_i^*/),这取决于它们的预测是在(varepsilon)管之上还是之下。</target>
        </trans-unit>
        <trans-unit id="3799f87043c16356967f90e71e555e0bd9e10d17" translate="yes" xml:space="preserve">
          <source>Here, we combine 3 learners (linear and non-linear) and use a ridge regressor to combine their outputs together.</source>
          <target state="translated">在这里,我们将3个学习者(线性和非线性)结合在一起,并使用一个山脊回归器将他们的输出结合在一起。</target>
        </trans-unit>
        <trans-unit id="1a1c9125aac07396a0a83a0358b1364478df4bbe" translate="yes" xml:space="preserve">
          <source>Here, we plot the partial dependence curves for a single feature, &amp;ldquo;age&amp;rdquo;, on the same axes. In this case, &lt;code&gt;tree_disp.axes_&lt;/code&gt; is passed into the second plot function.</source>
          <target state="translated">在这里，我们在同一根轴上绘制单个特征&amp;ldquo;年龄&amp;rdquo;的部分依赖曲线。在这种情况下， &lt;code&gt;tree_disp.axes_&lt;/code&gt; 被传递到第二个绘图函数中。</target>
        </trans-unit>
        <trans-unit id="69bdd751bbdbd7bb5746658b628d98b5240af47b" translate="yes" xml:space="preserve">
          <source>Here, we used the default hyperparameters for the gradient boosting model without any preprocessing as tree-based models are naturally robust to monotonic transformations of numerical features.</source>
          <target state="translated">在这里,我们使用了默认的梯度提升模型的超参数,而没有进行任何预处理,因为基于树的模型对数值特征的单调变换具有天然的鲁棒性。</target>
        </trans-unit>
        <trans-unit id="264995c0dc7ac7309d4709ed0ce1258e4439b015" translate="yes" xml:space="preserve">
          <source>Hessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method of solving the regularization problem of LLE. It revolves around a hessian-based quadratic form at each neighborhood which is used to recover the locally linear structure. Though other implementations note its poor scaling with data size, &lt;code&gt;sklearn&lt;/code&gt; implements some algorithmic improvements which make its cost comparable to that of other LLE variants for small output dimension. HLLE can be performed with function &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt;&lt;code&gt;locally_linear_embedding&lt;/code&gt;&lt;/a&gt; or its object-oriented counterpart &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;, with the keyword &lt;code&gt;method = 'hessian'&lt;/code&gt;. It requires &lt;code&gt;n_neighbors &amp;gt; n_components * (n_components + 3) / 2&lt;/code&gt;.</source>
          <target state="translated">Hessian特征映射（也称为基于Hessian的LLE：HLLE）是解决LLE正则化问题的另一种方法。它围绕每个邻域的基于粗麻布的二次形式旋转，用于恢复局部线性结构。尽管其他实现注意到其在数据大小上的缩放性较差，但 &lt;code&gt;sklearn&lt;/code&gt; 实现了一些算法上的改进，使其成本可与小输出尺寸的其他LLE变体相比。 HLLE可以通过函数&lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt; &lt;code&gt;locally_linear_embedding&lt;/code&gt; &lt;/a&gt;或其面向对象的对&lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt; LocallyLinearEmbedding来执行，并使用关键字 &lt;code&gt;method = 'hessian'&lt;/code&gt; 。它需要 &lt;code&gt;n_neighbors &amp;gt; n_components * (n_components + 3) / 2&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4b22ade72c6627b1b562254b0df9c6b4d814d7ac" translate="yes" xml:space="preserve">
          <source>Hidden Activation sampled from the model distribution, where batch_size in the number of examples per minibatch and n_components is the number of hidden units.</source>
          <target state="translated">隐藏的Activation从模型分布中取样,其中batch_size为每个minibatch的例子数量,n_components为隐藏单元的数量。</target>
        </trans-unit>
        <trans-unit id="afac02e66e409c4004e2cc2adafb5b5e842109eb" translate="yes" xml:space="preserve">
          <source>Hierarchical agglomerative clustering: Ward</source>
          <target state="translated">层次聚类聚类。沃德</target>
        </trans-unit>
        <trans-unit id="09f7d65b121068e93f6d1d655d20b242aded6b7b" translate="yes" xml:space="preserve">
          <source>Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. See the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;Wikipedia page&lt;/a&gt; for more details.</source>
          <target state="translated">分层聚类是一般的聚类算法家族，它们通过依次合并或拆分嵌套聚类来构建它们。群集的这种层次结构表示为树（或树状图）。树的根是聚集所有样本的唯一簇，叶子是只有一个样本的簇。有关更多详细信息，请参见&lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;Wikipedia页面&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="dbe40063cd6e20f1519715e45afa5ca7ed6442de" translate="yes" xml:space="preserve">
          <source>Hierarchical clustering: structured vs unstructured ward</source>
          <target state="translated">层次聚类:结构化病房与非结构化病房的比较</target>
        </trans-unit>
        <trans-unit id="645ba4388b8ba9172558b321f188082e4d2fd9ef" translate="yes" xml:space="preserve">
          <source>High-dimensional datasets can be very difficult to visualize. While data in two or three dimensions can be plotted to show the inherent structure of the data, equivalent high-dimensional plots are much less intuitive. To aid visualization of the structure of a dataset, the dimension must be reduced in some way.</source>
          <target state="translated">高维数据集可能非常难以可视化。虽然可以绘制二维或三维数据来显示数据的固有结构,但等效的高维图就不那么直观了。为了帮助数据集结构的可视化,必须以某种方式减少维度。</target>
        </trans-unit>
        <trans-unit id="769f5c4cc60f755dfe8d93fd7b194f0c3a9b7156" translate="yes" xml:space="preserve">
          <source>Hinge (soft-margin): equivalent to Support Vector Classification. \(L(y_i, f(x_i)) = \max(0, 1 - y_i f(x_i))\).</source>
          <target state="translated">铰链(软边距):相当于支持向量分类。\(L(y_i,f(x_i))=\max(0,1-y_i f(x_i))\)。</target>
        </trans-unit>
        <trans-unit id="75c18021e736bcb99a099c597122a642054caa8c" translate="yes" xml:space="preserve">
          <source>Hinge: (soft-margin) Support Vector Machines.</source>
          <target state="translated">铰 。(软边际)支持向量机。</target>
        </trans-unit>
        <trans-unit id="a319ae13863bb8d6da087a8b6e0305de9278e27f" translate="yes" xml:space="preserve">
          <source>Hinton, Geoffrey E.</source>
          <target state="translated">Hinton,Geoffrey E.</target>
        </trans-unit>
        <trans-unit id="04790fed22fa2f8c9274791cf963a575a884ed64" translate="yes" xml:space="preserve">
          <source>Hispanic</source>
          <target state="translated">Hispanic</target>
        </trans-unit>
        <trans-unit id="d6215d83f5c22f8bb7c1095fa0298c0dd2d51f9d" translate="yes" xml:space="preserve">
          <source>Histogram-based Gradient Boosting Classification Tree.</source>
          <target state="translated">基于直方图的梯度提升分类树。</target>
        </trans-unit>
        <trans-unit id="772913778b88215dfb5f0c612cecbcae2c4b1a8f" translate="yes" xml:space="preserve">
          <source>Histogram-based Gradient Boosting Regression Tree.</source>
          <target state="translated">基于直方图的梯度提升回归树。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
