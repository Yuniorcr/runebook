<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="d4e46a5f1bb78af76b3523ee33b19eb9c786ce08" translate="yes" xml:space="preserve">
          <source>A common practice for evaluating the results of image denoising is by looking at the difference between the reconstruction and the original image. If the reconstruction is perfect this will look like Gaussian noise.</source>
          <target state="translated">评估图像去噪结果的一个常见做法是通过观察重建和原始图像之间的差异。如果重建是完美的,这将看起来像高斯噪声。</target>
        </trans-unit>
        <trans-unit id="4ed0a0436668c33a0351f627732f0587ac7983ed" translate="yes" xml:space="preserve">
          <source>A comparison of a several classifiers in scikit-learn on synthetic datasets. The point of this example is to illustrate the nature of decision boundaries of different classifiers. This should be taken with a grain of salt, as the intuition conveyed by these examples does not necessarily carry over to real datasets.</source>
          <target state="translated">scikit-learn中的几种分类器在合成数据集上的比较。这个例子的目的是为了说明不同分类器的决策边界的性质。这应该被视为一种盐分,因为这些例子所传达的直觉不一定能带到真实的数据集上。</target>
        </trans-unit>
        <trans-unit id="08896462e97fc3765d56051f9cf494df2d13585e" translate="yes" xml:space="preserve">
          <source>A comparison of different values for regularization parameter &amp;lsquo;alpha&amp;rsquo; on synthetic datasets. The plot shows that different alphas yield different decision functions.</source>
          <target state="translated">合成数据集上正则化参数&amp;ldquo; alpha&amp;rdquo;的不同值的比较。该图显示不同的alpha产生不同的决策函数。</target>
        </trans-unit>
        <trans-unit id="5ffdb4122629b0408fc648de5eb2e8856b001ee3" translate="yes" xml:space="preserve">
          <source>A comparison of the clustering algorithms in scikit-learn</source>
          <target state="translated">scikit-learn中聚类算法的比较。</target>
        </trans-unit>
        <trans-unit id="3aba9c608b9764b22ebdea01be3ad9e7396b0707" translate="yes" xml:space="preserve">
          <source>A comparison of the outlier detection algorithms in scikit-learn. Local Outlier Factor (LOF) does not show a decision boundary in black as it has no predict method to be applied on new data when it is used for outlier detection.</source>
          <target state="translated">scikit-learn中的离群点检测算法的比较。局部离群值因子(LOF)在用于离群值检测时,由于没有对新数据进行预测的方法,所以没有显示黑色的决策边界。</target>
        </trans-unit>
        <trans-unit id="d90a43f9bc5a8cfdf7c44591b758b4103cd4b78a" translate="yes" xml:space="preserve">
          <source>A complete example of this classification problem is available as an example that you can run and study: &lt;a href=&quot;../../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;Recognizing hand-written digits&lt;/a&gt;.</source>
          <target state="translated">您可以运行和研究一个有关此分类问题的完整示例：&lt;a href=&quot;../../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;识别手写数字&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="5fa8afb91b45355653b6f013837f985ec55a809c" translate="yes" xml:space="preserve">
          <source>A constant prediction baseline</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4711e9024cec2c4a1383670cb0218b599982c096" translate="yes" xml:space="preserve">
          <source>A context object for caching a function&amp;rsquo;s return value each time it is called with the same input arguments.</source>
          <target state="translated">一个上下文对象，用于在每次使用相同的输入参数调用函数时将其返回值缓存。</target>
        </trans-unit>
        <trans-unit id="198c09de283f0c6001f8ae5be18eb83356d81a3a" translate="yes" xml:space="preserve">
          <source>A contiguous slice of distance matrix, optionally processed by &lt;code&gt;reduce_func&lt;/code&gt;.</source>
          <target state="translated">距离矩阵的连续切片，可以选择由 &lt;code&gt;reduce_func&lt;/code&gt; 处理。</target>
        </trans-unit>
        <trans-unit id="2ac4a68a26c3c2eae7c82c7be7376e0b1206d26c" translate="yes" xml:space="preserve">
          <source>A contingency matrix given by the &lt;code&gt;contingency_matrix&lt;/code&gt; function. If value is &lt;code&gt;None&lt;/code&gt;, it will be computed, otherwise the given value is used, with &lt;code&gt;labels_true&lt;/code&gt; and &lt;code&gt;labels_pred&lt;/code&gt; ignored.</source>
          <target state="translated">由 &lt;code&gt;contingency_matrix&lt;/code&gt; 函数给定的列联矩阵。如果value为 &lt;code&gt;None&lt;/code&gt; ，它将被计算，否则使用给定的值，而 &lt;code&gt;labels_true&lt;/code&gt; 和 &lt;code&gt;labels_pred&lt;/code&gt; 被忽略。</target>
        </trans-unit>
        <trans-unit id="e9c011a87c5032bcc5be043ba28885f86dd1572a" translate="yes" xml:space="preserve">
          <source>A continuous log-uniform random variable is available through &lt;code&gt;loguniform&lt;/code&gt;. This is a continuous version of log-spaced parameters. For example to specify &lt;code&gt;C&lt;/code&gt; above, &lt;code&gt;loguniform(1,
100)&lt;/code&gt; can be used instead of &lt;code&gt;[1, 10, 100]&lt;/code&gt; or &lt;code&gt;np.logspace(0, 2,
num=1000)&lt;/code&gt;. This is an alias to SciPy&amp;rsquo;s &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.reciprocal.html&quot;&gt;stats.reciprocal&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c54ec3e229cd991b7b39939aa120ad2133da3723" translate="yes" xml:space="preserve">
          <source>A copy of the &lt;code&gt;classes&lt;/code&gt; parameter where provided, or otherwise, the sorted set of classes found when fitting.</source>
          <target state="translated">提供 &lt;code&gt;classes&lt;/code&gt; 参数的副本，或者提供适合时找到的分类的排序集。</target>
        </trans-unit>
        <trans-unit id="0460778fea705652baf3b5d397ac145e2c8d559f" translate="yes" xml:space="preserve">
          <source>A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.</source>
          <target state="translated">因此,文档的语料库可以用一个矩阵来表示,每个文档为一行,语料库中出现的每个标记(如单词)为一列。</target>
        </trans-unit>
        <trans-unit id="b1d91a72335bd05329f277d8169a200f3a7e7461" translate="yes" xml:space="preserve">
          <source>A cross-validation generator splits the whole dataset k times in training and test data. Subsets of the training set with varying sizes will be used to train the estimator and a score for each training subset size and the test set will be computed. Afterwards, the scores will be averaged over all k runs for each training subset size.</source>
          <target state="translated">一个交叉验证生成器将整个数据集的训练和测试数据分割k次。训练集的不同大小的子集将被用来训练估计器,并计算每个训练子集大小和测试集的得分。之后,将对每个训练子集大小的所有k次运行的分数进行平均。</target>
        </trans-unit>
        <trans-unit id="2e60520122aededd78427759b8d5e2ce36a7e309" translate="yes" xml:space="preserve">
          <source>A dataset is a dictionary-like object that holds all the data and some metadata about the data. This data is stored in the &lt;code&gt;.data&lt;/code&gt; member, which is a &lt;code&gt;n_samples, n_features&lt;/code&gt; array. In the case of supervised problem, one or more response variables are stored in the &lt;code&gt;.target&lt;/code&gt; member. More details on the different datasets can be found in the &lt;a href=&quot;../../datasets/index#datasets&quot;&gt;dedicated section&lt;/a&gt;.</source>
          <target state="translated">数据集是一个类似于字典的对象，其中包含所有数据和有关该数据的一些元数据。此数据存储在 &lt;code&gt;.data&lt;/code&gt; 成员中，该成员是一个 &lt;code&gt;n_samples, n_features&lt;/code&gt; 数组。在监督问题的情况下，一个或多个响应变量存储在 &lt;code&gt;.target&lt;/code&gt; 成员中。有关不同数据集的更多详细信息，请参见&lt;a href=&quot;../../datasets/index#datasets&quot;&gt;专用部分&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c689c278ed2c402419ef82b79e626634acb9d7dd" translate="yes" xml:space="preserve">
          <source>A dataset is uniquely specified by its &lt;code&gt;data_id&lt;/code&gt;, but not necessarily by its name. Several different &amp;ldquo;versions&amp;rdquo; of a dataset with the same name can exist which can contain entirely different datasets. If a particular version of a dataset has been found to contain significant issues, it might be deactivated. Using a name to specify a dataset will yield the earliest version of a dataset that is still active. That means that &lt;code&gt;fetch_openml(name=&quot;miceprotein&quot;)&lt;/code&gt; can yield different results at different times if earlier versions become inactive. You can see that the dataset with &lt;code&gt;data_id&lt;/code&gt; 40966 that we fetched above is the version 1 of the &amp;ldquo;miceprotein&amp;rdquo; dataset:</source>
          <target state="translated">数据集由其 &lt;code&gt;data_id&lt;/code&gt; 唯一指定，但不一定由其名称指定。可以存在具有相同名称的数据集的多个不同&amp;ldquo;版本&amp;rdquo;，它们可以包含完全不同的数据集。如果发现某个数据集的特定版本包含重大问题，则可以将其停用。使用名称指定数据集将产生仍处于活动状态的数据集的最早版本。这意味着，如果早期版本不活动，则 &lt;code&gt;fetch_openml(name=&quot;miceprotein&quot;)&lt;/code&gt; 可以在不同时间产生不同的结果。您可以看到我们上面获取的 &lt;code&gt;data_id&lt;/code&gt; 为 40966 的数据集是&amp;ldquo;小鼠蛋白&amp;rdquo;数据集的版本1：</target>
        </trans-unit>
        <trans-unit id="015b04be42703b2ddb8ba533bada1a317c896a28" translate="yes" xml:space="preserve">
          <source>A decision tree classifier.</source>
          <target state="translated">决策树分类器。</target>
        </trans-unit>
        <trans-unit id="2630b9dcfdbd7604846f7a233491e190f45a9740" translate="yes" xml:space="preserve">
          <source>A decision tree is boosted using the AdaBoost.R2 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt; algorithm on a 1D sinusoidal dataset with a small amount of Gaussian noise. 299 boosts (300 decision trees) is compared with a single decision tree regressor. As the number of boosts is increased the regressor can fit more detail.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c1457482037e3da549c4d599a20c71477d87290a" translate="yes" xml:space="preserve">
          <source>A decision tree is boosted using the AdaBoost.R2 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; algorithm on a 1D sinusoidal dataset with a small amount of Gaussian noise. 299 boosts (300 decision trees) is compared with a single decision tree regressor. As the number of boosts is increased the regressor can fit more detail.</source>
          <target state="translated">在具有少量高斯噪声的一维正弦数据集上，使用AdaBoost.R2 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;算法增强决策树。将299个提升（300个决策树）与单个决策树回归器进行比较。随着增强次数的增加，回归器可以拟合更多细节。</target>
        </trans-unit>
        <trans-unit id="98423fef8caf500459be69ac8cff91bec1b557c8" translate="yes" xml:space="preserve">
          <source>A decision tree regressor.</source>
          <target state="translated">一个决策树回归器。</target>
        </trans-unit>
        <trans-unit id="fac54dbce7b4a9529cb6b07cf132d857e0fb7a2e" translate="yes" xml:space="preserve">
          <source>A demo of K-Means clustering on the handwritten digits data</source>
          <target state="translated">手写数字数据的K-Means聚类演示</target>
        </trans-unit>
        <trans-unit id="985d8495d6de7661e6b4b5a0919641f87e6bc3f6" translate="yes" xml:space="preserve">
          <source>A demo of structured Ward hierarchical clustering on an image of coins</source>
          <target state="translated">在硬币图像上进行结构化Ward层次聚类的演示。</target>
        </trans-unit>
        <trans-unit id="728da4fce5aa5c78dc45e16c348781662170d8aa" translate="yes" xml:space="preserve">
          <source>A demo of the Spectral Biclustering algorithm</source>
          <target state="translated">频谱双聚类算法的演示。</target>
        </trans-unit>
        <trans-unit id="9435014b37ab8720ce8e8dead6831ddaa2609878" translate="yes" xml:space="preserve">
          <source>A demo of the Spectral Co-Clustering algorithm</source>
          <target state="translated">Spectral Co-Clustering算法的演示</target>
        </trans-unit>
        <trans-unit id="35f2c0aaf840683641795d09aa0a1e201f37f9de" translate="yes" xml:space="preserve">
          <source>A demo of the mean-shift clustering algorithm</source>
          <target state="translated">均值移动聚类算法的演示。</target>
        </trans-unit>
        <trans-unit id="d3781231daa22a497eef6f674b9f3cd7b216bf2a" translate="yes" xml:space="preserve">
          <source>A demonstration of feature discretization on synthetic classification datasets. Feature discretization decomposes each feature into a set of bins, here equally distributed in width. The discrete values are then one-hot encoded, and given to a linear classifier. This preprocessing enables a non-linear behavior even though the classifier is linear.</source>
          <target state="translated">在合成分类数据集上演示特征离散化。特征离散化将每个特征分解成一组bins,这里的bins宽度分布相等。然后将离散值进行一热编码,并交给一个线性分类器。这种预处理使得即使分类器是线性的,也能实现非线性行为。</target>
        </trans-unit>
        <trans-unit id="95eced0791e7dfa2447624723a21d4f3bf963a74" translate="yes" xml:space="preserve">
          <source>A detailed description of the algorithm can be found in the documentation of the &lt;code&gt;linear_model&lt;/code&gt; sub-package.</source>
          <target state="translated">可以在 &lt;code&gt;linear_model&lt;/code&gt; 子软件包的文档中找到该算法的详细说明。</target>
        </trans-unit>
        <trans-unit id="85ef0d1a4425c2a7d76f392327f60a1b7513d0f1" translate="yes" xml:space="preserve">
          <source>A dict of arrays containing the score/time arrays for each scorer is returned. The possible keys for this &lt;code&gt;dict&lt;/code&gt; are:</source>
          <target state="translated">返回包含每个得分者得分/时间数组的数组的字典。此 &lt;code&gt;dict&lt;/code&gt; 的可能键为：</target>
        </trans-unit>
        <trans-unit id="111521e1b62dc92a5d55cc421f659d37509b3d83" translate="yes" xml:space="preserve">
          <source>A dict with keys as column headers and values as columns, that can be imported into a pandas &lt;code&gt;DataFrame&lt;/code&gt;.</source>
          <target state="translated">可以将键作为列标题和值作为列的字典，可以将其导入pandas &lt;code&gt;DataFrame&lt;/code&gt; 中。</target>
        </trans-unit>
        <trans-unit id="c8159c56c705a647167c0db6b7eec6aec52babc5" translate="yes" xml:space="preserve">
          <source>A dictionary mapping feature names to feature indices.</source>
          <target state="translated">一个将特征名称映射到特征索引的字典。</target>
        </trans-unit>
        <trans-unit id="d69a0d863960291435690723c91aee6afd547cfb" translate="yes" xml:space="preserve">
          <source>A dictionary of {dataset_name: data_dict}, or {dataset_name: (data_dict, ordering). &lt;code&gt;data_dict&lt;/code&gt; itself is a dictionary of {column_name: data_array}, and &lt;code&gt;ordering&lt;/code&gt; is a list of column_names to determine the ordering in the data set (see &lt;code&gt;fake_mldata&lt;/code&gt; for details).</source>
          <target state="translated">{dataset_name：data_dict}或{dataset_name：（data_dict，排序）的字典。 &lt;code&gt;data_dict&lt;/code&gt; 本身是{column_name：data_array}的字典，而 &lt;code&gt;ordering&lt;/code&gt; 是column_name的列表，用于确定数据集中的顺序（有关详细信息，请参见 &lt;code&gt;fake_mldata&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="f0df6656a799b1b490376f17c246c83878449378" translate="yes" xml:space="preserve">
          <source>A different approach for approximating an additive variant of the chi squared kernel.</source>
          <target state="translated">一种不同的方法来逼近chi平方核的加法变体。</target>
        </trans-unit>
        <trans-unit id="183fe0e3f615bfbf70ad6cec7cbf647c2be26b19" translate="yes" xml:space="preserve">
          <source>A discrepancy between the number of terms reported for DictVectorizer and for FeatureHasher is to be expected due to hash collisions.</source>
          <target state="translated">DictVectorizer和FeatureHasher报告的术语数之间存在差异,这是由于哈希碰撞造成的。</target>
        </trans-unit>
        <trans-unit id="5359a8a92b18a35ca1595de7bb20b2a0c7e2882d" translate="yes" xml:space="preserve">
          <source>A distance matrix D such that D_{i, j} is the distance between the ith and jth vectors of the given matrix X, if Y is None. If Y is not None, then D_{i, j} is the distance between the ith array from X and the jth array from Y.</source>
          <target state="translated">一个距离矩阵D,使D_{i,j}是给定矩阵X的第i个数组和第j个数组之间的距离,如果Y是None。如果Y不是None,那么D_{i,j}是第i个数组与第j个数组之间的距离。</target>
        </trans-unit>
        <trans-unit id="91104d203f36a74818ec42e8962e98d2f258acd0" translate="yes" xml:space="preserve">
          <source>A document is a sequence of \(N\) words.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ce7ab60e9677f7708dd8a607942d2c0cccce5364" translate="yes" xml:space="preserve">
          <source>A feature array, or array of distances between samples if &lt;code&gt;metric='precomputed'&lt;/code&gt;.</source>
          <target state="translated">特征数组，或者如果 &lt;code&gt;metric='precomputed'&lt;/code&gt; ，则样本之间的距离数组。</target>
        </trans-unit>
        <trans-unit id="b2db61a5e11f04d01b2535f105cf88de339d5444" translate="yes" xml:space="preserve">
          <source>A feature array, or array of distances between samples if metric=&amp;rsquo;precomputed&amp;rsquo;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb559e312d64d98c335a9c4be5e943c082d9b127" translate="yes" xml:space="preserve">
          <source>A feature array, or array of distances between samples if metric=&amp;rsquo;precomputed&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="798664f5f45f763ed33c1b77b763944574aa4f4f" translate="yes" xml:space="preserve">
          <source>A few definitions:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86908273dab820320f981714d4826279093534cf" translate="yes" xml:space="preserve">
          <source>A few definitions: a &lt;em&gt;claim&lt;/em&gt; is the request made by a policyholder to the insurer to compensate for a loss covered by the insurance. The &lt;em&gt;claim amount&lt;/em&gt; is the amount of money that the insurer must pay. The &lt;em&gt;exposure&lt;/em&gt; is the duration of the insurance coverage of a given policy, in years.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="24d02e76c875ea70e2d1746a2ae2b7543f98dedf" translate="yes" xml:space="preserve">
          <source>A few features available in this model:</source>
          <target state="translated">这款车型所具备的一些功能。</target>
        </trans-unit>
        <trans-unit id="a596b46f1e97e6f01add8ad1641f2aa816638c4f" translate="yes" xml:space="preserve">
          <source>A figure object onto which the plots will be drawn, after the figure has been cleared. By default, a new one is created.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="24026427c908afb044a16ec627f1f7bf5b040d61" translate="yes" xml:space="preserve">
          <source>A fitted estimator object implementing &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predict&lt;/a&gt;, &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt;, or &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt;. Multioutput-multiclass classifiers are not supported.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="06304e4581de12feb43d312c564ae9a70106b9ba" translate="yes" xml:space="preserve">
          <source>A fitted gradient boosting model.</source>
          <target state="translated">一个拟合的梯度提升模型。</target>
        </trans-unit>
        <trans-unit id="ad78d890244343cb481ec0d46fb95ae01e19e178" translate="yes" xml:space="preserve">
          <source>A function to handle preprocessing, tokenization and n-grams generation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5c6e87ebdac3e8a70f2ac508aaddfaa8df544da7" translate="yes" xml:space="preserve">
          <source>A function to preprocess the text before tokenization.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="004f6b369bbbaaeac6e581b99b63af10868df162" translate="yes" xml:space="preserve">
          <source>A function to split a string into a sequence of tokens.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70c3f4d0379ae967616e2ce28baef3bbea4be9cf" translate="yes" xml:space="preserve">
          <source>A generator over parameter settings, constructed from param_distributions.</source>
          <target state="translated">参数设置的生成器,由param_distributions构建。</target>
        </trans-unit>
        <trans-unit id="ab5af31ec50522e8f84b5fd320b5185d1e649aae" translate="yes" xml:space="preserve">
          <source>A good introduction to Bayesian methods is given in C. Bishop: Pattern Recognition and Machine learning</source>
          <target state="translated">C.Bishop中对贝叶斯方法做了很好的介绍。模式识别和机器学习</target>
        </trans-unit>
        <trans-unit id="2abfd97b78d2efa9a468ce9e53e4e40c6a740a0c" translate="yes" xml:space="preserve">
          <source>A good value reported by this method does not imply the best information retrieval.</source>
          <target state="translated">这种方法报告的数值好,并不意味着信息检索效果最好。</target>
        </trans-unit>
        <trans-unit id="ee0139a4c757902f8a15e776dbb86ec75b8b58ee" translate="yes" xml:space="preserve">
          <source>A graphical overview of basic areas of machine learning, and guidance which kind of algorithms to use in a given situation.</source>
          <target state="translated">以图文并茂的方式概述机器学习的基本领域,并指导在特定情况下使用哪种算法。</target>
        </trans-unit>
        <trans-unit id="09adb6eadfd2176464d83487c19995d3ec4f7162" translate="yes" xml:space="preserve">
          <source>A histogram is a simple visualization of data where bins are defined, and the number of data points within each bin is tallied. An example of a histogram can be seen in the upper-left panel of the following figure:</source>
          <target state="translated">直方图是一种简单的数据可视化方法,在这里,定义了一个区块,并统计了每个区块内的数据点的数量。下图左上角是一个直方图的例子。</target>
        </trans-unit>
        <trans-unit id="7bef6839991533f8c855219658630fb1d97c3e13" translate="yes" xml:space="preserve">
          <source>A kernel between the gene sequences is defined using R-convolution &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;1&lt;/a&gt; by integrating a binary letter-wise kernel over all pairs of letters among a pair of strings.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9d7d25ec6ddbab84b2f2e99ec27b1c5e75159286" translate="yes" xml:space="preserve">
          <source>A kernel hyperparameter&amp;rsquo;s specification in form of a namedtuple.</source>
          <target state="translated">以namedtuple形式的内核超参数规范。</target>
        </trans-unit>
        <trans-unit id="9dd2e21d515f945fa766366930e3f2f884774833" translate="yes" xml:space="preserve">
          <source>A kernel matrix K such that K_{i, j} is the kernel between the ith and jth vectors of the given matrix X, if Y is None. If Y is not None, then K_{i, j} is the kernel between the ith array from X and the jth array from Y.</source>
          <target state="translated">一个核矩阵K,使得K_{i,j}是给定矩阵X的第i个和第j个向量之间的核,如果Y是None。如果Y不是None,那么K_{i,j}是X的第i个数组和Y的第j个数组之间的核。</target>
        </trans-unit>
        <trans-unit id="1c832d0ad2e131abf1539cfeafad1048954835e4" translate="yes" xml:space="preserve">
          <source>A larger &lt;code&gt;leaf_size&lt;/code&gt; leads to a faster tree construction time, because fewer nodes need to be created</source>
          <target state="translated">较大的 &lt;code&gt;leaf_size&lt;/code&gt; 导致更快的树构建时间，因为需要创建的节点更少</target>
        </trans-unit>
        <trans-unit id="5116c45ec5d86e0d701daa6fab158df33c292bf3" translate="yes" xml:space="preserve">
          <source>A larger number of split will provide no benefits if the number of training samples is large enough. Indeed, the training time will increase. &lt;code&gt;cv&lt;/code&gt; is not used for model evaluation but for prediction.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f498b24974153e962b445930a2302cf78d89cfc" translate="yes" xml:space="preserve">
          <source>A last major parameter is also the possibility to do predictions in bulk or one-at-a-time mode.</source>
          <target state="translated">最后一个主要参数也是可以进行批量或一次模式的预测。</target>
        </trans-unit>
        <trans-unit id="b50d9ba875cfa3e1bbfa88757252d5cc5b663f5e" translate="yes" xml:space="preserve">
          <source>A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. Consider the following example where we plot the learning curve of a naive Bayes classifier and an SVM.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7ca8b138a43ce55d825b3e612cada7898d228b4c" translate="yes" xml:space="preserve">
          <source>A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. If both the validation score and the training score converge to a value that is too low with increasing size of the training set, we will not benefit much from more training data. In the following plot you can see an example: naive Bayes roughly converges to a low score.</source>
          <target state="translated">学习曲线显示了一个估计器在不同数量的训练样本下的验证和训练得分。它是一种工具,可以发现我们从增加更多的训练数据中获得了多少好处,以及估计器是否遭受了更多的方差误差或偏差误差。如果验证得分和训练得分都收敛到一个太低的值,随着训练集大小的增加,我们将不会从更多的训练数据中获得多少好处。在下面的图中,你可以看到一个例子:奈夫贝叶斯大致收敛到一个低分。</target>
        </trans-unit>
        <trans-unit id="2cc6d3a8d1e5bdfb3693da69e5fd147a58a0255e" translate="yes" xml:space="preserve">
          <source>A list of arguments name to ignore in the hashing</source>
          <target state="translated">在散列中要忽略的参数名称列表。</target>
        </trans-unit>
        <trans-unit id="977999f316fb9d6c637d2852021b1d8363940c27" translate="yes" xml:space="preserve">
          <source>A list of arrays of length &lt;code&gt;len(estimators_)&lt;/code&gt; containing the class labels for each estimator in the chain.</source>
          <target state="translated">长度为 &lt;code&gt;len(estimators_)&lt;/code&gt; 的数组列表，其中包含链中每个估计量的类标签。</target>
        </trans-unit>
        <trans-unit id="8b21b4fe65d64da1a4d1d84446f550bb55759446" translate="yes" xml:space="preserve">
          <source>A list of class labels known to the classifier.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c5eaed1c27ab8dfc53ee38efebd6c102a71098c5" translate="yes" xml:space="preserve">
          <source>A list of classes or column indices to select some (or to force inclusion of classes absent from the data)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a8fb99b6c7b15c5d0f9079fcee9eb1e89cb6ab1c" translate="yes" xml:space="preserve">
          <source>A list of clones of base_estimator.</source>
          <target state="translated">base_estimator的克隆列表。</target>
        </trans-unit>
        <trans-unit id="53426e5188a28c01da334e8bfdbe7c9957c50862" translate="yes" xml:space="preserve">
          <source>A list of feature names.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="371683d834fcd55b1b47a55ccdff2980b0102eb8" translate="yes" xml:space="preserve">
          <source>A list of length n_features containing the feature names (e.g., &amp;ldquo;f=ham&amp;rdquo; and &amp;ldquo;f=spam&amp;rdquo;).</source>
          <target state="translated">包含要素名称（例如&amp;ldquo; f = ham&amp;rdquo;和&amp;ldquo; f = spam&amp;rdquo;）的长度为n_features的列表。</target>
        </trans-unit>
        <trans-unit id="3e9dc6d758a5816faa5f49abd255ba444796f1a7" translate="yes" xml:space="preserve">
          <source>A list of length n_features containing the feature names. If None generic names will be used (&amp;ldquo;feature_0&amp;rdquo;, &amp;ldquo;feature_1&amp;rdquo;, &amp;hellip;).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7951c01a96e3650048b499324860f7763c837156" translate="yes" xml:space="preserve">
          <source>A list of stop words.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c84a50cbe409f7bcbe9aef60e47724b3d0374c13" translate="yes" xml:space="preserve">
          <source>A logistic regression with L1 penalty yields sparse models, and can thus be used to perform feature selection, as detailed in &lt;a href=&quot;feature_selection#l1-feature-selection&quot;&gt;L1-based feature selection&lt;/a&gt;.</source>
          <target state="translated">具有L1罚分的逻辑回归产生稀疏模型，因此可以用于执行特征选择，如&lt;a href=&quot;feature_selection#l1-feature-selection&quot;&gt;基于L1的特征选择中所述&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="2e8f6767450aa8f965fa6ba15414ee47e53d061d" translate="yes" xml:space="preserve">
          <source>A logistic regression with \(\ell_1\) penalty yields sparse models, and can thus be used to perform feature selection, as detailed in &lt;a href=&quot;feature_selection#l1-feature-selection&quot;&gt;L1-based feature selection&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed6a763f542c3b72714c079a2432308cb03a943b" translate="yes" xml:space="preserve">
          <source>A major difference is that GPR can choose the kernel&amp;rsquo;s hyperparameters based on gradient-ascent on the marginal likelihood function while KRR needs to perform a grid search on a cross-validated loss function (mean-squared error loss). A further difference is that GPR learns a generative, probabilistic model of the target function and can thus provide meaningful confidence intervals and posterior samples along with the predictions while KRR only provides predictions.</source>
          <target state="translated">主要区别在于，GPR可以基于边际似然函数中的梯度上升来选择内核的超参数，而KRR需要对交叉验证的损失函数（均方误差损失）执行网格搜索。另一个不同之处在于，GPR学习了目标函数的生成概率模型，因此可以提供有意义的置信区间和后验样本以及预测，而KRR仅提供预测。</target>
        </trans-unit>
        <trans-unit id="413fd0e2f5ff27577a8397f2320f65ec5d748528" translate="yes" xml:space="preserve">
          <source>A major motivation of this method is F1-scoring, when the positive class is in the minority.</source>
          <target state="translated">这种方法的一个主要动因是F1评分,当正类是少数。</target>
        </trans-unit>
        <trans-unit id="ef86aa8565e00b57d2c91be8664b6a11d798a158" translate="yes" xml:space="preserve">
          <source>A major problem with histograms, however, is that the choice of binning can have a disproportionate effect on the resulting visualization. Consider the upper-right panel of the above figure. It shows a histogram over the same data, with the bins shifted right. The results of the two visualizations look entirely different, and might lead to different interpretations of the data.</source>
          <target state="translated">然而,直方图的一个主要问题是,分层的选择会对结果的可视化产生不成比例的影响。考虑上图的右上角面板。它显示的是相同数据的直方图,但分层向右移动。两种可视化的结果看起来完全不同,并可能导致对数据的不同解释。</target>
        </trans-unit>
        <trans-unit id="b4945c81d17c82b3985167f2fe21eac206551aeb" translate="yes" xml:space="preserve">
          <source>A mapping of terms to feature indices.</source>
          <target state="translated">术语与特征指数的映射。</target>
        </trans-unit>
        <trans-unit id="d80b1965676bd061b195356f9e0d7d581d27bcda" translate="yes" xml:space="preserve">
          <source>A mask of the observations that have been used to compute the raw robust estimates of location and shape, before correction and re-weighting.</source>
          <target state="translated">在校正和重新加权之前,用于计算位置和形状的原始稳健估计的观测值的掩码。</target>
        </trans-unit>
        <trans-unit id="0ec3f28dc02dc03ba22d583507907375d9c62b5f" translate="yes" xml:space="preserve">
          <source>A mask of the observations that have been used to compute the re-weighted robust location and covariance estimates.</source>
          <target state="translated">用于计算重新加权的稳健位置和协方差估计的观测值的掩码。</target>
        </trans-unit>
        <trans-unit id="adea06a15a23aac7c9d9327f52e8025add2d08fa" translate="yes" xml:space="preserve">
          <source>A mask of the observations that have been used to compute the robust estimates of location and shape.</source>
          <target state="translated">用于计算位置和形状的稳健估计的观测值的掩码。</target>
        </trans-unit>
        <trans-unit id="8be3be97461376c787b398256771e5f8ab1ed15d" translate="yes" xml:space="preserve">
          <source>A matrix containing only 1s ands 0s.</source>
          <target state="translated">一个只包含1s和0s的矩阵。</target>
        </trans-unit>
        <trans-unit id="957a75d1eeb1c72e470b64334da958786ce3e979" translate="yes" xml:space="preserve">
          <source>A matrix of shape (n_samples, n_samples) will be created from this.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="259daba30c783e777c062485d929bc6df3c76ec7" translate="yes" xml:space="preserve">
          <source>A matrix of term/token counts.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="151a7f165ba08edb6c7ba1bd07841827b86e653b" translate="yes" xml:space="preserve">
          <source>A matrix such that &lt;code&gt;y_indicator[i, j] = 1&lt;/code&gt; iff &lt;code&gt;classes_[j]&lt;/code&gt; is in &lt;code&gt;y[i]&lt;/code&gt;, and 0 otherwise.</source>
          <target state="translated">的矩阵使得 &lt;code&gt;y_indicator[i, j] = 1&lt;/code&gt; 当且仅当 &lt;code&gt;classes_[j]&lt;/code&gt; 的是在 &lt;code&gt;y[i]&lt;/code&gt; ，否则为0。</target>
        </trans-unit>
        <trans-unit id="6ac1281f3fac07a890a7987a1940a19d553f0540" translate="yes" xml:space="preserve">
          <source>A model is trained using \(k-1\) of the folds as training data;</source>
          <target state="translated">用折线的/(k-1)/作为训练数据来训练模型。</target>
        </trans-unit>
        <trans-unit id="ca1ea4d381438e8c7ee31dbdc82c2c3f3b443e43" translate="yes" xml:space="preserve">
          <source>A more detailed summary of the search is available at &lt;code&gt;gs_clf.cv_results_&lt;/code&gt;.</source>
          <target state="translated">有关搜索的详细摘要，请访问 &lt;code&gt;gs_clf.cv_results_&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0abf01b95edac52233877bfdb4963759d68aad5b" translate="yes" xml:space="preserve">
          <source>A more sophisticated approach is to use the &lt;a href=&quot;generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;IterativeImputer&lt;/code&gt;&lt;/a&gt; class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output &lt;code&gt;y&lt;/code&gt; and the other feature columns are treated as inputs &lt;code&gt;X&lt;/code&gt;. A regressor is fit on &lt;code&gt;(X,
y)&lt;/code&gt; for known &lt;code&gt;y&lt;/code&gt;. Then, the regressor is used to predict the missing values of &lt;code&gt;y&lt;/code&gt;. This is done for each feature in an iterative fashion, and then is repeated for &lt;code&gt;max_iter&lt;/code&gt; imputation rounds. The results of the final imputation round are returned.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e1a254f5635c860c720fd64108216f6d8c4b1064" translate="yes" xml:space="preserve">
          <source>A more traditional (and possibly better) way to predict on a sparse subset of input features would be to use univariate feature selection followed by a traditional (l2-penalised) logistic regression model.</source>
          <target state="translated">在输入特征的稀疏子集上进行预测的更传统(可能更好)的方法是使用单变量特征选择,然后使用传统的(l2-penalised)逻辑回归模型。</target>
        </trans-unit>
        <trans-unit id="fc3b3b1809b0fddd0d084276f73f16c23ecf93ac" translate="yes" xml:space="preserve">
          <source>A multi-label model that arranges binary classifiers into a chain.</source>
          <target state="translated">一个多标签模型,将二元分类器排列成一个链条。</target>
        </trans-unit>
        <trans-unit id="07157eeb8d3c41fd6ae5f1ce4e96d98c52fa44ce" translate="yes" xml:space="preserve">
          <source>A multi-label model that arranges regressions into a chain.</source>
          <target state="translated">一个多标签模型,将回归安排成一个链条。</target>
        </trans-unit>
        <trans-unit id="9e05b7bdec595d3e973dea3540db250a47de0ecd" translate="yes" xml:space="preserve">
          <source>A multi-output problem is a supervised learning problem with several outputs to predict, that is when Y is a 2d array of size &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt;.</source>
          <target state="translated">多输出问题是一种有监督的学习问题，需要预测多个输出，也就是说，当Y是大小为 &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt; 的2d数组时。</target>
        </trans-unit>
        <trans-unit id="0d0f038bfe45a99c12fb61542c6de1b37df0ed3c" translate="yes" xml:space="preserve">
          <source>A new plotting API is available for creating visualizations. This new API allows for quickly adjusting the visuals of a plot without involving any recomputation. It is also possible to add different plots to the same figure. The following example illustrates &lt;a href=&quot;../../modules/generated/sklearn.metrics.plot_roc_curve#sklearn.metrics.plot_roc_curve&quot;&gt;&lt;code&gt;plot_roc_curve&lt;/code&gt;&lt;/a&gt;, but other plots utilities are supported like &lt;a href=&quot;../../modules/generated/sklearn.inspection.plot_partial_dependence#sklearn.inspection.plot_partial_dependence&quot;&gt;&lt;code&gt;plot_partial_dependence&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.metrics.plot_precision_recall_curve#sklearn.metrics.plot_precision_recall_curve&quot;&gt;&lt;code&gt;plot_precision_recall_curve&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.metrics.plot_confusion_matrix#sklearn.metrics.plot_confusion_matrix&quot;&gt;&lt;code&gt;plot_confusion_matrix&lt;/code&gt;&lt;/a&gt;. Read more about this new API in the &lt;a href=&quot;https://scikit-learn.org/0.23/visualizations.html#visualizations&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="df611f66ef7be3083c893378a9603e78fdf46201" translate="yes" xml:space="preserve">
          <source>A new sample is inserted into the root of the CF Tree which is a CF Node. It is then merged with the subcluster of the root, that has the smallest radius after merging, constrained by the threshold and branching factor conditions. If the subcluster has any child node, then this is done repeatedly till it reaches a leaf. After finding the nearest subcluster in the leaf, the properties of this subcluster and the parent subclusters are recursively updated.</source>
          <target state="translated">在CF树的根部插入一个新的样本,它是一个CF节点。然后,在阈值和分支因子条件的限制下,将其与根的子簇合并,合并后的子簇半径最小。如果该子簇有任何子节点,则反复进行,直到达到叶子。在叶子中找到最近的子簇后,递归更新这个子簇和父子簇的属性。</target>
        </trans-unit>
        <trans-unit id="06b4df58df49d31652c1e508653814db2c7c4b45" translate="yes" xml:space="preserve">
          <source>A node will be split if this split induces a decrease of the impurity greater than or equal to this value.</source>
          <target state="translated">如果这次分裂引起的杂质减少量大于或等于这个值,那么一个节点将被分裂。</target>
        </trans-unit>
        <trans-unit id="f2f0f4cbbbaee87b8b7904e37994705d9bd8774b" translate="yes" xml:space="preserve">
          <source>A noise-free case</source>
          <target state="translated">无噪音案例</target>
        </trans-unit>
        <trans-unit id="a9448dea0ed4384de356358aa919a44c7d032339" translate="yes" xml:space="preserve">
          <source>A noisy case with known noise-level per datapoint</source>
          <target state="translated">每个数据点的噪声水平已知的情况下,有噪声。</target>
        </trans-unit>
        <trans-unit id="b96ec23e53414db9b11937cf799cd6c1f6432a32" translate="yes" xml:space="preserve">
          <source>A non-negative floating point value (the best value is 0.0), or an array of floating point values, one for each individual target.</source>
          <target state="translated">一个非负的浮点值(最佳值为0.0),或一个浮点值的数组,每个目标都有一个。</target>
        </trans-unit>
        <trans-unit id="2ced62a814e481bd22a6aa7667bcac7ac64675cc" translate="yes" xml:space="preserve">
          <source>A non-negative floating point value (the best value is 0.0).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5ef179d616825b29b180ecdb28f9b1e0bfb64faf" translate="yes" xml:space="preserve">
          <source>A non-parametric supervised learning method used for classification. Creates a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9c1cbeaf3f9c4d5469e9c590dbc955ea1a377cb" translate="yes" xml:space="preserve">
          <source>A number between 0 and 1 will require fewer classifiers than one-vs-the-rest. In theory, &lt;code&gt;log2(n_classes) / n_classes&lt;/code&gt; is sufficient to represent each class unambiguously. However, in practice, it may not lead to good accuracy since &lt;code&gt;log2(n_classes)&lt;/code&gt; is much smaller than n_classes.</source>
          <target state="translated">一个介于0到1之间的数字将需要的分类器少于其余的1。从理论上讲， &lt;code&gt;log2(n_classes) / n_classes&lt;/code&gt; 足以明确表示每个类。但是，实际上，由于 &lt;code&gt;log2(n_classes)&lt;/code&gt; 比n_classes小得多，因此可能不会导致较高的准确性。</target>
        </trans-unit>
        <trans-unit id="75b2970d65b634b9e917fd730d5f1ac86be5fb84" translate="yes" xml:space="preserve">
          <source>A number greater than 1 will require more classifiers than one-vs-the-rest. In this case, some classifiers will in theory correct for the mistakes made by other classifiers, hence the name &amp;ldquo;error-correcting&amp;rdquo;. In practice, however, this may not happen as classifier mistakes will typically be correlated. The error-correcting output codes have a similar effect to bagging.</source>
          <target state="translated">大于1的数字将需要更多的分类器。在这种情况下，某些分类器理论上将纠正其他分类器所犯的错误，因此命名为&amp;ldquo;纠错&amp;rdquo;。然而，实际上，这可能不会发生，因为分类器错误通常会被关联。纠错输出代码与装袋具有相似的效果。</target>
        </trans-unit>
        <trans-unit id="303cdccb0a0b9ef8ee02f659c78579cbfb972892" translate="yes" xml:space="preserve">
          <source>A object of that type is instantiated for each grid point. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a &lt;code&gt;score&lt;/code&gt; function, or &lt;code&gt;scoring&lt;/code&gt; must be passed.</source>
          <target state="translated">将为每个网格点实例化该类型的对象。假定这样做是为了实现scikit-learn估计器接口。估算者需要提供 &lt;code&gt;score&lt;/code&gt; 函数，或者必须通过 &lt;code&gt;scoring&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="56be8330a991d6aa887f39d43a91aa1ac6760e20" translate="yes" xml:space="preserve">
          <source>A one-dimensional array of distances</source>
          <target state="translated">一个一维的距离数组</target>
        </trans-unit>
        <trans-unit id="77da2f75f17e6ea0f8d3b4beab5a8c65d7801934" translate="yes" xml:space="preserve">
          <source>A paragraph describing the characteristic of the dataset: its source, reference, etc.</source>
          <target state="translated">描述数据集特征的段落:数据集的来源、参考文献等。</target>
        </trans-unit>
        <trans-unit id="8831c41b4fb10cdce1530c46ecbd5c89f650c66c" translate="yes" xml:space="preserve">
          <source>A parameter can be given to allow K-means to be run in parallel, called &lt;code&gt;n_jobs&lt;/code&gt;. Giving this parameter a positive value uses that many processors (default: 1). A value of -1 uses all available processors, with -2 using one less, and so on. Parallelization generally speeds up computation at the cost of memory (in this case, multiple copies of centroids need to be stored, one for each job).</source>
          <target state="translated">可以指定一个参数以允许K-means并行运行，称为 &lt;code&gt;n_jobs&lt;/code&gt; 。给该参数一个正值将使用那么多处理器（默认值：1）。值-1使用所有可用的处理器，值-2减少一个，依此类推。并行化通常以内存为代价来加快计算速度（在这种情况下，需要存储多个质心副本，每个作业一个）。</target>
        </trans-unit>
        <trans-unit id="c76c7128eb10e519c4cde4416a2b5ebd1e864530" translate="yes" xml:space="preserve">
          <source>A plot that compares the various Beta-divergence loss functions supported by the Multiplicative-Update (&amp;lsquo;mu&amp;rsquo;) solver in &lt;a href=&quot;../../modules/generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;sklearn.decomposition.NMF&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">该图比较&lt;a href=&quot;../../modules/generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;sklearn.decomposition.NMF&lt;/code&gt; 中&lt;/a&gt;的乘性更新（'mu'）求解器支持的各种Beta散度损失函数。</target>
        </trans-unit>
        <trans-unit id="c428b2107a14d30575de38f7fc951c176630f469" translate="yes" xml:space="preserve">
          <source>A plot that compares the various convex loss functions supported by &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt; .</source>
          <target state="translated">比较&lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt; &lt;/a&gt;支持的各种凸损失函数的图。</target>
        </trans-unit>
        <trans-unit id="9c371e4431820ea4253b6da8f883638e2996e440" translate="yes" xml:space="preserve">
          <source>A plot will appear showing the top 5 most uncertain digits for each iteration of training. These may or may not contain mistakes, but we will train the next model with their true labels.</source>
          <target state="translated">会出现一个图,显示每次训练迭代的前5位最不确定的数字。这些数字可能包含或不包含错误,但我们将用它们的真实标签训练下一个模型。</target>
        </trans-unit>
        <trans-unit id="d0a927be776ddcc1a3b0a115e5ff9bf99815db03" translate="yes" xml:space="preserve">
          <source>A positive floating point value (the best value is 0.0).</source>
          <target state="translated">一个正的浮点值(最佳值是0.0)。</target>
        </trans-unit>
        <trans-unit id="2e47d08f91be7ad2360165e81b098e1d32db277c" translate="yes" xml:space="preserve">
          <source>A positive monotonic constraint is a constraint of the form:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9fa1e2038dd5a4a82d42f5d5bcbe0df63da34a6c" translate="yes" xml:space="preserve">
          <source>A practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridge&amp;rsquo;s stability under rotation.</source>
          <target state="translated">在Lasso和Ridge之间进行折衷的一个实际优势是，它允许Elastic-Net继承旋转条件下Ridge的某些稳定性。</target>
        </trans-unit>
        <trans-unit id="fd73c2da7375a102550eb9b6f798b301111f7f0e" translate="yes" xml:space="preserve">
          <source>A practical advantage of trading-off between Lasso and Ridge is that it allows Elastic-Net to inherit some of Ridge&amp;rsquo;s stability under rotation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0644879ffb5a73e0cac4122d0a74647c32667b9b" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator object or a seed for it if int. If &lt;code&gt;init='random'&lt;/code&gt;, &lt;code&gt;random_state&lt;/code&gt; is used to initialize the random transformation. If &lt;code&gt;init='pca'&lt;/code&gt;, &lt;code&gt;random_state&lt;/code&gt; is passed as an argument to PCA when initializing the transformation. Pass an int for reproducible results across multiple function calls. See :term: &lt;code&gt;Glossary &amp;lt;random_state&amp;gt;&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4370bf36aea658b96c83523c6e4601f6e89a2b33" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when &lt;code&gt;eigen_solver='amg'&lt;/code&gt; and by the K-Means initialization. Use an int to make the randomness deterministic. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5ce137982e9454ec15b3a9266e29af4a861559f3" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when eigen_solver == &amp;lsquo;amg&amp;rsquo; and by the K-Means initialization. Use an int to make the randomness deterministic. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">当eigen_solver =='amg'并通过K-Means初始化时，用于初始化lobpcg本征向量分解的伪随机数生成器。使用int可以确定随机性。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4da70ee0892b2024f8c0c33c157ae6f4ed05fab9" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when eigen_solver == &amp;lsquo;amg&amp;rsquo; and by the K-Means initialization. Use an int to make the randomness deterministic. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0c41ce7b7d007fb54c5cb75f35d5c711d3e908a9" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator used for the initialization of the lobpcg eigenvectors decomposition. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;amg&amp;rsquo;.</source>
          <target state="translated">用于lobpcg特征向量分解初始化的伪随机数生成器。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。在 &lt;code&gt;solver&lt;/code&gt; =='amg'时使用。</target>
        </trans-unit>
        <trans-unit id="8d91d68f1ffad3cd242144da0cec28854cf478b1" translate="yes" xml:space="preserve">
          <source>A pseudo random number generator used for the initialization of the lobpcg eigenvectors. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;amg&amp;rsquo;.</source>
          <target state="translated">用于lobpcg特征向量初始化的伪随机数生成器。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。在 &lt;code&gt;solver&lt;/code&gt; =='amg'时使用。</target>
        </trans-unit>
        <trans-unit id="ee2bf152e538d224d6fde70d9b0dd7a1cdbb5755" translate="yes" xml:space="preserve">
          <source>A random forest classifier.</source>
          <target state="translated">随机森林分类器。</target>
        </trans-unit>
        <trans-unit id="e60aacdcc2887bf4d3963c64796dccc1910a3c9c" translate="yes" xml:space="preserve">
          <source>A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if &lt;code&gt;bootstrap=True&lt;/code&gt; (default).</source>
          <target state="translated">随机森林是一种元估计量，它适合数据集各个子样本上的许多分类决策树，并使用平均数来提高预测准确性和控制过度拟合。子样本大小始终与原始输入样本大小相同，但是如果 &lt;code&gt;bootstrap=True&lt;/code&gt; （默认值），则将替换绘制样本。</target>
        </trans-unit>
        <trans-unit id="6d7fe13f3eb79eb708ba336fe6e8fed09167f0c0" translate="yes" xml:space="preserve">
          <source>A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the &lt;code&gt;max_samples&lt;/code&gt; parameter if &lt;code&gt;bootstrap=True&lt;/code&gt; (default), otherwise the whole dataset is used to build each tree.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6afddbc046b0606e01c6a10f7c579e615a468bc2" translate="yes" xml:space="preserve">
          <source>A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if &lt;code&gt;bootstrap=True&lt;/code&gt; (default).</source>
          <target state="translated">随机森林是一种元估计量，它适合数据集各个子样本上的许多决策树分类器，并使用平均数来提高预测准确性和控制过度拟合。子样本大小始终与原始输入样本大小相同，但是如果 &lt;code&gt;bootstrap=True&lt;/code&gt; （默认值），则将替换绘制样本。</target>
        </trans-unit>
        <trans-unit id="8049315f314116153326eb819ff277cc1b244e8c" translate="yes" xml:space="preserve">
          <source>A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the &lt;code&gt;max_samples&lt;/code&gt; parameter if &lt;code&gt;bootstrap=True&lt;/code&gt; (default), otherwise the whole dataset is used to build each tree.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4c30f02c56b168ad378f278a145ee7ca666d2b4b" translate="yes" xml:space="preserve">
          <source>A random forest regressor.</source>
          <target state="translated">一个随机森林回归者。</target>
        </trans-unit>
        <trans-unit id="517674d6fed45c030e35daafc36d33d7e1cb17fe" translate="yes" xml:space="preserve">
          <source>A random number generator instance to define the state of the random permutations generator. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator.</source>
          <target state="translated">一个随机数发生器实例,用于定义随机排列发生器的状态。如果给定一个整数,则固定种子。默认为全局的numpy随机数生成器。</target>
        </trans-unit>
        <trans-unit id="ad42d75eb3232815220e50c42e13b1c5a583a979" translate="yes" xml:space="preserve">
          <source>A random number generator instance to define the state of the random permutations generator. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">随机数生成器实例，用于定义随机排列生成器的状态。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="282455615e66e12f3be6646a3e4853abf573194f" translate="yes" xml:space="preserve">
          <source>A random number generator instance to define the state of the random permutations generator. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3de0a58d62c3c2bcc234bd4e43f83d387ef4775e" translate="yes" xml:space="preserve">
          <source>A random order for each round.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4166975533e11bd5bc32126880dea04c49362bfe" translate="yes" xml:space="preserve">
          <source>A randomized algorithm for the decomposition of matrices Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert</source>
          <target state="translated">矩阵分解的随机算法 Per-Gunnar Martinsson,Vladimir Rokhlin and Mark Tygert</target>
        </trans-unit>
        <trans-unit id="1bef857d80e5f022acdf94565f064371e78877b1" translate="yes" xml:space="preserve">
          <source>A recursive feature elimination example showing the relevance of pixels in a digit classification task.</source>
          <target state="translated">一个递归特征消除的例子显示了数字分类任务中像素的相关性。</target>
        </trans-unit>
        <trans-unit id="59abf2d80f7d73a7cc0db592c8bcede424657b87" translate="yes" xml:space="preserve">
          <source>A recursive feature elimination example with automatic tuning of the number of features selected with cross-validation.</source>
          <target state="translated">一个递归特征消除的例子,自动调整交叉验证选择特征的数量。</target>
        </trans-unit>
        <trans-unit id="dcc2f34db796875ac21f4e9f9639246afd97a032" translate="yes" xml:space="preserve">
          <source>A reference (and not a copy) of the first argument in the &lt;code&gt;fit()&lt;/code&gt; method is stored for future reference. If that array changes between the use of &lt;code&gt;fit()&lt;/code&gt; and &lt;code&gt;predict()&lt;/code&gt; you will have unexpected results.</source>
          <target state="translated">将保存 &lt;code&gt;fit()&lt;/code&gt; 方法中第一个参数的引用（而不是副本）以供将来参考。如果该数组在 &lt;code&gt;fit()&lt;/code&gt; 和 &lt;code&gt;predict()&lt;/code&gt; 的使用之间发生变化，您将得到意外的结果。</target>
        </trans-unit>
        <trans-unit id="d5fde252cc168f1a7ffd9faadab134fe0513cbda" translate="yes" xml:space="preserve">
          <source>A regressor which will be used to combine the base estimators. The default regressor is a &lt;code&gt;RidgeCV&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b815536c2d161fd6acf2bcd856fde57736561c74" translate="yes" xml:space="preserve">
          <source>A representation of the full diabetes dataset would involve 11 dimensions (10 feature dimensions and one of the target variable). It is hard to develop an intuition on such representation, but it may be useful to keep in mind that it would be a fairly &lt;em&gt;empty&lt;/em&gt; space.</source>
          <target state="translated">完整的糖尿病数据集的表示将涉及11个维度（10个特征维度和一个目标变量）。很难就这种表示法形成直觉，但记住这将是一个相当&lt;em&gt;空的&lt;/em&gt;空间，这可能会很有用。</target>
        </trans-unit>
        <trans-unit id="fb7d808e9a808e07d251ec0ab6593856a70a9b8b" translate="yes" xml:space="preserve">
          <source>A scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;.</source>
          <target state="translated">具有签名计 &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; 可调用对象/函数。</target>
        </trans-unit>
        <trans-unit id="cd458b3d90a800480ecc75cb9b07374a934f767b" translate="yes" xml:space="preserve">
          <source>A search consists of:</source>
          <target state="translated">搜索包括:</target>
        </trans-unit>
        <trans-unit id="4faa007b7ff82792cbf9f6b91017a8816af3cc88" translate="yes" xml:space="preserve">
          <source>A second feature array only if X has shape [n_samples_a, n_features].</source>
          <target state="translated">只有当X的形状为[n_samples_a,n_features]时,才会有第二个特征数组。</target>
        </trans-unit>
        <trans-unit id="b3743bb79cbeb6292d7788299c27fa0302da1677" translate="yes" xml:space="preserve">
          <source>A selection of dtypes to exclude. For more details, see &lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes&quot;&gt;&lt;code&gt;pandas.DataFrame.select_dtypes&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ed7392ea2bcc5742fc5a21d5c3ccda544426c7d" translate="yes" xml:space="preserve">
          <source>A selection of dtypes to include. For more details, see &lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes&quot;&gt;&lt;code&gt;pandas.DataFrame.select_dtypes&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee9834606bdd77c4a06472befa3674177dd91a7e" translate="yes" xml:space="preserve">
          <source>A seq of Axis objects, one for each subplot.</source>
          <target state="translated">轴对象的序列,每个子图一个。</target>
        </trans-unit>
        <trans-unit id="09a68a5ad2629329af5cff530eb7ba2ddf7ad320" translate="yes" xml:space="preserve">
          <source>A sequence of dicts signifies a sequence of grids to search, and is useful to avoid exploring parameter combinations that make no sense or have no effect. See the examples below.</source>
          <target state="translated">dicts序列表示要搜索的网格序列,对于避免探索没有意义或没有效果的参数组合很有用。请看下面的例子。</target>
        </trans-unit>
        <trans-unit id="5ad6cb6da544247e185919510fbbf9bbac2dfc37" translate="yes" xml:space="preserve">
          <source>A set of labels (any orderable and hashable object) for each sample. If the &lt;code&gt;classes&lt;/code&gt; parameter is set, &lt;code&gt;y&lt;/code&gt; will not be iterated.</source>
          <target state="translated">每个样本的一组标签（任何可排序和可哈希的对象）。如果设置了 &lt;code&gt;classes&lt;/code&gt; 参数，则不会迭代 &lt;code&gt;y&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7b19a5a4863d81ac862d0e9be41e4683721f5dd5" translate="yes" xml:space="preserve">
          <source>A similar clustering at multiple values of eps. Our implementation is optimized for memory usage.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eb593a41404fe5b14fb797c942b194b27f9badb7" translate="yes" xml:space="preserve">
          <source>A similar clustering for a specified neighborhood radius (eps). Our implementation is optimized for runtime.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="134a4703b97e34542f6c5ec20a8b6025ee87dd8f" translate="yes" xml:space="preserve">
          <source>A simple choice to construct \(R_ij\) so that it is nonnegative and symmetric is:</source>
          <target state="translated">一个简单的选择是构建一个非负的和对称的R_ij(R_ij)是:</target>
        </trans-unit>
        <trans-unit id="11b768780ec203929a5802877f9f67a2310663fd" translate="yes" xml:space="preserve">
          <source>A simple example shipped with scikit-learn: iris dataset</source>
          <target state="translated">scikit-learn的一个简单例子:虹膜数据集。</target>
        </trans-unit>
        <trans-unit id="8e839688e01ba01e9dfe145feb09335f1aa963d0" translate="yes" xml:space="preserve">
          <source>A simple example:</source>
          <target state="translated">一个简单的例子。</target>
        </trans-unit>
        <trans-unit id="2a7be91a45219aa7eb5e6e5584cdc7ab00269326" translate="yes" xml:space="preserve">
          <source>A simple graphical frontend for Libsvm mainly intended for didactic purposes. You can create data points by point and click and visualize the decision region induced by different kernels and parameter settings.</source>
          <target state="translated">Libsvm的一个简单的图形化前端,主要用于教学目的。您可以通过点和点击来创建数据点,并可视化不同内核和参数设置所引起的决策区域。</target>
        </trans-unit>
        <trans-unit id="08a70c5b3e130b5b3a3556a3a9d846d499b809af" translate="yes" xml:space="preserve">
          <source>A simple linear generative model with Gaussian latent variables.</source>
          <target state="translated">高斯潜变量的简单线性生成模型。</target>
        </trans-unit>
        <trans-unit id="7ed72a91acd5901b2715fbe7249ca1b4b882cab1" translate="yes" xml:space="preserve">
          <source>A simple one-dimensional regression example computed in two different ways:</source>
          <target state="translated">一个简单的一维回归例子,用两种不同的方式计算。</target>
        </trans-unit>
        <trans-unit id="d0a2166f9acbabe38964ccee31d9e7127ebb3bd0" translate="yes" xml:space="preserve">
          <source>A simple toy dataset to visualize clustering and classification algorithms.</source>
          <target state="translated">一个简单的玩具数据集来可视化聚类和分类算法。</target>
        </trans-unit>
        <trans-unit id="47127c0ac17be6b42bd52820b6c53e975ec33072" translate="yes" xml:space="preserve">
          <source>A simple toy dataset to visualize clustering and classification algorithms. Read more in the &lt;a href=&quot;../../datasets/index#sample-generators&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">一个简单的玩具数据集，用于可视化聚类和分类算法。在《&lt;a href=&quot;../../datasets/index#sample-generators&quot;&gt;用户指南》中&lt;/a&gt;阅读更多内容。</target>
        </trans-unit>
        <trans-unit id="0963c917bb15b907590115ef7d8ee882d54970b8" translate="yes" xml:space="preserve">
          <source>A single str (see &lt;a href=&quot;../model_evaluation#scoring-parameter&quot;&gt;The scoring parameter: defining model evaluation rules&lt;/a&gt;) or a callable (see &lt;a href=&quot;../model_evaluation#scoring&quot;&gt;Defining your scoring strategy from metric functions&lt;/a&gt;) to evaluate the predictions on the test set.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6cc426ce246dcb8426ecbbb6a60a4ff7a07e84ce" translate="yes" xml:space="preserve">
          <source>A single string (see &lt;a href=&quot;../model_evaluation#scoring-parameter&quot;&gt;The scoring parameter: defining model evaluation rules&lt;/a&gt;) or a callable (see &lt;a href=&quot;../model_evaluation#scoring&quot;&gt;Defining your scoring strategy from metric functions&lt;/a&gt;) to evaluate the predictions on the test set.</source>
          <target state="translated">单个字符串（请参阅&lt;a href=&quot;../model_evaluation#scoring-parameter&quot;&gt;评分参数：定义模型评估规则&lt;/a&gt;）或可调用项（请参阅&lt;a href=&quot;../model_evaluation#scoring&quot;&gt;从度量函数定义评分策略&lt;/a&gt;）以评估测试集上的预测。</target>
        </trans-unit>
        <trans-unit id="2e53707c3f177aad6668f2e995ecf10221437949" translate="yes" xml:space="preserve">
          <source>A small value of &lt;code&gt;C&lt;/code&gt; includes more/all the observations, allowing the margins to be calculated using all the data in the area.</source>
          <target state="translated">较小的 &lt;code&gt;C&lt;/code&gt; 值包含更多/所有观测值，从而可以使用该区域中的所有数据来计算边距。</target>
        </trans-unit>
        <trans-unit id="e33601e97cbb479e0128e302bee052e1e2030a89" translate="yes" xml:space="preserve">
          <source>A solution in high-dimensional statistical learning is to &lt;em&gt;shrink&lt;/em&gt; the regression coefficients to zero: any two randomly chosen set of observations are likely to be uncorrelated. This is called &lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt; regression:</source>
          <target state="translated">高维统计学习的一种解决方案是将回归系数&lt;em&gt;缩小&lt;/em&gt;为零：任意两个随机选择的观察值可能不相关。这称为&lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt;回归：</target>
        </trans-unit>
        <trans-unit id="dcd1f54984c6dffbbcff2667b694f4185f6ec040" translate="yes" xml:space="preserve">
          <source>A solution to this problem is a procedure called &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-validation_(statistics)&quot;&gt;cross-validation&lt;/a&gt; (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called &lt;em&gt;k&lt;/em&gt;-fold CV, the training set is split into &lt;em&gt;k&lt;/em&gt; smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the &lt;em&gt;k&lt;/em&gt; &amp;ldquo;folds&amp;rdquo;:</source>
          <target state="translated">解决此问题的方法是称为&lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-validation_(statistics)&quot;&gt;交叉验证&lt;/a&gt;（CV）的过程。仍然应保留测试集以进行最终评估，但是进行CV时不再需要验证集。在称为&lt;em&gt;k&lt;/em&gt;倍CV 的基本方法中，将训练集分为&lt;em&gt;k个&lt;/em&gt;较小的集（以下介绍了其他方法，但通常遵循相同的原理）。&lt;em&gt;k个&lt;/em&gt; &amp;ldquo;折叠&amp;rdquo;中的每一个都遵循以下过程：</target>
        </trans-unit>
        <trans-unit id="538e1506057d7a3220a186af08ab5489676c59df" translate="yes" xml:space="preserve">
          <source>A sparse radius neighborhood graph (where missing entries are presumed to be out of eps) can be precomputed in a memory-efficient way and dbscan can be run over this with &lt;code&gt;metric='precomputed'&lt;/code&gt;. See &lt;a href=&quot;generated/sklearn.neighbors.nearestneighbors#sklearn.neighbors.NearestNeighbors.radius_neighbors_graph&quot;&gt;&lt;code&gt;sklearn.neighbors.NearestNeighbors.radius_neighbors_graph&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以以内存高效的方式预先计算稀疏半径邻域图（假定缺少的条目不在eps之内），并且dbscan可以使用 &lt;code&gt;metric='precomputed'&lt;/code&gt; 在其上运行。请参阅&lt;a href=&quot;generated/sklearn.neighbors.nearestneighbors#sklearn.neighbors.NearestNeighbors.radius_neighbors_graph&quot;&gt; &lt;code&gt;sklearn.neighbors.NearestNeighbors.radius_neighbors_graph&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="5434739c8a85282e43e6d41eec6550c57895597f" translate="yes" xml:space="preserve">
          <source>A star marks the expected sample for each class; its size reflects the probability of selecting that class label.</source>
          <target state="translated">星星标志着每个类的预期样本;它的大小反映了选择该类标签的概率。</target>
        </trans-unit>
        <trans-unit id="a226eb090c390674deacec5886c6aefd3ca76b60" translate="yes" xml:space="preserve">
          <source>A str (see model evaluation documentation) or a scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; which should return only a single value.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7313aa5f13cd15f69aa6d50563adc57324fb0eb6" translate="yes" xml:space="preserve">
          <source>A str (see model evaluation documentation) or a scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d85d389496ba61c19ce84f015d0fceafa4c84844" translate="yes" xml:space="preserve">
          <source>A str, giving an expression as a function of n_jobs, as in &amp;lsquo;2*n_jobs&amp;rsquo;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="31ecf6c64ee79cd0b8c2879dbc516ad7450905ee" translate="yes" xml:space="preserve">
          <source>A strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9cc68f40152c74e41efd28d7122ce96e71e57739" translate="yes" xml:space="preserve">
          <source>A strategy to implement out-of-core scaling is to stream data to the estimator in mini-batches. Each mini-batch is vectorized using &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt; so as to guarantee that the input space of the estimator has always the same dimensionality. The amount of memory used at any time is thus bounded by the size of a mini-batch. Although there is no limit to the amount of data that can be ingested using such an approach, from a practical point of view the learning time is often limited by the CPU time one wants to spend on the task.</source>
          <target state="translated">实施核外扩展的策略是以小批量方式将数据流传输到估计器。使用&lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; &lt;/a&gt;对每个微型批处理进行矢量化处理，以确保估计器的输入空间始终具有相同的维数。因此，任何时候使用的内存量都受微型批处理大小的限制。尽管使用这种方法可以摄取的数据量没有限制，但是从实际的角度来看，学习时间通常受到要花费在任务上的CPU时间的限制。</target>
        </trans-unit>
        <trans-unit id="7c0e35a146ea0efee84f42e6ee454306c5da4827" translate="yes" xml:space="preserve">
          <source>A string (see model evaluation documentation) or a scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;.</source>
          <target state="translated">一个字符串（请参阅模型评估文档）或一个带有签名 &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; 可调用对象/函数。</target>
        </trans-unit>
        <trans-unit id="fad6fe3853f8abecb73cc5e995f9131ee653eef0" translate="yes" xml:space="preserve">
          <source>A string (see model evaluation documentation) or a scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;. For a list of scoring functions that can be used, look at &lt;a href=&quot;../classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt;. The default scoring option used is &amp;lsquo;accuracy&amp;rsquo;.</source>
          <target state="translated">一个字符串（请参阅模型评估文档）或一个带有签名 &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt; 可调用对象/函数。有关可以使用的评分功能的列表，请查看&lt;a href=&quot;../classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; &lt;/a&gt;。使用的默认评分选项是&amp;ldquo;准确性&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="1963bcb955646804f10ec7ad2d71cbad666ea477" translate="yes" xml:space="preserve">
          <source>A string (see model evaluation documentation) or a scorer callable object / function with signature &lt;code&gt;scorer(estimator, X, y)&lt;/code&gt;. If None, the negative mean squared error if cv is &amp;lsquo;auto&amp;rsquo; or None (i.e. when using generalized cross-validation), and r2 score otherwise.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e94d51ba3f9b9473065d98a0a11ca48ef9ac3a04" translate="yes" xml:space="preserve">
          <source>A string of unicode symbols.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="93b243cc3849b7302c5dad12c7987dfe501650ad" translate="yes" xml:space="preserve">
          <source>A string, giving an expression as a function of n_jobs, as in &amp;lsquo;2*n_jobs&amp;rsquo;</source>
          <target state="translated">字符串，根据n_jobs给出表达式，如'2 * n_jobs'</target>
        </trans-unit>
        <trans-unit id="82f0cd7ca226a86963abc9437ca213dfd9c32ccc" translate="yes" xml:space="preserve">
          <source>A sub-pipeline can also be extracted using the slicing notation commonly used for Python Sequences such as lists or strings (although only a step of 1 is permitted). This is convenient for performing only some of the transformations (or their inverse):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="84dd7b04896fc19b0373bd22f46ebced4bcbae51" translate="yes" xml:space="preserve">
          <source>A supervised learning estimator with a &lt;code&gt;fit&lt;/code&gt; method that provides information about feature importance either through a &lt;code&gt;coef_&lt;/code&gt; attribute or through a &lt;code&gt;feature_importances_&lt;/code&gt; attribute.</source>
          <target state="translated">具有 &lt;code&gt;fit&lt;/code&gt; 方法的监督学习估计器，该方法通过 &lt;code&gt;coef_&lt;/code&gt; 属性或 &lt;code&gt;feature_importances_&lt;/code&gt; 属性提供有关要素重要性的信息。</target>
        </trans-unit>
        <trans-unit id="be778589a42569621d412c8d143e2c4ecab0280f" translate="yes" xml:space="preserve">
          <source>A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.</source>
          <target state="translated">支持向量机在高维或无限维空间中构造一个超平面或超平面集,它可以用于分类、回归或其他任务。直观地说,良好的分离度是由与任何类最近的训练数据点距离最大的超平面(所谓的函数余量)实现的,因为一般来说,余量越大,分类器的泛化误差越小。</target>
        </trans-unit>
        <trans-unit id="b0f8a884660f9ed7117b59341660ac6dff079372" translate="yes" xml:space="preserve">
          <source>A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure below shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called &amp;ldquo;support vectors&amp;rdquo;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a7e8b3dcde624d3d8f97ed6de9c3edf28cc3212" translate="yes" xml:space="preserve">
          <source>A synthetic random regression problem is generated. The targets &lt;code&gt;y&lt;/code&gt; are modified by: (i) translating all targets such that all entries are non-negative and (ii) applying an exponential function to obtain non-linear targets which cannot be fitted using a simple linear model.</source>
          <target state="translated">生成了一个综合随机回归问题。通过以下方式修改目标 &lt;code&gt;y&lt;/code&gt; ：（i）转换所有目标，使所有条目均为非负数；（ii）应用指数函数以获得无法使用简单线性模型拟合的非线性目标。</target>
        </trans-unit>
        <trans-unit id="0903abbf1faeca209d35b8701b7569a1792a87c6" translate="yes" xml:space="preserve">
          <source>A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly.</source>
          <target state="translated">一个高召回率但低精度的系统会返回很多结果,但与训练标签相比,它的大部分预测标签都是错误的。高精度但低召回的系统正好相反,返回的结果很少,但与训练标签相比,大部分预测标签都是正确的。一个高精度和高召回率的理想系统将返回许多结果,所有结果的标签都是正确的。</target>
        </trans-unit>
        <trans-unit id="173f587454933c3828450ea8ca8b53629bab1434" translate="yes" xml:space="preserve">
          <source>A thin wrapper around the functionality of the kernels in sklearn.metrics.pairwise.</source>
          <target state="translated">sklearn.metrics.pairwise中内核功能的一个薄包装。</target>
        </trans-unit>
        <trans-unit id="66676f8254f2529112cd062a4d4d85367f781237" translate="yes" xml:space="preserve">
          <source>A trivial solution to this problem is to set all the points on the origin. In order to avoid that, the disparities \(\hat{d}_{ij}\) are normalized.</source>
          <target state="translated">这个问题的一个微不足道的解决办法是将所有的点都设置在原点上。为了避免这种情况的发生,将差异性(hat{d}_{ij})进行归一化处理。</target>
        </trans-unit>
        <trans-unit id="92110b9d440ed0f7743804ea18583b68d039b315" translate="yes" xml:space="preserve">
          <source>A tutorial exercise for using different SVM kernels.</source>
          <target state="translated">使用不同SVM内核的教程练习。</target>
        </trans-unit>
        <trans-unit id="b027711ce35320d3d1f8472a3216da00739d6438" translate="yes" xml:space="preserve">
          <source>A tutorial exercise regarding the use of classification techniques on the Digits dataset.</source>
          <target state="translated">关于在Digits数据集上使用分类技术的教程练习。</target>
        </trans-unit>
        <trans-unit id="f2776a357a1de5e5a81f9ce6aa4ab14f87148c15" translate="yes" xml:space="preserve">
          <source>A tutorial exercise using Cross-validation with an SVM on the Digits dataset.</source>
          <target state="translated">在Digits数据集上使用SVM进行交叉验证的教程练习。</target>
        </trans-unit>
        <trans-unit id="b2628903b486483a05c00dab06ec0310f6a300cc" translate="yes" xml:space="preserve">
          <source>A tutorial exercise which uses cross-validation with linear models.</source>
          <target state="translated">一个使用线性模型交叉验证的教程练习。</target>
        </trans-unit>
        <trans-unit id="cd8b8f382c4d69a8c061676d971b4b4296bfd8b7" translate="yes" xml:space="preserve">
          <source>A tutorial on statistical-learning for scientific data processing</source>
          <target state="translated">科学数据处理的统计学习教程</target>
        </trans-unit>
        <trans-unit id="59f7f31eba85f0c9b2945699e7dc03221dcc0732" translate="yes" xml:space="preserve">
          <source>A two-dimensional classification example showing iso-probability lines for the predicted probabilities.</source>
          <target state="translated">二维分类实例,显示预测概率的等概率线。</target>
        </trans-unit>
        <trans-unit id="027a04b42d0e6eeb4156be0ce184aaf749b2c919" translate="yes" xml:space="preserve">
          <source>A typical &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/blob/master/benchmarks/bench_sparsify.py&quot;&gt;benchmark&lt;/a&gt; on synthetic data yields a &amp;gt;30% decrease in latency when both the model and input are sparse (with 0.000024 and 0.027400 non-zero coefficients ratio respectively). Your mileage may vary depending on the sparsity and size of your data and model. Furthermore, sparsifying can be very useful to reduce the memory usage of predictive models deployed on production servers.</source>
          <target state="translated">当模型和输入都稀疏时（分别具有0.000024和0.027400非零​​系数比率），合成数据的典型&lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/blob/master/benchmarks/bench_sparsify.py&quot;&gt;基准会&lt;/a&gt;导致延迟减少&amp;gt; 30％。您的里程可能因数据和模型的稀疏性和大小而异。此外，稀疏化对于减少部署在生产服务器上的预测模型的内存使用非常有用。</target>
        </trans-unit>
        <trans-unit id="ec7bb89c7f50ac1476a296b31de1d976e3adfdc7" translate="yes" xml:space="preserve">
          <source>A valid representation of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multilabel&quot;&gt;multilabel&lt;/a&gt;&lt;code&gt;y&lt;/code&gt; is an either dense or sparse &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-binary&quot;&gt;binary&lt;/a&gt; matrix of shape &lt;code&gt;(n_samples, n_classes)&lt;/code&gt;. Each column represents a class. The &lt;code&gt;1&lt;/code&gt;&amp;rsquo;s in each row denote the positive classes a sample has been labelled with. An example of a dense matrix &lt;code&gt;y&lt;/code&gt; for 3 samples:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd911da25d74dfd6322d9ef6e738de82cbe5f6c0" translate="yes" xml:space="preserve">
          <source>A valid representation of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multioutput&quot;&gt;multioutput&lt;/a&gt;&lt;code&gt;y&lt;/code&gt; is a dense matrix of shape &lt;code&gt;(n_samples, n_classes)&lt;/code&gt; of class labels. A column wise concatenation of 1d &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multiclass&quot;&gt;multiclass&lt;/a&gt; variables. An example of &lt;code&gt;y&lt;/code&gt; for 3 samples:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b5488b6a26a1d957fe479981670f261e01fd257" translate="yes" xml:space="preserve">
          <source>A valid representation of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-multioutput&quot;&gt;multioutput&lt;/a&gt;&lt;code&gt;y&lt;/code&gt; is a dense matrix of shape &lt;code&gt;(n_samples, n_classes)&lt;/code&gt; of floats. A column wise concatenation of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-continuous&quot;&gt;continuous&lt;/a&gt; variables. An example of &lt;code&gt;y&lt;/code&gt; for 3 samples:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f7b09774b632b4a2428bc3dfc2607f68bc8654b3" translate="yes" xml:space="preserve">
          <source>A value ranges from 0 to 1. Radius neighbors will be searched until the ratio between total neighbors within the radius and the total candidates becomes less than this value unless it is terminated by hash length reaching &lt;code&gt;min_hash_match&lt;/code&gt;.</source>
          <target state="translated">值的范围是0到1。除非半径值达到 &lt;code&gt;min_hash_match&lt;/code&gt; 终止，否则将搜索半径邻居，直到半径内的总邻居数与候选总数之比小于该值。</target>
        </trans-unit>
        <trans-unit id="2582a27b00b61e2b9522f341f9a346d9f6709d01" translate="yes" xml:space="preserve">
          <source>A vector of size n_samples with the values of Xred assigned to each of the cluster of samples.</source>
          <target state="translated">一个大小为n_samples的向量,其中包含分配给每个样本群的Xred值。</target>
        </trans-unit>
        <trans-unit id="d6682290692f64a366a7947dddf0230094bb4014" translate="yes" xml:space="preserve">
          <source>A very short introduction into machine learning problems and how to solve them using scikit-learn. Introduced basic concepts and conventions.</source>
          <target state="translated">非常简短的介绍了机器学习问题,以及如何使用scikit-learn解决这些问题。介绍了基本概念和约定。</target>
        </trans-unit>
        <trans-unit id="c7690c5e442eb70039d5b5e2448652422a789b71" translate="yes" xml:space="preserve">
          <source>A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f36937969f4f8fe7fe83a3d1b4c3ed127cd62323" translate="yes" xml:space="preserve">
          <source>A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction. We will use three different regressors to predict the data: &lt;a href=&quot;../../modules/generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt;&lt;code&gt;RandomForestRegressor&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;../../modules/generated/sklearn.linear_model.linearregression#sklearn.linear_model.LinearRegression&quot;&gt;&lt;code&gt;LinearRegression&lt;/code&gt;&lt;/a&gt;). Then the above 3 regressors will be used for the &lt;a href=&quot;../../modules/generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt;&lt;code&gt;VotingRegressor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9e5f7b45681bf1702c76ccf69a63504aee5a5896" translate="yes" xml:space="preserve">
          <source>A {n_samples by n_samples} size matrix will be created from this</source>
          <target state="translated">一个{n_samples by n_samples}的大小矩阵将被创建。</target>
        </trans-unit>
        <trans-unit id="fcc549a5a2b9c6ffeadeffd9e820de04a6f70e50" translate="yes" xml:space="preserve">
          <source>A. Kraskov, H. Stogbauer and P. Grassberger, &amp;ldquo;Estimating mutual information&amp;rdquo;. Phys. Rev. E 69, 2004.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2f18974816a09bbd22c4a7050c6bc6030b7596e3" translate="yes" xml:space="preserve">
          <source>A. McCallum and K. Nigam (1998). &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.1529&quot;&gt;A comparison of event models for Naive Bayes text classification.&lt;/a&gt; Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.</source>
          <target state="translated">A. McCallum和K. Nigam（1998）。&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.1529&quot;&gt;Naive Bayes文本分类的事件模型的比较。&lt;/a&gt;进程 AAAI / ICML-98文本分类学习研讨会，第41-48页。</target>
        </trans-unit>
        <trans-unit id="09ff19e0224a8e064521a1f035498a8575f6b1a4" translate="yes" xml:space="preserve">
          <source>A. McCallum and K. Nigam (1998). A comparison of event models for naive Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.</source>
          <target state="translated">A.McCallum和K.Nigam(1998年)。A.McCallum and K.Nigam (1998).A comparison of event models for naive Bayes text classification.A.McCallum and K.Nigam (1998).AAAI/ICML-98 Workshop on Learning for Text Categorization,pp.41-48.</target>
        </trans-unit>
        <trans-unit id="664157aad39c267bf285cc58317f2c271aa8f944" translate="yes" xml:space="preserve">
          <source>A. Noll, R. Salzmann and M.V. Wuthrich, Case Study: French Motor Third-Party Liability Claims (November 8, 2018). &lt;a href=&quot;http://dx.doi.org/10.2139/ssrn.3164764&quot;&gt;doi:10.2139/ssrn.3164764&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4aaae5b2c1983dccf44b8ac2ef0c2ae6d4b3ffdb" translate="yes" xml:space="preserve">
          <source>AGE</source>
          <target state="translated">AGE</target>
        </trans-unit>
        <trans-unit id="4aecbad800270fae47218d640b45ace7ade395c4" translate="yes" xml:space="preserve">
          <source>AGE proportion of owner-occupied units built prior to 1940</source>
          <target state="translated">1940年以前建成的自住单位的年龄比例。</target>
        </trans-unit>
        <trans-unit id="80cd3c2daea500fa1de49e5116e93d233f023eef" translate="yes" xml:space="preserve">
          <source>AIC is the Akaike information criterion and BIC is the Bayes Information criterion. Such criteria are useful to select the value of the regularization parameter by making a trade-off between the goodness of fit and the complexity of the model. A good model should explain well the data while being simple.</source>
          <target state="translated">AIC是Akaike信息准则,BIC是Bayes信息准则。这种准则对于选择正则化参数的值很有用,可以在拟合度的好坏和模型的复杂程度之间进行权衡。一个好的模型应该在简单的同时很好地解释数据。</target>
        </trans-unit>
        <trans-unit id="fedf36066038cc2bbe4c1f8d6f37bca1d4a271f3" translate="yes" xml:space="preserve">
          <source>AMI</source>
          <target state="translated">AMI</target>
        </trans-unit>
        <trans-unit id="977d221308080b656b11030badda34761fa1379d" translate="yes" xml:space="preserve">
          <source>ANOVA F-value between label/feature for classification tasks.</source>
          <target state="translated">分类任务的标签/特征之间的方差分析F值。</target>
        </trans-unit>
        <trans-unit id="500622cc8c5af556f9bf30d45e5a5dc4d38771d9" translate="yes" xml:space="preserve">
          <source>AP and the trapezoidal area under the operating points (&lt;a href=&quot;../../modules/generated/sklearn.metrics.auc#sklearn.metrics.auc&quot;&gt;&lt;code&gt;sklearn.metrics.auc&lt;/code&gt;&lt;/a&gt;) are common ways to summarize a precision-recall curve that lead to different results. Read more in the &lt;a href=&quot;../../modules/model_evaluation#precision-recall-f-measure-metrics&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">AP和工作点下的梯形区域（&lt;a href=&quot;../../modules/generated/sklearn.metrics.auc#sklearn.metrics.auc&quot;&gt; &lt;code&gt;sklearn.metrics.auc&lt;/code&gt; &lt;/a&gt;）是汇总精确召回曲线的通用方法，可得出不同的结果。在《&lt;a href=&quot;../../modules/model_evaluation#precision-recall-f-measure-metrics&quot;&gt;用户指南》中&lt;/a&gt;阅读更多内容。</target>
        </trans-unit>
        <trans-unit id="7ce6548ae70272727979635a979ae4f9ea8a3a93" translate="yes" xml:space="preserve">
          <source>AP summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight:</source>
          <target state="translated">AP将精密度-召回曲线总结为在每个阈值下所达到的精密度的加权平均值,并以前一个阈值的召回率的增加作为权重。</target>
        </trans-unit>
        <trans-unit id="d93d10ff0fbef1b4aa0ddc24e10e907746d3c85a" translate="yes" xml:space="preserve">
          <source>API</source>
          <target state="translated">API</target>
        </trans-unit>
        <trans-unit id="b276f94cd8d0e74a21de6e5939b8c10ca9a975d6" translate="yes" xml:space="preserve">
          <source>API Reference</source>
          <target state="translated">API参考</target>
        </trans-unit>
        <trans-unit id="044c42df47a473e04593a603631e399678960367" translate="yes" xml:space="preserve">
          <source>ARD is also known in the literature as &lt;em&gt;Sparse Bayesian Learning&lt;/em&gt; and &lt;em&gt;Relevance Vector Machine&lt;/em&gt;&lt;a href=&quot;#id16&quot; id=&quot;id12&quot;&gt;3&lt;/a&gt;&lt;a href=&quot;#id18&quot; id=&quot;id13&quot;&gt;4&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc664a8aed1b7702083559e78f4a1b25e63a7739" translate="yes" xml:space="preserve">
          <source>ARD is also known in the literature as &lt;em&gt;Sparse Bayesian Learning&lt;/em&gt; and &lt;em&gt;Relevance Vector Machine&lt;/em&gt;&lt;a href=&quot;#id20&quot; id=&quot;id16&quot;&gt;[3]&lt;/a&gt;&lt;a href=&quot;#id21&quot; id=&quot;id17&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">ARD在文献中也称为&lt;em&gt;稀疏贝叶斯学习&lt;/em&gt;和&lt;em&gt;相关矢量机&lt;/em&gt;&lt;a href=&quot;#id20&quot; id=&quot;id16&quot;&gt;[3] &lt;/a&gt;&lt;a href=&quot;#id21&quot; id=&quot;id17&quot;&gt;[4]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="a1df128dfacd3f460cbb61bb4087bb92287d3fcb" translate="yes" xml:space="preserve">
          <source>ARI</source>
          <target state="translated">ARI</target>
        </trans-unit>
        <trans-unit id="8c442cbb4124477c731be288798913c884c906af" translate="yes" xml:space="preserve">
          <source>ARI is a symmetric measure:</source>
          <target state="translated">ARI是一种对称测量。</target>
        </trans-unit>
        <trans-unit id="b6cdde34aa4cbbe354d4a8fe12fbb26711ad6710" translate="yes" xml:space="preserve">
          <source>ARI is symmetric, so labelings that have pure clusters with members coming from the same classes but unnecessary splits are penalized:</source>
          <target state="translated">ARI是对称的,所以有纯簇的标签,其成员来自同一个类,但不必要的分裂会受到惩罚。</target>
        </trans-unit>
        <trans-unit id="5045d88e766edf44d9736d9751b0aa0b647864ac" translate="yes" xml:space="preserve">
          <source>A[i, j] is assigned the weight of edge that connects i to j.</source>
          <target state="translated">A[i,j]被赋予连接i和j的边缘的权重。</target>
        </trans-unit>
        <trans-unit id="9f4a63ae0bf1594deea5ea12c509b27f409ce3db" translate="yes" xml:space="preserve">
          <source>Aaron Defazio, Francis Bach, Simon Lacoste-Julien: &lt;a href=&quot;https://arxiv.org/abs/1407.0202&quot;&gt;SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives.&lt;/a&gt;</source>
          <target state="translated">Aaron Defazio，Francis Bach，Simon Lacoste-Julien：&lt;a href=&quot;https://arxiv.org/abs/1407.0202&quot;&gt;SAGA：一种支持非强凸复合物镜的快速增量梯度方法。&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e82c46cc189803ea687c96da2038982e9dcb6a4b" translate="yes" xml:space="preserve">
          <source>Ability to use shared memory efficiently with worker processes for large numpy-based datastructures.</source>
          <target state="translated">在基于numpy的大型数据结构中,能够用工作进程有效地使用共享内存。</target>
        </trans-unit>
        <trans-unit id="54d1141ddccb7851744d76084cc8611f93e2cfc3" translate="yes" xml:space="preserve">
          <source>Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. See &lt;a href=&quot;#tree-algorithms&quot;&gt;algorithms&lt;/a&gt; for more information.</source>
          <target state="translated">能够处理数字和分类数据。其他技术通常专用于分析仅具有一种类型的变量的数据集。有关更多信息，请参见&lt;a href=&quot;#tree-algorithms&quot;&gt;算法&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="2367c2ca7022225c20205571d9433b5b6b84cd68" translate="yes" xml:space="preserve">
          <source>Able to handle multi-output problems.</source>
          <target state="translated">能够处理多输出问题。</target>
        </trans-unit>
        <trans-unit id="da22d4ef976c68e8fef4ef4e1a2681784cdddf3a" translate="yes" xml:space="preserve">
          <source>Above, we limited this regularization to a very little amount. Regularization improves the conditioning of the problem and reduces the variance of the estimates. RidgeCV applies cross validation in order to determine which value of the regularization parameter (&lt;code&gt;alpha&lt;/code&gt;) is best suited for prediction.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1c09e3c38ba7261734dc4ab72dcf1ef3efd8a152" translate="yes" xml:space="preserve">
          <source>Absolute threshold for a singular value of X to be considered significant, used to estimate the rank of X. Dimensions whose singular values are non-significant are discarded. Only used if solver is &amp;lsquo;svd&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2aa749baee7b6d0eb7f8d7395643fd7e673c649b" translate="yes" xml:space="preserve">
          <source>Absolute threshold for a singular value to be considered significant, used to estimate the rank of &lt;code&gt;Xk&lt;/code&gt; where &lt;code&gt;Xk&lt;/code&gt; is the centered matrix of samples in class k. This parameter does not affect the predictions. It only controls a warning that is raised when features are considered to be colinear.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f587c3583da9a191a72d5ff01fbfedde272f2898" translate="yes" xml:space="preserve">
          <source>Absolute tolerance for equivalence of arrays. Default = 1E-10.</source>
          <target state="translated">数组等价的绝对容差。默认值=1E-10。</target>
        </trans-unit>
        <trans-unit id="d79faf207bac3f682107ababce293b4eceadf34c" translate="yes" xml:space="preserve">
          <source>Acceptable data types for the parameter.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a62a0f9649f4eb6fd92f87e85b475e87182d6e46" translate="yes" xml:space="preserve">
          <source>Access the fitted transformer by name.</source>
          <target state="translated">按名称访问已安装的变压器。</target>
        </trans-unit>
        <trans-unit id="9e0e7377700ae6d6f91649710d89b9dd54aaa033" translate="yes" xml:space="preserve">
          <source>According to the JL lemma, projecting 500 samples without too much distortion will require at least several thousands dimensions, irrespective of the number of features of the original dataset.</source>
          <target state="translated">根据JL lemma,不管原始数据集的特征数是多少,在没有太大失真的情况下,投影500个样本至少需要几千个维度。</target>
        </trans-unit>
        <trans-unit id="caff4ed631e2a64d90ad9e75e695e4cb077ff929" translate="yes" xml:space="preserve">
          <source>According to the model above, the log of the posterior is:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e10c7e434a3850cc1ce980a85ce2c40965ebbd1" translate="yes" xml:space="preserve">
          <source>According to the observed data, the frequency of accidents is higher for drivers younger than 30 years old, and is positively correlated with the &lt;code&gt;BonusMalus&lt;/code&gt; variable. Our model is able to mostly correctly model this behaviour.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7b168ce1b26b0ed19f0857c9a9e0275bc2a86de2" translate="yes" xml:space="preserve">
          <source>Accuracy classification score.</source>
          <target state="translated">准确率分类得分。</target>
        </trans-unit>
        <trans-unit id="ed1b27307b9c829fdd35ed1ccf294d2e82a6dfcb" translate="yes" xml:space="preserve">
          <source>Accuracy of the Model</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b171ff92fbbc89c347ecf11b61ad701495ae9495" translate="yes" xml:space="preserve">
          <source>Accuracy vs alpha for training and testing sets</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3159fc0c287c6fa355557d0a32e1e4fbdd60d32b" translate="yes" xml:space="preserve">
          <source>Across the module, we designate the vector \(w = (w_1, ..., w_p)\) as &lt;code&gt;coef_&lt;/code&gt; and \(w_0\) as &lt;code&gt;intercept_&lt;/code&gt;.</source>
          <target state="translated">在整个模块中，我们将向量\（w =（w_1，...，w_p）\）指定为 &lt;code&gt;coef_&lt;/code&gt; ,并将\（w_0 \）指定为 &lt;code&gt;intercept_&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1e74068a27362b129f4808ec6412c747359a90d1" translate="yes" xml:space="preserve">
          <source>Activation function for the hidden layer.</source>
          <target state="translated">隐层的激活函数。</target>
        </trans-unit>
        <trans-unit id="13466b0d67f826e470f7f662a0fa9b98771bd57d" translate="yes" xml:space="preserve">
          <source>Actual class (observation)</source>
          <target state="translated">实际班级(观察)</target>
        </trans-unit>
        <trans-unit id="4bda103291a01841e5d33c04e072de1679753a3f" translate="yes" xml:space="preserve">
          <source>Actual number of iteration for each Cs.</source>
          <target state="translated">每个Cs的实际迭代次数。</target>
        </trans-unit>
        <trans-unit id="b9e108d70e5d6e1ab362876cb4abfe8b3ea0d61c" translate="yes" xml:space="preserve">
          <source>Actual number of iterations for all classes, folds and Cs. In the binary or multinomial cases, the first dimension is equal to 1.</source>
          <target state="translated">所有类、折线和Cs的实际迭代次数。在二元或多元的情况下,第一维等于1。</target>
        </trans-unit>
        <trans-unit id="e15b0921ef0e026565cb96038a661f88e504a7d1" translate="yes" xml:space="preserve">
          <source>Actual number of iterations for all classes, folds and Cs. In the binary or multinomial cases, the first dimension is equal to 1. If &lt;code&gt;penalty='elasticnet'&lt;/code&gt;, the shape is &lt;code&gt;(n_classes, n_folds,
n_cs, n_l1_ratios)&lt;/code&gt; or &lt;code&gt;(1, n_folds, n_cs, n_l1_ratios)&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e7c00438aea9f14670d9e5ce3ad69eadb9e56fa7" translate="yes" xml:space="preserve">
          <source>Actual number of iterations for all classes. If binary or multinomial, it returns only 1 element. For liblinear solver, only the maximum number of iteration across all classes is given.</source>
          <target state="translated">所有类的实际迭代次数。如果是二进制或多项式,它只返回1个元素。对于liblinear求解器,只给出所有类的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="41eb0ffb0b86a2b7fa1c30caad1eab7979f038bb" translate="yes" xml:space="preserve">
          <source>Actual number of iterations for each target. Available only for sag and lsqr solvers. Other solvers will return None.</source>
          <target state="translated">每个目标的实际迭代次数。只适用于sag和lsqr求解器。其他解算器将返回 &quot;无&quot;。</target>
        </trans-unit>
        <trans-unit id="5f183a64b01f4bbfef50fc734f02625115726f0b" translate="yes" xml:space="preserve">
          <source>Actual number of iterations used in the solver.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a5f57bad41ea8a1cc3f7ad65dabd66fae549152b" translate="yes" xml:space="preserve">
          <source>Actual number of iterations.</source>
          <target state="translated">实际迭代次数。</target>
        </trans-unit>
        <trans-unit id="e88dbeae46a975e549a3e1b429ac335f1717a327" translate="yes" xml:space="preserve">
          <source>AdaBoost can be used both for classification and regression problems:</source>
          <target state="translated">AdaBoost既可以用于分类问题,也可以用于回归问题。</target>
        </trans-unit>
        <trans-unit id="1c3d2a8d1e7ee52688369f7b268da1f090f613a4" translate="yes" xml:space="preserve">
          <source>Adam is similar to SGD in a sense that it is a stochastic optimizer, but it can automatically adjust the amount to update parameters based on adaptive estimates of lower-order moments.</source>
          <target state="translated">Adam与SGD类似,它是一个随机优化器,但它可以根据低阶矩的自适应估计自动调整量来更新参数。</target>
        </trans-unit>
        <trans-unit id="37193f0def1de066166921b64d73e81a4207486f" translate="yes" xml:space="preserve">
          <source>Add plots</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="71042ebf81b88cec72926e5e2da3d76737a8fc09" translate="yes" xml:space="preserve">
          <source>Adding a constant kernel is equivalent to adding a constant:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="57edf9b3cf9cc602f38d55f2f3497cc8e4458efe" translate="yes" xml:space="preserve">
          <source>Adding parameters that do not influence the performance does not decrease efficiency.</source>
          <target state="translated">增加不影响性能的参数并不会降低效率。</target>
        </trans-unit>
        <trans-unit id="d8b82f8ac1fd93a8c5863d25cc6558cb656b7f40" translate="yes" xml:space="preserve">
          <source>Additional Resources</source>
          <target state="translated">其他资源</target>
        </trans-unit>
        <trans-unit id="28c6073bb46543d76ed56428ba51e1f98f703736" translate="yes" xml:space="preserve">
          <source>Additional fit parameters.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3bc09d37c7749b14dd88f92786b449d978558dbf" translate="yes" xml:space="preserve">
          <source>Additional keyword arguments for the metric function.</source>
          <target state="translated">计量函数的附加关键字参数。</target>
        </trans-unit>
        <trans-unit id="448e0ea40ef1f0816d1ba3c00c7fd9818ee1579c" translate="yes" xml:space="preserve">
          <source>Additional keyword arguments for the metric function. For most metrics will be same with &lt;code&gt;metric_params&lt;/code&gt; parameter, but may also contain the &lt;code&gt;p&lt;/code&gt; parameter value if the &lt;code&gt;effective_metric_&lt;/code&gt; attribute is set to &amp;lsquo;minkowski&amp;rsquo;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9a84da277628ee33f1ee92b82eb0c5a6af6d0524" translate="yes" xml:space="preserve">
          <source>Additional number of random vectors to sample the range of M so as to ensure proper conditioning. The total number of random vectors used to find the range of M is n_components + n_oversamples. Smaller number can improve speed but can negatively impact the quality of approximation of singular vectors and singular values.</source>
          <target state="translated">额外的随机向量数量,用于对M的范围进行采样,以确保适当的调节。用于寻找M的范围的随机向量的总数量为n_成分+n_过采样。较小的数量可以提高速度,但会对奇异向量和奇异值的逼近质量产生负面影响。</target>
        </trans-unit>
        <trans-unit id="7cc2d5826a730e9f6abc3aeeee28797fe00b8570" translate="yes" xml:space="preserve">
          <source>Additional parameter passed to the fit function of the estimator.</source>
          <target state="translated">传递给估计器拟合函数的附加参数。</target>
        </trans-unit>
        <trans-unit id="df9b29356cfac6bb58ec5dca9394f2d9dfc4703b" translate="yes" xml:space="preserve">
          <source>Additional parameters (keyword arguments) for kernel function passed as callable object.</source>
          <target state="translated">作为可调用对象传递的内核函数的附加参数(关键字参数)。</target>
        </trans-unit>
        <trans-unit id="39a26d6ec93b4bd557ff0d3e3333c06661f787d5" translate="yes" xml:space="preserve">
          <source>Additional parameters to be passed to score_func.</source>
          <target state="translated">要传递给 score_func 的其他参数。</target>
        </trans-unit>
        <trans-unit id="71247cd4c08f9e15e8cf580ef4b1b215b5be4541" translate="yes" xml:space="preserve">
          <source>Additional parameters to be passed to the tree for use with the metric. For more information, see the documentation of &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">要传递给树以用于度量标准的其他参数。有关更多信息，请参见&lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt;的文档。</target>
        </trans-unit>
        <trans-unit id="36da43716a16aade125d9759679637e39761ccc9" translate="yes" xml:space="preserve">
          <source>Additionally compute class covariance matrix (default False), used only in &amp;lsquo;svd&amp;rsquo; solver.</source>
          <target state="translated">另外，计算类协方差矩阵（默认为False），仅在&amp;ldquo; svd&amp;rdquo;求解器中使用。</target>
        </trans-unit>
        <trans-unit id="cb731300b5471fde45bf8790ae9ce49720693175" translate="yes" xml:space="preserve">
          <source>Additionally, &lt;code&gt;Pipeline&lt;/code&gt; can be instantiated with the &lt;code&gt;memory&lt;/code&gt; argument to memoize the transformers within the pipeline, avoiding to fit again the same transformers over and over.</source>
          <target state="translated">此外，可以使用 &lt;code&gt;memory&lt;/code&gt; 参数实例化 &lt;code&gt;Pipeline&lt;/code&gt; ，以记住管道内的转换器，而不必一次又一次地安装相同的转换器。</target>
        </trans-unit>
        <trans-unit id="4fcf36ef35c92f01311334b57dea32bb115ca779" translate="yes" xml:space="preserve">
          <source>Additionally, latent semantic analysis can also be used to reduce dimensionality and discover latent patterns in the data.</source>
          <target state="translated">此外,潜伏语义分析还可以用来降低维度,发现数据中的潜伏模式。</target>
        </trans-unit>
        <trans-unit id="c13231af636098d832d9fddca4838809fd525e6e" translate="yes" xml:space="preserve">
          <source>Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).</source>
          <target state="translated">Additive (Laplace/Lidstone)平滑参数(0代表没有平滑)。</target>
        </trans-unit>
        <trans-unit id="b8c1aaec1a2a62d157d744d0faee837709b2772c" translate="yes" xml:space="preserve">
          <source>Adjacency matrix of the graph</source>
          <target state="translated">图形的邻接矩阵</target>
        </trans-unit>
        <trans-unit id="250995e59d0465859871db4d7d37821f2e9e268c" translate="yes" xml:space="preserve">
          <source>Adjusted Mutual Information</source>
          <target state="translated">调整后的相互信息</target>
        </trans-unit>
        <trans-unit id="11e5941af4a261b664e9f0491f0ecd18e8412b46" translate="yes" xml:space="preserve">
          <source>Adjusted Mutual Information (AMI) is an adjustment of the Mutual Information (MI) score to account for chance. It accounts for the fact that the MI is generally higher for two clusterings with a larger number of clusters, regardless of whether there is actually more information shared. For two clusterings \(U\) and \(V\), the AMI is given as:</source>
          <target state="translated">调整后的相互信息(AMI)是对相互信息(MI)得分的调整,以考虑到偶然性。它考虑到了这样一个事实,即无论实际上是否有更多的信息共享,两个聚类的聚类数量较多时,MI通常较高。对于两个聚类(U/U)和(V/V),互信息指数的计算公式为:</target>
        </trans-unit>
        <trans-unit id="4a1b6ee1509942e0e22b1880790459ca848319f4" translate="yes" xml:space="preserve">
          <source>Adjusted Mutual Information (adjusted against chance)</source>
          <target state="translated">调整后的相互信息(按机会调整)</target>
        </trans-unit>
        <trans-unit id="4630f03a5a119c76517981dad27161c68958ec63" translate="yes" xml:space="preserve">
          <source>Adjusted Mutual Information between two clusterings.</source>
          <target state="translated">调整后的两个聚类之间的相互信息。</target>
        </trans-unit>
        <trans-unit id="1802b8bdeb7acaeda57735feed31e0ee765d7139" translate="yes" xml:space="preserve">
          <source>Adjusted Rand Index</source>
          <target state="translated">调整后的兰特指数</target>
        </trans-unit>
        <trans-unit id="31e214397fb70765d3e9e011e4c9138d3d446804" translate="yes" xml:space="preserve">
          <source>Adjusted against chance Mutual Information</source>
          <target state="translated">相互信息调整后</target>
        </trans-unit>
        <trans-unit id="a058b0be3c60201e595de35ba7261933ab6a3173" translate="yes" xml:space="preserve">
          <source>Adjusted for chance measure such as ARI display some random variations centered around a mean score of 0.0 for any number of samples and clusters.</source>
          <target state="translated">调整后的偶然性测量,如ARI显示一些随机变化,围绕着0.0的平均分数为任何数量的样本和集群。</target>
        </trans-unit>
        <trans-unit id="b80cf8c9df3b30a05f03bffeb2976e09c960b38a" translate="yes" xml:space="preserve">
          <source>Adjustment for chance in clustering performance evaluation</source>
          <target state="translated">调整聚类绩效评价中的偶然性。</target>
        </trans-unit>
        <trans-unit id="7a26e45b68e65e5d705e3f16c6dd31629c5f071a" translate="yes" xml:space="preserve">
          <source>Advanced Plotting With Partial Dependence</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c22815e21882a0aa7a6a9b8a93ee5e8e17ca7614" translate="yes" xml:space="preserve">
          <source>Affects shape of transform output only when voting=&amp;rsquo;soft&amp;rsquo; If voting=&amp;rsquo;soft&amp;rsquo; and flatten_transform=True, transform method returns matrix with shape (n_samples, n_classifiers * n_classes). If flatten_transform=False, it returns (n_classifiers, n_samples, n_classes).</source>
          <target state="translated">仅当voting ='soft'时才影响变换输出的形状。如果voting ='soft'并且flatten_transform = True，则transform方法将返回具有形状的矩阵（n_samples，n_classifiers * n_classes）。如果flatten_transform = False，则返回（n_classifiers，n_samples，n_classes）。</target>
        </trans-unit>
        <trans-unit id="6be196ba0996dde7d9ed4c8d27213a288c55819b" translate="yes" xml:space="preserve">
          <source>Affinity Propagation can be interesting as it chooses the number of clusters based on the data provided. For this purpose, the two important parameters are the &lt;em&gt;preference&lt;/em&gt;, which controls how many exemplars are used, and the &lt;em&gt;damping factor&lt;/em&gt; which damps the responsibility and availability messages to avoid numerical oscillations when updating these messages.</source>
          <target state="translated">亲和力传播可能会很有趣，因为它会根据提供的数据选择簇的数量。为此，两个重要参数是：&lt;em&gt;preference（首选项）&lt;/em&gt;，它控制使用多少个示例；以及&lt;em&gt;阻尼因数&lt;/em&gt;，它&lt;em&gt;衰减&lt;/em&gt;责任和可用性消息，以避免在更新这些消息时出现数值振荡。</target>
        </trans-unit>
        <trans-unit id="f0a35e8a6a6b08571acff68f4d3ca53b52b812c7" translate="yes" xml:space="preserve">
          <source>Affinity matrix used for clustering. Available only if after calling &lt;code&gt;fit&lt;/code&gt;.</source>
          <target state="translated">用于聚类的亲和矩阵。仅在调用 &lt;code&gt;fit&lt;/code&gt; 之后可用。</target>
        </trans-unit>
        <trans-unit id="63246ae129dc66d8916faa22ac2026722fe870df" translate="yes" xml:space="preserve">
          <source>Affinity propagation</source>
          <target state="translated">亲和力传播</target>
        </trans-unit>
        <trans-unit id="50cd6f7778a2976318486216f7be1102ea3c2dbd" translate="yes" xml:space="preserve">
          <source>Affinity_matrix constructed from samples or precomputed.</source>
          <target state="translated">亲和力_matrix从样品中构建或预先计算。</target>
        </trans-unit>
        <trans-unit id="9d6ec54ac3f9f07d1c79bc7828709b2a1ebe0b76" translate="yes" xml:space="preserve">
          <source>After being fitted, the model can then be used to predict new values:</source>
          <target state="translated">拟合后,模型就可以用来预测新的数值。</target>
        </trans-unit>
        <trans-unit id="56033184bfd4d9eb6d5cedda898ff603a56457b5" translate="yes" xml:space="preserve">
          <source>After being fitted, the model can then be used to predict the class of samples:</source>
          <target state="translated">拟合后,模型就可以用来预测样本的类别。</target>
        </trans-unit>
        <trans-unit id="d1a9437feb3595025967367029f3b25aca08ed48" translate="yes" xml:space="preserve">
          <source>After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.</source>
          <target state="translated">调用此方法后,用 partial_fit 方法(如果有的话)进行进一步的拟合,在调用 densify 之前将无法工作。</target>
        </trans-unit>
        <trans-unit id="1ac4b13a36333967e31b510f39d76728b1ade858" translate="yes" xml:space="preserve">
          <source>After discretization, linear regression and decision tree make exactly the same prediction. As features are constant within each bin, any model must predict the same value for all points within a bin. Compared with the result before discretization, linear model become much more flexible while decision tree gets much less flexible. Note that binning features generally has no beneficial effect for tree-based models, as these models can learn to split up the data anywhere.</source>
          <target state="translated">差异化后,线性回归和决策树做出的预测完全相同。由于每个bin内的特征是恒定的,任何模型都必须对bin内的所有点进行相同的预测。与离散化之前的结果相比,线性模型变得更加灵活,而决策树则变得不那么灵活。需要注意的是,分仓特征一般对基于树的模型没有好处,因为这些模型可以学习在任何地方对数据进行分割。</target>
        </trans-unit>
        <trans-unit id="eef177bc3be0e883eb6e86188495b80899ebaefb" translate="yes" xml:space="preserve">
          <source>After fitting (training), the model can predict labels for new samples:</source>
          <target state="translated">拟合(训练)后,模型可以预测新样本的标签。</target>
        </trans-unit>
        <trans-unit id="5e6425357ee2c6e34c7ccf4e621fb9b2e0d28394" translate="yes" xml:space="preserve">
          <source>After fitting a model, row and column cluster membership can be found in the &lt;code&gt;rows_&lt;/code&gt; and &lt;code&gt;columns_&lt;/code&gt; attributes. &lt;code&gt;rows_[i]&lt;/code&gt; is a binary vector with nonzero entries corresponding to rows that belong to bicluster &lt;code&gt;i&lt;/code&gt;. Similarly, &lt;code&gt;columns_[i]&lt;/code&gt; indicates which columns belong to bicluster &lt;code&gt;i&lt;/code&gt;.</source>
          <target state="translated">拟合模型后，可以在 &lt;code&gt;rows_&lt;/code&gt; 和 &lt;code&gt;columns_&lt;/code&gt; 属性中找到行集群列成员资格。 &lt;code&gt;rows_[i]&lt;/code&gt; 是与对应于属于bicluster行的非零项的双元载体 &lt;code&gt;i&lt;/code&gt; 。类似地， &lt;code&gt;columns_[i]&lt;/code&gt; 指示哪些列属于bicluster &lt;code&gt;i&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="71ae50af31deda9eff4041050d01116db2fe0b76" translate="yes" xml:space="preserve">
          <source>After normalizing, the first few singular vectors are computed, just as in the Spectral Co-Clustering algorithm.</source>
          <target state="translated">归一化后,计算出前几个奇异向量,就像光谱共聚算法一样。</target>
        </trans-unit>
        <trans-unit id="c326685e6de3fbd40d347b380da45cf33d03051c" translate="yes" xml:space="preserve">
          <source>After this operation, \(U_k \Sigma_k^\top\) is the transformed training set with \(k\) features (called &lt;code&gt;n_components&lt;/code&gt; in the API).</source>
          <target state="translated">完成此操作后，\（U_k \ Sigma_k ^ \ top \）是具有\（k \）功能（在API中称为 &lt;code&gt;n_components&lt;/code&gt; ）的转换训练集。</target>
        </trans-unit>
        <trans-unit id="d10a093c92eea73e29249b26fe409d51c485f495" translate="yes" xml:space="preserve">
          <source>After training a scikit-learn model, it is desirable to have a way to persist the model for future use without having to retrain. The following section gives you an example of how to persist a model with pickle. We&amp;rsquo;ll also review a few security and maintainability issues when working with pickle serialization.</source>
          <target state="translated">训练了scikit学习模型后，希望有一种方法可以持久保存模型以备将来使用，而不必重新训练。下一节为您提供了一个如何使用pickle持久化模型的示例。在处理pickle序列化时，我们还将回顾一些安全性和可维护性问题。</target>
        </trans-unit>
        <trans-unit id="24449d2cd36cc7fedaa85e80b22b4a39b7f8e2cc" translate="yes" xml:space="preserve">
          <source>After using such a procedure to fit the dictionary, the transform is simply a sparse coding step that shares the same implementation with all dictionary learning objects (see &lt;a href=&quot;#sparsecoder&quot;&gt;Sparse coding with a precomputed dictionary&lt;/a&gt;).</source>
          <target state="translated">在使用了这样一种适合字典的过程之后，变换只是一个稀疏编码步骤，与所有字典学习对象共享相同的实现（请参见&lt;a href=&quot;#sparsecoder&quot;&gt;使用预计算字典的稀疏编码&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="2b35bdc816ab119e837a0f526e39f92fef72f3ba" translate="yes" xml:space="preserve">
          <source>Again please see the &lt;a href=&quot;classes#text-feature-extraction-ref&quot;&gt;reference documentation&lt;/a&gt; for the details on all the parameters.</source>
          <target state="translated">同样，请参阅&lt;a href=&quot;classes#text-feature-extraction-ref&quot;&gt;参考文档&lt;/a&gt;以获取所有参数的详细信息。</target>
        </trans-unit>
        <trans-unit id="99e1009ea0e8d7b9e4e16a4193c8696d68c4efff" translate="yes" xml:space="preserve">
          <source>Again, we check the performance of the computed model using, for example, the median absolute error of the model and the R squared coefficient.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff9f1ff32120d8b893c1ded522d49590353b29a6" translate="yes" xml:space="preserve">
          <source>Age</source>
          <target state="translated">Age</target>
        </trans-unit>
        <trans-unit id="e869aecf975cf3b336fce1699b0d4503e42564de" translate="yes" xml:space="preserve">
          <source>Agglomerate features.</source>
          <target state="translated">砾岩特征。</target>
        </trans-unit>
        <trans-unit id="dbb914491e4e2790ae24eaabcabbe0b9aa470c77" translate="yes" xml:space="preserve">
          <source>Agglomerative Clustering</source>
          <target state="translated">聚类聚类</target>
        </trans-unit>
        <trans-unit id="35bb423e043758761d2f3e248257c1ca705a5ac8" translate="yes" xml:space="preserve">
          <source>Agglomerative cluster has a &amp;ldquo;rich get richer&amp;rdquo; behavior that leads to uneven cluster sizes. In this regard, single linkage is the worst strategy, and Ward gives the most regular sizes. However, the affinity (or distance used in clustering) cannot be varied with Ward, thus for non Euclidean metrics, average linkage is a good alternative. Single linkage, while not robust to noisy data, can be computed very efficiently and can therefore be useful to provide hierarchical clustering of larger datasets. Single linkage can also perform well on non-globular data.</source>
          <target state="translated">聚集集群具有&amp;ldquo;致富变富&amp;rdquo;的行为，导致集群大小不均。在这方面，单联是最差的策略，而沃德给出的规则尺寸最大。但是，亲和力（或聚类中使用的距离）不能随Ward一起变化，因此对于非欧几里得度量，平均链接是一个很好的选择。单一链接虽然对嘈杂的数据不强健，但可以非常有效地进行计算，因此可用于提供较大数据集的分层聚类。单链接也可以在非球形数据上表现良好。</target>
        </trans-unit>
        <trans-unit id="890d73cb8666b8b11b8fc128eb14fe1823b0fbff" translate="yes" xml:space="preserve">
          <source>Agglomerative clustering</source>
          <target state="translated">聚类聚类</target>
        </trans-unit>
        <trans-unit id="21c09abace100a2c8351e8288af87cf02023a3a7" translate="yes" xml:space="preserve">
          <source>Agglomerative clustering with and without structure</source>
          <target state="translated">有结构和无结构的聚合聚类</target>
        </trans-unit>
        <trans-unit id="06e589f9bfed18f64327ae7d57413054b3c6f8de" translate="yes" xml:space="preserve">
          <source>Agglomerative clustering with different metrics</source>
          <target state="translated">不同指标的聚合聚类</target>
        </trans-unit>
        <trans-unit id="4d6b62f2cefed7f7111116c687eea9e3676aa5b0" translate="yes" xml:space="preserve">
          <source>Agnostic</source>
          <target state="translated">Agnostic</target>
        </trans-unit>
        <trans-unit id="ac9f2566d02b5e4600cd56af348af55d70c023a7" translate="yes" xml:space="preserve">
          <source>Agnostic:</source>
          <target state="translated">Agnostic:</target>
        </trans-unit>
        <trans-unit id="a679479691140b63a2247391cf7941ca13e56589" translate="yes" xml:space="preserve">
          <source>Agriculture / weather modeling: number of rain events per year (Poisson), amount of rainfall per event (Gamma), total rainfall per year (Tweedie / Compound Poisson Gamma).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a88029fe8eca09e8e53e31e892c06950617c42cc" translate="yes" xml:space="preserve">
          <source>Akaike information criterion for the current model on the input X.</source>
          <target state="translated">当前模型对输入X的Akaike信息准则。</target>
        </trans-unit>
        <trans-unit id="471a24dc120a414840a5a390be002dd1fb20dce7" translate="yes" xml:space="preserve">
          <source>Alcalinity of Ash:</source>
          <target state="translated">灰分的碱度。</target>
        </trans-unit>
        <trans-unit id="1c996292fc0f34e9abb2ed1bce253f665b2889f0" translate="yes" xml:space="preserve">
          <source>Alcalinity of ash</source>
          <target state="translated">灰分的碱度</target>
        </trans-unit>
        <trans-unit id="35fa6a7b518a822ef300d3ac95936a0d4551ed5f" translate="yes" xml:space="preserve">
          <source>Alcohol</source>
          <target state="translated">Alcohol</target>
        </trans-unit>
        <trans-unit id="4537230d988c507a37af5da4b6f068cb25fae3fd" translate="yes" xml:space="preserve">
          <source>Alcohol:</source>
          <target state="translated">Alcohol:</target>
        </trans-unit>
        <trans-unit id="a755c4c6a00484a10fdd39c78f5316741e33717a" translate="yes" xml:space="preserve">
          <source>Alexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good Probabilities With Supervised Learning, in Proceedings of the 22nd International Conference on Machine Learning (ICML). See section 4 (Qualitative Analysis of Predictions).</source>
          <target state="translated">Alexandru Niculescu-Mizil 和 Rich Caruana (2005)Predicting Good Probabilities With Supervised Learning,in Proceedings of the 22nd International Conference on Machine Learning (ICML).见第4节(预测的定性分析)。</target>
        </trans-unit>
        <trans-unit id="e00df3c495c9c818a9560b719b9887130853cc0a" translate="yes" xml:space="preserve">
          <source>Algorithm to use for nearest neighbors search, passed to neighbors.NearestNeighbors instance.</source>
          <target state="translated">用于最近邻搜索的算法,传递给neighbors.NearestNeighbors实例。</target>
        </trans-unit>
        <trans-unit id="c6ad136737477e6b0db26cf9286713a801e6cd45" translate="yes" xml:space="preserve">
          <source>Algorithm to use in the optimization problem.</source>
          <target state="translated">在优化问题中使用的算法。</target>
        </trans-unit>
        <trans-unit id="c739143784c9537893f5a8383165eeec97c8efbf" translate="yes" xml:space="preserve">
          <source>Algorithm used to compute the nearest neighbors:</source>
          <target state="translated">用于计算最近邻居的算法。</target>
        </trans-unit>
        <trans-unit id="cb151b633578d2167df8c911cd7019b2a2913090" translate="yes" xml:space="preserve">
          <source>Algorithm used to transform the data lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection &lt;code&gt;dictionary * X'&lt;/code&gt;</source>
          <target state="translated">用于转换数据的算法lars：使用最小角度回归方法（linear_model.lars_path）lasso_lars：使用Lars计算套索解决方案lasso_cd：使用坐标下降法计算套索解决方案（linear_model.Lasso）。如果估计的组件稀疏，则lasso_lars会更快。 omp：使用正交匹配追踪来估计稀疏解阈值：将投影 &lt;code&gt;dictionary * X'&lt;/code&gt; 所有小于alpha的系数压缩为零</target>
        </trans-unit>
        <trans-unit id="59c5f2e6c851221e36e62f9314e77aa5b776cd59" translate="yes" xml:space="preserve">
          <source>Algorithm used to transform the data. lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection dictionary * X&amp;rsquo;</source>
          <target state="translated">用于转换数据的算法。 lars：使用最小角度回归方法（linear_model.lars_path）lasso_lars：使用Lars计算套索解lasso_cd：使用坐标下降法计算套索解（linear_model.Lasso）。如果估计的组件稀疏，则lasso_lars会更快。 omp：使用正交匹配追踪来估计稀疏解阈值：将投影字典* X'中所有小于alpha的系数压缩为零</target>
        </trans-unit>
        <trans-unit id="d57ea62b7e17ad495e3491babf97d100e18a1702" translate="yes" xml:space="preserve">
          <source>Algorithm used to transform the data: lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection &lt;code&gt;dictionary * X'&lt;/code&gt;</source>
          <target state="translated">用于转换数据的算法：lars：使用最小角度回归方法（linear_model.lars_path）lasso_lars：使用Lars计算套索解lasso_cd：使用坐标下降法计算套索解（linear_model.Lasso）。如果估计的组件稀疏，则lasso_lars会更快。 omp：使用正交匹配追踪来估计稀疏解阈值：将投影 &lt;code&gt;dictionary * X'&lt;/code&gt; 所有小于alpha的系数压缩为零</target>
        </trans-unit>
        <trans-unit id="6dd589ff391e3fe9203a6e5ffaf1bc46cb8b74f7" translate="yes" xml:space="preserve">
          <source>Algorithms also differ in how rows and columns may be assigned to biclusters, which leads to different bicluster structures. Block diagonal or checkerboard structures occur when rows and columns are divided into partitions.</source>
          <target state="translated">算法在如何将行和列分配给双簇的方式上也有所不同,这导致了不同的双簇结构。当行和列被划分为分区时,会出现块状对角线或棋盘结构。</target>
        </trans-unit>
        <trans-unit id="06560884189da8b18b9514c9f15fcee660a6560e" translate="yes" xml:space="preserve">
          <source>Algorithms differ in how they define biclusters. Some of the common types include:</source>
          <target state="translated">算法在如何定义双簇方面有所不同。一些常见的类型包括:</target>
        </trans-unit>
        <trans-unit id="f7b4372e3de7ce07bac66d661704328db6f955a4" translate="yes" xml:space="preserve">
          <source>Alias for field number 0</source>
          <target state="translated">字段号0的别称</target>
        </trans-unit>
        <trans-unit id="bbef6b362c3d3509157f18014e4e5a25eb4e07ea" translate="yes" xml:space="preserve">
          <source>Alias for field number 1</source>
          <target state="translated">字段1的别称</target>
        </trans-unit>
        <trans-unit id="cb7d09e2006a3aec07c13d82496b6f2adb24a1f7" translate="yes" xml:space="preserve">
          <source>Alias for field number 2</source>
          <target state="translated">字段2的别称</target>
        </trans-unit>
        <trans-unit id="2116d748feb69a3af8d3d3f32852bff649bb421e" translate="yes" xml:space="preserve">
          <source>Alias for field number 3</source>
          <target state="translated">字段号3的别称</target>
        </trans-unit>
        <trans-unit id="8ba0e524b527e040b1c69a39c8c3727540cd8735" translate="yes" xml:space="preserve">
          <source>Alias for field number 4</source>
          <target state="translated">字段号4的别称</target>
        </trans-unit>
        <trans-unit id="e972f77e5bbab5ce66decc0654515ea9c4cce33b" translate="yes" xml:space="preserve">
          <source>All Gaussian process kernels are interoperable with &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt; and vice versa: instances of subclasses of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt; can be passed as &lt;code&gt;metric&lt;/code&gt; to &lt;code&gt;pairwise_kernels&lt;/code&gt; from &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt;. Moreover, kernel functions from pairwise can be used as GP kernels by using the wrapper class &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.pairwisekernel#sklearn.gaussian_process.kernels.PairwiseKernel&quot;&gt;&lt;code&gt;PairwiseKernel&lt;/code&gt;&lt;/a&gt;. The only caveat is that the gradient of the hyperparameters is not analytic but numeric and all those kernels support only isotropic distances. The parameter &lt;code&gt;gamma&lt;/code&gt; is considered to be a hyperparameter and may be optimized. The other kernel parameters are set directly at initialization and are kept fixed.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c99cb316a9285e1cd15f4f7521535d0751148726" translate="yes" xml:space="preserve">
          <source>All Gaussian process kernels are interoperable with &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt; and vice versa: instances of subclasses of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt; can be passed as &lt;code&gt;metric&lt;/code&gt; to pairwise_kernels`` from &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt;. Moreover, kernel functions from pairwise can be used as GP kernels by using the wrapper class &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.pairwisekernel#sklearn.gaussian_process.kernels.PairwiseKernel&quot;&gt;&lt;code&gt;PairwiseKernel&lt;/code&gt;&lt;/a&gt;. The only caveat is that the gradient of the hyperparameters is not analytic but numeric and all those kernels support only isotropic distances. The parameter &lt;code&gt;gamma&lt;/code&gt; is considered to be a hyperparameter and may be optimized. The other kernel parameters are set directly at initialization and are kept fixed.</source>
          <target state="translated">所有的高斯过程内核都可以与&lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; &lt;/a&gt;互操作，反之亦然：可以将&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt;子类的实例作为 &lt;code&gt;metric&lt;/code&gt; 从&lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; &lt;/a&gt;传递给pairwise_kernels。此外，通过使用包装器类&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.pairwisekernel#sklearn.gaussian_process.kernels.PairwiseKernel&quot;&gt; &lt;code&gt;PairwiseKernel&lt;/code&gt; ,&lt;/a&gt;可以将pairwise的内核函数用作GP内核。唯一的警告是，超参数的梯度不是解析的而是数值的，所有这些内核仅支持各向同性的距离。参数 &lt;code&gt;gamma&lt;/code&gt; 被认为是超参数，可以进行优化。其他内核参数在初始化时直接设置，并保持固定。</target>
        </trans-unit>
        <trans-unit id="facb53030a78059078bd7255fa4340b830781f6e" translate="yes" xml:space="preserve">
          <source>All above functions (i.e. &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt;&lt;code&gt;scale&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.preprocessing.minmax_scale#sklearn.preprocessing.minmax_scale&quot;&gt;&lt;code&gt;minmax_scale&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.preprocessing.maxabs_scale#sklearn.preprocessing.maxabs_scale&quot;&gt;&lt;code&gt;maxabs_scale&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/sklearn.preprocessing.robust_scale#sklearn.preprocessing.robust_scale&quot;&gt;&lt;code&gt;robust_scale&lt;/code&gt;&lt;/a&gt;) accept 1D array which can be useful in some specific case.</source>
          <target state="translated">以上所有函数（即&lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt; &lt;code&gt;scale&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.preprocessing.minmax_scale#sklearn.preprocessing.minmax_scale&quot;&gt; &lt;code&gt;minmax_scale&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.preprocessing.maxabs_scale#sklearn.preprocessing.maxabs_scale&quot;&gt; &lt;code&gt;maxabs_scale&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.preprocessing.robust_scale#sklearn.preprocessing.robust_scale&quot;&gt; &lt;code&gt;robust_scale&lt;/code&gt; &lt;/a&gt;）都接受一维数组，这在某些特定情况下很有用。</target>
        </trans-unit>
        <trans-unit id="e88245fc777e7533587c2f8e718d50cd7116a3f1" translate="yes" xml:space="preserve">
          <source>All available versions</source>
          <target state="translated">所有可用版本</target>
        </trans-unit>
        <trans-unit id="03af622cdb4aac726985802a2b9d3765c7a41749" translate="yes" xml:space="preserve">
          <source>All bins in each feature have identical widths.</source>
          <target state="translated">每个特征中的所有料仓都有相同的宽度。</target>
        </trans-unit>
        <trans-unit id="59c634de3245b210109c9f97187eafdfb201b388" translate="yes" xml:space="preserve">
          <source>All bins in each feature have the same number of points.</source>
          <target state="translated">每个功能中的所有分仓都有相同的点数。</target>
        </trans-unit>
        <trans-unit id="8e7d5dca10b34ad961d839250513b9c8a0893f0e" translate="yes" xml:space="preserve">
          <source>All classifiers in scikit-learn do multiclass classification out-of-the-box. You don&amp;rsquo;t need to use the &lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt;&lt;code&gt;sklearn.multiclass&lt;/code&gt;&lt;/a&gt; module unless you want to experiment with different multiclass strategies.</source>
          <target state="translated">scikit-learn中的所有分类器都可以直接进行多类分类。除非您要尝试不同的多类策略，否则无需使用&lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt; &lt;code&gt;sklearn.multiclass&lt;/code&gt; &lt;/a&gt;模块。</target>
        </trans-unit>
        <trans-unit id="026ccd1f86687bfe3e5f08bd5747feaae826b1dc" translate="yes" xml:space="preserve">
          <source>All classifiers in scikit-learn implement multiclass classification; you only need to use this module if you want to experiment with custom multiclass strategies.</source>
          <target state="translated">scikit-learn中的所有分类器都实现了多类分类;如果你想实验自定义多类策略,只需要使用这个模块。</target>
        </trans-unit>
        <trans-unit id="c78fdb80201033cef297ce6fb5232c2383bd7521" translate="yes" xml:space="preserve">
          <source>All decision trees use &lt;code&gt;np.float32&lt;/code&gt; arrays internally. If training data is not in this format, a copy of the dataset will be made.</source>
          <target state="translated">所有决策树都在内部使用 &lt;code&gt;np.float32&lt;/code&gt; 数组。如果训练数据不是这种格式，则将复制数据集。</target>
        </trans-unit>
        <trans-unit id="41b549b6eacc2f621d683cbb4d8d7de1b7ed3ca8" translate="yes" xml:space="preserve">
          <source>All entries of this dict (if any) are passed as keyword arguments to the pairwise kernel function.</source>
          <target state="translated">这个dict的所有条目(如果有的话)都会作为关键字参数传递给配对内核函数。</target>
        </trans-unit>
        <trans-unit id="ff969c4b66fd28ef01340faf70c8e905bbf72131" translate="yes" xml:space="preserve">
          <source>All estimator objects expose a &lt;code&gt;fit&lt;/code&gt; method that takes a dataset (usually a 2-d array):</source>
          <target state="translated">所有估算器对象都公开一个带数据集（通常为二维数组）的 &lt;code&gt;fit&lt;/code&gt; 方法：</target>
        </trans-unit>
        <trans-unit id="8781771bbb6ededabb6ec83fb1b33a34b860476e" translate="yes" xml:space="preserve">
          <source>All estimators in a pipeline, except the last one, must be transformers (i.e. must have a &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;transform&lt;/a&gt; method). The last estimator may be any type (transformer, classifier, etc.).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e9616c63898ea085d32d1415c7461f768e275753" translate="yes" xml:space="preserve">
          <source>All estimators in a pipeline, except the last one, must be transformers (i.e. must have a &lt;code&gt;transform&lt;/code&gt; method). The last estimator may be any type (transformer, classifier, etc.).</source>
          <target state="translated">除最后一个管道外，管道中的所有估计器都必须是转换器（即必须具有 &lt;code&gt;transform&lt;/code&gt; 方法）。最后的估算器可以是任何类型（变压器，分类器等）。</target>
        </trans-unit>
        <trans-unit id="7c3fa432791090758c929f832210b21538e1fa2f" translate="yes" xml:space="preserve">
          <source>All estimators in the pipeline must support &lt;code&gt;inverse_transform&lt;/code&gt;.</source>
          <target state="translated">流水线中的所有估计器都必须支持 &lt;code&gt;inverse_transform&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b2ef621d2a7c2b25a3be25bb6ebef0201032958b" translate="yes" xml:space="preserve">
          <source>All estimators should specify all the parameters that can be set at the class level in their &lt;code&gt;__init__&lt;/code&gt; as explicit keyword arguments (no &lt;code&gt;*args&lt;/code&gt; or &lt;code&gt;**kwargs&lt;/code&gt;).</source>
          <target state="translated">所有估算器都应在其 &lt;code&gt;__init__&lt;/code&gt; 中将可在类级别设置的所有参数指定为显式关键字参数（无 &lt;code&gt;*args&lt;/code&gt; 或 &lt;code&gt;**kwargs&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="24e3691f195d78233bed4d7732d29903c7a0bfe6" translate="yes" xml:space="preserve">
          <source>All last five solvers support both dense and sparse data. However, only &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;saga&amp;rsquo; supports sparse input when &lt;code&gt;fit_intercept&lt;/code&gt; is True.</source>
          <target state="translated">最后五个求解器均支持密集和稀疏数据。但是，当 &lt;code&gt;fit_intercept&lt;/code&gt; 为True 时，仅'sag'和'saga'支持稀疏输入。</target>
        </trans-unit>
        <trans-unit id="61f9c2a4b604c0e088d2123f4269900a4c32a338" translate="yes" xml:space="preserve">
          <source>All last five solvers support both dense and sparse data. However, only &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;saga&amp;rsquo; supports sparse input when`fit_intercept` is True.</source>
          <target state="translated">最后五个求解器均支持密集和稀疏数据。但是，当fit_intercept为True时，只有'sag'和'saga'支持稀疏输入。</target>
        </trans-unit>
        <trans-unit id="f728f573fb469cf76a885b25e7cd8d2d6661b65d" translate="yes" xml:space="preserve">
          <source>All last five solvers support both dense and sparse data. However, only &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;sparse_cg&amp;rsquo; supports sparse input when &lt;code&gt;fit_intercept&lt;/code&gt; is True.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9b477d9d300bf316e6d73adbb21d261c724739c5" translate="yes" xml:space="preserve">
          <source>All of X is processed as a single batch. This is intended for cases when &lt;a href=&quot;#sklearn.preprocessing.MaxAbsScaler.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is not feasible due to very large number of &lt;code&gt;n_samples&lt;/code&gt; or because X is read from a continuous stream.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b3fc8e75fa02a8bfaeaedb8d116922428a0fe800" translate="yes" xml:space="preserve">
          <source>All of X is processed as a single batch. This is intended for cases when &lt;a href=&quot;#sklearn.preprocessing.MinMaxScaler.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is not feasible due to very large number of &lt;code&gt;n_samples&lt;/code&gt; or because X is read from a continuous stream.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0771ca4e648606c13a5db59d2963430f3dd6f313" translate="yes" xml:space="preserve">
          <source>All of X is processed as a single batch. This is intended for cases when &lt;a href=&quot;#sklearn.preprocessing.StandardScaler.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; is not feasible due to very large number of &lt;code&gt;n_samples&lt;/code&gt; or because X is read from a continuous stream.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8e879aa8d08a0dbe80a902ae3611f1bc99c99eb6" translate="yes" xml:space="preserve">
          <source>All of the above are supported by &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe669fffadfc5710970ab03a9fde631f4966ca91" translate="yes" xml:space="preserve">
          <source>All of the above are supported by &lt;code&gt;sklearn.linear_model.stochastic_gradient&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;sklearn.linear_model.stochastic_gradient&lt;/code&gt; 支持以上所有内容。</target>
        </trans-unit>
        <trans-unit id="b5ed2e5e9ebf405638935350a19afaab363efb49" translate="yes" xml:space="preserve">
          <source>All of the above loss functions can be regarded as an upper bound on the misclassification error (Zero-one loss) as shown in the Figure below.</source>
          <target state="translated">上述损失函数都可以看作是误分类误差的上界(零一损失),如下图所示。</target>
        </trans-unit>
        <trans-unit id="0050a0a20cce6934eea13e5fb5f70c03d8a3e6cd" translate="yes" xml:space="preserve">
          <source>All penalization parameters explored.</source>
          <target state="translated">所有处罚参数都进行了探讨。</target>
        </trans-unit>
        <trans-unit id="955d9d437b529cc06e590b033bee6c6f48038cb1" translate="yes" xml:space="preserve">
          <source>All scikit-learn classifiers are capable of multiclass classification, but the meta-estimators offered by &lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt;&lt;code&gt;sklearn.multiclass&lt;/code&gt;&lt;/a&gt; permit changing the way they handle more than two classes because this may have an effect on classifier performance (either in terms of generalization error or required computational resources).</source>
          <target state="translated">所有scikit-learn分类器都能够进行多类分类，但是&lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt; &lt;code&gt;sklearn.multiclass&lt;/code&gt; &lt;/a&gt;提供的元估计器允许更改它们处理两个以上类的方式，因为这可能会影响分类器的性能（无论是泛化错误还是所需的计算能力）资源）。</target>
        </trans-unit>
        <trans-unit id="196b45b70e9cf2661d4f75a2029c5a0c8898e090" translate="yes" xml:space="preserve">
          <source>All settings, not just those presently modified, will be returned to their previous values when the context manager is exited. This is not thread-safe.</source>
          <target state="translated">当上下文管理器退出时,所有的设置,不仅仅是当前修改的设置,都将返回到以前的值。这不是线程安全的。</target>
        </trans-unit>
        <trans-unit id="aa66fd6d2b23c29862eb90bc96dba9c2ff577f39" translate="yes" xml:space="preserve">
          <source>All supervised &lt;a href=&quot;https://en.wikipedia.org/wiki/Estimator&quot;&gt;estimators&lt;/a&gt; in scikit-learn implement a &lt;code&gt;fit(X, y)&lt;/code&gt; method to fit the model and a &lt;code&gt;predict(X)&lt;/code&gt; method that, given unlabeled observations &lt;code&gt;X&lt;/code&gt;, returns the predicted labels &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">scikit-learn中所有受监督的&lt;a href=&quot;https://en.wikipedia.org/wiki/Estimator&quot;&gt;估计器都&lt;/a&gt;实现了 &lt;code&gt;fit(X, y)&lt;/code&gt; 模型的fit（X，y）方法和给定未标记观察值 &lt;code&gt;X&lt;/code&gt; 的 &lt;code&gt;predict(X)&lt;/code&gt; 方法，返回预测标签 &lt;code&gt;y&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5fd3433668568da7d84a79ee4e0518a15dd72a27" translate="yes" xml:space="preserve">
          <source>All the input data is provided matrix X (labeled and unlabeled) and corresponding label matrix y with a dedicated marker value for unlabeled samples.</source>
          <target state="translated">所有的输入数据都是提供矩阵X(有标签和无标签)和相应的标签矩阵y,无标签的样品有专门的标记值。</target>
        </trans-unit>
        <trans-unit id="799926dcab2e28e1fbc27d0dc0ec06e58ac91465" translate="yes" xml:space="preserve">
          <source>All these estimators can compute internally the nearest neighbors, but most of them also accept precomputed nearest neighbors &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-sparse-graph&quot;&gt;sparse graph&lt;/a&gt;, as given by &lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;kneighbors_graph&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neighbors.radius_neighbors_graph#sklearn.neighbors.radius_neighbors_graph&quot;&gt;&lt;code&gt;radius_neighbors_graph&lt;/code&gt;&lt;/a&gt;. With mode &lt;code&gt;mode='connectivity'&lt;/code&gt;, these functions return a binary adjacency sparse graph as required, for instance, in &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt;&lt;code&gt;SpectralClustering&lt;/code&gt;&lt;/a&gt;. Whereas with &lt;code&gt;mode='distance'&lt;/code&gt;, they return a distance sparse graph as required, for instance, in &lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;DBSCAN&lt;/code&gt;&lt;/a&gt;. To include these functions in a scikit-learn pipeline, one can also use the corresponding classes &lt;a href=&quot;generated/sklearn.neighbors.kneighborstransformer#sklearn.neighbors.KNeighborsTransformer&quot;&gt;&lt;code&gt;KNeighborsTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neighbors.radiusneighborstransformer#sklearn.neighbors.RadiusNeighborsTransformer&quot;&gt;&lt;code&gt;RadiusNeighborsTransformer&lt;/code&gt;&lt;/a&gt;. The benefits of this sparse graph API are multiple.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3103275737fc127cec8d65804a5477574232497" translate="yes" xml:space="preserve">
          <source>All three models are significantly better than chance but also very far from making perfect predictions.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68201d9ddadd1b10424b1028599271aea0a81e03" translate="yes" xml:space="preserve">
          <source>All values are cached on the filesystem, in a deep directory structure.</source>
          <target state="translated">所有的值都缓存在文件系统中,在一个深层的目录结构中。</target>
        </trans-unit>
        <trans-unit id="ad92361405543cadf3390872502b0368070801b1" translate="yes" xml:space="preserve">
          <source>All, &lt;a href=&quot;generated/sklearn.metrics.mutual_info_score#sklearn.metrics.mutual_info_score&quot;&gt;&lt;code&gt;mutual_info_score&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.adjusted_mutual_info_score#sklearn.metrics.adjusted_mutual_info_score&quot;&gt;&lt;code&gt;adjusted_mutual_info_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt;&lt;code&gt;normalized_mutual_info_score&lt;/code&gt;&lt;/a&gt; are symmetric: swapping the argument does not change the score. Thus they can be used as a &lt;strong&gt;consensus measure&lt;/strong&gt;:</source>
          <target state="translated">所有，&lt;a href=&quot;generated/sklearn.metrics.mutual_info_score#sklearn.metrics.mutual_info_score&quot;&gt; &lt;code&gt;mutual_info_score&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.metrics.adjusted_mutual_info_score#sklearn.metrics.adjusted_mutual_info_score&quot;&gt; &lt;code&gt;adjusted_mutual_info_score&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt; &lt;code&gt;normalized_mutual_info_score&lt;/code&gt; &lt;/a&gt;是对称的：交换的说法不改变比分。因此，它们可以用作&lt;strong&gt;共识措施&lt;/strong&gt;：</target>
        </trans-unit>
        <trans-unit id="9cc3cbd54e82f76756e09725006efed713dfb14b" translate="yes" xml:space="preserve">
          <source>Allow to bypass several input checking. Don&amp;rsquo;t use this parameter unless you know what you do.</source>
          <target state="translated">允许绕过多个输入检查。除非您知道自己要做什么，否则不要使用此参数。</target>
        </trans-unit>
        <trans-unit id="b2988d20e10f74030eeb38e913a183de75189dbb" translate="yes" xml:space="preserve">
          <source>Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.</source>
          <target state="translated">允许的输入是列表、numpy数组、scipy-sparse矩阵或pandas数据框。</target>
        </trans-unit>
        <trans-unit id="246073e83f7bacaf2b27a7cb44f4ce78f91064b5" translate="yes" xml:space="preserve">
          <source>Allows NaN in the input.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7d1bd9e91706d2b4396d936c8102ad0f0e779fcd" translate="yes" xml:space="preserve">
          <source>Allows NaN/Inf in the input if the underlying estimator does as well.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b369263cc488ea5fca6946112c8afe7517b680b5" translate="yes" xml:space="preserve">
          <source>Allows simple indexing of lists or arrays.</source>
          <target state="translated">允许对列表或数组进行简单的索引。</target>
        </trans-unit>
        <trans-unit id="bd0b505b007b0a8de2703b572d4c9c27c39f3415" translate="yes" xml:space="preserve">
          <source>Allows to examine the spread of each true cluster across predicted clusters and vice versa.</source>
          <target state="translated">允许检查每个真簇在预测簇之间的分布,反之亦然。</target>
        </trans-unit>
        <trans-unit id="036fd67d51f9b182736132cac7fb44d0406f98ef" translate="yes" xml:space="preserve">
          <source>Almost every group is distinguished by whether headers such as &lt;code&gt;NNTP-Posting-Host:&lt;/code&gt; and &lt;code&gt;Distribution:&lt;/code&gt; appear more or less often.</source>
          <target state="translated">几乎每个组都通过标题（例如 &lt;code&gt;NNTP-Posting-Host:&lt;/code&gt; 和 &lt;code&gt;Distribution:&lt;/code&gt; 出现的次数有所不同。</target>
        </trans-unit>
        <trans-unit id="54076ee073b95c8e2227a7529bbd3c040278da4f" translate="yes" xml:space="preserve">
          <source>Alpaydin (alpaydin &amp;lsquo;@&amp;rsquo; boun.edu.tr)</source>
          <target state="translated">Alpaydin（alpaydin'@'boun.edu.tr）</target>
        </trans-unit>
        <trans-unit id="78d537ced7a0f9450c0b7f5b446829da6458dfc9" translate="yes" xml:space="preserve">
          <source>Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.</source>
          <target state="translated">Alpaydin,C.Kaynak(1998)Cascading Classifiers,Kybernetika。</target>
        </trans-unit>
        <trans-unit id="bc0f2eb3e0375e2eb0ff510eb47611a2b3ce02af" translate="yes" xml:space="preserve">
          <source>Alpha is a parameter for regularization term, aka penalty term, that combats overfitting by constraining the size of the weights. Increasing alpha may fix high variance (a sign of overfitting) by encouraging smaller weights, resulting in a decision boundary plot that appears with lesser curvatures. Similarly, decreasing alpha may fix high bias (a sign of underfitting) by encouraging larger weights, potentially resulting in a more complicated decision boundary.</source>
          <target state="translated">Alpha是正则化项的一个参数,也就是惩罚项,通过限制权重的大小来对抗过拟合。增加α可以通过鼓励较小的权重来修正高方差(过度拟合的标志),从而使决策边界图出现较小的曲率。同样,降低阿尔法可以通过鼓励较大的权重来解决高偏差(拟合不足的迹象),可能导致决策边界更加复杂。</target>
        </trans-unit>
        <trans-unit id="a01b7375f883c68c86f4064f9c436e0a18c14d5d" translate="yes" xml:space="preserve">
          <source>Alpha is again treated as a random variable that is to be estimated from the data.</source>
          <target state="translated">阿尔法又被当作一个随机变量,要从数据中估计出来。</target>
        </trans-unit>
        <trans-unit id="9bc4ba22610a89d08abe53892ec649b1e8a46ce9" translate="yes" xml:space="preserve">
          <source>Also for multiple metric evaluation, the attributes &lt;code&gt;best_index_&lt;/code&gt;, &lt;code&gt;best_score_&lt;/code&gt; and &lt;code&gt;best_params_&lt;/code&gt; will only be available if &lt;code&gt;refit&lt;/code&gt; is set and all of them will be determined w.r.t this specific scorer.</source>
          <target state="translated">也可为多指标评价，属性 &lt;code&gt;best_index_&lt;/code&gt; ， &lt;code&gt;best_score_&lt;/code&gt; 和 &lt;code&gt;best_params_&lt;/code&gt; 如果将只提供 &lt;code&gt;refit&lt;/code&gt; 并进行所有的人都会WRT这个特定的得分手来确定。</target>
        </trans-unit>
        <trans-unit id="7c913c796a2efc191b66ad3c6a4c672e085ca88e" translate="yes" xml:space="preserve">
          <source>Also from the thickness of the silhouette plot the cluster size can be visualized. The silhouette plot for cluster 0 when &lt;code&gt;n_clusters&lt;/code&gt; is equal to 2, is bigger in size owing to the grouping of the 3 sub clusters into one big cluster. However when the &lt;code&gt;n_clusters&lt;/code&gt; is equal to 4, all the plots are more or less of similar thickness and hence are of similar sizes as can be also verified from the labelled scatter plot on the right.</source>
          <target state="translated">同样从轮廓图的厚度可以看到簇的大小。当 &lt;code&gt;n_clusters&lt;/code&gt; 等于2 时，簇0的轮廓图的尺寸较大，这是因为将3个子簇分组为一个大簇。但是，当 &lt;code&gt;n_clusters&lt;/code&gt; 等于4时，所有图或多或少都具有相似的厚度，因此具有相似的大小，这也可以从右侧的标记散点图进行验证。</target>
        </trans-unit>
        <trans-unit id="11074bab0d4cf60477f10f484031e0877ac22e6b" translate="yes" xml:space="preserve">
          <source>Also known as one-vs-all, this strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only &lt;code&gt;n_classes&lt;/code&gt; classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multiclass classification and is a fair default choice.</source>
          <target state="translated">也称为&amp;ldquo;一对多&amp;rdquo;，此策略包括为每个类配备一个分类器。对于每个分类器，该分类将与所有其他分类进行拟合。除了其计算效率（仅需要 &lt;code&gt;n_classes&lt;/code&gt; 个分类器）之外，此方法的一个优点是其可解释性。由于每个类别仅由一个和一个分类器表示，因此可以通过检查其对应的分类器来获取有关该类别的知识。这是用于多类分类的最常用策略，并且是合理的默认选择。</target>
        </trans-unit>
        <trans-unit id="cc34e41c8fa23138f4c5476496b3bc4a71c58fb4" translate="yes" xml:space="preserve">
          <source>Also note that both random features have very low importances (close to 0) as expected.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f4d07ff4d50db829057964fee42d298814f3483" translate="yes" xml:space="preserve">
          <source>Also note that even though Box-Cox seems to perform better than Yeo-Johnson for lognormal and chi-squared distributions, keep in mind that Box-Cox does not support inputs with negative values.</source>
          <target state="translated">另外要注意的是,尽管Box-Cox对于对数正态分布和齐次方分布的表现似乎比Yeo-Johnson更好,但请记住Box-Cox不支持负值的输入。</target>
        </trans-unit>
        <trans-unit id="0af9b79bdfeacc61c23bfadd6f95c644ff726db7" translate="yes" xml:space="preserve">
          <source>Also note that for the linear case, the algorithm used in &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; by the &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; implementation is much more efficient than its &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;-based &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; counterpart and can scale almost linearly to millions of samples and/or features.</source>
          <target state="translated">另外请注意，对于线性的情况下，在所使用的算法&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;由&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt;实现更有效的比&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;LIBSVM&lt;/a&gt;为基础的&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt;对应，并且几乎可以线性扩展到数百万的样品和/或功能。</target>
        </trans-unit>
        <trans-unit id="5de10864fc42a72508361320516be4ba6cf282ee" translate="yes" xml:space="preserve">
          <source>Also note that the digits labels roughly match the natural grouping found by t-SNE while the linear 2D projection of the PCA model yields a representation where label regions largely overlap. This is a strong clue that this data can be well separated by non linear methods that focus on the local structure (e.g. an SVM with a Gaussian RBF kernel). However, failing to visualize well separated homogeneously labeled groups with t-SNE in 2D does not necessarily imply that the data cannot be correctly classified by a supervised model. It might be the case that 2 dimensions are not low enough to accurately represents the internal structure of the data.</source>
          <target state="translated">同时注意到,数字标签与t-SNE发现的自然分组大致吻合,而PCA模型的线性二维投影则产生了标签区域基本重叠的表示。这是一个强烈的线索,即这种数据可以通过专注于局部结构的非线性方法(例如,具有高斯RBF内核的SVM)进行很好的分离。然而,未能在2D中用t-SNE可视化很好分离的同质标记组,并不一定意味着数据不能被监督模型正确分类。可能是2维度不够低,无法准确表示数据的内部结构。</target>
        </trans-unit>
        <trans-unit id="ab26a7a4f86bc166b4ac9642686ddf7141b2b207" translate="yes" xml:space="preserve">
          <source>Also note that we set a low value for the tolerance to make sure that the model has converged before collecting the coefficients.</source>
          <target state="translated">另外需要注意的是,我们为容差设置了一个低值,以确保模型在收集系数之前已经收敛。</target>
        </trans-unit>
        <trans-unit id="7d37adf37d9aef6924cdccc898bbb4ca10bb0758" translate="yes" xml:space="preserve">
          <source>Also useful for lower-level tasks is the function &lt;a href=&quot;generated/sklearn.linear_model.lasso_path#sklearn.linear_model.lasso_path&quot;&gt;&lt;code&gt;lasso_path&lt;/code&gt;&lt;/a&gt; that computes the coefficients along the full path of possible values.</source>
          <target state="translated">对于较低级任务也有用的是函数&lt;a href=&quot;generated/sklearn.linear_model.lasso_path#sklearn.linear_model.lasso_path&quot;&gt; &lt;code&gt;lasso_path&lt;/code&gt; &lt;/a&gt;，它沿可能值的完整路径计算系数。</target>
        </trans-unit>
        <trans-unit id="2587ad1f75524017fb1ecdf6376d89726a0f6825" translate="yes" xml:space="preserve">
          <source>Also, by evaluating log marginal likelihood (L) of these models, we can determine which one is better. It can be concluded that the model with larger L is more likely.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0b5de16e36a8364053840b53f5f6fe23ce35a1bc" translate="yes" xml:space="preserve">
          <source>Also, the EXPERIENCE and AGE are strongly linearly correlated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4a2b5f07f08a470a2b2e7ef1ae5a0e4e195af3e0" translate="yes" xml:space="preserve">
          <source>Also, these routines have not been tested for graphs with negative distances. Negative distances can lead to infinite cycles that must be handled by specialized algorithms.</source>
          <target state="translated">另外,这些例程还没有针对负距离的图形进行测试。负距离可能导致无限循环,必须用专门的算法来处理。</target>
        </trans-unit>
        <trans-unit id="9ba49a974ea195b74ff8d485a45574a6c2262ea1" translate="yes" xml:space="preserve">
          <source>Also, this estimator is different from the R implementation of Robust Regression (&lt;a href=&quot;http://www.ats.ucla.edu/stat/r/dae/rreg.htm&quot;&gt;http://www.ats.ucla.edu/stat/r/dae/rreg.htm&lt;/a&gt;) because the R implementation does a weighted least squares implementation with weights given to each sample on the basis of how much the residual is greater than a certain threshold.</source>
          <target state="translated">同样，此估算器与R的&amp;ldquo;稳健回归&amp;rdquo;实现（&lt;a href=&quot;http://www.ats.ucla.edu/stat/r/dae/rreg.htm&quot;&gt;http://www.ats.ucla.edu/stat/r/dae/rreg.htm&lt;/a&gt;）不同，因为R实现了加权最小二乘实现，且权重为每个样本都基于残差大于某个阈值的多少。</target>
        </trans-unit>
        <trans-unit id="8c3843c6d1de204ea6ff77660712728c7734da35" translate="yes" xml:space="preserve">
          <source>Alternate label propagation strategy more robust to noise</source>
          <target state="translated">备用的标签传播策略对噪声更稳健</target>
        </trans-unit>
        <trans-unit id="a1df7e48a8694bc511c33f4fdb83c6299c349bb3" translate="yes" xml:space="preserve">
          <source>Alternate output array in which to place the result. The default is &lt;code&gt;None&lt;/code&gt;; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See &lt;code&gt;doc.ufuncs&lt;/code&gt; for details.</source>
          <target state="translated">要在其中放置结果的备用输出数组。默认为 &lt;code&gt;None&lt;/code&gt; ; 如果提供的话，它的形状必须与预期的输出形状相同，但是如果需要的话，将强制转换类型。有关详细信息，请参见 &lt;code&gt;doc.ufuncs&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1444a36563243cd516125e47e643f6c86af2b2d3" translate="yes" xml:space="preserve">
          <source>Alternating Least Squares (Fast HALS).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="16e8b22da1997a9e7a46f4b2f3065cb731917af6" translate="yes" xml:space="preserve">
          <source>Alternative implementation that does incremental updates of the centers&amp;rsquo; positions using mini-batches.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9319ffecfc2401bba3b77152ec4f1cf3bfc1dbce" translate="yes" xml:space="preserve">
          <source>Alternative online implementation that does incremental updates of the centers positions using mini-batches. For large scale learning (say n_samples &amp;gt; 10k) MiniBatchKMeans is probably much faster than the default batch implementation.</source>
          <target state="translated">替代性在线实施，使用微型批次对中心位置进行增量更新。对于大规模学习（例如n_samples&amp;gt; 10k），MiniBatchKMeans可能比默认的批处理实现要快得多。</target>
        </trans-unit>
        <trans-unit id="203de1a7976ebc427fc76b372cb16da457115e31" translate="yes" xml:space="preserve">
          <source>Alternatively binaries for graphviz can be downloaded from the graphviz project homepage, and the Python wrapper installed from pypi with &lt;code&gt;pip install graphviz&lt;/code&gt;.</source>
          <target state="translated">另外，可以从graphviz项目主页下载graphviz的二进制文件，然后使用 &lt;code&gt;pip install graphviz&lt;/code&gt; 从pypi安装Python包装器。</target>
        </trans-unit>
        <trans-unit id="a86f6ee74cd1a30708099bd1d22a36b2f08e9a0f" translate="yes" xml:space="preserve">
          <source>Alternatively the backend can be passed directly as an instance.</source>
          <target state="translated">另外,后台也可以直接作为一个实例传递。</target>
        </trans-unit>
        <trans-unit id="76c8a1ac67e02525fd024c6509339aad00aeced5" translate="yes" xml:space="preserve">
          <source>Alternatively, it can be set by the &amp;lsquo;SCIKIT_LEARN_DATA&amp;rsquo; environment variable or programmatically by giving an explicit folder path. The &amp;lsquo;~&amp;rsquo; symbol is expanded to the user home folder.</source>
          <target state="translated">或者，可以通过环境变量&amp;ldquo; SCIKIT_LEARN_DATA&amp;rdquo;来设置它，也可以通过提供一个明确的文件夹路径来设置它。&amp;ldquo;〜&amp;rdquo;符号将展开到用户主文件夹。</target>
        </trans-unit>
        <trans-unit id="adf895b415f65b1116cbe9c2579f72b5380f5476" translate="yes" xml:space="preserve">
          <source>Alternatively, one can directly model the total loss with a unique Compound Poisson Gamma generalized linear model (with a log link function). This model is a special case of the Tweedie GLM with a &amp;ldquo;power&amp;rdquo; parameter \(p \in (1, 2)\). Here, we fix apriori the &lt;code&gt;power&lt;/code&gt; parameter of the Tweedie model to some arbitrary value (1.9) in the valid range. Ideally one would select this value via grid-search by minimizing the negative log-likelihood of the Tweedie model, but unfortunately the current implementation does not allow for this (yet).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5dd953719fa9152bd6124ae49c16c66b8c780a34" translate="yes" xml:space="preserve">
          <source>Alternatively, one can use the &lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; classes directly to find nearest neighbors. This is the functionality wrapped by the &lt;a href=&quot;generated/sklearn.neighbors.nearestneighbors#sklearn.neighbors.NearestNeighbors&quot;&gt;&lt;code&gt;NearestNeighbors&lt;/code&gt;&lt;/a&gt; class used above. The Ball Tree and KD Tree have the same interface; we&amp;rsquo;ll show an example of using the KD Tree here:</source>
          <target state="translated">或者，可以直接使用&lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt;类来查找最近的邻居。这是上面使用的&lt;a href=&quot;generated/sklearn.neighbors.nearestneighbors#sklearn.neighbors.NearestNeighbors&quot;&gt; &lt;code&gt;NearestNeighbors&lt;/code&gt; &lt;/a&gt;类包装的功能。球树和KD树具有相同的接口；我们将在此处显示使用KD树的示例：</target>
        </trans-unit>
        <trans-unit id="a2b3bf63690d9fdf9c988aa4c9e184e0fbef9897" translate="yes" xml:space="preserve">
          <source>Alternatively, orthogonal matching pursuit can target a specific error instead of a specific number of non-zero coefficients. This can be expressed as:</source>
          <target state="translated">另外,正交匹配追求可以针对特定的误差而不是特定数量的非零系数。这可以表示为:</target>
        </trans-unit>
        <trans-unit id="0f7ec2ff37de8d3d768baf118074df7d6149a11f" translate="yes" xml:space="preserve">
          <source>Alternatively, the &lt;code&gt;scoring&lt;/code&gt; argument can be provided to specify an alternative scoring method.</source>
          <target state="translated">可替代地，可以提供 &lt;code&gt;scoring&lt;/code&gt; 参数来指定替代的计分方法。</target>
        </trans-unit>
        <trans-unit id="ea61f1eddfe969d509258a2bd8bcd0672a722400" translate="yes" xml:space="preserve">
          <source>Alternatively, the estimator &lt;a href=&quot;generated/sklearn.linear_model.lassolarsic#sklearn.linear_model.LassoLarsIC&quot;&gt;&lt;code&gt;LassoLarsIC&lt;/code&gt;&lt;/a&gt; proposes to use the Akaike information criterion (AIC) and the Bayes Information criterion (BIC). It is a computationally cheaper alternative to find the optimal value of alpha as the regularization path is computed only once instead of k+1 times when using k-fold cross-validation. However, such criteria needs a proper estimation of the degrees of freedom of the solution, are derived for large samples (asymptotic results) and assume the model is correct, i.e. that the data are actually generated by this model. They also tend to break when the problem is badly conditioned (more features than samples).</source>
          <target state="translated">备选地，估计器&lt;a href=&quot;generated/sklearn.linear_model.lassolarsic#sklearn.linear_model.LassoLarsIC&quot;&gt; &lt;code&gt;LassoLarsIC&lt;/code&gt; &lt;/a&gt;建议使用Akaike信息标准（AIC）和贝叶斯信息标准（BIC）。由于使用k倍交叉验证时仅对正则化路径进行一次计算而不是k + 1次计算，因此找到alpha的最佳值在计算上更便宜。但是，此类标准需要对解决方案的自由度进行适当的估计，并针对大样本（渐近结果）推导得出，并假设模型是正确的，即数据实际上是由该模型生成的。当问题处理不当时（功能比示例更多），它们也倾向于断开。</target>
        </trans-unit>
        <trans-unit id="f454663104fe93624afb99e1fe4ea6ef9bcd01cf" translate="yes" xml:space="preserve">
          <source>Alternatively, the probability of each class can be predicted, which is the fraction of training samples of the same class in a leaf:</source>
          <target state="translated">另外,还可以预测每个类的概率,即叶子中同一类的训练样本的分数。</target>
        </trans-unit>
        <trans-unit id="5c74d35687e3587d17adc94ee7b35af54787e26a" translate="yes" xml:space="preserve">
          <source>Alternatively, the tree can also be exported in textual format with the function &lt;a href=&quot;generated/sklearn.tree.export_text#sklearn.tree.export_text&quot;&gt;&lt;code&gt;export_text&lt;/code&gt;&lt;/a&gt;. This method doesn&amp;rsquo;t require the installation of external libraries and is more compact:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3d7768459c8a27625c6b1edd47cdb7cdd42a8dbd" translate="yes" xml:space="preserve">
          <source>Alternatively, using &lt;code&gt;precomputed&lt;/code&gt;, a user-provided affinity matrix can be used.</source>
          <target state="translated">替代地，使用 &lt;code&gt;precomputed&lt;/code&gt; ，可以使用用户提供的亲和力矩阵。</target>
        </trans-unit>
        <trans-unit id="408564450de584fdcd4c6ffe287273935ea61be3" translate="yes" xml:space="preserve">
          <source>Alternatively, you can control the tree size by specifying the number of leaf nodes via the parameter &lt;code&gt;max_leaf_nodes&lt;/code&gt;. In this case, trees will be grown using best-first search where nodes with the highest improvement in impurity will be expanded first. A tree with &lt;code&gt;max_leaf_nodes=k&lt;/code&gt; has &lt;code&gt;k - 1&lt;/code&gt; split nodes and thus can model interactions of up to order &lt;code&gt;max_leaf_nodes - 1&lt;/code&gt; .</source>
          <target state="translated">另外，您可以通过参数 &lt;code&gt;max_leaf_nodes&lt;/code&gt; 指定叶节点的数量来控制树的大小。在这种情况下，将使用最佳优先搜索来生长树，其中将首先扩展具有最大杂质改进的节点。一棵 &lt;code&gt;max_leaf_nodes=k&lt;/code&gt; 的树具有 &lt;code&gt;k - 1&lt;/code&gt; 分裂节点，因此可以建模多达 &lt;code&gt;max_leaf_nodes - 1&lt;/code&gt; 阶的交互。</target>
        </trans-unit>
        <trans-unit id="9d8e5a012d3af6b5916c5e0e392c2cf43112fb2f" translate="yes" xml:space="preserve">
          <source>Although GMM are often used for clustering, we can compare the obtained clusters with the actual classes from the dataset. We initialize the means of the Gaussians with the means of the classes from the training set to make this comparison valid.</source>
          <target state="translated">虽然GMM经常用于聚类,但我们可以将得到的聚类与数据集中的实际类进行比较。我们用训练集中的类的均值来初始化高斯的均值,以使这种比较有效。</target>
        </trans-unit>
        <trans-unit id="9e3f75157b99771eea902cdfb247d8150fcbbb65" translate="yes" xml:space="preserve">
          <source>Although a list of sets or tuples is a very intuitive format for multilabel data, it is unwieldy to process. This transformer converts between this intuitive format and the supported multilabel format: a (samples x classes) binary matrix indicating the presence of a class label.</source>
          <target state="translated">虽然对于多标签数据来说,集合或元组的列表是一种非常直观的格式,但处理起来却很麻烦。这个变换器在这种直观的格式和支持的多标签格式之间进行转换:一个(样本x类)二进制矩阵表示类标签的存在。</target>
        </trans-unit>
        <trans-unit id="6b97831f2bf852269cd56a6950af72a787322f90" translate="yes" xml:space="preserve">
          <source>Although online method is guaranteed to converge to a local optimum point, the quality of the optimum point and the speed of convergence may depend on mini-batch size and attributes related to learning rate setting.</source>
          <target state="translated">虽然在线方法可以保证收敛到局部最优点,但最优点的质量和收敛速度可能取决于小批量的大小和与学习率设置相关的属性。</target>
        </trans-unit>
        <trans-unit id="be1a3f200b8c31300cb53ca0638c27940f9fd773" translate="yes" xml:space="preserve">
          <source>Although the online method is guaranteed to converge to a local optimum point, the quality of the optimum point and the speed of convergence may depend on mini-batch size and attributes related to learning rate setting.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b824f979746fff21942ed5a8526c30831b67aa67" translate="yes" xml:space="preserve">
          <source>Always ignored, exists for compatibility.</source>
          <target state="translated">总是被忽略,为了兼容而存在。</target>
        </trans-unit>
        <trans-unit id="f671e33354d7ef61d0b19d9b0bec50200bf9529c" translate="yes" xml:space="preserve">
          <source>Always ignored, exists for compatibility. &lt;code&gt;np.zeros(n_samples)&lt;/code&gt; may be used as a placeholder.</source>
          <target state="translated">始终被忽略，出于兼容性考虑而存在。 &lt;code&gt;np.zeros(n_samples)&lt;/code&gt; 可用作占位符。</target>
        </trans-unit>
        <trans-unit id="86f5a3adc76607b6ec083bd7a2c05aeeca017cf3" translate="yes" xml:space="preserve">
          <source>Amount of ridge shrinkage to apply in order to improve conditioning when calling the transform method.</source>
          <target state="translated">在调用变换方法时,为了改善调节,要应用的脊缩量。</target>
        </trans-unit>
        <trans-unit id="a7500e59c81a09edaea684ef869962960b125989" translate="yes" xml:space="preserve">
          <source>Amount of ridge shrinkage to apply in order to improve conditioning.</source>
          <target state="translated">为了提高调理效果,要施加的脊缩量。</target>
        </trans-unit>
        <trans-unit id="a93d54da303ddad51202c963e4858505c997f8d0" translate="yes" xml:space="preserve">
          <source>Amount of verbosity.</source>
          <target state="translated">啰嗦的数量。</target>
        </trans-unit>
        <trans-unit id="40cf2c52a2e789ddb84ca29dcc3ddee6f4122f49" translate="yes" xml:space="preserve">
          <source>An AdaBoost [1] classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.</source>
          <target state="translated">AdaBoost[1]分类器是一个元估计器,它首先在原始数据集上拟合一个分类器,然后在同一数据集上拟合分类器的其他副本,但其中错误分类实例的权重会被调整,从而使后续的分类器更加关注困难的情况。</target>
        </trans-unit>
        <trans-unit id="043d85ce90b3af29bb38c14a359edb987b27e86d" translate="yes" xml:space="preserve">
          <source>An AdaBoost [1] regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases.</source>
          <target state="translated">AdaBoost[1]回归器是一个元估计器,它首先在原始数据集上拟合一个回归器,然后在同一数据集上拟合更多的回归器副本,但实例的权重会根据当前预测的误差进行调整。因此,后续的回归器会更加关注困难的案例。</target>
        </trans-unit>
        <trans-unit id="a0726965169fa1a924e6755570036e923d55d00c" translate="yes" xml:space="preserve">
          <source>An AdaBoost classifier.</source>
          <target state="translated">一个AdaBoost分类器。</target>
        </trans-unit>
        <trans-unit id="3388a3be3246d31996f2f786da9e288b6011901a" translate="yes" xml:space="preserve">
          <source>An AdaBoost regressor that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0f161ccb66d6dfc0409a9a54a8f62d08c472ddf6" translate="yes" xml:space="preserve">
          <source>An AdaBoost regressor.</source>
          <target state="translated">一个AdaBoost回归器。</target>
        </trans-unit>
        <trans-unit id="6356fae027d92d258a26309d7a3f256df2c9cc55" translate="yes" xml:space="preserve">
          <source>An Exception object.</source>
          <target state="translated">一个异常对象。</target>
        </trans-unit>
        <trans-unit id="91b401262ec54e41ccc28fc97e3dca77764edee1" translate="yes" xml:space="preserve">
          <source>An already fitted classifier can be calibrated by setting &lt;code&gt;cv=&quot;prefit&quot;&lt;/code&gt;. In this case, the data is only used to fit the regressor. It is up to the user make sure that the data used for fitting the classifier is disjoint from the data used for fitting the regressor.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="96b2243b1ac08495762e9c93f82c1e26829c8a97" translate="yes" xml:space="preserve">
          <source>An alternative and recommended approach is to use &lt;code&gt;StandardScaler&lt;/code&gt; in a &lt;code&gt;Pipeline&lt;/code&gt;</source>
          <target state="translated">一种推荐的替代方法是在 &lt;code&gt;Pipeline&lt;/code&gt; 使用 &lt;code&gt;StandardScaler&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="6bcd8e2a5d780cc52692b09904c485c359805e94" translate="yes" xml:space="preserve">
          <source>An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using &lt;a href=&quot;generated/sklearn.preprocessing.minmaxscaler#sklearn.preprocessing.MinMaxScaler&quot;&gt;&lt;code&gt;MinMaxScaler&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.preprocessing.maxabsscaler#sklearn.preprocessing.MaxAbsScaler&quot;&gt;&lt;code&gt;MaxAbsScaler&lt;/code&gt;&lt;/a&gt;, respectively.</source>
          <target state="translated">另一种标准化方法是将特征缩放到给定的最小值和最大值之间，通常介于零和一之间，或者将每个特征的最大绝对值缩放到单位大小。可以分别使用&lt;a href=&quot;generated/sklearn.preprocessing.minmaxscaler#sklearn.preprocessing.MinMaxScaler&quot;&gt; &lt;code&gt;MinMaxScaler&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;generated/sklearn.preprocessing.maxabsscaler#sklearn.preprocessing.MaxAbsScaler&quot;&gt; &lt;code&gt;MaxAbsScaler&lt;/code&gt; &lt;/a&gt;来实现。</target>
        </trans-unit>
        <trans-unit id="dc1db7ec650725e3eb3f215f1e02c7a5ab5d599a" translate="yes" xml:space="preserve">
          <source>An alternative task, Face Recognition or Face Identification is: given the picture of the face of an unknown person, identify the name of the person by referring to a gallery of previously seen pictures of identified persons.</source>
          <target state="translated">另一个任务,人脸识别或人脸识别是:给定一个未知人物的脸部照片,通过参考以前看到的已识别人物的图片库,识别出这个人的名字。</target>
        </trans-unit>
        <trans-unit id="9f1396eaada67d4771525d15c3b9982306c357b5" translate="yes" xml:space="preserve">
          <source>An alternative to pickling is to export the model to another format using one of the model export tools listed under &lt;a href=&quot;https://scikit-learn.org/0.23/related_projects.html#related-projects&quot;&gt;Related Projects&lt;/a&gt;. Unlike pickling, once exported you cannot recover the full Scikit-learn estimator object, but you can deploy the model for prediction, usually by using tools supporting open model interchange formats such as &lt;a href=&quot;https://onnx.ai/&quot;&gt;ONNX&lt;/a&gt; or &lt;a href=&quot;http://dmg.org/pmml/v4-4/GeneralStructure.html&quot;&gt;PMML&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="63983f26490d9d0dd330cb65d368efc57d509991" translate="yes" xml:space="preserve">
          <source>An application of the different &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;Manifold learning&lt;/a&gt; techniques on a spherical data-set. Here one can see the use of dimensionality reduction in order to gain some intuition regarding the manifold learning methods. Regarding the dataset, the poles are cut from the sphere, as well as a thin slice down its side. This enables the manifold learning techniques to &amp;lsquo;spread it open&amp;rsquo; whilst projecting it onto two dimensions.</source>
          <target state="translated">不同&lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;流形学习&lt;/a&gt;技术在球形数据集上的应用。在这里可以看到降维的使用，以便获得有关流形学习方法的一些直觉。关于数据集，将极点从球体上切下，并将其侧面切成薄片。这使多种学习技术可以&amp;ldquo;展开它&amp;rdquo;，同时将其投影到两个维度上。</target>
        </trans-unit>
        <trans-unit id="7255375783faf14b00594786271748afe79b54ea" translate="yes" xml:space="preserve">
          <source>An approximate solution to the optimal normalized cut may be found via the generalized eigenvalue decomposition of the Laplacian of the graph. Usually this would mean working directly with the Laplacian matrix. If the original data matrix \(A\) has shape \(m \times n\), the Laplacian matrix for the corresponding bipartite graph has shape \((m + n) \times (m + n)\). However, in this case it is possible to work directly with \(A\), which is smaller and more efficient.</source>
          <target state="translated">最佳归一化切割的近似解可以通过图形的拉普拉斯矩阵的广义特征值分解来找到。通常,这意味着直接对拉普拉斯矩阵进行处理。如果原始数据矩阵(A/)的形状为(m \times n\),那么相应的二分图的拉普拉奇矩阵的形状为((m+n)\times (m+n)\)。但是,在这种情况下,可以直接用(A/)来工作,它的体积更小,效率更高。</target>
        </trans-unit>
        <trans-unit id="abda993d7d851644a2cce6e5b3201fc8e649cfdf" translate="yes" xml:space="preserve">
          <source>An approximation to the RBF kernel using random Fourier features.</source>
          <target state="translated">使用随机傅里叶特征对RBF核进行近似。</target>
        </trans-unit>
        <trans-unit id="56451d3e9b79dc1326101c93a26155d78a7c4638" translate="yes" xml:space="preserve">
          <source>An array of arrays of indices of the approximate nearest points from the population matrix that lie within a ball of size &lt;code&gt;radius&lt;/code&gt; around the query points.</source>
          <target state="translated">人口矩阵中最接近的点的索引的数组，这些索引位于查询点周围的大小 &lt;code&gt;radius&lt;/code&gt; 的球内。</target>
        </trans-unit>
        <trans-unit id="69a7cca6f3965cdbc1f2a0f6316fc86cbe697d1b" translate="yes" xml:space="preserve">
          <source>An array of norms along given axis for X. When X is sparse, a NotImplementedError will be raised for norm &amp;lsquo;l1&amp;rsquo; or &amp;lsquo;l2&amp;rsquo;.</source>
          <target state="translated">X沿给定轴的一组规范数组。当X稀疏时，将针对规范'l1'或'l2'引发NotImplementedError。</target>
        </trans-unit>
        <trans-unit id="9eb7e49edfb49a7576a2bc3ab22563bf222603f7" translate="yes" xml:space="preserve">
          <source>An array of points to query</source>
          <target state="translated">要查询的点的数组</target>
        </trans-unit>
        <trans-unit id="deb131dede53f65a17272e64b8336596d48d67e7" translate="yes" xml:space="preserve">
          <source>An array of points to query. Last dimension should match dimension of training data (n_features).</source>
          <target state="translated">一个要查询的点的数组。最后一个维度应该与训练数据的维度(n_features)相匹配。</target>
        </trans-unit>
        <trans-unit id="ce1fbc6a1ce43e2f3a3c1ddf094fbdf1907a173f" translate="yes" xml:space="preserve">
          <source>An array of points to query. Last dimension should match dimension of training data.</source>
          <target state="translated">要查询的点的数组。最后一个维度应该与训练数据的维度相匹配。</target>
        </trans-unit>
        <trans-unit id="f7729a5fd61fb2317d29018126beb8ea2ea03054" translate="yes" xml:space="preserve">
          <source>An array of type np.float</source>
          <target state="translated">np.float类型的数组</target>
        </trans-unit>
        <trans-unit id="9febb5eef4fe704c0e2f76fc17196ae84e5bb310" translate="yes" xml:space="preserve">
          <source>An array with shape (n_samples_X, n_features).</source>
          <target state="translated">形状为(n_samples_X,n_features)的数组。</target>
        </trans-unit>
        <trans-unit id="43ecc2546079acafbd51894522d368c0004da034" translate="yes" xml:space="preserve">
          <source>An array with shape (n_samples_X, n_samples_Y).</source>
          <target state="translated">形状为(n_samples_X,n_samples_Y)的数组。</target>
        </trans-unit>
        <trans-unit id="4e6f0a1fd0fa00a29e1c148218c8f262ec65e6a9" translate="yes" xml:space="preserve">
          <source>An array with shape (n_samples_Y, n_features).</source>
          <target state="translated">形状为(n_samples_Y,n_features)的数组。</target>
        </trans-unit>
        <trans-unit id="55ae6635fc7f12f40e0f75b8c113526f67ceb18f" translate="yes" xml:space="preserve">
          <source>An axis object onto which the plots will be drawn.</source>
          <target state="translated">绘制图形的轴对象。</target>
        </trans-unit>
        <trans-unit id="25ca61d5a0ba319e8f5dd67899dd1b75491f14b8" translate="yes" xml:space="preserve">
          <source>An early approach to taking advantage of this aggregate information was the &lt;em&gt;KD tree&lt;/em&gt; data structure (short for &lt;em&gt;K-dimensional tree&lt;/em&gt;), which generalizes two-dimensional &lt;em&gt;Quad-trees&lt;/em&gt; and 3-dimensional &lt;em&gt;Oct-trees&lt;/em&gt; to an arbitrary number of dimensions. The KD tree is a binary tree structure which recursively partitions the parameter space along the data axes, dividing it into nested orthotropic regions into which data points are filed. The construction of a KD tree is very fast: because partitioning is performed only along the data axes, no \(D\)-dimensional distances need to be computed. Once constructed, the nearest neighbor of a query point can be determined with only \(O[\log(N)]\) distance computations. Though the KD tree approach is very fast for low-dimensional (\(D &amp;lt; 20\)) neighbors searches, it becomes inefficient as \(D\) grows very large: this is one manifestation of the so-called &amp;ldquo;curse of dimensionality&amp;rdquo;. In scikit-learn, KD tree neighbors searches are specified using the keyword &lt;code&gt;algorithm = 'kd_tree'&lt;/code&gt;, and are computed using the class &lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">利用这种聚合信息的早期方法是&lt;em&gt;KD树&lt;/em&gt;数据结构（&lt;em&gt;K维树的&lt;/em&gt;缩写），该结构概括了二维&lt;em&gt;四&lt;/em&gt;&lt;em&gt;叉&lt;/em&gt;&lt;em&gt;树&lt;/em&gt;和3维&lt;em&gt;八&lt;/em&gt;&lt;em&gt;叉&lt;/em&gt;&lt;em&gt;树。&lt;/em&gt;到任意数量的尺寸。 KD树是一种二叉树结构，它沿数据轴递归划分参数空间，将其划分为嵌套的正交各向异性区域，并在其中填充数据点。 KD树的构建非常快：由于仅沿数据轴执行分区，因此无需计算\（D \）维距离。构造完成后，仅可以使用\（O [\ log（N）] \）距离计算来确定查询点的最近邻居。尽管KD树方法对于低维（\（D &amp;lt;20 \））邻居搜索非常快，但是随着\（D \）变得非常大时，它变得无效：这是所谓的&amp;ldquo;维数诅咒&amp;rdquo;的一种体现&amp;rdquo;。在scikit-learn中，使用关键字 &lt;code&gt;algorithm = 'kd_tree'&lt;/code&gt; 指定KD树邻居搜索和使用类&lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt;计算。</target>
        </trans-unit>
        <trans-unit id="099567709025ab0aa48e57d57280b0443b9a24ed" translate="yes" xml:space="preserve">
          <source>An empty dict signifies default parameters.</source>
          <target state="translated">空的dict表示默认参数。</target>
        </trans-unit>
        <trans-unit id="ffa5ed2f9779625aa20adf682c3351ed0a53fb7f" translate="yes" xml:space="preserve">
          <source>An encoding can also be called a &amp;lsquo;character set&amp;rsquo;, but this term is less accurate: several encodings can exist for a single character set.</source>
          <target state="translated">编码也可以称为&amp;ldquo;字符集&amp;rdquo;，但该术语的准确性较差：单个字符集可以存在多种编码。</target>
        </trans-unit>
        <trans-unit id="15c84c56e7d6dd7f29f03b3419a89a1fa5861864" translate="yes" xml:space="preserve">
          <source>An ensemble of totally random trees.</source>
          <target state="translated">一个完全随机的树的集合。</target>
        </trans-unit>
        <trans-unit id="372a9ef151ce0e02d102211d2e14fc577127d782" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predict&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b0193485700595d6695b40a0a211b9e29e959231" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and one of &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt; or &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="215970b6b504c6bfb744566538a8accc41cd92e5" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt;, &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-score&quot;&gt;score&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ed51316796470f5285f1ea5227efd05cf75b44c0" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">实现 &lt;code&gt;fit&lt;/code&gt; 和 &lt;code&gt;predict&lt;/code&gt; 的估计对象。</target>
        </trans-unit>
        <trans-unit id="2c736b1e847c7ee4aecf59b3bb14cb64e436dee2" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;code&gt;fit&lt;/code&gt; and one of &lt;code&gt;decision_function&lt;/code&gt; or &lt;code&gt;predict_proba&lt;/code&gt;.</source>
          <target state="translated">一种估计对象实施 &lt;code&gt;fit&lt;/code&gt; 和一个 &lt;code&gt;decision_function&lt;/code&gt; 或 &lt;code&gt;predict_proba&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="016774ede32e1f200ea316bb37e829ebc5d620e2" translate="yes" xml:space="preserve">
          <source>An estimator object implementing &lt;code&gt;fit&lt;/code&gt;, &lt;code&gt;score&lt;/code&gt; and &lt;code&gt;predict_proba&lt;/code&gt;.</source>
          <target state="translated">实现 &lt;code&gt;fit&lt;/code&gt; ， &lt;code&gt;score&lt;/code&gt; 和 &lt;code&gt;predict_proba&lt;/code&gt; 的估计器对象。</target>
        </trans-unit>
        <trans-unit id="bc6d2e5beaf0995cbcda2b429dbc0e874dd47ce4" translate="yes" xml:space="preserve">
          <source>An estimator object that is used to compute the initial predictions. &lt;code&gt;init&lt;/code&gt; has to provide &lt;a href=&quot;#sklearn.ensemble.GradientBoostingClassifier.fit&quot;&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;#sklearn.ensemble.GradientBoostingClassifier.predict_proba&quot;&gt;&lt;code&gt;predict_proba&lt;/code&gt;&lt;/a&gt;. If &amp;lsquo;zero&amp;rsquo;, the initial raw predictions are set to zero. By default, a &lt;code&gt;DummyEstimator&lt;/code&gt; predicting the classes priors is used.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd1e7319f04194cc6e7a0ff87c048ca39034bdd7" translate="yes" xml:space="preserve">
          <source>An estimator object that is used to compute the initial predictions. &lt;code&gt;init&lt;/code&gt; has to provide &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict&quot;&gt;predict&lt;/a&gt;. If &amp;lsquo;zero&amp;rsquo;, the initial raw predictions are set to zero. By default a &lt;code&gt;DummyEstimator&lt;/code&gt; is used, predicting either the average target value (for loss=&amp;rsquo;ls&amp;rsquo;), or a quantile for the other losses.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3e3cbd1a80e427709b1e224e290f485edaa6e701" translate="yes" xml:space="preserve">
          <source>An estimator object that is used to compute the initial predictions. &lt;code&gt;init&lt;/code&gt; has to provide &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;. If None it uses &lt;code&gt;loss.init_estimator&lt;/code&gt;.</source>
          <target state="translated">一个估计器对象，用于计算初始预测。 &lt;code&gt;init&lt;/code&gt; 必须提供 &lt;code&gt;fit&lt;/code&gt; 和 &lt;code&gt;predict&lt;/code&gt; 。如果为None，则使用 &lt;code&gt;loss.init_estimator&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="cf6861479f811af12e4fa2074ef55363e1ab1784" translate="yes" xml:space="preserve">
          <source>An estimator that has already been &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fitted&quot;&gt;fitted&lt;/a&gt; and is compatible with &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-scorer&quot;&gt;scorer&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8add3305e3de95c19db62ec2f9ba60a9992f6c8c" translate="yes" xml:space="preserve">
          <source>An estimator to inspect.</source>
          <target state="translated">估计员来检查。</target>
        </trans-unit>
        <trans-unit id="0c14b451345a9bebab4624cdfc1eb448ab003b65" translate="yes" xml:space="preserve">
          <source>An example comparing nearest neighbors classification with and without Neighborhood Components Analysis.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0583622ca0751c993f08e9d98041286805dcca3a" translate="yes" xml:space="preserve">
          <source>An example comparing the effect of reconstructing noisy fragments of a raccoon face image using firstly online &lt;a href=&quot;../../modules/decomposition#dictionarylearning&quot;&gt;Dictionary Learning&lt;/a&gt; and various transform methods.</source>
          <target state="translated">一个示例，比较了首先使用在线&lt;a href=&quot;../../modules/decomposition#dictionarylearning&quot;&gt;词典学习&lt;/a&gt;和各种变换方法重建浣熊面部图像的嘈杂片段的效果。</target>
        </trans-unit>
        <trans-unit id="d3ed7be079cbbb4b66d35ed08e7318047e508129" translate="yes" xml:space="preserve">
          <source>An example illustrating the approximation of the feature map of an RBF kernel.</source>
          <target state="translated">一个例子说明了RBF核的特征图的近似。</target>
        </trans-unit>
        <trans-unit id="5f84ab4970b6751b484f351a6364e917ee9e2e2e" translate="yes" xml:space="preserve">
          <source>An example of a chunked operation adhering to this setting is &lt;code&gt;metric.pairwise_distances_chunked&lt;/code&gt;, which facilitates computing row-wise reductions of a pairwise distance matrix.</source>
          <target state="translated">遵循此设置的分块操作的一个示例是 &lt;code&gt;metric.pairwise_distances_chunked&lt;/code&gt; ，它有助于计算成对距离矩阵的按行缩减。</target>
        </trans-unit>
        <trans-unit id="d30da83c344e58d7fad635964b30a7c96a578d3f" translate="yes" xml:space="preserve">
          <source>An example of an estimator is the class &lt;code&gt;sklearn.svm.SVC&lt;/code&gt;, which implements &lt;a href=&quot;https://en.wikipedia.org/wiki/Support_vector_machine&quot;&gt;support vector classification&lt;/a&gt;. The estimator&amp;rsquo;s constructor takes as arguments the model&amp;rsquo;s parameters.</source>
          <target state="translated">估计器的一个示例是 &lt;code&gt;sklearn.svm.SVC&lt;/code&gt; 类，该类实现&lt;a href=&quot;https://en.wikipedia.org/wiki/Support_vector_machine&quot;&gt;支持向量分类&lt;/a&gt;。估计器的构造函数将模型的参数作为参数。</target>
        </trans-unit>
        <trans-unit id="2a28c2a5262858339108957d662b2d9ad76c60fe" translate="yes" xml:space="preserve">
          <source>An example of biclusters formed by partitioning rows and columns.</source>
          <target state="translated">通过分割行和列形成的双簇的例子。</target>
        </trans-unit>
        <trans-unit id="4ec3a210a521488a01f06a7254c4cbcafb1acb47" translate="yes" xml:space="preserve">
          <source>An example of checkerboard biclusters.</source>
          <target state="translated">棋盘双联体的例子。</target>
        </trans-unit>
        <trans-unit id="263d7481d0f6606e447078322b3482f64ed58b85" translate="yes" xml:space="preserve">
          <source>An example of estimating sources from noisy data.</source>
          <target state="translated">一个从噪声数据中估计来源的例子。</target>
        </trans-unit>
        <trans-unit id="ba8a60e5ae0a34a0a9e8c9683b9f2c1dae098cc6" translate="yes" xml:space="preserve">
          <source>An example of reshaping data would be the digits dataset</source>
          <target state="translated">重塑数据的一个例子是数字数据集。</target>
        </trans-unit>
        <trans-unit id="fbe3a4b0f4df558e025b4b6c278ce0b8f2e24f89" translate="yes" xml:space="preserve">
          <source>An example of the HTML output can be seen in the &lt;strong&gt;HTML representation of Pipeline&lt;/strong&gt; section of &lt;a href=&quot;../auto_examples/compose/plot_column_transformer_mixed_types#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py&quot;&gt;Column Transformer with Mixed Types&lt;/a&gt;. As an alternative, the HTML can be written to a file using &lt;a href=&quot;generated/sklearn.utils.estimator_html_repr#sklearn.utils.estimator_html_repr&quot;&gt;&lt;code&gt;estimator_html_repr&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1fbe1528edfe5c2ef6e2542ba5d3a7a740cb4806" translate="yes" xml:space="preserve">
          <source>An example of the same &lt;code&gt;y&lt;/code&gt; in sparse matrix form:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d2d2bef04bd500898cd35385a3db44c487620221" translate="yes" xml:space="preserve">
          <source>An example showing how different online solvers perform on the hand-written digits dataset.</source>
          <target state="translated">一个例子显示了不同的在线求解器在手写数字数据集上的表现。</target>
        </trans-unit>
        <trans-unit id="2aae52ed09d5204233cf6faccc3b62129219ab9a" translate="yes" xml:space="preserve">
          <source>An example showing how the scikit-learn can be used to recognize images of hand-written digits.</source>
          <target state="translated">一个展示如何使用scikit-learn来识别手写数字图像的例子。</target>
        </trans-unit>
        <trans-unit id="dc6fbf3a5e9982b266901310af78a9acc8d8b83c" translate="yes" xml:space="preserve">
          <source>An example showing univariate feature selection.</source>
          <target state="translated">一个显示单变量特征选择的例子。</target>
        </trans-unit>
        <trans-unit id="a63d45f44a041fe05e157072e07ec905aa6168f0" translate="yes" xml:space="preserve">
          <source>An example to compare multi-output regression with random forest and the &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;multioutput.MultiOutputRegressor&lt;/a&gt; meta-estimator.</source>
          <target state="translated">比较随机森林和&lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;multioutput.MultiOutputRegressor&lt;/a&gt;元估计器的多输出回归的示例。</target>
        </trans-unit>
        <trans-unit id="d69827da4fe888278b671529418570646583bc79" translate="yes" xml:space="preserve">
          <source>An example to illustrate multi-output regression with decision tree.</source>
          <target state="translated">以一个例子来说明用决策树进行多结果回归。</target>
        </trans-unit>
        <trans-unit id="278a8759739f9d084f6d58de5359105935a98d5a" translate="yes" xml:space="preserve">
          <source>An example to show covariance estimation with the Mahalanobis distances on Gaussian distributed data.</source>
          <target state="translated">在高斯分布数据上,用马哈兰诺比斯距离来展示协方差估计的例子。</target>
        </trans-unit>
        <trans-unit id="79be1ff44d921e5597feeeddf48473c82478df46" translate="yes" xml:space="preserve">
          <source>An example using &lt;a href=&quot;../../modules/generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt;&lt;code&gt;sklearn.ensemble.IsolationForest&lt;/code&gt;&lt;/a&gt; for anomaly detection.</source>
          <target state="translated">使用&lt;a href=&quot;../../modules/generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt; &lt;code&gt;sklearn.ensemble.IsolationForest&lt;/code&gt; &lt;/a&gt;进行异常检测的示例。</target>
        </trans-unit>
        <trans-unit id="c015cad72773b21af2994319c7e4935c0925533a" translate="yes" xml:space="preserve">
          <source>An example using a one-class SVM for novelty detection.</source>
          <target state="translated">一个使用单类SVM进行新奇度检测的例子。</target>
        </trans-unit>
        <trans-unit id="e3e6ce179701e2cc84a27da4032032e9c54e1a80" translate="yes" xml:space="preserve">
          <source>An extra-trees classifier.</source>
          <target state="translated">一种树外分类器。</target>
        </trans-unit>
        <trans-unit id="402fd1d845155a85adc98aea2772c11c414eb821" translate="yes" xml:space="preserve">
          <source>An extra-trees regressor.</source>
          <target state="translated">一个树外回归器。</target>
        </trans-unit>
        <trans-unit id="36e1ed7c225c31bf0ba3eb4dff97aa286624cf9d" translate="yes" xml:space="preserve">
          <source>An extremely randomized tree classifier.</source>
          <target state="translated">一种极其随机的树形分类器。</target>
        </trans-unit>
        <trans-unit id="2963ff7051490c77b458af39546e6bfce2134347" translate="yes" xml:space="preserve">
          <source>An extremely randomized tree regressor.</source>
          <target state="translated">一个极其随机的树状回归器。</target>
        </trans-unit>
        <trans-unit id="2e305a3161f1f3a1a75895f3bf89737b2fa1110e" translate="yes" xml:space="preserve">
          <source>An illustration of Swiss Roll reduction with locally linear embedding</source>
          <target state="translated">局部线性嵌入的瑞士卷还原图示</target>
        </trans-unit>
        <trans-unit id="ff57c22861af0b46c7c064d86f712f1102614de2" translate="yes" xml:space="preserve">
          <source>An illustration of dimensionality reduction on the S-curve dataset with various manifold learning methods.</source>
          <target state="translated">在S曲线数据集上用各种歧化学习方法进行维度降低的说明。</target>
        </trans-unit>
        <trans-unit id="461c40d4fafb15fd1dbf5dddb7defd80f66220bc" translate="yes" xml:space="preserve">
          <source>An illustration of t-SNE on the two concentric circles and the S-curve datasets for different perplexity values.</source>
          <target state="translated">两个同心圆上的t-SNE和不同困惑值的S曲线数据集的说明。</target>
        </trans-unit>
        <trans-unit id="4217a4a05133f009315b7adf6df3894014f6eea5" translate="yes" xml:space="preserve">
          <source>An illustration of the isotonic regression on generated data. The isotonic regression finds a non-decreasing approximation of a function while minimizing the mean squared error on the training data. The benefit of such a model is that it does not assume any form for the target function such as linearity. For comparison a linear regression is also presented.</source>
          <target state="translated">对生成的数据进行同调回归的说明。等差回归找到了一个函数的非递减近似值,同时使训练数据的均方误差最小化。这种模型的好处是,它不假设目标函数的任何形式,如线性。为了比较,还介绍了一种线性回归。</target>
        </trans-unit>
        <trans-unit id="95f191130b61aa88bf4d9f0c56ed2e78e6bd9498" translate="yes" xml:space="preserve">
          <source>An illustration of the metric and non-metric MDS on generated noisy data.</source>
          <target state="translated">对生成的噪声数据进行度量和非度量MDS的说明。</target>
        </trans-unit>
        <trans-unit id="4ed4596d6f2bc0049548fe72a3bd1528d6ff1a95" translate="yes" xml:space="preserve">
          <source>An illustration of various embeddings on the digits dataset.</source>
          <target state="translated">数字数据集上各种嵌入的说明。</target>
        </trans-unit>
        <trans-unit id="5349a7af33dbb0da98921a0576af67237bdedc22" translate="yes" xml:space="preserve">
          <source>An illustration of various linkage option for agglomerative clustering on a 2D embedding of the digits dataset.</source>
          <target state="translated">在数字数据集的二维嵌入上进行聚类聚类的各种链接选项的说明。</target>
        </trans-unit>
        <trans-unit id="9431c782d6d1945d9109d26f9bdc51c8ca5b4cc9" translate="yes" xml:space="preserve">
          <source>An implementation of a randomized algorithm for principal component analysis A. Szlam et al. 2014</source>
          <target state="translated">主成分分析随机算法的实现 A.Szlam等,2014。</target>
        </trans-unit>
        <trans-unit id="97eec2318d7e6e3549c069ff7f108d7733cec0af" translate="yes" xml:space="preserve">
          <source>An important aspect of performance optimization is also that it can hurt prediction accuracy. Indeed, simpler models (e.g. linear instead of non-linear, or with fewer parameters) often run faster but are not always able to take into account the same exact properties of the data as more complex ones.</source>
          <target state="translated">性能优化的一个重要方面也是它会伤害预测的准确性。事实上,较简单的模型(如线性而非非线性,或参数较少)往往运行得更快,但并不总是能够像更复杂的模型一样考虑到数据的确切属性。</target>
        </trans-unit>
        <trans-unit id="44f31261fda2a48216b1472f9c206633a0f349f1" translate="yes" xml:space="preserve">
          <source>An important notion of robust fitting is that of breakdown point: the fraction of data that can be outlying for the fit to start missing the inlying data.</source>
          <target state="translated">稳健拟合的一个重要概念是击穿点:指拟合开始遗漏的数据中,可以出现偏离的数据的分数。</target>
        </trans-unit>
        <trans-unit id="98153081fcfad9474299ea23738f0ec83b423b23" translate="yes" xml:space="preserve">
          <source>An important question is how can the Dirichlet process use an infinite, unbounded number of clusters and still be consistent. While a full explanation doesn&amp;rsquo;t fit this manual, one can think of its &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process&quot;&gt;stick breaking process&lt;/a&gt; analogy to help understanding it. The stick breaking process is a generative story for the Dirichlet process. We start with a unit-length stick and in each step we break off a portion of the remaining stick. Each time, we associate the length of the piece of the stick to the proportion of points that falls into a group of the mixture. At the end, to represent the infinite mixture, we associate the last remaining piece of the stick to the proportion of points that don&amp;rsquo;t fall into all the other groups. The length of each piece is a random variable with probability proportional to the concentration parameter. Smaller value of the concentration will divide the unit-length into larger pieces of the stick (defining more concentrated distribution). Larger concentration values will create smaller pieces of the stick (increasing the number of components with non zero weights).</source>
          <target state="translated">一个重要的问题是Dirichlet进程如何使用无限，无限制的群集并且仍保持一致。虽然完整的说明不适合本手册，但可以想到其&lt;a href=&quot;https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process&quot;&gt;折断过程&lt;/a&gt;类比以帮助理解它。棒破过程是狄利克雷过程的一个创举。我们从单位长度的棒开始，然后在每个步骤中将剩余的棒的一部分折断。每次，我们将一根棍子的长度与落入一组混合物中的点的比例相关联。最后，为了表示无限的混合，我们将剩余的最后一根棍子与不属于所有其他组的点的比例相关联。每片的长度是一个随机变量，其概率与浓度参数成正比。较小的浓度值会将单位长度分成较大的条状（定义更集中的分布）。较大的浓度值将产生较小的棒块（增加非零重量的组分数量）。</target>
        </trans-unit>
        <trans-unit id="627c954c961a3acd43edae0e16b57eae5630ce43" translate="yes" xml:space="preserve">
          <source>An improvement of the Ledoit-Wolf shrinkage, the &lt;a href=&quot;../../modules/generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;sklearn.covariance.OAS&lt;/code&gt;&lt;/a&gt;, proposed by Chen et al. Its convergence is significantly better under the assumption that the data are Gaussian, in particular for small samples.</source>
          <target state="translated">在Ledoit-狼收缩的改善，&lt;a href=&quot;../../modules/generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt; &lt;code&gt;sklearn.covariance.OAS&lt;/code&gt; &lt;/a&gt;，陈等人提出。在数据是高斯的假设下，尤其是对于小样本，其收敛性要好得多。</target>
        </trans-unit>
        <trans-unit id="370f3995d8ef065bc41012596e70d603a42a06c3" translate="yes" xml:space="preserve">
          <source>An index that selects the retained features from a feature vector. If &lt;code&gt;indices&lt;/code&gt; is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If &lt;code&gt;indices&lt;/code&gt; is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector.</source>
          <target state="translated">从特征向量中选择保留特征的索引。如果 &lt;code&gt;indices&lt;/code&gt; 为False，则这是一个形状为[＃input features]的布尔数组，其中元素为True时（如果已选择其对应的特征进行保留）。如果 &lt;code&gt;indices&lt;/code&gt; 为True，则这是一个形状为[＃输出要素]的整数数组，其值是输入要素向量的索引。</target>
        </trans-unit>
        <trans-unit id="537bad4eab470a047cae67c34b767bf9f3d01c01" translate="yes" xml:space="preserve">
          <source>An instance of the estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d0f36ad8d88245eefc6653aba92b1989e3a7de69" translate="yes" xml:space="preserve">
          <source>An int, giving the exact number of total jobs that are spawned</source>
          <target state="translated">一个int,给出被生成的作业总数的确切数量。</target>
        </trans-unit>
        <trans-unit id="efb853d408e1363bd14470415d5367521c99f9fa" translate="yes" xml:space="preserve">
          <source>An interesting aspect of &lt;a href=&quot;generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt;&lt;code&gt;AgglomerativeClustering&lt;/code&gt;&lt;/a&gt; is that connectivity constraints can be added to this algorithm (only adjacent clusters can be merged together), through a connectivity matrix that defines for each sample the neighboring samples following a given structure of the data. For instance, in the swiss-roll example below, the connectivity constraints forbid the merging of points that are not adjacent on the swiss roll, and thus avoid forming clusters that extend across overlapping folds of the roll.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt; &lt;code&gt;AgglomerativeClustering&lt;/code&gt; &lt;/a&gt;有趣的方面是，可以通过一个连通性矩阵将连通性约束添加到该算法中（只能将相邻的聚类合并在一起），该矩阵为每个样本定义遵循给定数据结构的相邻样本。例如，在下面的瑞士卷示例中，连接性约束禁止合并瑞士卷上不相邻的点，从而避免形成在卷的重叠折叠部分延伸的簇。</target>
        </trans-unit>
        <trans-unit id="edca22917cc04c2cee4dca2af159124718d78594" translate="yes" xml:space="preserve">
          <source>An interesting development of using a &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt; is the ability to perform &lt;a href=&quot;https://en.wikipedia.org/wiki/Out-of-core_algorithm&quot;&gt;out-of-core&lt;/a&gt; scaling. This means that we can learn from data that does not fit into the computer&amp;rsquo;s main memory.</source>
          <target state="translated">使用&lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; 的&lt;/a&gt;一个有趣的发展是可以执行&lt;a href=&quot;https://en.wikipedia.org/wiki/Out-of-core_algorithm&quot;&gt;核外&lt;/a&gt;缩放。这意味着我们可以从不适合计算机主存储器的数据中学习。</target>
        </trans-unit>
        <trans-unit id="e061f170f65c98ddc125ba623de0d484bced6052" translate="yes" xml:space="preserve">
          <source>An introduction to machine learning with scikit-learn</source>
          <target state="translated">使用scikit-learn进行机器学习的介绍。</target>
        </trans-unit>
        <trans-unit id="f8000912a05784b4ed09f166d2281bb382154409" translate="yes" xml:space="preserve">
          <source>An iterable which yields either str, unicode or file objects.</source>
          <target state="translated">一个可以产生str、unicode或文件对象的迭代函数。</target>
        </trans-unit>
        <trans-unit id="ed21dc2777fffee9a18d3be232cdf0f7a675b104" translate="yes" xml:space="preserve">
          <source>An iterable yielding (train, test) splits as arrays of indices.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="26021975f66731cfc83771c3f9a3b8617a224127" translate="yes" xml:space="preserve">
          <source>An iterable yielding train, test splits.</source>
          <target state="translated">一个可迭代的屈服列车,测试分裂。</target>
        </trans-unit>
        <trans-unit id="793ecf7b6fedca1d245cd9de3dfad1789ad435c2" translate="yes" xml:space="preserve">
          <source>An iterable yielding train/test splits.</source>
          <target state="translated">迭代产生的列车/测试分量。</target>
        </trans-unit>
        <trans-unit id="2c74b450a38327bc579731e9a6b693f5a72972a9" translate="yes" xml:space="preserve">
          <source>An object for detecting outliers in a Gaussian distributed dataset.</source>
          <target state="translated">用于检测高斯分布式数据集中的异常值的对象。</target>
        </trans-unit>
        <trans-unit id="bb3e30a61bd34f61e007c1401c3e81f0d43c8edb" translate="yes" xml:space="preserve">
          <source>An object of that type which is cloned for each validation.</source>
          <target state="translated">该类型的对象,每次验证时都会被克隆。</target>
        </trans-unit>
        <trans-unit id="9720a88e8aa2bbe2107788283b8978f9a7cb4b22" translate="yes" xml:space="preserve">
          <source>An object to be used as a cross-validation generator,</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bbffbe09f32930b2800698ca47bd093017747bdf" translate="yes" xml:space="preserve">
          <source>An object to be used as a cross-validation generator.</source>
          <target state="translated">用于交叉验证生成器的对象。</target>
        </trans-unit>
        <trans-unit id="a7002171a59c54b21633748422d618355d53d5f0" translate="yes" xml:space="preserve">
          <source>An optional mask of the image, to consider only part of the pixels.</source>
          <target state="translated">图像的可选遮罩,只考虑部分像素。</target>
        </trans-unit>
        <trans-unit id="fe8123eda49913b2b31a4b7b815dc029043ed233" translate="yes" xml:space="preserve">
          <source>An optional progress meter.</source>
          <target state="translated">一个可选的进度表。</target>
        </trans-unit>
        <trans-unit id="229c2b7073ef4d939c3f4e9dc0933cc0c25d3c6e" translate="yes" xml:space="preserve">
          <source>An optional second feature array. Only allowed if metric != &amp;ldquo;precomputed&amp;rdquo;.</source>
          <target state="translated">可选的第二要素数组。仅当度量！=&amp;ldquo; precomputed&amp;rdquo;时才允许。</target>
        </trans-unit>
        <trans-unit id="0072ba612e2ac9888a715c6d07a5f35671c0b0f4" translate="yes" xml:space="preserve">
          <source>An ordered array of unique labels.</source>
          <target state="translated">一个独特标签的有序数组。</target>
        </trans-unit>
        <trans-unit id="a760a22c84feac040244658e5cd6e733d7fd7a9c" translate="yes" xml:space="preserve">
          <source>An unsupervised transformation of a dataset to a high-dimensional sparse representation. A datapoint is coded according to which leaf of each tree it is sorted into. Using a one-hot encoding of the leaves, this leads to a binary coding with as many ones as there are trees in the forest.</source>
          <target state="translated">一个数据集到高维稀疏表示的无监督转换。一个数据点根据它被分类到的每棵树的哪片叶子进行编码。使用叶子的一热编码,这将导致一个二进制编码,森林里有多少树,就有多少个1。</target>
        </trans-unit>
        <trans-unit id="8bba08ddfbe8b42a7ddb2ba7f5d85088746ededd" translate="yes" xml:space="preserve">
          <source>An upper bound on the fraction of margin errors (see &lt;a href=&quot;../svm#nu-svc&quot;&gt;User Guide&lt;/a&gt;) and a lower bound of the fraction of support vectors. Should be in the interval (0, 1].</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0d82cfe79ef09c873c85764ea87217dd841df582" translate="yes" xml:space="preserve">
          <source>An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1].</source>
          <target state="translated">训练误差分数的上界和支持向量分数的下界。应在区间(0,1]内。</target>
        </trans-unit>
        <trans-unit id="5f9f028f3aaaed07cbca0fd41b78759c2fe2428d" translate="yes" xml:space="preserve">
          <source>An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1]. By default 0.5 will be taken.</source>
          <target state="translated">训练误差分数的上界和支持向量分数的下界。应该在区间(0,1]中。默认取0.5。</target>
        </trans-unit>
        <trans-unit id="c638b2709b286483a90c4415602f89072f7cd76d" translate="yes" xml:space="preserve">
          <source>Analysis of the plots</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="130d90555949c1ba7863adb5fc8aa4d5f4dd611e" translate="yes" xml:space="preserve">
          <source>Analyzing a portion of the ROC curve. McClish, 1989</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b98195adea1583e9b237e74652523f846b2b1879" translate="yes" xml:space="preserve">
          <source>And for multiple metric evaluation, the return value is a dict with the following keys - &lt;code&gt;['test_&amp;lt;scorer1_name&amp;gt;', 'test_&amp;lt;scorer2_name&amp;gt;', 'test_&amp;lt;scorer...&amp;gt;', 'fit_time', 'score_time']&lt;/code&gt;</source>
          <target state="translated">对于多指标评估，返回值是具有以下键的字典- &lt;code&gt;['test_&amp;lt;scorer1_name&amp;gt;', 'test_&amp;lt;scorer2_name&amp;gt;', 'test_&amp;lt;scorer...&amp;gt;', 'fit_time', 'score_time']&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c48696369bc4cd123e13a474e64394fa9533d40a" translate="yes" xml:space="preserve">
          <source>And some work with binary and multilabel (but not multiclass) problems:</source>
          <target state="translated">而且有的还能处理二进制和多标签(但不是多类)问题。</target>
        </trans-unit>
        <trans-unit id="551fc600bc5b2c147bfd7d608b1227c6f0c95818" translate="yes" xml:space="preserve">
          <source>And the L2-normalized tf-idf changes to</source>
          <target state="translated">而L2归一化的tf-idf变化为</target>
        </trans-unit>
        <trans-unit id="fd06a66dae1b69b6eb93cccfb52c33888c5a371c" translate="yes" xml:space="preserve">
          <source>And the classifier &amp;ldquo;predictions&amp;rdquo; are perfect:</source>
          <target state="translated">分类器&amp;ldquo;预测&amp;rdquo;是完美的：</target>
        </trans-unit>
        <trans-unit id="b646eac54bde60aa4d7c89014b7fd028412efa2f" translate="yes" xml:space="preserve">
          <source>Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A conditional entropy-based external cluster evaluation measure</source>
          <target state="translated">Andrew Rosenberg和Julia Hirschberg,2007年。V-Measure。基于条件熵的外部群集评价措施。</target>
        </trans-unit>
        <trans-unit id="5ca338dedf0d8331238e60ad1d0242b94a749529" translate="yes" xml:space="preserve">
          <source>Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and J&amp;ouml;rg Sander. &amp;ldquo;OPTICS: ordering points to identify the clustering structure.&amp;rdquo; ACM SIGMOD Record 28, no. 2 (1999): 49-60.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6e2b1b23fa02a3f4b2a576f98b5ef19fd6944167" translate="yes" xml:space="preserve">
          <source>Another alternative is to take a symmetric version of the k nearest neighbors connectivity matrix of the points.</source>
          <target state="translated">另一种选择是取点的k个最近邻居连接矩阵的对称版本。</target>
        </trans-unit>
        <trans-unit id="c8a5ccc165016bcc8540311d11c1eec6480387f0" translate="yes" xml:space="preserve">
          <source>Another approach is to monitor convergence on a validation score. In this case, the input data is split into a training set and a validation set. The model is then fitted on the training set and the stopping criterion is based on the prediction score computed on the validation set. This enables us to find the least number of iterations which is sufficient to build a model that generalizes well to unseen data and reduces the chance of over-fitting the training data.</source>
          <target state="translated">另一种方法是监测验证分数的收敛性。在这种情况下,输入数据被分割成一个训练集和一个验证集。然后在训练集上拟合模型,停止标准是基于在验证集上计算的预测分数。这样我们就可以找到最少的迭代次数,这足以建立一个对未见数据有很好的泛化能力的模型,减少训练数据过度拟合的机会。</target>
        </trans-unit>
        <trans-unit id="be7184b0e2bd98f85cd552cd8ec12d77ca9e3964" translate="yes" xml:space="preserve">
          <source>Another aspect to consider when choosing a proper algorithm is that not all of them put the same importance on each example over time. Namely, the &lt;code&gt;Perceptron&lt;/code&gt; is still sensitive to badly labeled examples even after many examples whereas the &lt;code&gt;SGD*&lt;/code&gt; and &lt;code&gt;PassiveAggressive*&lt;/code&gt; families are more robust to this kind of artifacts. Conversely, the latter also tend to give less importance to remarkably different, yet properly labeled examples when they come late in the stream as their learning rate decreases over time.</source>
          <target state="translated">选择合适的算法时要考虑的另一个方面是，随着时间的推移，并非所有实例都对每个示例都具有相同的重要性。也就是说，即使在许多示例之后， &lt;code&gt;Perceptron&lt;/code&gt; 仍然对标记错误的示例仍然敏感，而 &lt;code&gt;SGD*&lt;/code&gt; 和 &lt;code&gt;PassiveAggressive*&lt;/code&gt; 系列则对此类工件更为健壮。相反，当学习速度随着时间的流逝而降低时，后者对于趋向于明显不同但正确标记的示例也往往不太重视。</target>
        </trans-unit>
        <trans-unit id="32a1cd2cb87665a11effa4dc4df8f03517ac0638" translate="yes" xml:space="preserve">
          <source>Another common application is to use time information: for instance the groups could be the year of collection of the samples and thus allow for cross-validation against time-based splits.</source>
          <target state="translated">另一种常见的应用是使用时间信息:例如,各组可以是收集样本的年份,从而允许对基于时间的分裂进行交叉验证。</target>
        </trans-unit>
        <trans-unit id="8230731c6a46697977b2ab40afba474c6e0f5aba" translate="yes" xml:space="preserve">
          <source>Another efficient way to perform outlier detection on moderately high dimensional datasets is to use the Local Outlier Factor (LOF) algorithm.</source>
          <target state="translated">另一种对中等高维数据集进行离群值检测的有效方法是使用局部离群值因子(LOF)算法。</target>
        </trans-unit>
        <trans-unit id="421e4566a6c577d45c51d939beed6fdb20866913" translate="yes" xml:space="preserve">
          <source>Another evaluation measure for multi-class classification is macro-averaging, which gives equal weight to the classification of each label.</source>
          <target state="translated">多类分类的另一种评价措施是宏观平均法,即对每个标签的分类给予同等的权重。</target>
        </trans-unit>
        <trans-unit id="9aeb8b5cf9580937985ce741399f76caf7f788f7" translate="yes" xml:space="preserve">
          <source>Another evaluation measure for multi-label classification is macro-averaging, which gives equal weight to the classification of each label.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="103287c3ab74c62412ea0fba62e90af2b9fbac2a" translate="yes" xml:space="preserve">
          <source>Another important metric to care about when sizing production systems is the throughput i.e. the number of predictions you can make in a given amount of time. Here is a benchmark from the &lt;a href=&quot;../auto_examples/applications/plot_prediction_latency#sphx-glr-auto-examples-applications-plot-prediction-latency-py&quot;&gt;Prediction Latency&lt;/a&gt; example that measures this quantity for a number of estimators on synthetic data:</source>
          <target state="translated">调整生产系统规模时要考虑的另一个重要指标是吞吐量，即在给定时间内可以做出的预测数量。以下是&amp;ldquo; &lt;a href=&quot;../auto_examples/applications/plot_prediction_latency#sphx-glr-auto-examples-applications-plot-prediction-latency-py&quot;&gt;预测延迟&amp;rdquo;&lt;/a&gt;示例中的基准，该基准针对合成数据中的许多估计量来测量此数量：</target>
        </trans-unit>
        <trans-unit id="57f765049776b7061ec3ebaa517aed91ea1819db" translate="yes" xml:space="preserve">
          <source>Another option is the &lt;a href=&quot;../../modules/generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;sklearn.impute.IterativeImputer&lt;/code&gt;&lt;/a&gt;. This uses round-robin linear regression, modeling each feature with missing values as a function of other features, in turn. The version implemented assumes Gaussian (output) variables. If your features are obviously non-normal, consider transforming them to look more normal to potentially improve performance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c8eb27e015d30244a13ff2ea0a8b9e91973d57ab" translate="yes" xml:space="preserve">
          <source>Another option is to use an iterable yielding (train, test) splits as arrays of indices, for example:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ebe975675e662edf348908a46736c6c5923bfe28" translate="yes" xml:space="preserve">
          <source>Another possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K, also known as one-hot or dummy encoding. This type of encoding can be obtained with the &lt;a href=&quot;generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;OneHotEncoder&lt;/code&gt;&lt;/a&gt;, which transforms each categorical feature with &lt;code&gt;n_categories&lt;/code&gt; possible values into &lt;code&gt;n_categories&lt;/code&gt; binary features, with one of them 1, and all others 0.</source>
          <target state="translated">将分类特征转换为可与scikit-learn估计器一起使用的特征的另一种可能性是使用K的一，也称为单热或伪编码。可以使用&lt;a href=&quot;generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt; &lt;code&gt;OneHotEncoder&lt;/code&gt; &lt;/a&gt;获得这种类型的编码，该编码器将具有 &lt;code&gt;n_categories&lt;/code&gt; 个可能值的每个分类特征转换为 &lt;code&gt;n_categories&lt;/code&gt; 个二进制特征，其中一个为1，所有其他为0。</target>
        </trans-unit>
        <trans-unit id="be92131a59ab295e72fcba022db0017e8b66e901" translate="yes" xml:space="preserve">
          <source>Another possibility to take into account correlated variables in the dataset, is to estimate sparse coefficients. In some way we already did it manually when we dropped the AGE column in a previous Ridge estimation.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="bd4ca3c02f5c305e57327d7ebc584dba1a900469" translate="yes" xml:space="preserve">
          <source>Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.</source>
          <target state="translated">在tf基础上的另一个改进是对语料库中出现在许多文档中的词进行降权,因此这些词的信息量比那些只出现在语料库中较小部分的词要小。</target>
        </trans-unit>
        <trans-unit id="b069cfdd4eba1168499f693bf3d06a42576bfe6a" translate="yes" xml:space="preserve">
          <source>Another set of biclusters like &lt;code&gt;a&lt;/code&gt;.</source>
          <target state="translated">另一组biclusters的喜欢 &lt;code&gt;a&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="dbed96b9cee766d1a5bef03a5ad136b7f76de1ff" translate="yes" xml:space="preserve">
          <source>Another significant feature involves whether the sender is affiliated with a university, as indicated either by their headers or their signature.</source>
          <target state="translated">另一个重要的特征涉及到发件人是否隶属于某所大学,这可以从他们的标题或签名中看出。</target>
        </trans-unit>
        <trans-unit id="943b31c92155c4448885494636d7164b8dc15821" translate="yes" xml:space="preserve">
          <source>Another strategy to reduce the variance is by subsampling the features analogous to the random splits in &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt; . The number of subsampled features can be controlled via the &lt;code&gt;max_features&lt;/code&gt; parameter.</source>
          <target state="translated">减少方差的另一种策略是通过对与&lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt; &lt;code&gt;RandomForestClassifier&lt;/code&gt; 中&lt;/a&gt;的随机分裂相似的特征进行二次采样。可以通过 &lt;code&gt;max_features&lt;/code&gt; 参数控制子采样特征的数量。</target>
        </trans-unit>
        <trans-unit id="9bb442d3ca7e5d02384e286ef663eab8b5ac3081" translate="yes" xml:space="preserve">
          <source>Another way to compare the curves is to plot them on top of each other. Here, we create a figure with one row and two columns. The axes are passed into the &lt;a href=&quot;../../modules/generated/sklearn.inspection.partialdependencedisplay#sklearn.inspection.PartialDependenceDisplay.plot&quot;&gt;&lt;code&gt;plot&lt;/code&gt;&lt;/a&gt; function as a list, which will plot the partial dependence curves of each model on the same axes. The length of the axes list must be equal to the number of plots drawn.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f7f320421764ca2c9e221a3ad89c0802b9dda00c" translate="yes" xml:space="preserve">
          <source>Another way to reduce memory and computation time is to remove (near-)duplicate points and use &lt;code&gt;sample_weight&lt;/code&gt; instead.</source>
          <target state="translated">减少内存和计算时间的另一种方法是删除（近）重复的点并改用 &lt;code&gt;sample_weight&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c89aea8f1a22bc16a7ca2249f6b1aa681a716bbf" translate="yes" xml:space="preserve">
          <source>Any core sample is part of a cluster, by definition. Any sample that is not a core sample, and is at least &lt;code&gt;eps&lt;/code&gt; in distance from any core sample, is considered an outlier by the algorithm.</source>
          <target state="translated">根据定义，任何核心样本都是集群的一部分。该算法将不是核心样本且与任何核心样本之间的距离至少为 &lt;code&gt;eps&lt;/code&gt; 的任何样本视为异常值。</target>
        </trans-unit>
        <trans-unit id="3a31e829c68185fcae1e17d8f929a1d6c89d47e3" translate="yes" xml:space="preserve">
          <source>Any estimator using the Huber loss would also be robust to outliers, e.g. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;loss='huber'&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="68c9d8de07d4bc7df0a34b129a047212dbcfefbc" translate="yes" xml:space="preserve">
          <source>Any further parameters are passed directly to the distance function. If using a &lt;code&gt;scipy.spatial.distance&lt;/code&gt; metric, the parameters are still metric dependent. See the scipy docs for usage examples.</source>
          <target state="translated">任何其他参数都直接传递给距离函数。如果使用 &lt;code&gt;scipy.spatial.distance&lt;/code&gt; 度量，则参数仍取决于度量。有关用法示例，请参见scipy文档。</target>
        </trans-unit>
        <trans-unit id="38f47dd09163cb40bcb092eb016347a6afcf892a" translate="yes" xml:space="preserve">
          <source>Any further parameters are passed directly to the distance function. If using a scipy.spatial.distance metric, the parameters are still metric dependent. See the scipy docs for usage examples.</source>
          <target state="translated">任何进一步的参数都会直接传递给距离函数。如果使用 scipy.spatial.distance metric,参数仍然依赖于 metric。参见scipy文档中的使用示例。</target>
        </trans-unit>
        <trans-unit id="3ac4cf049edcb6bb9798310f2f664afd413547f9" translate="yes" xml:space="preserve">
          <source>Any further parameters are passed directly to the kernel function.</source>
          <target state="translated">任何进一步的参数都会直接传递给内核函数。</target>
        </trans-unit>
        <trans-unit id="02eaff0fa77b6c45a1e6d5d1d919c0643a037a82" translate="yes" xml:space="preserve">
          <source>Any pairwise distance</source>
          <target state="translated">任何配对距离</target>
        </trans-unit>
        <trans-unit id="b551d3f373e53d945ef845b2e543fa8abdb837a0" translate="yes" xml:space="preserve">
          <source>Any parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, use:</source>
          <target state="translated">构建估计器时提供的任何参数都可以通过这种方式进行优化。具体来说,要找到一个给定估计器的所有参数的名称和当前值,请使用。</target>
        </trans-unit>
        <trans-unit id="767d84a7d28245adee41a36e340241d2beefe72a" translate="yes" xml:space="preserve">
          <source>Apart from a scalar or a single item list, the column selection can be specified as a list of multiple items, an integer array, a slice, a boolean mask, or with a &lt;a href=&quot;generated/sklearn.compose.make_column_selector#sklearn.compose.make_column_selector&quot;&gt;&lt;code&gt;make_column_selector&lt;/code&gt;&lt;/a&gt;. The &lt;a href=&quot;generated/sklearn.compose.make_column_selector#sklearn.compose.make_column_selector&quot;&gt;&lt;code&gt;make_column_selector&lt;/code&gt;&lt;/a&gt; is used to select columns based on data type or column name:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7c69ec44124fc60e723479a565a556b4cda43066" translate="yes" xml:space="preserve">
          <source>Apart from a scalar or a single item list, the column selection can be specified as a list of multiple items, an integer array, a slice, or a boolean mask. Strings can reference columns if the input is a DataFrame, integers are always interpreted as the positional columns.</source>
          <target state="translated">除了标量或单项列表外,还可以将列选择指定为多个项目的列表、整数数组、分片或布尔掩码。如果输入是DataFrame,字符串可以引用列,整数总是被解释为位置列。</target>
        </trans-unit>
        <trans-unit id="4989a2f6df8d0581f9c1d9a7bb746db90f17c597" translate="yes" xml:space="preserve">
          <source>Apple Accelerate and vecLib frameworks (OSX only)</source>
          <target state="translated">Apple Accelerate和vecLib框架(仅限OSX)。</target>
        </trans-unit>
        <trans-unit id="a10c2e4fa481f8ae4b6f36b04c8c93e1d4615114" translate="yes" xml:space="preserve">
          <source>Applications to real world problems with some medium sized datasets or interactive user interface.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="781951ec1e6932c1ea70532655c1b03656c5caff" translate="yes" xml:space="preserve">
          <source>Applies fit_predict of last step in pipeline after transforms.</source>
          <target state="translated">在变换后,应用管道中最后一步的fit_predict。</target>
        </trans-unit>
        <trans-unit id="b55937b5acc0d0ace1d8fc39be387e51d59bfd33" translate="yes" xml:space="preserve">
          <source>Applies fit_transforms of a pipeline to the data, followed by the fit_predict method of the final estimator in the pipeline. Valid only if the final estimator implements fit_predict.</source>
          <target state="translated">将管道的fit_transform应用到数据上,然后是管道中最终估计器的fit_predict方法。只有当最终估计器实现了fit_predict方法时才有效。</target>
        </trans-unit>
        <trans-unit id="e8dcdf95b2627510067599d6b4df4640274e7f9c" translate="yes" xml:space="preserve">
          <source>Applies the learned transformation to the given data.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e21a5ab651c86df31d86409fda574ccc4b1b0811" translate="yes" xml:space="preserve">
          <source>Applies transformers to columns of an array or pandas DataFrame.</source>
          <target state="translated">将变换器应用于数组或pandas DataFrame的列。</target>
        </trans-unit>
        <trans-unit id="bc1fc2d494d882727c783481b882146100e6cdea" translate="yes" xml:space="preserve">
          <source>Apply Term Frequency Inverse Document Frequency normalization to a sparse matrix of occurrence counts.</source>
          <target state="translated">将术语频率反文档频率归一化应用于出现次数的稀疏矩阵。</target>
        </trans-unit>
        <trans-unit id="48ea2e033ed029c8a6ff54b6ec3dbb5f6c117a68" translate="yes" xml:space="preserve">
          <source>Apply a correction to raw Minimum Covariance Determinant estimates.</source>
          <target state="translated">对原始的最小协方差决定因素估计值进行修正。</target>
        </trans-unit>
        <trans-unit id="3438e96e607439758fb53dafc23a0586474578cd" translate="yes" xml:space="preserve">
          <source>Apply a power transform featurewise to make data more Gaussian-like.</source>
          <target state="translated">从特征上应用功率变换,使数据更像高斯。</target>
        </trans-unit>
        <trans-unit id="2abb9abf1584e2411d2ab4707e053695cc0afad4" translate="yes" xml:space="preserve">
          <source>Apply approximate feature map to X.</source>
          <target state="translated">将近似特征图应用于X。</target>
        </trans-unit>
        <trans-unit id="72616a249f3c4471d707b9e1210b174fad823e8d" translate="yes" xml:space="preserve">
          <source>Apply clustering to a projection of the normalized Laplacian.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b5fc36d5b38e5cc48bf65799eb3ba1f5f5dd4c40" translate="yes" xml:space="preserve">
          <source>Apply clustering to a projection to the normalized laplacian.</source>
          <target state="translated">将聚类应用到归一化拉普拉斯的投影上。</target>
        </trans-unit>
        <trans-unit id="8426b1a96ac4159519d28aa77b01e6e7ff1b8a94" translate="yes" xml:space="preserve">
          <source>Apply decision function to an array of samples.</source>
          <target state="translated">将决策函数应用于一个样本数组。</target>
        </trans-unit>
        <trans-unit id="ae4427937ffbc660a10ac85e3e1e095b79e74a1a" translate="yes" xml:space="preserve">
          <source>Apply dimensionality reduction to X using the model.</source>
          <target state="translated">利用模型对X进行降维。</target>
        </trans-unit>
        <trans-unit id="94efd724519284be8c4f7f9c1263433e19686d0b" translate="yes" xml:space="preserve">
          <source>Apply dimensionality reduction to X.</source>
          <target state="translated">对X应用维度还原。</target>
        </trans-unit>
        <trans-unit id="067a30cda0bacc8769ba06b4343f4bb4f1512cf6" translate="yes" xml:space="preserve">
          <source>Apply feature map to X.</source>
          <target state="translated">将特征图应用于X。</target>
        </trans-unit>
        <trans-unit id="019b3cb4563fab68f9b7e6cb2810fdfe13086cd9" translate="yes" xml:space="preserve">
          <source>Apply inverse transformations in reverse order</source>
          <target state="translated">按相反的顺序进行逆向变换</target>
        </trans-unit>
        <trans-unit id="8ba6ffbae8fd06f4432eaf3f6080e23e84a08a61" translate="yes" xml:space="preserve">
          <source>Apply parallel or deflational algorithm for FastICA.</source>
          <target state="translated">应用FastICA的并行或放缩算法。</target>
        </trans-unit>
        <trans-unit id="052f087320408b50bb181caf401c6c09884401a8" translate="yes" xml:space="preserve">
          <source>Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).</source>
          <target state="translated">应用次线性tf缩放,即用1+log(tf)代替tf。</target>
        </trans-unit>
        <trans-unit id="e79510482ec62ac16315b7e449f2ba4c8500bc2e" translate="yes" xml:space="preserve">
          <source>Apply the approximate feature map to X.</source>
          <target state="translated">将近似特征图应用于X。</target>
        </trans-unit>
        <trans-unit id="125fbeef808b6029be65b8119e0b6dcb36461fee" translate="yes" xml:space="preserve">
          <source>Apply the dimension reduction learned on the train data.</source>
          <target state="translated">在训练数据上应用所学的维度减少。</target>
        </trans-unit>
        <trans-unit id="ccff7a762ba43bf734237ef3a3f86ce4e5f76a2a" translate="yes" xml:space="preserve">
          <source>Apply the inverse power transformation using the fitted lambdas.</source>
          <target state="translated">利用拟合的lambdas进行反功率变换。</target>
        </trans-unit>
        <trans-unit id="0977868498dbf8902c5244b7ef40fc62ab65dab8" translate="yes" xml:space="preserve">
          <source>Apply the power transform to each feature using the fitted lambdas.</source>
          <target state="translated">将功率变换应用于每个特征,使用拟合的lambdas。</target>
        </trans-unit>
        <trans-unit id="82b34785a3e300efc01281606b13c116a0d93f34" translate="yes" xml:space="preserve">
          <source>Apply transforms to the data, and predict with the final estimator</source>
          <target state="translated">对数据进行变换,并用最终的估计器进行预测。</target>
        </trans-unit>
        <trans-unit id="ca1ed4363821df5fce03d98ccc01802e4bf47ad5" translate="yes" xml:space="preserve">
          <source>Apply transforms, and decision_function of the final estimator</source>
          <target state="translated">应用变换和最终估计器的decision_function。</target>
        </trans-unit>
        <trans-unit id="2f4f9e2645ee8d058965cbdacfaf2a7eb9788d03" translate="yes" xml:space="preserve">
          <source>Apply transforms, and predict_log_proba of the final estimator</source>
          <target state="translated">应用变换,并对最终估计器进行预测_log_proba。</target>
        </trans-unit>
        <trans-unit id="476ce75d7df4464fc2850d9c9869c6985e6ab19f" translate="yes" xml:space="preserve">
          <source>Apply transforms, and predict_proba of the final estimator</source>
          <target state="translated">应用变换,并预测最终估计器的predict_proba。</target>
        </trans-unit>
        <trans-unit id="4521c8878b7f577e2b39e24b7f82d01d4c53bdb4" translate="yes" xml:space="preserve">
          <source>Apply transforms, and score with the final estimator</source>
          <target state="translated">应用变换,并用最终的估计器进行评分。</target>
        </trans-unit>
        <trans-unit id="d02b9d52a2cce495b36832a6628a7547a7253b34" translate="yes" xml:space="preserve">
          <source>Apply transforms, and score_samples of the final estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef451a675f7839fb36e9b71a5c79c50af524605d" translate="yes" xml:space="preserve">
          <source>Apply transforms, and transform with the final estimator</source>
          <target state="translated">应用变换,并与最终估计器进行变换。</target>
        </trans-unit>
        <trans-unit id="de81ef7ceb9f90b36294e9caf98f17d7f5fb716c" translate="yes" xml:space="preserve">
          <source>Apply trees in the ensemble to X, return leaf indices.</source>
          <target state="translated">将集合中的树应用于X,返回叶指数。</target>
        </trans-unit>
        <trans-unit id="46e9e1565b766b82cfa48c8669ac20be4ae1efca" translate="yes" xml:space="preserve">
          <source>Apply trees in the forest to X, return leaf indices.</source>
          <target state="translated">将森林中的树木应用于X,返回叶指数。</target>
        </trans-unit>
        <trans-unit id="c1f7ec5473437cc6f706dcde780cf75354c5c9db" translate="yes" xml:space="preserve">
          <source>Approximate a kernel map using a subset of the training data.</source>
          <target state="translated">使用训练数据的子集来逼近核图。</target>
        </trans-unit>
        <trans-unit id="99c9230e6f99f216d46b329fe823b6165b309c69" translate="yes" xml:space="preserve">
          <source>Approximate feature map for additive chi2 kernel.</source>
          <target state="translated">加性chi2核的近似特征图。</target>
        </trans-unit>
        <trans-unit id="08c300a2e35faf622b6d14b8c7a89fd4c1146cc4" translate="yes" xml:space="preserve">
          <source>Approximate nearest neighbors in TSNE</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79a233ba78739efc19c54579d317a0c2897cb08a" translate="yes" xml:space="preserve">
          <source>Approximated breakdown point.</source>
          <target state="translated">近似故障点:</target>
        </trans-unit>
        <trans-unit id="80c9ab0b0026778753b68f0b79aed7c300d9ec18" translate="yes" xml:space="preserve">
          <source>Approximates feature map of an RBF kernel by Monte Carlo approximation of its Fourier transform.</source>
          <target state="translated">通过对RBF核的傅里叶变换进行蒙特卡洛逼近来逼近其特征图。</target>
        </trans-unit>
        <trans-unit id="c5cd123a52fffa6abe1c128079596542a14c93e1" translate="yes" xml:space="preserve">
          <source>Approximates feature map of the &amp;ldquo;skewed chi-squared&amp;rdquo; kernel by Monte Carlo approximation of its Fourier transform.</source>
          <target state="translated">通过&amp;ldquo;傅立叶&amp;rdquo;变换的蒙特卡洛近似，近似&amp;ldquo;倾斜的卡方&amp;rdquo;核的特征图。</target>
        </trans-unit>
        <trans-unit id="0747da454303400f203ff73991292f2e6b1d8099" translate="yes" xml:space="preserve">
          <source>Approximations to the Likelihood Gradient. International Conference on Machine Learning (ICML) 2008</source>
          <target state="translated">似然梯度的逼近。2008年机器学习国际会议(ICML)。</target>
        </trans-unit>
        <trans-unit id="3121863aa17e74b4053a8b1b5b5e368203cc9eae" translate="yes" xml:space="preserve">
          <source>Are computed such that:</source>
          <target state="translated">计算出的结果是:</target>
        </trans-unit>
        <trans-unit id="2745debaa64a20eedb49d9f14a0b807c87aa2d2a" translate="yes" xml:space="preserve">
          <source>Area</source>
          <target state="translated">Area</target>
        </trans-unit>
        <trans-unit id="40f1ed31aa37ba1df6f1e12267ca0106d885adfa" translate="yes" xml:space="preserve">
          <source>Area under ROC curve. If None, the roc_auc score is not shown.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef8721938ea54cef3b6761a4de1893a7d8b789aa" translate="yes" xml:space="preserve">
          <source>Area under ROC for the multiclass problem</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9981f730845b36824e5458276d9c866bb62f0b81" translate="yes" xml:space="preserve">
          <source>Area under the precision-recall curve</source>
          <target state="translated">精度-回收曲线下的面积</target>
        </trans-unit>
        <trans-unit id="0d216eb7e93b45a2be7855da027249526cd9509f" translate="yes" xml:space="preserve">
          <source>Argument to the kernel.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="5e08f171a6f1a9519e5708f61bf31cc48538834b" translate="yes" xml:space="preserve">
          <source>Arguments to send to the functional form. If empty and if fun=&amp;rsquo;logcosh&amp;rsquo;, fun_args will take value {&amp;lsquo;alpha&amp;rsquo; : 1.0}.</source>
          <target state="translated">要发送给函数形式的参数。如果为空且fun ='logcosh'，则fun_args的值为{'alpha'：1.0}。</target>
        </trans-unit>
        <trans-unit id="c45228c98edaaa951ea722f923e4cffb69731e7a" translate="yes" xml:space="preserve">
          <source>Ariel Sharon</source>
          <target state="translated">Ariel Sharon</target>
        </trans-unit>
        <trans-unit id="11f09cc06ef4f6b2c8293917d6e2b9c293adc607" translate="yes" xml:space="preserve">
          <source>Array 1 for distance computation.</source>
          <target state="translated">阵列1用于距离计算。</target>
        </trans-unit>
        <trans-unit id="2b2aa765e7ec61bcc3223e5c31148344ebd52c37" translate="yes" xml:space="preserve">
          <source>Array 2 for distance computation.</source>
          <target state="translated">阵列2用于距离计算。</target>
        </trans-unit>
        <trans-unit id="8a65824231356d07c95aeae6f87001027ea0bef6" translate="yes" xml:space="preserve">
          <source>Array containing labels.</source>
          <target state="translated">包含标签的阵列。</target>
        </trans-unit>
        <trans-unit id="e6449394cdaf60dea5ac71fdd09a0ed9e9fcf7ee" translate="yes" xml:space="preserve">
          <source>Array containing numbers whose mean is desired. If &lt;code&gt;a&lt;/code&gt; is not an array, a conversion is attempted.</source>
          <target state="translated">包含期望平均值的数字的数组。如果 &lt;code&gt;a&lt;/code&gt; 不是数组，则尝试进行转换。</target>
        </trans-unit>
        <trans-unit id="c018e704aa26bba356828629d984b5289beaf058" translate="yes" xml:space="preserve">
          <source>Array containing pairwise preference constraints (qid in svmlight format).</source>
          <target state="translated">包含成对偏好约束的数组(svmlight格式的qid)。</target>
        </trans-unit>
        <trans-unit id="91335d05435a126405d2a8a45dca985aad48295a" translate="yes" xml:space="preserve">
          <source>Array containing points.</source>
          <target state="translated">包含点数的阵列。</target>
        </trans-unit>
        <trans-unit id="6f0a003e95ad4e5813fec46743d334c40fd183c4" translate="yes" xml:space="preserve">
          <source>Array dimensions of training vector &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e25bba3f8d7bf2293b0ad16bc13aa9e306ca9914" translate="yes" xml:space="preserve">
          <source>Array mapping from feature integer indices to feature name</source>
          <target state="translated">从特征整数指数到特征名称的阵列映射。</target>
        </trans-unit>
        <trans-unit id="ea79422025037006dbb9fa56f6952fdafa7b797f" translate="yes" xml:space="preserve">
          <source>Array mapping from feature integer indices to feature name.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="70e68dbf5038082528f259bb78c1b52974cdd1b7" translate="yes" xml:space="preserve">
          <source>Array of C i.e. inverse of regularization parameter values used for cross-validation.</source>
          <target state="translated">C的数组,即用于交叉验证的正则化参数值的倒数。</target>
        </trans-unit>
        <trans-unit id="f23db680f590249d8c9bf389de4a77cfbca412ab" translate="yes" xml:space="preserve">
          <source>Array of C that maps to the best scores across every class. If refit is set to False, then for each class, the best C is the average of the C&amp;rsquo;s that correspond to the best scores for each fold. &lt;code&gt;C_&lt;/code&gt; is of shape(n_classes,) when the problem is binary.</source>
          <target state="translated">C数组，它映射到每个班级的最高分数。如果将refit设置为False，则对于每个类别，最佳C是与每个折叠的最佳得分相对应的C的平均值。问题为二进制时， &lt;code&gt;C_&lt;/code&gt; 的形状为（n_classes，）。</target>
        </trans-unit>
        <trans-unit id="a55c3729a06f5322394066f1c0c8cd702750ec36" translate="yes" xml:space="preserve">
          <source>Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to &lt;code&gt;1 / (2C)&lt;/code&gt; in other linear models such as &lt;a href=&quot;sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ee7705f66f6136357dc8015abf939ac9ba5f0a41" translate="yes" xml:space="preserve">
          <source>Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to &lt;code&gt;1 / (2C)&lt;/code&gt; in other linear models such as &lt;a href=&quot;sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;&lt;/a&gt;. If using generalized cross-validation, alphas must be positive.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1bcf7c58e5cd52bc0017cb778f5c70ae9fb7f4c3" translate="yes" xml:space="preserve">
          <source>Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to &lt;code&gt;C^-1&lt;/code&gt; in other linear models such as LogisticRegression or LinearSVC.</source>
          <target state="translated">要尝试的Alpha值数组。正则强度；必须为正浮点数。正则化改善了问题的条件，并减少了估计的方差。较大的值表示更强的正则化。Alpha对应于其他线性模型（例如LogisticRegression或LinearSVC）中的 &lt;code&gt;C^-1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="56fbe430ec1266429078a0ab631106976ff42e91" translate="yes" xml:space="preserve">
          <source>Array of feature names.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f66e9a562c0b4e0d27e475880657f80a6deb9514" translate="yes" xml:space="preserve">
          <source>Array of feature-wise means to update with the new data X.</source>
          <target state="translated">用新数据X更新的特征手段数组。</target>
        </trans-unit>
        <trans-unit id="f6c98b4462671d9e778e0d714eb0f164a1614fcd" translate="yes" xml:space="preserve">
          <source>Array of feature-wise var to update with the new data X.</source>
          <target state="translated">要用新数据X更新的特征变量数组。</target>
        </trans-unit>
        <trans-unit id="5fcd28abe38a5e4606c96fde73b0772080b7a651" translate="yes" xml:space="preserve">
          <source>Array of images from which to extract patches. For color images, the last dimension specifies the channel: a RGB image would have &lt;code&gt;n_channels=3&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6f81a8f931ec6cc551fbd84a217b7d3ae7ae6320" translate="yes" xml:space="preserve">
          <source>Array of indices to be used in a subsample. Can be of length less than n_samples in the case of a subsample, or equal to n_samples in the case of a bootstrap subsample with repeated indices. If None, the sample weight will be calculated over the full sample. Only &amp;ldquo;balanced&amp;rdquo; is supported for class_weight if this is provided.</source>
          <target state="translated">子样本中使用的索引数组。对于子样本，长度可以小于n_samples；对于具有重复索引的引导子样本，长度可以等于n_samples。如果为None，则将在整个样本中计算样本重量。如果提供了class_weight，则仅支持&amp;ldquo; balanced&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="68493ef46dcb2bca2edbdc205fc9d64a4faedf4e" translate="yes" xml:space="preserve">
          <source>Array of l1_ratio that maps to the best scores across every class. If refit is set to False, then for each class, the best l1_ratio is the average of the l1_ratio&amp;rsquo;s that correspond to the best scores for each fold. &lt;code&gt;l1_ratio_&lt;/code&gt; is of shape(n_classes,) when the problem is binary.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fac2540408435b364a56995dfc908b1ce3b3f1b5" translate="yes" xml:space="preserve">
          <source>Array of l1_ratios used for cross-validation. If no l1_ratio is used (i.e. penalty is not &amp;lsquo;elasticnet&amp;rsquo;), this is set to &lt;code&gt;[None]&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eeab30110685755d1d6fcfd1bdec963e11fd2b40" translate="yes" xml:space="preserve">
          <source>Array of labels assigned to the input data. if partial_fit is used instead of fit, they are assigned to the last batch of data.</source>
          <target state="translated">如果使用partial_fit而不是fit,它们被分配给最后一批数据。</target>
        </trans-unit>
        <trans-unit id="7713344316e6072a30843b91363d87cccda00c35" translate="yes" xml:space="preserve">
          <source>Array of matplotlib axes. &lt;code&gt;None&lt;/code&gt; if &lt;code&gt;include_values&lt;/code&gt; is false.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d4370a66f6bded85adaf7548951f4179c0716e58" translate="yes" xml:space="preserve">
          <source>Array of modal values.</source>
          <target state="translated">模态值阵列。</target>
        </trans-unit>
        <trans-unit id="1fd74761ce4c1dfdbf88f10700afe7876dc7373c" translate="yes" xml:space="preserve">
          <source>Array of ordered feature names used in the dataset.</source>
          <target state="translated">数据集中使用的有序特征名称阵列。</target>
        </trans-unit>
        <trans-unit id="0f885b177d721fef81c6f0296028811159f9ce61" translate="yes" xml:space="preserve">
          <source>Array of original class labels per sample.</source>
          <target state="translated">每个样本的原始类标签数组。</target>
        </trans-unit>
        <trans-unit id="7800a77bf571d12be284509e1c1715411139d56e" translate="yes" xml:space="preserve">
          <source>Array of original class labels per sample;</source>
          <target state="translated">每个样本的原始类标签数组。</target>
        </trans-unit>
        <trans-unit id="96778e504b9d48e677b9fd575a4cafc7576e9a4e" translate="yes" xml:space="preserve">
          <source>Array of pairwise distances between samples, or a feature array.</source>
          <target state="translated">样本之间的对偶距离数组,或一个特征数组。</target>
        </trans-unit>
        <trans-unit id="e091a26de25f0600efda87ce52273e245aa786c4" translate="yes" xml:space="preserve">
          <source>Array of pairwise kernels between samples, or a feature array.</source>
          <target state="translated">样本之间的对偶核数组,或一个特征数组。</target>
        </trans-unit>
        <trans-unit id="7ad46daa8cb15b6370dc0bc3d5a37d6d0d152fc8" translate="yes" xml:space="preserve">
          <source>Array of positive distances. If vertex i is connected to vertex j, then dist_matrix[i,j] gives the distance between the vertices. If vertex i is not connected to vertex j, then dist_matrix[i,j] = 0</source>
          <target state="translated">正距离数组。如果顶点i与顶点j相连,那么dist_matrix[i,j]给出顶点之间的距离。如果顶点i与顶点j不相连,那么dist_matrix[i,j]=0。</target>
        </trans-unit>
        <trans-unit id="4e5f1b979067f81092a1e7cd9ffe302fc627e739" translate="yes" xml:space="preserve">
          <source>Array of precomputed feature-wise values to use for scaling.</source>
          <target state="translated">用于缩放的预先计算的特征值的阵列。</target>
        </trans-unit>
        <trans-unit id="6812a29eee4afd325221734d77b9a4dae2cd6936" translate="yes" xml:space="preserve">
          <source>Array of precomputed sample-wise values to use for scaling.</source>
          <target state="translated">用于缩放的预先计算的样本值的阵列。</target>
        </trans-unit>
        <trans-unit id="fdeed78489293ac715c2ab2eb4b0404b2b3869aa" translate="yes" xml:space="preserve">
          <source>Array of samples (test vectors).</source>
          <target state="translated">样本阵列(测试向量)。</target>
        </trans-unit>
        <trans-unit id="24f920c86b265a90a5e0d4a36297a04667e3e9df" translate="yes" xml:space="preserve">
          <source>Array of samples/test vectors.</source>
          <target state="translated">样本/测试向量阵列。</target>
        </trans-unit>
        <trans-unit id="3fbc39a14b9b4da5186fad432e1588423be425d4" translate="yes" xml:space="preserve">
          <source>Array of scores of the estimator for each run of the cross validation.</source>
          <target state="translated">每一次交叉验证的估计器的分数阵列。</target>
        </trans-unit>
        <trans-unit id="45ad4c3a294e4989b4c11ac5e758294b0026c988" translate="yes" xml:space="preserve">
          <source>Array of shape (Nx, D), representing Nx points in D dimensions.</source>
          <target state="translated">形状(Nx,D)的阵列,代表D维中的Nx点。</target>
        </trans-unit>
        <trans-unit id="fc8b99cd3247136d0c5bc8738933867c6b7310fe" translate="yes" xml:space="preserve">
          <source>Array of shape (Ny, D), representing Ny points in D dimensions. If not specified, then Y=X.</source>
          <target state="translated">形状(Ny,D)数组,代表D维中的Ny个点,如果没有指定,则Y=X。如果没有指定,则Y=X。</target>
        </trans-unit>
        <trans-unit id="250d74905474bfb993e7135f2cc0cab46c1b40f6" translate="yes" xml:space="preserve">
          <source>Array of the classes occurring in the data, as given by &lt;code&gt;np.unique(y_org)&lt;/code&gt; with &lt;code&gt;y_org&lt;/code&gt; the original class labels.</source>
          <target state="translated">数据中出现的类的数组，由 &lt;code&gt;np.unique(y_org)&lt;/code&gt; 和 &lt;code&gt;y_org&lt;/code&gt; 原始类标签给出。</target>
        </trans-unit>
        <trans-unit id="b78e425c6e49838f94af51fc89b925b7dde837d3" translate="yes" xml:space="preserve">
          <source>Array of weighted counts for each mode.</source>
          <target state="translated">每个模式的加权计数数组。</target>
        </trans-unit>
        <trans-unit id="94dd9c0571a23f9d9a0e09a5e6b6a6f23711b3ea" translate="yes" xml:space="preserve">
          <source>Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.</source>
          <target state="translated">分配给各个样本的权重数组。如果没有提供,则给每个样本分配单位权重。</target>
        </trans-unit>
        <trans-unit id="2d23bb743e4774802fe069cba46f38744d42d230" translate="yes" xml:space="preserve">
          <source>Array representing the cosine distances to each point, only present if return_distance=True.</source>
          <target state="translated">代表每个点余弦距离的数组,只有当return_distance=True时才会出现。</target>
        </trans-unit>
        <trans-unit id="a5f32f9aefd1234d15dbae57ece52e2602c42baf" translate="yes" xml:space="preserve">
          <source>Array representing the distances to each point, only present if return_distance=True. The distance values are computed according to the &lt;code&gt;metric&lt;/code&gt; constructor parameter.</source>
          <target state="translated">表示到每个点的距离的数组，仅当return_distance = True时才存在。距离值是根据 &lt;code&gt;metric&lt;/code&gt; 构造函数参数计算的。</target>
        </trans-unit>
        <trans-unit id="dadd9836b98c2a8bfba00660688e29f8956840ae" translate="yes" xml:space="preserve">
          <source>Array representing the lengths to points, only present if return_distance=True</source>
          <target state="translated">代表点的长度的数组,只有当return_distance=True时才会出现。</target>
        </trans-unit>
        <trans-unit id="128b0965caa89fb141c0f27357f3b344c8ae14e0" translate="yes" xml:space="preserve">
          <source>Array with class_weight_vect[i] the weight for i-th class</source>
          <target state="translated">数组中的class_weight_vect[i]是第i个类的权重。</target>
        </trans-unit>
        <trans-unit id="8071ea6e98367794e2540e75731701074b3502c7" translate="yes" xml:space="preserve">
          <source>Array with sample weights as applied to the original y</source>
          <target state="translated">应用于原始y的样本权重阵列</target>
        </trans-unit>
        <trans-unit id="454c01b5a8f0e04bcddbd63e1a11312ba378a0a5" translate="yes" xml:space="preserve">
          <source>Arrays containing points.</source>
          <target state="translated">含有点的数组。</target>
        </trans-unit>
        <trans-unit id="55e1c09be8a72c22c42432b49cce52b8593ad9f3" translate="yes" xml:space="preserve">
          <source>Arrays containing points. Respective shapes (n_samples1, n_features) and (n_samples2, n_features)</source>
          <target state="translated">包含点的数组。各自的形状 (n_samples1,n_features)和 (n_samples2,n_features)</target>
        </trans-unit>
        <trans-unit id="617153f408d9e36a02ce1e1b9ef6e01cb4b4ace4" translate="yes" xml:space="preserve">
          <source>Arrays for storing tree data, index, node data and node bounds.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c2af9f8f5cb14e6db46f51362b9496e9213d67c" translate="yes" xml:space="preserve">
          <source>Art B. Owen (2006), A robust hybrid of lasso and ridge regression. &lt;a href=&quot;http://statweb.stanford.edu/~owen/reports/hhu.pdf&quot;&gt;http://statweb.stanford.edu/~owen/reports/hhu.pdf&lt;/a&gt;</source>
          <target state="translated">Art B. Owen（2006），套索和岭回归的强大混合体。&lt;a href=&quot;http://statweb.stanford.edu/~owen/reports/hhu.pdf&quot;&gt;http://statweb.stanford.edu/~owen/reports/hhu.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7c8d50e29fabbeb579c5217b8bfc0480f3d25842" translate="yes" xml:space="preserve">
          <source>Art B. Owen (2006), A robust hybrid of lasso and ridge regression. &lt;a href=&quot;https://statweb.stanford.edu/~owen/reports/hhu.pdf&quot;&gt;https://statweb.stanford.edu/~owen/reports/hhu.pdf&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="280c547a1f162abeda960694ff094a09bc3e0f0d" translate="yes" xml:space="preserve">
          <source>As &lt;code&gt;RobustScaler&lt;/code&gt;, &lt;code&gt;QuantileTransformer&lt;/code&gt; is robust to outliers in the sense that adding or removing outliers in the training set will yield approximately the same transformation on held out data. But contrary to &lt;code&gt;RobustScaler&lt;/code&gt;, &lt;code&gt;QuantileTransformer&lt;/code&gt; will also automatically collapse any outlier by setting them to the a priori defined range boundaries (0 and 1).</source>
          <target state="translated">作为 &lt;code&gt;RobustScaler&lt;/code&gt; ， &lt;code&gt;QuantileTransformer&lt;/code&gt; 对异常值具有鲁棒性，因为在训练集中添加或删除异常值将对保留的数据产生大致相同的变换。但是与 &lt;code&gt;RobustScaler&lt;/code&gt; 相反， &lt;code&gt;QuantileTransformer&lt;/code&gt; 还将通过将它们设置为先验定义的范围边界（0和1）来自动折叠任何异常值。</target>
        </trans-unit>
        <trans-unit id="4f9342a94a131883760ca784477b6aa2e1e17246" translate="yes" xml:space="preserve">
          <source>As &lt;code&gt;StandardScaler&lt;/code&gt;, &lt;code&gt;MinMaxScaler&lt;/code&gt; is very sensitive to the presence of outliers.</source>
          <target state="translated">作为 &lt;code&gt;StandardScaler&lt;/code&gt; ， &lt;code&gt;MinMaxScaler&lt;/code&gt; 对异常值的存在非常敏感。</target>
        </trans-unit>
        <trans-unit id="0473784855cee607ac3c3e8942e446aa86c45018" translate="yes" xml:space="preserve">
          <source>As &lt;code&gt;leaf_size&lt;/code&gt; increases, the memory required to store a tree structure decreases. This is especially important in the case of ball tree, which stores a \(D\)-dimensional centroid for each node. The required storage space for &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; is approximately &lt;code&gt;1 / leaf_size&lt;/code&gt; times the size of the training set.</source>
          <target state="translated">随着 &lt;code&gt;leaf_size&lt;/code&gt; 的增加，存储树结构所需的内存减少。这在球树的情况下尤其重要，球树为每个节点存储一个\（D \）维质心。&lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt;所需的存储空间约为训练集大小的 &lt;code&gt;1 / leaf_size&lt;/code&gt; 乘以。</target>
        </trans-unit>
        <trans-unit id="5ba99528590cd9030f3ddfaa7772e7d00f041704" translate="yes" xml:space="preserve">
          <source>As F-test captures only linear dependency, it rates x_1 as the most discriminative feature. On the other hand, mutual information can capture any kind of dependency between variables and it rates x_2 as the most discriminative feature, which probably agrees better with our intuitive perception for this example. Both methods correctly marks x_3 as irrelevant.</source>
          <target state="translated">由于F检验只捕捉到线性依赖性,所以它将x_1评为最具判别力的特征。另一方面,相互信息可以捕捉到变量之间的任何一种依赖性,它将x_2评为最有辨别力的特征,这可能更符合我们对这个例子的直观感受。两种方法都正确地将x_3标记为不相关。</target>
        </trans-unit>
        <trans-unit id="d26c8ba867b91d66c4620ebdebaef0da4c712fd2" translate="yes" xml:space="preserve">
          <source>As \(\nu\rightarrow\infty\), the Mat&amp;eacute;rn kernel converges to the RBF kernel. When \(\nu = 1/2\), the Mat&amp;eacute;rn kernel becomes identical to the absolute exponential kernel, i.e.,</source>
          <target state="translated">随着\（\ nu \ rightarrow \ infty \），Mat&amp;eacute;rn内核收敛到RBF内核。当\（\ nu = 1/2 \）时，Mat&amp;eacute;rn内核与绝对指数内核相同，即</target>
        </trans-unit>
        <trans-unit id="fd1774dd2550566b72e9349a744134b65f1d5b37" translate="yes" xml:space="preserve">
          <source>As \(k\) becomes large compared to \(N\), the ability to prune branches in a tree-based query is reduced. In this situation, Brute force queries can be more efficient.</source>
          <target state="translated">当/(k/)与/(N/)相比变得很大时,在基于树的查询中修剪分支的能力就会降低。在这种情况下,蛮力查询可以更有效。</target>
        </trans-unit>
        <trans-unit id="faf1e694d41950e191ce698208593222026d0616" translate="yes" xml:space="preserve">
          <source>As a general rule, most authors, and empirical evidence, suggest that 5- or 10- fold cross validation should be preferred to LOO.</source>
          <target state="translated">作为一般规则,大多数作者和经验证据表明,5或10折交叉验证应优于LOO。</target>
        </trans-unit>
        <trans-unit id="7511794679052faea86272ecd7141a948a5ee019" translate="yes" xml:space="preserve">
          <source>As a rule of thumb you can consider that if the sparsity ratio is greater than 90% you can probably benefit from sparse formats. Check Scipy&amp;rsquo;s sparse matrix formats &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;documentation&lt;/a&gt; for more information on how to build (or convert your data to) sparse matrix formats. Most of the time the &lt;code&gt;CSR&lt;/code&gt; and &lt;code&gt;CSC&lt;/code&gt; formats work best.</source>
          <target state="translated">根据经验，您可以考虑如果稀疏率大于90％，则可能会受益于稀疏格式。有关如何构建（或将数据转换为）稀疏矩阵格式的更多信息，请参见Scipy的稀疏矩阵格式&lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;文档&lt;/a&gt;。大多数时候， &lt;code&gt;CSR&lt;/code&gt; 和 &lt;code&gt;CSC&lt;/code&gt; 格式效果最好。</target>
        </trans-unit>
        <trans-unit id="e6d544b03b9c4badced861e6939e6f09869b8c86" translate="yes" xml:space="preserve">
          <source>As a rule of thumb you can consider that if the sparsity ratio is greater than 90% you can probably benefit from sparse formats. Check Scipy&amp;rsquo;s sparse matrix formats &lt;a href=&quot;https://docs.scipy.org/doc/scipy/reference/sparse.html&quot;&gt;documentation&lt;/a&gt; for more information on how to build (or convert your data to) sparse matrix formats. Most of the time the &lt;code&gt;CSR&lt;/code&gt; and &lt;code&gt;CSC&lt;/code&gt; formats work best.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="353bdb2ad3c8962f2cd7a3693251833639b2a66a" translate="yes" xml:space="preserve">
          <source>As a stochastic method, the loss function is not necessarily decreasing at each iteration, and convergence is only guaranteed in expectation. For this reason, monitoring the convergence on the loss function can be difficult.</source>
          <target state="translated">作为一种随机方法,损失函数在每次迭代时不一定是递减的,只有在期望值上保证收敛。由于这个原因,监测损失函数的收敛性是很困难的。</target>
        </trans-unit>
        <trans-unit id="42def8933b75f8dc81489a17ca6bf2a4d3b1aab6" translate="yes" xml:space="preserve">
          <source>As a user, you may control the backend that joblib will use (regardless of what scikit-learn recommends) by using a context manager:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c5c1051c685f7a17c464794391d76e887c975059" translate="yes" xml:space="preserve">
          <source>As alpha tends toward zero the coefficients found by Ridge regression stabilize towards the randomly sampled vector w. For big alpha (strong regularisation) the coefficients are smaller (eventually converging at 0) leading to a simpler and biased solution. These dependencies can be observed on the left plot.</source>
          <target state="translated">当α趋向于零时,Ridge回归发现的系数会稳定在随机采样向量w的方向上。这些依赖性可以在左图中观察到。</target>
        </trans-unit>
        <trans-unit id="24f37f11c718f612ad3749aae9a0a774bc0cfd6c" translate="yes" xml:space="preserve">
          <source>As an alternative, the permutation importances of &lt;code&gt;rf&lt;/code&gt; are computed on a held out test set. This shows that the low cardinality categorical feature, &lt;code&gt;sex&lt;/code&gt; is the most important feature.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3c04824c2f2c674faf4d7d29dd0f1d9a774a897e" translate="yes" xml:space="preserve">
          <source>As an example, consider a word-level natural language processing task that needs features extracted from &lt;code&gt;(token, part_of_speech)&lt;/code&gt; pairs. One could use a Python generator function to extract features:</source>
          <target state="translated">例如，考虑一个单词级自然语言处理任务，该任务需要从 &lt;code&gt;(token, part_of_speech)&lt;/code&gt; 对中提取特征。可以使用Python生成器函数提取功能：</target>
        </trans-unit>
        <trans-unit id="440e47d9bdf54f0d76b208827e3f8a7246ce9187" translate="yes" xml:space="preserve">
          <source>As an example, suppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is given by</source>
          <target state="translated">举个例子,假设我们有一个具有布尔特征的数据集,我们希望删除所有在80%以上的样本中为1或0(开启或关闭)的特征。布尔特征是伯努利随机变量,这种变量的方差由以下公式给出</target>
        </trans-unit>
        <trans-unit id="0fc331775bcbccb582b3c6648f3387e8f638bc0a" translate="yes" xml:space="preserve">
          <source>As an iterable of string metrics::</source>
          <target state="translated">作为字符串度量的可迭代:。</target>
        </trans-unit>
        <trans-unit id="670a68b39de7d2dc4b153165f015b7c7235ba10c" translate="yes" xml:space="preserve">
          <source>As an optimization problem, binary class L2 penalized logistic regression minimizes the following cost function:</source>
          <target state="translated">作为一个优化问题,二元类L2惩罚性逻辑回归最小化以下成本函数。</target>
        </trans-unit>
        <trans-unit id="f278c77eb641c2fca3187e677f3037bfd6ce972e" translate="yes" xml:space="preserve">
          <source>As an optimization problem, binary class \(\ell_2\) penalized logistic regression minimizes the following cost function:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a0923a21facc4895041c36b66fbffdd3ccd57707" translate="yes" xml:space="preserve">
          <source>As currently implemented, Dijkstra&amp;rsquo;s algorithm does not work for graphs with direction-dependent distances when directed == False. i.e., if dist_matrix[i,j] and dist_matrix[j,i] are not equal and both are nonzero, method=&amp;rsquo;D&amp;rsquo; will not necessarily yield the correct result.</source>
          <target state="translated">按照当前的实现，当有向== False时，Dijkstra的算法不适用于方向与距离相关的图。即，如果dist_matrix [i，j]和dist_matrix [j，i]不相等且两者都不为零，则method ='D'不一定会产生正确的结果。</target>
        </trans-unit>
        <trans-unit id="18ae6c61fa89653926b85f1ce590dca96bbcbbc7" translate="yes" xml:space="preserve">
          <source>As described on the original website:</source>
          <target state="translated">如原网站上所述。</target>
        </trans-unit>
        <trans-unit id="5aa4aad7dfb43ef2769419ae7f460971c9284dbe" translate="yes" xml:space="preserve">
          <source>As described previously, the most widely used distance function is the squared Frobenius norm, which is an obvious extension of the Euclidean norm to matrices:</source>
          <target state="translated">如前所述,最广泛使用的距离函数是Frobenius平方法则,它是欧几里得法则在矩阵上的明显扩展。</target>
        </trans-unit>
        <trans-unit id="e81ebe44fbcb1c67bb2bf8d6d2417ec1c47e735b" translate="yes" xml:space="preserve">
          <source>As expected the confusion matrix shows that posts from the newsgroups on atheism and Christianity are more often confused for one another than with computer graphics.</source>
          <target state="translated">正如预期的那样,混淆矩阵显示,来自新闻组的关于无神论和基督教的帖子,比起计算机图形,更多的是相互混淆。</target>
        </trans-unit>
        <trans-unit id="a21f305b83f6ecc940d012c4c2d2b2809b12f48f" translate="yes" xml:space="preserve">
          <source>As expected, &lt;code&gt;VarianceThreshold&lt;/code&gt; has removed the first column, which has a probability \(p = 5/6 &amp;gt; .8\) of containing a zero.</source>
          <target state="translated">不出所料， &lt;code&gt;VarianceThreshold&lt;/code&gt; 删除了第一列，该列很有可能\（p = 5/6&amp;gt; .8 \）包含零。</target>
        </trans-unit>
        <trans-unit id="0a862c171ad9c6c90137a8961c4a49a8109f123e" translate="yes" xml:space="preserve">
          <source>As expected, the dummy regressor is unable to correctly rank the samples and therefore performs the worst on this plot.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1461782e678132cdda4c11cb8a9f0f36fdc4dc12" translate="yes" xml:space="preserve">
          <source>As expected, the plot suggests that 3 features are informative, while the remaining are not.</source>
          <target state="translated">正如预期的那样,该图表明,有3个特征是有信息的,而其余的特征则没有。</target>
        </trans-unit>
        <trans-unit id="21dea3d956b97f59f6554850f2e02a18d3e3f037" translate="yes" xml:space="preserve">
          <source>As for the &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt;&lt;code&gt;Normalizer&lt;/code&gt;&lt;/a&gt;, the utility class &lt;a href=&quot;generated/sklearn.preprocessing.binarizer#sklearn.preprocessing.Binarizer&quot;&gt;&lt;code&gt;Binarizer&lt;/code&gt;&lt;/a&gt; is meant to be used in the early stages of &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;. The &lt;code&gt;fit&lt;/code&gt; method does nothing as each sample is treated independently of others:</source>
          <target state="translated">至于&lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt; &lt;code&gt;Normalizer&lt;/code&gt; &lt;/a&gt;，实用程序类&lt;a href=&quot;generated/sklearn.preprocessing.binarizer#sklearn.preprocessing.Binarizer&quot;&gt; &lt;code&gt;Binarizer&lt;/code&gt; &lt;/a&gt;打算在&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt;的早期阶段使用。由于每个样品都独立对待，因此 &lt;code&gt;fit&lt;/code&gt; 方法无济于事：</target>
        </trans-unit>
        <trans-unit id="448fad632016e4e86e34b67a2dd205dc93981193" translate="yes" xml:space="preserve">
          <source>As for the &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt;&lt;code&gt;Normalizer&lt;/code&gt;&lt;/a&gt; classes, the preprocessing module provides a companion function &lt;a href=&quot;generated/sklearn.preprocessing.binarize#sklearn.preprocessing.binarize&quot;&gt;&lt;code&gt;binarize&lt;/code&gt;&lt;/a&gt; to be used when the transformer API is not necessary.</source>
          <target state="translated">对于&lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt; &lt;code&gt;Normalizer&lt;/code&gt; &lt;/a&gt;类，预处理模块提供了一个配套函数&lt;a href=&quot;generated/sklearn.preprocessing.binarize#sklearn.preprocessing.binarize&quot;&gt; &lt;code&gt;binarize&lt;/code&gt; &lt;/a&gt;值化，以便在不需要转换器API时使用。</target>
        </trans-unit>
        <trans-unit id="7e4621ab54b3c13fd3d03c5dfae345e68a0fd95a" translate="yes" xml:space="preserve">
          <source>As in &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt;&lt;code&gt;IncrementalPCA&lt;/code&gt;&lt;/a&gt; centers but does not scale the input data for each feature before applying the SVD.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="79a9833b8f532d00a6b35f7523ca2a620de3d115" translate="yes" xml:space="preserve">
          <source>As in the classification setting, the fit method will take as argument arrays X and y, only that in this case y is expected to have floating point values instead of integer values:</source>
          <target state="translated">如同在分类设置中一样,拟合方法将把数组X和y作为参数,只是在这种情况下,y的值应该是浮点值而不是整数值。</target>
        </trans-unit>
        <trans-unit id="357549c786d11a83e9ad1e4e4484c10d048f813a" translate="yes" xml:space="preserve">
          <source>As is shown in the result before discretization, linear model is fast to build and relatively straightforward to interpret, but can only model linear relationships, while decision tree can build a much more complex model of the data. One way to make linear model more powerful on continuous data is to use discretization (also known as binning). In the example, we discretize the feature and one-hot encode the transformed data. Note that if the bins are not reasonably wide, there would appear to be a substantially increased risk of overfitting, so the discretizer parameters should usually be tuned under cross validation.</source>
          <target state="translated">从离散化之前的结果可以看出,线性模型的建立速度快,解释起来也相对简单,但只能建立线性关系的模型,而决策树可以建立更复杂的数据模型。让线性模型在连续数据上变得更强大的一个方法是使用离散化(也就是分片)。在本例中,我们对特征进行离散化,并对转换后的数据进行一热编码。需要注意的是,如果分众不合理,似乎会大大增加过拟合的风险,所以通常应该在交叉验证下对分众参数进行调整。</target>
        </trans-unit>
        <trans-unit id="67535506d22839df286d2c74a169c907b47bc78e" translate="yes" xml:space="preserve">
          <source>As mentioned above, we can interpret LDA as assigning \(x\) to the class whose mean \(\mu_k\) is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities. Alternatively, LDA is equivalent to first &lt;em&gt;sphering&lt;/em&gt; the data so that the covariance matrix is the identity, and then assigning \(x\) to the closest mean in terms of Euclidean distance (still accounting for the class priors).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4d80681701a6bd5948db2415d41ef49851684994" translate="yes" xml:space="preserve">
          <source>As mentioned in the introduction, the total claim amount per unit of exposure can be modeled as the product of the prediction of the frequency model by the prediction of the severity model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9026eb9b1171dae37e620c4611084cbf5214d244" translate="yes" xml:space="preserve">
          <source>As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are zeros (typically more than 99% of them).</source>
          <target state="translated">由于大多数文档通常会使用语料库中使用的单词的一个很小的子集,因此产生的矩阵将有许多特征值为零(通常超过99%)。</target>
        </trans-unit>
        <trans-unit id="80903649b427f38994df0cc2bd896c257816e4cc" translate="yes" xml:space="preserve">
          <source>As neighboring data points are more likely to lie within the same leaf of a tree, the transformation performs an implicit, non-parametric density estimation.</source>
          <target state="translated">由于相邻的数据点更有可能位于一棵树的同一片叶子内,因此变换执行了隐式的、非参数化的密度估计。</target>
        </trans-unit>
        <trans-unit id="604835d70709f3a91a0d5148ef02789e712875c2" translate="yes" xml:space="preserve">
          <source>As neither of these datasets have missing values, we will remove some values to create new versions with artificially missing data. The performance of &lt;a href=&quot;../../modules/generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt;&lt;code&gt;RandomForestRegressor&lt;/code&gt;&lt;/a&gt; on the full original dataset is then compared the performance on the altered datasets with the artificially missing values imputed using different techniques.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4b3b8dbd06a75124d0b25055322ec22921d45503" translate="yes" xml:space="preserve">
          <source>As noted above, for small sample sizes a brute force search can be more efficient than a tree-based query. This fact is accounted for in the ball tree and KD tree by internally switching to brute force searches within leaf nodes. The level of this switch can be specified with the parameter &lt;code&gt;leaf_size&lt;/code&gt;. This parameter choice has many effects:</source>
          <target state="translated">如上所述，对于小样本量，蛮力搜索可能比基于树的查询更有效。在球树和KD树中，可以通过内部切换到叶节点内的蛮力搜索来解决这个问题。可以使用参数 &lt;code&gt;leaf_size&lt;/code&gt; 指定此开关的级别。此参数选择具有许多效果：</target>
        </trans-unit>
        <trans-unit id="02cd1e4aacf73905da270451ea593d54a1eead69" translate="yes" xml:space="preserve">
          <source>As other classifiers, &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt;&lt;code&gt;NuSVC&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; take as input two arrays: an array &lt;code&gt;X&lt;/code&gt; of shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt; holding the training samples, and an array &lt;code&gt;y&lt;/code&gt; of class labels (strings or integers), of shape &lt;code&gt;(n_samples)&lt;/code&gt;:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="480621b1ef230a82c36b7031ee3539b5a66ef9c0" translate="yes" xml:space="preserve">
          <source>As other classifiers, &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt;&lt;code&gt;NuSVC&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; take as input two arrays: an array X of size &lt;code&gt;[n_samples,
n_features]&lt;/code&gt; holding the training samples, and an array y of class labels (strings or integers), size &lt;code&gt;[n_samples]&lt;/code&gt;:</source>
          <target state="translated">与其他分类器一样，&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt; &lt;code&gt;NuSVC&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;将两个数组作为输入：大小为 &lt;code&gt;[n_samples, n_features]&lt;/code&gt; 的数组X 存放训练样本，以及类别标签（字符串或整数）的数组y，大小为 &lt;code&gt;[n_samples]&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="7f73865c72a9e8a8b8d3c39d4c4f128f641ec5e7" translate="yes" xml:space="preserve">
          <source>As other classifiers, SGD has to be fitted with two arrays: an array &lt;code&gt;X&lt;/code&gt; of shape (n_samples, n_features) holding the training samples, and an array y of shape (n_samples,) holding the target values (class labels) for the training samples:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="86b96557b2fb9eb69b558f788f743c67cbdf526d" translate="yes" xml:space="preserve">
          <source>As other classifiers, SGD has to be fitted with two arrays: an array X of size [n_samples, n_features] holding the training samples, and an array Y of size [n_samples] holding the target values (class labels) for the training samples:</source>
          <target state="translated">与其他分类器一样,SGD必须用两个数组来拟合:一个大小为[n_samples,n_features]的数组X持有训练样本,另一个大小为[n_samples]的数组Y持有训练样本的目标值(类标签)。</target>
        </trans-unit>
        <trans-unit id="ba8715d103d1eabb437e2b429e94c0804a44a2eb" translate="yes" xml:space="preserve">
          <source>As other classifiers, forest classifiers have to be fitted with two arrays: a sparse or dense array X of size &lt;code&gt;[n_samples, n_features]&lt;/code&gt; holding the training samples, and an array Y of size &lt;code&gt;[n_samples]&lt;/code&gt; holding the target values (class labels) for the training samples:</source>
          <target state="translated">与其他分类器一样，森林分类器必须配备两个数组：大小为 &lt;code&gt;[n_samples, n_features]&lt;/code&gt; 的稀疏或密集数组X ，用于存放训练样本；大小为 &lt;code&gt;[n_samples]&lt;/code&gt; 的数组Y，用于存储目标值（类标签）训练样本：</target>
        </trans-unit>
        <trans-unit id="9d7e042981f1b86e720810a559a0b5f85e715465" translate="yes" xml:space="preserve">
          <source>As said above (see &amp;ldquo;&lt;a href=&quot;#the-pipeline&quot;&gt;The machine-learning pipeline&lt;/a&gt;&amp;rdquo;), we could also choose to scale numerical values before training the model. This can be useful to apply a similar amount regularization to all of them in the Ridge. The preprocessor is redefined in order to subtract the mean and scale variables to unit variance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d9c002345c84e7b31630185ef2325d2598748527" translate="yes" xml:space="preserve">
          <source>As scikit-learn relies heavily on Numpy/Scipy and linear algebra in general it makes sense to take explicit care of the versions of these libraries. Basically, you ought to make sure that Numpy is built using an optimized &lt;a href=&quot;https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms&quot;&gt;BLAS&lt;/a&gt; / &lt;a href=&quot;https://en.wikipedia.org/wiki/LAPACK&quot;&gt;LAPACK&lt;/a&gt; library.</source>
          <target state="translated">由于scikit-learn在很大程度上依赖于Numpy / Scipy和线性代数，因此，有必要特别注意这些库的版本。基本上，您应该确保Numpy是使用优化的&lt;a href=&quot;https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms&quot;&gt;BLAS&lt;/a&gt; / &lt;a href=&quot;https://en.wikipedia.org/wiki/LAPACK&quot;&gt;LAPACK&lt;/a&gt;库构建的。</target>
        </trans-unit>
        <trans-unit id="52ae38c76153b785a0e48dca405bc79c3254bacc" translate="yes" xml:space="preserve">
          <source>As seen previously, the dataset contains columns with different data types and we need to apply a specific preprocessing for each data types. In particular categorical variables cannot be included in linear model if not coded as integers first. In addition, to avoid categorical features to be treated as ordered values, we need to one-hot-encode them. Our pre-processor will</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="de312b28ec8e48929f9dedcb0b6804aa5a471fe2" translate="yes" xml:space="preserve">
          <source>As shown below, t-SNE for higher perplexities finds meaningful topology of two concentric circles, however the size and the distance of the circles varies slightly from the original. Contrary to the two circles dataset, the shapes visually diverge from S-curve topology on the S-curve dataset even for larger perplexity values.</source>
          <target state="translated">如下图所示,较高plexities的t-SNE发现了两个同心圆的有意义的拓扑结构,然而圆的大小和距离与原来略有不同。与两个圆数据集相反,即使是较大的plexity值,在S-curve数据集上的形状也与S-curve拓扑有视觉上的差异。</target>
        </trans-unit>
        <trans-unit id="d368728d274b369ac777e6745357cba862711ccd" translate="yes" xml:space="preserve">
          <source>As such variance is dataset dependent, R&amp;sup2; may not be meaningfully comparable across different datasets. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R&amp;sup2; score of 0.0.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="8731c43edea0fd3a0536e8161228e2825929062b" translate="yes" xml:space="preserve">
          <source>As tf&amp;ndash;idf is very often used for text features, there is also another class called &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;TfidfVectorizer&lt;/code&gt;&lt;/a&gt; that combines all the options of &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; in a single model:</source>
          <target state="translated">由于tf&amp;ndash;idf通常用于文本功能，因此还有另一个名为&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;TfidfVectorizer&lt;/code&gt; 的&lt;/a&gt;类，它将&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; 的&lt;/a&gt;所有选项组合在一个模型中：</target>
        </trans-unit>
        <trans-unit id="9ec60e63b98738f893cbc703b7a17df4dbce6a3b" translate="yes" xml:space="preserve">
          <source>As the Earth is nearly spherical, the haversine formula provides a good approximation of the distance between two points of the Earth surface, with a less than 1% error on average.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff6adce24795bb660ac39f8e682709377eb07fe5" translate="yes" xml:space="preserve">
          <source>As the Lasso regression yields sparse models, it can thus be used to perform feature selection, as detailed in &lt;a href=&quot;feature_selection#l1-feature-selection&quot;&gt;L1-based feature selection&lt;/a&gt;.</source>
          <target state="translated">由于Lasso回归会产生稀疏模型，因此可以将其用于执行特征选择，如&lt;a href=&quot;feature_selection#l1-feature-selection&quot;&gt;基于L1的特征选择中所述&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="dc66d7007c269247f1b0a3bd5e17ce48735d32b4" translate="yes" xml:space="preserve">
          <source>As the algorithm tries to balance the volume (ie balance the region sizes), if we take circles with different sizes, the segmentation fails.</source>
          <target state="translated">由于算法试图平衡体积(即平衡区域大小),如果我们取不同大小的圆,分割失败。</target>
        </trans-unit>
        <trans-unit id="ca54f3bcc5c655eebc87fa446a643204f651e5fe" translate="yes" xml:space="preserve">
          <source>As the ground truth is known here, we also apply different cluster quality metrics to judge the goodness of fit of the cluster labels to the ground truth.</source>
          <target state="translated">由于这里已经知道了地面真相,我们还应用不同的聚类质量指标来判断聚类标签与地面真相的拟合度好坏。</target>
        </trans-unit>
        <trans-unit id="26f19c73cbdda7d655c1c419bc1b28d6aee89730" translate="yes" xml:space="preserve">
          <source>As the negative of a distance, this kernel is only conditionally positive definite.</source>
          <target state="translated">作为距离的负数,这个核只有条件正定。</target>
        </trans-unit>
        <trans-unit id="74315005f7ce6ac1c1dafe71dd19e66293fa501a" translate="yes" xml:space="preserve">
          <source>As the prior on the weights is a Gaussian prior, the histogram of the estimated weights is Gaussian.</source>
          <target state="translated">由于权重的先验是高斯先验,所以估计权重的直方图是高斯的。</target>
        </trans-unit>
        <trans-unit id="f78b7e4ef3de6f9908e43081399b599b6d846fba" translate="yes" xml:space="preserve">
          <source>As this algorithm maximizes only the likelihood, it will not bias the means towards zero, or bias the cluster sizes to have specific structures that might or might not apply.</source>
          <target state="translated">由于这种算法只最大化可能性,所以不会将平均值偏向于零,也不会将聚类大小偏向于具有可能或可能不适用的特定结构。</target>
        </trans-unit>
        <trans-unit id="492cebfc31b9e0ae2de0bb7669534987ce58057c" translate="yes" xml:space="preserve">
          <source>As usual the best way to adjust the feature extraction parameters is to use a cross-validated grid search, for instance by pipelining the feature extractor with a classifier:</source>
          <target state="translated">通常情况下,调整特征提取参数的最好方法是使用交叉验证的网格搜索,例如将特征提取器与分类器进行管道化。</target>
        </trans-unit>
        <trans-unit id="ea78a4c7988b659783feac98fcd695de89f8d61f" translate="yes" xml:space="preserve">
          <source>As we have seen, every estimator exposes a &lt;code&gt;score&lt;/code&gt; method that can judge the quality of the fit (or the prediction) on new data. &lt;strong&gt;Bigger is better&lt;/strong&gt;.</source>
          <target state="translated">正如我们所看到的，每次估计暴露了 &lt;code&gt;score&lt;/code&gt; ，可以判断出适合的新数据的质量（或预测）方法。&lt;strong&gt;越大越好&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="fff4f5f3ab6438d863abba19bbb7c007a060f360" translate="yes" xml:space="preserve">
          <source>As we&amp;rsquo;ll see, some cross-validation objects do specific things with labeled data, others behave differently with grouped data, and others do not use this information.</source>
          <target state="translated">正如我们将看到的，一些交叉验证对象对带有标签的数据执行特定的操作，另一些对分组数据的行为有所不同，而另一些则不使用此信息。</target>
        </trans-unit>
        <trans-unit id="c2a5e94d01ebd06105b2b62c25b4b493ea944024" translate="yes" xml:space="preserve">
          <source>As with &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt;&lt;code&gt;scale&lt;/code&gt;&lt;/a&gt;, the module further provides convenience functions &lt;a href=&quot;generated/sklearn.preprocessing.minmax_scale#sklearn.preprocessing.minmax_scale&quot;&gt;&lt;code&gt;minmax_scale&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.preprocessing.maxabs_scale#sklearn.preprocessing.maxabs_scale&quot;&gt;&lt;code&gt;maxabs_scale&lt;/code&gt;&lt;/a&gt; if you don&amp;rsquo;t want to create an object.</source>
          <target state="translated">与&lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt; &lt;code&gt;scale&lt;/code&gt; 一样&lt;/a&gt;，如果您不想创建对象，该模块还将提供便捷功能&lt;a href=&quot;generated/sklearn.preprocessing.minmax_scale#sklearn.preprocessing.minmax_scale&quot;&gt; &lt;code&gt;minmax_scale&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.preprocessing.maxabs_scale#sklearn.preprocessing.maxabs_scale&quot;&gt; &lt;code&gt;maxabs_scale&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c97bbdc8d450f400aad05e1727632d201bf17f90" translate="yes" xml:space="preserve">
          <source>As with classification classes, the fit method will take as argument vectors X, y, only that in this case y is expected to have floating point values instead of integer values:</source>
          <target state="translated">与分类类一样,拟合方法将把向量X,y作为参数,只是在这种情况下,y有望具有浮点值而不是整数值。</target>
        </trans-unit>
        <trans-unit id="320d01bbc29ff09708f415fa50fd9da6e6972d95" translate="yes" xml:space="preserve">
          <source>As with other classifiers, &lt;a href=&quot;generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt;&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;&lt;/a&gt; takes as input two arrays: an array X, sparse or dense, of size &lt;code&gt;[n_samples, n_features]&lt;/code&gt; holding the training samples, and an array Y of integer values, size &lt;code&gt;[n_samples]&lt;/code&gt;, holding the class labels for the training samples:</source>
          <target state="translated">与其他分类器一样，&lt;a href=&quot;generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt; &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; &lt;/a&gt;将两个数组作为输入：稀疏或密集的数组X，其大小为 &lt;code&gt;[n_samples, n_features]&lt;/code&gt; 存放训练样本；以及数组Y，其整数值，大小为 &lt;code&gt;[n_samples]&lt;/code&gt; ，用于保存以下项的类标签：训练样本：</target>
        </trans-unit>
        <trans-unit id="211544b1f9de9dd9d971b4d12a5c9dd1ee84dd6a" translate="yes" xml:space="preserve">
          <source>As with other linear models, &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt; will take in its &lt;code&gt;fit&lt;/code&gt; method arrays X, y and will store the coefficients \(w\) of the linear model in its &lt;code&gt;coef_&lt;/code&gt; member:</source>
          <target state="translated">与其他线性模型一样，&lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt;将采用其 &lt;code&gt;fit&lt;/code&gt; 方法数组X，y，并将线性模型的系数\（w \）存储在其 &lt;code&gt;coef_&lt;/code&gt; 成员中：</target>
        </trans-unit>
        <trans-unit id="0550c97cec757ba988ca448a8879bb1fa8d6048a" translate="yes" xml:space="preserve">
          <source>As you can imagine, if one extracts such a context around each individual word of a corpus of documents the resulting matrix will be very wide (many one-hot-features) with most of them being valued to zero most of the time. So as to make the resulting data structure able to fit in memory the &lt;code&gt;DictVectorizer&lt;/code&gt; class uses a &lt;code&gt;scipy.sparse&lt;/code&gt; matrix by default instead of a &lt;code&gt;numpy.ndarray&lt;/code&gt;.</source>
          <target state="translated">可以想象，如果围绕文档语料库的每个单词提取这样的上下文，则生成的矩阵将非常宽（许多&amp;ldquo;一键通&amp;rdquo;功能），其中大多数在大多数情况下都为零。为了使结果数据结构能够适合内存， &lt;code&gt;DictVectorizer&lt;/code&gt; 类默认使用 &lt;code&gt;scipy.sparse&lt;/code&gt; 矩阵而不是 &lt;code&gt;numpy.ndarray&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="bf1b1999c59fa9b3c0bf1aeb80e6b35e3cde33f4" translate="yes" xml:space="preserve">
          <source>As you can see, by default the KFold cross-validation iterator does not take either datapoint class or group into consideration. We can change this by using the &lt;code&gt;StratifiedKFold&lt;/code&gt; like so.</source>
          <target state="translated">如您所见，默认情况下，KFold交叉验证迭代器不考虑数据点类或组。我们可以像这样使用 &lt;code&gt;StratifiedKFold&lt;/code&gt; 来改变它。</target>
        </trans-unit>
        <trans-unit id="4243f9f7d96f83f5f8fe40416e149534c5c3a7c1" translate="yes" xml:space="preserve">
          <source>As you can see, it is a challenging task: after all, the images are of poor resolution. Do you agree with the classifier?</source>
          <target state="translated">正如你所看到的,这是一项具有挑战性的任务:毕竟,这些图像的分辨率很低。你同意这个分类器吗?</target>
        </trans-unit>
        <trans-unit id="91d172a6c45b01ca0f06c000f8b26183873978d6" translate="yes" xml:space="preserve">
          <source>As you can see, it returns [[0.5]], and [[2]], which means that the element is at distance 0.5 and is the third element of samples (indexes start at 0). You can also query for multiple points:</source>
          <target state="translated">如你所见,它返回[[0.5]],和[[2]],这意味着元素在距离0.5,是样本的第三个元素(索引从0开始)。你也可以查询多个点。</target>
        </trans-unit>
        <trans-unit id="285f08a9d27e1e1ebbb2de47dbd27bd9e8a8b2c7" translate="yes" xml:space="preserve">
          <source>As you can see, the &lt;code&gt;[1, 0]&lt;/code&gt; is comfortably classified as &lt;code&gt;1&lt;/code&gt; since the first two samples are ignored due to their sample weights.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1273827574a8e654ba4e8fa3b455e8b899058288" translate="yes" xml:space="preserve">
          <source>Ash</source>
          <target state="translated">Ash</target>
        </trans-unit>
        <trans-unit id="7e00bdd748db07e532eed9a3a30221202b903645" translate="yes" xml:space="preserve">
          <source>Ash:</source>
          <target state="translated">Ash:</target>
        </trans-unit>
        <trans-unit id="fa6467e4d61dfb46a998615f44d406b657f3e65c" translate="yes" xml:space="preserve">
          <source>Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).</source>
          <target state="translated">给训练集的任何文档中出现的每个词分配一个固定的整数id(例如通过从词到整数索引建立一个字典)。</target>
        </trans-unit>
        <trans-unit id="07fb5452be9437f82662fee13a0f5824d7cc38d4" translate="yes" xml:space="preserve">
          <source>Assign biclusters from one set to another in a one-to-one fashion to maximize the sum of their similarities. This step is performed using the Hungarian algorithm.</source>
          <target state="translated">以一对一的方式将双簇从一个集合分配到另一个集合,以最大化它们的相似性之和。这一步骤使用匈牙利算法进行。</target>
        </trans-unit>
        <trans-unit id="341c86e4b7fe6ff7ce7735c5402c926cbe3cd37f" translate="yes" xml:space="preserve">
          <source>Assume that there are no ties in y_score (which is likely to be the case if y_score is continuous) for efficiency gains.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eeab86fb8cce55c225e83b0d2dc18f408531cb78" translate="yes" xml:space="preserve">
          <source>Assume two label assignments (of the same N objects), \(U\) and \(V\). Their entropy is the amount of uncertainty for a partition set, defined by:</source>
          <target state="translated">假设有两个标签分配(相同的N个对象),分别为:①(U\)和②(V\)。它们的熵是分割集的不确定性量,定义如下:</target>
        </trans-unit>
        <trans-unit id="0e5d6eebf8d779ccbfdee109f1d6bfd1baee97c2" translate="yes" xml:space="preserve">
          <source>Assuming that some data is Independent and Identically Distributed (i.i.d.) is making the assumption that all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples.</source>
          <target state="translated">假设一些数据是独立同分布(i.i.d.),就是假设所有的样本都来自同一个生成过程,并且假设生成过程对过去生成的样本没有记忆。</target>
        </trans-unit>
        <trans-unit id="fc888006cf2fe36cad38d794da87b3aa0beb9fc6" translate="yes" xml:space="preserve">
          <source>At each stage the decision tree \(h_m(x)\) is chosen to minimize the loss function \(L\) given the current model \(F_{m-1}\) and its fit \(F_{m-1}(x_i)\)</source>
          <target state="translated">在每个阶段,根据当前模型(F_{m-1})及其拟合度(F_{m-1}(x_i)/),选择决策树(h_m(x)/)以最小化损失函数(L/)。</target>
        </trans-unit>
        <trans-unit id="bfe84c1effa1f2d8d0a150671e591a10f6f5331c" translate="yes" xml:space="preserve">
          <source>At first, a linear model will be applied on the original targets. Due to the non-linearity, the model trained will not be precise during the prediction. Subsequently, a logarithmic function is used to linearize the targets, allowing better prediction even with a similar linear model as reported by the median absolute error (MAE).</source>
          <target state="translated">起初,将在原始目标上应用线性模型。由于非线性,在预测过程中,所训练的模型将不精确。随后,使用对数函数对目标进行线性化处理,即使使用类似的线性模型,也能获得更好的预测效果,如中位绝对误差(MAE)报告。</target>
        </trans-unit>
        <trans-unit id="3103892102cc58ce3e902fe26301aa542c051586" translate="yes" xml:space="preserve">
          <source>At fitting time, one binary classifier per bit in the code book is fitted. At prediction time, the classifiers are used to project new points in the class space and the class closest to the points is chosen.</source>
          <target state="translated">在拟合时,对码本中每个位的二元分类器进行拟合。在预测时,用分类器在类空间中预测新的点,并选择最接近点的类。</target>
        </trans-unit>
        <trans-unit id="a0707d969d20d6e3136e641e1fe81f003f9dea63" translate="yes" xml:space="preserve">
          <source>At learning time, this simply consists in learning one regressor or binary classifier per class. In doing so, one needs to convert multi-class labels to binary labels (belong or does not belong to the class). LabelBinarizer makes this process easy with the transform method.</source>
          <target state="translated">在学习时,简单来说就是每个类学习一个回归器或二进制分类器。在这样做的时候,需要将多类标签转换为二进制标签(属于或不属于该类)。LabelBinarizer通过转换方法让这个过程变得简单。</target>
        </trans-unit>
        <trans-unit id="934e61570171dfc9a4fde1dc5a30d65977fe4f97" translate="yes" xml:space="preserve">
          <source>At prediction time, one assigns the class for which the corresponding model gave the greatest confidence. LabelBinarizer makes this easy with the inverse_transform method.</source>
          <target state="translated">在预测时,人们会分配相应模型给出最大置信度的类。LabelBinarizer通过inverse_transform方法让这一切变得简单。</target>
        </trans-unit>
        <trans-unit id="01685dbfb47dea411fa9fd7d65acd3b07b6bad19" translate="yes" xml:space="preserve">
          <source>At present, no metric in &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; supports the multioutput-multiclass classification task.</source>
          <target state="translated">当前，&lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; 中&lt;/a&gt;没有度量标准支持多输出-多类分类任务。</target>
        </trans-unit>
        <trans-unit id="e6b9e03e5fa2d5e24dd0fa35a80f06e2085ef3ae" translate="yes" xml:space="preserve">
          <source>At the end, the top 10 most uncertain predictions will be shown.</source>
          <target state="translated">最后,将显示出最不确定的十大预测。</target>
        </trans-unit>
        <trans-unit id="7dcf986df80f51359fc05a76ec4e2e0108cdc275" translate="yes" xml:space="preserve">
          <source>At the moment, we also don&amp;rsquo;t allow &amp;ldquo;multiclass-multioutput&amp;rdquo; input type.</source>
          <target state="translated">目前，我们还不允许使用&amp;ldquo;多类多输出&amp;rdquo;输入类型。</target>
        </trans-unit>
        <trans-unit id="c524e1372c32ce7785a6874af380c32795f14a92" translate="yes" xml:space="preserve">
          <source>At the time of writing (2019), NumPy and SciPy packages distributed on pypi.org (used by &lt;code&gt;pip&lt;/code&gt;) and on the conda-forge channel are linked with OpenBLAS, while conda packages shipped on the &amp;ldquo;defaults&amp;rdquo; channel from anaconda.org are linked by default with MKL.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9df8fa75ddc8543ff453e545d20ad90388cfe52d" translate="yes" xml:space="preserve">
          <source>Atlas (need hardware specific tuning by rebuilding on the target machine)</source>
          <target state="translated">阿特拉斯(需要通过在目标机上重建来进行具体的硬件调整)</target>
        </trans-unit>
        <trans-unit id="ea953d1f635660a779a7b571a3caa0a7e2c19942" translate="yes" xml:space="preserve">
          <source>Attribute Information</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9f84e7881c1e8ab9dc1a7da3d9d41fc35c6c92bb" translate="yes" xml:space="preserve">
          <source>Attribute Information (in order)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="023272de4f331b86e2df4744d3b1e32a0350f127" translate="yes" xml:space="preserve">
          <source>Attribute Information (in order):</source>
          <target state="translated">属性信息(按顺序):</target>
        </trans-unit>
        <trans-unit id="f66b29f74a8ebab57ad67bac7a0ee9e151e247d3" translate="yes" xml:space="preserve">
          <source>Attribute Information:</source>
          <target state="translated">属性信息:</target>
        </trans-unit>
        <trans-unit id="08359a131c436e0cfbcef05e6e7b301827c6d117" translate="yes" xml:space="preserve">
          <source>Attribute name(s) given as string or a list/tuple of strings Eg.: &lt;code&gt;[&quot;coef_&quot;, &quot;estimator_&quot;, ...], &quot;coef_&quot;&lt;/code&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3d9c6c9c4fb8ae7805f1c9bc9a51e56b622aec7" translate="yes" xml:space="preserve">
          <source>Attribute to access any fitted sub-estimators by name.</source>
          <target state="translated">属性,用于按名称访问任何拟合的子估计器。</target>
        </trans-unit>
        <trans-unit id="e30390c6b25519953f15954ce4132cba67fdd587" translate="yes" xml:space="preserve">
          <source>AttributeError</source>
          <target state="translated">AttributeError</target>
        </trans-unit>
        <trans-unit id="a6652617f2c799eb11ee727b16c5646c48af6905" translate="yes" xml:space="preserve">
          <source>Attributes</source>
          <target state="translated">Attributes</target>
        </trans-unit>
        <trans-unit id="12b65a8e129440be6a666c5f76241c3731cf3a1b" translate="yes" xml:space="preserve">
          <source>Attributes of named_steps map to keys, enabling tab completion in interactive environments:</source>
          <target state="translated">named_steps的属性映射到键,实现了交互式环境中的标签完成。</target>
        </trans-unit>
        <trans-unit id="b8087185e5ee37cef4c337de5697d35d75d909fd" translate="yes" xml:space="preserve">
          <source>Attributes:</source>
          <target state="translated">Attributes:</target>
        </trans-unit>
        <trans-unit id="662dd9f8d8390601f16824595a7c397648198b36" translate="yes" xml:space="preserve">
          <source>Augment dataset with an additional dummy feature.</source>
          <target state="translated">用一个额外的虚拟特征增加数据集。</target>
        </trans-unit>
        <trans-unit id="6060232846b2935a9975d9bd33986976db84e5d5" translate="yes" xml:space="preserve">
          <source>Authors : Kemal Eren License: BSD 3 clause</source>
          <target state="translated">作者:Kemal Eren 授权许可:BSD 3条款 Kemal Eren 授权许可:BSD 3条款</target>
        </trans-unit>
        <trans-unit id="c8696543eb048b80bb9394a738477bc586b5e9f7" translate="yes" xml:space="preserve">
          <source>Authors: &lt;a href=&quot;mailto:mks542%40nyu.edu&quot;&gt;Manoj Kumar&lt;/a&gt;, &lt;a href=&quot;https://github.com/maikia&quot;&gt;Maria Telenczuk&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="b98ce7a4dae4be4f8e868a70ee741cf11d8125c8" translate="yes" xml:space="preserve">
          <source>Automatic Relevance Determination Regression (ARD)</source>
          <target state="translated">自动相关性确定回归(ARD)</target>
        </trans-unit>
        <trans-unit id="79d2e9f45916ec9baf098fb8e9c666bf3b326d9b" translate="yes" xml:space="preserve">
          <source>Automatic grouping of similar objects into sets.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ef442c781e8f4741229b1581adda336c7e2079fb" translate="yes" xml:space="preserve">
          <source>Automatic selection</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e1af4aed4c1976557d8776224b00250d32e50c1e" translate="yes" xml:space="preserve">
          <source>Automatic selection:</source>
          <target state="translated">自动选择。</target>
        </trans-unit>
        <trans-unit id="023744d610124c3af17954739cd092606ebe3ff4" translate="yes" xml:space="preserve">
          <source>Automatically extract clusters according to the Xi-steep method.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6cfaabc56d1e51ca7fb8958edc60fddd81d43492" translate="yes" xml:space="preserve">
          <source>Available Metrics</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="66f9440a5b62bebabad643e5080d2b83ccab9f91" translate="yes" xml:space="preserve">
          <source>Available Metrics The following lists the string metric identifiers and the associated distance metric classes:</source>
          <target state="translated">可用的度量标准 下面列出了字符串度量标准标识符和相关的距离度量标准类别。</target>
        </trans-unit>
        <trans-unit id="7d386374f19b16d2a68aae1af1097f01c0217752" translate="yes" xml:space="preserve">
          <source>Available losses for regression are &amp;lsquo;least_squares&amp;rsquo;, &amp;lsquo;least_absolute_deviation&amp;rsquo;, which is less sensitive to outliers, and &amp;lsquo;poisson&amp;rsquo;, which is well suited to model counts and frequencies. For classification, &amp;lsquo;binary_crossentropy&amp;rsquo; is used for binary classification and &amp;lsquo;categorical_crossentropy&amp;rsquo; is used for multiclass classification. By default the loss is &amp;lsquo;auto&amp;rsquo; and will select the appropriate loss depending on &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-177&quot;&gt;y&lt;/a&gt; passed to &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="eca931fa62c0bd7287f5c68d38715d784e965be8" translate="yes" xml:space="preserve">
          <source>AveBedrms average number of bedrooms</source>
          <target state="translated">AveBedrms 平均卧室数</target>
        </trans-unit>
        <trans-unit id="a283071b7681c2d8946f1976d0c70df9b6239788" translate="yes" xml:space="preserve">
          <source>AveOccup average house occupancy</source>
          <target state="translated">AveOccup 平均房屋出租率</target>
        </trans-unit>
        <trans-unit id="17df5670db0dd155c5ad44fbbe5780684721e711" translate="yes" xml:space="preserve">
          <source>AveRooms average number of rooms</source>
          <target state="translated">AveRooms平均房间数</target>
        </trans-unit>
        <trans-unit id="09b414b4c6acadd2002166fddf247cff6dfe513e" translate="yes" xml:space="preserve">
          <source>Average anomaly score of X of the base classifiers.</source>
          <target state="translated">基础分类器的平均异常得分X。</target>
        </trans-unit>
        <trans-unit id="aaed8a8037e41bff9561818c652346d8c7001f53" translate="yes" xml:space="preserve">
          <source>Average blood pressure</source>
          <target state="translated">平均血压</target>
        </trans-unit>
        <trans-unit id="6a54e71704b0c1b179b46e11f33d130b77d3a0d5" translate="yes" xml:space="preserve">
          <source>Average hinge loss (non-regularized)</source>
          <target state="translated">平均铰链损失(无规则化)</target>
        </trans-unit>
        <trans-unit id="6b044db8b528a2f509603d248114c200773b5ebd" translate="yes" xml:space="preserve">
          <source>Average log-likelihood of the samples under the current model</source>
          <target state="translated">当前模型下样本的平均对数似然性</target>
        </trans-unit>
        <trans-unit id="eb68b4c700400b34d7c3583315c40c631209f07e" translate="yes" xml:space="preserve">
          <source>Average log-likelihood of the samples under the current model.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c993da9cb729cacf8ff6fdcfed939fa26cd8fbe9" translate="yes" xml:space="preserve">
          <source>Average of each column of kernel matrix</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="85f369d4432a9c6380c3e6b6c74d7a111da4715c" translate="yes" xml:space="preserve">
          <source>Average of kernel matrix</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d0fd4fa14e78cac718fcf8ea4ba2f14bf0250758" translate="yes" xml:space="preserve">
          <source>Average of the decision functions of the base classifiers.</source>
          <target state="translated">基础分类器决策函数的平均值。</target>
        </trans-unit>
        <trans-unit id="6599ad46541efb6c59215eff61a003e0c6f84646" translate="yes" xml:space="preserve">
          <source>Average precision. If None, the average precision is not shown.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="af6f79d617c2284de9fdf1ff445ccb0bef8560e1" translate="yes" xml:space="preserve">
          <source>Averaged weights assigned to the features.</source>
          <target state="translated">分配给特征的平均权重。</target>
        </trans-unit>
        <trans-unit id="f2d4013e90ff32393ce2936382f74ff4d17a82cb" translate="yes" xml:space="preserve">
          <source>Averaged weights assigned to the features. Only available if &lt;code&gt;average=True&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="964142eff5ddb8396f41dd41083f429f4f575768" translate="yes" xml:space="preserve">
          <source>Avoid computation of the row norms of X.</source>
          <target state="translated">避免计算X的行规范。</target>
        </trans-unit>
        <trans-unit id="9941876a6243b1a2b044ada9bc274f0eac824c27" translate="yes" xml:space="preserve">
          <source>Axes object to plot on. If &lt;code&gt;None&lt;/code&gt;, a new figure and axes is created.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cdbc35f14da17d95fedf51be8b6d0e5ffbe47113" translate="yes" xml:space="preserve">
          <source>Axes to plot to. If None, use current axis. Any previous content is cleared.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="4301ccb7939c0c6e359abdb2441c99fb2531f767" translate="yes" xml:space="preserve">
          <source>Axes with ROC Curve.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ff3b86b5f5ed2e8ea1a56f1b370527db0b52d66c" translate="yes" xml:space="preserve">
          <source>Axes with confusion matrix.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d788fc3169dcbb97f9fc05c8f3617be236c43224" translate="yes" xml:space="preserve">
          <source>Axes with precision recall curve.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="94256a7c3ba8c276f8434ee8dd2981a7e1ebede5" translate="yes" xml:space="preserve">
          <source>Axis along which the argmin and distances are to be computed.</source>
          <target state="translated">计算argmin和距离的轴线。</target>
        </trans-unit>
        <trans-unit id="9912ed29fe6a32ef154522ba9276f54ba95862ff" translate="yes" xml:space="preserve">
          <source>Axis along which the axis should be computed.</source>
          <target state="translated">应沿轴计算的轴。</target>
        </trans-unit>
        <trans-unit id="e908ee13d1946f0bd5dd05b5fc4432878bcf585b" translate="yes" xml:space="preserve">
          <source>Axis along which to operate. Default is 0, i.e. the first axis.</source>
          <target state="translated">操作的轴线,默认为0,即第一轴。默认为0,即第一轴。</target>
        </trans-unit>
        <trans-unit id="5bcbcdbcc90358e775edd4243e64cbf53abf5bcb" translate="yes" xml:space="preserve">
          <source>Axis or axes along which the means are computed. The default is to compute the mean of the flattened array.</source>
          <target state="translated">计算平均值的轴。默认情况下是计算扁平化数组的平均值。</target>
        </trans-unit>
        <trans-unit id="ebbac4e2a88e22f8f6d03a59528261348ca05f77" translate="yes" xml:space="preserve">
          <source>Axis used to compute the means and standard deviations along. If 0, transform each feature, otherwise (if 1) transform each sample.</source>
          <target state="translated">用于计算平均值和标准差的轴沿。如果为0,则变换每个特征,否则(如果为1)变换每个样本。</target>
        </trans-unit>
        <trans-unit id="1f1b4d8a7c2bf89e7c753b7dde1a7de7538ad410" translate="yes" xml:space="preserve">
          <source>Axis used to scale along. If 0, independently scale each feature, otherwise (if 1) scale each sample.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ae4f281df5a5d0ff3cad6371f76d5c29b6d953ec" translate="yes" xml:space="preserve">
          <source>B</source>
          <target state="translated">B</target>
        </trans-unit>
        <trans-unit id="f9392f6fda3d4e0cffc8528669a8abc510a0238a" translate="yes" xml:space="preserve">
          <source>B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</source>
          <target state="translated">B 1000(Bk-0.63)^2,其中Bk为各镇的黑人比例。</target>
        </trans-unit>
        <trans-unit id="0286abf8af6c149d27c85dea93b91d7cc057559a" translate="yes" xml:space="preserve">
          <source>B. C. Ross &amp;ldquo;Mutual Information between Discrete and Continuous Data Sets&amp;rdquo;. PLoS ONE 9(2), 2014.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e776d2e85d14b59e337010c96ae3a1db78bda84a" translate="yes" xml:space="preserve">
          <source>B12</source>
          <target state="translated">B12</target>
        </trans-unit>
        <trans-unit id="598b91099876ac145b645491cf669799147b8703" translate="yes" xml:space="preserve">
          <source>Back-projection to the original space.</source>
          <target state="translated">回投到原空间。</target>
        </trans-unit>
        <trans-unit id="e935196220898869d9edfac83ba997f81f0fb5f8" translate="yes" xml:space="preserve">
          <source>Bad (e.g. independent labelings) have negative or close to 0.0 scores:</source>
          <target state="translated">坏的(如独立标签)有负分或接近0.0分。</target>
        </trans-unit>
        <trans-unit id="739e4c11d3b87131b9d408fbaefcc186a18d8f06" translate="yes" xml:space="preserve">
          <source>Bad (e.g. independent labelings) have non-positive scores:</source>
          <target state="translated">坏的(如独立标签)有非正分。</target>
        </trans-unit>
        <trans-unit id="3594d99d3d09849089991e6e49125443f8f3dc1b" translate="yes" xml:space="preserve">
          <source>Bad (e.g. independent labelings) have zero scores:</source>
          <target state="translated">坏的(如独立标签)得零分。</target>
        </trans-unit>
        <trans-unit id="ab5267c44135f5b9260b00b1b8283ecbffae624a" translate="yes" xml:space="preserve">
          <source>Bagging methods come in many flavours but mostly differ from each other by the way they draw random subsets of the training set:</source>
          <target state="translated">袋装方法有很多种,但大多是通过抽取训练集的随机子集的方式来区别的。</target>
        </trans-unit>
        <trans-unit id="c18dc564424c60fd05a12c3420895fdf2cfdfe52" translate="yes" xml:space="preserve">
          <source>Bags of words</source>
          <target state="translated">字袋</target>
        </trans-unit>
        <trans-unit id="c108b519256622b208a24e6481a9d957224afa52" translate="yes" xml:space="preserve">
          <source>Balance model complexity and cross-validated score</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="95bff14a4bbf238a2164b4e0cae458c0fe2685ef" translate="yes" xml:space="preserve">
          <source>Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights (&lt;code&gt;sample_weight&lt;/code&gt;) for each class to the same value. Also note that weight-based pre-pruning criteria, such as &lt;code&gt;min_weight_fraction_leaf&lt;/code&gt;, will then be less biased toward dominant classes than criteria that are not aware of the sample weights, like &lt;code&gt;min_samples_leaf&lt;/code&gt;.</source>
          <target state="translated">在训练之前平衡数据集，以防止树偏向主导类。可以通过从每个类别中采样相等数量的样本，或者最好通过将每个类别的样本权重之和（ &lt;code&gt;sample_weight&lt;/code&gt; ）归一化为相同值来完成类别平衡。另外请注意，基于权重的预修剪标准，如 &lt;code&gt;min_weight_fraction_leaf&lt;/code&gt; ，然后将比是不知道样品的权重，如标准朝向主导类较少施力 &lt;code&gt;min_samples_leaf&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="05f675285d8ed3983e70827edce654b1139be242" translate="yes" xml:space="preserve">
          <source>Balanced Accuracy as described in &lt;a href=&quot;#urbanowicz2015&quot; id=&quot;id10&quot;&gt;[Urbanowicz2015]&lt;/a&gt;: the average of sensitivity and specificity is computed for each class and then averaged over total number of classes.</source>
          <target state="translated">&lt;a href=&quot;#urbanowicz2015&quot; id=&quot;id10&quot;&gt;[Urbanowicz2015]中&lt;/a&gt;所述的平衡精度：计算每个类别的敏感性和特异性的平均值，然后在类别总数中取平均值。</target>
        </trans-unit>
        <trans-unit id="784fa76a8b1c1ec60f28239d94fc65e8d7be9167" translate="yes" xml:space="preserve">
          <source>Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the accuracy of prediction algorithms for classification: an overview</source>
          <target state="translated">Baldi、Brunak、Chauvin、Andersen和Nielsen(2000年)。评估分类预测算法的准确性:概述。</target>
        </trans-unit>
        <trans-unit id="06f9184e762e9e45e71254eb60f8399d4c411472" translate="yes" xml:space="preserve">
          <source>Ball tree for fast generalized N-point problems.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7faaf3d73987f1f3ca80fb71528f4fcc07c23b8e" translate="yes" xml:space="preserve">
          <source>BallTree for fast generalized N-point problems</source>
          <target state="translated">用于快速广义N点问题的BallTree。</target>
        </trans-unit>
        <trans-unit id="fef5acd09f6d09b9952e2688ee6340fe9a396e5b" translate="yes" xml:space="preserve">
          <source>BallTree(X, leaf_size=40, metric=&amp;rsquo;minkowski&amp;rsquo;, **kwargs)</source>
          <target state="translated">BallTree（X，leaf_size = 40，metric ='minkowski'，** kwargs）</target>
        </trans-unit>
        <trans-unit id="f72ce994093395e37676061f1dd51a4ea80c1082" translate="yes" xml:space="preserve">
          <source>Bandwidth used in the RBF kernel.</source>
          <target state="translated">在RBF内核中使用的带宽。</target>
        </trans-unit>
        <trans-unit id="0c486c3167e8e5060c79997e836642fbe914971f" translate="yes" xml:space="preserve">
          <source>Barnes-Hut is an approximation of the exact method. The approximation is parameterized with the angle parameter, therefore the angle parameter is unused when method=&amp;rdquo;exact&amp;rdquo;</source>
          <target state="translated">Barnes-Hut是精确方法的近似值。近似值是使用angle参数进行参数化的，因此，当method =&amp;ldquo; exact&amp;rdquo;时，角度参数未使用</target>
        </trans-unit>
        <trans-unit id="3b0ab9922dcf0fcbb725191c9b8bc0a7c9ecc19c" translate="yes" xml:space="preserve">
          <source>Barnes-Hut is significantly more scalable. Barnes-Hut can be used to embed hundred of thousands of data points while the exact method can handle thousands of samples before becoming computationally intractable</source>
          <target state="translated">Barnes-Hut的可扩展性明显更高。Barnes-Hut可用于嵌入数十万个数据点,而精确方法可处理数千个样本,然后在计算上变得难以解决。</target>
        </trans-unit>
        <trans-unit id="b10a9a2aa738bc222bc8e12bf3c9da484bcf459f" translate="yes" xml:space="preserve">
          <source>Barnes-Hut only works with dense input data. Sparse data matrices can only be embedded with the exact method or can be approximated by a dense low rank projection for instance using &lt;a href=&quot;generated/sklearn.decomposition.truncatedsvd#sklearn.decomposition.TruncatedSVD&quot;&gt;&lt;code&gt;sklearn.decomposition.TruncatedSVD&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Barnes-Hut仅适用于密集的输入数据。稀疏数据矩阵只能用确切的方法嵌入，或者可以通过密集的低秩投影来近似，例如使用&lt;a href=&quot;generated/sklearn.decomposition.truncatedsvd#sklearn.decomposition.TruncatedSVD&quot;&gt; &lt;code&gt;sklearn.decomposition.TruncatedSVD&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7063b67add41dd6938d0fe03decc31a786e18ce9" translate="yes" xml:space="preserve">
          <source>Base class for all estimators in scikit-learn</source>
          <target state="translated">scikit-learn中所有估计器的基础类。</target>
        </trans-unit>
        <trans-unit id="b3ed1a2c57def49034ad9434aa34e34b64c59294" translate="yes" xml:space="preserve">
          <source>Base class for all kernels.</source>
          <target state="translated">所有内核的基类。</target>
        </trans-unit>
        <trans-unit id="597d1d5f179914ea7470a3760bd8ee3a2400c1bc" translate="yes" xml:space="preserve">
          <source>Base classes</source>
          <target state="translated">基础班</target>
        </trans-unit>
        <trans-unit id="425994da22262a2dfe067d38bec718a9f15fcaea" translate="yes" xml:space="preserve">
          <source>Base classes for all estimators.</source>
          <target state="translated">所有估算器的基础类。</target>
        </trans-unit>
        <trans-unit id="b0cdf2e205135846db5a02006385fb2f510f153e" translate="yes" xml:space="preserve">
          <source>Base classifier for this ensemble.</source>
          <target state="translated">个基础分类器。</target>
        </trans-unit>
        <trans-unit id="61ae0e2f9a4051180ae0aa7361de2f44f1e6a032" translate="yes" xml:space="preserve">
          <source>Base estimator for this ensemble.</source>
          <target state="translated">这个集合的基础估计器。</target>
        </trans-unit>
        <trans-unit id="96ce9dc70ae7445a77ac8ebb388087887574787e" translate="yes" xml:space="preserve">
          <source>Base estimator object which implements the following methods:</source>
          <target state="translated">实现以下方法的基础估计器对象。</target>
        </trans-unit>
        <trans-unit id="a4f9906b1c0bdc268249c9569eb1485c9b45d816" translate="yes" xml:space="preserve">
          <source>Base estimators which will be stacked together. Each element of the list is defined as a tuple of string (i.e. name) and an estimator instance. An estimator can be set to &amp;lsquo;drop&amp;rsquo; using &lt;code&gt;set_params&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d3f0bf51e2f9cb27fcde269872aa74bb4b0677d2" translate="yes" xml:space="preserve">
          <source>Base of the logarithm used for the discount. A low value means a sharper discount (top results are more important).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f6856d61f849440214e260ddb8260a2004382c80" translate="yes" xml:space="preserve">
          <source>Based on these bin intervals, &lt;code&gt;X&lt;/code&gt; is transformed as follows:</source>
          <target state="translated">基于这些bin间隔， &lt;code&gt;X&lt;/code&gt; 转换如下：</target>
        </trans-unit>
        <trans-unit id="623eb094427cb27d5c4cd4b74097a0449f38ab15" translate="yes" xml:space="preserve">
          <source>Basically, 1. may be a reader that yields instances from files on a hard drive, a database, from a network stream etc. However, details on how to achieve this are beyond the scope of this documentation.</source>
          <target state="translated">基本上,1.可能是一个从硬盘上的文件、数据库、从网络流等产生实例的阅读器。然而,关于如何实现这一目标的细节超出了本文档的范围。</target>
        </trans-unit>
        <trans-unit id="93b2071c2229aa4389154bc62f3ee3617cb13ae0" translate="yes" xml:space="preserve">
          <source>Bayesian ARD regression.</source>
          <target state="translated">贝叶斯ARD回归。</target>
        </trans-unit>
        <trans-unit id="53c3e11f0d41baa7b5753d8723cd4012a9d964c3" translate="yes" xml:space="preserve">
          <source>Bayesian Ridge Regression</source>
          <target state="translated">贝叶斯岭回归</target>
        </trans-unit>
        <trans-unit id="5f8e5b5e24015d9adc82586dba204556446370ab" translate="yes" xml:space="preserve">
          <source>Bayesian Ridge Regression is used for regression:</source>
          <target state="translated">贝叶斯岭回归是用于回归的。</target>
        </trans-unit>
        <trans-unit id="c13c97854320151b1c6271dc7d1a9fd73998cc11" translate="yes" xml:space="preserve">
          <source>Bayesian information criterion for the current model on the input X.</source>
          <target state="translated">当前模型对输入X的贝叶斯信息准则。</target>
        </trans-unit>
        <trans-unit id="9caaf8acefcf32287fe9e5bedbc0bc32de3a2bfd" translate="yes" xml:space="preserve">
          <source>Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand.</source>
          <target state="translated">贝叶斯回归技术可以用来在估计程序中加入正则化参数:正则化参数不是硬性设置的,而是根据手头的数据进行调整。</target>
        </trans-unit>
        <trans-unit id="f02284fac00f823c24638d260dbf19c33e10b7f5" translate="yes" xml:space="preserve">
          <source>Bayesian regressors</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2b720ec1817dce1bf9d7dcbcab6e206b85efdaa6" translate="yes" xml:space="preserve">
          <source>Bayesian ridge regression</source>
          <target state="translated">贝叶斯岭回归</target>
        </trans-unit>
        <trans-unit id="66a81420e9a002d3d7820b1130f20af6976fb2b7" translate="yes" xml:space="preserve">
          <source>Bayesian ridge regression.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="13244381ac86550f492dc53b5005b711b59fb204" translate="yes" xml:space="preserve">
          <source>Be aware that the number of features in the output array scales polynomially in the number of features of the input array, and exponentially in the degree. High degrees can cause overfitting.</source>
          <target state="translated">要知道,输出数组中的特征数与输入数组的特征数呈多项式缩放,而度数则是指数级的。高度会导致过拟合。</target>
        </trans-unit>
        <trans-unit id="3faa94ddadd318647d89d2a02f2d45606c753b0d" translate="yes" xml:space="preserve">
          <source>Be invariant to class label: relabelling &lt;code&gt;y = [&quot;Happy&quot;, &quot;Sad&quot;]&lt;/code&gt; to &lt;code&gt;y = [1, 0]&lt;/code&gt; should not change the indices generated.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="3a63e14a349843f9c954f28468802eec39d0181f" translate="yes" xml:space="preserve">
          <source>Be mindful that this function is an order of magnitude slower than other metrics, such as the Adjusted Rand Index.</source>
          <target state="translated">请注意,这个函数比其他指标(如调整后的兰德指数)慢一个数量级。</target>
        </trans-unit>
        <trans-unit id="83921ebb17d38b8a4b1a1e3f6653eb2aa6327ec9" translate="yes" xml:space="preserve">
          <source>Because LARS is based upon an iterative refitting of the residuals, it would appear to be especially sensitive to the effects of noise. This problem is discussed in detail by Weisberg in the discussion section of the Efron et al. (2004) Annals of Statistics article.</source>
          <target state="translated">由于LARS是基于对残差的反复调整,它似乎对噪声的影响特别敏感。Weisberg在Efron等人(2004年)的《统计年鉴》一文的讨论部分详细讨论了这个问题。</target>
        </trans-unit>
        <trans-unit id="5dbb3cd2ef2ab4d2456b8c42be279603ef8045ba" translate="yes" xml:space="preserve">
          <source>Because of scaling performed by this method, it is discouraged to use it together with methods that are not scale-invariant (like SVMs)</source>
          <target state="translated">由于该方法进行了缩放,因此不鼓励将其与非尺度不变的方法(如SVMs)一起使用。</target>
        </trans-unit>
        <trans-unit id="63be2fc59867472cdd54d70a5100dc5be4d8ddfa" translate="yes" xml:space="preserve">
          <source>Because of the Python object overhead involved in calling the python function, this will be fairly slow, but it will have the same scaling as other distances.</source>
          <target state="translated">由于在调用python函数时涉及到Python对象的开销,这将相当缓慢,但它将具有与其他距离相同的缩放。</target>
        </trans-unit>
        <trans-unit id="8e7a8c8acef8e25d8225bca274332d6c56e271be" translate="yes" xml:space="preserve">
          <source>Because the models in each chain are arranged randomly there is significant variation in performance among the chains. Presumably there is an optimal ordering of the classes in a chain that will yield the best performance. However we do not know that ordering a priori. Instead we can construct an voting ensemble of classifier chains by averaging the binary predictions of the chains and apply a threshold of 0.5. The Jaccard similarity score of the ensemble is greater than that of the independent models and tends to exceed the score of each chain in the ensemble (although this is not guaranteed with randomly ordered chains).</source>
          <target state="translated">由于每条链中的模型是随机排列的,所以各链之间的性能差异很大。据推测,链中的类有一个最佳排序,将产生最佳性能。然而,我们并不能先验地知道这个排序。相反,我们可以通过对分类器链的二元预测进行平均,并应用0.5的阈值来构建一个分类器链的投票集合。合集的Jaccard相似度得分大于独立模型的相似度得分,并趋向于超过合集中每条链的得分(尽管随机排序的链不能保证这一点)。</target>
        </trans-unit>
        <trans-unit id="7223d10fe616d4b0ef82e34ecf5c077bba0a2d4c" translate="yes" xml:space="preserve">
          <source>Because the number of neighbors of each point is not necessarily equal, the results for multiple query points cannot be fit in a standard data array. For efficiency, &lt;code&gt;radius_neighbors&lt;/code&gt; returns arrays of objects, where each object is a 1D array of indices or distances.</source>
          <target state="translated">因为每个点的邻居数不一定相等，所以多个查询点的结果不能适合标准数据数组。为了提高效率， &lt;code&gt;radius_neighbors&lt;/code&gt; 返回对象数组，其中每个对象都是一维索引或距离数组。</target>
        </trans-unit>
        <trans-unit id="5b27b93fd142831db995faecc3827454df4cf37d" translate="yes" xml:space="preserve">
          <source>Because the query set matches the training set, the nearest neighbor of each point is the point itself, at a distance of zero.</source>
          <target state="translated">因为查询集与训练集相匹配,所以每个点的最近邻居是点本身,距离为零。</target>
        </trans-unit>
        <trans-unit id="40a1ac9f9c5627c60c3da1e0a0050f7b91b8daf1" translate="yes" xml:space="preserve">
          <source>Because this implementation uses a flat kernel and a Ball Tree to look up members of each kernel, the complexity will tend towards O(T*n*log(n)) in lower dimensions, with n the number of samples and T the number of points. In higher dimensions the complexity will tend towards O(T*n^2).</source>
          <target state="translated">因为这个实现使用了一个平核和一个球形树来查找每个核的成员,所以在较低维度上,复杂度将趋向于O(T*n*log(n)),n是样本数,T是点的数量。在较高维度上,复杂度将趋向于O(T*n^2)。</target>
        </trans-unit>
        <trans-unit id="5430204387d0c34e554350a9ea0af84d5f7d324c" translate="yes" xml:space="preserve">
          <source>Before we can use Ames dataset we still need to do some preprocessing. First, the dataset has many missing values. To impute them, we will exchange categorical missing values with the new category &amp;lsquo;missing&amp;rsquo; while the numerical missing values with the &amp;lsquo;mean&amp;rsquo; of the column. We will also encode the categories with either &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt;&lt;code&gt;sklearn.preprocessing.OrdinalEncoder&lt;/code&gt;&lt;/a&gt; depending for which type of model we will use them (linear or non-linear model). To falicitate this preprocessing we will make two pipelines. You can skip this section if your data is ready to use and does not need preprocessing</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="f3c17036de4b5ca609e4df67622292c35365a396" translate="yes" xml:space="preserve">
          <source>Behaviour of the &lt;code&gt;decision_function&lt;/code&gt; which can be either &amp;lsquo;old&amp;rsquo; or &amp;lsquo;new&amp;rsquo;. Passing &lt;code&gt;behaviour='new'&lt;/code&gt; makes the &lt;code&gt;decision_function&lt;/code&gt; change to match other anomaly detection algorithm API which will be the default behaviour in the future. As explained in details in the &lt;code&gt;offset_&lt;/code&gt; attribute documentation, the &lt;code&gt;decision_function&lt;/code&gt; becomes dependent on the contamination parameter, in such a way that 0 becomes its natural threshold to detect outliers.</source>
          <target state="translated">该行为 &lt;code&gt;decision_function&lt;/code&gt; 它可以是&amp;ldquo;老&amp;rdquo;或&amp;ldquo;新&amp;rdquo;。传递 &lt;code&gt;behaviour='new'&lt;/code&gt; 进行 &lt;code&gt;decision_function&lt;/code&gt; 更改，以匹配其他异常检测算法API，这将是将来的默认行为。如 &lt;code&gt;offset_&lt;/code&gt; attribute文档中的详细说明， &lt;code&gt;decision_function&lt;/code&gt; 依赖于污染参数，其方式是0成为检测异常值的自然阈值。</target>
        </trans-unit>
        <trans-unit id="20bc631a66b3cc1b1a3392b3fbbb196ee59f89ba" translate="yes" xml:space="preserve">
          <source>Being a forward feature selection method like &lt;a href=&quot;#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt;, orthogonal matching pursuit can approximate the optimum solution vector with a fixed number of non-zero elements:</source>
          <target state="translated">作为&lt;a href=&quot;#least-angle-regression&quot;&gt;最小角度回归&lt;/a&gt;等前向特征选择方法，正交匹配追踪可以使用固定数量的非零元素来逼近最优解向量：</target>
        </trans-unit>
        <trans-unit id="305fd1e902f73a5b60bc86a4bdd6b4a6ea0e533f" translate="yes" xml:space="preserve">
          <source>Below are examples of Box-Cox and Yeo-Johnson applied to various probability distributions. Note that when applied to certain distributions, the power transforms achieve very Gaussian-like results, but with others, they are ineffective. This highlights the importance of visualizing the data before and after transformation.</source>
          <target state="translated">下面是Box-Cox和Yeo-Johnson应用于各种概率分布的例子。请注意,当应用于某些分布时,幂级变换可以达到非常类似高斯的结果,但对于其他分布,它们是无效的。这凸显了在变换前后对数据进行可视化的重要性。</target>
        </trans-unit>
        <trans-unit id="15f2be220fb72bee910f5e62c59989aad06195d5" translate="yes" xml:space="preserve">
          <source>Below is a summary of the classifiers supported by scikit-learn grouped by strategy; you don&amp;rsquo;t need the meta-estimators in this class if you&amp;rsquo;re using one of these, unless you want custom multiclass behavior:</source>
          <target state="translated">以下是scikit-learn支持的分类器（按策略分组）的摘要；如果您使用的是其中之一，则不需要此类中的元估计器，除非您需要自定义多类行为：</target>
        </trans-unit>
        <trans-unit id="108ff87be25a1c1dd2316c9f1fb3ba28787755c8" translate="yes" xml:space="preserve">
          <source>Below is an example graphviz export of the above tree trained on the entire iris dataset; the results are saved in an output file &lt;code&gt;iris.pdf&lt;/code&gt;:</source>
          <target state="translated">下面是在整个虹膜数据集上训练的上述树的示例graphviz导出；结果保存在输出文件 &lt;code&gt;iris.pdf&lt;/code&gt; 中：</target>
        </trans-unit>
        <trans-unit id="f4264cf8db047b13d2c0262177f6f646451f4fbb" translate="yes" xml:space="preserve">
          <source>Below is an example of multiclass learning using Output-Codes:</source>
          <target state="translated">下面是一个使用输出代码进行多类学习的例子。</target>
        </trans-unit>
        <trans-unit id="b49bf188ea86adad6efb34f880d43936d0fc0ac2" translate="yes" xml:space="preserve">
          <source>Below is an example of multiclass learning using OvO:</source>
          <target state="translated">下面是一个使用OvO进行多类学习的例子。</target>
        </trans-unit>
        <trans-unit id="687c4cfff2712863f68152405d4b2638d130c286" translate="yes" xml:space="preserve">
          <source>Below is an example of multiclass learning using OvR:</source>
          <target state="translated">下面是一个使用OvR进行多类学习的例子。</target>
        </trans-unit>
        <trans-unit id="c80585dafc7b9164da5261a6f83e5756b02054c8" translate="yes" xml:space="preserve">
          <source>Below is an example of multioutput classification:</source>
          <target state="translated">下面是一个多输出分类的例子。</target>
        </trans-unit>
        <trans-unit id="5323f7dfcb00efe99d53b8ad7c93f56a3771bb8d" translate="yes" xml:space="preserve">
          <source>Below is an example of multioutput regression:</source>
          <target state="translated">下面是一个多产出回归的例子。</target>
        </trans-unit>
        <trans-unit id="c1920b9ad7a1724faf2c50a13254e745783c4989" translate="yes" xml:space="preserve">
          <source>Below is an example of the iris dataset, which is comprised of 4 features, projected on the 2 dimensions that explain most variance:</source>
          <target state="translated">下面是一个虹膜数据集的例子,该数据集由4个特征组成,投影在解释大部分差异的2个维度上。</target>
        </trans-unit>
        <trans-unit id="fe179d558d43e31394bc90d2c2bd912f3d7ad712" translate="yes" xml:space="preserve">
          <source>Belsley, Kuh &amp;amp; Welsch, &amp;lsquo;Regression diagnostics: Identifying Influential Data and Sources of Collinearity&amp;rsquo;, Wiley, 1980. 244-261.</source>
          <target state="translated">Belsley，Kuh＆Welsch，&amp;ldquo;回归诊断：确定共线性的影响数据和来源&amp;rdquo;，Wiley，1980年。244-261。</target>
        </trans-unit>
        <trans-unit id="9f292c5936105126748d157311146c4c77a1cc65" translate="yes" xml:space="preserve">
          <source>Benchmark classifiers</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fe07a59cb91f8e159239a44b010528ab1c647c88" translate="yes" xml:space="preserve">
          <source>Bergstra, J. and Bengio, Y., Random search for hyper-parameter optimization, The Journal of Machine Learning Research (2012)</source>
          <target state="translated">Bergstra,J.and Bengio,Y.,Random search for hyper-parameter optimization,The Journal of Machine Learning Research (2012)</target>
        </trans-unit>
        <trans-unit id="acb0e0e454302529b80959f07b2776ace520ba67" translate="yes" xml:space="preserve">
          <source>Bernhard Schoelkopf, Alexander J. Smola, and Klaus-Robert Mueller. 1999. Kernel principal component analysis. In Advances in kernel methods, MIT Press, Cambridge, MA, USA 327-352.</source>
          <target state="translated">Bernhard Schoelkopf,Alexander J.Smola和Klaus-Robert Mueller。1999.内核主成分分析。In Advances in kernel methods,MIT Press,Cambridge,MA,USA 327-352.</target>
        </trans-unit>
        <trans-unit id="d8b44488165a02f44f4a67e6ba1ce39e5b9c9bb9" translate="yes" xml:space="preserve">
          <source>Bernoulli Restricted Boltzmann Machine (RBM).</source>
          <target state="translated">伯努利受限波尔兹曼机(RBM)。</target>
        </trans-unit>
        <trans-unit id="5923a4b6b18db19ea579977a7a07a6737359d838" translate="yes" xml:space="preserve">
          <source>Besides scikit-learn, NumPy and SciPy also use BLAS internally, as explained earlier.</source>
          <target state="translated">除了scikit-learn,NumPy和SciPy内部也使用BLAS,前面已经解释过。</target>
        </trans-unit>
        <trans-unit id="7d9a50881bd5d34faa0a8f3ad342e9a7c84f5b08" translate="yes" xml:space="preserve">
          <source>Best fitted model (copy of the &lt;code&gt;base_estimator&lt;/code&gt; object).</source>
          <target state="translated">最佳拟合模型（ &lt;code&gt;base_estimator&lt;/code&gt; 对象的副本）。</target>
        </trans-unit>
        <trans-unit id="12a49239d2ca4095c10a87ae8e694332610beadd" translate="yes" xml:space="preserve">
          <source>Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">最佳可能的分数是1.0,而且可以是负数(因为模型可以任意变坏)。一个恒定的模型,如果总是预测y的期望值,不考虑输入特征,那么R^2得分将为0.0。</target>
        </trans-unit>
        <trans-unit id="792cb529b422a1c11c9b2f8bb241ed31b32984d9" translate="yes" xml:space="preserve">
          <source>Best possible score is 1.0, lower values are worse.</source>
          <target state="translated">最好的分数是1.0,数值越低越差。</target>
        </trans-unit>
        <trans-unit id="28c6b37b189c99aed8957aff1f021f2f9f59464b" translate="yes" xml:space="preserve">
          <source>Beta-divergence loss functions</source>
          <target state="translated">β-偏离损失函数</target>
        </trans-unit>
        <trans-unit id="d2c95a0bd9e48b10416c4f3dc812e08319356fa2" translate="yes" xml:space="preserve">
          <source>Beware not to use a regression scoring function with a classification problem, you will get useless results.</source>
          <target state="translated">小心不要在分类问题上使用回归评分函数,你会得到无用的结果。</target>
        </trans-unit>
        <trans-unit id="a9f3a13f44f7a1a81c6757a16dde70a0eb1d4b70" translate="yes" xml:space="preserve">
          <source>Bias</source>
          <target state="translated">Bias</target>
        </trans-unit>
        <trans-unit id="ba237b5bed1b1a1e028b039eda8969e7ed30ca0a" translate="yes" xml:space="preserve">
          <source>Bias and variance are inherent properties of estimators and we usually have to select learning algorithms and hyperparameters so that both bias and variance are as low as possible (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias-variance_dilemma&quot;&gt;Bias-variance dilemma&lt;/a&gt;). Another way to reduce the variance of a model is to use more training data. However, you should only collect more training data if the true function is too complex to be approximated by an estimator with a lower variance.</source>
          <target state="translated">偏差和方差是估计量的固有属性，我们通常必须选择学习算法和超参数，以使偏差和方差都尽可能低（请参见&lt;a href=&quot;https://en.wikipedia.org/wiki/Bias-variance_dilemma&quot;&gt;偏差方差难题&lt;/a&gt;）。减少模型差异的另一种方法是使用更多的训练数据。但是，仅当真实函数过于复杂而无法由方差较小的估算器近似时，才应收集更多训练数据。</target>
        </trans-unit>
        <trans-unit id="06f19409a98653c678fe6ff3c120895b283ca5ff" translate="yes" xml:space="preserve">
          <source>Bias-variance trade-off when setting the shrinkage: comparing the choices of Ledoit-Wolf and OAS estimators</source>
          <target state="translated">设置收缩率时的偏差-方差权衡:比较Ledoit-Wolf和OAS估计器的选择。</target>
        </trans-unit>
        <trans-unit id="7c5f1fb15e060b340fb270b747d162cde9961929" translate="yes" xml:space="preserve">
          <source>Bias.</source>
          <target state="translated">Bias.</target>
        </trans-unit>
        <trans-unit id="e26ae344044922af518669ed7912f3779c4b00f9" translate="yes" xml:space="preserve">
          <source>Bias:</source>
          <target state="translated">Bias:</target>
        </trans-unit>
        <trans-unit id="6b339e821e48cfc38068b641fe05c82a58e3e342" translate="yes" xml:space="preserve">
          <source>Biases of the hidden units.</source>
          <target state="translated">隐性单位的偏差。</target>
        </trans-unit>
        <trans-unit id="e50dfa9d24499c67dd80dc7ae0f62d209963644d" translate="yes" xml:space="preserve">
          <source>Biases of the visible units.</source>
          <target state="translated">可见单位的偏差。</target>
        </trans-unit>
        <trans-unit id="d4404195d10795d2fac0ea59f4bd8efbaf381e98" translate="yes" xml:space="preserve">
          <source>Biclustering</source>
          <target state="translated">Biclustering</target>
        </trans-unit>
        <trans-unit id="a252b47dd9e6fba33ce507e3a8e6ec1bb1e9c7d0" translate="yes" xml:space="preserve">
          <source>Biclustering can be performed with the module &lt;a href=&quot;classes#module-sklearn.cluster.bicluster&quot;&gt;&lt;code&gt;sklearn.cluster.bicluster&lt;/code&gt;&lt;/a&gt;. Biclustering algorithms simultaneously cluster rows and columns of a data matrix. These clusters of rows and columns are known as biclusters. Each determines a submatrix of the original data matrix with some desired properties.</source>
          <target state="translated">可以使用&lt;a href=&quot;classes#module-sklearn.cluster.bicluster&quot;&gt; &lt;code&gt;sklearn.cluster.bicluster&lt;/code&gt; &lt;/a&gt;模块进行二元组化。双簇算法同时对数据矩阵的行和列进行聚类。这些行和列的簇称为双簇。每个函数都确定具有某些所需属性的原始数据矩阵的子矩阵。</target>
        </trans-unit>
        <trans-unit id="edf2774b82282aef622b14eeaa9c74d15c4f211e" translate="yes" xml:space="preserve">
          <source>Biclustering can be performed with the module &lt;code&gt;sklearn.cluster.bicluster&lt;/code&gt;. Biclustering algorithms simultaneously cluster rows and columns of a data matrix. These clusters of rows and columns are known as biclusters. Each determines a submatrix of the original data matrix with some desired properties.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ec46a574c9badafd039dd72d03169fe1273b5603" translate="yes" xml:space="preserve">
          <source>Biclustering documents with the Spectral Co-clustering algorithm</source>
          <target state="translated">用光谱共聚算法对文档进行双聚类处理。</target>
        </trans-unit>
        <trans-unit id="ee86c7bb4d236ce105b92fe63ada05dd4ec5870a" translate="yes" xml:space="preserve">
          <source>Biclustering has many other names in different fields including co-clustering, two-mode clustering, two-way clustering, block clustering, coupled two-way clustering, etc. The names of some algorithms, such as the Spectral Co-Clustering algorithm, reflect these alternate names.</source>
          <target state="translated">双聚类在不同领域还有很多其他的名称,包括共聚类、双模聚类、双向聚类、块聚类、耦合双向聚类等。一些算法的名称,如谱系共聚类算法,就反映了这些另类的名称。</target>
        </trans-unit>
        <trans-unit id="9dbdcbb1f58605adc858ff7ea9bd66e335bd0acc" translate="yes" xml:space="preserve">
          <source>Biclustering metrics</source>
          <target state="translated">双集群指标</target>
        </trans-unit>
        <trans-unit id="d9cf1c1fb4521641a79265f68d061558d2a8f977" translate="yes" xml:space="preserve">
          <source>Bigger is better, i.e. large values correspond to inliers.</source>
          <target state="translated">越大越好,即大值对应离群值。</target>
        </trans-unit>
        <trans-unit id="8a391f29f990251d22520f34bdb4ac600bbfd771" translate="yes" xml:space="preserve">
          <source>Bin continuous data into intervals.</source>
          <target state="translated">将连续的数据划分为区间。</target>
        </trans-unit>
        <trans-unit id="271e5dda8b524cc22b71bebf5401052359debca7" translate="yes" xml:space="preserve">
          <source>Binarization is a common operation on text count data where the analyst can decide to only consider the presence or absence of a feature rather than a quantified number of occurrences for instance.</source>
          <target state="translated">Binarization是对文本计数数据的一种常见操作,分析师可以决定只考虑特征的存在或不存在,而不是量化的出现次数,例如。</target>
        </trans-unit>
        <trans-unit id="98a952cd8923e662b6dc4cb343f8e07b8c748673" translate="yes" xml:space="preserve">
          <source>Binarize data (set feature values to 0 or 1) according to a threshold</source>
          <target state="translated">根据阈值对数据进行二值化(将特征值设置为0或1)。</target>
        </trans-unit>
        <trans-unit id="e6093920f16b7a0a90021c0a6f1b3ba3a339b537" translate="yes" xml:space="preserve">
          <source>Binarize each element of X</source>
          <target state="translated">二进制化X的每个元素</target>
        </trans-unit>
        <trans-unit id="0b5b7e155ae5a5495c55eaab6001751a5f50a1e4" translate="yes" xml:space="preserve">
          <source>Binarize labels in a one-vs-all fashion</source>
          <target state="translated">以一物降一物的方式实现标签的二元化。</target>
        </trans-unit>
        <trans-unit id="44b97fdbb0f9ad559fc3a3794b99c9bc30004c1c" translate="yes" xml:space="preserve">
          <source>Binarizes labels in a one-vs-all fashion.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="2c6db33026290c5eceb25ad459dd62fbc5de5524" translate="yes" xml:space="preserve">
          <source>Binary and multiclass labels are supported. Only in the binary case does this relate to information about true and false positives and negatives. See references below.</source>
          <target state="translated">支持二进制和多类标签。只有在二进制的情况下,才涉及到真假阳性和阴性的信息。参见下面的参考文献。</target>
        </trans-unit>
        <trans-unit id="76f6ce139477b2ff40abfa10ccd09f75138f8a5a" translate="yes" xml:space="preserve">
          <source>Binary array containing the code of each class.</source>
          <target state="translated">包含每个类的代码的二进制数组。</target>
        </trans-unit>
        <trans-unit id="d5752168a3a19813275e09ded6a3644b6b365056" translate="yes" xml:space="preserve">
          <source>Binary indicators for missing values.</source>
          <target state="translated">缺失值的二元指标。</target>
        </trans-unit>
        <trans-unit id="d20fae796db569b25a51c85d3dbd034384870434" translate="yes" xml:space="preserve">
          <source>Binary probability estimates for loss=&amp;rdquo;modified_huber&amp;rdquo; are given by (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions it is necessary to perform proper probability calibration by wrapping the classifier with &lt;a href=&quot;sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt;&lt;code&gt;sklearn.calibration.CalibratedClassifierCV&lt;/code&gt;&lt;/a&gt; instead.</source>
          <target state="translated">损失=&amp;ldquo; modified_huber&amp;rdquo;的二进制概率估计值由（clip（decision_function（X），-1，1）+ 1）/ 2给出。对于其他损失函数，有必要通过用&lt;a href=&quot;sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt; &lt;code&gt;sklearn.calibration.CalibratedClassifierCV&lt;/code&gt; &lt;/a&gt;包装分类器来执行适当的概率校准。 Calibration.CalibratedClassifierCV代替。</target>
        </trans-unit>
        <trans-unit id="6ea0caebd3956174ca222001571ec7f4a771808d" translate="yes" xml:space="preserve">
          <source>Binary target values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e17841c821731bb47f4b19f4849e509ece4c5657" translate="yes" xml:space="preserve">
          <source>Binary targets transform to a column vector</source>
          <target state="translated">二进制目标转换为列向量</target>
        </trans-unit>
        <trans-unit id="2a2767f039bc988b0f4c4e7163ccdacd052867c1" translate="yes" xml:space="preserve">
          <source>Binding of the cross-validation routine (low-level routine)</source>
          <target state="translated">交叉验证例行程序(低级例行程序)的约束;</target>
        </trans-unit>
        <trans-unit id="64fb71b36aba4bcefad2e5d527659e300a2b2c01" translate="yes" xml:space="preserve">
          <source>Binomial deviance (&lt;code&gt;'deviance'&lt;/code&gt;): The negative binomial log-likelihood loss function for binary classification (provides probability estimates). The initial model is given by the log odds-ratio.</source>
          <target state="translated">二项式偏差（ &lt;code&gt;'deviance'&lt;/code&gt; ）：二元分类的负二项式对数似然损失函数（提供概率估计）。初始模型由对数比值比给出。</target>
        </trans-unit>
        <trans-unit id="7eef6382001e9a152cc75ac79b202341870bb6db" translate="yes" xml:space="preserve">
          <source>Birch</source>
          <target state="translated">Birch</target>
        </trans-unit>
        <trans-unit id="c8804a672e163b7d85424df4080099668af078fd" translate="yes" xml:space="preserve">
          <source>Birch does not scale very well to high dimensional data. As a rule of thumb if &lt;code&gt;n_features&lt;/code&gt; is greater than twenty, it is generally better to use MiniBatchKMeans.</source>
          <target state="translated">桦木无法很好地缩放到高维数据。根据经验，如果 &lt;code&gt;n_features&lt;/code&gt; 大于20，通常最好使用MiniBatchKMeans。</target>
        </trans-unit>
        <trans-unit id="055a71a716f70327eef9729a95cd9e98a091a68c" translate="yes" xml:space="preserve">
          <source>Bishop, &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;Pattern recognition and machine learning&lt;/a&gt;, chapter 7 Sparse Kernel Machines</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="35b99b7929918827477b2be5f13db5f62d5bed94" translate="yes" xml:space="preserve">
          <source>Bishop, Christopher M. (2006). &amp;ldquo;Pattern recognition and machine learning&amp;rdquo;. Vol. 4 No. 4. New York: Springer.</source>
          <target state="translated">主教，克里斯托弗&amp;middot;M（2006）。&amp;ldquo;模式识别和机器学习&amp;rdquo;。卷 4号。4.纽约：施普林格。</target>
        </trans-unit>
        <trans-unit id="3ee00b8c0e8ff24c1a59dc452e853cf83ab3d2bf" translate="yes" xml:space="preserve">
          <source>Blei, David M. and Michael I. Jordan. (2006). &amp;ldquo;Variational inference for Dirichlet process mixtures&amp;rdquo;. Bayesian analysis 1.1</source>
          <target state="translated">Blei，David M.和Michael I. Jordan。（2006）。&amp;ldquo; Dirichlet过程混合物的变量推论&amp;rdquo;。贝叶斯分析1.1</target>
        </trans-unit>
        <trans-unit id="66370792731dff7f31408706eed1fd3ef5297d11" translate="yes" xml:space="preserve">
          <source>Blind source separation using FastICA</source>
          <target state="translated">使用FastICA进行盲源分离</target>
        </trans-unit>
        <trans-unit id="7d44bc449c2a26374800a503f10f3d8949505f40" translate="yes" xml:space="preserve">
          <source>Blue</source>
          <target state="translated">Blue</target>
        </trans-unit>
        <trans-unit id="d1411ae3cdd27fda5ea57577c408be223f586cf0" translate="yes" xml:space="preserve">
          <source>Body mass index</source>
          <target state="translated">身体质量指数</target>
        </trans-unit>
        <trans-unit id="eeb4978eef8f0138e90d13575223e62053c06fa3" translate="yes" xml:space="preserve">
          <source>Bonus point if the utility is able to give a confidence level for its predictions.</source>
          <target state="translated">如果该实用程序能够为其预测提供一个置信度,则加分。</target>
        </trans-unit>
        <trans-unit id="20ee87c5c904919ec390ea5b4c0a66ba0775ae58" translate="yes" xml:space="preserve">
          <source>BonusMalus</source>
          <target state="translated">BonusMalus</target>
        </trans-unit>
        <trans-unit id="3605ba73833c24e0fbda3c252c7bb0e601382c92" translate="yes" xml:space="preserve">
          <source>Boolean flag indicating wether the output of &lt;code&gt;transform&lt;/code&gt; is a sparse matrix or a dense numpy array, which depends on the output of the individual transformers and the &lt;code&gt;sparse_threshold&lt;/code&gt; keyword.</source>
          <target state="translated">布尔值标志，指示 &lt;code&gt;transform&lt;/code&gt; 的输出是稀疏矩阵还是密集的numpy数组，这取决于各个转换器的输出和 &lt;code&gt;sparse_threshold&lt;/code&gt; 关键字。</target>
        </trans-unit>
        <trans-unit id="c9d24ce1f33e1a2da8b7d2e8d02488db3a1a0ff6" translate="yes" xml:space="preserve">
          <source>Boolean flag indicating whether the output of &lt;code&gt;transform&lt;/code&gt; is a sparse matrix or a dense numpy array, which depends on the output of the individual transformers and the &lt;code&gt;sparse_threshold&lt;/code&gt; keyword.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="dd883a95149825523369daab7f0f8ca0aca6e0ac" translate="yes" xml:space="preserve">
          <source>Boolean mask of inliers classified as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">分类为 &lt;code&gt;True&lt;/code&gt; 的Inlier的布尔布尔掩码。</target>
        </trans-unit>
        <trans-unit id="d2ef2625261226f9417113c581f3bab51f99fc11" translate="yes" xml:space="preserve">
          <source>Boolean mask or indices for test set.</source>
          <target state="translated">测试集的布尔掩码或指数。</target>
        </trans-unit>
        <trans-unit id="30de559b973b97451b964227c7184d452eca2270" translate="yes" xml:space="preserve">
          <source>Boolean mask or indices for training set.</source>
          <target state="translated">训练集的布尔掩码或指数。</target>
        </trans-unit>
        <trans-unit id="a38162711f2499e7a5519304c06ab030205ccd3e" translate="yes" xml:space="preserve">
          <source>Boolean mask or list of indices (as returned by the get_support member of feature selectors).</source>
          <target state="translated">布尔掩码或索引列表(由特征选择器的get_support成员返回)。</target>
        </trans-unit>
        <trans-unit id="c3ff5ac4f1442b62c49b82219fbca08a9aaf6fc5" translate="yes" xml:space="preserve">
          <source>Boolean thresholding of array-like or scipy.sparse matrix</source>
          <target state="translated">类数组或scipy.sparse矩阵的布尔阈值化。</target>
        </trans-unit>
        <trans-unit id="52747050c0ad2a1de070c473421346e630cdac3f" translate="yes" xml:space="preserve">
          <source>Both &amp;lsquo;ascii&amp;rsquo; and &amp;lsquo;unicode&amp;rsquo; use NFKD normalization from &lt;a href=&quot;https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize&quot;&gt;&lt;code&gt;unicodedata.normalize&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">'ascii'和'unicode'都使用&lt;a href=&quot;https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize&quot;&gt; &lt;code&gt;unicodedata.normalize&lt;/code&gt; 中的&lt;/a&gt; NFKD规范化。</target>
        </trans-unit>
        <trans-unit id="cc08d18c56138ef4e57769f94344c2fcddc0c71c" translate="yes" xml:space="preserve">
          <source>Both &lt;a href=&quot;../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt;&lt;code&gt;make_blobs&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt;&lt;code&gt;make_classification&lt;/code&gt;&lt;/a&gt; create multiclass datasets by allocating each class one or more normally-distributed clusters of points. &lt;a href=&quot;../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt;&lt;code&gt;make_blobs&lt;/code&gt;&lt;/a&gt; provides greater control regarding the centers and standard deviations of each cluster, and is used to demonstrate clustering. &lt;a href=&quot;../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt;&lt;code&gt;make_classification&lt;/code&gt;&lt;/a&gt; specialises in introducing noise by way of: correlated, redundant and uninformative features; multiple Gaussian clusters per class; and linear transformations of the feature space.</source>
          <target state="translated">&lt;a href=&quot;../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt; &lt;code&gt;make_blobs&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt; &lt;code&gt;make_classification&lt;/code&gt; &lt;/a&gt;都通过为每个类分配一个或多个正态分布的点簇来创建多类数据集。&lt;a href=&quot;../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt; &lt;code&gt;make_blobs&lt;/code&gt; &lt;/a&gt;提供有关每个聚类的中心和标准偏差的更好控制，并用于演示聚类。&lt;a href=&quot;../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt; &lt;code&gt;make_classification&lt;/code&gt; &lt;/a&gt;专门通过以下方式引入噪声：相关，冗余和非信息性特征；每个类别有多个高斯聚类；以及特征空间的线性变换。</target>
        </trans-unit>
        <trans-unit id="4b01e39314cd0b6e82319a49bae49952bbdf5790" translate="yes" xml:space="preserve">
          <source>Both &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; support &lt;code&gt;warm_start=True&lt;/code&gt; which allows you to add more estimators to an already fitted model.</source>
          <target state="translated">无论&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt;支持 &lt;code&gt;warm_start=True&lt;/code&gt; 该让你更估计添加到已拟合模型。</target>
        </trans-unit>
        <trans-unit id="0fdd261ec2b6eab839fe6617683b34303696fb85" translate="yes" xml:space="preserve">
          <source>Both &lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;SimpleImputer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;IterativeImputer&lt;/code&gt;&lt;/a&gt; can be used in a Pipeline as a way to build a composite estimator that supports imputation. See &lt;a href=&quot;../auto_examples/impute/plot_missing_values#sphx-glr-auto-examples-impute-plot-missing-values-py&quot;&gt;Imputing missing values before building an estimator&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cf6c6c4310ce5d3e1dff50dd933fcb1d3d517f90" translate="yes" xml:space="preserve">
          <source>Both &lt;a href=&quot;generated/sklearn.neural_network.mlpregressor#sklearn.neural_network.MLPRegressor&quot;&gt;&lt;code&gt;MLPRegressor&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neural_network.mlpclassifier#sklearn.neural_network.MLPClassifier&quot;&gt;&lt;code&gt;MLPClassifier&lt;/code&gt;&lt;/a&gt; use parameter &lt;code&gt;alpha&lt;/code&gt; for regularization (L2 regularization) term which helps in avoiding overfitting by penalizing weights with large magnitudes. Following plot displays varying decision function with value of alpha.</source>
          <target state="translated">既&lt;a href=&quot;generated/sklearn.neural_network.mlpregressor#sklearn.neural_network.MLPRegressor&quot;&gt; &lt;code&gt;MLPRegressor&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.neural_network.mlpclassifier#sklearn.neural_network.MLPClassifier&quot;&gt; &lt;code&gt;MLPClassifier&lt;/code&gt; &lt;/a&gt;使用参数 &lt;code&gt;alpha&lt;/code&gt; 为正则化（L2正则化）术语，其有助于避免通过与大幅度惩罚加权过度拟合。下图显示了具有alpha值的变化决策函数。</target>
        </trans-unit>
        <trans-unit id="b53b3117e47a61020f59f56f0ab84443cf5a1cc4" translate="yes" xml:space="preserve">
          <source>Both &lt;strong&gt;tf&lt;/strong&gt; and &lt;strong&gt;tf&amp;ndash;idf&lt;/strong&gt; can be computed as follows using &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">两个&lt;strong&gt;TF&lt;/strong&gt;和&lt;strong&gt;TF-IDF&lt;/strong&gt;可以计算使用如下&lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="4bf93c5f6114d3860df00831377e70ae0b54b0c9" translate="yes" xml:space="preserve">
          <source>Both Face Verification and Face Recognition are tasks that are typically performed on the output of a model trained to perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the OpenCV library. The LFW faces were extracted by this face detector from various online websites.</source>
          <target state="translated">人脸验证和人脸识别都是典型的任务,它们都是在训练成的模型的输出上进行人脸检测。最流行的人脸检测模型叫做Viola-Jones,是在OpenCV库中实现的。LFW人脸是由该人脸检测器从各种在线网站上提取的。</target>
        </trans-unit>
        <trans-unit id="ca7325084a8f64bcbbcfe176b54a08f8e59fb609" translate="yes" xml:space="preserve">
          <source>Both LDA and QDA can be derived from simple probabilistic models which model the class conditional distribution of the data \(P(X|y=k)\) for each class \(k\). Predictions can then be obtained by using Bayes&amp;rsquo; rule, for each training sample \(x \in \mathcal{R}^d\):</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="91e41139595583cca1f08b85e6f25218cb1d806a" translate="yes" xml:space="preserve">
          <source>Both LDA and QDA can be derived from simple probabilistic models which model the class conditional distribution of the data \(P(X|y=k)\) for each class \(k\). Predictions can then be obtained by using Bayes&amp;rsquo; rule:</source>
          <target state="translated">LDA和QDA都可以从简单的概率模型中得出，该模型为每个类\（k \）建模数据\（P（X | y = k）\）的类条件分布。然后可以使用贝叶斯规则获得预测：</target>
        </trans-unit>
        <trans-unit id="88ef4403ed65b61f6ce7d70445c003a80e194980" translate="yes" xml:space="preserve">
          <source>Both a large or small &lt;code&gt;leaf_size&lt;/code&gt; can lead to suboptimal query cost. For &lt;code&gt;leaf_size&lt;/code&gt; approaching 1, the overhead involved in traversing nodes can significantly slow query times. For &lt;code&gt;leaf_size&lt;/code&gt; approaching the size of the training set, queries become essentially brute force. A good compromise between these is &lt;code&gt;leaf_size = 30&lt;/code&gt;, the default value of the parameter.</source>
          <target state="translated">较大或较小的 &lt;code&gt;leaf_size&lt;/code&gt; 都可能导致查询成本欠佳。对于 &lt;code&gt;leaf_size&lt;/code&gt; 接近1的情况，遍历节点所涉及的开销可能会大大减慢查询时间。对于接近训练集大小的 &lt;code&gt;leaf_size&lt;/code&gt; ，查询本质上成为蛮力。两者之间的一个很好的折衷是参数的默认值 &lt;code&gt;leaf_size = 30&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="fa5698e1f194999c5b948025698f212260de6074" translate="yes" xml:space="preserve">
          <source>Both for the &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_people#sklearn.datasets.fetch_lfw_people&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_lfw_people&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_pairs#sklearn.datasets.fetch_lfw_pairs&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_lfw_pairs&lt;/code&gt;&lt;/a&gt; function it is possible to get an additional dimension with the RGB color channels by passing &lt;code&gt;color=True&lt;/code&gt;, in that case the shape will be &lt;code&gt;(2200, 2, 62, 47, 3)&lt;/code&gt;.</source>
          <target state="translated">对于&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_people#sklearn.datasets.fetch_lfw_people&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_lfw_people&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_pairs#sklearn.datasets.fetch_lfw_pairs&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_lfw_pairs&lt;/code&gt; &lt;/a&gt;函数，都可以通过传递 &lt;code&gt;color=True&lt;/code&gt; 获得RGB颜色通道的附加尺寸，在这种情况下，形状将为 &lt;code&gt;(2200, 2, 62, 47, 3)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f849c057a89abeca98a91c38c141c4422d4b9044" translate="yes" xml:space="preserve">
          <source>Both kernel ridge regression (KRR) and GPR learn a target function by employing internally the &amp;ldquo;kernel trick&amp;rdquo;. KRR learns a linear function in the space induced by the respective kernel which corresponds to a non-linear function in the original space. The linear function in the kernel space is chosen based on the mean-squared error loss with ridge regularization. GPR uses the kernel to define the covariance of a prior distribution over the target functions and uses the observed training data to define a likelihood function. Based on Bayes theorem, a (Gaussian) posterior distribution over target functions is defined, whose mean is used for prediction.</source>
          <target state="translated">内核岭回归（KRR）和GPR都通过内部使用&amp;ldquo;内核技巧&amp;rdquo;来学习目标函数。 KRR学习由相应内核诱导的空间中的线性函数，该函数对应于原始空间中的非线性函数。基于具有岭正则化的均方误差损失来选择内核空间中的线性函数。 GPR使用内核定义目标函数上先验分布的协方差，并使用观察到的训练数据定义似然函数。基于贝叶斯定理，定义了目标函数的（高斯）后验分布，其均值用于预测。</target>
        </trans-unit>
        <trans-unit id="23db30f840724e288d4f8f1371ac596206d41d32" translate="yes" xml:space="preserve">
          <source>Both kernel ridge regression (KRR) and Gaussian process regression (GPR) learn a target function by employing internally the &amp;ldquo;kernel trick&amp;rdquo;. KRR learns a linear function in the space induced by the respective kernel which corresponds to a non-linear function in the original space. The linear function in the kernel space is chosen based on the mean-squared error loss with ridge regularization. GPR uses the kernel to define the covariance of a prior distribution over the target functions and uses the observed training data to define a likelihood function. Based on Bayes theorem, a (Gaussian) posterior distribution over target functions is defined, whose mean is used for prediction.</source>
          <target state="translated">内核岭回归（KRR）和高斯过程回归（GPR）都通过内部采用&amp;ldquo;内核技巧&amp;rdquo;来学习目标函数。 KRR学习由相应内核诱导的空间中的线性函数，该函数对应于原始空间中的非线性函数。基于具有岭正则化的均方误差损失来选择内核空间中的线性函数。 GPR使用内核定义目标函数上先验分布的协方差，并使用观察到的训练数据定义似然函数。基于贝叶斯定理，定义了目标函数的（高斯）后验分布，其均值用于预测。</target>
        </trans-unit>
        <trans-unit id="a2ae11bd288fcc4a889903f5cd55e2c7dc5062c9" translate="yes" xml:space="preserve">
          <source>Both kernel ridge regression (KRR) and SVR learn a non-linear function by employing the kernel trick, i.e., they learn a linear function in the space induced by the respective kernel which corresponds to a non-linear function in the original space. They differ in the loss functions (ridge versus epsilon-insensitive loss). In contrast to SVR, fitting a KRR can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR at prediction-time.</source>
          <target state="translated">核脊回归(KRR)和SVR都是通过采用内核技巧来学习一个非线性函数,即它们在各自内核诱导的空间中学习一个线性函数,而这个线性函数对应的是原始空间中的一个非线性函数。它们在损失函数上有所不同(脊与epsilon不敏感损失)。与SVR相比,拟合KRR可以在闭合形式中完成,通常对于中等规模的数据集来说速度更快。另一方面,学习的模型是非稀疏的,因此在预测时比SVR慢。</target>
        </trans-unit>
        <trans-unit id="9f6802fa6da263e373ed1a0b728e154415b2fd58" translate="yes" xml:space="preserve">
          <source>Both kinds of calibration can fix this issue and yield nearly identical results. This shows that sigmoid calibration can deal with situations where the calibration curve of the base classifier is sigmoid (e.g., for LinearSVC) but not where it is transposed-sigmoid (e.g., Gaussian naive Bayes).</source>
          <target state="translated">两种校准都可以解决这个问题,并得到几乎相同的结果。这表明,sigmoid校准可以处理基础分类器的校准曲线是sigmoid的情况(如LinearSVC),但不能处理转置-sigmoid的情况(如Gaussian naive Bayes)。</target>
        </trans-unit>
        <trans-unit id="708135c11a370a819f586687e493e8e334b863bb" translate="yes" xml:space="preserve">
          <source>Both linear models have linear decision boundaries (intersecting hyperplanes) while the non-linear kernel models (polynomial or Gaussian RBF) have more flexible non-linear decision boundaries with shapes that depend on the kind of kernel and its parameters.</source>
          <target state="translated">两种线性模型的决策边界都是线性的(相交的超平面),而非线性内核模型(多项式或高斯RBF)的非线性决策边界更加灵活,其形状取决于内核的种类及其参数。</target>
        </trans-unit>
        <trans-unit id="26e0a0e806824f91854b61cb523786b2b1be2f09" translate="yes" xml:space="preserve">
          <source>Both loaders and fetchers functions return a &lt;a href=&quot;../modules/generated/sklearn.utils.bunch#sklearn.utils.Bunch&quot;&gt;&lt;code&gt;sklearn.utils.Bunch&lt;/code&gt;&lt;/a&gt; object holding at least two items: an array of shape &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; with key &lt;code&gt;data&lt;/code&gt; (except for 20newsgroups) and a numpy array of length &lt;code&gt;n_samples&lt;/code&gt;, containing the target values, with key &lt;code&gt;target&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fc56722e0a950bb51331dfd1a0000b2f92f9cd59" translate="yes" xml:space="preserve">
          <source>Both loaders and fetchers functions return a dictionary-like object holding at least two items: an array of shape &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; with key &lt;code&gt;data&lt;/code&gt; (except for 20newsgroups) and a numpy array of length &lt;code&gt;n_samples&lt;/code&gt;, containing the target values, with key &lt;code&gt;target&lt;/code&gt;.</source>
          <target state="translated">既装载机和取程序函数返回一个类字典对象保持至少两个项目：形状的阵列 &lt;code&gt;n_samples&lt;/code&gt; * &lt;code&gt;n_features&lt;/code&gt; 与关键 &lt;code&gt;data&lt;/code&gt; （除了20newsgroups）和长度的numpy的阵列 &lt;code&gt;n_samples&lt;/code&gt; ，将含有目标值，与关键 &lt;code&gt;target&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c429ee44d91d629f81b84bf7db1e8151de09efd7" translate="yes" xml:space="preserve">
          <source>Both methods are compared in a regression problem using a BayesianRidge as supervised estimator.</source>
          <target state="translated">在使用BayesianRidge作为监督估计器的回归问题中,两种方法进行了比较。</target>
        </trans-unit>
        <trans-unit id="1f719d18cf26f246fcd8f594b634dc25f50e93c4" translate="yes" xml:space="preserve">
          <source>Both models are able to rank policyholders by risky-ness significantly better than chance although they are also both far from perfect due to the natural difficulty of the prediction problem from few features.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9c567880fbb130b2f00326a7eed156cd91692677" translate="yes" xml:space="preserve">
          <source>Both models essentially estimate a Gaussian with a low-rank covariance matrix. Because both models are probabilistic they can be integrated in more complex models, e.g. Mixture of Factor Analysers. One gets very different models (e.g. &lt;a href=&quot;generated/sklearn.decomposition.fastica#sklearn.decomposition.FastICA&quot;&gt;&lt;code&gt;FastICA&lt;/code&gt;&lt;/a&gt;) if non-Gaussian priors on the latent variables are assumed.</source>
          <target state="translated">两种模型本质上都估计具有低秩协方差矩阵的高斯模型。由于这两个模型都是概率模型，因此可以将它们集成到更复杂的模型中，例如因子分析仪的混合物。如果假设潜在变量的非高斯先验，则得到的模型会非常不同（例如&lt;a href=&quot;generated/sklearn.decomposition.fastica#sklearn.decomposition.FastICA&quot;&gt; &lt;code&gt;FastICA&lt;/code&gt; &lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="a62e9a7de64eaa33971b5c3675cc0b062a65784c" translate="yes" xml:space="preserve">
          <source>Both models have access to five components with which to fit the data. Note that the Expectation Maximisation model will necessarily use all five components while the Variational Inference model will effectively only use as many as are needed for a good fit. Here we can see that the Expectation Maximisation model splits some components arbitrarily, because it is trying to fit too many components, while the Dirichlet Process model adapts it number of state automatically.</source>
          <target state="translated">这两种模型都可以使用五个组件来拟合数据。请注意,期望值最大化模型必然会使用所有五个组件,而变量推断模型将有效地只使用良好拟合所需的组件。在这里我们可以看到,期望最大化模型任意分割一些组件,因为它试图拟合太多的组件,而Dirichlet Process模型则自动适应它的状态数。</target>
        </trans-unit>
        <trans-unit id="5b17a14614025b8fc55b61f491f7a477ebf75742" translate="yes" xml:space="preserve">
          <source>Both scores have positive values between 0.0 and 1.0, larger values being desirable.</source>
          <target state="translated">这两个分数的正值都在0.0~1.0之间,数值越大越好。</target>
        </trans-unit>
        <trans-unit id="be67d7e7cbf7e3b7b92ab971146cc493ee899201" translate="yes" xml:space="preserve">
          <source>Box-Cox can only be applied to strictly positive data. In both methods, the transformation is parameterized by \(\lambda\), which is determined through maximum likelihood estimation. Here is an example of using Box-Cox to map samples drawn from a lognormal distribution to a normal distribution:</source>
          <target state="translated">Box-Cox只能应用于严格的正数据。在这两种方法中,变换的参数是通过最大似然估计确定的。下面是一个使用Box-Cox将样本从对数正态分布映射到正态分布的例子。</target>
        </trans-unit>
        <trans-unit id="3b31bca12137b60e3b02c5fa6fcbe997bf063055" translate="yes" xml:space="preserve">
          <source>Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data.</source>
          <target state="translated">Box-Cox要求输入数据严格为正,而Yeo-Johnson支持正或负数据。</target>
        </trans-unit>
        <trans-unit id="3a33122410e3d55ce36b5d7a297f38716ddc9fb2" translate="yes" xml:space="preserve">
          <source>BrayCurtisDistance</source>
          <target state="translated">BrayCurtisDistance</target>
        </trans-unit>
        <trans-unit id="9567cb56bbf5a464a471c7e7c0603402f701c1b5" translate="yes" xml:space="preserve">
          <source>Breiman, &amp;ldquo;Arcing Classifiers&amp;rdquo;, Annals of Statistics 1998.</source>
          <target state="translated">Breiman，&amp;ldquo; Arcing分类器&amp;rdquo;，《统计年鉴》，1998年。</target>
        </trans-unit>
        <trans-unit id="b92ea166bdb68582b38ba5d28898e88166331fb5" translate="yes" xml:space="preserve">
          <source>Breiman, &amp;ldquo;Random Forests&amp;rdquo;, Machine Learning, 45(1), 5-32, 2001.</source>
          <target state="translated">Breiman，&amp;ldquo;随机森林&amp;rdquo;，机器学习，45（1），5-32，2001。</target>
        </trans-unit>
        <trans-unit id="e706a24019de3d6255ea9f7b340895835c77d481" translate="yes" xml:space="preserve">
          <source>Brendan J. Frey and Delbert Dueck, &amp;ldquo;Clustering by Passing Messages Between Data Points&amp;rdquo;, Science Feb. 2007</source>
          <target state="translated">Brendan J. Frey和Delbert Dueck，&amp;ldquo;通过在数据点之间传递消息进行聚类&amp;rdquo;，《科学》，2007年2月</target>
        </trans-unit>
        <trans-unit id="91c2780c7ce208d81dbc70c79b98b19c560e01f7" translate="yes" xml:space="preserve">
          <source>Breunig, Kriegel, Ng, and Sander (2000) &lt;a href=&quot;http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf&quot;&gt;LOF: identifying density-based local outliers.&lt;/a&gt; Proc. ACM SIGMOD</source>
          <target state="translated">Breunig，Kriegel，Ng和Sander（2000）&lt;a href=&quot;http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf&quot;&gt;LOF：识别基于密度的局部离群值。&lt;/a&gt;进程 ACM SIGMOD</target>
        </trans-unit>
        <trans-unit id="b62edbfb07bbbc25ba8cf8bbc25ed044eba7bbe5" translate="yes" xml:space="preserve">
          <source>Breunig, M. M., Kriegel, H. P., Ng, R. T., &amp;amp; Sander, J. (2000, May). LOF: identifying density-based local outliers. In ACM sigmod record.</source>
          <target state="translated">Breunig，MM，Kriegel，HP，Ng，RT，＆Sander，J.（2000年5月）。LOF：确定基于密度的局部离群值。在ACM sigmod记录中。</target>
        </trans-unit>
        <trans-unit id="f5333384a370e2b255c6a3cf2b193f53e9cafb20" translate="yes" xml:space="preserve">
          <source>Briefly, a first-order Taylor approximation says that \(l(z) \approx l(a) + (z - a) \frac{\partial l(a)}{\partial a}\). Here, \(z\) corresponds to \(F_{m - 1}(x_i) + h_m(x_i)\), and \(a\) corresponds to \(F_{m-1}(x_i)\)</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="fb0165005c94e23be947347cfac674726ddcc8f3" translate="yes" xml:space="preserve">
          <source>Brier score</source>
          <target state="translated">Brier得分</target>
        </trans-unit>
        <trans-unit id="ce9563b2203c4e120af29d083264cdec661286f1" translate="yes" xml:space="preserve">
          <source>Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010). The balanced accuracy and its posterior distribution. Proceedings of the 20th International Conference on Pattern Recognition, 3121-24.</source>
          <target state="translated">Brodersen,K.H.;Ong,C.S.;Stephan,K.E.;Buhmann,J.M.(2010).平衡精度及其后验分布。第20届国际模式识别会议论文集,3121-24。</target>
        </trans-unit>
        <trans-unit id="3a47350b7d7cb691a01ccff5ad5d3aca87290c28" translate="yes" xml:space="preserve">
          <source>Brown</source>
          <target state="translated">Brown</target>
        </trans-unit>
        <trans-unit id="5b66173631e06f3f28d6125b3cb26a28cb7fca86" translate="yes" xml:space="preserve">
          <source>Build a Bagging ensemble of estimators from the training</source>
          <target state="translated">从训练中建立一个估计器的袋装集合。</target>
        </trans-unit>
        <trans-unit id="794678e50f47205b9017d4e509f3518d6f5fadf5" translate="yes" xml:space="preserve">
          <source>Build a Bagging ensemble of estimators from the training set (X, y).</source>
          <target state="translated">从训练集(X,y)建立一个估计器的Bagging合集。</target>
        </trans-unit>
        <trans-unit id="187f33f95926319c2456d01efd577acf94dfa6ae" translate="yes" xml:space="preserve">
          <source>Build a CF Tree for the input data.</source>
          <target state="translated">为输入的数据建立一个CF树。</target>
        </trans-unit>
        <trans-unit id="1367324c6e1505253779ffa63a9ab3c72dfbf04f" translate="yes" xml:space="preserve">
          <source>Build a HTML representation of an estimator.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1e09fb3267f6b0d5980c9be2a9dffb892209f693" translate="yes" xml:space="preserve">
          <source>Build a boosted classifier from the training set (X, y).</source>
          <target state="translated">从训练集(X,y)建立一个提升分类器。</target>
        </trans-unit>
        <trans-unit id="bb1580cf6761f3cb6ad38eda4b239ef5f0001320" translate="yes" xml:space="preserve">
          <source>Build a boosted regressor from the training set (X, y).</source>
          <target state="translated">从训练集(X,y)建立一个提升的回归器。</target>
        </trans-unit>
        <trans-unit id="250228e6d05087ab14d0d947456ba0aa59ea7050" translate="yes" xml:space="preserve">
          <source>Build a contingency matrix describing the relationship between labels.</source>
          <target state="translated">建立一个描述标签之间关系的应急矩阵。</target>
        </trans-unit>
        <trans-unit id="d74890ceefdff850d4364b5427ecc2c4535dee78" translate="yes" xml:space="preserve">
          <source>Build a decision tree classifier from the training set (X, y).</source>
          <target state="translated">从训练集(X,y)建立一个决策树分类器。</target>
        </trans-unit>
        <trans-unit id="2b1b91e53bab7f86a619b51147d8ac61611972f5" translate="yes" xml:space="preserve">
          <source>Build a decision tree regressor from the training set (X, y).</source>
          <target state="translated">从训练集(X,y)建立决策树回归器。</target>
        </trans-unit>
        <trans-unit id="255fad2483c531d4d8beb88f94f03694ee2b25aa" translate="yes" xml:space="preserve">
          <source>Build a forest of trees from the training set (X, y).</source>
          <target state="translated">从训练集(X,y)建立一个树林。</target>
        </trans-unit>
        <trans-unit id="54ca7aba833ca6776a4b555adf51f52c37364032" translate="yes" xml:space="preserve">
          <source>Build a text report showing the main classification metrics</source>
          <target state="translated">建立一个文本报告,显示主要的分类指标。</target>
        </trans-unit>
        <trans-unit id="0acb360cebaf298906fb4db3eabfa00d9453c9d3" translate="yes" xml:space="preserve">
          <source>Build a text report showing the main classification metrics.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="02e35795301c2dfa78447e621bba28f5e7d78fd9" translate="yes" xml:space="preserve">
          <source>Build a text report showing the rules of a decision tree.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="657256d2b304ff4131eae140a5931256c38a3879" translate="yes" xml:space="preserve">
          <source>Build or fetch the effective stop words list</source>
          <target state="translated">建立或获取有效的停顿词列表</target>
        </trans-unit>
        <trans-unit id="c274aea56fd427bf062945c347f3184c61ce0dda" translate="yes" xml:space="preserve">
          <source>Build or fetch the effective stop words list.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1696170a9053ac1d87e48bfb4ce1d22a7e994777" translate="yes" xml:space="preserve">
          <source>Building a pipeline</source>
          <target state="translated">建立一个管道</target>
        </trans-unit>
        <trans-unit id="d58a993cf5326c10970b9b4db170c35508c44151" translate="yes" xml:space="preserve">
          <source>Bunch objects are sometimes used as an output for functions and methods. They extend dictionaries by enabling values to be accessed by key, &lt;code&gt;bunch[&quot;value_key&quot;]&lt;/code&gt;, or by an attribute, &lt;code&gt;bunch.value_key&lt;/code&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="6b9d2f152fc732c61d6802508ad1d27938fd60fa" translate="yes" xml:space="preserve">
          <source>By &lt;strong&gt;averaging&lt;/strong&gt; the estimates of predictive ability over several randomized trees one can &lt;strong&gt;reduce the variance&lt;/strong&gt; of such an estimate and use it for feature selection. This is known as the mean decrease in impurity, or MDI. Refer to &lt;a href=&quot;#l2014&quot; id=&quot;id7&quot;&gt;[L2014]&lt;/a&gt; for more information on MDI and feature importance evaluation with Random Forests.</source>
          <target state="translated">通过对几棵随机树上的预测能力估计值求&lt;strong&gt;平均&lt;/strong&gt;，可以&lt;strong&gt;减少&lt;/strong&gt;这种估计值&lt;strong&gt;的方差&lt;/strong&gt;，并将其用于特征选择。这称为杂质的平均减少量或MDI。有关随机森林的MDI和功能重要性评估的更多信息，请参见&lt;a href=&quot;#l2014&quot; id=&quot;id7&quot;&gt;[L2014]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4271aec9443f091b8f5d73284775ad1de7a16657" translate="yes" xml:space="preserve">
          <source>By contrast, in &lt;strong&gt;boosting methods&lt;/strong&gt;, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.</source>
          <target state="translated">相比之下，在&lt;strong&gt;增强方法中&lt;/strong&gt;，基础估计器是按顺序构建的，并且尝试减小组合估计器的偏差。这样做的动机是结合几个弱模型以产生强大的整体。</target>
        </trans-unit>
        <trans-unit id="6c32bdf454c04c997b80ade2a7001005f06bf56a" translate="yes" xml:space="preserve">
          <source>By default \(\alpha_1 = \alpha_2 = \lambda_1 = \lambda_2 = 10^{-6}\).</source>
          <target state="translated">默认情况下((alpha_1=alpha_2=lambda_1=lambda_2=10^{-6})\。</target>
        </trans-unit>
        <trans-unit id="471aed0559b4b66da1109f2dd8bd4a3412768eaf" translate="yes" xml:space="preserve">
          <source>By default all available workers will be used (&lt;code&gt;n_jobs=-1&lt;/code&gt;) unless the caller passes an explicit value for the &lt;code&gt;n_jobs&lt;/code&gt; parameter.</source>
          <target state="translated">默认情况下，将使用所有可用的工作程序（ &lt;code&gt;n_jobs=-1&lt;/code&gt; ），除非调用者为 &lt;code&gt;n_jobs&lt;/code&gt; 参数传递一个显式值。</target>
        </trans-unit>
        <trans-unit id="5f6c87aa078eb9e4a070352402cf9ac4b38861b8" translate="yes" xml:space="preserve">
          <source>By default no shuffling occurs, including for the (stratified) K fold cross- validation performed by specifying &lt;code&gt;cv=some_integer&lt;/code&gt; to &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt;, grid search, etc. Keep in mind that &lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt;&lt;code&gt;train_test_split&lt;/code&gt;&lt;/a&gt; still returns a random split.</source>
          <target state="translated">默认情况下，不会发生混洗，包括通过指定 &lt;code&gt;cv=some_integer&lt;/code&gt; 到&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;，网格搜索等进行的（分层）K折交叉验证。请记住，&lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt; &lt;code&gt;train_test_split&lt;/code&gt; &lt;/a&gt;仍然返回随机分割。</target>
        </trans-unit>
        <trans-unit id="e7a2fba0300cf0e1fb0a656c1e5e19ff70aee869" translate="yes" xml:space="preserve">
          <source>By default the data dir is set to a folder named &amp;lsquo;scikit_learn_data&amp;rsquo; in the user home folder.</source>
          <target state="translated">默认情况下，数据目录设置为用户主文件夹中名为&amp;ldquo; scikit_learn_data&amp;rdquo;的文件夹。</target>
        </trans-unit>
        <trans-unit id="78999f1442e19c854d03f2d5d120c491c2044f43" translate="yes" xml:space="preserve">
          <source>By default the estimator&amp;rsquo;s &lt;code&gt;score&lt;/code&gt; method is used to compute the individual scores.</source>
          <target state="translated">默认情况下，估算器的 &lt;code&gt;score&lt;/code&gt; 方法用于计算各个得分。</target>
        </trans-unit>
        <trans-unit id="4b5dc2d778b41c185a986f8e9ad3bff91cf8ed7b" translate="yes" xml:space="preserve">
          <source>By default the following backends are available:</source>
          <target state="translated">默认情况下,以下后端可用。</target>
        </trans-unit>
        <trans-unit id="c9a567aacd54237b7723e5584e83694cffe0556c" translate="yes" xml:space="preserve">
          <source>By default the gradient calculation algorithm uses Barnes-Hut approximation running in O(NlogN) time. method=&amp;rsquo;exact&amp;rsquo; will run on the slower, but exact, algorithm in O(N^2) time. The exact algorithm should be used when nearest-neighbor errors need to be better than 3%. However, the exact method cannot scale to millions of examples.</source>
          <target state="translated">默认情况下，梯度计算算法使用以O（NlogN）时间运行的Barnes-Hut逼近。method ='exact'将在O（N ^ 2）时间内以较慢但精确的算法运行。当最近邻居误差需要大于3％时，应使用精确算法。但是，确切的方法无法扩展到数百万个示例。</target>
        </trans-unit>
        <trans-unit id="572245e5c8e088605558017246250c8893d52627" translate="yes" xml:space="preserve">
          <source>By default the order will be determined by the order of columns in the label matrix Y.:</source>
          <target state="translated">默认情况下,顺序将由标签矩阵Y中列的顺序决定。</target>
        </trans-unit>
        <trans-unit id="cab46542ee630d0794253cdc9adb3976a1835662" translate="yes" xml:space="preserve">
          <source>By default the output is one-hot encoded into a sparse matrix (See &lt;a href=&quot;#preprocessing-categorical-features&quot;&gt;Encoding categorical features&lt;/a&gt;) and this can be configured with the &lt;code&gt;encode&lt;/code&gt; parameter. For each feature, the bin edges are computed during &lt;code&gt;fit&lt;/code&gt; and together with the number of bins, they will define the intervals. Therefore, for the current example, these intervals are defined as:</source>
          <target state="translated">默认情况下，将输出一键编码到稀疏矩阵中（请参阅&lt;a href=&quot;#preprocessing-categorical-features&quot;&gt;Encoding categorical features&lt;/a&gt;），并且可以使用 &lt;code&gt;encode&lt;/code&gt; 参数进行配置。对于每个特征，将在 &lt;code&gt;fit&lt;/code&gt; 过程中计算仓边，并与仓数一起定义间隔。因此，对于当前示例，这些间隔定义为：</target>
        </trans-unit>
        <trans-unit id="7638d24983f6f0c8d02846709e02c43bfb3124b9" translate="yes" xml:space="preserve">
          <source>By default, &lt;a href=&quot;generated/sklearn.decomposition.minibatchdictionarylearning#sklearn.decomposition.MiniBatchDictionaryLearning&quot;&gt;&lt;code&gt;MiniBatchDictionaryLearning&lt;/code&gt;&lt;/a&gt; divides the data into mini-batches and optimizes in an online manner by cycling over the mini-batches for the specified number of iterations. However, at the moment it does not implement a stopping condition.</source>
          <target state="translated">默认情况下，&lt;a href=&quot;generated/sklearn.decomposition.minibatchdictionarylearning#sklearn.decomposition.MiniBatchDictionaryLearning&quot;&gt; &lt;code&gt;MiniBatchDictionaryLearning&lt;/code&gt; &lt;/a&gt;将数据划分为多个迷你批，并通过在迷你批上循环指定的迭代次数以在线方式进行优化。但是，目前还没有实现停止条件。</target>
        </trans-unit>
        <trans-unit id="2eb2435c3e98faf455297cd059d456431f056aed" translate="yes" xml:space="preserve">
          <source>By default, &lt;code&gt;float16&lt;/code&gt; results are computed using &lt;code&gt;float32&lt;/code&gt; intermediates for extra precision.</source>
          <target state="translated">默认情况下，使用 &lt;code&gt;float32&lt;/code&gt; 中间体计算 &lt;code&gt;float16&lt;/code&gt; 结果可获得更高的精度。</target>
        </trans-unit>
        <trans-unit id="06b991078a7a86c59d05afe16f66234f800e038c" translate="yes" xml:space="preserve">
          <source>By default, LocalOutlierFactor is only meant to be used for outlier detection (novelty=False). Set novelty to True if you want to use LocalOutlierFactor for novelty detection. In this case be aware that that you should only use predict, decision_function and score_samples on new unseen data and not on the training set.</source>
          <target state="translated">默认情况下,LocalOutlierFactor只用于离群值检测(novelty=False)。如果你想使用LocalOutlierFactor进行新奇点检测,请将novelty设置为True。在这种情况下,请注意,你应该只在新的未见数据上使用predict,decision_function和score_samples,而不是训练集。</target>
        </trans-unit>
        <trans-unit id="2fb7ae3e457e09c80061e4212540f2949867d13f" translate="yes" xml:space="preserve">
          <source>By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.</source>
          <target state="translated">默认情况下,它执行广义交叉验证,这是一种高效的留一漏一交叉验证的形式。</target>
        </trans-unit>
        <trans-unit id="76317967f826fa82a2b4a34c8b9f11ee88f99afc" translate="yes" xml:space="preserve">
          <source>By default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation. Currently, only the n_features &amp;gt; n_samples case is handled efficiently.</source>
          <target state="translated">默认情况下，它执行通用交叉验证，这是一种有效的&amp;ldquo;留一法&amp;rdquo;交叉验证的形式。当前，仅有效处理n_features&amp;gt; n_samples种情况。</target>
        </trans-unit>
        <trans-unit id="c04cdaf41bfb70ed4b9c123ddaef4117557b0e3d" translate="yes" xml:space="preserve">
          <source>By default, only the specified columns in &lt;code&gt;transformers&lt;/code&gt; are transformed and combined in the output, and the non-specified columns are dropped. (default of &lt;code&gt;'drop'&lt;/code&gt;). By specifying &lt;code&gt;remainder='passthrough'&lt;/code&gt;, all remaining columns that were not specified in &lt;code&gt;transformers&lt;/code&gt; will be automatically passed through. This subset of columns is concatenated with the output of the transformers. By setting &lt;code&gt;remainder&lt;/code&gt; to be an estimator, the remaining non-specified columns will use the &lt;code&gt;remainder&lt;/code&gt; estimator. The estimator must support &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;transform&lt;/a&gt;.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="15494fd5cbda86aa1f330192fb9882a6711f9c4e" translate="yes" xml:space="preserve">
          <source>By default, only the specified columns in &lt;code&gt;transformers&lt;/code&gt; are transformed and combined in the output, and the non-specified columns are dropped. (default of &lt;code&gt;'drop'&lt;/code&gt;). By specifying &lt;code&gt;remainder='passthrough'&lt;/code&gt;, all remaining columns that were not specified in &lt;code&gt;transformers&lt;/code&gt; will be automatically passed through. This subset of columns is concatenated with the output of the transformers. By setting &lt;code&gt;remainder&lt;/code&gt; to be an estimator, the remaining non-specified columns will use the &lt;code&gt;remainder&lt;/code&gt; estimator. The estimator must support &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;transform&lt;/a&gt;. Note that using this feature requires that the DataFrame columns input at &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-fit&quot;&gt;fit&lt;/a&gt; and &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-transform&quot;&gt;transform&lt;/a&gt; have identical order.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7750d963aa2e941a76e470078ac6c086adc6dcda" translate="yes" xml:space="preserve">
          <source>By default, only the specified columns in &lt;code&gt;transformers&lt;/code&gt; are transformed and combined in the output, and the non-specified columns are dropped. (default of &lt;code&gt;'drop'&lt;/code&gt;). By specifying &lt;code&gt;remainder='passthrough'&lt;/code&gt;, all remaining columns that were not specified in &lt;code&gt;transformers&lt;/code&gt; will be automatically passed through. This subset of columns is concatenated with the output of the transformers. By setting &lt;code&gt;remainder&lt;/code&gt; to be an estimator, the remaining non-specified columns will use the &lt;code&gt;remainder&lt;/code&gt; estimator. The estimator must support &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;transform&lt;/code&gt;.</source>
          <target state="translated">默认情况下，仅 &lt;code&gt;transformers&lt;/code&gt; 中指定的列会在输出中进行转换和合并，未指定的列将被删除。 （默认为 &lt;code&gt;'drop'&lt;/code&gt; ）。通过指定 &lt;code&gt;remainder='passthrough'&lt;/code&gt; ，所有未在 &lt;code&gt;transformers&lt;/code&gt; 指定的列将自动通过。列的此子集与变压器的输出串联在一起。通过将 &lt;code&gt;remainder&lt;/code&gt; 设置为估算器，其余未指定的列将使用 &lt;code&gt;remainder&lt;/code&gt; 估算器。估计器必须支持 &lt;code&gt;fit&lt;/code&gt; 和 &lt;code&gt;transform&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4a782d670d6fc36b236c4a8940c016b292a90c5d" translate="yes" xml:space="preserve">
          <source>By default, parameter search uses the &lt;code&gt;score&lt;/code&gt; function of the estimator to evaluate a parameter setting. These are the &lt;a href=&quot;generated/sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt;&lt;code&gt;sklearn.metrics.accuracy_score&lt;/code&gt;&lt;/a&gt; for classification and &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;sklearn.metrics.r2_score&lt;/code&gt;&lt;/a&gt; for regression. For some applications, other scoring functions are better suited (for example in unbalanced classification, the accuracy score is often uninformative). An alternative scoring function can be specified via the &lt;code&gt;scoring&lt;/code&gt; parameter to &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt;&lt;code&gt;RandomizedSearchCV&lt;/code&gt;&lt;/a&gt; and many of the specialized cross-validation tools described below. See &lt;a href=&quot;model_evaluation#scoring-parameter&quot;&gt;The scoring parameter: defining model evaluation rules&lt;/a&gt; for more details.</source>
          <target state="translated">默认情况下，参数搜索使用估计器的 &lt;code&gt;score&lt;/code&gt; 功能来评估参数设置。这些是用于分类的&lt;a href=&quot;generated/sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt; &lt;code&gt;sklearn.metrics.accuracy_score&lt;/code&gt; &lt;/a&gt;和用于回归的&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;sklearn.metrics.r2_score&lt;/code&gt; &lt;/a&gt;。对于某些应用程序，其他计分功能更适合（例如，在不平衡分类中，准确性得分通常不具信息性）。可以通过&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt; &lt;code&gt;RandomizedSearchCV&lt;/code&gt; &lt;/a&gt;的 &lt;code&gt;scoring&lt;/code&gt; 参数以及下面介绍的许多专用交叉验证工具来指定替代评分功能。有关更多详细信息，请参阅&lt;a href=&quot;model_evaluation#scoring-parameter&quot;&gt;评分参数：定义模型评估规则&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="14b262313cef67a36d56fb94a78ec0ffe131d0f9" translate="yes" xml:space="preserve">
          <source>By default, the &amp;lsquo;recursion&amp;rsquo; method is used on tree-based estimators that support it, and &amp;lsquo;brute&amp;rsquo; is used for the rest.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="cd89643870e89d7d33668ed08a7b04e11606d0f7" translate="yes" xml:space="preserve">
          <source>By default, the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; uses a 3-fold cross-validation. However, if it detects that a classifier is passed, rather than a regressor, it uses a stratified 3-fold. The default will change to a 5-fold cross-validation in version 0.22.</source>
          <target state="translated">默认情况下，&lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt;使用3倍交叉验证。但是，如果检测到通过了分类器而不是回归器，则使用分层的3折。默认值将在0.22版中更改为5倍交叉验证。</target>
        </trans-unit>
        <trans-unit id="d01628e3c1ff7c3c146d98c4400e308b8d21e62e" translate="yes" xml:space="preserve">
          <source>By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the &lt;code&gt;categories&lt;/code&gt; manually.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="aabd7b3f7cca56e260a395d6492aceaf31d56d74" translate="yes" xml:space="preserve">
          <source>By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the &lt;code&gt;categories&lt;/code&gt; manually. The OneHotEncoder previously assumed that the input features take on values in the range [0, max(values)). This behaviour is deprecated.</source>
          <target state="translated">默认情况下，编码器会根据每个功能中的唯一值得出类别。或者，您也可以手动指定 &lt;code&gt;categories&lt;/code&gt; 。OneHotEncoder先前假设输入要素的取值范围为[0，max（values））。不建议使用此行为。</target>
        </trans-unit>
        <trans-unit id="e22c55145c4d94c4a5b62e5b6f40a60aa6e4907e" translate="yes" xml:space="preserve">
          <source>By default, the initial model \(F_{0}\) is chosen as the constant that minimizes the loss: for a least-squares loss, this is the empirical mean of the target values. The initial model can also be specified via the &lt;code&gt;init&lt;/code&gt; argument.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="e16205e1ed0a918834ac9076544d9991fc9c4ca6" translate="yes" xml:space="preserve">
          <source>By default, the input is checked to be a non-empty 2D array containing only finite values. If the dtype of the array is object, attempt converting to float, raising on failure.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="0397cff741cdca99d6793cf68ca8a230e768dce2" translate="yes" xml:space="preserve">
          <source>By default, the input is converted to an at least 2D numpy array. If the dtype of the array is object, attempt converting to float, raising on failure.</source>
          <target state="translated">默认情况下,输入的数据会被转换为至少一个2D的numpy数组。如果数组的dtype是object,则尝试转换为float,失败时引发。</target>
        </trans-unit>
        <trans-unit id="9dfabb9221fc44d77597df9d22d00887ce88c6cb" translate="yes" xml:space="preserve">
          <source>By default, the provided functions are checked at each fit to be the inverse of each other. However, it is possible to bypass this checking by setting &lt;code&gt;check_inverse&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;:</source>
          <target state="translated">默认情况下，将在每次拟合时检查提供的函数是否彼此相反。但是，可以通过将 &lt;code&gt;check_inverse&lt;/code&gt; 设置为 &lt;code&gt;False&lt;/code&gt; 来绕过此检查：</target>
        </trans-unit>
        <trans-unit id="761cb5c85c42ad01cf3595b928efe71009e68e6e" translate="yes" xml:space="preserve">
          <source>By default, the score computed at each CV iteration is the &lt;code&gt;score&lt;/code&gt; method of the estimator. It is possible to change this by using the scoring parameter:</source>
          <target state="translated">默认情况下，在每次CV迭代中计算的 &lt;code&gt;score&lt;/code&gt; 是估算器的分数方法。可以通过使用评分参数来更改此设置：</target>
        </trans-unit>
        <trans-unit id="61fe756b7e3e334c670a2007b6939f399986a294" translate="yes" xml:space="preserve">
          <source>By default, the values each feature can take is inferred automatically from the dataset and can be found in the &lt;code&gt;categories_&lt;/code&gt; attribute:</source>
          <target state="translated">默认情况下，每个功能可以采用的值是从数据集中自动推断出的，并且可以在 &lt;code&gt;categories_&lt;/code&gt; 属性中找到：</target>
        </trans-unit>
        <trans-unit id="588af344f8a3913fdf9179a25370573310fcee34" translate="yes" xml:space="preserve">
          <source>By default, zero-mean, unit-variance normalization is applied to the transformed data.</source>
          <target state="translated">默认情况下,对转换后的数据采用零均值、单位方差归一化。</target>
        </trans-unit>
        <trans-unit id="11243048cfb311c05ea90a5cca047fa81467c316" translate="yes" xml:space="preserve">
          <source>By definition a confusion matrix \(C\) is such that \(C_{i, j}\) is equal to the number of observations known to be in group \(i\) and predicted to be in group \(j\).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c874644c0e92eeddd8de26271a422d136d288139" translate="yes" xml:space="preserve">
          <source>By definition a confusion matrix \(C\) is such that \(C_{i, j}\) is equal to the number of observations known to be in group \(i\) but predicted to be in group \(j\).</source>
          <target state="translated">根据定义,混淆矩阵(C\)是这样的,即(C_{i,j})等于已知属于(i)组但预测属于(j)组的观测值的数量。</target>
        </trans-unit>
        <trans-unit id="7edbd0cd8c2bac6d61783ee719a48e462c1cfcf6" translate="yes" xml:space="preserve">
          <source>By definition, entry \(i, j\) in a confusion matrix is the number of observations actually in group \(i\), but predicted to be in group \(j\). Here is an example:</source>
          <target state="translated">根据定义,混淆矩阵中的条目(i,j)是指实际在组(i)中,但预测在组(j)中的观测值的数量。下面是一个例子。</target>
        </trans-unit>
        <trans-unit id="70e3cb436db4ad1c973fa29cd922d3f24b1b4676" translate="yes" xml:space="preserve">
          <source>By imposing a positive (increasing) or negative (decreasing) constraint on the features during the learning process, the estimator is able to properly follow the general trend instead of being subject to the variations.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="1dfab6b69e192f9e6ae185588957d2e4ef21b660" translate="yes" xml:space="preserve">
          <source>C parameter in C-Support Vector Classification</source>
          <target state="translated">C-Support Vector Classification中的C参数</target>
        </trans-unit>
        <trans-unit id="cdda2bb3aa8ccb7b0f69433506f364d34d2e9052" translate="yes" xml:space="preserve">
          <source>C parameter in C-Support Vector Classification. 1 by default.</source>
          <target state="translated">C-Support Vector Classification中的C参数。默认为1。</target>
        </trans-unit>
        <trans-unit id="613d7a720c0f1c9138a31585cc88659b028aac73" translate="yes" xml:space="preserve">
          <source>C-Support Vector Classification.</source>
          <target state="translated">C-支持向量分类。</target>
        </trans-unit>
        <trans-unit id="bd2a390d1ac4f130ef2be0118bce6d064e5c8f80" translate="yes" xml:space="preserve">
          <source>C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their Applications to Handwritten Digit Recognition, MSc Thesis, Institute of Graduate Studies in Science and Engineering, Bogazici University.</source>
          <target state="translated">C.Kaynak(1995年),&quot;Methods of Combining Multiple Classifiers and Their Applications to Handwritten Digit Recognition&quot;,硕士论文,博加济奇大学科学与工程研究生院。</target>
        </trans-unit>
        <trans-unit id="233528e4f33123dff21e034b86ff445b99733c7a" translate="yes" xml:space="preserve">
          <source>C. Molnar, &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;&gt;Interpretable Machine Learning&lt;/a&gt;, Section 5.1, 2019.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="da3d99d58e8919ce7a794351fb460091679af9c1" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Sch&amp;uuml;tze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 118-120.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="da9635593b5caf5d4585f5ff3fe3cc183bfd45ee" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Sch&amp;uuml;tze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265.</source>
          <target state="translated">CD曼宁，拉格万（P. Raghavan）和许舒特（H.Sch&amp;uuml;tze）（2008）。信息检索简介。剑桥大学出版社，第234-265页。</target>
        </trans-unit>
        <trans-unit id="14a7f63b48e4cd57491d39ae8fbc9659df0b4285" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Sch&amp;uuml;tze (2008). Introduction to Information Retrieval. Cambridge University Press. &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&lt;/a&gt;</source>
          <target state="translated">CD曼宁，拉格万（P. Raghavan）和许舒（H.Sch&amp;uuml;tze）（2008）。信息检索简介。剑桥大学出版社。&lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d8731241db66672efece43e5d06ce590569a5d55" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Sch&amp;uuml;tze (2008). Introduction to Information Retrieval. Cambridge University Press. &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&quot;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="7f2e75005cfe431b03b43e4219e3b46d7acd0ff4" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&lt;/a&gt;</source>
          <target state="translated">CD曼宁，拉格万（P. Raghavan）和舒特（H.Schuetze）（2008）。信息检索简介。剑桥大学出版社，第234-265页。&lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="75ec24d6932a118f83e8d64145c0a2ded87f2b67" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&lt;/a&gt;</source>
          <target state="translated">CD曼宁，拉格万（P. Raghavan）和舒特（H.Schuetze）（2008）。信息检索简介。剑桥大学出版社，第234-265页。&lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&quot;&gt;http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="01f3fb182680db69cebca5748ed6286feb50b1a9" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&quot;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9600fefe6b9e4b354eca34f86431cfc60b66dab3" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&quot;&gt;https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html&lt;/a&gt;</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="d19ae811e7ffeb32597a550f21eb906e9869f42b" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan, H. Sch&amp;uuml;tze, &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html&quot;&gt;Introduction to Information Retrieval&lt;/a&gt;, 2008.</source>
          <target state="translated">CD Manning，P。Raghavan，H。Sch&amp;uuml;tze，&lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html&quot;&gt;&amp;ldquo;信息检索简介&amp;rdquo;&lt;/a&gt;，2008年。</target>
        </trans-unit>
        <trans-unit id="015610fbd8f75c742ed46a08abb26c26092afe90" translate="yes" xml:space="preserve">
          <source>C.D. Manning, P. Raghavan, H. Sch&amp;uuml;tze, &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html&quot;&gt;Introduction to Information Retrieval&lt;/a&gt;, 2008.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="ac35b04dfbcb3d2d718c41403e2829f9afacc158" translate="yes" xml:space="preserve">
          <source>C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer, p. 209.</source>
          <target state="translated">C.M.Bishop(2006)。Pattern Recognition and Machine Learning.Springer,p.209.</target>
        </trans-unit>
        <trans-unit id="4e28cad37c371942b28c3b9844d1c63eab0cd98c" translate="yes" xml:space="preserve">
          <source>C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule&amp;rsquo;s precondition if the accuracy of the rule improves without it.</source>
          <target state="translated">C4.5是ID3的后继版本，它通过动态定义离散属性（基于数字变量）来消除要素必须分类的限制，该离散属性将连续属性值划分为离散的间隔集。C4.5将训练后的树（即ID3算法的输出）转换为if-then规则集。然后评估每个规则的这些准确性，以确定应该应用它们的顺序。如果没有规则，规则的准确性会提高，则删除规则的先决条件即可进行修剪。</target>
        </trans-unit>
        <trans-unit id="f38b90367c7c83e3cb66fb496411f772ac84712e" translate="yes" xml:space="preserve">
          <source>C5.0 is Quinlan&amp;rsquo;s latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate.</source>
          <target state="translated">C5.0是Quinlan在专有许可下的最新版本。与C4.5相比，它使用更少的内存并构建更小的规则集，同时更加准确。</target>
        </trans-unit>
        <trans-unit id="d173d3b539a344f61d41ecea5a62c323f7d19142" translate="yes" xml:space="preserve">
          <source>CCA Canonical Correlation Analysis.</source>
          <target state="translated">CCA Canonical Correlation Analysis。</target>
        </trans-unit>
        <trans-unit id="b29de26d602ff110fad786ea8b03d97b6805af56" translate="yes" xml:space="preserve">
          <source>CCA inherits from PLS with mode=&amp;rdquo;B&amp;rdquo; and deflation_mode=&amp;rdquo;canonical&amp;rdquo;.</source>
          <target state="translated">CCA继承自具有模式=&amp;ldquo; B&amp;rdquo;和deflation_mode =&amp;ldquo;规范&amp;rdquo;的PLS。</target>
        </trans-unit>
        <trans-unit id="3b8d88f34c322c835ae4b658c65d92bfd4b52c68" translate="yes" xml:space="preserve">
          <source>CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</source>
          <target state="translated">CHAS 查尔斯河虚拟变量(=1,如果地块与河流接壤,否则为0)</target>
        </trans-unit>
        <trans-unit id="65e7931e7d9586486554e7903a3ba08da00b37e4" translate="yes" xml:space="preserve">
          <source>CRIM per capita crime rate by town</source>
          <target state="translated">CRIM 按城镇分列的人均犯罪率</target>
        </trans-unit>
        <trans-unit id="c1c39cb0e3578f55a7a5c64e876c1d8c4ac8dfba" translate="yes" xml:space="preserve">
          <source>CSR, CSC, and LIL sparse matrices are supported. COO sparse matrices are not supported.</source>
          <target state="translated">支持CSR、CSC和LIL稀疏矩阵。不支持COO稀疏矩阵。</target>
        </trans-unit>
        <trans-unit id="73f41c9b08913eecfdd5c30b35b021c1e2d148fd" translate="yes" xml:space="preserve">
          <source>Cache size for gram matrix columns (in megabytes). 100 by default.</source>
          <target state="translated">克矩阵列的缓存大小(单位:兆字节)。默认为100。</target>
        </trans-unit>
        <trans-unit id="596160dd9a9ac13e48baafc6fcbec2fd25ca71f9" translate="yes" xml:space="preserve">
          <source>Caching nearest neighbors</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="9ff21fb5eb8a4fa17d7e155397ec6ccfabcba0a4" translate="yes" xml:space="preserve">
          <source>Caching transformers within a &lt;code&gt;Pipeline&lt;/code&gt;</source>
          <target state="translated">在 &lt;code&gt;Pipeline&lt;/code&gt; 缓存变压器</target>
        </trans-unit>
        <trans-unit id="bc2913cfd35cdeb7164299f7eca5904d9beb65fc" translate="yes" xml:space="preserve">
          <source>Calculate approximate log-likelihood as score.</source>
          <target state="translated">计算近似对数似然作为分数。</target>
        </trans-unit>
        <trans-unit id="c1ebfc4b9bd037366318ba68c2c2cca59e1e8374" translate="yes" xml:space="preserve">
          <source>Calculate approximate perplexity for data X.</source>
          <target state="translated">计算数据X的近似迷惑性。</target>
        </trans-unit>
        <trans-unit id="d43ab93e0150f4ef29d06ad998812dd6ba91c9c1" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from &lt;a href=&quot;sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt;&lt;code&gt;accuracy_score&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">计算指标为每个实例，并找到他们的平均（仅适用于多标签分类有意义的地方这不同于&lt;a href=&quot;sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt; &lt;code&gt;accuracy_score&lt;/code&gt; &lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="e282cf212dffcdd940b3d9463a8434aa0c623305" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each instance, and find their average (only meaningful for multilabel classification).</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="59762e6da26cf0edea79b5cf712b11b4295f7c3a" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each instance, and find their average.</source>
          <target state="translated">计算每个实例的指标,并找出其平均值。</target>
        </trans-unit>
        <trans-unit id="b0449c6e69f12738c5fc9e626586315cc55b3e5c" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters &amp;lsquo;macro&amp;rsquo; to account for label imbalance; it can result in an F-score that is not between precision and recall.</source>
          <target state="translated">计算每个标签的指标，并找到它们的平均加权权重（每个标签的真实实例数）。这改变了&amp;ldquo;宏观&amp;rdquo;以解决标签的不平衡。这可能导致F得分不在精确度和召回率之间。</target>
        </trans-unit>
        <trans-unit id="bc72c4ff58beeced2951accfefb204124157b6da" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label).</source>
          <target state="translated">计算每个标签的度量,并找到它们的平均值,通过支持度(每个标签的真实实例数)加权。</target>
        </trans-unit>
        <trans-unit id="a0980fad9587146eedd0aa21fe16b429b81ffb7a" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters &amp;lsquo;macro&amp;rsquo; to account for label imbalance.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c9463a17edbcd91794095643439dd340e0b094af" translate="yes" xml:space="preserve">
          <source>Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.</source>
          <target state="translated">计算每个标签的指标,并找出它们的非加权平均值。这并没有考虑到标签的不平衡性。</target>
        </trans-unit>
        <trans-unit id="a029600dc316664ad0a95bc83365c7c5938aa93b" translate="yes" xml:space="preserve">
          <source>Calculate metrics globally by considering each element of the label indicator matrix as a label.</source>
          <target state="translated">通过将标签指标矩阵中的每个元素视为标签,在全局范围内计算指标。</target>
        </trans-unit>
        <trans-unit id="734c6eed79b78874fafa14f9d8cbb9961b3a854c" translate="yes" xml:space="preserve">
          <source>Calculate metrics globally by counting the total true positives, false negatives and false positives.</source>
          <target state="translated">通过计算总的真阳性、假阴性和假阳性来计算全球的指标。</target>
        </trans-unit>
        <trans-unit id="23f0cbdf1f8f2eb83817496535820388353f4a46" translate="yes" xml:space="preserve">
          <source>Calculate the euclidean distances in the presence of missing values.</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="49fff0d8477b2f29bd766898572fb8e020082b5d" translate="yes" xml:space="preserve">
          <source>Calculates a covariance matrix shrunk on the diagonal</source>
          <target state="translated">计算对角线上收缩的协方差矩阵。</target>
        </trans-unit>
        <trans-unit id="f0383dd4fd81be714fa7344f5483fa52315c3c0b" translate="yes" xml:space="preserve">
          <source>Calculating &lt;a href=&quot;https://en.wikipedia.org/wiki/False_positive_rate&quot;&gt;fall out&lt;/a&gt; (also called the false positive rate) for each class:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="89465d26eac0f77a714d2eb41333a3104974a090" translate="yes" xml:space="preserve">
          <source>Calculating &lt;a href=&quot;https://en.wikipedia.org/wiki/False_positives_and_false_negatives&quot;&gt;miss rate&lt;/a&gt; (also called the false negative rate) for each class:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="485cdc580455f224386f0fdb1e7fd3b7d1083ba4" translate="yes" xml:space="preserve">
          <source>Calculating &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;recall&lt;/a&gt; (also called the true positive rate or the sensitivity) for each class:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="a9101242444ef9428e2eaa8ec947f98e495b1f80" translate="yes" xml:space="preserve">
          <source>Calculating &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;specificity&lt;/a&gt; (also called the true negative rate) for each class:</source>
          <target state="new"/>
        </trans-unit>
        <trans-unit id="c97307180f2f1c1353791c3b1ff02010b40d4cbc" translate="yes" xml:space="preserve">
          <source>Calculation over a dense representation, however, may leverage highly optimised vector operations and multithreading in BLAS, and tends to result in fewer CPU cache misses. So the sparsity should typically be quite high (10% non-zeros max, to be checked depending on the hardware) for the sparse input representation to be faster than the dense input representation on a machine with many CPUs and an optimized BLAS implementation.</source>
          <target state="translated">然而,在密集表示上的计算可能会利用BLAS中高度优化的向量运算和多线程,并且往往会导致较少的CPU缓存错过。所以稀疏度通常应该相当高(最大10%非零,要根据硬件情况进行检查),这样在一个有很多CPU和优化的BLAS实现的机器上,稀疏输入表示法才会比密集输入表示法快。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
