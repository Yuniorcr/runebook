<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="4fae2d49ee8a5e8d8ea52343db3e2b05ff45988e" translate="yes" xml:space="preserve">
          <source>Fit the ridge classifier.</source>
          <target state="translated">契合山脊分级机。</target>
        </trans-unit>
        <trans-unit id="2b96765d688b574c756064537ec2686c8ac36571" translate="yes" xml:space="preserve">
          <source>Fit the transformer on X.</source>
          <target state="translated">在X上安装变压器。</target>
        </trans-unit>
        <trans-unit id="acd485cd941415835a11d2180278c2146ce5888c" translate="yes" xml:space="preserve">
          <source>Fit the weights of a regression model, using an ARD prior. The weights of the regression model are assumed to be in Gaussian distributions. Also estimate the parameters lambda (precisions of the distributions of the weights) and alpha (precision of the distribution of the noise). The estimation is done by an iterative procedures (Evidence Maximization)</source>
          <target state="translated">使用ARD先验,拟合回归模型的权重。回归模型的权重被假定为高斯分布。同时估计参数lambda(权重分布的精度)和alpha(噪声分布的精度)。估计是通过一个迭代程序(证据最大化)来完成的。</target>
        </trans-unit>
        <trans-unit id="ea18404b90123396c3f41537d11afad54c507780" translate="yes" xml:space="preserve">
          <source>Fit to data, then transform it.</source>
          <target state="translated">对数据进行拟合,然后进行转换。</target>
        </trans-unit>
        <trans-unit id="c2cf341635a3875675d46dd618b9a1757d9a6c7c" translate="yes" xml:space="preserve">
          <source>Fit transformer by checking X.</source>
          <target state="translated">通过检查X安装变压器。</target>
        </trans-unit>
        <trans-unit id="eb75d7eb91b3dadf245e1b3bf8f4c37a27824085" translate="yes" xml:space="preserve">
          <source>Fit underlying estimators.</source>
          <target state="translated">契合基本估算器。</target>
        </trans-unit>
        <trans-unit id="c093c0cee7f3b9fa93ffb32acb026baea322889f" translate="yes" xml:space="preserve">
          <source>Fits a Minimum Covariance Determinant with the FastMCD algorithm.</source>
          <target state="translated">用FastMCD算法拟合一个最小协方差确定因子。</target>
        </trans-unit>
        <trans-unit id="f30506afc59d08eae550fdeb02ae856731ea996b" translate="yes" xml:space="preserve">
          <source>Fits all the transforms one after the other and transforms the data, then uses fit_transform on transformed data with the final estimator.</source>
          <target state="translated">将所有的变换一一拟合,并对数据进行变换,然后在变换后的数据上使用fit_transform与最终的估计器。</target>
        </trans-unit>
        <trans-unit id="c5c1cc9c0352ec3e2d5fcc0df63b71ce152f382f" translate="yes" xml:space="preserve">
          <source>Fits the GraphicalLasso covariance model to X.</source>
          <target state="translated">将GraphicalLasso协方差模型拟合到X上。</target>
        </trans-unit>
        <trans-unit id="268ae9ae0a9043d80b2e575c7e344f83f78e662d" translate="yes" xml:space="preserve">
          <source>Fits the GraphicalLasso model to X.</source>
          <target state="translated">将GraphicalLasso模型拟合到X上。</target>
        </trans-unit>
        <trans-unit id="643e04850030e1e1aeeaf619a88e7dab7e6a06be" translate="yes" xml:space="preserve">
          <source>Fits the Ledoit-Wolf shrunk covariance model according to the given training data and parameters.</source>
          <target state="translated">根据给定的训练数据和参数,拟合Ledoit-Wolf收缩协方差模型。</target>
        </trans-unit>
        <trans-unit id="60a2c06b8221e361c36d5fdce8ee644d8456bfce" translate="yes" xml:space="preserve">
          <source>Fits the Maximum Likelihood Estimator covariance model according to the given training data and parameters.</source>
          <target state="translated">根据给定的训练数据和参数,拟合最大似然估计器协方差模型。</target>
        </trans-unit>
        <trans-unit id="9b00c787fd8f13356df0bf86c478f2b7848ac4d7" translate="yes" xml:space="preserve">
          <source>Fits the Oracle Approximating Shrinkage covariance model according to the given training data and parameters.</source>
          <target state="translated">根据给定的训练数据和参数,拟合Oracle近似收缩协方差模型。</target>
        </trans-unit>
        <trans-unit id="2fb3d8548147a6429a7eeed9e3fbe0456049bc8c" translate="yes" xml:space="preserve">
          <source>Fits the estimator.</source>
          <target state="translated">契合估计器。</target>
        </trans-unit>
        <trans-unit id="98f6beb1ac87a74b86b8c1fedd5ae0ed0ac89461" translate="yes" xml:space="preserve">
          <source>Fits the shrunk covariance model according to the given training data and parameters.</source>
          <target state="translated">根据给定的训练数据和参数来拟合收缩协方差模型。</target>
        </trans-unit>
        <trans-unit id="f072640da94e1ad56e67373f3632aeaebdd8b854" translate="yes" xml:space="preserve">
          <source>Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.</source>
          <target state="translated">用可选的参数fit_params对X和y进行拟合变换,并返回X的变换版本。</target>
        </trans-unit>
        <trans-unit id="fd568de48dadd27cd4e3ca2395da082642e8f8ec" translate="yes" xml:space="preserve">
          <source>Fitted regressor.</source>
          <target state="translated">合适的回归者。</target>
        </trans-unit>
        <trans-unit id="14c6da7cd39661e1b0c6468a3c6a5907d4f99a9c" translate="yes" xml:space="preserve">
          <source>Fitting transformers may be computationally expensive. With its &lt;code&gt;memory&lt;/code&gt; parameter set, &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; will cache each transformer after calling &lt;code&gt;fit&lt;/code&gt;. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical. A typical example is the case of a grid search in which the transformers can be fitted only once and reused for each configuration.</source>
          <target state="translated">安装变压器可能在计算上很昂贵。通过设置其 &lt;code&gt;memory&lt;/code&gt; 参数，&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt;将在调用 &lt;code&gt;fit&lt;/code&gt; 之后缓存每个转换器。如果参数和输入数据相同，则使用此功能可避免计算管道内的配合变压器。一个典型的例子是在电网搜索中，其中的变压器只能安装一次，并且对于每种配置都可以重复使用。</target>
        </trans-unit>
        <trans-unit id="01fe05c223cb56d84d085e38ca62de1932a87e50" translate="yes" xml:space="preserve">
          <source>Flag indicating if the cross-validation values corresponding to each alpha should be stored in the &lt;code&gt;cv_values_&lt;/code&gt; attribute (see below). This flag is only compatible with &lt;code&gt;cv=None&lt;/code&gt; (i.e. using Generalized Cross-Validation).</source>
          <target state="translated">指示是否应将与每个alpha对应的交叉验证值存储在 &lt;code&gt;cv_values_&lt;/code&gt; 属性中的标志（请参见下文）。该标志仅与 &lt;code&gt;cv=None&lt;/code&gt; 兼容（即使用通用交叉验证）。</target>
        </trans-unit>
        <trans-unit id="1f498759924682e2363e94f7b83282b97429fcf5" translate="yes" xml:space="preserve">
          <source>Flag indicating which strategy to use when performing Generalized Cross-Validation. Options are:</source>
          <target state="translated">标志表示在执行广义交叉验证时要使用的策略。选项有:</target>
        </trans-unit>
        <trans-unit id="3407c4421a1f6ede0cab565dc5123546e65ddde6" translate="yes" xml:space="preserve">
          <source>Flat geometry, good for density estimation</source>
          <target state="translated">平坦的几何形状,有利于密度估算</target>
        </trans-unit>
        <trans-unit id="748a38982c93bb25fbfeb18b34277c35439ac98c" translate="yes" xml:space="preserve">
          <source>Flavanoids</source>
          <target state="translated">Flavanoids</target>
        </trans-unit>
        <trans-unit id="f55beb472c3b08362b7861294963760ddd037d08" translate="yes" xml:space="preserve">
          <source>Flavanoids:</source>
          <target state="translated">Flavanoids:</target>
        </trans-unit>
        <trans-unit id="1bf94453d6aa9e9092828eefafa6394692100339" translate="yes" xml:space="preserve">
          <source>Flexible pickling control for the communication to and from the worker processes.</source>
          <target state="translated">灵活的酸洗控制,用于与工人流程之间的沟通。</target>
        </trans-unit>
        <trans-unit id="2d83a2dbf42ef510856c4fe5eb69b3efa4599763" translate="yes" xml:space="preserve">
          <source>Flow Chart</source>
          <target state="translated">流程图</target>
        </trans-unit>
        <trans-unit id="87698cca8f914c77b735bad53fe489d2af135e70" translate="yes" xml:space="preserve">
          <source>Folder to be used by the pool for memmapping large arrays for sharing memory with worker processes. If None, this will try in order:</source>
          <target state="translated">池使用的文件夹,用于memmapping大型数组,以便与工作进程共享内存。如果无,将按顺序尝试。</target>
        </trans-unit>
        <trans-unit id="313fc38f449c398563f152e4416c92af47202923" translate="yes" xml:space="preserve">
          <source>Follows Algorithm 4.3 of Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) http://arxiv.org/pdf/0909.4061</source>
          <target state="translated">遵循《用随机性寻找结构》的算法4.3。构建近似矩阵分解的随机算法 Halko等人,2009年(arXiv:909)http://arxiv.org/pdf/0909.4061。</target>
        </trans-unit>
        <trans-unit id="ec0c3b76630fd745381cc215a284820af75a683a" translate="yes" xml:space="preserve">
          <source>Footnotes</source>
          <target state="translated">Footnotes</target>
        </trans-unit>
        <trans-unit id="41e2c901c13d4daacf7c9adad4d559c077a8e51e" translate="yes" xml:space="preserve">
          <source>For &amp;ldquo;one-vs-rest&amp;rdquo; &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; the attributes &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; have the shape &lt;code&gt;[n_class, n_features]&lt;/code&gt; and &lt;code&gt;[n_class]&lt;/code&gt; respectively. Each row of the coefficients corresponds to one of the &lt;code&gt;n_class&lt;/code&gt; many &amp;ldquo;one-vs-rest&amp;rdquo; classifiers and similar for the intercepts, in the order of the &amp;ldquo;one&amp;rdquo; class.</source>
          <target state="translated">对于&amp;ldquo;一对一休息&amp;rdquo;的&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;，属性 &lt;code&gt;coef_&lt;/code&gt; 和 &lt;code&gt;intercept_&lt;/code&gt; 分别具有 &lt;code&gt;[n_class, n_features]&lt;/code&gt; 和 &lt;code&gt;[n_class]&lt;/code&gt; 的形状。系数的每一行对应于 &lt;code&gt;n_class&lt;/code&gt; 的许多&amp;ldquo; one-vs-rest&amp;rdquo;分类器之一，并且对于截距，按&amp;ldquo; one&amp;rdquo;类的顺序类似。</target>
        </trans-unit>
        <trans-unit id="c6cc35fe8de003f58f84a7c02bbc9d4b652dc227" translate="yes" xml:space="preserve">
          <source>For &amp;ldquo;pairwise&amp;rdquo; metrics, between &lt;em&gt;samples&lt;/em&gt; and not estimators or predictions, see the &lt;a href=&quot;metrics#metrics&quot;&gt;Pairwise metrics, Affinities and Kernels&lt;/a&gt; section.</source>
          <target state="translated">对于&amp;ldquo;成对&amp;rdquo;度量，在&lt;em&gt;样本&lt;/em&gt;之间而不是估计量或预测之间，请参阅&lt;a href=&quot;metrics#metrics&quot;&gt;成对度量，亲和力和内核&lt;/a&gt;部分。</target>
        </trans-unit>
        <trans-unit id="935f9d2961eec1b64a8d3f62208c3a16e0e7ef63" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; of trees (e.g. RandomForest, GBT, ExtraTrees etc) the number of trees and their depth play the most important role. Latency and throughput should scale linearly with the number of trees. In this case we used directly the &lt;code&gt;n_estimators&lt;/code&gt; parameter of &lt;code&gt;sklearn.ensemble.gradient_boosting.GradientBoostingRegressor&lt;/code&gt;.</source>
          <target state="translated">对于树木的&lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt;（例如RandomForest，GBT，ExtraTrees等），树木的数量及其深度起着最重要的作用。延迟和吞吐量应与树的数量成线性比例。在这种情况下，我们直接使用 &lt;code&gt;n_estimators&lt;/code&gt; 的参数 &lt;code&gt;sklearn.ensemble.gradient_boosting.GradientBoostingRegressor&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c2e53d45d0579b4b39658069206cb04a03ac3808" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;classes#module-sklearn.linear_model&quot;&gt;&lt;code&gt;sklearn.linear_model&lt;/code&gt;&lt;/a&gt; (e.g. Lasso, ElasticNet, SGDClassifier/Regressor, Ridge &amp;amp; RidgeClassifier, PassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression&amp;hellip;) the decision function that is applied at prediction time is the same (a dot product) , so latency should be equivalent.</source>
          <target state="translated">对于&lt;a href=&quot;classes#module-sklearn.linear_model&quot;&gt; &lt;code&gt;sklearn.linear_model&lt;/code&gt; &lt;/a&gt;（例如Lasso，ElasticNet，SGDClassifier / Regressor，Ridge＆RidgeClassifier，PassiveAggressiveClassifier / Regressor，LinearSVC，LogisticRegression&amp;hellip;），在预测时应用的决策函数是相同的（点积），因此延迟应相等。</target>
        </trans-unit>
        <trans-unit id="8582a7ae6ed830b76bb2d9fd21363d4d3995f59c" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; (and &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;) any input passed as a numpy array will be copied and converted to the liblinear internal sparse data representation (double precision floats and int32 indices of non-zero components). If you want to fit a large-scale linear classifier without copying a dense numpy C-contiguous double precision array as input we suggest to use the &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; class instead. The objective function can be configured to be almost the same as the &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; model.</source>
          <target state="translated">对于&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;（和&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;），将复制作为numpy数组传递的任何输入并将其转换为liblinear内部稀疏数据表示形式（双精度浮点数和非零分量的int32索引）。如果要适合大型线性分类器而不复制密集的numpy C连续双精度数组作为输入，我们建议改用&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;类。可以将目标函数配置为与&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;模型几乎相同。</target>
        </trans-unit>
        <trans-unit id="e56c72d645a0047396221ed2eb5407914a9b6bf6" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;make_classification&lt;/code&gt;, three binary and two multi-class classification datasets are generated, with different numbers of informative features and clusters per class.</source>
          <target state="translated">对于 &lt;code&gt;make_classification&lt;/code&gt; ，将生成三个二进制和两个多类分类数据集，每个类具有不同数量的信息特征和聚类。</target>
        </trans-unit>
        <trans-unit id="dda7c631740b122861937f9ebbfdde73fd44b016" translate="yes" xml:space="preserve">
          <source>For Gaussian distributed data, the distance of an observation \(x_i\) to the mode of the distribution can be computed using its Mahalanobis distance: \(d_{(\mu,\Sigma)}(x_i)^2 = (x_i - \mu)'\Sigma^{-1}(x_i - \mu)\) where \(\mu\) and \(\Sigma\) are the location and the covariance of the underlying Gaussian distribution.</source>
          <target state="translated">对于高斯分布的数据,观测值/(x_i/)到分布模式的距离可以用它的马哈兰诺比斯距离来计算。\(d_{(\mu,\Sigma)}(x_i)^2=(x_i-\mu)'\Sigma^{-1}(x_i-\mu)\)其中,\(\mu)和\(\Sigma\)是基本高斯分布的位置和协方差。</target>
        </trans-unit>
        <trans-unit id="3fb903a20f5aad7e20f9123d2edfa2a0638dc6bc" translate="yes" xml:space="preserve">
          <source>For \(k\) clusters, the Calinski-Harabaz score \(s\) is given as the ratio of the between-clusters dispersion mean and the within-cluster dispersion:</source>
          <target state="translated">对于/(k)个聚类,Calinski-Harabaz得分/(s)是以聚类间离散度平均值和聚类内离散度之比给出的。</target>
        </trans-unit>
        <trans-unit id="d27bcc7c91650762beefd01dc3089ec6978d5c87" translate="yes" xml:space="preserve">
          <source>For a classification model, the predicted class for each sample in X is returned. For a regression model, the predicted value based on X is returned.</source>
          <target state="translated">对于分类模型,返回X中每个样本的预测类。对于回归模型,返回基于X的预测值。</target>
        </trans-unit>
        <trans-unit id="e6fd66f776dfd09a091bc857e8c9d10d50ac3ba8" translate="yes" xml:space="preserve">
          <source>For a comparison of the different scalers, transformers, and normalizers, see &lt;a href=&quot;../../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;examples/preprocessing/plot_all_scaling.py&lt;/a&gt;.</source>
          <target state="translated">有关不同缩放器，转换器和规范化器的比较，请参阅&lt;a href=&quot;../../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;examples / preprocessing / plot_all_scaling.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="33f57a2a03940da66a0808ac0a9d7e45bc98afe2" translate="yes" xml:space="preserve">
          <source>For a complete probabilistic model we also need a prior distribution for the latent variable \(h\). The most straightforward assumption (based on the nice properties of the Gaussian distribution) is \(h \sim \mathcal{N}(0, \mathbf{I})\). This yields a Gaussian as the marginal distribution of \(x\):</source>
          <target state="translated">对于一个完整的概率模型,我们还需要一个潜变量的先验分布(h)。最直接的假设(基于高斯分布的良好特性)是(h sim \mathcal{N}(0,\mathbf{I})\)。这就产生了一个高斯分布,作为边际分布的(x)。</target>
        </trans-unit>
        <trans-unit id="7e3b25cfbbacb17bf9ce066c3983b6610e3fab10" translate="yes" xml:space="preserve">
          <source>For a constant learning rate use &lt;code&gt;learning_rate='constant'&lt;/code&gt; and use &lt;code&gt;eta0&lt;/code&gt; to specify the learning rate.</source>
          <target state="translated">对于恒定的学习率，使用 &lt;code&gt;learning_rate='constant'&lt;/code&gt; 并使用 &lt;code&gt;eta0&lt;/code&gt; 指定学习率。</target>
        </trans-unit>
        <trans-unit id="76a9227cf28fa05c9bb19673e15a2774476a035c" translate="yes" xml:space="preserve">
          <source>For a description of the implementation and details of the algorithms used, please refer to</source>
          <target state="translated">关于实施的描述和所使用的算法的细节,请参见</target>
        </trans-unit>
        <trans-unit id="ccaff5036b34bf3986d666eb2f98e4ef943f3dfd" translate="yes" xml:space="preserve">
          <source>For a discussion and comparison of these algorithms, see the &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;manifold module page&lt;/a&gt;</source>
          <target state="translated">有关这些算法的讨论和比较，请参阅&lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;歧管模块页面&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7b7d7f03d534f8340da15e8579a79cb680de77bd" translate="yes" xml:space="preserve">
          <source>For a document generated from multiple topics, all topics are weighted equally in generating its bag of words.</source>
          <target state="translated">对于一个由多个主题生成的文档,在生成它的词袋时,所有主题的权重相同。</target>
        </trans-unit>
        <trans-unit id="b87483db50bfd800e7f61326ccd19592abcc3547" translate="yes" xml:space="preserve">
          <source>For a few of the best biclusters, its most common document categories and its ten most important words get printed. The best biclusters are determined by their normalized cut. The best words are determined by comparing their sums inside and outside the bicluster.</source>
          <target state="translated">对于几个最好的双联体,它最常见的文档类别和它的十个最重要的词得到印证。最佳双簇由其归一化切割决定。最佳词汇是通过比较双簇内外的总和来确定的。</target>
        </trans-unit>
        <trans-unit id="859f5c51d38da3ad244e41ebf28b507a4a99bc62" translate="yes" xml:space="preserve">
          <source>For a full code example that demonstrates using a &lt;a href=&quot;generated/sklearn.preprocessing.functiontransformer#sklearn.preprocessing.FunctionTransformer&quot;&gt;&lt;code&gt;FunctionTransformer&lt;/code&gt;&lt;/a&gt; to do custom feature selection, see &lt;a href=&quot;../auto_examples/preprocessing/plot_function_transformer#sphx-glr-auto-examples-preprocessing-plot-function-transformer-py&quot;&gt;Using FunctionTransformer to select columns&lt;/a&gt;</source>
          <target state="translated">有关演示使用&lt;a href=&quot;generated/sklearn.preprocessing.functiontransformer#sklearn.preprocessing.FunctionTransformer&quot;&gt; &lt;code&gt;FunctionTransformer&lt;/code&gt; &lt;/a&gt;进行自定义功能选择的完整代码示例，请参见&lt;a href=&quot;../auto_examples/preprocessing/plot_function_transformer#sphx-glr-auto-examples-preprocessing-plot-function-transformer-py&quot;&gt;使用FunctionTransformer选择列&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="a29c02b5f33160de772eacbd74337a53f6625181" translate="yes" xml:space="preserve">
          <source>For a full-fledged example of out-of-core scaling in a text classification task see &lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core classification of text documents&lt;/a&gt;.</source>
          <target state="translated">有关文本分类任务中核外缩放的完整示例，请参见&lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;文本文档的核外分类&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="d78aafa74d01fdbbdb67982f22aabb8fb92e6131" translate="yes" xml:space="preserve">
          <source>For a given value of &lt;code&gt;n_components&lt;/code&gt;&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; is often less accurate as &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; is cheaper to compute, though, making use of larger feature spaces more efficient.</source>
          <target state="translated">对于一个给定的值 &lt;code&gt;n_components&lt;/code&gt; &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;常常是作为不太精确&lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;。但是，&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; 的&lt;/a&gt;计算成本较低，从而可以更有效地利用较大的特征空间。</target>
        </trans-unit>
        <trans-unit id="46cec00c813e8ea8e2f5bd58264262a28ac481cf" translate="yes" xml:space="preserve">
          <source>For a good choice of alpha, the &lt;a href=&quot;linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; can fully recover the exact set of non-zero variables using only few observations, provided certain specific conditions are met. In particular, the number of samples should be &amp;ldquo;sufficiently large&amp;rdquo;, or L1 models will perform at random, where &amp;ldquo;sufficiently large&amp;rdquo; depends on the number of non-zero coefficients, the logarithm of the number of features, the amount of noise, the smallest absolute value of non-zero coefficients, and the structure of the design matrix X. In addition, the design matrix must display certain specific properties, such as not being too correlated.</source>
          <target state="translated">为了很好地选择alpha ，只要满足某些特定条件，&lt;a href=&quot;linear_model#lasso&quot;&gt;套索&lt;/a&gt;就可以使用很少的观测值完全恢复非零变量的确切集合。特别是，样本数量应为&amp;ldquo;足够大&amp;rdquo;，否则L1模型将随机执行，其中&amp;ldquo;足够大&amp;rdquo;取决于非零系数的数量，特征数量的对数，噪声量，非零系数的最小绝对值，以及设计矩阵X的结构。此外，设计矩阵必须显示某些特定的属性，例如，不要过于相关。</target>
        </trans-unit>
        <trans-unit id="283fe9d87c4a4faac62d4b9cee8a3089a1cb638f" translate="yes" xml:space="preserve">
          <source>For a multi-label classification problem with N classes, N binary classifiers are assigned an integer between 0 and N-1. These integers define the order of models in the chain. Each classifier is then fit on the available training data plus the true labels of the classes whose models were assigned a lower number.</source>
          <target state="translated">对于一个有N个类的多标签分类问题,N个二元分类器被分配一个0到N-1之间的整数。这些整数定义了链中模型的顺序。然后,在现有的训练数据上对每个分类器进行拟合,再加上模型被赋予较低数字的类的真实标签。</target>
        </trans-unit>
        <trans-unit id="73e6ca6403df9166903acb4326e48adf2d2e8f55" translate="yes" xml:space="preserve">
          <source>For a multi_class problem, if multi_class is set to be &amp;ldquo;multinomial&amp;rdquo; the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes.</source>
          <target state="translated">对于multi_class问题，如果将multi_class设置为&amp;ldquo;多项式&amp;rdquo;，则使用softmax函数查找每个类别的预测概率。否则，使用一对多休息法，即使用逻辑函数，假设每个类别为正，则计算它们的概率。并在所有类别中标准化这些值。</target>
        </trans-unit>
        <trans-unit id="642c44d27e63bf2bd3e040832cd67d4f4b97be3d" translate="yes" xml:space="preserve">
          <source>For a multiclass problem, the hyperparameters for each class are computed using the best scores got by doing a one-vs-rest in parallel across all folds and classes. Hence this is not the true multinomial loss.</source>
          <target state="translated">对于多类问题,每个类的超参数都是用在所有折线和类上并行进行onevs-rest得到的最佳分数来计算的。因此,这不是真正的多义损失。</target>
        </trans-unit>
        <trans-unit id="3c102da8b9e1a48c3e9639790d9170902789d884" translate="yes" xml:space="preserve">
          <source>For a new point entering the root, it is merged with the subcluster closest to it and the linear sum, squared sum and the number of samples of that subcluster are updated. This is done recursively till the properties of the leaf node are updated.</source>
          <target state="translated">对于进入根节点的一个新点,将其与最接近它的子簇合并,并更新该子簇的线性和、平方和以及样本数。这样反复进行,直到更新叶子节点的属性。</target>
        </trans-unit>
        <trans-unit id="d36945b7ba93218440d9a3fb3620ffe9bc7ebec1" translate="yes" xml:space="preserve">
          <source>For a similar example, where the methods are applied to a sphere dataset, see &lt;a href=&quot;plot_manifold_sphere#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py&quot;&gt;Manifold Learning methods on a severed sphere&lt;/a&gt;</source>
          <target state="translated">对于类似的示例（将方法应用于球体数据集的情况），请参见&lt;a href=&quot;plot_manifold_sphere#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py&quot;&gt;切割球体上的流形学习方法。&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="945c76e7faeb6fe6d013381d6851dfb972b0f8ae" translate="yes" xml:space="preserve">
          <source>For a similar example, where the methods are applied to the S-curve dataset, see &lt;a href=&quot;plot_compare_methods#sphx-glr-auto-examples-manifold-plot-compare-methods-py&quot;&gt;Comparison of Manifold Learning methods&lt;/a&gt;</source>
          <target state="translated">对于类似的示例，其中将方法应用于S曲线数据集，请参阅&lt;a href=&quot;plot_compare_methods#sphx-glr-auto-examples-manifold-plot-compare-methods-py&quot;&gt;流形学习方法比较&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5693da1a0a426bf5e6f357791de67cea3dcb709e" translate="yes" xml:space="preserve">
          <source>For an adaptively decreasing learning rate, use &lt;code&gt;learning_rate='adaptive'&lt;/code&gt; and use &lt;code&gt;eta0&lt;/code&gt; to specify the starting learning rate. When the stopping criterion is reached, the learning rate is divided by 5, and the algorithm does not stop. The algorithm stops when the learning rate goes below 1e-6.</source>
          <target state="translated">对于自适应降低的学习率，请使用 &lt;code&gt;learning_rate='adaptive'&lt;/code&gt; 并使用 &lt;code&gt;eta0&lt;/code&gt; 指定起始学习率。当达到停止标准时，学习率将除以5，并且算法不会停止。当学习率低于1e-6时，算法将停止。</target>
        </trans-unit>
        <trans-unit id="289eda38dfb97e5735805aabc918de776eb35066" translate="yes" xml:space="preserve">
          <source>For an estimator to be effective, you need the distance between neighboring points to be less than some value \(d\), which depends on the problem. In one dimension, this requires on average \(n \sim 1/d\) points. In the context of the above \(k\)-NN example, if the data is described by just one feature with values ranging from 0 to 1 and with \(n\) training observations, then new data will be no further away than \(1/n\). Therefore, the nearest neighbor decision rule will be efficient as soon as \(1/n\) is small compared to the scale of between-class feature variations.</source>
          <target state="translated">为了使估算器有效,你需要相邻点之间的距离小于某个值 \(d\),这取决于问题。在一个维度上,这需要平均的点(k/d)。在上面的上下文中,如果数据只用一个特征来描述,其值范围在0到1之间,并且有(n)个训练观测值,那么新数据的距离不会超过(1/n/)。因此,与类间特征变化的规模相比,只要 \(1/n\)小,最近邻决策规则就会有效。</target>
        </trans-unit>
        <trans-unit id="cc92ccd33be99f33389b25ab23e30ca5c4b36d12" translate="yes" xml:space="preserve">
          <source>For an example of using this dataset with scikit-learn, see &lt;a href=&quot;../../auto_examples/applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;examples/applications/plot_species_distribution_modeling.py&lt;/a&gt;.</source>
          <target state="translated">有关将此数据集与scikit-learn一起使用的示例，请参见&lt;a href=&quot;../../auto_examples/applications/plot_species_distribution_modeling#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py&quot;&gt;examples / applications / plot_species_distribution_modeling.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="84cc57ff6bd3680e44c549367baf76fa37633107" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_affinity_propagation#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py&quot;&gt;examples/cluster/plot_affinity_propagation.py&lt;/a&gt;.</source>
          <target state="translated">有关示例，请参见&lt;a href=&quot;../../auto_examples/cluster/plot_affinity_propagation#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py&quot;&gt;examples / cluster / plot_affinity_propagation.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="550540d863fd72ef27788284d554fe3cf91d4d31" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_dbscan#sphx-glr-auto-examples-cluster-plot-dbscan-py&quot;&gt;examples/cluster/plot_dbscan.py&lt;/a&gt;.</source>
          <target state="translated">有关示例，请参见&lt;a href=&quot;../../auto_examples/cluster/plot_dbscan#sphx-glr-auto-examples-cluster-plot-dbscan-py&quot;&gt;examples / cluster / plot_dbscan.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="5a609c98d64d09ee64c2c225998c2e63797d885b" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/cluster/plot_mean_shift#sphx-glr-auto-examples-cluster-plot-mean-shift-py&quot;&gt;examples/cluster/plot_mean_shift.py&lt;/a&gt;.</source>
          <target state="translated">有关示例，请参见&lt;a href=&quot;../../auto_examples/cluster/plot_mean_shift#sphx-glr-auto-examples-cluster-plot-mean-shift-py&quot;&gt;examples / cluster / plot_mean_shift.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0c7aeec7cbc3121a908ff21339ef38d8e3682ea4" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_ard#sphx-glr-auto-examples-linear-model-plot-ard-py&quot;&gt;examples/linear_model/plot_ard.py&lt;/a&gt;.</source>
          <target state="translated">有关示例，请参见&lt;a href=&quot;../../auto_examples/linear_model/plot_ard#sphx-glr-auto-examples-linear-model-plot-ard-py&quot;&gt;examples / linear_model / plot_ard.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="8606b3a4ce46a8dc7d8f3fc386175108176c1a2f" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_bayesian_ridge#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py&quot;&gt;examples/linear_model/plot_bayesian_ridge.py&lt;/a&gt;.</source>
          <target state="translated">有关示例，请参见&lt;a href=&quot;../../auto_examples/linear_model/plot_bayesian_ridge#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py&quot;&gt;examples / linear_model / plot_bayesian_ridge.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="1a253fcf6bd3baedce8e1812aafc9e481fd05853" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_coordinate_descent_path#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py&quot;&gt;examples/linear_model/plot_lasso_coordinate_descent_path.py&lt;/a&gt;.</source>
          <target state="translated">有关示例，请参见&lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_coordinate_descent_path#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py&quot;&gt;examples / linear_model / plot_lasso_coordinate_descent_path.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="dd893fae3c875581ed42afc5493c8a73ae57ea3a" translate="yes" xml:space="preserve">
          <source>For an example, see &lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_model_selection#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py&quot;&gt;examples/linear_model/plot_lasso_model_selection.py&lt;/a&gt;.</source>
          <target state="translated">有关示例，请参见&lt;a href=&quot;../../auto_examples/linear_model/plot_lasso_model_selection#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py&quot;&gt;examples / linear_model / plot_lasso_model_selection.py&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="7185c2666c1903f0809fab3c9082ec1324c6a9c7" translate="yes" xml:space="preserve">
          <source>For an introduction to Unicode and character encodings in general, see Joel Spolsky&amp;rsquo;s &lt;a href=&quot;http://www.joelonsoftware.com/articles/Unicode.html&quot;&gt;Absolute Minimum Every Software Developer Must Know About Unicode&lt;/a&gt;.</source>
          <target state="translated">有关Unicode和字符编码的一般介绍，请参见Joel Spolsky的&amp;ldquo; &lt;a href=&quot;http://www.joelonsoftware.com/articles/Unicode.html&quot;&gt;每个软件开发人员必须了解的Unicode最低要求&amp;rdquo;&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="3a88e794655306a2809e5d03a41b7ab87506c6aa" translate="yes" xml:space="preserve">
          <source>For an one-class model, +1 (inlier) or -1 (outlier) is returned.</source>
          <target state="translated">对于一个单类模型,返回+1(inlier)或-1(outlier)。</target>
        </trans-unit>
        <trans-unit id="6b041f627b95dafb713c53f369d3fb59c9505ee0" translate="yes" xml:space="preserve">
          <source>For an one-class model, +1 or -1 is returned.</source>
          <target state="translated">对于单类模型,返回+1或-1。</target>
        </trans-unit>
        <trans-unit id="90c9667034ee59a28024b8500c3a1c0772f75e0b" translate="yes" xml:space="preserve">
          <source>For an overview of available strategies in scikit-learn, see also the &lt;a href=&quot;computing#scaling-strategies&quot;&gt;out-of-core learning&lt;/a&gt; documentation.</source>
          <target state="translated">有关scikit-learn中可用策略的概述，另请参见&lt;a href=&quot;computing#scaling-strategies&quot;&gt;核心学习&lt;/a&gt;文档。</target>
        </trans-unit>
        <trans-unit id="bd83f9999f935e2f6fce3641b48e5b3393b97a71" translate="yes" xml:space="preserve">
          <source>For binary classification with a true label \(y \in \{0,1\}\) and a probability estimate \(p = \operatorname{Pr}(y = 1)\), the log loss per sample is the negative log-likelihood of the classifier given the true label:</source>
          <target state="translated">对于二元分类,如果有一个真标签(y \in \{0,1\}\)和一个概率估计(p=\operatorname{Pr}(y=1)\),每个样本的对数损失是给定真标签的分类器的负对数似然。</target>
        </trans-unit>
        <trans-unit id="23a4f6b8b8e57d58b02ac23ef6f32b82b6049d45" translate="yes" xml:space="preserve">
          <source>For binary classification, \(f(x)\) passes through the logistic function \(g(z)=1/(1+e^{-z})\) to obtain output values between zero and one. A threshold, set to 0.5, would assign samples of outputs larger or equal 0.5 to the positive class, and the rest to the negative class.</source>
          <target state="translated">对于二元分类,\(f(x)\)通过逻辑函数\(g(z)=1/(1+e^{-z})\)来获得0和1之间的输出值。将阈值设为0.5,则将大于或等于0.5的输出样本归入正级,其余的归入负级。</target>
        </trans-unit>
        <trans-unit id="883efcc2dc17e184b74392564fb44d2a6f9c0bf6" translate="yes" xml:space="preserve">
          <source>For binary problems, we can get counts of true negatives, false positives, false negatives and true positives as follows:</source>
          <target state="translated">对于二元问题,我们可以得到真否定、假阳性、假否定和真阳性的计数如下。</target>
        </trans-unit>
        <trans-unit id="7f954d6786e07c3ef37ff8e0245acb91efbb4b2a" translate="yes" xml:space="preserve">
          <source>For classification with &lt;code&gt;loss='deviance'&lt;/code&gt; the target response is logit(p).</source>
          <target state="translated">对于 &lt;code&gt;loss='deviance'&lt;/code&gt; 的分类，目标响应为logit（p）。</target>
        </trans-unit>
        <trans-unit id="4118e1de638d62fd337275c2f8d28f38f1e40db3" translate="yes" xml:space="preserve">
          <source>For classification with a logistic loss, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">对于具有逻辑损失的分类，具有平均策略的SGD的另一个变体与随机平均梯度（SAG）算法一起提供，可以在&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; 中&lt;/a&gt;用作求解器。</target>
        </trans-unit>
        <trans-unit id="7a750c34f262db1f2c0c844eb9774104416e6f5c" translate="yes" xml:space="preserve">
          <source>For classification you can think of it as the regression score before the link function.</source>
          <target state="translated">对于分类来说,你可以把它看作是链接函数之前的回归分数。</target>
        </trans-unit>
        <trans-unit id="6a3d9b2c887776af95639587c4c76f62b0d1c8f1" translate="yes" xml:space="preserve">
          <source>For classification, &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveclassifier#sklearn.linear_model.PassiveAggressiveClassifier&quot;&gt;&lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt;&lt;/a&gt; can be used with &lt;code&gt;loss='hinge'&lt;/code&gt; (PA-I) or &lt;code&gt;loss='squared_hinge'&lt;/code&gt; (PA-II). For regression, &lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveregressor#sklearn.linear_model.PassiveAggressiveRegressor&quot;&gt;&lt;code&gt;PassiveAggressiveRegressor&lt;/code&gt;&lt;/a&gt; can be used with &lt;code&gt;loss='epsilon_insensitive'&lt;/code&gt; (PA-I) or &lt;code&gt;loss='squared_epsilon_insensitive'&lt;/code&gt; (PA-II).</source>
          <target state="translated">对于分类，&lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveclassifier#sklearn.linear_model.PassiveAggressiveClassifier&quot;&gt; &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; &lt;/a&gt;可以与 &lt;code&gt;loss='hinge'&lt;/code&gt; （PA-I）或 &lt;code&gt;loss='squared_hinge'&lt;/code&gt; （PA-II）一起使用。对于回归，&lt;a href=&quot;generated/sklearn.linear_model.passiveaggressiveregressor#sklearn.linear_model.PassiveAggressiveRegressor&quot;&gt; &lt;code&gt;PassiveAggressiveRegressor&lt;/code&gt; &lt;/a&gt;可以与 &lt;code&gt;loss='epsilon_insensitive'&lt;/code&gt; （PA-I）或 &lt;code&gt;loss='squared_epsilon_insensitive'&lt;/code&gt; （PA-II）一起使用。</target>
        </trans-unit>
        <trans-unit id="27898fb8346ff80dce087f616900965fd4196842" translate="yes" xml:space="preserve">
          <source>For classification, a somewhat important thing to note is that although a stateless feature extraction routine may be able to cope with new/unseen attributes, the incremental learner itself may be unable to cope with new/unseen targets classes. In this case you have to pass all the possible classes to the first &lt;code&gt;partial_fit&lt;/code&gt; call using the &lt;code&gt;classes=&lt;/code&gt; parameter.</source>
          <target state="translated">对于分类，需要注意的一点重要一点是，尽管无状态特征提取例程可能能够应对新的/看不见的属性，但是增量学习者本身可能无法应对新的/看不见的目标类。在这种情况下，您必须使用 &lt;code&gt;classes=&lt;/code&gt; 参数将所有可能的类传递给第一个 &lt;code&gt;partial_fit&lt;/code&gt; 调用。</target>
        </trans-unit>
        <trans-unit id="fda4c776ec3f2170d3e99301b50afa3144298d48" translate="yes" xml:space="preserve">
          <source>For classification, as in the labeling &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;iris&lt;/a&gt; task, linear regression is not the right approach as it will give too much weight to data far from the decision frontier. A linear approach is to fit a sigmoid function or &lt;strong&gt;logistic&lt;/strong&gt; function:</source>
          <target state="translated">对于分类，如在标记&lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;虹膜&lt;/a&gt;任务中一样，线性回归不是正确的方法，因为它将对远离决策边界的数据赋予过多的权重。线性方法是拟合S型或&lt;strong&gt;Logistic&lt;/strong&gt;函数：</target>
        </trans-unit>
        <trans-unit id="049512f622ab4a1900a694aa9ec5fcf56244d47a" translate="yes" xml:space="preserve">
          <source>For classification: &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt;&lt;code&gt;chi2&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.f_classif#sklearn.feature_selection.f_classif&quot;&gt;&lt;code&gt;f_classif&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt;&lt;code&gt;mutual_info_classif&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">对于分类：&lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt; &lt;code&gt;chi2&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.feature_selection.f_classif#sklearn.feature_selection.f_classif&quot;&gt; &lt;code&gt;f_classif&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt; &lt;code&gt;mutual_info_classif&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2b5c234d472222c920c562abfd3ef4ec80e6cfee" translate="yes" xml:space="preserve">
          <source>For comparison, a quantized image using a random codebook (colors picked up randomly) is also shown.</source>
          <target state="translated">为了进行比较,还显示了使用随机代码本(随机拾取的颜色)的量化图像。</target>
        </trans-unit>
        <trans-unit id="3be5878f7d3ebd4495804d5a6a55058ed71eceb1" translate="yes" xml:space="preserve">
          <source>For comparison, the documents are also clustered using MiniBatchKMeans. The document clusters derived from the biclusters achieve a better V-measure than clusters found by MiniBatchKMeans.</source>
          <target state="translated">为了比较,还使用MiniBatchKMeans对文档进行聚类。由双聚类得出的文档聚类比MiniBatchKMeans发现的聚类取得了更好的V-measure。</target>
        </trans-unit>
        <trans-unit id="6d421941474530194b10c6be8d7b89e2eb530191" translate="yes" xml:space="preserve">
          <source>For comparison, we also add the output from &lt;code&gt;preprocessing.QuantileTransformer&lt;/code&gt;. It can force any arbitrary distribution into a gaussian, provided that there are enough training samples (thousands). Because it is a non-parametric method, it is harder to interpret than the parametric ones (Box-Cox and Yeo-Johnson).</source>
          <target state="translated">为了进行比较，我们还添加了 &lt;code&gt;preprocessing.QuantileTransformer&lt;/code&gt; 的输出。只要有足够的训练样本（数千个），它就可以将任意分布强加给高斯。因为它是非参数方法，所以比参数方法（Box-Cox和Yeo-Johnson）更难解释。</target>
        </trans-unit>
        <trans-unit id="3fedc899dde91ba75e541f7b8d87d7a3665ca193" translate="yes" xml:space="preserve">
          <source>For compatibility, user code relying on this method should wrap its calls in &lt;code&gt;np.asarray&lt;/code&gt; to avoid type issues.</source>
          <target state="translated">为了兼容，依赖此方法的用户代码应将其调用包装在 &lt;code&gt;np.asarray&lt;/code&gt; 中,以避免类型问题。</target>
        </trans-unit>
        <trans-unit id="6f31aee2196032d492cf3c67f7f43ab3f6981996" translate="yes" xml:space="preserve">
          <source>For continuous parameters, such as &lt;code&gt;C&lt;/code&gt; above, it is important to specify a continuous distribution to take full advantage of the randomization. This way, increasing &lt;code&gt;n_iter&lt;/code&gt; will always lead to a finer search.</source>
          <target state="translated">对于连续参数，例如上面的 &lt;code&gt;C&lt;/code&gt; ，重要的是指定连续分布以充分利用随机化。这样，增加 &lt;code&gt;n_iter&lt;/code&gt; 总是会导致更好的搜索。</target>
        </trans-unit>
        <trans-unit id="4288bdf523218f188616117af5e7ab510c1e81a7" translate="yes" xml:space="preserve">
          <source>For cross-validation, we use 20-fold with 2 algorithms to compute the Lasso path: coordinate descent, as implemented by the LassoCV class, and Lars (least angle regression) as implemented by the LassoLarsCV class. Both algorithms give roughly the same results. They differ with regards to their execution speed and sources of numerical errors.</source>
          <target state="translated">对于交叉验证,我们使用20倍与2种算法来计算Lasso路径:LassoCV类实现的坐标下降和LassoLarsCV类实现的Lars(最小角度回归)。这两种算法给出的结果大致相同。它们的不同之处在于执行速度和数值误差的来源。</target>
        </trans-unit>
        <trans-unit id="c8cbf2457961ac615bed8ee5d0896fee418cd0e6" translate="yes" xml:space="preserve">
          <source>For custom messages if &amp;ldquo;%(name)s&amp;rdquo; is present in the message string, it is substituted for the estimator name.</source>
          <target state="translated">对于自定义消息，如果消息字符串中包含&amp;ldquo;％（name）s&amp;rdquo;，则将其替换为估算器名称。</target>
        </trans-unit>
        <trans-unit id="67e0a596cb4bf62edc844af1488251663768a571" translate="yes" xml:space="preserve">
          <source>For details on the precise mathematical formulation of the provided kernel functions and how &lt;code&gt;gamma&lt;/code&gt;, &lt;code&gt;coef0&lt;/code&gt; and &lt;code&gt;degree&lt;/code&gt; affect each other, see the corresponding section in the narrative documentation: &lt;a href=&quot;../svm#svm-kernels&quot;&gt;Kernel functions&lt;/a&gt;.</source>
          <target state="translated">有关所提供内核函数的精确数学公式以及 &lt;code&gt;gamma&lt;/code&gt; ， &lt;code&gt;coef0&lt;/code&gt; 和 &lt;code&gt;degree&lt;/code&gt; 如何相互影响的详细信息，请参见叙述性文档中的相应部分：&lt;a href=&quot;../svm#svm-kernels&quot;&gt;内核函数&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e4ed0f1e2f8642affc7014ca7518ae7bfac1b31c" translate="yes" xml:space="preserve">
          <source>For each class k an array of shape [n_features, n_k], with &lt;code&gt;n_k = min(n_features, number of elements in class k)&lt;/code&gt; It is the rotation of the Gaussian distribution, i.e. its principal axis.</source>
          <target state="translated">对于每个类别k，形状为[n_features，n_k]的形状的数组，其中 &lt;code&gt;n_k = min(n_features, number of elements in class k)&lt;/code&gt; 这是高斯分布的旋转，即其主轴。</target>
        </trans-unit>
        <trans-unit id="a5ae820e12dddee4457f060f6b705b304ca3a1e3" translate="yes" xml:space="preserve">
          <source>For each class k an array of shape [n_k]. It contains the scaling of the Gaussian distributions along its principal axes, i.e. the variance in the rotated coordinate system.</source>
          <target state="translated">对于每一类k,一个形状[n_k]的数组。它包含了沿其主轴的高斯分布的缩放,即旋转坐标系的方差。</target>
        </trans-unit>
        <trans-unit id="bb1ef71f090300eb5b67a4f2bd338bada7005d30" translate="yes" xml:space="preserve">
          <source>For each class of models we make the model complexity vary through the choice of relevant model parameters and measure the influence on both computational performance (latency) and predictive power (MSE or Hamming Loss).</source>
          <target state="translated">对于每一类模型,我们通过相关模型参数的选择使模型复杂度发生变化,并衡量对计算性能(延迟)和预测能力(MSE或汉明损失)的影响。</target>
        </trans-unit>
        <trans-unit id="51af5a1cbe5b213e1b6f97e9040e40d195d22222" translate="yes" xml:space="preserve">
          <source>For each component k, find the weights u, v that maximizes max corr(Xk u, Yk v), such that &lt;code&gt;|u| = |v| = 1&lt;/code&gt;</source>
          <target state="translated">对于每个分量k，找到使max corr（Xk u，Yk v）最大化的权重u，v，使得 &lt;code&gt;|u| = |v| = 1&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="0acc93a412fe0bc296f4de29ad2df21b9415e5fb" translate="yes" xml:space="preserve">
          <source>For each component k, find weights u, v that optimize:</source>
          <target state="translated">对于每个分量k,找到优化的权重u,v。</target>
        </trans-unit>
        <trans-unit id="34ffb7458447c8e84aeb0c0dcae78f73f1c9783e" translate="yes" xml:space="preserve">
          <source>For each component k, find weights u, v that optimizes: &lt;code&gt;max corr(Xk u, Yk v) * std(Xk u) std(Yk u)&lt;/code&gt;, such that &lt;code&gt;|u| = 1&lt;/code&gt;</source>
          <target state="translated">对于每个分量k，找到优化的权重u，v： &lt;code&gt;max corr(Xk u, Yk v) * std(Xk u) std(Yk u)&lt;/code&gt; ，使得 &lt;code&gt;|u| = 1&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f9db939b51ba3d78630796e1c0444915001bc251" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator.</source>
          <target state="translated">对于X中的每一个数据点x和合集中的每一棵树,返回叶子x在每个估计器中的索引。</target>
        </trans-unit>
        <trans-unit id="4571d700205de7840eb7f685393b9c35a449521a" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator. In the case of binary classification n_classes is 1.</source>
          <target state="translated">对于X中的每一个数据点x和合集中的每一棵树,返回叶子x在每个估计器中的索引。在二元分类的情况下,n_classes为1。</target>
        </trans-unit>
        <trans-unit id="44ac20da24a40ac490697d0897d69873261c6900" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in.</source>
          <target state="translated">对于X中的每一个数据点x和森林中的每一棵树,返回x最终所在的叶子的索引。</target>
        </trans-unit>
        <trans-unit id="d82941d49be2d46b8acb0305feded896c81f7a70" translate="yes" xml:space="preserve">
          <source>For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered within &lt;code&gt;[0; self.tree_.node_count)&lt;/code&gt;, possibly with gaps in the numbering.</source>
          <target state="translated">对于X中的每个数据点x，返回以x结尾的叶子的索引。叶子的编号在 &lt;code&gt;[0; self.tree_.node_count)&lt;/code&gt; ，可能在编号上存在间隙。</target>
        </trans-unit>
        <trans-unit id="7b85d3d77de0d2c34f470b25ce92cd73fbf8293c" translate="yes" xml:space="preserve">
          <source>For each dataset, 15% of samples are generated as random uniform noise. This proportion is the value given to the nu parameter of the OneClassSVM and the contamination parameter of the other outlier detection algorithms. Decision boundaries between inliers and outliers are displayed in black except for Local Outlier Factor (LOF) as it has no predict method to be applied on new data when it is used for outlier detection.</source>
          <target state="translated">对于每个数据集,15%的样本被生成为随机均匀噪声。这个比例是给OneClassSVM的nu参数和其他离群检测算法的污染参数的值。除了局部离群值因子(LOF)外,离群值和离群值之间的决策边界用黑色显示,因为它在用于离群值检测时,没有预测方法应用于新数据。</target>
        </trans-unit>
        <trans-unit id="894d665cd020f2e7e68923ae49f3798173d1a959" translate="yes" xml:space="preserve">
          <source>For each document &lt;code&gt;#i&lt;/code&gt;, count the number of occurrences of each word &lt;code&gt;w&lt;/code&gt; and store it in &lt;code&gt;X[i, j]&lt;/code&gt; as the value of feature &lt;code&gt;#j&lt;/code&gt; where &lt;code&gt;j&lt;/code&gt; is the index of word &lt;code&gt;w&lt;/code&gt; in the dictionary.</source>
          <target state="translated">对于每个文档 &lt;code&gt;#i&lt;/code&gt; ，计算每个单词 &lt;code&gt;w&lt;/code&gt; 的出现次数，并将其存储在 &lt;code&gt;X[i, j]&lt;/code&gt; 作为特征 &lt;code&gt;#j&lt;/code&gt; 的值，其中 &lt;code&gt;j&lt;/code&gt; 是词典中单词 &lt;code&gt;w&lt;/code&gt; 的索引。</target>
        </trans-unit>
        <trans-unit id="f8df1081a030b15d2d8afea6cd05ff167080d1cd" translate="yes" xml:space="preserve">
          <source>For each document \(d\), draw \(\theta_d \sim \mathrm{Dirichlet}(\alpha), \: d=1...D\)</source>
          <target state="translated">对于每份文件(d),画出(theta_d)(sim mathrm{Dirichlet}(alpha),d=1...D)。</target>
        </trans-unit>
        <trans-unit id="d88ce97fd881b7b3225357f99ece9e603e7378b5" translate="yes" xml:space="preserve">
          <source>For each observation, tells whether or not (+1 or -1) it should be considered as an inlier according to the fitted model.</source>
          <target state="translated">对于每个观测值,告诉人们根据拟合模型,是否应该将其视为(+1或-1)inlier。</target>
        </trans-unit>
        <trans-unit id="ceeb3b0129a3e252c3947f10f0ffdea6343158bd" translate="yes" xml:space="preserve">
          <source>For each pair of iris features, the decision tree learns decision boundaries made of combinations of simple thresholding rules inferred from the training samples.</source>
          <target state="translated">对于每一对虹膜特征,决策树学习由训练样本推断的简单阈值规则组合而成的决策边界。</target>
        </trans-unit>
        <trans-unit id="ebc579ef4368e5d7216210ea27aaa16d7216a1cd" translate="yes" xml:space="preserve">
          <source>For each sample, the generative process is:</source>
          <target state="translated">对于每个样本,生成过程是。</target>
        </trans-unit>
        <trans-unit id="884540e9625ad90cb4316f6468437987a56adbd4" translate="yes" xml:space="preserve">
          <source>For each topic \(k\), draw \(\beta_k \sim \mathrm{Dirichlet}(\eta),\: k =1...K\)</source>
          <target state="translated">对于每一个主题(k),画出(beta_k)(sim mathrm{Dirichlet}(eta),\:k =1...\ K)</target>
        </trans-unit>
        <trans-unit id="ebc60a8584824b9b236f9d24a5aa9b01b10427f0" translate="yes" xml:space="preserve">
          <source>For each value of &lt;code&gt;n_components&lt;/code&gt;, we plot:</source>
          <target state="translated">对于 &lt;code&gt;n_components&lt;/code&gt; 的每个值，我们绘制：</target>
        </trans-unit>
        <trans-unit id="6ad9736839514923dd21aa4c5541f4da9ec0e497" translate="yes" xml:space="preserve">
          <source>For each value of the &amp;lsquo;target&amp;rsquo; features in the &lt;code&gt;grid&lt;/code&gt; the partial dependence function need to marginalize the predictions of a tree over all possible values of the &amp;lsquo;complement&amp;rsquo; features. In decision trees this function can be evaluated efficiently without reference to the training data. For each grid point a weighted tree traversal is performed: if a split node involves a &amp;lsquo;target&amp;rsquo; feature, the corresponding left or right branch is followed, otherwise both branches are followed, each branch is weighted by the fraction of training samples that entered that branch. Finally, the partial dependence is given by a weighted average of all visited leaves. For tree ensembles the results of each individual tree are again averaged.</source>
          <target state="translated">对于 &lt;code&gt;grid&lt;/code&gt; &amp;ldquo;目标&amp;rdquo;特征的每个值，部分依赖函数需要在&amp;ldquo;互补&amp;rdquo;特征的所有可能值上边缘化树的预测。在决策树中，无需参考训练数据即可有效评估此功能。对于每个网格点，将执行加权树遍历：如果拆分节点涉及&amp;ldquo;目标&amp;rdquo;特征，则遵循相应的左或右分支，否则遵循两个分支，每个分支均按输入的训练样本的分数加权科。最后，偏倚由所有访问过的叶子的加权平均值给出。对于树集合，再次将每棵树的结果平均。</target>
        </trans-unit>
        <trans-unit id="719d4a30c8dd97a24789edd5bdd1f3a14cdc8a6c" translate="yes" xml:space="preserve">
          <source>For each word \(i\) in document \(d\):</source>
          <target state="translated">对于文件中的每一个字(i),(d)。</target>
        </trans-unit>
        <trans-unit id="d13eb916a8aa10457ca38f56195d8da451f22b82" translate="yes" xml:space="preserve">
          <source>For efficiency reasons, the euclidean distance between a pair of row vector x and y is computed as:</source>
          <target state="translated">为了提高效率,一对行向量x和y之间的欧氏距离计算为:。</target>
        </trans-unit>
        <trans-unit id="9aa1f675d2607b44cb2ebebaba9400cb8fdf4c6d" translate="yes" xml:space="preserve">
          <source>For evaluating multiple metrics, either give a list of (unique) strings or a dict with names as keys and callables as values.</source>
          <target state="translated">为了评估多个指标,可以给出一个(唯一的)字符串列表或一个以名字作为键和可调用值的dict。</target>
        </trans-unit>
        <trans-unit id="091a4026feae279bd855844156c1035b747b54da" translate="yes" xml:space="preserve">
          <source>For example &lt;code&gt;average_precision&lt;/code&gt; or the area under the roc curve can not be computed using discrete predictions alone.</source>
          <target state="translated">例如，不能单独使用离散预测来计算 &lt;code&gt;average_precision&lt;/code&gt; 或roc曲线下的面积。</target>
        </trans-unit>
        <trans-unit id="d73a71b2256308d0d5e12341f8e7d64f3e849f98" translate="yes" xml:space="preserve">
          <source>For example try instead of the &lt;code&gt;SVC&lt;/code&gt;:</source>
          <target state="translated">例如，尝试代替 &lt;code&gt;SVC&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="541edf61321b8728dd0c7cafa11d713cadb8fb1e" translate="yes" xml:space="preserve">
          <source>For example, a less computationally intensive alternative to &lt;code&gt;LeavePGroupsOut(p=10)&lt;/code&gt; would be &lt;code&gt;GroupShuffleSplit(test_size=10, n_splits=100)&lt;/code&gt;.</source>
          <target state="translated">例如，可以使用 &lt;code&gt;GroupShuffleSplit(test_size=10, n_splits=100)&lt;/code&gt; 代替 &lt;code&gt;LeavePGroupsOut(p=10)&lt;/code&gt; 来减少计算量。</target>
        </trans-unit>
        <trans-unit id="65cec63d764ed66c423ae4b0636bcfb02973f7ba" translate="yes" xml:space="preserve">
          <source>For example, a simple linear regression can be extended by constructing &lt;strong&gt;polynomial features&lt;/strong&gt; from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data:</source>
          <target state="translated">例如，可以通过从系数构造&lt;strong&gt;多项式特征&lt;/strong&gt;来扩展简单的线性回归。在标准线性回归的情况下，对于二维数据，您可能具有一个类似于以下的模型：</target>
        </trans-unit>
        <trans-unit id="f87725ef6020293ce39f30b54128c30f50570f0e" translate="yes" xml:space="preserve">
          <source>For example, if each point is just a single number (8 bytes), then an effective \(k\)-NN estimator in a paltry \(p \sim 20\) dimensions would require more training data than the current estimated size of the entire internet (&amp;plusmn;1000 Exabytes or so).</source>
          <target state="translated">例如，如果每个点仅是一个数字（8个字节），则在微不足道的\（p \ sim 20 \）中有效的\（k \）-NN估计量将需要比当前估计大小更多的训练数据。整个互联网（&amp;plusmn;1000艾字节左右）。</target>
        </trans-unit>
        <trans-unit id="5e44134c443036a12804aff41c3842c8f74c39ce" translate="yes" xml:space="preserve">
          <source>For example, in random projection, this warning is raised when the number of components, which quantifies the dimensionality of the target projection space, is higher than the number of features, which quantifies the dimensionality of the original source space, to imply that the dimensionality of the problem will not be reduced.</source>
          <target state="translated">例如,在随机投影中,当量化目标投影空间维度的分量数高于量化原始源空间维度的特征数时,就会发出这个警告,以暗示问题的维度不会降低。</target>
        </trans-unit>
        <trans-unit id="4bb7f297d1cf6895bcaff7553e1e2a2edd1164e9" translate="yes" xml:space="preserve">
          <source>For example, in the cases of multiple experiments, &lt;a href=&quot;generated/sklearn.model_selection.leaveonegroupout#sklearn.model_selection.LeaveOneGroupOut&quot;&gt;&lt;code&gt;LeaveOneGroupOut&lt;/code&gt;&lt;/a&gt; can be used to create a cross-validation based on the different experiments: we create a training set using the samples of all the experiments except one:</source>
          <target state="translated">例如，在多个实验的情况下，&lt;a href=&quot;generated/sklearn.model_selection.leaveonegroupout#sklearn.model_selection.LeaveOneGroupOut&quot;&gt; &lt;code&gt;LeaveOneGroupOut&lt;/code&gt; &lt;/a&gt;可用于基于不同的实验创建交叉验证：我们使用除一个实验之外的所有实验样本创建训练集：</target>
        </trans-unit>
        <trans-unit id="9dc98c27045ddaa13bc1efc30f0c70951a11ebf1" translate="yes" xml:space="preserve">
          <source>For example, let&amp;rsquo;s look at the results of a multinomial Naive Bayes classifier, which is fast to train and achieves a decent F-score:</source>
          <target state="translated">例如，让我们看一下多项朴素贝叶斯分类器的结果，该分类器可以快速训练并获得不错的F分数：</target>
        </trans-unit>
        <trans-unit id="85d6aa34dc31a086da5a0b5a46fa6367e968ccaf" translate="yes" xml:space="preserve">
          <source>For example, let&amp;rsquo;s say we&amp;rsquo;re dealing with a corpus of two documents: &lt;code&gt;['words', 'wprds']&lt;/code&gt;. The second document contains a misspelling of the word &amp;lsquo;words&amp;rsquo;. A simple bag of words representation would consider these two as very distinct documents, differing in both of the two possible features. A character 2-gram representation, however, would find the documents matching in 4 out of 8 features, which may help the preferred classifier decide better:</source>
          <target state="translated">例如，假设我们正在处理两个文档的主体： &lt;code&gt;['words', 'wprds']&lt;/code&gt; 。第二份文档包含单词&amp;ldquo; words&amp;rdquo;的拼写错误。一个简单的单词表示袋会将这两个文档视为非常不同的文档，两个可能的特征都不同。但是，字符2克表示法会在8个功能中的4个中找到匹配的文档，这可能有助于首选分类器更好地做出决定：</target>
        </trans-unit>
        <trans-unit id="a34fc3c98b3c662bed7829df5d3e68b291f14f22" translate="yes" xml:space="preserve">
          <source>For example, suppose that we have a first algorithm that extracts Part of Speech (PoS) tags that we want to use as complementary tags for training a sequence classifier (e.g. a chunker). The following dict could be such a window of features extracted around the word &amp;lsquo;sat&amp;rsquo; in the sentence &amp;lsquo;The cat sat on the mat.&amp;rsquo;:</source>
          <target state="translated">例如，假设我们有第一个算法提取词性（PoS）标签，我们希望将其用作训练序列分类器（例如，分块器）的补充标签。以下命令可能是这样的窗口：在&amp;ldquo;猫坐在垫子上&amp;rdquo;句子中的&amp;ldquo; sat&amp;rdquo;一词周围提取特征：</target>
        </trans-unit>
        <trans-unit id="875d6b4f0ebd92b4525ff9ff007e35a4ef09b992" translate="yes" xml:space="preserve">
          <source>For example, the following snippet uses &lt;code&gt;chardet&lt;/code&gt; (not shipped with scikit-learn, must be installed separately) to figure out the encoding of three texts. It then vectorizes the texts and prints the learned vocabulary. The output is not shown here.</source>
          <target state="translated">例如，以下代码段使用 &lt;code&gt;chardet&lt;/code&gt; （scikit-learn不附带，必须单独安装）确定三个文本的编码。然后对文本进行矢量化处理，并打印学习到的词汇。输出未在此处显示。</target>
        </trans-unit>
        <trans-unit id="a5b55b26c17fcbb577abf03053fdea355f9d1a85" translate="yes" xml:space="preserve">
          <source>For example, this warning may occur when the user</source>
          <target state="translated">例如,当用户在以下情况下可能会出现这个警告</target>
        </trans-unit>
        <trans-unit id="fa61683485e98ae724ab66c0cc502f9a28b6f341" translate="yes" xml:space="preserve">
          <source>For example, to download a dataset of gene expressions in mice brains:</source>
          <target state="translated">例如,要下载小鼠大脑的基因表达数据集。</target>
        </trans-unit>
        <trans-unit id="bea8752fb35944e93422e8c1c3a159c63e531901" translate="yes" xml:space="preserve">
          <source>For example, we can compute the tf-idf of the first term in the first document in the &lt;code&gt;counts&lt;/code&gt; array as follows:</source>
          <target state="translated">例如，我们可以如下计算 &lt;code&gt;counts&lt;/code&gt; 数组中第一个文档中第一个词的tf-idf ：</target>
        </trans-unit>
        <trans-unit id="8e5e851ba9e40a81086c0f41a19109e3eeaaee54" translate="yes" xml:space="preserve">
          <source>For example, when dealing with boolean features, \(x_i^n = x_i\) for all \(n\) and is therefore useless; but \(x_i x_j\) represents the conjunction of two booleans. This way, we can solve the XOR problem with a linear classifier:</source>
          <target state="translated">例如,在处理布尔特征时,对于所有的(n/),\(x_i^n=x_i/\),因此是无用的;但\(x_i x_j/\)代表两个布尔的结合。这样,我们就可以用线性分类器解决XOR问题。</target>
        </trans-unit>
        <trans-unit id="dfe2d757676022996b9aa748350ec295d5357a7e" translate="yes" xml:space="preserve">
          <source>For example, when using a validation set, set the &lt;code&gt;test_fold&lt;/code&gt; to 0 for all samples that are part of the validation set, and to -1 for all other samples.</source>
          <target state="translated">例如，使用验证集时，将属于验证集的所有样本的 &lt;code&gt;test_fold&lt;/code&gt; 设置为0，将所有其他样本的test_fold设置为-1。</target>
        </trans-unit>
        <trans-unit id="98a47ab600b3d6adb5169d4d316e6fcca6b662b8" translate="yes" xml:space="preserve">
          <source>For examples on how it is to be used refer to the sections below.</source>
          <target state="translated">关于如何使用的例子,请参考以下章节。</target>
        </trans-unit>
        <trans-unit id="1fd7dfc113fdc87e0b1f446fd7d77e9da35e9457" translate="yes" xml:space="preserve">
          <source>For further details on bias-variance decomposition, see section 7.3 of &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">有关偏差方差分解的更多详细信息，请参见&lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]的&lt;/a&gt; 7.3节。</target>
        </trans-unit>
        <trans-unit id="2f1f137cd461ac2e31c6cc087610e0dfea901ed1" translate="yes" xml:space="preserve">
          <source>For further details, &amp;ldquo;How to Use t-SNE Effectively&amp;rdquo; &lt;a href=&quot;http://distill.pub/2016/misread-tsne/&quot;&gt;http://distill.pub/2016/misread-tsne/&lt;/a&gt; provides a good discussion of the effects of various parameters, as well as interactive plots to explore those effects.</source>
          <target state="translated">有关更多详细信息，&amp;ldquo;如何有效使用t-SNE&amp;rdquo; &lt;a href=&quot;http://distill.pub/2016/misread-tsne/&quot;&gt;http://distill.pub/2016/misread-tsne/&lt;/a&gt;提供了有关各种参数效果的良好讨论，并提供了交互式图来探索这些效果。</target>
        </trans-unit>
        <trans-unit id="0f1bbe6e8000be72ab1e63dd45896ee0e4b6eb7a" translate="yes" xml:space="preserve">
          <source>For greyscale image data where pixel values can be interpreted as degrees of blackness on a white background, like handwritten digit recognition, the Bernoulli Restricted Boltzmann machine model (&lt;a href=&quot;../../modules/generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt;) can perform effective non-linear feature extraction.</source>
          <target state="translated">对于像手写数字识别这样的像素值可以解释为白色背景上的黑度的灰度图像数据，Bernoulli Restricted Boltzmann机器模型（&lt;a href=&quot;../../modules/generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; &lt;/a&gt;）可以执行有效的非线性特征提取。</target>
        </trans-unit>
        <trans-unit id="866314f9db098f25a642d6cb6f4a133adac68ce1" translate="yes" xml:space="preserve">
          <source>For high-dimensional datasets with many collinear regressors, &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt; is most often preferable. However, &lt;a href=&quot;generated/sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV&quot;&gt;&lt;code&gt;LassoLarsCV&lt;/code&gt;&lt;/a&gt; has the advantage of exploring more relevant values of &lt;code&gt;alpha&lt;/code&gt; parameter, and if the number of samples is very small compared to the number of features, it is often faster than &lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">对于具有许多共线性回归的高维数据集，&lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt; &lt;code&gt;LassoCV&lt;/code&gt; &lt;/a&gt;最常用。但是，&lt;a href=&quot;generated/sklearn.linear_model.lassolarscv#sklearn.linear_model.LassoLarsCV&quot;&gt; &lt;code&gt;LassoLarsCV&lt;/code&gt; &lt;/a&gt;的优势是可以探索更相关的 &lt;code&gt;alpha&lt;/code&gt; 参数值，并且如果与特征数量相比样本数量非常少，则通常比&lt;a href=&quot;generated/sklearn.linear_model.lassocv#sklearn.linear_model.LassoCV&quot;&gt; &lt;code&gt;LassoCV&lt;/code&gt; &lt;/a&gt;更快。</target>
        </trans-unit>
        <trans-unit id="aff7e1aca1f3054316321a609c5b24bbfe0a02a6" translate="yes" xml:space="preserve">
          <source>For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994.</source>
          <target state="translated">关于NIST预处理程序的信息,见M.D.Garris,J.L.Blue,G.T.Candela,D.L.Dimmick,J.Geist,P.J.Grother,S.A.Janet,and C.L.Wilson,NIST Form-Based Handprint Recognition System,NISTIR 5469,1994。</target>
        </trans-unit>
        <trans-unit id="ee57e485cfe4c61d12541e3ea6e169aab79014e8" translate="yes" xml:space="preserve">
          <source>For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.</source>
          <target state="translated">例如,一个由10,000个短文文档(如电子邮件)组成的集合将使用一个总大小为100,000个独特单词的词汇表,而每个文档将单独使用100到1000个独特单词。</target>
        </trans-unit>
        <trans-unit id="4e9f2f3fee78ca296cddfe5ea5b4e060f79a0a4e" translate="yes" xml:space="preserve">
          <source>For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.</source>
          <target state="translated">例如学习算法的目标函数中使用的许多元素(如支持向量机的RBF核或线性模型的L1和L2正则化器)都假设所有特征都以0为中心,并且具有相同顺序的方差。如果一个特征的方差比其他特征的方差大一个数量级,它可能会主导目标函数,使估计器无法像预期的那样从其他特征中正确学习。</target>
        </trans-unit>
        <trans-unit id="9c54ff6618aa4505fc044efee7a1b7aa2d457afd" translate="yes" xml:space="preserve">
          <source>For instance the below given table</source>
          <target state="translated">例如下面的表格</target>
        </trans-unit>
        <trans-unit id="61fc71afaf6946d5d1cad0702c1f4030326fcb83" translate="yes" xml:space="preserve">
          <source>For instance the groups could be the year of collection of the samples and thus allow for cross-validation against time-based splits.</source>
          <target state="translated">例如,各组可以是收集样本的年份,从而允许对基于时间的分割进行交叉验证。</target>
        </trans-unit>
        <trans-unit id="c007161505dacd37b43b63ce0c84bfbd3ce25160" translate="yes" xml:space="preserve">
          <source>For instance, assuming that the inlier data are Gaussian distributed, it will estimate the inlier location and covariance in a robust way (i.e. without being influenced by outliers). The Mahalanobis distances obtained from this estimate is used to derive a measure of outlyingness. This strategy is illustrated below.</source>
          <target state="translated">例如,假设离群数据是高斯分布的,它将以一种稳健的方式(即不受离群值的影响)估计离群位置和协方差。从这个估计中得到的马哈兰诺比斯距离被用来推导出离群值的测量。这一策略在下文中进行了说明。</target>
        </trans-unit>
        <trans-unit id="7519828cefe279ed0b1f593fd20db7243c914832" translate="yes" xml:space="preserve">
          <source>For instance, given a matrix of shape &lt;code&gt;(10, 10)&lt;/code&gt;, one possible bicluster with three rows and two columns induces a submatrix of shape &lt;code&gt;(3, 2)&lt;/code&gt;:</source>
          <target state="translated">例如，给定一个形状为 &lt;code&gt;(10, 10)&lt;/code&gt; 的矩阵，一个具有三行两列的可能的双曲线会诱导一个形状为 &lt;code&gt;(3, 2)&lt;/code&gt; 的子矩阵：</target>
        </trans-unit>
        <trans-unit id="66b8e52235d720becae09b00e19696b279a13527" translate="yes" xml:space="preserve">
          <source>For instance, if \(p\) singular vectors were calculated, the \(q\) best are found as described, where \(q&amp;lt;p\). Let \(U\) be the matrix with columns the \(q\) best left singular vectors, and similarly \(V\) for the right. To partition the rows, the rows of \(A\) are projected to a \(q\) dimensional space: \(A * V\). Treating the \(m\) rows of this \(m \times q\) matrix as samples and clustering using k-means yields the row labels. Similarly, projecting the columns to \(A^{\top} * U\) and clustering this \(n \times q\) matrix yields the column labels.</source>
          <target state="translated">例如，如果计算了\（p \）奇异矢量，则按照描述找到最佳的\（q \），其中\（q &amp;lt;p \）。令\（U \）为具有（\ q \）最佳左奇异向量，右为\（V \）列的矩阵。为了对行进行分区，\（A \）的行被投影到\（q \）维空间：\（A * V \）。将此\（m \ times q \）矩阵的\（m \）行视为样本，并使用k-means进行聚类，得到行标签。同样，将列投影到\（A ^ {\ top} * U \）并对该\（n \ times q \）矩阵进行聚类将产生列标签。</target>
        </trans-unit>
        <trans-unit id="040e034a022032a75dc3b85f4ce9de7cff88a6fc" translate="yes" xml:space="preserve">
          <source>For instance, if we work with 64x64 pixel gray-level pictures for face recognition, the dimensionality of the data is 4096 and it is slow to train an RBF support vector machine on such wide data. Furthermore we know that the intrinsic dimensionality of the data is much lower than 4096 since all pictures of human faces look somewhat alike. The samples lie on a manifold of much lower dimension (say around 200 for instance). The PCA algorithm can be used to linearly transform the data while both reducing the dimensionality and preserve most of the explained variance at the same time.</source>
          <target state="translated">例如,如果我们用64x64像素的灰度级图片来进行人脸识别,数据的维度是4096,在这么宽的数据上训练RBF支持向量机是很慢的。另外我们知道数据的内在维度要比4096低得多,因为所有的人脸图片看起来都有些相似。样本位于一个维度更低的歧管上(比如说200左右)。PCA算法可以用来对数据进行线性变换,同时既降低了维度,又保留了大部分的解释方差。</target>
        </trans-unit>
        <trans-unit id="f676ab37a8d5ec2f850de1fcd3ee779d6ce55a52" translate="yes" xml:space="preserve">
          <source>For instance, in the case of the digits dataset, &lt;code&gt;digits.data&lt;/code&gt; gives access to the features that can be used to classify the digits samples:</source>
          <target state="translated">例如，对于digits数据集， &lt;code&gt;digits.data&lt;/code&gt; 可以访问可用于对digits样本进行分类的功能：</target>
        </trans-unit>
        <trans-unit id="0b72faa109feccaf14265d5a54672e188bc4b73a" translate="yes" xml:space="preserve">
          <source>For instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.</source>
          <target state="translated">例如,在下面的例子中,决策树从数据中学习,用一组if-then-else决策规则来逼近一条正弦曲线。树越深,决策规则越复杂,模型越适合。</target>
        </trans-unit>
        <trans-unit id="bb4b6181584be99d136bb8fd3d82dd09da37eec0" translate="yes" xml:space="preserve">
          <source>For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.</source>
          <target state="translated">例如,学习算法的目标函数中使用的许多元素(如支持向量机的RBF核或线性模型的l1和l2正则化器)假设所有特征都以零为中心,并且具有相同顺序的方差。如果一个特征的方差比其他特征的方差大一个数量级,它可能会主导目标函数,使估计器不能像预期的那样从其他特征中正确学习。</target>
        </trans-unit>
        <trans-unit id="2500a85d8a54066d44afc291418c2bdbdb2d8331" translate="yes" xml:space="preserve">
          <source>For instance, the following shows 16 sample portraits (centered around 0.0) from the Olivetti dataset. On the right hand side are the first 16 singular vectors reshaped as portraits. Since we only require the top 16 singular vectors of a dataset with size \(n_{samples} = 400\) and \(n_{features} = 64 \times 64 = 4096\), the computation time is less than 1s:</source>
          <target state="translated">例如,下面显示了来自Olivetti数据集的16个样本肖像(以0.0为中心)。右手边是前16个奇异向量被重塑为肖像画。由于我们只需要一个数据集的前16个奇异向量,其大小为 \(n_{samples}=400\)和 \(n_{features}=64 \times 64=4096\),计算时间小于1s。</target>
        </trans-unit>
        <trans-unit id="8f189274df939cb66095eb188cc3a399c86cb294" translate="yes" xml:space="preserve">
          <source>For instance, we can perform a \(\chi^2\) test to the samples to retrieve only the two best features as follows:</source>
          <target state="translated">例如,我们可以对样本进行如下测试,只检索出两个最好的特征。</target>
        </trans-unit>
        <trans-unit id="abc897209b2f98b7966665fa36a5eddbbc44f66d" translate="yes" xml:space="preserve">
          <source>For instance:</source>
          <target state="translated">例如:</target>
        </trans-unit>
        <trans-unit id="21205df9d4ba13a75af14823666b84f64bd04084" translate="yes" xml:space="preserve">
          <source>For integer/None inputs &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="translated">对于整数/无输入，使用 &lt;code&gt;KFold&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f4cdf9352c6e062816193041b97f5514c42b421e" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, &lt;code&gt;KFold&lt;/code&gt; is used.</source>
          <target state="translated">对于整数/无输入，使用 &lt;code&gt;KFold&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f206c091dc56a7e693c1c1efe6b0899c57cec04a" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used, else, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">对于整数/无输入，如果 &lt;code&gt;y&lt;/code&gt; 是二进制或多类，则使用&lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt;，否则使用&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="84373ac49af10a751441a8470e060e3de62490b1" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. If &lt;code&gt;y&lt;/code&gt; is neither binary nor multiclass, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">对于整数/无输入，如果 &lt;code&gt;y&lt;/code&gt; 是二进制或多类，则使用&lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt;。如果 &lt;code&gt;y&lt;/code&gt; 既不是二进制也不是多类，则使用&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="8d4fea32021fed22e35126e0e01d0c620369dc78" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if &lt;code&gt;y&lt;/code&gt; is binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. If the estimator is a classifier or if &lt;code&gt;y&lt;/code&gt; is neither binary nor multiclass, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">对于整数/无输入，如果 &lt;code&gt;y&lt;/code&gt; 是二进制或多类，则使用&lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;sklearn.model_selection.StratifiedKFold&lt;/code&gt; &lt;/a&gt;。如果估计器是分类器，或者 &lt;code&gt;y&lt;/code&gt; 既不是二进制也不是多类，则使用&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;sklearn.model_selection.KFold&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="68ad85210cd514ab63c161f2689020aa738ee186" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if classifier is True and &lt;code&gt;y&lt;/code&gt; is either binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. In all other cases, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">对于整数/无输入，如果分类器为True， &lt;code&gt;y&lt;/code&gt; 为二进制或多类，则使用&lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt;。在所有其他情况下，都使用&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="821cadb32f750528bd31875526972b81201437b9" translate="yes" xml:space="preserve">
          <source>For integer/None inputs, if the estimator is a classifier and &lt;code&gt;y&lt;/code&gt; is either binary or multiclass, &lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; is used. In all other cases, &lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is used.</source>
          <target state="translated">对于整数/无输入，如果估计量是分类器， &lt;code&gt;y&lt;/code&gt; 是二进制或多类，则使用&lt;a href=&quot;sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt;。在所有其他情况下，都使用&lt;a href=&quot;sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="cfc9bcb00c8530f8c99a68584e1330a4da8fc56a" translate="yes" xml:space="preserve">
          <source>For intermediate values, we can see on the second plot that good models can be found on a diagonal of &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt;. Smooth models (lower &lt;code&gt;gamma&lt;/code&gt; values) can be made more complex by increasing the importance of classifying each point correctly (larger &lt;code&gt;C&lt;/code&gt; values) hence the diagonal of good performing models.</source>
          <target state="translated">对于中间值，我们可以在第二个图上看到可以在 &lt;code&gt;C&lt;/code&gt; 和 &lt;code&gt;gamma&lt;/code&gt; 的对角线上找到好的模型。通过增加正确分类每个点的重要性（较大的 &lt;code&gt;C&lt;/code&gt; 值），可以提高平滑模型（较低的 &lt;code&gt;gamma&lt;/code&gt; 值）的复杂度，从而提高对等性能模型的对角线。</target>
        </trans-unit>
        <trans-unit id="eb96f16ecd15a6be088f1dc93fa28ca4ca7ecca5" translate="yes" xml:space="preserve">
          <source>For kernel=&amp;rdquo;precomputed&amp;rdquo;, the expected shape of X is (n_samples_test, n_samples_train).</source>
          <target state="translated">对于内核=&amp;ldquo;预先计算&amp;rdquo;，X的预期形状为（n_samples_test，n_samples_train）。</target>
        </trans-unit>
        <trans-unit id="93c16e02e4641d6fe7bdb8a83439c237817b53cd" translate="yes" xml:space="preserve">
          <source>For kernel=&amp;rdquo;precomputed&amp;rdquo;, the expected shape of X is [n_samples_test, n_samples_train]</source>
          <target state="translated">对于内核=&amp;ldquo;预先计算&amp;rdquo;，X的预期形状为[n_samples_test，n_samples_train]</target>
        </trans-unit>
        <trans-unit id="9cb299cfc771ccbc3a241c16ffeed37fabbcff7c" translate="yes" xml:space="preserve">
          <source>For large dataset, you may also consider using &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; with &amp;lsquo;log&amp;rsquo; loss.</source>
          <target state="translated">对于大型数据集，您也可以考虑使用&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;并避免 'log'丢失。</target>
        </trans-unit>
        <trans-unit id="98187e1f181515ca77d41de7fa27ac44b69c7c11" translate="yes" xml:space="preserve">
          <source>For many estimators, including the SVMs, having datasets with unit standard deviation for each feature is important to get good prediction.</source>
          <target state="translated">对于许多估计器,包括SVMs,拥有每个特征的单位标准差的数据集对获得良好的预测很重要。</target>
        </trans-unit>
        <trans-unit id="f1359c1e0656157adbc7e3ee11ae253cb961bb70" translate="yes" xml:space="preserve">
          <source>For mono-output tasks it is:</source>
          <target state="translated">对于单项输出任务,它是:</target>
        </trans-unit>
        <trans-unit id="c24592da8118b35d1dd067bf2a75576669aef344" translate="yes" xml:space="preserve">
          <source>For more information see: Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) &amp;ldquo;Least Angle Regression,&amp;rdquo; Annals of Statistics (with discussion), 407-499. (&lt;a href=&quot;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&quot;&gt;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&lt;/a&gt;)</source>
          <target state="translated">有关更多信息，请参见：Bradley Efron，Trevor Hastie，Iain Johnstone和Robert Tibshirani（2004）&amp;ldquo;最小角度回归&amp;rdquo;，《统计年鉴》（带讨论），407-499。（&lt;a href=&quot;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&quot;&gt;http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="a4bc4c3f735998ec8c1614ec6127a37e3e7a02d8" translate="yes" xml:space="preserve">
          <source>For more information, see &lt;a href=&quot;../../modules/clustering#hierarchical-clustering&quot;&gt;Hierarchical clustering&lt;/a&gt;.</source>
          <target state="translated">有关更多信息，请参见&lt;a href=&quot;../../modules/clustering#hierarchical-clustering&quot;&gt;层次集群&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="b089e1ecd97ba592f22037a3c3fabf2924387c39" translate="yes" xml:space="preserve">
          <source>For more on usage see the &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">有关用法的更多信息，请参见《&lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;用户指南》&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="2e682df49d58d058f1f4b4c26ca6fb15a2f979d8" translate="yes" xml:space="preserve">
          <source>For multi-class classification, &lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt;&lt;code&gt;AdaBoostClassifier&lt;/code&gt;&lt;/a&gt; implements AdaBoost-SAMME and AdaBoost-SAMME.R &lt;a href=&quot;#zzrh2009&quot; id=&quot;id11&quot;&gt;[ZZRH2009]&lt;/a&gt;.</source>
          <target state="translated">对于多类分类，&lt;a href=&quot;generated/sklearn.ensemble.adaboostclassifier#sklearn.ensemble.AdaBoostClassifier&quot;&gt; &lt;code&gt;AdaBoostClassifier&lt;/code&gt; &lt;/a&gt;实现AdaBoost-SAMME和AdaBoost-SAMME.R &lt;a href=&quot;#zzrh2009&quot; id=&quot;id11&quot;&gt;[ZZRH2009]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="416ab9ed1829c79f2f091ddf9b13cfb5bb7486ce" translate="yes" xml:space="preserve">
          <source>For multi-class classification, n_class classifiers are trained in a one-versus-all approach. Concretely, this is implemented by taking advantage of the multi-variate response support in Ridge.</source>
          <target state="translated">对于多类分类,n_class分类器采用一比一的方式进行训练。具体来说,就是利用Ridge中的多变量响应支持来实现。</target>
        </trans-unit>
        <trans-unit id="65042013a5d26811a6a7088f4c47e70c6ddb0074" translate="yes" xml:space="preserve">
          <source>For multi-class models, you need to set the class label for which the PDPs should be created via the &lt;code&gt;label&lt;/code&gt; argument:</source>
          <target state="translated">对于多类模型，您需要通过 &lt;code&gt;label&lt;/code&gt; 参数设置应为其创建PDP的类标签：</target>
        </trans-unit>
        <trans-unit id="ccc2264ef7a998ec0c6ed9c92245080e0f0807c7" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, the scores for all the scorers are available in the &lt;code&gt;cv_results_&lt;/code&gt; dict at the keys ending with that scorer&amp;rsquo;s name (&lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt;) instead of &lt;code&gt;'_score'&lt;/code&gt; shown above. (&amp;lsquo;split0_test_precision&amp;rsquo;, &amp;lsquo;mean_train_precision&amp;rsquo; etc.)</source>
          <target state="translated">对于多指标评估，所有得分手的得分都可以在 &lt;code&gt;cv_results_&lt;/code&gt; dict中以该得分手的名字（ &lt;code&gt;'_&amp;lt;scorer_name&amp;gt;'&lt;/code&gt; ）结尾的键获得，而不是上面显示的 &lt;code&gt;'_score'&lt;/code&gt; 。（&amp;ldquo; split0_test_precision&amp;rdquo;，&amp;ldquo; mean_train_precision&amp;rdquo;等）</target>
        </trans-unit>
        <trans-unit id="6cd27769ef18013ec211b9a824f9bced7dd1ce74" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this attribute holds the validated &lt;code&gt;scoring&lt;/code&gt; dict which maps the scorer key to the scorer callable.</source>
          <target state="translated">对于多指标评估，此属性包含将 &lt;code&gt;scoring&lt;/code&gt; 者键映射到可调用得分者的已验证得分 dict。</target>
        </trans-unit>
        <trans-unit id="39c0f65b87a914c1f3244978c44bbc4d0d1190be" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this attribute is present only if &lt;code&gt;refit&lt;/code&gt; is specified.</source>
          <target state="translated">对于多指标评估，仅当指定 &lt;code&gt;refit&lt;/code&gt; ，才存在此属性。</target>
        </trans-unit>
        <trans-unit id="dd788cb84c37fa5cbd110321907573fb764ddce9" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this is not available if &lt;code&gt;refit&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;. See &lt;code&gt;refit&lt;/code&gt; parameter for more information.</source>
          <target state="translated">对于多指标评估，如果 &lt;code&gt;refit&lt;/code&gt; 为 &lt;code&gt;False&lt;/code&gt; ，则此功能不可用。有关更多信息，请参见 &lt;code&gt;refit&lt;/code&gt; 参数。</target>
        </trans-unit>
        <trans-unit id="4367b150d838b05eaff7170a1426f1dc4e6edc76" translate="yes" xml:space="preserve">
          <source>For multi-metric evaluation, this is present only if &lt;code&gt;refit&lt;/code&gt; is specified.</source>
          <target state="translated">对于多指标评估，仅当指定 &lt;code&gt;refit&lt;/code&gt; 时才存在。</target>
        </trans-unit>
        <trans-unit id="d328aa31182c6b57c5d921c1da1c7333c03486fe" translate="yes" xml:space="preserve">
          <source>For multi-output tasks it is:</source>
          <target state="translated">对于多输出任务,它是:</target>
        </trans-unit>
        <trans-unit id="22c12fa701004eab18f67dae2fb9f6cb3b68f428" translate="yes" xml:space="preserve">
          <source>For multi-output, the weights of each column of y will be multiplied.</source>
          <target state="translated">对于多输出,将y的每一列的权重相乘。</target>
        </trans-unit>
        <trans-unit id="77f8b599f18368a52e2142c2cada7c61eef9fbca" translate="yes" xml:space="preserve">
          <source>For multiclass classification with a &amp;ldquo;negative class&amp;rdquo;, it is possible to exclude some labels:</source>
          <target state="translated">对于带有&amp;ldquo;否定类&amp;rdquo;的多类分类，可以排除一些标签：</target>
        </trans-unit>
        <trans-unit id="393c73f8bfb73747a6a85987ccf51d73c8d3636f" translate="yes" xml:space="preserve">
          <source>For multiclass problems, only &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo;, &amp;lsquo;saga&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; handle multinomial loss; &amp;lsquo;liblinear&amp;rsquo; is limited to one-versus-rest schemes.</source>
          <target state="translated">对于多类问题，只有'newton-cg'，'sag'，'saga'和'lbfgs'处理多项式损失。&amp;ldquo; liblinear&amp;rdquo;仅限于&amp;ldquo;一站式&amp;rdquo;计划。</target>
        </trans-unit>
        <trans-unit id="de227a68bb98af9d7f682ac4556427f028c04d66" translate="yes" xml:space="preserve">
          <source>For multiple labels per instance, use &lt;a href=&quot;generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt;&lt;code&gt;MultiLabelBinarizer&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">对于每个实例多个标签，请使用&lt;a href=&quot;generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt; &lt;code&gt;MultiLabelBinarizer&lt;/code&gt; &lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="088c3cd08ec3b30c1e8d705dc9f773b25266ffd1" translate="yes" xml:space="preserve">
          <source>For multiple metric evaluation, this needs to be a string denoting the scorer is used to find the best parameters for refitting the estimator at the end.</source>
          <target state="translated">对于多度量评价,这需要是一个字符串,表示评分器用于寻找最佳参数,以便在最后重新装配估计器。</target>
        </trans-unit>
        <trans-unit id="ab406c3bb6ddaeec6408e58ba4985d8a5097ee33" translate="yes" xml:space="preserve">
          <source>For multiple metric evaluation, this needs to be a string denoting the scorer that would be used to find the best parameters for refitting the estimator at the end.</source>
          <target state="translated">对于多重度量评估,这需要是一个字符串,表示将用于寻找最佳参数的评分器,以便在最后重新装配估计器。</target>
        </trans-unit>
        <trans-unit id="f895ac59b8264ca94c275f903e2d6c6c438b4c9c" translate="yes" xml:space="preserve">
          <source>For multiplicative-update (&amp;lsquo;mu&amp;rsquo;) solver, the Frobenius norm (0.5 * ||X - WH||_Fro^2) can be changed into another beta-divergence loss, by changing the beta_loss parameter.</source>
          <target state="translated">对于乘更新（'mu'）求解器，可以通过更改beta_loss参数将Frobenius范数（0.5 * || X-WH || _Fro ^ 2）更改为另一个beta发散损失。</target>
        </trans-unit>
        <trans-unit id="4d0bed9bc5aa3b36bb0ba6ad6bc592a5bb3e78af" translate="yes" xml:space="preserve">
          <source>For n_components == &amp;lsquo;mle&amp;rsquo;, this class uses the method of &lt;code&gt;Minka, T. P. &amp;ldquo;Automatic choice of dimensionality for PCA&amp;rdquo;. In NIPS, pp. 598-604&lt;/code&gt;</source>
          <target state="translated">对于n_components =='mle'，此类使用 &lt;code&gt;Minka, T. P. &amp;ldquo;Automatic choice of dimensionality for PCA&amp;rdquo;. In NIPS, pp. 598-604&lt;/code&gt; 的方法TP&amp;ldquo; PCA的尺寸自动选择&amp;rdquo;。在NIPS中，第598-604页</target>
        </trans-unit>
        <trans-unit id="9f27cc961ae174b8e96b15764c2907159174c2c2" translate="yes" xml:space="preserve">
          <source>For non-sparse models, i.e. when there are not many zeros in &lt;code&gt;coef_&lt;/code&gt;, this may actually &lt;em&gt;increase&lt;/em&gt; memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with &lt;code&gt;(coef_ == 0).sum()&lt;/code&gt;, must be more than 50% for this to provide significant benefits.</source>
          <target state="translated">对于非稀疏模型，即 &lt;code&gt;coef_&lt;/code&gt; 中没有太多零时，这实际上可能会&lt;em&gt;增加&lt;/em&gt;内存使用量，因此请谨慎使用此方法。一条经验法则是，可以使用 &lt;code&gt;(coef_ == 0).sum()&lt;/code&gt; 计算的零元素数量必须大于50％，以提供显着的好处。</target>
        </trans-unit>
        <trans-unit id="e62cf2ed735d43186d3b55660d3dd2856258814f" translate="yes" xml:space="preserve">
          <source>For normalized mutual information and adjusted mutual information, the normalizing value is typically some &lt;em&gt;generalized&lt;/em&gt; mean of the entropies of each clustering. Various generalized means exist, and no firm rules exist for preferring one over the others. The decision is largely a field-by-field basis; for instance, in community detection, the arithmetic mean is most common. Each normalizing method provides &amp;ldquo;qualitatively similar behaviours&amp;rdquo; [YAT2016]. In our implementation, this is controlled by the &lt;code&gt;average_method&lt;/code&gt; parameter.</source>
          <target state="translated">对于归一化的互信息和调整后的互信息，归一化值通常是每个聚类的熵的某个&lt;em&gt;广义&lt;/em&gt;均值。存在各种广义的手段，并且不存在确定一种优先于另一种的固定规则。该决定在很大程度上是逐个领域进行的；例如，在社区检测中，算术平均值最为常见。每种归一化方法都提供&amp;ldquo;质量上相似的行为&amp;rdquo; [YAT2016]。在我们的实现中，这是由 &lt;code&gt;average_method&lt;/code&gt; 参数控制的。</target>
        </trans-unit>
        <trans-unit id="5573de0c3d8eae2871980794db1007675aa56eed" translate="yes" xml:space="preserve">
          <source>For now, we will consider the estimator as a black box:</source>
          <target state="translated">现在,我们将把估计器视为一个黑盒子。</target>
        </trans-unit>
        <trans-unit id="a7f8c21b68cc173c251c1992bff906fb9b13f276" translate="yes" xml:space="preserve">
          <source>For parameter estimation, the posterior distribution is:</source>
          <target state="translated">对于参数估计,后置分布为:。</target>
        </trans-unit>
        <trans-unit id="acaedbca04fb48c0d8436452cabe74f03629d275" translate="yes" xml:space="preserve">
          <source>For regression the default learning rate schedule is inverse scaling (&lt;code&gt;learning_rate='invscaling'&lt;/code&gt;), given by</source>
          <target state="translated">为了进行回归，默认学习率计划是反比例缩放（ &lt;code&gt;learning_rate='invscaling'&lt;/code&gt; ），由</target>
        </trans-unit>
        <trans-unit id="17d6bf85a6ee02e9c1e3f5f799f4a791f96ca437" translate="yes" xml:space="preserve">
          <source>For regression with a squared loss and a l2 penalty, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">对于具有平方损失和12罚分的回归，可以使用随机平均梯度（SAG）算法获得具有平均策略的SGD的另一个变体，该算法可作为&lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; 中&lt;/a&gt;的求解器使用。</target>
        </trans-unit>
        <trans-unit id="82e8631b28597b123d5803439222a08ee2e59047" translate="yes" xml:space="preserve">
          <source>For regression, &lt;a href=&quot;generated/sklearn.ensemble.adaboostregressor#sklearn.ensemble.AdaBoostRegressor&quot;&gt;&lt;code&gt;AdaBoostRegressor&lt;/code&gt;&lt;/a&gt; implements AdaBoost.R2 &lt;a href=&quot;#d1997&quot; id=&quot;id12&quot;&gt;[D1997]&lt;/a&gt;.</source>
          <target state="translated">对于回归，&lt;a href=&quot;generated/sklearn.ensemble.adaboostregressor#sklearn.ensemble.AdaBoostRegressor&quot;&gt; &lt;code&gt;AdaBoostRegressor&lt;/code&gt; &lt;/a&gt;实现AdaBoost.R2 &lt;a href=&quot;#d1997&quot; id=&quot;id12&quot;&gt;[D1997]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="faab950ebeb88e87f11e22c6efcd943439e07aea" translate="yes" xml:space="preserve">
          <source>For regression, MLP uses the Square Error loss function; written as,</source>
          <target state="translated">对于回归,MLP使用平方误差损失函数;写成:</target>
        </trans-unit>
        <trans-unit id="a8c842b7da02e24dac30b073413aa7112e52aecd" translate="yes" xml:space="preserve">
          <source>For regression: &lt;a href=&quot;generated/sklearn.feature_selection.f_regression#sklearn.feature_selection.f_regression&quot;&gt;&lt;code&gt;f_regression&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt;&lt;code&gt;mutual_info_regression&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">对于回归：&lt;a href=&quot;generated/sklearn.feature_selection.f_regression#sklearn.feature_selection.f_regression&quot;&gt; &lt;code&gt;f_regression&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt; &lt;code&gt;mutual_info_regression&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e8aa16ccbf6b94ae32b7c5b78e608f800f0eb6cd" translate="yes" xml:space="preserve">
          <source>For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was handled by returning a dense np.matrix instance. Going forward, np.ndarray returns an np.ndarray, as expected.</source>
          <target state="translated">在 scikit-learn 0.14.1 及以前的版本中,return_as=np.ndarray 是通过返回一个密集的 np.matrix 实例来处理的。今后,np.ndarray将按照预期返回一个np.ndarray。</target>
        </trans-unit>
        <trans-unit id="2410f1ccaa1a03065aaeec2b709967381feb9cea" translate="yes" xml:space="preserve">
          <source>For simple transformations, instead of a Transformer object, a pair of functions can be passed, defining the transformation and its inverse mapping:</source>
          <target state="translated">对于简单的变换,可以通过一对函数来代替Transformer对象,定义变换和它的反映射。</target>
        </trans-unit>
        <trans-unit id="2f72f7e3c1f68f97fc714ad1c06f3f5738fb15a6" translate="yes" xml:space="preserve">
          <source>For simplicity the equation above is written for a single training example. The gradient with respect to the weights is formed of two terms corresponding to the ones above. They are usually known as the positive gradient and the negative gradient, because of their respective signs. In this implementation, the gradients are estimated over mini-batches of samples.</source>
          <target state="translated">为了简单起见,上面的方程是针对单个训练实例写的。相对于权重的梯度是由两个项组成的,与上面的对应。它们通常被称为正梯度和负梯度,因为它们各自的符号。在本实施例中,梯度是通过微型样本批次来估计的。</target>
        </trans-unit>
        <trans-unit id="27c46746207f2a31ae25da1632ef3ccf3ef87e4d" translate="yes" xml:space="preserve">
          <source>For single metric evaluation, where the scoring parameter is a string, callable or None, the keys will be - &lt;code&gt;['test_score', 'fit_time', 'score_time']&lt;/code&gt;</source>
          <target state="translated">对于单指标评估，评分参数是字符串，可调用或无，则键将是- &lt;code&gt;['test_score', 'fit_time', 'score_time']&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a0a8bb77034843b440cd9ae1f7c55bd3aa47ece1" translate="yes" xml:space="preserve">
          <source>For small data sets (\(N\) less than 30 or so), \(\log(N)\) is comparable to \(N\), and brute force algorithms can be more efficient than a tree-based approach. Both &lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; address this through providing a &lt;em&gt;leaf size&lt;/em&gt; parameter: this controls the number of samples at which a query switches to brute-force. This allows both algorithms to approach the efficiency of a brute-force computation for small \(N\).</source>
          <target state="translated">对于较小的数据集（小于30的\（N \）），\（\ log（N）\）与\（N \）相当，并且蛮力算法可能比基于树的方法更有效。&lt;a href=&quot;generated/sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt;都通过提供一个&lt;em&gt;叶子大小&lt;/em&gt;参数来解决此问题：这控制了查询切换为蛮力时的样本数。这使两种算法都可以针对小\（N \）接近蛮力计算的效率。</target>
        </trans-unit>
        <trans-unit id="4638d963661692a289a12b8cac2d92a9d2c758fa" translate="yes" xml:space="preserve">
          <source>For small datasets, &amp;lsquo;liblinear&amp;rsquo; is a good choice, whereas &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;saga&amp;rsquo; are faster for large ones.</source>
          <target state="translated">对于小型数据集，&amp;ldquo; liblinear&amp;rdquo;是一个不错的选择，而对于大型数据集，&amp;ldquo; sag&amp;rdquo;和&amp;ldquo; saga&amp;rdquo;则更快。</target>
        </trans-unit>
        <trans-unit id="dffce5e2239efe7c22f00e78e9cfce148c8ba698" translate="yes" xml:space="preserve">
          <source>For some applications the amount of examples, features (or both) and/or the speed at which they need to be processed are challenging for traditional approaches. In these cases scikit-learn has a number of options you can consider to make your system scale.</source>
          <target state="translated">对于一些应用来说,例子的数量、特征(或两者)和/或需要处理的速度对于传统的方法来说是一个挑战。在这些情况下,scikit-learn有很多选择,您可以考虑让您的系统进行扩展。</target>
        </trans-unit>
        <trans-unit id="d0fa030cdd6e029de147eaddac9b22a69b1ced78" translate="yes" xml:space="preserve">
          <source>For some applications the performance (mainly latency and throughput at prediction time) of estimators is crucial. It may also be of interest to consider the training throughput but this is often less important in a production setup (where it often takes place offline).</source>
          <target state="translated">对于某些应用来说,估计器的性能(主要是预测时的延迟和吞吐量)是至关重要的。考虑训练吞吐量可能也很有意义,但在生产设置中,这往往不那么重要(通常是离线进行)。</target>
        </trans-unit>
        <trans-unit id="7400fa7073eb75f62370e5aadbb0f2aef8d5fc81" translate="yes" xml:space="preserve">
          <source>For some datasets, a pre-defined split of the data into training- and validation fold or into several cross-validation folds already exists. Using &lt;a href=&quot;generated/sklearn.model_selection.predefinedsplit#sklearn.model_selection.PredefinedSplit&quot;&gt;&lt;code&gt;PredefinedSplit&lt;/code&gt;&lt;/a&gt; it is possible to use these folds e.g. when searching for hyperparameters.</source>
          <target state="translated">对于某些数据集，已经预先定义了将数据分为训练折叠和验证折叠或几个交叉验证折叠的数据。使用&lt;a href=&quot;generated/sklearn.model_selection.predefinedsplit#sklearn.model_selection.PredefinedSplit&quot;&gt; &lt;code&gt;PredefinedSplit&lt;/code&gt; &lt;/a&gt;可以使用这些折叠，例如在搜索超参数时。</target>
        </trans-unit>
        <trans-unit id="c50bf24a893de08f1d0809fe397202f1a031fb85" translate="yes" xml:space="preserve">
          <source>For some miscellaneous data such as images, videos, and audio, you may wish to refer to:</source>
          <target state="translated">对于一些杂项数据,如图片、视频和音频,你可以参考。</target>
        </trans-unit>
        <trans-unit id="303cfe0bd7811405e77868ed843c4904556ebde5" translate="yes" xml:space="preserve">
          <source>For sparse input the data is &lt;strong&gt;converted to the Compressed Sparse Rows representation&lt;/strong&gt; (see &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt;) before being fed to efficient Cython routines. To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.</source>
          <target state="translated">对于稀疏输入，在将数据馈入有效的Cython例程之前， &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; 其&lt;strong&gt;转换为Compressed Sparse Rows表示形式&lt;/strong&gt;（请参见scipy.sparse.csr_matrix）。为避免不必要的内存复制，建议选择上游的CSR表示形式。</target>
        </trans-unit>
        <trans-unit id="44ae99861a7942ab4350b3f16d27ddb33207e51f" translate="yes" xml:space="preserve">
          <source>For sparse input the data is &lt;strong&gt;converted to the Compressed Sparse Rows representation&lt;/strong&gt; (see &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt;). To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.</source>
          <target state="translated">对于稀疏输入，数据将&lt;strong&gt;转换为&amp;ldquo;压缩稀疏行&amp;rdquo;表示形式&lt;/strong&gt;（请参见 &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; ）。为避免不必要的内存复制，建议选择上游的CSR表示形式。</target>
        </trans-unit>
        <trans-unit id="a6ddfb481ebd700db5464677bebe705030e704c9" translate="yes" xml:space="preserve">
          <source>For speed and space efficiency reasons &lt;code&gt;scikit-learn&lt;/code&gt; loads the target attribute as an array of integers that corresponds to the index of the category name in the &lt;code&gt;target_names&lt;/code&gt; list. The category integer id of each sample is stored in the &lt;code&gt;target&lt;/code&gt; attribute:</source>
          <target state="translated">出于速度和空间效率的原因， &lt;code&gt;scikit-learn&lt;/code&gt; 将target属性加载为整数数组，该整数数组对应于 &lt;code&gt;target_names&lt;/code&gt; 列表中类别名称的索引。每个样本的类别整数id存储在 &lt;code&gt;target&lt;/code&gt; 属性中：</target>
        </trans-unit>
        <trans-unit id="7ace947ef3298ab26b0edee5253deecf977a3b02" translate="yes" xml:space="preserve">
          <source>For speed, all real work is done at the C level in function copy_predict (libsvm_helper.c).</source>
          <target state="translated">为了提高速度,所有真正的工作都是在函数copy_predict(libsvm_helper.c)的C层完成的。</target>
        </trans-unit>
        <trans-unit id="1c3a0f29bcc1c543ffc020478b77d5b706223ce4" translate="yes" xml:space="preserve">
          <source>For splitting the data according to explicit domain-specific stratification of the dataset.</source>
          <target state="translated">用于根据明确的特定领域分层对数据集进行拆分。</target>
        </trans-unit>
        <trans-unit id="0adf7a63adc917db1adffd6d4cf61e05de34a6e7" translate="yes" xml:space="preserve">
          <source>For splitting the data according to explicit, domain-specific stratification of the dataset.</source>
          <target state="translated">用于根据明确的、特定领域的数据集分层来分割数据。</target>
        </trans-unit>
        <trans-unit id="ab4a742934d8510715858d09854f728742beaaec" translate="yes" xml:space="preserve">
          <source>For svd_solver == &amp;lsquo;arpack&amp;rsquo;, refer to &lt;code&gt;scipy.sparse.linalg.svds&lt;/code&gt;.</source>
          <target state="translated">对于svd_solver =='arpack'，请参考 &lt;code&gt;scipy.sparse.linalg.svds&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="25caaa7ea914cf58c60f262a55964ee17d21e090" translate="yes" xml:space="preserve">
          <source>For svd_solver == &amp;lsquo;randomized&amp;rsquo;, see: &lt;code&gt;Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). &amp;ldquo;Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions&amp;rdquo;. SIAM review, 53(2), 217-288.&lt;/code&gt; and also &lt;code&gt;Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). &amp;ldquo;A randomized algorithm for the decomposition of matrices&amp;rdquo;. Applied and Computational Harmonic Analysis, 30(1), 47-68.&lt;/code&gt;</source>
          <target state="translated">有关svd_solver =='randomized'的信息，请参见： &lt;code&gt;Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). &amp;ldquo;Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions&amp;rdquo;. SIAM review, 53(2), 217-288.&lt;/code&gt; 以及 &lt;code&gt;Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). &amp;ldquo;A randomized algorithm for the decomposition of matrices&amp;rdquo;. Applied and Computational Harmonic Analysis, 30(1), 47-68.&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="e7341be727234aad9fa4e331ba9d61b6fce122ff" translate="yes" xml:space="preserve">
          <source>For the &amp;lsquo;liblinear&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers set verbose to any positive number for verbosity.</source>
          <target state="translated">对于'liblinear'，'sag'和'lbfgs'求解器将冗长设置为冗长的任何正数。</target>
        </trans-unit>
        <trans-unit id="a081b6c50a07cf5457333031806f4c48e54ea42e" translate="yes" xml:space="preserve">
          <source>For the &lt;a href=&quot;classes#module-sklearn.svm&quot;&gt;&lt;code&gt;sklearn.svm&lt;/code&gt;&lt;/a&gt; family of algorithms with a non-linear kernel, the latency is tied to the number of support vectors (the fewer the faster). Latency and throughput should (asymptotically) grow linearly with the number of support vectors in a SVC or SVR model. The kernel will also influence the latency as it is used to compute the projection of the input vector once per support vector. In the following graph the &lt;code&gt;nu&lt;/code&gt; parameter of &lt;code&gt;sklearn.svm.classes.NuSVR&lt;/code&gt; was used to influence the number of support vectors.</source>
          <target state="translated">对于具有非线性内核的&lt;a href=&quot;classes#module-sklearn.svm&quot;&gt; &lt;code&gt;sklearn.svm&lt;/code&gt; &lt;/a&gt;算法系列，延迟与支持向量的数量有关（越少越快）。延迟和吞吐量应（渐近地）随着SVC或SVR模型中支持向量的数量线性增长。内核还将影响等待时间，因为它用于每个支持向量计算一次输入向量的投影。在下图中，使用了 &lt;code&gt;sklearn.svm.classes.NuSVR&lt;/code&gt; 的 &lt;code&gt;nu&lt;/code&gt; 参数来影响支持向量的数量。</target>
        </trans-unit>
        <trans-unit id="49dfac47eea992144c43ccc29f61713fabd0e5ae" translate="yes" xml:space="preserve">
          <source>For the &lt;code&gt;l2&lt;/code&gt; penalty case, the best result comes from the case where &lt;code&gt;C&lt;/code&gt; is not scaled.</source>
          <target state="translated">对于 &lt;code&gt;l2&lt;/code&gt; 罚金的情况，最好的结果来自于 &lt;code&gt;C&lt;/code&gt; 不缩放的情况。</target>
        </trans-unit>
        <trans-unit id="d9f81a56586341e43516abb99b238b1b5d6587c8" translate="yes" xml:space="preserve">
          <source>For the grid of Cs values (that are set by default to be ten values in a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter. In the case of newton-cg and lbfgs solvers, we warm start along the path i.e guess the initial coefficients of the present fit to be the coefficients got after convergence in the previous fit, so it is supposed to be faster for high-dimensional dense data.</source>
          <target state="translated">对于Cs值的网格(默认设置为1e-4和1e-4之间的10个对数值),最佳超参数由交叉验证器StratifiedKFold选择,但可以通过cv参数进行修改。在newton-cg和lbfgs求解器的情况下,我们沿着路径进行热启动,即把本次拟合的初始系数猜测为前一次拟合收敛后得到的系数,所以对于高维密集数据来说,应该是比较快的。</target>
        </trans-unit>
        <trans-unit id="93f0b6841feed67e5fc00af0443562656921cce7" translate="yes" xml:space="preserve">
          <source>For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.</source>
          <target state="translated">对于liblinear和lbfgs解算器,将verbose设置为任意正数的verbosity。</target>
        </trans-unit>
        <trans-unit id="4c4a7d0fb25ecd0231acfef000eb4ebb4024b077" translate="yes" xml:space="preserve">
          <source>For the most common use cases, you can designate a scorer object with the &lt;code&gt;scoring&lt;/code&gt; parameter; the table below shows all possible values. All scorer objects follow the convention that &lt;strong&gt;higher return values are better than lower return values&lt;/strong&gt;. Thus metrics which measure the distance between the model and the data, like &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;metrics.mean_squared_error&lt;/code&gt;&lt;/a&gt;, are available as neg_mean_squared_error which return the negated value of the metric.</source>
          <target state="translated">对于最常见的用例，您可以使用 &lt;code&gt;scoring&lt;/code&gt; 参数指定一个计分器对象。下表显示了所有可能的值。所有计分器对象均遵循以下约定：&lt;strong&gt;较高的返回值比较低的返回值更好&lt;/strong&gt;。因此，用于度量模型与数据之间距离的度量（如&lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;metrics.mean_squared_error&lt;/code&gt; )&lt;/a&gt;可用作neg_mean_squared_error，该度量返回度量的取反值。</target>
        </trans-unit>
        <trans-unit id="5c5d7d9872e083b1cf44dccc9ef51ebf6d1fc473" translate="yes" xml:space="preserve">
          <source>For the rationale behind the names &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt;, i.e. naive Bayes as a linear classifier, see J. Rennie et al. (2003), Tackling the poor assumptions of naive Bayes text classifiers, ICML.</source>
          <target state="translated">有关名称 &lt;code&gt;coef_&lt;/code&gt; 和 &lt;code&gt;intercept_&lt;/code&gt; 的基本原理，即朴素的贝叶斯作为线性分类器，请参见J. Rennie等。（2003年），处理朴素的贝叶斯文本分类器的糟糕假设，ICML。</target>
        </trans-unit>
        <trans-unit id="08233f540a36ed45603c5cb01e2e4f593cd79c27" translate="yes" xml:space="preserve">
          <source>For the simple task of finding the nearest neighbors between two sets of data, the unsupervised algorithms within &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; can be used:</source>
          <target state="translated">对于查找两组数据之间最近的邻居的简单任务，可以使用&lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; 中&lt;/a&gt;的无监督算法：</target>
        </trans-unit>
        <trans-unit id="daed0c58d42835f25cc91f4ef37c8c2918d442fd" translate="yes" xml:space="preserve">
          <source>For this data, we might want to encode the &lt;code&gt;'city'&lt;/code&gt; column as a categorical variable, but apply a &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt; to the &lt;code&gt;'title'&lt;/code&gt; column. As we might use multiple feature extraction methods on the same column, we give each transformer a unique name, say &lt;code&gt;'city_category'&lt;/code&gt; and &lt;code&gt;'title_bow'&lt;/code&gt;. By default, the remaining rating columns are ignored (&lt;code&gt;remainder='drop'&lt;/code&gt;):</source>
          <target state="translated">对于此数据，我们可能希望将 &lt;code&gt;'city'&lt;/code&gt; 列编码为分类变量，但将一个&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;feature_extraction.text.CountVectorizer&lt;/code&gt; &lt;/a&gt;应用于 &lt;code&gt;'title'&lt;/code&gt; 列。由于我们可能在同一列上使用多种特征提取方法，因此我们给每个转换器一个唯一的名称，例如 &lt;code&gt;'city_category'&lt;/code&gt; 和 &lt;code&gt;'title_bow'&lt;/code&gt; 。默认情况下，其余的评级列将被忽略（ &lt;code&gt;remainder='drop'&lt;/code&gt; ）：</target>
        </trans-unit>
        <trans-unit id="2e47bbc09921a29cf4a007e2d92242f5a8a9f3d8" translate="yes" xml:space="preserve">
          <source>For this example we will use the &lt;a href=&quot;http://mldata.org/repository/data/viewslug/yeast&quot;&gt;yeast&lt;/a&gt; dataset which contains 2417 datapoints each with 103 features and 14 possible labels. Each data point has at least one label. As a baseline we first train a logistic regression classifier for each of the 14 labels. To evaluate the performance of these classifiers we predict on a held-out test set and calculate the &lt;a href=&quot;../../modules/model_evaluation#jaccard-similarity-score&quot;&gt;jaccard similarity score&lt;/a&gt;.</source>
          <target state="translated">在此示例中，我们将使用&lt;a href=&quot;http://mldata.org/repository/data/viewslug/yeast&quot;&gt;酵母&lt;/a&gt;数据集，其中包含2417个数据点，每个数据点具有103个特征和14个可能的标签。每个数据点至少具有一个标签。作为基线，我们首先为14个标签中的每一个训练逻辑回归分类器。为了评估这些分类器的性能，我们在保留的测试集上进行预测并计算&lt;a href=&quot;../../modules/model_evaluation#jaccard-similarity-score&quot;&gt;jaccard相似度得分&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6ab77ac3ad8536d0d4bc113a409f055607cd6e01" translate="yes" xml:space="preserve">
          <source>For this method, M may be a dense matrix, sparse matrix, or general linear operator. Warning: ARPACK can be unstable for some problems. It is best to try several random seeds in order to check results.</source>
          <target state="translated">对于这种方法,M可以是稠密矩阵、稀疏矩阵或一般线性算子。警告:对于某些问题,ARPACK可能不稳定。ARPACK对于某些问题可能是不稳定的.最好是尝试几个随机种子,以便检查结果。</target>
        </trans-unit>
        <trans-unit id="201d282b655d8d026b78eb9f9255553e50678277" translate="yes" xml:space="preserve">
          <source>For this purpose, the estimators use a &amp;lsquo;connectivity&amp;rsquo; matrix, giving which samples are connected.</source>
          <target state="translated">为此，估计器使用&amp;ldquo;连接性&amp;rdquo;矩阵，给出了连接的样本。</target>
        </trans-unit>
        <trans-unit id="764ea2ccb7951b3db73f825ee916559c0e4bce1d" translate="yes" xml:space="preserve">
          <source>For this reason, the functions that load 20 Newsgroups data provide a parameter called &lt;strong&gt;remove&lt;/strong&gt;, telling it what kinds of information to strip out of each file. &lt;strong&gt;remove&lt;/strong&gt; should be a tuple containing any subset of &lt;code&gt;('headers', 'footers', 'quotes')&lt;/code&gt;, telling it to remove headers, signature blocks, and quotation blocks respectively.</source>
          <target state="translated">因此，加载20个新闻组数据的函数提供了一个称为&lt;strong&gt;remove&lt;/strong&gt;的参数，告诉它&lt;strong&gt;要从&lt;/strong&gt;每个文件中&lt;strong&gt;删除&lt;/strong&gt;哪些信息。&lt;strong&gt;remove&lt;/strong&gt;应该是一个包含 &lt;code&gt;('headers', 'footers', 'quotes')&lt;/code&gt; 任何子集的元组，告诉它分别删除标头，签名块和引号块。</target>
        </trans-unit>
        <trans-unit id="f2d2e6058597b408c702846b2d537e901630ce3a" translate="yes" xml:space="preserve">
          <source>For two clusters, it solves a convex relaxation of the &lt;a href=&quot;http://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf&quot;&gt;normalised cuts&lt;/a&gt; problem on the similarity graph: cutting the graph in two so that the weight of the edges cut is small compared to the weights of the edges inside each cluster. This criteria is especially interesting when working on images: graph vertices are pixels, and edges of the similarity graph are a function of the gradient of the image.</source>
          <target state="translated">对于两个聚类，它解决了相似性图上&lt;a href=&quot;http://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf&quot;&gt;归一化切割&lt;/a&gt;问题的凸松弛问题：将图切成两半，以使所切边缘的权重比每个聚类内边缘的权重小。在处理图像时，此标准特别有趣：图的顶点是像素，相似图的边缘是图像的梯度的函数。</target>
        </trans-unit>
        <trans-unit id="4092abbbb6ead577ab2b40e6704455f3cb4d3df5" translate="yes" xml:space="preserve">
          <source>For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data. See the &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;Glossary of Common Terms and API Elements&lt;/a&gt; entry on imputation.</source>
          <target state="translated">由于各种原因，许多现实世界的数据集包含缺失值，通常将其编码为空白，NaN或其他占位符。但是，此类数据集与scikit-learn估计器不兼容，后者假定数组中的所有值都是数字，并且都具有并具有含义。使用不完整数据集的基本策略是丢弃包含缺失值的整个行和/或列。但是，这是以丢失有价值的数据为代价的（即使数据不完整）。更好的策略是估算缺失值，即从数据的已知部分推断出缺失值。参见插补&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#glossary&quot;&gt;的通用术语表和API元素&lt;/a&gt;条目。</target>
        </trans-unit>
        <trans-unit id="7f12e7919ffa1c3009c9eefff46504b7c0642e13" translate="yes" xml:space="preserve">
          <source>For visualization purpose (which is the main use case of t-SNE), using the Barnes-Hut method is strongly recommended. The exact t-SNE method is useful for checking the theoretically properties of the embedding possibly in higher dimensional space but limit to small datasets due to computational constraints.</source>
          <target state="translated">对于可视化目的(这是t-SNE的主要用途),强烈建议使用Barnes-Hut方法。确切的t-SNE方法对于检查嵌入的理论属性是有用的,可能在更高维度的空间,但由于计算的限制,仅限于小数据集。</target>
        </trans-unit>
        <trans-unit id="e7a5b4b1244321faa67509dff73df9a23d7da1b3" translate="yes" xml:space="preserve">
          <source>For visualization purposes, given a bicluster, the rows and columns of the data matrix may be rearranged to make the bicluster contiguous.</source>
          <target state="translated">为了可视化的目的,给定一个双簇,数据矩阵的行和列可以重新排列,使双簇连续。</target>
        </trans-unit>
        <trans-unit id="d62d3122e2e4eef979e7c46fd629936aec0233be" translate="yes" xml:space="preserve">
          <source>For visualization purposes, we need to lay out the different symbols on a 2D canvas. For this we use &lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;Manifold learning&lt;/a&gt; techniques to retrieve 2D embedding.</source>
          <target state="translated">出于可视化目的，我们需要在2D画布上布置不同的符号。为此，我们使用&lt;a href=&quot;../../modules/manifold#manifold&quot;&gt;流形学习&lt;/a&gt;技术来检索2D嵌入。</target>
        </trans-unit>
        <trans-unit id="c502fd7960fae5affa9295a7a329adeddad6ab37" translate="yes" xml:space="preserve">
          <source>Force row-by-row generation by reducing &lt;code&gt;working_memory&lt;/code&gt;:</source>
          <target state="translated">通过减少 &lt;code&gt;working_memory&lt;/code&gt; 强制逐行生成：</target>
        </trans-unit>
        <trans-unit id="4bb98e5d778957b0dd66fa6aed87be22d170768c" translate="yes" xml:space="preserve">
          <source>Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.</source>
          <target state="translated">Forina,M.et al,PARVUS-An Extendible Package for Data Exploration,Classification and Correlation.Institute of Pharmaceutical and Food Analysis and Technologies,Via Brigata Salerno,16147 Genoa,Italy.</target>
        </trans-unit>
        <trans-unit id="35705e005c1f18ed14dab92df9e1435742858283" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the average precision is defined as</source>
          <target state="translated">从形式上看,给定一个二进制指标矩阵的地面真相标签/(y \in \left\{0,1 \right\}^{n_text{samples}\times n_text{labels}})和与每个标签相关的分数/(\hat{f}\in \mathbb{R}^{n_text{samples}\times n_text{labels}}/),平均精度定义为:1.</target>
        </trans-unit>
        <trans-unit id="4f395914b8fb9e643646835cc07cbc38c9742edc" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the coverage is defined as</source>
          <target state="translated">从形式上看,给定一个二进制指标矩阵的地面真相标签(y \in \left\{0,1 \right\}^{n_text{samples}\times n_text{labels}})和与每个标签相关的分数(\hat{f}\in \mathbb{R}^{n_text{samples}\times n_text{labels}}),覆盖率被定义为</target>
        </trans-unit>
        <trans-unit id="c175d46f254d733413b5b0ee831c9d600136a7b6" translate="yes" xml:space="preserve">
          <source>Formally, given a binary indicator matrix of the ground truth labels \(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the score associated with each label \(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\), the ranking loss is defined as</source>
          <target state="translated">从形式上看,给定一个二进制指标矩阵的地面真相标签/(y \in \left\{0,1 \right\}^{n_text{samples}\times n_text{labels}})和与每个标签相关的分数/(\hat{f}\in \mathbb{R}^{n_text{samples}\times n_text{labels}}/),排名损失定义如下</target>
        </trans-unit>
        <trans-unit id="47fb5045fef615598469a37da8a59110352753ff" translate="yes" xml:space="preserve">
          <source>Forms an affinity matrix given by the specified function and applies spectral decomposition to the corresponding graph laplacian. The resulting transformation is given by the value of the eigenvectors for each data point.</source>
          <target state="translated">形成一个由指定函数给定的亲和矩阵,并对相应的图形拉普拉斯进行光谱分解。所产生的变换由每个数据点的特征向量值给出。</target>
        </trans-unit>
        <trans-unit id="638babaaa209a18fe959f40f19725a01af068351" translate="yes" xml:space="preserve">
          <source>Fortunately, &lt;strong&gt;most values in X will be zeros&lt;/strong&gt; since for a given document less than a few thousand distinct words will be used. For this reason we say that bags of words are typically &lt;strong&gt;high-dimensional sparse datasets&lt;/strong&gt;. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory.</source>
          <target state="translated">幸运的是，&lt;strong&gt;X中的大多数值将为零，&lt;/strong&gt;因为对于给定的文档，将使用少于数千个不同的单词。因此，我们说单词袋通常是&lt;strong&gt;高维稀疏数据集&lt;/strong&gt;。通过仅将特征向量的非零部分存储在内存中，我们可以节省大量内存。</target>
        </trans-unit>
        <trans-unit id="659b18cdaec75234c8e955e09af5dc004ab6498a" translate="yes" xml:space="preserve">
          <source>Frequently asked questions about the project and contributing.</source>
          <target state="translated">关于项目和贡献的常见问题。</target>
        </trans-unit>
        <trans-unit id="c378e5372fbcc6968de3f23916b1cc385b9617be" translate="yes" xml:space="preserve">
          <source>Friedman et al, &lt;a href=&quot;http://biostatistics.oxfordjournals.org/content/9/3/432.short&quot;&gt;&amp;ldquo;Sparse inverse covariance estimation with the graphical lasso&amp;rdquo;&lt;/a&gt;, Biostatistics 9, pp 432, 2008</source>
          <target state="translated">Friedman等人，&lt;a href=&quot;http://biostatistics.oxfordjournals.org/content/9/3/432.short&quot;&gt;&amp;ldquo;使用图形套索进行稀疏逆协方差估计&amp;rdquo;&lt;/a&gt;，《生物统计》第9期，第432页，2008年</target>
        </trans-unit>
        <trans-unit id="8438e27109208985a133518d65493568dedc6924" translate="yes" xml:space="preserve">
          <source>Friedman, &amp;ldquo;Stochastic Gradient Boosting&amp;rdquo;, 1999</source>
          <target state="translated">弗里德曼，&amp;ldquo;随机梯度提升&amp;rdquo;，1999年</target>
        </trans-unit>
        <trans-unit id="9fcad16d5a3614a8ac9a3dd3615a46004936d92d" translate="yes" xml:space="preserve">
          <source>Friedman, Stochastic Gradient Boosting, 1999</source>
          <target state="translated">弗里德曼,《随机梯度提升》,1999年。</target>
        </trans-unit>
        <trans-unit id="d93c5ad51427861e9927c0f93ba75f21fa7b3769" translate="yes" xml:space="preserve">
          <source>Frobenius norm of the matrix difference, or beta-divergence, between the training data &lt;code&gt;X&lt;/code&gt; and the reconstructed data &lt;code&gt;WH&lt;/code&gt; from the fitted model.</source>
          <target state="translated">训练数据 &lt;code&gt;X&lt;/code&gt; 与拟合模型中的重构数据 &lt;code&gt;WH&lt;/code&gt; 之间的矩阵差或&amp;beta;散度的Frobenius范数。</target>
        </trans-unit>
        <trans-unit id="2a2bd03e6f160e636919837a5a755bde731a1eeb" translate="yes" xml:space="preserve">
          <source>From images</source>
          <target state="translated">来自图片</target>
        </trans-unit>
        <trans-unit id="5ff0ffd1e24dbd90ba4e307313dc3fed8b0cd6c4" translate="yes" xml:space="preserve">
          <source>From occurrences to frequencies</source>
          <target state="translated">从发生到频率</target>
        </trans-unit>
        <trans-unit id="4a6ea847ae49dd26abc66504268644d690f3206b" translate="yes" xml:space="preserve">
          <source>From scikit-learn: [&amp;lsquo;cityblock&amp;rsquo;, &amp;lsquo;cosine&amp;rsquo;, &amp;lsquo;euclidean&amp;rsquo;, &amp;lsquo;l1&amp;rsquo;, &amp;lsquo;l2&amp;rsquo;, &amp;lsquo;manhattan&amp;rsquo;]. These metrics support sparse matrix inputs.</source>
          <target state="translated">来自scikit-learn：['cityblock'，'cosine'，'euclidean'，'l1'，'l2'，'manhattan']。这些指标支持稀疏矩阵输入。</target>
        </trans-unit>
        <trans-unit id="6d8d962b98fbbe50de709ee2f1e71db53d579c2e" translate="yes" xml:space="preserve">
          <source>From scipy.spatial.distance: [&amp;lsquo;braycurtis&amp;rsquo;, &amp;lsquo;canberra&amp;rsquo;, &amp;lsquo;chebyshev&amp;rsquo;, &amp;lsquo;correlation&amp;rsquo;, &amp;lsquo;dice&amp;rsquo;, &amp;lsquo;hamming&amp;rsquo;, &amp;lsquo;jaccard&amp;rsquo;, &amp;lsquo;kulsinski&amp;rsquo;, &amp;lsquo;mahalanobis&amp;rsquo;, &amp;lsquo;minkowski&amp;rsquo;, &amp;lsquo;rogerstanimoto&amp;rsquo;, &amp;lsquo;russellrao&amp;rsquo;, &amp;lsquo;seuclidean&amp;rsquo;, &amp;lsquo;sokalmichener&amp;rsquo;, &amp;lsquo;sokalsneath&amp;rsquo;, &amp;lsquo;sqeuclidean&amp;rsquo;, &amp;lsquo;yule&amp;rsquo;] See the documentation for scipy.spatial.distance for details on these metrics. These metrics do not support sparse matrix inputs.</source>
          <target state="translated">来自scipy.spatial.distance：['braycurtis'，'canberra'，'chebyshev'，'correlation'，'dice，'hamming'，'jaccard'，'kulsinski'，'mahalanobis'，'minkowski'，'rogerstanimoto '，'russellrao'，'seuclidean'，'sokalmichener'，'sokalsneath'，'sqeuclidean'，'yule']有关这些指标的详细信息，请参见scipy.spatial.distance的文档。这些度量标准不支持稀疏矩阵输入。</target>
        </trans-unit>
        <trans-unit id="d3990f36d057d6745fedc272447b2563e02193f7" translate="yes" xml:space="preserve">
          <source>From text</source>
          <target state="translated">来自文本</target>
        </trans-unit>
        <trans-unit id="13bce2493b501286a428cebcb4e0bc57e6083c63" translate="yes" xml:space="preserve">
          <source>From the implementation point of view, this is just plain Ordinary Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.</source>
          <target state="translated">从实现的角度来看,这只是普通的普通最小二乘法(scipy.linalg.lstsq),包装成一个预测对象。</target>
        </trans-unit>
        <trans-unit id="ae8e3bdf9c1967ed71af43f356a6c2d5d1712708" translate="yes" xml:space="preserve">
          <source>From the programming standpoint, it is interesting because it shows how to use the online API of the scikit-learn to process a very large dataset by chunks. The way we proceed is that we load an image at a time and extract randomly 50 patches from this image. Once we have accumulated 500 of these patches (using 10 images), we run the &lt;code&gt;partial_fit&lt;/code&gt; method of the online KMeans object, MiniBatchKMeans.</source>
          <target state="translated">从编程的角度来看，这很有趣，因为它显示了如何使用scikit-learn的在线API来按块处理非常大的数据集。进行的方式是一次加载一张图像，并从该图像中随机抽取50个色块。一旦我们累积了500个这些补丁（使用10张图像），便运行在线KMeans对象MiniBatchKMeans 的 &lt;code&gt;partial_fit&lt;/code&gt; 方法。</target>
        </trans-unit>
        <trans-unit id="f1e410ad1472b42cb42cc98962428637290b6706" translate="yes" xml:space="preserve">
          <source>Function</source>
          <target state="translated">Function</target>
        </trans-unit>
        <trans-unit id="9f410a9e5384dfe1720c4cd228fe7bb63965656b" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores. Default is f_classif (see below &amp;ldquo;See also&amp;rdquo;). The default function only works with classification tasks.</source>
          <target state="translated">函数采用两个数组X和y，并返回一对数组（分数，pvalue）或带分数的单个数组。默认值为f_classif（请参见下文&amp;ldquo;另请参见&amp;rdquo;）。默认功能仅适用于分类任务。</target>
        </trans-unit>
        <trans-unit id="a8696032e0adf35ffec7c9da28cd036adeb91c99" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below &amp;ldquo;See also&amp;rdquo;). The default function only works with classification tasks.</source>
          <target state="translated">函数接受两个数组X和y，并返回一对数组（分数，pvalue）。默认值为f_classif（请参见下文&amp;ldquo;另请参见&amp;rdquo;）。默认功能仅适用于分类任务。</target>
        </trans-unit>
        <trans-unit id="acf1f055cd0885a9fc7d245efda7d1c727fca691" translate="yes" xml:space="preserve">
          <source>Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). For modes &amp;lsquo;percentile&amp;rsquo; or &amp;lsquo;kbest&amp;rsquo; it can return a single array scores.</source>
          <target state="translated">函数接受两个数组X和y，并返回一对数组（分数，pvalue）。对于&amp;ldquo;百分位数&amp;rdquo;或&amp;ldquo; kbest&amp;rdquo;模式，它可以返回单个数组分数。</target>
        </trans-unit>
        <trans-unit id="0c64f21c81859fb42c302c0d2cd301e40332c2c7" translate="yes" xml:space="preserve">
          <source>Function to apply to &lt;code&gt;y&lt;/code&gt; before passing to &lt;code&gt;fit&lt;/code&gt;. Cannot be set at the same time as &lt;code&gt;transformer&lt;/code&gt;. The function needs to return a 2-dimensional array. If &lt;code&gt;func&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, the function used will be the identity function.</source>
          <target state="translated">在传递给 &lt;code&gt;fit&lt;/code&gt; 之前应用于 &lt;code&gt;y&lt;/code&gt; 的函数。不能与 &lt;code&gt;transformer&lt;/code&gt; 同时设置。该函数需要返回一个二维数组。如果 &lt;code&gt;func&lt;/code&gt; 为 &lt;code&gt;None&lt;/code&gt; ，则使用的功能将是身份功能。</target>
        </trans-unit>
        <trans-unit id="f712e33ad68950dd5132b77ad3129994bf2cbbce" translate="yes" xml:space="preserve">
          <source>Function to apply to the prediction of the regressor. Cannot be set at the same time as &lt;code&gt;transformer&lt;/code&gt; as well. The function needs to return a 2-dimensional array. The inverse function is used to return predictions to the same space of the original training labels.</source>
          <target state="translated">应用于预测回归的功能。不能与 &lt;code&gt;transformer&lt;/code&gt; 同时设置。该函数需要返回一个二维数组。逆函数用于将预测返回到原始训练标签的相同空间。</target>
        </trans-unit>
        <trans-unit id="2b961dea1dc0c60ddf9a2c8e9d090f6f7d082483" translate="yes" xml:space="preserve">
          <source>Functions</source>
          <target state="translated">Functions</target>
        </trans-unit>
        <trans-unit id="c216053588b385d3de175b467017426b8b421912" translate="yes" xml:space="preserve">
          <source>Further discussion on the importance of centering and scaling data is available on this FAQ: &lt;a href=&quot;http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html&quot;&gt;Should I normalize/standardize/rescale the data?&lt;/a&gt;</source>
          <target state="translated">常见问题解答中提供了有关对数据进行居中和缩放的重要性的进一步讨论：&lt;a href=&quot;http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html&quot;&gt;是否应该对数据进行归一化/标准化/缩放？&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="606b0f774f1d5e4151969dbb768df62ebca8a20e" translate="yes" xml:space="preserve">
          <source>Further removes the linear correlation across features with &amp;lsquo;whiten=True&amp;rsquo;.</source>
          <target state="translated">进一步删除带有'whiten = True'的特征之间的线性相关性。</target>
        </trans-unit>
        <trans-unit id="ec9ba56eabfa3f70786eb84612f0623df80dfc4d" translate="yes" xml:space="preserve">
          <source>Further, the model supports &lt;a href=&quot;multiclass#multiclass&quot;&gt;multi-label classification&lt;/a&gt; in which a sample can belong to more than one class. For each class, the raw output passes through the logistic function. Values larger or equal to &lt;code&gt;0.5&lt;/code&gt; are rounded to &lt;code&gt;1&lt;/code&gt;, otherwise to &lt;code&gt;0&lt;/code&gt;. For a predicted output of a sample, the indices where the value is &lt;code&gt;1&lt;/code&gt; represents the assigned classes of that sample:</source>
          <target state="translated">此外，该模型支持&lt;a href=&quot;multiclass#multiclass&quot;&gt;多标签分类&lt;/a&gt;，其中样本可以属于一个以上的类别。对于每个类，原始输出通过逻辑函数传递。大于或等于 &lt;code&gt;0.5&lt;/code&gt; 的值将四舍五入为 &lt;code&gt;1&lt;/code&gt; ，否则为 &lt;code&gt;0&lt;/code&gt; 。对于样本的预测输出，值为 &lt;code&gt;1&lt;/code&gt; 的索引表示该样本的分配类别：</target>
        </trans-unit>
        <trans-unit id="cc118108875cca01a2724ce6e20debf4e124a846" translate="yes" xml:space="preserve">
          <source>Furthermore, &lt;a href=&quot;generated/sklearn.metrics.adjusted_rand_score#sklearn.metrics.adjusted_rand_score&quot;&gt;&lt;code&gt;adjusted_rand_score&lt;/code&gt;&lt;/a&gt; is &lt;strong&gt;symmetric&lt;/strong&gt;: swapping the argument does not change the score. It can thus be used as a &lt;strong&gt;consensus measure&lt;/strong&gt;:</source>
          <target state="translated">此外，&lt;a href=&quot;generated/sklearn.metrics.adjusted_rand_score#sklearn.metrics.adjusted_rand_score&quot;&gt; &lt;code&gt;adjusted_rand_score&lt;/code&gt; &lt;/a&gt;是&lt;strong&gt;对称的&lt;/strong&gt;：交换参数不会更改分数。因此，它可以用作&lt;strong&gt;共识措施&lt;/strong&gt;：</target>
        </trans-unit>
        <trans-unit id="7218de362b7befd5a71b1a5a01365e3552aa1087" translate="yes" xml:space="preserve">
          <source>Furthermore, it also shows the evolution of the performance of different algorithms with the number of processed examples.</source>
          <target state="translated">此外,它还显示了不同算法的性能随处理实例数量的变化。</target>
        </trans-unit>
        <trans-unit id="17753e7322d4f150d032ddf1f2dbdf4fe6d38592" translate="yes" xml:space="preserve">
          <source>Furthermore, the default parameter &lt;code&gt;smooth_idf=True&lt;/code&gt; adds &amp;ldquo;1&amp;rdquo; to the numerator and denominator as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions:</source>
          <target state="translated">此外，默认参数 &lt;code&gt;smooth_idf=True&lt;/code&gt; 将&amp;ldquo; 1&amp;rdquo;添加到分子和分母，就好像看到一个额外的文档恰好包含一次集合中的每个术语，从而防止了零除法：</target>
        </trans-unit>
        <trans-unit id="2e93583dd7fd8dcf1f0371a9818f0db1fd3c80a7" translate="yes" xml:space="preserve">
          <source>Furthermore, the formulas used to compute tf and idf depend on parameter settings that correspond to the SMART notation used in IR as follows:</source>
          <target state="translated">此外,用于计算tf和idf的公式取决于与IR中使用的SMART符号相对应的参数设置,具体如下:</target>
        </trans-unit>
        <trans-unit id="2b6ca190d547b1e777d8fa3e93274ce6ad7c42b4" translate="yes" xml:space="preserve">
          <source>G. Brier, &lt;a href=&quot;ftp://ftp.library.noaa.gov/docs.lib/htdocs/rescue/mwr/078/mwr-078-01-0001.pdf&quot;&gt;Verification of forecasts expressed in terms of probability&lt;/a&gt;, Monthly weather review 78.1 (1950)</source>
          <target state="translated">G.布莱尔，&lt;a href=&quot;ftp://ftp.library.noaa.gov/docs.lib/htdocs/rescue/mwr/078/mwr-078-01-0001.pdf&quot;&gt;用概率表示的预报的确认&lt;/a&gt;，每月天气回顾78.1（1950）</target>
        </trans-unit>
        <trans-unit id="8ccf25498da17f5ff69133909511a6d98d2976f3" translate="yes" xml:space="preserve">
          <source>G. Celeux, M. El Anbari, J.-M. Marin, C. P. Robert, &amp;ldquo;Regularization in regression: comparing Bayesian and frequentist methods in a poorly informative situation&amp;rdquo;, 2009.</source>
          <target state="translated">G.Celeux，M.El Anbari，J.-M。Marin，CP罗伯特，&amp;ldquo;回归中的正则化：在信息匮乏的情况下比较贝叶斯方法和频繁主义者的方法&amp;rdquo;，2009年。</target>
        </trans-unit>
        <trans-unit id="755f0c9208b383f3b380dd0d2b1a156d6d5865c4" translate="yes" xml:space="preserve">
          <source>G. James, D. Witten, T. Hastie, R Tibshirani, &lt;a href=&quot;http://www-bcf.usc.edu/~gareth/ISL&quot;&gt;An Introduction to Statistical Learning&lt;/a&gt;, Springer 2013.</source>
          <target state="translated">G. James，D. Witten，T. Hastie，R Tibshirani，&lt;a href=&quot;http://www-bcf.usc.edu/~gareth/ISL&quot;&gt;《统计学习入门》&lt;/a&gt;，Springer 2013。</target>
        </trans-unit>
        <trans-unit id="28ef1689ee2219c624cfde5d7c88afdcae0138ec" translate="yes" xml:space="preserve">
          <source>G. Louppe and P. Geurts, &amp;ldquo;Ensembles on Random Patches&amp;rdquo;, Machine Learning and Knowledge Discovery in Databases, 346-361, 2012.</source>
          <target state="translated">G. Louppe和P. Geurts，&amp;ldquo;随机补丁集合&amp;rdquo;，数据库中的机器学习和知识发现，346-361，2012年。</target>
        </trans-unit>
        <trans-unit id="c722e87d5d9d7dbc54dd2b811a759cc621efb047" translate="yes" xml:space="preserve">
          <source>G. Louppe, &amp;ldquo;Understanding Random Forests: From Theory to Practice&amp;rdquo;, PhD Thesis, U. of Liege, 2014.</source>
          <target state="translated">G. Louppe，&amp;ldquo;理解随机森林：从理论到实践&amp;rdquo;，博士论文，列日大学，2014年。</target>
        </trans-unit>
        <trans-unit id="a8ed6bad205ec1f52f0b48e7f8377435663ec074" translate="yes" xml:space="preserve">
          <source>G.E.P. Box and D.R. Cox, &amp;ldquo;An Analysis of Transformations&amp;rdquo;, Journal of the Royal Statistical Society B, 26, 211-252 (1964).</source>
          <target state="translated">GEP Box和DR Cox，&amp;ldquo;转型分析&amp;rdquo;，皇家统计学会杂志B，第26期，第211-252页（1964年）。</target>
        </trans-unit>
        <trans-unit id="b8cb867b444fe174ab482df0a111ed147a9ceddf" translate="yes" xml:space="preserve">
          <source>GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage &lt;code&gt;n_classes_&lt;/code&gt; regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.</source>
          <target state="translated">GB以渐进的阶段方式建立加性模型；它允许优化任意微分损失函数。在每个阶段，将 &lt;code&gt;n_classes_&lt;/code&gt; 回归树拟合到二项式或多项式偏差损失函数的负梯度上。二进制分类是一种特殊情况，其中仅诱导单个回归树。</target>
        </trans-unit>
        <trans-unit id="80f39c4fc4a6461ea00d5d7be636d9c6f77055de" translate="yes" xml:space="preserve">
          <source>GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.</source>
          <target state="translated">GB以前向阶段性的方式建立一个加法模型;它允许对任意可微分损失函数进行优化。在每个阶段,都会在给定损失函数的负梯度上拟合一棵回归树。</target>
        </trans-unit>
        <trans-unit id="2c1af0078ebec6d87c6fe14b52a6ca7ecb93e0e6" translate="yes" xml:space="preserve">
          <source>GBRT considers additive models of the following form:</source>
          <target state="translated">GBRT考虑以下形式的加法模型。</target>
        </trans-unit>
        <trans-unit id="348ddf733ebe39c89fe60cc4aea0def489f0df0c" translate="yes" xml:space="preserve">
          <source>GMM covariances</source>
          <target state="translated">GMM协方差</target>
        </trans-unit>
        <trans-unit id="89a541e422be32f4e38c95b70a35778f6b3b29a5" translate="yes" xml:space="preserve">
          <source>G[i,j] gives the shortest distance from point i to point j along the graph.</source>
          <target state="translated">G[i,j]给出了从点i到点j沿图的最短距离。</target>
        </trans-unit>
        <trans-unit id="dc7da4ca9757d9015c0ba1d2228560006792966e" translate="yes" xml:space="preserve">
          <source>Gallery generated by Sphinx-Gallery</source>
          <target state="translated">画廊由斯芬克斯画廊生成</target>
        </trans-unit>
        <trans-unit id="24f0f86d8b8da4a3eb66c5315b49fb7db14a0fa6" translate="yes" xml:space="preserve">
          <source>Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels. Interpretation of the default value is left to the kernel; see the documentation for sklearn.metrics.pairwise. Ignored by other kernels.</source>
          <target state="translated">RBF、laplacian、多项式、指数chi2和sigmoid内核的Gamma参数。默认值的解释由内核决定,参见sklearn.metrics.pairwise的文档。被其他内核忽略。</target>
        </trans-unit>
        <trans-unit id="8abb933fe9bd6d8a92eb104bdc2fd613c351d44f" translate="yes" xml:space="preserve">
          <source>Gamma parameter in rbf, poly and sigmoid kernels. Ignored by other kernels. 0.1 by default.</source>
          <target state="translated">rbf、poly和sigmoid核的gamma参数。其他内核忽略。默认为0.1。</target>
        </trans-unit>
        <trans-unit id="86050f4573c138fca290821e4b579d6d320e40d1" translate="yes" xml:space="preserve">
          <source>Gates, G.W. (1972) &amp;ldquo;The Reduced Nearest Neighbor Rule&amp;rdquo;. IEEE Transactions on Information Theory, May 1972, 431-433.</source>
          <target state="translated">盖茨（Gates）（1972）&amp;ldquo;减少的最近邻居规则&amp;rdquo;。IEEE Transactions on Information Theory，1972年5月，431-433。</target>
        </trans-unit>
        <trans-unit id="46a57bcdd34ea523f3417e94b431a41097b638e9" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Ellipsoids</source>
          <target state="translated">高斯混合物模型 椭圆体</target>
        </trans-unit>
        <trans-unit id="2f22bd1dad8340bd3d8973db40a56083b791482c" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Selection</source>
          <target state="translated">高斯混合物模型选择</target>
        </trans-unit>
        <trans-unit id="7ada59d703243073c5122ce20c200108df4cf582" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture Model Sine Curve</source>
          <target state="translated">高斯混合模型 正弦曲线</target>
        </trans-unit>
        <trans-unit id="662a25df4ddd527b4e6e6b4415fd19857fcb55fc" translate="yes" xml:space="preserve">
          <source>Gaussian Mixture.</source>
          <target state="translated">高斯混合体。</target>
        </trans-unit>
        <trans-unit id="52d32c3ce740bd6bf6fa9b8c9a00c471e2b8ab61" translate="yes" xml:space="preserve">
          <source>Gaussian Naive Bayes (GaussianNB)</source>
          <target state="translated">高斯奈夫贝叶斯(GaussianNB)</target>
        </trans-unit>
        <trans-unit id="3e71cc209c706f89187660af28df9dbd656b7dfb" translate="yes" xml:space="preserve">
          <source>Gaussian Processes regression: basic introductory example</source>
          <target state="translated">高斯过程回归:基本的介绍性例子。</target>
        </trans-unit>
        <trans-unit id="7c9060d2e2a8ab44211d4b8690374c1230f1b7f2" translate="yes" xml:space="preserve">
          <source>Gaussian kernel (&lt;code&gt;kernel = 'gaussian'&lt;/code&gt;)</source>
          <target state="translated">高斯核（ &lt;code&gt;kernel = 'gaussian'&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="16bd9bbb5a5342036acd14278f2e03ad41c57f6a" translate="yes" xml:space="preserve">
          <source>Gaussian mixture model fit with a variational inference.</source>
          <target state="translated">高斯混合物模型拟合与变异推理。</target>
        </trans-unit>
        <trans-unit id="c4278ff51902cddfc2c28028add69085822b616d" translate="yes" xml:space="preserve">
          <source>Gaussian mixture models, useful for clustering, are described in &lt;a href=&quot;mixture#mixture&quot;&gt;another chapter of the documentation&lt;/a&gt; dedicated to mixture models. KMeans can be seen as a special case of Gaussian mixture model with equal covariance per component.</source>
          <target state="translated">高斯混合模型可用于聚类，在专用于混合模型&lt;a href=&quot;mixture#mixture&quot;&gt;的文档的另一章中进行了&lt;/a&gt;描述。KMeans可以看作是高斯混合模型的特例，每个分量具有相同的协方差。</target>
        </trans-unit>
        <trans-unit id="52102b8851b98924c7d8b1f347902fc1a6a2f6c4" translate="yes" xml:space="preserve">
          <source>Gaussian mixtures</source>
          <target state="translated">高斯混合物</target>
        </trans-unit>
        <trans-unit id="fb2ed046d4b5b73ab490df316744dbd7803b27c6" translate="yes" xml:space="preserve">
          <source>Gaussian process classification (GPC) based on Laplace approximation.</source>
          <target state="translated">基于拉普拉斯近似的高斯过程分类(GPC)。</target>
        </trans-unit>
        <trans-unit id="6022eb0f0e245ca9c1dcd7d4b4311ff01e4db354" translate="yes" xml:space="preserve">
          <source>Gaussian process classification (GPC) on iris dataset</source>
          <target state="translated">虹膜数据集上的高斯过程分类(GPC)</target>
        </trans-unit>
        <trans-unit id="21a63bbdb2d774ad21ffa6c87b635dc00ddbdcbd" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR) on Mauna Loa CO2 data.</source>
          <target state="translated">Mauna Loa二氧化碳数据的高斯过程回归(GPR);</target>
        </trans-unit>
        <trans-unit id="0c7b8e025d47923893c509b893c584646dec60f9" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR) with noise-level estimation</source>
          <target state="translated">高斯过程回归(GPR)与噪声水平估计。</target>
        </trans-unit>
        <trans-unit id="e020234a1ce464bccd79fd7ca6cd9571320c3263" translate="yes" xml:space="preserve">
          <source>Gaussian process regression (GPR).</source>
          <target state="translated">高斯过程回归(GPR)。</target>
        </trans-unit>
        <trans-unit id="3eef2758f8f04922436ba69e73f365c3b677d080" translate="yes" xml:space="preserve">
          <source>GaussianNaiveBayes tends to push probabilities to 0 or 1 (note the counts in the histograms). This is mainly because it makes the assumption that features are conditionally independent given the class, which is not the case in this dataset which contains 2 redundant features.</source>
          <target state="translated">GaussianNaiveBayes倾向于将概率推到0或1(注意直方图中的计数)。这主要是因为它做出了一个假设,即特征在给定类的情况下是有条件独立的,而在这个包含2个冗余特征的数据集中,情况并非如此。</target>
        </trans-unit>
        <trans-unit id="9ee50bfb8852bcfbfa07c7c7a246c842043563a2" translate="yes" xml:space="preserve">
          <source>General KDD structure :</source>
          <target state="translated">KDD的总体结构</target>
        </trans-unit>
        <trans-unit id="340183f53d5a585fe2f90b1573169f80622dc9bd" translate="yes" xml:space="preserve">
          <source>General-purpose, even cluster size, flat geometry, not too many clusters</source>
          <target state="translated">通用型,集群大小均匀,几何形状平坦,集群数量不多。</target>
        </trans-unit>
        <trans-unit id="a807e718c7c2444084ecd599b5293f02618f18b0" translate="yes" xml:space="preserve">
          <source>Generally speaking, when model complexity increases, predictive power and latency are supposed to increase. Increasing predictive power is usually interesting, but for many applications we would better not increase prediction latency too much. We will now review this idea for different families of supervised models.</source>
          <target state="translated">一般来说,当模型复杂度增加时,预测能力和延迟都应该增加。增加预测能力通常是有趣的,但对于很多应用来说,我们最好不要过多地增加预测延迟。现在我们将针对不同系列的监督模型来回顾一下这个想法。</target>
        </trans-unit>
        <trans-unit id="af9887d0c879889fc0d4b97d28831fef1da0e335" translate="yes" xml:space="preserve">
          <source>Generate a distance matrix chunk by chunk with optional reduction</source>
          <target state="translated">逐块生成一个距离矩阵,并可选择还原。</target>
        </trans-unit>
        <trans-unit id="06c2a79c89c40ddc99e314455bfeabb348baaefc" translate="yes" xml:space="preserve">
          <source>Generate a mostly low rank matrix with bell-shaped singular values</source>
          <target state="translated">生成一个具有钟形奇异值的大部分低秩矩阵。</target>
        </trans-unit>
        <trans-unit id="c1825817fcf44112a4d64fe6f2acf131fceae396" translate="yes" xml:space="preserve">
          <source>Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].</source>
          <target state="translated">生成一个新的特征矩阵,由所有度数小于或等于指定度数的特征的多项式组合组成。例如,如果一个输入样本是二维的,形式是[a,b],那么度数2的多项式特征是[1,a,b,a^2,ab,b^2]。</target>
        </trans-unit>
        <trans-unit id="138afdc51f7a90d9b74b5dc5c84735ab7ad5ab97" translate="yes" xml:space="preserve">
          <source>Generate a random multilabel classification problem.</source>
          <target state="translated">生成一个随机多标签分类问题。</target>
        </trans-unit>
        <trans-unit id="6e53d56707f7eb93fc64a285e9e5b0c1571546a7" translate="yes" xml:space="preserve">
          <source>Generate a random n-class classification problem.</source>
          <target state="translated">生成一个随机的n类分类问题。</target>
        </trans-unit>
        <trans-unit id="45b70aa4bfe7b5254dd4845949fd163391dae828" translate="yes" xml:space="preserve">
          <source>Generate a random regression problem with sparse uncorrelated design</source>
          <target state="translated">生成一个具有稀疏非相关设计的随机回归问题。</target>
        </trans-unit>
        <trans-unit id="097811da2f026de1c67525043ab17d6d057450a6" translate="yes" xml:space="preserve">
          <source>Generate a random regression problem.</source>
          <target state="translated">生成一个随机回归问题。</target>
        </trans-unit>
        <trans-unit id="90aba5bbbbad8863550c06ced91ee520b1c0caff" translate="yes" xml:space="preserve">
          <source>Generate a random symmetric, positive-definite matrix.</source>
          <target state="translated">生成一个随机对称的正定矩阵。</target>
        </trans-unit>
        <trans-unit id="b303920886f4c442ac72ea67b8bd3cb1b7460430" translate="yes" xml:space="preserve">
          <source>Generate a signal as a sparse combination of dictionary elements.</source>
          <target state="translated">产生一个信号,作为字典元素的稀疏组合。</target>
        </trans-unit>
        <trans-unit id="035b22a208f9d34d7467f70a3f8e5a4c27edb9b2" translate="yes" xml:space="preserve">
          <source>Generate a sparse random projection matrix</source>
          <target state="translated">生成一个稀疏的随机投影矩阵。</target>
        </trans-unit>
        <trans-unit id="4dc557ac054fd2b6925cea078345560226a5469c" translate="yes" xml:space="preserve">
          <source>Generate a sparse symmetric definite positive matrix.</source>
          <target state="translated">生成一个稀疏的对称定正矩阵。</target>
        </trans-unit>
        <trans-unit id="2b6ed08a20bd86f602cf70906530ae751a13aa6a" translate="yes" xml:space="preserve">
          <source>Generate a swiss roll dataset.</source>
          <target state="translated">生成一个瑞士卷数据集。</target>
        </trans-unit>
        <trans-unit id="2f7e815b3b193bc1cd3e7e4a28307316625909c7" translate="yes" xml:space="preserve">
          <source>Generate an S curve dataset.</source>
          <target state="translated">生成一个S曲线数据集。</target>
        </trans-unit>
        <trans-unit id="a97cf86ca659bda28267893fc11990f8622b62e7" translate="yes" xml:space="preserve">
          <source>Generate an array with block checkerboard structure for biclustering.</source>
          <target state="translated">生成一个具有块状棋盘结构的数组进行双聚类。</target>
        </trans-unit>
        <trans-unit id="a9f13a8783d09446e6122b3e3234e1d6fcb95591" translate="yes" xml:space="preserve">
          <source>Generate an array with constant block diagonal structure for biclustering.</source>
          <target state="translated">生成一个具有恒定块对角线结构的数组进行双聚类。</target>
        </trans-unit>
        <trans-unit id="afeaee3f091598162e7eb33b08779a77e0e748f4" translate="yes" xml:space="preserve">
          <source>Generate cross-validated estimates for each input data point</source>
          <target state="translated">为每个输入数据点生成交叉验证的估计值</target>
        </trans-unit>
        <trans-unit id="99b9ba538a40d50737f63d924a3c7ce27d75993f" translate="yes" xml:space="preserve">
          <source>Generate datasets. We choose the size big enough to see the scalability of the algorithms, but not too big to avoid too long running times</source>
          <target state="translated">生成数据集。我们选择足够大的尺寸,以查看算法的可扩展性,但不要太大,以避免运行时间过长。</target>
        </trans-unit>
        <trans-unit id="c00dd920cc2725de42546dcb337634c4ac897029" translate="yes" xml:space="preserve">
          <source>Generate indices to split data into training and test set.</source>
          <target state="translated">生成指数,将数据分割成训练集和测试集。</target>
        </trans-unit>
        <trans-unit id="5107cc8a6ff57cac684ccce1f62420eaa4260507" translate="yes" xml:space="preserve">
          <source>Generate isotropic Gaussian and label samples by quantile</source>
          <target state="translated">生成各向同性高斯,并通过分位数标记样本</target>
        </trans-unit>
        <trans-unit id="8e89de3bc63d92fa78eda36337c27db80aab71fe" translate="yes" xml:space="preserve">
          <source>Generate isotropic Gaussian blobs for clustering.</source>
          <target state="translated">生成各向同性高斯布点进行聚类。</target>
        </trans-unit>
        <trans-unit id="37d03dbfefb10390fe483e5ed2d7b03c5a459fa1" translate="yes" xml:space="preserve">
          <source>Generate missing values indicator for X.</source>
          <target state="translated">生成X的缺失值指标。</target>
        </trans-unit>
        <trans-unit id="462cab2784077aa54955d18bb40a9de12e6edf3c" translate="yes" xml:space="preserve">
          <source>Generate polynomial and interaction features.</source>
          <target state="translated">生成多项式和交互特征。</target>
        </trans-unit>
        <trans-unit id="d1bba874447d3710a4261bda204e3775c6148149" translate="yes" xml:space="preserve">
          <source>Generate random samples from the fitted Gaussian distribution.</source>
          <target state="translated">从拟合的高斯分布中产生随机样本。</target>
        </trans-unit>
        <trans-unit id="ce67c2d91c83a1d56ab9a9ee35d822063af6506a" translate="yes" xml:space="preserve">
          <source>Generate random samples from the model.</source>
          <target state="translated">从模型中产生随机样本。</target>
        </trans-unit>
        <trans-unit id="f4defed702b6f02ff908f1cd9f8b411c35ee40dd" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #1&amp;rdquo; regression problem</source>
          <target state="translated">生成&amp;ldquo; Friedman＃1&amp;rdquo;回归问题</target>
        </trans-unit>
        <trans-unit id="75088d435099809ee2a5f0ec830b6e2b26fb0500" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #2&amp;rdquo; regression problem</source>
          <target state="translated">生成&amp;ldquo; Friedman＃2&amp;rdquo;回归问题</target>
        </trans-unit>
        <trans-unit id="18ca02f4b303dec3c31289cd6db22246b19d8adb" translate="yes" xml:space="preserve">
          <source>Generate the &amp;ldquo;Friedman #3&amp;rdquo; regression problem</source>
          <target state="translated">生成&amp;ldquo; Friedman＃3&amp;rdquo;回归问题</target>
        </trans-unit>
        <trans-unit id="1526c84b2e9b495f9ed3216009ebf8b31d461518" translate="yes" xml:space="preserve">
          <source>Generates data for binary classification used in Hastie et al.</source>
          <target state="translated">生成用于Hastie等人的二元分类的数据。</target>
        </trans-unit>
        <trans-unit id="c2cb269fed6a06711794c0a014b9a89e92300ddb" translate="yes" xml:space="preserve">
          <source>Generates data for binary classification used in Hastie et al. 2009, Example 10.2.</source>
          <target state="translated">生成Hastie等人2009年使用的二元分类数据,例10.2。</target>
        </trans-unit>
        <trans-unit id="fbfd61fc35f16aea2f376426724b313bf45b644a" translate="yes" xml:space="preserve">
          <source>Generates indices to split data into training and test set.</source>
          <target state="translated">生成指数,将数据分割成训练集和测试集。</target>
        </trans-unit>
        <trans-unit id="9a963ad633fdf36ff4f1d429308e1f3d90a2ceea" translate="yes" xml:space="preserve">
          <source>Generates train/test indices based on predefined splits.</source>
          <target state="translated">根据预定义的分割生成训练/测试指数。</target>
        </trans-unit>
        <trans-unit id="4678269441c5cad2dec162c29e80b19e70944794" translate="yes" xml:space="preserve">
          <source>Generates train/test indices based on random permutation.</source>
          <target state="translated">基于随机排列生成训练/测试指数。</target>
        </trans-unit>
        <trans-unit id="025efadf6f18cb5d61732c8188dd311431f2fe8b" translate="yes" xml:space="preserve">
          <source>Generator on parameters sampled from given distributions.</source>
          <target state="translated">从给定分布中取样的参数生成器。</target>
        </trans-unit>
        <trans-unit id="9008c79b50b6e856f48dd8a1acb75bd481c83565" translate="yes" xml:space="preserve">
          <source>Generator to create n_packs slices going up to n.</source>
          <target state="translated">生成n_packs切片,最多可生成n个。</target>
        </trans-unit>
        <trans-unit id="6a34af9aa1c17133e53bdde13fa952c7bcbcf3f6" translate="yes" xml:space="preserve">
          <source>Geometry (metric used)</source>
          <target state="translated">几何学(公制)</target>
        </trans-unit>
        <trans-unit id="e5f048789e3e59e8993091df470af502112331aa" translate="yes" xml:space="preserve">
          <source>George W Bush</source>
          <target state="translated">乔治-布什</target>
        </trans-unit>
        <trans-unit id="b583db923d23716d80d92ca8bb6a609aa1f738a2" translate="yes" xml:space="preserve">
          <source>Gerhard Schroeder</source>
          <target state="translated">格哈德-施罗德</target>
        </trans-unit>
        <trans-unit id="33868dad5f60b783d41cfb7c4e686fd5af82ea02" translate="yes" xml:space="preserve">
          <source>Get a list of all estimators from sklearn.</source>
          <target state="translated">从sklearn获取所有估算师的名单。</target>
        </trans-unit>
        <trans-unit id="c89b4f911ae16fa0b7caa09ce0c140306df6a7bd" translate="yes" xml:space="preserve">
          <source>Get a mask, or integer index, of the features selected</source>
          <target state="translated">获取所选特征的掩码或整数索引。</target>
        </trans-unit>
        <trans-unit id="72908cf84377de645c7534a22afeddeeaba91d9d" translate="yes" xml:space="preserve">
          <source>Get a scorer from string</source>
          <target state="translated">从字符串中获取一个得分器</target>
        </trans-unit>
        <trans-unit id="45a250b2600ca82b0e59f392e6c981ee3cc2728d" translate="yes" xml:space="preserve">
          <source>Get feature names from all transformers.</source>
          <target state="translated">从所有变压器中获取特征名称。</target>
        </trans-unit>
        <trans-unit id="4be0c520942fc8926cfd53e42cd4ae1d1cc70df9" translate="yes" xml:space="preserve">
          <source>Get parameters for this estimator.</source>
          <target state="translated">获取该估计器的参数。</target>
        </trans-unit>
        <trans-unit id="fe15f50ace10fe1b8c70139542f4a1796682abb3" translate="yes" xml:space="preserve">
          <source>Get parameters of this kernel.</source>
          <target state="translated">获取该内核的参数。</target>
        </trans-unit>
        <trans-unit id="1314abe875bac1db97b1a7155d7b4a8c13c230ee" translate="yes" xml:space="preserve">
          <source>Get predictions from each split of cross-validation for diagnostic purposes.</source>
          <target state="translated">从交叉验证的每个分割中获得预测,以达到诊断的目的。</target>
        </trans-unit>
        <trans-unit id="dd0a065fc935a1fd709e1a1d7d55ca6c3433dca5" translate="yes" xml:space="preserve">
          <source>Get the given distance metric from the string identifier.</source>
          <target state="translated">从字符串标识符中获取给定的距离度量。</target>
        </trans-unit>
        <trans-unit id="df2089c702273c8bc78b6842775813fe9702ad55" translate="yes" xml:space="preserve">
          <source>Get the parameters of the VotingClassifier</source>
          <target state="translated">获取VotingClassifier的参数。</target>
        </trans-unit>
        <trans-unit id="1f6030226293d5ed7b4d4b045e215d6de20db61c" translate="yes" xml:space="preserve">
          <source>Getter for the precision matrix.</source>
          <target state="translated">精密矩阵的获取器。</target>
        </trans-unit>
        <trans-unit id="53379a8bafa1cbd8bc5da14050f14d3817f01039" translate="yes" xml:space="preserve">
          <source>Given 2 multivariate covarying two-dimensional datasets, X, and Y, PLS extracts the &amp;lsquo;directions of covariance&amp;rsquo;, i.e. the components of each datasets that explain the most shared variance between both datasets. This is apparent on the &lt;strong&gt;scatterplot matrix&lt;/strong&gt; display: components 1 in dataset X and dataset Y are maximally correlated (points lie around the first diagonal). This is also true for components 2 in both dataset, however, the correlation across datasets for different components is weak: the point cloud is very spherical.</source>
          <target state="translated">给定2个多元协变二维数据集X和Y，PLS提取&amp;ldquo;协方差方向&amp;rdquo;，即解释两个数据集之间共享程度最大的每个数据集的组成部分。这在&lt;strong&gt;散点图矩阵&lt;/strong&gt;显示中很明显：数据集X和数据集Y中的分量1最大相关（点位于第一个对角线上）。这两个数据集中的分量2也是如此，但是，数据集中不同分量之间的相关性很弱：点云非常球形。</target>
        </trans-unit>
        <trans-unit id="16179644ab5a4c2a1f730ff634ab3d4d3a869791" translate="yes" xml:space="preserve">
          <source>Given a candidate centroid \(x_i\) for iteration \(t\), the candidate is updated according to the following equation:</source>
          <target state="translated">给定一个迭代的候选中心点(x_i/),候选中心点按以下公式更新。</target>
        </trans-unit>
        <trans-unit id="4368fa47ed8eb35b757e7b3d5aaf6d7ee1cd4ff6" translate="yes" xml:space="preserve">
          <source>Given a dataset with two features, we let the encoder find the unique values per feature and transform the data to a binary one-hot encoding.</source>
          <target state="translated">给定一个具有两个特征的数据集,我们让编码器找到每个特征的唯一值,并将数据转化为二进制一热编码。</target>
        </trans-unit>
        <trans-unit id="a65060cb3a96ad97e8800308b9076a9a49180060" translate="yes" xml:space="preserve">
          <source>Given a dataset with two features, we let the encoder find the unique values per feature and transform the data to an ordinal encoding.</source>
          <target state="translated">给定一个具有两个特征的数据集,我们让编码器找到每个特征的唯一值,并将数据转换为序数编码。</target>
        </trans-unit>
        <trans-unit id="6cc0cdb4252ae3fe585bd759a612161dfe7c6d85" translate="yes" xml:space="preserve">
          <source>Given a set of training examples \((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\) where \(x_i \in \mathbf{R}^n\) and \(y_i \in \{0, 1\}\), a one hidden layer one hidden neuron MLP learns the function \(f(x) = W_2 g(W_1^T x + b_1) + b_2\) where \(W_1 \in \mathbf{R}^m\) and \(W_2, b_1, b_2 \in \mathbf{R}\) are model parameters. \(W_1, W_2\) represent the weights of the input layer and hidden layer, respectively; and \(b_1, b_2\) represent the bias added to the hidden layer and the output layer, respectively. \(g(\cdot) : R \rightarrow R\) is the activation function, set by default as the hyperbolic tan. It is given as,</source>
          <target state="translated">给出一组训练例子((x_1,y_1),(x_2,y_2),(x_n,y_n))其中(x_i在mathbf{R}^n)和(y_i在{0,1}/))。一个一隐藏层一隐藏神经元MLP学习的函数/(f(x)=W_2 g(W_1^T x+b_1)+b_2/),其中/(W_1 in \mathbf{R}^m)和/(W_2,b_1,b_2 \in \mathbf{R}/)是模型参数。\(W_1,W_2\)分别代表输入层和隐藏层的权重;\(b_1,b_2\)分别代表加在隐藏层和输出层的偏置。\(g(\cdot):R \rightarrow R\)为激活函数,默认设置为双曲tan。它的给定值为:。</target>
        </trans-unit>
        <trans-unit id="99b85508f1069fad6e9945b3624fea4140b5fbae" translate="yes" xml:space="preserve">
          <source>Given a set of training examples \((x_1, y_1), \ldots, (x_n, y_n)\) where \(x_i \in \mathbf{R}^m\) and \(y_i \in \{-1,1\}\), our goal is to learn a linear scoring function \(f(x) = w^T x + b\) with model parameters \(w \in \mathbf{R}^m\) and intercept \(b \in \mathbf{R}\). In order to make predictions, we simply look at the sign of \(f(x)\). A common choice to find the model parameters is by minimizing the regularized training error given by</source>
          <target state="translated">给出一组训练示例(((x_1,y_1),ldots,(x_n,y_n)\)其中 \(x_i \mathbf{R}^m)和 \(y_i \mathbf{R}^m)。我们的目标是学习一个线性评分函数(f(x)=w^T x+b\),其模型参数为(w \in \mathbf{R}^m\)和截距(b \in \mathbf{R})。为了进行预测,我们只需看一下\(f(x)\)的符号。寻找模型参数的一个常见选择是通过最小化正则化训练误差来寻找模型参数,给定的模型参数如下所示。</target>
        </trans-unit>
        <trans-unit id="f05ffd1dc56829aeb2ce3b1aa47183d5a5a71272" translate="yes" xml:space="preserve">
          <source>Given an exception, a callable to raise the exception, and a message string, tests that the correct exception is raised and that the message is a substring of the error thrown. Used to test that the specific message thrown during an exception is correct.</source>
          <target state="translated">给定一个异常、一个引发异常的可调用对象和一个消息字符串,测试是否引发了正确的异常,并且消息是抛出的错误的子串。用于测试在异常期间抛出的特定消息是否正确。</target>
        </trans-unit>
        <trans-unit id="d5588778e54082615cf481fafbc7dcf0b337d76d" translate="yes" xml:space="preserve">
          <source>Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (&lt;a href=&quot;generated/sklearn.feature_selection.rfe#sklearn.feature_selection.RFE&quot;&gt;&lt;code&gt;RFE&lt;/code&gt;&lt;/a&gt;) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a &lt;code&gt;coef_&lt;/code&gt; attribute or through a &lt;code&gt;feature_importances_&lt;/code&gt; attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.</source>
          <target state="translated">给定将权重分配给特征（例如线性模型的系数）的外部估计器，递归特征消除（&lt;a href=&quot;generated/sklearn.feature_selection.rfe#sklearn.feature_selection.RFE&quot;&gt; &lt;code&gt;RFE&lt;/code&gt; &lt;/a&gt;）将通过递归考虑越来越少的特征集来选择特征。首先，对估计器进行初始特征集训练，并通过 &lt;code&gt;coef_&lt;/code&gt; 属性或 &lt;code&gt;feature_importances_&lt;/code&gt; 属性获得每个特征的重要性。然后，从当前的一组特征中删除最不重要的特征，然后对该过程进行递归重复，直到最终达到所需的特征数量。</target>
        </trans-unit>
        <trans-unit id="0a3e62329db7e0582a525546102e4bc5a3e414ee" translate="yes" xml:space="preserve">
          <source>Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a &lt;code&gt;coef_&lt;/code&gt; attribute or through a &lt;code&gt;feature_importances_&lt;/code&gt; attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.</source>
          <target state="translated">给定一个将权重分配给特征（例如线性模型的系数）的外部估计器，递归特征消除（RFE）的目标是通过递归考虑越来越少的特征集来选择特征。首先，对估计器进行初始特征集训练，并通过 &lt;code&gt;coef_&lt;/code&gt; 属性或 &lt;code&gt;feature_importances_&lt;/code&gt; 属性获得每个特征的重要性。然后，从当前功能集中删除最不重要的功能。在修剪的集上递归地重复该过程，直到最终达到所需的要选择的特征数量。</target>
        </trans-unit>
        <trans-unit id="2e4a90e9413cabdb8d0d79c137af8efe3fbd16ef" translate="yes" xml:space="preserve">
          <source>Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the &lt;code&gt;init='k-means++'&lt;/code&gt; parameter). This initializes the centroids to be (generally) distant from each other, leading to provably better results than random initialization, as shown in the reference.</source>
          <target state="translated">给定足够的时间，K均值将始终收敛，但这可能是局部最小值。这在很大程度上取决于质心的初始化。结果，该计算通常使用质心的不同初始化进行多次。解决此问题的一种方法是k-means ++初始化方案，该方案已在scikit-learn中实现（使用 &lt;code&gt;init='k-means++'&lt;/code&gt; 参数）。如参考文献中所示，这将质心初始化为（通常）彼此远离，从而导致可证明比随机初始化更好的结果。</target>
        </trans-unit>
        <trans-unit id="74d4aecb20e2cdcd5c8865136aad914eecac7d61" translate="yes" xml:space="preserve">
          <source>Given the iris dataset, if we knew that there were 3 types of iris, but did not have access to a taxonomist to label them: we could try a &lt;strong&gt;clustering task&lt;/strong&gt;: split the observations into well-separated group called &lt;em&gt;clusters&lt;/em&gt;.</source>
          <target state="translated">给定虹膜数据集，如果我们知道虹膜有3种类型，但没有分类学家来标记它们的话：我们可以尝试&lt;strong&gt;聚类任务&lt;/strong&gt;：将观察结果分成分离良好的组，称为&lt;em&gt;clusters&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="7ffdaa4cdda4b54b62086a7f5ac68bd7ea3b5908" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments &lt;code&gt;labels_true&lt;/code&gt; and our clustering algorithm assignments of the same samples &lt;code&gt;labels_pred&lt;/code&gt;, the &lt;strong&gt;Mutual Information&lt;/strong&gt; is a function that measures the &lt;strong&gt;agreement&lt;/strong&gt; of the two assignments, ignoring permutations. Two different normalized versions of this measure are available, &lt;strong&gt;Normalized Mutual Information (NMI)&lt;/strong&gt; and &lt;strong&gt;Adjusted Mutual Information (AMI)&lt;/strong&gt;. NMI is often used in the literature, while AMI was proposed more recently and is &lt;strong&gt;normalized against chance&lt;/strong&gt;:</source>
          <target state="translated">有了基本事实类分配 &lt;code&gt;labels_true&lt;/code&gt; 和我们的相同样本 &lt;code&gt;labels_pred&lt;/code&gt; 的聚类算法分配的知识，&lt;strong&gt;互信息&lt;/strong&gt;就是一个函数，它测量两个分配的&lt;strong&gt;一致性&lt;/strong&gt;，而忽略排列。可以使用此度量的两个不同的规范化版本：&lt;strong&gt;规范化互信息（NMI）&lt;/strong&gt;和&lt;strong&gt;调整后的互信息（AMI）&lt;/strong&gt;。 NMI在文献中经常使用，而AMI是最近才提出的，并且&lt;strong&gt;针对偶然性进行了标准化&lt;/strong&gt;：</target>
        </trans-unit>
        <trans-unit id="943836cb04e0640667940c68f56d5deeb3e35898" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments &lt;code&gt;labels_true&lt;/code&gt; and our clustering algorithm assignments of the same samples &lt;code&gt;labels_pred&lt;/code&gt;, the &lt;strong&gt;adjusted Rand index&lt;/strong&gt; is a function that measures the &lt;strong&gt;similarity&lt;/strong&gt; of the two assignments, ignoring permutations and &lt;strong&gt;with chance normalization&lt;/strong&gt;:</source>
          <target state="translated">有了基本事实类分配 &lt;code&gt;labels_true&lt;/code&gt; 和我们的相同样本 &lt;code&gt;labels_pred&lt;/code&gt; 的聚类算法分配的知识，&lt;strong&gt;调整后的兰德指数&lt;/strong&gt;就是一个函数，它测量两个分配的&lt;strong&gt;相似性&lt;/strong&gt;，而忽略了排列并&lt;strong&gt;通过机会归一化&lt;/strong&gt;：</target>
        </trans-unit>
        <trans-unit id="3a989bbd6a98db5dab53799fee5637e2080ce141" translate="yes" xml:space="preserve">
          <source>Given the knowledge of the ground truth class assignments of the samples, it is possible to define some intuitive metric using conditional entropy analysis.</source>
          <target state="translated">鉴于对样本的地真类赋值的了解,可以用条件熵分析来定义一些直观的度量。</target>
        </trans-unit>
        <trans-unit id="4d7a7b1af5c7c7276434270fce7100038c705add" translate="yes" xml:space="preserve">
          <source>Given these singular vectors, they are ranked according to which can be best approximated by a piecewise-constant vector. The approximations for each vector are found using one-dimensional k-means and scored using the Euclidean distance. Some subset of the best left and right singular vector are selected. Next, the data is projected to this best subset of singular vectors and clustered.</source>
          <target state="translated">给定这些奇异向量,根据哪个向量能被一个计件常数向量最好地逼近进行排序。使用一维k-means找到每个向量的近似值,并使用欧氏距离进行评分。选出一些最好的左、右奇异向量的子集。接下来,将数据投射到这个最佳子集的奇异向量上,并进行聚类。</target>
        </trans-unit>
        <trans-unit id="21675a464e2ca3b8f99eef191d00e106aa21c0dd" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in R^n\), i=1,&amp;hellip;, l and a label vector \(y \in R^l\), a decision tree recursively partitions the space such that the samples with the same labels are grouped together.</source>
          <target state="translated">给定训练向量\（x_i \ in R ^ n \），i = 1，&amp;hellip;，l和标签向量\（y \ in R ^ l \），决策树递归地划分空间，使得具有相同样本的样本标签分组在一起。</target>
        </trans-unit>
        <trans-unit id="02fd4db44c84fce9026584422f7727ba079bc40a" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in \mathbb{R}^p\), i=1,&amp;hellip;, n, and a vector \(y \in \mathbb{R}^n\)\(\varepsilon\)-SVR solves the following primal problem:</source>
          <target state="translated">给定训练向量\（x_i \ in \ mathbb {R} ^ p \），i = 1，&amp;hellip;，n和向量\（y \ in \ mathbb {R} ^ p \）\（\ varepsilon \）- SVR解决了以下主要问题：</target>
        </trans-unit>
        <trans-unit id="70e397398a5003e0a6b00de067e9804bfe571e70" translate="yes" xml:space="preserve">
          <source>Given training vectors \(x_i \in \mathbb{R}^p\), i=1,&amp;hellip;, n, in two classes, and a vector \(y \in \{1, -1\}^n\), SVC solves the following primal problem:</source>
          <target state="translated">给定训练向量\（x_i \ in \ mathbb {R} ^ p \），在两个类别中i = 1，&amp;hellip;，n，以及向量\（y \ in \ {1，-1 \} ^ n \） ，SVC解决了以下主要问题：</target>
        </trans-unit>
        <trans-unit id="e44bf83eca8aa1cc0c5bdaa89da0afa702f51625" translate="yes" xml:space="preserve">
          <source>Gives the number of (complex) sampling points.</source>
          <target state="translated">给出(复杂的)采样点的数量。</target>
        </trans-unit>
        <trans-unit id="f36c7685daa8ebc7e1344aa0d6e3a7d679decebf" translate="yes" xml:space="preserve">
          <source>Global structure is not explicitly preserved. This is problem is mitigated by initializing points with PCA (using &lt;code&gt;init=&amp;rsquo;pca&amp;rsquo;&lt;/code&gt;).</source>
          <target state="translated">全局结构未明确保留。通过使用PCA初始化点（使用 &lt;code&gt;init=&amp;rsquo;pca&amp;rsquo;&lt;/code&gt; ）可以缓解此问题。</target>
        </trans-unit>
        <trans-unit id="178c27bf7200da0534de904ea7e6ca7da842dbb5" translate="yes" xml:space="preserve">
          <source>Glorot, Xavier, and Yoshua Bengio. &amp;ldquo;Understanding the difficulty of</source>
          <target state="translated">Glorot，Xavier和Yoshua Bengio。&amp;ldquo;了解困难</target>
        </trans-unit>
        <trans-unit id="7427cf697be16a4ec1d916910128a59d920125e7" translate="yes" xml:space="preserve">
          <source>Glossary</source>
          <target state="translated">Glossary</target>
        </trans-unit>
        <trans-unit id="f7c22aaad44fb28f4ee8f06d6d4f4f14ac9ce899" translate="yes" xml:space="preserve">
          <source>Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,</source>
          <target state="translated">Golub和C.Van Loan.矩阵计算,第三版,第5章。</target>
        </trans-unit>
        <trans-unit id="1de5b736be2f9def46d07ed88549feeeea5a97b0" translate="yes" xml:space="preserve">
          <source>Gorodkin, (2004). Comparing two K-category assignments by a K-category correlation coefficient</source>
          <target state="translated">Gorodkin(2004年)。通过K类相关系数比较两个K类分配。</target>
        </trans-unit>
        <trans-unit id="46268d41f41f8e1954ca3d54fd29ddb1959ea6db" translate="yes" xml:space="preserve">
          <source>Gradient Boosting Out-of-Bag estimates</source>
          <target state="translated">梯度提升袋外估计值</target>
        </trans-unit>
        <trans-unit id="3e3d95a92c5a33953c001956fd3fd6ac3b1082fa" translate="yes" xml:space="preserve">
          <source>Gradient Boosting attempts to solve this minimization problem numerically via steepest descent: The steepest descent direction is the negative gradient of the loss function evaluated at the current model \(F_{m-1}\) which can be calculated for any differentiable loss function:</source>
          <target state="translated">梯度提升法试图通过最陡下降法以数值方式解决这个最小化问题。最陡降方向是在当前模型上评估的损失函数的负梯度,可以计算出任何可微分损失函数。</target>
        </trans-unit>
        <trans-unit id="be45c92854a0f55592d6c3c1c28201cf75d59d94" translate="yes" xml:space="preserve">
          <source>Gradient Boosting for classification.</source>
          <target state="translated">分类的梯度提升。</target>
        </trans-unit>
        <trans-unit id="65fd480d2da13d80eb18643fd08c31b9e5239c9a" translate="yes" xml:space="preserve">
          <source>Gradient Boosting for regression.</source>
          <target state="translated">回归的梯度提升。</target>
        </trans-unit>
        <trans-unit id="23dcf8253cdacbdd915f0e5e69e684c3457ad1df" translate="yes" xml:space="preserve">
          <source>Gradient Boosting regression</source>
          <target state="translated">梯度提升回归</target>
        </trans-unit>
        <trans-unit id="33b1659de13c2a7e036f71b3c26eda1d552a4b1c" translate="yes" xml:space="preserve">
          <source>Gradient Boosting regularization</source>
          <target state="translated">梯度提升正则化</target>
        </trans-unit>
        <trans-unit id="cf9557c4e6e59de44aebd2e8b07221ff19e46958" translate="yes" xml:space="preserve">
          <source>Gradient boosting is an ensembling technique where several weak learners (regression trees) are combined to yield a powerful single model, in an iterative fashion.</source>
          <target state="translated">梯度提升是一种集合技术,将多个弱学习器(回归树)结合起来,以迭代的方式产生一个强大的单一模型。</target>
        </trans-unit>
        <trans-unit id="c4611f197e5e7430aa271445ae503720ad1cf3d4" translate="yes" xml:space="preserve">
          <source>Gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta. Only returned when eval_gradient is True.</source>
          <target state="translated">在theta位置上,相对于核超参数的对数边际似然的梯度。仅当eval_gradient为True时返回。</target>
        </trans-unit>
        <trans-unit id="f77edae6db0cdcd4449adeeb038c653af7406ea3" translate="yes" xml:space="preserve">
          <source>Gram Orthogonal Matching Pursuit (OMP)</source>
          <target state="translated">革兰氏正交匹配追求(OMP)</target>
        </trans-unit>
        <trans-unit id="10ef9123115df39a65f62ffa3d9d0e10899ca7cd" translate="yes" xml:space="preserve">
          <source>Gram matrix of the input data: X.T * X</source>
          <target state="translated">输入数据的Gram矩阵。X.T*X</target>
        </trans-unit>
        <trans-unit id="a83784084519ce853a92535121a74c85019c19b0" translate="yes" xml:space="preserve">
          <source>Graph distance (e.g. nearest-neighbor graph)</source>
          <target state="translated">图形距离(如最近邻图)</target>
        </trans-unit>
        <trans-unit id="8d5c9a04db77341319c1b38643f0d38066fc8710" translate="yes" xml:space="preserve">
          <source>Graph of the pixel-to-pixel connections</source>
          <target state="translated">像素与像素之间的连接图</target>
        </trans-unit>
        <trans-unit id="1b6f746d097f9fe3740f364d944363a7e3d991f9" translate="yes" xml:space="preserve">
          <source>Graph of the pixel-to-pixel gradient connections</source>
          <target state="translated">像素与像素之间的梯度连接图。</target>
        </trans-unit>
        <trans-unit id="933bf21afdd55a0d2283845fed0e7bbdd1f5db49" translate="yes" xml:space="preserve">
          <source>Green</source>
          <target state="translated">Green</target>
        </trans-unit>
        <trans-unit id="9786dcbe8afbab8ac93bdfcd6653b6cd7aa7993b" translate="yes" xml:space="preserve">
          <source>Grid of Cs used for cross-validation.</source>
          <target state="translated">用于交叉验证的Cs网格。</target>
        </trans-unit>
        <trans-unit id="5bd85812ea7e2436359885d902fd71d10cd1c2d9" translate="yes" xml:space="preserve">
          <source>Grid of parameters with a discrete number of values for each.</source>
          <target state="translated">参数网格,每个参数都有一个离散的数值。</target>
        </trans-unit>
        <trans-unit id="4a6f9190abeab5c3ccde3d9c276bc4db019e7d38" translate="yes" xml:space="preserve">
          <source>Grid search can also be performed on the different preprocessing steps defined in the &lt;code&gt;ColumnTransformer&lt;/code&gt; object, together with the classifier&amp;rsquo;s hyperparameters as part of the &lt;code&gt;Pipeline&lt;/code&gt;. We will search for both the imputer strategy of the numeric preprocessing and the regularization parameter of the logistic regression using &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">网格搜索还可以在 &lt;code&gt;ColumnTransformer&lt;/code&gt; 对象中定义的不同预处理步骤以及作为 &lt;code&gt;Pipeline&lt;/code&gt; 一部分的分类器超参数中执行。我们将使用&lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt; &lt;/a&gt;搜索数值预处理的不合理策略和逻辑回归的正则化参数。</target>
        </trans-unit>
        <trans-unit id="71a1782f5aa6d2b7cc26a083f91eef66c1cf3aff" translate="yes" xml:space="preserve">
          <source>Grid-search</source>
          <target state="translated">Grid-search</target>
        </trans-unit>
        <trans-unit id="ed926e289de9aa5047e6b09f7b537df04bde4bbf" translate="yes" xml:space="preserve">
          <source>Grid-search and cross-validated estimators</source>
          <target state="translated">网格搜索和交叉验证的估计器。</target>
        </trans-unit>
        <trans-unit id="64ba146c44fdd8e95f622a314398320f76845aed" translate="yes" xml:space="preserve">
          <source>GridSearchCV implements a &amp;ldquo;fit&amp;rdquo; and a &amp;ldquo;score&amp;rdquo; method. It also implements &amp;ldquo;predict&amp;rdquo;, &amp;ldquo;predict_proba&amp;rdquo;, &amp;ldquo;decision_function&amp;rdquo;, &amp;ldquo;transform&amp;rdquo; and &amp;ldquo;inverse_transform&amp;rdquo; if they are implemented in the estimator used.</source>
          <target state="translated">GridSearchCV实现&amp;ldquo;拟合&amp;rdquo;和&amp;ldquo;得分&amp;rdquo;方法。如果在所使用的估计器中实现了&amp;ldquo;预测&amp;rdquo;，&amp;ldquo;预测_proba&amp;rdquo;，&amp;ldquo;决策功能&amp;rdquo;，&amp;ldquo;变换&amp;rdquo;和&amp;ldquo;逆变换&amp;rdquo;，则还可以实现它们。</target>
        </trans-unit>
        <trans-unit id="2e6f2bdd92d1c5e33352841cf6b10ed864b19fa7" translate="yes" xml:space="preserve">
          <source>Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification: An Overview. International Journal of Data Warehousing &amp;amp; Mining, 3(3), 1-13, July-September 2007.</source>
          <target state="translated">Grigorios Tsoumakas，Ioannis Katakis。多标签分类：概述。国际数据仓库与采矿杂志，2007年7月至9月，第3（3）页，第1-13页。</target>
        </trans-unit>
        <trans-unit id="796325c68f51f69a2afcc84a9fb61fa1d8420435" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) labels for n_samples samples.</source>
          <target state="translated">n_samples样本的地面真实(正确)标签。</target>
        </trans-unit>
        <trans-unit id="740dd68aa13d511b42941c79135a52aa5a0f5bc4" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) labels.</source>
          <target state="translated">地实(正确)标签。</target>
        </trans-unit>
        <trans-unit id="cf154969e860842a471602bf65b740057751e47b" translate="yes" xml:space="preserve">
          <source>Ground truth (correct) target values.</source>
          <target state="translated">基本事实(正确)目标值。</target>
        </trans-unit>
        <trans-unit id="691f624e8ff75b4d50175631f407699ccfb7e35d" translate="yes" xml:space="preserve">
          <source>Ground truth class labels to be used as a reference</source>
          <target state="translated">作为参考的地面真相类标签。</target>
        </trans-unit>
        <trans-unit id="2859baca63ac3255284d20bc28f887a4c54fefb4" translate="yes" xml:space="preserve">
          <source>Group labels for the samples used while splitting the dataset into train/test set.</source>
          <target state="translated">将数据集拆分为训练/测试集时使用的样本的组标签。</target>
        </trans-unit>
        <trans-unit id="f5ee660cf40b3d432d2833cbe2c4255cb71b873d" translate="yes" xml:space="preserve">
          <source>Group labels for the samples used while splitting the dataset into train/test set. This &amp;lsquo;groups&amp;rsquo; parameter must always be specified to calculate the number of splits, though the other parameters can be omitted.</source>
          <target state="translated">将数据集拆分为训练/测试集时使用的样本的标签分组。尽管可以省略其他参数，但必须始终指定此&amp;ldquo; groups&amp;rdquo;参数来计算拆分数。</target>
        </trans-unit>
        <trans-unit id="2fe58cc1aca321453c1632eb3218b2ee2034ed27" translate="yes" xml:space="preserve">
          <source>Grow a tree with &lt;code&gt;max_leaf_nodes&lt;/code&gt; in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</source>
          <target state="translated">以最佳优先方式种植带有 &lt;code&gt;max_leaf_nodes&lt;/code&gt; 的树。最佳节点定义为杂质的相对减少。如果为None，则叶节点数不受限制。</target>
        </trans-unit>
        <trans-unit id="9f319cd9d13cdc03649579ff252c6b96c720508d" translate="yes" xml:space="preserve">
          <source>Grow trees with &lt;code&gt;max_leaf_nodes&lt;/code&gt; in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</source>
          <target state="translated">以最佳优先方式种植具有 &lt;code&gt;max_leaf_nodes&lt;/code&gt; 的树。最佳节点定义为杂质的相对减少。如果为None，则叶节点数不受限制。</target>
        </trans-unit>
        <trans-unit id="bf073fae640ded81eeb7a4cee70faff4a623c16c" translate="yes" xml:space="preserve">
          <source>Guide</source>
          <target state="translated">Guide</target>
        </trans-unit>
        <trans-unit id="1fd932db6b504d046b60a30c3273eb39ba2ac7a5" translate="yes" xml:space="preserve">
          <source>Guyon, I., Weston, J., Barnhill, S., &amp;amp; Vapnik, V., &amp;ldquo;Gene selection for cancer classification using support vector machines&amp;rdquo;, Mach. Learn., 46(1-3), 389&amp;ndash;422, 2002.</source>
          <target state="translated">马萨诸塞州，Guyon，I.，Weston，J.，Barnhill，S。，＆Vapnik，V。，&amp;ldquo;使用支持向量机进行癌症分类的基因选择&amp;rdquo;。Learn。，46（1-3），389-422，2002。</target>
        </trans-unit>
        <trans-unit id="dd4d457c816b0cb358c91f5b8813986bac26cb3d" translate="yes" xml:space="preserve">
          <source>H. Zhang (2004). &lt;a href=&quot;http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf&quot;&gt;The optimality of Naive Bayes.&lt;/a&gt; Proc. FLAIRS.</source>
          <target state="translated">张海（2004）。&lt;a href=&quot;http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf&quot;&gt;朴素贝叶斯的最优性。&lt;/a&gt;进程 天赋。</target>
        </trans-unit>
        <trans-unit id="f5b6915b0e377ea69d7b62d27d3f027cc63657d7" translate="yes" xml:space="preserve">
          <source>Hagai Attias. (2000). &amp;ldquo;A Variational Bayesian Framework for Graphical Models&amp;rdquo;. In Advances in Neural Information Processing Systems 12.</source>
          <target state="translated">Hagai Attias。（2000）。&amp;ldquo;图形模型的变分贝叶斯框架&amp;rdquo;。神经信息处理系统进展12。</target>
        </trans-unit>
        <trans-unit id="8446ed65374f4c03b547ffebe7ab69437207be78" translate="yes" xml:space="preserve">
          <source>Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). &amp;ldquo;On Clustering Validation Techniques&amp;rdquo; Journal of Intelligent Information Systems, 17(2-3), 107-145. &lt;a href=&quot;http://dx.doi.org/10.1023/A:1012801612483&quot;&gt;doi:10.1023/A:1012801612483&lt;/a&gt;.</source>
          <target state="translated">玛丽亚&amp;middot;哈尔基迪（Halkidi）扬尼斯&amp;middot;巴蒂斯塔基斯；Vazirgiannis，Michalis（2001）。&amp;ldquo;关于聚类验证技术&amp;rdquo;，《智能信息系统杂志》，第17（2-3）页，第107-145页。&lt;a href=&quot;http://dx.doi.org/10.1023/A:1012801612483&quot;&gt;doi：10.1023 / A：1012801612483&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="57fe625410e680c160d128700bfb1af1b965809e" translate="yes" xml:space="preserve">
          <source>HammingDistance</source>
          <target state="translated">HammingDistance</target>
        </trans-unit>
        <trans-unit id="dee17735ec3038cb9f5dda5413031eefdf59071a" translate="yes" xml:space="preserve">
          <source>Handle or name of the output file. If &lt;code&gt;None&lt;/code&gt;, the result is returned as a string.</source>
          <target state="translated">输出文件的句柄或名称。如果为 &lt;code&gt;None&lt;/code&gt; ，则结果以字符串形式返回。</target>
        </trans-unit>
        <trans-unit id="077bb86f8a4736a0992a0b108c1d4b8e9298e04f" translate="yes" xml:space="preserve">
          <source>Hard constraint to select the backend. If set to &amp;lsquo;sharedmem&amp;rsquo;, the selected backend will be single-host and thread-based even if the user asked for a non-thread based backend with parallel_backend.</source>
          <target state="translated">很难选择后端。如果设置为&amp;ldquo; sharedmem&amp;rdquo;，则即使用户要求带有parallel_backend的非基于线程的后端，选定的后端也将是基于单主机和基于线程的后端。</target>
        </trans-unit>
        <trans-unit id="9b9156693e970a15a3c18a9425374c7bf2903574" translate="yes" xml:space="preserve">
          <source>Hard limit on iterations within solver, or -1 for no limit.</source>
          <target state="translated">对求解器内的迭代进行硬限制,如果没有限制,则为-1。</target>
        </trans-unit>
        <trans-unit id="73dd008516fbc283773051e5943e3b658488b1b1" translate="yes" xml:space="preserve">
          <source>Harrison, D. and Rubinfeld, D.L.</source>
          <target state="translated">Harrison,D.和Rubinfeld,D.L.</target>
        </trans-unit>
        <trans-unit id="c23f4e8aad7e2235e0ebdbc3c9d2bf9b602d6e3d" translate="yes" xml:space="preserve">
          <source>Hash function g(p,x) for a tree is an array of 32 randomly generated float arrays with the same dimension as the data set. This array is stored in GaussianRandomProjectionHash object and can be obtained from &lt;code&gt;components_&lt;/code&gt; attribute.</source>
          <target state="translated">树的哈希函数g（p，x）是由32个随机生成的float数组组成的数组，其维数与数据集的维数相同。该数组存储在GaussianRandomProjectionHash对象中，可以从 &lt;code&gt;components_&lt;/code&gt; attribute获取。</target>
        </trans-unit>
        <trans-unit id="8b5d87d4a16c0b826cb8988befd77a0e52c765c1" translate="yes" xml:space="preserve">
          <source>Hashing feature transformation using Totally Random Trees</source>
          <target state="translated">使用完全随机树进行哈希特征转换</target>
        </trans-unit>
        <trans-unit id="717a562588a8bf4bd25fb65069c4d3192c7a16dc" translate="yes" xml:space="preserve">
          <source>HashingVectorizer does not provide IDF weighting as this is a stateless model (the fit method does nothing). When IDF weighting is needed it can be added by pipelining its output to a TfidfTransformer instance.</source>
          <target state="translated">HashingVectorizer不提供IDF加权,因为这是一个无状态模型(拟合方法不做任何事情)。当需要IDF加权时,可以通过管道将其输出添加到TfidfTransformer实例中。</target>
        </trans-unit>
        <trans-unit id="d06cc92706967f16b8b9c95848cf5aff7ec1c456" translate="yes" xml:space="preserve">
          <source>HashingVectorizer hashes word occurrences to a fixed dimensional space, possibly with collisions. The word count vectors are then normalized to each have l2-norm equal to one (projected to the euclidean unit-ball) which seems to be important for k-means to work in high dimensional space.</source>
          <target state="translated">HashingVectorizer 将词的出现次数散列到一个固定的维度空间,可能会有碰撞。然后将词数向量归一化,使每个词数的l2-norm等于1(投射到欧氏单位球),这对于k-means在高维空间中的工作似乎很重要。</target>
        </trans-unit>
        <trans-unit id="28041ffc119d6685560d28cedcd34e917cd495e5" translate="yes" xml:space="preserve">
          <source>Hastie, R. Tibshirani and J. Friedman, &amp;ldquo;Elements of Statistical Learning Ed. 2&amp;rdquo;, Springer, 2009.</source>
          <target state="translated">Hastie，R。Tibshirani和J. Friedman，&amp;ldquo;统计学习的要素&amp;rdquo; 2&amp;rdquo;，施普林格，2009年。</target>
        </trans-unit>
        <trans-unit id="b8dd0d155e19e8a71f19b1bbe40cdccabf151805" translate="yes" xml:space="preserve">
          <source>Have a look at the &lt;a href=&quot;../../modules/feature_extraction#hashing-vectorizer&quot;&gt;Hashing Vectorizer&lt;/a&gt; as a memory efficient alternative to &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">看看&lt;a href=&quot;../../modules/feature_extraction#hashing-vectorizer&quot;&gt;Hashing Vectorizer&lt;/a&gt;是&lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; 的&lt;/a&gt;一种内存有效替代方案。</target>
        </trans-unit>
        <trans-unit id="a899619755f5d06da20b9b2964b88739a1ab106e" translate="yes" xml:space="preserve">
          <source>Have a look at using &lt;a href=&quot;../../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core Classification&lt;/a&gt; to learn from data that would not fit into the computer main memory.</source>
          <target state="translated">看一看使用&lt;a href=&quot;../../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;核外分类&lt;/a&gt;从不适合计算机主内存的数据中学习。</target>
        </trans-unit>
        <trans-unit id="27a688f240efc4c1f8111e73298dc1d5dd7e9964" translate="yes" xml:space="preserve">
          <source>HaversineDistance</source>
          <target state="translated">HaversineDistance</target>
        </trans-unit>
        <trans-unit id="260c7f8bcac0cff0858b268328a3c57270e6d05b" translate="yes" xml:space="preserve">
          <source>He, Kaiming, et al. &amp;ldquo;Delving deep into rectifiers: Surpassing human-level</source>
          <target state="translated">他，Kaiming等。&amp;ldquo;深入研究整流器：超越人类水平</target>
        </trans-unit>
        <trans-unit id="2f8a00b4f7c2990e23253c9271642cb45a1f2224" translate="yes" xml:space="preserve">
          <source>Helper class for readable parallel mapping.</source>
          <target state="translated">可读并行映射的辅助类。</target>
        </trans-unit>
        <trans-unit id="15e3ecfce92d858c5fac5d21e2153dba45c36e72" translate="yes" xml:space="preserve">
          <source>Helper function to test the message raised in an exception.</source>
          <target state="translated">用于测试异常情况下发出的消息的辅助函数。</target>
        </trans-unit>
        <trans-unit id="e22b8152bb5ec7ad5480951d5d1692b1809abba4" translate="yes" xml:space="preserve">
          <source>Hence using random projections on the digits dataset which only has 64 features in the input space does not make sense: it does not allow for dimensionality reduction in this case.</source>
          <target state="translated">因此,在输入空间中只有64个特征的数字数据集上使用随机投影是没有意义的:在这种情况下,它不允许减少维度。</target>
        </trans-unit>
        <trans-unit id="858c4ba42a503184b8af0061cb8145e1add6548c" translate="yes" xml:space="preserve">
          <source>Hence words that were not seen in the training corpus will be completely ignored in future calls to the transform method:</source>
          <target state="translated">因此,在训练语料中没有看到的词,在未来调用转换方法时将完全被忽略。</target>
        </trans-unit>
        <trans-unit id="3fea43b2d3bbf05cef0fdbdec4ca7b01a2de9eb5" translate="yes" xml:space="preserve">
          <source>Hence, the None case results in:</source>
          <target state="translated">因此,&quot;无 &quot;的结果是:</target>
        </trans-unit>
        <trans-unit id="4fffc6a6ec537bad9c19e154df4fbf4c1dee1839" translate="yes" xml:space="preserve">
          <source>Here &lt;code&gt;func&lt;/code&gt; is a function which takes two one-dimensional numpy arrays, and returns a distance. Note that in order to be used within the BallTree, the distance must be a true metric: i.e. it must satisfy the following properties</source>
          <target state="translated">这里的 &lt;code&gt;func&lt;/code&gt; 是一个函数，它接受两个一维numpy数组，并返回一个距离。请注意，为了在BallTree中使用该距离，必须是一个真实的度量标准：即它必须满足以下属性</target>
        </trans-unit>
        <trans-unit id="8918252717f29fe05952e0490941948a7c1afcd2" translate="yes" xml:space="preserve">
          <source>Here a sine function is fit with a polynomial of order 3, for values close to zero.</source>
          <target state="translated">这里的正弦函数是用一个3阶的多项式来拟合的,数值接近于零。</target>
        </trans-unit>
        <trans-unit id="dd2684d229285b3b1a04454d9cd068cd1e054408" translate="yes" xml:space="preserve">
          <source>Here a small example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; function with a svm classifier in a binary class problem:</source>
          <target state="translated">这是一个小示例，该示例演示在二进制类问题中将&lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt;函数与svm分类器一起使用：</target>
        </trans-unit>
        <trans-unit id="5113795d86cf9b1916006aecdb0ceee73192da33" translate="yes" xml:space="preserve">
          <source>Here a small excerpt which illustrates how to use the Gaussian random projection transformer:</source>
          <target state="translated">这里摘录一个小片段,说明如何使用高斯随机投影变换器。</target>
        </trans-unit>
        <trans-unit id="d838251a264bfc0a86e50e7c2f5d9ef6f54d10aa" translate="yes" xml:space="preserve">
          <source>Here a small excerpt which illustrates how to use the sparse random projection transformer:</source>
          <target state="translated">这里摘录一小段,说明如何使用稀疏随机投影变换器。</target>
        </trans-unit>
        <trans-unit id="32f51b9dd909238771016da8eae995fa183bb752" translate="yes" xml:space="preserve">
          <source>Here are a few suggestions to help further your scikit-learn intuition upon the completion of this tutorial:</source>
          <target state="translated">以下是一些建议,帮助你在完成本教程后,进一步提升你的scikit-learn直觉。</target>
        </trans-unit>
        <trans-unit id="8d2ed57227d29b030e11d07a6ad14619156d6baa" translate="yes" xml:space="preserve">
          <source>Here are some recommended ways to load standard columnar data into a format usable by scikit-learn:</source>
          <target state="translated">这里有一些推荐的方法可以将标准的列式数据加载到scikit-learn可用的格式中。</target>
        </trans-unit>
        <trans-unit id="6caa2e3f5fa319efda163f3ada59f70b9af4251d" translate="yes" xml:space="preserve">
          <source>Here are some small examples in binary classification:</source>
          <target state="translated">下面是二进制分类中的一些小例子。</target>
        </trans-unit>
        <trans-unit id="cad58f968788a0c8b830200526f46c2e8380af6d" translate="yes" xml:space="preserve">
          <source>Here is a list of incremental estimators for different tasks:</source>
          <target state="translated">以下是不同任务的增量估算器列表。</target>
        </trans-unit>
        <trans-unit id="e5cc3ef05cd44a377ff0113c5a0144a6cd05b3f4" translate="yes" xml:space="preserve">
          <source>Here is a sample output of a run on a quad-core machine:</source>
          <target state="translated">下面是在四核机上运行的输出示例。</target>
        </trans-unit>
        <trans-unit id="00dac27806e77f637d738445566eb627365e0881" translate="yes" xml:space="preserve">
          <source>Here is a sketch of a system designed to achieve this goal:</source>
          <target state="translated">下面是为实现这一目标而设计的一个系统的草图。</target>
        </trans-unit>
        <trans-unit id="c595619fa24f58ee3930e8429960f874f9b329e7" translate="yes" xml:space="preserve">
          <source>Here is a small example illustrating the usage of the &lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt;&lt;code&gt;matthews_corrcoef&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">这是一个小示例，说明了&lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt; &lt;code&gt;matthews_corrcoef&lt;/code&gt; &lt;/a&gt;函数的用法：</target>
        </trans-unit>
        <trans-unit id="423aaa3f8753fc630af578bc1fbb46728b5a06e5" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">这是一个&lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; &lt;/a&gt;变量用法的小例子：</target>
        </trans-unit>
        <trans-unit id="cb41f576a02130e8636700bae5b58c941781076b" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt;&lt;code&gt;mean_absolute_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">这是一个使用&lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt; &lt;code&gt;mean_absolute_error&lt;/code&gt; &lt;/a&gt;函数的小例子：</target>
        </trans-unit>
        <trans-unit id="7c0ba7d72bd4599fa8b6676ec86f846e3705f7da" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;mean_squared_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">这是一个使用&lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;mean_squared_error&lt;/code&gt; &lt;/a&gt;函数的小例子：</target>
        </trans-unit>
        <trans-unit id="0be0450f469be9534c036908ab2afdbd59b24548" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt;&lt;code&gt;mean_squared_log_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">这是一个使用&lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt; &lt;code&gt;mean_squared_log_error&lt;/code&gt; &lt;/a&gt;函数的小例子：</target>
        </trans-unit>
        <trans-unit id="f8da86e09b21d704ee9aa6f7fcb4b0cf6258a18d" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt;&lt;code&gt;median_absolute_error&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">这是使用&lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt; &lt;code&gt;median_absolute_error&lt;/code&gt; &lt;/a&gt;函数的一个小示例：</target>
        </trans-unit>
        <trans-unit id="e0060b4a19332fa9cdf176d47debc4e3de22af1f" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of the &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">这是使用&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt;函数的一个小示例：</target>
        </trans-unit>
        <trans-unit id="2121874e07dc9ac1fb205417370f94e728d5e5e6" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of this function:</source>
          <target state="translated">下面是这个函数使用的一个小例子。</target>
        </trans-unit>
        <trans-unit id="f03ea6f9a5b7db0b84376e166dbfe9d87d690fa9" translate="yes" xml:space="preserve">
          <source>Here is a small example of usage of this function::</source>
          <target state="translated">下面是这个函数使用的一个小例子:。</target>
        </trans-unit>
        <trans-unit id="da9291cb119f102218681b72119ede84a1e93115" translate="yes" xml:space="preserve">
          <source>Here is a usage example:</source>
          <target state="translated">下面是一个使用实例。</target>
        </trans-unit>
        <trans-unit id="ec46f6fe41e667dcb81fcf9a89e2aaf0a6763af5" translate="yes" xml:space="preserve">
          <source>Here is a visual representation of such a confusion matrix (this figure comes from the &lt;a href=&quot;../auto_examples/model_selection/plot_confusion_matrix#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py&quot;&gt;Confusion matrix&lt;/a&gt; example):</source>
          <target state="translated">这是这种混淆矩阵的直观表示（此图来自&lt;a href=&quot;../auto_examples/model_selection/plot_confusion_matrix#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py&quot;&gt;混淆矩阵&lt;/a&gt;示例）：</target>
        </trans-unit>
        <trans-unit id="876abdb2188ee5022ae84c77928e2082f05a478c" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior.</source>
          <target state="translated">下面是一个可视化的交叉验证行为。</target>
        </trans-unit>
        <trans-unit id="d5a8fd11bd11ae3f4eb764b39ba1acfee92579af" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior. Note that &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; is not affected by classes or groups.</source>
          <target state="translated">这是交叉验证行为的可视化。请注意，&lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;不受类或组的影响。</target>
        </trans-unit>
        <trans-unit id="e10cd61d7e44ad9e6bb0d4cec30745248d4c4e93" translate="yes" xml:space="preserve">
          <source>Here is a visualization of the cross-validation behavior. Note that &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt;&lt;code&gt;ShuffleSplit&lt;/code&gt;&lt;/a&gt; is not affected by classes or groups.</source>
          <target state="translated">这是交叉验证行为的可视化。请注意，&lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt; &lt;code&gt;ShuffleSplit&lt;/code&gt; &lt;/a&gt;不受类或组的影响。</target>
        </trans-unit>
        <trans-unit id="0b166c480658b240c273df0a43ce9ffa8405561c" translate="yes" xml:space="preserve">
          <source>Here is an example demonstrating the use of the &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; function with a svm classifier in a multiclass problem:</source>
          <target state="translated">这是一个示例，演示在多类问题中对svm分类器使用&lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt;函数：</target>
        </trans-unit>
        <trans-unit id="58bc8e3595ba3624485387b6def524d2d31bae65" translate="yes" xml:space="preserve">
          <source>Here is an example of &lt;code&gt;cross_validate&lt;/code&gt; using a single metric:</source>
          <target state="translated">以下是使用单个指标的 &lt;code&gt;cross_validate&lt;/code&gt; 示例：</target>
        </trans-unit>
        <trans-unit id="f7e50cdf4078c7823c206e72ce0bf5486f1e2a9f" translate="yes" xml:space="preserve">
          <source>Here is an example of applying this idea to one-dimensional data, using polynomial features of varying degrees:</source>
          <target state="translated">下面是一个将这一思想应用于一维数据的例子,使用不同程度的多项式特征。</target>
        </trans-unit>
        <trans-unit id="8375acd14d3c16b75f14ad4cf9799bf09154cba1" translate="yes" xml:space="preserve">
          <source>Here is an example of building custom scorers, and of using the &lt;code&gt;greater_is_better&lt;/code&gt; parameter:</source>
          <target state="translated">以下是构建自定义评分器以及使用 &lt;code&gt;greater_is_better&lt;/code&gt; 参数的示例：</target>
        </trans-unit>
        <trans-unit id="120ffb4ca9b2da814644df8eb634b8cef584b2a0" translate="yes" xml:space="preserve">
          <source>Here is an example to scale a toy data matrix to the &lt;code&gt;[0, 1]&lt;/code&gt; range:</source>
          <target state="translated">这是将玩具数据矩阵缩放到 &lt;code&gt;[0, 1]&lt;/code&gt; 范围的示例：</target>
        </trans-unit>
        <trans-unit id="8f89ae42a83e786b17cd6f1b83024799754e5687" translate="yes" xml:space="preserve">
          <source>Here is an example using &lt;code&gt;sklearn.linear_model.stochastic_gradient.SGDClassifier&lt;/code&gt; with the &lt;code&gt;elasticnet&lt;/code&gt; penalty. The regularization strength is globally controlled by the &lt;code&gt;alpha&lt;/code&gt; parameter. With a sufficiently high &lt;code&gt;alpha&lt;/code&gt;, one can then increase the &lt;code&gt;l1_ratio&lt;/code&gt; parameter of &lt;code&gt;elasticnet&lt;/code&gt; to enforce various levels of sparsity in the model coefficients. Higher sparsity here is interpreted as less model complexity as we need fewer coefficients to describe it fully. Of course sparsity influences in turn the prediction time as the sparse dot-product takes time roughly proportional to the number of non-zero coefficients.</source>
          <target state="translated">这是将 &lt;code&gt;sklearn.linear_model.stochastic_gradient.SGDClassifier&lt;/code&gt; 与 &lt;code&gt;elasticnet&lt;/code&gt; 罚分一起使用的示例。正则化强度由 &lt;code&gt;alpha&lt;/code&gt; 参数全局控制。具有足够高 &lt;code&gt;alpha&lt;/code&gt; ，可以再增加 &lt;code&gt;l1_ratio&lt;/code&gt; 的参数 &lt;code&gt;elasticnet&lt;/code&gt; 执行模型中的系数的稀疏性的各种级别。此处，稀疏度越高，模型的复杂度越低，因为我们需要更少的系数来充分描述它。当然，稀疏性反过来会影响预测时间，因为稀疏点积花费的时间与非零系数的数量大致成比例。</target>
        </trans-unit>
        <trans-unit id="540ee2aaf7182c6dfc449b18e5accb694e3b0894" translate="yes" xml:space="preserve">
          <source>Here is an example:</source>
          <target state="translated">下面是一个例子。</target>
        </trans-unit>
        <trans-unit id="a1ce1cc95adf7777aaf8483ebc72e46f7e0c5dd5" translate="yes" xml:space="preserve">
          <source>Here is how to use the toy data from the previous example with this scaler:</source>
          <target state="translated">下面是如何用这个标尺使用前面例子中的玩具数据。</target>
        </trans-unit>
        <trans-unit id="da00252cb105e8e07c4719d8131543c2597c6b64" translate="yes" xml:space="preserve">
          <source>Here is sample code that illustrates the use of the &lt;code&gt;sparsify()&lt;/code&gt; method:</source>
          <target state="translated">以下示例代码说明了 &lt;code&gt;sparsify()&lt;/code&gt; 方法的用法：</target>
        </trans-unit>
        <trans-unit id="b1b76d97b9ed98e3661e06b53d247e6f552362c3" translate="yes" xml:space="preserve">
          <source>Here is sample code to test the sparsity of your input:</source>
          <target state="translated">下面是测试输入稀疏度的示例代码。</target>
        </trans-unit>
        <trans-unit id="7a4f1fdf399f62578619e41a5fba4a345597683a" translate="yes" xml:space="preserve">
          <source>Here is the list of models benefiting from the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) for automated model selection:</source>
          <target state="translated">以下是受益于Akaike信息准则(AIC)或贝叶斯信息准则(BIC)的自动模型选择的模型列表。</target>
        </trans-unit>
        <trans-unit id="24d46233c5b1cf5947d798926d1e317b272fc656" translate="yes" xml:space="preserve">
          <source>Here is the list of such models:</source>
          <target state="translated">以下是这类车型的清单。</target>
        </trans-unit>
        <trans-unit id="8029b08717fd12d596415c9951c99ad442611512" translate="yes" xml:space="preserve">
          <source>Here the computation is achieved thanks to Martinsson&amp;rsquo;s Randomized SVD algorithm implemented in scikit-learn.</source>
          <target state="translated">这里的计算是通过scikit-learn中实现的Martinsson的随机SVD算法实现的。</target>
        </trans-unit>
        <trans-unit id="baa6fd34087f3f3b80a068e5198c152eb2224084" translate="yes" xml:space="preserve">
          <source>Here the results are not as good as they could be as our choice for the regularization parameter C was not the best. In real life applications this parameter is usually chosen using &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt;.</source>
          <target state="translated">在这里，结果不如我们期望的那样好，因为正则化参数C并不是最好的选择。在实际应用中，通常使用&lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;调整估计器的超参数&lt;/a&gt;来选择此参数。</target>
        </trans-unit>
        <trans-unit id="e33a1b9fd8981a72cf8c17c638c489933e2535f4" translate="yes" xml:space="preserve">
          <source>Here we choose the SAGA solver because it can efficiently optimize for the Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.</source>
          <target state="translated">这里我们之所以选择SAGA求解器,是因为它可以有效地对Logistic回归损失进行非平稳、稀疏性诱导的L1惩罚优化。</target>
        </trans-unit>
        <trans-unit id="d4668460d1dfffc9a12dd3f0ca645c8016c22599" translate="yes" xml:space="preserve">
          <source>Here we compare 3 approaches:</source>
          <target state="translated">这里我们比较3种方法。</target>
        </trans-unit>
        <trans-unit id="3aadfee7aa5bef8aeacf179790398b01b017dc93" translate="yes" xml:space="preserve">
          <source>Here we describe variational inference algorithms on Dirichlet process mixture. The Dirichlet process is a prior probability distribution on &lt;em&gt;clusterings with an infinite, unbounded, number of partitions&lt;/em&gt;. Variational techniques let us incorporate this prior structure on Gaussian mixture models at almost no penalty in inference time, comparing with a finite Gaussian mixture model.</source>
          <target state="translated">在这里，我们描述了Dirichlet过程混合的变分推理算法。Dirichlet过程是&lt;em&gt;具有无限，无界分区数的聚类&lt;/em&gt;的先验概率分布。与有限的高斯混合模型相比，变分技术使我们可以将这种先验结构并入高斯混合模型中，而在推理时间上几乎没有损失。</target>
        </trans-unit>
        <trans-unit id="19eba1946aa4b2b6c504a0a7a0b9c334a19205ae" translate="yes" xml:space="preserve">
          <source>Here we fit a multinomial logistic regression with L1 penalty on a subset of the MNIST digits classification task. We use the SAGA algorithm for this purpose: this a solver that is fast when the number of samples is significantly larger than the number of features and is able to finely optimize non-smooth objective functions which is the case with the l1-penalty. Test accuracy reaches &amp;gt; 0.8, while weight vectors remains &lt;em&gt;sparse&lt;/em&gt; and therefore more easily &lt;em&gt;interpretable&lt;/em&gt;.</source>
          <target state="translated">在这里，我们对MNIST数字分类任务的子集进行L1罚分的多项式逻辑回归。为此，我们使用SAGA算法：这是一种求解器，当样本数量明显大于特征数量时，它是快速的，并且能够很好地优化非平滑目标函数，这就是l1罚分的情况。测试精度达到&amp;gt; 0.8，而权重向量仍然&lt;em&gt;稀疏&lt;/em&gt;，因此更容易&lt;em&gt;解释&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="bc0bdb5bd44175832d7fcee805ba26710508e043" translate="yes" xml:space="preserve">
          <source>Here we have used &lt;code&gt;kernel='gaussian'&lt;/code&gt;, as seen above. Mathematically, a kernel is a positive function \(K(x;h)\) which is controlled by the bandwidth parameter \(h\). Given this kernel form, the density estimate at a point \(y\) within a group of points \(x_i; i=1\cdots N\) is given by:</source>
          <target state="translated">如上所示，这里我们使用 &lt;code&gt;kernel='gaussian'&lt;/code&gt; 。从数学上讲，内核是由带宽参数\（h \）控制的正函数\（K（x; h）\）。给定该内核形式，在一组点\（x_i; i = 1 \ cdots N \）内的点\（y \）处的密度估计为：</target>
        </trans-unit>
        <trans-unit id="9bc4f467db4b070f940a4ae98fc40a9d9951075c" translate="yes" xml:space="preserve">
          <source>Here we simulate independent sources using a highly non-Gaussian process, 2 student T with a low number of degrees of freedom (top left figure). We mix them to create observations (top right figure). In this raw observation space, directions identified by PCA are represented by orange vectors. We represent the signal in the PCA space, after whitening by the variance corresponding to the PCA vectors (lower left). Running ICA corresponds to finding a rotation in this space to identify the directions of largest non-Gaussianity (lower right).</source>
          <target state="translated">在这里,我们使用一个高度非高斯过程模拟独立的来源,2学生T与低自由度数(左上图)。我们将它们混合起来创建观测值(右上图)。在这个原始观测空间中,由PCA识别的方向由橙色向量表示。我们在PCA空间中表示信号,通过PCA向量对应的方差进行白化后(左下图)。运行ICA相当于在这个空间中找到一个旋转,以识别最大的非高斯性的方向(右下)。</target>
        </trans-unit>
        <trans-unit id="4efad2f65eb490631f07741acecbb3c6dbc45f0c" translate="yes" xml:space="preserve">
          <source>Here we use the l1 sparsity that trims the weights of not informative features to zero. This is good if the goal is to extract the strongly discriminative vocabulary of each class. If the goal is to get the best predictive accuracy, it is better to use the non sparsity-inducing l2 penalty instead.</source>
          <target state="translated">在这里,我们使用l1稀疏性,将非信息特征的权重修剪为零。如果目标是提取每个类的强区分性词汇,这是很好的。如果目标是获得最好的预测精度,最好使用非稀疏性诱导的l2惩罚来代替。</target>
        </trans-unit>
        <trans-unit id="0260adbe3be54cb933a36e08a92f87d76459f0fc" translate="yes" xml:space="preserve">
          <source>Here, \(\alpha \geq 0\) is a complexity parameter that controls the amount of shrinkage: the larger the value of \(\alpha\), the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.</source>
          <target state="translated">这里,\(\alpha \geq 0)\是一个控制收缩量的复杂性参数:\(\alpha \)的值越大,收缩量越大,因此系数对一致性的影响也越大。</target>
        </trans-unit>
        <trans-unit id="9412689bc5806775f9bf0419d0db25d5c39e9741" translate="yes" xml:space="preserve">
          <source>Here, the classifier is &lt;code&gt;fit()&lt;/code&gt; on a 2d binary label representation of &lt;code&gt;y&lt;/code&gt;, using the &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.labelbinarizer#sklearn.preprocessing.LabelBinarizer&quot;&gt;&lt;code&gt;LabelBinarizer&lt;/code&gt;&lt;/a&gt;. In this case &lt;code&gt;predict()&lt;/code&gt; returns a 2d array representing the corresponding multilabel predictions.</source>
          <target state="translated">在这里，分类器使用&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.labelbinarizer#sklearn.preprocessing.LabelBinarizer&quot;&gt; &lt;code&gt;LabelBinarizer&lt;/code&gt; &lt;/a&gt;在 &lt;code&gt;y&lt;/code&gt; 的2d二进制标签表示上进行 &lt;code&gt;fit()&lt;/code&gt; 。在这种情况下， &lt;code&gt;predict()&lt;/code&gt; 返回表示相应多标签预测的2d数组。</target>
        </trans-unit>
        <trans-unit id="e42ec4b790491f01a91defa6334fdc833c4f6019" translate="yes" xml:space="preserve">
          <source>Here, the default kernel &lt;code&gt;rbf&lt;/code&gt; is first changed to &lt;code&gt;linear&lt;/code&gt; via &lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC.set_params&quot;&gt;&lt;code&gt;SVC.set_params()&lt;/code&gt;&lt;/a&gt; after the estimator has been constructed, and changed back to &lt;code&gt;rbf&lt;/code&gt; to refit the estimator and to make a second prediction.</source>
          <target state="translated">这里，默认内核 &lt;code&gt;rbf&lt;/code&gt; 首先改变为 &lt;code&gt;linear&lt;/code&gt; 经由&lt;a href=&quot;../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC.set_params&quot;&gt; &lt;code&gt;SVC.set_params()&lt;/code&gt; &lt;/a&gt;估计器被构造之后和变回 &lt;code&gt;rbf&lt;/code&gt; 改装的估计器和使一个第二预测。</target>
        </trans-unit>
        <trans-unit id="379b23b33ba6458bba2f568a4c2b136ad7c827a5" translate="yes" xml:space="preserve">
          <source>Here, the first &lt;code&gt;predict()&lt;/code&gt; returns an integer array, since &lt;code&gt;iris.target&lt;/code&gt; (an integer array) was used in &lt;code&gt;fit&lt;/code&gt;. The second &lt;code&gt;predict()&lt;/code&gt; returns a string array, since &lt;code&gt;iris.target_names&lt;/code&gt; was for fitting.</source>
          <target state="translated">在这里，第一个 &lt;code&gt;predict()&lt;/code&gt; 返回一个整数数组，因为在 &lt;code&gt;fit&lt;/code&gt; 中使用了 &lt;code&gt;iris.target&lt;/code&gt; （一个整数数组）。由于 &lt;code&gt;iris.target_names&lt;/code&gt; 适合拟合，所以第二个 &lt;code&gt;predict()&lt;/code&gt; 返回一个字符串数组。</target>
        </trans-unit>
        <trans-unit id="21a97ae1e0557499a4ed4420c47199de8f6f0cde" translate="yes" xml:space="preserve">
          <source>Here, the number of samples is slightly larger than the number of dimensions, thus the empirical covariance is still invertible. However, as the observations are strongly correlated, the empirical covariance matrix is ill-conditioned and as a result its inverse &amp;ndash;the empirical precision matrix&amp;ndash; is very far from the ground truth.</source>
          <target state="translated">在这里，样本数略大于维度数，因此经验协方差仍然是可逆的。但是，由于观测值之间有很强的相关性，因此经验协方差矩阵条件不佳，因此其逆矩阵（经验精度矩阵）与基本事实相距甚远。</target>
        </trans-unit>
        <trans-unit id="2c6a31e993187ebfe932ff15824a46e0c83fd078" translate="yes" xml:space="preserve">
          <source>Here, the predicted class label is 2, since it has the highest average probability.</source>
          <target state="translated">这里,预测的类标签是2,因为它的平均概率最高。</target>
        </trans-unit>
        <trans-unit id="264995c0dc7ac7309d4709ed0ce1258e4439b015" translate="yes" xml:space="preserve">
          <source>Hessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method of solving the regularization problem of LLE. It revolves around a hessian-based quadratic form at each neighborhood which is used to recover the locally linear structure. Though other implementations note its poor scaling with data size, &lt;code&gt;sklearn&lt;/code&gt; implements some algorithmic improvements which make its cost comparable to that of other LLE variants for small output dimension. HLLE can be performed with function &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt;&lt;code&gt;locally_linear_embedding&lt;/code&gt;&lt;/a&gt; or its object-oriented counterpart &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;, with the keyword &lt;code&gt;method = 'hessian'&lt;/code&gt;. It requires &lt;code&gt;n_neighbors &amp;gt; n_components * (n_components + 3) / 2&lt;/code&gt;.</source>
          <target state="translated">Hessian特征映射（也称为基于Hessian的LLE：HLLE）是解决LLE正则化问题的另一种方法。它围绕每个邻域的基于粗麻布的二次形式旋转，用于恢复局部线性结构。尽管其他实现注意到其在数据大小上的缩放性较差，但 &lt;code&gt;sklearn&lt;/code&gt; 实现了一些算法上的改进，使其成本可与小输出尺寸的其他LLE变体相比。 HLLE可以通过函数&lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt; &lt;code&gt;locally_linear_embedding&lt;/code&gt; &lt;/a&gt;或其面向对象的对&lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt; LocallyLinearEmbedding来执行，并使用关键字 &lt;code&gt;method = 'hessian'&lt;/code&gt; 。它需要 &lt;code&gt;n_neighbors &amp;gt; n_components * (n_components + 3) / 2&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="afac02e66e409c4004e2cc2adafb5b5e842109eb" translate="yes" xml:space="preserve">
          <source>Hierarchical agglomerative clustering: Ward</source>
          <target state="translated">层次聚类聚类。沃德</target>
        </trans-unit>
        <trans-unit id="09f7d65b121068e93f6d1d655d20b242aded6b7b" translate="yes" xml:space="preserve">
          <source>Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. See the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;Wikipedia page&lt;/a&gt; for more details.</source>
          <target state="translated">分层聚类是一般的聚类算法家族，它们通过依次合并或拆分嵌套聚类来构建它们。群集的这种层次结构表示为树（或树状图）。树的根是聚集所有样本的唯一簇，叶子是只有一个样本的簇。有关更多详细信息，请参见&lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;Wikipedia页面&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="dbe40063cd6e20f1519715e45afa5ca7ed6442de" translate="yes" xml:space="preserve">
          <source>Hierarchical clustering: structured vs unstructured ward</source>
          <target state="translated">层次聚类:结构化病房与非结构化病房的比较</target>
        </trans-unit>
        <trans-unit id="645ba4388b8ba9172558b321f188082e4d2fd9ef" translate="yes" xml:space="preserve">
          <source>High-dimensional datasets can be very difficult to visualize. While data in two or three dimensions can be plotted to show the inherent structure of the data, equivalent high-dimensional plots are much less intuitive. To aid visualization of the structure of a dataset, the dimension must be reduced in some way.</source>
          <target state="translated">高维数据集可能非常难以可视化。虽然可以绘制二维或三维数据来显示数据的固有结构,但等效的高维图就不那么直观了。为了帮助数据集结构的可视化,必须以某种方式减少维度。</target>
        </trans-unit>
        <trans-unit id="75c18021e736bcb99a099c597122a642054caa8c" translate="yes" xml:space="preserve">
          <source>Hinge: (soft-margin) Support Vector Machines.</source>
          <target state="translated">铰 。(软边际)支持向量机。</target>
        </trans-unit>
        <trans-unit id="a319ae13863bb8d6da087a8b6e0305de9278e27f" translate="yes" xml:space="preserve">
          <source>Hinton, Geoffrey E.</source>
          <target state="translated">Hinton,Geoffrey E.</target>
        </trans-unit>
        <trans-unit id="b8cd925555526484cde7ba65174f600e33250f6b" translate="yes" xml:space="preserve">
          <source>Hochreiter, Bodenhofer, et. al., 2010. &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/&quot;&gt;FABIA: factor analysis for bicluster acquisition&lt;/a&gt;.</source>
          <target state="translated">Hochreiter，Bodenhofer等。等人，2010年&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/&quot;&gt;。FABIA：双峰收购的因素分析&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="7a5c3ac7ac58dcde82acc59c23c0dce0d31966d1" translate="yes" xml:space="preserve">
          <source>Holds the label for each class.</source>
          <target state="translated">保持每个班级的标签。</target>
        </trans-unit>
        <trans-unit id="4b7dceb5fe5f7e92199815a2b66fef8fdf05dc27" translate="yes" xml:space="preserve">
          <source>Homogeneity and completeness scores are formally given by:</source>
          <target state="translated">同质性和完整性得分正式给出:</target>
        </trans-unit>
        <trans-unit id="7c13e77bc830b382c041eec0e3fcc1723f375e05" translate="yes" xml:space="preserve">
          <source>Homogeneity metric of a cluster labeling given a ground truth.</source>
          <target state="translated">给定地真值的聚类标签的同质性度量。</target>
        </trans-unit>
        <trans-unit id="3afc9b67230d985e8782525c7b58b615e013e8f9" translate="yes" xml:space="preserve">
          <source>Homogeneity, completeness and V-measure can be computed at once using &lt;a href=&quot;generated/sklearn.metrics.homogeneity_completeness_v_measure#sklearn.metrics.homogeneity_completeness_v_measure&quot;&gt;&lt;code&gt;homogeneity_completeness_v_measure&lt;/code&gt;&lt;/a&gt; as follows:</source>
          <target state="translated">可以使用&lt;a href=&quot;generated/sklearn.metrics.homogeneity_completeness_v_measure#sklearn.metrics.homogeneity_completeness_v_measure&quot;&gt; &lt;code&gt;homogeneity_completeness_v_measure&lt;/code&gt; &lt;/a&gt;如下计算同质性，完整性和V度量：</target>
        </trans-unit>
        <trans-unit id="0878824f511837fc1a1c8d27240af19053ebdbd4" translate="yes" xml:space="preserve">
          <source>HouseAge median house age in block</source>
          <target state="translated">房龄 板块房龄中位数</target>
        </trans-unit>
        <trans-unit id="be45c283b4c54643c38f84bc65a4bfc525d6d30a" translate="yes" xml:space="preserve">
          <source>How often to evaluate perplexity. Only used in &lt;code&gt;fit&lt;/code&gt; method. set it to 0 or negative number to not evalute perplexity in training at all. Evaluating perplexity can help you check convergence in training process, but it will also increase total training time. Evaluating perplexity in every iteration might increase training time up to two-fold.</source>
          <target state="translated">多久评估一次困惑。仅用于 &lt;code&gt;fit&lt;/code&gt; 方法。将其设置为0或负数完全不会评估训练中的困惑。评估困惑度可以帮助您检查训练过程中的收敛性，但同时也会增加总的训练时间。在每次迭代中评估困惑度可能会使训练时间增加两倍。</target>
        </trans-unit>
        <trans-unit id="060faa287065b4ad6ba6c00f598635b42adc21de" translate="yes" xml:space="preserve">
          <source>How to compute the normalizer in the denominator. Possible options are &amp;lsquo;min&amp;rsquo;, &amp;lsquo;geometric&amp;rsquo;, &amp;lsquo;arithmetic&amp;rsquo;, and &amp;lsquo;max&amp;rsquo;. If &amp;lsquo;warn&amp;rsquo;, &amp;lsquo;geometric&amp;rsquo; will be used. The default will change to &amp;lsquo;arithmetic&amp;rsquo; in version 0.22.</source>
          <target state="translated">如何在分母中计算归一化。可能的选项是&amp;ldquo;最小&amp;rdquo;，&amp;ldquo;几何&amp;rdquo;，&amp;ldquo;算术&amp;rdquo;和&amp;ldquo;最大&amp;rdquo;。如果&amp;ldquo;警告&amp;rdquo;，将使用&amp;ldquo;几何&amp;rdquo;。默认值将在0.22版中更改为&amp;ldquo;算术&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="b8efa217d0db9ce56ac60653645beebe151304f9" translate="yes" xml:space="preserve">
          <source>How to compute the normalizer in the denominator. Possible options are &amp;lsquo;min&amp;rsquo;, &amp;lsquo;geometric&amp;rsquo;, &amp;lsquo;arithmetic&amp;rsquo;, and &amp;lsquo;max&amp;rsquo;. If &amp;lsquo;warn&amp;rsquo;, &amp;lsquo;max&amp;rsquo; will be used. The default will change to &amp;lsquo;arithmetic&amp;rsquo; in version 0.22.</source>
          <target state="translated">如何在分母中计算归一化。可能的选项是&amp;ldquo;最小&amp;rdquo;，&amp;ldquo;几何&amp;rdquo;，&amp;ldquo;算术&amp;rdquo;和&amp;ldquo;最大&amp;rdquo;。如果&amp;ldquo;警告&amp;rdquo;，将使用&amp;ldquo;最大&amp;rdquo;。默认值将在0.22版中更改为&amp;ldquo;算术&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="cf894bb3f8fceedab10cdd5da9a5edd37e00865d" translate="yes" xml:space="preserve">
          <source>How to construct the affinity matrix.</source>
          <target state="translated">如何构建亲和力矩阵。</target>
        </trans-unit>
        <trans-unit id="d34268ba2716d71aaaeee2c35e527ca55a46dd2f" translate="yes" xml:space="preserve">
          <source>However ARI can also be useful in a purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection (TODO).</source>
          <target state="translated">然而ARI也可以在纯粹的无监督环境中作为共识指数的构建模块,用于聚类模型的选择(TODO)。</target>
        </trans-unit>
        <trans-unit id="0121c2b22395d1a9db3fd77f24d0a9f0e41170e3" translate="yes" xml:space="preserve">
          <source>However MI-based measures can also be useful in purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection.</source>
          <target state="translated">然而,基于MI的测量方法在纯粹的无监督环境下也可以作为共识指数的构建模块,用于聚类模型的选择。</target>
        </trans-unit>
        <trans-unit id="117b6230e0e8ab3fcdc1b277507becef8a05f759" translate="yes" xml:space="preserve">
          <source>However care must taken to always make the affinity matrix symmetric so that the eigenvector decomposition works as expected.</source>
          <target state="translated">然而必须注意始终使亲和矩阵对称,这样特征向量分解才能达到预期效果。</target>
        </trans-unit>
        <trans-unit id="925f5b77eb2888a89c04118c35bff0f0ace7255e" translate="yes" xml:space="preserve">
          <source>However the RI score does not guarantee that random label assignments will get a value close to zero (esp. if the number of clusters is in the same order of magnitude as the number of samples).</source>
          <target state="translated">然而RI得分并不能保证随机标签分配会得到一个接近于零的值(特别是当聚类数量与样本数量在同一数量级时)。</target>
        </trans-unit>
        <trans-unit id="11d179b15971b8eaae6270be9fce57c0bd0d2416" translate="yes" xml:space="preserve">
          <source>However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.</source>
          <target state="translated">然而,通过将可用数据划分为三组,我们大大减少了可用于学习模型的样本数量,结果可能取决于对(训练、验证)集的特定随机选择。</target>
        </trans-unit>
        <trans-unit id="56c680421b5fb07e56baa9a65f13a80fce385b54" translate="yes" xml:space="preserve">
          <source>However, coefficient estimates for Ordinary Least Squares rely on the independence of the model terms. When terms are correlated and the columns of the design matrix \(X\) have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed response, producing a large variance. This situation of &lt;em&gt;multicollinearity&lt;/em&gt; can arise, for example, when data are collected without an experimental design.</source>
          <target state="translated">但是，普通最小二乘的系数估计取决于模型项的独立性。当项相关并且设计矩阵\（X \）的列具有近似线性相关性时，设计矩阵变得接近奇异，结果，最小二乘估计对观察到的响应中的随机误差变得高度敏感，产生很大的差异。例如，在没有实验设计的情况下收集数据时，可能会出现&lt;em&gt;多重共线性的&lt;/em&gt;情况。</target>
        </trans-unit>
        <trans-unit id="bf734282463bfc3a9cb343729f546342ec401691" translate="yes" xml:space="preserve">
          <source>However, if the learning curve is steep for the training size in question, then 5- or 10- fold cross validation can overestimate the generalization error.</source>
          <target state="translated">然而,如果相关训练规模的学习曲线很陡峭,那么5倍或10倍的交叉验证会高估泛化误差。</target>
        </trans-unit>
        <trans-unit id="fc162d85afa0b20b4064f40b16eb0e55ca89c629" translate="yes" xml:space="preserve">
          <source>However, it is sometimes helpful to plot the influence of a single hyperparameter on the training score and the validation score to find out whether the estimator is overfitting or underfitting for some hyperparameter values.</source>
          <target state="translated">然而,有时绘制单个超参数对训练得分和验证得分的影响,以了解估计器对某些超参数值的拟合过度或拟合不足,是很有帮助的。</target>
        </trans-unit>
        <trans-unit id="5c8b24673bb3f660f66f90035a856044be6d6f9e" translate="yes" xml:space="preserve">
          <source>However, note that this transformer will only do a binary one-hot encoding when feature values are of type string. If categorical features are represented as numeric values such as int, the DictVectorizer can be followed by &lt;a href=&quot;sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt;&lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt;&lt;/a&gt; to complete binary one-hot encoding.</source>
          <target state="translated">但是，请注意，当特征值的类型为string时，此转换器只会执行二进制一键编码。如果将分类特征表示为数字值（例如int），则DictVectorizer后面可以跟着&lt;a href=&quot;sklearn.preprocessing.onehotencoder#sklearn.preprocessing.OneHotEncoder&quot;&gt; &lt;code&gt;sklearn.preprocessing.OneHotEncoder&lt;/code&gt; ,&lt;/a&gt;以完成二进制一键编码。</target>
        </trans-unit>
        <trans-unit id="8b25d9ad009118aef0894664f601ac10786f8b49" translate="yes" xml:space="preserve">
          <source>However, this is not the most precise way of doing this computation, and the distance matrix returned by this function may not be exactly symmetric as required by, e.g., &lt;code&gt;scipy.spatial.distance&lt;/code&gt; functions.</source>
          <target state="translated">但是，这不是进行此计算的最精确方法，并且此函数返回的距离矩阵可能不像 &lt;code&gt;scipy.spatial.distance&lt;/code&gt; 函数所要求的那样完全对称。</target>
        </trans-unit>
        <trans-unit id="379cfb166aa26713fe1131478e9b37a4224780ad" translate="yes" xml:space="preserve">
          <source>Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent</source>
          <target state="translated">余祥福,黄芳兰,林志仁(2011).雙座標下降</target>
        </trans-unit>
        <trans-unit id="15d1d4b26d2ba06629e368bf6c8a860d8762ef89" translate="yes" xml:space="preserve">
          <source>Huber (&lt;code&gt;'huber'&lt;/code&gt;): Another robust loss function that combines least squares and least absolute deviation; use &lt;code&gt;alpha&lt;/code&gt; to control the sensitivity with regards to outliers (see &lt;a href=&quot;#f2001&quot; id=&quot;id15&quot;&gt;[F2001]&lt;/a&gt; for more details).</source>
          <target state="translated">Huber（ &lt;code&gt;'huber'&lt;/code&gt; ）：另一个结合最小二乘和最小绝对偏差的鲁棒损失函数；使用 &lt;code&gt;alpha&lt;/code&gt; 控制离群值的敏感度（有关更多详细信息，请参见&lt;a href=&quot;#f2001&quot; id=&quot;id15&quot;&gt;[F2001]&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="3d12d436101cbc3212af50bf81000f6d78d4cf01" translate="yes" xml:space="preserve">
          <source>HuberRegressor vs Ridge on dataset with strong outliers</source>
          <target state="translated">HuberRegressor与Ridge在具有强离群值的数据集上的对比。</target>
        </trans-unit>
        <trans-unit id="7e58a6e8d89e8504ad31e135de9b485ad40f05f6" translate="yes" xml:space="preserve">
          <source>Hue</source>
          <target state="translated">Hue</target>
        </trans-unit>
        <trans-unit id="c7a8b2b20a9c45f674f17cd8ef7ece305e1c36eb" translate="yes" xml:space="preserve">
          <source>Hue:</source>
          <target state="translated">Hue:</target>
        </trans-unit>
        <trans-unit id="4e99bcdee413a9c98d317e0e8e4199a2bf582f90" translate="yes" xml:space="preserve">
          <source>Hugo Chavez</source>
          <target state="translated">乌戈-查韦斯</target>
        </trans-unit>
        <trans-unit id="135e7e12c7ab5ea7496649e72ee134478ecf558e" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter. Default is 1.e-6.</source>
          <target state="translated">超参数:Gamma分布在alpha参数上的反比例参数(速率参数)。默认为1.e-6。</target>
        </trans-unit>
        <trans-unit id="761054fe5bafcad49a16f057764235860452b8da" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. Default is 1.e-6</source>
          <target state="translated">超参数:Gamma分布在lambda参数上的反比例参数(速率参数)。默认值为1.e-6</target>
        </trans-unit>
        <trans-unit id="6001ea6392d3003569381e7107254e88f75fd600" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. Default is 1.e-6.</source>
          <target state="translated">超参数:Gamma分布在lambda参数上的反比例参数(速率参数)。默认为1.e-6。</target>
        </trans-unit>
        <trans-unit id="81e171654bf22a490946ec147c219e96694497ff" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. Default is 1.e-6</source>
          <target state="translated">Hyper-parameter:Gamma分布在alpha参数上的形状参数。默认为1.e-6</target>
        </trans-unit>
        <trans-unit id="b07af48fd68aeaacb4df041ef30bae006150c237" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. Default is 1.e-6.</source>
          <target state="translated">Hyper-parameter:Gamma分布在alpha参数上的形状参数,默认为1.e-6。默认为1.e-6。</target>
        </trans-unit>
        <trans-unit id="1398aea0b1e181e76b6d9d73db4040ccf06ee2f7" translate="yes" xml:space="preserve">
          <source>Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter. Default is 1.e-6.</source>
          <target state="translated">超参数:Gamma分布在lambda参数上的形状参数。默认为1.e-6。</target>
        </trans-unit>
        <trans-unit id="7a5b8a439bb2492412d2944256add4dcdf337928" translate="yes" xml:space="preserve">
          <source>Hyper-parameter optimizers</source>
          <target state="translated">超参数优化器</target>
        </trans-unit>
        <trans-unit id="223bf115da53d3d9cdf837b624135b565596fd92" translate="yes" xml:space="preserve">
          <source>Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;kernel&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt; for Support Vector Classifier, &lt;code&gt;alpha&lt;/code&gt; for Lasso, etc.</source>
          <target state="translated">超参数是无法在估算器中直接学习的参数。在scikit-learn中，它们作为参数传递给estimator类的构造函数。典型示例包括 &lt;code&gt;C&lt;/code&gt; ，支持向量分类器的 &lt;code&gt;kernel&lt;/code&gt; 和 &lt;code&gt;gamma&lt;/code&gt; ，Lasso的 &lt;code&gt;alpha&lt;/code&gt; 等等。</target>
        </trans-unit>
        <trans-unit id="568b05951392672a52de0358537dd29fcafbe544" translate="yes" xml:space="preserve">
          <source>Hyper-parameters of an estimator can be updated after it has been constructed via the &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-set-params&quot;&gt;set_params()&lt;/a&gt; method. Calling &lt;code&gt;fit()&lt;/code&gt; more than once will overwrite what was learned by any previous &lt;code&gt;fit()&lt;/code&gt;:</source>
          <target state="translated">通过&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-set-params&quot;&gt;set_params（）&lt;/a&gt;方法构造估算器的超参数后，可以对其进行更新。多次调用 &lt;code&gt;fit()&lt;/code&gt; 会覆盖以前的 &lt;code&gt;fit()&lt;/code&gt; 所学的内容：</target>
        </trans-unit>
        <trans-unit id="1db8c072507305b4aa23189287be39423349b8f4" translate="yes" xml:space="preserve">
          <source>Hyperparameter of the ridge regression that learns the inverse transform (when fit_inverse_transform=True).</source>
          <target state="translated">学习反变换的岭回归的超参数(当fit_inverse_transform=True)。</target>
        </trans-unit>
        <trans-unit id="181eca8daf7aaeed93f61701c7eddb643dc6b36a" translate="yes" xml:space="preserve">
          <source>Hyperparameters:</source>
          <target state="translated">Hyperparameters:</target>
        </trans-unit>
        <trans-unit id="8bb86931be2a9d0449c3eec151da751cb88591f1" translate="yes" xml:space="preserve">
          <source>I. Guyon, &amp;ldquo;Design of experiments for the NIPS 2003 variable selection benchmark&amp;rdquo;, 2003.</source>
          <target state="translated">I. Guyon，&amp;ldquo; NIPS 2003变量选择基准的实验设计&amp;rdquo;，2003年。</target>
        </trans-unit>
        <trans-unit id="a238a89365b9d0ce7f5fb26e189eb03cdc08fbe5" translate="yes" xml:space="preserve">
          <source>ICA can also be used as yet another non linear decomposition that finds components with some sparsity:</source>
          <target state="translated">ICA也可以作为另一种非线性分解,找到具有一定稀疏性的成分。</target>
        </trans-unit>
        <trans-unit id="fcc34dd193c826ae2f0c8b804c532252b4a25480" translate="yes" xml:space="preserve">
          <source>INDUS proportion of non-retail business acres per town</source>
          <target state="translated">每镇非零售商业亩数的工业比例。</target>
        </trans-unit>
        <trans-unit id="44a4d7b7db7815be999da6a406f4dadd2c4327c5" translate="yes" xml:space="preserve">
          <source>Identification number of each sample, as ordered in dataset.data.</source>
          <target state="translated">每个样本的识别号,按dataset.data的顺序排列。</target>
        </trans-unit>
        <trans-unit id="02d51b4f13558cbcfc807b53522b1ffb156ad7e7" translate="yes" xml:space="preserve">
          <source>Identity: d(x, y) = 0 if and only if x == y</source>
          <target state="translated">特征:如果且仅当x==y时,d(x,y)=0。</target>
        </trans-unit>
        <trans-unit id="35bd2069c37f2c6a308bc5401948b247d5bcfc02" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;all&amp;rdquo;, the imputer mask will represent all features.</source>
          <target state="translated">如果为&amp;ldquo;全部&amp;rdquo;，则防毒面具将代表所有功能。</target>
        </trans-unit>
        <trans-unit id="84934b5d658c0a370c458ee55fa3255bb65884a6" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo; (default), the imputer mask will be of same type as input.</source>
          <target state="translated">如果为&amp;ldquo;自动&amp;rdquo;（默认），则防毒面具与输入的类型相同。</target>
        </trans-unit>
        <trans-unit id="7cdc1bc49e801caf1ff212e1000d88bb13d7e93e" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo;, then &lt;code&gt;max_features=n_features&lt;/code&gt;.</source>
          <target state="translated">如果为&amp;ldquo; auto&amp;rdquo;，则 &lt;code&gt;max_features=n_features&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="164a2722286c1b34bc2df80a90c75397afce3e6b" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt;.</source>
          <target state="translated">如果为&amp;ldquo; auto&amp;rdquo;，则 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="911a50d98b398312fa01572b5d7b864da542117b" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;auto&amp;rdquo;, then &lt;code&gt;max_samples=min(256, n_samples)&lt;/code&gt;.</source>
          <target state="translated">如果为&amp;ldquo; auto&amp;rdquo;，则 &lt;code&gt;max_samples=min(256, n_samples)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="dca169b413a6ec050ae0928eb38f43b00b9c08e0" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;constant&amp;rdquo;, then replace missing values with fill_value. Can be used with strings or numeric data.</source>
          <target state="translated">如果为&amp;ldquo;常量&amp;rdquo;，则将缺失的值替换为fill_value。可以与字符串或数字数据一起使用。</target>
        </trans-unit>
        <trans-unit id="fed653e1ff76c14b62a8cb9c0f4474c620b2641e" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;log2&amp;rdquo;, then &lt;code&gt;max_features=log2(n_features)&lt;/code&gt;.</source>
          <target state="translated">如果为&amp;ldquo; log2&amp;rdquo;，则 &lt;code&gt;max_features=log2(n_features)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e799052bdd1932be1b28378fc91f87421f6d1065" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;mean&amp;rdquo;, then replace missing values using the mean along each column. Can only be used with numeric data.</source>
          <target state="translated">如果是&amp;ldquo;平均值&amp;rdquo;，则使用每列中的平均值替换缺失值。只能与数字数据一起使用。</target>
        </trans-unit>
        <trans-unit id="8c28cbae695709f5ae6daaba6d2035fa26d4e040" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;mean&amp;rdquo;, then replace missing values using the mean along the axis.</source>
          <target state="translated">如果为&amp;ldquo;平均值&amp;rdquo;，则使用沿轴的平均值替换缺失值。</target>
        </trans-unit>
        <trans-unit id="d5353b7f39f25231d62cbbc36fcd604e05d2faa0" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;median&amp;rdquo;, then replace missing values using the median along each column. Can only be used with numeric data.</source>
          <target state="translated">如果为&amp;ldquo;中位数&amp;rdquo;，则使用每列中的中位数替换缺失值。只能与数字数据一起使用。</target>
        </trans-unit>
        <trans-unit id="2c7bf0a70af62c9d1ff80c38810d3732da415b46" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;median&amp;rdquo;, then replace missing values using the median along the axis.</source>
          <target state="translated">如果为&amp;ldquo;中位数&amp;rdquo;，则使用沿轴的中位数替换缺失值。</target>
        </trans-unit>
        <trans-unit id="b9dfb246debbef95e2bc6e78da2b0aca54b8e768" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;missing-only&amp;rdquo; (default), the imputer mask will only represent features containing missing values during fit time.</source>
          <target state="translated">如果为&amp;ldquo;仅缺失&amp;rdquo;（默认），则不当面罩将仅表示在拟合时间内包含缺失值的要素。</target>
        </trans-unit>
        <trans-unit id="fb9cd590a090a11e857ebbc7c5d49f19787d4a57" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;most_frequent&amp;rdquo;, then replace missing using the most frequent value along each column. Can be used with strings or numeric data.</source>
          <target state="translated">如果为&amp;ldquo; most_frequent&amp;rdquo;，则使用每一列中的最频繁值替换&amp;ldquo; missing&amp;rdquo;。可以与字符串或数字数据一起使用。</target>
        </trans-unit>
        <trans-unit id="a90ab2bff0e7c4a2db0c7d70bcb17fa44e1b8cb3" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;most_frequent&amp;rdquo;, then replace missing using the most frequent value along the axis.</source>
          <target state="translated">如果为&amp;ldquo; most_frequent&amp;rdquo;，则使用沿轴的最频繁值替换&amp;ldquo; missing&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="d25972b438acba3aa495003bff5b880c3dc78f95" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;prefit&amp;rdquo; is passed, it is assumed that base_estimator has been fitted already and all data is used for calibration.</source>
          <target state="translated">如果通过&amp;ldquo; prefit&amp;rdquo;，则假定已经安装了base_estimator，并且所有数据都用于校准。</target>
        </trans-unit>
        <trans-unit id="ddc01f2adfc0b5da49bb0bea104c04055f37082b" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;sqrt&amp;rdquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; (same as &amp;ldquo;auto&amp;rdquo;).</source>
          <target state="translated">如果为&amp;ldquo; sqrt&amp;rdquo;，则 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; （与&amp;ldquo; auto&amp;rdquo;相同）。</target>
        </trans-unit>
        <trans-unit id="050de520f25e08567f8dcb7bcdaa6887bd8ca53c" translate="yes" xml:space="preserve">
          <source>If &amp;ldquo;sqrt&amp;rdquo;, then &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt;.</source>
          <target state="translated">如果为&amp;ldquo; sqrt&amp;rdquo;，则 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="274dd12ab1d70a7a9d00df8bbe2aa7f35f2ca3c4" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;SAMME.R&amp;rsquo; then use the SAMME.R real boosting algorithm. &lt;code&gt;base_estimator&lt;/code&gt; must support calculation of class probabilities. If &amp;lsquo;SAMME&amp;rsquo; then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.</source>
          <target state="translated">如果为&amp;ldquo; SAMME.R&amp;rdquo;，则使用SAMME.R实际增强算法。 &lt;code&gt;base_estimator&lt;/code&gt; 必须支持类概率的计算。如果为&amp;ldquo; SAMME&amp;rdquo;，则使用SAMME离散提升算法。SAMME.R算法通常比SAMME收敛更快，从而以更少的提升迭代次数实现了更低的测试误差。</target>
        </trans-unit>
        <trans-unit id="7bac8214432dad215f7331c0af16dfd3868b9e19" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;balanced&amp;rsquo;, class weights will be given by &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;. If a dictionary is given, keys are classes and values are corresponding class weights. If None is given, the class weights will be uniform.</source>
          <target state="translated">如果&amp;ldquo;平衡&amp;rdquo;，则类权重将由 &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; 。如果给出字典，则键为类，值为相应的类权重。如果未指定，则班级权重将是统一的。</target>
        </trans-unit>
        <trans-unit id="456401c87cfc2a15cdd408f2dd8be315dc3e833e" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;english&amp;rsquo;, a built-in stop word list for English is used. There are several known issues with &amp;lsquo;english&amp;rsquo; and you should consider an alternative (see &lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;Using stop words&lt;/a&gt;).</source>
          <target state="translated">如果为&amp;ldquo; english&amp;rdquo;，则使用内置的英语停用词列表。&amp;ldquo;英语&amp;rdquo;存在几个已知的问题，您应该考虑一种替代方法（请参阅&lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;使用停用词&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="f2019bdbdfa8956b7b54e8a5677954f4996f6374" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;file&amp;rsquo;, the sequence items must have a &amp;lsquo;read&amp;rsquo; method (file-like object) that is called to fetch the bytes in memory.</source>
          <target state="translated">如果为&amp;ldquo;文件&amp;rdquo;，则序列项必须具有一个&amp;ldquo;读取&amp;rdquo;方法（类似文件的对象），该方法被调用以获取内存中的字节。</target>
        </trans-unit>
        <trans-unit id="3dd1f3ca74314afe1ddf15184f6ab04977d1a20b" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;filename&amp;rsquo;, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.</source>
          <target state="translated">如果为'filename'，则作为参数传递给fit的序列应该是需要读取以获取原始内容进行分析的文件名列表。</target>
        </trans-unit>
        <trans-unit id="6381402e5f026c95872c614d3f284a846d432d3e" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;hard&amp;rsquo;, uses predicted class labels for majority rule voting. Else if &amp;lsquo;soft&amp;rsquo;, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.</source>
          <target state="translated">如果为&amp;ldquo;硬&amp;rdquo;，则将预测的类别标签用于多数规则投票。否则，如果为&amp;ldquo; soft&amp;rdquo;，则根据预测的概率之和的argmax来预测类别标签，建议对经过良好校准的分类器进行组合。</target>
        </trans-unit>
        <trans-unit id="034a602ff18129b75a77961464f62c916fb178cb" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;precomputed&amp;rsquo;, the training input X is expected to be a distance matrix.</source>
          <target state="translated">如果&amp;ldquo;预先计算&amp;rdquo;，则训练输入X预期为距离矩阵。</target>
        </trans-unit>
        <trans-unit id="efe2693abb0ad6fb0bb32fc7410403c6f8a6131c" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;warm_start&amp;rsquo; is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. In that case, &amp;lsquo;n_init&amp;rsquo; is ignored and only a single initialization occurs upon the first call. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">如果'warm_start'为True，则最后一次拟合的解决方案将用作下一次fit（）的初始化。在类似问题上多次调用拟合时，这可以加快收敛速度​​。在这种情况下，&amp;ldquo; n_init&amp;rdquo;将被忽略，并且在第一次调用时仅发生一次初始化。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="245f671779646599b33075b7dcf37bf735b34555" translate="yes" xml:space="preserve">
          <source>If &amp;lsquo;warm_start&amp;rsquo; is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">如果'warm_start'为True，则最后一次拟合的解决方案将用作下一次fit（）的初始化。在类似问题上多次调用拟合时，这可以加快收敛速度​​。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e311856e2dbc16a358c30263eb21c62e3f976c09" translate="yes" xml:space="preserve">
          <source>If &lt;a href=&quot;generated/sklearn.preprocessing.minmaxscaler#sklearn.preprocessing.MinMaxScaler&quot;&gt;&lt;code&gt;MinMaxScaler&lt;/code&gt;&lt;/a&gt; is given an explicit &lt;code&gt;feature_range=(min, max)&lt;/code&gt; the full formula is:</source>
          <target state="translated">如果给&lt;a href=&quot;generated/sklearn.preprocessing.minmaxscaler#sklearn.preprocessing.MinMaxScaler&quot;&gt; &lt;code&gt;MinMaxScaler&lt;/code&gt; &lt;/a&gt;一个明确的 &lt;code&gt;feature_range=(min, max)&lt;/code&gt; 则完整公式为：</target>
        </trans-unit>
        <trans-unit id="6f352ac5787c6e0994736f1793f840aefef2eb74" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;0 &amp;lt; n_components &amp;lt; 1&lt;/code&gt; and &lt;code&gt;svd_solver == 'full'&lt;/code&gt;, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components.</source>
          <target state="translated">如果 &lt;code&gt;0 &amp;lt; n_components &amp;lt; 1&lt;/code&gt; 并且 &lt;code&gt;svd_solver == 'full'&lt;/code&gt; ，则选择组件数，以使需要解释的方差量大于n_components指定的百分比。</target>
        </trans-unit>
        <trans-unit id="6154e481a1bfebf053da4021c41ed6b15075ac75" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, &lt;code&gt;Gram&lt;/code&gt; is overwritten.</source>
          <target state="translated">如果 &lt;code&gt;False&lt;/code&gt; ，则 &lt;code&gt;Gram&lt;/code&gt; 被覆盖。</target>
        </trans-unit>
        <trans-unit id="c8ea59a59509714d84c6c3be2a8959e87ca2c339" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt; is overwritten.</source>
          <target state="translated">如果 &lt;code&gt;False&lt;/code&gt; ，则 &lt;code&gt;X&lt;/code&gt; 被覆盖。</target>
        </trans-unit>
        <trans-unit id="ecd953eee019b7cf39fa95c1745e9486ce5fb903" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, return the number of correctly classified samples. Otherwise, return the fraction of correctly classified samples.</source>
          <target state="translated">如果为 &lt;code&gt;False&lt;/code&gt; ，则返回正确分类的样本数。否则，返回正确分类的样本的分数。</target>
        </trans-unit>
        <trans-unit id="8dd52da2b8b6d856acbbc6b969b86fd0a4246941" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, return the number of misclassifications. Otherwise, return the fraction of misclassifications.</source>
          <target state="translated">如果为 &lt;code&gt;False&lt;/code&gt; ，则返回错误分类的数量。否则，返回错误分类的分数。</target>
        </trans-unit>
        <trans-unit id="85fcfc531652a8814592a07e791b2030fbc9598e" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, return the sum of the Jaccard similarity coefficient over the sample set. Otherwise, return the average of Jaccard similarity coefficient.</source>
          <target state="translated">如果为 &lt;code&gt;False&lt;/code&gt; ，则返回样本集上的Jaccard相似系数之和。否则，返回Jaccard相似系数的平均值。</target>
        </trans-unit>
        <trans-unit id="c21045a17e85201e2f77134fc96d9edd698a8ef9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;False&lt;/code&gt;, the &lt;code&gt;cv_results_&lt;/code&gt; attribute will not include training scores.</source>
          <target state="translated">如果为 &lt;code&gt;False&lt;/code&gt; ，则 &lt;code&gt;cv_results_&lt;/code&gt; 属性将不包括训练分数。</target>
        </trans-unit>
        <trans-unit id="4f7a2b9af6d7b5ad533a302e51ca948f564886c6" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt; the estimator&amp;rsquo;s default scorer is used.</source>
          <target state="translated">如果为 &lt;code&gt;None&lt;/code&gt; ，则使用估算器的默认计分器。</target>
        </trans-unit>
        <trans-unit id="0206caad9c301767e28c1eb37b72a3a7f7598e94" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;None&lt;/code&gt;, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:</source>
          <target state="translated">如果为 &lt;code&gt;None&lt;/code&gt; ，则返回每个班级的分数。否则，这将确定对数据执行的平均类型：</target>
        </trans-unit>
        <trans-unit id="f1170dbf5a5618e80add067200212cb5804a58cd" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt; the full path is stored in the &lt;code&gt;coef_path_&lt;/code&gt; attribute. If you compute the solution for a large problem or many targets, setting &lt;code&gt;fit_path&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; will lead to a speedup, especially with a small alpha.</source>
          <target state="translated">如果为 &lt;code&gt;True&lt;/code&gt; ,则完整路径将存储在 &lt;code&gt;coef_path_&lt;/code&gt; 属性中。如果您针对一个大问题或许多目标计算解决方案， &lt;code&gt;fit_path&lt;/code&gt; 设置为 &lt;code&gt;False&lt;/code&gt; 将导致加速，尤其是对于较小的alpha而言。</target>
        </trans-unit>
        <trans-unit id="e703de20e5680ee264e2b1b950a8b1ca587cd24f" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, X will be copied; else, it may be overwritten.</source>
          <target state="translated">如果为 &lt;code&gt;True&lt;/code&gt; ，将复制X；否则为X。否则，它可能会被覆盖。</target>
        </trans-unit>
        <trans-unit id="8e26b0bb501133486ba99496e311f44a49ce472b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, perform metric MDS; otherwise, perform nonmetric MDS.</source>
          <target state="translated">如果为 &lt;code&gt;True&lt;/code&gt; ，则执行度量MDS；否则，执行非度量MDS。</target>
        </trans-unit>
        <trans-unit id="a2b129bca8e38a348fd53d1896c7796acded2f57" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, return a sparse feature matrix</source>
          <target state="translated">如果为 &lt;code&gt;True&lt;/code&gt; ，则返回一个稀疏特征矩阵</target>
        </trans-unit>
        <trans-unit id="887d26ef7077238a636d223d3985225a211c8d82" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, return the prior class probability and conditional probabilities of features given classes, from which the data was drawn.</source>
          <target state="translated">如果为 &lt;code&gt;True&lt;/code&gt; ，则返回给定类的特征的先验类概率和条件概率，并从中得出数据。</target>
        </trans-unit>
        <trans-unit id="0b7bef40bad08d9d2c4e67da6c9b56ea79751005" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;True&lt;/code&gt;, some instances might not belong to any class.</source>
          <target state="translated">如果为 &lt;code&gt;True&lt;/code&gt; ，则某些实例可能不属于任何类。</target>
        </trans-unit>
        <trans-unit id="e223ba26a8622e808f0df02e86007844177282d6" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;algorithm=&amp;rsquo;lasso_lars&amp;rsquo;&lt;/code&gt; or &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the penalty applied to the L1 norm. If &lt;code&gt;algorithm=&amp;rsquo;threshold&amp;rsquo;&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the absolute value of the threshold below which coefficients will be squashed to zero. If &lt;code&gt;algorithm=&amp;rsquo;omp&amp;rsquo;&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides &lt;code&gt;n_nonzero_coefs&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;algorithm=&amp;rsquo;lasso_lars&amp;rsquo;&lt;/code&gt; 或 &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt; ，则 &lt;code&gt;alpha&lt;/code&gt; 是应用于L1范数的惩罚。如果 &lt;code&gt;algorithm=&amp;rsquo;threshold&amp;rsquo;&lt;/code&gt; ，则 &lt;code&gt;alpha&lt;/code&gt; 是阈值的绝对值，低于该阈值时系数将被压缩为零。如果 &lt;code&gt;algorithm=&amp;rsquo;omp&amp;rsquo;&lt;/code&gt; ，则 &lt;code&gt;alpha&lt;/code&gt; 为容差参数：目标重建误差的值。在这种情况下，它将覆盖 &lt;code&gt;n_nonzero_coefs&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="a88caf8f6b6232395c9c1524315c6ed672bcf763" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=0&lt;/code&gt; and X is encoded as a CSR matrix;</source>
          <target state="translated">如果 &lt;code&gt;axis=0&lt;/code&gt; 且X编码为CSR矩阵；</target>
        </trans-unit>
        <trans-unit id="b74f02ef0c3e3aceaf2040e39764c8bf5d153fee" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=0&lt;/code&gt;, then impute along columns.</source>
          <target state="translated">如果 &lt;code&gt;axis=0&lt;/code&gt; ，则沿列插补。</target>
        </trans-unit>
        <trans-unit id="231cba4e2ed9eee1fca5c3a6b02c0c6f66c47550" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=1&lt;/code&gt; and X is encoded as a CSC matrix.</source>
          <target state="translated">如果 &lt;code&gt;axis=1&lt;/code&gt; 并且X编码为CSC矩阵。</target>
        </trans-unit>
        <trans-unit id="4405a4c1e894889993d89bb6694cef1a6a8f7db3" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;axis=1&lt;/code&gt;, then impute along rows.</source>
          <target state="translated">如果 &lt;code&gt;axis=1&lt;/code&gt; ，则沿行插补。</target>
        </trans-unit>
        <trans-unit id="bf71a4a70c1ff1b9076f02c43d89e78c4b0ffc27" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;backend&lt;/code&gt; is a string it must match a previously registered implementation using the &lt;code&gt;register_parallel_backend&lt;/code&gt; function.</source>
          <target state="translated">如果 &lt;code&gt;backend&lt;/code&gt; 是字符串，则必须使用 &lt;code&gt;register_parallel_backend&lt;/code&gt; 函数匹配先前注册的实现。</target>
        </trans-unit>
        <trans-unit id="6456a4494f2ba1f052aff4cf6d35ef66e787bc14" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;base_estimator&lt;/code&gt; is None, then &lt;code&gt;base_estimator=sklearn.linear_model.LinearRegression()&lt;/code&gt; is used for target values of dtype float.</source>
          <target state="translated">如果 &lt;code&gt;base_estimator&lt;/code&gt; 为None，则 &lt;code&gt;base_estimator=sklearn.linear_model.LinearRegression()&lt;/code&gt; 用于dtype float的目标值。</target>
        </trans-unit>
        <trans-unit id="898731158b64382ef9aad2307b39d22b5cd2a315" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;dense&lt;/code&gt; return &lt;code&gt;Y&lt;/code&gt; in the dense binary indicator format. If &lt;code&gt;'sparse'&lt;/code&gt; return &lt;code&gt;Y&lt;/code&gt; in the sparse binary indicator format. &lt;code&gt;False&lt;/code&gt; returns a list of lists of labels.</source>
          <target state="translated">如果为 &lt;code&gt;dense&lt;/code&gt; ，则以密集二进制指示符格式返回 &lt;code&gt;Y&lt;/code&gt; 。如果为 &lt;code&gt;'sparse'&lt;/code&gt; ，则以稀疏二进制指示符格式返回 &lt;code&gt;Y&lt;/code&gt; 。 &lt;code&gt;False&lt;/code&gt; 返回标签列表的列表。</target>
        </trans-unit>
        <trans-unit id="b1213caecc623f2a5139e82ba5db339edb5f6265" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;fit_intercept&lt;/code&gt; is set to False, the intercept is set to zero. &lt;code&gt;intercept_&lt;/code&gt; is of shape (1,) when the given problem is binary. In particular, when &lt;code&gt;multi_class=&amp;rsquo;multinomial&amp;rsquo;&lt;/code&gt;, &lt;code&gt;intercept_&lt;/code&gt; corresponds to outcome 1 (True) and &lt;code&gt;-intercept_&lt;/code&gt; corresponds to outcome 0 (False).</source>
          <target state="translated">如果 &lt;code&gt;fit_intercept&lt;/code&gt; 设置为False，则截距设置为零。当给定问题为二进制时， &lt;code&gt;intercept_&lt;/code&gt; 的形状为（1，）。特别是，当 &lt;code&gt;multi_class=&amp;rsquo;multinomial&amp;rsquo;&lt;/code&gt; ， &lt;code&gt;intercept_&lt;/code&gt; 对应于结果1（真），而 &lt;code&gt;-intercept_&lt;/code&gt; 对应于结果0（假）。</target>
        </trans-unit>
        <trans-unit id="4504cae87026fef1f6989cfa20e2e5bc171d37e0" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;fit_intercept&lt;/code&gt; is set to False, the intercept is set to zero. &lt;code&gt;intercept_&lt;/code&gt; is of shape(1,) when the problem is binary.</source>
          <target state="translated">如果 &lt;code&gt;fit_intercept&lt;/code&gt; 设置为False，则截距设置为零。问题为二进制时， &lt;code&gt;intercept_&lt;/code&gt; 的形状为（1，）。</target>
        </trans-unit>
        <trans-unit id="646836188c841e4fea39e4e4200d1f27e6191986" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;loss&lt;/code&gt; is a callable, then it should be a function that takes two arrays as inputs, the true and predicted value and returns a 1-D array with the i-th value of the array corresponding to the loss on &lt;code&gt;X[i]&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;loss&lt;/code&gt; 是可调用的，则它应该是一个将两个数组作为输入的真实值和预测值，并返回一维数组的函数，该数组的第i个值对应于 &lt;code&gt;X[i]&lt;/code&gt; 上的损失。</target>
        </trans-unit>
        <trans-unit id="c42d62680a26d66fc0f439c634f24ee01c670c77" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;memory&lt;/code&gt; is not joblib.Memory-like.</source>
          <target state="translated">如果 &lt;code&gt;memory&lt;/code&gt; 不是joblib。类似内存。</target>
        </trans-unit>
        <trans-unit id="e14dd7c153267d74f6b2214ce66bcf041482d792" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_bins&lt;/code&gt; is an array, and there is an ignored feature at index &lt;code&gt;i&lt;/code&gt;, &lt;code&gt;n_bins[i]&lt;/code&gt; will be ignored.</source>
          <target state="translated">如果 &lt;code&gt;n_bins&lt;/code&gt; 是一个数组，并且在索引 &lt;code&gt;i&lt;/code&gt; 处有被忽略的功能，则 &lt;code&gt;n_bins[i]&lt;/code&gt; 将被忽略。</target>
        </trans-unit>
        <trans-unit id="20ab457ec31da79f104a0f7a337ba6bfe94b5438" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_clusters&lt;/code&gt; is set to None, the data is reduced from 100,000 samples to a set of 158 clusters. This can be viewed as a preprocessing step before the final (global) clustering step that further reduces these 158 clusters to 100 clusters.</source>
          <target state="translated">如果 &lt;code&gt;n_clusters&lt;/code&gt; 设置为None，则数据从100,000个样本减少到158个群集。这可以看作是最终（全局）群集步骤之前的预处理步骤，该步骤进一步将这158个群集减少到100个群集。</target>
        </trans-unit>
        <trans-unit id="1769c2fe615105013ff722090827f85ac960dff9" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_components == 'mle'&lt;/code&gt; and &lt;code&gt;svd_solver == 'full'&lt;/code&gt;, Minka&amp;rsquo;s MLE is used to guess the dimension. Use of &lt;code&gt;n_components == 'mle'&lt;/code&gt; will interpret &lt;code&gt;svd_solver == 'auto'&lt;/code&gt; as &lt;code&gt;svd_solver == 'full'&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;n_components == 'mle'&lt;/code&gt; 并且 &lt;code&gt;svd_solver == 'full'&lt;/code&gt; ，则使用Minka的MLE来猜测尺寸。使用 &lt;code&gt;n_components == 'mle'&lt;/code&gt; 会将 &lt;code&gt;svd_solver == 'auto'&lt;/code&gt; 为 &lt;code&gt;svd_solver == 'full'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="959758e689ea656dd0e72e3bda18b0a45bbef2e0" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_components&lt;/code&gt; is not set then all components are stored and the sum of the ratios is equal to 1.0.</source>
          <target state="translated">如果未设置 &lt;code&gt;n_components&lt;/code&gt; ,则将存储所有分量，并且比率之和等于1.0。</target>
        </trans-unit>
        <trans-unit id="4aaa2df3fa37c88b24d06638ef7188d7e8ebe112" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_jobs&lt;/code&gt; was set to a value higher than one, the data is copied for each parameter setting(and not &lt;code&gt;n_jobs&lt;/code&gt; times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available. A workaround in this case is to set &lt;code&gt;pre_dispatch&lt;/code&gt;. Then, the memory is copied only &lt;code&gt;pre_dispatch&lt;/code&gt; many times. A reasonable value for &lt;code&gt;pre_dispatch&lt;/code&gt; is &lt;code&gt;2 * n_jobs&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;n_jobs&lt;/code&gt; 的值设置为大于1，则将为每个参数设置复制数据（而不是 &lt;code&gt;n_jobs&lt;/code&gt; 次）。出于效率原因，如果单个作业花费的时间很少，则执行此操作，但是如果数据集很大且没有足够的可用内存，则可能会引发错误。在这种情况下，一种解决方法是设置 &lt;code&gt;pre_dispatch&lt;/code&gt; 。然后，仅将 &lt;code&gt;pre_dispatch&lt;/code&gt; 复制多次内存。一个合理的值 &lt;code&gt;pre_dispatch&lt;/code&gt; 是 &lt;code&gt;2 * n_jobs&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="53682a81a25d0884d79ca09b064b0fc6e7cabd67" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_jobs&lt;/code&gt; was set to a value higher than one, the data is copied for each point in the grid (and not &lt;code&gt;n_jobs&lt;/code&gt; times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available. A workaround in this case is to set &lt;code&gt;pre_dispatch&lt;/code&gt;. Then, the memory is copied only &lt;code&gt;pre_dispatch&lt;/code&gt; many times. A reasonable value for &lt;code&gt;pre_dispatch&lt;/code&gt; is &lt;code&gt;2 * n_jobs&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;n_jobs&lt;/code&gt; 的值设置为大于1，则将为网格中的每个点复制数据（而不是 &lt;code&gt;n_jobs&lt;/code&gt; 次）。出于效率原因，如果单个作业花费的时间很少，则执行此操作，但是如果数据集很大且没有足够的可用内存，则可能会引发错误。在这种情况下，一种解决方法是设置 &lt;code&gt;pre_dispatch&lt;/code&gt; 。然后，仅将 &lt;code&gt;pre_dispatch&lt;/code&gt; 复制多次内存。一个合理的值 &lt;code&gt;pre_dispatch&lt;/code&gt; 是 &lt;code&gt;2 * n_jobs&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c1cc035dd2ff12188f95601d6fe5c4679ad6bb78" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;n_samples == 10000&lt;/code&gt;, storing &lt;code&gt;X&lt;/code&gt; as a NumPy array of type float32 would require 10000 x 100000 x 4 bytes = &lt;strong&gt;4GB in RAM&lt;/strong&gt; which is barely manageable on today&amp;rsquo;s computers.</source>
          <target state="translated">如果 &lt;code&gt;n_samples == 10000&lt;/code&gt; ，将 &lt;code&gt;X&lt;/code&gt; 存储为float32类型的NumPy数组将需要10000 x 100000 x 4字节= &lt;strong&gt;4GB RAM&lt;/strong&gt;，这在当今的计算机上几乎无法管理。</target>
        </trans-unit>
        <trans-unit id="8e65ba558868cb56b0c8a76632ea67901b254afe" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalize == True&lt;/code&gt;, return the average Jaccard similarity coefficient, else it returns the sum of the Jaccard similarity coefficient over the sample set.</source>
          <target state="translated">如果 &lt;code&gt;normalize == True&lt;/code&gt; ，则返回平均Jaccard相似系数，否则返回样本集上Jaccard相似系数的总和。</target>
        </trans-unit>
        <trans-unit id="c606413521700e073d3e669024415faa2300a113" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalize == True&lt;/code&gt;, return the fraction of correctly classified samples (float), else returns the number of correctly classified samples (int).</source>
          <target state="translated">如果 &lt;code&gt;normalize == True&lt;/code&gt; ，则返回正确分类的样本的分数（浮点数），否则返回正确分类的样本数（整数）。</target>
        </trans-unit>
        <trans-unit id="cd9dc36a4d7167817c2823908b4ce633913b9765" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;normalize == True&lt;/code&gt;, return the fraction of misclassifications (float), else it returns the number of misclassifications (int).</source>
          <target state="translated">如果 &lt;code&gt;normalize == True&lt;/code&gt; ，则返回错误分类的分数（浮点数），否则返回错误分类的数目（整数）。</target>
        </trans-unit>
        <trans-unit id="d0b860961bc5b4a4c45189c0fd4de5c2d61f1ab4" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;out=None&lt;/code&gt;, returns a new array containing the mean values, otherwise a reference to the output array is returned.</source>
          <target state="translated">如果 &lt;code&gt;out=None&lt;/code&gt; ，则返回一个包含平均值的新数组，否则返回对输出数组的引用。</target>
        </trans-unit>
        <trans-unit id="a1e72f87e91fc5bb6f8882ece59a46bf9ee089e2" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;pos_label is None&lt;/code&gt; and in binary classification, this function returns the average precision, recall and F-measure if &lt;code&gt;average&lt;/code&gt; is one of &lt;code&gt;'micro'&lt;/code&gt;, &lt;code&gt;'macro'&lt;/code&gt;, &lt;code&gt;'weighted'&lt;/code&gt; or &lt;code&gt;'samples'&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;pos_label is None&lt;/code&gt; 并且在二进制分类中，则该函数将返回平均精度，查全率和F度量，如果 &lt;code&gt;average&lt;/code&gt; 是 &lt;code&gt;'micro'&lt;/code&gt; ， &lt;code&gt;'macro'&lt;/code&gt; ， &lt;code&gt;'weighted'&lt;/code&gt; 或 &lt;code&gt;'samples'&lt;/code&gt; 之一。</target>
        </trans-unit>
        <trans-unit id="7bb4c6eca31ede3ca3e8fe5a9b41ecd9a55b266b" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;return_path==True&lt;/code&gt; returns the entire path, else returns only the last point of the path.</source>
          <target state="translated">如果 &lt;code&gt;return_path==True&lt;/code&gt; 返回整个路径，否则仅返回路径的最后一点。</target>
        </trans-unit>
        <trans-unit id="9e477bb8072307702a812f869b8166fe31bf9ca0" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;smooth_idf=True&lt;/code&gt; (the default), the constant &amp;ldquo;1&amp;rdquo; is added to the numerator and denominator of the idf as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions: idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.</source>
          <target state="translated">如果 &lt;code&gt;smooth_idf=True&lt;/code&gt; （默认值），则将常数&amp;ldquo; 1&amp;rdquo;添加到idf的分子和分母，就好像看到一个额外的文档恰好包含一次集合中的每个术语一样，这防止了零除：idf（d，t ）= log [（1 + n）/（1 + df（d，t））] + 1。</target>
        </trans-unit>
        <trans-unit id="3f772487671a5560a2a2bb3464b54cf0bad5b348" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;svd_solver == 'arpack'&lt;/code&gt;, the number of components must be strictly less than the minimum of n_features and n_samples.</source>
          <target state="translated">如果 &lt;code&gt;svd_solver == 'arpack'&lt;/code&gt; ，则组件的数量必须严格小于n_features和n_samples的最小值。</target>
        </trans-unit>
        <trans-unit id="abdb8ed2b871d03510343cf9ee77736674d298ab" translate="yes" xml:space="preserve">
          <source>If &lt;code&gt;validate&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt; will be checked.</source>
          <target state="translated">如果 &lt;code&gt;validate&lt;/code&gt; 为 &lt;code&gt;True&lt;/code&gt; ，将检查 &lt;code&gt;X&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5cfb594032bd50fcef2738a25df520e95867f411" translate="yes" xml:space="preserve">
          <source>If C is a ground truth class assignment and K the clustering, let us define \(a\) and \(b\) as:</source>
          <target state="translated">如果C是一个地道的类赋值,K是聚类,让我们把\(a\)和\(b\)定义为。</target>
        </trans-unit>
        <trans-unit id="40e72ab25b1921db07187a1c526cc9080a10eaea" translate="yes" xml:space="preserve">
          <source>If False, X will be overwritten. &lt;code&gt;copy=False&lt;/code&gt; can be used to save memory but is unsafe for general use.</source>
          <target state="translated">如果为False，则X将被覆盖。 &lt;code&gt;copy=False&lt;/code&gt; 可以用来节省内存，但是对于一般用途来说是不安全的。</target>
        </trans-unit>
        <trans-unit id="b5379fd8e8700833a560e6ab84ea58c40e10b6a8" translate="yes" xml:space="preserve">
          <source>If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead.</source>
          <target state="translated">如果False,传递给fit的数据会被覆盖,运行fit(X).transform(X)不会得到预期的结果,请使用fit_transform(X)代替。</target>
        </trans-unit>
        <trans-unit id="3d545281a4ef31d03ffb244079b728a3c5cc8b18" translate="yes" xml:space="preserve">
          <source>If False, data passed to fit are overwritten. Defaults to True.</source>
          <target state="translated">如果为False,则覆盖传递给拟合的数据。默认值为True。</target>
        </trans-unit>
        <trans-unit id="4bf616e8d9d604d2525c59d889782525e410270c" translate="yes" xml:space="preserve">
          <source>If False, distances will not be returned</source>
          <target state="translated">如果为 &quot;False&quot;,将不返回距离。</target>
        </trans-unit>
        <trans-unit id="d8cdf8e9cb326e6212f67c80aca3a4f04326fc4c" translate="yes" xml:space="preserve">
          <source>If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site.</source>
          <target state="translated">如果False,如果数据在本地不可用,则引发一个IOError,而不是试图从源站点下载数据。</target>
        </trans-unit>
        <trans-unit id="707f36c34b2f81eacbaf143a8e62cb9371b1332e" translate="yes" xml:space="preserve">
          <source>If False, raise an IOError if the data is not locally available instead of trying to download the data from the source site.</source>
          <target state="translated">如果False,如果数据在本地不可用,则引发一个IOError,而不是试图从源站点下载数据。</target>
        </trans-unit>
        <trans-unit id="d32001af2bcb0806daa431ab8cf432f700c0bb79" translate="yes" xml:space="preserve">
          <source>If False, the imputer mask will be a numpy array.</source>
          <target state="translated">如果False,imputer mask将是一个numpy数组。</target>
        </trans-unit>
        <trans-unit id="2823ebb07c9bdb5cae0bfca227f5db48d585ba5a" translate="yes" xml:space="preserve">
          <source>If False, the input arrays X and dictionary will not be checked.</source>
          <target state="translated">如果False,则不会检查输入数组X和字典。</target>
        </trans-unit>
        <trans-unit id="bd8e933f9aa74b9b27da566d4ffb96f4e62218cf" translate="yes" xml:space="preserve">
          <source>If False, the input arrays X and y will not be checked.</source>
          <target state="translated">如果False,则不检查输入数组X和y。</target>
        </trans-unit>
        <trans-unit id="85aa52dd8c7d5d29b6bebfd616a7ca3fe91cde14" translate="yes" xml:space="preserve">
          <source>If False, the projected data uses a sparse representation if the input is sparse.</source>
          <target state="translated">如果False,如果输入是稀疏的,则投影数据使用稀疏表示。</target>
        </trans-unit>
        <trans-unit id="7bfec8f3204bdf713e2d3557ec53ea6f3960ad24" translate="yes" xml:space="preserve">
          <source>If False, there is no input validation.</source>
          <target state="translated">如果为False,则没有输入验证。</target>
        </trans-unit>
        <trans-unit id="a67320198a0b746d35fcc941198f1221ee73c87b" translate="yes" xml:space="preserve">
          <source>If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned.</source>
          <target state="translated">如果为False,则尽量避免复制,而是进行原地缩放。这并不能保证总是在原地缩放;例如,如果数据不是 NumPy 数组或 scipy.sparse CSR 矩阵,仍然可能返回一个副本。</target>
        </trans-unit>
        <trans-unit id="fcd08eda0bca1685e28de89ae046095006b92653" translate="yes" xml:space="preserve">
          <source>If None (default), load all the categories. If not None, list of category names to load (other categories ignored).</source>
          <target state="translated">如果为 &quot;无&quot;(默认),加载所有类别。如果不为 &quot;无&quot;,则加载类别名称列表(其他类别忽略)。</target>
        </trans-unit>
        <trans-unit id="a7cbd99fe721a3cd173045eddaf688b83e7620c1" translate="yes" xml:space="preserve">
          <source>If None the estimator&amp;rsquo;s default scorer, if available, is used.</source>
          <target state="translated">如果为None，则使用估算器的默认评分器（如果有）。</target>
        </trans-unit>
        <trans-unit id="96ad58db5ee1b903109c173fcab72b0955bb0408" translate="yes" xml:space="preserve">
          <source>If None, defaults to 1.0 / n_features</source>
          <target state="translated">如果无,则默认为1.0/n_features。</target>
        </trans-unit>
        <trans-unit id="e5ce9a9046a52014390758ba790166ae01779c1f" translate="yes" xml:space="preserve">
          <source>If None, do not try to decode the content of the files (e.g. for images or other non-text content). If not None, encoding to use to decode text files to Unicode if load_content is True.</source>
          <target state="translated">如果为 &quot;无&quot;,不要尝试对文件的内容进行解码(例如图像或其他非文本内容)。如果不是None,如果load_content为True,则使用编码将文本文件解码为Unicode。</target>
        </trans-unit>
        <trans-unit id="3e5e8d666168a7a15a80edb16364256ae0a379e4" translate="yes" xml:space="preserve">
          <source>If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms.</source>
          <target state="translated">max_df可以设置为一个范围[0.7,1.0)的值,以根据语料库内术语的文档频率自动检测和过滤停止词。</target>
        </trans-unit>
        <trans-unit id="02542a43a2f09f5328657402d69f49ce442cb6c2" translate="yes" xml:space="preserve">
          <source>If None, pairwise_distances_chunked returns a generator of vertical chunks of the distance matrix.</source>
          <target state="translated">如果为None,则pairwise_distances_chunked返回距离矩阵垂直分块的生成器。</target>
        </trans-unit>
        <trans-unit id="eb5d73cb83520641b0e2c815c109159135d569cc" translate="yes" xml:space="preserve">
          <source>If None, the estimator&amp;rsquo;s default scorer (if available) is used.</source>
          <target state="translated">如果为None，则使用估算器的默认计分器（如果有）。</target>
        </trans-unit>
        <trans-unit id="651c3653c15c90a6722a00465ba57c19c30adb9b" translate="yes" xml:space="preserve">
          <source>If None, the threshold is assumed to be half way between neg_label and pos_label.</source>
          <target state="translated">如果None,则假定阈值为neg_label和pos_label之间的一半。</target>
        </trans-unit>
        <trans-unit id="ac652d29bc285e4e46f5aaf2fe5415c63aee1f09" translate="yes" xml:space="preserve">
          <source>If None, then &lt;code&gt;max_features=n_features&lt;/code&gt;.</source>
          <target state="translated">如果为None，则 &lt;code&gt;max_features=n_features&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2f9e5ee96434f57529c2481b71d631d9dd0cb5e7" translate="yes" xml:space="preserve">
          <source>If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled.</source>
          <target state="translated">如果为真(默认),平方误差标准值除以n_features。如果为False,则不会重新调整平方误差标准值。</target>
        </trans-unit>
        <trans-unit id="da82574bb396bf8045c493d20398be74e4e9ef51" translate="yes" xml:space="preserve">
          <source>If True (default), then include a bias column, the feature in which all polynomial powers are zero (i.e. a column of ones - acts as an intercept term in a linear model).</source>
          <target state="translated">如果为True(默认),则包含一个偏置列,即所有多项式幂为零的特征(即一列1--作为线性模型中的截距项)。</target>
        </trans-unit>
        <trans-unit id="d150b2a4c21e929dfd726f6463d03ca9f005e91a" translate="yes" xml:space="preserve">
          <source>If True (default), transform will raise an error when there are features with missing values in transform that have no missing values in fit This is applicable only when &lt;code&gt;features=&quot;missing-only&quot;&lt;/code&gt;.</source>
          <target state="translated">如果为True（默认值），则当变换中的要素缺失值且拟合中没有缺失值时，变换将引发错误。仅当 &lt;code&gt;features=&quot;missing-only&quot;&lt;/code&gt; 时适用。</target>
        </trans-unit>
        <trans-unit id="d91ad850130f94be79fb668041bf3eecd01a29b0" translate="yes" xml:space="preserve">
          <source>If True and if X is sparse, the method also returns the intercept, and the solver is automatically changed to &amp;lsquo;sag&amp;rsquo;. This is only a temporary fix for fitting the intercept with sparse data. For dense data, use sklearn.linear_model._preprocess_data before your regression.</source>
          <target state="translated">如果为True且X为稀疏，则该方法还将返回截距，并且求解器会自动更改为'sag'。这只是将截距与稀疏数据拟合的临时解决方案。对于密集数据，请在回归之前使用sklearn.linear_model._preprocess_data。</target>
        </trans-unit>
        <trans-unit id="a68108e0ea5bf983075f127e077ee80517d6dfdd" translate="yes" xml:space="preserve">
          <source>If True the covariance matrices are computed and stored in the &lt;code&gt;self.covariance_&lt;/code&gt; attribute.</source>
          <target state="translated">如果为True，则计算协方差矩阵并将其存储在 &lt;code&gt;self.covariance_&lt;/code&gt; 属性中。</target>
        </trans-unit>
        <trans-unit id="5c5d5facc126a265032a533a4664c8926339ade0" translate="yes" xml:space="preserve">
          <source>If True the full path is stored in the &lt;code&gt;coef_path_&lt;/code&gt; attribute. If you compute the solution for a large problem or many targets, setting &lt;code&gt;fit_path&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; will lead to a speedup, especially with a small alpha.</source>
          <target state="translated">如果为True，则完整路径将存储在 &lt;code&gt;coef_path_&lt;/code&gt; 属性中。如果您针对一个大问题或许多目标计算解决方案， &lt;code&gt;fit_path&lt;/code&gt; 设置为 &lt;code&gt;False&lt;/code&gt; 将导致加速，尤其是对于较小的alpha而言。</target>
        </trans-unit>
        <trans-unit id="53e960778922c6ba257a9c66f18d0e260655f043" translate="yes" xml:space="preserve">
          <source>If True the function returns the pairwise distance matrix else it returns the componentwise L1 pairwise-distances. Not supported for sparse matrix inputs.</source>
          <target state="translated">如果为真,函数返回对偶距离矩阵,否则返回分量L1对偶距离。不支持稀疏矩阵输入。</target>
        </trans-unit>
        <trans-unit id="2546c89362b151bbba35dab463b810d1f7c0a359" translate="yes" xml:space="preserve">
          <source>If True the order of the dataset is shuffled to avoid having images of the same person grouped.</source>
          <target state="translated">如果为True,数据集的顺序会被洗牌,以避免同一人的图像被分组。</target>
        </trans-unit>
        <trans-unit id="618a67ac95fc4ccc3385ae319143bf344e1ffb63" translate="yes" xml:space="preserve">
          <source>If True then raise a warning if conversion is required.</source>
          <target state="translated">如果为True,则在需要转换时发出警告。</target>
        </trans-unit>
        <trans-unit id="19362eed638b2dc6d204e12092075aedd87e6e93" translate="yes" xml:space="preserve">
          <source>If True then raise an exception if array is not symmetric.</source>
          <target state="translated">如果为True,那么如果数组不是对称的,则引发异常。</target>
        </trans-unit>
        <trans-unit id="baf82faf959595f525e5d5a94c6b8526ad942779" translate="yes" xml:space="preserve">
          <source>If True, X will be copied; else, it may be overwritten.</source>
          <target state="translated">如果为True,X将被复制;否则,可能被覆盖。</target>
        </trans-unit>
        <trans-unit id="aa91f074d713faca021e35ddd5331805b9848f9e" translate="yes" xml:space="preserve">
          <source>If True, a copy of X will be created. If False, a copy may still be returned if X&amp;rsquo;s dtype is not a floating point type.</source>
          <target state="translated">如果为True，将创建X的副本。如果为False，则如果X的dtype不是浮点类型，则仍可能返回副本。</target>
        </trans-unit>
        <trans-unit id="054b374d036034be5d7258a6a975038eb8fec935" translate="yes" xml:space="preserve">
          <source>If True, a copy of X will be created. If False, imputation will be done in-place whenever possible. Note that, in the following cases, a new copy will always be made, even if &lt;code&gt;copy=False&lt;/code&gt;:</source>
          <target state="translated">如果为True，将创建X的副本。如果为False，则插补将在任何可能的地方进行。请注意，在以下情况下，即使 &lt;code&gt;copy=False&lt;/code&gt; ，也将始终创建一个新副本：</target>
        </trans-unit>
        <trans-unit id="f237ade75e04520befb53fc36267d58be41dc5ae" translate="yes" xml:space="preserve">
          <source>If True, a persistent copy of the training data is stored in the object. Otherwise, just a reference to the training data is stored, which might cause predictions to change if the data is modified externally.</source>
          <target state="translated">如果为真,则在对象中存储训练数据的持久化副本。否则,只是存储了一个对训练数据的引用,如果从外部修改数据,可能会导致预测结果改变。</target>
        </trans-unit>
        <trans-unit id="5b4ca3bdeb594ab34289df8cfc7fa70c9919ad95" translate="yes" xml:space="preserve">
          <source>If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.</source>
          <target state="translated">如果True,则所有非零计数都设置为1,这对于模拟二进制事件而不是整数计数的离散概率模型非常有用。</target>
        </trans-unit>
        <trans-unit id="0beec2b2d6b910456e155063f83ec1e59c6c0df1" translate="yes" xml:space="preserve">
          <source>If True, all non-zero term counts are set to 1. This does not mean outputs will have only 0/1 values, only that the tf term in tf-idf is binary. (Set idf and normalization to False to get 0/1 outputs.)</source>
          <target state="translated">如果为True,所有非零的项数都被设置为1,这并不意味着输出只有0/1的值,只是tf-idf中的tf项是二进制的。(将idf和归一化设置为False,得到0/1的输出。)</target>
        </trans-unit>
        <trans-unit id="b69242de00e60526a79082b37846a2a724d7b3dd" translate="yes" xml:space="preserve">
          <source>If True, center the data before scaling.</source>
          <target state="translated">如果为真,则在缩放前将数据居中。</target>
        </trans-unit>
        <trans-unit id="6c9a4d2da449884c97015be12ce056a53dc04757" translate="yes" xml:space="preserve">
          <source>If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.</source>
          <target state="translated">如果为True,在缩放前将数据居中。这在稀疏矩阵上是行不通的(并且会引发一个异常),因为将数据居中需要建立一个密集矩阵,而这个密集矩阵在一般的使用情况下可能会太大而无法放入内存。</target>
        </trans-unit>
        <trans-unit id="9d7135e468914be214009091b1fc11f6721afd0a" translate="yes" xml:space="preserve">
          <source>If True, center the data before scaling. This will cause &lt;code&gt;transform&lt;/code&gt; to raise an exception when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.</source>
          <target state="translated">如果为True，则在缩放之前将数据居中。尝试使用稀疏矩阵时，这将导致 &lt;code&gt;transform&lt;/code&gt; 引发异常，因为将它们居中需要构建密集的矩阵，在常见的用例中，该矩阵可能太大而无法容纳在内存中。</target>
        </trans-unit>
        <trans-unit id="20839df17b3bca6b55533337f17b7c17f0881d1a" translate="yes" xml:space="preserve">
          <source>If True, compute the objective function at each step of the model. Default is False</source>
          <target state="translated">如果为真,在模型的每一步计算目标函数。默认为False</target>
        </trans-unit>
        <trans-unit id="afb80999f635ad0619301e31bb4cd6b353188af0" translate="yes" xml:space="preserve">
          <source>If True, compute the objective function at each step of the model. Default is False.</source>
          <target state="translated">如果为真,在模型的每一步计算目标函数。默认为False。</target>
        </trans-unit>
        <trans-unit id="9df36ef1b24eb49a93a9d932c395b3324554689c" translate="yes" xml:space="preserve">
          <source>If True, data are not centered before computation. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, data are centered before computation.</source>
          <target state="translated">如果为真,计算前数据不居中。用于处理平均数显著等于零但不完全为零的数据。如果为假,数据在计算前居中。</target>
        </trans-unit>
        <trans-unit id="cdd266c276f30f1ed8fec5e418bed1790d829f9f" translate="yes" xml:space="preserve">
          <source>If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data are centered before computation.</source>
          <target state="translated">如果为真,数据在计算前不居中。在处理平均数几乎为零但不完全为零的数据时很有用。如果为False(默认),数据在计算前居中。</target>
        </trans-unit>
        <trans-unit id="b4e1adfc1374b7a62eee31a2ac4be08eea958149" translate="yes" xml:space="preserve">
          <source>If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data are centered before computation.</source>
          <target state="translated">如果为真,数据在计算前不居中。在处理平均数几乎为零但不完全为零的数据时很有用。如果为假,数据在计算前居中。</target>
        </trans-unit>
        <trans-unit id="a2902b07926590508dee697d1e97c541f35cd531" translate="yes" xml:space="preserve">
          <source>If True, ensure that the output of the random projection is a dense numpy array even if the input and random projection matrix are both sparse. In practice, if the number of components is small the number of zero components in the projected data will be very small and it will be more CPU and memory efficient to use a dense representation.</source>
          <target state="translated">如果为True,即使输入和随机投影矩阵都是稀疏的,也要保证随机投影的输出是一个密集的numpy数组。在实际应用中,如果分量较少,投影数据中零分量的数量会非常少,使用密实的表示方式会更节省CPU和内存。</target>
        </trans-unit>
        <trans-unit id="a3946c2202800dbed0f15da39a81b563f2054e41" translate="yes" xml:space="preserve">
          <source>If True, individual trees are fit on random subsets of the training data sampled with replacement. If False, sampling without replacement is performed.</source>
          <target state="translated">如果为真,则在训练数据的随机子集上对各个树进行替换采样拟合。如果为 &quot;False&quot;,则执行无替换采样。</target>
        </trans-unit>
        <trans-unit id="47a8f3ebe1bf3e97122b0404e375e9c09f1cef3f" translate="yes" xml:space="preserve">
          <source>If True, input X is copied and stored by the model in the &lt;code&gt;X_fit_&lt;/code&gt; attribute. If no further changes will be done to X, setting &lt;code&gt;copy_X=False&lt;/code&gt; saves memory by storing a reference.</source>
          <target state="translated">如果为True，则模型将输入X复制并存储在 &lt;code&gt;X_fit_&lt;/code&gt; 属性中。如果对X不再做任何更改，则将 &lt;code&gt;copy_X=False&lt;/code&gt; 设置为通过存储引用来节省内存。</target>
        </trans-unit>
        <trans-unit id="10ab7d9c4ef9751f0861c3cfbcd363da08ee1196" translate="yes" xml:space="preserve">
          <source>If True, return a sparse CSR continency matrix. If &lt;code&gt;eps is not None&lt;/code&gt;, and &lt;code&gt;sparse is True&lt;/code&gt;, will throw ValueError.</source>
          <target state="translated">如果为True，则返回一个稀疏的CSR连续性矩阵。如果 &lt;code&gt;eps is not None&lt;/code&gt; ，而 &lt;code&gt;sparse is True&lt;/code&gt; ，将抛出ValueError。</target>
        </trans-unit>
        <trans-unit id="bbadcb21277fb2fd7500e5e018984adc0515f847" translate="yes" xml:space="preserve">
          <source>If True, return output as dict</source>
          <target state="translated">如果为True,返回dict的输出</target>
        </trans-unit>
        <trans-unit id="4c0f661b8fac7f22363a5a1e55327c6967bb56d8" translate="yes" xml:space="preserve">
          <source>If True, return the average score across folds, weighted by the number of samples in each test set. In this case, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds. If False, return the average score across folds. Default is True, but will change to False in version 0.21, to correspond to the standard definition of cross-validation.</source>
          <target state="translated">如果为真,返回各褶皱的平均得分,按每个测试集的样本数加权。在这种情况下,假设数据在各个褶皱中是完全相同的分布,最小化的损失是每个样本的总损失,而不是各个褶皱的平均损失。如果为False,则返回各褶皱的平均分数。默认为True,但在0.21版本中将改为False,以符合交叉验证的标准定义。</target>
        </trans-unit>
        <trans-unit id="cd7031ac688b02e25258c5831c7d3ea164486db6" translate="yes" xml:space="preserve">
          <source>If True, return the distance between the clusters.</source>
          <target state="translated">如果为True,返回簇之间的距离。</target>
        </trans-unit>
        <trans-unit id="48eeb388f7c432fd1b00c136703cc691939b77aa" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(data, target)&lt;/code&gt; instead of a Bunch object. See below for more information about the &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; object.</source>
          <target state="translated">如果为True，则返回 &lt;code&gt;(data, target)&lt;/code&gt; 而不是Bunch对象。有关 &lt;code&gt;data&lt;/code&gt; 和 &lt;code&gt;target&lt;/code&gt; 对象的更多信息，请参见下文。</target>
        </trans-unit>
        <trans-unit id="b8ef976b3bf0a8c5fe0d328bf69ca77595314915" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(data, target)&lt;/code&gt; instead of a Bunch object. See below for more information about the &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; objects.</source>
          <target state="translated">如果为True，则返回 &lt;code&gt;(data, target)&lt;/code&gt; 而不是Bunch对象。有关 &lt;code&gt;data&lt;/code&gt; 和 &lt;code&gt;target&lt;/code&gt; 对象的更多信息，请参见下文。</target>
        </trans-unit>
        <trans-unit id="d467c22a2bd71ab649cbad26364f06a31ce58ac1" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(data.data, data.target)&lt;/code&gt; instead of a Bunch object.</source>
          <target state="translated">如果为True，则返回 &lt;code&gt;(data.data, data.target)&lt;/code&gt; 而不是Bunch对象。</target>
        </trans-unit>
        <trans-unit id="248b6d0d481e47f352d5f3203242e6800072c658" translate="yes" xml:space="preserve">
          <source>If True, returns &lt;code&gt;(dataset.data, dataset.target)&lt;/code&gt; instead of a Bunch object. See below for more information about the &lt;code&gt;dataset.data&lt;/code&gt; and &lt;code&gt;dataset.target&lt;/code&gt; object.</source>
          <target state="translated">如果为True，则返回 &lt;code&gt;(dataset.data, dataset.target)&lt;/code&gt; 而不是Bunch对象。请参阅下文，以获取有关 &lt;code&gt;dataset.data&lt;/code&gt; 和 &lt;code&gt;dataset.target&lt;/code&gt; 对象的更多信息。</target>
        </trans-unit>
        <trans-unit id="c220af25fde2fda1e9985d78b63166a331905d63" translate="yes" xml:space="preserve">
          <source>If True, scale the data to interquartile range.</source>
          <target state="translated">如果为 &quot;True&quot;,将数据放大到四分位数范围。</target>
        </trans-unit>
        <trans-unit id="0f395f8257fe0bdfb8a823c88f3be9843583f8a6" translate="yes" xml:space="preserve">
          <source>If True, scale the data to unit variance (or equivalently, unit standard deviation).</source>
          <target state="translated">如果为 &quot;True&quot;,则将数据缩放为单位方差(或等价的单位标准差)。</target>
        </trans-unit>
        <trans-unit id="a491ca8d8797fa01293c195b8787d725c30e073a" translate="yes" xml:space="preserve">
          <source>If True, the clusters are put on the vertices of a hypercube. If False, the clusters are put on the vertices of a random polytope.</source>
          <target state="translated">如果为真,簇被放在超立方体的顶点上,如果为假,簇被放在随机多角体的顶点上。如果为 &quot;False&quot;,则聚类被放在随机多角体的顶点上。</target>
        </trans-unit>
        <trans-unit id="dc426ca785aa82bc3726faf833e38673875ff1ab" translate="yes" xml:space="preserve">
          <source>If True, the coefficients of the underlying linear model are returned.</source>
          <target state="translated">如果为True,则返回底层线性模型的系数。</target>
        </trans-unit>
        <trans-unit id="5724be8fac97575b0a1ba6783afc1c2fbe0fcac8" translate="yes" xml:space="preserve">
          <source>If True, the covariance of the joint predictive distribution at the query points is returned along with the mean</source>
          <target state="translated">如果为True,则返回查询点的联合预测分布的协方差和平均值。</target>
        </trans-unit>
        <trans-unit id="d28765388f526f443e8440713d9f120592d23759" translate="yes" xml:space="preserve">
          <source>If True, the gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta is returned additionally. If True, theta must not be None.</source>
          <target state="translated">如果为True,则额外返回theta位置上相对于核超参数的对数边际似然的梯度。如果为True,则theta不能为None。</target>
        </trans-unit>
        <trans-unit id="2ecaf6f4ca3e741011335b25b38b91313c972ea3" translate="yes" xml:space="preserve">
          <source>If True, the gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta is returned additionally. Note that gradient computation is not supported for non-binary classification. If True, theta must not be None.</source>
          <target state="translated">如果为True,则额外返回在位置theta处相对于内核超参数的对数边际似然的梯度。注意,梯度计算不支持非二进制分类。如果为True,theta不能为None。</target>
        </trans-unit>
        <trans-unit id="65ddb79c8d6a76beebeeb976d10455d7eb1fb46d" translate="yes" xml:space="preserve">
          <source>If True, the imputer mask will be a sparse matrix.</source>
          <target state="translated">如果为True,imputer mask将是一个稀疏矩阵。</target>
        </trans-unit>
        <trans-unit id="a134a28236267fd097c12ab6f4f7b85d957aa9ca" translate="yes" xml:space="preserve">
          <source>If True, the method also returns &lt;code&gt;n_iter&lt;/code&gt;, the actual number of iteration performed by the solver.</source>
          <target state="translated">如果为True，则该方法还返回 &lt;code&gt;n_iter&lt;/code&gt; ，即求解器执行的实际迭代次数。</target>
        </trans-unit>
        <trans-unit id="af50e40087f45610f2fbcc323b88ccd93dcf9324" translate="yes" xml:space="preserve">
          <source>If True, the regressors X will be normalized before regression. This parameter is ignored when &lt;code&gt;fit_intercept&lt;/code&gt; is set to False. When the regressors are normalized, note that this makes the hyperparameters learned more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; before calling &lt;code&gt;fit&lt;/code&gt; on an estimator with &lt;code&gt;normalize=False&lt;/code&gt;.</source>
          <target state="translated">如果为True，则在回归之前将对回归变量X进行归一化。当 &lt;code&gt;fit_intercept&lt;/code&gt; 设置为False 时，将忽略此参数。在对回归值进行归一化时，请注意，这会使学习到的超参数更健壮，并且几乎与样本数无关。相同的属性对于标准化数据无效。但是，如果您希望标准化，请在使用 &lt;code&gt;normalize=False&lt;/code&gt; 的估算器上调用 &lt;code&gt;fit&lt;/code&gt; 之前，使用 &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="016e8fefc2c3108a2b7a4351de86dada7ecbd68e" translate="yes" xml:space="preserve">
          <source>If True, the regressors X will be normalized before regression. This parameter is ignored when &lt;code&gt;fit_intercept&lt;/code&gt; is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; before calling &lt;code&gt;fit&lt;/code&gt; on an estimator with &lt;code&gt;normalize=False&lt;/code&gt;.</source>
          <target state="translated">如果为True，则在回归之前将对回归变量X进行归一化。当 &lt;code&gt;fit_intercept&lt;/code&gt; 设置为False 时，将忽略此参数。在对回归值进行归一化时，请注意，这会使学习到的超参数更健壮，并且几乎与样本数无关。相同的属性对于标准化数据无效。但是，如果您希望标准化，请在使用 &lt;code&gt;normalize=False&lt;/code&gt; 的估算器上调用 &lt;code&gt;fit&lt;/code&gt; 之前，使用 &lt;code&gt;preprocessing.StandardScaler&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c3fdb7e36f654834d666d698b8cb7219451a4481" translate="yes" xml:space="preserve">
          <source>If True, the return value will be an array of integers, rather than a boolean mask.</source>
          <target state="translated">如果为True,返回值将是一个整数数组,而不是一个布尔掩码。</target>
        </trans-unit>
        <trans-unit id="d17d4bbcdf2df4ef33c01a7114516c2e4e90d7d8" translate="yes" xml:space="preserve">
          <source>If True, the standard-deviation of the predictive distribution at the query points is returned along with the mean.</source>
          <target state="translated">如果为真,则返回查询点的预测分布的标准差和平均值。</target>
        </trans-unit>
        <trans-unit id="3f294130716f356d91654999eee9757bd2ba6c1c" translate="yes" xml:space="preserve">
          <source>If True, the support of robust location and covariance estimates is computed, and a covariance estimate is recomputed from it, without centering the data. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, the robust location and covariance are directly computed with the FastMCD algorithm without additional treatment.</source>
          <target state="translated">如果True,计算鲁棒位置和协方差估计的支持度,并从中重新计算协方差估计,而不将数据居中。对于平均数显著等于零但不完全为零的数据很有用。如果False,则直接用FastMCD算法计算鲁棒位置和协方差,而不做额外处理。</target>
        </trans-unit>
        <trans-unit id="667a36b1a1b94aa0d760aa610f277fcd1918ae0a" translate="yes" xml:space="preserve">
          <source>If True, the support of the robust location and the covariance estimates is computed, and a covariance estimate is recomputed from it, without centering the data. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, the robust location and covariance are directly computed with the FastMCD algorithm without additional treatment.</source>
          <target state="translated">如果为True,则计算稳健位置和协方差估计的支持度,并从中重新计算协方差估计,而不将数据居中。对于处理均值显著等于零但不完全为零的数据很有用。如果False,则直接用FastMCD算法计算鲁棒位置和协方差,而不做额外处理。</target>
        </trans-unit>
        <trans-unit id="28346b69fb6f1a64d688b2ef42aabb524b6563d8" translate="yes" xml:space="preserve">
          <source>If True, then X will be converted to a 2-dimensional NumPy array or sparse matrix. If the conversion is not possible an exception is raised.</source>
          <target state="translated">如果为True,那么X将被转换为一个二维的NumPy数组或稀疏矩阵。如果转换不可能,则会引发异常。</target>
        </trans-unit>
        <trans-unit id="5c0fef9c1e6748fc4d3cb84e62459147a039a4fd" translate="yes" xml:space="preserve">
          <source>If True, then all components with zero eigenvalues are removed, so that the number of components in the output may be &amp;lt; n_components (and sometimes even zero due to numerical instability). When n_components is None, this parameter is ignored and components with zero eigenvalues are removed regardless.</source>
          <target state="translated">如果为True，则将删除所有具有零特征值的分量，以使输出中的分量数可能小于n_components（有时由于数值不稳定性甚至为零）。当n_components为None时，将忽略此参数，并删除特征值为零的组件。</target>
        </trans-unit>
        <trans-unit id="42874c4e7adb064c98a0fb44835161462f985220" translate="yes" xml:space="preserve">
          <source>If True, then compute normalized Laplacian.</source>
          <target state="translated">如果为True,则计算归一化的拉普拉斯系数。</target>
        </trans-unit>
        <trans-unit id="7818bd11ce52f2496690f2a19701e1fdc3bace9d" translate="yes" xml:space="preserve">
          <source>If True, transpose the downloaded data array.</source>
          <target state="translated">如果为True,则对下载的数据阵列进行转置。</target>
        </trans-unit>
        <trans-unit id="09f8cdcb36703e0413a45867eb062d48bc42e4a4" translate="yes" xml:space="preserve">
          <source>If True, validation for finiteness will be skipped, saving time, but leading to potential crashes. If False, validation for finiteness will be performed, avoiding error. Global default: False.</source>
          <target state="translated">如果为真,将跳过有限性验证,节省时间,但会导致潜在的崩溃。如果为False,则会执行有限性验证,避免错误。全局默认。False.</target>
        </trans-unit>
        <trans-unit id="f770d204acb3934762188e63b6bd0977cfe619aa" translate="yes" xml:space="preserve">
          <source>If True, will return the parameters for this estimator and contained subobjects that are estimators.</source>
          <target state="translated">如果为True,将返回这个估计器和包含的子对象的估计器的参数。</target>
        </trans-unit>
        <trans-unit id="edc518974aa9f8ea115d0f36469bc2b1e2cd15a2" translate="yes" xml:space="preserve">
          <source>If True, will return the query_id array for each file.</source>
          <target state="translated">如果为True,将返回每个文件的query_id数组。</target>
        </trans-unit>
        <trans-unit id="80fdba1026cc980884a84dfa72ad1747c034afc6" translate="yes" xml:space="preserve">
          <source>If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied.</source>
          <target state="translated">如果X和y不是C-ordered且连续的np.float64数组,并且X不是scipy.sparse.csr_matrix,那么X和/或y可以被复制。</target>
        </trans-unit>
        <trans-unit id="a601440183ce5a872479c357e441563196aab652" translate="yes" xml:space="preserve">
          <source>If X is a dense array, then the other methods will not support sparse matrices as input.</source>
          <target state="translated">如果X是一个密集数组,那么其他方法将不支持稀疏矩阵作为输入。</target>
        </trans-unit>
        <trans-unit id="efab9063b47a2afb85758f067069188b21b34f3e" translate="yes" xml:space="preserve">
          <source>If X is encoded as a CSR matrix.</source>
          <target state="translated">如果X被编码为CSR矩阵。</target>
        </trans-unit>
        <trans-unit id="da96e9fabf18fc39905756390120e4a7a1e09f97" translate="yes" xml:space="preserve">
          <source>If X is not a C-ordered contiguous array it is copied.</source>
          <target state="translated">如果X不是一个C顺序的连续数组,就会被复制。</target>
        </trans-unit>
        <trans-unit id="9cc8f34afbd30e04cc65d91f1b233abc1c382996" translate="yes" xml:space="preserve">
          <source>If X is not an array of floating values;</source>
          <target state="translated">如果X不是一个浮动值数组。</target>
        </trans-unit>
        <trans-unit id="e7ca7ce1d419c3d60265041304210343f2e8b91d" translate="yes" xml:space="preserve">
          <source>If X is our multivariate data, then the problem that we are trying to solve is to rewrite it on a different observational basis: we want to learn loadings L and a set of components C such that &lt;em&gt;X = L C&lt;/em&gt;. Different criteria exist to choose the components</source>
          <target state="translated">如果X是我们的多元数据，那么我们试图解决的问题是在不同的观察基础上重写它：我们想学习载荷L和一组分量C，使得&lt;em&gt;X = LC&lt;/em&gt;。存在选择组件的不同标准</target>
        </trans-unit>
        <trans-unit id="4691b6eeb6f44a63af5f24ac30dc066ab023aa61" translate="yes" xml:space="preserve">
          <source>If X is sparse and &lt;code&gt;missing_values=0&lt;/code&gt;;</source>
          <target state="translated">如果X稀疏并且 &lt;code&gt;missing_values=0&lt;/code&gt; ;</target>
        </trans-unit>
        <trans-unit id="5bf131136f9283115170d3f032466f07678834a5" translate="yes" xml:space="preserve">
          <source>If Y is given (default is None), then the returned matrix is the pairwise distance between the arrays from both X and Y.</source>
          <target state="translated">如果给定Y(默认为None),那么返回的矩阵是X和Y两个数组之间的对距离。</target>
        </trans-unit>
        <trans-unit id="a6e7fe3e345be28d5e67984fa49ad01aeaa444dd" translate="yes" xml:space="preserve">
          <source>If Y is given (default is None), then the returned matrix is the pairwise kernel between the arrays from both X and Y.</source>
          <target state="translated">如果给定Y(默认为None),那么返回的矩阵是X和Y两个数组之间的对核。</target>
        </trans-unit>
        <trans-unit id="15e0fd61e3b85d8ebad2fc2135f6d5822723ce41" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}\) is the estimated target output, \(y\) the corresponding (correct) target output, and \(Var\) is &lt;a href=&quot;https://en.wikipedia.org/wiki/Variance&quot;&gt;Variance&lt;/a&gt;, the square of the standard deviation, then the explained variance is estimated as follow:</source>
          <target state="translated">如果\（\ hat {y} \）是估计的目标输出，\（y \）是相应的（正确）目标输出，并且\（Var \）是&lt;a href=&quot;https://en.wikipedia.org/wiki/Variance&quot;&gt;Variance&lt;/a&gt;，即标准偏差的平方，则解释的方差为估计如下：</target>
        </trans-unit>
        <trans-unit id="c8fb389b8a60a2b57fc22c9161412c484a73dd18" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the 0-1 loss \(L_{0-1}\) is defined as:</source>
          <target state="translated">如果/(hat{y}_i/)是第i个样本的预测值,而/(y_i/)是相应的真实值,那么0-1损失/(L_{0-1}/)定义为:。</target>
        </trans-unit>
        <trans-unit id="01fda7f04ce93ad5d6b5843c80c53ee91e04866d" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the fraction of correct predictions over \(n_\text{samples}\) is defined as</source>
          <target state="translated">如果(hat{y}_i)是第1个样本的预测值,而(y_i)是相应的真实值,那么正确预测的分数对(n_text{样本})的定义为:1.</target>
        </trans-unit>
        <trans-unit id="af46aeec0a0c654990b43c84ec26ca8a3817bbd3" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the median absolute error (MedAE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">如果(hat{y}_i)是第i个样本的预测值,而(y_i)是相应的真实值,那么对(n_{text{samples}})估计的绝对误差中位数(MedAE)定义为</target>
        </trans-unit>
        <trans-unit id="27ab05c62dcfc0b1ca98228a2c106c2bd25d72f6" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample and \(y_i\) is the corresponding true value, then the score R&amp;sup2; estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">如果\（\ hat {y} _i \）是第\（i \）个样本的预测值，而\（y_i \）是相应的真实值，则在\（n _ {\ text {样品}} \）定义为</target>
        </trans-unit>
        <trans-unit id="e9be139031431f33624d9549cf24272bbec27cad" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean absolute error (MAE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">如果(hat{y}_i)是第1个样本的预测值,而(y_i)是相应的真实值,那么对(n_{文本{样本})估计的平均绝对误差(MAE)被定义为</target>
        </trans-unit>
        <trans-unit id="b7836e2114e345225a74c22cbe0d3b5d52c8f253" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean squared error (MSE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">如果(hat{y}_i)是第i个样本的预测值,而(y_i)是相应的真实值,那么对(n_{样本})估计的均方误差(MSE)定义如下</target>
        </trans-unit>
        <trans-unit id="1cf47fc0a0aaaffd1c0a818b8704218b8ae722c1" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample, and \(y_i\) is the corresponding true value, then the mean squared logarithmic error (MSLE) estimated over \(n_{\text{samples}}\) is defined as</source>
          <target state="translated">如果(hat{y}_i)是第i个样本的预测值,而(y_i)是相应的真实值,那么对(n_{text{samples}})估计的均值平方对数误差(MSLE)定义如下</target>
        </trans-unit>
        <trans-unit id="f295fa8851d145ac326bc63b92809e2fbf3d1f72" translate="yes" xml:space="preserve">
          <source>If \(\hat{y}_j\) is the predicted value for the \(j\)-th label of a given sample, \(y_j\) is the corresponding true value, and \(n_\text{labels}\) is the number of classes or labels, then the Hamming loss \(L_{Hamming}\) between two samples is defined as:</source>
          <target state="translated">如果\(\hat{y}_j/)是给定样本的第1个标签的预测值,\(y_j/)是相应的真实值,\(n_text{labels}/)是类或标签的数量,那么两个样本之间的汉明损失(L_{Hamming}/)定义为。</target>
        </trans-unit>
        <trans-unit id="7fa4bf510f83c73a55e8dec038abaacf960351ca" translate="yes" xml:space="preserve">
          <source>If \(c_0 = 0\) the kernel is said to be homogeneous.</source>
          <target state="translated">如果(c_0=0),则说内核是同质的。</target>
        </trans-unit>
        <trans-unit id="c7d07701826b4f4c8efa455b46a49990993930f2" translate="yes" xml:space="preserve">
          <source>If \(h_i\) is given, the above equation automatically implies the following probabilistic interpretation:</source>
          <target state="translated">如果给定(h_i\),上述等式自动意味着以下的概率解释。</target>
        </trans-unit>
        <trans-unit id="9d9af8bc9b90a6fbf0d539b126efbd694bdaddf6" translate="yes" xml:space="preserve">
          <source>If \(y_i\) is the true value of the \(i\)-th sample, and \(w_i\) is the corresponding sample weight, then we adjust the sample weight to:</source>
          <target state="translated">如果(y_i\)是第i个样本的真实值,而(w_i\)是对应的样本权重,那么我们将样本权重调整为。</target>
        </trans-unit>
        <trans-unit id="4c76ddad0ef7a743f202685a0861880cbc2062e2" translate="yes" xml:space="preserve">
          <source>If \(y_w\) is the predicted decision for true label and \(y_t\) is the maximum of the predicted decisions for all other labels, where predicted decisions are output by decision function, then multiclass hinge loss is defined by:</source>
          <target state="translated">如果(y_w\)是真标签的预测决策,而(y_t\)是所有其他标签的预测决策的最大值,其中预测决策是由决策函数输出的,那么多类铰链损失的定义为:。</target>
        </trans-unit>
        <trans-unit id="b8371056060d53aef38082b274a12c6e92dd981b" translate="yes" xml:space="preserve">
          <source>If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by accept_sparse, accept_large_sparse will cause it to be accepted only if its indices are stored with a 32-bit dtype.</source>
          <target state="translated">如果提供了一个CSR、CSC、COO或BSR稀疏矩阵并被accept_sparse接受,那么accept_large_sparse将导致只有当它的索引以32位dtype存储时才会被接受。</target>
        </trans-unit>
        <trans-unit id="ca2f554a4272574081b19f205bd8db66223aa9f8" translate="yes" xml:space="preserve">
          <source>If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by accept_sparse, accept_large_sparse=False will cause it to be accepted only if its indices are stored with a 32-bit dtype.</source>
          <target state="translated">如果提供了一个CSR、CSC、COO或BSR稀疏矩阵并被accept_sparse接受,accept_large_sparse=False将导致只有当它的索引以32位dtype存储时才会被接受。</target>
        </trans-unit>
        <trans-unit id="b0090a224443cfea422c2167734e98ec705e71a5" translate="yes" xml:space="preserve">
          <source>If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.</source>
          <target state="translated">如果传递了一个callable,它被用来从原始的、未处理的输入中提取特征序列。</target>
        </trans-unit>
        <trans-unit id="dcd8eacb988e0fa27afa1a7692943530b07747f6" translate="yes" xml:space="preserve">
          <source>If a callable is passed, it should take arguments X, k and and a random state and return an initialization.</source>
          <target state="translated">如果传递一个callable,它应该接受参数X、k和一个随机状态,并返回一个初始化。</target>
        </trans-unit>
        <trans-unit id="15b5ddf5d35e7b6d7436896e31247316b726ba30" translate="yes" xml:space="preserve">
          <source>If a float, that value is added to all values in the contingency matrix. This helps to stop NaN propagation. If &lt;code&gt;None&lt;/code&gt;, nothing is adjusted.</source>
          <target state="translated">如果是浮点型，则将该值添加到列联矩阵中的所有值。这有助于阻止NaN传播。如果为 &lt;code&gt;None&lt;/code&gt; ，则不进行任何调整。</target>
        </trans-unit>
        <trans-unit id="aec58020de2b924f9656034ee47c95a7ace302db" translate="yes" xml:space="preserve">
          <source>If a list is passed it&amp;rsquo;s expected to be one of n_targets such arrays. The varying values of the coefficients along the path. It is not present if the &lt;code&gt;fit_path&lt;/code&gt; parameter is &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">如果传递了一个列表，则它应该是n_targets这样的数组之一。沿路径的系数的变化值。如果 &lt;code&gt;fit_path&lt;/code&gt; 参数为 &lt;code&gt;False&lt;/code&gt; 则不存在。</target>
        </trans-unit>
        <trans-unit id="8e3b6cd9422926a607fefd39c3e9bd3020c06d14" translate="yes" xml:space="preserve">
          <source>If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if &lt;code&gt;analyzer == 'word'&lt;/code&gt;.</source>
          <target state="translated">如果是列表，则假定该列表包含停用词，所有停用词将从结果标记中删除。仅在 &lt;code&gt;analyzer == 'word'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="58d1b02436a7aa9160e580f582400827e1ad046d" translate="yes" xml:space="preserve">
          <source>If a string, it is passed to _check_stop_list and the appropriate stop list is returned. &amp;lsquo;english&amp;rsquo; is currently the only supported string value. There are several known issues with &amp;lsquo;english&amp;rsquo; and you should consider an alternative (see &lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;Using stop words&lt;/a&gt;).</source>
          <target state="translated">如果是字符串，则将其传递到_check_stop_list并返回适当的停止列表。&amp;ldquo; english&amp;rdquo;是当前唯一受支持的字符串值。&amp;ldquo;英语&amp;rdquo;存在几个已知的问题，您应该考虑一种替代方法（请参阅&lt;a href=&quot;../feature_extraction#stop-words&quot;&gt;使用停用词&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="564b43bc82acf22a1de3b85bb28ac591ff97b4bf" translate="yes" xml:space="preserve">
          <source>If a string, this may be one of &amp;lsquo;nearest_neighbors&amp;rsquo;, &amp;lsquo;precomputed&amp;rsquo;, &amp;lsquo;rbf&amp;rsquo; or one of the kernels supported by &lt;code&gt;sklearn.metrics.pairwise_kernels&lt;/code&gt;.</source>
          <target state="translated">如果是字符串，则可以是'nearest_neighbors'，'precomputed'，'rbf'之一或 &lt;code&gt;sklearn.metrics.pairwise_kernels&lt;/code&gt; 支持的内核之一。</target>
        </trans-unit>
        <trans-unit id="675cbfc234c52633edd36cba3388cd72c1b8e2d3" translate="yes" xml:space="preserve">
          <source>If a target is a classification outcome taking on values 0,1,&amp;hellip;,K-1, for node \(m\), representing a region \(R_m\) with \(N_m\) observations, let</source>
          <target state="translated">如果目标是分类结点，对节点\（m \）取值0,1，&amp;hellip;，K-1，表示具有\（N_m \）个观测值的区域\（R_m \），则令</target>
        </trans-unit>
        <trans-unit id="373500a68bbbd934744d157d24ba37240f790a20" translate="yes" xml:space="preserve">
          <source>If affinity is &amp;ldquo;precomputed&amp;rdquo; X : array-like, shape (n_samples, n_samples), Interpret X as precomputed adjacency graph computed from samples.</source>
          <target state="translated">如果亲和力是&amp;ldquo;预先计算的&amp;rdquo; X：类似数组的形状（n_samples，n_samples），则将X解释为根据样本计算得出的预先计算的邻接图。</target>
        </trans-unit>
        <trans-unit id="c306493486d7abaed871db69a1b0f3a0d3e230f4" translate="yes" xml:space="preserve">
          <source>If affinity is the adjacency matrix of a graph, this method can be used to find normalized graph cuts.</source>
          <target state="translated">如果亲和力是一个图形的邻接矩阵,那么这个方法可以用来寻找归一化图形切割。</target>
        </trans-unit>
        <trans-unit id="fef3ba1186af33eef8e244a6a4bb530d43ffdf84" translate="yes" xml:space="preserve">
          <source>If all examples are from the same class, it uses a one-class SVM.</source>
          <target state="translated">如果所有的例子都是来自同一个类,它使用的是一个类的SVM。</target>
        </trans-unit>
        <trans-unit id="b53805960d76925767243aca9c19243fc1b08d06" translate="yes" xml:space="preserve">
          <source>If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for continuous parameters.</source>
          <target state="translated">如果所有的参数都以列表的形式呈现,则进行无替换抽样。如果至少有一个参数是以分布形式给出的,则使用替换抽样。强烈建议对连续参数使用连续分布。</target>
        </trans-unit>
        <trans-unit id="4f219d1953670922fbbfe88159427df7222c9397" translate="yes" xml:space="preserve">
          <source>If an algorithm, such as a linear support vector machine or PCA, relies only on the scalar product of data points \(x_i\), one may use the value of \(k(x_i, x_j)\), which corresponds to applying the algorithm to the mapped data points \(\phi(x_i)\). The advantage of using \(k\) is that the mapping \(\phi\) never has to be calculated explicitly, allowing for arbitrary large features (even infinite).</source>
          <target state="translated">如果一个算法,如线性支持向量机或PCA,只依赖于数据点的标量乘积(x_i/),那么我们可以使用(k(x_i,x_j)\)的值,这相当于将算法应用于映射的数据点(\phi(x_i)\)。使用/(k/)的好处是映射/(\phi/)永远不需要明确计算,允许任意大的特征(甚至是无限的)。</target>
        </trans-unit>
        <trans-unit id="9fea95f95d0577b3d2b8dde1c99a4f5f11240b1d" translate="yes" xml:space="preserve">
          <source>If an exception is triggered, use &lt;code&gt;%debug&lt;/code&gt; to fire-up a post mortem ipdb session.</source>
          <target state="translated">如果触发了异常，请使用 &lt;code&gt;%debug&lt;/code&gt; 启动事后ipdb会话。</target>
        </trans-unit>
        <trans-unit id="5c4a826b768bf0ea44b0de0cf9a279c59410da1d" translate="yes" xml:space="preserve">
          <source>If an integer is given, it fixes the number of points on the grids of alpha to be used. If a list is given, it gives the grid to be used. See the notes in the class docstring for more details.</source>
          <target state="translated">如果给定一个整数,它固定了要使用的alpha网格上的点数。如果给定一个列表,它给出了要使用的网格。更多细节请参见类 docstring 中的注释。</target>
        </trans-unit>
        <trans-unit id="3f54c86c80b29ba93fdb4405121f2a742f42412e" translate="yes" xml:space="preserve">
          <source>If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.</source>
          <target state="translated">如果传递一个ndarray,它应该是形状(n_clusters,n_features)并给出初始中心。</target>
        </trans-unit>
        <trans-unit id="7bf20b6ab9e24e0313be1f29e5bd87350c62fc72" translate="yes" xml:space="preserve">
          <source>If bandwidth is not given, it is determined using a heuristic based on the median of all pairwise distances. This will take quadratic time in the number of samples. The sklearn.cluster.estimate_bandwidth function can be used to do this more efficiently.</source>
          <target state="translated">如果没有给定带宽,则使用基于所有对偶距离中值的启发式方法来确定。这将耗费样本数的二次方时间。可以使用sklearn.cluster.eestimate_bandwidth函数来更有效地完成这一工作。</target>
        </trans-unit>
        <trans-unit id="307d133eac653119e68c3f800803dcc4776573b9" translate="yes" xml:space="preserve">
          <source>If bool, then determines whether to consider all features discrete or continuous. If array, then it should be either a boolean mask with shape (n_features,) or array with indices of discrete features. If &amp;lsquo;auto&amp;rsquo;, it is assigned to False for dense &lt;code&gt;X&lt;/code&gt; and to True for sparse &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="translated">如果是布尔值，则确定是考虑所有特征是离散特征还是连续特征。如果是数组，则它应该是具有形状（n_features）的布尔蒙版，或者是具有离散特征索引的数组。如果为'auto'，则对于密集 &lt;code&gt;X&lt;/code&gt; ，将其分配为False；对于稀疏 &lt;code&gt;X&lt;/code&gt; ，将其分配为True 。</target>
        </trans-unit>
        <trans-unit id="14d26c2cb6ccf4f84a440b5eee94360749d30490" translate="yes" xml:space="preserve">
          <source>If boolean, whether or not to fit the isotonic regression with y increasing or decreasing.</source>
          <target state="translated">如果是布尔值,是否拟合y递增或递减的同调回归。</target>
        </trans-unit>
        <trans-unit id="c7380002883f78b7443a4cf144369e2cdf9c7dd5" translate="yes" xml:space="preserve">
          <source>If bytes or files are given to analyze, this encoding is used to decode.</source>
          <target state="translated">如果给定字节或文件进行分析,则使用此编码进行解码。</target>
        </trans-unit>
        <trans-unit id="b1cd46fc8b5b18d3d8ec258c92a5832fe49ecb69" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally in-complete, hence the AMI is null:</source>
          <target state="translated">如果班级成员完全被分割在不同的群组中,则赋值完全不完整,因此AMI为空。</target>
        </trans-unit>
        <trans-unit id="e6f2dbc2c288fdff952bc5d6d0612fad044f50c2" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally in-complete, hence the NMI is null:</source>
          <target state="translated">如果班级成员完全被分割在不同的群组中,则赋值完全不完整,因此NMI为空。</target>
        </trans-unit>
        <trans-unit id="60b93fba5d2befe30dad173ef2989a32caf2707d" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally incomplete, hence the ARI is very low:</source>
          <target state="translated">如果班级成员完全被分割在不同的群组中,那么分配是完全不完整的,因此ARI很低。</target>
        </trans-unit>
        <trans-unit id="e02bb35b2a969fdf2ff25b865a0b3ba938fb92a9" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally incomplete, hence the V-Measure is null:</source>
          <target state="translated">如果班级成员完全被分割在不同的簇中,那么分配是完全不完整的,因此V-Measure是空的。</target>
        </trans-unit>
        <trans-unit id="d0974a75f074fb11fd0a08f495fdb803227dd0c6" translate="yes" xml:space="preserve">
          <source>If classes members are completely split across different clusters, the assignment is totally random, hence the FMI is null:</source>
          <target state="translated">如果班级成员在不同的簇中完全分裂,那么分配是完全随机的,因此FMI为空。</target>
        </trans-unit>
        <trans-unit id="3a4c44f6cadbe5141305e79bc3f8068062652c4d" translate="yes" xml:space="preserve">
          <source>If classes members are split across different clusters, the assignment cannot be complete:</source>
          <target state="translated">如果班级成员分散在不同的群组中,则无法完成任务。</target>
        </trans-unit>
        <trans-unit id="baf96abe9df5cd386826eafcd47454c9dcc36819" translate="yes" xml:space="preserve">
          <source>If copy is False, the affinity matrix is modified inplace by the algorithm, for memory efficiency</source>
          <target state="translated">如果copy为False,则算法会在原地修改亲和矩阵,以提高内存效率</target>
        </trans-unit>
        <trans-unit id="671e4e16873255449d2ba54f06975c272f73c34d" translate="yes" xml:space="preserve">
          <source>If density = &amp;lsquo;auto&amp;rsquo;, the value is set to the minimum density as recommended by Ping Li et al.: 1 / sqrt(n_features).</source>
          <target state="translated">如果density ='auto'，则按Ping Li等人的建议将值设置为最小密度：1 / sqrt（n_features）。</target>
        </trans-unit>
        <trans-unit id="391517cb3cfce3ac9c6c32b7ceac049807282afc" translate="yes" xml:space="preserve">
          <source>If documents are pre-tokenized by an external package, then store them in files (or strings) with the tokens separated by whitespace and pass &lt;code&gt;analyzer=str.split&lt;/code&gt;</source>
          <target state="translated">如果文档是由外部软件包预先加标记的，则将它们存储在文件（或字符串）中，其标记由空格分隔，并通过 &lt;code&gt;analyzer=str.split&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="80416b24b5f24b22b80d50d90aa942e77a2dadfc" translate="yes" xml:space="preserve">
          <source>If each row and each column belongs to exactly one bicluster, then rearranging the rows and columns of the data matrix reveals the biclusters on the diagonal. Here is an example of this structure where biclusters have higher average values than the other rows and columns:</source>
          <target state="translated">如果每行每列正好属于一个双簇,那么重新排列数据矩阵的行和列,就会发现对角线上的双簇。下面是这种结构的一个例子,双簇的平均值高于其他行和列。</target>
        </trans-unit>
        <trans-unit id="1db9c10662b6d49b6b84ca8b7b7ce2a9580afa10" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default (the parameter is unspecified), the value is set to 0.1. The default will change in version 0.21. It will remain 0.1 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">如果为float，则应在0.0到1.0之间，并且代表要包括在测试拆分中的数据集的比例。如果为int，则表示测试样本的绝对数量。如果为None，则将值设置为火车尺寸的补码。默认情况下（未指定参数），该值设置为0.1。默认值将在0.21版中更改。这将保持0.1只有 &lt;code&gt;train_size&lt;/code&gt; 是不确定的，否则将补充规定 &lt;code&gt;train_size&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="73ce93bb920d84bef49715afdf02f771938f8206" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.1. The default will change in version 0.21. It will remain 0.1 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">如果为float，则应在0.0到1.0之间，并且代表要包括在测试拆分中的数据集的比例。如果为int，则表示测试样本的绝对数量。如果为None，则将值设置为火车尺寸的补码。默认情况下，该值设置为0.1。默认值将在0.21版中更改。这将保持0.1只有 &lt;code&gt;train_size&lt;/code&gt; 是不确定的，否则将补充规定 &lt;code&gt;train_size&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="aa74ab3ce21513c4e7c83e9b91dad63582c835b8" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.2. The default will change in version 0.21. It will remain 0.2 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">如果为float，则应在0.0到1.0之间，并且代表要包括在测试拆分中的数据集的比例。如果为int，则表示测试样本的绝对数量。如果为None，则将值设置为火车尺寸的补码。默认情况下，该值设置为0.2。默认值将在0.21版中更改。这将保持0.2只有 &lt;code&gt;train_size&lt;/code&gt; 是不确定的，否则将补充规定 &lt;code&gt;train_size&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="dfe0bc4ba0825c6b9e54b3177711e714d6e28a2b" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.25. The default will change in version 0.21. It will remain 0.25 only if &lt;code&gt;train_size&lt;/code&gt; is unspecified, otherwise it will complement the specified &lt;code&gt;train_size&lt;/code&gt;.</source>
          <target state="translated">如果为float，则应在0.0到1.0之间，并且代表要包括在测试拆分中的数据集的比例。如果为int，则表示测试样本的绝对数量。如果为None，则将值设置为火车尺寸的补码。默认情况下，该值设置为0.25。默认值将在0.21版中更改。它将保持0.25只有 &lt;code&gt;train_size&lt;/code&gt; 是不确定的，否则将补充规定 &lt;code&gt;train_size&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="a3b7da8f21403a8e0fce55a369b8d82b4da5bfd1" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.</source>
          <target state="translated">如果是float,应该在0.0和1.0之间,代表训练分割中要包含的数据集的比例。如果是int,代表训练样本的绝对数量。如果为None,则该值自动设置为测试大小的补数。</target>
        </trans-unit>
        <trans-unit id="62b47f7a89d7c976d2813299381cdc4f4f5b3cad" translate="yes" xml:space="preserve">
          <source>If float, should be between 0.0 and 1.0 and represent the proportion of the groups to include in the train split. If int, represents the absolute number of train groups. If None, the value is automatically set to the complement of the test size.</source>
          <target state="translated">如果是float,应该在0.0和1.0之间,代表列车分割中要包含的组数比例。如果是int,代表训练组的绝对数量。如果为None,则该值自动设置为测试大小的补数。</target>
        </trans-unit>
        <trans-unit id="b0ffbda1c59db43809cf9245f33efa45a65a5c05" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;max_features&lt;/code&gt; is a fraction and &lt;code&gt;int(max_features * n_features)&lt;/code&gt; features are considered at each split.</source>
          <target state="translated">如果为float，则 &lt;code&gt;max_features&lt;/code&gt; 是一个分数，并且在每个分割处均考虑 &lt;code&gt;int(max_features * n_features)&lt;/code&gt; 特征。</target>
        </trans-unit>
        <trans-unit id="47d0c1ebc9dc44a7018a0ce88d0d45201068f385" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_leaf&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; are the minimum number of samples for each node.</source>
          <target state="translated">如果为float，则 &lt;code&gt;min_samples_leaf&lt;/code&gt; 是一个分数，而 &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; 是每个节点的最小样本数。</target>
        </trans-unit>
        <trans-unit id="c217707834c84f95b745c6fd735e46ef1d5cb29d" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_leaf&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; is the minimum number of samples for each node.</source>
          <target state="translated">如果为float，则 &lt;code&gt;min_samples_leaf&lt;/code&gt; 是一个分数，而 &lt;code&gt;ceil(min_samples_leaf * n_samples)&lt;/code&gt; 是每个节点的最小样本数。</target>
        </trans-unit>
        <trans-unit id="f5813e9c1656f619ed6234eb86815e1c76ec9071" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_split&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; are the minimum number of samples for each split.</source>
          <target state="translated">如果为float，则 &lt;code&gt;min_samples_split&lt;/code&gt; 是一个分数，而 &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; 是每个拆分的最小样本数。</target>
        </trans-unit>
        <trans-unit id="c32957ea85344bbb6b41aa874b4a5e7763ff6b76" translate="yes" xml:space="preserve">
          <source>If float, then &lt;code&gt;min_samples_split&lt;/code&gt; is a fraction and &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; is the minimum number of samples for each split.</source>
          <target state="translated">如果为float，则 &lt;code&gt;min_samples_split&lt;/code&gt; 是一个分数，而 &lt;code&gt;ceil(min_samples_split * n_samples)&lt;/code&gt; 是每个拆分的最小样本数。</target>
        </trans-unit>
        <trans-unit id="81fe24c94c96599e85080c0cc195542bdb1ce722" translate="yes" xml:space="preserve">
          <source>If float, then draw &lt;code&gt;max_features * X.shape[1]&lt;/code&gt; features.</source>
          <target state="translated">如果为float，则绘制 &lt;code&gt;max_features * X.shape[1]&lt;/code&gt; 特征。</target>
        </trans-unit>
        <trans-unit id="a2b1676fcae8577852e20614ce418906e2f79102" translate="yes" xml:space="preserve">
          <source>If float, then draw &lt;code&gt;max_samples * X.shape[0]&lt;/code&gt; samples.</source>
          <target state="translated">如果为float，则绘制 &lt;code&gt;max_samples * X.shape[0]&lt;/code&gt; 样本。</target>
        </trans-unit>
        <trans-unit id="f234188a9694365b66e83538b40de1fa4059a074" translate="yes" xml:space="preserve">
          <source>If greater than or equal to 1, then &lt;code&gt;step&lt;/code&gt; corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then &lt;code&gt;step&lt;/code&gt; corresponds to the percentage (rounded down) of features to remove at each iteration.</source>
          <target state="translated">如果大于或等于1，则 &lt;code&gt;step&lt;/code&gt; 对应于每次迭代要删除的（整数）个特征。如果在（0.0，1.0）之内，则 &lt;code&gt;step&lt;/code&gt; 对应于每次迭代要删除的要素的百分比（向下舍入）。</target>
        </trans-unit>
        <trans-unit id="d3aeb6a39c457b69bdde12a943780461d08c388d" translate="yes" xml:space="preserve">
          <source>If greater than or equal to 1, then &lt;code&gt;step&lt;/code&gt; corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then &lt;code&gt;step&lt;/code&gt; corresponds to the percentage (rounded down) of features to remove at each iteration. Note that the last iteration may remove fewer than &lt;code&gt;step&lt;/code&gt; features in order to reach &lt;code&gt;min_features_to_select&lt;/code&gt;.</source>
          <target state="translated">如果大于或等于1，则 &lt;code&gt;step&lt;/code&gt; 对应于每次迭代要删除的（整数）个特征。如果在（0.0，1.0）之内，则 &lt;code&gt;step&lt;/code&gt; 对应于每次迭代要删除的要素的百分比（向下舍入）。请注意，为了达到 &lt;code&gt;min_features_to_select&lt;/code&gt; ，最后一次迭代可能会删除少于 &lt;code&gt;step&lt;/code&gt; 要素。</target>
        </trans-unit>
        <trans-unit id="1351e34960b08f78869e356e9d9129a529a17168" translate="yes" xml:space="preserve">
          <source>If in the QDA model one assumes that the covariance matrices are diagonal, then the inputs are assumed to be conditionally independent in each class, and the resulting classifier is equivalent to the Gaussian Naive Bayes classifier &lt;a href=&quot;generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt;&lt;code&gt;naive_bayes.GaussianNB&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">如果在QDA模型中假设协方差矩阵是对角线，则假定输入在每个类别中是有条件独立的，并且所得分类器等效于Gaussian Naive Bayes分类器&lt;a href=&quot;generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt; &lt;code&gt;naive_bayes.GaussianNB&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="47884bf7f490577d7025ceb970813cb997acfc82" translate="yes" xml:space="preserve">
          <source>If init=&amp;rsquo;custom&amp;rsquo;, it is used as initial guess for the solution.</source>
          <target state="translated">如果init ='custom'，则用作解决方案的初始猜测。</target>
        </trans-unit>
        <trans-unit id="5b9c788e52ff7adb618d2b8cf3dd396e088800c8" translate="yes" xml:space="preserve">
          <source>If int, it is the total number of points equally divided among clusters. If array-like, each element of the sequence indicates the number of samples per cluster.</source>
          <target state="translated">如果是int,则是各簇平分的总点数。如果是类数组,序列的每个元素表示每个簇的样本数。</target>
        </trans-unit>
        <trans-unit id="2403d0bd3d2ac8c8a9756c4cde9cd9202cbe9926" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="676c2454734bf9216c5797d643595f0b850b4e7a" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Note that different initializations might result in different local minima of the cost function.</source>
          <target state="translated">如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。注意，不同的初始化可能导致成本函数的局部最小值。</target>
        </trans-unit>
        <trans-unit id="a1933900181bd8e24f0237ebecc7a06b2ef8b486" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Only used when &lt;code&gt;svd_method&lt;/code&gt; equals &amp;lsquo;randomized&amp;rsquo;.</source>
          <target state="translated">如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。仅在 &lt;code&gt;svd_method&lt;/code&gt; 等于&amp;ldquo;随机化&amp;rdquo;时使用。</target>
        </trans-unit>
        <trans-unit id="4914440c8828885c0b1efea7679daefd5f1e4eaf" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;eigen_solver&lt;/code&gt; == &amp;lsquo;arpack&amp;rsquo;.</source>
          <target state="translated">如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。在 &lt;code&gt;eigen_solver&lt;/code&gt; =='arpack'时使用。</target>
        </trans-unit>
        <trans-unit id="90657492e3c46d11fb3a1c799a4569e4854211ed" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;shuffle&lt;/code&gt; == True.</source>
          <target state="translated">如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。在 &lt;code&gt;shuffle&lt;/code&gt; == True时使用。</target>
        </trans-unit>
        <trans-unit id="7e39c8ed74a38762200c178da772671eaa6b3f52" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;shuffle&lt;/code&gt; is True.</source>
          <target state="translated">如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。在 &lt;code&gt;shuffle&lt;/code&gt; 为True时使用。</target>
        </trans-unit>
        <trans-unit id="636d95a8f3edcd08bb7a122de05f8944c1a330ee" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;solver&lt;/code&gt; == &amp;lsquo;arpack&amp;rsquo;.</source>
          <target state="translated">如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。在 &lt;code&gt;solver&lt;/code&gt; =='arpack'时使用。</target>
        </trans-unit>
        <trans-unit id="d8bb54edf7a318cb4f3734b3f7721b6c840db8b1" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;svd_solver&lt;/code&gt; == &amp;lsquo;arpack&amp;rsquo; or &amp;lsquo;randomized&amp;rsquo;.</source>
          <target state="translated">如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。在 &lt;code&gt;svd_solver&lt;/code&gt; =='arpack'或'randomized'时使用。</target>
        </trans-unit>
        <trans-unit id="49fc99a0f8ec79ab5cf6633c8b91ad397447b0ae" translate="yes" xml:space="preserve">
          <source>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Note that this is used by subsampling and smoothing noise.</source>
          <target state="translated">如果是int,random_state是随机数发生器使用的种子;如果是RandomState实例,random_state是随机数发生器;如果是None,随机数发生器是np.random使用的RandomState实例。注意,这是由子采样和平滑噪声使用的。</target>
        </trans-unit>
        <trans-unit id="c8faba5e55a8f0e899120109354364cac8c2354b" translate="yes" xml:space="preserve">
          <source>If int, then consider &lt;code&gt;max_features&lt;/code&gt; features at each split.</source>
          <target state="translated">如果为int，则在每个拆分中考虑 &lt;code&gt;max_features&lt;/code&gt; 功能。</target>
        </trans-unit>
        <trans-unit id="79438cfe8b8de1684467307814da6af61cdfe6ba" translate="yes" xml:space="preserve">
          <source>If int, then consider &lt;code&gt;min_samples_leaf&lt;/code&gt; as the minimum number.</source>
          <target state="translated">如果为int，则将 &lt;code&gt;min_samples_leaf&lt;/code&gt; 视为最小值。</target>
        </trans-unit>
        <trans-unit id="69e04ca78560d3ef445be4d724f5c0cc8198187a" translate="yes" xml:space="preserve">
          <source>If int, then consider &lt;code&gt;min_samples_split&lt;/code&gt; as the minimum number.</source>
          <target state="translated">如果为int，则将 &lt;code&gt;min_samples_split&lt;/code&gt; 视为最小值。</target>
        </trans-unit>
        <trans-unit id="a8d276c242fbe315ce14903af35e7ebf9a0c3619" translate="yes" xml:space="preserve">
          <source>If int, then draw &lt;code&gt;max_features&lt;/code&gt; features.</source>
          <target state="translated">如果为int，则绘制 &lt;code&gt;max_features&lt;/code&gt; 功能。</target>
        </trans-unit>
        <trans-unit id="0771ca4ef29dd427aac0ffda56943aa541e3af54" translate="yes" xml:space="preserve">
          <source>If int, then draw &lt;code&gt;max_samples&lt;/code&gt; samples.</source>
          <target state="translated">如果为int，则绘制 &lt;code&gt;max_samples&lt;/code&gt; 样本。</target>
        </trans-unit>
        <trans-unit id="4430c154e22158c0d6435f75a2d640312ee73ab2" translate="yes" xml:space="preserve">
          <source>If log normalization was used, all the singular vectors are meaningful. However, if independent normalization or bistochastization were used, the first singular vectors, \(u_1\) and \(v_1\). are discarded. From now on, the &amp;ldquo;first&amp;rdquo; singular vectors refers to \(u_2 \dots u_{p+1}\) and \(v_2 \dots v_{p+1}\) except in the case of log normalization.</source>
          <target state="translated">如果使用对数归一化，则所有奇异向量都是有意义的。但是，如果使用独立归一化或双稳态，则第一个奇异矢量\（u_1 \）和\（v_1 \）。被丢弃。从现在开始，除对数归一化的情况外，&amp;ldquo;第一个&amp;rdquo;奇异向量是指\（u_2 \ dots u_ {p + 1} \）和\（v_2 \ dots v_ {p + 1} \）。</target>
        </trans-unit>
        <trans-unit id="f38434d38fce86523bd80aac7625c65019a5f868" translate="yes" xml:space="preserve">
          <source>If max_samples is larger than the number of samples provided, all samples will be used for all trees (no sampling).</source>
          <target state="translated">如果max_samples大于所提供的样本数,则所有的样本将用于所有的树(无采样)。</target>
        </trans-unit>
        <trans-unit id="0512919782ceb898f5e82137605d37f1918f7fe8" translate="yes" xml:space="preserve">
          <source>If method == &amp;ldquo;auto&amp;rdquo;, the ratio of n_samples / n_population is used to determine which algorithm to use: If ratio is between 0 and 0.01, tracking selection is used. If ratio is between 0.01 and 0.99, numpy.random.permutation is used. If ratio is greater than 0.99, reservoir sampling is used. The order of the selected integers is undefined. If a random order is desired, the selected subset should be shuffled.</source>
          <target state="translated">如果method ==&amp;ldquo; auto&amp;rdquo;，则使用n_samples / n_population的比率来确定要使用的算法：如果比率在0到0.01之间，则使用跟踪选择。如果比率在0.01到0.99之间，则使用numpy.random.permutation。如果比率大于0.99，则使用储层采样。所选整数的顺序不确定。如果需要随机顺序，则应将选定的子集混洗。</target>
        </trans-unit>
        <trans-unit id="aa3ae5990e2e00fef99c00cc07049cdbc9d8e931" translate="yes" xml:space="preserve">
          <source>If method == &amp;ldquo;pool&amp;rdquo;, a pool based algorithm is particularly fast, even faster than the tracking selection method. Hovewer, a vector containing the entire population has to be initialized. If n_samples ~ n_population, the reservoir sampling method is faster.</source>
          <target state="translated">如果method ==&amp;ldquo; pool&amp;rdquo;，则基于池的算法特别快，甚至比跟踪选择方法快。但是，必须初始化包含整个种群的向量。如果n_samples〜n_population，则储层采样方法更快。</target>
        </trans-unit>
        <trans-unit id="6916dcd6d6c8f00e1ac0ef865ab4c75ff38ebedf" translate="yes" xml:space="preserve">
          <source>If method == &amp;ldquo;reservoir_sampling&amp;rdquo;, a reservoir sampling algorithm is used which is suitable for high memory constraint or when O(&lt;code&gt;n_samples&lt;/code&gt;) ~ O(&lt;code&gt;n_population&lt;/code&gt;). The order of the selected integers is undefined. If a random order is desired, the selected subset should be shuffled.</source>
          <target state="translated">如果方法==&amp;ldquo; reservoir_sampling&amp;rdquo;，则使用适合于高内存约束或当O（ &lt;code&gt;n_samples&lt;/code&gt; ） &lt;code&gt;n_population&lt;/code&gt; （n_population）时使用的储层采样算法。所选整数的顺序不确定。如果需要随机顺序，则应将选定的子集混洗。</target>
        </trans-unit>
        <trans-unit id="0dea8a6c91cef90e0430014d895bb3954c8fb19c" translate="yes" xml:space="preserve">
          <source>If method ==&amp;rdquo;tracking_selection&amp;rdquo;, a set based implementation is used which is suitable for &lt;code&gt;n_samples&lt;/code&gt; &amp;lt;&amp;lt;&amp;lt; &lt;code&gt;n_population&lt;/code&gt;.</source>
          <target state="translated">如果方法==&amp;ldquo; tracking_selection&amp;rdquo;，则使用基于集合的实现，适用于 &lt;code&gt;n_samples&lt;/code&gt; &amp;lt;&amp;lt;&amp;lt; &lt;code&gt;n_population&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="79ae0bba548597b9902c41c70fbd6bf9602e8534" translate="yes" xml:space="preserve">
          <source>If metric is &amp;lsquo;precomputed&amp;rsquo;, Y is ignored and X is returned.</source>
          <target state="translated">如果度量是&amp;ldquo;预先计算的&amp;rdquo;，则忽略Y并返回X。</target>
        </trans-unit>
        <trans-unit id="ca8cb47e72e166fd730a116349e050254e6876d5" translate="yes" xml:space="preserve">
          <source>If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy&amp;rsquo;s metrics, but is less efficient than passing the metric name as a string.</source>
          <target state="translated">如果metric是可调用的函数，则会在每对实例（行）上调用它，并记录结果值。可调用对象应将两个数组作为输入并返回一个值，指示它们之间的距离。这适用于Scipy的指标，但效率不如将指标名称作为字符串传递。</target>
        </trans-unit>
        <trans-unit id="27305db22802f1bb3d3e2d60db076f6f0d275369" translate="yes" xml:space="preserve">
          <source>If mini-batch k-means is used, the best initialization is chosen and the algorithm runs once. Otherwise, the algorithm is run for each initialization and the best solution chosen.</source>
          <target state="translated">如果使用mini-batch k-means,则选择最佳初始化,算法运行一次。否则,每次初始化都要运行算法,并选择最佳解。</target>
        </trans-unit>
        <trans-unit id="16e6684b95c26e373af21b6c0d2ea50b705c0505" translate="yes" xml:space="preserve">
          <source>If multioutput is &amp;lsquo;raw_values&amp;rsquo;, then mean absolute error is returned for each output separately. If multioutput is &amp;lsquo;uniform_average&amp;rsquo; or an ndarray of weights, then the weighted average of all output errors is returned.</source>
          <target state="translated">如果多输出为&amp;ldquo; raw_values&amp;rdquo;，则分别为每个输出返回平均绝对错误。如果multioutput是'uniform_average'或权重的ndarray，则返回所有输出错误的加权平均值。</target>
        </trans-unit>
        <trans-unit id="caf3d42023b133b9efdfbb493fc66df503092e71" translate="yes" xml:space="preserve">
          <source>If no scoring is specified and the estimator has no score function, we can either return None or raise an exception.</source>
          <target state="translated">如果没有指定评分,并且估计器没有评分函数,我们可以返回None或者引发异常。</target>
        </trans-unit>
        <trans-unit id="5519c1c6825bf59bfd06c08f68bb64b64ee1a0ae" translate="yes" xml:space="preserve">
          <source>If no valid consensus set could be found. This occurs if &lt;code&gt;is_data_valid&lt;/code&gt; and &lt;code&gt;is_model_valid&lt;/code&gt; return False for all &lt;code&gt;max_trials&lt;/code&gt; randomly chosen sub-samples.</source>
          <target state="translated">如果找不到有效的共识集。如果 &lt;code&gt;is_data_valid&lt;/code&gt; 和 &lt;code&gt;is_model_valid&lt;/code&gt; 对所有 &lt;code&gt;max_trials&lt;/code&gt; 随机选择的子样本返回False，则会发生这种情况。</target>
        </trans-unit>
        <trans-unit id="e48a960f323664c14eed43108cfafa1159796090" translate="yes" xml:space="preserve">
          <source>If normalize is &lt;code&gt;True&lt;/code&gt;, return the fraction of misclassifications (float), else it returns the number of misclassifications (int). The best performance is 0.</source>
          <target state="translated">如果normalize为 &lt;code&gt;True&lt;/code&gt; ，则返回错误分类的分数（浮点数），否则返回错误分类的数目（整数）。最佳性能为0。</target>
        </trans-unit>
        <trans-unit id="d93355ca0c397a92c0eb63483bbae0b531d00cf1" translate="yes" xml:space="preserve">
          <source>If not &lt;code&gt;None&lt;/code&gt;, the standardized partial AUC &lt;a href=&quot;#r4bb7c4558997-3&quot; id=&quot;id1&quot;&gt;[3]&lt;/a&gt; over the range [0, max_fpr] is returned.</source>
          <target state="translated">如果不为 &lt;code&gt;None&lt;/code&gt; ，则返回范围为[0，max_fpr] 的标准化部分AUC &lt;a href=&quot;#r4bb7c4558997-3&quot; id=&quot;id1&quot;&gt;[3]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="954d968337062d6fae676f5915fb0dc48db9ccef" translate="yes" xml:space="preserve">
          <source>If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.</source>
          <target state="translated">如果不是None,建立一个只考虑整个语料库中按词频排序的top max_features的词汇。</target>
        </trans-unit>
        <trans-unit id="6b6dff5f6d294c2bdbfe5ee6b0ee56319193880c" translate="yes" xml:space="preserve">
          <source>If not None, data is split in a stratified fashion, using this as the class labels.</source>
          <target state="translated">如果不是None,则以这个作为类标签,将数据进行分层分割。</target>
        </trans-unit>
        <trans-unit id="d9fe4271c08ca870db7143f08e0938aa49f2d1d0" translate="yes" xml:space="preserve">
          <source>If not None, set the highest value of the fit to y_max.</source>
          <target state="translated">如果不是None,将拟合度的最高值设置为y_max。</target>
        </trans-unit>
        <trans-unit id="3c138b5d1ed12eddb3226ed7535814059b7a615c" translate="yes" xml:space="preserve">
          <source>If not None, set the lowest value of the fit to y_min.</source>
          <target state="translated">如果不是None,将拟合的最低值设置为y_min。</target>
        </trans-unit>
        <trans-unit id="ebcf44116da09ed76a723aed5cadbe6d4ed2530d" translate="yes" xml:space="preserve">
          <source>If not None, this argument is passed as &lt;code&gt;sample_weight&lt;/code&gt; keyword argument to the &lt;code&gt;score&lt;/code&gt; method of the final estimator.</source>
          <target state="translated">如果不是None，则将此参数作为 &lt;code&gt;sample_weight&lt;/code&gt; 关键字参数传递给最终估算器的 &lt;code&gt;score&lt;/code&gt; 方法。</target>
        </trans-unit>
        <trans-unit id="f0f7d0b7263b16cf926e8314af8096b9ae6c9066" translate="yes" xml:space="preserve">
          <source>If not given, the bandwidth is estimated using sklearn.cluster.estimate_bandwidth; see the documentation for that function for hints on scalability (see also the Notes, below).</source>
          <target state="translated">如果没有给定,则使用sklearn.cluster.eestimate_bandwidth估计带宽;关于可扩展性的提示,请参见该函数的文档(也请参见下面的注释)。</target>
        </trans-unit>
        <trans-unit id="3fc57ade66d3b29b2e5dacfcee394aac7f4ec951" translate="yes" xml:space="preserve">
          <source>If not provided, labels will be inferred from y_true. If &lt;code&gt;labels&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; has shape (n_samples,) the labels are assumed to be binary and are inferred from &lt;code&gt;y_true&lt;/code&gt;. .. versionadded:: 0.18</source>
          <target state="translated">如果未提供，将从y_true推断标签。如果 &lt;code&gt;labels&lt;/code&gt; 为 &lt;code&gt;None&lt;/code&gt; 且 &lt;code&gt;y_pred&lt;/code&gt; 具有形状（n_samples），则假定标签为二进制，并从 &lt;code&gt;y_true&lt;/code&gt; 推论得出。..版本添加：： 0.18</target>
        </trans-unit>
        <trans-unit id="d3a1f4e96f04c6f8dfd4835d50de53587903b2e7" translate="yes" xml:space="preserve">
          <source>If one-of-K coding is applied to categorical features, this will include the constructed feature names but not the original ones.</source>
          <target state="translated">如果对分类特征应用one-of-K编码,这将包括构建的特征名,但不包括原始特征名。</target>
        </trans-unit>
        <trans-unit id="c3b8faf61102e14148418b48bf3dbb3389d54ef3" translate="yes" xml:space="preserve">
          <source>If only the diagonal of the auto-covariance is being used, the method &lt;code&gt;diag()&lt;/code&gt; of a kernel can be called, which is more computationally efficient than the equivalent call to &lt;code&gt;__call__&lt;/code&gt;: &lt;code&gt;np.diag(k(X, X)) == k.diag(X)&lt;/code&gt;</source>
          <target state="translated">如果仅使用自协方差的对角线，则可以调用内核的 &lt;code&gt;diag()&lt;/code&gt; 方法，其计算效率比对 &lt;code&gt;__call__&lt;/code&gt; 的等效调用高： &lt;code&gt;np.diag(k(X, X)) == k.diag(X)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="db7f35e5fc1dd73c10c86bdccb4a2449d5a89ec7" translate="yes" xml:space="preserve">
          <source>If order is &amp;lsquo;random&amp;rsquo; a random ordering will be used.</source>
          <target state="translated">如果订单是&amp;ldquo;随机的&amp;rdquo;，将使用随机排序。</target>
        </trans-unit>
        <trans-unit id="f42275492b00fc14b5861ea85e0f4944992d0324" translate="yes" xml:space="preserve">
          <source>If passed, include the name of the estimator in warning messages.</source>
          <target state="translated">如果通过,在警告信息中包含估计器的名称。</target>
        </trans-unit>
        <trans-unit id="908cd551a5eab6201799122746b2ad3d99f4a3d2" translate="yes" xml:space="preserve">
          <source>If positive, restrict regression coefficients to be positive</source>
          <target state="translated">如果为正,则限制回归系数为正。</target>
        </trans-unit>
        <trans-unit id="c5484f943f94af044829e2453a87ea1beff675d6" translate="yes" xml:space="preserve">
          <source>If return_costs is True, the objective function and dual gap at each iteration are returned.</source>
          <target state="translated">如果return_costs为True,则返回目标函数和每次迭代时的双倍差距。</target>
        </trans-unit>
        <trans-unit id="3a07f641c209d2556442bcd652915ffb4ab857db" translate="yes" xml:space="preserve">
          <source>If safe is false, clone will fall back to a deep copy on objects that are not estimators.</source>
          <target state="translated">如果safe为false,clone会在不是估计器的对象上回落到深度复制。</target>
        </trans-unit>
        <trans-unit id="179d83839b7c246b21dd4fad6260ec3c338cc783" translate="yes" xml:space="preserve">
          <source>If seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError.</source>
          <target state="translated">如果seed是None,返回np.random使用的RandomState单体。如果seed是int,返回一个新的RandomState实例。如果 seed 已经是一个 RandomState 实例,则返回它。否则会引发ValueError。</target>
        </trans-unit>
        <trans-unit id="7108bbb3c9ecad70c2ad038e49ece7ce906a1c8f" translate="yes" xml:space="preserve">
          <source>If seq[i] is an int or a tuple with one int value, a one-way PDP is created; if seq[i] is a tuple of two ints, a two-way PDP is created. If feature_names is specified and seq[i] is an int, seq[i] must be &amp;lt; len(feature_names). If seq[i] is a string, feature_names must be specified, and seq[i] must be in feature_names.</source>
          <target state="translated">如果seq [i]是一个int或具有一个int值的元组，则创建一个单向PDP；否则，将创建一个单向PDP。如果seq [i]是两个整数的元组，则会创建一个双向PDP。如果指定了feature_names并且seq [i]是一个int，则seq [i]必须小于len（feature_names）。如果seq [i]是字符串，则必须指定feature_names，并且seq [i]必须在feature_names中。</target>
        </trans-unit>
        <trans-unit id="b01ea458e6ed79ec076f21dd79564eecc3f7a882" translate="yes" xml:space="preserve">
          <source>If set to &amp;lsquo;random&amp;rsquo;, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to &amp;lsquo;random&amp;rsquo;) often leads to significantly faster convergence especially when tol is higher than 1e-4</source>
          <target state="translated">如果设置为&amp;ldquo;随机&amp;rdquo;，则随机系数将在每次迭代时更新，而不是默认情况下按顺序遍历要素。这（设置为&amp;ldquo;随机&amp;rdquo;）通常会导致收敛更快，尤其是当tol高于1e-4时</target>
        </trans-unit>
        <trans-unit id="7a4374896942a67a58d05d59133607fc7483d7a2" translate="yes" xml:space="preserve">
          <source>If set to &amp;lsquo;random&amp;rsquo;, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to &amp;lsquo;random&amp;rsquo;) often leads to significantly faster convergence especially when tol is higher than 1e-4.</source>
          <target state="translated">如果设置为&amp;ldquo;随机&amp;rdquo;，则随机系数将在每次迭代时更新，而不是默认情况下按顺序遍历要素。这（设置为&amp;ldquo;随机&amp;rdquo;）通常会导致收敛更快，尤其是当tol高于1e-4时。</target>
        </trans-unit>
        <trans-unit id="d15f7a009cd3b6e81a757534913f0edc7a2b7947" translate="yes" xml:space="preserve">
          <source>If set to True, forces coefficients to be positive. (Only allowed when &lt;code&gt;y.ndim == 1&lt;/code&gt;).</source>
          <target state="translated">如果设置为True，则强制系数为正。（仅当 &lt;code&gt;y.ndim == 1&lt;/code&gt; 时允许）。</target>
        </trans-unit>
        <trans-unit id="6bd31584b0a279bb6a357ab195ba9534cb5cad4e" translate="yes" xml:space="preserve">
          <source>If set to True, the scores are averaged across all folds, and the coefs and the C that corresponds to the best score is taken, and a final refit is done using these parameters. Otherwise the coefs, intercepts and C that correspond to the best scores across folds are averaged.</source>
          <target state="translated">如果设置为True,则对所有折线的分数进行平均,并取对应于最佳分数的coefs和C,并使用这些参数进行最后的调整。否则,则取各折线中与最佳分数对应的coefs、intercepts和C的平均值。</target>
        </trans-unit>
        <trans-unit id="f3d43f9f7c9e3af1ca6eddc0268b8ee91bb07ee3" translate="yes" xml:space="preserve">
          <source>If set, scikit-learn will attempt to limit the size of temporary arrays to this number of MiB (per job when parallelised), often saving both computation time and memory on expensive operations that can be performed in chunks. Global default: 1024.</source>
          <target state="translated">如果设置了,scikit-learn会尝试将临时数组的大小限制在这个MiB的数量(并行化时的每个作业),通常会在可以分块执行的昂贵操作上节省计算时间和内存。全局默认:1024。</target>
        </trans-unit>
        <trans-unit id="cd9d66e1ab8be1fe689482ddb0cbca43b44a3950" translate="yes" xml:space="preserve">
          <source>If strictly positive, stop reading any new line of data once the position in the file has reached the (offset + length) bytes threshold.</source>
          <target state="translated">如果严格来说是正值,一旦文件中的位置达到(偏移量+长度)字节阈值,就停止读取任何新的数据行。</target>
        </trans-unit>
        <trans-unit id="af99c20b0f1015ebcecc8bfb6a50ca848ab08d15" translate="yes" xml:space="preserve">
          <source>If string, specifies the path that will contain the data. If file-like, data will be written to f. f should be opened in binary mode.</source>
          <target state="translated">如果是字符串,指定包含数据的路径。如果是类文件,数据将被写入f,f应该以二进制模式打开。</target>
        </trans-unit>
        <trans-unit id="d25cbbfb18995acebbdc8e788e3994e16b21b8ae" translate="yes" xml:space="preserve">
          <source>If sum_over_features is False shape is (n_samples_X * n_samples_Y, n_features) and D contains the componentwise L1 pairwise-distances (ie. absolute difference), else shape is (n_samples_X, n_samples_Y) and D contains the pairwise L1 distances.</source>
          <target state="translated">如果sum_over_features为False,则shape为(n_samples_X*n_samples_Y,n_features),D包含分量的L1对偶距离(即绝对差),否则shape为(n_samples_X,n_samples_Y),D包含L1对偶距离。</target>
        </trans-unit>
        <trans-unit id="1979731cc29c616c5ac5593ab888192599b2d46b" translate="yes" xml:space="preserve">
          <source>If the &lt;code&gt;loss&lt;/code&gt; does not support probabilities.</source>
          <target state="translated">如果 &lt;code&gt;loss&lt;/code&gt; 不支持概率。</target>
        </trans-unit>
        <trans-unit id="41f96f9118cd39448d94e68aca8aa6d327b368ef" translate="yes" xml:space="preserve">
          <source>If the algorithm is &amp;ldquo;deflation&amp;rdquo;, n_iter is the maximum number of iterations run across all components. Else they are just the number of iterations taken to converge.</source>
          <target state="translated">如果算法是&amp;ldquo;放气&amp;rdquo;，则n_iter是所有组件之间运行的最大迭代次数。否则，它们只是收敛的迭代次数。</target>
        </trans-unit>
        <trans-unit id="b72ab6a8a780a6da86f798b7381c54dc2236b80c" translate="yes" xml:space="preserve">
          <source>If the algorithm stops before fully converging (because of &lt;code&gt;tol&lt;/code&gt; of &lt;code&gt;max_iter&lt;/code&gt;), &lt;code&gt;labels_&lt;/code&gt; and &lt;code&gt;means_&lt;/code&gt; will not be consistent, i.e. the &lt;code&gt;means_&lt;/code&gt; will not be the means of the points in each cluster. Also, the estimator will reassign &lt;code&gt;labels_&lt;/code&gt; after the last iteration to make &lt;code&gt;labels_&lt;/code&gt; consistent with &lt;code&gt;predict&lt;/code&gt; on the training set.</source>
          <target state="translated">如果算法停止之前完全融合（因为 &lt;code&gt;tol&lt;/code&gt; 的 &lt;code&gt;max_iter&lt;/code&gt; ）， &lt;code&gt;labels_&lt;/code&gt; 和 &lt;code&gt;means_&lt;/code&gt; 不会是一致的，即 &lt;code&gt;means_&lt;/code&gt; 不会在每个集群点的手段。同样，估算器将在最后一次迭代后重新分配 &lt;code&gt;labels_&lt;/code&gt; ，以使 &lt;code&gt;labels_&lt;/code&gt; 与训练集上的 &lt;code&gt;predict&lt;/code&gt; 一致。</target>
        </trans-unit>
        <trans-unit id="4043c787702cc081c41f25b031d69ea98cf35c42" translate="yes" xml:space="preserve">
          <source>If the array is not symmetric, then a symmetrized version is returned. Optionally, a warning or exception is raised if the matrix is not symmetric.</source>
          <target state="translated">如果数组不是对称的,那么将返回一个对称的版本。如果矩阵不是对称的,可以选择发出警告或异常。</target>
        </trans-unit>
        <trans-unit id="d4238935d45ce51eea0be6c47146a609e05c26ed" translate="yes" xml:space="preserve">
          <source>If the attributes are not found.</source>
          <target state="translated">如果没有找到属性。</target>
        </trans-unit>
        <trans-unit id="10f86bc0a8ef8d94dd88200305e21d6ac290743f" translate="yes" xml:space="preserve">
          <source>If the classifier performs equally well on either class, this term reduces to the conventional accuracy (i.e., the number of correct predictions divided by the total number of predictions).</source>
          <target state="translated">如果分类器在任何一个类上的表现都一样好,这个项就降为传统的准确率(即正确预测数除以总预测数)。</target>
        </trans-unit>
        <trans-unit id="9d0651dbf433477af9dfe8c9482b03c0b28a7aea" translate="yes" xml:space="preserve">
          <source>If the data ordering is not arbitrary (e.g. samples with the same class label are contiguous), shuffling it first may be essential to get a meaningful cross- validation result. However, the opposite may be true if the samples are not independently and identically distributed. For example, if samples correspond to news articles, and are ordered by their time of publication, then shuffling the data will likely lead to a model that is overfit and an inflated validation score: it will be tested on samples that are artificially similar (close in time) to training samples.</source>
          <target state="translated">如果数据排序不是任意的(例如,具有相同类标签的样本是连续的),先洗牌可能是获得有意义的交叉验证结果的关键。然而,如果样本不是独立和相同分布,则情况可能相反。例如,如果样本对应的是新闻文章,并按其发表时间排序,那么洗牌数据很可能会导致模型过拟合和验证分数夸大:它将在与训练样本人为相似(时间上接近)的样本上进行测试。</target>
        </trans-unit>
        <trans-unit id="4fdc5debb409dcec7a673c288152f0ef6e4738ef" translate="yes" xml:space="preserve">
          <source>If the default value is passed, then &lt;code&gt;keepdims&lt;/code&gt; will not be passed through to the &lt;code&gt;mean&lt;/code&gt; method of sub-classes of &lt;code&gt;ndarray&lt;/code&gt;, however any non-default value will be. If the sub-class&amp;rsquo; method does not implement &lt;code&gt;keepdims&lt;/code&gt; any exceptions will be raised.</source>
          <target state="translated">如果传递了默认值，则 &lt;code&gt;keepdims&lt;/code&gt; 不会传递给 &lt;code&gt;ndarray&lt;/code&gt; 的子类的 &lt;code&gt;mean&lt;/code&gt; 方法，但是任何非默认值都将传递。如果子类的方法未实现 &lt;code&gt;keepdims&lt;/code&gt; ,则将引发任何异常。</target>
        </trans-unit>
        <trans-unit id="ca5777d1057fb92ff835301c12f93dc71bd51069" translate="yes" xml:space="preserve">
          <source>If the difference between the current prediction and the correct label is below this threshold, the model is not updated.</source>
          <target state="translated">如果当前预测与正确标签之间的差异低于这个阈值,则不更新模型。</target>
        </trans-unit>
        <trans-unit id="54f187a0c12dbeb2b22455f8308653334a568512" translate="yes" xml:space="preserve">
          <source>If the estimator supports incremental learning, this will be used to speed up fitting for different training set sizes.</source>
          <target state="translated">如果估计器支持增量学习,这将用于加快不同训练集大小的拟合速度。</target>
        </trans-unit>
        <trans-unit id="28446974a089033b0f005a14dd2e5e7cde0d019d" translate="yes" xml:space="preserve">
          <source>If the file does not exist yet, it is downloaded from mldata.org .</source>
          <target state="translated">如果文件还不存在,可从mldata.org下载。</target>
        </trans-unit>
        <trans-unit id="3377386ec971b5f97505ad0b0efacd641307a6b2" translate="yes" xml:space="preserve">
          <source>If the folder does not already exist, it is automatically created.</source>
          <target state="translated">如果文件夹还不存在,则会自动创建。</target>
        </trans-unit>
        <trans-unit id="bcf86cd76452a354a39a384d6cc008f0521fadc0" translate="yes" xml:space="preserve">
          <source>If the gradient norm is below this threshold, the optimization will be stopped.</source>
          <target state="translated">如果梯度规范低于这个阈值,优化将被停止。</target>
        </trans-unit>
        <trans-unit id="aaea0ac91de1101ebb5583d72a39edadc546a9ed" translate="yes" xml:space="preserve">
          <source>If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient (&lt;a href=&quot;generated/sklearn.metrics.silhouette_score#sklearn.metrics.silhouette_score&quot;&gt;&lt;code&gt;sklearn.metrics.silhouette_score&lt;/code&gt;&lt;/a&gt;) is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:</source>
          <target state="translated">如果不知道地面真相标签，则必须使用模型本身进行评估。轮廓系数（&lt;a href=&quot;generated/sklearn.metrics.silhouette_score#sklearn.metrics.silhouette_score&quot;&gt; &lt;code&gt;sklearn.metrics.silhouette_score&lt;/code&gt; &lt;/a&gt;）是这种评估的一个示例，其中较高的轮廓系数得分与具有更好定义的群集的模型有关。为每个样本定义了轮廓系数，该系数由两个分数组成：</target>
        </trans-unit>
        <trans-unit id="bb3f5944370bdcf362ff4bffb46e8bf1ded41ef1" translate="yes" xml:space="preserve">
          <source>If the ground truth labels are not known, the Calinski-Harabaz index (&lt;a href=&quot;generated/sklearn.metrics.calinski_harabaz_score#sklearn.metrics.calinski_harabaz_score&quot;&gt;&lt;code&gt;sklearn.metrics.calinski_harabaz_score&lt;/code&gt;&lt;/a&gt;) - also known as the Variance Ratio Criterion - can be used to evaluate the model, where a higher Calinski-Harabaz score relates to a model with better defined clusters.</source>
          <target state="translated">如果不知道地面真相标签，则可以使用Calinski-Harabaz指数（&lt;a href=&quot;generated/sklearn.metrics.calinski_harabaz_score#sklearn.metrics.calinski_harabaz_score&quot;&gt; &lt;code&gt;sklearn.metrics.calinski_harabaz_score&lt;/code&gt; &lt;/a&gt;）（也称为方差比标准）来评估模型，其中较高的Calinski-Harabaz得分与具有以下特征的模型相关：定义更好的集群。</target>
        </trans-unit>
        <trans-unit id="a4a519d35f95c18e319df7e8878b98f013f9bd44" translate="yes" xml:space="preserve">
          <source>If the ground truth labels are not known, the Davies-Bouldin index (&lt;a href=&quot;generated/sklearn.metrics.davies_bouldin_score#sklearn.metrics.davies_bouldin_score&quot;&gt;&lt;code&gt;sklearn.metrics.davies_bouldin_score&lt;/code&gt;&lt;/a&gt;) can be used to evaluate the model, where a lower Davies-Bouldin index relates to a model with better separation between the clusters.</source>
          <target state="translated">如果不知道地面真相标签，则可以使用Davies-Bouldin索引（&lt;a href=&quot;generated/sklearn.metrics.davies_bouldin_score#sklearn.metrics.davies_bouldin_score&quot;&gt; &lt;code&gt;sklearn.metrics.davies_bouldin_score&lt;/code&gt; &lt;/a&gt;）评估模型，其中较低的Davies-Bouldin索引与具有更好群集分离度的模型相关。</target>
        </trans-unit>
        <trans-unit id="7ce6861d9ec6948a6bc8aef858e97abae7ed0654" translate="yes" xml:space="preserve">
          <source>If the input is a sparse matrix, only the non-zero values are subject to update by the Binarizer class.</source>
          <target state="translated">如果输入是一个稀疏矩阵,那么只有非零值才会被Binarizer类更新。</target>
        </trans-unit>
        <trans-unit id="c10a8d9d8c40f9f50dafe36b727f5b47e7075f19" translate="yes" xml:space="preserve">
          <source>If the input matrix X is very sparse, it is recommended to convert to sparse &lt;code&gt;csc_matrix&lt;/code&gt; before calling fit and sparse &lt;code&gt;csr_matrix&lt;/code&gt; before calling predict. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples.</source>
          <target state="translated">如果输入矩阵X是非常稀疏，建议转换为稀疏 &lt;code&gt;csc_matrix&lt;/code&gt; 调用配合和稀疏之前 &lt;code&gt;csr_matrix&lt;/code&gt; 调用预测之前。当特征在大多数样本中为零时，与稀疏矩阵输入相比，稀疏矩阵输入的训练时间可以快几个数量级。</target>
        </trans-unit>
        <trans-unit id="661cb29a3f5fe68fda8ea5f8c53273efb7753ce1" translate="yes" xml:space="preserve">
          <source>If the labels are encoded with +1 and -1, \(y\): is the true value, and \(w\) is the predicted decisions as output by &lt;code&gt;decision_function&lt;/code&gt;, then the hinge loss is defined as:</source>
          <target state="translated">如果标签用+1和-1编码，则\（y \）：是真实值，\（w \）是Decision_function输出的预测 &lt;code&gt;decision_function&lt;/code&gt; ，则铰链损耗定义为：</target>
        </trans-unit>
        <trans-unit id="30e7605353fedb22ff0c25c7418abbab28137e70" translate="yes" xml:space="preserve">
          <source>If the loss on a sample is greater than the &lt;code&gt;residual_threshold&lt;/code&gt;, then this sample is classified as an outlier.</source>
          <target state="translated">如果样本的损失大于 &lt;code&gt;residual_threshold&lt;/code&gt; 阈值，则将该样本分类为离群值。</target>
        </trans-unit>
        <trans-unit id="fd47c1f065810b1b45f0d8d994aa0a4ff29505d4" translate="yes" xml:space="preserve">
          <source>If the metric constructor parameter is &amp;ldquo;precomputed&amp;rdquo;, X is assumed to be the distance matrix between the data to be predicted and &lt;code&gt;self.centroids_&lt;/code&gt;.</source>
          <target state="translated">如果度量构造函数参数是&amp;ldquo;预先计算的&amp;rdquo;，则将X假定为要预测的数据与 &lt;code&gt;self.centroids_&lt;/code&gt; 之间的距离矩阵。</target>
        </trans-unit>
        <trans-unit id="c8476d320358236ab03a9b2e5775d6207658d4f7" translate="yes" xml:space="preserve">
          <source>If the metric is &amp;lsquo;precomputed&amp;rsquo; X must be a square distance matrix. Otherwise it contains a sample per row.</source>
          <target state="translated">如果度量是&amp;ldquo;预先计算的&amp;rdquo;，则X必须是平方距离矩阵。否则，它每行包含一个样本。</target>
        </trans-unit>
        <trans-unit id="ed2846275337b6c05f70ce353bc177c3fd2fc1b0" translate="yes" xml:space="preserve">
          <source>If the metric is &amp;lsquo;precomputed&amp;rsquo; X must be a square distance matrix. Otherwise it contains a sample per row. If the method is &amp;lsquo;exact&amp;rsquo;, X may be a sparse matrix of type &amp;lsquo;csr&amp;rsquo;, &amp;lsquo;csc&amp;rsquo; or &amp;lsquo;coo&amp;rsquo;.</source>
          <target state="translated">如果度量是&amp;ldquo;预先计算的&amp;rdquo;，则X必须是平方距离矩阵。否则，它每行包含一个样本。如果方法是&amp;ldquo;精确&amp;rdquo;，则X可以是类型为&amp;ldquo; csr&amp;rdquo;，&amp;ldquo; csc&amp;rdquo;或&amp;ldquo; coo&amp;rdquo;的稀疏矩阵。</target>
        </trans-unit>
        <trans-unit id="be2b4ccc2ee21bcc622b72ad8a09e29905f86d22" translate="yes" xml:space="preserve">
          <source>If the number of features is \(p\), you now require \(n \sim 1/d^p\) points. Let&amp;rsquo;s say that we require 10 points in one dimension: now \(10^p\) points are required in \(p\) dimensions to pave the \([0, 1]\) space. As \(p\) becomes large, the number of training points required for a good estimator grows exponentially.</source>
          <target state="translated">如果要素数量为\（p \），则现在需要\（n \ sim 1 / d ^ p \）点。假设我们在一维中需要10个点：现在，在\（p \）维度中需要\（10 ^ p \）个点才能铺平\（[0，1] \）空间。随着\（p \）变大，一个好的估计量所需的训练点数将呈指数增长。</target>
        </trans-unit>
        <trans-unit id="31f6fadae8fdb5e25318685f0dd36c90a3234b90" translate="yes" xml:space="preserve">
          <source>If the number of features is much greater than the number of samples, avoid over-fitting in choosing &lt;a href=&quot;#svm-kernels&quot;&gt;Kernel functions&lt;/a&gt; and regularization term is crucial.</source>
          <target state="translated">如果特征数量远大于样本数量，则在选择&lt;a href=&quot;#svm-kernels&quot;&gt;内核函数&lt;/a&gt;时应避免过度拟合，并且正则化项至关重要。</target>
        </trans-unit>
        <trans-unit id="421f2017551080d266c05ad587f0ba1ecff6bff8" translate="yes" xml:space="preserve">
          <source>If the number of instances of data needs to be reduced, or if one wants a large number of subclusters either as a preprocessing step or otherwise, Birch is more useful than MiniBatchKMeans.</source>
          <target state="translated">如果需要减少数据的实例数量,或者想要大量的子群作为预处理步骤或其他,Birch比MiniBatchKMeans更有用。</target>
        </trans-unit>
        <trans-unit id="0169ea68b458a3b315ac7acca32e943f8bdb1bed" translate="yes" xml:space="preserve">
          <source>If the option chosen is &amp;lsquo;ovr&amp;rsquo;, then a binary problem is fit for each label. For &amp;lsquo;multinomial&amp;rsquo; the loss minimised is the multinomial loss fit across the entire probability distribution, &lt;em&gt;even when the data is binary&lt;/em&gt;. &amp;lsquo;multinomial&amp;rsquo; is unavailable when solver=&amp;rsquo;liblinear&amp;rsquo;. &amp;lsquo;auto&amp;rsquo; selects &amp;lsquo;ovr&amp;rsquo; if the data is binary, or if solver=&amp;rsquo;liblinear&amp;rsquo;, and otherwise selects &amp;lsquo;multinomial&amp;rsquo;.</source>
          <target state="translated">如果选择的选项是&amp;ldquo; ovr&amp;rdquo;，则每个标签都适合二进制问题。对于&amp;ldquo;多项式&amp;rdquo;，&lt;em&gt;即使数据是二进制的&lt;/em&gt;，最小化的损失也就是整个概率分布中的多项式损失拟合。当solver ='liblinear'时，'multinomial'不可用。如果数据是二进制的，或者如果Solver ='liblinear'，则'auto'选择'ovr'，否则选择'multinomial'。</target>
        </trans-unit>
        <trans-unit id="fb7bde2a134f67333f646adb3cb422fdb3b0a619" translate="yes" xml:space="preserve">
          <source>If the prediction task is to classify the observations in a set of finite labels, in other words to &amp;ldquo;name&amp;rdquo; the objects observed, the task is said to be a &lt;strong&gt;classification&lt;/strong&gt; task. On the other hand, if the goal is to predict a continuous target variable, it is said to be a &lt;strong&gt;regression&lt;/strong&gt; task.</source>
          <target state="translated">如果预测任务是将观察结果分类为一组有限标签，换句话说就是&amp;ldquo;命名&amp;rdquo;观察到的对象，则该任务称为&lt;strong&gt;分类&lt;/strong&gt;任务。另一方面，如果目标是预测连续目标变量，则称其为&lt;strong&gt;回归&lt;/strong&gt;任务。</target>
        </trans-unit>
        <trans-unit id="c305135e1b17987e86652a7910deafacf0f5dc9d" translate="yes" xml:space="preserve">
          <source>If the pyamg package is installed, it is used: this greatly speeds up computation.</source>
          <target state="translated">如果安装了pyamg包,就会使用它:这大大加快了计算速度。</target>
        </trans-unit>
        <trans-unit id="04f07265ff7d7b70145be5aec52784e1b24d6f09" translate="yes" xml:space="preserve">
          <source>If the radius of the subcluster obtained by merging the new sample and the nearest subcluster is greater than the square of the threshold and if the number of subclusters is greater than the branching factor, then a space is temporarily allocated to this new sample. The two farthest subclusters are taken and the subclusters are divided into two groups on the basis of the distance between these subclusters.</source>
          <target state="translated">如果新样本与最近的子群合并得到的子群半径大于阈值的平方,且子群数量大于分支系数,则暂时给这个新样本分配一个空间。取最远的两个子簇,根据这些子簇之间的距离,将子簇分为两组。</target>
        </trans-unit>
        <trans-unit id="41eef1b8a501219131b2619b097a82294e78c28a" translate="yes" xml:space="preserve">
          <source>If the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning criterion such as &lt;code&gt;min_weight_fraction_leaf&lt;/code&gt;, which ensure that leaf nodes contain at least a fraction of the overall sum of the sample weights.</source>
          <target state="translated">如果对样本进行加权，则使用基于权重的预修剪标准（例如 &lt;code&gt;min_weight_fraction_leaf&lt;/code&gt; )可以更轻松地优化树结构，该标准可确保叶节点至少包含样本权重总和的一小部分。</target>
        </trans-unit>
        <trans-unit id="0c1baaebbfab363e25ace0c6b6ee91539fab116b" translate="yes" xml:space="preserve">
          <source>If the selected solver is &amp;lsquo;L-BFGS&amp;rsquo;, training does not support online nor mini-batch learning.</source>
          <target state="translated">如果选择的求解器为&amp;ldquo; L-BFGS&amp;rdquo;，则培训不支持在线学习或小批量学习。</target>
        </trans-unit>
        <trans-unit id="6b79b273c5ccf0e85d810ff5a4ad56e9102a13be" translate="yes" xml:space="preserve">
          <source>If the target is a continuous value, then for node \(m\), representing a region \(R_m\) with \(N_m\) observations, common criteria to minimise as for determining locations for future splits are Mean Squared Error, which minimizes the L2 error using mean values at terminal nodes, and Mean Absolute Error, which minimizes the L1 error using median values at terminal nodes.</source>
          <target state="translated">如果目标是一个连续的值,那么对于节点 \(m\),代表一个区域 \(R_m\)与 \(N_m\)的观测值,在确定未来分割的位置时,常见的最小化标准是均值平方误差,它利用终端节点的平均值将L2误差最小化,以及均值绝对误差,它利用终端节点的中值将L1误差最小化。</target>
        </trans-unit>
        <trans-unit id="bd360b0d17d9758c91116a373249c96c3c493365" translate="yes" xml:space="preserve">
          <source>If the text is in a mish-mash of encodings that is simply too hard to sort out (which is the case for the 20 Newsgroups dataset), you can fall back on a simple single-byte encoding such as &lt;code&gt;latin-1&lt;/code&gt;. Some text may display incorrectly, but at least the same sequence of bytes will always represent the same feature.</source>
          <target state="translated">如果文本的编码杂乱无章，难以整理（这是20个新闻组数据集的情况），则可以使用简单的单字节编码，如 &lt;code&gt;latin-1&lt;/code&gt; 。某些文本可能显示不正确，但是至少相同的字节序列将始终表示相同的功能。</target>
        </trans-unit>
        <trans-unit id="21286b430a50cd4c4f50df67c996d4b5b475416f" translate="yes" xml:space="preserve">
          <source>If the text you are loading is not actually encoded with UTF-8, however, you will get a &lt;code&gt;UnicodeDecodeError&lt;/code&gt;. The vectorizers can be told to be silent about decoding errors by setting the &lt;code&gt;decode_error&lt;/code&gt; parameter to either &lt;code&gt;&quot;ignore&quot;&lt;/code&gt; or &lt;code&gt;&quot;replace&quot;&lt;/code&gt;. See the documentation for the Python function &lt;code&gt;bytes.decode&lt;/code&gt; for more details (type &lt;code&gt;help(bytes.decode)&lt;/code&gt; at the Python prompt).</source>
          <target state="translated">但是，如果要加载的文本实际上未使用UTF-8编码，则将收到 &lt;code&gt;UnicodeDecodeError&lt;/code&gt; 。该vectorizers可以告诉保持沉默被设定解码错误 &lt;code&gt;decode_error&lt;/code&gt; 参数设置为 &lt;code&gt;&quot;ignore&quot;&lt;/code&gt; 或 &lt;code&gt;&quot;replace&quot;&lt;/code&gt; 。有关更多详细信息，请参见Python函数 &lt;code&gt;bytes.decode&lt;/code&gt; 的文档 &lt;code&gt;help(bytes.decode)&lt;/code&gt; 在Python提示符下键入help（bytes.decode））。</target>
        </trans-unit>
        <trans-unit id="13a9f813159d9e6258a164870cb1dc6a301dddd5" translate="yes" xml:space="preserve">
          <source>If the training score and the validation score are both low, the estimator will be underfitting. If the training score is high and the validation score is low, the estimator is overfitting and otherwise it is working very well. A low training score and a high validation score is usually not possible. All three cases can be found in the plot below where we vary the parameter \(\gamma\) of an SVM on the digits dataset.</source>
          <target state="translated">如果训练得分和验证得分都很低,估计器将是欠拟合的。如果训练得分高而验证得分低,则估计器过拟合,否则工作得很好。训练得分低,验证得分高,通常是不可能的。这三种情况都可以在下面的图中找到,我们在数字数据集上改变一个SVM的参数\(\gamma\)。</target>
        </trans-unit>
        <trans-unit id="80e789647361ff21671194a00300fe312dc530d2" translate="yes" xml:space="preserve">
          <source>If the transformed output consists of a mix of sparse and dense data, it will be stacked as a sparse matrix if the density is lower than this value. Use &lt;code&gt;sparse_threshold=0&lt;/code&gt; to always return dense. When the transformed output consists of all sparse or all dense data, the stacked result will be sparse or dense, respectively, and this keyword will be ignored.</source>
          <target state="translated">如果转换后的输出包含稀疏和密集数据的混合，则如果密度低于此值，则它将作为稀疏矩阵堆叠。使用 &lt;code&gt;sparse_threshold=0&lt;/code&gt; 总是返回密集值。当转换后的输出包含所有稀疏数据或所有密集数据时，堆叠结果将分别为稀疏或密集数据，并且将忽略此关键字。</target>
        </trans-unit>
        <trans-unit id="efca83041c066057e65d83989c4190b74b69dba6" translate="yes" xml:space="preserve">
          <source>If the underlying graph has nodes with much more connections than the average node, the algorithm will miss some of these connections.</source>
          <target state="translated">如果底层图的节点的连接数比一般节点多得多,算法就会漏掉其中的一些连接。</target>
        </trans-unit>
        <trans-unit id="27f470c8c1d74aa01f92e63860f4d2b9149dd0bb" translate="yes" xml:space="preserve">
          <source>If there are few data points per dimension, noise in the observations induces high variance:</source>
          <target state="translated">如果每个维度的数据点很少,观测值中的噪声会诱发高方差。</target>
        </trans-unit>
        <trans-unit id="fbaec65eed1139944f6f906201326eaef7bc4d08" translate="yes" xml:space="preserve">
          <source>If there are more than two classes, \(f(x)\) itself would be a vector of size (n_classes,). Instead of passing through logistic function, it passes through the softmax function, which is written as,</source>
          <target state="translated">如果有两个以上的类,/(f(x)/)本身将是一个大小为(n_classes,)的向量。它不通过logistic函数,而是通过softmax函数,写成。</target>
        </trans-unit>
        <trans-unit id="e2c9b002eac60ce02e4a3cafb47196266855c43e" translate="yes" xml:space="preserve">
          <source>If there are more than two labels, &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; uses a multiclass variant due to Crammer &amp;amp; Singer. &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf&quot;&gt;Here&lt;/a&gt; is the paper describing it.</source>
          <target state="translated">如果标签不止两个，&lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt;由于Crammer＆Singer的缘故，铰链损耗会使用多类变体。&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf&quot;&gt;这&lt;/a&gt;是描述它的论文。</target>
        </trans-unit>
        <trans-unit id="362b6e0ad023f937918b9ce5c294c8243f2c8ea1" translate="yes" xml:space="preserve">
          <source>If there is a possibility that the training data might have missing categorical features, it can often be better to specify &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; instead of setting the &lt;code&gt;categories&lt;/code&gt; manually as above. When &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; is specified and unknown categories are encountered during transform, no error will be raised but the resulting one-hot encoded columns for this feature will be all zeros (&lt;code&gt;handle_unknown='ignore'&lt;/code&gt; is only supported for one-hot encoding):</source>
          <target state="translated">如果训练数据有可能缺少分类特征，则通常最好指定 &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; 而不是如上所述手动设置 &lt;code&gt;categories&lt;/code&gt; 。当指定 &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; 且在转换过程中遇到未知类别时，不会引发错误，但此功能生成的一键编码列将全为零（ &lt;code&gt;handle_unknown='ignore'&lt;/code&gt; 仅支持一键编码）：</target>
        </trans-unit>
        <trans-unit id="76c3aee0f8dcd7757eeddd2a91b271bc2108fb17" translate="yes" xml:space="preserve">
          <source>If there is more than one such value, only the first is returned. The bin-count for the modal bins is also returned.</source>
          <target state="translated">如果有多个这样的值,只返回第一个。同时返回模态仓的仓数。</target>
        </trans-unit>
        <trans-unit id="e4599c6e53b844db2376ed9e56ed7b49e63c6ec3" translate="yes" xml:space="preserve">
          <source>If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis or all the axes as before.</source>
          <target state="translated">如果这是一个ints的元组,则在多个轴上执行平均数,而不是像以前那样在一个轴或所有轴上执行。</target>
        </trans-unit>
        <trans-unit id="015e500928e7c3f86f2c9b5121c746246fcd7a9f" translate="yes" xml:space="preserve">
          <source>If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.</source>
          <target state="translated">如果设置为True,被缩小的轴将作为尺寸为1的尺寸留在结果中。使用这个选项,结果将正确地在输入数组中广播。</target>
        </trans-unit>
        <trans-unit id="1d47783a4de427039b4db643f305b3dd195e7ba2" translate="yes" xml:space="preserve">
          <source>If this split node has a parent subcluster and there is room for a new subcluster, then the parent is split into two. If there is no room, then this node is again split into two and the process is continued recursively, till it reaches the root.</source>
          <target state="translated">如果这个被拆分的节点有一个父子簇,并且有空间容纳一个新的子簇,那么父子簇就被一分为二。如果没有空间,那么这个节点又被一分为二,这个过程继续递归,直到到达根节点。</target>
        </trans-unit>
        <trans-unit id="012f5a7e85e6e4a424dae20b5f0c103c4fa516ee" translate="yes" xml:space="preserve">
          <source>If true (default), use a breadth-first approach to the problem. Otherwise use a depth-first approach.</source>
          <target state="translated">如果为true(默认),使用广度优先的方法来解决这个问题。否则使用深度优先的方法。</target>
        </trans-unit>
        <trans-unit id="8002efc50268110232cc0ffbdad97c50440caadd" translate="yes" xml:space="preserve">
          <source>If true, X and y will be centered.</source>
          <target state="translated">如果为真,X和y将居中。</target>
        </trans-unit>
        <trans-unit id="de73b79cb6d725781d21d3fce59be150e3f40a4c" translate="yes" xml:space="preserve">
          <source>If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. Ignored if seeds argument is not None.</source>
          <target state="translated">如果为真,初始核的位置不是所有点的位置,而是点的离散化版本的位置,在这个版本中,点被分层到一个网格上,其粗度与带宽相对应。将这个选项设置为True会加快算法的速度,因为需要初始化的种子较少。如果 seeds 参数不是 None,则忽略。</target>
        </trans-unit>
        <trans-unit id="382a329c7d03e3dc0838d8f846116bb309725e41" translate="yes" xml:space="preserve">
          <source>If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. default value: False Ignored if seeds argument is not None.</source>
          <target state="translated">如果为真,初始核位置不是所有点的位置,而是点的离散化版本的位置,点被分层到一个网格上,其粗度与带宽相对应。将此选项设置为True会加快算法的速度,因为会有更少的种子被初始化。 默认值。False 如果 seeds 参数不是 None,则忽略该选项。</target>
        </trans-unit>
        <trans-unit id="5f8f1503f4ad4447aabcfddb9ab2a5dcd7bfde79" translate="yes" xml:space="preserve">
          <source>If true, only interaction features are produced: features that are products of at most &lt;code&gt;degree&lt;/code&gt;&lt;em&gt;distinct&lt;/em&gt; input features (so not &lt;code&gt;x[1] ** 2&lt;/code&gt;, &lt;code&gt;x[0] * x[2] ** 3&lt;/code&gt;, etc.).</source>
          <target state="translated">如果为真，只有相互作用特征产生：即至多产品特征 &lt;code&gt;degree&lt;/code&gt; &lt;em&gt;不同&lt;/em&gt;的输入功能（因此不是 &lt;code&gt;x[1] ** 2&lt;/code&gt; ， &lt;code&gt;x[0] * x[2] ** 3&lt;/code&gt; 等）。</target>
        </trans-unit>
        <trans-unit id="2d6ce18ffe19be253728c95b7f8882b85215b418" translate="yes" xml:space="preserve">
          <source>If true, randomize the order of coordinates in the CD solver.</source>
          <target state="translated">如果为真,则随机化CD解算器中的坐标顺序。</target>
        </trans-unit>
        <trans-unit id="c9d7c7ecbdd08be425848806cc6f9d68c29d7323" translate="yes" xml:space="preserve">
          <source>If true, return the mean loss per sample. Otherwise, return the sum of the per-sample losses.</source>
          <target state="translated">如果为真,返回每个样本的平均损失。否则,返回每个样本损失的总和。</target>
        </trans-unit>
        <trans-unit id="6d32e9cabd3e10e0279ad9dd2b7c71514057bf52" translate="yes" xml:space="preserve">
          <source>If true, then all points are clustered, even those orphans that are not within any kernel. Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label -1.</source>
          <target state="translated">如果为真,那么所有的点都会被聚类,甚至是那些不在任何核内的孤核。孤儿被分配给最近的核。如果为假,那么孤儿被赋予簇标签-1。</target>
        </trans-unit>
        <trans-unit id="87535a59e28d16a49b32c83a6882f2aefd67503d" translate="yes" xml:space="preserve">
          <source>If true, use a dualtree algorithm. Otherwise, use a single-tree algorithm. Dual tree algorithms can have better scaling for large N.</source>
          <target state="translated">如果为真,则使用双树算法,否则,使用单树算法。否则,使用单树算法。双树算法对于大的N来说,可以有更好的伸缩性。</target>
        </trans-unit>
        <trans-unit id="a62f22814f97612f53d5c62d8ea50a484acea3fc" translate="yes" xml:space="preserve">
          <source>If two variables are almost equally correlated with the response, then their coefficients should increase at approximately the same rate. The algorithm thus behaves as intuition would expect, and also is more stable.</source>
          <target state="translated">如果两个变量与响应的相关性几乎相等,那么它们的系数应该以大致相同的速度增加。因此,该算法的表现与直觉预期的一样,也比较稳定。</target>
        </trans-unit>
        <trans-unit id="c6d813716240a58cb1e341ee096ed78ff4d17824" translate="yes" xml:space="preserve">
          <source>If verbose is True, the objective function and dual gap are plotted at each iteration.</source>
          <target state="translated">如果verbose为True,则在每次迭代时绘制目标函数和双倍差距。</target>
        </trans-unit>
        <trans-unit id="c845cf733c967b921d034159fbd6e6d551e8f5a1" translate="yes" xml:space="preserve">
          <source>If verbose is True, the objective function and dual gap are printed at each iteration.</source>
          <target state="translated">如果verbose为True,则在每次迭代时打印目标函数和双倍差距。</target>
        </trans-unit>
        <trans-unit id="d04bb65f95446e17ac813ad52d517cc52bee8bef" translate="yes" xml:space="preserve">
          <source>If verbose is True, the objective function and duality gap are printed at each iteration.</source>
          <target state="translated">如果verbose为True,则在每次迭代时打印目标函数和二元性差距。</target>
        </trans-unit>
        <trans-unit id="c297e2c2dea63aea70529d05594e08d33ba95930" translate="yes" xml:space="preserve">
          <source>If warm-starts are enabled, the solution of the last Newton iteration on the Laplace approximation of the posterior mode is used as initialization for the next call of _posterior_mode(). This can speed up convergence when _posterior_mode is called several times on similar problems as in hyperparameter optimization. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">如果启用了热启动，则后验模式的拉普拉斯近似上的最后一次牛顿迭代的解将用作下一个_posterior_mode（）调用的初始化。在与超参数优化中类似的问题上多次调用_posterior_mode时，这可以加快收敛速度​​。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0beda1307d8fba8bf455ad043421d0e1b5743334" translate="yes" xml:space="preserve">
          <source>If we consider the loss function to be the individual error per sample, then the data-fit term, or the sum of the error for each sample, will increase as we add more samples. The penalization term, however, will not increase.</source>
          <target state="translated">如果我们认为损失函数是每个样本的单个误差,那么数据拟合项,或每个样本的误差总和,将随着我们增加更多的样本而增加。然而,惩罚项不会增加。</target>
        </trans-unit>
        <trans-unit id="232b0b83965c86185ba5cb4047fca7ca1eb2e5e8" translate="yes" xml:space="preserve">
          <source>If we define &lt;code&gt;s = 1 / density&lt;/code&gt;, the elements of the random matrix are drawn from</source>
          <target state="translated">如果我们定义 &lt;code&gt;s = 1 / density&lt;/code&gt; ，则随机矩阵的元素将从</target>
        </trans-unit>
        <trans-unit id="1427e0ae27c6ada10257117ddf3e602836e812e6" translate="yes" xml:space="preserve">
          <source>If we note &lt;code&gt;s = 1 / density&lt;/code&gt; the components of the random matrix are drawn from:</source>
          <target state="translated">如果我们注意到 &lt;code&gt;s = 1 / density&lt;/code&gt; ，则随机矩阵的分量来自：</target>
        </trans-unit>
        <trans-unit id="f9cbc4d2964857d1865d4f91b696f12d3cce3cff" translate="yes" xml:space="preserve">
          <source>If we note \(n_{\max} = \max(n_{\mathrm{samples}}, n_{\mathrm{features}})\) and \(n_{\min} = \min(n_{\mathrm{samples}}, n_{\mathrm{features}})\), the time complexity of the randomized &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is \(O(n_{\max}^2 \cdot n_{\mathrm{components}})\) instead of \(O(n_{\max}^2 \cdot n_{\min})\) for the exact method implemented in &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">如果我们注意到\（n _ {\ max} = \ max（n _ {\ mathrm {samples}}，n _ {\ mathrm {features}}）\）\）和\（n _ {\ min} = \ min（n _ {\ mathrm {samples}}，n _ {\ mathrm {features}}）\），则随机&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;的时间复杂度为\（O（n _ {\ max} ^ 2 \ cdot n _ {\ mathrm {components}}）\）代替\（O（n _ {\ max} ^ 2 \ cdot n _ {\ min}）\）的值，以实现在&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; 中&lt;/a&gt;实现的确切方法。</target>
        </trans-unit>
        <trans-unit id="483854d9b52bcc6a7814b6c99e197398a7767c0e" translate="yes" xml:space="preserve">
          <source>If we use l2 shrinkage, as with the Ledoit-Wolf estimator, as the number of samples is small, we need to shrink a lot. As a result, the Ledoit-Wolf precision is fairly close to the ground truth precision, that is not far from being diagonal, but the off-diagonal structure is lost.</source>
          <target state="translated">如果我们使用l2收缩,就像Ledoit-Wolf估计器一样,由于样本数量很少,我们需要收缩很多。结果,Ledoit-Wolf的精度相当接近于地真精度,也就是离对角线不远,但失去了非对角线结构。</target>
        </trans-unit>
        <trans-unit id="c048d2d806d4fc664d0d49e2240da1f9038d7165" translate="yes" xml:space="preserve">
          <source>If we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this:</source>
          <target state="translated">如果我们想给数据拟合一个抛物线而不是一个平面,我们可以用二阶多项式来结合特征,这样的模型就像这样。</target>
        </trans-unit>
        <trans-unit id="9cd2bdd13889db844fd297c5019d226d5f2214ec" translate="yes" xml:space="preserve">
          <source>If we would restrict the model further, by assuming that the Gaussian noise is even isotropic (all diagonal entries are the same) we would obtain &lt;code&gt;PPCA&lt;/code&gt;.</source>
          <target state="translated">如果我们进一步限制模型，则通过假设高斯噪声是各向同性的（所有对角线入口都相同），我们将获得 &lt;code&gt;PPCA&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="722d8ecf13f293f3aeb1f206f30063d8afe3b1aa" translate="yes" xml:space="preserve">
          <source>If whiten is false, the data is already considered to be whitened, and no whitening is performed.</source>
          <target state="translated">如果whiten为假,则已经认为数据已经被白化,不进行白化。</target>
        </trans-unit>
        <trans-unit id="7d7146f3cf5f6ee3a12daad9561636b3070c19dc" translate="yes" xml:space="preserve">
          <source>If whitening is enabled, inverse_transform will compute the exact inverse operation, which includes reversing whitening.</source>
          <target state="translated">如果启用了白化,inverse_transform将计算精确的反操作,其中包括反白化。</target>
        </trans-unit>
        <trans-unit id="d6701cdf426d6a22381b3dd206332eab0defeb4b" translate="yes" xml:space="preserve">
          <source>If you apply SGD to features extracted using PCA we found that it is often wise to scale the feature values by some constant &lt;code&gt;c&lt;/code&gt; such that the average L2 norm of the training data equals one.</source>
          <target state="translated">如果将SGD应用于使用PCA提取的特征，我们发现通常明智的做法是将特征值按某个常数 &lt;code&gt;c&lt;/code&gt; 进行缩放，以使训练数据的平均L2范数等于1。</target>
        </trans-unit>
        <trans-unit id="88e7b5f8ea1b769958767cd4c4986cb0d84b69d4" translate="yes" xml:space="preserve">
          <source>If you are having trouble decoding text, here are some things to try:</source>
          <target state="translated">如果你在解码文本时遇到困难,这里有一些事情可以尝试。</target>
        </trans-unit>
        <trans-unit id="79c8dbf5a9dd8e0bcb21dbc0b3d21d4a002af1f5" translate="yes" xml:space="preserve">
          <source>If you are interested in controlling the L1 and L2 penalty separately, keep in mind that this is equivalent to:</source>
          <target state="translated">如果你对分别控制L1和L2的处罚感兴趣,请记住,这相当于。</target>
        </trans-unit>
        <trans-unit id="fda5569b1e927ca58d1243554ef8331577e43162" translate="yes" xml:space="preserve">
          <source>If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data.</source>
          <target state="translated">如果你没有提供一个先验词典,也没有使用分析器进行某种特征选择,那么特征的数量将等于分析数据发现的词汇量。</target>
        </trans-unit>
        <trans-unit id="44906a85511569286aafb87826155b3c2905ddf9" translate="yes" xml:space="preserve">
          <source>If you don&amp;rsquo;t have labels, try using &lt;a href=&quot;../../auto_examples/text/plot_document_clustering#sphx-glr-auto-examples-text-plot-document-clustering-py&quot;&gt;Clustering&lt;/a&gt; on your problem.</source>
          <target state="translated">如果没有标签，请尝试对问题使用&lt;a href=&quot;../../auto_examples/text/plot_document_clustering#sphx-glr-auto-examples-text-plot-document-clustering-py&quot;&gt;群集&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="1e3c9a76f4703e92f09f761bb01632ba0994c7fc" translate="yes" xml:space="preserve">
          <source>If you experience hanging subprocesses with &lt;code&gt;n_jobs&amp;gt;1&lt;/code&gt; or &lt;code&gt;n_jobs=-1&lt;/code&gt;, make sure you have a single-threaded BLAS library, or set &lt;code&gt;n_jobs=1&lt;/code&gt;, or upgrade to Python 3.4 which has a new version of &lt;code&gt;multiprocessing&lt;/code&gt; that should be immune to this problem.</source>
          <target state="translated">如果您遇到 &lt;code&gt;n_jobs&amp;gt;1&lt;/code&gt; 或 &lt;code&gt;n_jobs=-1&lt;/code&gt; 的子进程挂起，请确保您具有单线程BLAS库，或者将 &lt;code&gt;n_jobs=1&lt;/code&gt; 设置，或者升级到Python 3.4，该版本应具有新版本的 &lt;code&gt;multiprocessing&lt;/code&gt; ，因此对此不受影响。问题。</target>
        </trans-unit>
        <trans-unit id="b4d3f940390acd5b7c567c108aba27222ddceb7d" translate="yes" xml:space="preserve">
          <source>If you have a kernel matrix of a kernel \(K\) that computes a dot product in a feature space defined by function \(phi\), a &lt;a href=&quot;generated/sklearn.preprocessing.kernelcenterer#sklearn.preprocessing.KernelCenterer&quot;&gt;&lt;code&gt;KernelCenterer&lt;/code&gt;&lt;/a&gt; can transform the kernel matrix so that it contains inner products in the feature space defined by \(phi\) followed by removal of the mean in that space.</source>
          <target state="translated">如果您有一个内核\（K \）的内核矩阵，可以在函数\（phi \）定义的特征空间中计算点积，则&lt;a href=&quot;generated/sklearn.preprocessing.kernelcenterer#sklearn.preprocessing.KernelCenterer&quot;&gt; &lt;code&gt;KernelCenterer&lt;/code&gt; &lt;/a&gt;可以转换内核矩阵，以便它在定义的特征空间中包含内部乘积通过\（phi \），然后去除该空间中的均值。</target>
        </trans-unit>
        <trans-unit id="2615ef2dcc8005e7f8f1fe1a8b864f8c5c8b40ba" translate="yes" xml:space="preserve">
          <source>If you have an affinity matrix, such as a distance matrix, for which 0 means identical elements, and high values means very dissimilar elements, it can be transformed in a similarity matrix that is well suited for the algorithm by applying the Gaussian (RBF, heat) kernel:</source>
          <target state="translated">如果你有一个亲和矩阵,比如距离矩阵,对于这个矩阵,0代表相同的元素,高值代表非常不相似的元素,可以应用高斯(RBF,热)核将其转化为非常适合算法的相似度矩阵。</target>
        </trans-unit>
        <trans-unit id="fe35ae96a69c356c4c790a684b706493b567f769" translate="yes" xml:space="preserve">
          <source>If you have multiple labels per document, e.g categories, have a look at the &lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;Multiclass and multilabel section&lt;/a&gt;.</source>
          <target state="translated">如果每个文档有多个标签，例如类别，请查看&lt;a href=&quot;../../modules/multiclass#multiclass&quot;&gt;Multiclass and multilabel部分&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4c5f38a361bcbafd12cc29508483a444dfcf9728" translate="yes" xml:space="preserve">
          <source>If you have several classes to predict, an option often used is to fit one-versus-all classifiers and then use a voting heuristic for the final decision.</source>
          <target state="translated">如果你有好几个类要预测,经常使用的一个选项是拟合一个与所有分类器,然后使用投票启发式进行最终决策。</target>
        </trans-unit>
        <trans-unit id="f97615967054f5695c197161c38be5160cb380e9" translate="yes" xml:space="preserve">
          <source>If you need the raw values of the partial dependence function rather than the plots you can use the &lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.partial_dependence#sklearn.ensemble.partial_dependence.partial_dependence&quot;&gt;&lt;code&gt;partial_dependence&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">如果您需要局部依赖函数的原始值而不是绘图，则可以使用&lt;a href=&quot;generated/sklearn.ensemble.partial_dependence.partial_dependence#sklearn.ensemble.partial_dependence.partial_dependence&quot;&gt; &lt;code&gt;partial_dependence&lt;/code&gt; &lt;/a&gt;函数：</target>
        </trans-unit>
        <trans-unit id="57ce2ca8cd47ab25ffa05da2880829913ab0ea8d" translate="yes" xml:space="preserve">
          <source>If you really want to use &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you can instantiate the estimator with the &lt;code&gt;novelty&lt;/code&gt; parameter set to &lt;code&gt;True&lt;/code&gt; before fitting the estimator. In this case, &lt;code&gt;fit_predict&lt;/code&gt; is not available.</source>
          <target state="translated">如果您确实要使用&lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt;进行新颖性检测，即预测标签或计算新的看不见数据的异常分数，则可以在拟合估算器之前将 &lt;code&gt;novelty&lt;/code&gt; 参数设置为 &lt;code&gt;True&lt;/code&gt; 来实例化估算器。在这种情况下， &lt;code&gt;fit_predict&lt;/code&gt; 不可用。</target>
        </trans-unit>
        <trans-unit id="9f2c392b35ce2be9956cf3b6740e21a9cb0e7eb8" translate="yes" xml:space="preserve">
          <source>If you set load_content=True, you should also specify the encoding of the text using the &amp;lsquo;encoding&amp;rsquo; parameter. For many modern text files, &amp;lsquo;utf-8&amp;rsquo; will be the correct encoding. If you leave encoding equal to None, then the content will be made of bytes instead of Unicode, and you will not be able to use most functions in &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt;.</source>
          <target state="translated">如果设置load_content = True，则还应该使用'encoding'参数指定文本的编码。对于许多现代文本文件，&amp;ldquo; utf-8&amp;rdquo;将是正确的编码。如果保留等于None的编码，则内容将由字节而不是Unicode组成，并且您将无法使用 &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; 中的大多数功能。</target>
        </trans-unit>
        <trans-unit id="bc2de8e7176de3a7ec5f2ba66eebe7612bd92fb1" translate="yes" xml:space="preserve">
          <source>If you specify &lt;code&gt;max_depth=h&lt;/code&gt; then complete binary trees of depth &lt;code&gt;h&lt;/code&gt; will be grown. Such trees will have (at most) &lt;code&gt;2**h&lt;/code&gt; leaf nodes and &lt;code&gt;2**h - 1&lt;/code&gt; split nodes.</source>
          <target state="translated">如果指定 &lt;code&gt;max_depth=h&lt;/code&gt; ，则将增长深度为 &lt;code&gt;h&lt;/code&gt; 的完整二叉树。这样的树将（最多）具有 &lt;code&gt;2**h&lt;/code&gt; 叶节点和 &lt;code&gt;2**h - 1&lt;/code&gt; 分裂节点。</target>
        </trans-unit>
        <trans-unit id="b5e591b33a0bd24a7e9f3db1117e493d465fb600" translate="yes" xml:space="preserve">
          <source>If you use sparse data (i.e. data represented as sparse matrices), &lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt;&lt;code&gt;chi2&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt;&lt;code&gt;mutual_info_regression&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt;&lt;code&gt;mutual_info_classif&lt;/code&gt;&lt;/a&gt; will deal with the data without making it dense.</source>
          <target state="translated">如果你使用稀疏数据（即表示为稀疏矩阵数据），&lt;a href=&quot;generated/sklearn.feature_selection.chi2#sklearn.feature_selection.chi2&quot;&gt; &lt;code&gt;chi2&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_regression#sklearn.feature_selection.mutual_info_regression&quot;&gt; &lt;code&gt;mutual_info_regression&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.feature_selection.mutual_info_classif#sklearn.feature_selection.mutual_info_classif&quot;&gt; &lt;code&gt;mutual_info_classif&lt;/code&gt; &lt;/a&gt;将处理数据，而使其密集。</target>
        </trans-unit>
        <trans-unit id="a1ac2d367786359c2f555cec450b280865094e6b" translate="yes" xml:space="preserve">
          <source>If you want more control over stopping criteria or learning rate in SGD, or want to do additional monitoring, using &lt;code&gt;warm_start=True&lt;/code&gt; and &lt;code&gt;max_iter=1&lt;/code&gt; and iterating yourself can be helpful:</source>
          <target state="translated">如果您想更好地控制SGD中的停止条件或学习率，或者想进行其他监视，请使用 &lt;code&gt;warm_start=True&lt;/code&gt; 和 &lt;code&gt;max_iter=1&lt;/code&gt; 并自己进行迭代可能会有所帮助：</target>
        </trans-unit>
        <trans-unit id="5572f4380789e92c52811040d71f90082bdb6315" translate="yes" xml:space="preserve">
          <source>If you want to know more about these issues and explore other possible serialization methods, please refer to this &lt;a href=&quot;http://pyvideo.org/video/2566/pickles-are-for-delis-not-software&quot;&gt;talk by Alex Gaynor&lt;/a&gt;.</source>
          <target state="translated">如果您想更多地了解这些问题并探索其他可能的序列化方法，请参阅&lt;a href=&quot;http://pyvideo.org/video/2566/pickles-are-for-delis-not-software&quot;&gt;Alex Gaynor的演讲&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c9d28c176517636a408517ab464ebf5a8ed78a10" translate="yes" xml:space="preserve">
          <source>If your attributes have an intrinsic scale (e.g. word frequencies or indicator features) scaling is not needed.</source>
          <target state="translated">如果你的属性有一个内在的尺度(如词频或指标特征),就不需要缩放。</target>
        </trans-unit>
        <trans-unit id="1c54fdc8274e03b04e776296dd28d5ff9fd81085" translate="yes" xml:space="preserve">
          <source>If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use &lt;a href=&quot;generated/sklearn.preprocessing.robust_scale#sklearn.preprocessing.robust_scale&quot;&gt;&lt;code&gt;robust_scale&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.preprocessing.robustscaler#sklearn.preprocessing.RobustScaler&quot;&gt;&lt;code&gt;RobustScaler&lt;/code&gt;&lt;/a&gt; as drop-in replacements instead. They use more robust estimates for the center and range of your data.</source>
          <target state="translated">如果您的数据包含许多离群值，则使用数据的均值和方差进行缩放可能效果不佳。在这些情况下，您可以改用&lt;a href=&quot;generated/sklearn.preprocessing.robust_scale#sklearn.preprocessing.robust_scale&quot;&gt; &lt;code&gt;robust_scale&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.preprocessing.robustscaler#sklearn.preprocessing.RobustScaler&quot;&gt; &lt;code&gt;RobustScaler&lt;/code&gt; &lt;/a&gt;作为替代产品。他们对数据的中心和范围使用更可靠的估计。</target>
        </trans-unit>
        <trans-unit id="a10bfba29a759d89e81956a541262290109cf294" translate="yes" xml:space="preserve">
          <source>If your number of features is high, it may be useful to reduce it with an unsupervised step prior to supervised steps. Many of the &lt;a href=&quot;http://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning&quot;&gt;Unsupervised learning&lt;/a&gt; methods implement a &lt;code&gt;transform&lt;/code&gt; method that can be used to reduce the dimensionality. Below we discuss two specific example of this pattern that are heavily used.</source>
          <target state="translated">如果您的功能数量很多，则在进行有监督的步骤之前先通过无监督的步骤来减少功能可能会很有用。许多&lt;a href=&quot;http://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning&quot;&gt;无监督学习&lt;/a&gt;方法都实现了可用于降低维度的 &lt;code&gt;transform&lt;/code&gt; 方法。下面，我们讨论大量使用此模式的两个特定示例。</target>
        </trans-unit>
        <trans-unit id="933c257247173f2464c3cf8d4de4af2d678e5a7c" translate="yes" xml:space="preserve">
          <source>If your number of observations is not large compared to the number of edges in your underlying graph, you will not recover it.</source>
          <target state="translated">如果你的观测数与底层图中的边数相比并不多,你将无法恢复。</target>
        </trans-unit>
        <trans-unit id="4f34c772816074a373c0d5e918e2e9ac136448b7" translate="yes" xml:space="preserve">
          <source>Ignore the offset first bytes by seeking forward, then discarding the following bytes up until the next new line character.</source>
          <target state="translated">通过向前寻求忽略偏移的第一个字节,然后丢弃下面的字节,直到下一个新的行字符。</target>
        </trans-unit>
        <trans-unit id="78fee1435d74666b84850cd5e82c18229351da5d" translate="yes" xml:space="preserve">
          <source>Ignored</source>
          <target state="translated">Ignored</target>
        </trans-unit>
        <trans-unit id="1e65bb4eca2d3c71529c96890a4b735eb7dafeac" translate="yes" xml:space="preserve">
          <source>Ignored.</source>
          <target state="translated">Ignored.</target>
        </trans-unit>
        <trans-unit id="1e417badfc4d52f79664b451110854e41b4a0daf" translate="yes" xml:space="preserve">
          <source>Ignored. This parameter exists only for compatibility with sklearn.pipeline.Pipeline.</source>
          <target state="translated">忽略。这个参数只存在于与sklearn.pipeline.Pipeline的兼容性。</target>
        </trans-unit>
        <trans-unit id="2d34b7c897f7b41a0f0625575a2c9cc21b1078a7" translate="yes" xml:space="preserve">
          <source>Illustration of &lt;code&gt;Pipeline&lt;/code&gt; and &lt;code&gt;GridSearchCV&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;Pipeline&lt;/code&gt; 和 &lt;code&gt;GridSearchCV&lt;/code&gt; 的插图</target>
        </trans-unit>
        <trans-unit id="643998f34944846c305de7d49de1c3e80f814d2d" translate="yes" xml:space="preserve">
          <source>Illustration of Gaussian process classification (GPC) on the XOR dataset</source>
          <target state="translated">XOR数据集上的高斯过程分类(GPC)图解</target>
        </trans-unit>
        <trans-unit id="c2cd661f8089fd4df71dfb566ea137083aa22024" translate="yes" xml:space="preserve">
          <source>Illustration of how the performance of an estimator on unseen data (test data) is not the same as the performance on training data. As the regularization increases the performance on train decreases while the performance on test is optimal within a range of values of the regularization parameter. The example with an Elastic-Net regression model and the performance is measured using the explained variance a.k.a. R^2.</source>
          <target state="translated">说明估计器在未见数据(测试数据)上的性能与在训练数据上的性能如何不同。随着正则化程度的提高,训练数据的性能会下降,而测试数据的性能在正则化参数的范围内是最佳的。带有弹性网络回归模型的例子,性能是用解释方差也就是R^2来衡量的。</target>
        </trans-unit>
        <trans-unit id="5790a5aaa3a6c4543a820b9b12ce6d261eeb0581" translate="yes" xml:space="preserve">
          <source>Illustration of prior and posterior Gaussian process for different kernels</source>
          <target state="translated">不同核的前高斯过程和后高斯过程的说明。</target>
        </trans-unit>
        <trans-unit id="27c062ea4e410688effe23ac51313a8ab9a70f1c" translate="yes" xml:space="preserve">
          <source>Illustration of the effect of different regularization strategies for Gradient Boosting. The example is taken from Hastie et al 2009 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">说明了不同的正则化策略对梯度提升的影响。这个例子取自Hastie et al 2009 &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="5beb1d257bbb65ddb7ec568445a1c3acdcb42d37" translate="yes" xml:space="preserve">
          <source>Image denoising using dictionary learning</source>
          <target state="translated">利用字典学习进行图像去噪</target>
        </trans-unit>
        <trans-unit id="5ab7decf36c80b04aff06a11c0e8ef068c85a1b9" translate="yes" xml:space="preserve">
          <source>Image histogram</source>
          <target state="translated">图像直方图</target>
        </trans-unit>
        <trans-unit id="5c328038b14054033bab1147ef5d1ad234b3373d" translate="yes" xml:space="preserve">
          <source>Imagine you have three subjects, each with an associated number from 1 to 3:</source>
          <target state="translated">想象一下,你有三个科目,每个科目都有一个相关的数字,从1到3。</target>
        </trans-unit>
        <trans-unit id="8781d615fd77be9578225c40ac67b9471394cced" translate="yes" xml:space="preserve">
          <source>Implementation</source>
          <target state="translated">Implementation</target>
        </trans-unit>
        <trans-unit id="8d522809f4125f5930c1f4f77ec91f8735a003d8" translate="yes" xml:space="preserve">
          <source>Implementation based on &lt;code&gt;A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430&lt;/code&gt;</source>
          <target state="translated">基于 &lt;code&gt;A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="b6ac8df85fe47d2d00b4a78e1facdef4fbcae73b" translate="yes" xml:space="preserve">
          <source>Implementation of Support Vector Machine classifier using libsvm: the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does. Furthermore SVC multi-class mode is implemented using one vs one scheme while LinearSVC uses one vs the rest. It is possible to implement one vs the rest with SVC by using the &lt;a href=&quot;sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; wrapper. Finally SVC can fit dense data without memory copy if the input is C-contiguous. Sparse data will still incur memory copy though.</source>
          <target state="translated">支持向量机分类器使用libsvm的实现：内核可以是非线性的，但其SMO算法不能像LinearSVC那样扩展到大量样本。此外，SVC多类模式使用一种对一种方案实现，而LinearSVC使用一种对另一种方案。通过使用&lt;a href=&quot;sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; &lt;/a&gt;包装器，可以用SVC来实现一个。最后，如果输入是C连续的，则SVC可以适合密集数据而无需复制内存。稀疏数据仍然会导致内存复制。</target>
        </trans-unit>
        <trans-unit id="78b58091d5da65fb36aa747d9975841d97302dec" translate="yes" xml:space="preserve">
          <source>Implementation of Support Vector Machine classifier using the same library as this class (liblinear).</source>
          <target state="translated">支持向量机分类器的实现,使用与该类相同的库(liblinear)。</target>
        </trans-unit>
        <trans-unit id="0faf8832b17d93a1b230f7a6ca4feb36d15cc4ac" translate="yes" xml:space="preserve">
          <source>Implementation of Support Vector Machine regression using libsvm: the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does.</source>
          <target state="translated">使用libsvm实现支持向量机回归:内核可以是非线性的,但其SMO算法不能像LinearSVC那样扩展到大量的样本。</target>
        </trans-unit>
        <trans-unit id="adae10003f16f5885f71700e866f2cc76e2c6af9" translate="yes" xml:space="preserve">
          <source>Implements feature hashing, aka the hashing trick.</source>
          <target state="translated">实现特征哈希,也就是哈希技巧。</target>
        </trans-unit>
        <trans-unit id="d98e09b894119d4a2d55bf3e2f04052ec103359a" translate="yes" xml:space="preserve">
          <source>Implements resampling with replacement. If False, this will implement (sliced) random permutations.</source>
          <target state="translated">实现重采样与替换。如果为False,将实现(切片)随机排列。</target>
        </trans-unit>
        <trans-unit id="9f9d0b6a3b9dbc770ff8e17c3a6979d6ebb5425d" translate="yes" xml:space="preserve">
          <source>Implements the Birch clustering algorithm.</source>
          <target state="translated">实施Birch聚类算法。</target>
        </trans-unit>
        <trans-unit id="82028db75262dc1a82a0dc4cf2e6f254032ff9f7" translate="yes" xml:space="preserve">
          <source>Implements the incremental PCA model from: &lt;code&gt;D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.&lt;/code&gt; See &lt;a href=&quot;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&quot;&gt;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&lt;/a&gt;</source>
          <target state="translated">从以下方面实现增量PCA模型： &lt;code&gt;D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.&lt;/code&gt; 请参阅&lt;a href=&quot;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&quot;&gt;http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="06be3bf25c44efb35fdd12e8816c051b94a6e5d6" translate="yes" xml:space="preserve">
          <source>Implements the probabilistic PCA model from: &lt;a href=&quot;#id1&quot;&gt;&lt;span id=&quot;id2&quot;&gt;`&lt;/span&gt;&lt;/a&gt;Tipping, M. E., and Bishop, C. M. (1999). &amp;ldquo;Probabilistic principal component analysis&amp;rdquo;. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3), 611-622. via the score and score_samples methods. See &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;</source>
          <target state="translated">通过以下方式实现概率PCA模型：&lt;a href=&quot;#id1&quot;&gt;&lt;span id=&quot;id2&quot;&gt;`&lt;/span&gt;&lt;/a&gt; Tipping，ME和Bishop，CM（1999）。&amp;ldquo;概率主成分分析&amp;rdquo;。皇家统计学会杂志：B系列（统计方法），61（3），611-622。通过score和score_samples方法。参见&lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="496d8573358dc0bbf8fad0d466b95c37d153e5fd" translate="yes" xml:space="preserve">
          <source>Importance of Feature Scaling</source>
          <target state="translated">特征缩放的重要性</target>
        </trans-unit>
        <trans-unit id="dee0fbd7a096536203f3e083c7a95f20ef772057" translate="yes" xml:space="preserve">
          <source>Important members are fit, predict.</source>
          <target state="translated">重要成员适合,预测。</target>
        </trans-unit>
        <trans-unit id="0004bf233145469d6159f141af0ae0b05f3c5e9a" translate="yes" xml:space="preserve">
          <source>Imputation transformer for completing missing values.</source>
          <target state="translated">用于完成缺失值的推演变压器。</target>
        </trans-unit>
        <trans-unit id="8154b566118976ff2097cfffb2c92470797b0a69" translate="yes" xml:space="preserve">
          <source>Impute all missing values in X.</source>
          <target state="translated">输入X中的所有缺失值。</target>
        </trans-unit>
        <trans-unit id="510c592fb9a4fd828788fc0bdd902c165ca78889" translate="yes" xml:space="preserve">
          <source>Imputing missing values before building an estimator</source>
          <target state="translated">在建立估计器之前,计算缺失值。</target>
        </trans-unit>
        <trans-unit id="e5d148df74ab3f703a9d283fda0c99f4936ff674" translate="yes" xml:space="preserve">
          <source>In &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt;, L1 and L2 priors can be added to the loss function in order to regularize the model. The L2 prior uses the Frobenius norm, while the L1 prior uses an elementwise L1 norm. As in &lt;code&gt;ElasticNet&lt;/code&gt;, we control the combination of L1 and L2 with the &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) parameter, and the intensity of the regularization with the &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) parameter. Then the priors terms are:</source>
          <target state="translated">在&lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; 中&lt;/a&gt;，可以将L1和L2先验值添加到损失函数中，以便对模型进行正则化。L2先验使用Frobenius范数，而L1先验使用元素级L1范数。就像在 &lt;code&gt;ElasticNet&lt;/code&gt; 中一样，我们使用 &lt;code&gt;l1_ratio&lt;/code&gt; （\（\ rho \））参数控制L1和L2的组合，并使用 &lt;code&gt;alpha&lt;/code&gt; （\（\ alpha \））参数控制正则化的强度。然后，先验条件为：</target>
        </trans-unit>
        <trans-unit id="eee03375c59654f18a55a7961e4f26a36fbc2cee" translate="yes" xml:space="preserve">
          <source>In &lt;a href=&quot;generated/sklearn.multiclass.outputcodeclassifier#sklearn.multiclass.OutputCodeClassifier&quot;&gt;&lt;code&gt;OutputCodeClassifier&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;code_size&lt;/code&gt; attribute allows the user to control the number of classifiers which will be used. It is a percentage of the total number of classes.</source>
          <target state="translated">在&lt;a href=&quot;generated/sklearn.multiclass.outputcodeclassifier#sklearn.multiclass.OutputCodeClassifier&quot;&gt; &lt;code&gt;OutputCodeClassifier&lt;/code&gt; 中&lt;/a&gt;， &lt;code&gt;code_size&lt;/code&gt; 属性允许用户控制将使用的分类器的数量。它是课程总数的百分比。</target>
        </trans-unit>
        <trans-unit id="92869ea1268ab85728d32cc145b2fc2a3cf98201" translate="yes" xml:space="preserve">
          <source>In &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;, if data for classification are unbalanced (e.g. many positive and few negative), set &lt;code&gt;class_weight='balanced'&lt;/code&gt; and/or try different penalty parameters &lt;code&gt;C&lt;/code&gt;.</source>
          <target state="translated">在&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt;中，如果用于分类数据是不平衡（例如许多积极的和几个阴性），组 &lt;code&gt;class_weight='balanced'&lt;/code&gt; 和/或尝试不同的惩罚参数 &lt;code&gt;C&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d4a2dd93e9c8bc18123ea577d336109c88e8c2c3" translate="yes" xml:space="preserve">
          <source>In &lt;strong&gt;averaging methods&lt;/strong&gt;, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.</source>
          <target state="translated">在&lt;strong&gt;平均方法中&lt;/strong&gt;，驱动原理是独立建立多个估计量，然后平均其预测。平均而言，由于组合估计量的方差减小，因此组合估计量通常比任何单个基本估计量都要好。</target>
        </trans-unit>
        <trans-unit id="baeac0931c0e4b4385579000935f2bb52ceb9f07" translate="yes" xml:space="preserve">
          <source>In a binary classification task, the terms &amp;lsquo;&amp;rsquo;positive&amp;rsquo;&amp;rsquo; and &amp;lsquo;&amp;rsquo;negative&amp;rsquo;&amp;rsquo; refer to the classifier&amp;rsquo;s prediction, and the terms &amp;lsquo;&amp;rsquo;true&amp;rsquo;&amp;rsquo; and &amp;lsquo;&amp;rsquo;false&amp;rsquo;&amp;rsquo; refer to whether that prediction corresponds to the external judgment (sometimes known as the &amp;lsquo;&amp;rsquo;observation&amp;rsquo;&amp;lsquo;). Given these definitions, we can formulate the following table:</source>
          <target state="translated">在二元分类任务中，术语``正''和``负''指分类器的预测，术语``真''和``假''指该预测是否对应于外部判断（有时也称为&amp;ldquo;观测&amp;rdquo;）。给定这些定义，我们可以制定下表：</target>
        </trans-unit>
        <trans-unit id="995a1ae8b5be72e5d8cbdf391052b665e77e9964" translate="yes" xml:space="preserve">
          <source>In a first step, the hierarchical clustering is performed without connectivity constraints on the structure and is solely based on distance, whereas in a second step the clustering is restricted to the k-Nearest Neighbors graph: it&amp;rsquo;s a hierarchical clustering with structure prior.</source>
          <target state="translated">第一步，在不对结构进行连接限制的情况下执行层次聚类，并且仅基于距离，而在第二步中，聚类仅限于k最近邻居图：它是具有结构优先级的层次聚类。</target>
        </trans-unit>
        <trans-unit id="0336cb4d8e7c9adb72abdea9417802db49cccd1f" translate="yes" xml:space="preserve">
          <source>In a large text corpus, some words will be very present (e.g. &amp;ldquo;the&amp;rdquo;, &amp;ldquo;a&amp;rdquo;, &amp;ldquo;is&amp;rdquo; in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.</source>
          <target state="translated">在大型文本语料库中，某些单词会非常出现（例如英语中的&amp;ldquo; the&amp;rdquo;，&amp;ldquo; a&amp;rdquo;，&amp;ldquo; is&amp;rdquo;），因此几乎没有关于文档实际内容的有意义的信息。如果我们将直接计数数据直接提供给分类器，那么那些非常频繁的术语会掩盖稀有但更有趣的术语的频率。</target>
        </trans-unit>
        <trans-unit id="cd0ed349168abd35a1fce87e6ae9e8ccc5ff58f4" translate="yes" xml:space="preserve">
          <source>In a nutshell, the following table summarizes the solvers characteristics:</source>
          <target state="translated">简而言之,下表总结了解算器的特点。</target>
        </trans-unit>
        <trans-unit id="b4ec4bcaff3e86d4d99c938f5623ab4b737e65c7" translate="yes" xml:space="preserve">
          <source>In a real world setting, the &lt;code&gt;n_features&lt;/code&gt; parameter can be left to its default value of &lt;code&gt;2 ** 20&lt;/code&gt; (roughly one million possible features). If memory or downstream models size is an issue selecting a lower value such as &lt;code&gt;2 **
18&lt;/code&gt; might help without introducing too many additional collisions on typical text classification tasks.</source>
          <target state="translated">在实际设置中， &lt;code&gt;n_features&lt;/code&gt; 参数可以保留为其默认值 &lt;code&gt;2 ** 20&lt;/code&gt; （大约一百万个可能的特征）。如果内存或下游模型的大小成为问题，则选择一个较低的值（例如 &lt;code&gt;2 ** 18&lt;/code&gt; 可能会有所帮助，而不会在典型的文本分类任务上引入过多的其他冲突。</target>
        </trans-unit>
        <trans-unit id="c75e295f24e05d06cacc1a2f9bb570a61bdd802e" translate="yes" xml:space="preserve">
          <source>In a similar manner, the boston housing data set is used to show the impact of transforming the targets before learning a model. In this example, the targets to be predicted corresponds to the weighted distances to the five Boston employment centers.</source>
          <target state="translated">以类似的方式,波士顿住房数据集被用来显示在学习模型之前转换目标的影响。在这个例子中,要预测的目标对应于到五个波士顿就业中心的加权距离。</target>
        </trans-unit>
        <trans-unit id="c210295417ef2f29cc593be46bbc5efed5892a5f" translate="yes" xml:space="preserve">
          <source>In addition of using an imputing method, we can also keep an indication of the missing information using &lt;a href=&quot;../modules/generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt;&lt;code&gt;sklearn.impute.MissingIndicator&lt;/code&gt;&lt;/a&gt; which might carry some information.</source>
          <target state="translated">除了使用插补方法外，我们还可以使用&lt;a href=&quot;../modules/generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt; &lt;code&gt;sklearn.impute.MissingIndicator&lt;/code&gt; 保留&lt;/a&gt;可能包含某些信息的丢失信息的指示。</target>
        </trans-unit>
        <trans-unit id="45e3263783ee36dc90c5821ae7ffc8d210750151" translate="yes" xml:space="preserve">
          <source>In addition to its current contents, this module will eventually be home to refurbished versions of Pipeline and FeatureUnion.</source>
          <target state="translated">除了目前的内容,这个模块最终将成为Pipeline和FeatureUnion的翻新版本。</target>
        </trans-unit>
        <trans-unit id="3dda0db479e61a3dc2539c918fe85f072a3cc4a4" translate="yes" xml:space="preserve">
          <source>In addition to standard scikit-learn estimator API, GaussianProcessRegressor:</source>
          <target state="translated">除了标准的scikit-learn估计器API,GaussianProcessRegressor。</target>
        </trans-unit>
        <trans-unit id="6c259b1081efe473add72a6c01ee26dbc96ee486" translate="yes" xml:space="preserve">
          <source>In addition to the mean of the predictive distribution, also its standard deviation can be returned.</source>
          <target state="translated">除了预测分布的平均值外,还可以返回其标准差。</target>
        </trans-unit>
        <trans-unit id="f5f01da0b407208bd57121f71ed7408cbbce3fe6" translate="yes" xml:space="preserve">
          <source>In addition, as there is no useful information in the intensity of the image, or its gradient, we choose to perform the spectral clustering on a graph that is only weakly informed by the gradient. This is close to performing a Voronoi partition of the graph.</source>
          <target state="translated">此外,由于图像的强度或其梯度中没有有用的信息,我们选择在一个仅由梯度提供弱信息的图上执行光谱聚类。这接近于执行图的Voronoi分割。</target>
        </trans-unit>
        <trans-unit id="8be2789c3e2f5a1d410c34b62e20c28c8adc9fef" translate="yes" xml:space="preserve">
          <source>In addition, if the &lt;code&gt;dask&lt;/code&gt; and &lt;code&gt;distributed&lt;/code&gt; Python packages are installed, it is possible to use the &amp;lsquo;dask&amp;rsquo; backend for better scheduling of nested parallel calls without over-subscription and potentially distribute parallel calls over a networked cluster of several hosts.</source>
          <target state="translated">此外，如果安装了 &lt;code&gt;dask&lt;/code&gt; 和 &lt;code&gt;distributed&lt;/code&gt; Python软件包，则可以使用&amp;ldquo; dask&amp;rdquo;后端更好地调度嵌套并行调用，而不会超额预订，并且有可能在多个主机的网络群集上分配并行调用。</target>
        </trans-unit>
        <trans-unit id="1068dbee0dd3c16e2fc93e44b0ce455f5b052f8b" translate="yes" xml:space="preserve">
          <source>In addition, scikit-learn includes various random sample generators that can be used to build artificial datasets of controlled size and complexity.</source>
          <target state="translated">此外,scikit-learn还包括各种随机样本生成器,可以用来构建规模和复杂度可控的人工数据集。</target>
        </trans-unit>
        <trans-unit id="67ed28f1d0cd9fef0560dd0bbfe2b568680ea5a6" translate="yes" xml:space="preserve">
          <source>In addition, there are also miscellanous tools to load datasets of other formats or from other locations, described in the &lt;a href=&quot;#loading-other-datasets&quot;&gt;Loading other datasets&lt;/a&gt; section.</source>
          <target state="translated">此外，还有其他工具可用于加载其他格式或从其他位置&lt;a href=&quot;#loading-other-datasets&quot;&gt;加载数据集&lt;/a&gt;，如&amp;ldquo; 加载其他数据集&amp;rdquo;部分所述。</target>
        </trans-unit>
        <trans-unit id="846c6c3d11b49bd5243a8b72c066c7aae06dcfe3" translate="yes" xml:space="preserve">
          <source>In addition, we use the mask of the objects to restrict the graph to the outline of the objects. In this example, we are interested in separating the objects one from the other, and not from the background.</source>
          <target state="translated">此外,我们使用对象的掩码将图形限制在对象的轮廓上。在这个例子中,我们感兴趣的是将对象一个个分开,而不是与背景分开。</target>
        </trans-unit>
        <trans-unit id="b490744f01019d4237b3a8568465e031a5ae6e1f" translate="yes" xml:space="preserve">
          <source>In all these strategies, the &lt;code&gt;predict&lt;/code&gt; method completely ignores the input data.</source>
          <target state="translated">在所有这些策略中， &lt;code&gt;predict&lt;/code&gt; 方法完全忽略了输入数据。</target>
        </trans-unit>
        <trans-unit id="450c8a41f7c5da3bb2b7da9a15306b194b36c681" translate="yes" xml:space="preserve">
          <source>In an &lt;strong&gt;unsupervised setting&lt;/strong&gt; it can be used to group similar documents together by applying clustering algorithms such as &lt;a href=&quot;clustering#k-means&quot;&gt;K-means&lt;/a&gt;:</source>
          <target state="translated">在&lt;strong&gt;无人监督的情况下&lt;/strong&gt;，可以通过应用聚类算法（例如&lt;a href=&quot;clustering#k-means&quot;&gt;K-means）&lt;/a&gt;将相似的文档分组在一起：</target>
        </trans-unit>
        <trans-unit id="e5af8b183011d6bf7238d58f71b94384a5a96f84" translate="yes" xml:space="preserve">
          <source>In any case be warned that decreasing model complexity can hurt accuracy as mentioned above. For instance a non-linearly separable problem can be handled with a speedy linear model but prediction power will very likely suffer in the process.</source>
          <target state="translated">在任何情况下都要注意,如上所述,降低模型的复杂度会影响精度。例如,一个非线性可分离的问题可以用一个快速的线性模型来处理,但预测能力很可能在这个过程中受到影响。</target>
        </trans-unit>
        <trans-unit id="8c17ce8abfedb506f7ed46ebde27121f581c8bb7" translate="yes" xml:space="preserve">
          <source>In applications where a high false positive rate is not tolerable the parameter &lt;code&gt;max_fpr&lt;/code&gt; of &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt; can be used to summarize the ROC curve up to the given limit.</source>
          <target state="translated">在 &lt;code&gt;max_fpr&lt;/code&gt; 高误报率的应用中，可以使用roc_auc_score的&lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; &lt;/a&gt;参数总结ROC曲线，直至达到给定极限。</target>
        </trans-unit>
        <trans-unit id="bbcc07c440f8193b3e7ecf64eaaca0e386adcbad" translate="yes" xml:space="preserve">
          <source>In bin edges for feature &lt;code&gt;i&lt;/code&gt;, the first and last values are used only for &lt;code&gt;inverse_transform&lt;/code&gt;. During transform, bin edges are extended to:</source>
          <target state="translated">在特征 &lt;code&gt;i&lt;/code&gt; 的 bin边缘中，第一个和最后一个值仅用于 &lt;code&gt;inverse_transform&lt;/code&gt; 。在转换期间，bin边缘将扩展为：</target>
        </trans-unit>
        <trans-unit id="9d083c0b55e1a00133d4b27dd85f5ea26aca3922" translate="yes" xml:space="preserve">
          <source>In binary and multiclass classification, the Jaccard similarity coefficient score is equal to the classification accuracy.</source>
          <target state="translated">在二元分类和多类分类中,Jaccard相似度系数得分等于分类精度。</target>
        </trans-unit>
        <trans-unit id="b0b30958f6975dadd3d7eaf24f5447254d56c629" translate="yes" xml:space="preserve">
          <source>In binary and multiclass classification, this function is equal to the &lt;code&gt;jaccard_similarity_score&lt;/code&gt; function.</source>
          <target state="translated">在二进制和多类分类中，此函数等于 &lt;code&gt;jaccard_similarity_score&lt;/code&gt; 函数。</target>
        </trans-unit>
        <trans-unit id="e731c9654f26a8d7255165d2c0d5f78e47c3bd9a" translate="yes" xml:space="preserve">
          <source>In binary and multiclass classification, this function is equivalent to the &lt;code&gt;accuracy_score&lt;/code&gt;. It differs in the multilabel classification problem.</source>
          <target state="translated">在二进制和多类分类中，此函数等效于 &lt;code&gt;accuracy_score&lt;/code&gt; 。在多标签分类问题上有所不同。</target>
        </trans-unit>
        <trans-unit id="c4cb57bb3e2c0bb1485829473f6ca6362cee5b90" translate="yes" xml:space="preserve">
          <source>In binary class case, assuming labels in y_true are encoded with +1 and -1, when a prediction mistake is made, &lt;code&gt;margin = y_true * pred_decision&lt;/code&gt; is always negative (since the signs disagree), implying &lt;code&gt;1 - margin&lt;/code&gt; is always greater than 1. The cumulated hinge loss is therefore an upper bound of the number of mistakes made by the classifier.</source>
          <target state="translated">在二元类情况下，假设y_true中的标签用+1和-1编码，则在发生预测错误时， &lt;code&gt;margin = y_true * pred_decision&lt;/code&gt; 始终为负（因为符号不同），这意味着 &lt;code&gt;1 - margin&lt;/code&gt; 始终大于1。因此，累积的铰链损耗是分类器所犯错误数量的上限。</target>
        </trans-unit>
        <trans-unit id="f2c8a5d61695d64c32adbdec8051b4ecc7f404cb" translate="yes" xml:space="preserve">
          <source>In binary classification settings</source>
          <target state="translated">在二进制分类设置中</target>
        </trans-unit>
        <trans-unit id="0e8524872beef3003e748e1d9b4f90c7ce280313" translate="yes" xml:space="preserve">
          <source>In both cases, the criterion is evaluated once by epoch, and the algorithm stops when the criterion does not improve &lt;code&gt;n_iter_no_change&lt;/code&gt; times in a row. The improvement is evaluated with a tolerance &lt;code&gt;tol&lt;/code&gt;, and the algorithm stops in any case after a maximum number of iteration &lt;code&gt;max_iter&lt;/code&gt;.</source>
          <target state="translated">在这两种情况下，该标准都会由一个时期进行一次评估，并且当该标准没有连续提高 &lt;code&gt;n_iter_no_change&lt;/code&gt; 时间时，算法就会停止。改进以容差 &lt;code&gt;tol&lt;/code&gt; 进行评估，并且在最大迭代次数 &lt;code&gt;max_iter&lt;/code&gt; 之后，算法无论如何都将停止。</target>
        </trans-unit>
        <trans-unit id="f03bffae8069d1108a85252dc34a4485434865b8" translate="yes" xml:space="preserve">
          <source>In both cases, the kernel&amp;rsquo;s parameters are estimated using the maximum likelihood principle.</source>
          <target state="translated">在两种情况下，均使用最大似然原理估算内核参数。</target>
        </trans-unit>
        <trans-unit id="dc8004c8d5b437fc6c479e2e5882eb635fa97592" translate="yes" xml:space="preserve">
          <source>In both examples below, the main result is that the empirical covariance estimate, as a non-robust one, is highly influenced by the heterogeneous structure of the observations. Although the robust covariance estimate is able to focus on the main mode of the data distribution, it sticks to the assumption that the data should be Gaussian distributed, yielding some biased estimation of the data structure, but yet accurate to some extent. The One-Class SVM does not assume any parametric form of the data distribution and can therefore model the complex shape of the data much better.</source>
          <target state="translated">在下面的两个例子中,主要结果是,作为非稳健估计的经验协方差估计,受观测值的异质结构影响很大。虽然鲁棒协方差估计能够关注数据分布的主模式,但它坚持了数据应该是高斯分布的假设,得出的数据结构估计有一定的偏差,但却在一定程度上准确。One-Class SVM不假设数据分布的任何参数形式,因此能够更好地模拟数据的复杂形状。</target>
        </trans-unit>
        <trans-unit id="7f2b051010be4e679b8556fa182a1724fa1e49b9" translate="yes" xml:space="preserve">
          <source>In case the file contains a pairwise preference constraint (known as &amp;ldquo;qid&amp;rdquo; in the svmlight format) these are ignored unless the query_id parameter is set to True. These pairwise preference constraints can be used to constraint the combination of samples when using pairwise loss functions (as is the case in some learning to rank problems) so that only pairs with the same query_id value are considered.</source>
          <target state="translated">如果文件包含成对的首选项约束（在svmlight格式中称为&amp;ldquo; qid&amp;rdquo;），除非query_id参数设置为True，否则将忽略这些约束。当使用成对损失函数时（如某些学习排序问题的情况），这些成对偏好约束可用于约束样本组合，从而仅考虑具有相同query_id值的对。</target>
        </trans-unit>
        <trans-unit id="b2d3bbc7ab1d1edcd850a10f6fb01a251c14a28b" translate="yes" xml:space="preserve">
          <source>In case unknown categories are encountered (all zero&amp;rsquo;s in the one-hot encoding), &lt;code&gt;None&lt;/code&gt; is used to represent this category.</source>
          <target state="translated">如果遇到未知类别（在&amp;ldquo;一键编码&amp;rdquo;中全为零）， &lt;code&gt;None&lt;/code&gt; ）代表该类别。</target>
        </trans-unit>
        <trans-unit id="ca499264726caa99e06bab43fc3d4644ea84df01" translate="yes" xml:space="preserve">
          <source>In cases where not all of a pairwise distance matrix needs to be stored at once, this is used to calculate pairwise distances in &lt;code&gt;working_memory&lt;/code&gt;-sized chunks. If &lt;code&gt;reduce_func&lt;/code&gt; is given, it is run on each chunk and its return values are concatenated into lists, arrays or sparse matrices.</source>
          <target state="translated">在并非所有成对距离矩阵都需要一次存储的情况下，这用于计算 &lt;code&gt;working_memory&lt;/code&gt; 大小的块中的成对距离。如果给出了 &lt;code&gt;reduce_func&lt;/code&gt; ，它将在每个块上运行，并将其返回值连接到列表，数组或稀疏矩阵中。</target>
        </trans-unit>
        <trans-unit id="261de18f8066fcaced5cb3f145cb26c170301e09" translate="yes" xml:space="preserve">
          <source>In cases where the data is not uniformly sampled, radius-based neighbors classification in &lt;a href=&quot;generated/sklearn.neighbors.radiusneighborsclassifier#sklearn.neighbors.RadiusNeighborsClassifier&quot;&gt;&lt;code&gt;RadiusNeighborsClassifier&lt;/code&gt;&lt;/a&gt; can be a better choice. The user specifies a fixed radius \(r\), such that points in sparser neighborhoods use fewer nearest neighbors for the classification. For high-dimensional parameter spaces, this method becomes less effective due to the so-called &amp;ldquo;curse of dimensionality&amp;rdquo;.</source>
          <target state="translated">如果没有统一采样数据，则&lt;a href=&quot;generated/sklearn.neighbors.radiusneighborsclassifier#sklearn.neighbors.RadiusNeighborsClassifier&quot;&gt; &lt;code&gt;RadiusNeighborsClassifier&lt;/code&gt; 中&lt;/a&gt;基于半径的邻居分类可能是更好的选择。用户指定一个固定半径\（r \），这样稀疏邻域中的点将较少的最近邻用于分类。对于高维参数空间，此方法由于所谓的&amp;ldquo;维数诅咒&amp;rdquo;而变得无效。</target>
        </trans-unit>
        <trans-unit id="46149a533d1136e96a72fc2595f06ccb02814862" translate="yes" xml:space="preserve">
          <source>In certain cases Theil-Sen performs better than &lt;a href=&quot;../../modules/linear_model#ransac-regression&quot;&gt;RANSAC&lt;/a&gt; which is also a robust method. This is illustrated in the second example below where outliers with respect to the x-axis perturb RANSAC. Tuning the &lt;code&gt;residual_threshold&lt;/code&gt; parameter of RANSAC remedies this but in general a priori knowledge about the data and the nature of the outliers is needed. Due to the computational complexity of Theil-Sen it is recommended to use it only for small problems in terms of number of samples and features. For larger problems the &lt;code&gt;max_subpopulation&lt;/code&gt; parameter restricts the magnitude of all possible combinations of p subsample points to a randomly chosen subset and therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger problems with the drawback of losing some of its mathematical properties since it then works on a random subset.</source>
          <target state="translated">在某些情况下，Theil-Sen的性能优于&lt;a href=&quot;../../modules/linear_model#ransac-regression&quot;&gt;RANSAC&lt;/a&gt;，这也是一种可靠的方法。在下面的第二个示例中对此进行了说明，其中第二个示例相对于x轴扰动RANSAC。调整RANSAC 的 &lt;code&gt;residual_threshold&lt;/code&gt; 参数可解决此问题，但通常需要有关数据和异常值性质的先验知识。由于Theil-Sen的计算复杂性，建议仅将其用于样本数量和特征较小的问题。对于较大的问题， &lt;code&gt;max_subpopulation&lt;/code&gt; 参数将p个子采样点的所有可能组合的大小限制为随机选择的子集，因此也限制了运行时间。因此，Theil-Sen适用于较大的问题，因为它随后会在随机子集上工作，因此会失去一些数学特性。</target>
        </trans-unit>
        <trans-unit id="08403787ed9849b402f6d04f68a0bae46063dfaf" translate="yes" xml:space="preserve">
          <source>In contrast to &lt;a href=&quot;#id13&quot;&gt;Bayesian Ridge Regression&lt;/a&gt;, each coordinate of \(w_{i}\) has its own standard deviation \(\lambda_i\). The prior over all \(\lambda_i\) is chosen to be the same gamma distribution given by hyperparameters \(\lambda_1\) and \(\lambda_2\).</source>
          <target state="translated">与&lt;a href=&quot;#id13&quot;&gt;贝叶斯岭回归&lt;/a&gt;相反，\（w_ {i} \）的每个坐标都有其自己的标准偏差\（\ lambda_i \）。所有\（\ lambda_i \）的优先级被选择为由超参数\（\ lambda_1 \）和\（\ lambda_2 \）给出的相同伽玛分布。</target>
        </trans-unit>
        <trans-unit id="741dc2ca1b0b96b753a4293cdc66da483cb961b9" translate="yes" xml:space="preserve">
          <source>In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.</source>
          <target state="translated">与GridSearchCV不同的是,并不是所有的参数值都被试过,而是从指定的分布中抽取固定数量的参数设置。试用的参数设置的数量由n_iter给出。</target>
        </trans-unit>
        <trans-unit id="f7007cbebb915951a7329a621ec59e7bfd3c1528" translate="yes" xml:space="preserve">
          <source>In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities.</source>
          <target state="translated">与多数投票(硬投票)相比,软投票将类标签返回为预测概率之和的argmax。</target>
        </trans-unit>
        <trans-unit id="a5222e41535c7e60d0bed8020d5a39a4cdb9c58d" translate="yes" xml:space="preserve">
          <source>In contrast to the original publication &lt;a href=&quot;#b2001&quot; id=&quot;id6&quot;&gt;[B2001]&lt;/a&gt;, the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.</source>
          <target state="translated">与原始出版物&lt;a href=&quot;#b2001&quot; id=&quot;id6&quot;&gt;[B2001]相比&lt;/a&gt;，scikit-learn实现通过平均分类器的概率预测来组合分类器，而不是让每个分类器对单个分类投票。</target>
        </trans-unit>
        <trans-unit id="092465bd0b61837459fb29bf14c2dda6ed20e949" translate="yes" xml:space="preserve">
          <source>In contrast to the regression setting, the posterior of the latent function \(f\) is not Gaussian even for a GP prior since a Gaussian likelihood is inappropriate for discrete class labels. Rather, a non-Gaussian likelihood corresponding to the logistic link function (logit) is used. GaussianProcessClassifier approximates the non-Gaussian posterior with a Gaussian based on the Laplace approximation. More details can be found in Chapter 3 of &lt;a href=&quot;#rw2006&quot; id=&quot;id4&quot;&gt;[RW2006]&lt;/a&gt;.</source>
          <target state="translated">与回归设置相反，即使对于GP先验，潜在函数\（f \）的后验也不是高斯，因为高斯似然不适用于离散类标签。而是，使用与逻辑链接函数（logit）相对应的非高斯似然。 GaussianProcessClassifier基于Laplace逼近，以高斯近似非高斯后验。可以在&lt;a href=&quot;#rw2006&quot; id=&quot;id4&quot;&gt;[RW2006]的&lt;/a&gt;第3章中找到更多详细信息。</target>
        </trans-unit>
        <trans-unit id="2d7f12a42ea8277b24625f1aff8d53cb363a14e0" translate="yes" xml:space="preserve">
          <source>In contrast, if the conventional accuracy is above chance only because the classifier takes advantage of an imbalanced test set, then the balanced accuracy, as appropriate, will drop to \(\frac{1}{\text{n\_classes}}\).</source>
          <target state="translated">相反,如果传统的准确率高于偶然性,只是因为分类器利用了不平衡的测试集,那么平衡的准确率就会酌情降到/(frac{1}{text{n_classes}})。</target>
        </trans-unit>
        <trans-unit id="89611c1358b346353d5469c5670ea64fb02fcdd7" translate="yes" xml:space="preserve">
          <source>In descending order of quality, when trained (outside of this example) on all 4 features using 30 estimators and scored using 10 fold cross validation, we see:</source>
          <target state="translated">按照质量降序,当使用 30 个估计器对所有 4 个特征进行训练(本例之外),并使用 10 倍交叉验证进行评分时,我们看到。</target>
        </trans-unit>
        <trans-unit id="0732cca6c2251b860da4c331fa5748d479b14945" translate="yes" xml:space="preserve">
          <source>In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. These methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. In many cases, bagging methods constitute a very simple way to improve with respect to a single model, without making it necessary to adapt the underlying base algorithm. As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees).</source>
          <target state="translated">在合集算法中,袋法构成了一类算法,它在原始训练集的随机子集上建立多个黑盒估计器实例,然后将它们的各个预测汇总起来,形成最终的预测。这些方法被用作减少基础估计器(如决策树)方差的一种方法,通过在其构建过程中引入随机化,然后将其做成一个集合。在许多情况下,袋法构成了一种非常简单的方法来改进单一模型,而不需要调整基础算法。由于袋法提供了一种减少过拟合的方法,因此,袋法对强而复杂的模型(例如,完全开发的决策树)效果最好,而升压法通常对弱模型(例如,浅层决策树)效果最好。</target>
        </trans-unit>
        <trans-unit id="5305d1e9b70806a8391e61e804a0df6abd8f6cc5" translate="yes" xml:space="preserve">
          <source>In extending a binary metric to multiclass or multilabel problems, the data is treated as a collection of binary problems, one for each class. There are then a number of ways to average binary metric calculations across the set of classes, each of which may be useful in some scenario. Where available, you should select among these using the &lt;code&gt;average&lt;/code&gt; parameter.</source>
          <target state="translated">在将二进制度量标准扩展到多类或多标签问题时，数据被视为二进制问题的集合，每个类一个。然后，有多种方法可以对一组类的平均二进制度量计算取平均值，每种方法在某些情况下可能很有用。如果可用，您应该使用 &lt;code&gt;average&lt;/code&gt; 参数从中选择。</target>
        </trans-unit>
        <trans-unit id="e87cfc9dff0fe670bd40ebf7e26edaa15ca842ad" translate="yes" xml:space="preserve">
          <source>In extremely randomized trees (see &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt;&lt;code&gt;ExtraTreesClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.extratreesregressor#sklearn.ensemble.ExtraTreesRegressor&quot;&gt;&lt;code&gt;ExtraTreesRegressor&lt;/code&gt;&lt;/a&gt; classes), randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias:</source>
          <target state="translated">在&lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt; &lt;code&gt;ExtraTreesClassifier&lt;/code&gt; &lt;/a&gt;随机化的树中（请参见ExtraTreesClassifier和&lt;a href=&quot;generated/sklearn.ensemble.extratreesregressor#sklearn.ensemble.ExtraTreesRegressor&quot;&gt; &lt;code&gt;ExtraTreesRegressor&lt;/code&gt; &lt;/a&gt;类），随机性在计算拆分的方式上进一步向前迈进了一步。像在随机森林中一样，使用候选特征的随机子集，但不是寻找最有区别的阈值，而是为每个候选特征随机绘制阈值，并选择这些随机生成的阈值中的最佳阈值作为划分规则。这通常可以更大程度地减少模型的方差，但要以更大的偏差增加为代价：</target>
        </trans-unit>
        <trans-unit id="5c1305e3ce4cbb99adc8d313e42a43efab81ea5c" translate="yes" xml:space="preserve">
          <source>In fact, this dataset only has one version. The iris dataset on the other hand has multiple versions:</source>
          <target state="translated">事实上,这个数据集只有一个版本。而虹膜数据集则有多个版本。</target>
        </trans-unit>
        <trans-unit id="63493dde535d33b43819cf48666bb2a9620c2476" translate="yes" xml:space="preserve">
          <source>In french but still a reference: Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.</source>
          <target state="translated">法文,但仍可作为参考。Tenenhaus,M.(1998年)。La regression PLS:theorie et pratique。巴黎。Editions Technic。</target>
        </trans-unit>
        <trans-unit id="6e95c3ada3b2525ed5f608da19594b4a42ad3dc4" translate="yes" xml:space="preserve">
          <source>In general doing predictions in bulk (many instances at the same time) is more efficient for a number of reasons (branching predictability, CPU cache, linear algebra libraries optimizations etc.). Here we see on a setting with few features that independently of estimator choice the bulk mode is always faster, and for some of them by 1 to 2 orders of magnitude:</source>
          <target state="translated">一般来说,由于一些原因(分支可预测性、CPU缓存、线性代数库优化等),批量做预测(许多实例同时进行)更有效率。在这里,我们看到在少数特征的设置上,独立于估计器的选择,批量模式总是更快,而且对其中一些来说,快了1到2个数量级。</target>
        </trans-unit>
        <trans-unit id="d5f14cdf8cb9c0df1b6ffce69bd866cdeffd9355" translate="yes" xml:space="preserve">
          <source>In general, a learning problem considers a set of n &lt;a href=&quot;https://en.wikipedia.org/wiki/Sample_(statistics)&quot;&gt;samples&lt;/a&gt; of data and then tries to predict properties of unknown data. If each sample is more than a single number and, for instance, a multi-dimensional entry (aka &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_random_variable&quot;&gt;multivariate&lt;/a&gt; data), it is said to have several attributes or &lt;strong&gt;features&lt;/strong&gt;.</source>
          <target state="translated">通常，学习问题会考虑一组n 个数据&lt;a href=&quot;https://en.wikipedia.org/wiki/Sample_(statistics)&quot;&gt;样本&lt;/a&gt;，然后尝试预测未知数据的属性。如果每个样本都大于一个数字，例如是一个多维条目（也称为&lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_random_variable&quot;&gt;多元&lt;/a&gt;数据），则称其具有多个属性或&lt;strong&gt;特征&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="9cf7334c38597a2189c7af702ab9abdbe9f10093" translate="yes" xml:space="preserve">
          <source>In general, is a technique used for analyzing similarity or dissimilarity data. &lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt;&lt;code&gt;MDS&lt;/code&gt;&lt;/a&gt; attempts to model similarity or dissimilarity data as distances in a geometric spaces. The data can be ratings of similarity between objects, interaction frequencies of molecules, or trade indices between countries.</source>
          <target state="translated">通常，是一种用于分析相似性或不相似性数据的技术。&lt;a href=&quot;generated/sklearn.manifold.mds#sklearn.manifold.MDS&quot;&gt; &lt;code&gt;MDS&lt;/code&gt; &lt;/a&gt;尝试将相似性或不相似性数据建模为几何空间中的距离。数据可以是对象之间的相似性等级，分子的相互作用频率或国家之间的贸易指数。</target>
        </trans-unit>
        <trans-unit id="71aab6786f00490669e72ac36911ce2d2486dab4" translate="yes" xml:space="preserve">
          <source>In general, it is about to learn a rough, close frontier delimiting the contour of the initial observations distribution, plotted in embedding \(p\)-dimensional space. Then, if further observations lay within the frontier-delimited subspace, they are considered as coming from the same population than the initial observations. Otherwise, if they lay outside the frontier, we can say that they are abnormal with a given confidence in our assessment.</source>
          <target state="translated">一般来说,它是关于学习一个粗略的、接近的边界限定初始观测分布的轮廓,绘制在嵌入/(p)-维空间中。然后,如果进一步的观测值位于边界限定的子空间内,则认为它们来自于与初始观测值相同的群体。否则,如果它们位于边界之外,我们可以说它们是异常的,我们的评估具有给定的信心。</target>
        </trans-unit>
        <trans-unit id="c9bca25ec918e4e036ec8a37ec502896ec56d542" translate="yes" xml:space="preserve">
          <source>In general, learning algorithms benefit from standardization of the data set. If some outliers are present in the set, robust scalers or transformers are more appropriate. The behaviors of the different scalers, transformers, and normalizers on a dataset containing marginal outliers is highlighted in &lt;a href=&quot;../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;Compare the effect of different scalers on data with outliers&lt;/a&gt;.</source>
          <target state="translated">通常，学习算法受益于数据集的标准化。如果集合中存在一些异常值，则更适合使用健壮的缩放器或转换器。在&lt;a href=&quot;../auto_examples/preprocessing/plot_all_scaling#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py&quot;&gt;比较不同缩放器对数据与异常值的影响中，&lt;/a&gt;着重强调了不同缩放器，转换器和规范化器在包含边缘异常值的数据集上的行为。</target>
        </trans-unit>
        <trans-unit id="baeb2b7a43c2bc0dd04675c021d6ed663a58bf2d" translate="yes" xml:space="preserve">
          <source>In general, the run time cost to construct a balanced binary tree is \(O(n_{samples}n_{features}\log(n_{samples}))\) and query time \(O(\log(n_{samples}))\). Although the tree construction algorithm attempts to generate balanced trees, they will not always be balanced. Assuming that the subtrees remain approximately balanced, the cost at each node consists of searching through \(O(n_{features})\) to find the feature that offers the largest reduction in entropy. This has a cost of \(O(n_{features}n_{samples}\log(n_{samples}))\) at each node, leading to a total cost over the entire trees (by summing the cost at each node) of \(O(n_{features}n_{samples}^{2}\log(n_{samples}))\).</source>
          <target state="translated">一般来说,构建平衡二叉树的运行时间成本为/(O(n_{samples}n_{features}log(n_{samples}))/),查询时间为/(O(\log(n_{samples}))/)。虽然树的构造算法试图生成平衡的树,但它们不会总是平衡的。假设子树保持近似平衡,每个节点的成本包括通过搜索(O(n_{features}))来寻找能提供最大熵减少的特征。这在每个节点上的成本为 \(O(n_{features}n_{samples}/log(n_{samples}))\),导致整个树的总成本(通过将每个节点的成本相加)为 \(O(n_{features}n_{samples}^{2}/log(n_{samples}))\)。</target>
        </trans-unit>
        <trans-unit id="635895acc09f2d99381585bc2d144c9a66a85f3a" translate="yes" xml:space="preserve">
          <source>In gradient descent, the gradient \(\nabla Loss_{W}\) of the loss with respect to the weights is computed and deducted from \(W\). More formally, this is expressed as,</source>
          <target state="translated">在梯度下降中,计算出损失相对于权重的梯度(Loss_{W}),并从(W)中扣除。更正式地说,它表示为:</target>
        </trans-unit>
        <trans-unit id="2c51a2af5a19ac0ce7e4fb04fd6d887c03b6fecb" translate="yes" xml:space="preserve">
          <source>In high-dimensional spaces, linear classifiers often achieve excellent accuracy. For sparse binary data, BernoulliNB is particularly well-suited. The bottom row compares the decision boundary obtained by BernoulliNB in the transformed space with an ExtraTreesClassifier forests learned on the original data.</source>
          <target state="translated">在高维空间中,线性分类器通常能达到很好的精度。对于稀疏的二进制数据,BernoulliNB尤其适合。底部一行比较了BernoulliNB在变换空间中获得的决策边界与在原始数据上学习的ExtraTreesClassifier森林。</target>
        </trans-unit>
        <trans-unit id="7b577c96674cf299faa19ce0d11e2224d3c2c813" translate="yes" xml:space="preserve">
          <source>In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier.</source>
          <target state="translated">在多数投票中,特定样本的预测类标签是代表每个单独分类器预测的类标签的多数(模式)的类标签。</target>
        </trans-unit>
        <trans-unit id="589394183aec0e7af2afe4b456559f6baedc9992" translate="yes" xml:space="preserve">
          <source>In many cases it is thus recommended to carefully time and profile your feature extraction code as it may be a good place to start optimizing when your overall latency is too slow for your application.</source>
          <target state="translated">因此,在许多情况下,建议仔细地对你的特征提取代码进行时间和配置文件,因为当你的整体延迟对你的应用程序来说太慢时,它可能是一个很好的开始优化的地方。</target>
        </trans-unit>
        <trans-unit id="aeae04273a5ed1fc88f796de718e3c2190c04f0d" translate="yes" xml:space="preserve">
          <source>In many modeling scenarios, normality of the features in a dataset is desirable. Power transforms are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a Gaussian distribution as possible in order to stabilize variance and minimize skewness.</source>
          <target state="translated">在许多建模场景中,数据集中特征的正态性是可取的。功率变换是一系列参数化的单调变换,其目的是将来自任何分布的数据映射到尽可能接近高斯分布的数据中,以稳定方差并最大限度地减少偏斜。</target>
        </trans-unit>
        <trans-unit id="c82f65d47c3f4e11ad468a4165bdc787c51720a5" translate="yes" xml:space="preserve">
          <source>In many real-world examples, there are many ways to extract features from a dataset. Often it is beneficial to combine several methods to obtain good performance. This example shows how to use &lt;code&gt;FeatureUnion&lt;/code&gt; to combine features obtained by PCA and univariate selection.</source>
          <target state="translated">在许多实际示例中，有许多方法可以从数据集中提取要素。通常，组合几种方法以获得良好的性能是有益的。本示例说明如何使用 &lt;code&gt;FeatureUnion&lt;/code&gt; 组合通过PCA和单变量选择获得的特征。</target>
        </trans-unit>
        <trans-unit id="9c0b7f3861d3fe001968b978c49f3447d1233fa3" translate="yes" xml:space="preserve">
          <source>In mathematics, the Johnson-Lindenstrauss lemma is a result concerning low-distortion embeddings of points from high-dimensional into low-dimensional Euclidean space. The lemma states that a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved. The map used for the embedding is at least Lipschitz, and can even be taken to be an orthogonal projection.</source>
          <target state="translated">在数学中,约翰逊-林登斯特劳斯定律是一个关于高维空间中的点嵌入低维欧氏空间的低失真结果。该定律指出,高维空间中的一小部分点可以被嵌入到一个低维空间中,而点与点之间的距离几乎被保存下来。用于嵌入的映射至少是Lipschitz的,甚至可以认为是正交投影。</target>
        </trans-unit>
        <trans-unit id="35a3805825da50966c5f8cb649b1d2ea852b8f59" translate="yes" xml:space="preserve">
          <source>In maximizing the log-likelihood, the positive gradient makes the model prefer hidden states that are compatible with the observed training data. Because of the bipartite structure of RBMs, it can be computed efficiently. The negative gradient, however, is intractable. Its goal is to lower the energy of joint states that the model prefers, therefore making it stay true to the data. It can be approximated by Markov chain Monte Carlo using block Gibbs sampling by iteratively sampling each of \(v\) and \(h\) given the other, until the chain mixes. Samples generated in this way are sometimes referred as fantasy particles. This is inefficient and it is difficult to determine whether the Markov chain mixes.</source>
          <target state="translated">在最大化对数似然的过程中,正梯度使得模型更倾向于与观察到的训练数据兼容的隐藏状态。由于RBMs的二段式结构,它可以有效地计算。然而,负梯度是难以解决的。它的目标是降低模型偏好的联合状态的能量,因此使其保持对数据的真实性。它可以通过马尔科夫链蒙特卡洛使用块吉布斯抽样来近似,通过迭代抽样每一个给定的/(v/)和/(h/)/),直到链混合。以这种方式生成的样本有时被称为幻想粒子。这种方式效率很低,而且很难确定马尔科夫链是否混合。</target>
        </trans-unit>
        <trans-unit id="54db7da5f1b2e2f16e8f4dc3a375dac661b78213" translate="yes" xml:space="preserve">
          <source>In multi-label classification, the &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt; function is extended by averaging over the labels as &lt;a href=&quot;#average&quot;&gt;above&lt;/a&gt;.</source>
          <target state="translated">在多标签分类，&lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; &lt;/a&gt;功能是通过平均在标签为扩展&lt;a href=&quot;#average&quot;&gt;以上&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="d9be5dcb267dcb84c278d12d7b1a881ada760886" translate="yes" xml:space="preserve">
          <source>In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.</source>
          <target state="translated">在多标签分类中,这是子集准确率,这是一个苛刻的指标,因为你要求每个样本的每个标签集都能正确预测。</target>
        </trans-unit>
        <trans-unit id="9ff5420b9cd3095ee44bf9941c38c72dce6d517a" translate="yes" xml:space="preserve">
          <source>In multi-label settings</source>
          <target state="translated">在多标签设置中</target>
        </trans-unit>
        <trans-unit id="cf7a69d811fd496380ea6a3966d13bf17ca83f43" translate="yes" xml:space="preserve">
          <source>In multiclass and multilabel classification task, the notions of precision, recall, and F-measures can be applied to each label independently. There are a few ways to combine results across labels, specified by the &lt;code&gt;average&lt;/code&gt; argument to the &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt; (multilabel only), &lt;a href=&quot;generated/sklearn.metrics.f1_score#sklearn.metrics.f1_score&quot;&gt;&lt;code&gt;f1_score&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt;&lt;code&gt;fbeta_score&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt;&lt;code&gt;precision_recall_fscore_support&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.precision_score#sklearn.metrics.precision_score&quot;&gt;&lt;code&gt;precision_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.recall_score#sklearn.metrics.recall_score&quot;&gt;&lt;code&gt;recall_score&lt;/code&gt;&lt;/a&gt; functions, as described &lt;a href=&quot;#average&quot;&gt;above&lt;/a&gt;. Note that if all labels are included, &amp;ldquo;micro&amp;rdquo;-averaging in a multiclass setting will produce precision, recall and \(F\) that are all identical to accuracy. Also note that &amp;ldquo;weighted&amp;rdquo; averaging may produce an F-score that is not between precision and recall.</source>
          <target state="translated">在多类和多标签分类任务中，可以将精度，召回率和F量度的概念分别应用于每个标签。有几种方法可以横跨标签结果组合，由指定的 &lt;code&gt;average&lt;/code&gt; 参数来&lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt; &lt;code&gt;average_precision_score&lt;/code&gt; &lt;/a&gt;（多标记只），&lt;a href=&quot;generated/sklearn.metrics.f1_score#sklearn.metrics.f1_score&quot;&gt; &lt;code&gt;f1_score&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt; &lt;code&gt;fbeta_score&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.metrics.precision_recall_fscore_support#sklearn.metrics.precision_recall_fscore_support&quot;&gt; &lt;code&gt;precision_recall_fscore_support&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.metrics.precision_score#sklearn.metrics.precision_score&quot;&gt; &lt;code&gt;precision_score&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.metrics.recall_score#sklearn.metrics.recall_score&quot;&gt; &lt;code&gt;recall_score&lt;/code&gt; &lt;/a&gt;功能，如所描述的&lt;a href=&quot;#average&quot;&gt;上述&lt;/a&gt;。请注意，如果包括所有标签，则在多类设置中进行&amp;ldquo;微&amp;rdquo;平均将产生与精度相同的精度，查全率和\（F \）。另请注意，&amp;ldquo;加权&amp;rdquo;平均可能会产生介于精度和召回率之间的F分数。</target>
        </trans-unit>
        <trans-unit id="afc91520f5287da47360dcd6fd00b4fb446bcf96" translate="yes" xml:space="preserve">
          <source>In multiclass case, the function expects that either all the labels are included in y_true or an optional labels argument is provided which contains all the labels. The multilabel margin is calculated according to Crammer-Singer&amp;rsquo;s method. As in the binary case, the cumulated hinge loss is an upper bound of the number of mistakes made by the classifier.</source>
          <target state="translated">在多类情况下，该函数期望所有标签都包含在y_true中，或者提供一个包含所有标签的可选标签参数。多标签边距是根据Crammer-Singer方法计算的。与二元情况一样，累积的铰链损失是分类器所犯错误数量的上限。</target>
        </trans-unit>
        <trans-unit id="a7ec36140af641cfb5e4e5e11dec536798cfb2f8" translate="yes" xml:space="preserve">
          <source>In multiclass classification, the Hamming loss correspond to the Hamming distance between &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; which is equivalent to the subset &lt;code&gt;zero_one_loss&lt;/code&gt; function.</source>
          <target state="translated">在多类分类中，汉明损失对应于 &lt;code&gt;y_true&lt;/code&gt; 和 &lt;code&gt;y_pred&lt;/code&gt; 之间的汉明距离，该距离等效于子集 &lt;code&gt;zero_one_loss&lt;/code&gt; 函数。</target>
        </trans-unit>
        <trans-unit id="ff1916ae5265c4d87d1472e5cc3e0c2594a22de8" translate="yes" xml:space="preserve">
          <source>In multiclass classification, the Hamming loss corresponds to the Hamming distance between &lt;code&gt;y_true&lt;/code&gt; and &lt;code&gt;y_pred&lt;/code&gt; which is similar to the &lt;a href=&quot;#zero-one-loss&quot;&gt;Zero one loss&lt;/a&gt; function. However, while zero-one loss penalizes prediction sets that do not strictly match true sets, the Hamming loss penalizes individual labels. Thus the Hamming loss, upper bounded by the zero-one loss, is always between zero and one, inclusive; and predicting a proper subset or superset of the true labels will give a Hamming loss between zero and one, exclusive.</source>
          <target state="translated">在多类分类中，汉明损失对应于 &lt;code&gt;y_true&lt;/code&gt; 和 &lt;code&gt;y_pred&lt;/code&gt; 之间的汉明距离，类似于&lt;a href=&quot;#zero-one-loss&quot;&gt;零一损失&lt;/a&gt;函数。但是，虽然零一损失惩罚了不严格匹配真实集的预测集，但汉明损失惩罚了各个标签。因此，以零一损失为上限的汉明损失始终在零和一之间（包括零）。并预测正确标签的正确子集或超集将使汉明损失介于0和1之间（不包括）。</target>
        </trans-unit>
        <trans-unit id="cf7ce831a18d046dad4e38dc2cae92648b792778" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the &lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt;&lt;code&gt;zero_one_loss&lt;/code&gt;&lt;/a&gt; scores a subset as one if its labels strictly match the predictions, and as a zero if there are any errors. By default, the function returns the percentage of imperfectly predicted subsets. To get the count of such subsets instead, set &lt;code&gt;normalize&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;</source>
          <target state="translated">在多标签分类中，&lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt; &lt;code&gt;zero_one_loss&lt;/code&gt; &lt;/a&gt;如果其标签严格匹配预测，则将子集评分为1，如果存在任何错误，则评分为0。默认情况下，该函数返回不完全预测的子集的百分比。要获取此类子集的计数，请将 &lt;code&gt;normalize&lt;/code&gt; 设置为 &lt;code&gt;False&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="2cdc777c3fd9aacea19e984339f1423c55608098" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the Hamming loss is different from the subset zero-one loss. The zero-one loss considers the entire set of labels for a given sample incorrect if it does entirely match the true set of labels. Hamming loss is more forgiving in that it penalizes the individual labels.</source>
          <target state="translated">在多标签分类中,汉明损失与子集零一损失不同。零一损失认为,如果给定样本的整个标签集确实与真实的标签集完全匹配,那么它的标签集就不正确。汉明损失更宽容,它对单个标签进行惩罚。</target>
        </trans-unit>
        <trans-unit id="00e9bece59054d08c4ac787e06eeb4fc8070bdab" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.</source>
          <target state="translated">在多标签分类中,该函数返回子集精度。如果一个样本的整个预测标签集与真实标签集严格匹配,那么子集准确率为1.0;否则为0.0。</target>
        </trans-unit>
        <trans-unit id="7cd1b88a6c55666089bdc7543f7e259d70d5898d" translate="yes" xml:space="preserve">
          <source>In multilabel classification, the zero_one_loss function corresponds to the subset zero-one loss: for each sample, the entire set of labels must be correctly predicted, otherwise the loss for that sample is equal to one.</source>
          <target state="translated">在多标签分类中,zero_one_loss函数对应的是子集零一损失:对于每个样本,必须正确预测整个标签集,否则该样本的损失等于一。</target>
        </trans-unit>
        <trans-unit id="c56a96e702a01557c0cb1c7c6c5d254cdaebcc8b" translate="yes" xml:space="preserve">
          <source>In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must &lt;em&gt;exactly&lt;/em&gt; match the corresponding set of labels in y_true.</source>
          <target state="translated">在多标签分类中，此函数计算子集准确性：为样本预测的标签集必须与y_true中的相应标签集&lt;em&gt;完全&lt;/em&gt;匹配。</target>
        </trans-unit>
        <trans-unit id="3fad4287dcc0210ad8169708b233947ca706f077" translate="yes" xml:space="preserve">
          <source>In multilabel learning, each sample can have any number of ground truth labels associated with it. The goal is to give high scores and better rank to the ground truth labels.</source>
          <target state="translated">在多标签学习中,每个样本可以有任意数量的地真标签与之相关联。其目标是给地真标签以高分和更好的排名。</target>
        </trans-unit>
        <trans-unit id="9d6449537c42279d12e406059563c338784d06f3" translate="yes" xml:space="preserve">
          <source>In multilabel learning, the joint set of binary classification tasks is expressed with label binary indicator array: each sample is one row of a 2d array of shape (n_samples, n_classes) with binary values: the one, i.e. the non zero elements, corresponds to the subset of labels. An array such as &lt;code&gt;np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]])&lt;/code&gt; represents label 0 in the first sample, labels 1 and 2 in the second sample, and no labels in the third sample.</source>
          <target state="translated">在多标签学习中，二进制分类任务的联合集用标签二进制指示符数组表示：每个样本是具有二进制值的二维形状数组（n_samples，n_classes）的一行：一个（即非零元素）对应于标签的子集。诸如 &lt;code&gt;np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]])&lt;/code&gt; 类的数组在第一个样本中表示标签0，在第二个样本中表示标签1和2。 ，而第三个样本中没有标签。</target>
        </trans-unit>
        <trans-unit id="6c2c0f769c8a98dc6df3f2e7afe566ac80c0f339" translate="yes" xml:space="preserve">
          <source>In normal usage, the Calinski-Harabaz index is applied to the results of a cluster analysis.</source>
          <target state="translated">在通常情况下,Calinski-Harabaz指数适用于聚类分析的结果。</target>
        </trans-unit>
        <trans-unit id="5f0c7d20ec265094d1673fd625fd38165b384452" translate="yes" xml:space="preserve">
          <source>In normal usage, the Davies-Bouldin index is applied to the results of a cluster analysis as follows:</source>
          <target state="translated">在通常情况下,Davies-Bouldin指数应用于聚类分析的结果,具体如下。</target>
        </trans-unit>
        <trans-unit id="0488e7351783ef8ef785f4bdea49af8c75724adf" translate="yes" xml:space="preserve">
          <source>In normal usage, the Silhouette Coefficient is applied to the results of a cluster analysis.</source>
          <target state="translated">在通常情况下,剪影系数被应用于聚类分析的结果。</target>
        </trans-unit>
        <trans-unit id="af7916eabb756a4304309b1e18ceea097a7a5071" translate="yes" xml:space="preserve">
          <source>In order to address the wider task of Natural Language Understanding, the local structure of sentences and paragraphs should thus be taken into account. Many such models will thus be casted as &amp;ldquo;Structured output&amp;rdquo; problems which are currently outside of the scope of scikit-learn.</source>
          <target state="translated">为了解决自然语言理解的更广泛任务，因此应考虑句子和段落的局部结构。因此，许多这样的模型将被视为&amp;ldquo;结构化输出&amp;rdquo;问题，目前不在scikit-learn的范围内。</target>
        </trans-unit>
        <trans-unit id="819693d214fc959100941f9c2bf3cb570fc069ec" translate="yes" xml:space="preserve">
          <source>In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:</source>
          <target state="translated">为了解决这个问题,scikit-learn提供了从文本内容中提取数字特征的最常见方法的实用程序,即。</target>
        </trans-unit>
        <trans-unit id="5bdd52099ccc039c40b609f18b326c63aea62fae" translate="yes" xml:space="preserve">
          <source>In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation such as the implementations available in the &lt;code&gt;scipy.sparse&lt;/code&gt; package.</source>
          <target state="translated">为了能够将这样的矩阵存储在内存中并且还可以加快代数运算矩阵/向量，实现通常会使用稀疏表示，例如 &lt;code&gt;scipy.sparse&lt;/code&gt; 包中可用的实现。</target>
        </trans-unit>
        <trans-unit id="b0bf98f40bc311f4824763dea8c552bc0812d861" translate="yes" xml:space="preserve">
          <source>In order to feed predictive or clustering models with the text data, one first need to turn the text into vectors of numerical values suitable for statistical analysis. This can be achieved with the utilities of the &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; as demonstrated in the following example that extract &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;TF-IDF&lt;/a&gt; vectors of unigram tokens from a subset of 20news:</source>
          <target state="translated">为了向预测或聚类模型提供文本数据，首先需要将文本转换为适合统计分析的数值向量。可以使用 &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; 的实用程序来实现此目的，如以下示例所示，该示例从20news的子集中提取unigram令牌的&lt;a href=&quot;https://en.wikipedia.org/wiki/Tf-idf&quot;&gt;TF-IDF&lt;/a&gt;向量：</target>
        </trans-unit>
        <trans-unit id="a439a73e36b65ee0a94b3f1d9d89e3ac154697cf" translate="yes" xml:space="preserve">
          <source>In order to get faster execution times for this first example we will work on a partial dataset with only 4 categories out of the 20 available in the dataset:</source>
          <target state="translated">为了让第一个例子获得更快的执行时间,我们将在部分数据集上工作,在数据集的20个可用类别中,只有4个类别。</target>
        </trans-unit>
        <trans-unit id="da7edac191ef2f2a6bab6d167570c5dc3d626b83" translate="yes" xml:space="preserve">
          <source>In order to learn good latent representations from a small dataset, we artificially generate more labeled data by perturbing the training data with linear shifts of 1 pixel in each direction.</source>
          <target state="translated">为了从一个小的数据集中学习良好的潜伏表征,我们通过对训练数据进行每个方向1个像素的线性移动,人工生成更多的标签数据。</target>
        </trans-unit>
        <trans-unit id="6983d2c6ff1cbf277ea5d9522b128070bfd0a615" translate="yes" xml:space="preserve">
          <source>In order to make the vectorizer =&amp;gt; transformer =&amp;gt; classifier easier to work with, &lt;code&gt;scikit-learn&lt;/code&gt; provides a &lt;a href=&quot;../../modules/generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; class that behaves like a compound classifier:</source>
          <target state="translated">为了使矢量化器=&amp;gt;变压器=&amp;gt;分类器更易于使用， &lt;code&gt;scikit-learn&lt;/code&gt; 提供了行为类似于复合分类器的&lt;a href=&quot;../../modules/generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt;类：</target>
        </trans-unit>
        <trans-unit id="5257e11193f291f6f81d5d2347e3cbb71ec9f310" translate="yes" xml:space="preserve">
          <source>In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors.</source>
          <target state="translated">为了对文本文档进行机器学习,我们首先需要将文本内容转化为数字特征向量。</target>
        </trans-unit>
        <trans-unit id="7b973d24b18f4331d1cc68b945953f9c40c766fe" translate="yes" xml:space="preserve">
          <source>In order to predict the class labels based on the predicted class-probabilities (scikit-learn estimators in the VotingClassifier must support &lt;code&gt;predict_proba&lt;/code&gt; method):</source>
          <target state="translated">为了基于预测的类概率来预测类标签（VotingClassifier中的scikit-learn估计器必须支持 &lt;code&gt;predict_proba&lt;/code&gt; 方法）：</target>
        </trans-unit>
        <trans-unit id="a7ffbb7849ad7a74935991324e062c6b6722378d" translate="yes" xml:space="preserve">
          <source>In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf&amp;ndash;idf transform.</source>
          <target state="translated">为了将计数特征重新加权为适合分类器使用的浮点值，通常使用tf&amp;ndash;idf变换。</target>
        </trans-unit>
        <trans-unit id="4707665df8a323c1a68b209bc6166b3798e4ea75" translate="yes" xml:space="preserve">
          <source>In order to rebuild a similar model with future versions of scikit-learn, additional metadata should be saved along the pickled model:</source>
          <target state="translated">为了用未来版本的scikit-learn重建一个类似的模型,应该沿着腌制的模型保存额外的元数据。</target>
        </trans-unit>
        <trans-unit id="168239ecf279021917cbfef805f1d7d711ae1c44" translate="yes" xml:space="preserve">
          <source>In order to test if a classification score is significative a technique in repeating the classification procedure after randomizing, permuting, the labels. The p-value is then given by the percentage of runs for which the score obtained is greater than the classification score obtained in the first place.</source>
          <target state="translated">为了测试一个分类分数是否是有意义的,一种技术是在随机、换位、标签后重复分类程序。然后通过所获得的分数大于最初获得的分类分数的运行百分比给出p值。</target>
        </trans-unit>
        <trans-unit id="fdc8e1656ba1332f0933f9f656403151b15252d2" translate="yes" xml:space="preserve">
          <source>In other words, return an input X_original whose transform would be X.</source>
          <target state="translated">换句话说,返回一个输入X_original,其变换将是X。</target>
        </trans-unit>
        <trans-unit id="f84fbaf022a2c87e2f72b92c7b8059751d7f8963" translate="yes" xml:space="preserve">
          <source>In other words, we &lt;em&gt;decomposed&lt;/em&gt; matrix \(\mathbf{X}\).</source>
          <target state="translated">换句话说，我们&lt;em&gt;分解了&lt;/em&gt;矩阵\（\ mathbf {X} \）。</target>
        </trans-unit>
        <trans-unit id="573ad5780d66d8749d635925a4f90732aa002652" translate="yes" xml:space="preserve">
          <source>In particular Rosenberg and Hirschberg (2007) define the following two desirable objectives for any cluster assignment:</source>
          <target state="translated">特别是Rosenberg和Hirschberg(2007)为任何集群分配定义了以下两个理想目标:</target>
        </trans-unit>
        <trans-unit id="dafd8fff090495231531a6dce6a0d9bf23cd3c87" translate="yes" xml:space="preserve">
          <source>In particular in a &lt;strong&gt;supervised setting&lt;/strong&gt; it can be successfully combined with fast and scalable linear models to train &lt;strong&gt;document classifiers&lt;/strong&gt;, for instance:</source>
          <target state="translated">特别是在有&lt;strong&gt;监督的环境中，&lt;/strong&gt;它可以成功地与快速且可扩展的线性模型组合以训练&lt;strong&gt;文档分类器&lt;/strong&gt;，例如：</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
