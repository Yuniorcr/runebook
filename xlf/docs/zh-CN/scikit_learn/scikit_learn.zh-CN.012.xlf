<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="b70db829e86f8b0b87ee4b4ea9165e2800cb135e" translate="yes" xml:space="preserve">
          <source>In particular the interrogative form &amp;ldquo;Is this&amp;rdquo; is only present in the last document:</source>
          <target state="translated">特别是疑问句形式&amp;ldquo;这是&amp;rdquo;仅出现在最后的文档中：</target>
        </trans-unit>
        <trans-unit id="48a72aaef5f57348c3c02ddfbd84f34663c56133" translate="yes" xml:space="preserve">
          <source>In particular we name:</source>
          <target state="translated">我们特别指出:</target>
        </trans-unit>
        <trans-unit id="32bae48b70c29501503578b88b61dfab45b0637c" translate="yes" xml:space="preserve">
          <source>In particular, \(\nu = 3/2\):</source>
          <target state="translated">特别是,(3/2)。</target>
        </trans-unit>
        <trans-unit id="ff34c019cd4067dd0b8a1a6d1536db31ff351b58" translate="yes" xml:space="preserve">
          <source>In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA).</source>
          <target state="translated">特别是,截断的SVD工作在术语计数/tf-idf矩阵上,这些矩阵由sklearn.feature_extraction.text中的向量器返回。在这种情况下,它被称为潜在语义分析(LSA)。</target>
        </trans-unit>
        <trans-unit id="74d856933005d819af2a4b7d2ce24317b5186453" translate="yes" xml:space="preserve">
          <source>In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance when clusters are nested circles on the 2D plan.</source>
          <target state="translated">在实践中,当单个簇的结构是高度非凸的时候,或者更一般的情况下,当测量簇的中心和分布并不适合描述完整的簇时,光谱聚类是非常有用的。例如,当聚类是二维平面上的嵌套圆时。</target>
        </trans-unit>
        <trans-unit id="5662b7bb73c6d21ae298513382716bd3fe526ba2" translate="yes" xml:space="preserve">
          <source>In practice the local density is obtained from the k-nearest neighbors. The LOF score of an observation is equal to the ratio of the average local density of his k-nearest neighbors, and its own local density: a normal instance is expected to have a local density similar to that of its neighbors, while abnormal data are expected to have much smaller local density.</source>
          <target state="translated">在实践中,局部密度是由k个最近的邻居获得的。一个观测值的LOF分数等于他的k个最近邻居的平均局部密度和它自己的局部密度之比:一个正常的实例预计会有一个类似于其邻居的局部密度,而异常数据的局部密度会小得多。</target>
        </trans-unit>
        <trans-unit id="7ab811a62d63edef5c9695fca6cafd8cc266404c" translate="yes" xml:space="preserve">
          <source>In practice those estimates are stored as an attribute named &lt;code&gt;feature_importances_&lt;/code&gt; on the fitted model. This is an array with shape &lt;code&gt;(n_features,)&lt;/code&gt; whose values are positive and sum to 1.0. The higher the value, the more important is the contribution of the matching feature to the prediction function.</source>
          <target state="translated">实际上，这些估计值被存储为拟合模型上名为 &lt;code&gt;feature_importances_&lt;/code&gt; 的属性。这是一个形状为 &lt;code&gt;(n_features,)&lt;/code&gt; 的数组，其值是正数，总和为1.0。值越高，匹配特征对预测函数的贡献就越重要。</target>
        </trans-unit>
        <trans-unit id="2f91c327e74b19930bc0b8d2d9c2f5d99fe44af7" translate="yes" xml:space="preserve">
          <source>In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.</source>
          <target state="translated">在实际工作中,我们常常忽略分布的形状,只需将数据进行变换,将每个特征的均值去掉,然后用非常态特征除以其标准差,将数据居中。</target>
        </trans-unit>
        <trans-unit id="4c632dd9d37d8e850afe2fbbbdbddfedb108d119" translate="yes" xml:space="preserve">
          <source>In practice, \(\mu\) and \(\Sigma\) are replaced by some estimates. The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set and therefor, the corresponding Mahalanobis distances are. One would better have to use a robust estimator of covariance to guarantee that the estimation is resistant to &amp;ldquo;erroneous&amp;rdquo; observations in the data set and that the associated Mahalanobis distances accurately reflect the true organisation of the observations.</source>
          <target state="translated">实际上，\（\ mu \）和\（\ Sigma \）被一些估计值所代替。通常的协方差最大似然估计对数据集中异常值的存在非常敏感，因此相应的马氏距离是。最好使用健壮的协方差估计器来确保估计值可以抵抗数据集中的&amp;ldquo;错误&amp;rdquo;观测值，并且相关的Mahalanobis距离可以准确地反映观测值的真实组织。</target>
        </trans-unit>
        <trans-unit id="7f19bfe5f66f3783151b8147191d95599d9b587d" translate="yes" xml:space="preserve">
          <source>In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That&amp;rsquo;s why it can be useful to restart it several times.</source>
          <target state="translated">实际上，k-means算法非常快（是可用的最快的聚类算法之一），但它属于局部最小值。这就是为什么多次重新启动它会很有用。</target>
        </trans-unit>
        <trans-unit id="a9809378b0338436bd7bbe8f2e2070a6272b570d" translate="yes" xml:space="preserve">
          <source>In problems where it is desired to give more importance to certain classes or certain individual samples keywords &lt;code&gt;class_weight&lt;/code&gt; and &lt;code&gt;sample_weight&lt;/code&gt; can be used.</source>
          <target state="translated">在希望更加重视某些类或某些单独样本的 &lt;code&gt;class_weight&lt;/code&gt; ，可以使用class_weight和 &lt;code&gt;sample_weight&lt;/code&gt; 关键字。</target>
        </trans-unit>
        <trans-unit id="cae7f41bfdb577edc832687cbceee9865d3220c5" translate="yes" xml:space="preserve">
          <source>In random forests (see &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;RandomForestClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt;&lt;code&gt;RandomForestRegressor&lt;/code&gt;&lt;/a&gt; classes), each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. In addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. As a result of this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.</source>
          <target state="translated">在随机森林中（请参见&lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt; &lt;code&gt;RandomForestClassifier&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.ensemble.randomforestregressor#sklearn.ensemble.RandomForestRegressor&quot;&gt; &lt;code&gt;RandomForestRegressor&lt;/code&gt; &lt;/a&gt;类），集合中的每棵树都是根据训练集中替换后的样本（即引导样本）构建的。此外，在树的构造过程中拆分节点时，选择的拆分不再是所有要素中的最佳拆分。取而代之的是，选取的分割是要素的随机子集中最佳的分割。由于这种随机性，森林的偏见通常会略有增加（相对于单个非随机树的偏见），但是由于求平均值，其方差也会减少，通常大于补偿偏见的增加，因此产生了一个整体更好的模型。</target>
        </trans-unit>
        <trans-unit id="eee63fdeeb00f1867cdf7e3f336a4276e1d12ae2" translate="yes" xml:space="preserve">
          <source>In regression, the expected mean squared error of an estimator can be decomposed in terms of bias, variance and noise. On average over datasets of the regression problem, the bias term measures the average amount by which the predictions of the estimator differ from the predictions of the best possible estimator for the problem (i.e., the Bayes model). The variance term measures the variability of the predictions of the estimator when fit over different instances LS of the problem. Finally, the noise measures the irreducible part of the error which is due the variability in the data.</source>
          <target state="translated">在回归中,估计器的预期均方误差可以用偏差、方差和噪声来分解。在回归问题的平均数据集上,偏差项衡量估计器的预测与问题的最佳估计器(即贝叶斯模型)的预测的平均差异量。方差项衡量估计器的预测在问题的不同实例LS上拟合时的变异性。最后,噪声衡量误差中不可重复的部分,这是由于数据的变异性造成的。</target>
        </trans-unit>
        <trans-unit id="3eae88b0a075df5d4090eee16e911849fedcc7b3" translate="yes" xml:space="preserve">
          <source>In regression, the output remains as \(f(x)\); therefore, output activation function is just the identity function.</source>
          <target state="translated">在回归中,输出仍为/(f(x)/);因此,输出激活函数只是身份函数。</target>
        </trans-unit>
        <trans-unit id="253be6f032627aec5a3b2c4240659e2190b9fba2" translate="yes" xml:space="preserve">
          <source>In scikit-learn a random split into training and test sets can be quickly computed with the &lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt;&lt;code&gt;train_test_split&lt;/code&gt;&lt;/a&gt; helper function. Let&amp;rsquo;s load the iris data set to fit a linear support vector machine on it:</source>
          <target state="translated">在scikit-learn中，可以使用&lt;a href=&quot;generated/sklearn.model_selection.train_test_split#sklearn.model_selection.train_test_split&quot;&gt; &lt;code&gt;train_test_split&lt;/code&gt; &lt;/a&gt;辅助函数快速将随机分为训练和测试集的部分进行计算。让我们加载虹膜数据集以使其适合线性支持向量机：</target>
        </trans-unit>
        <trans-unit id="bdcdb5bf0b220e10633a04e3b3d7b23fb832fcf9" translate="yes" xml:space="preserve">
          <source>In scikit-learn, an estimator for classification is a Python object that implements the methods &lt;code&gt;fit(X, y)&lt;/code&gt; and &lt;code&gt;predict(T)&lt;/code&gt;.</source>
          <target state="translated">在scikit-learn中，用于分类的估计量是一个Python对象，该对象实现了 &lt;code&gt;fit(X, y)&lt;/code&gt; 和 &lt;code&gt;predict(T)&lt;/code&gt; 方法。</target>
        </trans-unit>
        <trans-unit id="a15700735758e7e1606b13cd4f276e5fe1e0ef96" translate="yes" xml:space="preserve">
          <source>In scikit-learn, bagging methods are offered as a unified &lt;a href=&quot;generated/sklearn.ensemble.baggingclassifier#sklearn.ensemble.BaggingClassifier&quot;&gt;&lt;code&gt;BaggingClassifier&lt;/code&gt;&lt;/a&gt; meta-estimator (resp. &lt;a href=&quot;generated/sklearn.ensemble.baggingregressor#sklearn.ensemble.BaggingRegressor&quot;&gt;&lt;code&gt;BaggingRegressor&lt;/code&gt;&lt;/a&gt;), taking as input a user-specified base estimator along with parameters specifying the strategy to draw random subsets. In particular, &lt;code&gt;max_samples&lt;/code&gt; and &lt;code&gt;max_features&lt;/code&gt; control the size of the subsets (in terms of samples and features), while &lt;code&gt;bootstrap&lt;/code&gt; and &lt;code&gt;bootstrap_features&lt;/code&gt; control whether samples and features are drawn with or without replacement. When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting &lt;code&gt;oob_score=True&lt;/code&gt;. As an example, the snippet below illustrates how to instantiate a bagging ensemble of &lt;code&gt;KNeighborsClassifier&lt;/code&gt; base estimators, each built on random subsets of 50% of the samples and 50% of the features.</source>
          <target state="translated">在scikit-learn中，装袋方法以统一的&lt;a href=&quot;generated/sklearn.ensemble.baggingclassifier#sklearn.ensemble.BaggingClassifier&quot;&gt; &lt;code&gt;BaggingClassifier&lt;/code&gt; &lt;/a&gt;元估计器（即&lt;a href=&quot;generated/sklearn.ensemble.baggingregressor#sklearn.ensemble.BaggingRegressor&quot;&gt; &lt;code&gt;BaggingRegressor&lt;/code&gt; &lt;/a&gt;）的形式提供，将用户指定的基本估计器以及指定绘制随机子集的策略的参数作为输入。特别地， &lt;code&gt;max_samples&lt;/code&gt; 和 &lt;code&gt;max_features&lt;/code&gt; 控制子集的大小（就样本和特征而言），而 &lt;code&gt;bootstrap&lt;/code&gt; 和 &lt;code&gt;bootstrap_features&lt;/code&gt; 控制是否绘制或替换时绘制样本和特征。当使用可用样本的子集时，可以通过设置 &lt;code&gt;oob_score=True&lt;/code&gt; 来估计袋外样本的泛化精度。例如，下面的代码段说明了如何实例化 &lt;code&gt;KNeighborsClassifier&lt;/code&gt; 基本估计器的装袋集合，每个估计器都基于50％的样本和50％的特征的随机子集。</target>
        </trans-unit>
        <trans-unit id="0848ab34fddbc88dcbfdd395d0519986e8d182eb" translate="yes" xml:space="preserve">
          <source>In scikit-learn, this transformation (with a user-defined shrinkage coefficient) can be directly applied to a pre-computed covariance with the &lt;a href=&quot;generated/sklearn.covariance.shrunk_covariance#sklearn.covariance.shrunk_covariance&quot;&gt;&lt;code&gt;shrunk_covariance&lt;/code&gt;&lt;/a&gt; method. Also, a shrunk estimator of the covariance can be fitted to data with a &lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance&quot;&gt;&lt;code&gt;ShrunkCovariance&lt;/code&gt;&lt;/a&gt; object and its &lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance.fit&quot;&gt;&lt;code&gt;ShrunkCovariance.fit&lt;/code&gt;&lt;/a&gt; method. Again, results depend on whether the data are centered, so one may want to use the &lt;code&gt;assume_centered&lt;/code&gt; parameter accurately.</source>
          <target state="translated">在scikit-learn中，可以使用&lt;a href=&quot;generated/sklearn.covariance.shrunk_covariance#sklearn.covariance.shrunk_covariance&quot;&gt; &lt;code&gt;shrunk_covariance&lt;/code&gt; &lt;/a&gt;方法将该转换（具有用户定义的收缩系数）直接应用于预先计算的协方差。同样，可以使用&lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance&quot;&gt; &lt;code&gt;ShrunkCovariance&lt;/code&gt; &lt;/a&gt;对象及其&lt;a href=&quot;generated/sklearn.covariance.shrunkcovariance#sklearn.covariance.ShrunkCovariance.fit&quot;&gt; &lt;code&gt;ShrunkCovariance.fit&lt;/code&gt; &lt;/a&gt;方法将协方差的缩小估算器拟合到数据。同样，结果取决于数据是否居中，因此可能需要准确地使用 &lt;code&gt;assume_centered&lt;/code&gt; 参数。</target>
        </trans-unit>
        <trans-unit id="77c9e3634b9d256acc307751b68c900b0b833a29" translate="yes" xml:space="preserve">
          <source>In single precision, &lt;code&gt;mean&lt;/code&gt; can be inaccurate:</source>
          <target state="translated">以单精度表示， &lt;code&gt;mean&lt;/code&gt; 可能不准确：</target>
        </trans-unit>
        <trans-unit id="640c5c337251b299605fe0c1704cd43d50fcda84" translate="yes" xml:space="preserve">
          <source>In some cases it&amp;rsquo;s not necessary to include higher powers of any single feature, but only the so-called &lt;em&gt;interaction features&lt;/em&gt; that multiply together at most \(d\) distinct features. These can be gotten from &lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt;&lt;code&gt;PolynomialFeatures&lt;/code&gt;&lt;/a&gt; with the setting &lt;code&gt;interaction_only=True&lt;/code&gt;.</source>
          <target state="translated">在某些情况下，没有必要包含任何单个特征的较高幂，而​​只需包含最多可将\（d \）个不同特征相乘的所谓&lt;em&gt;交互&lt;/em&gt;特征。这些可以从&lt;a href=&quot;generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt; &lt;code&gt;PolynomialFeatures&lt;/code&gt; 中&lt;/a&gt;获得，设置为 &lt;code&gt;interaction_only=True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4976a8ab8df4ced9a6ceb9c8368b484dbc953550" translate="yes" xml:space="preserve">
          <source>In some cases, only interaction terms among features are required, and it can be gotten with the setting &lt;code&gt;interaction_only=True&lt;/code&gt;:</source>
          <target state="translated">在某些情况下，仅需要要素之间的交互项，并且可以通过设置 &lt;code&gt;interaction_only=True&lt;/code&gt; 来获得：</target>
        </trans-unit>
        <trans-unit id="1b40a5cc0ca592bd5c1306138f251a81fe534579" translate="yes" xml:space="preserve">
          <source>In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters. (For theoretical reasons why naive Bayes works well, and on which types of data it does, see the references below.)</source>
          <target state="translated">尽管它们的假设显然过于简单,但奈夫贝叶斯分类器在许多真实世界的情况下工作得相当好,著名的是文档分类和垃圾邮件过滤。它们需要少量的训练数据来估计必要的参数。(关于为什么天真贝叶斯工作得很好,以及在哪些类型的数据上工作得很好的理论原因,请参见下面的参考文献)。</target>
        </trans-unit>
        <trans-unit id="389828ed3005eb646a2fbe827835555cbdc74437" translate="yes" xml:space="preserve">
          <source>In terms of accuracy, LOO often results in high variance as an estimator for the test error. Intuitively, since \(n - 1\) of the \(n\) samples are used to build each model, models constructed from folds are virtually identical to each other and to the model built from the entire training set.</source>
          <target state="translated">在精度方面,LOO作为测试误差的估计器,往往会导致高方差。直观地讲,由于每一个模型的建立都要用到折线样本中的(n-1/)个样本,所以由折线构建的模型彼此之间以及由整个训练集构建的模型实际上是相同的。</target>
        </trans-unit>
        <trans-unit id="36132aafc76511f4279f2a1765dcbaeb9d7a44b1" translate="yes" xml:space="preserve">
          <source>In terms of time and space complexity, Theil-Sen scales according to</source>
          <target state="translated">在时间和空间的复杂性方面,Theil-Sen的尺度是根据</target>
        </trans-unit>
        <trans-unit id="8cac8320893acecd46013a1cd740f5237cabb213" translate="yes" xml:space="preserve">
          <source>In that case, the model with 2 components and full covariance (which corresponds to the true generative model) is selected.</source>
          <target state="translated">在这种情况下,选择具有2个分量和全协方差的模型(相当于真正的生成模型)。</target>
        </trans-unit>
        <trans-unit id="df23be83a828beae97b01644c9cebfd6ec568f81" translate="yes" xml:space="preserve">
          <source>In the &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;TfidfVectorizer&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;smooth_idf=False&lt;/code&gt;, the &amp;ldquo;1&amp;rdquo; count is added to the idf instead of the idf&amp;rsquo;s denominator:</source>
          <target state="translated">在带有 &lt;code&gt;smooth_idf=False&lt;/code&gt; 的&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;TfidfVectorizer&lt;/code&gt; 中&lt;/a&gt;，将&amp;ldquo; 1&amp;rdquo;计数添加到idf而不是idf的分母：</target>
        </trans-unit>
        <trans-unit id="5bd53e1aa867b99daf4cb138797f33e24d9cfda1" translate="yes" xml:space="preserve">
          <source>In the &lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt;&lt;code&gt;BernoulliRBM&lt;/code&gt;&lt;/a&gt;, all units are binary stochastic units. This means that the input data should either be binary, or real-valued between 0 and 1 signifying the probability that the visible unit would turn on or off. This is a good model for character recognition, where the interest is on which pixels are active and which aren&amp;rsquo;t. For images of natural scenes it no longer fits because of background, depth and the tendency of neighbouring pixels to take the same values.</source>
          <target state="translated">在&lt;a href=&quot;generated/sklearn.neural_network.bernoullirbm#sklearn.neural_network.BernoulliRBM&quot;&gt; &lt;code&gt;BernoulliRBM&lt;/code&gt; 中&lt;/a&gt;，所有单位都是二进制随机单位。这意味着输入数据应该是二进制的，或者是介于0和1之间的实值，表示可见单位将打开或关闭的可能性。这是一个很好的字符识别模型，关注的是哪些像素处于活动状态，哪些像素未处于活动状态。对于自然场景的图像，由于背景，深度和相邻像素采用相同值的趋势而不再适合。</target>
        </trans-unit>
        <trans-unit id="729860dcb0b5963ed7d873f5d8718ecbded39d56" translate="yes" xml:space="preserve">
          <source>In the &lt;code&gt;l1&lt;/code&gt; case, theory says that prediction consistency (i.e. that under given hypothesis, the estimator learned predicts as well as a model knowing the true distribution) is not possible because of the bias of the &lt;code&gt;l1&lt;/code&gt;. It does say, however, that model consistency, in terms of finding the right set of non-zero parameters as well as their signs, can be achieved by scaling &lt;code&gt;C1&lt;/code&gt;.</source>
          <target state="translated">在 &lt;code&gt;l1&lt;/code&gt; 的情况下，理论认为，由于 &lt;code&gt;l1&lt;/code&gt; 的偏差，不可能实现预测一致性（即在给定的假设下，所学习的估计量可以预测以及知道真实分布的模型）。但是，它确实说，通过缩放 &lt;code&gt;C1&lt;/code&gt; 可以在找到正确的非零参数集及其符号方面实现模型一致性。</target>
        </trans-unit>
        <trans-unit id="3ac113ba1d5ea8579f40e00fa885b5d980c4ba43" translate="yes" xml:space="preserve">
          <source>In the &lt;code&gt;l1&lt;/code&gt; penalty case, the cross-validation-error correlates best with the test-error, when scaling our &lt;code&gt;C&lt;/code&gt; with the number of samples, &lt;code&gt;n&lt;/code&gt;, which can be seen in the first figure.</source>
          <target state="translated">在 &lt;code&gt;l1&lt;/code&gt; 罚金的情况下，当用样本数 &lt;code&gt;n&lt;/code&gt; 缩放 &lt;code&gt;C&lt;/code&gt; 时，交叉验证误差与测试误差最相关，这可以从第一个图中看出。</target>
        </trans-unit>
        <trans-unit id="4495b7082e60ec7cb3a7d3cf1946536697a0e6b3" translate="yes" xml:space="preserve">
          <source>In the above case, the classifier is fit on a 1d array of multiclass labels and the &lt;code&gt;predict()&lt;/code&gt; method therefore provides corresponding multiclass predictions. It is also possible to fit upon a 2d array of binary label indicators:</source>
          <target state="translated">在上述情况下，分类器适合一维多类标签数组，因此 &lt;code&gt;predict()&lt;/code&gt; 方法提供了相应的多类预测。也有可能适合二维标签指示符的二维数组：</target>
        </trans-unit>
        <trans-unit id="41d2181ce120b72b14a943b5e6f5608fe64d404d" translate="yes" xml:space="preserve">
          <source>In the above example, &lt;code&gt;char_wb&lt;/code&gt; analyzer is used, which creates n-grams only from characters inside word boundaries (padded with space on each side). The &lt;code&gt;char&lt;/code&gt; analyzer, alternatively, creates n-grams that span across words:</source>
          <target state="translated">在上面的示例中，使用了 &lt;code&gt;char_wb&lt;/code&gt; 分析器，该分析器仅从单词边界内的字符（每边填充空格）创建n-gram。所述 &lt;code&gt;char&lt;/code&gt; 分析器，可替代地，将创建的n-gram跨单词跨度：</target>
        </trans-unit>
        <trans-unit id="02dd6b844a6f6c7bb7b63318a0172b18e25d4984" translate="yes" xml:space="preserve">
          <source>In the above example, the &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; expects a 1D array as input and therefore the columns were specified as a string (&lt;code&gt;'city'&lt;/code&gt;). However, other transformers generally expect 2D data, and in that case you need to specify the column as a list of strings (&lt;code&gt;['city']&lt;/code&gt;).</source>
          <target state="translated">在上面的示例中，&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt;期望将一维数组作为输入，因此将列指定为字符串（ &lt;code&gt;'city'&lt;/code&gt; ）。但是，其他转换器通常需要2D数据，在这种情况下，您需要将列指定为字符串列表（ &lt;code&gt;['city']&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="8be3ffb93abc4e32a08a3f61b709f6ed3e6124c4" translate="yes" xml:space="preserve">
          <source>In the above example-code, we firstly use the &lt;code&gt;fit(..)&lt;/code&gt; method to fit our estimator to the data and secondly the &lt;code&gt;transform(..)&lt;/code&gt; method to transform our count-matrix to a tf-idf representation. These two steps can be combined to achieve the same end result faster by skipping redundant processing. This is done through using the &lt;code&gt;fit_transform(..)&lt;/code&gt; method as shown below, and as mentioned in the note in the previous section:</source>
          <target state="translated">在上面的示例代码中，我们首先使用 &lt;code&gt;fit(..)&lt;/code&gt; 方法使估计量适合数据，其次使用 &lt;code&gt;transform(..)&lt;/code&gt; 方法将计数矩阵转换为tf-idf表示形式。通过跳过冗余处理，可以将这两个步骤结合起来以更快地达到相同的最终结果。这是通过使用 &lt;code&gt;fit_transform(..)&lt;/code&gt; 方法完成的，如下所示，并且如上一节中的注释所述：</target>
        </trans-unit>
        <trans-unit id="fc1fc9feffcc3ab7062a5968abf403f994be456d" translate="yes" xml:space="preserve">
          <source>In the above process, rejection sampling is used to make sure that n is more than 2, and that the document length is never zero. Likewise, we reject classes which have already been chosen. The documents that are assigned to both classes are plotted surrounded by two colored circles.</source>
          <target state="translated">在上述过程中,拒绝采样是用来确保n大于2,并且文档长度永远不会为零。同样,我们也会拒绝那些已经被选择的类。被分配到这两个类的文档被两个彩色圆圈包围绘制出来。</target>
        </trans-unit>
        <trans-unit id="35ac86e1f976c024d854c94dc1a07d9250df373b" translate="yes" xml:space="preserve">
          <source>In the above process, rejection sampling is used to make sure that n is never zero or more than &lt;code&gt;n_classes&lt;/code&gt;, and that the document length is never zero. Likewise, we reject classes which have already been chosen.</source>
          <target state="translated">在上述过程中，使用拒绝采样来确保n永远不为零或大于 &lt;code&gt;n_classes&lt;/code&gt; ，并且文档长度永远不为零。同样，我们拒绝已经选择的类。</target>
        </trans-unit>
        <trans-unit id="35b3eed71c5956697e4e941c9abda7fa7875d908" translate="yes" xml:space="preserve">
          <source>In the binary (two-class) case, \(tp\), \(tn\), \(fp\) and \(fn\) are respectively the number of true positives, true negatives, false positives and false negatives, the MCC is defined as</source>
          <target state="translated">在二元(两类)情况下,\(tp/)、\(tn/)、\(fp/)和\(fn/)分别是真阳性、真阴性、假阳性和假阴性的数量,MCC定义为:1.</target>
        </trans-unit>
        <trans-unit id="8cf754386b9e93bff61012cc7eebd891fe098125" translate="yes" xml:space="preserve">
          <source>In the binary case, balanced accuracy is equal to the arithmetic mean of &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;sensitivity&lt;/a&gt; (true positive rate) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;specificity&lt;/a&gt; (true negative rate), or the area under the ROC curve with binary predictions rather than scores.</source>
          <target state="translated">在二进制情况下，平衡精度等于&lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;灵敏度&lt;/a&gt;（真阳性率）和&lt;a href=&quot;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&quot;&gt;特异性&lt;/a&gt;（真阴性率）的算术平均值，或者说是用二进制预测而不是分数在ROC曲线下的面积。</target>
        </trans-unit>
        <trans-unit id="a998ef5238ae7921fe9486295ae64f00deab3566" translate="yes" xml:space="preserve">
          <source>In the binary case, we can extract true positives, etc as follows:</source>
          <target state="translated">在二进制的情况下,我们可以提取真阳性等,如下。</target>
        </trans-unit>
        <trans-unit id="2482e7f51309cef70ec85538824011f95f0813b1" translate="yes" xml:space="preserve">
          <source>In the case of &amp;ldquo;one-vs-one&amp;rdquo; &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt;, the layout of the attributes is a little more involved. In the case of having a linear kernel, the attributes &lt;code&gt;coef_&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; have the shape &lt;code&gt;[n_class * (n_class - 1) / 2, n_features]&lt;/code&gt; and &lt;code&gt;[n_class * (n_class - 1) / 2]&lt;/code&gt; respectively. This is similar to the layout for &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt; described above, with each row now corresponding to a binary classifier. The order for classes 0 to n is &amp;ldquo;0 vs 1&amp;rdquo;, &amp;ldquo;0 vs 2&amp;rdquo; , &amp;hellip; &amp;ldquo;0 vs n&amp;rdquo;, &amp;ldquo;1 vs 2&amp;rdquo;, &amp;ldquo;1 vs 3&amp;rdquo;, &amp;ldquo;1 vs n&amp;rdquo;, . . . &amp;ldquo;n-1 vs n&amp;rdquo;.</source>
          <target state="translated">在&amp;ldquo;一对一&amp;rdquo; &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt;的情况下，属性的布局要稍微复杂一些。在具有线性核的情况下，属性 &lt;code&gt;coef_&lt;/code&gt; 和 &lt;code&gt;intercept_&lt;/code&gt; 分别具有 &lt;code&gt;[n_class * (n_class - 1) / 2, n_features]&lt;/code&gt; 和 &lt;code&gt;[n_class * (n_class - 1) / 2]&lt;/code&gt; 。这类似于上述用于&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;的布局，现在每一行都对应一个二进制分类器。类0到n的顺序是&amp;ldquo; 0对1&amp;rdquo;，&amp;ldquo; 0对2&amp;rdquo;，&amp;hellip;&amp;hellip;&amp;ldquo; 0对n&amp;rdquo;，&amp;ldquo; 1对2&amp;rdquo;，&amp;ldquo; 1对3&amp;rdquo;，&amp;ldquo; 1对n&amp;rdquo;，...。 。 。 &amp;ldquo; n-1 vs n&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="7befa9fe69dc29e17ce8c14ce1f24dcd596f25dc" translate="yes" xml:space="preserve">
          <source>In the case of Gaussian process classification, &amp;ldquo;one_vs_one&amp;rdquo; might be computationally cheaper since it has to solve many problems involving only a subset of the whole training set rather than fewer problems on the whole dataset. Since Gaussian process classification scales cubically with the size of the dataset, this might be considerably faster. However, note that &amp;ldquo;one_vs_one&amp;rdquo; does not support predicting probability estimates but only plain predictions. Moreover, note that &lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessclassifier#sklearn.gaussian_process.GaussianProcessClassifier&quot;&gt;&lt;code&gt;GaussianProcessClassifier&lt;/code&gt;&lt;/a&gt; does not (yet) implement a true multi-class Laplace approximation internally, but as discussed above is based on solving several binary classification tasks internally, which are combined using one-versus-rest or one-versus-one.</source>
          <target state="translated">在高斯过程分类的情况下，&amp;ldquo; one_vs_one&amp;rdquo;在计算上可能更便宜，因为它必须解决仅涉及整个训练集的一个子集的许多问题，而不是解决整个数据集上较少的问题。由于高斯过程分类与数据集的大小成三次方缩放，因此这可能会更快。但是，请注意，&amp;ldquo; one_vs_one&amp;rdquo;不支持预测概率估计，而仅支持简单预测。此外，请注意，&lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessclassifier#sklearn.gaussian_process.GaussianProcessClassifier&quot;&gt; &lt;code&gt;GaussianProcessClassifier&lt;/code&gt; &lt;/a&gt;尚未（尚未）在内部实现真正的多类Laplace逼近，但如上所述，它基于内部解决几个二进制分类任务的方法，这些任务使用&amp;ldquo;一对多&amp;rdquo;或&amp;ldquo;一对一&amp;rdquo;进行组合。</target>
        </trans-unit>
        <trans-unit id="35a5ada3d16c18d2778299b423e5960182d45740" translate="yes" xml:space="preserve">
          <source>In the case of LDA, the Gaussians for each class are assumed to share the same covariance matrix: \(\Sigma_k = \Sigma\) for all \(k\). This leads to linear decision surfaces, which can be seen by comparing the log-probability ratios \(\log[P(y=k | X) / P(y=l | X)]\):</source>
          <target state="translated">在LDA的情况下,假设每个类的高斯指数共享相同的协方差矩阵。\对于所有的高斯马(k)来说,(高斯马_k=高斯马)。这就导致了线性决策面,这可以通过比较对数概率比率 \(\log[P(y=k | X)/P(y=l | X)]\)看出。</target>
        </trans-unit>
        <trans-unit id="9191afe7181654f4c123a5274615786e30f48b2b" translate="yes" xml:space="preserve">
          <source>In the case of QDA, there are no assumptions on the covariance matrices \(\Sigma_k\) of the Gaussians, leading to quadratic decision surfaces. See &lt;a href=&quot;#id4&quot; id=&quot;id1&quot;&gt;[3]&lt;/a&gt; for more details.</source>
          <target state="translated">在QDA的情况下，对高斯的协方差矩阵\（\ Sigma_k \）没有假设，从而导致二次决策面。有关更多详细信息，请参见&lt;a href=&quot;#id4&quot; id=&quot;id1&quot;&gt;[3]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="be50ffb98df62eb79c354d934aa76c5587bf8cba" translate="yes" xml:space="preserve">
          <source>In the case of multi-class classification &lt;code&gt;coef_&lt;/code&gt; is a two-dimensional array of &lt;code&gt;shape=[n_classes, n_features]&lt;/code&gt; and &lt;code&gt;intercept_&lt;/code&gt; is a one-dimensional array of &lt;code&gt;shape=[n_classes]&lt;/code&gt;. The i-th row of &lt;code&gt;coef_&lt;/code&gt; holds the weight vector of the OVA classifier for the i-th class; classes are indexed in ascending order (see attribute &lt;code&gt;classes_&lt;/code&gt;). Note that, in principle, since they allow to create a probability model, &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; and &lt;code&gt;loss=&quot;modified_huber&quot;&lt;/code&gt; are more suitable for one-vs-all classification.</source>
          <target state="translated">在多类分类的情况下 &lt;code&gt;coef_&lt;/code&gt; 是一个二维阵列 &lt;code&gt;shape=[n_classes, n_features]&lt;/code&gt; 和 &lt;code&gt;intercept_&lt;/code&gt; 是一个一维阵列 &lt;code&gt;shape=[n_classes]&lt;/code&gt; 。 &lt;code&gt;coef_&lt;/code&gt; 的第i行保存了第i类的OVA分类器的权重向量；类按升序索引（请参阅属性 &lt;code&gt;classes_&lt;/code&gt; ）。注意，原则上，由于它们允许创建概率模型，所以 &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; 和 &lt;code&gt;loss=&quot;modified_huber&quot;&lt;/code&gt; 更适用于一对多分类。</target>
        </trans-unit>
        <trans-unit id="e1959581346965192dc91c946ef9926bceb1c51a" translate="yes" xml:space="preserve">
          <source>In the case of multi-class classification, the mean log-marginal likelihood of the one-versus-rest classifiers are returned.</source>
          <target state="translated">在多类分类的情况下,返回单类与其余分类器的平均对数边际似然。</target>
        </trans-unit>
        <trans-unit id="a45d03e82085c1e194b2b19d0700767439a7ac42" translate="yes" xml:space="preserve">
          <source>In the case of one-hot/one-of-K coding, the constructed feature names and values are returned rather than the original ones.</source>
          <target state="translated">在one-hot/one-of-K编码的情况下,返回的是构建的特征名和值,而不是原始的。</target>
        </trans-unit>
        <trans-unit id="108b9e0576d5a78538771fb415a46ae76d1a6e26" translate="yes" xml:space="preserve">
          <source>In the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier. &lt;code&gt;BernoulliNB&lt;/code&gt; might perform better on some datasets, especially those with shorter documents. It is advisable to evaluate both models, if time permits.</source>
          <target state="translated">在文本分类的情况下，可以使用单词出现向量（而不是单词计数向量）来训练和使用此分类器。 &lt;code&gt;BernoulliNB&lt;/code&gt; 可能在某些数据集上表现更好，尤其是那些文档较短的数据集。如果时间允许，建议评估两个模型。</target>
        </trans-unit>
        <trans-unit id="1ee37ddaa2e2b7fb0103fdddd162d9ad76a8f2dd" translate="yes" xml:space="preserve">
          <source>In the case of the digits dataset, the task is to predict, given an image, which digit it represents. We are given samples of each of the 10 possible classes (the digits zero through nine) on which we &lt;em&gt;fit&lt;/em&gt; an &lt;a href=&quot;https://en.wikipedia.org/wiki/Estimator&quot;&gt;estimator&lt;/a&gt; to be able to &lt;em&gt;predict&lt;/em&gt; the classes to which unseen samples belong.</source>
          <target state="translated">对于数字数据集，任务是在给定图像的情况下预测其代表的数字。我们为10种可能的类别（数字从零到九）中的每一个提供了样本，我们在其上&lt;em&gt;拟合&lt;/em&gt;了一个&lt;a href=&quot;https://en.wikipedia.org/wiki/Estimator&quot;&gt;估计量&lt;/a&gt;，以能够&lt;em&gt;预测&lt;/em&gt;未见样本所属的类别。</target>
        </trans-unit>
        <trans-unit id="0484e6facaecfeba6a4ff8e0552fa9a5ac1c52dd" translate="yes" xml:space="preserve">
          <source>In the case that one or more classes are absent in a training portion, a default score needs to be assigned to all instances for that class if &lt;code&gt;method&lt;/code&gt; produces columns per class, as in {&amp;lsquo;decision_function&amp;rsquo;, &amp;lsquo;predict_proba&amp;rsquo;, &amp;lsquo;predict_log_proba&amp;rsquo;}. For &lt;code&gt;predict_proba&lt;/code&gt; this value is 0. In order to ensure finite output, we approximate negative infinity by the minimum finite float value for the dtype in other cases.</source>
          <target state="translated">如果训练部分中缺少一个或多个类，则如果 &lt;code&gt;method&lt;/code&gt; 为每个类生成列，则需要为该类的所有实例分配默认分数，例如{'decision_function'，'predict_proba'，'predict_log_proba'} 。对于 &lt;code&gt;predict_proba&lt;/code&gt; ,此值为0。为了确保有限的输出，在其他情况下，我们用dtype的最小有限浮点值来近似负无穷大。</target>
        </trans-unit>
        <trans-unit id="74ff5bfdda6b3e93f59169f7c22fc2d68fa3fcf3" translate="yes" xml:space="preserve">
          <source>In the case when the binary labels are fractional (probabilistic), inverse_transform chooses the class with the greatest value. Typically, this allows to use the output of a linear model&amp;rsquo;s decision_function method directly as the input of inverse_transform.</source>
          <target state="translated">在二进制标签是小数（概率）的情况下，inverse_transform选择具有最大值的类。通常，这允许将线性模型的Decision_function方法的输出直接用作inverse_transform的输入。</target>
        </trans-unit>
        <trans-unit id="2d852af4ac330c07eec96a31a9559a88b3b70655" translate="yes" xml:space="preserve">
          <source>In the cases of a tie, the &lt;code&gt;VotingClassifier&lt;/code&gt; will select the class based on the ascending sort order. E.g., in the following scenario</source>
          <target state="translated">在平局的情况下， &lt;code&gt;VotingClassifier&lt;/code&gt; 将根据升序排列选择类别。例如，在以下情况下</target>
        </trans-unit>
        <trans-unit id="84352a0fa93e8d3c09e63ba562819b09bff22e0e" translate="yes" xml:space="preserve">
          <source>In the checkerboard case, each row belongs to all column clusters, and each column belongs to all row clusters. Here is an example of this structure where the variance of the values within each bicluster is small:</source>
          <target state="translated">在棋盘的情况下,每行属于所有列簇,每列属于所有行簇。下面是这种结构的一个例子,每个双簇内的值的方差很小。</target>
        </trans-unit>
        <trans-unit id="6a06bacf0f95acd504bad8493dc228833ae72576" translate="yes" xml:space="preserve">
          <source>In the event that the 95% confidence interval based on Fisher transform spans zero, a warning is raised.</source>
          <target state="translated">如果基于Fisher变换的95%置信区间跨度为零,则会发出警告。</target>
        </trans-unit>
        <trans-unit id="2da0fe066fd806cee05903c8f41b8c38bb726d66" translate="yes" xml:space="preserve">
          <source>In the example below, using a small shrink threshold increases the accuracy of the model from 0.81 to 0.82.</source>
          <target state="translated">在下面的例子中,使用一个小的收缩阈值将模型的精度从0.81提高到0.82。</target>
        </trans-unit>
        <trans-unit id="ae3527cc8009043f3459062f8b3ac5f4c7cdc080" translate="yes" xml:space="preserve">
          <source>In the figure below, the color indicates cluster membership, with large circles indicating core samples found by the algorithm. Smaller circles are non-core samples that are still part of a cluster. Moreover, the outliers are indicated by black points below.</source>
          <target state="translated">在下图中,颜色表示簇成员资格,大圆圈表示算法发现的核心样本。较小的圆圈为非核心样本,它们仍然是聚类的一部分。此外,离群值用下面的黑点表示。</target>
        </trans-unit>
        <trans-unit id="a915ebedbc9fc712ae336eb7409727fc370425dc" translate="yes" xml:space="preserve">
          <source>In the first row, the classifiers are built using the sepal width and the sepal length features only, on the second row using the petal length and sepal length only, and on the third row using the petal width and the petal length only.</source>
          <target state="translated">在第一行中,只使用萼片宽度和萼片长度特征建立分类器,在第二行中只使用花瓣长度和萼片长度特征建立分类器,在第三行中只使用花瓣宽度和花瓣长度特征建立分类器。</target>
        </trans-unit>
        <trans-unit id="bd9366b471cf174a5dc26260b1ee3e4775bd8a95" translate="yes" xml:space="preserve">
          <source>In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask who&amp;rsquo;s the closest point to [1, 1, 1]:</source>
          <target state="translated">在下面的示例中，我们从一个代表数据集的数组构造一个NeighborsClassifier类，并询问谁是最接近[1，1，1]的点：</target>
        </trans-unit>
        <trans-unit id="f873541d5e32ccd97b454877a7265b9e862eeb9a" translate="yes" xml:space="preserve">
          <source>In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask who&amp;rsquo;s the closest point to [1,1,1]</source>
          <target state="translated">在下面的示例中，我们从代表我们的数据集的数组构造NeighborsClassifier类，并询问谁是最接近[1,1,1]的点</target>
        </trans-unit>
        <trans-unit id="65f9f2f58e8b0fd298381aa88835a40b1607c17f" translate="yes" xml:space="preserve">
          <source>In the following figure, 100 points are drawn from a bimodal distribution, and the kernel density estimates are shown for three choices of kernels:</source>
          <target state="translated">在下图中,从双峰分布中抽取100个点,并显示了三种核的选择的核密度估计。</target>
        </trans-unit>
        <trans-unit id="24263a7351535bc4435386a661d4637b721eb5a0" translate="yes" xml:space="preserve">
          <source>In the following plot, we see a function \(f(x) = \cos (\frac{3}{2} \pi x)\) and some noisy samples from that function. We use three different estimators to fit the function: linear regression with polynomial features of degree 1, 4 and 15. We see that the first estimator can at best provide only a poor fit to the samples and the true function because it is too simple (high bias), the second estimator approximates it almost perfectly and the last estimator approximates the training data perfectly but does not fit the true function very well, i.e. it is very sensitive to varying training data (high variance).</source>
          <target state="translated">在下面的图中,我们看到一个函数/(f(x)=\cos (\frac{3}{2}\pi x)\)和该函数的一些噪声样本。我们使用三种不同的估计器来拟合该函数:具有1、4和15度多项式特征的线性回归。我们看到,第一个估计器最多只能对样本和真实函数提供较差的拟合,因为它太简单了(高偏差),第二个估计器几乎完美地拟合了它,最后一个估计器完美地拟合了训练数据,但不能很好地拟合真实函数,即它对变化的训练数据非常敏感(高方差)。</target>
        </trans-unit>
        <trans-unit id="de54ef0bf525631d31eb0623dcd55f8a9ddc120b" translate="yes" xml:space="preserve">
          <source>In the following sub-sections, we will describe each of those functions, preceded by some notes on common API and metric definition.</source>
          <target state="translated">在下面的小节中,我们将对这些函数进行逐一描述,前面还有一些关于常见API和度量衡定义的说明。</target>
        </trans-unit>
        <trans-unit id="7549668dfe247eb9e0e172cc89f620a63604992b" translate="yes" xml:space="preserve">
          <source>In the following we will use the built-in dataset loader for 20 newsgroups from scikit-learn. Alternatively, it is possible to download the dataset manually from the website and use the &lt;a href=&quot;../../modules/generated/sklearn.datasets.load_files#sklearn.datasets.load_files&quot;&gt;&lt;code&gt;sklearn.datasets.load_files&lt;/code&gt;&lt;/a&gt; function by pointing it to the &lt;code&gt;20news-bydate-train&lt;/code&gt; sub-folder of the uncompressed archive folder.</source>
          <target state="translated">在下面的内容中，我们将使用内置的数据集加载器加载来自scikit-learn的20个新闻组。另外，也可以从网站手动下载数据集，并通过将&lt;a href=&quot;../../modules/generated/sklearn.datasets.load_files#sklearn.datasets.load_files&quot;&gt; &lt;code&gt;sklearn.datasets.load_files&lt;/code&gt; &lt;/a&gt;函数指向未压缩存档文件夹的 &lt;code&gt;20news-bydate-train&lt;/code&gt; 子文件夹来使用它。</target>
        </trans-unit>
        <trans-unit id="2cb0b9817ecf09ea4893bb9df9d328ce75ef2d1b" translate="yes" xml:space="preserve">
          <source>In the following, &amp;ldquo;city&amp;rdquo; is a categorical attribute while &amp;ldquo;temperature&amp;rdquo; is a traditional numerical feature:</source>
          <target state="translated">在下文中，&amp;ldquo;城市&amp;rdquo;是分类属性，而&amp;ldquo;温度&amp;rdquo;是传统的数字特征：</target>
        </trans-unit>
        <trans-unit id="3e019b4cbe3ce7f1554fa08ce08898c560cb8a3b" translate="yes" xml:space="preserve">
          <source>In the following, we start a Python interpreter from our shell and then load the &lt;code&gt;iris&lt;/code&gt; and &lt;code&gt;digits&lt;/code&gt; datasets. Our notational convention is that &lt;code&gt;$&lt;/code&gt; denotes the shell prompt while &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; denotes the Python interpreter prompt:</source>
          <target state="translated">接下来，我们从外壳启动Python解释器，然后加载 &lt;code&gt;iris&lt;/code&gt; 和 &lt;code&gt;digits&lt;/code&gt; 数据集。我们的符号约定是 &lt;code&gt;$&lt;/code&gt; 表示shell提示符，而 &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; 表示Python解释器提示符：</target>
        </trans-unit>
        <trans-unit id="fe30ab8cfcc104ee94b5fb01793274570a377034" translate="yes" xml:space="preserve">
          <source>In the formula above, \(\mathbf{b}\) and \(\mathbf{c}\) are the intercept vectors for the visible and hidden layers, respectively. The joint probability of the model is defined in terms of the energy:</source>
          <target state="translated">在上式中,\(mathbf{b}\)和\(mathbf{c}\)分别是可见层和隐藏层的截距向量。模型的联合概率用能量来定义。</target>
        </trans-unit>
        <trans-unit id="66070e8da21856ec34fc0b507e5d723e9475a567" translate="yes" xml:space="preserve">
          <source>In the multi-class and multi-label case, this is the average of the F1 score of each class with weighting depending on the &lt;code&gt;average&lt;/code&gt; parameter.</source>
          <target state="translated">在多类别和多标签的情况下，这是每个类别的F1分数的平均值，其权重取决于 &lt;code&gt;average&lt;/code&gt; 参数。</target>
        </trans-unit>
        <trans-unit id="47604d7ff8f733fb868ecbca5a77b859a57fe5aa" translate="yes" xml:space="preserve">
          <source>In the multiclass case, the Matthews correlation coefficient can be &lt;a href=&quot;http://rk.kvl.dk/introduction/index.html&quot;&gt;defined&lt;/a&gt; in terms of a &lt;a href=&quot;generated/sklearn.metrics.confusion_matrix#sklearn.metrics.confusion_matrix&quot;&gt;&lt;code&gt;confusion_matrix&lt;/code&gt;&lt;/a&gt;\(C\) for \(K\) classes. To simplify the definition consider the following intermediate variables:</source>
          <target state="translated">在多类情况下，可以根据con （K \）类的&lt;a href=&quot;generated/sklearn.metrics.confusion_matrix#sklearn.metrics.confusion_matrix&quot;&gt; &lt;code&gt;confusion_matrix&lt;/code&gt; &lt;/a&gt; \（C \）&lt;a href=&quot;http://rk.kvl.dk/introduction/index.html&quot;&gt;定义&lt;/a&gt; Matthews相关系数。为了简化定义，请考虑以下中间变量：</target>
        </trans-unit>
        <trans-unit id="b8d01a57cb617acafda7dfb6fdd570d7cb46be7c" translate="yes" xml:space="preserve">
          <source>In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the &amp;lsquo;multi_class&amp;rsquo; option is set to &amp;lsquo;ovr&amp;rsquo;, and uses the cross- entropy loss if the &amp;lsquo;multi_class&amp;rsquo; option is set to &amp;lsquo;multinomial&amp;rsquo;. (Currently the &amp;lsquo;multinomial&amp;rsquo; option is supported only by the &amp;lsquo;lbfgs&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;newton-cg&amp;rsquo; solvers.)</source>
          <target state="translated">在多类情况下，如果将&amp;ldquo; multi_class&amp;rdquo;选项设置为&amp;ldquo; ovr&amp;rdquo;，则训练算法将使用&amp;ldquo;一对多休息&amp;rdquo;（OvR）方案；如果将&amp;ldquo; multi_class&amp;rdquo;选项设置为&amp;ldquo;多项式&amp;rdquo;，则将使用交叉熵损失。 '。（当前，只有&amp;ldquo; lbfgs&amp;rdquo;，&amp;ldquo; sag&amp;rdquo;和&amp;ldquo; newton-cg&amp;rdquo;求解器支持&amp;ldquo;多项式&amp;rdquo;选项。）</target>
        </trans-unit>
        <trans-unit id="0fc004eb44e3b89b80f2b6796f35d04551e921cc" translate="yes" xml:space="preserve">
          <source>In the multiclass case:</source>
          <target state="translated">在多类情况下。</target>
        </trans-unit>
        <trans-unit id="d77598124fa8bad46b51e89decd127c4c83bc1e3" translate="yes" xml:space="preserve">
          <source>In the multilabel case with binary label indicators, where the first label set [0,1] has an error:</source>
          <target state="translated">在有二进制标签指示器的多标签情况下,第一个标签集[0,1]有错误。</target>
        </trans-unit>
        <trans-unit id="4557110f167a92f3d0af48823270c458eb95839b" translate="yes" xml:space="preserve">
          <source>In the multilabel case with binary label indicators:</source>
          <target state="translated">在多标签与二元标签指标的情况下。</target>
        </trans-unit>
        <trans-unit id="3dbb497f0422701e359a6bab16a477019a81aa92" translate="yes" xml:space="preserve">
          <source>In the multilabel learning literature, OvR is also known as the binary relevance method.</source>
          <target state="translated">在多标签学习文献中,OvR也被称为二元相关性方法。</target>
        </trans-unit>
        <trans-unit id="b098a0ed402e179dee6b5de05c6210f893507735" translate="yes" xml:space="preserve">
          <source>In the new space, each dimension is the distance to the cluster centers. Note that even if X is sparse, the array returned by &lt;code&gt;transform&lt;/code&gt; will typically be dense.</source>
          <target state="translated">在新空间中，每个维度都是到群集中心的距离。请注意，即使X是稀疏的， &lt;code&gt;transform&lt;/code&gt; 返回的数组通常也会很密集。</target>
        </trans-unit>
        <trans-unit id="a90aa674a36fbb7c4f65ac18f5e6a5a0eb4c7bc3" translate="yes" xml:space="preserve">
          <source>In the official &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/README.txt&quot;&gt;README.txt&lt;/a&gt; this task is described as the &amp;ldquo;Restricted&amp;rdquo; task. As I am not sure as to implement the &amp;ldquo;Unrestricted&amp;rdquo; variant correctly, I left it as unsupported for now.</source>
          <target state="translated">在官方的&lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/README.txt&quot;&gt;README.txt中，&lt;/a&gt;此任务称为&amp;ldquo;受限&amp;rdquo;任务。由于我不确定如何正确实施&amp;ldquo;不受限制&amp;rdquo;变体，因此暂时不支持该变体。</target>
        </trans-unit>
        <trans-unit id="901c760a214576d30b4ced7bbcce771984b42233" translate="yes" xml:space="preserve">
          <source>In the simple one-dimensional problem that we have seen in the example it is easy to see whether the estimator suffers from bias or variance. However, in high-dimensional spaces, models can become very difficult to visualize. For this reason, it is often helpful to use the tools described below.</source>
          <target state="translated">在我们在例子中看到的简单的一维问题中,很容易看到估计器是否存在偏差或方差。然而,在高维空间中,模型可能变得非常难以可视化。出于这个原因,使用下面描述的工具通常是有帮助的。</target>
        </trans-unit>
        <trans-unit id="bada0a0c8458a65354b2c23e7134e865cc4bf85c" translate="yes" xml:space="preserve">
          <source>In the single label multiclass case, the rows of the returned matrix sum to 1.</source>
          <target state="translated">在单标签多类的情况下,返回矩阵的行数相加为1。</target>
        </trans-unit>
        <trans-unit id="696912c12d134eed0fdc2e472302634288905dc5" translate="yes" xml:space="preserve">
          <source>In the small-samples situation, in which &lt;code&gt;n_samples&lt;/code&gt; is on the order of &lt;code&gt;n_features&lt;/code&gt; or smaller, sparse inverse covariance estimators tend to work better than shrunk covariance estimators. However, in the opposite situation, or for very correlated data, they can be numerically unstable. In addition, unlike shrinkage estimators, sparse estimators are able to recover off-diagonal structure.</source>
          <target state="translated">在小样本情况下，其中 &lt;code&gt;n_samples&lt;/code&gt; 为 &lt;code&gt;n_features&lt;/code&gt; 或更小数量级，稀疏逆协方差估计器往往比收缩的协方差估计器更好。但是，在相反的情况下，或者对于非常相关的数据，它们可能在数值上不稳定。另外，与收缩估计器不同，稀疏估计器能够恢复非对角线结构。</target>
        </trans-unit>
        <trans-unit id="d5f33dda5b96ece3c041650e9fa8db02e62bfd46" translate="yes" xml:space="preserve">
          <source>In the specific case of scikit-learn, it may be better to use joblib&amp;rsquo;s replacement of pickle (&lt;code&gt;joblib.dump&lt;/code&gt; &amp;amp; &lt;code&gt;joblib.load&lt;/code&gt;), which is more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators, but can only pickle to the disk and not to a string:</source>
          <target state="translated">在scikit-learn的特定情况下，最好使用joblib代替pickle（ &lt;code&gt;joblib.dump&lt;/code&gt; 和 &lt;code&gt;joblib.load&lt;/code&gt; ），这在内部携带大型numpy数组的对象上效率更高，这通常适合于scikit-学习估计器，但只能将其腌制到磁盘而不是字符串中：</target>
        </trans-unit>
        <trans-unit id="4d7c6f4a78fb7d42f5b6b076760e4c0fb27e052c" translate="yes" xml:space="preserve">
          <source>In the specific case of scikit-learn, it may be more interesting to use joblib&amp;rsquo;s replacement for pickle (&lt;code&gt;joblib.dump&lt;/code&gt; &amp;amp; &lt;code&gt;joblib.load&lt;/code&gt;), which is more efficient on big data but it can only pickle to the disk and not to a string:</source>
          <target state="translated">在scikit-learn的特定情况下，使用joblib代替pickle（ &lt;code&gt;joblib.dump&lt;/code&gt; 和 &lt;code&gt;joblib.load&lt;/code&gt; ）可能会更有趣，这在大数据上效率更高，但只能在磁盘而不是字符串中进行酸洗。 ：</target>
        </trans-unit>
        <trans-unit id="4e8940d2e745f9a24bd23b0a1547dcf715a870bb" translate="yes" xml:space="preserve">
          <source>In the total set of features, only the 4 first ones are significant. We can see that they have the highest score with univariate feature selection. The SVM assigns a large weight to one of these features, but also Selects many of the non-informative features. Applying univariate feature selection before the SVM increases the SVM weight attributed to the significant features, and will thus improve classification.</source>
          <target state="translated">在全部特征集中,只有前4个特征是显著的。我们可以看到,它们的单变量特征选择得分最高。SVM对其中的一个特征赋予了很大的权重,但同时也选择了很多非信息特征。在SVM之前应用单变量特征选择,可以增加SVM归属于重要特征的权重,从而将提高分类效果。</target>
        </trans-unit>
        <trans-unit id="a3dd53135ff49dbe7e421a248bf614a3ea8f0d5e" translate="yes" xml:space="preserve">
          <source>In the vector quantization literature, &lt;code&gt;cluster_centers_&lt;/code&gt; is called the code book and each value returned by &lt;code&gt;predict&lt;/code&gt; is the index of the closest code in the code book.</source>
          <target state="translated">在矢量量化文献中， &lt;code&gt;cluster_centers_&lt;/code&gt; 被称为代码簿， &lt;code&gt;predict&lt;/code&gt; 返回的每个值都是代码簿中最接近的代码的索引。</target>
        </trans-unit>
        <trans-unit id="9a63d5086bf9614df56a7612405271e0122a8145" translate="yes" xml:space="preserve">
          <source>In their 2004 paper &lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;, O. Ledoit and M. Wolf propose a formula to compute the optimal shrinkage coefficient \(\alpha\) that minimizes the Mean Squared Error between the estimated and the real covariance matrix.</source>
          <target state="translated">O. Ledoit和M. Wolf 在其2004年的论文中&lt;a href=&quot;#id3&quot; id=&quot;id2&quot;&gt;[1]&lt;/a&gt;提出了一个计算最佳收缩系数\（\ alpha \）的公式，该公式使估计和实际协方差矩阵之间的均方误差最小。</target>
        </trans-unit>
        <trans-unit id="ee9767309b3df05ebf7c392a0bb4e8915eee3d46" translate="yes" xml:space="preserve">
          <source>In these settings, the &lt;a href=&quot;../../modules/clustering#spectral-clustering&quot;&gt;Spectral clustering&lt;/a&gt; approach solves the problem know as &amp;lsquo;normalized graph cuts&amp;rsquo;: the image is seen as a graph of connected voxels, and the spectral clustering algorithm amounts to choosing graph cuts defining regions while minimizing the ratio of the gradient along the cut, and the volume of the region.</source>
          <target state="translated">在这些设置中，&lt;a href=&quot;../../modules/clustering#spectral-clustering&quot;&gt;光谱聚类&lt;/a&gt;方法解决了称为&amp;ldquo;归一化图割&amp;rdquo;的问题：将图像视为已连接体素的图，并且光谱聚类算法相当于选择定义区域的图割，同时将沿切割，以及该区域的体积。</target>
        </trans-unit>
        <trans-unit id="e2e4475ec0999dd975ba681e19179d817d6681ae" translate="yes" xml:space="preserve">
          <source>In this case we would like to know if a model trained on a particular set of groups generalizes well to the unseen groups. To measure this, we need to ensure that all the samples in the validation fold come from groups that are not represented at all in the paired training fold.</source>
          <target state="translated">在这种情况下,我们想知道在特定组上训练的模型是否能很好地泛化到未见组。为了衡量这一点,我们需要确保验证折中的所有样本都来自于配对训练折中完全没有代表的群体。</target>
        </trans-unit>
        <trans-unit id="8e6586aaac37d3a887b7276aee6797fdd6471b11" translate="yes" xml:space="preserve">
          <source>In this case, &lt;code&gt;X_train&lt;/code&gt; and &lt;code&gt;X_test&lt;/code&gt; are guaranteed to have the same number of features. Another way to achieve the same result is to fix the number of features:</source>
          <target state="translated">在这种情况下，保证 &lt;code&gt;X_train&lt;/code&gt; 和 &lt;code&gt;X_test&lt;/code&gt; 具有相同数量的功能。获得相同结果的另一种方法是修复功能数量：</target>
        </trans-unit>
        <trans-unit id="c94c921d6a6a8582b29da8ef5a3a44fe1ea80a89" translate="yes" xml:space="preserve">
          <source>In this case, the classifier is fit upon instances each assigned multiple labels. The &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt;&lt;code&gt;MultiLabelBinarizer&lt;/code&gt;&lt;/a&gt; is used to binarize the 2d array of multilabels to &lt;code&gt;fit&lt;/code&gt; upon. As a result, &lt;code&gt;predict()&lt;/code&gt; returns a 2d array with multiple predicted labels for each instance.</source>
          <target state="translated">在这种情况下，分类器适合每个分配了多个标签的实例。所述&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.multilabelbinarizer#sklearn.preprocessing.MultiLabelBinarizer&quot;&gt; &lt;code&gt;MultiLabelBinarizer&lt;/code&gt; &lt;/a&gt;用于multilabels的2D阵列二进制化到 &lt;code&gt;fit&lt;/code&gt; 时。结果， &lt;code&gt;predict()&lt;/code&gt; 返回一个二维数组，每个实例带有多个预测标签。</target>
        </trans-unit>
        <trans-unit id="4367a423584d0b6ba622189fa17b21bb3e206f2c" translate="yes" xml:space="preserve">
          <source>In this case, the cross-validation retained the same ratio of classes across each CV split. Next we&amp;rsquo;ll visualize this behavior for a number of CV iterators.</source>
          <target state="translated">在这种情况下，交叉验证在每个CV划分中保留相同的类比例。接下来，我们将可视化许多CV迭代器的行为。</target>
        </trans-unit>
        <trans-unit id="d121f450bc55250670235f93c8cd2083eb40a561" translate="yes" xml:space="preserve">
          <source>In this context, we can define the notions of precision, recall and F-measure:</source>
          <target state="translated">在这种情况下,我们可以定义精确性、召回率和F-measure的概念。</target>
        </trans-unit>
        <trans-unit id="6b7e5d4a758a26d1b659ba54387246d5cebcf12f" translate="yes" xml:space="preserve">
          <source>In this example the dependent variable Y is set as a function of the input features: y = X*w + c. The coefficient vector w is randomly sampled from a normal distribution, whereas the bias term c is set to a constant.</source>
          <target state="translated">在这个例子中,因变量Y被设置为输入特征的函数:y=X*w+c。系数向量w是从正态分布中随机抽取的,而偏置项c被设置为一个常数。</target>
        </trans-unit>
        <trans-unit id="5d530c717737ac885c81ddc70c9c4fe51f2f2e42" translate="yes" xml:space="preserve">
          <source>In this example the silhouette analysis is used to choose an optimal value for &lt;code&gt;n_clusters&lt;/code&gt;. The silhouette plot shows that the &lt;code&gt;n_clusters&lt;/code&gt; value of 3, 5 and 6 are a bad pick for the given data due to the presence of clusters with below average silhouette scores and also due to wide fluctuations in the size of the silhouette plots. Silhouette analysis is more ambivalent in deciding between 2 and 4.</source>
          <target state="translated">在此示例中，轮廓分析用于为 &lt;code&gt;n_clusters&lt;/code&gt; 选择最佳值。轮廓图显示，对于给定的数据， &lt;code&gt;n_clusters&lt;/code&gt; 值3、5和6是一个不好的选择，这是因为存在轮廓轮廓分数低于平均水平的聚类，并且轮廓图的大小波动很大。在确定2到4之间时，轮廓分析更为矛盾。</target>
        </trans-unit>
        <trans-unit id="550c896ceafe31d2e76547c4031642097a79581f" translate="yes" xml:space="preserve">
          <source>In this example we compare the various initialization strategies for K-means in terms of runtime and quality of the results.</source>
          <target state="translated">在这个例子中,我们从运行时间和结果质量方面比较了K-means的各种初始化策略。</target>
        </trans-unit>
        <trans-unit id="d9bed364e1f96090d42e72d8ad4e31b8f81dfc1d" translate="yes" xml:space="preserve">
          <source>In this example we prefer the &lt;code&gt;elasticnet&lt;/code&gt; penalty as it is often a good compromise between model compactness and prediction power. One can also further tune the &lt;code&gt;l1_ratio&lt;/code&gt; parameter (in combination with the regularization strength &lt;code&gt;alpha&lt;/code&gt;) to control this tradeoff.</source>
          <target state="translated">在此示例中，我们首选 &lt;code&gt;elasticnet&lt;/code&gt; 惩罚，因为它通常是模型紧凑性和预测能力之间的良好折衷。还可以进一步调整 &lt;code&gt;l1_ratio&lt;/code&gt; 参数（与正则化强度 &lt;code&gt;alpha&lt;/code&gt; 组合）以控制这一折衷。</target>
        </trans-unit>
        <trans-unit id="5f4ca84332e1fc2168e90c43c86e1d784ebd5f8c" translate="yes" xml:space="preserve">
          <source>In this example we see how to robustly fit a linear model to faulty data using the RANSAC algorithm.</source>
          <target state="translated">在这个例子中,我们将看到如何使用RANSAC算法对有问题的数据进行稳健的线性模型拟合。</target>
        </trans-unit>
        <trans-unit id="38fc37287fc51222e73dd7c83e6c92e563107ff6" translate="yes" xml:space="preserve">
          <source>In this example you might try to:</source>
          <target state="translated">在这个例子中,你可以尝试:</target>
        </trans-unit>
        <trans-unit id="8a61e0fa0737723bbfe9d0174ce3aad285419f4d" translate="yes" xml:space="preserve">
          <source>In this example, &lt;code&gt;X&lt;/code&gt; is &lt;code&gt;float32&lt;/code&gt;, which is cast to &lt;code&gt;float64&lt;/code&gt; by &lt;code&gt;fit_transform(X)&lt;/code&gt;.</source>
          <target state="translated">在此示例中， &lt;code&gt;X&lt;/code&gt; 为 &lt;code&gt;float32&lt;/code&gt; ，它由 &lt;code&gt;fit_transform(X)&lt;/code&gt; 强制转换为 &lt;code&gt;float64&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2812da8873763c11175cae962f9ab9000ab381c4" translate="yes" xml:space="preserve">
          <source>In this example, an image with connected circles is generated and spectral clustering is used to separate the circles.</source>
          <target state="translated">在这个例子中,生成了一个具有连接圆的图像,并使用光谱聚类来分离圆。</target>
        </trans-unit>
        <trans-unit id="69af0b849be70a0524a821dde21a609feb16811a" translate="yes" xml:space="preserve">
          <source>In this example, pixels are represented in a 3D-space and K-means is used to find 64 color clusters. In the image processing literature, the codebook obtained from K-means (the cluster centers) is called the color palette. Using a single byte, up to 256 colors can be addressed, whereas an RGB encoding requires 3 bytes per pixel. The GIF file format, for example, uses such a palette.</source>
          <target state="translated">在这个例子中,像素在3D空间中表示,K-means被用来寻找64个颜色簇。在图像处理文献中,由K-means(簇中心)得到的代码本称为调色板。使用一个字节,最多可寻址256种颜色,而RGB编码每像素需要3个字节。例如,GIF文件格式就使用了这样的调色板。</target>
        </trans-unit>
        <trans-unit id="2f0fb947da0f2bfc5faf32b771a3cb10ff049eda" translate="yes" xml:space="preserve">
          <source>In this example, the numeric data is standard-scaled after mean-imputation, while the categorical data is one-hot encoded after imputing missing values with a new category (&lt;code&gt;'missing'&lt;/code&gt;).</source>
          <target state="translated">在此示例中，数字数据在均值输入后进行标准缩放，而分类数据在使用新类别（ &lt;code&gt;'missing'&lt;/code&gt; ）插入缺失值后进行一次热编码。</target>
        </trans-unit>
        <trans-unit id="d88656bc2e040320cf9595554acac12be98f916c" translate="yes" xml:space="preserve">
          <source>In this example, we compare the estimation errors that are made when using various types of location and covariance estimates on contaminated Gaussian distributed data sets:</source>
          <target state="translated">在这个例子中,我们比较了在污染的高斯分布数据集上使用各种类型的位置和协方差估计时的估计误差。</target>
        </trans-unit>
        <trans-unit id="2b7bcaf87ef3b0730f7083836942b0b038810927" translate="yes" xml:space="preserve">
          <source>In this example, we give an overview of the &lt;a href=&quot;../../modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor&quot;&gt;&lt;code&gt;sklearn.compose.TransformedTargetRegressor&lt;/code&gt;&lt;/a&gt;. Two examples illustrate the benefit of transforming the targets before learning a linear regression model. The first example uses synthetic data while the second example is based on the Boston housing data set.</source>
          <target state="translated">在此示例中，我们概述了&lt;a href=&quot;../../modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor&quot;&gt; &lt;code&gt;sklearn.compose.TransformedTargetRegressor&lt;/code&gt; &lt;/a&gt;。两个示例说明了在学习线性回归模型之前转换目标的好处。第一个示例使用合成数据，而第二个示例基于Boston住房数据集。</target>
        </trans-unit>
        <trans-unit id="fd9410f53a0f1d1aa2f5ff77c7bafaf9751d4c08" translate="yes" xml:space="preserve">
          <source>In this example, we set the value of &lt;code&gt;gamma&lt;/code&gt; manually. To find good values for these parameters, we can use tools such as &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;grid search&lt;/a&gt; and &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;cross validation&lt;/a&gt;.</source>
          <target state="translated">在此示例中，我们手动设置了 &lt;code&gt;gamma&lt;/code&gt; 值。要为这些参数找到合适的值，我们可以使用诸如&lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;网格搜索&lt;/a&gt;和&lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;交叉验证之类的工具&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="9e0782ea6d7859077c07d60aaa0a30b1c4373f50" translate="yes" xml:space="preserve">
          <source>In this plot you can see the training scores and validation scores of an SVM for different values of the kernel parameter gamma. For very low values of gamma, you can see that both the training score and the validation score are low. This is called underfitting. Medium values of gamma will result in high values for both scores, i.e. the classifier is performing fairly well. If gamma is too high, the classifier will overfit, which means that the training score is good but the validation score is poor.</source>
          <target state="translated">在这个图中,你可以看到一个SVM在不同的内核参数gamma值下的训练得分和验证得分。对于非常低的gamma值,你可以看到训练得分和验证得分都很低。这就是所谓的欠拟合。中等的gamma值会导致两个分数都很高,即分类器的性能相当好。如果gamma值过高,分类器就会过拟合,这意味着训练得分很好,但验证得分很差。</target>
        </trans-unit>
        <trans-unit id="2c39a03080473177a8509645110953edafebbd76" translate="yes" xml:space="preserve">
          <source>In this scheme, features and samples are defined as follows:</source>
          <target state="translated">在本方案中,特征和样本定义如下。</target>
        </trans-unit>
        <trans-unit id="5941fbb58c226f551ff80660bcd51a84bcc2bae1" translate="yes" xml:space="preserve">
          <source>In this section we will see how to:</source>
          <target state="translated">在本节中,我们将看到如何。</target>
        </trans-unit>
        <trans-unit id="6ce5845b6414a0cfccffc603f3efdd4b47c7ce4b" translate="yes" xml:space="preserve">
          <source>In this section, we introduce the &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;machine learning&lt;/a&gt; vocabulary that we use throughout scikit-learn and give a simple learning example.</source>
          <target state="translated">在本节中，我们介绍了在整个scikit-learn中使用的&lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;机器学习&lt;/a&gt;词汇，并给出了一个简单的学习示例。</target>
        </trans-unit>
        <trans-unit id="e7b54ae8e73f20fa370a273bbb52814367b82582" translate="yes" xml:space="preserve">
          <source>In this snippet we make use of a &lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;&lt;/a&gt; coupled with &lt;a href=&quot;generated/sklearn.feature_selection.selectfrommodel#sklearn.feature_selection.SelectFromModel&quot;&gt;&lt;code&gt;sklearn.feature_selection.SelectFromModel&lt;/code&gt;&lt;/a&gt; to evaluate feature importances and select the most relevant features. Then, a &lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt;&lt;code&gt;sklearn.ensemble.RandomForestClassifier&lt;/code&gt;&lt;/a&gt; is trained on the transformed output, i.e. using only relevant features. You can perform similar operations with the other feature selection methods and also classifiers that provide a way to evaluate feature importances of course. See the &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt; examples for more details.</source>
          <target state="translated">在这个片段中，我们使用一个的&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt; &lt;/a&gt;加上&lt;a href=&quot;generated/sklearn.feature_selection.selectfrommodel#sklearn.feature_selection.SelectFromModel&quot;&gt; &lt;code&gt;sklearn.feature_selection.SelectFromModel&lt;/code&gt; &lt;/a&gt;评估功能重要度，并选择最相关的功能。然后，在转换后的输出上训练&lt;a href=&quot;generated/sklearn.ensemble.randomforestclassifier#sklearn.ensemble.RandomForestClassifier&quot;&gt; &lt;code&gt;sklearn.ensemble.RandomForestClassifier&lt;/code&gt; &lt;/a&gt;，即仅使用相关特征。您可以使用其他功能选择方法以及分类器执行类似的操作，这些分类器提供了一种评估功能的重要性的方法。有关更多详细信息，请参见&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt;示例。</target>
        </trans-unit>
        <trans-unit id="87aee0924ee2a28e2d573efd8e050c2a5c1632a3" translate="yes" xml:space="preserve">
          <source>In unsupervised learning we only have a dataset \(X = \{x_1, x_2, \dots, x_n \}\). How can this dataset be described mathematically? A very simple &lt;code&gt;continuous latent variable&lt;/code&gt; model for \(X\) is</source>
          <target state="translated">在无监督学习中，我们只有一个数据集\（X = \ {x_1，x_2，\ dots，x_n \} \）。如何用数学方式描述此数据集？\（X \）的一个非常简单的 &lt;code&gt;continuous latent variable&lt;/code&gt; 模型是</target>
        </trans-unit>
        <trans-unit id="c89a6ca6f29b888687c7afd577a282b5b95a2be5" translate="yes" xml:space="preserve">
          <source>Incorporating statistics from test data into the preprocessors makes cross-validation scores unreliable (known as &lt;em&gt;data leakage&lt;/em&gt;), for example in the case of scalers or imputing missing values.</source>
          <target state="translated">将测试数据中的统计信息合并到预处理器中，会使交叉验证得分不可靠（称为&lt;em&gt;数据泄漏&lt;/em&gt;），例如在定标器或估算缺失值的情况下。</target>
        </trans-unit>
        <trans-unit id="4c33f1e1286c254eaae3fbe03197d5578f08f56a" translate="yes" xml:space="preserve">
          <source>Increasing &lt;code&gt;max_depth&lt;/code&gt; for AdaBoost lowers the standard deviation of the scores (but the average score does not improve).</source>
          <target state="translated">增加AdaBoost的 &lt;code&gt;max_depth&lt;/code&gt; 会降低分数的标准偏差（但平均分数不会提高）。</target>
        </trans-unit>
        <trans-unit id="e79b1358981354168a853701629e2643ba45bf93" translate="yes" xml:space="preserve">
          <source>Increasing false positive rates such that element i is the false positive rate of predictions with score &amp;gt;= thresholds[i].</source>
          <target state="translated">增加假阳性率，使得元素i是得分&amp;gt; =阈值[i]的预测的假阳性率。</target>
        </trans-unit>
        <trans-unit id="3ca08d3a2216068596512fa76cc1f85e2464a3a8" translate="yes" xml:space="preserve">
          <source>Increasing thresholds on the decision function used to compute precision and recall.</source>
          <target state="translated">提高用于计算精度和召回的决策函数的阈值。</target>
        </trans-unit>
        <trans-unit id="7ae5f53b337e575381bac1d47d2d4a4d2e4839b6" translate="yes" xml:space="preserve">
          <source>Increasing true positive rates such that element i is the true positive rate of predictions with score &amp;gt;= thresholds[i].</source>
          <target state="translated">增加真实肯定率，使得元素i是得分&amp;gt; =阈值[i]的预测的真实肯定率。</target>
        </trans-unit>
        <trans-unit id="54206634ab03f8962d59d7c24e12c85ebd45b5e1" translate="yes" xml:space="preserve">
          <source>Incremental PCA</source>
          <target state="translated">增量PCA</target>
        </trans-unit>
        <trans-unit id="acaf3165fc4e0ddec759b9648ee12eed48089691" translate="yes" xml:space="preserve">
          <source>Incremental fit on a batch of samples.</source>
          <target state="translated">对一批样品进行增量拟合。</target>
        </trans-unit>
        <trans-unit id="a79a34aec33f8c8b084e4316cf8e243a4d6601e6" translate="yes" xml:space="preserve">
          <source>Incremental fit with X.</source>
          <target state="translated">与X的增量配合。</target>
        </trans-unit>
        <trans-unit id="87210470540ea5af2ee40f330fdeea4017f1c0aa" translate="yes" xml:space="preserve">
          <source>Incremental fit with X. All of X is processed as a single batch.</source>
          <target state="translated">与X进行增量拟合,将X全部作为单批处理。</target>
        </trans-unit>
        <trans-unit id="5b9d567927b0a80924b0a28fdea6cf19b23d2e57" translate="yes" xml:space="preserve">
          <source>Incremental principal component analysis (IPCA) is typically used as a replacement for principal component analysis (PCA) when the dataset to be decomposed is too large to fit in memory. IPCA builds a low-rank approximation for the input data using an amount of memory which is independent of the number of input data samples. It is still dependent on the input data features, but changing the batch size allows for control of memory usage.</source>
          <target state="translated">增量主成分分析(IPCA)通常被用来替代主成分分析(PCA),当需要分解的数据集太大而无法放入内存时。IPCA使用与输入数据样本数量无关的内存量为输入数据建立一个低阶近似。它仍然依赖于输入数据特征,但改变批次大小可以控制内存的使用。</target>
        </trans-unit>
        <trans-unit id="66088e706ece2d903ed2071fa2d42be9315774b6" translate="yes" xml:space="preserve">
          <source>Incremental principal components analysis (IPCA).</source>
          <target state="translated">增量主成分分析(IPCA)。</target>
        </trans-unit>
        <trans-unit id="ef7722207a6c2343d08e45f401cd00ccd19381c7" translate="yes" xml:space="preserve">
          <source>Incrementally fit the model to data.</source>
          <target state="translated">增加模型与数据的拟合度。</target>
        </trans-unit>
        <trans-unit id="505bf67b8aa7cec37d64a9ce9b03d73f70b38b8b" translate="yes" xml:space="preserve">
          <source>Incrementally fit the model to data. Fit a separate model for each output variable.</source>
          <target state="translated">增量拟合模型与数据。为每个输出变量拟合一个单独的模型。</target>
        </trans-unit>
        <trans-unit id="9e12e704fa3eb16be83d58c2167c9c4f83379a1d" translate="yes" xml:space="preserve">
          <source>Indeed many estimators are designed with the assumption that each feature takes values close to zero or more importantly that all features vary on comparable scales. In particular, metric-based and gradient-based estimators often assume approximately standardized data (centered features with unit variances). A notable exception are decision tree-based estimators that are robust to arbitrary scaling of the data.</source>
          <target state="translated">事实上,许多估计器在设计时都假设每个特征的值接近于零,或者更重要的是,所有特征的变化都在可比的范围内。特别是,基于度量和基于梯度的估计器通常假设近似于标准化的数据(单位方差的中心特征)。一个明显的例外是基于决策树的估计器,它对数据的任意缩放是稳健的。</target>
        </trans-unit>
        <trans-unit id="7933c6d72de999f40e22d3286d9c782fd636305b" translate="yes" xml:space="preserve">
          <source>Independent Component Analysis: ICA</source>
          <target state="translated">独立成分分析。ICA</target>
        </trans-unit>
        <trans-unit id="d170598045cdc9e2df037718a96d1706ed03e640" translate="yes" xml:space="preserve">
          <source>Independent component analysis separates a multivariate signal into additive subcomponents that are maximally independent. It is implemented in scikit-learn using the &lt;a href=&quot;generated/sklearn.decomposition.fastica#sklearn.decomposition.FastICA&quot;&gt;&lt;code&gt;Fast ICA&lt;/code&gt;&lt;/a&gt; algorithm. Typically, ICA is not used for reducing dimensionality but for separating superimposed signals. Since the ICA model does not include a noise term, for the model to be correct, whitening must be applied. This can be done internally using the whiten argument or manually using one of the PCA variants.</source>
          <target state="translated">独立分量分析将多变量信号分离为最大独立的加性子分量。它是使用scikit-learn使用&lt;a href=&quot;generated/sklearn.decomposition.fastica#sklearn.decomposition.FastICA&quot;&gt; &lt;code&gt;Fast ICA&lt;/code&gt; &lt;/a&gt;算法实现的。通常，ICA并不用于降低尺寸，而是用于分离叠加的信号。由于ICA模型不包含噪声项，因此对于正确的模型，必须应用白化。这可以在内部使用whiten参数完成，也可以使用PCA变体之一手动完成。</target>
        </trans-unit>
        <trans-unit id="420bdf88d0c37e0c49c684e7be83be0c3065749b" translate="yes" xml:space="preserve">
          <source>Independent component analysis, a latent variable model with non-Gaussian latent variables.</source>
          <target state="translated">独立成分分析,是一种非高斯潜变量的潜变量模型。</target>
        </trans-unit>
        <trans-unit id="e81fd2ba1ed4351b51becec6b3e044be03272d29" translate="yes" xml:space="preserve">
          <source>Independent parameter in poly/sigmoid kernel.</source>
          <target state="translated">多/西格慕核的独立参数。</target>
        </trans-unit>
        <trans-unit id="75b2e172573134b992419919380eaa4d379125c8" translate="yes" xml:space="preserve">
          <source>Independent parameter in poly/sigmoid kernel. 0 by default.</source>
          <target state="translated">在 poly/sigmoid 内核中的独立参数,默认为 0。默认为0。</target>
        </trans-unit>
        <trans-unit id="93ccb8f475af3ead0828a4d704d80fd7c1bcca2a" translate="yes" xml:space="preserve">
          <source>Independent term in decision function.</source>
          <target state="translated">决策函数中的独立项:</target>
        </trans-unit>
        <trans-unit id="d60b4ce63cb13d9b546e71bb468305b122e1ff12" translate="yes" xml:space="preserve">
          <source>Independent term in decision function. Set to 0.0 if &lt;code&gt;fit_intercept = False&lt;/code&gt;.</source>
          <target state="translated">决策功能中的独立术语。如果 &lt;code&gt;fit_intercept = False&lt;/code&gt; 则设置为0.0 。</target>
        </trans-unit>
        <trans-unit id="62ca36e367478a373b265bf6692ac1dd0b87c736" translate="yes" xml:space="preserve">
          <source>Independent term in kernel function. It is only significant in &amp;lsquo;poly&amp;rsquo; and &amp;lsquo;sigmoid&amp;rsquo;.</source>
          <target state="translated">内核函数中的独立术语。它仅在&amp;ldquo; poly&amp;rdquo;和&amp;ldquo; Sigmoid&amp;rdquo;中有意义。</target>
        </trans-unit>
        <trans-unit id="498514c5789196d4e7f38be6d2ccefad9084f660" translate="yes" xml:space="preserve">
          <source>Independent term in poly and sigmoid kernels. Ignored by other kernels.</source>
          <target state="translated">多核和sigmoid核中的独立项。被其他核忽略。</target>
        </trans-unit>
        <trans-unit id="c818388cffefe0c1449b9099e6b8c434f2466b05" translate="yes" xml:space="preserve">
          <source>Independent term in the decision function.</source>
          <target state="translated">决定函数中的独立项:</target>
        </trans-unit>
        <trans-unit id="b8069da00c91cf6e966b739b57f9cbe347e859d2" translate="yes" xml:space="preserve">
          <source>Independent term in the linear model.</source>
          <target state="translated">线性模型中的独立项。</target>
        </trans-unit>
        <trans-unit id="5c78017fad7dc4be13e21b61b07a09cb5931df33" translate="yes" xml:space="preserve">
          <source>Index of the cluster each sample belongs to.</source>
          <target state="translated">每个样本所属群的索引。</target>
        </trans-unit>
        <trans-unit id="79bbc01c7afbd569e88078c06011f6a142dc18ba" translate="yes" xml:space="preserve">
          <source>Index of the column of X to be swapped.</source>
          <target state="translated">要交换的X列的索引。</target>
        </trans-unit>
        <trans-unit id="ed1d58c02de7a13d74564b832a9effc7dd7512f7" translate="yes" xml:space="preserve">
          <source>Index of the row of X to be swapped.</source>
          <target state="translated">要交换的X行的索引。</target>
        </trans-unit>
        <trans-unit id="ec76f2d92b2be403363a104dc4a87849c9c331a9" translate="yes" xml:space="preserve">
          <source>Indexable data-structures can be arrays, lists, dataframes or scipy sparse matrices with consistent first dimension.</source>
          <target state="translated">可索引的数据结构可以是数组、列表、数据框或第一维一致的scipy稀疏矩阵。</target>
        </trans-unit>
        <trans-unit id="436737ead6b730ec05aa4979d3ab186eb46b0b4a" translate="yes" xml:space="preserve">
          <source>Indexes the data on its second axis. Integers are interpreted as positional columns, while strings can reference DataFrame columns by name. A scalar string or int should be used where &lt;code&gt;transformer&lt;/code&gt; expects X to be a 1d array-like (vector), otherwise a 2d array will be passed to the transformer. A callable is passed the input data &lt;code&gt;X&lt;/code&gt; and can return any of the above.</source>
          <target state="translated">在第二个轴上索引数据。整数被解释为位置列，而字符串可以按名称引用DataFrame列。标量字符串或整数应在 &lt;code&gt;transformer&lt;/code&gt; 期望X像一维数组（矢量）的情况下使用，否则会将二维数组传递给转换器。可调用对象将传递输入数据 &lt;code&gt;X&lt;/code&gt; ,并且可以返回上述任何一种。</target>
        </trans-unit>
        <trans-unit id="33324894ea0a98c1ef1f880dc97967e00c913318" translate="yes" xml:space="preserve">
          <source>Indicate that func accepts a sparse matrix as input. If validate is False, this has no effect. Otherwise, if accept_sparse is false, sparse matrix inputs will cause an exception to be raised.</source>
          <target state="translated">表示func接受一个稀疏矩阵作为输入。如果validate为False,则没有任何影响,否则,如果accept_sparse为false,稀疏矩阵的输入将导致异常发生。否则,如果accept_sparse为false,稀疏矩阵的输入将导致一个异常被引发。</target>
        </trans-unit>
        <trans-unit id="eb7cd0d8cb7fae1e80a3e28d3771772ae8884b48" translate="yes" xml:space="preserve">
          <source>Indicate that the input X array should be checked before calling &lt;code&gt;func&lt;/code&gt;. The possibilities are:</source>
          <target state="translated">指示在调用 &lt;code&gt;func&lt;/code&gt; 之前应检查输入X数组。可能是：</target>
        </trans-unit>
        <trans-unit id="ae286e88bfa268243cfd38132ccb40e58428bfed" translate="yes" xml:space="preserve">
          <source>Indicate that transform should forward the y argument to the inner callable.</source>
          <target state="translated">表示transform应该将y参数转发给内部的可调用对象。</target>
        </trans-unit>
        <trans-unit id="9d241566a403a6506d3449cf17d407da2b6e2613" translate="yes" xml:space="preserve">
          <source>Indicates an ordering for the class labels</source>
          <target state="translated">表示类标签的排序</target>
        </trans-unit>
        <trans-unit id="9f992c130abda366b807271662ae2ae17c7305e7" translate="yes" xml:space="preserve">
          <source>Indices according to which X will be subsampled.</source>
          <target state="translated">据此对X进行子抽样的指数。</target>
        </trans-unit>
        <trans-unit id="13d9e5d82e7cab8325fb7843fa12831d259a69f1" translate="yes" xml:space="preserve">
          <source>Indices of &lt;code&gt;components_&lt;/code&gt; in the training set.</source>
          <target state="translated">训练集中的 &lt;code&gt;components_&lt;/code&gt; 的索引。</target>
        </trans-unit>
        <trans-unit id="eeb5984b85169d88759ac10f7fe9d10f5246bc74" translate="yes" xml:space="preserve">
          <source>Indices of active variables at the end of the path.</source>
          <target state="translated">处于路径末端的活动变量的指数。</target>
        </trans-unit>
        <trans-unit id="fff8cc57ffbca371ebcb1bd938597e8d339ff509" translate="yes" xml:space="preserve">
          <source>Indices of cluster centers</source>
          <target state="translated">集群中心指数</target>
        </trans-unit>
        <trans-unit id="2c68ceeb78b6311d290c266259420efe85138435" translate="yes" xml:space="preserve">
          <source>Indices of columns in the dataset that belong to the bicluster.</source>
          <target state="translated">数据集中属于双簇的列的指数。</target>
        </trans-unit>
        <trans-unit id="d4d2ad6637f8190da67b396b7e452baac4f7558f" translate="yes" xml:space="preserve">
          <source>Indices of core samples.</source>
          <target state="translated">核心样品的指数:</target>
        </trans-unit>
        <trans-unit id="c71c298055da26ecac09942b9115f5fc73f7640d" translate="yes" xml:space="preserve">
          <source>Indices of rows in the dataset that belong to the bicluster.</source>
          <target state="translated">数据集中属于双簇的行的指数。</target>
        </trans-unit>
        <trans-unit id="a5ecd973c55633f90c97a971f74a80fe11af3310" translate="yes" xml:space="preserve">
          <source>Indices of support vectors.</source>
          <target state="translated">支持向量的指数。</target>
        </trans-unit>
        <trans-unit id="9ccd80ce2c5e0529264583d000f7c9651892271d" translate="yes" xml:space="preserve">
          <source>Indices of the approximate nearest points in the population matrix.</source>
          <target state="translated">人口矩阵中近似最近的点的指数。</target>
        </trans-unit>
        <trans-unit id="94147abfa5127a12fe3c3b0c7153a32b171d401a" translate="yes" xml:space="preserve">
          <source>Indices of the nearest points in the population matrix.</source>
          <target state="translated">人口矩阵中最近的点的指数。</target>
        </trans-unit>
        <trans-unit id="3db97f5586a1b88a52d6998bdcb68ce6424bd44e" translate="yes" xml:space="preserve">
          <source>Individual decision trees can be interpreted easily by simply visualizing the tree structure. Gradient boosting models, however, comprise hundreds of regression trees thus they cannot be easily interpreted by visual inspection of the individual trees. Fortunately, a number of techniques have been proposed to summarize and interpret gradient boosting models.</source>
          <target state="translated">单个决策树可以很容易地通过简单的可视化树结构来解释。然而,梯度提升模型由数百棵回归树组成,因此不能简单地通过对单个树的视觉检查来解释。幸运的是,已经有一些技术被提出来总结和解释梯度提升模型。</target>
        </trans-unit>
        <trans-unit id="dda28c621b6ebb6a75808a25ba823af15c148423" translate="yes" xml:space="preserve">
          <source>Individual decision trees intrinsically perform feature selection by selecting appropriate split points. This information can be used to measure the importance of each feature; the basic idea is: the more often a feature is used in the split points of a tree the more important that feature is. This notion of importance can be extended to decision tree ensembles by simply averaging the feature importance of each tree (see &lt;a href=&quot;#random-forest-feature-importance&quot;&gt;Feature importance evaluation&lt;/a&gt; for more details).</source>
          <target state="translated">各个决策树通过选择适当的分割点本质上执行特征选择。此信息可用于衡量每个功能的重要性。基本思想是：在树的拆分点中使用功能的频率越高，该功能越重要。可以通过简单地平均每棵树的特征重要性来将重要性的概念扩展到决策树集合（有关更多详细信息，请参见&lt;a href=&quot;#random-forest-feature-importance&quot;&gt;特征重要性评估&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="e3bee3019e098ad6b65e2ff793a0e712b529b8f1" translate="yes" xml:space="preserve">
          <source>Individual samples are assumed to be files stored a two levels folder structure such as the following:</source>
          <target state="translated">单个样本假设为文件,存储两级文件夹结构,如以下。</target>
        </trans-unit>
        <trans-unit id="c0fc01010c6e24d626cecdd62fd33ec75c0c929c" translate="yes" xml:space="preserve">
          <source>Individual steps may also be replaced as parameters, and non-final steps may be ignored by setting them to &lt;code&gt;None&lt;/code&gt;:</source>
          <target state="translated">也可以将各个步骤替换为参数，并将非最终步骤设置为 &lt;code&gt;None&lt;/code&gt; 可以忽略它们：</target>
        </trans-unit>
        <trans-unit id="1005c12f11beba0f3b6f9dd6a7112c31b922588d" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample</source>
          <target state="translated">每个样本的单独权重</target>
        </trans-unit>
        <trans-unit id="1fda26bba39629c5adc019bfeebf23b6ea1cae92" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample raises error if sample_weight is passed and base_estimator fit method does not support it.</source>
          <target state="translated">如果传递了sample_weight,并且base_estimator拟合方法不支持,那么每个样本的单独权重会引发错误。</target>
        </trans-unit>
        <trans-unit id="fa75f1e4d13933a19ded3aa860129fc7cab3ed7d" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample, ignored if None is passed.</source>
          <target state="translated">每个样本的单独权重,如果通过 &quot;无 &quot;则忽略。</target>
        </trans-unit>
        <trans-unit id="718e842694bb04faf5cc021c83e29c576f98c12d" translate="yes" xml:space="preserve">
          <source>Individual weights for each sample. If sample_weight is not None and solver=&amp;rsquo;auto&amp;rsquo;, the solver will be set to &amp;lsquo;cholesky&amp;rsquo;.</source>
          <target state="translated">每个样品的重量。如果sample_weight不为None且solver ='auto'，则求解器将设置为'cholesky'。</target>
        </trans-unit>
        <trans-unit id="f221e5d04a99f60098722e2605a369c8541e6c9c" translate="yes" xml:space="preserve">
          <source>Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called &amp;ldquo;curse of dimensionality&amp;rdquo;). Running a dimensionality reduction algorithm such as &lt;a href=&quot;pca&quot;&gt;PCA&lt;/a&gt; prior to k-means clustering can alleviate this problem and speed up the computations.</source>
          <target state="translated">惯性不是标准化的度量标准：我们只知道较低的值更好，而零是最佳。但是在高维空间中，欧几里得距离趋于膨胀（这是所谓的&amp;ldquo;维数诅咒&amp;rdquo;的一个实例）。在k均值聚类之前运行降维算法（例如&lt;a href=&quot;pca&quot;&gt;PCA）&lt;/a&gt;可以缓解此问题并加快计算速度。</target>
        </trans-unit>
        <trans-unit id="e010b0c9058bbaf9975d3f14818f3861029f83a1" translate="yes" xml:space="preserve">
          <source>Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes.</source>
          <target state="translated">惯性做出的假设是,集群是凸的和各向同性的,但事实并非总是如此。它对拉长的集群或形状不规则的表层反应较差。</target>
        </trans-unit>
        <trans-unit id="2d57b1c4d1f958efe3710f23677dd552a8a1384c" translate="yes" xml:space="preserve">
          <source>Inertia, or the within-cluster sum of squares criterion, can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:</source>
          <target state="translated">惯性,或集群内的平方和标准,可以被认为是对集群内部一致性程度的衡量。它存在着各种缺点。</target>
        </trans-unit>
        <trans-unit id="1b9b7d4cd56309d7954eee8c5d88a8d58559a22d" translate="yes" xml:space="preserve">
          <source>Inference of the model can be time consuming.</source>
          <target state="translated">模型的推断可能会很耗时。</target>
        </trans-unit>
        <trans-unit id="8f5d2c4b74b9f6de8f1a369f5793fd5ad3db1bd0" translate="yes" xml:space="preserve">
          <source>Influence of outliers on location and covariance estimates</source>
          <target state="translated">离群值对位置和协方差估计的影响。</target>
        </trans-unit>
        <trans-unit id="b8c700f6663aab653644d35fb6aa9a0e53919c01" translate="yes" xml:space="preserve">
          <source>Information on how to contribute. This also contains useful information for advanced users, for example how to build their own estimators.</source>
          <target state="translated">关于如何作出贡献的信息。这也包含对高级用户有用的信息,例如如何建立自己的估算器。</target>
        </trans-unit>
        <trans-unit id="c4fbdb7aab44015fbed39936864f9255a99074c1" translate="yes" xml:space="preserve">
          <source>Information-criterion based model selection is very fast, but it relies on a proper estimation of degrees of freedom, are derived for large samples (asymptotic results) and assume the model is correct, i.e. that the data are actually generated by this model. They also tend to break when the problem is badly conditioned (more features than samples).</source>
          <target state="translated">基于信息准则的模型选择速度非常快,但它依赖于自由度的正确估计,是针对大样本得出的(渐近结果),并且假设模型是正确的,即数据实际上是由这个模型产生的。当问题的条件不好(特征多于样本)时,它们也往往会被打破。</target>
        </trans-unit>
        <trans-unit id="168bf67b39abe0e5515698a4ec915c75e07ca524" translate="yes" xml:space="preserve">
          <source>Initial value for the dictionary for warm restart scenarios.</source>
          <target state="translated">暖式重启场景下字典的初始值。</target>
        </trans-unit>
        <trans-unit id="64b2b9f72c239478fc1b9e586ac8147218ca87bd" translate="yes" xml:space="preserve">
          <source>Initial value for the sparse code for warm restart scenarios.</source>
          <target state="translated">暖式重启方案的稀疏代码的初始值。</target>
        </trans-unit>
        <trans-unit id="1a6282c9a9baf1c9231fe6132d9510c0860835e2" translate="yes" xml:space="preserve">
          <source>Initial values for the components for warm restart scenarios.</source>
          <target state="translated">暖式重启情景下各组件的初始值。</target>
        </trans-unit>
        <trans-unit id="ff1f0a80e07dd6cd648bd3a5e935a909ec2cb299" translate="yes" xml:space="preserve">
          <source>Initial values for the loadings for warm restart scenarios.</source>
          <target state="translated">暖式重启情况下负荷的初始值。</target>
        </trans-unit>
        <trans-unit id="e597ad1e68022a1929058718fd92959e281b3619" translate="yes" xml:space="preserve">
          <source>Initialization of embedding. Possible options are &amp;lsquo;random&amp;rsquo;, &amp;lsquo;pca&amp;rsquo;, and a numpy array of shape (n_samples, n_components). PCA initialization cannot be used with precomputed distances and is usually more globally stable than random initialization.</source>
          <target state="translated">嵌入的初始化。可能的选项是&amp;ldquo; random&amp;rdquo;，&amp;ldquo; pca&amp;rdquo;和形状的numpy数组（n_samples，n_components）。PCA初始化不能与预先计算的距离一起使用，并且通常比随机初始化更全局稳定。</target>
        </trans-unit>
        <trans-unit id="e644154875d3e41452e4b87177ec7a98e4e47540" translate="yes" xml:space="preserve">
          <source>Initialization value for coefficients of logistic regression. Useless for liblinear solver.</source>
          <target state="translated">对数回归系数的初始化值。对liblinear求解器无用。</target>
        </trans-unit>
        <trans-unit id="ed7011971816dd0f1903ad54a411facedaee4ded" translate="yes" xml:space="preserve">
          <source>Initialization value of the sparse codes. Only used if &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt;.</source>
          <target state="translated">稀疏代码的初始化值。仅在 &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5f66672d1a211feccb2663e524389cc7bbdc9544" translate="yes" xml:space="preserve">
          <source>Initialize self. See help(type(self)) for accurate signature.</source>
          <target state="translated">初始化self。参见 help(type(self))以获得准确的签名。</target>
        </trans-unit>
        <trans-unit id="f4694fddfebcd07057574f3bedbe073aecef8cbe" translate="yes" xml:space="preserve">
          <source>Inliers are labeled 1, while outliers are labeled -1. The predict method makes use of a threshold on the raw scoring function computed by the estimator. This scoring function is accessible through the &lt;code&gt;score_samples&lt;/code&gt; method, while the threshold can be controlled by the &lt;code&gt;contamination&lt;/code&gt; parameter.</source>
          <target state="translated">异常值标记为1，而异常值标记为-1。预测方法利用了由估算器计算的原始评分函数的阈值。可通过 &lt;code&gt;score_samples&lt;/code&gt; 方法访问此评分功能，而阈值可由 &lt;code&gt;contamination&lt;/code&gt; 参数控制。</target>
        </trans-unit>
        <trans-unit id="2495be2cc170e277a5d9d5dd99a3f779ff0175c9" translate="yes" xml:space="preserve">
          <source>Inner sufficient statistics that are kept by the algorithm. Passing them at initialization is useful in online settings, to avoid loosing the history of the evolution. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix</source>
          <target state="translated">内在的充分统计,由算法保存。在初始化时传递它们在在线设置中很有用,可以避免丢失演化的历史。A(n_components,n_components)是字典协方差矩阵。B (n_features,n_components)是数据近似矩阵。</target>
        </trans-unit>
        <trans-unit id="4cb705151cf2a5dcfea8029a0e74d39c67469fa5" translate="yes" xml:space="preserve">
          <source>Inplace column scaling of a CSC/CSR matrix.</source>
          <target state="translated">CSC/CSR矩阵的列内缩放。</target>
        </trans-unit>
        <trans-unit id="15ad21d3f40808b965488683b0faaff07ad4b5e9" translate="yes" xml:space="preserve">
          <source>Inplace column scaling of a CSR matrix.</source>
          <target state="translated">输入CSR矩阵的列标尺。</target>
        </trans-unit>
        <trans-unit id="75290bdbc975498c98c7572ec067e43ddceb0c68" translate="yes" xml:space="preserve">
          <source>Inplace row normalize using the l1 norm</source>
          <target state="translated">使用L1法线进行行归一化。</target>
        </trans-unit>
        <trans-unit id="bc0180825da8415aba8c76f27c29d7e471b70c2a" translate="yes" xml:space="preserve">
          <source>Inplace row normalize using the l2 norm</source>
          <target state="translated">使用 l2 norm 进行嵌位行归一化。</target>
        </trans-unit>
        <trans-unit id="d91ae689358e283ef6d343e24e55244f1fb1cad2" translate="yes" xml:space="preserve">
          <source>Inplace row scaling of a CSR or CSC matrix.</source>
          <target state="translated">输入CSR或CSC矩阵的行比例。</target>
        </trans-unit>
        <trans-unit id="16ca749420dd58126c1f3f4ccf95ce2fc49387c9" translate="yes" xml:space="preserve">
          <source>Input array.</source>
          <target state="translated">输入阵列。</target>
        </trans-unit>
        <trans-unit id="f6fcca00499ce6b21e6114d56b450f6d3d43ccb2" translate="yes" xml:space="preserve">
          <source>Input checker utility for building a cross-validator</source>
          <target state="translated">用于建立交叉验证器的输入检查器实用程序。</target>
        </trans-unit>
        <trans-unit id="3f43a2e4863dbf39da8cfbd5e4e557f3052ec269" translate="yes" xml:space="preserve">
          <source>Input data</source>
          <target state="translated">输入数据</target>
        </trans-unit>
        <trans-unit id="41aa04ac5754100b497c803419573444b7e1d42b" translate="yes" xml:space="preserve">
          <source>Input data representation and sparsity</source>
          <target state="translated">输入数据的表示和稀疏性</target>
        </trans-unit>
        <trans-unit id="66a8a4e34fe15bd5eafb5d984d77b9c0e3866728" translate="yes" xml:space="preserve">
          <source>Input data that will be transformed.</source>
          <target state="translated">将要转换的输入数据。</target>
        </trans-unit>
        <trans-unit id="fbb05f66a147e8520f226c078931507f60d0caac" translate="yes" xml:space="preserve">
          <source>Input data that will be transformed. It cannot be sparse.</source>
          <target state="translated">将被转换的输入数据。不能是稀疏的。</target>
        </trans-unit>
        <trans-unit id="1e3e0a570b83c9cbe639106afeb9bedd23e3fcfd" translate="yes" xml:space="preserve">
          <source>Input data to be transformed.</source>
          <target state="translated">要转换的输入数据。</target>
        </trans-unit>
        <trans-unit id="071feefe9143dba47a473de169ba49367bfce443" translate="yes" xml:space="preserve">
          <source>Input data to be transformed. Use &lt;code&gt;dtype=np.float32&lt;/code&gt; for maximum efficiency. Sparse matrices are also supported, use sparse &lt;code&gt;csr_matrix&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">要转换的输入数据。使用 &lt;code&gt;dtype=np.float32&lt;/code&gt; 可获得最大效率。还支持稀疏矩阵，请使用稀疏 &lt;code&gt;csr_matrix&lt;/code&gt; 以获得最大效率。</target>
        </trans-unit>
        <trans-unit id="688f24dc1e25fac1dc510cf29e6e51a5cdee42ba" translate="yes" xml:space="preserve">
          <source>Input data used to build forests. Use &lt;code&gt;dtype=np.float32&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">输入用于构建林的数据。使用 &lt;code&gt;dtype=np.float32&lt;/code&gt; 可获得最大效率。</target>
        </trans-unit>
        <trans-unit id="ece9df27fcdc2d8935842ef4ed6ac1e8d53f8b6c" translate="yes" xml:space="preserve">
          <source>Input data, of which specified subsets are used to fit the transformers.</source>
          <target state="translated">输入数据,其中指定的子集被用来匹配变压器。</target>
        </trans-unit>
        <trans-unit id="8c020a67c0398f86d112987222d02c36283701ba" translate="yes" xml:space="preserve">
          <source>Input data, target values.</source>
          <target state="translated">输入数据,目标值。</target>
        </trans-unit>
        <trans-unit id="0d15fc28721eb2bcf2d53de59215da674d786463" translate="yes" xml:space="preserve">
          <source>Input data, used to fit transformers.</source>
          <target state="translated">输入数据,用于配合变压器。</target>
        </trans-unit>
        <trans-unit id="aee2f5203193bb3069f6e2f7b08e833e91d53841" translate="yes" xml:space="preserve">
          <source>Input data, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples and &lt;code&gt;n_features&lt;/code&gt; is the number of features.</source>
          <target state="translated">输入数据，其中 &lt;code&gt;n_samples&lt;/code&gt; 是样本数， &lt;code&gt;n_features&lt;/code&gt; 是要素数。</target>
        </trans-unit>
        <trans-unit id="8f9c683e36e31c7c38c7e8d6a2daeccf181d4c94" translate="yes" xml:space="preserve">
          <source>Input data, where n_samples is the number of samples and n_features is the number of features.</source>
          <target state="translated">输入数据,其中n_samples为样本数,n_features为特征数。</target>
        </trans-unit>
        <trans-unit id="260753716624ad2077f13f38ada17a33c0f48cba" translate="yes" xml:space="preserve">
          <source>Input data.</source>
          <target state="translated">输入数据:</target>
        </trans-unit>
        <trans-unit id="c414149f534c72aa4d2016c93404604f17aa41fd" translate="yes" xml:space="preserve">
          <source>Input data. Columns are assumed to have unit norm.</source>
          <target state="translated">输入数据。假设各列有单位法线。</target>
        </trans-unit>
        <trans-unit id="fc921091c020b61d18864b21129c4eb989ef6d69" translate="yes" xml:space="preserve">
          <source>Input data. If &lt;code&gt;None&lt;/code&gt;, the output will be the pairwise similarities between all samples in &lt;code&gt;X&lt;/code&gt;.</source>
          <target state="translated">输入数据。如果为 &lt;code&gt;None&lt;/code&gt; ，则输出将为 &lt;code&gt;X&lt;/code&gt; 中所有样本之间的成对相似性。</target>
        </trans-unit>
        <trans-unit id="afaf08d18a1423dc1a78bf3492e5ce0eb50d8638" translate="yes" xml:space="preserve">
          <source>Input data. If &lt;code&gt;dissimilarity=='precomputed'&lt;/code&gt;, the input should be the dissimilarity matrix.</source>
          <target state="translated">输入数据。如果 &lt;code&gt;dissimilarity=='precomputed'&lt;/code&gt; ，则输入应为不相似矩阵。</target>
        </trans-unit>
        <trans-unit id="c7e2acaca5145e47486c9c2928ab5532aee98fd4" translate="yes" xml:space="preserve">
          <source>Input data. If X is not provided, only the global clustering step is done.</source>
          <target state="translated">输入数据。如果没有提供X,只进行全局聚类步骤。</target>
        </trans-unit>
        <trans-unit id="d39fe8b9c923e952612de77f70ad5aff60c34542" translate="yes" xml:space="preserve">
          <source>Input object to check / convert.</source>
          <target state="translated">要检查/转换的输入对象。</target>
        </trans-unit>
        <trans-unit id="ca7fd27e447d5e335b2e80d1b2bb1406dceac239" translate="yes" xml:space="preserve">
          <source>Input object to check / convert. Must be two-dimensional and square, otherwise a ValueError will be raised.</source>
          <target state="translated">要检查/转换的输入对象。必须是二维和正方形,否则将引发一个ValueError。</target>
        </trans-unit>
        <trans-unit id="f24bf584af83fad1f47a5035ff05cdc62fd1daef" translate="yes" xml:space="preserve">
          <source>Input points.</source>
          <target state="translated">输入点:</target>
        </trans-unit>
        <trans-unit id="71b2d21e1f2e71c0dcf88bd09dfe9ef6fd499ea3" translate="yes" xml:space="preserve">
          <source>Input targets</source>
          <target state="translated">输入目标</target>
        </trans-unit>
        <trans-unit id="b58e4bce1b7ebb41f970a06dbe933522f9ad2c46" translate="yes" xml:space="preserve">
          <source>Input targets multiplied by X: X.T * y</source>
          <target state="translated">输入目标乘以X:X.T*y。</target>
        </trans-unit>
        <trans-unit id="69a0e9f3a009e8ccbba64367545e9fbe88306b19" translate="yes" xml:space="preserve">
          <source>Input targets.</source>
          <target state="translated">输入目标:</target>
        </trans-unit>
        <trans-unit id="51c9f9fb1fac83429c51404b5e9b2caee57cc2f2" translate="yes" xml:space="preserve">
          <source>Input validation for standard estimators.</source>
          <target state="translated">标准估计器的输入验证。</target>
        </trans-unit>
        <trans-unit id="346da7aa7d7e4eb884706a35a69404b7d3fc9daa" translate="yes" xml:space="preserve">
          <source>Input validation on an array, list, sparse matrix or similar.</source>
          <target state="translated">在数组、列表、稀疏矩阵或类似情况下进行输入验证。</target>
        </trans-unit>
        <trans-unit id="8344beaf285df55c120907e8b74a9a1f87253895" translate="yes" xml:space="preserve">
          <source>Inputs &lt;code&gt;X&lt;/code&gt; are 4 independent features uniformly distributed on the intervals:</source>
          <target state="translated">输入 &lt;code&gt;X&lt;/code&gt; 是在区间上均匀分布的4个独立特征：</target>
        </trans-unit>
        <trans-unit id="bcbcb56a88ddeef69ce01bcc9a78a456071e2cf0" translate="yes" xml:space="preserve">
          <source>Inputs &lt;code&gt;X&lt;/code&gt; are independent features uniformly distributed on the interval [0, 1]. The output &lt;code&gt;y&lt;/code&gt; is created according to the formula:</source>
          <target state="translated">输入 &lt;code&gt;X&lt;/code&gt; 是独立的特征，均匀分布在间隔[0，1]上。根据以下公式创建输出 &lt;code&gt;y&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="fffa8f8e3b740ecfc583b9bf477ffcbdb298b533" translate="yes" xml:space="preserve">
          <source>Inserts new data into the already fitted LSH Forest.</source>
          <target state="translated">将新数据插入到已经拟合的LSH森林中。</target>
        </trans-unit>
        <trans-unit id="e78cacac23222d74508b7d4b79fbb8a5cb79c6fc" translate="yes" xml:space="preserve">
          <source>Inserts new data into the already fitted LSH Forest. Cost is proportional to new total size, so additions should be batched.</source>
          <target state="translated">在已经拟合的LSH森林中插入新数据。成本与新的总规模成正比,所以添加时应分批进行。</target>
        </trans-unit>
        <trans-unit id="58768f013d8600aed4da42a9f67c30c0b0e7f2be" translate="yes" xml:space="preserve">
          <source>Instead of computing with a set of cardinality &amp;lsquo;n choose k&amp;rsquo;, where n is the number of samples and k is the number of subsamples (at least number of features), consider only a stochastic subpopulation of a given maximal size if &amp;lsquo;n choose k&amp;rsquo; is larger than max_subpopulation. For other than small problem sizes this parameter will determine memory usage and runtime if n_subsamples is not changed.</source>
          <target state="translated">与其使用基数为&amp;ldquo; n select k&amp;rdquo;的集合（其中n是样本数，k是子样本数（至少特征数））进行计算，如果选择&amp;ldquo; n select&amp;rdquo;，则仅考虑给定最大大小的随机子种群k'大于max_subpopulation。对于较小的问题，如果不更改n_subsamples，则此参数将确定内存使用情况和运行时间。</target>
        </trans-unit>
        <trans-unit id="ce6171dee8019fcd810326710a2a425d2ef2e21c" translate="yes" xml:space="preserve">
          <source>Instead of giving a vector result, the LARS solution consists of a curve denoting the solution for each value of the L1 norm of the parameter vector. The full coefficients path is stored in the array &lt;code&gt;coef_path_&lt;/code&gt;, which has size (n_features, max_features+1). The first column is always zero.</source>
          <target state="translated">LARS解决方案没有给出矢量结果，而是由一条曲线表示，该曲线表示针对参数矢量L1范数的每个值的解决方案。完整系数路径存储在数组 &lt;code&gt;coef_path_&lt;/code&gt; 中，该数组的大小为（n_features，max_features + 1）。第一列始终为零。</target>
        </trans-unit>
        <trans-unit id="1c47d2e573aac76a94273f4c46c066cf6f2a8ad1" translate="yes" xml:space="preserve">
          <source>Instead of tweaking the parameters of the various components of the chain, it is possible to run an exhaustive search of the best parameters on a grid of possible values. We try out all classifiers on either words or bigrams, with or without idf, and with a penalty parameter of either 0.01 or 0.001 for the linear SVM:</source>
          <target state="translated">而不是调整链上各个组件的参数,而是可以在一个可能值的网格上运行一个最佳参数的详尽搜索。我们在单词或大词组上试用所有分类器,有无idf,线性SVM的惩罚参数为0.01或0.001。</target>
        </trans-unit>
        <trans-unit id="db33f6d449a5c5c7a074dd03bb12ec7fc077641c" translate="yes" xml:space="preserve">
          <source>Instead the caller is expected to either set explicitly &lt;code&gt;with_centering=False&lt;/code&gt; (in that case, only variance scaling will be performed on the features of the CSR matrix) or to call &lt;code&gt;X.toarray()&lt;/code&gt; if he/she expects the materialized dense array to fit in memory.</source>
          <target state="translated">取而代之的是，调用者应该显式设置 &lt;code&gt;with_centering=False&lt;/code&gt; （在这种情况下，将仅对CSR矩阵的特征执行方差缩放），或者如果调用者 &lt;code&gt;X.toarray()&lt;/code&gt; 化的密集数组适合，则调用X.toarray（）。在记忆中。</target>
        </trans-unit>
        <trans-unit id="f080b277d95a6b1142abd6eb9ea11a07abcb1917" translate="yes" xml:space="preserve">
          <source>Instead the caller is expected to either set explicitly &lt;code&gt;with_mean=False&lt;/code&gt; (in that case, only variance scaling will be performed on the features of the CSC matrix) or to call &lt;code&gt;X.toarray()&lt;/code&gt; if he/she expects the materialized dense array to fit in memory.</source>
          <target state="translated">取而代之的是，预期调用者要么显式设置 &lt;code&gt;with_mean=False&lt;/code&gt; （在这种情况下，仅对CSC矩阵的特征执行方差缩放），或者如果调用 &lt;code&gt;X.toarray()&lt;/code&gt; 希望物化密集数组适合，则调用X.toarray（）。在记忆中。</target>
        </trans-unit>
        <trans-unit id="b11f1ba476938b01d18dd66d0c3826617a20151e" translate="yes" xml:space="preserve">
          <source>Instead, the distribution over \(w\) is assumed to be an axis-parallel, elliptical Gaussian distribution.</source>
          <target state="translated">取而代之的是,假设对/(w/)的分布是一个轴平行的椭圆高斯分布。</target>
        </trans-unit>
        <trans-unit id="33a2873657f7cc53fbafced5857dd217868f1368" translate="yes" xml:space="preserve">
          <source>Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given &lt;code&gt;encoding&lt;/code&gt;. By default, it is &amp;lsquo;strict&amp;rsquo;, meaning that a UnicodeDecodeError will be raised. Other values are &amp;lsquo;ignore&amp;rsquo; and &amp;lsquo;replace&amp;rsquo;.</source>
          <target state="translated">如果给出一个字节序列进行分析，但其中包含的字符不是给定 &lt;code&gt;encoding&lt;/code&gt; 字符，该怎么办。默认情况下，它是&amp;ldquo;严格的&amp;rdquo;，这意味着将引发UnicodeDecodeError。其他值是&amp;ldquo;忽略&amp;rdquo;和&amp;ldquo;替换&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="d22b7ba366228e805a5817961de5812cf7af3a5e" translate="yes" xml:space="preserve">
          <source>Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given &lt;code&gt;encoding&lt;/code&gt;. Passed as keyword argument &amp;lsquo;errors&amp;rsquo; to bytes.decode.</source>
          <target state="translated">如果给出一个字节序列进行分析，但其中包含的字符不是给定 &lt;code&gt;encoding&lt;/code&gt; 字符，该怎么办。作为关键字参数&amp;ldquo;错误&amp;rdquo;传递给bytes.decode。</target>
        </trans-unit>
        <trans-unit id="98ae123013fca86e4cc21f01a470888e055215cc" translate="yes" xml:space="preserve">
          <source>Integer array of labels. If not provided, labels will be inferred from y_true and y_pred.</source>
          <target state="translated">整数个标签数组,如果没有提供,标签将由y_true和y_pred推断。如果没有提供,标签将由y_true和y_pred推断。</target>
        </trans-unit>
        <trans-unit id="fb86aae8ca1d5ea8c3a2f0216a09b115ca2c4371" translate="yes" xml:space="preserve">
          <source>Intercept (a.k.a. bias) added to the decision function.</source>
          <target state="translated">在决策函数中加入截距(也就是偏置)。</target>
        </trans-unit>
        <trans-unit id="02c60e7ce23b1ba7da9aadaca682e74dd23bd987" translate="yes" xml:space="preserve">
          <source>Intercept term.</source>
          <target state="translated">拦截项。</target>
        </trans-unit>
        <trans-unit id="077392291decf12f1b024c471b5bea6bcd10e56c" translate="yes" xml:space="preserve">
          <source>Internal sufficient statistics that are kept by the algorithm. Keeping them is useful in online settings, to avoid loosing the history of the evolution, but they shouldn&amp;rsquo;t have any use for the end user. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix</source>
          <target state="translated">该算法保留的内部足够的统计信息。保留它们在联机设置中很有用，以避免丢失演化历史，但对于最终用户而言，它们不应有任何用处。A（n_components，n_components）是字典协方差矩阵。B（n_features，n_components）是数据近似矩阵</target>
        </trans-unit>
        <trans-unit id="14f5f43f255d2aa36ff5598f3fb3ace3d6d04389" translate="yes" xml:space="preserve">
          <source>Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian.</source>
          <target state="translated">在内部,Laplace近似用于用高斯来近似非高斯后验。</target>
        </trans-unit>
        <trans-unit id="0b925a293764508f95547bba83dbd960f81b58e6" translate="yes" xml:space="preserve">
          <source>Internally, the target &lt;code&gt;y&lt;/code&gt; is always converted into a 2-dimensional array to be used by scikit-learn transformers. At the time of prediction, the output will be reshaped to a have the same number of dimensions as &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">在内部，目标 &lt;code&gt;y&lt;/code&gt; 始终会转换为二维数组，以供scikit-learn转换器使用。在预测时，输出将被整形为具有与 &lt;code&gt;y&lt;/code&gt; 相同数量的维。</target>
        </trans-unit>
        <trans-unit id="a6c7ce41d2f8fb06b74993c6b6972d365c014219" translate="yes" xml:space="preserve">
          <source>Internally, we use &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; and &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; to handle all computations. These libraries are wrapped using C and Cython.</source>
          <target state="translated">在内部，我们使用&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;和&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt;处理所有计算。这些库使用C和Cython包装。</target>
        </trans-unit>
        <trans-unit id="a2d983855292bfa7e006da9cc5e0020136bdcd0e" translate="yes" xml:space="preserve">
          <source>Interruption of multiprocesses jobs with &amp;lsquo;Ctrl-C&amp;rsquo;</source>
          <target state="translated">使用&amp;ldquo; Ctrl-C&amp;rdquo;中断多进程作业</target>
        </trans-unit>
        <trans-unit id="c8666d7061618ff72086e37218ea77619df4e168" translate="yes" xml:space="preserve">
          <source>Intuitive interpretation: clustering with bad V-measure can be &lt;strong&gt;qualitatively analyzed in terms of homogeneity and completeness&lt;/strong&gt; to better feel what &amp;lsquo;kind&amp;rsquo; of mistakes is done by the assignment.</source>
          <target state="translated">直观的解释：与劣V-措施聚类可以&lt;strong&gt;定性分析的同质性和完整性的条款&lt;/strong&gt;，以更好地感受什么错误的&amp;ldquo;类&amp;rdquo;是由分配完成。</target>
        </trans-unit>
        <trans-unit id="b3bf13a5a75c5bcae60f4d54f651f7f504b37960" translate="yes" xml:space="preserve">
          <source>Intuitively, &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Precision&quot;&gt;precision&lt;/a&gt; is the ability of the classifier not to label as positive a sample that is negative, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Recall&quot;&gt;recall&lt;/a&gt; is the ability of the classifier to find all the positive samples.</source>
          <target state="translated">直觉上，&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Precision&quot;&gt;精度&lt;/a&gt;是分类器不将阴性样本标记为正样本的能力，&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall#Recall&quot;&gt;召回率&lt;/a&gt;是分类器查找所有阳性样本的能力。</target>
        </trans-unit>
        <trans-unit id="d7d0867c1bea54b1fdaded0f6d4a137c7b95792e" translate="yes" xml:space="preserve">
          <source>Intuitively, one can also think of a histogram as a stack of blocks, one block per point. By stacking the blocks in the appropriate grid space, we recover the histogram. But what if, instead of stacking the blocks on a regular grid, we center each block on the point it represents, and sum the total height at each location? This idea leads to the lower-left visualization. It is perhaps not as clean as a histogram, but the fact that the data drive the block locations mean that it is a much better representation of the underlying data.</source>
          <target state="translated">直观上,我们也可以把直方图看成是一个块的堆叠,每个点一个块。通过在适当的网格空间中堆叠区块,我们就可以恢复直方图。但是,如果我们不把块堆叠在规则的网格上,而是把每个块集中在它所代表的点上,并对每个位置的总高度进行求和呢?这个想法导致了左下角的可视化。它也许没有直方图那么干净,但数据驱动块位置的事实意味着它能更好地表达底层数据。</target>
        </trans-unit>
        <trans-unit id="a413ab311fb3ee6ba0089ad38e522b4769b873e8" translate="yes" xml:space="preserve">
          <source>Intuitively, the &lt;code&gt;gamma&lt;/code&gt; parameter defines how far the influence of a single training example reaches, with low values meaning &amp;lsquo;far&amp;rsquo; and high values meaning &amp;lsquo;close&amp;rsquo;. The &lt;code&gt;gamma&lt;/code&gt; parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.</source>
          <target state="translated">直观地， &lt;code&gt;gamma&lt;/code&gt; 参数定义单个训练示例的影响达到的程度，低值表示&amp;ldquo;远&amp;rdquo;，高值表示&amp;ldquo;接近&amp;rdquo;。的 &lt;code&gt;gamma&lt;/code&gt; 参数可以被看作是由模型支持向量选择的样本的影响的半径的倒数。</target>
        </trans-unit>
        <trans-unit id="0af317bc827b64b57bcc63f42ad5928a61b8cb1f" translate="yes" xml:space="preserve">
          <source>Intuitively, this matrix can be interpreted as a matrix of pseudo features (the points raised to some power). The matrix is akin to (but different from) the matrix induced by a polynomial kernel.</source>
          <target state="translated">直观地讲,这个矩阵可以解释为一个伪特征的矩阵(点提高到某个幂级)。该矩阵类似于(但又不同于)多项式核所引起的矩阵。</target>
        </trans-unit>
        <trans-unit id="20dc7b25181635b005eb94a34d79a1d1ef88f5eb" translate="yes" xml:space="preserve">
          <source>Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.</source>
          <target state="translated">正则化强度的倒数;必须是正数浮点数。与支持向量机一样,数值越小,正则化程度越强。</target>
        </trans-unit>
        <trans-unit id="33bf667eeef9f8f87ba0b221f0610de05f350c0d" translate="yes" xml:space="preserve">
          <source>Inverse the transformation.</source>
          <target state="translated">逆向变换。</target>
        </trans-unit>
        <trans-unit id="c6d1024dc4c416573a81f58d53b390ce79e27d74" translate="yes" xml:space="preserve">
          <source>Inverse the transformation. Return a vector of size nb_features with the values of Xred assigned to each group of features</source>
          <target state="translated">反向变换。返回一个大小为nb_features的向量,其中包含分配给每组特征的Xred值。</target>
        </trans-unit>
        <trans-unit id="68776e7556a932d7c1772f163bcd0ea5d3036f2f" translate="yes" xml:space="preserve">
          <source>Inverse transform matrix. Only available when &lt;code&gt;fit_inverse_transform&lt;/code&gt; is True.</source>
          <target state="translated">逆变换矩阵。仅在 &lt;code&gt;fit_inverse_transform&lt;/code&gt; 为True 时可用。</target>
        </trans-unit>
        <trans-unit id="a53229d5506328691d3b32e8898ac28b845cf1d2" translate="yes" xml:space="preserve">
          <source>Inverse transformed array.</source>
          <target state="translated">逆向变换阵列。</target>
        </trans-unit>
        <trans-unit id="6d0db9202e10d4b2a1eb16356a668a8027a929fc" translate="yes" xml:space="preserve">
          <source>Invokes the passed method name of the passed estimator. For method=&amp;rsquo;predict_proba&amp;rsquo;, the columns correspond to the classes in sorted order.</source>
          <target state="translated">调用传递的估计量的传递的方法名称。对于method ='predict_proba'，列对应于已排序的类。</target>
        </trans-unit>
        <trans-unit id="d37270b3f9f32ae673296712eb4a194d52812d8f" translate="yes" xml:space="preserve">
          <source>Invoking the &lt;code&gt;fit&lt;/code&gt; method on the &lt;code&gt;VotingClassifier&lt;/code&gt; will fit clones of those original estimators that will be stored in the class attribute &lt;code&gt;self.estimators_&lt;/code&gt;. An estimator can be set to &lt;code&gt;None&lt;/code&gt; using &lt;code&gt;set_params&lt;/code&gt;.</source>
          <target state="translated">在 &lt;code&gt;VotingClassifier&lt;/code&gt; 上调用 &lt;code&gt;fit&lt;/code&gt; 方法将对将存储在类属性 &lt;code&gt;self.estimators_&lt;/code&gt; 中的那些原始估计量的克隆进行调整。可以使用 &lt;code&gt;set_params&lt;/code&gt; 将估算器设置为 &lt;code&gt;None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="42b4a555867c758d3e1c4078b74a325ea5729a8f" translate="yes" xml:space="preserve">
          <source>Iris-Setosa</source>
          <target state="translated">Iris-Setosa</target>
        </trans-unit>
        <trans-unit id="0e4a66fb06fc31fa26bb267122a303163869bd83" translate="yes" xml:space="preserve">
          <source>Iris-Versicolour</source>
          <target state="translated">Iris-Versicolour</target>
        </trans-unit>
        <trans-unit id="c11352543468838c7f536aa067f758dd5cf065cc" translate="yes" xml:space="preserve">
          <source>Iris-Virginica</source>
          <target state="translated">Iris-Virginica</target>
        </trans-unit>
        <trans-unit id="bb0f5655f4fe0f8adc1a787c53ae1e836f4be186" translate="yes" xml:space="preserve">
          <source>Iso-probability lines for Gaussian Processes classification (GPC)</source>
          <target state="translated">高斯过程分类(GPC)的等概率线。</target>
        </trans-unit>
        <trans-unit id="2b50512539d0e21a6687a0e4968f704ff8cc80fe" translate="yes" xml:space="preserve">
          <source>Isolation Forest Algorithm</source>
          <target state="translated">隔离林算法</target>
        </trans-unit>
        <trans-unit id="00617c131e78d4c4ef41c400773154d235217731" translate="yes" xml:space="preserve">
          <source>IsolationForest example</source>
          <target state="translated">隔离林示例</target>
        </trans-unit>
        <trans-unit id="3a2755971bbebbe11d424139f5382799c401f262" translate="yes" xml:space="preserve">
          <source>Isomap Embedding</source>
          <target state="translated">等值线图嵌入</target>
        </trans-unit>
        <trans-unit id="fe769adce6faebe1974c95ecc576637486cbe643" translate="yes" xml:space="preserve">
          <source>Isotone Optimization in R : Pool-Adjacent-Violators Algorithm (PAVA) and Active Set Methods Leeuw, Hornik, Mair Journal of Statistical Software 2009</source>
          <target state="translated">R中的同调优化:池-相邻-违约者算法(PAVA)和主动集方法 Leeuw,Hornik,Mair 统计软件杂志 2009年</target>
        </trans-unit>
        <trans-unit id="906c68921cb26d68c13066c88efbe4d7d97d1205" translate="yes" xml:space="preserve">
          <source>Isotonic Median Regression: A Linear Programming Approach Nilotpal Chakravarti Mathematics of Operations Research Vol. 14, No. 2 (May, 1989), pp. 303-308</source>
          <target state="translated">同调中值回归。Nilotpal Chakravarti,《运筹学》第14卷第2期(1989年5月),第303-308页。</target>
        </trans-unit>
        <trans-unit id="73b36c35655a3846d59943ac16d2df052178f43b" translate="yes" xml:space="preserve">
          <source>Isotonic Regression</source>
          <target state="translated">同位素回归</target>
        </trans-unit>
        <trans-unit id="c214056f848cd4e39c52f175df94ac0d422815da" translate="yes" xml:space="preserve">
          <source>Isotonic fit of y.</source>
          <target state="translated">y的同调配合。</target>
        </trans-unit>
        <trans-unit id="350a83a6eea9b1b3e9903b81e34485a4ebed4999" translate="yes" xml:space="preserve">
          <source>Isotonic regression model.</source>
          <target state="translated">同位素回归模型。</target>
        </trans-unit>
        <trans-unit id="7c5ae8804283297e052b100d9986cbd5cd009701" translate="yes" xml:space="preserve">
          <source>Issue a warning when the function is called/the class is instantiated and adds a warning to the docstring.</source>
          <target state="translated">当函数被调用/类被实例化时发出警告,并在docstring中添加一个警告。</target>
        </trans-unit>
        <trans-unit id="4de98053a0f4264ca5362b17521388fcee7300ef" translate="yes" xml:space="preserve">
          <source>It adapts to the data at hand.</source>
          <target state="translated">它能适应手头的数据。</target>
        </trans-unit>
        <trans-unit id="d00cd2eb4ac76616c412b13d0e3140cdba7905a2" translate="yes" xml:space="preserve">
          <source>It allows specifying multiple metrics for evaluation.</source>
          <target state="translated">它允许指定多个评估指标。</target>
        </trans-unit>
        <trans-unit id="f7ac040f9311efb440d25da16c027a81ab8e3ad5" translate="yes" xml:space="preserve">
          <source>It also can be expressed in set cardinality formulation:</source>
          <target state="translated">也可以用集卡式表达。</target>
        </trans-unit>
        <trans-unit id="aedad5338d2a0edf1701c1d5c20ad5954bfd8c84" translate="yes" xml:space="preserve">
          <source>It can also be directly used as the &lt;code&gt;kernel&lt;/code&gt; argument:</source>
          <target state="translated">它也可以直接用作 &lt;code&gt;kernel&lt;/code&gt; 参数：</target>
        </trans-unit>
        <trans-unit id="9678c3fa14b59b03394b92e8e0080149cf3f64c8" translate="yes" xml:space="preserve">
          <source>It can also be used as a pre-processing step for estimators that consider boolean random variables (e.g. modelled using the Bernoulli distribution in a Bayesian setting).</source>
          <target state="translated">它也可以作为考虑布尔随机变量的估计器的预处理步骤(例如,在贝叶斯环境中使用伯努利分布建模)。</target>
        </trans-unit>
        <trans-unit id="94554b8e34efbb328f639daf4ccda2adc301f69d" translate="yes" xml:space="preserve">
          <source>It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.</source>
          <target state="translated">它也可以用来将非数字标签(只要它们是可哈希和可比较的)转换为数字标签。</target>
        </trans-unit>
        <trans-unit id="f96e6d208d3d13906cbf9cd9c045b4122e99a4e4" translate="yes" xml:space="preserve">
          <source>It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels:</source>
          <target state="translated">它也可以用来将非数字标签(只要它们是可哈希和可比较的)转换为数字标签。</target>
        </trans-unit>
        <trans-unit id="52f6dad43e1775ee0bbb04be9ef515bae958e0a2" translate="yes" xml:space="preserve">
          <source>It can also have a regularization term added to the loss function that shrinks model parameters to prevent overfitting.</source>
          <target state="translated">它还可以在损失函数中加入一个正则化项,缩小模型参数以防止过拟合。</target>
        </trans-unit>
        <trans-unit id="998bd5d13863b9f1e85f5a6708bf38f625d563b0" translate="yes" xml:space="preserve">
          <source>It can also use the scipy.sparse.linalg ARPACK implementation of the truncated SVD.</source>
          <target state="translated">它也可以使用scipy.sparse.linalg ARPACK实现截断的SVD。</target>
        </trans-unit>
        <trans-unit id="a5f9c7ba1af0aaff84e6645b602de8095311d995" translate="yes" xml:space="preserve">
          <source>It can be called with parameters &lt;code&gt;(estimator, X, y)&lt;/code&gt;, where &lt;code&gt;estimator&lt;/code&gt; is the model that should be evaluated, &lt;code&gt;X&lt;/code&gt; is validation data, and &lt;code&gt;y&lt;/code&gt; is the ground truth target for &lt;code&gt;X&lt;/code&gt; (in the supervised case) or &lt;code&gt;None&lt;/code&gt; (in the unsupervised case).</source>
          <target state="translated">可以使用参数 &lt;code&gt;(estimator, X, y)&lt;/code&gt; 进行调用，其中 &lt;code&gt;estimator&lt;/code&gt; 是应评估的模型， &lt;code&gt;X&lt;/code&gt; 是验证数据， &lt;code&gt;y&lt;/code&gt; 是 &lt;code&gt;X&lt;/code&gt; （在有监督的情况下）或 &lt;code&gt;None&lt;/code&gt; （在无监督的情况下）的真实目标。案件）。</target>
        </trans-unit>
        <trans-unit id="9a6afc7a825a539f282e6908ea3004d59da105e7" translate="yes" xml:space="preserve">
          <source>It can be downloaded/loaded using the &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_california_housing#sklearn.datasets.fetch_california_housing&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_california_housing&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">可以使用&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_california_housing#sklearn.datasets.fetch_california_housing&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_california_housing&lt;/code&gt; &lt;/a&gt;函数下载/加载。</target>
        </trans-unit>
        <trans-unit id="afe5a10e4cd1db3d3b82e37290c3a4b0be9670c8" translate="yes" xml:space="preserve">
          <source>It can be interpreted as a weighted difference per entry.</source>
          <target state="translated">可以理解为每个条目的加权差额。</target>
        </trans-unit>
        <trans-unit id="d898e853ebb8a8ce7531765c1307531f5ab826e6" translate="yes" xml:space="preserve">
          <source>It can be noted that k-means (and minibatch k-means) are very sensitive to feature scaling and that in this case the IDF weighting helps improve the quality of the clustering by quite a lot as measured against the &amp;ldquo;ground truth&amp;rdquo; provided by the class label assignments of the 20 newsgroups dataset.</source>
          <target state="translated">可以注意到，k均值（和小批量k均值）对特征缩放非常敏感，在这种情况下，IDF加权有助于将聚类的质量提高很多，这是针对20个新闻组数据集的类标签分配。</target>
        </trans-unit>
        <trans-unit id="074f1a9d1908eeea94dd9624b8e4e74f70971f1f" translate="yes" xml:space="preserve">
          <source>It can be seen from the plots that the results of &lt;a href=&quot;../../modules/linear_model#omp&quot;&gt;Orthogonal Matching Pursuit (OMP)&lt;/a&gt; with two non-zero coefficients is a bit less biased than when keeping only one (the edges look less prominent). It is in addition closer from the ground truth in Frobenius norm.</source>
          <target state="translated">从图中可以看出，具有两个非零系数的&lt;a href=&quot;../../modules/linear_model#omp&quot;&gt;正交匹配追踪（OMP）的结果&lt;/a&gt;与仅保留一个系数时（边缘看起来不太明显）相比，偏差较小。此外，它离Frobenius规范中的地面真理更近。</target>
        </trans-unit>
        <trans-unit id="0cf8fb702abea7c91fd29d6847c4f9bb34be57f9" translate="yes" xml:space="preserve">
          <source>It can be shown that the \(\nu\)-SVC formulation is a reparameterization of the \(C\)-SVC and therefore mathematically equivalent.</source>
          <target state="translated">可以看出,/(C/nu/)-SVC公式是/(C/C/)-SVC的重新参数化,因此在数学上是等价的。</target>
        </trans-unit>
        <trans-unit id="157aa7190191e4be1be236c86eabe4e67d5e1efd" translate="yes" xml:space="preserve">
          <source>It can be used for univariate features selection, read more in the &lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;User Guide&lt;/a&gt;.</source>
          <target state="translated">它可用于单变量特征选择，请参阅《&lt;a href=&quot;../feature_selection#univariate-feature-selection&quot;&gt;用户指南》中的更多内容&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="711c50760d4f6c264d6b8a92b5297202a600fa0b" translate="yes" xml:space="preserve">
          <source>It can be used to include regularization parameters in the estimation procedure.</source>
          <target state="translated">它可以用来在估计程序中加入正则化参数。</target>
        </trans-unit>
        <trans-unit id="e52b5bc871c7db656a1b43abcce93011714c74d2" translate="yes" xml:space="preserve">
          <source>It does not require a learning rate.</source>
          <target state="translated">它不需要学习率。</target>
        </trans-unit>
        <trans-unit id="4d19424efe5e9e20338f3273e68fb2ccbb132c12" translate="yes" xml:space="preserve">
          <source>It doesn&amp;rsquo;t give a single metric to use as an objective for clustering optimisation.</source>
          <target state="translated">它没有给出一个单一的指标来用作集群优化的目标。</target>
        </trans-unit>
        <trans-unit id="0a46e66645323d1f8dad68441e68b478eacb85f4" translate="yes" xml:space="preserve">
          <source>It has been observed in [Hoyer, 2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;[2]&lt;/a&gt; that, when carefully constrained, &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; can produce a parts-based representation of the dataset, resulting in interpretable models. The following example displays 16 sparse components found by &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; from the images in the Olivetti faces dataset, in comparison with the PCA eigenfaces.</source>
          <target state="translated">在[Hoyer，2004] &lt;a href=&quot;#id12&quot; id=&quot;id6&quot;&gt;[2]&lt;/a&gt;中已经观察到，经过仔细约束，&lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt;可以产生基于零件的数据集表示，从而产生可解释的模型。以下示例显示了与PCA特征脸相比，&lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt;从Olivetti人脸数据集中的图像中发现的16个稀疏分量。</target>
        </trans-unit>
        <trans-unit id="8dcb00db48002a7fdf6f8f4ffd6c64f833e7dfb1" translate="yes" xml:space="preserve">
          <source>It has properties that are similar to the exponentiated chi squared kernel often used in computer vision, but allows for a simple Monte Carlo approximation of the feature map.</source>
          <target state="translated">它的特性类似于计算机视觉中经常使用的指数化chi平方核,但允许对特征图进行简单的蒙特卡洛近似。</target>
        </trans-unit>
        <trans-unit id="37dc8b6f979214e286ace27e5272dd91d61126bc" translate="yes" xml:space="preserve">
          <source>It has proven useful in ML applied to noiseless data. See e.g. &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;Machine learning for quantum mechanics in a nutshell&lt;/a&gt;.</source>
          <target state="translated">它被证明对应用于无噪声数据的机器学习很有用。&lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/&quot;&gt;简而言之，&lt;/a&gt;参见例如机器学习中的量子力学。</target>
        </trans-unit>
        <trans-unit id="e3ea912466304dbf8e7c52052ad02a2c286c1ad1" translate="yes" xml:space="preserve">
          <source>It implements a variant of Random Kitchen Sinks.[1]</source>
          <target state="translated">它实现了随机厨房水槽的一个变种[1]。</target>
        </trans-unit>
        <trans-unit id="74d404b8e11acc4d9a6402146bf70c71d78d2e94" translate="yes" xml:space="preserve">
          <source>It is a Linear Model trained with an L1 prior as regularizer.</source>
          <target state="translated">它是一个线性模型,以L1先验作为正则化器进行训练。</target>
        </trans-unit>
        <trans-unit id="971eff281c404ac7ff23799c2f2e17c93f769de1" translate="yes" xml:space="preserve">
          <source>It is a memory-efficient, online-learning algorithm provided as an alternative to &lt;a href=&quot;sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt;&lt;code&gt;MiniBatchKMeans&lt;/code&gt;&lt;/a&gt;. It constructs a tree data structure with the cluster centroids being read off the leaf. These can be either the final cluster centroids or can be provided as input to another clustering algorithm such as &lt;a href=&quot;sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt;&lt;code&gt;AgglomerativeClustering&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">它是一种内存有效的在线学习算法，可以替代&lt;a href=&quot;sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt; &lt;code&gt;MiniBatchKMeans&lt;/code&gt; &lt;/a&gt;。它构造一个树数据结构，其中簇质心从叶中读取。这些可以是最终的聚类质心，也可以作为其他聚类算法（例如&lt;a href=&quot;sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt; &lt;code&gt;AgglomerativeClustering&lt;/code&gt; )的&lt;/a&gt;输入提供。</target>
        </trans-unit>
        <trans-unit id="1a9933b24a1c1c056c0577574d6613078e271127" translate="yes" xml:space="preserve">
          <source>It is a parameter that control learning rate in the online learning method. The value should be set between (0.5, 1.0] to guarantee asymptotic convergence. When the value is 0.0 and batch_size is &lt;code&gt;n_samples&lt;/code&gt;, the update method is same as batch learning. In the literature, this is called kappa.</source>
          <target state="translated">它是控制在线学习方法中学习率的参数。该值应设置在（0.5，1.0]之间，以确保渐近收敛。当值为0.0且batch_size为 &lt;code&gt;n_samples&lt;/code&gt; 时，更新方法与批处理学习相同，在文献中，这称为kappa。</target>
        </trans-unit>
        <trans-unit id="15f125826dc7be5a6512e2415a2ab7dc87afbdb7" translate="yes" xml:space="preserve">
          <source>It is advised to set the parameter &lt;code&gt;epsilon&lt;/code&gt; to 1.35 to achieve 95% statistical efficiency.</source>
          <target state="translated">建议将参数 &lt;code&gt;epsilon&lt;/code&gt; 设置为1.35，以达到95％的统计效率。</target>
        </trans-unit>
        <trans-unit id="542f7392581e6c4610879b36a37978fc74650959" translate="yes" xml:space="preserve">
          <source>It is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice.</source>
          <target state="translated">在文本处理界,使用二进制特征值也是很常见的(可能是为了简化概率推理),即使归一化计数(也就是术语频率)或TF-IDF值特征在实践中往往表现得稍好。</target>
        </trans-unit>
        <trans-unit id="005dab4eb22b6ead110b29a8850c3898f552d977" translate="yes" xml:space="preserve">
          <source>It is also known as the Variance Ratio Criterion.</source>
          <target state="translated">它也被称为方差比标准。</target>
        </trans-unit>
        <trans-unit id="4221678f503b8b29e4ba191986027706279048ac" translate="yes" xml:space="preserve">
          <source>It is also possible to constrain the dictionary and/or code to be positive to match constraints that may be present in the data. Below are the faces with different positivity constraints applied. Red indicates negative values, blue indicates positive values, and white represents zeros.</source>
          <target state="translated">也可以对字典和/或代码进行正向约束,以匹配数据中可能存在的约束。下面是应用了不同正性约束的面。红色表示负值,蓝色表示正值,白色表示零。</target>
        </trans-unit>
        <trans-unit id="cd4e94997f77b01819c6451ba1d9a9d98a78db7c" translate="yes" xml:space="preserve">
          <source>It is also possible to efficiently produce a sparse graph showing the connections between neighboring points:</source>
          <target state="translated">也可以有效地生成一个稀疏图,显示相邻点之间的连接。</target>
        </trans-unit>
        <trans-unit id="e5cc911e1a3213d4a6ea82327423c6b7195a9251" translate="yes" xml:space="preserve">
          <source>It is also possible to map data to a normal distribution using &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt;&lt;code&gt;QuantileTransformer&lt;/code&gt;&lt;/a&gt; by setting &lt;code&gt;output_distribution='normal'&lt;/code&gt;. Using the earlier example with the iris dataset:</source>
          <target state="translated">通过设置 &lt;code&gt;output_distribution='normal'&lt;/code&gt; 还可以使用&lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt; &lt;code&gt;QuantileTransformer&lt;/code&gt; &lt;/a&gt;将数据映射到正态分布。将先前的示例与虹膜数据集结合使用：</target>
        </trans-unit>
        <trans-unit id="f35eb3fdcbe153523a6b78440df1aad8edf3b026" translate="yes" xml:space="preserve">
          <source>It is also possible to use other cross validation strategies by passing a cross validation iterator instead, for instance:</source>
          <target state="translated">也可以通过传递一个交叉验证迭代器来代替使用其他交叉验证策略,例如。</target>
        </trans-unit>
        <trans-unit id="c62a692f1bef88aa9ea1dc55b02f68e6ac2b429f" translate="yes" xml:space="preserve">
          <source>It is classically used to separate mixed signals (a problem known as &lt;em&gt;blind source separation&lt;/em&gt;), as in the example below:</source>
          <target state="translated">传统上，它用于分离混合信号（称为&lt;em&gt;盲源分离&lt;/em&gt;的问题），如下例所示：</target>
        </trans-unit>
        <trans-unit id="579f13cf2a54010546e31ecfaa7ced83f4da4e12" translate="yes" xml:space="preserve">
          <source>It is computationally just as fast as forward selection and has the same order of complexity as an ordinary least squares.</source>
          <target state="translated">它的计算速度与正向选择一样快,复杂程度与普通最小二乘法相同。</target>
        </trans-unit>
        <trans-unit id="2a0b5a3028e23e8f66ba4845cf98605a35e74ca3" translate="yes" xml:space="preserve">
          <source>It is converted to an F score then to a p-value.</source>
          <target state="translated">它被转换为F分,然后再转换为P值。</target>
        </trans-unit>
        <trans-unit id="9da4ca4cacbca0baec3287f1b2124c4dcd00df7a" translate="yes" xml:space="preserve">
          <source>It is easily modified to produce solutions for other estimators, like the Lasso.</source>
          <target state="translated">它很容易修改,以产生其他估计器的解决方案,如Lasso。</target>
        </trans-unit>
        <trans-unit id="a180f7cced602efdc3c3224733570427c990972f" translate="yes" xml:space="preserve">
          <source>It is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that aren&amp;rsquo;t from this window of time.</source>
          <target state="translated">分类器很容易过度拟合出现在20个新闻组数据中的特定内容，例如新闻组标题。许多分类器获得很高的F分数，但是其结果不会推广到不在此时间范围内的其他文档。</target>
        </trans-unit>
        <trans-unit id="2e5aa329cff0eb3a121eaf66246e864cad7413ee" translate="yes" xml:space="preserve">
          <source>It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples. For more tips see Laurens van der Maaten&amp;rsquo;s FAQ [2].</source>
          <target state="translated">如果特征数量非常多，强烈建议使用另一种降维方法（例如，对于密集数据使用PCA或对于稀疏数据使用TruncatedSVD）将尺寸数量减少到合理的数量（例如50个）。这将抑制一些噪声并加快样本之间成对距离的计算。有关更多技巧，请参阅Laurens van der Maaten的常见问题解答[2]。</target>
        </trans-unit>
        <trans-unit id="4c694641d1b1cd9e68259bd2f7aabf747615adda" translate="yes" xml:space="preserve">
          <source>It is important to assign an identifier to unlabeled points along with the labeled data when training the model with the &lt;code&gt;fit&lt;/code&gt; method. The identifier that this implementation uses is the integer value \(-1\).</source>
          <target state="translated">使用 &lt;code&gt;fit&lt;/code&gt; 方法训练模型时，将标识符与标记的数据一起分配给未标记的点很重要。此实现使用的标识符是整数值\（-1 \）。</target>
        </trans-unit>
        <trans-unit id="a067b4f8fd8c4002a8fc9abd7aa015e146d303ad" translate="yes" xml:space="preserve">
          <source>It is important to note that when the number of samples is much larger than the number of features, one would expect that no shrinkage would be necessary. The intuition behind this is that if the population covariance is full rank, when the number of sample grows, the sample covariance will also become positive definite. As a result, no shrinkage would necessary and the method should automatically do this.</source>
          <target state="translated">需要注意的是,当样本数量远大于特征数量时,人们会期望不需要缩减。这背后的直觉是,如果种群协方差是全秩的,当样本数量增长时,样本协方差也会变成正定值。因此,没有必要进行缩减,方法应该自动进行缩减。</target>
        </trans-unit>
        <trans-unit id="d9e45bb570908f10d72f7b51c91c236b78670a3c" translate="yes" xml:space="preserve">
          <source>It is made of 150 observations of irises, each described by 4 features: their sepal and petal length and width, as detailed in &lt;code&gt;iris.DESCR&lt;/code&gt;.</source>
          <target state="translated">它由150个虹膜观察组成，每个虹膜都有4个特征：它们的萼片和花瓣的长度和宽度，详细信息请参见 &lt;code&gt;iris.DESCR&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1856ef8269f110a1ccc7c37c9db810b8176fc5f8" translate="yes" xml:space="preserve">
          <source>It is more efficient than the LassoCV if only a small number of features are selected compared to the total number, for instance if there are very few samples compared to the number of features.</source>
          <target state="translated">如果只选取与总特征数量相比较少的特征,例如与特征数量相比样本非常少的情况下,它比LassoCV更有效率。</target>
        </trans-unit>
        <trans-unit id="1d58fe1839ae301f10f6b9aaac159ed67a9eabfe" translate="yes" xml:space="preserve">
          <source>It is not appropriate to pass these predictions into an evaluation metric. Use &lt;a href=&quot;sklearn.model_selection.cross_validate#sklearn.model_selection.cross_validate&quot;&gt;&lt;code&gt;cross_validate&lt;/code&gt;&lt;/a&gt; to measure generalization error.</source>
          <target state="translated">将这些预测传递到评估指标中是不合适的。使用&lt;a href=&quot;sklearn.model_selection.cross_validate#sklearn.model_selection.cross_validate&quot;&gt; &lt;code&gt;cross_validate&lt;/code&gt; &lt;/a&gt;来衡量泛化错误。</target>
        </trans-unit>
        <trans-unit id="9af2e9f8fa22ff915f29e1a168987ff536812b91" translate="yes" xml:space="preserve">
          <source>It is not recommended to hard-code the backend name in a call to Parallel in a library. Instead it is recommended to set soft hints (prefer) or hard constraints (require) so as to make it possible for library users to change the backend from the outside using the parallel_backend context manager.</source>
          <target state="translated">不推荐在库中调用Parallel时对后端名称进行硬编码。相反,建议设置软提示(prefer)或硬约束(require),这样可以让库用户使用parallel_backend上下文管理器从外部改变后端。</target>
        </trans-unit>
        <trans-unit id="b26cf025ddad5c1f883715bf24d85887eccade22" translate="yes" xml:space="preserve">
          <source>It is not regularized (penalized).</source>
          <target state="translated">它没有被规范化(处罚)。</target>
        </trans-unit>
        <trans-unit id="f18c5e38092754d2adb7bb6eb5c0799854e297b3" translate="yes" xml:space="preserve">
          <source>It is numerically efficient in contexts where p &amp;gt;&amp;gt; n (i.e., when the number of dimensions is significantly greater than the number of points)</source>
          <target state="translated">在p &amp;gt;&amp;gt; n的情况下（即，当维数明显大于点数时），在数值上有效</target>
        </trans-unit>
        <trans-unit id="8c7122bd43c891f087ca247f2fcde5236f637b0c" translate="yes" xml:space="preserve">
          <source>It is often interesting to project data to a lower-dimensional space that preserves most of the variance, by dropping the singular vector of components associated with lower singular values.</source>
          <target state="translated">将数据投射到一个保留大部分方差的低维空间,通过放弃与较低的奇异值相关的分量的奇异向量,通常是有趣的。</target>
        </trans-unit>
        <trans-unit id="5ed0af274291a2311daa7ee05d7bd79f85fc7e49" translate="yes" xml:space="preserve">
          <source>It is possible and recommended to search the hyper-parameter space for the best &lt;a href=&quot;cross_validation#cross-validation&quot;&gt;cross validation&lt;/a&gt; score.</source>
          <target state="translated">可能并建议在超参数空间中搜索最佳&lt;a href=&quot;cross_validation#cross-validation&quot;&gt;交叉验证&lt;/a&gt;得分。</target>
        </trans-unit>
        <trans-unit id="19b21329d1ba1e2fd90b4634d03905cf0f5e7826" translate="yes" xml:space="preserve">
          <source>It is possible to adjust the threshold of the binarizer:</source>
          <target state="translated">可以调整二值机的阈值。</target>
        </trans-unit>
        <trans-unit id="5fdc4b2c9af36a36fca156373f6cb7c574f550b3" translate="yes" xml:space="preserve">
          <source>It is possible to compute per-label precisions, recalls, F1-scores and supports instead of averaging:</source>
          <target state="translated">可以计算每个标签的精确度、召回、F1分数和支持率,而不是平均。</target>
        </trans-unit>
        <trans-unit id="c890fcaf4f6baafc6ccf39a67fce7daf92b8b950" translate="yes" xml:space="preserve">
          <source>It is possible to control the randomness for reproducibility of the results by explicitly seeding the &lt;code&gt;random_state&lt;/code&gt; pseudo random number generator.</source>
          <target state="translated">通过显式 &lt;code&gt;random_state&lt;/code&gt; 伪随机数生成器，可以控制结果可重复性的随机性。</target>
        </trans-unit>
        <trans-unit id="8c436001d07579c89b669f127dbaf0c3bd65de34" translate="yes" xml:space="preserve">
          <source>It is possible to customize the behavior by passing a callable to the vectorizer constructor:</source>
          <target state="translated">可以通过传递一个可调用的向量器构造函数来定制行为。</target>
        </trans-unit>
        <trans-unit id="0839b4d3a34db46e778e581b781425b62631583b" translate="yes" xml:space="preserve">
          <source>It is possible to disable either centering or scaling by either passing &lt;code&gt;with_mean=False&lt;/code&gt; or &lt;code&gt;with_std=False&lt;/code&gt; to the constructor of &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">通过将 &lt;code&gt;with_mean=False&lt;/code&gt; 或 &lt;code&gt;with_std=False&lt;/code&gt; 传递给&lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; &lt;/a&gt;的构造函数，可以禁用居中或缩放。</target>
        </trans-unit>
        <trans-unit id="85a1eed4b8a0f5199be27070848fbc013f8f8638" translate="yes" xml:space="preserve">
          <source>It is possible to get back the category names as follows:</source>
          <target state="translated">可以通过以下方式取回类别名称。</target>
        </trans-unit>
        <trans-unit id="6b11842b410c7ed9014abd60118219965dd51782" translate="yes" xml:space="preserve">
          <source>It is possible to introspect the scaler attributes to find about the exact nature of the transformation learned on the training data:</source>
          <target state="translated">可以对标尺属性进行反省,找到在训练数据上学习到的转化的确切性质。</target>
        </trans-unit>
        <trans-unit id="d994dbf018869cdf387e647211852d55a08f6930" translate="yes" xml:space="preserve">
          <source>It is possible to load only a sub-selection of the categories by passing the list of the categories to load to the &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt;&lt;/a&gt; function:</source>
          <target state="translated">通过将要加载的类别列表传递给&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt; &lt;/a&gt;函数，可以仅加载类别的子选择：</target>
        </trans-unit>
        <trans-unit id="03749a52f5e2e7e976928d01767369407c7307c4" translate="yes" xml:space="preserve">
          <source>It is possible to mix sparse and dense arrays in the same run:</source>
          <target state="translated">可以在同一运行中混合使用稀疏数组和密集数组。</target>
        </trans-unit>
        <trans-unit id="be16ce674bae3bf54f6cdc3d4d41c5990ca746b6" translate="yes" xml:space="preserve">
          <source>It is possible to overcome those limitations by combining the &amp;ldquo;hashing trick&amp;rdquo; (&lt;a href=&quot;#feature-hashing&quot;&gt;Feature hashing&lt;/a&gt;) implemented by the &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;sklearn.feature_extraction.FeatureHasher&lt;/code&gt;&lt;/a&gt; class and the text preprocessing and tokenization features of the &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">它可以通过组合的&amp;ldquo;散列绝招&amp;rdquo;（克服这些局限性&lt;a href=&quot;#feature-hashing&quot;&gt;特点哈希&lt;/a&gt;通过实施）&lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;sklearn.feature_extraction.FeatureHasher&lt;/code&gt; &lt;/a&gt;类和文本预处理和符号化的特征&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="737f1fd475d1dc22b14b4896563d476e36ccb4e8" translate="yes" xml:space="preserve">
          <source>It is possible to save a model in scikit-learn by using Python&amp;rsquo;s built-in persistence model, &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;:</source>
          <target state="translated">通过使用Python的内置持久性模型&lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;，可以在scikit-learn中保存模型：</target>
        </trans-unit>
        <trans-unit id="1d319918af937e7b588f1bdf4ba0c9bc1d2e6a8f" translate="yes" xml:space="preserve">
          <source>It is possible to save a model in scikit-learn by using Python&amp;rsquo;s built-in persistence model, namely &lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;:</source>
          <target state="translated">使用Python的内置持久性模型&lt;a href=&quot;https://docs.python.org/2/library/pickle.html&quot;&gt;pickle&lt;/a&gt;可以将模型保存在scikit-learn中：</target>
        </trans-unit>
        <trans-unit id="33bdca276514e666ea92e40ef8d9c04e5206a96b" translate="yes" xml:space="preserve">
          <source>It is possible to specify this explicitly using the parameter &lt;code&gt;categories&lt;/code&gt;. There are two genders, four possible continents and four web browsers in our dataset:</source>
          <target state="translated">可以使用参数 &lt;code&gt;categories&lt;/code&gt; 来明确指定。我们的数据集中有两种性别，四个可能的大洲和四个网络浏览器：</target>
        </trans-unit>
        <trans-unit id="cbfa3a3539d1ad40958cd50540686b2891b5e349" translate="yes" xml:space="preserve">
          <source>It is sometimes not enough to center and scale the features independently, since a downstream model can further make some assumption on the linear independence of the features.</source>
          <target state="translated">有时独立地对特征进行居中和缩放是不够的,因为下游模型可以进一步对特征的线性独立性做一些假设。</target>
        </trans-unit>
        <trans-unit id="311a1593daf1f1187805481bab88c0d399c3cecf" translate="yes" xml:space="preserve">
          <source>It is sometimes worthwhile storing the state of a specific transformer since it could be used again. Using a pipeline in &lt;code&gt;GridSearchCV&lt;/code&gt; triggers such situations. Therefore, we use the argument &lt;code&gt;memory&lt;/code&gt; to enable caching.</source>
          <target state="translated">有时值得存储特定变压器的状态，因为它可以再次使用。在 &lt;code&gt;GridSearchCV&lt;/code&gt; 中使用管道会触发这种情况。因此，我们使用参数 &lt;code&gt;memory&lt;/code&gt; 来启用缓存。</target>
        </trans-unit>
        <trans-unit id="ba51434e495bfdf85ff2401c563345468fae8389" translate="yes" xml:space="preserve">
          <source>It is the fastest algorithm for learning mixture models</source>
          <target state="translated">它是学习混合物模型的最快算法。</target>
        </trans-unit>
        <trans-unit id="5c9cedaa4c291702a05bee05d8b7517536cf8c97" translate="yes" xml:space="preserve">
          <source>It is the opposite as as bigger is better, i.e. large values correspond to inliers.</source>
          <target state="translated">恰恰相反,因为越大越好,即大值对应离群值。</target>
        </trans-unit>
        <trans-unit id="b0c456c256349cc53ca134d105c8be601465dd39" translate="yes" xml:space="preserve">
          <source>It is worth noting that RandomForests and ExtraTrees can be fitted in parallel on many cores as each tree is built independently of the others. AdaBoost&amp;rsquo;s samples are built sequentially and so do not use multiple cores.</source>
          <target state="translated">值得注意的是，RandomForests和ExtraTrees可以并行安装在许多内核上，因为每棵树都是独立于其他树构建的。AdaBoost的样本是按顺序构建的，因此请勿使用多个内核。</target>
        </trans-unit>
        <trans-unit id="a4029704b1c865bc18db0f7f71b472d5421882ac" translate="yes" xml:space="preserve">
          <source>It produces a full piecewise linear solution path, which is useful in cross-validation or similar attempts to tune the model.</source>
          <target state="translated">它能产生一个完整的片状线性解路径,这在交叉验证或类似的调整模型的尝试中很有用。</target>
        </trans-unit>
        <trans-unit id="a4dee1947755b5fe4ca1a29e0b9b0f0b85817660" translate="yes" xml:space="preserve">
          <source>It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score.</source>
          <target state="translated">它返回一个dict,除了测试分数外,还包含拟合时间、得分时间(以及可选的训练分数以及拟合估计器)。</target>
        </trans-unit>
        <trans-unit id="be2089f68dcca4fd6c2250f878425dd499fc444a" translate="yes" xml:space="preserve">
          <source>It returns a dictionary-like object, with the following attributes:</source>
          <target state="translated">它返回一个类似字典的对象,具有以下属性。</target>
        </trans-unit>
        <trans-unit id="6f65f619abd94296c7075a4b5d91a76ac1e641bc" translate="yes" xml:space="preserve">
          <source>It returns a floating point number that quantifies the &lt;code&gt;estimator&lt;/code&gt; prediction quality on &lt;code&gt;X&lt;/code&gt;, with reference to &lt;code&gt;y&lt;/code&gt;. Again, by convention higher numbers are better, so if your scorer returns loss, that value should be negated.</source>
          <target state="translated">它返回一个浮点数，该浮点数参考 &lt;code&gt;y&lt;/code&gt; 量化 &lt;code&gt;X&lt;/code&gt; 上的 &lt;code&gt;estimator&lt;/code&gt; 预测质量。同样，按照惯例，数字越大越好，因此，如果您的计分员返回亏损，则该值应为负。</target>
        </trans-unit>
        <trans-unit id="58c0c1b9288f5ba70bfdf3e509c8376ea38265d4" translate="yes" xml:space="preserve">
          <source>It should be noted that Johnson-Lindenstrauss lemma can yield very conservative estimated of the required number of components as it makes no assumption on the structure of the dataset.</source>
          <target state="translated">需要注意的是,Johnson-Lindenstrauss lemma可以得出非常保守的所需分量的估计值,因为它没有对数据集的结构做出假设。</target>
        </trans-unit>
        <trans-unit id="5bcecde02163a3a6b9fb69b7700a66c21be36347" translate="yes" xml:space="preserve">
          <source>It shows how to use &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; to approximate the feature map of an RBF kernel for classification with an SVM on the digits dataset. Results using a linear SVM in the original space, a linear SVM using the approximate mappings and using a kernelized SVM are compared. Timings and accuracy for varying amounts of Monte Carlo samplings (in the case of &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;, which uses random Fourier features) and different sized subsets of the training set (for &lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt;) for the approximate mapping are shown.</source>
          <target state="translated">它显示了如何使用&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;来近似RBF内核的特征图，以便在数字数据集上使用SVM进行分类。比较了使用原始空间中的线性SVM，使用近似映射和使用内核化SVM的结果。显示了不同数量的蒙特卡洛采样的时间和精度（在&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;的情况下，它使用随机傅立叶特征）和训练集的不同大小的子集（对于&lt;a href=&quot;../modules/generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;）进行了近似映射。</target>
        </trans-unit>
        <trans-unit id="45249c231a1e8d583e28deb277d22f5fe88e16b7" translate="yes" xml:space="preserve">
          <source>It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm=&amp;rsquo;l1&amp;rsquo; or projected on the euclidean unit sphere if norm=&amp;rsquo;l2&amp;rsquo;.</source>
          <target state="translated">它将文本文档的集合转换为包含令牌出现次数（或二进制出现信息）的稀疏矩阵，如果norm ='l1'，则可能归一化为令牌频率，如果norm ='l2'，则投影到欧几里得单位球上。</target>
        </trans-unit>
        <trans-unit id="9b65a724f589693294d8b39fded9beef68bf84ef" translate="yes" xml:space="preserve">
          <source>It updates its model only on mistakes.</source>
          <target state="translated">它只在错误的时候更新其模型。</target>
        </trans-unit>
        <trans-unit id="4ef1ebaa3d2757730ff62ab1b50211fc95aec89a" translate="yes" xml:space="preserve">
          <source>It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.</source>
          <target state="translated">它采用LAPACK实现的全SVD或Halko等人2009年方法的随机截断SVD,这取决于输入数据的形状和提取成分的数量。</target>
        </trans-unit>
        <trans-unit id="eb35f7366145b28ea8e69aba84c19929cb4fd162" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s also possible for almost all of these function to constrain the output to be a tuple containing only the data and the target, by setting the &lt;code&gt;return_X_y&lt;/code&gt; parameter to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">通过将 &lt;code&gt;return_X_y&lt;/code&gt; 参数设置为 &lt;code&gt;True&lt;/code&gt; ，几乎所有这些函数还可以将输出约束为仅包含数据和目标的元组。</target>
        </trans-unit>
        <trans-unit id="522ad20a1aa12ab3e8e796322f388a9318be6368" translate="yes" xml:space="preserve">
          <source>It&amp;rsquo;s clear how the kernel shape affects the smoothness of the resulting distribution. The scikit-learn kernel density estimator can be used as follows:</source>
          <target state="translated">很明显，内核形状如何影响结果分布的平滑度。scikit-learn内核密度估计器可以按以下方式使用：</target>
        </trans-unit>
        <trans-unit id="262f72bd253b7e8f886f3645ecd5afaaf624d7fc" translate="yes" xml:space="preserve">
          <source>Iterate 2 and 3 until convergence.</source>
          <target state="translated">迭代2和3,直到收敛。</target>
        </trans-unit>
        <trans-unit id="f2f172891cc8c1241e8513ed23c4f46ceb939f0f" translate="yes" xml:space="preserve">
          <source>Iterative procedure to maximize the evidence</source>
          <target state="translated">证据最大化的迭代程序</target>
        </trans-unit>
        <trans-unit id="1e87dcaf344d15783f1af4ad18b162b497d772d4" translate="yes" xml:space="preserve">
          <source>Its dual is</source>
          <target state="translated">它的双重性是</target>
        </trans-unit>
        <trans-unit id="4ba292a3729a3ffa6797e98ae7a24bba4f0e087f" translate="yes" xml:space="preserve">
          <source>J. Davis, M. Goadrich, &lt;a href=&quot;http://www.machinelearning.org/proceedings/icml2006/030_The_Relationship_Bet.pdf&quot;&gt;The Relationship Between Precision-Recall and ROC Curves&lt;/a&gt;, ICML 2006.</source>
          <target state="translated">J.Davis，M.Goadrich，&lt;a href=&quot;http://www.machinelearning.org/proceedings/icml2006/030_The_Relationship_Bet.pdf&quot;&gt;《精确召回与ROC曲线之间的关系》&lt;/a&gt;，ICML 2006。</target>
        </trans-unit>
        <trans-unit id="9f9ca6a90c561398be254053aedc4a945c9160d7" translate="yes" xml:space="preserve">
          <source>J. Friedman, &amp;ldquo;Multivariate adaptive regression splines&amp;rdquo;, The Annals of Statistics 19 (1), pages 1-67, 1991.</source>
          <target state="translated">J. Friedman，&amp;ldquo;多元自适应回归样条&amp;rdquo;，《统计年鉴》 19（1），第1-67页，1991年。</target>
        </trans-unit>
        <trans-unit id="f3a4e2abf1b3937c134504328e857f33b32a50ea" translate="yes" xml:space="preserve">
          <source>J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</source>
          <target state="translated">J.Friedman,Greedy Function Approximation:A Gradient Boosting Machine,The Annals of Statistics,Vol.29,No.5,2001.</target>
        </trans-unit>
        <trans-unit id="37cd6f13ac969b2cba8a5a7a242580515d06793b" translate="yes" xml:space="preserve">
          <source>J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning for sparse coding (&lt;a href=&quot;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt;)</source>
          <target state="translated">J. Mairal，F.Bach，J.Ponce，G.Sapiro，2009年：用于稀疏编码的在线词典学习（&lt;a href=&quot;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&quot;&gt;http://www.di.ens.fr/sierra/pdfs/icml09.pdf&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="d67bb0042b556d1819c5bcf6e551c8393e0d921f" translate="yes" xml:space="preserve">
          <source>J. Nothman, H. Qin and R. Yurchak (2018). &lt;a href=&quot;http://aclweb.org/anthology/W18-2502&quot;&gt;&amp;ldquo;Stop Word Lists in Free Open-source Software Packages&amp;rdquo;&lt;/a&gt;. In &lt;em&gt;Proc. Workshop for NLP Open Source Software&lt;/em&gt;.</source>
          <target state="translated">诺思曼（J. Nothman），秦宏（H. Qin）和尤尔查克（R.Yurchak）（2018）。&lt;a href=&quot;http://aclweb.org/anthology/W18-2502&quot;&gt;&amp;ldquo;免费开源软件包中的停止单词列表&amp;rdquo;&lt;/a&gt;。在过程中&lt;em&gt;。NLP开源软件研讨会&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="692e866d29ab5ac27b12eb939942a283756e56c6" translate="yes" xml:space="preserve">
          <source>J. Zhu, H. Zou, S. Rosset, T. Hastie. &amp;ldquo;Multi-class AdaBoost&amp;rdquo;, 2009.</source>
          <target state="translated">J. Zhu，H。Zou，S。Rosset，T。Hastie。&amp;ldquo;多类AdaBoost&amp;rdquo;，2009年。</target>
        </trans-unit>
        <trans-unit id="5d92020b429e9c336d3ae7d33c4ba163d63036bb" translate="yes" xml:space="preserve">
          <source>J.R. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993.</source>
          <target state="translated">J.R.Quinlan.C4.5:机器学习的程序。Morgan Kaufmann,1993.</target>
        </trans-unit>
        <trans-unit id="b259e0488f66e10ad76098d33440aa4c5f23c876" translate="yes" xml:space="preserve">
          <source>JA Wegelin &lt;a href=&quot;https://www.stat.washington.edu/research/reports/2000/tr371.pdf&quot;&gt;A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case&lt;/a&gt;</source>
          <target state="translated">JA Wegelin &lt;a href=&quot;https://www.stat.washington.edu/research/reports/2000/tr371.pdf&quot;&gt;对偏最小二乘（PLS）方法的调查，着重于两块案例&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="89591613ce2ead27076b0dd7b68b18da1f4e31d9" translate="yes" xml:space="preserve">
          <source>Jaccard similarity coefficient score</source>
          <target state="translated">贾卡德相似性系数得分</target>
        </trans-unit>
        <trans-unit id="3dd35b446a7d3de6ee5688cfabde9bb7cc55f61a" translate="yes" xml:space="preserve">
          <source>JaccardDistance</source>
          <target state="translated">JaccardDistance</target>
        </trans-unit>
        <trans-unit id="493395686693db33a59d5eea00e82ad6c02c5742" translate="yes" xml:space="preserve">
          <source>Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case. Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000.</source>
          <target state="translated">Jacob A.Wegelin。部分最小二乘法的调查,重点是两块情况。技术报告371,华盛顿大学统计系,西雅图,2000年。</target>
        </trans-unit>
        <trans-unit id="80af07b09c2acb231d89e62f1382b88433574f92" translate="yes" xml:space="preserve">
          <source>Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank,</source>
          <target state="translated">Jesse Read,Bernhard Pfahringer,Geoff Holmes,Yew Frank,</target>
        </trans-unit>
        <trans-unit id="6f9c9a3eee3a8f7459d68946df6ef289f22fee94" translate="yes" xml:space="preserve">
          <source>Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank, &amp;ldquo;Classifier Chains for Multi-label Classification&amp;rdquo;, 2009.</source>
          <target state="translated">Jesse Read，Bernhard Pfahringer，Geoff Holmes，Eibe Frank，&amp;ldquo;多标签分类的分类器链&amp;rdquo;，2009年。</target>
        </trans-unit>
        <trans-unit id="2da78ef6529cd970b51628e985f5f2ea249ac134" translate="yes" xml:space="preserve">
          <source>Johanna Hardin, David M Rocke. The distribution of robust distances. Journal of Computational and Graphical Statistics. December 1, 2005, 14(4): 928-946.</source>
          <target state="translated">Johanna Hardin,David M Rocke.The distribution of robust distances.Journal of Computational and Graphical Statistics.December 1,2005,14(4):928-946.</target>
        </trans-unit>
        <trans-unit id="f669c43cc07002b0d2c74696c2e577f06e2f830d" translate="yes" xml:space="preserve">
          <source>John. D. Kelleher, Brian Mac Namee, Aoife D&amp;rsquo;Arcy, (2015). &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies&lt;/a&gt;.</source>
          <target state="translated">约翰。D.Kelleher，Brian Mac Namee，Aoife D'Arcy，（2015年）。&lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;预测数据分析的机器学习基础：算法，有效示例和案例研究&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="7aafef76ed32e6bfff8b0b682dc86da3ac5a13fa" translate="yes" xml:space="preserve">
          <source>John. D. Kelleher, Brian Mac Namee, Aoife D&amp;rsquo;Arcy, &lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies&lt;/a&gt;, 2015.</source>
          <target state="translated">约翰。D.Kelleher，Brian Mac Namee，Aoife D'Arcy，《&lt;a href=&quot;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&quot;&gt;用于预测数据分析的机器学习基础：算法，实例和案例研究》&lt;/a&gt;，2015年。</target>
        </trans-unit>
        <trans-unit id="19e6bf8efc7133dd97d0abbd89569a0495139bdc" translate="yes" xml:space="preserve">
          <source>Joint feature selection with multi-task Lasso</source>
          <target state="translated">用多任务Lasso进行联合特征选择</target>
        </trans-unit>
        <trans-unit id="6e210d8e33bded6f565ddf30568ce6ee46546dcb" translate="yes" xml:space="preserve">
          <source>Joint parameter selection</source>
          <target state="translated">联合参数选择</target>
        </trans-unit>
        <trans-unit id="5dcd2dd79faa568a08732dcdc7a1c5d001632db6" translate="yes" xml:space="preserve">
          <source>Journal of Machine Learning Research 15(Oct):3221-3245, 2014. &lt;a href=&quot;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</source>
          <target state="translated">杂志的机器学习研究15（10月）的：3221-3245，2014年&lt;a href=&quot;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&quot;&gt;http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="6689749f561220cbe925de6f0809b1dc75c6258d" translate="yes" xml:space="preserve">
          <source>July, 1988</source>
          <target state="translated">1988年7月</target>
        </trans-unit>
        <trans-unit id="854e66ede1ccc0e35f92ec3068666dcad934aaf9" translate="yes" xml:space="preserve">
          <source>July; 1998</source>
          <target state="translated">1998年7月</target>
        </trans-unit>
        <trans-unit id="a2a7da9b458fe4f43b31552673f0b66352445d61" translate="yes" xml:space="preserve">
          <source>Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN Error Measures in MultiClass Prediction</source>
          <target state="translated">Jurman,Riccadonna,Furlanello,(2012)。多类预测中MCC和CEN误差测量的比较。</target>
        </trans-unit>
        <trans-unit id="a8dcc7a6052d083397dd89c88efdae4d16610c1e" translate="yes" xml:space="preserve">
          <source>Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) and similar &lt;a href=&quot;http://scikit-learn.org/stable/data_transforms.html#data-transforms&quot;&gt;data transformations&lt;/a&gt; similarly should be learnt from a training set and applied to held-out data for prediction:</source>
          <target state="translated">正如测试预测器对训练，预处理（例如，标准化，特征选择等）所保留的数据非常重要，类似的&lt;a href=&quot;http://scikit-learn.org/stable/data_transforms.html#data-transforms&quot;&gt;数据转换&lt;/a&gt;也应类似地从训练集中学习并应用于保留的数据以进行预测：</target>
        </trans-unit>
        <trans-unit id="19ba747b37c5ad6ac1cc4689022bdfff83622716" translate="yes" xml:space="preserve">
          <source>Just like self.assertTrue(a in b), but with a nicer default message.</source>
          <target state="translated">就像 self.assertTrue(a in b)一样,但有一个更好的默认信息。</target>
        </trans-unit>
        <trans-unit id="a7abce2837c2684f6308e0a3abb93654038ccc5f" translate="yes" xml:space="preserve">
          <source>Just like self.assertTrue(a not in b), but with a nicer default message.</source>
          <target state="translated">就像 self.assertTrue(a not in b)一样,但默认信息更好。</target>
        </trans-unit>
        <trans-unit id="57113affe2edb0e21a97719d086746970040c08f" translate="yes" xml:space="preserve">
          <source>K&amp;auml;rkk&amp;auml;inen and S. &amp;Auml;yr&amp;auml;m&amp;ouml;: &lt;a href=&quot;http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf&quot;&gt;On Computation of Spatial Median for Robust Data Mining.&lt;/a&gt;</source>
          <target state="translated">K&amp;auml;rkk&amp;auml;inen和S.&amp;Auml;yr&amp;auml;m&amp;ouml;：&lt;a href=&quot;http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf&quot;&gt;关于稳健数据挖掘的空间中位数计算。&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7c859ded2bd8aa408f6c0369beaf2c2cf3f0ddde" translate="yes" xml:space="preserve">
          <source>K(X, Y) = &amp;lt;X, Y&amp;gt; / (||X||*||Y||)</source>
          <target state="translated">K（X，Y）= &amp;lt;X，Y&amp;gt; /（|| X || * || Y ||）</target>
        </trans-unit>
        <trans-unit id="86beb78a3bdf4132202cbc165378339bb7f278e3" translate="yes" xml:space="preserve">
          <source>K-Folds cross-validator</source>
          <target state="translated">K-Folds交叉验证器</target>
        </trans-unit>
        <trans-unit id="66e29f0aeaaf6f3b77934175874c79014b658ea2" translate="yes" xml:space="preserve">
          <source>K-Means</source>
          <target state="translated">K-Means</target>
        </trans-unit>
        <trans-unit id="bc6e2dbca5eeaca5cfd908f6085c13e70dbbe207" translate="yes" xml:space="preserve">
          <source>K-Means clustering</source>
          <target state="translated">K-Means聚类</target>
        </trans-unit>
        <trans-unit id="c532c5671424d23a3a3bc85d7cee5f6f8a964404" translate="yes" xml:space="preserve">
          <source>K-fold iterator variant with non-overlapping groups.</source>
          <target state="translated">具有非重叠组的K-fold迭代器变体。</target>
        </trans-unit>
        <trans-unit id="8434c9f312099287fd33427192dcba6bdae1583b" translate="yes" xml:space="preserve">
          <source>K-means Clustering</source>
          <target state="translated">K-means Clustering</target>
        </trans-unit>
        <trans-unit id="16176fa529a1e6d30521129cdc8f04353aaff22e" translate="yes" xml:space="preserve">
          <source>K-means algorithm to use. The classical EM-style algorithm is &amp;ldquo;full&amp;rdquo;. The &amp;ldquo;elkan&amp;rdquo; variation is more efficient by using the triangle inequality, but currently doesn&amp;rsquo;t support sparse data. &amp;ldquo;auto&amp;rdquo; chooses &amp;ldquo;elkan&amp;rdquo; for dense data and &amp;ldquo;full&amp;rdquo; for sparse data.</source>
          <target state="translated">使用K均值算法。经典的EM风格算法是&amp;ldquo;完整&amp;rdquo;的。通过使用三角形不等式，&amp;ldquo; elkan&amp;rdquo;变体更为有效，但目前不支持稀疏数据。&amp;ldquo;自动&amp;rdquo;为密集数据选择&amp;ldquo; elkan&amp;rdquo;，为稀疏数据选择&amp;ldquo;完整&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="848dff73d6f69d92cd5b01b40f76a731abde9743" translate="yes" xml:space="preserve">
          <source>K-means can be used for vector quantization. This is achieved using the transform method of a trained model of &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt;&lt;code&gt;KMeans&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">K均值可用于矢量量化。这是通过使用&lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt; &lt;code&gt;KMeans&lt;/code&gt; &lt;/a&gt;训练模型的变换方法来实现的。</target>
        </trans-unit>
        <trans-unit id="ba78203e9e9f38ce3f7e015938283eb704622fc1" translate="yes" xml:space="preserve">
          <source>K-means clustering</source>
          <target state="translated">K-means聚类</target>
        </trans-unit>
        <trans-unit id="4c31918fe250fba32eafec9c8bd2408d0665baa0" translate="yes" xml:space="preserve">
          <source>K-means clustering algorithm.</source>
          <target state="translated">K-means聚类算法。</target>
        </trans-unit>
        <trans-unit id="3f9399be9d9993e05f4712a210efb7bcf391430f" translate="yes" xml:space="preserve">
          <source>K-means is equivalent to the expectation-maximization algorithm with a small, all-equal, diagonal covariance matrix.</source>
          <target state="translated">K-means相当于期望最大化算法,其协方差矩阵很小,全等,对角线。</target>
        </trans-unit>
        <trans-unit id="f931e58c5b02fb6c60e80955646f359bee6ac7ee" translate="yes" xml:space="preserve">
          <source>K-means is often referred to as Lloyd&amp;rsquo;s algorithm. In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method being to choose \(k\) samples from the dataset \(X\). After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly.</source>
          <target state="translated">K均值通常被称为劳埃德算法。从根本上讲，该算法分为三个步骤。第一步选择初始质心，最基本的方法是从数据集\（X \）中选择\（k \）个样本。初始化后，K-means包括其他两个步骤之间的循环。第一步将每个样本分配给其最近的质心。第二步通过获取分配给每个先前质心的所有样本的平均值来创建新质心。计算新旧质心之间的差异，算法重复最后两个步骤，直到该值小于阈值。换句话说，它会重复直到质心不明显移动为止。</target>
        </trans-unit>
        <trans-unit id="c91b0be65ee9c7db25b71aa279369cca08edc7ca" translate="yes" xml:space="preserve">
          <source>K-means quantization</source>
          <target state="translated">K-means量化</target>
        </trans-unit>
        <trans-unit id="5cf295fcd230ab825b1fa5bcf82b9dac494d126c" translate="yes" xml:space="preserve">
          <source>KDTree for fast generalized N-point problems</source>
          <target state="translated">快速广义N点问题的KDT树</target>
        </trans-unit>
        <trans-unit id="34d74f913e8bd68fa4a9d1c4d3966f34ca72fc15" translate="yes" xml:space="preserve">
          <source>KDTree(X, leaf_size=40, metric=&amp;rsquo;minkowski&amp;rsquo;, **kwargs)</source>
          <target state="translated">KDTree（X，leaf_size = 40，metric ='minkowski'，** kwargs）</target>
        </trans-unit>
        <trans-unit id="8a130d990c735953536ce43a1c5cf50c4989bca1" translate="yes" xml:space="preserve">
          <source>Kappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators.</source>
          <target state="translated">二元或多类问题可以计算Kappa分数,但多标签问题不能计算Kappa分数(除了手动计算每个标签的分数),也不能计算两个以上注释者的分数。</target>
        </trans-unit>
        <trans-unit id="992bd2f88020b43ae9d881f8a8ecb43504cd4a74" translate="yes" xml:space="preserve">
          <source>Keep the 3 RGB channels instead of averaging them to a single gray level channel. If color is True the shape of the data has one more dimension than the shape with color = False.</source>
          <target state="translated">保留3个RGB通道,而不是将它们平均到一个灰度通道。如果color为True,则数据的形状比color=False的形状多一个维度。</target>
        </trans-unit>
        <trans-unit id="e85b73c38883acb129e419e1a2f1719d75a444c5" translate="yes" xml:space="preserve">
          <source>Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin. Linear dimensionalityreduction using relevance weighted LDA. School of Electrical and Electronic Engineering Nanyang Technological University. 2005.</source>
          <target state="translated">Ken Tang and Ponnuthurai N.Suganthan and Xi Yao and A.Kai Qin.Linear dimensionalityreduction using relevance weighted LDA.School of Electrical and Electronic Engineering Nanyang Technological University.2005.</target>
        </trans-unit>
        <trans-unit id="4ac337776123607052d628758806e2172a140241" translate="yes" xml:space="preserve">
          <source>Kernel Density Estimate of Species Distributions</source>
          <target state="translated">物种分布的核密度估计</target>
        </trans-unit>
        <trans-unit id="1794dd0445cf0665650fb5446983f4ef8a3519d3" translate="yes" xml:space="preserve">
          <source>Kernel Density Estimation</source>
          <target state="translated">核心密度估计</target>
        </trans-unit>
        <trans-unit id="3bd4b1d4f074cf6b0f30ea849b2a75ad1d3777d9" translate="yes" xml:space="preserve">
          <source>Kernel PCA</source>
          <target state="translated">PCA内核</target>
        </trans-unit>
        <trans-unit id="e5cb129fc99d7ba99fe28de6d8de36380920334b" translate="yes" xml:space="preserve">
          <source>Kernel PCA was introduced in:</source>
          <target state="translated">核PCA被引入在。</target>
        </trans-unit>
        <trans-unit id="ba5a4a64bda1b4288aa7730d4a3cc2a5a99cf5dc" translate="yes" xml:space="preserve">
          <source>Kernel Principal component analysis (KPCA)</source>
          <target state="translated">内核主成分分析(KPCA)</target>
        </trans-unit>
        <trans-unit id="f9f3967ca79560e0b7bba219989bbd17450e2f6e" translate="yes" xml:space="preserve">
          <source>Kernel bandwidth.</source>
          <target state="translated">核心带宽。</target>
        </trans-unit>
        <trans-unit id="8f8874978483d89d1eb3e15131193d11bfd798e3" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for &amp;lsquo;rbf&amp;rsquo;, &amp;lsquo;poly&amp;rsquo; and &amp;lsquo;sigmoid&amp;rsquo;.</source>
          <target state="translated">&amp;ldquo; rbf&amp;rdquo;，&amp;ldquo; poly&amp;rdquo;和&amp;ldquo; Sigmoid&amp;rdquo;的内核系数。</target>
        </trans-unit>
        <trans-unit id="97392135d656893f41c86b28ea3abd0d9e018bae" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf kernel.</source>
          <target state="translated">rbf核的核系数。</target>
        </trans-unit>
        <trans-unit id="0ae546d11d3317bcab299286e34e2f35ebfa8832" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other kernels.</source>
          <target state="translated">rbf、poly和sigmoid核的核系数。其他核忽略。</target>
        </trans-unit>
        <trans-unit id="4a5a36cb73b6fa8cb90e406a9b203038f766b3f9" translate="yes" xml:space="preserve">
          <source>Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels. Ignored for &lt;code&gt;affinity='nearest_neighbors'&lt;/code&gt;.</source>
          <target state="translated">rbf，poly，Sigmoid，laplacian和chi2内核的内核系数。被忽略为 &lt;code&gt;affinity='nearest_neighbors'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7ed139f80ae0f25db98c92c5cce6311e8435271b" translate="yes" xml:space="preserve">
          <source>Kernel density estimation in scikit-learn is implemented in the &lt;a href=&quot;generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt;&lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt;&lt;/a&gt; estimator, which uses the Ball Tree or KD Tree for efficient queries (see &lt;a href=&quot;neighbors#neighbors&quot;&gt;Nearest Neighbors&lt;/a&gt; for a discussion of these). Though the above example uses a 1D data set for simplicity, kernel density estimation can be performed in any number of dimensions, though in practice the curse of dimensionality causes its performance to degrade in high dimensions.</source>
          <target state="translated">scikit-learn中的内核密度估计是在&lt;a href=&quot;generated/sklearn.neighbors.kerneldensity#sklearn.neighbors.KernelDensity&quot;&gt; &lt;code&gt;sklearn.neighbors.KernelDensity&lt;/code&gt; &lt;/a&gt;估计器中实现的，该估计器使用Ball Tree或KD Tree进行有效查询（有关这些内容的讨论，请参阅&lt;a href=&quot;neighbors#neighbors&quot;&gt;最近邻居&lt;/a&gt;）。尽管为简化起见，上面的示例使用一维数据集，但是可以在任意多个维度上执行内核密度估计，尽管在实践中，维度的诅咒会导致其性能在高维度上下降。</target>
        </trans-unit>
        <trans-unit id="55e8fbe20e17e26ae0f3d4e88a1aeba0651c9393" translate="yes" xml:space="preserve">
          <source>Kernel hyperparameters for which the log-marginal likelihood is evaluated. If None, the precomputed log_marginal_likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt; is returned.</source>
          <target state="translated">对数边际似然被评估的内核超参数。如果为None，则返回 &lt;code&gt;self.kernel_.theta&lt;/code&gt; 的预先计算的log_marginal_likelihood。</target>
        </trans-unit>
        <trans-unit id="aef459d7999942524bf342d4727b96b71e8fe80a" translate="yes" xml:space="preserve">
          <source>Kernel hyperparameters for which the log-marginal likelihood is evaluated. In the case of multi-class classification, theta may be the hyperparameters of the compound kernel or of an individual kernel. In the latter case, all individual kernel get assigned the same theta values. If None, the precomputed log_marginal_likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt; is returned.</source>
          <target state="translated">对数边际似然被评估的内核超参数。在多类分类的情况下，theta可能是复合内核或单个内核的超参数。在后一种情况下，将为所有单个内核分配相同的theta值。如果为None，则返回 &lt;code&gt;self.kernel_.theta&lt;/code&gt; 的预先计算的log_marginal_likelihood。</target>
        </trans-unit>
        <trans-unit id="e5fe7d4b4a2b4b1f4287c0408af092681ae17306" translate="yes" xml:space="preserve">
          <source>Kernel k(X, Y)</source>
          <target state="translated">核k(X,Y)</target>
        </trans-unit>
        <trans-unit id="3ec24bca52509370dd13e99805241c7649952db1" translate="yes" xml:space="preserve">
          <source>Kernel map to be approximated. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number.</source>
          <target state="translated">要近似的内核映射。一个可调用的对象应该接受两个参数和作为kernel_params传递给这个对象的关键字参数,并且应该返回一个浮点数。</target>
        </trans-unit>
        <trans-unit id="819d0e343c77a54a44f85a514be1d98b92643c33" translate="yes" xml:space="preserve">
          <source>Kernel mapping used internally. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number. Set to &amp;ldquo;precomputed&amp;rdquo; in order to pass a precomputed kernel matrix to the estimator methods instead of samples.</source>
          <target state="translated">内部使用的内核映射。一个callable应该接受两个参数和作为参数传递给该对象的关键字参数kernel_params，并且应该返回一个浮点数。设置为&amp;ldquo;预先计算&amp;rdquo;以便将预先计算的内核矩阵传递给估计器方法，而不是样本。</target>
        </trans-unit>
        <trans-unit id="5470105c2039f2210b1a2c9d8e55edfd818f2e42" translate="yes" xml:space="preserve">
          <source>Kernel matrix.</source>
          <target state="translated">核心矩阵。</target>
        </trans-unit>
        <trans-unit id="839a7f66845e964bf2afbaec5611c3402217b903" translate="yes" xml:space="preserve">
          <source>Kernel methods like support vector machines or kernelized PCA rely on a property of reproducing kernel Hilbert spaces. For any positive definite kernel function \(k\) (a so called Mercer kernel), it is guaranteed that there exists a mapping \(\phi\) into a Hilbert space \(\mathcal{H}\), such that</source>
          <target state="translated">像支持向量机或核化PCA这样的核方法依赖于重现核希尔伯特空间的特性。对于任何一个正定核函数(k)(一个所谓的Mercer核),保证存在一个映射(phi)到一个Hilbert空间(mathcal{H})的映射,因此</target>
        </trans-unit>
        <trans-unit id="a3bb404582c234b1b5161269097e65342126edc8" translate="yes" xml:space="preserve">
          <source>Kernel methods to project data into alternate dimensional spaces</source>
          <target state="translated">将数据投射到交替维度空间的内核方法</target>
        </trans-unit>
        <trans-unit id="7853e504e205e94517ed94484ade6d5285c25255" translate="yes" xml:space="preserve">
          <source>Kernel operators take one or two base kernels and combine them into a new kernel. The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt;&lt;code&gt;Sum&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k1\) and \(k2\) and combines them via \(k_{sum}(X, Y) = k1(X, Y) + k2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt;&lt;code&gt;Product&lt;/code&gt;&lt;/a&gt; kernel takes two kernels \(k1\) and \(k2\) and combines them via \(k_{product}(X, Y) = k1(X, Y) * k2(X, Y)\). The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt;&lt;code&gt;Exponentiation&lt;/code&gt;&lt;/a&gt; kernel takes one base kernel and a scalar parameter \(exponent\) and combines them via \(k_{exp}(X, Y) = k(X, Y)^\text{exponent}\).</source>
          <target state="translated">内核运算符采用一个或两个基本内核，并将它们组合成一个新内核。的&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt; &lt;code&gt;Sum&lt;/code&gt; &lt;/a&gt;内核需要两个内核\（K1 \）和\（K2 \），并通过将它们组合\（{K_总和}（X，Y）= K1（X，Y）+ K2（X，Y）\）。该&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt; &lt;code&gt;Product&lt;/code&gt; &lt;/a&gt;的内核需要两个内核\（K1 \）和\（K2 \），并通过将它们组合\（{K_产物}（X，Y）= K1（X，Y）* K2（X，Y）\）。所述&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.exponentiation#sklearn.gaussian_process.kernels.Exponentiation&quot;&gt; &lt;code&gt;Exponentiation&lt;/code&gt; &lt;/a&gt;内核需要一个基本内核，并通过一个标量参数\（指数\）并且将它们组合\（{K_ EXP}（X，Y）= K（X，Y）^ \ {文本指数} \）。</target>
        </trans-unit>
        <trans-unit id="9dc320ddac29ab60da57cafc47693079e4b6b082" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression (KRR) &lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;[M2012]&lt;/a&gt; combines &lt;a href=&quot;linear_model#ridge-regression&quot;&gt;Ridge Regression&lt;/a&gt; (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.</source>
          <target state="translated">内核岭回归（KRR）&lt;a href=&quot;#m2012&quot; id=&quot;id1&quot;&gt;[M2012]&lt;/a&gt;结合了&lt;a href=&quot;linear_model#ridge-regression&quot;&gt;岭回归&lt;/a&gt;（具有L2-范数正则化的线性最小二乘）和内核技巧。因此，它学习了由各个内核和数据产生的空间中的线性函数。对于非线性内核，这对应于原始空间中的非线性函数。</target>
        </trans-unit>
        <trans-unit id="7d585be11bb912be319b898c908d63ce568dd8c0" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression (KRR) combines ridge regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.</source>
          <target state="translated">内核岭回归(KRR)将岭回归(线性最小二乘法与l2-正则化)与内核技巧相结合。因此,它在各自内核和数据引起的空间中学习一个线性函数。对于非线性内核,这相当于原始空间中的非线性函数。</target>
        </trans-unit>
        <trans-unit id="262cee695a2ed79939315817b4a3a26823167afe" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression combines ridge regression with the kernel trick</source>
          <target state="translated">内核岭回归将岭回归与内核技巧相结合</target>
        </trans-unit>
        <trans-unit id="589ad014b6254add975c198ac204f9560e253ac1" translate="yes" xml:space="preserve">
          <source>Kernel ridge regression.</source>
          <target state="translated">核脊回归。</target>
        </trans-unit>
        <trans-unit id="a797077c9a6730a652ad75f039a934d138c2b41f" translate="yes" xml:space="preserve">
          <source>Kernel to use in the model: linear, polynomial, RBF, sigmoid or precomputed.</source>
          <target state="translated">模型中使用的内核:线性、多项式、RBF、sigmoid或预计算。</target>
        </trans-unit>
        <trans-unit id="407ab400408caf91955e34873fdbfe1f6ae14b07" translate="yes" xml:space="preserve">
          <source>Kernel to use in the model: linear, polynomial, RBF, sigmoid or precomputed. &amp;lsquo;rbf&amp;rsquo; by default.</source>
          <target state="translated">模型中使用的内核：线性，多项式，RBF，S形或预先计算。默认情况下为&amp;ldquo; rbf&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="716837a63a81bd1da24c9f2580ff0581777fc381" translate="yes" xml:space="preserve">
          <source>Kernel which is composed of a set of other kernels.</source>
          <target state="translated">由一组其他内核组成的内核。</target>
        </trans-unit>
        <trans-unit id="a170413f32a293189023e0700b83d22ea6042972" translate="yes" xml:space="preserve">
          <source>Kernel. Default=&amp;rdquo;linear&amp;rdquo;.</source>
          <target state="translated">核心。默认值=&amp;ldquo;线性&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="e3cb275740ef8ee4f25f4b8b1bb2cb56094f01c1" translate="yes" xml:space="preserve">
          <source>Kernels (also called &amp;ldquo;covariance functions&amp;rdquo; in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by defining the &amp;ldquo;similarity&amp;rdquo; of two datapoints combined with the assumption that similar datapoints should have similar target values. Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values \(k(x_i, x_j)= k(d(x_i, x_j))\) and are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. For more details, we refer to Chapter 4 of &lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt;.</source>
          <target state="translated">核（在GP中也称为&amp;ldquo;协方差函数&amp;rdquo;）是GP的重要组成部分，它决定了GP前后的形状。他们通过定义两个数据点的&amp;ldquo;相似性&amp;rdquo;，并结合相似数据点应具有相似目标值的假设，对正在学习的函数的假设进行编码。可以区分两类内核：固定内核仅取决于两个数据点的距离，而不取决于它们的绝对值\（k（x_i，x_j）= k（d（x_i，x_j））\），因此不变于转换在输入空间中，非平稳内核也取决于数据点的特定值。固定核可以进一步细分为各向同性和各向异性的核，其中各向同性核也对输入空间中的旋转不变。更多细节，我们参考第4章&lt;a href=&quot;#rw2006&quot; id=&quot;id5&quot;&gt;[RW2006]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0ad8dd8fec70a9d46c4f724f1ce47b4b45810363" translate="yes" xml:space="preserve">
          <source>Kernels are measures of similarity, i.e. &lt;code&gt;s(a, b) &amp;gt; s(a, c)&lt;/code&gt; if objects &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are considered &amp;ldquo;more similar&amp;rdquo; than objects &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt;. A kernel must also be positive semi-definite.</source>
          <target state="translated">内核是相似的措施，即， &lt;code&gt;s(a, b) &amp;gt; s(a, c)&lt;/code&gt; 如果对象 &lt;code&gt;a&lt;/code&gt; 和 &lt;code&gt;b&lt;/code&gt; 被认为是&amp;ldquo;更类似于&amp;rdquo;比对象 &lt;code&gt;a&lt;/code&gt; 和 &lt;code&gt;c&lt;/code&gt; 。内核还必须是正半定数。</target>
        </trans-unit>
        <trans-unit id="cd28143394596209b24bd87df6806973641c2997" translate="yes" xml:space="preserve">
          <source>Kernels are parameterized by a vector \(\theta\) of hyperparameters. These hyperparameters can for instance control length-scales or periodicity of a kernel (see below). All kernels support computing analytic gradients of of the kernel&amp;rsquo;s auto-covariance with respect to \(\theta\) via setting &lt;code&gt;eval_gradient=True&lt;/code&gt; in the &lt;code&gt;__call__&lt;/code&gt; method. This gradient is used by the Gaussian process (both regressor and classifier) in computing the gradient of the log-marginal-likelihood, which in turn is used to determine the value of \(\theta\), which maximizes the log-marginal-likelihood, via gradient ascent. For each hyperparameter, the initial value and the bounds need to be specified when creating an instance of the kernel. The current value of \(\theta\) can be get and set via the property &lt;code&gt;theta&lt;/code&gt; of the kernel object. Moreover, the bounds of the hyperparameters can be accessed by the property &lt;code&gt;bounds&lt;/code&gt; of the kernel. Note that both properties (theta and bounds) return log-transformed values of the internally used values since those are typically more amenable to gradient-based optimization. The specification of each hyperparameter is stored in the form of an instance of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt;&lt;code&gt;Hyperparameter&lt;/code&gt;&lt;/a&gt; in the respective kernel. Note that a kernel using a hyperparameter with name &amp;ldquo;x&amp;rdquo; must have the attributes self.x and self.x_bounds.</source>
          <target state="translated">内核通过超参数向量\（\ theta \）进行参数化。这些超参数可以例如控制内核的长度尺度或周期性（请参见下文）。通过在 &lt;code&gt;__call__&lt;/code&gt; 方法中设置 &lt;code&gt;eval_gradient=True&lt;/code&gt; ，所有内核都支持针对\（\ theta \）计算内核自协方差的解析梯度。高斯过程（回归器和分类器）都使用此梯度来计算对数边际似然率的梯度，而对数边际似然率的梯度又用于确定\（\ theta \）的值，从而使对数边际最大化。通过梯度上升的可能性。对于每个超参数，在创建内核实例时都需要指定初始值和界限。 \（\ theta \）的当前值可以通过该属性获取和设置 &lt;code&gt;theta&lt;/code&gt; 内核对象的theta。而且，超参数的界限可以通过内核的属性 &lt;code&gt;bounds&lt;/code&gt; 来访问。请注意，这两个属性（&amp;theta;和范围）都返回内部使用值的对数转换后的值，因为这些值通常更适合基于梯度的优化。每个超参数的规范被存储在的实例的形式&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.hyperparameter#sklearn.gaussian_process.kernels.Hyperparameter&quot;&gt; &lt;code&gt;Hyperparameter&lt;/code&gt; &lt;/a&gt;在相应的内核。注意，使用名称为&amp;ldquo; x&amp;rdquo;的超参数的内核必须具有self.x和self.x_bounds属性。</target>
        </trans-unit>
        <trans-unit id="2a754d09a87b01a5043bf319d676ca0f6cb6a853" translate="yes" xml:space="preserve">
          <source>Kernels:</source>
          <target state="translated">Kernels:</target>
        </trans-unit>
        <trans-unit id="c3b9fc0d0d17c07a841795715ed044ed9e710926" translate="yes" xml:space="preserve">
          <source>Kevin P. Murphy &amp;ldquo;Machine Learning: A Probabilistic Perspective&amp;rdquo;, The MIT Press chapter 14.4.3, pp. 492-493</source>
          <target state="translated">凯文&amp;middot;墨菲（Kevin P. Murphy），《机器学习：概率论》，麻省理工学院出版社第14.4.3章，第492-493页</target>
        </trans-unit>
        <trans-unit id="1ebff3fd3bf929976eef25f0da78c334d18a2c1d" translate="yes" xml:space="preserve">
          <source>Keys are parameter names that can be passed to &lt;a href=&quot;sklearn.set_config#sklearn.set_config&quot;&gt;&lt;code&gt;set_config&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">键是可以传递给&lt;a href=&quot;sklearn.set_config#sklearn.set_config&quot;&gt; &lt;code&gt;set_config&lt;/code&gt; 的&lt;/a&gt;参数名称。</target>
        </trans-unit>
        <trans-unit id="c16cf0c8b95cb6641127d4ecde39c2d13ee54107" translate="yes" xml:space="preserve">
          <source>Keyword arguments allow to adapt these defaults to specific data sets (see parameters &lt;code&gt;target_name&lt;/code&gt;, &lt;code&gt;data_name&lt;/code&gt;, &lt;code&gt;transpose_data&lt;/code&gt;, and the examples below).</source>
          <target state="translated">关键字参数允许将这些默认值修改为特定的数据集（请参阅参数 &lt;code&gt;target_name&lt;/code&gt; ， &lt;code&gt;data_name&lt;/code&gt; ， &lt;code&gt;transpose_data&lt;/code&gt; 和以下示例）。</target>
        </trans-unit>
        <trans-unit id="bd2209e677c2e2331711a5337dc06706ac2ee537" translate="yes" xml:space="preserve">
          <source>Keyword arguments to pass to specified metric function.</source>
          <target state="translated">要传递给指定的度量函数的关键字参数。</target>
        </trans-unit>
        <trans-unit id="b6574be8c6baa963e814d600a049a18b07924f05" translate="yes" xml:space="preserve">
          <source>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). &lt;a href=&quot;http://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;Feature hashing for large scale multitask learning&lt;/a&gt;. Proc. ICML.</source>
          <target state="translated">Kilian Weinberger，Anirban Dasgupta，John Langford，Alex Smola和Josh Attenberg（2009）。&lt;a href=&quot;http://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;特征哈希用于大规模多任务学习&lt;/a&gt;。进程 ICML。</target>
        </trans-unit>
        <trans-unit id="35a95e3949c1091022c84b09bdfaee477e2ca247" translate="yes" xml:space="preserve">
          <source>Kingma, Diederik, and Jimmy Ba. &amp;ldquo;Adam: A method for stochastic</source>
          <target state="translated">Kingma，Diederik和Jimmy Ba。&amp;ldquo;亚当：一种随机的方法</target>
        </trans-unit>
        <trans-unit id="7bf0d4f9044d36fbabdb373fe028824c8f48b797" translate="yes" xml:space="preserve">
          <source>Kluger, Y., Basri, R., Chang, J. T., &amp;amp; Gerstein, M. (2003). Spectral biclustering of microarray data: coclustering genes and conditions. Genome research, 13(4), 703-716.</source>
          <target state="translated">Kluger，Y.，Basri，R.，Chang，JT，＆Gerstein，M.（2003年）。芯片数据的光谱二聚化：基因和条件的聚类。基因组研究，13（4），703-716。</target>
        </trans-unit>
        <trans-unit id="454573718b795c598350a3ed3c4e500004992423" translate="yes" xml:space="preserve">
          <source>Kluger, Yuval, et. al., 2003. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608&quot;&gt;Spectral biclustering of microarray data: coclustering genes and conditions&lt;/a&gt;.</source>
          <target state="translated">Kluger，Yuval等。等人，2003年。&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608&quot;&gt;微阵列数据的光谱二聚化：基因和条件的聚聚&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="c956cdb3811d15bc82b9ab562e4744234449e302" translate="yes" xml:space="preserve">
          <source>Knowing only the number of samples, the &lt;a href=&quot;generated/sklearn.random_projection.johnson_lindenstrauss_min_dim#sklearn.random_projection.johnson_lindenstrauss_min_dim&quot;&gt;&lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt;&lt;/a&gt; estimates conservatively the minimal size of the random subspace to guarantee a bounded distortion introduced by the random projection:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.random_projection.johnson_lindenstrauss_min_dim#sklearn.random_projection.johnson_lindenstrauss_min_dim&quot;&gt; &lt;code&gt;sklearn.random_projection.johnson_lindenstrauss_min_dim&lt;/code&gt; &lt;/a&gt;仅知道样本数，因此保守估计随机子空间的最小大小，以确保随机投影引入的有界失真：</target>
        </trans-unit>
        <trans-unit id="dc8be79b794b57340c1a9b2bf6e67594910f3213" translate="yes" xml:space="preserve">
          <source>Koby Crammer, Yoram Singer. On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines. Journal of Machine Learning Research 2, (2001), 265-292</source>
          <target state="translated">Koby Crammer,Yoram Singer.On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines.Journal of Machine Learning Research 2,(2001),265-292。</target>
        </trans-unit>
        <trans-unit id="5c3682641cb862b7b72f47a7d095c9e12f698d72" translate="yes" xml:space="preserve">
          <source>Kullback-Leibler divergence after optimization.</source>
          <target state="translated">优化后的Kullback-Leibler分歧。</target>
        </trans-unit>
        <trans-unit id="58f9065948558949c0307af59f2acaf3f9203c82" translate="yes" xml:space="preserve">
          <source>KulsinskiDistance</source>
          <target state="translated">KulsinskiDistance</target>
        </trans-unit>
        <trans-unit id="cb6565437657bdf8e9b94faf7a832064c7b5f242" translate="yes" xml:space="preserve">
          <source>L-BFGS is a solver that approximates the Hessian matrix which represents the second-order partial derivative of a function. Further it approximates the inverse of the Hessian matrix to perform parameter updates. The implementation uses the Scipy version of &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGS&lt;/a&gt;.</source>
          <target state="translated">L-BFGS是一种求解器，它近似表示代表函数二阶偏导数的黑森州矩阵。此外，它近似于Hessian矩阵的逆矩阵以执行参数更新。该实现使用&lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html&quot;&gt;L-BFGS&lt;/a&gt;的Scipy版本。</target>
        </trans-unit>
        <trans-unit id="a9d5151f1c406ba9642eb6d20ad7472462d4b8c9" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Bagging predictors&amp;rdquo;, Machine Learning 24, pages 123-140, 1996.</source>
          <target state="translated">L. Breiman，&amp;ldquo;装袋预测器&amp;rdquo;，机器学习24，第123-140页，1996年。</target>
        </trans-unit>
        <trans-unit id="05401786a74b32c74f5aaf77879ff5fe2a1ce4dc" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Bagging predictors&amp;rdquo;, Machine Learning, 24(2), 123-140, 1996.</source>
          <target state="translated">L. Breiman，&amp;ldquo;装袋预测变量&amp;rdquo;，机器学习，24（2），123-140，1996年。</target>
        </trans-unit>
        <trans-unit id="ae813a657051355d781d3ca7a4417546370b5fb0" translate="yes" xml:space="preserve">
          <source>L. Breiman, &amp;ldquo;Pasting small votes for classification in large databases and on-line&amp;rdquo;, Machine Learning, 36(1), 85-103, 1999.</source>
          <target state="translated">L. Breiman，&amp;ldquo;粘贴小票以在大型数据库中在线分类&amp;rdquo;，Machine Learning，36（1），85-103，1999年。</target>
        </trans-unit>
        <trans-unit id="93aaad4c8bcdef78f99bc463e879b251fb063491" translate="yes" xml:space="preserve">
          <source>L. Breiman, J. Friedman, R. Olshen, and C. Stone, &amp;ldquo;Classification and Regression Trees&amp;rdquo;, Wadsworth, Belmont, CA, 1984.</source>
          <target state="translated">L. Breiman，J。Friedman，R。Olshen和C.Stone，&amp;ldquo;分类和回归树&amp;rdquo;，沃兹沃思，贝尔蒙特，加利福尼亚，1984年。</target>
        </trans-unit>
        <trans-unit id="728ad1a9616394c8f19b0d53311780e8eed780ec" translate="yes" xml:space="preserve">
          <source>L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984.</source>
          <target state="translated">L.Breiman,J.Friedman,R.Olshen,and C.Stone.分类和回归树。Wadsworth,Belmont,CA,1984.</target>
        </trans-unit>
        <trans-unit id="26e831dbfd841f8bca5cddecddc5d95f765adc3b" translate="yes" xml:space="preserve">
          <source>L. Breiman, P. Spector &lt;a href=&quot;http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf&quot;&gt;Submodel selection and evaluation in regression: The X-random case&lt;/a&gt;, International Statistical Review 1992;</source>
          <target state="translated">L. Breiman，P. Spector &lt;a href=&quot;http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf&quot;&gt;回归中的子模型选择和评估：X随机案例&lt;/a&gt;，国际统计评论1992；</target>
        </trans-unit>
        <trans-unit id="da524759b928a0c6c0410a2ba55315d0723efbf9" translate="yes" xml:space="preserve">
          <source>L. Breiman, and A. Cutler, &amp;ldquo;Random Forests&amp;rdquo;, &lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&lt;/a&gt;</source>
          <target state="translated">L. Breiman和A. Cutler，&amp;ldquo;随机森林&amp;rdquo;，&lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;http：//www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="27e9c034667fd587e63afe1b6bd9ac5dd761c4eb" translate="yes" xml:space="preserve">
          <source>L1 AND L2 Regularization for Multiclass Hinge Loss Models by Robert C. Moore, John DeNero.</source>
          <target state="translated">L1 AND L2 Regularization for Multiclass Hinge Loss Models by Robert C.Moore,John DeNero.</target>
        </trans-unit>
        <trans-unit id="739dce23f089e2bc4737d849cf6e6812aaac6b25" translate="yes" xml:space="preserve">
          <source>L1 Penalty and Sparsity in Logistic Regression</source>
          <target state="translated">L1惩罚和逻辑回归中的稀疏性。</target>
        </trans-unit>
        <trans-unit id="8d79d7e84774c8797e94aafbcec78896f21a814d" translate="yes" xml:space="preserve">
          <source>L1 norm: \(R(w) := \sum_{i=1}^{n} |w_i|\), which leads to sparse solutions.</source>
          <target state="translated">L1规范。\(R(w):=\sum_{i=1}^{n})|w_i|\),从而导致稀疏的解。</target>
        </trans-unit>
        <trans-unit id="ae8ca0f194d88f499adeb94f8b5c01af62268b9f" translate="yes" xml:space="preserve">
          <source>L2 norm: \(R(w) := \frac{1}{2} \sum_{i=1}^{n} w_i^2\),</source>
          <target state="translated">L2规范。\(R(w):=\frac{1}{2})\sum_{i=1}^{n}w_i^2/)。)</target>
        </trans-unit>
        <trans-unit id="e55996560b375d2b1311657b3550d521d2224094" translate="yes" xml:space="preserve">
          <source>L2 penalty (regularization term) parameter.</source>
          <target state="translated">L2惩罚(正则化项)参数。</target>
        </trans-unit>
        <trans-unit id="7d7eb4b58ee70885659f8b6dfa6b739d18b840b6" translate="yes" xml:space="preserve">
          <source>LIBLINEAR &amp;ndash; A Library for Large Linear Classification</source>
          <target state="translated">LIBLINEAR &amp;ndash;大型线性分类的库</target>
        </trans-unit>
        <trans-unit id="23f600324ae930d885bf27049a430c382dc77087" translate="yes" xml:space="preserve">
          <source>LIBLINEAR: A Library for Large Linear Classification</source>
          <target state="translated">LIBLINEAR:大型线性分类库</target>
        </trans-unit>
        <trans-unit id="2f7204b5759b40e38407ab9bdcb1553f2d733475" translate="yes" xml:space="preserve">
          <source>LSA is also known as latent semantic indexing, LSI, though strictly that refers to its use in persistent indexes for information retrieval purposes.</source>
          <target state="translated">LSA也被称为潜在语义索引,LSI,不过严格来说,这是指它在持久性索引中用于信息检索的目的。</target>
        </trans-unit>
        <trans-unit id="f4a5095ae748443324845cf5a2f1b28d147ed2ca" translate="yes" xml:space="preserve">
          <source>LSH Forest being an approximate method, some true neighbors from the indexed dataset might be missing from the results.</source>
          <target state="translated">LSH森林是一种近似的方法,一些来自索引数据集的真实邻居可能会从结果中丢失。</target>
        </trans-unit>
        <trans-unit id="afceea8d4c81422ac802414c94f3f49075a51ec4" translate="yes" xml:space="preserve">
          <source>LSH Forest: Locality Sensitive Hashing forest [1] is an alternative method for vanilla approximate nearest neighbor search methods. LSH forest data structure has been implemented using sorted arrays and binary search and 32 bit fixed-length hashes. Random projection is used as the hash family which approximates cosine distance.</source>
          <target state="translated">LSH森林。Locality Sensitive Hashing forest[1]是香草近似最近邻搜索方法的一种替代方法。LSH森林的数据结构已经使用排序数组和二进制搜索以及32位固定长度哈希来实现。随机投影被用作近似余弦距离的哈希族。</target>
        </trans-unit>
        <trans-unit id="497cbd9196f20980eefacbc5b295901fb0a6c25f" translate="yes" xml:space="preserve">
          <source>LSTAT % lower status of the population</source>
          <target state="translated">LSTAT:人口中地位较低的百分比</target>
        </trans-unit>
        <trans-unit id="10e8ec7cf1b34af007bc1d6b016abc85aa0b454d" translate="yes" xml:space="preserve">
          <source>Label Propagation classifier</source>
          <target state="translated">标签传播</target>
        </trans-unit>
        <trans-unit id="abaf5a09ed6812e5734e77c1313bb44d953f5d5d" translate="yes" xml:space="preserve">
          <source>Label Propagation digits active learning</source>
          <target state="translated">标签传播数字主动学习</target>
        </trans-unit>
        <trans-unit id="a45a75b5c87b437cf487b153831ce5b94e5322d0" translate="yes" xml:space="preserve">
          <source>Label Propagation digits: Demonstrating performance</source>
          <target state="translated">标签传播位数。演示性能</target>
        </trans-unit>
        <trans-unit id="f15baf6416f92a52b1527f1d28d49a335fe3d388" translate="yes" xml:space="preserve">
          <source>Label Propagation learning a complex structure</source>
          <target state="translated">标签传播学习复杂结构</target>
        </trans-unit>
        <trans-unit id="5f24cba3626113f57fbd8f2c1a1dac90f055831d" translate="yes" xml:space="preserve">
          <source>Label assigned to each item via the transduction.</source>
          <target state="translated">通过转导分配给每个项目的标签。</target>
        </trans-unit>
        <trans-unit id="0154541a5d5e8e0b2444f876377737f91ad447a9" translate="yes" xml:space="preserve">
          <source>Label considered as positive and others are considered negative.</source>
          <target state="translated">标签被认为是积极的,其他的被认为是消极的。</target>
        </trans-unit>
        <trans-unit id="e1c383c45e91a1b41ae4aea8504e1ff71ada889a" translate="yes" xml:space="preserve">
          <source>Label is 1 for an inlier and -1 for an outlier according to the LOF score and the contamination parameter.</source>
          <target state="translated">根据LOF评分和污染参数,标签为1表示不入流,-1表示离群。</target>
        </trans-unit>
        <trans-unit id="4a4a633c5d3b5ebf2a9c4453fb41f8475e350bc9" translate="yes" xml:space="preserve">
          <source>Label of the positive class. If None, the maximum label is used as positive class</source>
          <target state="translated">正类的标签。如果无,则以最大标签作为正类。</target>
        </trans-unit>
        <trans-unit id="91ed314c98998b774c857769b601470c2a4233d0" translate="yes" xml:space="preserve">
          <source>Label propagation denotes a few variations of semi-supervised graph inference algorithms.</source>
          <target state="translated">标签传播表示半监督图推理算法的一些变化。</target>
        </trans-unit>
        <trans-unit id="3a4c36d2f1914cbaa6f86d2f3e759e05f747e6f8" translate="yes" xml:space="preserve">
          <source>Label propagation models have two built-in kernel methods. Choice of kernel effects both scalability and performance of the algorithms. The following are available:</source>
          <target state="translated">标签传播模型有两种内置内核方法。内核的选择会影响算法的可扩展性和性能。有以下几种方法:</target>
        </trans-unit>
        <trans-unit id="d9c8943fba1565dfa00ecc788417147c59e84b5a" translate="yes" xml:space="preserve">
          <source>Label ranking average precision (LRAP) averages over the samples the answer to the following question: for each ground truth label, what fraction of higher-ranked labels were true labels? This performance measure will be higher if you are able to give better rank to the labels associated with each sample. The obtained score is always strictly greater than 0, and the best value is 1. If there is exactly one relevant label per sample, label ranking average precision is equivalent to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_reciprocal_rank&quot;&gt;mean reciprocal rank&lt;/a&gt;.</source>
          <target state="translated">标签排名平均精度（LRAP）对样本取平均值，得出以下问题的答案：对于每个地面真相标签，排名靠前的标签中有多少部分是真实标签？如果您能够对与每个样本相关的标签进行更好的排名，则此性能指标将更高。获得的分数始终严格大于0，最佳值为1。如果每个样本中只有一个相关标签，则标签排名的平均精度等于&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_reciprocal_rank&quot;&gt;平均倒数排名&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="12d27d4c8cd4504c7079d27029449977fea3fa44" translate="yes" xml:space="preserve">
          <source>Label ranking average precision (LRAP) is the average over each ground truth label assigned to each sample, of the ratio of true vs. total labels with lower score.</source>
          <target state="translated">标签排名平均精度(LRAP)是指分配给每个样本的每个地面真值标签的平均值,即真值与总标签的比值,分数较低。</target>
        </trans-unit>
        <trans-unit id="57882529b52287495d04cf4c6bba559a970b02d4" translate="yes" xml:space="preserve">
          <source>Label, which is given for outlier samples (samples with no neighbors on given radius). If set to None, ValueError is raised, when outlier is detected.</source>
          <target state="translated">标签,它是为离群值样本(在给定半径上没有邻居的样本)提供的。如果设置为None,当检测到离群值时,会引发ValueError。</target>
        </trans-unit>
        <trans-unit id="a3ea7d5af24c9f7706e04a90b4cc006ad64537bf" translate="yes" xml:space="preserve">
          <source>LabelSpreading model for semi-supervised learning</source>
          <target state="translated">半监督学习的LabelSpreading模型</target>
        </trans-unit>
        <trans-unit id="82b6583f37d4a090f2277f71261de91f41eff15e" translate="yes" xml:space="preserve">
          <source>Labelings that assign all classes members to the same clusters are complete be not always pure, hence penalized:</source>
          <target state="translated">将所有类成员分配到同一簇的标签是完整的不一定是纯粹的,因此受到惩罚。</target>
        </trans-unit>
        <trans-unit id="a59d28cce33bc578e32e7790445917276a69fe16" translate="yes" xml:space="preserve">
          <source>Labelings that assign all classes members to the same clusters are complete be not homogeneous, hence penalized:</source>
          <target state="translated">将所有类成员都分配到同一簇的标签是完整的不是同质的,因此受到惩罚。</target>
        </trans-unit>
        <trans-unit id="2625047637f13a503b1aa26353d53ce007980d47" translate="yes" xml:space="preserve">
          <source>Labelings that have pure clusters with members coming from the same classes are homogeneous but un-necessary splits harms completeness and thus penalize V-measure as well:</source>
          <target state="translated">有纯簇的标签,其成员来自相同的类,是同质的,但不必要的分裂会损害完整性,因此也会惩罚V-measure。</target>
        </trans-unit>
        <trans-unit id="040e8af7f9faa240f939c7eb15dd2f3691882d68" translate="yes" xml:space="preserve">
          <source>Labelled data.</source>
          <target state="translated">标记的数据。</target>
        </trans-unit>
        <trans-unit id="a8a910f7e8e66128e5f0f93a7ebe3b1d5812067b" translate="yes" xml:space="preserve">
          <source>Labelling a new sample is performed by finding the nearest centroid for a given sample.</source>
          <target state="translated">通过寻找给定样本的最近中心点来标记一个新样本。</target>
        </trans-unit>
        <trans-unit id="47fc9fa69e29f326a363aa6376f6761fa85e0797" translate="yes" xml:space="preserve">
          <source>Labels assigned by the first annotator.</source>
          <target state="translated">由第一个注释者指定的标签。</target>
        </trans-unit>
        <trans-unit id="bdb7346e56bb733f97e8f0b9d11cce2ffadf9042" translate="yes" xml:space="preserve">
          <source>Labels assigned by the second annotator. The kappa statistic is symmetric, so swapping &lt;code&gt;y1&lt;/code&gt; and &lt;code&gt;y2&lt;/code&gt; doesn&amp;rsquo;t change the value.</source>
          <target state="translated">由第二个注释器分配的标签。卡伯统计信息是对称的，因此交换 &lt;code&gt;y1&lt;/code&gt; 和 &lt;code&gt;y2&lt;/code&gt; 不会更改该值。</target>
        </trans-unit>
        <trans-unit id="202396c3dbc4d15cb0462523b4fd7f2f49834479" translate="yes" xml:space="preserve">
          <source>Labels assigned to the centroids of the subclusters after they are clustered globally.</source>
          <target state="translated">在对子群进行全局聚类后,分配给子群中心点的标签。</target>
        </trans-unit>
        <trans-unit id="86a5303314971b15773b1ad8460967a7978fc1e6" translate="yes" xml:space="preserve">
          <source>Labels associated to each face image. Those labels are ranging from 0-39 and correspond to the Subject IDs.</source>
          <target state="translated">与每个人脸图像相关的标签。这些标签的范围在0-39之间,对应于主体ID。</target>
        </trans-unit>
        <trans-unit id="b8a8237c586e7a43e02e7a221af16786bca65b16" translate="yes" xml:space="preserve">
          <source>Labels associated to each face image. Those labels range from 0-5748 and correspond to the person IDs.</source>
          <target state="translated">与每个人脸图像相关的标签。这些标签的范围在0-5748之间,与人名ID相对应。</target>
        </trans-unit>
        <trans-unit id="639c7a5f12221be9fa16d4184a91d960ee8d5fb6" translate="yes" xml:space="preserve">
          <source>Labels associated to each pair of images. The two label values being different persons or the same person.</source>
          <target state="translated">与每对图像相关联的标签。两个标签值是不同的人或同一个人。</target>
        </trans-unit>
        <trans-unit id="dd9359ae6e29bf7b087516560ad1a2e91d10cfb0" translate="yes" xml:space="preserve">
          <source>Labels for X.</source>
          <target state="translated">标签为X.</target>
        </trans-unit>
        <trans-unit id="0b53b6571e9267409e85ff23873e0a0824df02a7" translate="yes" xml:space="preserve">
          <source>Labels of each point</source>
          <target state="translated">各点的标签</target>
        </trans-unit>
        <trans-unit id="4350a7104cda6c17ed013efe2d00ebaae03eeb73" translate="yes" xml:space="preserve">
          <source>Labels of each point (if compute_labels is set to True).</source>
          <target state="translated">每个点的标签(如果compute_labels设置为True)。</target>
        </trans-unit>
        <trans-unit id="8c76fdcbe4be61c2bbf79d2e67413441e31eb988" translate="yes" xml:space="preserve">
          <source>Labels of each point.</source>
          <target state="translated">每个点的标签。</target>
        </trans-unit>
        <trans-unit id="9caa2dbfb17c8c2f4ae17aa6bab878223c8520e3" translate="yes" xml:space="preserve">
          <source>Labels to constrain permutation within groups, i.e. &lt;code&gt;y&lt;/code&gt; values are permuted among samples with the same group identifier. When not specified, &lt;code&gt;y&lt;/code&gt; values are permuted among all samples.</source>
          <target state="translated">标签以限制组内的置换，即，在具有相同组标识符的样本之间置换 &lt;code&gt;y&lt;/code&gt; 值。如果未指定，则在所有样本中排列 &lt;code&gt;y&lt;/code&gt; 值。</target>
        </trans-unit>
        <trans-unit id="c21c4f0b2fc516030c767721367e1d2fba51e007" translate="yes" xml:space="preserve">
          <source>Labels.</source>
          <target state="translated">Labels.</target>
        </trans-unit>
        <trans-unit id="45efe9972f3bf7c62e3db1678d501faf12d10c1b" translate="yes" xml:space="preserve">
          <source>Large &lt;code&gt;n_clusters&lt;/code&gt; and &lt;code&gt;n_samples&lt;/code&gt;</source>
          <target state="translated">大的 &lt;code&gt;n_clusters&lt;/code&gt; 和 &lt;code&gt;n_samples&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="28e08fa26129e68210c4b016ca6da1b08a1a37e9" translate="yes" xml:space="preserve">
          <source>Large &lt;code&gt;n_samples&lt;/code&gt; and &lt;code&gt;n_clusters&lt;/code&gt;</source>
          <target state="translated">大的 &lt;code&gt;n_samples&lt;/code&gt; 和 &lt;code&gt;n_clusters&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="d40959dcecc27d1d44b2e4cffa59a304e9a052a1" translate="yes" xml:space="preserve">
          <source>Large dataset, outlier removal, data reduction.</source>
          <target state="translated">大数据集,去除异常值,减少数据。</target>
        </trans-unit>
        <trans-unit id="8f1784e927c9c4e578edb46d860596eed4a90b35" translate="yes" xml:space="preserve">
          <source>Large outliers</source>
          <target state="translated">大的离群值</target>
        </trans-unit>
        <trans-unit id="20dfcd03ef69fe3c6c6e8549019c43956f87d5db" translate="yes" xml:space="preserve">
          <source>Lars computes a path solution only for each kink in the path. As a result, it is very efficient when there are only of few kinks, which is the case if there are few features or samples. Also, it is able to compute the full path without setting any meta parameter. On the opposite, coordinate descent compute the path points on a pre-specified grid (here we use the default). Thus it is more efficient if the number of grid points is smaller than the number of kinks in the path. Such a strategy can be interesting if the number of features is really large and there are enough samples to select a large amount. In terms of numerical errors, for heavily correlated variables, Lars will accumulate more errors, while the coordinate descent algorithm will only sample the path on a grid.</source>
          <target state="translated">Lars只对路径中的每个结点计算一个路径解。因此,当只有少数几个结点时,它的效率非常高,如果特征或样本较少,就会出现这种情况。同时,它能够在不设置任何元参数的情况下计算出完整的路径。相反,坐标下降法是在一个预先指定的网格上计算路径点(这里我们使用默认的)。因此,如果网格点的数量小于路径中的结点数量,则效率更高。如果特征的数量真的很大,并且有足够的样本来选择大量的特征,这样的策略就会很有趣。在数值误差方面,对于重度相关的变量,Lars会积累更多的误差,而坐标下降算法只会对网格上的路径进行采样。</target>
        </trans-unit>
        <trans-unit id="fafbf93538200568ab2506c2a63168c161506b4f" translate="yes" xml:space="preserve">
          <source>Lasso and Elastic Net</source>
          <target state="translated">拉索和弹性网</target>
        </trans-unit>
        <trans-unit id="64045413f4cce0f6cc0a64e33254b9beab1142d8" translate="yes" xml:space="preserve">
          <source>Lasso and Elastic Net for Sparse Signals</source>
          <target state="translated">稀疏信号的Lasso和弹性网络</target>
        </trans-unit>
        <trans-unit id="02b3c1dbfc5f6c26007e2282ba4be10a77581a65" translate="yes" xml:space="preserve">
          <source>Lasso and elastic net (L1 and L2 penalisation) implemented using a coordinate descent.</source>
          <target state="translated">Lasso和弹性网(L1和L2惩罚)采用坐标下降法实现。</target>
        </trans-unit>
        <trans-unit id="721bb6d50a67145009b7e81abd6add7dc9980ff6" translate="yes" xml:space="preserve">
          <source>Lasso computed by least-angle regression</source>
          <target state="translated">用最小角度回归法计算的套索。</target>
        </trans-unit>
        <trans-unit id="c805258f4c266592bbe9892ca4c6fe8fe41525e3" translate="yes" xml:space="preserve">
          <source>Lasso linear model with iterative fitting along a regularization path</source>
          <target state="translated">沿正则化路径迭代拟合的Lasso线性模型。</target>
        </trans-unit>
        <trans-unit id="af3dece2cf6ae684f46dbebc7279e4f62e00335d" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Lars using BIC or AIC for model selection</source>
          <target state="translated">Lasso模型与Lars拟合,使用BIC或AIC进行模型选择。</target>
        </trans-unit>
        <trans-unit id="050a0d126029facc258b43169ac1e55a978389bf" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Least Angle Regression a.k.a.</source>
          <target state="translated">Lasso模型拟合与最小角度回归,又称。</target>
        </trans-unit>
        <trans-unit id="9cd5532bfae0b1e27ef3555196bbd1195b2078fe" translate="yes" xml:space="preserve">
          <source>Lasso model fit with Least Angle Regression a.k.a. Lars</source>
          <target state="translated">Lasso模型拟合与最小角度回归,也就是Lars。</target>
        </trans-unit>
        <trans-unit id="7cbdf91f396ae23c8822ab30bdd340882655aa26" translate="yes" xml:space="preserve">
          <source>Lasso model selection: Cross-Validation / AIC / BIC</source>
          <target state="translated">Lasso模型选择。交叉验证/AIC/BIC。</target>
        </trans-unit>
        <trans-unit id="51c5bc73e17f640c8a180ee453dbdca923d8c408" translate="yes" xml:space="preserve">
          <source>Lasso on dense and sparse data</source>
          <target state="translated">密集和稀疏数据上的Lasso。</target>
        </trans-unit>
        <trans-unit id="4222e17e965145615293d33dd92e1394e71c2b5b" translate="yes" xml:space="preserve">
          <source>Lasso path using LARS</source>
          <target state="translated">使用LARS的套索路径</target>
        </trans-unit>
        <trans-unit id="1acac83cf58491df993404acd51caed4c4458648" translate="yes" xml:space="preserve">
          <source>Lasso using coordinate descent (&lt;a href=&quot;linear_model#lasso&quot;&gt;Lasso&lt;/a&gt;)</source>
          <target state="translated">套索使用坐标下降（&lt;a href=&quot;linear_model#lasso&quot;&gt;Lasso&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="e33c33e9d593ce188f7d437b3dd829993a23358f" translate="yes" xml:space="preserve">
          <source>Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents.</source>
          <target state="translated">Latent Dirichlet Allocation是一种用于文本体等离散数据集的生成性概率模型。它也是一种主题模型,用于从文档集合中发现抽象主题。</target>
        </trans-unit>
        <trans-unit id="b259b9fed25933f3361602dc71394efbeb9d0882" translate="yes" xml:space="preserve">
          <source>Latent Dirichlet Allocation with online variational Bayes algorithm</source>
          <target state="translated">用在线变量贝叶斯算法进行潜在的Dirichlet分配</target>
        </trans-unit>
        <trans-unit id="691257140e4ed31a708c6cf301cec44aee34c69f" translate="yes" xml:space="preserve">
          <source>Latent representations of the data.</source>
          <target state="translated">数据的潜在表现形式。</target>
        </trans-unit>
        <trans-unit id="7972223ce1d5a83652f334b349de24d196516da5" translate="yes" xml:space="preserve">
          <source>Later you can load back the pickled model (possibly in another Python process) with:</source>
          <target state="translated">稍后您可以用以下方法将腌制好的模型加载回来 (可能在另一个 Python 进程中)。</target>
        </trans-unit>
        <trans-unit id="af705669290f66a0c593b0deebc97c8dff7d4996" translate="yes" xml:space="preserve">
          <source>Later, you can reload the pickled model (possibly in another Python process) with:</source>
          <target state="translated">稍后,您可以用以下方法重新加载腌制的模型 (可能在另一个 Python 进程中)。</target>
        </trans-unit>
        <trans-unit id="87b4154b3c380b9ca1fa8f1419dd8e2c1d34065a" translate="yes" xml:space="preserve">
          <source>Latitude house block latitude</source>
          <target state="translated">纬度住宅小区纬度</target>
        </trans-unit>
        <trans-unit id="5d6517da9252e690b07eb861ecaf7b79646512be" translate="yes" xml:space="preserve">
          <source>Leaf size passed to &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt;. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">叶大小传递给&lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt;。这会影响构造和查询的速度，以及存储树所需的内存。最佳值取决于问题的性质。</target>
        </trans-unit>
        <trans-unit id="adfd1a5c3117b99a14c45a4ae06038fd4593b137" translate="yes" xml:space="preserve">
          <source>Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">传递给BallTree或KDTree的叶子大小。这可能会影响构造和查询的速度,以及存储树所需的内存。最佳值取决于问题的性质。</target>
        </trans-unit>
        <trans-unit id="2f4f4f9d9992d30c454ebca3af5182554c5dd5d3" translate="yes" xml:space="preserve">
          <source>Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.</source>
          <target state="translated">传递给 BallTree 或 cKDTree 的叶子大小。这可能会影响构造和查询的速度,以及存储树所需的内存。最佳值取决于问题的性质。</target>
        </trans-unit>
        <trans-unit id="31743e5f5ee8b348cb24154ab26179446399d075" translate="yes" xml:space="preserve">
          <source>Learn a NMF model for the data X and returns the transformed data.</source>
          <target state="translated">学习数据X的NMF模型,并返回变换后的数据。</target>
        </trans-unit>
        <trans-unit id="a49199fe15b3d192e2f8e78d2cfcc004b5bb592f" translate="yes" xml:space="preserve">
          <source>Learn a NMF model for the data X.</source>
          <target state="translated">学习数据X的NMF模型。</target>
        </trans-unit>
        <trans-unit id="f28a5a2a8197ba712162f1642134c8c32dff12de" translate="yes" xml:space="preserve">
          <source>Learn a list of feature name -&amp;gt; indices mappings and transform X.</source>
          <target state="translated">了解功能名称列表-&amp;gt;索引映射并转换X。</target>
        </trans-unit>
        <trans-unit id="8c410f4ecac33d5545793d5deb3e8b1121157db0" translate="yes" xml:space="preserve">
          <source>Learn a list of feature name -&amp;gt; indices mappings.</source>
          <target state="translated">了解功能名称列表-&amp;gt;索引映射。</target>
        </trans-unit>
        <trans-unit id="a753afaf1f2a5a0c1c19f381e2c844f4e69ccf16" translate="yes" xml:space="preserve">
          <source>Learn a vocabulary dictionary of all tokens in the raw documents.</source>
          <target state="translated">学习原始文件中所有令牌的词汇词典。</target>
        </trans-unit>
        <trans-unit id="d9349583a45dc48570d0d3236e8faf8ecfef570b" translate="yes" xml:space="preserve">
          <source>Learn and apply the dimension reduction on the train data.</source>
          <target state="translated">在训练数据上学习和应用减维。</target>
        </trans-unit>
        <trans-unit id="c8e9cfdd99f37695b9bb2a2cf234f653fe10a376" translate="yes" xml:space="preserve">
          <source>Learn empirical variances from X.</source>
          <target state="translated">从X中学习经验方差。</target>
        </trans-unit>
        <trans-unit id="650a6ae9c550e7f32470024973e3b36aee2841fa" translate="yes" xml:space="preserve">
          <source>Learn model for the data X with variational Bayes method.</source>
          <target state="translated">用方差贝叶斯方法学习数据X的模型。</target>
        </trans-unit>
        <trans-unit id="dacb80f7c7ce4a5db80b953f259da8b386886101" translate="yes" xml:space="preserve">
          <source>Learn the idf vector (global term weights)</source>
          <target state="translated">学习idf向量(全局项权重)</target>
        </trans-unit>
        <trans-unit id="b331e0a3149fc26c2099c41ac3e9655d530c7a47" translate="yes" xml:space="preserve">
          <source>Learn the inverse transform for non-precomputed kernels. (i.e. learn to find the pre-image of a point)</source>
          <target state="translated">学习非计算核的反变换。(即学会寻找点的前像)</target>
        </trans-unit>
        <trans-unit id="ee96e1f94ac61b3bff29cbb75afd2fdb8a437bed" translate="yes" xml:space="preserve">
          <source>Learn the vocabulary dictionary and return term-document matrix.</source>
          <target state="translated">学习词汇词典,并返回术语-文档矩阵。</target>
        </trans-unit>
        <trans-unit id="65adc2e107d7d619d7107cf14005ab5e9c9cd5ef" translate="yes" xml:space="preserve">
          <source>Learn vocabulary and idf from training set.</source>
          <target state="translated">从训练集学习词汇和idf。</target>
        </trans-unit>
        <trans-unit id="d9ba5b6f4cc6cff1198de21973fdc3c62d64336f" translate="yes" xml:space="preserve">
          <source>Learn vocabulary and idf, return term-document matrix.</source>
          <target state="translated">学习词汇和idf,返回术语-文档矩阵。</target>
        </trans-unit>
        <trans-unit id="5b86400dde56a045e486ecc10d7618c7daf0f573" translate="yes" xml:space="preserve">
          <source>Learning a graph structure</source>
          <target state="translated">学习图结构</target>
        </trans-unit>
        <trans-unit id="f89176d3f1741099f1479699aa585a0c6906b634" translate="yes" xml:space="preserve">
          <source>Learning and predicting</source>
          <target state="translated">学习和预测</target>
        </trans-unit>
        <trans-unit id="5087c606edcdf30c07ac8bd6a14c9b96c0975b25" translate="yes" xml:space="preserve">
          <source>Learning curve.</source>
          <target state="translated">学习曲线。</target>
        </trans-unit>
        <trans-unit id="af86142d107ea3e7d568509ca68cbef348748b05" translate="yes" xml:space="preserve">
          <source>Learning problems fall into a few categories:</source>
          <target state="translated">学习问题可分为几类。</target>
        </trans-unit>
        <trans-unit id="213b18cf4e4c891522544c2231435e470a8853a1" translate="yes" xml:space="preserve">
          <source>Learning rate schedule for weight updates.</source>
          <target state="translated">权重更新的学习率时间表。</target>
        </trans-unit>
        <trans-unit id="ad3970bc51aa8e2c82bc13dcb9d4922e01a06590" translate="yes" xml:space="preserve">
          <source>Learning rate shrinks the contribution of each classifier by &lt;code&gt;learning_rate&lt;/code&gt;. There is a trade-off between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">学习率通过 &lt;code&gt;learning_rate&lt;/code&gt; 缩小每个分类器的贡献。在 &lt;code&gt;learning_rate&lt;/code&gt; 和 &lt;code&gt;n_estimators&lt;/code&gt; 之间需要权衡。</target>
        </trans-unit>
        <trans-unit id="cc055e36b16b7ea8669e5252649d8e3ee1af0b11" translate="yes" xml:space="preserve">
          <source>Learning rate shrinks the contribution of each regressor by &lt;code&gt;learning_rate&lt;/code&gt;. There is a trade-off between &lt;code&gt;learning_rate&lt;/code&gt; and &lt;code&gt;n_estimators&lt;/code&gt;.</source>
          <target state="translated">学习率通过 &lt;code&gt;learning_rate&lt;/code&gt; 缩小每个回归变量的贡献。在 &lt;code&gt;learning_rate&lt;/code&gt; 和 &lt;code&gt;n_estimators&lt;/code&gt; 之间需要权衡。</target>
        </trans-unit>
        <trans-unit id="8982fb177d3b5a895540d84670a324c9b8376572" translate="yes" xml:space="preserve">
          <source>Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called &lt;strong&gt;overfitting&lt;/strong&gt;. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a &lt;strong&gt;test set&lt;/strong&gt;&lt;code&gt;X_test, y_test&lt;/code&gt;. Note that the word &amp;ldquo;experiment&amp;rdquo; is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally.</source>
          <target state="translated">学习预测函数的参数并在相同的数据上对其进行测试是一个方法上的错误：一个仅重复其刚刚看到的样本标签的模型将获得完美的评分，但无法预测任何有用的信息，看不见的数据。这种情况称为&lt;strong&gt;过拟合&lt;/strong&gt;。为了避免这种情况，执行（监督）机器学习实验时通常的做法是将部分可用数据保留为&lt;strong&gt;测试集&lt;/strong&gt; &lt;code&gt;X_test, y_test&lt;/code&gt; 。请注意，&amp;ldquo;实验&amp;rdquo;一词并非仅表示学术用途，因为即使在商业环境中，机器学习通常也是从实验开始的。</target>
        </trans-unit>
        <trans-unit id="93c3e1794e48ba7d8637b32d813e97686cf36d4f" translate="yes" xml:space="preserve">
          <source>Learns each output independently rather than chaining.</source>
          <target state="translated">独立学习每个输出,而不是连锁。</target>
        </trans-unit>
        <trans-unit id="5fce8b00092369b98dfb920b76a7ee0efe5e00b1" translate="yes" xml:space="preserve">
          <source>Least Angle Regression model a.k.a.</source>
          <target state="translated">最小角度回归模型又称。</target>
        </trans-unit>
        <trans-unit id="3b28e26eb21f16fdbdefabf1ed5ad375edeafb8b" translate="yes" xml:space="preserve">
          <source>Least Angle Regression model a.k.a. LAR</source>
          <target state="translated">最小角度回归模型又称LAR模型</target>
        </trans-unit>
        <trans-unit id="b8ab306ac662259fba4aa6725b193c75be140b61" translate="yes" xml:space="preserve">
          <source>Least Squares projection of the data onto the sparse components.</source>
          <target state="translated">将数据的最小二乘投影到稀疏分量上。</target>
        </trans-unit>
        <trans-unit id="2c3aa035aea93ac3dc79ecee5528b7c8dcfba4ab" translate="yes" xml:space="preserve">
          <source>Least absolute deviation (&lt;code&gt;'lad'&lt;/code&gt;): A robust loss function for regression. The initial model is given by the median of the target values.</source>
          <target state="translated">最小绝对偏差（ &lt;code&gt;'lad'&lt;/code&gt; ）：用于回归的鲁棒损失函数。初始模型由目标值的中位数给出。</target>
        </trans-unit>
        <trans-unit id="3aeaacb76e6b5d496047f324133ddd0747e1d2c6" translate="yes" xml:space="preserve">
          <source>Least squares (&lt;code&gt;'ls'&lt;/code&gt;): The natural choice for regression due to its superior computational properties. The initial model is given by the mean of the target values.</source>
          <target state="translated">最小二乘（ &lt;code&gt;'ls'&lt;/code&gt; ）：回归的自然选择，因为其具有优越的计算性能。初始模型由目标值的平均值给出。</target>
        </trans-unit>
        <trans-unit id="7963186b092849241b779637d34ce64214b0375a" translate="yes" xml:space="preserve">
          <source>Least-Squares: Ridge Regression.</source>
          <target state="translated">最小二乘法。山脊回归:</target>
        </trans-unit>
        <trans-unit id="acf6db0396d489bb160af474285d57fb823df68a" translate="yes" xml:space="preserve">
          <source>Least-angle regression (&lt;a href=&quot;linear_model#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt;)</source>
          <target state="translated">最小角度回归（&lt;a href=&quot;linear_model#least-angle-regression&quot;&gt;最小角度回归&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="818f02ffe71d576f8833c06d3318f5d50790be37" translate="yes" xml:space="preserve">
          <source>Least-angle regression (LARS) is a regression algorithm for high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani. LARS is similar to forward stepwise regression. At each step, it finds the predictor most correlated with the response. When there are multiple predictors having equal correlation, instead of continuing along the same predictor, it proceeds in a direction equiangular between the predictors.</source>
          <target state="translated">最小角度回归(LARS)是一种针对高维数据的回归算法,由Bradley Efron、Trevor Hastie、Iain Johnstone和Robert Tibshirani开发。LARS类似于正向逐步回归。在每一步中,它都会找到与反应最相关的预测因子。当有多个预测因子具有相同的相关性时,它不是沿着同一个预测因子继续前进,而是沿着预测因子之间的等比方向前进。</target>
        </trans-unit>
        <trans-unit id="5cc9936fd171dfb4c941611970a01e31c9182cee" translate="yes" xml:space="preserve">
          <source>Leave One Group Out cross-validator</source>
          <target state="translated">撇开一组交叉验证器</target>
        </trans-unit>
        <trans-unit id="708b3ff9ed12b2c6f3635d37f516d672f76ad26e" translate="yes" xml:space="preserve">
          <source>Leave P Group(s) Out cross-validator</source>
          <target state="translated">离开P组,退出交叉验证器。</target>
        </trans-unit>
        <trans-unit id="b1d423c90dfa79c0db1cf2e91d8b80c110d2debb" translate="yes" xml:space="preserve">
          <source>Leave P groups out.</source>
          <target state="translated">不提P组。</target>
        </trans-unit>
        <trans-unit id="2e788c12c63436d5bbf2b3d54792d07b4ad5906d" translate="yes" xml:space="preserve">
          <source>Leave P observations out.</source>
          <target state="translated">不提P观察。</target>
        </trans-unit>
        <trans-unit id="23a4dfbb0e55172e2c29fa75763519463b465b57" translate="yes" xml:space="preserve">
          <source>Leave one observation out.</source>
          <target state="translated">漏掉一个意见。</target>
        </trans-unit>
        <trans-unit id="96e7c056605d5580183d915f0e8250d81cc4028b" translate="yes" xml:space="preserve">
          <source>Leave-One-Out cross-validator</source>
          <target state="translated">留一漏一交叉验证器</target>
        </trans-unit>
        <trans-unit id="a3d5fb094bf6540a5945dfebfc612a40422d0970" translate="yes" xml:space="preserve">
          <source>Leave-P-Out cross-validator</source>
          <target state="translated">留空交叉验证器</target>
        </trans-unit>
        <trans-unit id="7fa92633d7eb4070a1a9e7f3ffd6a6dd808d5514" translate="yes" xml:space="preserve">
          <source>Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix. The Journal of Portfolio Management 30(4), 110-119, 2004.</source>
          <target state="translated">Ledoit O,Wolf M.Honey,I Shrunk the Sample Covariance Matrix.The Journal of Portfolio Management 30(4),110-119,2004.</target>
        </trans-unit>
        <trans-unit id="b6a08e295c1dafc447ef93ac82d0e6a70b01528e" translate="yes" xml:space="preserve">
          <source>Ledoit-Wolf is a particular form of shrinkage, where the shrinkage coefficient is computed using O. Ledoit and M. Wolf&amp;rsquo;s formula as described in &amp;ldquo;A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices&amp;rdquo;, Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411.</source>
          <target state="translated">Ledoit-Wolf是一种特殊的收缩形式，其中收缩系数是使用O.Ledoit和M.Wolf的公式计算的，如《 Ledit和Wolf，多变量分析杂志》&amp;ldquo;大范围协方差矩阵的适当条件估计器&amp;rdquo;中所述。 ，第88卷，第2期，2004年2月，第365-411页。</target>
        </trans-unit>
        <trans-unit id="b450ff5574aa7547a2d2804a59fde9043d1f11e3" translate="yes" xml:space="preserve">
          <source>Ledoit-Wolf vs OAS estimation</source>
          <target state="translated">Ledoit-Wolf与OAS估计数</target>
        </trans-unit>
        <trans-unit id="74b56641357b357e1a04f8ba20caa0211258f1b9" translate="yes" xml:space="preserve">
          <source>LedoitWolf Estimator</source>
          <target state="translated">LedoitWolf估算器</target>
        </trans-unit>
        <trans-unit id="a7127a921977497178bbe9d19b374d5b3660e695" translate="yes" xml:space="preserve">
          <source>Left argument of the returned kernel k(X, Y)</source>
          <target state="translated">返回的内核k(X,Y)的左参数。</target>
        </trans-unit>
        <trans-unit id="75bf879e9683d8e42f9cdbce4ac2378477aafa4c" translate="yes" xml:space="preserve">
          <source>Length of the path. &lt;code&gt;eps=1e-3&lt;/code&gt; means that &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt;</source>
          <target state="translated">路径的长度。 &lt;code&gt;eps=1e-3&lt;/code&gt; 表示 &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c25fd55e85b58584519914fbcbbc8e0b71dacb4b" translate="yes" xml:space="preserve">
          <source>Length of the path. &lt;code&gt;eps=1e-3&lt;/code&gt; means that &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt;.</source>
          <target state="translated">路径的长度。 &lt;code&gt;eps=1e-3&lt;/code&gt; 表示 &lt;code&gt;alpha_min / alpha_max = 1e-3&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="820f47ab7dc5f77aa60df5d42cf3669d7140be19" translate="yes" xml:space="preserve">
          <source>Less sensitivity to the number of parameters:</source>
          <target state="translated">对参数数量的敏感度较低。</target>
        </trans-unit>
        <trans-unit id="650648dcfa58ca5d69540fc9d7c76c71c03cdd8d" translate="yes" xml:space="preserve">
          <source>Let K(x, z) be a kernel defined by phi(x)^T phi(z), where phi is a function mapping x to a Hilbert space. KernelCenterer centers (i.e., normalize to have zero mean) the data without explicitly computing phi(x). It is equivalent to centering phi(x) with sklearn.preprocessing.StandardScaler(with_std=False).</source>
          <target state="translated">让K(x,z)是一个由phi(x)^T phi(z)定义的内核,其中phi是一个将x映射到希尔伯特空间的函数。KernelCenterer将数据居中(即归一化为零均值),而不需要明确计算phi(x)。它相当于用sklearn.preprocessing.StandardScaler(with_std=False)对phi(x)进行居中处理。</target>
        </trans-unit>
        <trans-unit id="4d3e4f22e7563805ce13fd93e247765bb5c10653" translate="yes" xml:space="preserve">
          <source>Let \(S\) be the similarity matrix, and \(X\) the coordinates of the \(n\) input points. Disparities \(\hat{d}_{ij}\) are transformation of the similarities chosen in some optimal ways. The objective, called the stress, is then defined by \(sum_{i &amp;lt; j} d_{ij}(X) - \hat{d}_{ij}(X)\)</source>
          <target state="translated">令\（S \）为相似矩阵，\（X \）为\（n \）输入点的坐标。差异\（\ hat {d} _ {ij} \）是以某些最佳方式选择的相似性的转换。然后通过\（sum_ {i &amp;lt;j} d_ {ij}（X）-\ hat {d} _ {ij}（X）\）定义称为压力的目标。</target>
        </trans-unit>
        <trans-unit id="c0a7cf3804b0fab7efc2f61eebe3b5930a870c77" translate="yes" xml:space="preserve">
          <source>Let the data at node \(m\) be represented by \(Q\). For each candidate split \(\theta = (j, t_m)\) consisting of a feature \(j\) and threshold \(t_m\), partition the data into \(Q_{left}(\theta)\) and \(Q_{right}(\theta)\) subsets</source>
          <target state="translated">让节点/(m/)的数据用/(Q/)表示。对于每一个候选拆分/(theta=(j,t_m)/)由特征/(j/)和阈值/(t_m/)组成,将数据分割成/(Q_{left}(\theta)/)和/(Q_{right}(\theta)/)子集。</target>
        </trans-unit>
        <trans-unit id="0eec5761b9ded7fa59a47d01c0fcb2883cae7dd4" translate="yes" xml:space="preserve">
          <source>Let us now try to reconstruct the original image from the patches by averaging on overlapping areas:</source>
          <target state="translated">现在让我们尝试通过对重叠区域的平均来重建原始图像的补丁。</target>
        </trans-unit>
        <trans-unit id="106ecb5f7c6bb4669d70caeae32d17518696e61b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s print the first lines of the first loaded file:</source>
          <target state="translated">让我们打印第一个已加载文件的第一行：</target>
        </trans-unit>
        <trans-unit id="bcd495b6fedeb570ab62363e5dbb308b08a2e969" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 25, and 50, and want to know their class name.</source>
          <target state="translated">假设您对样本10、25和50感兴趣，并想知道它们的类名。</target>
        </trans-unit>
        <trans-unit id="65595eba1f80a7173dc24674f2afc2d5837968b2" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 50, and 85, and want to know their class name.</source>
          <target state="translated">假设您对样本10、50和85感兴趣，并想知道它们的类名。</target>
        </trans-unit>
        <trans-unit id="fb46983e946ca9f3803c9b6fd00931719bb67d7b" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s say you are interested in the samples 10, 80, and 140, and want to know their class name.</source>
          <target state="translated">假设您对样本10、80和140感兴趣，并想知道它们的类名。</target>
        </trans-unit>
        <trans-unit id="36ae1b66774b74a6fe504ba4aa0655c5c58c7d06" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s see how it looks for the &lt;code&gt;KFold&lt;/code&gt; cross-validation object:</source>
          <target state="translated">让我们看看它如何查找 &lt;code&gt;KFold&lt;/code&gt; 交叉验证对象：</target>
        </trans-unit>
        <trans-unit id="9c035fe2592c35e4264add67f9ea318300cfbf19" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s take a look at what the most informative features are:</source>
          <target state="translated">让我们看一下最有用的功能是：</target>
        </trans-unit>
        <trans-unit id="fc26adc7a4427a7251d50d414ace6a50d7fbe76d" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s take an example with the following counts. The first term is present 100% of the time hence not very interesting. The two other features only in less than 50% of the time hence probably more representative of the content of the documents:</source>
          <target state="translated">让我们以以下几点为例。第一项出现在100％的时间中，因此不是很有趣。其他两个功能仅在不到50％的时间内出现，因此可能更能代表文档的内容：</target>
        </trans-unit>
        <trans-unit id="2f95c182cf5fa2e13ce2622defe8af992d765313" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s try again with the default setting:</source>
          <target state="translated">让我们使用默认设置再试一次：</target>
        </trans-unit>
        <trans-unit id="c3a6b9996c4370e6a8670703084aa093c6face20" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s use it to tokenize and count the word occurrences of a minimalistic corpus of text documents:</source>
          <target state="translated">让我们用它来标记和计算文本文档的极简语料库中的单词出现次数：</target>
        </trans-unit>
        <trans-unit id="c77ba9dd4a1f63d6e3e47c6ccfdd6637c48fa284" translate="yes" xml:space="preserve">
          <source>Let&amp;rsquo;s visually compare the cross validation behavior for many scikit-learn cross-validation objects. Below we will loop through several common cross-validation objects, visualizing the behavior of each.</source>
          <target state="translated">让我们直观地比较许多scikit-learn交叉验证对象的交叉验证行为。下面，我们将遍历几个常见的交叉验证对象，以可视化方式查看每个对象的行为。</target>
        </trans-unit>
        <trans-unit id="6c69807d4e78cfb8da9f8e8c21f378d88124782a" translate="yes" xml:space="preserve">
          <source>Level of verbosity.</source>
          <target state="translated">啰嗦程度。</target>
        </trans-unit>
        <trans-unit id="ee9267aef527ceaeed70f092da783571e1b2536d" translate="yes" xml:space="preserve">
          <source>Libsvm GUI</source>
          <target state="translated">Libsvm GUI</target>
        </trans-unit>
        <trans-unit id="538c09161b8497f998404cafc34964ed3a445575" translate="yes" xml:space="preserve">
          <source>Licensed under the 3-clause BSD License.</source>
          <target state="translated">采用3-clause BSD授权。</target>
        </trans-unit>
        <trans-unit id="d119b02c417272fad54f56fb5c480a5a866c4e2e" translate="yes" xml:space="preserve">
          <source>Lichman, M. (2013). UCI Machine Learning Repository [&lt;a href=&quot;http://archive.ics.uci.edu/ml&quot;&gt;http://archive.ics.uci.edu/ml&lt;/a&gt;]. Irvine, CA: University of California, School of Information and Computer Science.</source>
          <target state="translated">Lichman，M.（2013年）。UCI机器学习存储库[ &lt;a href=&quot;http://archive.ics.uci.edu/ml&quot;&gt;http://archive.ics.uci.edu/ml&lt;/a&gt; ]。加州尔湾市：加利福尼亚大学信息与计算机科学学院。</target>
        </trans-unit>
        <trans-unit id="4a2cabe35d47f4d173451a3dc4bce594fc9e8434" translate="yes" xml:space="preserve">
          <source>Like &lt;a href=&quot;tree#tree&quot;&gt;decision trees&lt;/a&gt;, forests of trees also extend to &lt;a href=&quot;tree#tree-multioutput&quot;&gt;multi-output problems&lt;/a&gt; (if Y is an array of size &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt;).</source>
          <target state="translated">像&lt;a href=&quot;tree#tree&quot;&gt;决策树&lt;/a&gt;一样，树林也扩展到&lt;a href=&quot;tree#tree-multioutput&quot;&gt;多输出问题&lt;/a&gt;（如果Y是大小 &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt; 的数组）。</target>
        </trans-unit>
        <trans-unit id="f8c94a1d14fd76e50a18d8c5112493b7c41a75b8" translate="yes" xml:space="preserve">
          <source>Like &lt;code&gt;Pipeline&lt;/code&gt;, individual steps may be replaced using &lt;code&gt;set_params&lt;/code&gt;, and ignored by setting to &lt;code&gt;'drop'&lt;/code&gt;:</source>
          <target state="translated">与 &lt;code&gt;Pipeline&lt;/code&gt; 一样，可以使用 &lt;code&gt;set_params&lt;/code&gt; 替换各个步骤，并通过将其设置为 &lt;code&gt;'drop'&lt;/code&gt; 来忽略它们：</target>
        </trans-unit>
        <trans-unit id="09c07eeb7023cd495f9b67aa8c5832e9de0a3634" translate="yes" xml:space="preserve">
          <source>Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.</source>
          <target state="translated">和MultinomialNB一样,这个分类器也适用于离散数据。不同的是,MultinomialNB适用于发生次数,而BernoulliNB则是针对二进制/boolean特征设计的。</target>
        </trans-unit>
        <trans-unit id="33e640491ab99ccee8501c0c94b8deb9f93a420b" translate="yes" xml:space="preserve">
          <source>Like fit(X) followed by transform(X), but does not require materializing X in memory.</source>
          <target state="translated">类似于fit(X)之后的transform(X),但不需要在内存中实现X的实体化。</target>
        </trans-unit>
        <trans-unit id="07a7d71492c9370f4c5f214183352ca2f48a9590" translate="yes" xml:space="preserve">
          <source>Like in Pipeline and FeatureUnion, this allows the transformer and its parameters to be set using &lt;code&gt;set_params&lt;/code&gt; and searched in grid search.</source>
          <target state="translated">像在Pipeline和FeatureUnion中一样，这允许使用 &lt;code&gt;set_params&lt;/code&gt; 设置变压器及其参数，并在网格搜索中进行搜索。</target>
        </trans-unit>
        <trans-unit id="ace16ab25f8f0e2cb278ad02989604150a81258c" translate="yes" xml:space="preserve">
          <source>Like pipelines, feature unions have a shorthand constructor called &lt;a href=&quot;generated/sklearn.pipeline.make_union#sklearn.pipeline.make_union&quot;&gt;&lt;code&gt;make_union&lt;/code&gt;&lt;/a&gt; that does not require explicit naming of the components.</source>
          <target state="translated">像管道一样，要素联合具有一个名为&lt;a href=&quot;generated/sklearn.pipeline.make_union#sklearn.pipeline.make_union&quot;&gt; &lt;code&gt;make_union&lt;/code&gt; &lt;/a&gt;的简写构造函数，该构造函数不需要显式命名组件。</target>
        </trans-unit>
        <trans-unit id="29685b73b0fbefa3dc8a3378779801b7f7266cf2" translate="yes" xml:space="preserve">
          <source>Like scalers, &lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt;&lt;code&gt;QuantileTransformer&lt;/code&gt;&lt;/a&gt; puts all features into the same, known range or distribution. However, by performing a rank transformation, it smooths out unusual distributions and is less influenced by outliers than scaling methods. It does, however, distort correlations and distances within and across features.</source>
          <target state="translated">与缩放器一样，&lt;a href=&quot;generated/sklearn.preprocessing.quantiletransformer#sklearn.preprocessing.QuantileTransformer&quot;&gt; &lt;code&gt;QuantileTransformer&lt;/code&gt; &lt;/a&gt;会将所有功能置于相同的已知范围或分布中。但是，通过执行秩变换，它可以平滑异常分布，并且与缩放方法相比，不受异常值的影响。但是，它确实扭曲了要素内部和要素之间的相关性和距离。</target>
        </trans-unit>
        <trans-unit id="0ae4ed5af04ee97eab148462e283fb7149bd9d04" translate="yes" xml:space="preserve">
          <source>Limit in bytes of the size of the cache.</source>
          <target state="translated">缓存大小的字节数限制。</target>
        </trans-unit>
        <trans-unit id="bbd76c46a461ce6867ca433ec8697501cc65b137" translate="yes" xml:space="preserve">
          <source>Limiting distance of neighbors to return. (default is the value passed to the constructor).</source>
          <target state="translated">限制返回的邻居的距离。(默认是传递给构造函数的值)。</target>
        </trans-unit>
        <trans-unit id="62c917554a7197d63486db913ca90de577c0bfe0" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis</source>
          <target state="translated">线性判别分析</target>
        </trans-unit>
        <trans-unit id="719a12bbe391db4f9a1b1f0f22d958d133e79356" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) and Quadratic Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt;&lt;code&gt;discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively.</source>
          <target state="translated">线性判别分析（&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt;）和二次判别分析（&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt; &lt;code&gt;discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt;）是两个经典分类器，顾名思义，它们分别是线性决策面和二次决策面。</target>
        </trans-unit>
        <trans-unit id="e36f5257c349ab3d8389a5027fa29765ef7a78a4" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance &lt;em&gt;between classes&lt;/em&gt;. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.</source>
          <target state="translated">线性判别分析（LDA）试图识别出&lt;em&gt;类别之间&lt;/em&gt;差异最大的属性。尤其是，与PCA相比，LDA是使用已知类别标签的受监督方法。</target>
        </trans-unit>
        <trans-unit id="02924b985796944d65c857ba377ee96748a5fefe" translate="yes" xml:space="preserve">
          <source>Linear Discriminant Analysis and Quadratic Discriminant Analysis</source>
          <target state="translated">线性判别分析和二次判别分析。</target>
        </trans-unit>
        <trans-unit id="fe99070400d8a366d4438afb34b3817ed643e76c" translate="yes" xml:space="preserve">
          <source>Linear Model trained with L1 prior as regularizer (aka the Lasso)</source>
          <target state="translated">以L1先验作为正则器(又称Lasso)训练的线性模型。</target>
        </trans-unit>
        <trans-unit id="b4819d272193c458d14d3c2a02b6439edb693339" translate="yes" xml:space="preserve">
          <source>Linear Regression Example</source>
          <target state="translated">线性回归实例</target>
        </trans-unit>
        <trans-unit id="85494d31f5cd31cf05c6e37284f8e968283c0002" translate="yes" xml:space="preserve">
          <source>Linear SVC is not a probabilistic classifier by default but it has a built-in calibration option enabled in this example (&lt;code&gt;probability=True&lt;/code&gt;).</source>
          <target state="translated">线性SVC在默认情况下不是概率分类器，但在此示例中启用了内置校准选项（ &lt;code&gt;probability=True&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="e97d7a71e4408e1f570cb8d2ee92b68f661724af" translate="yes" xml:space="preserve">
          <source>Linear SVMs</source>
          <target state="translated">线性SVMs</target>
        </trans-unit>
        <trans-unit id="73af0f0fe2656e7c704e9d2782f72d490054905e" translate="yes" xml:space="preserve">
          <source>Linear Sum - A n-dimensional vector holding the sum of all samples</source>
          <target state="translated">线性和-一个n维向量,包含所有样本的总和。</target>
        </trans-unit>
        <trans-unit id="1cd7978197df4491cb006d18687f0ce787689e06" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Classification (&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;LinearSVC&lt;/code&gt;&lt;/a&gt;) shows an even more sigmoid curve as the RandomForestClassifier, which is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana &lt;a href=&quot;#id4&quot; id=&quot;id3&quot;&gt;[4]&lt;/a&gt;), which focus on hard samples that are close to the decision boundary (the support vectors).</source>
          <target state="translated">线性支持向量分类（&lt;a href=&quot;generated/sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;LinearSVC&lt;/code&gt; &lt;/a&gt;）与RandomForestClassifier相比，显示出更大的S型曲线，这是最大利润率方法（比较Niculescu-Mizil和Caruana &lt;a href=&quot;#id4&quot; id=&quot;id3&quot;&gt;[4]&lt;/a&gt;）的典型方法，该方法侧重于接近决策边界的硬样本（支持向量）。</target>
        </trans-unit>
        <trans-unit id="88aaad048f30298d89bc0519c1e6f4cfbb7c20ea" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Classification.</source>
          <target state="translated">线性支持向量分类。</target>
        </trans-unit>
        <trans-unit id="4669e7bb12c975a34b6d592ccfe985850a9e31eb" translate="yes" xml:space="preserve">
          <source>Linear Support Vector Regression.</source>
          <target state="translated">线性支持向量回归。</target>
        </trans-unit>
        <trans-unit id="299f04ebeb7ad11bec6b5498c6b639ccade4023d" translate="yes" xml:space="preserve">
          <source>Linear and Quadratic Discriminant Analysis with covariance ellipsoid</source>
          <target state="translated">带有协方差椭圆体的线性和四元判别分析方法</target>
        </trans-unit>
        <trans-unit id="c0463594ed874e4d015c682e8a6395a05e3fbd8b" translate="yes" xml:space="preserve">
          <source>Linear classifiers (SVM, logistic regression, a.o.) with SGD training.</source>
          <target state="translated">线性分类器(SVM、逻辑回归、a.o.)与SGD训练。</target>
        </trans-unit>
        <trans-unit id="fa82faf2d530b479b3e87ec39c80fc313d729e93" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of centered data, keeping only the most significant singular vectors to project the data to a lower dimensional space.</source>
          <target state="translated">利用中心数据的奇异值分解进行线性降维,只保留最重要的奇异向量,将数据投射到低维空间。</target>
        </trans-unit>
        <trans-unit id="9db7130b75e27bc47e2764b6ec7d1ce03bb7f92f" translate="yes" xml:space="preserve">
          <source>Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space.</source>
          <target state="translated">利用数据的奇异值分解将其投射到一个较低维度的空间,进行线性降维。</target>
        </trans-unit>
        <trans-unit id="212b70af3cba5b501136f7c4f46821ce9f54ad31" translate="yes" xml:space="preserve">
          <source>Linear kernel (&lt;code&gt;kernel = 'linear'&lt;/code&gt;)</source>
          <target state="translated">线性核（ &lt;code&gt;kernel = 'linear'&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="1196f0388e6edcd3bda2236746717385556b159a" translate="yes" xml:space="preserve">
          <source>Linear least squares with l2 regularization.</source>
          <target state="translated">l2正则化的线性最小二乘法。</target>
        </trans-unit>
        <trans-unit id="0663410286eb390a6a91a4885ecdb0348930bc50" translate="yes" xml:space="preserve">
          <source>Linear model fitted by minimizing a regularized empirical loss with SGD</source>
          <target state="translated">通过用SGD最小化正则化经验损失来拟合线性模型。</target>
        </trans-unit>
        <trans-unit id="8d6556caff9af87efd1e0ccffe2463b6a45189f7" translate="yes" xml:space="preserve">
          <source>Linear model for testing the individual effect of each of many regressors. This is a scoring function to be used in a feature selection procedure, not a free standing feature selection procedure.</source>
          <target state="translated">线性模型,用于检验众多回归者中每个回归者的个体效应。这是一个在特征选择程序中使用的评分函数,而不是一个自由的特征选择程序。</target>
        </trans-unit>
        <trans-unit id="8f05719b5c26a33c08ae54e21caa15631a4bbbf1" translate="yes" xml:space="preserve">
          <source>Linear model: from regression to sparsity</source>
          <target state="translated">线性模型:从回归到稀疏性。</target>
        </trans-unit>
        <trans-unit id="2c94cc16a66b49675f2acef482a0fbcd40d606ee" translate="yes" xml:space="preserve">
          <source>Linear models: \(y = X\beta + \epsilon\)</source>
          <target state="translated">线性模型。\(y=Xbeta+epsilon)</target>
        </trans-unit>
        <trans-unit id="b501f602569674c31fc384f2cd7a29bcf6c1ce1f" translate="yes" xml:space="preserve">
          <source>Linear regression</source>
          <target state="translated">线性回归</target>
        </trans-unit>
        <trans-unit id="d8f88b232d41c327138bbda59458fa5fc4086fff" translate="yes" xml:space="preserve">
          <source>Linear regression model that is robust to outliers.</source>
          <target state="translated">线性回归模型,对离群值是稳健的。</target>
        </trans-unit>
        <trans-unit id="597ff76dcbb7bc322f194ba001977a736c193c2d" translate="yes" xml:space="preserve">
          <source>Linear regression with combined L1 and L2 priors as regularizer.</source>
          <target state="translated">以L1和L2组合前导为正则化的线性回归。</target>
        </trans-unit>
        <trans-unit id="0a2d386e0774637a1788b00b4abdb8b2c6c38c74" translate="yes" xml:space="preserve">
          <source>Linear ridge regression.</source>
          <target state="translated">线性山脊回归。</target>
        </trans-unit>
        <trans-unit id="c7ed3fbb6680836b95c3db482cfaf054c37a8419" translate="yes" xml:space="preserve">
          <source>List containing train-test split of inputs.</source>
          <target state="translated">包含输入的训练-测试分割的列表。</target>
        </trans-unit>
        <trans-unit id="7946c78611ea79ca25491c94f60dac5182c68016" translate="yes" xml:space="preserve">
          <source>List of (name, class), where &lt;code&gt;name&lt;/code&gt; is the class name as string and &lt;code&gt;class&lt;/code&gt; is the actuall type of the class.</source>
          <target state="translated">（名称，类别）的列表，其中 &lt;code&gt;name&lt;/code&gt; 是作为字符串的类别名称，而 &lt;code&gt;class&lt;/code&gt; 是类别的实际类型。</target>
        </trans-unit>
        <trans-unit id="01f72260e79a828ac37c6e1b27f0158a5c017639" translate="yes" xml:space="preserve">
          <source>List of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an estimator.</source>
          <target state="translated">连锁的(名称,变换)元组(实现拟合/变换)列表,按照它们连锁的顺序,最后一个对象是估计器。</target>
        </trans-unit>
        <trans-unit id="da3552a00ac25869a883c68bd3a0b9b483a759ac" translate="yes" xml:space="preserve">
          <source>List of (name, transformer, column(s)) tuples specifying the transformer objects to be applied to subsets of the data.</source>
          <target state="translated">指定要应用于数据子集的变换对象的(名称、变换器、列)元组列表。</target>
        </trans-unit>
        <trans-unit id="9ce9067b559ab6542ebc584f224960b4d8e01fb3" translate="yes" xml:space="preserve">
          <source>List of &lt;code&gt;n_features&lt;/code&gt;-dimensional data points. Each row corresponds to a single data point.</source>
          <target state="translated">&lt;code&gt;n_features&lt;/code&gt; 维数据点列表。每行对应一个数据点。</target>
        </trans-unit>
        <trans-unit id="5f38bb9ffb369276ed25fb7c04fb0e0e029823d8" translate="yes" xml:space="preserve">
          <source>List of all the classes that can possibly appear in the y vector.</source>
          <target state="translated">y向量中可能出现的所有类的列表。</target>
        </trans-unit>
        <trans-unit id="7cd6d854280958549421b40e7d622e1782f63df4" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If &lt;code&gt;None&lt;/code&gt; alphas are set automatically</source>
          <target state="translated">用于计算模型的Alpha列表。如果 &lt;code&gt;None&lt;/code&gt; 自动设置Alpha</target>
        </trans-unit>
        <trans-unit id="917b5a956108e84a4893edaf20f4507c6507d0e2" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If None alphas are set automatically</source>
          <target state="translated">计算模型的字母列表。如果None,则自动设置Alphas。</target>
        </trans-unit>
        <trans-unit id="d057f35a68cef6d291f5ea686ce0f4438a6a2951" translate="yes" xml:space="preserve">
          <source>List of alphas where to compute the models. If not provided, set automatically.</source>
          <target state="translated">计算模型的字母列表。如果没有提供,则自动设置。</target>
        </trans-unit>
        <trans-unit id="fb8d4641f5ca2701f801733b19cf7bb77974f371" translate="yes" xml:space="preserve">
          <source>List of arrays of terms.</source>
          <target state="translated">术语数组列表。</target>
        </trans-unit>
        <trans-unit id="6ecedd8bbbc6137125014e8bb7a429cbcef11be8" translate="yes" xml:space="preserve">
          <source>List of built-in kernels.</source>
          <target state="translated">内置内核的列表。</target>
        </trans-unit>
        <trans-unit id="36c7ba17f19f78b4b0b98a1a27cecbfd22dc65e4" translate="yes" xml:space="preserve">
          <source>List of coefficients for the Logistic Regression model. If fit_intercept is set to True then the second dimension will be n_features + 1, where the last item represents the intercept. For &lt;code&gt;multiclass='multinomial'&lt;/code&gt;, the shape is (n_classes, n_cs, n_features) or (n_classes, n_cs, n_features + 1).</source>
          <target state="translated">Logistic回归模型的系数列表。如果fit_intercept设置为True，则第二维将为n_features +1，其中最后一项表示截距。对于 &lt;code&gt;multiclass='multinomial'&lt;/code&gt; ，形状为（n_classes，n_cs，n_features）或（n_classes，n_cs，n_features +1）。</target>
        </trans-unit>
        <trans-unit id="568d5fc554d78a8c3f420990686843b1d52522c9" translate="yes" xml:space="preserve">
          <source>List of labels to index the matrix. This may be used to reorder or select a subset of labels. If none is given, those that appear at least once in &lt;code&gt;y_true&lt;/code&gt; or &lt;code&gt;y_pred&lt;/code&gt; are used in sorted order.</source>
          <target state="translated">索引矩阵的标签列表。这可用于重新排序或选择标签的子集。如果没有给出， &lt;code&gt;y_true&lt;/code&gt; 或 &lt;code&gt;y_pred&lt;/code&gt; 至少出现一次的那些将按排序顺序使用。</target>
        </trans-unit>
        <trans-unit id="4904457db6e3ad315971a386c35727cdd591b70f" translate="yes" xml:space="preserve">
          <source>List of labels to index the matrix. This may be used to select a subset of labels. If None, all labels that appear at least once in &lt;code&gt;y1&lt;/code&gt; or &lt;code&gt;y2&lt;/code&gt; are used.</source>
          <target state="translated">索引矩阵的标签列表。这可以用于选择标签的子集。如果为None，则使用在 &lt;code&gt;y1&lt;/code&gt; 或 &lt;code&gt;y2&lt;/code&gt; 中至少出现一次的所有标签。</target>
        </trans-unit>
        <trans-unit id="b841f355bd90388eac15a6584a26192e6c900c97" translate="yes" xml:space="preserve">
          <source>List of n_features-dimensional data points. Each row corresponds to a single data point.</source>
          <target state="translated">n_features-dimensional数据点的列表。每一行对应一个数据点。</target>
        </trans-unit>
        <trans-unit id="af1051d092002bc2f98d27cb1ada3b5cc2dacea1" translate="yes" xml:space="preserve">
          <source>List of n_features-dimensional data points. Each row corresponds to a single query.</source>
          <target state="translated">n_features-dimensional数据点的列表。每一行对应一个查询。</target>
        </trans-unit>
        <trans-unit id="85e7a2833a6b5505d28e95f0c1116dee51aa01e1" translate="yes" xml:space="preserve">
          <source>List of objects to ensure sliceability.</source>
          <target state="translated">确保可分片的对象清单。</target>
        </trans-unit>
        <trans-unit id="5538dc428bf1dd702d4666daf2c6801367c4f065" translate="yes" xml:space="preserve">
          <source>List of sample weights attached to the data X.</source>
          <target state="translated">附在数据X上的样本权重列表。</target>
        </trans-unit>
        <trans-unit id="af4d88e1f955adfe14752a1cab15db410dc25046" translate="yes" xml:space="preserve">
          <source>List of samples.</source>
          <target state="translated">样本清单;</target>
        </trans-unit>
        <trans-unit id="9fa149a90ccae2cfe066dfb859bf8a7c95ef01ca" translate="yes" xml:space="preserve">
          <source>List of transformer objects to be applied to the data. The first half of each tuple is the name of the transformer.</source>
          <target state="translated">要应用于数据的变换器对象的列表。每个元组的前半部分是变换器的名称。</target>
        </trans-unit>
        <trans-unit id="f2f499a9d9cf5fba3b5aa16bff4e7ad9f538a51f" translate="yes" xml:space="preserve">
          <source>List of values for the regularization parameter or integer specifying the number of regularization parameters that should be used. In this case, the parameters will be chosen in a logarithmic scale between 1e-4 and 1e4.</source>
          <target state="translated">正则化参数的值列表或指定应使用的正则化参数数量的整数。在这种情况下,参数将在1e-4和1e-4之间按对数比例选择。</target>
        </trans-unit>
        <trans-unit id="d742bd356ab53d1131907c9ca41e9f89956bc677" translate="yes" xml:space="preserve">
          <source>List of weighting type to calculate the score. None means no weighted; &amp;ldquo;linear&amp;rdquo; means linear weighted; &amp;ldquo;quadratic&amp;rdquo; means quadratic weighted.</source>
          <target state="translated">计权类型列表以计算分数。没有表示没有加权；&amp;ldquo;线性&amp;rdquo;是指线性加权；&amp;ldquo;二次方&amp;rdquo;是指二次加权。</target>
        </trans-unit>
        <trans-unit id="ccaadae3fd2b8d525242b8298319bebc15b1d7f7" translate="yes" xml:space="preserve">
          <source>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. &amp;ldquo;Isolation forest.&amp;rdquo; Data Mining, 2008. ICDM&amp;lsquo;08. Eighth IEEE International Conference on.</source>
          <target state="translated">刘飞飞，丁婷，启明和周志华。&amp;ldquo;隔离林。&amp;rdquo; 数据挖掘，2008年。ICDM'08。第八届IEEE国际会议。</target>
        </trans-unit>
        <trans-unit id="8d858831be3c025b5261ad0994fdd43e36106d2f" translate="yes" xml:space="preserve">
          <source>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. &amp;ldquo;Isolation-based anomaly detection.&amp;rdquo; ACM Transactions on Knowledge Discovery from Data (TKDD) 6.1 (2012): 3.</source>
          <target state="translated">刘飞飞，丁婷，启明和周志华。&amp;ldquo;基于隔离的异常检测。&amp;rdquo; ACM数据知识发现交易（TKDD）6.1（2012）：3。</target>
        </trans-unit>
        <trans-unit id="bc1c89a3655919cbe107b23bf70fdaef2d59b7e4" translate="yes" xml:space="preserve">
          <source>Load a datasets as downloaded from &lt;a href=&quot;http://mlcomp.org&quot;&gt;http://mlcomp.org&lt;/a&gt;</source>
          <target state="translated">加载从&lt;a href=&quot;http://mlcomp.org&quot;&gt;http://mlcomp.org&lt;/a&gt;下载的数据集</target>
        </trans-unit>
        <trans-unit id="10fbb828ccf5ef978744b601a27eefff85b82acd" translate="yes" xml:space="preserve">
          <source>Load and return the boston house-prices dataset (regression).</source>
          <target state="translated">加载并返回波士顿房价数据集(回归)。</target>
        </trans-unit>
        <trans-unit id="f0b03288037dddab02ba1bf0d814f5cc8cf63088" translate="yes" xml:space="preserve">
          <source>Load and return the breast cancer wisconsin dataset (classification).</source>
          <target state="translated">加载并返回乳腺癌威斯康星州数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="fb9c782009d54032f572c4b8eb05f6ff3c69b6ee" translate="yes" xml:space="preserve">
          <source>Load and return the diabetes dataset (regression).</source>
          <target state="translated">加载并返回糖尿病数据集(回归)。</target>
        </trans-unit>
        <trans-unit id="5ee0c3f160bd1db558fab50ff07fd2d60e875939" translate="yes" xml:space="preserve">
          <source>Load and return the digits dataset (classification).</source>
          <target state="translated">加载并返回数字数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="91627f9a236f04bf8e67f696e6012e55dde096ca" translate="yes" xml:space="preserve">
          <source>Load and return the iris dataset (classification).</source>
          <target state="translated">加载并返回虹膜数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="08308ecd69078eb0533ddcbcb38611925dd58ae7" translate="yes" xml:space="preserve">
          <source>Load and return the linnerud dataset (multivariate regression).</source>
          <target state="translated">加载并返回linnerud数据集(多变量回归)。</target>
        </trans-unit>
        <trans-unit id="0a61d81b3e38cd33952ad8e4ab4da4e0afb0ac23" translate="yes" xml:space="preserve">
          <source>Load and return the wine dataset (classification).</source>
          <target state="translated">加载并返回葡萄酒数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="907ca9fec180a2f563a6eb0b2c208dd89483dfe5" translate="yes" xml:space="preserve">
          <source>Load dataset from multiple files in SVMlight format</source>
          <target state="translated">从多个文件中加载SVMlight格式的数据集。</target>
        </trans-unit>
        <trans-unit id="e0287d019fcfe4320ef71958ec3623d393a07d68" translate="yes" xml:space="preserve">
          <source>Load datasets in the svmlight / libsvm format into sparse CSR matrix</source>
          <target state="translated">将svmlight/libsvm格式的数据集加载到稀疏的CSR矩阵中。</target>
        </trans-unit>
        <trans-unit id="15df99bbc404778e529956fb3833b1f8b300577d" translate="yes" xml:space="preserve">
          <source>Load sample images for image manipulation.</source>
          <target state="translated">加载样本图像进行图像处理。</target>
        </trans-unit>
        <trans-unit id="93b606a5680687306536f14272c219f02caf9a74" translate="yes" xml:space="preserve">
          <source>Load text files with categories as subfolder names.</source>
          <target state="translated">装入以类别为子文件夹名的文本文件。</target>
        </trans-unit>
        <trans-unit id="ada1ba97e9c53b7b56715b1a2824f0ae676a78c6" translate="yes" xml:space="preserve">
          <source>Load the 20 newsgroups dataset and vectorize it into token counts (classification).</source>
          <target state="translated">加载20个新闻组数据集,并将其矢量化为标记计数(分类)。</target>
        </trans-unit>
        <trans-unit id="4f7a062fa00aaafd76d451b474abbba6455b18d1" translate="yes" xml:space="preserve">
          <source>Load the California housing dataset (regression).</source>
          <target state="translated">加载加州住房数据集(回归)。</target>
        </trans-unit>
        <trans-unit id="d369acbb02d6ae84bdcebcaf52c16540c4d5f177" translate="yes" xml:space="preserve">
          <source>Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).</source>
          <target state="translated">加载标注的野生面孔(LFW)对数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="fdf290fe8f8a97ef39a92df3e1f665ba9f5137b7" translate="yes" xml:space="preserve">
          <source>Load the Labeled Faces in the Wild (LFW) people dataset (classification).</source>
          <target state="translated">加载标注在野外的面孔(LFW)人数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="c1d9dfefbd2137b268a0489f71dee7b704510f30" translate="yes" xml:space="preserve">
          <source>Load the Olivetti faces data-set from AT&amp;amp;T (classification).</source>
          <target state="translated">从AT＆T（分类）中加载Olivetti人脸数据集。</target>
        </trans-unit>
        <trans-unit id="2772bcfa51d7f467cdc1ff56dd2a38098daf99c8" translate="yes" xml:space="preserve">
          <source>Load the RCV1 multilabel dataset (classification).</source>
          <target state="translated">加载RCV1多标签数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="b34ac8eb475e3d0c92e532ea1f16cafea2826dba" translate="yes" xml:space="preserve">
          <source>Load the covertype dataset (classification).</source>
          <target state="translated">加载覆盖类型数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="54322fa6d75ea033036ee5315e01f5a9e265e0ca" translate="yes" xml:space="preserve">
          <source>Load the filenames and data from the 20 newsgroups dataset (classification).</source>
          <target state="translated">从20个新闻组数据集中加载文件名和数据(分类)。</target>
        </trans-unit>
        <trans-unit id="ae3c786b5593f01e176137f6a4960d769f8b9d22" translate="yes" xml:space="preserve">
          <source>Load the kddcup99 dataset (classification).</source>
          <target state="translated">加载kddcup99数据集(分类)。</target>
        </trans-unit>
        <trans-unit id="820329ef76c355bc57213e87e726caebf3ec8e17" translate="yes" xml:space="preserve">
          <source>Load the numpy array of a single sample image</source>
          <target state="translated">加载单个样本图像的numpy数组。</target>
        </trans-unit>
        <trans-unit id="6565057c8bbe701655d34466bc255155c3ea2c6e" translate="yes" xml:space="preserve">
          <source>Loader for species distribution dataset from Phillips et.</source>
          <target state="translated">菲利普斯等人的物种分布数据集加载器。</target>
        </trans-unit>
        <trans-unit id="00912c83de18e685a34ddbd42e1354c697eb14e0" translate="yes" xml:space="preserve">
          <source>Loader for species distribution dataset from Phillips et. al. (2006)</source>
          <target state="translated">菲利普斯等人的物种分布数据集加载器(2006年)。(2006)</target>
        </trans-unit>
        <trans-unit id="4f514b04ed6b877534da140af8e12cab5016f713" translate="yes" xml:space="preserve">
          <source>Loaders</source>
          <target state="translated">Loaders</target>
        </trans-unit>
        <trans-unit id="1d603b233f1badee343cd4d051b0c74346bf8ab5" translate="yes" xml:space="preserve">
          <source>Loading an example dataset</source>
          <target state="translated">加载一个示例数据集</target>
        </trans-unit>
        <trans-unit id="afb9453c6f5c0750a61be0390918061037ab3605" translate="yes" xml:space="preserve">
          <source>Loading from external datasets</source>
          <target state="translated">从外部数据集加载</target>
        </trans-unit>
        <trans-unit id="b4240e57d982043f1f905f33f107714b1056ff0f" translate="yes" xml:space="preserve">
          <source>Loading the 20 newsgroups dataset</source>
          <target state="translated">加载20个新闻组数据集。</target>
        </trans-unit>
        <trans-unit id="bf453b7e00694519c6d048cddce89c9acdc80f61" translate="yes" xml:space="preserve">
          <source>Loads both, &lt;code&gt;china&lt;/code&gt; and &lt;code&gt;flower&lt;/code&gt;.</source>
          <target state="translated">加载 &lt;code&gt;china&lt;/code&gt; 和 &lt;code&gt;flower&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="5a8b86a7fef7215f7de926bc65cb224b10c3ccba" translate="yes" xml:space="preserve">
          <source>Locally Linear Embedding</source>
          <target state="translated">本地线性嵌入</target>
        </trans-unit>
        <trans-unit id="f71746cee5cf3673e7e527aaea93ab0ac960ab66" translate="yes" xml:space="preserve">
          <source>Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding.</source>
          <target state="translated">局部线性嵌入(LLE)寻求数据的低维投影,以保持局部邻域内的距离。它可以被认为是一系列的局部主成分分析,这些分析在全球范围内进行比较,以找到最佳的非线性嵌入。</target>
        </trans-unit>
        <trans-unit id="ba721026e6725be51f569c81e87377b42c664dd5" translate="yes" xml:space="preserve">
          <source>Locally linear embedding can be performed with function &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt;&lt;code&gt;locally_linear_embedding&lt;/code&gt;&lt;/a&gt; or its object-oriented counterpart &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以使用&lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt; &lt;code&gt;locally_linear_embedding&lt;/code&gt; &lt;/a&gt;函数或其面向对象的对等&lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt;执行局部线性嵌入。</target>
        </trans-unit>
        <trans-unit id="120996393a2755aae459a0342f6a159574a0420b" translate="yes" xml:space="preserve">
          <source>Log likelihood of the Gaussian mixture given X.</source>
          <target state="translated">给定X的高斯混合物的对数似然。</target>
        </trans-unit>
        <trans-unit id="10dac5cbd2ef695e498b42bff8cc166d8d6c8a26" translate="yes" xml:space="preserve">
          <source>Log loss is undefined for p=0 or p=1, so probabilities are clipped to max(eps, min(1 - eps, p)).</source>
          <target state="translated">p=0或p=1时,对数损失是未定义的,所以概率被削为max(eps,min(1-eps,p))。</target>
        </trans-unit>
        <trans-unit id="8742f15984971d3e598576d7cde59958d4df18a1" translate="yes" xml:space="preserve">
          <source>Log loss, aka logistic loss or cross-entropy loss.</source>
          <target state="translated">对数损失,又称逻辑损失或交叉熵损失。</target>
        </trans-unit>
        <trans-unit id="3332ed47adb99d618a3191081bd1b56f7df887fc" translate="yes" xml:space="preserve">
          <source>Log loss, also called logistic regression loss or cross-entropy loss, is defined on probability estimates. It is commonly used in (multinomial) logistic regression and neural networks, as well as in some variants of expectation-maximization, and can be used to evaluate the probability outputs (&lt;code&gt;predict_proba&lt;/code&gt;) of a classifier instead of its discrete predictions.</source>
          <target state="translated">对数损失（也称为逻辑回归损失或交叉熵损失）在概率估计中定义。它通常用于（多项式）Logistic回归和神经网络，以及期望最大化的某些变体中，并且可以用于评估分类器的概率输出（ &lt;code&gt;predict_proba&lt;/code&gt; ），而不是其离散预测。</target>
        </trans-unit>
        <trans-unit id="f8ceba0d5dd7df5e53e4d9ba0bfe4881369ef7f1" translate="yes" xml:space="preserve">
          <source>Log of probability estimates.</source>
          <target state="translated">概率估计的对数。</target>
        </trans-unit>
        <trans-unit id="b2dede1f561914a3bc83cf7a3f85dcff6bab8c76" translate="yes" xml:space="preserve">
          <source>Log probabilities of each data point in X.</source>
          <target state="translated">X中每个数据点的对数概率。</target>
        </trans-unit>
        <trans-unit id="ce21bba36fd356086ab08edfbf5461d606fc0046" translate="yes" xml:space="preserve">
          <source>Log probability of each class (smoothed).</source>
          <target state="translated">各类的对数概率(平滑化)。</target>
        </trans-unit>
        <trans-unit id="10521a3daec9ae1f69d9eb092c8ffd785e7a6414" translate="yes" xml:space="preserve">
          <source>Log-likelihood of each sample under the current model</source>
          <target state="translated">当前模型下每个样本的对数似然率</target>
        </trans-unit>
        <trans-unit id="9c1e8dc95e554810186fcd38490e4f1fe6e53c32" translate="yes" xml:space="preserve">
          <source>Log-likelihood score on left-out data across folds.</source>
          <target state="translated">对数似然率得分在左出数据上跨折。</target>
        </trans-unit>
        <trans-unit id="af6fc4d4c535e2fcc7787b2d2b354e641a5cdf07" translate="yes" xml:space="preserve">
          <source>Log-marginal likelihood of theta for training data.</source>
          <target state="translated">训练数据Theta的对数边际似然率。</target>
        </trans-unit>
        <trans-unit id="a79f6e0f430c7ecad68ae2bba39688851de03cfd" translate="yes" xml:space="preserve">
          <source>Log: Logistic Regression.</source>
          <target state="translated">Log:Logistic Regression.</target>
        </trans-unit>
        <trans-unit id="667a374e42016ea0491009bae949bbc3eb5a98fe" translate="yes" xml:space="preserve">
          <source>Logistic Regression (aka logit, MaxEnt) classifier.</source>
          <target state="translated">Logistic Regression(又名logit,MaxEnt)分类器。</target>
        </trans-unit>
        <trans-unit id="7553fecbacc2ab6c754b732dd2a40625b016efa3" translate="yes" xml:space="preserve">
          <source>Logistic Regression 3-class Classifier</source>
          <target state="translated">Logistic回归3类分类器</target>
        </trans-unit>
        <trans-unit id="67b9d1bed8ce4778bb74ee8f32cf37a9f88e56b4" translate="yes" xml:space="preserve">
          <source>Logistic Regression CV (aka logit, MaxEnt) classifier.</source>
          <target state="translated">Logistic Regression CV(又名logit,MaxEnt)分类器。</target>
        </trans-unit>
        <trans-unit id="4c4251ffdad99c44ab6ab03dc44e767ed73a387b" translate="yes" xml:space="preserve">
          <source>Logistic function</source>
          <target state="translated">Logistic函数</target>
        </trans-unit>
        <trans-unit id="2c0f1438d10823ae208574adad9979316ecf1f7d" translate="yes" xml:space="preserve">
          <source>Logistic regression on raw pixel values is presented for comparison. The example shows that the features extracted by the BernoulliRBM help improve the classification accuracy.</source>
          <target state="translated">提出了对原始像素值的Logistic回归进行比较。该例子表明,BernoulliRBM提取的特征有助于提高分类精度。</target>
        </trans-unit>
        <trans-unit id="f05fe21aed88fc82a5a0513559ae877673e205fc" translate="yes" xml:space="preserve">
          <source>Logistic regression with built-in cross validation</source>
          <target state="translated">内置交叉验证的Logistic回归</target>
        </trans-unit>
        <trans-unit id="d3b2957f5500f497ec4678d49dfe4396dcf43781" translate="yes" xml:space="preserve">
          <source>Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_function&quot;&gt;logistic function&lt;/a&gt;.</source>
          <target state="translated">逻辑回归尽管有其名称，但它是用于分类而非回归的线性模型。逻辑回归在文献中也称为对数回归，最大熵分类（MaxEnt）或对数线性分类器。在此模型中，使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_function&quot;&gt;logistic函数&lt;/a&gt;对描述单个试验可能结果的概率进行建模。</target>
        </trans-unit>
        <trans-unit id="de456a9443564fc60f026f7b3757765c6c521491" translate="yes" xml:space="preserve">
          <source>LogisticRegression returns well calibrated predictions as it directly optimizes log-loss. In contrast, the other methods return biased probabilities, with different biases per method:</source>
          <target state="translated">LogisticRegression由于直接优化了对数损失,所以会返回校准好的预测。相比之下,其他方法返回的概率是有偏差的,每个方法的偏差不同。</target>
        </trans-unit>
        <trans-unit id="83ec89bbbb1925d31612bf071115de8d555d9924" translate="yes" xml:space="preserve">
          <source>Longitude house block longitude</source>
          <target state="translated">经度房屋幢经度</target>
        </trans-unit>
        <trans-unit id="f2ebf0012d7d593bf1ef0d0a316102397c08a9f0" translate="yes" xml:space="preserve">
          <source>Low-level methods</source>
          <target state="translated">低级方法</target>
        </trans-unit>
        <trans-unit id="ea609f61be1ccbae7413cc55d9401ee01dff16e3" translate="yes" xml:space="preserve">
          <source>Lower bound value on the likelihood (of the training data with respect to the model) of the best fit of inference.</source>
          <target state="translated">推理的最佳拟合度的似然性(训练数据相对于模型的似然性)的下限值。</target>
        </trans-unit>
        <trans-unit id="af301438554e0ee8815f3548a50754545e52e051" translate="yes" xml:space="preserve">
          <source>Lower bound value on the log-likelihood (of the training data with respect to the model) of the best fit of EM.</source>
          <target state="translated">EM最佳拟合度的对数似然值(训练数据相对于模型)的下限值。</target>
        </trans-unit>
        <trans-unit id="4ada54abc98e483baeab7ae15def52027a7aae96" translate="yes" xml:space="preserve">
          <source>Lower-triangular Cholesky decomposition of the kernel in &lt;code&gt;X_train_&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;X_train_&lt;/code&gt; 中内核的下三角Cholesky分解</target>
        </trans-unit>
        <trans-unit id="b93b2eafc7fea724b9bcb08bb1fb9109b8d1d561" translate="yes" xml:space="preserve">
          <source>M. Bawa, T. Condie and P. Ganesan, &amp;ldquo;LSH Forest: Self-Tuning Indexes for Similarity Search&amp;rdquo;, WWW &amp;lsquo;05 Proceedings of the 14th international conference on World Wide Web, 651-660, 2005.</source>
          <target state="translated">M. Bawa，T。Condie和P. Ganesan，&amp;ldquo; LSH森林：相似性搜索的自调整索引&amp;rdquo;，第14届万维网国际会议的WWW '05会议记录，651-660，2005年。</target>
        </trans-unit>
        <trans-unit id="e8445854a0cf4ad63f8ee64cb2fc2359051f4c85" translate="yes" xml:space="preserve">
          <source>M. Dumont et al, &lt;a href=&quot;http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf&quot;&gt;Fast multi-class image annotation with random subwindows and multiple output randomized trees&lt;/a&gt;, International Conference on Computer Vision Theory and Applications 2009</source>
          <target state="translated">M. Dumont等人，&lt;a href=&quot;http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf&quot;&gt;带有随机子窗口和多个输出随机树的快速多类图像注释&lt;/a&gt;，国际计算机视觉理论与应用会议2009</target>
        </trans-unit>
        <trans-unit id="2422710e8cdc4f555670a3606a875134eadd99fe" translate="yes" xml:space="preserve">
          <source>M. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman, &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.5766&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;The Pascal Visual Object Classes (VOC) Challenge&lt;/a&gt;, IJCV 2010.</source>
          <target state="translated">M.Everingham，L.Van Gool，CKI Williams，J.Winn，A.Zisserman，&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.5766&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;《 Pascal视觉对象类（VOC）挑战》&lt;/a&gt;，IJCV 2010。</target>
        </trans-unit>
        <trans-unit id="aa09c5b3704d1cab7c2f9d80f35ab989ea05cba1" translate="yes" xml:space="preserve">
          <source>MAE output is non-negative floating point. The best value is 0.0.</source>
          <target state="translated">MAE输出为非负浮点数。最佳值为0.0。</target>
        </trans-unit>
        <trans-unit id="f4d1d18b18dbadb43dc94aafefb0919124514bb4" translate="yes" xml:space="preserve">
          <source>MEDV Median value of owner-occupied homes in $1000&amp;rsquo;s</source>
          <target state="translated">MEDV自有住房的中位数价值（以1000美元计）</target>
        </trans-unit>
        <trans-unit id="33379c640ef1bcb7b4dbc3ceb61d0f9854342e44" translate="yes" xml:space="preserve">
          <source>MKL</source>
          <target state="translated">MKL</target>
        </trans-unit>
        <trans-unit id="a8e1fd8b99167af6d3e02ac86d0a101dabaf0e42" translate="yes" xml:space="preserve">
          <source>MLP can fit a non-linear model to the training data. &lt;code&gt;clf.coefs_&lt;/code&gt; contains the weight matrices that constitute the model parameters:</source>
          <target state="translated">MLP可以将非线性模型拟合到训练数据。 &lt;code&gt;clf.coefs_&lt;/code&gt; 包含构成模型参数的权重矩阵：</target>
        </trans-unit>
        <trans-unit id="7fc5f2a7a15f6ccd1641b37c2fb96c6ce75018c2" translate="yes" xml:space="preserve">
          <source>MLP is sensitive to feature scaling.</source>
          <target state="translated">MLP对特征缩放很敏感。</target>
        </trans-unit>
        <trans-unit id="08431dee59de79a71b4718dbf6ee28e75fee38c3" translate="yes" xml:space="preserve">
          <source>MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.</source>
          <target state="translated">MLP需要调整一些超参数,如隐藏神经元数量、层数和迭代次数。</target>
        </trans-unit>
        <trans-unit id="e82dbcf8b443c94c79e54d7d53faa6f5db46762a" translate="yes" xml:space="preserve">
          <source>MLP trains on two arrays: array X of size (n_samples, n_features), which holds the training samples represented as floating point feature vectors; and array y of size (n_samples,), which holds the target values (class labels) for the training samples:</source>
          <target state="translated">MLP在两个数组上进行训练:大小为(n_samples,n_features)的数组X,存放以浮点特征向量表示的训练样本;大小为(n_samples,)的数组y,存放训练样本的目标值(类标签)。</target>
        </trans-unit>
        <trans-unit id="f0c27305c85163e665d40daa0f2ca2e458a2e63e" translate="yes" xml:space="preserve">
          <source>MLP trains using &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;Stochastic Gradient Descent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt;, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS&lt;/a&gt;. Stochastic Gradient Descent (SGD) updates parameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e.</source>
          <target state="translated">MLP使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;随机梯度下降&lt;/a&gt;，&lt;a href=&quot;http://arxiv.org/abs/1412.6980&quot;&gt;Adam&lt;/a&gt;或&lt;a href=&quot;https://en.wikipedia.org/wiki/Limited-memory_BFGS&quot;&gt;L-BFGS进行&lt;/a&gt;训练。随机梯度下降（SGD）使用损失函数相对于需要调整的参数的梯度来更新参数，即</target>
        </trans-unit>
        <trans-unit id="f27922032032bfc1325082b9b33d5f3a9228ddf6" translate="yes" xml:space="preserve">
          <source>MLP trains using Backpropagation. More precisely, it trains using some form of gradient descent and the gradients are calculated using Backpropagation. For classification, it minimizes the Cross-Entropy loss function, giving a vector of probability estimates \(P(y|x)\) per sample \(x\):</source>
          <target state="translated">MLP使用Backpropagation进行训练。更准确地说,它使用某种形式的梯度下降进行训练,梯度的计算使用Backpropagation。对于分类,它将交叉熵损失函数最小化,给出每个样本的概率估计矢量/(P(y|x)\)。</target>
        </trans-unit>
        <trans-unit id="c8c1d3b7c59691465cb0496f22bcb3604bca5a60" translate="yes" xml:space="preserve">
          <source>MLP uses different loss functions depending on the problem type. The loss function for classification is Cross-Entropy, which in binary case is given as,</source>
          <target state="translated">MLP根据问题类型的不同,使用不同的损失函数。分类的损失函数是Cross-Antropy,在二进制情况下,它被赋予:</target>
        </trans-unit>
        <trans-unit id="d03f0b750d6ad970862b8ceab4b82a667eb1bc44" translate="yes" xml:space="preserve">
          <source>MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.</source>
          <target state="translated">带有隐藏层的MLP具有非凸损失函数,其中存在一个以上的局部最小值。因此不同的随机权重初始化会导致不同的验证精度。</target>
        </trans-unit>
        <trans-unit id="14160d0f2e53b28f2f5c2a7702cb0510220c29b8" translate="yes" xml:space="preserve">
          <source>MLPClassifier trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters.</source>
          <target state="translated">MLPClassifier 迭代训练,因为在每个时间步骤中,损失函数与模型参数的部分导数被计算出来,以更新参数。</target>
        </trans-unit>
        <trans-unit id="8b27d0c0c6a8a44ae6f32f660e2bfb892d109024" translate="yes" xml:space="preserve">
          <source>MLPRegressor trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters.</source>
          <target state="translated">MLPRegressor 迭代训练,因为在每个时间步长计算损失函数相对于模型参数的部分导数以更新参数。</target>
        </trans-unit>
        <trans-unit id="8e70290c1fc16432a8f4b616e4fd6fbaa4abfbea" translate="yes" xml:space="preserve">
          <source>MNIST classfification using multinomial logistic + L1</source>
          <target state="translated">MNIST类化,采用多项逻辑+L1的方法</target>
        </trans-unit>
        <trans-unit id="25845da185fe3a02cb60c18fcf84202e8f31d1e7" translate="yes" xml:space="preserve">
          <source>Machine learning algorithms need data. Go to each &lt;code&gt;$TUTORIAL_HOME/data&lt;/code&gt; sub-folder and run the &lt;code&gt;fetch_data.py&lt;/code&gt; script from there (after having read them first).</source>
          <target state="translated">机器学习算法需要数据。转到每个 &lt;code&gt;$TUTORIAL_HOME/data&lt;/code&gt; 子文件夹，然后从那里运行 &lt;code&gt;fetch_data.py&lt;/code&gt; 脚本（首先阅读它们之后）。</target>
        </trans-unit>
        <trans-unit id="45f2bd27f62f0226a5b3177e6a59d79cc23fea68" translate="yes" xml:space="preserve">
          <source>Machine learning is about learning some properties of a data set and then testing those properties against another data set. A common practice in machine learning is to evaluate an algorithm by splitting a data set into two. We call one of those sets the &lt;strong&gt;training set&lt;/strong&gt;, on which we learn some properties; we call the other set the &lt;strong&gt;testing set&lt;/strong&gt;, on which we test the learned properties.</source>
          <target state="translated">机器学习是关于学习数据集的某些属性，然后针对另一个数据集测试这些属性。机器学习中的一种常见做法是通过将数据集分成两部分来评估算法。我们称其中一组为&lt;strong&gt;训练集&lt;/strong&gt;，在该&lt;strong&gt;训练集&lt;/strong&gt;上我们学习一些属性；我们将另一个集称为&lt;strong&gt;测试集&lt;/strong&gt;，在其上测试学习的属性。</target>
        </trans-unit>
        <trans-unit id="17dc705c260bdc393406dc006335656d8788b655" translate="yes" xml:space="preserve">
          <source>Machine learning: the problem setting</source>
          <target state="translated">机器学习:问题设置</target>
        </trans-unit>
        <trans-unit id="e6a69273199992ddfe41f469dda4cc1f6b79ceb0" translate="yes" xml:space="preserve">
          <source>Magnesium</source>
          <target state="translated">Magnesium</target>
        </trans-unit>
        <trans-unit id="2bb08573261ae718ebb52db49951821b64f9a80c" translate="yes" xml:space="preserve">
          <source>Magnesium:</source>
          <target state="translated">Magnesium:</target>
        </trans-unit>
        <trans-unit id="91059cae8d3b76142d7879b78c0a2ccaab268e7d" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances of the training set (on which &lt;code&gt;fit&lt;/code&gt; is called) observations.</source>
          <target state="translated">训练集（称为 &lt;code&gt;fit&lt;/code&gt; 度）观测值的马氏距离。</target>
        </trans-unit>
        <trans-unit id="d469f730cc4a1b58c5ef61209e446f59013c3c80" translate="yes" xml:space="preserve">
          <source>Mahalanobis distances to centers</source>
          <target state="translated">Mahalanobis到中心的距离</target>
        </trans-unit>
        <trans-unit id="6f98cc22ed52c0a1e40fae778fadcd35627c25b5" translate="yes" xml:space="preserve">
          <source>MahalanobisDistance</source>
          <target state="translated">MahalanobisDistance</target>
        </trans-unit>
        <trans-unit id="62bce9422ff2d14f69ab80a154510232fc8a9afd" translate="yes" xml:space="preserve">
          <source>Main</source>
          <target state="translated">Main</target>
        </trans-unit>
        <trans-unit id="8d6381188443dad8aa5d016fb4ec69dd96237200" translate="yes" xml:space="preserve">
          <source>Make a copy of input data.</source>
          <target state="translated">对输入数据进行复制。</target>
        </trans-unit>
        <trans-unit id="f11963f5d19078a49cfab3cc41da5922accd3330" translate="yes" xml:space="preserve">
          <source>Make a large circle containing a smaller circle in 2d.</source>
          <target state="translated">用2d做一个大圆包含一个小圆。</target>
        </trans-unit>
        <trans-unit id="1cce5fef6c99c293eee32e22f026e768f4b5d892" translate="yes" xml:space="preserve">
          <source>Make a scorer from a performance metric or loss function.</source>
          <target state="translated">从性能指标或损失函数中制作一个评分器。</target>
        </trans-unit>
        <trans-unit id="bdd3abd6a5ef3ffb2c4167fd4f1b6dc60d7a55bd" translate="yes" xml:space="preserve">
          <source>Make arrays indexable for cross-validation.</source>
          <target state="translated">使数组可以进行交叉验证的索引。</target>
        </trans-unit>
        <trans-unit id="2d28cad808149ac4eb3c924434a0979417f24a68" translate="yes" xml:space="preserve">
          <source>Make sure that X has a minimum number of samples in its first axis (rows for a 2D array).</source>
          <target state="translated">确保X在其第一轴上有一个最小的样本数(2D阵列的行)。</target>
        </trans-unit>
        <trans-unit id="b2b1ee415b35f3ec33d28dbc91a8479edeb8d71e" translate="yes" xml:space="preserve">
          <source>Make sure that array is 2D, square and symmetric.</source>
          <target state="translated">确保阵列是二维的、正方形的、对称的。</target>
        </trans-unit>
        <trans-unit id="6885424e5e7bfa46a7e7c7cb1bd5d6e804bbccd9" translate="yes" xml:space="preserve">
          <source>Make sure that the 2D array has some minimum number of features (columns). The default value of 1 rejects empty datasets. This check is only enforced when X has effectively 2 dimensions or is originally 1D and &lt;code&gt;ensure_2d&lt;/code&gt; is True. Setting to 0 disables this check.</source>
          <target state="translated">确保2D阵列具有最少数量的要素（列）。默认值1拒绝空数据集。仅当X有效具有2维或最初为1D并 &lt;code&gt;ensure_2d&lt;/code&gt; 为True 时，才强制执行此检查。设置为0将禁用此检查。</target>
        </trans-unit>
        <trans-unit id="37a276f69711964822e7fcec88111a5f6d2f84a0" translate="yes" xml:space="preserve">
          <source>Make sure that the 2D array has some minimum number of features (columns). The default value of 1 rejects empty datasets. This check is only enforced when the input data has effectively 2 dimensions or is originally 1D and &lt;code&gt;ensure_2d&lt;/code&gt; is True. Setting to 0 disables this check.</source>
          <target state="translated">确保2D阵列具有最少数量的要素（列）。默认值1拒绝空数据集。仅当输入数据有效地具有2维或原始为1D且 &lt;code&gt;ensure_2d&lt;/code&gt; 为True 时，才强制执行此检查。设置为0将禁用此检查。</target>
        </trans-unit>
        <trans-unit id="c24d8a1e4fcddcfde73957adfbe65fb40c76c786" translate="yes" xml:space="preserve">
          <source>Make sure that the array has a minimum number of samples in its first axis (rows for a 2D array). Setting to 0 disables this check.</source>
          <target state="translated">确保阵列在其第一轴上有一个最小的样本数(2D阵列的行)。设置为0将禁用这项检查。</target>
        </trans-unit>
        <trans-unit id="fdc3084b2db3fff3561874bdbe81f2954a4b0ffc" translate="yes" xml:space="preserve">
          <source>Make sure the same scale is used over all features. Because manifold learning methods are based on a nearest-neighbor search, the algorithm may perform poorly otherwise. See &lt;a href=&quot;preprocessing#preprocessing-scaler&quot;&gt;StandardScaler&lt;/a&gt; for convenient ways of scaling heterogeneous data.</source>
          <target state="translated">确保在所有功能上使用相同的比例。由于流形学习方法是基于最近邻居搜索的，因此该算法的执行效果可能不佳。有关缩放异构数据的便捷方法，请参见&lt;a href=&quot;preprocessing#preprocessing-scaler&quot;&gt;StandardScaler&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="7f7e62e13bb8885a4df4d0d5a8e1dba7c3c65c15" translate="yes" xml:space="preserve">
          <source>Make sure you permute (shuffle) your training data before fitting the model or use &lt;code&gt;shuffle=True&lt;/code&gt; to shuffle after each iteration.</source>
          <target state="translated">确保在拟合模型之前对训练数据进行置换（混洗），或在每次迭代后使用 &lt;code&gt;shuffle=True&lt;/code&gt; 进行混洗。</target>
        </trans-unit>
        <trans-unit id="f48f3a474378f962985a41275dc98355541c63d2" translate="yes" xml:space="preserve">
          <source>Make two interleaving half circles</source>
          <target state="translated">做两个交错的半圆</target>
        </trans-unit>
        <trans-unit id="0a0a9871e0af603535e4f6104cfca3266e203a87" translate="yes" xml:space="preserve">
          <source>Malic Acid:</source>
          <target state="translated">苹果酸。</target>
        </trans-unit>
        <trans-unit id="245748b8f3a70aaac9759204b7d3978b9d337db8" translate="yes" xml:space="preserve">
          <source>Malic acid</source>
          <target state="translated">苹果酸</target>
        </trans-unit>
        <trans-unit id="a81a721fb7e702ed0a37d056ec4a9d2f925e70b0" translate="yes" xml:space="preserve">
          <source>ManhattanDistance</source>
          <target state="translated">ManhattanDistance</target>
        </trans-unit>
        <trans-unit id="ccbe2127be70aaa7c3514b3155dd29902fae6143" translate="yes" xml:space="preserve">
          <source>Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications.</source>
          <target state="translated">Manifold Learning可以被认为是对PCA等线性框架进行泛化的尝试,使其对数据中的非线性结构敏感。虽然存在监督变体,但典型的歧路学习问题是无监督的:它从数据本身学习数据的高维结构,而不使用预定的分类。</target>
        </trans-unit>
        <trans-unit id="7a0e60acb472080022463866637a1ea7c0251335" translate="yes" xml:space="preserve">
          <source>Manifold Learning methods on a severed sphere</source>
          <target state="translated">割裂球体上的Manifold学习方法。</target>
        </trans-unit>
        <trans-unit id="aca365adba00c10f7a3cc50cff4a88afd0947dd9" translate="yes" xml:space="preserve">
          <source>Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high.</source>
          <target state="translated">Manifold学习是一种非线性降维的方法。这个任务的算法是基于许多数据集的维度只是人为的高的想法。</target>
        </trans-unit>
        <trans-unit id="1e718f0bccbec4566b4c8536fdd24c184318a8a9" translate="yes" xml:space="preserve">
          <source>Manifold learning on handwritten digits: Locally Linear Embedding, Isomap&amp;hellip;</source>
          <target state="translated">手写数字上的流形学习：局部线性嵌入，Isomap&amp;hellip;</target>
        </trans-unit>
        <trans-unit id="0d3695eb907329bab9f0e9752d7ff00d200420c9" translate="yes" xml:space="preserve">
          <source>Many applications require being able to decide whether a new observation belongs to the same distribution as existing observations (it is an &lt;em&gt;inlier&lt;/em&gt;), or should be considered as different (it is an &lt;em&gt;outlier&lt;/em&gt;). Often, this ability is used to clean real data sets. Two important distinctions must be made:</source>
          <target state="translated">许多应用需要能够决定一个新的观察是否属于相同的分布与现有观测（它是一个&lt;em&gt;内点&lt;/em&gt;），或（这是一个应该被视为不同的&lt;em&gt;异常值&lt;/em&gt;）。通常，此功能用于清除实际数据集。必须做出两个重要的区分：</target>
        </trans-unit>
        <trans-unit id="626f0980ad5cd6d2b6f18a99ff094a7bf141dc9a" translate="yes" xml:space="preserve">
          <source>Many clusters, possibly connectivity constraints</source>
          <target state="translated">许多集群,可能是连接性限制</target>
        </trans-unit>
        <trans-unit id="9d1190903d42ddc70f3db2311f157c56b6260b92" translate="yes" xml:space="preserve">
          <source>Many clusters, possibly connectivity constraints, non Euclidean distances</source>
          <target state="translated">许多集群,可能是连通性的限制,非欧氏距离</target>
        </trans-unit>
        <trans-unit id="ac65e2f8a158fa7cc404d708906171f5ea9f26fd" translate="yes" xml:space="preserve">
          <source>Many clusters, uneven cluster size, non-flat geometry</source>
          <target state="translated">群落多,群落大小不均,几何形状不平坦。</target>
        </trans-unit>
        <trans-unit id="241eda779a46dbe514b8b7f2a96e98aed4d935af" translate="yes" xml:space="preserve">
          <source>Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps. Often it is easiest to preprocess data before applying scikit-learn methods, for example using &lt;a href=&quot;http://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt;. Processing your data before passing it to scikit-learn might be problematic for one of the following reasons:</source>
          <target state="translated">许多数据集包含不同类型的要素，例如文本，浮点数和日期，其中每种类型的要素都需要单独的预处理或要素提取步骤。通常，在应用scikit-learn方法之前，例如使用&lt;a href=&quot;http://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt;，对数据进行预处理最容易。由于以下原因之一，在将数据传递给scikit-learn之前进行处理可能会出现问题：</target>
        </trans-unit>
        <trans-unit id="192c25a6ed904d1327958fde4c93098505cb86b8" translate="yes" xml:space="preserve">
          <source>Many metrics are not given names to be used as &lt;code&gt;scoring&lt;/code&gt; values, sometimes because they require additional parameters, such as &lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt;&lt;code&gt;fbeta_score&lt;/code&gt;&lt;/a&gt;. In such cases, you need to generate an appropriate scoring object. The simplest way to generate a callable object for scoring is by using &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt;&lt;code&gt;make_scorer&lt;/code&gt;&lt;/a&gt;. That function converts metrics into callables that can be used for model evaluation.</source>
          <target state="translated">许多度量未指定名称用作 &lt;code&gt;scoring&lt;/code&gt; 值，有时是因为它们需要附加参数，例如&lt;a href=&quot;generated/sklearn.metrics.fbeta_score#sklearn.metrics.fbeta_score&quot;&gt; &lt;code&gt;fbeta_score&lt;/code&gt; &lt;/a&gt;。在这种情况下，您需要生成一个适当的计分对象。生成用于评分的可调用对象的最简单方法是使用&lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt; &lt;code&gt;make_scorer&lt;/code&gt; &lt;/a&gt;。该函数将指标转换为可用于模型评估的可调用对象。</target>
        </trans-unit>
        <trans-unit id="dacd10610ea3bac36f91971d47d3019273ca964d" translate="yes" xml:space="preserve">
          <source>Many statistical problems require the estimation of a population&amp;rsquo;s covariance matrix, which can be seen as an estimation of data set scatter plot shape. Most of the time, such an estimation has to be done on a sample whose properties (size, structure, homogeneity) have a large influence on the estimation&amp;rsquo;s quality. The &lt;code&gt;sklearn.covariance&lt;/code&gt; package provides tools for accurately estimating a population&amp;rsquo;s covariance matrix under various settings.</source>
          <target state="translated">许多统计问题都需要估计总体的协方差矩阵，这可以看作是对数据集散点图形状的估计。在大多数情况下，必须对其属性（大小，结构，同质性）对估计质量有很大影响的样本进行这种估计。所述 &lt;code&gt;sklearn.covariance&lt;/code&gt; 包提供用于在各种设置精确地估计群体的协方差矩阵的工具。</target>
        </trans-unit>
        <trans-unit id="be7bf3b7e371f4bec9a03a7522f6dcf31d112a68" translate="yes" xml:space="preserve">
          <source>Many, many more &amp;hellip;</source>
          <target state="translated">很多很多&amp;hellip;&amp;hellip;</target>
        </trans-unit>
        <trans-unit id="01a4f781a04bf81d6d3609180ff5b158082bf232" translate="yes" xml:space="preserve">
          <source>Map data to a normal distribution</source>
          <target state="translated">将数据映射到正态分布</target>
        </trans-unit>
        <trans-unit id="16409bc40b2df043ac11786860ad0f327aa511b9" translate="yes" xml:space="preserve">
          <source>Maps data to a normal distribution using a power transformation.</source>
          <target state="translated">使用功率转换将数据映射到正态分布。</target>
        </trans-unit>
        <trans-unit id="05aecccd2b32722fa423ccbd7840d48763834385" translate="yes" xml:space="preserve">
          <source>Maps data to a standard normal distribution with the parameter &lt;code&gt;output_distribution=&amp;rsquo;normal&amp;rsquo;&lt;/code&gt;.</source>
          <target state="translated">使用参数 &lt;code&gt;output_distribution=&amp;rsquo;normal&amp;rsquo;&lt;/code&gt; 将数据映射到标准正态分布。</target>
        </trans-unit>
        <trans-unit id="2546740d19a0cb39e3dfa40dd82138fa02a22968" translate="yes" xml:space="preserve">
          <source>Maps each categorical feature name to a list of values, such that the value encoded as i is ith in the list.</source>
          <target state="translated">将每个分类特征名称映射到一个值列表中,使编码为i的值是列表中的第i个。</target>
        </trans-unit>
        <trans-unit id="601b228138151f5d614818578f5a990f06465ee3" translate="yes" xml:space="preserve">
          <source>Marginal distribution for the transformed data. The choices are &amp;lsquo;uniform&amp;rsquo; (default) or &amp;lsquo;normal&amp;rsquo;.</source>
          <target state="translated">转换后数据的边际分布。选择为&amp;ldquo;统一&amp;rdquo;（默认）或&amp;ldquo;正常&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="32cec489ab51eb304acc5d56c34e0b5894817af1" translate="yes" xml:space="preserve">
          <source>Mark Schmidt, Nicolas Le Roux, and Francis Bach: &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;Minimizing Finite Sums with the Stochastic Average Gradient.&lt;/a&gt;</source>
          <target state="translated">马克&amp;middot;施密特（Mark Schmidt），尼古拉斯&amp;middot;勒&amp;middot;鲁（Nicolas Le Roux）和弗朗西斯&amp;middot;巴赫（Francis Bach）：&lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;通过随机平均梯度最小化有限和。&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b66f191d027329ba9273c4c5f9be765f9b3745f5" translate="yes" xml:space="preserve">
          <source>Mask to be used on X.</source>
          <target state="translated">掩码要用在X上。</target>
        </trans-unit>
        <trans-unit id="54a21a4d94fa24c6c092fd6e4c2ec05359c76c09" translate="yes" xml:space="preserve">
          <source>MatchingDistance</source>
          <target state="translated">MatchingDistance</target>
        </trans-unit>
        <trans-unit id="38c6b835ca8294e13538ac219d64ae1244beb7fc" translate="yes" xml:space="preserve">
          <source>Matern kernel.</source>
          <target state="translated">产核。</target>
        </trans-unit>
        <trans-unit id="c2846fd5b2a8440131137c07fea40912afc701b7" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with \(\ell_1\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">数学上,它由一个线性模型组成,该模型以正则器为前导训练。要最小化的目标函数是:</target>
        </trans-unit>
        <trans-unit id="bf1818d1c45b1997515a16368907c8cf902bab56" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\) prior and \(\ell_2\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">在数学上,它由一个线性模型组成,该模型由混合的先验和先验作为正则器进行训练。需要最小化的目标函数是:</target>
        </trans-unit>
        <trans-unit id="1d9fe275a9038555cabcfe46b4a675067788d84f" translate="yes" xml:space="preserve">
          <source>Mathematically, it consists of a linear model trained with a mixed \(\ell_1\)\(\ell_2\) prior as regularizer. The objective function to minimize is:</source>
          <target state="translated">在数学上,它由一个线性模型组成,该模型以混合的前值作为正则器进行训练。需要最小化的目标函数是:</target>
        </trans-unit>
        <trans-unit id="461064fec990b9f56bd78c5da4699263962dc67b" translate="yes" xml:space="preserve">
          <source>Mathematically, this shrinkage consists in reducing the ratio between the smallest and the largest eigenvalues of the empirical covariance matrix. It can be done by simply shifting every eigenvalue according to a given offset, which is equivalent of finding the l2-penalized Maximum Likelihood Estimator of the covariance matrix. In practice, shrinkage boils down to a simple a convex transformation : \(\Sigma_{\rm shrunk} = (1-\alpha)\hat{\Sigma} + \alpha\frac{{\rm Tr}\hat{\Sigma}}{p}\rm Id\).</source>
          <target state="translated">从数学上讲,这种收缩包括减少经验协方差矩阵的最小和最大特征值之间的比值。它可以通过简单地根据给定的偏移量移动每一个特征值来实现,这相当于找到协方差矩阵的l2-penalized Maximum Likelihood Estimator。在实践中,收缩归结为一个简单的一个凸变换。\(\Sigma_{\rm shrunk}=(1-\alpha)hat{Sigma}+\alphafrac{{\rm Tr}hat{Sigma}}{p}rm Id\)。</target>
        </trans-unit>
        <trans-unit id="7f2fa948973686599d9719cd22bf9c261bdbf5a5" translate="yes" xml:space="preserve">
          <source>Mathematically, truncated SVD applied to training samples \(X\) produces a low-rank approximation \(X\):</source>
          <target state="translated">数学上,截断SVD应用于训练样本(X/)产生一个低阶近似(X/)。</target>
        </trans-unit>
        <trans-unit id="b8c6141893596b10260b39727bf4a66986a56a95" translate="yes" xml:space="preserve">
          <source>Matrices:</source>
          <target state="translated">Matrices:</target>
        </trans-unit>
        <trans-unit id="878abbe8708b2c0d949ede6590fbf80f3b3ca712" translate="yes" xml:space="preserve">
          <source>Matrix \(C\) such that \(C_{i, j}\) is the number of samples in true class \(i\) and in predicted class \(j\). If &lt;code&gt;eps is None&lt;/code&gt;, the dtype of this array will be integer. If &lt;code&gt;eps&lt;/code&gt; is given, the dtype will be float. Will be a &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; if &lt;code&gt;sparse=True&lt;/code&gt;.</source>
          <target state="translated">矩阵\（C \）使得\（C_ {i，j} \）是真实类\（i \）和预测类\（j \）中的样本数。如果 &lt;code&gt;eps is None&lt;/code&gt; ，则此数组的dtype将为整数。如果给出了 &lt;code&gt;eps&lt;/code&gt; ，则dtype将为float。如果 &lt;code&gt;sparse=True&lt;/code&gt; ,将是 &lt;code&gt;scipy.sparse.csr_matrix&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="9a9d3bf25623c95ec103c9ba9ebefafc39b31e10" translate="yes" xml:space="preserve">
          <source>Matrix of similarities between points</source>
          <target state="translated">点间相似性矩阵</target>
        </trans-unit>
        <trans-unit id="fe09cc11ed56c787dbf583e1d3c86930e73d13b7" translate="yes" xml:space="preserve">
          <source>Matrix to be scaled.</source>
          <target state="translated">矩阵要缩放。</target>
        </trans-unit>
        <trans-unit id="f581a8973d13aee9e6d301f776c7ac0aca9937df" translate="yes" xml:space="preserve">
          <source>Matrix to decompose</source>
          <target state="translated">矩阵分解</target>
        </trans-unit>
        <trans-unit id="64c58969af0dfb121c9b8582a353fed07f3ae81a" translate="yes" xml:space="preserve">
          <source>Matrix to normalize using the variance of the features.</source>
          <target state="translated">使用特征的方差进行标准化的矩阵。</target>
        </trans-unit>
        <trans-unit id="dc67599b55e21aadb81c15a2c42c0d243f238c7f" translate="yes" xml:space="preserve">
          <source>Matrix whose two columns are to be swapped.</source>
          <target state="translated">矩阵的两列要交换。</target>
        </trans-unit>
        <trans-unit id="2ea8698954891f702cc6556af5a70f4a4777910c" translate="yes" xml:space="preserve">
          <source>Matrix whose two rows are to be swapped.</source>
          <target state="translated">两行要交换的矩阵。</target>
        </trans-unit>
        <trans-unit id="91c27cd36373d9f3e97d3e8652e4d5420038195e" translate="yes" xml:space="preserve">
          <source>Max number of iterations for updating document topic distribution in the E-step.</source>
          <target state="translated">电子步骤中更新文档主题分布的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="00c71f39eb3784f568496e9b201bef34cf1fba1e" translate="yes" xml:space="preserve">
          <source>MaxAbsScaler</source>
          <target state="translated">MaxAbsScaler</target>
        </trans-unit>
        <trans-unit id="b19f6ae06ce0301b0f2f115ace4b151976f71361" translate="yes" xml:space="preserve">
          <source>Maximizing ELBO is equivalent to minimizing the Kullback-Leibler(KL) divergence between \(q(z,\theta,\beta)\) and the true posterior \(p(z, \theta, \beta |w, \alpha, \eta)\).</source>
          <target state="translated">最大化ELBO相当于最小化/(q(z,/theta,/beta)/)和真实后验/(p(z,/theta,/beta |w,/alpha,/eta)/)之间的Kullback-Leibler(KL)分歧。</target>
        </trans-unit>
        <trans-unit id="c7118c6c94bd33474c6bd73b2a0ef4d05bd61b9a" translate="yes" xml:space="preserve">
          <source>Maximizing the log-marginal-likelihood after subtracting the target&amp;rsquo;s mean yields the following kernel with an LML of -83.214:</source>
          <target state="translated">减去目标均值后，最大化对数边际似然率将产生以下内核，其LML为-83.214：</target>
        </trans-unit>
        <trans-unit id="3cc68e53e734e045e272d9b61d6ce440314a7c5a" translate="yes" xml:space="preserve">
          <source>Maximum distortion rate as defined by the Johnson-Lindenstrauss lemma. If an array is given, it will compute a safe number of components array-wise.</source>
          <target state="translated">由Johnson-Lindenstrauss定理定义的最大失真率。如果给定一个数组,它将以数组的方式计算一个安全的分量数。</target>
        </trans-unit>
        <trans-unit id="2648af65469bf6064127630edb239d63fa9087cb" translate="yes" xml:space="preserve">
          <source>Maximum likelihood covariance estimator</source>
          <target state="translated">最大似然协方差估计器</target>
        </trans-unit>
        <trans-unit id="c549d82a160dc50758b33cda113fa1dc7a80727c" translate="yes" xml:space="preserve">
          <source>Maximum norm of the residual. If not None, overrides n_nonzero_coefs.</source>
          <target state="translated">残差的最大规范。如果不是None,则覆盖n_nonzero_coefs。</target>
        </trans-unit>
        <trans-unit id="c4d8a154588727ab7c620ef2088eb03d03fdb2cd" translate="yes" xml:space="preserve">
          <source>Maximum number of CF subclusters in each node. If a new samples enters such that the number of subclusters exceed the branching_factor then that node is split into two nodes with the subclusters redistributed in each. The parent subcluster of that node is removed and two new subclusters are added as parents of the 2 split nodes.</source>
          <target state="translated">每个节点中CF子簇的最大数量。如果有新的样本进入,使子簇的数量超过了 branching_factor,那么该节点就会被分割成两个节点,每个节点中的子簇都会被重新分配。该节点的父子簇被移除,并增加两个新的子簇作为2个分裂节点的父子簇。</target>
        </trans-unit>
        <trans-unit id="ffcbfb393ae9341f5e6cf4dea80093dbb65c654d" translate="yes" xml:space="preserve">
          <source>Maximum number of epochs to not meet &lt;code&gt;tol&lt;/code&gt; improvement. Only effective when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;</source>
          <target state="translated">时代的最大数目不能满足 &lt;code&gt;tol&lt;/code&gt; 改善。仅在Solver ='sgd'或'adam'时有效</target>
        </trans-unit>
        <trans-unit id="2d31095f21fc709b8362fbfbc8701ee49bed43f4" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations</source>
          <target state="translated">最大迭代次数</target>
        </trans-unit>
        <trans-unit id="8d0f629c611a546c50fbd29c0a5c09d14523502f" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations before timing out.</source>
          <target state="translated">计时结束前的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="e5ab15aeae2ebd19c6cc8dd9b722001d143407e7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations during fit.</source>
          <target state="translated">拟合期间的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="4bd775e3e4801f1199d0d4b78f5390120406b7db" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for arpack. If None, optimal value will be chosen by arpack.</source>
          <target state="translated">arpack的最大迭代次数。如果为 &quot;无&quot;,则由arpack选择最佳值。</target>
        </trans-unit>
        <trans-unit id="f374a3956c9375a530255caa54ee43ca08273ef7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. For &amp;lsquo;sparse_cg&amp;rsquo; and &amp;lsquo;lsqr&amp;rsquo; solvers, the default value is determined by scipy.sparse.linalg. For &amp;lsquo;sag&amp;rsquo; solver, the default value is 1000.</source>
          <target state="translated">共轭梯度求解器的最大迭代次数。对于&amp;ldquo; sparse_cg&amp;rdquo;和&amp;ldquo; lsqr&amp;rdquo;求解器，默认值由scipy.sparse.linalg确定。对于&amp;ldquo;下垂&amp;rdquo;求解器，默认值为1000。</target>
        </trans-unit>
        <trans-unit id="11f341e8f36dbf9c1af7cfa8e66dcc9686e34f6b" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. For the &amp;lsquo;sparse_cg&amp;rsquo; and &amp;lsquo;lsqr&amp;rsquo; solvers, the default value is determined by scipy.sparse.linalg. For &amp;lsquo;sag&amp;rsquo; and saga solver, the default value is 1000.</source>
          <target state="translated">共轭梯度求解器的最大迭代次数。对于&amp;ldquo; sparse_cg&amp;rdquo;和&amp;ldquo; lsqr&amp;rdquo;求解器，默认值由scipy.sparse.linalg确定。对于&amp;ldquo; sag&amp;rdquo;和&amp;ldquo; saga求解器&amp;rdquo;，默认值为1000。</target>
        </trans-unit>
        <trans-unit id="c5354ceb6cfff4ad460f2a35427697293dda616c" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for conjugate gradient solver. The default value is determined by scipy.sparse.linalg.</source>
          <target state="translated">共轭梯度求解器的最大迭代次数。默认值由scipy.sparse.linalg决定。</target>
        </trans-unit>
        <trans-unit id="1e7dfd80e629f3bb34ea64892d98c69b612b79e1" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for random sample selection.</source>
          <target state="translated">随机样本选择的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="ecd130d87b8a2f3d02f709f684a50be372cae2c9" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the arpack solver. not used if eigen_solver == &amp;lsquo;dense&amp;rsquo;.</source>
          <target state="translated">arpack求解器的最大迭代次数。如果eigen_solver =='dense'，则不使用。</target>
        </trans-unit>
        <trans-unit id="9de38c6ff2395ad8506cb6d44f0cafffcf62c788" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the calculation of spatial median.</source>
          <target state="translated">计算空间中位数的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="07f904d8faecfe25c803428c8e7a88d0070e3e3e" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the optimization. Should be at least 250.</source>
          <target state="translated">优化的最大迭代次数。应至少为250次。</target>
        </trans-unit>
        <trans-unit id="dce17045503a4e7dfc48c40d90d1aeae843b7e1f" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations for the solver.</source>
          <target state="translated">求解器的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="e1c1739cc631f47e83bf7484461b685fd99cca3d" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the SMACOF algorithm for a single run.</source>
          <target state="translated">SMACOF算法单次运行的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="7f1e71a3c23990192b71291657a3bae2f490d4ee" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the k-means algorithm for a single run.</source>
          <target state="translated">k-means算法单次运行的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="f0e6e9653318c3bc8385e39576298e438fdcd759" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the k-means algorithm to run.</source>
          <target state="translated">k-means算法运行的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="368dd40a437636dbd3f559d65e7725494d2a9fb1" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations of the optimization algorithm.</source>
          <target state="translated">优化算法的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="63c06831f070b2a52428ee45e49cb4b88ea5a09d" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics.</source>
          <target state="translated">在停止之前,独立于任何早期停止标准的启发式方法,对完整数据集的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="7899fd5b3a78738f0401cfc3b35e8ce4d2309778" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations that can be skipped due to finding zero inliers or invalid data defined by &lt;code&gt;is_data_valid&lt;/code&gt; or invalid models defined by &lt;code&gt;is_model_valid&lt;/code&gt;.</source>
          <target state="translated">迭代，可以是最大数量跳过由于发现零个内围层或由下式定义的无效数据 &lt;code&gt;is_data_valid&lt;/code&gt; 由下式定义的或无效的模型 &lt;code&gt;is_model_valid&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e7fba252520d1990cf2d4eb5716def2f32bbae99" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations that scipy.optimize.fmin_l_bfgs_b should run for.</source>
          <target state="translated">scipy.optimization.fmin_l_bfgs_b应该运行的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="4e1098501827a192b62ebb4f1cab51c2d422cf00" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform if &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt;.</source>
          <target state="translated">如果 &lt;code&gt;algorithm=&amp;rsquo;lasso_cd&amp;rsquo;&lt;/code&gt; 执行的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="aa990833ac4f020e2d41da9d6169624866ecf0ee" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform in the Lars algorithm.</source>
          <target state="translated">在Lars算法中执行的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="d0f4ce7794b613699161c8c6e0e45882e1e59e63" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform, set to infinity for no limit.</source>
          <target state="translated">执行的最大迭代次数,无限制时设置为无穷大。</target>
        </trans-unit>
        <trans-unit id="6484f135db2cde17daa0042bcd9839216d734460" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform.</source>
          <target state="translated">执行的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="db2ca83257c5e157920232d66349b60febf20184" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations to perform. Can be used for early stopping.</source>
          <target state="translated">执行的最大迭代次数。可用于提前停止。</target>
        </trans-unit>
        <trans-unit id="3ae0883a212da2486dca35b829a405dbbd2b8c29" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations without progress before we abort the optimization, used after 250 initial iterations with early exaggeration. Note that progress is only checked every 50 iterations so this value is rounded to the next multiple of 50.</source>
          <target state="translated">在我们中止优化之前没有进展的最大迭代次数,在250次初始迭代后使用,早期夸大。注意,每50次迭代才检查一次进度,所以这个值四舍五入到50的下一个倍数。</target>
        </trans-unit>
        <trans-unit id="5919ba2c46521b64537304f167c62803da4391df" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations, per seed point before the clustering operation terminates (for that seed point), if has not converged yet.</source>
          <target state="translated">如果尚未收敛,则在聚类操作终止之前,每个种子点的最大迭代次数(对于该种子点)。</target>
        </trans-unit>
        <trans-unit id="3fcea6ff6580050eb63d7142d23e7d7896de7626" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations.</source>
          <target state="translated">最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="2bd71b5c83b9f5da0d4a94baa31a035271906ce7" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. Default is 300</source>
          <target state="translated">最大迭代次数。默认为300次</target>
        </trans-unit>
        <trans-unit id="6740f9eed55c5399b0f5fe47513d42802b2d72aa" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. Default is 300.</source>
          <target state="translated">最大迭代次数。默认为300次。</target>
        </trans-unit>
        <trans-unit id="1d3427b734648d0ef9c00f4282011f095b732bba" translate="yes" xml:space="preserve">
          <source>Maximum number of iterations. The solver iterates until convergence (determined by &amp;lsquo;tol&amp;rsquo;) or this number of iterations. For stochastic solvers (&amp;lsquo;sgd&amp;rsquo;, &amp;lsquo;adam&amp;rsquo;), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.</source>
          <target state="translated">最大迭代次数。求解器迭代直到收敛（由&amp;ldquo; tol&amp;rdquo;确定）或此迭代次数。对于随机求解器（&amp;ldquo; sgd&amp;rdquo;，&amp;ldquo; adam&amp;rdquo;），请注意，这确定了时期数（每个数据点将使用多少次），而不是梯度步数。</target>
        </trans-unit>
        <trans-unit id="647156dc0e4267e82660e321d855d0ab57ee6ce8" translate="yes" xml:space="preserve">
          <source>Maximum number of samples used to estimate the quantiles for computational efficiency. Note that the subsampling procedure may differ for value-identical sparse and dense matrices.</source>
          <target state="translated">为了计算效率,用于估计量子数的最大样本数。请注意,对于数值相同的稀疏矩阵和密集矩阵,子抽样程序可能有所不同。</target>
        </trans-unit>
        <trans-unit id="415a2ec1c451656db8760ffe077b89b191d3a2b3" translate="yes" xml:space="preserve">
          <source>Maximum numbers of iterations to perform, therefore maximum features to include. 10% of &lt;code&gt;n_features&lt;/code&gt; but at least 5 if available.</source>
          <target state="translated">要执行的最大迭代次数，因此要包含的最大功能。 &lt;code&gt;n_features&lt;/code&gt; 的 10％，但至少5个（如果有）。</target>
        </trans-unit>
        <trans-unit id="631012abffd401f8346d1251260aa1bdd321bf8a" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;max_iter&lt;/code&gt;, &lt;code&gt;n_features&lt;/code&gt; or the number of nodes in the path with &lt;code&gt;alpha &amp;gt;= alpha_min&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">每次迭代的最大协方差（绝对值）。 &lt;code&gt;n_alphas&lt;/code&gt; 是 &lt;code&gt;max_iter&lt;/code&gt; ， &lt;code&gt;n_features&lt;/code&gt; 或 &lt;code&gt;alpha &amp;gt;= alpha_min&lt;/code&gt; 的路径中的节点数，以较小者为准。</target>
        </trans-unit>
        <trans-unit id="705f01a5b973480d43f7edb8b4f1d8b46afffacc" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;max_iter&lt;/code&gt;, &lt;code&gt;n_features&lt;/code&gt;, or the number of nodes in the path with correlation greater than &lt;code&gt;alpha&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">每次迭代的最大协方差（绝对值）。 &lt;code&gt;n_alphas&lt;/code&gt; 可以是 &lt;code&gt;max_iter&lt;/code&gt; ， &lt;code&gt;n_features&lt;/code&gt; 或相关性大于 &lt;code&gt;alpha&lt;/code&gt; 的路径中的节点数（以较小者为准）。</target>
        </trans-unit>
        <trans-unit id="62989d4a3b259ca439d6192faa2834aa0e75a2e0" translate="yes" xml:space="preserve">
          <source>Maximum of covariances (in absolute value) at each iteration. &lt;code&gt;n_alphas&lt;/code&gt; is either &lt;code&gt;n_nonzero_coefs&lt;/code&gt; or &lt;code&gt;n_features&lt;/code&gt;, whichever is smaller.</source>
          <target state="translated">每次迭代的最大协方差（绝对值）。 &lt;code&gt;n_alphas&lt;/code&gt; 是 &lt;code&gt;n_nonzero_coefs&lt;/code&gt; 或 &lt;code&gt;n_features&lt;/code&gt; 中的较小者。</target>
        </trans-unit>
        <trans-unit id="e56eb85aade57d415023e1a3a8ae03f5b942a0cd" translate="yes" xml:space="preserve">
          <source>Maximum residual for a data sample to be classified as an inlier. By default the threshold is chosen as the MAD (median absolute deviation) of the target values &lt;code&gt;y&lt;/code&gt;.</source>
          <target state="translated">数据样本的最大残差被归类为一个正常值。默认情况下，将阈值选择为目标值 &lt;code&gt;y&lt;/code&gt; 的MAD（中值绝对偏差）。</target>
        </trans-unit>
        <trans-unit id="6dd6334c9c1bb29cde2ace0d7ca9c039e7de14bd" translate="yes" xml:space="preserve">
          <source>Maximum size for a single training set.</source>
          <target state="translated">单个训练集的最大尺寸。</target>
        </trans-unit>
        <trans-unit id="3feffec2bfb871f0142dbd2d75d410b437245309" translate="yes" xml:space="preserve">
          <source>Maximum squared sum of X over samples. Used only in SAG solver. If None, it will be computed, going through all the samples. The value should be precomputed to speed up cross validation.</source>
          <target state="translated">X对样本的最大平方和。仅在SAG求解器中使用。如果无,将通过所有样本进行计算。该值应该预先计算,以加快交叉验证。</target>
        </trans-unit>
        <trans-unit id="229948f9503f6467f2a53d61f4254093e7ca3738" translate="yes" xml:space="preserve">
          <source>Maximum step size (regularization). Defaults to 1.0.</source>
          <target state="translated">最大步长(正则化)。默认值为1.0。</target>
        </trans-unit>
        <trans-unit id="843c61e3ff0f74449c911dbb02faa43821e29850" translate="yes" xml:space="preserve">
          <source>Maximum value of a bicluster.</source>
          <target state="translated">双集群的最大值。</target>
        </trans-unit>
        <trans-unit id="ce9a39e13a20e687ebff9fcf4496175bdfa0afbb" translate="yes" xml:space="preserve">
          <source>Maximum value of input array &lt;code&gt;X_&lt;/code&gt; for right bound.</source>
          <target state="translated">右边界的输入数组 &lt;code&gt;X_&lt;/code&gt; 的最大值。</target>
        </trans-unit>
        <trans-unit id="9fa80bb15d05b082522b43fcb83b05beb6f022c6" translate="yes" xml:space="preserve">
          <source>May be the string &amp;ldquo;jaccard&amp;rdquo; to use the Jaccard coefficient, or any function that takes four arguments, each of which is a 1d indicator vector: (a_rows, a_columns, b_rows, b_columns).</source>
          <target state="translated">可以是使用&amp;ldquo; Jaccard系数&amp;rdquo;的字符串&amp;ldquo; jaccard&amp;rdquo;，也可以是任何带有四个参数的函数，每个参数都是一个一维指示符向量：（a_rows，a_columns，b_rows，b_columns）。</target>
        </trans-unit>
        <trans-unit id="4fad1e9d11d435bd5f0db307b217272d94f19197" translate="yes" xml:space="preserve">
          <source>May contain any subset of (&amp;lsquo;headers&amp;rsquo;, &amp;lsquo;footers&amp;rsquo;, &amp;lsquo;quotes&amp;rsquo;). Each of these are kinds of text that will be detected and removed from the newsgroup posts, preventing classifiers from overfitting on metadata.</source>
          <target state="translated">可以包含（&amp;ldquo;页眉&amp;rdquo;，&amp;ldquo;页脚&amp;rdquo;，&amp;ldquo;引号&amp;rdquo;）的任何子集。这些文本中的每一种都是将被检测到并从新闻组帖子中删除的文本类型，从而防止分类器过度适合元数据。</target>
        </trans-unit>
        <trans-unit id="4f0935dfe9ab3f30e90c245d2338ed727682177f" translate="yes" xml:space="preserve">
          <source>Mean Absolute Error:</source>
          <target state="translated">平均绝对误差。</target>
        </trans-unit>
        <trans-unit id="007ffde203dd83c4c710b22da1cbe0b3700a98d1" translate="yes" xml:space="preserve">
          <source>Mean Silhouette Coefficient for all samples.</source>
          <target state="translated">所有样品的平均轮廓系数。</target>
        </trans-unit>
        <trans-unit id="2762f10f75116f5e4a70c10eb14cf5f478eb498f" translate="yes" xml:space="preserve">
          <source>Mean Squared Error:</source>
          <target state="translated">平均平方误差。</target>
        </trans-unit>
        <trans-unit id="43559adecf21dbddbbe17afba52b16b4a67e4402" translate="yes" xml:space="preserve">
          <source>Mean absolute error regression loss</source>
          <target state="translated">平均绝对误差回归损失</target>
        </trans-unit>
        <trans-unit id="ec9517dd8574c2a6b45d6a307e2a503f5e6d275d" translate="yes" xml:space="preserve">
          <source>Mean accuracy of self.predict(X) wrt. y.</source>
          <target state="translated">self.predict(X)wrt.y的平均精度。</target>
        </trans-unit>
        <trans-unit id="7493a61b1729d0e0247689ac97555930f03706df" translate="yes" xml:space="preserve">
          <source>Mean cross-validated score of the best_estimator</source>
          <target state="translated">最佳估计器的平均交叉验证得分。</target>
        </trans-unit>
        <trans-unit id="140100875ffefa42eddb6a75afe4c55e05443030" translate="yes" xml:space="preserve">
          <source>Mean cross-validated score of the best_estimator.</source>
          <target state="translated">Best_estimator的平均交叉验证得分。</target>
        </trans-unit>
        <trans-unit id="dd2a669704ec2ab03a0e4ca8e2abc9c2d42f71c6" translate="yes" xml:space="preserve">
          <source>Mean of predictive distribution a query points</source>
          <target state="translated">预测分布的平均值a查询点</target>
        </trans-unit>
        <trans-unit id="b609d0f7d96a7b9c8e60baf85298ffc173cbc6f7" translate="yes" xml:space="preserve">
          <source>Mean of predictive distribution of query points.</source>
          <target state="translated">查询点预测分布的平均值。</target>
        </trans-unit>
        <trans-unit id="e7f5a133eabd3f3a470b8b7cb54eeb045b64973a" translate="yes" xml:space="preserve">
          <source>Mean or median or quantile of the training targets or constant value given by the user.</source>
          <target state="translated">训练目标的平均值或中位数或分位数或用户给出的定值。</target>
        </trans-unit>
        <trans-unit id="9ede03e41402c8eec43dd01264f8f822af5fb92b" translate="yes" xml:space="preserve">
          <source>Mean shift clustering aims to discover &amp;ldquo;blobs&amp;rdquo; in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.</source>
          <target state="translated">均值漂移聚类旨在发现平滑密度的样本中的&amp;ldquo;斑点&amp;rdquo;。这是基于质心的算法，它通过将质心的候选更新为给定区域内点的均值来工作。然后在后处理阶段对这些候选对象进行过滤，以消除几乎重复的部分，从而形成最终的形心集。</target>
        </trans-unit>
        <trans-unit id="08b2e6d37eec1f5ceae1376ffba9071609b6547f" translate="yes" xml:space="preserve">
          <source>Mean shift clustering using a flat kernel.</source>
          <target state="translated">使用平核进行平均移位聚类。</target>
        </trans-unit>
        <trans-unit id="4484f1a9abfaeee06549ff0a6b75712b44fd35f2" translate="yes" xml:space="preserve">
          <source>Mean square error for the test set on each fold, varying l1_ratio and alpha.</source>
          <target state="translated">每个折线上测试集的平均平方误差,改变l1_ratio和alpha。</target>
        </trans-unit>
        <trans-unit id="4a0031a2d59450a58aeaa638a064cb2e2c9a0da5" translate="yes" xml:space="preserve">
          <source>Mean squared error regression loss</source>
          <target state="translated">平均平方误差回归损失</target>
        </trans-unit>
        <trans-unit id="831bfb250ab69b773c25fae60f230a00bfdc7239" translate="yes" xml:space="preserve">
          <source>Mean squared logarithmic error regression loss</source>
          <target state="translated">平均平方对数误差回归损失</target>
        </trans-unit>
        <trans-unit id="2db7f6881ab1082c632822482db18fa9fc34ed90" translate="yes" xml:space="preserve">
          <source>Mean-shift</source>
          <target state="translated">Mean-shift</target>
        </trans-unit>
        <trans-unit id="df21241945c8fd60618f888d81f315be3f7af674" translate="yes" xml:space="preserve">
          <source>Measure the similarity of two clusterings of a set of points.</source>
          <target state="translated">测量一组点的两个聚类的相似度。</target>
        </trans-unit>
        <trans-unit id="e49da6a85d81735a7b28ef79fc256dec989d2443" translate="yes" xml:space="preserve">
          <source>Measurement errors in X</source>
          <target state="translated">X中的测量误差</target>
        </trans-unit>
        <trans-unit id="471fba4dfe2d4f61d0ae5efc77acb722551abb7b" translate="yes" xml:space="preserve">
          <source>Measurement errors in y</source>
          <target state="translated">测量误差(y)</target>
        </trans-unit>
        <trans-unit id="d59aa4a9911bb1573c0ba0a2779ea96a4989f27f" translate="yes" xml:space="preserve">
          <source>MedInc median income in block</source>
          <target state="translated">MedInc 整體收入中位數</target>
        </trans-unit>
        <trans-unit id="bb82014fc42479d50d7886c5d05c20bd8db97f56" translate="yes" xml:space="preserve">
          <source>Median absolute error regression loss</source>
          <target state="translated">绝对误差中位数回归损失</target>
        </trans-unit>
        <trans-unit id="9e7082d8eb8f2409deaa71605e5d6dbf5b190217" translate="yes" xml:space="preserve">
          <source>Medium &lt;code&gt;n_samples&lt;/code&gt;, small &lt;code&gt;n_clusters&lt;/code&gt;</source>
          <target state="translated">中等 &lt;code&gt;n_samples&lt;/code&gt; ，小的 &lt;code&gt;n_clusters&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="da13fe6da16d1b0d3601ee1416010215c9da2b3d" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;coef_&lt;/code&gt; holds the weights \(w\)</source>
          <target state="translated">成员 &lt;code&gt;coef_&lt;/code&gt; 持有权重\（w \）</target>
        </trans-unit>
        <trans-unit id="b26ca7073281a8f4da3f64aafb4932ce0dc723cc" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;intercept_&lt;/code&gt; holds \(b\)</source>
          <target state="translated">成员 &lt;code&gt;intercept_&lt;/code&gt; 持有\（b \）</target>
        </trans-unit>
        <trans-unit id="8e7e4ea63f467ef992e1b5515c3662fd092327fd" translate="yes" xml:space="preserve">
          <source>Member &lt;code&gt;intercept_&lt;/code&gt; holds the intercept (aka offset or bias):</source>
          <target state="translated">成员 &lt;code&gt;intercept_&lt;/code&gt; 持有截距（又名偏移量或偏差）：</target>
        </trans-unit>
        <trans-unit id="b6ef7f0fdf735583a61dbfc359227479e43bf842" translate="yes" xml:space="preserve">
          <source>Memmapping mode for numpy arrays passed to workers. See &amp;lsquo;max_nbytes&amp;rsquo; parameter documentation for more details.</source>
          <target state="translated">传递给worker的numpy数组的映射模式。有关更多详细信息，请参见'max_nbytes'参数文档。</target>
        </trans-unit>
        <trans-unit id="424b610ebd2af23e4bb8a29dcabbc0551e9c3d87" translate="yes" xml:space="preserve">
          <source>Memory consumption for large sample sizes</source>
          <target state="translated">大样本量的内存消耗</target>
        </trans-unit>
        <trans-unit id="5418f36b831a9c825f5d841c5ee3b1bdb2dd3e28" translate="yes" xml:space="preserve">
          <source>Meta-estimator to regress on a transformed target.</source>
          <target state="translated">元估计器对变换后的目标进行回归。</target>
        </trans-unit>
        <trans-unit id="79599678d3d5e2500fd2a7f727461a2dac0b6cd4" translate="yes" xml:space="preserve">
          <source>Meta-estimators for building composite models with transformers</source>
          <target state="translated">用于建立带有变压器的复合模型的元估计器。</target>
        </trans-unit>
        <trans-unit id="d5a7b3579e10eeaa00ced1884380b174708caebd" translate="yes" xml:space="preserve">
          <source>Meta-transformer for selecting features based on importance weights.</source>
          <target state="translated">基于重要性权重选择特征的元变换器。</target>
        </trans-unit>
        <trans-unit id="88306943fea7e76f9cd57cae0ea6d8b32d2e8434" translate="yes" xml:space="preserve">
          <source>Method</source>
          <target state="translated">Method</target>
        </trans-unit>
        <trans-unit id="3dad9226be4bd937f8a455ee0badad4ee6cceff1" translate="yes" xml:space="preserve">
          <source>Method for initialization of k-means algorithm; defaults to &amp;lsquo;k-means++&amp;rsquo;.</source>
          <target state="translated">一种k均值算法的初始化方法；默认为'k-means ++'。</target>
        </trans-unit>
        <trans-unit id="2c3f3dc3ba37d9b2f1af18176cea15628b89a09c" translate="yes" xml:space="preserve">
          <source>Method for initialization, default to &amp;lsquo;k-means++&amp;rsquo;:</source>
          <target state="translated">初始化方法，默认为'k-means ++'：</target>
        </trans-unit>
        <trans-unit id="62b8a3dc56fc8229948d624cc5b38920d22e2365" translate="yes" xml:space="preserve">
          <source>Method for initialization, defaults to &amp;lsquo;k-means++&amp;rsquo;:</source>
          <target state="translated">初始化方法，默认为'k-means ++'：</target>
        </trans-unit>
        <trans-unit id="3b1389e0e832a05337d6ccb31e50ea1425ca91a8" translate="yes" xml:space="preserve">
          <source>Method name</source>
          <target state="translated">方法名称</target>
        </trans-unit>
        <trans-unit id="b78ea13fd7ce3e01d82dac91ffffedca9d6a516f" translate="yes" xml:space="preserve">
          <source>Method of normalizing and converting singular vectors into biclusters. May be one of &amp;lsquo;scale&amp;rsquo;, &amp;lsquo;bistochastic&amp;rsquo;, or &amp;lsquo;log&amp;rsquo;. The authors recommend using &amp;lsquo;log&amp;rsquo;. If the data is sparse, however, log normalization will not work, which is why the default is &amp;lsquo;bistochastic&amp;rsquo;. CAUTION: if &lt;code&gt;method=&amp;rsquo;log&amp;rsquo;&lt;/code&gt;, the data must not be sparse.</source>
          <target state="translated">将奇异向量归一化并将其转换为双簇的方法。可以是&amp;ldquo;比例&amp;rdquo;，&amp;ldquo;双随机&amp;rdquo;或&amp;ldquo;对数&amp;rdquo;之一。作者建议使用&amp;ldquo;日志&amp;rdquo;。但是，如果数据稀疏，则日志规范化将不起作用，这就是默认值为&amp;ldquo;双随机&amp;rdquo;的原因。注意：如果 &lt;code&gt;method=&amp;rsquo;log&amp;rsquo;&lt;/code&gt; ，则数据不能稀疏。</target>
        </trans-unit>
        <trans-unit id="7e7b59d1db0b41f1f7de6a768474fa98a959edfd" translate="yes" xml:space="preserve">
          <source>Method to use in finding shortest path.</source>
          <target state="translated">用来寻找最短路径的方法。</target>
        </trans-unit>
        <trans-unit id="46674c498c855af96974ed544b15ae6396d6f74f" translate="yes" xml:space="preserve">
          <source>Method used to encode the transformed result.</source>
          <target state="translated">用于对转换结果进行编码的方法。</target>
        </trans-unit>
        <trans-unit id="155758829048f282b684f453070e5e32e8a3b098" translate="yes" xml:space="preserve">
          <source>Method used to initialize the procedure. Default: &amp;lsquo;nndsvd&amp;rsquo; if n_components &amp;lt; n_features, otherwise random. Valid options:</source>
          <target state="translated">用于初始化过程的方法。默认值：如果n_components &amp;lt;n_features，则为'nndsvd'，否则为随机数。有效选项：</target>
        </trans-unit>
        <trans-unit id="2c32a8fabfe7118c5a18a023b129a09f5774d869" translate="yes" xml:space="preserve">
          <source>Method used to update &lt;code&gt;_component&lt;/code&gt;. Only used in &lt;code&gt;fit&lt;/code&gt; method. In general, if the data size is large, the online update will be much faster than the batch update.</source>
          <target state="translated">用于更新 &lt;code&gt;_component&lt;/code&gt; 的方法。仅用于 &lt;code&gt;fit&lt;/code&gt; 方法。通常，如果数据量很大，则联机更新将比批处理更新快得多。</target>
        </trans-unit>
        <trans-unit id="7e4ac6803c9159c694f63d089cb06b2519c16aba" translate="yes" xml:space="preserve">
          <source>Methods</source>
          <target state="translated">Methods</target>
        </trans-unit>
        <trans-unit id="ef01ecfb88c8d650a45a85cec9ebc18d89f4ecbc" translate="yes" xml:space="preserve">
          <source>Metric used to compute the linkage. Can be &amp;ldquo;euclidean&amp;rdquo;, &amp;ldquo;l1&amp;rdquo;, &amp;ldquo;l2&amp;rdquo;, &amp;ldquo;manhattan&amp;rdquo;, &amp;ldquo;cosine&amp;rdquo;, or &amp;lsquo;precomputed&amp;rsquo;. If linkage is &amp;ldquo;ward&amp;rdquo;, only &amp;ldquo;euclidean&amp;rdquo; is accepted.</source>
          <target state="translated">用于计算链接的度量。可以是&amp;ldquo;欧几里得&amp;rdquo;，&amp;ldquo; l1&amp;rdquo;，&amp;ldquo; l2&amp;rdquo;，&amp;ldquo;曼哈顿&amp;rdquo;，&amp;ldquo;余弦&amp;rdquo;或&amp;ldquo;预计算&amp;rdquo;。如果链接为&amp;ldquo;病房&amp;rdquo;，则仅接受&amp;ldquo;欧几里得&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="b996dbf9b464efe667f55d3c7b947b9e2ffb345f" translate="yes" xml:space="preserve">
          <source>Metrics available for various machine learning tasks are detailed in sections below.</source>
          <target state="translated">各种机器学习任务的可用指标在下面的章节中详细介绍。</target>
        </trans-unit>
        <trans-unit id="276b36ad13c4507935dcfa6095085df1bf048be3" translate="yes" xml:space="preserve">
          <source>Michael E. Tipping: &lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;Sparse Bayesian Learning and the Relevance Vector Machine&lt;/a&gt;</source>
          <target state="translated">Michael E. Tipping：&lt;a href=&quot;http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf&quot;&gt;稀疏贝叶斯学习和相关向量机&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ee22c86ee428b82d33b13bdebced2deed71d63a1" translate="yes" xml:space="preserve">
          <source>Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)</source>
          <target state="translated">Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)</target>
        </trans-unit>
        <trans-unit id="f33a348553a7d85d27424bb525b1eca4fb8a5155" translate="yes" xml:space="preserve">
          <source>MinMaxScaler</source>
          <target state="translated">MinMaxScaler</target>
        </trans-unit>
        <trans-unit id="6fad9f3e5fbaefddf87807ab8e89f2398827a6e0" translate="yes" xml:space="preserve">
          <source>Mini-Batch K-Means clustering</source>
          <target state="translated">迷你批量K-Means聚类法</target>
        </trans-unit>
        <trans-unit id="8a7343b748199980306d06faf24494c5fb233c16" translate="yes" xml:space="preserve">
          <source>Mini-batch Sparse Principal Components Analysis</source>
          <target state="translated">小批量稀疏主成分分析</target>
        </trans-unit>
        <trans-unit id="4a04231399e807603297c55fa730ae6cac785e8b" translate="yes" xml:space="preserve">
          <source>Mini-batch dictionary learning</source>
          <target state="translated">小批量字典学习</target>
        </trans-unit>
        <trans-unit id="b36d4bb746673223e9f3eddf90497433b367472a" translate="yes" xml:space="preserve">
          <source>Mini-batch sparse PCA (&lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt;&lt;code&gt;MiniBatchSparsePCA&lt;/code&gt;&lt;/a&gt;) is a variant of &lt;a href=&quot;generated/sklearn.decomposition.sparsepca#sklearn.decomposition.SparsePCA&quot;&gt;&lt;code&gt;SparsePCA&lt;/code&gt;&lt;/a&gt; that is faster but less accurate. The increased speed is reached by iterating over small chunks of the set of features, for a given number of iterations.</source>
          <target state="translated">小批量稀疏PCA（&lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt; &lt;code&gt;MiniBatchSparsePCA&lt;/code&gt; &lt;/a&gt;）是的一个变种&lt;a href=&quot;generated/sklearn.decomposition.sparsepca#sklearn.decomposition.SparsePCA&quot;&gt; &lt;code&gt;SparsePCA&lt;/code&gt; &lt;/a&gt;是快，但不准确。对于给定的迭代次数，通过迭代功能部件的小块可以达到提高的速度。</target>
        </trans-unit>
        <trans-unit id="338b69eb058f4b8de205ae0e6a0b364261aebe7e" translate="yes" xml:space="preserve">
          <source>Minimizes the objective function:</source>
          <target state="translated">使目标函数最小化。</target>
        </trans-unit>
        <trans-unit id="84c971787220fb3e13d325cba22644ed7cfc6396" translate="yes" xml:space="preserve">
          <source>Minimizing Finite Sums with the Stochastic Average Gradient &lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;https://hal.inria.fr/hal-00860051/document&lt;/a&gt;</source>
          <target state="translated">用随机平均梯度最小化有限求和&lt;a href=&quot;https://hal.inria.fr/hal-00860051/document&quot;&gt;https://hal.inria.fr/hal-00860051/document&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="40d428add0b0bb2b15690324c7a65b1e95d21444" translate="yes" xml:space="preserve">
          <source>Minimum Covariance Determinant (MCD): robust estimator of covariance.</source>
          <target state="translated">最小协方差导数(MCD):协方差的鲁棒估计器。</target>
        </trans-unit>
        <trans-unit id="f0d923ebaec99475dba3ff68622a8b582426df2b" translate="yes" xml:space="preserve">
          <source>Minimum Covariance Determinant Estimator</source>
          <target state="translated">最小协方差决定因素估计器</target>
        </trans-unit>
        <trans-unit id="acdf76216ef7494ca3a405d1a4760970f1dcb045" translate="yes" xml:space="preserve">
          <source>Minimum correlation along the path. It corresponds to the regularization parameter alpha parameter in the Lasso.</source>
          <target state="translated">沿着路径的最小相关度。它对应于Lasso中的正则化参数alpha参数。</target>
        </trans-unit>
        <trans-unit id="45261c0e2275ffe7c782578b7e04c16d232cec64" translate="yes" xml:space="preserve">
          <source>Minimum number of candidates evaluated per estimator, assuming enough items meet the &lt;code&gt;min_hash_match&lt;/code&gt; constraint.</source>
          <target state="translated">假设有足够的项目满足 &lt;code&gt;min_hash_match&lt;/code&gt; 约束，则每个估算器评估的最小候选数。</target>
        </trans-unit>
        <trans-unit id="1f6032c543b0bedd4ef1a29303943c7332e81e08" translate="yes" xml:space="preserve">
          <source>Minimum number of samples chosen randomly from original data. Treated as an absolute number of samples for &lt;code&gt;min_samples &amp;gt;= 1&lt;/code&gt;, treated as a relative number &lt;code&gt;ceil(min_samples * X.shape[0]&lt;/code&gt;) for &lt;code&gt;min_samples &amp;lt; 1&lt;/code&gt;. This is typically chosen as the minimal number of samples necessary to estimate the given &lt;code&gt;base_estimator&lt;/code&gt;. By default a &lt;code&gt;sklearn.linear_model.LinearRegression()&lt;/code&gt; estimator is assumed and &lt;code&gt;min_samples&lt;/code&gt; is chosen as &lt;code&gt;X.shape[1] + 1&lt;/code&gt;.</source>
          <target state="translated">从原始数据中随机选择的最小样本数。当作用于样本的绝对数量 &lt;code&gt;min_samples &amp;gt;= 1&lt;/code&gt; ，作为相对数处理 &lt;code&gt;ceil(min_samples * X.shape[0]&lt;/code&gt; ），用于 &lt;code&gt;min_samples &amp;lt; 1&lt;/code&gt; 。通常将其选择为估计给定 &lt;code&gt;base_estimator&lt;/code&gt; 所需的最少样本数。默认情况下，假定 &lt;code&gt;sklearn.linear_model.LinearRegression()&lt;/code&gt; 估计器，并将 &lt;code&gt;min_samples&lt;/code&gt; 选择为 &lt;code&gt;X.shape[1] + 1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="3a6bb55043794a0a93e8ff0524f08e79cbc35225" translate="yes" xml:space="preserve">
          <source>Minimum value of a bicluster.</source>
          <target state="translated">双集群的最小值。</target>
        </trans-unit>
        <trans-unit id="59e81ca4cbc76e95e029c93b9fa76bb8c2828a22" translate="yes" xml:space="preserve">
          <source>Minimum value of input array &lt;code&gt;X_&lt;/code&gt; for left bound.</source>
          <target state="translated">左边界的输入数组 &lt;code&gt;X_&lt;/code&gt; 的最小值。</target>
        </trans-unit>
        <trans-unit id="2b5d457149fe5be167ed99387c99dd4725835fe8" translate="yes" xml:space="preserve">
          <source>MinkowskiDistance</source>
          <target state="translated">MinkowskiDistance</target>
        </trans-unit>
        <trans-unit id="f3547bc4550b1de5a83615a3b3bc38cc23770ce0" translate="yes" xml:space="preserve">
          <source>Mirrors &lt;code&gt;class_log_prior_&lt;/code&gt; for interpreting MultinomialNB as a linear model.</source>
          <target state="translated">镜像 &lt;code&gt;class_log_prior_&lt;/code&gt; ，以将MultinomialNB解释为线性模型。</target>
        </trans-unit>
        <trans-unit id="7814bd45bfd54f380e5f0cd3f0461e627752ef7b" translate="yes" xml:space="preserve">
          <source>Mirrors &lt;code&gt;feature_log_prob_&lt;/code&gt; for interpreting MultinomialNB as a linear model.</source>
          <target state="translated">镜像 &lt;code&gt;feature_log_prob_&lt;/code&gt; 以便将MultinomialNB解释为线性模型。</target>
        </trans-unit>
        <trans-unit id="e446df504bb1a7ba9afc2f86aa7e483abdbc1937" translate="yes" xml:space="preserve">
          <source>Missing Attribute Values:</source>
          <target state="translated">缺少属性值。</target>
        </trans-unit>
        <trans-unit id="391b82fea55e1ed8699fe6e91400e8d6055ab0df" translate="yes" xml:space="preserve">
          <source>Missing values can be replaced by the mean, the median or the most frequent value using the basic &lt;a href=&quot;../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt;&lt;/a&gt;. The median is a more robust estimator for data with high magnitude variables which could dominate results (otherwise known as a &amp;lsquo;long tail&amp;rsquo;).</source>
          <target state="translated">可以使用基本&lt;a href=&quot;../modules/generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;sklearn.impute.SimpleImputer&lt;/code&gt; &lt;/a&gt;将缺失值替换为平均值，中位数或最频繁的值。中位数是具有高强度变量的数据的更可靠的估计器，这些变量可能会主导结果（也称为&amp;ldquo;长尾巴&amp;rdquo;）。</target>
        </trans-unit>
        <trans-unit id="7657a2d6545adb4955b10f53c4131bc5602e90eb" translate="yes" xml:space="preserve">
          <source>Missing values in the &amp;lsquo;data&amp;rsquo; are represented as NaN&amp;rsquo;s. Missing values in &amp;lsquo;target&amp;rsquo; are represented as NaN&amp;rsquo;s (numerical target) or None (categorical target)</source>
          <target state="translated">&amp;ldquo;数据&amp;rdquo;中的缺失值表示为NaN。&amp;ldquo;目标&amp;rdquo;中的缺失值表示为NaN（数字目标）或&amp;ldquo;无&amp;rdquo;（类别目标）</target>
        </trans-unit>
        <trans-unit id="6a6932c856f91eed3dd44780fa6b9fe69490c4b8" translate="yes" xml:space="preserve">
          <source>Mixin class for all bicluster estimators in scikit-learn</source>
          <target state="translated">scikit-learn中所有双聚类估计器的mixin类。</target>
        </trans-unit>
        <trans-unit id="2c10e3ce37d297d342507e753914a98859330d14" translate="yes" xml:space="preserve">
          <source>Mixin class for all classifiers in scikit-learn.</source>
          <target state="translated">scikit-learn中所有分类器的mixin类。</target>
        </trans-unit>
        <trans-unit id="5fa39e3354bc95759f1ac752182b15d93d7771cd" translate="yes" xml:space="preserve">
          <source>Mixin class for all cluster estimators in scikit-learn.</source>
          <target state="translated">scikit-learn中所有集群估计器的mixin类。</target>
        </trans-unit>
        <trans-unit id="eb8addc65b16d7fa21479da43bfb0ac745b8fd54" translate="yes" xml:space="preserve">
          <source>Mixin class for all density estimators in scikit-learn.</source>
          <target state="translated">scikit-learn中所有密度估计器的mixin类。</target>
        </trans-unit>
        <trans-unit id="6ac045bb154d5d0dbd1cc9d29eba911e1ea2781a" translate="yes" xml:space="preserve">
          <source>Mixin class for all regression estimators in scikit-learn.</source>
          <target state="translated">scikit-learn中所有回归估计器的mixin类。</target>
        </trans-unit>
        <trans-unit id="f73bd7177b7212616f9ae4cd764e784bac6a7ce1" translate="yes" xml:space="preserve">
          <source>Mixin class for all transformers in scikit-learn.</source>
          <target state="translated">在scikit-learn中,所有变压器的mixin类。</target>
        </trans-unit>
        <trans-unit id="4d9a44acff48ccb4a2b026d4835ebe86a95495fc" translate="yes" xml:space="preserve">
          <source>Model Complexity Influence</source>
          <target state="translated">模型复杂度影响</target>
        </trans-unit>
        <trans-unit id="7d5e06ce8e5a0fb1e8e99aee47dbf3426de865fe" translate="yes" xml:space="preserve">
          <source>Model Selection Interface</source>
          <target state="translated">模型选择界面</target>
        </trans-unit>
        <trans-unit id="d9b7f2bb0f8fc0d29940e1aefd1565fcc5b449f7" translate="yes" xml:space="preserve">
          <source>Model blending: When predictions of one supervised estimator are used to train another estimator in ensemble methods.</source>
          <target state="translated">模型混合。当一个有监督的估计器的预测被用于训练另一个估计器时,在集合方法中。</target>
        </trans-unit>
        <trans-unit id="c3b027b1bc55171725d0853107d2cd63b70cf1b0" translate="yes" xml:space="preserve">
          <source>Model complexity</source>
          <target state="translated">模型复杂性</target>
        </trans-unit>
        <trans-unit id="088cfdc97cd06f5c2647d7bc4d07170997a1804d" translate="yes" xml:space="preserve">
          <source>Model compression in scikit-learn only concerns linear models for the moment. In this context it means that we want to control the model sparsity (i.e. the number of non-zero coordinates in the model vectors). It is generally a good idea to combine model sparsity with sparse input data representation.</source>
          <target state="translated">scikit-learn中的模型压缩目前只涉及线性模型。在这种情况下,这意味着我们要控制模型的稀疏性(即模型向量中非零坐标的数量)。一般来说,将模型稀疏性与稀疏的输入数据表示结合起来是个好主意。</target>
        </trans-unit>
        <trans-unit id="12cb4d758358636a28aab0195639a05c4c6adf09" translate="yes" xml:space="preserve">
          <source>Model persistence</source>
          <target state="translated">模型持久性</target>
        </trans-unit>
        <trans-unit id="c38101ec23202ddb2bcf9ed4925ad6d1c9181511" translate="yes" xml:space="preserve">
          <source>Model reshaping consists in selecting only a portion of the available features to fit a model. In other words, if a model discards features during the learning phase we can then strip those from the input. This has several benefits. Firstly it reduces memory (and therefore time) overhead of the model itself. It also allows to discard explicit feature selection components in a pipeline once we know which features to keep from a previous run. Finally, it can help reduce processing time and I/O usage upstream in the data access and feature extraction layers by not collecting and building features that are discarded by the model. For instance if the raw data come from a database, it can make it possible to write simpler and faster queries or reduce I/O usage by making the queries return lighter records. At the moment, reshaping needs to be performed manually in scikit-learn. In the case of sparse input (particularly in &lt;code&gt;CSR&lt;/code&gt; format), it is generally sufficient to not generate the relevant features, leaving their columns empty.</source>
          <target state="translated">模型重塑包括仅选择一部分可用特征以适合模型。换句话说，如果模型在学习阶段放弃了特征，那么我们可以从输入中去除那些特征。这有几个好处。首先，它减少了模型本身的内存（从而减少了时间）开销。一旦我们知道上次运行要保留哪些功能，它还允许丢弃管道中的显式功能选择组件。最后，它可以通过不收集和构建模型丢弃的特征来帮助减少数据访问和特征提取层中上游的处理时间和I / O使用率。例如，如果原始数据来自数据库，则可以通过使查询返回更轻的记录来编写更简单，更快速的查询或减少I / O使用量。在这一刻，重塑需要在scikit-learn中手动执行。在稀疏输入的情况下（尤其是在 &lt;code&gt;CSR&lt;/code&gt; 格式），通常只要不生成相关功能就足够了，而不必将其列留空。</target>
        </trans-unit>
        <trans-unit id="12aba00cd9b6d07b68e1c4795de07ac9d7b738d7" translate="yes" xml:space="preserve">
          <source>Model selection and evaluation using tools, such as &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;model_selection.GridSearchCV&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;model_selection.cross_val_score&lt;/code&gt;&lt;/a&gt;, take a &lt;code&gt;scoring&lt;/code&gt; parameter that controls what metric they apply to the estimators evaluated.</source>
          <target state="translated">使用诸如&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;model_selection.GridSearchCV&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;model_selection.cross_val_score&lt;/code&gt; 之&lt;/a&gt;类的工具进行模型选择和评估，会采用一个 &lt;code&gt;scoring&lt;/code&gt; 参数，该参数控制它们对所评估的估算器应用何种度量。</target>
        </trans-unit>
        <trans-unit id="855cd5c7a76f661266d80a6648a2f964caf6a19e" translate="yes" xml:space="preserve">
          <source>Model selection by evaluating various parameter settings can be seen as a way to use the labeled data to &amp;ldquo;train&amp;rdquo; the parameters of the grid.</source>
          <target state="translated">通过评估各种参数设置进行的模型选择可以看作是使用标记数据&amp;ldquo;训练&amp;rdquo;网格参数的一种方法。</target>
        </trans-unit>
        <trans-unit id="89917070f2baaaaf3d7a4bc37b23fe9e19c05135" translate="yes" xml:space="preserve">
          <source>Model selection with Probabilistic PCA and Factor Analysis (FA)</source>
          <target state="translated">利用概率PCA和因子分析(FA)进行模型选择。</target>
        </trans-unit>
        <trans-unit id="9c731d5ab6adf4a98df3f1383f23cf8f7eed3338" translate="yes" xml:space="preserve">
          <source>Model selection without nested CV uses the same data to tune model parameters and evaluate model performance. Information may thus &amp;ldquo;leak&amp;rdquo; into the model and overfit the data. The magnitude of this effect is primarily dependent on the size of the dataset and the stability of the model. See Cawley and Talbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; for an analysis of these issues.</source>
          <target state="translated">没有嵌套CV的模型选择使用相同的数据来调整模型参数并评估模型性能。因此，信息可能会&amp;ldquo;渗入&amp;rdquo;模型并过度拟合数据。这种影响的大小主要取决于数据集的大小和模型的稳定性。有关这些问题的分析，请参见Cawley和Talbot &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="74392d3518eac75d4c193fc75b5f4945cf9be3f9" translate="yes" xml:space="preserve">
          <source>Model selection: choosing estimators and their parameters</source>
          <target state="translated">模型选择:选择估计器及其参数</target>
        </trans-unit>
        <trans-unit id="ad79a801df8015a66d5501cf36f7ffcd2a41ddf8" translate="yes" xml:space="preserve">
          <source>Model validation</source>
          <target state="translated">模型验证</target>
        </trans-unit>
        <trans-unit id="04c7998384d3cc95ade6e27711f83c95af26806e" translate="yes" xml:space="preserve">
          <source>Modeling species&amp;rsquo; geographic distributions is an important problem in conservation biology. In this example we model the geographic distribution of two south american mammals given past observations and 14 environmental variables. Since we have only positive examples (there are no unsuccessful observations), we cast this problem as a density estimation problem and use the &lt;code&gt;OneClassSVM&lt;/code&gt; provided by the package &lt;code&gt;sklearn.svm&lt;/code&gt; as our modeling tool. The dataset is provided by Phillips et. al. (2006). If available, the example uses &lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;basemap&lt;/a&gt; to plot the coast lines and national boundaries of South America.</source>
          <target state="translated">对物种的地理分布进行建模是保护生物学中的一个重要问题。在此示例中，我们根据过去的观察结果和14个环境变量对两个南美哺乳动物的地理分布进行了建模。由于我们只有正面的例子（没有不成功的意见），我们投这个问题是一个密度估计问题，并使用 &lt;code&gt;OneClassSVM&lt;/code&gt; 由包提供 &lt;code&gt;sklearn.svm&lt;/code&gt; 作为我们的建模工具。数据集由Phillips等提供。等 （2006）。如果可用，该示例将使用&lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;底图&lt;/a&gt;来绘制南美的海岸线和国家边界。</target>
        </trans-unit>
        <trans-unit id="41be465b762359b2fa053c297894959404d2b4b5" translate="yes" xml:space="preserve">
          <source>Module &lt;a href=&quot;#module-sklearn.kernel_ridge&quot;&gt;&lt;code&gt;sklearn.kernel_ridge&lt;/code&gt;&lt;/a&gt; implements kernel ridge regression.</source>
          <target state="translated">模块&lt;a href=&quot;#module-sklearn.kernel_ridge&quot;&gt; &lt;code&gt;sklearn.kernel_ridge&lt;/code&gt; &lt;/a&gt;实现内核岭回归。</target>
        </trans-unit>
        <trans-unit id="7c19bb73223842069c348f5ce2be56f6bdc47336" translate="yes" xml:space="preserve">
          <source>Momentum for gradient descent update. Should be between 0 and 1. Only used when solver=&amp;rsquo;sgd&amp;rsquo;.</source>
          <target state="translated">梯度下降更新的动量。应该在0到1之间。仅在solver ='sgd'时使用。</target>
        </trans-unit>
        <trans-unit id="afdb29f8a2a5c8088f948d58228ab92ec4b8dbc8" translate="yes" xml:space="preserve">
          <source>Moosmann, F. and Triggs, B. and Jurie, F. &amp;ldquo;Fast discriminative visual codebooks using randomized clustering forests&amp;rdquo; NIPS 2007</source>
          <target state="translated">Moosmann，F.和Triggs，B.和Jurie，F.&amp;ldquo;使用随机聚类森林的快速判别式视觉密码本&amp;rdquo; NIPS 2007</target>
        </trans-unit>
        <trans-unit id="ea951c164724999b1e82491617fa7550c41c4ea4" translate="yes" xml:space="preserve">
          <source>More details can be found in the article &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Bayesian Interpolation&lt;/a&gt; by MacKay, David J. C.</source>
          <target state="translated">可以在MacKay的David JC的&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;贝叶斯插值&lt;/a&gt;文章中找到更多详细信息。</target>
        </trans-unit>
        <trans-unit id="5be68ba5f49b8c23c2004cef7b8f1738816fd7ff" translate="yes" xml:space="preserve">
          <source>More details can be found in the documentation of &lt;a href=&quot;sgd&quot;&gt;SGD&lt;/a&gt;</source>
          <target state="translated">可以在&lt;a href=&quot;sgd&quot;&gt;SGD&lt;/a&gt;的文档中找到更多详细信息</target>
        </trans-unit>
        <trans-unit id="eeddded239db4ba79d40ed239189aa60eea25acb" translate="yes" xml:space="preserve">
          <source>More details on tools available for model selection can be found in the sections on &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;Cross-validation: evaluating estimator performance&lt;/a&gt; and &lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt;.</source>
          <target state="translated">有关可用于模型选择的工具的更多详细信息，请参见&amp;ldquo; &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;交叉验证：评估估计器性能&lt;/a&gt;和&lt;a href=&quot;../../modules/grid_search#grid-search&quot;&gt;调整&lt;/a&gt;估计器的超参数&amp;rdquo;部分。</target>
        </trans-unit>
        <trans-unit id="d431b615f9733586de025b4f9872bf1a8badc9bd" translate="yes" xml:space="preserve">
          <source>More formally, the responsibility of a sample \(k\) to be the exemplar of sample \(i\) is given by:</source>
          <target state="translated">更正式地说,样本(k)作为样本(i)的典范的责任是:</target>
        </trans-unit>
        <trans-unit id="e5fcb8eb05185b2ae6a7561ecfd54e7e78d1825d" translate="yes" xml:space="preserve">
          <source>More formally, we define a core sample as being a sample in the dataset such that there exist &lt;code&gt;min_samples&lt;/code&gt; other samples within a distance of &lt;code&gt;eps&lt;/code&gt;, which are defined as &lt;em&gt;neighbors&lt;/em&gt; of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of &lt;em&gt;their&lt;/em&gt; neighbors that are core samples, and so on. A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples. Intuitively, these samples are on the fringes of a cluster.</source>
          <target state="translated">更正式地说，我们将核心样本定义为数据集中的样本，以便在 &lt;code&gt;eps&lt;/code&gt; 距离内存在 &lt;code&gt;min_samples&lt;/code&gt; 其他样本，这些样本被定义为核心样本的&lt;em&gt;邻居&lt;/em&gt;。这告诉我们核心样本位于向量空间的密集区域。群集是一组核心样本，可以通过递归取岩芯样品，发现其所有的邻居都岩芯样品，发现所有的构建&lt;em&gt;自己的&lt;/em&gt;邻居是岩心样品，等等。集群还具有一组非核心样本，这些样本是集群中核心样本的邻居，但本身不是核心样本。直观地讲，这些样本位于群集的边缘。&lt;em&gt;&lt;/em&gt;&lt;em&gt;&lt;/em&gt;</target>
        </trans-unit>
        <trans-unit id="39d3fe53d51c5218d78f036015830e2502c27f2b" translate="yes" xml:space="preserve">
          <source>More generally, when the accuracy of a classifier is too close to random, it probably means that something went wrong: features are not helpful, a hyperparameter is not correctly tuned, the classifier is suffering from class imbalance, etc&amp;hellip;</source>
          <target state="translated">更一般而言，当分类器的准确性太接近随机性时，这可能意味着出了点问题：功能无济于事，超参数未正确调整，分类器正遭受类不平衡之苦，等等&amp;hellip;&amp;hellip;</target>
        </trans-unit>
        <trans-unit id="63b4a4241c78c35e803f9a1e6a808d1813f2fb30" translate="yes" xml:space="preserve">
          <source>More information can be found on the &lt;a href=&quot;http://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;Scipy install page&lt;/a&gt; and in this &lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;blog post&lt;/a&gt; from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu.</source>
          <target state="translated">更多信息可以在找到&lt;a href=&quot;http://docs.scipy.org/doc/numpy/user/install.html&quot;&gt;SciPy的安装页面&lt;/a&gt;，并在此&lt;a href=&quot;http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/&quot;&gt;博客文章&lt;/a&gt;从丹尼尔&amp;middot;努里其中有一些很好的一步一步安装的Debian / Ubuntu的指令。</target>
        </trans-unit>
        <trans-unit id="5d4e98f0a8d8595ea60691c38a1427dece6499fb" translate="yes" xml:space="preserve">
          <source>More metadata from OpenML</source>
          <target state="translated">更多来自OpenML的元数据</target>
        </trans-unit>
        <trans-unit id="12db8232292ca8d1cf35bc6b9168f2c8b63d47ca" translate="yes" xml:space="preserve">
          <source>More precisely its the expectation of the target response after accounting for the initial model; partial dependence plots do not include the &lt;code&gt;init&lt;/code&gt; model.</source>
          <target state="translated">更准确地说，是在考虑了初始模型后对目标响应的期望；部分依赖图不包括 &lt;code&gt;init&lt;/code&gt; 模型。</target>
        </trans-unit>
        <trans-unit id="73794f226fb348eb5da6ad63afeb16cb41727d95" translate="yes" xml:space="preserve">
          <source>More readable code, in particular since it avoids constructing list of arguments.</source>
          <target state="translated">更易读的代码,特别是它避免了构造参数列表。</target>
        </trans-unit>
        <trans-unit id="a22dda2285328f04695cb396d42b64909dfc0d90" translate="yes" xml:space="preserve">
          <source>More specifically, for linear and quadratic discriminant analysis, \(P(X|y)\) is modeled as a multivariate Gaussian distribution with density:</source>
          <target state="translated">更具体地说,对于线性和二次判别分析,/(P(X|y)/)被建模为具有密度的多变量高斯分布。</target>
        </trans-unit>
        <trans-unit id="79ecb6d9275fdfeccdbd69d6aa3919b92952032e" translate="yes" xml:space="preserve">
          <source>Most commonly, disparities are set to \(\hat{d}_{ij} = b S_{ij}\).</source>
          <target state="translated">最常见的是,将差距设定为/(hat{d}_{ij}=b S_{ij})。</target>
        </trans-unit>
        <trans-unit id="13411f05832555677b503d8db5e64b4930c99086" translate="yes" xml:space="preserve">
          <source>Most of the variance can be explained by a bell-shaped curve of width effective_rank: the low rank part of the singular values profile is:</source>
          <target state="translated">大部分方差可以用宽度为effective_rank的钟形曲线来解释:奇异值曲线的低秩部分是。</target>
        </trans-unit>
        <trans-unit id="9a311c70d6fa85e99fb6533c84253a4d2c760cf7" translate="yes" xml:space="preserve">
          <source>Most scikit-learn models are usually pretty fast as they are implemented either with compiled Cython extensions or optimized computing libraries. On the other hand, in many real world applications the feature extraction process (i.e. turning raw data like database rows or network packets into numpy arrays) governs the overall prediction time. For example on the Reuters text classification task the whole preparation (reading and parsing SGML files, tokenizing the text and hashing it into a common vector space) is taking 100 to 500 times more time than the actual prediction code, depending on the chosen model.</source>
          <target state="translated">大多数scikit-learn模型通常是相当快的,因为它们是通过编译的Cython扩展或优化的计算库实现的。另一方面,在许多实际应用中,特征提取过程(即把数据库行或网络数据包等原始数据转化为numpy数组)控制着整个预测时间。例如在路透社的文本分类任务中,整个准备工作(读取和解析SGML文件,将文本标记化,并将其散列到一个通用的向量空间中)所花费的时间是实际预测代码的100到500倍,这取决于所选择的模型。</target>
        </trans-unit>
        <trans-unit id="1f57c7d2294fbf421c865e0ff805433c9e9164a6" translate="yes" xml:space="preserve">
          <source>Most treatments of LSA in the natural language processing (NLP) and information retrieval (IR) literature swap the axes of the matrix \(X\) so that it has shape &lt;code&gt;n_features&lt;/code&gt; &amp;times; &lt;code&gt;n_samples&lt;/code&gt;. We present LSA in a different way that matches the scikit-learn API better, but the singular values found are the same.</source>
          <target state="translated">在自然语言处理（NLP）和信息检索（IR）文献中，大多数LSA处理都交换矩阵\（X \）的轴，因此其形状为 &lt;code&gt;n_features&lt;/code&gt; &amp;times; &lt;code&gt;n_samples&lt;/code&gt; 。我们以另一种方式更好地匹配scikit-learn API来提供LSA，但是找到的奇异值是相同的。</target>
        </trans-unit>
        <trans-unit id="56ac69cc3d5e8e713d723baf0656a6eefef8f81b" translate="yes" xml:space="preserve">
          <source>Multi target classification</source>
          <target state="translated">多目标分类</target>
        </trans-unit>
        <trans-unit id="b9b406b23aa7207ecf1f2aef41fc5c5ad0ba0c31" translate="yes" xml:space="preserve">
          <source>Multi target regression</source>
          <target state="translated">多目标回归</target>
        </trans-unit>
        <trans-unit id="332c064d1606c8de1a2522f9e4ee668dee56478e" translate="yes" xml:space="preserve">
          <source>Multi-class AdaBoosted Decision Trees</source>
          <target state="translated">多类AdaBoosted决策树。</target>
        </trans-unit>
        <trans-unit id="d384b7095ac166d1b587c1dafb0cadad28beb4c2" translate="yes" xml:space="preserve">
          <source>Multi-class targets.</source>
          <target state="translated">多类目标。</target>
        </trans-unit>
        <trans-unit id="3243798e9c1a783043187bb0ea60ba4b8d0dfc62" translate="yes" xml:space="preserve">
          <source>Multi-class targets. An indicator matrix turns on multilabel classification.</source>
          <target state="translated">多类目标。指标矩阵开启多标签分类。</target>
        </trans-unit>
        <trans-unit id="552ba9a8fb8ef0b9cf8d9ea68e7f0c182ae9af5e" translate="yes" xml:space="preserve">
          <source>Multi-dimensional scaling</source>
          <target state="translated">多维缩放</target>
        </trans-unit>
        <trans-unit id="9815dac6e8971893d838904dc7e1cfe16372af94" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron classifier.</source>
          <target state="translated">多层Perceptron分类器。</target>
        </trans-unit>
        <trans-unit id="b994a134c1a31489af71fc772bdcadb38a217ddf" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0, 1] or [-1, +1], or standardize it to have mean 0 and variance 1. Note that you must apply the &lt;em&gt;same&lt;/em&gt; scaling to the test set for meaningful results. You can use &lt;code&gt;StandardScaler&lt;/code&gt; for standardization.</source>
          <target state="translated">多层Perceptron对特征缩放很敏感，因此强烈建议对数据进行缩放。例如，将输入向量X上的每个属性缩放为[0，1]或[-1，+1]，或将其标准化为均值0和方差1。请注意，您必须对测试集应用&lt;em&gt;相同的&lt;/em&gt;缩放比例有意义的结果。您可以使用 &lt;code&gt;StandardScaler&lt;/code&gt; 进行标准化。</target>
        </trans-unit>
        <trans-unit id="8b22895cdf3840f5acfe1ac32cbc8961e8fd336a" translate="yes" xml:space="preserve">
          <source>Multi-layer Perceptron regressor.</source>
          <target state="translated">多层感知器回归器。</target>
        </trans-unit>
        <trans-unit id="dc72474a07afc8bb8057ac9bf6d8fbc65e56a63e" translate="yes" xml:space="preserve">
          <source>Multi-output Decision Tree Regression</source>
          <target state="translated">多输出决策树回归</target>
        </trans-unit>
        <trans-unit id="2627f8f7a5d9294ea8dcfe47a04508977edd8f7c" translate="yes" xml:space="preserve">
          <source>Multi-output targets predicted across multiple predictors. Note: Separate models are generated for each predictor.</source>
          <target state="translated">跨多个预测器预测的多产出目标。注:为每个预测器生成单独的模型。</target>
        </trans-unit>
        <trans-unit id="4da1e42d60732d934b60458ac859ed1253e6bfbd" translate="yes" xml:space="preserve">
          <source>Multi-output targets.</source>
          <target state="translated">多产出目标。</target>
        </trans-unit>
        <trans-unit id="d25d7d780166f0481648cccd463a78a5e417f6f3" translate="yes" xml:space="preserve">
          <source>Multi-output targets. An indicator matrix turns on multilabel estimation.</source>
          <target state="translated">多输出目标。指标矩阵开启多标签估计。</target>
        </trans-unit>
        <trans-unit id="775030d60513b2f729789206b06d021b6661e16d" translate="yes" xml:space="preserve">
          <source>Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer</source>
          <target state="translated">以L1/L2混合正则化训练的多任务ElasticNet模型。</target>
        </trans-unit>
        <trans-unit id="7259143bf01ac8062e2b5725644f1e005f808315" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 ElasticNet with built-in cross-validation.</source>
          <target state="translated">多任务L1/L2 ElasticNet,内置交叉验证。</target>
        </trans-unit>
        <trans-unit id="5c1ad40e838b03e514631adae1736168414d6605" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 Lasso with built-in cross-validation</source>
          <target state="translated">多任务L1/L2 Lasso,内置交叉验证。</target>
        </trans-unit>
        <trans-unit id="a743aa48cf046a15087b0f4886f436159c359dd5" translate="yes" xml:space="preserve">
          <source>Multi-task L1/L2 Lasso with built-in cross-validation.</source>
          <target state="translated">多任务L1/L2 Lasso,内置交叉验证。</target>
        </trans-unit>
        <trans-unit id="6377873684d0ac47f9792cf0130c074e6b5d5c8f" translate="yes" xml:space="preserve">
          <source>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer</source>
          <target state="translated">以L1/L2混合正则化训练的多任务Lasso模型。</target>
        </trans-unit>
        <trans-unit id="669e809a0e7044a9302d0da3188c44feddafe180" translate="yes" xml:space="preserve">
          <source>Multiclass and multilabel classification strategies</source>
          <target state="translated">多类和多标签分类策略</target>
        </trans-unit>
        <trans-unit id="957cc5ae23e389ffa9c767fc16d7ac37036b0153" translate="yes" xml:space="preserve">
          <source>Multiclass classification</source>
          <target state="translated">多级分类</target>
        </trans-unit>
        <trans-unit id="f43fb647f0e5eccf5a3760b6eafe5e21b79a50a6" translate="yes" xml:space="preserve">
          <source>Multiclass probability estimates are derived from binary (one-vs.-rest) estimates by simple normalization, as recommended by Zadrozny and Elkan.</source>
          <target state="translated">如Zadrozny和Elkan所推荐的那样,多类概率估计是通过简单的归一化从二元(一与休息)估计中得出的。</target>
        </trans-unit>
        <trans-unit id="3debc5753cd55840fa65a540929e237591ad2faf" translate="yes" xml:space="preserve">
          <source>Multiclass settings</source>
          <target state="translated">多类设置</target>
        </trans-unit>
        <trans-unit id="e3f8736465f26b4a50bfa9739f8adbcfb24ccc56" translate="yes" xml:space="preserve">
          <source>Multiclass sparse logisitic regression on newgroups20</source>
          <target state="translated">对新群体的多类稀疏对数回归20</target>
        </trans-unit>
        <trans-unit id="38a70920d0cd2001f4ef8f9ee41dfa6b122f018c" translate="yes" xml:space="preserve">
          <source>Multiclass spectral clustering, 2003 Stella X. Yu, Jianbo Shi &lt;a href=&quot;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</source>
          <target state="translated">多类光谱聚类，2003年Stella X.Yu，Shijianbo &lt;a href=&quot;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&quot;&gt;http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c5d0b14c4e8dd95e44e1cd37847a8b6674049750" translate="yes" xml:space="preserve">
          <source>Multiclass vs. multilabel fitting</source>
          <target state="translated">多类与多标签拟合</target>
        </trans-unit>
        <trans-unit id="dbc4079d7d6495ef3cfc4fdaba01141075b60d89" translate="yes" xml:space="preserve">
          <source>Multidimensional scaling</source>
          <target state="translated">多维缩放</target>
        </trans-unit>
        <trans-unit id="7c33b81ffc3ca04c62af4f5074ba33e510028ebd" translate="yes" xml:space="preserve">
          <source>Multilabel classification</source>
          <target state="translated">多标签分类</target>
        </trans-unit>
        <trans-unit id="c720ba81272f13af125e464e56bd5648c8146ada" translate="yes" xml:space="preserve">
          <source>Multilabel ranking metrics</source>
          <target state="translated">多标签排名指标</target>
        </trans-unit>
        <trans-unit id="b6031d58e46d313eca93045d9598faea168256f9" translate="yes" xml:space="preserve">
          <source>Multimetric scoring can either be specified as a list of strings of predefined scores names or a dict mapping the scorer name to the scorer function and/or the predefined scorer name(s). See &lt;a href=&quot;model_evaluation#multimetric-scoring&quot;&gt;Using multiple metric evaluation&lt;/a&gt; for more details.</source>
          <target state="translated">可以将多度量计分指定为预定义分数名称的字符串列表，也可以将dict映射为计分者名称与计分器功能和/或预定义计分者名称。有关更多详细信息，请参见&lt;a href=&quot;model_evaluation#multimetric-scoring&quot;&gt;使用多重度量评估&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="71e5ed6f7fb13f64a7d1e47fd6ffef12dfd5580e" translate="yes" xml:space="preserve">
          <source>Multinomial + L1 penalty</source>
          <target state="translated">多项式+L1罚款</target>
        </trans-unit>
        <trans-unit id="82d79c421161a2e0a31300a79127c9804ae62ed5" translate="yes" xml:space="preserve">
          <source>Multinomial + L2 penalty</source>
          <target state="translated">多项式+二级惩罚</target>
        </trans-unit>
        <trans-unit id="efccef2252a812759badf849dd9e2acd4cd7eb95" translate="yes" xml:space="preserve">
          <source>Multinomial deviance (&lt;code&gt;'deviance'&lt;/code&gt;): The negative multinomial log-likelihood loss function for multi-class classification with &lt;code&gt;n_classes&lt;/code&gt; mutually exclusive classes. It provides probability estimates. The initial model is given by the prior probability of each class. At each iteration &lt;code&gt;n_classes&lt;/code&gt; regression trees have to be constructed which makes GBRT rather inefficient for data sets with a large number of classes.</source>
          <target state="translated">多项式偏差（ &lt;code&gt;'deviance'&lt;/code&gt; ）：具有 &lt;code&gt;n_classes&lt;/code&gt; 互斥类的多类分类的负多项式对数似然损失函数。它提供了概率估计。初始模型由每个类别的先验概率给出。在每次迭代中，都必须构造 &lt;code&gt;n_classes&lt;/code&gt; 回归树，这使得GBRT对于具有大量类的数据集而言效率很低。</target>
        </trans-unit>
        <trans-unit id="313293589005fec34a4137f7e7a462e44753a91e" translate="yes" xml:space="preserve">
          <source>Multioutput classification support can be added to any classifier with &lt;code&gt;MultiOutputClassifier&lt;/code&gt;. This strategy consists of fitting one classifier per target. This allows multiple target variable classifications. The purpose of this class is to extend estimators to be able to estimate a series of target functions (f1,f2,f3&amp;hellip;,fn) that are trained on a single X predictor matrix to predict a series of responses (y1,y2,y3&amp;hellip;,yn).</source>
          <target state="translated">可以使用MultiOutputClassifier将多输出分类支持添加到任何分类 &lt;code&gt;MultiOutputClassifier&lt;/code&gt; 。该策略包括为每个目标配备一个分类器。这允许多个目标变量分类。此类的目的是扩展估计器，以便能够估计在单个X预测矩阵上训练的一系列目标函数（f1，f2，f3 ...，fn），以预测一系列响应（y1，y2，y3 &amp;hellip;，yn）。</target>
        </trans-unit>
        <trans-unit id="086b68ade408f93caaac80f71e1eabb4cc44f3fd" translate="yes" xml:space="preserve">
          <source>Multioutput regression support can be added to any regressor with &lt;code&gt;MultiOutputRegressor&lt;/code&gt;. This strategy consists of fitting one regressor per target. Since each target is represented by exactly one regressor it is possible to gain knowledge about the target by inspecting its corresponding regressor. As &lt;code&gt;MultiOutputRegressor&lt;/code&gt; fits one regressor per target it can not take advantage of correlations between targets.</source>
          <target state="translated">多输出回归支持可以通过MultiOutputRegressor添加到任何回归 &lt;code&gt;MultiOutputRegressor&lt;/code&gt; 。该策略包括为每个目标安装一个回归器。由于每个目标仅由一个回归变量表示，因此可以通过检查其相应的回归变量来获取有关目标的知识。由于 &lt;code&gt;MultiOutputRegressor&lt;/code&gt; 每个目标可容纳一个回归器，因此无法利用目标之间的相关性。</target>
        </trans-unit>
        <trans-unit id="96e252b1f2ecf6cba5d585af259eddb308663e2e" translate="yes" xml:space="preserve">
          <source>Multiple metric evaluation using &lt;code&gt;cross_validate&lt;/code&gt; (please refer the &lt;code&gt;scoring&lt;/code&gt; parameter doc for more information)</source>
          <target state="translated">使用 &lt;code&gt;cross_validate&lt;/code&gt; 进行多指标评估（更多信息，请参考 &lt;code&gt;scoring&lt;/code&gt; 参数doc）</target>
        </trans-unit>
        <trans-unit id="629b6c06ee9b92eec539c00c0d5b033d1b11a26d" translate="yes" xml:space="preserve">
          <source>Multiple metric parameter search can be done by setting the &lt;code&gt;scoring&lt;/code&gt; parameter to a list of metric scorer names or a dict mapping the scorer names to the scorer callables.</source>
          <target state="translated">可以通过将 &lt;code&gt;scoring&lt;/code&gt; 参数设置为度量记分器名称列表或将记分器名称映射到记分器可调用项的字典来完成多个度量参数的搜索。</target>
        </trans-unit>
        <trans-unit id="85ae0fa16a20d62aa04d6b821cea2aa9547567a2" translate="yes" xml:space="preserve">
          <source>Multiplicative weights for features per transformer. Keys are transformer names, values the weights.</source>
          <target state="translated">每个变压器特征的乘法权重。键是变压器名称,值是权重。</target>
        </trans-unit>
        <trans-unit id="0634d761605b2ffdac7cc3b47cb937d2911bb7fc" translate="yes" xml:space="preserve">
          <source>Multiplicative weights for features per transformer. The output of the transformer is multiplied by these weights. Keys are transformer names, values the weights.</source>
          <target state="translated">每台变压器的特征乘法权重。变压器的输出乘以这些权重。键是变压器名称,值是权重。</target>
        </trans-unit>
        <trans-unit id="7f4f1f6c0e0110908215d6d402a5fd0376794171" translate="yes" xml:space="preserve">
          <source>Multiply features by the specified value. If None, then features are scaled by a random value drawn in [1, 100]. Note that scaling happens after shifting.</source>
          <target state="translated">将特征乘以指定的值。如果没有,那么特征将被一个在[1,100]中抽取的随机值所缩放。注意,缩放发生在移位之后。</target>
        </trans-unit>
        <trans-unit id="773db00cec71fc706de69e832dff6b23a68d6b97" translate="yes" xml:space="preserve">
          <source>Multithreaded BLAS libraries sometimes conflict with Python&amp;rsquo;s &lt;code&gt;multiprocessing&lt;/code&gt; module, which is used by e.g. &lt;code&gt;GridSearchCV&lt;/code&gt; and most other estimators that take an &lt;code&gt;n_jobs&lt;/code&gt; argument (with the exception of &lt;code&gt;SGDClassifier&lt;/code&gt;, &lt;code&gt;SGDRegressor&lt;/code&gt;, &lt;code&gt;Perceptron&lt;/code&gt;, &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; and tree-based methods such as random forests). This is true of Apple&amp;rsquo;s Accelerate and OpenBLAS when built with OpenMP support.</source>
          <target state="translated">多线程BLAS库有时会与Python的 &lt;code&gt;multiprocessing&lt;/code&gt; 模块发生冲突，例如 &lt;code&gt;GridSearchCV&lt;/code&gt; 和采用 &lt;code&gt;n_jobs&lt;/code&gt; 参数的大多数其他估计器都使用该模块（除了 &lt;code&gt;SGDClassifier&lt;/code&gt; ， &lt;code&gt;SGDRegressor&lt;/code&gt; ， &lt;code&gt;Perceptron&lt;/code&gt; ， &lt;code&gt;PassiveAggressiveClassifier&lt;/code&gt; 和基于树的方法（例如随机森林））。当使用OpenMP支持构建时，Apple的Accelerate和OpenBLAS就是如此。</target>
        </trans-unit>
        <trans-unit id="425dc1fa519b0f6261993bae28e1ad51c131bb66" translate="yes" xml:space="preserve">
          <source>Must be provided at the first call to partial_fit, can be omitted in subsequent calls.</source>
          <target state="translated">必须在第一次调用 partial_fit 时提供,可以在后续调用中省略。</target>
        </trans-unit>
        <trans-unit id="b6845c300d4f945b800f2e50de745703cc5cc891" translate="yes" xml:space="preserve">
          <source>Must fulfill the input assumptions of the underlying estimator.</source>
          <target state="translated">必须满足基础估计器的输入假设。</target>
        </trans-unit>
        <trans-unit id="214188886e4a84a8788bdd82b6f8744f5146fead" translate="yes" xml:space="preserve">
          <source>Mutual Information (not adjusted for chance)</source>
          <target state="translated">相互信息(不按机会调整)</target>
        </trans-unit>
        <trans-unit id="16b7cc0e7a5234ba809ed1e09a3a8960dff39693" translate="yes" xml:space="preserve">
          <source>Mutual Information between two clusterings.</source>
          <target state="translated">两个聚类之间的相互信息。</target>
        </trans-unit>
        <trans-unit id="81e08bee8a8968c08bd07aed8cef44d9fb7a13f3" translate="yes" xml:space="preserve">
          <source>Mutual information (MI) &lt;a href=&quot;#r37d39d7589e2-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.</source>
          <target state="translated">两个随机变量之间的互信息（MI）&lt;a href=&quot;#r37d39d7589e2-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;是一个非负值，用于衡量变量之间的相关性。当且仅当两个随机变量是独立的，并且等于较高的值意味着较高的依赖性时，它等于零。</target>
        </trans-unit>
        <trans-unit id="92d0e5dc6672a19ad8c5b9523b6a6d1a9b82c89e" translate="yes" xml:space="preserve">
          <source>Mutual information (MI) &lt;a href=&quot;#r50b872b699c4-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt; between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.</source>
          <target state="translated">两个随机变量之间的互信息（MI）&lt;a href=&quot;#r50b872b699c4-1&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;是一个非负值，用于衡量变量之间的相关性。当且仅当两个随机变量是独立的，并且等于较高的值意味着较高的依赖性时，它等于零。</target>
        </trans-unit>
        <trans-unit id="4276bd70be44db9c6fb9548906a97ab826aba297" translate="yes" xml:space="preserve">
          <source>Mutual information between features and the target.</source>
          <target state="translated">特征与目标之间的相互信息。</target>
        </trans-unit>
        <trans-unit id="33ca9360bf5453bcb4bf9aed03e658f161f23932" translate="yes" xml:space="preserve">
          <source>Mutual information for a continuous target.</source>
          <target state="translated">连续目标的相互信息。</target>
        </trans-unit>
        <trans-unit id="aa199ad103c044c23c4e2e0edbe572bad088827c" translate="yes" xml:space="preserve">
          <source>Mutual information for a contnuous target.</source>
          <target state="translated">互通信息,以利于连续目标。</target>
        </trans-unit>
        <trans-unit id="ef9610a089a978dd0d661be292e2bde712e413d1" translate="yes" xml:space="preserve">
          <source>Mutual information for a discrete target.</source>
          <target state="translated">离散目标的相互信息。</target>
        </trans-unit>
        <trans-unit id="2555b04ef28112b324874c1cc2f3bf2b5b5c384e" translate="yes" xml:space="preserve">
          <source>Mutual information, a non-negative value</source>
          <target state="translated">相互信息,一个非负值。</target>
        </trans-unit>
        <trans-unit id="b51a60734da64be0e618bacbea2865a8a7dcd669" translate="yes" xml:space="preserve">
          <source>N</source>
          <target state="translated">N</target>
        </trans-unit>
        <trans-unit id="4e1221dedd7ee34eb6931a44dc15d9a84ca69a81" translate="yes" xml:space="preserve">
          <source>N : number of dimensions</source>
          <target state="translated">N:尺寸数</target>
        </trans-unit>
        <trans-unit id="8daf5ce04352d841160e980446540ca50cce58e4" translate="yes" xml:space="preserve">
          <source>N-grams to the rescue! Instead of building a simple collection of unigrams (n=1), one might prefer a collection of bigrams (n=2), where occurrences of pairs of consecutive words are counted.</source>
          <target state="translated">N-grams来拯救你! 与其建立一个简单的单字格集合(n=1),不如建立一个大字格集合(n=2),在这个集合中,一对连续的单词的出现次数被计算在内。</target>
        </trans-unit>
        <trans-unit id="235c4fdc295d941494351e73dad0edc432affd04" translate="yes" xml:space="preserve">
          <source>NFF : number of dims in which both values are False</source>
          <target state="translated">NFF:两个值都为假的dims数。</target>
        </trans-unit>
        <trans-unit id="f242d8e1cdbbc8cb628621d8d57f10327047707d" translate="yes" xml:space="preserve">
          <source>NFT : number of dims in which the first value is False, second is True</source>
          <target state="translated">NFT:第一个值为假,第二个值为真的dims数。</target>
        </trans-unit>
        <trans-unit id="ced12bb5137dbf26fd788e77cae54623cdb8b2e8" translate="yes" xml:space="preserve">
          <source>NMF is best used with the &lt;code&gt;fit_transform&lt;/code&gt; method, which returns the matrix W. The matrix H is stored into the fitted model in the &lt;code&gt;components_&lt;/code&gt; attribute; the method &lt;code&gt;transform&lt;/code&gt; will decompose a new matrix X_new based on these stored components:</source>
          <target state="translated">NMF最好与 &lt;code&gt;fit_transform&lt;/code&gt; 方法一起使用，该方法返回矩阵W。矩阵H存储在 &lt;code&gt;components_&lt;/code&gt; 属性中的拟合模型中。方法 &lt;code&gt;transform&lt;/code&gt; 将基于这些存储的分量分解新的矩阵X_new：</target>
        </trans-unit>
        <trans-unit id="9546ef450bf032f2a099e2b8894066e314108bcc" translate="yes" xml:space="preserve">
          <source>NMI and MI are not adjusted against chance.</source>
          <target state="translated">NMI和MI不作偶然调整。</target>
        </trans-unit>
        <trans-unit id="ec8506cc20e415f16975d43b2c6e163b63b7c223" translate="yes" xml:space="preserve">
          <source>NNEQ / (NNEQ + 0.5 * NTT)</source>
          <target state="translated">NNEQ/(NNEQ+0.5*NTT)</target>
        </trans-unit>
        <trans-unit id="64142d93685b184d0f4668dd2d38de67d364504a" translate="yes" xml:space="preserve">
          <source>NNEQ / (NTT + NNZ)</source>
          <target state="translated">NNEQ/(NTT+NNZ)</target>
        </trans-unit>
        <trans-unit id="9e2ca45598fef4852f298770d7c7037071a195c1" translate="yes" xml:space="preserve">
          <source>NNEQ / N</source>
          <target state="translated">NNEQ/N</target>
        </trans-unit>
        <trans-unit id="bd22d441438dd8339012b8925c55919834498020" translate="yes" xml:space="preserve">
          <source>NNEQ / NNZ</source>
          <target state="translated">NNEQ/NNZ</target>
        </trans-unit>
        <trans-unit id="a4e22ff89a7f8daef1da10b2c311e81f8eb57054" translate="yes" xml:space="preserve">
          <source>NNEQ : number of non-equal dimensions, NNEQ = NTF + NFT</source>
          <target state="translated">NNEQ:非等维数,NNEQ=NTF+NFT。</target>
        </trans-unit>
        <trans-unit id="80bfd3623c0e507836f83286688a2ee41b18b00e" translate="yes" xml:space="preserve">
          <source>NNZ / N</source>
          <target state="translated">NNZ/NN</target>
        </trans-unit>
        <trans-unit id="93209a2edd337e6dc4e7c870a3c72537cea28fdf" translate="yes" xml:space="preserve">
          <source>NNZ : number of nonzero dimensions, NNZ = NTF + NFT + NTT</source>
          <target state="translated">NNZ:非零维数,NNZ=NTF+NFT+NTT。</target>
        </trans-unit>
        <trans-unit id="a8ad860c15810cce0e7beac1c91da3ab2cb22c47" translate="yes" xml:space="preserve">
          <source>NOTE</source>
          <target state="translated">NOTE</target>
        </trans-unit>
        <trans-unit id="b81cbdff62e50c72d48e4feea8a9ed88bea18bef" translate="yes" xml:space="preserve">
          <source>NOTE that when using custom scorers, each scorer should return a single value. Metric functions returning a list/array of values can be wrapped into multiple scorers that return one value each.</source>
          <target state="translated">注意,当使用自定义评分器时,每个评分器应返回一个值。返回一个列表/数组值的度量函数可以被包装成多个评分器,每个评分器返回一个值。</target>
        </trans-unit>
        <trans-unit id="9764dfb854390dc404102ac64200b55e363e83df" translate="yes" xml:space="preserve">
          <source>NOX nitric oxides concentration (parts per 10 million)</source>
          <target state="translated">NOX 一氧化氮浓度(千万分之一)</target>
        </trans-unit>
        <trans-unit id="99542bc2231d38286b9a1dbe4685e8690203b845" translate="yes" xml:space="preserve">
          <source>NTF : number of dims in which the first value is True, second is False</source>
          <target state="translated">NTF:第一值为真,第二值为假的dims数。</target>
        </trans-unit>
        <trans-unit id="d7aff2fba38c5d47fc1d509779237efeccf9cd66" translate="yes" xml:space="preserve">
          <source>NTT : number of dims in which both values are True</source>
          <target state="translated">NTT:两个值都为真的dims数。</target>
        </trans-unit>
        <trans-unit id="6e2518fe965a665a40ec6f1bf71cbacd3d7014df" translate="yes" xml:space="preserve">
          <source>NaNs are ignored in the algorithm.</source>
          <target state="translated">算法中忽略了NaNs。</target>
        </trans-unit>
        <trans-unit id="7b2cc2bc3bfa4ab2fba6e73cce899558220dd79a" translate="yes" xml:space="preserve">
          <source>NaNs are treated as missing values: disregarded in fit, and maintained in transform.</source>
          <target state="translated">NaNs作为缺失值处理:在拟合中不考虑,在变换中保持。</target>
        </trans-unit>
        <trans-unit id="d13d7452647efb26ab0d2b1a3596526a7f4ca5d6" translate="yes" xml:space="preserve">
          <source>NaNs are treated as missing values: disregarded to compute the statistics, and maintained during the data transformation.</source>
          <target state="translated">NaNs被视为缺失值:在计算统计数据时不予考虑,并在数据转换过程中予以保留。</target>
        </trans-unit>
        <trans-unit id="80d8f13b4e334c4342adf34b360ad118e5e25aa3" translate="yes" xml:space="preserve">
          <source>Naive Bayes classifier for multinomial models</source>
          <target state="translated">贝叶斯</target>
        </trans-unit>
        <trans-unit id="92990e6c1a566f0d055f974e25026ec604b9ccd9" translate="yes" xml:space="preserve">
          <source>Naive Bayes classifier for multivariate Bernoulli models.</source>
          <target state="translated">Naive Bayes classifier for multivariate Bernoulli models.</target>
        </trans-unit>
        <trans-unit id="c95f9acb4985f23ad6962fd01ae91dec549e7273" translate="yes" xml:space="preserve">
          <source>Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.</source>
          <target state="translated">与更复杂的方法相比,奈夫贝叶斯学习器和分类器的速度可以非常快。类条件特征分布的解耦意味着每个分布都可以作为一个一维分布进行独立估计。这反过来又有助于缓解来自维度诅咒的问题。</target>
        </trans-unit>
        <trans-unit id="bb7ceea48fd3728ed03cf0ba21b4839b323fc974" translate="yes" xml:space="preserve">
          <source>Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes&amp;rsquo; theorem with the &amp;ldquo;naive&amp;rdquo; assumption of conditional independence between every pair of features given the value of the class variable. Bayes&amp;rsquo; theorem states the following relationship, given class variable \(y\) and dependent feature vector \(x_1\) through \(x_n\), :</source>
          <target state="translated">朴素贝叶斯方法是一组监督学习算法，基于贝叶斯定理，在给定类变量值的情况下，每对特征之间具有条件独立性的&amp;ldquo;朴素&amp;rdquo;假设适用于贝叶斯定理。贝叶斯定理给出以下关系，给定类变量\（y \）和从属特征向量\（x_1 \）到\（x_n \），</target>
        </trans-unit>
        <trans-unit id="f65600325bc091b7b293639582ad70691e2ac960" translate="yes" xml:space="preserve">
          <source>Naive Bayes models can be used to tackle large scale classification problems for which the full training set might not fit in memory. To handle this case, &lt;a href=&quot;generated/sklearn.naive_bayes.multinomialnb#sklearn.naive_bayes.MultinomialNB&quot;&gt;&lt;code&gt;MultinomialNB&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.naive_bayes.bernoullinb#sklearn.naive_bayes.BernoulliNB&quot;&gt;&lt;code&gt;BernoulliNB&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt;&lt;code&gt;GaussianNB&lt;/code&gt;&lt;/a&gt; expose a &lt;code&gt;partial_fit&lt;/code&gt; method that can be used incrementally as done with other classifiers as demonstrated in &lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;Out-of-core classification of text documents&lt;/a&gt;. All naive Bayes classifiers support sample weighting.</source>
          <target state="translated">朴素贝叶斯模型可用于解决大规模分类问题，而完整的训练集可能无法满足这些分类问题。为了处理这种情况，&lt;a href=&quot;generated/sklearn.naive_bayes.multinomialnb#sklearn.naive_bayes.MultinomialNB&quot;&gt; &lt;code&gt;MultinomialNB&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.naive_bayes.bernoullinb#sklearn.naive_bayes.BernoulliNB&quot;&gt; &lt;code&gt;BernoulliNB&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.naive_bayes.gaussiannb#sklearn.naive_bayes.GaussianNB&quot;&gt; &lt;code&gt;GaussianNB&lt;/code&gt; &lt;/a&gt;公开了 &lt;code&gt;partial_fit&lt;/code&gt; 方法，该方法可以像其他分类器一样逐步使用，如&lt;a href=&quot;../auto_examples/applications/plot_out_of_core_classification#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py&quot;&gt;文本文档的核心分类中所示&lt;/a&gt;。所有朴素贝叶斯分类器均支持样本加权。</target>
        </trans-unit>
        <trans-unit id="7f39a3bd9bee33098b86f18fcaf23fc1b1f211a0" translate="yes" xml:space="preserve">
          <source>Name of dataset</source>
          <target state="translated">数据集名称</target>
        </trans-unit>
        <trans-unit id="325b56c17b8389991fec124de840a19f36bc1993" translate="yes" xml:space="preserve">
          <source>Name of each feature; feature_names[i] holds the name of the feature with index i.</source>
          <target state="translated">每个特征的名称;feature_names[i]持有索引为i的特征名称。</target>
        </trans-unit>
        <trans-unit id="1d63bfd9357f039d867c5652a85ab61996c87f94" translate="yes" xml:space="preserve">
          <source>Name of the data set on mldata.org, e.g.: &amp;ldquo;leukemia&amp;rdquo;, &amp;ldquo;Whistler Daily Snowfall&amp;rdquo;, etc. The raw name is automatically converted to a mldata.org URL .</source>
          <target state="translated">mldata.org上的数据集名称，例如：&amp;ldquo;白血病&amp;rdquo;，&amp;ldquo;惠斯勒每日降雪量&amp;rdquo;等。原始名称会自动转换为mldata.org URL。</target>
        </trans-unit>
        <trans-unit id="866f4401ef93cfed3b044fff6753dc1e913659b2" translate="yes" xml:space="preserve">
          <source>Name of the output activation function.</source>
          <target state="translated">输出激活功能的名称。</target>
        </trans-unit>
        <trans-unit id="5a3a86d298c7e4314e724bb2623d8c6979ee2b6e" translate="yes" xml:space="preserve">
          <source>Name of the parameter that will be varied.</source>
          <target state="translated">要改变的参数名称。</target>
        </trans-unit>
        <trans-unit id="ce3ec81584fa2d87df12f144e2480deecc5a975a" translate="yes" xml:space="preserve">
          <source>Name or index of the column containing the data.</source>
          <target state="translated">包含数据的列名或索引。</target>
        </trans-unit>
        <trans-unit id="0ae5e537b1c061ee0b3ccea7c63ace2088ca02dd" translate="yes" xml:space="preserve">
          <source>Name or index of the column containing the target values.</source>
          <target state="translated">包含目标值的列名或索引。</target>
        </trans-unit>
        <trans-unit id="3170e49e906772d2e2d83c510613a736bad3f541" translate="yes" xml:space="preserve">
          <source>Named features not encountered during fit or fit_transform will be silently ignored.</source>
          <target state="translated">在fit或fit_transform过程中没有遇到的命名特征将被默默忽略。</target>
        </trans-unit>
        <trans-unit id="dd3283d9f71127c2e2cb8ea6f07a41ece4a049ce" translate="yes" xml:space="preserve">
          <source>Names of each of the features.</source>
          <target state="translated">每个特征的名称:</target>
        </trans-unit>
        <trans-unit id="99983f06243c41c70b7f7a98a21b26cf2a2ec6a9" translate="yes" xml:space="preserve">
          <source>Names of each of the target classes in ascending numerical order. Only relevant for classification and not supported for multi-output. If &lt;code&gt;True&lt;/code&gt;, shows a symbolic representation of the class name.</source>
          <target state="translated">每个目标类的名称按升序排列。仅与分类相关，不支持多输出。如果为 &lt;code&gt;True&lt;/code&gt; ，则显示类名的符号表示。</target>
        </trans-unit>
        <trans-unit id="e9e6ba24a1711383d87f42cc11bb29b29e568b73" translate="yes" xml:space="preserve">
          <source>Names of each target (RCV1 topics), as ordered in dataset.target.</source>
          <target state="translated">每个目标(RCV1专题)的名称,按dataset.target排序。</target>
        </trans-unit>
        <trans-unit id="5ee798b80fce1c26ac31847940e2cbb9594ee08b" translate="yes" xml:space="preserve">
          <source>Names of the features produced by transform.</source>
          <target state="translated">通过变换产生的特征名称。</target>
        </trans-unit>
        <trans-unit id="4aded465f8c4c45d7d7ec8d437901909b15f0b3f" translate="yes" xml:space="preserve">
          <source>Natural handling of data of mixed type (= heterogeneous features)</source>
          <target state="translated">混合类型数据的自然处理(=异质特征</target>
        </trans-unit>
        <trans-unit id="0a4d2a1303aed1ff654767155d9269907f0d020c" translate="yes" xml:space="preserve">
          <source>Nearest Centroid Classification</source>
          <target state="translated">最近的中心体分类</target>
        </trans-unit>
        <trans-unit id="fa1459036257eab60db8e1afe6d9886bbc5e8a42" translate="yes" xml:space="preserve">
          <source>Nearest Neighbors Classification</source>
          <target state="translated">最近鄰居分類</target>
        </trans-unit>
        <trans-unit id="c7b70d3a90c9b413590f1c3fdfeae8dc16304398" translate="yes" xml:space="preserve">
          <source>Nearest Neighbors regression</source>
          <target state="translated">最近邻域回归</target>
        </trans-unit>
        <trans-unit id="cc8575a20e3e28eef4bfc70e48146f9994bd7318" translate="yes" xml:space="preserve">
          <source>Nearest centroid classifier.</source>
          <target state="translated">近心器。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
