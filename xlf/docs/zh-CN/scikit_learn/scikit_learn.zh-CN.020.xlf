<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="e3ab7886f45f0e5bcfcb494c5dda13f6aee54058" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;mean_fit_time&lt;/code&gt;, &lt;code&gt;std_fit_time&lt;/code&gt;, &lt;code&gt;mean_score_time&lt;/code&gt; and &lt;code&gt;std_score_time&lt;/code&gt; are all in seconds.</source>
          <target state="translated">该 &lt;code&gt;mean_fit_time&lt;/code&gt; ， &lt;code&gt;std_fit_time&lt;/code&gt; ， &lt;code&gt;mean_score_time&lt;/code&gt; 和 &lt;code&gt;std_score_time&lt;/code&gt; 都在秒。</target>
        </trans-unit>
        <trans-unit id="5067a7dbb7441ffa442bc35298e784d3bbb5d81c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;out_of_bounds&lt;/code&gt; parameter handles how &lt;code&gt;X&lt;/code&gt; values outside of the training domain are handled. When set to &amp;ldquo;nan&amp;rdquo;, predictions will be NaN. When set to &amp;ldquo;clip&amp;rdquo;, predictions will be set to the value corresponding to the nearest train interval endpoint. When set to &amp;ldquo;raise&amp;rdquo; a &lt;code&gt;ValueError&lt;/code&gt; is raised.</source>
          <target state="translated">该 &lt;code&gt;out_of_bounds&lt;/code&gt; 参数程序如何处理 &lt;code&gt;X&lt;/code&gt; 重视培训域之外进行处理。设置为&amp;ldquo; nan&amp;rdquo;时，预测将为NaN。当设置为&amp;ldquo; clip&amp;rdquo;时，预测将设置为与最近的火车间隔终点相对应的值。设置为&amp;ldquo; raise&amp;rdquo;时，将引发 &lt;code&gt;ValueError&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4dc710fdb008b124893854b1db0d772123e2f23c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;out_of_bounds&lt;/code&gt; parameter handles how x-values outside of the training domain are handled. When set to &amp;ldquo;nan&amp;rdquo;, predicted y-values will be NaN. When set to &amp;ldquo;clip&amp;rdquo;, predicted y-values will be set to the value corresponding to the nearest train interval endpoint. When set to &amp;ldquo;raise&amp;rdquo;, allow &lt;code&gt;interp1d&lt;/code&gt; to throw ValueError.</source>
          <target state="translated">该 &lt;code&gt;out_of_bounds&lt;/code&gt; x值的训练域之外是如何处理的参数句柄。当设置为&amp;ldquo; nan&amp;rdquo;时，预测的y值将为NaN。当设置为&amp;ldquo; clip&amp;rdquo;时，预测的y值将设置为与最近的火车间隔终点相对应的值。当设置为&amp;ldquo; raise&amp;rdquo;时，允许 &lt;code&gt;interp1d&lt;/code&gt; 抛出ValueError。</target>
        </trans-unit>
        <trans-unit id="3c0e73047db48d9d2c7fabbdd5b52d96e2b806a9" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;partial_fit&lt;/code&gt; method call of naive Bayes models introduces some computational overhead. It is recommended to use data chunk sizes that are as large as possible, that is as the available RAM allows.</source>
          <target state="translated">朴素贝叶斯模型的 &lt;code&gt;partial_fit&lt;/code&gt; 方法调用引入了一些计算开销。建议使用尽可能大的数据块大小，即可用RAM允许的大小。</target>
        </trans-unit>
        <trans-unit id="12fb3214c445789e7d1fb007eb13c0f9c87b3271" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;penalty&lt;/code&gt; parameter determines the regularization to be used (see description above in the classification section).</source>
          <target state="translated">的 &lt;code&gt;penalty&lt;/code&gt; 参数确定要使用的正则化（见上文中的分类部分描述）。</target>
        </trans-unit>
        <trans-unit id="c075895944959b3ce70972d2605db496c74ee36b" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;preprocessing&lt;/code&gt; module further provides a utility class &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt;&lt;code&gt;Normalizer&lt;/code&gt;&lt;/a&gt; that implements the same operation using the &lt;code&gt;Transformer&lt;/code&gt; API (even though the &lt;code&gt;fit&lt;/code&gt; method is useless in this case: the class is stateless as this operation treats samples independently).</source>
          <target state="translated">所述 &lt;code&gt;preprocessing&lt;/code&gt; 模块还提供了一种工具类&lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt; &lt;code&gt;Normalizer&lt;/code&gt; &lt;/a&gt;实现使用相同的操作 &lt;code&gt;Transformer&lt;/code&gt; API（即使 &lt;code&gt;fit&lt;/code&gt; 方法是在这种情况下无用：类是无状态的，因为这操作对待样品独立地）。</target>
        </trans-unit>
        <trans-unit id="deb9c47cdc32b652eb6b8e87508a50e73ddd6520" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;preprocessing&lt;/code&gt; module further provides a utility class &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt; that implements the &lt;code&gt;Transformer&lt;/code&gt; API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. This class is hence suitable for use in the early steps of a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">该 &lt;code&gt;preprocessing&lt;/code&gt; 模块还提供了一个实用工具类&lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; &lt;/a&gt;实现了 &lt;code&gt;Transformer&lt;/code&gt; API来计算在训练集的平均值和标准偏差，从而可以稍后重新应用测试集相同的转换。因此，此类适合在&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt;的早期步骤中使用：</target>
        </trans-unit>
        <trans-unit id="1c988d2d2cf78f045059c60f8f22ddfe99f6b9b8" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;random_state&lt;/code&gt; parameter defaults to &lt;code&gt;None&lt;/code&gt;, meaning that the shuffling will be different every time &lt;code&gt;KFold(..., shuffle=True)&lt;/code&gt; is iterated. However, &lt;code&gt;GridSearchCV&lt;/code&gt; will use the same shuffling for each set of parameters validated by a single call to its &lt;code&gt;fit&lt;/code&gt; method.</source>
          <target state="translated">该 &lt;code&gt;random_state&lt;/code&gt; 参数默认为 &lt;code&gt;None&lt;/code&gt; ，这意味着洗牌每一次将不同 &lt;code&gt;KFold(..., shuffle=True)&lt;/code&gt; 迭代。但是， &lt;code&gt;GridSearchCV&lt;/code&gt; 将对通过一次调用其 &lt;code&gt;fit&lt;/code&gt; 方法验证的每组参数使用相同的改组。</target>
        </trans-unit>
        <trans-unit id="269b993dc29ab1401256004d5e02aa3f5322b3b6" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;remainder&lt;/code&gt; parameter can be set to an estimator to transform the remaining rating columns. The transformed values are appended to the end of the transformation:</source>
          <target state="translated">该 &lt;code&gt;remainder&lt;/code&gt; 参数可以设置为一个估计改造其余评级列。转换后的值将附加到转换的末尾：</target>
        </trans-unit>
        <trans-unit id="ac9a3f7a78a3eea81f462ceabf4e7d7b7695c3e1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;roc_auc_score&lt;/code&gt; function can also be used in multi-class classification. Two averaging strategies are currently supported: the one-vs-one algorithm computes the average of the pairwise ROC AUC scores, and the one-vs-rest algorithm computes the average of the ROC AUC scores for each class against all other classes. In both cases, the multiclass ROC AUC scores are computed from the probability estimates that a sample belongs to a particular class according to the model. The OvO and OvR algorithms support weighting uniformly (&lt;code&gt;average='macro'&lt;/code&gt;) and weighting by the prevalence (&lt;code&gt;average='weighted'&lt;/code&gt;).</source>
          <target state="translated">该 &lt;code&gt;roc_auc_score&lt;/code&gt; 功能也可以在多类分类中。当前支持两种平均策略：&amp;ldquo;一对多&amp;rdquo;算法计算成对的ROC AUC分数的平均值，&amp;ldquo;一对多&amp;rdquo;算法计算每个类别相对于所有其他类别的ROC AUC分数的平均值。在这两种情况下，根据模型根据样本属于特定类别的概率估计来计算多类ROC AUC分数。 OvO和OvR算法支持统一加权（ &lt;code&gt;average='macro'&lt;/code&gt; ）和按流行度加权（ &lt;code&gt;average='weighted'&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="5cf025ac399ddc1c71c004c7cb8bd73171357561" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;shrinkage&lt;/code&gt; parameter can also be manually set between 0 and 1. In particular, a value of 0 corresponds to no shrinkage (which means the empirical covariance matrix will be used) and a value of 1 corresponds to complete shrinkage (which means that the diagonal matrix of variances will be used as an estimate for the covariance matrix). Setting this parameter to a value between these two extrema will estimate a shrunk version of the covariance matrix.</source>
          <target state="translated">该 &lt;code&gt;shrinkage&lt;/code&gt; 的参数也可以被手动设定在0和1具体而言，0对应于无收缩的值（这意味着经验协方差矩阵将被使用）和1个对应于完全收缩的值（之间，这意味着对角线方差矩阵将用作协方差矩阵的估计值）。将此参数设置为这两个极值之间的值将估计协方差矩阵的缩小版本。</target>
        </trans-unit>
        <trans-unit id="ac317fd98a77c706ac5d8a7e74a96d7bdbebfe03" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.covariance&lt;/code&gt; package implements a robust estimator of covariance, the Minimum Covariance Determinant &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;3&lt;/a&gt;.</source>
          <target state="translated">所述 &lt;code&gt;sklearn.covariance&lt;/code&gt; 包实现协方差的估计健壮，最小协方差行列式&lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;3&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="b439b9c10b67475737ae3e151e90a6ed815edbd1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.covariance&lt;/code&gt; package implements a robust estimator of covariance, the Minimum Covariance Determinant &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;[3]&lt;/a&gt;.</source>
          <target state="translated">所述 &lt;code&gt;sklearn.covariance&lt;/code&gt; 包实现协方差的估计健壮，最小协方差行列式&lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;[3] &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6908af1748b7fe666814b53ca6e0a8cab66eb012" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.datasets&lt;/code&gt; package embeds some small toy datasets as introduced in the &lt;a href=&quot;../tutorial/basic/tutorial#loading-example-dataset&quot;&gt;Getting Started&lt;/a&gt; section.</source>
          <target state="translated">该 &lt;code&gt;sklearn.datasets&lt;/code&gt; 包嵌入如引入了一些小玩具的数据集&lt;a href=&quot;../tutorial/basic/tutorial#loading-example-dataset&quot;&gt;入门&lt;/a&gt;部分。</target>
        </trans-unit>
        <trans-unit id="82c9f1d0e48a88b08f4d85a858e0d70b67f55eec" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.datasets&lt;/code&gt; package is able to download datasets from the repository using the function &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_openml#sklearn.datasets.fetch_openml&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_openml&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">该 &lt;code&gt;sklearn.datasets&lt;/code&gt; 包能够从使用功能库下载的数据集&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_openml#sklearn.datasets.fetch_openml&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_openml&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="8a80b4da04f92037fdd1ffd5771651becd3a649e" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.preprocessing&lt;/code&gt; package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.</source>
          <target state="translated">所述 &lt;code&gt;sklearn.preprocessing&lt;/code&gt; 包提供几种常见的实用功能和变压器类来改变原始的特征矢量为表示更适合为下游估计。</target>
        </trans-unit>
        <trans-unit id="9a666cbe94ae4d4ef285ecec57ff29087d1b1c92" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;stop_words_&lt;/code&gt; attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling.</source>
          <target state="translated">酸洗时， &lt;code&gt;stop_words_&lt;/code&gt; 属性会变大并增加模型大小。该属性仅用于自省，可以在使用酸洗之前使用delattr安全删除或将其设置为None。</target>
        </trans-unit>
        <trans-unit id="a2df492be59e0b5e94ab9ece47f7ab58734f8d4a" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;svm.OneClassSVM&lt;/code&gt; is known to be sensitive to outliers and thus does not perform very well for outlier detection. This estimator is best suited for novelty detection when the training set is not contaminated by outliers. That said, outlier detection in high-dimension, or without any assumptions on the distribution of the inlying data is very challenging, and a One-class SVM might give useful results in these situations depending on the value of its hyperparameters.</source>
          <target state="translated">所述 &lt;code&gt;svm.OneClassSVM&lt;/code&gt; 已知是对异常值敏感并因此对异常值检测不执行得非常好。当训练集不受异常值污染时，此估计器最适合新颖性检测。也就是说，在高维中进行离群值检测，或者不对基础数据的分布进行任何假设都是非常具有挑战性的，一类SVM在这些情况下可能会根据其超参数的值给出有用的结果。</target>
        </trans-unit>
        <trans-unit id="ae04908924f55e8ffa997283ff07a48e73585d0d" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;tree_disp&lt;/code&gt; and &lt;code&gt;mlp_disp&lt;/code&gt;&lt;a href=&quot;../../modules/generated/sklearn.inspection.partialdependencedisplay#sklearn.inspection.PartialDependenceDisplay&quot;&gt;&lt;code&gt;PartialDependenceDisplay&lt;/code&gt;&lt;/a&gt; objects contain all the computed information needed to recreate the partial dependence curves. This means we can easily create additional plots without needing to recompute the curves.</source>
          <target state="translated">的 &lt;code&gt;tree_disp&lt;/code&gt; 和 &lt;code&gt;mlp_disp&lt;/code&gt; &lt;a href=&quot;../../modules/generated/sklearn.inspection.partialdependencedisplay#sklearn.inspection.PartialDependenceDisplay&quot;&gt; &lt;code&gt;PartialDependenceDisplay&lt;/code&gt; &lt;/a&gt;对象包含重建部分依赖曲线所需的所有计算出的信息。这意味着我们可以轻松创建其他图，而无需重新计算曲线。</target>
        </trans-unit>
        <trans-unit id="a0172c625369e43fa830669101114953189c751e" translate="yes" xml:space="preserve">
          <source>The &lt;em&gt;kernel function&lt;/em&gt; can be any of the following:</source>
          <target state="translated">该&lt;em&gt;内核函数&lt;/em&gt;可以是任何如下：</target>
        </trans-unit>
        <trans-unit id="9153a9bf6c588ab4d00c19f8aab755b4d4790f32" translate="yes" xml:space="preserve">
          <source>The &lt;em&gt;reachability&lt;/em&gt; distances generated by OPTICS allow for variable density extraction of clusters within a single data set. As shown in the above plot, combining &lt;em&gt;reachability&lt;/em&gt; distances and data set &lt;code&gt;ordering_&lt;/code&gt; produces a &lt;em&gt;reachability plot&lt;/em&gt;, where point density is represented on the Y-axis, and points are ordered such that nearby points are adjacent. &amp;lsquo;Cutting&amp;rsquo; the reachability plot at a single value produces DBSCAN like results; all points above the &amp;lsquo;cut&amp;rsquo; are classified as noise, and each time that there is a break when reading from left to right signifies a new cluster. The default cluster extraction with OPTICS looks at the steep slopes within the graph to find clusters, and the user can define what counts as a steep slope using the parameter &lt;code&gt;xi&lt;/code&gt;. There are also other possibilities for analysis on the graph itself, such as generating hierarchical representations of the data through reachability-plot dendrograms, and the hierarchy of clusters detected by the algorithm can be accessed through the &lt;code&gt;cluster_hierarchy_&lt;/code&gt; parameter. The plot above has been color-coded so that cluster colors in planar space match the linear segment clusters of the reachability plot. Note that the blue and red clusters are adjacent in the reachability plot, and can be hierarchically represented as children of a larger parent cluster.</source>
          <target state="translated">由OPTICS生成的&lt;em&gt;可达&lt;/em&gt;距离允许在单个数据集中可变密度提取群集。如上图所示，将&lt;em&gt;可达&lt;/em&gt;距离和数据集 &lt;code&gt;ordering_&lt;/code&gt; 结合起来可得出&lt;em&gt;可达性图&lt;/em&gt;，其中点密度在Y轴上表示，并且对点进行排序以使附近的点相邻。将可达性图&amp;ldquo;剪切&amp;rdquo;成单个值会产生类似DBSCAN的结果； &amp;ldquo;剪切&amp;rdquo;上方的所有点都被归类为噪声，并且每次从左向右读取时出现中断都表示一个新的簇。使用OPTICS进行默认的聚类提取会查看图形中的陡坡以找到聚类，用户可以使用参数 &lt;code&gt;xi&lt;/code&gt; 定义算作陡坡的计数。还可以对图本身进行分析，例如通过可达性图树状图生成数据的层次表示，并且可以通过 &lt;code&gt;cluster_hierarchy_&lt;/code&gt; 访问算法检测到的聚类的层次。范围。上面的图已进行了颜色编码，以使平面空间中的簇颜色与可达性图的线性段簇匹配。请注意，蓝色和红色群集在可达性图中相邻，并且可以按层次表示为较大父群集的子群集。</target>
        </trans-unit>
        <trans-unit id="6eff32d476fb25ebcdfdcb46623a8711aa972809" translate="yes" xml:space="preserve">
          <source>The &lt;strong&gt;conditional entropy of clusters given class&lt;/strong&gt;\(H(K|C)\) and the &lt;strong&gt;entropy of clusters&lt;/strong&gt;\(H(K)\) are defined in a symmetric manner.</source>
          <target state="translated">&lt;strong&gt;给定类&lt;/strong&gt; \（H（K | C）\）&lt;strong&gt;的聚类&lt;/strong&gt;的&lt;strong&gt;条件熵和聚类&lt;/strong&gt; \（H（K）\）的&lt;strong&gt;熵&lt;/strong&gt;以对称方式定义。</target>
        </trans-unit>
        <trans-unit id="5adbfb7d3b47e961b1e75927796552dd26ad41e4" translate="yes" xml:space="preserve">
          <source>The &lt;strong&gt;exposure&lt;/strong&gt; is the duration of the insurance coverage of a given policy, in years.</source>
          <target state="translated">该&lt;strong&gt;曝光&lt;/strong&gt;是给定策略的保险期限，以年。</target>
        </trans-unit>
        <trans-unit id="a256791561f56278bffe5a6063bb1e7227590f9a" translate="yes" xml:space="preserve">
          <source>The AGE and EXPERIENCE coefficients are affected by strong variability which might be due to the collinearity between the 2 features: as AGE and EXPERIENCE vary together in the data, their effect is difficult to tease apart.</source>
          <target state="translated">年龄和经验系数受到强烈的变异性影响,这可能是由于2个特征之间的关联性造成的:由于年龄和经验在数据中一起变化,它们的影响很难区分。</target>
        </trans-unit>
        <trans-unit id="e076b7d1d24895a169d091c6b0d7ae51431e1b5b" translate="yes" xml:space="preserve">
          <source>The AGE coefficient is expressed in &amp;ldquo;dollars/hour per living years&amp;rdquo; while the EDUCATION one is expressed in &amp;ldquo;dollars/hour per years of education&amp;rdquo;. This representation of the coefficients has the benefit of making clear the practical predictions of the model: an increase of \(1\) year in AGE means a decrease of \(0.030867\) dollars/hour, while an increase of \(1\) year in EDUCATION means an increase of \(0.054699\) dollars/hour. On the other hand, categorical variables (as UNION or SEX) are adimensional numbers taking either the value 0 or 1. Their coefficients are expressed in dollars/hour. Then, we cannot compare the magnitude of different coefficients since the features have different natural scales, and hence value ranges, because of their different unit of measure. This is more visible if we plot the coefficients.</source>
          <target state="translated">AGE系数以&amp;ldquo;每生活年的美元/小时&amp;rdquo;表示，而教育程度以&amp;ldquo;每教育年的美元/小时&amp;rdquo;表示。系数的这种表示有利于阐明模型的实际预测：AGE中\（1 \）年的增加意味着每小时（\ 0.030867 \）美元的减少，而\（1 \ ）年的教育费用，意味着每小时增加\（0.054699 \）美元。另一方面，类别变量（如UNION或SEX）是取值为0或1的无穷数。它们的系数以美元/小时表示。然后，由于特征具有不同的自然尺度，因此由于其度量单位不同，因此无法比较不同系数的大小。如果我们绘制系数，这将更加明显。</target>
        </trans-unit>
        <trans-unit id="2143ba51f2aad99e99e8cd60a62ccfdecbf0f265" translate="yes" xml:space="preserve">
          <source>The AMI returns a value of 1 when the two partitions are identical (ie perfectly matched). Random partitions (independent labellings) have an expected AMI around 0 on average hence can be negative.</source>
          <target state="translated">当两个分区相同(即完全匹配)时,AMI返回的值为1。随机分区(独立标签)的预期AMI平均在0左右,因此可能为负值。</target>
        </trans-unit>
        <trans-unit id="ad838931339007a1bda099c18480308bbb327339" translate="yes" xml:space="preserve">
          <source>The API is experimental (particularly the return value structure), and might have small backward-incompatible changes in future releases.</source>
          <target state="translated">API是实验性的(尤其是返回值结构),在未来的版本中可能会有小的向后不兼容的变化。</target>
        </trans-unit>
        <trans-unit id="00e3722885bdadae5430c6fecaacfc1cced893f6" translate="yes" xml:space="preserve">
          <source>The API is experimental in version 0.20 (particularly the return value structure), and might have small backward-incompatible changes in future releases.</source>
          <target state="translated">API在0.20版本中是实验性的(尤其是返回值结构),在未来的版本中可能会有小的向后不兼容的变化。</target>
        </trans-unit>
        <trans-unit id="e07138bd94d8d9dd92474342c51f134fa3423d8d" translate="yes" xml:space="preserve">
          <source>The Ames housing dataset is not shipped with scikit-learn and therefore we will fetch it from &lt;a href=&quot;https://www.openml.org/d/42165&quot;&gt;OpenML&lt;/a&gt;.</source>
          <target state="translated">Ames住房数据集未随scikit-learn一起提供，因此我们将从&lt;a href=&quot;https://www.openml.org/d/42165&quot;&gt;OpenML中&lt;/a&gt;获取它。</target>
        </trans-unit>
        <trans-unit id="22556f00c2cdfc8c60673e1c663299619a64901c" translate="yes" xml:space="preserve">
          <source>The BIC criterion can be used to select the number of components in a Gaussian Mixture in an efficient way. In theory, it recovers the true number of components only in the asymptotic regime (i.e. if much data is available and assuming that the data was actually generated i.i.d. from a mixture of Gaussian distribution). Note that using a &lt;a href=&quot;#bgmm&quot;&gt;Variational Bayesian Gaussian mixture&lt;/a&gt; avoids the specification of the number of components for a Gaussian mixture model.</source>
          <target state="translated">BIC标准可用于以有效方式选择高斯混合物中的组分数。从理论上讲，它仅在渐近状态下才恢复成分的真实数量（即，如果有大量数据可用，并假设数据实际上是从高斯分布的混合中生成的）。请注意，使用&lt;a href=&quot;#bgmm&quot;&gt;变分贝叶斯高斯混合&lt;/a&gt;可避免指定高斯混合模型的分量数。</target>
        </trans-unit>
        <trans-unit id="c912c462511f5921d016fed26135f626922ccc57" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut implementation only works when the target dimensionality is 3 or less. The 2D case is typical when building visualizations.</source>
          <target state="translated">Barnes-Hut的实现只有在目标维度为3或更小的情况下才有效。在构建可视化时,2D情况是典型的。</target>
        </trans-unit>
        <trans-unit id="49027a83447d6307ec530f0dd81bd5ad76360faa" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.</source>
          <target state="translated">Barnes-Hut t-SNE方法仅限于二维或三维嵌入。</target>
        </trans-unit>
        <trans-unit id="f05f901deabcb1ee376f8697fc79932cbf746ce6" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut t-SNE that has been implemented here is usually much slower than other manifold learning algorithms. The optimization is quite difficult and the computation of the gradient is \(O[d N log(N)]\), where \(d\) is the number of output dimensions and \(N\) is the number of samples. The Barnes-Hut method improves on the exact method where t-SNE complexity is \(O[d N^2]\), but has several other notable differences:</source>
          <target state="translated">这里已经实现的Barnes-Hut t-SNE通常比其他歧管学习算法慢得多。优化是相当困难的,梯度的计算是 \(O[d N log(N)]\),其中 \(d\)是输出维数,\(N\)是样本数。Barnes-Hut方法在精确方法的基础上进行了改进,其中t-SNE的复杂度为/(O[d N^2]\),但还有其他一些明显的不同。</target>
        </trans-unit>
        <trans-unit id="e724094f6d5c410991717a9e2fc045d6798def45" translate="yes" xml:space="preserve">
          <source>The Birch algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters.</source>
          <target state="translated">Birch算法有两个参数,即阈值和分支因子。分支因子限制了一个节点中子群的数量,阈值限制了进入样本与现有子群之间的距离。</target>
        </trans-unit>
        <trans-unit id="8fa70be719c3475e8e012f73acc572a21c03cdd8" translate="yes" xml:space="preserve">
          <source>The Boston house-price data has been used in many machine learning papers that address regression problems.</source>
          <target state="translated">波士顿房价数据已经被许多机器学习论文用来解决回归问题。</target>
        </trans-unit>
        <trans-unit id="455243540177422da87f9c3aa93de30456a2bae3" translate="yes" xml:space="preserve">
          <source>The Boston house-price data of Harrison, D. and Rubinfeld, D.L. &amp;lsquo;Hedonic prices and the demand for clean air&amp;rsquo;, J. Environ. Economics &amp;amp; Management, vol.5, 81-102, 1978. Used in Belsley, Kuh &amp;amp; Welsch, &amp;lsquo;Regression diagnostics &amp;hellip;&amp;rsquo;, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter.</source>
          <target state="translated">波士顿哈里森（Harrison）和鲁宾菲尔德（Rubinfeld）的房价数据，DL&amp;ldquo;享乐主义价格和对清洁空气的需求&amp;rdquo;，J。Environ。《经济与管理》，第5卷，第81-102页，1978年。在Belsley，Kuh和Welsch中使用，&amp;ldquo;回归诊断&amp;hellip;&amp;hellip;&amp;rdquo;，Wiley，1980年。注意后者的244-261页上的表格中使用了各种转换。</target>
        </trans-unit>
        <trans-unit id="7bc398b9797cca816dee96d9ed40f902ff743772" translate="yes" xml:space="preserve">
          <source>The Bunch object is a dictionary that exposes its keys are attributes. For more information about Bunch object, see &lt;a href=&quot;../modules/generated/sklearn.utils.bunch#sklearn.utils.Bunch&quot;&gt;&lt;code&gt;sklearn.utils.Bunch&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">Bunch对象是一个字典，它公开其键是属性。有关Bunch对象的更多信息，请参见&lt;a href=&quot;../modules/generated/sklearn.utils.bunch#sklearn.utils.Bunch&quot;&gt; &lt;code&gt;sklearn.utils.Bunch&lt;/code&gt; &lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="0e6c056d908bee991f648998f41ff72e8a7301c2" translate="yes" xml:space="preserve">
          <source>The CF Subclusters hold the necessary information for clustering which prevents the need to hold the entire input data in memory. This information includes:</source>
          <target state="translated">CF子簇保存了聚类所需的信息,从而避免了在内存中保存全部输入数据的需要。这些信息包括:</target>
        </trans-unit>
        <trans-unit id="b64782d12e8b4e97de28df15435a193ab8a372e9" translate="yes" xml:space="preserve">
          <source>The Calinski-Harabasz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">凸簇的Calinski-Harabasz指数一般要高于其他概念的簇,如通过DBSCAN得到的基于密度的簇。</target>
        </trans-unit>
        <trans-unit id="b96c855c122e21633007587f3600e092339a5383" translate="yes" xml:space="preserve">
          <source>The Calinski-Harabaz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">Calinski-Harabaz指数对于凸簇来说,一般要高于其他概念的簇,比如像通过DBSCAN得到的基于密度的簇。</target>
        </trans-unit>
        <trans-unit id="6cdbc3a888667e8344981c2a8742122994627087" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier described in Rennie et al.</source>
          <target state="translated">Rennie等人描述的补全奈夫贝叶斯分类器。</target>
        </trans-unit>
        <trans-unit id="2b0b8b0a4dd5523203a6ee4b5d195c2e6a35c0fe" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier described in Rennie et al. (2003).</source>
          <target state="translated">Rennie等人(2003)中描述的补全Naive Bayes分类器。</target>
        </trans-unit>
        <trans-unit id="3398d87a3aceb1ea8375c52fcc6a67cec5c63df9" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier was designed to correct the &amp;ldquo;severe assumptions&amp;rdquo; made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.</source>
          <target state="translated">补充朴素贝叶斯分类器旨在纠正标准多项朴素贝叶斯分类器所做的&amp;ldquo;严峻假设&amp;rdquo;。它特别适合于不平衡的数据集。</target>
        </trans-unit>
        <trans-unit id="7c176cfd9628fae51161daa03c193aeae01d49ea" translate="yes" xml:space="preserve">
          <source>The Contrastive Divergence method suggests to stop the chain after a small number of iterations, \(k\), usually even 1. This method is fast and has low variance, but the samples are far from the model distribution.</source>
          <target state="translated">Contrastive Divergence方法建议在少量迭代后停止链,通常甚至是1,这种方法速度快,方差小,但样本远离模型分布。</target>
        </trans-unit>
        <trans-unit id="1d09474bd461f8756796df6e9a77749a8489b6d3" translate="yes" xml:space="preserve">
          <source>The DBSCAN algorithm is deterministic, always generating the same clusters when given the same data in the same order. However, the results can differ when data is provided in a different order. First, even though the core samples will always be assigned to the same clusters, the labels of those clusters will depend on the order in which those samples are encountered in the data. Second and more importantly, the clusters to which non-core samples are assigned can differ depending on the data order. This would happen when a non-core sample has a distance lower than &lt;code&gt;eps&lt;/code&gt; to two core samples in different clusters. By the triangular inequality, those two core samples must be more distant than &lt;code&gt;eps&lt;/code&gt; from each other, or they would be in the same cluster. The non-core sample is assigned to whichever cluster is generated first in a pass through the data, and so the results will depend on the data ordering.</source>
          <target state="translated">DBSCAN算法是确定性的，当以相同的顺序提供相同的数据时，总是会生成相同的簇。但是，当以不同顺序提供数据时，结果可能会有所不同。首先，即使将核心样本始终分配给相同的聚类，但这些聚类的标签将取决于在数据中遇到这些样本的顺序。其次，更重要的是，分配非核心样本的群集可能会根据数据顺序而有所不同。当非核心样本的距离小于 &lt;code&gt;eps&lt;/code&gt; 到不同聚类中的两个核心样本的距离时，就会发生这种情况。通过三角不等式，这两个核心样本必须比 &lt;code&gt;eps&lt;/code&gt; 更远彼此之间，否则它们将在同一群集中。非核心样本将分配给在通过数据时首先生成的任何集群，因此结果将取决于数据顺序。</target>
        </trans-unit>
        <trans-unit id="b5783662e991f62ab3cc63e2843b11c10be0af6e" translate="yes" xml:space="preserve">
          <source>The Davies-Boulding index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained from DBSCAN.</source>
          <target state="translated">Davies-Boulding指数对于凸型聚类来说,一般比其他聚类的概念要高,比如基于密度的聚类,比如从DBSCAN得到的聚类。</target>
        </trans-unit>
        <trans-unit id="9dae6187a2e4264ce3764c47d6ced56e22112bc3" translate="yes" xml:space="preserve">
          <source>The Digit Dataset</source>
          <target state="translated">数字数据集</target>
        </trans-unit>
        <trans-unit id="15949e73904f01f7a29e0206a3232b388b3af43d" translate="yes" xml:space="preserve">
          <source>The Dirichlet process prior allows to define an infinite number of components and automatically selects the correct number of components: it activates a component only if it is necessary.</source>
          <target state="translated">Dirichlet过程之前允许定义无限数量的组件,并自动选择正确的组件数量:只有在必要的情况下才会激活一个组件。</target>
        </trans-unit>
        <trans-unit id="a257bf9309fabcccadc69c1bb14244b979e6f754" translate="yes" xml:space="preserve">
          <source>The Discounted Cumulative Gain divided by the Ideal Discounted Cumulative Gain (the DCG obtained for a perfect ranking), in order to have a score between 0 and 1.</source>
          <target state="translated">折现累计收益除以理想折现累计收益(完美排名所得到的DCG),才能有0到1之间的分数。</target>
        </trans-unit>
        <trans-unit id="d2e49448333a2cd92baf05825e927765f1835316" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is commonly combined with exponentiation.</source>
          <target state="translated">DotProduct内核通常与指数相结合。</target>
        </trans-unit>
        <trans-unit id="842c0c72bca462c8ce3e9ff0cead821c33430a8a" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is non-stationary and can be obtained from linear regression by putting N(0, 1) priors on the coefficients of x_d (d = 1, . . . , D) and a prior of N(0, sigma_0^2) on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter sigma_0^2. For sigma_0^2 =0, the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</source>
          <target state="translated">DotProduct核是非稳态的,可以通过对x_d的系数(d=1,......,D)设置N(0,1)前导,对偏置设置N(0,sigma_0^2)前导,从线性回归中得到。DotProduct核对原点坐标的旋转不变,但对平移不变。它的参数是sigma_0^2。对于sigma_0^2=0,这个核被称为同质线性核,否则就是非同质的。该核由以下公式给出</target>
        </trans-unit>
        <trans-unit id="078b4d2b566f931479e95c97a38f05103a2ecb7d" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is non-stationary and can be obtained from linear regression by putting \(N(0, 1)\) priors on the coefficients of \(x_d (d = 1, . . . , D)\) and a prior of \(N(0, \sigma_0^2)\) on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter sigma_0 \(\sigma\) which controls the inhomogenity of the kernel. For \(\sigma_0^2 =0\), the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</source>
          <target state="translated">DotProduct核是非稳态的,可以从线性回归中得到,通过在x_d(d=1,...,D)/的系数上设置(N(0,...,D)/)前导,在偏置上设置(N(0,...,...,D)/)前导。DotProduct核对原点坐标的旋转不变,但对平移不变。它的参数化是一个参数sigma_0 Һ(sigma/sigma/),它控制核的不均匀性。对于/(sigma_0^2 =0)/(),该核被称为同质线性核,否则为非同质核。该核由以下公式给出</target>
        </trans-unit>
        <trans-unit id="c219b28ff4dd0afb8b865b96896a279315780153" translate="yes" xml:space="preserve">
          <source>The Elastic Net mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15.</source>
          <target state="translated">0 &amp;lt;= l1_ratio &amp;lt;= 1的弹性网混合参数。l1_ratio = 0对应于L2惩罚，l1_ratio = 1到L1。默认值为0.15。</target>
        </trans-unit>
        <trans-unit id="bc71ca7c36fa98be5da488c2c0295a9dca2a49c6" translate="yes" xml:space="preserve">
          <source>The Elastic Net mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Only used if &lt;code&gt;penalty&lt;/code&gt; is &amp;lsquo;elasticnet&amp;rsquo;.</source>
          <target state="translated">0 &amp;lt;= l1_ratio &amp;lt;= 1的弹性网混合参数。l1_ratio = 0对应于L2惩罚，l1_ratio = 1到L1。仅在 &lt;code&gt;penalty&lt;/code&gt; 为&amp;ldquo; elasticnet&amp;rdquo;时使用。</target>
        </trans-unit>
        <trans-unit id="557cde32667038df92bd17ed0a660f8397aa9f15" translate="yes" xml:space="preserve">
          <source>The Elastic-Net mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. Only used if &lt;code&gt;penalty='elasticnet'&lt;/code&gt;. Setting &lt;code&gt;l1_ratio=0&lt;/code&gt; is equivalent to using &lt;code&gt;penalty='l2'&lt;/code&gt;, while setting &lt;code&gt;l1_ratio=1&lt;/code&gt; is equivalent to using &lt;code&gt;penalty='l1'&lt;/code&gt;. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt;1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="translated">&lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt; 的Elastic-Net混合参数。仅在 &lt;code&gt;penalty='elasticnet'&lt;/code&gt; 。设置 &lt;code&gt;l1_ratio=0&lt;/code&gt; 等效于使用 &lt;code&gt;penalty='l2'&lt;/code&gt; ，而设置 &lt;code&gt;l1_ratio=1&lt;/code&gt; 等效于使用 &lt;code&gt;penalty='l1'&lt;/code&gt; 。对于 &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt;1&lt;/code&gt; ，惩罚是L1和L2的组合。</target>
        </trans-unit>
        <trans-unit id="71317f65b5997839fb7f2d86b8c73dc4399ad254" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with 0 &amp;lt; l1_ratio &amp;lt;= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1/L2 and L2.</source>
          <target state="translated">ElasticNet混合参数，其中0 &amp;lt;l1_ratio &amp;lt;=1。对于l1_ratio = 1，惩罚是L1 / L2惩罚。对于l1_ratio = 0，这是L2损失。对于 &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; ，惩罚是L1 / L2和L2的组合。</target>
        </trans-unit>
        <trans-unit id="f913d5ab29d41b1d0b8510f8960f88320058e481" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with 0 &amp;lt; l1_ratio &amp;lt;= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1/L2 and L2. This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in &lt;code&gt;[.1, .5, .7,
.9, .95, .99, 1]&lt;/code&gt;</source>
          <target state="translated">ElasticNet混合参数，其中0 &amp;lt;l1_ratio &amp;lt;=1。对于l1_ratio = 1，惩罚是L1 / L2惩罚。对于l1_ratio = 0，这是L2损失。对于 &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; ，惩罚是L1 / L2和L2的组合。此参数可以是一个列表，在这种情况下，通过交叉验证测试不同的值，并使用给出最佳预测得分的值。请注意，最好选择l1_ratio的值列表，如 &lt;code&gt;[.1, .5, .7, .9, .95, .99, 1]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="62ded71b403044434993092527c854b8f53e5c17" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. For &lt;code&gt;l1_ratio = 0&lt;/code&gt; the penalty is an L2 penalty. &lt;code&gt;For l1_ratio = 1&lt;/code&gt; it is an L1 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="translated">ElasticNet混合参数， &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt; 。对于 &lt;code&gt;l1_ratio = 0&lt;/code&gt; ，惩罚是L2惩罚。 &lt;code&gt;For l1_ratio = 1&lt;/code&gt; 这是L1损失。对于 &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; ，惩罚是L1和L2的组合。</target>
        </trans-unit>
        <trans-unit id="706ede4c7ae02c53bd29139976a5c9ab95013fca" translate="yes" xml:space="preserve">
          <source>The ExpSineSquared kernel allows modeling periodic functions. It is parameterized by a length-scale parameter length_scale&amp;gt;0 and a periodicity parameter periodicity&amp;gt;0. Only the isotropic variant where l is a scalar is supported at the moment. The kernel given by:</source>
          <target state="translated">ExpSineSquared内核允许对周期性函数进行建模。它由长度比例参数length_scale&amp;gt; 0和周期性参数周期性&amp;gt; 0参数化。目前仅支持l为标量的各向同性变体。内核由：</target>
        </trans-unit>
        <trans-unit id="648ff6d48b0697b9c2efa5d7b7f9a5acf1d3efbe" translate="yes" xml:space="preserve">
          <source>The ExpSineSquared kernel allows one to model functions which repeat themselves exactly. It is parameterized by a length scale parameter \(l&amp;gt;0\) and a periodicity parameter \(p&amp;gt;0\). Only the isotropic variant where \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="translated">ExpSineSquared内核允许对能够精确重复自身的函数进行建模。它由长度比例参数\（l&amp;gt; 0 \）和周期性参数\（p&amp;gt; 0 \）进行参数化。目前仅支持\（l \）是标量的各向同性变体。内核由下式给出：</target>
        </trans-unit>
        <trans-unit id="31f2158684fd941b75a9b2f295ee3be83d4fb497" translate="yes" xml:space="preserve">
          <source>The Exponentiation kernel takes one base kernel and a scalar parameter \(p\) and combines them via</source>
          <target state="translated">指数核取一个基核和一个标量参数 \(p\),并通过以下方式结合起来</target>
        </trans-unit>
        <trans-unit id="bd1aecbb19e2286cfafdebb8292f1071a3eb508c" translate="yes" xml:space="preserve">
          <source>The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.</source>
          <target state="translated">F-beta得分可以解释为精度和召回率的加权谐波平均值,F-beta得分在1时达到最佳值,在0时达到最差值。</target>
        </trans-unit>
        <trans-unit id="fcc8a075bfea5f42d0e38256200386182d33cc42" translate="yes" xml:space="preserve">
          <source>The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0.</source>
          <target state="translated">F-beta得分是精度和召回率的加权谐波平均值,达到最佳值为1,最差值为0。</target>
        </trans-unit>
        <trans-unit id="553773b1d0f7903e424f857dbba13f4b2343a5a1" translate="yes" xml:space="preserve">
          <source>The F-beta score weights recall more than precision by a factor of &lt;code&gt;beta&lt;/code&gt;. &lt;code&gt;beta == 1.0&lt;/code&gt; means recall and precision are equally important.</source>
          <target state="translated">F-beta得分权重比 &lt;code&gt;beta&lt;/code&gt; 精确度高得多。 &lt;code&gt;beta == 1.0&lt;/code&gt; 表示召回率和精确度同等重要。</target>
        </trans-unit>
        <trans-unit id="3f53f44feaa854c0fdf5365ef5e43bc7fa440b01" translate="yes" xml:space="preserve">
          <source>The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:</source>
          <target state="translated">F1得分可以理解为精度和召回的加权平均值,F1得分在1时达到最佳值,0时达到最差值,精度和召回对F1得分的相对贡献是相等的。F1得分的计算公式为:。</target>
        </trans-unit>
        <trans-unit id="518509d08bd98ece386f11a6583f35b4f559fdac" translate="yes" xml:space="preserve">
          <source>The Figure below shows four one-way and one two-way partial dependence plots for the California housing dataset:</source>
          <target state="translated">下图显示了加州住房数据集的四个单向和一个双向部分依赖图。</target>
        </trans-unit>
        <trans-unit id="8d40f6c8f7d760d5a9c3769f72b85293905bcf21" translate="yes" xml:space="preserve">
          <source>The Figure below shows the contours of the different regularization terms in a 2-dimensional parameter space (\(m=2\)) when \(R(w) = 1\).</source>
          <target state="translated">下图显示了二维参数空间中不同正则化项的轮廓(/(m=2//)),当/(R(w)=1)/。</target>
        </trans-unit>
        <trans-unit id="2280ec0b9d5ba5eef024fd7f6041defdab8c9d48" translate="yes" xml:space="preserve">
          <source>The Figure below shows the contours of the different regularization terms in the parameter space when \(R(w) = 1\).</source>
          <target state="translated">下图显示了参数空间中不同正则化项的轮廓,当/(R(w)=1/)。</target>
        </trans-unit>
        <trans-unit id="af21e4c771514d1aca6cd1c14dcff1ce67a0e477" translate="yes" xml:space="preserve">
          <source>The Fowlkes-Mallows index (&lt;a href=&quot;generated/sklearn.metrics.fowlkes_mallows_score#sklearn.metrics.fowlkes_mallows_score&quot;&gt;&lt;code&gt;sklearn.metrics.fowlkes_mallows_score&lt;/code&gt;&lt;/a&gt;) can be used when the ground truth class assignments of the samples is known. The Fowlkes-Mallows score FMI is defined as the geometric mean of the pairwise precision and recall:</source>
          <target state="translated">当已知样本的地面真理类分配时，可以使用Fowlkes-Mallows索引（&lt;a href=&quot;generated/sklearn.metrics.fowlkes_mallows_score#sklearn.metrics.fowlkes_mallows_score&quot;&gt; &lt;code&gt;sklearn.metrics.fowlkes_mallows_score&lt;/code&gt; &lt;/a&gt;）。 Fowlkes-Mallows分数FMI定义为成对精度和召回率的几何平均值：</target>
        </trans-unit>
        <trans-unit id="bfdc8ed29be1f90cbfb4dc9576ce17a736f93294" translate="yes" xml:space="preserve">
          <source>The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of the precision and recall:</source>
          <target state="translated">Fowlkes-Mallows指数(FMI)被定义为精确度和召回率之间的几何平均值。</target>
        </trans-unit>
        <trans-unit id="fb7ab21b6034e9b9d6397353d40d09f0a02e126e" translate="yes" xml:space="preserve">
          <source>The French Motor Third-Party Liability Claims dataset</source>
          <target state="translated">法国汽车第三方责任索赔数据集</target>
        </trans-unit>
        <trans-unit id="90c804ef552ac504772d87a0c90e2454ea50931f" translate="yes" xml:space="preserve">
          <source>The GP prior mean is assumed to be zero. The prior&amp;rsquo;s covariance is specified by a passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="translated">假设GP先验均值为零。先验的协方差由传递的&lt;a href=&quot;#gp-kernels&quot;&gt;内核&lt;/a&gt;对象指定。通过基于传递的 &lt;code&gt;optimizer&lt;/code&gt; 最大化对数边际似然性（LML）来优化GaussianProcessRegressor期间，优化内核的超参数。由于LML可能具有多个局部最优值，因此可以通过指定 &lt;code&gt;n_restarts_optimizer&lt;/code&gt; 重复启动优化器。始终从内核的初始超参数值开始进行第一次运行；随后的运行是从已从允许值范围内随机选择的超参数值进行的。如果初始超参数应保持固定，则不能将 &lt;code&gt;None&lt;/code&gt; 参数作为优化器传递。</target>
        </trans-unit>
        <trans-unit id="060746131ee7579fe50d723eccf0216e72cc0775" translate="yes" xml:space="preserve">
          <source>The GP prior mean is assumed to be zero. The prior&amp;rsquo;s covariance is specified by passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="translated">假设GP先验均值为零。通过传递&lt;a href=&quot;#gp-kernels&quot;&gt;内核&lt;/a&gt;对象来指定先验的协方差。通过基于传递的 &lt;code&gt;optimizer&lt;/code&gt; 最大化对数边际似然性（LML）来优化GaussianProcessRegressor期间，优化内核的超参数。由于LML可能具有多个局部最优值，因此可以通过指定 &lt;code&gt;n_restarts_optimizer&lt;/code&gt; 重复启动优化器。始终从内核的初始超参数值开始进行第一次运行；随后的运行是根据已从允许值范围内随机选择的超参数值进行的。如果初始超参数应保持固定，则不能将 &lt;code&gt;None&lt;/code&gt; 参数作为优化器传递。</target>
        </trans-unit>
        <trans-unit id="d9ff401cc389e116b429c312d580f50e1ed79479" translate="yes" xml:space="preserve">
          <source>The Gini coefficient (based on the area under the curve) can be used as a model selection metric to quantify the ability of the model to rank policyholders. Note that this metric does not reflect the ability of the models to make accurate predictions in terms of absolute value of total claim amounts but only in terms of relative amounts as a ranking metric.</source>
          <target state="translated">基尼系数(基于曲线下的面积)可以作为模型选择的指标,量化模型对保单持有人的排序能力。需要注意的是,这个指标并不能反映模型在总索赔金额的绝对值方面做出准确预测的能力,而只能作为排名指标,反映相对金额的能力。</target>
        </trans-unit>
        <trans-unit id="18dcaaaea80b191d9100a9a4ae22f95aa636616a" translate="yes" xml:space="preserve">
          <source>The Gini index reflects the ability of a model to rank predictions irrespective of their absolute values, and therefore only assess their ranking power.</source>
          <target state="translated">基尼指数反映的是模型的预测能力,不考虑其绝对值,因此只评估其排名能力。</target>
        </trans-unit>
        <trans-unit id="01a974069deaad9f4a696951139fa6f180c4610d" translate="yes" xml:space="preserve">
          <source>The HLLE algorithm comprises three stages:</source>
          <target state="translated">HLLE算法包括三个阶段:</target>
        </trans-unit>
        <trans-unit id="eaec5120030b979edc1dcb8e844874e0db8c76f1" translate="yes" xml:space="preserve">
          <source>The Hamming loss is the fraction of labels that are incorrectly predicted.</source>
          <target state="translated">汉明损失是指错误预测的标签的分数。</target>
        </trans-unit>
        <trans-unit id="304ac7ce83417558cbe44e7723fbf3a93a036677" translate="yes" xml:space="preserve">
          <source>The Hamming loss is upperbounded by the subset zero-one loss, when &lt;code&gt;normalize&lt;/code&gt; parameter is set to True. It is always between 0 and 1, lower being better.</source>
          <target state="translated">当 &lt;code&gt;normalize&lt;/code&gt; 参数设置为True时，汉明损失由子集零一损失限制。总是在0到1之间，越低越好。</target>
        </trans-unit>
        <trans-unit id="ee89b1587a7c7b8d189cccf628a41e55c5d7ebe4" translate="yes" xml:space="preserve">
          <source>The Hamming loss is upperbounded by the subset zero-one loss. When normalized over samples, the Hamming loss is always between 0 and 1.</source>
          <target state="translated">汉明损耗是由子集零一损耗上界的。当对样本进行归一化处理时,汉明损耗总是在0和1之间。</target>
        </trans-unit>
        <trans-unit id="e3001f46869621e71f2b9c72a2ca6b3e38966a32" translate="yes" xml:space="preserve">
          <source>The Haversine (or great circle) distance is the angular distance between two points on the surface of a sphere. The first distance of each point is assumed to be the latitude, the second is the longitude, given in radians. The dimension of the data must be 2.</source>
          <target state="translated">哈弗辛(或大圆)距离是球面上两点之间的角距离。每个点的第一个距离假定为纬度,第二个是经度,用弧度表示。数据的维度必须是2。</target>
        </trans-unit>
        <trans-unit id="2f575d5541e123fdf35ce0dd5c6fb4373f14dde3" translate="yes" xml:space="preserve">
          <source>The Huber Regressor optimizes the squared loss for the samples where &lt;code&gt;|(y - X'w) / sigma| &amp;lt; epsilon&lt;/code&gt; and the absolute loss for the samples where &lt;code&gt;|(y - X'w) / sigma| &amp;gt; epsilon&lt;/code&gt;, where w and sigma are parameters to be optimized. The parameter sigma makes sure that if y is scaled up or down by a certain factor, one does not need to rescale epsilon to achieve the same robustness. Note that this does not take into account the fact that the different features of X may be of different scales.</source>
          <target state="translated">Huber回归器针对 &lt;code&gt;|(y - X'w) / sigma| &amp;lt; epsilon&lt;/code&gt; 的样本优化平方损失。&amp;lt;epsilon和样本的绝对损耗，其中 &lt;code&gt;|(y - X'w) / sigma| &amp;gt; epsilon&lt;/code&gt; ，其中w和sigma是要优化的参数。参数sigma可确保如果y放大或缩小某个因子，则无需重新缩放epsilon即可达到相同的鲁棒性。请注意，这没有考虑X的不同特征可能具有不同比例的事实。</target>
        </trans-unit>
        <trans-unit id="a428acf86562c566ebfc0a4a1d3e42c700a1a75f" translate="yes" xml:space="preserve">
          <source>The Huber and epsilon-insensitive loss functions can be used for robust regression. The width of the insensitive region has to be specified via the parameter &lt;code&gt;epsilon&lt;/code&gt;. This parameter depends on the scale of the target variables.</source>
          <target state="translated">Huber和&amp;epsilon;不敏感的损失函数可用于鲁棒回归。不敏感区域的宽度必须通过参数 &lt;code&gt;epsilon&lt;/code&gt; 指定。此参数取决于目标变量的范围。</target>
        </trans-unit>
        <trans-unit id="0a2150cee6da11610a14f68e745e5d0639a39459" translate="yes" xml:space="preserve">
          <source>The Iris Dataset</source>
          <target state="translated">虹膜数据集</target>
        </trans-unit>
        <trans-unit id="7c0ef25371cb6aecb434ff0290ad88905a1c2367" translate="yes" xml:space="preserve">
          <source>The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour and Virginica) with 4 attributes: sepal length, sepal width, petal length and petal width.</source>
          <target state="translated">鸢尾花数据集代表了3种鸢尾花(Setosa、Versicolour和Virginica),有4个属性:萼片长度、萼片宽度、花瓣长度和花瓣宽度。</target>
        </trans-unit>
        <trans-unit id="3877561d1fbee6aff8708ef4ec5fcabe6115833e" translate="yes" xml:space="preserve">
          <source>The IsolationForest &amp;lsquo;isolates&amp;rsquo; observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.</source>
          <target state="translated">IsolationForest通过随机选择特征，然后随机选择选定特征的最大值和最小值之间的分割值来&amp;ldquo;隔离&amp;rdquo;观察结果。</target>
        </trans-unit>
        <trans-unit id="9e47d533732129c7b308a9673031913c04e17afd" translate="yes" xml:space="preserve">
          <source>The Isomap algorithm comprises three stages:</source>
          <target state="translated">等值线图算法包括三个阶段。</target>
        </trans-unit>
        <trans-unit id="43f0627e56441244b0d44b83d4abbf8510d9eac0" translate="yes" xml:space="preserve">
          <source>The Jaccard index [1], or Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets, is used to compare set of predicted labels for a sample to the corresponding set of labels in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">Jaccard索引[1]或Jaccard相似系数，定义为交集的大小除以两个标签集的并集的大小，用于将样本的预测标签集与 &lt;code&gt;y_true&lt;/code&gt; 中的相应标签集进行比较。</target>
        </trans-unit>
        <trans-unit id="54b0fb2f21ad6f516f16dd3562af3d3b867660fe" translate="yes" xml:space="preserve">
          <source>The Jaccard similarity coefficient of the \(i\)-th samples, with a ground truth label set \(y_i\) and predicted label set \(\hat{y}_i\), is defined as</source>
          <target state="translated">第(i)个样本的Jaccard相似性系数,地面真实标签集(y_i\)和预测标签集(\hat{y}_i\),定义为</target>
        </trans-unit>
        <trans-unit id="1b6ab133b67b3354e1eaee98fb0609deafb4a4e0" translate="yes" xml:space="preserve">
          <source>The Johnson-Lindenstrauss bound for embedding with random projections</source>
          <target state="translated">嵌入与随机投影的Johnson-Lindenstrauss约束。</target>
        </trans-unit>
        <trans-unit id="e9607194f44934f99bc18e8a1d649dcabdd2e40a" translate="yes" xml:space="preserve">
          <source>The K-means algorithm aims to choose centroids that minimise the &lt;strong&gt;inertia&lt;/strong&gt;, or &lt;strong&gt;within-cluster sum-of-squares criterion&lt;/strong&gt;:</source>
          <target state="translated">K-means算法的目的是选择使&lt;strong&gt;惯性&lt;/strong&gt;最小的质心或&lt;strong&gt;群集内平方和标准&lt;/strong&gt;：</target>
        </trans-unit>
        <trans-unit id="2e420c9d276fc6531e2d5a1fd3367ac4465f247c" translate="yes" xml:space="preserve">
          <source>The KDD Cup &amp;lsquo;99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the &lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting &amp;lsquo;abnormal&amp;rsquo; data, ie</source>
          <target state="translated">KDD Cup '99数据集是通过处理MIT Lincoln Lab [1]创建的1998 DARPA入侵检测系统（IDS）评估数据集的tcpdump部分而创建的。人工数据（在&lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;数据集的首页&lt;/a&gt;上进行了描述）是使用封闭网络生成的，并通过手动注入攻击来产生大量不同类型的攻击，而这些攻击在后台都是正常活动的。由于最初的目标是为监督学习算法生成大型训练集，因此有很大一部分（80.1％）异常数据在现实世界中是不现实的，因此不适合用于旨在检测&amp;ldquo;异常&amp;rdquo;数据的无监督异常检测，即</target>
        </trans-unit>
        <trans-unit id="c580c78b1750c851ebf65b6b32d643554e25c44f" translate="yes" xml:space="preserve">
          <source>The KDD Cup &amp;lsquo;99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the &lt;a href=&quot;https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting &amp;lsquo;abnormal&amp;rsquo; data, ie</source>
          <target state="translated">KDD Cup '99数据集是通过处理MIT Lincoln Lab [1]创建的1998 DARPA入侵检测系统（IDS）评估数据集的tcpdump部分而创建的。人工数据（在&lt;a href=&quot;https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;数据集首页&lt;/a&gt;上进行了描述）是使用封闭网络生成的，并通过手动注入攻击来产生大量不同类型的攻击，而这些攻击在后台都是正常活动的。由于最初的目标是为监督学习算法生成大型训练集，因此有很大一部分（80.1％）异常数据在现实世界中是不现实的，因此不适合用于旨在检测&amp;ldquo;异常&amp;rdquo;数据的无监督异常检测， IE</target>
        </trans-unit>
        <trans-unit id="42488af2a88df53e7b3301c7d5a125afdc9cd2ce" translate="yes" xml:space="preserve">
          <source>The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will be minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different initializations will end up in local minima of the KL divergence. Hence, it is sometimes useful to try different seeds and select the embedding with the lowest KL divergence.</source>
          <target state="translated">原始空间和嵌入空间的联合概率的Kullback-Leibler(KL)发散将通过梯度下降最小化。需要注意的是,KL分歧并不是凸的,也就是说,用不同的初始化进行多次重启会导致KL分歧的局部最小化。因此,有时尝试不同的种子并选择KL分歧最小的嵌入是有用的。</target>
        </trans-unit>
        <trans-unit id="c1f022c5d8701b9b00fce305ae262139841dede3" translate="yes" xml:space="preserve">
          <source>The L2 regularization parameter. Use 0 for no regularization.</source>
          <target state="translated">L2正则化参数。使用0表示没有正则化。</target>
        </trans-unit>
        <trans-unit id="5b439efc6a95c460b62f48e9b2ed58ff5ae6d780" translate="yes" xml:space="preserve">
          <source>The L2 regularization parameter. Use &lt;code&gt;0&lt;/code&gt; for no regularization (default).</source>
          <target state="translated">L2正则化参数。使用 &lt;code&gt;0&lt;/code&gt; 表示不进行正则化（默认）。</target>
        </trans-unit>
        <trans-unit id="9a35ab88c062292779ce626a30b50f45159613b2" translate="yes" xml:space="preserve">
          <source>The LARS model can be used using estimator &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt;&lt;code&gt;Lars&lt;/code&gt;&lt;/a&gt;, or its low-level implementation &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.linear_model.lars_path_gram#sklearn.linear_model.lars_path_gram&quot;&gt;&lt;code&gt;lars_path_gram&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以使用估计器&lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt; &lt;code&gt;Lars&lt;/code&gt; &lt;/a&gt;或其低级实现&lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;generated/sklearn.linear_model.lars_path_gram#sklearn.linear_model.lars_path_gram&quot;&gt; &lt;code&gt;lars_path_gram&lt;/code&gt; &lt;/a&gt;来使用LARS模型。</target>
        </trans-unit>
        <trans-unit id="5c5e824f9a2e5664a81c2abf8d8de696419cc018" translate="yes" xml:space="preserve">
          <source>The LARS model can be used using estimator &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt;&lt;code&gt;Lars&lt;/code&gt;&lt;/a&gt;, or its low-level implementation &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以使用估算器&lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt; &lt;code&gt;Lars&lt;/code&gt; &lt;/a&gt;或它的底层实现&lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; &lt;/a&gt;来使用LARS模型。</target>
        </trans-unit>
        <trans-unit id="5604e6549ba6e538c77a361942c228433b92a65e" translate="yes" xml:space="preserve">
          <source>The LTSA algorithm comprises three stages:</source>
          <target state="translated">LTSA算法包括三个阶段。</target>
        </trans-unit>
        <trans-unit id="5921a49032d4468242b223b9e118c35472eff0fa" translate="yes" xml:space="preserve">
          <source>The Lars algorithm provides the full path of the coefficients along the regularization parameter almost for free, thus a common operation consist of retrieving the path with function &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Lars算法几乎免费提供沿着正则化参数的系数的完整路径，因此常见的操作包括使用函数&lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; &lt;/a&gt;检索路径</target>
        </trans-unit>
        <trans-unit id="066ac8e9a006935363e41b727fa078b6cbe4a689" translate="yes" xml:space="preserve">
          <source>The Lars algorithm provides the full path of the coefficients along the regularization parameter almost for free, thus a common operation is to retrieve the path with one of the functions &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.linear_model.lars_path_gram#sklearn.linear_model.lars_path_gram&quot;&gt;&lt;code&gt;lars_path_gram&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Lars算法几乎免费提供沿着正则化参数的系数的完整路径，因此常见的操作是使用函数&lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;generated/sklearn.linear_model.lars_path_gram#sklearn.linear_model.lars_path_gram&quot;&gt; &lt;code&gt;lars_path_gram&lt;/code&gt; &lt;/a&gt;之一检索路径。</target>
        </trans-unit>
        <trans-unit id="9503ea7db44738c355b5bdf4d0264864cc9e2161" translate="yes" xml:space="preserve">
          <source>The Lasso is a linear model that estimates sparse coefficients with l1 regularization.</source>
          <target state="translated">Lasso是一个线性模型,它可以用l1个正则化来估计稀疏系数。</target>
        </trans-unit>
        <trans-unit id="0f5aa687d48724082c940128def6bdb3f6066405" translate="yes" xml:space="preserve">
          <source>The Lasso optimization function varies for mono and multi-outputs.</source>
          <target state="translated">Lasso优化函数对于单输出和多输出来说是不同的。</target>
        </trans-unit>
        <trans-unit id="3c1b2c3b1551762e6f1da2d8a29e9acf6404c123" translate="yes" xml:space="preserve">
          <source>The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where number of features is greater than number of samples. Elsewhere prefer cd which is more numerically stable.</source>
          <target state="translated">使用的Lasso求解器:坐标下降法或LARS。对于非常稀疏的底层图形,即特征数大于样本数的情况,使用LARS。其他地方首选cd,数值上更稳定。</target>
        </trans-unit>
        <trans-unit id="2de8b3228aef7c3b031e79cb56d858268d66f64e" translate="yes" xml:space="preserve">
          <source>The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where p &amp;gt; n. Elsewhere prefer cd which is more numerically stable.</source>
          <target state="translated">要使用的套索求解器：协调下降或LARS。将LARS用于非常稀疏的基础图，其中p&amp;gt; n。在其他地方，首选CD数值更稳定。</target>
        </trans-unit>
        <trans-unit id="59ce670bca31c79340517b84d85b37c8ca4886a2" translate="yes" xml:space="preserve">
          <source>The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt;&lt;code&gt;ledoit_wolf&lt;/code&gt;&lt;/a&gt; function of the &lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; package, or it can be otherwise obtained by fitting a &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt;&lt;code&gt;LedoitWolf&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">可以使用&lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt; &lt;code&gt;sklearn.covariance&lt;/code&gt; &lt;/a&gt;包的&lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt; &lt;code&gt;ledoit_wolf&lt;/code&gt; &lt;/a&gt;函数在样本上计算协方差矩阵的Ledoit-Wolf估计量，或者可以通过将&lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt; &lt;code&gt;LedoitWolf&lt;/code&gt; &lt;/a&gt;对象拟合到同一样本来获得协方差矩阵的Ledoit-Wolf估计量。</target>
        </trans-unit>
        <trans-unit id="6c4c48b93f6a66f7991382ba54fcfe5fb18a6ebb" translate="yes" xml:space="preserve">
          <source>The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt;&lt;code&gt;ledoit_wolf&lt;/code&gt;&lt;/a&gt; function of the &lt;code&gt;sklearn.covariance&lt;/code&gt; package, or it can be otherwise obtained by fitting a &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt;&lt;code&gt;LedoitWolf&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">可以使用 &lt;code&gt;sklearn.covariance&lt;/code&gt; 包的&lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt; &lt;code&gt;ledoit_wolf&lt;/code&gt; &lt;/a&gt;函数在样本上计算协方差矩阵的Ledoit-Wolf估计量，或者可以通过将&lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt; &lt;code&gt;LedoitWolf&lt;/code&gt; &lt;/a&gt;对象拟合到同一样本来获得协方差矩阵的Ledoit-Wolf估计量。</target>
        </trans-unit>
        <trans-unit id="aec2f6a4d0fb4e3ecc2ba56ea33e122b851a535b" translate="yes" xml:space="preserve">
          <source>The Linnerud dataset constains two small dataset:</source>
          <target state="translated">Linnerud数据集制约着两个小数据集。</target>
        </trans-unit>
        <trans-unit id="6be9aee52b8392e35ed4aeda9bdbad80150f0295" translate="yes" xml:space="preserve">
          <source>The Linnerud dataset is a multi-output regression dataset. It consists of three excercise (data) and three physiological (target) variables collected from twenty middle-aged men in a fitness club:</source>
          <target state="translated">Linnerud数据集是一个多输出回归数据集。它由三个运动(数据)和三个生理(目标)变量组成,这些变量是从一家健身俱乐部的20名中年男子那里收集的。</target>
        </trans-unit>
        <trans-unit id="0460202c91e1eee28482597354a8366af4e5eb02" translate="yes" xml:space="preserve">
          <source>The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for novelty detection. Note that when LOF is used for novelty detection you MUST not use predict, decision_function and score_samples on the training set as this would lead to wrong results. You must only use these methods on new unseen data (which are not in the training set). See &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;User Guide&lt;/a&gt;: for details on the difference between outlier detection and novelty detection and how to use LOF for outlier detection.</source>
          <target state="translated">局部离群因子（LOF）算法是一种无监督的异常检测方法，可计算给定数据点相对于其邻居的局部密度偏差。它认为密度远低于其邻居的样本为异常值。本示例说明如何使用LOF进行新颖性检测。请注意，将LOF用于新颖性检测时，切勿在训练集上使用预测，decision_function和score_samples，因为这会导致错误的结果。您只能对新的看不见的数据（不在训练集中）使用这些方法。有关离群值检测和新颖性检测之间的区别以及如何使用LOF进行离群值检测的详细信息，请参见《&lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;用户指南》&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="d0e3831e71a419f2c6e3df80ad462189c4208703" translate="yes" xml:space="preserve">
          <source>The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for outlier detection which is the default use case of this estimator in scikit-learn. Note that when LOF is used for outlier detection it has no predict, decision_function and score_samples methods. See &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;User Guide&lt;/a&gt;: for details on the difference between outlier detection and novelty detection and how to use LOF for novelty detection.</source>
          <target state="translated">局部离群因子（LOF）算法是一种无监督的异常检测方法，可计算给定数据点相对于其邻居的局部密度偏差。它认为密度远低于其邻居的样本为异常值。本示例说明如何使用LOF进行异常检测，这是scikit-learn中此估计器的默认用例。请注意，将LOF用于离群值检测时，它没有预测，decision_function和score_samples方法。有关离群值检测和新颖性检测之间的区别以及如何使用LOF进行新颖性检测的详细信息，请参见《&lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;用户指南》&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e334ea68d0fb9bc258e189ffac7524671f8600d5" translate="yes" xml:space="preserve">
          <source>The MLLE algorithm comprises three stages:</source>
          <target state="translated">MLLE算法包括三个阶段:</target>
        </trans-unit>
        <trans-unit id="514baa8efb372e193dd4209109d33c4d6c755f88" translate="yes" xml:space="preserve">
          <source>The Matplotlib Figure object.</source>
          <target state="translated">Matplotlib图对象。</target>
        </trans-unit>
        <trans-unit id="5adf2471ff49dbf27acc1a8b4862b530e5b52366" translate="yes" xml:space="preserve">
          <source>The Matthews correlation coefficient (+1 represents a perfect prediction, 0 an average random prediction and -1 and inverse prediction).</source>
          <target state="translated">马修斯相关系数(+1代表完美预测,0代表平均随机预测,-1代表反向预测)。</target>
        </trans-unit>
        <trans-unit id="21efcbf7188af02e8a17818fd892c631c3cc436c" translate="yes" xml:space="preserve">
          <source>The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary and multiclass classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient. [source: Wikipedia]</source>
          <target state="translated">马修斯相关系数在机器学习中用于衡量二元和多类分类的质量。它考虑到了真阳性和假阳性和假阴性,通常被认为是一个平衡的衡量标准,即使在类的大小非常不同的情况下也可以使用。MCC本质上是一个介于-1和+1之间的相关系数值。系数为+1代表完美预测,0代表平均随机预测,-1代表反向预测。该统计量也被称为phi系数。[来源:维基百科]</target>
        </trans-unit>
        <trans-unit id="6faebd1cede47746805364be40ce28b059dea40d" translate="yes" xml:space="preserve">
          <source>The Mean Squared Error (in the sense of the Frobenius norm) between &lt;code&gt;self&lt;/code&gt; and &lt;code&gt;comp_cov&lt;/code&gt; covariance estimators.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 和 &lt;code&gt;comp_cov&lt;/code&gt; 协方差估计量之间的均方误差（按照Frobenius范式）。</target>
        </trans-unit>
        <trans-unit id="d3bca24cc7fb644916020d52b90a1ddb7136c5be" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant covariance estimator is to be applied on Gaussian-distributed data, but could still be relevant on data drawn from a unimodal, symmetric distribution. It is not meant to be used with multi-modal data (the algorithm used to fit a MinCovDet object is likely to fail in such a case). One should consider projection pursuit methods to deal with multi-modal datasets.</source>
          <target state="translated">最小协方差确定因子协方差估计器适用于高斯分布的数据,但仍可适用于单模态、对称分布的数据。它并不是要用于多模态数据(用于拟合MinCovDet对象的算法很可能在这种情况下失败)。人们应该考虑使用投影追寻方法来处理多模态数据集。</target>
        </trans-unit>
        <trans-unit id="451133ac20cdcd95892f17968e1c348a4b0f4b8f" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">PJRousseuw在&lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]中&lt;/a&gt;引入了最小协方差行列式估计器（MCD）。</target>
        </trans-unit>
        <trans-unit id="e452597962db33c03eccea44c483adb03cd8dfa7" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in &lt;a href=&quot;#id6&quot; id=&quot;id3&quot;&gt;3&lt;/a&gt;.</source>
          <target state="translated">PJRousseuw在&lt;a href=&quot;#id6&quot; id=&quot;id3&quot;&gt;3中&lt;/a&gt;引入了最小协方差行列式估计器（MCD）。</target>
        </trans-unit>
        <trans-unit id="d811e159b509e69af06fa8b1a6b421d8a9ea40ca" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in [1].</source>
          <target state="translated">P.J.Rousseuw在[1]中提出了最小协方差确定估计器(MCD)。</target>
        </trans-unit>
        <trans-unit id="ecfb18631be5aab9ee26a60642b369e5e4e48a3b" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust estimator of a data set&amp;rsquo;s covariance introduced by P.J. Rousseeuw in &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;3&lt;/a&gt;. The idea is to find a given proportion (h) of &amp;ldquo;good&amp;rdquo; observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations (&amp;ldquo;consistency step&amp;rdquo;). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set (&amp;ldquo;reweighting step&amp;rdquo;).</source>
          <target state="translated">最小协方差行列式估计器是PJ Rousseeuw在&lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;3中&lt;/a&gt;引入的数据集协方差的鲁棒估计器。这个想法是找到给定比例（h）的&amp;ldquo;良好&amp;rdquo;观测值，这些观测值不是离群值，然后计算它们的经验协方差矩阵。然后，对这个经验协方差矩阵进行重新缩放，以补偿观测值的执行选择（&amp;ldquo;一致性步骤&amp;rdquo;）。在计算了最小协方差决定因素估计量后，可以根据观测值的马氏距离对权重进行加权，从而对数据集的协方差矩阵进行加权估计（&amp;ldquo;加权步骤&amp;rdquo;）。</target>
        </trans-unit>
        <trans-unit id="92f2af418b12a7a58727045874e736340d33efbd" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust estimator of a data set&amp;rsquo;s covariance introduced by P.J. Rousseeuw in &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;[3]&lt;/a&gt;. The idea is to find a given proportion (h) of &amp;ldquo;good&amp;rdquo; observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations (&amp;ldquo;consistency step&amp;rdquo;). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set (&amp;ldquo;reweighting step&amp;rdquo;).</source>
          <target state="translated">最小协方差行列式估计器是PJ Rousseeuw在&lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;[3]中&lt;/a&gt;引入的数据集协方差的鲁棒估计器。这个想法是找到给定比例（h）的&amp;ldquo;良好&amp;rdquo;观测值，这些观测值不是离群值，然后计算它们的经验协方差矩阵。然后，对这个经验协方差矩阵进行重新缩放，以补偿观测值的执行选择（&amp;ldquo;一致性步骤&amp;rdquo;）。在计算了最小协方差决定因素估计量后，可以根据观测值的马氏距离对权重进行加权，从而对数据集的协方差矩阵进行加权估计（&amp;ldquo;加权步骤&amp;rdquo;）。</target>
        </trans-unit>
        <trans-unit id="26a5ec87ffc45e2d236a64ca891cc8c307910b23" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to \(\frac{n_\text{samples} - n_\text{features}-1}{2}\) outliers) estimator of covariance. The idea is to find \(\frac{n_\text{samples} + n_\text{features}+1}{2}\) observations whose empirical covariance has the smallest determinant, yielding a &amp;ldquo;pure&amp;rdquo; subset of observations from which to compute standards estimates of location and covariance. After a correction step aiming at compensating the fact that the estimates were learned from only a portion of the initial data, we end up with robust estimates of the data set location and covariance.</source>
          <target state="translated">最小协方差行列式估计器是一个可靠的高分解点（即，它可以用于估算高度污染的数据集的协方差矩阵，最高可达\（\ frac {n_ \ text {samples}-n_ \ text {features}- 1} {2} \）离群值）的协方差估计量。这个想法是找到\（\ frac {n_ \ text {samples} + n_ \ text {features} +1} {2} \）个观测值，它们的经验协方差具有最小的决定因素，从而产生&amp;ldquo;纯&amp;rdquo;观测值子集，计算位置和协方差的标准估计值。经过旨在补偿估计仅从一部分初始数据中获悉的事实的校正步骤后，我们最终得到了数据集位置和协方差的可靠估计。</target>
        </trans-unit>
        <trans-unit id="868c6b821132444289ad5893f70525225594997c" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to \(\frac{n_\text{samples}-n_\text{features}-1}{2}\) outliers) estimator of covariance. The idea is to find \(\frac{n_\text{samples}+n_\text{features}+1}{2}\) observations whose empirical covariance has the smallest determinant, yielding a &amp;ldquo;pure&amp;rdquo; subset of observations from which to compute standards estimates of location and covariance.</source>
          <target state="translated">最小协方差行列式估计器是一个稳健的高分解点（即，它可以用于估计高度污染的数据集的协方差矩阵，最高可达\（\ frac {n_ \ text {samples} -n_ \ text {features}- 1} {2} \）离群值）的协方差估计量。这个想法是找到\（\ frac {n_ \ text {samples} + n_ \ text {features} +1} {2} \）个观测值，它们的经验协方差具有最小的行列式，从而产生&amp;ldquo;纯&amp;rdquo;观测值子集，计算位置和协方差的标准估计值。</target>
        </trans-unit>
        <trans-unit id="b2ce758bd900de6631654a32b9b8962161ffb45f" translate="yes" xml:space="preserve">
          <source>The Mutual Information is a measure of the similarity between two labels of the same data. Where \(|U_i|\) is the number of the samples in cluster \(U_i\) and \(|V_j|\) is the number of the samples in cluster \(V_j\), the Mutual Information between clusterings \(U\) and \(V\) is given as:</source>
          <target state="translated">相互信息(Mutual Information)是衡量同一数据的两个标签之间相似度的方法。其中\(|U_i|\)是指聚类中的样本数(U_i\),\(|V_j|\)是指聚类中的样本数(V_j\),聚类中的互信息(U\)和(V\)之间的互信息给出为。</target>
        </trans-unit>
        <trans-unit id="4607a569600ef039f2071e80730b55efccd8dd8d" translate="yes" xml:space="preserve">
          <source>The Nystroem method, as implemented in &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is a general method for low-rank approximations of kernels. It achieves this by essentially subsampling the data on which the kernel is evaluated. By default &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; uses the &lt;code&gt;rbf&lt;/code&gt; kernel, but it can use any kernel function or a precomputed kernel matrix. The number of samples used - which is also the dimensionality of the features computed - is given by the parameter &lt;code&gt;n_components&lt;/code&gt;.</source>
          <target state="translated">如在&lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; 中&lt;/a&gt;实现的Nystroem方法是用于内核的低秩逼近的通用方法。它通过实质上对评估内核的数据进行二次采样来实现。默认情况下，&lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;使用 &lt;code&gt;rbf&lt;/code&gt; 内核，但它可以使用任何核函数或预先计算内核矩阵。参数 &lt;code&gt;n_components&lt;/code&gt; 给出了所使用的样本数（也是所计算特征的维数）。</target>
        </trans-unit>
        <trans-unit id="acd97cd02b3cf6e9137b2a9dda0c8e62c31c52e4" translate="yes" xml:space="preserve">
          <source>The OAS estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt;&lt;code&gt;oas&lt;/code&gt;&lt;/a&gt; function of the &lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; package, or it can be otherwise obtained by fitting an &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;OAS&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">协方差矩阵的OAS估计量可以使用&lt;a href=&quot;classes#module-sklearn.covariance&quot;&gt; &lt;code&gt;sklearn.covariance&lt;/code&gt; &lt;/a&gt;包的&lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt; &lt;code&gt;oas&lt;/code&gt; &lt;/a&gt;函数对样本进行计算，也可以通过将&lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt; &lt;code&gt;OAS&lt;/code&gt; &lt;/a&gt;对象拟合到同一样本来获得。</target>
        </trans-unit>
        <trans-unit id="ad47de32445041d6ef99f77621d867a03cd9751f" translate="yes" xml:space="preserve">
          <source>The OAS estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt;&lt;code&gt;oas&lt;/code&gt;&lt;/a&gt; function of the &lt;code&gt;sklearn.covariance&lt;/code&gt; package, or it can be otherwise obtained by fitting an &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;OAS&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">协方差矩阵的OAS估计量可以使用 &lt;code&gt;sklearn.covariance&lt;/code&gt; 包的&lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt; &lt;code&gt;oas&lt;/code&gt; &lt;/a&gt;函数对样本进行计算，也可以通过将&lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt; &lt;code&gt;OAS&lt;/code&gt; &lt;/a&gt;对象拟合到同一样本来获得。</target>
        </trans-unit>
        <trans-unit id="6f45a4e82bc4c1b489f40318ecca4028acd0f46e" translate="yes" xml:space="preserve">
          <source>The One-Class SVM has been introduced by Sch&amp;ouml;lkopf et al. for that purpose and implemented in the &lt;a href=&quot;svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; module in the &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; object. It requires the choice of a kernel and a scalar parameter to define a frontier. The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter. This is the default in the scikit-learn implementation. The \(\nu\) parameter, also known as the margin of the One-Class SVM, corresponds to the probability of finding a new, but regular, observation outside the frontier.</source>
          <target state="translated">一类SVM已由Sch&amp;ouml;lkopf等人介绍。为此目的，并在&lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;svm.OneClassSVM&lt;/code&gt; &lt;/a&gt;对象的&lt;a href=&quot;svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt;模块中实现。它需要选择内核和标量参数来定义边界。尽管没有确切的公式或算法来设置其带宽参数，但通常会选择RBF内核。这是scikit-learn实现中的默认设置。 \（\ nu \）参数，也称为一类SVM的边距，对应于在边界之外找到新的但有规律的观测值的概率。</target>
        </trans-unit>
        <trans-unit id="ce00ae4f245475e7026810e89464783863843edd" translate="yes" xml:space="preserve">
          <source>The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction.</source>
          <target state="translated">PCA做的是无监督的维度降低,而逻辑回归做的是预测。</target>
        </trans-unit>
        <trans-unit id="04f0f7d7b53f41cd954be6291735d195a21f52b7" translate="yes" xml:space="preserve">
          <source>The Poisson deviance cannot be computed on non-positive values predicted by the model. For models that do return a few non-positive predictions (e.g. &lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;) we ignore the corresponding samples, meaning that the obtained Poisson deviance is approximate. An alternative approach could be to use &lt;a href=&quot;../../modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor&quot;&gt;&lt;code&gt;TransformedTargetRegressor&lt;/code&gt;&lt;/a&gt; meta-estimator to map &lt;code&gt;y_pred&lt;/code&gt; to a strictly positive domain.</source>
          <target state="translated">泊松偏差不能在模型预测的非正值上计算。对于确实返回一些非阳性预测的模型（例如&lt;a href=&quot;../../modules/generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt;），我们将忽略相应的样本，这意味着获得的泊松偏差是近似的。一种替代方法是使用&lt;a href=&quot;../../modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor&quot;&gt; &lt;code&gt;TransformedTargetRegressor&lt;/code&gt; &lt;/a&gt;元估计器将 &lt;code&gt;y_pred&lt;/code&gt; 映射到严格的正域。</target>
        </trans-unit>
        <trans-unit id="bc5a34aa26f7428f3e35a346a70f3689822fe771" translate="yes" xml:space="preserve">
          <source>The Poisson deviance computed as an evaluation metric reflects both the calibration and the ranking power of the model. It also makes a linear assumption on the ideal relationship between the expected value and the variance of the response variable. For the sake of conciseness we did not check whether this assumption holds.</source>
          <target state="translated">作为评价指标计算的泊松偏差既反映了模型的标定力,也反映了模型的排名力。它还对响应变量的期望值和方差之间的理想关系做了一个线性假设。为了简洁起见,我们没有检查这个假设是否成立。</target>
        </trans-unit>
        <trans-unit id="450889f232106f285e0f6ca8296ef879dd035563" translate="yes" xml:space="preserve">
          <source>The Probability Density Functions (PDF) of these distributions are illustrated in the following figure,</source>
          <target state="translated">这些分布的概率密度函数(PDF)如下图所示。</target>
        </trans-unit>
        <trans-unit id="935532caca213849eac9af7588fc52963397d517" translate="yes" xml:space="preserve">
          <source>The R2 score used when calling &lt;code&gt;score&lt;/code&gt; on a regressor uses &lt;code&gt;multioutput='uniform_average'&lt;/code&gt; from version 0.23 to keep consistent with default value of &lt;a href=&quot;sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt;. This influences the &lt;code&gt;score&lt;/code&gt; method of all the multioutput regressors (except for &lt;a href=&quot;#sklearn.multioutput.MultiOutputRegressor&quot;&gt;&lt;code&gt;MultiOutputRegressor&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">在回归器上调用 &lt;code&gt;score&lt;/code&gt; 时使用的R2得分使用版本0.23中的 &lt;code&gt;multioutput='uniform_average'&lt;/code&gt; 来保持与&lt;a href=&quot;sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; 的&lt;/a&gt;默认值一致。这影响了 &lt;code&gt;score&lt;/code&gt; 的所有多输出回归量的方法（除了&lt;a href=&quot;#sklearn.multioutput.MultiOutputRegressor&quot;&gt; &lt;code&gt;MultiOutputRegressor&lt;/code&gt; &lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="11e4fa9675506dfc6e48ea958dcc499e9784ad90" translate="yes" xml:space="preserve">
          <source>The R2 score used when calling &lt;code&gt;score&lt;/code&gt; on a regressor uses &lt;code&gt;multioutput='uniform_average'&lt;/code&gt; from version 0.23 to keep consistent with default value of &lt;a href=&quot;sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt;. This influences the &lt;code&gt;score&lt;/code&gt; method of all the multioutput regressors (except for &lt;a href=&quot;sklearn.multioutput.multioutputregressor#sklearn.multioutput.MultiOutputRegressor&quot;&gt;&lt;code&gt;MultiOutputRegressor&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">在回归器上调用 &lt;code&gt;score&lt;/code&gt; 时使用的R2得分使用版本0.23中的 &lt;code&gt;multioutput='uniform_average'&lt;/code&gt; 来保持与&lt;a href=&quot;sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; 的&lt;/a&gt;默认值一致。这影响了 &lt;code&gt;score&lt;/code&gt; 的所有多输出回归量的方法（除了&lt;a href=&quot;sklearn.multioutput.multioutputregressor#sklearn.multioutput.MultiOutputRegressor&quot;&gt; &lt;code&gt;MultiOutputRegressor&lt;/code&gt; &lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="a83459e6ef7baec271df4e0325c8da4e8711242a" translate="yes" xml:space="preserve">
          <source>The RBF kernel is a stationary kernel. It is also known as the &amp;ldquo;squared exponential&amp;rdquo; kernel. It is parameterized by a length scale parameter \(l&amp;gt;0\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">RBF内核是固定内核。它也被称为&amp;ldquo;平方指数&amp;rdquo;内核。它由长度比例参数\（l&amp;gt; 0 \）参数化，该参数可以是标量（内核的各向同性变体），也可以是维数与输入X相同的向量（内核的各向异性变体）。内核由下式给出：</target>
        </trans-unit>
        <trans-unit id="33c279bdcb922f55f225b113a682e1a5cf28cc6d" translate="yes" xml:space="preserve">
          <source>The RBF kernel is a stationary kernel. It is also known as the &amp;ldquo;squared exponential&amp;rdquo; kernel. It is parameterized by a length-scale parameter length_scale&amp;gt;0, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">RBF内核是固定内核。它也被称为&amp;ldquo;平方指数&amp;rdquo;内核。它由长度比例参数length_scale&amp;gt; 0进行参数化，该参数可以是标量（内核的各向同性变体），也可以是维数与输入X相同的向量（内核的各向异性变体）。内核由：</target>
        </trans-unit>
        <trans-unit id="f47997c4010f2e20accd04690970d6547a71bfc2" translate="yes" xml:space="preserve">
          <source>The RBF kernel will produce a fully connected graph which is represented in memory by a dense matrix. This matrix may be very large and combined with the cost of performing a full matrix multiplication calculation for each iteration of the algorithm can lead to prohibitively long running times. On the other hand, the KNN kernel will produce a much more memory-friendly sparse matrix which can drastically reduce running times.</source>
          <target state="translated">RBF内核将产生一个完全连接的图形,这个图形在内存中用一个密集的矩阵来表示,这个矩阵可能非常大,再加上每次迭代时执行全矩阵乘法计算的成本,会导致运行时间过长。这个矩阵可能会非常大,再加上每次迭代算法时进行全矩阵乘法计算的成本,会导致运行时间过长。另一方面,KNN内核将产生一个对内存更友好的稀疏矩阵,这可以大大减少运行时间。</target>
        </trans-unit>
        <trans-unit id="512de356729b7d551b43d5d76bd8681dea941e19" translate="yes" xml:space="preserve">
          <source>The RBM tries to maximize the likelihood of the data using a particular graphical model. The parameter learning algorithm used (&lt;a href=&quot;#sml&quot;&gt;Stochastic Maximum Likelihood&lt;/a&gt;) prevents the representations from straying far from the input data, which makes them capture interesting regularities, but makes the model less useful for small datasets, and usually not useful for density estimation.</source>
          <target state="translated">RBM尝试使用特定的图形模型来最大化数据的可能性。使用的参数学习算法（&lt;a href=&quot;#sml&quot;&gt;随机最大似然&lt;/a&gt;）可防止表示偏离输入数据，从而使它们捕获有趣的规律性，但使模型对小型数据集的用处不大，通常对密度估计不起作用。</target>
        </trans-unit>
        <trans-unit id="95d48012cfd5cdfda48a80ad45de8292db32665c" translate="yes" xml:space="preserve">
          <source>The R^2 score or ndarray of scores if &amp;lsquo;multioutput&amp;rsquo; is &amp;lsquo;raw_values&amp;rsquo;.</source>
          <target state="translated">如果'multioutput'为'raw_values'，则为R ^ 2分数或分数的ndarray。</target>
        </trans-unit>
        <trans-unit id="57cf85e67b74d4b36ce4e00a60cc2afb37409d42" translate="yes" xml:space="preserve">
          <source>The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.</source>
          <target state="translated">兰德指数通过考虑所有的样本对,计算两个聚类之间的相似度测量,并计算在预测聚类和真实聚类中分配在相同或不同聚类中的对。</target>
        </trans-unit>
        <trans-unit id="21d3f8892ca41bf337636e90759152be22d9788c" translate="yes" xml:space="preserve">
          <source>The RandomTreesEmbedding, from the &lt;a href=&quot;../../modules/classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module, is not technically a manifold embedding method, as it learn a high-dimensional representation on which we apply a dimensionality reduction method. However, it is often useful to cast a dataset into a representation in which the classes are linearly-separable.</source>
          <target state="translated">从&lt;a href=&quot;../../modules/classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt;模块中获取的RandomTreesEmbedding从技术上讲不是多种多样的嵌入方法，因为它学习了我们应用降维方法的高维表示形式。但是，将数据集转换为类可线性分离的表示形式通常很有用。</target>
        </trans-unit>
        <trans-unit id="9863f2af9163cbec042d3a8db1d9d0da7a79bddf" translate="yes" xml:space="preserve">
          <source>The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length scales. It is parameterized by a length scale parameter \(l&amp;gt;0\) and a scale mixture parameter \(\alpha&amp;gt;0\). Only the isotropic variant where length_scale \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="translated">可以将RationalQuadratic内核视为具有不同特征长度尺度的RBF内核的尺度混合（无穷大）。它由长度比例参数\（l&amp;gt; 0 \）和比例混合参数\（\ alpha&amp;gt; 0 \）参数化。目前，仅支持length_scale \（l \）为标量的各向同性变体。内核由下式给出：</target>
        </trans-unit>
        <trans-unit id="ddaea903844387569c2d558e504bb67163af0e53" translate="yes" xml:space="preserve">
          <source>The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length-scales. It is parameterized by a length-scale parameter length_scale&amp;gt;0 and a scale mixture parameter alpha&amp;gt;0. Only the isotropic variant where length_scale is a scalar is supported at the moment. The kernel given by:</source>
          <target state="translated">可以将RationalQuadratic内核视为具有不同特征长度尺度的RBF内核的尺度混合（无穷大）。它由长度比例参数length_scale&amp;gt; 0和比例混合参数alpha&amp;gt; 0参数化。目前，仅支持length_scale为标量的各向同性变体。内核由：</target>
        </trans-unit>
        <trans-unit id="4290d451a4bc3d8974b88b0877f706f5c5e0d4c1" translate="yes" xml:space="preserve">
          <source>The SAGA solver supports both float64 and float32 bit arrays.</source>
          <target state="translated">SAGA解算器支持float64和float32位数组。</target>
        </trans-unit>
        <trans-unit id="66cebb865fc92b18216814d7cca47981b6cb6b41" translate="yes" xml:space="preserve">
          <source>The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a multidimensional scaling algorithm which minimizes an objective function (the &lt;em&gt;stress&lt;/em&gt;) using a majorization technique. Stress majorization, also known as the Guttman Transform, guarantees a monotone convergence of stress, and is more powerful than traditional techniques such as gradient descent.</source>
          <target state="translated">SMACOF（通过简化包含多项式的函数进行缩放）算法是一种多维缩放算法，它使用最大化技术来最小化目标函数（&lt;em&gt;应力&lt;/em&gt;）。应力集中化也称为Guttman变换，可确保应力的单调收敛，并且比诸如梯度下降之类的传统技术更强大。</target>
        </trans-unit>
        <trans-unit id="e2c4971923aff3f7fe2f8678fb678a8a1fe12716" translate="yes" xml:space="preserve">
          <source>The SMACOF algorithm for metric MDS can summarized by the following steps:</source>
          <target state="translated">度量MDS的SMACOF算法可以概括为以下几个步骤。</target>
        </trans-unit>
        <trans-unit id="9991accadfafce42fd0097ac5bf1452a92166503" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient &lt;em&gt;s&lt;/em&gt; for a single sample is then given as:</source>
          <target state="translated">轮廓系数&lt;em&gt;&amp;scaron;&lt;/em&gt;单个样品然后被给定为：</target>
        </trans-unit>
        <trans-unit id="977018d27e576be441891586d29895a67820e9bb" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample.</source>
          <target state="translated">一组样本的轮廓系数是以每个样本的轮廓系数的平均值给出的。</target>
        </trans-unit>
        <trans-unit id="a8aa62253fcb64807b1a0f2a014811a76f4c09cc" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is a measure of how well samples are clustered with samples that are similar to themselves. Clustering models with a high Silhouette Coefficient are said to be dense, where samples in the same cluster are similar to each other, and well separated, where samples in different clusters are not very similar to each other.</source>
          <target state="translated">轮廓系数是衡量样本与自身相似的样本聚类程度的指标。具有高轮廓系数的聚类模型被认为是致密的,即同一聚类中的样本彼此相似,而分离度高,即不同聚类中的样本彼此不是很相似。</target>
        </trans-unit>
        <trans-unit id="5da8604230381cdf66d3aec27ea08ace466e606c" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is calculated using the mean intra-cluster distance (&lt;code&gt;a&lt;/code&gt;) and the mean nearest-cluster distance (&lt;code&gt;b&lt;/code&gt;) for each sample. The Silhouette Coefficient for a sample is &lt;code&gt;(b - a) / max(a,
b)&lt;/code&gt;. Note that Silhouette Coefficient is only defined if number of labels is 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</source>
          <target state="translated">使用每个样本的平均集群内距离（ &lt;code&gt;a&lt;/code&gt; ）和平均最近集群距离（ &lt;code&gt;b&lt;/code&gt; ）计算出Silhouette系数。样本的轮廓系数为 &lt;code&gt;(b - a) / max(a, b)&lt;/code&gt; 。请注意，仅当标签数为2 &amp;lt;= n_labels &amp;lt;= n_samples-1时，才定义轮廓系数。</target>
        </trans-unit>
        <trans-unit id="4273a368cac49099b8caa7c819b214a9d85c0ff4" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is calculated using the mean intra-cluster distance (&lt;code&gt;a&lt;/code&gt;) and the mean nearest-cluster distance (&lt;code&gt;b&lt;/code&gt;) for each sample. The Silhouette Coefficient for a sample is &lt;code&gt;(b - a) / max(a,
b)&lt;/code&gt;. To clarify, &lt;code&gt;b&lt;/code&gt; is the distance between a sample and the nearest cluster that the sample is not a part of. Note that Silhouette Coefficient is only defined if number of labels is 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</source>
          <target state="translated">使用每个样本的平均集群内距离（ &lt;code&gt;a&lt;/code&gt; ）和平均最近集群距离（ &lt;code&gt;b&lt;/code&gt; ）计算出Silhouette系数。样本的轮廓系数为 &lt;code&gt;(b - a) / max(a, b)&lt;/code&gt; 。为了明确起见， &lt;code&gt;b&lt;/code&gt; 是样本与该样本不属于的最近群集之间的距离。请注意，仅当标签数为2 &amp;lt;= n_labels &amp;lt;= n_samples-1时，才定义轮廓系数。</target>
        </trans-unit>
        <trans-unit id="74a577cf3f9facf0caa36a3cd46ce91a25197ef1" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">凸簇的轮廓系数一般比其他概念的簇要高,比如像通过DBSCAN得到的密度型簇。</target>
        </trans-unit>
        <trans-unit id="b08373a116f8a28b8f59b4a939f8e86bc18a285c" translate="yes" xml:space="preserve">
          <source>The Spearman correlation coefficient is estimated from the data, and the sign of the resulting estimate is used as the result.</source>
          <target state="translated">从数据中估计出Spearman相关系数,并将所得估计值的符号作为结果。</target>
        </trans-unit>
        <trans-unit id="1aca461bec942a3ea49c28ca350e80c03610ed92" translate="yes" xml:space="preserve">
          <source>The Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages:</source>
          <target state="translated">光谱嵌入(拉普拉斯特征图)算法包括三个阶段。</target>
        </trans-unit>
        <trans-unit id="5c2cf1989e094ea2eafd60dd4bfcc701bf5b4819" translate="yes" xml:space="preserve">
          <source>The Stack Exchange family of sites hosts &lt;a href=&quot;https://meta.stackexchange.com/q/130524&quot;&gt;multiple subdomains for Machine Learning questions&lt;/a&gt;.</source>
          <target state="translated">Stack Exchange系列站点托管&lt;a href=&quot;https://meta.stackexchange.com/q/130524&quot;&gt;用于机器学习问题的多个子域&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="530048073ab2f0fd4beae1a41150629fce3bbc04" translate="yes" xml:space="preserve">
          <source>The TF-IDF vectorized posts form a word frequency matrix, which is then biclustered using Dhillon&amp;rsquo;s Spectral Co-Clustering algorithm. The resulting document-word biclusters indicate subsets words used more often in those subsets documents.</source>
          <target state="translated">TF-IDF矢量化后的帖子形成一个词频矩阵，然后使用Dhillon的&amp;ldquo;频谱共聚&amp;rdquo;算法对其进行二聚。产生的文档词双词条表示在那些子集文档中使用频率更高的子集词。</target>
        </trans-unit>
        <trans-unit id="a51b4bd7337a8a43bf76bb00c70355d636e98cf2" translate="yes" xml:space="preserve">
          <source>The V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean &lt;a href=&quot;#b2011&quot; id=&quot;id15&quot;&gt;[B2011]&lt;/a&gt;.</source>
          <target state="translated">V度量实际上等效于上面讨论的互信息（NMI），聚合函数是算术平均值&lt;a href=&quot;#b2011&quot; id=&quot;id15&quot;&gt;[B2011]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4af22841a0f64abdacec5694995ece03a0f97488" translate="yes" xml:space="preserve">
          <source>The V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean &lt;a href=&quot;#b2011&quot; id=&quot;id16&quot;&gt;[B2011]&lt;/a&gt;.</source>
          <target state="translated">V度量实际上等效于上面讨论的互信息（NMI），聚合函数是算术平均值&lt;a href=&quot;#b2011&quot; id=&quot;id16&quot;&gt;[B2011]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="28e22a992327910c0281c7997fd0381a055b2da5" translate="yes" xml:space="preserve">
          <source>The V-measure is the harmonic mean between homogeneity and completeness:</source>
          <target state="translated">V量是同质性和完整性之间的谐波平均值。</target>
        </trans-unit>
        <trans-unit id="8e9e469c4bec48fbecca2280bcc57b2302181e26" translate="yes" xml:space="preserve">
          <source>The WAGE is increasing when EDUCATION is increasing. Note that the dependence between WAGE and EDUCATION represented here is a marginal dependence, i.e., it describes the behavior of a specific variable without keeping the others fixed.</source>
          <target state="translated">当教育程度增加时,工资是增加的。请注意,这里所表示的工资和教育之间的依赖关系是一种边际依赖关系,即它描述的是一个特定变量的行为,而不是保持其他变量固定不变。</target>
        </trans-unit>
        <trans-unit id="9d831d6fbe54cab7c78889ea027eda02af846327" translate="yes" xml:space="preserve">
          <source>The Yeo-Johnson transform is given by:</source>
          <target state="translated">Yeo-Johnson变换由:</target>
        </trans-unit>
        <trans-unit id="dd310b3ef8cade9b7b44463cf24d9960d1a07902" translate="yes" xml:space="preserve">
          <source>The \(\ell = \lceil \log_2 k \rceil\) singular vectors, starting from the second, provide the desired partitioning information. They are used to form the matrix \(Z\):</source>
          <target state="translated">从第二个开始,奇异向量提供了所需的分区信息。它们被用来形成矩阵(Z)。</target>
        </trans-unit>
        <trans-unit id="0151312e57232e026343f2a42d71b34eecec6ec6" translate="yes" xml:space="preserve">
          <source>The \(\nu\)-SVC formulation &lt;a href=&quot;#id17&quot; id=&quot;id8&quot;&gt;15&lt;/a&gt; is a reparameterization of the \(C\)-SVC and therefore mathematically equivalent.</source>
          <target state="translated">\（\ nu \）-SVC公式&lt;a href=&quot;#id17&quot; id=&quot;id8&quot;&gt;15&lt;/a&gt;是\（C \）-SVC的重新参数化，因此在数学上是等效的。</target>
        </trans-unit>
        <trans-unit id="6bae7eee8e57958cb7a015c09e85982ab959fe35" translate="yes" xml:space="preserve">
          <source>The \(k\)-neighbors classification in &lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt; is the most commonly used technique. The optimal choice of the value \(k\) is highly data-dependent: in general a larger \(k\) suppresses the effects of noise, but makes the classification boundaries less distinct.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt; &lt;code&gt;KNeighborsClassifier&lt;/code&gt; 中&lt;/a&gt;的\（k \）-邻居分类是最常用的技术。值\（k \）的最佳选择高度依赖于数据：通常，较大的\（k \）会抑制噪声的影响，但会使分类边界不那么明显。</target>
        </trans-unit>
        <trans-unit id="e8862ed89c87547157e194d96a8f490026b2e0fb" translate="yes" xml:space="preserve">
          <source>The ability to reproduce the data of the regularized model is similar to the one of the non-regularized model.</source>
          <target state="translated">正则化模型的数据重现能力与非正则化模型相似。</target>
        </trans-unit>
        <trans-unit id="7ea48a47287b56e3f66c9ec81d51c291e8fedf59" translate="yes" xml:space="preserve">
          <source>The above vectorization scheme is simple but the fact that it holds an &lt;strong&gt;in- memory mapping from the string tokens to the integer feature indices&lt;/strong&gt; (the &lt;code&gt;vocabulary_&lt;/code&gt; attribute) causes several &lt;strong&gt;problems when dealing with large datasets&lt;/strong&gt;:</source>
          <target state="translated">上面的矢量化方案很简单，但是它拥有&lt;strong&gt;从字符串标记到整数特征索引&lt;/strong&gt;（ &lt;code&gt;vocabulary_&lt;/code&gt; 属性）的&lt;strong&gt;内存映射，&lt;/strong&gt;这一事实&lt;strong&gt;在处理大型数据集时&lt;/strong&gt;会引起一些&lt;strong&gt;问题&lt;/strong&gt;：</target>
        </trans-unit>
        <trans-unit id="f78fe42a387595f69f94294d9712aa8fff9cf7ee" translate="yes" xml:space="preserve">
          <source>The absolute tolerance to use when comparing scores during early stopping. The higher the tolerance, the more likely we are to early stop: higher tolerance means that it will be harder for subsequent iterations to be considered an improvement upon the reference score.</source>
          <target state="translated">在提前停止期间比较分数时使用的绝对容差。容忍度越高,我们就越有可能提前停止:容忍度越高,意味着后续的迭代将更难被认为是对参考分数的改进。</target>
        </trans-unit>
        <trans-unit id="a971e24181d9be44e4bb51f3963013ec9c9d01d5" translate="yes" xml:space="preserve">
          <source>The absolute tolerance to use when comparing scores. The higher the tolerance, the more likely we are to early stop: higher tolerance means that it will be harder for subsequent iterations to be considered an improvement upon the reference score.</source>
          <target state="translated">比较分数时使用的绝对容差。容忍度越高,我们就越有可能提前停止:容忍度越高,意味着后续的迭代将更难被认为是对参考分数的改进。</target>
        </trans-unit>
        <trans-unit id="58279b870becff96bab0fc34e17d7f045af03b34" translate="yes" xml:space="preserve">
          <source>The abstract base class for all kernels is &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt;. Kernel implements a similar interface as &lt;code&gt;Estimator&lt;/code&gt;, providing the methods &lt;code&gt;get_params()&lt;/code&gt;, &lt;code&gt;set_params()&lt;/code&gt;, and &lt;code&gt;clone()&lt;/code&gt;. This allows setting kernel values also via meta-estimators such as &lt;code&gt;Pipeline&lt;/code&gt; or &lt;code&gt;GridSearch&lt;/code&gt;. Note that due to the nested structure of kernels (by applying kernel operators, see below), the names of kernel parameters might become relatively complicated. In general, for a binary kernel operator, parameters of the left operand are prefixed with &lt;code&gt;k1__&lt;/code&gt; and parameters of the right operand with &lt;code&gt;k2__&lt;/code&gt;. An additional convenience method is &lt;code&gt;clone_with_theta(theta)&lt;/code&gt;, which returns a cloned version of the kernel but with the hyperparameters set to &lt;code&gt;theta&lt;/code&gt;. An illustrative example:</source>
          <target state="translated">所有内核的抽象基类是&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt;。内核实现了与 &lt;code&gt;Estimator&lt;/code&gt; 类似的接口，提供了 &lt;code&gt;get_params()&lt;/code&gt; ， &lt;code&gt;set_params()&lt;/code&gt; 和 &lt;code&gt;clone()&lt;/code&gt; 方法。这还允许通过元估计器（例如 &lt;code&gt;Pipeline&lt;/code&gt; 或 &lt;code&gt;GridSearch&lt;/code&gt; )设置内核值。注意，由于内核的嵌套结构（通过应用内核运算符，请参见下文），内核参数的名称可能会变得相对复杂。一般来说，对于一个二进制内核运算符，左操作数的参数都带有前缀 &lt;code&gt;k1__&lt;/code&gt; 和正确操作的参数 &lt;code&gt;k2__&lt;/code&gt; 。另一个方便的方法是 &lt;code&gt;clone_with_theta(theta)&lt;/code&gt; ，它返回内核的克隆版本，但是超参数设置为 &lt;code&gt;theta&lt;/code&gt; 。一个示例：</target>
        </trans-unit>
        <trans-unit id="d6dfb2a865fc7561c9990f60831354c8c2b2acf1" translate="yes" xml:space="preserve">
          <source>The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: &amp;ldquo;Robust Linear Programming Discrimination of Two Linearly Inseparable Sets&amp;rdquo;, Optimization Methods and Software 1, 1992, 23-34].</source>
          <target state="translated">用于获得3维空间中分离平面的实际线性程序的描述如下：[KP Bennett和OL Mangasarian：&amp;ldquo;两个线性不可分集合的鲁棒线性程序判别&amp;rdquo;，优化方法和软件1，1992，23- 34]。</target>
        </trans-unit>
        <trans-unit id="5c16c787f9e7f87c21ce4f86533b13082a6022ce" translate="yes" xml:space="preserve">
          <source>The actual number of iteration performed by the solver. Only returned if &lt;code&gt;return_n_iter&lt;/code&gt; is True.</source>
          <target state="translated">求解器执行的实际迭代次数。仅在 &lt;code&gt;return_n_iter&lt;/code&gt; 为True时返回。</target>
        </trans-unit>
        <trans-unit id="9cc109786e99b4c544e4621619329291c5062f5c" translate="yes" xml:space="preserve">
          <source>The actual number of iterations before reaching the stopping criterion.</source>
          <target state="translated">达到停止标准前的实际迭代次数。</target>
        </trans-unit>
        <trans-unit id="68fff8e45d31e6c0edd22bde7c766ffe083896ee" translate="yes" xml:space="preserve">
          <source>The actual number of iterations before reaching the stopping criterion. For multiclass fits, it is the maximum over every binary fit.</source>
          <target state="translated">在达到停止标准之前的实际迭代次数。對于多類別的配合,它是每個二元配合的最大值。</target>
        </trans-unit>
        <trans-unit id="c6a597b70b1c3dba7fc91a33b2b458a1718ae376" translate="yes" xml:space="preserve">
          <source>The actual number of iterations to reach the stopping criterion.</source>
          <target state="translated">达到停止标准的实际迭代次数。</target>
        </trans-unit>
        <trans-unit id="be3e6ed927427ff958a3dc691ec6f2ca38b153e8" translate="yes" xml:space="preserve">
          <source>The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over every binary fit.</source>
          <target state="translated">达到停止标准的实际迭代次数。對于多類別的配合,它是每個二元配合的最大值。</target>
        </trans-unit>
        <trans-unit id="905122fa4ae495ec172d69201d288d9cbd8eb107" translate="yes" xml:space="preserve">
          <source>The actual number of neighbors used for &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt;&lt;code&gt;kneighbors&lt;/code&gt;&lt;/a&gt; queries.</source>
          <target state="translated">用于&lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt; &lt;code&gt;kneighbors&lt;/code&gt; &lt;/a&gt;查询的邻居的实际数量。</target>
        </trans-unit>
        <trans-unit id="d1e2217f69f6ffa7102e10d2dcd6eed0b5a03524" translate="yes" xml:space="preserve">
          <source>The actual number of quantiles used to discretize the cumulative distribution function.</source>
          <target state="translated">用于分解累积分布函数的实际量子数。</target>
        </trans-unit>
        <trans-unit id="99f948dd8ff8ebdf8ccc1268f27f992c4d54a1e0" translate="yes" xml:space="preserve">
          <source>The actual number of samples</source>
          <target state="translated">实际样本数</target>
        </trans-unit>
        <trans-unit id="83f4574f37ccf239bd98ad8929c323f413ece25a" translate="yes" xml:space="preserve">
          <source>The actual number of samples.</source>
          <target state="translated">样品的实际数量。</target>
        </trans-unit>
        <trans-unit id="257683b273bf01e4ad40af7f18f19c426c837478" translate="yes" xml:space="preserve">
          <source>The additive chi squared kernel as used here is given by</source>
          <target state="translated">这里使用的加法齐次方核由下列公式给出</target>
        </trans-unit>
        <trans-unit id="721e722946fa16cdc1d4e80a122a59df383d3b35" translate="yes" xml:space="preserve">
          <source>The additive chi squared kernel is a kernel on histograms, often used in computer vision.</source>
          <target state="translated">加性齐次方核是一种关于直方图的核,常用于计算机视觉中。</target>
        </trans-unit>
        <trans-unit id="509930cd9616089e31b59f87837fde9a2850682a" translate="yes" xml:space="preserve">
          <source>The additive version of this kernel</source>
          <target state="translated">这个内核的加法版本</target>
        </trans-unit>
        <trans-unit id="479259eedb54024948ef963b0d114b00fbb05f1b" translate="yes" xml:space="preserve">
          <source>The adjacency matrix is used to compute a normalized graph Laplacian whose spectrum (especially the eigenvectors associated to the smallest eigenvalues) has an interpretation in terms of minimal number of cuts necessary to split the graph into comparably sized components.</source>
          <target state="translated">邻接矩阵用于计算一个归一化的图形Laplacian,其频谱(特别是与最小特征值相关的特征向量)以最小的切割次数来解释,以将图形分割成大小相当的成分。</target>
        </trans-unit>
        <trans-unit id="b4f6e52ff52334d95b6f16934aecd26800758ca5" translate="yes" xml:space="preserve">
          <source>The adjacency matrix of the graph to embed.</source>
          <target state="translated">要嵌入的图形的邻接矩阵。</target>
        </trans-unit>
        <trans-unit id="f8c06d6c9bebb104301c962acd25687eac4074b4" translate="yes" xml:space="preserve">
          <source>The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation).</source>
          <target state="translated">因此,调整后的Rand指数可以保证随机标签的值接近0.0,与聚类和样本数量无关,而当聚类相同时(最多换算),Rand指数的值正好为1.0。</target>
        </trans-unit>
        <trans-unit id="83e2303d70450b875b04241e83f4a4153932e0a9" translate="yes" xml:space="preserve">
          <source>The advantage of using approximate explicit feature maps compared to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_trick&quot;&gt;kernel trick&lt;/a&gt;, which makes use of feature maps implicitly, is that explicit mappings can be better suited for online learning and can significantly reduce the cost of learning with very large datasets. Standard kernelized SVMs do not scale well to large datasets, but using an approximate kernel map it is possible to use much more efficient linear SVMs. In particular, the combination of kernel map approximations with &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; can make non-linear learning on large datasets possible.</source>
          <target state="translated">与隐式使用特征图的&lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_trick&quot;&gt;内核技巧&lt;/a&gt;相比，使用近似显式特征图的优势在于显式映射可以更好地适合在线学习，并且可以显着降低使用超大型数据集的学习成本。标准的内核化SVM不能很好地扩展到大型数据集，但是使用近似的内核图，可以使用效率更高的线性SVM。特别是，将内核映射近似与&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;结合使用可以在大型数据集上进行非线性学习。</target>
        </trans-unit>
        <trans-unit id="e8cf8d3d0c8dfd7b6a3c8351c56803f2fdf0d2d8" translate="yes" xml:space="preserve">
          <source>The advantages of Bayesian Regression are:</source>
          <target state="translated">贝叶斯回归法的优点是:。</target>
        </trans-unit>
        <trans-unit id="d7269fefe21b18a1d9b1e8a11da2f35edf3e36ec" translate="yes" xml:space="preserve">
          <source>The advantages of GBRT are:</source>
          <target state="translated">GBRT的优点是:</target>
        </trans-unit>
        <trans-unit id="6bdcff89c23af609c3f964058bbddc38e45558f8" translate="yes" xml:space="preserve">
          <source>The advantages of Gaussian processes are:</source>
          <target state="translated">高斯过程的优点是:</target>
        </trans-unit>
        <trans-unit id="c632bf8abb99b63a04554183dcc32e66bb0cf004" translate="yes" xml:space="preserve">
          <source>The advantages of LARS are:</source>
          <target state="translated">LARS的优点是:</target>
        </trans-unit>
        <trans-unit id="54923c815d40d3138e9fbe2ad92c97dd60ef1b2e" translate="yes" xml:space="preserve">
          <source>The advantages of Multi-layer Perceptron are:</source>
          <target state="translated">多层感知器的优势在于:1.</target>
        </trans-unit>
        <trans-unit id="1dabd14761114fadc83f1114e989744a341117db" translate="yes" xml:space="preserve">
          <source>The advantages of Stochastic Gradient Descent are:</source>
          <target state="translated">随机梯度下降法的优点是。</target>
        </trans-unit>
        <trans-unit id="a12cea148a9927c20c87185a7ad428a4bd953760" translate="yes" xml:space="preserve">
          <source>The advantages of support vector machines are:</source>
          <target state="translated">支持向量机的优点是:</target>
        </trans-unit>
        <trans-unit id="b3ce019dea8f1fa7178760b3ed7e1bed715f9894" translate="yes" xml:space="preserve">
          <source>The affinity matrix describing the relationship of the samples to embed. &lt;strong&gt;Must be symmetric&lt;/strong&gt;.</source>
          <target state="translated">亲和度矩阵描述要嵌入的样本之间的关系。&lt;strong&gt;必须对称&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="f8f51936a34abb4d8304c65310991fbeb781a2b1" translate="yes" xml:space="preserve">
          <source>The algorithm automatically sets the number of clusters, instead of relying on a parameter &lt;code&gt;bandwidth&lt;/code&gt;, which dictates the size of the region to search through. This parameter can be set manually, but can be estimated using the provided &lt;code&gt;estimate_bandwidth&lt;/code&gt; function, which is called if the bandwidth is not set.</source>
          <target state="translated">该算法自动地设定的，而不是依赖于参数的簇的数量， &lt;code&gt;bandwidth&lt;/code&gt; ，该区域的大小，以搜寻通过该使然。该参数可以手动设置，但是可以使用提供的 &lt;code&gt;estimate_bandwidth&lt;/code&gt; 函数进行估计，如果未设置带宽，则调用该函数。</target>
        </trans-unit>
        <trans-unit id="999ae100a463cc0ff60234ca1a1687522770fa40" translate="yes" xml:space="preserve">
          <source>The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and efficiency. Since the number of least square solutions is &amp;ldquo;n_samples choose n_subsamples&amp;rdquo;, it can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets are chosen randomly. In a final step, the spatial median (or L1 median) is calculated of all least square solutions.</source>
          <target state="translated">该算法以X为样本大小n_subsample的子集计算最小二乘解。在特征和样本数量之间的n_subsample的任何值都会导致估计器的健壮性和效率之间折衷。由于最小二乘解的数量为&amp;ldquo; n_samples选择n_subsamples&amp;rdquo;，因此其数量可能非常大，因此可能受到max_subpopulation的限制。如果达到此限制，则随机选择子集。在最后一步中，将计算所有最小二乘解的空间中位数（或L1中位数）。</target>
        </trans-unit>
        <trans-unit id="19207c781cd295df6565aab588711bf1a9323bef" translate="yes" xml:space="preserve">
          <source>The algorithm can also be understood through the concept of &lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;Voronoi diagrams&lt;/a&gt;. First the Voronoi diagram of the points is calculated using the current centroids. Each segment in the Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated to the mean of each segment. The algorithm then repeats this until a stopping criterion is fulfilled. Usually, the algorithm stops when the relative decrease in the objective function between iterations is less than the given tolerance value. This is not the case in this implementation: iteration stops when centroids move less than the tolerance.</source>
          <target state="translated">该算法也可以通过&lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;Voronoi图&lt;/a&gt;的概念来理解。首先，使用当前质心计算点的Voronoi图。Voronoi图中的每个段都成为一个单独的群集。其次，质心更新为每个段的均值。然后，算法重复此操作，直到满足停止条件为止。通常，当迭代之间目标函数的相对减小小于给定的容差值时，算法将停止。在此实现中不是这种情况：当质心的运动小于公差时，迭代将停止。</target>
        </trans-unit>
        <trans-unit id="ac387733191797f4111e67a021bb9641f7899ac2" translate="yes" xml:space="preserve">
          <source>The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R &lt;code&gt;glasso&lt;/code&gt; package.</source>
          <target state="translated">用于解决此问题的算法是GLasso算法，来自Friedman 2008 Biostatistics论文。它与R &lt;code&gt;glasso&lt;/code&gt; 软件包中的算法相同。</target>
        </trans-unit>
        <trans-unit id="2d7b95845283c4d98cc4167d4e0748ee6c69e84f" translate="yes" xml:space="preserve">
          <source>The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. &amp;ldquo;Algorithms for computing the sample variance: Analysis and recommendations.&amp;rdquo; The American Statistician 37.3 (1983): 242-247:</source>
          <target state="translated">Chan，Tony F.，Gene H. Golub和Randall J. LeVeque的公式1.5a，b中给出了增量均值和std的算法。&amp;ldquo;计算样本方差的算法：分析和建议。&amp;rdquo; 美国统计学家37.3（1983）：242-247：</target>
        </trans-unit>
        <trans-unit id="c4e655f6aa7cf22a58a123dc60b12c815d9f7df3" translate="yes" xml:space="preserve">
          <source>The algorithm is adapted from Guyon [1] and was designed to generate the &amp;ldquo;Madelon&amp;rdquo; dataset.</source>
          <target state="translated">该算法改编自Guyon [1]，旨在生成&amp;ldquo; Madelon&amp;rdquo;数据集。</target>
        </trans-unit>
        <trans-unit id="da28c971693f926f3030184039a925bcc2b1ca76" translate="yes" xml:space="preserve">
          <source>The algorithm is from Marsland [1].</source>
          <target state="translated">该算法来自Marsland[1]。</target>
        </trans-unit>
        <trans-unit id="94c105f8aebba6a360eae1c93878348f10f58fca" translate="yes" xml:space="preserve">
          <source>The algorithm is not highly scalable, as it requires multiple nearest neighbor searches during the execution of the algorithm. The algorithm is guaranteed to converge, however the algorithm will stop iterating when the change in centroids is small.</source>
          <target state="translated">该算法的可扩展性不高,因为在算法执行过程中需要进行多次最近邻搜索。该算法可以保证收敛,但是当中心点变化较小时,该算法将停止迭代。</target>
        </trans-unit>
        <trans-unit id="da3ddd72ab3f78c12c541d3f34c2389bc8e810e8" translate="yes" xml:space="preserve">
          <source>The algorithm is similar to forward stepwise regression, but instead of including features at each step, the estimated coefficients are increased in a direction equiangular to each one&amp;rsquo;s correlations with the residual.</source>
          <target state="translated">该算法类似于正向逐步回归，但不是在每个步骤中都包含特征，而是在与每个人的残差相关性成等角的方向上增加了估计的系数。</target>
        </trans-unit>
        <trans-unit id="3dc6012ab672dce7fe920dc60add16368612e345" translate="yes" xml:space="preserve">
          <source>The algorithm is similar to forward stepwise regression, but instead of including variables at each step, the estimated parameters are increased in a direction equiangular to each one&amp;rsquo;s correlations with the residual.</source>
          <target state="translated">该算法类似于正向逐步回归，但不是在每个步骤中都包含变量，而是在与每个人的相关性与残差相关的方向上增加了估计的参数。</target>
        </trans-unit>
        <trans-unit id="0335c4d6784d824f2c45b2464a3517f723de00c5" translate="yes" xml:space="preserve">
          <source>The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error.</source>
          <target state="translated">该算法是随机的,用不同的种子多次重启会产生不同的嵌入。然而,选择误差最小的嵌入是完全合法的。</target>
        </trans-unit>
        <trans-unit id="7c7de93467b285ca3f0b63a2764ebf7e5ab511ed" translate="yes" xml:space="preserve">
          <source>The algorithm iterates between two major steps, similar to vanilla k-means. In the first step, \(b\) samples are drawn randomly from the dataset, to form a mini-batch. These are then assigned to the nearest centroid. In the second step, the centroids are updated. In contrast to k-means, this is done on a per-sample basis. For each sample in the mini-batch, the assigned centroid is updated by taking the streaming average of the sample and all previous samples assigned to that centroid. This has the effect of decreasing the rate of change for a centroid over time. These steps are performed until convergence or a predetermined number of iterations is reached.</source>
          <target state="translated">该算法在两个主要步骤之间迭代,类似于普通的k-means。第一步,从数据集中随机抽取/(b/)个样本,形成一个小批量。然后,这些样本被分配到最近的中心点。第二步,更新中心点。与k-means相比,这是在每个样本的基础上完成的。对于迷你批次中的每个样本,通过取样本的流式平均值和之前分配给该中心点的所有样本来更新分配的中心点。这具有随着时间的推移降低中心点变化率的效果。这些步骤一直进行到收敛或达到预定的迭代次数为止。</target>
        </trans-unit>
        <trans-unit id="eac8adf7ae77ba9768138371c6c71edd5a3a76a7" translate="yes" xml:space="preserve">
          <source>The algorithm partitions the rows and columns of a matrix so that a corresponding blockwise-constant checkerboard matrix provides a good approximation to the original matrix.</source>
          <target state="translated">该算法对矩阵的行和列进行分割,使相应的块状常数棋盘矩阵对原矩阵有一个很好的近似。</target>
        </trans-unit>
        <trans-unit id="36533938d0e94afa89a69e7b9609c61646ab71f5" translate="yes" xml:space="preserve">
          <source>The algorithm splits the complete input sample data into a set of inliers, which may be subject to noise, and outliers, which are e.g. caused by erroneous measurements or invalid hypotheses about the data. The resulting model is then estimated only from the determined inliers.</source>
          <target state="translated">该算法将完整的输入样本数据分割成一组离群值(可能受噪声影响)和离群值(例如,由错误的测量或关于数据的无效假设引起)。然后,只根据确定的离群值估计所产生的模型。</target>
        </trans-unit>
        <trans-unit id="76e87cc3ec2713663d8449fef17b4e0c4101c305" translate="yes" xml:space="preserve">
          <source>The algorithm stops when it reaches a preset maximum number of iterations; or when the improvement in loss is below a certain, small number.</source>
          <target state="translated">当达到预设的最大迭代次数时,算法就会停止;或者当损失的改善低于某个小数时,算法就会停止。</target>
        </trans-unit>
        <trans-unit id="841b170fb7ba2429816cb7c51af4dbae57b471af" translate="yes" xml:space="preserve">
          <source>The algorithm supports sample weights, which can be given by a parameter &lt;code&gt;sample_weight&lt;/code&gt;. This allows to assign more weight to some samples when computing cluster centers and values of inertia. For example, assigning a weight of 2 to a sample is equivalent to adding a duplicate of that sample to the dataset \(X\).</source>
          <target state="translated">该算法支持样本权重，可以通过参数 &lt;code&gt;sample_weight&lt;/code&gt; 给出。在计算聚类中心和惯性值时，这可以为某些样本分配更多权重。例如，为一个样本分配2的权重等效于将该样本的重复项添加到数据集\（X \）中。</target>
        </trans-unit>
        <trans-unit id="a8238e48cf484817aaf23aa2adc5e4a70681d78f" translate="yes" xml:space="preserve">
          <source>The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details.</source>
          <target state="translated">NearestNeighbors模块用来计算点向距离和寻找最近邻居的算法。详情请参见NearestNeighbors模块文档。</target>
        </trans-unit>
        <trans-unit id="3602b334476c395c578aab7a7d09d75f92100f28" translate="yes" xml:space="preserve">
          <source>The algorithm treats the input data matrix as a bipartite graph: the rows and columns of the matrix correspond to the two sets of vertices, and each entry corresponds to an edge between a row and a column. The algorithm approximates the normalized cut of this graph to find heavy subgraphs.</source>
          <target state="translated">该算法将输入的数据矩阵看作是一个二元图:矩阵的行和列对应两组顶点,每个条目对应行和列之间的一条边。该算法对该图进行近似的归一化切割,以找到重子图。</target>
        </trans-unit>
        <trans-unit id="e4086364ea17b4442e0265a08361358aa304a247" translate="yes" xml:space="preserve">
          <source>The algorithm used to estimate the weights. It will be called n_components times, i.e. once for each iteration of the outer loop.</source>
          <target state="translated">用于估计权重的算法。它将被调用n_components次,即外循环每次迭代一次。</target>
        </trans-unit>
        <trans-unit id="e4e3e486ab0114bca02d02635f9410b77ca99a25" translate="yes" xml:space="preserve">
          <source>The algorithm used to fit the model is coordinate descent.</source>
          <target state="translated">用于拟合模型的算法是坐标下降法。</target>
        </trans-unit>
        <trans-unit id="17e35eb9c6004e2d60b73104c93585aacf4a2e98" translate="yes" xml:space="preserve">
          <source>The algorithmic complexity of affinity propagation is quadratic in the number of points.</source>
          <target state="translated">亲和力传播的算法复杂度在点数上是二次方的。</target>
        </trans-unit>
        <trans-unit id="4c657d81ec38ba678d89f7b7de187e376e50c0f3" translate="yes" xml:space="preserve">
          <source>The algorithms for regression and classification only differ in the concrete loss function used.</source>
          <target state="translated">回归和分类的算法只是在使用的具体损失函数上有所不同。</target>
        </trans-unit>
        <trans-unit id="50542b2bc530bb15fc1bcd67d7b073d61bea03ab" translate="yes" xml:space="preserve">
          <source>The alpha parameter in the stability selection article used to randomly scale the features. Should be between 0 and 1.</source>
          <target state="translated">稳定性选择文章中用于随机缩放特征的α参数。应该在0和1之间。</target>
        </trans-unit>
        <trans-unit id="35d70c04237d10e5da496ff67ac57fce44665490" translate="yes" xml:space="preserve">
          <source>The alpha parameter of the GraphicalLasso setting the sparsity of the model is set by internal cross-validation in the GraphicalLassoCV. As can be seen on figure 2, the grid to compute the cross-validation score is iteratively refined in the neighborhood of the maximum.</source>
          <target state="translated">GraphicalLasso设置模型稀疏度的α参数是由GraphicalLassoCV中的内部交叉验证设置的。从图2可以看出,计算交叉验证得分的网格是在最大值的附近反复细化的。</target>
        </trans-unit>
        <trans-unit id="2075fb3135d145905cc5a41db16871a4bf5168da" translate="yes" xml:space="preserve">
          <source>The alpha-quantile of the huber loss function and the quantile loss function. Only if &lt;code&gt;loss='huber'&lt;/code&gt; or &lt;code&gt;loss='quantile'&lt;/code&gt;.</source>
          <target state="translated">Huber损失函数和分位数损失函数的alpha分位数。仅当 &lt;code&gt;loss='huber'&lt;/code&gt; 或 &lt;code&gt;loss='quantile'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="28a2d4f18d224d5f96bb30d3faa50cdca5ce2935" translate="yes" xml:space="preserve">
          <source>The alphas along the path where models are computed.</source>
          <target state="translated">沿着计算模型的路径的阿尔法。</target>
        </trans-unit>
        <trans-unit id="4390acc7061cc013783802ff89b6303810b0044d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set.</source>
          <target state="translated">数据集的污染量,即数据集中离群值的比例。</target>
        </trans-unit>
        <trans-unit id="a3cf6bdbcc68281bb427886cdd573766d75b2a0b" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Range is (0, 0.5).</source>
          <target state="translated">数据集的污染量,即数据集中离群值的比例。范围为(0,0.5)。</target>
        </trans-unit>
        <trans-unit id="db48eba23a1b19c738dbbe1eb3626efb11bd507f" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. If &amp;lsquo;auto&amp;rsquo;, the decision function threshold is determined as in the original paper.</source>
          <target state="translated">数据集的污染量，即数据集中异常值的比例。在拟合时用于定义决策函数的阈值时使用。如果为&amp;ldquo;自动&amp;rdquo;，则决定功能阈值与原始论文相同。</target>
        </trans-unit>
        <trans-unit id="f37071a773077642311acdcd66bbd39bda02514d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the scores of the samples.</source>
          <target state="translated">数据集的污染量,即数据集中离群值的比例。拟合时用于定义样本分数的阈值。</target>
        </trans-unit>
        <trans-unit id="0b69accc98489279e23dd931886f63d4aafd913d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the decision function. If &amp;ldquo;auto&amp;rdquo;, the decision function threshold is determined as in the original paper.</source>
          <target state="translated">数据集的污染量，即数据集中异常值的比例。拟合时，用于定义决策函数的阈值。如果为&amp;ldquo;自动&amp;rdquo;，则决定功能阈值与原始纸张相同。</target>
        </trans-unit>
        <trans-unit id="a618727acf5955128fa616ec196fb65bef331d06" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the scores of the samples.</source>
          <target state="translated">数据集的污染量,即数据集中离群值的比例。在拟合时,它被用来定义样本分数的阈值。</target>
        </trans-unit>
        <trans-unit id="057670e523b801c82d76306d2019fbc0b1e55fa7" translate="yes" xml:space="preserve">
          <source>The amount of penalization chosen by cross validation</source>
          <target state="translated">交叉验证所选择的惩罚量;</target>
        </trans-unit>
        <trans-unit id="0a8889ae5955aac97641248ca40c9031408985ae" translate="yes" xml:space="preserve">
          <source>The amount of variance explained by each of the selected components.</source>
          <target state="translated">每个选定成分所解释的方差量。</target>
        </trans-unit>
        <trans-unit id="0c35119665a5a383bf2a2ea2491d6548297b77b9" translate="yes" xml:space="preserve">
          <source>The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.</source>
          <target state="translated">输入样本的异常得分计算为森林中树木的平均异常得分。</target>
        </trans-unit>
        <trans-unit id="f5bff141bb39bd3c0bc3c7fd23ee875e735e27a8" translate="yes" xml:space="preserve">
          <source>The anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. These are considered outliers.</source>
          <target state="translated">每个样本的异常得分称为局部离群因子。它衡量一个给定样本的密度相对于其邻居的局部偏差。它是局部的,因为异常得分取决于对象相对于周围邻居的孤立程度。更准确地说,局部性是由k个最近的邻居给出的,其距离被用来估计局部密度。通过比较一个样本的局部密度和其邻居的局部密度,可以识别出密度大大低于其邻居的样本。这些样本被认为是离群值。</target>
        </trans-unit>
        <trans-unit id="9d2b05e02bf58b122225781451ec470ef3bbad43" translate="yes" xml:space="preserve">
          <source>The anomaly score of the input samples. The lower, the more abnormal.</source>
          <target state="translated">输入样本的异常得分。越低,说明越不正常。</target>
        </trans-unit>
        <trans-unit id="7ea9f6456fff678e2acc40131f68202326a3c878" translate="yes" xml:space="preserve">
          <source>The anomaly score of the input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.</source>
          <target state="translated">输入样本的异常得分。越低,越是异常。负分代表异常值,正分代表离群值。</target>
        </trans-unit>
        <trans-unit id="f8f5cba472ebac3a5205adfc98746f94cfb55c44" translate="yes" xml:space="preserve">
          <source>The approximate feature map provided by &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt;&lt;code&gt;AdditiveChi2Sampler&lt;/code&gt;&lt;/a&gt; can be combined with the approximate feature map provided by &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; to yield an approximate feature map for the exponentiated chi squared kernel. See the &lt;a href=&quot;#vz2010&quot; id=&quot;id5&quot;&gt;[VZ2010]&lt;/a&gt; for details and &lt;a href=&quot;#vvz2010&quot; id=&quot;id6&quot;&gt;[VVZ2010]&lt;/a&gt; for combination with the &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">所提供的近似的特征映射&lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt; &lt;code&gt;AdditiveChi2Sampler&lt;/code&gt; &lt;/a&gt;可以与所提供的近似的特征图组合&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;，得到一个近似特征地图的取幂卡方内核。参见&lt;a href=&quot;#vz2010&quot; id=&quot;id5&quot;&gt;[VZ2010]&lt;/a&gt;对于细节和&lt;a href=&quot;#vvz2010&quot; id=&quot;id6&quot;&gt;[VVZ2010]&lt;/a&gt;用于与组合&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="98f8783bf76339c13180f5a0c21989e5d9843d3d" translate="yes" xml:space="preserve">
          <source>The approximate number of singular vectors required to explain most of the data by linear combinations.</source>
          <target state="translated">用线性组合解释大部分数据所需的奇异向量的大致数量。</target>
        </trans-unit>
        <trans-unit id="01540c9a0c0d75da953bc96aafe62b761a8754d9" translate="yes" xml:space="preserve">
          <source>The approximate number of singular vectors required to explain most of the input data by linear combinations. Using this kind of singular spectrum in the input allows the generator to reproduce the correlations often observed in practice.</source>
          <target state="translated">用线性组合解释大部分输入数据所需的奇异向量的大致数量。在输入中使用这种奇异频谱可以使生成器重现实践中经常观察到的相关性。</target>
        </trans-unit>
        <trans-unit id="9b1212b40ba5f9b8205d7b64e2e9c20d35b415b3" translate="yes" xml:space="preserve">
          <source>The arithmetic mean is the sum of the elements along the axis divided by the number of elements.</source>
          <target state="translated">算术平均值是沿轴线的元素之和除以元素数。</target>
        </trans-unit>
        <trans-unit id="35a07a5d9cec4bce6db2b0fe073857a7979c5b16" translate="yes" xml:space="preserve">
          <source>The array has 0.16% of non zero values.</source>
          <target state="translated">该数组有0.16%的非零值。</target>
        </trans-unit>
        <trans-unit id="a77140549775c60e078b0c26c1b965d02c08ebb3" translate="yes" xml:space="preserve">
          <source>The array of (log)-density evaluations</source>
          <target state="translated">(对数)密度评估数组。</target>
        </trans-unit>
        <trans-unit id="449fd4c0217076d90d22633d6cb32b7300c30336" translate="yes" xml:space="preserve">
          <source>The array of (log)-density evaluations, shape = X.shape[:-1]</source>
          <target state="translated">(对数)密度评价数组,shape=X.shape[:-1]。</target>
        </trans-unit>
        <trans-unit id="d00a652e1f004fd3ddfd653b6794ad2aba8108c7" translate="yes" xml:space="preserve">
          <source>The array of log(density) evaluations.</source>
          <target state="translated">对数(密度)评估的数组。</target>
        </trans-unit>
        <trans-unit id="0e2b34732ef7f3aed76135ded438ba1b0e2806d3" translate="yes" xml:space="preserve">
          <source>The array of log(density) evaluations. These are normalized to be probability densities, so values will be low for high-dimensional data.</source>
          <target state="translated">对数(密度)评估的数组。这些都被归一化为概率密度,所以对于高维数据来说,数值会很低。</target>
        </trans-unit>
        <trans-unit id="531360783ebbdd4b65c83458a2d186f114e4086b" translate="yes" xml:space="preserve">
          <source>The automatic estimation from Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604 by Thomas P. Minka is also compared.</source>
          <target state="translated">从自动选择维度进行PCA的自动估计。NIPS 2000:598-604,由Thomas P.Minka撰写,也进行了比较。</target>
        </trans-unit>
        <trans-unit id="37bf2d4736a8a0a6781e876ebdb656796aca8913" translate="yes" xml:space="preserve">
          <source>The available cross validation iterators are introduced in the following section.</source>
          <target state="translated">下文将介绍可用的交叉验证迭代器。</target>
        </trans-unit>
        <trans-unit id="52d681893d146fe7dad6c4424a94a534bab69431" translate="yes" xml:space="preserve">
          <source>The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration.</source>
          <target state="translated">平均复杂度由O(k n T)给出,其中n是样本数,T是迭代次数。</target>
        </trans-unit>
        <trans-unit id="d9092bb0da47977d9b43f41f12fd7dcc75d26801" translate="yes" xml:space="preserve">
          <source>The average number of labels per instance. More precisely, the number of labels per sample is drawn from a Poisson distribution with &lt;code&gt;n_labels&lt;/code&gt; as its expected value, but samples are bounded (using rejection sampling) by &lt;code&gt;n_classes&lt;/code&gt;, and must be nonzero if &lt;code&gt;allow_unlabeled&lt;/code&gt; is False.</source>
          <target state="translated">每个实例的平均标签数。更准确地说，每个样本的标签数是从泊松分布中以 &lt;code&gt;n_labels&lt;/code&gt; 作为其期望值得出的，但是样本受 &lt;code&gt;n_classes&lt;/code&gt; 限制（使用拒绝采样），并且如果 &lt;code&gt;allow_unlabeled&lt;/code&gt; 为False，则该值必须为非零。</target>
        </trans-unit>
        <trans-unit id="50521765649b7ccece3cb452f18896c14c55c5d8" translate="yes" xml:space="preserve">
          <source>The average precision score in multi-label settings</source>
          <target state="translated">多标签设置中的平均精度分数</target>
        </trans-unit>
        <trans-unit id="66fdff3c7719e9049c28a125538dc0d9761fb599" translate="yes" xml:space="preserve">
          <source>The averaged NDCG scores for all samples.</source>
          <target state="translated">所有样本的NDCG平均分。</target>
        </trans-unit>
        <trans-unit id="7f192c1d1db49dfae3574eb70a5374ae8608eb71" translate="yes" xml:space="preserve">
          <source>The averaged intercept term.</source>
          <target state="translated">平均截距项。</target>
        </trans-unit>
        <trans-unit id="9595497d17dae79a02bc4117e4b89bb9f68b4736" translate="yes" xml:space="preserve">
          <source>The averaged intercept term. Only available if &lt;code&gt;average=True&lt;/code&gt;.</source>
          <target state="translated">平均截距项。仅在 &lt;code&gt;average=True&lt;/code&gt; 时可用。</target>
        </trans-unit>
        <trans-unit id="c831f95b759b1e5121cd7c90ec4906a408d65670" translate="yes" xml:space="preserve">
          <source>The averaged sample DCG scores.</source>
          <target state="translated">平均的DCG样本分数。</target>
        </trans-unit>
        <trans-unit id="5f1e3570b35bda8ded1b9651eb310bcefe0589f6" translate="yes" xml:space="preserve">
          <source>The axes with which the grid has been created or None if the grid has been given.</source>
          <target state="translated">创建网格的轴,如果给定了网格,则为 &quot;无&quot;。</target>
        </trans-unit>
        <trans-unit id="da553a7d6d695e12dca5e56a77d2e07dce1c7a46" translate="yes" xml:space="preserve">
          <source>The axis along which &lt;code&gt;X&lt;/code&gt; will be subsampled. &lt;code&gt;axis=0&lt;/code&gt; will select rows while &lt;code&gt;axis=1&lt;/code&gt; will select columns.</source>
          <target state="translated">&lt;code&gt;X&lt;/code&gt; 将被沿其采样的轴。 &lt;code&gt;axis=0&lt;/code&gt; 将选择行，而 &lt;code&gt;axis=1&lt;/code&gt; 将选择列。</target>
        </trans-unit>
        <trans-unit id="2af1a0461163ef9a917dd17f9a6c7032c07c79bb" translate="yes" xml:space="preserve">
          <source>The axis along which to impute.</source>
          <target state="translated">沿着这个轴线进行推算。</target>
        </trans-unit>
        <trans-unit id="efae1008b127cbb418b23469c7f222e2a6660b67" translate="yes" xml:space="preserve">
          <source>The bag of words representation is quite simplistic but surprisingly useful in practice.</source>
          <target state="translated">词袋的表示方法相当简单,但在实践中却出奇的有用。</target>
        </trans-unit>
        <trans-unit id="a41734ff5b2ce49bfc6f83ebb1bb324e76d37d70" translate="yes" xml:space="preserve">
          <source>The bags of words representation implies that &lt;code&gt;n_features&lt;/code&gt; is the number of distinct words in the corpus: this number is typically larger than 100,000.</source>
          <target state="translated">单词袋表示意味着 &lt;code&gt;n_features&lt;/code&gt; 是语料库中不同单词的数量：该数量通常大于100,000。</target>
        </trans-unit>
        <trans-unit id="eac13f6898f72e0eaad5c85bd1b456a5882d1a72" translate="yes" xml:space="preserve">
          <source>The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.</source>
          <target state="translated">二元和多类分类问题中的平衡精度,以处理不平衡的数据集。它被定义为在每个类上获得的召回率的平均值。</target>
        </trans-unit>
        <trans-unit id="fd188197709d9753bf6d2d7d8ebbb1b211377cd3" translate="yes" xml:space="preserve">
          <source>The bandwidth here acts as a smoothing parameter, controlling the tradeoff between bias and variance in the result. A large bandwidth leads to a very smooth (i.e. high-bias) density distribution. A small bandwidth leads to an unsmooth (i.e. high-variance) density distribution.</source>
          <target state="translated">带宽在这里作为一个平滑参数,控制结果中的偏差和方差之间的权衡。大的带宽会导致一个非常平滑(即高偏置)的密度分布,小的带宽会导致一个不平滑(即高变异)的密度分布。小的带宽会导致不平滑(即高变异)的密度分布。</target>
        </trans-unit>
        <trans-unit id="220a3cae014ca1f6f44493020405f4e460f4038d" translate="yes" xml:space="preserve">
          <source>The bandwidth of the kernel.</source>
          <target state="translated">内核的带宽。</target>
        </trans-unit>
        <trans-unit id="eaa1089b1fdc51cb43f528072a19cd3f3ec50c61" translate="yes" xml:space="preserve">
          <source>The bandwidth parameter.</source>
          <target state="translated">带宽参数。</target>
        </trans-unit>
        <trans-unit id="ca9efc037882c6303cbf300aed185a1d9a7dd7ae" translate="yes" xml:space="preserve">
          <source>The bar plot indicates the accuracy, training time (normalized) and test time (normalized) of each classifier.</source>
          <target state="translated">条形图表示每个分类器的精度、训练时间(归一化)和测试时间(归一化)。</target>
        </trans-unit>
        <trans-unit id="6ef0fbe42e5306306c086ebee574632386d4e293" translate="yes" xml:space="preserve">
          <source>The base classifier is a random forest classifier with 25 base estimators (trees). If this classifier is trained on all 800 training datapoints, it is overly confident in its predictions and thus incurs a large log-loss. Calibrating an identical classifier, which was trained on 600 datapoints, with method=&amp;rsquo;sigmoid&amp;rsquo; on the remaining 200 datapoints reduces the confidence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center. This calibration results in a lower log-loss. Note that an alternative would have been to increase the number of base estimators which would have resulted in a similar decrease in log-loss.</source>
          <target state="translated">基本分类器是具有25个基本估计量（树）的随机森林分类器。如果该分类器在所有800个训练数据点上都进行了训练，则对其预测过于自信，因此会导致大量对数损失。使用在剩余的200个数据点上使用method ='sigmoid'校准在600个数据点上训练的相同分类器，会降低预测的可信度，即，将概率矢量从单纯形的边缘移向中心。该校准导致较低的对数损失。注意，另一种选择是增加基本估计量，这将导致对数损失的类似减少。</target>
        </trans-unit>
        <trans-unit id="3f89ef1e6cb2a54f74eb0a982beea8ffd3333592" translate="yes" xml:space="preserve">
          <source>The base classifier is a random forest classifier with 25 base estimators (trees). If this classifier is trained on all 800 training datapoints, it is overly confident in its predictions and thus incurs a large log-loss. Calibrating an identical classifier, which was trained on 600 datapoints, with method=&amp;rsquo;sigmoid&amp;rsquo; on the remaining 200 datapoints reduces the confidence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center:</source>
          <target state="translated">基本分类器是具有25个基本估计量（树）的随机森林分类器。如果该分类器在所有800个训练数据点上都进行了训练，则对其预测过于自信，因此会导致大量对数损失。使用在其余200个数据点上使用method ='sigmoid'校准在600个数据点上训练的相同分类器，会降低预测的可信度，即，将概率矢量从单纯形的边缘移向中心：</target>
        </trans-unit>
        <trans-unit id="2000944fc04d6f6e7393d659c7173e725c27d458" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;.</source>
          <target state="translated">从中构建增强后的合奏的基本估计量。如果为 &lt;code&gt;None&lt;/code&gt; ，则基本估计量为 &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="cdab13e5017fd96224833fdcfcd75615e152e1af" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper &lt;code&gt;classes_&lt;/code&gt; and &lt;code&gt;n_classes_&lt;/code&gt; attributes. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;</source>
          <target state="translated">从中构建增强合奏的基本估计量。需要支持样本加权以及正确的 &lt;code&gt;classes_&lt;/code&gt; 和 &lt;code&gt;n_classes_&lt;/code&gt; 属性。如果为 &lt;code&gt;None&lt;/code&gt; ，则基本估计量为 &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="7ad7363185e9af79d948e6b752e7cc25ad567f4b" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper &lt;code&gt;classes_&lt;/code&gt; and &lt;code&gt;n_classes_&lt;/code&gt; attributes. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;.</source>
          <target state="translated">从中构建增强后的合奏的基本估计量。需要支持样本加权以及适当的 &lt;code&gt;classes_&lt;/code&gt; 和 &lt;code&gt;n_classes_&lt;/code&gt; 属性。如果为 &lt;code&gt;None&lt;/code&gt; ，则基本估计量为 &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="39ba9d176a83a208ed96d60f33bec321ac051ac5" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;</source>
          <target state="translated">从中构建增强合奏的基本估计量。需要支持样本加权。如果为 &lt;code&gt;None&lt;/code&gt; ，则基本估计量为 &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="09e53f3d87743d060ad05fb04ae38587d00307b0" translate="yes" xml:space="preserve">
          <source>The base estimator from which the classifier chain is built.</source>
          <target state="translated">用于建立分类器链的基础估计器。</target>
        </trans-unit>
        <trans-unit id="c229b061f41bb16a20d56916c466508eaa778c17" translate="yes" xml:space="preserve">
          <source>The base estimator from which the ensemble is grown.</source>
          <target state="translated">基准估计器,从这个估计器中生长出集合。</target>
        </trans-unit>
        <trans-unit id="5066de8ea8cb5c40493cb3ffbb0e912b0ff83ff9" translate="yes" xml:space="preserve">
          <source>The base estimator from which the transformer is built. This can be both a fitted (if &lt;code&gt;prefit&lt;/code&gt; is set to True) or a non-fitted estimator. The estimator must have either a &lt;code&gt;feature_importances_&lt;/code&gt; or &lt;code&gt;coef_&lt;/code&gt; attribute after fitting.</source>
          <target state="translated">用来构建变压器的基本估算器。这可以是拟合的（如果 &lt;code&gt;prefit&lt;/code&gt; 拟合设置为True）或非拟合的估计器。拟合后，估算器必须具有 &lt;code&gt;feature_importances_&lt;/code&gt; 或 &lt;code&gt;coef_&lt;/code&gt; 属性。</target>
        </trans-unit>
        <trans-unit id="8935e197daa0e6c85ff9f9b0e235723b5ff13893" translate="yes" xml:space="preserve">
          <source>The base estimator from which the transformer is built. This is stored only when a non-fitted estimator is passed to the &lt;code&gt;SelectFromModel&lt;/code&gt;, i.e when prefit is False.</source>
          <target state="translated">用来构建变压器的基本估算器。仅当将非拟合估计量传递给 &lt;code&gt;SelectFromModel&lt;/code&gt; 时（即，当prefit为False时）才存储此字段。</target>
        </trans-unit>
        <trans-unit id="8062156aa44a8a655e18edffbba0a99e0e098c8a" translate="yes" xml:space="preserve">
          <source>The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.</source>
          <target state="translated">在数据集的随机子集上拟合的基础估计器,如果无,则基础估计器是决策树。如果无,则基础估计器是决策树。</target>
        </trans-unit>
        <trans-unit id="d80c39c018abc387662fda22711c8b7bb4707717" translate="yes" xml:space="preserve">
          <source>The base kernel</source>
          <target state="translated">基本内核</target>
        </trans-unit>
        <trans-unit id="1b7bc12e1d1eb0bb8e39e15748d7f9b27d93ef1a" translate="yes" xml:space="preserve">
          <source>The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the &lt;code&gt;weights&lt;/code&gt; keyword. The default value, &lt;code&gt;weights = 'uniform'&lt;/code&gt;, assigns uniform weights to each neighbor. &lt;code&gt;weights = 'distance'&lt;/code&gt; assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights.</source>
          <target state="translated">基本的最近邻居分类使用统一的权重：即，分配给查询点的值是根据最近邻居的简单多数票计算得出的。在某些情况下，最好对邻居加权，以使较近的邻居对拟合的贡献更大。这可以通过 &lt;code&gt;weights&lt;/code&gt; 关键字完成。默认值 &lt;code&gt;weights = 'uniform'&lt;/code&gt; ，为每个邻居分配统一的权重。 &lt;code&gt;weights = 'distance'&lt;/code&gt; 分配的权重与距查询点的距离成反比。可替代地，可以提供距离的用户定义函数来计算权重。</target>
        </trans-unit>
        <trans-unit id="650ceb2a1485e5f890a3dcff86c2748037640c3b" translate="yes" xml:space="preserve">
          <source>The basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. Under some circumstances, it can be advantageous to weight points such that nearby points contribute more to the regression than faraway points. This can be accomplished through the &lt;code&gt;weights&lt;/code&gt; keyword. The default value, &lt;code&gt;weights = 'uniform'&lt;/code&gt;, assigns equal weights to all points. &lt;code&gt;weights = 'distance'&lt;/code&gt; assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied, which will be used to compute the weights.</source>
          <target state="translated">基本最近邻回归使用统一的权重：也就是说，本地邻域中的每个点均对查询点的分类做出统一的贡献。在某些情况下，对权重点进行加权可能会比较有利，以使邻近的点比远离的点对回归的贡献更大。这可以通过 &lt;code&gt;weights&lt;/code&gt; 关键字完成。默认值 &lt;code&gt;weights = 'uniform'&lt;/code&gt; ，将相等的权重分配给所有点。 &lt;code&gt;weights = 'distance'&lt;/code&gt; 分配的权重与距查询点的距离成反比。或者，可以提供距离的用户定义功能，该功能将用于计算权重。</target>
        </trans-unit>
        <trans-unit id="1cb5ab68855135ab83ff44b066a7bba092358a07" translate="yes" xml:space="preserve">
          <source>The behavior of &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; is summarized in the following table.</source>
          <target state="translated">下表总结了&lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt;的行为。</target>
        </trans-unit>
        <trans-unit id="9c1d95adae9acea07102a8bfe1db513459ebeaee" translate="yes" xml:space="preserve">
          <source>The behavior of the model is very sensitive to the &lt;code&gt;gamma&lt;/code&gt; parameter. If &lt;code&gt;gamma&lt;/code&gt; is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with &lt;code&gt;C&lt;/code&gt; will be able to prevent overfitting.</source>
          <target state="translated">模型的行为对 &lt;code&gt;gamma&lt;/code&gt; 参数非常敏感。如果 &lt;code&gt;gamma&lt;/code&gt; 太大，则支持向量的影响区域的半径仅包括支持向量本身，并且使用 &lt;code&gt;C&lt;/code&gt; 进行的正则化量都无法防止过度拟合。</target>
        </trans-unit>
        <trans-unit id="2ab99fa4c73a04aa03f4ac31b15851c1ddbe51a1" translate="yes" xml:space="preserve">
          <source>The below plot uses the first two features. See &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;here&lt;/a&gt; for more information on this dataset.</source>
          <target state="translated">下图使用了前两个功能。有关此数据集的更多信息，请参见&lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;此处&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4992d67c6e1aff2322ba4c5bb3c9e0c82a7a36cf" translate="yes" xml:space="preserve">
          <source>The best model is selected by cross-validation.</source>
          <target state="translated">通过交叉验证,选出最佳模型。</target>
        </trans-unit>
        <trans-unit id="1a3f42d6835c4f6686d54e99d9f9cd0e3a597fec" translate="yes" xml:space="preserve">
          <source>The best performance is 1 with &lt;code&gt;normalize == True&lt;/code&gt; and the number of samples with &lt;code&gt;normalize == False&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;normalize == True&lt;/code&gt; ，最佳性能为1；归 &lt;code&gt;normalize == False&lt;/code&gt; 的样本数量。</target>
        </trans-unit>
        <trans-unit id="4b2ab3d16cd4ed6ed82ae137ea7b0ef6d79ff832" translate="yes" xml:space="preserve">
          <source>The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.</source>
          <target state="translated">最好的可能p值是1/(n_permutations+1),最差的是1.0。</target>
        </trans-unit>
        <trans-unit id="ed9ac623a873633695e926c14de64904b54167f4" translate="yes" xml:space="preserve">
          <source>The best possible score is 1.0, lower values are worse.</source>
          <target state="translated">最好的分数是1.0,数值越低越差。</target>
        </trans-unit>
        <trans-unit id="9230aaef9f0a1fa5e57a82e7f44b36cb5d04d36e" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.</source>
          <target state="translated">最佳值为1,最差值为-1。接近0的数值表示重叠的群组。</target>
        </trans-unit>
        <trans-unit id="6e2eac60da3011cdc30f86be85be8ffa7e5f1da5" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.</source>
          <target state="translated">最佳值为1,最差值为-1。接近0的值表示重叠的聚类,负值一般表示一个样本被分配到了错误的聚类,因为不同的聚类更相似。负值一般表示一个样本被分配到了错误的聚类,因为不同的聚类更相似。</target>
        </trans-unit>
        <trans-unit id="b4a689b86b5cda51ef6628ef622a0daf756424bd" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is 0 when &lt;code&gt;adjusted=False&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;adjusted=False&lt;/code&gt; 时，最佳值为1，最差值为0 。</target>
        </trans-unit>
        <trans-unit id="488558d8da77f986fd351608404cb6a07bd4fe58" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is 0.</source>
          <target state="translated">最佳值为1,最差值为0。</target>
        </trans-unit>
        <trans-unit id="b82813aa45a878f8df07ac95972e2e5116e63bed" translate="yes" xml:space="preserve">
          <source>The bias term in the underlying linear model.</source>
          <target state="translated">基础线性模型中的偏差项。</target>
        </trans-unit>
        <trans-unit id="d5675ace96b0ccc75bff7a0b67255343283bf5b9" translate="yes" xml:space="preserve">
          <source>The bicluster label of each column.</source>
          <target state="translated">每个列的双簇标签。</target>
        </trans-unit>
        <trans-unit id="d135838b65ddeda2dac8521941211167551d33d9" translate="yes" xml:space="preserve">
          <source>The bicluster label of each row.</source>
          <target state="translated">每行的双簇标签。</target>
        </trans-unit>
        <trans-unit id="b216c2ddafb03b97c09e36e1d0b96ac53ff1f7ba" translate="yes" xml:space="preserve">
          <source>The bins have identical widths.</source>
          <target state="translated">仓的宽度相同。</target>
        </trans-unit>
        <trans-unit id="c0eb1458ca2a62b6254994f184c01ee65afc52b4" translate="yes" xml:space="preserve">
          <source>The bins have the same number of samples and depend on &lt;code&gt;y_prob&lt;/code&gt;.</source>
          <target state="translated">箱具有相同数量的样本，并取决于 &lt;code&gt;y_prob&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4f153639172a63c8dba2ed27e16715aaa100936d" translate="yes" xml:space="preserve">
          <source>The bipartite structure allows for the use of efficient block Gibbs sampling for inference.</source>
          <target state="translated">两段式结构可以使用高效的块状吉布斯采样进行推理。</target>
        </trans-unit>
        <trans-unit id="a902e8cbc26950a56283f916fe814a1e5f8ed790" translate="yes" xml:space="preserve">
          <source>The bottleneck of a gradient boosting procedure is building the decision trees. Building a traditional decision tree (as in the other GBDTs &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;) requires sorting the samples at each node (for each feature). Sorting is needed so that the potential gain of a split point can be computed efficiently. Splitting a single node has thus a complexity of \(\mathcal{O}(n_\text{features} \times n \log(n))\) where \(n\) is the number of samples at the node.</source>
          <target state="translated">梯度提升过程的瓶颈在于构建决策树。构建传统的决策树（与其他GBDT中的&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; 一样&lt;/a&gt;）要求对每个节点（针对每个要素）的样本进行排序。需要排序，以便可以有效地计算分割点的潜在增益。因此，拆分单个节点的复杂度为\（\ mathcal {O}（n_ \ text {features} \ times n \ log（n））\），其中\（n \）是该节点处的样本数。</target>
        </trans-unit>
        <trans-unit id="b104bf424ac624d7987a6846ffafb73fcb6a3323" translate="yes" xml:space="preserve">
          <source>The bounding box for each cluster center when centers are generated at random.</source>
          <target state="translated">随机生成中心时,每个簇中心的边界框。</target>
        </trans-unit>
        <trans-unit id="e73634a1f979122b85e5e2e94c3d89b631e61d63" translate="yes" xml:space="preserve">
          <source>The boxes represent repeated sampling.</source>
          <target state="translated">方框代表重复抽样。</target>
        </trans-unit>
        <trans-unit id="de87510aaa8a4634fe7b670444f1141d0b934e04" translate="yes" xml:space="preserve">
          <source>The breast cancer dataset is a classic and very easy binary classification dataset.</source>
          <target state="translated">乳腺癌数据集是一个经典且非常简单的二元分类数据集。</target>
        </trans-unit>
        <trans-unit id="2045b20286b76d60e5fa1f74a1df8fc02a45c66b" translate="yes" xml:space="preserve">
          <source>The brier score loss is also between 0 to 1 and the lower the score (the mean square difference is smaller), the more accurate the prediction is. It can be thought of as a measure of the &amp;ldquo;calibration&amp;rdquo; of a set of probabilistic predictions.</source>
          <target state="translated">刺痛分数损失也在0到1之间，并且分数越低（均方差越小），则预测越准确。可以将其视为对一组概率预测的&amp;ldquo;校准&amp;rdquo;的度量。</target>
        </trans-unit>
        <trans-unit id="b76bd58217534fc8775d360bccb78e720219d44b" translate="yes" xml:space="preserve">
          <source>The calibration is based on the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt; method of the &lt;code&gt;base_estimator&lt;/code&gt; if it exists, else on &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt;.</source>
          <target state="translated">校准是基于&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-decision-function&quot;&gt;decision_function&lt;/a&gt;的方法 &lt;code&gt;base_estimator&lt;/code&gt; 如果它存在，否则就&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-predict-proba&quot;&gt;predict_proba&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6452eadf0058adcc76ac85d9ade38659e522fb0c" translate="yes" xml:space="preserve">
          <source>The calibration of the model can be assessed by plotting the mean observed value vs the mean predicted value on groups of test samples binned by predicted risk.</source>
          <target state="translated">模型的校准可以通过绘制按预测风险分层的测试样本组的平均观察值与平均预测值来评估。</target>
        </trans-unit>
        <trans-unit id="74481417cca6e6adb9f25dfbb8e31b8fc201eb01" translate="yes" xml:space="preserve">
          <source>The callable to use for the inverse transformation. This will be passed the same arguments as inverse transform, with args and kwargs forwarded. If inverse_func is None, then inverse_func will be the identity function.</source>
          <target state="translated">用于反变换的可调用。这将被传递与inverse transform相同的参数,args和kwargs将被转发。如果inverse_func为None,那么inverse_func将是身份函数。</target>
        </trans-unit>
        <trans-unit id="81ea3433c521d90153f147dfae64d0df290c9757" translate="yes" xml:space="preserve">
          <source>The callable to use for the transformation. This will be passed the same arguments as transform, with args and kwargs forwarded. If func is None, then func will be the identity function.</source>
          <target state="translated">用于转换的callable。这将被传递与 transform 相同的参数,args 和 kwargs 将被转发。如果func为None,那么func将是身份函数。</target>
        </trans-unit>
        <trans-unit id="a1419fd6aa69dd9ba89b3d48a7257eafd52e186a" translate="yes" xml:space="preserve">
          <source>The categorical Naive Bayes classifier is suitable for classification with discrete features that are categorically distributed. The categories of each feature are drawn from a categorical distribution.</source>
          <target state="translated">分类Naive Bayes分类器适用于分类具有分类分布的离散特征。每个特征的类别都是从分类分布中抽取的。</target>
        </trans-unit>
        <trans-unit id="1f38539ea641dd8b50f2f7d85a7f466ba670ba50" translate="yes" xml:space="preserve">
          <source>The categories of each feature determined during fitting (in order of the features in X and corresponding with the output of &lt;code&gt;transform&lt;/code&gt;).</source>
          <target state="translated">在拟合过程中确定的每个特征的类别（按X中特征的顺序，并与 &lt;code&gt;transform&lt;/code&gt; 的输出相对应）。</target>
        </trans-unit>
        <trans-unit id="12b8e6304a220ce13ab40041e41390ae67d86b4e" translate="yes" xml:space="preserve">
          <source>The categories of each feature determined during fitting (in order of the features in X and corresponding with the output of &lt;code&gt;transform&lt;/code&gt;). This includes the category specified in &lt;code&gt;drop&lt;/code&gt; (if any).</source>
          <target state="translated">在拟合过程中确定的每个特征的类别（按X中特征的顺序，并与 &lt;code&gt;transform&lt;/code&gt; 的输出相对应）。这包括 &lt;code&gt;drop&lt;/code&gt; 中指定的类别（如果有）。</target>
        </trans-unit>
        <trans-unit id="11dc40b09acf079281dfd9a81c3951e435409b7d" translate="yes" xml:space="preserve">
          <source>The centers of each cluster. Only returned if &lt;code&gt;return_centers=True&lt;/code&gt;.</source>
          <target state="translated">每个群集的中心。仅在 &lt;code&gt;return_centers=True&lt;/code&gt; 时返回。</target>
        </trans-unit>
        <trans-unit id="9d4177cfd25fe2d42f6a09b10ba8f700ec03cc45" translate="yes" xml:space="preserve">
          <source>The chi squared kernel is given by</source>
          <target state="translated">由以下公式可知,chi平方核</target>
        </trans-unit>
        <trans-unit id="e654b311ef425d553f0f6705f2dd43fc95968d41" translate="yes" xml:space="preserve">
          <source>The chi squared kernel is most commonly used on histograms (bags) of visual words.</source>
          <target state="translated">chi平方核最常用于视觉词的直方图(袋)上。</target>
        </trans-unit>
        <trans-unit id="085e153138bf3768d8fc53856c1f1238c6d3caff" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is a very popular choice for training non-linear SVMs in computer vision applications. It can be computed using &lt;a href=&quot;generated/sklearn.metrics.pairwise.chi2_kernel#sklearn.metrics.pairwise.chi2_kernel&quot;&gt;&lt;code&gt;chi2_kernel&lt;/code&gt;&lt;/a&gt; and then passed to an &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;sklearn.svm.SVC&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;kernel=&quot;precomputed&quot;&lt;/code&gt;:</source>
          <target state="translated">卡方核是在计算机视觉应用中训练非线性SVM的非常受欢迎的选择。可以使用&lt;a href=&quot;generated/sklearn.metrics.pairwise.chi2_kernel#sklearn.metrics.pairwise.chi2_kernel&quot;&gt; &lt;code&gt;chi2_kernel&lt;/code&gt; &lt;/a&gt;进行计算，然后使用 &lt;code&gt;kernel=&quot;precomputed&quot;&lt;/code&gt; 传递给&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;sklearn.svm.SVC&lt;/code&gt; &lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="9233e345f2318b5fc92b12dd745f5adf2df3c33c" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is computed between each pair of rows in X and Y. X and Y have to be non-negative. This kernel is most commonly applied to histograms.</source>
          <target state="translated">在X和Y中的每对行之间计算chi平方核,X和Y必须是非负数。这个核最常用于直方图。</target>
        </trans-unit>
        <trans-unit id="a8ba729b106653acd0bc97ad998e8dac7b2b635e" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is given by:</source>
          <target state="translated">芝方核的计算公式为:。</target>
        </trans-unit>
        <trans-unit id="959a186ffdea8e09c8857bbd475b2cf453d363af" translate="yes" xml:space="preserve">
          <source>The child estimator template used to create the collection of fitted sub-estimators.</source>
          <target state="translated">用于创建拟合子估计器集合的子估计器模板。</target>
        </trans-unit>
        <trans-unit id="5ac1548c51d0c4529f9b6250f5d16db6b392dc40" translate="yes" xml:space="preserve">
          <source>The children of each non-leaf node. Values less than &lt;code&gt;n_features&lt;/code&gt; correspond to leaves of the tree which are the original samples. A node &lt;code&gt;i&lt;/code&gt; greater than or equal to &lt;code&gt;n_features&lt;/code&gt; is a non-leaf node and has children &lt;code&gt;children_[i - n_features]&lt;/code&gt;. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node &lt;code&gt;n_features + i&lt;/code&gt;</source>
          <target state="translated">每个非叶节点的子级。小于 &lt;code&gt;n_features&lt;/code&gt; 的值对应于作为原始样本的树的叶子。大于或等于 &lt;code&gt;n_features&lt;/code&gt; 的节点 &lt;code&gt;i&lt;/code&gt; 是非叶节点，并且具有子级 &lt;code&gt;children_[i - n_features]&lt;/code&gt; 。或者，在第i次迭代中，children [i] [0]和children [i] [1]合并以形成节点 &lt;code&gt;n_features + i&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c8989abb78a441b49758b99ec48d8b090a3ce248" translate="yes" xml:space="preserve">
          <source>The children of each non-leaf node. Values less than &lt;code&gt;n_samples&lt;/code&gt; correspond to leaves of the tree which are the original samples. A node &lt;code&gt;i&lt;/code&gt; greater than or equal to &lt;code&gt;n_samples&lt;/code&gt; is a non-leaf node and has children &lt;code&gt;children_[i - n_samples]&lt;/code&gt;. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node &lt;code&gt;n_samples + i&lt;/code&gt;</source>
          <target state="translated">每个非叶节点的子级。小于 &lt;code&gt;n_samples&lt;/code&gt; 的值对应于作为原始样本的树的叶子。大于或等于 &lt;code&gt;n_samples&lt;/code&gt; 的节点 &lt;code&gt;i&lt;/code&gt; 是非叶节点，并且具有子级 &lt;code&gt;children_[i - n_samples]&lt;/code&gt; 。或者，在第i次迭代中，children [i] [0]和children [i] [1]合并以形成节点 &lt;code&gt;n_samples + i&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f3473fe6f2b500963d93290672a7a3ef3f212da2" translate="yes" xml:space="preserve">
          <source>The choice of features is not particularly helpful, but serves to illustrate the technique.</source>
          <target state="translated">特征的选择并没有特别的帮助,但起到了说明技术的作用。</target>
        </trans-unit>
        <trans-unit id="15475f4de31a0dbbaa5b0396fc18010629c57dd3" translate="yes" xml:space="preserve">
          <source>The choice of the distribution depends on the problem at hand:</source>
          <target state="translated">分布的选择取决于当前的问题。</target>
        </trans-unit>
        <trans-unit id="1f460fe2e0e34556a3c6a8a6d017a5f63eeb9ea9" translate="yes" xml:space="preserve">
          <source>The cholesky decomposition of the precision matrices of each mixture component. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">每个混合成分的精确矩阵的cholesky分解。精度矩阵是协方差矩阵的逆矩阵。协方差矩阵是对称正定的，因此可以通过精度矩阵等效地对高斯的混合进行参数化。存储精度矩阵而不是协方差矩阵使在测试时计算新样本的对数似然更有效率。形状取决于 &lt;code&gt;covariance_type&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="8e7b3855f3de3292a39f43c9800aebcf16bc77cb" translate="yes" xml:space="preserve">
          <source>The claim &lt;strong&gt;frequency&lt;/strong&gt; is the number of claims divided by the exposure, typically measured in number of claims per year.</source>
          <target state="translated">索赔&lt;strong&gt;频率&lt;/strong&gt;是索赔数量除以风险敞口，通常以每年的索赔数量来衡量。</target>
        </trans-unit>
        <trans-unit id="54b709e28c0bc302f278f75cf2f9281781fc40ea" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; used with the optional parameter &lt;code&gt;svd_solver='randomized'&lt;/code&gt; is very useful in that case: since we are going to drop most of the singular vectors it is much more efficient to limit the computation to an approximated estimate of the singular vectors we will keep to actually perform the transform.</source>
          <target state="translated">在这种情况下，与可选参数 &lt;code&gt;svd_solver='randomized'&lt;/code&gt; 一起使用的&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;类非常有用：由于我们将丢弃大多数奇异向量，因此将计算限制为对奇异向量的近似估计会更加有效。继续实际执行转换。</target>
        </trans-unit>
        <trans-unit id="9afc238719eb2575519db8e476eb36c39ec071a2" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.feature_extraction.dictvectorizer#sklearn.feature_extraction.DictVectorizer&quot;&gt;&lt;code&gt;DictVectorizer&lt;/code&gt;&lt;/a&gt; can be used to convert feature arrays represented as lists of standard Python &lt;code&gt;dict&lt;/code&gt; objects to the NumPy/SciPy representation used by scikit-learn estimators.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.dictvectorizer#sklearn.feature_extraction.DictVectorizer&quot;&gt; &lt;code&gt;DictVectorizer&lt;/code&gt; &lt;/a&gt;类可用于将表示为标准Python &lt;code&gt;dict&lt;/code&gt; 对象列表的要素数组转换为scikit-learn估计器使用的NumPy / SciPy表示形式。</target>
        </trans-unit>
        <trans-unit id="0b3d7d58279a2952a288a94db8a135a534d78ec7" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; is a high-speed, low-memory vectorizer that uses a technique known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_hashing&quot;&gt;feature hashing&lt;/a&gt;, or the &amp;ldquo;hashing trick&amp;rdquo;. Instead of building a hash table of the features encountered in training, as the vectorizers do, instances of &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; apply a hash function to the features to determine their column index in sample matrices directly. The result is increased speed and reduced memory usage, at the expense of inspectability; the hasher does not remember what the input features looked like and has no &lt;code&gt;inverse_transform&lt;/code&gt; method.</source>
          <target state="translated">类&lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt;是一种高速，低内存的矢量化程序，它使用一种称为&lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_hashing&quot;&gt;特征哈希&lt;/a&gt;或&amp;ldquo;哈希技巧&amp;rdquo;的技术。像矢量化程序一样，&lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt;实例不像训练器那样构建训练中遇到的特征的哈希表，而是将哈希函数应用于特征以直接确定其在样本矩阵中的列索引。结果是提高了速度，减少了内存使用，但以可检查性为代价；哈希器无法记住输入要素的外观，并且没有 &lt;code&gt;inverse_transform&lt;/code&gt; 方法。</target>
        </trans-unit>
        <trans-unit id="46148e4d160f9510def06597f1921af425f74fcb" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt;&lt;code&gt;IsotonicRegression&lt;/code&gt;&lt;/a&gt; fits a non-decreasing function to data. It solves the following problem:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt; &lt;code&gt;IsotonicRegression&lt;/code&gt; &lt;/a&gt;类将非递减函数拟合到数据。它解决了以下问题：</target>
        </trans-unit>
        <trans-unit id="0773ec8e22bf216d4df607f3329b18b1168f7c09" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt;&lt;code&gt;IsotonicRegression&lt;/code&gt;&lt;/a&gt; fits a non-decreasing real function to 1-dimensional data. It solves the following problem:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt; &lt;code&gt;IsotonicRegression&lt;/code&gt; &lt;/a&gt;类将一维递减实函数拟合到一维数据。它解决了以下问题：</target>
        </trans-unit>
        <trans-unit id="04846022f8485d75461ddcb301a4717897728672" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt;&lt;code&gt;AdditiveChi2Sampler&lt;/code&gt;&lt;/a&gt; implements this component wise deterministic sampling. Each component is sampled \(n\) times, yielding \(2n+1\) dimensions per input dimension (the multiple of two stems from the real and complex part of the Fourier transform). In the literature, \(n\) is usually chosen to be 1 or 2, transforming the dataset to size &lt;code&gt;n_samples * 5 * n_features&lt;/code&gt; (in the case of \(n=2\)).</source>
          <target state="translated">类&lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt; &lt;code&gt;AdditiveChi2Sampler&lt;/code&gt; &lt;/a&gt;实现此组件明智的确定性采样。每个分量采样\（n \）次，每个输入维度产生\（2n + 1 \）个维度（傅立叶变换的实数部分和复数部分的两个茎的倍数）。在文献中，\（n \）通常选择为1或2，将数据集转换为大小 &lt;code&gt;n_samples * 5 * n_features&lt;/code&gt; （在\（n = 2 \）的情况下）。</target>
        </trans-unit>
        <trans-unit id="9fdc0ed638cb0889fa5320ee5ada72b80a16402f" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.elasticnetcv#sklearn.linear_model.ElasticNetCV&quot;&gt;&lt;code&gt;ElasticNetCV&lt;/code&gt;&lt;/a&gt; can be used to set the parameters &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) and &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) by cross-validation.</source>
          <target state="translated">可以使用类&lt;a href=&quot;generated/sklearn.linear_model.elasticnetcv#sklearn.linear_model.ElasticNetCV&quot;&gt; &lt;code&gt;ElasticNetCV&lt;/code&gt; &lt;/a&gt;通过交叉验证来设置参数 &lt;code&gt;alpha&lt;/code&gt; （\（\ alpha \））和 &lt;code&gt;l1_ratio&lt;/code&gt; （\（\ rho \））。</target>
        </trans-unit>
        <trans-unit id="c89182d0633b09742e745213024a53ccec4fee0a" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnetcv#sklearn.linear_model.MultiTaskElasticNetCV&quot;&gt;&lt;code&gt;MultiTaskElasticNetCV&lt;/code&gt;&lt;/a&gt; can be used to set the parameters &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) and &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) by cross-validation.</source>
          <target state="translated">类&lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnetcv#sklearn.linear_model.MultiTaskElasticNetCV&quot;&gt; &lt;code&gt;MultiTaskElasticNetCV&lt;/code&gt; &lt;/a&gt;可以用来设置参数 &lt;code&gt;alpha&lt;/code&gt; （\（\阿尔法\））和 &lt;code&gt;l1_ratio&lt;/code&gt; （\（\ RHO \））通过交叉验证。</target>
        </trans-unit>
        <trans-unit id="6c66ee2c7a53fbda2b2b2da602d1e782a1fcc1b3" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a first-order SGD learning routine. The algorithm iterates over the training examples and for each example updates the model parameters according to the update rule given by</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;类实现一阶SGD学习例程。该算法迭代训练示例，并针对每个示例根据由给出的更新规则更新模型参数</target>
        </trans-unit>
        <trans-unit id="f128e57d09cf8b511f002f70c40429b250044323" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;类实现了简单的随机梯度下降学习例程，该例程支持不同的损失函数和分类惩罚。</target>
        </trans-unit>
        <trans-unit id="3fc72b8fffc5a512701e2a944bd472c169bc1771" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification. Below is the decision boundary of a &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; trained with the hinge loss, equivalent to a linear SVM.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;类实现了简单的随机梯度下降学习例程，该例程支持不同的损失函数和分类惩罚。以下是经过铰链损耗训练的&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;的决策边界，等效于线性SVM。</target>
        </trans-unit>
        <trans-unit id="9be5b4c1c13de0290ee1bddf8fed519138b844e9" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; is well suited for regression problems with a large number of training samples (&amp;gt; 10.000), for other problems we recommend &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt;, or &lt;a href=&quot;generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt;&lt;code&gt;ElasticNet&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;类实现了简单的随机梯度下降学习例程，该例程支持不同的损失函数和惩罚以拟合线性回归模型。&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;非常适合于具有大量训练样本（&amp;gt; 10.000）的回归问题，对于其他问题，我们建议使用&lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt; &lt;code&gt;ElasticNet&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="5ca16e4c08c0574f508b6c526beeb2111eade54a" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;OneClassSVM&lt;/code&gt;&lt;/a&gt; implements a One-Class SVM which is used in outlier detection.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;OneClassSVM&lt;/code&gt; &lt;/a&gt;类实现了用于异常值检测的One-Class SVM。</target>
        </trans-unit>
        <trans-unit id="5eaffd69df806c5b8353c88358cfd0a49b275168" translate="yes" xml:space="preserve">
          <source>The class label for which the PDPs should be computed. Only if gbrt is a multi-class model. Must be in &lt;code&gt;gbrt.classes_&lt;/code&gt;.</source>
          <target state="translated">应该为其计算PDP的类标签。仅当gbrt是多类模型时。必须在 &lt;code&gt;gbrt.classes_&lt;/code&gt; 中。</target>
        </trans-unit>
        <trans-unit id="2a31ab08188f24bd31e99f218cab5467eddf2ef0" translate="yes" xml:space="preserve">
          <source>The class labels.</source>
          <target state="translated">类标签。</target>
        </trans-unit>
        <trans-unit id="7c57202048886b9c3874e17b392c104a7a21da98" translate="yes" xml:space="preserve">
          <source>The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;.</source>
          <target state="translated">输入样本的类对数概率。类的顺序与属性&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_中&lt;/a&gt;的顺序相对应。</target>
        </trans-unit>
        <trans-unit id="075fd1f474c95d9f3895b75ead4b1f9c99be54a8" translate="yes" xml:space="preserve">
          <source>The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;.</source>
          <target state="translated">输入样本的类对数概率。类的顺序与属性 &lt;code&gt;classes_&lt;/code&gt; 中的顺序相对应。</target>
        </trans-unit>
        <trans-unit id="b5b69c523a5547b26d366d35f11f7dab77d71024" translate="yes" xml:space="preserve">
          <source>The class of Matern kernels is a generalization of the &lt;a href=&quot;sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt;. It has an additional parameter \(\nu\) which controls the smoothness of the resulting function. The smaller \(\nu\), the less smooth the approximated function is. As \(\nu\rightarrow\infty\), the kernel becomes equivalent to the &lt;a href=&quot;sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; kernel. When \(\nu = 1/2\), the Mat&amp;eacute;rn kernel becomes identical to the absolute exponential kernel. Important intermediate values are \(\nu=1.5\) (once differentiable functions) and \(\nu=2.5\) (twice differentiable functions).</source>
          <target state="translated">Matern核的类是&lt;a href=&quot;sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; &lt;/a&gt;的推广。它具有一个附加参数\（\ nu \），该参数控制所得函数的平滑度。\（\ nu \）越小，近似函数越不平滑。作为\（\ nu \ rightarrow \ infty \），内核变得等同于&lt;a href=&quot;sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; &lt;/a&gt;内核。当\（\ nu = 1/2 \）时，Mat&amp;eacute;rn内核与绝对指数内核相同。重要的中间值是\（\ nu = 1.5 \）（一次微分函数）和\（\ nu = 2.5 \）（两次微分函数）。</target>
        </trans-unit>
        <trans-unit id="f9e49144e04860d9a1f4a8512901a503e5d41c6f" translate="yes" xml:space="preserve">
          <source>The class of Matern kernels is a generalization of the RBF and the absolute exponential kernel parameterized by an additional parameter nu. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions).</source>
          <target state="translated">Matern核的类别是RBF和绝对指数核的泛化,其参数化为一个附加参数nu。nu越小,近似函数越不平滑。当nu=inf时,该核相当于RBF核,nu=0.5时相当于绝对指数核。重要的中间值是nu=1.5(一次可微分函数)和nu=2.5(两次可微分函数)。</target>
        </trans-unit>
        <trans-unit id="e48205f573bda857c27ec3937eb80960daed3556" translate="yes" xml:space="preserve">
          <source>The class ordering is preserved:</source>
          <target state="translated">类的排序被保留下来。</target>
        </trans-unit>
        <trans-unit id="0401a90c4f50bc0cacd4ce11e3ddd76a2d543405" translate="yes" xml:space="preserve">
          <source>The class prior probabilities. By default, the class proportions are inferred from the training data.</source>
          <target state="translated">类先验概率。默认情况下,类的比例是由训练数据推断出来的。</target>
        </trans-unit>
        <trans-unit id="e9e493bffc66549584cbf656024fe268e745fcd4" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples.</source>
          <target state="translated">输入样本的类概率。</target>
        </trans-unit>
        <trans-unit id="6a7a001ab832443f9ea75be4af1e68e4b9eb028c" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of outputs is the same of that of the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt; attribute.</source>
          <target state="translated">输入样本的分类概率。输出的顺序与&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;属性的顺序相同。</target>
        </trans-unit>
        <trans-unit id="81478ef84d2d645ae9c4e632247f7f3bc9052a92" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of outputs is the same of that of the &lt;code&gt;classes_&lt;/code&gt; attribute.</source>
          <target state="translated">输入样本的分类概率。输出顺序与 &lt;code&gt;classes_&lt;/code&gt; 属性的顺序相同。</target>
        </trans-unit>
        <trans-unit id="eb02560f00e2596a11150c32ba96675012f41151" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;.</source>
          <target state="translated">输入样本的分类概率。类的顺序与属性&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_中&lt;/a&gt;的顺序相对应。</target>
        </trans-unit>
        <trans-unit id="8ea2403e3ddde25cdaa7213df112f25928f5232d" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;.</source>
          <target state="translated">输入样本的分类概率。类的顺序与属性 &lt;code&gt;classes_&lt;/code&gt; 中的顺序相对应。</target>
        </trans-unit>
        <trans-unit id="64b22dfb54911895a611d3b9cfbaf3a3f14d97f5" translate="yes" xml:space="preserve">
          <source>The class to report if &lt;code&gt;average='binary'&lt;/code&gt; and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting &lt;code&gt;labels=[pos_label]&lt;/code&gt; and &lt;code&gt;average != 'binary'&lt;/code&gt; will report scores for that label only.</source>
          <target state="translated">如果 &lt;code&gt;average='binary'&lt;/code&gt; 且数据为二进制的报告类。如果数据是多类或多标签的，则将被忽略；设置 &lt;code&gt;labels=[pos_label]&lt;/code&gt; 和 &lt;code&gt;average != 'binary'&lt;/code&gt; 将仅报告该标签的分数。</target>
        </trans-unit>
        <trans-unit id="b02b6e7e5cca0ff24c2691f6e7d2bc9c247d17d1" translate="yes" xml:space="preserve">
          <source>The class to use to build the returned adjacency matrix.</source>
          <target state="translated">用来构建返回的邻接矩阵的类。</target>
        </trans-unit>
        <trans-unit id="08bf02b9f7a917eed20c9c94c384791c0ed4578f" translate="yes" xml:space="preserve">
          <source>The class with respect to which we perform a one-vs-all fit. If None, then it is assumed that the given problem is binary.</source>
          <target state="translated">我们执行一比一拟合的类。如果无,则假设给定的问题是二进制的。</target>
        </trans-unit>
        <trans-unit id="26dc8660d1112d77620f09027ce7b9fbee6027a3" translate="yes" xml:space="preserve">
          <source>The classes &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; provide functionality to fit linear models for classification and regression using different (convex) loss functions and different penalties. E.g., with &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt;, &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; fits a logistic regression model, while with &lt;code&gt;loss=&quot;hinge&quot;&lt;/code&gt; it fits a linear support vector machine (SVM).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;类提供的功能可以使用不同（凸）损失函数和不同惩罚来拟合线性模型，以进行分类和回归。例如，对于 &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; ，&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;适合于逻辑回归模型，而对于 &lt;code&gt;loss=&quot;hinge&quot;&lt;/code&gt; 则适合于线性支持向量机（SVM）。</target>
        </trans-unit>
        <trans-unit id="d29e2c71b01d97c9c93dc5aad937ec38e91b7ca9" translate="yes" xml:space="preserve">
          <source>The classes &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; provide two criteria to stop the algorithm when a given level of convergence is reached:</source>
          <target state="translated">当达到给定的&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;，SGDClassifier和&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;类提供了两个准则来停止算法：</target>
        </trans-unit>
        <trans-unit id="d7ef8ada32aed500913b0cf053368e18ea306f33" translate="yes" xml:space="preserve">
          <source>The classes in &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; can handle either NumPy arrays or &lt;code&gt;scipy.sparse&lt;/code&gt; matrices as input. For dense matrices, a large number of possible distance metrics are supported. For sparse matrices, arbitrary Minkowski metrics are supported for searches.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; 中&lt;/a&gt;的类可以处理NumPy数组或 &lt;code&gt;scipy.sparse&lt;/code&gt; 矩阵作为输入。对于密集矩阵，支持大量可能的距离度量。对于稀疏矩阵，支持使用任意Minkowski度量进行搜索。</target>
        </trans-unit>
        <trans-unit id="f39d71815c55889dd46cb64855cc1d40f837d2d9" translate="yes" xml:space="preserve">
          <source>The classes in the &lt;a href=&quot;classes#module-sklearn.feature_selection&quot;&gt;&lt;code&gt;sklearn.feature_selection&lt;/code&gt;&lt;/a&gt; module can be used for feature selection/dimensionality reduction on sample sets, either to improve estimators&amp;rsquo; accuracy scores or to boost their performance on very high-dimensional datasets.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.feature_selection&quot;&gt; &lt;code&gt;sklearn.feature_selection&lt;/code&gt; &lt;/a&gt;模块中的类可用于样本集的特征选择/ 降维，以提高估计量的准确性得分或提高其在超高维数据集上的性能。</target>
        </trans-unit>
        <trans-unit id="df672af473b055efb339763c1ea56e27edeec8c9" translate="yes" xml:space="preserve">
          <source>The classes in this submodule allow to approximate the embedding \(\phi\), thereby working explicitly with the representations \(\phi(x_i)\), which obviates the need to apply the kernel or store training examples.</source>
          <target state="translated">这个子模块中的类允许近似于嵌入/(\\phi/),从而明确地与表征/(\phi(x_i)\)一起工作,这就避免了应用内核或存储训练实例的需要。</target>
        </trans-unit>
        <trans-unit id="41647bba96e968e0bd58965ae65f2bc0365d42c6" translate="yes" xml:space="preserve">
          <source>The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).</source>
          <target state="translated">类标签(单输出问题),或类标签的数组列表(多输出问题)。</target>
        </trans-unit>
        <trans-unit id="772daeb9b0eaa408b264b0fa548b1cbd24779805" translate="yes" xml:space="preserve">
          <source>The classes labels.</source>
          <target state="translated">各类标签。</target>
        </trans-unit>
        <trans-unit id="13f18ce429d97546a8b16cf132e8e6c03147cb50" translate="yes" xml:space="preserve">
          <source>The classic implementation of the clustering method based on the Lloyd&amp;rsquo;s algorithm. It consumes the whole set of input data at each iteration.</source>
          <target state="translated">基于劳埃德算法的聚类方法的经典实现。它在每次迭代时消耗整个输入数据集。</target>
        </trans-unit>
        <trans-unit id="b3e4f700832d6cdb3a215961499bbf68e1aa40b8" translate="yes" xml:space="preserve">
          <source>The classification is performed by projecting to the first two principal components found by PCA and CCA for visualisation purposes, followed by using the &lt;a href=&quot;../../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; metaclassifier using two SVCs with linear kernels to learn a discriminative model for each class. Note that PCA is used to perform an unsupervised dimensionality reduction, while CCA is used to perform a supervised one.</source>
          <target state="translated">通过将PCA和CCA找到的前两个主要组件投影以进行可视化，然后通过使用&lt;a href=&quot;../../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; &lt;/a&gt;元分类器（使用两个带有线性内核的SVC来学习每个类的判别模型）来执行分类。请注意，PCA用于执行无监督的降维，而CCA用于执行无监督的降维。</target>
        </trans-unit>
        <trans-unit id="4b0c76ea0ae326d19f535ab9520b9dd024cf9124" translate="yes" xml:space="preserve">
          <source>The classification is performed by projecting to the first two principal components found by PCA and CCA for visualisation purposes, followed by using the &lt;a href=&quot;../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; metaclassifier using two SVCs with linear kernels to learn a discriminative model for each class. Note that PCA is used to perform an unsupervised dimensionality reduction, while CCA is used to perform a supervised one.</source>
          <target state="translated">通过将PCA和CCA发现的前两个主要组件投影以进行可视化，然后通过使用&lt;a href=&quot;../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; &lt;/a&gt;元分类器（使用两个带有线性内核的SVC来学习每个类的判别模型）来执行分类。请注意，PCA用于执行无监督的降维，而CCA用于执行无监督的降维。</target>
        </trans-unit>
        <trans-unit id="3f7b4b923ee0ab9dbf257d91692081f3d6b6945f" translate="yes" xml:space="preserve">
          <source>The classification target. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;target&lt;/code&gt; will be a pandas Series.</source>
          <target state="translated">分类目标。如果 &lt;code&gt;as_frame=True&lt;/code&gt; ， &lt;code&gt;target&lt;/code&gt; 将是熊猫系列。</target>
        </trans-unit>
        <trans-unit id="7c83c4f2209684d200eb3e2732a4357005bab63e" translate="yes" xml:space="preserve">
          <source>The classifier which predicts given the output of &lt;code&gt;estimators_&lt;/code&gt;.</source>
          <target state="translated">预测给定 &lt;code&gt;estimators_&lt;/code&gt; 的输出的分类器。</target>
        </trans-unit>
        <trans-unit id="d9bcc8debd316bfd1c4b9e73630f6ba7215b9e3d" translate="yes" xml:space="preserve">
          <source>The classifier whose output decision function needs to be calibrated to offer more accurate predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.</source>
          <target state="translated">需要校准输出决定函数的分类器,以提供更精确的predict_proba输出。如果cv=prefit,那么分类器必须已经在数据上进行了拟合。</target>
        </trans-unit>
        <trans-unit id="4188695350bfd2d35a500d1eee9968d78806da89" translate="yes" xml:space="preserve">
          <source>The classifier whose output need to be calibrated to provide more accurate &lt;code&gt;predict_proba&lt;/code&gt; outputs.</source>
          <target state="translated">需要对其输出进行校准以提供更准确的 &lt;code&gt;predict_proba&lt;/code&gt; 输出的分类器。</target>
        </trans-unit>
        <trans-unit id="917e6ef2892859cfeed06565c1a2e19769195da1" translate="yes" xml:space="preserve">
          <source>The cluster ordered list of sample indices.</source>
          <target state="translated">样本指数的集群排序列表。</target>
        </trans-unit>
        <trans-unit id="88e39a3dfd7087282361f3b39109354f7ff32ede" translate="yes" xml:space="preserve">
          <source>The code below also illustrates how the construction and the computation of the predictions can be parallelized within multiple jobs.</source>
          <target state="translated">下面的代码还说明了如何在多个作业中并行计算预测的构造和计算。</target>
        </trans-unit>
        <trans-unit id="6a1cc4b5491e9b42fd445850086762fcb248e836" translate="yes" xml:space="preserve">
          <source>The code below plots the dependency of y against individual x_i and normalized values of univariate F-tests statistics and mutual information.</source>
          <target state="translated">下面的代码绘制了y对各个x_i的依赖性,以及单变量F检验统计和相互信息的标准化值。</target>
        </trans-unit>
        <trans-unit id="4b0a65eb61a5277b5e28aef5c3b0e7ea577bdc07" translate="yes" xml:space="preserve">
          <source>The code-examples in the above tutorials are written in a &lt;em&gt;python-console&lt;/em&gt; format. If you wish to easily execute these examples in &lt;strong&gt;IPython&lt;/strong&gt;, use:</source>
          <target state="translated">以上教程中的代码示例均以&lt;em&gt;python-console&lt;/em&gt;格式编写。如果希望在&lt;strong&gt;IPython中&lt;/strong&gt;轻松执行这些示例，请使用：</target>
        </trans-unit>
        <trans-unit id="ce298b790cf6df954f5ce5058b04debe04deb707" translate="yes" xml:space="preserve">
          <source>The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the regression sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">系数R^2定义为(1-u/v),其中u为残差平方和((y_true-y_pred)**2).sum(),v为回归平方和((y_true-y_true.mean())**2).sum()。最佳可能的分数是1.0,而且可以是负数(因为模型可以任意变坏)。一个恒定的模型,总是预测y的期望值,不考虑输入特征,将得到0.0的R^2分数。</target>
        </trans-unit>
        <trans-unit id="48121e6567bc65fc6ea38acd7d461ac73e2ea472" translate="yes" xml:space="preserve">
          <source>The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">系数R^2定义为(1-u/v),其中u为残差平方和((y_true-y_pred)**2).sum(),v为总平方和((y_true-y_true.mean())**2).sum()。可能的最佳得分是1.0,它可以是负值(因为模型可以任意变坏)。一个总是预测y的期望值的常量模型,不考虑输入特征,将得到0.0的R^2分数。</target>
        </trans-unit>
        <trans-unit id="a7b8f1c4eb5f60ca5d7c57d0dcd2ad470def6e62" translate="yes" xml:space="preserve">
          <source>The coefficient estimates for Ordinary Least Squares rely on the independence of the features. When features are correlated and the columns of the design matrix \(X\) have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed target, producing a large variance. This situation of &lt;em&gt;multicollinearity&lt;/em&gt; can arise, for example, when data are collected without an experimental design.</source>
          <target state="translated">普通最小二乘的系数估计取决于特征的独立性。当特征相关并且设计矩阵\（X \）的列具有近似线性相关性时，设计矩阵变得接近奇异，结果，最小二乘估计对观察到的目标中的随机误差变得高度敏感，产生很大的差异。例如，当在没有实验设计的情况下收集数据时，可能会出现&lt;em&gt;多重共线性的&lt;/em&gt;情况。</target>
        </trans-unit>
        <trans-unit id="f0dabbd241b37afcd99edc97579580c22f2703f9" translate="yes" xml:space="preserve">
          <source>The coefficient of the underlying linear model. It is returned only if coef is True.</source>
          <target state="translated">基础线性模型的系数。只有当coef为真时才会返回。</target>
        </trans-unit>
        <trans-unit id="8b0531322a0447d32da022aa29f51bdb5853bf2b" translate="yes" xml:space="preserve">
          <source>The coefficients \(w\) of the model can be accessed:</source>
          <target state="translated">可以访问该模型的系数(w)。</target>
        </trans-unit>
        <trans-unit id="8bf2184b15d694a8d6ef65832d1a84477f9388bd" translate="yes" xml:space="preserve">
          <source>The coefficients are significantly different. AGE and EXPERIENCE coefficients are both positive but they now have less influence on the prediction.</source>
          <target state="translated">系数显著不同。年龄和经验系数均为正值,但现在对预测的影响较小。</target>
        </trans-unit>
        <trans-unit id="b2933da174c8fa332cc9ad841c0c25a1ab1996d2" translate="yes" xml:space="preserve">
          <source>The coefficients can be forced to be positive.</source>
          <target state="translated">可以强迫系数为正。</target>
        </trans-unit>
        <trans-unit id="3c89d0ba57b5481659a50cc73b873694c355fd9b" translate="yes" xml:space="preserve">
          <source>The coefficients of the linear model: &lt;code&gt;Y = X coef_ + Err&lt;/code&gt;</source>
          <target state="translated">线性模型的系数： &lt;code&gt;Y = X coef_ + Err&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="5395d49e7d69c07e1b2416a99bbd87627621ae2f" translate="yes" xml:space="preserve">
          <source>The coefficients, the residual sum of squares and the coefficient of determination are also calculated.</source>
          <target state="translated">同时计算系数、残差平方和和确定系数。</target>
        </trans-unit>
        <trans-unit id="a7757730afe97cb8593dea4757d82727872c7529" translate="yes" xml:space="preserve">
          <source>The coefficients, the residual sum of squares and the variance score are also calculated.</source>
          <target state="translated">同时计算系数、残差平方和和方差得分。</target>
        </trans-unit>
        <trans-unit id="f15e1f2c7fb96cd43c32056a6c407c9c1c570883" translate="yes" xml:space="preserve">
          <source>The collection of fitted base estimators.</source>
          <target state="translated">拟合基础估计器的集合。</target>
        </trans-unit>
        <trans-unit id="4d0721afe1a467c5d2eb2443395ac7ab7a4e3f8b" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators as defined in &lt;code&gt;estimators&lt;/code&gt; that are not &amp;lsquo;drop&amp;rsquo;.</source>
          <target state="translated">在定义装配子估计的集合 &lt;code&gt;estimators&lt;/code&gt; 不属于&amp;ldquo;降&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="e97d4f2642ef3372648a38fef3350e771647c3d6" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators as defined in &lt;code&gt;estimators&lt;/code&gt; that are not &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">装子估计的集合中定义的 &lt;code&gt;estimators&lt;/code&gt; 是不 &lt;code&gt;None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0b6e390487ac628e8895818624f2b32fd0c95d36" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators.</source>
          <target state="translated">合适的子估计器的集合。</target>
        </trans-unit>
        <trans-unit id="929cef17b33a8aadbb4cff8f87d4813d9e794dca" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators. &lt;code&gt;loss_.K&lt;/code&gt; is 1 for binary classification, otherwise n_classes.</source>
          <target state="translated">拟合子估计量的集合。对于二进制分类， &lt;code&gt;loss_.K&lt;/code&gt; 为1，否则为n_classes。</target>
        </trans-unit>
        <trans-unit id="34213cac4f6d6096a4a72655d4c46d8da28f907c" translate="yes" xml:space="preserve">
          <source>The collection of fitted transformers as tuples of (name, fitted_transformer, column). &lt;code&gt;fitted_transformer&lt;/code&gt; can be an estimator, &amp;lsquo;drop&amp;rsquo;, or &amp;lsquo;passthrough&amp;rsquo;. In case there were no columns selected, this will be the unfitted transformer. If there are remaining columns, the final element is a tuple of the form: (&amp;lsquo;remainder&amp;rsquo;, transformer, remaining_columns) corresponding to the &lt;code&gt;remainder&lt;/code&gt; parameter. If there are remaining columns, then &lt;code&gt;len(transformers_)==len(transformers)+1&lt;/code&gt;, otherwise &lt;code&gt;len(transformers_)==len(transformers)&lt;/code&gt;.</source>
          <target state="translated">拟合变压器的集合，作为（名称，fitt_transformer，列）的元组。 &lt;code&gt;fitted_transformer&lt;/code&gt; 可以是一个估算器，&amp;ldquo; drop&amp;rdquo;或&amp;ldquo; passthrough&amp;rdquo;。如果未选择任何列，则为未安装的变压器。如果还有剩余的列，则最后一个元素为以下形式的元组：（&amp;ldquo; remainder&amp;rdquo;，translator，remaining_columns），与 &lt;code&gt;remainder&lt;/code&gt; 参数相对应。如果还有剩余的列，则 &lt;code&gt;len(transformers_)==len(transformers)+1&lt;/code&gt; ，否则 &lt;code&gt;len(transformers_)==len(transformers)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="3626dc1999e74681fe5d5d9d41be14f043b3b76d" translate="yes" xml:space="preserve">
          <source>The collection of patches extracted from the image, where &lt;code&gt;n_patches&lt;/code&gt; is either &lt;code&gt;max_patches&lt;/code&gt; or the total number of patches that can be extracted.</source>
          <target state="translated">从图像中提取的补丁的集合，其中 &lt;code&gt;n_patches&lt;/code&gt; 是 &lt;code&gt;max_patches&lt;/code&gt; 或可以提取的补丁的总数。</target>
        </trans-unit>
        <trans-unit id="cd0348cc380532b01c9102e25a743ff6f1c3fc49" translate="yes" xml:space="preserve">
          <source>The collection of patches extracted from the images, where &lt;code&gt;n_patches&lt;/code&gt; is either &lt;code&gt;n_samples * max_patches&lt;/code&gt; or the total number of patches that can be extracted.</source>
          <target state="translated">从图像中提取的补丁的集合，其中 &lt;code&gt;n_patches&lt;/code&gt; 是 &lt;code&gt;n_samples * max_patches&lt;/code&gt; 或可以提取的补丁的总数。</target>
        </trans-unit>
        <trans-unit id="d6d471d12278a874c4225d83bbd25d7f04a0a976" translate="yes" xml:space="preserve">
          <source>The color map illustrates the decision function learned by the SVC.</source>
          <target state="translated">颜色图说明了SVC学习的决策函数。</target>
        </trans-unit>
        <trans-unit id="093f5986f8d055d5b47f11ad021ce6ecfcef91e9" translate="yes" xml:space="preserve">
          <source>The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.</source>
          <target state="translated">从指标[n_nodes_ptr[i]:n_nodes_ptr[i+1]]中列举出第i个估计器的指标值。</target>
        </trans-unit>
        <trans-unit id="f0cca3e664aa52c0e892615c12365c0a6af40452" translate="yes" xml:space="preserve">
          <source>The combination used in this example is not particularly helpful on this dataset and is only used to illustrate the usage of FeatureUnion.</source>
          <target state="translated">这个例子中使用的组合在这个数据集上并没有特别大的帮助,只是用来说明FeatureUnion的用法。</target>
        </trans-unit>
        <trans-unit id="238b85659a6f96b691291ac49250c2a48af41530" translate="yes" xml:space="preserve">
          <source>The complete set of patches. If the patches contain colour information, channels are indexed along the last dimension: RGB patches would have &lt;code&gt;n_channels=3&lt;/code&gt;.</source>
          <target state="translated">完整的补丁集。如果色块包含颜色信息，则会沿最后一个维度对通道进行索引：RGB色块的 &lt;code&gt;n_channels=3&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b4849a6d39c196dd952c4e98da8a227fce685751" translate="yes" xml:space="preserve">
          <source>The complexity parameter \(\alpha \geq 0\) controls the amount of shrinkage: the larger the value of \(\alpha\), the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.</source>
          <target state="translated">复杂度参数/(alpha \geq 0)/()控制着收缩量:/(alpha \geq)值越大,收缩量越大,因此系数对一致性的影响也越大。</target>
        </trans-unit>
        <trans-unit id="4524a6f67fb21e2d5fb712c219044b7460dbad0e" translate="yes" xml:space="preserve">
          <source>The components of the random matrix are drawn from N(0, 1 / n_components).</source>
          <target state="translated">随机矩阵的分量从N(0,1/n_components)中抽取。</target>
        </trans-unit>
        <trans-unit id="4d179dd17ec2e9e8ac79ee8fb26a8b8727033655" translate="yes" xml:space="preserve">
          <source>The compromise between l1 and l2 penalization chosen by cross validation</source>
          <target state="translated">通过交叉验证选择的l1和l2之间的折衷惩罚方法。</target>
        </trans-unit>
        <trans-unit id="37c8e8aa42af430070b2b87be4c5db5db294cc7b" translate="yes" xml:space="preserve">
          <source>The computation during &lt;code&gt;fit&lt;/code&gt; is:</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 期间的计算为：</target>
        </trans-unit>
        <trans-unit id="cc728704a5b1deaa221b61ada3dd42ad8e661fbd" translate="yes" xml:space="preserve">
          <source>The computation during &lt;code&gt;predict&lt;/code&gt; is:</source>
          <target state="translated">&lt;code&gt;predict&lt;/code&gt; 期间的计算为：</target>
        </trans-unit>
        <trans-unit id="cc5659751ed96e6a9a511b247760eba7bd9fab63" translate="yes" xml:space="preserve">
          <source>The computation of Davies-Bouldin is simpler than that of Silhouette scores.</source>
          <target state="translated">Davies-Bouldin的计算比Silhouette分数的计算要简单。</target>
        </trans-unit>
        <trans-unit id="d526bd934b93818cdf8a3fccada8e6b2b34d84f2" translate="yes" xml:space="preserve">
          <source>The computational overhead of each SVD is &lt;code&gt;O(batch_size * n_features ** 2)&lt;/code&gt;, but only 2 * batch_size samples remain in memory at a time. There will be &lt;code&gt;n_samples / batch_size&lt;/code&gt; SVD computations to get the principal components, versus 1 large SVD of complexity &lt;code&gt;O(n_samples * n_features ** 2)&lt;/code&gt; for PCA.</source>
          <target state="translated">每个SVD的计算开销为 &lt;code&gt;O(batch_size * n_features ** 2)&lt;/code&gt; ，但一次仅2 * batch_size个样本保留在内存中。将使用 &lt;code&gt;n_samples / batch_size&lt;/code&gt; SVD计算来获取主成分，而PCA则需要1个大型SVD，复杂度为 &lt;code&gt;O(n_samples * n_features ** 2)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2fa163515de812b4d78a2bbcef87e239a36ae4d1" translate="yes" xml:space="preserve">
          <source>The concept of early stopping is simple. We specify a &lt;code&gt;validation_fraction&lt;/code&gt; which denotes the fraction of the whole dataset that will be kept aside from training to assess the validation loss of the model. The gradient boosting model is trained using the training set and evaluated using the validation set. When each additional stage of regression tree is added, the validation set is used to score the model. This is continued until the scores of the model in the last &lt;code&gt;n_iter_no_change&lt;/code&gt; stages do not improve by atleast &lt;code&gt;tol&lt;/code&gt;. After that the model is considered to have converged and further addition of stages is &amp;ldquo;stopped early&amp;rdquo;.</source>
          <target state="translated">提前停止的概念很简单。我们指定一个 &lt;code&gt;validation_fraction&lt;/code&gt; ，它表示整个数据集的一部分，该部分将不进行训练以评估模型的验证损失。使用训练集训练梯度提升模型，并使用验证集进行评估。添加回归树的每个其他阶段时，将使用验证集对模型评分。这一直持续到模型中最后的得分 &lt;code&gt;n_iter_no_change&lt;/code&gt; 阶段不要ATLEAST提高 &lt;code&gt;tol&lt;/code&gt; 。此后，该模型被认为已经收敛，并且&amp;ldquo;早日停止&amp;rdquo;进一步添加阶段。</target>
        </trans-unit>
        <trans-unit id="feb60f640c62fd0856cb50e8401267d4f55d4774" translate="yes" xml:space="preserve">
          <source>The concrete &lt;code&gt;LossFunction&lt;/code&gt; object.</source>
          <target state="translated">具体的 &lt;code&gt;LossFunction&lt;/code&gt; 对象。</target>
        </trans-unit>
        <trans-unit id="4f21e1340272b60cf06a69153cc19d0140d625cf" translate="yes" xml:space="preserve">
          <source>The concrete loss function can be set via the &lt;code&gt;loss&lt;/code&gt; parameter. &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; supports the following loss functions:</source>
          <target state="translated">可以通过 &lt;code&gt;loss&lt;/code&gt; 参数设置具体的损失函数。&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;支持以下损失函数：</target>
        </trans-unit>
        <trans-unit id="c24a50928ec0729629e45e8b53b6d8b9f0112f79" translate="yes" xml:space="preserve">
          <source>The concrete loss function can be set via the &lt;code&gt;loss&lt;/code&gt; parameter. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; supports the following loss functions:</source>
          <target state="translated">可以通过 &lt;code&gt;loss&lt;/code&gt; 参数设置具体的损失函数。&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;支持以下损失函数：</target>
        </trans-unit>
        <trans-unit id="a5b98bb304bc3ec6a422167859a2f5da5580fa47" translate="yes" xml:space="preserve">
          <source>The concrete penalty can be set via the &lt;code&gt;penalty&lt;/code&gt; parameter. SGD supports the following penalties:</source>
          <target state="translated">可以通过 &lt;code&gt;penalty&lt;/code&gt; 参数设置具体的惩罚。SGD支持以下处罚：</target>
        </trans-unit>
        <trans-unit id="c0c66ccf788e88932593e02308e1dcae0bb5c1f0" translate="yes" xml:space="preserve">
          <source>The conditional probability distribution of each unit is given by the logistic sigmoid activation function of the input it receives:</source>
          <target state="translated">每个单元的条件概率分布由它所接受的输入的对数sigmoid激活函数给出。</target>
        </trans-unit>
        <trans-unit id="6e94f7f28c2cc9bab8c93a85a4438b802d82d10d" translate="yes" xml:space="preserve">
          <source>The confidence score for a sample is the signed distance of that sample to the hyperplane.</source>
          <target state="translated">一个样本的置信度分数是该样本到超平面的符号距离。</target>
        </trans-unit>
        <trans-unit id="b129f98741f30e6ae05e0872d86b02b5a921e905" translate="yes" xml:space="preserve">
          <source>The connectivity constraints are imposed via an connectivity matrix: a scipy sparse matrix that has elements only at the intersection of a row and a column with indices of the dataset that should be connected. This matrix can be constructed from a-priori information: for instance, you may wish to cluster web pages by only merging pages with a link pointing from one to another. It can also be learned from the data, for instance using &lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt;&lt;/a&gt; to restrict merging to nearest neighbors as in &lt;a href=&quot;../auto_examples/cluster/plot_agglomerative_clustering#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py&quot;&gt;this example&lt;/a&gt;, or using &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt;&lt;code&gt;sklearn.feature_extraction.image.grid_to_graph&lt;/code&gt;&lt;/a&gt; to enable only merging of neighboring pixels on an image, as in the &lt;a href=&quot;../auto_examples/cluster/plot_coin_ward_segmentation#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py&quot;&gt;coin&lt;/a&gt; example.</source>
          <target state="translated">连通性约束是通过连通性矩阵施加的：稀疏矩阵，其稀疏矩阵仅在具有应连接数据集索引的行和列的交点处具有元素。该矩阵可以由先验信息构成：例如，您可能希望仅通过合并页面与指向彼此的链接来对网页进行聚类。它也可以从数据了解到，例如使用&lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt; &lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt; &lt;/a&gt;限制合并到最近的邻居，如&lt;a href=&quot;../auto_examples/cluster/plot_agglomerative_clustering#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py&quot;&gt;本实施例中&lt;/a&gt;，或者使用&lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt; &lt;code&gt;sklearn.feature_extraction.image.grid_to_graph&lt;/code&gt; &lt;/a&gt;只启用相邻像素的图像上合并，如在在&lt;a href=&quot;../auto_examples/cluster/plot_coin_ward_segmentation#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py&quot;&gt;硬币&lt;/a&gt;的例子。</target>
        </trans-unit>
        <trans-unit id="cba08514635a6da99fcbf046720afd85a65680ea" translate="yes" xml:space="preserve">
          <source>The constant value which defines the covariance: k(x_1, x_2) = constant_value</source>
          <target state="translated">定义协方差的常数:k(x_1,x_2)=constant_value。</target>
        </trans-unit>
        <trans-unit id="83836055dd86bf18d0180179c00f49511034950c" translate="yes" xml:space="preserve">
          <source>The contingency table calculated is typically utilized in the calculation of a similarity statistic (like the others listed in this document) between the two clusterings.</source>
          <target state="translated">计算出的应急表通常被利用在计算两个聚类之间的相似度统计量(就像本文件中列出的其他统计量一样)。</target>
        </trans-unit>
        <trans-unit id="ca0505a0687635a00a190438e7d26956e48c468c" translate="yes" xml:space="preserve">
          <source>The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.</source>
          <target state="translated">收敛阈值。当下限平均增益低于此阈值时,EM迭代将停止。</target>
        </trans-unit>
        <trans-unit id="0e26ef32f94bda64e8f6e396c3c303a3311ac4e1" translate="yes" xml:space="preserve">
          <source>The convergence threshold. EM iterations will stop when the lower bound average gain on the likelihood (of the training data with respect to the model) is below this threshold.</source>
          <target state="translated">收敛阈值。当(训练数据相对于模型的)似然的下限平均增益低于这个阈值时,EM迭代将停止。</target>
        </trans-unit>
        <trans-unit id="80f05388942910150044c6d47584d8bddc73ef4c" translate="yes" xml:space="preserve">
          <source>The converse mapping from feature name to column index is stored in the &lt;code&gt;vocabulary_&lt;/code&gt; attribute of the vectorizer:</source>
          <target state="translated">从要素名称到列索引的相反映射存储在矢量化器的 &lt;code&gt;vocabulary_&lt;/code&gt; 属性中：</target>
        </trans-unit>
        <trans-unit id="42e30f7491cd0bbd1be9eaa1008d97761b319a5f" translate="yes" xml:space="preserve">
          <source>The converted and validated X.</source>
          <target state="translated">经过转换和验证的X。</target>
        </trans-unit>
        <trans-unit id="f4a00aeed47e5a0cd669edcdec893bcad4bc87b6" translate="yes" xml:space="preserve">
          <source>The converted and validated array.</source>
          <target state="translated">转换和验证的数组。</target>
        </trans-unit>
        <trans-unit id="043f2ec629cc510b9c1127fe6b10b6fe4448d241" translate="yes" xml:space="preserve">
          <source>The converted and validated y.</source>
          <target state="translated">经过转换和验证的y。</target>
        </trans-unit>
        <trans-unit id="81a7bc326e697c66d8802c043c85fabeabbf241b" translate="yes" xml:space="preserve">
          <source>The converted dataname.</source>
          <target state="translated">转换后的数据名。</target>
        </trans-unit>
        <trans-unit id="223ada656bcdd3fee604b90acc8ae4baf52723e0" translate="yes" xml:space="preserve">
          <source>The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is</source>
          <target state="translated">UCI ML乳腺癌威斯康星州(诊断)数据集的拷贝是</target>
        </trans-unit>
        <trans-unit id="44e5d228f37bd8405cfefcfa348ce3d27d939cb4" translate="yes" xml:space="preserve">
          <source>The copy of UCI ML Wine Data Set dataset is downloaded and modified to fit</source>
          <target state="translated">下载UCI ML Wine Data Set数据集的副本,并进行修改,使之适合于</target>
        </trans-unit>
        <trans-unit id="e11d4d7608fe1fa28c8a673b1389ac6181cb7877" translate="yes" xml:space="preserve">
          <source>The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights \(w_1\), \(w_2\), &amp;hellip;, \(w_N\) to each of the training samples. Initially, those weights are all set to \(w_i = 1/N\), so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence &lt;a href=&quot;#htf&quot; id=&quot;id10&quot;&gt;[HTF]&lt;/a&gt;.</source>
          <target state="translated">AdaBoost的核心原理是在重复修改的数据版本上适合一系列弱学习者（即，仅比随机猜测稍好一些的模型，例如小决策树）。然后，通过加权多数表决（或总和）将来自所有预测的预测进行组合以产生最终预测。在每个所谓的增强迭代中，数据修改包括对每个训练样本应用权重\（w_1 \），\（w_2 \），&amp;hellip;，\（w_N \）。最初，这些权重都设置为\（w_i = 1 / N \），因此第一步只是简单地在原始数据上训练弱学习者。对于每个连续的迭代，将分别修改样本权重，并将学习算法重新应用于重新加权的数据。在给定的步骤中在上一步中由增强模型错误预测的那些训练示例的权重增加了，而对于正确预测的那些权重则减小了。随着迭代的进行，难以预测的示例受到越来越多的影响。因此，每个随后的弱学习者都被迫专注于序列中先前的学习者遗漏的例子&lt;a href=&quot;#htf&quot; id=&quot;id10&quot;&gt;[HTF]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="2a94b871b5efce504e5077120d25fb83e224b7ee" translate="yes" xml:space="preserve">
          <source>The corpus is a collection of \(D\) documents.</source>
          <target state="translated">语料库是一个集合的文件(D)。</target>
        </trans-unit>
        <trans-unit id="81757ae28a30627b83ded3cb76d108f655541caf" translate="yes" xml:space="preserve">
          <source>The correlation between each regressor and the target is computed, that is, ((X[:, i] - mean(X[:, i])) * (y - mean_y)) / (std(X[:, i]) * std(y)).</source>
          <target state="translated">计算各回归因子与目标之间的相关性,即((X[:,i]-均值(X[:,i]))*(y-mean_y))/(std(X[:,i])*std(y))。</target>
        </trans-unit>
        <trans-unit id="302d596f93db30183d7b9abc1999d9cfb95f2d1f" translate="yes" xml:space="preserve">
          <source>The corresponding image is:</source>
          <target state="translated">对应的形象是:</target>
        </trans-unit>
        <trans-unit id="9a9313289c48f29b6d60eb0302d43d099de5f93d" translate="yes" xml:space="preserve">
          <source>The corresponding training labels.</source>
          <target state="translated">相应的训练标签。</target>
        </trans-unit>
        <trans-unit id="8f6395cc147644bb3051e08a46acbc9cd47145af" translate="yes" xml:space="preserve">
          <source>The cosine distance is defined as &lt;code&gt;1 - cosine_similarity&lt;/code&gt;: the lowest value is 0 (identical point) but it is bounded above by 2 for the farthest points. Its value does not depend on the norm of the vector points but only on their relative angles.</source>
          <target state="translated">余弦距离定义为 &lt;code&gt;1 - cosine_similarity&lt;/code&gt; ：最小值为0（相同点），但最远点的界限为2以上。它的值不取决于矢量点的范数，而仅取决于它们的相对角度。</target>
        </trans-unit>
        <trans-unit id="c4b95b481d689de7dde58522ad60a16deab9f841" translate="yes" xml:space="preserve">
          <source>The cosine distance is equivalent to the half the squared euclidean distance if each sample is normalized to unit norm</source>
          <target state="translated">如果每个样本都归一化为单位正态,则余弦距离相当于欧几里得距离平方的一半。</target>
        </trans-unit>
        <trans-unit id="59fadedc84e4bf70f9182651c0ff609a02cebdec" translate="yes" xml:space="preserve">
          <source>The cost complexity measure of a single node is \(R_\alpha(t)=R(t)+\alpha\). The branch, \(T_t\), is defined to be a tree where node \(t\) is its root. In general, the impurity of a node is greater than the sum of impurities of its terminal nodes, \(R(T_t)&amp;lt;R(t)\). However, the cost complexity measure of a node, \(t\), and its branch, \(T_t\), can be equal depending on \(\alpha\). We define the effective \(\alpha\) of a node to be the value where they are equal, \(R_\alpha(T_t)=R_\alpha(t)\) or \(\alpha_{eff}(t)=\frac{R(t)-R(T_t)}{|T|-1}\). A non-terminal node with the smallest value of \(\alpha_{eff}\) is the weakest link and will be pruned. This process stops when the pruned tree&amp;rsquo;s minimal \(\alpha_{eff}\) is greater than the &lt;code&gt;ccp_alpha&lt;/code&gt; parameter.</source>
          <target state="translated">单个节点的成本复杂度度量为\（R_ \ alpha（t）= R（t）+ \ alpha \）。分支\（T_t \）被定义为一棵树，其中节点\（t \）是其根。通常，节点的杂质大于其末端节点的杂质总和\（R（T_t）&amp;lt;R（t）\）。但是，节点\（t \）及其分支\（T_t \）的成本复杂性度量可以取决于\（\ alpha \）相等。我们将节点的有效\（\ alpha \）定义为其相等的值，即\（R_ \ alpha（T_t）= R_ \ alpha（t）\）或\（\ alpha_ {eff}（t） = \ frac {R（t）-R（T_t）} {| T | -1} \）。值为\（\ alpha_ {eff} \）最小的非终端节点是最弱的链接，将被修剪。当修剪的树的最小\（\ alpha_ {eff} \）大于 &lt;code&gt;ccp_alpha&lt;/code&gt; 参数时，此过程停止。</target>
        </trans-unit>
        <trans-unit id="de99d6d126cf03f253c85bcda8fea4fdd47045eb" translate="yes" xml:space="preserve">
          <source>The cost function of an isomap embedding is</source>
          <target state="translated">同构图嵌入的成本函数为</target>
        </trans-unit>
        <trans-unit id="40200d49debb9a752670d9f4e91dc77cd9d66f0a" translate="yes" xml:space="preserve">
          <source>The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.</source>
          <target state="translated">使用该树(即预测数据)的成本是对数,即用于训练该树的数据点数量。</target>
        </trans-unit>
        <trans-unit id="c9b6813659bffb0c08aeb974fcecdaaf97e786b0" translate="yes" xml:space="preserve">
          <source>The covariance matrix of a data set is known to be well approximated by the classical &lt;em&gt;maximum likelihood estimator&lt;/em&gt; (or &amp;ldquo;empirical covariance&amp;rdquo;), provided the number of observations is large enough compared to the number of features (the variables describing the observations). More precisely, the Maximum Likelihood Estimator of a sample is an unbiased estimator of the corresponding population&amp;rsquo;s covariance matrix.</source>
          <target state="translated">已知数据集的协方差矩阵可以通过经典&lt;em&gt;最大似然估计器&lt;/em&gt;（或&amp;ldquo;经验协方差&amp;rdquo;）很好地近似，前提是观测的数量要比特征的数量（描述观测的变量）大得多。更准确地说，样本的最大似然估计量是相应总体的协方差矩阵的无偏估计量。</target>
        </trans-unit>
        <trans-unit id="ff9747571dfcab155502f0d51c20ced0a4eda87c" translate="yes" xml:space="preserve">
          <source>The covariance matrix will be this value times the unit matrix. This dataset only produces symmetric normal distributions.</source>
          <target state="translated">协方差矩阵将是这个值乘以单位矩阵。这个数据集只产生对称正态分布。</target>
        </trans-unit>
        <trans-unit id="d4f25f1f739fb6455b136c8d6d48a91032e89970" translate="yes" xml:space="preserve">
          <source>The covariance of each mixture component. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">每个混合物成分的协方差。形状取决于 &lt;code&gt;covariance_type&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="37e0aa90ea88b46d75d1f0f3b55b29aae27815f8" translate="yes" xml:space="preserve">
          <source>The covariance to compare with.</source>
          <target state="translated">的协方差来比较。</target>
        </trans-unit>
        <trans-unit id="a8664fdb20de58ef24fb2b254f60a073e2dad9f4" translate="yes" xml:space="preserve">
          <source>The cross decomposition module contains two main families of algorithms: the partial least squares (PLS) and the canonical correlation analysis (CCA).</source>
          <target state="translated">交叉分解模块包含两个主要的算法系列:部分最小二乘法(PLS)和规范相关分析(CCA)。</target>
        </trans-unit>
        <trans-unit id="5725093c72b0120c682574584b056d0b295efdd9" translate="yes" xml:space="preserve">
          <source>The cross validation score obtained on the training data</source>
          <target state="translated">在训练数据上获得的交叉验证得分。</target>
        </trans-unit>
        <trans-unit id="7d9ee6d7f0b44964f013a45e604e6c29cc8a3f8f" translate="yes" xml:space="preserve">
          <source>The cross-validation can then be performed easily:</source>
          <target state="translated">然后可以很容易地进行交叉验证。</target>
        </trans-unit>
        <trans-unit id="58fa7e854fcd94e774f9d9c000c917c76d680086" translate="yes" xml:space="preserve">
          <source>The cross-validation involved in Platt scaling is an expensive operation for large datasets. In addition, the probability estimates may be inconsistent with the scores:</source>
          <target state="translated">Platt缩放中涉及的交叉验证对于大型数据集来说是一个昂贵的操作。此外,概率估计可能与分数不一致。</target>
        </trans-unit>
        <trans-unit id="b91dc81955b1dbc1a750c7c0ae1e516f1cb5789e" translate="yes" xml:space="preserve">
          <source>The cross-validation score can be directly calculated using the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; helper. Given an estimator, the cross-validation object and the input dataset, the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; splits the data repeatedly into a training and a testing set, trains the estimator using the training set and computes the scores based on the testing set for each iteration of cross-validation.</source>
          <target state="translated">可以使用&lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;帮助器直接计算交叉验证得分。给定一个估算器，交叉验证对象和输入数据集，&lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;将数据重复拆分为一个训练集和一个测试集，使用训练集训练该估算器，并根据测试集计算交叉算子的每次迭代的得分验证。</target>
        </trans-unit>
        <trans-unit id="0c9e67494c034c3aceceb77302443d5c2bb54301" translate="yes" xml:space="preserve">
          <source>The cross-validation scores such that &lt;code&gt;grid_scores_[i]&lt;/code&gt; corresponds to the CV score of the i-th subset of features.</source>
          <target state="translated">交叉验证得分，例如 &lt;code&gt;grid_scores_[i]&lt;/code&gt; 与第i个特征子集的CV得分相对应。</target>
        </trans-unit>
        <trans-unit id="7186db76e260a93ceff6e7a012a18028bce16f5b" translate="yes" xml:space="preserve">
          <source>The current implementation uses ball trees and kd-trees to determine the neighborhood of points, which avoids calculating the full distance matrix (as was done in scikit-learn versions before 0.14). The possibility to use custom metrics is retained; for details, see &lt;code&gt;NearestNeighbors&lt;/code&gt;.</source>
          <target state="translated">当前的实现使用球树和kd树来确定点的邻域，从而避免了计算完整距离矩阵（如在0.14之前的scikit-learn版本中所做的那样）。保留使用自定义指标的可能性；有关详细信息，请参见 &lt;code&gt;NearestNeighbors&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e3ec89890503565d6b8dae28b11789379133ea78" translate="yes" xml:space="preserve">
          <source>The current loss computed with the loss function.</source>
          <target state="translated">用损失函数计算出的当前损失。</target>
        </trans-unit>
        <trans-unit id="a415ef6f77731ce51df5a94c5015135ebd3f462c" translate="yes" xml:space="preserve">
          <source>The curse of dimensionality</source>
          <target state="translated">维度的诅咒</target>
        </trans-unit>
        <trans-unit id="3eaad00bc0bba0577db9b826b646317a24a90409" translate="yes" xml:space="preserve">
          <source>The data</source>
          <target state="translated">数据</target>
        </trans-unit>
        <trans-unit id="973622d96c176a0a77b0dde8f6175466c5afec2c" translate="yes" xml:space="preserve">
          <source>The data is always a 2D array, shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt;, although the original data may have had a different shape. In the case of the digits, each original sample is an image of shape &lt;code&gt;(8, 8)&lt;/code&gt; and can be accessed using:</source>
          <target state="translated">数据始终是2D数组，形状为 &lt;code&gt;(n_samples, n_features)&lt;/code&gt; ，尽管原始数据可能具有不同的形状。对于数字，每个原始样本都是形状为 &lt;code&gt;(8, 8)&lt;/code&gt; 的图像，可以使用以下方式访问：</target>
        </trans-unit>
        <trans-unit id="0e6af14908ee7619e613da95e5ef5e8434ef93d9" translate="yes" xml:space="preserve">
          <source>The data is assumed to be non-negative, and is often normalized to have an L1-norm of one. The normalization is rationalized with the connection to the chi squared distance, which is a distance between discrete probability distributions.</source>
          <target state="translated">假设数据是非负数,通常将其归一化,使其L1正态为1。归一化是合理化的,与chi平方距离有关,它是离散概率分布之间的距离。</target>
        </trans-unit>
        <trans-unit id="aa2161d468d436ce31e1b1a4a535d19d4acf7995" translate="yes" xml:space="preserve">
          <source>The data is generated with the &lt;code&gt;make_checkerboard&lt;/code&gt; function, then shuffled and passed to the Spectral Biclustering algorithm. The rows and columns of the shuffled matrix are rearranged to show the biclusters found by the algorithm.</source>
          <target state="translated">数据是使用 &lt;code&gt;make_checkerboard&lt;/code&gt; 函数生成的，然后经过混洗并传递给Spectral Biclustering算法。重新排列改组矩阵的行和列，以显示该算法找到的双曲线。</target>
        </trans-unit>
        <trans-unit id="cbdb41e13849fa39e43a0c0831b005495a0d0342" translate="yes" xml:space="preserve">
          <source>The data is split according to the cv parameter. Each sample belongs to exactly one test set, and its prediction is computed with an estimator fitted on the corresponding training set.</source>
          <target state="translated">根据cv参数对数据进行分割。每个样本恰好属于一个测试集,其预测结果是用一个拟合在相应训练集上的估计器来计算的。</target>
        </trans-unit>
        <trans-unit id="549dd87c3787b6b83f2456e4cff9d8aa392d12c1" translate="yes" xml:space="preserve">
          <source>The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine.</source>
          <target state="translated">这些数据是对意大利同一地区三个不同种植者种植的葡萄酒进行化学分析的结果。对这三种葡萄酒中的不同成分进行了13次不同的测量。</target>
        </trans-unit>
        <trans-unit id="92b1cf272aa9964f4bcd0a98099303474f1eb6db" translate="yes" xml:space="preserve">
          <source>The data list to learn.</source>
          <target state="translated">要学习的数据列表。</target>
        </trans-unit>
        <trans-unit id="279fbf7584e00e713412e8ff4546300536ffeb5a" translate="yes" xml:space="preserve">
          <source>The data matrix</source>
          <target state="translated">数据矩阵</target>
        </trans-unit>
        <trans-unit id="8a93d3171d08360f1c2b565ec2f68ceaa0cb40fb" translate="yes" xml:space="preserve">
          <source>The data matrix to learn.</source>
          <target state="translated">要学习的数据矩阵。</target>
        </trans-unit>
        <trans-unit id="a36ea8c8057a9ae699a69f099ee316c3295afaf3" translate="yes" xml:space="preserve">
          <source>The data matrix, with p features and n samples. The data set must be the one which was used to compute the raw estimates.</source>
          <target state="translated">数据矩阵,有p个特征和n个样本。数据集必须是用于计算原始估计的数据。</target>
        </trans-unit>
        <trans-unit id="f0ed1480aeed96966c83d245c0e82839723677ea" translate="yes" xml:space="preserve">
          <source>The data matrix.</source>
          <target state="translated">数据矩阵。</target>
        </trans-unit>
        <trans-unit id="3e01104c886db328397aa7550432fcf04c711803" translate="yes" xml:space="preserve">
          <source>The data matrix. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;data&lt;/code&gt; will be a pandas DataFrame.</source>
          <target state="translated">数据矩阵。如果 &lt;code&gt;as_frame=True&lt;/code&gt; ，则 &lt;code&gt;data&lt;/code&gt; 将为pandas DataFrame。</target>
        </trans-unit>
        <trans-unit id="70cee63c42777050ee29bb5c41f5ed813382b6c9" translate="yes" xml:space="preserve">
          <source>The data of the returned sparse matrix. By default it is int</source>
          <target state="translated">返回的稀疏矩阵的数据。默认情况下是int</target>
        </trans-unit>
        <trans-unit id="13b8870b16632bb144f99a987da0a17b912fd261" translate="yes" xml:space="preserve">
          <source>The data of the returned sparse matrix. By default it is the dtype of img</source>
          <target state="translated">返回的稀疏矩阵的数据。默认情况下是img的dtype。</target>
        </trans-unit>
        <trans-unit id="a9c1f6d9c961581c21421066f6ce8f7c709084a9" translate="yes" xml:space="preserve">
          <source>The data on which &lt;code&gt;gbrt&lt;/code&gt; was trained.</source>
          <target state="translated">训练 &lt;code&gt;gbrt&lt;/code&gt; 的数据。</target>
        </trans-unit>
        <trans-unit id="c66d20ce5405cff4e02a00c321afd4016f52379a" translate="yes" xml:space="preserve">
          <source>The data on which &lt;code&gt;gbrt&lt;/code&gt; was trained. It is used to generate a &lt;code&gt;grid&lt;/code&gt; for the &lt;code&gt;target_variables&lt;/code&gt;. The &lt;code&gt;grid&lt;/code&gt; comprises &lt;code&gt;grid_resolution&lt;/code&gt; equally spaced points between the two &lt;code&gt;percentiles&lt;/code&gt;.</source>
          <target state="translated">训练 &lt;code&gt;gbrt&lt;/code&gt; 的数据。它用于为 &lt;code&gt;target_variables&lt;/code&gt; 生成 &lt;code&gt;grid&lt;/code&gt; 。所述 &lt;code&gt;grid&lt;/code&gt; 包括 &lt;code&gt;grid_resolution&lt;/code&gt; 等距隔开所述两个点之间 &lt;code&gt;percentiles&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2ffaf51af185adce5e53447e39433481435abf03" translate="yes" xml:space="preserve">
          <source>The data samples transformed.</source>
          <target state="translated">数据样本的转化。</target>
        </trans-unit>
        <trans-unit id="65c0e8f3b46fedbf2f41b7e1589997d8de30db77" translate="yes" xml:space="preserve">
          <source>The data set contains images of hand-written digits: 10 classes where each class refers to a digit.</source>
          <target state="translated">该数据集包含手写数字的图像。10个等级,每个等级指的是一个数字。</target>
        </trans-unit>
        <trans-unit id="a49ccf8db4843f7af3fe75d2e385d4c5ae4a247b" translate="yes" xml:space="preserve">
          <source>The data that should be scaled.</source>
          <target state="translated">应该缩放的数据。</target>
        </trans-unit>
        <trans-unit id="4362ce6dec3d09ef8de03aa7af16c30845dd1b85" translate="yes" xml:space="preserve">
          <source>The data that should be transformed back.</source>
          <target state="translated">应转化回来的数据。</target>
        </trans-unit>
        <trans-unit id="47ba75ee664e21a51401b4c1203a08e7a876f44e" translate="yes" xml:space="preserve">
          <source>The data to be transformed by subset.</source>
          <target state="translated">按子集变换的数据。</target>
        </trans-unit>
        <trans-unit id="03bb048cfbbc3212fd60edf266a3822d3176ef87" translate="yes" xml:space="preserve">
          <source>The data to be transformed using a power transformation.</source>
          <target state="translated">要使用功率变换的数据。</target>
        </trans-unit>
        <trans-unit id="73ce115221fb870e05c90b8f043ca278fdee74a1" translate="yes" xml:space="preserve">
          <source>The data to be transformed.</source>
          <target state="translated">要转换的数据。</target>
        </trans-unit>
        <trans-unit id="df7f442215a9627450351a5239ccb7b837681c69" translate="yes" xml:space="preserve">
          <source>The data to binarize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">scipy.sparse矩阵应该是CSR格式,以避免不必要的复制。</target>
        </trans-unit>
        <trans-unit id="5401bf0fdb954957c9a3f345344e508dbd6bac54" translate="yes" xml:space="preserve">
          <source>The data to binarize, element by element. scipy.sparse matrices should be in CSR or CSC format to avoid an un-necessary copy.</source>
          <target state="translated">scipy.sparse矩阵应该是CSR或CSC格式,以避免不必要的复制。</target>
        </trans-unit>
        <trans-unit id="515108ae5aad51ada5b234ccff65d93cdd42711e" translate="yes" xml:space="preserve">
          <source>The data to center and scale.</source>
          <target state="translated">的数据,以中心和规模。</target>
        </trans-unit>
        <trans-unit id="975edd1be086184330a8a3ebbf935588408d4a98" translate="yes" xml:space="preserve">
          <source>The data to determine the categories of each feature.</source>
          <target state="translated">的数据来确定每个特征的类别。</target>
        </trans-unit>
        <trans-unit id="a77a7e1c81043cc97f700ad0d213253f486b563a" translate="yes" xml:space="preserve">
          <source>The data to encode.</source>
          <target state="translated">要编码的数据。</target>
        </trans-unit>
        <trans-unit id="d843c1d7e5e8986d5ad23251cff2a94ef8ee7955" translate="yes" xml:space="preserve">
          <source>The data to fit.</source>
          <target state="translated">拟合的数据。</target>
        </trans-unit>
        <trans-unit id="e7fbe72ba2fcf8fff679b5e28b0fef9589e7589d" translate="yes" xml:space="preserve">
          <source>The data to fit. Can be for example a list, or an array.</source>
          <target state="translated">要拟合的数据。例如,可以是一个列表,或一个数组。</target>
        </trans-unit>
        <trans-unit id="00725d5de44209593a99040fefb16433276154c2" translate="yes" xml:space="preserve">
          <source>The data to fit. Can be, for example a list, or an array at least 2d.</source>
          <target state="translated">拟合的数据。可以是,例如一个列表,或者一个至少2d的数组。</target>
        </trans-unit>
        <trans-unit id="53846ac28d489b02b9949ef562a942f522e52d48" translate="yes" xml:space="preserve">
          <source>The data to normalize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">scipy.sparse矩阵应该是CSR格式,以避免不必要的复制。</target>
        </trans-unit>
        <trans-unit id="c13e1ef3ee3dcba79bb567a64bd572c32814ce01" translate="yes" xml:space="preserve">
          <source>The data to normalize, row by row. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">scipy.sparse矩阵应该是CSR格式,以避免不必要的复制。</target>
        </trans-unit>
        <trans-unit id="89435a85b63a4550536a3843494521cdfd812011" translate="yes" xml:space="preserve">
          <source>The data to transform, row by row.</source>
          <target state="translated">要转换的数据,逐行转换。</target>
        </trans-unit>
        <trans-unit id="a092ffb0fb8aec81b90c0802c73b2ae6cfa80dcc" translate="yes" xml:space="preserve">
          <source>The data to transform, row by row. Sparse input should preferably be in CSC format.</source>
          <target state="translated">要转换的数据,逐行转换。稀疏输入最好是CSC格式。</target>
        </trans-unit>
        <trans-unit id="3f0555e5cf2c3f2eb80a653fe1f67911e223ec90" translate="yes" xml:space="preserve">
          <source>The data to transform.</source>
          <target state="translated">要转换的数据。</target>
        </trans-unit>
        <trans-unit id="f0a016c9443bbddef85796fd3691360a3c817ea0" translate="yes" xml:space="preserve">
          <source>The data used to compute the mean and standard deviation used for later scaling along the features axis.</source>
          <target state="translated">用于计算平均值和标准差的数据,用于以后沿着特征轴进行缩放。</target>
        </trans-unit>
        <trans-unit id="55438370e51e74e9e8aa5e6ed6a37b2c111bd898" translate="yes" xml:space="preserve">
          <source>The data used to compute the median and quantiles used for later scaling along the features axis.</source>
          <target state="translated">用于计算中位数和量子数的数据,用于以后沿着特征轴进行缩放。</target>
        </trans-unit>
        <trans-unit id="0ae3bec0fd19bdece5355d54cf80d1ae7bcc7c26" translate="yes" xml:space="preserve">
          <source>The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.</source>
          <target state="translated">用于计算每个特征的最小值和最大值的数据,用于以后沿特征轴的缩放。</target>
        </trans-unit>
        <trans-unit id="e26762b4f27727d3210d3fe2a64ad24b366c1062" translate="yes" xml:space="preserve">
          <source>The data used to estimate the optimal transformation parameters.</source>
          <target state="translated">用于估计最佳变换参数的数据。</target>
        </trans-unit>
        <trans-unit id="56c320de81ba4f246c360453cbd2da4e63d63c7f" translate="yes" xml:space="preserve">
          <source>The data used to fit the model. If &lt;code&gt;copy_X=False&lt;/code&gt;, then &lt;code&gt;X_fit_&lt;/code&gt; is a reference. This attribute is used for the calls to transform.</source>
          <target state="translated">用于拟合模型的数据。如果 &lt;code&gt;copy_X=False&lt;/code&gt; ，则 &lt;code&gt;X_fit_&lt;/code&gt; 是引用。此属性用于进行转换的调用。</target>
        </trans-unit>
        <trans-unit id="03376550c822ad450e2d14840686210c4804ebc6" translate="yes" xml:space="preserve">
          <source>The data used to scale along the features axis.</source>
          <target state="translated">用于沿特征轴缩放的数据。</target>
        </trans-unit>
        <trans-unit id="c4374e3fd9ee0863ad791a18672a80272e651c9f" translate="yes" xml:space="preserve">
          <source>The data used to scale along the features axis. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csc_matrix&lt;/code&gt;. Additionally, the sparse matrix needs to be nonnegative if &lt;code&gt;ignore_implicit_zeros&lt;/code&gt; is False.</source>
          <target state="translated">用于沿要素轴缩放的数据。如果提供了稀疏矩阵，它将被转换为稀疏 &lt;code&gt;csc_matrix&lt;/code&gt; 。此外，如果 &lt;code&gt;ignore_implicit_zeros&lt;/code&gt; 为False ，则稀疏矩阵需要为非负数。</target>
        </trans-unit>
        <trans-unit id="848647eb355f23af31ef1bf06cbb1e0d945eea47" translate="yes" xml:space="preserve">
          <source>The data used to scale along the specified axis.</source>
          <target state="translated">沿着指定的轴缩放的数据。</target>
        </trans-unit>
        <trans-unit id="db728424571c6d0aa73c940cbe613870747b4067" translate="yes" xml:space="preserve">
          <source>The data was used with many others for comparing various classifiers. The classes are separable, though only RDA has achieved 100% correct classification. (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) (All results using the leave-one-out technique)</source>
          <target state="translated">该数据与许多其他数据一起用于比较各种分类器。虽然只有RDA实现了100%的正确分类,但类是可以分离的。(RDA:100%,QDA 99.4%,LDA 98.9%,1NN 96.1% (z-transformed data))(所有结果均采用 &quot;留一 &quot;技术)</target>
        </trans-unit>
        <trans-unit id="660fe59aa4f1107f7d968ac4c0471ddbff4317b5" translate="yes" xml:space="preserve">
          <source>The data.</source>
          <target state="translated">数据:</target>
        </trans-unit>
        <trans-unit id="02bda070eb404181c9796aadeceacf78aac05356" translate="yes" xml:space="preserve">
          <source>The dataset can be compressed, either by removing exact duplicates if these occur in your data, or by using BIRCH. Then you only have a relatively small number of representatives for a large number of points. You can then provide a &lt;code&gt;sample_weight&lt;/code&gt; when fitting DBSCAN.</source>
          <target state="translated">可以压缩数据集，方法是删除精确的重复项（如果这些重复项出现在数据中），或者使用BIRCH。这样，对于大量的点，您只有相对较少的代表。然后，在拟合DBSCAN时可以提供 &lt;code&gt;sample_weight&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="6bbac4d66b74c67e38ab2b8e6e075ffda05e6522" translate="yes" xml:space="preserve">
          <source>The dataset is called &amp;ldquo;Twenty Newsgroups&amp;rdquo;. Here is the official description, quoted from the &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;website&lt;/a&gt;:</source>
          <target state="translated">该数据集称为&amp;ldquo;二十个新闻组&amp;rdquo;。这是官方的描述，引用自&lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;网站&lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="c4573e45f21f00151e7512eaf1b6ae1cd41e22bd" translate="yes" xml:space="preserve">
          <source>The dataset is from Zhu et al [1].</source>
          <target state="translated">数据集来自Zhu等人[1]。</target>
        </trans-unit>
        <trans-unit id="1a602d6b3832c4acaa760453be6cb4d26914f03f" translate="yes" xml:space="preserve">
          <source>The dataset is generated using the &lt;code&gt;make_biclusters&lt;/code&gt; function, which creates a matrix of small values and implants bicluster with large values. The rows and columns are then shuffled and passed to the Spectral Co-Clustering algorithm. Rearranging the shuffled matrix to make biclusters contiguous shows how accurately the algorithm found the biclusters.</source>
          <target state="translated">该数据集是使用 &lt;code&gt;make_biclusters&lt;/code&gt; 函数生成的，该函数创建一个较小值的矩阵并植入具有较大值的bicluster。然后将行和列混洗，并传递给&amp;ldquo;光谱共聚&amp;rdquo;算法。重新排列改组后的矩阵以使双簇相邻，这表明算法找到双簇的准确度。</target>
        </trans-unit>
        <trans-unit id="4131a4e690502c19383d02f0465052ba8df886e1" translate="yes" xml:space="preserve">
          <source>The dataset is structured such that points nearby in index order are nearby in parameter space, leading to an approximately block-diagonal matrix of K-nearest neighbors. Such a sparse graph is useful in a variety of circumstances which make use of spatial relationships between points for unsupervised learning: in particular, see &lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt;&lt;code&gt;sklearn.manifold.Isomap&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;sklearn.manifold.LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt;&lt;code&gt;sklearn.cluster.SpectralClustering&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">数据集的结构使得索引顺序附近的点在参数空间中附近，从而导致K近邻的近似块对角矩阵。这种稀疏图在各种情况下非常有用，这些情况利用点之间的空间关系进行无监督学习：尤其是，请参见&lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt; &lt;code&gt;sklearn.manifold.Isomap&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;sklearn.manifold.LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt; &lt;code&gt;sklearn.cluster.SpectralClustering&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="774649dea26c780c5e43d2464445b85ef137fcca" translate="yes" xml:space="preserve">
          <source>The dataset is the Boston Housing dataset (resp. 20 Newsgroups) for regression (resp. classification).</source>
          <target state="translated">数据集是波士顿住房数据集(简称20个新闻组),用于回归(简称分类)。</target>
        </trans-unit>
        <trans-unit id="c0f7ef482ab49522d3cd87d64587ff804f6727c1" translate="yes" xml:space="preserve">
          <source>The dataset used for evaluation is a 2D grid of isotropic Gaussian clusters widely spaced.</source>
          <target state="translated">用于评估的数据集是一个间距很宽的各向同性高斯簇的二维网格。</target>
        </trans-unit>
        <trans-unit id="c07599d3a55d8254ba468cc109830533370f5dcf" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is Reuters-21578 as provided by the UCI ML repository. It will be automatically downloaded and uncompressed on first run.</source>
          <target state="translated">本例中使用的数据集是由UCI ML资源库提供的Reuters-21578。它将在第一次运行时自动下载并解压。</target>
        </trans-unit>
        <trans-unit id="f610543312a82d7ead69951d61b82950d647ad3c" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is a preprocessed excerpt of the &amp;ldquo;Labeled Faces in the Wild&amp;rdquo;, aka &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;:</source>
          <target state="translated">本示例中使用的数据集是&amp;ldquo;野外标记面孔&amp;rdquo;（也称为&lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;）的预处理摘录：</target>
        </trans-unit>
        <trans-unit id="257296081e1c4b1a90a4ff7a918d81d5746f6705" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is a preprocessed excerpt of the &amp;ldquo;Labeled Faces in the Wild&amp;rdquo;, also known as &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;:</source>
          <target state="translated">本示例中使用的数据集是&amp;ldquo;野外标记面孔&amp;rdquo;（也称为&lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;）的预处理摘录：</target>
        </trans-unit>
        <trans-unit id="0e491b7a83df0aa734f40ae016e056d9d5ef3d92" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is the 20 newsgroups dataset which will be automatically downloaded and then cached and reused for the document classification example.</source>
          <target state="translated">本例中使用的数据集是20个新闻组数据集,该数据集将被自动下载,然后缓存并重新用于文档分类示例。</target>
        </trans-unit>
        <trans-unit id="b391209f39032c4c37a7d267548501e64198b514" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is the 20 newsgroups dataset. It will be automatically downloaded, then cached.</source>
          <target state="translated">本例中使用的数据集是 20 个新闻组数据集。它将被自动下载,然后缓存。</target>
        </trans-unit>
        <trans-unit id="b6261e60f08853c79c52801fadd1554784335b15" translate="yes" xml:space="preserve">
          <source>The dataset used is the Wine Dataset available at UCI. This dataset has continuous features that are heterogeneous in scale due to differing properties that they measure (i.e alcohol content, and malic acid).</source>
          <target state="translated">使用的数据集是UCI提供的Wine Dataset。这个数据集具有连续的特征,由于它们测量的属性不同(即酒精含量和苹果酸),因此在规模上是异质的。</target>
        </trans-unit>
        <trans-unit id="2320c3b81471635dac46a3209d55417f5e2427c2" translate="yes" xml:space="preserve">
          <source>The dataset will be downloaded from the &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume5/lewis04a/&quot;&gt;rcv1 homepage&lt;/a&gt; if necessary. The compressed size is about 656 MB.</source>
          <target state="translated">如有必要，将从&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume5/lewis04a/&quot;&gt;rcv1主页&lt;/a&gt;下载数据集。压缩后的大小约为656 MB。</target>
        </trans-unit>
        <trans-unit id="aadaaf4f9f002ac5fc13c03e41ad65b9ea025276" translate="yes" xml:space="preserve">
          <source>The dataset: wages</source>
          <target state="translated">数据集:工资</target>
        </trans-unit>
        <trans-unit id="a104c196a5a6b4961911da3fe75edb0c176425f0" translate="yes" xml:space="preserve">
          <source>The datasets also contain a full description in their &lt;code&gt;DESCR&lt;/code&gt; attribute and some contain &lt;code&gt;feature_names&lt;/code&gt; and &lt;code&gt;target_names&lt;/code&gt;. See the dataset descriptions below for details.</source>
          <target state="translated">数据集的 &lt;code&gt;DESCR&lt;/code&gt; 属性中也包含完整的描述，而某些数据集包含 &lt;code&gt;feature_names&lt;/code&gt; 和 &lt;code&gt;target_names&lt;/code&gt; 。有关详细信息，请参见下面的数据集描述。</target>
        </trans-unit>
        <trans-unit id="266ce3ccfa4a57091d07a7fe8bfc007064ff062e" translate="yes" xml:space="preserve">
          <source>The decision function computed the final estimator.</source>
          <target state="translated">决策函数计算出最终的估算器。</target>
        </trans-unit>
        <trans-unit id="26c7542245412a79b0296d695d58227ed1491d25" translate="yes" xml:space="preserve">
          <source>The decision function is equal (up to a constant factor) to the log-posterior of the model, i.e. &lt;code&gt;log p(y = k | x)&lt;/code&gt;. In a binary classification setting this instead corresponds to the difference &lt;code&gt;log p(y = 1 | x) - log p(y = 0 | x)&lt;/code&gt;. See &lt;a href=&quot;../lda_qda#lda-qda-math&quot;&gt;Mathematical formulation of the LDA and QDA classifiers&lt;/a&gt;.</source>
          <target state="translated">决策函数等于模型的对数后验（即 &lt;code&gt;log p(y = k | x)&lt;/code&gt; （最大为一个常数）。在二进制分类设置中，它对应于差异 &lt;code&gt;log p(y = 1 | x) - log p(y = 0 | x)&lt;/code&gt; 。请参阅&lt;a href=&quot;../lda_qda#lda-qda-math&quot;&gt;LDA和QDA分类器的数学公式&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="f5da29d39d3005818dbe52be4adbd180f7105540" translate="yes" xml:space="preserve">
          <source>The decision function is:</source>
          <target state="translated">决定功能是:</target>
        </trans-unit>
        <trans-unit id="fdcfe75a710515596cfa4af6113e103288190672" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">输入样本的决策函数，该函数对应于从集合树预测的原始值。这些类与属性&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_中&lt;/a&gt;的类相对应。回归和二进制分类是 &lt;code&gt;k == 1&lt;/code&gt; 特殊情况，否则 &lt;code&gt;k==n_classes&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="13c22c73db1fc45ace2498b233349bae422df959" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="translated">输入样本的决策函数，该函数对应于从集合树预测的原始值。类的顺序与属性&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_中&lt;/a&gt;的顺序相对应。回归和二进制分类产生一个形状为[n_samples]的数组。</target>
        </trans-unit>
        <trans-unit id="c0e6f8aea5cc6c4291ed60eb8cccabc092233917" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The columns correspond to the classes in sorted order, as they appear in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">输入样本的决策函数。列按类别顺序对应于类，因为它们出现在属性 &lt;code&gt;classes_&lt;/code&gt; 中。回归和二进制分类是 &lt;code&gt;k == 1&lt;/code&gt; 特殊情况，否则 &lt;code&gt;k==n_classes&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ae769935874da2534c2ccb6ca3e5794a84e4d439" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of outputs is the same of that of the &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt; attribute. Binary classification is a special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;. For binary classification, values closer to -1 or 1 mean more like the first or second class in &lt;code&gt;classes_&lt;/code&gt;, respectively.</source>
          <target state="translated">输入样本的决策函数。输出的顺序与&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;属性的顺序相同。二进制分类是 &lt;code&gt;k == 1&lt;/code&gt; 的特殊情况，否则 &lt;code&gt;k==n_classes&lt;/code&gt; 。对于二进制分类，更接近-1或1的值分别意味着分别 &lt;code&gt;classes_&lt;/code&gt; 的第一类或第二类。</target>
        </trans-unit>
        <trans-unit id="06ac27333a17bb0e5a70b7bdd7a4a5eafb52284d" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of outputs is the same of that of the &lt;code&gt;classes_&lt;/code&gt; attribute. Binary classification is a special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;. For binary classification, values closer to -1 or 1 mean more like the first or second class in &lt;code&gt;classes_&lt;/code&gt;, respectively.</source>
          <target state="translated">输入样本的决策函数。输出顺序与 &lt;code&gt;classes_&lt;/code&gt; 属性的顺序相同。二进制分类是 &lt;code&gt;k == 1&lt;/code&gt; 的特殊情况，否则 &lt;code&gt;k==n_classes&lt;/code&gt; 。对于二进制分类，更接近-1或1的值分别意味着分别 &lt;code&gt;classes_&lt;/code&gt; 的第一类或第二类。</target>
        </trans-unit>
        <trans-unit id="7a79704b70e2487a7278def4173b216c7abdf67d" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_&lt;/a&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="translated">输入样本的决策函数。类的顺序与属性&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-classes&quot;&gt;classes_中&lt;/a&gt;的顺序相对应。回归和二进制分类产生一个形状为[n_samples]的数组。</target>
        </trans-unit>
        <trans-unit id="271144c81e33420b0281bbd1b4c29fbb1374c6ac" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">输入样本的决策函数。类的顺序与属性 &lt;code&gt;classes_&lt;/code&gt; 中的顺序相对应。回归和二进制分类是 &lt;code&gt;k == 1&lt;/code&gt; 特殊情况，否则 &lt;code&gt;k==n_classes&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c04fb9b1c64a374fbc2424dc1dc2e69edec92fae" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="translated">输入样本的决策函数。类的顺序与属性 &lt;code&gt;classes_&lt;/code&gt; 中的顺序相对应。回归和二进制分类产生一个形状为[n_samples]的数组。</target>
        </trans-unit>
        <trans-unit id="7a4d1c2d06404b468f740213ed27426daa73066a" translate="yes" xml:space="preserve">
          <source>The decision rule for Bernoulli naive Bayes is based on</source>
          <target state="translated">Bernoulli naive Bayes的决策规则是基于</target>
        </trans-unit>
        <trans-unit id="5c243c13540ddb2ab9ba5e07515f5ccc5bddcdc4" translate="yes" xml:space="preserve">
          <source>The decision tree estimator to be exported. It can be an instance of DecisionTreeClassifier or DecisionTreeRegressor.</source>
          <target state="translated">要输出的决策树估计器。它可以是DecisionTreeClassifier或DecisionTreeRegressor的实例。</target>
        </trans-unit>
        <trans-unit id="0e0b3ca71cdfb4dae3b0e05518ff98bb98da9e44" translate="yes" xml:space="preserve">
          <source>The decision tree structure can be analysed to gain further insight on the relation between the features and the target to predict. In this example, we show how to retrieve:</source>
          <target state="translated">可以对决策树结构进行分析,以进一步了解特征和预测目标之间的关系。在这个例子中,我们展示了如何检索。</target>
        </trans-unit>
        <trans-unit id="7930dea6ad7960124a06f25452c69dc408a6af03" translate="yes" xml:space="preserve">
          <source>The decision tree to be exported to GraphViz.</source>
          <target state="translated">要输出到GraphViz的决策树。</target>
        </trans-unit>
        <trans-unit id="096b758652a55c20db43d0b3794cde42f9ae935e" translate="yes" xml:space="preserve">
          <source>The decision tree to be plotted.</source>
          <target state="translated">要绘制的决策树。</target>
        </trans-unit>
        <trans-unit id="41d54cbac8ff4bb674faa62fe3a273da70a2c8ec" translate="yes" xml:space="preserve">
          <source>The decision values for the samples are computed by adding the normalized sum of pair-wise classification confidence levels to the votes in order to disambiguate between the decision values when the votes for all the classes are equal leading to a tie.</source>
          <target state="translated">当所有类的票数相同导致平局时,为了在决定值之间进行混淆,样本的决定值是通过对票数加一对分类置信度的归一化和来计算的。</target>
        </trans-unit>
        <trans-unit id="9db4a0cde49703606f721a9d732403d53161167d" translate="yes" xml:space="preserve">
          <source>The decoding strategy depends on the vectorizer parameters.</source>
          <target state="translated">解码策略取决于矢量器参数。</target>
        </trans-unit>
        <trans-unit id="8ab5c7033c87e7cea9882c536950d8e60cf30550" translate="yes" xml:space="preserve">
          <source>The default coding of images is based on the &lt;code&gt;uint8&lt;/code&gt; dtype to spare memory. Often machine learning algorithms work best if the input is converted to a floating point representation first. Also, if you plan to use &lt;code&gt;matplotlib.pyplpt.imshow&lt;/code&gt; don&amp;rsquo;t forget to scale to the range 0 - 1 as done in the following example.</source>
          <target state="translated">图像的默认编码基于 &lt;code&gt;uint8&lt;/code&gt; dtype来备用。如果首先将输入转换为浮点表示形式，则机器学习算法通常效果最好。另外，如果您打算使用 &lt;code&gt;matplotlib.pyplpt.imshow&lt;/code&gt; ,请不要忘记将其缩放到0-1的范围，如以下示例所示。</target>
        </trans-unit>
        <trans-unit id="ec537a685ef4f8a1d5936e658cd9e5b93a584d77" translate="yes" xml:space="preserve">
          <source>The default configuration tokenizes the string by extracting words of at least 2 letters. The specific function that does this step can be requested explicitly:</source>
          <target state="translated">默认的配置是通过提取至少两个字母的单词来标记字符串。这一步的具体功能可以明确要求。</target>
        </trans-unit>
        <trans-unit id="3d025c574ea3cc62ecf87d5b0a9f83d7b3c75d6a" translate="yes" xml:space="preserve">
          <source>The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module &lt;a href=&quot;../classes#module-sklearn.model_selection&quot;&gt;&lt;code&gt;sklearn.model_selection&lt;/code&gt;&lt;/a&gt; module for the list of possible cross-validation objects.</source>
          <target state="translated">使用的默认交叉验证生成器是&amp;ldquo;分层K折&amp;rdquo;。如果提供整数，则为使用的折叠数。有关可能的交叉验证对象的列表，请参见模块&lt;a href=&quot;../classes#module-sklearn.model_selection&quot;&gt; &lt;code&gt;sklearn.model_selection&lt;/code&gt; &lt;/a&gt;模块。</target>
        </trans-unit>
        <trans-unit id="24e256639376dfcfda93b74d1117680d7d248e59" translate="yes" xml:space="preserve">
          <source>The default dataset is the 20 newsgroups dataset. To run the example on the digits dataset, pass the &lt;code&gt;--use-digits-dataset&lt;/code&gt; command line argument to this script.</source>
          <target state="translated">默认数据集是20个新闻组数据集。要在digits数据集上运行示例，请将 &lt;code&gt;--use-digits-dataset&lt;/code&gt; 命令行参数传递给此脚本。</target>
        </trans-unit>
        <trans-unit id="53050da8c5d49a141b5e2d80a70c0fa67fd7eb00" translate="yes" xml:space="preserve">
          <source>The default dataset is the digits dataset. To run the example on the twenty newsgroups dataset, pass the &amp;ndash;twenty-newsgroups command line argument to this script.</source>
          <target state="translated">默认数据集是数字数据集。要在20个新闻组数据集中运行该示例，请将&amp;ndash;twenty-newsgroups命令行参数传递给此脚本。</target>
        </trans-unit>
        <trans-unit id="8018aa0e057c3b81a78860eae3f5892dde5e1c45" translate="yes" xml:space="preserve">
          <source>The default error message is, &amp;ldquo;This %(name)s instance is not fitted yet. Call &amp;lsquo;fit&amp;rsquo; with appropriate arguments before using this estimator.&amp;rdquo;</source>
          <target state="translated">默认错误消息是&amp;ldquo;此％（name）s实例尚未安装。使用此估算器之前，请先使用适当的参数调用&amp;ldquo;适合&amp;rdquo;。&amp;rdquo;</target>
        </trans-unit>
        <trans-unit id="713044bd0fefe6c8ff38e7a00814412176e5caf0" translate="yes" xml:space="preserve">
          <source>The default error message is, &amp;ldquo;This %(name)s instance is not fitted yet. Call &amp;lsquo;fit&amp;rsquo; with appropriate arguments before using this method.&amp;rdquo;</source>
          <target state="translated">默认错误消息是&amp;ldquo;此％（name）s实例尚未安装。使用此方法之前，请先使用适当的参数调用'fit'。&amp;rdquo;</target>
        </trans-unit>
        <trans-unit id="bd1a08c69ea4f3f536ea9f567759a2eca8958b5b" translate="yes" xml:space="preserve">
          <source>The default parameters (n_samples / n_features / n_components) should make the example runnable in a couple of tens of seconds. You can try to increase the dimensions of the problem, but be aware that the time complexity is polynomial in NMF. In LDA, the time complexity is proportional to (n_samples * iterations).</source>
          <target state="translated">默认参数(n_samples/n_features/n_components)应该可以让这个例子在几十秒内运行。你可以尝试增加问题的维度,但要注意在NMF中,时间复杂度是多项式的。在LDA中,时间复杂度与(n_samples *迭代)成正比。</target>
        </trans-unit>
        <trans-unit id="6bb3234d6819185e21b8b8e0778371ddb1ae1e23" translate="yes" xml:space="preserve">
          <source>The default setting is &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt;. The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net &lt;a href=&quot;#id15&quot; id=&quot;id1&quot;&gt;11&lt;/a&gt; solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter &lt;code&gt;l1_ratio&lt;/code&gt; controls the convex combination of L1 and L2 penalty.</source>
          <target state="translated">默认设置为 &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; 。L1惩罚导致解决方案稀疏，将大多数系数驱动为零。当存在高度相关的属性时，Elastic Net &lt;a href=&quot;#id15&quot; id=&quot;id1&quot;&gt;11&lt;/a&gt;解决了L1损失的一些不足。参数 &lt;code&gt;l1_ratio&lt;/code&gt; 控制L1和L2惩罚的凸组合。</target>
        </trans-unit>
        <trans-unit id="e964bf4dcf7d784d0dee842886fe27c5060b1af1" translate="yes" xml:space="preserve">
          <source>The default setting is &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt;. The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter &lt;code&gt;l1_ratio&lt;/code&gt; controls the convex combination of L1 and L2 penalty.</source>
          <target state="translated">默认设置为 &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; 。L1惩罚导致解决方案稀疏，将大多数系数驱动为零。当存在高度相关的属性时，Elastic Net解决了L1惩罚的一些缺陷。参数 &lt;code&gt;l1_ratio&lt;/code&gt; 控制L1和L2惩罚的凸组合。</target>
        </trans-unit>
        <trans-unit id="d600d29df52c52da3fc6f38fdcf73036ecf8cbdf" translate="yes" xml:space="preserve">
          <source>The default slice is a rectangular shape around the face, removing most of the background:</source>
          <target state="translated">默认的切片是围绕脸部的矩形,去除大部分背景。</target>
        </trans-unit>
        <trans-unit id="f2f8fa0d6181b12438bd7660924ca4c2f89302ad" translate="yes" xml:space="preserve">
          <source>The default solver is &amp;lsquo;svd&amp;rsquo;. It can perform both classification and transform, and it does not rely on the calculation of the covariance matrix. This can be an advantage in situations where the number of features is large. However, the &amp;lsquo;svd&amp;rsquo; solver cannot be used with shrinkage.</source>
          <target state="translated">默认求解器为&amp;ldquo; svd&amp;rdquo;。它可以执行分类和变换，并且不依赖于协方差矩阵的计算。在功能数量很多的情况下，这可能是一个优势。但是，&amp;ldquo; svd&amp;rdquo;求解器不能与收缩一起使用。</target>
        </trans-unit>
        <trans-unit id="2307e8c1575f885543e9c12c3e37adbd9d66c3dd" translate="yes" xml:space="preserve">
          <source>The default strategy implements one step of the bootstrapping procedure.</source>
          <target state="translated">默认策略实现了引导程序的一个步骤。</target>
        </trans-unit>
        <trans-unit id="f17725906dcf13fd5a8abccea776a77c1270d7a9" translate="yes" xml:space="preserve">
          <source>The default value &lt;code&gt;max_features=&quot;auto&quot;&lt;/code&gt; uses &lt;code&gt;n_features&lt;/code&gt; rather than &lt;code&gt;n_features / 3&lt;/code&gt;. The latter was originally suggested in [1], whereas the former was more recently justified empirically in [2].</source>
          <target state="translated">默认值 &lt;code&gt;max_features=&quot;auto&quot;&lt;/code&gt; 使用 &lt;code&gt;n_features&lt;/code&gt; 而不是 &lt;code&gt;n_features / 3&lt;/code&gt; 。后者最初是在[1]中提出的，而前者最近在经验上是合理的[2]。</target>
        </trans-unit>
        <trans-unit id="71b5a78b2592e847cac7e9a1c29294e2be25d4eb" translate="yes" xml:space="preserve">
          <source>The default value of &lt;code&gt;copy&lt;/code&gt; changed from False to True in 0.23.</source>
          <target state="translated">&lt;code&gt;copy&lt;/code&gt; 的默认值在0.23中从False更改为True。</target>
        </trans-unit>
        <trans-unit id="8ad5c264779356671425d0f732ca8de7347758f3" translate="yes" xml:space="preserve">
          <source>The default values for the parameters controlling the size of the trees (e.g. &lt;code&gt;max_depth&lt;/code&gt;, &lt;code&gt;min_samples_leaf&lt;/code&gt;, etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values.</source>
          <target state="translated">控制树大小的参数的默认值（例如 &lt;code&gt;max_depth&lt;/code&gt; ， &lt;code&gt;min_samples_leaf&lt;/code&gt; 等）会导致树的完全生长和未修剪，这在某些数据集上可能非常大。为了减少内存消耗，应通过设置这些参数值来控制树的复杂性和大小。</target>
        </trans-unit>
        <trans-unit id="4209f696edb2b5615bc39c07cc72cc1b347ebd54" translate="yes" xml:space="preserve">
          <source>The definitive description of key concepts and API elements for using scikit-learn and developing compatible tools.</source>
          <target state="translated">使用scikit-learn和开发兼容工具的关键概念和API元素的权威描述。</target>
        </trans-unit>
        <trans-unit id="767ee5df767908c4f8729c932c4a3d808fe558cd" translate="yes" xml:space="preserve">
          <source>The degree of the polynomial features. Default = 2.</source>
          <target state="translated">多项式特征的度数。默认值=2。</target>
        </trans-unit>
        <trans-unit id="c0ae4038f8b612638e22a63002b9a0592ebf4ed6" translate="yes" xml:space="preserve">
          <source>The density of w, between 0 and 1</source>
          <target state="translated">w的密度,在0和1之间</target>
        </trans-unit>
        <trans-unit id="1385416aaed19b5930c017ff407294aed6e786b1" translate="yes" xml:space="preserve">
          <source>The depth of a tree is the maximum distance between the root and any leaf.</source>
          <target state="translated">树的深度是指树根和任何叶子之间的最大距离。</target>
        </trans-unit>
        <trans-unit id="64211b28a272c8cc95fc17412f0b29158a9e57aa" translate="yes" xml:space="preserve">
          <source>The desired absolute tolerance of the result. A larger tolerance will generally lead to faster execution. Default is 0.</source>
          <target state="translated">所需结果的绝对公差。较大的容差通常会导致更快的执行速度。默认值为0。</target>
        </trans-unit>
        <trans-unit id="d976f3ec62dae27876361a8dc74b8b1f3089859a" translate="yes" xml:space="preserve">
          <source>The desired relative tolerance of the result. A larger tolerance will generally lead to faster execution. Default is 1E-8.</source>
          <target state="translated">所需结果的相对容差。较大的容差通常会导致更快的执行速度。默认值是1E-8。</target>
        </trans-unit>
        <trans-unit id="e06750706d15ac4ccfc7d440948abe624b2a5a8e" translate="yes" xml:space="preserve">
          <source>The diabetes dataset consists of 10 physiological variables (age, sex, weight, blood pressure) measure on 442 patients, and an indication of disease progression after one year:</source>
          <target state="translated">糖尿病数据集包括对442名患者的10个生理变量(年龄、性别、体重、血压)的测量,以及一年后疾病进展的指示。</target>
        </trans-unit>
        <trans-unit id="9fc4977037d4a4b0eddabbe726bd2d0de7d06481" translate="yes" xml:space="preserve">
          <source>The dict at &lt;code&gt;search.cv_results_['params'][search.best_index_]&lt;/code&gt; gives the parameter setting for the best model, that gives the highest mean score (&lt;code&gt;search.best_score_&lt;/code&gt;).</source>
          <target state="translated">&lt;code&gt;search.cv_results_['params'][search.best_index_]&lt;/code&gt; 上的字典给出了最佳模型的参数设置，该模型给出了最高的平均分数（ &lt;code&gt;search.best_score_&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="bfaa3d320392a9f6c8addb9130a129b654790105" translate="yes" xml:space="preserve">
          <source>The dictionary atoms used for sparse coding. Lines are assumed to be normalized to unit norm.</source>
          <target state="translated">用于稀疏编码的字典原子。假设行数标准化为单位标准。</target>
        </trans-unit>
        <trans-unit id="7373631cc038017694fe3a2fdfb8d4bfc8e42605" translate="yes" xml:space="preserve">
          <source>The dictionary factor in the matrix factorization.</source>
          <target state="translated">矩阵分解中的字典因子。</target>
        </trans-unit>
        <trans-unit id="09a0fc68f904b631d6aa1871e7b81e42ac764443" translate="yes" xml:space="preserve">
          <source>The dictionary is fitted on the distorted left half of the image, and subsequently used to reconstruct the right half. Note that even better performance could be achieved by fitting to an undistorted (i.e. noiseless) image, but here we start from the assumption that it is not available.</source>
          <target state="translated">词典被拟合在失真的左半部分图像上,随后用于重建右半部分图像。请注意,通过对未扭曲(即无噪声)的图像进行拟合,可以获得更好的性能,但这里我们从假设它不可用开始。</target>
        </trans-unit>
        <trans-unit id="03531d72d72ed916646e794ac15c0622fe9c9b59" translate="yes" xml:space="preserve">
          <source>The dictionary learning objects offer, via the &lt;code&gt;split_code&lt;/code&gt; parameter, the possibility to separate the positive and negative values in the results of sparse coding. This is useful when dictionary learning is used for extracting features that will be used for supervised learning, because it allows the learning algorithm to assign different weights to negative loadings of a particular atom, from to the corresponding positive loading.</source>
          <target state="translated">字典学习对象通过 &lt;code&gt;split_code&lt;/code&gt; 参数提供了将稀疏编码结果中的正值和负值分开的可能性。当词典学习用于提取将用于监督学习的功能时，这很有用，因为它允许学习算法将特定原子的负负载从不同的权重分配给相应的正负载。</target>
        </trans-unit>
        <trans-unit id="c5012873b1ea68e5deed2b04cdb4e90687b2c340" translate="yes" xml:space="preserve">
          <source>The dictionary matrix against which to solve the sparse coding of the data. Some of the algorithms assume normalized rows for meaningful output.</source>
          <target state="translated">对照字典矩阵来解决数据的稀疏编码。一些算法假定标准化的行为有意义的输出。</target>
        </trans-unit>
        <trans-unit id="f137f3a53fdf8d92f7463e89f6383b5a88f3c320" translate="yes" xml:space="preserve">
          <source>The dictionary with normalized components (D).</source>
          <target state="translated">归一化成分的字典(D)。</target>
        </trans-unit>
        <trans-unit id="d49d4bb0bf7407d291ade3beb50a3cbdf55548f8" translate="yes" xml:space="preserve">
          <source>The difference between LeavePGroupsOut and GroupShuffleSplit is that the former generates splits using all subsets of size &lt;code&gt;p&lt;/code&gt; unique groups, whereas GroupShuffleSplit generates a user-determined number of random test splits, each with a user-determined fraction of unique groups.</source>
          <target state="translated">LeavePGroupsOut和GroupShuffleSplit之间的区别在于，前者使用大小为 &lt;code&gt;p&lt;/code&gt; 的唯一组的所有子集生成拆分，而GroupShuffleSplit生成用户确定数量的随机测试拆分，每个都由用户确定比例的唯一组。</target>
        </trans-unit>
        <trans-unit id="f8e596d2cfcc0c0ae56904e18bdffdb0d463a655" translate="yes" xml:space="preserve">
          <source>The difference between LeavePGroupsOut and LeaveOneGroupOut is that the former builds the test sets with all the samples assigned to &lt;code&gt;p&lt;/code&gt; different values of the groups while the latter uses samples all assigned the same groups.</source>
          <target state="translated">LeavePGroupsOut和LeaveOneGroupOut之间的区别在于，前者使用分配给 &lt;code&gt;p&lt;/code&gt; 个不同组值的所有样本构建测试集，而后者使用均分配了相同组的样本。</target>
        </trans-unit>
        <trans-unit id="123fc66dcb39304255b8e25c6c10c654dbb2b0f3" translate="yes" xml:space="preserve">
          <source>The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of \(P(x_i \mid y)\).</source>
          <target state="translated">不同的奈夫贝叶斯分类器的不同之处主要在于它们对分布的假设(P(x_i \mid y)\)。</target>
        </trans-unit>
        <trans-unit id="fc85907a08e94b0f17af8c0c01aad9ef06729185" translate="yes" xml:space="preserve">
          <source>The digits dataset is made of 1797 8x8 images of hand-written digits</source>
          <target state="translated">数字数据集由1797张8x8手写数字图像组成。</target>
        </trans-unit>
        <trans-unit id="46ad3d3589f97f144056aa3785c730497272c03e" translate="yes" xml:space="preserve">
          <source>The dimension of the projected subspace.</source>
          <target state="translated">投影子空间的尺寸。</target>
        </trans-unit>
        <trans-unit id="5572a493038acd1179abd12e1cd0c48e6709dea1" translate="yes" xml:space="preserve">
          <source>The dimension of the projection subspace.</source>
          <target state="translated">投影子空间的尺寸。</target>
        </trans-unit>
        <trans-unit id="984d56bddca949d9f1aa278aca87eb46a4072ffd" translate="yes" xml:space="preserve">
          <source>The dimensionality of the resulting representation is &lt;code&gt;n_out &amp;lt;= n_estimators * max_leaf_nodes&lt;/code&gt;. If &lt;code&gt;max_leaf_nodes == None&lt;/code&gt;, the number of leaf nodes is at most &lt;code&gt;n_estimators * 2 ** max_depth&lt;/code&gt;.</source>
          <target state="translated">结果表示的维数为 &lt;code&gt;n_out &amp;lt;= n_estimators * max_leaf_nodes&lt;/code&gt; 。如果 &lt;code&gt;max_leaf_nodes == None&lt;/code&gt; ，则叶节点的数量最多为 &lt;code&gt;n_estimators * 2 ** max_depth&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="068779d9c792d7cec6633fa1a442091cb7f6f6f3" translate="yes" xml:space="preserve">
          <source>The dimensions and distribution of Random Projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset.</source>
          <target state="translated">随机投影矩阵的尺寸和分布受到控制,以便保留数据集的任何两个样本之间的对偶距离。</target>
        </trans-unit>
        <trans-unit id="1fc3554ec8e7cb3510bb9bcb462301a368807c4a" translate="yes" xml:space="preserve">
          <source>The dimensions and distribution of random projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset. Thus random projection is a suitable approximation technique for distance based method.</source>
          <target state="translated">随机投影矩阵的维度和分布是可控的,这样可以保留数据集中任意两个样本之间的对偶距离。因此随机投影是一种适合于基于距离的方法的近似技术。</target>
        </trans-unit>
        <trans-unit id="42010c6a5240a459a14ffe4007a4a9d645a82c6f" translate="yes" xml:space="preserve">
          <source>The dimensions of one patch.</source>
          <target state="translated">一个补丁的尺寸。</target>
        </trans-unit>
        <trans-unit id="16b41d4b4452bd551af457fee2f8b60407215654" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet).</source>
          <target state="translated">各分量在重量分布上的迪里希特浓度(Dirichlet)。</target>
        </trans-unit>
        <trans-unit id="6b13240ddd6531c8fdb797758cabfbb4633f5e80" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet). The type depends on &lt;code&gt;weight_concentration_prior_type&lt;/code&gt;:</source>
          <target state="translated">每个组分的狄利克雷浓度在重量分布上（狄利克雷）。类型取决于 &lt;code&gt;weight_concentration_prior_type&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="b470809fd29296951902d887ca553f032b8a40c4" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet). This is commonly called gamma in the literature. The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the mixture weights simplex. The value of the parameter must be greater than 0. If it is None, it&amp;rsquo;s set to &lt;code&gt;1. / n_components&lt;/code&gt;.</source>
          <target state="translated">每个组分的狄利克雷浓度在重量分布上（狄利克雷）。这在文献中通常称为伽马。较高的浓度会使更多的质量集中在中心，并导致更多的组分处于活动状态，而较低的浓度参数将导致混合权重单纯形的边缘出现更多的质量。参数的值必须大于0。如果为None，则将其设置为 &lt;code&gt;1. / n_components&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="cbf40d766d38685ebd59a9222ccafd32de07a34f" translate="yes" xml:space="preserve">
          <source>The disadvantages of Bayesian regression include:</source>
          <target state="translated">贝叶斯回归的缺点包括:1:</target>
        </trans-unit>
        <trans-unit id="ec84046fbad625e7f2f2f6744eab5866f4d72369" translate="yes" xml:space="preserve">
          <source>The disadvantages of GBRT are:</source>
          <target state="translated">GBRT的缺点是:</target>
        </trans-unit>
        <trans-unit id="cfaf1a3af8c2483843ec10c36faa80968b886bf2" translate="yes" xml:space="preserve">
          <source>The disadvantages of Gaussian processes include:</source>
          <target state="translated">高斯过程的缺点包括:</target>
        </trans-unit>
        <trans-unit id="c8e0c1974308fc5ae09e195effcd7396a8db8601" translate="yes" xml:space="preserve">
          <source>The disadvantages of Multi-layer Perceptron (MLP) include:</source>
          <target state="translated">多层感知器(MLP)的缺点包括:1:</target>
        </trans-unit>
        <trans-unit id="c71179ab39a085455ac6f6888a2f6fd3afe84d6f" translate="yes" xml:space="preserve">
          <source>The disadvantages of Stochastic Gradient Descent include:</source>
          <target state="translated">随机梯度下降法的缺点包括。</target>
        </trans-unit>
        <trans-unit id="84c28c658fb7eb00eb9347365efe97c2f04c30f3" translate="yes" xml:space="preserve">
          <source>The disadvantages of decision trees include:</source>
          <target state="translated">决策树的缺点包括:</target>
        </trans-unit>
        <trans-unit id="47feb0b4f4476b90d13a7091a56a1b0f68ad3a62" translate="yes" xml:space="preserve">
          <source>The disadvantages of support vector machines include:</source>
          <target state="translated">支持向量机的缺点包括:。</target>
        </trans-unit>
        <trans-unit id="266f0e7f0f21de8e86a9f2ce2320de66f17358df" translate="yes" xml:space="preserve">
          <source>The disadvantages of the LARS method include:</source>
          <target state="translated">LARS方法的缺点包括:</target>
        </trans-unit>
        <trans-unit id="fe9e13a1ad2c431f3f290607f881625ead9408c0" translate="yes" xml:space="preserve">
          <source>The disadvantages to using t-SNE are roughly:</source>
          <target state="translated">使用t-SNE的缺点大概是。</target>
        </trans-unit>
        <trans-unit id="cd2f4d25623f857cb298f31868b60b26f2d776d4" translate="yes" xml:space="preserve">
          <source>The display objects store the computed values that were passed as arguments. This allows for the visualizations to be easliy combined using matplotlib&amp;rsquo;s API. In the following example, we place the displays next to each other in a row.</source>
          <target state="translated">显示对象存储作为参数传递的计算值。这允许使用matplotlib的API轻松组合可视化效果。在下面的示例中，我们将显示器排成一行。</target>
        </trans-unit>
        <trans-unit id="a7ea673185d2a40501e91d63f573c417ac055cd1" translate="yes" xml:space="preserve">
          <source>The distance metric to use</source>
          <target state="translated">使用的距离指标</target>
        </trans-unit>
        <trans-unit id="fade8506c426ca456f7b5f3ebd671dcd06e51421" translate="yes" xml:space="preserve">
          <source>The distance metric to use. It will be same as the &lt;code&gt;metric&lt;/code&gt; parameter or a synonym of it, e.g. &amp;lsquo;euclidean&amp;rsquo; if the &lt;code&gt;metric&lt;/code&gt; parameter set to &amp;lsquo;minkowski&amp;rsquo; and &lt;code&gt;p&lt;/code&gt; parameter set to 2.</source>
          <target state="translated">要使用的距离度量。它与 &lt;code&gt;metric&lt;/code&gt; 参数相同或与其相同，例如，如果将 &lt;code&gt;metric&lt;/code&gt; 参数设置为&amp;ldquo; minkowski&amp;rdquo;而将 &lt;code&gt;p&lt;/code&gt; 参数设置为2，则为&amp;ldquo; euclidean&amp;rdquo; 。</target>
        </trans-unit>
        <trans-unit id="0063e1a06973d2332dc280d77426f57f299b9005" translate="yes" xml:space="preserve">
          <source>The distance metric to use. Note that not all metrics are valid with all algorithms. Refer to the documentation of &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; for a description of available algorithms. Note that the normalization of the density output is correct only for the Euclidean distance metric. Default is &amp;lsquo;euclidean&amp;rsquo;.</source>
          <target state="translated">使用的距离度量。请注意，并非所有指标对所有算法都有效。有关可用算法的描述，请参考&lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt;的文档。注意，密度输出的归一化仅对于欧几里得距离度量是正确的。默认值为&amp;ldquo;欧几里得&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="3c4790f8b52fd2ea34a9c9dfcea72e27e7f13da0" translate="yes" xml:space="preserve">
          <source>The distance metric used to calculate the k-Neighbors for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is &amp;lsquo;euclidean&amp;rsquo; (&amp;lsquo;minkowski&amp;rsquo; metric with the p param equal to 2.)</source>
          <target state="translated">用于计算每个采样点的k邻居的距离度量。DistanceMetric类提供了可用指标的列表。默认距离为'euclidean'（&amp;ldquo; minkowski&amp;rdquo;度量标准，p参数等于2。）</target>
        </trans-unit>
        <trans-unit id="f687007319ee4f163c7b6668932fdc048712a932" translate="yes" xml:space="preserve">
          <source>The distance metric used to calculate the neighbors within a given radius for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is &amp;lsquo;euclidean&amp;rsquo; (&amp;lsquo;minkowski&amp;rsquo; metric with the param equal to 2.)</source>
          <target state="translated">用于计算每个采样点在给定半径内的邻居的距离度量。DistanceMetric类提供了可用指标的列表。默认距离是'euclidean'（参数等于2的'minkowski'度量）。</target>
        </trans-unit>
        <trans-unit id="8c96fb568d2fd154ab3cf2ef1e009cb92f123321" translate="yes" xml:space="preserve">
          <source>The distance metric used. It will be same as the &lt;code&gt;metric&lt;/code&gt; parameter or a synonym of it, e.g. &amp;lsquo;euclidean&amp;rsquo; if the &lt;code&gt;metric&lt;/code&gt; parameter set to &amp;lsquo;minkowski&amp;rsquo; and &lt;code&gt;p&lt;/code&gt; parameter set to 2.</source>
          <target state="translated">使用的距离度量。它与 &lt;code&gt;metric&lt;/code&gt; 参数相同或与其相同，例如，如果将 &lt;code&gt;metric&lt;/code&gt; 参数设置为&amp;ldquo; minkowski&amp;rdquo;而将 &lt;code&gt;p&lt;/code&gt; 参数设置为2，则为&amp;ldquo; euclidean&amp;rdquo; 。</target>
        </trans-unit>
        <trans-unit id="a36fd43a3aae80628a6e6b30a7a5575b2bb66390" translate="yes" xml:space="preserve">
          <source>The distances between the row vectors of &lt;code&gt;X&lt;/code&gt; and the row vectors of &lt;code&gt;Y&lt;/code&gt; can be evaluated using &lt;a href=&quot;generated/sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt;&lt;code&gt;pairwise_distances&lt;/code&gt;&lt;/a&gt;. If &lt;code&gt;Y&lt;/code&gt; is omitted the pairwise distances of the row vectors of &lt;code&gt;X&lt;/code&gt; are calculated. Similarly, &lt;a href=&quot;generated/sklearn.metrics.pairwise.pairwise_kernels#sklearn.metrics.pairwise.pairwise_kernels&quot;&gt;&lt;code&gt;pairwise.pairwise_kernels&lt;/code&gt;&lt;/a&gt; can be used to calculate the kernel between &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; using different kernel functions. See the API reference for more details.</source>
          <target state="translated">&lt;code&gt;X&lt;/code&gt; 的行向量和 &lt;code&gt;Y&lt;/code&gt; 的行向量之间的距离可以使用&lt;a href=&quot;generated/sklearn.metrics.pairwise_distances#sklearn.metrics.pairwise_distances&quot;&gt; &lt;code&gt;pairwise_distances&lt;/code&gt; &lt;/a&gt;进行评估。如果省略 &lt;code&gt;Y&lt;/code&gt; ，则计算 &lt;code&gt;X&lt;/code&gt; 的行向量的成对距离。类似地，可以使用&lt;a href=&quot;generated/sklearn.metrics.pairwise.pairwise_kernels#sklearn.metrics.pairwise.pairwise_kernels&quot;&gt; &lt;code&gt;pairwise.pairwise_kernels&lt;/code&gt; &lt;/a&gt;使用不同的内核函数来计算 &lt;code&gt;X&lt;/code&gt; 和 &lt;code&gt;Y&lt;/code&gt; 之间的内核。有关更多详细信息，请参见API参考。</target>
        </trans-unit>
        <trans-unit id="7e12a027c556a152948ba7c771e2cb65d4ac3e99" translate="yes" xml:space="preserve">
          <source>The distinct labels used in classifying instances.</source>
          <target state="translated">用于对实例进行分类的不同标签。</target>
        </trans-unit>
        <trans-unit id="2333c2cc37e50157e20c383f74b00f781cb37a1b" translate="yes" xml:space="preserve">
          <source>The distortion introduced by a random projection &lt;code&gt;p&lt;/code&gt; is asserted by the fact that &lt;code&gt;p&lt;/code&gt; is defining an eps-embedding with good probability as defined by:</source>
          <target state="translated">随机投影 &lt;code&gt;p&lt;/code&gt; 引入的失真是由以下事实所断定的： &lt;code&gt;p&lt;/code&gt; 定义eps嵌入的概率很高，如下所示：</target>
        </trans-unit>
        <trans-unit id="befef2d542b10663820383e2f1e59843332d14a0" translate="yes" xml:space="preserve">
          <source>The distortion introduced by a random projection &lt;code&gt;p&lt;/code&gt; only changes the distance between two points by a factor (1 +- eps) in an euclidean space with good probability. The projection &lt;code&gt;p&lt;/code&gt; is an eps-embedding as defined by:</source>
          <target state="translated">由随机投影 &lt;code&gt;p&lt;/code&gt; 引入的失真在欧几里得空间中仅以几率（1 +-eps）改变两点之间的距离，几率很高。投影 &lt;code&gt;p&lt;/code&gt; 是eps嵌入，其定义如下：</target>
        </trans-unit>
        <trans-unit id="b6d88e183a439e17415e4b06c82155c9e34fa344" translate="yes" xml:space="preserve">
          <source>The distributions in &lt;code&gt;scipy.stats&lt;/code&gt; prior to version scipy 0.16 do not allow specifying a random state. Instead, they use the global numpy random state, that can be seeded via &lt;code&gt;np.random.seed&lt;/code&gt; or set using &lt;code&gt;np.random.set_state&lt;/code&gt;. However, beginning scikit-learn 0.18, the &lt;a href=&quot;classes#module-sklearn.model_selection&quot;&gt;&lt;code&gt;sklearn.model_selection&lt;/code&gt;&lt;/a&gt; module sets the random state provided by the user if scipy &amp;gt;= 0.16 is also available.</source>
          <target state="translated">在分布 &lt;code&gt;scipy.stats&lt;/code&gt; 之前版本SciPy的0.16不允许指定随机状态。而是使用全局numpy随机状态，该状态可以通过 &lt;code&gt;np.random.seed&lt;/code&gt; 设置种子，也可以使用 &lt;code&gt;np.random.set_state&lt;/code&gt; 设置。但是，从scikit-learn 0.18开始，如果scipy&amp;gt; = 0.16也可用，则&lt;a href=&quot;classes#module-sklearn.model_selection&quot;&gt; &lt;code&gt;sklearn.model_selection&lt;/code&gt; &lt;/a&gt;模块将设置用户提供的随机状态。</target>
        </trans-unit>
        <trans-unit id="e4d3a4f449c4df99f288fa9346370fef4e50edc3" translate="yes" xml:space="preserve">
          <source>The dual gap at the end of the optimization for the optimal alpha (&lt;code&gt;alpha_&lt;/code&gt;).</source>
          <target state="translated">最佳alpha（ &lt;code&gt;alpha_&lt;/code&gt; ）优化结束时的双重间隔。</target>
        </trans-unit>
        <trans-unit id="1c5b7990e4f078095970dbd44954a4ff0f6bdb22" translate="yes" xml:space="preserve">
          <source>The dual gaps at the end of the optimization for each alpha.</source>
          <target state="translated">各个阿尔法的优化结束后的双重差距。</target>
        </trans-unit>
        <trans-unit id="eceb770d3ab0f05ce2328559f3653f372ef1e2c9" translate="yes" xml:space="preserve">
          <source>The dual problem is</source>
          <target state="translated">双重问题是</target>
        </trans-unit>
        <trans-unit id="aa2b6f5781584ece88d8eb541efa4219ed937f23" translate="yes" xml:space="preserve">
          <source>The dual problem to the primal is</source>
          <target state="translated">与原始的双重问题是</target>
        </trans-unit>
        <trans-unit id="c16a68f2d6169f4f9aa27b4b94f9b3db38d73c1f" translate="yes" xml:space="preserve">
          <source>The dummy regression model predicts a constant frequency. This model does not attribute the same tied rank to all samples but is none-the-less globally well calibrated (to estimate the mean frequency of the entire population).</source>
          <target state="translated">虚拟回归模型预测的是一个恒定的频率。这个模型并没有给所有样本赋予相同的绑定等级,但却在全球范围内得到了很好的校准(以估计整个人口的平均频率)。</target>
        </trans-unit>
        <trans-unit id="b5eac57a33eeb0d09a6d5ebfcbe611bef2da07d8" translate="yes" xml:space="preserve">
          <source>The edges of each bin. Contain arrays of varying shapes &lt;code&gt;(n_bins_, )&lt;/code&gt; Ignored features will have empty arrays.</source>
          <target state="translated">每个垃圾箱的边缘。包含各种形状的数组 &lt;code&gt;(n_bins_, )&lt;/code&gt; 忽略的要素将具有空数组。</target>
        </trans-unit>
        <trans-unit id="50c07dc6eab301b9d9953e2bd5849c6aa3abe8d7" translate="yes" xml:space="preserve">
          <source>The effect of the transformer is weaker than on the synthetic data. However, the transform induces a decrease of the MAE.</source>
          <target state="translated">变换器的影响比合成数据要弱。但是,变换会导致MAE的降低。</target>
        </trans-unit>
        <trans-unit id="8b7256d0b08ed885751aee08f85ac36bb70ae336" translate="yes" xml:space="preserve">
          <source>The effective size of the batch is computed here. If there are no more jobs to dispatch, return False, else return True.</source>
          <target state="translated">这里计算出该批作业的有效规模。如果没有更多的作业要派发,返回False,否则返回True。</target>
        </trans-unit>
        <trans-unit id="a044cf1265a1684d79e7b48898279d20ad6f0dac" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities</source>
          <target state="translated">要使用的特征值分解策略。AMG需要安装pyamg。它在处理大型稀疏问题时速度较快,但也可能导致不稳定。</target>
        </trans-unit>
        <trans-unit id="1dc685b8ff4dd3ccaa26f5f653c44ef0324bd82f" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities.</source>
          <target state="translated">要使用的特征值分解策略。AMG需要安装pyamg。它在处理大型稀疏问题时速度更快,但也可能导致不稳定。</target>
        </trans-unit>
        <trans-unit id="e9d800f8ff5b9787b828397b459d27c3b21cb754" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems.</source>
          <target state="translated">要使用的特征值分解策略。AMG需要安装pyamg。它在处理非常大的稀疏问题时可以更快。</target>
        </trans-unit>
        <trans-unit id="f14ed9796539634c3492800089020cfe2ebbd7af" translate="yes" xml:space="preserve">
          <source>The elastic net optimization function varies for mono and multi-outputs.</source>
          <target state="translated">弹性网优化函数在单输出和多输出的情况下有所不同。</target>
        </trans-unit>
        <trans-unit id="4d9a50594cec642f130c977c2c5c095fc619b302" translate="yes" xml:space="preserve">
          <source>The elements of the estimators parameter, having been fitted on the training data. If an estimator has been set to &lt;code&gt;'drop'&lt;/code&gt;, it will not appear in &lt;code&gt;estimators_&lt;/code&gt;.</source>
          <target state="translated">估计参数的元素已拟合在训练数据上。如果估算器已设置为 &lt;code&gt;'drop'&lt;/code&gt; ，它将不会出现在 &lt;code&gt;estimators_&lt;/code&gt; 中。</target>
        </trans-unit>
        <trans-unit id="e0191353bc3bca50b627784afbd555eb11c1f3dd" translate="yes" xml:space="preserve">
          <source>The empirical covariance matrix of a sample can be computed using the &lt;a href=&quot;generated/sklearn.covariance.empirical_covariance#sklearn.covariance.empirical_covariance&quot;&gt;&lt;code&gt;empirical_covariance&lt;/code&gt;&lt;/a&gt; function of the package, or by fitting an &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt;&lt;code&gt;EmpiricalCovariance&lt;/code&gt;&lt;/a&gt; object to the data sample with the &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.fit&quot;&gt;&lt;code&gt;EmpiricalCovariance.fit&lt;/code&gt;&lt;/a&gt; method. Be careful that results depend on whether the data are centered, so one may want to use the &lt;code&gt;assume_centered&lt;/code&gt; parameter accurately. More precisely, if &lt;code&gt;assume_centered=False&lt;/code&gt;, then the test set is supposed to have the same mean vector as the training set. If not, both should be centered by the user, and &lt;code&gt;assume_centered=True&lt;/code&gt; should be used.</source>
          <target state="translated">可以使用包的&lt;a href=&quot;generated/sklearn.covariance.empirical_covariance#sklearn.covariance.empirical_covariance&quot;&gt; &lt;code&gt;empirical_covariance&lt;/code&gt; &lt;/a&gt;函数或通过使用&lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.fit&quot;&gt; &lt;code&gt;EmpiricalCovariance.fit&lt;/code&gt; &lt;/a&gt;方法将&lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt; &lt;code&gt;EmpiricalCovariance&lt;/code&gt; &lt;/a&gt;对象拟合到数据样本来计算样本的经验协方差矩阵。请注意，结果取决于数据是否居中，因此可能需要准确地使用 &lt;code&gt;assume_centered&lt;/code&gt; 参数。更准确地说，如果假设 &lt;code&gt;assume_centered=False&lt;/code&gt; ，则假定测试集具有与训练集相同的平均向量。如果不是，则两者都应由用户居中，并应使用 &lt;code&gt;assume_centered=True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d4b0da98d5cb047aa932ef62858edca40e188517" translate="yes" xml:space="preserve">
          <source>The encoded signal (Y).</source>
          <target state="translated">编码信号(Y)。</target>
        </trans-unit>
        <trans-unit id="cc2fdf3023f61dc3df7bba02538cce691e7cf2cd" translate="yes" xml:space="preserve">
          <source>The energy function measures the quality of a joint assignment:</source>
          <target state="translated">能量函数衡量联合作业的质量。</target>
        </trans-unit>
        <trans-unit id="1bfb5cae50ef0b1032c729ea4ab68ff502e7f906" translate="yes" xml:space="preserve">
          <source>The entry &lt;code&gt;test_fold[i]&lt;/code&gt; represents the index of the test set that sample &lt;code&gt;i&lt;/code&gt; belongs to. It is possible to exclude sample &lt;code&gt;i&lt;/code&gt; from any test set (i.e. include sample &lt;code&gt;i&lt;/code&gt; in every training set) by setting &lt;code&gt;test_fold[i]&lt;/code&gt; equal to -1.</source>
          <target state="translated">条目 &lt;code&gt;test_fold[i]&lt;/code&gt; 代表样本 &lt;code&gt;i&lt;/code&gt; 所属的测试集的索引。通过将 &lt;code&gt;test_fold[i]&lt;/code&gt; 设置为-1，可以从任何测试集中排除样本 &lt;code&gt;i&lt;/code&gt; （即在每个训练集中包括样本 &lt;code&gt;i&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="503425d1f7e07b98f32316b1d85343640a4e1294" translate="yes" xml:space="preserve">
          <source>The equivalence between &lt;code&gt;alpha&lt;/code&gt; and the regularization parameter of SVM, &lt;code&gt;C&lt;/code&gt; is given by &lt;code&gt;alpha = 1 / C&lt;/code&gt; or &lt;code&gt;alpha = 1 / (n_samples * C)&lt;/code&gt;, depending on the estimator and the exact objective function optimized by the model.</source>
          <target state="translated">&lt;code&gt;alpha&lt;/code&gt; 和SVM的正则化参数 &lt;code&gt;C&lt;/code&gt; 的等价性由 &lt;code&gt;alpha = 1 / C&lt;/code&gt; 或 &lt;code&gt;alpha = 1 / (n_samples * C)&lt;/code&gt; ，具体取决于估计器和模型优化的确切目标函数。</target>
        </trans-unit>
        <trans-unit id="2fec157ea85a1f513b11852f437f8dc13c9361bb" translate="yes" xml:space="preserve">
          <source>The error message or a substring of the error message.</source>
          <target state="translated">错误信息或错误信息的子串。</target>
        </trans-unit>
        <trans-unit id="67a9569df158acc253e75d9fa812424d1f2b58c6" translate="yes" xml:space="preserve">
          <source>The estimated (sparse) precision matrix.</source>
          <target state="translated">估计的(稀疏)精度矩阵。</target>
        </trans-unit>
        <trans-unit id="cdf0d65945f589fd5da3781e66a0a81a30e934b1" translate="yes" xml:space="preserve">
          <source>The estimated covariance matrix.</source>
          <target state="translated">估计的协方差矩阵;</target>
        </trans-unit>
        <trans-unit id="4b02a2f0d176ac6fb9b484de9fcc2a6b0be67a5a" translate="yes" xml:space="preserve">
          <source>The estimated labels.</source>
          <target state="translated">估计的标签。</target>
        </trans-unit>
        <trans-unit id="77a18d9bfc3d1fc8448c4fb559b59c860611bbc7" translate="yes" xml:space="preserve">
          <source>The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See &amp;ldquo;Pattern Recognition and Machine Learning&amp;rdquo; by C. Bishop, 12.2.1 p. 574 or &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;.</source>
          <target state="translated">根据Tipping and Bishop 1999的概率PCA模型估计的噪声协方差。请参见C. Bishop的&amp;ldquo;模式识别和机器学习&amp;rdquo;，第12.2.1页。574或&lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="735cdf6c1c61f38b1092aa1bd4262b34c0d5f0f3" translate="yes" xml:space="preserve">
          <source>The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See &amp;ldquo;Pattern Recognition and Machine Learning&amp;rdquo; by C. Bishop, 12.2.1 p. 574 or &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;. It is required to compute the estimated data covariance and score samples.</source>
          <target state="translated">根据Tipping and Bishop 1999的概率PCA模型估计的噪声协方差。请参见C. Bishop的&amp;ldquo;模式识别和机器学习&amp;rdquo;，第12.2.1页。574或&lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;。需要计算估计的数据协方差和分数样本。</target>
        </trans-unit>
        <trans-unit id="cc49065d7efcb582d87899f7997c53b1f8d60c78" translate="yes" xml:space="preserve">
          <source>The estimated noise variance for each feature.</source>
          <target state="translated">每个特征的估计噪声方差。</target>
        </trans-unit>
        <trans-unit id="960e70b3d078f74487289ecbb537290d1f7473d4" translate="yes" xml:space="preserve">
          <source>The estimated number of components. Relevant when &lt;code&gt;n_components=None&lt;/code&gt;.</source>
          <target state="translated">估计的组件数。当 &lt;code&gt;n_components=None&lt;/code&gt; 时相关。</target>
        </trans-unit>
        <trans-unit id="df011fa4ccaad62cb4a83a0955409a4c4b3035c8" translate="yes" xml:space="preserve">
          <source>The estimated number of components. When n_components is set to &amp;lsquo;mle&amp;rsquo; or a number between 0 and 1 (with svd_solver == &amp;lsquo;full&amp;rsquo;) this number is estimated from input data. Otherwise it equals the parameter n_components, or the lesser value of n_features and n_samples if n_components is None.</source>
          <target state="translated">估计的组件数。当n_components设置为'mle'或0到1之间的数字（svd_solver =='full'）时，将从输入数据中估计该数字。否则，它等于参数n_components，如果n_components为None，则等于n_features和n_samples的较小值。</target>
        </trans-unit>
        <trans-unit id="cd106f5afe35a52902113758dbc47d95941530e6" translate="yes" xml:space="preserve">
          <source>The estimated number of connected components in the graph.</source>
          <target state="translated">图中连接的组件的估计数量。</target>
        </trans-unit>
        <trans-unit id="54994eb61f1dce6c8894f0827b84346580c11cb2" translate="yes" xml:space="preserve">
          <source>The estimation of the EXPERIENCE coefficient is now less variable and remain important for all models trained during cross-validation.</source>
          <target state="translated">EXPERIENCE系数的估计现在变量较少,对交叉验证期间训练的所有模型仍然很重要。</target>
        </trans-unit>
        <trans-unit id="a98c4d1bc7caeb1163bf14c014655348909c79f3" translate="yes" xml:space="preserve">
          <source>The estimation of the model is done by calculating the slopes and intercepts of a subpopulation of all possible combinations of p subsample points. If an intercept is fitted, p must be greater than or equal to n_features + 1. The final slope and intercept is then defined as the spatial median of these slopes and intercepts.</source>
          <target state="translated">模型的估计是通过计算p个子样本点的所有可能组合的子群的斜率和截距来完成的。如果截距被拟合,p必须大于或等于n_features+1。最后的斜率和截距被定义为这些斜率和截距的空间中值。</target>
        </trans-unit>
        <trans-unit id="aed02faa2e2402cc32a5968b7e86a54cff535c4e" translate="yes" xml:space="preserve">
          <source>The estimation of the model is done by iteratively maximizing the marginal log-likelihood of the observations.</source>
          <target state="translated">模型的估计是通过迭代最大化观测值的边际对数似然来完成的。</target>
        </trans-unit>
        <trans-unit id="44fe06813d3627aaa56531c52961387365e10d66" translate="yes" xml:space="preserve">
          <source>The estimation of the number of degrees of freedom is given by:</source>
          <target state="translated">自由度数的估计方法是:</target>
        </trans-unit>
        <trans-unit id="2bed56c931836016f42a33afdedfac5cb8f555f4" translate="yes" xml:space="preserve">
          <source>The estimator also implements &lt;code&gt;partial_fit&lt;/code&gt;, which updates the dictionary by iterating only once over a mini-batch. This can be used for online learning when the data is not readily available from the start, or for when the data does not fit into the memory.</source>
          <target state="translated">估计器还实现了 &lt;code&gt;partial_fit&lt;/code&gt; ，该函数通过在微型批处理中仅迭代一次来更新字典。当数据从一开始就不容易获得时，或者当数据不适合内存时，可以将其用于在线学习。</target>
        </trans-unit>
        <trans-unit id="efc1e3841cf31ff119d8e77fb55d7093c1df4dd4" translate="yes" xml:space="preserve">
          <source>The estimator objects for each cv split. This is available only if &lt;code&gt;return_estimator&lt;/code&gt; parameter is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">每个简历拆分的估算器对象。仅当 &lt;code&gt;return_estimator&lt;/code&gt; 参数设置为 &lt;code&gt;True&lt;/code&gt; 时,此选项才可用。</target>
        </trans-unit>
        <trans-unit id="f33cc8b973e26f8a535386dbd610021f87dfdf8f" translate="yes" xml:space="preserve">
          <source>The estimator or group of estimators to be cloned</source>
          <target state="translated">要克隆的估计器或估计器组。</target>
        </trans-unit>
        <trans-unit id="c1ff61d32d7a7a2deb658878854ba14950f2c891" translate="yes" xml:space="preserve">
          <source>The estimator or group of estimators to be cloned.</source>
          <target state="translated">要克隆的估计器或估计器组。</target>
        </trans-unit>
        <trans-unit id="65944411996b76786bcb63db12ac31b08b55648a" translate="yes" xml:space="preserve">
          <source>The estimator that provides the initial predictions. Set via the &lt;code&gt;init&lt;/code&gt; argument or &lt;code&gt;loss.init_estimator&lt;/code&gt;.</source>
          <target state="translated">提供初始预测的估算器。通过 &lt;code&gt;init&lt;/code&gt; 参数或 &lt;code&gt;loss.init_estimator&lt;/code&gt; 设置。</target>
        </trans-unit>
        <trans-unit id="f7ae9218cda2abb5c086ad75a4ab1e94b87ebec1" translate="yes" xml:space="preserve">
          <source>The estimator to use at each step of the round-robin imputation. If &lt;code&gt;sample_posterior&lt;/code&gt; is True, the estimator must support &lt;code&gt;return_std&lt;/code&gt; in its &lt;code&gt;predict&lt;/code&gt; method.</source>
          <target state="translated">在循环插补的每个步骤中使用的估计量。如果 &lt;code&gt;sample_posterior&lt;/code&gt; 为True，则估计器必须在其 &lt;code&gt;predict&lt;/code&gt; 方法中支持 &lt;code&gt;return_std&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e68c5f93929a570a29c52208fc20a3ad933e6e26" translate="yes" xml:space="preserve">
          <source>The estimator to visualize.</source>
          <target state="translated">估计器来可视化。</target>
        </trans-unit>
        <trans-unit id="5314881b9357d8103e571e2b27bb56496dd3b052" translate="yes" xml:space="preserve">
          <source>The estimators of a pipeline are stored as a list in the &lt;code&gt;steps&lt;/code&gt; attribute, but can be accessed by index or name by indexing (with &lt;code&gt;[idx]&lt;/code&gt;) the Pipeline:</source>
          <target state="translated">管道的估计量以列表形式存储在 &lt;code&gt;steps&lt;/code&gt; 属性中，但是可以通过索引或通过对管道建立索引（使用 &lt;code&gt;[idx]&lt;/code&gt; ）进行名称访问：</target>
        </trans-unit>
        <trans-unit id="ade400270e658832b6e128c7ef187979eae33356" translate="yes" xml:space="preserve">
          <source>The estimators of a pipeline are stored as a list in the &lt;code&gt;steps&lt;/code&gt; attribute:</source>
          <target state="translated">管道的估计量在列表中存储为 &lt;code&gt;steps&lt;/code&gt; 属性：</target>
        </trans-unit>
        <trans-unit id="bc948c82039e32a77f975544660f3043a6111d3d" translate="yes" xml:space="preserve">
          <source>The estimators of the pipeline can be retrieved by index:</source>
          <target state="translated">管道的估计器可以通过索引检索。</target>
        </trans-unit>
        <trans-unit id="a46349c122935a13db8091d877d4179b37995289" translate="yes" xml:space="preserve">
          <source>The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. For example, it is possible to use these estimators to turn a binary classifier or a regressor into a multiclass classifier. It is also possible to use these estimators with multiclass estimators in the hope that their accuracy or runtime performance improves.</source>
          <target state="translated">本模块提供的估计器是元估计器:它们需要在其构造函数中提供一个基本估计器。例如,可以使用这些估计器将二元分类器或回归器转化为多类分类器。也可以将这些估计器与多类估计器一起使用,希望提高它们的准确性或运行时性能。</target>
        </trans-unit>
        <trans-unit id="1868049da2c813b87c2e3d48a5c71f72cc892852" translate="yes" xml:space="preserve">
          <source>The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. The meta-estimator extends single output estimators to multioutput estimators.</source>
          <target state="translated">本模块提供的估计器是元估计器:它们需要在其构造函数中提供一个基本估计器。元估计器将单输出估计器扩展为多输出估计器。</target>
        </trans-unit>
        <trans-unit id="fe77b9f934c3d4135d1310eb25c7f2c922f35971" translate="yes" xml:space="preserve">
          <source>The exact API of all functions and classes, as given by the docstrings. The API documents expected types and allowed features for all functions, and all parameters available for the algorithms.</source>
          <target state="translated">所有函数和类的准确API,由docstrings给出。API记录了所有函数的预期类型和允许的功能,以及算法的所有可用参数。</target>
        </trans-unit>
        <trans-unit id="213a31afd656f2a0c8b37dd55b40fc5308db3ca1" translate="yes" xml:space="preserve">
          <source>The exact additive chi squared kernel.</source>
          <target state="translated">确切的加法chi平方核。</target>
        </trans-unit>
        <trans-unit id="14b1a49c77fdcb8a0d3244090e7c6df0b2a6cd07" translate="yes" xml:space="preserve">
          <source>The exact chi squared kernel.</source>
          <target state="translated">确切的chi平方核。</target>
        </trans-unit>
        <trans-unit id="21c56a998f59a2c6be6550db485efcc395359e33" translate="yes" xml:space="preserve">
          <source>The example below demonstrates how the OOB error can be measured at the addition of each new tree during training. The resulting plot allows a practitioner to approximate a suitable value of &lt;code&gt;n_estimators&lt;/code&gt; at which the error stabilizes.</source>
          <target state="translated">下面的示例演示了如何在训练过程中添加每棵新树时测量OOB错误。结果图允许从业者近似估计 &lt;code&gt;n_estimators&lt;/code&gt; 的合适值，误差稳定在该值上。</target>
        </trans-unit>
        <trans-unit id="556b3073adbd8a84d1d4b2fe21db4807c2daf4c8" translate="yes" xml:space="preserve">
          <source>The example below uses a support vector classifier with a non-linear kernel to build a model with optimized hyperparameters by grid search. We compare the performance of non-nested and nested CV strategies by taking the difference between their scores.</source>
          <target state="translated">下面的例子使用了一个带有非线性内核的支持向量分类器,通过网格搜索建立一个具有优化超参数的模型。我们通过取非嵌套和嵌套CV策略的得分差来比较它们的性能。</target>
        </trans-unit>
        <trans-unit id="4cd198cf307950e0cb10d5d2e6b86c91624772aa" translate="yes" xml:space="preserve">
          <source>The example compares prediction result of linear regression (linear model) and decision tree (tree based model) with and without discretization of real-valued features.</source>
          <target state="translated">本例比较了线性回归(线性模型)和决策树(基于树的模型)的预测结果,有无对实值特征进行离散化。</target>
        </trans-unit>
        <trans-unit id="3101cd4fceb55d1be63d1bff5cb3f45996fd9aa0" translate="yes" xml:space="preserve">
          <source>The example demonstrates syntax and speed only; it doesn&amp;rsquo;t actually do anything useful with the extracted vectors. See the example scripts {document_classification_20newsgroups,clustering}.py for actual learning on text documents.</source>
          <target state="translated">该示例仅演示语法和速度。它实际上对提取的向量没有任何帮助。有关实际学习文本文档的信息，请参见示例脚本{document_classification_20newsgroups，clustering} .py。</target>
        </trans-unit>
        <trans-unit id="32ed3af5edbdaa27f9eba4e5a30255c6afab4414" translate="yes" xml:space="preserve">
          <source>The example is engineered to show the effect of the choice of different metrics. It is applied to waveforms, which can be seen as high-dimensional vector. Indeed, the difference between metrics is usually more pronounced in high dimension (in particular for euclidean and cityblock).</source>
          <target state="translated">这个例子是为了显示选择不同指标的效果而设计的。它适用于波形,波形可以被看作是高维向量。事实上,在高维度上,度量之间的差异通常更为明显(尤其是对于欧氏和城块)。</target>
        </trans-unit>
        <trans-unit id="36ed82faabe70da57df289b8a54bb16e8830773a" translate="yes" xml:space="preserve">
          <source>The example shows that the predictions in ridge are strongly influenced by the outliers present in the dataset. The Huber regressor is less influenced by the outliers since the model uses the linear loss for these. As the parameter epsilon is increased for the Huber regressor, the decision function approaches that of the ridge.</source>
          <target state="translated">这个例子表明,山脊的预测受数据集中存在的离群值影响很大。Huber回归器受离群值的影响较小,因为模型对这些离群值使用了线性损失。当Huber回归器的参数epsilon增加时,决策函数接近于山脊的决策函数。</target>
        </trans-unit>
        <trans-unit id="889bc091af3b0106e91a81884d921d1b2df9bdd6" translate="yes" xml:space="preserve">
          <source>The examples below compare Gaussian mixture models with a fixed number of components, to the variational Gaussian mixture models with a Dirichlet process prior. Here, a classical Gaussian mixture is fitted with 5 components on a dataset composed of 2 clusters. We can see that the variational Gaussian mixture with a Dirichlet process prior is able to limit itself to only 2 components whereas the Gaussian mixture fits the data with a fixed number of components that has to be set a priori by the user. In this case the user has selected &lt;code&gt;n_components=5&lt;/code&gt; which does not match the true generative distribution of this toy dataset. Note that with very little observations, the variational Gaussian mixture models with a Dirichlet process prior can take a conservative stand, and fit only one component.</source>
          <target state="translated">下面的示例将具有固定数量组件的高斯混合模型与先验Dirichlet过程的变分高斯混合模型进行了比较。在这里，一个经典的高斯混合体在由2个聚类组成的数据集中具有5个成分。我们可以看到具有Dirichlet过程先验的变分高斯混合能够将自身限制为仅2个成分，而高斯混合则将数据与固定数量的成分拟合，而用户必须事先设置这些固定数量的成分。在这种情况下，用户选择的 &lt;code&gt;n_components=5&lt;/code&gt; 与该玩具数据集的真实生成分布不匹配。请注意，很少观察到，具有Dirichlet过程先验的变分高斯混合模型可以采取保守的立场，并且只能拟合一个分量。</target>
        </trans-unit>
        <trans-unit id="2cf55b61801dba638e3df4951d7127011c13c263" translate="yes" xml:space="preserve">
          <source>The expected value for the mutual information can be calculated using the following equation &lt;a href=&quot;#veb2009&quot; id=&quot;id13&quot;&gt;[VEB2009]&lt;/a&gt;. In this equation, \(a_i = |U_i|\) (the number of elements in \(U_i\)) and \(b_j = |V_j|\) (the number of elements in \(V_j\)).</source>
          <target state="translated">可以使用以下公式&lt;a href=&quot;#veb2009&quot; id=&quot;id13&quot;&gt;[VEB2009]&lt;/a&gt;计算互信息的期望值。在此等式中，\（a_i = | U_i | \）（\（U_i \）中的元素数）和\（b_j = | V_j | \）（\（V_j \）中的元素数）。</target>
        </trans-unit>
        <trans-unit id="e5c4a6233958dced6338c85ecba7ef5860ffcbbc" translate="yes" xml:space="preserve">
          <source>The expected value for the mutual information can be calculated using the following equation [VEB2009]. In this equation, \(a_i = |U_i|\) (the number of elements in \(U_i\)) and \(b_j = |V_j|\) (the number of elements in \(V_j\)).</source>
          <target state="translated">相互信息的预期值可以用以下公式计算[VEB2009]。在这个等式中,/(a_i=|U_i|/)(在/(U_i/)中的元素数量)和/(b_j=|V_j|/)(在/(V_j/)中的元素数量)。</target>
        </trans-unit>
        <trans-unit id="5075a63ec7be1cbe42a641e5ae277f9e4b63f160" translate="yes" xml:space="preserve">
          <source>The experiment is performed on an artificial dataset for binary classification with 100,000 samples (1,000 of them are used for model fitting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The first figure shows the estimated probabilities obtained with logistic regression, Gaussian naive Bayes, and Gaussian naive Bayes with both isotonic calibration and sigmoid calibration. The calibration performance is evaluated with Brier score, reported in the legend (the smaller the better). One can observe here that logistic regression is well calibrated while raw Gaussian naive Bayes performs very badly. This is because of the redundant features which violate the assumption of feature-independence and result in an overly confident classifier, which is indicated by the typical transposed-sigmoid curve.</source>
          <target state="translated">实验在二元分类的人工数据集上进行,有10万个样本(其中1000个用于模型拟合),有20个特征。在20个特征中,只有2个是信息量大的,10个是冗余的。第一张图显示了用逻辑回归、高斯奈夫贝叶斯和高斯奈夫贝叶斯进行同调校准和sigmoid校准得到的估计概率。校准性能用Brier评分来评估,在图例中报告(越小越好)。在这里可以观察到,逻辑回归的校准效果很好,而原始高斯奈夫贝叶斯的表现非常糟糕。这是因为冗余特征违反了特征独立的假设,导致分类器过于自信,典型的转置-sigmoid曲线就说明了这一点。</target>
        </trans-unit>
        <trans-unit id="b61a812b707dfd6196631d345ad23c1f697f044e" translate="yes" xml:space="preserve">
          <source>The experimental data presents a long tail distribution for &lt;code&gt;y&lt;/code&gt;. In all models, we predict the expected frequency of a random variable, so we will have necessarily fewer extreme values than for the observed realizations of that random variable. This explains that the mode of the histograms of model predictions doesn&amp;rsquo;t necessarily correspond to the smallest value. Additionally, the normal distribution used in &lt;code&gt;Ridge&lt;/code&gt; has a constant variance, while for the Poisson distribution used in &lt;code&gt;PoissonRegressor&lt;/code&gt; and &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt;, the variance is proportional to the predicted expected value.</source>
          <target state="translated">实验数据显示了 &lt;code&gt;y&lt;/code&gt; 的长尾分布。在所有模型中，我们都可以预测随机变量的预期频率，因此，与观察到的随机变量实现相比，我们的极值必然更少。这解释了模型预测直方图的模式不一定与最小值相对应。此外， &lt;code&gt;Ridge&lt;/code&gt; 中使用的正态分布具有恒定的方差，而 &lt;code&gt;PoissonRegressor&lt;/code&gt; 和 &lt;code&gt;HistGradientBoostingRegressor&lt;/code&gt; 中使用的Poisson分布的方差与预测的期望值成比例。</target>
        </trans-unit>
        <trans-unit id="e1db213d528f30cf745c1554ad2a43991932646f" translate="yes" xml:space="preserve">
          <source>The explained variance or ndarray if &amp;lsquo;multioutput&amp;rsquo; is &amp;lsquo;raw_values&amp;rsquo;.</source>
          <target state="translated">如果'multioutput'为'raw_values'，则说明变量或ndarray。</target>
        </trans-unit>
        <trans-unit id="dff7adf6114ae66681ccc96cdb5c8abce36c0555" translate="yes" xml:space="preserve">
          <source>The explicit constant as predicted by the &amp;ldquo;constant&amp;rdquo; strategy. This parameter is useful only for the &amp;ldquo;constant&amp;rdquo; strategy.</source>
          <target state="translated">如&amp;ldquo;常量&amp;rdquo;策略所预测的显式常量。此参数仅对&amp;ldquo;恒定&amp;rdquo;策略有用。</target>
        </trans-unit>
        <trans-unit id="96483cbe0c7b434b8387960f89d9c2700f0af59b" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate [default 0.5].</source>
          <target state="translated">反比例学习率的指数[默认0.5]。</target>
        </trans-unit>
        <trans-unit id="fe088102cec241da3f7e07b043fd901378b61425" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate.</source>
          <target state="translated">反比例学习率的指数。</target>
        </trans-unit>
        <trans-unit id="1fa85799d065de026e0253585d451384c9ee6013" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to &amp;lsquo;invscaling&amp;rsquo;. Only used when solver=&amp;rsquo;sgd&amp;rsquo;.</source>
          <target state="translated">反比例学习率的指数。当learning_rate设置为&amp;ldquo; invscaling&amp;rdquo;时，它用于更新有效学习率。仅在Solver ='sgd'时使用。</target>
        </trans-unit>
        <trans-unit id="c7acf68e47ad4fe8cc8cf54d82e56ffc97c60a11" translate="yes" xml:space="preserve">
          <source>The exponent for the base kernel</source>
          <target state="translated">基核的指数</target>
        </trans-unit>
        <trans-unit id="fe68de8b995171a61c550ec2f7815b01a2d28ec7" translate="yes" xml:space="preserve">
          <source>The exponentiated version of the kernel, which is usually preferable.</source>
          <target state="translated">指数版的内核,通常是比较好的。</target>
        </trans-unit>
        <trans-unit id="d1490a4bcb15c22959ee30800b231e4413480004" translate="yes" xml:space="preserve">
          <source>The external estimator fit on the reduced dataset.</source>
          <target state="translated">缩小数据集的外部估计器拟合。</target>
        </trans-unit>
        <trans-unit id="18d0cd5609d8f78695dc43242ffb9cf995a65ee9" translate="yes" xml:space="preserve">
          <source>The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features):</source>
          <target state="translated">提取的TF-IDF向量非常稀疏,在一个30000多维的空间中,按样本平均有159个非零分量(非零特征不到0.5%)。</target>
        </trans-unit>
        <trans-unit id="e5bc1ae1e0e90623cdbf461e900aded771c1b79d" translate="yes" xml:space="preserve">
          <source>The extracted dataset will only retain pictures of people that have at least &lt;code&gt;min_faces_per_person&lt;/code&gt; different pictures.</source>
          <target state="translated">提取的数据集将仅保留具有至少 &lt;code&gt;min_faces_per_person&lt;/code&gt; 个不同图片的人的图片。</target>
        </trans-unit>
        <trans-unit id="413a1ee9e7f0b0741202aa45f471425edb5c2085" translate="yes" xml:space="preserve">
          <source>The extraction method used to extract clusters using the calculated reachability and ordering. Possible values are &amp;ldquo;xi&amp;rdquo; and &amp;ldquo;dbscan&amp;rdquo;.</source>
          <target state="translated">用于使用计算的可达性和排序来提取群集的提取方法。可能的值为&amp;ldquo; xi&amp;rdquo;和&amp;ldquo; dbscan&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="7cb7e9ef70987101280d538c40ac3f0463557635" translate="yes" xml:space="preserve">
          <source>The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task easier.</source>
          <target state="translated">乘以超立方体大小的因子。值越大,聚类/类越分散,分类任务越容易。</target>
        </trans-unit>
        <trans-unit id="65d18b0a37e6414a591311e75d9ded04a4293e5e" translate="yes" xml:space="preserve">
          <source>The factory can be any callable that takes no argument and return an instance of &lt;code&gt;ParallelBackendBase&lt;/code&gt;.</source>
          <target state="translated">工厂可以是任何不带任何参数并返回 &lt;code&gt;ParallelBackendBase&lt;/code&gt; 实例的可调用对象。</target>
        </trans-unit>
        <trans-unit id="0cd5de0c793252ae54dc423f82b86e165f05af5f" translate="yes" xml:space="preserve">
          <source>The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher&amp;rsquo;s paper. Note that it&amp;rsquo;s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points.</source>
          <target state="translated">著名的鸢尾花数据库，最早由RA Fisher爵士使用。该数据集摘自Fisher的论文。请注意，它与R中的相同，但与UCI机器学习存储库中的R相同，后者具有两个错误的数据点。</target>
        </trans-unit>
        <trans-unit id="64135ddd2f4a94ed0611a8692e8099bf4451f815" translate="yes" xml:space="preserve">
          <source>The feature (e.g. &lt;code&gt;[0]&lt;/code&gt;) or pair of interacting features (e.g. &lt;code&gt;[(0, 1)]&lt;/code&gt;) for which the partial dependency should be computed.</source>
          <target state="translated">应为其计算部分依赖性的特征（例如 &lt;code&gt;[0]&lt;/code&gt; ）或相互作用对对（例如 &lt;code&gt;[(0, 1)]&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="e9c1f9439f6a49e182ea739a4c5151e4783dfb88" translate="yes" xml:space="preserve">
          <source>The feature importance scores of a fit gradient boosting model can be accessed via the &lt;code&gt;feature_importances_&lt;/code&gt; property:</source>
          <target state="translated">可以通过 &lt;code&gt;feature_importances_&lt;/code&gt; 属性访问拟合梯度增强模型的特征重要性得分：</target>
        </trans-unit>
        <trans-unit id="29293679b6f2571a09f6b43f7ca55454fa94d5ff" translate="yes" xml:space="preserve">
          <source>The feature importances.</source>
          <target state="translated">特点进口。</target>
        </trans-unit>
        <trans-unit id="596d7f9b612d1877ad65dac2d6189030efea06e6" translate="yes" xml:space="preserve">
          <source>The feature matrix &lt;code&gt;X&lt;/code&gt; should be standardized before fitting. This ensures that the penalty treats features equally.</source>
          <target state="translated">在拟合之前，应将特征矩阵 &lt;code&gt;X&lt;/code&gt; 标准化。这确保了惩罚平等对待特征。</target>
        </trans-unit>
        <trans-unit id="40d3ffb0824ad292679a9b4129b9e33a1cef7858" translate="yes" xml:space="preserve">
          <source>The feature matrix. Categorical features are encoded as ordinals.</source>
          <target state="translated">特征矩阵。分类特征被编码为序数。</target>
        </trans-unit>
        <trans-unit id="0a487efba080872bb151e33f60795da2b55ed658" translate="yes" xml:space="preserve">
          <source>The feature ranking, such that &lt;code&gt;ranking_[i]&lt;/code&gt; corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1.</source>
          <target state="translated">特征等级，使得 &lt;code&gt;ranking_[i]&lt;/code&gt; 对应于第i个特征的等级位置。选定的（即最佳估计）特征分配给等级1。</target>
        </trans-unit>
        <trans-unit id="c61b5bb2da747f9d4f147f5fd2e2f8308d20750d" translate="yes" xml:space="preserve">
          <source>The features and estimators that are experimental aren&amp;rsquo;t subject to deprecation cycles. Use them at your own risks!</source>
          <target state="translated">实验性的功能和估计量不受弃用周期的限制。使用它们需要您自担风险！</target>
        </trans-unit>
        <trans-unit id="acbf27a6428b599cd8900c8ca4f4ecae3f0cc8b0" translate="yes" xml:space="preserve">
          <source>The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and &lt;code&gt;max_features=n_features&lt;/code&gt;, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, &lt;code&gt;random_state&lt;/code&gt; has to be fixed.</source>
          <target state="translated">每次分割时，特征总是随机排列的。因此，即使在搜索最佳分割期间枚举的几个分割的标准改进相同的情况下，即使使用相同的训练数据和 &lt;code&gt;max_features=n_features&lt;/code&gt; ，找到的最佳分割也可能会有所不同。为了在拟合期间获得确定性行为，必须固定 &lt;code&gt;random_state&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c25846da01d7420f77d01daefe6ea3229aead8d7" translate="yes" xml:space="preserve">
          <source>The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, &lt;code&gt;max_features=n_features&lt;/code&gt; and &lt;code&gt;bootstrap=False&lt;/code&gt;, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, &lt;code&gt;random_state&lt;/code&gt; has to be fixed.</source>
          <target state="translated">每次分割时，特征总是随机排列的。因此，即使对于相同的训练数据， &lt;code&gt;max_features=n_features&lt;/code&gt; 和 &lt;code&gt;bootstrap=False&lt;/code&gt; ，即使在搜索最佳分割时枚举的几个分割标准的改善相同，最佳分割也可能会有所不同。为了在拟合期间获得确定性行为，必须固定 &lt;code&gt;random_state&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="79b5615baa935539329fdffd05466c4966130183" translate="yes" xml:space="preserve">
          <source>The features indices which will be returned when calling &lt;code&gt;transform&lt;/code&gt;. They are computed during &lt;code&gt;fit&lt;/code&gt;. For &lt;code&gt;features='all'&lt;/code&gt;, it is to &lt;code&gt;range(n_features)&lt;/code&gt;.</source>
          <target state="translated">调用 &lt;code&gt;transform&lt;/code&gt; 时将返回的要素索引。它们在 &lt;code&gt;fit&lt;/code&gt; 期间进行计算。对于 &lt;code&gt;features='all'&lt;/code&gt; ，它是 &lt;code&gt;range(n_features)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7bb18d685bec7903d9b61cd6026a227300d22db3" translate="yes" xml:space="preserve">
          <source>The features of &lt;code&gt;X&lt;/code&gt; have been transformed from \([x_1, x_2]\) to \([1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\), and can now be used within any linear model.</source>
          <target state="translated">&lt;code&gt;X&lt;/code&gt; 的特征已从\（[x_1，x_2] \）转换为\（[1，x_1，x_2，x_1 ^ 2，x_1 x_2，x_2 ^ 2] \），现在可以在任何线性模型中使用。</target>
        </trans-unit>
        <trans-unit id="b791adba0e25e8ba284f5da492871659f500ccc6" translate="yes" xml:space="preserve">
          <source>The features of X have been transformed from \((X_1, X_2)\) to \((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\).</source>
          <target state="translated">X的特征已经从((X_1,X_2))转化为((1,X_1,X_2,X_1^2,X_1X_2,X_2^2))。</target>
        </trans-unit>
        <trans-unit id="0ec9e3dee3b599b742aa9c67dee3f4cab82e4d00" translate="yes" xml:space="preserve">
          <source>The features of X have been transformed from \((X_1, X_2, X_3)\) to \((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\).</source>
          <target state="translated">X的特征已经从((X_1,X_2,X_3)/)转化为((1,X_1,X_2,X_3,X_1X_2,X_1X_3,X_2X_3,X_1X_2X_3)/)。</target>
        </trans-unit>
        <trans-unit id="eaccd9379a9dbc6cce41fc318001c8bf42339abe" translate="yes" xml:space="preserve">
          <source>The figure below illustrates the effect of shrinkage and subsampling on the goodness-of-fit of the model. We can clearly see that shrinkage outperforms no-shrinkage. Subsampling with shrinkage can further increase the accuracy of the model. Subsampling without shrinkage, on the other hand, does poorly.</source>
          <target state="translated">下图说明了收缩和子抽样对模型拟合度的影响。我们可以清楚地看到,收缩率优于无收缩率。带收缩的子抽样可以进一步提高模型的准确性。而没有收缩的子抽样则表现不佳。</target>
        </trans-unit>
        <trans-unit id="409a640cc1c72ef47cfd3f6ea68f986d77585eba" translate="yes" xml:space="preserve">
          <source>The figure below shows four one-way and one two-way partial dependence plots for the California housing dataset, with a &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">下图显示了带有&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;的加利福尼亚住房数据集的四个单向和两个双向偏倚图：</target>
        </trans-unit>
        <trans-unit id="50a711fcdd2b061e3d348535a1d7a4d9bddb7ed5" translate="yes" xml:space="preserve">
          <source>The figure below shows the results of applying &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; with least squares loss and 500 base learners to the Boston house price dataset (&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt;&lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt;&lt;/a&gt;). The plot on the left shows the train and test error at each iteration. The train error at each iteration is stored in the &lt;code&gt;train_score_&lt;/code&gt; attribute of the gradient boosting model. The test error at each iterations can be obtained via the &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt;&lt;code&gt;staged_predict&lt;/code&gt;&lt;/a&gt; method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. &lt;code&gt;n_estimators&lt;/code&gt;) by early stopping. The plot on the right shows the feature importances which can be obtained via the &lt;code&gt;feature_importances_&lt;/code&gt; property.</source>
          <target state="translated">下图显示了将具有最小平方损失和500个基本学习者的&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;应用到波士顿房价数据集（&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt; &lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt; &lt;/a&gt;）的结果。左图显示了每次迭代时的训练误差和测试误差。每次迭代时的火车误差都存储在梯度提升模型的 &lt;code&gt;train_score_&lt;/code&gt; 属性中。可以通过&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt; &lt;code&gt;staged_predict&lt;/code&gt; &lt;/a&gt;方法获得每次迭代时的测试错误，该方法返回一个生成器，该生成器在每个阶段产生预测。像这样的图可以用来通过提前停止来确定最佳的树数（即 &lt;code&gt;n_estimators&lt;/code&gt; ）。右图显示了功能重要性，可以通过 &lt;code&gt;feature_importances_&lt;/code&gt; 属性。</target>
        </trans-unit>
        <trans-unit id="76597dd5c3aa4f5be5177f9ebf1af69e02fc15a8" translate="yes" xml:space="preserve">
          <source>The figure below shows the results of applying &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; with least squares loss and 500 base learners to the Boston house price dataset (&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt;&lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt;&lt;/a&gt;). The plot on the left shows the train and test error at each iteration. The train error at each iteration is stored in the &lt;code&gt;train_score_&lt;/code&gt; attribute of the gradient boosting model. The test error at each iterations can be obtained via the &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt;&lt;code&gt;staged_predict&lt;/code&gt;&lt;/a&gt; method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. &lt;code&gt;n_estimators&lt;/code&gt;) by early stopping. The plot on the right shows the impurity-based feature importances which can be obtained via the &lt;code&gt;feature_importances_&lt;/code&gt; property.</source>
          <target state="translated">下图显示了将具有最小平方损失和500个基本学习者的&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;应用到波士顿房价数据集（&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt; &lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt; &lt;/a&gt;）的结果。左图显示了每次迭代时的训练误差和测试误差。每次迭代时的火车误差都存储在梯度提升模型的 &lt;code&gt;train_score_&lt;/code&gt; 属性中。可以通过&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt; &lt;code&gt;staged_predict&lt;/code&gt; &lt;/a&gt;方法获得每个迭代的测试错误，该方法返回生成器，该生成器在每个阶段产生预测。这样的图可用于通过提前停止来确定最佳树数（即 &lt;code&gt;n_estimators&lt;/code&gt; ）。右图显示了可以通过以下方式获得的基于杂质的特征重要性： &lt;code&gt;feature_importances_&lt;/code&gt; 属性。</target>
        </trans-unit>
        <trans-unit id="cc8de6d2f1f6034186ab0af10d91754eadd34954" translate="yes" xml:space="preserve">
          <source>The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly 2*pi (6.28), while KRR chooses the doubled periodicity 4*pi. Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (&amp;ldquo;curse of dimensionality&amp;rdquo;). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.</source>
          <target state="translated">该图显示两种方法都可以学习目标函数的合理模型。 GPR正确地将函数的周期性标识为大约2 * pi（6.28），而KRR选择了两倍的周期性4 * pi。除此之外，GPR为预测提供了合理的置信范围，而KRR则无法使用。两种方法之间的主要区别是拟合和预测所需的时间：尽管拟合KRR原则上是快速的，但是针对超参数优化的网格搜索会随着超参数的数量（&amp;ldquo;维数的诅咒&amp;rdquo;）成指数级增长。 GPR中基于梯度的参数优化不受此指数缩放的影响，因此在具有3维超参数空间的此示例中，速度显着提高。预测时间相似。然而，产生GPR预测分布的方差比仅预测平均值要花费更长的时间。</target>
        </trans-unit>
        <trans-unit id="7a70cd6dc569a4ad4c8d12310e45e3bf56f13e7d" translate="yes" xml:space="preserve">
          <source>The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly \(2*\pi\) (6.28), while KRR chooses the doubled periodicity \(4*\pi\) . Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (&amp;ldquo;curse of dimensionality&amp;rdquo;). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.</source>
          <target state="translated">该图显示两种方法都可以学习目标函数的合理模型。 GPR正确地将函数的周期性标识为大约\（2 * \ pi \）（6.28），而KRR选择了两倍的周期性\（4 * \ pi \）。除此之外，GPR为预测提供了合理的置信范围，而KRR则无法使用。两种方法之间的主要区别是拟合和预测所需的时间：尽管拟合KRR原则上是快速的，但是针对超参数优化的网格搜索会随着超参数的数量（&amp;ldquo;维数的诅咒&amp;rdquo;）成指数级增长。 GPR中基于梯度的参数优化不受此指数缩放的影响，因此在具有3维超参数空间的此示例中，速度显着提高。预测时间相似。然而，产生GPR预测分布的方差比仅预测平均值要花费更长的时间。</target>
        </trans-unit>
        <trans-unit id="4c819c571e9cf27ccc7a880b7ae493959bac8667" translate="yes" xml:space="preserve">
          <source>The figure shows the cumulative sum of the negative OOB improvements as a function of the boosting iteration. As you can see, it tracks the test loss for the first hundred iterations but then diverges in a pessimistic way. The figure also shows the performance of 3-fold cross validation which usually gives a better estimate of the test loss but is computationally more demanding.</source>
          <target state="translated">该图显示了负OOB改进的累积总和作为提升迭代的函数。正如您所看到的,它跟踪了前一百次迭代的测试损失,但随后以一种悲观的方式分化。该图还显示了3倍交叉验证的性能,它通常可以更好地估计测试损失,但对计算的要求更高。</target>
        </trans-unit>
        <trans-unit id="37bfa29a2e04a43879779df5389399bec3580356" translate="yes" xml:space="preserve">
          <source>The figure shows the trade-off between cross-validated score and the number of PCA components. The balanced case is when n_components=10 and accuracy=0.88, which falls into the range within 1 standard deviation of the best accuracy score.</source>
          <target state="translated">图中显示了交叉验证得分和PCA组件数量之间的权衡。平衡的情况是当n_components=10,准确率=0.88时,属于最佳准确率分数的1个标准差以内的范围。</target>
        </trans-unit>
        <trans-unit id="fc5bec6cdf25a030ca7f5736bc605af5657e6e59" translate="yes" xml:space="preserve">
          <source>The figures below are used to illustrate the effect of scaling our &lt;code&gt;C&lt;/code&gt; to compensate for the change in the number of samples, in the case of using an &lt;code&gt;l1&lt;/code&gt; penalty, as well as the &lt;code&gt;l2&lt;/code&gt; penalty.</source>
          <target state="translated">下图用于说明在使用 &lt;code&gt;l1&lt;/code&gt; 罚分和 &lt;code&gt;l2&lt;/code&gt; 罚分的情况下，缩放 &lt;code&gt;C&lt;/code&gt; 以补偿样本数量变化的效果。</target>
        </trans-unit>
        <trans-unit id="0b0d592cc5daeb578d9b82714e3800c3092f375f" translate="yes" xml:space="preserve">
          <source>The figures illustrate the interpolating property of the Gaussian Process model as well as its probabilistic nature in the form of a pointwise 95% confidence interval.</source>
          <target state="translated">这些数字说明了高斯过程模型的插值特性,以及它的概率性质,其形式是一个点向95%的置信区间。</target>
        </trans-unit>
        <trans-unit id="e96dfdb18e509ef4ea2f5731757d5c9fff16f17b" translate="yes" xml:space="preserve">
          <source>The figures show the confusion matrix with and without normalization by class support size (number of elements in each class). This kind of normalization can be interesting in case of class imbalance to have a more visual interpretation of which class is being misclassified.</source>
          <target state="translated">图中显示了按类支持规模(每个类的元素数量)进行归一化和不归一化的混淆矩阵。这种归一化在班级不平衡的情况下很有意思,可以更直观地解释哪个班级被错误分类。</target>
        </trans-unit>
        <trans-unit id="e0f5d06a7d2ed2f3c6fb0ae21cf62a8ad73fbd7f" translate="yes" xml:space="preserve">
          <source>The filenames for the images.</source>
          <target state="translated">图像的文件名。</target>
        </trans-unit>
        <trans-unit id="4c01e855d5c880b7b53c9ce8ac56ca5941d4bf79" translate="yes" xml:space="preserve">
          <source>The filenames holding the dataset.</source>
          <target state="translated">保存数据集的文件名。</target>
        </trans-unit>
        <trans-unit id="83d2d2e5c2f316a3531e949e8bac6210f7e21821" translate="yes" xml:space="preserve">
          <source>The files themselves are loaded in memory in the &lt;code&gt;data&lt;/code&gt; attribute. For reference the filenames are also available:</source>
          <target state="translated">文件本身被加载到 &lt;code&gt;data&lt;/code&gt; 属性的内存中。作为参考，文件名也可用：</target>
        </trans-unit>
        <trans-unit id="4834eeb362cb8f80edd0fa52b738fe7db255a20e" translate="yes" xml:space="preserve">
          <source>The filesystem path to the root folder where MLComp datasets are stored, if mlcomp_root is None, the MLCOMP_DATASETS_HOME environment variable is looked up instead.</source>
          <target state="translated">存储MLComp数据集的根文件夹的文件系统路径,如果mlcomp_root为None,则查找MLCOMP_DATASETS_HOME环境变量。</target>
        </trans-unit>
        <trans-unit id="5e83705f6f5cf8453734b26cfa75b02cefbf9a06" translate="yes" xml:space="preserve">
          <source>The final sum of similarities is divided by the size of the larger set.</source>
          <target state="translated">最后的相似性之和除以大集的大小。</target>
        </trans-unit>
        <trans-unit id="6023cd4bbdadd0261fe4fb93bbac4f79dd623983" translate="yes" xml:space="preserve">
          <source>The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set).</source>
          <target state="translated">惯性标准的最终值(训练集中所有观测值到最近中心点的平方距离之和)。</target>
        </trans-unit>
        <trans-unit id="65db46a7a2384cdd4de97d3c70536cd9da4c8616" translate="yes" xml:space="preserve">
          <source>The final value of the stress (sum of squared distance of the disparities and the distances for all constrained points).</source>
          <target state="translated">应力的最终值(差距的平方距离与所有约束点的距离之和)。</target>
        </trans-unit>
        <trans-unit id="379910ba877ee4fe0038b0877574596b2d4e7156" translate="yes" xml:space="preserve">
          <source>The first 4 plots use the &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt;&lt;code&gt;make_classification&lt;/code&gt;&lt;/a&gt; with different numbers of informative features, clusters per class and classes. The final 2 plots use &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt;&lt;code&gt;make_blobs&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;../../modules/generated/sklearn.datasets.make_gaussian_quantiles#sklearn.datasets.make_gaussian_quantiles&quot;&gt;&lt;code&gt;make_gaussian_quantiles&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">前四个图使用&lt;a href=&quot;../../modules/generated/sklearn.datasets.make_classification#sklearn.datasets.make_classification&quot;&gt; &lt;code&gt;make_classification&lt;/code&gt; &lt;/a&gt;带有不同数量的信息功能，每个类和每个类的聚类。最后两个图使用&lt;a href=&quot;../../modules/generated/sklearn.datasets.make_blobs#sklearn.datasets.make_blobs&quot;&gt; &lt;code&gt;make_blobs&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;../../modules/generated/sklearn.datasets.make_gaussian_quantiles#sklearn.datasets.make_gaussian_quantiles&quot;&gt; &lt;code&gt;make_gaussian_quantiles&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="7e7a148ceab046969e5524f650bd9948ad6ae0ac" translate="yes" xml:space="preserve">
          <source>The first &lt;code&gt;[.9, .1]&lt;/code&gt; in &lt;code&gt;y_pred&lt;/code&gt; denotes 90% probability that the first sample has label 0. The log loss is non-negative.</source>
          <target state="translated">第一 &lt;code&gt;[.9, .1]&lt;/code&gt; 中 &lt;code&gt;y_pred&lt;/code&gt; 表示90％概率的是，第一样本具有标签0。日志损失是非负的。</target>
        </trans-unit>
        <trans-unit id="c41f4a219241dbe01041bd4816c28f057f1f8fc9" translate="yes" xml:space="preserve">
          <source>The first &lt;code&gt;n_samples % n_splits&lt;/code&gt; folds have size &lt;code&gt;n_samples // n_splits + 1&lt;/code&gt;, other folds have size &lt;code&gt;n_samples // n_splits&lt;/code&gt;, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples.</source>
          <target state="translated">前 &lt;code&gt;n_samples % n_splits&lt;/code&gt; 折叠的大小为 &lt;code&gt;n_samples // n_splits + 1&lt;/code&gt; ，其他折叠的大小为 &lt;code&gt;n_samples // n_splits&lt;/code&gt; ，其中 &lt;code&gt;n_samples&lt;/code&gt; 是样本数。</target>
        </trans-unit>
        <trans-unit id="318062ceb48883ba19b024b3ac85479b5d766c8b" translate="yes" xml:space="preserve">
          <source>The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices. In general, multiple points can be queried at the same time.</source>
          <target state="translated">返回的第一个数组包含与所有接近1.6的点的距离,而第二个数组包含它们的指数。一般来说,可以同时查询多个点。</target>
        </trans-unit>
        <trans-unit id="b072f11f22e8fd40f8f011ced740f859610c5839" translate="yes" xml:space="preserve">
          <source>The first base-kernel of the product-kernel</source>
          <target state="translated">产品内核的第一基核。</target>
        </trans-unit>
        <trans-unit id="2843deb348cef2ebb64b5cb6c478da3efa931340" translate="yes" xml:space="preserve">
          <source>The first base-kernel of the sum-kernel</source>
          <target state="translated">和核的第一基核。</target>
        </trans-unit>
        <trans-unit id="23baf5dbd5fca89380aba285e5a6dca77d9ff4b8" translate="yes" xml:space="preserve">
          <source>The first column of images shows true faces. The next columns illustrate how extremely randomized trees, k nearest neighbors, linear regression and ridge regression complete the lower half of those faces.</source>
          <target state="translated">第一列图像显示的是真实的面孔。下一列说明了极度随机化的树、k个最近的邻居、线性回归和脊回归如何完成这些面孔的下半部分。</target>
        </trans-unit>
        <trans-unit id="758127cc4d0b762bc4a77876dd4551f141ccd7cd" translate="yes" xml:space="preserve">
          <source>The first corresponds to a model with a high noise level and a large length scale, which explains all variations in the data by noise.</source>
          <target state="translated">第一种对应于高噪声水平和大长度尺度的模型,它能用噪声解释数据的所有变化。</target>
        </trans-unit>
        <trans-unit id="2d71e99749591e16661c08d2c19738355ed2fad8" translate="yes" xml:space="preserve">
          <source>The first element of each line can be used to store a target variable to predict.</source>
          <target state="translated">每一行的第一个元素可以用来存储一个要预测的目标变量。</target>
        </trans-unit>
        <trans-unit id="d151d2a5867c5ba125b375d964535691cae0b0d6" translate="yes" xml:space="preserve">
          <source>The first example illustrates how robust covariance estimation can help concentrating on a relevant cluster when another one exists. Here, many observations are confounded into one and break down the empirical covariance estimation. Of course, some screening tools would have pointed out the presence of two clusters (Support Vector Machines, Gaussian Mixture Models, univariate outlier detection, &amp;hellip;). But had it been a high-dimensional example, none of these could be applied that easily.</source>
          <target state="translated">第一个示例说明了健壮的协方差估计如何在存在另一个集群时帮助其集中在相关集群上。在这里，许多观察结果被混淆为一个，并分解了经验协方差估计。当然，某些筛选工具会指出存在两个聚类（支持向量机，高斯混合模型，单变量离群值检测&amp;hellip;&amp;hellip;）。但是，如果这是一个高维度的例子，那么所有这些都不容易被应用。</target>
        </trans-unit>
        <trans-unit id="3ed755a713ce6230a16c443825e8cd0dee1503e8" translate="yes" xml:space="preserve">
          <source>The first example illustrates how the Minimum Covariance Determinant robust estimator can help concentrate on a relevant cluster when outlying points exist. Here the empirical covariance estimation is skewed by points outside of the main cluster. Of course, some screening tools would have pointed out the presence of two clusters (Support Vector Machines, Gaussian Mixture Models, univariate outlier detection, &amp;hellip;). But had it been a high-dimensional example, none of these could be applied that easily.</source>
          <target state="translated">第一个示例说明了最小协方差决定因素鲁棒估计器如何在离群点存在时帮助将注意力集中在相关聚类上。在这里，经验协方差估计被主聚类之外的点所偏斜。当然，某些筛选工具会指出存在两个聚类（支持向量机，高斯混合模型，单变量离群值检测&amp;hellip;&amp;hellip;）。但是，如果这是一个高维度的例子，那么所有这些都不容易被应用。</target>
        </trans-unit>
        <trans-unit id="ecfbbb8300d863a918a602e1c4f90841d4b71a3f" translate="yes" xml:space="preserve">
          <source>The first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning):</source>
          <target state="translated">第一个加载器用于人脸识别任务:多类分类任务(因此是监督学习)。</target>
        </trans-unit>
        <trans-unit id="1272790baab931c65e23f13d4b17128820578899" translate="yes" xml:space="preserve">
          <source>The first model is a classical Gaussian Mixture Model with 10 components fit with the Expectation-Maximization algorithm.</source>
          <target state="translated">第一个模型是一个经典的高斯混合物模型,有10个成分,用期望-最大化算法拟合。</target>
        </trans-unit>
        <trans-unit id="a28da4c4f339f2d2823befd3907c74a3bd7b1459" translate="yes" xml:space="preserve">
          <source>The first plot is a visualization of the decision function for a variety of parameter values on a simplified classification problem involving only 2 input features and 2 possible target classes (binary classification). Note that this kind of plot is not possible to do for problems with more features or target classes.</source>
          <target state="translated">第一张图是一个简化的分类问题的各种参数值的决策函数的可视化,只涉及2个输入特征和2个可能的目标类(二元分类)。请注意,对于有更多特征或目标类的问题,这种图是不可能做的。</target>
        </trans-unit>
        <trans-unit id="fdb1a20b80ba5f009466ef7acd3785afda979734" translate="yes" xml:space="preserve">
          <source>The first plot shows one of the problems with using histograms to visualize the density of points in 1D. Intuitively, a histogram can be thought of as a scheme in which a unit &amp;ldquo;block&amp;rdquo; is stacked above each point on a regular grid. As the top two panels show, however, the choice of gridding for these blocks can lead to wildly divergent ideas about the underlying shape of the density distribution. If we instead center each block on the point it represents, we get the estimate shown in the bottom left panel. This is a kernel density estimation with a &amp;ldquo;top hat&amp;rdquo; kernel. This idea can be generalized to other kernel shapes: the bottom-right panel of the first figure shows a Gaussian kernel density estimate over the same distribution.</source>
          <target state="translated">第一张图显示了使用直方图可视化一维点密度的问题之一。从直觉上讲，直方图可以看作是一种方案，其中单位&amp;ldquo;块&amp;rdquo;堆叠在规则网格的每个点上方。但是，如前两个面板所示，这些块的网格选择可能导致有关密度分布的基本形状的想法大相径庭。如果我们改为将每个块放在其表示的点上居中，则会得到显示在左下方面板中的估计值。这是使用&amp;ldquo;高顶礼帽&amp;rdquo;内核的内核密度估计。这个想法可以推广到其他核形状：第一个图的右下图显示了在相同分布上的高斯核密度估计。</target>
        </trans-unit>
        <trans-unit id="80a6f35d6de94c1655a0ab570a2d81b8e9f0c281" translate="yes" xml:space="preserve">
          <source>The first plot shows that with an increasing number of samples &lt;code&gt;n_samples&lt;/code&gt;, the minimal number of dimensions &lt;code&gt;n_components&lt;/code&gt; increased logarithmically in order to guarantee an &lt;code&gt;eps&lt;/code&gt;-embedding.</source>
          <target state="translated">第一个图显示，随着样本 &lt;code&gt;n_samples&lt;/code&gt; 数量的增加，最小维度 &lt;code&gt;n_components&lt;/code&gt; 的数量以对数形式增加，以确保 &lt;code&gt;eps&lt;/code&gt; 嵌入。</target>
        </trans-unit>
        <trans-unit id="0fa5069414b33f645c4a3a3261f97c50a380bf97" translate="yes" xml:space="preserve">
          <source>The first plot shows the best inertia reached for each combination of the model (&lt;code&gt;KMeans&lt;/code&gt; or &lt;code&gt;MiniBatchKMeans&lt;/code&gt;) and the init method (&lt;code&gt;init=&quot;random&quot;&lt;/code&gt; or &lt;code&gt;init=&quot;kmeans++&quot;&lt;/code&gt;) for increasing values of the &lt;code&gt;n_init&lt;/code&gt; parameter that controls the number of initializations.</source>
          <target state="translated">第一张图显示了模型（ &lt;code&gt;KMeans&lt;/code&gt; 或 &lt;code&gt;MiniBatchKMeans&lt;/code&gt; ）和init方法（ &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; 或 &lt;code&gt;init=&quot;kmeans++&quot;&lt;/code&gt; ）的每种组合的最佳惯性，以增加控制初始化次数的 &lt;code&gt;n_init&lt;/code&gt; 参数的值。</target>
        </trans-unit>
        <trans-unit id="35a5988878bfd98d1cc6f738d4af80878102b501" translate="yes" xml:space="preserve">
          <source>The first row of output array indicates that there are three samples whose true cluster is &amp;ldquo;a&amp;rdquo;. Of them, two are in predicted cluster 0, one is in 1, and none is in 2. And the second row indicates that there are three samples whose true cluster is &amp;ldquo;b&amp;rdquo;. Of them, none is in predicted cluster 0, one is in 1 and two are in 2.</source>
          <target state="translated">输出数组的第一行指示存在三个样本，其真实簇为&amp;ldquo; a&amp;rdquo;。其中两个在预测聚类0中，一个在1中，一个都不在2中。第二行表示存在三个样本，其真实聚类为&amp;ldquo; b&amp;rdquo;。其中，没有一个在预测聚类0中，一个在1中，两个在2中。</target>
        </trans-unit>
        <trans-unit id="94d22c244cece5d36684b54eb3a3c36ef460564d" translate="yes" xml:space="preserve">
          <source>The first two loss functions are lazy, they only update the model parameters if an example violates the margin constraint, which makes training very efficient and may result in sparser models, even when L2 penalty is used.</source>
          <target state="translated">前两个损失函数是懒惰的,它们只有在一个例子违反了边际约束时才会更新模型参数,这使得训练非常高效,即使使用L2惩罚,也可能导致模型更加稀疏。</target>
        </trans-unit>
        <trans-unit id="90159f341e51eef650aa99c9ddd91af9915c59a3" translate="yes" xml:space="preserve">
          <source>The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions, using the &lt;code&gt;transform&lt;/code&gt; method.</source>
          <target state="translated">拟合模型还可以使用 &lt;code&gt;transform&lt;/code&gt; 方法，通过将输入投影到最有区别的方向来减少输入的维数。</target>
        </trans-unit>
        <trans-unit id="865ac3c349894331005e5c9a9918fbe1c4576705" translate="yes" xml:space="preserve">
          <source>The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.</source>
          <target state="translated">拟合的模型还可以通过将其投射到最有判别力的方向来降低输入的维度。</target>
        </trans-unit>
        <trans-unit id="47ade7eb6fd49b2a025d9a87896bfcf3ba24402d" translate="yes" xml:space="preserve">
          <source>The fitted model.</source>
          <target state="translated">拟合模型。</target>
        </trans-unit>
        <trans-unit id="474bb96d2aba10d56f1c494aef7da54c3ce49fa5" translate="yes" xml:space="preserve">
          <source>The flattened data matrix. If &lt;code&gt;as_frame=True&lt;/code&gt;, &lt;code&gt;data&lt;/code&gt; will be a pandas DataFrame.</source>
          <target state="translated">扁平化的数据矩阵。如果 &lt;code&gt;as_frame=True&lt;/code&gt; ，则 &lt;code&gt;data&lt;/code&gt; 将为pandas DataFrame。</target>
        </trans-unit>
        <trans-unit id="14120dbe50cfe7a93d61eb3782205eb1e9322e9d" translate="yes" xml:space="preserve">
          <source>The flexibility of controlling the smoothness of the learned function via \(\nu\) allows adapting to the properties of the true underlying functional relation. The prior and posterior of a GP resulting from a Mat&amp;eacute;rn kernel are shown in the following figure:</source>
          <target state="translated">通过\（\ nu \）控制学习函数的平滑度的灵活性允许适应真正的基础函数关系的属性。下图显示了由Mat&amp;eacute;rn内核生成的GP的先验和后验：</target>
        </trans-unit>
        <trans-unit id="130cb7c407aba5d31bddc566759f6c2692fa934a" translate="yes" xml:space="preserve">
          <source>The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data.</source>
          <target state="translated">下面的流程图是为了给用户提供一点粗略的指导,让他们知道如何处理关于在你的数据上尝试使用哪些估算器的问题。</target>
        </trans-unit>
        <trans-unit id="f162534c96a36fe6b7c86b6775d49d882723bf35" translate="yes" xml:space="preserve">
          <source>The folder names are used as supervised signal label names. The individual file names are not important.</source>
          <target state="translated">文件夹名称用作监督信号标签名称。单个文件名并不重要。</target>
        </trans-unit>
        <trans-unit id="f2db8b6a5929e1458b6943c5d307e1ce8a02db89" translate="yes" xml:space="preserve">
          <source>The folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold.</source>
          <target state="translated">褶皱是近似平衡的,即每个褶皱中不同组的数量近似相同。</target>
        </trans-unit>
        <trans-unit id="7910d480ac1425d50ff9fab631fdf912810ec26e" translate="yes" xml:space="preserve">
          <source>The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the features. In mathematical notation, if \(\hat{y}\) is the predicted value.</source>
          <target state="translated">以下是一套用于回归的方法,在这些方法中,预期目标值是特征的线性组合。在数学符号中,如果(\hat{y}\)是预测值。</target>
        </trans-unit>
        <trans-unit id="b43c7b928f1a786f0281c067a04292ccfddc7963" translate="yes" xml:space="preserve">
          <source>The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the input variables. In mathematical notion, if \(\hat{y}\) is the predicted value.</source>
          <target state="translated">以下是一套用于回归的方法,在这种方法中,预期目标值是输入变量的线性组合。在数学概念中,如果(hat{y}/)是预测值。</target>
        </trans-unit>
        <trans-unit id="81301a81f070c1a1a05c02b69a367906c9b88d21" translate="yes" xml:space="preserve">
          <source>The following clustering assignment is slightly better, since it is homogeneous but not complete:</source>
          <target state="translated">下面的聚类赋值稍好,因为它是同质的,但不完整。</target>
        </trans-unit>
        <trans-unit id="563a9a6f3a15e2fd51ba9e9b647870430bf12e58" translate="yes" xml:space="preserve">
          <source>The following code defines a linear kernel and creates a classifier instance that will use that kernel:</source>
          <target state="translated">下面的代码定义了一个线性内核,并创建了一个使用该内核的分类器实例。</target>
        </trans-unit>
        <trans-unit id="b8e362395586349d1fcb8b526001a536e06f5d5d" translate="yes" xml:space="preserve">
          <source>The following code is a bit verbose, feel free to jump directly to the analysis of the &lt;a href=&quot;#results&quot;&gt;results&lt;/a&gt;.</source>
          <target state="translated">以下代码有点冗长，可以直接跳转到&lt;a href=&quot;#results&quot;&gt;结果&lt;/a&gt;分析。</target>
        </trans-unit>
        <trans-unit id="b414b22c1d4fa210fe910463f00d0b843e42262f" translate="yes" xml:space="preserve">
          <source>The following cross-validation splitters can be used to do that. The grouping identifier for the samples is specified via the &lt;code&gt;groups&lt;/code&gt; parameter.</source>
          <target state="translated">以下交叉验证拆分器可用于执行此操作。样本的分组标识符是通过 &lt;code&gt;groups&lt;/code&gt; 参数指定的。</target>
        </trans-unit>
        <trans-unit id="6757b5cfa24458748d25f81af60d324180a74dc4" translate="yes" xml:space="preserve">
          <source>The following cross-validators can be used in such cases.</source>
          <target state="translated">在这种情况下,可以使用以下交叉验证器。</target>
        </trans-unit>
        <trans-unit id="6da3fe3659ba9973062031cbded9ac1ee9e80559" translate="yes" xml:space="preserve">
          <source>The following dataset has integer features, two of which are the same in every sample. These are removed with the default setting for threshold:</source>
          <target state="translated">以下数据集有整数特征,其中两个特征在每个样本中都是相同的。这些特征在阈值的默认设置下被删除。</target>
        </trans-unit>
        <trans-unit id="54024b35aa398c3f2441f7e2cd8d0e6044cd9d46" translate="yes" xml:space="preserve">
          <source>The following estimators have built-in variable selection fitting procedures, but any estimator using a L1 or elastic-net penalty also performs variable selection: typically &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; with an appropriate penalty.</source>
          <target state="translated">以下估算器具有内置的变量选择拟合过程，但是任何使用L1或弹性网罚分的估算器也将执行变量选择：通常是带有适当罚分的&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="f50629b49593a5773731e8a4d6d7fbab3d06afa6" translate="yes" xml:space="preserve">
          <source>The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):</source>
          <target state="translated">下面的例子演示了如何通过拆分数据、拟合模型和连续5次计算分数(每次拆分不同)来估计线性核支持向量机在虹膜数据集上的准确性。</target>
        </trans-unit>
        <trans-unit id="cd40f6b0be33059696fbdf36c2c2af62f27e4578" translate="yes" xml:space="preserve">
          <source>The following example highlights the limitations of impurity-based feature importance in contrast to permutation-based feature importance: &lt;a href=&quot;../auto_examples/inspection/plot_permutation_importance#sphx-glr-auto-examples-inspection-plot-permutation-importance-py&quot;&gt;Permutation Importance vs Random Forest Feature Importance (MDI)&lt;/a&gt;.</source>
          <target state="translated">以下示例突出了与基于置换的特征重要性相比，基于杂质的特征重要性的局限性：&lt;a href=&quot;../auto_examples/inspection/plot_permutation_importance#sphx-glr-auto-examples-inspection-plot-permutation-importance-py&quot;&gt;置换重要性与随机森林特征重要性（MDI）&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="50761302c7bb4483750fc53752f14a1bf26cb023" translate="yes" xml:space="preserve">
          <source>The following example illustrates 16 components extracted using sparse PCA from the Olivetti faces dataset. It can be seen how the regularization term induces many zeros. Furthermore, the natural structure of the data causes the non-zero coefficients to be vertically adjacent. The model does not enforce this mathematically: each component is a vector \(h \in \mathbf{R}^{4096}\), and there is no notion of vertical adjacency except during the human-friendly visualization as 64x64 pixel images. The fact that the components shown below appear local is the effect of the inherent structure of the data, which makes such local patterns minimize reconstruction error. There exist sparsity-inducing norms that take into account adjacency and different kinds of structure; see &lt;a href=&quot;#jen09&quot; id=&quot;id2&quot;&gt;[Jen09]&lt;/a&gt; for a review of such methods. For more details on how to use Sparse PCA, see the Examples section, below.</source>
          <target state="translated">以下示例说明了使用稀疏PCA从Olivetti人脸数据集中提取的16个分量。可以看出正则项如何诱发许多零。此外，数据的自然结构导致非零系数垂直相邻。该模型没有在数学上强制执行此操作：每个分量都是向量\（h \ in \ mathbf {R} ^ {4096} \），并且没有垂直邻接的概念，除非在人类友好的可视化期间为64x64像素图像。下面显示的组件显示为本地的事实是数据固有结构的影响，这使得此类本地模式将重构误差降至最低。存在一些稀疏性准则，这些准则考虑了邻接关系和不同类型的结构。见&lt;a href=&quot;#jen09&quot; id=&quot;id2&quot;&gt;[Jen09]&lt;/a&gt;回顾这些方法。有关如何使用稀疏PCA的更多详细信息，请参见下面的&amp;ldquo;示例&amp;rdquo;部分。</target>
        </trans-unit>
        <trans-unit id="218baecbc1e50d08c530ba0bc6ca1335ad228789" translate="yes" xml:space="preserve">
          <source>The following example illustrates how the decision regions may change when a soft &lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; is used based on an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier:</source>
          <target state="translated">以下示例说明了基于线性支持向量机，决策树和K近邻分类器使用软&lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt; &lt;code&gt;VotingClassifier&lt;/code&gt; &lt;/a&gt;时决策区域如何变化：</target>
        </trans-unit>
        <trans-unit id="0b4e5d16e2ae8075091f7ec9c797f8dd043b5cfe" translate="yes" xml:space="preserve">
          <source>The following example illustrates how the decision regions may change when a soft &lt;code&gt;VotingClassifier&lt;/code&gt; is used based on an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier:</source>
          <target state="translated">以下示例说明了基于线性支持向量机，决策树和K近邻分类器使用软 &lt;code&gt;VotingClassifier&lt;/code&gt; 时决策区域如何变化：</target>
        </trans-unit>
        <trans-unit id="fcb0630f60eb0a1b1e5514ec57d9a7f6cbd21ce7" translate="yes" xml:space="preserve">
          <source>The following example illustrates the effect of scaling the regularization parameter when using &lt;a href=&quot;../../modules/svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; for &lt;a href=&quot;../../modules/svm#svm-classification&quot;&gt;classification&lt;/a&gt;. For SVC classification, we are interested in a risk minimization for the equation:</source>
          <target state="translated">以下示例说明了使用&lt;a href=&quot;../../modules/svm#svm&quot;&gt;支持向量机&lt;/a&gt;进行&lt;a href=&quot;../../modules/svm#svm-classification&quot;&gt;分类&lt;/a&gt;时缩放正则化参数的效果。对于SVC分类，我们对方程的风险最小化感兴趣：</target>
        </trans-unit>
        <trans-unit id="35227ebb51bdeaa9a401c5e44c0ca9ca35e1f0e6" translate="yes" xml:space="preserve">
          <source>The following example shows a color-coded representation of the relative importances of each individual pixel for a face recognition task using a &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt;&lt;code&gt;ExtraTreesClassifier&lt;/code&gt;&lt;/a&gt; model.</source>
          <target state="translated">以下示例显示了使用&lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt; &lt;code&gt;ExtraTreesClassifier&lt;/code&gt; &lt;/a&gt;模型以彩色编码表示的每个单独像素在面部识别任务中的相对重要性。</target>
        </trans-unit>
        <trans-unit id="e4e3a609b2ed09432a121fc3917b934875c3544c" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit an AdaBoost classifier with 100 weak learners:</source>
          <target state="translated">下面的例子展示了如何用100个弱学习者来拟合AdaBoost分类器。</target>
        </trans-unit>
        <trans-unit id="08beea172467f4b9200a143a3d6a2c2ed164664c" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit the VotingRegressor:</source>
          <target state="translated">下面的例子显示了如何适应VotingRegressor。</target>
        </trans-unit>
        <trans-unit id="486be98f63ccda08601d245e4a85066d3abba297" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit the majority rule classifier:</source>
          <target state="translated">下面的例子显示了如何拟合多数规则分类器。</target>
        </trans-unit>
        <trans-unit id="6a236adbadcca90d4c1ec977d69fa3c364957c12" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the 5 most informative features in the Friedman #1 dataset.</source>
          <target state="translated">下面的例子显示了如何在Friedman #1数据集中检索5个信息量最大的特征。</target>
        </trans-unit>
        <trans-unit id="376fe34989034d77dfd504e8201c3d4b2dfa1573" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the 5 right informative features in the Friedman #1 dataset.</source>
          <target state="translated">下面的例子显示了如何检索Friedman #1数据集中的5个正确的信息特征。</target>
        </trans-unit>
        <trans-unit id="25b5efc453e06d92e8572618fefd1930c5c8aab3" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the a-priori not known 5 informative features in the Friedman #1 dataset.</source>
          <target state="translated">下面的例子显示了如何在Friedman #1数据集中检索先验不知道的5个信息特征。</target>
        </trans-unit>
        <trans-unit id="e99af0f029f79715308d4d1c9ecbe2acb8983e4d" translate="yes" xml:space="preserve">
          <source>The following example will, for instance, transform some British spelling to American spelling:</source>
          <target state="translated">例如,下面的例子将把一些英式拼法转化为美式拼法。</target>
        </trans-unit>
        <trans-unit id="ad9262c5496a82045b0e3b64071f384beed06659" translate="yes" xml:space="preserve">
          <source>The following experiment is performed on an artificial dataset for binary classification with 100,000 samples (1,000 of them are used for model fitting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The figure shows the estimated probabilities obtained with logistic regression, a linear support-vector classifier (SVC), and linear SVC with both isotonic calibration and sigmoid calibration. The Brier score is a metric which is a combination of calibration loss and refinement loss, &lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt;&lt;code&gt;brier_score_loss&lt;/code&gt;&lt;/a&gt;, reported in the legend (the smaller the better). Calibration loss is defined as the mean squared deviation from empirical probabilities derived from the slope of ROC segments. Refinement loss can be defined as the expected optimal loss as measured by the area under the optimal cost curve.</source>
          <target state="translated">以下实验是在具有100,000个样本（其中1,000个用于模型拟合）的具有20个特征的二进制分类的人工数据集上进行的。在这20个功能中，只有2个具有信息性，而10个是冗余的。该图显示了通过逻辑回归，线性支持向量分类器（SVC）和具有等渗校准和S形校准的线性SVC获得的估计概率。布里尔得分是一个指标，它是校准损耗和精炼损耗的组合，&lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt; &lt;code&gt;brier_score_loss&lt;/code&gt; &lt;/a&gt;，在图例中报告（越小越好）。校准损失定义为与从ROC段的斜率得出的经验概率的均方差。精炼损失可以定义为通过最佳成本曲线下的面积测得的预期最佳损失。</target>
        </trans-unit>
        <trans-unit id="7e7b7eaddffc735bc9fdc4de0de59e2218717940" translate="yes" xml:space="preserve">
          <source>The following figure compares &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The learned model of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; is plotted, where both complexity/regularization and bandwidth of the RBF kernel have been optimized using grid-search. The learned functions are very similar; however, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is approximately seven times faster than fitting &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; (both with grid-search). However, prediction of 100000 target values is more than three times faster with &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt; since it has learned a sparse model using only approximately 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="translated">下图比较了人工数据集上的&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;，该数据集由正弦目标函数和添加到每五个数据点的强噪声组成。绘制了学习的&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;模型，其中使用网格搜索对RBF内核的复杂度/正则化和带宽进行了优化。学习的功能非常相似。但是，安装&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;大约比安装&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;（两者都使用网格搜索）快7倍。但是，使用&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;预测100000个目标值的速度要快三倍以上 因为它已经学会了仅使用100个训练数据点中的大约1/3作为支持向量的稀疏模型。</target>
        </trans-unit>
        <trans-unit id="2b3bbe94b0947452c8fc360afc3d0e6f8832ae22" translate="yes" xml:space="preserve">
          <source>The following figure compares &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The learned model of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; is plotted, where both complexity/regularization and bandwidth of the RBF kernel have been optimized using grid-search. The learned functions are very similar; however, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is approx. seven times faster than fitting &lt;code&gt;SVR&lt;/code&gt; (both with grid-search). However, prediction of 100000 target values is more than three times faster with SVR since it has learned a sparse model using only approx. 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="translated">下图比较了人工数据集上的&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;和 &lt;code&gt;SVR&lt;/code&gt; ，该数据集由正弦目标函数和强噪声添加到每五个数据点组成。绘制了学习的&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;和 &lt;code&gt;SVR&lt;/code&gt; 模型，其中使用网格搜索对RBF内核的复杂度/正则化和带宽进行了优化。学习的功能非常相似。但是，拟合&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;约为。比安装 &lt;code&gt;SVR&lt;/code&gt; 快7倍（两者都使用网格搜索）。但是，使用SVR预测100000个目标值的速度要快三倍以上，因为它仅使用了约10个像素就学会了稀疏模型。 100个训练数据点的1/3作为支持向量。</target>
        </trans-unit>
        <trans-unit id="7f576baedfe43e5a873ff30c0a921c0894cd8a99" translate="yes" xml:space="preserve">
          <source>The following figure compares the location of the non-zero entries in the coefficient matrix W obtained with a simple Lasso or a MultiTaskLasso. The Lasso estimates yield scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns.</source>
          <target state="translated">下图比较了用简单的Lasso或MultiTaskLasso得到的系数矩阵W中非零项的位置。Lasso估计得到的是分散的非零,而MultiTaskLasso的非零是满列的。</target>
        </trans-unit>
        <trans-unit id="b11b5ffd2f0d8dbd878722867b9b243b2e73a5ea" translate="yes" xml:space="preserve">
          <source>The following figure compares the location of the non-zeros in W obtained with a simple Lasso or a MultiTaskLasso. The Lasso estimates yields scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns.</source>
          <target state="translated">下图比较了用简单的Lasso或MultiTaskLasso得到的W中非零的位置。Lasso估计得到的是分散的非零,而MultiTaskLasso的非零是满列的。</target>
        </trans-unit>
        <trans-unit id="c66df17d5ba5efe635efde7f93de1fc62e75c222" translate="yes" xml:space="preserve">
          <source>The following figure illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise. The figure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited for learning periodic functions. The kernel&amp;rsquo;s hyperparameters control the smoothness (length_scale) and periodicity of the kernel (periodicity). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR.</source>
          <target state="translated">下图说明了在人工数据集上的两种方法，该数据集由正弦目标函数和强噪声组成。该图比较了基于ExpSineSquared内核的KRR和GPR的学习模型，该模型适合于学习周期性函数。内核的超参数控制内核的平滑度（length_scale）和周期性（周期性）。此外，数据的噪声级别由GPR通过内核中的其他WhiteKernel组件以及KRR的正则化参数alpha明确学习。</target>
        </trans-unit>
        <trans-unit id="7af807be2b2e68501b3d7cc153c6244aca835299" translate="yes" xml:space="preserve">
          <source>The following guide focuses on &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt;&lt;code&gt;GradientBoostingClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt;, which might be preferred for small sample sizes since binning may lead to split points that are too approximate in this setting.</source>
          <target state="translated">以下指南重点介绍&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier&quot;&gt; &lt;code&gt;GradientBoostingClassifier&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;，这对于小样本量可能是首选，因为合并可能导致在此设置中分裂点过于近似。</target>
        </trans-unit>
        <trans-unit id="c2a01fd3fe0b5b9cd28910783764aacd71509b19" translate="yes" xml:space="preserve">
          <source>The following illustrate the probability density functions of the target before and after applying the logarithmic functions.</source>
          <target state="translated">下面说明应用对数函数前后目标的概率密度函数。</target>
        </trans-unit>
        <trans-unit id="b0293fdb9cc21af9db21d1f9f61ccde2a4565147" translate="yes" xml:space="preserve">
          <source>The following image shows how a dictionary learned from 4x4 pixel image patches extracted from part of the image of a raccoon face looks like.</source>
          <target state="translated">下图显示了从浣熊脸部的部分图像中提取的4x4像素图像补丁所学习的字典是怎样的。</target>
        </trans-unit>
        <trans-unit id="a988a0a50289dff96522065b976c597b6e4e63e8" translate="yes" xml:space="preserve">
          <source>The following image shows on the data above the estimated probability using a Gaussian naive Bayes classifier without calibration, with a sigmoid calibration and with a non-parametric isotonic calibration. One can observe that the non-parametric model provides the most accurate probability estimates for samples in the middle, i.e., 0.5.</source>
          <target state="translated">下图在上面的数据上显示了使用高斯奈夫贝叶斯分类器在没有校准的情况下,使用sigmoid校准和使用非参数同位素校准的估计概率。可以观察到,非参数模型为中间的样本提供了最准确的概率估计,即0.5。</target>
        </trans-unit>
        <trans-unit id="b45c55863d114ed9459a8475837831820ff65766" translate="yes" xml:space="preserve">
          <source>The following images demonstrate the benefit of probability calibration. The first image present a dataset with 2 classes and 3 blobs of data. The blob in the middle contains random samples of each class. The probability for the samples in this blob should be 0.5.</source>
          <target state="translated">下面的图片展示了概率校准的好处。第一张图片展示了一个有2个类和3个数据块的数据集。中间的blob包含每个类的随机样本。这个blob中样本的概率应该是0.5。</target>
        </trans-unit>
        <trans-unit id="ba49479bf1cc39dce48fe5a925ee39da6796af0a" translate="yes" xml:space="preserve">
          <source>The following lists the string metric identifiers and the associated distance metric classes:</source>
          <target state="translated">下面列出了字符串度量标识符和相关的距离度量类。</target>
        </trans-unit>
        <trans-unit id="807c0e6975f025a025e2951d9c9164c1f3454c71" translate="yes" xml:space="preserve">
          <source>The following loss functions are supported and can be specified using the parameter &lt;code&gt;loss&lt;/code&gt;:</source>
          <target state="translated">支持以下损失函数，可以使用参数 &lt;code&gt;loss&lt;/code&gt; 指定：</target>
        </trans-unit>
        <trans-unit id="f56ebb3690526678e7e0211eddd43dd69c89de3e" translate="yes" xml:space="preserve">
          <source>The following parts are parallelized:</source>
          <target state="translated">以下部分是平行的。</target>
        </trans-unit>
        <trans-unit id="69eaa3387fa8263cde077e4bb7511faccc7173e0" translate="yes" xml:space="preserve">
          <source>The following plot compares how well the probabilistic predictions of different classifiers are calibrated, using &lt;a href=&quot;generated/sklearn.calibration.calibration_curve#sklearn.calibration.calibration_curve&quot;&gt;&lt;code&gt;calibration_curve&lt;/code&gt;&lt;/a&gt;. The x axis represents the average predicted probability in each bin. The y axis is the &lt;em&gt;fraction of positives&lt;/em&gt;, i.e. the proportion of samples whose class is the positive class (in each bin).</source>
          <target state="translated">下图比较了使用&lt;a href=&quot;generated/sklearn.calibration.calibration_curve#sklearn.calibration.calibration_curve&quot;&gt; &lt;code&gt;calibration_curve&lt;/code&gt; &lt;/a&gt;不同分类器的概率预测的程度。x轴表示每个bin中的平均预测概率。y轴是正数的&lt;em&gt;分数&lt;/em&gt;，即，类别为正类别（在每个仓中）的样本比例。</target>
        </trans-unit>
        <trans-unit id="70624e811991b8fac875c2f0dd0db6a8fb2e1e2b" translate="yes" xml:space="preserve">
          <source>The following plots demonstrate the impact of the number of clusters and number of samples on various clustering performance evaluation metrics.</source>
          <target state="translated">下面的图展示了聚类数量和样本数量对各种聚类性能评价指标的影响。</target>
        </trans-unit>
        <trans-unit id="661cd7d1cca4025c0fd4f4a1e938e7140f6ef24b" translate="yes" xml:space="preserve">
          <source>The following sections contain further explanations and examples that illustrate how the tf-idfs are computed exactly and how the tf-idfs computed in scikit-learn&amp;rsquo;s &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;TfidfVectorizer&lt;/code&gt;&lt;/a&gt; differ slightly from the standard textbook notation that defines the idf as</source>
          <target state="translated">以下各节包含进一步的说明和示例，这些示例和示例说明了如何精确计算tf-idfs，以及scikit-learn的&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;TfidfVectorizer&lt;/code&gt; 中&lt;/a&gt;计算的tf-idfs 与标准教科书符号（将idf定义为</target>
        </trans-unit>
        <trans-unit id="78ad5f101e233b65633dffc5c68bec29c1eae0b0" translate="yes" xml:space="preserve">
          <source>The following sections list utilities to generate indices that can be used to generate dataset splits according to different cross validation strategies.</source>
          <target state="translated">以下各节列出了用于生成指数的实用程序,这些指数可用于根据不同的交叉验证策略生成数据集分割。</target>
        </trans-unit>
        <trans-unit id="999c74375c41b701ce62b520b704cf4b739a66b1" translate="yes" xml:space="preserve">
          <source>The following snippet demonstrates how to replace missing values, encoded as &lt;code&gt;np.nan&lt;/code&gt;, using the mean feature value of the two nearest neighbors of samples with missing values:</source>
          <target state="translated">以下代码段演示了如何使用具有缺失值的样本的两个最近邻居的平均特征值替换编码为 &lt;code&gt;np.nan&lt;/code&gt; 的缺失值：</target>
        </trans-unit>
        <trans-unit id="5c75d78bf5e5b93a438a72e22f9eee2db09a82ed" translate="yes" xml:space="preserve">
          <source>The following snippet demonstrates how to replace missing values, encoded as &lt;code&gt;np.nan&lt;/code&gt;, using the mean value of the columns (axis 0) that contain the missing values:</source>
          <target state="translated">以下代码段演示了如何使用包含缺失值的列（轴0）的平均值替换编码为 &lt;code&gt;np.nan&lt;/code&gt; 的缺失值：</target>
        </trans-unit>
        <trans-unit id="35e4b9d817818dd84b86f006c91682ff56db6021" translate="yes" xml:space="preserve">
          <source>The following subsections are only rough guidelines: the same estimator can fall into multiple categories, depending on its parameters.</source>
          <target state="translated">以下各小节只是粗略的准则:同一估算器可以根据其参数分为多个类别。</target>
        </trans-unit>
        <trans-unit id="b6c9fc0377f0a1e8adb0ffabc9be6ea63e8b1dd6" translate="yes" xml:space="preserve">
          <source>The following table lists some specific EDMs and their unit deviance (all of these are instances of the Tweedie family):</source>
          <target state="translated">下表列出了一些具体的EDM及其单位偏差(这些都是特威迪家族的实例)。</target>
        </trans-unit>
        <trans-unit id="8aa5b699e7b8122b521ff7ffa5779f4500a14f28" translate="yes" xml:space="preserve">
          <source>The following table summarizes the penalties supported by each solver:</source>
          <target state="translated">下表总结了各解算器所支持的惩罚。</target>
        </trans-unit>
        <trans-unit id="9d55ab5466e0380dae4e49e3b2bfc5d99624d130" translate="yes" xml:space="preserve">
          <source>The following toy example demonstrates how the model ignores the samples with zero sample weights:</source>
          <target state="translated">下面的玩具示例演示了模型如何忽略样本权重为零的样本。</target>
        </trans-unit>
        <trans-unit id="46781e9d9b616f25e94b9f1718ce2c06cffa693e" translate="yes" xml:space="preserve">
          <source>The following two references explain the iterations used in the coordinate descent solver of scikit-learn, as well as the duality gap computation used for convergence control.</source>
          <target state="translated">以下两个参考文献解释了scikit-learn的坐标下降求解器中使用的迭代,以及用于收敛控制的二元性缺口计算。</target>
        </trans-unit>
        <trans-unit id="5c7ef4cd14485e27e01a8eae72d8f974a84e41ab" translate="yes" xml:space="preserve">
          <source>The form of the model learned by &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is identical to support vector regression (&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;). However, different loss functions are used: KRR uses squared error loss while support vector regression uses \(\epsilon\)-insensitive loss, both combined with l2 regularization. In contrast to &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than &lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt;&lt;code&gt;SVR&lt;/code&gt;&lt;/a&gt;, which learns a sparse model for \(\epsilon &amp;gt; 0\), at prediction-time.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;学习的模型的形式与支持向量回归（&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;）相同。但是，使用了不同的损失函数：KRR使用平方误差损失，而支持向量回归使用对\（\ epsilon \）不敏感的损失，二者均与12正则化结合。与&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; 相比&lt;/a&gt;，拟合&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;可以封闭形式进行，对于中等规模的数据集通常更快。另一方面，学习的模型是非稀疏的，因此比&lt;a href=&quot;generated/sklearn.svm.svr#sklearn.svm.SVR&quot;&gt; &lt;code&gt;SVR&lt;/code&gt; &lt;/a&gt;慢，后者在预测时学习\（\ epsilon&amp;gt; 0 \）的稀疏模型。</target>
        </trans-unit>
        <trans-unit id="d492404a63c6d65da32b70e6dcf87e75a94b6a4d" translate="yes" xml:space="preserve">
          <source>The form of the model learned by &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is identical to support vector regression (&lt;code&gt;SVR&lt;/code&gt;). However, different loss functions are used: KRR uses squared error loss while support vector regression uses \(\epsilon\)-insensitive loss, both combined with l2 regularization. In contrast to &lt;code&gt;SVR&lt;/code&gt;, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for \(\epsilon &amp;gt; 0\), at prediction-time.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;学习的模型的形式与支持向量回归（ &lt;code&gt;SVR&lt;/code&gt; ）相同。但是，使用了不同的损失函数：KRR使用平方误差损失，而支持向量回归使用对\（\ epsilon \）不敏感的损失，二者均与12正则化结合。与 &lt;code&gt;SVR&lt;/code&gt; 相比，拟合&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;可以封闭形式进行，对于中等规模的数据集通常更快。另一方面，学习的模型是非稀疏的，因此比SVR慢，后者在预测时学习\（\ epsilon&amp;gt; 0 \）的稀疏模型。</target>
        </trans-unit>
        <trans-unit id="cb6eb14fd1b6ffac38e7d6c15d36b7076beafa85" translate="yes" xml:space="preserve">
          <source>The form of the model learned by KRR is identical to support vector regression (SVR). However, different loss functions are used: KRR uses squared error loss while support vector regression uses epsilon-insensitive loss, both combined with l2 regularization. In contrast to SVR, fitting a KRR model can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for epsilon &amp;gt; 0, at prediction-time.</source>
          <target state="translated">KRR学习的模型的形式与支持向量回归（SVR）相同。但是，使用了不同的损失函数：KRR使用平方误差损失，而支持向量回归使用epsilon不敏感损失，两者均与12正则化结合。与SVR相比，拟合KRR模型可以封闭形式进行，对于中等规模的数据集通常更快。另一方面，学习的模型是非稀疏的，因此比SVR慢，后者在预测时学习epsilon&amp;gt; 0的稀疏模型。</target>
        </trans-unit>
        <trans-unit id="785bfd19f7d4d298d07ec2673131bad97bee7c7c" translate="yes" xml:space="preserve">
          <source>The form of these kernels is as follows:</source>
          <target state="translated">这些内核的形式如下:</target>
        </trans-unit>
        <trans-unit id="cecc632cfa798838e6560a2f37e713a3ae87c1a6" translate="yes" xml:space="preserve">
          <source>The formula that is used to compute the tf-idf for a term t of a document d in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is computed as idf(t) = log [ n / df(t) ] + 1 (if &lt;code&gt;smooth_idf=False&lt;/code&gt;), where n is the total number of documents in the document set and df(t) is the document frequency of t; the document frequency is the number of documents in the document set that contain the term t. The effect of adding &amp;ldquo;1&amp;rdquo; to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(t) = log [ n / (df(t) + 1) ]).</source>
          <target state="translated">用于计算文档集中文档d中项t的tf-idf的公式为tf-idf（t，d）= tf（t，d）* idf（t），并且计算了idf如idf（t）= log [n / df（t）] + 1（如果 &lt;code&gt;smooth_idf=False&lt;/code&gt; ），其中n是文档集中的文档总数，而df（t）是t的文档频率；文档频率是文档集中包含项t的文档数。在上式中的idf中添加&amp;ldquo; 1&amp;rdquo;的效果是idf为零的项（即出现在训练集中所有文档中的项）将不会被完全忽略。 （请注意，上面的idf公式不同于将idf定义为idf（t）= log [n /（df（t）+ 1）的标准教科书符号）。）</target>
        </trans-unit>
        <trans-unit id="99fa4cbad7a403537a73165a2b1d717c58bb9b6b" translate="yes" xml:space="preserve">
          <source>The formula that is used to compute the tf-idf of term t is tf-idf(d, t) = tf(t) * idf(d, t), and the idf is computed as idf(d, t) = log [ n / df(d, t) ] + 1 (if &lt;code&gt;smooth_idf=False&lt;/code&gt;), where n is the total number of documents and df(d, t) is the document frequency; the document frequency is the number of documents d that contain term t. The effect of adding &amp;ldquo;1&amp;rdquo; to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(d, t) = log [ n / (df(d, t) + 1) ]).</source>
          <target state="translated">用于计算项t的tf-idf的公式为tf-idf（d，t）= tf（t）* idf（d，t），并且idf计算为idf（d，t）= log [n / df（d，t）] + 1（如果 &lt;code&gt;smooth_idf=False&lt;/code&gt; ），其中n是文档总数，df（d，t）是文档频率；文档频率是包含项t的文档数d。在上式中的idf中添加&amp;ldquo; 1&amp;rdquo;的效果是，不会完全忽略idf为零的项，即出现在训练集中所有文档中的项。 （请注意，上面的idf公式不同于标准教科书中将idf定义为idf（d，t）= log [n /（df（d，t）+ 1）]的符号）。</target>
        </trans-unit>
        <trans-unit id="2757038bccf5826fff274cfe8684f779a3893302" translate="yes" xml:space="preserve">
          <source>The formula used here does not correspond to the one given in the article. In the original article, formula (23) states that 2/p is multiplied by Trace(cov*cov) in both the numerator and denominator, but this operation is omitted because for a large p, the value of 2/p is so small that it doesn&amp;rsquo;t affect the value of the estimator.</source>
          <target state="translated">这里使用的公式与文章中给出的公式不对应。在原始文章中，公式（23）指出在分子和分母中将2 / p乘以Trace（cov * cov），但是省略了此运算，因为对于大的p，2 / p的值是如此之小它不会影响估计器的值。</target>
        </trans-unit>
        <trans-unit id="25426ebad0f81610af376f208a1d7f7cdb7c96c5" translate="yes" xml:space="preserve">
          <source>The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. &lt;code&gt;subsample&lt;/code&gt; interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;. Choosing &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; leads to a reduction of variance and an increase in bias.</source>
          <target state="translated">用于拟合各个基础学习者的样本比例。如果小于1.0，则将导致随机梯度增强。 &lt;code&gt;subsample&lt;/code&gt; 与参数 &lt;code&gt;n_estimators&lt;/code&gt; 交互。选择 &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; 会导致方差减少和偏差增加。</target>
        </trans-unit>
        <trans-unit id="568fbbbfe83b2ef390104a99310641880cb62ce5" translate="yes" xml:space="preserve">
          <source>The fraction of samples to be used in each randomized design. Should be between 0 and 1. If 1, all samples are used.</source>
          <target state="translated">每个随机设计中使用的样本分数。应该在0和1之间,如果是1,则使用所有样本。</target>
        </trans-unit>
        <trans-unit id="52b16a6d5efc38a8b1d594773d860718286a7706" translate="yes" xml:space="preserve">
          <source>The fraction of samples whose class are randomly exchanged. Larger values introduce noise in the labels and make the classification task harder.</source>
          <target state="translated">类随机交换的样本的分数。较大的数值会在标签中引入噪声,使分类任务更加困难。</target>
        </trans-unit>
        <trans-unit id="3e4c940191ac2ce629537cccbeb4e1290fc15133" translate="yes" xml:space="preserve">
          <source>The fraction of samples whose class is assigned randomly. Larger values introduce noise in the labels and make the classification task harder. Note that the default setting flip_y &amp;gt; 0 might lead to less than n_classes in y in some cases.</source>
          <target state="translated">随机分配类别的样本所占的比例。较大的值会在标签中引入噪音，并使分类任务更加困难。请注意，在某些情况下，默认设置flip_y&amp;gt; 0可能导致y中的n_class类少于n_class。</target>
        </trans-unit>
        <trans-unit id="2b8586795acd1de1d253165ce5ff74dbee8020e0" translate="yes" xml:space="preserve">
          <source>The free parameters in the model are C and epsilon.</source>
          <target state="translated">模型中的自由参数为C和epsilon。</target>
        </trans-unit>
        <trans-unit id="852901dbf93c375963721fc84a4888e6b45c78dd" translate="yes" xml:space="preserve">
          <source>The full description of the dataset</source>
          <target state="translated">数据集的完整描述</target>
        </trans-unit>
        <trans-unit id="d9127a02d65d598dd00fad372a9aa50b138962b6" translate="yes" xml:space="preserve">
          <source>The full description of the dataset.</source>
          <target state="translated">数据集的完整描述;</target>
        </trans-unit>
        <trans-unit id="19877feb57bb09f6d25fbaaa466f42d45ca4d944" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.feature_extraction.image.img_to_graph#sklearn.feature_extraction.image.img_to_graph&quot;&gt;&lt;code&gt;img_to_graph&lt;/code&gt;&lt;/a&gt; returns such a matrix from a 2D or 3D image. Similarly, &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt;&lt;code&gt;grid_to_graph&lt;/code&gt;&lt;/a&gt; build a connectivity matrix for images given the shape of these image.</source>
          <target state="translated">函数&lt;a href=&quot;generated/sklearn.feature_extraction.image.img_to_graph#sklearn.feature_extraction.image.img_to_graph&quot;&gt; &lt;code&gt;img_to_graph&lt;/code&gt; &lt;/a&gt;从2D或3D图像返回这样的矩阵。类似地，&lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt; &lt;code&gt;grid_to_graph&lt;/code&gt; &lt;/a&gt;在给定图像形状的情况下为图像构建连接矩阵。</target>
        </trans-unit>
        <trans-unit id="0a9dad7d233dabf5b0bcafb3e330d5014a951e15" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.linear_model.lasso_path#sklearn.linear_model.lasso_path&quot;&gt;&lt;code&gt;lasso_path&lt;/code&gt;&lt;/a&gt; is useful for lower-level tasks, as it computes the coefficients along the full path of possible values.</source>
          <target state="translated">函数&lt;a href=&quot;generated/sklearn.linear_model.lasso_path#sklearn.linear_model.lasso_path&quot;&gt; &lt;code&gt;lasso_path&lt;/code&gt; &lt;/a&gt;对于较低级别的任务很有用，因为它沿着可能值的完整路径计算系数。</target>
        </trans-unit>
        <trans-unit id="6bbdd98a7d5d87ddd3bbdfb8569481a5e5e04495" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.cohen_kappa_score#sklearn.metrics.cohen_kappa_score&quot;&gt;&lt;code&gt;cohen_kappa_score&lt;/code&gt;&lt;/a&gt; computes &lt;a href=&quot;https://en.wikipedia.org/wiki/Cohen%27s_kappa&quot;&gt;Cohen&amp;rsquo;s kappa&lt;/a&gt; statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.</source>
          <target state="translated">函数&lt;a href=&quot;generated/sklearn.metrics.cohen_kappa_score#sklearn.metrics.cohen_kappa_score&quot;&gt; &lt;code&gt;cohen_kappa_score&lt;/code&gt; &lt;/a&gt;计算&lt;a href=&quot;https://en.wikipedia.org/wiki/Cohen%27s_kappa&quot;&gt;科恩的kappa&lt;/a&gt;统计信息。此措施旨在比较不同的人类注释者（而不是分类者）与地面事实的标签。</target>
        </trans-unit>
        <trans-unit id="93f21bf6b976a0d8004246b9140ac3f9b0310fba" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.laplacian_kernel#sklearn.metrics.pairwise.laplacian_kernel&quot;&gt;&lt;code&gt;laplacian_kernel&lt;/code&gt;&lt;/a&gt; is a variant on the radial basis function kernel defined as:</source>
          <target state="translated">函数&lt;a href=&quot;generated/sklearn.metrics.pairwise.laplacian_kernel#sklearn.metrics.pairwise.laplacian_kernel&quot;&gt; &lt;code&gt;laplacian_kernel&lt;/code&gt; &lt;/a&gt;是径向基函数内核的一个变体，定义为：</target>
        </trans-unit>
        <trans-unit id="3fe1ad7d2ae4a1d31d30a7b5c63e81d53b7568bb" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt;&lt;code&gt;linear_kernel&lt;/code&gt;&lt;/a&gt; computes the linear kernel, that is, a special case of &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt;&lt;code&gt;polynomial_kernel&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;degree=1&lt;/code&gt; and &lt;code&gt;coef0=0&lt;/code&gt; (homogeneous). If &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are column vectors, their linear kernel is:</source>
          <target state="translated">功能&lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt; &lt;code&gt;linear_kernel&lt;/code&gt; &lt;/a&gt;计算线性核，也就是，的一种特殊情况&lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt; &lt;code&gt;polynomial_kernel&lt;/code&gt; &lt;/a&gt;与 &lt;code&gt;degree=1&lt;/code&gt; 和 &lt;code&gt;coef0=0&lt;/code&gt; （均相）。如果 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 是列向量，则它们的线性核为：</target>
        </trans-unit>
        <trans-unit id="72295044331d14363e1e1fe37cea504256d7c67f" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt;&lt;code&gt;polynomial_kernel&lt;/code&gt;&lt;/a&gt; computes the degree-d polynomial kernel between two vectors. The polynomial kernel represents the similarity between two vectors. Conceptually, the polynomial kernels considers not only the similarity between vectors under the same dimension, but also across dimensions. When used in machine learning algorithms, this allows to account for feature interaction.</source>
          <target state="translated">函数&lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt; &lt;code&gt;polynomial_kernel&lt;/code&gt; &lt;/a&gt;计算两个向量之间的度数多项式内核。多项式核代表两个向量之间的相似性。从概念上讲，多项式内核不仅考虑相同维度下向量之间的相似性，而且考虑跨维度。在机器学习算法中使用时，这可以解决要素交互问题。</target>
        </trans-unit>
        <trans-unit id="2f5b051b13aa3478b0b4a17587fa0bec1e6b87b4" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.rbf_kernel#sklearn.metrics.pairwise.rbf_kernel&quot;&gt;&lt;code&gt;rbf_kernel&lt;/code&gt;&lt;/a&gt; computes the radial basis function (RBF) kernel between two vectors. This kernel is defined as:</source>
          <target state="translated">函数&lt;a href=&quot;generated/sklearn.metrics.pairwise.rbf_kernel#sklearn.metrics.pairwise.rbf_kernel&quot;&gt; &lt;code&gt;rbf_kernel&lt;/code&gt; &lt;/a&gt;计算两个向量之间的径向基函数（RBF）内核。该内核定义为：</target>
        </trans-unit>
        <trans-unit id="a4ac87fe4bd82908551f017a1e984b845dfe00ca" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.sigmoid_kernel#sklearn.metrics.pairwise.sigmoid_kernel&quot;&gt;&lt;code&gt;sigmoid_kernel&lt;/code&gt;&lt;/a&gt; computes the sigmoid kernel between two vectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer Perceptron (because, in the neural network field, it is often used as neuron activation function). It is defined as:</source>
          <target state="translated">函数&lt;a href=&quot;generated/sklearn.metrics.pairwise.sigmoid_kernel#sklearn.metrics.pairwise.sigmoid_kernel&quot;&gt; &lt;code&gt;sigmoid_kernel&lt;/code&gt; &lt;/a&gt;计算两个向量之间的Sigmoid内核。乙状结肠也称为双曲正切或多层感知器（因为在神经网络领域，它经常被用作神经元激活函数）。它定义为：</target>
        </trans-unit>
        <trans-unit id="2e327cd188c9064345dfa4a6a6764985ae429bc7" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt;&lt;code&gt;roc_curve&lt;/code&gt;&lt;/a&gt; computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;receiver operating characteristic curve, or ROC curve&lt;/a&gt;. Quoting Wikipedia :</source>
          <target state="translated">函数&lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt; &lt;code&gt;roc_curve&lt;/code&gt; &lt;/a&gt;计算&lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;接收器工作特性曲线或ROC曲线&lt;/a&gt;。引用维基百科：</target>
        </trans-unit>
        <trans-unit id="a52cb65c85990e02ab77ee98f8132a1c3b7a7b6c" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; has a similar interface to &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt;, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised).</source>
          <target state="translated">功能&lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt;具有相似的接口&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;，但返回时，为输入中的每个元素，这是该元件得到的，当它是在测试组中的预测。只能使用将所有元素完全一次分配给测试集的交叉验证策略（否则会引发异常）。</target>
        </trans-unit>
        <trans-unit id="9da333e152484b3ac872458a2a577c489d5042de" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.model_selection.validation_curve#sklearn.model_selection.validation_curve&quot;&gt;&lt;code&gt;validation_curve&lt;/code&gt;&lt;/a&gt; can help in this case:</source>
          <target state="translated">在这种情况下，&lt;a href=&quot;generated/sklearn.model_selection.validation_curve#sklearn.model_selection.validation_curve&quot;&gt; &lt;code&gt;validation_curve&lt;/code&gt; &lt;/a&gt;函数可以提供帮助：</target>
        </trans-unit>
        <trans-unit id="ef1bfedb8d96897b52fb746234adf67b6ec8f0b2" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt;&lt;code&gt;normalize&lt;/code&gt;&lt;/a&gt; provides a quick and easy way to perform this operation on a single array-like dataset, either using the &lt;code&gt;l1&lt;/code&gt; or &lt;code&gt;l2&lt;/code&gt; norms:</source>
          <target state="translated">通过使用 &lt;code&gt;l1&lt;/code&gt; 或 &lt;code&gt;l2&lt;/code&gt; 规范，函数&lt;a href=&quot;generated/sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt; &lt;code&gt;normalize&lt;/code&gt; &lt;/a&gt;提供了一种快速简单的方法来对单个类似数组的数据集执行此操作：</target>
        </trans-unit>
        <trans-unit id="621182b188f45e3dd8884ee1d03015ee530041c8" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt;&lt;code&gt;scale&lt;/code&gt;&lt;/a&gt; provides a quick and easy way to perform this operation on a single array-like dataset:</source>
          <target state="translated">函数&lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt; &lt;code&gt;scale&lt;/code&gt; &lt;/a&gt;提供了一种快速简单的方法来对单个类似数组的数据集执行此操作：</target>
        </trans-unit>
        <trans-unit id="6ffd4cf30ad5a8e8b6ec54710ec8f9345471233e" translate="yes" xml:space="preserve">
          <source>The function &lt;code&gt;plot_regression_results&lt;/code&gt; is used to plot the predicted and true targets.</source>
          <target state="translated">函数 &lt;code&gt;plot_regression_results&lt;/code&gt; 用于绘制预测目标和真实目标。</target>
        </trans-unit>
        <trans-unit id="14656519a034ad24c06a19460128dde85e00e72d" translate="yes" xml:space="preserve">
          <source>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in &lt;a href=&quot;#r37d39d7589e2-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; and &lt;a href=&quot;#r37d39d7589e2-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Both methods are based on the idea originally proposed in &lt;a href=&quot;#r37d39d7589e2-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">该函数依赖于非参数方法，该方法基于&lt;a href=&quot;#r37d39d7589e2-2&quot; id=&quot;id2&quot;&gt;[&lt;/a&gt; k]最近邻居距离的熵估计，如[2]和&lt;a href=&quot;#r37d39d7589e2-3&quot; id=&quot;id3&quot;&gt;[3]中所述&lt;/a&gt;。两种方法都基于&lt;a href=&quot;#r37d39d7589e2-4&quot; id=&quot;id4&quot;&gt;[4]中&lt;/a&gt;最初提出的想法。</target>
        </trans-unit>
        <trans-unit id="cf1d60e35345e80734f9e0bef9342c1dede910a1" translate="yes" xml:space="preserve">
          <source>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in &lt;a href=&quot;#r50b872b699c4-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; and &lt;a href=&quot;#r50b872b699c4-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Both methods are based on the idea originally proposed in &lt;a href=&quot;#r50b872b699c4-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">该函数依赖于非参数方法，该方法基于&lt;a href=&quot;#r50b872b699c4-2&quot; id=&quot;id2&quot;&gt;[&lt;/a&gt; k]最近邻居距离的熵估计，如[2]和&lt;a href=&quot;#r50b872b699c4-3&quot; id=&quot;id3&quot;&gt;[3]中所述&lt;/a&gt;。两种方法都基于&lt;a href=&quot;#r50b872b699c4-4&quot; id=&quot;id4&quot;&gt;[4]中&lt;/a&gt;最初提出的想法。</target>
        </trans-unit>
        <trans-unit id="fb8314efd5d7c36c4d228413206cd5331b82e456" translate="yes" xml:space="preserve">
          <source>The function requires either the argument &lt;code&gt;grid&lt;/code&gt; which specifies the values of the target features on which the partial dependence function should be evaluated or the argument &lt;code&gt;X&lt;/code&gt; which is a convenience mode for automatically creating &lt;code&gt;grid&lt;/code&gt; from the training data. If &lt;code&gt;X&lt;/code&gt; is given, the &lt;code&gt;axes&lt;/code&gt; value returned by the function gives the axis for each target feature.</source>
          <target state="translated">该函数需要参数 &lt;code&gt;grid&lt;/code&gt; 该参数网格指定应在其上评估部分依赖函数的目标特征的值）或参数 &lt;code&gt;X&lt;/code&gt; (这是一种方便模式，用于根据训练数据自动创建 &lt;code&gt;grid&lt;/code&gt; 。如果给定 &lt;code&gt;X&lt;/code&gt; ，则函数返回的 &lt;code&gt;axes&lt;/code&gt; 值将给出每个目标特征的轴。</target>
        </trans-unit>
        <trans-unit id="d2fedea237bdc334f6322c4692b523df3b5f6fbf" translate="yes" xml:space="preserve">
          <source>The function to be decorated</source>
          <target state="translated">需要装饰的功能</target>
        </trans-unit>
        <trans-unit id="f7fc9606332ff37416d86060e52aa56595a5e3ce" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;friedman_mse&amp;rdquo; for the mean squared error with improvement score by Friedman, &amp;ldquo;mse&amp;rdquo; for mean squared error, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error. The default value of &amp;ldquo;friedman_mse&amp;rdquo; is generally the best as it can provide a better approximation in some cases.</source>
          <target state="translated">衡量分割质量的功能。支持的标准是&amp;ldquo;弗里德曼_mse&amp;rdquo;（Friedman_mse）表示具有弗里德曼改进得分的均方误差，&amp;ldquo; mse&amp;rdquo;表示均方误差，而&amp;ldquo; mae&amp;rdquo;表示平均绝对误差。通常，默认值&amp;ldquo; friedman_mse&amp;rdquo;是最好的，因为它在某些情况下可以提供更好的近似值。</target>
        </trans-unit>
        <trans-unit id="8c0bede693f27f1257cedfa964bd89180e6bfada" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;gini&amp;rdquo; for the Gini impurity and &amp;ldquo;entropy&amp;rdquo; for the information gain.</source>
          <target state="translated">衡量分割质量的功能。支持的标准是对于基尼杂质为&amp;ldquo;基尼&amp;rdquo;，对于信息增益为&amp;ldquo;熵&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="03e746b06910ee8c07bc239c0b780e5294140dfb" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;gini&amp;rdquo; for the Gini impurity and &amp;ldquo;entropy&amp;rdquo; for the information gain. Note: this parameter is tree-specific.</source>
          <target state="translated">衡量分割质量的功能。支持的标准是对于基尼杂质为&amp;ldquo;基尼&amp;rdquo;，对于信息增益为&amp;ldquo;熵&amp;rdquo;。注意：此参数是特定于树的。</target>
        </trans-unit>
        <trans-unit id="c211c5fb9289c19b7ab7fb609c5f42d0fd7fb417" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;mse&amp;rdquo; for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, &amp;ldquo;friedman_mse&amp;rdquo;, which uses mean squared error with Friedman&amp;rsquo;s improvement score for potential splits, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error, which minimizes the L1 loss using the median of each terminal node.</source>
          <target state="translated">衡量分割质量的功能。支持的标准是均方误差的&amp;ldquo; mse&amp;rdquo;，等于方差减少作为特征选择标准，并且使用每个终端节点的均值&amp;ldquo; friedman_mse&amp;rdquo;来最小化L2损失，该方法使用均方误差和弗里德曼改进分数作为潜在值拆分，并使用&amp;ldquo; mae&amp;rdquo;表示平均绝对误差，使用每个终端节点的中值将L1损耗最小化。</target>
        </trans-unit>
        <trans-unit id="bf422f8c78875448c6e34d2c008baf4a5621c47d" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;mse&amp;rdquo; for the mean squared error, which is equal to variance reduction as feature selection criterion, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error.</source>
          <target state="translated">衡量分割质量的功能。支持的标准是均方误差的&amp;ldquo; mse&amp;rdquo;（等于特征选择标准的方差减少）和均值绝对误差的&amp;ldquo; mae&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="8599be908b4b8dfee70cafa139c14ed8be2f9fde" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;lsquo;friedman_mse&amp;rsquo; for the mean squared error with improvement score by Friedman, &amp;lsquo;mse&amp;rsquo; for mean squared error, and &amp;lsquo;mae&amp;rsquo; for the mean absolute error. The default value of &amp;lsquo;friedman_mse&amp;rsquo; is generally the best as it can provide a better approximation in some cases.</source>
          <target state="translated">衡量分割质量的功能。支持的标准为&amp;ldquo; friedman_mse&amp;rdquo;（均方误差，Friedman对其进行评分），&amp;ldquo; mse&amp;rdquo;（均方误差），&amp;ldquo; mae&amp;rdquo;（均绝对误差）。&amp;ldquo; friedman_mse&amp;rdquo;的默认值通常是最好的，因为在某些情况下它可以提供更好的近似值。</target>
        </trans-unit>
        <trans-unit id="8b3c9548b1b42f5af71c7bfe5f885c712284a1e6" translate="yes" xml:space="preserve">
          <source>The function which is applied on each chunk of the distance matrix, reducing it to needed values. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; is called repeatedly, where &lt;code&gt;D_chunk&lt;/code&gt; is a contiguous vertical slice of the pairwise distance matrix, starting at row &lt;code&gt;start&lt;/code&gt;. It should return an array, a list, or a sparse matrix of length &lt;code&gt;D_chunk.shape[0]&lt;/code&gt;, or a tuple of such objects.</source>
          <target state="translated">应用于距离矩阵的每个块的函数，将其减小为所需的值。 &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; 被重复调用，其中 &lt;code&gt;D_chunk&lt;/code&gt; 是成对距离矩阵的连续垂直切片，从行 &lt;code&gt;start&lt;/code&gt; 开始。它应该返回一个数组，一个列表或一个长度为 &lt;code&gt;D_chunk.shape[0]&lt;/code&gt; 的稀疏矩阵，或者一个此类对象的元组。</target>
        </trans-unit>
        <trans-unit id="0114f911a9d8b12f31f5da2c60626b44d9e0ba26" translate="yes" xml:space="preserve">
          <source>The function which is applied on each chunk of the distance matrix, reducing it to needed values. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; is called repeatedly, where &lt;code&gt;D_chunk&lt;/code&gt; is a contiguous vertical slice of the pairwise distance matrix, starting at row &lt;code&gt;start&lt;/code&gt;. It should return one of: None; an array, a list, or a sparse matrix of length &lt;code&gt;D_chunk.shape[0]&lt;/code&gt;; or a tuple of such objects. Returning None is useful for in-place operations, rather than reductions.</source>
          <target state="translated">该函数应用于距离矩阵的每个块，将其减小为所需的值。 &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; 被重复调用，其中 &lt;code&gt;D_chunk&lt;/code&gt; 是成对距离矩阵的连续垂直切片，从行 &lt;code&gt;start&lt;/code&gt; 开始。它应该返回以下之一：无；长度为 &lt;code&gt;D_chunk.shape[0]&lt;/code&gt; 的数组，列表或稀疏矩阵；或此类对象的元组。不返回任何值对于就地操作很有用，而不是减少操作。</target>
        </trans-unit>
        <trans-unit id="34f983a96e6e1ba7a36ae52fb70df739e3a5aca9" translate="yes" xml:space="preserve">
          <source>The functional form of the G function used in the approximation to neg-entropy. Could be either &amp;lsquo;logcosh&amp;rsquo;, &amp;lsquo;exp&amp;rsquo;, or &amp;lsquo;cube&amp;rsquo;. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. Example:</source>
          <target state="translated">G函数的函数形式，用于近似负熵。可以是&amp;ldquo; logcosh&amp;rdquo;，&amp;ldquo; exp&amp;rdquo;或&amp;ldquo; cube&amp;rdquo;。您也可以提供自己的功能。它应该返回一个包含该函数值及其派生值的元组。例：</target>
        </trans-unit>
        <trans-unit id="f3580ae3ff67a44925f6075f1fc46034cd1f3d14" translate="yes" xml:space="preserve">
          <source>The generated array.</source>
          <target state="translated">生成的数组。</target>
        </trans-unit>
        <trans-unit id="22dd341f8c92381f47326cd6fbe4fff46dd59a6b" translate="yes" xml:space="preserve">
          <source>The generated matrix.</source>
          <target state="translated">生成的矩阵。</target>
        </trans-unit>
        <trans-unit id="b670a7959baa25f4a9c290d5df951114cede8bad" translate="yes" xml:space="preserve">
          <source>The generated samples.</source>
          <target state="translated">生成的样品。</target>
        </trans-unit>
        <trans-unit id="61a0705074aa05bf429a2ff6dd9f09842d1c86f2" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the centers. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">生成器用于初始化中心。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="17ea6c727e52953fb5e03ae24f1b3fcbc132ab70" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the centers. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">生成器用于初始化中心。为多个函数调用传递可重复输出的int值。请参阅&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="473c7ed23812d4bf8fcf6c97ab0551b7301ccf95" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the codebook. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">用于初始化密码本的生成器。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="a0fb8deb3b7c972152c6c4cb8a6e10d0c386fcd5" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the codebook. Pass an int for reproducible output across multiple function calls. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">用于初始化代码本的生成器。为多个函数调用传递可重复输出的int值。请参阅&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4e831547e83750929880db075c40fc4aa61eb4b8" translate="yes" xml:space="preserve">
          <source>The generator used to randomize the design. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">生成器用于随机化设计。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="cd84f22d9e60e3f86bb2455d78aaf109647d76b1" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select a subset of samples. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;sample_size is not None&lt;/code&gt;.</source>
          <target state="translated">生成器用于随机选择样本子集。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。当 &lt;code&gt;sample_size is not None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="36aa9d7d033388d006150bbc613bf9f61afb2f31" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select the samples from input points for bandwidth estimation. Use an int to make the randomness deterministic. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">该发生器用于从输入点随机选择样本以进行带宽估计。使用int可以确定随机性。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="eebbb245bcabbcd7cdd05b74aa699ac85737839b" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select the samples from input points for bandwidth estimation. Use an int to make the randomness deterministic. See &lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">该发生器用于从输入点随机选择样本以进行带宽估计。使用int可以确定随机性。请参阅&lt;a href=&quot;https://scikit-learn.org/0.23/glossary.html#term-random-state&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="ff752b880533b887a66601fd8d804091da8b44b7" translate="yes" xml:space="preserve">
          <source>The goal is to compare different estimators to see which one is best for the &lt;a href=&quot;../../modules/generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt;&lt;code&gt;sklearn.impute.IterativeImputer&lt;/code&gt;&lt;/a&gt; when using a &lt;a href=&quot;../../modules/generated/sklearn.linear_model.bayesianridge#sklearn.linear_model.BayesianRidge&quot;&gt;&lt;code&gt;sklearn.linear_model.BayesianRidge&lt;/code&gt;&lt;/a&gt; estimator on the California housing dataset with a single value randomly removed from each row.</source>
          <target state="translated">目标是比较不同的估算器，以查看在加利福尼亚住房数据集中使用&lt;a href=&quot;../../modules/generated/sklearn.linear_model.bayesianridge#sklearn.linear_model.BayesianRidge&quot;&gt; &lt;code&gt;sklearn.linear_model.BayesianRidge&lt;/code&gt; &lt;/a&gt;估算器时，哪一个最适合&lt;a href=&quot;../../modules/generated/sklearn.impute.iterativeimputer#sklearn.impute.IterativeImputer&quot;&gt; &lt;code&gt;sklearn.impute.IterativeImputer&lt;/code&gt; &lt;/a&gt;，并且从各行中随机删除一个值。</target>
        </trans-unit>
        <trans-unit id="6c9c479a6511fc655c70a7c51ab0a85c04034b21" translate="yes" xml:space="preserve">
          <source>The goal is to measure the latency one can expect when doing predictions either in bulk or atomic (i.e. one by one) mode.</source>
          <target state="translated">目标是测量在批量或原子(即逐个)模式下进行预测时可以预期的延迟。</target>
        </trans-unit>
        <trans-unit id="95b3715b7309073ce23a5e1ae6bd1c83fb5ab128" translate="yes" xml:space="preserve">
          <source>The goal of &lt;strong&gt;ensemble methods&lt;/strong&gt; is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.</source>
          <target state="translated">&lt;strong&gt;集成方法&lt;/strong&gt;的目标是将使用给定学习算法构建的几个基本估计量的预测结合起来，以提高单个估计量的通用性/鲁棒性。</target>
        </trans-unit>
        <trans-unit id="a736cc9a27a226bf9910cdd3892bdfd0fe07d378" translate="yes" xml:space="preserve">
          <source>The goal of NCA is to learn an optimal linear transformation matrix of size &lt;code&gt;(n_components, n_features)&lt;/code&gt;, which maximises the sum over all samples \(i\) of the probability \(p_i\) that \(i\) is correctly classified, i.e.:</source>
          <target state="translated">NCA的目标是学习一个大小为 &lt;code&gt;(n_components, n_features)&lt;/code&gt; 的最佳线性变换矩阵，该矩阵将对\（i \）正确分类的概率\（p_i \）的所有样本\（i \）的和最大化， IE：</target>
        </trans-unit>
        <trans-unit id="4be143708c968fbd1f8e4e8288622de6f1251f81" translate="yes" xml:space="preserve">
          <source>The goal of this example is to analyze the graph of links inside wikipedia articles to rank articles by relative importance according to this eigenvector centrality.</source>
          <target state="translated">这个例子的目标是分析维基百科文章里面的链接图,根据这个特征向量的中心性对文章进行相对重要性排名。</target>
        </trans-unit>
        <trans-unit id="5095c3c5239296131fc48d25860f3771ab9b91e3" translate="yes" xml:space="preserve">
          <source>The goal of this example is to show intuitively how the metrics behave, and not to find good clusters for the digits. This is why the example works on a 2D embedding.</source>
          <target state="translated">这个例子的目标是直观地展示度量的行为,而不是为数字找到好的聚类。这就是为什么这个例子在二维嵌入上工作的原因。</target>
        </trans-unit>
        <trans-unit id="20bd11aa678c9e04777a40ec0f194ab6b7581aa6" translate="yes" xml:space="preserve">
          <source>The goal of this guide is to explore some of the main &lt;code&gt;scikit-learn&lt;/code&gt; tools on a single practical task: analyzing a collection of text documents (newsgroups posts) on twenty different topics.</source>
          <target state="translated">本指南的目的是探索一项实际任务中的一些主要 &lt;code&gt;scikit-learn&lt;/code&gt; 工具：分析有关二十个不同主题的文本文档（新闻组帖子）的集合。</target>
        </trans-unit>
        <trans-unit id="c224302bdf144aacfdfc260a6638fe1107c2e8d4" translate="yes" xml:space="preserve">
          <source>The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.</source>
          <target state="translated">使用tf-idf代替给定文档中token出现的原始频率的目的,是为了降低在给定语料库中出现频率很高的token的影响,因此这些token的经验信息量比在训练语料库中出现的一小部分特征要小。</target>
        </trans-unit>
        <trans-unit id="7c4e551c63ec44d66aa3a175c12b51d46fb0ca3e" translate="yes" xml:space="preserve">
          <source>The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when &lt;code&gt;eval_gradient&lt;/code&gt; is True.</source>
          <target state="translated">内核k（X，X）相对于内核超参数的梯度。仅当 &lt;code&gt;eval_gradient&lt;/code&gt; 为True时返回。</target>
        </trans-unit>
        <trans-unit id="3325084d0d02139eb921a2545ba48cff39657720" translate="yes" xml:space="preserve">
          <source>The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.</source>
          <target state="translated">核k(X,X)相对于核的超参数的梯度.仅当eval_gradient为True时返回。</target>
        </trans-unit>
        <trans-unit id="1d28aeaa2593827c70ee0cbf35f8d5891fd13e08" translate="yes" xml:space="preserve">
          <source>The graph data is fetched from the DBpedia dumps. DBpedia is an extraction of the latent structured data of the Wikipedia content.</source>
          <target state="translated">图形数据是从DBpedia转储中获取的。DBpedia是对维基百科内容的潜在结构化数据的提取。</target>
        </trans-unit>
        <trans-unit id="94a8641249b9d38c741294dd700ba9c013e60d15" translate="yes" xml:space="preserve">
          <source>The graph should contain only one connect component, elsewhere the results make little sense.</source>
          <target state="translated">图形中应该只包含一个连接组件,其他地方的结果意义不大。</target>
        </trans-unit>
        <trans-unit id="0aca214d3abec7143ce0e3db3703c7759829a517" translate="yes" xml:space="preserve">
          <source>The graphical model of LDA is a three-level Bayesian model:</source>
          <target state="translated">LDA的图形模型是一个三层贝叶斯模型。</target>
        </trans-unit>
        <trans-unit id="53c200c93d4342eee587d3c6f82bd52b691b78c8" translate="yes" xml:space="preserve">
          <source>The graphical model of LDA is a three-level generative model:</source>
          <target state="translated">LDA的图形模型是一个三级生成模型。</target>
        </trans-unit>
        <trans-unit id="b70b496cfa90301063390d7d501e40e03165e505" translate="yes" xml:space="preserve">
          <source>The graphical model of an RBM is a fully-connected bipartite graph.</source>
          <target state="translated">RBM的图形模型是一个完全连接的二元图形。</target>
        </trans-unit>
        <trans-unit id="43e8165a75805bcff11ce07ae2103365830550d8" translate="yes" xml:space="preserve">
          <source>The grid of &lt;code&gt;target_variables&lt;/code&gt; values for which the partial dependecy should be evaluated (either &lt;code&gt;grid&lt;/code&gt; or &lt;code&gt;X&lt;/code&gt; must be specified).</source>
          <target state="translated">应该评估部分依赖度的 &lt;code&gt;target_variables&lt;/code&gt; 值的网格（必须指定 &lt;code&gt;grid&lt;/code&gt; 或 &lt;code&gt;X&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="1fb55e8edd66947697c84e91d6ac3fa247bd29e9" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting</source>
          <target state="translated">用于拟合的字母网格</target>
        </trans-unit>
        <trans-unit id="c8ce42c94fb7c4644fc397d481823f721280ec22" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting, for each l1_ratio</source>
          <target state="translated">用于拟合的阿尔法网格,对于每个L1_ratio</target>
        </trans-unit>
        <trans-unit id="92edf0193fba09badba27bf7739525bce6da7601" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting, for each l1_ratio.</source>
          <target state="translated">用于拟合的阿尔法网格,对于每个l1_ratio。</target>
        </trans-unit>
        <trans-unit id="22daba72217df04ec1af1a8a340277c45da4b54e" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting.</source>
          <target state="translated">用于拟合的字母的网格。</target>
        </trans-unit>
        <trans-unit id="37c7f40c413d2aeb11b8645a838ea3c671b821b6" translate="yes" xml:space="preserve">
          <source>The grid points between 0 and 1: alpha/alpha_max</source>
          <target state="translated">0和1之间的网格点:alpha/alpha_max。</target>
        </trans-unit>
        <trans-unit id="1e90c7186240eacb6693334f5a2cb603522a3c95" translate="yes" xml:space="preserve">
          <source>The grid search instance behaves like a normal &lt;code&gt;scikit-learn&lt;/code&gt; model. Let&amp;rsquo;s perform the search on a smaller subset of the training data to speed up the computation:</source>
          <target state="translated">网格搜索实例的行为类似于普通的 &lt;code&gt;scikit-learn&lt;/code&gt; 模型。让我们对训练数据的较小子集执行搜索以加快计算速度：</target>
        </trans-unit>
        <trans-unit id="0fb72e7d32db09c24209afb30ebd0c3ef9469a12" translate="yes" xml:space="preserve">
          <source>The grid search provided by &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; exhaustively generates candidates from a grid of parameter values specified with the &lt;code&gt;param_grid&lt;/code&gt; parameter. For instance, the following &lt;code&gt;param_grid&lt;/code&gt;:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt;提供的网格搜索从使用 &lt;code&gt;param_grid&lt;/code&gt; 参数指定的参数值网格中详尽地生成候选对象。例如，以下 &lt;code&gt;param_grid&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="5d877967ec11cd12214237a4561547e48123553d" translate="yes" xml:space="preserve">
          <source>The guidelines for choosing a metric is to use one that maximizes the distance between samples in different classes, and minimizes that within each class.</source>
          <target state="translated">选择度量的准则是使用一个能使不同类中样本之间的距离最大化,并使每个类内的距离最小化的度量。</target>
        </trans-unit>
        <trans-unit id="42b20fdbf19bf6fb28b9336fb1c931e7ebd9ff54" translate="yes" xml:space="preserve">
          <source>The handwritten digit dataset has 1797 total points. The model will be trained using all points, but only 30 will be labeled. Results in the form of a confusion matrix and a series of metrics over each class will be very good.</source>
          <target state="translated">手写数字数据集共有1797个点。该模型将使用所有的点进行训练,但只有30个点会被标记。以混淆矩阵和一系列指标的形式在每个类上的结果将是非常好的。</target>
        </trans-unit>
        <trans-unit id="1dbebc05eff701a160c5a2149017afcbb50998d6" translate="yes" xml:space="preserve">
          <source>The hash function employed is the signed 32-bit version of Murmurhash3.</source>
          <target state="translated">采用的哈希函数是Murmurhash3的签名32位版本。</target>
        </trans-unit>
        <trans-unit id="86a9e346ee809bf0b27001ea98677db7fe898a9e" translate="yes" xml:space="preserve">
          <source>The higher &lt;code&gt;p&lt;/code&gt; the less weight is given to extreme deviations between true and predicted targets.</source>
          <target state="translated">&lt;code&gt;p&lt;/code&gt; 越高，权重在真实目标和预测目标之间的权重就越小。</target>
        </trans-unit>
        <trans-unit id="6ddf3e17ce9800f43a67078b41067ddc0d8c6a4c" translate="yes" xml:space="preserve">
          <source>The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the simplex.</source>
          <target state="translated">较高的浓度使更多的质量在中心,将导致更多的成分被激活,而较低的浓度参数将导致更多的质量在简单的边缘。</target>
        </trans-unit>
        <trans-unit id="d55d0516aec7ebe6efc3bc1e9f0b83c115b36898" translate="yes" xml:space="preserve">
          <source>The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.</source>
          <target state="translated">越高,特征越重要。一个特征的重要性是以该特征带来的标准的(归一化)总减量来计算的。它也被称为吉尼重要性。</target>
        </trans-unit>
        <trans-unit id="1006ebebebbef4245b7568132e1eabf566a79140" translate="yes" xml:space="preserve">
          <source>The highest p-value for features to be kept.</source>
          <target state="translated">要保留的特征的最高p值。</target>
        </trans-unit>
        <trans-unit id="0c1d204d0071e8e2a8101579dd644bade4aef0d9" translate="yes" xml:space="preserve">
          <source>The highest uncorrected p-value for features to keep.</source>
          <target state="translated">保留特征的最高未经修正的p值。</target>
        </trans-unit>
        <trans-unit id="3708900f6a079bc16cd76c37da5647812d1a0427" translate="yes" xml:space="preserve">
          <source>The histogram of the estimated weights is very peaked, as a sparsity-inducing prior is implied on the weights.</source>
          <target state="translated">估计权重的直方图非常尖锐,因为在权重上隐含了一个稀疏性诱导先验。</target>
        </trans-unit>
        <trans-unit id="d24bdc7c6b1e4dbeb474c6e9f76042366464386e" translate="yes" xml:space="preserve">
          <source>The hyperparameters</source>
          <target state="translated">超参数</target>
        </trans-unit>
        <trans-unit id="25f29cead3cfba36338bcf026da5a846edf25636" translate="yes" xml:space="preserve">
          <source>The i-th score &lt;code&gt;train_score_[i]&lt;/code&gt; is the deviance (= loss) of the model at iteration &lt;code&gt;i&lt;/code&gt; on the in-bag sample. If &lt;code&gt;subsample == 1&lt;/code&gt; this is the deviance on the training data.</source>
          <target state="translated">第i个得分 &lt;code&gt;train_score_[i]&lt;/code&gt; 是袋内样本在第 &lt;code&gt;i&lt;/code&gt; 次迭代时模型的偏差（=损失）。如果 &lt;code&gt;subsample == 1&lt;/code&gt; 这是训练数据的偏差。</target>
        </trans-unit>
        <trans-unit id="7768180d6d5c29f9be6eb9d80f99cbf93007e25a" translate="yes" xml:space="preserve">
          <source>The i.i.d. assumption is broken if the underlying generative process yield groups of dependent samples.</source>
          <target state="translated">如果底层的生成过程产生了一组依赖性样本,则打破了i.i.d.假设。</target>
        </trans-unit>
        <trans-unit id="d5e94b9dd17268ac8457a9d26f68d07b8365584a" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt;&lt;code&gt;VotingClassifier&lt;/code&gt;&lt;/a&gt; is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.ensemble.votingclassifier#sklearn.ensemble.VotingClassifier&quot;&gt; &lt;code&gt;VotingClassifier&lt;/code&gt; &lt;/a&gt;背后的想法是将概念上不同的机器学习分类器组合在一起，并使用多数表决或平均预测概率（软表决）来预测类标签。这样的分类器可用于一组性能良好的模型，以平衡其各自的弱点。</target>
        </trans-unit>
        <trans-unit id="1f4d07aca22136c4b5a250fb6a846554681fb509" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;a href=&quot;generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt;&lt;code&gt;VotingRegressor&lt;/code&gt;&lt;/a&gt; is to combine conceptually different machine learning regressors and return the average predicted values. Such a regressor can be useful for a set of equally well performing models in order to balance out their individual weaknesses.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.ensemble.votingregressor#sklearn.ensemble.VotingRegressor&quot;&gt; &lt;code&gt;VotingRegressor&lt;/code&gt; &lt;/a&gt;背后的想法是组合概念上不同的机器学习回归器并返回平均预测值。这样的回归变量可用于一组性能良好的模型，以平衡其各自的弱点。</target>
        </trans-unit>
        <trans-unit id="190c7529dfd22ae7ed70d4f84d1deb499abbe749" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;code&gt;VotingClassifier&lt;/code&gt; is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.</source>
          <target state="translated">&lt;code&gt;VotingClassifier&lt;/code&gt; 背后的想法是将概念上不同的机器学习分类器组合在一起，并使用多数表决或平均预测概率（软表决）来预测类标签。这样的分类器可用于一组性能良好的模型，以平衡其各自的弱点。</target>
        </trans-unit>
        <trans-unit id="f4f9efa7d25a183372e2e9ce75bfe20581578033" translate="yes" xml:space="preserve">
          <source>The image as a numpy array: height x width x color</source>
          <target state="translated">图像作为一个numpy数组:高度x宽度x颜色。</target>
        </trans-unit>
        <trans-unit id="77bae2f833c03e6f85b6a43b3aa3ead8f81ce2e5" translate="yes" xml:space="preserve">
          <source>The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms.</source>
          <target state="translated">图像被量化为256个灰度级别,并存储为无符号的8位整数;加载器将把这些值转换为区间[0,1]的浮点值,这对许多算法来说更容易处理。</target>
        </trans-unit>
        <trans-unit id="6db946ce103c288b47eadb4f1488a8fdc91a7488" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients. See &lt;a href=&quot;#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt; for another implementation:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt;类中的实现使用坐标下降作为算法来拟合系数。有关其他实现，请参见&lt;a href=&quot;#least-angle-regression&quot;&gt;最小角度回归&lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="64dabeaa3f8e38c30333615b1b174cbd1348b11e" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt;&lt;code&gt;MultiTaskElasticNet&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt; &lt;code&gt;MultiTaskElasticNet&lt;/code&gt; &lt;/a&gt;类中的实现使用坐标下降作为算法来拟合系数。</target>
        </trans-unit>
        <trans-unit id="555e24352725a2133c35e7a52f56a702cd7f81c4" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt;&lt;code&gt;MultiTaskLasso&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt; &lt;code&gt;MultiTaskLasso&lt;/code&gt; &lt;/a&gt;类中的实现使用坐标下降作为算法来拟合系数。</target>
        </trans-unit>
        <trans-unit id="f9cb17c43bed4736104925d68075ebd6b07ace68" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 2.1 of &lt;a href=&quot;#rw2006&quot; id=&quot;id1&quot;&gt;[RW2006]&lt;/a&gt;. In addition to the API of standard scikit-learn estimators, GaussianProcessRegressor:</source>
          <target state="translated">该实现基于&lt;a href=&quot;#rw2006&quot; id=&quot;id1&quot;&gt;[RW2006]的&lt;/a&gt;算法2.1 。除了标准scikit-learn估算器的API外，GaussianProcessRegressor：</target>
        </trans-unit>
        <trans-unit id="70780cc3dac260d0fb4e85faed94f4134c9dc0ae" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 2.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.</source>
          <target state="translated">其实现是基于Rasmussen和Williams所著的机器学习高斯过程(GPML)的算法2.1。</target>
        </trans-unit>
        <trans-unit id="1904d3648b9818da40270920e1855bd9dcd752d1" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.</source>
          <target state="translated">其实现基于Rasmussen和Williams所著的《机器学习的高斯过程》(GPML)的算法3.1、3.2和5.1。</target>
        </trans-unit>
        <trans-unit id="0eeedaeb1d413ea7240926a8ca5811e478d2d62b" translate="yes" xml:space="preserve">
          <source>The implementation is based on an ensemble of ExtraTreeRegressor. The maximum depth of each tree is set to &lt;code&gt;ceil(log_2(n))&lt;/code&gt; where \(n\) is the number of samples used to build the tree (see (Liu et al., 2008) for more details).</source>
          <target state="translated">该实现基于ExtraTreeRegressor的集合。每棵树的最大深度设置为 &lt;code&gt;ceil(log_2(n))&lt;/code&gt; ，其中\（n \）是用于构建树的样本数（有关更多详细信息，请参阅（Liu等人，2008））。</target>
        </trans-unit>
        <trans-unit id="166f55526ba60cbc129406f3899569f7b0eb4efd" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm.</source>
          <target state="translated">它的实现是基于libsvm的。</target>
        </trans-unit>
        <trans-unit id="cb75b7ddb016087965f5fad7be27fc6c1cb16290" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.</source>
          <target state="translated">实现是基于libsvm的。拟合时间的复杂度与样本数的关系大于二次方,这使得它很难扩展到10000个样本以上的数据集。</target>
        </trans-unit>
        <trans-unit id="fa8767a0fc5ecb59976beb65f6a5637eaddaef83" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to datasets with more than a couple of 10000 samples. For large datasets consider using &lt;a href=&quot;sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVR&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDRegressor&lt;/code&gt;&lt;/a&gt; instead, possibly after a &lt;a href=&quot;sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;sklearn.kernel_approximation.Nystroem&lt;/code&gt;&lt;/a&gt; transformer.</source>
          <target state="translated">该实现基于libsvm。拟合时间的复杂度是样本数量的两倍以上，这使得很难扩展到具有多个10000个样本的数据集。对于大型数据集，请考虑改用&lt;a href=&quot;sklearn.svm.linearsvr#sklearn.svm.LinearSVR&quot;&gt; &lt;code&gt;sklearn.svm.LinearSVR&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;sklearn.linear_model.SGDRegressor&lt;/code&gt; &lt;/a&gt;，可能在&lt;a href=&quot;sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;sklearn.kernel_approximation.Nystroem&lt;/code&gt; &lt;/a&gt;转换器之后。</target>
        </trans-unit>
        <trans-unit id="c37eaa47201071ab85ed7ee61d61deb2b696067c" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using &lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt;&lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt;&lt;/a&gt; instead, possibly after a &lt;a href=&quot;sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;sklearn.kernel_approximation.Nystroem&lt;/code&gt;&lt;/a&gt; transformer.</source>
          <target state="translated">该实现基于libsvm。拟合时间至少与样本数量成平方比例，并且超过成千上万的样本可能不切实际。对于大型数据集，请考虑使用&lt;a href=&quot;sklearn.svm.linearsvc#sklearn.svm.LinearSVC&quot;&gt; &lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;sklearn.linear_model.SGDClassifier&lt;/code&gt; &lt;/a&gt;代替，可能在&lt;a href=&quot;sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;sklearn.kernel_approximation.Nystroem&lt;/code&gt; &lt;/a&gt;转换器之后。</target>
        </trans-unit>
        <trans-unit id="a4dec30aaf65f05fda35ca6a3928a075e5f1587d" translate="yes" xml:space="preserve">
          <source>The implementation is designed to:</source>
          <target state="translated">实施的目的是:</target>
        </trans-unit>
        <trans-unit id="e44f638bc5a5bdc8405d919df1e1a354ca31f099" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt;&lt;code&gt;ensemble.IsolationForest&lt;/code&gt;&lt;/a&gt; is based on an ensemble of &lt;a href=&quot;generated/sklearn.tree.extratreeregressor#sklearn.tree.ExtraTreeRegressor&quot;&gt;&lt;code&gt;tree.ExtraTreeRegressor&lt;/code&gt;&lt;/a&gt;. Following Isolation Forest original paper, the maximum depth of each tree is set to \(\lceil \log_2(n) \rceil\) where \(n\) is the number of samples used to build the tree (see (Liu et al., 2008) for more details).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.ensemble.isolationforest#sklearn.ensemble.IsolationForest&quot;&gt; &lt;code&gt;ensemble.IsolationForest&lt;/code&gt; &lt;/a&gt;的实现基于&lt;a href=&quot;generated/sklearn.tree.extratreeregressor#sklearn.tree.ExtraTreeRegressor&quot;&gt; &lt;code&gt;tree.ExtraTreeRegressor&lt;/code&gt; &lt;/a&gt;的整体。根据Isolation Forest原始论文，每棵树的最大深度设置为\（\ lceil \ log_2（n）\ rceil \），其中\（n \）是用于构建树的样本数（请参阅（Liu等（2008年）以获取更多详细信息）。</target>
        </trans-unit>
        <trans-unit id="08d84e0cf0c38a38100a9dad48e0efc744fd2db8" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; in scikit-learn follows a generalization to a multivariate linear regression model &lt;a href=&quot;#f1&quot; id=&quot;id30&quot;&gt;[8]&lt;/a&gt; using the spatial median which is a generalization of the median to multiple dimensions &lt;a href=&quot;#f2&quot; id=&quot;id31&quot;&gt;[9]&lt;/a&gt;.</source>
          <target state="translated">scikit-learn 中&lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; &lt;/a&gt;的实现遵循使用空间中位数的多元线性回归模型&lt;a href=&quot;#f1&quot; id=&quot;id30&quot;&gt;[8]&lt;/a&gt;的推广，该模型是中位数到多个维度的推广&lt;a href=&quot;#f2&quot; id=&quot;id31&quot;&gt;[9]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="fda50d88e9c81601cfc445f1c078677bdd2f3d40" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; in scikit-learn follows a generalization to a multivariate linear regression model &lt;a href=&quot;#f1&quot; id=&quot;id36&quot;&gt;12&lt;/a&gt; using the spatial median which is a generalization of the median to multiple dimensions &lt;a href=&quot;#f2&quot; id=&quot;id37&quot;&gt;13&lt;/a&gt;.</source>
          <target state="translated">在scikit-learn中&lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; &lt;/a&gt;的实现遵循使用空间中位数的多元线性回归模型&lt;a href=&quot;#f1&quot; id=&quot;id36&quot;&gt;12&lt;/a&gt;的推广，而空间中位数是中位数到多维&lt;a href=&quot;#f2&quot; id=&quot;id37&quot;&gt;13&lt;/a&gt;的推广。</target>
        </trans-unit>
        <trans-unit id="eb185b2e148e3613cdde933dd05a6b32e3b9acbe" translate="yes" xml:space="preserve">
          <source>The implementation of SGD is influenced by the &lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot;&gt;Stochastic Gradient SVM&lt;/a&gt; of L&amp;eacute;on Bottou. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse feature vectors, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from Shalev-Shwartz et al. 2007. For multi-class classification, a &amp;ldquo;one versus all&amp;rdquo; approach is used. We use the truncated gradient algorithm proposed by Tsuruoka et al. 2009 for L1 regularization (and the Elastic Net). The code is written in Cython.</source>
          <target state="translated">SGD的实施受&lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot;&gt;随机梯度SVM影响&lt;/a&gt;莱昂&amp;middot;波托（L&amp;eacute;onBottou）类似于SvmSGD，权重向量表示为标量和向量的乘积，在L2正则化的情况下，该向量允许有效地更新权重。在稀疏特征向量的情况下，以较小的学习率（乘以0.01）更新截距，以说明其更新频率更高的事实。在每个观察到的例子之后，顺序地挑选训练例子，并降低学习率。我们采用了Shalev-Shwartz等人的学习率表。 2007年。对于多类别分类，使用了&amp;ldquo;一对一&amp;rdquo;的方法。我们使用Tsuruoka等人提出的截断梯度算法。 2009年用于L1正则化（和Elastic Net）。该代码用Cython编写。</target>
        </trans-unit>
        <trans-unit id="3db47613e45248213b8540e2c5b681f6afec29a2" translate="yes" xml:space="preserve">
          <source>The implementation of SGD is influenced by the &lt;code&gt;Stochastic Gradient SVM&lt;/code&gt; of &lt;a href=&quot;#id10&quot; id=&quot;id7&quot;&gt;7&lt;/a&gt;. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse input &lt;code&gt;X&lt;/code&gt;, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from &lt;a href=&quot;#id12&quot; id=&quot;id8&quot;&gt;8&lt;/a&gt;. For multi-class classification, a &amp;ldquo;one versus all&amp;rdquo; approach is used. We use the truncated gradient algorithm proposed in &lt;a href=&quot;#id13&quot; id=&quot;id9&quot;&gt;9&lt;/a&gt; for L1 regularization (and the Elastic Net). The code is written in Cython.</source>
          <target state="translated">SGD的实施受&lt;a href=&quot;#id10&quot; id=&quot;id7&quot;&gt;7&lt;/a&gt;的 &lt;code&gt;Stochastic Gradient SVM&lt;/code&gt; 影响。类似于SvmSGD，权重向量表示为标量和向量的乘积，在L2正则化的情况下，该向量允许有效地更新权重。在输入 &lt;code&gt;X&lt;/code&gt; 稀疏的情况下，截距将以较小的学习率（乘以0.01）进行更新，以说明其更新频率更高的事实。在每个观察到的例子之后，训练例子被顺序地拾取，并且学习率降低。我们从&lt;a href=&quot;#id12&quot; id=&quot;id8&quot;&gt;8开始&lt;/a&gt;采用学习率表。对于多类别分类，使用&amp;ldquo;一对一&amp;rdquo;方法。我们使用&lt;a href=&quot;#id13&quot; id=&quot;id9&quot;&gt;9中&lt;/a&gt;提出的截断梯度算法用于L1正则化（和Elastic Net）。该代码用Cython编写。</target>
        </trans-unit>
        <trans-unit id="234c6abfe6fe107b5770fe372b7d9e14789fc069" translate="yes" xml:space="preserve">
          <source>The implementation of logistic regression in scikit-learn can be accessed from class &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;. This implementation can fit binary, One-vs- Rest, or multinomial logistic regression with optional L2 or L1 regularization.</source>
          <target state="translated">可以从&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;类访问scikit-learn中的logistic回归实现。此实现可以通过可选的L2或L1正则化来拟合二进制，一对一静止或多项逻辑回归。</target>
        </trans-unit>
        <trans-unit id="11a45f793137837fb140d6499ba2bfe32aacbae3" translate="yes" xml:space="preserve">
          <source>The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.</source>
          <target state="translated">一个特征的重要性是以该特征带来的标准的(归一化)总减少量来计算的。它也被称为吉尼重要性。</target>
        </trans-unit>
        <trans-unit id="285ba1aeef83b4f72aaed81188dabeb94680bb69" translate="yes" xml:space="preserve">
          <source>The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. &lt;code&gt;oob_improvement_[0]&lt;/code&gt; is the improvement in loss of the first stage over the &lt;code&gt;init&lt;/code&gt; estimator.</source>
          <target state="translated">相对于先前的迭代，袋装样本的损失（=偏差）的改善。 &lt;code&gt;oob_improvement_[0]&lt;/code&gt; 是 &lt;code&gt;init&lt;/code&gt; 估算器在第一阶段损失方面的改进。</target>
        </trans-unit>
        <trans-unit id="820150544fe550e2e074294e51088eda9cdc8014" translate="yes" xml:space="preserve">
          <source>The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. &lt;code&gt;oob_improvement_[0]&lt;/code&gt; is the improvement in loss of the first stage over the &lt;code&gt;init&lt;/code&gt; estimator. Only available if &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt;</source>
          <target state="translated">相对于先前的迭代，袋装样本的损失（=偏差）的改善。 &lt;code&gt;oob_improvement_[0]&lt;/code&gt; 是 &lt;code&gt;init&lt;/code&gt; 估计器对第一阶段损失的改善。仅在 &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; 可用</target>
        </trans-unit>
        <trans-unit id="d60773790f2e7bcb156876ca46ea3f1191f69076" translate="yes" xml:space="preserve">
          <source>The impurity at \(m\) is computed using an impurity function \(H()\), the choice of which depends on the task being solved (classification or regression)</source>
          <target state="translated">使用不纯度函数计算不纯度(H()\),其选择取决于正在解决的任务(分类或回归)</target>
        </trans-unit>
        <trans-unit id="d750c1db4cd012b1969f8fbe25c31782ef1879ba" translate="yes" xml:space="preserve">
          <source>The impurity-based feature importance ranks the numerical features to be the most important features. As a result, the non-predictive &lt;code&gt;random_num&lt;/code&gt; variable is ranked the most important!</source>
          <target state="translated">基于杂质的特征重要性将数值特征列为最重要的特征。结果，非预测性 &lt;code&gt;random_num&lt;/code&gt; 变量被列为最重要的变量！</target>
        </trans-unit>
        <trans-unit id="eb74ef34fa0b599fafdb1111102755a10d538f2b" translate="yes" xml:space="preserve">
          <source>The impurity-based feature importances computed on tree-based models suffer from two flaws that can lead to misleading conclusions. First they are computed on statistics derived from the training dataset and therefore &lt;strong&gt;do not necessarily inform us on which features are most important to make good predictions on held-out dataset&lt;/strong&gt;. Secondly, &lt;strong&gt;they favor high cardinality features&lt;/strong&gt;, that is features with many unique values. &lt;a href=&quot;permutation_importance#permutation-importance&quot;&gt;Permutation feature importance&lt;/a&gt; is an alternative to impurity-based feature importance that does not suffer from these flaws. These two methods of obtaining feature importance are explored in: &lt;a href=&quot;../auto_examples/inspection/plot_permutation_importance#sphx-glr-auto-examples-inspection-plot-permutation-importance-py&quot;&gt;Permutation Importance vs Random Forest Feature Importance (MDI)&lt;/a&gt;.</source>
          <target state="translated">在基于树的模型上计算的基于杂质的特征重要性具有两个缺陷，这些缺陷可能导致得出令人误解的结论。首先，它们是根据从训练数据集获得的统计数据进行计算的，因此&lt;strong&gt;不一定告知我们哪些特征对于对保留的数据集做出良好的预测最重要&lt;/strong&gt;。其次，&lt;strong&gt;他们倾向于高基数特征&lt;/strong&gt;，即具有许多唯一值的特征。&lt;a href=&quot;permutation_importance#permutation-importance&quot;&gt;排列特征重要性&lt;/a&gt;是基于杂质的特征重要性的替代方法，不受这些缺陷的影响。获取特征重要性的这两种方法在以下方面进行了探讨：&lt;a href=&quot;../auto_examples/inspection/plot_permutation_importance#sphx-glr-auto-examples-inspection-plot-permutation-importance-py&quot;&gt;排列重要性与随机森林特征重要性（MDI）&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="a85ca2be296db64c4eea5e4c85e7d3c8770ab473" translate="yes" xml:space="preserve">
          <source>The impurity-based feature importances.</source>
          <target state="translated">基于杂质的特征进口。</target>
        </trans-unit>
        <trans-unit id="f89a5a9d5e4b0a0db4c9a50975418d448408f475" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature if axis == 0.</source>
          <target state="translated">如果轴==0,则每个特征的推算填充值。</target>
        </trans-unit>
        <trans-unit id="52ad716a60ba342ef84206f96283ef6240a59825" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature.</source>
          <target state="translated">每个特征的推算填充值。</target>
        </trans-unit>
        <trans-unit id="66edd575acaaa453b1fb3cfd5f9ed2a63e5987ae" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature. Computing statistics can result in &lt;code&gt;np.nan&lt;/code&gt; values. During &lt;a href=&quot;#sklearn.impute.SimpleImputer.transform&quot;&gt;&lt;code&gt;transform&lt;/code&gt;&lt;/a&gt;, features corresponding to &lt;code&gt;np.nan&lt;/code&gt; statistics will be discarded.</source>
          <target state="translated">每个要素的插补填充值。计算统计信息可以 &lt;code&gt;np.nan&lt;/code&gt; 值。在&lt;a href=&quot;#sklearn.impute.SimpleImputer.transform&quot;&gt; &lt;code&gt;transform&lt;/code&gt; &lt;/a&gt;，与 &lt;code&gt;np.nan&lt;/code&gt; 统计信息相对应的特征将被丢弃。</target>
        </trans-unit>
        <trans-unit id="7cd5e57b66f65afed5b5b066bdba074e0cee3d73" translate="yes" xml:space="preserve">
          <source>The imputation strategy.</source>
          <target state="translated">归因策略。</target>
        </trans-unit>
        <trans-unit id="d67cf9b9c87d5f16b287aedd5a057873a2e85c3c" translate="yes" xml:space="preserve">
          <source>The imputed dataset. &lt;code&gt;n_output_features&lt;/code&gt; is the number of features that is not always missing during &lt;code&gt;fit&lt;/code&gt;.</source>
          <target state="translated">估算数据集。 &lt;code&gt;n_output_features&lt;/code&gt; 是 &lt;code&gt;fit&lt;/code&gt; 期间并不总是缺少的特征数。</target>
        </trans-unit>
        <trans-unit id="fc59b667379a9f19464aefa0413c2d5b611edad6" translate="yes" xml:space="preserve">
          <source>The imputed input data.</source>
          <target state="translated">推算的输入数据;</target>
        </trans-unit>
        <trans-unit id="aa1b27a8286b1b98805decd6657b8fa02347921e" translate="yes" xml:space="preserve">
          <source>The index (of the &lt;code&gt;cv_results_&lt;/code&gt; arrays) which corresponds to the best candidate parameter setting.</source>
          <target state="translated">与最佳候选参数设置相对应的（ &lt;code&gt;cv_results_&lt;/code&gt; 数组的）索引。</target>
        </trans-unit>
        <trans-unit id="d8f1885dbc976f2ceb3f7711b10b324e6546b97b" translate="yes" xml:space="preserve">
          <source>The index is computed only quantities and features inherent to the dataset.</source>
          <target state="translated">该指数只计算数据集固有的数量和特征。</target>
        </trans-unit>
        <trans-unit id="eaf4dd06369196819331db8b96a42731aed3d8e0" translate="yes" xml:space="preserve">
          <source>The index is defined as the average similarity between each cluster \(C_i\) for \(i=1, ..., k\) and its most similar one \(C_j\). In the context of this index, similarity is defined as a measure \(R_{ij}\) that trades off:</source>
          <target state="translated">该指数被定义为每个群组(C_i/)(i=1,...,k/)与其最相似的群组(C_j/)之间的平均相似度。在该指数中,相似度被定义为衡量衡量(R_{ij}/)的交易。</target>
        </trans-unit>
        <trans-unit id="7ad5522250591909d8f74ff707e1fa5837de3ae6" translate="yes" xml:space="preserve">
          <source>The index is the ratio of the sum of between-clusters dispersion and of inter-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared):</source>
          <target state="translated">该指数是所有群组的群组间离散度之和与群组间离散度之和的比率(其中离散度定义为距离平方之和)。</target>
        </trans-unit>
        <trans-unit id="f4af6c67193225ac6c1c304797e9ea3402785083" translate="yes" xml:space="preserve">
          <source>The index of the cluster.</source>
          <target state="translated">集群的指数。</target>
        </trans-unit>
        <trans-unit id="bf1ee02857437dc2bc3e741c8e3e56ef69d1ae10" translate="yes" xml:space="preserve">
          <source>The index value of a word in the vocabulary is linked to its frequency in the whole training corpus.</source>
          <target state="translated">词汇中一个词的索引值与其在整个训练语料中的频率挂钩。</target>
        </trans-unit>
        <trans-unit id="03c2241f87945c783a2d50da20a997d6e73ac147" translate="yes" xml:space="preserve">
          <source>The indexes of the sorted training input samples. If many tree are grown on the same dataset, this allows the ordering to be cached between trees. If None, the data will be sorted here. Don&amp;rsquo;t use this parameter unless you know what to do.</source>
          <target state="translated">排序后的训练输入样本的索引。如果在同一数据集上生长了许多树，则可以在树之间缓存顺序。如果为None，则将在此处对数据进行排序。除非您知道要做什么，否则不要使用此参数。</target>
        </trans-unit>
        <trans-unit id="0f615a1a241d635bbd7f8073a9962f88455d5840" translate="yes" xml:space="preserve">
          <source>The indicators for cluster membership of each column.</source>
          <target state="translated">每一栏的分组成员指标;</target>
        </trans-unit>
        <trans-unit id="7c7c8e5dd3219610397a571246f91a523e9b0296" translate="yes" xml:space="preserve">
          <source>The indicators for cluster membership of each row.</source>
          <target state="translated">每一行的分组成员指标。</target>
        </trans-unit>
        <trans-unit id="1d09056572af0ab7d260b8a9a458cace43041c54" translate="yes" xml:space="preserve">
          <source>The inertia matrix uses a Heapq-based representation.</source>
          <target state="translated">惯性矩阵采用基于Heapq的表示方法。</target>
        </trans-unit>
        <trans-unit id="4f68a65921a2da6924b081b35cf28ef22fa8adaa" translate="yes" xml:space="preserve">
          <source>The inferred value of max_features.</source>
          <target state="translated">max_features的推断值。</target>
        </trans-unit>
        <trans-unit id="9af964a5bdc6c7f6e3e23dbd4ccea9f871ad0d0c" translate="yes" xml:space="preserve">
          <source>The initial coefficients to warm-start the optimization.</source>
          <target state="translated">预热启动优化的初始系数。</target>
        </trans-unit>
        <trans-unit id="e326d26a40d08d9ebf56e15d175f98f61ace6983" translate="yes" xml:space="preserve">
          <source>The initial guess for the covariance.</source>
          <target state="translated">协方差的初步猜测。</target>
        </trans-unit>
        <trans-unit id="a31ad0b6b83b24ae472799017e15e5984848ac41" translate="yes" xml:space="preserve">
          <source>The initial guess of the noise variance for each feature. If None, it defaults to np.ones(n_features)</source>
          <target state="translated">每个特征的噪声方差的初始猜测。如果为None,则默认为np.ones(n_features)</target>
        </trans-unit>
        <trans-unit id="5af1f38d3d578b440a3ef0c3e0d6f491e678143f" translate="yes" xml:space="preserve">
          <source>The initial intercept to warm-start the optimization.</source>
          <target state="translated">初始拦截,以预热启动优化。</target>
        </trans-unit>
        <trans-unit id="ffb7aede167c9f36a232ef307d2df81471bb1b4f" translate="yes" xml:space="preserve">
          <source>The initial learning rate for the &amp;lsquo;constant&amp;rsquo;, &amp;lsquo;invscaling&amp;rsquo; or &amp;lsquo;adaptive&amp;rsquo; schedules. The default value is 0.0 as eta0 is not used by the default schedule &amp;lsquo;optimal&amp;rsquo;.</source>
          <target state="translated">&amp;ldquo;固定&amp;rdquo;，&amp;ldquo;增量&amp;rdquo;或&amp;ldquo;自适应&amp;rdquo;进度表的初始学习率。默认值为0.0，因为默认计划&amp;ldquo;最佳&amp;rdquo;未使用eta0。</target>
        </trans-unit>
        <trans-unit id="3434635df97d9946f02b9082e79d95e6999ccc03" translate="yes" xml:space="preserve">
          <source>The initial learning rate for the &amp;lsquo;constant&amp;rsquo;, &amp;lsquo;invscaling&amp;rsquo; or &amp;lsquo;adaptive&amp;rsquo; schedules. The default value is 0.01.</source>
          <target state="translated">&amp;ldquo;固定&amp;rdquo;，&amp;ldquo;增量&amp;rdquo;或&amp;ldquo;自适应&amp;rdquo;进度表的初始学习率。默认值为0.01。</target>
        </trans-unit>
        <trans-unit id="f396af141edab02acfc3d8c05ea1f6e174dd0d5b" translate="yes" xml:space="preserve">
          <source>The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;.</source>
          <target state="translated">使用的初始学习率。它在更新权重时控制步长。仅在Solver ='sgd'或'adam'时使用。</target>
        </trans-unit>
        <trans-unit id="8aa315cba5e51992abb63a7c33e92703aa91a364" translate="yes" xml:space="preserve">
          <source>The initial model \(F_{0}\) is problem specific, for least-squares regression one usually chooses the mean of the target values.</source>
          <target state="translated">初始模型/(F_{0}/)是针对问题的,对于最小二乘回归,通常选择目标值的平均值。</target>
        </trans-unit>
        <trans-unit id="fcbcdc2001232ebf2d1267dbd5c1db8c05bef33b" translate="yes" xml:space="preserve">
          <source>The initial model can also be specified via the &lt;code&gt;init&lt;/code&gt; argument. The passed object has to implement &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">初始模型也可以通过 &lt;code&gt;init&lt;/code&gt; 参数指定。传递的对象必须实现 &lt;code&gt;fit&lt;/code&gt; 和 &lt;code&gt;predict&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2b0323f433eec5618cbc80c984794d0b5575bec5" translate="yes" xml:space="preserve">
          <source>The initial transformation will be a random array of shape &lt;code&gt;(n_components, n_features)&lt;/code&gt;. Each value is sampled from the standard normal distribution.</source>
          <target state="translated">初始变换将是形状的随机数组 &lt;code&gt;(n_components, n_features)&lt;/code&gt; 。每个值均从标准正态分布中采样。</target>
        </trans-unit>
        <trans-unit id="17bd189eec62d21587058ccd88b444461d73119b" translate="yes" xml:space="preserve">
          <source>The initial values of the coefficients.</source>
          <target state="translated">系数的初始值。</target>
        </trans-unit>
        <trans-unit id="32a92ee0ef2ffe7a34cba56e64f123bd9b2ca181" translate="yes" xml:space="preserve">
          <source>The input data consists of 28x28 pixel handwritten digits, leading to 784 features in the dataset. Therefore the first layer weight matrix have the shape (784, hidden_layer_sizes[0]). We can therefore visualize a single column of the weight matrix as a 28x28 pixel image.</source>
          <target state="translated">输入数据由28x28像素的手写数字组成,导致数据集中有784个特征,因此第一层权重矩阵的形状为(784,hidden_layer_sizes[0])。因此第一层权重矩阵的形状为(784,hidden_layer_sizes[0])。因此我们可以将权重矩阵的单列可视化为28x28像素的图像。</target>
        </trans-unit>
        <trans-unit id="c8ff70c87eb4edb0d6df0b02362dd0f826eb86c0" translate="yes" xml:space="preserve">
          <source>The input data matrix</source>
          <target state="translated">输入数据矩阵</target>
        </trans-unit>
        <trans-unit id="28402823f8037a445e34f883189d18c9c8586801" translate="yes" xml:space="preserve">
          <source>The input data to complete.</source>
          <target state="translated">要完成的输入数据。</target>
        </trans-unit>
        <trans-unit id="10ea3f8f2392cebb3ea568adacc2ca87c3d24f21" translate="yes" xml:space="preserve">
          <source>The input data to project into a smaller dimensional space.</source>
          <target state="translated">将输入的数据投射到一个较小维度的空间。</target>
        </trans-unit>
        <trans-unit id="8b941f334dfce6c774093743936f754bf8137c6b" translate="yes" xml:space="preserve">
          <source>The input data.</source>
          <target state="translated">输入的数据;</target>
        </trans-unit>
        <trans-unit id="a383a11075a72cc4c498a6da492b12429e0d8bd0" translate="yes" xml:space="preserve">
          <source>The input matrix \(A\) is first normalized to make the checkerboard pattern more obvious. There are three possible methods:</source>
          <target state="translated">首先对输入的矩阵/(A/)进行归一化处理,使棋盘模式更加明显。有三种可能的方法。</target>
        </trans-unit>
        <trans-unit id="5296b71b9c59f1ea88d1300f56d80b11acdd4263" translate="yes" xml:space="preserve">
          <source>The input matrix \(A\) is preprocessed as follows:</source>
          <target state="translated">输入矩阵/(A/)的预处理如下:</target>
        </trans-unit>
        <trans-unit id="1dfee05c320291671e1507b730cbc56e48fcc05b" translate="yes" xml:space="preserve">
          <source>The input samples with only the selected features.</source>
          <target state="translated">只有选定特征的输入样本。</target>
        </trans-unit>
        <trans-unit id="aa9c51d1052bb07178d99dc6b9998838f3fd04f4" translate="yes" xml:space="preserve">
          <source>The input samples.</source>
          <target state="translated">输入的样本。</target>
        </trans-unit>
        <trans-unit id="691f5d8dd518149d763a76be65224ad6a47a3de2" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, it will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt; and if a sparse matrix is provided to a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">输入样本。在内部，它将转换为 &lt;code&gt;dtype=np.float32&lt;/code&gt; ,并且如果将稀疏矩阵提供给稀疏 &lt;code&gt;csr_matrix&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="80a0f34908e0a755318c3eb528444068386165b2" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">输入样本。在内部，其dtype将转换为 &lt;code&gt;dtype=np.float32&lt;/code&gt; 。如果提供了一个稀疏矩阵，它将被转换为一个稀疏 &lt;code&gt;csr_matrix&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ad241eeecc4b3d71885329d69a3ecbb931dd0903" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted to a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">输入样本。在内部，其dtype将转换为 &lt;code&gt;dtype=np.float32&lt;/code&gt; 。如果提供了稀疏矩阵，它将被转换为稀疏 &lt;code&gt;csr_matrix&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="574b784828dbe4184d33a76077708bbde6ac5b78" translate="yes" xml:space="preserve">
          <source>The input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.</source>
          <target state="translated">输入的样本。稀疏矩阵可以是CSC、CSR、COO、DOK或LIL。COO、DOK和LIL被转换为CSR。</target>
        </trans-unit>
        <trans-unit id="b99e327cb34f5c84fcbd4e91cfb496e38c8c302f" translate="yes" xml:space="preserve">
          <source>The input samples. Use &lt;code&gt;dtype=np.float32&lt;/code&gt; for maximum efficiency. Sparse matrices are also supported, use sparse &lt;code&gt;csc_matrix&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">输入样本。使用 &lt;code&gt;dtype=np.float32&lt;/code&gt; 可获得最大效率。还支持稀疏矩阵，请使用稀疏 &lt;code&gt;csc_matrix&lt;/code&gt; 以获得最大效率。</target>
        </trans-unit>
        <trans-unit id="6e6a9dea87222450502841b10c214ed0343cd360" translate="yes" xml:space="preserve">
          <source>The input set can either be well conditioned (by default) or have a low rank-fat tail singular profile. See &lt;a href=&quot;sklearn.datasets.make_low_rank_matrix#sklearn.datasets.make_low_rank_matrix&quot;&gt;&lt;code&gt;make_low_rank_matrix&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">输入集可以是条件良好的（默认情况下），也可以是低阶脂肪尾部奇异轮廓。有关更多详细信息，请参见&lt;a href=&quot;sklearn.datasets.make_low_rank_matrix#sklearn.datasets.make_low_rank_matrix&quot;&gt; &lt;code&gt;make_low_rank_matrix&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="289f5749e232fcbed1540450cf860647da777cb8" translate="yes" xml:space="preserve">
          <source>The input set is well conditioned, centered and gaussian with unit variance.</source>
          <target state="translated">输入集具有良好的条件,居中和高斯的单位方差。</target>
        </trans-unit>
        <trans-unit id="9196dd161fc035e27746b1e485ce67d2bc4a2f97" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are converted to ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature.</source>
          <target state="translated">这个变换器的输入应该是一个整数或字符串的数组,表示分类(离散)特征的值。这些特征被转换为序数整数。这样每个特征就会有一列整数(0到n_categories-1)。</target>
        </trans-unit>
        <trans-unit id="a912a205d28c73bb81095b58a51d5b9233f6732c" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka &amp;lsquo;one-of-K&amp;rsquo; or &amp;lsquo;dummy&amp;rsquo;) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array (depending on the &lt;code&gt;sparse&lt;/code&gt; parameter)</source>
          <target state="translated">该转换器的输入应为整数或字符串之类的数组，表示分类（离散）特征所采用的值。使用单热（又称&amp;ldquo; one-of-K&amp;rdquo;或&amp;ldquo; dummy&amp;rdquo;）编码方案对特征进行编码。这将为每个类别创建一个二进制列，并返回一个稀疏矩阵或密集数组（取决于 &lt;code&gt;sparse&lt;/code&gt; 参数）</target>
        </trans-unit>
        <trans-unit id="6fca358f78b22e3193c17b331b39a22e0cf2c917" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka &amp;lsquo;one-of-K&amp;rsquo; or &amp;lsquo;dummy&amp;rsquo;) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array.</source>
          <target state="translated">此转换器的输入应为整数或字符串之类的数组，表示分类（离散）特征所采用的值。使用单热（又名&amp;ldquo; one-of-K&amp;rdquo;或&amp;ldquo; dummy&amp;rdquo;）编码方案对特征进行编码。这将为每个类别创建一个二进制列，并返回一个稀疏矩阵或密集数组。</target>
        </trans-unit>
        <trans-unit id="c9721cff229ea27eaddea830cb215d9ecb24c54d" translate="yes" xml:space="preserve">
          <source>The instance.</source>
          <target state="translated">该实例。</target>
        </trans-unit>
        <trans-unit id="d6846270bbc73ec820141552eb32b90be4589180" translate="yes" xml:space="preserve">
          <source>The integer id or the string name metadata of the MLComp dataset to load</source>
          <target state="translated">要加载的MLComp数据集的整数id或字符串名称元数据。</target>
        </trans-unit>
        <trans-unit id="0ff5072970d94a45dfd0b3dc59cc200193becdfd" translate="yes" xml:space="preserve">
          <source>The integer labels (0 or 1) for class membership of each sample.</source>
          <target state="translated">每个样本的类别成员的整数标签(0或1)。</target>
        </trans-unit>
        <trans-unit id="a6afcce2561a80c34c914f95ed66620399eb0d7c" translate="yes" xml:space="preserve">
          <source>The integer labels for class membership of each sample.</source>
          <target state="translated">每个样本的类别成员的整数标签。</target>
        </trans-unit>
        <trans-unit id="868af1be557e4830c6f25b4ff3211b7a9aad7eb0" translate="yes" xml:space="preserve">
          <source>The integer labels for cluster membership of each sample.</source>
          <target state="translated">每个样本的集群成员资格的整数标签。</target>
        </trans-unit>
        <trans-unit id="05d4e37f01ed47119c132248581df1196214fb7d" translate="yes" xml:space="preserve">
          <source>The integer labels for quantile membership of each sample.</source>
          <target state="translated">每个样本的整数标签。</target>
        </trans-unit>
        <trans-unit id="7da460eaa0239b5647e6b97a087bcede08e5a0c5" translate="yes" xml:space="preserve">
          <source>The intercept of the model. Only returned if &lt;code&gt;return_intercept&lt;/code&gt; is True and if X is a scipy sparse array.</source>
          <target state="translated">模型的截距。仅当 &lt;code&gt;return_intercept&lt;/code&gt; 为True且X为稀疏数组时才返回。</target>
        </trans-unit>
        <trans-unit id="f0408bacce1626a183a8eadd09dc2d6f8b9e549c" translate="yes" xml:space="preserve">
          <source>The intercept term.</source>
          <target state="translated">拦截项。</target>
        </trans-unit>
        <trans-unit id="39d99a92784fd6547680c69e2a029d63ab730943" translate="yes" xml:space="preserve">
          <source>The inverse document frequency (IDF) vector; only defined if &lt;code&gt;use_idf&lt;/code&gt; is True.</source>
          <target state="translated">逆文档频率（IDF）向量；仅在 &lt;code&gt;use_idf&lt;/code&gt; 为True时定义。</target>
        </trans-unit>
        <trans-unit id="70e42545b4f301fb133ffa67ef01dbd1c81a14a6" translate="yes" xml:space="preserve">
          <source>The inverse of the Box-Cox transformation is given by:</source>
          <target state="translated">Box-Cox变换的反式为:。</target>
        </trans-unit>
        <trans-unit id="1296b0fc0246edf109c97645c02261b1d3d028bb" translate="yes" xml:space="preserve">
          <source>The inverse of the Yeo-Johnson transformation is given by:</source>
          <target state="translated">Yeo-Johnson变换的反式为:。</target>
        </trans-unit>
        <trans-unit id="8621ba40f721d68ec6f12f67b5e3cc856e6fc6d9" translate="yes" xml:space="preserve">
          <source>The iris dataset is a classic and very easy multi-class classification dataset.</source>
          <target state="translated">虹膜数据集是一个经典且非常简单的多类分类数据集。</target>
        </trans-unit>
        <trans-unit id="82f3a3d98edcec3969a3e26fc0789c7c6c1f74ef" translate="yes" xml:space="preserve">
          <source>The iris dataset is a classification task consisting in identifying 3 different types of irises (Setosa, Versicolour, and Virginica) from their petal and sepal length and width:</source>
          <target state="translated">虹膜数据集是一个分类任务,包括从花瓣和萼片的长度和宽度识别3种不同类型的虹膜(Setosa,Versicolour和Virginica)。</target>
        </trans-unit>
        <trans-unit id="968fa226a66bed28f98c84df2ac306ac992e59c2" translate="yes" xml:space="preserve">
          <source>The isotonic regression optimization problem is defined by:</source>
          <target state="translated">等差回归优化问题的定义为:。</target>
        </trans-unit>
        <trans-unit id="710a9703f51d61fb5f41b8c77a926ca070febd6c" translate="yes" xml:space="preserve">
          <source>The iteration will stop when &lt;code&gt;max{|proj g_i | i = 1, ..., n}&lt;/code&gt; &amp;lt;= &lt;code&gt;tol&lt;/code&gt; where pg_i is the i-th component of the projected gradient.</source>
          <target state="translated">当 &lt;code&gt;max{|proj g_i | i = 1, ..., n}&lt;/code&gt; &amp;lt;= &lt;code&gt;tol&lt;/code&gt; 其中pg_i是投影梯度的第i个分量。</target>
        </trans-unit>
        <trans-unit id="d1f978f02193a9d7a16e2ac4f28051f1be20fa33" translate="yes" xml:space="preserve">
          <source>The iterator consumption and dispatching is protected by the same lock so calling this function should be thread safe.</source>
          <target state="translated">迭代器的消耗和派发是由同一个锁保护的,所以调用这个函数应该是线程安全的。</target>
        </trans-unit>
        <trans-unit id="e6bcf49b626a9dedb0663face3bd765b3dedb378" translate="yes" xml:space="preserve">
          <source>The ith element in the list represents the bias vector corresponding to layer i + 1.</source>
          <target state="translated">列表中的第i个元素代表i+1层对应的偏置向量。</target>
        </trans-unit>
        <trans-unit id="4b396b269a89e1a7151872f147950b58bd31d2c2" translate="yes" xml:space="preserve">
          <source>The ith element in the list represents the weight matrix corresponding to layer i.</source>
          <target state="translated">列表中的第i个元素代表第i层对应的权重矩阵。</target>
        </trans-unit>
        <trans-unit id="8ce9197f7d2eba2389cfe44c6d9806383e84232a" translate="yes" xml:space="preserve">
          <source>The ith element represents the number of neurons in the ith hidden layer.</source>
          <target state="translated">第i个元素代表第i个隐藏层的神经元数量。</target>
        </trans-unit>
        <trans-unit id="110dde91ce5735296cfcdbdf4849eb2634f008ea" translate="yes" xml:space="preserve">
          <source>The justification for the formula in the loss=&amp;rdquo;modified_huber&amp;rdquo; case is in the appendix B in: &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&lt;/a&gt;</source>
          <target state="translated">在loss =&amp;ldquo; modified_huber&amp;rdquo;情况下公式的理由在附录B中的位置：&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&quot;&gt;http&lt;/a&gt; : //jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf</target>
        </trans-unit>
        <trans-unit id="08f434f0a998f2afa3af3de6e921bb2935ec3174" translate="yes" xml:space="preserve">
          <source>The k-means algorithm divides a set of \(N\) samples \(X\) into \(K\) disjoint clusters \(C\), each described by the mean \(\mu_j\) of the samples in the cluster. The means are commonly called the cluster &amp;ldquo;centroids&amp;rdquo;; note that they are not, in general, points from \(X\), although they live in the same space.</source>
          <target state="translated">k-均值算法将一组\（N \）个样本\（X \）划分为\（K \）个不相交的簇\（C \），每个簇由样本中的样本均值\（\ mu_j \）来描述簇。该方法通常称为簇&amp;ldquo;质心&amp;rdquo;。请注意，尽管它们生活在同一空间中，但它们通常不是来自\（X \）的点。</target>
        </trans-unit>
        <trans-unit id="2ad3a7b0acdb773b3a5a78e7f86e7401f887f5a4" translate="yes" xml:space="preserve">
          <source>The k-means algorithm divides a set of \(N\) samples \(X\) into \(K\) disjoint clusters \(C\), each described by the mean \(\mu_j\) of the samples in the cluster. The means are commonly called the cluster &amp;ldquo;centroids&amp;rdquo;; note that they are not, in general, points from \(X\), although they live in the same space. The K-means algorithm aims to choose centroids that minimise the &lt;em&gt;inertia&lt;/em&gt;, or within-cluster sum of squared criterion:</source>
          <target state="translated">k-均值算法将一组\（N \）个样本\（X \）划分为\（K \）个不相交的聚类\（C \），每个聚类由簇。该方法通常称为簇&amp;ldquo;质心&amp;rdquo;。请注意，尽管它们位于相同的空间中，但它们通常不是\（X \）的点。K-means算法旨在选择最小化&lt;em&gt;惯性&lt;/em&gt;或质心内平方和的质心：</target>
        </trans-unit>
        <trans-unit id="3042a74be3a11ce6ced2d21d393c8c55a0b044ff" translate="yes" xml:space="preserve">
          <source>The k-means problem is solved using either Lloyd&amp;rsquo;s or Elkan&amp;rsquo;s algorithm.</source>
          <target state="translated">使用Lloyd或Elkan算法可以解决k均值问题。</target>
        </trans-unit>
        <trans-unit id="698fe2144ac8be0b801483c0a1d13dc4d07c73ce" translate="yes" xml:space="preserve">
          <source>The kappa score (see docstring) is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).</source>
          <target state="translated">kappa得分(见docstring)是一个介于-1和1之间的数字。得分高于0.8的一般被认为是好的协议;0或更低的意味着没有协议(实际上是随机标签)。</target>
        </trans-unit>
        <trans-unit id="951d69c60ea24190bd0c5a4dcae9aeb94c1c913d" translate="yes" xml:space="preserve">
          <source>The kappa statistic, which is a number between -1 and 1. The maximum value means complete agreement; zero or lower means chance agreement.</source>
          <target state="translated">kappa统计量,是一个介于-1和1之间的数字,最大值表示完全一致,零或更低表示偶然一致。</target>
        </trans-unit>
        <trans-unit id="e7d87035112e23c601a476b538c50df786049966" translate="yes" xml:space="preserve">
          <source>The kernel density estimator can be used with any of the valid distance metrics (see &lt;a href=&quot;generated/sklearn.neighbors.distancemetric#sklearn.neighbors.DistanceMetric&quot;&gt;&lt;code&gt;sklearn.neighbors.DistanceMetric&lt;/code&gt;&lt;/a&gt; for a list of available metrics), though the results are properly normalized only for the Euclidean metric. One particularly useful metric is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;Haversine distance&lt;/a&gt; which measures the angular distance between points on a sphere. Here is an example of using a kernel density estimate for a visualization of geospatial data, in this case the distribution of observations of two different species on the South American continent:</source>
          <target state="translated">内核密度估计器可以与任何有效的距离度量标准一起使用（有关可用度量标准的列表，请参阅&lt;a href=&quot;generated/sklearn.neighbors.distancemetric#sklearn.neighbors.DistanceMetric&quot;&gt; &lt;code&gt;sklearn.neighbors.DistanceMetric&lt;/code&gt; &lt;/a&gt;），尽管仅对欧几里德度量标准对结果进行了适当的标准化。一种特别有用的度量标准是&lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;Haversine距离&lt;/a&gt;，它测量球体上各点之间的角距离。这是一个使用核密度估计值可视化地理空间数据的示例，在这种情况下，是南美大陆上两种不同物种的观测值分布：</target>
        </trans-unit>
        <trans-unit id="f5acb5ee70e93822d6214e7faa0671c1edbb5e04" translate="yes" xml:space="preserve">
          <source>The kernel is composed of several terms that are responsible for explaining different properties of the signal:</source>
          <target state="translated">内核由几个术语组成,负责解释信号的不同特性。</target>
        </trans-unit>
        <trans-unit id="075d7f089a3306b950fac8e515a19a5954a7251d" translate="yes" xml:space="preserve">
          <source>The kernel is given by:</source>
          <target state="translated">核心是由:</target>
        </trans-unit>
        <trans-unit id="b2a264496619ccec6a15428322dff95c6847afef" translate="yes" xml:space="preserve">
          <source>The kernel specifying the covariance function of the GP. If None is passed, the kernel &amp;ldquo;1.0 * RBF(1.0)&amp;rdquo; is used as default. Note that the kernel&amp;rsquo;s hyperparameters are optimized during fitting.</source>
          <target state="translated">指定GP协方差函数的内核。如果未通过，则默认使用内核&amp;ldquo; 1.0 * RBF（1.0）&amp;rdquo;。请注意，内核的超参数在拟合过程中已优化。</target>
        </trans-unit>
        <trans-unit id="3f4b36434e95db38ce416f2948e370303e8d7ce4" translate="yes" xml:space="preserve">
          <source>The kernel to use. Valid kernels are [&amp;lsquo;gaussian&amp;rsquo;|&amp;rsquo;tophat&amp;rsquo;|&amp;rsquo;epanechnikov&amp;rsquo;|&amp;rsquo;exponential&amp;rsquo;|&amp;rsquo;linear&amp;rsquo;|&amp;rsquo;cosine&amp;rsquo;] Default is &amp;lsquo;gaussian&amp;rsquo;.</source>
          <target state="translated">要使用的内核。有效内核为['gaussian'|'tophat'|'epanechnikov'|'exponential'|'linear'|'cosine']默认值为'gaussian'。</target>
        </trans-unit>
        <trans-unit id="7d7a3328bfc6be5d4c52eb80a02db6810f6af97a" translate="yes" xml:space="preserve">
          <source>The kernel used for prediction. In case of binary classification, the structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters. In case of multi-class classification, a CompoundKernel is returned which consists of the different kernels used in the one-versus-rest classifiers.</source>
          <target state="translated">用于预测的内核。在二元分类的情况下,内核的结构与作为参数传递的内核相同,但使用了优化的超参数。在多类分类的情况下,返回一个CompoundKernel,它由单类与休整分类器中使用的不同内核组成。</target>
        </trans-unit>
        <trans-unit id="5a1810e4f4370ea9d4b3eb4b69c5661264f9680f" translate="yes" xml:space="preserve">
          <source>The kernel used for prediction. The structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters</source>
          <target state="translated">用于预测的内核。内核的结构与作为参数传入的内核相同,但优化了超参数。</target>
        </trans-unit>
        <trans-unit id="317ceb705f764af792a5c1b02b0c75da4f7767d2" translate="yes" xml:space="preserve">
          <source>The key &lt;code&gt;'params'&lt;/code&gt; is used to store a list of parameter settings dicts for all the parameter candidates.</source>
          <target state="translated">键 &lt;code&gt;'params'&lt;/code&gt; 用于存储所有候选参数的参数设置字典列表。</target>
        </trans-unit>
        <trans-unit id="467dfd2cbee563f1dc16d74e0763e99176db4d68" translate="yes" xml:space="preserve">
          <source>The l1-penalized estimator can recover part of this off-diagonal structure. It learns a sparse precision. It is not able to recover the exact sparsity pattern: it detects too many non-zero coefficients. However, the highest non-zero coefficients of the l1 estimated correspond to the non-zero coefficients in the ground truth. Finally, the coefficients of the l1 precision estimate are biased toward zero: because of the penalty, they are all smaller than the corresponding ground truth value, as can be seen on the figure.</source>
          <target state="translated">l1-penalized估计器可以恢复这种对角线外结构的一部分。它学习的是一种稀疏精度。它不能恢复精确的稀疏模式:它检测到了太多的非零系数。然而,估计的l1个最高的非零系数对应于地真中的非零系数。最后,l1精度估计的系数偏向于零:由于惩罚,它们都小于对应的地真值,这在图上可以看到。</target>
        </trans-unit>
        <trans-unit id="3f115dd7f6ab226f3d1efb2261d982c44eb021f0" translate="yes" xml:space="preserve">
          <source>The label of the positive class</source>
          <target state="translated">正类的标签</target>
        </trans-unit>
        <trans-unit id="1daeeb6b0700e2caf583964dcec09c381eeb8ea9" translate="yes" xml:space="preserve">
          <source>The label of the positive class. Only applied to binary &lt;code&gt;y_true&lt;/code&gt;. For multilabel-indicator &lt;code&gt;y_true&lt;/code&gt;, &lt;code&gt;pos_label&lt;/code&gt; is fixed to 1.</source>
          <target state="translated">正类的标签。仅适用于二进制 &lt;code&gt;y_true&lt;/code&gt; 。对于multilabel-indicator &lt;code&gt;y_true&lt;/code&gt; ， &lt;code&gt;pos_label&lt;/code&gt; 固定为1。</target>
        </trans-unit>
        <trans-unit id="e8fa4295eafd0b055339770364309144cc85ebe4" translate="yes" xml:space="preserve">
          <source>The label of the positive class. When &lt;code&gt;pos_label=None&lt;/code&gt;, if y_true is in {-1, 1} or {0, 1}, &lt;code&gt;pos_label&lt;/code&gt; is set to 1, otherwise an error will be raised.</source>
          <target state="translated">阳性类别的标签。当 &lt;code&gt;pos_label=None&lt;/code&gt; 时，如果y_true在{-1，1}或{0，1}中，则 &lt;code&gt;pos_label&lt;/code&gt; 设置为1，否则将引发错误。</target>
        </trans-unit>
        <trans-unit id="844ad03ed5a5991044f5c2a9015a7d46ffc83176" translate="yes" xml:space="preserve">
          <source>The label sets.</source>
          <target state="translated">标签集。</target>
        </trans-unit>
        <trans-unit id="4e35273308f7d89cd0147256c1b5ea034960bf40" translate="yes" xml:space="preserve">
          <source>The labels assigned to samples. Points which are not included in any cluster are labeled as -1.</source>
          <target state="translated">分配给样本的标签。不包括在任何簇中的点被标记为-1。</target>
        </trans-unit>
        <trans-unit id="34e6595f06fcaf1ac9d996e42fb91cf1dc35ee40" translate="yes" xml:space="preserve">
          <source>The labels of the clusters.</source>
          <target state="translated">各组的标签;</target>
        </trans-unit>
        <trans-unit id="f920ebdbb736d4fac6e624f1f0a24fae7f686c27" translate="yes" xml:space="preserve">
          <source>The laplacian kernel is defined as:</source>
          <target state="translated">拉普拉斯核的定义为:</target>
        </trans-unit>
        <trans-unit id="06271ccbc4593ebfa05907a5273f644dc124ef42" translate="yes" xml:space="preserve">
          <source>The lasso estimate thus solves the minimization of the least-squares penalty with \(\alpha ||w||_1\) added, where \(\alpha\) is a constant and \(||w||_1\) is the \(\ell_1\)-norm of the coefficient vector.</source>
          <target state="translated">因此,拉索估计解决了最小二乘惩罚的最小化问题,并加入了 \(\alpha ||w||_1\),其中 \(\alpha\)是一个常数,\(||w||_1\)是系数向量的 \(\ell_1\)-norm。</target>
        </trans-unit>
        <trans-unit id="321fd6d2804d3c8ea8df389dda895240e812e24c" translate="yes" xml:space="preserve">
          <source>The lasso estimate thus solves the minimization of the least-squares penalty with \(\alpha ||w||_1\) added, where \(\alpha\) is a constant and \(||w||_1\) is the \(\ell_1\)-norm of the parameter vector.</source>
          <target state="translated">因此,拉索估计解决了最小二乘惩罚的最小化问题,并加入了 \(\alpha |||w|_1/\),其中 \(\alpha\)是一个常数,\(||w|_1/\)是参数向量的 \(\ell_1/\)-norm。</target>
        </trans-unit>
        <trans-unit id="374010cdf4f6f26c62c956d36c36442b1f12b646" translate="yes" xml:space="preserve">
          <source>The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser.</source>
          <target state="translated">最后一个特征意味着Perceptron的训练速度要比SGD的铰链损失略快,而且得到的模型更稀疏。</target>
        </trans-unit>
        <trans-unit id="6312a138a2adc5ee223c24bc72b6c9e5099fd7ea" translate="yes" xml:space="preserve">
          <source>The last dataset is an example of a &amp;lsquo;null&amp;rsquo; situation for clustering: the data is homogeneous, and there is no good clustering. For this example, the null dataset uses the same parameters as the dataset in the row above it, which represents a mismatch in the parameter values and the data structure.</source>
          <target state="translated">最后一个数据集是聚类为&amp;ldquo;空&amp;rdquo;情况的一个示例：数据是同质的，并且没有良好的聚类。对于此示例，空数据集使用与其上方一行中的数据集相同的参数，这表示参数值和数据结构不匹配。</target>
        </trans-unit>
        <trans-unit id="2cc9bc385685f028c8287b17d0d958b5050b56a0" translate="yes" xml:space="preserve">
          <source>The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold. This ensures that the graph starts on the y axis.</source>
          <target state="translated">最后的精度和召回值分别为1.和0.,没有相应的阈值。这样可以保证图形从y轴开始。</target>
        </trans-unit>
        <trans-unit id="9ddff80583e9f2c8a48f3b64e4a9a06a8a8cefcd" translate="yes" xml:space="preserve">
          <source>The last two panels show how we can sample from the last two models. The resulting samples distributions do not look exactly like the original data distribution. The difference primarily stems from the approximation error we made by using a model that assumes that the data was generated by a finite number of Gaussian components instead of a continuous noisy sine curve.</source>
          <target state="translated">最后两个面板显示了我们如何从最后两个模型中进行采样。由此产生的样本分布看起来并不完全像原始数据分布。这种差异主要源于我们使用的模型所产生的近似误差,该模型假设数据是由有限数量的高斯分量产生的,而不是连续的噪声正弦曲线。</target>
        </trans-unit>
        <trans-unit id="a4514c8e853c44e1e4a78bf33662f9b32277ba44" translate="yes" xml:space="preserve">
          <source>The latent variables of X.</source>
          <target state="translated">X的潜变量。</target>
        </trans-unit>
        <trans-unit id="03a214378b3ef2e13a3cc6617d05b3fe221146c8" translate="yes" xml:space="preserve">
          <source>The learning rate \(\eta\) can be either constant or gradually decaying. For classification, the default learning rate schedule (&lt;code&gt;learning_rate='optimal'&lt;/code&gt;) is given by</source>
          <target state="translated">学习速率\（\ eta \）可以是恒定的，也可以是逐渐衰减的。对于分类，默认学习率时间表（ &lt;code&gt;learning_rate='optimal'&lt;/code&gt; ）由下式给出</target>
        </trans-unit>
        <trans-unit id="36c5b5659a911c2db600cd9fbf9c0485b5053e95" translate="yes" xml:space="preserve">
          <source>The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a &amp;lsquo;ball&amp;rsquo; with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help.</source>
          <target state="translated">t-SNE的学习率通常在[10.0，1000.0]范围内。如果学习率太高，则数据可能看起来像一个&amp;ldquo;球&amp;rdquo;，其任何一点都与其最近的邻居等距。如果学习率太低，大多数点可能看起来像压缩在密集的云中，没有异常值。如果成本函数陷入不良的局部最小值中，则提高学习率可能会有所帮助。</target>
        </trans-unit>
        <trans-unit id="55a4138aa9f2827acbf6f24a7d7d78c5c9231271" translate="yes" xml:space="preserve">
          <source>The learning rate for weight updates. It is &lt;em&gt;highly&lt;/em&gt; recommended to tune this hyper-parameter. Reasonable values are in the 10**[0., -3.] range.</source>
          <target state="translated">体重更新的学习率。这是&lt;em&gt;强烈&lt;/em&gt;建议调整该超参数。合理的值在10 ** [0。，-3。]范围内。</target>
        </trans-unit>
        <trans-unit id="aa4707838d034cc31029160760df726f17933ef5" translate="yes" xml:space="preserve">
          <source>The learning rate schedule:</source>
          <target state="translated">学习率表。</target>
        </trans-unit>
        <trans-unit id="3497cec934b609e53594ddceecdfac3a47086873" translate="yes" xml:space="preserve">
          <source>The learning rate, also known as &lt;em&gt;shrinkage&lt;/em&gt;. This is used as a multiplicative factor for the leaves values. Use &lt;code&gt;1&lt;/code&gt; for no shrinkage.</source>
          <target state="translated">学习率，也称为&lt;em&gt;收缩率&lt;/em&gt;。这用作叶子值的乘法因子。使用 &lt;code&gt;1&lt;/code&gt; 表示没有收缩。</target>
        </trans-unit>
        <trans-unit id="0f65b2c5f287b4c02eec9ec5be4e57eb68fdc4c9" translate="yes" xml:space="preserve">
          <source>The least squares loss (along with the implicit use of the identity link function) of the Ridge regression model seems to cause this model to be badly calibrated. In particular, it tends to underestimate the risk and can even predict invalid negative frequencies.</source>
          <target state="translated">Ridge回归模型的最小二乘损失(以及隐含的身份联系函数的使用)似乎导致这个模型被严重校准。特别是,它往往低估了风险,甚至可以预测无效的负频率。</target>
        </trans-unit>
        <trans-unit id="ed4ad540aa3d79251b7dbdba80cd56b31d18c164" translate="yes" xml:space="preserve">
          <source>The least squares solution is computed using the singular value decomposition of X. If X is a matrix of shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt; this method has a cost of \(O(n_{\text{samples}} n_{\text{features}}^2)\), assuming that \(n_{\text{samples}} \geq n_{\text{features}}\).</source>
          <target state="translated">最小二乘解是使用X的奇异值分解来计算的。如果X是形状的矩阵 &lt;code&gt;(n_samples, n_features)&lt;/code&gt; 此方法的成本为\（O（n _ {\ text {samples}} n _ {\ text {features }} ^ 2）\），假设\（n _ {\ text {samples}} \ geq n _ {\ text {features}} \\）。</target>
        </trans-unit>
        <trans-unit id="866660dd294846977c41d9eb05a5f823762d12dd" translate="yes" xml:space="preserve">
          <source>The left and right examples highlight the &lt;code&gt;n_labels&lt;/code&gt; parameter: more of the samples in the right plot have 2 or 3 labels.</source>
          <target state="translated">左右示例突出显示 &lt;code&gt;n_labels&lt;/code&gt; 参数：右图中的更多样本具有2或3个标签。</target>
        </trans-unit>
        <trans-unit id="3656a1b3513126216409c003f2c19b4a1bde366e" translate="yes" xml:space="preserve">
          <source>The leftmost layer, known as the input layer, consists of a set of neurons \(\{x_i | x_1, x_2, ..., x_m\}\) representing the input features. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation \(w_1x_1 + w_2x_2 + ... + w_mx_m\), followed by a non-linear activation function \(g(\cdot):R \rightarrow R\) - like the hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values.</source>
          <target state="translated">最左边的一层,称为输入层,由一组神经元 \(\{x_i | x_1,x_2,...,x_m}\)组成,代表输入特征。隐藏层中的每个神经元将上一层的值用加权线性和(w_1x_1+w_2x_2+...+w_mx_m/m)进行变换,然后用非线性激活函数(g(\cdot):R \rightarrow R\)-如双曲探函数。输出层接收最后一个隐藏层的值,并将其转化为输出值。</target>
        </trans-unit>
        <trans-unit id="7b2e6a226728025cdbf60737ae02746b4f5067e4" translate="yes" xml:space="preserve">
          <source>The length scale of the kernel.</source>
          <target state="translated">内核的长度尺度。</target>
        </trans-unit>
        <trans-unit id="658641ffb44e40c4155905b8a662123f9fbe36a4" translate="yes" xml:space="preserve">
          <source>The length scale of the kernel. If a float, an isotropic kernel is used. If an array, an anisotropic kernel is used where each dimension of l defines the length-scale of the respective feature dimension.</source>
          <target state="translated">内核的长度范围。如果是浮点数,则使用各向同性核。如果是数组,则使用各向异性核,其中每个维度定义各自特征维度的长度范围。</target>
        </trans-unit>
        <trans-unit id="c8dac18109c7577d20154429eb0e5e79a703a997" translate="yes" xml:space="preserve">
          <source>The likelihood of the data set with &lt;code&gt;self.covariance_&lt;/code&gt; as an estimator of its covariance matrix.</source>
          <target state="translated">使用 &lt;code&gt;self.covariance_&lt;/code&gt; 作为其协方差矩阵的估计量的数据集的可能性。</target>
        </trans-unit>
        <trans-unit id="9d0f580fc795d85a96fff8300e9ac1a9bc3bbb0b" translate="yes" xml:space="preserve">
          <source>The linear model trained on polynomial features is able to exactly recover the input polynomial coefficients.</source>
          <target state="translated">在多项式特征上训练的线性模型能够精确地恢复输入的多项式系数。</target>
        </trans-unit>
        <trans-unit id="9cf1fb22576b6c52a81ed3fb3cd57a1b7c534cc0" translate="yes" xml:space="preserve">
          <source>The linear models &lt;code&gt;LinearSVC()&lt;/code&gt; and &lt;code&gt;SVC(kernel='linear')&lt;/code&gt; yield slightly different decision boundaries. This can be a consequence of the following differences:</source>
          <target state="translated">线性模型 &lt;code&gt;LinearSVC()&lt;/code&gt; 和 &lt;code&gt;SVC(kernel='linear')&lt;/code&gt; 得出的决策边界略有不同。这可能是由于以下差异造成的：</target>
        </trans-unit>
        <trans-unit id="e79cf5350a0d3a4c8abb9ba5736a33d5a0c53fd5" translate="yes" xml:space="preserve">
          <source>The linear models assume no interactions between the input variables which likely causes under-fitting. Inserting a polynomial feature extractor (&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt;&lt;code&gt;PolynomialFeatures&lt;/code&gt;&lt;/a&gt;) indeed increases their discrimative power by 2 points of Gini index. In particular it improves the ability of the models to identify the top 5% riskiest profiles.</source>
          <target state="translated">线性模型假设输入变量之间没有相互作用，这可能会导致拟合不足。插入多项式特征提取器（&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.polynomialfeatures#sklearn.preprocessing.PolynomialFeatures&quot;&gt; &lt;code&gt;PolynomialFeatures&lt;/code&gt; &lt;/a&gt;）确实将其判别能力提高了2个基尼系数。特别是，它提高了模型识别风险最高的5％的个人资料的能力。</target>
        </trans-unit>
        <trans-unit id="086fe9ce38700dda3eefe00cc625a2fbfd905812" translate="yes" xml:space="preserve">
          <source>The linear operator to apply to the data to get the independent sources. This is equal to the unmixing matrix when &lt;code&gt;whiten&lt;/code&gt; is False, and equal to &lt;code&gt;np.dot(unmixing_matrix, self.whitening_)&lt;/code&gt; when &lt;code&gt;whiten&lt;/code&gt; is True.</source>
          <target state="translated">线性运算符应用于数据以获取独立源。这等于混合矩阵时 &lt;code&gt;whiten&lt;/code&gt; 为False，并且等于 &lt;code&gt;np.dot(unmixing_matrix, self.whitening_)&lt;/code&gt; 时 &lt;code&gt;whiten&lt;/code&gt; 为True。</target>
        </trans-unit>
        <trans-unit id="a3db33e367d642c4ed744d98677d2b41b0f8967e" translate="yes" xml:space="preserve">
          <source>The linear transformation learned during fitting.</source>
          <target state="translated">拟合过程中学习的线性变换。</target>
        </trans-unit>
        <trans-unit id="c3af4059661dbcc46e497f8eb762a99fb7562b4b" translate="yes" xml:space="preserve">
          <source>The link function is determined by the &lt;code&gt;link&lt;/code&gt; parameter.</source>
          <target state="translated">链接功能由 &lt;code&gt;link&lt;/code&gt; 参数确定。</target>
        </trans-unit>
        <trans-unit id="7828a03a3850299c9821c4410a12edc3f4715fc7" translate="yes" xml:space="preserve">
          <source>The link function of the GLM, i.e. mapping from linear predictor &lt;code&gt;X @ coeff + intercept&lt;/code&gt; to prediction &lt;code&gt;y_pred&lt;/code&gt;. Option &amp;lsquo;auto&amp;rsquo; sets the link depending on the chosen family as follows:</source>
          <target state="translated">GLM的链接函数，即从线性预测变量 &lt;code&gt;X @ coeff + intercept&lt;/code&gt; 映射到预测 &lt;code&gt;y_pred&lt;/code&gt; 。选项&amp;ldquo;自动&amp;rdquo;根据所选族设置链接，如下所示：</target>
        </trans-unit>
        <trans-unit id="df16104005b0ad36b12ecec77e07a777907b9657" translate="yes" xml:space="preserve">
          <source>The linkage distance threshold above which, clusters will not be merged. If not &lt;code&gt;None&lt;/code&gt;, &lt;code&gt;n_clusters&lt;/code&gt; must be &lt;code&gt;None&lt;/code&gt; and &lt;code&gt;compute_full_tree&lt;/code&gt; must be &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">链接距离阈值，超过该距离时，群集将不会合并。如果不是 &lt;code&gt;None&lt;/code&gt; ，则 &lt;code&gt;n_clusters&lt;/code&gt; 必须为 &lt;code&gt;None&lt;/code&gt; ,而 &lt;code&gt;compute_full_tree&lt;/code&gt; 必须为 &lt;code&gt;True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e68f70b09864b10f1aea37e78800b7b9e97f9614" translate="yes" xml:space="preserve">
          <source>The list of Elastic-Net mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. Only used if &lt;code&gt;penalty='elasticnet'&lt;/code&gt;. A value of 0 is equivalent to using &lt;code&gt;penalty='l2'&lt;/code&gt;, while 1 is equivalent to using &lt;code&gt;penalty='l1'&lt;/code&gt;. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt;1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="translated">Elastic-Net混合参数的列表，其中 &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt; 。仅在 &lt;code&gt;penalty='elasticnet'&lt;/code&gt; 。值0等效于使用 &lt;code&gt;penalty='l2'&lt;/code&gt; ，而值1等效于使用 &lt;code&gt;penalty='l1'&lt;/code&gt; 。对于 &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt;1&lt;/code&gt; ，惩罚是L1和L2的组合。</target>
        </trans-unit>
        <trans-unit id="7b28aee2c83cc27005872232d54b931e2f0eba84" translate="yes" xml:space="preserve">
          <source>The list of calibrated classifiers, one for each cross-validation fold, which has been fitted on all but the validation fold and calibrated on the validation fold.</source>
          <target state="translated">经过校准的分类器列表,每个交叉验证折线都有一个,它已经在除验证折线外的所有折线上拟合,并在验证折线上进行了校准。</target>
        </trans-unit>
        <trans-unit id="02b9c4dcf92e3aac8d7274c2bb8e453c5501153c" translate="yes" xml:space="preserve">
          <source>The list of calibrated classifiers, one for each crossvalidation fold, which has been fitted on all but the validation fold and calibrated on the validation fold.</source>
          <target state="translated">已校准的分类器列表,每个交叉验证折线都有一个,它已在除验证折线外的所有折线上拟合,并在验证折线上进行了校准。</target>
        </trans-unit>
        <trans-unit id="89a2f4c0656f755e155226e23cfb6b592b80d152" translate="yes" xml:space="preserve">
          <source>The list of clusters in the form of &lt;code&gt;[start, end]&lt;/code&gt; in each row, with all indices inclusive. The clusters are ordered according to &lt;code&gt;(end,
-start)&lt;/code&gt; (ascending) so that larger clusters encompassing smaller clusters come after such nested smaller clusters. Since &lt;code&gt;labels&lt;/code&gt; does not reflect the hierarchy, usually &lt;code&gt;len(clusters) &amp;gt;
np.unique(labels)&lt;/code&gt;.</source>
          <target state="translated">每行中以 &lt;code&gt;[start, end]&lt;/code&gt; 形式出现的集群列表，包括所有索引。群集根据 &lt;code&gt;(end, -start)&lt;/code&gt; （升序）排序，以便包含较小群集的较大群集位于此类嵌套的较小群集之后。由于 &lt;code&gt;labels&lt;/code&gt; 不能反映层次结构，因此通常 &lt;code&gt;len(clusters) &amp;gt; np.unique(labels)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4b68d75689eb0ac219c6fca1ed0b8741a6d57de9" translate="yes" xml:space="preserve">
          <source>The list of clusters in the form of &lt;code&gt;[start, end]&lt;/code&gt; in each row, with all indices inclusive. The clusters are ordered according to &lt;code&gt;(end, -start)&lt;/code&gt; (ascending) so that larger clusters encompassing smaller clusters come after those smaller ones. Since &lt;code&gt;labels_&lt;/code&gt; does not reflect the hierarchy, usually &lt;code&gt;len(cluster_hierarchy_) &amp;gt; np.unique(optics.labels_)&lt;/code&gt;. Please also note that these indices are of the &lt;code&gt;ordering_&lt;/code&gt;, i.e. &lt;code&gt;X[ordering_][start:end + 1]&lt;/code&gt; form a cluster. Only available when &lt;code&gt;cluster_method='xi'&lt;/code&gt;.</source>
          <target state="translated">每行中以 &lt;code&gt;[start, end]&lt;/code&gt; 形式出现的集群列表，包括所有索引。集群根据 &lt;code&gt;(end, -start)&lt;/code&gt; （升序）进行排序，以便包含较小集群的较大集群位于那些较小集群之后。由于 &lt;code&gt;labels_&lt;/code&gt; 不反映层次结构，因此通常 &lt;code&gt;len(cluster_hierarchy_) &amp;gt; np.unique(optics.labels_)&lt;/code&gt; 。还请注意，这些索引是 &lt;code&gt;ordering_&lt;/code&gt; 的索引，即 &lt;code&gt;X[ordering_][start:end + 1]&lt;/code&gt; 组成一个簇。仅在 &lt;code&gt;cluster_method='xi'&lt;/code&gt; 时可用。</target>
        </trans-unit>
        <trans-unit id="662dd2cdb21b969ce9a44b42150a945acbfed523" translate="yes" xml:space="preserve">
          <source>The list of values of the objective function and the dual gap at each iteration. Returned only if return_costs is True.</source>
          <target state="translated">目标函数的值和每次迭代时的双倍差距的列表。仅当return_costs为True时返回。</target>
        </trans-unit>
        <trans-unit id="177835998b74cd6b15a39a13b3bf0448af9a1ccf" translate="yes" xml:space="preserve">
          <source>The local outlier factor (LOF) of a sample captures its supposed &amp;lsquo;degree of abnormality&amp;rsquo;. It is the average of the ratio of the local reachability density of a sample and those of its k-nearest neighbors.</source>
          <target state="translated">样本的局部离群因子（LOF）捕获了其所谓的&amp;ldquo;异常程度&amp;rdquo;。它是样本及其k近邻的局部可达性密度之比的平均值。</target>
        </trans-unit>
        <trans-unit id="f5a19f67242aeecdaccb15b2c01bb7b8a32da9cd" translate="yes" xml:space="preserve">
          <source>The log likelihood at each iteration.</source>
          <target state="translated">每次迭代时的对数似然。</target>
        </trans-unit>
        <trans-unit id="d6423c4071c8619e84a84f9bcba64b070be431d3" translate="yes" xml:space="preserve">
          <source>The log-marginal-likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;self.kernel_.theta&lt;/code&gt; 的对数边际可能性</target>
        </trans-unit>
        <trans-unit id="28c02970f0172adf1161a050ed9ce282eb08888a" translate="yes" xml:space="preserve">
          <source>The log-posterior of LDA can also be written &lt;a href=&quot;#id7&quot; id=&quot;id1&quot;&gt;3&lt;/a&gt; as:</source>
          <target state="translated">LDA的对数后也可以写为&lt;a href=&quot;#id7&quot; id=&quot;id1&quot;&gt;3&lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="b9f3852ad5acaf7d90c57ecdaa290bd2f37a1610" translate="yes" xml:space="preserve">
          <source>The log-transformed bounds on the kernel&amp;rsquo;s hyperparameters theta</source>
          <target state="translated">内核超参数theta的对数转换范围</target>
        </trans-unit>
        <trans-unit id="997c53b0e19e31f9cbe762633463d7411a04ec12" translate="yes" xml:space="preserve">
          <source>The logarithm used is the natural logarithm (base-e).</source>
          <target state="translated">使用的对数是自然对数(基数-e)。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
