<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="a344504140059db6f88458066c961354f795c6a7" translate="yes" xml:space="preserve">
          <source>This method has some performance overhead hence it is better to call partial_fit on chunks of data that are as large as possible (as long as fitting in the memory budget) to hide the overhead.</source>
          <target state="translated">这个方法有一定的性能开销,因此最好在尽可能大的数据块上调用partial_fit(只要在内存预算内拟合)来隐藏开销。</target>
        </trans-unit>
        <trans-unit id="ceeeb1e178fb9959d4ffe786c9917a8fc68013bb" translate="yes" xml:space="preserve">
          <source>This method has the same order of complexity than an &lt;a href=&quot;#ordinary-least-squares&quot;&gt;Ordinary Least Squares&lt;/a&gt;.</source>
          <target state="translated">此方法的复杂度与&lt;a href=&quot;#ordinary-least-squares&quot;&gt;普通最小二乘法相同&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="8149d43c4db3023940e20ddee18dfd4caaeb24b1" translate="yes" xml:space="preserve">
          <source>This method is expected to be called several times consecutively on different chunks of a dataset so as to implement out-of-core or online learning.</source>
          <target state="translated">这个方法有望在一个数据集的不同块上连续调用几次,从而实现核心外学习或在线学习。</target>
        </trans-unit>
        <trans-unit id="7914e16bc2009e2d4fc76b2006a65a031a42eb76" translate="yes" xml:space="preserve">
          <source>This method is just there to implement the usual API and hence work in pipelines.</source>
          <target state="translated">这个方法只是为了实现通常的API,从而在管道中工作。</target>
        </trans-unit>
        <trans-unit id="53a4e9f6af590018ff1a37b6f68db2405dd79282" translate="yes" xml:space="preserve">
          <source>This method is just there to mark the fact that this transformer can work in a streaming setup.</source>
          <target state="translated">这个方法只是为了标志这个变压器可以在流媒体设置中工作。</target>
        </trans-unit>
        <trans-unit id="1ad1f3e2c791502dcf55d0ea0c930c86843ade76" translate="yes" xml:space="preserve">
          <source>This method is meant to be called concurrently by the multiprocessing callback. We rely on the thread-safety of dispatch_one_batch to protect against concurrent consumption of the unprotected iterator.</source>
          <target state="translated">这个方法是要被多进程回调并发调用的。我们依靠dispatch_one_batch的线程安全保护来防止未受保护的迭代器的并发消耗。</target>
        </trans-unit>
        <trans-unit id="4e81340dab29281a8d6b3bd99833383bb408f46c" translate="yes" xml:space="preserve">
          <source>This method is not deterministic: it computes a quantity called the free energy on X, then on a randomly corrupted version of X, and returns the log of the logistic function of the difference.</source>
          <target state="translated">这种方法不是确定性的:它在X上计算一个叫做自由能的量,然后在X的随机损坏版本上计算,并返回差值的对数函数的对数。</target>
        </trans-unit>
        <trans-unit id="483c17ab697933f17e74386d9739e36cf3fc93e7" translate="yes" xml:space="preserve">
          <source>This method is only available for log loss and modified Huber loss.</source>
          <target state="translated">这种方法只适用于对数损失和修正的Huber损失。</target>
        </trans-unit>
        <trans-unit id="d7475ebc10f647671bee9a4be7afee1b81279276" translate="yes" xml:space="preserve">
          <source>This method provides a safe way to take a distance matrix as input, while preserving compatibility with many other algorithms that take a vector array.</source>
          <target state="translated">这种方法提供了一种安全的方法,将距离矩阵作为输入,同时保留了与许多其他采用向量数组的算法的兼容性。</target>
        </trans-unit>
        <trans-unit id="b5d8e2fef5ebecb0c66f96f2926a64438412f355" translate="yes" xml:space="preserve">
          <source>This method provides a safe way to take a kernel matrix as input, while preserving compatibility with many other algorithms that take a vector array.</source>
          <target state="translated">这种方法提供了一种安全的方式来接收一个内核矩阵作为输入,同时保留了与其他许多接收向量数组的算法的兼容性。</target>
        </trans-unit>
        <trans-unit id="0fc2db598aaa9c1a0947d8f73a1238d30285a532" translate="yes" xml:space="preserve">
          <source>This method takes either a vector array or a distance matrix, and returns a distance matrix. If the input is a vector array, the distances are computed. If the input is a distances matrix, it is returned instead.</source>
          <target state="translated">本方法接受一个向量数组或距离矩阵,并返回一个距离矩阵。如果输入的是一个向量数组,则计算距离。如果输入的是距离矩阵,则返回距离矩阵。</target>
        </trans-unit>
        <trans-unit id="924736c0bae89c3f4376281e6a105fa549739eb4" translate="yes" xml:space="preserve">
          <source>This method takes either a vector array or a kernel matrix, and returns a kernel matrix. If the input is a vector array, the kernels are computed. If the input is a kernel matrix, it is returned instead.</source>
          <target state="translated">本方法接受一个向量数组或内核矩阵,并返回一个内核矩阵。如果输入的是一个向量数组,则计算核数。如果输入是一个向量数组,则计算核数,如果输入是一个核矩阵,则返回核数。</target>
        </trans-unit>
        <trans-unit id="b80df7fbbffdde8aff9c30af4a5bee17e602075b" translate="yes" xml:space="preserve">
          <source>This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme.</source>
          <target state="translated">这种方法将特征转换为遵循均匀或正态分布。因此,对于一个给定的特征,这种转换倾向于分散最常见的值。它还可以减少(边际)离群值的影响:因此,这是一种稳健的预处理方案。</target>
        </trans-unit>
        <trans-unit id="4a6448f2646e45809baed97b35289a513191212b" translate="yes" xml:space="preserve">
          <source>This method works similarly to the builtin &lt;code&gt;apply&lt;/code&gt;, except that the function is called only if the cache is not up to date.</source>
          <target state="translated">此方法的工作方式与内置 &lt;code&gt;apply&lt;/code&gt; 相似，只是仅在缓存不是最新的时才调用该函数。</target>
        </trans-unit>
        <trans-unit id="3b270b097c02b54b15c6706faba2a05992e48391" translate="yes" xml:space="preserve">
          <source>This metric is furthermore symmetric: switching &lt;code&gt;label_true&lt;/code&gt; with &lt;code&gt;label_pred&lt;/code&gt; will return the same score value. This can be useful to measure the agreement of two independent label assignments strategies on the same dataset when the real ground truth is not known.</source>
          <target state="translated">该指标是进一步对称：开关 &lt;code&gt;label_true&lt;/code&gt; 与 &lt;code&gt;label_pred&lt;/code&gt; 将返回相同的分数值。当不知道真实的地面事实时，这对于测量两个独立标签分配策略在同一数据集上的一致性很有用。</target>
        </trans-unit>
        <trans-unit id="b8da4b4fabd4786b82c03e2c15a17659173e15c8" translate="yes" xml:space="preserve">
          <source>This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won&amp;rsquo;t change the score value in any way.</source>
          <target state="translated">此度量标准与标签的绝对值无关：类或群集标签值的排列不会以任何方式更改得分值。</target>
        </trans-unit>
        <trans-unit id="bfbb6fef2be45da43d1153172735208301268ab3" translate="yes" xml:space="preserve">
          <source>This metric is not symmetric: switching &lt;code&gt;label_true&lt;/code&gt; with &lt;code&gt;label_pred&lt;/code&gt; will return the &lt;a href=&quot;sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt;&lt;code&gt;completeness_score&lt;/code&gt;&lt;/a&gt; which will be different in general.</source>
          <target state="translated">该指标是不对称的：开关 &lt;code&gt;label_true&lt;/code&gt; 与 &lt;code&gt;label_pred&lt;/code&gt; 将返回&lt;a href=&quot;sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt; &lt;code&gt;completeness_score&lt;/code&gt; &lt;/a&gt;这将是不同一般。</target>
        </trans-unit>
        <trans-unit id="d6ecae2ce63387462768b5daf2f548b32eba4de4" translate="yes" xml:space="preserve">
          <source>This metric is not symmetric: switching &lt;code&gt;label_true&lt;/code&gt; with &lt;code&gt;label_pred&lt;/code&gt; will return the &lt;a href=&quot;sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt;&lt;code&gt;homogeneity_score&lt;/code&gt;&lt;/a&gt; which will be different in general.</source>
          <target state="translated">该指标是不对称的：开关 &lt;code&gt;label_true&lt;/code&gt; 与 &lt;code&gt;label_pred&lt;/code&gt; 将返回&lt;a href=&quot;sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt; &lt;code&gt;homogeneity_score&lt;/code&gt; &lt;/a&gt;这将是不同一般。</target>
        </trans-unit>
        <trans-unit id="e3d29b108d9b9b14881da7a1115d336ab614cf19" translate="yes" xml:space="preserve">
          <source>This metric is used in multilabel ranking problem, where the goal is to give better rank to the labels associated to each sample.</source>
          <target state="translated">这个度量用于多标签排名问题,目标是给每个样本相关的标签更好的排名。</target>
        </trans-unit>
        <trans-unit id="70091de439c388c847d5db9bb63c11ef6af9aff3" translate="yes" xml:space="preserve">
          <source>This might be made more clear by an example:</source>
          <target state="translated">举个例子也许能更清楚地说明这一点。</target>
        </trans-unit>
        <trans-unit id="87c35466b9ea86a2466ad7ff0fff224c499553d9" translate="yes" xml:space="preserve">
          <source>This model has many parameters, however the default values are quite reasonable (please see the &lt;a href=&quot;classes#text-feature-extraction-ref&quot;&gt;reference documentation&lt;/a&gt; for the details):</source>
          <target state="translated">该模型具有许多参数，但是默认值是相当合理的（有关详细信息，请参见&lt;a href=&quot;classes#text-feature-extraction-ref&quot;&gt;参考文档&lt;/a&gt;）：</target>
        </trans-unit>
        <trans-unit id="a96824cdb923fbde7424d806cedfbff3d185c97a" translate="yes" xml:space="preserve">
          <source>This model is an extension of the Sequential Karhunen-Loeve Transform from: &lt;code&gt;A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and its Application to Images, IEEE Transactions on Image Processing, Volume 9, Number 8, pp. 1371-1374, August 2000.&lt;/code&gt; See &lt;a href=&quot;http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf&quot;&gt;http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf&lt;/a&gt;</source>
          <target state="translated">该模型是以下顺序Karhunen-Loeve变换的扩展： &lt;code&gt;A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and its Application to Images, IEEE Transactions on Image Processing, Volume 9, Number 8, pp. 1371-1374, August 2000.&lt;/code&gt; 请参见&lt;a href=&quot;http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf&quot;&gt;http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5eaf31e8c5d94894f814f950dadb507546a62c7a" translate="yes" xml:space="preserve">
          <source>This model is similar to the basic Label Propagation algorithm, but uses affinity matrix based on the normalized graph Laplacian and soft clamping across the labels.</source>
          <target state="translated">该模型类似于基本的标签传播算法,但使用基于归一化图Laplacian的亲和矩阵和跨标签的软箝制。</target>
        </trans-unit>
        <trans-unit id="31cfc2556e1a899818d23b9328ee0744c892d322" translate="yes" xml:space="preserve">
          <source>This model optimizes the log-loss function using LBFGS or stochastic gradient descent.</source>
          <target state="translated">该模型采用LBFGS或随机梯度下降法优化对数损失函数。</target>
        </trans-unit>
        <trans-unit id="a4064d8d27531f23ac21cbcbd5928e344df2525d" translate="yes" xml:space="preserve">
          <source>This model optimizes the squared-loss using LBFGS or stochastic gradient descent.</source>
          <target state="translated">该模型采用LBFGS或随机梯度下降法优化平方亏损。</target>
        </trans-unit>
        <trans-unit id="5745ffae87fdf8e03232a3372f515cd928402ef0" translate="yes" xml:space="preserve">
          <source>This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).</source>
          <target state="translated">该模型求解回归模型,损失函数为线性最小二乘函数,正则化由l2-norm给出。也称为岭回归或Tikhonov正则化。该估计器内置了对多变量回归的支持(即当y是形状为[n_samples,n_targets]的二维数组时)。</target>
        </trans-unit>
        <trans-unit id="f19c01936c0bc27e43d782c2c60b0838b0f4894d" translate="yes" xml:space="preserve">
          <source>This module contains both distance metrics and kernels. A brief summary is given on the two here.</source>
          <target state="translated">这个模块包含了距离度量和内核两个方面。在这里对两者做一个简单的总结。</target>
        </trans-unit>
        <trans-unit id="ea7156035377b3d98062532f66578932992ce326" translate="yes" xml:space="preserve">
          <source>This module contains two loaders. The first one, &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt;&lt;/a&gt;, returns a list of the raw texts that can be fed to text feature extractors such as &lt;a href=&quot;../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt;&lt;/a&gt; with custom parameters so as to extract feature vectors. The second one, &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized#sklearn.datasets.fetch_20newsgroups_vectorized&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_20newsgroups_vectorized&lt;/code&gt;&lt;/a&gt;, returns ready-to-use features, i.e., it is not necessary to use a feature extractor.</source>
          <target state="translated">该模块包含两个装载程序。第一个是&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt; &lt;/a&gt;，返回原始文本的列表，这些原始文本可以通过自定义参数馈送到文本特征提取器（例如&lt;a href=&quot;../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text.CountVectorizer&lt;/code&gt; )&lt;/a&gt;，以提取特征向量。第二个是&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized#sklearn.datasets.fetch_20newsgroups_vectorized&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_20newsgroups_vectorized&lt;/code&gt; &lt;/a&gt;，返回即用型功能，即，不必使用功能提取器。</target>
        </trans-unit>
        <trans-unit id="e5dbda685e3f3b6ebc21c265d6368ea8386c5f03" translate="yes" xml:space="preserve">
          <source>This module implements multiclass learning algorithms:</source>
          <target state="translated">本模块实现了多类学习算法。</target>
        </trans-unit>
        <trans-unit id="40fee252ae7de928b5fc80e9416986beafe64ef4" translate="yes" xml:space="preserve">
          <source>This module implements multioutput regression and classification.</source>
          <target state="translated">该模块实现了多输出回归和分类。</target>
        </trans-unit>
        <trans-unit id="f4b5a1fcc345642615c5317116147407ee22511b" translate="yes" xml:space="preserve">
          <source>This module offers support for multi-output problems by implementing this strategy in both &lt;a href=&quot;generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt;&lt;code&gt;DecisionTreeClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.tree.decisiontreeregressor#sklearn.tree.DecisionTreeRegressor&quot;&gt;&lt;code&gt;DecisionTreeRegressor&lt;/code&gt;&lt;/a&gt;. If a decision tree is fit on an output array Y of size &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt; then the resulting estimator will:</source>
          <target state="translated">该模块通过在&lt;a href=&quot;generated/sklearn.tree.decisiontreeclassifier#sklearn.tree.DecisionTreeClassifier&quot;&gt; &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.tree.decisiontreeregressor#sklearn.tree.DecisionTreeRegressor&quot;&gt; &lt;code&gt;DecisionTreeRegressor&lt;/code&gt; 中&lt;/a&gt;实施此策略，为多输出问题提供支持。如果决策树适合大小为 &lt;code&gt;[n_samples, n_outputs]&lt;/code&gt; 的输出数组Y，则结果估计器将：</target>
        </trans-unit>
        <trans-unit id="69b4d83c7d3d58178d932db005c229bef475361e" translate="yes" xml:space="preserve">
          <source>This normalization is implemented by the &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; class:</source>
          <target state="translated">该规范化由&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt;类实现：</target>
        </trans-unit>
        <trans-unit id="36bde3d3011eb8df53f7587f509f261b5588364f" translate="yes" xml:space="preserve">
          <source>This object uses workers to compute in parallel the application of a function to many different arguments. The main functionality it brings in addition to using the raw multiprocessing or concurrent.futures API are (see examples for details):</source>
          <target state="translated">这个对象使用worker来并行计算一个函数对许多不同参数的应用。除了使用原始的多处理或concurrent.futures API外,它带来的主要功能有(详见示例)。</target>
        </trans-unit>
        <trans-unit id="43f85d31a1122b5ce913b6ac1587dc97b9fac8c9" translate="yes" xml:space="preserve">
          <source>This package also features helpers to fetch larger datasets commonly used by the machine learning community to benchmark algorithms on data that comes from the &amp;lsquo;real world&amp;rsquo;.</source>
          <target state="translated">该软件包还具有帮助者获取大型数据集的功能，这些数据集通常被机器学习社区用来对来自&amp;ldquo;现实世界&amp;rdquo;的数据的算法进行基准测试。</target>
        </trans-unit>
        <trans-unit id="f678792533c8ea573ae326139ccc463721df5e43" translate="yes" xml:space="preserve">
          <source>This parameter has been renamed to n_components and will be removed in version 0.21. .. deprecated:: 0.19</source>
          <target state="translated">这个参数已经改名为n_components,并将在0.21版本中被移除。 ...废弃的:。0.19</target>
        </trans-unit>
        <trans-unit id="2008376a104f1f0d722babab4554d9900556a331" translate="yes" xml:space="preserve">
          <source>This parameter is ignored if vocabulary is not None.</source>
          <target state="translated">如果词汇不是None,这个参数会被忽略。</target>
        </trans-unit>
        <trans-unit id="4896edc233b2b945e9bfe76cd7c644f9b147f395" translate="yes" xml:space="preserve">
          <source>This parameter is ignored when &lt;code&gt;fit_intercept&lt;/code&gt; is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use &lt;a href=&quot;sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;sklearn.preprocessing.StandardScaler&lt;/code&gt;&lt;/a&gt; before calling &lt;code&gt;fit&lt;/code&gt; on an estimator with &lt;code&gt;normalize=False&lt;/code&gt;.</source>
          <target state="translated">当 &lt;code&gt;fit_intercept&lt;/code&gt; 设置为False 时，将忽略此参数。如果为True，则将在回归之前通过减去均值并除以l2-范数来对回归变量X进行归一化。如果你想规范，请使用&lt;a href=&quot;sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;sklearn.preprocessing.StandardScaler&lt;/code&gt; &lt;/a&gt;之前调用 &lt;code&gt;fit&lt;/code&gt; 于与估计 &lt;code&gt;normalize=False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="18c6fb9267c5496a0a4415bd237b294d4f877c13" translate="yes" xml:space="preserve">
          <source>This parameter is required for multiclass/multilabel targets. If &lt;code&gt;None&lt;/code&gt;, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:</source>
          <target state="translated">对于多类/多标签目标，此参数是必需的。如果为 &lt;code&gt;None&lt;/code&gt; ，则返回每个班级的分数。否则，这将确定对数据执行的平均类型：</target>
        </trans-unit>
        <trans-unit id="4b54c5323e385131687ecd8fe6362000ef5ec12b" translate="yes" xml:space="preserve">
          <source>This parameters can be accessed through the members &lt;code&gt;dual_coef_&lt;/code&gt; which holds the product \(y_i \alpha_i\), &lt;code&gt;support_vectors_&lt;/code&gt; which holds the support vectors, and &lt;code&gt;intercept_&lt;/code&gt; which holds the independent term \(\rho\) :</source>
          <target state="translated">可以通过成员 &lt;code&gt;dual_coef_&lt;/code&gt; 其包含乘积\（y_i \ alpha_i \））， &lt;code&gt;support_vectors_&lt;/code&gt; (其支持支撑向量）和 &lt;code&gt;intercept_&lt;/code&gt; (其具有独立项\（\ rho \））来访问此参数：</target>
        </trans-unit>
        <trans-unit id="f0e92a41ff311d2df395e780ee2f06d2a63e9cbc" translate="yes" xml:space="preserve">
          <source>This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.</source>
          <target state="translated">这个路径长度,是这种随机树的森林的平均值,是规范性的测量,也是我们的决策函数。</target>
        </trans-unit>
        <trans-unit id="80dec09fc285dd6f9c561fc2931162bad1982cdb" translate="yes" xml:space="preserve">
          <source>This plot compares the decision surfaces learned by a decision tree classifier (first column), by a random forest classifier (second column), by an extra- trees classifier (third column) and by an AdaBoost classifier (fourth column).</source>
          <target state="translated">该图比较了决策树分类器(第一列)、随机森林分类器(第二列)、树外分类器(第三列)和AdaBoost分类器(第四列)学习的决策面。</target>
        </trans-unit>
        <trans-unit id="8e05dc7c1a0ec44a96abb884d1ce621cca93db4e" translate="yes" xml:space="preserve">
          <source>This problem can safely be ignored when the number of samples is more than a thousand and the number of clusters is less than 10. &lt;strong&gt;For smaller sample sizes or larger number of clusters it is safer to use an adjusted index such as the Adjusted Rand Index (ARI)&lt;/strong&gt;.</source>
          <target state="translated">当样本数大于一千且聚类数小于10时，可以安全地忽略此问题。&lt;strong&gt;对于较小的样本量或较大的聚类数，使用调整后的索引（如调整后的兰德指数（ ARI）&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="fcd7e110cbbcbd004685bcd45cf928dd3da1b5b1" translate="yes" xml:space="preserve">
          <source>This procedure (spectral clustering on an image) is an efficient approximate solution for finding normalized graph cuts.</source>
          <target state="translated">这个过程(图像上的光谱聚类)是寻找归一化图形切割的有效近似解。</target>
        </trans-unit>
        <trans-unit id="7a8651d966c336f363771d222433438f229cb5c2" translate="yes" xml:space="preserve">
          <source>This regressor is useful as a simple baseline to compare with other (real) regressors. Do not use it for real problems.</source>
          <target state="translated">这个回归器作为一个简单的基线是有用的,可以与其他(真实的)回归器进行比较。不要将其用于实际问题。</target>
        </trans-unit>
        <trans-unit id="566769fe300350777b617d7ceed39620171814da" translate="yes" xml:space="preserve">
          <source>This scaler can also be applied to sparse CSR or CSC matrices by passing &lt;code&gt;with_mean=False&lt;/code&gt; to avoid breaking the sparsity structure of the data.</source>
          <target state="translated">还可以通过传递 &lt;code&gt;with_mean=False&lt;/code&gt; 来将此缩放器应用于稀疏CSR或CSC矩阵，以避免破坏数据的稀疏性结构。</target>
        </trans-unit>
        <trans-unit id="bf350412f5695ebe05a62d114269ff308d8edf9f" translate="yes" xml:space="preserve">
          <source>This scaler can also be applied to sparse CSR or CSC matrices.</source>
          <target state="translated">该缩放器也可以应用于稀疏的CSR或CSC矩阵。</target>
        </trans-unit>
        <trans-unit id="096cae15373bbc176ca80052b34794345347f334" translate="yes" xml:space="preserve">
          <source>This score can be used to select the n_features features with the highest values for the test chi-squared statistic from X, which must contain only non-negative features such as booleans or frequencies (e.g., term counts in document classification), relative to the classes.</source>
          <target state="translated">这个分数可以用来从X中选择测试chi-squared统计值最高的n_features特征,相对于类而言,它必须只包含布尔值或频率等非负值特征(例如,文档分类中的术语计数)。</target>
        </trans-unit>
        <trans-unit id="bc67f7a4c884a57cb8d65e3051862bc296a2adb5" translate="yes" xml:space="preserve">
          <source>This score is identical to &lt;a href=&quot;sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt;&lt;code&gt;normalized_mutual_info_score&lt;/code&gt;&lt;/a&gt; with the &lt;code&gt;'arithmetic'&lt;/code&gt; option for averaging.</source>
          <target state="translated">该分数与带有 &lt;code&gt;'arithmetic'&lt;/code&gt; 选项进行平均的&lt;a href=&quot;sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt; &lt;code&gt;normalized_mutual_info_score&lt;/code&gt; &lt;/a&gt;相同。</target>
        </trans-unit>
        <trans-unit id="81b377dd4306b3470c7efb10edcaa8d55b57210b" translate="yes" xml:space="preserve">
          <source>This section illustrates the use of a &lt;code&gt;Pipeline&lt;/code&gt; with &lt;code&gt;GridSearchCV&lt;/code&gt;</source>
          <target state="translated">本节说明了将 &lt;code&gt;Pipeline&lt;/code&gt; 与 &lt;code&gt;GridSearchCV&lt;/code&gt; 一起使用</target>
        </trans-unit>
        <trans-unit id="929a45e65dc1a0cd0b685d6194be9ae4708eb857" translate="yes" xml:space="preserve">
          <source>This should make it possible to check that the cross-validation score is in the same range as before.</source>
          <target state="translated">这样一来,应该可以检查出交叉验证的分数是否和以前一样在同一个范围内。</target>
        </trans-unit>
        <trans-unit id="08af461e03baea2ad13f22a738ff3dde29c2a50f" translate="yes" xml:space="preserve">
          <source>This shows an example of a neighbors-based query (in particular a kernel density estimate) on geospatial data, using a Ball Tree built upon the Haversine distance metric &amp;ndash; i.e. distances over points in latitude/longitude. The dataset is provided by Phillips et. al. (2006). If available, the example uses &lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;basemap&lt;/a&gt; to plot the coast lines and national boundaries of South America.</source>
          <target state="translated">这显示了一个示例，该示例使用基于Haversine距离度量（即，经度/纬度上的点的距离）构建的球树，对地理空间数据进行基于邻居的查询（尤其是内核密度估计）。数据集由Phillips等提供。等 （2006）。如果可用，该示例将使用&lt;a href=&quot;http://matplotlib.org/basemap&quot;&gt;底图&lt;/a&gt;来绘制南美的海岸线和国家边界。</target>
        </trans-unit>
        <trans-unit id="38716491ad409e2f991bc1db8f7b1d944921bf99" translate="yes" xml:space="preserve">
          <source>This sort of preprocessing can be streamlined with the &lt;a href=&quot;compose#pipeline&quot;&gt;Pipeline&lt;/a&gt; tools. A single object representing a simple polynomial regression can be created and used as follows:</source>
          <target state="translated">可以使用&lt;a href=&quot;compose#pipeline&quot;&gt;管道&lt;/a&gt;工具简化此类预处理。可以创建并使用一个代表简单多项式回归的对象，如下所示：</target>
        </trans-unit>
        <trans-unit id="4e6050ab2083fb67a848ffe7f83ae292a8f60f62" translate="yes" xml:space="preserve">
          <source>This strategy can also be used for multilabel learning, where a classifier is used to predict multiple labels for instance, by fitting on a 2-d matrix in which cell [i, j] is 1 if sample i has label j and 0 otherwise.</source>
          <target state="translated">这种策略也可以用于多标签学习,比如一个分类器用于预测多个标签,在一个2-d矩阵上进行拟合,其中如果样本i有标签j,则单元[i,j]为1,否则为0。</target>
        </trans-unit>
        <trans-unit id="8cd6a64f79d725ef1cdd342315685305aab51887" translate="yes" xml:space="preserve">
          <source>This strategy consists in fitting one classifier per class pair. At prediction time, the class which received the most votes is selected. Since it requires to fit &lt;code&gt;n_classes * (n_classes - 1) / 2&lt;/code&gt; classifiers, this method is usually slower than one-vs-the-rest, due to its O(n_classes^2) complexity. However, this method may be advantageous for algorithms such as kernel algorithms which don&amp;rsquo;t scale well with &lt;code&gt;n_samples&lt;/code&gt;. This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used &lt;code&gt;n_classes&lt;/code&gt; times.</source>
          <target state="translated">该策略包括为每个类别对配备一个分类器。在预测时，选择获得最多选票的班级。由于该方法需要适合 &lt;code&gt;n_classes * (n_classes - 1) / 2&lt;/code&gt; 分类器，因此，由于其O（n_classes ^ 2）复杂性，因此该方法通常比其余方法慢一倍。但是，此方法对于无法很好地与 &lt;code&gt;n_samples&lt;/code&gt; 进行缩放的算法（例如内核算法）可能是有利的。这是因为每个单独的学习问题仅涉及数据的一小部分，而相对于其余部分，完整数据集的使用次数为 &lt;code&gt;n_classes&lt;/code&gt; 次。</target>
        </trans-unit>
        <trans-unit id="de640b51f281cc0046c1a9e652c58d4e96ebef0b" translate="yes" xml:space="preserve">
          <source>This strategy consists of fitting one classifier per target. This is a simple strategy for extending classifiers that do not natively support multi-target classification</source>
          <target state="translated">这种策略包括为每个目标拟合一个分类器。这是对不支持多目标分类的分类器进行扩展的简单策略。</target>
        </trans-unit>
        <trans-unit id="7d6fb6c6a7f2b79b31f4627b09bbb93dcc2f3bc4" translate="yes" xml:space="preserve">
          <source>This strategy consists of fitting one regressor per target. This is a simple strategy for extending regressors that do not natively support multi-target regression.</source>
          <target state="translated">这个策略包括对每个目标拟合一个回归器。这是一个简单的策略,用于扩展不支持多目标回归的回归器。</target>
        </trans-unit>
        <trans-unit id="27cbaceed22a6f90d5ae07c700825d27bfca1f78" translate="yes" xml:space="preserve">
          <source>This strategy has several advantages:</source>
          <target state="translated">这种策略有几个优点。</target>
        </trans-unit>
        <trans-unit id="91d49a77db9793bc904e6dbf1c0dda029b6c110b" translate="yes" xml:space="preserve">
          <source>This strategy is illustrated below.</source>
          <target state="translated">这种策略如下图所示。</target>
        </trans-unit>
        <trans-unit id="02849272fb07ed52ea9b00f7ccc02a748c971372" translate="yes" xml:space="preserve">
          <source>This strategy, also known as &lt;strong&gt;one-vs-all&lt;/strong&gt;, is implemented in &lt;a href=&quot;generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;OneVsRestClassifier&lt;/code&gt;&lt;/a&gt;. The strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only &lt;code&gt;n_classes&lt;/code&gt; classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and only one classifier, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy and is a fair default choice.</source>
          <target state="translated">这一策略，也被称为&lt;strong&gt;一个-VS-所有&lt;/strong&gt;，在实施&lt;a href=&quot;generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;OneVsRestClassifier&lt;/code&gt; &lt;/a&gt;。该策略包括为每个类别配备一个分类器。对于每个分类器，该分类将与所有其他分类进行拟合。除了其计算效率（仅需要 &lt;code&gt;n_classes&lt;/code&gt; 个分类器）之外，此方法的一个优点是其可解释性。由于每个类别仅由一个分类器表示，因此可以通过检查其对应的分类器来获取有关该类别的知识。这是最常用的策略，并且是合理的默认选择。</target>
        </trans-unit>
        <trans-unit id="27ac94ab878e8b852843daf6fdc6644656d86a0e" translate="yes" xml:space="preserve">
          <source>This submodule contains functions that approximate the feature mappings that correspond to certain kernels, as they are used for example in support vector machines (see &lt;a href=&quot;svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt;). The following feature functions perform non-linear transformations of the input, which can serve as a basis for linear classification or other algorithms.</source>
          <target state="translated">该子模块包含近似于与某些内核对应的特征映射的函数，例如在支持向量机中使用的功能（请参阅&lt;a href=&quot;svm#svm&quot;&gt;支持向量机&lt;/a&gt;）。以下功能函数执行输入的非线性转换，可以用作线性分类或其他算法的基础。</target>
        </trans-unit>
        <trans-unit id="fcbae9cfcf0bd03c252edbce0ec54b00175fa149" translate="yes" xml:space="preserve">
          <source>This test can be applied to classes or instances. Classes currently have some additional tests that related to construction, while passing instances allows the testing of multiple options.</source>
          <target state="translated">这个测试可以应用于类或实例。目前,类有一些与构造有关的附加测试,而通过实例可以测试多个选项。</target>
        </trans-unit>
        <trans-unit id="fe4512e0330ee3f9d4d908b10acadfb35dcb4b46" translate="yes" xml:space="preserve">
          <source>This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.</source>
          <target state="translated">这个文本向量器的实现使用哈希技巧来寻找标记字符串名称到特征整数索引的映射。</target>
        </trans-unit>
        <trans-unit id="90704510327932a48fb3c52155398163f97475e2" translate="yes" xml:space="preserve">
          <source>This transformation is often used as an alternative to zero mean, unit variance scaling.</source>
          <target state="translated">这种变换经常被用作零均值、单位方差缩放的替代方法。</target>
        </trans-unit>
        <trans-unit id="2f720ce11fea82f150f2e1314efc3b6e74d2aa65" translate="yes" xml:space="preserve">
          <source>This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion).</source>
          <target state="translated">这个变换器既能处理密集的numpy数组,也能处理scipy.sparse矩阵(如果你想避免复制/转换的负担,请使用CSR格式)。</target>
        </trans-unit>
        <trans-unit id="22f51ce5936d304512713b7c2d6420a9339ed07c" translate="yes" xml:space="preserve">
          <source>This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently.</source>
          <target state="translated">本变换器通过截断奇异值分解(SVD)进行线性维度降低。与PCA不同的是,这个估计器在计算奇异值分解之前并不对数据进行居中处理,这意味着它可以有效地处理scipy.sparse矩阵。这意味着它可以高效地处理scipy.sparse矩阵。</target>
        </trans-unit>
        <trans-unit id="c058bdf39166834e7b3e02bee2a3dc32df1bea5c" translate="yes" xml:space="preserve">
          <source>This transformer turns lists of mappings (dict-like objects) of feature names to feature values into Numpy arrays or scipy.sparse matrices for use with scikit-learn estimators.</source>
          <target state="translated">这个变换器将特征名与特征值的映射列表(类似dict的对象)变成Numpy数组或scipy.sparse矩阵,用于scikit-learn估计器。</target>
        </trans-unit>
        <trans-unit id="6fa17f9065133747f6dee2c04fd6c387874c04ab" translate="yes" xml:space="preserve">
          <source>This tutorial will explore &lt;em&gt;statistical learning&lt;/em&gt;, the use of machine learning techniques with the goal of &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_inference&quot;&gt;statistical inference&lt;/a&gt;: drawing conclusions on the data at hand.</source>
          <target state="translated">本教程将探讨&lt;em&gt;统计学习&lt;/em&gt;，以及将机器学习技术用于&lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_inference&quot;&gt;统计推断&lt;/a&gt;的目的：根据手头的数据得出结论。</target>
        </trans-unit>
        <trans-unit id="d770164eec068c2686e18d9f67f7678a7fe3d2ab" translate="yes" xml:space="preserve">
          <source>This uses the Benjamini-Hochberg procedure. &lt;code&gt;alpha&lt;/code&gt; is an upper bound on the expected false discovery rate.</source>
          <target state="translated">这使用了Benjamini-Hochberg过程。 &lt;code&gt;alpha&lt;/code&gt; 是预期的错误发现率的上限。</target>
        </trans-unit>
        <trans-unit id="25630de50e6415b67bb72ea47abf6e457ed32d31" translate="yes" xml:space="preserve">
          <source>This uses the score defined by &lt;code&gt;scoring&lt;/code&gt; where provided, and the &lt;code&gt;best_estimator_.score&lt;/code&gt; method otherwise.</source>
          <target state="translated">这将使用通过 &lt;code&gt;scoring&lt;/code&gt; 在提供的地方）定义的评分，否则使用 &lt;code&gt;best_estimator_.score&lt;/code&gt; 方法。</target>
        </trans-unit>
        <trans-unit id="57e78bad29e459afb6f7e8e9a46e0b5e6e5f4fa9" translate="yes" xml:space="preserve">
          <source>This value is valid if class_weight parameter in fit() is not set.</source>
          <target state="translated">如果fit()中的class_weight参数没有设置,这个值是有效的。</target>
        </trans-unit>
        <trans-unit id="0973d55bbbd406d7d050b325e278935eae6a368e" translate="yes" xml:space="preserve">
          <source>This value of the mutual information and also the normalized variant is not adjusted for chance and will tend to increase as the number of different labels (clusters) increases, regardless of the actual amount of &amp;ldquo;mutual information&amp;rdquo; between the label assignments.</source>
          <target state="translated">互信息的值以及归一化的变体都不会偶然调整，并且会随着不同标签（集群）数量的增加而增加，而与标签分配之间的&amp;ldquo;互信息&amp;rdquo;的实际数量无关。</target>
        </trans-unit>
        <trans-unit id="ec27c204380d1bf1bec671104c4e5cc563667983" translate="yes" xml:space="preserve">
          <source>This visualization is an example of a &lt;em&gt;kernel density estimation&lt;/em&gt;, in this case with a top-hat kernel (i.e. a square block at each point). We can recover a smoother distribution by using a smoother kernel. The bottom-right plot shows a Gaussian kernel density estimate, in which each point contributes a Gaussian curve to the total. The result is a smooth density estimate which is derived from the data, and functions as a powerful non-parametric model of the distribution of points.</source>
          <target state="translated">该可视化是&lt;em&gt;内核密度估计&lt;/em&gt;的示例，在这种情况下，使用高礼帽内核（即每个点处的正方形块）。我们可以使用更平滑的内核来恢复更平滑的分布。右下角的图显示了高斯核密度估计，其中每个点对总和贡献了高斯曲线。结果是一个平滑的密度估计值，该估计值是从数据得出的，并且可以用作功能强大的点分布的非参数模型。</target>
        </trans-unit>
        <trans-unit id="1cad85e71e9b226b43b5778c8058de4fe70516a7" translate="yes" xml:space="preserve">
          <source>This warning is used to notify the user that BLAS was not used for dot operation and hence the efficiency may be affected.</source>
          <target state="translated">该警告用于通知用户,BLAS没有用于点操作,因此效率可能会受到影响。</target>
        </trans-unit>
        <trans-unit id="7b4c8162b5298ba9d922a2200274b38ddddf44e8" translate="yes" xml:space="preserve">
          <source>This warning notifies the user that the efficiency may not be optimal due to some reason which may be included as a part of the warning message. This may be subclassed into a more specific Warning class.</source>
          <target state="translated">这个警告通知用户,由于某些原因,效率可能不是最佳的,这些原因可能会作为警告信息的一部分。这可能会被子类化为一个更具体的警告类。</target>
        </trans-unit>
        <trans-unit id="ec79da6e5e29f4afd0662e82ec298c39ed6dbabd" translate="yes" xml:space="preserve">
          <source>This warning occurs when some input data needs to be converted or interpreted in a way that may not match the user&amp;rsquo;s expectations.</source>
          <target state="translated">当某些输入数据需要以不符合用户期望的方式进行转换或解释时，会发生此警告。</target>
        </trans-unit>
        <trans-unit id="14994b75958434504d6803fa4be46a86d6219fc9" translate="yes" xml:space="preserve">
          <source>This was originally a term weighting scheme developed for information retrieval (as a ranking function for search engines results) that has also found good use in document classification and clustering.</source>
          <target state="translated">这原本是一种为信息检索而开发的术语加权方案(作为搜索引擎结果的排名功能),在文档分类和聚类中也有很好的应用。</target>
        </trans-unit>
        <trans-unit id="90d00e9f85af52e63288d2fca3d9f513dce9de12" translate="yes" xml:space="preserve">
          <source>This, however, is not the case in the Ledoit-Wolf procedure when the population covariance happens to be a multiple of the identity matrix. In this case, the Ledoit-Wolf shrinkage estimate approaches 1 as the number of samples increases. This indicates that the optimal estimate of the covariance matrix in the Ledoit-Wolf sense is multiple of the identity. Since the population covariance is already a multiple of the identity matrix, the Ledoit-Wolf solution is indeed a reasonable estimate.</source>
          <target state="translated">然而,在Ledoit-Wolf程序中,当人口协方差恰好是身份矩阵的倍数时,情况并非如此。在这种情况下,随着样本数量的增加,Ledoit-Wolf的收缩估计值接近1。这说明Ledoit-Wolf意义上的协方差矩阵的最优估计是身份的倍数。由于种群协方差已经是身份矩阵的倍数,所以Ledoit-Wolf解确实是一个合理的估计。</target>
        </trans-unit>
        <trans-unit id="e911226999d28ae4c4eb95cef049955b008548cf" translate="yes" xml:space="preserve">
          <source>Those 3 metrics are independent of the absolute values of the labels: a permutation of the class or cluster label values won&amp;rsquo;t change the score values in any way.</source>
          <target state="translated">这3个指标与标签的绝对值无关：类或群集标签值的排列不会以任何方式更改得分值。</target>
        </trans-unit>
        <trans-unit id="a17151aff3f6e79d6bfb3bc3e7c5d30ff3b77b7d" translate="yes" xml:space="preserve">
          <source>Those metrics are based on normalized conditional entropy measures of the clustering labeling to evaluate given the knowledge of a Ground Truth class labels of the same samples.</source>
          <target state="translated">这些度量是基于聚类标签的归一化条件熵度量,来评估给定相同样本的Ground Truth类标签的知识。</target>
        </trans-unit>
        <trans-unit id="831e093286e91d34e1415d38600e7c8277f14a07" translate="yes" xml:space="preserve">
          <source>Though not technically a variant of LLE, Local tangent space alignment (LTSA) is algorithmically similar enough to LLE that it can be put in this category. Rather than focusing on preserving neighborhood distances as in LLE, LTSA seeks to characterize the local geometry at each neighborhood via its tangent space, and performs a global optimization to align these local tangent spaces to learn the embedding. LTSA can be performed with function &lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt;&lt;code&gt;locally_linear_embedding&lt;/code&gt;&lt;/a&gt; or its object-oriented counterpart &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;, with the keyword &lt;code&gt;method = 'ltsa'&lt;/code&gt;.</source>
          <target state="translated">尽管从技术上讲不是LLE的变体，但局部切线空间对齐（LTSA）在算法上与LLE足够相似，因此可以将其归入此类。 LTSA并没有像LLE中那样关注保留邻域距离，而是寻求通过其切线空间表征每个邻域的局部几何形状，并执行全局优化以对齐这些局部切线空间以学习嵌入。 LTSA可以使用&lt;a href=&quot;generated/sklearn.manifold.locally_linear_embedding#sklearn.manifold.locally_linear_embedding&quot;&gt; &lt;code&gt;locally_linear_embedding&lt;/code&gt; &lt;/a&gt;函数或其面向对象的对&lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt;通过关键字 &lt;code&gt;method = 'ltsa'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="3dcfc8b5bdf930c1b66451c5dc30f486901100ee" translate="yes" xml:space="preserve">
          <source>Three different types of SVM-Kernels are displayed below. The polynomial and RBF are especially useful when the data-points are not linearly separable.</source>
          <target state="translated">下面显示了三种不同类型的SVM-Kernels。当数据点不可线性分离时,多项式和RBF特别有用。</target>
        </trans-unit>
        <trans-unit id="2eb0d5d5e8d716a06a5bc42a652c1781cf721343" translate="yes" xml:space="preserve">
          <source>Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors.</source>
          <target state="translated">采样特征二值化(映射到布尔运算)的阈值。如果无,则假定输入已经由二进制向量组成。</target>
        </trans-unit>
        <trans-unit id="ac359cd376aaf3163dffdc564a92f8f55ebdbfe9" translate="yes" xml:space="preserve">
          <source>Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.</source>
          <target state="translated">树木生长中早期停止的阈值。如果一个节点的不纯度高于阈值,则该节点将分裂,否则就是叶子。</target>
        </trans-unit>
        <trans-unit id="d2dde1e4fd07fa9e4ff99bf50a843f5c394281b4" translate="yes" xml:space="preserve">
          <source>Threshold for shrinking centroids to remove features.</source>
          <target state="translated">缩小中心点以去除特征的阈值。</target>
        </trans-unit>
        <trans-unit id="5b50eca69565a6240c9cb586697767c09ac4525e" translate="yes" xml:space="preserve">
          <source>Threshold on the size of arrays passed to the workers that triggers automated memory mapping in temp_folder. Can be an int in Bytes, or a human-readable string, e.g., &amp;lsquo;1M&amp;rsquo; for 1 megabyte. Use None to disable memmapping of large arrays. Only active when backend=&amp;rdquo;loky&amp;rdquo; or &amp;ldquo;multiprocessing&amp;rdquo;.</source>
          <target state="translated">传递给工作线程的数组大小的阈值，该阈值触发temp_folder中的自动内存映射。可以是Byte形式的int值，也可以是人类可读的字符串，例如1 MB的'1M'。使用None禁用大型阵列的映射。仅在backend =&amp;ldquo; loky&amp;rdquo;或&amp;ldquo; multiprocessing&amp;rdquo;时激活。</target>
        </trans-unit>
        <trans-unit id="3bf722c4ec04176f091be4d50fbd629d5b754a20" translate="yes" xml:space="preserve">
          <source>Threshold used for rank estimation in SVD solver.</source>
          <target state="translated">在SVD求解器中用于等级估计的阈值。</target>
        </trans-unit>
        <trans-unit id="0168a115989469a76c56e8c46c0d56b1a01f88c6" translate="yes" xml:space="preserve">
          <source>Threshold used for rank estimation.</source>
          <target state="translated">用于等级估计的阈值。</target>
        </trans-unit>
        <trans-unit id="558232b0add0e7cf1e4001c15b7a509781ecfb59" translate="yes" xml:space="preserve">
          <source>Threshold used in the binary and multi-label cases.</source>
          <target state="translated">二元和多标签情况下使用的阈值。</target>
        </trans-unit>
        <trans-unit id="d260a173cc06214ee3d2352996ea165371c17c29" translate="yes" xml:space="preserve">
          <source>Thresholding</source>
          <target state="translated">Thresholding</target>
        </trans-unit>
        <trans-unit id="8265a18b28c2cb3c5a28ceb45384d9a49c2f7715" translate="yes" xml:space="preserve">
          <source>Thresholding is clearly not useful for denoising, but it is here to show that it can produce a suggestive output with very high speed, and thus be useful for other tasks such as object classification, where performance is not necessarily related to visualisation.</source>
          <target state="translated">阈值对于去噪显然是没有用的,但在这里要说明的是,它可以以非常高的速度产生提示性的输出,因此对于其他任务,如对象分类是有用的,而这些任务的性能与可视化没有必然联系。</target>
        </trans-unit>
        <trans-unit id="c4d29a75003891e7d5c5dbb3dea7166bf19f4ab9" translate="yes" xml:space="preserve">
          <source>Thresholding is very fast but it does not yield accurate reconstructions. They have been shown useful in literature for classification tasks. For image reconstruction tasks, orthogonal matching pursuit yields the most accurate, unbiased reconstruction.</source>
          <target state="translated">阈值化的速度非常快,但它不能产生准确的重建。在文献中,它们已被证明对分类任务是有用的。对于图像重建任务,正交匹配追求产生最准确、无偏的重建。</target>
        </trans-unit>
        <trans-unit id="3904c870d9e800cc53a98ecb8acef59d010fad3d" translate="yes" xml:space="preserve">
          <source>Throw a ValueError if X contains NaN or infinity.</source>
          <target state="translated">如果X包含NaN或无穷大,则抛出一个ValueError。</target>
        </trans-unit>
        <trans-unit id="8b8612c016401dc529cb09be5ddd6996fe872d9c" translate="yes" xml:space="preserve">
          <source>Thus in binary classification, the count of true negatives is \(C_{0,0}\), false negatives is \(C_{1,0}\), true positives is \(C_{1,1}\) and false positives is \(C_{0,1}\).</source>
          <target state="translated">因此,在二元分类中,真否定的计数是/(C_{0,0}/),假否定的计数是/(C_{1,0}/),真阳性的计数是/(C_{1,1}/),假阳性的计数是/(C_{0,1}/)。</target>
        </trans-unit>
        <trans-unit id="877864e25b035038afd6bbe5a72ca90fb8e0741e" translate="yes" xml:space="preserve">
          <source>Thus the median of the input becomes the mean of the output, centered at 0. The normal output is clipped so that the input&amp;rsquo;s minimum and maximum &amp;mdash; corresponding to the 1e-7 and 1 - 1e-7 quantiles respectively &amp;mdash; do not become infinite under the transformation.</source>
          <target state="translated">因此，输入的中位数变为以0为中心的输出的平均值。正常输出被裁剪，以便输入的最小和最大值（分别对应于1e-7和1-1e-7分位数）在以下条件下不会变为无限大转变。</target>
        </trans-unit>
        <trans-unit id="0808b4cdf67452766c8c5389635c6458f9990f5b" translate="yes" xml:space="preserve">
          <source>Thus, most of the target signal (34.4ppm) is explained by a long-term rising trend (length-scale 41.8 years). The periodic component has an amplitude of 3.27ppm, a decay time of 180 years and a length-scale of 1.44. The long decay time indicates that we have a locally very close to periodic seasonal component. The correlated noise has an amplitude of 0.197ppm with a length scale of 0.138 years and a white-noise contribution of 0.197ppm. Thus, the overall noise level is very small, indicating that the data can be very well explained by the model. The figure shows also that the model makes very confident predictions until around 2015</source>
          <target state="translated">因此,大部分目标信号(34.4ppm)由长期上升趋势(长度尺度41.8年)解释。周期性成分的振幅为3.27ppm,衰减时间为180年,长度尺度为1.44年。衰减时间长,说明我们有一个局部非常接近周期性的季节成分。相关噪声的振幅为0.197ppm,长度尺度为0.138年,白噪声贡献为0.197ppm。因此,整体的噪声水平非常小,说明数据可以被模型很好地解释。该图还显示,该模型在2015年左右之前都能做出非常有把握的预测。</target>
        </trans-unit>
        <trans-unit id="4ab3c0245825ab663f7197647adadf073e4b3e64" translate="yes" xml:space="preserve">
          <source>Thus, most of the target signal (34.4ppm) is explained by a long-term rising trend (length-scale 41.8 years). The periodic component has an amplitude of 3.27ppm, a decay time of 180 years and a length-scale of 1.44. The long decay time indicates that we have a locally very close to periodic seasonal component. The correlated noise has an amplitude of 0.197ppm with a length scale of 0.138 years and a white-noise contribution of 0.197ppm. Thus, the overall noise level is very small, indicating that the data can be very well explained by the model. The figure shows also that the model makes very confident predictions until around 2015.</source>
          <target state="translated">因此,大部分目标信号(34.4ppm)由长期上升趋势(长度尺度41.8年)解释。周期性成分的振幅为3.27ppm,衰减时间为180年,长度尺度为1.44年。衰减时间长,说明我们有一个局部非常接近周期性的季节成分。相关噪声的振幅为0.197ppm,长度尺度为0.138年,白噪声贡献为0.197ppm。因此,整体的噪声水平非常小,说明数据可以被模型很好地解释。该图还显示,该模型在2015年左右之前都能做出非常有把握的预测。</target>
        </trans-unit>
        <trans-unit id="af16f18f91308907d1dd8226e54112fa0bd29044" translate="yes" xml:space="preserve">
          <source>Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH: An efficient data clustering method for large databases. &lt;a href=&quot;http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf&quot;&gt;http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf&lt;/a&gt;</source>
          <target state="translated">张天，Raghu Ramakrishnan，Maron Livny BIRCH：一种用于大型数据库的有效数据聚类方法。&lt;a href=&quot;http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf&quot;&gt;http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="d53cad37906f55db18b858cf86bfeef9ad9688eb" translate="yes" xml:space="preserve">
          <source>Tibshirani, R., Hastie, T., Narasimhan, B., &amp;amp; Chu, G. (2002). Diagnosis of multiple cancer types by shrunken centroids of gene expression. Proceedings of the National Academy of Sciences of the United States of America, 99(10), 6567-6572. The National Academy of Sciences.</source>
          <target state="translated">Tibshirani，R.，Hastie，T.，Narasimhan，B.，＆Chu，G.（2002）。通过基因表达的收缩质心诊断多种癌症。美国国家科学院院刊，99（10），6567-6572。美国国家科学院。</target>
        </trans-unit>
        <trans-unit id="a297f524f28779281bb4e53d7b6af672dcac3672" translate="yes" xml:space="preserve">
          <source>Ties are broken using the secondary method from Leeuw, 1977.</source>
          <target state="translated">用Leeuw,1977年的次要方法破除纽带。</target>
        </trans-unit>
        <trans-unit id="59976e05663a4d82c80a3273030c2c2f87094f4d" translate="yes" xml:space="preserve">
          <source>Ties between features with equal scores will be broken in an unspecified way.</source>
          <target state="translated">得分相同的特征之间的平局将以一种不特定的方式被打破。</target>
        </trans-unit>
        <trans-unit id="c41dd9e78b42392c90f4c6ddfb54f7863f5482f1" translate="yes" xml:space="preserve">
          <source>Ties in &lt;code&gt;y_scores&lt;/code&gt; are broken by giving maximal rank that would have been assigned to all tied values.</source>
          <target state="translated">&lt;code&gt;y_scores&lt;/code&gt; 中的关系通过给出将分配给所有绑定值的最大等级来中断。</target>
        </trans-unit>
        <trans-unit id="ba73dffe02601a1abd345b6200b276334877401b" translate="yes" xml:space="preserve">
          <source>Time Series cross-validator</source>
          <target state="translated">时间序列交叉验证器</target>
        </trans-unit>
        <trans-unit id="bbad16d201e3f82cae87bba42e6286ebcef9d190" translate="yes" xml:space="preserve">
          <source>Time series data is characterised by the correlation between observations that are near in time (&lt;em&gt;autocorrelation&lt;/em&gt;). However, classical cross-validation techniques such as &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt;&lt;code&gt;ShuffleSplit&lt;/code&gt;&lt;/a&gt; assume the samples are independent and identically distributed, and would result in unreasonable correlation between training and testing instances (yielding poor estimates of generalisation error) on time series data. Therefore, it is very important to evaluate our model for time series data on the &amp;ldquo;future&amp;rdquo; observations least like those that are used to train the model. To achieve this, one solution is provided by &lt;a href=&quot;generated/sklearn.model_selection.timeseriessplit#sklearn.model_selection.TimeSeriesSplit&quot;&gt;&lt;code&gt;TimeSeriesSplit&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">时间序列数据的特征在于接近时间的观测值之间的相关性（&lt;em&gt;自相关&lt;/em&gt;）。但是，经典的交叉验证技术（例如&lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt; &lt;code&gt;ShuffleSplit&lt;/code&gt; )&lt;/a&gt;假设样本是独立且均匀分布的，并且会导致时间序列数据的训练实例与测试实例之间出现不合理的相关性（产生泛化误差的不良估计）。因此，对我们的模型进行评估以获取关于&amp;ldquo;未来&amp;rdquo;观测的时间序列数据非常重要，这至少要像用于训练模型的观测那样。为此，&lt;a href=&quot;generated/sklearn.model_selection.timeseriessplit#sklearn.model_selection.TimeSeriesSplit&quot;&gt; &lt;code&gt;TimeSeriesSplit&lt;/code&gt; &lt;/a&gt;提供了一种解决方案。</target>
        </trans-unit>
        <trans-unit id="a6268d75d578276d37dec9fce6dea804677e6b49" translate="yes" xml:space="preserve">
          <source>Timeout limit for each task to complete. If any task takes longer a TimeOutError will be raised. Only applied when n_jobs != 1</source>
          <target state="translated">每个任务完成的超时限制。如果任何任务需要更长的时间,将引发TimeOutError。只适用于n_jobs !=1时。</target>
        </trans-unit>
        <trans-unit id="8879146d32620a1e603bf26a188c9426e79a5ed0" translate="yes" xml:space="preserve">
          <source>To address the computational inefficiencies of the brute-force approach, a variety of tree-based data structures have been invented. In general, these structures attempt to reduce the required number of distance calculations by efficiently encoding aggregate distance information for the sample. The basic idea is that if point \(A\) is very distant from point \(B\), and point \(B\) is very close to point \(C\), then we know that points \(A\) and \(C\) are very distant, &lt;em&gt;without having to explicitly calculate their distance&lt;/em&gt;. In this way, the computational cost of a nearest neighbors search can be reduced to \(O[D N \log(N)]\) or better. This is a significant improvement over brute-force for large \(N\).</source>
          <target state="translated">为了解决暴力破解方法的计算效率低下的问题，发明了多种基于树的数据结构。通常，这些结构试图通过有效地编码样本的集合距离信息来减少所需的距离计算数量。基本思想是，如果点\（A \）与点\（B \）距离很远，并且点\（B \）非常接近点\（C \），那么我们知道点\（A \ ）和\（C \）距离很远，&lt;em&gt;而不必显式计算它们的距离&lt;/em&gt;。这样，可以将最近邻居搜索的计算成本降低到\（O [DN \ log（N）] \）或更好。对于大\（N \），这是对蛮力的重大改进。</target>
        </trans-unit>
        <trans-unit id="81ec528524d941df99755c9bb7fceaf80c6a8752" translate="yes" xml:space="preserve">
          <source>To address the inefficiencies of KD Trees in higher dimensions, the &lt;em&gt;ball tree&lt;/em&gt; data structure was developed. Where KD trees partition data along Cartesian axes, ball trees partition data in a series of nesting hyper-spheres. This makes tree construction more costly than that of the KD tree, but results in a data structure which can be very efficient on highly structured data, even in very high dimensions.</source>
          <target state="translated">为了解决高维KD树的低效率问题，开发了&lt;em&gt;球树&lt;/em&gt;数据结构。在KD树沿笛卡尔轴划分数据的地方，球树在一系列嵌套的超球体中划分数据。与KD树相比，这使树的构建成本更高，但导致的数据结构即使在非常高的维度上也可以非常高效地处理高度结构化的数据。</target>
        </trans-unit>
        <trans-unit id="a11d5f0b5df4ea3ced24dc7521fb6d9f97740ba3" translate="yes" xml:space="preserve">
          <source>To address this concern, a number of supervised and unsupervised linear dimensionality reduction frameworks have been designed, such as Principal Component Analysis (PCA), Independent Component Analysis, Linear Discriminant Analysis, and others. These algorithms define specific rubrics to choose an &amp;ldquo;interesting&amp;rdquo; linear projection of the data. These methods can be powerful, but often miss important non-linear structure in the data.</source>
          <target state="translated">为了解决此问题，已设计了许多有监督和无监督的线性降维框架，例如主成分分析（PCA），独立成分分析，线性判别分析等。这些算法定义了特定的原则，以选择数据的&amp;ldquo;有趣&amp;rdquo;线性投影。这些方法功能强大，但通常会丢失数据中重要的非线性结构。</target>
        </trans-unit>
        <trans-unit id="c134b5f4c4fa3b034f915a1c4077d9f58401c669" translate="yes" xml:space="preserve">
          <source>To address this issue you can use &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;sklearn.decomposition.PCA&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;whiten=True&lt;/code&gt; to further remove the linear correlation across features.</source>
          <target state="translated">要解决此问题，您可以将&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;sklearn.decomposition.PCA&lt;/code&gt; &lt;/a&gt;与 &lt;code&gt;whiten=True&lt;/code&gt; 一起使用，以进一步消除跨要素的线性相关性。</target>
        </trans-unit>
        <trans-unit id="5dfb268e42cc748904256c70f6b80b2da01edce9" translate="yes" xml:space="preserve">
          <source>To also transform a test set \(X\), we multiply it with \(V_k\):</source>
          <target state="translated">要转换一个测试集(X),我们将它与(V_k)相乘。</target>
        </trans-unit>
        <trans-unit id="8ef7600ab8e39fc13b7dc9585804325c8072844d" translate="yes" xml:space="preserve">
          <source>To avoid instability issues in case the system is under-determined, regularization can be applied (Ridge regression) via the &lt;code&gt;ridge_alpha&lt;/code&gt; parameter.</source>
          <target state="translated">为了避免在系统不确定的情况下出现不稳定问题，可以通过 &lt;code&gt;ridge_alpha&lt;/code&gt; 参数应用正则化（Ridge回归）。</target>
        </trans-unit>
        <trans-unit id="622754a0a375aafa66f9d8336ffd00fcfc0a1948" translate="yes" xml:space="preserve">
          <source>To avoid memory copy the caller should pass a CSC matrix.</source>
          <target state="translated">为了避免内存拷贝,调用者应该传递一个CSC矩阵。</target>
        </trans-unit>
        <trans-unit id="21a05d95ccb73f51d245c302b02e9a8f32df0276" translate="yes" xml:space="preserve">
          <source>To avoid memory copy the caller should pass a CSR matrix.</source>
          <target state="translated">为了避免内存拷贝,呼叫者应该传递一个CSR矩阵。</target>
        </trans-unit>
        <trans-unit id="e5ab0a4f687079cc593610e8d8c0f15b79824d4d" translate="yes" xml:space="preserve">
          <source>To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format.</source>
          <target state="translated">为了避免内存重新分配,建议直接使用该格式在内存中分配初始数据。</target>
        </trans-unit>
        <trans-unit id="ee93c7e2ac06e08c1567b0ca209ad480ea5f1b80" translate="yes" xml:space="preserve">
          <source>To avoid the computation of global clustering, for every call of &lt;code&gt;partial_fit&lt;/code&gt; the user is advised</source>
          <target state="translated">为了避免计算全局聚类，建议每次调用 &lt;code&gt;partial_fit&lt;/code&gt; 时都建议用户</target>
        </trans-unit>
        <trans-unit id="e81cbf7353acbc69eeae43ca8cf143e58e658d10" translate="yes" xml:space="preserve">
          <source>To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called &lt;code&gt;tf&lt;/code&gt; for Term Frequencies.</source>
          <target state="translated">为了避免这些潜在的差异，只需将文档中每个单词的出现次数除以文档中单词的总数即可：这些新功能称为术语频率 &lt;code&gt;tf&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="39f01dfdcdf5847fd1935ba52ba9be2bfc80430b" translate="yes" xml:space="preserve">
          <source>To avoid this problem, nested CV effectively uses a series of train/validation/test set splits. In the inner loop (here executed by &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt;), the score is approximately maximized by fitting a model to each training set, and then directly maximized in selecting (hyper)parameters over the validation set. In the outer loop (here in &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt;), generalization error is estimated by averaging test set scores over several dataset splits.</source>
          <target state="translated">为避免此问题，嵌套CV有效地使用了一系列训练/验证/测试集拆分。在内部循环（在此由&lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt;执行）中，通过将模型拟合到每个训练集来近似最大化分数，然后在验证集上选择（超级）参数时直接将其最大化。在外部循环中（此处为&lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;），通过对几个数据集拆分中的测试集得分求平均值来估计泛化误差。</target>
        </trans-unit>
        <trans-unit id="a37b39aad5fcf98e98548e781cdec5193cfe7b97" translate="yes" xml:space="preserve">
          <source>To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array.</source>
          <target state="translated">为了避免不必要的内存重复,fit方法的X参数应该直接以Fortran-contiguous numpy数组的形式传递。</target>
        </trans-unit>
        <trans-unit id="d9c2f7485084c926a2f68d8587d615406cc01649" translate="yes" xml:space="preserve">
          <source>To be in favorable recovery conditions, we sample the data from a model with a sparse inverse covariance matrix. In addition, we ensure that the data is not too much correlated (limiting the largest coefficient of the precision matrix) and that there a no small coefficients in the precision matrix that cannot be recovered. In addition, with a small number of observations, it is easier to recover a correlation matrix rather than a covariance, thus we scale the time series.</source>
          <target state="translated">为了在有利的恢复条件下,我们从一个具有稀疏反协方差矩阵的模型中抽取数据。此外,我们还要保证数据的相关性不大(限制精密矩阵的最大系数),并且精密矩阵中没有不能恢复的小系数。此外,在观测值较少的情况下,恢复相关矩阵比恢复协方差更容易,因此我们对时间序列进行了缩放。</target>
        </trans-unit>
        <trans-unit id="f7fd313aae703eaa110952d34fbc2e74f81a873c" translate="yes" xml:space="preserve">
          <source>To be removed in 0.21</source>
          <target state="translated">将在0.21中删除</target>
        </trans-unit>
        <trans-unit id="b656a9f4366f6cbcc5b1e6914e7bc1a8d099ee57" translate="yes" xml:space="preserve">
          <source>To be removed in 0.22</source>
          <target state="translated">将在0.22中删除</target>
        </trans-unit>
        <trans-unit id="bc387423485c5a73576ae6f9089ec34a8b143ae6" translate="yes" xml:space="preserve">
          <source>To begin with, all values for \(r\) and \(a\) are set to zero, and the calculation of each iterates until convergence. As discussed above, in order to avoid numerical oscillations when updating the messages, the damping factor \(\lambda\) is introduced to iteration process:</source>
          <target state="translated">首先,将所有的r/(r/)和a/(a/)的值设置为零,然后对每个值进行迭代计算,直到收敛。如上所述,为了避免更新信息时的数值振荡,在迭代过程中引入了阻尼因子\(\lambda\)。</target>
        </trans-unit>
        <trans-unit id="ebc5cb56aa5d3da850d595b902c1384fa4142906" translate="yes" xml:space="preserve">
          <source>To begin, we&amp;rsquo;ll visualize our data.</source>
          <target state="translated">首先，我们将可视化数据。</target>
        </trans-unit>
        <trans-unit id="57e47e513e200b11a216f9768279c1f81e7b3157" translate="yes" xml:space="preserve">
          <source>To benchmark different estimators for your case you can simply change the &lt;code&gt;n_features&lt;/code&gt; parameter in this example: &lt;a href=&quot;../auto_examples/applications/plot_prediction_latency#sphx-glr-auto-examples-applications-plot-prediction-latency-py&quot;&gt;Prediction Latency&lt;/a&gt;. This should give you an estimate of the order of magnitude of the prediction latency.</source>
          <target state="translated">要针对您的案例对不同的估计量进行基准测试，您可以在此示例中简单地更改 &lt;code&gt;n_features&lt;/code&gt; 参数：&lt;a href=&quot;../auto_examples/applications/plot_prediction_latency#sphx-glr-auto-examples-applications-plot-prediction-latency-py&quot;&gt;Prediction Latency&lt;/a&gt;。这应该给您估计预测延迟的数量级。</target>
        </trans-unit>
        <trans-unit id="892d831a9e807296347eacb2e8474830ca349663" translate="yes" xml:space="preserve">
          <source>To compare a set of found biclusters to the set of true biclusters, two similarity measures are needed: a similarity measure for individual biclusters, and a way to combine these individual similarities into an overall score.</source>
          <target state="translated">为了将一组发现的双簇与一组真正的双簇进行比较,需要两个相似性测量:一个是单个双簇的相似性测量,另一个是将这些单个相似性合并成一个总分的方法。</target>
        </trans-unit>
        <trans-unit id="f0ff37a06cd777b22ebe208ab3110388f720b201" translate="yes" xml:space="preserve">
          <source>To compare individual biclusters, several measures have been used. For now, only the Jaccard index is implemented:</source>
          <target state="translated">为了比较各个双联体,已经使用了几种测量方法。目前,只采用了Jaccard指数。</target>
        </trans-unit>
        <trans-unit id="30a2aa60dabe3d1d8b8497c6228442c6c55454f4" translate="yes" xml:space="preserve">
          <source>To control display of warnings.</source>
          <target state="translated">控制警告的显示。</target>
        </trans-unit>
        <trans-unit id="6c3d05eecff544d238db6888c87daeb42794f44b" translate="yes" xml:space="preserve">
          <source>To control the verbosity of the procedure.</source>
          <target state="translated">要控制程序的啰嗦。</target>
        </trans-unit>
        <trans-unit id="d66f891ca7bde7537002ad52d27fc9dd62dd5881" translate="yes" xml:space="preserve">
          <source>To convert categorical features to such integer codes, we can use the &lt;a href=&quot;generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt;&lt;code&gt;OrdinalEncoder&lt;/code&gt;&lt;/a&gt;. This estimator transforms each categorical feature to one new feature of integers (0 to n_categories - 1):</source>
          <target state="translated">要将分类特征转换为此类整数代码，我们可以使用&lt;a href=&quot;generated/sklearn.preprocessing.ordinalencoder#sklearn.preprocessing.OrdinalEncoder&quot;&gt; &lt;code&gt;OrdinalEncoder&lt;/code&gt; &lt;/a&gt;。此估算器将每个分类特征转换为一个新的整数特征（0到n_categories-1）：</target>
        </trans-unit>
        <trans-unit id="693e8d8ca1982fe1e279b5869b2b710976d06558" translate="yes" xml:space="preserve">
          <source>To counter this effect we can discount the expected RI \(E[\text{RI}]\) of random labelings by defining the adjusted Rand index as follows:</source>
          <target state="translated">为了应对这种效应,我们可以通过定义调整后的兰德指数,对随机标签的预期RI/(E[\text{RI}]\)进行贴现,如下所示。</target>
        </trans-unit>
        <trans-unit id="2ccfac714af4138a2df70ede11b2ff4e1963a414" translate="yes" xml:space="preserve">
          <source>To create positive examples click the left mouse button; to create negative examples click the right button.</source>
          <target state="translated">要创建正面的例子,请点击鼠标左键;要创建负面的例子,请点击右键。</target>
        </trans-unit>
        <trans-unit id="d0bfc36f728f01d8f998e7e774b5cc731a5652d7" translate="yes" xml:space="preserve">
          <source>To disable convergence detection based on inertia, set max_no_improvement to None.</source>
          <target state="translated">要禁用基于惯性的收敛检测,请将max_no_improvement设置为None。</target>
        </trans-unit>
        <trans-unit id="644ea86209186f0b63818c18611416bf68aa348b" translate="yes" xml:space="preserve">
          <source>To disable convergence detection based on normalized center change, set tol to 0.0 (default).</source>
          <target state="translated">要禁用基于归一化中心变化的收敛检测,请将tol设置为0.0(默认)。</target>
        </trans-unit>
        <trans-unit id="8f49411326bd4f684fb56ae33f90d4fd9150ab8c" translate="yes" xml:space="preserve">
          <source>To do the exercises, copy the content of the &amp;lsquo;skeletons&amp;rsquo; folder as a new folder named &amp;lsquo;workspace&amp;rsquo;:</source>
          <target state="translated">要进行练习，请将&amp;ldquo; skeletons&amp;rdquo;文件夹的内容复制为名为&amp;ldquo; workspace&amp;rdquo;的新文件夹：</target>
        </trans-unit>
        <trans-unit id="79cf44a84fa8878b10f291a31335b47430451015" translate="yes" xml:space="preserve">
          <source>To each column, a different transformation can be applied, such as preprocessing or a specific feature extraction method:</source>
          <target state="translated">对每一列,可以应用不同的变换,如预处理或特定的特征提取方法。</target>
        </trans-unit>
        <trans-unit id="a0a5ce85df1e1aafd2ebf61b7efd7098beb62d7b" translate="yes" xml:space="preserve">
          <source>To estimate a probabilistic model (e.g. a Gaussian model), estimating the precision matrix, that is the inverse covariance matrix, is as important as estimating the covariance matrix. Indeed a Gaussian model is parametrized by the precision matrix.</source>
          <target state="translated">要估计一个概率模型(如高斯模型),估计精密矩阵,即反协方差矩阵,与估计协方差矩阵同样重要。事实上,高斯模型是以精密矩阵为参数的。</target>
        </trans-unit>
        <trans-unit id="06985e50b51113b200d13cecad3eedd2a07fa798" translate="yes" xml:space="preserve">
          <source>To evaluate the impact of the scale of the dataset (&lt;code&gt;n_samples&lt;/code&gt; and &lt;code&gt;n_features&lt;/code&gt;) while controlling the statistical properties of the data (typically the correlation and informativeness of the features), it is also possible to generate synthetic data.</source>
          <target state="translated">为了在控制数据的统计属性（通常是特征的相关性和信息性）的同时评估数据集规模（ &lt;code&gt;n_samples&lt;/code&gt; 和 &lt;code&gt;n_features&lt;/code&gt; ）的影响，也可以生成综合数据。</target>
        </trans-unit>
        <trans-unit id="8b81da86f6a51388bf88e8748866dfbc1da10ddb" translate="yes" xml:space="preserve">
          <source>To fully specify a dataset, you need to provide a name and a version, though the version is optional, see &lt;a href=&quot;#openml-versions&quot;&gt;Dataset Versions&lt;/a&gt; below. The dataset contains a total of 1080 examples belonging to 8 different classes:</source>
          <target state="translated">要完全指定数据集，您需要提供名称和版本，尽管版本是可选的，请参见下面的&amp;ldquo; &lt;a href=&quot;#openml-versions&quot;&gt;数据集版本&amp;rdquo;&lt;/a&gt;。数据集总共包含1080个示例，这些示例属于8个不同的类别：</target>
        </trans-unit>
        <trans-unit id="eddbe44ecc236b178d14592239180b4231c2f462" translate="yes" xml:space="preserve">
          <source>To get a better measure of prediction accuracy (which we can use as a proxy for goodness of fit of the model), we can successively split the data in &lt;em&gt;folds&lt;/em&gt; that we use for training and testing:</source>
          <target state="translated">为了更好地衡量预测准确性（我们可以将其用作模型拟合优度的代理），我们可以将数据依次&lt;em&gt;折叠&lt;/em&gt;以用于训练和测试：</target>
        </trans-unit>
        <trans-unit id="8f2c7c86e5d1b0f8592203b4a46517414e54ce05" translate="yes" xml:space="preserve">
          <source>To get identical results for each split, set &lt;code&gt;random_state&lt;/code&gt; to an integer.</source>
          <target state="translated">要获得每个拆分相同的结果，请将 &lt;code&gt;random_state&lt;/code&gt; 设置为整数。</target>
        </trans-unit>
        <trans-unit id="0228140936e0aced4eaa7c77d90637025c4d0909" translate="yes" xml:space="preserve">
          <source>To get started with this tutorial, you must first install &lt;em&gt;scikit-learn&lt;/em&gt; and all of its required dependencies.</source>
          <target state="translated">要开始本教程，您必须首先安装&lt;em&gt;scikit-learn&lt;/em&gt;及其所有必需的依赖项。</target>
        </trans-unit>
        <trans-unit id="8d91bad777aec839541c338ab9f11be081ee54c6" translate="yes" xml:space="preserve">
          <source>To get the signed distance to the hyperplane use &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier.decision_function&quot;&gt;&lt;code&gt;SGDClassifier.decision_function&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">要获得到超平面的签名距离，请使用&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier.decision_function&quot;&gt; &lt;code&gt;SGDClassifier.decision_function&lt;/code&gt; &lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="6ae2612052e54b7be6598947088a287fffd01403" translate="yes" xml:space="preserve">
          <source>To illustrate &lt;a href=&quot;generated/sklearn.dummy.dummyclassifier#sklearn.dummy.DummyClassifier&quot;&gt;&lt;code&gt;DummyClassifier&lt;/code&gt;&lt;/a&gt;, first let&amp;rsquo;s create an imbalanced dataset:</source>
          <target state="translated">为了说明&lt;a href=&quot;generated/sklearn.dummy.dummyclassifier#sklearn.dummy.DummyClassifier&quot;&gt; &lt;code&gt;DummyClassifier&lt;/code&gt; &lt;/a&gt;，首先让我们创建一个不平衡的数据集：</target>
        </trans-unit>
        <trans-unit id="e7a02a8f3e922c68cd6ce64dca33ce54683ffb1b" translate="yes" xml:space="preserve">
          <source>To illustrate this with a simple example, let&amp;rsquo;s assume we have 3 classifiers and a 3-class classification problems where we assign equal weights to all classifiers: w1=1, w2=1, w3=1.</source>
          <target state="translated">为了用一个简单的例子说明这一点，我们假设我们有3个分类器和3类分类问题，其中我们为所有分类器分配相等的权重：w1 = 1，w2 = 1，w3 = 1。</target>
        </trans-unit>
        <trans-unit id="27fe4060cc8aa9166cda2609863b9fdd12999baf" translate="yes" xml:space="preserve">
          <source>To illustrate this, PCA is performed comparing the use of data with &lt;a href=&quot;../../modules/generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt; applied, to unscaled data. The results are visualized and a clear difference noted. The 1st principal component in the unscaled set can be seen. It can be seen that feature #13 dominates the direction, being a whole two orders of magnitude above the other features. This is contrasted when observing the principal component for the scaled version of the data. In the scaled version, the orders of magnitude are roughly the same across all the features.</source>
          <target state="translated">为了说明这一点，执行PCA，将应用了&lt;a href=&quot;../../modules/generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; &lt;/a&gt;的数据的使用与未缩放的数据进行比较。结果可视化，并指出明显的差异。可以看到未缩放集合中的第一个主成分。可以看出，特征＃13主导了方向，比其他特征高两个数量级。当观察数据的缩放版本的主成分时，这是相反的。在缩放版本中，所有功能的数量级大致相同。</target>
        </trans-unit>
        <trans-unit id="952f60109f87198cc4767d483a08ad921abb5966" translate="yes" xml:space="preserve">
          <source>To improve the conditioning of the problem (i.e. mitigating the &lt;a href=&quot;#curse-of-dimensionality&quot;&gt;The curse of dimensionality&lt;/a&gt;), it would be interesting to select only the informative features and set non-informative ones, like feature 2 to 0. Ridge regression will decrease their contribution, but not set them to zero. Another penalization approach, called &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; (least absolute shrinkage and selection operator), can set some coefficients to zero. Such methods are called &lt;strong&gt;sparse method&lt;/strong&gt; and sparsity can be seen as an application of Occam&amp;rsquo;s razor: &lt;em&gt;prefer simpler models&lt;/em&gt;.</source>
          <target state="translated">为了改善问题的状况（即减轻&lt;a href=&quot;#curse-of-dimensionality&quot;&gt;维数的诅咒&lt;/a&gt;），仅选择信息性特征并将非信息性特征（例如特征2设置为0）会很有趣。他们为零。另一种惩罚方法称为&lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt;（最小绝对收缩和选择算子），可以将一些系数设置为零。这种方法称为&lt;strong&gt;稀疏方法&lt;/strong&gt;，稀疏性可以看作是Occam剃刀的一种应用：&lt;em&gt;更喜欢简单的模型&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="f028c20036ea78694db90136b8cb3004f099e0bf" translate="yes" xml:space="preserve">
          <source>To limit the memory consumption, we queue examples up to a fixed amount before feeding them to the learner.</source>
          <target state="translated">为了限制内存消耗,我们将例子排到一个固定的数量,然后再将它们喂给学习者。</target>
        </trans-unit>
        <trans-unit id="61f860c325e06c4f97b9f4c7ced3d5279054856d" translate="yes" xml:space="preserve">
          <source>To load from an external dataset, please refer to &lt;a href=&quot;../../datasets/index#external-datasets&quot;&gt;loading external datasets&lt;/a&gt;.</source>
          <target state="translated">要从外部数据集&lt;a href=&quot;../../datasets/index#external-datasets&quot;&gt;加载&lt;/a&gt;，请参阅加载外部数据集。</target>
        </trans-unit>
        <trans-unit id="d787fd22509da728f07846c2b5d3ecac1d6b4105" translate="yes" xml:space="preserve">
          <source>To load the data and visualize the images:</source>
          <target state="translated">加载数据并将图像可视化。</target>
        </trans-unit>
        <trans-unit id="b4be4dde535adc435619c6f0e295e0ce05bad72b" translate="yes" xml:space="preserve">
          <source>To make the example run faster, we use very few hidden units, and train only for a very short time. Training longer would result in weights with a much smoother spatial appearance.</source>
          <target state="translated">为了让这个例子运行得更快,我们使用很少的隐藏单元,并且只在很短的时间内进行训练。训练时间越长,权重的空间外观就越平滑。</target>
        </trans-unit>
        <trans-unit id="96ba992a3c5af68caa74d107191c5a3c32806c93" translate="yes" xml:space="preserve">
          <source>To make the preprocessor, tokenizer and analyzers aware of the model parameters it is possible to derive from the class and override the &lt;code&gt;build_preprocessor&lt;/code&gt;, &lt;code&gt;build_tokenizer&lt;/code&gt; and &lt;code&gt;build_analyzer&lt;/code&gt; factory methods instead of passing custom functions.</source>
          <target state="translated">为了使预处理器，令牌生成器和分析器了解模型参数，可以从类派生并覆盖 &lt;code&gt;build_preprocessor&lt;/code&gt; ， &lt;code&gt;build_tokenizer&lt;/code&gt; 和 &lt;code&gt;build_analyzer&lt;/code&gt; 工厂方法，而不用传递自定义函数。</target>
        </trans-unit>
        <trans-unit id="c0f08b8475e4b67e5147698ce9ccb818f0394d27" translate="yes" xml:space="preserve">
          <source>To make this more explicit, consider the following notation:</source>
          <target state="translated">为了更明确地说明这一点,请考虑以下的记法。</target>
        </trans-unit>
        <trans-unit id="8ada09feb86f8f3751dffbeeaba0e1e4f69156a7" translate="yes" xml:space="preserve">
          <source>To obtain a fully probabilistic model, the output \(y\) is assumed to be Gaussian distributed around \(X w\):</source>
          <target state="translated">为了获得一个完全的概率模型,假设输出/(y)是围绕/(X w/)的高斯分布。</target>
        </trans-unit>
        <trans-unit id="65178eef58b048e690e1c210520e38da80789880" translate="yes" xml:space="preserve">
          <source>To perform classification with generalized linear models, see &lt;a href=&quot;#logistic-regression&quot;&gt;Logistic regression&lt;/a&gt;.</source>
          <target state="translated">要使用广义线性模型进行分类，请参阅&lt;a href=&quot;#logistic-regression&quot;&gt;Logistic回归&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="72dc32b7225454e8d7c0ec26f14d95b55df2c79a" translate="yes" xml:space="preserve">
          <source>To quantify estimation error, we plot the likelihood of unseen data for different values of the shrinkage parameter. We also show the choices by cross-validation, or with the LedoitWolf and OAS estimates.</source>
          <target state="translated">为了量化估计误差,我们绘制了不同收缩参数值的未见数据的可能性。我们还显示了通过交叉验证,或与LedoitWolf和OAS估计的选择。</target>
        </trans-unit>
        <trans-unit id="397393d3de29d9ed57b4f231bd553afc264bafab" translate="yes" xml:space="preserve">
          <source>To return the corresponding classical subsets of kddcup 99. If None, return the entire kddcup 99 dataset.</source>
          <target state="translated">要返回kddcup 99的相应经典子集。如果None,返回整个kddcup 99数据集。</target>
        </trans-unit>
        <trans-unit id="b0e502baa68f0434bb574994337b41db58fa07c4" translate="yes" xml:space="preserve">
          <source>To run cross-validation on multiple metrics and also to return train scores, fit times and score times.</source>
          <target state="translated">要在多个指标上运行交叉验证,也要返回训练得分、拟合时间和得分时间。</target>
        </trans-unit>
        <trans-unit id="2d79b95a40b4d1ea2f276f13509aebc984e2d932" translate="yes" xml:space="preserve">
          <source>To see how this generalizes the binary log loss given above, note that in the binary case, \(p_{i,0} = 1 - p_{i,1}\) and \(y_{i,0} = 1 - y_{i,1}\), so expanding the inner sum over \(y_{i,k} \in \{0,1\}\) gives the binary log loss.</source>
          <target state="translated">为了了解这如何概括上面给出的二元对数损失,请注意,在二元情况下,\(p_{i,0}=1-p_{i,1}\)和\(y_{i,0}=1-y_{i,1}\),因此在\(y_{i,k}\in \{0,1/}\)上扩展内和,得到二元对数损失。</target>
        </trans-unit>
        <trans-unit id="25684d8b1766d360b665e8498a44a626b2b5bd13" translate="yes" xml:space="preserve">
          <source>To set &lt;code&gt;n_clusters=None&lt;/code&gt; initially</source>
          <target state="translated">最初设置 &lt;code&gt;n_clusters=None&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="78f293aec6a6c458449c6dc3bcd71b696525a449" translate="yes" xml:space="preserve">
          <source>To speed up the algorithm, accept only those bins with at least min_bin_freq points as seeds.</source>
          <target state="translated">为了加快算法的速度,只接受那些至少有min_bin_freq点的bin作为种子。</target>
        </trans-unit>
        <trans-unit id="e1dfcbed698085a7ab462cf760bf24e65a9e8400" translate="yes" xml:space="preserve">
          <source>To speed up the algorithm, accept only those bins with at least min_bin_freq points as seeds. If not defined, set to 1.</source>
          <target state="translated">为了加快算法的速度,只接受那些至少有min_bin_freq点的bin作为种子。如果没有定义,设置为1。</target>
        </trans-unit>
        <trans-unit id="fd2f04d7c6e080a2cce0d2e7339e598ba17acac5" translate="yes" xml:space="preserve">
          <source>To try to predict the outcome on a new document we need to extract the features using almost the same feature extracting chain as before. The difference is that we call &lt;code&gt;transform&lt;/code&gt; instead of &lt;code&gt;fit_transform&lt;/code&gt; on the transformers, since they have already been fit to the training set:</source>
          <target state="translated">为了尝试预测新文档的结果，我们需要使用与以前几乎相同的特征提取链来提取特征。区别在于我们在转换 &lt;code&gt;fit_transform&lt;/code&gt; 上调用 &lt;code&gt;transform&lt;/code&gt; 而不是fit_transform，因为它们已经适合训练集：</target>
        </trans-unit>
        <trans-unit id="5da67914dc5314b6125944bb748e1a3d6c08f736" translate="yes" xml:space="preserve">
          <source>To understand the use of LDA in dimensionality reduction, it is useful to start with a geometric reformulation of the LDA classification rule explained above. We write \(K\) for the total number of target classes. Since in LDA we assume that all classes have the same estimated covariance \(\Sigma\), we can rescale the data so that this covariance is the identity:</source>
          <target state="translated">为了理解LDA在降维中的应用,我们可以从上面解释的LDA分类规则的几何重述开始。我们将目标类的总数量写为 \(K\)。由于在LDA中,我们假设所有的类都有相同的估计协方差 \(\Sigma\),我们可以重新调整数据的尺度,使这个协方差是身份。</target>
        </trans-unit>
        <trans-unit id="6df5e0eab0e1e02def9ae99e68c6ddf1a841d6d1" translate="yes" xml:space="preserve">
          <source>To use &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you need to instantiate the estimator with the &lt;code&gt;novelty&lt;/code&gt; parameter set to &lt;code&gt;True&lt;/code&gt; before fitting the estimator:</source>
          <target state="translated">要使用&lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt;进行新颖性检测，即预测标签或计算新的看不见数据的异常分数，您需要在将估算器拟合之前将 &lt;code&gt;novelty&lt;/code&gt; 参数设置为 &lt;code&gt;True&lt;/code&gt; 来实例化估算器：</target>
        </trans-unit>
        <trans-unit id="011ed6ad19bc2f123579a7e50ff8b4dad33bf360" translate="yes" xml:space="preserve">
          <source>To use joblib.Memory to cache the svmlight file:</source>
          <target state="translated">要使用joblib.Memory来缓存svmlight文件。</target>
        </trans-unit>
        <trans-unit id="e11936ea84de206f18d8b708b6f4eca9fb6c8b59" translate="yes" xml:space="preserve">
          <source>To use text files in a scikit-learn classification or clustering algorithm, you will need to use the &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; module to build a feature extraction transformer that suits your problem.</source>
          <target state="translated">要在scikit-learn分类或聚类算法中使用文本文件，您将需要使用 &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; 模块来构建适合您问题的特征提取转换器。</target>
        </trans-unit>
        <trans-unit id="c7aa2d2ef894f356c354736ecb4235681bee95b2" translate="yes" xml:space="preserve">
          <source>To use this dataset with scikit-learn, we transform each 8x8 image into a feature vector of length 64</source>
          <target state="translated">为了用scikit-learn来使用这个数据集,我们将每张8x8的图像转换为一个长度为64的特征向量</target>
        </trans-unit>
        <trans-unit id="f5d271c927cff9ea25de07089e51938b7e86a2a7" translate="yes" xml:space="preserve">
          <source>To use this model as a classifier, we just need to estimate from the training data the class priors \(P(y=k)\) (by the proportion of instances of class \(k\)), the class means \(\mu_k\) (by the empirical sample class means) and the covariance matrices (either by the empirical sample class covariance matrices, or by a regularized estimator: see the section on shrinkage below).</source>
          <target state="translated">为了使用这个模型作为分类器,我们只需要从训练数据中估计出类前值/(P(y=k)/)(通过类实例的比例/(k/)),类均值/(mu_k/)(通过经验样本类均值)和协方差矩阵(通过经验样本类协方差矩阵,或者通过正则化估计器:见下面关于收缩的部分)。</target>
        </trans-unit>
        <trans-unit id="5d015bfb570917361c4b4abaaa59f5e623d8c463" translate="yes" xml:space="preserve">
          <source>To validate a model we need a scoring function (see &lt;a href=&quot;model_evaluation#model-evaluation&quot;&gt;Model evaluation: quantifying the quality of predictions&lt;/a&gt;), for example accuracy for classifiers. The proper way of choosing multiple hyperparameters of an estimator are of course grid search or similar methods (see &lt;a href=&quot;grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt;) that select the hyperparameter with the maximum score on a validation set or multiple validation sets. Note that if we optimized the hyperparameters based on a validation score the validation score is biased and not a good estimate of the generalization any longer. To get a proper estimate of the generalization we have to compute the score on another test set.</source>
          <target state="translated">为了验证模型，我们需要一个评分函数（请参阅&lt;a href=&quot;model_evaluation#model-evaluation&quot;&gt;模型评估：量化预测的质量&lt;/a&gt;），例如分类器的准确性。选择估算器的多个超参数的正确方法当然是网格搜索或类似的方法（请参见&lt;a href=&quot;grid_search#grid-search&quot;&gt;调整估算器的超参数&lt;/a&gt;），该方法选择在一个或多个验证集上具有最高分数的超参数。请注意，如果我们基于验证分数优化超参数，则验证分数会产生偏差，不再是对泛化的良好估计。为了获得适当的泛化估计，我们必须在另一个测试集上计算分数。</target>
        </trans-unit>
        <trans-unit id="baa10199f999bc30e58b0035ac2f6e51132399ed" translate="yes" xml:space="preserve">
          <source>To visualize the probability weighting, we fit each classifier on the training set and plot the predicted class probabilities for the first sample in this example dataset.</source>
          <target state="translated">为了可视化概率权重,我们在训练集上拟合每个分类器,并绘制该示例数据集中第一个样本的预测类概率。</target>
        </trans-unit>
        <trans-unit id="ba234a16bb1a2ae4619585ca04988c1afd574060" translate="yes" xml:space="preserve">
          <source>Tokenize the documents and count the occurrences of token and return them as a sparse matrix</source>
          <target state="translated">对文档进行token化,计算token的出现次数,并以稀疏矩阵的形式返回。</target>
        </trans-unit>
        <trans-unit id="e89caeb25fc24a274e225b242d49cc6fb7ddfa72" translate="yes" xml:space="preserve">
          <source>Tokenizing text with &lt;code&gt;scikit-learn&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;scikit-learn&lt;/code&gt; 对文本进行标记</target>
        </trans-unit>
        <trans-unit id="45d4a0ebe499a5d042ac0f7bc4284501d3667758" translate="yes" xml:space="preserve">
          <source>Tolerance for &amp;lsquo;arpack&amp;rsquo; method Not used if eigen_solver==&amp;rsquo;dense&amp;rsquo;.</source>
          <target state="translated">'arpack'方法的公差如果eigen_solver =='dense'不使用。</target>
        </trans-unit>
        <trans-unit id="318dfc593e0123f93a8fe309f411532f48eea756" translate="yes" xml:space="preserve">
          <source>Tolerance for ARPACK. 0 means machine precision. Ignored by randomized SVD solver.</source>
          <target state="translated">ARPACK的公差。0表示机器精度。随机化SVD求解器忽略。</target>
        </trans-unit>
        <trans-unit id="13511570864a98fa2d61f43da929b11b4894937f" translate="yes" xml:space="preserve">
          <source>Tolerance for Hessian eigenmapping method. Only used if &lt;code&gt;method == 'hessian'&lt;/code&gt;</source>
          <target state="translated">黑森州特征映射方法的公差。仅在 &lt;code&gt;method == 'hessian'&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="e4c877ba267607a99e62c8b31f7891feda117cf7" translate="yes" xml:space="preserve">
          <source>Tolerance for Hessian eigenmapping method. Only used if method == &amp;lsquo;hessian&amp;rsquo;</source>
          <target state="translated">黑森州特征映射方法的公差。仅在方法=='hessian'时使用</target>
        </trans-unit>
        <trans-unit id="aeb25ea9c0101939a4336136b4e11db71f1bb1be" translate="yes" xml:space="preserve">
          <source>Tolerance for modified LLE method. Only used if &lt;code&gt;method == 'modified'&lt;/code&gt;</source>
          <target state="translated">修正的LLE方法的公差。仅在 &lt;code&gt;method == 'modified'&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="b6502cfb6f414093cd5faf0376953824eae5e86f" translate="yes" xml:space="preserve">
          <source>Tolerance for modified LLE method. Only used if method == &amp;lsquo;modified&amp;rsquo;</source>
          <target state="translated">修正的LLE方法的公差。仅在方法=='modified'时使用</target>
        </trans-unit>
        <trans-unit id="40eaf2d9a188116c07595886d4a67c9121557ecf" translate="yes" xml:space="preserve">
          <source>Tolerance for singular values computed by svd_solver == &amp;lsquo;arpack&amp;rsquo;.</source>
          <target state="translated">svd_solver =='arpack'计算的奇异值的公差。</target>
        </trans-unit>
        <trans-unit id="a495f50d68c5f0d21905244c442ac1ec46831c6d" translate="yes" xml:space="preserve">
          <source>Tolerance for stopping criteria.</source>
          <target state="translated">停止标准的容忍度。</target>
        </trans-unit>
        <trans-unit id="1f900b2be351c5e1d6397b25c9a2e6c5e5c36343" translate="yes" xml:space="preserve">
          <source>Tolerance for stopping criterion.</source>
          <target state="translated">停止标准的公差。</target>
        </trans-unit>
        <trans-unit id="4d73abe23fd3517118aa70ae58840719c14ae6a0" translate="yes" xml:space="preserve">
          <source>Tolerance for the early stopping. When the loss is not improving by at least tol for &lt;code&gt;n_iter_no_change&lt;/code&gt; iterations (if set to a number), the training stops.</source>
          <target state="translated">允许提前停止。当 &lt;code&gt;n_iter_no_change&lt;/code&gt; 迭代的损失至少没有改善时（如果设置为数字），训练将停止。</target>
        </trans-unit>
        <trans-unit id="334a1d6597d473e85cc8725e20828e0c9824ea02" translate="yes" xml:space="preserve">
          <source>Tolerance for the optimization. When the loss or score is not improving by at least &lt;code&gt;tol&lt;/code&gt; for &lt;code&gt;n_iter_no_change&lt;/code&gt; consecutive iterations, unless &lt;code&gt;learning_rate&lt;/code&gt; is set to &amp;lsquo;adaptive&amp;rsquo;, convergence is considered to be reached and training stops.</source>
          <target state="translated">优化公差。当损失或成绩不被至少提高 &lt;code&gt;tol&lt;/code&gt; 为 &lt;code&gt;n_iter_no_change&lt;/code&gt; 连续迭代，除非 &lt;code&gt;learning_rate&lt;/code&gt; 设置为&amp;ldquo;自适应&amp;rdquo;，融合被认为是达到和培训停止。</target>
        </trans-unit>
        <trans-unit id="6938a4dcb29969d15aaa6cafefb8f09b830ed305" translate="yes" xml:space="preserve">
          <source>Tolerance for the stopping condition.</source>
          <target state="translated">停止条件的公差。</target>
        </trans-unit>
        <trans-unit id="3a49445cc3e76e8c0deab47f4b10c5bd7dc33960" translate="yes" xml:space="preserve">
          <source>Tolerance of the stopping condition.</source>
          <target state="translated">停止条件的容忍度。</target>
        </trans-unit>
        <trans-unit id="48a48ded1ae1ed29a7ddaed19c15db301472918d" translate="yes" xml:space="preserve">
          <source>Tolerance on update at each iteration.</source>
          <target state="translated">每次迭代更新时的公差。</target>
        </trans-unit>
        <trans-unit id="a2223ba588ac8a94dc6928512bbe1ae559b46f6b" translate="yes" xml:space="preserve">
          <source>Tolerance used in the iterative algorithm default 1e-06.</source>
          <target state="translated">迭代算法中使用的公差,默认为1e-06。</target>
        </trans-unit>
        <trans-unit id="20a2955c412dcae35aa2ef964ce8c2d4b1c07dcb" translate="yes" xml:space="preserve">
          <source>Tolerance when calculating spatial median.</source>
          <target state="translated">计算空间中位数时的公差。</target>
        </trans-unit>
        <trans-unit id="f3d0c54c4b7882f5280f0492c26f2bf33d35d2a2" translate="yes" xml:space="preserve">
          <source>Tony Blair</source>
          <target state="translated">托尼-布莱尔</target>
        </trans-unit>
        <trans-unit id="e1781cb6d03ccb2216639c1d54de7540b9fc2c2b" translate="yes" xml:space="preserve">
          <source>Tools for imputing missing values are discussed at &lt;a href=&quot;impute#impute&quot;&gt;Imputation of missing values&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;impute#impute&quot;&gt;插补缺失值的&lt;/a&gt;工具在插补缺失值中进行了讨论。</target>
        </trans-unit>
        <trans-unit id="0d184ce2992ee425b9d4cc3d528da94fb4da399d" translate="yes" xml:space="preserve">
          <source>Tophat kernel (&lt;code&gt;kernel = 'tophat'&lt;/code&gt;)</source>
          <target state="translated">Tophat内核（ &lt;code&gt;kernel = 'tophat'&lt;/code&gt; ）</target>
        </trans-unit>
        <trans-unit id="0954aa60533f43dc3b2b9a9cbdee11a74f79eada" translate="yes" xml:space="preserve">
          <source>Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</source>
          <target state="translated">用非负矩阵因子化和Latent Dirichlet Allocation进行主题提取。</target>
        </trans-unit>
        <trans-unit id="97129616afbfcb01d33b44619c8bf267194395ac" translate="yes" xml:space="preserve">
          <source>Total Phenols:</source>
          <target state="translated">酚类总量:</target>
        </trans-unit>
        <trans-unit id="24a9e81269d05c734577a89440230faee238f7b2" translate="yes" xml:space="preserve">
          <source>Total log-likelihood of the data in X.</source>
          <target state="translated">X中数据的总对数似然。</target>
        </trans-unit>
        <trans-unit id="4055747cee4593e58e4a6dcbfa7d6ccc61845cf0" translate="yes" xml:space="preserve">
          <source>Total number of documents. Only used in the &lt;code&gt;partial_fit&lt;/code&gt; method.</source>
          <target state="translated">文件总数。仅在 &lt;code&gt;partial_fit&lt;/code&gt; 方法中使用。</target>
        </trans-unit>
        <trans-unit id="aed40ed5719d059f29eba1177189a60b01059871" translate="yes" xml:space="preserve">
          <source>Total phenols</source>
          <target state="translated">总酚类</target>
        </trans-unit>
        <trans-unit id="babba0bc0e9a3e36ce98f362a62519c8eacb94cb" translate="yes" xml:space="preserve">
          <source>Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method Andrew V. Knyazev &lt;a href=&quot;https://doi.org/10.1137%2FS1064827500366124&quot;&gt;https://doi.org/10.1137%2FS1064827500366124&lt;/a&gt;</source>
          <target state="translated">走向最佳预处理特征求解器：局部最佳块预处理共轭梯度法Andrew V. Knyazev &lt;a href=&quot;https://doi.org/10.1137%2FS1064827500366124&quot;&gt;https://doi.org/10.1137%2FS1064827500366124&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="e3ae1e8d052cc2d0c25bbda7e5d0370ec624b1e8" translate="yes" xml:space="preserve">
          <source>Toy example of 1D regression using linear, polynomial and RBF kernels.</source>
          <target state="translated">使用线性、多项式和RBF核的一维回归的玩具例子。</target>
        </trans-unit>
        <trans-unit id="264fa08a131d6382d6715d8c951f2b5bea1c373c" translate="yes" xml:space="preserve">
          <source>Traceback example, note how the line of the error is indicated as well as the values of the parameter passed to the function that triggered the exception, even though the traceback happens in the child process:</source>
          <target state="translated">回溯的例子,请注意,尽管回溯发生在子进程中,但如何显示错误的行以及传递给触发异常的函数的参数值。</target>
        </trans-unit>
        <trans-unit id="8718fa41b5577d15733c0d074d4e6ea2d5f88486" translate="yes" xml:space="preserve">
          <source>Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.</source>
          <target state="translated">Tracking,International Journal of Computer Vision,Volume 77,Issue 1-3,pp.125-141,May 2008.</target>
        </trans-unit>
        <trans-unit id="6e4826fce9da6f5f03d1b11115df13e0bc514c4a" translate="yes" xml:space="preserve">
          <source>Train all data by multiple calls to partial_fit.</source>
          <target state="translated">通过多次调用 partial_fit 来训练所有数据。</target>
        </trans-unit>
        <trans-unit id="bd98708380c7e60a9c0a687f254abca8478a36f8" translate="yes" xml:space="preserve">
          <source>Train and test sizes may be different in each fold, with a difference of at most &lt;code&gt;n_classes&lt;/code&gt;.</source>
          <target state="translated">每个折叠的训练和测试大小可能不同，最多 &lt;code&gt;n_classes&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1c08c1bee3835bcafdb50e8cdda68c68d71fa67e" translate="yes" xml:space="preserve">
          <source>Train error vs Test error</source>
          <target state="translated">训练误差与测试误差</target>
        </trans-unit>
        <trans-unit id="357c94d50b669e3c60f0758a6140ed17dc81af61" translate="yes" xml:space="preserve">
          <source>Train l1-penalized logistic regression models on a binary classification problem derived from the Iris dataset.</source>
          <target state="translated">在源于Iris数据集的二元分类问题上训练l1-penalized logistic回归模型。</target>
        </trans-unit>
        <trans-unit id="0cfcb0c276264df865da734aa7faab6c6b43fed6" translate="yes" xml:space="preserve">
          <source>Train the model using libsvm (low-level method)</source>
          <target state="translated">使用libsvm(低级方法)训练模型。</target>
        </trans-unit>
        <trans-unit id="ff5331ad7dc89bf5a9dd23c31ab738af7815c499" translate="yes" xml:space="preserve">
          <source>Training a classifier</source>
          <target state="translated">训练一个分类器</target>
        </trans-unit>
        <trans-unit id="0f0630eb2ecfdd0ed6f7defc6642e6c0143bcbf3" translate="yes" xml:space="preserve">
          <source>Training data</source>
          <target state="translated">训练数据</target>
        </trans-unit>
        <trans-unit id="6c7c988c62ce8a65ab6394bf4f62bdef696bbe60" translate="yes" xml:space="preserve">
          <source>Training data, requires length = n_samples</source>
          <target state="translated">训练数据,需要长度=n_samples。</target>
        </trans-unit>
        <trans-unit id="70fa8e3174eef9a1ccfc0bda8b38c7cfbf09ffd6" translate="yes" xml:space="preserve">
          <source>Training data, where n_samples in the number of samples and n_features is the number of features.</source>
          <target state="translated">训练数据,其中n_samples为样本数,n_features为特征数。</target>
        </trans-unit>
        <trans-unit id="1d999bb02f6364cf15c69e5533af993a3fc0fdd8" translate="yes" xml:space="preserve">
          <source>Training data, where n_samples is the number of samples and n_features is the number of features.</source>
          <target state="translated">训练数据,其中n_samples为样本数,n_features为特征数。</target>
        </trans-unit>
        <trans-unit id="f12731d4ed32a02266da09997a2bf0e000555cf6" translate="yes" xml:space="preserve">
          <source>Training data, which is also required for prediction. If kernel == &amp;ldquo;precomputed&amp;rdquo; this is instead the precomputed training matrix, shape = [n_samples, n_samples].</source>
          <target state="translated">训练数据，这也是预测所必需的。如果kernel ==&amp;ldquo; precomputed&amp;rdquo;，则它是预先计算的训练矩阵，shape = [n_samples，n_samples]。</target>
        </trans-unit>
        <trans-unit id="c5441fed149296831061b9151bd71d563327dc0d" translate="yes" xml:space="preserve">
          <source>Training data.</source>
          <target state="translated">训练数据:</target>
        </trans-unit>
        <trans-unit id="4319dec91a5574f9382b1b679ba9c82bf44c0f15" translate="yes" xml:space="preserve">
          <source>Training data. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] if metric=&amp;rsquo;precomputed&amp;rsquo;.</source>
          <target state="translated">训练数据。如果是数组或矩阵，则将其形状设置为[n_samples，n_features]，或者，如果metric ='precomputed'，则设置为[n_samples，n_samples]。</target>
        </trans-unit>
        <trans-unit id="3cc715a75ede17772899f7cc9ab69882475a79bc" translate="yes" xml:space="preserve">
          <source>Training data. If kernel == &amp;ldquo;precomputed&amp;rdquo; this is instead a precomputed kernel matrix, shape = [n_samples, n_samples].</source>
          <target state="translated">训练数据。如果kernel ==&amp;ldquo; precomputed&amp;rdquo;，则它是一个预先计算的内核矩阵，shape = [n_samples，n_samples]。</target>
        </trans-unit>
        <trans-unit id="b12ede4c226e6e2f235813d30bce55744269c03f" translate="yes" xml:space="preserve">
          <source>Training data. Must fulfill input requirements of first step of the pipeline.</source>
          <target state="translated">训练数据。必须满足流水线第一步的输入要求。</target>
        </trans-unit>
        <trans-unit id="d5044fd4a2ac02d5a0b137f2f3b7fd6b8f65a006" translate="yes" xml:space="preserve">
          <source>Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If &lt;code&gt;y&lt;/code&gt; is mono-output then &lt;code&gt;X&lt;/code&gt; can be sparse.</source>
          <target state="translated">训练数据。直接作为Fortran连续数据传递，以避免不必要的内存重复。如果 &lt;code&gt;y&lt;/code&gt; 是单输出，则 &lt;code&gt;X&lt;/code&gt; 可以稀疏。</target>
        </trans-unit>
        <trans-unit id="8ed7855d8da328d2505a0bcd1c3302665b72cb3d" translate="yes" xml:space="preserve">
          <source>Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If y is mono-output, X can be sparse.</source>
          <target state="translated">训练数据。直接以Fortran连续数据的形式传递,避免不必要的内存重复。如果y是单输出,X可以是稀疏的。</target>
        </trans-unit>
        <trans-unit id="4c004afef287030c2dcb4a937f43adb06e1cdf0e" translate="yes" xml:space="preserve">
          <source>Training data. Shape [n_samples, n_features], or [n_samples, n_samples] if affinity==&amp;rsquo;precomputed&amp;rsquo;.</source>
          <target state="translated">训练数据。形状为[n_samples，n_features]或[n_samples，n_samples]（如果affinity ==&amp;ldquo;预计算&amp;rdquo;）。</target>
        </trans-unit>
        <trans-unit id="be8959fb1d08ac2482d5adecb9cc6d42cd3487ff" translate="yes" xml:space="preserve">
          <source>Training instances to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous.</source>
          <target state="translated">训练实例进行聚类。必须注意的是,数据将被转换为C排序,如果给定的数据不是C连续的,将导致内存拷贝。</target>
        </trans-unit>
        <trans-unit id="1fb973e1446d09c43085a14e14217bfa82f35fac" translate="yes" xml:space="preserve">
          <source>Training set and testing set</source>
          <target state="translated">训练集和测试集</target>
        </trans-unit>
        <trans-unit id="ea59a824d416e7ea0dc63df33b1afa59cdb64566" translate="yes" xml:space="preserve">
          <source>Training set.</source>
          <target state="translated">训练集。</target>
        </trans-unit>
        <trans-unit id="3c518c488676e60e90ca53bcc0aa7271b669fe4d" translate="yes" xml:space="preserve">
          <source>Training set: only the shape is used to find optimal random matrix dimensions based on the theory referenced in the afore mentioned papers.</source>
          <target state="translated">训练集:根据上述论文中引用的理论,只用形状来寻找最优的随机矩阵维度。</target>
        </trans-unit>
        <trans-unit id="68d52cda6c0756d21c2527d22eda07d7e45f55d9" translate="yes" xml:space="preserve">
          <source>Training target.</source>
          <target state="translated">培训目标:</target>
        </trans-unit>
        <trans-unit id="32e48bd3169f82f98b7879700514da5daba97549" translate="yes" xml:space="preserve">
          <source>Training targets. Must fulfill label requirements for all steps of the pipeline.</source>
          <target state="translated">培训目标。必须满足流水线各步骤的标签要求。</target>
        </trans-unit>
        <trans-unit id="bc89d708a926da60c1e855065f294a150e4844da" translate="yes" xml:space="preserve">
          <source>Training vector, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples and &lt;code&gt;n_features&lt;/code&gt; is the total number of features.</source>
          <target state="translated">训练向量，其中 &lt;code&gt;n_samples&lt;/code&gt; 是样本数， &lt;code&gt;n_features&lt;/code&gt; 是特征总数。</target>
        </trans-unit>
        <trans-unit id="325dc392b957558d0accbc4c288eabf85d0d476c" translate="yes" xml:space="preserve">
          <source>Training vector, where n_samples in the number of samples and n_features is the number of features.</source>
          <target state="translated">训练向量,其中n_samples为样本数,n_features为特征数。</target>
        </trans-unit>
        <trans-unit id="01000b19ae19a1d02ea4ceb374852ca509745c92" translate="yes" xml:space="preserve">
          <source>Training vector, where n_samples in the number of samples and n_features is the number of features. Note that centroid shrinking cannot be used with sparse matrices.</source>
          <target state="translated">训练向量,其中n_samples为样本数,n_features为特征数。注意,中心子收缩不能用于稀疏矩阵。</target>
        </trans-unit>
        <trans-unit id="6a8354ff2f178d04d8fd18ac8502cba6a9d2e53e" translate="yes" xml:space="preserve">
          <source>Training vector, where n_samples is the number of samples and n_features is the number of features.</source>
          <target state="translated">训练向量,其中n_samples为样本数,n_features为特征数。</target>
        </trans-unit>
        <trans-unit id="66e0bce9861c05444da85a9795d5edcc3de5cb5e" translate="yes" xml:space="preserve">
          <source>Training vectors, where n_samples is the number of samples and n_features is the number of features.</source>
          <target state="translated">训练向量,其中n_samples为样本数,n_features为特征数。</target>
        </trans-unit>
        <trans-unit id="f64b8abd734d5648613b346b4bb3c97a56c66bbf" translate="yes" xml:space="preserve">
          <source>Training vectors, where n_samples is the number of samples and n_features is the number of features. For kernel=&amp;rdquo;precomputed&amp;rdquo;, the expected shape of X is (n_samples, n_samples).</source>
          <target state="translated">训练向量，其中n_samples是样本数，n_​​features是特征数。对于内核=&amp;ldquo;预先计算&amp;rdquo;，X的预期形状为（n_samples，n_samples）。</target>
        </trans-unit>
        <trans-unit id="ee5e82a19ba6d9a5b4fc8f431028f4e0ae5cae2a" translate="yes" xml:space="preserve">
          <source>Training vectors, where n_samples is the number of samples and n_features is the number of predictors.</source>
          <target state="translated">训练向量,其中n_samples为样本数,n_features为预测因子数。</target>
        </trans-unit>
        <trans-unit id="3dbb8cbc3c8d093280069e8e889d1e0c62e1afde" translate="yes" xml:space="preserve">
          <source>Transform X back to its original space.</source>
          <target state="translated">将X变换回原来的空间。</target>
        </trans-unit>
        <trans-unit id="dbfeebba6e53c937056143e8cf1258378ae1c26d" translate="yes" xml:space="preserve">
          <source>Transform X back to original space.</source>
          <target state="translated">将X变换回原来的空间。</target>
        </trans-unit>
        <trans-unit id="2fcdcd20eec681f04cc400d1e7a3d3a35f46ced9" translate="yes" xml:space="preserve">
          <source>Transform X into subcluster centroids dimension.</source>
          <target state="translated">将X转化为子群中心点维度。</target>
        </trans-unit>
        <trans-unit id="da3379264043ea94358e5b4b01ce80967c41f1a5" translate="yes" xml:space="preserve">
          <source>Transform X separately by each transformer, concatenate results.</source>
          <target state="translated">由各变压器分别变换X,将结果连起来。</target>
        </trans-unit>
        <trans-unit id="054e9dc484301382a53ef7807c44414f413c3b43" translate="yes" xml:space="preserve">
          <source>Transform X to a cluster-distance space.</source>
          <target state="translated">将X转化为簇-距离空间。</target>
        </trans-unit>
        <trans-unit id="9340d4e978871cfc2faf3772609beb4370b76837" translate="yes" xml:space="preserve">
          <source>Transform X to ordinal codes.</source>
          <target state="translated">将X转化为序码。</target>
        </trans-unit>
        <trans-unit id="d750cda6e828d45a370fec4538601ee99b5443be" translate="yes" xml:space="preserve">
          <source>Transform X using one-hot encoding.</source>
          <target state="translated">使用一热编码变换X。</target>
        </trans-unit>
        <trans-unit id="f9e88a65d85f54852f98655b3f250fdbf7750c92" translate="yes" xml:space="preserve">
          <source>Transform X using the forward function.</source>
          <target state="translated">用正向函数变换X。</target>
        </trans-unit>
        <trans-unit id="fb06535ce9222390887b51d0862f28eec382f495" translate="yes" xml:space="preserve">
          <source>Transform X using the inverse function.</source>
          <target state="translated">利用反函数对X进行变换。</target>
        </trans-unit>
        <trans-unit id="55b2dc92fd17631d37a113cafae1257246c63b9f" translate="yes" xml:space="preserve">
          <source>Transform X.</source>
          <target state="translated">变换X。</target>
        </trans-unit>
        <trans-unit id="df5b966033d10ab5ffd4498c25f3563581fac3a4" translate="yes" xml:space="preserve">
          <source>Transform a count matrix to a normalized tf or tf-idf representation</source>
          <target state="translated">将计数矩阵转换为归一化的tf或tf-idf表示。</target>
        </trans-unit>
        <trans-unit id="c6579300b554475d257c93a2551d1e7ac8d00f29" translate="yes" xml:space="preserve">
          <source>Transform a count matrix to a tf or tf-idf representation</source>
          <target state="translated">将计数矩阵转换为tf或tf-idf表示法</target>
        </trans-unit>
        <trans-unit id="cfe77beec60d283a1ae2557849fffc568b20c2b6" translate="yes" xml:space="preserve">
          <source>Transform a new matrix using the built clustering</source>
          <target state="translated">使用建立的聚类转换一个新的矩阵。</target>
        </trans-unit>
        <trans-unit id="eb758f2f9f4d3b4a21a0f5aa711d86b7f433cb44" translate="yes" xml:space="preserve">
          <source>Transform a sequence of documents to a document-term matrix.</source>
          <target state="translated">将文档序列转换为文档项矩阵。</target>
        </trans-unit>
        <trans-unit id="90d7961623626a54873e65ae75f5e5aedaf80a7d" translate="yes" xml:space="preserve">
          <source>Transform a sequence of instances to a scipy.sparse matrix.</source>
          <target state="translated">将一个实例序列转换为scipy.sparse矩阵。</target>
        </trans-unit>
        <trans-unit id="482237f55f57c5ab1436ea9ad6e0ca3a5497f2c8" translate="yes" xml:space="preserve">
          <source>Transform a signal as a sparse combination of Ricker wavelets. This example visually compares different sparse coding methods using the &lt;a href=&quot;../../modules/generated/sklearn.decomposition.sparsecoder#sklearn.decomposition.SparseCoder&quot;&gt;&lt;code&gt;sklearn.decomposition.SparseCoder&lt;/code&gt;&lt;/a&gt; estimator. The Ricker (also known as Mexican hat or the second derivative of a Gaussian) is not a particularly good kernel to represent piecewise constant signals like this one. It can therefore be seen how much adding different widths of atoms matters and it therefore motivates learning the dictionary to best fit your type of signals.</source>
          <target state="translated">将信号转换为Ricker小波的稀疏组合。本示例使用&lt;a href=&quot;../../modules/generated/sklearn.decomposition.sparsecoder#sklearn.decomposition.SparseCoder&quot;&gt; &lt;code&gt;sklearn.decomposition.SparseCoder&lt;/code&gt; &lt;/a&gt;估计器直观地比较了不同的稀疏编码方法。Ricker（也称为墨西哥帽或高斯的二阶导数）并不是一种特别好的内核，无法表示像这样的分段常数信号。因此，可以看出添加不同宽度的原子有多重要，因此可以激发学习字典以最适合您的信号类型的需求。</target>
        </trans-unit>
        <trans-unit id="3c3158f9e95a76dac9ab046600d246dc683b1322" translate="yes" xml:space="preserve">
          <source>Transform array or sparse matrix X back to feature mappings.</source>
          <target state="translated">将数组或稀疏矩阵X变换回特征映射。</target>
        </trans-unit>
        <trans-unit id="43aed443a30ff04a0a7d38cae0c2e3f2c765ad45" translate="yes" xml:space="preserve">
          <source>Transform between iterable of iterables and a multilabel format</source>
          <target state="translated">在iterable的iterable和多标签格式之间进行转换。</target>
        </trans-unit>
        <trans-unit id="8428b18b095eb02611727f6a1283e0146f4aea18" translate="yes" xml:space="preserve">
          <source>Transform binary labels back to multi-class labels</source>
          <target state="translated">将二进制标签转换回多类标签</target>
        </trans-unit>
        <trans-unit id="2d5fb2d774241a80b97c22822072a1cd5822cad7" translate="yes" xml:space="preserve">
          <source>Transform data X according to the fitted model.</source>
          <target state="translated">根据拟合模型变换数据X。</target>
        </trans-unit>
        <trans-unit id="e993947ab9336eb409d6a8eb55c55e2b5b858d46" translate="yes" xml:space="preserve">
          <source>Transform data back to its original space.</source>
          <target state="translated">将数据变换回原来的空间。</target>
        </trans-unit>
        <trans-unit id="b922af176e5b4295d0d766cd496f7523d4754428" translate="yes" xml:space="preserve">
          <source>Transform data to polynomial features</source>
          <target state="translated">将数据转化为多项式特征</target>
        </trans-unit>
        <trans-unit id="f1a4a6b05048c3643e26b0d505b52199b7895296" translate="yes" xml:space="preserve">
          <source>Transform dataset.</source>
          <target state="translated">转换数据集。</target>
        </trans-unit>
        <trans-unit id="0146265304f248a8c03f040ea5d584981839ec0b" translate="yes" xml:space="preserve">
          <source>Transform documents to document-term matrix.</source>
          <target state="translated">将文件转化为文档术语矩阵。</target>
        </trans-unit>
        <trans-unit id="778e7579ae52504e167839efee081ba3167de93f" translate="yes" xml:space="preserve">
          <source>Transform feature-&amp;gt;value dicts to array or sparse matrix.</source>
          <target state="translated">将feature-&amp;gt; value dict转换为数组或稀疏矩阵。</target>
        </trans-unit>
        <trans-unit id="c8f4f5c3bee4bfd8c782321e0d4eb227c2d3191b" translate="yes" xml:space="preserve">
          <source>Transform features using quantiles information.</source>
          <target state="translated">利用量子信息变换特征。</target>
        </trans-unit>
        <trans-unit id="ace4ae2489dd9688eddb3a58e732664d39d28a92" translate="yes" xml:space="preserve">
          <source>Transform labels back to original encoding.</source>
          <target state="translated">将标签转换回原始编码。</target>
        </trans-unit>
        <trans-unit id="76c682df30bb4975f2641f2e89f16cc0b5f2d625" translate="yes" xml:space="preserve">
          <source>Transform labels to normalized encoding.</source>
          <target state="translated">将标签转换为归一化编码。</target>
        </trans-unit>
        <trans-unit id="e6d8f7568400d53b2f444fa6cbf018c08b09552e" translate="yes" xml:space="preserve">
          <source>Transform multi-class labels to binary labels</source>
          <target state="translated">将多类标签转换为二进制标签</target>
        </trans-unit>
        <trans-unit id="ec1f3a72d306387b537de1b3b116fbdf51b17550" translate="yes" xml:space="preserve">
          <source>Transform new data by linear interpolation</source>
          <target state="translated">通过线性插值转换新数据</target>
        </trans-unit>
        <trans-unit id="7e25dbc81754715628745ec728c6c249ac9d1737" translate="yes" xml:space="preserve">
          <source>Transform new points into embedding space.</source>
          <target state="translated">将新点转化为嵌入空间。</target>
        </trans-unit>
        <trans-unit id="17e15b65d999776fc7cb047cfc38d87f9b340eec" translate="yes" xml:space="preserve">
          <source>Transform the data X according to the fitted NMF model</source>
          <target state="translated">根据拟合的NMF模型对数据X进行变换。</target>
        </trans-unit>
        <trans-unit id="28a4737ac1d13b4e451237dc699b89c49f7fb862" translate="yes" xml:space="preserve">
          <source>Transform the given indicator matrix into label sets</source>
          <target state="translated">将给定的指标矩阵转化为标签集</target>
        </trans-unit>
        <trans-unit id="38739bda11e07f48ac023acb8323ed328f115bd5" translate="yes" xml:space="preserve">
          <source>Transform the given label sets</source>
          <target state="translated">转换给定的标签集</target>
        </trans-unit>
        <trans-unit id="92a052e88a019f5aca9bb96a9137d202560617b4" translate="yes" xml:space="preserve">
          <source>Transform the sources back to the mixed data (apply mixing matrix).</source>
          <target state="translated">将源转换回混合数据(应用混合矩阵)。</target>
        </trans-unit>
        <trans-unit id="6414c408546f181e607c3ec28647dd72e64872ea" translate="yes" xml:space="preserve">
          <source>Transform your features into a higher dimensional, sparse space. Then train a linear model on these features.</source>
          <target state="translated">将你的特征转化为更高维度的稀疏空间。然后在这些特征上训练一个线性模型。</target>
        </trans-unit>
        <trans-unit id="d3709f378c935401f6b259df9cce5a50135da098" translate="yes" xml:space="preserve">
          <source>Transformed array.</source>
          <target state="translated">转化的数组。</target>
        </trans-unit>
        <trans-unit id="4a8a97e010ec7ac27b50257ef7ee542c13ba8846" translate="yes" xml:space="preserve">
          <source>Transformed data</source>
          <target state="translated">转换后的数据</target>
        </trans-unit>
        <trans-unit id="d460e113769e190612a2b959c1c729d7e8676439" translate="yes" xml:space="preserve">
          <source>Transformed data in the binned space.</source>
          <target state="translated">在二进制空间中变换数据。</target>
        </trans-unit>
        <trans-unit id="14642329121567cf9f5775d8a6512d3b978fccd2" translate="yes" xml:space="preserve">
          <source>Transformed data matrix</source>
          <target state="translated">变换后的数据矩阵</target>
        </trans-unit>
        <trans-unit id="0d3a338b719647431757293955a1513d13c572f4" translate="yes" xml:space="preserve">
          <source>Transformed data.</source>
          <target state="translated">转化的数据。</target>
        </trans-unit>
        <trans-unit id="08b12f8aaa8632b66a6a22bc4de550a469c3cc9c" translate="yes" xml:space="preserve">
          <source>Transformed dataset.</source>
          <target state="translated">转换后的数据集。</target>
        </trans-unit>
        <trans-unit id="eeb85e59603c1cea29acb31c92a29204737376ea" translate="yes" xml:space="preserve">
          <source>Transformed input.</source>
          <target state="translated">转化输入。</target>
        </trans-unit>
        <trans-unit id="0a6145f06a4913811002ff339bc5284d2892e790" translate="yes" xml:space="preserve">
          <source>Transformed samples</source>
          <target state="translated">改造后的样品</target>
        </trans-unit>
        <trans-unit id="6d517e36599e67ec967c7be2afac6d7777579d1a" translate="yes" xml:space="preserve">
          <source>Transformer used in &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">用于 &lt;code&gt;predict&lt;/code&gt; 和 &lt;code&gt;fit&lt;/code&gt; 变压器。</target>
        </trans-unit>
        <trans-unit id="43471a7ace97310a9577002aa9803e00d83e6192" translate="yes" xml:space="preserve">
          <source>Transformers are usually combined with classifiers, regressors or other estimators to build a composite estimator. The most common tool is a &lt;a href=&quot;#pipeline&quot;&gt;Pipeline&lt;/a&gt;. Pipeline is often used in combination with &lt;a href=&quot;#feature-union&quot;&gt;FeatureUnion&lt;/a&gt; which concatenates the output of transformers into a composite feature space. &lt;a href=&quot;#transformed-target-regressor&quot;&gt;TransformedTargetRegressor&lt;/a&gt; deals with transforming the &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-target&quot;&gt;target&lt;/a&gt; (i.e. log-transform &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-171&quot;&gt;y&lt;/a&gt;). In contrast, Pipelines only transform the observed data (&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-x&quot;&gt;X&lt;/a&gt;).</source>
          <target state="translated">变压器通常与分类器，回归器或其他估计器组合在一起，以构建复合估计器。最常用的工具是&lt;a href=&quot;#pipeline&quot;&gt;管道&lt;/a&gt;。流水线通常与&lt;a href=&quot;#feature-union&quot;&gt;FeatureUnion&lt;/a&gt;结合使用，FeatureUnion将变压器的输出连接到复合特征空间中。&lt;a href=&quot;#transformed-target-regressor&quot;&gt;TransformedTargetRegressor&lt;/a&gt;处理转换&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-target&quot;&gt;目标&lt;/a&gt;（即log-transform &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-171&quot;&gt;y&lt;/a&gt;）。相反，管道仅转换观察到的数据（&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-x&quot;&gt;X&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="b69fe15775501662a569d2ed632ddbe970e558ef" translate="yes" xml:space="preserve">
          <source>Transformers for missing value imputation</source>
          <target state="translated">缺失值推算的变压器</target>
        </trans-unit>
        <trans-unit id="4804df5ca1652cab2567ab10e41eae2d30b7e99a" translate="yes" xml:space="preserve">
          <source>Transforming Classifier Scores into Accurate Multiclass Probability Estimates, B. Zadrozny &amp;amp; C. Elkan, (KDD 2002)</source>
          <target state="translated">B. Zadrozny＆C. Elkan，将分类器得分转换为准确的多类概率估计，（KDD 2002）</target>
        </trans-unit>
        <trans-unit id="74f517360774a680819178109aa2b52b87d4fd99" translate="yes" xml:space="preserve">
          <source>Transforming distance to well-behaved similarities</source>
          <target state="translated">将距离转化为行为良好的相似性</target>
        </trans-unit>
        <trans-unit id="dfe17b4b683a10ef2eafef30897d9c629bf96dd6" translate="yes" xml:space="preserve">
          <source>Transforms discretized data back to original feature space.</source>
          <target state="translated">将离散化数据转换回原始特征空间。</target>
        </trans-unit>
        <trans-unit id="5bd7a9a7032f01002afe1d21dc1635e87bd5dbc6" translate="yes" xml:space="preserve">
          <source>Transforms features by scaling each feature to a given range.</source>
          <target state="translated">通过将每个特征缩放到给定范围来转换特征。</target>
        </trans-unit>
        <trans-unit id="45675a7235910659531092f94ca2cac1226cb6a9" translate="yes" xml:space="preserve">
          <source>Transforms lists of feature-value mappings to vectors.</source>
          <target state="translated">将特征值映射列表转换为向量。</target>
        </trans-unit>
        <trans-unit id="ff596a653686d4986dda1851c66682d846f4bf4d" translate="yes" xml:space="preserve">
          <source>Transforms the image samples in X into a matrix of patch data.</source>
          <target state="translated">将X中的图像样本转换为补丁数据的矩阵。</target>
        </trans-unit>
        <trans-unit id="d6a25d0e3691aa7e7e60fcc1d61ee6046c7d09b3" translate="yes" xml:space="preserve">
          <source>Tree-based estimators (see the &lt;a href=&quot;classes#module-sklearn.tree&quot;&gt;&lt;code&gt;sklearn.tree&lt;/code&gt;&lt;/a&gt; module and forest of trees in the &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module) can be used to compute feature importances, which in turn can be used to discard irrelevant features (when coupled with the &lt;a href=&quot;generated/sklearn.feature_selection.selectfrommodel#sklearn.feature_selection.SelectFromModel&quot;&gt;&lt;code&gt;sklearn.feature_selection.SelectFromModel&lt;/code&gt;&lt;/a&gt; meta-transformer):</source>
          <target state="translated">基于树的估计量（请参阅&lt;a href=&quot;classes#module-sklearn.tree&quot;&gt; &lt;code&gt;sklearn.tree&lt;/code&gt; &lt;/a&gt;模块和sklearn.ensemble模块中的&lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt;）可用于计算特征重要性，而特征重要性又可用于丢弃不相关的特征（与&lt;a href=&quot;generated/sklearn.feature_selection.selectfrommodel#sklearn.feature_selection.SelectFromModel&quot;&gt; &lt;code&gt;sklearn.feature_selection.SelectFromModel&lt;/code&gt; &lt;/a&gt;结合使用时）元变压器）：</target>
        </trans-unit>
        <trans-unit id="81d8ac0c0739336d0bbd6f8053b2fde8039cdb1c" translate="yes" xml:space="preserve">
          <source>Triangle Inequality: d(x, y) + d(y, z) &amp;gt;= d(x, z)</source>
          <target state="translated">三角不等式：d（x，y）+ d（y，z）&amp;gt; = d（x，z）</target>
        </trans-unit>
        <trans-unit id="331d2c199452ae22aa8941c5bcbe6a7fe41c68b5" translate="yes" xml:space="preserve">
          <source>Tristan Fletcher: &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.651.8603&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Relevance Vector Machines explained&lt;/a&gt;</source>
          <target state="translated">Tristan Fletcher：&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.651.8603&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;相关向量机介绍&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="3b19d80cd81c13647ace615b9d73da08b4d8c61b" translate="yes" xml:space="preserve">
          <source>True : always precompute distances</source>
          <target state="translated">真:总是预先计算距离</target>
        </trans-unit>
        <trans-unit id="ebac1d7d68472a848a069828ba36b6b2dd6227bb" translate="yes" xml:space="preserve">
          <source>True binary labels in binary indicator format.</source>
          <target state="translated">二进制指标格式的真实二进制标签。</target>
        </trans-unit>
        <trans-unit id="94ad072572f1b0d8a6896ff3fd5d5269c3006cce" translate="yes" xml:space="preserve">
          <source>True binary labels or binary label indicators.</source>
          <target state="translated">真正的二元标签或二元标签指标。</target>
        </trans-unit>
        <trans-unit id="173029937373f6d16ed7438491b1a8131b2bc4cb" translate="yes" xml:space="preserve">
          <source>True binary labels. If labels are not either {-1, 1} or {0, 1}, then pos_label should be explicitly given.</source>
          <target state="translated">真正的二进制标签。如果标签不是{-1,1}或者{0,1},那么应该明确给出pos_label。</target>
        </trans-unit>
        <trans-unit id="cba1cd7ae39f91f6d0e2908d3956200bdf94de07" translate="yes" xml:space="preserve">
          <source>True if estimator is a classifier and False otherwise.</source>
          <target state="translated">如果估计器是一个分类器,则为真,否则为假。</target>
        </trans-unit>
        <trans-unit id="7e34070b9f977933411df9b814860396cade02bb" translate="yes" xml:space="preserve">
          <source>True if estimator is a regressor and False otherwise.</source>
          <target state="translated">如果估计器是回归者,则为真,否则为假。</target>
        </trans-unit>
        <trans-unit id="d039a95b006860b5b92d23f84015c0458d6fdb1a" translate="yes" xml:space="preserve">
          <source>True if the array returned from predict is to be in sparse CSC format. Is automatically set to True if the input y is passed in sparse format.</source>
          <target state="translated">如果predict返回的数组是稀疏的CSC格式,则为True。如果输入的y是稀疏格式,则自动设置为True。</target>
        </trans-unit>
        <trans-unit id="018f28ffd2c7241c63be51b46bba3e8aa528d907" translate="yes" xml:space="preserve">
          <source>True if the input data to transform is given as a sparse matrix, False otherwise.</source>
          <target state="translated">如果要转换的输入数据为稀疏矩阵,则为真,否则为假。</target>
        </trans-unit>
        <trans-unit id="06236e43536e8bd62b7d950e36ddd9dca022a999" translate="yes" xml:space="preserve">
          <source>True if the output at fit is 2d, else false.</source>
          <target state="translated">如果拟合时的输出为2d,则为真,否则为假。</target>
        </trans-unit>
        <trans-unit id="abda54d00232aa3c71419926e38966e372a6e200" translate="yes" xml:space="preserve">
          <source>True if the returned array from transform is desired to be in sparse CSR format.</source>
          <target state="translated">如果希望变换后返回的数组是稀疏的CSR格式,则为真。</target>
        </trans-unit>
        <trans-unit id="b3f5d1c4b9aeea8d97315ada02d3f0f3b6e0dbc5" translate="yes" xml:space="preserve">
          <source>True labels for X.</source>
          <target state="translated">X的真实标签。</target>
        </trans-unit>
        <trans-unit id="24a7816a0ae25d0715f83e367246b56738200fc8" translate="yes" xml:space="preserve">
          <source>True mutual information can&amp;rsquo;t be negative. If its estimate turns out to be negative, it is replaced by zero.</source>
          <target state="translated">真正的相互信息不能是负面的。如果其估计结果为负，则将其替换为零。</target>
        </trans-unit>
        <trans-unit id="e857c90bead41164c28f265fe201e3c7a69f5d75" translate="yes" xml:space="preserve">
          <source>True target, consisting of integers of two values. The positive label must be greater than the negative label.</source>
          <target state="translated">真目标,由两个值的整数组成。正标必须大于负标。</target>
        </trans-unit>
        <trans-unit id="5f6f5563a268706baa91536cfcd1565c453cd8e7" translate="yes" xml:space="preserve">
          <source>True targets of binary classification in range {-1, 1} or {0, 1}.</source>
          <target state="translated">二元分类的真目标在{-1,1}或{0,1}范围内。</target>
        </trans-unit>
        <trans-unit id="6e2bbfc40bf0e63b31fb5b0351be961b49cc74be" translate="yes" xml:space="preserve">
          <source>True targets.</source>
          <target state="translated">真正的目标。</target>
        </trans-unit>
        <trans-unit id="18dd5ee40d70767a2f6629e8ff8969a87290115e" translate="yes" xml:space="preserve">
          <source>True values for X</source>
          <target state="translated">X的真实值</target>
        </trans-unit>
        <trans-unit id="81e3774c236b4c61a22388a1a822b3e69fb3e5a6" translate="yes" xml:space="preserve">
          <source>True values for X.</source>
          <target state="translated">X的真值。</target>
        </trans-unit>
        <trans-unit id="0d7ce48badf2f0a91a36bec7511768633417749f" translate="yes" xml:space="preserve">
          <source>True when convergence was reached in fit(), False otherwise.</source>
          <target state="translated">当fit()中达到收敛时为true,否则为False。</target>
        </trans-unit>
        <trans-unit id="d333cd18e174fe06286d776d55b1b9eaf760ce1b" translate="yes" xml:space="preserve">
          <source>True: Force all values of X to be finite.</source>
          <target state="translated">真:强迫X的所有值都是有限的。</target>
        </trans-unit>
        <trans-unit id="260b9ffda14105c1fd2ffa31d36eae7c83268ddd" translate="yes" xml:space="preserve">
          <source>True: the results is casted to an unsigned int</source>
          <target state="translated">True:结果被转换为一个无符号的int。</target>
        </trans-unit>
        <trans-unit id="00b6f6ebc7b7070cf35772b16b427811573346a1" translate="yes" xml:space="preserve">
          <source>Try classifying classes 1 and 2 from the iris dataset with SVMs, with the 2 first features. Leave out 10% of each class and test prediction performance on these observations.</source>
          <target state="translated">试着用SVMs对虹膜数据集中的类1和类2进行分类,先用2个特征。留出每个类的10%,并在这些观测值上测试预测性能。</target>
        </trans-unit>
        <trans-unit id="5449ae93c54cf3f8e79ab0ee95bb4ee118bd4f84" translate="yes" xml:space="preserve">
          <source>Try classifying the digits dataset with nearest neighbors and a linear model. Leave out the last 10% and test prediction performance on these observations.</source>
          <target state="translated">尝试用最近的邻居和线性模型对数字数据集进行分类。留出最后10%,并对这些观测值的预测性能进行测试。</target>
        </trans-unit>
        <trans-unit id="7a783eca4388b1c7b8c830f476caa080328ba3c3" translate="yes" xml:space="preserve">
          <source>Try playing around with the &lt;code&gt;analyzer&lt;/code&gt; and &lt;code&gt;token normalisation&lt;/code&gt; under &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">尝试在&lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt;下使用 &lt;code&gt;analyzer&lt;/code&gt; 和 &lt;code&gt;token normalisation&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b7e468fa3f6cfdfb33fa6bf28dcdf3165bf89507" translate="yes" xml:space="preserve">
          <source>Try to differentiate the two first classes of the iris data</source>
          <target state="translated">试着区分虹膜数据的两个第一类。</target>
        </trans-unit>
        <trans-unit id="1bc09af6523c25787ea3631aac8c3f52f8bc29dc" translate="yes" xml:space="preserve">
          <source>Try using &lt;a href=&quot;../../modules/decomposition#lsa&quot;&gt;Truncated SVD&lt;/a&gt; for &lt;a href=&quot;https://en.wikipedia.org/wiki/Latent_semantic_analysis&quot;&gt;latent semantic analysis&lt;/a&gt;.</source>
          <target state="translated">尝试使用&lt;a href=&quot;../../modules/decomposition#lsa&quot;&gt;截断的SVD&lt;/a&gt;进行&lt;a href=&quot;https://en.wikipedia.org/wiki/Latent_semantic_analysis&quot;&gt;潜在语义分析&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="126c548fa4d4a4c65c7759b8f0eb82ce5677dab5" translate="yes" xml:space="preserve">
          <source>Tsoumakas, G., Katakis, I., &amp;amp; Vlahavas, I. (2010). Mining multi-label data. In Data mining and knowledge discovery handbook (pp. 667-685). Springer US.</source>
          <target state="translated">Tsoumakas，G.，Katakis，I.和Vlahavas，I.（2010）。挖掘多标签数据。在《数据挖掘和知识发现手册》（第667-685页）中。施普林格美国。</target>
        </trans-unit>
        <trans-unit id="0d016a3ee3141a6ebd01b9c31169fb6ec8d37fd6" translate="yes" xml:space="preserve">
          <source>Tuning the hyper-parameters of an estimator</source>
          <target state="translated">调整估计器的超参数。</target>
        </trans-unit>
        <trans-unit id="2e926727653886b165b872a4b6b2a62bf90f0bd1" translate="yes" xml:space="preserve">
          <source>Tuple of row and column indicators for a set of biclusters.</source>
          <target state="translated">一组双簇的行和列指标元组。</target>
        </trans-unit>
        <trans-unit id="c054700312acf34cbacbc98d115120c76da3a6d5" translate="yes" xml:space="preserve">
          <source>Turn seed into a np.random.RandomState instance</source>
          <target state="translated">将种子变成一个np.random.RandomState实例。</target>
        </trans-unit>
        <trans-unit id="007a671747688cedb01751baca6545483f05de7a" translate="yes" xml:space="preserve">
          <source>Tutorial setup</source>
          <target state="translated">教程设置</target>
        </trans-unit>
        <trans-unit id="025f75efad84ed2b985f2818a53e81aa77abca7c" translate="yes" xml:space="preserve">
          <source>Tutorial: A tutorial on statistical-learning for scientific data processing</source>
          <target state="translated">教程。关于科学数据处理的统计学习教程</target>
        </trans-unit>
        <trans-unit id="e7df3b5ebbaf2fd3b4b4579edbd7ce42f46db699" translate="yes" xml:space="preserve">
          <source>Tutorial: An introduction to machine learning with scikit-learn</source>
          <target state="translated">教程。使用scikit-learn进行机器学习的介绍。</target>
        </trans-unit>
        <trans-unit id="8682fb6c27e32858369b74e47f829fad6c8d3a68" translate="yes" xml:space="preserve">
          <source>Tutorial: Choosing the right estimator</source>
          <target state="translated">教程。选择正确的估算器</target>
        </trans-unit>
        <trans-unit id="2865c0d93493065de0c34da92792e35a845226d3" translate="yes" xml:space="preserve">
          <source>Tutorial: Model selection</source>
          <target state="translated">教程:模型选择</target>
        </trans-unit>
        <trans-unit id="b7709b919b68974b71489ceb95a00ef31c4eda72" translate="yes" xml:space="preserve">
          <source>Tutorial: Putting it all together</source>
          <target state="translated">教程。把所有的东西放在一起</target>
        </trans-unit>
        <trans-unit id="b0b3bc4bbf4e62230750bf24baeb122d7a994298" translate="yes" xml:space="preserve">
          <source>Tutorial: Statistical learning</source>
          <target state="translated">教程。统计学习</target>
        </trans-unit>
        <trans-unit id="a149365421f01d98250256737c350c42a4cd4b82" translate="yes" xml:space="preserve">
          <source>Tutorial: Supervised learning</source>
          <target state="translated">教程。监督学习</target>
        </trans-unit>
        <trans-unit id="4353f067a68843e09ae0691ab9f9c44ef2e6db23" translate="yes" xml:space="preserve">
          <source>Tutorial: Unsupervised learning</source>
          <target state="translated">教程。无监督学习</target>
        </trans-unit>
        <trans-unit id="206fac7baeed5ee14f8990630b6607a7c33e8644" translate="yes" xml:space="preserve">
          <source>Tutorial: Working With Text Data</source>
          <target state="translated">教程。使用文本数据</target>
        </trans-unit>
        <trans-unit id="b919de3c63710fd07133db7062fb5a1fbffa0bfe" translate="yes" xml:space="preserve">
          <source>Tutorial: scikit-learn Tutorials</source>
          <target state="translated">教程:scikit-learn 教程</target>
        </trans-unit>
        <trans-unit id="654171647baa6be8557a5d627cf35c7075ebb257" translate="yes" xml:space="preserve">
          <source>Tutorials</source>
          <target state="translated">Tutorials</target>
        </trans-unit>
        <trans-unit id="995550b74403db560a3a2ea8d3906cd56b901336" translate="yes" xml:space="preserve">
          <source>Two algorithms are demoed: ordinary k-means and its more scalable cousin minibatch k-means.</source>
          <target state="translated">演示了两种算法:普通k-means和它的更可扩展的表兄minibatch k-means。</target>
        </trans-unit>
        <trans-unit id="3b934d458351995534fed7d234de7b14c38f4cd4" translate="yes" xml:space="preserve">
          <source>Two approaches for performing calibration of probabilistic predictions are provided: a parametric approach based on Platt&amp;rsquo;s sigmoid model and a non-parametric approach based on isotonic regression (&lt;a href=&quot;classes#module-sklearn.isotonic&quot;&gt;&lt;code&gt;sklearn.isotonic&lt;/code&gt;&lt;/a&gt;). Probability calibration should be done on new data not used for model fitting. The class &lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt;&lt;code&gt;CalibratedClassifierCV&lt;/code&gt;&lt;/a&gt; uses a cross-validation generator and estimates for each split the model parameter on the train samples and the calibration of the test samples. The probabilities predicted for the folds are then averaged. Already fitted classifiers can be calibrated by &lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt;&lt;code&gt;CalibratedClassifierCV&lt;/code&gt;&lt;/a&gt; via the parameter cv=&amp;rdquo;prefit&amp;rdquo;. In this case, the user has to take care manually that data for model fitting and calibration are disjoint.</source>
          <target state="translated">提供了两种执行概率预测标定的方法：基于Platt的S型模型的参数方法和基于等张回归的非参数方法（&lt;a href=&quot;classes#module-sklearn.isotonic&quot;&gt; &lt;code&gt;sklearn.isotonic&lt;/code&gt; &lt;/a&gt;）。概率校准应针对未用于模型拟合的新数据进行。&lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt; &lt;code&gt;CalibratedClassifierCV&lt;/code&gt; &lt;/a&gt;类使用交叉验证生成器，并针对每个拆分估计样本上的模型参数和测试样本的校准。然后将对折的预测概率进行平均。&lt;a href=&quot;generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV&quot;&gt; &lt;code&gt;CalibratedClassifierCV&lt;/code&gt; 可以&lt;/a&gt;通过参数cv =&amp;ldquo; prefit&amp;rdquo; 来校准已经拟合的分类器。在这种情况下，用户必须手动注意用于模型拟合和校准的数据是不相交的。</target>
        </trans-unit>
        <trans-unit id="de7e8d6ad699213a292d0c528c5ddad33bca14ae" translate="yes" xml:space="preserve">
          <source>Two consequences of imposing a connectivity can be seen. First clustering with a connectivity matrix is much faster.</source>
          <target state="translated">可以看出施加连通性的两个后果。首先用连通性矩阵进行聚类要快得多。</target>
        </trans-unit>
        <trans-unit id="73ece4ca1e1779fbf5110031e848f2313deac958" translate="yes" xml:space="preserve">
          <source>Two cross-validation loops are performed in parallel: one by the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; estimator to set &lt;code&gt;gamma&lt;/code&gt; and the other one by &lt;code&gt;cross_val_score&lt;/code&gt; to measure the prediction performance of the estimator. The resulting scores are unbiased estimates of the prediction score on new data.</source>
          <target state="translated">并行执行两个交叉验证循环：一个由&lt;a href=&quot;../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt;估计器设置 &lt;code&gt;gamma&lt;/code&gt; ，另一个由 &lt;code&gt;cross_val_score&lt;/code&gt; 来测量估计器的预测性能。所得分数是对新数据的预测分数的无偏估计。</target>
        </trans-unit>
        <trans-unit id="d467efdd44bf60415fc895b8589d9d81d46d02ec" translate="yes" xml:space="preserve">
          <source>Two families of ensemble methods are usually distinguished:</source>
          <target state="translated">通常有两类合集方法的区别。</target>
        </trans-unit>
        <trans-unit id="e8ac95de27d48015555c5b981483208d51b8f268" translate="yes" xml:space="preserve">
          <source>Two feature extraction methods can be used in this example:</source>
          <target state="translated">本例中可以使用两种特征提取方法。</target>
        </trans-unit>
        <trans-unit id="ebb2ce8305b879c94bfe5ff4f307e7a35d679e99" translate="yes" xml:space="preserve">
          <source>Two plots will be shown for each scaler/normalizer/transformer. The left figure will show a scatter plot of the full data set while the right figure will exclude the extreme values considering only 99 % of the data set, excluding marginal outliers. In addition, the marginal distributions for each feature will be shown on the side of the scatter plot.</source>
          <target state="translated">每个缩放器/归一化器/变换器将显示两张图。左图将显示全部数据集的散点图,而右图将排除极端值,只考虑数据集的99%,排除边缘异常值。此外,每个特征的边际分布将显示在散点图的侧面。</target>
        </trans-unit>
        <trans-unit id="348f286c4d7d6b276984cd38d102dc527023a237" translate="yes" xml:space="preserve">
          <source>Two separate datasets are used for the two different plots. The reason behind this is the &lt;code&gt;l1&lt;/code&gt; case works better on sparse data, while &lt;code&gt;l2&lt;/code&gt; is better suited to the non-sparse case.</source>
          <target state="translated">两个不同的数据集使用两个单独的数据集。这背后的原因是 &lt;code&gt;l1&lt;/code&gt; 情况在稀疏数据上效果更好，而 &lt;code&gt;l2&lt;/code&gt; 情况更适合于非稀疏情况。</target>
        </trans-unit>
        <trans-unit id="32895c2e5eacd1283051e2d5a4f1fd3f826fb6ed" translate="yes" xml:space="preserve">
          <source>Two-class AdaBoost</source>
          <target state="translated">两级AdaBoost</target>
        </trans-unit>
        <trans-unit id="b8fde32df7d701e50fc79883cdf21005c9469e51" translate="yes" xml:space="preserve">
          <source>Type casting</source>
          <target state="translated">铸件类型</target>
        </trans-unit>
        <trans-unit id="7b90464c9a3a0593a486a1facdfd06e25cac2162" translate="yes" xml:space="preserve">
          <source>Type of SVM: C SVC, nu SVC, one class, epsilon SVR, nu SVR</source>
          <target state="translated">SVM的类型。C SVC,nu SVC,一类,epsilon SVR,nu SVR。</target>
        </trans-unit>
        <trans-unit id="fb24034e0a15fdb11753c4c561fec377a847eca1" translate="yes" xml:space="preserve">
          <source>Type of SVM: C_SVC, NuSVC, OneClassSVM, EpsilonSVR or NuSVR respectively. 0 by default.</source>
          <target state="translated">SVM的类型。C_SVC,NuSVC,OneClassSVM,EpsilonSVR 或 NuSVR.默认为0。</target>
        </trans-unit>
        <trans-unit id="05734831eef4f60aabd73eed1535149e1780b49e" translate="yes" xml:space="preserve">
          <source>Type of kernel.</source>
          <target state="translated">内核的类型。</target>
        </trans-unit>
        <trans-unit id="7784bde958a1d323776ea14d0478698cc397c040" translate="yes" xml:space="preserve">
          <source>Type of returned matrix: &amp;lsquo;connectivity&amp;rsquo; will return the connectivity matrix with ones and zeros, and &amp;lsquo;distance&amp;rsquo; will return the distances between neighbors according to the given metric.</source>
          <target state="translated">返回矩阵的类型：'connectivity'将返回具有1和0的连通性矩阵，而'distance'将根据给定的度量返回邻居之间的距离。</target>
        </trans-unit>
        <trans-unit id="029a83801426f186d4049ef92d5f3d3590b1d125" translate="yes" xml:space="preserve">
          <source>Type of returned matrix: &amp;lsquo;connectivity&amp;rsquo; will return the connectivity matrix with ones and zeros, in &amp;lsquo;distance&amp;rsquo; the edges are Euclidean distance between points.</source>
          <target state="translated">返回矩阵的类型：'connectivity'将返回具有1和0的连通性矩阵，在'distance'中，边为点之间的欧几里得距离。</target>
        </trans-unit>
        <trans-unit id="e2af4c36790c9137ba49cdb815accc60f6f75311" translate="yes" xml:space="preserve">
          <source>Type of store backend for reading/writing cache files. Default: &amp;lsquo;local&amp;rsquo;. The &amp;lsquo;local&amp;rsquo; backend is using regular filesystem operations to manipulate data (open, mv, etc) in the backend.</source>
          <target state="translated">用于读取/写入缓存文件的存储后端的类型。默认值：&amp;ldquo;本地&amp;rdquo;。&amp;ldquo;本地&amp;rdquo;后端正在使用常规文件系统操作来操纵后端中的数据（打开，MV等）。</target>
        </trans-unit>
        <trans-unit id="858cba7a97e85950fd69a9661ce88f3dff1bf729" translate="yes" xml:space="preserve">
          <source>Type of the matrix returned by fit_transform() or transform().</source>
          <target state="translated">fit_transform()或transform()返回的矩阵类型。</target>
        </trans-unit>
        <trans-unit id="b6e792a3d08a7bd144dac10e42edb461fd3dd2e3" translate="yes" xml:space="preserve">
          <source>Type to use in computing the mean. For integer inputs, the default is &lt;code&gt;float64&lt;/code&gt;; for floating point inputs, it is the same as the input dtype.</source>
          <target state="translated">用于计算平均值的类型。对于整数输入，默认值为 &lt;code&gt;float64&lt;/code&gt; ；对于浮点输入，它与输入dtype相同。</target>
        </trans-unit>
        <trans-unit id="f77afa338a167babd59a76b7f498d6550ba586ff" translate="yes" xml:space="preserve">
          <source>Under the assumption that the data are Gaussian distributed, Chen et al. &lt;a href=&quot;#id6&quot; id=&quot;id5&quot;&gt;[2]&lt;/a&gt; derived a formula aimed at choosing a shrinkage coefficient that yields a smaller Mean Squared Error than the one given by Ledoit and Wolf&amp;rsquo;s formula. The resulting estimator is known as the Oracle Shrinkage Approximating estimator of the covariance.</source>
          <target state="translated">在假设数据是高斯分布的前提下，Chen等人。&lt;a href=&quot;#id6&quot; id=&quot;id5&quot;&gt;[2]&lt;/a&gt;得出了一个旨在选择收缩系数的公式，该公式产生的均方误差比Ledoit和Wolf公式给出的均方误差小。所得的估计量称为协方差的Oracle收缩率近似估计量。</target>
        </trans-unit>
        <trans-unit id="b57ce0246b95ca0ff3d95c499722ea513c24180d" translate="yes" xml:space="preserve">
          <source>Underfitting vs. Overfitting</source>
          <target state="translated">低配与过配</target>
        </trans-unit>
        <trans-unit id="10dab5fb240281c20bb10ad043cbba82ec3b0bd6" translate="yes" xml:space="preserve">
          <source>Understanding the decision tree structure</source>
          <target state="translated">了解决策树结构</target>
        </trans-unit>
        <trans-unit id="a381b476a1bdd0042346ffd8d02655a7131aa344" translate="yes" xml:space="preserve">
          <source>Undo the scaling of X according to feature_range.</source>
          <target state="translated">根据feature_range撤销X的缩放。</target>
        </trans-unit>
        <trans-unit id="11b4a2e4a2b6531b9e75ad23f03f25fd4f8ecde0" translate="yes" xml:space="preserve">
          <source>Uniform weights are used by default.</source>
          <target state="translated">默认使用统一的权重。</target>
        </trans-unit>
        <trans-unit id="9421754583ed6d327fe582bef2d0e2d0f17ba0c4" translate="yes" xml:space="preserve">
          <source>Unique class labels.</source>
          <target state="translated">独特的类标签。</target>
        </trans-unit>
        <trans-unit id="6efd4cf40567c19c24a13e3421a6d1109a47da44" translate="yes" xml:space="preserve">
          <source>Uniquely holds the label for each class.</source>
          <target state="translated">唯一地保存每个类的标签。</target>
        </trans-unit>
        <trans-unit id="15f758514c6db2ef9033ee5636e4d09d40ce9747" translate="yes" xml:space="preserve">
          <source>Univariate Feature Selection</source>
          <target state="translated">单变量特征选择</target>
        </trans-unit>
        <trans-unit id="820dda4dd874419c514343cc2737763cc18b33d1" translate="yes" xml:space="preserve">
          <source>Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the &lt;code&gt;transform&lt;/code&gt; method:</source>
          <target state="translated">单变量特征选择通过基于单变量统计检验选择最佳特征来工作。可以将其视为估算器的预处理步骤。Scikit-learn将要素选择例程公开为实现了 &lt;code&gt;transform&lt;/code&gt; 方法的对象：</target>
        </trans-unit>
        <trans-unit id="b943e7c2ae0f248f889b02c7d797d243c0d56e6a" translate="yes" xml:space="preserve">
          <source>Univariate feature selector with configurable mode.</source>
          <target state="translated">单变量特征选择器,可配置模式。</target>
        </trans-unit>
        <trans-unit id="05b44ce5dcc153b8702db1e0eab0c9af1fb62f9c" translate="yes" xml:space="preserve">
          <source>Univariate feature selector with configurable strategy.</source>
          <target state="translated">具有可配置策略的单变量特征选择器。</target>
        </trans-unit>
        <trans-unit id="d9b7ebeeb7d99a7c69a9087085473be1721215ee" translate="yes" xml:space="preserve">
          <source>Univariate linear regression tests.</source>
          <target state="translated">单变量线性回归检验。</target>
        </trans-unit>
        <trans-unit id="833fdc74927caa030c0f5f51bade98d55541b93d" translate="yes" xml:space="preserve">
          <source>Unlabeled entries in &lt;code&gt;y&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;y&lt;/code&gt; 中未标记的条目</target>
        </trans-unit>
        <trans-unit id="d5f2b47c1710490958929f8f01f5858025c133e1" translate="yes" xml:space="preserve">
          <source>Unless otherwise specified, input will be cast to &lt;code&gt;float64&lt;/code&gt;:</source>
          <target state="translated">除非另有说明，否则输入将 &lt;code&gt;float64&lt;/code&gt; 为float64：</target>
        </trans-unit>
        <trans-unit id="6027b38892a9b0df12f36988c98a41a4655ff464" translate="yes" xml:space="preserve">
          <source>Unlike &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt;, the representation of a vector is obtained in an additive fashion, by superimposing the components, without subtracting. Such additive models are efficient for representing images and text.</source>
          <target state="translated">与&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;不同，矢量的表示形式是通过加法而不叠加地以加法方式获得的。这样的加性模型对于表示图像和文本是有效的。</target>
        </trans-unit>
        <trans-unit id="f73d55c7488edaa74f7dc85966062a8a5be2b94b" translate="yes" xml:space="preserve">
          <source>Unlike most other scores, R^2 score may be negative (it need not actually be the square of a quantity R).</source>
          <target state="translated">与其他大多数分数不同的是,R^2分数可能是负数(它实际上不需要是一个量R的平方)。</target>
        </trans-unit>
        <trans-unit id="fb0dd07f15380472f302743b85f6d7c3f60efb9d" translate="yes" xml:space="preserve">
          <source>Unlike the previous scalers, the centering and scaling statistics of this scaler are based on percentiles and are therefore not influenced by a few number of very large marginal outliers. Consequently, the resulting range of the transformed feature values is larger than for the previous scalers and, more importantly, are approximately similar: for both features most of the transformed values lie in a [-2, 3] range as seen in the zoomed-in figure. Note that the outliers themselves are still present in the transformed data. If a separate outlier clipping is desirable, a non-linear transformation is required (see below).</source>
          <target state="translated">与之前的标度器不同,该标度器的居中和标度统计是基于百分数的,因此不会受到少数非常大的边际离群值的影响。因此,转换后的特征值范围比之前的标度器大,更重要的是,它们大致相似:对于这两个特征,大多数转换后的值位于[-2,3]范围内,如放大图所示。请注意,离群值本身仍然存在于变换后的数据中。如果希望单独剪除离群值,则需要进行非线性变换(见下文)。</target>
        </trans-unit>
        <trans-unit id="be4091e1f0941887f57bdebf1b1a9b607356f1f1" translate="yes" xml:space="preserve">
          <source>Unlike the previous transformations, normalization refers to a per sample transformation instead of a per feature transformation.</source>
          <target state="translated">与之前的变换不同,归一化指的是每个样本的变换,而不是每个特征的变换。</target>
        </trans-unit>
        <trans-unit id="d6efdeaf0fd8663d7b74628a841a2a21988919e0" translate="yes" xml:space="preserve">
          <source>Unregularized graph based semi-supervised learning</source>
          <target state="translated">基于无规则化图的半监督学习方式</target>
        </trans-unit>
        <trans-unit id="27bee227769f6c4dd6bbb550e3dab104adba94bb" translate="yes" xml:space="preserve">
          <source>Unsupervised Outlier Detection using Local Outlier Factor (LOF)</source>
          <target state="translated">使用局部离群因子(LOF)进行无监督离群检测。</target>
        </trans-unit>
        <trans-unit id="7b5353048e77b9864be0e146d8fe78c3034a09d3" translate="yes" xml:space="preserve">
          <source>Unsupervised Outlier Detection.</source>
          <target state="translated">无监督外差检测。</target>
        </trans-unit>
        <trans-unit id="091dbb252c60dafbec16ee540eb9588c7d736a5b" translate="yes" xml:space="preserve">
          <source>Unsupervised learner for implementing neighbor searches.</source>
          <target state="translated">用于实现邻域搜索的无监督学习器。</target>
        </trans-unit>
        <trans-unit id="336bcbb510eed89e13ba021c352635cdc0677016" translate="yes" xml:space="preserve">
          <source>Unsupervised learning: seeking representations of the data</source>
          <target state="translated">无监督学习:寻求数据的表示方法</target>
        </trans-unit>
        <trans-unit id="568c5820f9e5362ca266c9125e695403019435a8" translate="yes" xml:space="preserve">
          <source>Unused parameter.</source>
          <target state="translated">未使用的参数。</target>
        </trans-unit>
        <trans-unit id="207a5be036cc811b3313bce86d86c7d5b4302176" translate="yes" xml:space="preserve">
          <source>Update k means estimate on a single mini-batch X.</source>
          <target state="translated">更新k是指对单个小批量X的估计。</target>
        </trans-unit>
        <trans-unit id="718430f889e80a3494a47ee7bec5ca3329673e5e" translate="yes" xml:space="preserve">
          <source>Updated feature-wise means.</source>
          <target state="translated">更新功能上的手段。</target>
        </trans-unit>
        <trans-unit id="4f10d24907ba29eca99bd941c7ead98539a4f5b4" translate="yes" xml:space="preserve">
          <source>Updated feature-wise variances.</source>
          <target state="translated">更新了功能方面的差异。</target>
        </trans-unit>
        <trans-unit id="693a7de21c7466734e7ffa03c5ae997209e7997d" translate="yes" xml:space="preserve">
          <source>Updated number of seen samples.</source>
          <target state="translated">更新了看到的样本数量。</target>
        </trans-unit>
        <trans-unit id="410e0e09369f3d862bca36022b47e478be0933f7" translate="yes" xml:space="preserve">
          <source>Updates the model using the data in X as a mini-batch.</source>
          <target state="translated">使用X中的数据作为小批量更新模型。</target>
        </trans-unit>
        <trans-unit id="48b5dd5eaa54e931a34dd1d6396ec1c9d66da80b" translate="yes" xml:space="preserve">
          <source>Urbanowicz R.J., Moore, J.H. &lt;a href=&quot;https://doi.org/10.1007/s12065-015-0128-8&quot;&gt;ExSTraCS 2.0: description and evaluation of a scalable learning classifier system&lt;/a&gt;, Evol. Intel. (2015) 8: 89.</source>
          <target state="translated">Urbanowicz RJ，Moore，JH &lt;a href=&quot;https://doi.org/10.1007/s12065-015-0128-8&quot;&gt;ExSTraCS 2.0：可扩展学习分类系统&lt;/a&gt; Evol的描述和评估。英特尔。（2015）8：89。</target>
        </trans-unit>
        <trans-unit id="173610cb31251b28e80fadc258036215d99d7128" translate="yes" xml:space="preserve">
          <source>Usage examples:</source>
          <target state="translated">用例。</target>
        </trans-unit>
        <trans-unit id="272998fc40498f57127bf4e7cf71805cd53c9500" translate="yes" xml:space="preserve">
          <source>Use 0 when &lt;code&gt;Y&lt;/code&gt; contains the output of decision_function (classifier). Use 0.5 when &lt;code&gt;Y&lt;/code&gt; contains the output of predict_proba.</source>
          <target state="translated">当 &lt;code&gt;Y&lt;/code&gt; 包含Decision_function（分类器）的输出时，请使用0 。当 &lt;code&gt;Y&lt;/code&gt; 包含predict_proba的输出时，请使用0.5 。</target>
        </trans-unit>
        <trans-unit id="e620712d1a8872ff21d8a3d8ca61cf7867c4f8c8" translate="yes" xml:space="preserve">
          <source>Use &lt;code&gt;min_samples_split&lt;/code&gt; or &lt;code&gt;min_samples_leaf&lt;/code&gt; to ensure that multiple samples inform every decision in the tree, by controlling which splits will be considered. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. Try &lt;code&gt;min_samples_leaf=5&lt;/code&gt; as an initial value. If the sample size varies greatly, a float number can be used as percentage in these two parameters. While &lt;code&gt;min_samples_split&lt;/code&gt; can create arbitrarily small leaves, &lt;code&gt;min_samples_leaf&lt;/code&gt; guarantees that each leaf has a minimum size, avoiding low-variance, over-fit leaf nodes in regression problems. For classification with few classes, &lt;code&gt;min_samples_leaf=1&lt;/code&gt; is often the best choice.</source>
          <target state="translated">使用 &lt;code&gt;min_samples_split&lt;/code&gt; 或 &lt;code&gt;min_samples_leaf&lt;/code&gt; 可通过控制要考虑的分割来确保多个样本通知树中的每个决策。数量很少通常意味着树会过拟合，而数量很大将阻止树学习数据。尝试将 &lt;code&gt;min_samples_leaf=5&lt;/code&gt; 用作初始值。如果样本大小变化很大，则可以将浮点数用作这两个参数中的百分比。尽管 &lt;code&gt;min_samples_split&lt;/code&gt; 可以创建任意小的叶子，但 &lt;code&gt;min_samples_leaf&lt;/code&gt; 保证每个叶子都具有最小的大小，从而避免了回归问题中的低方差，过度拟合叶子节点。对于很少分类的分类， &lt;code&gt;min_samples_leaf=1&lt;/code&gt; 通常是最佳选择。</target>
        </trans-unit>
        <trans-unit id="3429b333ce93c8bae91a85fe794f080132090825" translate="yes" xml:space="preserve">
          <source>Use &lt;code&gt;negative_outlier_factor_&lt;/code&gt;</source>
          <target state="translated">使用 &lt;code&gt;negative_outlier_factor_&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="7760fd58346bed0a2b559e055012bd8a21868552" translate="yes" xml:space="preserve">
          <source>Use SelectFromModel meta-transformer along with Lasso to select the best couple of features from the Boston dataset.</source>
          <target state="translated">使用SelectFromModel元变换器与Lasso一起从波士顿数据集中选择最佳的几个特征。</target>
        </trans-unit>
        <trans-unit id="71685d673fcc400f8aa4ec7ef3e4a61bf3bbf5f7" translate="yes" xml:space="preserve">
          <source>Use approximate bound as score.</source>
          <target state="translated">用近似界限作为分数。</target>
        </trans-unit>
        <trans-unit id="400d526bc775314ded26ebb1519045ccc0979588" translate="yes" xml:space="preserve">
          <source>Use density = 1 / 3.0 if you want to reproduce the results from Achlioptas, 2001.</source>
          <target state="translated">如果你想重现Achlioptas,2001年的结果,使用密度=1/3.0。</target>
        </trans-unit>
        <trans-unit id="38fb3c866f165060e0d95ec1a873c702ff2c91dc" translate="yes" xml:space="preserve">
          <source>Use only on new data</source>
          <target state="translated">仅用于新数据</target>
        </trans-unit>
        <trans-unit id="0eb6d7f6360fc3b257840e6d0ece909142d961e3" translate="yes" xml:space="preserve">
          <source>Use splitting criteria that compute the average reduction across all n outputs.</source>
          <target state="translated">使用分割标准,计算所有n个输出的平均减少量。</target>
        </trans-unit>
        <trans-unit id="d4a5711bd46bd2a4542a66497bb7f3342ea23a7f" translate="yes" xml:space="preserve">
          <source>Use the Akaike information criterion (AIC), the Bayes Information criterion (BIC) and cross-validation to select an optimal value of the regularization parameter alpha of the &lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;Lasso&lt;/a&gt; estimator.</source>
          <target state="translated">使用Akaike信息标准（AIC），贝叶斯信息标准（BIC）和交叉验证来选择&lt;a href=&quot;../../modules/linear_model#lasso&quot;&gt;套索&lt;/a&gt;估计器的正则化参数alpha的最佳值。</target>
        </trans-unit>
        <trans-unit id="35f6c244b8fd2da4794beb17214246bc1c30610f" translate="yes" xml:space="preserve">
          <source>Usecase</source>
          <target state="translated">Usecase</target>
        </trans-unit>
        <trans-unit id="2d6d1bb4bf090f9031a078f80f81b5cfa346c5fb" translate="yes" xml:space="preserve">
          <source>Used for internal caching. By default, no caching is done. If a string is given, it is the path to the caching directory.</source>
          <target state="translated">用于内部缓存。默认情况下,不进行缓存。如果给定一个字符串,则是缓存目录的路径。</target>
        </trans-unit>
        <trans-unit id="a659f5ae4cfedf0686976ca2f311fa3c53dbc2ce" translate="yes" xml:space="preserve">
          <source>Used for randomizing the singular value decomposition and the k-means initialization. Use an int to make the randomness deterministic. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">用于随机化奇异值分解和k-means初始化。使用int可以确定随机性。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="04206996c9eca941c8da97d474cf5a147f4b8713" translate="yes" xml:space="preserve">
          <source>Used to cache the fitted transformers of the pipeline. By default, no caching is performed. If a string is given, it is the path to the caching directory. Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly. Use the attribute &lt;code&gt;named_steps&lt;/code&gt; or &lt;code&gt;steps&lt;/code&gt; to inspect estimators within the pipeline. Caching the transformers is advantageous when fitting is time consuming.</source>
          <target state="translated">用于缓存管道的已装配变压器。默认情况下，不执行缓存。如果给出了字符串，则它是缓存目录的路径。启用缓存会在安装之前触发变压器的克隆。因此，不能直接检查提供给管道的变压器实例。使用 &lt;code&gt;named_steps&lt;/code&gt; 或 &lt;code&gt;steps&lt;/code&gt; 属性检查管道中的估计量。当安装耗时时，缓存变压器是有利的。</target>
        </trans-unit>
        <trans-unit id="b831b0ccc618367ad6990c063d4f6b68c21b54a0" translate="yes" xml:space="preserve">
          <source>Used to cache the output of the computation of the tree. By default, no caching is done. If a string is given, it is the path to the caching directory.</source>
          <target state="translated">用于缓存树的计算输出。默认情况下,不进行缓存。如果给出一个字符串,则是缓存目录的路径。</target>
        </trans-unit>
        <trans-unit id="ec2d1021c796c8396406488bdd5b004de6014166" translate="yes" xml:space="preserve">
          <source>Used to specify the norm used in the penalization. The &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo; and &amp;lsquo;lbfgs&amp;rsquo; solvers support only l2 penalties.</source>
          <target state="translated">用于指定惩罚中使用的规范。'newton-cg'，'sag'和'lbfgs'求解器仅支持l2罚分。</target>
        </trans-unit>
        <trans-unit id="5a789f5832dbb34972e6ab11f85f2125e32dddce" translate="yes" xml:space="preserve">
          <source>Useful for applying a non-linear transformation in regression problems. This transformation can be given as a Transformer such as the QuantileTransformer or as a function and its inverse such as &lt;code&gt;log&lt;/code&gt; and &lt;code&gt;exp&lt;/code&gt;.</source>
          <target state="translated">对于在回归问题中应用非线性变换很有用。可以将其转换为Transformer（例如QuantileTransformer）或函数及其逆函数（例如 &lt;code&gt;log&lt;/code&gt; 和 &lt;code&gt;exp&lt;/code&gt; )。</target>
        </trans-unit>
        <trans-unit id="665be16622d5dce4c265443eced33f5309efed0f" translate="yes" xml:space="preserve">
          <source>Useful only for the newton-cg, sag and lbfgs solvers. Maximum number of iterations taken for the solvers to converge.</source>
          <target state="translated">仅对牛顿-CG、下垂和lbfgs求解器有用。求解器收敛的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="1f7081fc6e8837c157dbcac4dfe150624d77daa3" translate="yes" xml:space="preserve">
          <source>Useful only when the solver &amp;lsquo;liblinear&amp;rsquo; is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a &amp;ldquo;synthetic&amp;rdquo; feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes &lt;code&gt;intercept_scaling * synthetic_feature_weight&lt;/code&gt;.</source>
          <target state="translated">仅在使用求解器'liblinear'并将self.fit_intercept设置为True时有用。在这种情况下，x变为[x，self.intercept_scaling]，即，将常量值等于intercept_scaling的&amp;ldquo;合成&amp;rdquo;特征附加到实例矢量。截距将变为 &lt;code&gt;intercept_scaling * synthetic_feature_weight&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="cb4f9242d2c5bef801309a7b11564e9f2917779f" translate="yes" xml:space="preserve">
          <source>Useful tutorials for developing a feel for some of scikit-learn's applications in the machine learning field.</source>
          <target state="translated">有用的教程,用于培养对scikit-learn在机器学习领域的一些应用的感觉。</target>
        </trans-unit>
        <trans-unit id="bec249e659662f7d5947bf09a1ea1d4a552885b0" translate="yes" xml:space="preserve">
          <source>User Guide</source>
          <target state="translated">用户指南</target>
        </trans-unit>
        <trans-unit id="32b4edc9350cb6d93cc0316d635ce26e35fa63d2" translate="yes" xml:space="preserve">
          <source>Uses BLAS GEMM as replacement for numpy.dot where possible to avoid unnecessary copies.</source>
          <target state="translated">尽可能使用BLAS GEMM来替代numpy.dot,以避免不必要的拷贝。</target>
        </trans-unit>
        <trans-unit id="ac4bd4f4f631e790604905794abbd6ed09d66803" translate="yes" xml:space="preserve">
          <source>Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.</source>
          <target state="translated">在决策函数中使用训练点的子集(称为支持向量),所以它也是内存高效的。</target>
        </trans-unit>
        <trans-unit id="4a4b56e0bea50fff706430a1a94289a4e5e03040" translate="yes" xml:space="preserve">
          <source>Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.</source>
          <target state="translated">使用白盒模型。如果给定的情况在模型中是可以观察到的,那么条件的解释很容易用布尔逻辑来解释。相反,在黑箱模型中(例如,在人工神经网络中),结果可能更难解释。</target>
        </trans-unit>
        <trans-unit id="ced5d25baa7aaf39837296d764096d52eb67f5ca" translate="yes" xml:space="preserve">
          <source>Uses sampling the fourier transform of the kernel characteristic at regular intervals.</source>
          <target state="translated">采用定时采样的方式对内核特征进行傅里叶变换。</target>
        </trans-unit>
        <trans-unit id="2c633fc259072170ae02b4cf8b2266258b09ddc5" translate="yes" xml:space="preserve">
          <source>Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform).</source>
          <target state="translated">使用由fit(或fit_transform)学习的词汇和文档频率(df)。</target>
        </trans-unit>
        <trans-unit id="cdea6e25f0fe24b03bf907dbf26253d4f40a11df" translate="yes" xml:space="preserve">
          <source>Using &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; or &lt;code&gt;loss=&quot;modified_huber&quot;&lt;/code&gt; enables the &lt;code&gt;predict_proba&lt;/code&gt; method, which gives a vector of probability estimates \(P(y|x)\) per sample \(x\):</source>
          <target state="translated">使用 &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; 或 &lt;code&gt;loss=&quot;modified_huber&quot;&lt;/code&gt; 启用 &lt;code&gt;predict_proba&lt;/code&gt; 方法，该方法为每个样本\（x \）提供了概率估计向量\（P（y | x）\）：</target>
        </trans-unit>
        <trans-unit id="4072d118d17ebbe341d060ec8f347bb287543668" translate="yes" xml:space="preserve">
          <source>Using FunctionTransformer to select columns</source>
          <target state="translated">使用FunctionTransformer选择列</target>
        </trans-unit>
        <trans-unit id="af570039f4e6335c176e5a0ced20f61ade37ec58" translate="yes" xml:space="preserve">
          <source>Using KBinsDiscretizer to discretize continuous features</source>
          <target state="translated">使用KBinsDiscretizer对连续特征进行离散化处理。</target>
        </trans-unit>
        <trans-unit id="58dd6560cd1dd1ff16a956c31444ffff4530a294" translate="yes" xml:space="preserve">
          <source>Using L1 penalization as provided by &lt;code&gt;LinearSVC(loss='l2', penalty='l1',
dual=False)&lt;/code&gt; yields a sparse solution, i.e. only a subset of feature weights is different from zero and contribute to the decision function. Increasing &lt;code&gt;C&lt;/code&gt; yields a more complex model (more feature are selected). The &lt;code&gt;C&lt;/code&gt; value that yields a &amp;ldquo;null&amp;rdquo; model (all weights equal to zero) can be calculated using &lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt;&lt;code&gt;l1_min_c&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">使用由 &lt;code&gt;LinearSVC(loss='l2', penalty='l1', dual=False)&lt;/code&gt; 提供的L1惩罚（损失=&amp;ldquo; l2&amp;rdquo;，惩罚=&amp;ldquo; l1&amp;rdquo;，对偶=假）会产生稀疏解，即，只有特征权重的子集不同于零并且有助于决策函数。 &lt;code&gt;C&lt;/code&gt; 的增加将产生更复杂的模型（选择了更多功能）。可以使用&lt;a href=&quot;generated/sklearn.svm.l1_min_c#sklearn.svm.l1_min_c&quot;&gt; &lt;code&gt;l1_min_c&lt;/code&gt; &lt;/a&gt;计算得出&amp;ldquo;空&amp;rdquo;模型（所有权重等于零）的 &lt;code&gt;C&lt;/code&gt; 值。</target>
        </trans-unit>
        <trans-unit id="0033ecf2999fee9c8f71baf5ea186698e26a6aa1" translate="yes" xml:space="preserve">
          <source>Using a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; without cache enabled, it is possible to inspect the original instance such as:</source>
          <target state="translated">使用未启用缓存的&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt;，可以检查原始实例，例如：</target>
        </trans-unit>
        <trans-unit id="4b38a6f3576ed137cc4e9a9f59798fb707d4dde4" translate="yes" xml:space="preserve">
          <source>Using a single underlying feature the model learns both the x and y coordinate as output.</source>
          <target state="translated">使用单一的底层特征,模型学习x和y坐标作为输出。</target>
        </trans-unit>
        <trans-unit id="6d5ea28ea8efb863c08e76177dc50acce9324f64" translate="yes" xml:space="preserve">
          <source>Using a small &lt;code&gt;max_features&lt;/code&gt; value can significantly decrease the runtime.</source>
          <target state="translated">使用较小的 &lt;code&gt;max_features&lt;/code&gt; 值可以大大减少运行时间。</target>
        </trans-unit>
        <trans-unit id="03c1a9468c05dba06aec630e70cb3912379c2cb0" translate="yes" xml:space="preserve">
          <source>Using its &lt;code&gt;partial_fit&lt;/code&gt; method on chunks of data fetched sequentially from the local hard drive or a network database.</source>
          <target state="translated">在从本地硬盘驱动器或网络数据库顺序获取的数据块上使用其 &lt;code&gt;partial_fit&lt;/code&gt; 方法。</target>
        </trans-unit>
        <trans-unit id="ee5e8e298a940fdf98e8e52d2f16edd9327907f7" translate="yes" xml:space="preserve">
          <source>Using kernels</source>
          <target state="translated">使用内核</target>
        </trans-unit>
        <trans-unit id="9cfba89ca182507cccdcf4094af12bc5a4c62801" translate="yes" xml:space="preserve">
          <source>Using orthogonal matching pursuit for recovering a sparse signal from a noisy measurement encoded with a dictionary</source>
          <target state="translated">使用正交匹配追求从用字典编码的噪声测量中恢复稀疏信号。</target>
        </trans-unit>
        <trans-unit id="24a0ae926e510d4f01c040899e37f954b1d9b717" translate="yes" xml:space="preserve">
          <source>Using pre_dispatch in a producer/consumer situation, where the data is generated on the fly. Note how the producer is first called 3 times before the parallel loop is initiated, and then called to generate new data on the fly:</source>
          <target state="translated">在生产者/消费者的情况下使用pre_dispatch,数据是在飞行中生成的。请注意,在并行循环启动之前,生产者先被调用3次,然后再被调用以在飞行中生成新数据。</target>
        </trans-unit>
        <trans-unit id="efd1182f39233190c4734b5ce34b9cfcf1bbe0bb" translate="yes" xml:space="preserve">
          <source>Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008.</source>
          <target state="translated">Using t-SNE.Journal of Machine Learning Research 9:2579-2605,2008.</target>
        </trans-unit>
        <trans-unit id="52758be5ab8f6ab028b2bf9ad6db5d2b69896663" translate="yes" xml:space="preserve">
          <source>Using the &lt;code&gt;TfidfTransformer&lt;/code&gt;&amp;rsquo;s default settings, &lt;code&gt;TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)&lt;/code&gt; the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which is computed as</source>
          <target state="translated">使用 &lt;code&gt;TfidfTransformer&lt;/code&gt; 的默认设置， &lt;code&gt;TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)&lt;/code&gt; ，术语频率，即术语在给定文档中出现的次数，乘以idf分量，计算为</target>
        </trans-unit>
        <trans-unit id="95d62a973e97428e1fb3d7b86f3a392143b00b8c" translate="yes" xml:space="preserve">
          <source>Using the GraphicalLasso estimator to learn a covariance and sparse precision from a small number of samples.</source>
          <target state="translated">使用GraphicalLasso估计器从少量样本中学习一个协方差和稀疏精度。</target>
        </trans-unit>
        <trans-unit id="f9e94a3c6c72e38b7c72fe500879db6b6bb31872" translate="yes" xml:space="preserve">
          <source>Using the Iris dataset, we can construct a tree as follows:</source>
          <target state="translated">使用Iris数据集,我们可以构建一棵树,如下所示。</target>
        </trans-unit>
        <trans-unit id="f990799abb31a15673ef02f50b5506399075290a" translate="yes" xml:space="preserve">
          <source>Using the expected value, the adjusted mutual information can then be calculated using a similar form to that of the adjusted Rand index:</source>
          <target state="translated">利用预期值,然后可以用类似于调整后的兰德指数的形式计算调整后的相互信息。</target>
        </trans-unit>
        <trans-unit id="525fdffa79943fda791ea83f2bc571cbb29e45c1" translate="yes" xml:space="preserve">
          <source>Using the naive conditional independence assumption that</source>
          <target state="translated">利用天真的条件独立性假设,即</target>
        </trans-unit>
        <trans-unit id="ded2353583b49d229cc248063161251fcd75da59" translate="yes" xml:space="preserve">
          <source>Using the prediction pipeline in a grid search</source>
          <target state="translated">在网格搜索中使用预测管道。</target>
        </trans-unit>
        <trans-unit id="4b65de55e2dd198ac6e2aecd67614d2f3e1d68d9" translate="yes" xml:space="preserve">
          <source>Using the results of the previous exercises and the &lt;code&gt;cPickle&lt;/code&gt; module of the standard library, write a command line utility that detects the language of some text provided on &lt;code&gt;stdin&lt;/code&gt; and estimate the polarity (positive or negative) if the text is written in English.</source>
          <target state="translated">使用先前练习的结果和标准库的 &lt;code&gt;cPickle&lt;/code&gt; 模块，编写一个命令行实用程序，该程序可检测 &lt;code&gt;stdin&lt;/code&gt; 上提供的某些文本的语言，并估计文本（如果是英语）的极性（正或负）。</target>
        </trans-unit>
        <trans-unit id="271df6087c1c40487d3dd610dcce2afcb5a005f8" translate="yes" xml:space="preserve">
          <source>Using this modification, the tf-idf of the third term in document 1 changes to 1.8473:</source>
          <target state="translated">通过这一修改,文件1中第三个术语的tf-idf变为1.8473。</target>
        </trans-unit>
        <trans-unit id="35033b7b1c0300bd76803da2e755fdbe07a7c28b" translate="yes" xml:space="preserve">
          <source>Utilities from joblib:</source>
          <target state="translated">来自joblib的实用工具。</target>
        </trans-unit>
        <trans-unit id="c9ee5681d3c59f7541c27a38b67edf46259e187b" translate="yes" xml:space="preserve">
          <source>V</source>
          <target state="translated">V</target>
        </trans-unit>
        <trans-unit id="8d1950c14bc870de437b37f806aaa51c635a9ec3" translate="yes" xml:space="preserve">
          <source>V measure</source>
          <target state="translated">V措施</target>
        </trans-unit>
        <trans-unit id="a6ed7787c295565530f8c589d9ab12370f5f5b3d" translate="yes" xml:space="preserve">
          <source>V or VI</source>
          <target state="translated">五或六</target>
        </trans-unit>
        <trans-unit id="e659ac0cd03fda8c2776727471548e055d6ecaa7" translate="yes" xml:space="preserve">
          <source>V-Measure (NMI with arithmetic mean option.)</source>
          <target state="translated">V-Measure(带有算术平均值选项的NMI)。</target>
        </trans-unit>
        <trans-unit id="893c35d89ea6a5c89935fd8eeed462af4410524f" translate="yes" xml:space="preserve">
          <source>V-Measure is furthermore symmetric: swapping &lt;code&gt;labels_true&lt;/code&gt; and &lt;code&gt;label_pred&lt;/code&gt; will give the same score. This does not hold for homogeneity and completeness. V-Measure is identical to &lt;a href=&quot;sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt;&lt;code&gt;normalized_mutual_info_score&lt;/code&gt;&lt;/a&gt; with the arithmetic averaging method.</source>
          <target state="translated">V-测量是对称此外：交换 &lt;code&gt;labels_true&lt;/code&gt; 和 &lt;code&gt;label_pred&lt;/code&gt; 将给出相同的分数。这不适用于同质性和完整性。使用算术平均方法，V度量与&lt;a href=&quot;sklearn.metrics.normalized_mutual_info_score#sklearn.metrics.normalized_mutual_info_score&quot;&gt; &lt;code&gt;normalized_mutual_info_score&lt;/code&gt; &lt;/a&gt;相同。</target>
        </trans-unit>
        <trans-unit id="47faeee4990a814efec079e593cccd036ad6778a" translate="yes" xml:space="preserve">
          <source>V-measure cluster labeling given a ground truth.</source>
          <target state="translated">在给定地真值的情况下,进行V度量簇标签。</target>
        </trans-unit>
        <trans-unit id="a4fd517acce42be80ab9791260ae88f5cbdf1a52" translate="yes" xml:space="preserve">
          <source>V. Metsis, I. Androutsopoulos and G. Paliouras (2006). &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.61.5542&quot;&gt;Spam filtering with Naive Bayes &amp;ndash; Which Naive Bayes?&lt;/a&gt; 3rd Conf. on Email and Anti-Spam (CEAS).</source>
          <target state="translated">V. Metsis，I。Androutsopoulos和G. Paliouras（2006）。&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.61.5542&quot;&gt;使用朴素贝叶斯过滤垃圾邮件&amp;ndash;哪些朴素贝叶斯？&lt;/a&gt;第三届会议 在电子邮件和反垃圾邮件（CEAS）上。</target>
        </trans-unit>
        <trans-unit id="942786415758a60976a047b84669d53dbc12ecc2" translate="yes" xml:space="preserve">
          <source>V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with naive Bayes &amp;ndash; Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).</source>
          <target state="translated">V. Metsis，I。Androutsopoulos和G. Paliouras（2006）。使用朴素贝叶斯过滤垃圾邮件&amp;ndash;哪些朴素贝叶斯？第三届会议 在电子邮件和反垃圾邮件（CEAS）上。</target>
        </trans-unit>
        <trans-unit id="ca4c1a38268237e7698432accf1a9a5a48b6fb08" translate="yes" xml:space="preserve">
          <source>Valid metrics for pairwise_distances.</source>
          <target state="translated">pairwise_distances的有效指标。</target>
        </trans-unit>
        <trans-unit id="26700c8a25eea6fd902a0bb4963f14783c982650" translate="yes" xml:space="preserve">
          <source>Valid metrics for pairwise_kernels</source>
          <target state="translated">pairwise_kernels的有效指标</target>
        </trans-unit>
        <trans-unit id="850c962c3f063b586578621ee12bbb84d6f38bb7" translate="yes" xml:space="preserve">
          <source>Valid options:</source>
          <target state="translated">有效选项:</target>
        </trans-unit>
        <trans-unit id="493108de26f61e3b76acfe363b14fa409e1fdc9a" translate="yes" xml:space="preserve">
          <source>Valid parameter keys can be listed with &lt;code&gt;get_params()&lt;/code&gt;.</source>
          <target state="translated">有效的参数键可以使用 &lt;code&gt;get_params()&lt;/code&gt; 列出。</target>
        </trans-unit>
        <trans-unit id="1a791ec45bdf21e7b9592679ed5472b3c5ab8093" translate="yes" xml:space="preserve">
          <source>Valid parameter keys can be listed with get_params().</source>
          <target state="translated">有效的参数键可以用get_params()列出。</target>
        </trans-unit>
        <trans-unit id="90d9fefcb561047bbbd742b13c3608c6fcf1e657" translate="yes" xml:space="preserve">
          <source>Valid values for metric are:</source>
          <target state="translated">度量的有效值是:</target>
        </trans-unit>
        <trans-unit id="47edea5ff3c24dcb16dc15647447ec5c4a9d77c2" translate="yes" xml:space="preserve">
          <source>Valid values for metric are::</source>
          <target state="translated">度量的有效值为:。</target>
        </trans-unit>
        <trans-unit id="ec59a2a93f21b1aa3fbc16cd26b3b2dbab6fa78b" translate="yes" xml:space="preserve">
          <source>Validation curve.</source>
          <target state="translated">验证曲线。</target>
        </trans-unit>
        <trans-unit id="675b8482a7f9f38fa965a1efe10c6a6ee6ec5cdb" translate="yes" xml:space="preserve">
          <source>Value added to the diagonal of the kernel matrix during fitting. Larger values correspond to increased noise level in the observations. This can also prevent a potential numerical issue during fitting, by ensuring that the calculated values form a positive definite matrix. If an array is passed, it must have the same number of entries as the data used for fitting and is used as datapoint-dependent noise level. Note that this is equivalent to adding a WhiteKernel with c=alpha. Allowing to specify the noise level directly as a parameter is mainly for convenience and for consistency with Ridge.</source>
          <target state="translated">拟合过程中添加到内核矩阵对角线上的值。数值越大,说明观测值的噪声越大。这也可以防止在拟合过程中出现潜在的数字问题,确保计算值形成一个正定矩阵。如果传递了一个数组,它的条目数必须与用于拟合的数据相同,并作为数据点相关的噪声水平。注意,这相当于添加了一个c=alpha的WhiteKernel。允许直接指定噪声水平作为参数,主要是为了方便和与Ridge保持一致。</target>
        </trans-unit>
        <trans-unit id="cdffc29f88adeffd4bcff100fb7aa53a66956d52" translate="yes" xml:space="preserve">
          <source>Value for numerical stability in adam. Only used when solver=&amp;rsquo;adam&amp;rsquo;</source>
          <target state="translated">亚当数值稳定性的值。仅在solver ='adam'时使用</target>
        </trans-unit>
        <trans-unit id="c6350d6ef6528ee88ef12a12465bebefd1891dd8" translate="yes" xml:space="preserve">
          <source>Value of the pseudo-likelihood (proxy for likelihood).</source>
          <target state="translated">伪可能性的值(可能性的代理)。</target>
        </trans-unit>
        <trans-unit id="7c9f9f5dcfc8aebd4eea110198ededa32c4b1278" translate="yes" xml:space="preserve">
          <source>Value to assign to the score if an error occurs in estimator fitting. If set to &amp;lsquo;raise&amp;rsquo;, the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. Default is &amp;lsquo;raise&amp;rsquo; but from version 0.22 it will change to np.nan.</source>
          <target state="translated">如果估算器拟合出现错误，则分配给分数的值。如果设置为&amp;ldquo; raise&amp;rdquo;，则会引发错误。如果给出数值，则引发FitFailedWarning。此参数不会影响重新安装步骤，这将始终引发错误。默认值为&amp;ldquo; raise&amp;rdquo;，但从0.22版开始，它将更改为np.nan。</target>
        </trans-unit>
        <trans-unit id="51fe27463b1bbffac6c175b98fff3a09a8ef007a" translate="yes" xml:space="preserve">
          <source>Value to assign to the score if an error occurs in estimator fitting. If set to &amp;lsquo;raise&amp;rsquo;, the error is raised. If set to &amp;lsquo;raise-deprecating&amp;rsquo;, a FutureWarning is printed before the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. Default is &amp;lsquo;raise-deprecating&amp;rsquo; but from version 0.22 it will change to np.nan.</source>
          <target state="translated">如果估算器拟合出现错误，则分配给分数的值。如果设置为&amp;ldquo; raise&amp;rdquo;，则会引发错误。如果设置为'raise-preprecating'，则会在出现错误之前打印出FutureWarning。如果给出数值，则引发FitFailedWarning。此参数不会影响重新安装步骤，这将始终引发错误。默认值为&amp;ldquo;不赞成使用&amp;rdquo;，但从0.22版开始，它将更改为np.nan。</target>
        </trans-unit>
        <trans-unit id="402a4cfb84a22d3670431b1576518e6587de93b8" translate="yes" xml:space="preserve">
          <source>Value to use for the dummy feature.</source>
          <target state="translated">假人功能使用的值。</target>
        </trans-unit>
        <trans-unit id="82321dd8f607145fb8d2875c3e367d82d45dfc71" translate="yes" xml:space="preserve">
          <source>Value with which negative labels must be encoded.</source>
          <target state="translated">必须对负标签进行编码的值。</target>
        </trans-unit>
        <trans-unit id="4e0758fceaa4f106e89501aa7c196eea7d1ad1c2" translate="yes" xml:space="preserve">
          <source>Value with which positive labels must be encoded.</source>
          <target state="translated">必须对正标签进行编码的值。</target>
        </trans-unit>
        <trans-unit id="6a7ed2e67e56dace630120ac5c7bd01e4d032523" translate="yes" xml:space="preserve">
          <source>Values greater than the threshold map to 1, while values less than or equal to the threshold map to 0. With the default threshold of 0, only positive values map to 1.</source>
          <target state="translated">大于阈值的值映射为1,而小于或等于阈值的值映射为0,默认阈值为0时,只有正值映射为1。</target>
        </trans-unit>
        <trans-unit id="0a659f48fb09b2c2dd949774cc3bd6b7ffd6fd87" translate="yes" xml:space="preserve">
          <source>Values in each bin have the same nearest center of a 1D k-means cluster.</source>
          <target state="translated">每个bin中的数值都有相同的1D k-means簇的最近中心。</target>
        </trans-unit>
        <trans-unit id="c67d763b94a97115ba7ee9d49115a272cb508cf6" translate="yes" xml:space="preserve">
          <source>Values of n_samples samples drawn from Gaussian process and evaluated at query points.</source>
          <target state="translated">从高斯过程中抽取的n_samples样本的值,并在查询点进行评估。</target>
        </trans-unit>
        <trans-unit id="1cd1b62dfd3b6a63572d1bf631789bd088451d1d" translate="yes" xml:space="preserve">
          <source>Values of the visible layer after one Gibbs step.</source>
          <target state="translated">一个吉布斯步骤后可见层的数值。</target>
        </trans-unit>
        <trans-unit id="c9500aef779ad4ab4355590ca05b9c0de0aa083a" translate="yes" xml:space="preserve">
          <source>Values of the visible layer to start from.</source>
          <target state="translated">可见层的值要从哪里开始。</target>
        </trans-unit>
        <trans-unit id="d9ca5115511a5b00d878e1de5b9daa8f530b632c" translate="yes" xml:space="preserve">
          <source>Values of the visible layer. Must be all-boolean (not checked).</source>
          <target state="translated">可见层的数值。必须是全布尔值(不勾选)。</target>
        </trans-unit>
        <trans-unit id="445d09e482944669dc8a6e893591f45bda28254b" translate="yes" xml:space="preserve">
          <source>Vanschoren, van Rijn, Bischl and Torgo &lt;a href=&quot;https://arxiv.org/pdf/1407.7722.pdf&quot;&gt;&amp;ldquo;OpenML: networked science in machine learning&amp;rdquo;&lt;/a&gt;, ACM SIGKDD Explorations Newsletter, 15(2), 49-60, 2014.</source>
          <target state="translated">Vanschoren，van Rijn，Bischl和Torgo，&lt;a href=&quot;https://arxiv.org/pdf/1407.7722.pdf&quot;&gt;&amp;ldquo;&lt;/a&gt; OpenML ：机器学习中的网络科学&amp;rdquo;，ACM SIGKDD Explorations Newsletter，15（2），49-60，2014年。</target>
        </trans-unit>
        <trans-unit id="ba47c39bbafea14cc75438e6420272a547d1a3e3" translate="yes" xml:space="preserve">
          <source>Variance explained by each of the selected components.</source>
          <target state="translated">每个选定的组成部分所解释的差异。</target>
        </trans-unit>
        <trans-unit id="17b5cb397e6ad9830d59b5fc66bebb45024c7ef4" translate="yes" xml:space="preserve">
          <source>Variances of individual features.</source>
          <target state="translated">各个特征的差异。</target>
        </trans-unit>
        <trans-unit id="7933f72bf76c6cfbc1dde87498522f5e833878e6" translate="yes" xml:space="preserve">
          <source>Variational Bayesian estimation of a Gaussian mixture.</source>
          <target state="translated">高斯混合物的变异贝叶斯估计。</target>
        </trans-unit>
        <trans-unit id="e459652719d6e0edf38bb3c9f14dba040888c62f" translate="yes" xml:space="preserve">
          <source>Variational inference is an extension of expectation-maximization that maximizes a lower bound on model evidence (including priors) instead of data likelihood. The principle behind variational methods is the same as expectation-maximization (that is both are iterative algorithms that alternate between finding the probabilities for each point to be generated by each mixture and fitting the mixture to these assigned points), but variational methods add regularization by integrating information from prior distributions. This avoids the singularities often found in expectation-maximization solutions but introduces some subtle biases to the model. Inference is often notably slower, but not usually as much so as to render usage unpractical.</source>
          <target state="translated">变异推理是期望最大化的延伸,它最大化了模型证据(包括前值)的下限,而不是数据似然。变分方法的原理与期望最大化相同(即两者都是在寻找每个混合物产生的每个点的概率和将混合物拟合到这些分配点之间交替进行的迭代算法),但变分方法通过整合来自先验分布的信息来增加正则化。这避免了期望最大化解决方案中经常发现的奇异性,但会给模型引入一些微妙的偏差。推断通常会明显变慢,但通常不至于使使用不切实际。</target>
        </trans-unit>
        <trans-unit id="009e019794c4b5a288d65b8e0eabe9064e298c81" translate="yes" xml:space="preserve">
          <source>Variational inference techniques for the Dirichlet process still work with a finite approximation to this infinite mixture model, but instead of having to specify a priori how many components one wants to use, one just specifies the concentration parameter and an upper bound on the number of mixture components (this upper bound, assuming it is higher than the &amp;ldquo;true&amp;rdquo; number of components, affects only algorithmic complexity, not the actual number of components used).</source>
          <target state="translated">Dirichlet过程的变分推理技术仍可在此无限混合模型的有限近似下使用，但不必先验地指定一个人要使用多少组分，只需指定浓度参数和混合物数量的上限即可组件（假设该上限高于&amp;ldquo;真实&amp;rdquo;的组件数量，则仅影响算法复杂度，而不影响实际使用的组件数量）。</target>
        </trans-unit>
        <trans-unit id="bfc42eb1bb86ce5f62b086e9ffd3c67a080b0730" translate="yes" xml:space="preserve">
          <source>Variational parameters for topic word distribution. Since the complete conditional for topic word distribution is a Dirichlet, &lt;code&gt;components_[i, j]&lt;/code&gt; can be viewed as pseudocount that represents the number of times word &lt;code&gt;j&lt;/code&gt; was assigned to topic &lt;code&gt;i&lt;/code&gt;. It can also be viewed as distribution over the words for each topic after normalization: &lt;code&gt;model.components_ / model.components_.sum(axis=1)[:, np.newaxis]&lt;/code&gt;.</source>
          <target state="translated">主题词分布的变体参数。由于主题词分配的完整条件是Dirichlet， &lt;code&gt;components_[i, j]&lt;/code&gt; 可以看作是伪计数，它表示将词 &lt;code&gt;j&lt;/code&gt; 分配给主题 &lt;code&gt;i&lt;/code&gt; 的次数。归一化后，也可以将其视为每个主题的单词分布： &lt;code&gt;model.components_ / model.components_.sum(axis=1)[:, np.newaxis]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="65cb8d21f19957f8f56d7ec3aeccadf0c1efbc6f" translate="yes" xml:space="preserve">
          <source>Various Agglomerative Clustering on a 2D embedding of digits</source>
          <target state="translated">在数字的二维嵌入上进行各种聚类分析</target>
        </trans-unit>
        <trans-unit id="b1e276370580ceeca94d9a25d3b75abf89a69938" translate="yes" xml:space="preserve">
          <source>Varying regularization in Multi-layer Perceptron</source>
          <target state="translated">多层感知器的可变正则化</target>
        </trans-unit>
        <trans-unit id="1e178759402bc070ae202c1ae1b1666da0dd6a9c" translate="yes" xml:space="preserve">
          <source>Vector Quantization Example</source>
          <target state="translated">矢量量化实例</target>
        </trans-unit>
        <trans-unit id="ba8b3829eecac2c5ad85a64a161b0e7f2fae54cf" translate="yes" xml:space="preserve">
          <source>Vector of errors at each iteration.</source>
          <target state="translated">每次迭代的误差向量。</target>
        </trans-unit>
        <trans-unit id="93c6f0903309fd539ceb7d4710cf07f7db8cb1d8" translate="yes" xml:space="preserve">
          <source>Verbose mode when fitting the model.</source>
          <target state="translated">拟合模型时采用Verbose模式。</target>
        </trans-unit>
        <trans-unit id="eeb343db47ab9124ef417c41d9bdb92839772dc0" translate="yes" xml:space="preserve">
          <source>Verbose output during PD computations. Defaults to 0.</source>
          <target state="translated">在PD计算中的详细输出。默认值为0。</target>
        </trans-unit>
        <trans-unit id="65d3f9d357c8217e4b4161cc31c9983e755e9511" translate="yes" xml:space="preserve">
          <source>Verbosity flag, controls the debug messages that are issued as functions are evaluated.</source>
          <target state="translated">Verbosity标志,控制函数执行时发出的调试信息。</target>
        </trans-unit>
        <trans-unit id="a606460a9db19f2456900341e545d542d386dcd0" translate="yes" xml:space="preserve">
          <source>Verbosity level.</source>
          <target state="translated">啰嗦程度。</target>
        </trans-unit>
        <trans-unit id="9cadb350a887ea26b759cec53fba259b94be0b70" translate="yes" xml:space="preserve">
          <source>Verbosity level. Setting verbose &amp;gt; 0 will display additional information depending on the solver used.</source>
          <target state="translated">详细程度。设置详细&amp;gt; 0将显示其他信息，具体取决于所使用的求解器。</target>
        </trans-unit>
        <trans-unit id="c09635f4883cd7798a06597eead68509e08bef52" translate="yes" xml:space="preserve">
          <source>Verbosity mode.</source>
          <target state="translated">啰嗦模式。</target>
        </trans-unit>
        <trans-unit id="4ee9c427e0cc678ca91bc7294708dc43db03512b" translate="yes" xml:space="preserve">
          <source>Versatile: different &lt;a href=&quot;#gp-kernels&quot;&gt;kernels&lt;/a&gt; can be specified. Common kernels are provided, but it is also possible to specify custom kernels.</source>
          <target state="translated">多功能：可以指定不同的&lt;a href=&quot;#gp-kernels&quot;&gt;内核&lt;/a&gt;。提供了通用内核，但是也可以指定自定义内核。</target>
        </trans-unit>
        <trans-unit id="4b16fac84e3102257e26d97dc6bcc2775a76c275" translate="yes" xml:space="preserve">
          <source>Versatile: different &lt;a href=&quot;#svm-kernels&quot;&gt;Kernel functions&lt;/a&gt; can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.</source>
          <target state="translated">多功能：可以为决策功能指定不同的&lt;a href=&quot;#svm-kernels&quot;&gt;内核&lt;/a&gt;功能。提供了通用内核，但是也可以指定自定义内核。</target>
        </trans-unit>
        <trans-unit id="102caab3d934ebf62d9240e41edbdfb96ec830ff" translate="yes" xml:space="preserve">
          <source>Version of the dataset. Can only be provided if also &lt;code&gt;name&lt;/code&gt; is given. If &amp;lsquo;active&amp;rsquo; the oldest version that&amp;rsquo;s still active is used. Since there may be more than one active version of a dataset, and those versions may fundamentally be different from one another, setting an exact version is highly recommended.</source>
          <target state="translated">数据集的版本。仅当提供 &lt;code&gt;name&lt;/code&gt; 才能提供。如果&amp;ldquo;活动&amp;rdquo;，则使用仍在活动的最旧版本。由于一个数据集可能有多个活动版本，并且这些版本在根本上可能彼此不同，因此强烈建议设置一个确切的版本。</target>
        </trans-unit>
        <trans-unit id="2d0ab9e3d9a896817cdfc684c77fbf9f0c4f1aac" translate="yes" xml:space="preserve">
          <source>Version: RCV1-v2, vectors, full sets, topics multilabels.</source>
          <target state="translated">版本。RCV1-v2,向量,全集,主题多标签。</target>
        </trans-unit>
        <trans-unit id="3496b8ff284f2499d5b89bafe815a9579d5a779b" translate="yes" xml:space="preserve">
          <source>Very large &lt;code&gt;n_samples&lt;/code&gt;, medium &lt;code&gt;n_clusters&lt;/code&gt;</source>
          <target state="translated">非常大的 &lt;code&gt;n_samples&lt;/code&gt; ，中 &lt;code&gt;n_clusters&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="90edfe52a065a045edbdd25bf2f2316a234658ac" translate="yes" xml:space="preserve">
          <source>Very large &lt;code&gt;n_samples&lt;/code&gt;, medium &lt;code&gt;n_clusters&lt;/code&gt; with &lt;a href=&quot;#mini-batch-kmeans&quot;&gt;MiniBatch code&lt;/a&gt;</source>
          <target state="translated">非常大的 &lt;code&gt;n_samples&lt;/code&gt; ，中 &lt;code&gt;n_clusters&lt;/code&gt; 与&lt;a href=&quot;#mini-batch-kmeans&quot;&gt;MiniBatch代码&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="56b71e89fb1079caaadefd0889e9a22e8b0560e3" translate="yes" xml:space="preserve">
          <source>Videos</source>
          <target state="translated">Videos</target>
        </trans-unit>
        <trans-unit id="f74a22805e87ed187a5bf75da0c74d88f9fc4e05" translate="yes" xml:space="preserve">
          <source>Vinh et al. (2010) named variants of NMI and AMI by their averaging method [VEB2010]. Their &amp;lsquo;sqrt&amp;rsquo; and &amp;lsquo;sum&amp;rsquo; averages are the geometric and arithmetic means; we use these more broadly common names.</source>
          <target state="translated">Vinh等。（2010年）通过平均方法命名了NMI和AMI的变体[VEB2010]。他们的&amp;ldquo; sqrt&amp;rdquo;和&amp;ldquo; sum&amp;rdquo;平均值是几何和算术平均值。我们使用这些更广泛的通用名称。</target>
        </trans-unit>
        <trans-unit id="40fdfddfc4699e8e891d021e6ce4e4252243f9c7" translate="yes" xml:space="preserve">
          <source>Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance, JMLR</source>
          <target state="translated">Vinh,Epps,and Bailey,(2010)。信息理论措施的聚类比较。Variants,Properties,Normalization and Correction for Chance,JMLR。</target>
        </trans-unit>
        <trans-unit id="64eeb8915eff8290b0627c8aa96dd46ee4bda6f1" translate="yes" xml:space="preserve">
          <source>Visualise your tree as you are training by using the &lt;code&gt;export&lt;/code&gt; function. Use &lt;code&gt;max_depth=3&lt;/code&gt; as an initial tree depth to get a feel for how the tree is fitting to your data, and then increase the depth.</source>
          <target state="translated">使用 &lt;code&gt;export&lt;/code&gt; 功能在训练时可视化您的树。使用 &lt;code&gt;max_depth=3&lt;/code&gt; 作为初始树的深度，以了解树如何适合您的数据，然后增加深度。</target>
        </trans-unit>
        <trans-unit id="d175985b87dd9f620aa960059c730b4a35e3bcb5" translate="yes" xml:space="preserve">
          <source>Visualization</source>
          <target state="translated">Visualization</target>
        </trans-unit>
        <trans-unit id="38623835e09f3cd243cdf966707dad9de4ecfeda" translate="yes" xml:space="preserve">
          <source>Visualization of MLP weights on MNIST</source>
          <target state="translated">MNIST上MLP权重的可视化。</target>
        </trans-unit>
        <trans-unit id="a291ff40a157c9afd67fb01fd3c6b6d368d78978" translate="yes" xml:space="preserve">
          <source>Visualization of predictions obtained from different models.</source>
          <target state="translated">从不同模型获得的预测的可视化。</target>
        </trans-unit>
        <trans-unit id="a6c65fa336dc53edc04e670583358fb288a99997" translate="yes" xml:space="preserve">
          <source>Visualize cross-validation indices for many CV objects</source>
          <target state="translated">可视化许多CV对象的交叉验证指数。</target>
        </trans-unit>
        <trans-unit id="4a2bddc914855cb9100c33fc5e346e89e1926136" translate="yes" xml:space="preserve">
          <source>Visualize our data</source>
          <target state="translated">将我们的数据可视化</target>
        </trans-unit>
        <trans-unit id="28ba2de8813583360eb0588f62cb6dc19a1f4072" translate="yes" xml:space="preserve">
          <source>Visualize the resulting regions</source>
          <target state="translated">视觉化的结果区域</target>
        </trans-unit>
        <trans-unit id="b77da1f257213eacf5971ec98d2f34c8fe910374" translate="yes" xml:space="preserve">
          <source>Visualizing cross-validation behavior in scikit-learn</source>
          <target state="translated">在scikit-learn中可视化交叉验证行为。</target>
        </trans-unit>
        <trans-unit id="e6614e53d3137ea4329f7b88a52f014060c402bb" translate="yes" xml:space="preserve">
          <source>Visualizing the stock market structure</source>
          <target state="translated">视觉化的股票市场结构</target>
        </trans-unit>
        <trans-unit id="3e1e0ef10e7a115946f530e934380438421c5b34" translate="yes" xml:space="preserve">
          <source>Vocabulary: classification and regression</source>
          <target state="translated">词汇:分类和回归</target>
        </trans-unit>
        <trans-unit id="dd7b37acd93acaf11b82a9ebbc3eea819b85e0ef" translate="yes" xml:space="preserve">
          <source>W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) 163-171.</source>
          <target state="translated">W.H.Wolberg,W.N.Street,and O.L.Mangasarian.从细针吸液中诊断乳腺癌的机器学习技术。Cancer Letters 77(1994)163-171.</target>
        </trans-unit>
        <trans-unit id="985d7c09beb3a8622b6c063b009de9296fdb89ed" translate="yes" xml:space="preserve">
          <source>W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis. IS&amp;amp;T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San Jose, CA, 1993.</source>
          <target state="translated">WN街，WH Wolberg和OL Mangasarian。核特征提取用于乳腺肿瘤诊断。IS＆T / SPIE 1993年国际电子成像研讨会：科学和技术，第1905卷，第861-870页，加利福尼亚州圣何塞，1993年。</target>
        </trans-unit>
        <trans-unit id="0525374f4c7331dc5d256feba32a265d327160ed" translate="yes" xml:space="preserve">
          <source>WDBC-Benign</source>
          <target state="translated">WDBC-Benign</target>
        </trans-unit>
        <trans-unit id="fd630df285b07b6076d3b38d88445f6031d3902e" translate="yes" xml:space="preserve">
          <source>WDBC-Malignant</source>
          <target state="translated">WDBC-Malignant</target>
        </trans-unit>
        <trans-unit id="9b852a8108b3e892136da9e7da9f0a5bb56540ea" translate="yes" xml:space="preserve">
          <source>WMinkowskiDistance</source>
          <target state="translated">WMinkowskiDistance</target>
        </trans-unit>
        <trans-unit id="4e8ee595c7db5dd5f284f8fb603cc66c6c8287ae" translate="yes" xml:space="preserve">
          <source>Ward clustering based on a Feature matrix.</source>
          <target state="translated">基于特征矩阵的病房聚类。</target>
        </trans-unit>
        <trans-unit id="1af684513cf70467c9307765e01677f8970cff6e" translate="yes" xml:space="preserve">
          <source>Ward hierarchical clustering</source>
          <target state="translated">病房分层聚类</target>
        </trans-unit>
        <trans-unit id="d2173ac976f5809b703436c0de33dab1598b674c" translate="yes" xml:space="preserve">
          <source>Ward is the most effective method for noisy data.</source>
          <target state="translated">Ward是对噪声数据最有效的方法。</target>
        </trans-unit>
        <trans-unit id="e9c45563358e813f157ba81b33143542165ba84e" translate="yes" xml:space="preserve">
          <source>Warning</source>
          <target state="translated">Warning</target>
        </trans-unit>
        <trans-unit id="44d79cfceaac3d6c963709a9f443a7578dfdd3fa" translate="yes" xml:space="preserve">
          <source>Warning class used if there is an error while fitting the estimator.</source>
          <target state="translated">拟合估计器时有错误时使用的警告类。</target>
        </trans-unit>
        <trans-unit id="c56f46b7e83e71f0bcaf54dacd8cfc15c71ef274" translate="yes" xml:space="preserve">
          <source>Warning class used to notify the user of any change in the behavior.</source>
          <target state="translated">警告类用于通知用户行为的任何变化。</target>
        </trans-unit>
        <trans-unit id="43a2ad1c4144ef567b3006486da55db4df5bcce7" translate="yes" xml:space="preserve">
          <source>Warning used to notify implicit data conversions happening in the code.</source>
          <target state="translated">用于通知代码中发生的隐式数据转换的警告。</target>
        </trans-unit>
        <trans-unit id="69bb37448a64e3ca58327c210b042b57b19259a7" translate="yes" xml:space="preserve">
          <source>Warning used to notify the user of inefficient computation.</source>
          <target state="translated">用于通知用户计算效率低下的警告。</target>
        </trans-unit>
        <trans-unit id="95fde5bcc048210bdd2da0e9628c10dadee1ce1e" translate="yes" xml:space="preserve">
          <source>Warning used when the dot operation does not use BLAS.</source>
          <target state="translated">当点运算不使用BLAS时使用的警告。</target>
        </trans-unit>
        <trans-unit id="77d4a9d6a0a46436c153d66828a62d378a7e1f5d" translate="yes" xml:space="preserve">
          <source>Warning used when the metric is invalid</source>
          <target state="translated">当度量无效时使用的警告</target>
        </trans-unit>
        <trans-unit id="d0aaba5d13d0d7235d440530a46ccfa6343b56ff" translate="yes" xml:space="preserve">
          <source>Warning: Extra-trees should only be used within ensemble methods.</source>
          <target state="translated">警告:Extra-trees只能在集合方法中使用。额外树只能在集合方法中使用。</target>
        </trans-unit>
        <trans-unit id="c2eb6fdf9d13ab34884681ef0c66f41e16b52aad" translate="yes" xml:space="preserve">
          <source>Warning: this function is experimental and subject to change in a future version of joblib.</source>
          <target state="translated">警告:这个功能是实验性的,在未来的joblib版本中可能会改变。</target>
        </trans-unit>
        <trans-unit id="b33d3bb4e4bfe5e80c2407f4ead429c7fa466ef0" translate="yes" xml:space="preserve">
          <source>We achieved 83.5% accuracy. Let&amp;rsquo;s see if we can do better with a linear &lt;a href=&quot;../../modules/svm#svm&quot;&gt;support vector machine (SVM)&lt;/a&gt;, which is widely regarded as one of the best text classification algorithms (although it&amp;rsquo;s also a bit slower than na&amp;iuml;ve Bayes). We can change the learner by simply plugging a different classifier object into our pipeline:</source>
          <target state="translated">我们达到了83.5％的准确性。让我们看看是否可以使用线性&lt;a href=&quot;../../modules/svm#svm&quot;&gt;支持向量机（SVM）&lt;/a&gt;来做得更好，线性支持向量机被广泛认为是最好的文本分类算法之一（尽管它比朴素的贝叶斯算法还慢一些）。我们可以通过简单地将另一个分类器对象插入管道来更改学习者：</target>
        </trans-unit>
        <trans-unit id="5e4234559a6f8eb7829d45ed1528d4d2b014e858" translate="yes" xml:space="preserve">
          <source>We achieved 91.3% accuracy using the SVM. &lt;code&gt;scikit-learn&lt;/code&gt; provides further utilities for more detailed performance analysis of the results:</source>
          <target state="translated">使用SVM，我们达到了91.3％的准确性。 &lt;code&gt;scikit-learn&lt;/code&gt; 提供了更多实用程序来对结果进行更详细的性能分析：</target>
        </trans-unit>
        <trans-unit id="4073bf855ec3741a8d17116f66549a3127ee1b52" translate="yes" xml:space="preserve">
          <source>We add observation noise to these waveforms. We generate very sparse noise: only 6% of the time points contain noise. As a result, the l1 norm of this noise (ie &amp;ldquo;cityblock&amp;rdquo; distance) is much smaller than it&amp;rsquo;s l2 norm (&amp;ldquo;euclidean&amp;rdquo; distance). This can be seen on the inter-class distance matrices: the values on the diagonal, that characterize the spread of the class, are much bigger for the Euclidean distance than for the cityblock distance.</source>
          <target state="translated">我们将观察噪声添加到这些波形中。我们会产生非常稀疏的噪声：只有6％的时间点包含噪声。结果，此噪声的l1范数（即&amp;ldquo;街区&amp;rdquo;距离）比其l2范数（&amp;ldquo;欧几里得&amp;rdquo;距离）小得多。这可以从类间距离矩阵中看出：代表欧几里得距离的对角线上的值（代表类的扩展），比城市街区距离要大得多。</target>
        </trans-unit>
        <trans-unit id="5fc281c3e8b120a8bce90f392d5367731bdfa8e6" translate="yes" xml:space="preserve">
          <source>We also plot predictions and uncertainties for ARD for one dimensional regression using polynomial feature expansion. Note the uncertainty starts going up on the right side of the plot. This is because these test samples are outside of the range of the training samples.</source>
          <target state="translated">我们还绘制了使用多项式特征扩展的一维回归的ARD的预测和不确定性。注意不确定性在图的右侧开始上升。这是因为这些测试样本在训练样本的范围之外。</target>
        </trans-unit>
        <trans-unit id="bda5a1ba58eab4f9acd36ef763104051f247918e" translate="yes" xml:space="preserve">
          <source>We also plot predictions and uncertainties for Bayesian Ridge Regression for one dimensional regression using polynomial feature expansion. Note the uncertainty starts going up on the right side of the plot. This is because these test samples are outside of the range of the training samples.</source>
          <target state="translated">我们还绘制了使用多项式特征扩展的一维回归的贝叶斯岭回归的预测和不确定性。注意不确定性在图的右侧开始上升。这是因为这些测试样本在训练样本的范围之外。</target>
        </trans-unit>
        <trans-unit id="b29f4bff482ccd99c1af96e146d78b516ea318aa" translate="yes" xml:space="preserve">
          <source>We also use warm_start=True which means that the coefficients of the models are reused to initialize the next model fit to speed-up the computation of the full-path.</source>
          <target state="translated">我们还使用了warm_start=True,这意味着模型的系数会被重新用于初始化下一个模型的拟合,以加快全路径的计算速度。</target>
        </trans-unit>
        <trans-unit id="0f24896ee5efd7713cddd30b90821fa31665d4c0" translate="yes" xml:space="preserve">
          <source>We assume that the observations are independent and identically distributed (i.i.d.).</source>
          <target state="translated">我们假设观测值是独立和同分布的(i.i.d.)。</target>
        </trans-unit>
        <trans-unit id="b19a456f973e90e2b4e21cdba9f00b15173fd0ef" translate="yes" xml:space="preserve">
          <source>We call &lt;strong&gt;vectorization&lt;/strong&gt; the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the &lt;strong&gt;Bag of Words&lt;/strong&gt; or &amp;ldquo;Bag of n-grams&amp;rdquo; representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.</source>
          <target state="translated">我们&lt;strong&gt;将向量化&lt;/strong&gt;称为将文本文档集合转换为数字特征向量的一般过程。这种特定的策略（标记化，计数和归一化）称为&amp;ldquo; &lt;strong&gt;词袋&amp;rdquo;&lt;/strong&gt;或&amp;ldquo; n &lt;strong&gt;字词&lt;/strong&gt;袋&amp;rdquo;表示。通过单词出现来描述文档，而完全忽略文档中单词的相对位置信息。</target>
        </trans-unit>
        <trans-unit id="096029ae48dbb08bf7baa8066a510ed0e5cf55ab" translate="yes" xml:space="preserve">
          <source>We can also predict based on an unfitted model by using the GP prior. In addition to the mean of the predictive distribution, also its standard deviation (return_std=True) or covariance (return_cov=True). Note that at most one of the two can be requested.</source>
          <target state="translated">我们也可以通过使用GP先验来基于非拟合模型进行预测。除了预测分布的均值外,还可以得到它的标准差(return_std=True)或协方差(return_cov=True)。需要注意的是,最多可以要求两者中的一个。</target>
        </trans-unit>
        <trans-unit id="7f60df7f123136fa11f4ddf222fab31a6a72d17d" translate="yes" xml:space="preserve">
          <source>We can choose &lt;code&gt;alpha&lt;/code&gt; to minimize left out error, this time using the diabetes dataset rather than our synthetic data:</source>
          <target state="translated">我们可以选择 &lt;code&gt;alpha&lt;/code&gt; 以最大程度地减少遗漏的错误，这次使用糖尿病数据集而不是我们的综合数据：</target>
        </trans-unit>
        <trans-unit id="469d4e6eabf44c56eec1620cbb6087744c30d3f6" translate="yes" xml:space="preserve">
          <source>We can clearly see that the median house price shows a linear relationship with the median income (top left) and that the house price drops when the avg. occupants per household increases (top middle). The top right plot shows that the house age in a district does not have a strong influence on the (median) house price; so does the average rooms per household. The tick marks on the x-axis represent the deciles of the feature values in the training data.</source>
          <target state="translated">我们可以清楚地看到,房价中位数与收入中位数呈线性关系(左上角),当每户平均居住人数增加时,房价会下降(中上角)。右上图显示,一个地区的房龄对(中位数)房价影响不大;每户平均房间数也是如此。x轴上的勾号代表了训练数据中特征值的分位数。</target>
        </trans-unit>
        <trans-unit id="c32b4a200d58f731852c3f4a8eefb32ef18f31ed" translate="yes" xml:space="preserve">
          <source>We can keep the remaining rating columns by setting &lt;code&gt;remainder='passthrough'&lt;/code&gt;. The values are appended to the end of the transformation:</source>
          <target state="translated">我们可以通过设置 &lt;code&gt;remainder='passthrough'&lt;/code&gt; 来保留剩余的评分栏。这些值将附加到转换的末尾：</target>
        </trans-unit>
        <trans-unit id="a59ff6e4288992e31a9513b51da5a036927e8ec8" translate="yes" xml:space="preserve">
          <source>We can now load the list of files matching those categories as follows:</source>
          <target state="translated">现在我们可以加载与这些类别相匹配的文件列表,如下所示。</target>
        </trans-unit>
        <trans-unit id="f1f864cfbdff004081b5667459fd9c49d8fcf703" translate="yes" xml:space="preserve">
          <source>We can now quickly sample a training set while holding out 40% of the data for testing (evaluating) our classifier:</source>
          <target state="translated">现在,我们可以快速地对一个训练集进行采样,同时保留40%的数据来测试(评估)我们的分类器。</target>
        </trans-unit>
        <trans-unit id="850dcea5aea59348ecbdc161cc86fe56ad9b64b5" translate="yes" xml:space="preserve">
          <source>We can reduce the dimension even more, to a chosen \(L\), by projecting onto the linear subspace \(H_L\) which maximizes the variance of the \(\mu^*_k\) after projection (in effect, we are doing a form of PCA for the transformed class means \(\mu^*_k\)). This \(L\) corresponds to the &lt;code&gt;n_components&lt;/code&gt; parameter used in the &lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform&quot;&gt;&lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.transform&lt;/code&gt;&lt;/a&gt; method. See &lt;a href=&quot;#id4&quot; id=&quot;id2&quot;&gt;[3]&lt;/a&gt; for more details.</source>
          <target state="translated">通过投影到线性子空间\（H_L \）上，我们可以将尺寸进一步减小到选定的\（L \），这样可以最大化投影后\（\ mu ^ * _ k \）的方差（实际上，我们正在为转换后的类手段\（\ mu ^ * _ k \））做PCA形式。此\（L \）对应于&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform&quot;&gt; &lt;code&gt;discriminant_analysis.LinearDiscriminantAnalysis.transform&lt;/code&gt; &lt;/a&gt;方法中使用的 &lt;code&gt;n_components&lt;/code&gt; 参数。有关更多详细信息，请参见&lt;a href=&quot;#id4&quot; id=&quot;id2&quot;&gt;[3]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="bee45302ee9842b9e6d2e72404b6369d8b7e97cc" translate="yes" xml:space="preserve">
          <source>We can see that for low values of &lt;code&gt;n_components&lt;/code&gt; the distribution is wide with many distorted pairs and a skewed distribution (due to the hard limit of zero ratio on the left as distances are always positives) while for larger values of n_components the distortion is controlled and the distances are well preserved by the random projection.</source>
          <target state="translated">我们可以看到，对于 &lt;code&gt;n_components&lt;/code&gt; 较低的值，分布较宽，具有许多扭曲对和偏斜的分布（由于左侧的零比率的硬性限制，因为距离始终为正值），而对于n_components较大的值，则可以控制失真，并且通过随机投影可以很好地保持距离。</target>
        </trans-unit>
        <trans-unit id="d51b2a4e76fb7eb99807202c5234812c15585e4c" translate="yes" xml:space="preserve">
          <source>We can see that if the maximum depth of the tree (controlled by the &lt;code&gt;max_depth&lt;/code&gt; parameter) is set too high, the decision trees learn too fine details of the training data and learn from the noise, i.e. they overfit.</source>
          <target state="translated">我们可以看到，如果将树的最大深度（由 &lt;code&gt;max_depth&lt;/code&gt; 参数控制）设置得太高，则决策树将学习训练数据的细节，并从噪声中学习，即它们过度拟合。</target>
        </trans-unit>
        <trans-unit id="5bc27bfd8c709ab5505734b2124db3dbd09e7af4" translate="yes" xml:space="preserve">
          <source>We can see that, although feature 2 has a strong coefficient on the full model, it conveys little information on &lt;code&gt;y&lt;/code&gt; when considered with feature 1.</source>
          <target state="translated">我们可以看到，尽管特征2在整个模型中具有很强的系数，但是当与特征1一起考虑时，它几乎没有传达关于 &lt;code&gt;y&lt;/code&gt; 的信息。</target>
        </trans-unit>
        <trans-unit id="208e5f1f40787dd5dcd62a253128bd90d5a3414b" translate="yes" xml:space="preserve">
          <source>We can turn those concept as scores &lt;a href=&quot;generated/sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt;&lt;code&gt;homogeneity_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt;&lt;code&gt;completeness_score&lt;/code&gt;&lt;/a&gt;. Both are bounded below by 0.0 and above by 1.0 (higher is better):</source>
          <target state="translated">我们可以将这些概念转化为score &lt;a href=&quot;generated/sklearn.metrics.homogeneity_score#sklearn.metrics.homogeneity_score&quot;&gt; &lt;code&gt;homogeneity_score&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.metrics.completeness_score#sklearn.metrics.completeness_score&quot;&gt; &lt;code&gt;completeness_score&lt;/code&gt; &lt;/a&gt;。两者都在0.0之下限制在1.0范围之内（越高越好）：</target>
        </trans-unit>
        <trans-unit id="592289b1a6fdac7a6256bbd4a62b88701d16e505" translate="yes" xml:space="preserve">
          <source>We can use the function &lt;a href=&quot;generated/sklearn.model_selection.learning_curve#sklearn.model_selection.learning_curve&quot;&gt;&lt;code&gt;learning_curve&lt;/code&gt;&lt;/a&gt; to generate the values that are required to plot such a learning curve (number of samples that have been used, the average scores on the training sets and the average scores on the validation sets):</source>
          <target state="translated">我们可以使用函数&lt;a href=&quot;generated/sklearn.model_selection.learning_curve#sklearn.model_selection.learning_curve&quot;&gt; &lt;code&gt;learning_curve&lt;/code&gt; &lt;/a&gt;生成绘制这样的学习曲线所需的值（已使用的样本数，训练集的平均分数和验证集的平均分数）：</target>
        </trans-unit>
        <trans-unit id="3c55a99ecafce9d11edefa5f7981438b525c546b" translate="yes" xml:space="preserve">
          <source>We classify 8x8 images of digits into two classes: 0-4 against 5-9. The visualization shows coefficients of the models for varying C.</source>
          <target state="translated">我们将8x8的数字图像分为两类。0-4与5-9。可视化显示了不同C的模型的系数。</target>
        </trans-unit>
        <trans-unit id="4c2ceff57e3b51d317fe78664f4ea8312ed35966" translate="yes" xml:space="preserve">
          <source>We consider 3 features x_1, x_2, x_3 distributed uniformly over [0, 1], the target depends on them as follows:</source>
          <target state="translated">我们考虑3个特征x_1,x_2,x_3均匀分布在[0,1]上,目标取决于它们,如下。</target>
        </trans-unit>
        <trans-unit id="8b738423194ebf355d81c34c5848e446f6f25c86" translate="yes" xml:space="preserve">
          <source>We create a multi-label dataset, to illustrate the precision-recall in multi-label settings</source>
          <target state="translated">我们创建了一个多标签数据集,以说明在多标签设置中的精确性-回归性</target>
        </trans-unit>
        <trans-unit id="7df3a30efca0a13e3a362147c9be1fa02a3e02fd" translate="yes" xml:space="preserve">
          <source>We don&amp;rsquo;t allow:</source>
          <target state="translated">我们不允许：</target>
        </trans-unit>
        <trans-unit id="c17cba12ad6e5c2f931d1de0fcadd712e7bb0a0a" translate="yes" xml:space="preserve">
          <source>We first find the separating plane with a plain SVC and then plot (dashed) the separating hyperplane with automatically correction for unbalanced classes.</source>
          <target state="translated">我们先用普通SVC找到分离平面,然后绘制(虚线)自动修正不平衡类的分离超平面。</target>
        </trans-unit>
        <trans-unit id="916afde7457af1caed530318ff87e1a7a139918e" translate="yes" xml:space="preserve">
          <source>We found that &lt;code&gt;max_leaf_nodes=k&lt;/code&gt; gives comparable results to &lt;code&gt;max_depth=k-1&lt;/code&gt; but is significantly faster to train at the expense of a slightly higher training error. The parameter &lt;code&gt;max_leaf_nodes&lt;/code&gt; corresponds to the variable &lt;code&gt;J&lt;/code&gt; in the chapter on gradient boosting in &lt;a href=&quot;#f2001&quot; id=&quot;id14&quot;&gt;[F2001]&lt;/a&gt; and is related to the parameter &lt;code&gt;interaction.depth&lt;/code&gt; in R&amp;rsquo;s gbm package where &lt;code&gt;max_leaf_nodes == interaction.depth + 1&lt;/code&gt; .</source>
          <target state="translated">我们发现 &lt;code&gt;max_leaf_nodes=k&lt;/code&gt; 可以提供与 &lt;code&gt;max_depth=k-1&lt;/code&gt; 相当的结果，但是训练起来明显更快，但代价是稍高的训练误差。参数 &lt;code&gt;max_leaf_nodes&lt;/code&gt; 对应于&lt;a href=&quot;#f2001&quot; id=&quot;id14&quot;&gt;[F2001]中&lt;/a&gt;有关梯度增强一章中的变量 &lt;code&gt;J&lt;/code&gt; ，并且与R的gbm包中的参数 &lt;code&gt;interaction.depth&lt;/code&gt; 有关，其中 &lt;code&gt;max_leaf_nodes == interaction.depth + 1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="72f7189dc223c470430f67b7332d9ced78363339" translate="yes" xml:space="preserve">
          <source>We found that Averaged SGD works best with a larger number of features and a higher eta0</source>
          <target state="translated">我们发现,在特征数量较多、eta0较高的情况下,Averaged SGD效果最好。</target>
        </trans-unit>
        <trans-unit id="ecc641e4f0c3be544f8ba1b6978d12b64c778e22" translate="yes" xml:space="preserve">
          <source>We generate data from three groups of waveforms. Two of the waveforms (waveform 1 and waveform 2) are proportional one to the other. The cosine distance is invariant to a scaling of the data, as a result, it cannot distinguish these two waveforms. Thus even with no noise, clustering using this distance will not separate out waveform 1 and 2.</source>
          <target state="translated">我们从三组波形中生成数据。其中有两个波形(波形1和波形2)是相互成比例的。余弦距离对数据的缩放是不变的,因此,它不能区分这两个波形。因此,即使在没有噪声的情况下,用这个距离进行聚类也不能将波形1和波形2分离出来。</target>
        </trans-unit>
        <trans-unit id="03835cacd7901d07f747c14f209b80543758c4fd" translate="yes" xml:space="preserve">
          <source>We have seen that some estimators can transform data and that some estimators can predict variables. We can also create combined estimators:</source>
          <target state="translated">我们已经看到,有些估计器可以转换数据,有些估计器可以预测变量。我们还可以创建组合估计器。</target>
        </trans-unit>
        <trans-unit id="996013b3c282443801da622c65fee1deca3cef01" translate="yes" xml:space="preserve">
          <source>We have seen that sparsity could be used to mitigate the curse of dimensionality, &lt;em&gt;i.e&lt;/em&gt; an insufficient amount of observations compared to the number of features. Another approach is to merge together similar features: &lt;strong&gt;feature agglomeration&lt;/strong&gt;. This approach can be implemented by clustering in the feature direction, in other words clustering the transposed data.</source>
          <target state="translated">我们已经看到稀疏性可以用来减轻维数的诅咒，&lt;em&gt;即&lt;/em&gt;与特征数量相比，观测值不足。另一种方法是将相似的特征融合在一起：&lt;strong&gt;特征集聚&lt;/strong&gt;。可以通过在特征方向上进行聚类（换句话说，对转置后的数据进行聚类）来实现此方法。</target>
        </trans-unit>
        <trans-unit id="f70b46435230cf70b0b3d749ceef620b9bcdee9d" translate="yes" xml:space="preserve">
          <source>We have specifically abstained from an optimization used by authors of both papers, a QR decomposition used in specific situations to reduce the algorithmic complexity of the SVD. The source for this technique is &lt;code&gt;Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5, section 5.4.4, pp 252-253.&lt;/code&gt;. This technique has been omitted because it is advantageous only when decomposing a matrix with &lt;code&gt;n_samples&lt;/code&gt; (rows) &amp;gt;= 5/3 * &lt;code&gt;n_features&lt;/code&gt; (columns), and hurts the readability of the implemented algorithm. This would be a good opportunity for future optimization, if it is deemed necessary.</source>
          <target state="translated">我们特别放弃了两篇论文的作者所使用的优化，即在特定情况下使用QR分解来降低SVD的算法复杂性。该技术的来源是《 &lt;code&gt;Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5, section 5.4.4, pp 252-253.&lt;/code&gt; 。该技术已被省略，因为仅在分解 &lt;code&gt;n_samples&lt;/code&gt; （行）&amp;gt; = 5/3 * &lt;code&gt;n_features&lt;/code&gt; （列）的矩阵时才有优势，并且会损害所实现算法的可读性。如果认为必要的话，这将是未来优化的好机会。</target>
        </trans-unit>
        <trans-unit id="db298eced453f06f8efed43be73a03415c98ca01" translate="yes" xml:space="preserve">
          <source>We have to reconstruct model and parameters to make sure we stay in sync with the python object.</source>
          <target state="translated">我们必须重建模型和参数,以确保我们与python对象保持同步。</target>
        </trans-unit>
        <trans-unit id="f2683785e1f3e3ccb40d3941fed7d5fa67a33cfc" translate="yes" xml:space="preserve">
          <source>We introduce a new parameter \(\nu\) which controls the number of support vectors and training errors. The parameter \(\nu \in (0, 1]\) is an upper bound on the fraction of training errors and a lower bound of the fraction of support vectors.</source>
          <target state="translated">我们引入一个新的参数\(\nu\)来控制支持向量和训练错误的数量。这个参数(nu(0,1))是训练错误分数的上限和支持向量分数的下限。</target>
        </trans-unit>
        <trans-unit id="284f00654c2880797bacd24e85e3bc2f5ec8be8d" translate="yes" xml:space="preserve">
          <source>We no longer get the collisions, but this comes at the expense of a much larger dimensionality of the output space. Of course, other terms than the 19 used here might still collide with each other.</source>
          <target state="translated">我们不再得到碰撞,但这是以输出空间的维度大大增加为代价的。当然,除了这里使用的19个术语外,其他术语仍可能相互碰撞。</target>
        </trans-unit>
        <trans-unit id="0e7a6d4d2e128e719f76bf1799c4515162612bfb" translate="yes" xml:space="preserve">
          <source>We observe a tendency towards clearer shapes as the preplexity value increases.</source>
          <target state="translated">我们观察到,随着复杂度值的增加,形状趋于清晰。</target>
        </trans-unit>
        <trans-unit id="b4e7d2188aa62b346706f6bd2a1ae94095756883" translate="yes" xml:space="preserve">
          <source>We plot predicted labels on both training and held out test data using a variety of GMM covariance types on the iris dataset. We compare GMMs with spherical, diagonal, full, and tied covariance matrices in increasing order of performance. Although one would expect full covariance to perform best in general, it is prone to overfitting on small datasets and does not generalize well to held out test data.</source>
          <target state="translated">我们绘制了使用各种GMM协方差类型在虹膜数据集上对训练和持出测试数据的预测标签。我们比较了GMM与球形、对角线、全协方差和并列协方差矩阵的性能递增顺序。虽然人们期望全协方差在一般情况下表现最好,但它在小数据集上容易过度拟合,并且不能很好地推广到保持出来的测试数据上。</target>
        </trans-unit>
        <trans-unit id="8a444360e57dc0870f14b3c959a7b626922e7571" translate="yes" xml:space="preserve">
          <source>We see that &lt;code&gt;SVC&lt;/code&gt; doesn&amp;rsquo;t do much better than a dummy classifier. Now, let&amp;rsquo;s change the kernel:</source>
          <target state="translated">我们看到 &lt;code&gt;SVC&lt;/code&gt; 并没有比虚拟分类器好得多。现在，让我们更改内核：</target>
        </trans-unit>
        <trans-unit id="9619f66671fbeb88bbee2c1b3d850c22bdb6ab25" translate="yes" xml:space="preserve">
          <source>We see that the accuracy was boosted to almost 100%. A cross validation strategy is recommended for a better estimate of the accuracy, if it is not too CPU costly. For more information see the &lt;a href=&quot;cross_validation#cross-validation&quot;&gt;Cross-validation: evaluating estimator performance&lt;/a&gt; section. Moreover if you want to optimize over the parameter space, it is highly recommended to use an appropriate methodology; see the &lt;a href=&quot;grid_search#grid-search&quot;&gt;Tuning the hyper-parameters of an estimator&lt;/a&gt; section for details.</source>
          <target state="translated">我们看到准确性提高到几乎100％。如果CPU的成本不太高，建议使用交叉验证策略以更好地估计准确性。有关更多信息，请参见&amp;ldquo; &lt;a href=&quot;cross_validation#cross-validation&quot;&gt;交叉验证：评估估计器性能&amp;rdquo;&lt;/a&gt;部分。此外，如果要在参数空间上进行优化，强烈建议使用适当的方法。有关详细信息，请参见&lt;a href=&quot;grid_search#grid-search&quot;&gt;调整估计器的超参数&lt;/a&gt;部分。</target>
        </trans-unit>
        <trans-unit id="d680bb4e5101fe3a9df93911d26cfa982767e679" translate="yes" xml:space="preserve">
          <source>We see that the resulting &lt;em&gt;polynomial regression&lt;/em&gt; is in the same class of linear models we&amp;rsquo;d considered above (i.e. the model is linear in \(w\)) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data.</source>
          <target state="translated">我们看到，所得的&lt;em&gt;多项式回归&lt;/em&gt;属于我们上面考虑的同一类线性模型（即，该模型在\（w \）中是线性的），并且可以通过相同的技术来求解。通过考虑使用这些基本函数构建的高维空间中的线性拟合，该模型可以灵活地适应更大范围的数据。</target>
        </trans-unit>
        <trans-unit id="1a0e7af8231b7a460dcfd9acf44c6323ad1ea844" translate="yes" xml:space="preserve">
          <source>We selected two sets of two variables from the Boston housing data set as an illustration of what kind of analysis can be done with several outlier detection tools. For the purpose of visualization, we are working with two-dimensional examples, but one should be aware that things are not so trivial in high-dimension, as it will be pointed out.</source>
          <target state="translated">我们从波士顿住房数据集中选取了两组两个变量,来说明用几种离群值检测工具可以做什么样的分析。为了可视化的目的,我们使用的是二维的例子,但我们应该意识到,在高维的情况下,事情并不那么琐碎,这一点将被指出。</target>
        </trans-unit>
        <trans-unit id="940e7a9289cb117779f9b0e089fc6f41ec456057" translate="yes" xml:space="preserve">
          <source>We should also note that small differences in scores results from the random splits of the cross-validation procedure. Those spurious variations can be smoothed out by increasing the number of CV iterations &lt;code&gt;n_splits&lt;/code&gt; at the expense of compute time. Increasing the value number of &lt;code&gt;C_range&lt;/code&gt; and &lt;code&gt;gamma_range&lt;/code&gt; steps will increase the resolution of the hyper-parameter heat map.</source>
          <target state="translated">我们还应该注意，分数的微小差异是由交叉验证过程的随机分裂造成的。可以通过增加CV迭代次数 &lt;code&gt;n_splits&lt;/code&gt; 来消除那些虚假的变化，但会浪费计算时间。增加 &lt;code&gt;C_range&lt;/code&gt; 和 &lt;code&gt;gamma_range&lt;/code&gt; 步骤的值数量将增加超参数热图的分辨率。</target>
        </trans-unit>
        <trans-unit id="1cbc5d306e08afa1d68b1045fc6a567611cf26d2" translate="yes" xml:space="preserve">
          <source>We show that linear_model.Lasso provides the same results for dense and sparse data and that in the case of sparse data the speed is improved.</source>
          <target state="translated">我们表明,linear_model.Lasso对密集数据和稀疏数据提供了同样的结果,而且在稀疏数据的情况下,速度得到了提高。</target>
        </trans-unit>
        <trans-unit id="efbf964638e7ca0fbdc3e1bd2a5029a9cbed6b8c" translate="yes" xml:space="preserve">
          <source>We start by training a label propagation model with only 10 labeled points, then we select the top five most uncertain points to label. Next, we train with 15 labeled points (original 10 + 5 new ones). We repeat this process four times to have a model trained with 30 labeled examples. Note you can increase this to label more than 30 by changing &lt;code&gt;max_iterations&lt;/code&gt;. Labeling more than 30 can be useful to get a sense for the speed of convergence of this active learning technique.</source>
          <target state="translated">我们首先训练仅带有10个标记点的标签传播模型，然后选择前五个不确定性最高的点进行标记。接下来，我们训练15个带标记的点（原始10个+ 5个新点）。我们重复此过程四次，以训练带有30个带标签的示例的模型。请注意，您可以通过更改 &lt;code&gt;max_iterations&lt;/code&gt; 将其增加以标记30个以上。标记超过30个可能有助于了解这种主动学习技术的融合速度。</target>
        </trans-unit>
        <trans-unit id="b926aa4a7c89ca684b4dc714781356e70fac60ed" translate="yes" xml:space="preserve">
          <source>We thus transform the KDD Data set into two different data sets: SA and SF.</source>
          <target state="translated">因此,我们将KDD数据集转化为两个不同的数据集。SA和SF。</target>
        </trans-unit>
        <trans-unit id="13122bf873819e99a4ec7d32c926bbee95474f9b" translate="yes" xml:space="preserve">
          <source>We use a GridSearchCV to set the dimensionality of the PCA</source>
          <target state="translated">我们使用GridSearchCV来设置PCA的维度。</target>
        </trans-unit>
        <trans-unit id="83c28baded6fcaa48849ef740f04075945b33344" translate="yes" xml:space="preserve">
          <source>We use clustering to group together quotes that behave similarly. Here, amongst the &lt;a href=&quot;../../modules/clustering#clustering&quot;&gt;various clustering techniques&lt;/a&gt; available in the scikit-learn, we use &lt;a href=&quot;../../modules/clustering#affinity-propagation&quot;&gt;Affinity Propagation&lt;/a&gt; as it does not enforce equal-size clusters, and it can choose automatically the number of clusters from the data.</source>
          <target state="translated">我们使用聚类将表现相似的报价分组在一起。在这里，在scikit-learn中可用的&lt;a href=&quot;../../modules/clustering#clustering&quot;&gt;各种聚类技术&lt;/a&gt;中，我们使用&amp;ldquo; &lt;a href=&quot;../../modules/clustering#affinity-propagation&quot;&gt;相似性传播&amp;rdquo;，&lt;/a&gt;因为它不强制执行大小相等的聚类，并且可以从数据中自动选择聚类数量。</target>
        </trans-unit>
        <trans-unit id="78d7bc0e66fad92fa188a5bbaa0a7aaa581101c6" translate="yes" xml:space="preserve">
          <source>We use sparse inverse covariance estimation to find which quotes are correlated conditionally on the others. Specifically, sparse inverse covariance gives us a graph, that is a list of connection. For each symbol, the symbols that it is connected too are those useful to explain its fluctuations.</source>
          <target state="translated">我们使用稀疏反协方差估计来寻找哪些行情对其他行情有条件的相关性。具体来说,稀疏反协方差给我们提供了一个图,也就是一个连接的列表。对于每个符号来说,它所连接的符号也是那些对解释其波动有用的符号。</target>
        </trans-unit>
        <trans-unit id="4e3f49993eefd3c100d45584ffc552355d0d52e7" translate="yes" xml:space="preserve">
          <source>We validate the above bounds on the digits dataset or on the 20 newsgroups text document (TF-IDF word frequencies) dataset:</source>
          <target state="translated">我们在数字数据集或20个新闻组文本文档(TF-IDF词频)数据集上验证上述界限。</target>
        </trans-unit>
        <trans-unit id="4bd2e73735072e414ab7c853666f3b851cc359c1" translate="yes" xml:space="preserve">
          <source>We want to compare the performance of the MiniBatchKMeans and KMeans: the MiniBatchKMeans is faster, but gives slightly different results (see &lt;a href=&quot;../../modules/clustering#mini-batch-kmeans&quot;&gt;Mini Batch K-Means&lt;/a&gt;).</source>
          <target state="translated">我们想比较MiniBatchKMeans和KMeans的性能：MiniBatchKMeans更快，但给出的结果略有不同（请参阅&lt;a href=&quot;../../modules/clustering#mini-batch-kmeans&quot;&gt;Mini Batch K-Means&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="b8ae1ec8b6dd301757e7e59be0a9aeff645f7f18" translate="yes" xml:space="preserve">
          <source>We will cluster a set of data, first with KMeans and then with MiniBatchKMeans, and plot the results. We will also plot the points that are labelled differently between the two algorithms.</source>
          <target state="translated">我们将对一组数据进行聚类,先用KMeans,再用MiniBatchKMeans,并绘制结果。我们还将绘制两种算法之间标注不同的点。</target>
        </trans-unit>
        <trans-unit id="b4cfcf9a2f9d6095c0707be11ca996fe9017bf9d" translate="yes" xml:space="preserve">
          <source>We will probably have to use an estimator or a parametrization of the current estimator that can learn more complex concepts (i.e. has a lower bias). If the training score is much greater than the validation score for the maximum number of training samples, adding more training samples will most likely increase generalization. In the following plot you can see that the SVM could benefit from more training examples.</source>
          <target state="translated">我们可能要使用一个估计器或当前估计器的参数化,它可以学习更复杂的概念(即具有较低的偏差)。如果最大训练样本数的训练得分远大于验证得分,那么增加更多的训练样本很可能会增加泛化。在下面的图中,你可以看到,SVM可以从更多的训练样本中受益。</target>
        </trans-unit>
        <trans-unit id="3f09acb611dde4d0822059b7dd910a6bf0d4be22" translate="yes" xml:space="preserve">
          <source>We will review here the orders of magnitude you can expect from a number of scikit-learn estimators in different contexts and provide some tips and tricks for overcoming performance bottlenecks.</source>
          <target state="translated">我们将在这里回顾一下在不同情况下你可以从一些scikit-learn估计器中得到的数量级,并提供一些克服性能瓶颈的技巧和窍门。</target>
        </trans-unit>
        <trans-unit id="5ab95440492dec088ab7d086a3622b86acadc7e7" translate="yes" xml:space="preserve">
          <source>We&amp;rsquo;ll define a function that lets us visualize the behavior of each cross-validation object. We&amp;rsquo;ll perform 4 splits of the data. On each split, we&amp;rsquo;ll visualize the indices chosen for the training set (in blue) and the test set (in red).</source>
          <target state="translated">我们将定义一个函数，使我们可以可视化每个交叉验证对象的行为。我们将对数据进行4次拆分。在每个分组上，我们将为训练集（蓝色）和测试集（红色）可视化选择的索引。</target>
        </trans-unit>
        <trans-unit id="70c023ae490fc71ae664cdd80527ab708a5db4b9" translate="yes" xml:space="preserve">
          <source>We&amp;rsquo;ve already encountered some parameters such as &lt;code&gt;use_idf&lt;/code&gt; in the &lt;code&gt;TfidfTransformer&lt;/code&gt;. Classifiers tend to have many parameters as well; e.g., &lt;code&gt;MultinomialNB&lt;/code&gt; includes a smoothing parameter &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;SGDClassifier&lt;/code&gt; has a penalty parameter &lt;code&gt;alpha&lt;/code&gt; and configurable loss and penalty terms in the objective function (see the module documentation, or use the Python &lt;code&gt;help&lt;/code&gt; function to get a description of these).</source>
          <target state="translated">我们已经在 &lt;code&gt;use_idf&lt;/code&gt; 中遇到了一些参数，例如 &lt;code&gt;TfidfTransformer&lt;/code&gt; 。分类器也往往具有许多参数。例如， &lt;code&gt;MultinomialNB&lt;/code&gt; 包括平滑参数 &lt;code&gt;alpha&lt;/code&gt; 和 &lt;code&gt;SGDClassifier&lt;/code&gt; 具有惩罚参数 &lt;code&gt;alpha&lt;/code&gt; 的目标函数和可配置的损失和惩罚项（见模块文档，或使用Python &lt;code&gt;help&lt;/code&gt; 函数来获取这些的说明）。</target>
        </trans-unit>
        <trans-unit id="96df76d7fa199e301349be570d5ef4d0bb6a7f3d" translate="yes" xml:space="preserve">
          <source>Weight given to each sample.</source>
          <target state="translated">给予每个样本的重量。</target>
        </trans-unit>
        <trans-unit id="a7a3ce3e7a16ef99378bfef64690454462da25e9" translate="yes" xml:space="preserve">
          <source>Weight matrix, where n_features in the number of visible units and n_components is the number of hidden units.</source>
          <target state="translated">权重矩阵,其中n_features为可见单元数,n_components为隐藏单元数。</target>
        </trans-unit>
        <trans-unit id="d0664e46a183d0a2a2e3afcbc3b6c5ba30a9d4ef" translate="yes" xml:space="preserve">
          <source>Weight of each sample, such that a sample with a weight of at least &lt;code&gt;min_samples&lt;/code&gt; is by itself a core sample; a sample with negative weight may inhibit its eps-neighbor from being core. Note that weights are absolute, and default to 1.</source>
          <target state="translated">每个样品的重量，以使重量至少为 &lt;code&gt;min_samples&lt;/code&gt; 的样品本身就是核心样品；负重量的样本可能会阻止其eps邻居成为核心。请注意，权重是绝对的，默认为1。</target>
        </trans-unit>
        <trans-unit id="5cc536fd8cf249ec4c2e295665e0070f1b9cec67" translate="yes" xml:space="preserve">
          <source>Weight of precision in harmonic mean.</source>
          <target state="translated">精度在谐波平均值中的比重。</target>
        </trans-unit>
        <trans-unit id="6cd90e03c276712f974984f65620519bcea49500" translate="yes" xml:space="preserve">
          <source>Weight vector(s).</source>
          <target state="translated">权重向量。</target>
        </trans-unit>
        <trans-unit id="ac0d2c9a738f9c54a5d208ddac8f22019ff9c627" translate="yes" xml:space="preserve">
          <source>Weight, Waist and Pulse.</source>
          <target state="translated">体重、腰围和脉搏。</target>
        </trans-unit>
        <trans-unit id="c74e4e7c5caf95682fb65872b5814741f06c7fac" translate="yes" xml:space="preserve">
          <source>Weighted average</source>
          <target state="translated">加权平均数</target>
        </trans-unit>
        <trans-unit id="39392047d0260b6fc1e042825fdae9116d630e28" translate="yes" xml:space="preserve">
          <source>Weighted average probability for each class per sample.</source>
          <target state="translated">每个班级每个样本的加权平均概率。</target>
        </trans-unit>
        <trans-unit id="9a702ae7f12a23bf0cde9786ae821e6b340d4991" translate="yes" xml:space="preserve">
          <source>Weights applied to individual samples (1. for unweighted).</source>
          <target state="translated">应用于单个样本的权重(1.为未加权)。</target>
        </trans-unit>
        <trans-unit id="0c68217fe30f051f7b997da07ad569653cfdb104" translate="yes" xml:space="preserve">
          <source>Weights applied to individual samples. If not provided, uniform weights are assumed.</source>
          <target state="translated">适用于单个样本的权重。如果没有提供,则假定为统一的权重。</target>
        </trans-unit>
        <trans-unit id="3070fe087be2b6fbe15f65d4db644297c1112686" translate="yes" xml:space="preserve">
          <source>Weights applied to individual samples. If not provided, uniform weights are assumed. These weights will be multiplied with class_weight (passed through the constructor) if class_weight is specified</source>
          <target state="translated">适用于单个样本的权重。如果没有提供,则假定为统一权重。如果指定了class_weight,这些权重将与class_weight(通过构造函数传递)相乘。</target>
        </trans-unit>
        <trans-unit id="86b1d5826d4836f8e129b6346c9e9e60976aafee" translate="yes" xml:space="preserve">
          <source>Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel.</source>
          <target state="translated">分配给特征的权重(原始问题中的系数)。这只有在线性核的情况下才有。</target>
        </trans-unit>
        <trans-unit id="4b149f5e057b5bff95048ebeff46bfac0e7a368d" translate="yes" xml:space="preserve">
          <source>Weights assigned to the features.</source>
          <target state="translated">分配给特征的权重。</target>
        </trans-unit>
        <trans-unit id="8064bf8d5d6b0c15f7ed813823cc0da43a8bc7e6" translate="yes" xml:space="preserve">
          <source>Weights associated with classes in the form &lt;code&gt;{class_label: weight}&lt;/code&gt;. If not given, all classes are supposed to have weight one.</source>
          <target state="translated">与类关联的权重，形式为 &lt;code&gt;{class_label: weight}&lt;/code&gt; 。如果未给出，则所有类都应具有权重一。</target>
        </trans-unit>
        <trans-unit id="96f3238c530c2df09403ab213fee9515feb36c99" translate="yes" xml:space="preserve">
          <source>Weights associated with classes in the form &lt;code&gt;{class_label: weight}&lt;/code&gt;. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.</source>
          <target state="translated">与类关联的权重，形式为 &lt;code&gt;{class_label: weight}&lt;/code&gt; 。如果未给出，则所有类都应具有权重一。对于多输出问题，可以按与y列相同的顺序提供字典列表。</target>
        </trans-unit>
        <trans-unit id="4a87f3dd4dfb432aa1a51752552e165c9cc23301" translate="yes" xml:space="preserve">
          <source>Weights associated with classes. If not given, all classes are supposed to have weight one.</source>
          <target state="translated">与班级相关的权重。如果没有给定,所有班级的权重都是1。</target>
        </trans-unit>
        <trans-unit id="4837ab63e8195a91fca82bbd82590df1bbe7fcc4" translate="yes" xml:space="preserve">
          <source>Weights for each estimator in the boosted ensemble.</source>
          <target state="translated">在增强的集合中,每个估计器的权重。</target>
        </trans-unit>
        <trans-unit id="55ddc90a49d39d4b40d722b720afa82441019829" translate="yes" xml:space="preserve">
          <source>Weights on each point of the regression. If None, weight is set to 1 (equal weights).</source>
          <target state="translated">回归中每个点的权重。如果无,权重设为1(权重相等)。</target>
        </trans-unit>
        <trans-unit id="0946f675292deb36a2ff9f4a33806ccd9e9e1833" translate="yes" xml:space="preserve">
          <source>Weights. If set to None, all weights will be set to 1 (equal weights).</source>
          <target state="translated">权重。如果设置为 &quot;无&quot;,则所有权重都将设置为1(权重相等)。</target>
        </trans-unit>
        <trans-unit id="c1f521c553b00dab458900dd1fb3f949a6ba99ab" translate="yes" xml:space="preserve">
          <source>Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level. For instance a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a predict_proba value close to 0.8, approx. 80% actually belong to the positive class.</source>
          <target state="translated">校准良好的分类器是一种概率分类器,预测_proba方法的输出可以直接解释为置信度。例如,一个经过良好校准的(二进制)分类器应该对样本进行分类,在它给出的预测_proba值接近0.8的样本中,大约有80%的样本实际上属于正类。</target>
        </trans-unit>
        <trans-unit id="6db2509535d857954c618d97c17994b5d42574ae" translate="yes" xml:space="preserve">
          <source>Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level. For instance, a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a predict_proba value close to 0.8, approximately 80% actually belong to the positive class. The following plot compares how well the probabilistic predictions of different classifiers are calibrated:</source>
          <target state="translated">校准良好的分类器是一种概率分类器,预测_proba方法的输出可以直接解释为置信度。例如,一个校准良好的(二进制)分类器应该对样本进行分类,在它给出的预测_proba值接近0.8的样本中,大约80%的样本实际上属于正类。下图比较了不同分类器的概率预测的校准程度。</target>
        </trans-unit>
        <trans-unit id="830bc34728ca0799f710b4883d58631c6dfded76" translate="yes" xml:space="preserve">
          <source>Wether to include meta-estimators that are somehow special and can not be default-constructed sensibly. These are currently Pipeline, FeatureUnion and GridSearchCV</source>
          <target state="translated">是否包括一些特殊的元估计器,不能合理的默认构造。目前这些元估计器是Pipeline、FeatureUnion和GridSearchCV。</target>
        </trans-unit>
        <trans-unit id="d4f157bc9962e4b0dc2a197ed14e50902555d749" translate="yes" xml:space="preserve">
          <source>What are all the various decision tree algorithms and how do they differ from each other? Which one is implemented in scikit-learn?</source>
          <target state="translated">各种决策树算法都有哪些,它们之间有什么不同?scikit-learn中实现的是哪一种?</target>
        </trans-unit>
        <trans-unit id="a1f5f9cd3d06157b8582b1ca4000a5bc395e765a" translate="yes" xml:space="preserve">
          <source>What this example shows us is the behavior &amp;ldquo;rich getting richer&amp;rdquo; of agglomerative clustering that tends to create uneven cluster sizes. This behavior is pronounced for the average linkage strategy, that ends up with a couple of singleton clusters, while in the case of single linkage we get a single central cluster with all other clusters being drawn from noise points around the fringes.</source>
          <target state="translated">该示例向我们展示的是聚集聚类的行为&amp;ldquo;越富越富&amp;rdquo;，这往往会导致大小不均的聚类。对于平均链接策略，此行为是明显的，它以几个单例群集结束，而在单链接的情况下，我们得到一个中央群集，所有其他群集都从边缘周围的噪声点得出。</target>
        </trans-unit>
        <trans-unit id="32580ac608fcdadc1c2050695a6c24cae1fcef1f" translate="yes" xml:space="preserve">
          <source>What we can see that:</source>
          <target state="translated">我们可以看到的是,。</target>
        </trans-unit>
        <trans-unit id="bd10ebd98b3e733be938ffeb72efbd3793589a2c" translate="yes" xml:space="preserve">
          <source>When &lt;a href=&quot;generated/sklearn.decomposition.latentdirichletallocation#sklearn.decomposition.LatentDirichletAllocation&quot;&gt;&lt;code&gt;LatentDirichletAllocation&lt;/code&gt;&lt;/a&gt; is applied on a &amp;ldquo;document-term&amp;rdquo; matrix, the matrix will be decomposed into a &amp;ldquo;topic-term&amp;rdquo; matrix and a &amp;ldquo;document-topic&amp;rdquo; matrix. While &amp;ldquo;topic-term&amp;rdquo; matrix is stored as &lt;code&gt;components_&lt;/code&gt; in the model, &amp;ldquo;document-topic&amp;rdquo; matrix can be calculated from &lt;code&gt;transform&lt;/code&gt; method.</source>
          <target state="translated">当将&lt;a href=&quot;generated/sklearn.decomposition.latentdirichletallocation#sklearn.decomposition.LatentDirichletAllocation&quot;&gt; &lt;code&gt;LatentDirichletAllocation&lt;/code&gt; &lt;/a&gt;应用于&amp;ldquo;文档术语&amp;rdquo;矩阵时，该矩阵将分解为&amp;ldquo;主题术语&amp;rdquo;矩阵和&amp;ldquo;文档主题&amp;rdquo;矩阵。当&amp;ldquo;主题词&amp;rdquo;矩阵作为 &lt;code&gt;components_&lt;/code&gt; 存储在模型中时，&amp;ldquo;文档主题&amp;rdquo;矩阵可以通过 &lt;code&gt;transform&lt;/code&gt; 方法来计算。</target>
        </trans-unit>
        <trans-unit id="d35c02f0322e905eb57e225a27ca83f7c5f6cc47" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;axis=0&lt;/code&gt;, columns which only contained missing values at &lt;code&gt;fit&lt;/code&gt; are discarded upon &lt;code&gt;transform&lt;/code&gt;.</source>
          <target state="translated">当 &lt;code&gt;axis=0&lt;/code&gt; 时，仅包含 &lt;code&gt;fit&lt;/code&gt; 缺失值的列在 &lt;code&gt;transform&lt;/code&gt; 被丢弃。</target>
        </trans-unit>
        <trans-unit id="7cf98e7188203ecb0d30e3564b161d5393433799" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;axis=1&lt;/code&gt;, an exception is raised if there are rows for which it is not possible to fill in the missing values (e.g., because they only contain missing values).</source>
          <target state="translated">当 &lt;code&gt;axis=1&lt;/code&gt; 时，如果存在无法填充缺失值的行（例如，因为它们仅包含缺失值），则会引发异常。</target>
        </trans-unit>
        <trans-unit id="4bce6b6fbd4b9fe79bfc7468f7df9f938d3ce841" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;fit&lt;/code&gt; does not converge, &lt;code&gt;cluster_centers_&lt;/code&gt; becomes an empty array and all training samples will be labelled as &lt;code&gt;-1&lt;/code&gt;. In addition, &lt;code&gt;predict&lt;/code&gt; will then label every sample as &lt;code&gt;-1&lt;/code&gt;.</source>
          <target state="translated">当 &lt;code&gt;fit&lt;/code&gt; 不收敛时， &lt;code&gt;cluster_centers_&lt;/code&gt; 变为一个空数组，所有训练样本将被标记为 &lt;code&gt;-1&lt;/code&gt; 。此外， &lt;code&gt;predict&lt;/code&gt; 然后将每个样本标记为 &lt;code&gt;-1&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="f7c77c5939a6b7b3a7ed9ce47827d49725522a57" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;gamma&lt;/code&gt; is very small, the model is too constrained and cannot capture the complexity or &amp;ldquo;shape&amp;rdquo; of the data. The region of influence of any selected support vector would include the whole training set. The resulting model will behave similarly to a linear model with a set of hyperplanes that separate the centers of high density of any pair of two classes.</source>
          <target state="translated">当 &lt;code&gt;gamma&lt;/code&gt; 非常小时，该模型太受约束，无法捕获数据的复杂性或&amp;ldquo;形状&amp;rdquo;。任何选择的支持向量的影响区域将包括整个训练集。所得模型的行为将类似于具有一组超平面的线性模型，该超平面将两个类别的任何一对的高密度中心分开。</target>
        </trans-unit>
        <trans-unit id="6579a89bba02e2aa5596acf0a356de3285b5831f" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;learning_method&lt;/code&gt; is &amp;lsquo;online&amp;rsquo;, use mini-batch update. Otherwise, use batch update.</source>
          <target state="translated">当 &lt;code&gt;learning_method&lt;/code&gt; 为&amp;ldquo;在线&amp;rdquo;时，请使用小批量更新。否则，请使用批量更新。</target>
        </trans-unit>
        <trans-unit id="1789662661fe322dc45828ec3c3eb62749643034" translate="yes" xml:space="preserve">
          <source>When &lt;code&gt;novelty&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt; be aware that you must only use &lt;code&gt;predict&lt;/code&gt;, &lt;code&gt;decision_function&lt;/code&gt; and &lt;code&gt;score_samples&lt;/code&gt; on new unseen data and not on the training samples as this would lead to wrong results. The scores of abnormality of the training samples are always accessible through the &lt;code&gt;negative_outlier_factor_&lt;/code&gt; attribute.</source>
          <target state="translated">当将 &lt;code&gt;novelty&lt;/code&gt; 设置为 &lt;code&gt;True&lt;/code&gt; 时，请注意，您只能在新的看不见的数据上而不是训练样本上使用 &lt;code&gt;predict&lt;/code&gt; ， &lt;code&gt;decision_function&lt;/code&gt; 和 &lt;code&gt;score_samples&lt;/code&gt; ，因为这会导致错误的结果。始终可以通过 &lt;code&gt;negative_outlier_factor_&lt;/code&gt; 属性访问训练样本的异常分数。</target>
        </trans-unit>
        <trans-unit id="f3329fe0cead2a208482794593628ff4dce53789" translate="yes" xml:space="preserve">
          <source>When False, either &lt;code&gt;a&lt;/code&gt; or &lt;code&gt;b&lt;/code&gt; being sparse will yield sparse output. When True, output will always be an array.</source>
          <target state="translated">如果为False，则稀疏的 &lt;code&gt;a&lt;/code&gt; 或 &lt;code&gt;b&lt;/code&gt; 都会产生稀疏的输出。如果为True，则输出将始终是一个数组。</target>
        </trans-unit>
        <trans-unit id="0baad85c0e4d647371baa109869739d44cb0f600" translate="yes" xml:space="preserve">
          <source>When True (False by default) the &lt;code&gt;components_&lt;/code&gt; vectors are divided by &lt;code&gt;n_samples&lt;/code&gt; times &lt;code&gt;components_&lt;/code&gt; to ensure uncorrelated outputs with unit component-wise variances.</source>
          <target state="translated">为True（默认为False）时， &lt;code&gt;components_&lt;/code&gt; 向量除以 &lt;code&gt;n_samples&lt;/code&gt; 乘以 &lt;code&gt;components_&lt;/code&gt; ,以确保具有单位分量方差的不相关输出。</target>
        </trans-unit>
        <trans-unit id="3eaa2881688032030c765cb871d4c787aff03fe7" translate="yes" xml:space="preserve">
          <source>When True (False by default) the &lt;code&gt;components_&lt;/code&gt; vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances.</source>
          <target state="translated">为True（默认情况下为False）时， &lt;code&gt;components_&lt;/code&gt; 向量乘以n_samples的平方根，然后除以奇异值，以确保具有单位分量方差的不相关输出。</target>
        </trans-unit>
        <trans-unit id="afb8087ad0f4a499dd14f72be9807792c351ecd8" translate="yes" xml:space="preserve">
          <source>When True, an absolute value is applied to the features matrix prior to returning it. When used in conjunction with alternate_sign=True, this significantly reduces the inner product preservation property.</source>
          <target state="translated">当True时,在返回特征矩阵之前,会对其应用一个绝对值。当与alternate_sign=True一起使用时,会显著降低内积保存属性。</target>
        </trans-unit>
        <trans-unit id="204d5c48d22bd9cd74d36182840231c3ecac4f55" translate="yes" xml:space="preserve">
          <source>When True, an alternating sign is added to the features as to approximately conserve the inner product in the hashed space even for small n_features. This approach is similar to sparse random projection.</source>
          <target state="translated">当True时,一个交替符号被添加到特征上,即使对于小的n_特征,也能在哈希空间中近似地保存内积。这种方法类似于稀疏随机投影。</target>
        </trans-unit>
        <trans-unit id="e08ec276d8bba6d4be8a6e677715ca1a35d4dd15" translate="yes" xml:space="preserve">
          <source>When a grouped cross-validator is used, the group labels are also passed on to the &lt;code&gt;split&lt;/code&gt; method of the cross-validator. The cross-validator uses them for grouping the samples while splitting the dataset into train/test set.</source>
          <target state="translated">当使用分组的交叉验证器时，组标签也将传递到交叉验证器的 &lt;code&gt;split&lt;/code&gt; 方法。交叉验证器使用它们对样本进行分组，同时将数据集分为训练/测试集。</target>
        </trans-unit>
        <trans-unit id="6c0f90cdc44694d62adb6ac9bccd5513d6bf148f" translate="yes" xml:space="preserve">
          <source>When all training samples have equal similarities and equal preferences, the assignment of cluster centers and labels depends on the preference. If the preference is smaller than the similarities, &lt;code&gt;fit&lt;/code&gt; will result in a single cluster center and label &lt;code&gt;0&lt;/code&gt; for every sample. Otherwise, every training sample becomes its own cluster center and is assigned a unique label.</source>
          <target state="translated">当所有训练样本具有相同的相似性和相同的偏好时，聚类中心和标签的分配取决于偏好。如果首选项小于相似性，则 &lt;code&gt;fit&lt;/code&gt; 将导致单个聚类中心，并为每个样本标记 &lt;code&gt;0&lt;/code&gt; 。否则，每个训练样本将成为其自己的聚类中心，并被分配一个唯一的标签。</target>
        </trans-unit>
        <trans-unit id="c440a8e563c9cc7b9752f14072284d81697bdfcd" translate="yes" xml:space="preserve">
          <source>When all training samples have equal similarities and equal preferences, the assignment of cluster centers and labels depends on the preference. If the preference is smaller than the similarities, a single cluster center and label &lt;code&gt;0&lt;/code&gt; for every sample will be returned. Otherwise, every training sample becomes its own cluster center and is assigned a unique label.</source>
          <target state="translated">当所有训练样本具有相同的相似性和相同的偏好时，聚类中心和标签的分配取决于偏好。如果首选项小于相似性，则将返回单个聚类中心和每个样本的标签 &lt;code&gt;0&lt;/code&gt; 。否则，每个训练样本将成为其自己的聚类中心，并被分配一个唯一的标签。</target>
        </trans-unit>
        <trans-unit id="2494d11b03713922afdad3fa1bc52bb7064577b4" translate="yes" xml:space="preserve">
          <source>When alpha is very large, the regularization effect dominates the squared loss function and the coefficients tend to zero. At the end of the path, as alpha tends toward zero and the solution tends towards the ordinary least squares, coefficients exhibit big oscillations. In practise it is necessary to tune alpha in such a way that a balance is maintained between both.</source>
          <target state="translated">当α非常大时,正则化效应主导平方损失函数,系数趋向于零。在路径的末端,由于α趋向于零,解趋向于普通最小二乘,系数表现出大的振荡。在实际工作中,有必要对α进行调整,使两者之间保持平衡。</target>
        </trans-unit>
        <trans-unit id="e93a4fdf68d407dc869190eca6cd7dfaf57ad986" translate="yes" xml:space="preserve">
          <source>When applying LOF for outlier detection, there are no &lt;code&gt;predict&lt;/code&gt;, &lt;code&gt;decision_function&lt;/code&gt; and &lt;code&gt;score_samples&lt;/code&gt; methods but only a &lt;code&gt;fit_predict&lt;/code&gt; method. The scores of abnormality of the training samples are accessible through the &lt;code&gt;negative_outlier_factor_&lt;/code&gt; attribute. Note that &lt;code&gt;predict&lt;/code&gt;, &lt;code&gt;decision_function&lt;/code&gt; and &lt;code&gt;score_samples&lt;/code&gt; can be used on new unseen data when LOF is applied for novelty detection, i.e. when the &lt;code&gt;novelty&lt;/code&gt; parameter is set to &lt;code&gt;True&lt;/code&gt;. See &lt;a href=&quot;#novelty-with-lof&quot;&gt;Novelty detection with Local Outlier Factor&lt;/a&gt;.</source>
          <target state="translated">在将LOF应用于异常值检测时，没有 &lt;code&gt;predict&lt;/code&gt; ， &lt;code&gt;decision_function&lt;/code&gt; 和 &lt;code&gt;score_samples&lt;/code&gt; 方法，而只有 &lt;code&gt;fit_predict&lt;/code&gt; 方法。可以通过 &lt;code&gt;negative_outlier_factor_&lt;/code&gt; 属性访问训练样本的异常分数。请注意，当将LOF应用于新颖性检测时，即，当 &lt;code&gt;novelty&lt;/code&gt; 参数设置为 &lt;code&gt;True&lt;/code&gt; 时，可以对新的看不见的数据使用 &lt;code&gt;predict&lt;/code&gt; ， &lt;code&gt;decision_function&lt;/code&gt; 和 &lt;code&gt;score_samples&lt;/code&gt; 。请参阅&lt;a href=&quot;#novelty-with-lof&quot;&gt;具有局部异常值因素的新颖性检测&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="869141a5ff3e1444543cdaec8c9968684d76b66e" translate="yes" xml:space="preserve">
          <source>When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.</source>
          <target state="translated">在构建词汇时,忽略那些文档频率严格高于给定阈值的词汇(特定语料库的停止词)。如果是float,该参数代表文档的比例,整数绝对数。如果词汇不为None,则忽略该参数。</target>
        </trans-unit>
        <trans-unit id="1f13ad80586b74ad6fdc521fde036c051cf6e0e7" translate="yes" xml:space="preserve">
          <source>When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.</source>
          <target state="translated">在构建词汇时,忽略那些文档频率严格低于给定阈值的词汇。这个值在文献中也称为cut-off。如果是float,该参数代表文档的比例,整数绝对数。如果词汇不为None,则忽略该参数。</target>
        </trans-unit>
        <trans-unit id="8ed788b8f95069d89dcb15b1bea5b5e7da9a9648" translate="yes" xml:space="preserve">
          <source>When calling &lt;code&gt;fit&lt;/code&gt;, an affinity matrix is constructed using either kernel function such the Gaussian (aka RBF) kernel of the euclidean distanced &lt;code&gt;d(X, X)&lt;/code&gt;:</source>
          <target state="translated">调用 &lt;code&gt;fit&lt;/code&gt; 时，将使用任一个核函数（例如，距离为 &lt;code&gt;d(X, X)&lt;/code&gt; 的欧式）的高斯（aka RBF）核构造亲和矩阵：</target>
        </trans-unit>
        <trans-unit id="da9c4333516559f145c3dcae1d1fd9f5e0da891d" translate="yes" xml:space="preserve">
          <source>When doing classification in scikit-learn, &lt;code&gt;y&lt;/code&gt; is a vector of integers or strings.</source>
          <target state="translated">在scikit-learn中进行分类时， &lt;code&gt;y&lt;/code&gt; 是整数或字符串的向量。</target>
        </trans-unit>
        <trans-unit id="fb08dbb1ad477236e66b72b0cef9bccfd1ceec61" translate="yes" xml:space="preserve">
          <source>When doing supervised learning, a simple sanity check consists of comparing one&amp;rsquo;s estimator against simple rules of thumb. &lt;a href=&quot;generated/sklearn.dummy.dummyclassifier#sklearn.dummy.DummyClassifier&quot;&gt;&lt;code&gt;DummyClassifier&lt;/code&gt;&lt;/a&gt; implements several such simple strategies for classification:</source>
          <target state="translated">在进行监督学习时，简单的健全性检查包括将自己的估计量与简单的经验法则进行比较。&lt;a href=&quot;generated/sklearn.dummy.dummyclassifier#sklearn.dummy.DummyClassifier&quot;&gt; &lt;code&gt;DummyClassifier&lt;/code&gt; &lt;/a&gt;实现了几种这样的简单分类策略：</target>
        </trans-unit>
        <trans-unit id="3bf9748662d1dbe365a15ba24b002d016f11408b" translate="yes" xml:space="preserve">
          <source>When evaluating different settings (&amp;ldquo;hyperparameters&amp;rdquo;) for estimators, such as the &lt;code&gt;C&lt;/code&gt; setting that must be manually set for an SVM, there is still a risk of overfitting &lt;em&gt;on the test set&lt;/em&gt; because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can &amp;ldquo;leak&amp;rdquo; into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called &amp;ldquo;validation set&amp;rdquo;: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.</source>
          <target state="translated">当评估用于估计不同的设置（&amp;ldquo;超参数&amp;rdquo;），如 &lt;code&gt;C&lt;/code&gt; 必须为一个SVM手动设定的设定，仍然存在过度拟合的风险&lt;em&gt;在测试组&lt;/em&gt;，因为这些参数可以进行调整，直到估计器执行最优。这样，有关测试集的知识可以&amp;ldquo;渗入&amp;rdquo;模型，并且评估指标不再报告泛化性能。为了解决此问题，可以将数据集的另一部分保留为所谓的&amp;ldquo;验证集&amp;rdquo;：对训练集进行训练，然后对验证集进行评估，以及实验何时成功，可以对测试集进行最终评估。</target>
        </trans-unit>
        <trans-unit id="b6d52e3adfb55f9048c5ef57545d77d640ae7db4" translate="yes" xml:space="preserve">
          <source>When evaluating text classifiers on the 20 Newsgroups data, you should strip newsgroup-related metadata. In scikit-learn, you can do this by setting &lt;code&gt;remove=('headers', 'footers', 'quotes')&lt;/code&gt;. The F-score will be lower because it is more realistic.</source>
          <target state="translated">当评估20个新闻组数据上的文本分类器时，应删除与新闻组相关的元数据。在scikit-learn中，您可以通过设置 &lt;code&gt;remove=('headers', 'footers', 'quotes')&lt;/code&gt; 。F分数将较低，因为它更现实。</target>
        </trans-unit>
        <trans-unit id="b44ace677df67ec762b5f7d213266d4181953b1c" translate="yes" xml:space="preserve">
          <source>When evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process: it is recommended to split the data into a &lt;strong&gt;development set&lt;/strong&gt; (to be fed to the &lt;code&gt;GridSearchCV&lt;/code&gt; instance) and an &lt;strong&gt;evaluation set&lt;/strong&gt; to compute performance metrics.</source>
          <target state="translated">在评估结果模型时，重要的是对在网格搜索过程中未看到的保留样本进行处理：建议将数据拆分为&lt;strong&gt;开发集&lt;/strong&gt;（供入 &lt;code&gt;GridSearchCV&lt;/code&gt; 实例）和&lt;strong&gt;评估集&lt;/strong&gt;计算性能指标。</target>
        </trans-unit>
        <trans-unit id="1a86b5b9ee94c593acad1b7968fd00ab3b487df9" translate="yes" xml:space="preserve">
          <source>When feature values are strings, this transformer will do a binary one-hot (aka one-of-K) coding: one boolean-valued feature is constructed for each of the possible string values that the feature can take on. For instance, a feature &amp;ldquo;f&amp;rdquo; that can take on the values &amp;ldquo;ham&amp;rdquo; and &amp;ldquo;spam&amp;rdquo; will become two features in the output, one signifying &amp;ldquo;f=ham&amp;rdquo;, the other &amp;ldquo;f=spam&amp;rdquo;.</source>
          <target state="translated">当特征值是字符串时，此转换器将执行二进制一键式（又名K之一）编码：为该功能可以采用的每个可能的字符串值构造一个布尔值特征。例如，可以采用值&amp;ldquo; ham&amp;rdquo;和&amp;ldquo; spam&amp;rdquo;的功能&amp;ldquo; f&amp;rdquo;将成为输出中的两个功能，一个表示&amp;ldquo; f = ham&amp;rdquo;，另一个表示&amp;ldquo; f = spam&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="e3d8c3576d6baaf6f77dc4e34223e91a48a8c662" translate="yes" xml:space="preserve">
          <source>When fitting a model to a matrix X_train and evaluating it against a matrix X_test, it is essential that X_train and X_test have the same number of features (X_train.shape[1] == X_test.shape[1]). This may not be the case if you load the files individually with load_svmlight_file.</source>
          <target state="translated">当将模型拟合到一个矩阵X_train并对矩阵X_test进行评估时,X_train和X_test必须具有相同数量的特征(X_train.shape[1]==X_test.shape[1])。如果您使用 load_svmlight_file 单独加载文件,情况可能不是这样。</target>
        </trans-unit>
        <trans-unit id="618912ddd6add9bc398f8cf8343e255bb5f0ac59" translate="yes" xml:space="preserve">
          <source>When in doubt, use &lt;a href=&quot;#ransac-regression&quot;&gt;RANSAC&lt;/a&gt;</source>
          <target state="translated">如有疑问，请使用&lt;a href=&quot;#ransac-regression&quot;&gt;RANSAC&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="7ebc317e66e94c9c3ce7d1b975021eefa97b880b" translate="yes" xml:space="preserve">
          <source>When individual estimators are fast to train or predict using &lt;code&gt;n_jobs&amp;gt;1&lt;/code&gt; can result in slower performance due to the overhead of spawning processes.</source>
          <target state="translated">当使用 &lt;code&gt;n_jobs&amp;gt;1&lt;/code&gt; 快速估计或预测单个估计量时，由于生成过程的开销而导致性能降低。</target>
        </trans-unit>
        <trans-unit id="5223d409a8ec7650bd8559a52e22ddb32a950ada" translate="yes" xml:space="preserve">
          <source>When loss=&amp;rdquo;modified_huber&amp;rdquo;, probability estimates may be hard zeros and ones, so taking the logarithm is not possible.</source>
          <target state="translated">当损失=&amp;ldquo; modified_huber&amp;rdquo;时，概率估计可能是硬零和一，因此不可能采用对数。</target>
        </trans-unit>
        <trans-unit id="63fb46936ec9f0c71e69b897d1f9d91941d422e3" translate="yes" xml:space="preserve">
          <source>When modeling text corpora, the model assumes the following generative process for a corpus with \(D\) documents and \(K\) topics:</source>
          <target state="translated">当对文本语料库进行建模时,该模型假设一个语料库具有 \(D\)文档和 \(K\)主题的生成过程如下。</target>
        </trans-unit>
        <trans-unit id="744347c999c68da3080dd50ae1b985e4b97e385e" translate="yes" xml:space="preserve">
          <source>When one has insufficiently many points per mixture, estimating the covariance matrices becomes difficult, and the algorithm is known to diverge and find solutions with infinite likelihood unless one regularizes the covariances artificially.</source>
          <target state="translated">当人们对每个混合物的点数不够多时,估计协方差矩阵就会变得很困难,除非人为地将协方差正则化,否则该算法就会出现分歧,并找到具有无限似然的解。</target>
        </trans-unit>
        <trans-unit id="e24f09e860711fd3a63a21415134601f90e4efc0" translate="yes" xml:space="preserve">
          <source>When parametrized by error using the parameter &lt;code&gt;tol&lt;/code&gt;: argmin ||gamma||_0 subject to ||y - Xgamma||^2 &amp;lt;= tol</source>
          <target state="translated">使用参数 &lt;code&gt;tol&lt;/code&gt; 进行参数错误设置时：argmin || gamma || _0服从|| y-Xgamma || ^ 2 &amp;lt;= tol</target>
        </trans-unit>
        <trans-unit id="70b3334d846e26366c45a04fb1a4a39af8e3ec45" translate="yes" xml:space="preserve">
          <source>When parametrized by the number of non-zero coefficients using &lt;code&gt;n_nonzero_coefs&lt;/code&gt;: argmin ||y - Xgamma||^2 subject to ||gamma||_0 &amp;lt;= n_{nonzero coefs}</source>
          <target state="translated">当使用 &lt;code&gt;n_nonzero_coefs&lt;/code&gt; 对非零系数的数量进行参数化时：argmin || y-Xgamma || ^ 2服从|| gamma || _0 &amp;lt;= n_ {nonzero coefs}</target>
        </trans-unit>
        <trans-unit id="10d326cee514d3b967129b254954126bcda90793" translate="yes" xml:space="preserve">
          <source>When performing classification one often wants to predict not only the class label, but also the associated probability. This probability gives some kind of confidence on the prediction. This example demonstrates how to display how well calibrated the predicted probabilities are and how to calibrate an uncalibrated classifier.</source>
          <target state="translated">在进行分类时,人们往往不仅要预测类标签,还要预测相关的概率。这个概率给了预测的某种信心。这个例子演示了如何显示预测概率的校准情况,以及如何校准未校准的分类器。</target>
        </trans-unit>
        <trans-unit id="533cc79868c5585705042f97bd64dbf9b1c6133f" translate="yes" xml:space="preserve">
          <source>When performing classification you often want not only to predict the class label, but also obtain a probability of the respective label. This probability gives you some kind of confidence on the prediction. Some models can give you poor estimates of the class probabilities and some even do not support probability prediction. The calibration module allows you to better calibrate the probabilities of a given model, or to add support for probability prediction.</source>
          <target state="translated">在进行分类时,你往往不仅要预测类标签,还要获得各自标签的概率。这个概率给你对预测的某种信心。有些模型可以给你很差的类概率估计,有些甚至不支持概率预测。校准模块可以让你更好地校准给定模型的概率,或者增加对概率预测的支持。</target>
        </trans-unit>
        <trans-unit id="ede14543224daf4bd7da97ebebdbcb4bdb8fe514" translate="yes" xml:space="preserve">
          <source>When performing classification you often want to predict not only the class label, but also the associated probability. This probability gives you some kind of confidence on the prediction. However, not all classifiers provide well-calibrated probabilities, some being over-confident while others being under-confident. Thus, a separate calibration of predicted probabilities is often desirable as a postprocessing. This example illustrates two different methods for this calibration and evaluates the quality of the returned probabilities using Brier&amp;rsquo;s score (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Brier_score&quot;&gt;https://en.wikipedia.org/wiki/Brier_score&lt;/a&gt;).</source>
          <target state="translated">执行分类时，您通常不仅要预测类别标签，还要预测相关的概率。这种可能性使您对预测有某种信心。但是，并非所有分类器都提供了经过良好校准的概率，有些分类器过于自信，而另一些分类器则不太自信。因此，通常期望对预测概率进行单独的校准作为后处理。本示例说明了此校准的两种不同方法，并使用Brier分数评估了返回的概率的质量（请参见&lt;a href=&quot;https://en.wikipedia.org/wiki/Brier_score&quot;&gt;https://en.wikipedia.org/wiki/Brier_score&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="e99cb78745b2020137c83400ee3d8d22f37293c0" translate="yes" xml:space="preserve">
          <source>When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified, ensuring X is C-contiguous. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean, in this case it will also not ensure that data is C-contiguous which may cause a significant slowdown.</source>
          <target state="translated">在预计算距离时,先将数据居中,在数值上更准确。如果copy_x为True(默认),则不修改原始数据,保证X是C-contiguous。如果False,则修改原始数据,在函数返回之前放回原处,但可能会通过减去数据均值再加上数据均值而引入小的数值差异,这种情况下也不能保证数据是C-连续的,可能会造成明显的减速。</target>
        </trans-unit>
        <trans-unit id="55ac178846c6596090a6f8d0133ba62fcd26aebb" translate="yes" xml:space="preserve">
          <source>When predicting, the true labels will not be available. Instead the predictions of each model are passed on to the subsequent models in the chain to be used as features.</source>
          <target state="translated">在预测时,将无法获得真实的标签。相反,每个模型的预测将被传递给链中的后续模型,作为特征使用。</target>
        </trans-unit>
        <trans-unit id="441463f4c28b96b4ca0739417ce251a6d1637336" translate="yes" xml:space="preserve">
          <source>When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces &lt;a href=&quot;#h1998&quot; id=&quot;id3&quot;&gt;[H1998]&lt;/a&gt;.</source>
          <target state="translated">当将数据集的随机子集绘制为要素的随机子集时，该方法称为随机子空间&lt;a href=&quot;#h1998&quot; id=&quot;id3&quot;&gt;[H1998]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="ed3a388f4c5d9809937b1b2fceecd5d5d41437fc" translate="yes" xml:space="preserve">
          <source>When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting &lt;a href=&quot;#b1999&quot; id=&quot;id1&quot;&gt;[B1999]&lt;/a&gt;.</source>
          <target state="translated">当将数据集的随机子集绘制为样本的随机子集时，该算法称为粘贴&lt;a href=&quot;#b1999&quot; id=&quot;id1&quot;&gt;[B1999]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="f494cf947867cf3181ff2c5eaa996adc8d0ce783" translate="yes" xml:space="preserve">
          <source>When requesting a dataset with a name that is in mock_datasets, this object creates a fake dataset in a StringIO object and returns it. Otherwise, it raises an HTTPError.</source>
          <target state="translated">当请求一个名字在mock_datasets中的数据集时,这个对象会在StringIO对象中创建一个假的数据集并返回。否则,它会引发一个HTTPError。</target>
        </trans-unit>
        <trans-unit id="d66c9681351cfa02a7cc3145d8e8cc7e7a3877b3" translate="yes" xml:space="preserve">
          <source>When samples are drawn with replacement, then the method is known as Bagging &lt;a href=&quot;#b1996&quot; id=&quot;id2&quot;&gt;[B1996]&lt;/a&gt;.</source>
          <target state="translated">当抽取样本进行替换时，该方法称为Bagging &lt;a href=&quot;#b1996&quot; id=&quot;id2&quot;&gt;[B1996]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="b67d588266cee585b7c0608db5b116728771921f" translate="yes" xml:space="preserve">
          <source>When self.fit_intercept is True, instance vector x becomes &lt;code&gt;[x, self.intercept_scaling]&lt;/code&gt;, i.e. a &amp;ldquo;synthetic&amp;rdquo; feature with constant value equals to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased.</source>
          <target state="translated">当self.fit_intercept为True时，实例向量x变为 &lt;code&gt;[x, self.intercept_scaling]&lt;/code&gt; ，即，将具有等于intercept_scaling的恒定值的&amp;ldquo;合成&amp;rdquo;特征附加到实例向量。截距变为intercept_scaling *综合特征权重注意！与所有其他特征一样，合成特征权重也要经过l1 / l2正则化。为了减轻正则化对合成特征权重（以及因此对截距）的影响，必须增加intercept_scaling。</target>
        </trans-unit>
        <trans-unit id="339fba0d20ce2052ad9daa9e3c6cc55589d832bc" translate="yes" xml:space="preserve">
          <source>When self.fit_intercept is True, instance vector x becomes [x, self.intercept_scaling], i.e. a &amp;ldquo;synthetic&amp;rdquo; feature with constant value equals to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased.</source>
          <target state="translated">当self.fit_intercept为True时，实例矢量x变为[x，self.intercept_scaling]，即，将常量值等于intercept_scaling的&amp;ldquo;合成&amp;rdquo;特征附加到实例矢量。截距变为intercept_scaling *综合特征权重注意！与所有其他特征一样，合成特征权重也要经过l1 / l2正则化。为了减轻正则化对合成特征权重（以及因此对截距）的影响，必须增加intercept_scaling。</target>
        </trans-unit>
        <trans-unit id="ad06c0ce7c1fc91387cf888768a953ddfc43c4b6" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;False&lt;/code&gt;, ignore special characters for PostScript compatibility.</source>
          <target state="translated">设置为 &lt;code&gt;False&lt;/code&gt; 时，请忽略特殊字符以实现PostScript兼容性。</target>
        </trans-unit>
        <trans-unit id="d1b93df55570608a785000a0ebdde0c1a478b680" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, change the display of &amp;lsquo;values&amp;rsquo; and/or &amp;lsquo;samples&amp;rsquo; to be proportions and percentages respectively.</source>
          <target state="translated">设置为 &lt;code&gt;True&lt;/code&gt; 时，将&amp;ldquo;值&amp;rdquo;和/或&amp;ldquo;样本&amp;rdquo;的显示分别更改为比例和百分比。</target>
        </trans-unit>
        <trans-unit id="3843d20a53a1ef3b59460be1fa7cb77d3c2ee1f6" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, draw all leaf nodes at the bottom of the tree.</source>
          <target state="translated">设置为 &lt;code&gt;True&lt;/code&gt; 时，在树的底部绘制所有叶节点。</target>
        </trans-unit>
        <trans-unit id="1dd8441b2d9ea649f69d55eb070005c43dff3ea1" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, draw node boxes with rounded corners and use Helvetica fonts instead of Times-Roman.</source>
          <target state="translated">设置为 &lt;code&gt;True&lt;/code&gt; 时，绘制带有圆角的节点框，并使用Helvetica字体代替Times-Roman。</target>
        </trans-unit>
        <trans-unit id="37e5e7c90dcc2881b20280bd58a636a2601aa0c6" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, forces the coefficients to be positive.</source>
          <target state="translated">设置为 &lt;code&gt;True&lt;/code&gt; 时，将系数强制为正。</target>
        </trans-unit>
        <trans-unit id="a556178389cf10e9a8f97ab94b5cbc137168d877" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, orient tree left to right rather than top-down.</source>
          <target state="translated">设置为 &lt;code&gt;True&lt;/code&gt; 时，将树从左到右而不是自上而下定向。</target>
        </trans-unit>
        <trans-unit id="db2d0a81092e78238cdb916143e482a964d5a789" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, paint nodes to indicate majority class for classification, extremity of values for regression, or purity of node for multi-output.</source>
          <target state="translated">设置为 &lt;code&gt;True&lt;/code&gt; 时，绘制节点以表示多数类用于分类，值的极值用于回归，或表示节点的纯度用于多输出。</target>
        </trans-unit>
        <trans-unit id="387704ecb7f5fc4e8c941c08ae18f72e58104663" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">设置为 &lt;code&gt;True&lt;/code&gt; 时，请重用上一个调用的解决方案以适合并在集合中添加更多的估计量，否则，只需擦除以前的解决方案即可。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e08becb1d7f3dabb059c61e3163efbfc5e9d6bd5" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">设置为 &lt;code&gt;True&lt;/code&gt; 时，请重用上一个调用的解决方案以适合并在集合中添加更多估计量，否则，仅适合整个新森林。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="1ddb18c96e1fca0312edb00f224f2db160c80a30" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">设置为 &lt;code&gt;True&lt;/code&gt; 时，请重用上一个调用的解决方案以适合初始化，否则，只需擦除之前的解决方案即可。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="303e5cc9af6656c2592224b864d50a2e7f014b54" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, show the ID number on each node.</source>
          <target state="translated">设置为 &lt;code&gt;True&lt;/code&gt; 时，在每个节点上显示ID号。</target>
        </trans-unit>
        <trans-unit id="d19057d05dc608969ed0862b9054b2defc3fb482" translate="yes" xml:space="preserve">
          <source>When set to &lt;code&gt;True&lt;/code&gt;, show the impurity at each node.</source>
          <target state="translated">设置为 &lt;code&gt;True&lt;/code&gt; 时，显示每个节点上的杂质。</target>
        </trans-unit>
        <trans-unit id="0199409f1eeb1aa7581c717d5c23e182af166761" translate="yes" xml:space="preserve">
          <source>When set to True, computes the averaged SGD weights and stores the result in the &lt;code&gt;coef_&lt;/code&gt; attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So &lt;code&gt;average=10&lt;/code&gt; will begin averaging after seeing 10 samples.</source>
          <target state="translated">设置为True时，将计算平均SGD权重并将结果存储在 &lt;code&gt;coef_&lt;/code&gt; 属性中。如果将int设置为大于1，则一旦看到的样本总数达到平均值，就会开始平均。因此， &lt;code&gt;average=10&lt;/code&gt; 将在看到10个样本后开始平均。</target>
        </trans-unit>
        <trans-unit id="19c4ca669b90d48df2f3aa161b89103ea08c0cbd" translate="yes" xml:space="preserve">
          <source>When set to True, computes the averaged SGD weights and stores the result in the &lt;code&gt;coef_&lt;/code&gt; attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples.</source>
          <target state="translated">设置为True时，将计算平均SGD权重并将结果存储在 &lt;code&gt;coef_&lt;/code&gt; 属性中。如果将int设置为大于1，则一旦看到的样本总数达到平均值，就会开始平均。因此，平均值= 10将在看到10个样本后开始平均。</target>
        </trans-unit>
        <trans-unit id="e60b70114bd43f63c876204c608aaaaca2e5c6d2" translate="yes" xml:space="preserve">
          <source>When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">设置为True时，请重用上一个调用的解决方案以适合并为集合添加更多估计量，否则，仅适合一个全新的集合。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0615ecfccfe9c12eea86adfdd92570d0b3a6f9cf" translate="yes" xml:space="preserve">
          <source>When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">设置为True时，请重用上一个调用的解决方案以适合初始化，否则，只需擦除以前的解决方案即可。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="869683b3c71be56286e995de01f21c989fc735d8" translate="yes" xml:space="preserve">
          <source>When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;the Glossary&lt;/a&gt;.</source>
          <target state="translated">设置为True时，请重用上一个调用的解决方案以适合初始化，否则，只需擦除以前的解决方案即可。对于liblinear求解器无用。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-warm-start&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="61362ce0a02977970071e88e9dc71d066251fcde" translate="yes" xml:space="preserve">
          <source>When specifying multiple metrics, the &lt;code&gt;refit&lt;/code&gt; parameter must be set to the metric (string) for which the &lt;code&gt;best_params_&lt;/code&gt; will be found and used to build the &lt;code&gt;best_estimator_&lt;/code&gt; on the whole dataset. If the search should not be refit, set &lt;code&gt;refit=False&lt;/code&gt;. Leaving refit to the default value &lt;code&gt;None&lt;/code&gt; will result in an error when using multiple metrics.</source>
          <target state="translated">当指定多个指标时，必须将 &lt;code&gt;refit&lt;/code&gt; 参数设置为将为其找到 &lt;code&gt;best_params_&lt;/code&gt; 的指标（字符串），并用于在整个数据集上构建 &lt;code&gt;best_estimator_&lt;/code&gt; 。如果搜索不应该进行调整，则设置 &lt;code&gt;refit=False&lt;/code&gt; 。如果将调整保留为默认值&amp;ldquo; &lt;code&gt;None&lt;/code&gt; 则在使用多个指标时将导致错误。</target>
        </trans-unit>
        <trans-unit id="765386eefc1dd2e9ed203024ee007c99e07f6618" translate="yes" xml:space="preserve">
          <source>When strategy == &amp;ldquo;constant&amp;rdquo;, fill_value is used to replace all occurrences of missing_values. If left to the default, fill_value will be 0 when imputing numerical data and &amp;ldquo;missing_value&amp;rdquo; for strings or object data types.</source>
          <target state="translated">当strategy ==&amp;ldquo; constant&amp;rdquo;时，fill_value用于替换所有missing_values出现的情况。如果保留默认值，则在填充数字数据时fill_value将为0，而对于字符串或对象数据类型则为&amp;ldquo; missing_value&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="b3973bbeeefced2bd7eb8aaa05fcc0fadc5e600b" translate="yes" xml:space="preserve">
          <source>When the &lt;code&gt;cv&lt;/code&gt; argument is an integer, &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; uses the &lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt;&lt;code&gt;KFold&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;generated/sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt;&lt;code&gt;StratifiedKFold&lt;/code&gt;&lt;/a&gt; strategies by default, the latter being used if the estimator derives from &lt;a href=&quot;generated/sklearn.base.classifiermixin#sklearn.base.ClassifierMixin&quot;&gt;&lt;code&gt;ClassifierMixin&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">当 &lt;code&gt;cv&lt;/code&gt; 参数是整数时，默认情况下，&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;使用&lt;a href=&quot;generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold&quot;&gt; &lt;code&gt;KFold&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;generated/sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold&quot;&gt; &lt;code&gt;StratifiedKFold&lt;/code&gt; &lt;/a&gt;策略，如果估计器派生自&lt;a href=&quot;generated/sklearn.base.classifiermixin#sklearn.base.ClassifierMixin&quot;&gt; &lt;code&gt;ClassifierMixin&lt;/code&gt; &lt;/a&gt;，则使用后者。</target>
        </trans-unit>
        <trans-unit id="c7d238875ff93bafa2620f7a2d7675b937a2183b" translate="yes" xml:space="preserve">
          <source>When the algorithm does not converge, it returns an empty array as &lt;code&gt;cluster_center_indices&lt;/code&gt; and &lt;code&gt;-1&lt;/code&gt; as label for each training sample.</source>
          <target state="translated">当算法不收敛，则返回一个空数组作为 &lt;code&gt;cluster_center_indices&lt;/code&gt; 和 &lt;code&gt;-1&lt;/code&gt; 作为标签每个训练样本。</target>
        </trans-unit>
        <trans-unit id="b4fdfb364ee084bf57bd04a0f7c8f0c41739edf4" translate="yes" xml:space="preserve">
          <source>When the data is not initially in the &lt;code&gt;(n_samples, n_features)&lt;/code&gt; shape, it needs to be preprocessed in order to be used by scikit-learn.</source>
          <target state="translated">当数据最初不是 &lt;code&gt;(n_samples, n_features)&lt;/code&gt; 形状时，需要对其进行预处理，以供scikit-learn使用。</target>
        </trans-unit>
        <trans-unit id="aaeec02a52e8579f06c37808c9e18fa50d637a08" translate="yes" xml:space="preserve">
          <source>When there are more than two labels, the value of the MCC will no longer range between -1 and +1. Instead the minimum value will be somewhere between -1 and 0 depending on the number and distribution of ground true labels. The maximum value is always +1.</source>
          <target state="translated">当有两个以上的标签时,MCC的值将不再在-1和+1之间。取而代之的是,最小值将介于-1和0之间,这取决于地面真标签的数量和分布。最大值总是+1。</target>
        </trans-unit>
        <trans-unit id="5c69ffd1e0dc71befa67c00726bc6370582b5df5" translate="yes" xml:space="preserve">
          <source>When there is no correlation between the outputs, a very simple way to solve this kind of problem is to build n independent models, i.e. one for each output, and then to use those models to independently predict each one of the n outputs. However, because it is likely that the output values related to the same input are themselves correlated, an often better way is to build a single model capable of predicting simultaneously all n outputs. First, it requires lower training time since only a single estimator is built. Second, the generalization accuracy of the resulting estimator may often be increased.</source>
          <target state="translated">当输出之间不存在相关性时,解决这类问题的一个非常简单的方法是建立n个独立的模型,即每个输出一个模型,然后用这些模型独立预测n个输出中的每一个。然而,由于与同一输入相关的输出值本身很可能是相关的,所以一个通常更好的方法是建立一个能够同时预测所有n个输出的单一模型。首先,由于只建立一个估计器,所以需要的训练时间较少。其次,所得到的估计器的泛化精度往往可以提高。</target>
        </trans-unit>
        <trans-unit id="8c7fa59e2b8c1ce3abb542d1d5f76caddd1816f1" translate="yes" xml:space="preserve">
          <source>When this environment variable is set to a non zero value, scikit-learn uses the site joblib rather than its vendored version. Consequently, joblib must be installed for scikit-learn to run. Note that using the site joblib is at your own risks: the versions of scikt-learn and joblib need to be compatible. In addition, dumps from joblib.Memory might be incompatible, and you might loose some caches and have to redownload some datasets.</source>
          <target state="translated">当这个环境变量被设置为非零时,scikit-learn会使用站点的joblib而不是它的销售版本。因此,scikit-learn必须安装joblib才能运行。请注意,使用 site joblib 是要自己承担风险的:scikt-learn 和 joblib 的版本需要兼容。此外,来自 joblib.Memory 的转储可能不兼容,你可能会丢失一些缓存,不得不重新下载一些数据集。</target>
        </trans-unit>
        <trans-unit id="743d4762d28a3d61488ddf12d10bce762b152657" translate="yes" xml:space="preserve">
          <source>When this environment variable is set to a non zero value, the tests that need network access are skipped.</source>
          <target state="translated">当这个环境变量被设置为非零时,需要网络访问的测试将被跳过。</target>
        </trans-unit>
        <trans-unit id="beb3490e0a85d3bcf9f4888cb75a6b1ea2e1e6a8" translate="yes" xml:space="preserve">
          <source>When training an SVM with the &lt;em&gt;Radial Basis Function&lt;/em&gt; (RBF) kernel, two parameters must be considered: &lt;code&gt;C&lt;/code&gt; and &lt;code&gt;gamma&lt;/code&gt;. The parameter &lt;code&gt;C&lt;/code&gt;, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low &lt;code&gt;C&lt;/code&gt; makes the decision surface smooth, while a high &lt;code&gt;C&lt;/code&gt; aims at classifying all training examples correctly. &lt;code&gt;gamma&lt;/code&gt; defines how much influence a single training example has. The larger &lt;code&gt;gamma&lt;/code&gt; is, the closer other examples must be to be affected.</source>
          <target state="translated">当使用&lt;em&gt;径向基函数&lt;/em&gt;（RBF）内核训练SVM时，必须考虑两个参数： &lt;code&gt;C&lt;/code&gt; 和 &lt;code&gt;gamma&lt;/code&gt; 。所有SVM内核共有的参数 &lt;code&gt;C&lt;/code&gt; 权衡了训练示例的错误分类和决策面的简单性。低 &lt;code&gt;C&lt;/code&gt; 可使决策表面平滑，而高 &lt;code&gt;C&lt;/code&gt; 旨在正确分类所有训练示例。 &lt;code&gt;gamma&lt;/code&gt; 定义单个培训示例的影响力。 &lt;code&gt;gamma&lt;/code&gt; 值越大，其他示例必须受到的影响越近。</target>
        </trans-unit>
        <trans-unit id="4f5d7a3d8a7ab119798fbec5ead8471db219e63d" translate="yes" xml:space="preserve">
          <source>When true, the result is adjusted for chance, so that random performance would score 0, and perfect performance scores 1.</source>
          <target state="translated">当为真时,根据偶然性对结果进行调整,因此随机表现将得0分,完美表现得1分。</target>
        </trans-unit>
        <trans-unit id="a756308318fb23f07be0498c8c83a0d088f9279a" translate="yes" xml:space="preserve">
          <source>When truncated SVD is applied to term-document matrices (as returned by &lt;code&gt;CountVectorizer&lt;/code&gt; or &lt;code&gt;TfidfVectorizer&lt;/code&gt;), this transformation is known as &lt;a href=&quot;http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf&quot;&gt;latent semantic analysis&lt;/a&gt; (LSA), because it transforms such matrices to a &amp;ldquo;semantic&amp;rdquo; space of low dimensionality. In particular, LSA is known to combat the effects of synonymy and polysemy (both of which roughly mean there are multiple meanings per word), which cause term-document matrices to be overly sparse and exhibit poor similarity under measures such as cosine similarity.</source>
          <target state="translated">当将截短的SVD应用于术语文档矩阵（由 &lt;code&gt;CountVectorizer&lt;/code&gt; 或 &lt;code&gt;TfidfVectorizer&lt;/code&gt; 返回）时，此转换称为&lt;a href=&quot;http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf&quot;&gt;潜在语义分析&lt;/a&gt;（LSA），因为它将此类矩阵转换为低维的&amp;ldquo;语义&amp;rdquo;空间。尤其是，众所周知，LSA可以消除同义词和多义性的影响（这两者都大致意味着每个单词具有多种含义），这会导致术语文档矩阵过于稀疏，并且在诸如余弦相似度的测量下显示出较差的相似度。</target>
        </trans-unit>
        <trans-unit id="726430099b436b4edbed2772b4e46aec59296fe0" translate="yes" xml:space="preserve">
          <source>When used for text classification with tf-idf vectors, this classifier is also known as the Rocchio classifier.</source>
          <target state="translated">当使用tf-idf向量进行文本分类时,这种分类器也被称为Rocchio分类器。</target>
        </trans-unit>
        <trans-unit id="b1373e66b4937bbac8defef2fa925bed61a8d596" translate="yes" xml:space="preserve">
          <source>When used to &lt;em&gt;transform&lt;/em&gt; data, PCA can reduce the dimensionality of the data by projecting on a principal subspace.</source>
          <target state="translated">当用于&lt;em&gt;转换&lt;/em&gt;数据时，PCA可以通过投影在主要子空间上来降低数据的维数。</target>
        </trans-unit>
        <trans-unit id="93429c223efcd8d6622a4c8e5f0e71f13358e226" translate="yes" xml:space="preserve">
          <source>When using &lt;a href=&quot;../../modules/classes#module-sklearn.multiclass&quot;&gt;&lt;code&gt;multiclass classifiers&lt;/code&gt;&lt;/a&gt;, the learning and prediction task that is performed is dependent on the format of the target data fit upon:</source>
          <target state="translated">使用&lt;a href=&quot;../../modules/classes#module-sklearn.multiclass&quot;&gt; &lt;code&gt;multiclass classifiers&lt;/code&gt; &lt;/a&gt;，执行的学习和预测任务取决于适合的目标数据的格式：</target>
        </trans-unit>
        <trans-unit id="cb56aa339cbb48c9a75d42d2c7bf7e7858b3170b" translate="yes" xml:space="preserve">
          <source>When using ensemble methods base upon bagging, i.e. generating new training sets using sampling with replacement, part of the training set remains unused. For each classifier in the ensemble, a different part of the training set is left out.</source>
          <target state="translated">当使用基于袋的合集方法时,即使用采样与替换的方法生成新的训练集,部分训练集仍未使用。对于合集中的每一个分类器,训练集的不同部分都会被遗漏。</target>
        </trans-unit>
        <trans-unit id="a9696826aefafecc5b32845bc270f3c2cabe6664" translate="yes" xml:space="preserve">
          <source>When using these images, please give credit to AT&amp;amp;T Laboratories Cambridge.</source>
          <target state="translated">使用这些图像时，请归功于AT＆T剑桥实验室。</target>
        </trans-unit>
        <trans-unit id="5a6f9e7437ff7d6762455e24a46c4771dc2e0a37" translate="yes" xml:space="preserve">
          <source>When using, for example, &lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;cross validation&lt;/a&gt;, to set the amount of regularization with &lt;code&gt;C&lt;/code&gt;, there will be a different amount of samples between the main problem and the smaller problems within the folds of the cross validation.</source>
          <target state="translated">例如，当使用&lt;a href=&quot;../../modules/cross_validation#cross-validation&quot;&gt;交叉验证&lt;/a&gt;来设置 &lt;code&gt;C&lt;/code&gt; 的正则化量时，在交叉验证范围内，主要问题和较小问题之间将存在不同数量的样本。</target>
        </trans-unit>
        <trans-unit id="5dae0cb2a4d8c33bb8e68ee9ffd452db73e27eb2" translate="yes" xml:space="preserve">
          <source>When we apply clustering to the data, we find that the clustering reflects what was in the distance matrices. Indeed, for the Euclidean distance, the classes are ill-separated because of the noise, and thus the clustering does not separate the waveforms. For the cityblock distance, the separation is good and the waveform classes are recovered. Finally, the cosine distance does not separate at all waveform 1 and 2, thus the clustering puts them in the same cluster.</source>
          <target state="translated">当我们对数据应用聚类时,我们发现聚类反映了距离矩阵中的内容。事实上,对于欧氏距离,由于噪声的存在,类的分离度不高,因此聚类不能分离波形。对于城块距离,分离度很好,波形类被恢复。最后,余弦距离完全不能分离波形1和2,因此聚类将它们放在同一个聚类中。</target>
        </trans-unit>
        <trans-unit id="8975b7dd097c19a6af3c2b4b090f357f96aab52d" translate="yes" xml:space="preserve">
          <source>When working with covariance estimation, the usual approach is to use a maximum likelihood estimator, such as the &lt;a href=&quot;../../modules/generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt;&lt;code&gt;sklearn.covariance.EmpiricalCovariance&lt;/code&gt;&lt;/a&gt;. It is unbiased, i.e. it converges to the true (population) covariance when given many observations. However, it can also be beneficial to regularize it, in order to reduce its variance; this, in turn, introduces some bias. This example illustrates the simple regularization used in &lt;a href=&quot;../../modules/covariance#shrunk-covariance&quot;&gt;Shrunk Covariance&lt;/a&gt; estimators. In particular, it focuses on how to set the amount of regularization, i.e. how to choose the bias-variance trade-off.</source>
          <target state="translated">使用协方差估计时，通常的方法是使用最大似然估计器，例如&lt;a href=&quot;../../modules/generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt; &lt;code&gt;sklearn.covariance.EmpiricalCovariance&lt;/code&gt; &lt;/a&gt;。它是无偏的，即，在进行大量观察时，它收敛到真实的（种群）协方差。但是，对它进行正则化以减少其方差也可能是有益的。这反过来带来了一些偏见。此示例说明了&lt;a href=&quot;../../modules/covariance#shrunk-covariance&quot;&gt;收缩协方差&lt;/a&gt;估计器中使用的简单正则化。特别是，它着重于如何设置正则化量，即如何选择偏差方差折衷。</target>
        </trans-unit>
        <trans-unit id="17b704aa73a46ef6f9edddecff620d33c0b705d7" translate="yes" xml:space="preserve">
          <source>When you want to apply different transformations to each field of the data, see the related class &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt; (see &lt;a href=&quot;#column-transformer&quot;&gt;user guide&lt;/a&gt;).</source>
          <target state="translated">如果要对数据的每个字段应用不同的转换，请参见相关的类&lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt; &lt;/a&gt;（请参阅&lt;a href=&quot;#column-transformer&quot;&gt;用户指南&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="ffd889b6ef09600260c419603787a00c67ae551d" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;TP&lt;/code&gt; is the number of &lt;strong&gt;True Positive&lt;/strong&gt; (i.e. the number of pair of points that belong to the same clusters in both the true labels and the predicted labels), &lt;code&gt;FP&lt;/code&gt; is the number of &lt;strong&gt;False Positive&lt;/strong&gt; (i.e. the number of pair of points that belong to the same clusters in the true labels and not in the predicted labels) and &lt;code&gt;FN&lt;/code&gt; is the number of &lt;strong&gt;False Negative&lt;/strong&gt; (i.e the number of pair of points that belongs in the same clusters in the predicted labels and not in the true labels).</source>
          <target state="translated">其中 &lt;code&gt;TP&lt;/code&gt; 是&lt;strong&gt;True Positive&lt;/strong&gt;的数量（即，在真标签和预测标签中属于同一聚类的点对的数量）， &lt;code&gt;FP&lt;/code&gt; 是&lt;strong&gt;False Positive&lt;/strong&gt;的数量（即属于&lt;strong&gt;正&lt;/strong&gt;标签的点对的数量）表示真实标签而不是预测标签中的相同聚类）， &lt;code&gt;FN&lt;/code&gt; 是&lt;strong&gt;假阴性&lt;/strong&gt;（即，在预测标签中而不是真实标签中属于同一聚类的点对的数量）。</target>
        </trans-unit>
        <trans-unit id="276f699fa82da6208d110c0a23b40d61550922dd" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;TP&lt;/code&gt; is the number of &lt;strong&gt;True Positive&lt;/strong&gt; (i.e. the number of pair of points that belongs in the same clusters in both &lt;code&gt;labels_true&lt;/code&gt; and &lt;code&gt;labels_pred&lt;/code&gt;), &lt;code&gt;FP&lt;/code&gt; is the number of &lt;strong&gt;False Positive&lt;/strong&gt; (i.e. the number of pair of points that belongs in the same clusters in &lt;code&gt;labels_true&lt;/code&gt; and not in &lt;code&gt;labels_pred&lt;/code&gt;) and &lt;code&gt;FN&lt;/code&gt; is the number of &lt;strong&gt;False Negative&lt;/strong&gt; (i.e the number of pair of points that belongs in the same clusters in &lt;code&gt;labels_pred&lt;/code&gt; and not in &lt;code&gt;labels_True&lt;/code&gt;).</source>
          <target state="translated">其中 &lt;code&gt;TP&lt;/code&gt; 是&lt;strong&gt;True正数&lt;/strong&gt;（即， &lt;code&gt;labels_true&lt;/code&gt; 和 &lt;code&gt;labels_pred&lt;/code&gt; 中属于同一群集的点对的数量）， &lt;code&gt;FP&lt;/code&gt; 是&lt;strong&gt;False Positive&lt;/strong&gt;（即，同一簇中的点对的数量）在 &lt;code&gt;labels_true&lt;/code&gt; 中而不在 &lt;code&gt;labels_pred&lt;/code&gt; 中），并且 &lt;code&gt;FN&lt;/code&gt; 是&lt;strong&gt;假负数&lt;/strong&gt;（即，在 &lt;code&gt;labels_pred&lt;/code&gt; 中属于同一集群而不在 &lt;code&gt;labels_True&lt;/code&gt; 中的点对的数量）。</target>
        </trans-unit>
        <trans-unit id="e78195e2eb2711f3bb8a0d7f4e3c56eea492c48d" translate="yes" xml:space="preserve">
          <source>Where &lt;code&gt;delta&lt;/code&gt; is a free parameter representing the width of the Gaussian kernel.</source>
          <target state="translated">其中 &lt;code&gt;delta&lt;/code&gt; 是代表高斯核宽度的自由参数。</target>
        </trans-unit>
        <trans-unit id="6d88fb8179777bb060d39d8e880a1a6ec89efb59" translate="yes" xml:space="preserve">
          <source>Where C is the number of permutations whose score &amp;gt;= the true score.</source>
          <target state="translated">其中C是其分数&amp;gt; =真实分数的排列数量。</target>
        </trans-unit>
        <trans-unit id="0426d1b8d26623c0079356962f68cf2595f6d67a" translate="yes" xml:space="preserve">
          <source>Where D is the matrix of distances for the input data X, D_fit is the matrix of distances for the output embedding X_fit, and K is the isomap kernel:</source>
          <target state="translated">其中,D为输入数据X的距离矩阵,D_fit为输出嵌入X_fit的距离矩阵,K为同构核。</target>
        </trans-unit>
        <trans-unit id="77844a8258430d31f41a5c27fd5c3c817a4c46f5" translate="yes" xml:space="preserve">
          <source>Where \(C_2^{n_{samples}}\) is the total number of possible pairs in the dataset (without ordering).</source>
          <target state="translated">其中/(C_2^{n_{samples}}/)是数据集中可能的数据对总数(不排序)。</target>
        </trans-unit>
        <trans-unit id="45f9706f8e40bfee3b8f4082279b7c1694d8aead" translate="yes" xml:space="preserve">
          <source>Where \(K\) is the precision matrix to be estimated, and \(S\) is the sample covariance matrix. \(\|K\|_1\) is the sum of the absolute values of off-diagonal coefficients of \(K\). The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R &lt;code&gt;glasso&lt;/code&gt; package.</source>
          <target state="translated">其中\（K \）是要估计的精度矩阵，\（S \）是样本协方差矩阵。\（\ | K \ | _1 \）是\（K \）的非对角系数的绝对值之和。用于解决此问题的算法是GLasso算法，该算法来自Friedman 2008 Biostatistics论文。它与R &lt;code&gt;glasso&lt;/code&gt; 软件包中的算法相同。</target>
        </trans-unit>
        <trans-unit id="23038cc6fb25ab648004d5485267f6db76cb9eda" translate="yes" xml:space="preserve">
          <source>Where \(N(x_i)\) is the neighborhood of samples within a given distance around \(x_i\) and \(m\) is the &lt;em&gt;mean shift&lt;/em&gt; vector that is computed for each centroid that points towards a region of the maximum increase in the density of points. This is computed using the following equation, effectively updating a centroid to be the mean of the samples within its neighborhood:</source>
          <target state="translated">其中\（N（x_i）\）是在\（x_i \）周围给定距离内的样本邻域，而\（m \）是针对每个指向最大增加区域的质心计算的&lt;em&gt;平均偏移&lt;/em&gt;矢量在点的密度。这是使用以下公式计算的，可以有效地将质心更新为其附近样本的平均值：</target>
        </trans-unit>
        <trans-unit id="c4be16a40b65a723b122e1219966e4db066c1f56" translate="yes" xml:space="preserve">
          <source>Where \(R\) is the diagonal matrix with entry \(i\) equal to \(\sum_{j} A_{ij}\) and \(C\) is the diagonal matrix with entry \(j\) equal to \(\sum_{i} A_{ij}\).</source>
          <target state="translated">其中/(R/\)是对角矩阵,条目/(i/)等于/(sum_{j}A_{ij}/),而/(C/)是对角矩阵,条目/(j)等于/(sum_{i}A_{ij}/)。</target>
        </trans-unit>
        <trans-unit id="06bd15907339a321f45a7707ee037e3bf4bb294c" translate="yes" xml:space="preserve">
          <source>Where \(\langle \cdot, \cdot \rangle\) denotes the inner product in the Hilbert space.</source>
          <target state="translated">其中,(langle cdot,cdot rangle/)表示Hilbert空间的内积。</target>
        </trans-unit>
        <trans-unit id="b505305e3f68e0055136674f6671623549265da7" translate="yes" xml:space="preserve">
          <source>Where \(\log_e (x)\) means the natural logarithm of \(x\). This metric is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate.</source>
          <target state="translated">其中\(\log_e (x)\)是指\(x)的自然对数。当目标具有指数增长时,如人口数量、某商品多年的平均销售量等,最好使用这种度量方法。需要注意的是,这一指标对预测不足的估计值比预测过高的估计值进行惩罚。</target>
        </trans-unit>
        <trans-unit id="dc652afd01d2a671ac597240d27fcb8fb6f2cb88" translate="yes" xml:space="preserve">
          <source>Where \(s(i, k)\) is the similarity between samples \(i\) and \(k\). The availability of sample \(k\) to be the exemplar of sample \(i\) is given by:</source>
          <target state="translated">其中,(s(i,k))是样本(i)和(k)之间的相似度。样本(k)是否能成为样本(i)的典范由以下公式给出:</target>
        </trans-unit>
        <trans-unit id="9fa1e5b532b4ed4720d23061371103281dda3d83" translate="yes" xml:space="preserve">
          <source>Where r is defined per sample, we need to make use of &lt;code&gt;start&lt;/code&gt;:</source>
          <target state="translated">其中每个样本定义了r，我们需要使用 &lt;code&gt;start&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="c16b06fa7e959786262fbf5823a1d1a66514be0c" translate="yes" xml:space="preserve">
          <source>Where the step length \(\gamma_m\) is chosen using line search:</source>
          <target state="translated">其中步长/(gamma_m/)是用行搜索选择的。</target>
        </trans-unit>
        <trans-unit id="1e4c7785f80a06d28d2e2b965c377764abc1c026" translate="yes" xml:space="preserve">
          <source>Where to from here</source>
          <target state="translated">从这里去哪里</target>
        </trans-unit>
        <trans-unit id="17eb390ca1dec9880beb722610077dafb8edc9ac" translate="yes" xml:space="preserve">
          <source>Where u and v are any rows taken from a dataset of shape [n_samples, n_features] and p is a projection by a random Gaussian N(0, 1) matrix with shape [n_components, n_features] (or a sparse Achlioptas matrix).</source>
          <target state="translated">其中u和v是从形状为[n_样本,n_特征]的数据集中取出的任意行,p是由形状为[n_成分,n_特征]的随机高斯N(0,1)矩阵(或稀疏的Achlioptas矩阵)进行投影。</target>
        </trans-unit>
        <trans-unit id="c5759c4abe89df832c23e0269eba38e4f1ad0ad7" translate="yes" xml:space="preserve">
          <source>Where u and v are any rows taken from a dataset of shape [n_samples, n_features], eps is in ]0, 1[ and p is a projection by a random Gaussian N(0, 1) matrix with shape [n_components, n_features] (or a sparse Achlioptas matrix).</source>
          <target state="translated">其中u和v是从形状为[n_samples,n_features]的数据集中取出的任意行,eps是在]0,1[中,p是由形状为[n_components,n_features]的随机高斯N(0,1)矩阵(或稀疏的Achlioptas矩阵)投影。</target>
        </trans-unit>
        <trans-unit id="7e741bc3dcef0123eeda11543758853be2aac149" translate="yes" xml:space="preserve">
          <source>Where:</source>
          <target state="translated">Where:</target>
        </trans-unit>
        <trans-unit id="07f1abf8acdb3dcd49bde3ee8a201c3421831b31" translate="yes" xml:space="preserve">
          <source>Whether &lt;code&gt;feature_names_&lt;/code&gt; and &lt;code&gt;vocabulary_&lt;/code&gt; should be sorted when fitting. True by default.</source>
          <target state="translated">拟合时是否应该对 &lt;code&gt;feature_names_&lt;/code&gt; 和 &lt;code&gt;vocabulary_&lt;/code&gt; 进行排序。默认为True。</target>
        </trans-unit>
        <trans-unit id="e61b5eefa6a0d8caaa65c0cf06600523e6eded8c" translate="yes" xml:space="preserve">
          <source>Whether a forced copy will be triggered. If copy=False, a copy might be triggered by a conversion.</source>
          <target state="translated">是否会触发强制复制。如果copy=False,则可能通过转换来触发复制。</target>
        </trans-unit>
        <trans-unit id="6817dee267fec06b200988a35092c9fafdb28df4" translate="yes" xml:space="preserve">
          <source>Whether a prefit model is expected to be passed into the constructor directly or not. If True, &lt;code&gt;transform&lt;/code&gt; must be called directly and SelectFromModel cannot be used with &lt;code&gt;cross_val_score&lt;/code&gt;, &lt;code&gt;GridSearchCV&lt;/code&gt; and similar utilities that clone the estimator. Otherwise train the model using &lt;code&gt;fit&lt;/code&gt; and then &lt;code&gt;transform&lt;/code&gt; to do feature selection.</source>
          <target state="translated">预设模型是否应该直接传递给构造函数。如果为True，则必须直接调用 &lt;code&gt;transform&lt;/code&gt; ,并且SelectFromModel不能与 &lt;code&gt;cross_val_score&lt;/code&gt; ， &lt;code&gt;GridSearchCV&lt;/code&gt; 和克隆估算器的类似实用程序一起使用。否则，使用 &lt;code&gt;fit&lt;/code&gt; 训练模型，然后进行 &lt;code&gt;transform&lt;/code&gt; 以进行特征选择。</target>
        </trans-unit>
        <trans-unit id="e67f675f1639113224879739e0229eb2671df0d3" translate="yes" xml:space="preserve">
          <source>Whether an array will be forced to be fortran or c-style.</source>
          <target state="translated">一个数组是否会被强制为fortran或c-style。</target>
        </trans-unit>
        <trans-unit id="57d1f88164d54372295bc307285273118c662089" translate="yes" xml:space="preserve">
          <source>Whether an array will be forced to be fortran or c-style. When order is None (default), then if copy=False, nothing is ensured about the memory layout of the output array; otherwise (copy=True) the memory layout of the returned array is kept as close as possible to the original array.</source>
          <target state="translated">数组是否会被强制为fortran或c-style。当order为None(默认)时,如果copy=False,则不保证输出数组的内存布局;否则(copy=True),返回数组的内存布局将尽可能地接近原始数组。</target>
        </trans-unit>
        <trans-unit id="f6541283616583040ce3e73f070cbb436dbc5da9" translate="yes" xml:space="preserve">
          <source>Whether bootstrap samples are used when building trees.</source>
          <target state="translated">建树时是否使用引导样本。</target>
        </trans-unit>
        <trans-unit id="344e324f858395af07d0534305d3cd485a522869" translate="yes" xml:space="preserve">
          <source>Whether column indices in f are zero-based (True) or one-based (False). If column indices are one-based, they are transformed to zero-based to match Python/NumPy conventions. If set to &amp;ldquo;auto&amp;rdquo;, a heuristic check is applied to determine this from the file contents. Both kinds of files occur &amp;ldquo;in the wild&amp;rdquo;, but they are unfortunately not self-identifying. Using &amp;ldquo;auto&amp;rdquo; or True should always be safe when no &lt;code&gt;offset&lt;/code&gt; or &lt;code&gt;length&lt;/code&gt; is passed. If &lt;code&gt;offset&lt;/code&gt; or &lt;code&gt;length&lt;/code&gt; are passed, the &amp;ldquo;auto&amp;rdquo; mode falls back to &lt;code&gt;zero_based=True&lt;/code&gt; to avoid having the heuristic check yield inconsistent results on different segments of the file.</source>
          <target state="translated">f中的列索引是从零开始（真）还是从一开始（假）。如果列索引基于1，则将其转换为基于0的索引，以匹配Python / NumPy约定。如果设置为&amp;ldquo;自动&amp;rdquo;，则将应用启发式检查从文件内容中确定。两种文件都&amp;ldquo;野外&amp;rdquo;出现，但不幸的是它们无法自我识别。当没有 &lt;code&gt;offset&lt;/code&gt; 或 &lt;code&gt;length&lt;/code&gt; 传递时，使用&amp;ldquo; auto&amp;rdquo;或&amp;ldquo; True&amp;rdquo;应该总是安全的。如果传递了 &lt;code&gt;offset&lt;/code&gt; 或 &lt;code&gt;length&lt;/code&gt; ，则&amp;ldquo;自动&amp;rdquo;模式会退回到 &lt;code&gt;zero_based=True&lt;/code&gt; ,以避免在文件的不同段上启发式检查产生不一致的结果。</target>
        </trans-unit>
        <trans-unit id="f00bfe1387e4927051573cb3d574287560206c66" translate="yes" xml:space="preserve">
          <source>Whether column indices in f are zero-based (True) or one-based (False). If column indices are one-based, they are transformed to zero-based to match Python/NumPy conventions. If set to &amp;ldquo;auto&amp;rdquo;, a heuristic check is applied to determine this from the file contents. Both kinds of files occur &amp;ldquo;in the wild&amp;rdquo;, but they are unfortunately not self-identifying. Using &amp;ldquo;auto&amp;rdquo; or True should always be safe when no offset or length is passed. If offset or length are passed, the &amp;ldquo;auto&amp;rdquo; mode falls back to zero_based=True to avoid having the heuristic check yield inconsistent results on different segments of the file.</source>
          <target state="translated">f中的列索引是从零开始（真）还是从一开始（假）。如果列索引基于1，则将其转换为基于0的索引，以匹配Python / NumPy约定。如果设置为&amp;ldquo;自动&amp;rdquo;，则将应用启发式检查从文件内容中确定。两种文件都&amp;ldquo;野外&amp;rdquo;出现，但不幸的是它们无法自我识别。当没有偏移或长度传递时，使用&amp;ldquo; auto&amp;rdquo;或&amp;ldquo; True&amp;rdquo;应该总是安全的。如果传递了偏移量或长度，则&amp;ldquo;自动&amp;rdquo;模式会退回到zero_based = True，以避免在文件的不同段上启发式检查产生不一致的结果。</target>
        </trans-unit>
        <trans-unit id="be05ee9a303aba9073e602f5af4606db8dba467c" translate="yes" xml:space="preserve">
          <source>Whether column indices should be written zero-based (True) or one-based (False).</source>
          <target state="translated">列索引是否应该写成基于零的(True)或基于一的(False)。</target>
        </trans-unit>
        <trans-unit id="426c392bb98aa08b186e867710abfa024378fa01" translate="yes" xml:space="preserve">
          <source>Whether features are drawn with replacement.</source>
          <target state="translated">是否特征画与替换。</target>
        </trans-unit>
        <trans-unit id="b1153e9c8e50c0d286811aaf218a31fa047ae93d" translate="yes" xml:space="preserve">
          <source>Whether or not a second normalization of the weights is performed. The default behavior mirrors the implementations found in Mahout and Weka, which do not follow the full algorithm described in Table 9 of the paper.</source>
          <target state="translated">是否对权重进行第二次归一化。默认行为反映了Mahout和Weka中的实现,它们没有遵循本文表9中描述的完整算法。</target>
        </trans-unit>
        <trans-unit id="1500a013d74d8899d6c67c8489e35470d425474d" translate="yes" xml:space="preserve">
          <source>Whether or not the model should use an intercept, i.e. a biased hyperplane, is controlled by the parameter &lt;code&gt;fit_intercept&lt;/code&gt;.</source>
          <target state="translated">模型是否应使用截距（即，偏置超平面）由参数 &lt;code&gt;fit_intercept&lt;/code&gt; 控制。</target>
        </trans-unit>
        <trans-unit id="c17699d69c93a1c9674665b22c836f689e7a71a8" translate="yes" xml:space="preserve">
          <source>Whether or not the training data should be shuffled after each epoch.</source>
          <target state="translated">是否应该在每个纪元后对训练数据进行洗牌。</target>
        </trans-unit>
        <trans-unit id="2f96d4cd24a907c94c91a597b369db5ae9d183fe" translate="yes" xml:space="preserve">
          <source>Whether or not the training data should be shuffled after each epoch. Defaults to True.</source>
          <target state="translated">训练数据是否应该在每个纪元后进行洗牌。默认值为True。</target>
        </trans-unit>
        <trans-unit id="21e287c040da0ab9f7612eda4a9df397d21447be" translate="yes" xml:space="preserve">
          <source>Whether or not to compute labels for each fit.</source>
          <target state="translated">是否为每个拟合计算标签。</target>
        </trans-unit>
        <trans-unit id="ae7627e3aed47d9187a8dde354d4bd8908f66e18" translate="yes" xml:space="preserve">
          <source>Whether or not to consider raw Mahalanobis distances as the decision function. Must be False (default) for compatibility with the others outlier detection tools.</source>
          <target state="translated">是否考虑原始马哈兰诺比距离作为决策函数。必须为False(默认),以便与其他异常点检测工具兼容。</target>
        </trans-unit>
        <trans-unit id="d7d36162415efc2dece0359749393df764e8f212" translate="yes" xml:space="preserve">
          <source>Whether or not to fit the intercept. This can be set to False if the data is already centered around the origin.</source>
          <target state="translated">是否拟合截距。如果数据已经以原点为中心,可以设置为False。</target>
        </trans-unit>
        <trans-unit id="99ecd63bc30b233e173e08d6a58d2b609112b08a" translate="yes" xml:space="preserve">
          <source>Whether or not to make a copy of the given data. If set to False, the initial data will be overwritten.</source>
          <target state="translated">是否对给定的数据进行复制,如果设置为False,则会覆盖初始数据。如果设置为False,初始数据将被覆盖。</target>
        </trans-unit>
        <trans-unit id="0d8aa347bdbdfa6dd79051c8fee2f95fc5666522" translate="yes" xml:space="preserve">
          <source>Whether or not to mark each sample as the first nearest neighbor to itself. If &lt;code&gt;None&lt;/code&gt;, then True is used for mode=&amp;rsquo;connectivity&amp;rsquo; and False for mode=&amp;rsquo;distance&amp;rsquo; as this will preserve backwards compatibility.</source>
          <target state="translated">是否将每个样本标记为其自身的第一个最近邻居。如果为 &lt;code&gt;None&lt;/code&gt; ，则将true用作mode ='connectivity'，将False用作mode ='distance'，因为这将保留向后兼容性。</target>
        </trans-unit>
        <trans-unit id="a1d6eb056f01f6a77d40d6f787612d8008f1be4b" translate="yes" xml:space="preserve">
          <source>Whether or not to return a sparse CSR matrix, as default behavior, or to return a dense array compatible with dense pipeline operators.</source>
          <target state="translated">是否返回一个稀疏的CSR矩阵,作为默认行为,或者返回一个与密集管道操作者兼容的密集数组。</target>
        </trans-unit>
        <trans-unit id="8f592bf838896fb605ecc15c063970bf58250eab" translate="yes" xml:space="preserve">
          <source>Whether or not to return the number of iterations.</source>
          <target state="translated">是否返回迭代次数。</target>
        </trans-unit>
        <trans-unit id="3433b032133d6a741db0a6ccce9ce8005a84877d" translate="yes" xml:space="preserve">
          <source>Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.</source>
          <target state="translated">是否在分割前对数据进行洗牌。如果shuffle=False,那么stratify必须为None。</target>
        </trans-unit>
        <trans-unit id="d797771ee8d7ac8a664344b3d1655c54bf4a7c5a" translate="yes" xml:space="preserve">
          <source>Whether or not to shuffle the data: might be important for models that make the assumption that the samples are independent and identically distributed (i.i.d.), such as stochastic gradient descent.</source>
          <target state="translated">是否对数据进行洗牌:对于假设样本是独立和同分布(i.i.d.)的模型来说可能很重要,比如随机梯度下降模型。</target>
        </trans-unit>
        <trans-unit id="169aa17090e9b82766c818a4a09acd1cb51fcb24" translate="yes" xml:space="preserve">
          <source>Whether samples are drawn with replacement.</source>
          <target state="translated">是否抽样与更换。</target>
        </trans-unit>
        <trans-unit id="b61c81ce74fc75ce6a90c790bfafe984d14c7994" translate="yes" xml:space="preserve">
          <source>Whether score_func is a score function (default), meaning high is good, or a loss function, meaning low is good. In the latter case, the scorer object will sign-flip the outcome of the score_func.</source>
          <target state="translated">score_func是一个得分函数(默认),意思是高就是好,还是一个损失函数,意思是低就是好。在后一种情况下,评分器对象将对 score_func 的结果进行符号翻转。</target>
        </trans-unit>
        <trans-unit id="01c7a3b4947bb7aed2272a78dd753a14b9e24fdc" translate="yes" xml:space="preserve">
          <source>Whether score_func requires predict_proba to get probability estimates out of a classifier.</source>
          <target state="translated">score_func是否需要predict_proba来获得分类器的概率估计。</target>
        </trans-unit>
        <trans-unit id="e86601bb1c2db478564381eb9b1a33fc72f3ad69" translate="yes" xml:space="preserve">
          <source>Whether score_func takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method.</source>
          <target state="translated">score_func是否采取连续的决策确定性。这只适用于使用具有decision_function或predict_proba方法的估计器的二元分类。</target>
        </trans-unit>
        <trans-unit id="62e384e32324c7d8c05ac06534cda98d3cbc0def" translate="yes" xml:space="preserve">
          <source>Whether support is a list of indices.</source>
          <target state="translated">支持是否是一个指数列表。</target>
        </trans-unit>
        <trans-unit id="970ea0e030e6e4be6469b0282edfb558e0e9066d" translate="yes" xml:space="preserve">
          <source>Whether the algorithm should be applied to M.T instead of M. The result should approximately be the same. The &amp;lsquo;auto&amp;rsquo; mode will trigger the transposition if M.shape[1] &amp;gt; M.shape[0] since this implementation of randomized SVD tend to be a little faster in that case.</source>
          <target state="translated">该算法是否应应用于MT而不是M。结果应大致相同。如果M.shape [1]&amp;gt; M.shape [0]，则&amp;ldquo;自动&amp;rdquo;模式将触发换位，因为在这种情况下，这种随机化SVD的实现往往会更快一些。</target>
        </trans-unit>
        <trans-unit id="3226951d2ead1297a694651602f8538f5e93757c" translate="yes" xml:space="preserve">
          <source>Whether the covariance vector Xy must be copied by the algorithm. If False, it may be overwritten.</source>
          <target state="translated">协方差向量Xy是否必须被算法复制。如果为False,则可能被覆盖。</target>
        </trans-unit>
        <trans-unit id="d2b667c3b7746e3e678455d8b2d703997aeb5e60" translate="yes" xml:space="preserve">
          <source>Whether the deflation be done on a copy. Let the default value to True unless you don&amp;rsquo;t care about side effects</source>
          <target state="translated">是否在副本上进行紧缩。除非您不关心副作用，否则将默认值设置为True</target>
        </trans-unit>
        <trans-unit id="1cba92780e42c1ebe55ada467260206516898de8" translate="yes" xml:space="preserve">
          <source>Whether the deflation should be done on a copy. Let the default value to True unless you don&amp;rsquo;t care about side effect</source>
          <target state="translated">是否应在副​​本上进行紧缩。除非您不关心副作用，否则将默认值设置为True</target>
        </trans-unit>
        <trans-unit id="99f0df0508624fcfafef99671905c951f197a398" translate="yes" xml:space="preserve">
          <source>Whether the design matrix X must be copied by the algorithm. A false value is only helpful if X is already Fortran-ordered, otherwise a copy is made anyway.</source>
          <target state="translated">设计矩阵X是否必须被算法复制。假值只有在X已经是Fortran排序的情况下才有帮助,否则还是会复制。</target>
        </trans-unit>
        <trans-unit id="f62ef19aafedabcbf9aa2a6c1cbcccaa900e840f" translate="yes" xml:space="preserve">
          <source>Whether the feature should be made of word or character n-grams.</source>
          <target state="translated">该功能是由字词还是字符n格组成。</target>
        </trans-unit>
        <trans-unit id="9c1354d44a66effd212e821b1aba74fe08612248" translate="yes" xml:space="preserve">
          <source>Whether the feature should be made of word or character n-grams. Option &amp;lsquo;char_wb&amp;rsquo; creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.</source>
          <target state="translated">该功能部件应由单词还是字符n-gram组成。选项&amp;ldquo; char_wb&amp;rdquo;仅从单词边界内的文本创建字符n-gram；单词边缘的n-gram用空格填充。</target>
        </trans-unit>
        <trans-unit id="824ad07968fefbc8aa1fba0ade7c849f0d099562" translate="yes" xml:space="preserve">
          <source>Whether the gram matrix must be copied by the algorithm. A false value is only helpful if it is already Fortran-ordered, otherwise a copy is made anyway.</source>
          <target state="translated">克矩阵是否必须被算法复制。假值只有在已经是Fortran排序的情况下才有帮助,否则还是会复制。</target>
        </trans-unit>
        <trans-unit id="23571fa5c9f0e8d5b2ce0915807aa480e23639d3" translate="yes" xml:space="preserve">
          <source>Whether the imputer mask format should be sparse or dense.</source>
          <target state="translated">imputer mask格式应该是稀疏的还是密集的。</target>
        </trans-unit>
        <trans-unit id="b70758cdff0513f8822d9fb7393f16dda3f13af9" translate="yes" xml:space="preserve">
          <source>Whether the imputer mask should represent all or a subset of features.</source>
          <target state="translated">imputer掩码是应该代表全部特征还是子集特征。</target>
        </trans-unit>
        <trans-unit id="11709e37813efd2480c1d891188cf0d8201c8a69" translate="yes" xml:space="preserve">
          <source>Whether the intercept should be estimated or not. If &lt;code&gt;False&lt;/code&gt;, the data is assumed to be already centered.</source>
          <target state="translated">是否应该估计截距。如果为 &lt;code&gt;False&lt;/code&gt; ，则假定数据已经居中。</target>
        </trans-unit>
        <trans-unit id="0ccd5f6458ed970eecf03c787120d2831e50f140" translate="yes" xml:space="preserve">
          <source>Whether the intercept should be estimated or not. If False, the data is assumed to be already centered.</source>
          <target state="translated">是否应该估计截距。如果为 &quot;False&quot;,则假设数据已经居中。</target>
        </trans-unit>
        <trans-unit id="2fafec857734808cd372cb104d91a568d25acbf6" translate="yes" xml:space="preserve">
          <source>Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True.</source>
          <target state="translated">是否应该估计截距。如果为False,则假设数据已经居中。默认值为True。</target>
        </trans-unit>
        <trans-unit id="efe887a2120302f3ddf508ce39115dacb905298c" translate="yes" xml:space="preserve">
          <source>Whether the parameter was found to be a named parameter of the estimator&amp;rsquo;s fit method.</source>
          <target state="translated">是否发现参数是估计器的fit方法的命名参数。</target>
        </trans-unit>
        <trans-unit id="e3e6070e7b1bf06bd46c63ade906ee83f84569ff" translate="yes" xml:space="preserve">
          <source>Whether the power iterations are normalized with step-by-step QR factorization (the slowest but most accurate), &amp;lsquo;none&amp;rsquo; (the fastest but numerically unstable when &lt;code&gt;n_iter&lt;/code&gt; is large, e.g. typically 5 or larger), or &amp;lsquo;LU&amp;rsquo; factorization (numerically stable but can lose slightly in accuracy). The &amp;lsquo;auto&amp;rsquo; mode applies no normalization if &lt;code&gt;n_iter&lt;/code&gt; &amp;lt;= 2 and switches to LU otherwise.</source>
          <target state="translated">功率迭代是通过逐步QR因式分解（最慢但最准确），&amp;ldquo;无&amp;rdquo;（当 &lt;code&gt;n_iter&lt;/code&gt; 大（例如通常为5或更大）时最快但在数值上不稳定）或&amp;ldquo; LU&amp;rdquo;因式分解（以数字方式）进行归一化稳定，但精度可能会略有下降）。如果 &lt;code&gt;n_iter&lt;/code&gt; &amp;lt;= 2，则&amp;ldquo;自动&amp;rdquo;模式不应用规范化，否则切换到LU。</target>
        </trans-unit>
        <trans-unit id="00c2bc5048e0182a905dad0c4dec40740dd521b8" translate="yes" xml:space="preserve">
          <source>Whether the relationship is increasing or decreasing.</source>
          <target state="translated">关系是增加还是减少。</target>
        </trans-unit>
        <trans-unit id="dc0314689b038e45038d5534e1499b2766f8b916" translate="yes" xml:space="preserve">
          <source>Whether the return value is an array of sparse matrix depends on the type of the input X.</source>
          <target state="translated">返回值是否是稀疏矩阵的数组,取决于输入X的类型。</target>
        </trans-unit>
        <trans-unit id="ed44b1dc96dab239fb6ac2dc375f2ee5fe3ae793" translate="yes" xml:space="preserve">
          <source>Whether the target values y are normalized, i.e., the mean of the observed target values become zero. This parameter should be set to True if the target values&amp;rsquo; mean is expected to differ considerable from zero. When enabled, the normalization effectively modifies the GP&amp;rsquo;s prior based on the data, which contradicts the likelihood principle; normalization is thus disabled per default.</source>
          <target state="translated">目标值y是否被归一化，即观察到的目标值的平均值变为零。如果期望目标值的均值与零的差值很大，则应将此参数设置为True。启用后，归一化会根据数据有效地修改GP的先验，这与似然原理相矛盾；因此默认情况下禁用标准化。</target>
        </trans-unit>
        <trans-unit id="4d56de522c2f2c8fa83698a0d6921644d35a5428" translate="yes" xml:space="preserve">
          <source>Whether the task is a classification task, in which case stratified KFold will be used.</source>
          <target state="translated">任务是否为分类任务,在这种情况下将使用分层KFold。</target>
        </trans-unit>
        <trans-unit id="6cb255093bce0eff775d5414b35fcd6fbd07931b" translate="yes" xml:space="preserve">
          <source>Whether this is a multilabel classifier</source>
          <target state="translated">是否为多标签分类器</target>
        </trans-unit>
        <trans-unit id="b1be5efdade94da8e67722b4ba2f983efa0225c5" translate="yes" xml:space="preserve">
          <source>Whether to allow 2-d y (array or sparse matrix). If false, y will be validated as a vector. y cannot have np.nan or np.inf values if multi_output=True.</source>
          <target state="translated">是否允许2-d的y(数组或稀疏矩阵),如果为false,y将被验证为向量。如果multi_output=True,y将被验证为一个向量。如果multi_output=True,y不能有np.nan或np.inf值。</target>
        </trans-unit>
        <trans-unit id="8ecc2d4e014d3f2172c25597969963bd34e18f05" translate="yes" xml:space="preserve">
          <source>Whether to allow X.ndim &amp;gt; 2.</source>
          <target state="translated">是否允许X.ndim&amp;gt; 2。</target>
        </trans-unit>
        <trans-unit id="d5dcbf9253f384309cfbc4a594ef7b057c1cb7e6" translate="yes" xml:space="preserve">
          <source>Whether to also return the code U or just the dictionary V.</source>
          <target state="translated">是否也返回代码U或只返回字典V。</target>
        </trans-unit>
        <trans-unit id="c6289192e1f815e2a760fe289e8841e73f51c42c" translate="yes" xml:space="preserve">
          <source>Whether to be verbose.</source>
          <target state="translated">是否要啰嗦。</target>
        </trans-unit>
        <trans-unit id="ccada94fe77cfa7bf4094a2aaa2265ce9f9f4e5f" translate="yes" xml:space="preserve">
          <source>Whether to cache downloaded datasets using joblib.</source>
          <target state="translated">是否使用joblib缓存下载的数据集。</target>
        </trans-unit>
        <trans-unit id="86614eccba121d18979d29b5e6a1aceed9dc9343" translate="yes" xml:space="preserve">
          <source>Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (e.g. data is expected to be already centered).</source>
          <target state="translated">是否计算该模型的截距。如果设置为 &quot;假&quot;,则在计算中不使用截距(例如,预计数据已经居中)。</target>
        </trans-unit>
        <trans-unit id="943768cddf06aea5cdead26cd3dff69cb74196ef" translate="yes" xml:space="preserve">
          <source>Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).</source>
          <target state="translated">是否计算该模型的截距。如果设置为false,在计算中不使用截距(例如,预计数据已经居中)。</target>
        </trans-unit>
        <trans-unit id="27f8a383f138130dd1f45baee7b1d68ad4ce59f4" translate="yes" xml:space="preserve">
          <source>Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be already centered).</source>
          <target state="translated">是否计算该模型的截距。如果设置为false,则计算中不使用截距(即预计数据已经居中)。</target>
        </trans-unit>
        <trans-unit id="0336ff17d311b84566800e5cf35b415ab2b27f38" translate="yes" xml:space="preserve">
          <source>Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations.</source>
          <target state="translated">是否计算该模型的截距。如果设置为false,则计算中不使用截距。</target>
        </trans-unit>
        <trans-unit id="471ed2aff4494fad57e380482ee0a58232333b20" translate="yes" xml:space="preserve">
          <source>Whether to check that &lt;code&gt;transform&lt;/code&gt; followed by &lt;code&gt;inverse_transform&lt;/code&gt; or &lt;code&gt;func&lt;/code&gt; followed by &lt;code&gt;inverse_func&lt;/code&gt; leads to the original targets.</source>
          <target state="translated">是否检查 &lt;code&gt;transform&lt;/code&gt; 后跟 &lt;code&gt;inverse_transform&lt;/code&gt; 还是检查 &lt;code&gt;func&lt;/code&gt; 跟 &lt;code&gt;inverse_func&lt;/code&gt; 导致原始目标。</target>
        </trans-unit>
        <trans-unit id="3c61d748141856e990caff79c0b01fd8a4086321" translate="yes" xml:space="preserve">
          <source>Whether to check that or &lt;code&gt;func&lt;/code&gt; followed by &lt;code&gt;inverse_func&lt;/code&gt; leads to the original inputs. It can be used for a sanity check, raising a warning when the condition is not fulfilled.</source>
          <target state="translated">是先检查还是在 &lt;code&gt;func&lt;/code&gt; 后面加上 &lt;code&gt;inverse_func&lt;/code&gt; 导致原始输入。它可以用于完整性检查，在不满足条件时发出警告。</target>
        </trans-unit>
        <trans-unit id="50850a0df7fa2328560b0d7ebe31ee1142901517" translate="yes" xml:space="preserve">
          <source>Whether to compute &lt;code&gt;y_&lt;/code&gt; is increasing (if set to True) or decreasing (if set to False)</source>
          <target state="translated">计算 &lt;code&gt;y_&lt;/code&gt; 是增加（如果设置为True）还是减少（如果设置为False）</target>
        </trans-unit>
        <trans-unit id="5fbe7e9ef9385d70942ace9201de11e6cead8f8d" translate="yes" xml:space="preserve">
          <source>Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned.</source>
          <target state="translated">是否计算平方误差法线或误差法线。如果为真(默认),返回平方误差法线。如果为False,则返回误差法线。</target>
        </trans-unit>
        <trans-unit id="01086dba4779bb032a62d90a9f9152dc1b833baf" translate="yes" xml:space="preserve">
          <source>Whether to copy X and Y, or perform in-place computations.</source>
          <target state="translated">是复制X和Y,还是进行原地计算。</target>
        </trans-unit>
        <trans-unit id="725302de0fd34aabdbfc0e0affbd4f4fb754127c" translate="yes" xml:space="preserve">
          <source>Whether to copy X and Y, or perform in-place normalization.</source>
          <target state="translated">是复制X和Y,还是进行就地归一化。</target>
        </trans-unit>
        <trans-unit id="7462b314a4fe2fff7847f10014abb6b1183fa84f" translate="yes" xml:space="preserve">
          <source>Whether to copy X and operate on the copy or perform in-place operations.</source>
          <target state="translated">是复制X并对副本进行操作,还是进行原地操作。</target>
        </trans-unit>
        <trans-unit id="3692936737114d51a6bb410cb3531454c9ad1d68" translate="yes" xml:space="preserve">
          <source>Whether to copy the precomputed covariance matrix; if False, it may be overwritten.</source>
          <target state="translated">是否复制预先计算的协方差矩阵;如果为False,则可能被覆盖。</target>
        </trans-unit>
        <trans-unit id="324d6fe1307968790ddbb142ded331e1979517da" translate="yes" xml:space="preserve">
          <source>Whether to create a copy of X and operate on it or to perform inplace computation (default behaviour).</source>
          <target state="translated">是创建一个X的副本并对其进行操作,还是进行inplace计算(默认行为)。</target>
        </trans-unit>
        <trans-unit id="62527ff1b5ed50e080e833b6d4fd94a862b78590" translate="yes" xml:space="preserve">
          <source>Whether to drop some suboptimal thresholds which would not appear on a plotted ROC curve. This is useful in order to create lighter ROC curves.</source>
          <target state="translated">是否放弃一些不会出现在绘制的ROC曲线上的次优阈值。这对于创建更轻的ROC曲线很有用。</target>
        </trans-unit>
        <trans-unit id="9851e7fdf1748b99ff7c24c940b7ce5f334a6a27" translate="yes" xml:space="preserve">
          <source>Whether to drop the first eigenvector. For spectral embedding, this should be True as the first eigenvector should be constant vector for connected graph, but for spectral clustering, this should be kept as False to retain the first eigenvector.</source>
          <target state="translated">是否放弃第一特征向量。对于频谱嵌入,应该为True,因为第一特征向量应该是连接图的常量向量,但对于频谱聚类,应该保持为False,以保留第一特征向量。</target>
        </trans-unit>
        <trans-unit id="5f99ffcc1bc3b15acf95363130c37cd5b381a91e" translate="yes" xml:space="preserve">
          <source>Whether to enable probability estimates. This must be enabled prior to calling &lt;code&gt;fit&lt;/code&gt;, and will slow down that method.</source>
          <target state="translated">是否启用概率估计。必须在调用 &lt;code&gt;fit&lt;/code&gt; 之前将其启用，并且会降低该方法的速度。</target>
        </trans-unit>
        <trans-unit id="4a4497ffca40d00ae7a4f4bda08e5dece2337700" translate="yes" xml:space="preserve">
          <source>Whether to enforce positivity when finding the code.</source>
          <target state="translated">在找码时,是否要执行积极性。</target>
        </trans-unit>
        <trans-unit id="2920a1115fbc090ce8de93fb299e0d53a98e0404" translate="yes" xml:space="preserve">
          <source>Whether to enforce positivity when finding the dictionary</source>
          <target state="translated">在查找字典时是否要执行积极的态度</target>
        </trans-unit>
        <trans-unit id="cc323427a6369f3d4c345df0c7eab9b613a4b184" translate="yes" xml:space="preserve">
          <source>Whether to enforce positivity when finding the dictionary.</source>
          <target state="translated">在查找字典时,是否要执行积极性。</target>
        </trans-unit>
        <trans-unit id="0c86815e9d15a8d79f49100b7de99ab0894ffb4b" translate="yes" xml:space="preserve">
          <source>Whether to enforce positivity when finding the encoding.</source>
          <target state="translated">在寻找编码时,是否要强制执行正。</target>
        </trans-unit>
        <trans-unit id="67962e408072673f64867d41053d2df5bd8ffd92" translate="yes" xml:space="preserve">
          <source>Whether to ensure that y has a numeric type. If dtype of y is object, it is converted to float64. Should only be used for regression algorithms.</source>
          <target state="translated">是否确保y有一个数字类型。如果y的dtype是object,则转换为float64。应该只用于回归算法。</target>
        </trans-unit>
        <trans-unit id="a89a89f1fbc0cff73f883e4748e4c43034f0361e" translate="yes" xml:space="preserve">
          <source>Whether to filter invalid parameters or not.</source>
          <target state="translated">是否过滤无效参数。</target>
        </trans-unit>
        <trans-unit id="8bd7eb051fe8793acf6505d7ff57ac8a40be3e89" translate="yes" xml:space="preserve">
          <source>Whether to fit an intercept for the model. In this case the shape of the returned array is (n_cs, n_features + 1).</source>
          <target state="translated">是否为模型拟合一个截距。在这种情况下,返回数组的形状是(n_cs,n_features+1)。</target>
        </trans-unit>
        <trans-unit id="8e39ad37924100e174516b8fe05585ae0c11a660" translate="yes" xml:space="preserve">
          <source>Whether to include &amp;ldquo;special&amp;rdquo; label estimator or test processors.</source>
          <target state="translated">是否包括&amp;ldquo;特殊&amp;rdquo;标签估计器或测试处理器。</target>
        </trans-unit>
        <trans-unit id="84818ba086581abd044063f9dd3c5b2beb12eda7" translate="yes" xml:space="preserve">
          <source>Whether to include meta-estimators that can be constructed using an estimator as their first argument. These are currently BaseEnsemble, OneVsOneClassifier, OutputCodeClassifier, OneVsRestClassifier, RFE, RFECV.</source>
          <target state="translated">是否包含可以使用估计器作为第一个参数来构造的元估计器,目前有BaseEnsemble、OneVsOneClassifier、OutputCodeClassifier、OneVsRestClassifier、RFE、RFECV。目前有BaseEnsemble、OneVsOneClassifier、OutputCodeClassifier、OneVsRestClassifier、RFE、RFECV。</target>
        </trans-unit>
        <trans-unit id="74d7014a87bc3e04b7cb4d5881013af41aaf9d5f" translate="yes" xml:space="preserve">
          <source>Whether to include train scores.</source>
          <target state="translated">是否纳入火车成绩。</target>
        </trans-unit>
        <trans-unit id="b770fc1f2ccfc02ef3107a2ecac741b2276c37e8" translate="yes" xml:space="preserve">
          <source>Whether to learn class prior probabilities or not. If false, a uniform prior will be used.</source>
          <target state="translated">是否学习类先验概率。如果为假,将使用统一的先验概率。</target>
        </trans-unit>
        <trans-unit id="8606bf192804810fba78c6bd2b8805fda4483156" translate="yes" xml:space="preserve">
          <source>Whether to load only 10 percent of the data.</source>
          <target state="translated">是否只加载10%的数据。</target>
        </trans-unit>
        <trans-unit id="3156faf4c491e77c08d3500dd2b8c76947137632" translate="yes" xml:space="preserve">
          <source>Whether to load or not the content of the different files. If true a &amp;lsquo;data&amp;rsquo; attribute containing the text information is present in the data structure returned. If not, a filenames attribute gives the path to the files.</source>
          <target state="translated">是否加载不同文件的内容。如果为true，则在返回的数据结构中包含包含文本信息的'data'属性。如果不是，则使用filenames属性提供文件的路径。</target>
        </trans-unit>
        <trans-unit id="81f8d6a01f7cffb7f53496e9cfd84f1ce27de740" translate="yes" xml:space="preserve">
          <source>Whether to make X at least 2d.</source>
          <target state="translated">是否让X至少2d。</target>
        </trans-unit>
        <trans-unit id="2a578215f5e1bceb4eddd518c894532f84a10916" translate="yes" xml:space="preserve">
          <source>Whether to make a copy of X. If &lt;code&gt;False&lt;/code&gt;, the input X gets overwritten during fitting.</source>
          <target state="translated">是否制作X的副本。如果为 &lt;code&gt;False&lt;/code&gt; ，则在拟合期间输入X被覆盖。</target>
        </trans-unit>
        <trans-unit id="e2abdc017941ef25090c0789fe34541086dc5677" translate="yes" xml:space="preserve">
          <source>Whether to make a copy of the given data. If set to False, the initial data will be overwritten.</source>
          <target state="translated">是否对给定的数据进行复制,如果设置为False,则会覆盖初始数据。如果设置为False,初始数据将被覆盖。</target>
        </trans-unit>
        <trans-unit id="df62a350cab30808585d28e267100de2daf97811" translate="yes" xml:space="preserve">
          <source>Whether to normalize the output matrix to make the leading diagonal elements all 1</source>
          <target state="translated">是否对输出矩阵进行归一化处理,使其前导对角线元素全部为1。</target>
        </trans-unit>
        <trans-unit id="ba6415e4db38e5cea33cf7fab1a514fcf5285867" translate="yes" xml:space="preserve">
          <source>Whether to perform precomputations. Improves performance when n_targets or n_samples is very large.</source>
          <target state="translated">是否进行预计算。当n_targets或n_samples非常大时,提高性能。</target>
        </trans-unit>
        <trans-unit id="4010b2bff9133aaf08486f7fb645e8e39634583e" translate="yes" xml:space="preserve">
          <source>Whether to presort the data to speed up the finding of best splits in fitting. Auto mode by default will use presorting on dense data and default to normal sorting on sparse data. Setting presort to true on sparse data will raise an error.</source>
          <target state="translated">是否对数据进行预排序,以加快在拟合中寻找最佳分割。自动模式默认会对密集数据使用预排序,对稀疏数据默认为正常排序。在稀疏数据上将预排序设置为true,会出现错误。</target>
        </trans-unit>
        <trans-unit id="29f75c6794195c888ce8399680c96d5b14e65048" translate="yes" xml:space="preserve">
          <source>Whether to presort the data to speed up the finding of best splits in fitting. For the default settings of a decision tree on large datasets, setting this to true may slow down the training process. When using either a smaller dataset or a restricted depth, this may speed up the training.</source>
          <target state="translated">是否对数据进行预排序,以加快在拟合中寻找最佳分割。对于大数据集上决策树的默认设置,将此设置为true可能会减慢训练过程。当使用较小的数据集或限制深度时,这可能会加快训练速度。</target>
        </trans-unit>
        <trans-unit id="cc4d3f96c9bc487e50b4fc4701212f323c65bca6" translate="yes" xml:space="preserve">
          <source>Whether to print progress messages to stdout.</source>
          <target state="translated">是否将进度信息打印到stdout。</target>
        </trans-unit>
        <trans-unit id="b44ea19ee67df3ef30c54f3be25f24e67c4f3a60" translate="yes" xml:space="preserve">
          <source>Whether to raise a value error if X is not 2d.</source>
          <target state="translated">如果X不是2d,是否会引发数值错误。</target>
        </trans-unit>
        <trans-unit id="b5d06b3c83946e2cd2c05192b834ad6e01c6a1d5" translate="yes" xml:space="preserve">
          <source>Whether to raise an error on np.inf and np.nan in X. The possibilities are:</source>
          <target state="translated">是否要对X中的np.inf和np.nan提出错误,可能性有。</target>
        </trans-unit>
        <trans-unit id="eaa329f6263ead71ba810671bba1e6a99379040d" translate="yes" xml:space="preserve">
          <source>Whether to raise an error on np.inf and np.nan in X. This parameter does not influence whether y can have np.inf or np.nan values. The possibilities are:</source>
          <target state="translated">是否对X中的np.inf和np.nan产生错误,这个参数不影响y是否有np.inf或np.nan值。可能性有:</target>
        </trans-unit>
        <trans-unit id="c551edd4edb4cf1081c3e3d3eb3a36ad8f839a51" translate="yes" xml:space="preserve">
          <source>Whether to raise an error or ignore if an unknown categorical feature is present during transform (default is to raise). When this parameter is set to &amp;lsquo;ignore&amp;rsquo; and an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. In the inverse transform, an unknown category will be denoted as None.</source>
          <target state="translated">在转换过程中是否引发错误或忽略是否存在未知分类特征（默认为引发）。当此参数设置为&amp;ldquo;忽略&amp;rdquo;并且在转换过程中遇到未知类别时，此功能生成的一键编码列将全为零。在逆变换中，未知类别将表示为&amp;ldquo;无&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="55b5cf4021bca4319afc6cff07e1e1c5a8e21c80" translate="yes" xml:space="preserve">
          <source>Whether to return a one-vs-rest (&amp;lsquo;ovr&amp;rsquo;) decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one (&amp;lsquo;ovo&amp;rsquo;) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2).</source>
          <target state="translated">是否返回形状（n_samples，n_classes）的一对一（ovr）决策函数作为所有其他分类器，还是返回形状为（n_samples）的libsvm原始一对一（'ov'）决策函数，n_classes *（n_classes-1）/ 2）。</target>
        </trans-unit>
        <trans-unit id="3e008ca3901c2f4656b69b334cd2bbf88df838cc" translate="yes" xml:space="preserve">
          <source>Whether to return a one-vs-rest (&amp;lsquo;ovr&amp;rsquo;) decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one (&amp;lsquo;ovo&amp;rsquo;) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one (&amp;lsquo;ovo&amp;rsquo;) is always used as multi-class strategy.</source>
          <target state="translated">是否返回形状（n_samples，n_classes）的一对一（ovr）决策函数作为所有其他分类器，还是返回形状为（n_samples）的libsvm原始一对一（'ov'）决策函数，n_classes *（n_classes-1）/ 2）。但是，始终将&amp;ldquo;一对一&amp;rdquo;（'ovo'）用作多类别策略。</target>
        </trans-unit>
        <trans-unit id="ebccc28d7f21db5d9325894eec5fc9e6d02d15c3" translate="yes" xml:space="preserve">
          <source>Whether to return dense output even when the input is sparse. If &lt;code&gt;False&lt;/code&gt;, the output is sparse if both input arrays are sparse.</source>
          <target state="translated">是否即使输入稀疏也返回密集输出。如果为 &lt;code&gt;False&lt;/code&gt; ，则两个输入数组均为稀疏时，输出为稀疏。</target>
        </trans-unit>
        <trans-unit id="12da7a3ff0421b885caca94ef2caa65b1b016c5f" translate="yes" xml:space="preserve">
          <source>Whether to return every value of the nonzero coefficients along the forward path. Useful for cross-validation.</source>
          <target state="translated">是否沿正向路径返回非零系数的每个值。对交叉验证有用。</target>
        </trans-unit>
        <trans-unit id="6a16a084b2e44866fb8e9c04ea892f46ef9407d0" translate="yes" xml:space="preserve">
          <source>Whether to return the estimators fitted on each split.</source>
          <target state="translated">是否返回每次拆分时拟合的估计子。</target>
        </trans-unit>
        <trans-unit id="642c6fa9e4316671c7770b59f9d72b6641ef628f" translate="yes" xml:space="preserve">
          <source>Whether to return the number of iterations.</source>
          <target state="translated">是否返回迭代次数。</target>
        </trans-unit>
        <trans-unit id="b549a24da790409f4ec3623e7b38a8b83191c416" translate="yes" xml:space="preserve">
          <source>Whether to return the standard deviation of posterior prediction.</source>
          <target state="translated">是否返回后置预测的标准差。</target>
        </trans-unit>
        <trans-unit id="b33ff2a27948731af021590a907c614a500fc29d" translate="yes" xml:space="preserve">
          <source>Whether to return the standard deviation of posterior prediction. All zeros in this case.</source>
          <target state="translated">是否返回后验预测的标准差。这种情况下全部为零。</target>
        </trans-unit>
        <trans-unit id="ef9a1d5fe208dc604ffba3d0e60394e32f61bdb0" translate="yes" xml:space="preserve">
          <source>Whether to scale X and Y.</source>
          <target state="translated">是否缩放X和Y。</target>
        </trans-unit>
        <trans-unit id="06ccde346dfb7273af2d0810793b1fb5e47df0dc" translate="yes" xml:space="preserve">
          <source>Whether to show informative labels for impurity, etc. Options include &amp;lsquo;all&amp;rsquo; to show at every node, &amp;lsquo;root&amp;rsquo; to show only at the top root node, or &amp;lsquo;none&amp;rsquo; to not show at any node.</source>
          <target state="translated">是否显示杂质的信息性标签等。选项包括&amp;ldquo; all&amp;rdquo;显示在每个节点上，&amp;ldquo; root&amp;rdquo;显示在顶部根节点上，或&amp;ldquo; none&amp;rdquo;显示在任何节点上不显示。</target>
        </trans-unit>
        <trans-unit id="bcd035c2f558018eebfd4e5534f5df5bef89069a" translate="yes" xml:space="preserve">
          <source>Whether to shuffle dataset.</source>
          <target state="translated">是否要洗牌数据集。</target>
        </trans-unit>
        <trans-unit id="8704d580bb26c2f6617363a0297f26abfb9fda30" translate="yes" xml:space="preserve">
          <source>Whether to shuffle each stratification of the data before splitting into batches.</source>
          <target state="translated">是否在分批前对每个分层数据进行洗牌。</target>
        </trans-unit>
        <trans-unit id="5de7b9303caa771da78304a93ebeac224ba77f9b" translate="yes" xml:space="preserve">
          <source>Whether to shuffle samples in each iteration. Only used when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;.</source>
          <target state="translated">是否在每次迭代中随机播放样本。仅在Solver ='sgd'或'adam'时使用。</target>
        </trans-unit>
        <trans-unit id="3558a0c3a77b9ce3c242cc621a2d776c46a133af" translate="yes" xml:space="preserve">
          <source>Whether to shuffle the data before splitting into batches.</source>
          <target state="translated">是否在分批前对数据进行洗牌。</target>
        </trans-unit>
        <trans-unit id="ddc8f26baf73b311e3fd82ce49af7a5449330660" translate="yes" xml:space="preserve">
          <source>Whether to shuffle the data before splitting it in batches.</source>
          <target state="translated">是否要先洗牌再分批处理数据。</target>
        </trans-unit>
        <trans-unit id="f23a63355a4e388bf6ee14cbad5c7c9c8b8c8007" translate="yes" xml:space="preserve">
          <source>Whether to shuffle the samples.</source>
          <target state="translated">是否要对样品进行洗牌。</target>
        </trans-unit>
        <trans-unit id="a5c75421672ae29d738aa02687f5d9f3ec0cf20c" translate="yes" xml:space="preserve">
          <source>Whether to shuffle training data before taking prefixes of it based on``train_sizes``.</source>
          <target state="translated">是否根据``train_sizes``对训练数据进行洗牌后再取其前缀。</target>
        </trans-unit>
        <trans-unit id="ce33fd98a79a5db67d420efb6fcabed70acb96c4" translate="yes" xml:space="preserve">
          <source>Whether to sort x before computing. If False, assume that x must be either monotonic increasing or monotonic decreasing. If True, y is used to break ties when sorting x. Make sure that y has a monotonic relation to x when setting reorder to True.</source>
          <target state="translated">计算前是否对x进行排序。如果False,假设x必须是单调递增或单调递减。如果为True,则在对x进行排序时,y是用来打破平局的,当将重排序设置为True时,要确保y与x有单调关系。</target>
        </trans-unit>
        <trans-unit id="5091491cac888f3972a1197cfef1256978c18b5f" translate="yes" xml:space="preserve">
          <source>Whether to split the sparse feature vector into the concatenation of its negative part and its positive part. This can improve the performance of downstream classifiers.</source>
          <target state="translated">是否将稀疏特征向量拆分为其负向部分和正向部分的并集。这样可以提高下游分类器的性能。</target>
        </trans-unit>
        <trans-unit id="8b58e41338c55eaf50346244bd6082e4f3c1a628" translate="yes" xml:space="preserve">
          <source>Whether to use Nesterov&amp;rsquo;s momentum. Only used when solver=&amp;rsquo;sgd&amp;rsquo; and momentum &amp;gt; 0.</source>
          <target state="translated">是否使用内斯特罗夫的势头。仅在Solver ='sgd'且动量&amp;gt; 0时使用。</target>
        </trans-unit>
        <trans-unit id="d97545296a16ed9ee24e38ded3551b9344d66c64" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram and Xy matrix to speed up calculations. Improves performance when &lt;code&gt;n_targets&lt;/code&gt; or &lt;code&gt;n_samples&lt;/code&gt; is very large. Note that if you already have such matrices, you can pass them directly to the fit method.</source>
          <target state="translated">是否使用预先计算的Gram和Xy矩阵来加快计算速度。当 &lt;code&gt;n_targets&lt;/code&gt; 或 &lt;code&gt;n_samples&lt;/code&gt; 非常大时提高性能。请注意，如果您已经有这样的矩阵，则可以将它们直接传递给fit方法。</target>
        </trans-unit>
        <trans-unit id="7dc600168d6ae4c500452eb14badcdfddafb11ff" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram matrix to speed up calculations. If set to &amp;lsquo;auto&amp;rsquo; let us decide. The Gram matrix can also be passed as argument, but it will be used only for the selection of parameter alpha, if alpha is &amp;lsquo;aic&amp;rsquo; or &amp;lsquo;bic&amp;rsquo;.</source>
          <target state="translated">是否使用预先计算的Gram矩阵来加快计算速度。如果设置为&amp;ldquo;自动&amp;rdquo;，请让我们决定。Gram矩阵也可以作为参数传递，但是如果alpha是'aic'或'bic'，则它将仅用于参数alpha的选择。</target>
        </trans-unit>
        <trans-unit id="0057a82fcc5e3ed086a2d1961e27e45b3f58597f" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram matrix to speed up calculations. If set to &lt;code&gt;'auto'&lt;/code&gt; let us decide. The Gram matrix can also be passed as argument.</source>
          <target state="translated">是否使用预先计算的Gram矩阵来加快计算速度。如果设置为 &lt;code&gt;'auto'&lt;/code&gt; 让我们决定。语法矩阵也可以作为参数传递。</target>
        </trans-unit>
        <trans-unit id="419f7e60c7c4f4f84360a1078ab4b36ce5bf208a" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram matrix to speed up calculations. If set to &lt;code&gt;'auto'&lt;/code&gt; let us decide. The Gram matrix can also be passed as argument. For sparse input this option is always &lt;code&gt;True&lt;/code&gt; to preserve sparsity.</source>
          <target state="translated">是否使用预先计算的Gram矩阵来加快计算速度。如果设置为 &lt;code&gt;'auto'&lt;/code&gt; 让我们决定。语法矩阵也可以作为参数传递。对于稀疏输入，此选项始终为 &lt;code&gt;True&lt;/code&gt; 以保留稀疏性。</target>
        </trans-unit>
        <trans-unit id="18cfe378dd4d6390fca460de4e70af73f097cdf4" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram matrix to speed up calculations. If set to &lt;code&gt;'auto'&lt;/code&gt; let us decide. The Gram matrix cannot be passed as argument since we will use only subsets of X.</source>
          <target state="translated">是否使用预先计算的Gram矩阵来加快计算速度。如果设置为 &lt;code&gt;'auto'&lt;/code&gt; 让我们决定。不能将Gram矩阵作为参数传递，因为我们将仅使用X的子集。</target>
        </trans-unit>
        <trans-unit id="2bc24bd08e69b99e2a7b63ee3e5ac92e16a75bc1" translate="yes" xml:space="preserve">
          <source>Whether to use a precomputed Gram matrix to speed up calculations. The Gram matrix can also be passed as argument. For sparse input this option is always &lt;code&gt;True&lt;/code&gt; to preserve sparsity.</source>
          <target state="translated">是否使用预先计算的Gram矩阵来加快计算速度。语法矩阵也可以作为参数传递。对于稀疏输入，此选项始终为 &lt;code&gt;True&lt;/code&gt; 以保留稀疏性。</target>
        </trans-unit>
        <trans-unit id="1efc80d653c0b8715eb15bdf1b19f3becd74a957" translate="yes" xml:space="preserve">
          <source>Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside a fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.</source>
          <target state="translated">当验证得分没有提高时,是否使用提前停止来终止训练。如果设置为True,则会自动预留一部分训练数据作为验证,当验证得分在连续的n_iter_no_change epoch中至少tol以上没有改善时,终止训练。</target>
        </trans-unit>
        <trans-unit id="9af303ba091050935df1b1843777cae63d69ca4c" translate="yes" xml:space="preserve">
          <source>Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least &lt;code&gt;tol&lt;/code&gt; for &lt;code&gt;n_iter_no_change&lt;/code&gt; consecutive epochs. Only effective when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;</source>
          <target state="translated">当验证分数没有提高时是否使用提前停止来终止训练。如果设置为true，它会自动设置的训练数据，验证预留10％，结束训练的时候验证比分没有被至少提高 &lt;code&gt;tol&lt;/code&gt; 为 &lt;code&gt;n_iter_no_change&lt;/code&gt; 连续的时期。仅在Solver ='sgd'或'adam'时有效</target>
        </trans-unit>
        <trans-unit id="998484dc55c21823abb36e2b54f5b8f623a0a41f" translate="yes" xml:space="preserve">
          <source>Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for &lt;code&gt;n_iter_no_change&lt;/code&gt; consecutive epochs. Only effective when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;</source>
          <target state="translated">当验证分数没有提高时是否使用提前停止来终止训练。如果设置为true，它将自动预留10％的训练数据作为验证，并在 &lt;code&gt;n_iter_no_change&lt;/code&gt; 连续时期验证分数没有改善至少tol时终止训练。仅在Solver ='sgd'或'adam'时有效</target>
        </trans-unit>
        <trans-unit id="cec695b146ca339e70be6934dce1a5005aa741f4" translate="yes" xml:space="preserve">
          <source>Whether to use early stopping to terminate training when validation. score is not improving. If set to True, it will automatically set aside a fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.</source>
          <target state="translated">当验证得分没有提高时,是否使用提前停止来终止训练。如果设置为True,则会自动预留一部分训练数据作为验证,当验证得分在连续的n_iter_no_change epoch中至少tol没有改善时,终止训练。</target>
        </trans-unit>
        <trans-unit id="aad0324e5e5b11eda436b08ef9513e34456be6fd" translate="yes" xml:space="preserve">
          <source>Whether to use mini-batch k-means, which is faster but may get different results.</source>
          <target state="translated">是否使用mini-batch k-means,这样速度更快,但可能会得到不同的结果。</target>
        </trans-unit>
        <trans-unit id="098e9f05a9707c21daca2709c375c5f67a6fc326" translate="yes" xml:space="preserve">
          <source>Whether to use out-of-bag samples to estimate the R^2 on unseen data.</source>
          <target state="translated">是否使用袋外样本来估计未见数据的R^2。</target>
        </trans-unit>
        <trans-unit id="cf1378e2f07c46392b05b68a8d3fb4a1b87de256" translate="yes" xml:space="preserve">
          <source>Whether to use out-of-bag samples to estimate the generalization accuracy.</source>
          <target state="translated">是否使用袋外样本来估计泛化精度。</target>
        </trans-unit>
        <trans-unit id="b9d7a8d80bd713aecc3b75a6342d626d4ac28b69" translate="yes" xml:space="preserve">
          <source>Whether to use out-of-bag samples to estimate the generalization error.</source>
          <target state="translated">是否使用袋外样本来估计泛化误差。</target>
        </trans-unit>
        <trans-unit id="3aa24f38e2caae33363ee03e7cecc2915d7b4a6e" translate="yes" xml:space="preserve">
          <source>Whether to use the shrinking heuristic.</source>
          <target state="translated">是否使用缩水启发式。</target>
        </trans-unit>
        <trans-unit id="0fcee6cb3493ee9e39e4cf5b70ffc63bc5b7fc72" translate="yes" xml:space="preserve">
          <source>Whether to zip the stored data on disk. If an integer is given, it should be between 1 and 9, and sets the amount of compression. Note that compressed arrays cannot be read by memmapping.</source>
          <target state="translated">是否将存储的数据压缩到磁盘上。如果给出一个整数,应该在1到9之间,并设置压缩量。注意,压缩后的数组不能通过memmapping读取。</target>
        </trans-unit>
        <trans-unit id="a8c18609d5573425cb13e22e1562611896bad931" translate="yes" xml:space="preserve">
          <source>Whether transform should produce scipy.sparse matrices. True by default.</source>
          <target state="translated">变换是否应该生成 scipy.sparse 矩阵。默认为true。</target>
        </trans-unit>
        <trans-unit id="96e3c213b0373954a6f42c2953a288bf58e9c8e1" translate="yes" xml:space="preserve">
          <source>Whether y_prob needs to be normalized into the bin [0, 1], i.e. is not a proper probability. If True, the smallest value in y_prob is mapped onto 0 and the largest one onto 1.</source>
          <target state="translated">y_prob 是否需要归一化到bin [0,1],即不是一个合适的概率。如果为True,则y_prob中的最小值映射到0,最大值映射到1。</target>
        </trans-unit>
        <trans-unit id="673ccf9c3156ee120e948d124a09a8f79d0986df" translate="yes" xml:space="preserve">
          <source>Which SVD method to use. If &amp;lsquo;lapack&amp;rsquo; use standard SVD from scipy.linalg, if &amp;lsquo;randomized&amp;rsquo; use fast &lt;code&gt;randomized_svd&lt;/code&gt; function. Defaults to &amp;lsquo;randomized&amp;rsquo;. For most applications &amp;lsquo;randomized&amp;rsquo; will be sufficiently precise while providing significant speed gains. Accuracy can also be improved by setting higher values for &lt;code&gt;iterated_power&lt;/code&gt;. If this is not sufficient, for maximum precision you should choose &amp;lsquo;lapack&amp;rsquo;.</source>
          <target state="translated">使用哪种SVD方法。如果&amp;ldquo; lapack&amp;rdquo;使用scipy.linalg中的标准SVD，如果&amp;ldquo;随机化&amp;rdquo;，则使用快速 &lt;code&gt;randomized_svd&lt;/code&gt; 函数。默认为&amp;ldquo;随机化&amp;rdquo;。对于大多数应用程序，&amp;ldquo;随机化&amp;rdquo;将足够精确，同时可显着提高速度。通过为 &lt;code&gt;iterated_power&lt;/code&gt; 设置更高的值也可以提高精度。如果这还不够，为了获得最大的精度，您应该选择&amp;ldquo; lapack&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="f51286fa5438dabdb2fdc9e6a35c79244bd208d2" translate="yes" xml:space="preserve">
          <source>Which affinity to use. At the moment &lt;code&gt;precomputed&lt;/code&gt; and &lt;code&gt;euclidean&lt;/code&gt; are supported. &lt;code&gt;euclidean&lt;/code&gt; uses the negative squared euclidean distance between points.</source>
          <target state="translated">使用哪个亲和力。目前支持 &lt;code&gt;precomputed&lt;/code&gt; 和 &lt;code&gt;euclidean&lt;/code&gt; 。 &lt;code&gt;euclidean&lt;/code&gt; 使用点之间的负平方欧几里得距离。</target>
        </trans-unit>
        <trans-unit id="0a291d7f5d694ce4dae77d370f938e385f2f41cf" translate="yes" xml:space="preserve">
          <source>Which kind of estimators should be returned. If None, no filter is applied and all estimators are returned. Possible values are &amp;lsquo;classifier&amp;rsquo;, &amp;lsquo;regressor&amp;rsquo;, &amp;lsquo;cluster&amp;rsquo; and &amp;lsquo;transformer&amp;rsquo; to get estimators only of these specific types, or a list of these to get the estimators that fit at least one of the types.</source>
          <target state="translated">应该返回哪种估计量。如果为None，则不应用任何过滤器，并返回所有估算器。可能的值是'classifier'，'regressor'，'cluster'和'transformer'，以仅获取这些特定类型的估计量，或者是这些列表以获取至少适合这些类型之一的估计量。</target>
        </trans-unit>
        <trans-unit id="9ab873abc046d5e79c6628d1e73d15a9a8c8a011" translate="yes" xml:space="preserve">
          <source>Which linkage criterion to use. The linkage criterion determines which distance to use between sets of features. The algorithm will merge the pairs of cluster that minimize this criterion.</source>
          <target state="translated">使用哪种联系标准。链接准则决定了在特征集之间使用哪个距离。算法将合并最小化这个标准的聚类对。</target>
        </trans-unit>
        <trans-unit id="17ba6fc895d15866ea1e60a4db791726755dc58f" translate="yes" xml:space="preserve">
          <source>Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion.</source>
          <target state="translated">使用哪种联系标准。联结准则决定了观察集之间使用哪个距离。算法将合并最小化该标准的簇对。</target>
        </trans-unit>
        <trans-unit id="7ca40022d1c5dcd68056f855d9cb29fbb48c9062" translate="yes" xml:space="preserve">
          <source>Which model is the best is a matter of subjective judgement: do we want to favor models that only capture the big picture to summarize and explain most of the structure of the data while ignoring the details or do we prefer models that closely follow the high density regions of the signal?</source>
          <target state="translated">哪种模型是最好的,这是一个主观判断的问题:我们是要偏向于只抓住大局来总结和解释数据的大部分结构而忽略细节的模型,还是偏向于紧跟信号高密度区域的模型?</target>
        </trans-unit>
        <trans-unit id="e8cddae54ed0b1b16ee030ff58543e83ff547ded" translate="yes" xml:space="preserve">
          <source>While Isomap, LLE and variants are best suited to unfold a single continuous low dimensional manifold, t-SNE will focus on the local structure of the data and will tend to extract clustered local groups of samples as highlighted on the S-curve example. This ability to group samples based on the local structure might be beneficial to visually disentangle a dataset that comprises several manifolds at once as is the case in the digits dataset.</source>
          <target state="translated">Isomap、LLE和变体最适合展开单个连续的低维歧管,而t-SNE将关注数据的局部结构,并倾向于提取S曲线示例上突出显示的聚类局部样本组。这种基于局部结构对样本进行分组的能力可能有利于直观地拆分一个同时由多个歧管组成的数据集,就像数字数据集的情况一样。</target>
        </trans-unit>
        <trans-unit id="d80a7676bafb31f5f9bcb99f7aed7e1817a9d535" translate="yes" xml:space="preserve">
          <source>While SVM models derived from &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt; and &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt; use &lt;code&gt;C&lt;/code&gt; as regularization parameter, most other estimators use &lt;code&gt;alpha&lt;/code&gt;. The exact equivalence between the amount of regularization of two models depends on the exact objective function optimized by the model. For example, when the estimator used is &lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt; regression, the relation between them is given as \(C = \frac{1}{alpha}\).</source>
          <target state="translated">虽然从&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/libsvm/&quot;&gt;libsvm&lt;/a&gt;和&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/liblinear/&quot;&gt;liblinear&lt;/a&gt;派生的SVM模型使用 &lt;code&gt;C&lt;/code&gt; 作为正则化参数，但大多数其他估计量都使用 &lt;code&gt;alpha&lt;/code&gt; 。两个模型的正则化量之间的确切等价关系取决于模型优化的精确目标函数。例如，当使用的估计量为 &lt;code&gt;sklearn.linear_model.Ridge&lt;/code&gt; 回归时，它们之间的关系为\（C = \ frac {1} {alpha} \）。</target>
        </trans-unit>
        <trans-unit id="c20daeb41107431c48223b43ad4fe138aa4845d5" translate="yes" xml:space="preserve">
          <source>While experimenting with any learning algorithm, it is important not to test the prediction of an estimator on the data used to fit the estimator as this would not be evaluating the performance of the estimator on &lt;strong&gt;new data&lt;/strong&gt;. This is why datasets are often split into &lt;em&gt;train&lt;/em&gt; and &lt;em&gt;test&lt;/em&gt; data.</source>
          <target state="translated">在尝试使用任何学习算法进行实验时，重要的是不要在用于拟合估计器的数据上测试估计器的预测，因为这将不会在&lt;strong&gt;新数据&lt;/strong&gt;上评估估计器的性能。这就是为什么数据集经常被分成&lt;em&gt;训练&lt;/em&gt;和&lt;em&gt;测试&lt;/em&gt;数据的原因。</target>
        </trans-unit>
        <trans-unit id="d04cd5b9d0463d9bc89f841d571873fec4953569" translate="yes" xml:space="preserve">
          <source>While i.i.d. data is a common assumption in machine learning theory, it rarely holds in practice. If one knows that the samples have been generated using a time-dependent process, it&amp;rsquo;s safer to use a &lt;a href=&quot;#timeseries-cv&quot;&gt;time-series aware cross-validation scheme&lt;/a&gt; Similarly if we know that the generative process has a group structure (samples from collected from different subjects, experiments, measurement devices) it safer to use &lt;a href=&quot;#group-cv&quot;&gt;group-wise cross-validation&lt;/a&gt;.</source>
          <target state="translated">尽管iid数据是机器学习理论中的常见假设，但在实践中却很少见。如果知道样品是使用时间相关的过程生成的，则使用具有&lt;a href=&quot;#timeseries-cv&quot;&gt;时间序列意识的交叉验证方案会&lt;/a&gt;更安全。类似地，如果我们知道生成过程具有组结构（从不同受试者收集的样品，实验，测量设备）更安全地使用&lt;a href=&quot;#group-cv&quot;&gt;逐组交叉验证&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="0526a7a936d1a8c691dcdaa23691304fc8ebc7ef" translate="yes" xml:space="preserve">
          <source>While in the spirit of an online algorithm, the class &lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt;&lt;code&gt;MiniBatchSparsePCA&lt;/code&gt;&lt;/a&gt; does not implement &lt;code&gt;partial_fit&lt;/code&gt; because the algorithm is online along the features direction, not the samples direction.</source>
          <target state="translated">尽管本着在线算法的精神，但&lt;a href=&quot;generated/sklearn.decomposition.minibatchsparsepca#sklearn.decomposition.MiniBatchSparsePCA&quot;&gt; &lt;code&gt;MiniBatchSparsePCA&lt;/code&gt; &lt;/a&gt;类不实现 &lt;code&gt;partial_fit&lt;/code&gt; ,因为该算法沿要素方向而不是样本方向在线。</target>
        </trans-unit>
        <trans-unit id="c8d4a37562dd2bb8e9910fd82f43df80f682c63a" translate="yes" xml:space="preserve">
          <source>While many algorithms (such as SVM, K-nearest neighbors, and logistic regression) require features to be normalized, intuitively we can think of Principle Component Analysis (PCA) as being a prime example of when normalization is important. In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the &amp;lsquo;weight&amp;rsquo; axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect.</source>
          <target state="translated">尽管许多算法（例如SVM，K近邻算法和逻辑回归）都需要对功能进行规范化，但直觉上我们可以将主成分分析（PCA）视为规范化非常重要的主要示例。在PCA中，我们对最大化方差的组件感兴趣。如果一个组件（例如，人类身高）因其各自的尺度（米与公斤）而变化的幅度小于另一组件（例如，体重），则PCA可能会确定最大方差的方向与&amp;ldquo;体重&amp;rdquo;轴更接近，如果这些特征不缩放。因为一米的高度变化比一公斤的重量变化重要得多，所以这显然是不正确的。</target>
        </trans-unit>
        <trans-unit id="3e272f5577ff59f34cc0835d41b4f8bd0e7fa207" translate="yes" xml:space="preserve">
          <source>While models saved using one version of scikit-learn might load in other versions, this is entirely unsupported and inadvisable. It should also be kept in mind that operations performed on such data could give different and unexpected results.</source>
          <target state="translated">虽然使用一个版本的 scikit-learn 保存的模型可能会在其他版本中加载,但这是完全不支持也不可取的。还应该记住,对这些数据进行的操作可能会得到不同的、意想不到的结果。</target>
        </trans-unit>
        <trans-unit id="5cdd1c0f02e03a5a1b156076678017404f8561ab" translate="yes" xml:space="preserve">
          <source>While multiclass data is provided to the metric, like binary targets, as an array of class labels, multilabel data is specified as an indicator matrix, in which cell &lt;code&gt;[i, j]&lt;/code&gt; has value 1 if sample &lt;code&gt;i&lt;/code&gt; has label &lt;code&gt;j&lt;/code&gt; and value 0 otherwise.</source>
          <target state="translated">虽然将多类数据（如二进制目标）作为类标签数组提供给度量标准，但将多标签数据指定为指标矩阵，如果样本 &lt;code&gt;i&lt;/code&gt; 具有标签 &lt;code&gt;j&lt;/code&gt; ，则单元格 &lt;code&gt;[i, j]&lt;/code&gt; 值为1 ，否则为0 。</target>
        </trans-unit>
        <trans-unit id="ed422f68a65d31027201e13d55d1a7c59ba06f45" translate="yes" xml:space="preserve">
          <source>While not particularly fast to process, Python&amp;rsquo;s &lt;code&gt;dict&lt;/code&gt; has the advantages of being convenient to use, being sparse (absent features need not be stored) and storing feature names in addition to values.</source>
          <target state="translated">尽管处理起来不是特别快，但是Python的 &lt;code&gt;dict&lt;/code&gt; 具有以下优点：易于使用，稀疏（无需存储缺少的功能）以及除值外还存储功能名称。</target>
        </trans-unit>
        <trans-unit id="6575e62afd8d9d86f5e5759f0c1f4bf12add49e4" translate="yes" xml:space="preserve">
          <source>While some local positioning information can be preserved by extracting n-grams instead of individual words, bag of words and bag of n-grams destroy most of the inner structure of the document and hence most of the meaning carried by that internal structure.</source>
          <target state="translated">虽然通过提取n-gram而不是单个单词,可以保留一些局部定位信息,但单词袋和n-gram袋破坏了文档的大部分内部结构,从而破坏了该内部结构所承载的大部分意义。</target>
        </trans-unit>
        <trans-unit id="f9eb71a899b2efe02a5e86463ae919f1b60ed0f7" translate="yes" xml:space="preserve">
          <source>While the &lt;a href=&quot;generated/sklearn.decomposition.truncatedsvd#sklearn.decomposition.TruncatedSVD&quot;&gt;&lt;code&gt;TruncatedSVD&lt;/code&gt;&lt;/a&gt; transformer works with any (sparse) feature matrix, using it on tf&amp;ndash;idf matrices is recommended over raw frequency counts in an LSA/document processing setting. In particular, sublinear scaling and inverse document frequency should be turned on (&lt;code&gt;sublinear_tf=True, use_idf=True&lt;/code&gt;) to bring the feature values closer to a Gaussian distribution, compensating for LSA&amp;rsquo;s erroneous assumptions about textual data.</source>
          <target state="translated">尽管&lt;a href=&quot;generated/sklearn.decomposition.truncatedsvd#sklearn.decomposition.TruncatedSVD&quot;&gt; &lt;code&gt;TruncatedSVD&lt;/code&gt; &lt;/a&gt;转换器可与任何（稀疏）特征矩阵配合使用，但建议在LSA /文档处理设置中的原始频率计数上，在tf&amp;ndash;idf矩阵上使用它。特别是，应该打开亚线性缩放和逆文档频率（ &lt;code&gt;sublinear_tf=True, use_idf=True&lt;/code&gt; ），以使特征值更接近高斯分布，从而补偿LSA对文本数据的错误假设。</target>
        </trans-unit>
        <trans-unit id="29a2650e25fbd5181744282ce886a2b96e4ac93d" translate="yes" xml:space="preserve">
          <source>While the above example sets the &lt;code&gt;standardize&lt;/code&gt; option to &lt;code&gt;False&lt;/code&gt;, &lt;a href=&quot;generated/sklearn.preprocessing.powertransformer#sklearn.preprocessing.PowerTransformer&quot;&gt;&lt;code&gt;PowerTransformer&lt;/code&gt;&lt;/a&gt; will apply zero-mean, unit-variance normalization to the transformed output by default.</source>
          <target state="translated">尽管上面的示例将 &lt;code&gt;standardize&lt;/code&gt; 选项设置为 &lt;code&gt;False&lt;/code&gt; ，但&lt;a href=&quot;generated/sklearn.preprocessing.powertransformer#sklearn.preprocessing.PowerTransformer&quot;&gt; &lt;code&gt;PowerTransformer&lt;/code&gt; &lt;/a&gt;默认将零均值，单位方差归一化应用于转换后的输出。</target>
        </trans-unit>
        <trans-unit id="e50cabfff84e84e9590f53c8299406f023e4e46d" translate="yes" xml:space="preserve">
          <source>While the hyperparameters chosen by optimizing LML have a considerable larger LML, they perform slightly worse according to the log-loss on test data. The figure shows that this is because they exhibit a steep change of the class probabilities at the class boundaries (which is good) but have predicted probabilities close to 0.5 far away from the class boundaries (which is bad) This undesirable effect is caused by the Laplace approximation used internally by GPC.</source>
          <target state="translated">虽然通过优化LML选择的超参数具有相当大的LML,但根据测试数据上的对数损失,它们的表现稍差。从图中可以看出,这是因为它们在类边界处表现出类概率的陡峭变化(这是好的),但在远离类边界处的预测概率接近0.5(这是坏的)这种不良影响是由GPC内部使用的拉普拉斯近似引起的。</target>
        </trans-unit>
        <trans-unit id="17e6df329175ba9fee759ed839a4afe469b184fd" translate="yes" xml:space="preserve">
          <source>While the tf&amp;ndash;idf normalization is often very useful, there might be cases where the binary occurrence markers might offer better features. This can be achieved by using the &lt;code&gt;binary&lt;/code&gt; parameter of &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;. In particular, some estimators such as &lt;a href=&quot;naive_bayes#bernoulli-naive-bayes&quot;&gt;Bernoulli Naive Bayes&lt;/a&gt; explicitly model discrete boolean random variables. Also, very short texts are likely to have noisy tf&amp;ndash;idf values while the binary occurrence info is more stable.</source>
          <target state="translated">尽管tf&amp;ndash;idf归一化通常非常有用，但在某些情况下，二进制出现标记可能会提供更好的功能。这可以通过使用&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt;的 &lt;code&gt;binary&lt;/code&gt; 参数来实现。特别是，某些估计器（例如&lt;a href=&quot;naive_bayes#bernoulli-naive-bayes&quot;&gt;Bernoulli Naive Bayes）&lt;/a&gt;明确地对离散布尔随机变量建模。同样，很短的文本可能带有嘈杂的tf&amp;ndash;idf值，而二进制出现信息则更稳定。</target>
        </trans-unit>
        <trans-unit id="ed181b0221327c005991b804ef6da84808fa6b75" translate="yes" xml:space="preserve">
          <source>While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional data.</source>
          <target state="translated">虽然这些例子给出了一些关于算法的直觉,但这种直觉可能不适用于非常高维的数据。</target>
        </trans-unit>
        <trans-unit id="20e141fdf1f5d5d16051f029790d3b9e841c723d" translate="yes" xml:space="preserve">
          <source>While using a grid of parameter settings is currently the most widely used method for parameter optimization, other search methods have more favourable properties. &lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt;&lt;code&gt;RandomizedSearchCV&lt;/code&gt;&lt;/a&gt; implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:</source>
          <target state="translated">尽管使用参数设置网格是当前最广泛用于参数优化的方法，但其他搜索方法具有更有利的属性。&lt;a href=&quot;generated/sklearn.model_selection.randomizedsearchcv#sklearn.model_selection.RandomizedSearchCV&quot;&gt; &lt;code&gt;RandomizedSearchCV&lt;/code&gt; &lt;/a&gt;实现对参数的随机搜索，其中每个设置都是从​​可能的参数值的分布中采样的。与详尽搜索相比，这有两个主要优点：</target>
        </trans-unit>
        <trans-unit id="ae7c1638fd1917cb535ca7c68b4bd5f19a47ea30" translate="yes" xml:space="preserve">
          <source>White kernel.</source>
          <target state="translated">白仁。</target>
        </trans-unit>
        <trans-unit id="6eaf9e8193566018ccba0d72a95d7647c23f2585" translate="yes" xml:space="preserve">
          <source>Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions.</source>
          <target state="translated">白化会从变换后的信号中去除一些信息(分量的相对方差尺度),但有时可以通过使其数据尊重一些硬性假设来提高下游估计器的预测精度。</target>
        </trans-unit>
        <trans-unit id="07aaa00ad7b994406ce70b8bd7598f7e15e6859a" translate="yes" xml:space="preserve">
          <source>Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometimes improve the predictive accuracy of the downstream estimators by making data respect some hard-wired assumptions.</source>
          <target state="translated">白化会从变换后的信号中删除一些信息(分量的相对方差尺度),但有时可以通过使数据尊重一些硬性假设来提高下游估计器的预测精度。</target>
        </trans-unit>
        <trans-unit id="b5ed864ec9d16ad31c6639d1d4c3bf64e3372001" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for Davies-Bouldin index.</source>
          <target state="translated">维基百科上的Davies-Bouldin索引条目。</target>
        </trans-unit>
        <trans-unit id="a0184957526e21d06d99d8f077fe30eb4aaec4f9" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for contingency matrix</source>
          <target state="translated">维基百科上的应急矩阵条目</target>
        </trans-unit>
        <trans-unit id="55a3b17abc1268c1d436ba897c97b456b993b4ea" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the (normalized) Mutual Information</source>
          <target state="translated">维基百科上的(标准化)相互信息条目</target>
        </trans-unit>
        <trans-unit id="1f069c9fec7504cb4f8a493de2e1b54ffc547081" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Adjusted Mutual Information</source>
          <target state="translated">维基百科上的调整后相互信息条目</target>
        </trans-unit>
        <trans-unit id="ca2fe3eff096e2c0ff94d3c0f6ce61af74cc646f" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Brier score.</source>
          <target state="translated">维基百科上的布里尔得分条目。</target>
        </trans-unit>
        <trans-unit id="ffd655e9eb3a21416da69aac696bc5ce043a000f" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Cohen&amp;rsquo;s kappa.</source>
          <target state="translated">维基百科中有关科恩（Cohen）的kappa的条目。</target>
        </trans-unit>
        <trans-unit id="8d8ae14fc3bcf00321ca2d4b9c37c609195c6275" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the F1-score</source>
          <target state="translated">维基百科的F1分数条目</target>
        </trans-unit>
        <trans-unit id="0d85777073541b6f8aecb3488f1962f6903fd77c" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Fowlkes-Mallows Index</source>
          <target state="translated">维基百科上的Fowlkes-Mallows索引条目</target>
        </trans-unit>
        <trans-unit id="738fb31d9583a6207339f58c0335e89437aa096f" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Jaccard index</source>
          <target state="translated">维基百科上的贾卡德索引条目</target>
        </trans-unit>
        <trans-unit id="d69dce297a7e32abae3549494346594b424875bc" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Matthews Correlation Coefficient</source>
          <target state="translated">马修斯相关系数的维基百科词条</target>
        </trans-unit>
        <trans-unit id="d1c0692994293b3fef98ac5de7dd74e23175c8d1" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Precision and recall</source>
          <target state="translated">维基百科上的精密和召回条目</target>
        </trans-unit>
        <trans-unit id="6c2dd7ccbd3afed766d1ee6ce92b068445c27bbb" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the Receiver operating characteristic</source>
          <target state="translated">维基百科上的接收机工作特性条目</target>
        </trans-unit>
        <trans-unit id="caae1d529b64ebeb0d4804273e9107122a389ac6" translate="yes" xml:space="preserve">
          <source>Wikipedia entry for the adjusted Rand index</source>
          <target state="translated">维基百科上关于调整后的兰德指数的条目</target>
        </trans-unit>
        <trans-unit id="af0472efa729237e92d89bb05e9ca0c8e7f37b5f" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Coefficient of determination</source>
          <target state="translated">维基百科上关于测定系数的条目</target>
        </trans-unit>
        <trans-unit id="e345be5719f19335870d8d3a8cdd20b6bd307aa0" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Hamming distance</source>
          <target state="translated">维基百科关于汉明距离的词条</target>
        </trans-unit>
        <trans-unit id="1857fa6b095ad66d104ea60f4be3df45f12529a3" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Hinge loss</source>
          <target state="translated">维基百科关于铰链损失的条目</target>
        </trans-unit>
        <trans-unit id="d4ccd1b47442c7552ebe73794cdd38515c5ffdef" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Lasso</source>
          <target state="translated">维基百科上的拉索条目</target>
        </trans-unit>
        <trans-unit id="8751f23b19110bb289e70c6d8c900548f6c9b761" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Least-angle regression</source>
          <target state="translated">维基百科上关于最小角度回归的条目。</target>
        </trans-unit>
        <trans-unit id="ed8f4a303fe71f9ad0ec1e1b74ef6fe644dad80d" translate="yes" xml:space="preserve">
          <source>Wikipedia entry on the Silhouette Coefficient</source>
          <target state="translated">维基百科上关于剪影系数的条目。</target>
        </trans-unit>
        <trans-unit id="0b665174747365aef367583fb0c32fb021d06a22" translate="yes" xml:space="preserve">
          <source>Wikipedia principal eigenvector</source>
          <target state="translated">维基百科主特征向量</target>
        </trans-unit>
        <trans-unit id="713348b23d025b202ea7f033591c046a82a1973b" translate="yes" xml:space="preserve">
          <source>Will be ignored when &lt;code&gt;y_true&lt;/code&gt; is binary.</source>
          <target state="translated">当 &lt;code&gt;y_true&lt;/code&gt; 为二进制时将被忽略。</target>
        </trans-unit>
        <trans-unit id="af498f4dd6f24dbc1f93745e77fe6ed29d0b9d0c" translate="yes" xml:space="preserve">
          <source>Will return sparse matrix if set True else will return an array.</source>
          <target state="translated">如果设置为True,将返回稀疏矩阵,否则将返回一个数组。</target>
        </trans-unit>
        <trans-unit id="f02c359862a5df44abc185413e06bdb77cfc5770" translate="yes" xml:space="preserve">
          <source>Williams, C.K.I. and Seeger, M. &amp;ldquo;Using the Nystroem method to speed up kernel machines&amp;rdquo;, Advances in neural information processing systems 2001</source>
          <target state="translated">CKI的Williams和M.的Seeger，M。&amp;ldquo;使用Nystroem方法加速内核机器&amp;rdquo;，神经信息处理系统的进步，2001年</target>
        </trans-unit>
        <trans-unit id="a20af0cf6ba0496377888d152bfba536fcfdefc1" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;adjusted=True&lt;/code&gt;, balanced accuracy reports the relative increase from \(\texttt{balanced-accuracy}(y, \mathbf{0}, w) = \frac{1}{\text{n\_classes}}\). In the binary case, this is also known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Youden%27s_J_statistic&quot;&gt;*Youden&amp;rsquo;s J statistic*&lt;/a&gt;, or &lt;em&gt;informedness&lt;/em&gt;.</source>
          <target state="translated">与 &lt;code&gt;adjusted=True&lt;/code&gt; ，平衡准确性报告从\（\ texttt {平衡精度}（Y，\ mathbf {0}，W）= \压裂{1} {\文本{N \ _classes}} \）的相对增加。在二进制情况下，这也称为&lt;a href=&quot;https://en.wikipedia.org/wiki/Youden%27s_J_statistic&quot;&gt;* Youden's J statistic *&lt;/a&gt;或&lt;em&gt;notifyness&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="8a7d860e7dc8979710329f97e747eaff0d3415d3" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;early_stopping=False&lt;/code&gt;, the model is fitted on the entire input data and the stopping criterion is based on the objective function computed on the input data.</source>
          <target state="translated">如果 &lt;code&gt;early_stopping=False&lt;/code&gt; ，则将模型拟合到整个输入数据上，并且停止条件基于在输入数据上计算出的目标函数。</target>
        </trans-unit>
        <trans-unit id="4ceb9e226f3e04a8a66252e3801eed93f740afd9" translate="yes" xml:space="preserve">
          <source>With &lt;code&gt;early_stopping=True&lt;/code&gt;, the input data is split into a training set and a validation set. The model is then fitted on the training set, and the stopping criterion is based on the prediction score computed on the validation set. The size of the validation set can be changed with the parameter &lt;code&gt;validation_fraction&lt;/code&gt;.</source>
          <target state="translated">使用 &lt;code&gt;early_stopping=True&lt;/code&gt; ，输入数据分为训练集和验证集。然后将模型拟合到训练集上，并且停止标准基于在验证集上计算出的预测得分。验证集的大小可以使用参数 &lt;code&gt;validation_fraction&lt;/code&gt; 进行更改。</target>
        </trans-unit>
        <trans-unit id="318aced6d4dfc924ad223bd54e79ede301143b06" translate="yes" xml:space="preserve">
          <source>With SGD or Adam, training supports online and mini-batch learning.</source>
          <target state="translated">使用SGD或Adam,培训支持在线和小批量学习。</target>
        </trans-unit>
        <trans-unit id="bebfaf6a5f7ee4311c7425773ef87a0b1b61dcc0" translate="yes" xml:space="preserve">
          <source>With SVMs and logistic-regression, the parameter C controls the sparsity: the smaller C the fewer features selected. With Lasso, the higher the alpha parameter, the fewer features selected.</source>
          <target state="translated">对于SVM和逻辑回归,参数C控制了稀疏度:C越小,选择的特征越少。对于Lasso,α参数越大,选择的特征越少。</target>
        </trans-unit>
        <trans-unit id="2e07775067fbbb8cee792ed1d4b4b0282fd223be" translate="yes" xml:space="preserve">
          <source>With \(P'(j) = |V_j| / N\). The mutual information (MI) between \(U\) and \(V\) is calculated by:</source>
          <target state="translated">随着(P'(j)=|V_j|/N\)。(U)和(V)之间的相互信息(MI)计算如下:</target>
        </trans-unit>
        <trans-unit id="5313dd287c9c493fc21b86c81282cea7d0608304" translate="yes" xml:space="preserve">
          <source>With agglomerative clustering, it is possible to specify which samples can be clustered together by giving a connectivity graph. Graphs in scikit-learn are represented by their adjacency matrix. Often, a sparse matrix is used. This can be useful, for instance, to retrieve connected regions (sometimes also referred to as connected components) when clustering an image:</source>
          <target state="translated">通过聚类聚类,可以通过给出一个连接图来指定哪些样本可以聚在一起。scikit-learn中的图由它们的邻接矩阵来表示。通常情况下,会使用一个稀疏矩阵。例如,在对图像进行聚类时,这对检索连接区域(有时也称为连接成分)很有用。</target>
        </trans-unit>
        <trans-unit id="ebae629f7af13ae26b867ab75161458173d10bdc" translate="yes" xml:space="preserve">
          <source>With regard to decision trees, this strategy can readily be used to support multi-output problems. This requires the following changes:</source>
          <target state="translated">关于决策树,这种策略可以很容易地用于支持多产出问题。这就需要作出以下改变:</target>
        </trans-unit>
        <trans-unit id="d6eab2b8513179355ba20cab88473d0665849027" translate="yes" xml:space="preserve">
          <source>With such an abundance of clues that distinguish newsgroups, the classifiers barely have to identify topics from text at all, and they all perform at the same high level.</source>
          <target state="translated">在如此丰富的区分新闻组的线索下,分类器几乎完全不需要从文本中识别主题,它们的表现都是一样的高水准。</target>
        </trans-unit>
        <trans-unit id="ba25a12704b8225df22eb5cee35ebe73afb76c8b" translate="yes" xml:space="preserve">
          <source>With sum_over_features equal to False it returns the componentwise distances.</source>
          <target state="translated">当sum_over_features等于False时,它返回分量的距离。</target>
        </trans-unit>
        <trans-unit id="03d84c3da120d3c6633bcd8017a1f00e1bb6dad8" translate="yes" xml:space="preserve">
          <source>With this class, the base_estimator is fit on the train set of the cross-validation generator and the test set is used for calibration. The probabilities for each of the folds are then averaged for prediction. In case that cv=&amp;rdquo;prefit&amp;rdquo; is passed to __init__, it is assumed that base_estimator has been fitted already and all data is used for calibration. Note that data for fitting the classifier and for calibrating it must be disjoint.</source>
          <target state="translated">对于此类，将base_estimator拟合到交叉验证生成器的训练集上，并使用测试集进行校准。然后将每个折痕的概率取平均值进行预测。如果将cv =&amp;ldquo; prefit&amp;rdquo;传递给__init__，则假定base_estimator已被拟合，并且所有数据都用于校准。注意，用于分类器和校准的数据必须是不相交的。</target>
        </trans-unit>
        <trans-unit id="07ec442186310e3d4d8de1ef730f033183a12d2a" translate="yes" xml:space="preserve">
          <source>With this re-labeling of the data, our problem can be written</source>
          <target state="translated">通过这种对数据的重新标注,我们的问题可以写为</target>
        </trans-unit>
        <trans-unit id="bb9dc2936468de0109f9958206c68ba68552df6f" translate="yes" xml:space="preserve">
          <source>With this setup, a single distance calculation between a test point and the centroid is sufficient to determine a lower and upper bound on the distance to all points within the node. Because of the spherical geometry of the ball tree nodes, it can out-perform a &lt;em&gt;KD-tree&lt;/em&gt; in high dimensions, though the actual performance is highly dependent on the structure of the training data. In scikit-learn, ball-tree-based neighbors searches are specified using the keyword &lt;code&gt;algorithm = 'ball_tree'&lt;/code&gt;, and are computed using the class &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;sklearn.neighbors.BallTree&lt;/code&gt;&lt;/a&gt;. Alternatively, the user can work with the &lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; class directly.</source>
          <target state="translated">使用此设置，测试点和质心之间的单个距离计算就足以确定到节点内所有点的距离的上下限。由于球树节点的球形几何形状，因此它在高维度上的性能可能&lt;em&gt;优于KD树&lt;/em&gt;，尽管实际性能在很大程度上取决于训练数据的结构。在scikit-learn中，使用关键字 &lt;code&gt;algorithm = 'ball_tree'&lt;/code&gt; 指定基于球树的邻居搜索，并使用&lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;sklearn.neighbors.BallTree&lt;/code&gt; &lt;/a&gt;类进行计算。或者，用户可以直接使用&lt;a href=&quot;generated/sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt;类。</target>
        </trans-unit>
        <trans-unit id="c5b891d6db4b2b53f2c329c62187e665d9759f8a" translate="yes" xml:space="preserve">
          <source>Without any prior information on the sample, the number of projections required to reconstruct the image is of the order of the linear size &lt;code&gt;l&lt;/code&gt; of the image (in pixels). For simplicity we consider here a sparse image, where only pixels on the boundary of objects have a non-zero value. Such data could correspond for example to a cellular material. Note however that most images are sparse in a different basis, such as the Haar wavelets. Only &lt;code&gt;l/7&lt;/code&gt; projections are acquired, therefore it is necessary to use prior information available on the sample (its sparsity): this is an example of &lt;strong&gt;compressive sensing&lt;/strong&gt;.</source>
          <target state="translated">在样本上没有任何先验信息的情况下，重建图像所需的投影数量约为图像的线性大小 &lt;code&gt;l&lt;/code&gt; （以像素为单位）。为简单起见，我们在这里考虑一个稀疏图像，其中只有对象边界上的像素具有非零值。这样的数据可以例如对应于蜂窝材料。但是请注意，大多数图像在不同的基础上都是稀疏的，例如Haar小波。仅 &lt;code&gt;l/7&lt;/code&gt; 突起被获取，因此，有必要使用可在样品（其稀疏性）先验信息：这是一个例子&lt;strong&gt;压缩感测&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="087783a9ac4373b41b03a4f66d5eaf61d7d47ff1" translate="yes" xml:space="preserve">
          <source>Without reduce_func:</source>
          <target state="translated">没有reduce_func。</target>
        </trans-unit>
        <trans-unit id="e6002e635270be50830b0534ba0aafc304922d8b" translate="yes" xml:space="preserve">
          <source>Without shuffling, &lt;code&gt;X&lt;/code&gt; horizontally stacks features in the following order: the primary &lt;code&gt;n_informative&lt;/code&gt; features, followed by &lt;code&gt;n_redundant&lt;/code&gt; linear combinations of the informative features, followed by &lt;code&gt;n_repeated&lt;/code&gt; duplicates, drawn randomly with replacement from the informative and redundant features. The remaining features are filled with random noise. Thus, without shuffling, all useful features are contained in the columns &lt;code&gt;X[:, :n_informative + n_redundant + n_repeated]&lt;/code&gt;.</source>
          <target state="translated">在不进行改组的情况下， &lt;code&gt;X&lt;/code&gt; 按以下顺序水平堆叠 &lt;code&gt;n_informative&lt;/code&gt; ：主要的n_informative特征，然后是 &lt;code&gt;n_redundant&lt;/code&gt; 线性的信息特征组合，然后是 &lt;code&gt;n_repeated&lt;/code&gt; 重复，从信息和冗余特征中随机替换。其余功能充满了随机噪声。因此，无需混洗，所有有用的功能都包含在 &lt;code&gt;X[:, :n_informative + n_redundant + n_repeated]&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d2a146386973596d64e5c0f348ec45ab36bab658" translate="yes" xml:space="preserve">
          <source>Working With Text Data</source>
          <target state="translated">使用文本数据</target>
        </trans-unit>
        <trans-unit id="5b5ef6667bd92ea247084ea267c265251f4aa7de" translate="yes" xml:space="preserve">
          <source>Works with sparse matrices. Only works if &lt;code&gt;rows_&lt;/code&gt; and &lt;code&gt;columns_&lt;/code&gt; attributes exist.</source>
          <target state="translated">适用于稀疏矩阵。仅当存在 &lt;code&gt;rows_&lt;/code&gt; 和 &lt;code&gt;columns_&lt;/code&gt; 属性时有效。</target>
        </trans-unit>
        <trans-unit id="926da419b9cc98b9060a6d00fb8d48cd55be86f9" translate="yes" xml:space="preserve">
          <source>Wrapper for kernels in sklearn.metrics.pairwise.</source>
          <target state="translated">sklearn.metrics.pairwise中kernels的封装器。</target>
        </trans-unit>
        <trans-unit id="f986c2ac1f7dce99239d5b1ba2c2c97de265f3fa" translate="yes" xml:space="preserve">
          <source>Write a text classification pipeline to classify movie reviews as either positive or negative.</source>
          <target state="translated">编写一个文本分类流水线,将电影评论分为正面或负面。</target>
        </trans-unit>
        <trans-unit id="c1b32a0493a32a44864910f6b6b9c9398af4b20e" translate="yes" xml:space="preserve">
          <source>Write a text classification pipeline using a custom preprocessor and &lt;code&gt;CharNGramAnalyzer&lt;/code&gt; using data from Wikipedia articles as training set.</source>
          <target state="translated">使用自定义预处理器和 &lt;code&gt;CharNGramAnalyzer&lt;/code&gt; (使用Wikipedia文章中的数据作为训练集）编写文本分类管道。</target>
        </trans-unit>
        <trans-unit id="c72e193d2469d6cfb2918ba7a00dbc8ed1d451d6" translate="yes" xml:space="preserve">
          <source>Wu, Lin and Weng, &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf&quot;&gt;&amp;ldquo;Probability estimates for multi-class classification by pairwise coupling&amp;rdquo;&lt;/a&gt;, JMLR 5:975-1005, 2004.</source>
          <target state="translated">Wu，Lin和Weng，&lt;a href=&quot;http://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf&quot;&gt;&amp;ldquo;通过成对耦合的多类别分类的概率估计&amp;rdquo;&lt;/a&gt;，JMLR 5：975-1005，2004。</target>
        </trans-unit>
        <trans-unit id="c8c1574205d07b839af62817660ba2b78f320cd0" translate="yes" xml:space="preserve">
          <source>X block loadings vectors.</source>
          <target state="translated">X块载荷向量。</target>
        </trans-unit>
        <trans-unit id="b8076ad410e1a569012d16107ff003e5d358439f" translate="yes" xml:space="preserve">
          <source>X block to latents rotations.</source>
          <target state="translated">X块到潜伏旋转。</target>
        </trans-unit>
        <trans-unit id="a6b8640132f42899bc713ee5acc307439a5b7049" translate="yes" xml:space="preserve">
          <source>X block weights vectors.</source>
          <target state="translated">X块权重向量。</target>
        </trans-unit>
        <trans-unit id="e6bc2e58339df2a473a9897261f25e31780f738c" translate="yes" xml:space="preserve">
          <source>X is projected on the first principal components previously extracted from a training set.</source>
          <target state="translated">X投射在之前从训练集提取的第一主成分上。</target>
        </trans-unit>
        <trans-unit id="7228d382c859d348525ccb5bce51e5752c38bc04" translate="yes" xml:space="preserve">
          <source>X is stored for future use, as &lt;code&gt;transform&lt;/code&gt; needs X to interpolate new input data.</source>
          <target state="translated">X存储起来供将来使用，因为 &lt;code&gt;transform&lt;/code&gt; 需要X插入新的输入数据。</target>
        </trans-unit>
        <trans-unit id="3074bef8d8da5f206ce501f5438e3d5abb038064" translate="yes" xml:space="preserve">
          <source>X must have been produced by this DictVectorizer&amp;rsquo;s transform or fit_transform method; it may only have passed through transformers that preserve the number of features and their order.</source>
          <target state="translated">X必须是由DictVectorizer的transform或fit_transform方法产生的；它可能只通过了保留特征数量及其顺序的变压器。</target>
        </trans-unit>
        <trans-unit id="d0ad8e13f68af13dec8ad59c4f3a6a0df7a4de08" translate="yes" xml:space="preserve">
          <source>X scores.</source>
          <target state="translated">X分。</target>
        </trans-unit>
        <trans-unit id="28a3e4c54c0fde2f1aaa67a11fe405d430c2fe41" translate="yes" xml:space="preserve">
          <source>X transformed in the new space.</source>
          <target state="translated">X在新的空间里转化。</target>
        </trans-unit>
        <trans-unit id="feedfda54d7431e28acb98075b2c2bd9cf8331f2" translate="yes" xml:space="preserve">
          <source>Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon University, 2002 &lt;a href=&quot;http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf&quot;&gt;http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf&lt;/a&gt;</source>
          <target state="translated">朱小金和邹斌&amp;middot;格哈拉玛尼。通过标签传播从标记和未标记的数据中学习。卡内基梅隆大学技术报告CMU-CALD-02-107，2002年&lt;a href=&quot;http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf&quot;&gt;http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5c986ee528dd9bdf7a6c8d0101f77976c47ae9d2" translate="yes" xml:space="preserve">
          <source>Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang: &lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;Theil-Sen Estimators in a Multiple Linear Regression Model.&lt;/a&gt;</source>
          <target state="translated">当心，彭汉祥，王学勤和张和平：&lt;a href=&quot;http://home.olemiss.edu/~xdang/papers/MTSE.pdf&quot;&gt;多元线性回归模型中的Theil-Sen估计。&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="c93217dd923de34853280b8058e56203ef9ee737" translate="yes" xml:space="preserve">
          <source>Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed.</source>
          <target state="translated">Xy=np.dot(X.T,y),可以预先计算。只有在预计算Gram矩阵时才有用。</target>
        </trans-unit>
        <trans-unit id="23eb4d3f4155395a74e9d534f97ff4c1908f5aac" translate="yes" xml:space="preserve">
          <source>Y</source>
          <target state="translated">Y</target>
        </trans-unit>
        <trans-unit id="aac13ced89d2b311880e53ba16f36f4513402a98" translate="yes" xml:space="preserve">
          <source>Y block loadings vectors.</source>
          <target state="translated">Y块载荷向量。</target>
        </trans-unit>
        <trans-unit id="148708c0aec99251158277d2fc4d038d62f32551" translate="yes" xml:space="preserve">
          <source>Y block to latents rotations.</source>
          <target state="translated">Y块到潜伏旋转。</target>
        </trans-unit>
        <trans-unit id="8f4ded8aca1a84f4452774f8bc622751045ade48" translate="yes" xml:space="preserve">
          <source>Y block weights vectors.</source>
          <target state="translated">Y块权重向量。</target>
        </trans-unit>
        <trans-unit id="780dd8f1641062cfc0af001d2fcfedba3262be26" translate="yes" xml:space="preserve">
          <source>Y scores.</source>
          <target state="translated">Y的成绩。</target>
        </trans-unit>
        <trans-unit id="93b5936ef31b077aecad8b961838c412166e9fd0" translate="yes" xml:space="preserve">
          <source>Y. Freund, R. Schapire, &amp;ldquo;A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting&amp;rdquo;, 1995.</source>
          <target state="translated">Y. Freund，R。Schapire，&amp;ldquo;在线学习的决策理论概括及其对Boosting的应用&amp;rdquo;，1995年。</target>
        </trans-unit>
        <trans-unit id="bf931371fe813e68af145bc28f1f0c59ead42876" translate="yes" xml:space="preserve">
          <source>Y. Freund, and R. Schapire, &amp;ldquo;A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting&amp;rdquo;, 1997.</source>
          <target state="translated">Y. Freund和R. Schapire，&amp;ldquo;在线学习的决策理论概括及其对Boosting的应用&amp;rdquo;，1997年。</target>
        </trans-unit>
        <trans-unit id="982b5c305af507a5853864a0280fe0330e5fb9d9" translate="yes" xml:space="preserve">
          <source>Y[argmin[i], :] is the row in Y that is closest to X[i, :].</source>
          <target state="translated">Y[argmin[i],:]是Y中最接近X[i,:]的行。</target>
        </trans-unit>
        <trans-unit id="3526f607bcd4f51ad0bc05f814579a42c2c0ba57" translate="yes" xml:space="preserve">
          <source>Yellow</source>
          <target state="translated">Yellow</target>
        </trans-unit>
        <trans-unit id="44e848b37858df8125129ee3d3911783b05fb21f" translate="yes" xml:space="preserve">
          <source>Yields indices to split data into training and test sets.</source>
          <target state="translated">产生指数,将数据分割成训练和测试集。</target>
        </trans-unit>
        <trans-unit id="c970e3f1e790a2a4cd28b40401902501b9bc2d74" translate="yes" xml:space="preserve">
          <source>Yields:</source>
          <target state="translated">Yields:</target>
        </trans-unit>
        <trans-unit id="e285a8203a02b899c727c909bb971f8a5290d1a9" translate="yes" xml:space="preserve">
          <source>You can &lt;a href=&quot;grid_search#grid-search&quot;&gt;grid search&lt;/a&gt; over parameters of all estimators in the pipeline at once.</source>
          <target state="translated">您可以一次对管道中所有估计量的参数进行&lt;a href=&quot;grid_search#grid-search&quot;&gt;网格搜索&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e4f0eb08d1e594cb4ba39dda583b2124c29e8d3e" translate="yes" xml:space="preserve">
          <source>You can adjust the number of categories by giving their names to the dataset loader or setting them to None to get the 20 of them.</source>
          <target state="translated">你可以通过给数据集加载器赋予它们的名称来调整类别的数量,或者将它们设置为 &quot;无 &quot;来获得其中的20个类别。</target>
        </trans-unit>
        <trans-unit id="d6a91645b832623d5d5110588ef04aebdc451880" translate="yes" xml:space="preserve">
          <source>You can already copy the skeletons into a new folder somewhere on your hard-drive named &lt;code&gt;sklearn_tut_workspace&lt;/code&gt; where you will edit your own files for the exercises while keeping the original skeletons intact:</source>
          <target state="translated">您已经可以将骨骼复制到硬盘上名为 &lt;code&gt;sklearn_tut_workspace&lt;/code&gt; 的新文件夹中，您将在练习中编辑自己的文件，同时保持原始骨骼完整：</target>
        </trans-unit>
        <trans-unit id="1cf0d94595a131d36f8236532aeafb049eaa6dbc" translate="yes" xml:space="preserve">
          <source>You can also specify both the name and the version, which also uniquely identifies the dataset:</source>
          <target state="translated">你也可以同时指定名称和版本,这也是对数据集的唯一标识。</target>
        </trans-unit>
        <trans-unit id="ae5f0da778f6ff8c0d5d1c2ccd6f86bebea3d9e0" translate="yes" xml:space="preserve">
          <source>You can also use your own defined kernels by passing a function to the keyword &lt;code&gt;kernel&lt;/code&gt; in the constructor.</source>
          <target state="translated">您还可以通过将函数传递给构造函数中的关键字 &lt;code&gt;kernel&lt;/code&gt; 来使用自己定义的内核。</target>
        </trans-unit>
        <trans-unit id="c90fd70bc9584ad72350fadce865213cd8c472be" translate="yes" xml:space="preserve">
          <source>You can combine &lt;code&gt;KBinsDiscretizer&lt;/code&gt; with &lt;a href=&quot;sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt; if you only want to preprocess part of the features.</source>
          <target state="translated">如果只想预处理部分功能，则可以将 &lt;code&gt;KBinsDiscretizer&lt;/code&gt; 与&lt;a href=&quot;sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;sklearn.compose.ColumnTransformer&lt;/code&gt; &lt;/a&gt;结合使用。</target>
        </trans-unit>
        <trans-unit id="579522a3d8d5bf3b4958e7c2d681266388521dce" translate="yes" xml:space="preserve">
          <source>You can define your own kernels by either giving the kernel as a python function or by precomputing the Gram matrix.</source>
          <target state="translated">你可以通过将内核以python函数的形式给出,或者通过预先计算Gram矩阵来定义自己的内核。</target>
        </trans-unit>
        <trans-unit id="5a14b3f83e3bd81fa3adef5b677dc7c591d450db" translate="yes" xml:space="preserve">
          <source>You can display the BLAS / LAPACK implementation used by your NumPy / SciPy / scikit-learn install with the following commands:</source>
          <target state="translated">您可以使用以下命令显示您的NumPy/SciPy/scikit-learn安装所使用的BLAS/LAPACK实现。</target>
        </trans-unit>
        <trans-unit id="929abd63168ac2d721d4708b8ef8be3cd51b08a0" translate="yes" xml:space="preserve">
          <source>You can ensure that &lt;code&gt;func&lt;/code&gt; and &lt;code&gt;inverse_func&lt;/code&gt; are the inverse of each other by setting &lt;code&gt;check_inverse=True&lt;/code&gt; and calling &lt;code&gt;fit&lt;/code&gt; before &lt;code&gt;transform&lt;/code&gt;. Please note that a warning is raised and can be turned into an error with a &lt;code&gt;filterwarnings&lt;/code&gt;:</source>
          <target state="translated">您可以通过设置 &lt;code&gt;check_inverse=True&lt;/code&gt; 并在 &lt;code&gt;transform&lt;/code&gt; 之前调用 &lt;code&gt;fit&lt;/code&gt; 来确保 &lt;code&gt;func&lt;/code&gt; 和 &lt;code&gt;inverse_func&lt;/code&gt; 彼此相反。请注意，将发出警告，并可以通过 &lt;code&gt;filterwarnings&lt;/code&gt; 变成错误：</target>
        </trans-unit>
        <trans-unit id="8fe8cd91261eb27750ae7b2f82fa15c24077f346" translate="yes" xml:space="preserve">
          <source>You can generate even more flexible model scorers by constructing your own scoring object from scratch, without using the &lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt;&lt;code&gt;make_scorer&lt;/code&gt;&lt;/a&gt; factory. For a callable to be a scorer, it needs to meet the protocol specified by the following two rules:</source>
          <target state="translated">您可以通过从头开始构建自己的评分对象而无需使用&lt;a href=&quot;generated/sklearn.metrics.make_scorer#sklearn.metrics.make_scorer&quot;&gt; &lt;code&gt;make_scorer&lt;/code&gt; &lt;/a&gt;工厂来生成更加灵活的模型评分器。要使可呼叫者成为记分员，它需要满足以下两个规则所指定的协议：</target>
        </trans-unit>
        <trans-unit id="cb07d258c61c328d902779de990b642f82ba2beb" translate="yes" xml:space="preserve">
          <source>You can get more information on the dataset by looking at the &lt;code&gt;DESCR&lt;/code&gt; and &lt;code&gt;details&lt;/code&gt; attributes:</source>
          <target state="translated">您可以通过查看 &lt;code&gt;DESCR&lt;/code&gt; 和 &lt;code&gt;details&lt;/code&gt; 属性来获取有关数据集的更多信息：</target>
        </trans-unit>
        <trans-unit id="316dc294ff0e2890db335b30189c691f1a723809" translate="yes" xml:space="preserve">
          <source>You can now see many things that these features have overfit to:</source>
          <target state="translated">你现在可以看到很多东西,这些功能已经过度到。</target>
        </trans-unit>
        <trans-unit id="ee86b8b814976ee239ecfc8e86907133be9d3afc" translate="yes" xml:space="preserve">
          <source>You can see that 16 non-zero feature tokens were extracted in the vector output: this is less than the 19 non-zeros extracted previously by the &lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; on the same toy corpus. The discrepancy comes from hash function collisions because of the low value of the &lt;code&gt;n_features&lt;/code&gt; parameter.</source>
          <target state="translated">您可以看到在向量输出中提取了16个非零特征标记：这比&lt;a href=&quot;generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt;先前在同一玩具主体上提取的19个非零标记少。由于 &lt;code&gt;n_features&lt;/code&gt; 参数的值较低，因此差异来自哈希函数冲突。</target>
        </trans-unit>
        <trans-unit id="c31033fd31d22147ea7534b97a7d63f646fe3e48" translate="yes" xml:space="preserve">
          <source>You can then edit the content of the workspace without fear of losing the original exercise instructions.</source>
          <target state="translated">然后,你可以编辑工作区的内容,而不用担心丢失原来的练习指令。</target>
        </trans-unit>
        <trans-unit id="e21dcf7dc4d8353d8949b3e6ddc2c35c364a9b4a" translate="yes" xml:space="preserve">
          <source>You cannot nest objects with parallel computing (&lt;code&gt;n_jobs&lt;/code&gt; different than 1).</source>
          <target state="translated">您不能使用并行计算嵌套对象（ &lt;code&gt;n_jobs&lt;/code&gt; 不同于1）。</target>
        </trans-unit>
        <trans-unit id="d086a1b811e8ad563a3cd7d98758c535aff811c7" translate="yes" xml:space="preserve">
          <source>You could try UTF-8 and disregard the errors. You can decode byte strings with &lt;code&gt;bytes.decode(errors='replace')&lt;/code&gt; to replace all decoding errors with a meaningless character, or set &lt;code&gt;decode_error='replace'&lt;/code&gt; in the vectorizer. This may damage the usefulness of your features.</source>
          <target state="translated">您可以尝试使用UTF-8，而忽略错误。您可以使用 &lt;code&gt;bytes.decode(errors='replace')&lt;/code&gt; 解码字节字符串，以将所有解码错误替换为无意义的字符，或者在矢量化器中设置 &lt;code&gt;decode_error='replace'&lt;/code&gt; 。这可能会损害功能的实用性。</target>
        </trans-unit>
        <trans-unit id="c0d5a5afa92ed6aa301d13299623473f530c94ba" translate="yes" xml:space="preserve">
          <source>You may also load two (or more) datasets at once:</source>
          <target state="translated">你也可以同时加载两个(或多个)数据集。</target>
        </trans-unit>
        <trans-unit id="a822ec525f0ce269b9b885feec474e1f8b512e04" translate="yes" xml:space="preserve">
          <source>You may also retain the estimator fitted on each training set by setting &lt;code&gt;return_estimator=True&lt;/code&gt;.</source>
          <target state="translated">您还可以通过设置 &lt;code&gt;return_estimator=True&lt;/code&gt; 来保留适合每个训练集的估计器。</target>
        </trans-unit>
        <trans-unit id="c9bcd37e9efb4d07a927274f7ad017afc3f094c8" translate="yes" xml:space="preserve">
          <source>You may be able to find out what kind of encoding it is in general using the UNIX command &lt;code&gt;file&lt;/code&gt;. The Python &lt;code&gt;chardet&lt;/code&gt; module comes with a script called &lt;code&gt;chardetect.py&lt;/code&gt; that will guess the specific encoding, though you cannot rely on its guess being correct.</source>
          <target state="translated">使用UNIX命令 &lt;code&gt;file&lt;/code&gt; 您也许可以找到一般的编码方式。Python &lt;code&gt;chardet&lt;/code&gt; 模块带有一个名为 &lt;code&gt;chardetect.py&lt;/code&gt; 的脚本，该脚本将猜测特定的编码，尽管您不能依靠其猜测是正确的。</target>
        </trans-unit>
        <trans-unit id="04405b99190799597dab92e2ef615219d1447404" translate="yes" xml:space="preserve">
          <source>You may load a dataset like as follows:</source>
          <target state="translated">你可以加载一个数据集,如下所示。</target>
        </trans-unit>
        <trans-unit id="f9f71500b978e09c529098f893f05269c79caaff" translate="yes" xml:space="preserve">
          <source>You may want to include the parameters of the preprocessors in a &lt;a href=&quot;grid_search#grid-search&quot;&gt;parameter search&lt;/a&gt;.</source>
          <target state="translated">您可能需要在&lt;a href=&quot;grid_search#grid-search&quot;&gt;参数搜索中&lt;/a&gt;包括预处理器的参数。</target>
        </trans-unit>
        <trans-unit id="4cbe0908270a3a4effe7f03ed10c6fc1b573bdb1" translate="yes" xml:space="preserve">
          <source>You might get slightly different results with the solver liblinear than with the others since this uses LIBLINEAR which penalizes the intercept.</source>
          <target state="translated">你用liblinear解算器得到的结果可能会与其他解算器略有不同,因为它使用的是LIBLINEAR,会对截距进行惩罚。</target>
        </trans-unit>
        <trans-unit id="eacc5e93bbce61c3d762f60af9c0b85d6ab90006" translate="yes" xml:space="preserve">
          <source>You might have noticed that the samples were shuffled randomly when we called &lt;code&gt;fetch_20newsgroups(..., shuffle=True, random_state=42)&lt;/code&gt;: this is useful if you wish to select only a subset of samples to quickly train a model and get a first idea of the results before re-training on the complete dataset later.</source>
          <target state="translated">您可能已经注意到，当我们调用 &lt;code&gt;fetch_20newsgroups(..., shuffle=True, random_state=42)&lt;/code&gt; 时，样本是随机洗牌的：如果您希望仅选择样本子集以快速训练模型并获得第一个样本，则这很有用在重新训练完整的数据集之前，先对结果有想法。</target>
        </trans-unit>
        <trans-unit id="1c0c1bb33d891f47deca1c516d19aa08bd8443b9" translate="yes" xml:space="preserve">
          <source>You only have to call &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt; once on your data to fit a whole sequence of estimators.</source>
          <target state="translated">你只需要调用 &lt;code&gt;fit&lt;/code&gt; 和 &lt;code&gt;predict&lt;/code&gt; 的数据一次，以适应估计的整个序列。</target>
        </trans-unit>
        <trans-unit id="8f00f0e599f4c3a7b71dcab47e41c43fc685f526" translate="yes" xml:space="preserve">
          <source>You should also make sure that the stop word list has had the same preprocessing and tokenization applied as the one used in the vectorizer. The word &lt;em&gt;we&amp;rsquo;ve&lt;/em&gt; is split into &lt;em&gt;we&lt;/em&gt; and &lt;em&gt;ve&lt;/em&gt; by CountVectorizer&amp;rsquo;s default tokenizer, so if &lt;em&gt;we&amp;rsquo;ve&lt;/em&gt; is in &lt;code&gt;stop_words&lt;/code&gt;, but &lt;em&gt;ve&lt;/em&gt; is not, &lt;em&gt;ve&lt;/em&gt; will be retained from &lt;em&gt;we&amp;rsquo;ve&lt;/em&gt; in transformed text. Our vectorizers will try to identify and warn about some kinds of inconsistencies.</source>
          <target state="translated">您还应该确保停用词列表的预处理和标记化与矢量化程序中使用的预处理和标记化相同。这个词&lt;em&gt;我们已经&lt;/em&gt;被分成&lt;em&gt;我们&lt;/em&gt;和&lt;em&gt;已经&lt;/em&gt;通过CountVectorizer的默认标记生成器，所以如果&lt;em&gt;我们已经&lt;/em&gt;在 &lt;code&gt;stop_words&lt;/code&gt; ，但&lt;em&gt;已经&lt;/em&gt;不大，&lt;em&gt;已经&lt;/em&gt;从将被保留&lt;em&gt;，我们已经&lt;/em&gt;在转化文本。我们的矢量化程序将尝试识别并警告某些不一致之处。</target>
        </trans-unit>
        <trans-unit id="03e4dbf3891b38cb2bcd04772f91e994b5e9c01b" translate="yes" xml:space="preserve">
          <source>Your dataset consists of heterogeneous data types (e.g. raster images and text captions)</source>
          <target state="translated">您的数据集由不同的数据类型组成(如栅格图像和文本标题)。</target>
        </trans-unit>
        <trans-unit id="7b0a68e70dc900bed821b4c342a07edfa667e451" translate="yes" xml:space="preserve">
          <source>Your dataset is stored in a Pandas DataFrame and different columns require different processing pipelines.</source>
          <target state="translated">你的数据集存储在Pandas DataFrame中,不同的列需要不同的处理管道。</target>
        </trans-unit>
        <trans-unit id="cf246e4fd612425ede440737acf49a3220f42916" translate="yes" xml:space="preserve">
          <source>Your kernel must take as arguments two matrices of shape &lt;code&gt;(n_samples_1, n_features)&lt;/code&gt;, &lt;code&gt;(n_samples_2, n_features)&lt;/code&gt; and return a kernel matrix of shape &lt;code&gt;(n_samples_1, n_samples_2)&lt;/code&gt;.</source>
          <target state="translated">你的内核必须采取作为参数形状的两个矩阵 &lt;code&gt;(n_samples_1, n_features)&lt;/code&gt; ， &lt;code&gt;(n_samples_2, n_features)&lt;/code&gt; 并返回形状的核心矩阵 &lt;code&gt;(n_samples_1, n_samples_2)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="a97cec9f16597107c699027a6e02cf1c0426b74a" translate="yes" xml:space="preserve">
          <source>ZN proportion of residential land zoned for lots over 25,000 sq.ft.</source>
          <target state="translated">ZN劃作25,000平方呎以上地段的住宅用地比例。</target>
        </trans-unit>
        <trans-unit id="712d097b167e76a6e9d59b3e5e274cb4dc4edfe4" translate="yes" xml:space="preserve">
          <source>Zadrozny and Elkan, &amp;ldquo;Transforming classifier scores into multiclass probability estimates&amp;rdquo;, SIGKDD&amp;lsquo;02, &lt;a href=&quot;http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf&quot;&gt;http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf&lt;/a&gt;</source>
          <target state="translated">Zadrozny和Elkan，&amp;ldquo;将分类器分数转换为多类概率估计&amp;rdquo;，SIGKDD'02，&lt;a href=&quot;http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf&quot;&gt;http：//www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="f05a65af97509516c00dcac126500e3f1415b5be" translate="yes" xml:space="preserve">
          <source>Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels.</source>
          <target state="translated">多项式和sigmoid核的系数为零。被其他核忽略。</target>
        </trans-unit>
        <trans-unit id="e35caa5ca631cf4323249c1e10ca37b600a29376" translate="yes" xml:space="preserve">
          <source>Zero is the lowest possible score. Values closer to zero indicate a better partition.</source>
          <target state="translated">零是可能的最低分数。接近零的数值表示更好的分区。</target>
        </trans-unit>
        <trans-unit id="4196df3003bc4705f3359c145eca39ac9042a13b" translate="yes" xml:space="preserve">
          <source>Zero-one classification loss.</source>
          <target state="translated">零一分类损失。</target>
        </trans-unit>
        <trans-unit id="ee137211a128584365e4b492f8f1e31e317a831d" translate="yes" xml:space="preserve">
          <source>Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C. Local features and kernels for classification of texture and object categories: A comprehensive study International Journal of Computer Vision 2007 &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf&quot;&gt;http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf&lt;/a&gt;</source>
          <target state="translated">Zhang，J.和Marszalek，M.和Lazebnik，S.和Schmid，C.用于纹理和对象类别分类的局部特征和内核：全面研究International Journal of Computer Vision 2007 &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf&quot;&gt;http://research.microsoft.com/ zh-cn / um / people / manik / projects / trade-off / papers / ZhangIJCV06.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="2f2ef1b5180fd57b17245a5c505519733d35270d" translate="yes" xml:space="preserve">
          <source>Zhu, H. Zou, S. Rosset, T. Hastie, &amp;ldquo;Multi-class AdaBoost&amp;rdquo;, 2009.</source>
          <target state="translated">Zhu H. Zou，S。Rosset，T。Hastie，&amp;ldquo;多类AdaBoost&amp;rdquo;，2009年。</target>
        </trans-unit>
        <trans-unit id="8ce45cc584babf565a133f667c041638840fdfd3" translate="yes" xml:space="preserve">
          <source>Zoubir A., Koivunen V., Chakhchoukh Y. and Muma M. (2012). Robust estimation in signal processing: A tutorial-style treatment of fundamental concepts. IEEE Signal Processing Magazine 29(4), 61-80.</source>
          <target state="translated">Zoubir A.、Koivunen V.、Chakhchoukh Y.和Muma M.(2012)。信号处理中的稳健估计。基本概念的教程式处理。IEEE信号处理杂志29(4),61-80。</target>
        </trans-unit>
        <trans-unit id="cd3417b4282b09dc45879fe7c77bee8859983780" translate="yes" xml:space="preserve">
          <source>[&amp;lsquo;rbf&amp;rsquo;, &amp;lsquo;sigmoid&amp;rsquo;, &amp;lsquo;polynomial&amp;rsquo;, &amp;lsquo;poly&amp;rsquo;, &amp;lsquo;linear&amp;rsquo;, &amp;lsquo;cosine&amp;rsquo;]</source>
          <target state="translated">['rbf'，'sigmoid'，'多项式'，'poly'，'linear'，'cosine']</target>
        </trans-unit>
        <trans-unit id="7c0453b88eaf6a5b1a0ac2faa1dec6c20e0dda6a" translate="yes" xml:space="preserve">
          <source>[1, x_2, x_2 ** 2, x_2 ** 3, &amp;hellip;], &amp;hellip;]</source>
          <target state="translated">[1，x_2，x_2 ** 2，x_2 ** 3，&amp;hellip;]，&amp;hellip;]</target>
        </trans-unit>
        <trans-unit id="af237073ca841ce40d3b1c3f9ec3b84ba9e8c1ce" translate="yes" xml:space="preserve">
          <source>[1] &amp;ldquo;Online Learning for Latent Dirichlet Allocation&amp;rdquo;, Matthew D. Hoffman,</source>
          <target state="translated">[1]&amp;ldquo;在线学习以进行潜在狄利克雷分配&amp;rdquo;，Matthew D. Hoffman，</target>
        </trans-unit>
        <trans-unit id="a5828c16246e11e0eda2596d27fdd402ee57d009" translate="yes" xml:space="preserve">
          <source>[1] &amp;ldquo;Shrinkage Algorithms for MMSE Covariance Estimation&amp;rdquo; Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.</source>
          <target state="translated">[1]&amp;ldquo; MMSE协方差估计的收缩算法&amp;rdquo; Chen等人，IEEE Trans。在标志上。Proc。，第58卷，第10期，2010年10月。</target>
        </trans-unit>
        <trans-unit id="c5ae55965c66d78c700f954c5d28c9832964e702" translate="yes" xml:space="preserve">
          <source>[1] &amp;ldquo;Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning&amp;rdquo; by A. Rahimi and Benjamin Recht. (&lt;a href=&quot;http://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf&quot;&gt;http://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf&lt;/a&gt;)</source>
          <target state="translated">[1] A. Rahimi和Benjamin Recht的&amp;ldquo;加权随机厨房水槽之和：用学习中的随机性代替最小化&amp;rdquo;。（&lt;a href=&quot;http://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf&quot;&gt;http://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf&lt;/a&gt;）</target>
        </trans-unit>
        <trans-unit id="851ede0920efe80a8308115ddfdc22058d99b224" translate="yes" xml:space="preserve">
          <source>[1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for</source>
          <target state="translated">[1] Hinton，GE，Osindero，S。和Teh，Y。一种快速学习算法，用于</target>
        </trans-unit>
        <trans-unit id="c4ab6918e1971671fbb440a7f8e61bfcc4315791" translate="yes" xml:space="preserve">
          <source>[1] P. J. Rousseeuw. Least median of squares regression. J. Am</source>
          <target state="translated">[1] PJ Rousseeuw。平方回归的最小中位数。J.安</target>
        </trans-unit>
        <trans-unit id="4becf43125cdaf0ec29f63ac1f954b679ab8e6bc" translate="yes" xml:space="preserve">
          <source>[1] Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux. In Semi-Supervised Learning (2006), pp. 193-216</source>
          <target state="translated">[1] Yoshua Bengio，Olivier Delalleau，Nicolas Le Roux。在《半监督学习》（2006年），第193-216页</target>
        </trans-unit>
        <trans-unit id="9a201577697a06c9ac689a946ae70d44d48c0e7c" translate="yes" xml:space="preserve">
          <source>[1] van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional Data</source>
          <target state="translated">[1] van der Maaten，LJP；GE的Hinton，可视化高维数据</target>
        </trans-unit>
        <trans-unit id="8eeff125eef3cfca1ff3f8b3157054b95e0b3509" translate="yes" xml:space="preserve">
          <source>[2] &amp;ldquo;Stochastic Variational Inference&amp;rdquo;, Matthew D. Hoffman, David M. Blei,</source>
          <target state="translated">[2]&amp;ldquo;随机变分推论&amp;rdquo;，马修&amp;middot;霍夫曼，大卫&amp;middot;布雷</target>
        </trans-unit>
        <trans-unit id="ea3c887d7b7624a41f686043b166f388d3617ff3" translate="yes" xml:space="preserve">
          <source>[2] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. Efficient Non-Parametric Function Induction in Semi-Supervised Learning. AISTAT 2005 &lt;a href=&quot;http://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf&quot;&gt;http://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf&lt;/a&gt;</source>
          <target state="translated">[2] Olivier Delalleau，Yoshua Bengio，Nicolas Le Roux。半监督学习中的有效非参数函数归纳。AISTAT 2005 &lt;a href=&quot;http://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf&quot;&gt;http://research.microsoft.com/en-us/people/nicolasl/ficient_ssl.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="390f7912993134abee39b500fe8ff987c558bcc7" translate="yes" xml:space="preserve">
          <source>[2] Tieleman, T. Training Restricted Boltzmann Machines using</source>
          <target state="translated">[2] Tieleman，T.使用限制训练Boltzmann机器</target>
        </trans-unit>
        <trans-unit id="936e8131576c3002ff436671f77d81f6c95e71d7" translate="yes" xml:space="preserve">
          <source>[2] Wilson, E. B., &amp;amp; Hilferty, M. M. (1931). The distribution of chi-square.</source>
          <target state="translated">[2]威尔逊（EB）和希尔法蒂（MM）（1931）。卡方的分布。</target>
        </trans-unit>
        <trans-unit id="5cccbf6c7fe7c1f50410b68e37c12f00b67f9330" translate="yes" xml:space="preserve">
          <source>[2] van der Maaten, L.J.P. t-Distributed Stochastic Neighbor Embedding</source>
          <target state="translated">[2] van der Maaten，LJP t分布随机邻居嵌入</target>
        </trans-unit>
        <trans-unit id="740947d1c8302c56dc8a9209234aab173f75acad" translate="yes" xml:space="preserve">
          <source>[3] L.J.P. van der Maaten. Accelerating t-SNE using Tree-Based Algorithms.</source>
          <target state="translated">[3] LJP van der Maaten。使用基于树的算法加速t-SNE。</target>
        </trans-unit>
        <trans-unit id="a16dde9090b3c419b1ba6d8027b90785ddf73263" translate="yes" xml:space="preserve">
          <source>[3] Matthew D. Hoffman&amp;rsquo;s onlineldavb code. Link:</source>
          <target state="translated">[3] Matthew D. Hoffman的onlineldavb代码。链接：</target>
        </trans-unit>
        <trans-unit id="2091fb37b7afd77ae2e8e60855d4cad2538ba378" translate="yes" xml:space="preserve">
          <source>[B1996]</source>
          <target state="translated">[B1996]</target>
        </trans-unit>
        <trans-unit id="66fa89cedf249bba6f8bbd7ca59a6edba1eb2520" translate="yes" xml:space="preserve">
          <source>[B1998]</source>
          <target state="translated">[B1998]</target>
        </trans-unit>
        <trans-unit id="7665023d9511c3ea5a7a3e7baca06057eba900d6" translate="yes" xml:space="preserve">
          <source>[B1999]</source>
          <target state="translated">[B1999]</target>
        </trans-unit>
        <trans-unit id="5c075f95c1e65a7e49dec5ad30ee36d1fb13b2b6" translate="yes" xml:space="preserve">
          <source>[B2001]</source>
          <target state="translated">[B2001]</target>
        </trans-unit>
        <trans-unit id="04bec92cc809290da2bf608da574e1877293633f" translate="yes" xml:space="preserve">
          <source>[B2011]</source>
          <target state="translated">[B2011]</target>
        </trans-unit>
        <trans-unit id="2b6c9f7f2623b948c281da789073abc37bb7f8fa" translate="yes" xml:space="preserve">
          <source>[ButlerDavies]</source>
          <target state="translated">[ButlerDavies]</target>
        </trans-unit>
        <trans-unit id="1d442f2d1661e89f1bc869a582a8d649d24881e4" translate="yes" xml:space="preserve">
          <source>[D1997]</source>
          <target state="translated">[D1997]</target>
        </trans-unit>
        <trans-unit id="20e70caf2f764d35f0c5f51eb6c2cc52a6f947f2" translate="yes" xml:space="preserve">
          <source>[Davis2006]</source>
          <target state="translated">[Davis2006]</target>
        </trans-unit>
        <trans-unit id="3b0f17e8250c1e7b54512123b77c31bb0899ae7a" translate="yes" xml:space="preserve">
          <source>[Everingham2010]</source>
          <target state="translated">[Everingham2010]</target>
        </trans-unit>
        <trans-unit id="e5ff04dac92d8d5710e2d4ade54a4899d9a01662" translate="yes" xml:space="preserve">
          <source>[F1999]</source>
          <target state="translated">[F1999]</target>
        </trans-unit>
        <trans-unit id="ddfaba8b68f822d387eefcc49dbcf89bee83fcbe" translate="yes" xml:space="preserve">
          <source>[F2001]</source>
          <target state="translated">[F2001]</target>
        </trans-unit>
        <trans-unit id="5712ff07224dea2f09976ad1b5f9060cea098ee8" translate="yes" xml:space="preserve">
          <source>[FS1995]</source>
          <target state="translated">[FS1995]</target>
        </trans-unit>
        <trans-unit id="111b120f6e2f3a7d9723c16133fcfb1ce7b7557b" translate="yes" xml:space="preserve">
          <source>[Flach2015]</source>
          <target state="translated">[Flach2015]</target>
        </trans-unit>
        <trans-unit id="0be1b91bf292e6f295e73f8ba1626aa315ca1709" translate="yes" xml:space="preserve">
          <source>[Guyon2015]</source>
          <target state="translated">[Guyon2015]</target>
        </trans-unit>
        <trans-unit id="16deff704a4f867ca18d98b22e63afcb70ec96d2" translate="yes" xml:space="preserve">
          <source>[H1998]</source>
          <target state="translated">[H1998]</target>
        </trans-unit>
        <trans-unit id="346ddc10d1a23f1ff0ba05ea1da881f4666595c5" translate="yes" xml:space="preserve">
          <source>[HTF2009]</source>
          <target state="translated">[HTF2009]</target>
        </trans-unit>
        <trans-unit id="2b6bce181ae06e6796d39628b4dc12ca65767e20" translate="yes" xml:space="preserve">
          <source>[HTF]</source>
          <target state="translated">[HTF]</target>
        </trans-unit>
        <trans-unit id="8f4756ba18c793a637ad7568fd4450479f47b718" translate="yes" xml:space="preserve">
          <source>[Hubert1985]</source>
          <target state="translated">[Hubert1985]</target>
        </trans-unit>
        <trans-unit id="1c2acae56920363d695dc395b30aa0dac0656aa7" translate="yes" xml:space="preserve">
          <source>[Jen09]</source>
          <target state="translated">[Jen09]</target>
        </trans-unit>
        <trans-unit id="52833f723be6af6645b6622da4d471b65e89d942" translate="yes" xml:space="preserve">
          <source>[Kelleher2015]</source>
          <target state="translated">[Kelleher2015]</target>
        </trans-unit>
        <trans-unit id="8d63f432cd9715591fb04662784fbed01426124f" translate="yes" xml:space="preserve">
          <source>[L2014]</source>
          <target state="translated">[L2014]</target>
        </trans-unit>
        <trans-unit id="e6f810474b9d9bf1966e66d024bfd2d0d9af7983" translate="yes" xml:space="preserve">
          <source>[LG2012]</source>
          <target state="translated">[LG2012]</target>
        </trans-unit>
        <trans-unit id="4108a351bec333bc41371d8be81bbf1f0bd216a0" translate="yes" xml:space="preserve">
          <source>[LS2010]</source>
          <target state="translated">[LS2010]</target>
        </trans-unit>
        <trans-unit id="36c1f9470816b8da81a8ed80471ace8df2456c31" translate="yes" xml:space="preserve">
          <source>[M2012]</source>
          <target state="translated">[M2012]</target>
        </trans-unit>
        <trans-unit id="536dc6f43e65eedbc7dcd92d64ef850b0a7951d3" translate="yes" xml:space="preserve">
          <source>[MRS2008]</source>
          <target state="translated">[MRS2008]</target>
        </trans-unit>
        <trans-unit id="350f14f810397ce0cd7520f35f0fae7d54d72023" translate="yes" xml:space="preserve">
          <source>[Manning2008]</source>
          <target state="translated">[Manning2008]</target>
        </trans-unit>
        <trans-unit id="0cc214a1564fbbf90b4daa0b0972b6f8408e3afc" translate="yes" xml:space="preserve">
          <source>[Mosley2013]</source>
          <target state="translated">[Mosley2013]</target>
        </trans-unit>
        <trans-unit id="73be0b37b87d3c88f49438b3a7ca251dc3d3c1d2" translate="yes" xml:space="preserve">
          <source>[Mrl09]</source>
          <target state="translated">[Mrl09]</target>
        </trans-unit>
        <trans-unit id="690e639d5d468ec30ab9e54cb03287a1d07d286f" translate="yes" xml:space="preserve">
          <source>[NQY18]</source>
          <target state="translated">[NQY18]</target>
        </trans-unit>
        <trans-unit id="95849b59dfe0de62fa4f930bc19ca3b64ad51c5b" translate="yes" xml:space="preserve">
          <source>[R2007]</source>
          <target state="translated">[R2007]</target>
        </trans-unit>
        <trans-unit id="d27f89b25dd2806ef3b69d37ac341ea761b1f775" translate="yes" xml:space="preserve">
          <source>[RR2007]</source>
          <target state="translated">[RR2007]</target>
        </trans-unit>
        <trans-unit id="99aae3a9e5135b3c3c9e6f81c5583f53ea54b161" translate="yes" xml:space="preserve">
          <source>[RVD]</source>
          <target state="translated">[RVD]</target>
        </trans-unit>
        <trans-unit id="7fc4fd0834c6c78f63966f47416f1e672a05b032" translate="yes" xml:space="preserve">
          <source>[RVDriessen]</source>
          <target state="translated">[RVDriessen]</target>
        </trans-unit>
        <trans-unit id="9a3d290ec7e0cf466e2e530b732c430fd93742e5" translate="yes" xml:space="preserve">
          <source>[RW2006]</source>
          <target state="translated">[RW2006]</target>
        </trans-unit>
        <trans-unit id="ab40883d6ce3b576febad86ad20a220ae60fc722" translate="yes" xml:space="preserve">
          <source>[Rouseeuw1984]</source>
          <target state="translated">[Rouseeuw1984]</target>
        </trans-unit>
        <trans-unit id="f4ff5aad65e1461e94ef70a659337011d0c58126" translate="yes" xml:space="preserve">
          <source>[Rousseeuw]</source>
          <target state="translated">[Rousseeuw]</target>
        </trans-unit>
        <trans-unit id="74f2d2c5b044f5dc96afc1e0482d2495c57d5370" translate="yes" xml:space="preserve">
          <source>[Urbanowicz2015]</source>
          <target state="translated">[Urbanowicz2015]</target>
        </trans-unit>
        <trans-unit id="34cb3f1593778c84c25ed6665ee9a323140a68d9" translate="yes" xml:space="preserve">
          <source>[VEB2009] Vinh, Epps, and Bailey, (2009). &amp;ldquo;Information theoretic measures for clusterings comparison&amp;rdquo;. Proceedings of the 26th Annual International Conference on Machine Learning - ICML &amp;lsquo;09. &lt;a href=&quot;https://dl.acm.org/citation.cfm?doid=1553374.1553511&quot;&gt;doi:10.1145/1553374.1553511&lt;/a&gt;. ISBN 9781605585161.</source>
          <target state="translated">[VEB2009] Vinh，Epps和Bailey，（2009年）。&amp;ldquo;用于聚类比较的信息理论方法&amp;rdquo;。第26届年度机器学习国际会议论文集-ICML '09。&lt;a href=&quot;https://dl.acm.org/citation.cfm?doid=1553374.1553511&quot;&gt;doi：10.1145 / 1553374.1553511&lt;/a&gt;。ISBN 9781605585161。</target>
        </trans-unit>
        <trans-unit id="48a5d15f42b24744d500812bf01a83ad55ecae83" translate="yes" xml:space="preserve">
          <source>[VEB2010] Vinh, Epps, and Bailey, (2010). &amp;ldquo;Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance&amp;rdquo;. JMLR &amp;lt;&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&lt;/a&gt;&amp;gt;</source>
          <target state="translated">[VEB2010] Vinh，Epps和Bailey，（2010年）。&amp;ldquo;用于聚类比较的信息理论方法：变异，属性，归一化和机会校正&amp;rdquo;。JMLR &amp;lt; &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&lt;/a&gt; &amp;gt;</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
