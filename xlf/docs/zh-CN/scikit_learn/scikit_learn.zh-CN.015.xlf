<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="zh-CN" datatype="htmlbody" original="scikit_learn">
    <body>
      <group id="scikit_learn">
        <trans-unit id="65eaa1a409cbf0736a7b1da17a35a153fc9af91f" translate="yes" xml:space="preserve">
          <source>Test samples</source>
          <target state="translated">测试样品</target>
        </trans-unit>
        <trans-unit id="29446ed524d3e237184352cbcf6e1c5aaeb464e4" translate="yes" xml:space="preserve">
          <source>Test samples with shape = (n_samples, n_features) or None. For some estimators this may be a precomputed kernel matrix instead, shape = (n_samples, n_samples_fitted], where n_samples_fitted is the number of samples used in the fitting for the estimator. Passing None as test samples gives the same result as passing real test samples, since DummyRegressor operates independently of the sampled observations.</source>
          <target state="translated">测试样本的形状=(n_samples,n_features)或None。对于某些估计器来说,这可能是一个预计算的内核矩阵,shape=(n_samples,n_samples_fitted],其中n_samples_fitted是估计器拟合时使用的样本数。传递None作为测试样本与传递真实测试样本的结果是一样的,因为DummyRegressor的操作是独立于采样观测值的。</target>
        </trans-unit>
        <trans-unit id="12cad09d9d4837878fb37fd506e5f7fb71a801c9" translate="yes" xml:space="preserve">
          <source>Test samples with shape = (n_samples, n_features) or None. Passing None as test samples gives the same result as passing real test samples, since DummyClassifier operates independently of the sampled observations.</source>
          <target state="translated">测试样本的形状=(n_samples,n_features)或None。传递None作为测试样本与传递真实测试样本的结果是一样的,因为DummyClassifier的操作是独立于采样观测值的。</target>
        </trans-unit>
        <trans-unit id="0c1d5bbb82f5cfc66b7e35e84179b02f4b13f8b1" translate="yes" xml:space="preserve">
          <source>Test samples.</source>
          <target state="translated">测试样品。</target>
        </trans-unit>
        <trans-unit id="bdec5057d32ebe97bd4c525fff2435009f4be0a0" translate="yes" xml:space="preserve">
          <source>Test samples. For some estimators this may be a precomputed kernel matrix instead, shape = (n_samples, n_samples_fitted], where n_samples_fitted is the number of samples used in the fitting for the estimator.</source>
          <target state="translated">测试样本。对于一些估计器来说,这可能是一个预计算的核矩阵,而不是,shape=(n_samples,n_samples_fitted],其中n_samples_fitted是估计器在拟合中使用的样本数。</target>
        </trans-unit>
        <trans-unit id="7f9baccc70399290d29568f9812594e6335c4ae0" translate="yes" xml:space="preserve">
          <source>Test with permutations the significance of a classification score</source>
          <target state="translated">检验分类分值的显著性。</target>
        </trans-unit>
        <trans-unit id="c67f73aee0c3dc1034cba236cfe8c8526d7a9123" translate="yes" xml:space="preserve">
          <source>Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.</source>
          <target state="translated">文本分析是机器学习算法的一个主要应用领域。然而原始数据、符号序列不能直接反馈给算法本身,因为大多数算法希望得到固定大小的数字特征向量,而不是长度可变的原始文本文档。</target>
        </trans-unit>
        <trans-unit id="990226708ed5e3f7319c13e5c3e22d22fd438346" translate="yes" xml:space="preserve">
          <source>Text is made of characters, but files are made of bytes. These bytes represent characters according to some &lt;em&gt;encoding&lt;/em&gt;. To work with text files in Python, their bytes must be &lt;em&gt;decoded&lt;/em&gt; to a character set called Unicode. Common encodings are ASCII, Latin-1 (Western Europe), KOI8-R (Russian) and the universal encodings UTF-8 and UTF-16. Many others exist.</source>
          <target state="translated">文本由字符组成，但文件由字节组成。这些字节根据某种&lt;em&gt;编码&lt;/em&gt;表示字符。要使用Python处理文本文件，必须将其字节&lt;em&gt;解码&lt;/em&gt;为称为Unicode的字符集。常见编码为ASCII，Latin-1（西欧），KOI8-R（俄语）以及通用编码UTF-8和UTF-16。存在许多其他。</target>
        </trans-unit>
        <trans-unit id="2a2f9f7e298485c4a85bf6b791046a1b63087cf9" translate="yes" xml:space="preserve">
          <source>Text preprocessing, tokenizing and filtering of stopwords are all included in &lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt;, which builds a dictionary of features and transforms documents to feature vectors:</source>
          <target state="translated">&lt;a href=&quot;../../modules/generated/sklearn.feature_extraction.text.countvectorizer#sklearn.feature_extraction.text.CountVectorizer&quot;&gt; &lt;code&gt;CountVectorizer&lt;/code&gt; &lt;/a&gt;包含文本预处理，停用词的标记化和过滤功能，该功能可构建功能字典并将文档转换为功能向量：</target>
        </trans-unit>
        <trans-unit id="79142cb36f8945e4d341c82cf5dc060dc02c8f0f" translate="yes" xml:space="preserve">
          <source>Text summary of the precision, recall, F1 score for each class. Dictionary returned if output_dict is True. Dictionary has the following structure:</source>
          <target state="translated">每类的精度、召回率、F1得分的文本摘要。如果output_dict为True,则返回字典。字典的结构如下。</target>
        </trans-unit>
        <trans-unit id="f042ff208f7aff2fdebc20ebdcfe3611681222ed" translate="yes" xml:space="preserve">
          <source>Tf is &amp;ldquo;n&amp;rdquo; (natural) by default, &amp;ldquo;l&amp;rdquo; (logarithmic) when &lt;code&gt;sublinear_tf=True&lt;/code&gt;. Idf is &amp;ldquo;t&amp;rdquo; when use_idf is given, &amp;ldquo;n&amp;rdquo; (none) otherwise. Normalization is &amp;ldquo;c&amp;rdquo; (cosine) when &lt;code&gt;norm='l2'&lt;/code&gt;, &amp;ldquo;n&amp;rdquo; (none) when &lt;code&gt;norm=None&lt;/code&gt;.</source>
          <target state="translated">Tf默认为&amp;ldquo; n&amp;rdquo;（自然），当 &lt;code&gt;sublinear_tf=True&lt;/code&gt; 时为&amp;ldquo; l&amp;rdquo;（对数）。给定use_idf时，Idf为&amp;ldquo; t&amp;rdquo;，否则为&amp;ldquo; n&amp;rdquo;（无）。当 &lt;code&gt;norm='l2'&lt;/code&gt; ，归一化为&amp;ldquo; c&amp;rdquo;（余弦），当 &lt;code&gt;norm=None&lt;/code&gt; 时，归一化为 &amp;ldquo; n&amp;rdquo;（无）。</target>
        </trans-unit>
        <trans-unit id="97730bbab5383bbe19dd65de91be719c29295304" translate="yes" xml:space="preserve">
          <source>Tf means &lt;strong&gt;term-frequency&lt;/strong&gt; while tf&amp;ndash;idf means term-frequency times &lt;strong&gt;inverse document-frequency&lt;/strong&gt;: \(\text{tf-idf(t,d)}=\text{tf(t,d)} \times \text{idf(t)}\).</source>
          <target state="translated">Tf表示&lt;strong&gt;术语频率，&lt;/strong&gt;而tf&amp;ndash;idf表示术语频率乘以&lt;strong&gt;文档&lt;/strong&gt;频率的&lt;strong&gt;倒数&lt;/strong&gt;：\（\ text {tf-idf（t，d）} = \ text {tf（t，d）} \ times \ text {idf （t）} \）。</target>
        </trans-unit>
        <trans-unit id="f1cc0d39fa695e88ce231644990ae2b503602ddb" translate="yes" xml:space="preserve">
          <source>Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.</source>
          <target state="translated">Tf是指术语频率,而tf-idf是指术语频率乘以反文档频率。这是信息检索中常用的术语加权方案,在文档分类中也得到了很好的应用。</target>
        </trans-unit>
        <trans-unit id="771178a448f62a1d367a9045d97d08b26eb6869c" translate="yes" xml:space="preserve">
          <source>Tf-idf-weighted document-term matrix.</source>
          <target state="translated">Tf-idf-加权文件术语矩阵。</target>
        </trans-unit>
        <trans-unit id="daaa1c74bc4f107e3ab2cba2520cc29b4809af00" translate="yes" xml:space="preserve">
          <source>TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most frequent words to features indices and hence compute a word occurrence frequency (sparse) matrix. The word frequencies are then reweighted using the Inverse Document Frequency (IDF) vector collected feature-wise over the corpus.</source>
          <target state="translated">TfidfVectorizer使用内存词汇(python dict)将最频繁的单词映射到特征指数上,从而计算出单词出现频率(稀疏)矩阵。然后,使用在语料库上收集的反文档频率(IDF)向量对词频进行特征加权。</target>
        </trans-unit>
        <trans-unit id="2c1e749a66bcce49e10daf1b4a981af3bebb9284" translate="yes" xml:space="preserve">
          <source>That this function takes time at least quadratic in n_samples. For large datasets, it&amp;rsquo;s wise to set that parameter to a small value.</source>
          <target state="translated">该函数在n_samples中至少花费二次时间。对于大型数据集，将参数设置为较小的值是明智的。</target>
        </trans-unit>
        <trans-unit id="fb20b5f965f5eb1a2ab7f1c1219e1e8adbfdfc00" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced&amp;rdquo; heuristic is inspired by Logistic Regression in Rare Events Data, King, Zen, 2001.</source>
          <target state="translated">&amp;ldquo;平衡的&amp;rdquo;启发式方法是根据《稀有事件数据》中的逻辑回归得出的，King，Zen，2001。</target>
        </trans-unit>
        <trans-unit id="f4d692dace6b9f963c8a80ff6fb54e77b0936bf5" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced&amp;rdquo; mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;</source>
          <target state="translated">&amp;ldquo;平衡&amp;rdquo;模式使用y的值自动将权重与输入数据中的类频率成反比地调整为 &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="ef34b0ee7fbdfc2770447dcdf0759da38193f229" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced&amp;rdquo; mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;.</source>
          <target state="translated">&amp;ldquo;平衡&amp;rdquo;模式使用y的值自动将权重与输入数据中的类频率成反比地调整为 &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="66610aa2288acbb0ecf69897c27cb9799d361b2f" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced&amp;rdquo; mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data: &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt;.</source>
          <target state="translated">&amp;ldquo;平衡&amp;rdquo;模式使用y的值来自动调整与输入数据中的类频率成反比的权重： &lt;code&gt;n_samples / (n_classes * np.bincount(y))&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ef1bbf84c17d648e39cb34085672c6faaeb47082" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;balanced_subsample&amp;rdquo; mode is the same as &amp;ldquo;balanced&amp;rdquo; except that weights are computed based on the bootstrap sample for every tree grown.</source>
          <target state="translated">&amp;ldquo; balanced_subsample&amp;rdquo;模式与&amp;ldquo; balanced&amp;rdquo;相同，不同之处在于，权重是根据每个树生长的引导程序样本计算的。</target>
        </trans-unit>
        <trans-unit id="9570d1ba54b75e486c6a8fe70ab0af402d1567cc" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;lbfgs&amp;rdquo;, &amp;ldquo;sag&amp;rdquo; and &amp;ldquo;newton-cg&amp;rdquo; solvers only support L2 penalization and are found to converge faster for some high dimensional data. Setting &lt;code&gt;multi_class&lt;/code&gt; to &amp;ldquo;multinomial&amp;rdquo; with these solvers learns a true multinomial logistic regression model &lt;a href=&quot;#id26&quot; id=&quot;id23&quot;&gt;[5]&lt;/a&gt;, which means that its probability estimates should be better calibrated than the default &amp;ldquo;one-vs-rest&amp;rdquo; setting.</source>
          <target state="translated">&amp;ldquo; lbfgs&amp;rdquo;，&amp;ldquo; sag&amp;rdquo;和&amp;ldquo; newton-cg&amp;rdquo;求解器仅支持L2罚分，并且发现对于某些高维数据收敛更快。通过这些求解器将 &lt;code&gt;multi_class&lt;/code&gt; 设置为&amp;ldquo;多项式&amp;rdquo;，可以学习到真正的多项式Lo​​gistic回归模型&lt;a href=&quot;#id26&quot; id=&quot;id23&quot;&gt;[5]&lt;/a&gt;，这意味着其概率估计应比默认的&amp;ldquo; one-vs-rest&amp;rdquo;设置更好。</target>
        </trans-unit>
        <trans-unit id="c76cf618fdea0e603c8990092e5d960628df7c0c" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;new&amp;rdquo; data consists of linear combinations of the input data, with weights probabilistically drawn given the KDE model.</source>
          <target state="translated">&amp;ldquo;新&amp;rdquo;数据由输入数据的线性组合组成，并在给定KDE模型的情况下概率得出权重。</target>
        </trans-unit>
        <trans-unit id="420229f24d72cfc948f72b9aaf53e46dfcb25b62" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;sag&amp;rdquo; solver uses a Stochastic Average Gradient descent &lt;a href=&quot;#id27&quot; id=&quot;id24&quot;&gt;[6]&lt;/a&gt;. It is faster than other solvers for large datasets, when both the number of samples and the number of features are large.</source>
          <target state="translated">&amp;ldquo;下垂&amp;rdquo;求解器使用随机平均梯度下降&lt;a href=&quot;#id27&quot; id=&quot;id24&quot;&gt;[6]&lt;/a&gt;。当样本数量和特征数量都很大时，它比大型数据集的其他求解器更快。</target>
        </trans-unit>
        <trans-unit id="cf55bf4220bbf6e5ad8c38b76c827b53a1e3f193" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;saga&amp;rdquo; solver &lt;a href=&quot;#id28&quot; id=&quot;id25&quot;&gt;[7]&lt;/a&gt; is a variant of &amp;ldquo;sag&amp;rdquo; that also supports the non-smooth &lt;code&gt;penalty=&amp;rdquo;l1&amp;rdquo;&lt;/code&gt; option. This is therefore the solver of choice for sparse multinomial logistic regression.</source>
          <target state="translated">&amp;ldquo; saga&amp;rdquo;求解器&lt;a href=&quot;#id28&quot; id=&quot;id25&quot;&gt;[7]&lt;/a&gt;是&amp;ldquo; sag&amp;rdquo;的一种变体，它也支持非平滑 &lt;code&gt;penalty=&amp;rdquo;l1&amp;rdquo;&lt;/code&gt; 选项。因此，这是稀疏多项式逻辑回归的首选求解器。</target>
        </trans-unit>
        <trans-unit id="77bf2f7306c562160b3a78c9199a57470ad6395e" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;saga&amp;rdquo; solver is often the best choice. The &amp;ldquo;liblinear&amp;rdquo; solver is used by default for historical reasons.</source>
          <target state="translated">&amp;ldquo;传奇&amp;rdquo;求解器通常是最佳选择。由于历史原因，默认情况下使用&amp;ldquo; liblinear&amp;rdquo;求解器。</target>
        </trans-unit>
        <trans-unit id="068bc43bd479e1422a1e2139866c2ca587dbb3ad" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;steepness&amp;rdquo; of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.</source>
          <target state="translated">ROC曲线的&amp;ldquo;陡度&amp;rdquo;也很重要，因为理想的是最大程度地提高真实阳性率，同时最小化错误阳性率。</target>
        </trans-unit>
        <trans-unit id="0eb5d5532023d8acbeeebff557bc347056c3c6a7" translate="yes" xml:space="preserve">
          <source>The &amp;ldquo;target&amp;rdquo; for this database is an integer from 0 to 39 indicating the identity of the person pictured; however, with only 10 examples per class, this relatively small dataset is more interesting from an unsupervised or semi-supervised perspective.</source>
          <target state="translated">该数据库的&amp;ldquo;目标&amp;rdquo;是从0到39的整数，表示被摄人物的身份。但是，每个类只有10个示例，从无监督或半监督的角度来看，这个相对较小的数据集更加有趣。</target>
        </trans-unit>
        <trans-unit id="6893a2ecba3f5b3ceba43b94c7037a23940a0678" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;auto&amp;rsquo; mode is the default and is intended to pick the cheaper option of the two depending upon the shape and format of the training data.</source>
          <target state="translated">默认为&amp;ldquo;自动&amp;rdquo;模式，旨在根据训练数据的形状和格式选择两者中较便宜的选项。</target>
        </trans-unit>
        <trans-unit id="0f80449b3a36a9645d51b541d6ac4415080a7df2" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;cd&amp;rsquo; solver can only optimize the Frobenius norm. Due to the underlying non-convexity of NMF, the different solvers may converge to different minima, even when optimizing the same distance function.</source>
          <target state="translated">&amp;ldquo; cd&amp;rdquo;求解器只能优化Frobenius范数。由于NMF的潜在非凸性，即使优化相同的距离函数，不同的求解器也可能会收敛到不同的最小值。</target>
        </trans-unit>
        <trans-unit id="370b11b6ae177f24cc2d42a049dda5c0d7e30775" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;eigen&amp;rsquo; solver is based on the optimization of the between class scatter to within class scatter ratio. It can be used for both classification and transform, and it supports shrinkage. However, the &amp;lsquo;eigen&amp;rsquo; solver needs to compute the covariance matrix, so it might not be suitable for situations with a high number of features.</source>
          <target state="translated">&amp;ldquo;本征&amp;rdquo;求解器基于类间散布到类内散布比率的优化。它既可用于分类也可用于变换，并且支持收缩。但是，&amp;ldquo;本征&amp;rdquo;求解器需要计算协方差矩阵，因此它可能不适合具有大量特征的情况。</target>
        </trans-unit>
        <trans-unit id="ccda076fda793672987d7568e3ca12c3047fb684" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;log&amp;rsquo; loss gives logistic regression, a probabilistic classifier. &amp;lsquo;modified_huber&amp;rsquo; is another smooth loss that brings tolerance to outliers as well as probability estimates. &amp;lsquo;squared_hinge&amp;rsquo; is like hinge but is quadratically penalized. &amp;lsquo;perceptron&amp;rsquo; is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see SGDRegressor for a description.</source>
          <target state="translated">对数损失使逻辑回归成为概率分类器。'modified_huber'是另一个平滑的损失，它使异常值和概率估计具有容忍度。&amp;ldquo; squared_hinge&amp;rdquo;就像铰链一样，但是被二次惩罚。&amp;ldquo;感知器&amp;rdquo;是感知器算法使用的线性损耗。其他损失是为回归而设计的，但也可用于分类。有关说明，请参见SGDRegressor。</target>
        </trans-unit>
        <trans-unit id="5302138e8a256151a982f3c737747f8db1fec2f7" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;lsqr&amp;rsquo; solver is an efficient algorithm that only works for classification. It supports shrinkage.</source>
          <target state="translated">&amp;ldquo; lsqr&amp;rdquo;求解器是仅适用于分类的高效算法。它支持收缩。</target>
        </trans-unit>
        <trans-unit id="e2a91334301a1b93c477cd479a707ce044fefdf3" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;newton-cg&amp;rsquo;, &amp;lsquo;sag&amp;rsquo;, and &amp;lsquo;lbfgs&amp;rsquo; solvers support only L2 regularization with primal formulation. The &amp;lsquo;liblinear&amp;rsquo; solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.</source>
          <target state="translated">&amp;ldquo; newton-cg&amp;rdquo;，&amp;ldquo; sag&amp;rdquo;和&amp;ldquo; lbfgs&amp;rdquo;求解器仅支持具有原始公式的L2正则化。'liblinear'求解器支持L1和L2正则化，仅针对L2罚分采用双重公式。</target>
        </trans-unit>
        <trans-unit id="ade2e6c6872bcfb8e63408411f739881d7395764" translate="yes" xml:space="preserve">
          <source>The &amp;lsquo;squared_loss&amp;rsquo; refers to the ordinary least squares fit. &amp;lsquo;huber&amp;rsquo; modifies &amp;lsquo;squared_loss&amp;rsquo; to focus less on getting outliers correct by switching from squared to linear loss past a distance of epsilon. &amp;lsquo;epsilon_insensitive&amp;rsquo; ignores errors less than epsilon and is linear past that; this is the loss function used in SVR. &amp;lsquo;squared_epsilon_insensitive&amp;rsquo; is the same but becomes squared loss past a tolerance of epsilon.</source>
          <target state="translated">&amp;ldquo; squared_loss&amp;rdquo;是指普通的最小二乘拟合。'huber'修改了'squared_loss'，以便通过将平方损失转换为线性损失超过&amp;epsilon;距离，从而减少对异常值校正的关注。'epsilon_insensitive'忽略小于epsilon的错误，并且线性地超过该错误；这是SVR中使用的损失函数。'squared_epsilon_insensitive'是相同的，但是变成超过&amp;epsilon;容差的平方损耗。</target>
        </trans-unit>
        <trans-unit id="388443bd992c152f7c80a788085a15982e280e0a" translate="yes" xml:space="preserve">
          <source>The (scaled) interquartile range for each feature in the training set.</source>
          <target state="translated">训练集中每个特征的(按比例)四分位数范围。</target>
        </trans-unit>
        <trans-unit id="b3533a4edec1fdb05f12a2a421dc320606ca77c6" translate="yes" xml:space="preserve">
          <source>The (sometimes surprising) observation is that this is &lt;em&gt;still a linear model&lt;/em&gt;: to see this, imagine creating a new variable</source>
          <target state="translated">（有时令人惊讶的）观察结果是，这&lt;em&gt;仍然是线性模型&lt;/em&gt;：要看到这一点，请想象创建一个新变量</target>
        </trans-unit>
        <trans-unit id="ae37fbc1863417aba870f086fd7dcb7d12932667" translate="yes" xml:space="preserve">
          <source>The (x,y) position of the lower-left corner, in degrees</source>
          <target state="translated">左下角的(x,y)位置,度数</target>
        </trans-unit>
        <trans-unit id="bcd6ca42c3472afbe27069a62710b5c531496d9b" translate="yes" xml:space="preserve">
          <source>The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper &amp;ldquo;Newsweeder: Learning to filter netnews,&amp;rdquo; though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.</source>
          <target state="translated">20个新闻组数据集是大约20,000个新闻组文档的集合，在20个不同的新闻组中平均（几乎）划分。据我们所知，它最初是由Ken Lang收集的，可能是因为他的论文&amp;ldquo; Newsweeder：学习过滤网络新闻&amp;rdquo;，尽管他没有明确提及该收集。20个新闻组集合已成为在机器学习技术的文本应用程序（例如文本分类和文本聚类）中进行实验的流行数据集。</target>
        </trans-unit>
        <trans-unit id="4b2a042059fffe007deb9ebabf02d8062c1e6bda" translate="yes" xml:space="preserve">
          <source>The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.</source>
          <target state="translated">20个新闻组数据集由20个主题的约18000个新闻组帖子组成,分为两个子集:一个用于训练(或开发),另一个用于测试(或性能评估)。训练集和测试集之间的分割是基于特定日期之前和之后发布的消息。</target>
        </trans-unit>
        <trans-unit id="c380ecdb017c04631da3ca1753b6ddf07ce8267f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.cluster&quot;&gt;&lt;code&gt;sklearn.cluster&lt;/code&gt;&lt;/a&gt; module gathers popular unsupervised clustering algorithms.</source>
          <target state="translated">该&lt;a href=&quot;#module-sklearn.cluster&quot;&gt; &lt;code&gt;sklearn.cluster&lt;/code&gt; &lt;/a&gt;模块收集流行的无监督聚类算法。</target>
        </trans-unit>
        <trans-unit id="6a798b177e574d6ff4ac12be7b93e3b8f8d74b71" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.covariance&quot;&gt;&lt;code&gt;sklearn.covariance&lt;/code&gt;&lt;/a&gt; module includes methods and algorithms to robustly estimate the covariance of features given a set of points. The precision matrix defined as the inverse of the covariance is also estimated. Covariance estimation is closely related to the theory of Gaussian Graphical Models.</source>
          <target state="translated">所述&lt;a href=&quot;#module-sklearn.covariance&quot;&gt; &lt;code&gt;sklearn.covariance&lt;/code&gt; &lt;/a&gt;模块包括这样的方法和算法来鲁棒地估计给定的一组点的特征的协方差。还估计了定义为协方差的倒数的精度矩阵。协方差估计与高斯图形模型理论密切相关。</target>
        </trans-unit>
        <trans-unit id="d1230decfda989b60168bc88df7b70ef79122b2d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.datasets&quot;&gt;&lt;code&gt;sklearn.datasets&lt;/code&gt;&lt;/a&gt; module includes utilities to load datasets, including methods to load and fetch popular reference datasets. It also features some artificial data generators.</source>
          <target state="translated">所述&lt;a href=&quot;#module-sklearn.datasets&quot;&gt; &lt;code&gt;sklearn.datasets&lt;/code&gt; &lt;/a&gt;模块包括公用事业负载数据集，包括方法来加载和读取流行参考数据集。它还具有一些人工数据生成器。</target>
        </trans-unit>
        <trans-unit id="94bdb0abc615359801b0dde8f5ed432fa774aae6" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.decomposition&quot;&gt;&lt;code&gt;sklearn.decomposition&lt;/code&gt;&lt;/a&gt; module includes matrix decomposition algorithms, including among others PCA, NMF or ICA. Most of the algorithms of this module can be regarded as dimensionality reduction techniques.</source>
          <target state="translated">所述&lt;a href=&quot;#module-sklearn.decomposition&quot;&gt; &lt;code&gt;sklearn.decomposition&lt;/code&gt; &lt;/a&gt;模块包括矩阵分解算法，包括除其他PCA，NMF或ICA。该模块的大多数算法都可以视为降维技术。</target>
        </trans-unit>
        <trans-unit id="cce83af2900332bbe995457713d2e3977e6cea91" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module includes ensemble-based methods for classification, regression and anomaly detection.</source>
          <target state="translated">所述&lt;a href=&quot;#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt;模块包括用于分类，回归和异常检测基于集合的方法。</target>
        </trans-unit>
        <trans-unit id="a3b34965d608c8571221686c4eaa7dd2128cda34" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.exceptions&quot;&gt;&lt;code&gt;sklearn.exceptions&lt;/code&gt;&lt;/a&gt; module includes all custom warnings and error classes used across scikit-learn.</source>
          <target state="translated">该&lt;a href=&quot;#module-sklearn.exceptions&quot;&gt; &lt;code&gt;sklearn.exceptions&lt;/code&gt; &lt;/a&gt;模块包括所有使用的自定义警告和错误类scikit学习。</target>
        </trans-unit>
        <trans-unit id="953e85b8304fe86126d3f8d4d49e2c347def818a" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.feature_extraction&quot;&gt;&lt;code&gt;sklearn.feature_extraction&lt;/code&gt;&lt;/a&gt; module deals with feature extraction from raw data. It currently includes methods to extract features from text and images.</source>
          <target state="translated">该&lt;a href=&quot;#module-sklearn.feature_extraction&quot;&gt; &lt;code&gt;sklearn.feature_extraction&lt;/code&gt; &lt;/a&gt;模块与原始数据特征提取交易。当前，它包括从文本和图像中提取特征的方法。</target>
        </trans-unit>
        <trans-unit id="2ff97b1fa019f5f400e3468860cd96aea04f63dc" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.feature_extraction.image&quot;&gt;&lt;code&gt;sklearn.feature_extraction.image&lt;/code&gt;&lt;/a&gt; submodule gathers utilities to extract features from images.</source>
          <target state="translated">该&lt;a href=&quot;#module-sklearn.feature_extraction.image&quot;&gt; &lt;code&gt;sklearn.feature_extraction.image&lt;/code&gt; &lt;/a&gt;子模块合工具来从图像中提取的特征。</target>
        </trans-unit>
        <trans-unit id="5fb7f21374928a29d973d39c8eed205507941559" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.feature_extraction.text&quot;&gt;&lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt;&lt;/a&gt; submodule gathers utilities to build feature vectors from text documents.</source>
          <target state="translated">该&lt;a href=&quot;#module-sklearn.feature_extraction.text&quot;&gt; &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; &lt;/a&gt;子模块合实用程序从文本文档建立特征向量。</target>
        </trans-unit>
        <trans-unit id="f8894b12541a4b0b591ccbf9c1c5e42bf4d0c13b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.feature_selection&quot;&gt;&lt;code&gt;sklearn.feature_selection&lt;/code&gt;&lt;/a&gt; module implements feature selection algorithms. It currently includes univariate filter selection methods and the recursive feature elimination algorithm.</source>
          <target state="translated">该&lt;a href=&quot;#module-sklearn.feature_selection&quot;&gt; &lt;code&gt;sklearn.feature_selection&lt;/code&gt; &lt;/a&gt;模块实现功能选择算法。目前，它包括单变量过滤器选择方法和递归特征消除算法。</target>
        </trans-unit>
        <trans-unit id="af392a06a08e896e0c1f9a845ceba81c0151ed14" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.gaussian_process&quot;&gt;&lt;code&gt;sklearn.gaussian_process&lt;/code&gt;&lt;/a&gt; module implements Gaussian Process based regression and classification.</source>
          <target state="translated">所述&lt;a href=&quot;#module-sklearn.gaussian_process&quot;&gt; &lt;code&gt;sklearn.gaussian_process&lt;/code&gt; &lt;/a&gt;模块实现高斯过程基于回归和分类。</target>
        </trans-unit>
        <trans-unit id="e25959a779b184ae02a906c2808f68686c73aab5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.kernel_approximation&quot;&gt;&lt;code&gt;sklearn.kernel_approximation&lt;/code&gt;&lt;/a&gt; module implements several approximate kernel feature maps base on Fourier transforms.</source>
          <target state="translated">该&lt;a href=&quot;#module-sklearn.kernel_approximation&quot;&gt; &lt;code&gt;sklearn.kernel_approximation&lt;/code&gt; &lt;/a&gt;模块实现了几个近似内核提供了地图上的傅立叶变换的基础。</target>
        </trans-unit>
        <trans-unit id="cf3dc31fd9ef458aef6de4af32bf51a7e61106a1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.linear_model&quot;&gt;&lt;code&gt;sklearn.linear_model&lt;/code&gt;&lt;/a&gt; module implements generalized linear models. It includes Ridge regression, Bayesian Regression, Lasso and Elastic Net estimators computed with Least Angle Regression and coordinate descent. It also implements Stochastic Gradient Descent related algorithms.</source>
          <target state="translated">该&lt;a href=&quot;#module-sklearn.linear_model&quot;&gt; &lt;code&gt;sklearn.linear_model&lt;/code&gt; &lt;/a&gt;模块实现广义线性模型。它包括利用最小角度回归和坐标下降计算的Ridge回归，贝叶斯回归，套索和弹性网估计量。它还实现了与随机梯度下降相关的算法。</target>
        </trans-unit>
        <trans-unit id="75e43c84de91a4a1d643ca88db0beef9e86494b8" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.manifold&quot;&gt;&lt;code&gt;sklearn.manifold&lt;/code&gt;&lt;/a&gt; module implements data embedding techniques.</source>
          <target state="translated">该&lt;a href=&quot;#module-sklearn.manifold&quot;&gt; &lt;code&gt;sklearn.manifold&lt;/code&gt; &lt;/a&gt;模块实现数据嵌入技术。</target>
        </trans-unit>
        <trans-unit id="f55aa3d6c230d41fe62ec5929fa59e00645b5d62" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; module includes score functions, performance metrics and pairwise metrics and distance computations.</source>
          <target state="translated">所述&lt;a href=&quot;#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; &lt;/a&gt;模块包括得分功能，性能度量和成对度量和距离计算。</target>
        </trans-unit>
        <trans-unit id="90553131dabe004a613ea2c87be35b6b6db9a1ae" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.metrics.cluster&quot;&gt;&lt;code&gt;sklearn.metrics.cluster&lt;/code&gt;&lt;/a&gt; submodule contains evaluation metrics for cluster analysis results. There are two forms of evaluation:</source>
          <target state="translated">该&lt;a href=&quot;#module-sklearn.metrics.cluster&quot;&gt; &lt;code&gt;sklearn.metrics.cluster&lt;/code&gt; &lt;/a&gt;子模块包含了聚类分析的结果评价指标。评估有两种形式：</target>
        </trans-unit>
        <trans-unit id="537333336506a029d4e76c0c5320f3e14636c908" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.mixture&quot;&gt;&lt;code&gt;sklearn.mixture&lt;/code&gt;&lt;/a&gt; module implements mixture modeling algorithms.</source>
          <target state="translated">所述&lt;a href=&quot;#module-sklearn.mixture&quot;&gt; &lt;code&gt;sklearn.mixture&lt;/code&gt; &lt;/a&gt;模块实现混合物建模算法。</target>
        </trans-unit>
        <trans-unit id="1b9bcfe9136ee1328197e574e66457c49aa39ded" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.naive_bayes&quot;&gt;&lt;code&gt;sklearn.naive_bayes&lt;/code&gt;&lt;/a&gt; module implements Naive Bayes algorithms. These are supervised learning methods based on applying Bayes&amp;rsquo; theorem with strong (naive) feature independence assumptions.</source>
          <target state="translated">该&lt;a href=&quot;#module-sklearn.naive_bayes&quot;&gt; &lt;code&gt;sklearn.naive_bayes&lt;/code&gt; &lt;/a&gt;模块实现朴素贝叶斯算法。这些是基于贝叶斯定理和强（天真）特征独立性假设的监督学习方法。</target>
        </trans-unit>
        <trans-unit id="31da4b6c2407f749b7e6e441bc1101265b243a97" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; module implements the k-nearest neighbors algorithm.</source>
          <target state="translated">该&lt;a href=&quot;#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; &lt;/a&gt;模块实现了最近邻居法。</target>
        </trans-unit>
        <trans-unit id="637db5b82af4c4775ad8c11b2cc086c407ac4adc" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.neural_network&quot;&gt;&lt;code&gt;sklearn.neural_network&lt;/code&gt;&lt;/a&gt; module includes models based on neural networks.</source>
          <target state="translated">该&lt;a href=&quot;#module-sklearn.neural_network&quot;&gt; &lt;code&gt;sklearn.neural_network&lt;/code&gt; &lt;/a&gt;模块包括基于神经网络模型。</target>
        </trans-unit>
        <trans-unit id="97c84f48ddcbce9eff5bb423de61ca9bed7742a5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline&lt;/code&gt;&lt;/a&gt; module implements utilities to build a composite estimator, as a chain of transforms and estimators.</source>
          <target state="translated">所述&lt;a href=&quot;#module-sklearn.pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline&lt;/code&gt; &lt;/a&gt;模块实现实用程序建立一个复合估计，如变换和估计的链。</target>
        </trans-unit>
        <trans-unit id="3e4a3abf94a63259dfe9d5546d6b613a02821c2d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.preprocessing&quot;&gt;&lt;code&gt;sklearn.preprocessing&lt;/code&gt;&lt;/a&gt; module includes scaling, centering, normalization, binarization and imputation methods.</source>
          <target state="translated">所述&lt;a href=&quot;#module-sklearn.preprocessing&quot;&gt; &lt;code&gt;sklearn.preprocessing&lt;/code&gt; &lt;/a&gt;模块包括缩放，定心，归一化，二值化和估算方法。</target>
        </trans-unit>
        <trans-unit id="5fd91efb13a21a364a66a195be3f60dbc3429cc3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.semi_supervised&quot;&gt;&lt;code&gt;sklearn.semi_supervised&lt;/code&gt;&lt;/a&gt; module implements semi-supervised learning algorithms. These algorithms utilized small amounts of labeled data and large amounts of unlabeled data for classification tasks. This module includes Label Propagation.</source>
          <target state="translated">该&lt;a href=&quot;#module-sklearn.semi_supervised&quot;&gt; &lt;code&gt;sklearn.semi_supervised&lt;/code&gt; &lt;/a&gt;模块实现半监督学习算法。这些算法将少量标记的数据和大量未标记的数据用于分类任务。该模块包括标签传播。</target>
        </trans-unit>
        <trans-unit id="6b1e5de562db4c7ae4499c2a4fcb3f75a3027318" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.svm&quot;&gt;&lt;code&gt;sklearn.svm&lt;/code&gt;&lt;/a&gt; module includes Support Vector Machine algorithms.</source>
          <target state="translated">该&lt;a href=&quot;#module-sklearn.svm&quot;&gt; &lt;code&gt;sklearn.svm&lt;/code&gt; &lt;/a&gt;模块包括支持向量机算法。</target>
        </trans-unit>
        <trans-unit id="ef3c16856f883650f7c10c3b8b62a045804fc7da" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.tree&quot;&gt;&lt;code&gt;sklearn.tree&lt;/code&gt;&lt;/a&gt; module includes decision tree-based models for classification and regression.</source>
          <target state="translated">该&lt;a href=&quot;#module-sklearn.tree&quot;&gt; &lt;code&gt;sklearn.tree&lt;/code&gt; &lt;/a&gt;模块包括用于分类和回归的决定基于树的模型。</target>
        </trans-unit>
        <trans-unit id="fd392963cb16a5b60813f22af8246fa4065eb565" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;#module-sklearn.utils&quot;&gt;&lt;code&gt;sklearn.utils&lt;/code&gt;&lt;/a&gt; module includes various utilities.</source>
          <target state="translated">所述&lt;a href=&quot;#module-sklearn.utils&quot;&gt; &lt;code&gt;sklearn.utils&lt;/code&gt; &lt;/a&gt;模块包括各种用途。</target>
        </trans-unit>
        <trans-unit id="9c614be243d558a71ca4ede548ecf4767e4adc7c" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;simple example on this dataset&lt;/a&gt; illustrates how starting from the original problem one can shape the data for consumption in scikit-learn.</source>
          <target state="translated">&lt;a href=&quot;../../auto_examples/classification/plot_digits_classification#sphx-glr-auto-examples-classification-plot-digits-classification-py&quot;&gt;这个数据集上&lt;/a&gt;的一个简单示例说明了如何从原始问题开始，将数据塑造为scikit-learn中的数据。</target>
        </trans-unit>
        <trans-unit id="424aa7402b9869b036306a671e3630b4177e36b0" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../modules/tree#tree&quot;&gt;decision trees&lt;/a&gt; is used to fit a sine curve with addition noisy observation. As a result, it learns local linear regressions approximating the sine curve.</source>
          <target state="translated">该&lt;a href=&quot;../../modules/tree#tree&quot;&gt;决策树&lt;/a&gt;来拟合与另外嘈杂观察正弦曲线。结果，它学习了近似正弦曲线的局部线性回归。</target>
        </trans-unit>
        <trans-unit id="eb2cbae46431d84a4889d55d659950b594e78664" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../../modules/tree#tree&quot;&gt;decision trees&lt;/a&gt; is used to predict simultaneously the noisy x and y observations of a circle given a single underlying feature. As a result, it learns local linear regressions approximating the circle.</source>
          <target state="translated">所述&lt;a href=&quot;../../modules/tree#tree&quot;&gt;决策树&lt;/a&gt;被用于同时预测嘈杂x和给定的一个底层特征的圆的Y观察。结果，它学习逼近圆的局部线性回归。</target>
        </trans-unit>
        <trans-unit id="4ea0ad8f51ec5bec92f088b272fa90a6ac2d5b55" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt;&lt;/a&gt; function is a data fetching / caching functions that downloads the data archive from the original &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;20 newsgroups website&lt;/a&gt;, extracts the archive contents in the &lt;code&gt;~/scikit_learn_data/20news_home&lt;/code&gt; folder and calls the &lt;a href=&quot;../modules/generated/sklearn.datasets.load_files#sklearn.datasets.load_files&quot;&gt;&lt;code&gt;sklearn.datasets.load_files&lt;/code&gt;&lt;/a&gt; on either the training or testing set folder, or both of them:</source>
          <target state="translated">该&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_20newsgroups#sklearn.datasets.fetch_20newsgroups&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_20newsgroups&lt;/code&gt; &lt;/a&gt;功能是数据读取/缓存功能是下载的数据从原来的存档&lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;20个新闻组的网站&lt;/a&gt;，在提取存档内容的 &lt;code&gt;~/scikit_learn_data/20news_home&lt;/code&gt; 文件夹，并调用&lt;a href=&quot;../modules/generated/sklearn.datasets.load_files#sklearn.datasets.load_files&quot;&gt; &lt;code&gt;sklearn.datasets.load_files&lt;/code&gt; &lt;/a&gt;在任培训或测试设置文件夹，或两者都测试：</target>
        </trans-unit>
        <trans-unit id="26c038b3ea935758dab579b3237ee5588d78f251" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_pairs#sklearn.datasets.fetch_lfw_pairs&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_lfw_pairs&lt;/code&gt;&lt;/a&gt; datasets is subdivided into 3 subsets: the development &lt;code&gt;train&lt;/code&gt; set, the development &lt;code&gt;test&lt;/code&gt; set and an evaluation &lt;code&gt;10_folds&lt;/code&gt; set meant to compute performance metrics using a 10-folds cross validation scheme.</source>
          <target state="translated">所述&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_lfw_pairs#sklearn.datasets.fetch_lfw_pairs&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_lfw_pairs&lt;/code&gt; &lt;/a&gt;数据集被分成3个子集：开发 &lt;code&gt;train&lt;/code&gt; 组，开发 &lt;code&gt;test&lt;/code&gt; 组和一个评估 &lt;code&gt;10_folds&lt;/code&gt; 集使用10折叠交叉验证方案意在计算性能指标。</target>
        </trans-unit>
        <trans-unit id="d14958ad2582740fd909337c2882b7ba18717e2a" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module includes two averaging algorithms based on randomized &lt;a href=&quot;tree#tree&quot;&gt;decision trees&lt;/a&gt;: the RandomForest algorithm and the Extra-Trees method. Both algorithms are perturb-and-combine techniques &lt;a href=&quot;#b1998&quot; id=&quot;id5&quot;&gt;[B1998]&lt;/a&gt; specifically designed for trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers.</source>
          <target state="translated">该&lt;a href=&quot;classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt;模块包括基于随机2种平均算法&lt;a href=&quot;tree#tree&quot;&gt;决策树&lt;/a&gt;：在随机森林算法和特树方法。两种算法都是专为树木设计的扰动与组合技术&lt;a href=&quot;#b1998&quot; id=&quot;id5&quot;&gt;[B1998]&lt;/a&gt;。这意味着通过在分类器构造中引入随机性来创建多样化的分类器集。集合的预测作为各个分类器的平均预测给出。</target>
        </trans-unit>
        <trans-unit id="fbeef59e0313a7e281a500dd36152abed677fa2e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.feature_extraction&quot;&gt;&lt;code&gt;sklearn.feature_extraction&lt;/code&gt;&lt;/a&gt; module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.</source>
          <target state="translated">所述&lt;a href=&quot;classes#module-sklearn.feature_extraction&quot;&gt; &lt;code&gt;sklearn.feature_extraction&lt;/code&gt; &lt;/a&gt;模块可用于提取特征在选自格式，如文本和图像的数据集由机器学习算法所支持的格式。</target>
        </trans-unit>
        <trans-unit id="565412031e53246181e593ab56b9ab7f3accb362" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; module implements several loss, score, and utility functions to measure classification performance. Some metrics might require probability estimates of the positive class, confidence values, or binary decisions values. Most implementations allow each sample to provide a weighted contribution to the overall score, through the &lt;code&gt;sample_weight&lt;/code&gt; parameter.</source>
          <target state="translated">该&lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; &lt;/a&gt;模块实现了几种损耗，得分，和实用功能来衡量分类性能。一些度量可能需要对正类的概率估计，置信度值或二进制决策值。大多数实现都允许每个样本通过 &lt;code&gt;sample_weight&lt;/code&gt; 参数为总得分提供加权贡献。</target>
        </trans-unit>
        <trans-unit id="6986be647f522d4ad92a86deccdacfb4588f163b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; module implements several loss, score, and utility functions to measure regression performance. Some of those have been enhanced to handle the multioutput case: &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;mean_squared_error&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt;&lt;code&gt;mean_absolute_error&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">该&lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; &lt;/a&gt;模块实现了几种损耗，得分和效用函数来衡量回归的表现。其中一些功能已得到增强，可以处理多输出情况：&lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;mean_squared_error&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt; &lt;code&gt;mean_absolute_error&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e3af9dc32993fb04e5c47da4dea690da48a6baa4" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt;&lt;code&gt;sklearn.metrics&lt;/code&gt;&lt;/a&gt; module implements several loss, score, and utility functions. For more information see the &lt;a href=&quot;clustering#clustering-evaluation&quot;&gt;Clustering performance evaluation&lt;/a&gt; section for instance clustering, and &lt;a href=&quot;biclustering#biclustering-evaluation&quot;&gt;Biclustering evaluation&lt;/a&gt; for biclustering.</source>
          <target state="translated">该&lt;a href=&quot;classes#module-sklearn.metrics&quot;&gt; &lt;code&gt;sklearn.metrics&lt;/code&gt; &lt;/a&gt;模块实现了几种损耗，得分，和实用功能。有关更多信息，请参见&amp;ldquo; &lt;a href=&quot;clustering#clustering-evaluation&quot;&gt;聚类性能评估&amp;rdquo;&lt;/a&gt;部分中的实例聚类，以及&amp;ldquo;聚类&lt;a href=&quot;biclustering#biclustering-evaluation&quot;&gt;评估&amp;rdquo;&lt;/a&gt;中的&amp;ldquo;双聚类&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="095cb4e1ad7cf586616a563cdbf95404fbb2310e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt;&lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt;&lt;/a&gt; submodule implements utilities to evaluate pairwise distances or affinity of sets of samples.</source>
          <target state="translated">该&lt;a href=&quot;classes#module-sklearn.metrics.pairwise&quot;&gt; &lt;code&gt;sklearn.metrics.pairwise&lt;/code&gt; &lt;/a&gt;子模块工具工具来评估成对距离或组样品的亲和力。</target>
        </trans-unit>
        <trans-unit id="60f0063776d96ccddba5880841f7defdb7f0d5d9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt;&lt;code&gt;sklearn.multiclass&lt;/code&gt;&lt;/a&gt; module implements &lt;em&gt;meta-estimators&lt;/em&gt; to solve &lt;code&gt;multiclass&lt;/code&gt; and &lt;code&gt;multilabel&lt;/code&gt; classification problems by decomposing such problems into binary classification problems. Multitarget regression is also supported.</source>
          <target state="translated">所述&lt;a href=&quot;classes#module-sklearn.multiclass&quot;&gt; &lt;code&gt;sklearn.multiclass&lt;/code&gt; &lt;/a&gt;模块实现&lt;em&gt;的元估计&lt;/em&gt;来解决 &lt;code&gt;multiclass&lt;/code&gt; 和 &lt;code&gt;multilabel&lt;/code&gt; 通过分解这样的问题为二进制分类问题分类问题。还支持多目标回归。</target>
        </trans-unit>
        <trans-unit id="8db5d205727541fd60809b9d143967244bf8e79b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;classes#module-sklearn.random_projection&quot;&gt;&lt;code&gt;sklearn.random_projection&lt;/code&gt;&lt;/a&gt; module implements a simple and computationally efficient way to reduce the dimensionality of the data by trading a controlled amount of accuracy (as additional variance) for faster processing times and smaller model sizes. This module implements two types of unstructured random matrix: &lt;a href=&quot;#gaussian-random-matrix&quot;&gt;Gaussian random matrix&lt;/a&gt; and &lt;a href=&quot;#sparse-random-matrix&quot;&gt;sparse random matrix&lt;/a&gt;.</source>
          <target state="translated">所述&lt;a href=&quot;classes#module-sklearn.random_projection&quot;&gt; &lt;code&gt;sklearn.random_projection&lt;/code&gt; &lt;/a&gt;模块实现了简单的和计算上有效的方式，通过进行更快的处理时间和更小的模型大小的交易精度受控量（作为附加方差）来减少数据的维数。该模块实现两种非结构化随机矩阵：&lt;a href=&quot;#gaussian-random-matrix&quot;&gt;高斯随机矩阵&lt;/a&gt;和&lt;a href=&quot;#sparse-random-matrix&quot;&gt;稀疏随机矩阵&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="aaa03339275413a44471cce8ed9110742f7e29fb" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt;&lt;code&gt;AgglomerativeClustering&lt;/code&gt;&lt;/a&gt; object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy:</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering&quot;&gt; &lt;code&gt;AgglomerativeClustering&lt;/code&gt; &lt;/a&gt;对象执行使用自下而上的方法分级聚类：在其自己的集群的每个观测开始，并且簇依次合并在一起。链接标准确定用于合并策略的度量：</target>
        </trans-unit>
        <trans-unit id="913b5a9805377fabb258d2653b5b70e8adeffb2c" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.bicluster.spectralbiclustering#sklearn.cluster.bicluster.SpectralBiclustering&quot;&gt;&lt;code&gt;SpectralBiclustering&lt;/code&gt;&lt;/a&gt; algorithm assumes that the input data matrix has a hidden checkerboard structure. The rows and columns of a matrix with this structure may be partitioned so that the entries of any bicluster in the Cartesian product of row clusters and column clusters are approximately constant. For instance, if there are two row partitions and three column partitions, each row will belong to three biclusters, and each column will belong to two biclusters.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.cluster.bicluster.spectralbiclustering#sklearn.cluster.bicluster.SpectralBiclustering&quot;&gt; &lt;code&gt;SpectralBiclustering&lt;/code&gt; &lt;/a&gt;算法假定该输入数据矩阵具有隐蔽棋盘结构。可以对具有这种结构的矩阵的行和列进行分区，以使行簇和列簇的笛卡尔积中任何双簇的条目近似恒定。例如，如果有两个行分区和三个列分区，则每行将属于三个bicluster，而每列将属于两个bicluster。</target>
        </trans-unit>
        <trans-unit id="96812a842015efa920168397038c120e6e957561" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.bicluster.spectralcoclustering#sklearn.cluster.bicluster.SpectralCoclustering&quot;&gt;&lt;code&gt;SpectralCoclustering&lt;/code&gt;&lt;/a&gt; algorithm finds biclusters with values higher than those in the corresponding other rows and columns. Each row and each column belongs to exactly one bicluster, so rearranging the rows and columns to make partitions contiguous reveals these high values along the diagonal:</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.cluster.bicluster.spectralcoclustering#sklearn.cluster.bicluster.SpectralCoclustering&quot;&gt; &lt;code&gt;SpectralCoclustering&lt;/code&gt; &lt;/a&gt;算法找到biclusters其值比在相应的其他的行和列更高。每行和每一列恰好属于一个二元组，因此重新排列行和列以使分区连续可以揭示沿对角线的这些高值：</target>
        </trans-unit>
        <trans-unit id="9debcd56df8be7e32ea091b79dc8e313d63ea1d3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.birch#sklearn.cluster.Birch&quot;&gt;&lt;code&gt;Birch&lt;/code&gt;&lt;/a&gt; builds a tree called the Characteristic Feature Tree (CFT) for the given data. The data is essentially lossy compressed to a set of Characteristic Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Characteristic Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children.</source>
          <target state="translated">的&lt;a href=&quot;generated/sklearn.cluster.birch#sklearn.cluster.Birch&quot;&gt; &lt;code&gt;Birch&lt;/code&gt; &lt;/a&gt;建立一个称为特征树（CFT），用于给定数据树。数据实质上是有损压缩到一组特征特征节点（CF节点）的。 CF节点具有许多称为特征特征子群集（CF子群集）的子群集，并且位于非终端CF节点中的这些CF子群集可以将CF节点作为子节点。</target>
        </trans-unit>
        <trans-unit id="e8b115edebda7f7bf86445503d0ce08900b4f8de" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt;&lt;code&gt;DBSCAN&lt;/code&gt;&lt;/a&gt; algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of &lt;em&gt;core samples&lt;/em&gt;, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, &lt;code&gt;min_samples&lt;/code&gt; and &lt;code&gt;eps&lt;/code&gt;, which define formally what we mean when we say &lt;em&gt;dense&lt;/em&gt;. Higher &lt;code&gt;min_samples&lt;/code&gt; or lower &lt;code&gt;eps&lt;/code&gt; indicate higher density necessary to form a cluster.</source>
          <target state="translated">的&lt;a href=&quot;generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN&quot;&gt; &lt;code&gt;DBSCAN&lt;/code&gt; &lt;/a&gt;算法观看簇高密度的区域分隔开由低密度的区域。由于这种相当普通的观点，DBSCAN发现的簇可以是任何形状，而k-均值则假定簇是凸形的。 DBSCAN的核心组件是&lt;em&gt;核心样本&lt;/em&gt;的概念，即高密度区域中的样本。因此，群集是一组核心样本，每个样本彼此接近（通过某种距离度量来测量），以及一组与核心样本接近（但本身不是核心样本）的非核心样本。有两个参数的算法， &lt;code&gt;min_samples&lt;/code&gt; 和 &lt;code&gt;eps&lt;/code&gt; ，其正式定义了我们，当我们说的意思是&lt;em&gt;密集&lt;/em&gt;。更高 &lt;code&gt;min_samples&lt;/code&gt; 或较低的 &lt;code&gt;eps&lt;/code&gt; 表示形成簇所需的较高密度。</target>
        </trans-unit>
        <trans-unit id="844222980d29de5ed47698200091f13bdd09a284" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.featureagglomeration#sklearn.cluster.FeatureAgglomeration&quot;&gt;&lt;code&gt;FeatureAgglomeration&lt;/code&gt;&lt;/a&gt; uses agglomerative clustering to group together features that look very similar, thus decreasing the number of features. It is a dimensionality reduction tool, see &lt;a href=&quot;unsupervised_reduction#data-reduction&quot;&gt;Unsupervised dimensionality reduction&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.cluster.featureagglomeration#sklearn.cluster.FeatureAgglomeration&quot;&gt; &lt;code&gt;FeatureAgglomeration&lt;/code&gt; &lt;/a&gt;使用聚集聚类将看起来非常相似的要素组合在一起，从而减少了要素数量。它是&lt;a href=&quot;unsupervised_reduction#data-reduction&quot;&gt;降维&lt;/a&gt;工具，请参阅无监督降维。</target>
        </trans-unit>
        <trans-unit id="3ccd68ae912b5d8e7b609345a756782aaea1f1b3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt;&lt;code&gt;KMeans&lt;/code&gt;&lt;/a&gt; algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the &lt;a href=&quot;inertia&quot;&gt;inertia&lt;/a&gt; or within-cluster sum-of-squares. This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.</source>
          <target state="translated">的&lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt; &lt;code&gt;KMeans&lt;/code&gt; &lt;/a&gt;通过尝试不同的样品中的n个组等于方差，最小化被称为一个准则算法簇的数据&lt;a href=&quot;inertia&quot;&gt;惯性&lt;/a&gt;或内簇求和的平方。该算法要求指定簇数。它可以很好地扩展到大量样品，并已在许多不同领域的广泛应用领域中使用。</target>
        </trans-unit>
        <trans-unit id="f8f9ba49e304c2e7e84cbf4122c9838a65e0d463" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt;&lt;code&gt;MiniBatchKMeans&lt;/code&gt;&lt;/a&gt; is a variant of the &lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt;&lt;code&gt;KMeans&lt;/code&gt;&lt;/a&gt; algorithm which uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function. Mini-batches are subsets of the input data, randomly sampled in each training iteration. These mini-batches drastically reduce the amount of computation required to converge to a local solution. In contrast to other algorithms that reduce the convergence time of k-means, mini-batch k-means produces results that are generally only slightly worse than the standard algorithm.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans&quot;&gt; &lt;code&gt;MiniBatchKMeans&lt;/code&gt; &lt;/a&gt;是的变种&lt;a href=&quot;generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans&quot;&gt; &lt;code&gt;KMeans&lt;/code&gt; &lt;/a&gt;算法，它采用迷你批次减少计算时间，同时仍试图优化相同的目标函数。迷你批处理是输入数据的子集，在每次训练迭代中均会随机采样。这些迷你批处理极大地减少了收敛到本地解决方案所需的计算量。与其他减少k均值收敛时间的算法相比，小批量k均值产生的结果通常仅比标准算法稍差。</target>
        </trans-unit>
        <trans-unit id="381def8c4d001638003d40e7acf9264b0a49ea0f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;ColumnTransformer&lt;/code&gt;&lt;/a&gt; helps performing different transformations for different columns of the data, within a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; that is safe from data leakage and that can be parametrized. &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;ColumnTransformer&lt;/code&gt;&lt;/a&gt; works on arrays, sparse matrices, and &lt;a href=&quot;http://pandas.pydata.org/pandas-docs/stable/&quot;&gt;pandas DataFrames&lt;/a&gt;.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;ColumnTransformer&lt;/code&gt; &lt;/a&gt;有助于对数据的不同列进行不同的变换，一个内&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt;是从数据泄漏的安全和可参数化。&lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;ColumnTransformer&lt;/code&gt; &lt;/a&gt;可处理数组，稀疏矩阵和&lt;a href=&quot;http://pandas.pydata.org/pandas-docs/stable/&quot;&gt;pandas DataFrames&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="37d727244bb97826f98eb0365b95bf6cd4afb239" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;compose.ColumnTransformer&lt;/code&gt;&lt;/a&gt; class is experimental and the API is subject to change.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;compose.ColumnTransformer&lt;/code&gt; &lt;/a&gt;类是实验和API可随时更改。</target>
        </trans-unit>
        <trans-unit id="83e5137b54932bec66ccc36542bace6834598b69" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.covariance.graphicallasso#sklearn.covariance.GraphicalLasso&quot;&gt;&lt;code&gt;GraphicalLasso&lt;/code&gt;&lt;/a&gt; estimator uses an l1 penalty to enforce sparsity on the precision matrix: the higher its &lt;code&gt;alpha&lt;/code&gt; parameter, the more sparse the precision matrix. The corresponding &lt;a href=&quot;generated/sklearn.covariance.graphicallassocv#sklearn.covariance.GraphicalLassoCV&quot;&gt;&lt;code&gt;GraphicalLassoCV&lt;/code&gt;&lt;/a&gt; object uses cross-validation to automatically set the &lt;code&gt;alpha&lt;/code&gt; parameter.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.covariance.graphicallasso#sklearn.covariance.GraphicalLasso&quot;&gt; &lt;code&gt;GraphicalLasso&lt;/code&gt; &lt;/a&gt;估计器使用L1惩罚执行关于精度矩阵的稀疏性：越高其 &lt;code&gt;alpha&lt;/code&gt; 参数，越稀疏的精度矩阵。相应的&lt;a href=&quot;generated/sklearn.covariance.graphicallassocv#sklearn.covariance.GraphicalLassoCV&quot;&gt; &lt;code&gt;GraphicalLassoCV&lt;/code&gt; &lt;/a&gt;对象使用交叉验证来自动设置 &lt;code&gt;alpha&lt;/code&gt; 参数。</target>
        </trans-unit>
        <trans-unit id="9447390bf3cfd368da76e6282f132428b32dfff8" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; object also provides a probabilistic interpretation of the PCA that can give a likelihood of data based on the amount of variance it explains. As such it implements a &lt;code&gt;score&lt;/code&gt; method that can be used in cross-validation:</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;对象还提供了可以得到基于方差它解释量的数据的似然性的PCA的概率解释。这样，它实现了一种可用于交叉验证的 &lt;code&gt;score&lt;/code&gt; 方法：</target>
        </trans-unit>
        <trans-unit id="de1125bcd2177e15b5b35e281621b5bbf18681e1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; object is very useful, but has certain limitations for large datasets. The biggest limitation is that &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; only supports batch processing, which means all of the data to be processed must fit in main memory. The &lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt;&lt;code&gt;IncrementalPCA&lt;/code&gt;&lt;/a&gt; object uses a different form of processing and allows for partial computations which almost exactly match the results of &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; while processing the data in a minibatch fashion. &lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt;&lt;code&gt;IncrementalPCA&lt;/code&gt;&lt;/a&gt; makes it possible to implement out-of-core Principal Component Analysis either by:</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;对象是非常有用的，但对大数据集的某些限制。最大的限制是&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;仅支持批处理，这意味着所有要处理的数据必须适合主存储器。所述&lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt; &lt;code&gt;IncrementalPCA&lt;/code&gt; &lt;/a&gt;对象使用不同形式的处理和允许部分计算其结果几乎完全匹配&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;而在minibatch方式处理数据。&lt;a href=&quot;generated/sklearn.decomposition.incrementalpca#sklearn.decomposition.IncrementalPCA&quot;&gt; &lt;code&gt;IncrementalPCA&lt;/code&gt; &lt;/a&gt;可以实施核心外主成分分析：</target>
        </trans-unit>
        <trans-unit id="ecd6b33d3bd2199aadcf263cff0e5246cde4bd39" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.decomposition.sparsecoder#sklearn.decomposition.SparseCoder&quot;&gt;&lt;code&gt;SparseCoder&lt;/code&gt;&lt;/a&gt; object is an estimator that can be used to transform signals into sparse linear combination of atoms from a fixed, precomputed dictionary such as a discrete wavelet basis. This object therefore does not implement a &lt;code&gt;fit&lt;/code&gt; method. The transformation amounts to a sparse coding problem: finding a representation of the data as a linear combination of as few dictionary atoms as possible. All variations of dictionary learning implement the following transform methods, controllable via the &lt;code&gt;transform_method&lt;/code&gt; initialization parameter:</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.decomposition.sparsecoder#sklearn.decomposition.SparseCoder&quot;&gt; &lt;code&gt;SparseCoder&lt;/code&gt; &lt;/a&gt;对象是可用于将信号转换成从一个固定原子的稀疏线性组合的估计，预先计算字典诸如离散波基。因此，该对象未实现 &lt;code&gt;fit&lt;/code&gt; 方法。这种转换带来了一个稀疏的编码问题：找到尽可能多的字典原子的线性组合的数据表示形式。字典学习的所有变体都实现了以下变换方法，这些变换方法可以通过 &lt;code&gt;transform_method&lt;/code&gt; 初始化参数来控制：</target>
        </trans-unit>
        <trans-unit id="51b760d25143490b212d3dce7b63a475beffe57d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.feature_extraction.image.extract_patches_2d#sklearn.feature_extraction.image.extract_patches_2d&quot;&gt;&lt;code&gt;extract_patches_2d&lt;/code&gt;&lt;/a&gt; function extracts patches from an image stored as a two-dimensional array, or three-dimensional with color information along the third axis. For rebuilding an image from all its patches, use &lt;a href=&quot;generated/sklearn.feature_extraction.image.reconstruct_from_patches_2d#sklearn.feature_extraction.image.reconstruct_from_patches_2d&quot;&gt;&lt;code&gt;reconstruct_from_patches_2d&lt;/code&gt;&lt;/a&gt;. For example let use generate a 4x4 pixel picture with 3 color channels (e.g. in RGB format):</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.feature_extraction.image.extract_patches_2d#sklearn.feature_extraction.image.extract_patches_2d&quot;&gt; &lt;code&gt;extract_patches_2d&lt;/code&gt; &lt;/a&gt;从图像提取功能的补丁存储为二维阵列，或三维的，沿所述第三轴线的颜色信息。要从所有修补程序重建映像，请使用&lt;a href=&quot;generated/sklearn.feature_extraction.image.reconstruct_from_patches_2d#sklearn.feature_extraction.image.reconstruct_from_patches_2d&quot;&gt; &lt;code&gt;reconstruct_from_patches_2d&lt;/code&gt; &lt;/a&gt;。例如，让我们使用生成具有3个颜色通道（例如RGB格式）的4x4像素图片：</target>
        </trans-unit>
        <trans-unit id="13de12da96c62cdbe967814b1ded04e8c85eef13" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.feature_extraction.image.patchextractor#sklearn.feature_extraction.image.PatchExtractor&quot;&gt;&lt;code&gt;PatchExtractor&lt;/code&gt;&lt;/a&gt; class works in the same way as &lt;a href=&quot;generated/sklearn.feature_extraction.image.extract_patches_2d#sklearn.feature_extraction.image.extract_patches_2d&quot;&gt;&lt;code&gt;extract_patches_2d&lt;/code&gt;&lt;/a&gt;, only it supports multiple images as input. It is implemented as an estimator, so it can be used in pipelines. See:</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.feature_extraction.image.patchextractor#sklearn.feature_extraction.image.PatchExtractor&quot;&gt; &lt;code&gt;PatchExtractor&lt;/code&gt; &lt;/a&gt;以同样的方式作为类作品&lt;a href=&quot;generated/sklearn.feature_extraction.image.extract_patches_2d#sklearn.feature_extraction.image.extract_patches_2d&quot;&gt; &lt;code&gt;extract_patches_2d&lt;/code&gt; &lt;/a&gt;，只有它支持多种图像作为输入。它作为估计器实现，因此可以在管道中使用。看到：</target>
        </trans-unit>
        <trans-unit id="5f1d9b11617dc21530d4a8e947334af50d14c144" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt;&lt;code&gt;HashingVectorizer&lt;/code&gt;&lt;/a&gt; also comes with the following limitations:</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.feature_extraction.text.hashingvectorizer#sklearn.feature_extraction.text.HashingVectorizer&quot;&gt; &lt;code&gt;HashingVectorizer&lt;/code&gt; &lt;/a&gt;还带有以下限制：</target>
        </trans-unit>
        <trans-unit id="dec1e79879a530cf5d8d2ea5bf189d8118bb1961" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessclassifier#sklearn.gaussian_process.GaussianProcessClassifier&quot;&gt;&lt;code&gt;GaussianProcessClassifier&lt;/code&gt;&lt;/a&gt; implements Gaussian processes (GP) for classification purposes, more specifically for probabilistic classification, where test predictions take the form of class probabilities. GaussianProcessClassifier places a GP prior on a latent function \(f\), which is then squashed through a link function to obtain the probabilistic classification. The latent function \(f\) is a so-called nuisance function, whose values are not observed and are not relevant by themselves. Its purpose is to allow a convenient formulation of the model, and \(f\) is removed (integrated out) during prediction. GaussianProcessClassifier implements the logistic link function, for which the integral cannot be computed analytically but is easily approximated in the binary case.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessclassifier#sklearn.gaussian_process.GaussianProcessClassifier&quot;&gt; &lt;code&gt;GaussianProcessClassifier&lt;/code&gt; &lt;/a&gt;器具高斯过程（GP）用于分类目的，更具体地用于概率分类，其中测试的预测采取类概率的形式。 GaussianProcessClassifier将GP放在潜在函数\（f \）上，然后通过链接函数对其进行压缩以获得概率分类。潜函数\（f \）是一个所谓的讨厌的函数，其值不会被观察到并且与它们自身无关。其目的是允许方便地建立模型，并且在预测期间将\（f \）删除（积分）。 GaussianProcessClassifier实现了逻辑链接函数，该函数无法解析地计算积分，但在二进制情况下很容易近似。</target>
        </trans-unit>
        <trans-unit id="2a70b80f4163a2c6bfd08f3c8b84b458a9627cc7" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessregressor#sklearn.gaussian_process.GaussianProcessRegressor&quot;&gt;&lt;code&gt;GaussianProcessRegressor&lt;/code&gt;&lt;/a&gt; implements Gaussian processes (GP) for regression purposes. For this, the prior of the GP needs to be specified. The prior mean is assumed to be constant and zero (for &lt;code&gt;normalize_y=False&lt;/code&gt;) or the training data&amp;rsquo;s mean (for &lt;code&gt;normalize_y=True&lt;/code&gt;). The prior&amp;rsquo;s covariance is specified by a passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.gaussian_process.gaussianprocessregressor#sklearn.gaussian_process.GaussianProcessRegressor&quot;&gt; &lt;code&gt;GaussianProcessRegressor&lt;/code&gt; &lt;/a&gt;工具高斯过程（GP）回归的目的。为此，需要指定GP的优先级。假定先前的均值是常数且为零（对于 &lt;code&gt;normalize_y=False&lt;/code&gt; ）或训练数据的均值（对于 &lt;code&gt;normalize_y=True&lt;/code&gt; ）。先验的协方差由传递的&lt;a href=&quot;#gp-kernels&quot;&gt;内核&lt;/a&gt;对象指定。通过基于传递的 &lt;code&gt;optimizer&lt;/code&gt; 最大化对数边际似然性（LML）来优化GaussianProcessRegressor期间，优化内核的超参数。由于LML可能具有多个局部最优值，因此可以通过指定 &lt;code&gt;n_restarts_optimizer&lt;/code&gt; 重复启动优化器。。始终从内核的初始超参数值开始进行第一次运行；随后的运行是从已从允许值范围内随机选择的超参数值进行的。如果初始超参数应保持固定，则不能将 &lt;code&gt;None&lt;/code&gt; 参数作为优化器传递。</target>
        </trans-unit>
        <trans-unit id="a013933edad2184a36ef1d15fcbf21a794c0c7f5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.constantkernel#sklearn.gaussian_process.kernels.ConstantKernel&quot;&gt;&lt;code&gt;ConstantKernel&lt;/code&gt;&lt;/a&gt; kernel can be used as part of a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt;&lt;code&gt;Product&lt;/code&gt;&lt;/a&gt; kernel where it scales the magnitude of the other factor (kernel) or as part of a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt;&lt;code&gt;Sum&lt;/code&gt;&lt;/a&gt; kernel, where it modifies the mean of the Gaussian process. It depends on a parameter \(constant\_value\). It is defined as:</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.constantkernel#sklearn.gaussian_process.kernels.ConstantKernel&quot;&gt; &lt;code&gt;ConstantKernel&lt;/code&gt; &lt;/a&gt;内核可以作为一个组成部分&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.product#sklearn.gaussian_process.kernels.Product&quot;&gt; &lt;code&gt;Product&lt;/code&gt; &lt;/a&gt;，其中其缩放因子的其他（内核）的大小或作为一部分内核&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.sum#sklearn.gaussian_process.kernels.Sum&quot;&gt; &lt;code&gt;Sum&lt;/code&gt; &lt;/a&gt;内核，在那里它修改均值高斯过程。它取决于参数\（constant \ _value \）。它定义为：</target>
        </trans-unit>
        <trans-unit id="a1a78e3b1d5985ce1973c8989ff0075d7d078b79" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt; kernel is commonly combined with exponentiation. An example with exponent 2 is shown in the following figure:</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt; &lt;code&gt;DotProduct&lt;/code&gt; &lt;/a&gt;内核通常与幂组合。下图显示了一个指数为2的示例：</target>
        </trans-unit>
        <trans-unit id="299194d50f816028e01666a91e5aaf031d88d5c0" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt; kernel is non-stationary and can be obtained from linear regression by putting \(N(0, 1)\) priors on the coefficients of \(x_d (d = 1, . . . , D)\) and a prior of \(N(0, \sigma_0^2)\) on the bias. The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt;&lt;code&gt;DotProduct&lt;/code&gt;&lt;/a&gt; kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter \(\sigma_0^2\). For \(\sigma_0^2 = 0\), the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt; &lt;code&gt;DotProduct&lt;/code&gt; &lt;/a&gt;内核是非固定的，并且可以从线性回归通过将获得\（N（0,1）\）上的系数先验\（x_d（d = 1，。。。，d）\）和现有（\（N（0，\ sigma_0 ^ 2）\）的偏差。该&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.dotproduct#sklearn.gaussian_process.kernels.DotProduct&quot;&gt; &lt;code&gt;DotProduct&lt;/code&gt; &lt;/a&gt;内核是不变的关于原点的坐标，而不是翻译的旋转。它由参数\（\ sigma_0 ^ 2 \）参数化。对于\（\ sigma_0 ^ 2 = 0 \），该核称为齐次线性核，否则为不均一的。内核由</target>
        </trans-unit>
        <trans-unit id="9b254fd92e2584b8ca8c7f30ca756b0bac24ec2b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.expsinesquared#sklearn.gaussian_process.kernels.ExpSineSquared&quot;&gt;&lt;code&gt;ExpSineSquared&lt;/code&gt;&lt;/a&gt; kernel allows modeling periodic functions. It is parameterized by a length-scale parameter \(l&amp;gt;0\) and a periodicity parameter \(p&amp;gt;0\). Only the isotropic variant where \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.expsinesquared#sklearn.gaussian_process.kernels.ExpSineSquared&quot;&gt; &lt;code&gt;ExpSineSquared&lt;/code&gt; &lt;/a&gt;内核允许造型周期函数。它由长度比例参数\（l&amp;gt; 0 \）和周期性参数\（p&amp;gt; 0 \）进行参数化。目前仅支持\（l \）是标量的各向同性变体。内核由：</target>
        </trans-unit>
        <trans-unit id="4fa9c3925ee31fe17ddb7d4f95d9aa563f446e7b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.matern#sklearn.gaussian_process.kernels.Matern&quot;&gt;&lt;code&gt;Matern&lt;/code&gt;&lt;/a&gt; kernel is a stationary kernel and a generalization of the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; kernel. It has an additional parameter \(\nu\) which controls the smoothness of the resulting function. It is parameterized by a length-scale parameter \(l&amp;gt;0\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs \(x\) (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.matern#sklearn.gaussian_process.kernels.Matern&quot;&gt; &lt;code&gt;Matern&lt;/code&gt; &lt;/a&gt;内核是一个固定的内核和一个泛化&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; &lt;/a&gt;内核。它具有一个附加参数\（\ nu \），该参数控制所得函数的平滑度。它由长度比例参数\（l&amp;gt; 0 \）进行参数化，该参数可以是标量（内核的各向同性变体），也可以是具有与输入\（x \）相同维数的向量（各向异性变体的内核）。内核由：</target>
        </trans-unit>
        <trans-unit id="9cc391d0a3f24c35e3e834b704611ffc08dfc23c" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt;&lt;code&gt;RationalQuadratic&lt;/code&gt;&lt;/a&gt; kernel can be seen as a scale mixture (an infinite sum) of &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; kernels with different characteristic length-scales. It is parameterized by a length-scale parameter \(l&amp;gt;0\) and a scale mixture parameter \(\alpha&amp;gt;0\) Only the isotropic variant where \(l\) is a scalar is supported at the moment. The kernel is given by:</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rationalquadratic#sklearn.gaussian_process.kernels.RationalQuadratic&quot;&gt; &lt;code&gt;RationalQuadratic&lt;/code&gt; &lt;/a&gt;内核可以被看作是一个比例混合物（无限总和）&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; &lt;/a&gt;具有不同特征长度尺度的内核。它由长度比例参数\（l&amp;gt; 0 \）和比例混合参数\（\ alpha&amp;gt; 0 \）参数化。目前仅支持\（l \）为标量的各向同性变量。内核由：</target>
        </trans-unit>
        <trans-unit id="a0d4e8e5df8534d7f797dec945fa5951797b46d9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt;&lt;code&gt;RBF&lt;/code&gt;&lt;/a&gt; kernel is a stationary kernel. It is also known as the &amp;ldquo;squared exponential&amp;rdquo; kernel. It is parameterized by a length-scale parameter \(l&amp;gt;0\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs \(x\) (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.rbf#sklearn.gaussian_process.kernels.RBF&quot;&gt; &lt;code&gt;RBF&lt;/code&gt; &lt;/a&gt;内核是一个固定的内核。它也被称为&amp;ldquo;平方指数&amp;rdquo;内核。它由长度比例参数\（l&amp;gt; 0 \）进行参数化，该参数可以是标量（内核的各向同性变体），也可以是具有与输入\（x \）相同维数的向量（各向异性变体的内核）。内核由：</target>
        </trans-unit>
        <trans-unit id="8f4b1aa7c1e397df865fdc8d1fd65546b5eaaf2f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt;&lt;code&gt;MissingIndicator&lt;/code&gt;&lt;/a&gt; transformer is useful to transform a dataset into corresponding binary matrix indicating the presence of missing values in the dataset. This transformation is useful in conjunction with imputation. When using imputation, preserving the information about which values had been missing can be informative.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.impute.missingindicator#sklearn.impute.MissingIndicator&quot;&gt; &lt;code&gt;MissingIndicator&lt;/code&gt; &lt;/a&gt;变压器是有用的变换数据集成相应指示缺失值的数据集中的存在二进制矩阵。此转换与插补结合使用非常有用。使用插补时，保留有关缺少哪些值的信息可能会很有帮助。</target>
        </trans-unit>
        <trans-unit id="c0c9736c8ad276e3deabee46eb181026e0204e8e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;SimpleImputer&lt;/code&gt;&lt;/a&gt; class also supports categorical data represented as string values or pandas categoricals when using the &lt;code&gt;'most_frequent'&lt;/code&gt; or &lt;code&gt;'constant'&lt;/code&gt; strategy:</source>
          <target state="translated">当使用 &lt;code&gt;'most_frequent'&lt;/code&gt; 或 &lt;code&gt;'constant'&lt;/code&gt; 策略时，&lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;SimpleImputer&lt;/code&gt; &lt;/a&gt;类还支持表示为字符串值或熊猫分类的分类数据：</target>
        </trans-unit>
        <trans-unit id="4d17103c250c5ab6ac126fd9a857f81de53fed6f" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;SimpleImputer&lt;/code&gt;&lt;/a&gt; class also supports sparse matrices:</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;SimpleImputer&lt;/code&gt; &lt;/a&gt;类还支持稀疏矩阵：</target>
        </trans-unit>
        <trans-unit id="618df5d6360d655fcf582933900e1cb3bcf02379" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt;&lt;code&gt;SimpleImputer&lt;/code&gt;&lt;/a&gt; class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.impute.simpleimputer#sklearn.impute.SimpleImputer&quot;&gt; &lt;code&gt;SimpleImputer&lt;/code&gt; &lt;/a&gt;类提供基本策略用于输入缺失值。可以使用提供的恒定值或使用缺失值所在各列的统计信息（平均值，中位数或最频繁）来估算缺失值。此类还允许使用不同的缺失值编码。</target>
        </trans-unit>
        <trans-unit id="6f59968771d1dbbd03a744853045d3c0b7aa414b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; constructs an approximate mapping for the radial basis function kernel, also known as &lt;em&gt;Random Kitchen Sinks&lt;/em&gt;&lt;a href=&quot;#rr2007&quot; id=&quot;id2&quot;&gt;[RR2007]&lt;/a&gt;. This transformation can be used to explicitly model a kernel map, prior to applying a linear algorithm, for example a linear SVM:</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;构造为径向基函数内核的近似映射，也被称为&lt;em&gt;随机厨房水槽&lt;/em&gt;&lt;a href=&quot;#rr2007&quot; id=&quot;id2&quot;&gt;[RR2007] &lt;/a&gt;。在应用线性算法（例如线性SVM）之前，可以使用此转换对内核映射进行显式建模：</target>
        </trans-unit>
        <trans-unit id="44948166b7a6695399209dd8e1e1f1af0b0058e5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt;&lt;code&gt;HuberRegressor&lt;/code&gt;&lt;/a&gt; differs from using &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; with loss set to &lt;code&gt;huber&lt;/code&gt; in the following ways.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt; &lt;code&gt;HuberRegressor&lt;/code&gt; &lt;/a&gt;不同于使用&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;与损耗设定为 &lt;code&gt;huber&lt;/code&gt; 在以下几个方面。</target>
        </trans-unit>
        <trans-unit id="7ab9e90f2b3f808e98761c90cab46994be6820bb" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt;&lt;code&gt;HuberRegressor&lt;/code&gt;&lt;/a&gt; is different to &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt; because it applies a linear loss to samples that are classified as outliers. A sample is classified as an inlier if the absolute error of that sample is lesser than a certain threshold. It differs from &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.ransacregressor#sklearn.linear_model.RANSACRegressor&quot;&gt;&lt;code&gt;RANSACRegressor&lt;/code&gt;&lt;/a&gt; because it does not ignore the effect of the outliers but gives a lesser weight to them.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt; &lt;code&gt;HuberRegressor&lt;/code&gt; &lt;/a&gt;是不同&lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt;，因为它适用于被分类为异常值样品的线性损失。如果样本的绝对误差小于某个阈值，则将其分类为内部样本。它与&lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.linear_model.ransacregressor#sklearn.linear_model.RANSACRegressor&quot;&gt; &lt;code&gt;RANSACRegressor&lt;/code&gt; &lt;/a&gt;不同，因为它不会忽略异常值的影响，但会给它们带来较小的权重。</target>
        </trans-unit>
        <trans-unit id="70ef4a25a40856b26fd987533d68c36540345c20" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt; is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer parameter values, effectively reducing the number of variables upon which the given solution is dependent. For this reason, the Lasso and its variants are fundamental to the field of compressed sensing. Under certain conditions, it can recover the exact set of non-zero weights (see &lt;a href=&quot;../auto_examples/applications/plot_tomography_l1_reconstruction#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py&quot;&gt;Compressive sensing: tomography reconstruction with L1 prior (Lasso)&lt;/a&gt;).</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt;的是，估计稀疏系数线性模型。它在某些情况下很有用，因为它倾向于使用具有较少参数值的解决方案，从而有效地减少了给定解决方案所依赖的变量数量。因此，套索及其变体对于压缩感测领域至关重要。在某些条件下，它可以恢复非零权重的确切集合（请参阅&lt;a href=&quot;../auto_examples/applications/plot_tomography_l1_reconstruction#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py&quot;&gt;压缩感测：使用L1先验（Lasso）进行层析成像重建&lt;/a&gt;）。</target>
        </trans-unit>
        <trans-unit id="8c1095adf7bd87312f73373efdee9c54e778b1be" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt;&lt;code&gt;MultiTaskElasticNet&lt;/code&gt;&lt;/a&gt; is an elastic-net model that estimates sparse coefficients for multiple regression problems jointly: &lt;code&gt;Y&lt;/code&gt; is a 2D array, of shape &lt;code&gt;(n_samples, n_tasks)&lt;/code&gt;. The constraint is that the selected features are the same for all the regression problems, also called tasks.</source>
          <target state="translated">的&lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt; &lt;code&gt;MultiTaskElasticNet&lt;/code&gt; &lt;/a&gt;是估计用于多重回归问题共同稀疏系数的弹性网模型： &lt;code&gt;Y&lt;/code&gt; 是2D阵列，形状的 &lt;code&gt;(n_samples, n_tasks)&lt;/code&gt; 。约束条件是所有回归问题（也称为任务）的选定特征都相同。</target>
        </trans-unit>
        <trans-unit id="0855f24dbbabd45aa8775e800911f6fb3f3411c3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt;&lt;code&gt;MultiTaskLasso&lt;/code&gt;&lt;/a&gt; is a linear model that estimates sparse coefficients for multiple regression problems jointly: &lt;code&gt;y&lt;/code&gt; is a 2D array, of shape &lt;code&gt;(n_samples, n_tasks)&lt;/code&gt;. The constraint is that the selected features are the same for all the regression problems, also called tasks.</source>
          <target state="translated">的&lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt; &lt;code&gt;MultiTaskLasso&lt;/code&gt; &lt;/a&gt;是估计用于多重回归问题共同稀疏系数的线性模型： &lt;code&gt;y&lt;/code&gt; 是2D阵列，形状的 &lt;code&gt;(n_samples, n_tasks)&lt;/code&gt; 。约束条件是所有回归问题（也称为任务）的选定特征都相同。</target>
        </trans-unit>
        <trans-unit id="d927ce91bac1b6df649580c487bdf34ce5f21e13" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.perceptron#sklearn.linear_model.Perceptron&quot;&gt;&lt;code&gt;Perceptron&lt;/code&gt;&lt;/a&gt; is another simple classification algorithm suitable for large scale learning. By default:</source>
          <target state="translated">的&lt;a href=&quot;generated/sklearn.linear_model.perceptron#sklearn.linear_model.Perceptron&quot;&gt; &lt;code&gt;Perceptron&lt;/code&gt; &lt;/a&gt;是适合大规模学习另一种简单的分类算法。默认：</target>
        </trans-unit>
        <trans-unit id="dc6941408ce5829c04ee30dacb5eec313120d42e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; estimator uses a generalization of the median in multiple dimensions. It is thus robust to multivariate outliers. Note however that the robustness of the estimator decreases quickly with the dimensionality of the problem. It looses its robustness properties and becomes no better than an ordinary least squares in high dimension.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; &lt;/a&gt;估计器使用在多个维度中位数的推广。因此，它对于多元离群值是鲁棒的。但是请注意，估计器的鲁棒性随问题的维度而迅速降低。它失去了其鲁棒性，并且在高尺寸方面变得不比普通的最小二乘更好。</target>
        </trans-unit>
        <trans-unit id="497dd0db8e84137ac4b140cff3317a3423c09e22" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt;&lt;code&gt;accuracy_score&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Accuracy_and_precision&quot;&gt;accuracy&lt;/a&gt;, either the fraction (default) or the count (normalize=False) of correct predictions.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score&quot;&gt; &lt;code&gt;accuracy_score&lt;/code&gt; &lt;/a&gt;函数计算&lt;a href=&quot;https://en.wikipedia.org/wiki/Accuracy_and_precision&quot;&gt;精度&lt;/a&gt;，无论是分数（默认）或正确预测的计数（正规化=假）。</target>
        </trans-unit>
        <trans-unit id="734b9a83298cb0e5bb404a0eb951bb89a38c30c1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;http://en.wikipedia.org/w/index.php?title=Information_retrieval&amp;amp;oldid=793358396#Average_precision&quot;&gt;average precision&lt;/a&gt; (AP) from prediction scores. The value is between 0 and 1 and higher is better. AP is defined as</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt; &lt;code&gt;average_precision_score&lt;/code&gt; &lt;/a&gt;函数计算&lt;a href=&quot;http://en.wikipedia.org/w/index.php?title=Information_retrieval&amp;amp;oldid=793358396#Average_precision&quot;&gt;平均精度&lt;/a&gt;从预测分数（AP）。该值在0到1之间，越高越好。AP定义为</target>
        </trans-unit>
        <trans-unit id="4c1e547613cb8b25282a0a1d2e2d0fa8b86fab4a" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.balanced_accuracy_score#sklearn.metrics.balanced_accuracy_score&quot;&gt;&lt;code&gt;balanced_accuracy_score&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Accuracy_and_precision&quot;&gt;balanced accuracy&lt;/a&gt;, which avoids inflated performance estimates on imbalanced datasets. It is the macro-average of recall scores per class or, equivalently, raw accuracy where each sample is weighted according to the inverse prevalence of its true class. Thus for balanced datasets, the score is equal to accuracy.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.metrics.balanced_accuracy_score#sklearn.metrics.balanced_accuracy_score&quot;&gt; &lt;code&gt;balanced_accuracy_score&lt;/code&gt; &lt;/a&gt;函数计算的&lt;a href=&quot;https://en.wikipedia.org/wiki/Accuracy_and_precision&quot;&gt;平衡精度&lt;/a&gt;，从而避免了不平衡数据集膨胀性能估计。它是每个类别的召回得分的宏观平均值，或等效地是原始准确性，其中，每个样本均根据其真实类别的反普遍性进行加权。因此，对于平衡的数据集，分数等于准确性。</target>
        </trans-unit>
        <trans-unit id="6c29a08df4ccee7316d3d3b84f8a1be99122d005" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt;&lt;code&gt;brier_score_loss&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Brier_score&quot;&gt;Brier score&lt;/a&gt; for binary classes. Quoting Wikipedia:</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt; &lt;code&gt;brier_score_loss&lt;/code&gt; &lt;/a&gt;函数计算&lt;a href=&quot;https://en.wikipedia.org/wiki/Brier_score&quot;&gt;布来得分&lt;/a&gt;为二进制类。引用维基百科：</target>
        </trans-unit>
        <trans-unit id="80ef899573ebb3be6112621f7df5aadf4a0d99d9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.classification_report#sklearn.metrics.classification_report&quot;&gt;&lt;code&gt;classification_report&lt;/code&gt;&lt;/a&gt; function builds a text report showing the main classification metrics. Here is a small example with custom &lt;code&gt;target_names&lt;/code&gt; and inferred labels:</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.metrics.classification_report#sklearn.metrics.classification_report&quot;&gt; &lt;code&gt;classification_report&lt;/code&gt; &lt;/a&gt;功能构建出的主要分类指标文本报告。这是一个带有自定义 &lt;code&gt;target_names&lt;/code&gt; 和推断标签的小示例：</target>
        </trans-unit>
        <trans-unit id="646495d784f725b3203da7b1895753c47a46c957" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.confusion_matrix#sklearn.metrics.confusion_matrix&quot;&gt;&lt;code&gt;confusion_matrix&lt;/code&gt;&lt;/a&gt; function evaluates classification accuracy by computing the confusion matrix with each row corresponding to the true class &amp;lt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Confusion_matrix&quot;&gt;https://en.wikipedia.org/wiki/Confusion_matrix&lt;/a&gt;&amp;gt;`_. (Wikipedia and other references may use different convention for axes.)</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.metrics.confusion_matrix#sklearn.metrics.confusion_matrix&quot;&gt; &lt;code&gt;confusion_matrix&lt;/code&gt; &lt;/a&gt;功能通过计算与对应于所述真实的类中的每个行&amp;lt;混淆矩阵的计算结果的分类精度&lt;a href=&quot;https://en.wikipedia.org/wiki/Confusion_matrix&quot;&gt;https://en.wikipedia.org/wiki/Confusion_matrix&lt;/a&gt; &amp;gt;`_。（维基百科和其他参考文献可能对轴使用不同的约定。）</target>
        </trans-unit>
        <trans-unit id="627c762ce2611d603b6cf9dd93706bacfe9a64ab" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.coverage_error#sklearn.metrics.coverage_error&quot;&gt;&lt;code&gt;coverage_error&lt;/code&gt;&lt;/a&gt; function computes the average number of labels that have to be included in the final prediction such that all true labels are predicted. This is useful if you want to know how many top-scored-labels you have to predict in average without missing any true one. The best value of this metrics is thus the average number of true labels.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.metrics.coverage_error#sklearn.metrics.coverage_error&quot;&gt; &lt;code&gt;coverage_error&lt;/code&gt; &lt;/a&gt;函数计算必须被包括在最终的预测，使得所有真标签预测标签的平均数目。如果您想知道平均要预测多少个得分最高的标签而不丢失任何真实的标签，这将很有用。因此，此度量的最佳值是真实标签的平均数量。</target>
        </trans-unit>
        <trans-unit id="365c5eb64f0dc2e33be205a551210f568e81546d" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Explained_variation&quot;&gt;explained variance regression score&lt;/a&gt;.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; &lt;/a&gt;计算&lt;a href=&quot;https://en.wikipedia.org/wiki/Explained_variation&quot;&gt;解释方差回归得分&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="16accfb21d784810c328541c85b1894b818cde8e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.hamming_loss#sklearn.metrics.hamming_loss&quot;&gt;&lt;code&gt;hamming_loss&lt;/code&gt;&lt;/a&gt; computes the average Hamming loss or &lt;a href=&quot;https://en.wikipedia.org/wiki/Hamming_distance&quot;&gt;Hamming distance&lt;/a&gt; between two sets of samples.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.metrics.hamming_loss#sklearn.metrics.hamming_loss&quot;&gt; &lt;code&gt;hamming_loss&lt;/code&gt; &lt;/a&gt;计算平均汉明损失或&lt;a href=&quot;https://en.wikipedia.org/wiki/Hamming_distance&quot;&gt;汉明距离&lt;/a&gt;两组样品之间。</target>
        </trans-unit>
        <trans-unit id="6d1238c9791f472ba1850e50c0898d87bebfa2d3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt;&lt;code&gt;hinge_loss&lt;/code&gt;&lt;/a&gt; function computes the average distance between the model and the data using &lt;a href=&quot;https://en.wikipedia.org/wiki/Hinge_loss&quot;&gt;hinge loss&lt;/a&gt;, a one-sided metric that considers only prediction errors. (Hinge loss is used in maximal margin classifiers such as support vector machines.)</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.metrics.hinge_loss#sklearn.metrics.hinge_loss&quot;&gt; &lt;code&gt;hinge_loss&lt;/code&gt; &lt;/a&gt;函数计算模型，并使用数据之间的平均距离&lt;a href=&quot;https://en.wikipedia.org/wiki/Hinge_loss&quot;&gt;铰链损失&lt;/a&gt;，单面度量仅考虑预测误差。（铰链损耗用于最大余量分类器中，例如支持向量机。）</target>
        </trans-unit>
        <trans-unit id="29930da8eb2c0b1ff7129cc1cbfbb0416883031e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.jaccard_similarity_score#sklearn.metrics.jaccard_similarity_score&quot;&gt;&lt;code&gt;jaccard_similarity_score&lt;/code&gt;&lt;/a&gt; function computes the average (default) or sum of &lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;Jaccard similarity coefficients&lt;/a&gt;, also called the Jaccard index, between pairs of label sets.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.metrics.jaccard_similarity_score#sklearn.metrics.jaccard_similarity_score&quot;&gt; &lt;code&gt;jaccard_similarity_score&lt;/code&gt; &lt;/a&gt;函数计算的平均值（默认）或总和&lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;Jaccard相似系数&lt;/a&gt;，也被称为索引的Jaccard，对标签组之间。</target>
        </trans-unit>
        <trans-unit id="cbf6f35f94b93c090ec2e48a35d245f91dcfca65" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.label_ranking_average_precision_score#sklearn.metrics.label_ranking_average_precision_score&quot;&gt;&lt;code&gt;label_ranking_average_precision_score&lt;/code&gt;&lt;/a&gt; function implements label ranking average precision (LRAP). This metric is linked to the &lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt;&lt;code&gt;average_precision_score&lt;/code&gt;&lt;/a&gt; function, but is based on the notion of label ranking instead of precision and recall.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.metrics.label_ranking_average_precision_score#sklearn.metrics.label_ranking_average_precision_score&quot;&gt; &lt;code&gt;label_ranking_average_precision_score&lt;/code&gt; &lt;/a&gt;功能进行标签的排名平均精度（LRAP）。该指标链接到&lt;a href=&quot;generated/sklearn.metrics.average_precision_score#sklearn.metrics.average_precision_score&quot;&gt; &lt;code&gt;average_precision_score&lt;/code&gt; &lt;/a&gt;函数，但是基于标签排名的概念，而不是精度和召回率。</target>
        </trans-unit>
        <trans-unit id="79620a85c10a9be19922ad33cc7395bed79c9e4e" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.label_ranking_loss#sklearn.metrics.label_ranking_loss&quot;&gt;&lt;code&gt;label_ranking_loss&lt;/code&gt;&lt;/a&gt; function computes the ranking loss which averages over the samples the number of label pairs that are incorrectly ordered, i.e. true labels have a lower score than false labels, weighted by the inverse of the number of ordered pairs of false and true labels. The lowest achievable ranking loss is zero.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.metrics.label_ranking_loss#sklearn.metrics.label_ranking_loss&quot;&gt; &lt;code&gt;label_ranking_loss&lt;/code&gt; &lt;/a&gt;函数计算排名损失，这在样品的平均值被正确排序标签对的数目，即，真实的标签具有比假标签分数较低，由有序对虚实标签的数量的倒数加权。可实现的最低排名损失为零。</target>
        </trans-unit>
        <trans-unit id="4b0810d3cef1ca02b990e21053d774c24a0e430b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.log_loss#sklearn.metrics.log_loss&quot;&gt;&lt;code&gt;log_loss&lt;/code&gt;&lt;/a&gt; function computes log loss given a list of ground-truth labels and a probability matrix, as returned by an estimator&amp;rsquo;s &lt;code&gt;predict_proba&lt;/code&gt; method.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.metrics.log_loss#sklearn.metrics.log_loss&quot;&gt; &lt;code&gt;log_loss&lt;/code&gt; &lt;/a&gt;函数计算日志丢失给地面实况标签和概率矩阵的一个列表，由估计的返回的 &lt;code&gt;predict_proba&lt;/code&gt; 方法。</target>
        </trans-unit>
        <trans-unit id="3661b0b19cd7cbd747b2bf1ce7b4a383102a7abe" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt;&lt;code&gt;matthews_corrcoef&lt;/code&gt;&lt;/a&gt; function computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Matthews_correlation_coefficient&quot;&gt;Matthew&amp;rsquo;s correlation coefficient (MCC)&lt;/a&gt; for binary classes. Quoting Wikipedia:</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.metrics.matthews_corrcoef#sklearn.metrics.matthews_corrcoef&quot;&gt; &lt;code&gt;matthews_corrcoef&lt;/code&gt; &lt;/a&gt;函数计算&lt;a href=&quot;https://en.wikipedia.org/wiki/Matthews_correlation_coefficient&quot;&gt;Matthew的相关系数（MCC）&lt;/a&gt;为二进制类。引用维基百科：</target>
        </trans-unit>
        <trans-unit id="8af6e2db7d7843522a3d6755e0b8d1e9cbd1e8f1" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt;&lt;code&gt;mean_absolute_error&lt;/code&gt;&lt;/a&gt; function computes &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;&gt;mean absolute error&lt;/a&gt;, a risk metric corresponding to the expected value of the absolute error loss or \(l1\)-norm loss.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.metrics.mean_absolute_error#sklearn.metrics.mean_absolute_error&quot;&gt; &lt;code&gt;mean_absolute_error&lt;/code&gt; &lt;/a&gt;函数计算&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_absolute_error&quot;&gt;平均绝对误差&lt;/a&gt;，风险度量相对应的绝对误差损失或\（L1 \）的预期值-范损失。</target>
        </trans-unit>
        <trans-unit id="798074cb4600d0906a9ad4975942309f6067f034" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt;&lt;code&gt;mean_squared_error&lt;/code&gt;&lt;/a&gt; function computes &lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;&gt;mean square error&lt;/a&gt;, a risk metric corresponding to the expected value of the squared (quadratic) error or loss.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.metrics.mean_squared_error#sklearn.metrics.mean_squared_error&quot;&gt; &lt;code&gt;mean_squared_error&lt;/code&gt; &lt;/a&gt;函数计算&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;&gt;均方误差&lt;/a&gt;，风险度量相对应的平方（二次）错误或遗漏的预期值。</target>
        </trans-unit>
        <trans-unit id="e4bbccf6d12a691e935e91e2611be809f1bb4329" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt;&lt;code&gt;mean_squared_log_error&lt;/code&gt;&lt;/a&gt; function computes a risk metric corresponding to the expected value of the squared logarithmic (quadratic) error or loss.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.metrics.mean_squared_log_error#sklearn.metrics.mean_squared_log_error&quot;&gt; &lt;code&gt;mean_squared_log_error&lt;/code&gt; &lt;/a&gt;函数计算风险度量对应于对数平方（二次的）错误或损失的预期值。</target>
        </trans-unit>
        <trans-unit id="39f48c0bbd67ae6ad01f06368c176486b0da9e3b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt;&lt;code&gt;median_absolute_error&lt;/code&gt;&lt;/a&gt; does not support multioutput.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt; &lt;code&gt;median_absolute_error&lt;/code&gt; &lt;/a&gt;并不多输出支持。</target>
        </trans-unit>
        <trans-unit id="4c03eab2aa446025510f65f3a7e796a70c97a820" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt;&lt;code&gt;median_absolute_error&lt;/code&gt;&lt;/a&gt; is particularly interesting because it is robust to outliers. The loss is calculated by taking the median of all absolute differences between the target and the prediction.</source>
          <target state="translated">中&lt;a href=&quot;generated/sklearn.metrics.median_absolute_error#sklearn.metrics.median_absolute_error&quot;&gt; &lt;code&gt;median_absolute_error&lt;/code&gt; &lt;/a&gt;特别有趣，因为它对异常值具有鲁棒性。损失是通过计算目标与预测之间所有绝对差的中值来计算的。</target>
        </trans-unit>
        <trans-unit id="9f5631acff2238ea5bcb79376fef6d2e143756a6" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.precision_recall_curve#sklearn.metrics.precision_recall_curve&quot;&gt;&lt;code&gt;precision_recall_curve&lt;/code&gt;&lt;/a&gt; computes a precision-recall curve from the ground truth label and a score given by the classifier by varying a decision threshold.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.metrics.precision_recall_curve#sklearn.metrics.precision_recall_curve&quot;&gt; &lt;code&gt;precision_recall_curve&lt;/code&gt; &lt;/a&gt;计算从地面实况标签精确召回曲线，通过改变决策阈值由分类给予评分。</target>
        </trans-unit>
        <trans-unit id="d16f1c84f45a895677960253a6c6c45cf8b671e5" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt;&lt;code&gt;explained_variance_score&lt;/code&gt;&lt;/a&gt; accept an additional value &lt;code&gt;'variance_weighted'&lt;/code&gt; for the &lt;code&gt;multioutput&lt;/code&gt; parameter. This option leads to a weighting of each individual score by the variance of the corresponding target variable. This setting quantifies the globally captured unscaled variance. If the target variables are of different scale, then this score puts more importance on well explaining the higher variance variables. &lt;code&gt;multioutput='variance_weighted'&lt;/code&gt; is the default value for &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; for backward compatibility. This will be changed to &lt;code&gt;uniform_average&lt;/code&gt; in the future.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.metrics.explained_variance_score#sklearn.metrics.explained_variance_score&quot;&gt; &lt;code&gt;explained_variance_score&lt;/code&gt; &lt;/a&gt;接受额外的价值 &lt;code&gt;'variance_weighted'&lt;/code&gt; 的 &lt;code&gt;multioutput&lt;/code&gt; 参数。该选项通过相应目标变量的方差对每个单独的分数进行加权。此设置量化了全局捕获的未缩放方差。如果目标变量的规模不同，则该分数在很好地解释较高方差变量方面具有更高的重要性。为了向后兼容，&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt;的默认值为 &lt;code&gt;multioutput='variance_weighted'&lt;/code&gt; 。将来将其更改为 &lt;code&gt;uniform_average&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="093b69e0a2a3ccbb33c0abc5ad459d307bcf6553" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt;&lt;code&gt;r2_score&lt;/code&gt;&lt;/a&gt; function computes R&amp;sup2;, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Coefficient_of_determination&quot;&gt;coefficient of determination&lt;/a&gt;. It provides a measure of how well future samples are likely to be predicted by the model. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.metrics.r2_score#sklearn.metrics.r2_score&quot;&gt; &lt;code&gt;r2_score&lt;/code&gt; &lt;/a&gt;函数计算R 2时，&lt;a href=&quot;https://en.wikipedia.org/wiki/Coefficient_of_determination&quot;&gt;决定系数&lt;/a&gt;。它提供了一种衡量模型可能对未来样本进行预测的方式。最佳可能得分为1.0，并且可能为负（因为该模型可能会更差）。不管输入特征如何，始终预测y的期望值的常数模型将获得0.0的R ^ 2分数。</target>
        </trans-unit>
        <trans-unit id="6cdf15299db25ef1f3b3628cdbe3b0de1b0d0ab3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt;&lt;code&gt;roc_auc_score&lt;/code&gt;&lt;/a&gt; function computes the area under the receiver operating characteristic (ROC) curve, which is also denoted by AUC or AUROC. By computing the area under the roc curve, the curve information is summarized in one number. For more information see the &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve&quot;&gt;Wikipedia article on AUC&lt;/a&gt;.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.metrics.roc_auc_score#sklearn.metrics.roc_auc_score&quot;&gt; &lt;code&gt;roc_auc_score&lt;/code&gt; &lt;/a&gt;函数计算所述接收器操作特性（ROC）曲线，其也由AUC或AUROC表示下的面积。通过计算roc曲线下的面积，可将曲线信息汇总为一个。有关更多信息，请参阅&lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve&quot;&gt;AUC上&lt;/a&gt;的Wikipedia文章。</target>
        </trans-unit>
        <trans-unit id="fbc2259a8dc85fa7ed24780d0f0e2ac678fbcad9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt;&lt;code&gt;zero_one_loss&lt;/code&gt;&lt;/a&gt; function computes the sum or the average of the 0-1 classification loss (\(L_{0-1}\)) over \(n_{\text{samples}}\). By default, the function normalizes over the sample. To get the sum of the \(L_{0-1}\), set &lt;code&gt;normalize&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt;.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.metrics.zero_one_loss#sklearn.metrics.zero_one_loss&quot;&gt; &lt;code&gt;zero_one_loss&lt;/code&gt; &lt;/a&gt;函数计算的和或平均值的0-1分类损失（\（{L_ 0-1} \））超过\（N _ {\文本{样品}} \）的。默认情况下，该函数对样本进行标准化。要获得\（L_ {0-1} \）的总和，请将 &lt;code&gt;normalize&lt;/code&gt; 设置为 &lt;code&gt;False&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0ce78ef531a28f4df4b9620cf7454901e00bf965" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt;&lt;code&gt;BayesianGaussianMixture&lt;/code&gt;&lt;/a&gt; object implements a variant of the Gaussian mixture model with variational inference algorithms. The API is similar as the one defined by &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt;&lt;code&gt;GaussianMixture&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.mixture.bayesiangaussianmixture#sklearn.mixture.BayesianGaussianMixture&quot;&gt; &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; &lt;/a&gt;对象实现与变推理算法的高斯混合模型的变体。该API与&lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt; &lt;code&gt;GaussianMixture&lt;/code&gt; &lt;/a&gt;定义的API类似。</target>
        </trans-unit>
        <trans-unit id="7755b185d3bd9567bc77a31c3d84e8be2c332f90" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt;&lt;code&gt;GaussianMixture&lt;/code&gt;&lt;/a&gt; comes with different options to constrain the covariance of the difference classes estimated: spherical, diagonal, tied or full covariance.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt; &lt;code&gt;GaussianMixture&lt;/code&gt; &lt;/a&gt;带有不同的选项来约束差异类的协方差估计：球形，对角线，捆绑或全协方差。</target>
        </trans-unit>
        <trans-unit id="9cb1ef419e28ff804b54eef9fc18747641996ac9" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt;&lt;code&gt;GaussianMixture&lt;/code&gt;&lt;/a&gt; object implements the &lt;a href=&quot;#expectation-maximization&quot;&gt;expectation-maximization&lt;/a&gt; (EM) algorithm for fitting mixture-of-Gaussian models. It can also draw confidence ellipsoids for multivariate models, and compute the Bayesian Information Criterion to assess the number of clusters in the data. A &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture.fit&quot;&gt;&lt;code&gt;GaussianMixture.fit&lt;/code&gt;&lt;/a&gt; method is provided that learns a Gaussian Mixture Model from train data. Given test data, it can assign to each sample the Gaussian it mostly probably belong to using the &lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture.predict&quot;&gt;&lt;code&gt;GaussianMixture.predict&lt;/code&gt;&lt;/a&gt; method.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture&quot;&gt; &lt;code&gt;GaussianMixture&lt;/code&gt; &lt;/a&gt;对象实现&lt;a href=&quot;#expectation-maximization&quot;&gt;期望最大化&lt;/a&gt;（EM）算法用于装配混合物高斯的的模型。它还可以为多元模型绘制置信椭圆体，并计算贝叶斯信息准则以评估数据中的聚类数量。提供了一种&lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture.fit&quot;&gt; &lt;code&gt;GaussianMixture.fit&lt;/code&gt; &lt;/a&gt;方法，该方法可以从火车数据中学习高斯混合模型。给定测试数据，它可以使用&lt;a href=&quot;generated/sklearn.mixture.gaussianmixture#sklearn.mixture.GaussianMixture.predict&quot;&gt; &lt;code&gt;GaussianMixture.predict&lt;/code&gt; &lt;/a&gt;方法为每个样本分配最可能属于的高斯。</target>
        </trans-unit>
        <trans-unit id="0de55d1a8cabbad74e59064d298db364a4949211" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; instance implements the usual estimator API: when &amp;ldquo;fitting&amp;rdquo; it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt;实例可实现通常的估计API：当&amp;ldquo;嵌合&amp;rdquo;它在一个数据集中的所有参数值的可能组合进行评估和最佳组合被保留。</target>
        </trans-unit>
        <trans-unit id="45a8a9b77f54ed1a1a49e1eebdf058e73c1ad33a" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit&quot;&gt;&lt;code&gt;GroupShuffleSplit&lt;/code&gt;&lt;/a&gt; iterator behaves as a combination of &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt;&lt;code&gt;ShuffleSplit&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt;&lt;code&gt;LeavePGroupsOut&lt;/code&gt;&lt;/a&gt;, and generates a sequence of randomized partitions in which a subset of groups are held out for each split.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit&quot;&gt; &lt;code&gt;GroupShuffleSplit&lt;/code&gt; &lt;/a&gt;迭代器的操作与结合&lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt; &lt;code&gt;ShuffleSplit&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut&quot;&gt; &lt;code&gt;LeavePGroupsOut&lt;/code&gt; &lt;/a&gt;，并且生成其中烷基的子集被伸出为每个分割随机分区的序列。</target>
        </trans-unit>
        <trans-unit id="ec15ef26f9a075815cd5139e68468c23802a4936" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt;&lt;code&gt;ShuffleSplit&lt;/code&gt;&lt;/a&gt; iterator will generate a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit&quot;&gt; &lt;code&gt;ShuffleSplit&lt;/code&gt; &lt;/a&gt;迭代将产生独立列车/测试数据集分割的一个用户定义的编号。首先对样本进行混洗，然后将其分为一对训练和测试集。</target>
        </trans-unit>
        <trans-unit id="9d19931407bdaf26c2091f0ff552c9705b61d84b" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; (LOF) algorithm computes a score (called local outlier factor) reflecting the degree of abnormality of the observations. It measures the local density deviation of a given data point with respect to its neighbors. The idea is to detect the samples that have a substantially lower density than their neighbors.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt;（LOF）算法计算的分数（称为本地异常因子）反映了观察的异常的程度。它测量给定数据点相对于其相邻点的局部密度偏差。这个想法是要检测密度远低于其邻居的样品。</target>
        </trans-unit>
        <trans-unit id="7ee66eef276fb6deecd16d4b6508f3c8417f58ed" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt;&lt;code&gt;NearestCentroid&lt;/code&gt;&lt;/a&gt; classifier has a &lt;code&gt;shrink_threshold&lt;/code&gt; parameter, which implements the nearest shrunken centroid classifier. In effect, the value of each feature for each centroid is divided by the within-class variance of that feature. The feature values are then reduced by &lt;code&gt;shrink_threshold&lt;/code&gt;. Most notably, if a particular feature value crosses zero, it is set to zero. In effect, this removes the feature from affecting the classification. This is useful, for example, for removing noisy features.</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt; &lt;code&gt;NearestCentroid&lt;/code&gt; &lt;/a&gt;分类有 &lt;code&gt;shrink_threshold&lt;/code&gt; 参数，它实现了最近缩小重心分类。实际上，每个质心的每个特征的值除以该特征的类内方差。然后，将特征值减小 &lt;code&gt;shrink_threshold&lt;/code&gt; 。最值得注意的是，如果特定特征值超过零，则将其设置为零。实际上，这消除了影响分类的功能。例如，这对于删除嘈杂的功能很有用。</target>
        </trans-unit>
        <trans-unit id="43771e48f74cd166aa989881024956f81686fd39" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt;&lt;code&gt;NearestCentroid&lt;/code&gt;&lt;/a&gt; classifier is a simple algorithm that represents each class by the centroid of its members. In effect, this makes it similar to the label updating phase of the &lt;code&gt;sklearn.KMeans&lt;/code&gt; algorithm. It also has no parameters to choose, making it a good baseline classifier. It does, however, suffer on non-convex classes, as well as when classes have drastically different variances, as equal variance in all dimensions is assumed. See Linear Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt;&lt;code&gt;sklearn.discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) and Quadratic Discriminant Analysis (&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt;&lt;code&gt;sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt;&lt;/a&gt;) for more complex methods that do not make this assumption. Usage of the default &lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt;&lt;code&gt;NearestCentroid&lt;/code&gt;&lt;/a&gt; is simple:</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt; &lt;code&gt;NearestCentroid&lt;/code&gt; &lt;/a&gt;分类器是一种简单的算法由其成员的质心表示每个类。实际上，这使其类似于 &lt;code&gt;sklearn.KMeans&lt;/code&gt; 算法的标签更新阶段。它也没有可供选择的参数，使其成为良好的基线分类器。但是，在非凸类上以及当类具有完全不同的方差时，它确实会受到影响，因为假定所有维度上的方差都相等。请参阅线性判别分析（&lt;a href=&quot;generated/sklearn.discriminant_analysis.lineardiscriminantanalysis#sklearn.discriminant_analysis.LinearDiscriminantAnalysis&quot;&gt; &lt;code&gt;sklearn.discriminant_analysis.LinearDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt;）和二次判别分析（&lt;a href=&quot;generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&quot;&gt; &lt;code&gt;sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis&lt;/code&gt; &lt;/a&gt;），以了解没有做此假设的更复杂的方法。默认&lt;a href=&quot;generated/sklearn.neighbors.nearestcentroid#sklearn.neighbors.NearestCentroid&quot;&gt; &lt;code&gt;NearestCentroid&lt;/code&gt; 的&lt;/a&gt;用法 很简单：</target>
        </trans-unit>
        <trans-unit id="39bfe25102ea45948a285842eddb73a441e962a3" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;Pipeline&lt;/code&gt;&lt;/a&gt; is built using a list of &lt;code&gt;(key, value)&lt;/code&gt; pairs, where the &lt;code&gt;key&lt;/code&gt; is a string containing the name you want to give this step and &lt;code&gt;value&lt;/code&gt; is an estimator object:</source>
          <target state="translated">该&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;Pipeline&lt;/code&gt; &lt;/a&gt;是使用一个内置列表 &lt;code&gt;(key, value)&lt;/code&gt; 对，其中的 &lt;code&gt;key&lt;/code&gt; 是包含你想给这一步和名称的字符串 &lt;code&gt;value&lt;/code&gt; 是一个估计对象：</target>
        </trans-unit>
        <trans-unit id="c923ad3a865326ec6c72af48e5305bcc89674896" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.random_projection.gaussianrandomprojection#sklearn.random_projection.GaussianRandomProjection&quot;&gt;&lt;code&gt;sklearn.random_projection.GaussianRandomProjection&lt;/code&gt;&lt;/a&gt; reduces the dimensionality by projecting the original input space on a randomly generated matrix where components are drawn from the following distribution \(N(0, \frac{1}{n_{components}})\).</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.random_projection.gaussianrandomprojection#sklearn.random_projection.GaussianRandomProjection&quot;&gt; &lt;code&gt;sklearn.random_projection.GaussianRandomProjection&lt;/code&gt; &lt;/a&gt;通过投影，其中组分从下面的分布\绘制在随机生成的矩阵中的原始输入空间（N（0，\压裂{1} {{N_组件}}）\）减小的维数。</target>
        </trans-unit>
        <trans-unit id="bf402b0deb2f6872588357c0d1a4ee6663793993" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.random_projection.sparserandomprojection#sklearn.random_projection.SparseRandomProjection&quot;&gt;&lt;code&gt;sklearn.random_projection.SparseRandomProjection&lt;/code&gt;&lt;/a&gt; reduces the dimensionality by projecting the original input space using a sparse random matrix.</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.random_projection.sparserandomprojection#sklearn.random_projection.SparseRandomProjection&quot;&gt; &lt;code&gt;sklearn.random_projection.SparseRandomProjection&lt;/code&gt; &lt;/a&gt;通过投射使用稀疏随机矩阵原始输入空间减小的维数。</target>
        </trans-unit>
        <trans-unit id="bfc6d74a43acae56b2f26724a2d5ae8338eaf262" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;generated/sklearn.tree.export_graphviz#sklearn.tree.export_graphviz&quot;&gt;&lt;code&gt;export_graphviz&lt;/code&gt;&lt;/a&gt; exporter also supports a variety of aesthetic options, including coloring nodes by their class (or value for regression) and using explicit variable and class names if desired. Jupyter notebooks also render these plots inline automatically:</source>
          <target state="translated">所述&lt;a href=&quot;generated/sklearn.tree.export_graphviz#sklearn.tree.export_graphviz&quot;&gt; &lt;code&gt;export_graphviz&lt;/code&gt; &lt;/a&gt;出口也支持多种美学选项，包括可以通过类着色节点（或值回归）和如果需要的话使用显式的变量和类名称。Jupyter笔记本还会自动内联渲染这些图：</target>
        </trans-unit>
        <trans-unit id="4a15d0b16a3ea1ca7d8280fbccb678c643c12770" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;https://en.wikipedia.org/wiki/F1_score&quot;&gt;F-measure&lt;/a&gt; (\(F_\beta\) and \(F_1\) measures) can be interpreted as a weighted harmonic mean of the precision and recall. A \(F_\beta\) measure reaches its best value at 1 and its worst score at 0. With \(\beta = 1\), \(F_\beta\) and \(F_1\) are equivalent, and the recall and the precision are equally important.</source>
          <target state="translated">的&lt;a href=&quot;https://en.wikipedia.org/wiki/F1_score&quot;&gt;F值&lt;/a&gt;（\（F_ \测试\）和\（F_1 \）测量）可以被解释为的精确度和召回加权调和平均值。\（F_ \ beta \）度量在1达到最佳值，在0达到最差得分。对于\（\ beta = 1 \），\（F_ \ beta \）和\（F_1 \）是等效的，并且召回率和精度同样重要。</target>
        </trans-unit>
        <trans-unit id="88cf2fa597f50207550c56a5d725499fc42ad462" translate="yes" xml:space="preserve">
          <source>The &lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;Johnson-Lindenstrauss lemma&lt;/a&gt; states that any high dimensional dataset can be randomly projected into a lower dimensional Euclidean space while controlling the distortion in the pairwise distances.</source>
          <target state="translated">的&lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;约翰逊- Lindenstrauss引理&lt;/a&gt;指出任何高维数据集可以被随机地投影到低维欧几里得空间，同时控制在成对距离的失真。</target>
        </trans-unit>
        <trans-unit id="96678c00449216bcbe65a0961a8b25b8baf7a396" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; class can adapt its number of mixture components automatically. The parameter &lt;code&gt;weight_concentration_prior&lt;/code&gt; has a direct link with the resulting number of components with non-zero weights. Specifying a low value for the concentration prior will make the model put most of the weight on few components set the remaining components weights very close to zero. High values of the concentration prior will allow a larger number of components to be active in the mixture.</source>
          <target state="translated">所述 &lt;code&gt;BayesianGaussianMixture&lt;/code&gt; 类能自动适应其混合物组分的数量。参数 &lt;code&gt;weight_concentration_prior&lt;/code&gt; 与具有非零权重的所得组件数具有直接链接。为浓度先验指定一个较低的值将使模型将大部分权重放在少量组分上，而其余组分的权重则非常接近于零。先验的高浓度值将使混合物中有更多的组分具有活性。</target>
        </trans-unit>
        <trans-unit id="c17c439e95320993d0276d174b035cd14b7ce3b3" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;C&lt;/code&gt; parameter controls the amount of regularization in the &lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt; object: a large value for &lt;code&gt;C&lt;/code&gt; results in less regularization. &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; gives &lt;a href=&quot;#shrinkage&quot;&gt;Shrinkage&lt;/a&gt; (i.e. non-sparse coefficients), while &lt;code&gt;penalty=&quot;l1&quot;&lt;/code&gt; gives &lt;a href=&quot;#sparsity&quot;&gt;Sparsity&lt;/a&gt;.</source>
          <target state="translated">所述 &lt;code&gt;C&lt;/code&gt; 参数控制正规化在量&lt;a href=&quot;../../modules/generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;对象：对大的值 &lt;code&gt;C&lt;/code&gt; 在以下正规化的结果。 &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; 给出&lt;a href=&quot;#shrinkage&quot;&gt;收缩率&lt;/a&gt;（即非稀疏系数），而 &lt;code&gt;penalty=&quot;l1&quot;&lt;/code&gt; 给出&lt;a href=&quot;#sparsity&quot;&gt;收缩率&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="9164d9a9144eaecf5fe284f2e40277e82f0b8068" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;C&lt;/code&gt; parameter trades off correct classification of training examples against maximization of the decision function&amp;rsquo;s margin. For larger values of &lt;code&gt;C&lt;/code&gt;, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. A lower &lt;code&gt;C&lt;/code&gt; will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy. In other words``C`` behaves as a regularization parameter in the SVM.</source>
          <target state="translated">该 &lt;code&gt;C&lt;/code&gt; 参数折衷的训练样本对决策函数的利润率最大化正确分类。对于较大的 &lt;code&gt;C&lt;/code&gt; 值，如果决策函数可以更好地正确分类所有训练点，则可以接受较小的边距。较低的 &lt;code&gt;C&lt;/code&gt; 将鼓励更大的余量，因此会简化决策功能，但会降低训练的准确性。换句话说，C在SVM中充当正则化参数。</target>
        </trans-unit>
        <trans-unit id="9aa2de3f6ced8022ed53d959fe2e18d24e70ecf1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;DESCR&lt;/code&gt; contains a free-text description of the data, while &lt;code&gt;details&lt;/code&gt; contains a dictionary of meta-data stored by openml, like the dataset id. For more details, see the &lt;a href=&quot;https://docs.openml.org/#data&quot;&gt;OpenML documentation&lt;/a&gt; The &lt;code&gt;data_id&lt;/code&gt; of the mice protein dataset is 40966, and you can use this (or the name) to get more information on the dataset on the openml website:</source>
          <target state="translated">该 &lt;code&gt;DESCR&lt;/code&gt; 包含数据的自由文本描述，而 &lt;code&gt;details&lt;/code&gt; 包含openml存储，像集ID元数据的字典。有关更多详细信息，请参阅 &lt;code&gt;data_id&lt;/code&gt; &lt;a href=&quot;https://docs.openml.org/#data&quot;&gt;文档&lt;/a&gt;。小鼠蛋白质数据集的data_id为40966，您可以使用此名称（或名称）在openml网站上获取有关该数据集的更多信息：</target>
        </trans-unit>
        <trans-unit id="81a0037eca65e4ed69e2a49ca4871b0ce138bc10" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;Normalizer&lt;/code&gt; rescales the vector for each sample to have unit norm, independently of the distribution of the samples. It can be seen on both figures below where all samples are mapped onto the unit circle. In our example the two selected features have only positive values; therefore the transformed data only lie in the positive quadrant. This would not be the case if some original features had a mix of positive and negative values.</source>
          <target state="translated">的 &lt;code&gt;Normalizer&lt;/code&gt; 重新调整独立于样本的分布的每个样本具有单位范数的矢量。在下面的两个图中都可以看到，其中所有样本都映射到单位圆上。在我们的示例中，两个选定的特征仅具有正值。因此，转换后的数据仅位于正象限中。如果某些原始特征包含正值和负值，则情况并非如此。</target>
        </trans-unit>
        <trans-unit id="c108938c180fba7834742cb26f12054acc7a2184" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;PCA&lt;/code&gt; fitting is only computed at the evaluation of the first configuration of the &lt;code&gt;C&lt;/code&gt; parameter of the &lt;code&gt;LinearSVC&lt;/code&gt; classifier. The other configurations of &lt;code&gt;C&lt;/code&gt; will trigger the loading of the cached &lt;code&gt;PCA&lt;/code&gt; estimator data, leading to save processing time. Therefore, the use of caching the pipeline using &lt;code&gt;memory&lt;/code&gt; is highly beneficial when fitting a transformer is costly.</source>
          <target state="translated">所述 &lt;code&gt;PCA&lt;/code&gt; 拟合仅在的第一配置的评价计算 &lt;code&gt;C&lt;/code&gt; 所述的参数 &lt;code&gt;LinearSVC&lt;/code&gt; 分类器。 &lt;code&gt;C&lt;/code&gt; 的其他配置将触发加载缓存的 &lt;code&gt;PCA&lt;/code&gt; 估计器数据，从而节省了处理时间。因此，当安装变压器成本高昂时，使用 &lt;code&gt;memory&lt;/code&gt; 对管道进行缓存非常有用。</target>
        </trans-unit>
        <trans-unit id="17baab07595bc9a1b9010bf52fc5ebb7c1555943" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;RandomForestClassifier&lt;/code&gt; is trained using &lt;em&gt;bootstrap aggregation&lt;/em&gt;, where each new tree is fit from a bootstrap sample of the training observations \(z_i = (x_i, y_i)\). The &lt;em&gt;out-of-bag&lt;/em&gt; (OOB) error is the average error for each \(z_i\) calculated using predictions from the trees that do not contain \(z_i\) in their respective bootstrap sample. This allows the &lt;code&gt;RandomForestClassifier&lt;/code&gt; to be fit and validated whilst being trained &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">所述 &lt;code&gt;RandomForestClassifier&lt;/code&gt; 使用训练&lt;em&gt;自举聚合&lt;/em&gt;，其中每个新的树是从训练观测\（z_i =（X_I，Y_I）\）的自举样本配合。该&lt;em&gt;外的袋&lt;/em&gt;（OOB）误差是每个平均误差\各自的自举样本中（z_i \），使用从预测不包含树计算\（z_i \）。这允许 &lt;code&gt;RandomForestClassifier&lt;/code&gt; 在训练时进行拟合和验证&lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="093b9af7ff877738a1c191bf8fe58f666ce1b93c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;VotingClassifier&lt;/code&gt; can also be used together with &lt;code&gt;GridSearch&lt;/code&gt; in order to tune the hyperparameters of the individual estimators:</source>
          <target state="translated">所述 &lt;code&gt;VotingClassifier&lt;/code&gt; 也可以加合使用 &lt;code&gt;GridSearch&lt;/code&gt; 为了调整个体估计量的超参数：</target>
        </trans-unit>
        <trans-unit id="a92f24d7703ca3728952e82f17755a0b7604fe2c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;alpha&lt;/code&gt; parameter controls the degree of sparsity of the coefficients estimated.</source>
          <target state="translated">的 &lt;code&gt;alpha&lt;/code&gt; 参数控制估计出的系数的稀疏性的程度。</target>
        </trans-unit>
        <trans-unit id="d22b7c9a5ce685adf8551f77a733c13cab8fe2d4" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;best_estimator_&lt;/code&gt;, &lt;code&gt;best_index_&lt;/code&gt;, &lt;code&gt;best_score_&lt;/code&gt; and &lt;code&gt;best_params_&lt;/code&gt; correspond to the scorer (key) that is set to the &lt;code&gt;refit&lt;/code&gt; attribute.</source>
          <target state="translated">的 &lt;code&gt;best_estimator_&lt;/code&gt; ， &lt;code&gt;best_index_&lt;/code&gt; ， &lt;code&gt;best_score_&lt;/code&gt; 和 &lt;code&gt;best_params_&lt;/code&gt; 对应于射手（键），其被设置为 &lt;code&gt;refit&lt;/code&gt; 属性。</target>
        </trans-unit>
        <trans-unit id="ea04fcbf02a8de4070a990d8e4f932cdf0278b0d" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;beta&lt;/code&gt; parameter determines the weight of precision in the combined score. &lt;code&gt;beta &amp;lt; 1&lt;/code&gt; lends more weight to precision, while &lt;code&gt;beta &amp;gt; 1&lt;/code&gt; favors recall (&lt;code&gt;beta -&amp;gt; 0&lt;/code&gt; considers only precision, &lt;code&gt;beta -&amp;gt; inf&lt;/code&gt; only recall).</source>
          <target state="translated">该 &lt;code&gt;beta&lt;/code&gt; 参数确定的精度在组合分值的权重。 &lt;code&gt;beta &amp;lt; 1&lt;/code&gt; 赋予准确性更多的权重，而 &lt;code&gt;beta &amp;gt; 1&lt;/code&gt; 有利于召回（ &lt;code&gt;beta -&amp;gt; 0&lt;/code&gt; 仅考虑精度， &lt;code&gt;beta -&amp;gt; inf&lt;/code&gt; 仅inf召回）。</target>
        </trans-unit>
        <trans-unit id="1cbc8f71751f51b8523a564fa7c56816a950df46" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;clf&lt;/code&gt; (for classifier) estimator instance is first fitted to the model; that is, it must &lt;em&gt;learn&lt;/em&gt; from the model. This is done by passing our training set to the &lt;code&gt;fit&lt;/code&gt; method. For the training set, we&amp;rsquo;ll use all the images from our dataset, except for the last image, which we&amp;rsquo;ll reserve for our predicting. We select the training set with the &lt;code&gt;[:-1]&lt;/code&gt; Python syntax, which produces a new array that contains all but the last item from &lt;code&gt;digits.data&lt;/code&gt;:</source>
          <target state="translated">首先将 &lt;code&gt;clf&lt;/code&gt; （用于分类器）估计器实例拟合到模型；也就是说，它必须从模型中&lt;em&gt;学习&lt;/em&gt;。这是通过将我们的训练集传递给 &lt;code&gt;fit&lt;/code&gt; 方法来完成的。对于训练集，我们将使用数据集中的所有图像，但最后一个图像除外，我们将其保留用于预测。我们使用 &lt;code&gt;[:-1]&lt;/code&gt; Python语法选择训练集，这将产生一个新数组，其中包含除了 &lt;code&gt;digits.data&lt;/code&gt; 中的最后一项以外的所有内容：</target>
        </trans-unit>
        <trans-unit id="0655de3e2b717fc73c590188fdaa9881c37414a7" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;cross_validate&lt;/code&gt; function differs from &lt;code&gt;cross_val_score&lt;/code&gt; in two ways -</source>
          <target state="translated">该 &lt;code&gt;cross_validate&lt;/code&gt; 从功能不同 &lt;code&gt;cross_val_score&lt;/code&gt; 在两个方面-</target>
        </trans-unit>
        <trans-unit id="f3aad90428722c87b3422d97c1856aa8204966ed" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;cv_results_&lt;/code&gt; parameter can be easily imported into pandas as a &lt;code&gt;DataFrame&lt;/code&gt; for further inspection.</source>
          <target state="translated">所述 &lt;code&gt;cv_results_&lt;/code&gt; 参数可以很容易地导入到大熊猫作为 &lt;code&gt;DataFrame&lt;/code&gt; 以供进一步检查。</target>
        </trans-unit>
        <trans-unit id="6d0144f231c5dfa868fda545c176e4a6eca95147" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;data_id&lt;/code&gt; also uniquely identifies a dataset from OpenML:</source>
          <target state="translated">该 &lt;code&gt;data_id&lt;/code&gt; 也是唯一识别OpenML数据集：</target>
        </trans-unit>
        <trans-unit id="7bfd5d978dedbc7159d3a401f357dd25ad25428a" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;decision_function&lt;/code&gt; method is also defined from the scoring function, in such a way that negative values are outliers and non-negative ones are inliers:</source>
          <target state="translated">该 &lt;code&gt;decision_function&lt;/code&gt; 方法也从评分函数定义的，在这样一种方式，负值是异常值和非负的有内围层：</target>
        </trans-unit>
        <trans-unit id="62eacbcbc4132ef1f5317a0a939d640165e31df3" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;decision_function&lt;/code&gt; method of &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;SVC&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt;&lt;code&gt;NuSVC&lt;/code&gt;&lt;/a&gt; gives per-class scores for each sample (or a single score per sample in the binary case). When the constructor option &lt;code&gt;probability&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, class membership probability estimates (from the methods &lt;code&gt;predict_proba&lt;/code&gt; and &lt;code&gt;predict_log_proba&lt;/code&gt;) are enabled. In the binary case, the probabilities are calibrated using Platt scaling: logistic regression on the SVM&amp;rsquo;s scores, fit by an additional cross-validation on the training data. In the multiclass case, this is extended as per Wu et al. (2004).</source>
          <target state="translated">该 &lt;code&gt;decision_function&lt;/code&gt; 的方法&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;SVC&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.svm.nusvc#sklearn.svm.NuSVC&quot;&gt; &lt;code&gt;NuSVC&lt;/code&gt; &lt;/a&gt;给出每级的分数为每个样品（或在二进制的情况下，每个样品的单个分数）。当构造函数选项的 &lt;code&gt;probability&lt;/code&gt; 设置为 &lt;code&gt;True&lt;/code&gt; 时，将启用类成员资格概率估计（来自 &lt;code&gt;predict_proba&lt;/code&gt; 和 &lt;code&gt;predict_log_proba&lt;/code&gt; 方法）。在二进制情况下，概率使用Platt缩放比例进行校准：对SVM得分进行逻辑回归，并通过对训练数据进行额外的交叉验证进行拟合。在多类情况下，这根据Wu等人进行了扩展。 （2004）。</target>
        </trans-unit>
        <trans-unit id="ac9899847d23144b2277382b5ec5bf6360733941" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;features&lt;/code&gt; parameter can be set to &lt;code&gt;'all'&lt;/code&gt; to returned all features whether or not they contain missing values:</source>
          <target state="translated">该 &lt;code&gt;features&lt;/code&gt; 参数可以设置为 &lt;code&gt;'all'&lt;/code&gt; ，以他们是否包含缺失值返回的所有功能：</target>
        </trans-unit>
        <trans-unit id="0e29ac108f496200ac17f2eb1912fca379623586" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;features&lt;/code&gt; parameter is used to choose the features for which the mask is constructed. By default, it is &lt;code&gt;'missing-only'&lt;/code&gt; which returns the imputer mask of the features containing missing values at &lt;code&gt;fit&lt;/code&gt; time:</source>
          <target state="translated">的 &lt;code&gt;features&lt;/code&gt; 参数用于选择的量，掩模构造的特征。默认情况下，它是 &lt;code&gt;'missing-only'&lt;/code&gt; ，它将在 &lt;code&gt;fit&lt;/code&gt; 时返回包含缺失值的要素的不适当掩码：</target>
        </trans-unit>
        <trans-unit id="9b06298ca8347e48ebdf3df08a4c5d05e9d2dcbb" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;fit&lt;/code&gt; function takes two arguments: &lt;code&gt;n_components&lt;/code&gt;, which is the target dimensionality of the feature transform, and &lt;code&gt;gamma&lt;/code&gt;, the parameter of the RBF-kernel. A higher &lt;code&gt;n_components&lt;/code&gt; will result in a better approximation of the kernel and will yield results more similar to those produced by a kernel SVM. Note that &amp;ldquo;fitting&amp;rdquo; the feature function does not actually depend on the data given to the &lt;code&gt;fit&lt;/code&gt; function. Only the dimensionality of the data is used. Details on the method can be found in &lt;a href=&quot;#rr2007&quot; id=&quot;id3&quot;&gt;[RR2007]&lt;/a&gt;.</source>
          <target state="translated">该 &lt;code&gt;fit&lt;/code&gt; 函数有两个参数： &lt;code&gt;n_components&lt;/code&gt; ，这是该功能的目标维度转换和 &lt;code&gt;gamma&lt;/code&gt; 的RBF内核的参数。较高的 &lt;code&gt;n_components&lt;/code&gt; 将导致内核更好的近似，并将产生与内核SVM产生的结果更相似的结果。请注意，&amp;ldquo;拟合&amp;rdquo;特征函数实际上并不取决于赋予 &lt;code&gt;fit&lt;/code&gt; 函数的数据。仅使用数据的维数。有关该方法的详细信息，请参见&lt;a href=&quot;#rr2007&quot; id=&quot;id3&quot;&gt;[RR2007]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="759d68cbc1b0e0c960b6f5234a5fe3b985174684" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;init&lt;/code&gt; attribute determines the initialization method applied, which has a great impact on the performance of the method. &lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt;&lt;code&gt;NMF&lt;/code&gt;&lt;/a&gt; implements the method Nonnegative Double Singular Value Decomposition. NNDSVD &lt;a href=&quot;#id13&quot; id=&quot;id7&quot;&gt;[4]&lt;/a&gt; is based on two SVD processes, one approximating the data matrix, the other approximating positive sections of the resulting partial SVD factors utilizing an algebraic property of unit rank matrices. The basic NNDSVD algorithm is better fit for sparse factorization. Its variants NNDSVDa (in which all zeros are set equal to the mean of all elements of the data), and NNDSVDar (in which the zeros are set to random perturbations less than the mean of the data divided by 100) are recommended in the dense case.</source>
          <target state="translated">该 &lt;code&gt;init&lt;/code&gt; 属性确定所施加的初始化方法，这对本方法的性能有很大影响。&lt;a href=&quot;generated/sklearn.decomposition.nmf#sklearn.decomposition.NMF&quot;&gt; &lt;code&gt;NMF&lt;/code&gt; &lt;/a&gt;实现了非负双奇异值分解方法。 NNDSVD &lt;a href=&quot;#id13&quot; id=&quot;id7&quot;&gt;[4]&lt;/a&gt;基于两种SVD过程，一种近似数据矩阵，另一种利用单位秩矩阵的代数性质近似所得部分SVD因子的正截面。基本的NNDSVD算法更适合稀疏分解。在密集模式中，建议使用其变体NNDSVDa（其中所有零均设置为等于数据所有元素的均值）和NNDSVDar（其中零设置为随机扰动，小于数据均值除以100）案件。</target>
        </trans-unit>
        <trans-unit id="5aaf2ff68858d9c24eece58235794e4a322e1ce9" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;intercept_&lt;/code&gt; member is not converted.</source>
          <target state="translated">所述 &lt;code&gt;intercept_&lt;/code&gt; 构件不被转换。</target>
        </trans-unit>
        <trans-unit id="0e6b2b935d052640da205c359a0d82666ebb9942" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;is_data_valid&lt;/code&gt; and &lt;code&gt;is_model_valid&lt;/code&gt; functions allow to identify and reject degenerate combinations of random sub-samples. If the estimated model is not needed for identifying degenerate cases, &lt;code&gt;is_data_valid&lt;/code&gt; should be used as it is called prior to fitting the model and thus leading to better computational performance.</source>
          <target state="translated">的 &lt;code&gt;is_data_valid&lt;/code&gt; 和 &lt;code&gt;is_model_valid&lt;/code&gt; 函数允许识别和拒绝随机子样本的简并的组合。如果不需要用于估计退化案例的估计模型，则应使用 &lt;code&gt;is_data_valid&lt;/code&gt; ,因为它在拟合模型之前会被调用，从而导致更好的计算性能。</target>
        </trans-unit>
        <trans-unit id="a1aa8bc8d7f393abce9beb6161257c50a1665624" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;len(features)&lt;/code&gt; plots are arranged in a grid with &lt;code&gt;n_cols&lt;/code&gt; columns. Two-way partial dependence plots are plotted as contour plots.</source>
          <target state="translated">该 &lt;code&gt;len(features)&lt;/code&gt; 曲线被布置在与网格 &lt;code&gt;n_cols&lt;/code&gt; 列。双向偏相关图被绘制为等高线图。</target>
        </trans-unit>
        <trans-unit id="4d39915194cee376ca662b61de9924274942d60a" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;make_columntransformer&lt;/code&gt; function is available to more easily create a &lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt;&lt;code&gt;ColumnTransformer&lt;/code&gt;&lt;/a&gt; object. Specifically, the names will be given automatically. The equivalent for the above example would be:</source>
          <target state="translated">该 &lt;code&gt;make_columntransformer&lt;/code&gt; 功能可更容易地创建一个&lt;a href=&quot;generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer&quot;&gt; &lt;code&gt;ColumnTransformer&lt;/code&gt; &lt;/a&gt;对象。具体来说，名称将自动给出。以上示例的等效项为：</target>
        </trans-unit>
        <trans-unit id="e3ab7886f45f0e5bcfcb494c5dda13f6aee54058" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;mean_fit_time&lt;/code&gt;, &lt;code&gt;std_fit_time&lt;/code&gt;, &lt;code&gt;mean_score_time&lt;/code&gt; and &lt;code&gt;std_score_time&lt;/code&gt; are all in seconds.</source>
          <target state="translated">该 &lt;code&gt;mean_fit_time&lt;/code&gt; ， &lt;code&gt;std_fit_time&lt;/code&gt; ， &lt;code&gt;mean_score_time&lt;/code&gt; 和 &lt;code&gt;std_score_time&lt;/code&gt; 都在秒。</target>
        </trans-unit>
        <trans-unit id="4dc710fdb008b124893854b1db0d772123e2f23c" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;out_of_bounds&lt;/code&gt; parameter handles how x-values outside of the training domain are handled. When set to &amp;ldquo;nan&amp;rdquo;, predicted y-values will be NaN. When set to &amp;ldquo;clip&amp;rdquo;, predicted y-values will be set to the value corresponding to the nearest train interval endpoint. When set to &amp;ldquo;raise&amp;rdquo;, allow &lt;code&gt;interp1d&lt;/code&gt; to throw ValueError.</source>
          <target state="translated">该 &lt;code&gt;out_of_bounds&lt;/code&gt; x值的训练域之外是如何处理的参数句柄。当设置为&amp;ldquo; nan&amp;rdquo;时，预测的y值将为NaN。当设置为&amp;ldquo; clip&amp;rdquo;时，预测的y值将设置为与最近的火车间隔终点相对应的值。当设置为&amp;ldquo; raise&amp;rdquo;时，允许 &lt;code&gt;interp1d&lt;/code&gt; 抛出ValueError。</target>
        </trans-unit>
        <trans-unit id="3c0e73047db48d9d2c7fabbdd5b52d96e2b806a9" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;partial_fit&lt;/code&gt; method call of naive Bayes models introduces some computational overhead. It is recommended to use data chunk sizes that are as large as possible, that is as the available RAM allows.</source>
          <target state="translated">朴素贝叶斯模型的 &lt;code&gt;partial_fit&lt;/code&gt; 方法调用引入了一些计算开销。建议使用尽可能大的数据块大小，即可用RAM允许的大小。</target>
        </trans-unit>
        <trans-unit id="c075895944959b3ce70972d2605db496c74ee36b" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;preprocessing&lt;/code&gt; module further provides a utility class &lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt;&lt;code&gt;Normalizer&lt;/code&gt;&lt;/a&gt; that implements the same operation using the &lt;code&gt;Transformer&lt;/code&gt; API (even though the &lt;code&gt;fit&lt;/code&gt; method is useless in this case: the class is stateless as this operation treats samples independently).</source>
          <target state="translated">所述 &lt;code&gt;preprocessing&lt;/code&gt; 模块还提供了一种工具类&lt;a href=&quot;generated/sklearn.preprocessing.normalizer#sklearn.preprocessing.Normalizer&quot;&gt; &lt;code&gt;Normalizer&lt;/code&gt; &lt;/a&gt;实现使用相同的操作 &lt;code&gt;Transformer&lt;/code&gt; API（即使 &lt;code&gt;fit&lt;/code&gt; 方法是在这种情况下无用：类是无状态的，因为这操作对待样品独立地）。</target>
        </trans-unit>
        <trans-unit id="deb9c47cdc32b652eb6b8e87508a50e73ddd6520" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;preprocessing&lt;/code&gt; module further provides a utility class &lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt;&lt;code&gt;StandardScaler&lt;/code&gt;&lt;/a&gt; that implements the &lt;code&gt;Transformer&lt;/code&gt; API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. This class is hence suitable for use in the early steps of a &lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt;&lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">该 &lt;code&gt;preprocessing&lt;/code&gt; 模块还提供了一个实用工具类&lt;a href=&quot;generated/sklearn.preprocessing.standardscaler#sklearn.preprocessing.StandardScaler&quot;&gt; &lt;code&gt;StandardScaler&lt;/code&gt; &lt;/a&gt;实现了 &lt;code&gt;Transformer&lt;/code&gt; API来计算在训练集的平均值和标准偏差，从而可以稍后重新应用测试集相同的转换。因此，此类适合在&lt;a href=&quot;generated/sklearn.pipeline.pipeline#sklearn.pipeline.Pipeline&quot;&gt; &lt;code&gt;sklearn.pipeline.Pipeline&lt;/code&gt; &lt;/a&gt;的早期步骤中使用：</target>
        </trans-unit>
        <trans-unit id="1c988d2d2cf78f045059c60f8f22ddfe99f6b9b8" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;random_state&lt;/code&gt; parameter defaults to &lt;code&gt;None&lt;/code&gt;, meaning that the shuffling will be different every time &lt;code&gt;KFold(..., shuffle=True)&lt;/code&gt; is iterated. However, &lt;code&gt;GridSearchCV&lt;/code&gt; will use the same shuffling for each set of parameters validated by a single call to its &lt;code&gt;fit&lt;/code&gt; method.</source>
          <target state="translated">该 &lt;code&gt;random_state&lt;/code&gt; 参数默认为 &lt;code&gt;None&lt;/code&gt; ，这意味着洗牌每一次将不同 &lt;code&gt;KFold(..., shuffle=True)&lt;/code&gt; 迭代。但是， &lt;code&gt;GridSearchCV&lt;/code&gt; 将对通过一次调用其 &lt;code&gt;fit&lt;/code&gt; 方法验证的每组参数使用相同的改组。</target>
        </trans-unit>
        <trans-unit id="269b993dc29ab1401256004d5e02aa3f5322b3b6" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;remainder&lt;/code&gt; parameter can be set to an estimator to transform the remaining rating columns. The transformed values are appended to the end of the transformation:</source>
          <target state="translated">该 &lt;code&gt;remainder&lt;/code&gt; 参数可以设置为一个估计改造其余评级列。转换后的值将附加到转换的末尾：</target>
        </trans-unit>
        <trans-unit id="5cf025ac399ddc1c71c004c7cb8bd73171357561" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;shrinkage&lt;/code&gt; parameter can also be manually set between 0 and 1. In particular, a value of 0 corresponds to no shrinkage (which means the empirical covariance matrix will be used) and a value of 1 corresponds to complete shrinkage (which means that the diagonal matrix of variances will be used as an estimate for the covariance matrix). Setting this parameter to a value between these two extrema will estimate a shrunk version of the covariance matrix.</source>
          <target state="translated">该 &lt;code&gt;shrinkage&lt;/code&gt; 的参数也可以被手动设定在0和1具体而言，0对应于无收缩的值（这意味着经验协方差矩阵将被使用）和1个对应于完全收缩的值（之间，这意味着对角线方差矩阵将用作协方差矩阵的估计值）。将此参数设置为这两个极值之间的值将估计协方差矩阵的缩小版本。</target>
        </trans-unit>
        <trans-unit id="b439b9c10b67475737ae3e151e90a6ed815edbd1" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.covariance&lt;/code&gt; package implements a robust estimator of covariance, the Minimum Covariance Determinant &lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;[3]&lt;/a&gt;.</source>
          <target state="translated">所述 &lt;code&gt;sklearn.covariance&lt;/code&gt; 包实现协方差的估计健壮，最小协方差行列式&lt;a href=&quot;#id11&quot; id=&quot;id8&quot;&gt;[3] &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6908af1748b7fe666814b53ca6e0a8cab66eb012" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.datasets&lt;/code&gt; package embeds some small toy datasets as introduced in the &lt;a href=&quot;../tutorial/basic/tutorial#loading-example-dataset&quot;&gt;Getting Started&lt;/a&gt; section.</source>
          <target state="translated">该 &lt;code&gt;sklearn.datasets&lt;/code&gt; 包嵌入如引入了一些小玩具的数据集&lt;a href=&quot;../tutorial/basic/tutorial#loading-example-dataset&quot;&gt;入门&lt;/a&gt;部分。</target>
        </trans-unit>
        <trans-unit id="82c9f1d0e48a88b08f4d85a858e0d70b67f55eec" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.datasets&lt;/code&gt; package is able to download datasets from the repository using the function &lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_openml#sklearn.datasets.fetch_openml&quot;&gt;&lt;code&gt;sklearn.datasets.fetch_openml&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">该 &lt;code&gt;sklearn.datasets&lt;/code&gt; 包能够从使用功能库下载的数据集&lt;a href=&quot;../modules/generated/sklearn.datasets.fetch_openml#sklearn.datasets.fetch_openml&quot;&gt; &lt;code&gt;sklearn.datasets.fetch_openml&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="8a80b4da04f92037fdd1ffd5771651becd3a649e" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;sklearn.preprocessing&lt;/code&gt; package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.</source>
          <target state="translated">所述 &lt;code&gt;sklearn.preprocessing&lt;/code&gt; 包提供几种常见的实用功能和变压器类来改变原始的特征矢量为表示更适合为下游估计。</target>
        </trans-unit>
        <trans-unit id="9a666cbe94ae4d4ef285ecec57ff29087d1b1c92" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;stop_words_&lt;/code&gt; attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling.</source>
          <target state="translated">酸洗时， &lt;code&gt;stop_words_&lt;/code&gt; 属性会变大并增加模型大小。该属性仅用于自省，可以在使用酸洗之前使用delattr安全删除或将其设置为None。</target>
        </trans-unit>
        <trans-unit id="a2df492be59e0b5e94ab9ece47f7ab58734f8d4a" translate="yes" xml:space="preserve">
          <source>The &lt;code&gt;svm.OneClassSVM&lt;/code&gt; is known to be sensitive to outliers and thus does not perform very well for outlier detection. This estimator is best suited for novelty detection when the training set is not contaminated by outliers. That said, outlier detection in high-dimension, or without any assumptions on the distribution of the inlying data is very challenging, and a One-class SVM might give useful results in these situations depending on the value of its hyperparameters.</source>
          <target state="translated">所述 &lt;code&gt;svm.OneClassSVM&lt;/code&gt; 已知是对异常值敏感并因此对异常值检测不执行得非常好。当训练集不受异常值污染时，此估计器最适合新颖性检测。也就是说，在高维中进行离群值检测，或者不对基础数据的分布进行任何假设都是非常具有挑战性的，一类SVM在这些情况下可能会根据其超参数的值给出有用的结果。</target>
        </trans-unit>
        <trans-unit id="a0172c625369e43fa830669101114953189c751e" translate="yes" xml:space="preserve">
          <source>The &lt;em&gt;kernel function&lt;/em&gt; can be any of the following:</source>
          <target state="translated">该&lt;em&gt;内核函数&lt;/em&gt;可以是任何如下：</target>
        </trans-unit>
        <trans-unit id="6eff32d476fb25ebcdfdcb46623a8711aa972809" translate="yes" xml:space="preserve">
          <source>The &lt;strong&gt;conditional entropy of clusters given class&lt;/strong&gt;\(H(K|C)\) and the &lt;strong&gt;entropy of clusters&lt;/strong&gt;\(H(K)\) are defined in a symmetric manner.</source>
          <target state="translated">&lt;strong&gt;给定类&lt;/strong&gt; \（H（K | C）\）&lt;strong&gt;的聚类&lt;/strong&gt;的&lt;strong&gt;条件熵和聚类&lt;/strong&gt; \（H（K）\）的&lt;strong&gt;熵&lt;/strong&gt;以对称方式定义。</target>
        </trans-unit>
        <trans-unit id="2143ba51f2aad99e99e8cd60a62ccfdecbf0f265" translate="yes" xml:space="preserve">
          <source>The AMI returns a value of 1 when the two partitions are identical (ie perfectly matched). Random partitions (independent labellings) have an expected AMI around 0 on average hence can be negative.</source>
          <target state="translated">当两个分区相同(即完全匹配)时,AMI返回的值为1。随机分区(独立标签)的预期AMI平均在0左右,因此可能为负值。</target>
        </trans-unit>
        <trans-unit id="00e3722885bdadae5430c6fecaacfc1cced893f6" translate="yes" xml:space="preserve">
          <source>The API is experimental in version 0.20 (particularly the return value structure), and might have small backward-incompatible changes in future releases.</source>
          <target state="translated">API在0.20版本中是实验性的(尤其是返回值结构),在未来的版本中可能会有小的向后不兼容的变化。</target>
        </trans-unit>
        <trans-unit id="22556f00c2cdfc8c60673e1c663299619a64901c" translate="yes" xml:space="preserve">
          <source>The BIC criterion can be used to select the number of components in a Gaussian Mixture in an efficient way. In theory, it recovers the true number of components only in the asymptotic regime (i.e. if much data is available and assuming that the data was actually generated i.i.d. from a mixture of Gaussian distribution). Note that using a &lt;a href=&quot;#bgmm&quot;&gt;Variational Bayesian Gaussian mixture&lt;/a&gt; avoids the specification of the number of components for a Gaussian mixture model.</source>
          <target state="translated">BIC标准可用于以有效方式选择高斯混合物中的组分数。从理论上讲，它仅在渐近状态下才恢复成分的真实数量（即，如果有大量数据可用，并假设数据实际上是从高斯分布的混合中生成的）。请注意，使用&lt;a href=&quot;#bgmm&quot;&gt;变分贝叶斯高斯混合&lt;/a&gt;可避免指定高斯混合模型的分量数。</target>
        </trans-unit>
        <trans-unit id="c912c462511f5921d016fed26135f626922ccc57" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut implementation only works when the target dimensionality is 3 or less. The 2D case is typical when building visualizations.</source>
          <target state="translated">Barnes-Hut的实现只有在目标维度为3或更小的情况下才有效。在构建可视化时,2D情况是典型的。</target>
        </trans-unit>
        <trans-unit id="49027a83447d6307ec530f0dd81bd5ad76360faa" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.</source>
          <target state="translated">Barnes-Hut t-SNE方法仅限于二维或三维嵌入。</target>
        </trans-unit>
        <trans-unit id="f05f901deabcb1ee376f8697fc79932cbf746ce6" translate="yes" xml:space="preserve">
          <source>The Barnes-Hut t-SNE that has been implemented here is usually much slower than other manifold learning algorithms. The optimization is quite difficult and the computation of the gradient is \(O[d N log(N)]\), where \(d\) is the number of output dimensions and \(N\) is the number of samples. The Barnes-Hut method improves on the exact method where t-SNE complexity is \(O[d N^2]\), but has several other notable differences:</source>
          <target state="translated">这里已经实现的Barnes-Hut t-SNE通常比其他歧管学习算法慢得多。优化是相当困难的,梯度的计算是 \(O[d N log(N)]\),其中 \(d\)是输出维数,\(N\)是样本数。Barnes-Hut方法在精确方法的基础上进行了改进,其中t-SNE的复杂度为/(O[d N^2]\),但还有其他一些明显的不同。</target>
        </trans-unit>
        <trans-unit id="e724094f6d5c410991717a9e2fc045d6798def45" translate="yes" xml:space="preserve">
          <source>The Birch algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters.</source>
          <target state="translated">Birch算法有两个参数,即阈值和分支因子。分支因子限制了一个节点中子群的数量,阈值限制了进入样本与现有子群之间的距离。</target>
        </trans-unit>
        <trans-unit id="8fa70be719c3475e8e012f73acc572a21c03cdd8" translate="yes" xml:space="preserve">
          <source>The Boston house-price data has been used in many machine learning papers that address regression problems.</source>
          <target state="translated">波士顿房价数据已经被许多机器学习论文用来解决回归问题。</target>
        </trans-unit>
        <trans-unit id="455243540177422da87f9c3aa93de30456a2bae3" translate="yes" xml:space="preserve">
          <source>The Boston house-price data of Harrison, D. and Rubinfeld, D.L. &amp;lsquo;Hedonic prices and the demand for clean air&amp;rsquo;, J. Environ. Economics &amp;amp; Management, vol.5, 81-102, 1978. Used in Belsley, Kuh &amp;amp; Welsch, &amp;lsquo;Regression diagnostics &amp;hellip;&amp;rsquo;, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter.</source>
          <target state="translated">波士顿哈里森（Harrison）和鲁宾菲尔德（Rubinfeld）的房价数据，DL&amp;ldquo;享乐主义价格和对清洁空气的需求&amp;rdquo;，J。Environ。《经济与管理》，第5卷，第81-102页，1978年。在Belsley，Kuh和Welsch中使用，&amp;ldquo;回归诊断&amp;hellip;&amp;hellip;&amp;rdquo;，Wiley，1980年。注意后者的244-261页上的表格中使用了各种转换。</target>
        </trans-unit>
        <trans-unit id="0e6c056d908bee991f648998f41ff72e8a7301c2" translate="yes" xml:space="preserve">
          <source>The CF Subclusters hold the necessary information for clustering which prevents the need to hold the entire input data in memory. This information includes:</source>
          <target state="translated">CF子簇保存了聚类所需的信息,从而避免了在内存中保存全部输入数据的需要。这些信息包括:</target>
        </trans-unit>
        <trans-unit id="b96c855c122e21633007587f3600e092339a5383" translate="yes" xml:space="preserve">
          <source>The Calinski-Harabaz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">Calinski-Harabaz指数对于凸簇来说,一般要高于其他概念的簇,比如像通过DBSCAN得到的基于密度的簇。</target>
        </trans-unit>
        <trans-unit id="6cdbc3a888667e8344981c2a8742122994627087" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier described in Rennie et al.</source>
          <target state="translated">Rennie等人描述的补全奈夫贝叶斯分类器。</target>
        </trans-unit>
        <trans-unit id="2b0b8b0a4dd5523203a6ee4b5d195c2e6a35c0fe" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier described in Rennie et al. (2003).</source>
          <target state="translated">Rennie等人(2003)中描述的补全Naive Bayes分类器。</target>
        </trans-unit>
        <trans-unit id="3398d87a3aceb1ea8375c52fcc6a67cec5c63df9" translate="yes" xml:space="preserve">
          <source>The Complement Naive Bayes classifier was designed to correct the &amp;ldquo;severe assumptions&amp;rdquo; made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.</source>
          <target state="translated">补充朴素贝叶斯分类器旨在纠正标准多项朴素贝叶斯分类器所做的&amp;ldquo;严峻假设&amp;rdquo;。它特别适合于不平衡的数据集。</target>
        </trans-unit>
        <trans-unit id="7c176cfd9628fae51161daa03c193aeae01d49ea" translate="yes" xml:space="preserve">
          <source>The Contrastive Divergence method suggests to stop the chain after a small number of iterations, \(k\), usually even 1. This method is fast and has low variance, but the samples are far from the model distribution.</source>
          <target state="translated">Contrastive Divergence方法建议在少量迭代后停止链,通常甚至是1,这种方法速度快,方差小,但样本远离模型分布。</target>
        </trans-unit>
        <trans-unit id="1d09474bd461f8756796df6e9a77749a8489b6d3" translate="yes" xml:space="preserve">
          <source>The DBSCAN algorithm is deterministic, always generating the same clusters when given the same data in the same order. However, the results can differ when data is provided in a different order. First, even though the core samples will always be assigned to the same clusters, the labels of those clusters will depend on the order in which those samples are encountered in the data. Second and more importantly, the clusters to which non-core samples are assigned can differ depending on the data order. This would happen when a non-core sample has a distance lower than &lt;code&gt;eps&lt;/code&gt; to two core samples in different clusters. By the triangular inequality, those two core samples must be more distant than &lt;code&gt;eps&lt;/code&gt; from each other, or they would be in the same cluster. The non-core sample is assigned to whichever cluster is generated first in a pass through the data, and so the results will depend on the data ordering.</source>
          <target state="translated">DBSCAN算法是确定性的，当以相同的顺序提供相同的数据时，总是会生成相同的簇。但是，当以不同顺序提供数据时，结果可能会有所不同。首先，即使将核心样本始终分配给相同的聚类，但这些聚类的标签将取决于在数据中遇到这些样本的顺序。其次，更重要的是，分配非核心样本的群集可能会根据数据顺序而有所不同。当非核心样本的距离小于 &lt;code&gt;eps&lt;/code&gt; 到不同聚类中的两个核心样本的距离时，就会发生这种情况。通过三角不等式，这两个核心样本必须比 &lt;code&gt;eps&lt;/code&gt; 更远彼此之间，否则它们将在同一群集中。非核心样本将分配给在通过数据时首先生成的任何集群，因此结果将取决于数据顺序。</target>
        </trans-unit>
        <trans-unit id="b5783662e991f62ab3cc63e2843b11c10be0af6e" translate="yes" xml:space="preserve">
          <source>The Davies-Boulding index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained from DBSCAN.</source>
          <target state="translated">Davies-Boulding指数对于凸型聚类来说,一般比其他聚类的概念要高,比如基于密度的聚类,比如从DBSCAN得到的聚类。</target>
        </trans-unit>
        <trans-unit id="9dae6187a2e4264ce3764c47d6ced56e22112bc3" translate="yes" xml:space="preserve">
          <source>The Digit Dataset</source>
          <target state="translated">数字数据集</target>
        </trans-unit>
        <trans-unit id="15949e73904f01f7a29e0206a3232b388b3af43d" translate="yes" xml:space="preserve">
          <source>The Dirichlet process prior allows to define an infinite number of components and automatically selects the correct number of components: it activates a component only if it is necessary.</source>
          <target state="translated">Dirichlet过程之前允许定义无限数量的组件,并自动选择正确的组件数量:只有在必要的情况下才会激活一个组件。</target>
        </trans-unit>
        <trans-unit id="d2e49448333a2cd92baf05825e927765f1835316" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is commonly combined with exponentiation.</source>
          <target state="translated">DotProduct内核通常与指数相结合。</target>
        </trans-unit>
        <trans-unit id="842c0c72bca462c8ce3e9ff0cead821c33430a8a" translate="yes" xml:space="preserve">
          <source>The DotProduct kernel is non-stationary and can be obtained from linear regression by putting N(0, 1) priors on the coefficients of x_d (d = 1, . . . , D) and a prior of N(0, sigma_0^2) on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter sigma_0^2. For sigma_0^2 =0, the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</source>
          <target state="translated">DotProduct核是非稳态的,可以通过对x_d的系数(d=1,......,D)设置N(0,1)前导,对偏置设置N(0,sigma_0^2)前导,从线性回归中得到。DotProduct核对原点坐标的旋转不变,但对平移不变。它的参数是sigma_0^2。对于sigma_0^2=0,这个核被称为同质线性核,否则就是非同质的。该核由以下公式给出</target>
        </trans-unit>
        <trans-unit id="c219b28ff4dd0afb8b865b96896a279315780153" translate="yes" xml:space="preserve">
          <source>The Elastic Net mixing parameter, with 0 &amp;lt;= l1_ratio &amp;lt;= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15.</source>
          <target state="translated">0 &amp;lt;= l1_ratio &amp;lt;= 1的弹性网混合参数。l1_ratio = 0对应于L2惩罚，l1_ratio = 1到L1。默认值为0.15。</target>
        </trans-unit>
        <trans-unit id="71317f65b5997839fb7f2d86b8c73dc4399ad254" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with 0 &amp;lt; l1_ratio &amp;lt;= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1/L2 and L2.</source>
          <target state="translated">ElasticNet混合参数，其中0 &amp;lt;l1_ratio &amp;lt;=1。对于l1_ratio = 1，惩罚是L1 / L2惩罚。对于l1_ratio = 0，这是L2损失。对于 &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; ，惩罚是L1 / L2和L2的组合。</target>
        </trans-unit>
        <trans-unit id="f913d5ab29d41b1d0b8510f8960f88320058e481" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with 0 &amp;lt; l1_ratio &amp;lt;= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1/L2 and L2. This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in &lt;code&gt;[.1, .5, .7,
.9, .95, .99, 1]&lt;/code&gt;</source>
          <target state="translated">ElasticNet混合参数，其中0 &amp;lt;l1_ratio &amp;lt;=1。对于l1_ratio = 1，惩罚是L1 / L2惩罚。对于l1_ratio = 0，这是L2损失。对于 &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; ，惩罚是L1 / L2和L2的组合。此参数可以是一个列表，在这种情况下，通过交叉验证测试不同的值，并使用给出最佳预测得分的值。请注意，最好选择l1_ratio的值列表，如 &lt;code&gt;[.1, .5, .7, .9, .95, .99, 1]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="62ded71b403044434993092527c854b8f53e5c17" translate="yes" xml:space="preserve">
          <source>The ElasticNet mixing parameter, with &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt;. For &lt;code&gt;l1_ratio = 0&lt;/code&gt; the penalty is an L2 penalty. &lt;code&gt;For l1_ratio = 1&lt;/code&gt; it is an L1 penalty. For &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt;, the penalty is a combination of L1 and L2.</source>
          <target state="translated">ElasticNet混合参数， &lt;code&gt;0 &amp;lt;= l1_ratio &amp;lt;= 1&lt;/code&gt; 。对于 &lt;code&gt;l1_ratio = 0&lt;/code&gt; ，惩罚是L2惩罚。 &lt;code&gt;For l1_ratio = 1&lt;/code&gt; 这是L1损失。对于 &lt;code&gt;0 &amp;lt; l1_ratio &amp;lt; 1&lt;/code&gt; ，惩罚是L1和L2的组合。</target>
        </trans-unit>
        <trans-unit id="706ede4c7ae02c53bd29139976a5c9ab95013fca" translate="yes" xml:space="preserve">
          <source>The ExpSineSquared kernel allows modeling periodic functions. It is parameterized by a length-scale parameter length_scale&amp;gt;0 and a periodicity parameter periodicity&amp;gt;0. Only the isotropic variant where l is a scalar is supported at the moment. The kernel given by:</source>
          <target state="translated">ExpSineSquared内核允许对周期性函数进行建模。它由长度比例参数length_scale&amp;gt; 0和周期性参数周期性&amp;gt; 0参数化。目前仅支持l为标量的各向同性变体。内核由：</target>
        </trans-unit>
        <trans-unit id="bd1aecbb19e2286cfafdebb8292f1071a3eb508c" translate="yes" xml:space="preserve">
          <source>The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.</source>
          <target state="translated">F-beta得分可以解释为精度和召回率的加权谐波平均值,F-beta得分在1时达到最佳值,在0时达到最差值。</target>
        </trans-unit>
        <trans-unit id="fcc8a075bfea5f42d0e38256200386182d33cc42" translate="yes" xml:space="preserve">
          <source>The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0.</source>
          <target state="translated">F-beta得分是精度和召回率的加权谐波平均值,达到最佳值为1,最差值为0。</target>
        </trans-unit>
        <trans-unit id="553773b1d0f7903e424f857dbba13f4b2343a5a1" translate="yes" xml:space="preserve">
          <source>The F-beta score weights recall more than precision by a factor of &lt;code&gt;beta&lt;/code&gt;. &lt;code&gt;beta == 1.0&lt;/code&gt; means recall and precision are equally important.</source>
          <target state="translated">F-beta得分权重比 &lt;code&gt;beta&lt;/code&gt; 精确度高得多。 &lt;code&gt;beta == 1.0&lt;/code&gt; 表示召回率和精确度同等重要。</target>
        </trans-unit>
        <trans-unit id="3f53f44feaa854c0fdf5365ef5e43bc7fa440b01" translate="yes" xml:space="preserve">
          <source>The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:</source>
          <target state="translated">F1得分可以理解为精度和召回的加权平均值,F1得分在1时达到最佳值,0时达到最差值,精度和召回对F1得分的相对贡献是相等的。F1得分的计算公式为:。</target>
        </trans-unit>
        <trans-unit id="518509d08bd98ece386f11a6583f35b4f559fdac" translate="yes" xml:space="preserve">
          <source>The Figure below shows four one-way and one two-way partial dependence plots for the California housing dataset:</source>
          <target state="translated">下图显示了加州住房数据集的四个单向和一个双向部分依赖图。</target>
        </trans-unit>
        <trans-unit id="2280ec0b9d5ba5eef024fd7f6041defdab8c9d48" translate="yes" xml:space="preserve">
          <source>The Figure below shows the contours of the different regularization terms in the parameter space when \(R(w) = 1\).</source>
          <target state="translated">下图显示了参数空间中不同正则化项的轮廓,当/(R(w)=1/)。</target>
        </trans-unit>
        <trans-unit id="af21e4c771514d1aca6cd1c14dcff1ce67a0e477" translate="yes" xml:space="preserve">
          <source>The Fowlkes-Mallows index (&lt;a href=&quot;generated/sklearn.metrics.fowlkes_mallows_score#sklearn.metrics.fowlkes_mallows_score&quot;&gt;&lt;code&gt;sklearn.metrics.fowlkes_mallows_score&lt;/code&gt;&lt;/a&gt;) can be used when the ground truth class assignments of the samples is known. The Fowlkes-Mallows score FMI is defined as the geometric mean of the pairwise precision and recall:</source>
          <target state="translated">当已知样本的地面真理类分配时，可以使用Fowlkes-Mallows索引（&lt;a href=&quot;generated/sklearn.metrics.fowlkes_mallows_score#sklearn.metrics.fowlkes_mallows_score&quot;&gt; &lt;code&gt;sklearn.metrics.fowlkes_mallows_score&lt;/code&gt; &lt;/a&gt;）。 Fowlkes-Mallows分数FMI定义为成对精度和召回率的几何平均值：</target>
        </trans-unit>
        <trans-unit id="bfdc8ed29be1f90cbfb4dc9576ce17a736f93294" translate="yes" xml:space="preserve">
          <source>The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of the precision and recall:</source>
          <target state="translated">Fowlkes-Mallows指数(FMI)被定义为精确度和召回率之间的几何平均值。</target>
        </trans-unit>
        <trans-unit id="90c804ef552ac504772d87a0c90e2454ea50931f" translate="yes" xml:space="preserve">
          <source>The GP prior mean is assumed to be zero. The prior&amp;rsquo;s covariance is specified by a passing a &lt;a href=&quot;#gp-kernels&quot;&gt;kernel&lt;/a&gt; object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed &lt;code&gt;optimizer&lt;/code&gt;. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying &lt;code&gt;n_restarts_optimizer&lt;/code&gt;. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, &lt;code&gt;None&lt;/code&gt; can be passed as optimizer.</source>
          <target state="translated">假设GP先验均值为零。先验的协方差由传递的&lt;a href=&quot;#gp-kernels&quot;&gt;内核&lt;/a&gt;对象指定。通过基于传递的 &lt;code&gt;optimizer&lt;/code&gt; 最大化对数边际似然性（LML）来优化GaussianProcessRegressor期间，优化内核的超参数。由于LML可能具有多个局部最优值，因此可以通过指定 &lt;code&gt;n_restarts_optimizer&lt;/code&gt; 重复启动优化器。始终从内核的初始超参数值开始进行第一次运行；随后的运行是从已从允许值范围内随机选择的超参数值进行的。如果初始超参数应保持固定，则不能将 &lt;code&gt;None&lt;/code&gt; 参数作为优化器传递。</target>
        </trans-unit>
        <trans-unit id="01a974069deaad9f4a696951139fa6f180c4610d" translate="yes" xml:space="preserve">
          <source>The HLLE algorithm comprises three stages:</source>
          <target state="translated">HLLE算法包括三个阶段:</target>
        </trans-unit>
        <trans-unit id="eaec5120030b979edc1dcb8e844874e0db8c76f1" translate="yes" xml:space="preserve">
          <source>The Hamming loss is the fraction of labels that are incorrectly predicted.</source>
          <target state="translated">汉明损失是指错误预测的标签的分数。</target>
        </trans-unit>
        <trans-unit id="ee89b1587a7c7b8d189cccf628a41e55c5d7ebe4" translate="yes" xml:space="preserve">
          <source>The Hamming loss is upperbounded by the subset zero-one loss. When normalized over samples, the Hamming loss is always between 0 and 1.</source>
          <target state="translated">汉明损耗是由子集零一损耗上界的。当对样本进行归一化处理时,汉明损耗总是在0和1之间。</target>
        </trans-unit>
        <trans-unit id="2f575d5541e123fdf35ce0dd5c6fb4373f14dde3" translate="yes" xml:space="preserve">
          <source>The Huber Regressor optimizes the squared loss for the samples where &lt;code&gt;|(y - X'w) / sigma| &amp;lt; epsilon&lt;/code&gt; and the absolute loss for the samples where &lt;code&gt;|(y - X'w) / sigma| &amp;gt; epsilon&lt;/code&gt;, where w and sigma are parameters to be optimized. The parameter sigma makes sure that if y is scaled up or down by a certain factor, one does not need to rescale epsilon to achieve the same robustness. Note that this does not take into account the fact that the different features of X may be of different scales.</source>
          <target state="translated">Huber回归器针对 &lt;code&gt;|(y - X'w) / sigma| &amp;lt; epsilon&lt;/code&gt; 的样本优化平方损失。&amp;lt;epsilon和样本的绝对损耗，其中 &lt;code&gt;|(y - X'w) / sigma| &amp;gt; epsilon&lt;/code&gt; ，其中w和sigma是要优化的参数。参数sigma可确保如果y放大或缩小某个因子，则无需重新缩放epsilon即可达到相同的鲁棒性。请注意，这没有考虑X的不同特征可能具有不同比例的事实。</target>
        </trans-unit>
        <trans-unit id="a428acf86562c566ebfc0a4a1d3e42c700a1a75f" translate="yes" xml:space="preserve">
          <source>The Huber and epsilon-insensitive loss functions can be used for robust regression. The width of the insensitive region has to be specified via the parameter &lt;code&gt;epsilon&lt;/code&gt;. This parameter depends on the scale of the target variables.</source>
          <target state="translated">Huber和&amp;epsilon;不敏感的损失函数可用于鲁棒回归。不敏感区域的宽度必须通过参数 &lt;code&gt;epsilon&lt;/code&gt; 指定。此参数取决于目标变量的范围。</target>
        </trans-unit>
        <trans-unit id="0a2150cee6da11610a14f68e745e5d0639a39459" translate="yes" xml:space="preserve">
          <source>The Iris Dataset</source>
          <target state="translated">虹膜数据集</target>
        </trans-unit>
        <trans-unit id="7c0ef25371cb6aecb434ff0290ad88905a1c2367" translate="yes" xml:space="preserve">
          <source>The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour and Virginica) with 4 attributes: sepal length, sepal width, petal length and petal width.</source>
          <target state="translated">鸢尾花数据集代表了3种鸢尾花(Setosa、Versicolour和Virginica),有4个属性:萼片长度、萼片宽度、花瓣长度和花瓣宽度。</target>
        </trans-unit>
        <trans-unit id="3877561d1fbee6aff8708ef4ec5fcabe6115833e" translate="yes" xml:space="preserve">
          <source>The IsolationForest &amp;lsquo;isolates&amp;rsquo; observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.</source>
          <target state="translated">IsolationForest通过随机选择特征，然后随机选择选定特征的最大值和最小值之间的分割值来&amp;ldquo;隔离&amp;rdquo;观察结果。</target>
        </trans-unit>
        <trans-unit id="9e47d533732129c7b308a9673031913c04e17afd" translate="yes" xml:space="preserve">
          <source>The Isomap algorithm comprises three stages:</source>
          <target state="translated">等值线图算法包括三个阶段。</target>
        </trans-unit>
        <trans-unit id="43f0627e56441244b0d44b83d4abbf8510d9eac0" translate="yes" xml:space="preserve">
          <source>The Jaccard index [1], or Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets, is used to compare set of predicted labels for a sample to the corresponding set of labels in &lt;code&gt;y_true&lt;/code&gt;.</source>
          <target state="translated">Jaccard索引[1]或Jaccard相似系数，定义为交集的大小除以两个标签集的并集的大小，用于将样本的预测标签集与 &lt;code&gt;y_true&lt;/code&gt; 中的相应标签集进行比较。</target>
        </trans-unit>
        <trans-unit id="54b0fb2f21ad6f516f16dd3562af3d3b867660fe" translate="yes" xml:space="preserve">
          <source>The Jaccard similarity coefficient of the \(i\)-th samples, with a ground truth label set \(y_i\) and predicted label set \(\hat{y}_i\), is defined as</source>
          <target state="translated">第(i)个样本的Jaccard相似性系数,地面真实标签集(y_i\)和预测标签集(\hat{y}_i\),定义为</target>
        </trans-unit>
        <trans-unit id="1b6ab133b67b3354e1eaee98fb0609deafb4a4e0" translate="yes" xml:space="preserve">
          <source>The Johnson-Lindenstrauss bound for embedding with random projections</source>
          <target state="translated">嵌入与随机投影的Johnson-Lindenstrauss约束。</target>
        </trans-unit>
        <trans-unit id="2e420c9d276fc6531e2d5a1fd3367ac4465f247c" translate="yes" xml:space="preserve">
          <source>The KDD Cup &amp;lsquo;99 dataset was created by processing the tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset, created by MIT Lincoln Lab [1]. The artificial data (described on the &lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;dataset&amp;rsquo;s homepage&lt;/a&gt;) was generated using a closed network and hand-injected attacks to produce a large number of different types of attack with normal activity in the background. As the initial goal was to produce a large training set for supervised learning algorithms, there is a large proportion (80.1%) of abnormal data which is unrealistic in real world, and inappropriate for unsupervised anomaly detection which aims at detecting &amp;lsquo;abnormal&amp;rsquo; data, ie</source>
          <target state="translated">KDD Cup '99数据集是通过处理MIT Lincoln Lab [1]创建的1998 DARPA入侵检测系统（IDS）评估数据集的tcpdump部分而创建的。人工数据（在&lt;a href=&quot;http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html&quot;&gt;数据集的首页&lt;/a&gt;上进行了描述）是使用封闭网络生成的，并通过手动注入攻击来产生大量不同类型的攻击，而这些攻击在后台都是正常活动的。由于最初的目标是为监督学习算法生成大型训练集，因此有很大一部分（80.1％）异常数据在现实世界中是不现实的，因此不适合用于旨在检测&amp;ldquo;异常&amp;rdquo;数据的无监督异常检测，即</target>
        </trans-unit>
        <trans-unit id="42488af2a88df53e7b3301c7d5a125afdc9cd2ce" translate="yes" xml:space="preserve">
          <source>The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will be minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different initializations will end up in local minima of the KL divergence. Hence, it is sometimes useful to try different seeds and select the embedding with the lowest KL divergence.</source>
          <target state="translated">原始空间和嵌入空间的联合概率的Kullback-Leibler(KL)发散将通过梯度下降最小化。需要注意的是,KL分歧并不是凸的,也就是说,用不同的初始化进行多次重启会导致KL分歧的局部最小化。因此,有时尝试不同的种子并选择KL分歧最小的嵌入是有用的。</target>
        </trans-unit>
        <trans-unit id="5c5e824f9a2e5664a81c2abf8d8de696419cc018" translate="yes" xml:space="preserve">
          <source>The LARS model can be used using estimator &lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt;&lt;code&gt;Lars&lt;/code&gt;&lt;/a&gt;, or its low-level implementation &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">可以使用估算器&lt;a href=&quot;generated/sklearn.linear_model.lars#sklearn.linear_model.Lars&quot;&gt; &lt;code&gt;Lars&lt;/code&gt; &lt;/a&gt;或它的底层实现&lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; &lt;/a&gt;来使用LARS模型。</target>
        </trans-unit>
        <trans-unit id="5604e6549ba6e538c77a361942c228433b92a65e" translate="yes" xml:space="preserve">
          <source>The LTSA algorithm comprises three stages:</source>
          <target state="translated">LTSA算法包括三个阶段。</target>
        </trans-unit>
        <trans-unit id="5921a49032d4468242b223b9e118c35472eff0fa" translate="yes" xml:space="preserve">
          <source>The Lars algorithm provides the full path of the coefficients along the regularization parameter almost for free, thus a common operation consist of retrieving the path with function &lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt;&lt;code&gt;lars_path&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Lars算法几乎免费提供沿着正则化参数的系数的完整路径，因此常见的操作包括使用函数&lt;a href=&quot;generated/sklearn.linear_model.lars_path#sklearn.linear_model.lars_path&quot;&gt; &lt;code&gt;lars_path&lt;/code&gt; &lt;/a&gt;检索路径</target>
        </trans-unit>
        <trans-unit id="0f5aa687d48724082c940128def6bdb3f6066405" translate="yes" xml:space="preserve">
          <source>The Lasso optimization function varies for mono and multi-outputs.</source>
          <target state="translated">Lasso优化函数对于单输出和多输出来说是不同的。</target>
        </trans-unit>
        <trans-unit id="3c1b2c3b1551762e6f1da2d8a29e9acf6404c123" translate="yes" xml:space="preserve">
          <source>The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where number of features is greater than number of samples. Elsewhere prefer cd which is more numerically stable.</source>
          <target state="translated">使用的Lasso求解器:坐标下降法或LARS。对于非常稀疏的底层图形,即特征数大于样本数的情况,使用LARS。其他地方首选cd,数值上更稳定。</target>
        </trans-unit>
        <trans-unit id="2de8b3228aef7c3b031e79cb56d858268d66f64e" translate="yes" xml:space="preserve">
          <source>The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where p &amp;gt; n. Elsewhere prefer cd which is more numerically stable.</source>
          <target state="translated">要使用的套索求解器：协调下降或LARS。将LARS用于非常稀疏的基础图，其中p&amp;gt; n。在其他地方，首选CD数值更稳定。</target>
        </trans-unit>
        <trans-unit id="6c4c48b93f6a66f7991382ba54fcfe5fb18a6ebb" translate="yes" xml:space="preserve">
          <source>The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt;&lt;code&gt;ledoit_wolf&lt;/code&gt;&lt;/a&gt; function of the &lt;code&gt;sklearn.covariance&lt;/code&gt; package, or it can be otherwise obtained by fitting a &lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt;&lt;code&gt;LedoitWolf&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">可以使用 &lt;code&gt;sklearn.covariance&lt;/code&gt; 包的&lt;a href=&quot;generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf&quot;&gt; &lt;code&gt;ledoit_wolf&lt;/code&gt; &lt;/a&gt;函数在样本上计算协方差矩阵的Ledoit-Wolf估计量，或者可以通过将&lt;a href=&quot;generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf&quot;&gt; &lt;code&gt;LedoitWolf&lt;/code&gt; &lt;/a&gt;对象拟合到同一样本来获得协方差矩阵的Ledoit-Wolf估计量。</target>
        </trans-unit>
        <trans-unit id="aec2f6a4d0fb4e3ecc2ba56ea33e122b851a535b" translate="yes" xml:space="preserve">
          <source>The Linnerud dataset constains two small dataset:</source>
          <target state="translated">Linnerud数据集制约着两个小数据集。</target>
        </trans-unit>
        <trans-unit id="0460202c91e1eee28482597354a8366af4e5eb02" translate="yes" xml:space="preserve">
          <source>The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for novelty detection. Note that when LOF is used for novelty detection you MUST not use predict, decision_function and score_samples on the training set as this would lead to wrong results. You must only use these methods on new unseen data (which are not in the training set). See &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;User Guide&lt;/a&gt;: for details on the difference between outlier detection and novelty detection and how to use LOF for outlier detection.</source>
          <target state="translated">局部离群因子（LOF）算法是一种无监督的异常检测方法，可计算给定数据点相对于其邻居的局部密度偏差。它认为密度远低于其邻居的样本为异常值。本示例说明如何使用LOF进行新颖性检测。请注意，将LOF用于新颖性检测时，切勿在训练集上使用预测，decision_function和score_samples，因为这会导致错误的结果。您只能对新的看不见的数据（不在训练集中）使用这些方法。有关离群值检测和新颖性检测之间的区别以及如何使用LOF进行离群值检测的详细信息，请参见《&lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;用户指南》&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="d0e3831e71a419f2c6e3df80ad462189c4208703" translate="yes" xml:space="preserve">
          <source>The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for outlier detection which is the default use case of this estimator in scikit-learn. Note that when LOF is used for outlier detection it has no predict, decision_function and score_samples methods. See &lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;User Guide&lt;/a&gt;: for details on the difference between outlier detection and novelty detection and how to use LOF for novelty detection.</source>
          <target state="translated">局部离群因子（LOF）算法是一种无监督的异常检测方法，可计算给定数据点相对于其邻居的局部密度偏差。它认为密度远低于其邻居的样本为异常值。本示例说明如何使用LOF进行异常检测，这是scikit-learn中此估计器的默认用例。请注意，将LOF用于离群值检测时，它没有预测，decision_function和score_samples方法。有关离群值检测和新颖性检测之间的区别以及如何使用LOF进行新颖性检测的详细信息，请参见《&lt;a href=&quot;../../modules/outlier_detection#outlier-detection&quot;&gt;用户指南》&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="e334ea68d0fb9bc258e189ffac7524671f8600d5" translate="yes" xml:space="preserve">
          <source>The MLLE algorithm comprises three stages:</source>
          <target state="translated">MLLE算法包括三个阶段:</target>
        </trans-unit>
        <trans-unit id="514baa8efb372e193dd4209109d33c4d6c755f88" translate="yes" xml:space="preserve">
          <source>The Matplotlib Figure object.</source>
          <target state="translated">Matplotlib图对象。</target>
        </trans-unit>
        <trans-unit id="5adf2471ff49dbf27acc1a8b4862b530e5b52366" translate="yes" xml:space="preserve">
          <source>The Matthews correlation coefficient (+1 represents a perfect prediction, 0 an average random prediction and -1 and inverse prediction).</source>
          <target state="translated">马修斯相关系数(+1代表完美预测,0代表平均随机预测,-1代表反向预测)。</target>
        </trans-unit>
        <trans-unit id="21efcbf7188af02e8a17818fd892c631c3cc436c" translate="yes" xml:space="preserve">
          <source>The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary and multiclass classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient. [source: Wikipedia]</source>
          <target state="translated">马修斯相关系数在机器学习中用于衡量二元和多类分类的质量。它考虑到了真阳性和假阳性和假阴性,通常被认为是一个平衡的衡量标准,即使在类的大小非常不同的情况下也可以使用。MCC本质上是一个介于-1和+1之间的相关系数值。系数为+1代表完美预测,0代表平均随机预测,-1代表反向预测。该统计量也被称为phi系数。[来源:维基百科]</target>
        </trans-unit>
        <trans-unit id="d3bca24cc7fb644916020d52b90a1ddb7136c5be" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant covariance estimator is to be applied on Gaussian-distributed data, but could still be relevant on data drawn from a unimodal, symmetric distribution. It is not meant to be used with multi-modal data (the algorithm used to fit a MinCovDet object is likely to fail in such a case). One should consider projection pursuit methods to deal with multi-modal datasets.</source>
          <target state="translated">最小协方差确定因子协方差估计器适用于高斯分布的数据,但仍可适用于单模态、对称分布的数据。它并不是要用于多模态数据(用于拟合MinCovDet对象的算法很可能在这种情况下失败)。人们应该考虑使用投影追寻方法来处理多模态数据集。</target>
        </trans-unit>
        <trans-unit id="451133ac20cdcd95892f17968e1c348a4b0f4b8f" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in &lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]&lt;/a&gt;.</source>
          <target state="translated">PJRousseuw在&lt;a href=&quot;#id2&quot; id=&quot;id1&quot;&gt;[1]中&lt;/a&gt;引入了最小协方差行列式估计器（MCD）。</target>
        </trans-unit>
        <trans-unit id="d811e159b509e69af06fa8b1a6b421d8a9ea40ca" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw in [1].</source>
          <target state="translated">P.J.Rousseuw在[1]中提出了最小协方差确定估计器(MCD)。</target>
        </trans-unit>
        <trans-unit id="92f2af418b12a7a58727045874e736340d33efbd" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust estimator of a data set&amp;rsquo;s covariance introduced by P.J. Rousseeuw in &lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;[3]&lt;/a&gt;. The idea is to find a given proportion (h) of &amp;ldquo;good&amp;rdquo; observations which are not outliers and compute their empirical covariance matrix. This empirical covariance matrix is then rescaled to compensate the performed selection of observations (&amp;ldquo;consistency step&amp;rdquo;). Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set (&amp;ldquo;reweighting step&amp;rdquo;).</source>
          <target state="translated">最小协方差行列式估计器是PJ Rousseeuw在&lt;a href=&quot;#id11&quot; id=&quot;id9&quot;&gt;[3]中&lt;/a&gt;引入的数据集协方差的鲁棒估计器。这个想法是找到给定比例（h）的&amp;ldquo;良好&amp;rdquo;观测值，这些观测值不是离群值，然后计算它们的经验协方差矩阵。然后，对这个经验协方差矩阵进行重新缩放，以补偿观测值的执行选择（&amp;ldquo;一致性步骤&amp;rdquo;）。在计算了最小协方差决定因素估计量后，可以根据观测值的马氏距离对权重进行加权，从而对数据集的协方差矩阵进行加权估计（&amp;ldquo;加权步骤&amp;rdquo;）。</target>
        </trans-unit>
        <trans-unit id="26a5ec87ffc45e2d236a64ca891cc8c307910b23" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to \(\frac{n_\text{samples} - n_\text{features}-1}{2}\) outliers) estimator of covariance. The idea is to find \(\frac{n_\text{samples} + n_\text{features}+1}{2}\) observations whose empirical covariance has the smallest determinant, yielding a &amp;ldquo;pure&amp;rdquo; subset of observations from which to compute standards estimates of location and covariance. After a correction step aiming at compensating the fact that the estimates were learned from only a portion of the initial data, we end up with robust estimates of the data set location and covariance.</source>
          <target state="translated">最小协方差行列式估计器是一个可靠的高分解点（即，它可以用于估算高度污染的数据集的协方差矩阵，最高可达\（\ frac {n_ \ text {samples}-n_ \ text {features}- 1} {2} \）离群值）的协方差估计量。这个想法是找到\（\ frac {n_ \ text {samples} + n_ \ text {features} +1} {2} \）个观测值，它们的经验协方差具有最小的决定因素，从而产生&amp;ldquo;纯&amp;rdquo;观测值子集，计算位置和协方差的标准估计值。经过旨在补偿估计仅从一部分初始数据中获悉的事实的校正步骤后，我们最终得到了数据集位置和协方差的可靠估计。</target>
        </trans-unit>
        <trans-unit id="868c6b821132444289ad5893f70525225594997c" translate="yes" xml:space="preserve">
          <source>The Minimum Covariance Determinant estimator is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to \(\frac{n_\text{samples}-n_\text{features}-1}{2}\) outliers) estimator of covariance. The idea is to find \(\frac{n_\text{samples}+n_\text{features}+1}{2}\) observations whose empirical covariance has the smallest determinant, yielding a &amp;ldquo;pure&amp;rdquo; subset of observations from which to compute standards estimates of location and covariance.</source>
          <target state="translated">最小协方差行列式估计器是一个稳健的高分解点（即，它可以用于估计高度污染的数据集的协方差矩阵，最高可达\（\ frac {n_ \ text {samples} -n_ \ text {features}- 1} {2} \）离群值）的协方差估计量。这个想法是找到\（\ frac {n_ \ text {samples} + n_ \ text {features} +1} {2} \）个观测值，它们的经验协方差具有最小的行列式，从而产生&amp;ldquo;纯&amp;rdquo;观测值子集，计算位置和协方差的标准估计值。</target>
        </trans-unit>
        <trans-unit id="b2ce758bd900de6631654a32b9b8962161ffb45f" translate="yes" xml:space="preserve">
          <source>The Mutual Information is a measure of the similarity between two labels of the same data. Where \(|U_i|\) is the number of the samples in cluster \(U_i\) and \(|V_j|\) is the number of the samples in cluster \(V_j\), the Mutual Information between clusterings \(U\) and \(V\) is given as:</source>
          <target state="translated">相互信息(Mutual Information)是衡量同一数据的两个标签之间相似度的方法。其中\(|U_i|\)是指聚类中的样本数(U_i\),\(|V_j|\)是指聚类中的样本数(V_j\),聚类中的互信息(U\)和(V\)之间的互信息给出为。</target>
        </trans-unit>
        <trans-unit id="4607a569600ef039f2071e80730b55efccd8dd8d" translate="yes" xml:space="preserve">
          <source>The Nystroem method, as implemented in &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; is a general method for low-rank approximations of kernels. It achieves this by essentially subsampling the data on which the kernel is evaluated. By default &lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt;&lt;code&gt;Nystroem&lt;/code&gt;&lt;/a&gt; uses the &lt;code&gt;rbf&lt;/code&gt; kernel, but it can use any kernel function or a precomputed kernel matrix. The number of samples used - which is also the dimensionality of the features computed - is given by the parameter &lt;code&gt;n_components&lt;/code&gt;.</source>
          <target state="translated">如在&lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; 中&lt;/a&gt;实现的Nystroem方法是用于内核的低秩逼近的通用方法。它通过实质上对评估内核的数据进行二次采样来实现。默认情况下，&lt;a href=&quot;generated/sklearn.kernel_approximation.nystroem#sklearn.kernel_approximation.Nystroem&quot;&gt; &lt;code&gt;Nystroem&lt;/code&gt; &lt;/a&gt;使用 &lt;code&gt;rbf&lt;/code&gt; 内核，但它可以使用任何核函数或预先计算内核矩阵。参数 &lt;code&gt;n_components&lt;/code&gt; 给出了所使用的样本数（也是所计算特征的维数）。</target>
        </trans-unit>
        <trans-unit id="ad47de32445041d6ef99f77621d867a03cd9751f" translate="yes" xml:space="preserve">
          <source>The OAS estimator of the covariance matrix can be computed on a sample with the &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt;&lt;code&gt;oas&lt;/code&gt;&lt;/a&gt; function of the &lt;code&gt;sklearn.covariance&lt;/code&gt; package, or it can be otherwise obtained by fitting an &lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt;&lt;code&gt;OAS&lt;/code&gt;&lt;/a&gt; object to the same sample.</source>
          <target state="translated">协方差矩阵的OAS估计量可以使用 &lt;code&gt;sklearn.covariance&lt;/code&gt; 包的&lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.oas&quot;&gt; &lt;code&gt;oas&lt;/code&gt; &lt;/a&gt;函数对样本进行计算，也可以通过将&lt;a href=&quot;generated/sklearn.covariance.oas#sklearn.covariance.OAS&quot;&gt; &lt;code&gt;OAS&lt;/code&gt; &lt;/a&gt;对象拟合到同一样本来获得。</target>
        </trans-unit>
        <trans-unit id="6f45a4e82bc4c1b489f40318ecca4028acd0f46e" translate="yes" xml:space="preserve">
          <source>The One-Class SVM has been introduced by Sch&amp;ouml;lkopf et al. for that purpose and implemented in the &lt;a href=&quot;svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; module in the &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;svm.OneClassSVM&lt;/code&gt;&lt;/a&gt; object. It requires the choice of a kernel and a scalar parameter to define a frontier. The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter. This is the default in the scikit-learn implementation. The \(\nu\) parameter, also known as the margin of the One-Class SVM, corresponds to the probability of finding a new, but regular, observation outside the frontier.</source>
          <target state="translated">一类SVM已由Sch&amp;ouml;lkopf等人介绍。为此目的，并在&lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;svm.OneClassSVM&lt;/code&gt; &lt;/a&gt;对象的&lt;a href=&quot;svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt;模块中实现。它需要选择内核和标量参数来定义边界。尽管没有确切的公式或算法来设置其带宽参数，但通常会选择RBF内核。这是scikit-learn实现中的默认设置。 \（\ nu \）参数，也称为一类SVM的边距，对应于在边界之外找到新的但有规律的观测值的概率。</target>
        </trans-unit>
        <trans-unit id="ce00ae4f245475e7026810e89464783863843edd" translate="yes" xml:space="preserve">
          <source>The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction.</source>
          <target state="translated">PCA做的是无监督的维度降低,而逻辑回归做的是预测。</target>
        </trans-unit>
        <trans-unit id="33c279bdcb922f55f225b113a682e1a5cf28cc6d" translate="yes" xml:space="preserve">
          <source>The RBF kernel is a stationary kernel. It is also known as the &amp;ldquo;squared exponential&amp;rdquo; kernel. It is parameterized by a length-scale parameter length_scale&amp;gt;0, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel). The kernel is given by:</source>
          <target state="translated">RBF内核是固定内核。它也被称为&amp;ldquo;平方指数&amp;rdquo;内核。它由长度比例参数length_scale&amp;gt; 0进行参数化，该参数可以是标量（内核的各向同性变体），也可以是维数与输入X相同的向量（内核的各向异性变体）。内核由：</target>
        </trans-unit>
        <trans-unit id="f47997c4010f2e20accd04690970d6547a71bfc2" translate="yes" xml:space="preserve">
          <source>The RBF kernel will produce a fully connected graph which is represented in memory by a dense matrix. This matrix may be very large and combined with the cost of performing a full matrix multiplication calculation for each iteration of the algorithm can lead to prohibitively long running times. On the other hand, the KNN kernel will produce a much more memory-friendly sparse matrix which can drastically reduce running times.</source>
          <target state="translated">RBF内核将产生一个完全连接的图形,这个图形在内存中用一个密集的矩阵来表示,这个矩阵可能非常大,再加上每次迭代时执行全矩阵乘法计算的成本,会导致运行时间过长。这个矩阵可能会非常大,再加上每次迭代算法时进行全矩阵乘法计算的成本,会导致运行时间过长。另一方面,KNN内核将产生一个对内存更友好的稀疏矩阵,这可以大大减少运行时间。</target>
        </trans-unit>
        <trans-unit id="512de356729b7d551b43d5d76bd8681dea941e19" translate="yes" xml:space="preserve">
          <source>The RBM tries to maximize the likelihood of the data using a particular graphical model. The parameter learning algorithm used (&lt;a href=&quot;#sml&quot;&gt;Stochastic Maximum Likelihood&lt;/a&gt;) prevents the representations from straying far from the input data, which makes them capture interesting regularities, but makes the model less useful for small datasets, and usually not useful for density estimation.</source>
          <target state="translated">RBM尝试使用特定的图形模型来最大化数据的可能性。使用的参数学习算法（&lt;a href=&quot;#sml&quot;&gt;随机最大似然&lt;/a&gt;）可防止表示偏离输入数据，从而使它们捕获有趣的规律性，但使模型对小型数据集的用处不大，通常对密度估计不起作用。</target>
        </trans-unit>
        <trans-unit id="95d48012cfd5cdfda48a80ad45de8292db32665c" translate="yes" xml:space="preserve">
          <source>The R^2 score or ndarray of scores if &amp;lsquo;multioutput&amp;rsquo; is &amp;lsquo;raw_values&amp;rsquo;.</source>
          <target state="translated">如果'multioutput'为'raw_values'，则为R ^ 2分数或分数的ndarray。</target>
        </trans-unit>
        <trans-unit id="57cf85e67b74d4b36ce4e00a60cc2afb37409d42" translate="yes" xml:space="preserve">
          <source>The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.</source>
          <target state="translated">兰德指数通过考虑所有的样本对,计算两个聚类之间的相似度测量,并计算在预测聚类和真实聚类中分配在相同或不同聚类中的对。</target>
        </trans-unit>
        <trans-unit id="21d3f8892ca41bf337636e90759152be22d9788c" translate="yes" xml:space="preserve">
          <source>The RandomTreesEmbedding, from the &lt;a href=&quot;../../modules/classes#module-sklearn.ensemble&quot;&gt;&lt;code&gt;sklearn.ensemble&lt;/code&gt;&lt;/a&gt; module, is not technically a manifold embedding method, as it learn a high-dimensional representation on which we apply a dimensionality reduction method. However, it is often useful to cast a dataset into a representation in which the classes are linearly-separable.</source>
          <target state="translated">从&lt;a href=&quot;../../modules/classes#module-sklearn.ensemble&quot;&gt; &lt;code&gt;sklearn.ensemble&lt;/code&gt; &lt;/a&gt;模块中获取的RandomTreesEmbedding从技术上讲不是多种多样的嵌入方法，因为它学习了我们应用降维方法的高维表示形式。但是，将数据集转换为类可线性分离的表示形式通常很有用。</target>
        </trans-unit>
        <trans-unit id="ddaea903844387569c2d558e504bb67163af0e53" translate="yes" xml:space="preserve">
          <source>The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length-scales. It is parameterized by a length-scale parameter length_scale&amp;gt;0 and a scale mixture parameter alpha&amp;gt;0. Only the isotropic variant where length_scale is a scalar is supported at the moment. The kernel given by:</source>
          <target state="translated">可以将RationalQuadratic内核视为具有不同特征长度尺度的RBF内核的尺度混合（无穷大）。它由长度比例参数length_scale&amp;gt; 0和比例混合参数alpha&amp;gt; 0参数化。目前，仅支持length_scale为标量的各向同性变体。内核由：</target>
        </trans-unit>
        <trans-unit id="66cebb865fc92b18216814d7cca47981b6cb6b41" translate="yes" xml:space="preserve">
          <source>The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a multidimensional scaling algorithm which minimizes an objective function (the &lt;em&gt;stress&lt;/em&gt;) using a majorization technique. Stress majorization, also known as the Guttman Transform, guarantees a monotone convergence of stress, and is more powerful than traditional techniques such as gradient descent.</source>
          <target state="translated">SMACOF（通过简化包含多项式的函数进行缩放）算法是一种多维缩放算法，它使用最大化技术来最小化目标函数（&lt;em&gt;应力&lt;/em&gt;）。应力集中化也称为Guttman变换，可确保应力的单调收敛，并且比诸如梯度下降之类的传统技术更强大。</target>
        </trans-unit>
        <trans-unit id="e2c4971923aff3f7fe2f8678fb678a8a1fe12716" translate="yes" xml:space="preserve">
          <source>The SMACOF algorithm for metric MDS can summarized by the following steps:</source>
          <target state="translated">度量MDS的SMACOF算法可以概括为以下几个步骤。</target>
        </trans-unit>
        <trans-unit id="9991accadfafce42fd0097ac5bf1452a92166503" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient &lt;em&gt;s&lt;/em&gt; for a single sample is then given as:</source>
          <target state="translated">轮廓系数&lt;em&gt;&amp;scaron;&lt;/em&gt;单个样品然后被给定为：</target>
        </trans-unit>
        <trans-unit id="977018d27e576be441891586d29895a67820e9bb" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample.</source>
          <target state="translated">一组样本的轮廓系数是以每个样本的轮廓系数的平均值给出的。</target>
        </trans-unit>
        <trans-unit id="a8aa62253fcb64807b1a0f2a014811a76f4c09cc" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is a measure of how well samples are clustered with samples that are similar to themselves. Clustering models with a high Silhouette Coefficient are said to be dense, where samples in the same cluster are similar to each other, and well separated, where samples in different clusters are not very similar to each other.</source>
          <target state="translated">轮廓系数是衡量样本与自身相似的样本聚类程度的指标。具有高轮廓系数的聚类模型被认为是致密的,即同一聚类中的样本彼此相似,而分离度高,即不同聚类中的样本彼此不是很相似。</target>
        </trans-unit>
        <trans-unit id="5da8604230381cdf66d3aec27ea08ace466e606c" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is calculated using the mean intra-cluster distance (&lt;code&gt;a&lt;/code&gt;) and the mean nearest-cluster distance (&lt;code&gt;b&lt;/code&gt;) for each sample. The Silhouette Coefficient for a sample is &lt;code&gt;(b - a) / max(a,
b)&lt;/code&gt;. Note that Silhouette Coefficient is only defined if number of labels is 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</source>
          <target state="translated">使用每个样本的平均集群内距离（ &lt;code&gt;a&lt;/code&gt; ）和平均最近集群距离（ &lt;code&gt;b&lt;/code&gt; ）计算出Silhouette系数。样本的轮廓系数为 &lt;code&gt;(b - a) / max(a, b)&lt;/code&gt; 。请注意，仅当标签数为2 &amp;lt;= n_labels &amp;lt;= n_samples-1时，才定义轮廓系数。</target>
        </trans-unit>
        <trans-unit id="4273a368cac49099b8caa7c819b214a9d85c0ff4" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is calculated using the mean intra-cluster distance (&lt;code&gt;a&lt;/code&gt;) and the mean nearest-cluster distance (&lt;code&gt;b&lt;/code&gt;) for each sample. The Silhouette Coefficient for a sample is &lt;code&gt;(b - a) / max(a,
b)&lt;/code&gt;. To clarify, &lt;code&gt;b&lt;/code&gt; is the distance between a sample and the nearest cluster that the sample is not a part of. Note that Silhouette Coefficient is only defined if number of labels is 2 &amp;lt;= n_labels &amp;lt;= n_samples - 1.</source>
          <target state="translated">使用每个样本的平均集群内距离（ &lt;code&gt;a&lt;/code&gt; ）和平均最近集群距离（ &lt;code&gt;b&lt;/code&gt; ）计算出Silhouette系数。样本的轮廓系数为 &lt;code&gt;(b - a) / max(a, b)&lt;/code&gt; 。为了明确起见， &lt;code&gt;b&lt;/code&gt; 是样本与该样本不属于的最近群集之间的距离。请注意，仅当标签数为2 &amp;lt;= n_labels &amp;lt;= n_samples-1时，才定义轮廓系数。</target>
        </trans-unit>
        <trans-unit id="74a577cf3f9facf0caa36a3cd46ce91a25197ef1" translate="yes" xml:space="preserve">
          <source>The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</source>
          <target state="translated">凸簇的轮廓系数一般比其他概念的簇要高,比如像通过DBSCAN得到的密度型簇。</target>
        </trans-unit>
        <trans-unit id="b08373a116f8a28b8f59b4a939f8e86bc18a285c" translate="yes" xml:space="preserve">
          <source>The Spearman correlation coefficient is estimated from the data, and the sign of the resulting estimate is used as the result.</source>
          <target state="translated">从数据中估计出Spearman相关系数,并将所得估计值的符号作为结果。</target>
        </trans-unit>
        <trans-unit id="1aca461bec942a3ea49c28ca350e80c03610ed92" translate="yes" xml:space="preserve">
          <source>The Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages:</source>
          <target state="translated">光谱嵌入(拉普拉斯特征图)算法包括三个阶段。</target>
        </trans-unit>
        <trans-unit id="530048073ab2f0fd4beae1a41150629fce3bbc04" translate="yes" xml:space="preserve">
          <source>The TF-IDF vectorized posts form a word frequency matrix, which is then biclustered using Dhillon&amp;rsquo;s Spectral Co-Clustering algorithm. The resulting document-word biclusters indicate subsets words used more often in those subsets documents.</source>
          <target state="translated">TF-IDF矢量化后的帖子形成一个词频矩阵，然后使用Dhillon的&amp;ldquo;频谱共聚&amp;rdquo;算法对其进行二聚。产生的文档词双词条表示在那些子集文档中使用频率更高的子集词。</target>
        </trans-unit>
        <trans-unit id="a51b4bd7337a8a43bf76bb00c70355d636e98cf2" translate="yes" xml:space="preserve">
          <source>The V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean &lt;a href=&quot;#b2011&quot; id=&quot;id15&quot;&gt;[B2011]&lt;/a&gt;.</source>
          <target state="translated">V度量实际上等效于上面讨论的互信息（NMI），聚合函数是算术平均值&lt;a href=&quot;#b2011&quot; id=&quot;id15&quot;&gt;[B2011]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="28e22a992327910c0281c7997fd0381a055b2da5" translate="yes" xml:space="preserve">
          <source>The V-measure is the harmonic mean between homogeneity and completeness:</source>
          <target state="translated">V量是同质性和完整性之间的谐波平均值。</target>
        </trans-unit>
        <trans-unit id="9d831d6fbe54cab7c78889ea027eda02af846327" translate="yes" xml:space="preserve">
          <source>The Yeo-Johnson transform is given by:</source>
          <target state="translated">Yeo-Johnson变换由:</target>
        </trans-unit>
        <trans-unit id="dd310b3ef8cade9b7b44463cf24d9960d1a07902" translate="yes" xml:space="preserve">
          <source>The \(\ell = \lceil \log_2 k \rceil\) singular vectors, starting from the second, provide the desired partitioning information. They are used to form the matrix \(Z\):</source>
          <target state="translated">从第二个开始,奇异向量提供了所需的分区信息。它们被用来形成矩阵(Z)。</target>
        </trans-unit>
        <trans-unit id="6bae7eee8e57958cb7a015c09e85982ab959fe35" translate="yes" xml:space="preserve">
          <source>The \(k\)-neighbors classification in &lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt;&lt;code&gt;KNeighborsClassifier&lt;/code&gt;&lt;/a&gt; is the most commonly used technique. The optimal choice of the value \(k\) is highly data-dependent: in general a larger \(k\) suppresses the effects of noise, but makes the classification boundaries less distinct.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.neighbors.kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier&quot;&gt; &lt;code&gt;KNeighborsClassifier&lt;/code&gt; 中&lt;/a&gt;的\（k \）-邻居分类是最常用的技术。值\（k \）的最佳选择高度依赖于数据：通常，较大的\（k \）会抑制噪声的影响，但会使分类边界不那么明显。</target>
        </trans-unit>
        <trans-unit id="7ea48a47287b56e3f66c9ec81d51c291e8fedf59" translate="yes" xml:space="preserve">
          <source>The above vectorization scheme is simple but the fact that it holds an &lt;strong&gt;in- memory mapping from the string tokens to the integer feature indices&lt;/strong&gt; (the &lt;code&gt;vocabulary_&lt;/code&gt; attribute) causes several &lt;strong&gt;problems when dealing with large datasets&lt;/strong&gt;:</source>
          <target state="translated">上面的矢量化方案很简单，但是它拥有&lt;strong&gt;从字符串标记到整数特征索引&lt;/strong&gt;（ &lt;code&gt;vocabulary_&lt;/code&gt; 属性）的&lt;strong&gt;内存映射，&lt;/strong&gt;这一事实&lt;strong&gt;在处理大型数据集时&lt;/strong&gt;会引起一些&lt;strong&gt;问题&lt;/strong&gt;：</target>
        </trans-unit>
        <trans-unit id="58279b870becff96bab0fc34e17d7f045af03b34" translate="yes" xml:space="preserve">
          <source>The abstract base class for all kernels is &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt;. Kernel implements a similar interface as &lt;code&gt;Estimator&lt;/code&gt;, providing the methods &lt;code&gt;get_params()&lt;/code&gt;, &lt;code&gt;set_params()&lt;/code&gt;, and &lt;code&gt;clone()&lt;/code&gt;. This allows setting kernel values also via meta-estimators such as &lt;code&gt;Pipeline&lt;/code&gt; or &lt;code&gt;GridSearch&lt;/code&gt;. Note that due to the nested structure of kernels (by applying kernel operators, see below), the names of kernel parameters might become relatively complicated. In general, for a binary kernel operator, parameters of the left operand are prefixed with &lt;code&gt;k1__&lt;/code&gt; and parameters of the right operand with &lt;code&gt;k2__&lt;/code&gt;. An additional convenience method is &lt;code&gt;clone_with_theta(theta)&lt;/code&gt;, which returns a cloned version of the kernel but with the hyperparameters set to &lt;code&gt;theta&lt;/code&gt;. An illustrative example:</source>
          <target state="translated">所有内核的抽象基类是&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt;。内核实现了与 &lt;code&gt;Estimator&lt;/code&gt; 类似的接口，提供了 &lt;code&gt;get_params()&lt;/code&gt; ， &lt;code&gt;set_params()&lt;/code&gt; 和 &lt;code&gt;clone()&lt;/code&gt; 方法。这还允许通过元估计器（例如 &lt;code&gt;Pipeline&lt;/code&gt; 或 &lt;code&gt;GridSearch&lt;/code&gt; )设置内核值。注意，由于内核的嵌套结构（通过应用内核运算符，请参见下文），内核参数的名称可能会变得相对复杂。一般来说，对于一个二进制内核运算符，左操作数的参数都带有前缀 &lt;code&gt;k1__&lt;/code&gt; 和正确操作的参数 &lt;code&gt;k2__&lt;/code&gt; 。另一个方便的方法是 &lt;code&gt;clone_with_theta(theta)&lt;/code&gt; ，它返回内核的克隆版本，但是超参数设置为 &lt;code&gt;theta&lt;/code&gt; 。一个示例：</target>
        </trans-unit>
        <trans-unit id="d6dfb2a865fc7561c9990f60831354c8c2b2acf1" translate="yes" xml:space="preserve">
          <source>The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: &amp;ldquo;Robust Linear Programming Discrimination of Two Linearly Inseparable Sets&amp;rdquo;, Optimization Methods and Software 1, 1992, 23-34].</source>
          <target state="translated">用于获得3维空间中分离平面的实际线性程序的描述如下：[KP Bennett和OL Mangasarian：&amp;ldquo;两个线性不可分集合的鲁棒线性程序判别&amp;rdquo;，优化方法和软件1，1992，23- 34]。</target>
        </trans-unit>
        <trans-unit id="5c16c787f9e7f87c21ce4f86533b13082a6022ce" translate="yes" xml:space="preserve">
          <source>The actual number of iteration performed by the solver. Only returned if &lt;code&gt;return_n_iter&lt;/code&gt; is True.</source>
          <target state="translated">求解器执行的实际迭代次数。仅在 &lt;code&gt;return_n_iter&lt;/code&gt; 为True时返回。</target>
        </trans-unit>
        <trans-unit id="c6a597b70b1c3dba7fc91a33b2b458a1718ae376" translate="yes" xml:space="preserve">
          <source>The actual number of iterations to reach the stopping criterion.</source>
          <target state="translated">达到停止标准的实际迭代次数。</target>
        </trans-unit>
        <trans-unit id="be3e6ed927427ff958a3dc691ec6f2ca38b153e8" translate="yes" xml:space="preserve">
          <source>The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over every binary fit.</source>
          <target state="translated">达到停止标准的实际迭代次数。對于多類別的配合,它是每個二元配合的最大值。</target>
        </trans-unit>
        <trans-unit id="905122fa4ae495ec172d69201d288d9cbd8eb107" translate="yes" xml:space="preserve">
          <source>The actual number of neighbors used for &lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt;&lt;code&gt;kneighbors&lt;/code&gt;&lt;/a&gt; queries.</source>
          <target state="translated">用于&lt;a href=&quot;#sklearn.neighbors.LocalOutlierFactor.kneighbors&quot;&gt; &lt;code&gt;kneighbors&lt;/code&gt; &lt;/a&gt;查询的邻居的实际数量。</target>
        </trans-unit>
        <trans-unit id="99f948dd8ff8ebdf8ccc1268f27f992c4d54a1e0" translate="yes" xml:space="preserve">
          <source>The actual number of samples</source>
          <target state="translated">实际样本数</target>
        </trans-unit>
        <trans-unit id="257683b273bf01e4ad40af7f18f19c426c837478" translate="yes" xml:space="preserve">
          <source>The additive chi squared kernel as used here is given by</source>
          <target state="translated">这里使用的加法齐次方核由下列公式给出</target>
        </trans-unit>
        <trans-unit id="721e722946fa16cdc1d4e80a122a59df383d3b35" translate="yes" xml:space="preserve">
          <source>The additive chi squared kernel is a kernel on histograms, often used in computer vision.</source>
          <target state="translated">加性齐次方核是一种关于直方图的核,常用于计算机视觉中。</target>
        </trans-unit>
        <trans-unit id="509930cd9616089e31b59f87837fde9a2850682a" translate="yes" xml:space="preserve">
          <source>The additive version of this kernel</source>
          <target state="translated">这个内核的加法版本</target>
        </trans-unit>
        <trans-unit id="479259eedb54024948ef963b0d114b00fbb05f1b" translate="yes" xml:space="preserve">
          <source>The adjacency matrix is used to compute a normalized graph Laplacian whose spectrum (especially the eigenvectors associated to the smallest eigenvalues) has an interpretation in terms of minimal number of cuts necessary to split the graph into comparably sized components.</source>
          <target state="translated">邻接矩阵用于计算一个归一化的图形Laplacian,其频谱(特别是与最小特征值相关的特征向量)以最小的切割次数来解释,以将图形分割成大小相当的成分。</target>
        </trans-unit>
        <trans-unit id="b4f6e52ff52334d95b6f16934aecd26800758ca5" translate="yes" xml:space="preserve">
          <source>The adjacency matrix of the graph to embed.</source>
          <target state="translated">要嵌入的图形的邻接矩阵。</target>
        </trans-unit>
        <trans-unit id="f8c06d6c9bebb104301c962acd25687eac4074b4" translate="yes" xml:space="preserve">
          <source>The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation).</source>
          <target state="translated">因此,调整后的Rand指数可以保证随机标签的值接近0.0,与聚类和样本数量无关,而当聚类相同时(最多换算),Rand指数的值正好为1.0。</target>
        </trans-unit>
        <trans-unit id="83e2303d70450b875b04241e83f4a4153932e0a9" translate="yes" xml:space="preserve">
          <source>The advantage of using approximate explicit feature maps compared to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_trick&quot;&gt;kernel trick&lt;/a&gt;, which makes use of feature maps implicitly, is that explicit mappings can be better suited for online learning and can significantly reduce the cost of learning with very large datasets. Standard kernelized SVMs do not scale well to large datasets, but using an approximate kernel map it is possible to use much more efficient linear SVMs. In particular, the combination of kernel map approximations with &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; can make non-linear learning on large datasets possible.</source>
          <target state="translated">与隐式使用特征图的&lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_trick&quot;&gt;内核技巧&lt;/a&gt;相比，使用近似显式特征图的优势在于显式映射可以更好地适合在线学习，并且可以显着降低使用超大型数据集的学习成本。标准的内核化SVM不能很好地扩展到大型数据集，但是使用近似的内核图，可以使用效率更高的线性SVM。特别是，将内核映射近似与&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;结合使用可以在大型数据集上进行非线性学习。</target>
        </trans-unit>
        <trans-unit id="e8cf8d3d0c8dfd7b6a3c8351c56803f2fdf0d2d8" translate="yes" xml:space="preserve">
          <source>The advantages of Bayesian Regression are:</source>
          <target state="translated">贝叶斯回归法的优点是:。</target>
        </trans-unit>
        <trans-unit id="d7269fefe21b18a1d9b1e8a11da2f35edf3e36ec" translate="yes" xml:space="preserve">
          <source>The advantages of GBRT are:</source>
          <target state="translated">GBRT的优点是:</target>
        </trans-unit>
        <trans-unit id="6bdcff89c23af609c3f964058bbddc38e45558f8" translate="yes" xml:space="preserve">
          <source>The advantages of Gaussian processes are:</source>
          <target state="translated">高斯过程的优点是:</target>
        </trans-unit>
        <trans-unit id="c632bf8abb99b63a04554183dcc32e66bb0cf004" translate="yes" xml:space="preserve">
          <source>The advantages of LARS are:</source>
          <target state="translated">LARS的优点是:</target>
        </trans-unit>
        <trans-unit id="54923c815d40d3138e9fbe2ad92c97dd60ef1b2e" translate="yes" xml:space="preserve">
          <source>The advantages of Multi-layer Perceptron are:</source>
          <target state="translated">多层感知器的优势在于:1.</target>
        </trans-unit>
        <trans-unit id="1dabd14761114fadc83f1114e989744a341117db" translate="yes" xml:space="preserve">
          <source>The advantages of Stochastic Gradient Descent are:</source>
          <target state="translated">随机梯度下降法的优点是。</target>
        </trans-unit>
        <trans-unit id="a12cea148a9927c20c87185a7ad428a4bd953760" translate="yes" xml:space="preserve">
          <source>The advantages of support vector machines are:</source>
          <target state="translated">支持向量机的优点是:</target>
        </trans-unit>
        <trans-unit id="b3ce019dea8f1fa7178760b3ed7e1bed715f9894" translate="yes" xml:space="preserve">
          <source>The affinity matrix describing the relationship of the samples to embed. &lt;strong&gt;Must be symmetric&lt;/strong&gt;.</source>
          <target state="translated">亲和度矩阵描述要嵌入的样本之间的关系。&lt;strong&gt;必须对称&lt;/strong&gt;。</target>
        </trans-unit>
        <trans-unit id="f8f51936a34abb4d8304c65310991fbeb781a2b1" translate="yes" xml:space="preserve">
          <source>The algorithm automatically sets the number of clusters, instead of relying on a parameter &lt;code&gt;bandwidth&lt;/code&gt;, which dictates the size of the region to search through. This parameter can be set manually, but can be estimated using the provided &lt;code&gt;estimate_bandwidth&lt;/code&gt; function, which is called if the bandwidth is not set.</source>
          <target state="translated">该算法自动地设定的，而不是依赖于参数的簇的数量， &lt;code&gt;bandwidth&lt;/code&gt; ，该区域的大小，以搜寻通过该使然。该参数可以手动设置，但是可以使用提供的 &lt;code&gt;estimate_bandwidth&lt;/code&gt; 函数进行估计，如果未设置带宽，则调用该函数。</target>
        </trans-unit>
        <trans-unit id="999ae100a463cc0ff60234ca1a1687522770fa40" translate="yes" xml:space="preserve">
          <source>The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and efficiency. Since the number of least square solutions is &amp;ldquo;n_samples choose n_subsamples&amp;rdquo;, it can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets are chosen randomly. In a final step, the spatial median (or L1 median) is calculated of all least square solutions.</source>
          <target state="translated">该算法以X为样本大小n_subsample的子集计算最小二乘解。在特征和样本数量之间的n_subsample的任何值都会导致估计器的健壮性和效率之间折衷。由于最小二乘解的数量为&amp;ldquo; n_samples选择n_subsamples&amp;rdquo;，因此其数量可能非常大，因此可能受到max_subpopulation的限制。如果达到此限制，则随机选择子集。在最后一步中，将计算所有最小二乘解的空间中位数（或L1中位数）。</target>
        </trans-unit>
        <trans-unit id="19207c781cd295df6565aab588711bf1a9323bef" translate="yes" xml:space="preserve">
          <source>The algorithm can also be understood through the concept of &lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;Voronoi diagrams&lt;/a&gt;. First the Voronoi diagram of the points is calculated using the current centroids. Each segment in the Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated to the mean of each segment. The algorithm then repeats this until a stopping criterion is fulfilled. Usually, the algorithm stops when the relative decrease in the objective function between iterations is less than the given tolerance value. This is not the case in this implementation: iteration stops when centroids move less than the tolerance.</source>
          <target state="translated">该算法也可以通过&lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;Voronoi图&lt;/a&gt;的概念来理解。首先，使用当前质心计算点的Voronoi图。Voronoi图中的每个段都成为一个单独的群集。其次，质心更新为每个段的均值。然后，算法重复此操作，直到满足停止条件为止。通常，当迭代之间目标函数的相对减小小于给定的容差值时，算法将停止。在此实现中不是这种情况：当质心的运动小于公差时，迭代将停止。</target>
        </trans-unit>
        <trans-unit id="ac387733191797f4111e67a021bb9641f7899ac2" translate="yes" xml:space="preserve">
          <source>The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R &lt;code&gt;glasso&lt;/code&gt; package.</source>
          <target state="translated">用于解决此问题的算法是GLasso算法，来自Friedman 2008 Biostatistics论文。它与R &lt;code&gt;glasso&lt;/code&gt; 软件包中的算法相同。</target>
        </trans-unit>
        <trans-unit id="2d7b95845283c4d98cc4167d4e0748ee6c69e84f" translate="yes" xml:space="preserve">
          <source>The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. &amp;ldquo;Algorithms for computing the sample variance: Analysis and recommendations.&amp;rdquo; The American Statistician 37.3 (1983): 242-247:</source>
          <target state="translated">Chan，Tony F.，Gene H. Golub和Randall J. LeVeque的公式1.5a，b中给出了增量均值和std的算法。&amp;ldquo;计算样本方差的算法：分析和建议。&amp;rdquo; 美国统计学家37.3（1983）：242-247：</target>
        </trans-unit>
        <trans-unit id="c4e655f6aa7cf22a58a123dc60b12c815d9f7df3" translate="yes" xml:space="preserve">
          <source>The algorithm is adapted from Guyon [1] and was designed to generate the &amp;ldquo;Madelon&amp;rdquo; dataset.</source>
          <target state="translated">该算法改编自Guyon [1]，旨在生成&amp;ldquo; Madelon&amp;rdquo;数据集。</target>
        </trans-unit>
        <trans-unit id="da28c971693f926f3030184039a925bcc2b1ca76" translate="yes" xml:space="preserve">
          <source>The algorithm is from Marsland [1].</source>
          <target state="translated">该算法来自Marsland[1]。</target>
        </trans-unit>
        <trans-unit id="94c105f8aebba6a360eae1c93878348f10f58fca" translate="yes" xml:space="preserve">
          <source>The algorithm is not highly scalable, as it requires multiple nearest neighbor searches during the execution of the algorithm. The algorithm is guaranteed to converge, however the algorithm will stop iterating when the change in centroids is small.</source>
          <target state="translated">该算法的可扩展性不高,因为在算法执行过程中需要进行多次最近邻搜索。该算法可以保证收敛,但是当中心点变化较小时,该算法将停止迭代。</target>
        </trans-unit>
        <trans-unit id="3dc6012ab672dce7fe920dc60add16368612e345" translate="yes" xml:space="preserve">
          <source>The algorithm is similar to forward stepwise regression, but instead of including variables at each step, the estimated parameters are increased in a direction equiangular to each one&amp;rsquo;s correlations with the residual.</source>
          <target state="translated">该算法类似于正向逐步回归，但不是在每个步骤中都包含变量，而是在与每个人的相关性与残差相关的方向上增加了估计的参数。</target>
        </trans-unit>
        <trans-unit id="0335c4d6784d824f2c45b2464a3517f723de00c5" translate="yes" xml:space="preserve">
          <source>The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error.</source>
          <target state="translated">该算法是随机的,用不同的种子多次重启会产生不同的嵌入。然而,选择误差最小的嵌入是完全合法的。</target>
        </trans-unit>
        <trans-unit id="7c7de93467b285ca3f0b63a2764ebf7e5ab511ed" translate="yes" xml:space="preserve">
          <source>The algorithm iterates between two major steps, similar to vanilla k-means. In the first step, \(b\) samples are drawn randomly from the dataset, to form a mini-batch. These are then assigned to the nearest centroid. In the second step, the centroids are updated. In contrast to k-means, this is done on a per-sample basis. For each sample in the mini-batch, the assigned centroid is updated by taking the streaming average of the sample and all previous samples assigned to that centroid. This has the effect of decreasing the rate of change for a centroid over time. These steps are performed until convergence or a predetermined number of iterations is reached.</source>
          <target state="translated">该算法在两个主要步骤之间迭代,类似于普通的k-means。第一步,从数据集中随机抽取/(b/)个样本,形成一个小批量。然后,这些样本被分配到最近的中心点。第二步,更新中心点。与k-means相比,这是在每个样本的基础上完成的。对于迷你批次中的每个样本,通过取样本的流式平均值和之前分配给该中心点的所有样本来更新分配的中心点。这具有随着时间的推移降低中心点变化率的效果。这些步骤一直进行到收敛或达到预定的迭代次数为止。</target>
        </trans-unit>
        <trans-unit id="eac8adf7ae77ba9768138371c6c71edd5a3a76a7" translate="yes" xml:space="preserve">
          <source>The algorithm partitions the rows and columns of a matrix so that a corresponding blockwise-constant checkerboard matrix provides a good approximation to the original matrix.</source>
          <target state="translated">该算法对矩阵的行和列进行分割,使相应的块状常数棋盘矩阵对原矩阵有一个很好的近似。</target>
        </trans-unit>
        <trans-unit id="36533938d0e94afa89a69e7b9609c61646ab71f5" translate="yes" xml:space="preserve">
          <source>The algorithm splits the complete input sample data into a set of inliers, which may be subject to noise, and outliers, which are e.g. caused by erroneous measurements or invalid hypotheses about the data. The resulting model is then estimated only from the determined inliers.</source>
          <target state="translated">该算法将完整的输入样本数据分割成一组离群值(可能受噪声影响)和离群值(例如,由错误的测量或关于数据的无效假设引起)。然后,只根据确定的离群值估计所产生的模型。</target>
        </trans-unit>
        <trans-unit id="76e87cc3ec2713663d8449fef17b4e0c4101c305" translate="yes" xml:space="preserve">
          <source>The algorithm stops when it reaches a preset maximum number of iterations; or when the improvement in loss is below a certain, small number.</source>
          <target state="translated">当达到预设的最大迭代次数时,算法就会停止;或者当损失的改善低于某个小数时,算法就会停止。</target>
        </trans-unit>
        <trans-unit id="841b170fb7ba2429816cb7c51af4dbae57b471af" translate="yes" xml:space="preserve">
          <source>The algorithm supports sample weights, which can be given by a parameter &lt;code&gt;sample_weight&lt;/code&gt;. This allows to assign more weight to some samples when computing cluster centers and values of inertia. For example, assigning a weight of 2 to a sample is equivalent to adding a duplicate of that sample to the dataset \(X\).</source>
          <target state="translated">该算法支持样本权重，可以通过参数 &lt;code&gt;sample_weight&lt;/code&gt; 给出。在计算聚类中心和惯性值时，这可以为某些样本分配更多权重。例如，为一个样本分配2的权重等效于将该样本的重复项添加到数据集\（X \）中。</target>
        </trans-unit>
        <trans-unit id="a8238e48cf484817aaf23aa2adc5e4a70681d78f" translate="yes" xml:space="preserve">
          <source>The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details.</source>
          <target state="translated">NearestNeighbors模块用来计算点向距离和寻找最近邻居的算法。详情请参见NearestNeighbors模块文档。</target>
        </trans-unit>
        <trans-unit id="3602b334476c395c578aab7a7d09d75f92100f28" translate="yes" xml:space="preserve">
          <source>The algorithm treats the input data matrix as a bipartite graph: the rows and columns of the matrix correspond to the two sets of vertices, and each entry corresponds to an edge between a row and a column. The algorithm approximates the normalized cut of this graph to find heavy subgraphs.</source>
          <target state="translated">该算法将输入的数据矩阵看作是一个二元图:矩阵的行和列对应两组顶点,每个条目对应行和列之间的一条边。该算法对该图进行近似的归一化切割,以找到重子图。</target>
        </trans-unit>
        <trans-unit id="e4086364ea17b4442e0265a08361358aa304a247" translate="yes" xml:space="preserve">
          <source>The algorithm used to estimate the weights. It will be called n_components times, i.e. once for each iteration of the outer loop.</source>
          <target state="translated">用于估计权重的算法。它将被调用n_components次,即外循环每次迭代一次。</target>
        </trans-unit>
        <trans-unit id="e4e3e486ab0114bca02d02635f9410b77ca99a25" translate="yes" xml:space="preserve">
          <source>The algorithm used to fit the model is coordinate descent.</source>
          <target state="translated">用于拟合模型的算法是坐标下降法。</target>
        </trans-unit>
        <trans-unit id="17e35eb9c6004e2d60b73104c93585aacf4a2e98" translate="yes" xml:space="preserve">
          <source>The algorithmic complexity of affinity propagation is quadratic in the number of points.</source>
          <target state="translated">亲和力传播的算法复杂度在点数上是二次方的。</target>
        </trans-unit>
        <trans-unit id="4c657d81ec38ba678d89f7b7de187e376e50c0f3" translate="yes" xml:space="preserve">
          <source>The algorithms for regression and classification only differ in the concrete loss function used.</source>
          <target state="translated">回归和分类的算法只是在使用的具体损失函数上有所不同。</target>
        </trans-unit>
        <trans-unit id="50542b2bc530bb15fc1bcd67d7b073d61bea03ab" translate="yes" xml:space="preserve">
          <source>The alpha parameter in the stability selection article used to randomly scale the features. Should be between 0 and 1.</source>
          <target state="translated">稳定性选择文章中用于随机缩放特征的α参数。应该在0和1之间。</target>
        </trans-unit>
        <trans-unit id="35d70c04237d10e5da496ff67ac57fce44665490" translate="yes" xml:space="preserve">
          <source>The alpha parameter of the GraphicalLasso setting the sparsity of the model is set by internal cross-validation in the GraphicalLassoCV. As can be seen on figure 2, the grid to compute the cross-validation score is iteratively refined in the neighborhood of the maximum.</source>
          <target state="translated">GraphicalLasso设置模型稀疏度的α参数是由GraphicalLassoCV中的内部交叉验证设置的。从图2可以看出,计算交叉验证得分的网格是在最大值的附近反复细化的。</target>
        </trans-unit>
        <trans-unit id="2075fb3135d145905cc5a41db16871a4bf5168da" translate="yes" xml:space="preserve">
          <source>The alpha-quantile of the huber loss function and the quantile loss function. Only if &lt;code&gt;loss='huber'&lt;/code&gt; or &lt;code&gt;loss='quantile'&lt;/code&gt;.</source>
          <target state="translated">Huber损失函数和分位数损失函数的alpha分位数。仅当 &lt;code&gt;loss='huber'&lt;/code&gt; 或 &lt;code&gt;loss='quantile'&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="28a2d4f18d224d5f96bb30d3faa50cdca5ce2935" translate="yes" xml:space="preserve">
          <source>The alphas along the path where models are computed.</source>
          <target state="translated">沿着计算模型的路径的阿尔法。</target>
        </trans-unit>
        <trans-unit id="4390acc7061cc013783802ff89b6303810b0044d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set.</source>
          <target state="translated">数据集的污染量,即数据集中离群值的比例。</target>
        </trans-unit>
        <trans-unit id="db48eba23a1b19c738dbbe1eb3626efb11bd507f" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. If &amp;lsquo;auto&amp;rsquo;, the decision function threshold is determined as in the original paper.</source>
          <target state="translated">数据集的污染量，即数据集中异常值的比例。在拟合时用于定义决策函数的阈值时使用。如果为&amp;ldquo;自动&amp;rdquo;，则决定功能阈值与原始论文相同。</target>
        </trans-unit>
        <trans-unit id="0b69accc98489279e23dd931886f63d4aafd913d" translate="yes" xml:space="preserve">
          <source>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the decision function. If &amp;ldquo;auto&amp;rdquo;, the decision function threshold is determined as in the original paper.</source>
          <target state="translated">数据集的污染量，即数据集中异常值的比例。拟合时，用于定义决策函数的阈值。如果为&amp;ldquo;自动&amp;rdquo;，则决定功能阈值与原始纸张相同。</target>
        </trans-unit>
        <trans-unit id="057670e523b801c82d76306d2019fbc0b1e55fa7" translate="yes" xml:space="preserve">
          <source>The amount of penalization chosen by cross validation</source>
          <target state="translated">交叉验证所选择的惩罚量;</target>
        </trans-unit>
        <trans-unit id="0a8889ae5955aac97641248ca40c9031408985ae" translate="yes" xml:space="preserve">
          <source>The amount of variance explained by each of the selected components.</source>
          <target state="translated">每个选定成分所解释的方差量。</target>
        </trans-unit>
        <trans-unit id="0c35119665a5a383bf2a2ea2491d6548297b77b9" translate="yes" xml:space="preserve">
          <source>The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.</source>
          <target state="translated">输入样本的异常得分计算为森林中树木的平均异常得分。</target>
        </trans-unit>
        <trans-unit id="f5bff141bb39bd3c0bc3c7fd23ee875e735e27a8" translate="yes" xml:space="preserve">
          <source>The anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. These are considered outliers.</source>
          <target state="translated">每个样本的异常得分称为局部离群因子。它衡量一个给定样本的密度相对于其邻居的局部偏差。它是局部的,因为异常得分取决于对象相对于周围邻居的孤立程度。更准确地说,局部性是由k个最近的邻居给出的,其距离被用来估计局部密度。通过比较一个样本的局部密度和其邻居的局部密度,可以识别出密度大大低于其邻居的样本。这些样本被认为是离群值。</target>
        </trans-unit>
        <trans-unit id="9d2b05e02bf58b122225781451ec470ef3bbad43" translate="yes" xml:space="preserve">
          <source>The anomaly score of the input samples. The lower, the more abnormal.</source>
          <target state="translated">输入样本的异常得分。越低,说明越不正常。</target>
        </trans-unit>
        <trans-unit id="7ea9f6456fff678e2acc40131f68202326a3c878" translate="yes" xml:space="preserve">
          <source>The anomaly score of the input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.</source>
          <target state="translated">输入样本的异常得分。越低,越是异常。负分代表异常值,正分代表离群值。</target>
        </trans-unit>
        <trans-unit id="f8f5cba472ebac3a5205adfc98746f94cfb55c44" translate="yes" xml:space="preserve">
          <source>The approximate feature map provided by &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt;&lt;code&gt;AdditiveChi2Sampler&lt;/code&gt;&lt;/a&gt; can be combined with the approximate feature map provided by &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt; to yield an approximate feature map for the exponentiated chi squared kernel. See the &lt;a href=&quot;#vz2010&quot; id=&quot;id5&quot;&gt;[VZ2010]&lt;/a&gt; for details and &lt;a href=&quot;#vvz2010&quot; id=&quot;id6&quot;&gt;[VVZ2010]&lt;/a&gt; for combination with the &lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt;&lt;code&gt;RBFSampler&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">所提供的近似的特征映射&lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt; &lt;code&gt;AdditiveChi2Sampler&lt;/code&gt; &lt;/a&gt;可以与所提供的近似的特征图组合&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;，得到一个近似特征地图的取幂卡方内核。参见&lt;a href=&quot;#vz2010&quot; id=&quot;id5&quot;&gt;[VZ2010]&lt;/a&gt;对于细节和&lt;a href=&quot;#vvz2010&quot; id=&quot;id6&quot;&gt;[VVZ2010]&lt;/a&gt;用于与组合&lt;a href=&quot;generated/sklearn.kernel_approximation.rbfsampler#sklearn.kernel_approximation.RBFSampler&quot;&gt; &lt;code&gt;RBFSampler&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="98f8783bf76339c13180f5a0c21989e5d9843d3d" translate="yes" xml:space="preserve">
          <source>The approximate number of singular vectors required to explain most of the data by linear combinations.</source>
          <target state="translated">用线性组合解释大部分数据所需的奇异向量的大致数量。</target>
        </trans-unit>
        <trans-unit id="01540c9a0c0d75da953bc96aafe62b761a8754d9" translate="yes" xml:space="preserve">
          <source>The approximate number of singular vectors required to explain most of the input data by linear combinations. Using this kind of singular spectrum in the input allows the generator to reproduce the correlations often observed in practice.</source>
          <target state="translated">用线性组合解释大部分输入数据所需的奇异向量的大致数量。在输入中使用这种奇异频谱可以使生成器重现实践中经常观察到的相关性。</target>
        </trans-unit>
        <trans-unit id="9b1212b40ba5f9b8205d7b64e2e9c20d35b415b3" translate="yes" xml:space="preserve">
          <source>The arithmetic mean is the sum of the elements along the axis divided by the number of elements.</source>
          <target state="translated">算术平均值是沿轴线的元素之和除以元素数。</target>
        </trans-unit>
        <trans-unit id="35a07a5d9cec4bce6db2b0fe073857a7979c5b16" translate="yes" xml:space="preserve">
          <source>The array has 0.16% of non zero values.</source>
          <target state="translated">该数组有0.16%的非零值。</target>
        </trans-unit>
        <trans-unit id="449fd4c0217076d90d22633d6cb32b7300c30336" translate="yes" xml:space="preserve">
          <source>The array of (log)-density evaluations, shape = X.shape[:-1]</source>
          <target state="translated">(对数)密度评价数组,shape=X.shape[:-1]。</target>
        </trans-unit>
        <trans-unit id="d00a652e1f004fd3ddfd653b6794ad2aba8108c7" translate="yes" xml:space="preserve">
          <source>The array of log(density) evaluations.</source>
          <target state="translated">对数(密度)评估的数组。</target>
        </trans-unit>
        <trans-unit id="531360783ebbdd4b65c83458a2d186f114e4086b" translate="yes" xml:space="preserve">
          <source>The automatic estimation from Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604 by Thomas P. Minka is also compared.</source>
          <target state="translated">从自动选择维度进行PCA的自动估计。NIPS 2000:598-604,由Thomas P.Minka撰写,也进行了比较。</target>
        </trans-unit>
        <trans-unit id="37bf2d4736a8a0a6781e876ebdb656796aca8913" translate="yes" xml:space="preserve">
          <source>The available cross validation iterators are introduced in the following section.</source>
          <target state="translated">下文将介绍可用的交叉验证迭代器。</target>
        </trans-unit>
        <trans-unit id="52d681893d146fe7dad6c4424a94a534bab69431" translate="yes" xml:space="preserve">
          <source>The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration.</source>
          <target state="translated">平均复杂度由O(k n T)给出,其中n是样本数,T是迭代次数。</target>
        </trans-unit>
        <trans-unit id="d9092bb0da47977d9b43f41f12fd7dcc75d26801" translate="yes" xml:space="preserve">
          <source>The average number of labels per instance. More precisely, the number of labels per sample is drawn from a Poisson distribution with &lt;code&gt;n_labels&lt;/code&gt; as its expected value, but samples are bounded (using rejection sampling) by &lt;code&gt;n_classes&lt;/code&gt;, and must be nonzero if &lt;code&gt;allow_unlabeled&lt;/code&gt; is False.</source>
          <target state="translated">每个实例的平均标签数。更准确地说，每个样本的标签数是从泊松分布中以 &lt;code&gt;n_labels&lt;/code&gt; 作为其期望值得出的，但是样本受 &lt;code&gt;n_classes&lt;/code&gt; 限制（使用拒绝采样），并且如果 &lt;code&gt;allow_unlabeled&lt;/code&gt; 为False，则该值必须为非零。</target>
        </trans-unit>
        <trans-unit id="50521765649b7ccece3cb452f18896c14c55c5d8" translate="yes" xml:space="preserve">
          <source>The average precision score in multi-label settings</source>
          <target state="translated">多标签设置中的平均精度分数</target>
        </trans-unit>
        <trans-unit id="7f192c1d1db49dfae3574eb70a5374ae8608eb71" translate="yes" xml:space="preserve">
          <source>The averaged intercept term.</source>
          <target state="translated">平均截距项。</target>
        </trans-unit>
        <trans-unit id="5f1e3570b35bda8ded1b9651eb310bcefe0589f6" translate="yes" xml:space="preserve">
          <source>The axes with which the grid has been created or None if the grid has been given.</source>
          <target state="translated">创建网格的轴,如果给定了网格,则为 &quot;无&quot;。</target>
        </trans-unit>
        <trans-unit id="2af1a0461163ef9a917dd17f9a6c7032c07c79bb" translate="yes" xml:space="preserve">
          <source>The axis along which to impute.</source>
          <target state="translated">沿着这个轴线进行推算。</target>
        </trans-unit>
        <trans-unit id="efae1008b127cbb418b23469c7f222e2a6660b67" translate="yes" xml:space="preserve">
          <source>The bag of words representation is quite simplistic but surprisingly useful in practice.</source>
          <target state="translated">词袋的表示方法相当简单,但在实践中却出奇的有用。</target>
        </trans-unit>
        <trans-unit id="a41734ff5b2ce49bfc6f83ebb1bb324e76d37d70" translate="yes" xml:space="preserve">
          <source>The bags of words representation implies that &lt;code&gt;n_features&lt;/code&gt; is the number of distinct words in the corpus: this number is typically larger than 100,000.</source>
          <target state="translated">单词袋表示意味着 &lt;code&gt;n_features&lt;/code&gt; 是语料库中不同单词的数量：该数量通常大于100,000。</target>
        </trans-unit>
        <trans-unit id="eac13f6898f72e0eaad5c85bd1b456a5882d1a72" translate="yes" xml:space="preserve">
          <source>The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.</source>
          <target state="translated">二元和多类分类问题中的平衡精度,以处理不平衡的数据集。它被定义为在每个类上获得的召回率的平均值。</target>
        </trans-unit>
        <trans-unit id="fd188197709d9753bf6d2d7d8ebbb1b211377cd3" translate="yes" xml:space="preserve">
          <source>The bandwidth here acts as a smoothing parameter, controlling the tradeoff between bias and variance in the result. A large bandwidth leads to a very smooth (i.e. high-bias) density distribution. A small bandwidth leads to an unsmooth (i.e. high-variance) density distribution.</source>
          <target state="translated">带宽在这里作为一个平滑参数,控制结果中的偏差和方差之间的权衡。大的带宽会导致一个非常平滑(即高偏置)的密度分布,小的带宽会导致一个不平滑(即高变异)的密度分布。小的带宽会导致不平滑(即高变异)的密度分布。</target>
        </trans-unit>
        <trans-unit id="220a3cae014ca1f6f44493020405f4e460f4038d" translate="yes" xml:space="preserve">
          <source>The bandwidth of the kernel.</source>
          <target state="translated">内核的带宽。</target>
        </trans-unit>
        <trans-unit id="eaa1089b1fdc51cb43f528072a19cd3f3ec50c61" translate="yes" xml:space="preserve">
          <source>The bandwidth parameter.</source>
          <target state="translated">带宽参数。</target>
        </trans-unit>
        <trans-unit id="ca9efc037882c6303cbf300aed185a1d9a7dd7ae" translate="yes" xml:space="preserve">
          <source>The bar plot indicates the accuracy, training time (normalized) and test time (normalized) of each classifier.</source>
          <target state="translated">条形图表示每个分类器的精度、训练时间(归一化)和测试时间(归一化)。</target>
        </trans-unit>
        <trans-unit id="6ef0fbe42e5306306c086ebee574632386d4e293" translate="yes" xml:space="preserve">
          <source>The base classifier is a random forest classifier with 25 base estimators (trees). If this classifier is trained on all 800 training datapoints, it is overly confident in its predictions and thus incurs a large log-loss. Calibrating an identical classifier, which was trained on 600 datapoints, with method=&amp;rsquo;sigmoid&amp;rsquo; on the remaining 200 datapoints reduces the confidence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center. This calibration results in a lower log-loss. Note that an alternative would have been to increase the number of base estimators which would have resulted in a similar decrease in log-loss.</source>
          <target state="translated">基本分类器是具有25个基本估计量（树）的随机森林分类器。如果该分类器在所有800个训练数据点上都进行了训练，则对其预测过于自信，因此会导致大量对数损失。使用在剩余的200个数据点上使用method ='sigmoid'校准在600个数据点上训练的相同分类器，会降低预测的可信度，即，将概率矢量从单纯形的边缘移向中心。该校准导致较低的对数损失。注意，另一种选择是增加基本估计量，这将导致对数损失的类似减少。</target>
        </trans-unit>
        <trans-unit id="3f89ef1e6cb2a54f74eb0a982beea8ffd3333592" translate="yes" xml:space="preserve">
          <source>The base classifier is a random forest classifier with 25 base estimators (trees). If this classifier is trained on all 800 training datapoints, it is overly confident in its predictions and thus incurs a large log-loss. Calibrating an identical classifier, which was trained on 600 datapoints, with method=&amp;rsquo;sigmoid&amp;rsquo; on the remaining 200 datapoints reduces the confidence of the predictions, i.e., moves the probability vectors from the edges of the simplex towards the center:</source>
          <target state="translated">基本分类器是具有25个基本估计量（树）的随机森林分类器。如果该分类器在所有800个训练数据点上都进行了训练，则对其预测过于自信，因此会导致大量对数损失。使用在其余200个数据点上使用method ='sigmoid'校准在600个数据点上训练的相同分类器，会降低预测的可信度，即，将概率矢量从单纯形的边缘移向中心：</target>
        </trans-unit>
        <trans-unit id="cdab13e5017fd96224833fdcfcd75615e152e1af" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper &lt;code&gt;classes_&lt;/code&gt; and &lt;code&gt;n_classes_&lt;/code&gt; attributes. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;</source>
          <target state="translated">从中构建增强合奏的基本估计量。需要支持样本加权以及正确的 &lt;code&gt;classes_&lt;/code&gt; 和 &lt;code&gt;n_classes_&lt;/code&gt; 属性。如果为 &lt;code&gt;None&lt;/code&gt; ，则基本估计量为 &lt;code&gt;DecisionTreeClassifier(max_depth=1)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="39ba9d176a83a208ed96d60f33bec321ac051ac5" translate="yes" xml:space="preserve">
          <source>The base estimator from which the boosted ensemble is built. Support for sample weighting is required. If &lt;code&gt;None&lt;/code&gt;, then the base estimator is &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;</source>
          <target state="translated">从中构建增强合奏的基本估计量。需要支持样本加权。如果为 &lt;code&gt;None&lt;/code&gt; ，则基本估计量为 &lt;code&gt;DecisionTreeRegressor(max_depth=3)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="09e53f3d87743d060ad05fb04ae38587d00307b0" translate="yes" xml:space="preserve">
          <source>The base estimator from which the classifier chain is built.</source>
          <target state="translated">用于建立分类器链的基础估计器。</target>
        </trans-unit>
        <trans-unit id="c229b061f41bb16a20d56916c466508eaa778c17" translate="yes" xml:space="preserve">
          <source>The base estimator from which the ensemble is grown.</source>
          <target state="translated">基准估计器,从这个估计器中生长出集合。</target>
        </trans-unit>
        <trans-unit id="5066de8ea8cb5c40493cb3ffbb0e912b0ff83ff9" translate="yes" xml:space="preserve">
          <source>The base estimator from which the transformer is built. This can be both a fitted (if &lt;code&gt;prefit&lt;/code&gt; is set to True) or a non-fitted estimator. The estimator must have either a &lt;code&gt;feature_importances_&lt;/code&gt; or &lt;code&gt;coef_&lt;/code&gt; attribute after fitting.</source>
          <target state="translated">用来构建变压器的基本估算器。这可以是拟合的（如果 &lt;code&gt;prefit&lt;/code&gt; 拟合设置为True）或非拟合的估计器。拟合后，估算器必须具有 &lt;code&gt;feature_importances_&lt;/code&gt; 或 &lt;code&gt;coef_&lt;/code&gt; 属性。</target>
        </trans-unit>
        <trans-unit id="8935e197daa0e6c85ff9f9b0e235723b5ff13893" translate="yes" xml:space="preserve">
          <source>The base estimator from which the transformer is built. This is stored only when a non-fitted estimator is passed to the &lt;code&gt;SelectFromModel&lt;/code&gt;, i.e when prefit is False.</source>
          <target state="translated">用来构建变压器的基本估算器。仅当将非拟合估计量传递给 &lt;code&gt;SelectFromModel&lt;/code&gt; 时（即，当prefit为False时）才存储此字段。</target>
        </trans-unit>
        <trans-unit id="8062156aa44a8a655e18edffbba0a99e0e098c8a" translate="yes" xml:space="preserve">
          <source>The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.</source>
          <target state="translated">在数据集的随机子集上拟合的基础估计器,如果无,则基础估计器是决策树。如果无,则基础估计器是决策树。</target>
        </trans-unit>
        <trans-unit id="d80c39c018abc387662fda22711c8b7bb4707717" translate="yes" xml:space="preserve">
          <source>The base kernel</source>
          <target state="translated">基本内核</target>
        </trans-unit>
        <trans-unit id="1b7bc12e1d1eb0bb8e39e15748d7f9b27d93ef1a" translate="yes" xml:space="preserve">
          <source>The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the &lt;code&gt;weights&lt;/code&gt; keyword. The default value, &lt;code&gt;weights = 'uniform'&lt;/code&gt;, assigns uniform weights to each neighbor. &lt;code&gt;weights = 'distance'&lt;/code&gt; assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights.</source>
          <target state="translated">基本的最近邻居分类使用统一的权重：即，分配给查询点的值是根据最近邻居的简单多数票计算得出的。在某些情况下，最好对邻居加权，以使较近的邻居对拟合的贡献更大。这可以通过 &lt;code&gt;weights&lt;/code&gt; 关键字完成。默认值 &lt;code&gt;weights = 'uniform'&lt;/code&gt; ，为每个邻居分配统一的权重。 &lt;code&gt;weights = 'distance'&lt;/code&gt; 分配的权重与距查询点的距离成反比。可替代地，可以提供距离的用户定义函数来计算权重。</target>
        </trans-unit>
        <trans-unit id="650ceb2a1485e5f890a3dcff86c2748037640c3b" translate="yes" xml:space="preserve">
          <source>The basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. Under some circumstances, it can be advantageous to weight points such that nearby points contribute more to the regression than faraway points. This can be accomplished through the &lt;code&gt;weights&lt;/code&gt; keyword. The default value, &lt;code&gt;weights = 'uniform'&lt;/code&gt;, assigns equal weights to all points. &lt;code&gt;weights = 'distance'&lt;/code&gt; assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied, which will be used to compute the weights.</source>
          <target state="translated">基本最近邻回归使用统一的权重：也就是说，本地邻域中的每个点均对查询点的分类做出统一的贡献。在某些情况下，对权重点进行加权可能会比较有利，以使邻近的点比远离的点对回归的贡献更大。这可以通过 &lt;code&gt;weights&lt;/code&gt; 关键字完成。默认值 &lt;code&gt;weights = 'uniform'&lt;/code&gt; ，将相等的权重分配给所有点。 &lt;code&gt;weights = 'distance'&lt;/code&gt; 分配的权重与距查询点的距离成反比。或者，可以提供距离的用户定义功能，该功能将用于计算权重。</target>
        </trans-unit>
        <trans-unit id="1cb5ab68855135ab83ff44b066a7bba092358a07" translate="yes" xml:space="preserve">
          <source>The behavior of &lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt;&lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt;&lt;/a&gt; is summarized in the following table.</source>
          <target state="translated">下表总结了&lt;a href=&quot;generated/sklearn.neighbors.localoutlierfactor#sklearn.neighbors.LocalOutlierFactor&quot;&gt; &lt;code&gt;neighbors.LocalOutlierFactor&lt;/code&gt; &lt;/a&gt;的行为。</target>
        </trans-unit>
        <trans-unit id="9c1d95adae9acea07102a8bfe1db513459ebeaee" translate="yes" xml:space="preserve">
          <source>The behavior of the model is very sensitive to the &lt;code&gt;gamma&lt;/code&gt; parameter. If &lt;code&gt;gamma&lt;/code&gt; is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with &lt;code&gt;C&lt;/code&gt; will be able to prevent overfitting.</source>
          <target state="translated">模型的行为对 &lt;code&gt;gamma&lt;/code&gt; 参数非常敏感。如果 &lt;code&gt;gamma&lt;/code&gt; 太大，则支持向量的影响区域的半径仅包括支持向量本身，并且使用 &lt;code&gt;C&lt;/code&gt; 进行的正则化量都无法防止过度拟合。</target>
        </trans-unit>
        <trans-unit id="2ab99fa4c73a04aa03f4ac31b15851c1ddbe51a1" translate="yes" xml:space="preserve">
          <source>The below plot uses the first two features. See &lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;here&lt;/a&gt; for more information on this dataset.</source>
          <target state="translated">下图使用了前两个功能。有关此数据集的更多信息，请参见&lt;a href=&quot;https://en.wikipedia.org/wiki/Iris_flower_data_set&quot;&gt;此处&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="4992d67c6e1aff2322ba4c5bb3c9e0c82a7a36cf" translate="yes" xml:space="preserve">
          <source>The best model is selected by cross-validation.</source>
          <target state="translated">通过交叉验证,选出最佳模型。</target>
        </trans-unit>
        <trans-unit id="1a3f42d6835c4f6686d54e99d9f9cd0e3a597fec" translate="yes" xml:space="preserve">
          <source>The best performance is 1 with &lt;code&gt;normalize == True&lt;/code&gt; and the number of samples with &lt;code&gt;normalize == False&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;normalize == True&lt;/code&gt; ，最佳性能为1；归 &lt;code&gt;normalize == False&lt;/code&gt; 的样本数量。</target>
        </trans-unit>
        <trans-unit id="4b2ab3d16cd4ed6ed82ae137ea7b0ef6d79ff832" translate="yes" xml:space="preserve">
          <source>The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.</source>
          <target state="translated">最好的可能p值是1/(n_permutations+1),最差的是1.0。</target>
        </trans-unit>
        <trans-unit id="ed9ac623a873633695e926c14de64904b54167f4" translate="yes" xml:space="preserve">
          <source>The best possible score is 1.0, lower values are worse.</source>
          <target state="translated">最好的分数是1.0,数值越低越差。</target>
        </trans-unit>
        <trans-unit id="9230aaef9f0a1fa5e57a82e7f44b36cb5d04d36e" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.</source>
          <target state="translated">最佳值为1,最差值为-1。接近0的数值表示重叠的群组。</target>
        </trans-unit>
        <trans-unit id="6e2eac60da3011cdc30f86be85be8ffa7e5f1da5" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.</source>
          <target state="translated">最佳值为1,最差值为-1。接近0的值表示重叠的聚类,负值一般表示一个样本被分配到了错误的聚类,因为不同的聚类更相似。负值一般表示一个样本被分配到了错误的聚类,因为不同的聚类更相似。</target>
        </trans-unit>
        <trans-unit id="b4a689b86b5cda51ef6628ef622a0daf756424bd" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is 0 when &lt;code&gt;adjusted=False&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;adjusted=False&lt;/code&gt; 时，最佳值为1，最差值为0 。</target>
        </trans-unit>
        <trans-unit id="488558d8da77f986fd351608404cb6a07bd4fe58" translate="yes" xml:space="preserve">
          <source>The best value is 1 and the worst value is 0.</source>
          <target state="translated">最佳值为1,最差值为0。</target>
        </trans-unit>
        <trans-unit id="b82813aa45a878f8df07ac95972e2e5116e63bed" translate="yes" xml:space="preserve">
          <source>The bias term in the underlying linear model.</source>
          <target state="translated">基础线性模型中的偏差项。</target>
        </trans-unit>
        <trans-unit id="d5675ace96b0ccc75bff7a0b67255343283bf5b9" translate="yes" xml:space="preserve">
          <source>The bicluster label of each column.</source>
          <target state="translated">每个列的双簇标签。</target>
        </trans-unit>
        <trans-unit id="d135838b65ddeda2dac8521941211167551d33d9" translate="yes" xml:space="preserve">
          <source>The bicluster label of each row.</source>
          <target state="translated">每行的双簇标签。</target>
        </trans-unit>
        <trans-unit id="4f153639172a63c8dba2ed27e16715aaa100936d" translate="yes" xml:space="preserve">
          <source>The bipartite structure allows for the use of efficient block Gibbs sampling for inference.</source>
          <target state="translated">两段式结构可以使用高效的块状吉布斯采样进行推理。</target>
        </trans-unit>
        <trans-unit id="b104bf424ac624d7987a6846ffafb73fcb6a3323" translate="yes" xml:space="preserve">
          <source>The bounding box for each cluster center when centers are generated at random.</source>
          <target state="translated">随机生成中心时,每个簇中心的边界框。</target>
        </trans-unit>
        <trans-unit id="de87510aaa8a4634fe7b670444f1141d0b934e04" translate="yes" xml:space="preserve">
          <source>The breast cancer dataset is a classic and very easy binary classification dataset.</source>
          <target state="translated">乳腺癌数据集是一个经典且非常简单的二元分类数据集。</target>
        </trans-unit>
        <trans-unit id="2045b20286b76d60e5fa1f74a1df8fc02a45c66b" translate="yes" xml:space="preserve">
          <source>The brier score loss is also between 0 to 1 and the lower the score (the mean square difference is smaller), the more accurate the prediction is. It can be thought of as a measure of the &amp;ldquo;calibration&amp;rdquo; of a set of probabilistic predictions.</source>
          <target state="translated">刺痛分数损失也在0到1之间，并且分数越低（均方差越小），则预测越准确。可以将其视为对一组概率预测的&amp;ldquo;校准&amp;rdquo;的度量。</target>
        </trans-unit>
        <trans-unit id="74481417cca6e6adb9f25dfbb8e31b8fc201eb01" translate="yes" xml:space="preserve">
          <source>The callable to use for the inverse transformation. This will be passed the same arguments as inverse transform, with args and kwargs forwarded. If inverse_func is None, then inverse_func will be the identity function.</source>
          <target state="translated">用于反变换的可调用。这将被传递与inverse transform相同的参数,args和kwargs将被转发。如果inverse_func为None,那么inverse_func将是身份函数。</target>
        </trans-unit>
        <trans-unit id="81ea3433c521d90153f147dfae64d0df290c9757" translate="yes" xml:space="preserve">
          <source>The callable to use for the transformation. This will be passed the same arguments as transform, with args and kwargs forwarded. If func is None, then func will be the identity function.</source>
          <target state="translated">用于转换的callable。这将被传递与 transform 相同的参数,args 和 kwargs 将被转发。如果func为None,那么func将是身份函数。</target>
        </trans-unit>
        <trans-unit id="1f38539ea641dd8b50f2f7d85a7f466ba670ba50" translate="yes" xml:space="preserve">
          <source>The categories of each feature determined during fitting (in order of the features in X and corresponding with the output of &lt;code&gt;transform&lt;/code&gt;).</source>
          <target state="translated">在拟合过程中确定的每个特征的类别（按X中特征的顺序，并与 &lt;code&gt;transform&lt;/code&gt; 的输出相对应）。</target>
        </trans-unit>
        <trans-unit id="9d4177cfd25fe2d42f6a09b10ba8f700ec03cc45" translate="yes" xml:space="preserve">
          <source>The chi squared kernel is given by</source>
          <target state="translated">由以下公式可知,chi平方核</target>
        </trans-unit>
        <trans-unit id="e654b311ef425d553f0f6705f2dd43fc95968d41" translate="yes" xml:space="preserve">
          <source>The chi squared kernel is most commonly used on histograms (bags) of visual words.</source>
          <target state="translated">chi平方核最常用于视觉词的直方图(袋)上。</target>
        </trans-unit>
        <trans-unit id="085e153138bf3768d8fc53856c1f1238c6d3caff" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is a very popular choice for training non-linear SVMs in computer vision applications. It can be computed using &lt;a href=&quot;generated/sklearn.metrics.pairwise.chi2_kernel#sklearn.metrics.pairwise.chi2_kernel&quot;&gt;&lt;code&gt;chi2_kernel&lt;/code&gt;&lt;/a&gt; and then passed to an &lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt;&lt;code&gt;sklearn.svm.SVC&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;kernel=&quot;precomputed&quot;&lt;/code&gt;:</source>
          <target state="translated">卡方核是在计算机视觉应用中训练非线性SVM的非常受欢迎的选择。可以使用&lt;a href=&quot;generated/sklearn.metrics.pairwise.chi2_kernel#sklearn.metrics.pairwise.chi2_kernel&quot;&gt; &lt;code&gt;chi2_kernel&lt;/code&gt; &lt;/a&gt;进行计算，然后使用 &lt;code&gt;kernel=&quot;precomputed&quot;&lt;/code&gt; 传递给&lt;a href=&quot;generated/sklearn.svm.svc#sklearn.svm.SVC&quot;&gt; &lt;code&gt;sklearn.svm.SVC&lt;/code&gt; &lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="9233e345f2318b5fc92b12dd745f5adf2df3c33c" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is computed between each pair of rows in X and Y. X and Y have to be non-negative. This kernel is most commonly applied to histograms.</source>
          <target state="translated">在X和Y中的每对行之间计算chi平方核,X和Y必须是非负数。这个核最常用于直方图。</target>
        </trans-unit>
        <trans-unit id="a8ba729b106653acd0bc97ad998e8dac7b2b635e" translate="yes" xml:space="preserve">
          <source>The chi-squared kernel is given by:</source>
          <target state="translated">芝方核的计算公式为:。</target>
        </trans-unit>
        <trans-unit id="5ac1548c51d0c4529f9b6250f5d16db6b392dc40" translate="yes" xml:space="preserve">
          <source>The children of each non-leaf node. Values less than &lt;code&gt;n_features&lt;/code&gt; correspond to leaves of the tree which are the original samples. A node &lt;code&gt;i&lt;/code&gt; greater than or equal to &lt;code&gt;n_features&lt;/code&gt; is a non-leaf node and has children &lt;code&gt;children_[i - n_features]&lt;/code&gt;. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node &lt;code&gt;n_features + i&lt;/code&gt;</source>
          <target state="translated">每个非叶节点的子级。小于 &lt;code&gt;n_features&lt;/code&gt; 的值对应于作为原始样本的树的叶子。大于或等于 &lt;code&gt;n_features&lt;/code&gt; 的节点 &lt;code&gt;i&lt;/code&gt; 是非叶节点，并且具有子级 &lt;code&gt;children_[i - n_features]&lt;/code&gt; 。或者，在第i次迭代中，children [i] [0]和children [i] [1]合并以形成节点 &lt;code&gt;n_features + i&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="c8989abb78a441b49758b99ec48d8b090a3ce248" translate="yes" xml:space="preserve">
          <source>The children of each non-leaf node. Values less than &lt;code&gt;n_samples&lt;/code&gt; correspond to leaves of the tree which are the original samples. A node &lt;code&gt;i&lt;/code&gt; greater than or equal to &lt;code&gt;n_samples&lt;/code&gt; is a non-leaf node and has children &lt;code&gt;children_[i - n_samples]&lt;/code&gt;. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node &lt;code&gt;n_samples + i&lt;/code&gt;</source>
          <target state="translated">每个非叶节点的子级。小于 &lt;code&gt;n_samples&lt;/code&gt; 的值对应于作为原始样本的树的叶子。大于或等于 &lt;code&gt;n_samples&lt;/code&gt; 的节点 &lt;code&gt;i&lt;/code&gt; 是非叶节点，并且具有子级 &lt;code&gt;children_[i - n_samples]&lt;/code&gt; 。或者，在第i次迭代中，children [i] [0]和children [i] [1]合并以形成节点 &lt;code&gt;n_samples + i&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="f3473fe6f2b500963d93290672a7a3ef3f212da2" translate="yes" xml:space="preserve">
          <source>The choice of features is not particularly helpful, but serves to illustrate the technique.</source>
          <target state="translated">特征的选择并没有特别的帮助,但起到了说明技术的作用。</target>
        </trans-unit>
        <trans-unit id="1f460fe2e0e34556a3c6a8a6d017a5f63eeb9ea9" translate="yes" xml:space="preserve">
          <source>The cholesky decomposition of the precision matrices of each mixture component. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">每个混合成分的精确矩阵的cholesky分解。精度矩阵是协方差矩阵的逆矩阵。协方差矩阵是对称正定的，因此可以通过精度矩阵等效地对高斯的混合进行参数化。存储精度矩阵而不是协方差矩阵使在测试时计算新样本的对数似然更有效率。形状取决于 &lt;code&gt;covariance_type&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="54b709e28c0bc302f278f75cf2f9281781fc40ea" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; used with the optional parameter &lt;code&gt;svd_solver='randomized'&lt;/code&gt; is very useful in that case: since we are going to drop most of the singular vectors it is much more efficient to limit the computation to an approximated estimate of the singular vectors we will keep to actually perform the transform.</source>
          <target state="translated">在这种情况下，与可选参数 &lt;code&gt;svd_solver='randomized'&lt;/code&gt; 一起使用的&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;类非常有用：由于我们将丢弃大多数奇异向量，因此将计算限制为对奇异向量的近似估计会更加有效。继续实际执行转换。</target>
        </trans-unit>
        <trans-unit id="9afc238719eb2575519db8e476eb36c39ec071a2" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.feature_extraction.dictvectorizer#sklearn.feature_extraction.DictVectorizer&quot;&gt;&lt;code&gt;DictVectorizer&lt;/code&gt;&lt;/a&gt; can be used to convert feature arrays represented as lists of standard Python &lt;code&gt;dict&lt;/code&gt; objects to the NumPy/SciPy representation used by scikit-learn estimators.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.feature_extraction.dictvectorizer#sklearn.feature_extraction.DictVectorizer&quot;&gt; &lt;code&gt;DictVectorizer&lt;/code&gt; &lt;/a&gt;类可用于将表示为标准Python &lt;code&gt;dict&lt;/code&gt; 对象列表的要素数组转换为scikit-learn估计器使用的NumPy / SciPy表示形式。</target>
        </trans-unit>
        <trans-unit id="0b3d7d58279a2952a288a94db8a135a534d78ec7" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; is a high-speed, low-memory vectorizer that uses a technique known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_hashing&quot;&gt;feature hashing&lt;/a&gt;, or the &amp;ldquo;hashing trick&amp;rdquo;. Instead of building a hash table of the features encountered in training, as the vectorizers do, instances of &lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt;&lt;code&gt;FeatureHasher&lt;/code&gt;&lt;/a&gt; apply a hash function to the features to determine their column index in sample matrices directly. The result is increased speed and reduced memory usage, at the expense of inspectability; the hasher does not remember what the input features looked like and has no &lt;code&gt;inverse_transform&lt;/code&gt; method.</source>
          <target state="translated">类&lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt;是一种高速，低内存的矢量化程序，它使用一种称为&lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_hashing&quot;&gt;特征哈希&lt;/a&gt;或&amp;ldquo;哈希技巧&amp;rdquo;的技术。像矢量化程序一样，&lt;a href=&quot;generated/sklearn.feature_extraction.featurehasher#sklearn.feature_extraction.FeatureHasher&quot;&gt; &lt;code&gt;FeatureHasher&lt;/code&gt; &lt;/a&gt;实例不像训练器那样构建训练中遇到的特征的哈希表，而是将哈希函数应用于特征以直接确定其在样本矩阵中的列索引。结果是提高了速度，减少了内存使用，但以可检查性为代价；哈希器无法记住输入要素的外观，并且没有 &lt;code&gt;inverse_transform&lt;/code&gt; 方法。</target>
        </trans-unit>
        <trans-unit id="46148e4d160f9510def06597f1921af425f74fcb" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt;&lt;code&gt;IsotonicRegression&lt;/code&gt;&lt;/a&gt; fits a non-decreasing function to data. It solves the following problem:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.isotonic.isotonicregression#sklearn.isotonic.IsotonicRegression&quot;&gt; &lt;code&gt;IsotonicRegression&lt;/code&gt; &lt;/a&gt;类将非递减函数拟合到数据。它解决了以下问题：</target>
        </trans-unit>
        <trans-unit id="04846022f8485d75461ddcb301a4717897728672" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt;&lt;code&gt;AdditiveChi2Sampler&lt;/code&gt;&lt;/a&gt; implements this component wise deterministic sampling. Each component is sampled \(n\) times, yielding \(2n+1\) dimensions per input dimension (the multiple of two stems from the real and complex part of the Fourier transform). In the literature, \(n\) is usually chosen to be 1 or 2, transforming the dataset to size &lt;code&gt;n_samples * 5 * n_features&lt;/code&gt; (in the case of \(n=2\)).</source>
          <target state="translated">类&lt;a href=&quot;generated/sklearn.kernel_approximation.additivechi2sampler#sklearn.kernel_approximation.AdditiveChi2Sampler&quot;&gt; &lt;code&gt;AdditiveChi2Sampler&lt;/code&gt; &lt;/a&gt;实现此组件明智的确定性采样。每个分量采样\（n \）次，每个输入维度产生\（2n + 1 \）个维度（傅立叶变换的实数部分和复数部分的两个茎的倍数）。在文献中，\（n \）通常选择为1或2，将数据集转换为大小 &lt;code&gt;n_samples * 5 * n_features&lt;/code&gt; （在\（n = 2 \）的情况下）。</target>
        </trans-unit>
        <trans-unit id="9fdc0ed638cb0889fa5320ee5ada72b80a16402f" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.elasticnetcv#sklearn.linear_model.ElasticNetCV&quot;&gt;&lt;code&gt;ElasticNetCV&lt;/code&gt;&lt;/a&gt; can be used to set the parameters &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) and &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) by cross-validation.</source>
          <target state="translated">可以使用类&lt;a href=&quot;generated/sklearn.linear_model.elasticnetcv#sklearn.linear_model.ElasticNetCV&quot;&gt; &lt;code&gt;ElasticNetCV&lt;/code&gt; &lt;/a&gt;通过交叉验证来设置参数 &lt;code&gt;alpha&lt;/code&gt; （\（\ alpha \））和 &lt;code&gt;l1_ratio&lt;/code&gt; （\（\ rho \））。</target>
        </trans-unit>
        <trans-unit id="c89182d0633b09742e745213024a53ccec4fee0a" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnetcv#sklearn.linear_model.MultiTaskElasticNetCV&quot;&gt;&lt;code&gt;MultiTaskElasticNetCV&lt;/code&gt;&lt;/a&gt; can be used to set the parameters &lt;code&gt;alpha&lt;/code&gt; (\(\alpha\)) and &lt;code&gt;l1_ratio&lt;/code&gt; (\(\rho\)) by cross-validation.</source>
          <target state="translated">类&lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnetcv#sklearn.linear_model.MultiTaskElasticNetCV&quot;&gt; &lt;code&gt;MultiTaskElasticNetCV&lt;/code&gt; &lt;/a&gt;可以用来设置参数 &lt;code&gt;alpha&lt;/code&gt; （\（\阿尔法\））和 &lt;code&gt;l1_ratio&lt;/code&gt; （\（\ RHO \））通过交叉验证。</target>
        </trans-unit>
        <trans-unit id="6c66ee2c7a53fbda2b2b2da602d1e782a1fcc1b3" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a first-order SGD learning routine. The algorithm iterates over the training examples and for each example updates the model parameters according to the update rule given by</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;类实现一阶SGD学习例程。该算法迭代训练示例，并针对每个示例根据由给出的更新规则更新模型参数</target>
        </trans-unit>
        <trans-unit id="f128e57d09cf8b511f002f70c40429b250044323" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;类实现了简单的随机梯度下降学习例程，该例程支持不同的损失函数和分类惩罚。</target>
        </trans-unit>
        <trans-unit id="9be5b4c1c13de0290ee1bddf8fed519138b844e9" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; is well suited for regression problems with a large number of training samples (&amp;gt; 10.000), for other problems we recommend &lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt;&lt;code&gt;Ridge&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt;, or &lt;a href=&quot;generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt;&lt;code&gt;ElasticNet&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;类实现了简单的随机梯度下降学习例程，该例程支持不同的损失函数和惩罚以拟合线性回归模型。&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;非常适合于具有大量训练样本（&amp;gt; 10.000）的回归问题，对于其他问题，我们建议使用&lt;a href=&quot;generated/sklearn.linear_model.ridge#sklearn.linear_model.Ridge&quot;&gt; &lt;code&gt;Ridge&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt;或&lt;a href=&quot;generated/sklearn.linear_model.elasticnet#sklearn.linear_model.ElasticNet&quot;&gt; &lt;code&gt;ElasticNet&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="5ca16e4c08c0574f508b6c526beeb2111eade54a" translate="yes" xml:space="preserve">
          <source>The class &lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt;&lt;code&gt;OneClassSVM&lt;/code&gt;&lt;/a&gt; implements a One-Class SVM which is used in outlier detection.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.svm.oneclasssvm#sklearn.svm.OneClassSVM&quot;&gt; &lt;code&gt;OneClassSVM&lt;/code&gt; &lt;/a&gt;类实现了用于异常值检测的One-Class SVM。</target>
        </trans-unit>
        <trans-unit id="5eaffd69df806c5b8353c88358cfd0a49b275168" translate="yes" xml:space="preserve">
          <source>The class label for which the PDPs should be computed. Only if gbrt is a multi-class model. Must be in &lt;code&gt;gbrt.classes_&lt;/code&gt;.</source>
          <target state="translated">应该为其计算PDP的类标签。仅当gbrt是多类模型时。必须在 &lt;code&gt;gbrt.classes_&lt;/code&gt; 中。</target>
        </trans-unit>
        <trans-unit id="2a31ab08188f24bd31e99f218cab5467eddf2ef0" translate="yes" xml:space="preserve">
          <source>The class labels.</source>
          <target state="translated">类标签。</target>
        </trans-unit>
        <trans-unit id="075fd1f474c95d9f3895b75ead4b1f9c99be54a8" translate="yes" xml:space="preserve">
          <source>The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;.</source>
          <target state="translated">输入样本的类对数概率。类的顺序与属性 &lt;code&gt;classes_&lt;/code&gt; 中的顺序相对应。</target>
        </trans-unit>
        <trans-unit id="f9e49144e04860d9a1f4a8512901a503e5d41c6f" translate="yes" xml:space="preserve">
          <source>The class of Matern kernels is a generalization of the RBF and the absolute exponential kernel parameterized by an additional parameter nu. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions).</source>
          <target state="translated">Matern核的类别是RBF和绝对指数核的泛化,其参数化为一个附加参数nu。nu越小,近似函数越不平滑。当nu=inf时,该核相当于RBF核,nu=0.5时相当于绝对指数核。重要的中间值是nu=1.5(一次可微分函数)和nu=2.5(两次可微分函数)。</target>
        </trans-unit>
        <trans-unit id="e48205f573bda857c27ec3937eb80960daed3556" translate="yes" xml:space="preserve">
          <source>The class ordering is preserved:</source>
          <target state="translated">类的排序被保留下来。</target>
        </trans-unit>
        <trans-unit id="81478ef84d2d645ae9c4e632247f7f3bc9052a92" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of outputs is the same of that of the &lt;code&gt;classes_&lt;/code&gt; attribute.</source>
          <target state="translated">输入样本的分类概率。输出顺序与 &lt;code&gt;classes_&lt;/code&gt; 属性的顺序相同。</target>
        </trans-unit>
        <trans-unit id="8ea2403e3ddde25cdaa7213df112f25928f5232d" translate="yes" xml:space="preserve">
          <source>The class probabilities of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;.</source>
          <target state="translated">输入样本的分类概率。类的顺序与属性 &lt;code&gt;classes_&lt;/code&gt; 中的顺序相对应。</target>
        </trans-unit>
        <trans-unit id="64b22dfb54911895a611d3b9cfbaf3a3f14d97f5" translate="yes" xml:space="preserve">
          <source>The class to report if &lt;code&gt;average='binary'&lt;/code&gt; and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting &lt;code&gt;labels=[pos_label]&lt;/code&gt; and &lt;code&gt;average != 'binary'&lt;/code&gt; will report scores for that label only.</source>
          <target state="translated">如果 &lt;code&gt;average='binary'&lt;/code&gt; 且数据为二进制的报告类。如果数据是多类或多标签的，则将被忽略；设置 &lt;code&gt;labels=[pos_label]&lt;/code&gt; 和 &lt;code&gt;average != 'binary'&lt;/code&gt; 将仅报告该标签的分数。</target>
        </trans-unit>
        <trans-unit id="b02b6e7e5cca0ff24c2691f6e7d2bc9c247d17d1" translate="yes" xml:space="preserve">
          <source>The class to use to build the returned adjacency matrix.</source>
          <target state="translated">用来构建返回的邻接矩阵的类。</target>
        </trans-unit>
        <trans-unit id="08bf02b9f7a917eed20c9c94c384791c0ed4578f" translate="yes" xml:space="preserve">
          <source>The class with respect to which we perform a one-vs-all fit. If None, then it is assumed that the given problem is binary.</source>
          <target state="translated">我们执行一比一拟合的类。如果无,则假设给定的问题是二进制的。</target>
        </trans-unit>
        <trans-unit id="26dc8660d1112d77620f09027ce7b9fbee6027a3" translate="yes" xml:space="preserve">
          <source>The classes &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; provide functionality to fit linear models for classification and regression using different (convex) loss functions and different penalties. E.g., with &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt;, &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; fits a logistic regression model, while with &lt;code&gt;loss=&quot;hinge&quot;&lt;/code&gt; it fits a linear support vector machine (SVM).</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;类提供的功能可以使用不同（凸）损失函数和不同惩罚来拟合线性模型，以进行分类和回归。例如，对于 &lt;code&gt;loss=&quot;log&quot;&lt;/code&gt; ，&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;适合于逻辑回归模型，而对于 &lt;code&gt;loss=&quot;hinge&quot;&lt;/code&gt; 则适合于线性支持向量机（SVM）。</target>
        </trans-unit>
        <trans-unit id="d29e2c71b01d97c9c93dc5aad937ec38e91b7ca9" translate="yes" xml:space="preserve">
          <source>The classes &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; provide two criteria to stop the algorithm when a given level of convergence is reached:</source>
          <target state="translated">当达到给定的&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;，SGDClassifier和&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;类提供了两个准则来停止算法：</target>
        </trans-unit>
        <trans-unit id="d7ef8ada32aed500913b0cf053368e18ea306f33" translate="yes" xml:space="preserve">
          <source>The classes in &lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt;&lt;code&gt;sklearn.neighbors&lt;/code&gt;&lt;/a&gt; can handle either NumPy arrays or &lt;code&gt;scipy.sparse&lt;/code&gt; matrices as input. For dense matrices, a large number of possible distance metrics are supported. For sparse matrices, arbitrary Minkowski metrics are supported for searches.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.neighbors&quot;&gt; &lt;code&gt;sklearn.neighbors&lt;/code&gt; 中&lt;/a&gt;的类可以处理NumPy数组或 &lt;code&gt;scipy.sparse&lt;/code&gt; 矩阵作为输入。对于密集矩阵，支持大量可能的距离度量。对于稀疏矩阵，支持使用任意Minkowski度量进行搜索。</target>
        </trans-unit>
        <trans-unit id="f39d71815c55889dd46cb64855cc1d40f837d2d9" translate="yes" xml:space="preserve">
          <source>The classes in the &lt;a href=&quot;classes#module-sklearn.feature_selection&quot;&gt;&lt;code&gt;sklearn.feature_selection&lt;/code&gt;&lt;/a&gt; module can be used for feature selection/dimensionality reduction on sample sets, either to improve estimators&amp;rsquo; accuracy scores or to boost their performance on very high-dimensional datasets.</source>
          <target state="translated">&lt;a href=&quot;classes#module-sklearn.feature_selection&quot;&gt; &lt;code&gt;sklearn.feature_selection&lt;/code&gt; &lt;/a&gt;模块中的类可用于样本集的特征选择/ 降维，以提高估计量的准确性得分或提高其在超高维数据集上的性能。</target>
        </trans-unit>
        <trans-unit id="df672af473b055efb339763c1ea56e27edeec8c9" translate="yes" xml:space="preserve">
          <source>The classes in this submodule allow to approximate the embedding \(\phi\), thereby working explicitly with the representations \(\phi(x_i)\), which obviates the need to apply the kernel or store training examples.</source>
          <target state="translated">这个子模块中的类允许近似于嵌入/(\\phi/),从而明确地与表征/(\phi(x_i)\)一起工作,这就避免了应用内核或存储训练实例的需要。</target>
        </trans-unit>
        <trans-unit id="41647bba96e968e0bd58965ae65f2bc0365d42c6" translate="yes" xml:space="preserve">
          <source>The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).</source>
          <target state="translated">类标签(单输出问题),或类标签的数组列表(多输出问题)。</target>
        </trans-unit>
        <trans-unit id="772daeb9b0eaa408b264b0fa548b1cbd24779805" translate="yes" xml:space="preserve">
          <source>The classes labels.</source>
          <target state="translated">各类标签。</target>
        </trans-unit>
        <trans-unit id="13f18ce429d97546a8b16cf132e8e6c03147cb50" translate="yes" xml:space="preserve">
          <source>The classic implementation of the clustering method based on the Lloyd&amp;rsquo;s algorithm. It consumes the whole set of input data at each iteration.</source>
          <target state="translated">基于劳埃德算法的聚类方法的经典实现。它在每次迭代时消耗整个输入数据集。</target>
        </trans-unit>
        <trans-unit id="4b0c76ea0ae326d19f535ab9520b9dd024cf9124" translate="yes" xml:space="preserve">
          <source>The classification is performed by projecting to the first two principal components found by PCA and CCA for visualisation purposes, followed by using the &lt;a href=&quot;../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt;&lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt;&lt;/a&gt; metaclassifier using two SVCs with linear kernels to learn a discriminative model for each class. Note that PCA is used to perform an unsupervised dimensionality reduction, while CCA is used to perform a supervised one.</source>
          <target state="translated">通过将PCA和CCA发现的前两个主要组件投影以进行可视化，然后通过使用&lt;a href=&quot;../modules/generated/sklearn.multiclass.onevsrestclassifier#sklearn.multiclass.OneVsRestClassifier&quot;&gt; &lt;code&gt;sklearn.multiclass.OneVsRestClassifier&lt;/code&gt; &lt;/a&gt;元分类器（使用两个带有线性内核的SVC来学习每个类的判别模型）来执行分类。请注意，PCA用于执行无监督的降维，而CCA用于执行无监督的降维。</target>
        </trans-unit>
        <trans-unit id="d9bcc8debd316bfd1c4b9e73630f6ba7215b9e3d" translate="yes" xml:space="preserve">
          <source>The classifier whose output decision function needs to be calibrated to offer more accurate predict_proba outputs. If cv=prefit, the classifier must have been fit already on data.</source>
          <target state="translated">需要校准输出决定函数的分类器,以提供更精确的predict_proba输出。如果cv=prefit,那么分类器必须已经在数据上进行了拟合。</target>
        </trans-unit>
        <trans-unit id="88e39a3dfd7087282361f3b39109354f7ff32ede" translate="yes" xml:space="preserve">
          <source>The code below also illustrates how the construction and the computation of the predictions can be parallelized within multiple jobs.</source>
          <target state="translated">下面的代码还说明了如何在多个作业中并行计算预测的构造和计算。</target>
        </trans-unit>
        <trans-unit id="6a1cc4b5491e9b42fd445850086762fcb248e836" translate="yes" xml:space="preserve">
          <source>The code below plots the dependency of y against individual x_i and normalized values of univariate F-tests statistics and mutual information.</source>
          <target state="translated">下面的代码绘制了y对各个x_i的依赖性,以及单变量F检验统计和相互信息的标准化值。</target>
        </trans-unit>
        <trans-unit id="4b0a65eb61a5277b5e28aef5c3b0e7ea577bdc07" translate="yes" xml:space="preserve">
          <source>The code-examples in the above tutorials are written in a &lt;em&gt;python-console&lt;/em&gt; format. If you wish to easily execute these examples in &lt;strong&gt;IPython&lt;/strong&gt;, use:</source>
          <target state="translated">以上教程中的代码示例均以&lt;em&gt;python-console&lt;/em&gt;格式编写。如果希望在&lt;strong&gt;IPython中&lt;/strong&gt;轻松执行这些示例，请使用：</target>
        </trans-unit>
        <trans-unit id="ce298b790cf6df954f5ce5058b04debe04deb707" translate="yes" xml:space="preserve">
          <source>The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the regression sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">系数R^2定义为(1-u/v),其中u为残差平方和((y_true-y_pred)**2).sum(),v为回归平方和((y_true-y_true.mean())**2).sum()。最佳可能的分数是1.0,而且可以是负数(因为模型可以任意变坏)。一个恒定的模型,总是预测y的期望值,不考虑输入特征,将得到0.0的R^2分数。</target>
        </trans-unit>
        <trans-unit id="48121e6567bc65fc6ea38acd7d461ac73e2ea472" translate="yes" xml:space="preserve">
          <source>The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</source>
          <target state="translated">系数R^2定义为(1-u/v),其中u为残差平方和((y_true-y_pred)**2).sum(),v为总平方和((y_true-y_true.mean())**2).sum()。可能的最佳得分是1.0,它可以是负值(因为模型可以任意变坏)。一个总是预测y的期望值的常量模型,不考虑输入特征,将得到0.0的R^2分数。</target>
        </trans-unit>
        <trans-unit id="f0dabbd241b37afcd99edc97579580c22f2703f9" translate="yes" xml:space="preserve">
          <source>The coefficient of the underlying linear model. It is returned only if coef is True.</source>
          <target state="translated">基础线性模型的系数。只有当coef为真时才会返回。</target>
        </trans-unit>
        <trans-unit id="b2933da174c8fa332cc9ad841c0c25a1ab1996d2" translate="yes" xml:space="preserve">
          <source>The coefficients can be forced to be positive.</source>
          <target state="translated">可以强迫系数为正。</target>
        </trans-unit>
        <trans-unit id="3c89d0ba57b5481659a50cc73b873694c355fd9b" translate="yes" xml:space="preserve">
          <source>The coefficients of the linear model: &lt;code&gt;Y = X coef_ + Err&lt;/code&gt;</source>
          <target state="translated">线性模型的系数： &lt;code&gt;Y = X coef_ + Err&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="a7757730afe97cb8593dea4757d82727872c7529" translate="yes" xml:space="preserve">
          <source>The coefficients, the residual sum of squares and the variance score are also calculated.</source>
          <target state="translated">同时计算系数、残差平方和和方差得分。</target>
        </trans-unit>
        <trans-unit id="f15e1f2c7fb96cd43c32056a6c407c9c1c570883" translate="yes" xml:space="preserve">
          <source>The collection of fitted base estimators.</source>
          <target state="translated">拟合基础估计器的集合。</target>
        </trans-unit>
        <trans-unit id="e97d4f2642ef3372648a38fef3350e771647c3d6" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators as defined in &lt;code&gt;estimators&lt;/code&gt; that are not &lt;code&gt;None&lt;/code&gt;.</source>
          <target state="translated">装子估计的集合中定义的 &lt;code&gt;estimators&lt;/code&gt; 是不 &lt;code&gt;None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="0b6e390487ac628e8895818624f2b32fd0c95d36" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators.</source>
          <target state="translated">合适的子估计器的集合。</target>
        </trans-unit>
        <trans-unit id="929cef17b33a8aadbb4cff8f87d4813d9e794dca" translate="yes" xml:space="preserve">
          <source>The collection of fitted sub-estimators. &lt;code&gt;loss_.K&lt;/code&gt; is 1 for binary classification, otherwise n_classes.</source>
          <target state="translated">拟合子估计量的集合。对于二进制分类， &lt;code&gt;loss_.K&lt;/code&gt; 为1，否则为n_classes。</target>
        </trans-unit>
        <trans-unit id="34213cac4f6d6096a4a72655d4c46d8da28f907c" translate="yes" xml:space="preserve">
          <source>The collection of fitted transformers as tuples of (name, fitted_transformer, column). &lt;code&gt;fitted_transformer&lt;/code&gt; can be an estimator, &amp;lsquo;drop&amp;rsquo;, or &amp;lsquo;passthrough&amp;rsquo;. In case there were no columns selected, this will be the unfitted transformer. If there are remaining columns, the final element is a tuple of the form: (&amp;lsquo;remainder&amp;rsquo;, transformer, remaining_columns) corresponding to the &lt;code&gt;remainder&lt;/code&gt; parameter. If there are remaining columns, then &lt;code&gt;len(transformers_)==len(transformers)+1&lt;/code&gt;, otherwise &lt;code&gt;len(transformers_)==len(transformers)&lt;/code&gt;.</source>
          <target state="translated">拟合变压器的集合，作为（名称，fitt_transformer，列）的元组。 &lt;code&gt;fitted_transformer&lt;/code&gt; 可以是一个估算器，&amp;ldquo; drop&amp;rdquo;或&amp;ldquo; passthrough&amp;rdquo;。如果未选择任何列，则为未安装的变压器。如果还有剩余的列，则最后一个元素为以下形式的元组：（&amp;ldquo; remainder&amp;rdquo;，translator，remaining_columns），与 &lt;code&gt;remainder&lt;/code&gt; 参数相对应。如果还有剩余的列，则 &lt;code&gt;len(transformers_)==len(transformers)+1&lt;/code&gt; ，否则 &lt;code&gt;len(transformers_)==len(transformers)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d6d471d12278a874c4225d83bbd25d7f04a0a976" translate="yes" xml:space="preserve">
          <source>The color map illustrates the decision function learned by the SVC.</source>
          <target state="translated">颜色图说明了SVC学习的决策函数。</target>
        </trans-unit>
        <trans-unit id="093f5986f8d055d5b47f11ad021ce6ecfcef91e9" translate="yes" xml:space="preserve">
          <source>The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.</source>
          <target state="translated">从指标[n_nodes_ptr[i]:n_nodes_ptr[i+1]]中列举出第i个估计器的指标值。</target>
        </trans-unit>
        <trans-unit id="f0cca3e664aa52c0e892615c12365c0a6af40452" translate="yes" xml:space="preserve">
          <source>The combination used in this example is not particularly helpful on this dataset and is only used to illustrate the usage of FeatureUnion.</source>
          <target state="translated">这个例子中使用的组合在这个数据集上并没有特别大的帮助,只是用来说明FeatureUnion的用法。</target>
        </trans-unit>
        <trans-unit id="4524a6f67fb21e2d5fb712c219044b7460dbad0e" translate="yes" xml:space="preserve">
          <source>The components of the random matrix are drawn from N(0, 1 / n_components).</source>
          <target state="translated">随机矩阵的分量从N(0,1/n_components)中抽取。</target>
        </trans-unit>
        <trans-unit id="4d179dd17ec2e9e8ac79ee8fb26a8b8727033655" translate="yes" xml:space="preserve">
          <source>The compromise between l1 and l2 penalization chosen by cross validation</source>
          <target state="translated">通过交叉验证选择的l1和l2之间的折衷惩罚方法。</target>
        </trans-unit>
        <trans-unit id="37c8e8aa42af430070b2b87be4c5db5db294cc7b" translate="yes" xml:space="preserve">
          <source>The computation during &lt;code&gt;fit&lt;/code&gt; is:</source>
          <target state="translated">&lt;code&gt;fit&lt;/code&gt; 期间的计算为：</target>
        </trans-unit>
        <trans-unit id="cc728704a5b1deaa221b61ada3dd42ad8e661fbd" translate="yes" xml:space="preserve">
          <source>The computation during &lt;code&gt;predict&lt;/code&gt; is:</source>
          <target state="translated">&lt;code&gt;predict&lt;/code&gt; 期间的计算为：</target>
        </trans-unit>
        <trans-unit id="cc5659751ed96e6a9a511b247760eba7bd9fab63" translate="yes" xml:space="preserve">
          <source>The computation of Davies-Bouldin is simpler than that of Silhouette scores.</source>
          <target state="translated">Davies-Bouldin的计算比Silhouette分数的计算要简单。</target>
        </trans-unit>
        <trans-unit id="d526bd934b93818cdf8a3fccada8e6b2b34d84f2" translate="yes" xml:space="preserve">
          <source>The computational overhead of each SVD is &lt;code&gt;O(batch_size * n_features ** 2)&lt;/code&gt;, but only 2 * batch_size samples remain in memory at a time. There will be &lt;code&gt;n_samples / batch_size&lt;/code&gt; SVD computations to get the principal components, versus 1 large SVD of complexity &lt;code&gt;O(n_samples * n_features ** 2)&lt;/code&gt; for PCA.</source>
          <target state="translated">每个SVD的计算开销为 &lt;code&gt;O(batch_size * n_features ** 2)&lt;/code&gt; ，但一次仅2 * batch_size个样本保留在内存中。将使用 &lt;code&gt;n_samples / batch_size&lt;/code&gt; SVD计算来获取主成分，而PCA则需要1个大型SVD，复杂度为 &lt;code&gt;O(n_samples * n_features ** 2)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="2fa163515de812b4d78a2bbcef87e239a36ae4d1" translate="yes" xml:space="preserve">
          <source>The concept of early stopping is simple. We specify a &lt;code&gt;validation_fraction&lt;/code&gt; which denotes the fraction of the whole dataset that will be kept aside from training to assess the validation loss of the model. The gradient boosting model is trained using the training set and evaluated using the validation set. When each additional stage of regression tree is added, the validation set is used to score the model. This is continued until the scores of the model in the last &lt;code&gt;n_iter_no_change&lt;/code&gt; stages do not improve by atleast &lt;code&gt;tol&lt;/code&gt;. After that the model is considered to have converged and further addition of stages is &amp;ldquo;stopped early&amp;rdquo;.</source>
          <target state="translated">提前停止的概念很简单。我们指定一个 &lt;code&gt;validation_fraction&lt;/code&gt; ，它表示整个数据集的一部分，该部分将不进行训练以评估模型的验证损失。使用训练集训练梯度提升模型，并使用验证集进行评估。添加回归树的每个其他阶段时，将使用验证集对模型评分。这一直持续到模型中最后的得分 &lt;code&gt;n_iter_no_change&lt;/code&gt; 阶段不要ATLEAST提高 &lt;code&gt;tol&lt;/code&gt; 。此后，该模型被认为已经收敛，并且&amp;ldquo;早日停止&amp;rdquo;进一步添加阶段。</target>
        </trans-unit>
        <trans-unit id="feb60f640c62fd0856cb50e8401267d4f55d4774" translate="yes" xml:space="preserve">
          <source>The concrete &lt;code&gt;LossFunction&lt;/code&gt; object.</source>
          <target state="translated">具体的 &lt;code&gt;LossFunction&lt;/code&gt; 对象。</target>
        </trans-unit>
        <trans-unit id="4f21e1340272b60cf06a69153cc19d0140d625cf" translate="yes" xml:space="preserve">
          <source>The concrete loss function can be set via the &lt;code&gt;loss&lt;/code&gt; parameter. &lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt;&lt;code&gt;SGDClassifier&lt;/code&gt;&lt;/a&gt; supports the following loss functions:</source>
          <target state="translated">可以通过 &lt;code&gt;loss&lt;/code&gt; 参数设置具体的损失函数。&lt;a href=&quot;generated/sklearn.linear_model.sgdclassifier#sklearn.linear_model.SGDClassifier&quot;&gt; &lt;code&gt;SGDClassifier&lt;/code&gt; &lt;/a&gt;支持以下损失函数：</target>
        </trans-unit>
        <trans-unit id="c24a50928ec0729629e45e8b53b6d8b9f0112f79" translate="yes" xml:space="preserve">
          <source>The concrete loss function can be set via the &lt;code&gt;loss&lt;/code&gt; parameter. &lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt;&lt;code&gt;SGDRegressor&lt;/code&gt;&lt;/a&gt; supports the following loss functions:</source>
          <target state="translated">可以通过 &lt;code&gt;loss&lt;/code&gt; 参数设置具体的损失函数。&lt;a href=&quot;generated/sklearn.linear_model.sgdregressor#sklearn.linear_model.SGDRegressor&quot;&gt; &lt;code&gt;SGDRegressor&lt;/code&gt; &lt;/a&gt;支持以下损失函数：</target>
        </trans-unit>
        <trans-unit id="a5b98bb304bc3ec6a422167859a2f5da5580fa47" translate="yes" xml:space="preserve">
          <source>The concrete penalty can be set via the &lt;code&gt;penalty&lt;/code&gt; parameter. SGD supports the following penalties:</source>
          <target state="translated">可以通过 &lt;code&gt;penalty&lt;/code&gt; 参数设置具体的惩罚。SGD支持以下处罚：</target>
        </trans-unit>
        <trans-unit id="c0c66ccf788e88932593e02308e1dcae0bb5c1f0" translate="yes" xml:space="preserve">
          <source>The conditional probability distribution of each unit is given by the logistic sigmoid activation function of the input it receives:</source>
          <target state="translated">每个单元的条件概率分布由它所接受的输入的对数sigmoid激活函数给出。</target>
        </trans-unit>
        <trans-unit id="6e94f7f28c2cc9bab8c93a85a4438b802d82d10d" translate="yes" xml:space="preserve">
          <source>The confidence score for a sample is the signed distance of that sample to the hyperplane.</source>
          <target state="translated">一个样本的置信度分数是该样本到超平面的符号距离。</target>
        </trans-unit>
        <trans-unit id="b129f98741f30e6ae05e0872d86b02b5a921e905" translate="yes" xml:space="preserve">
          <source>The connectivity constraints are imposed via an connectivity matrix: a scipy sparse matrix that has elements only at the intersection of a row and a column with indices of the dataset that should be connected. This matrix can be constructed from a-priori information: for instance, you may wish to cluster web pages by only merging pages with a link pointing from one to another. It can also be learned from the data, for instance using &lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt;&lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt;&lt;/a&gt; to restrict merging to nearest neighbors as in &lt;a href=&quot;../auto_examples/cluster/plot_agglomerative_clustering#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py&quot;&gt;this example&lt;/a&gt;, or using &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt;&lt;code&gt;sklearn.feature_extraction.image.grid_to_graph&lt;/code&gt;&lt;/a&gt; to enable only merging of neighboring pixels on an image, as in the &lt;a href=&quot;../auto_examples/cluster/plot_coin_ward_segmentation#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py&quot;&gt;coin&lt;/a&gt; example.</source>
          <target state="translated">连通性约束是通过连通性矩阵施加的：稀疏矩阵，其稀疏矩阵仅在具有应连接数据集索引的行和列的交点处具有元素。该矩阵可以由先验信息构成：例如，您可能希望仅通过合并页面与指向彼此的链接来对网页进行聚类。它也可以从数据了解到，例如使用&lt;a href=&quot;generated/sklearn.neighbors.kneighbors_graph#sklearn.neighbors.kneighbors_graph&quot;&gt; &lt;code&gt;sklearn.neighbors.kneighbors_graph&lt;/code&gt; &lt;/a&gt;限制合并到最近的邻居，如&lt;a href=&quot;../auto_examples/cluster/plot_agglomerative_clustering#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py&quot;&gt;本实施例中&lt;/a&gt;，或者使用&lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt; &lt;code&gt;sklearn.feature_extraction.image.grid_to_graph&lt;/code&gt; &lt;/a&gt;只启用相邻像素的图像上合并，如在在&lt;a href=&quot;../auto_examples/cluster/plot_coin_ward_segmentation#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py&quot;&gt;硬币&lt;/a&gt;的例子。</target>
        </trans-unit>
        <trans-unit id="cba08514635a6da99fcbf046720afd85a65680ea" translate="yes" xml:space="preserve">
          <source>The constant value which defines the covariance: k(x_1, x_2) = constant_value</source>
          <target state="translated">定义协方差的常数:k(x_1,x_2)=constant_value。</target>
        </trans-unit>
        <trans-unit id="83836055dd86bf18d0180179c00f49511034950c" translate="yes" xml:space="preserve">
          <source>The contingency table calculated is typically utilized in the calculation of a similarity statistic (like the others listed in this document) between the two clusterings.</source>
          <target state="translated">计算出的应急表通常被利用在计算两个聚类之间的相似度统计量(就像本文件中列出的其他统计量一样)。</target>
        </trans-unit>
        <trans-unit id="ca0505a0687635a00a190438e7d26956e48c468c" translate="yes" xml:space="preserve">
          <source>The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.</source>
          <target state="translated">收敛阈值。当下限平均增益低于此阈值时,EM迭代将停止。</target>
        </trans-unit>
        <trans-unit id="0e26ef32f94bda64e8f6e396c3c303a3311ac4e1" translate="yes" xml:space="preserve">
          <source>The convergence threshold. EM iterations will stop when the lower bound average gain on the likelihood (of the training data with respect to the model) is below this threshold.</source>
          <target state="translated">收敛阈值。当(训练数据相对于模型的)似然的下限平均增益低于这个阈值时,EM迭代将停止。</target>
        </trans-unit>
        <trans-unit id="80f05388942910150044c6d47584d8bddc73ef4c" translate="yes" xml:space="preserve">
          <source>The converse mapping from feature name to column index is stored in the &lt;code&gt;vocabulary_&lt;/code&gt; attribute of the vectorizer:</source>
          <target state="translated">从要素名称到列索引的相反映射存储在矢量化器的 &lt;code&gt;vocabulary_&lt;/code&gt; 属性中：</target>
        </trans-unit>
        <trans-unit id="42e30f7491cd0bbd1be9eaa1008d97761b319a5f" translate="yes" xml:space="preserve">
          <source>The converted and validated X.</source>
          <target state="translated">经过转换和验证的X。</target>
        </trans-unit>
        <trans-unit id="043f2ec629cc510b9c1127fe6b10b6fe4448d241" translate="yes" xml:space="preserve">
          <source>The converted and validated y.</source>
          <target state="translated">经过转换和验证的y。</target>
        </trans-unit>
        <trans-unit id="81a7bc326e697c66d8802c043c85fabeabbf241b" translate="yes" xml:space="preserve">
          <source>The converted dataname.</source>
          <target state="translated">转换后的数据名。</target>
        </trans-unit>
        <trans-unit id="e11d4d7608fe1fa28c8a673b1389ac6181cb7877" translate="yes" xml:space="preserve">
          <source>The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights \(w_1\), \(w_2\), &amp;hellip;, \(w_N\) to each of the training samples. Initially, those weights are all set to \(w_i = 1/N\), so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence &lt;a href=&quot;#htf&quot; id=&quot;id10&quot;&gt;[HTF]&lt;/a&gt;.</source>
          <target state="translated">AdaBoost的核心原理是在重复修改的数据版本上适合一系列弱学习者（即，仅比随机猜测稍好一些的模型，例如小决策树）。然后，通过加权多数表决（或总和）将来自所有预测的预测进行组合以产生最终预测。在每个所谓的增强迭代中，数据修改包括对每个训练样本应用权重\（w_1 \），\（w_2 \），&amp;hellip;，\（w_N \）。最初，这些权重都设置为\（w_i = 1 / N \），因此第一步只是简单地在原始数据上训练弱学习者。对于每个连续的迭代，将分别修改样本权重，并将学习算法重新应用于重新加权的数据。在给定的步骤中在上一步中由增强模型错误预测的那些训练示例的权重增加了，而对于正确预测的那些权重则减小了。随着迭代的进行，难以预测的示例受到越来越多的影响。因此，每个随后的弱学习者都被迫专注于序列中先前的学习者遗漏的例子&lt;a href=&quot;#htf&quot; id=&quot;id10&quot;&gt;[HTF]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="81757ae28a30627b83ded3cb76d108f655541caf" translate="yes" xml:space="preserve">
          <source>The correlation between each regressor and the target is computed, that is, ((X[:, i] - mean(X[:, i])) * (y - mean_y)) / (std(X[:, i]) * std(y)).</source>
          <target state="translated">计算各回归因子与目标之间的相关性,即((X[:,i]-均值(X[:,i]))*(y-mean_y))/(std(X[:,i])*std(y))。</target>
        </trans-unit>
        <trans-unit id="302d596f93db30183d7b9abc1999d9cfb95f2d1f" translate="yes" xml:space="preserve">
          <source>The corresponding image is:</source>
          <target state="translated">对应的形象是:</target>
        </trans-unit>
        <trans-unit id="8f6395cc147644bb3051e08a46acbc9cd47145af" translate="yes" xml:space="preserve">
          <source>The cosine distance is defined as &lt;code&gt;1 - cosine_similarity&lt;/code&gt;: the lowest value is 0 (identical point) but it is bounded above by 2 for the farthest points. Its value does not depend on the norm of the vector points but only on their relative angles.</source>
          <target state="translated">余弦距离定义为 &lt;code&gt;1 - cosine_similarity&lt;/code&gt; ：最小值为0（相同点），但最远点的界限为2以上。它的值不取决于矢量点的范数，而仅取决于它们的相对角度。</target>
        </trans-unit>
        <trans-unit id="c4b95b481d689de7dde58522ad60a16deab9f841" translate="yes" xml:space="preserve">
          <source>The cosine distance is equivalent to the half the squared euclidean distance if each sample is normalized to unit norm</source>
          <target state="translated">如果每个样本都归一化为单位正态,则余弦距离相当于欧几里得距离平方的一半。</target>
        </trans-unit>
        <trans-unit id="de99d6d126cf03f253c85bcda8fea4fdd47045eb" translate="yes" xml:space="preserve">
          <source>The cost function of an isomap embedding is</source>
          <target state="translated">同构图嵌入的成本函数为</target>
        </trans-unit>
        <trans-unit id="40200d49debb9a752670d9f4e91dc77cd9d66f0a" translate="yes" xml:space="preserve">
          <source>The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.</source>
          <target state="translated">使用该树(即预测数据)的成本是对数,即用于训练该树的数据点数量。</target>
        </trans-unit>
        <trans-unit id="c9b6813659bffb0c08aeb974fcecdaaf97e786b0" translate="yes" xml:space="preserve">
          <source>The covariance matrix of a data set is known to be well approximated by the classical &lt;em&gt;maximum likelihood estimator&lt;/em&gt; (or &amp;ldquo;empirical covariance&amp;rdquo;), provided the number of observations is large enough compared to the number of features (the variables describing the observations). More precisely, the Maximum Likelihood Estimator of a sample is an unbiased estimator of the corresponding population&amp;rsquo;s covariance matrix.</source>
          <target state="translated">已知数据集的协方差矩阵可以通过经典&lt;em&gt;最大似然估计器&lt;/em&gt;（或&amp;ldquo;经验协方差&amp;rdquo;）很好地近似，前提是观测的数量要比特征的数量（描述观测的变量）大得多。更准确地说，样本的最大似然估计量是相应总体的协方差矩阵的无偏估计量。</target>
        </trans-unit>
        <trans-unit id="ff9747571dfcab155502f0d51c20ced0a4eda87c" translate="yes" xml:space="preserve">
          <source>The covariance matrix will be this value times the unit matrix. This dataset only produces symmetric normal distributions.</source>
          <target state="translated">协方差矩阵将是这个值乘以单位矩阵。这个数据集只产生对称正态分布。</target>
        </trans-unit>
        <trans-unit id="d4f25f1f739fb6455b136c8d6d48a91032e89970" translate="yes" xml:space="preserve">
          <source>The covariance of each mixture component. The shape depends on &lt;code&gt;covariance_type&lt;/code&gt;:</source>
          <target state="translated">每个混合物成分的协方差。形状取决于 &lt;code&gt;covariance_type&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="37e0aa90ea88b46d75d1f0f3b55b29aae27815f8" translate="yes" xml:space="preserve">
          <source>The covariance to compare with.</source>
          <target state="translated">的协方差来比较。</target>
        </trans-unit>
        <trans-unit id="a8664fdb20de58ef24fb2b254f60a073e2dad9f4" translate="yes" xml:space="preserve">
          <source>The cross decomposition module contains two main families of algorithms: the partial least squares (PLS) and the canonical correlation analysis (CCA).</source>
          <target state="translated">交叉分解模块包含两个主要的算法系列:部分最小二乘法(PLS)和规范相关分析(CCA)。</target>
        </trans-unit>
        <trans-unit id="5725093c72b0120c682574584b056d0b295efdd9" translate="yes" xml:space="preserve">
          <source>The cross validation score obtained on the training data</source>
          <target state="translated">在训练数据上获得的交叉验证得分。</target>
        </trans-unit>
        <trans-unit id="7d9ee6d7f0b44964f013a45e604e6c29cc8a3f8f" translate="yes" xml:space="preserve">
          <source>The cross-validation can then be performed easily:</source>
          <target state="translated">然后可以很容易地进行交叉验证。</target>
        </trans-unit>
        <trans-unit id="b91dc81955b1dbc1a750c7c0ae1e516f1cb5789e" translate="yes" xml:space="preserve">
          <source>The cross-validation score can be directly calculated using the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; helper. Given an estimator, the cross-validation object and the input dataset, the &lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt; splits the data repeatedly into a training and a testing set, trains the estimator using the training set and computes the scores based on the testing set for each iteration of cross-validation.</source>
          <target state="translated">可以使用&lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;帮助器直接计算交叉验证得分。给定一个估算器，交叉验证对象和输入数据集，&lt;a href=&quot;../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;将数据重复拆分为一个训练集和一个测试集，使用训练集训练该估算器，并根据测试集计算交叉算子的每次迭代的得分验证。</target>
        </trans-unit>
        <trans-unit id="0c9e67494c034c3aceceb77302443d5c2bb54301" translate="yes" xml:space="preserve">
          <source>The cross-validation scores such that &lt;code&gt;grid_scores_[i]&lt;/code&gt; corresponds to the CV score of the i-th subset of features.</source>
          <target state="translated">交叉验证得分，例如 &lt;code&gt;grid_scores_[i]&lt;/code&gt; 与第i个特征子集的CV得分相对应。</target>
        </trans-unit>
        <trans-unit id="7186db76e260a93ceff6e7a012a18028bce16f5b" translate="yes" xml:space="preserve">
          <source>The current implementation uses ball trees and kd-trees to determine the neighborhood of points, which avoids calculating the full distance matrix (as was done in scikit-learn versions before 0.14). The possibility to use custom metrics is retained; for details, see &lt;code&gt;NearestNeighbors&lt;/code&gt;.</source>
          <target state="translated">当前的实现使用球树和kd树来确定点的邻域，从而避免了计算完整距离矩阵（如在0.14之前的scikit-learn版本中所做的那样）。保留使用自定义指标的可能性；有关详细信息，请参见 &lt;code&gt;NearestNeighbors&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="e3ec89890503565d6b8dae28b11789379133ea78" translate="yes" xml:space="preserve">
          <source>The current loss computed with the loss function.</source>
          <target state="translated">用损失函数计算出的当前损失。</target>
        </trans-unit>
        <trans-unit id="a415ef6f77731ce51df5a94c5015135ebd3f462c" translate="yes" xml:space="preserve">
          <source>The curse of dimensionality</source>
          <target state="translated">维度的诅咒</target>
        </trans-unit>
        <trans-unit id="3eaad00bc0bba0577db9b826b646317a24a90409" translate="yes" xml:space="preserve">
          <source>The data</source>
          <target state="translated">数据</target>
        </trans-unit>
        <trans-unit id="973622d96c176a0a77b0dde8f6175466c5afec2c" translate="yes" xml:space="preserve">
          <source>The data is always a 2D array, shape &lt;code&gt;(n_samples, n_features)&lt;/code&gt;, although the original data may have had a different shape. In the case of the digits, each original sample is an image of shape &lt;code&gt;(8, 8)&lt;/code&gt; and can be accessed using:</source>
          <target state="translated">数据始终是2D数组，形状为 &lt;code&gt;(n_samples, n_features)&lt;/code&gt; ，尽管原始数据可能具有不同的形状。对于数字，每个原始样本都是形状为 &lt;code&gt;(8, 8)&lt;/code&gt; 的图像，可以使用以下方式访问：</target>
        </trans-unit>
        <trans-unit id="0e6af14908ee7619e613da95e5ef5e8434ef93d9" translate="yes" xml:space="preserve">
          <source>The data is assumed to be non-negative, and is often normalized to have an L1-norm of one. The normalization is rationalized with the connection to the chi squared distance, which is a distance between discrete probability distributions.</source>
          <target state="translated">假设数据是非负数,通常将其归一化,使其L1正态为1。归一化是合理化的,与chi平方距离有关,它是离散概率分布之间的距离。</target>
        </trans-unit>
        <trans-unit id="aa2161d468d436ce31e1b1a4a535d19d4acf7995" translate="yes" xml:space="preserve">
          <source>The data is generated with the &lt;code&gt;make_checkerboard&lt;/code&gt; function, then shuffled and passed to the Spectral Biclustering algorithm. The rows and columns of the shuffled matrix are rearranged to show the biclusters found by the algorithm.</source>
          <target state="translated">数据是使用 &lt;code&gt;make_checkerboard&lt;/code&gt; 函数生成的，然后经过混洗并传递给Spectral Biclustering算法。重新排列改组矩阵的行和列，以显示该算法找到的双曲线。</target>
        </trans-unit>
        <trans-unit id="549dd87c3787b6b83f2456e4cff9d8aa392d12c1" translate="yes" xml:space="preserve">
          <source>The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine.</source>
          <target state="translated">这些数据是对意大利同一地区三个不同种植者种植的葡萄酒进行化学分析的结果。对这三种葡萄酒中的不同成分进行了13次不同的测量。</target>
        </trans-unit>
        <trans-unit id="279fbf7584e00e713412e8ff4546300536ffeb5a" translate="yes" xml:space="preserve">
          <source>The data matrix</source>
          <target state="translated">数据矩阵</target>
        </trans-unit>
        <trans-unit id="a36ea8c8057a9ae699a69f099ee316c3295afaf3" translate="yes" xml:space="preserve">
          <source>The data matrix, with p features and n samples. The data set must be the one which was used to compute the raw estimates.</source>
          <target state="translated">数据矩阵,有p个特征和n个样本。数据集必须是用于计算原始估计的数据。</target>
        </trans-unit>
        <trans-unit id="f0ed1480aeed96966c83d245c0e82839723677ea" translate="yes" xml:space="preserve">
          <source>The data matrix.</source>
          <target state="translated">数据矩阵。</target>
        </trans-unit>
        <trans-unit id="70cee63c42777050ee29bb5c41f5ed813382b6c9" translate="yes" xml:space="preserve">
          <source>The data of the returned sparse matrix. By default it is int</source>
          <target state="translated">返回的稀疏矩阵的数据。默认情况下是int</target>
        </trans-unit>
        <trans-unit id="13b8870b16632bb144f99a987da0a17b912fd261" translate="yes" xml:space="preserve">
          <source>The data of the returned sparse matrix. By default it is the dtype of img</source>
          <target state="translated">返回的稀疏矩阵的数据。默认情况下是img的dtype。</target>
        </trans-unit>
        <trans-unit id="a9c1f6d9c961581c21421066f6ce8f7c709084a9" translate="yes" xml:space="preserve">
          <source>The data on which &lt;code&gt;gbrt&lt;/code&gt; was trained.</source>
          <target state="translated">训练 &lt;code&gt;gbrt&lt;/code&gt; 的数据。</target>
        </trans-unit>
        <trans-unit id="c66d20ce5405cff4e02a00c321afd4016f52379a" translate="yes" xml:space="preserve">
          <source>The data on which &lt;code&gt;gbrt&lt;/code&gt; was trained. It is used to generate a &lt;code&gt;grid&lt;/code&gt; for the &lt;code&gt;target_variables&lt;/code&gt;. The &lt;code&gt;grid&lt;/code&gt; comprises &lt;code&gt;grid_resolution&lt;/code&gt; equally spaced points between the two &lt;code&gt;percentiles&lt;/code&gt;.</source>
          <target state="translated">训练 &lt;code&gt;gbrt&lt;/code&gt; 的数据。它用于为 &lt;code&gt;target_variables&lt;/code&gt; 生成 &lt;code&gt;grid&lt;/code&gt; 。所述 &lt;code&gt;grid&lt;/code&gt; 包括 &lt;code&gt;grid_resolution&lt;/code&gt; 等距隔开所述两个点之间 &lt;code&gt;percentiles&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="65c0e8f3b46fedbf2f41b7e1589997d8de30db77" translate="yes" xml:space="preserve">
          <source>The data set contains images of hand-written digits: 10 classes where each class refers to a digit.</source>
          <target state="translated">该数据集包含手写数字的图像。10个等级,每个等级指的是一个数字。</target>
        </trans-unit>
        <trans-unit id="a49ccf8db4843f7af3fe75d2e385d4c5ae4a247b" translate="yes" xml:space="preserve">
          <source>The data that should be scaled.</source>
          <target state="translated">应该缩放的数据。</target>
        </trans-unit>
        <trans-unit id="4362ce6dec3d09ef8de03aa7af16c30845dd1b85" translate="yes" xml:space="preserve">
          <source>The data that should be transformed back.</source>
          <target state="translated">应转化回来的数据。</target>
        </trans-unit>
        <trans-unit id="47ba75ee664e21a51401b4c1203a08e7a876f44e" translate="yes" xml:space="preserve">
          <source>The data to be transformed by subset.</source>
          <target state="translated">按子集变换的数据。</target>
        </trans-unit>
        <trans-unit id="03bb048cfbbc3212fd60edf266a3822d3176ef87" translate="yes" xml:space="preserve">
          <source>The data to be transformed using a power transformation.</source>
          <target state="translated">要使用功率变换的数据。</target>
        </trans-unit>
        <trans-unit id="73ce115221fb870e05c90b8f043ca278fdee74a1" translate="yes" xml:space="preserve">
          <source>The data to be transformed.</source>
          <target state="translated">要转换的数据。</target>
        </trans-unit>
        <trans-unit id="df7f442215a9627450351a5239ccb7b837681c69" translate="yes" xml:space="preserve">
          <source>The data to binarize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">scipy.sparse矩阵应该是CSR格式,以避免不必要的复制。</target>
        </trans-unit>
        <trans-unit id="5401bf0fdb954957c9a3f345344e508dbd6bac54" translate="yes" xml:space="preserve">
          <source>The data to binarize, element by element. scipy.sparse matrices should be in CSR or CSC format to avoid an un-necessary copy.</source>
          <target state="translated">scipy.sparse矩阵应该是CSR或CSC格式,以避免不必要的复制。</target>
        </trans-unit>
        <trans-unit id="515108ae5aad51ada5b234ccff65d93cdd42711e" translate="yes" xml:space="preserve">
          <source>The data to center and scale.</source>
          <target state="translated">的数据,以中心和规模。</target>
        </trans-unit>
        <trans-unit id="975edd1be086184330a8a3ebbf935588408d4a98" translate="yes" xml:space="preserve">
          <source>The data to determine the categories of each feature.</source>
          <target state="translated">的数据来确定每个特征的类别。</target>
        </trans-unit>
        <trans-unit id="a77a7e1c81043cc97f700ad0d213253f486b563a" translate="yes" xml:space="preserve">
          <source>The data to encode.</source>
          <target state="translated">要编码的数据。</target>
        </trans-unit>
        <trans-unit id="d843c1d7e5e8986d5ad23251cff2a94ef8ee7955" translate="yes" xml:space="preserve">
          <source>The data to fit.</source>
          <target state="translated">拟合的数据。</target>
        </trans-unit>
        <trans-unit id="e7fbe72ba2fcf8fff679b5e28b0fef9589e7589d" translate="yes" xml:space="preserve">
          <source>The data to fit. Can be for example a list, or an array.</source>
          <target state="translated">要拟合的数据。例如,可以是一个列表,或一个数组。</target>
        </trans-unit>
        <trans-unit id="00725d5de44209593a99040fefb16433276154c2" translate="yes" xml:space="preserve">
          <source>The data to fit. Can be, for example a list, or an array at least 2d.</source>
          <target state="translated">拟合的数据。可以是,例如一个列表,或者一个至少2d的数组。</target>
        </trans-unit>
        <trans-unit id="53846ac28d489b02b9949ef562a942f522e52d48" translate="yes" xml:space="preserve">
          <source>The data to normalize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">scipy.sparse矩阵应该是CSR格式,以避免不必要的复制。</target>
        </trans-unit>
        <trans-unit id="c13e1ef3ee3dcba79bb567a64bd572c32814ce01" translate="yes" xml:space="preserve">
          <source>The data to normalize, row by row. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.</source>
          <target state="translated">scipy.sparse矩阵应该是CSR格式,以避免不必要的复制。</target>
        </trans-unit>
        <trans-unit id="a092ffb0fb8aec81b90c0802c73b2ae6cfa80dcc" translate="yes" xml:space="preserve">
          <source>The data to transform, row by row. Sparse input should preferably be in CSC format.</source>
          <target state="translated">要转换的数据,逐行转换。稀疏输入最好是CSC格式。</target>
        </trans-unit>
        <trans-unit id="3f0555e5cf2c3f2eb80a653fe1f67911e223ec90" translate="yes" xml:space="preserve">
          <source>The data to transform.</source>
          <target state="translated">要转换的数据。</target>
        </trans-unit>
        <trans-unit id="f0a016c9443bbddef85796fd3691360a3c817ea0" translate="yes" xml:space="preserve">
          <source>The data used to compute the mean and standard deviation used for later scaling along the features axis.</source>
          <target state="translated">用于计算平均值和标准差的数据,用于以后沿着特征轴进行缩放。</target>
        </trans-unit>
        <trans-unit id="55438370e51e74e9e8aa5e6ed6a37b2c111bd898" translate="yes" xml:space="preserve">
          <source>The data used to compute the median and quantiles used for later scaling along the features axis.</source>
          <target state="translated">用于计算中位数和量子数的数据,用于以后沿着特征轴进行缩放。</target>
        </trans-unit>
        <trans-unit id="0ae3bec0fd19bdece5355d54cf80d1ae7bcc7c26" translate="yes" xml:space="preserve">
          <source>The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.</source>
          <target state="translated">用于计算每个特征的最小值和最大值的数据,用于以后沿特征轴的缩放。</target>
        </trans-unit>
        <trans-unit id="e26762b4f27727d3210d3fe2a64ad24b366c1062" translate="yes" xml:space="preserve">
          <source>The data used to estimate the optimal transformation parameters.</source>
          <target state="translated">用于估计最佳变换参数的数据。</target>
        </trans-unit>
        <trans-unit id="56c320de81ba4f246c360453cbd2da4e63d63c7f" translate="yes" xml:space="preserve">
          <source>The data used to fit the model. If &lt;code&gt;copy_X=False&lt;/code&gt;, then &lt;code&gt;X_fit_&lt;/code&gt; is a reference. This attribute is used for the calls to transform.</source>
          <target state="translated">用于拟合模型的数据。如果 &lt;code&gt;copy_X=False&lt;/code&gt; ，则 &lt;code&gt;X_fit_&lt;/code&gt; 是引用。此属性用于进行转换的调用。</target>
        </trans-unit>
        <trans-unit id="03376550c822ad450e2d14840686210c4804ebc6" translate="yes" xml:space="preserve">
          <source>The data used to scale along the features axis.</source>
          <target state="translated">用于沿特征轴缩放的数据。</target>
        </trans-unit>
        <trans-unit id="c4374e3fd9ee0863ad791a18672a80272e651c9f" translate="yes" xml:space="preserve">
          <source>The data used to scale along the features axis. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csc_matrix&lt;/code&gt;. Additionally, the sparse matrix needs to be nonnegative if &lt;code&gt;ignore_implicit_zeros&lt;/code&gt; is False.</source>
          <target state="translated">用于沿要素轴缩放的数据。如果提供了稀疏矩阵，它将被转换为稀疏 &lt;code&gt;csc_matrix&lt;/code&gt; 。此外，如果 &lt;code&gt;ignore_implicit_zeros&lt;/code&gt; 为False ，则稀疏矩阵需要为非负数。</target>
        </trans-unit>
        <trans-unit id="848647eb355f23af31ef1bf06cbb1e0d945eea47" translate="yes" xml:space="preserve">
          <source>The data used to scale along the specified axis.</source>
          <target state="translated">沿着指定的轴缩放的数据。</target>
        </trans-unit>
        <trans-unit id="db728424571c6d0aa73c940cbe613870747b4067" translate="yes" xml:space="preserve">
          <source>The data was used with many others for comparing various classifiers. The classes are separable, though only RDA has achieved 100% correct classification. (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) (All results using the leave-one-out technique)</source>
          <target state="translated">该数据与许多其他数据一起用于比较各种分类器。虽然只有RDA实现了100%的正确分类,但类是可以分离的。(RDA:100%,QDA 99.4%,LDA 98.9%,1NN 96.1% (z-transformed data))(所有结果均采用 &quot;留一 &quot;技术)</target>
        </trans-unit>
        <trans-unit id="660fe59aa4f1107f7d968ac4c0471ddbff4317b5" translate="yes" xml:space="preserve">
          <source>The data.</source>
          <target state="translated">数据:</target>
        </trans-unit>
        <trans-unit id="02bda070eb404181c9796aadeceacf78aac05356" translate="yes" xml:space="preserve">
          <source>The dataset can be compressed, either by removing exact duplicates if these occur in your data, or by using BIRCH. Then you only have a relatively small number of representatives for a large number of points. You can then provide a &lt;code&gt;sample_weight&lt;/code&gt; when fitting DBSCAN.</source>
          <target state="translated">可以压缩数据集，方法是删除精确的重复项（如果这些重复项出现在数据中），或者使用BIRCH。这样，对于大量的点，您只有相对较少的代表。然后，在拟合DBSCAN时可以提供 &lt;code&gt;sample_weight&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="6bbac4d66b74c67e38ab2b8e6e075ffda05e6522" translate="yes" xml:space="preserve">
          <source>The dataset is called &amp;ldquo;Twenty Newsgroups&amp;rdquo;. Here is the official description, quoted from the &lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;website&lt;/a&gt;:</source>
          <target state="translated">该数据集称为&amp;ldquo;二十个新闻组&amp;rdquo;。这是官方的描述，引用自&lt;a href=&quot;http://people.csail.mit.edu/jrennie/20Newsgroups/&quot;&gt;网站&lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="c4573e45f21f00151e7512eaf1b6ae1cd41e22bd" translate="yes" xml:space="preserve">
          <source>The dataset is from Zhu et al [1].</source>
          <target state="translated">数据集来自Zhu等人[1]。</target>
        </trans-unit>
        <trans-unit id="1a602d6b3832c4acaa760453be6cb4d26914f03f" translate="yes" xml:space="preserve">
          <source>The dataset is generated using the &lt;code&gt;make_biclusters&lt;/code&gt; function, which creates a matrix of small values and implants bicluster with large values. The rows and columns are then shuffled and passed to the Spectral Co-Clustering algorithm. Rearranging the shuffled matrix to make biclusters contiguous shows how accurately the algorithm found the biclusters.</source>
          <target state="translated">该数据集是使用 &lt;code&gt;make_biclusters&lt;/code&gt; 函数生成的，该函数创建一个较小值的矩阵并植入具有较大值的bicluster。然后将行和列混洗，并传递给&amp;ldquo;光谱共聚&amp;rdquo;算法。重新排列改组后的矩阵以使双簇相邻，这表明算法找到双簇的准确度。</target>
        </trans-unit>
        <trans-unit id="4131a4e690502c19383d02f0465052ba8df886e1" translate="yes" xml:space="preserve">
          <source>The dataset is structured such that points nearby in index order are nearby in parameter space, leading to an approximately block-diagonal matrix of K-nearest neighbors. Such a sparse graph is useful in a variety of circumstances which make use of spatial relationships between points for unsupervised learning: in particular, see &lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt;&lt;code&gt;sklearn.manifold.Isomap&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt;&lt;code&gt;sklearn.manifold.LocallyLinearEmbedding&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt;&lt;code&gt;sklearn.cluster.SpectralClustering&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">数据集的结构使得索引顺序附近的点在参数空间中附近，从而导致K近邻的近似块对角矩阵。这种稀疏图在各种情况下非常有用，这些情况利用点之间的空间关系进行无监督学习：尤其是，请参见&lt;a href=&quot;generated/sklearn.manifold.isomap#sklearn.manifold.Isomap&quot;&gt; &lt;code&gt;sklearn.manifold.Isomap&lt;/code&gt; &lt;/a&gt;，&lt;a href=&quot;generated/sklearn.manifold.locallylinearembedding#sklearn.manifold.LocallyLinearEmbedding&quot;&gt; &lt;code&gt;sklearn.manifold.LocallyLinearEmbedding&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering&quot;&gt; &lt;code&gt;sklearn.cluster.SpectralClustering&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="774649dea26c780c5e43d2464445b85ef137fcca" translate="yes" xml:space="preserve">
          <source>The dataset is the Boston Housing dataset (resp. 20 Newsgroups) for regression (resp. classification).</source>
          <target state="translated">数据集是波士顿住房数据集(简称20个新闻组),用于回归(简称分类)。</target>
        </trans-unit>
        <trans-unit id="c0f7ef482ab49522d3cd87d64587ff804f6727c1" translate="yes" xml:space="preserve">
          <source>The dataset used for evaluation is a 2D grid of isotropic Gaussian clusters widely spaced.</source>
          <target state="translated">用于评估的数据集是一个间距很宽的各向同性高斯簇的二维网格。</target>
        </trans-unit>
        <trans-unit id="c07599d3a55d8254ba468cc109830533370f5dcf" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is Reuters-21578 as provided by the UCI ML repository. It will be automatically downloaded and uncompressed on first run.</source>
          <target state="translated">本例中使用的数据集是由UCI ML资源库提供的Reuters-21578。它将在第一次运行时自动下载并解压。</target>
        </trans-unit>
        <trans-unit id="f610543312a82d7ead69951d61b82950d647ad3c" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is a preprocessed excerpt of the &amp;ldquo;Labeled Faces in the Wild&amp;rdquo;, aka &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;:</source>
          <target state="translated">本示例中使用的数据集是&amp;ldquo;野外标记面孔&amp;rdquo;（也称为&lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;）的预处理摘录：</target>
        </trans-unit>
        <trans-unit id="257296081e1c4b1a90a4ff7a918d81d5746f6705" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is a preprocessed excerpt of the &amp;ldquo;Labeled Faces in the Wild&amp;rdquo;, also known as &lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;:</source>
          <target state="translated">本示例中使用的数据集是&amp;ldquo;野外标记面孔&amp;rdquo;（也称为&lt;a href=&quot;http://vis-www.cs.umass.edu/lfw/&quot;&gt;LFW&lt;/a&gt;）的预处理摘录：</target>
        </trans-unit>
        <trans-unit id="0e491b7a83df0aa734f40ae016e056d9d5ef3d92" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is the 20 newsgroups dataset which will be automatically downloaded and then cached and reused for the document classification example.</source>
          <target state="translated">本例中使用的数据集是20个新闻组数据集,该数据集将被自动下载,然后缓存并重新用于文档分类示例。</target>
        </trans-unit>
        <trans-unit id="b391209f39032c4c37a7d267548501e64198b514" translate="yes" xml:space="preserve">
          <source>The dataset used in this example is the 20 newsgroups dataset. It will be automatically downloaded, then cached.</source>
          <target state="translated">本例中使用的数据集是 20 个新闻组数据集。它将被自动下载,然后缓存。</target>
        </trans-unit>
        <trans-unit id="b6261e60f08853c79c52801fadd1554784335b15" translate="yes" xml:space="preserve">
          <source>The dataset used is the Wine Dataset available at UCI. This dataset has continuous features that are heterogeneous in scale due to differing properties that they measure (i.e alcohol content, and malic acid).</source>
          <target state="translated">使用的数据集是UCI提供的Wine Dataset。这个数据集具有连续的特征,由于它们测量的属性不同(即酒精含量和苹果酸),因此在规模上是异质的。</target>
        </trans-unit>
        <trans-unit id="2320c3b81471635dac46a3209d55417f5e2427c2" translate="yes" xml:space="preserve">
          <source>The dataset will be downloaded from the &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume5/lewis04a/&quot;&gt;rcv1 homepage&lt;/a&gt; if necessary. The compressed size is about 656 MB.</source>
          <target state="translated">如有必要，将从&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume5/lewis04a/&quot;&gt;rcv1主页&lt;/a&gt;下载数据集。压缩后的大小约为656 MB。</target>
        </trans-unit>
        <trans-unit id="a104c196a5a6b4961911da3fe75edb0c176425f0" translate="yes" xml:space="preserve">
          <source>The datasets also contain a full description in their &lt;code&gt;DESCR&lt;/code&gt; attribute and some contain &lt;code&gt;feature_names&lt;/code&gt; and &lt;code&gt;target_names&lt;/code&gt;. See the dataset descriptions below for details.</source>
          <target state="translated">数据集的 &lt;code&gt;DESCR&lt;/code&gt; 属性中也包含完整的描述，而某些数据集包含 &lt;code&gt;feature_names&lt;/code&gt; 和 &lt;code&gt;target_names&lt;/code&gt; 。有关详细信息，请参见下面的数据集描述。</target>
        </trans-unit>
        <trans-unit id="f5da29d39d3005818dbe52be4adbd180f7105540" translate="yes" xml:space="preserve">
          <source>The decision function is:</source>
          <target state="translated">决定功能是:</target>
        </trans-unit>
        <trans-unit id="c0e6f8aea5cc6c4291ed60eb8cccabc092233917" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The columns correspond to the classes in sorted order, as they appear in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">输入样本的决策函数。列按类别顺序对应于类，因为它们出现在属性 &lt;code&gt;classes_&lt;/code&gt; 中。回归和二进制分类是 &lt;code&gt;k == 1&lt;/code&gt; 特殊情况，否则 &lt;code&gt;k==n_classes&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="06ac27333a17bb0e5a70b7bdd7a4a5eafb52284d" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of outputs is the same of that of the &lt;code&gt;classes_&lt;/code&gt; attribute. Binary classification is a special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;. For binary classification, values closer to -1 or 1 mean more like the first or second class in &lt;code&gt;classes_&lt;/code&gt;, respectively.</source>
          <target state="translated">输入样本的决策函数。输出顺序与 &lt;code&gt;classes_&lt;/code&gt; 属性的顺序相同。二进制分类是 &lt;code&gt;k == 1&lt;/code&gt; 的特殊情况，否则 &lt;code&gt;k==n_classes&lt;/code&gt; 。对于二进制分类，更接近-1或1的值分别意味着分别 &lt;code&gt;classes_&lt;/code&gt; 的第一类或第二类。</target>
        </trans-unit>
        <trans-unit id="271144c81e33420b0281bbd1b4c29fbb1374c6ac" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification are special cases with &lt;code&gt;k == 1&lt;/code&gt;, otherwise &lt;code&gt;k==n_classes&lt;/code&gt;.</source>
          <target state="translated">输入样本的决策函数。类的顺序与属性 &lt;code&gt;classes_&lt;/code&gt; 中的顺序相对应。回归和二进制分类是 &lt;code&gt;k == 1&lt;/code&gt; 特殊情况，否则 &lt;code&gt;k==n_classes&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c04fb9b1c64a374fbc2424dc1dc2e69edec92fae" translate="yes" xml:space="preserve">
          <source>The decision function of the input samples. The order of the classes corresponds to that in the attribute &lt;code&gt;classes_&lt;/code&gt;. Regression and binary classification produce an array of shape [n_samples].</source>
          <target state="translated">输入样本的决策函数。类的顺序与属性 &lt;code&gt;classes_&lt;/code&gt; 中的顺序相对应。回归和二进制分类产生一个形状为[n_samples]的数组。</target>
        </trans-unit>
        <trans-unit id="7a4d1c2d06404b468f740213ed27426daa73066a" translate="yes" xml:space="preserve">
          <source>The decision rule for Bernoulli naive Bayes is based on</source>
          <target state="translated">Bernoulli naive Bayes的决策规则是基于</target>
        </trans-unit>
        <trans-unit id="0e0b3ca71cdfb4dae3b0e05518ff98bb98da9e44" translate="yes" xml:space="preserve">
          <source>The decision tree structure can be analysed to gain further insight on the relation between the features and the target to predict. In this example, we show how to retrieve:</source>
          <target state="translated">可以对决策树结构进行分析,以进一步了解特征和预测目标之间的关系。在这个例子中,我们展示了如何检索。</target>
        </trans-unit>
        <trans-unit id="7930dea6ad7960124a06f25452c69dc408a6af03" translate="yes" xml:space="preserve">
          <source>The decision tree to be exported to GraphViz.</source>
          <target state="translated">要输出到GraphViz的决策树。</target>
        </trans-unit>
        <trans-unit id="41d54cbac8ff4bb674faa62fe3a273da70a2c8ec" translate="yes" xml:space="preserve">
          <source>The decision values for the samples are computed by adding the normalized sum of pair-wise classification confidence levels to the votes in order to disambiguate between the decision values when the votes for all the classes are equal leading to a tie.</source>
          <target state="translated">当所有类的票数相同导致平局时,为了在决定值之间进行混淆,样本的决定值是通过对票数加一对分类置信度的归一化和来计算的。</target>
        </trans-unit>
        <trans-unit id="9db4a0cde49703606f721a9d732403d53161167d" translate="yes" xml:space="preserve">
          <source>The decoding strategy depends on the vectorizer parameters.</source>
          <target state="translated">解码策略取决于矢量器参数。</target>
        </trans-unit>
        <trans-unit id="8ab5c7033c87e7cea9882c536950d8e60cf30550" translate="yes" xml:space="preserve">
          <source>The default coding of images is based on the &lt;code&gt;uint8&lt;/code&gt; dtype to spare memory. Often machine learning algorithms work best if the input is converted to a floating point representation first. Also, if you plan to use &lt;code&gt;matplotlib.pyplpt.imshow&lt;/code&gt; don&amp;rsquo;t forget to scale to the range 0 - 1 as done in the following example.</source>
          <target state="translated">图像的默认编码基于 &lt;code&gt;uint8&lt;/code&gt; dtype来备用。如果首先将输入转换为浮点表示形式，则机器学习算法通常效果最好。另外，如果您打算使用 &lt;code&gt;matplotlib.pyplpt.imshow&lt;/code&gt; ,请不要忘记将其缩放到0-1的范围，如以下示例所示。</target>
        </trans-unit>
        <trans-unit id="ec537a685ef4f8a1d5936e658cd9e5b93a584d77" translate="yes" xml:space="preserve">
          <source>The default configuration tokenizes the string by extracting words of at least 2 letters. The specific function that does this step can be requested explicitly:</source>
          <target state="translated">默认的配置是通过提取至少两个字母的单词来标记字符串。这一步的具体功能可以明确要求。</target>
        </trans-unit>
        <trans-unit id="3d025c574ea3cc62ecf87d5b0a9f83d7b3c75d6a" translate="yes" xml:space="preserve">
          <source>The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module &lt;a href=&quot;../classes#module-sklearn.model_selection&quot;&gt;&lt;code&gt;sklearn.model_selection&lt;/code&gt;&lt;/a&gt; module for the list of possible cross-validation objects.</source>
          <target state="translated">使用的默认交叉验证生成器是&amp;ldquo;分层K折&amp;rdquo;。如果提供整数，则为使用的折叠数。有关可能的交叉验证对象的列表，请参见模块&lt;a href=&quot;../classes#module-sklearn.model_selection&quot;&gt; &lt;code&gt;sklearn.model_selection&lt;/code&gt; &lt;/a&gt;模块。</target>
        </trans-unit>
        <trans-unit id="53050da8c5d49a141b5e2d80a70c0fa67fd7eb00" translate="yes" xml:space="preserve">
          <source>The default dataset is the digits dataset. To run the example on the twenty newsgroups dataset, pass the &amp;ndash;twenty-newsgroups command line argument to this script.</source>
          <target state="translated">默认数据集是数字数据集。要在20个新闻组数据集中运行该示例，请将&amp;ndash;twenty-newsgroups命令行参数传递给此脚本。</target>
        </trans-unit>
        <trans-unit id="713044bd0fefe6c8ff38e7a00814412176e5caf0" translate="yes" xml:space="preserve">
          <source>The default error message is, &amp;ldquo;This %(name)s instance is not fitted yet. Call &amp;lsquo;fit&amp;rsquo; with appropriate arguments before using this method.&amp;rdquo;</source>
          <target state="translated">默认错误消息是&amp;ldquo;此％（name）s实例尚未安装。使用此方法之前，请先使用适当的参数调用'fit'。&amp;rdquo;</target>
        </trans-unit>
        <trans-unit id="bd1a08c69ea4f3f536ea9f567759a2eca8958b5b" translate="yes" xml:space="preserve">
          <source>The default parameters (n_samples / n_features / n_components) should make the example runnable in a couple of tens of seconds. You can try to increase the dimensions of the problem, but be aware that the time complexity is polynomial in NMF. In LDA, the time complexity is proportional to (n_samples * iterations).</source>
          <target state="translated">默认参数(n_samples/n_features/n_components)应该可以让这个例子在几十秒内运行。你可以尝试增加问题的维度,但要注意在NMF中,时间复杂度是多项式的。在LDA中,时间复杂度与(n_samples *迭代)成正比。</target>
        </trans-unit>
        <trans-unit id="e964bf4dcf7d784d0dee842886fe27c5060b1af1" translate="yes" xml:space="preserve">
          <source>The default setting is &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt;. The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter &lt;code&gt;l1_ratio&lt;/code&gt; controls the convex combination of L1 and L2 penalty.</source>
          <target state="translated">默认设置为 &lt;code&gt;penalty=&quot;l2&quot;&lt;/code&gt; 。L1惩罚导致解决方案稀疏，将大多数系数驱动为零。当存在高度相关的属性时，Elastic Net解决了L1惩罚的一些缺陷。参数 &lt;code&gt;l1_ratio&lt;/code&gt; 控制L1和L2惩罚的凸组合。</target>
        </trans-unit>
        <trans-unit id="d600d29df52c52da3fc6f38fdcf73036ecf8cbdf" translate="yes" xml:space="preserve">
          <source>The default slice is a rectangular shape around the face, removing most of the background:</source>
          <target state="translated">默认的切片是围绕脸部的矩形,去除大部分背景。</target>
        </trans-unit>
        <trans-unit id="f2f8fa0d6181b12438bd7660924ca4c2f89302ad" translate="yes" xml:space="preserve">
          <source>The default solver is &amp;lsquo;svd&amp;rsquo;. It can perform both classification and transform, and it does not rely on the calculation of the covariance matrix. This can be an advantage in situations where the number of features is large. However, the &amp;lsquo;svd&amp;rsquo; solver cannot be used with shrinkage.</source>
          <target state="translated">默认求解器为&amp;ldquo; svd&amp;rdquo;。它可以执行分类和变换，并且不依赖于协方差矩阵的计算。在功能数量很多的情况下，这可能是一个优势。但是，&amp;ldquo; svd&amp;rdquo;求解器不能与收缩一起使用。</target>
        </trans-unit>
        <trans-unit id="2307e8c1575f885543e9c12c3e37adbd9d66c3dd" translate="yes" xml:space="preserve">
          <source>The default strategy implements one step of the bootstrapping procedure.</source>
          <target state="translated">默认策略实现了引导程序的一个步骤。</target>
        </trans-unit>
        <trans-unit id="f17725906dcf13fd5a8abccea776a77c1270d7a9" translate="yes" xml:space="preserve">
          <source>The default value &lt;code&gt;max_features=&quot;auto&quot;&lt;/code&gt; uses &lt;code&gt;n_features&lt;/code&gt; rather than &lt;code&gt;n_features / 3&lt;/code&gt;. The latter was originally suggested in [1], whereas the former was more recently justified empirically in [2].</source>
          <target state="translated">默认值 &lt;code&gt;max_features=&quot;auto&quot;&lt;/code&gt; 使用 &lt;code&gt;n_features&lt;/code&gt; 而不是 &lt;code&gt;n_features / 3&lt;/code&gt; 。后者最初是在[1]中提出的，而前者最近在经验上是合理的[2]。</target>
        </trans-unit>
        <trans-unit id="8ad5c264779356671425d0f732ca8de7347758f3" translate="yes" xml:space="preserve">
          <source>The default values for the parameters controlling the size of the trees (e.g. &lt;code&gt;max_depth&lt;/code&gt;, &lt;code&gt;min_samples_leaf&lt;/code&gt;, etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values.</source>
          <target state="translated">控制树大小的参数的默认值（例如 &lt;code&gt;max_depth&lt;/code&gt; ， &lt;code&gt;min_samples_leaf&lt;/code&gt; 等）会导致树的完全生长和未修剪，这在某些数据集上可能非常大。为了减少内存消耗，应通过设置这些参数值来控制树的复杂性和大小。</target>
        </trans-unit>
        <trans-unit id="4209f696edb2b5615bc39c07cc72cc1b347ebd54" translate="yes" xml:space="preserve">
          <source>The definitive description of key concepts and API elements for using scikit-learn and developing compatible tools.</source>
          <target state="translated">使用scikit-learn和开发兼容工具的关键概念和API元素的权威描述。</target>
        </trans-unit>
        <trans-unit id="767ee5df767908c4f8729c932c4a3d808fe558cd" translate="yes" xml:space="preserve">
          <source>The degree of the polynomial features. Default = 2.</source>
          <target state="translated">多项式特征的度数。默认值=2。</target>
        </trans-unit>
        <trans-unit id="c0ae4038f8b612638e22a63002b9a0592ebf4ed6" translate="yes" xml:space="preserve">
          <source>The density of w, between 0 and 1</source>
          <target state="translated">w的密度,在0和1之间</target>
        </trans-unit>
        <trans-unit id="64211b28a272c8cc95fc17412f0b29158a9e57aa" translate="yes" xml:space="preserve">
          <source>The desired absolute tolerance of the result. A larger tolerance will generally lead to faster execution. Default is 0.</source>
          <target state="translated">所需结果的绝对公差。较大的容差通常会导致更快的执行速度。默认值为0。</target>
        </trans-unit>
        <trans-unit id="d976f3ec62dae27876361a8dc74b8b1f3089859a" translate="yes" xml:space="preserve">
          <source>The desired relative tolerance of the result. A larger tolerance will generally lead to faster execution. Default is 1E-8.</source>
          <target state="translated">所需结果的相对容差。较大的容差通常会导致更快的执行速度。默认值是1E-8。</target>
        </trans-unit>
        <trans-unit id="e06750706d15ac4ccfc7d440948abe624b2a5a8e" translate="yes" xml:space="preserve">
          <source>The diabetes dataset consists of 10 physiological variables (age, sex, weight, blood pressure) measure on 442 patients, and an indication of disease progression after one year:</source>
          <target state="translated">糖尿病数据集包括对442名患者的10个生理变量(年龄、性别、体重、血压)的测量,以及一年后疾病进展的指示。</target>
        </trans-unit>
        <trans-unit id="9fc4977037d4a4b0eddabbe726bd2d0de7d06481" translate="yes" xml:space="preserve">
          <source>The dict at &lt;code&gt;search.cv_results_['params'][search.best_index_]&lt;/code&gt; gives the parameter setting for the best model, that gives the highest mean score (&lt;code&gt;search.best_score_&lt;/code&gt;).</source>
          <target state="translated">&lt;code&gt;search.cv_results_['params'][search.best_index_]&lt;/code&gt; 上的字典给出了最佳模型的参数设置，该模型给出了最高的平均分数（ &lt;code&gt;search.best_score_&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="bfaa3d320392a9f6c8addb9130a129b654790105" translate="yes" xml:space="preserve">
          <source>The dictionary atoms used for sparse coding. Lines are assumed to be normalized to unit norm.</source>
          <target state="translated">用于稀疏编码的字典原子。假设行数标准化为单位标准。</target>
        </trans-unit>
        <trans-unit id="7373631cc038017694fe3a2fdfb8d4bfc8e42605" translate="yes" xml:space="preserve">
          <source>The dictionary factor in the matrix factorization.</source>
          <target state="translated">矩阵分解中的字典因子。</target>
        </trans-unit>
        <trans-unit id="09a0fc68f904b631d6aa1871e7b81e42ac764443" translate="yes" xml:space="preserve">
          <source>The dictionary is fitted on the distorted left half of the image, and subsequently used to reconstruct the right half. Note that even better performance could be achieved by fitting to an undistorted (i.e. noiseless) image, but here we start from the assumption that it is not available.</source>
          <target state="translated">词典被拟合在失真的左半部分图像上,随后用于重建右半部分图像。请注意,通过对未扭曲(即无噪声)的图像进行拟合,可以获得更好的性能,但这里我们从假设它不可用开始。</target>
        </trans-unit>
        <trans-unit id="03531d72d72ed916646e794ac15c0622fe9c9b59" translate="yes" xml:space="preserve">
          <source>The dictionary learning objects offer, via the &lt;code&gt;split_code&lt;/code&gt; parameter, the possibility to separate the positive and negative values in the results of sparse coding. This is useful when dictionary learning is used for extracting features that will be used for supervised learning, because it allows the learning algorithm to assign different weights to negative loadings of a particular atom, from to the corresponding positive loading.</source>
          <target state="translated">字典学习对象通过 &lt;code&gt;split_code&lt;/code&gt; 参数提供了将稀疏编码结果中的正值和负值分开的可能性。当词典学习用于提取将用于监督学习的功能时，这很有用，因为它允许学习算法将特定原子的负负载从不同的权重分配给相应的正负载。</target>
        </trans-unit>
        <trans-unit id="c5012873b1ea68e5deed2b04cdb4e90687b2c340" translate="yes" xml:space="preserve">
          <source>The dictionary matrix against which to solve the sparse coding of the data. Some of the algorithms assume normalized rows for meaningful output.</source>
          <target state="translated">对照字典矩阵来解决数据的稀疏编码。一些算法假定标准化的行为有意义的输出。</target>
        </trans-unit>
        <trans-unit id="f137f3a53fdf8d92f7463e89f6383b5a88f3c320" translate="yes" xml:space="preserve">
          <source>The dictionary with normalized components (D).</source>
          <target state="translated">归一化成分的字典(D)。</target>
        </trans-unit>
        <trans-unit id="d49d4bb0bf7407d291ade3beb50a3cbdf55548f8" translate="yes" xml:space="preserve">
          <source>The difference between LeavePGroupsOut and GroupShuffleSplit is that the former generates splits using all subsets of size &lt;code&gt;p&lt;/code&gt; unique groups, whereas GroupShuffleSplit generates a user-determined number of random test splits, each with a user-determined fraction of unique groups.</source>
          <target state="translated">LeavePGroupsOut和GroupShuffleSplit之间的区别在于，前者使用大小为 &lt;code&gt;p&lt;/code&gt; 的唯一组的所有子集生成拆分，而GroupShuffleSplit生成用户确定数量的随机测试拆分，每个都由用户确定比例的唯一组。</target>
        </trans-unit>
        <trans-unit id="f8e596d2cfcc0c0ae56904e18bdffdb0d463a655" translate="yes" xml:space="preserve">
          <source>The difference between LeavePGroupsOut and LeaveOneGroupOut is that the former builds the test sets with all the samples assigned to &lt;code&gt;p&lt;/code&gt; different values of the groups while the latter uses samples all assigned the same groups.</source>
          <target state="translated">LeavePGroupsOut和LeaveOneGroupOut之间的区别在于，前者使用分配给 &lt;code&gt;p&lt;/code&gt; 个不同组值的所有样本构建测试集，而后者使用均分配了相同组的样本。</target>
        </trans-unit>
        <trans-unit id="123fc66dcb39304255b8e25c6c10c654dbb2b0f3" translate="yes" xml:space="preserve">
          <source>The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of \(P(x_i \mid y)\).</source>
          <target state="translated">不同的奈夫贝叶斯分类器的不同之处主要在于它们对分布的假设(P(x_i \mid y)\)。</target>
        </trans-unit>
        <trans-unit id="fc85907a08e94b0f17af8c0c01aad9ef06729185" translate="yes" xml:space="preserve">
          <source>The digits dataset is made of 1797 8x8 images of hand-written digits</source>
          <target state="translated">数字数据集由1797张8x8手写数字图像组成。</target>
        </trans-unit>
        <trans-unit id="46ad3d3589f97f144056aa3785c730497272c03e" translate="yes" xml:space="preserve">
          <source>The dimension of the projected subspace.</source>
          <target state="translated">投影子空间的尺寸。</target>
        </trans-unit>
        <trans-unit id="5572a493038acd1179abd12e1cd0c48e6709dea1" translate="yes" xml:space="preserve">
          <source>The dimension of the projection subspace.</source>
          <target state="translated">投影子空间的尺寸。</target>
        </trans-unit>
        <trans-unit id="984d56bddca949d9f1aa278aca87eb46a4072ffd" translate="yes" xml:space="preserve">
          <source>The dimensionality of the resulting representation is &lt;code&gt;n_out &amp;lt;= n_estimators * max_leaf_nodes&lt;/code&gt;. If &lt;code&gt;max_leaf_nodes == None&lt;/code&gt;, the number of leaf nodes is at most &lt;code&gt;n_estimators * 2 ** max_depth&lt;/code&gt;.</source>
          <target state="translated">结果表示的维数为 &lt;code&gt;n_out &amp;lt;= n_estimators * max_leaf_nodes&lt;/code&gt; 。如果 &lt;code&gt;max_leaf_nodes == None&lt;/code&gt; ，则叶节点的数量最多为 &lt;code&gt;n_estimators * 2 ** max_depth&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="068779d9c792d7cec6633fa1a442091cb7f6f6f3" translate="yes" xml:space="preserve">
          <source>The dimensions and distribution of Random Projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset.</source>
          <target state="translated">随机投影矩阵的尺寸和分布受到控制,以便保留数据集的任何两个样本之间的对偶距离。</target>
        </trans-unit>
        <trans-unit id="1fc3554ec8e7cb3510bb9bcb462301a368807c4a" translate="yes" xml:space="preserve">
          <source>The dimensions and distribution of random projections matrices are controlled so as to preserve the pairwise distances between any two samples of the dataset. Thus random projection is a suitable approximation technique for distance based method.</source>
          <target state="translated">随机投影矩阵的维度和分布是可控的,这样可以保留数据集中任意两个样本之间的对偶距离。因此随机投影是一种适合于基于距离的方法的近似技术。</target>
        </trans-unit>
        <trans-unit id="16b41d4b4452bd551af457fee2f8b60407215654" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet).</source>
          <target state="translated">各分量在重量分布上的迪里希特浓度(Dirichlet)。</target>
        </trans-unit>
        <trans-unit id="6b13240ddd6531c8fdb797758cabfbb4633f5e80" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet). The type depends on &lt;code&gt;weight_concentration_prior_type&lt;/code&gt;:</source>
          <target state="translated">每个组分的狄利克雷浓度在重量分布上（狄利克雷）。类型取决于 &lt;code&gt;weight_concentration_prior_type&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="b470809fd29296951902d887ca553f032b8a40c4" translate="yes" xml:space="preserve">
          <source>The dirichlet concentration of each component on the weight distribution (Dirichlet). This is commonly called gamma in the literature. The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the mixture weights simplex. The value of the parameter must be greater than 0. If it is None, it&amp;rsquo;s set to &lt;code&gt;1. / n_components&lt;/code&gt;.</source>
          <target state="translated">每个组分的狄利克雷浓度在重量分布上（狄利克雷）。这在文献中通常称为伽马。较高的浓度会使更多的质量集中在中心，并导致更多的组分处于活动状态，而较低的浓度参数将导致混合权重单纯形的边缘出现更多的质量。参数的值必须大于0。如果为None，则将其设置为 &lt;code&gt;1. / n_components&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="cbf40d766d38685ebd59a9222ccafd32de07a34f" translate="yes" xml:space="preserve">
          <source>The disadvantages of Bayesian regression include:</source>
          <target state="translated">贝叶斯回归的缺点包括:1:</target>
        </trans-unit>
        <trans-unit id="ec84046fbad625e7f2f2f6744eab5866f4d72369" translate="yes" xml:space="preserve">
          <source>The disadvantages of GBRT are:</source>
          <target state="translated">GBRT的缺点是:</target>
        </trans-unit>
        <trans-unit id="cfaf1a3af8c2483843ec10c36faa80968b886bf2" translate="yes" xml:space="preserve">
          <source>The disadvantages of Gaussian processes include:</source>
          <target state="translated">高斯过程的缺点包括:</target>
        </trans-unit>
        <trans-unit id="c8e0c1974308fc5ae09e195effcd7396a8db8601" translate="yes" xml:space="preserve">
          <source>The disadvantages of Multi-layer Perceptron (MLP) include:</source>
          <target state="translated">多层感知器(MLP)的缺点包括:1:</target>
        </trans-unit>
        <trans-unit id="c71179ab39a085455ac6f6888a2f6fd3afe84d6f" translate="yes" xml:space="preserve">
          <source>The disadvantages of Stochastic Gradient Descent include:</source>
          <target state="translated">随机梯度下降法的缺点包括。</target>
        </trans-unit>
        <trans-unit id="84c28c658fb7eb00eb9347365efe97c2f04c30f3" translate="yes" xml:space="preserve">
          <source>The disadvantages of decision trees include:</source>
          <target state="translated">决策树的缺点包括:</target>
        </trans-unit>
        <trans-unit id="47feb0b4f4476b90d13a7091a56a1b0f68ad3a62" translate="yes" xml:space="preserve">
          <source>The disadvantages of support vector machines include:</source>
          <target state="translated">支持向量机的缺点包括:。</target>
        </trans-unit>
        <trans-unit id="266f0e7f0f21de8e86a9f2ce2320de66f17358df" translate="yes" xml:space="preserve">
          <source>The disadvantages of the LARS method include:</source>
          <target state="translated">LARS方法的缺点包括:</target>
        </trans-unit>
        <trans-unit id="fe9e13a1ad2c431f3f290607f881625ead9408c0" translate="yes" xml:space="preserve">
          <source>The disadvantages to using t-SNE are roughly:</source>
          <target state="translated">使用t-SNE的缺点大概是。</target>
        </trans-unit>
        <trans-unit id="a7ea673185d2a40501e91d63f573c417ac055cd1" translate="yes" xml:space="preserve">
          <source>The distance metric to use</source>
          <target state="translated">使用的距离指标</target>
        </trans-unit>
        <trans-unit id="0063e1a06973d2332dc280d77426f57f299b9005" translate="yes" xml:space="preserve">
          <source>The distance metric to use. Note that not all metrics are valid with all algorithms. Refer to the documentation of &lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt;&lt;code&gt;BallTree&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt;&lt;code&gt;KDTree&lt;/code&gt;&lt;/a&gt; for a description of available algorithms. Note that the normalization of the density output is correct only for the Euclidean distance metric. Default is &amp;lsquo;euclidean&amp;rsquo;.</source>
          <target state="translated">使用的距离度量。请注意，并非所有指标对所有算法都有效。有关可用算法的描述，请参考&lt;a href=&quot;sklearn.neighbors.balltree#sklearn.neighbors.BallTree&quot;&gt; &lt;code&gt;BallTree&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;sklearn.neighbors.kdtree#sklearn.neighbors.KDTree&quot;&gt; &lt;code&gt;KDTree&lt;/code&gt; &lt;/a&gt;的文档。注意，密度输出的归一化仅对于欧几里得距离度量是正确的。默认值为&amp;ldquo;欧几里得&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="3c4790f8b52fd2ea34a9c9dfcea72e27e7f13da0" translate="yes" xml:space="preserve">
          <source>The distance metric used to calculate the k-Neighbors for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is &amp;lsquo;euclidean&amp;rsquo; (&amp;lsquo;minkowski&amp;rsquo; metric with the p param equal to 2.)</source>
          <target state="translated">用于计算每个采样点的k邻居的距离度量。DistanceMetric类提供了可用指标的列表。默认距离为'euclidean'（&amp;ldquo; minkowski&amp;rdquo;度量标准，p参数等于2。）</target>
        </trans-unit>
        <trans-unit id="f687007319ee4f163c7b6668932fdc048712a932" translate="yes" xml:space="preserve">
          <source>The distance metric used to calculate the neighbors within a given radius for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is &amp;lsquo;euclidean&amp;rsquo; (&amp;lsquo;minkowski&amp;rsquo; metric with the param equal to 2.)</source>
          <target state="translated">用于计算每个采样点在给定半径内的邻居的距离度量。DistanceMetric类提供了可用指标的列表。默认距离是'euclidean'（参数等于2的'minkowski'度量）。</target>
        </trans-unit>
        <trans-unit id="7e12a027c556a152948ba7c771e2cb65d4ac3e99" translate="yes" xml:space="preserve">
          <source>The distinct labels used in classifying instances.</source>
          <target state="translated">用于对实例进行分类的不同标签。</target>
        </trans-unit>
        <trans-unit id="2333c2cc37e50157e20c383f74b00f781cb37a1b" translate="yes" xml:space="preserve">
          <source>The distortion introduced by a random projection &lt;code&gt;p&lt;/code&gt; is asserted by the fact that &lt;code&gt;p&lt;/code&gt; is defining an eps-embedding with good probability as defined by:</source>
          <target state="translated">随机投影 &lt;code&gt;p&lt;/code&gt; 引入的失真是由以下事实所断定的： &lt;code&gt;p&lt;/code&gt; 定义eps嵌入的概率很高，如下所示：</target>
        </trans-unit>
        <trans-unit id="befef2d542b10663820383e2f1e59843332d14a0" translate="yes" xml:space="preserve">
          <source>The distortion introduced by a random projection &lt;code&gt;p&lt;/code&gt; only changes the distance between two points by a factor (1 +- eps) in an euclidean space with good probability. The projection &lt;code&gt;p&lt;/code&gt; is an eps-embedding as defined by:</source>
          <target state="translated">由随机投影 &lt;code&gt;p&lt;/code&gt; 引入的失真在欧几里得空间中仅以几率（1 +-eps）改变两点之间的距离，几率很高。投影 &lt;code&gt;p&lt;/code&gt; 是eps嵌入，其定义如下：</target>
        </trans-unit>
        <trans-unit id="b6d88e183a439e17415e4b06c82155c9e34fa344" translate="yes" xml:space="preserve">
          <source>The distributions in &lt;code&gt;scipy.stats&lt;/code&gt; prior to version scipy 0.16 do not allow specifying a random state. Instead, they use the global numpy random state, that can be seeded via &lt;code&gt;np.random.seed&lt;/code&gt; or set using &lt;code&gt;np.random.set_state&lt;/code&gt;. However, beginning scikit-learn 0.18, the &lt;a href=&quot;classes#module-sklearn.model_selection&quot;&gt;&lt;code&gt;sklearn.model_selection&lt;/code&gt;&lt;/a&gt; module sets the random state provided by the user if scipy &amp;gt;= 0.16 is also available.</source>
          <target state="translated">在分布 &lt;code&gt;scipy.stats&lt;/code&gt; 之前版本SciPy的0.16不允许指定随机状态。而是使用全局numpy随机状态，该状态可以通过 &lt;code&gt;np.random.seed&lt;/code&gt; 设置种子，也可以使用 &lt;code&gt;np.random.set_state&lt;/code&gt; 设置。但是，从scikit-learn 0.18开始，如果scipy&amp;gt; = 0.16也可用，则&lt;a href=&quot;classes#module-sklearn.model_selection&quot;&gt; &lt;code&gt;sklearn.model_selection&lt;/code&gt; &lt;/a&gt;模块将设置用户提供的随机状态。</target>
        </trans-unit>
        <trans-unit id="e4d3a4f449c4df99f288fa9346370fef4e50edc3" translate="yes" xml:space="preserve">
          <source>The dual gap at the end of the optimization for the optimal alpha (&lt;code&gt;alpha_&lt;/code&gt;).</source>
          <target state="translated">最佳alpha（ &lt;code&gt;alpha_&lt;/code&gt; ）优化结束时的双重间隔。</target>
        </trans-unit>
        <trans-unit id="1c5b7990e4f078095970dbd44954a4ff0f6bdb22" translate="yes" xml:space="preserve">
          <source>The dual gaps at the end of the optimization for each alpha.</source>
          <target state="translated">各个阿尔法的优化结束后的双重差距。</target>
        </trans-unit>
        <trans-unit id="b5eac57a33eeb0d09a6d5ebfcbe611bef2da07d8" translate="yes" xml:space="preserve">
          <source>The edges of each bin. Contain arrays of varying shapes &lt;code&gt;(n_bins_, )&lt;/code&gt; Ignored features will have empty arrays.</source>
          <target state="translated">每个垃圾箱的边缘。包含各种形状的数组 &lt;code&gt;(n_bins_, )&lt;/code&gt; 忽略的要素将具有空数组。</target>
        </trans-unit>
        <trans-unit id="50c07dc6eab301b9d9953e2bd5849c6aa3abe8d7" translate="yes" xml:space="preserve">
          <source>The effect of the transformer is weaker than on the synthetic data. However, the transform induces a decrease of the MAE.</source>
          <target state="translated">变换器的影响比合成数据要弱。但是,变换会导致MAE的降低。</target>
        </trans-unit>
        <trans-unit id="8b7256d0b08ed885751aee08f85ac36bb70ae336" translate="yes" xml:space="preserve">
          <source>The effective size of the batch is computed here. If there are no more jobs to dispatch, return False, else return True.</source>
          <target state="translated">这里计算出该批作业的有效规模。如果没有更多的作业要派发,返回False,否则返回True。</target>
        </trans-unit>
        <trans-unit id="a044cf1265a1684d79e7b48898279d20ad6f0dac" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities</source>
          <target state="translated">要使用的特征值分解策略。AMG需要安装pyamg。它在处理大型稀疏问题时速度较快,但也可能导致不稳定。</target>
        </trans-unit>
        <trans-unit id="1dc685b8ff4dd3ccaa26f5f653c44ef0324bd82f" translate="yes" xml:space="preserve">
          <source>The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities.</source>
          <target state="translated">要使用的特征值分解策略。AMG需要安装pyamg。它在处理大型稀疏问题时速度更快,但也可能导致不稳定。</target>
        </trans-unit>
        <trans-unit id="f14ed9796539634c3492800089020cfe2ebbd7af" translate="yes" xml:space="preserve">
          <source>The elastic net optimization function varies for mono and multi-outputs.</source>
          <target state="translated">弹性网优化函数在单输出和多输出的情况下有所不同。</target>
        </trans-unit>
        <trans-unit id="e0191353bc3bca50b627784afbd555eb11c1f3dd" translate="yes" xml:space="preserve">
          <source>The empirical covariance matrix of a sample can be computed using the &lt;a href=&quot;generated/sklearn.covariance.empirical_covariance#sklearn.covariance.empirical_covariance&quot;&gt;&lt;code&gt;empirical_covariance&lt;/code&gt;&lt;/a&gt; function of the package, or by fitting an &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt;&lt;code&gt;EmpiricalCovariance&lt;/code&gt;&lt;/a&gt; object to the data sample with the &lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.fit&quot;&gt;&lt;code&gt;EmpiricalCovariance.fit&lt;/code&gt;&lt;/a&gt; method. Be careful that results depend on whether the data are centered, so one may want to use the &lt;code&gt;assume_centered&lt;/code&gt; parameter accurately. More precisely, if &lt;code&gt;assume_centered=False&lt;/code&gt;, then the test set is supposed to have the same mean vector as the training set. If not, both should be centered by the user, and &lt;code&gt;assume_centered=True&lt;/code&gt; should be used.</source>
          <target state="translated">可以使用包的&lt;a href=&quot;generated/sklearn.covariance.empirical_covariance#sklearn.covariance.empirical_covariance&quot;&gt; &lt;code&gt;empirical_covariance&lt;/code&gt; &lt;/a&gt;函数或通过使用&lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.fit&quot;&gt; &lt;code&gt;EmpiricalCovariance.fit&lt;/code&gt; &lt;/a&gt;方法将&lt;a href=&quot;generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance&quot;&gt; &lt;code&gt;EmpiricalCovariance&lt;/code&gt; &lt;/a&gt;对象拟合到数据样本来计算样本的经验协方差矩阵。请注意，结果取决于数据是否居中，因此可能需要准确地使用 &lt;code&gt;assume_centered&lt;/code&gt; 参数。更准确地说，如果假设 &lt;code&gt;assume_centered=False&lt;/code&gt; ，则假定测试集具有与训练集相同的平均向量。如果不是，则两者都应由用户居中，并应使用 &lt;code&gt;assume_centered=True&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="d4b0da98d5cb047aa932ef62858edca40e188517" translate="yes" xml:space="preserve">
          <source>The encoded signal (Y).</source>
          <target state="translated">编码信号(Y)。</target>
        </trans-unit>
        <trans-unit id="cc2fdf3023f61dc3df7bba02538cce691e7cf2cd" translate="yes" xml:space="preserve">
          <source>The energy function measures the quality of a joint assignment:</source>
          <target state="translated">能量函数衡量联合作业的质量。</target>
        </trans-unit>
        <trans-unit id="1bfb5cae50ef0b1032c729ea4ab68ff502e7f906" translate="yes" xml:space="preserve">
          <source>The entry &lt;code&gt;test_fold[i]&lt;/code&gt; represents the index of the test set that sample &lt;code&gt;i&lt;/code&gt; belongs to. It is possible to exclude sample &lt;code&gt;i&lt;/code&gt; from any test set (i.e. include sample &lt;code&gt;i&lt;/code&gt; in every training set) by setting &lt;code&gt;test_fold[i]&lt;/code&gt; equal to -1.</source>
          <target state="translated">条目 &lt;code&gt;test_fold[i]&lt;/code&gt; 代表样本 &lt;code&gt;i&lt;/code&gt; 所属的测试集的索引。通过将 &lt;code&gt;test_fold[i]&lt;/code&gt; 设置为-1，可以从任何测试集中排除样本 &lt;code&gt;i&lt;/code&gt; （即在每个训练集中包括样本 &lt;code&gt;i&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="503425d1f7e07b98f32316b1d85343640a4e1294" translate="yes" xml:space="preserve">
          <source>The equivalence between &lt;code&gt;alpha&lt;/code&gt; and the regularization parameter of SVM, &lt;code&gt;C&lt;/code&gt; is given by &lt;code&gt;alpha = 1 / C&lt;/code&gt; or &lt;code&gt;alpha = 1 / (n_samples * C)&lt;/code&gt;, depending on the estimator and the exact objective function optimized by the model.</source>
          <target state="translated">&lt;code&gt;alpha&lt;/code&gt; 和SVM的正则化参数 &lt;code&gt;C&lt;/code&gt; 的等价性由 &lt;code&gt;alpha = 1 / C&lt;/code&gt; 或 &lt;code&gt;alpha = 1 / (n_samples * C)&lt;/code&gt; ，具体取决于估计器和模型优化的确切目标函数。</target>
        </trans-unit>
        <trans-unit id="2fec157ea85a1f513b11852f437f8dc13c9361bb" translate="yes" xml:space="preserve">
          <source>The error message or a substring of the error message.</source>
          <target state="translated">错误信息或错误信息的子串。</target>
        </trans-unit>
        <trans-unit id="67a9569df158acc253e75d9fa812424d1f2b58c6" translate="yes" xml:space="preserve">
          <source>The estimated (sparse) precision matrix.</source>
          <target state="translated">估计的(稀疏)精度矩阵。</target>
        </trans-unit>
        <trans-unit id="cdf0d65945f589fd5da3781e66a0a81a30e934b1" translate="yes" xml:space="preserve">
          <source>The estimated covariance matrix.</source>
          <target state="translated">估计的协方差矩阵;</target>
        </trans-unit>
        <trans-unit id="77a18d9bfc3d1fc8448c4fb559b59c860611bbc7" translate="yes" xml:space="preserve">
          <source>The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See &amp;ldquo;Pattern Recognition and Machine Learning&amp;rdquo; by C. Bishop, 12.2.1 p. 574 or &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;.</source>
          <target state="translated">根据Tipping and Bishop 1999的概率PCA模型估计的噪声协方差。请参见C. Bishop的&amp;ldquo;模式识别和机器学习&amp;rdquo;，第12.2.1页。574或&lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="735cdf6c1c61f38b1092aa1bd4262b34c0d5f0f3" translate="yes" xml:space="preserve">
          <source>The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See &amp;ldquo;Pattern Recognition and Machine Learning&amp;rdquo; by C. Bishop, 12.2.1 p. 574 or &lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;. It is required to compute the estimated data covariance and score samples.</source>
          <target state="translated">根据Tipping and Bishop 1999的概率PCA模型估计的噪声协方差。请参见C. Bishop的&amp;ldquo;模式识别和机器学习&amp;rdquo;，第12.2.1页。574或&lt;a href=&quot;http://www.miketipping.com/papers/met-mppca.pdf&quot;&gt;http://www.miketipping.com/papers/met-mppca.pdf&lt;/a&gt;。需要计算估计的数据协方差和分数样本。</target>
        </trans-unit>
        <trans-unit id="cc49065d7efcb582d87899f7997c53b1f8d60c78" translate="yes" xml:space="preserve">
          <source>The estimated noise variance for each feature.</source>
          <target state="translated">每个特征的估计噪声方差。</target>
        </trans-unit>
        <trans-unit id="960e70b3d078f74487289ecbb537290d1f7473d4" translate="yes" xml:space="preserve">
          <source>The estimated number of components. Relevant when &lt;code&gt;n_components=None&lt;/code&gt;.</source>
          <target state="translated">估计的组件数。当 &lt;code&gt;n_components=None&lt;/code&gt; 时相关。</target>
        </trans-unit>
        <trans-unit id="df011fa4ccaad62cb4a83a0955409a4c4b3035c8" translate="yes" xml:space="preserve">
          <source>The estimated number of components. When n_components is set to &amp;lsquo;mle&amp;rsquo; or a number between 0 and 1 (with svd_solver == &amp;lsquo;full&amp;rsquo;) this number is estimated from input data. Otherwise it equals the parameter n_components, or the lesser value of n_features and n_samples if n_components is None.</source>
          <target state="translated">估计的组件数。当n_components设置为'mle'或0到1之间的数字（svd_solver =='full'）时，将从输入数据中估计该数字。否则，它等于参数n_components，如果n_components为None，则等于n_features和n_samples的较小值。</target>
        </trans-unit>
        <trans-unit id="cd106f5afe35a52902113758dbc47d95941530e6" translate="yes" xml:space="preserve">
          <source>The estimated number of connected components in the graph.</source>
          <target state="translated">图中连接的组件的估计数量。</target>
        </trans-unit>
        <trans-unit id="a98c4d1bc7caeb1163bf14c014655348909c79f3" translate="yes" xml:space="preserve">
          <source>The estimation of the model is done by calculating the slopes and intercepts of a subpopulation of all possible combinations of p subsample points. If an intercept is fitted, p must be greater than or equal to n_features + 1. The final slope and intercept is then defined as the spatial median of these slopes and intercepts.</source>
          <target state="translated">模型的估计是通过计算p个子样本点的所有可能组合的子群的斜率和截距来完成的。如果截距被拟合,p必须大于或等于n_features+1。最后的斜率和截距被定义为这些斜率和截距的空间中值。</target>
        </trans-unit>
        <trans-unit id="aed02faa2e2402cc32a5968b7e86a54cff535c4e" translate="yes" xml:space="preserve">
          <source>The estimation of the model is done by iteratively maximizing the marginal log-likelihood of the observations.</source>
          <target state="translated">模型的估计是通过迭代最大化观测值的边际对数似然来完成的。</target>
        </trans-unit>
        <trans-unit id="44fe06813d3627aaa56531c52961387365e10d66" translate="yes" xml:space="preserve">
          <source>The estimation of the number of degrees of freedom is given by:</source>
          <target state="translated">自由度数的估计方法是:</target>
        </trans-unit>
        <trans-unit id="2bed56c931836016f42a33afdedfac5cb8f555f4" translate="yes" xml:space="preserve">
          <source>The estimator also implements &lt;code&gt;partial_fit&lt;/code&gt;, which updates the dictionary by iterating only once over a mini-batch. This can be used for online learning when the data is not readily available from the start, or for when the data does not fit into the memory.</source>
          <target state="translated">估计器还实现了 &lt;code&gt;partial_fit&lt;/code&gt; ，该函数通过在微型批处理中仅迭代一次来更新字典。当数据从一开始就不容易获得时，或者当数据不适合内存时，可以将其用于在线学习。</target>
        </trans-unit>
        <trans-unit id="efc1e3841cf31ff119d8e77fb55d7093c1df4dd4" translate="yes" xml:space="preserve">
          <source>The estimator objects for each cv split. This is available only if &lt;code&gt;return_estimator&lt;/code&gt; parameter is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">每个简历拆分的估算器对象。仅当 &lt;code&gt;return_estimator&lt;/code&gt; 参数设置为 &lt;code&gt;True&lt;/code&gt; 时,此选项才可用。</target>
        </trans-unit>
        <trans-unit id="f33cc8b973e26f8a535386dbd610021f87dfdf8f" translate="yes" xml:space="preserve">
          <source>The estimator or group of estimators to be cloned</source>
          <target state="translated">要克隆的估计器或估计器组。</target>
        </trans-unit>
        <trans-unit id="65944411996b76786bcb63db12ac31b08b55648a" translate="yes" xml:space="preserve">
          <source>The estimator that provides the initial predictions. Set via the &lt;code&gt;init&lt;/code&gt; argument or &lt;code&gt;loss.init_estimator&lt;/code&gt;.</source>
          <target state="translated">提供初始预测的估算器。通过 &lt;code&gt;init&lt;/code&gt; 参数或 &lt;code&gt;loss.init_estimator&lt;/code&gt; 设置。</target>
        </trans-unit>
        <trans-unit id="ade400270e658832b6e128c7ef187979eae33356" translate="yes" xml:space="preserve">
          <source>The estimators of a pipeline are stored as a list in the &lt;code&gt;steps&lt;/code&gt; attribute:</source>
          <target state="translated">管道的估计量在列表中存储为 &lt;code&gt;steps&lt;/code&gt; 属性：</target>
        </trans-unit>
        <trans-unit id="a46349c122935a13db8091d877d4179b37995289" translate="yes" xml:space="preserve">
          <source>The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. For example, it is possible to use these estimators to turn a binary classifier or a regressor into a multiclass classifier. It is also possible to use these estimators with multiclass estimators in the hope that their accuracy or runtime performance improves.</source>
          <target state="translated">本模块提供的估计器是元估计器:它们需要在其构造函数中提供一个基本估计器。例如,可以使用这些估计器将二元分类器或回归器转化为多类分类器。也可以将这些估计器与多类估计器一起使用,希望提高它们的准确性或运行时性能。</target>
        </trans-unit>
        <trans-unit id="1868049da2c813b87c2e3d48a5c71f72cc892852" translate="yes" xml:space="preserve">
          <source>The estimators provided in this module are meta-estimators: they require a base estimator to be provided in their constructor. The meta-estimator extends single output estimators to multioutput estimators.</source>
          <target state="translated">本模块提供的估计器是元估计器:它们需要在其构造函数中提供一个基本估计器。元估计器将单输出估计器扩展为多输出估计器。</target>
        </trans-unit>
        <trans-unit id="fe77b9f934c3d4135d1310eb25c7f2c922f35971" translate="yes" xml:space="preserve">
          <source>The exact API of all functions and classes, as given by the docstrings. The API documents expected types and allowed features for all functions, and all parameters available for the algorithms.</source>
          <target state="translated">所有函数和类的准确API,由docstrings给出。API记录了所有函数的预期类型和允许的功能,以及算法的所有可用参数。</target>
        </trans-unit>
        <trans-unit id="213a31afd656f2a0c8b37dd55b40fc5308db3ca1" translate="yes" xml:space="preserve">
          <source>The exact additive chi squared kernel.</source>
          <target state="translated">确切的加法chi平方核。</target>
        </trans-unit>
        <trans-unit id="14b1a49c77fdcb8a0d3244090e7c6df0b2a6cd07" translate="yes" xml:space="preserve">
          <source>The exact chi squared kernel.</source>
          <target state="translated">确切的chi平方核。</target>
        </trans-unit>
        <trans-unit id="21c56a998f59a2c6be6550db485efcc395359e33" translate="yes" xml:space="preserve">
          <source>The example below demonstrates how the OOB error can be measured at the addition of each new tree during training. The resulting plot allows a practitioner to approximate a suitable value of &lt;code&gt;n_estimators&lt;/code&gt; at which the error stabilizes.</source>
          <target state="translated">下面的示例演示了如何在训练过程中添加每棵新树时测量OOB错误。结果图允许从业者近似估计 &lt;code&gt;n_estimators&lt;/code&gt; 的合适值，误差稳定在该值上。</target>
        </trans-unit>
        <trans-unit id="556b3073adbd8a84d1d4b2fe21db4807c2daf4c8" translate="yes" xml:space="preserve">
          <source>The example below uses a support vector classifier with a non-linear kernel to build a model with optimized hyperparameters by grid search. We compare the performance of non-nested and nested CV strategies by taking the difference between their scores.</source>
          <target state="translated">下面的例子使用了一个带有非线性内核的支持向量分类器,通过网格搜索建立一个具有优化超参数的模型。我们通过取非嵌套和嵌套CV策略的得分差来比较它们的性能。</target>
        </trans-unit>
        <trans-unit id="4cd198cf307950e0cb10d5d2e6b86c91624772aa" translate="yes" xml:space="preserve">
          <source>The example compares prediction result of linear regression (linear model) and decision tree (tree based model) with and without discretization of real-valued features.</source>
          <target state="translated">本例比较了线性回归(线性模型)和决策树(基于树的模型)的预测结果,有无对实值特征进行离散化。</target>
        </trans-unit>
        <trans-unit id="3101cd4fceb55d1be63d1bff5cb3f45996fd9aa0" translate="yes" xml:space="preserve">
          <source>The example demonstrates syntax and speed only; it doesn&amp;rsquo;t actually do anything useful with the extracted vectors. See the example scripts {document_classification_20newsgroups,clustering}.py for actual learning on text documents.</source>
          <target state="translated">该示例仅演示语法和速度。它实际上对提取的向量没有任何帮助。有关实际学习文本文档的信息，请参见示例脚本{document_classification_20newsgroups，clustering} .py。</target>
        </trans-unit>
        <trans-unit id="32ed3af5edbdaa27f9eba4e5a30255c6afab4414" translate="yes" xml:space="preserve">
          <source>The example is engineered to show the effect of the choice of different metrics. It is applied to waveforms, which can be seen as high-dimensional vector. Indeed, the difference between metrics is usually more pronounced in high dimension (in particular for euclidean and cityblock).</source>
          <target state="translated">这个例子是为了显示选择不同指标的效果而设计的。它适用于波形,波形可以被看作是高维向量。事实上,在高维度上,度量之间的差异通常更为明显(尤其是对于欧氏和城块)。</target>
        </trans-unit>
        <trans-unit id="36ed82faabe70da57df289b8a54bb16e8830773a" translate="yes" xml:space="preserve">
          <source>The example shows that the predictions in ridge are strongly influenced by the outliers present in the dataset. The Huber regressor is less influenced by the outliers since the model uses the linear loss for these. As the parameter epsilon is increased for the Huber regressor, the decision function approaches that of the ridge.</source>
          <target state="translated">这个例子表明,山脊的预测受数据集中存在的离群值影响很大。Huber回归器受离群值的影响较小,因为模型对这些离群值使用了线性损失。当Huber回归器的参数epsilon增加时,决策函数接近于山脊的决策函数。</target>
        </trans-unit>
        <trans-unit id="889bc091af3b0106e91a81884d921d1b2df9bdd6" translate="yes" xml:space="preserve">
          <source>The examples below compare Gaussian mixture models with a fixed number of components, to the variational Gaussian mixture models with a Dirichlet process prior. Here, a classical Gaussian mixture is fitted with 5 components on a dataset composed of 2 clusters. We can see that the variational Gaussian mixture with a Dirichlet process prior is able to limit itself to only 2 components whereas the Gaussian mixture fits the data with a fixed number of components that has to be set a priori by the user. In this case the user has selected &lt;code&gt;n_components=5&lt;/code&gt; which does not match the true generative distribution of this toy dataset. Note that with very little observations, the variational Gaussian mixture models with a Dirichlet process prior can take a conservative stand, and fit only one component.</source>
          <target state="translated">下面的示例将具有固定数量组件的高斯混合模型与先验Dirichlet过程的变分高斯混合模型进行了比较。在这里，一个经典的高斯混合体在由2个聚类组成的数据集中具有5个成分。我们可以看到具有Dirichlet过程先验的变分高斯混合能够将自身限制为仅2个成分，而高斯混合则将数据与固定数量的成分拟合，而用户必须事先设置这些固定数量的成分。在这种情况下，用户选择的 &lt;code&gt;n_components=5&lt;/code&gt; 与该玩具数据集的真实生成分布不匹配。请注意，很少观察到，具有Dirichlet过程先验的变分高斯混合模型可以采取保守的立场，并且只能拟合一个分量。</target>
        </trans-unit>
        <trans-unit id="e5c4a6233958dced6338c85ecba7ef5860ffcbbc" translate="yes" xml:space="preserve">
          <source>The expected value for the mutual information can be calculated using the following equation [VEB2009]. In this equation, \(a_i = |U_i|\) (the number of elements in \(U_i\)) and \(b_j = |V_j|\) (the number of elements in \(V_j\)).</source>
          <target state="translated">相互信息的预期值可以用以下公式计算[VEB2009]。在这个等式中,/(a_i=|U_i|/)(在/(U_i/)中的元素数量)和/(b_j=|V_j|/)(在/(V_j/)中的元素数量)。</target>
        </trans-unit>
        <trans-unit id="5075a63ec7be1cbe42a641e5ae277f9e4b63f160" translate="yes" xml:space="preserve">
          <source>The experiment is performed on an artificial dataset for binary classification with 100,000 samples (1,000 of them are used for model fitting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The first figure shows the estimated probabilities obtained with logistic regression, Gaussian naive Bayes, and Gaussian naive Bayes with both isotonic calibration and sigmoid calibration. The calibration performance is evaluated with Brier score, reported in the legend (the smaller the better). One can observe here that logistic regression is well calibrated while raw Gaussian naive Bayes performs very badly. This is because of the redundant features which violate the assumption of feature-independence and result in an overly confident classifier, which is indicated by the typical transposed-sigmoid curve.</source>
          <target state="translated">实验在二元分类的人工数据集上进行,有10万个样本(其中1000个用于模型拟合),有20个特征。在20个特征中,只有2个是信息量大的,10个是冗余的。第一张图显示了用逻辑回归、高斯奈夫贝叶斯和高斯奈夫贝叶斯进行同调校准和sigmoid校准得到的估计概率。校准性能用Brier评分来评估,在图例中报告(越小越好)。在这里可以观察到,逻辑回归的校准效果很好,而原始高斯奈夫贝叶斯的表现非常糟糕。这是因为冗余特征违反了特征独立的假设,导致分类器过于自信,典型的转置-sigmoid曲线就说明了这一点。</target>
        </trans-unit>
        <trans-unit id="e1db213d528f30cf745c1554ad2a43991932646f" translate="yes" xml:space="preserve">
          <source>The explained variance or ndarray if &amp;lsquo;multioutput&amp;rsquo; is &amp;lsquo;raw_values&amp;rsquo;.</source>
          <target state="translated">如果'multioutput'为'raw_values'，则说明变量或ndarray。</target>
        </trans-unit>
        <trans-unit id="dff7adf6114ae66681ccc96cdb5c8abce36c0555" translate="yes" xml:space="preserve">
          <source>The explicit constant as predicted by the &amp;ldquo;constant&amp;rdquo; strategy. This parameter is useful only for the &amp;ldquo;constant&amp;rdquo; strategy.</source>
          <target state="translated">如&amp;ldquo;常量&amp;rdquo;策略所预测的显式常量。此参数仅对&amp;ldquo;恒定&amp;rdquo;策略有用。</target>
        </trans-unit>
        <trans-unit id="96483cbe0c7b434b8387960f89d9c2700f0af59b" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate [default 0.5].</source>
          <target state="translated">反比例学习率的指数[默认0.5]。</target>
        </trans-unit>
        <trans-unit id="1fa85799d065de026e0253585d451384c9ee6013" translate="yes" xml:space="preserve">
          <source>The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to &amp;lsquo;invscaling&amp;rsquo;. Only used when solver=&amp;rsquo;sgd&amp;rsquo;.</source>
          <target state="translated">反比例学习率的指数。当learning_rate设置为&amp;ldquo; invscaling&amp;rdquo;时，它用于更新有效学习率。仅在Solver ='sgd'时使用。</target>
        </trans-unit>
        <trans-unit id="c7acf68e47ad4fe8cc8cf54d82e56ffc97c60a11" translate="yes" xml:space="preserve">
          <source>The exponent for the base kernel</source>
          <target state="translated">基核的指数</target>
        </trans-unit>
        <trans-unit id="fe68de8b995171a61c550ec2f7815b01a2d28ec7" translate="yes" xml:space="preserve">
          <source>The exponentiated version of the kernel, which is usually preferable.</source>
          <target state="translated">指数版的内核,通常是比较好的。</target>
        </trans-unit>
        <trans-unit id="d1490a4bcb15c22959ee30800b231e4413480004" translate="yes" xml:space="preserve">
          <source>The external estimator fit on the reduced dataset.</source>
          <target state="translated">缩小数据集的外部估计器拟合。</target>
        </trans-unit>
        <trans-unit id="18d0cd5609d8f78695dc43242ffb9cf995a65ee9" translate="yes" xml:space="preserve">
          <source>The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components by sample in a more than 30000-dimensional space (less than .5% non-zero features):</source>
          <target state="translated">提取的TF-IDF向量非常稀疏,在一个30000多维的空间中,按样本平均有159个非零分量(非零特征不到0.5%)。</target>
        </trans-unit>
        <trans-unit id="e5bc1ae1e0e90623cdbf461e900aded771c1b79d" translate="yes" xml:space="preserve">
          <source>The extracted dataset will only retain pictures of people that have at least &lt;code&gt;min_faces_per_person&lt;/code&gt; different pictures.</source>
          <target state="translated">提取的数据集将仅保留具有至少 &lt;code&gt;min_faces_per_person&lt;/code&gt; 个不同图片的人的图片。</target>
        </trans-unit>
        <trans-unit id="7cb7e9ef70987101280d538c40ac3f0463557635" translate="yes" xml:space="preserve">
          <source>The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task easier.</source>
          <target state="translated">乘以超立方体大小的因子。值越大,聚类/类越分散,分类任务越容易。</target>
        </trans-unit>
        <trans-unit id="0cd5de0c793252ae54dc423f82b86e165f05af5f" translate="yes" xml:space="preserve">
          <source>The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher&amp;rsquo;s paper. Note that it&amp;rsquo;s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points.</source>
          <target state="translated">著名的鸢尾花数据库，最早由RA Fisher爵士使用。该数据集摘自Fisher的论文。请注意，它与R中的相同，但与UCI机器学习存储库中的R相同，后者具有两个错误的数据点。</target>
        </trans-unit>
        <trans-unit id="e9c1f9439f6a49e182ea739a4c5151e4783dfb88" translate="yes" xml:space="preserve">
          <source>The feature importance scores of a fit gradient boosting model can be accessed via the &lt;code&gt;feature_importances_&lt;/code&gt; property:</source>
          <target state="translated">可以通过 &lt;code&gt;feature_importances_&lt;/code&gt; 属性访问拟合梯度增强模型的特征重要性得分：</target>
        </trans-unit>
        <trans-unit id="40d3ffb0824ad292679a9b4129b9e33a1cef7858" translate="yes" xml:space="preserve">
          <source>The feature matrix. Categorical features are encoded as ordinals.</source>
          <target state="translated">特征矩阵。分类特征被编码为序数。</target>
        </trans-unit>
        <trans-unit id="0a487efba080872bb151e33f60795da2b55ed658" translate="yes" xml:space="preserve">
          <source>The feature ranking, such that &lt;code&gt;ranking_[i]&lt;/code&gt; corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1.</source>
          <target state="translated">特征等级，使得 &lt;code&gt;ranking_[i]&lt;/code&gt; 对应于第i个特征的等级位置。选定的（即最佳估计）特征分配给等级1。</target>
        </trans-unit>
        <trans-unit id="acbf27a6428b599cd8900c8ca4f4ecae3f0cc8b0" translate="yes" xml:space="preserve">
          <source>The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and &lt;code&gt;max_features=n_features&lt;/code&gt;, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, &lt;code&gt;random_state&lt;/code&gt; has to be fixed.</source>
          <target state="translated">每次分割时，特征总是随机排列的。因此，即使在搜索最佳分割期间枚举的几个分割的标准改进相同的情况下，即使使用相同的训练数据和 &lt;code&gt;max_features=n_features&lt;/code&gt; ，找到的最佳分割也可能会有所不同。为了在拟合期间获得确定性行为，必须固定 &lt;code&gt;random_state&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="c25846da01d7420f77d01daefe6ea3229aead8d7" translate="yes" xml:space="preserve">
          <source>The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, &lt;code&gt;max_features=n_features&lt;/code&gt; and &lt;code&gt;bootstrap=False&lt;/code&gt;, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, &lt;code&gt;random_state&lt;/code&gt; has to be fixed.</source>
          <target state="translated">每次分割时，特征总是随机排列的。因此，即使对于相同的训练数据， &lt;code&gt;max_features=n_features&lt;/code&gt; 和 &lt;code&gt;bootstrap=False&lt;/code&gt; ，即使在搜索最佳分割时枚举的几个分割标准的改善相同，最佳分割也可能会有所不同。为了在拟合期间获得确定性行为，必须固定 &lt;code&gt;random_state&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="79b5615baa935539329fdffd05466c4966130183" translate="yes" xml:space="preserve">
          <source>The features indices which will be returned when calling &lt;code&gt;transform&lt;/code&gt;. They are computed during &lt;code&gt;fit&lt;/code&gt;. For &lt;code&gt;features='all'&lt;/code&gt;, it is to &lt;code&gt;range(n_features)&lt;/code&gt;.</source>
          <target state="translated">调用 &lt;code&gt;transform&lt;/code&gt; 时将返回的要素索引。它们在 &lt;code&gt;fit&lt;/code&gt; 期间进行计算。对于 &lt;code&gt;features='all'&lt;/code&gt; ，它是 &lt;code&gt;range(n_features)&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="7bb18d685bec7903d9b61cd6026a227300d22db3" translate="yes" xml:space="preserve">
          <source>The features of &lt;code&gt;X&lt;/code&gt; have been transformed from \([x_1, x_2]\) to \([1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\), and can now be used within any linear model.</source>
          <target state="translated">&lt;code&gt;X&lt;/code&gt; 的特征已从\（[x_1，x_2] \）转换为\（[1，x_1，x_2，x_1 ^ 2，x_1 x_2，x_2 ^ 2] \），现在可以在任何线性模型中使用。</target>
        </trans-unit>
        <trans-unit id="b791adba0e25e8ba284f5da492871659f500ccc6" translate="yes" xml:space="preserve">
          <source>The features of X have been transformed from \((X_1, X_2)\) to \((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\).</source>
          <target state="translated">X的特征已经从((X_1,X_2))转化为((1,X_1,X_2,X_1^2,X_1X_2,X_2^2))。</target>
        </trans-unit>
        <trans-unit id="0ec9e3dee3b599b742aa9c67dee3f4cab82e4d00" translate="yes" xml:space="preserve">
          <source>The features of X have been transformed from \((X_1, X_2, X_3)\) to \((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\).</source>
          <target state="translated">X的特征已经从((X_1,X_2,X_3)/)转化为((1,X_1,X_2,X_3,X_1X_2,X_1X_3,X_2X_3,X_1X_2X_3)/)。</target>
        </trans-unit>
        <trans-unit id="eaccd9379a9dbc6cce41fc318001c8bf42339abe" translate="yes" xml:space="preserve">
          <source>The figure below illustrates the effect of shrinkage and subsampling on the goodness-of-fit of the model. We can clearly see that shrinkage outperforms no-shrinkage. Subsampling with shrinkage can further increase the accuracy of the model. Subsampling without shrinkage, on the other hand, does poorly.</source>
          <target state="translated">下图说明了收缩和子抽样对模型拟合度的影响。我们可以清楚地看到,收缩率优于无收缩率。带收缩的子抽样可以进一步提高模型的准确性。而没有收缩的子抽样则表现不佳。</target>
        </trans-unit>
        <trans-unit id="50a711fcdd2b061e3d348535a1d7a4d9bddb7ed5" translate="yes" xml:space="preserve">
          <source>The figure below shows the results of applying &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt;&lt;code&gt;GradientBoostingRegressor&lt;/code&gt;&lt;/a&gt; with least squares loss and 500 base learners to the Boston house price dataset (&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt;&lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt;&lt;/a&gt;). The plot on the left shows the train and test error at each iteration. The train error at each iteration is stored in the &lt;code&gt;train_score_&lt;/code&gt; attribute of the gradient boosting model. The test error at each iterations can be obtained via the &lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt;&lt;code&gt;staged_predict&lt;/code&gt;&lt;/a&gt; method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. &lt;code&gt;n_estimators&lt;/code&gt;) by early stopping. The plot on the right shows the feature importances which can be obtained via the &lt;code&gt;feature_importances_&lt;/code&gt; property.</source>
          <target state="translated">下图显示了将具有最小平方损失和500个基本学习者的&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor&quot;&gt; &lt;code&gt;GradientBoostingRegressor&lt;/code&gt; &lt;/a&gt;应用到波士顿房价数据集（&lt;a href=&quot;generated/sklearn.datasets.load_boston#sklearn.datasets.load_boston&quot;&gt; &lt;code&gt;sklearn.datasets.load_boston&lt;/code&gt; &lt;/a&gt;）的结果。左图显示了每次迭代时的训练误差和测试误差。每次迭代时的火车误差都存储在梯度提升模型的 &lt;code&gt;train_score_&lt;/code&gt; 属性中。可以通过&lt;a href=&quot;generated/sklearn.ensemble.gradientboostingregressor#sklearn.ensemble.GradientBoostingRegressor.staged_predict&quot;&gt; &lt;code&gt;staged_predict&lt;/code&gt; &lt;/a&gt;方法获得每次迭代时的测试错误，该方法返回一个生成器，该生成器在每个阶段产生预测。像这样的图可以用来通过提前停止来确定最佳的树数（即 &lt;code&gt;n_estimators&lt;/code&gt; ）。右图显示了功能重要性，可以通过 &lt;code&gt;feature_importances_&lt;/code&gt; 属性。</target>
        </trans-unit>
        <trans-unit id="cc8de6d2f1f6034186ab0af10d91754eadd34954" translate="yes" xml:space="preserve">
          <source>The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly 2*pi (6.28), while KRR chooses the doubled periodicity 4*pi. Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (&amp;ldquo;curse of dimensionality&amp;rdquo;). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.</source>
          <target state="translated">该图显示两种方法都可以学习目标函数的合理模型。 GPR正确地将函数的周期性标识为大约2 * pi（6.28），而KRR选择了两倍的周期性4 * pi。除此之外，GPR为预测提供了合理的置信范围，而KRR则无法使用。两种方法之间的主要区别是拟合和预测所需的时间：尽管拟合KRR原则上是快速的，但是针对超参数优化的网格搜索会随着超参数的数量（&amp;ldquo;维数的诅咒&amp;rdquo;）成指数级增长。 GPR中基于梯度的参数优化不受此指数缩放的影响，因此在具有3维超参数空间的此示例中，速度显着提高。预测时间相似。然而，产生GPR预测分布的方差比仅预测平均值要花费更长的时间。</target>
        </trans-unit>
        <trans-unit id="7a70cd6dc569a4ad4c8d12310e45e3bf56f13e7d" translate="yes" xml:space="preserve">
          <source>The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly \(2*\pi\) (6.28), while KRR chooses the doubled periodicity \(4*\pi\) . Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (&amp;ldquo;curse of dimensionality&amp;rdquo;). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.</source>
          <target state="translated">该图显示两种方法都可以学习目标函数的合理模型。 GPR正确地将函数的周期性标识为大约\（2 * \ pi \）（6.28），而KRR选择了两倍的周期性\（4 * \ pi \）。除此之外，GPR为预测提供了合理的置信范围，而KRR则无法使用。两种方法之间的主要区别是拟合和预测所需的时间：尽管拟合KRR原则上是快速的，但是针对超参数优化的网格搜索会随着超参数的数量（&amp;ldquo;维数的诅咒&amp;rdquo;）成指数级增长。 GPR中基于梯度的参数优化不受此指数缩放的影响，因此在具有3维超参数空间的此示例中，速度显着提高。预测时间相似。然而，产生GPR预测分布的方差比仅预测平均值要花费更长的时间。</target>
        </trans-unit>
        <trans-unit id="4c819c571e9cf27ccc7a880b7ae493959bac8667" translate="yes" xml:space="preserve">
          <source>The figure shows the cumulative sum of the negative OOB improvements as a function of the boosting iteration. As you can see, it tracks the test loss for the first hundred iterations but then diverges in a pessimistic way. The figure also shows the performance of 3-fold cross validation which usually gives a better estimate of the test loss but is computationally more demanding.</source>
          <target state="translated">该图显示了负OOB改进的累积总和作为提升迭代的函数。正如您所看到的,它跟踪了前一百次迭代的测试损失,但随后以一种悲观的方式分化。该图还显示了3倍交叉验证的性能,它通常可以更好地估计测试损失,但对计算的要求更高。</target>
        </trans-unit>
        <trans-unit id="fc5bec6cdf25a030ca7f5736bc605af5657e6e59" translate="yes" xml:space="preserve">
          <source>The figures below are used to illustrate the effect of scaling our &lt;code&gt;C&lt;/code&gt; to compensate for the change in the number of samples, in the case of using an &lt;code&gt;l1&lt;/code&gt; penalty, as well as the &lt;code&gt;l2&lt;/code&gt; penalty.</source>
          <target state="translated">下图用于说明在使用 &lt;code&gt;l1&lt;/code&gt; 罚分和 &lt;code&gt;l2&lt;/code&gt; 罚分的情况下，缩放 &lt;code&gt;C&lt;/code&gt; 以补偿样本数量变化的效果。</target>
        </trans-unit>
        <trans-unit id="0b0d592cc5daeb578d9b82714e3800c3092f375f" translate="yes" xml:space="preserve">
          <source>The figures illustrate the interpolating property of the Gaussian Process model as well as its probabilistic nature in the form of a pointwise 95% confidence interval.</source>
          <target state="translated">这些数字说明了高斯过程模型的插值特性,以及它的概率性质,其形式是一个点向95%的置信区间。</target>
        </trans-unit>
        <trans-unit id="e96dfdb18e509ef4ea2f5731757d5c9fff16f17b" translate="yes" xml:space="preserve">
          <source>The figures show the confusion matrix with and without normalization by class support size (number of elements in each class). This kind of normalization can be interesting in case of class imbalance to have a more visual interpretation of which class is being misclassified.</source>
          <target state="translated">图中显示了按类支持规模(每个类的元素数量)进行归一化和不归一化的混淆矩阵。这种归一化在班级不平衡的情况下很有意思,可以更直观地解释哪个班级被错误分类。</target>
        </trans-unit>
        <trans-unit id="83d2d2e5c2f316a3531e949e8bac6210f7e21821" translate="yes" xml:space="preserve">
          <source>The files themselves are loaded in memory in the &lt;code&gt;data&lt;/code&gt; attribute. For reference the filenames are also available:</source>
          <target state="translated">文件本身被加载到 &lt;code&gt;data&lt;/code&gt; 属性的内存中。作为参考，文件名也可用：</target>
        </trans-unit>
        <trans-unit id="4834eeb362cb8f80edd0fa52b738fe7db255a20e" translate="yes" xml:space="preserve">
          <source>The filesystem path to the root folder where MLComp datasets are stored, if mlcomp_root is None, the MLCOMP_DATASETS_HOME environment variable is looked up instead.</source>
          <target state="translated">存储MLComp数据集的根文件夹的文件系统路径,如果mlcomp_root为None,则查找MLCOMP_DATASETS_HOME环境变量。</target>
        </trans-unit>
        <trans-unit id="5e83705f6f5cf8453734b26cfa75b02cefbf9a06" translate="yes" xml:space="preserve">
          <source>The final sum of similarities is divided by the size of the larger set.</source>
          <target state="translated">最后的相似性之和除以大集的大小。</target>
        </trans-unit>
        <trans-unit id="6023cd4bbdadd0261fe4fb93bbac4f79dd623983" translate="yes" xml:space="preserve">
          <source>The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set).</source>
          <target state="translated">惯性标准的最终值(训练集中所有观测值到最近中心点的平方距离之和)。</target>
        </trans-unit>
        <trans-unit id="65db46a7a2384cdd4de97d3c70536cd9da4c8616" translate="yes" xml:space="preserve">
          <source>The final value of the stress (sum of squared distance of the disparities and the distances for all constrained points).</source>
          <target state="translated">应力的最终值(差距的平方距离与所有约束点的距离之和)。</target>
        </trans-unit>
        <trans-unit id="7e7a148ceab046969e5524f650bd9948ad6ae0ac" translate="yes" xml:space="preserve">
          <source>The first &lt;code&gt;[.9, .1]&lt;/code&gt; in &lt;code&gt;y_pred&lt;/code&gt; denotes 90% probability that the first sample has label 0. The log loss is non-negative.</source>
          <target state="translated">第一 &lt;code&gt;[.9, .1]&lt;/code&gt; 中 &lt;code&gt;y_pred&lt;/code&gt; 表示90％概率的是，第一样本具有标签0。日志损失是非负的。</target>
        </trans-unit>
        <trans-unit id="c41f4a219241dbe01041bd4816c28f057f1f8fc9" translate="yes" xml:space="preserve">
          <source>The first &lt;code&gt;n_samples % n_splits&lt;/code&gt; folds have size &lt;code&gt;n_samples // n_splits + 1&lt;/code&gt;, other folds have size &lt;code&gt;n_samples // n_splits&lt;/code&gt;, where &lt;code&gt;n_samples&lt;/code&gt; is the number of samples.</source>
          <target state="translated">前 &lt;code&gt;n_samples % n_splits&lt;/code&gt; 折叠的大小为 &lt;code&gt;n_samples // n_splits + 1&lt;/code&gt; ，其他折叠的大小为 &lt;code&gt;n_samples // n_splits&lt;/code&gt; ，其中 &lt;code&gt;n_samples&lt;/code&gt; 是样本数。</target>
        </trans-unit>
        <trans-unit id="318062ceb48883ba19b024b3ac85479b5d766c8b" translate="yes" xml:space="preserve">
          <source>The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices. In general, multiple points can be queried at the same time.</source>
          <target state="translated">返回的第一个数组包含与所有接近1.6的点的距离,而第二个数组包含它们的指数。一般来说,可以同时查询多个点。</target>
        </trans-unit>
        <trans-unit id="b072f11f22e8fd40f8f011ced740f859610c5839" translate="yes" xml:space="preserve">
          <source>The first base-kernel of the product-kernel</source>
          <target state="translated">产品内核的第一基核。</target>
        </trans-unit>
        <trans-unit id="2843deb348cef2ebb64b5cb6c478da3efa931340" translate="yes" xml:space="preserve">
          <source>The first base-kernel of the sum-kernel</source>
          <target state="translated">和核的第一基核。</target>
        </trans-unit>
        <trans-unit id="23baf5dbd5fca89380aba285e5a6dca77d9ff4b8" translate="yes" xml:space="preserve">
          <source>The first column of images shows true faces. The next columns illustrate how extremely randomized trees, k nearest neighbors, linear regression and ridge regression complete the lower half of those faces.</source>
          <target state="translated">第一列图像显示的是真实的面孔。下一列说明了极度随机化的树、k个最近的邻居、线性回归和脊回归如何完成这些面孔的下半部分。</target>
        </trans-unit>
        <trans-unit id="758127cc4d0b762bc4a77876dd4551f141ccd7cd" translate="yes" xml:space="preserve">
          <source>The first corresponds to a model with a high noise level and a large length scale, which explains all variations in the data by noise.</source>
          <target state="translated">第一种对应于高噪声水平和大长度尺度的模型,它能用噪声解释数据的所有变化。</target>
        </trans-unit>
        <trans-unit id="2d71e99749591e16661c08d2c19738355ed2fad8" translate="yes" xml:space="preserve">
          <source>The first element of each line can be used to store a target variable to predict.</source>
          <target state="translated">每一行的第一个元素可以用来存储一个要预测的目标变量。</target>
        </trans-unit>
        <trans-unit id="d151d2a5867c5ba125b375d964535691cae0b0d6" translate="yes" xml:space="preserve">
          <source>The first example illustrates how robust covariance estimation can help concentrating on a relevant cluster when another one exists. Here, many observations are confounded into one and break down the empirical covariance estimation. Of course, some screening tools would have pointed out the presence of two clusters (Support Vector Machines, Gaussian Mixture Models, univariate outlier detection, &amp;hellip;). But had it been a high-dimensional example, none of these could be applied that easily.</source>
          <target state="translated">第一个示例说明了健壮的协方差估计如何在存在另一个集群时帮助其集中在相关集群上。在这里，许多观察结果被混淆为一个，并分解了经验协方差估计。当然，某些筛选工具会指出存在两个聚类（支持向量机，高斯混合模型，单变量离群值检测&amp;hellip;&amp;hellip;）。但是，如果这是一个高维度的例子，那么所有这些都不容易被应用。</target>
        </trans-unit>
        <trans-unit id="ecfbbb8300d863a918a602e1c4f90841d4b71a3f" translate="yes" xml:space="preserve">
          <source>The first loader is used for the Face Identification task: a multi-class classification task (hence supervised learning):</source>
          <target state="translated">第一个加载器用于人脸识别任务:多类分类任务(因此是监督学习)。</target>
        </trans-unit>
        <trans-unit id="1272790baab931c65e23f13d4b17128820578899" translate="yes" xml:space="preserve">
          <source>The first model is a classical Gaussian Mixture Model with 10 components fit with the Expectation-Maximization algorithm.</source>
          <target state="translated">第一个模型是一个经典的高斯混合物模型,有10个成分,用期望-最大化算法拟合。</target>
        </trans-unit>
        <trans-unit id="a28da4c4f339f2d2823befd3907c74a3bd7b1459" translate="yes" xml:space="preserve">
          <source>The first plot is a visualization of the decision function for a variety of parameter values on a simplified classification problem involving only 2 input features and 2 possible target classes (binary classification). Note that this kind of plot is not possible to do for problems with more features or target classes.</source>
          <target state="translated">第一张图是一个简化的分类问题的各种参数值的决策函数的可视化,只涉及2个输入特征和2个可能的目标类(二元分类)。请注意,对于有更多特征或目标类的问题,这种图是不可能做的。</target>
        </trans-unit>
        <trans-unit id="fdb1a20b80ba5f009466ef7acd3785afda979734" translate="yes" xml:space="preserve">
          <source>The first plot shows one of the problems with using histograms to visualize the density of points in 1D. Intuitively, a histogram can be thought of as a scheme in which a unit &amp;ldquo;block&amp;rdquo; is stacked above each point on a regular grid. As the top two panels show, however, the choice of gridding for these blocks can lead to wildly divergent ideas about the underlying shape of the density distribution. If we instead center each block on the point it represents, we get the estimate shown in the bottom left panel. This is a kernel density estimation with a &amp;ldquo;top hat&amp;rdquo; kernel. This idea can be generalized to other kernel shapes: the bottom-right panel of the first figure shows a Gaussian kernel density estimate over the same distribution.</source>
          <target state="translated">第一张图显示了使用直方图可视化一维点密度的问题之一。从直觉上讲，直方图可以看作是一种方案，其中单位&amp;ldquo;块&amp;rdquo;堆叠在规则网格的每个点上方。但是，如前两个面板所示，这些块的网格选择可能导致有关密度分布的基本形状的想法大相径庭。如果我们改为将每个块放在其表示的点上居中，则会得到显示在左下方面板中的估计值。这是使用&amp;ldquo;高顶礼帽&amp;rdquo;内核的内核密度估计。这个想法可以推广到其他核形状：第一个图的右下图显示了在相同分布上的高斯核密度估计。</target>
        </trans-unit>
        <trans-unit id="80a6f35d6de94c1655a0ab570a2d81b8e9f0c281" translate="yes" xml:space="preserve">
          <source>The first plot shows that with an increasing number of samples &lt;code&gt;n_samples&lt;/code&gt;, the minimal number of dimensions &lt;code&gt;n_components&lt;/code&gt; increased logarithmically in order to guarantee an &lt;code&gt;eps&lt;/code&gt;-embedding.</source>
          <target state="translated">第一个图显示，随着样本 &lt;code&gt;n_samples&lt;/code&gt; 数量的增加，最小维度 &lt;code&gt;n_components&lt;/code&gt; 的数量以对数形式增加，以确保 &lt;code&gt;eps&lt;/code&gt; 嵌入。</target>
        </trans-unit>
        <trans-unit id="0fa5069414b33f645c4a3a3261f97c50a380bf97" translate="yes" xml:space="preserve">
          <source>The first plot shows the best inertia reached for each combination of the model (&lt;code&gt;KMeans&lt;/code&gt; or &lt;code&gt;MiniBatchKMeans&lt;/code&gt;) and the init method (&lt;code&gt;init=&quot;random&quot;&lt;/code&gt; or &lt;code&gt;init=&quot;kmeans++&quot;&lt;/code&gt;) for increasing values of the &lt;code&gt;n_init&lt;/code&gt; parameter that controls the number of initializations.</source>
          <target state="translated">第一张图显示了模型（ &lt;code&gt;KMeans&lt;/code&gt; 或 &lt;code&gt;MiniBatchKMeans&lt;/code&gt; ）和init方法（ &lt;code&gt;init=&quot;random&quot;&lt;/code&gt; 或 &lt;code&gt;init=&quot;kmeans++&quot;&lt;/code&gt; ）的每种组合的最佳惯性，以增加控制初始化次数的 &lt;code&gt;n_init&lt;/code&gt; 参数的值。</target>
        </trans-unit>
        <trans-unit id="35a5988878bfd98d1cc6f738d4af80878102b501" translate="yes" xml:space="preserve">
          <source>The first row of output array indicates that there are three samples whose true cluster is &amp;ldquo;a&amp;rdquo;. Of them, two are in predicted cluster 0, one is in 1, and none is in 2. And the second row indicates that there are three samples whose true cluster is &amp;ldquo;b&amp;rdquo;. Of them, none is in predicted cluster 0, one is in 1 and two are in 2.</source>
          <target state="translated">输出数组的第一行指示存在三个样本，其真实簇为&amp;ldquo; a&amp;rdquo;。其中两个在预测聚类0中，一个在1中，一个都不在2中。第二行表示存在三个样本，其真实聚类为&amp;ldquo; b&amp;rdquo;。其中，没有一个在预测聚类0中，一个在1中，两个在2中。</target>
        </trans-unit>
        <trans-unit id="94d22c244cece5d36684b54eb3a3c36ef460564d" translate="yes" xml:space="preserve">
          <source>The first two loss functions are lazy, they only update the model parameters if an example violates the margin constraint, which makes training very efficient and may result in sparser models, even when L2 penalty is used.</source>
          <target state="translated">前两个损失函数是懒惰的,它们只有在一个例子违反了边际约束时才会更新模型参数,这使得训练非常高效,即使使用L2惩罚,也可能导致模型更加稀疏。</target>
        </trans-unit>
        <trans-unit id="865ac3c349894331005e5c9a9918fbe1c4576705" translate="yes" xml:space="preserve">
          <source>The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.</source>
          <target state="translated">拟合的模型还可以通过将其投射到最有判别力的方向来降低输入的维度。</target>
        </trans-unit>
        <trans-unit id="47ade7eb6fd49b2a025d9a87896bfcf3ba24402d" translate="yes" xml:space="preserve">
          <source>The fitted model.</source>
          <target state="translated">拟合模型。</target>
        </trans-unit>
        <trans-unit id="14120dbe50cfe7a93d61eb3782205eb1e9322e9d" translate="yes" xml:space="preserve">
          <source>The flexibility of controlling the smoothness of the learned function via \(\nu\) allows adapting to the properties of the true underlying functional relation. The prior and posterior of a GP resulting from a Mat&amp;eacute;rn kernel are shown in the following figure:</source>
          <target state="translated">通过\（\ nu \）控制学习函数的平滑度的灵活性允许适应真正的基础函数关系的属性。下图显示了由Mat&amp;eacute;rn内核生成的GP的先验和后验：</target>
        </trans-unit>
        <trans-unit id="130cb7c407aba5d31bddc566759f6c2692fa934a" translate="yes" xml:space="preserve">
          <source>The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data.</source>
          <target state="translated">下面的流程图是为了给用户提供一点粗略的指导,让他们知道如何处理关于在你的数据上尝试使用哪些估算器的问题。</target>
        </trans-unit>
        <trans-unit id="f162534c96a36fe6b7c86b6775d49d882723bf35" translate="yes" xml:space="preserve">
          <source>The folder names are used as supervised signal label names. The individual file names are not important.</source>
          <target state="translated">文件夹名称用作监督信号标签名称。单个文件名并不重要。</target>
        </trans-unit>
        <trans-unit id="f2db8b6a5929e1458b6943c5d307e1ce8a02db89" translate="yes" xml:space="preserve">
          <source>The folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold.</source>
          <target state="translated">褶皱是近似平衡的,即每个褶皱中不同组的数量近似相同。</target>
        </trans-unit>
        <trans-unit id="b43c7b928f1a786f0281c067a04292ccfddc7963" translate="yes" xml:space="preserve">
          <source>The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the input variables. In mathematical notion, if \(\hat{y}\) is the predicted value.</source>
          <target state="translated">以下是一套用于回归的方法,在这种方法中,预期目标值是输入变量的线性组合。在数学概念中,如果(hat{y}/)是预测值。</target>
        </trans-unit>
        <trans-unit id="81301a81f070c1a1a05c02b69a367906c9b88d21" translate="yes" xml:space="preserve">
          <source>The following clustering assignment is slightly better, since it is homogeneous but not complete:</source>
          <target state="translated">下面的聚类赋值稍好,因为它是同质的,但不完整。</target>
        </trans-unit>
        <trans-unit id="563a9a6f3a15e2fd51ba9e9b647870430bf12e58" translate="yes" xml:space="preserve">
          <source>The following code defines a linear kernel and creates a classifier instance that will use that kernel:</source>
          <target state="translated">下面的代码定义了一个线性内核,并创建了一个使用该内核的分类器实例。</target>
        </trans-unit>
        <trans-unit id="b8e362395586349d1fcb8b526001a536e06f5d5d" translate="yes" xml:space="preserve">
          <source>The following code is a bit verbose, feel free to jump directly to the analysis of the &lt;a href=&quot;#results&quot;&gt;results&lt;/a&gt;.</source>
          <target state="translated">以下代码有点冗长，可以直接跳转到&lt;a href=&quot;#results&quot;&gt;结果&lt;/a&gt;分析。</target>
        </trans-unit>
        <trans-unit id="b414b22c1d4fa210fe910463f00d0b843e42262f" translate="yes" xml:space="preserve">
          <source>The following cross-validation splitters can be used to do that. The grouping identifier for the samples is specified via the &lt;code&gt;groups&lt;/code&gt; parameter.</source>
          <target state="translated">以下交叉验证拆分器可用于执行此操作。样本的分组标识符是通过 &lt;code&gt;groups&lt;/code&gt; 参数指定的。</target>
        </trans-unit>
        <trans-unit id="6757b5cfa24458748d25f81af60d324180a74dc4" translate="yes" xml:space="preserve">
          <source>The following cross-validators can be used in such cases.</source>
          <target state="translated">在这种情况下,可以使用以下交叉验证器。</target>
        </trans-unit>
        <trans-unit id="6da3fe3659ba9973062031cbded9ac1ee9e80559" translate="yes" xml:space="preserve">
          <source>The following dataset has integer features, two of which are the same in every sample. These are removed with the default setting for threshold:</source>
          <target state="translated">以下数据集有整数特征,其中两个特征在每个样本中都是相同的。这些特征在阈值的默认设置下被删除。</target>
        </trans-unit>
        <trans-unit id="f50629b49593a5773731e8a4d6d7fbab3d06afa6" translate="yes" xml:space="preserve">
          <source>The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):</source>
          <target state="translated">下面的例子演示了如何通过拆分数据、拟合模型和连续5次计算分数(每次拆分不同)来估计线性核支持向量机在虹膜数据集上的准确性。</target>
        </trans-unit>
        <trans-unit id="50761302c7bb4483750fc53752f14a1bf26cb023" translate="yes" xml:space="preserve">
          <source>The following example illustrates 16 components extracted using sparse PCA from the Olivetti faces dataset. It can be seen how the regularization term induces many zeros. Furthermore, the natural structure of the data causes the non-zero coefficients to be vertically adjacent. The model does not enforce this mathematically: each component is a vector \(h \in \mathbf{R}^{4096}\), and there is no notion of vertical adjacency except during the human-friendly visualization as 64x64 pixel images. The fact that the components shown below appear local is the effect of the inherent structure of the data, which makes such local patterns minimize reconstruction error. There exist sparsity-inducing norms that take into account adjacency and different kinds of structure; see &lt;a href=&quot;#jen09&quot; id=&quot;id2&quot;&gt;[Jen09]&lt;/a&gt; for a review of such methods. For more details on how to use Sparse PCA, see the Examples section, below.</source>
          <target state="translated">以下示例说明了使用稀疏PCA从Olivetti人脸数据集中提取的16个分量。可以看出正则项如何诱发许多零。此外，数据的自然结构导致非零系数垂直相邻。该模型没有在数学上强制执行此操作：每个分量都是向量\（h \ in \ mathbf {R} ^ {4096} \），并且没有垂直邻接的概念，除非在人类友好的可视化期间为64x64像素图像。下面显示的组件显示为本地的事实是数据固有结构的影响，这使得此类本地模式将重构误差降至最低。存在一些稀疏性准则，这些准则考虑了邻接关系和不同类型的结构。见&lt;a href=&quot;#jen09&quot; id=&quot;id2&quot;&gt;[Jen09]&lt;/a&gt;回顾这些方法。有关如何使用稀疏PCA的更多详细信息，请参见下面的&amp;ldquo;示例&amp;rdquo;部分。</target>
        </trans-unit>
        <trans-unit id="0b4e5d16e2ae8075091f7ec9c797f8dd043b5cfe" translate="yes" xml:space="preserve">
          <source>The following example illustrates how the decision regions may change when a soft &lt;code&gt;VotingClassifier&lt;/code&gt; is used based on an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier:</source>
          <target state="translated">以下示例说明了基于线性支持向量机，决策树和K近邻分类器使用软 &lt;code&gt;VotingClassifier&lt;/code&gt; 时决策区域如何变化：</target>
        </trans-unit>
        <trans-unit id="fcb0630f60eb0a1b1e5514ec57d9a7f6cbd21ce7" translate="yes" xml:space="preserve">
          <source>The following example illustrates the effect of scaling the regularization parameter when using &lt;a href=&quot;../../modules/svm#svm&quot;&gt;Support Vector Machines&lt;/a&gt; for &lt;a href=&quot;../../modules/svm#svm-classification&quot;&gt;classification&lt;/a&gt;. For SVC classification, we are interested in a risk minimization for the equation:</source>
          <target state="translated">以下示例说明了使用&lt;a href=&quot;../../modules/svm#svm&quot;&gt;支持向量机&lt;/a&gt;进行&lt;a href=&quot;../../modules/svm#svm-classification&quot;&gt;分类&lt;/a&gt;时缩放正则化参数的效果。对于SVC分类，我们对方程的风险最小化感兴趣：</target>
        </trans-unit>
        <trans-unit id="35227ebb51bdeaa9a401c5e44c0ca9ca35e1f0e6" translate="yes" xml:space="preserve">
          <source>The following example shows a color-coded representation of the relative importances of each individual pixel for a face recognition task using a &lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt;&lt;code&gt;ExtraTreesClassifier&lt;/code&gt;&lt;/a&gt; model.</source>
          <target state="translated">以下示例显示了使用&lt;a href=&quot;generated/sklearn.ensemble.extratreesclassifier#sklearn.ensemble.ExtraTreesClassifier&quot;&gt; &lt;code&gt;ExtraTreesClassifier&lt;/code&gt; &lt;/a&gt;模型以彩色编码表示的每个单独像素在面部识别任务中的相对重要性。</target>
        </trans-unit>
        <trans-unit id="e4e3a609b2ed09432a121fc3917b934875c3544c" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit an AdaBoost classifier with 100 weak learners:</source>
          <target state="translated">下面的例子展示了如何用100个弱学习者来拟合AdaBoost分类器。</target>
        </trans-unit>
        <trans-unit id="486be98f63ccda08601d245e4a85066d3abba297" translate="yes" xml:space="preserve">
          <source>The following example shows how to fit the majority rule classifier:</source>
          <target state="translated">下面的例子显示了如何拟合多数规则分类器。</target>
        </trans-unit>
        <trans-unit id="376fe34989034d77dfd504e8201c3d4b2dfa1573" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the 5 right informative features in the Friedman #1 dataset.</source>
          <target state="translated">下面的例子显示了如何检索Friedman #1数据集中的5个正确的信息特征。</target>
        </trans-unit>
        <trans-unit id="25b5efc453e06d92e8572618fefd1930c5c8aab3" translate="yes" xml:space="preserve">
          <source>The following example shows how to retrieve the a-priori not known 5 informative features in the Friedman #1 dataset.</source>
          <target state="translated">下面的例子显示了如何在Friedman #1数据集中检索先验不知道的5个信息特征。</target>
        </trans-unit>
        <trans-unit id="e99af0f029f79715308d4d1c9ecbe2acb8983e4d" translate="yes" xml:space="preserve">
          <source>The following example will, for instance, transform some British spelling to American spelling:</source>
          <target state="translated">例如,下面的例子将把一些英式拼法转化为美式拼法。</target>
        </trans-unit>
        <trans-unit id="ad9262c5496a82045b0e3b64071f384beed06659" translate="yes" xml:space="preserve">
          <source>The following experiment is performed on an artificial dataset for binary classification with 100,000 samples (1,000 of them are used for model fitting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The figure shows the estimated probabilities obtained with logistic regression, a linear support-vector classifier (SVC), and linear SVC with both isotonic calibration and sigmoid calibration. The Brier score is a metric which is a combination of calibration loss and refinement loss, &lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt;&lt;code&gt;brier_score_loss&lt;/code&gt;&lt;/a&gt;, reported in the legend (the smaller the better). Calibration loss is defined as the mean squared deviation from empirical probabilities derived from the slope of ROC segments. Refinement loss can be defined as the expected optimal loss as measured by the area under the optimal cost curve.</source>
          <target state="translated">以下实验是在具有100,000个样本（其中1,000个用于模型拟合）的具有20个特征的二进制分类的人工数据集上进行的。在这20个功能中，只有2个具有信息性，而10个是冗余的。该图显示了通过逻辑回归，线性支持向量分类器（SVC）和具有等渗校准和S形校准的线性SVC获得的估计概率。布里尔得分是一个指标，它是校准损耗和精炼损耗的组合，&lt;a href=&quot;generated/sklearn.metrics.brier_score_loss#sklearn.metrics.brier_score_loss&quot;&gt; &lt;code&gt;brier_score_loss&lt;/code&gt; &lt;/a&gt;，在图例中报告（越小越好）。校准损失定义为与从ROC段的斜率得出的经验概率的均方差。精炼损失可以定义为通过最佳成本曲线下的面积测得的预期最佳损失。</target>
        </trans-unit>
        <trans-unit id="2b3bbe94b0947452c8fc360afc3d0e6f8832ae22" translate="yes" xml:space="preserve">
          <source>The following figure compares &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; on an artificial dataset, which consists of a sinusoidal target function and strong noise added to every fifth datapoint. The learned model of &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;SVR&lt;/code&gt; is plotted, where both complexity/regularization and bandwidth of the RBF kernel have been optimized using grid-search. The learned functions are very similar; however, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is approx. seven times faster than fitting &lt;code&gt;SVR&lt;/code&gt; (both with grid-search). However, prediction of 100000 target values is more than three times faster with SVR since it has learned a sparse model using only approx. 1/3 of the 100 training datapoints as support vectors.</source>
          <target state="translated">下图比较了人工数据集上的&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;和 &lt;code&gt;SVR&lt;/code&gt; ，该数据集由正弦目标函数和强噪声添加到每五个数据点组成。绘制了学习的&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;和 &lt;code&gt;SVR&lt;/code&gt; 模型，其中使用网格搜索对RBF内核的复杂度/正则化和带宽进行了优化。学习的功能非常相似。但是，拟合&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;约为。比安装 &lt;code&gt;SVR&lt;/code&gt; 快7倍（两者都使用网格搜索）。但是，使用SVR预测100000个目标值的速度要快三倍以上，因为它仅使用了约10个像素就学会了稀疏模型。 100个训练数据点的1/3作为支持向量。</target>
        </trans-unit>
        <trans-unit id="b11b5ffd2f0d8dbd878722867b9b243b2e73a5ea" translate="yes" xml:space="preserve">
          <source>The following figure compares the location of the non-zeros in W obtained with a simple Lasso or a MultiTaskLasso. The Lasso estimates yields scattered non-zeros while the non-zeros of the MultiTaskLasso are full columns.</source>
          <target state="translated">下图比较了用简单的Lasso或MultiTaskLasso得到的W中非零的位置。Lasso估计得到的是分散的非零,而MultiTaskLasso的非零是满列的。</target>
        </trans-unit>
        <trans-unit id="c66df17d5ba5efe635efde7f93de1fc62e75c222" translate="yes" xml:space="preserve">
          <source>The following figure illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise. The figure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited for learning periodic functions. The kernel&amp;rsquo;s hyperparameters control the smoothness (length_scale) and periodicity of the kernel (periodicity). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR.</source>
          <target state="translated">下图说明了在人工数据集上的两种方法，该数据集由正弦目标函数和强噪声组成。该图比较了基于ExpSineSquared内核的KRR和GPR的学习模型，该模型适合于学习周期性函数。内核的超参数控制内核的平滑度（length_scale）和周期性（周期性）。此外，数据的噪声级别由GPR通过内核中的其他WhiteKernel组件以及KRR的正则化参数alpha明确学习。</target>
        </trans-unit>
        <trans-unit id="c2a01fd3fe0b5b9cd28910783764aacd71509b19" translate="yes" xml:space="preserve">
          <source>The following illustrate the probability density functions of the target before and after applying the logarithmic functions.</source>
          <target state="translated">下面说明应用对数函数前后目标的概率密度函数。</target>
        </trans-unit>
        <trans-unit id="b0293fdb9cc21af9db21d1f9f61ccde2a4565147" translate="yes" xml:space="preserve">
          <source>The following image shows how a dictionary learned from 4x4 pixel image patches extracted from part of the image of a raccoon face looks like.</source>
          <target state="translated">下图显示了从浣熊脸部的部分图像中提取的4x4像素图像补丁所学习的字典是怎样的。</target>
        </trans-unit>
        <trans-unit id="a988a0a50289dff96522065b976c597b6e4e63e8" translate="yes" xml:space="preserve">
          <source>The following image shows on the data above the estimated probability using a Gaussian naive Bayes classifier without calibration, with a sigmoid calibration and with a non-parametric isotonic calibration. One can observe that the non-parametric model provides the most accurate probability estimates for samples in the middle, i.e., 0.5.</source>
          <target state="translated">下图在上面的数据上显示了使用高斯奈夫贝叶斯分类器在没有校准的情况下,使用sigmoid校准和使用非参数同位素校准的估计概率。可以观察到,非参数模型为中间的样本提供了最准确的概率估计,即0.5。</target>
        </trans-unit>
        <trans-unit id="b45c55863d114ed9459a8475837831820ff65766" translate="yes" xml:space="preserve">
          <source>The following images demonstrate the benefit of probability calibration. The first image present a dataset with 2 classes and 3 blobs of data. The blob in the middle contains random samples of each class. The probability for the samples in this blob should be 0.5.</source>
          <target state="translated">下面的图片展示了概率校准的好处。第一张图片展示了一个有2个类和3个数据块的数据集。中间的blob包含每个类的随机样本。这个blob中样本的概率应该是0.5。</target>
        </trans-unit>
        <trans-unit id="807c0e6975f025a025e2951d9c9164c1f3454c71" translate="yes" xml:space="preserve">
          <source>The following loss functions are supported and can be specified using the parameter &lt;code&gt;loss&lt;/code&gt;:</source>
          <target state="translated">支持以下损失函数，可以使用参数 &lt;code&gt;loss&lt;/code&gt; 指定：</target>
        </trans-unit>
        <trans-unit id="70624e811991b8fac875c2f0dd0db6a8fb2e1e2b" translate="yes" xml:space="preserve">
          <source>The following plots demonstrate the impact of the number of clusters and number of samples on various clustering performance evaluation metrics.</source>
          <target state="translated">下面的图展示了聚类数量和样本数量对各种聚类性能评价指标的影响。</target>
        </trans-unit>
        <trans-unit id="661cd7d1cca4025c0fd4f4a1e938e7140f6ef24b" translate="yes" xml:space="preserve">
          <source>The following sections contain further explanations and examples that illustrate how the tf-idfs are computed exactly and how the tf-idfs computed in scikit-learn&amp;rsquo;s &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt;&lt;code&gt;TfidfTransformer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt;&lt;code&gt;TfidfVectorizer&lt;/code&gt;&lt;/a&gt; differ slightly from the standard textbook notation that defines the idf as</source>
          <target state="translated">以下各节包含进一步的说明和示例，这些示例和示例说明了如何精确计算tf-idfs，以及scikit-learn的&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidftransformer#sklearn.feature_extraction.text.TfidfTransformer&quot;&gt; &lt;code&gt;TfidfTransformer&lt;/code&gt; &lt;/a&gt;和&lt;a href=&quot;generated/sklearn.feature_extraction.text.tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer&quot;&gt; &lt;code&gt;TfidfVectorizer&lt;/code&gt; 中&lt;/a&gt;计算的tf-idfs 与标准教科书符号（将idf定义为</target>
        </trans-unit>
        <trans-unit id="78ad5f101e233b65633dffc5c68bec29c1eae0b0" translate="yes" xml:space="preserve">
          <source>The following sections list utilities to generate indices that can be used to generate dataset splits according to different cross validation strategies.</source>
          <target state="translated">以下各节列出了用于生成指数的实用程序,这些指数可用于根据不同的交叉验证策略生成数据集分割。</target>
        </trans-unit>
        <trans-unit id="5c75d78bf5e5b93a438a72e22f9eee2db09a82ed" translate="yes" xml:space="preserve">
          <source>The following snippet demonstrates how to replace missing values, encoded as &lt;code&gt;np.nan&lt;/code&gt;, using the mean value of the columns (axis 0) that contain the missing values:</source>
          <target state="translated">以下代码段演示了如何使用包含缺失值的列（轴0）的平均值替换编码为 &lt;code&gt;np.nan&lt;/code&gt; 的缺失值：</target>
        </trans-unit>
        <trans-unit id="46781e9d9b616f25e94b9f1718ce2c06cffa693e" translate="yes" xml:space="preserve">
          <source>The following two references explain the iterations used in the coordinate descent solver of scikit-learn, as well as the duality gap computation used for convergence control.</source>
          <target state="translated">以下两个参考文献解释了scikit-learn的坐标下降求解器中使用的迭代,以及用于收敛控制的二元性缺口计算。</target>
        </trans-unit>
        <trans-unit id="d492404a63c6d65da32b70e6dcf87e75a94b6a4d" translate="yes" xml:space="preserve">
          <source>The form of the model learned by &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; is identical to support vector regression (&lt;code&gt;SVR&lt;/code&gt;). However, different loss functions are used: KRR uses squared error loss while support vector regression uses \(\epsilon\)-insensitive loss, both combined with l2 regularization. In contrast to &lt;code&gt;SVR&lt;/code&gt;, fitting &lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt;&lt;code&gt;KernelRidge&lt;/code&gt;&lt;/a&gt; can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for \(\epsilon &amp;gt; 0\), at prediction-time.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;学习的模型的形式与支持向量回归（ &lt;code&gt;SVR&lt;/code&gt; ）相同。但是，使用了不同的损失函数：KRR使用平方误差损失，而支持向量回归使用对\（\ epsilon \）不敏感的损失，二者均与12正则化结合。与 &lt;code&gt;SVR&lt;/code&gt; 相比，拟合&lt;a href=&quot;generated/sklearn.kernel_ridge.kernelridge#sklearn.kernel_ridge.KernelRidge&quot;&gt; &lt;code&gt;KernelRidge&lt;/code&gt; &lt;/a&gt;可以封闭形式进行，对于中等规模的数据集通常更快。另一方面，学习的模型是非稀疏的，因此比SVR慢，后者在预测时学习\（\ epsilon&amp;gt; 0 \）的稀疏模型。</target>
        </trans-unit>
        <trans-unit id="cb6eb14fd1b6ffac38e7d6c15d36b7076beafa85" translate="yes" xml:space="preserve">
          <source>The form of the model learned by KRR is identical to support vector regression (SVR). However, different loss functions are used: KRR uses squared error loss while support vector regression uses epsilon-insensitive loss, both combined with l2 regularization. In contrast to SVR, fitting a KRR model can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for epsilon &amp;gt; 0, at prediction-time.</source>
          <target state="translated">KRR学习的模型的形式与支持向量回归（SVR）相同。但是，使用了不同的损失函数：KRR使用平方误差损失，而支持向量回归使用epsilon不敏感损失，两者均与12正则化结合。与SVR相比，拟合KRR模型可以封闭形式进行，对于中等规模的数据集通常更快。另一方面，学习的模型是非稀疏的，因此比SVR慢，后者在预测时学习epsilon&amp;gt; 0的稀疏模型。</target>
        </trans-unit>
        <trans-unit id="785bfd19f7d4d298d07ec2673131bad97bee7c7c" translate="yes" xml:space="preserve">
          <source>The form of these kernels is as follows:</source>
          <target state="translated">这些内核的形式如下:</target>
        </trans-unit>
        <trans-unit id="99fa4cbad7a403537a73165a2b1d717c58bb9b6b" translate="yes" xml:space="preserve">
          <source>The formula that is used to compute the tf-idf of term t is tf-idf(d, t) = tf(t) * idf(d, t), and the idf is computed as idf(d, t) = log [ n / df(d, t) ] + 1 (if &lt;code&gt;smooth_idf=False&lt;/code&gt;), where n is the total number of documents and df(d, t) is the document frequency; the document frequency is the number of documents d that contain term t. The effect of adding &amp;ldquo;1&amp;rdquo; to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(d, t) = log [ n / (df(d, t) + 1) ]).</source>
          <target state="translated">用于计算项t的tf-idf的公式为tf-idf（d，t）= tf（t）* idf（d，t），并且idf计算为idf（d，t）= log [n / df（d，t）] + 1（如果 &lt;code&gt;smooth_idf=False&lt;/code&gt; ），其中n是文档总数，df（d，t）是文档频率；文档频率是包含项t的文档数d。在上式中的idf中添加&amp;ldquo; 1&amp;rdquo;的效果是，不会完全忽略idf为零的项，即出现在训练集中所有文档中的项。 （请注意，上面的idf公式不同于标准教科书中将idf定义为idf（d，t）= log [n /（df（d，t）+ 1）]的符号）。</target>
        </trans-unit>
        <trans-unit id="2757038bccf5826fff274cfe8684f779a3893302" translate="yes" xml:space="preserve">
          <source>The formula used here does not correspond to the one given in the article. In the original article, formula (23) states that 2/p is multiplied by Trace(cov*cov) in both the numerator and denominator, but this operation is omitted because for a large p, the value of 2/p is so small that it doesn&amp;rsquo;t affect the value of the estimator.</source>
          <target state="translated">这里使用的公式与文章中给出的公式不对应。在原始文章中，公式（23）指出在分子和分母中将2 / p乘以Trace（cov * cov），但是省略了此运算，因为对于大的p，2 / p的值是如此之小它不会影响估计器的值。</target>
        </trans-unit>
        <trans-unit id="25426ebad0f81610af376f208a1d7f7cdb7c96c5" translate="yes" xml:space="preserve">
          <source>The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. &lt;code&gt;subsample&lt;/code&gt; interacts with the parameter &lt;code&gt;n_estimators&lt;/code&gt;. Choosing &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; leads to a reduction of variance and an increase in bias.</source>
          <target state="translated">用于拟合各个基础学习者的样本比例。如果小于1.0，则将导致随机梯度增强。 &lt;code&gt;subsample&lt;/code&gt; 与参数 &lt;code&gt;n_estimators&lt;/code&gt; 交互。选择 &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; 会导致方差减少和偏差增加。</target>
        </trans-unit>
        <trans-unit id="568fbbbfe83b2ef390104a99310641880cb62ce5" translate="yes" xml:space="preserve">
          <source>The fraction of samples to be used in each randomized design. Should be between 0 and 1. If 1, all samples are used.</source>
          <target state="translated">每个随机设计中使用的样本分数。应该在0和1之间,如果是1,则使用所有样本。</target>
        </trans-unit>
        <trans-unit id="52b16a6d5efc38a8b1d594773d860718286a7706" translate="yes" xml:space="preserve">
          <source>The fraction of samples whose class are randomly exchanged. Larger values introduce noise in the labels and make the classification task harder.</source>
          <target state="translated">类随机交换的样本的分数。较大的数值会在标签中引入噪声,使分类任务更加困难。</target>
        </trans-unit>
        <trans-unit id="2b8586795acd1de1d253165ce5ff74dbee8020e0" translate="yes" xml:space="preserve">
          <source>The free parameters in the model are C and epsilon.</source>
          <target state="translated">模型中的自由参数为C和epsilon。</target>
        </trans-unit>
        <trans-unit id="852901dbf93c375963721fc84a4888e6b45c78dd" translate="yes" xml:space="preserve">
          <source>The full description of the dataset</source>
          <target state="translated">数据集的完整描述</target>
        </trans-unit>
        <trans-unit id="19877feb57bb09f6d25fbaaa466f42d45ca4d944" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.feature_extraction.image.img_to_graph#sklearn.feature_extraction.image.img_to_graph&quot;&gt;&lt;code&gt;img_to_graph&lt;/code&gt;&lt;/a&gt; returns such a matrix from a 2D or 3D image. Similarly, &lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt;&lt;code&gt;grid_to_graph&lt;/code&gt;&lt;/a&gt; build a connectivity matrix for images given the shape of these image.</source>
          <target state="translated">函数&lt;a href=&quot;generated/sklearn.feature_extraction.image.img_to_graph#sklearn.feature_extraction.image.img_to_graph&quot;&gt; &lt;code&gt;img_to_graph&lt;/code&gt; &lt;/a&gt;从2D或3D图像返回这样的矩阵。类似地，&lt;a href=&quot;generated/sklearn.feature_extraction.image.grid_to_graph#sklearn.feature_extraction.image.grid_to_graph&quot;&gt; &lt;code&gt;grid_to_graph&lt;/code&gt; &lt;/a&gt;在给定图像形状的情况下为图像构建连接矩阵。</target>
        </trans-unit>
        <trans-unit id="6bbdd98a7d5d87ddd3bbdfb8569481a5e5e04495" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.cohen_kappa_score#sklearn.metrics.cohen_kappa_score&quot;&gt;&lt;code&gt;cohen_kappa_score&lt;/code&gt;&lt;/a&gt; computes &lt;a href=&quot;https://en.wikipedia.org/wiki/Cohen%27s_kappa&quot;&gt;Cohen&amp;rsquo;s kappa&lt;/a&gt; statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.</source>
          <target state="translated">函数&lt;a href=&quot;generated/sklearn.metrics.cohen_kappa_score#sklearn.metrics.cohen_kappa_score&quot;&gt; &lt;code&gt;cohen_kappa_score&lt;/code&gt; &lt;/a&gt;计算&lt;a href=&quot;https://en.wikipedia.org/wiki/Cohen%27s_kappa&quot;&gt;科恩的kappa&lt;/a&gt;统计信息。此措施旨在比较不同的人类注释者（而不是分类者）与地面事实的标签。</target>
        </trans-unit>
        <trans-unit id="93f21bf6b976a0d8004246b9140ac3f9b0310fba" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.laplacian_kernel#sklearn.metrics.pairwise.laplacian_kernel&quot;&gt;&lt;code&gt;laplacian_kernel&lt;/code&gt;&lt;/a&gt; is a variant on the radial basis function kernel defined as:</source>
          <target state="translated">函数&lt;a href=&quot;generated/sklearn.metrics.pairwise.laplacian_kernel#sklearn.metrics.pairwise.laplacian_kernel&quot;&gt; &lt;code&gt;laplacian_kernel&lt;/code&gt; &lt;/a&gt;是径向基函数内核的一个变体，定义为：</target>
        </trans-unit>
        <trans-unit id="3fe1ad7d2ae4a1d31d30a7b5c63e81d53b7568bb" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt;&lt;code&gt;linear_kernel&lt;/code&gt;&lt;/a&gt; computes the linear kernel, that is, a special case of &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt;&lt;code&gt;polynomial_kernel&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;degree=1&lt;/code&gt; and &lt;code&gt;coef0=0&lt;/code&gt; (homogeneous). If &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are column vectors, their linear kernel is:</source>
          <target state="translated">功能&lt;a href=&quot;generated/sklearn.metrics.pairwise.linear_kernel#sklearn.metrics.pairwise.linear_kernel&quot;&gt; &lt;code&gt;linear_kernel&lt;/code&gt; &lt;/a&gt;计算线性核，也就是，的一种特殊情况&lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt; &lt;code&gt;polynomial_kernel&lt;/code&gt; &lt;/a&gt;与 &lt;code&gt;degree=1&lt;/code&gt; 和 &lt;code&gt;coef0=0&lt;/code&gt; （均相）。如果 &lt;code&gt;x&lt;/code&gt; 和 &lt;code&gt;y&lt;/code&gt; 是列向量，则它们的线性核为：</target>
        </trans-unit>
        <trans-unit id="72295044331d14363e1e1fe37cea504256d7c67f" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt;&lt;code&gt;polynomial_kernel&lt;/code&gt;&lt;/a&gt; computes the degree-d polynomial kernel between two vectors. The polynomial kernel represents the similarity between two vectors. Conceptually, the polynomial kernels considers not only the similarity between vectors under the same dimension, but also across dimensions. When used in machine learning algorithms, this allows to account for feature interaction.</source>
          <target state="translated">函数&lt;a href=&quot;generated/sklearn.metrics.pairwise.polynomial_kernel#sklearn.metrics.pairwise.polynomial_kernel&quot;&gt; &lt;code&gt;polynomial_kernel&lt;/code&gt; &lt;/a&gt;计算两个向量之间的度数多项式内核。多项式核代表两个向量之间的相似性。从概念上讲，多项式内核不仅考虑相同维度下向量之间的相似性，而且考虑跨维度。在机器学习算法中使用时，这可以解决要素交互问题。</target>
        </trans-unit>
        <trans-unit id="2f5b051b13aa3478b0b4a17587fa0bec1e6b87b4" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.rbf_kernel#sklearn.metrics.pairwise.rbf_kernel&quot;&gt;&lt;code&gt;rbf_kernel&lt;/code&gt;&lt;/a&gt; computes the radial basis function (RBF) kernel between two vectors. This kernel is defined as:</source>
          <target state="translated">函数&lt;a href=&quot;generated/sklearn.metrics.pairwise.rbf_kernel#sklearn.metrics.pairwise.rbf_kernel&quot;&gt; &lt;code&gt;rbf_kernel&lt;/code&gt; &lt;/a&gt;计算两个向量之间的径向基函数（RBF）内核。该内核定义为：</target>
        </trans-unit>
        <trans-unit id="a4ac87fe4bd82908551f017a1e984b845dfe00ca" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.pairwise.sigmoid_kernel#sklearn.metrics.pairwise.sigmoid_kernel&quot;&gt;&lt;code&gt;sigmoid_kernel&lt;/code&gt;&lt;/a&gt; computes the sigmoid kernel between two vectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer Perceptron (because, in the neural network field, it is often used as neuron activation function). It is defined as:</source>
          <target state="translated">函数&lt;a href=&quot;generated/sklearn.metrics.pairwise.sigmoid_kernel#sklearn.metrics.pairwise.sigmoid_kernel&quot;&gt; &lt;code&gt;sigmoid_kernel&lt;/code&gt; &lt;/a&gt;计算两个向量之间的Sigmoid内核。乙状结肠也称为双曲正切或多层感知器（因为在神经网络领域，它经常被用作神经元激活函数）。它定义为：</target>
        </trans-unit>
        <trans-unit id="2e327cd188c9064345dfa4a6a6764985ae429bc7" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt;&lt;code&gt;roc_curve&lt;/code&gt;&lt;/a&gt; computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;receiver operating characteristic curve, or ROC curve&lt;/a&gt;. Quoting Wikipedia :</source>
          <target state="translated">函数&lt;a href=&quot;generated/sklearn.metrics.roc_curve#sklearn.metrics.roc_curve&quot;&gt; &lt;code&gt;roc_curve&lt;/code&gt; &lt;/a&gt;计算&lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;接收器工作特性曲线或ROC曲线&lt;/a&gt;。引用维基百科：</target>
        </trans-unit>
        <trans-unit id="a52cb65c85990e02ab77ee98f8132a1c3b7a7b6c" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt;&lt;code&gt;cross_val_predict&lt;/code&gt;&lt;/a&gt; has a similar interface to &lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt;&lt;code&gt;cross_val_score&lt;/code&gt;&lt;/a&gt;, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised).</source>
          <target state="translated">功能&lt;a href=&quot;generated/sklearn.model_selection.cross_val_predict#sklearn.model_selection.cross_val_predict&quot;&gt; &lt;code&gt;cross_val_predict&lt;/code&gt; &lt;/a&gt;具有相似的接口&lt;a href=&quot;generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score&quot;&gt; &lt;code&gt;cross_val_score&lt;/code&gt; &lt;/a&gt;，但返回时，为输入中的每个元素，这是该元件得到的，当它是在测试组中的预测。只能使用将所有元素完全一次分配给测试集的交叉验证策略（否则会引发异常）。</target>
        </trans-unit>
        <trans-unit id="9da333e152484b3ac872458a2a577c489d5042de" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.model_selection.validation_curve#sklearn.model_selection.validation_curve&quot;&gt;&lt;code&gt;validation_curve&lt;/code&gt;&lt;/a&gt; can help in this case:</source>
          <target state="translated">在这种情况下，&lt;a href=&quot;generated/sklearn.model_selection.validation_curve#sklearn.model_selection.validation_curve&quot;&gt; &lt;code&gt;validation_curve&lt;/code&gt; &lt;/a&gt;函数可以提供帮助：</target>
        </trans-unit>
        <trans-unit id="ef1bfedb8d96897b52fb746234adf67b6ec8f0b2" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt;&lt;code&gt;normalize&lt;/code&gt;&lt;/a&gt; provides a quick and easy way to perform this operation on a single array-like dataset, either using the &lt;code&gt;l1&lt;/code&gt; or &lt;code&gt;l2&lt;/code&gt; norms:</source>
          <target state="translated">通过使用 &lt;code&gt;l1&lt;/code&gt; 或 &lt;code&gt;l2&lt;/code&gt; 规范，函数&lt;a href=&quot;generated/sklearn.preprocessing.normalize#sklearn.preprocessing.normalize&quot;&gt; &lt;code&gt;normalize&lt;/code&gt; &lt;/a&gt;提供了一种快速简单的方法来对单个类似数组的数据集执行此操作：</target>
        </trans-unit>
        <trans-unit id="621182b188f45e3dd8884ee1d03015ee530041c8" translate="yes" xml:space="preserve">
          <source>The function &lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt;&lt;code&gt;scale&lt;/code&gt;&lt;/a&gt; provides a quick and easy way to perform this operation on a single array-like dataset:</source>
          <target state="translated">函数&lt;a href=&quot;generated/sklearn.preprocessing.scale#sklearn.preprocessing.scale&quot;&gt; &lt;code&gt;scale&lt;/code&gt; &lt;/a&gt;提供了一种快速简单的方法来对单个类似数组的数据集执行此操作：</target>
        </trans-unit>
        <trans-unit id="14656519a034ad24c06a19460128dde85e00e72d" translate="yes" xml:space="preserve">
          <source>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in &lt;a href=&quot;#r37d39d7589e2-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; and &lt;a href=&quot;#r37d39d7589e2-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Both methods are based on the idea originally proposed in &lt;a href=&quot;#r37d39d7589e2-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">该函数依赖于非参数方法，该方法基于&lt;a href=&quot;#r37d39d7589e2-2&quot; id=&quot;id2&quot;&gt;[&lt;/a&gt; k]最近邻居距离的熵估计，如[2]和&lt;a href=&quot;#r37d39d7589e2-3&quot; id=&quot;id3&quot;&gt;[3]中所述&lt;/a&gt;。两种方法都基于&lt;a href=&quot;#r37d39d7589e2-4&quot; id=&quot;id4&quot;&gt;[4]中&lt;/a&gt;最初提出的想法。</target>
        </trans-unit>
        <trans-unit id="cf1d60e35345e80734f9e0bef9342c1dede910a1" translate="yes" xml:space="preserve">
          <source>The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances as described in &lt;a href=&quot;#r50b872b699c4-2&quot; id=&quot;id2&quot;&gt;[2]&lt;/a&gt; and &lt;a href=&quot;#r50b872b699c4-3&quot; id=&quot;id3&quot;&gt;[3]&lt;/a&gt;. Both methods are based on the idea originally proposed in &lt;a href=&quot;#r50b872b699c4-4&quot; id=&quot;id4&quot;&gt;[4]&lt;/a&gt;.</source>
          <target state="translated">该函数依赖于非参数方法，该方法基于&lt;a href=&quot;#r50b872b699c4-2&quot; id=&quot;id2&quot;&gt;[&lt;/a&gt; k]最近邻居距离的熵估计，如[2]和&lt;a href=&quot;#r50b872b699c4-3&quot; id=&quot;id3&quot;&gt;[3]中所述&lt;/a&gt;。两种方法都基于&lt;a href=&quot;#r50b872b699c4-4&quot; id=&quot;id4&quot;&gt;[4]中&lt;/a&gt;最初提出的想法。</target>
        </trans-unit>
        <trans-unit id="fb8314efd5d7c36c4d228413206cd5331b82e456" translate="yes" xml:space="preserve">
          <source>The function requires either the argument &lt;code&gt;grid&lt;/code&gt; which specifies the values of the target features on which the partial dependence function should be evaluated or the argument &lt;code&gt;X&lt;/code&gt; which is a convenience mode for automatically creating &lt;code&gt;grid&lt;/code&gt; from the training data. If &lt;code&gt;X&lt;/code&gt; is given, the &lt;code&gt;axes&lt;/code&gt; value returned by the function gives the axis for each target feature.</source>
          <target state="translated">该函数需要参数 &lt;code&gt;grid&lt;/code&gt; 该参数网格指定应在其上评估部分依赖函数的目标特征的值）或参数 &lt;code&gt;X&lt;/code&gt; (这是一种方便模式，用于根据训练数据自动创建 &lt;code&gt;grid&lt;/code&gt; 。如果给定 &lt;code&gt;X&lt;/code&gt; ，则函数返回的 &lt;code&gt;axes&lt;/code&gt; 值将给出每个目标特征的轴。</target>
        </trans-unit>
        <trans-unit id="d2fedea237bdc334f6322c4692b523df3b5f6fbf" translate="yes" xml:space="preserve">
          <source>The function to be decorated</source>
          <target state="translated">需要装饰的功能</target>
        </trans-unit>
        <trans-unit id="f7fc9606332ff37416d86060e52aa56595a5e3ce" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;friedman_mse&amp;rdquo; for the mean squared error with improvement score by Friedman, &amp;ldquo;mse&amp;rdquo; for mean squared error, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error. The default value of &amp;ldquo;friedman_mse&amp;rdquo; is generally the best as it can provide a better approximation in some cases.</source>
          <target state="translated">衡量分割质量的功能。支持的标准是&amp;ldquo;弗里德曼_mse&amp;rdquo;（Friedman_mse）表示具有弗里德曼改进得分的均方误差，&amp;ldquo; mse&amp;rdquo;表示均方误差，而&amp;ldquo; mae&amp;rdquo;表示平均绝对误差。通常，默认值&amp;ldquo; friedman_mse&amp;rdquo;是最好的，因为它在某些情况下可以提供更好的近似值。</target>
        </trans-unit>
        <trans-unit id="8c0bede693f27f1257cedfa964bd89180e6bfada" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;gini&amp;rdquo; for the Gini impurity and &amp;ldquo;entropy&amp;rdquo; for the information gain.</source>
          <target state="translated">衡量分割质量的功能。支持的标准是对于基尼杂质为&amp;ldquo;基尼&amp;rdquo;，对于信息增益为&amp;ldquo;熵&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="03e746b06910ee8c07bc239c0b780e5294140dfb" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;gini&amp;rdquo; for the Gini impurity and &amp;ldquo;entropy&amp;rdquo; for the information gain. Note: this parameter is tree-specific.</source>
          <target state="translated">衡量分割质量的功能。支持的标准是对于基尼杂质为&amp;ldquo;基尼&amp;rdquo;，对于信息增益为&amp;ldquo;熵&amp;rdquo;。注意：此参数是特定于树的。</target>
        </trans-unit>
        <trans-unit id="c211c5fb9289c19b7ab7fb609c5f42d0fd7fb417" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;mse&amp;rdquo; for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, &amp;ldquo;friedman_mse&amp;rdquo;, which uses mean squared error with Friedman&amp;rsquo;s improvement score for potential splits, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error, which minimizes the L1 loss using the median of each terminal node.</source>
          <target state="translated">衡量分割质量的功能。支持的标准是均方误差的&amp;ldquo; mse&amp;rdquo;，等于方差减少作为特征选择标准，并且使用每个终端节点的均值&amp;ldquo; friedman_mse&amp;rdquo;来最小化L2损失，该方法使用均方误差和弗里德曼改进分数作为潜在值拆分，并使用&amp;ldquo; mae&amp;rdquo;表示平均绝对误差，使用每个终端节点的中值将L1损耗最小化。</target>
        </trans-unit>
        <trans-unit id="bf422f8c78875448c6e34d2c008baf4a5621c47d" translate="yes" xml:space="preserve">
          <source>The function to measure the quality of a split. Supported criteria are &amp;ldquo;mse&amp;rdquo; for the mean squared error, which is equal to variance reduction as feature selection criterion, and &amp;ldquo;mae&amp;rdquo; for the mean absolute error.</source>
          <target state="translated">衡量分割质量的功能。支持的标准是均方误差的&amp;ldquo; mse&amp;rdquo;（等于特征选择标准的方差减少）和均值绝对误差的&amp;ldquo; mae&amp;rdquo;。</target>
        </trans-unit>
        <trans-unit id="8b3c9548b1b42f5af71c7bfe5f885c712284a1e6" translate="yes" xml:space="preserve">
          <source>The function which is applied on each chunk of the distance matrix, reducing it to needed values. &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; is called repeatedly, where &lt;code&gt;D_chunk&lt;/code&gt; is a contiguous vertical slice of the pairwise distance matrix, starting at row &lt;code&gt;start&lt;/code&gt;. It should return an array, a list, or a sparse matrix of length &lt;code&gt;D_chunk.shape[0]&lt;/code&gt;, or a tuple of such objects.</source>
          <target state="translated">应用于距离矩阵的每个块的函数，将其减小为所需的值。 &lt;code&gt;reduce_func(D_chunk, start)&lt;/code&gt; 被重复调用，其中 &lt;code&gt;D_chunk&lt;/code&gt; 是成对距离矩阵的连续垂直切片，从行 &lt;code&gt;start&lt;/code&gt; 开始。它应该返回一个数组，一个列表或一个长度为 &lt;code&gt;D_chunk.shape[0]&lt;/code&gt; 的稀疏矩阵，或者一个此类对象的元组。</target>
        </trans-unit>
        <trans-unit id="34f983a96e6e1ba7a36ae52fb70df739e3a5aca9" translate="yes" xml:space="preserve">
          <source>The functional form of the G function used in the approximation to neg-entropy. Could be either &amp;lsquo;logcosh&amp;rsquo;, &amp;lsquo;exp&amp;rsquo;, or &amp;lsquo;cube&amp;rsquo;. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. Example:</source>
          <target state="translated">G函数的函数形式，用于近似负熵。可以是&amp;ldquo; logcosh&amp;rdquo;，&amp;ldquo; exp&amp;rdquo;或&amp;ldquo; cube&amp;rdquo;。您也可以提供自己的功能。它应该返回一个包含该函数值及其派生值的元组。例：</target>
        </trans-unit>
        <trans-unit id="f3580ae3ff67a44925f6075f1fc46034cd1f3d14" translate="yes" xml:space="preserve">
          <source>The generated array.</source>
          <target state="translated">生成的数组。</target>
        </trans-unit>
        <trans-unit id="22dd341f8c92381f47326cd6fbe4fff46dd59a6b" translate="yes" xml:space="preserve">
          <source>The generated matrix.</source>
          <target state="translated">生成的矩阵。</target>
        </trans-unit>
        <trans-unit id="b670a7959baa25f4a9c290d5df951114cede8bad" translate="yes" xml:space="preserve">
          <source>The generated samples.</source>
          <target state="translated">生成的样品。</target>
        </trans-unit>
        <trans-unit id="61a0705074aa05bf429a2ff6dd9f09842d1c86f2" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the centers. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">生成器用于初始化中心。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="473c7ed23812d4bf8fcf6c97ab0551b7301ccf95" translate="yes" xml:space="preserve">
          <source>The generator used to initialize the codebook. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">用于初始化密码本的生成器。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="4e831547e83750929880db075c40fc4aa61eb4b8" translate="yes" xml:space="preserve">
          <source>The generator used to randomize the design. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;.</source>
          <target state="translated">生成器用于随机化设计。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="cd84f22d9e60e3f86bb2455d78aaf109647d76b1" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select a subset of samples. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by &lt;code&gt;np.random&lt;/code&gt;. Used when &lt;code&gt;sample_size is not None&lt;/code&gt;.</source>
          <target state="translated">生成器用于随机选择样本子集。如果为int，则random_state是随机数生成器使用的种子；否则为false。如果是RandomState实例，则random_state是随机数生成器；如果没有，随机数生成器所使用的RandomState实例 &lt;code&gt;np.random&lt;/code&gt; 。当 &lt;code&gt;sample_size is not None&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="36aa9d7d033388d006150bbc613bf9f61afb2f31" translate="yes" xml:space="preserve">
          <source>The generator used to randomly select the samples from input points for bandwidth estimation. Use an int to make the randomness deterministic. See &lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;Glossary&lt;/a&gt;.</source>
          <target state="translated">该发生器用于从输入点随机选择样本以进行带宽估计。使用int可以确定随机性。请参阅&lt;a href=&quot;http://scikit-learn.org/stable/glossary.html#term-random-state&quot;&gt;词汇表&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="6c9c479a6511fc655c70a7c51ab0a85c04034b21" translate="yes" xml:space="preserve">
          <source>The goal is to measure the latency one can expect when doing predictions either in bulk or atomic (i.e. one by one) mode.</source>
          <target state="translated">目标是测量在批量或原子(即逐个)模式下进行预测时可以预期的延迟。</target>
        </trans-unit>
        <trans-unit id="95b3715b7309073ce23a5e1ae6bd1c83fb5ab128" translate="yes" xml:space="preserve">
          <source>The goal of &lt;strong&gt;ensemble methods&lt;/strong&gt; is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.</source>
          <target state="translated">&lt;strong&gt;集成方法&lt;/strong&gt;的目标是将使用给定学习算法构建的几个基本估计量的预测结合起来，以提高单个估计量的通用性/鲁棒性。</target>
        </trans-unit>
        <trans-unit id="4be143708c968fbd1f8e4e8288622de6f1251f81" translate="yes" xml:space="preserve">
          <source>The goal of this example is to analyze the graph of links inside wikipedia articles to rank articles by relative importance according to this eigenvector centrality.</source>
          <target state="translated">这个例子的目标是分析维基百科文章里面的链接图,根据这个特征向量的中心性对文章进行相对重要性排名。</target>
        </trans-unit>
        <trans-unit id="5095c3c5239296131fc48d25860f3771ab9b91e3" translate="yes" xml:space="preserve">
          <source>The goal of this example is to show intuitively how the metrics behave, and not to find good clusters for the digits. This is why the example works on a 2D embedding.</source>
          <target state="translated">这个例子的目标是直观地展示度量的行为,而不是为数字找到好的聚类。这就是为什么这个例子在二维嵌入上工作的原因。</target>
        </trans-unit>
        <trans-unit id="20bd11aa678c9e04777a40ec0f194ab6b7581aa6" translate="yes" xml:space="preserve">
          <source>The goal of this guide is to explore some of the main &lt;code&gt;scikit-learn&lt;/code&gt; tools on a single practical task: analyzing a collection of text documents (newsgroups posts) on twenty different topics.</source>
          <target state="translated">本指南的目的是探索一项实际任务中的一些主要 &lt;code&gt;scikit-learn&lt;/code&gt; 工具：分析有关二十个不同主题的文本文档（新闻组帖子）的集合。</target>
        </trans-unit>
        <trans-unit id="c224302bdf144aacfdfc260a6638fe1107c2e8d4" translate="yes" xml:space="preserve">
          <source>The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.</source>
          <target state="translated">使用tf-idf代替给定文档中token出现的原始频率的目的,是为了降低在给定语料库中出现频率很高的token的影响,因此这些token的经验信息量比在训练语料库中出现的一小部分特征要小。</target>
        </trans-unit>
        <trans-unit id="3325084d0d02139eb921a2545ba48cff39657720" translate="yes" xml:space="preserve">
          <source>The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.</source>
          <target state="translated">核k(X,X)相对于核的超参数的梯度.仅当eval_gradient为True时返回。</target>
        </trans-unit>
        <trans-unit id="1d28aeaa2593827c70ee0cbf35f8d5891fd13e08" translate="yes" xml:space="preserve">
          <source>The graph data is fetched from the DBpedia dumps. DBpedia is an extraction of the latent structured data of the Wikipedia content.</source>
          <target state="translated">图形数据是从DBpedia转储中获取的。DBpedia是对维基百科内容的潜在结构化数据的提取。</target>
        </trans-unit>
        <trans-unit id="94a8641249b9d38c741294dd700ba9c013e60d15" translate="yes" xml:space="preserve">
          <source>The graph should contain only one connect component, elsewhere the results make little sense.</source>
          <target state="translated">图形中应该只包含一个连接组件,其他地方的结果意义不大。</target>
        </trans-unit>
        <trans-unit id="0aca214d3abec7143ce0e3db3703c7759829a517" translate="yes" xml:space="preserve">
          <source>The graphical model of LDA is a three-level Bayesian model:</source>
          <target state="translated">LDA的图形模型是一个三层贝叶斯模型。</target>
        </trans-unit>
        <trans-unit id="b70b496cfa90301063390d7d501e40e03165e505" translate="yes" xml:space="preserve">
          <source>The graphical model of an RBM is a fully-connected bipartite graph.</source>
          <target state="translated">RBM的图形模型是一个完全连接的二元图形。</target>
        </trans-unit>
        <trans-unit id="43e8165a75805bcff11ce07ae2103365830550d8" translate="yes" xml:space="preserve">
          <source>The grid of &lt;code&gt;target_variables&lt;/code&gt; values for which the partial dependecy should be evaluated (either &lt;code&gt;grid&lt;/code&gt; or &lt;code&gt;X&lt;/code&gt; must be specified).</source>
          <target state="translated">应该评估部分依赖度的 &lt;code&gt;target_variables&lt;/code&gt; 值的网格（必须指定 &lt;code&gt;grid&lt;/code&gt; 或 &lt;code&gt;X&lt;/code&gt; ）。</target>
        </trans-unit>
        <trans-unit id="1fb55e8edd66947697c84e91d6ac3fa247bd29e9" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting</source>
          <target state="translated">用于拟合的字母网格</target>
        </trans-unit>
        <trans-unit id="c8ce42c94fb7c4644fc397d481823f721280ec22" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting, for each l1_ratio</source>
          <target state="translated">用于拟合的阿尔法网格,对于每个L1_ratio</target>
        </trans-unit>
        <trans-unit id="92edf0193fba09badba27bf7739525bce6da7601" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting, for each l1_ratio.</source>
          <target state="translated">用于拟合的阿尔法网格,对于每个l1_ratio。</target>
        </trans-unit>
        <trans-unit id="22daba72217df04ec1af1a8a340277c45da4b54e" translate="yes" xml:space="preserve">
          <source>The grid of alphas used for fitting.</source>
          <target state="translated">用于拟合的字母的网格。</target>
        </trans-unit>
        <trans-unit id="37c7f40c413d2aeb11b8645a838ea3c671b821b6" translate="yes" xml:space="preserve">
          <source>The grid points between 0 and 1: alpha/alpha_max</source>
          <target state="translated">0和1之间的网格点:alpha/alpha_max。</target>
        </trans-unit>
        <trans-unit id="1e90c7186240eacb6693334f5a2cb603522a3c95" translate="yes" xml:space="preserve">
          <source>The grid search instance behaves like a normal &lt;code&gt;scikit-learn&lt;/code&gt; model. Let&amp;rsquo;s perform the search on a smaller subset of the training data to speed up the computation:</source>
          <target state="translated">网格搜索实例的行为类似于普通的 &lt;code&gt;scikit-learn&lt;/code&gt; 模型。让我们对训练数据的较小子集执行搜索以加快计算速度：</target>
        </trans-unit>
        <trans-unit id="0fb72e7d32db09c24209afb30ebd0c3ef9469a12" translate="yes" xml:space="preserve">
          <source>The grid search provided by &lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; exhaustively generates candidates from a grid of parameter values specified with the &lt;code&gt;param_grid&lt;/code&gt; parameter. For instance, the following &lt;code&gt;param_grid&lt;/code&gt;:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV&quot;&gt; &lt;code&gt;GridSearchCV&lt;/code&gt; &lt;/a&gt;提供的网格搜索从使用 &lt;code&gt;param_grid&lt;/code&gt; 参数指定的参数值网格中详尽地生成候选对象。例如，以下 &lt;code&gt;param_grid&lt;/code&gt; ：</target>
        </trans-unit>
        <trans-unit id="5d877967ec11cd12214237a4561547e48123553d" translate="yes" xml:space="preserve">
          <source>The guidelines for choosing a metric is to use one that maximizes the distance between samples in different classes, and minimizes that within each class.</source>
          <target state="translated">选择度量的准则是使用一个能使不同类中样本之间的距离最大化,并使每个类内的距离最小化的度量。</target>
        </trans-unit>
        <trans-unit id="42b20fdbf19bf6fb28b9336fb1c931e7ebd9ff54" translate="yes" xml:space="preserve">
          <source>The handwritten digit dataset has 1797 total points. The model will be trained using all points, but only 30 will be labeled. Results in the form of a confusion matrix and a series of metrics over each class will be very good.</source>
          <target state="translated">手写数字数据集共有1797个点。该模型将使用所有的点进行训练,但只有30个点会被标记。以混淆矩阵和一系列指标的形式在每个类上的结果将是非常好的。</target>
        </trans-unit>
        <trans-unit id="1dbebc05eff701a160c5a2149017afcbb50998d6" translate="yes" xml:space="preserve">
          <source>The hash function employed is the signed 32-bit version of Murmurhash3.</source>
          <target state="translated">采用的哈希函数是Murmurhash3的签名32位版本。</target>
        </trans-unit>
        <trans-unit id="6ddf3e17ce9800f43a67078b41067ddc0d8c6a4c" translate="yes" xml:space="preserve">
          <source>The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the simplex.</source>
          <target state="translated">较高的浓度使更多的质量在中心,将导致更多的成分被激活,而较低的浓度参数将导致更多的质量在简单的边缘。</target>
        </trans-unit>
        <trans-unit id="1006ebebebbef4245b7568132e1eabf566a79140" translate="yes" xml:space="preserve">
          <source>The highest p-value for features to be kept.</source>
          <target state="translated">要保留的特征的最高p值。</target>
        </trans-unit>
        <trans-unit id="0c1d204d0071e8e2a8101579dd644bade4aef0d9" translate="yes" xml:space="preserve">
          <source>The highest uncorrected p-value for features to keep.</source>
          <target state="translated">保留特征的最高未经修正的p值。</target>
        </trans-unit>
        <trans-unit id="3708900f6a079bc16cd76c37da5647812d1a0427" translate="yes" xml:space="preserve">
          <source>The histogram of the estimated weights is very peaked, as a sparsity-inducing prior is implied on the weights.</source>
          <target state="translated">估计权重的直方图非常尖锐,因为在权重上隐含了一个稀疏性诱导先验。</target>
        </trans-unit>
        <trans-unit id="d24bdc7c6b1e4dbeb474c6e9f76042366464386e" translate="yes" xml:space="preserve">
          <source>The hyperparameters</source>
          <target state="translated">超参数</target>
        </trans-unit>
        <trans-unit id="25f29cead3cfba36338bcf026da5a846edf25636" translate="yes" xml:space="preserve">
          <source>The i-th score &lt;code&gt;train_score_[i]&lt;/code&gt; is the deviance (= loss) of the model at iteration &lt;code&gt;i&lt;/code&gt; on the in-bag sample. If &lt;code&gt;subsample == 1&lt;/code&gt; this is the deviance on the training data.</source>
          <target state="translated">第i个得分 &lt;code&gt;train_score_[i]&lt;/code&gt; 是袋内样本在第 &lt;code&gt;i&lt;/code&gt; 次迭代时模型的偏差（=损失）。如果 &lt;code&gt;subsample == 1&lt;/code&gt; 这是训练数据的偏差。</target>
        </trans-unit>
        <trans-unit id="7768180d6d5c29f9be6eb9d80f99cbf93007e25a" translate="yes" xml:space="preserve">
          <source>The i.i.d. assumption is broken if the underlying generative process yield groups of dependent samples.</source>
          <target state="translated">如果底层的生成过程产生了一组依赖性样本,则打破了i.i.d.假设。</target>
        </trans-unit>
        <trans-unit id="190c7529dfd22ae7ed70d4f84d1deb499abbe749" translate="yes" xml:space="preserve">
          <source>The idea behind the &lt;code&gt;VotingClassifier&lt;/code&gt; is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.</source>
          <target state="translated">&lt;code&gt;VotingClassifier&lt;/code&gt; 背后的想法是将概念上不同的机器学习分类器组合在一起，并使用多数表决或平均预测概率（软表决）来预测类标签。这样的分类器可用于一组性能良好的模型，以平衡其各自的弱点。</target>
        </trans-unit>
        <trans-unit id="f4f9efa7d25a183372e2e9ce75bfe20581578033" translate="yes" xml:space="preserve">
          <source>The image as a numpy array: height x width x color</source>
          <target state="translated">图像作为一个numpy数组:高度x宽度x颜色。</target>
        </trans-unit>
        <trans-unit id="77bae2f833c03e6f85b6a43b3aa3ead8f81ce2e5" translate="yes" xml:space="preserve">
          <source>The image is quantized to 256 grey levels and stored as unsigned 8-bit integers; the loader will convert these to floating point values on the interval [0, 1], which are easier to work with for many algorithms.</source>
          <target state="translated">图像被量化为256个灰度级别,并存储为无符号的8位整数;加载器将把这些值转换为区间[0,1]的浮点值,这对许多算法来说更容易处理。</target>
        </trans-unit>
        <trans-unit id="6db946ce103c288b47eadb4f1488a8fdc91a7488" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt;&lt;code&gt;Lasso&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients. See &lt;a href=&quot;#least-angle-regression&quot;&gt;Least Angle Regression&lt;/a&gt; for another implementation:</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.lasso#sklearn.linear_model.Lasso&quot;&gt; &lt;code&gt;Lasso&lt;/code&gt; &lt;/a&gt;类中的实现使用坐标下降作为算法来拟合系数。有关其他实现，请参见&lt;a href=&quot;#least-angle-regression&quot;&gt;最小角度回归&lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="64dabeaa3f8e38c30333615b1b174cbd1348b11e" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt;&lt;code&gt;MultiTaskElasticNet&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.multitaskelasticnet#sklearn.linear_model.MultiTaskElasticNet&quot;&gt; &lt;code&gt;MultiTaskElasticNet&lt;/code&gt; &lt;/a&gt;类中的实现使用坐标下降作为算法来拟合系数。</target>
        </trans-unit>
        <trans-unit id="555e24352725a2133c35e7a52f56a702cd7f81c4" translate="yes" xml:space="preserve">
          <source>The implementation in the class &lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt;&lt;code&gt;MultiTaskLasso&lt;/code&gt;&lt;/a&gt; uses coordinate descent as the algorithm to fit the coefficients.</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.multitasklasso#sklearn.linear_model.MultiTaskLasso&quot;&gt; &lt;code&gt;MultiTaskLasso&lt;/code&gt; &lt;/a&gt;类中的实现使用坐标下降作为算法来拟合系数。</target>
        </trans-unit>
        <trans-unit id="f9cb17c43bed4736104925d68075ebd6b07ace68" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 2.1 of &lt;a href=&quot;#rw2006&quot; id=&quot;id1&quot;&gt;[RW2006]&lt;/a&gt;. In addition to the API of standard scikit-learn estimators, GaussianProcessRegressor:</source>
          <target state="translated">该实现基于&lt;a href=&quot;#rw2006&quot; id=&quot;id1&quot;&gt;[RW2006]的&lt;/a&gt;算法2.1 。除了标准scikit-learn估算器的API外，GaussianProcessRegressor：</target>
        </trans-unit>
        <trans-unit id="70780cc3dac260d0fb4e85faed94f4134c9dc0ae" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 2.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.</source>
          <target state="translated">其实现是基于Rasmussen和Williams所著的机器学习高斯过程(GPML)的算法2.1。</target>
        </trans-unit>
        <trans-unit id="1904d3648b9818da40270920e1855bd9dcd752d1" translate="yes" xml:space="preserve">
          <source>The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.</source>
          <target state="translated">其实现基于Rasmussen和Williams所著的《机器学习的高斯过程》(GPML)的算法3.1、3.2和5.1。</target>
        </trans-unit>
        <trans-unit id="166f55526ba60cbc129406f3899569f7b0eb4efd" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm.</source>
          <target state="translated">它的实现是基于libsvm的。</target>
        </trans-unit>
        <trans-unit id="cb75b7ddb016087965f5fad7be27fc6c1cb16290" translate="yes" xml:space="preserve">
          <source>The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.</source>
          <target state="translated">实现是基于libsvm的。拟合时间的复杂度与样本数的关系大于二次方,这使得它很难扩展到10000个样本以上的数据集。</target>
        </trans-unit>
        <trans-unit id="08d84e0cf0c38a38100a9dad48e0efc744fd2db8" translate="yes" xml:space="preserve">
          <source>The implementation of &lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt;&lt;code&gt;TheilSenRegressor&lt;/code&gt;&lt;/a&gt; in scikit-learn follows a generalization to a multivariate linear regression model &lt;a href=&quot;#f1&quot; id=&quot;id30&quot;&gt;[8]&lt;/a&gt; using the spatial median which is a generalization of the median to multiple dimensions &lt;a href=&quot;#f2&quot; id=&quot;id31&quot;&gt;[9]&lt;/a&gt;.</source>
          <target state="translated">scikit-learn 中&lt;a href=&quot;generated/sklearn.linear_model.theilsenregressor#sklearn.linear_model.TheilSenRegressor&quot;&gt; &lt;code&gt;TheilSenRegressor&lt;/code&gt; &lt;/a&gt;的实现遵循使用空间中位数的多元线性回归模型&lt;a href=&quot;#f1&quot; id=&quot;id30&quot;&gt;[8]&lt;/a&gt;的推广，该模型是中位数到多个维度的推广&lt;a href=&quot;#f2&quot; id=&quot;id31&quot;&gt;[9]&lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="eb185b2e148e3613cdde933dd05a6b32e3b9acbe" translate="yes" xml:space="preserve">
          <source>The implementation of SGD is influenced by the &lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot;&gt;Stochastic Gradient SVM&lt;/a&gt; of L&amp;eacute;on Bottou. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse feature vectors, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from Shalev-Shwartz et al. 2007. For multi-class classification, a &amp;ldquo;one versus all&amp;rdquo; approach is used. We use the truncated gradient algorithm proposed by Tsuruoka et al. 2009 for L1 regularization (and the Elastic Net). The code is written in Cython.</source>
          <target state="translated">SGD的实施受&lt;a href=&quot;http://leon.bottou.org/projects/sgd&quot;&gt;随机梯度SVM影响&lt;/a&gt;莱昂&amp;middot;波托（L&amp;eacute;onBottou）类似于SvmSGD，权重向量表示为标量和向量的乘积，在L2正则化的情况下，该向量允许有效地更新权重。在稀疏特征向量的情况下，以较小的学习率（乘以0.01）更新截距，以说明其更新频率更高的事实。在每个观察到的例子之后，顺序地挑选训练例子，并降低学习率。我们采用了Shalev-Shwartz等人的学习率表。 2007年。对于多类别分类，使用了&amp;ldquo;一对一&amp;rdquo;的方法。我们使用Tsuruoka等人提出的截断梯度算法。 2009年用于L1正则化（和Elastic Net）。该代码用Cython编写。</target>
        </trans-unit>
        <trans-unit id="234c6abfe6fe107b5770fe372b7d9e14789fc069" translate="yes" xml:space="preserve">
          <source>The implementation of logistic regression in scikit-learn can be accessed from class &lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt;&lt;code&gt;LogisticRegression&lt;/code&gt;&lt;/a&gt;. This implementation can fit binary, One-vs- Rest, or multinomial logistic regression with optional L2 or L1 regularization.</source>
          <target state="translated">可以从&lt;a href=&quot;generated/sklearn.linear_model.logisticregression#sklearn.linear_model.LogisticRegression&quot;&gt; &lt;code&gt;LogisticRegression&lt;/code&gt; &lt;/a&gt;类访问scikit-learn中的logistic回归实现。此实现可以通过可选的L2或L1正则化来拟合二进制，一对一静止或多项逻辑回归。</target>
        </trans-unit>
        <trans-unit id="11a45f793137837fb140d6499ba2bfe32aacbae3" translate="yes" xml:space="preserve">
          <source>The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.</source>
          <target state="translated">一个特征的重要性是以该特征带来的标准的(归一化)总减少量来计算的。它也被称为吉尼重要性。</target>
        </trans-unit>
        <trans-unit id="285ba1aeef83b4f72aaed81188dabeb94680bb69" translate="yes" xml:space="preserve">
          <source>The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. &lt;code&gt;oob_improvement_[0]&lt;/code&gt; is the improvement in loss of the first stage over the &lt;code&gt;init&lt;/code&gt; estimator.</source>
          <target state="translated">相对于先前的迭代，袋装样本的损失（=偏差）的改善。 &lt;code&gt;oob_improvement_[0]&lt;/code&gt; 是 &lt;code&gt;init&lt;/code&gt; 估算器在第一阶段损失方面的改进。</target>
        </trans-unit>
        <trans-unit id="d60773790f2e7bcb156876ca46ea3f1191f69076" translate="yes" xml:space="preserve">
          <source>The impurity at \(m\) is computed using an impurity function \(H()\), the choice of which depends on the task being solved (classification or regression)</source>
          <target state="translated">使用不纯度函数计算不纯度(H()\),其选择取决于正在解决的任务(分类或回归)</target>
        </trans-unit>
        <trans-unit id="f89a5a9d5e4b0a0db4c9a50975418d448408f475" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature if axis == 0.</source>
          <target state="translated">如果轴==0,则每个特征的推算填充值。</target>
        </trans-unit>
        <trans-unit id="52ad716a60ba342ef84206f96283ef6240a59825" translate="yes" xml:space="preserve">
          <source>The imputation fill value for each feature.</source>
          <target state="translated">每个特征的推算填充值。</target>
        </trans-unit>
        <trans-unit id="7cd5e57b66f65afed5b5b066bdba074e0cee3d73" translate="yes" xml:space="preserve">
          <source>The imputation strategy.</source>
          <target state="translated">归因策略。</target>
        </trans-unit>
        <trans-unit id="aa1b27a8286b1b98805decd6657b8fa02347921e" translate="yes" xml:space="preserve">
          <source>The index (of the &lt;code&gt;cv_results_&lt;/code&gt; arrays) which corresponds to the best candidate parameter setting.</source>
          <target state="translated">与最佳候选参数设置相对应的（ &lt;code&gt;cv_results_&lt;/code&gt; 数组的）索引。</target>
        </trans-unit>
        <trans-unit id="d8f1885dbc976f2ceb3f7711b10b324e6546b97b" translate="yes" xml:space="preserve">
          <source>The index is computed only quantities and features inherent to the dataset.</source>
          <target state="translated">该指数只计算数据集固有的数量和特征。</target>
        </trans-unit>
        <trans-unit id="eaf4dd06369196819331db8b96a42731aed3d8e0" translate="yes" xml:space="preserve">
          <source>The index is defined as the average similarity between each cluster \(C_i\) for \(i=1, ..., k\) and its most similar one \(C_j\). In the context of this index, similarity is defined as a measure \(R_{ij}\) that trades off:</source>
          <target state="translated">该指数被定义为每个群组(C_i/)(i=1,...,k/)与其最相似的群组(C_j/)之间的平均相似度。在该指数中,相似度被定义为衡量衡量(R_{ij}/)的交易。</target>
        </trans-unit>
        <trans-unit id="f4af6c67193225ac6c1c304797e9ea3402785083" translate="yes" xml:space="preserve">
          <source>The index of the cluster.</source>
          <target state="translated">集群的指数。</target>
        </trans-unit>
        <trans-unit id="bf1ee02857437dc2bc3e741c8e3e56ef69d1ae10" translate="yes" xml:space="preserve">
          <source>The index value of a word in the vocabulary is linked to its frequency in the whole training corpus.</source>
          <target state="translated">词汇中一个词的索引值与其在整个训练语料中的频率挂钩。</target>
        </trans-unit>
        <trans-unit id="03c2241f87945c783a2d50da20a997d6e73ac147" translate="yes" xml:space="preserve">
          <source>The indexes of the sorted training input samples. If many tree are grown on the same dataset, this allows the ordering to be cached between trees. If None, the data will be sorted here. Don&amp;rsquo;t use this parameter unless you know what to do.</source>
          <target state="translated">排序后的训练输入样本的索引。如果在同一数据集上生长了许多树，则可以在树之间缓存顺序。如果为None，则将在此处对数据进行排序。除非您知道要做什么，否则不要使用此参数。</target>
        </trans-unit>
        <trans-unit id="0f615a1a241d635bbd7f8073a9962f88455d5840" translate="yes" xml:space="preserve">
          <source>The indicators for cluster membership of each column.</source>
          <target state="translated">每一栏的分组成员指标;</target>
        </trans-unit>
        <trans-unit id="7c7c8e5dd3219610397a571246f91a523e9b0296" translate="yes" xml:space="preserve">
          <source>The indicators for cluster membership of each row.</source>
          <target state="translated">每一行的分组成员指标。</target>
        </trans-unit>
        <trans-unit id="1d09056572af0ab7d260b8a9a458cace43041c54" translate="yes" xml:space="preserve">
          <source>The inertia matrix uses a Heapq-based representation.</source>
          <target state="translated">惯性矩阵采用基于Heapq的表示方法。</target>
        </trans-unit>
        <trans-unit id="4f68a65921a2da6924b081b35cf28ef22fa8adaa" translate="yes" xml:space="preserve">
          <source>The inferred value of max_features.</source>
          <target state="translated">max_features的推断值。</target>
        </trans-unit>
        <trans-unit id="9af964a5bdc6c7f6e3e23dbd4ccea9f871ad0d0c" translate="yes" xml:space="preserve">
          <source>The initial coefficients to warm-start the optimization.</source>
          <target state="translated">预热启动优化的初始系数。</target>
        </trans-unit>
        <trans-unit id="e326d26a40d08d9ebf56e15d175f98f61ace6983" translate="yes" xml:space="preserve">
          <source>The initial guess for the covariance.</source>
          <target state="translated">协方差的初步猜测。</target>
        </trans-unit>
        <trans-unit id="a31ad0b6b83b24ae472799017e15e5984848ac41" translate="yes" xml:space="preserve">
          <source>The initial guess of the noise variance for each feature. If None, it defaults to np.ones(n_features)</source>
          <target state="translated">每个特征的噪声方差的初始猜测。如果为None,则默认为np.ones(n_features)</target>
        </trans-unit>
        <trans-unit id="5af1f38d3d578b440a3ef0c3e0d6f491e678143f" translate="yes" xml:space="preserve">
          <source>The initial intercept to warm-start the optimization.</source>
          <target state="translated">初始拦截,以预热启动优化。</target>
        </trans-unit>
        <trans-unit id="ffb7aede167c9f36a232ef307d2df81471bb1b4f" translate="yes" xml:space="preserve">
          <source>The initial learning rate for the &amp;lsquo;constant&amp;rsquo;, &amp;lsquo;invscaling&amp;rsquo; or &amp;lsquo;adaptive&amp;rsquo; schedules. The default value is 0.0 as eta0 is not used by the default schedule &amp;lsquo;optimal&amp;rsquo;.</source>
          <target state="translated">&amp;ldquo;固定&amp;rdquo;，&amp;ldquo;增量&amp;rdquo;或&amp;ldquo;自适应&amp;rdquo;进度表的初始学习率。默认值为0.0，因为默认计划&amp;ldquo;最佳&amp;rdquo;未使用eta0。</target>
        </trans-unit>
        <trans-unit id="f396af141edab02acfc3d8c05ea1f6e174dd0d5b" translate="yes" xml:space="preserve">
          <source>The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=&amp;rsquo;sgd&amp;rsquo; or &amp;lsquo;adam&amp;rsquo;.</source>
          <target state="translated">使用的初始学习率。它在更新权重时控制步长。仅在Solver ='sgd'或'adam'时使用。</target>
        </trans-unit>
        <trans-unit id="8aa315cba5e51992abb63a7c33e92703aa91a364" translate="yes" xml:space="preserve">
          <source>The initial model \(F_{0}\) is problem specific, for least-squares regression one usually chooses the mean of the target values.</source>
          <target state="translated">初始模型/(F_{0}/)是针对问题的,对于最小二乘回归,通常选择目标值的平均值。</target>
        </trans-unit>
        <trans-unit id="fcbcdc2001232ebf2d1267dbd5c1db8c05bef33b" translate="yes" xml:space="preserve">
          <source>The initial model can also be specified via the &lt;code&gt;init&lt;/code&gt; argument. The passed object has to implement &lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt;.</source>
          <target state="translated">初始模型也可以通过 &lt;code&gt;init&lt;/code&gt; 参数指定。传递的对象必须实现 &lt;code&gt;fit&lt;/code&gt; 和 &lt;code&gt;predict&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="17bd189eec62d21587058ccd88b444461d73119b" translate="yes" xml:space="preserve">
          <source>The initial values of the coefficients.</source>
          <target state="translated">系数的初始值。</target>
        </trans-unit>
        <trans-unit id="32a92ee0ef2ffe7a34cba56e64f123bd9b2ca181" translate="yes" xml:space="preserve">
          <source>The input data consists of 28x28 pixel handwritten digits, leading to 784 features in the dataset. Therefore the first layer weight matrix have the shape (784, hidden_layer_sizes[0]). We can therefore visualize a single column of the weight matrix as a 28x28 pixel image.</source>
          <target state="translated">输入数据由28x28像素的手写数字组成,导致数据集中有784个特征,因此第一层权重矩阵的形状为(784,hidden_layer_sizes[0])。因此第一层权重矩阵的形状为(784,hidden_layer_sizes[0])。因此我们可以将权重矩阵的单列可视化为28x28像素的图像。</target>
        </trans-unit>
        <trans-unit id="c8ff70c87eb4edb0d6df0b02362dd0f826eb86c0" translate="yes" xml:space="preserve">
          <source>The input data matrix</source>
          <target state="translated">输入数据矩阵</target>
        </trans-unit>
        <trans-unit id="28402823f8037a445e34f883189d18c9c8586801" translate="yes" xml:space="preserve">
          <source>The input data to complete.</source>
          <target state="translated">要完成的输入数据。</target>
        </trans-unit>
        <trans-unit id="10ea3f8f2392cebb3ea568adacc2ca87c3d24f21" translate="yes" xml:space="preserve">
          <source>The input data to project into a smaller dimensional space.</source>
          <target state="translated">将输入的数据投射到一个较小维度的空间。</target>
        </trans-unit>
        <trans-unit id="8b941f334dfce6c774093743936f754bf8137c6b" translate="yes" xml:space="preserve">
          <source>The input data.</source>
          <target state="translated">输入的数据;</target>
        </trans-unit>
        <trans-unit id="a383a11075a72cc4c498a6da492b12429e0d8bd0" translate="yes" xml:space="preserve">
          <source>The input matrix \(A\) is first normalized to make the checkerboard pattern more obvious. There are three possible methods:</source>
          <target state="translated">首先对输入的矩阵/(A/)进行归一化处理,使棋盘模式更加明显。有三种可能的方法。</target>
        </trans-unit>
        <trans-unit id="5296b71b9c59f1ea88d1300f56d80b11acdd4263" translate="yes" xml:space="preserve">
          <source>The input matrix \(A\) is preprocessed as follows:</source>
          <target state="translated">输入矩阵/(A/)的预处理如下:</target>
        </trans-unit>
        <trans-unit id="1dfee05c320291671e1507b730cbc56e48fcc05b" translate="yes" xml:space="preserve">
          <source>The input samples with only the selected features.</source>
          <target state="translated">只有选定特征的输入样本。</target>
        </trans-unit>
        <trans-unit id="aa9c51d1052bb07178d99dc6b9998838f3fd04f4" translate="yes" xml:space="preserve">
          <source>The input samples.</source>
          <target state="translated">输入的样本。</target>
        </trans-unit>
        <trans-unit id="691f5d8dd518149d763a76be65224ad6a47a3de2" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, it will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt; and if a sparse matrix is provided to a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">输入样本。在内部，它将转换为 &lt;code&gt;dtype=np.float32&lt;/code&gt; ,并且如果将稀疏矩阵提供给稀疏 &lt;code&gt;csr_matrix&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="80a0f34908e0a755318c3eb528444068386165b2" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted into a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">输入样本。在内部，其dtype将转换为 &lt;code&gt;dtype=np.float32&lt;/code&gt; 。如果提供了一个稀疏矩阵，它将被转换为一个稀疏 &lt;code&gt;csr_matrix&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="ad241eeecc4b3d71885329d69a3ecbb931dd0903" translate="yes" xml:space="preserve">
          <source>The input samples. Internally, its dtype will be converted to &lt;code&gt;dtype=np.float32&lt;/code&gt;. If a sparse matrix is provided, it will be converted to a sparse &lt;code&gt;csr_matrix&lt;/code&gt;.</source>
          <target state="translated">输入样本。在内部，其dtype将转换为 &lt;code&gt;dtype=np.float32&lt;/code&gt; 。如果提供了稀疏矩阵，它将被转换为稀疏 &lt;code&gt;csr_matrix&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="b99e327cb34f5c84fcbd4e91cfb496e38c8c302f" translate="yes" xml:space="preserve">
          <source>The input samples. Use &lt;code&gt;dtype=np.float32&lt;/code&gt; for maximum efficiency. Sparse matrices are also supported, use sparse &lt;code&gt;csc_matrix&lt;/code&gt; for maximum efficiency.</source>
          <target state="translated">输入样本。使用 &lt;code&gt;dtype=np.float32&lt;/code&gt; 可获得最大效率。还支持稀疏矩阵，请使用稀疏 &lt;code&gt;csc_matrix&lt;/code&gt; 以获得最大效率。</target>
        </trans-unit>
        <trans-unit id="6e6a9dea87222450502841b10c214ed0343cd360" translate="yes" xml:space="preserve">
          <source>The input set can either be well conditioned (by default) or have a low rank-fat tail singular profile. See &lt;a href=&quot;sklearn.datasets.make_low_rank_matrix#sklearn.datasets.make_low_rank_matrix&quot;&gt;&lt;code&gt;make_low_rank_matrix&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">输入集可以是条件良好的（默认情况下），也可以是低阶脂肪尾部奇异轮廓。有关更多详细信息，请参见&lt;a href=&quot;sklearn.datasets.make_low_rank_matrix#sklearn.datasets.make_low_rank_matrix&quot;&gt; &lt;code&gt;make_low_rank_matrix&lt;/code&gt; &lt;/a&gt;。</target>
        </trans-unit>
        <trans-unit id="289f5749e232fcbed1540450cf860647da777cb8" translate="yes" xml:space="preserve">
          <source>The input set is well conditioned, centered and gaussian with unit variance.</source>
          <target state="translated">输入集具有良好的条件,居中和高斯的单位方差。</target>
        </trans-unit>
        <trans-unit id="9196dd161fc035e27746b1e485ce67d2bc4a2f97" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are converted to ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature.</source>
          <target state="translated">这个变换器的输入应该是一个整数或字符串的数组,表示分类(离散)特征的值。这些特征被转换为序数整数。这样每个特征就会有一列整数(0到n_categories-1)。</target>
        </trans-unit>
        <trans-unit id="6fca358f78b22e3193c17b331b39a22e0cf2c917" translate="yes" xml:space="preserve">
          <source>The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka &amp;lsquo;one-of-K&amp;rsquo; or &amp;lsquo;dummy&amp;rsquo;) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array.</source>
          <target state="translated">此转换器的输入应为整数或字符串之类的数组，表示分类（离散）特征所采用的值。使用单热（又名&amp;ldquo; one-of-K&amp;rdquo;或&amp;ldquo; dummy&amp;rdquo;）编码方案对特征进行编码。这将为每个类别创建一个二进制列，并返回一个稀疏矩阵或密集数组。</target>
        </trans-unit>
        <trans-unit id="d6846270bbc73ec820141552eb32b90be4589180" translate="yes" xml:space="preserve">
          <source>The integer id or the string name metadata of the MLComp dataset to load</source>
          <target state="translated">要加载的MLComp数据集的整数id或字符串名称元数据。</target>
        </trans-unit>
        <trans-unit id="0ff5072970d94a45dfd0b3dc59cc200193becdfd" translate="yes" xml:space="preserve">
          <source>The integer labels (0 or 1) for class membership of each sample.</source>
          <target state="translated">每个样本的类别成员的整数标签(0或1)。</target>
        </trans-unit>
        <trans-unit id="a6afcce2561a80c34c914f95ed66620399eb0d7c" translate="yes" xml:space="preserve">
          <source>The integer labels for class membership of each sample.</source>
          <target state="translated">每个样本的类别成员的整数标签。</target>
        </trans-unit>
        <trans-unit id="868af1be557e4830c6f25b4ff3211b7a9aad7eb0" translate="yes" xml:space="preserve">
          <source>The integer labels for cluster membership of each sample.</source>
          <target state="translated">每个样本的集群成员资格的整数标签。</target>
        </trans-unit>
        <trans-unit id="05d4e37f01ed47119c132248581df1196214fb7d" translate="yes" xml:space="preserve">
          <source>The integer labels for quantile membership of each sample.</source>
          <target state="translated">每个样本的整数标签。</target>
        </trans-unit>
        <trans-unit id="7da460eaa0239b5647e6b97a087bcede08e5a0c5" translate="yes" xml:space="preserve">
          <source>The intercept of the model. Only returned if &lt;code&gt;return_intercept&lt;/code&gt; is True and if X is a scipy sparse array.</source>
          <target state="translated">模型的截距。仅当 &lt;code&gt;return_intercept&lt;/code&gt; 为True且X为稀疏数组时才返回。</target>
        </trans-unit>
        <trans-unit id="f0408bacce1626a183a8eadd09dc2d6f8b9e549c" translate="yes" xml:space="preserve">
          <source>The intercept term.</source>
          <target state="translated">拦截项。</target>
        </trans-unit>
        <trans-unit id="39d99a92784fd6547680c69e2a029d63ab730943" translate="yes" xml:space="preserve">
          <source>The inverse document frequency (IDF) vector; only defined if &lt;code&gt;use_idf&lt;/code&gt; is True.</source>
          <target state="translated">逆文档频率（IDF）向量；仅在 &lt;code&gt;use_idf&lt;/code&gt; 为True时定义。</target>
        </trans-unit>
        <trans-unit id="70e42545b4f301fb133ffa67ef01dbd1c81a14a6" translate="yes" xml:space="preserve">
          <source>The inverse of the Box-Cox transformation is given by:</source>
          <target state="translated">Box-Cox变换的反式为:。</target>
        </trans-unit>
        <trans-unit id="1296b0fc0246edf109c97645c02261b1d3d028bb" translate="yes" xml:space="preserve">
          <source>The inverse of the Yeo-Johnson transformation is given by:</source>
          <target state="translated">Yeo-Johnson变换的反式为:。</target>
        </trans-unit>
        <trans-unit id="8621ba40f721d68ec6f12f67b5e3cc856e6fc6d9" translate="yes" xml:space="preserve">
          <source>The iris dataset is a classic and very easy multi-class classification dataset.</source>
          <target state="translated">虹膜数据集是一个经典且非常简单的多类分类数据集。</target>
        </trans-unit>
        <trans-unit id="82f3a3d98edcec3969a3e26fc0789c7c6c1f74ef" translate="yes" xml:space="preserve">
          <source>The iris dataset is a classification task consisting in identifying 3 different types of irises (Setosa, Versicolour, and Virginica) from their petal and sepal length and width:</source>
          <target state="translated">虹膜数据集是一个分类任务,包括从花瓣和萼片的长度和宽度识别3种不同类型的虹膜(Setosa,Versicolour和Virginica)。</target>
        </trans-unit>
        <trans-unit id="968fa226a66bed28f98c84df2ac306ac992e59c2" translate="yes" xml:space="preserve">
          <source>The isotonic regression optimization problem is defined by:</source>
          <target state="translated">等差回归优化问题的定义为:。</target>
        </trans-unit>
        <trans-unit id="710a9703f51d61fb5f41b8c77a926ca070febd6c" translate="yes" xml:space="preserve">
          <source>The iteration will stop when &lt;code&gt;max{|proj g_i | i = 1, ..., n}&lt;/code&gt; &amp;lt;= &lt;code&gt;tol&lt;/code&gt; where pg_i is the i-th component of the projected gradient.</source>
          <target state="translated">当 &lt;code&gt;max{|proj g_i | i = 1, ..., n}&lt;/code&gt; &amp;lt;= &lt;code&gt;tol&lt;/code&gt; 其中pg_i是投影梯度的第i个分量。</target>
        </trans-unit>
        <trans-unit id="d1f978f02193a9d7a16e2ac4f28051f1be20fa33" translate="yes" xml:space="preserve">
          <source>The iterator consumption and dispatching is protected by the same lock so calling this function should be thread safe.</source>
          <target state="translated">迭代器的消耗和派发是由同一个锁保护的,所以调用这个函数应该是线程安全的。</target>
        </trans-unit>
        <trans-unit id="e6bcf49b626a9dedb0663face3bd765b3dedb378" translate="yes" xml:space="preserve">
          <source>The ith element in the list represents the bias vector corresponding to layer i + 1.</source>
          <target state="translated">列表中的第i个元素代表i+1层对应的偏置向量。</target>
        </trans-unit>
        <trans-unit id="4b396b269a89e1a7151872f147950b58bd31d2c2" translate="yes" xml:space="preserve">
          <source>The ith element in the list represents the weight matrix corresponding to layer i.</source>
          <target state="translated">列表中的第i个元素代表第i层对应的权重矩阵。</target>
        </trans-unit>
        <trans-unit id="8ce9197f7d2eba2389cfe44c6d9806383e84232a" translate="yes" xml:space="preserve">
          <source>The ith element represents the number of neurons in the ith hidden layer.</source>
          <target state="translated">第i个元素代表第i个隐藏层的神经元数量。</target>
        </trans-unit>
        <trans-unit id="110dde91ce5735296cfcdbdf4849eb2634f008ea" translate="yes" xml:space="preserve">
          <source>The justification for the formula in the loss=&amp;rdquo;modified_huber&amp;rdquo; case is in the appendix B in: &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&quot;&gt;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&lt;/a&gt;</source>
          <target state="translated">在loss =&amp;ldquo; modified_huber&amp;rdquo;情况下公式的理由在附录B中的位置：&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf&quot;&gt;http&lt;/a&gt; : //jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf</target>
        </trans-unit>
        <trans-unit id="2ad3a7b0acdb773b3a5a78e7f86e7401f887f5a4" translate="yes" xml:space="preserve">
          <source>The k-means algorithm divides a set of \(N\) samples \(X\) into \(K\) disjoint clusters \(C\), each described by the mean \(\mu_j\) of the samples in the cluster. The means are commonly called the cluster &amp;ldquo;centroids&amp;rdquo;; note that they are not, in general, points from \(X\), although they live in the same space. The K-means algorithm aims to choose centroids that minimise the &lt;em&gt;inertia&lt;/em&gt;, or within-cluster sum of squared criterion:</source>
          <target state="translated">k-均值算法将一组\（N \）个样本\（X \）划分为\（K \）个不相交的聚类\（C \），每个聚类由簇。该方法通常称为簇&amp;ldquo;质心&amp;rdquo;。请注意，尽管它们位于相同的空间中，但它们通常不是\（X \）的点。K-means算法旨在选择最小化&lt;em&gt;惯性&lt;/em&gt;或质心内平方和的质心：</target>
        </trans-unit>
        <trans-unit id="3042a74be3a11ce6ced2d21d393c8c55a0b044ff" translate="yes" xml:space="preserve">
          <source>The k-means problem is solved using either Lloyd&amp;rsquo;s or Elkan&amp;rsquo;s algorithm.</source>
          <target state="translated">使用Lloyd或Elkan算法可以解决k均值问题。</target>
        </trans-unit>
        <trans-unit id="698fe2144ac8be0b801483c0a1d13dc4d07c73ce" translate="yes" xml:space="preserve">
          <source>The kappa score (see docstring) is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).</source>
          <target state="translated">kappa得分(见docstring)是一个介于-1和1之间的数字。得分高于0.8的一般被认为是好的协议;0或更低的意味着没有协议(实际上是随机标签)。</target>
        </trans-unit>
        <trans-unit id="951d69c60ea24190bd0c5a4dcae9aeb94c1c913d" translate="yes" xml:space="preserve">
          <source>The kappa statistic, which is a number between -1 and 1. The maximum value means complete agreement; zero or lower means chance agreement.</source>
          <target state="translated">kappa统计量,是一个介于-1和1之间的数字,最大值表示完全一致,零或更低表示偶然一致。</target>
        </trans-unit>
        <trans-unit id="e7d87035112e23c601a476b538c50df786049966" translate="yes" xml:space="preserve">
          <source>The kernel density estimator can be used with any of the valid distance metrics (see &lt;a href=&quot;generated/sklearn.neighbors.distancemetric#sklearn.neighbors.DistanceMetric&quot;&gt;&lt;code&gt;sklearn.neighbors.DistanceMetric&lt;/code&gt;&lt;/a&gt; for a list of available metrics), though the results are properly normalized only for the Euclidean metric. One particularly useful metric is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;Haversine distance&lt;/a&gt; which measures the angular distance between points on a sphere. Here is an example of using a kernel density estimate for a visualization of geospatial data, in this case the distribution of observations of two different species on the South American continent:</source>
          <target state="translated">内核密度估计器可以与任何有效的距离度量标准一起使用（有关可用度量标准的列表，请参阅&lt;a href=&quot;generated/sklearn.neighbors.distancemetric#sklearn.neighbors.DistanceMetric&quot;&gt; &lt;code&gt;sklearn.neighbors.DistanceMetric&lt;/code&gt; &lt;/a&gt;），尽管仅对欧几里德度量标准对结果进行了适当的标准化。一种特别有用的度量标准是&lt;a href=&quot;https://en.wikipedia.org/wiki/Haversine_formula&quot;&gt;Haversine距离&lt;/a&gt;，它测量球体上各点之间的角距离。这是一个使用核密度估计值可视化地理空间数据的示例，在这种情况下，是南美大陆上两种不同物种的观测值分布：</target>
        </trans-unit>
        <trans-unit id="f5acb5ee70e93822d6214e7faa0671c1edbb5e04" translate="yes" xml:space="preserve">
          <source>The kernel is composed of several terms that are responsible for explaining different properties of the signal:</source>
          <target state="translated">内核由几个术语组成,负责解释信号的不同特性。</target>
        </trans-unit>
        <trans-unit id="b2a264496619ccec6a15428322dff95c6847afef" translate="yes" xml:space="preserve">
          <source>The kernel specifying the covariance function of the GP. If None is passed, the kernel &amp;ldquo;1.0 * RBF(1.0)&amp;rdquo; is used as default. Note that the kernel&amp;rsquo;s hyperparameters are optimized during fitting.</source>
          <target state="translated">指定GP协方差函数的内核。如果未通过，则默认使用内核&amp;ldquo; 1.0 * RBF（1.0）&amp;rdquo;。请注意，内核的超参数在拟合过程中已优化。</target>
        </trans-unit>
        <trans-unit id="3f4b36434e95db38ce416f2948e370303e8d7ce4" translate="yes" xml:space="preserve">
          <source>The kernel to use. Valid kernels are [&amp;lsquo;gaussian&amp;rsquo;|&amp;rsquo;tophat&amp;rsquo;|&amp;rsquo;epanechnikov&amp;rsquo;|&amp;rsquo;exponential&amp;rsquo;|&amp;rsquo;linear&amp;rsquo;|&amp;rsquo;cosine&amp;rsquo;] Default is &amp;lsquo;gaussian&amp;rsquo;.</source>
          <target state="translated">要使用的内核。有效内核为['gaussian'|'tophat'|'epanechnikov'|'exponential'|'linear'|'cosine']默认值为'gaussian'。</target>
        </trans-unit>
        <trans-unit id="7d7a3328bfc6be5d4c52eb80a02db6810f6af97a" translate="yes" xml:space="preserve">
          <source>The kernel used for prediction. In case of binary classification, the structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters. In case of multi-class classification, a CompoundKernel is returned which consists of the different kernels used in the one-versus-rest classifiers.</source>
          <target state="translated">用于预测的内核。在二元分类的情况下,内核的结构与作为参数传递的内核相同,但使用了优化的超参数。在多类分类的情况下,返回一个CompoundKernel,它由单类与休整分类器中使用的不同内核组成。</target>
        </trans-unit>
        <trans-unit id="5a1810e4f4370ea9d4b3eb4b69c5661264f9680f" translate="yes" xml:space="preserve">
          <source>The kernel used for prediction. The structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters</source>
          <target state="translated">用于预测的内核。内核的结构与作为参数传入的内核相同,但优化了超参数。</target>
        </trans-unit>
        <trans-unit id="317ceb705f764af792a5c1b02b0c75da4f7767d2" translate="yes" xml:space="preserve">
          <source>The key &lt;code&gt;'params'&lt;/code&gt; is used to store a list of parameter settings dicts for all the parameter candidates.</source>
          <target state="translated">键 &lt;code&gt;'params'&lt;/code&gt; 用于存储所有候选参数的参数设置字典列表。</target>
        </trans-unit>
        <trans-unit id="467dfd2cbee563f1dc16d74e0763e99176db4d68" translate="yes" xml:space="preserve">
          <source>The l1-penalized estimator can recover part of this off-diagonal structure. It learns a sparse precision. It is not able to recover the exact sparsity pattern: it detects too many non-zero coefficients. However, the highest non-zero coefficients of the l1 estimated correspond to the non-zero coefficients in the ground truth. Finally, the coefficients of the l1 precision estimate are biased toward zero: because of the penalty, they are all smaller than the corresponding ground truth value, as can be seen on the figure.</source>
          <target state="translated">l1-penalized估计器可以恢复这种对角线外结构的一部分。它学习的是一种稀疏精度。它不能恢复精确的稀疏模式:它检测到了太多的非零系数。然而,估计的l1个最高的非零系数对应于地真中的非零系数。最后,l1精度估计的系数偏向于零:由于惩罚,它们都小于对应的地真值,这在图上可以看到。</target>
        </trans-unit>
        <trans-unit id="3f115dd7f6ab226f3d1efb2261d982c44eb021f0" translate="yes" xml:space="preserve">
          <source>The label of the positive class</source>
          <target state="translated">正类的标签</target>
        </trans-unit>
        <trans-unit id="1daeeb6b0700e2caf583964dcec09c381eeb8ea9" translate="yes" xml:space="preserve">
          <source>The label of the positive class. Only applied to binary &lt;code&gt;y_true&lt;/code&gt;. For multilabel-indicator &lt;code&gt;y_true&lt;/code&gt;, &lt;code&gt;pos_label&lt;/code&gt; is fixed to 1.</source>
          <target state="translated">正类的标签。仅适用于二进制 &lt;code&gt;y_true&lt;/code&gt; 。对于multilabel-indicator &lt;code&gt;y_true&lt;/code&gt; ， &lt;code&gt;pos_label&lt;/code&gt; 固定为1。</target>
        </trans-unit>
        <trans-unit id="844ad03ed5a5991044f5c2a9015a7d46ffc83176" translate="yes" xml:space="preserve">
          <source>The label sets.</source>
          <target state="translated">标签集。</target>
        </trans-unit>
        <trans-unit id="34e6595f06fcaf1ac9d996e42fb91cf1dc35ee40" translate="yes" xml:space="preserve">
          <source>The labels of the clusters.</source>
          <target state="translated">各组的标签;</target>
        </trans-unit>
        <trans-unit id="f920ebdbb736d4fac6e624f1f0a24fae7f686c27" translate="yes" xml:space="preserve">
          <source>The laplacian kernel is defined as:</source>
          <target state="translated">拉普拉斯核的定义为:</target>
        </trans-unit>
        <trans-unit id="321fd6d2804d3c8ea8df389dda895240e812e24c" translate="yes" xml:space="preserve">
          <source>The lasso estimate thus solves the minimization of the least-squares penalty with \(\alpha ||w||_1\) added, where \(\alpha\) is a constant and \(||w||_1\) is the \(\ell_1\)-norm of the parameter vector.</source>
          <target state="translated">因此,拉索估计解决了最小二乘惩罚的最小化问题,并加入了 \(\alpha |||w|_1/\),其中 \(\alpha\)是一个常数,\(||w|_1/\)是参数向量的 \(\ell_1/\)-norm。</target>
        </trans-unit>
        <trans-unit id="374010cdf4f6f26c62c956d36c36442b1f12b646" translate="yes" xml:space="preserve">
          <source>The last characteristic implies that the Perceptron is slightly faster to train than SGD with the hinge loss and that the resulting models are sparser.</source>
          <target state="translated">最后一个特征意味着Perceptron的训练速度要比SGD的铰链损失略快,而且得到的模型更稀疏。</target>
        </trans-unit>
        <trans-unit id="6312a138a2adc5ee223c24bc72b6c9e5099fd7ea" translate="yes" xml:space="preserve">
          <source>The last dataset is an example of a &amp;lsquo;null&amp;rsquo; situation for clustering: the data is homogeneous, and there is no good clustering. For this example, the null dataset uses the same parameters as the dataset in the row above it, which represents a mismatch in the parameter values and the data structure.</source>
          <target state="translated">最后一个数据集是聚类为&amp;ldquo;空&amp;rdquo;情况的一个示例：数据是同质的，并且没有良好的聚类。对于此示例，空数据集使用与其上方一行中的数据集相同的参数，这表示参数值和数据结构不匹配。</target>
        </trans-unit>
        <trans-unit id="2cc9bc385685f028c8287b17d0d958b5050b56a0" translate="yes" xml:space="preserve">
          <source>The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold. This ensures that the graph starts on the y axis.</source>
          <target state="translated">最后的精度和召回值分别为1.和0.,没有相应的阈值。这样可以保证图形从y轴开始。</target>
        </trans-unit>
        <trans-unit id="9ddff80583e9f2c8a48f3b64e4a9a06a8a8cefcd" translate="yes" xml:space="preserve">
          <source>The last two panels show how we can sample from the last two models. The resulting samples distributions do not look exactly like the original data distribution. The difference primarily stems from the approximation error we made by using a model that assumes that the data was generated by a finite number of Gaussian components instead of a continuous noisy sine curve.</source>
          <target state="translated">最后两个面板显示了我们如何从最后两个模型中进行采样。由此产生的样本分布看起来并不完全像原始数据分布。这种差异主要源于我们使用的模型所产生的近似误差,该模型假设数据是由有限数量的高斯分量产生的,而不是连续的噪声正弦曲线。</target>
        </trans-unit>
        <trans-unit id="a4514c8e853c44e1e4a78bf33662f9b32277ba44" translate="yes" xml:space="preserve">
          <source>The latent variables of X.</source>
          <target state="translated">X的潜变量。</target>
        </trans-unit>
        <trans-unit id="03a214378b3ef2e13a3cc6617d05b3fe221146c8" translate="yes" xml:space="preserve">
          <source>The learning rate \(\eta\) can be either constant or gradually decaying. For classification, the default learning rate schedule (&lt;code&gt;learning_rate='optimal'&lt;/code&gt;) is given by</source>
          <target state="translated">学习速率\（\ eta \）可以是恒定的，也可以是逐渐衰减的。对于分类，默认学习率时间表（ &lt;code&gt;learning_rate='optimal'&lt;/code&gt; ）由下式给出</target>
        </trans-unit>
        <trans-unit id="36c5b5659a911c2db600cd9fbf9c0485b5053e95" translate="yes" xml:space="preserve">
          <source>The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a &amp;lsquo;ball&amp;rsquo; with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help.</source>
          <target state="translated">t-SNE的学习率通常在[10.0，1000.0]范围内。如果学习率太高，则数据可能看起来像一个&amp;ldquo;球&amp;rdquo;，其任何一点都与其最近的邻居等距。如果学习率太低，大多数点可能看起来像压缩在密集的云中，没有异常值。如果成本函数陷入不良的局部最小值中，则提高学习率可能会有所帮助。</target>
        </trans-unit>
        <trans-unit id="55a4138aa9f2827acbf6f24a7d7d78c5c9231271" translate="yes" xml:space="preserve">
          <source>The learning rate for weight updates. It is &lt;em&gt;highly&lt;/em&gt; recommended to tune this hyper-parameter. Reasonable values are in the 10**[0., -3.] range.</source>
          <target state="translated">体重更新的学习率。这是&lt;em&gt;强烈&lt;/em&gt;建议调整该超参数。合理的值在10 ** [0。，-3。]范围内。</target>
        </trans-unit>
        <trans-unit id="aa4707838d034cc31029160760df726f17933ef5" translate="yes" xml:space="preserve">
          <source>The learning rate schedule:</source>
          <target state="translated">学习率表。</target>
        </trans-unit>
        <trans-unit id="866660dd294846977c41d9eb05a5f823762d12dd" translate="yes" xml:space="preserve">
          <source>The left and right examples highlight the &lt;code&gt;n_labels&lt;/code&gt; parameter: more of the samples in the right plot have 2 or 3 labels.</source>
          <target state="translated">左右示例突出显示 &lt;code&gt;n_labels&lt;/code&gt; 参数：右图中的更多样本具有2或3个标签。</target>
        </trans-unit>
        <trans-unit id="3656a1b3513126216409c003f2c19b4a1bde366e" translate="yes" xml:space="preserve">
          <source>The leftmost layer, known as the input layer, consists of a set of neurons \(\{x_i | x_1, x_2, ..., x_m\}\) representing the input features. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation \(w_1x_1 + w_2x_2 + ... + w_mx_m\), followed by a non-linear activation function \(g(\cdot):R \rightarrow R\) - like the hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values.</source>
          <target state="translated">最左边的一层,称为输入层,由一组神经元 \(\{x_i | x_1,x_2,...,x_m}\)组成,代表输入特征。隐藏层中的每个神经元将上一层的值用加权线性和(w_1x_1+w_2x_2+...+w_mx_m/m)进行变换,然后用非线性激活函数(g(\cdot):R \rightarrow R\)-如双曲探函数。输出层接收最后一个隐藏层的值,并将其转化为输出值。</target>
        </trans-unit>
        <trans-unit id="7b2e6a226728025cdbf60737ae02746b4f5067e4" translate="yes" xml:space="preserve">
          <source>The length scale of the kernel.</source>
          <target state="translated">内核的长度尺度。</target>
        </trans-unit>
        <trans-unit id="658641ffb44e40c4155905b8a662123f9fbe36a4" translate="yes" xml:space="preserve">
          <source>The length scale of the kernel. If a float, an isotropic kernel is used. If an array, an anisotropic kernel is used where each dimension of l defines the length-scale of the respective feature dimension.</source>
          <target state="translated">内核的长度范围。如果是浮点数,则使用各向同性核。如果是数组,则使用各向异性核,其中每个维度定义各自特征维度的长度范围。</target>
        </trans-unit>
        <trans-unit id="c8dac18109c7577d20154429eb0e5e79a703a997" translate="yes" xml:space="preserve">
          <source>The likelihood of the data set with &lt;code&gt;self.covariance_&lt;/code&gt; as an estimator of its covariance matrix.</source>
          <target state="translated">使用 &lt;code&gt;self.covariance_&lt;/code&gt; 作为其协方差矩阵的估计量的数据集的可能性。</target>
        </trans-unit>
        <trans-unit id="9d0f580fc795d85a96fff8300e9ac1a9bc3bbb0b" translate="yes" xml:space="preserve">
          <source>The linear model trained on polynomial features is able to exactly recover the input polynomial coefficients.</source>
          <target state="translated">在多项式特征上训练的线性模型能够精确地恢复输入的多项式系数。</target>
        </trans-unit>
        <trans-unit id="9cf1fb22576b6c52a81ed3fb3cd57a1b7c534cc0" translate="yes" xml:space="preserve">
          <source>The linear models &lt;code&gt;LinearSVC()&lt;/code&gt; and &lt;code&gt;SVC(kernel='linear')&lt;/code&gt; yield slightly different decision boundaries. This can be a consequence of the following differences:</source>
          <target state="translated">线性模型 &lt;code&gt;LinearSVC()&lt;/code&gt; 和 &lt;code&gt;SVC(kernel='linear')&lt;/code&gt; 得出的决策边界略有不同。这可能是由于以下差异造成的：</target>
        </trans-unit>
        <trans-unit id="02b9c4dcf92e3aac8d7274c2bb8e453c5501153c" translate="yes" xml:space="preserve">
          <source>The list of calibrated classifiers, one for each crossvalidation fold, which has been fitted on all but the validation fold and calibrated on the validation fold.</source>
          <target state="translated">已校准的分类器列表,每个交叉验证折线都有一个,它已在除验证折线外的所有折线上拟合,并在验证折线上进行了校准。</target>
        </trans-unit>
        <trans-unit id="662dd2cdb21b969ce9a44b42150a945acbfed523" translate="yes" xml:space="preserve">
          <source>The list of values of the objective function and the dual gap at each iteration. Returned only if return_costs is True.</source>
          <target state="translated">目标函数的值和每次迭代时的双倍差距的列表。仅当return_costs为True时返回。</target>
        </trans-unit>
        <trans-unit id="177835998b74cd6b15a39a13b3bf0448af9a1ccf" translate="yes" xml:space="preserve">
          <source>The local outlier factor (LOF) of a sample captures its supposed &amp;lsquo;degree of abnormality&amp;rsquo;. It is the average of the ratio of the local reachability density of a sample and those of its k-nearest neighbors.</source>
          <target state="translated">样本的局部离群因子（LOF）捕获了其所谓的&amp;ldquo;异常程度&amp;rdquo;。它是样本及其k近邻的局部可达性密度之比的平均值。</target>
        </trans-unit>
        <trans-unit id="f5a19f67242aeecdaccb15b2c01bb7b8a32da9cd" translate="yes" xml:space="preserve">
          <source>The log likelihood at each iteration.</source>
          <target state="translated">每次迭代时的对数似然。</target>
        </trans-unit>
        <trans-unit id="d6423c4071c8619e84a84f9bcba64b070be431d3" translate="yes" xml:space="preserve">
          <source>The log-marginal-likelihood of &lt;code&gt;self.kernel_.theta&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;self.kernel_.theta&lt;/code&gt; 的对数边际可能性</target>
        </trans-unit>
        <trans-unit id="b9f3852ad5acaf7d90c57ecdaa290bd2f37a1610" translate="yes" xml:space="preserve">
          <source>The log-transformed bounds on the kernel&amp;rsquo;s hyperparameters theta</source>
          <target state="translated">内核超参数theta的对数转换范围</target>
        </trans-unit>
        <trans-unit id="997c53b0e19e31f9cbe762633463d7411a04ec12" translate="yes" xml:space="preserve">
          <source>The logarithm used is the natural logarithm (base-e).</source>
          <target state="translated">使用的对数是自然对数(基数-e)。</target>
        </trans-unit>
        <trans-unit id="b4b4750e8021650b4da5de07fd1cb495068051c3" translate="yes" xml:space="preserve">
          <source>The logistic regression with One-Vs-Rest is not a multiclass classifier out of the box. As a result it has more trouble in separating class 2 and 3 than the other estimators.</source>
          <target state="translated">One-Vs-Rest的逻辑回归不是一个开箱即用的多类分类器。因此,它在分离第2类和第3类时比其他估计器更麻烦。</target>
        </trans-unit>
        <trans-unit id="658bb63624739c29e98c4d11ac78e5bf2f1fae2c" translate="yes" xml:space="preserve">
          <source>The loss function that &lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt;&lt;code&gt;HuberRegressor&lt;/code&gt;&lt;/a&gt; minimizes is given by</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.linear_model.huberregressor#sklearn.linear_model.HuberRegressor&quot;&gt; &lt;code&gt;HuberRegressor&lt;/code&gt; &lt;/a&gt;最小化的损失函数由下式给出</target>
        </trans-unit>
        <trans-unit id="47dc1dfa051244f9df37eb81c616d6773441e3ed" translate="yes" xml:space="preserve">
          <source>The loss function to be used. Defaults to &amp;lsquo;hinge&amp;rsquo;, which gives a linear SVM.</source>
          <target state="translated">要使用的损失函数。默认为'hinge'，这将提供线性SVM。</target>
        </trans-unit>
        <trans-unit id="ced89f4970549dd56e97e03e89e06fddefc0d81d" translate="yes" xml:space="preserve">
          <source>The loss function to be used. The possible values are &amp;lsquo;squared_loss&amp;rsquo;, &amp;lsquo;huber&amp;rsquo;, &amp;lsquo;epsilon_insensitive&amp;rsquo;, or &amp;lsquo;squared_epsilon_insensitive&amp;rsquo;</source>
          <target state="translated">要使用的损失函数。可能的值为&amp;ldquo; squared_loss&amp;rdquo;，&amp;ldquo; huber&amp;rdquo;，&amp;ldquo; epsilon_insensitive&amp;rdquo;或&amp;ldquo; squared_epsilon_insensitive&amp;rdquo;</target>
        </trans-unit>
        <trans-unit id="dcd8a12c459702396e7eec22715d06aed2aee153" translate="yes" xml:space="preserve">
          <source>The loss function to be used: epsilon_insensitive: equivalent to PA-I in the reference paper. squared_epsilon_insensitive: equivalent to PA-II in the reference paper.</source>
          <target state="translated">要使用的损耗函数:epsilon_insensitive:相当于参考文献中的PA-I.squared_epsilon_insensitive:相当于参考文献中的PA-II。</target>
        </trans-unit>
        <trans-unit id="4c1d6cfb55920bc5e055d28ac3a6b6a90335ca56" translate="yes" xml:space="preserve">
          <source>The loss function to be used: hinge: equivalent to PA-I in the reference paper. squared_hinge: equivalent to PA-II in the reference paper.</source>
          <target state="translated">要使用的损失函数:铰链:相当于参考文件中的PA-I.平方_铰链:相当于参考文件中的PA-II。</target>
        </trans-unit>
        <trans-unit id="efb9498c0013e0aa10110f406847617e3f7359e7" translate="yes" xml:space="preserve">
          <source>The loss function to use when updating the weights after each boosting iteration.</source>
          <target state="translated">每次提升迭代后更新权重时使用的损失函数。</target>
        </trans-unit>
        <trans-unit id="76e41cb6fdfbaf660e2380953e681777b0e6378c" translate="yes" xml:space="preserve">
          <source>The loss function used is binomial deviance. Regularization via shrinkage (&lt;code&gt;learning_rate &amp;lt; 1.0&lt;/code&gt;) improves performance considerably. In combination with shrinkage, stochastic gradient boosting (&lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt;) can produce more accurate models by reducing the variance via bagging. Subsampling without shrinkage usually does poorly. Another strategy to reduce the variance is by subsampling the features analogous to the random splits in Random Forests (via the &lt;code&gt;max_features&lt;/code&gt; parameter).</source>
          <target state="translated">使用的损失函数是二项式偏差。通过收缩（ &lt;code&gt;learning_rate &amp;lt; 1.0&lt;/code&gt; ）进行正则化可以显着提高性能。结合收缩率，随机梯度增强（ &lt;code&gt;subsample &amp;lt; 1.0&lt;/code&gt; ）可以通过减少装袋的方差来生成更准确的模型。没有收缩的二次采样通常效果不佳。减少方差的另一种策略是通过对与随机森林中的随机分割相似的特征进行二次采样（通过 &lt;code&gt;max_features&lt;/code&gt; 参数）。</target>
        </trans-unit>
        <trans-unit id="b776e738f1fc829ad5cd97ce27a8c60ba5e6ea09" translate="yes" xml:space="preserve">
          <source>The low rank part of the profile can be considered the structured signal part of the data while the tail can be considered the noisy part of the data that cannot be summarized by a low number of linear components (singular vectors).</source>
          <target state="translated">剖面的低阶部分可以认为是数据的结构化信号部分,而尾部可以认为是数据的噪声部分,不能用低数量的线性分量(奇异向量)来概括。</target>
        </trans-unit>
        <trans-unit id="eab337b7facddd1f9b82eece11282a55e0039c48" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on alpha</source>
          <target state="translated">α的下限和上限</target>
        </trans-unit>
        <trans-unit id="59acfd562a002dfced1122413eab7f832a55dd1c" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on constant_value</source>
          <target state="translated">constant_value的下界和上界</target>
        </trans-unit>
        <trans-unit id="3a719a94ceaa10893529e6c7cb2929f1cad3fed1" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on gamma</source>
          <target state="translated">伽玛值的下限和上限</target>
        </trans-unit>
        <trans-unit id="1ed15fdef2e92d07e56ad7fd40f1816e9ce5774a" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on l</source>
          <target state="translated">l的下界和上界</target>
        </trans-unit>
        <trans-unit id="5a4d3b39beb7cd8132b2abe44c4adfccffd03b85" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on length_scale</source>
          <target state="translated">length_scale的下界和上界。</target>
        </trans-unit>
        <trans-unit id="23389af6ff1a664526029d031ba30d8bce5de030" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on noise_level</source>
          <target state="translated">噪音水平的下限和上限</target>
        </trans-unit>
        <trans-unit id="5d396b2f8516f35fa726d028e889486242bc7cef" translate="yes" xml:space="preserve">
          <source>The lower and upper bound on periodicity</source>
          <target state="translated">周期性的下限和上限</target>
        </trans-unit>
        <trans-unit id="67f0743ba9ff76a5a36032511baa5e62618c04ec" translate="yes" xml:space="preserve">
          <source>The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n &amp;lt;= n &amp;lt;= max_n will be used.</source>
          <target state="translated">要提取的不同n-gram的n值范围的上下边界。将使用所有min_n &amp;lt;= n &amp;lt;= max_n的n值。</target>
        </trans-unit>
        <trans-unit id="1cf3abbfa7f1a3041db626cf750d4ae3ee4a5f71" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used create the extreme values for the &lt;code&gt;grid&lt;/code&gt;. Only if &lt;code&gt;X&lt;/code&gt; is not None.</source>
          <target state="translated">所使用的上下百分位为 &lt;code&gt;grid&lt;/code&gt; 创建极值。仅当 &lt;code&gt;X&lt;/code&gt; 不为None时。</target>
        </trans-unit>
        <trans-unit id="ff88012ee3d2491153687a1e07b6eefe04629c89" translate="yes" xml:space="preserve">
          <source>The lower and upper percentile used to create the extreme values for the PDP axes.</source>
          <target state="translated">用于创建PDP轴极值的下百分位数和上百分位数。</target>
        </trans-unit>
        <trans-unit id="ca509231087e3737e2d90c6b71a37c419327789b" translate="yes" xml:space="preserve">
          <source>The lower left figure plots the pointwise decomposition of the expected mean squared error of a single decision tree. It confirms that the bias term (in blue) is low while the variance is large (in green). It also illustrates the noise part of the error which, as expected, appears to be constant and around &lt;code&gt;0.01&lt;/code&gt;.</source>
          <target state="translated">左下图显示了单个决策树的预期均方误差的逐点分解。它确认偏差项（蓝色）较低，而方差大（绿色）。它还说明了误差的噪声部分，正如预期的那样，该部分似乎是恒定的，约为 &lt;code&gt;0.01&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="caf1b50ddc9e3b679e122c659dff7510d4ed2966" translate="yes" xml:space="preserve">
          <source>The lower the better.</source>
          <target state="translated">越低越好。</target>
        </trans-unit>
        <trans-unit id="bfd900c27bac872165454194e9ba0284baa1f3aa" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems.</source>
          <target state="translated">在计算Cholesky对角线因子时的机器精度正则化。对于条件很差的系统,增加这个。</target>
        </trans-unit>
        <trans-unit id="69abd1713b39f884bea4ffeed0d502e131117e74" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &amp;lsquo;tol&amp;rsquo; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.</source>
          <target state="translated">Cholesky对角线因子计算中的机器精度正则化。对于条件非常恶劣的系统，请增加此值。与某些基于迭代优化的算法中的&amp;ldquo; tol&amp;rdquo;参数不同，此参数不控制优化的容差。</target>
        </trans-unit>
        <trans-unit id="03f1e7333f5d863384674ba69d2200c51c214771" translate="yes" xml:space="preserve">
          <source>The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the &lt;code&gt;tol&lt;/code&gt; parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.</source>
          <target state="translated">Cholesky对角线因子计算中的机器精度正则化。对于条件非常恶劣的系统，请增加此值。与某些基于迭代优化的算法中的 &lt;code&gt;tol&lt;/code&gt; 参数不同，此参数不控制优化的容差。</target>
        </trans-unit>
        <trans-unit id="686cc4307ca584641912f9ca8288a53bb10ce6de" translate="yes" xml:space="preserve">
          <source>The main advantage for Factor Analysis over &lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt;&lt;code&gt;PCA&lt;/code&gt;&lt;/a&gt; is that it can model the variance in every direction of the input space independently (heteroscedastic noise):</source>
          <target state="translated">与&lt;a href=&quot;generated/sklearn.decomposition.pca#sklearn.decomposition.PCA&quot;&gt; &lt;code&gt;PCA&lt;/code&gt; &lt;/a&gt;相比，因子分析的主要优势在于它可以独立地模拟输入空间每个方向上的方差（异方差噪声）：</target>
        </trans-unit>
        <trans-unit id="c6108525766abe52c974030991b5e8a7ddcb6e28" translate="yes" xml:space="preserve">
          <source>The main difficulty in learning Gaussian mixture models from unlabeled data is that it is one usually doesn&amp;rsquo;t know which points came from which latent component (if one has access to this information it gets very easy to fit a separate Gaussian distribution to each set of points). &lt;a href=&quot;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;Expectation-maximization&lt;/a&gt; is a well-founded statistical algorithm to get around this problem by an iterative process. First one assumes random components (randomly centered on data points, learned from k-means, or even just normally distributed around the origin) and computes for each point a probability of being generated by each component of the model. Then, one tweaks the parameters to maximize the likelihood of the data given those assignments. Repeating this process is guaranteed to always converge to a local optimum.</source>
          <target state="translated">从未标记的数据中学习高斯混合模型的主要困难在于，通常不知道哪个点来自哪个潜在分量（如果可以访问此信息，则很容易将单独的高斯分布拟合到每一组点）。&lt;a href=&quot;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;期望最大化&lt;/a&gt;是一种基于迭代的过程来解决此问题的可靠统计算法。第一个假设随机成分（随机地以数据点为中心，从k均值中获悉，或者甚至只是围绕原点正态分布），并为每个点计算由模型的每个成分生成的概率。然后，调整这些参数以在给定这些分配的情况下最大化数据的可能性。重复此过程可确保始终收敛到局部最优值。</target>
        </trans-unit>
        <trans-unit id="d2139f3f89c20ed8a9cf480b63f0750e12fef92b" translate="yes" xml:space="preserve">
          <source>The main documentation. This contains an in-depth description of all algorithms and how to apply them.</source>
          <target state="translated">主要文件。它包含了对所有算法的深入描述以及如何应用它们。</target>
        </trans-unit>
        <trans-unit id="5f94da6f46e5f5e5d800fbb67f1c2ae40caf9c89" translate="yes" xml:space="preserve">
          <source>The main drawback of Affinity Propagation is its complexity. The algorithm has a time complexity of the order \(O(N^2 T)\), where \(N\) is the number of samples and \(T\) is the number of iterations until convergence. Further, the memory complexity is of the order \(O(N^2)\) if a dense similarity matrix is used, but reducible if a sparse similarity matrix is used. This makes Affinity Propagation most appropriate for small to medium sized datasets.</source>
          <target state="translated">亲和传播的主要缺点是其复杂性。该算法的时间复杂度为 \(O(N^2 T)\),其中 \(N\)是样本数量,而 \(T\)是收敛前的迭代次数。此外,如果使用密集的相似性矩阵,内存复杂度的顺序为/(O(N^2)\),但如果使用稀疏的相似性矩阵,内存复杂度是可以降低的。这使得Affinity Propagation最适合中小型数据集。</target>
        </trans-unit>
        <trans-unit id="f23633268affa3b7ac5da4b687edb4096985bac5" translate="yes" xml:space="preserve">
          <source>The main factors that influence the prediction latency are</source>
          <target state="translated">影响预测延时的主要因素是</target>
        </trans-unit>
        <trans-unit id="26167f635fda3c2035f2a73e215b9329a59f7a43" translate="yes" xml:space="preserve">
          <source>The main observations to make are:</source>
          <target state="translated">要提出的主要意见是:</target>
        </trans-unit>
        <trans-unit id="b5b8962f0fe90f98d375c2c689eb4ed0da37d3b3" translate="yes" xml:space="preserve">
          <source>The main parameters to adjust when using these methods is &lt;code&gt;n_estimators&lt;/code&gt; and &lt;code&gt;max_features&lt;/code&gt;. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are &lt;code&gt;max_features=n_features&lt;/code&gt; for regression problems, and &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; for classification tasks (where &lt;code&gt;n_features&lt;/code&gt; is the number of features in the data). Good results are often achieved when setting &lt;code&gt;max_depth=None&lt;/code&gt; in combination with &lt;code&gt;min_samples_split=2&lt;/code&gt; (i.e., when fully developing the trees). Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM. The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are used by default (&lt;code&gt;bootstrap=True&lt;/code&gt;) while the default strategy for extra-trees is to use the whole dataset (&lt;code&gt;bootstrap=False&lt;/code&gt;). When using bootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. This can be enabled by setting &lt;code&gt;oob_score=True&lt;/code&gt;.</source>
          <target state="translated">使用这些方法时要调整的主要参数是 &lt;code&gt;n_estimators&lt;/code&gt; 和 &lt;code&gt;max_features&lt;/code&gt; 。前者是森林中的树木数量。越大越好，但计算所需的时间也就越长。此外，请注意，超过关键数量的树木，结果将不再明显好转。后者是分割节点时要考虑的要素随机子集的大小。越低，方差的减少越大，但偏差的增加也越大。经验良好的默认值对于回归问题为 &lt;code&gt;max_features=n_features&lt;/code&gt; ，对于分类任务为 &lt;code&gt;max_features=sqrt(n_features)&lt;/code&gt; （其中 &lt;code&gt;n_features&lt;/code&gt; 是数据中的要素数量）。当将 &lt;code&gt;max_depth=None&lt;/code&gt; 与 &lt;code&gt;min_samples_split=2&lt;/code&gt; 结合使用时（即，完全开发树时），通常会获得良好的结果。请记住，尽管这些值通常不是最佳值，并且可能导致模型消耗大量RAM。最佳参数值应始终交叉验证。另外，请注意，在随机森林中，默认情况下使用引导程序样本（ &lt;code&gt;bootstrap=True&lt;/code&gt; ），而额外树的默认策略是使用整个数据集（ &lt;code&gt;bootstrap=False&lt;/code&gt; ）。当使用自举抽样时，可以对遗漏的样本或袋外样本进行概括精度估计。可以通过设置 &lt;code&gt;oob_score=True&lt;/code&gt; 启用此功能。</target>
        </trans-unit>
        <trans-unit id="842b51b29e297ee475304c463fec6849d760da97" translate="yes" xml:space="preserve">
          <source>The main purpose of t-SNE is visualization of high-dimensional data. Hence, it works best when the data will be embedded on two or three dimensions.</source>
          <target state="translated">t-SNE的主要目的是高维数据的可视化。因此,当数据将被嵌入到二维或三维上时,它的效果最好。</target>
        </trans-unit>
        <trans-unit id="85533a7e662fe3db2975f7f26edf978cd22f568d" translate="yes" xml:space="preserve">
          <source>The main theoretical result behind the efficiency of random projection is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;Johnson-Lindenstrauss lemma (quoting Wikipedia)&lt;/a&gt;:</source>
          <target state="translated">随机投影效率背后的主要理论结果是&lt;a href=&quot;https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma&quot;&gt;Johnson-Lindenstrauss引理（引用Wikipedia）&lt;/a&gt;：</target>
        </trans-unit>
        <trans-unit id="c6fec5e2fce31198cf0394ae7c6624fc74feba7a" translate="yes" xml:space="preserve">
          <source>The main usage of a &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt;&lt;code&gt;Kernel&lt;/code&gt;&lt;/a&gt; is to compute the GP&amp;rsquo;s covariance between datapoints. For this, the method &lt;code&gt;__call__&lt;/code&gt; of the kernel can be called. This method can either be used to compute the &amp;ldquo;auto-covariance&amp;rdquo; of all pairs of datapoints in a 2d array X, or the &amp;ldquo;cross-covariance&amp;rdquo; of all combinations of datapoints of a 2d array X with datapoints in a 2d array Y. The following identity holds true for all kernels k (except for the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt;&lt;code&gt;WhiteKernel&lt;/code&gt;&lt;/a&gt;): &lt;code&gt;k(X) == K(X, Y=X)&lt;/code&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.kernel#sklearn.gaussian_process.kernels.Kernel&quot;&gt; &lt;code&gt;Kernel&lt;/code&gt; &lt;/a&gt;的主要用途是计算数据点之间的GP协方差。为此，可以 &lt;code&gt;__call__&lt;/code&gt; 内核的__call__方法。此方法既可以用于计算2d数组X中所有数据点对的&amp;ldquo;自协方差&amp;rdquo;，也可以用于计算2d数组X数据点与2d数组Y中的数据点的所有组合的&amp;ldquo;互协方差&amp;rdquo;。对于所有内核k（&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt; &lt;code&gt;WhiteKernel&lt;/code&gt; &lt;/a&gt;除外），以下标识均成立： &lt;code&gt;k(X) == K(X, Y=X)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="54123ac391766adafa62d20ad77d558f0edc6a11" translate="yes" xml:space="preserve">
          <source>The main use-case of the &lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt;&lt;code&gt;WhiteKernel&lt;/code&gt;&lt;/a&gt; kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter \(noise\_level\) corresponds to estimating the noise-level. It is defined as:e</source>
          <target state="translated">&lt;a href=&quot;generated/sklearn.gaussian_process.kernels.whitekernel#sklearn.gaussian_process.kernels.WhiteKernel&quot;&gt; &lt;code&gt;WhiteKernel&lt;/code&gt; &lt;/a&gt;内核的主要用例是sum-kernel的一部分，它解释了信号的噪声成分。调整其参数\（noise \ _level \）对应于估计噪声级别。它定义为：e</target>
        </trans-unit>
        <trans-unit id="3b785a852cf47604fe35c8c001dbd6b426026b75" translate="yes" xml:space="preserve">
          <source>The main use-case of this kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter corresponds to estimating the noise-level.</source>
          <target state="translated">这个核的主要用途是作为和核的一部分,解释信号的噪声成分。调整它的参数相当于估计噪声水平。</target>
        </trans-unit>
        <trans-unit id="b8d24bce3b0ca4f2f608d76c9973e497dd5a4966" translate="yes" xml:space="preserve">
          <source>The major advantage of SGD is its efficiency, which is basically linear in the number of training examples. If X is a matrix of size (n, p) training has a cost of \(O(k n \bar p)\), where k is the number of iterations (epochs) and \(\bar p\) is the average number of non-zero attributes per sample.</source>
          <target state="translated">SGD的主要优势在于它的效率,它的效率基本与训练实例的数量呈线性关系。如果X是一个大小为(n,p)的矩阵,则训练的成本为\(O(k n \bar p)\),其中k是迭代次数(epochs),\(\bar p\)是每个样本非零属性的平均数。</target>
        </trans-unit>
        <trans-unit id="c142aa304b117907c4ec82e7c8f2e0e1debc825c" translate="yes" xml:space="preserve">
          <source>The manifold learning implementations available in scikit-learn are summarized below</source>
          <target state="translated">scikit-learn中的多种学习实现总结如下</target>
        </trans-unit>
        <trans-unit id="d450abcdfe5ff938dd90f9482f51850eeea5e6fe" translate="yes" xml:space="preserve">
          <source>The mapping relies on a Monte Carlo approximation to the kernel values. The &lt;code&gt;fit&lt;/code&gt; function performs the Monte Carlo sampling, whereas the &lt;code&gt;transform&lt;/code&gt; method performs the mapping of the data. Because of the inherent randomness of the process, results may vary between different calls to the &lt;code&gt;fit&lt;/code&gt; function.</source>
          <target state="translated">映射依赖于对内核值的蒙特卡洛近似。所述 &lt;code&gt;fit&lt;/code&gt; 函数执行蒙特卡罗采样，而 &lt;code&gt;transform&lt;/code&gt; 方法执行中的数据的映射。由于过程固有的随机性，因此在对 &lt;code&gt;fit&lt;/code&gt; 函数的不同调用之间，结果可能会有所不同。</target>
        </trans-unit>
        <trans-unit id="1292a72996c3834d41499b1c7abdc4ace6de6204" translate="yes" xml:space="preserve">
          <source>The mask of selected features.</source>
          <target state="translated">所选特征的掩码。</target>
        </trans-unit>
        <trans-unit id="c45c8ea0546b1c79f5dc6ae49b8b9eba3b94ab9e" translate="yes" xml:space="preserve">
          <source>The mathematical formulation is the following:</source>
          <target state="translated">数学公式如下:</target>
        </trans-unit>
        <trans-unit id="d7ffadc6e201c4cc579f6014a4ba6a553d3e6d97" translate="yes" xml:space="preserve">
          <source>The matrix</source>
          <target state="translated">矩阵</target>
        </trans-unit>
        <trans-unit id="2cec1c9e3ed6a74b7dbd8e5442d20aacdeb523d2" translate="yes" xml:space="preserve">
          <source>The matrix dimension.</source>
          <target state="translated">矩阵维度:</target>
        </trans-unit>
        <trans-unit id="e194b944fcebb1be4c3463284dee201bd8108680" translate="yes" xml:space="preserve">
          <source>The matrix inverse of the covariance matrix, often called the precision matrix, is proportional to the partial correlation matrix. It gives the partial independence relationship. In other words, if two features are independent conditionally on the others, the corresponding coefficient in the precision matrix will be zero. This is why it makes sense to estimate a sparse precision matrix: the estimation of the covariance matrix is better conditioned by learning independence relations from the data. This is known as &lt;em&gt;covariance selection&lt;/em&gt;.</source>
          <target state="translated">协方差矩阵的矩阵逆（通常称为精度矩阵）与偏相关矩阵成比例。它给出了部分独立关系。换句话说，如果两个特征在条件上彼此独立，则精度矩阵中的相应系数将为零。这就是为什么估计稀疏精度矩阵是有道理的：通过从数据中学习独立关系，可以更好地调节协方差矩阵的条件。这称为&lt;em&gt;协方差选择&lt;/em&gt;。</target>
        </trans-unit>
        <trans-unit id="e578a80d83c508825e0e59f2ea21cfd3dab91a77" translate="yes" xml:space="preserve">
          <source>The matrix of features, where NP is the number of polynomial features generated from the combination of inputs.</source>
          <target state="translated">特征矩阵,其中NP为输入组合生成的多项式特征数。</target>
        </trans-unit>
        <trans-unit id="64f7c7c3f44e0d1f1fa6c715782355e1f1d2054c" translate="yes" xml:space="preserve">
          <source>The matrix.</source>
          <target state="translated">矩阵。</target>
        </trans-unit>
        <trans-unit id="f3c27fa93df86412f11493765849dd6aeb05082d" translate="yes" xml:space="preserve">
          <source>The maximum depth of each tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</source>
          <target state="translated">每个树的最大深度。如果为None,那么节点将被展开,直到所有叶子都是纯的,或者直到所有叶子包含的样本小于min_samples_split样本。</target>
        </trans-unit>
        <trans-unit id="52e0ef4a9206a897434b4d55c59a4824cb11d988" translate="yes" xml:space="preserve">
          <source>The maximum depth of the representation. If None, the tree is fully generated.</source>
          <target state="translated">表示的最大深度。如果无,则完全生成树。</target>
        </trans-unit>
        <trans-unit id="e0ae7481b9fb370fd4191031ea9f70e91832a43a" translate="yes" xml:space="preserve">
          <source>The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</source>
          <target state="translated">树的最大深度。如果为None,那么节点将被展开,直到所有叶子都是纯的,或者直到所有叶子包含的样本小于min_samples_split样本。</target>
        </trans-unit>
        <trans-unit id="0cec94a1183be1885b8ca965cd0a310a0d6ef03e" translate="yes" xml:space="preserve">
          <source>The maximum distance between two samples for them to be considered as in the same neighborhood.</source>
          <target state="translated">两个样本之间的最大距离,使它们被认为是在同一邻域。</target>
        </trans-unit>
        <trans-unit id="72469eac4bccf834885ed51dab58c805d8cdc971" translate="yes" xml:space="preserve">
          <source>The maximum number of concurrently running jobs, such as the number of Python worker processes when backend=&amp;rdquo;multiprocessing&amp;rdquo; or the size of the thread-pool when backend=&amp;rdquo;threading&amp;rdquo;. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. None is a marker for &amp;lsquo;unset&amp;rsquo; that will be interpreted as n_jobs=1 (sequential execution) unless the call is performed under a parallel_backend context manager that sets another value for n_jobs.</source>
          <target state="translated">并发运行的最大作业数，例如backend =&amp;ldquo; multiprocessing&amp;rdquo;时的Python工作进程数或backend =&amp;ldquo; threading&amp;rdquo;时的线程池大小。如果为-1，则使用所有CPU。如果给定1，则根本不使用任何并行计算代码，这对于调试很有用。对于小于-1的n_job，将使用（n_cpus +1 + n_jobs）。因此，对于n_jobs = -2，将使用除一个以外的所有CPU。除非调用是在为n_jobs设置另一个值的parallel_backend上下文管理器下执行的，否则不会将&amp;ldquo; unset&amp;rdquo;的标记解释为n_jobs = 1（顺序执行）。</target>
        </trans-unit>
        <trans-unit id="d68dbcc29192b28b1c0e16950a4955da0229e9e9" translate="yes" xml:space="preserve">
          <source>The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.</source>
          <target state="translated">终止升压的估计器的最大数量。在完全拟合的情况下,学习过程提前停止。</target>
        </trans-unit>
        <trans-unit id="0ace226674121a7119418c464f4452694b61318d" translate="yes" xml:space="preserve">
          <source>The maximum number of features selected scoring above &lt;code&gt;threshold&lt;/code&gt;. To disable &lt;code&gt;threshold&lt;/code&gt; and only select based on &lt;code&gt;max_features&lt;/code&gt;, set &lt;code&gt;threshold=-np.inf&lt;/code&gt;.</source>
          <target state="translated">所选功能得分超过 &lt;code&gt;threshold&lt;/code&gt; 的最大数量。要禁用 &lt;code&gt;threshold&lt;/code&gt; 并仅基于 &lt;code&gt;max_features&lt;/code&gt; 进行选择，请设置 &lt;code&gt;threshold=-np.inf&lt;/code&gt; 。</target>
        </trans-unit>
        <trans-unit id="1360fbfbf92ec231e131ca0c0a387fb9e5b6a69f" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations</source>
          <target state="translated">最大迭代次数</target>
        </trans-unit>
        <trans-unit id="07cf3627f06a0c97fd8b5fd02bbb00e02fc4d154" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations in Newton&amp;rsquo;s method for approximating the posterior during predict. Smaller values will reduce computation time at the cost of worse results.</source>
          <target state="translated">在预测过程中，牛顿方法中用于逼近后验的最大迭代次数。较小的值将以更差的结果为代价减少计算时间。</target>
        </trans-unit>
        <trans-unit id="5ecb214f49e2d4dbf3814aaa5686d1fc9fb18e01" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations is usually high enough and does not need any tuning. The optimization consists of two phases: the early exaggeration phase and the final optimization. During early exaggeration the joint probabilities in the original space will be artificially increased by multiplication with a given factor. Larger factors result in larger gaps between natural clusters in the data. If the factor is too high, the KL divergence could increase during this phase. Usually it does not have to be tuned. A critical parameter is the learning rate. If it is too low gradient descent will get stuck in a bad local minimum. If it is too high the KL divergence will increase during optimization. More tips can be found in Laurens van der Maaten&amp;rsquo;s FAQ (see references). The last parameter, angle, is a tradeoff between performance and accuracy. Larger angles imply that we can approximate larger regions by a single point, leading to better speed but less accurate results.</source>
          <target state="translated">通常，最大迭代次数足够高，不需要任何调整。优化包括两个阶段：早期夸大阶段和最终优化。在早期夸张期间，原始空间中的联合概率将通过与给定因子相乘而人为增加。较大的因素会导致数据中自然簇之间的差距更大。如果因子太高，则在此阶段KL散度可能会增加。通常不必调整它。关键参数是学习率。如果它太低，则梯度下降将卡在不良的局部最小值中。如果过高，则在优化过程中KL散度会增加。可以在Laurens van der Maaten的FAQ中找到更多提示（请参阅参考资料）。最后一个参数，角度，是性能和精度之间的权衡。较大的角度意味着我们可以通过单个点近似较大的区域，从而导致速度更快，但结果精度较低。</target>
        </trans-unit>
        <trans-unit id="3a263d39eeb80bbc61890473f9e4f7de61b380fb" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations to be run.</source>
          <target state="translated">要运行的最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="42a16ed4baf475d1973848504fece7b7ddcdacc6" translate="yes" xml:space="preserve">
          <source>The maximum number of iterations.</source>
          <target state="translated">最大迭代次数。</target>
        </trans-unit>
        <trans-unit id="06bd569f5fc7509f2f4591004e3b0ffe2a685cf9" translate="yes" xml:space="preserve">
          <source>The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the &lt;code&gt;fit&lt;/code&gt; method, and not the &lt;code&gt;partial_fit&lt;/code&gt;. Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.</source>
          <target state="translated">通过训练数据的最大次数（又称历元）。它仅影响 &lt;code&gt;fit&lt;/code&gt; 方法中的行为，而不影响 &lt;code&gt;partial_fit&lt;/code&gt; 。默认值为5。默认值为0.21或1000，如果tol不为None。</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
